<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0">
    <channel>
        <title>HN100 - Readable Contents</title>
        <link>https://hn.algolia.com/api/v1/search_by_date?tags=%28story,poll%29&amp;numericFilters=points%3E100</link>
        <description>Uses Readability to add bodies to the RSS feed</description>
        <lastBuildDate>Sat, 17 May 2025 19:30:01 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>en</language>
        <item>
            <title><![CDATA["We would be less confidential than Google" Proton threatens to quit Switzerland (139 pts)]]></title>
            <link>https://www.techradar.com/vpn/vpn-privacy-security/we-would-be-less-confidential-than-google-proton-threatens-to-quit-switzerland-over-new-surveillance-law</link>
            <guid>44014808</guid>
            <pubDate>Sat, 17 May 2025 14:59:10 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.techradar.com/vpn/vpn-privacy-security/we-would-be-less-confidential-than-google-proton-threatens-to-quit-switzerland-over-new-surveillance-law">https://www.techradar.com/vpn/vpn-privacy-security/we-would-be-less-confidential-than-google-proton-threatens-to-quit-switzerland-over-new-surveillance-law</a>, See on <a href="https://news.ycombinator.com/item?id=44014808">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-widget-type="contentparsed" id="content">
<section>
<div>
<div>
<picture data-new-v2-image="true">
<source type="image/webp" srcset="https://cdn.mos.cms.futurecdn.net/4VDmBUhWGSAGmvb4bQ6T5R-1920-80.jpg.webp 1920w, https://cdn.mos.cms.futurecdn.net/4VDmBUhWGSAGmvb4bQ6T5R-1200-80.jpg.webp 1200w, https://cdn.mos.cms.futurecdn.net/4VDmBUhWGSAGmvb4bQ6T5R-1024-80.jpg.webp 1024w, https://cdn.mos.cms.futurecdn.net/4VDmBUhWGSAGmvb4bQ6T5R-970-80.jpg.webp 970w, https://cdn.mos.cms.futurecdn.net/4VDmBUhWGSAGmvb4bQ6T5R-650-80.jpg.webp 650w, https://cdn.mos.cms.futurecdn.net/4VDmBUhWGSAGmvb4bQ6T5R-480-80.jpg.webp 480w, https://cdn.mos.cms.futurecdn.net/4VDmBUhWGSAGmvb4bQ6T5R-320-80.jpg.webp 320w" sizes="(min-width: 1000px) 600px, calc(100vw - 40px)">
<img src="https://cdn.mos.cms.futurecdn.net/4VDmBUhWGSAGmvb4bQ6T5R.jpg" alt="Proton CEO and founder Andy Yen poses next to the Proton logo at the headquarters of the encrypted email and VPN services company in Geneva." srcset="https://cdn.mos.cms.futurecdn.net/4VDmBUhWGSAGmvb4bQ6T5R-1920-80.jpg 1920w, https://cdn.mos.cms.futurecdn.net/4VDmBUhWGSAGmvb4bQ6T5R-1200-80.jpg 1200w, https://cdn.mos.cms.futurecdn.net/4VDmBUhWGSAGmvb4bQ6T5R-1024-80.jpg 1024w, https://cdn.mos.cms.futurecdn.net/4VDmBUhWGSAGmvb4bQ6T5R-970-80.jpg 970w, https://cdn.mos.cms.futurecdn.net/4VDmBUhWGSAGmvb4bQ6T5R-650-80.jpg 650w, https://cdn.mos.cms.futurecdn.net/4VDmBUhWGSAGmvb4bQ6T5R-480-80.jpg 480w, https://cdn.mos.cms.futurecdn.net/4VDmBUhWGSAGmvb4bQ6T5R-320-80.jpg 320w" sizes="(min-width: 1000px) 600px, calc(100vw - 40px)" data-original-mos="https://cdn.mos.cms.futurecdn.net/4VDmBUhWGSAGmvb4bQ6T5R.jpg" data-pin-media="https://cdn.mos.cms.futurecdn.net/4VDmBUhWGSAGmvb4bQ6T5R.jpg" data-pin-nopin="true" fetchpriority="high">
</picture>
</div>
<figcaption>
<span>(Image credit: Photo by FABRICE COFFRINI/AFP via Getty Images)</span>
</figcaption>
</div>

<div id="article-body">
<hr><ul><li><strong>Proton CEO confirmed the company will leave Switzerland if new controversial surveillance rules pass</strong></li><li><strong>An amendment to the current surveillance law would require VPNs and messaging apps to identify and retain user data</strong></li><li><strong>Another Swiss company, NymVPN, is also ready to leave the country instead of undermining its privacy and security infrastructure</strong></li></ul><hr><p>Proton confirms the company will leave Switzerland if new controversial surveillance rules pass.</p><p>Switzerland is <a data-analytics-id="inline-link" href="https://www.techradar.com/vpn/vpn-privacy-security/secure-encryption-and-online-anonymity-are-now-at-risk-in-switzerland-heres-what-you-need-to-know" data-before-rewrite-localise="https://www.techradar.com/vpn/vpn-privacy-security/secure-encryption-and-online-anonymity-are-now-at-risk-in-switzerland-heres-what-you-need-to-know">considering amending its surveillance law</a>, with experts warning against the risk to secure encryption and online anonymity in the country. Specifically, the amendment could require all VPN services, messaging apps, and social networks to identify and retain user data – an obligation that is now limited to mobile networks and internet service providers.</p><p>The firm behind one of the <a data-analytics-id="inline-link" href="https://www.techradar.com/vpn/best-vpn" data-before-rewrite-localise="https://www.techradar.com/vpn/best-vpn">best VPN</a> and encrypted email services, Proton, is ready to fight back on behalf of the privacy of its over 100 million users. Other Swiss-based companies, like <a data-analytics-id="inline-link" href="https://www.techradar.com/pro/vpn/nymvpn" data-before-rewrite-localise="https://www.techradar.com/pro/vpn/nymvpn">NymVPN</a>, are also doing the same.</p><h2 id="no-choice-but-to-leave-3">No choice but to leave</h2><p>In an <a data-analytics-id="inline-link" href="https://www.rts.ch/info/suisse/2025/article/proton-menace-de-quitter-la-suisse-face-aux-nouvelles-regles-de-surveillance-28883036.html" target="_blank" data-url="https://www.rts.ch/info/suisse/2025/article/proton-menace-de-quitter-la-suisse-face-aux-nouvelles-regles-de-surveillance-28883036.html" referrerpolicy="no-referrer-when-downgrade" data-hl-processed="none">interview with RTS</a> (Radio Télévision Suisse) on May 13, 2025, Proton CEO Andy Yen slammed the proposed amendment as a "major violation of the right to privacy" that will also harm the country's reputation and its ability to compete on an international level.</p><p>"This revision attempts to implement something that has been deemed illegal in the EU and the United States. The only country in Europe with a roughly equivalent law is Russia," said Yen.</p><p><a data-analytics-id="inline-link" href="https://www.news.admin.ch/fr/nsb?id=103968" target="_blank" data-url="https://www.news.admin.ch/fr/nsb?id=103968" referrerpolicy="no-referrer-when-downgrade" data-hl-processed="none">The amendment</a> aims to expand the number of service providers targeted to include so-called "derived service providers". Crucially, the new provisions will introduce three new types of information and two types of monitoring.</p><p>If the changes pass, Proton will be forced to modify how <a data-analytics-id="inline-link" href="https://www.techradar.com/reviews/protonmail-secure-email" data-before-rewrite-localise="https://www.techradar.com/reviews/protonmail-secure-email">Proton Mail</a> and <a data-analytics-id="inline-link" href="https://www.techradar.com/reviews/protonvpn" data-before-rewrite-localise="https://www.techradar.com/reviews/protonvpn">Proton VPN</a> handle encryption, alongside its strict no-log policies – something the company isn't willing to do.</p><p>"I think we would have no choice but to leave Switzerland," said Yen. "The law would become almost identical to the one in force today in Russia. It's an untenable situation. We would be less confidential as a company in Switzerland than Google, based in the United States. So it's impossible for our business model."</p><div><blockquote data-lang="en"><p lang="en" dir="ltr">In Switzerland, the new version of the surveillance law aims to make it impossible for Proton, Threema and@nymproject to operate from Switzerland. We are in the consultation phase. We will fight. https://t.co/BcMBxzIPFC<a href="https://twitter.com/cantworkitout/status/1904483355812377045" data-url="https://twitter.com/cantworkitout/status/1904483355812377045" target="_blank" referrerpolicy="no-referrer-when-downgrade" data-hl-processed="none">March 25, 2025</a></p></blockquote></div><p>Proton is not alone in feeling this way, though.</p><p>A new player in the VPN world, <a data-analytics-id="inline-link" href="https://www.techradar.com/vpn/vpn-services/nymvpn-is-now-live-heres-everything-you-need-to-know" data-before-rewrite-localise="https://www.techradar.com/vpn/vpn-services/nymvpn-is-now-live-heres-everything-you-need-to-know">NymVPN</a> has also been publicly fighting Swiss government plans since the beginning.</p><p>Talking to TechRadar, Nym's co-founder and COO, Alexis Roussel, confirmed that Nym will do the same and leave Switzerland if the new surveillance rules are enforced.</p><h2 id="what-s-next-3">What's next?</h2><p>As public consultations ended on May 6, 2025, we will now have to wait and see what the Swiss government decides.</p><p>Nonetheless, Roussel confirmed to TechRadar that there has been significant push-back from political parties and Swiss companies.</p><p>Some Cantons, <a data-analytics-id="inline-link" href="https://www.ge.ch/document/39174/telecharger" target="_blank" data-url="https://www.ge.ch/document/39174/telecharger" referrerpolicy="no-referrer-when-downgrade" data-hl-processed="none">including Geneva</a>, have even called on the right to digital integrity as an argument against these rules. Roussel was the main originator of the initiative that <a data-analytics-id="inline-link" href="https://www.swissinfo.ch/eng/democracy/how-swiss-federalism-is-helping-the-rise-of-a-new-digital-right/89023201" target="_blank" data-url="https://www.swissinfo.ch/eng/democracy/how-swiss-federalism-is-helping-the-rise-of-a-new-digital-right/89023201" referrerpolicy="no-referrer-when-downgrade" data-hl-processed="none">introduced this new right</a> to protect citizens' online privacy and data – in Geneva in 2023 and Neuchâtel in 2024 – with over 90% consensus.</p><p>Yen also told RTS to be more optimistic, despite pointing out how this matter shows the need for a more balanced approach when it comes to crafting new laws.</p><p>"If we can get Bern to adopt common-sense rules that allow companies like Proton to be competitive in Switzerland and around the world, I will stay, take my passport, and continue to invest in Switzerland," he added.</p><h3 id="section-you-might-also-like"><span>You might also like</span></h3><ul><li><a href="https://www.techradar.com/vpn/vpn-services/once-you-have-the-data-you-have-to-cooperate-windscribe-ceo-speak-out-against-global-threats-to-no-log-vpns" data-before-rewrite-localise="https://www.techradar.com/vpn/vpn-services/once-you-have-the-data-you-have-to-cooperate-windscribe-ceo-speak-out-against-global-threats-to-no-log-vpns">Windscribe CEO speaks out against global threats to no-log VPNs</a></li><li><a href="https://www.techradar.com/vpn/vpn-privacy-security/encryption-backdoors-privacy-can-be-misused-but-the-cost-of-a-world-without-is-so-much-higher" data-before-rewrite-localise="https://www.techradar.com/vpn/vpn-privacy-security/encryption-backdoors-privacy-can-be-misused-but-the-cost-of-a-world-without-is-so-much-higher">Encryption backdoors: privacy can be misused, "but the cost of a world without is so much higher"</a></li><li><a href="https://www.techradar.com/vpn/vpn-privacy-security/a-win-for-privacy-florida-rejects-the-encryption-backdoor-law-for-social-media" data-before-rewrite-localise="https://www.techradar.com/vpn/vpn-privacy-security/a-win-for-privacy-florida-rejects-the-encryption-backdoor-law-for-social-media">"A win for privacy" – Florida rejects the encryption backdoor law for social media</a></li></ul>
</div>

<div id="slice-container-authorBio-QWbBN83AJHY8NHNgbHUYZL"><p>Chiara is a multimedia journalist committed to covering stories to help promote the rights and denounce the abuses of the digital side of life – wherever cybersecurity, markets, and politics tangle up. She believes an open, uncensored, and private internet is a basic human need and wants to use her knowledge of VPNs to help readers take back control. She writes news, interviews, and analysis on data privacy, online censorship, digital rights, tech policies, and security software, with a special focus on VPNs, for TechRadar and TechRadar Pro. Got a story, tip-off, or something tech-interesting to say? Reach out to chiara.castro@futurenet.com</p></div>
</section>




</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Palette lighting tricks on the Nintendo 64 (151 pts)]]></title>
            <link>https://30fps.net/pages/palette-lighting-tricks-n64/</link>
            <guid>44014587</guid>
            <pubDate>Sat, 17 May 2025 14:28:59 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://30fps.net/pages/palette-lighting-tricks-n64/">https://30fps.net/pages/palette-lighting-tricks-n64/</a>, See on <a href="https://news.ycombinator.com/item?id=44014587">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="text">

<p><em>This article is a continuation to <a href="https://bsky.app/profile/pekkavaa.bsky.social/post/3lnnwax4vxk2v">my Bluesky thread</a> from April.</em></p>
<!-- ![](castello_screenshot.jpg) -->
<p>We made a Nintendo 64 demo for <a href="https://2025.revision-party.net/">Revision 2025</a>!</p>
<center>
<iframe width="100%" src="https://www.youtube.com/embed/v3wYV6gxJII" title="Real-time tech demo Castello (N64)" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen="">
</iframe>
</center>
<p>It has baked lighting with normal mapping and real-time specular shading, ahem, well sort of.
More on that later.
The beautiful song was made by <a href="https://bsky.app/profile/did:plc:svoizwy5mp6q4ol6j2pu74we">noby</a> with guitar performed by Moloko (<a href="https://soundcloud.com/sou_andrade">https://soundcloud.com/sou_andrade</a>).</p>
<p>Below I have some notes on the directional ambient and normal mapping techniques I developed.
They are both pretty simple in the end but I haven’t seen them used elsewhere.</p>
<h2 id="but-wait-normal-mapping-on-the-n64">But wait, normal mapping on the N64?</h2>
<p>I knew normal mapping on the N64 was possible due to earlier experiments by fellow homebrew developers WadeTyhon and <a href="https://www.youtube.com/@SpookyIluha">Spooky Iluha</a>. I had also done <a href="https://www.youtube.com/watch?v=UOHdDllyqOU">some emboss bump mapping hacks</a> myself.</p>
<p>The approach explained in this article is not new: <strong>the renderer computes lighting directly to textures at runtime</strong>.
It’s great because no specialized hardware support is needed and you can run arbitrary shading code on the CPU.
Too bad it’s so slow…</p>
<!-- The Nintendo 64 supports no shaders but it has a bunch of registers you can program for different combinations of textures and interpolated vertex colors. This kind of graphics hardware is known as a "register combiner" and it's also how early GeForce chips worked. -->
<h2 id="shading-a-palette-instead">Shading a palette instead</h2>
<p>So the idea is to do texture-space shading on the CPU.
But what if it’s a palette texture we’re shading?
Those are very common on the N64 anyway.
In that case it’s enough to update <em>only the palette</em> and the texture will respond as if we computed lighting for each texel.
Instant savings!</p>
<!-- In this case it's optimized by fitting a 256-color palette to the normal map, and then computing lighting only for each entry in the palette for speed. -->
<figure>
<img src="https://30fps.net/pages/palette-lighting-tricks-n64/palettes_example.jpg" alt="A demonstration of “palette-space” shading. When the palettes update, the full textures update too. When mapped to an object, it looks like the shading changed.">

</figure>
<p>The original palette is replaced with a shaded palette and the palette texture is applied as a regular texture to an object.
With just a diffuse “dot(N,L)” lighting the results look pretty good:
<!-- The shading model is basically `color=basecolor*dot(normal, light)`, but since this is on the CPU side, you could use any formula. --></p>
<figure>
<img src="https://30fps.net/pages/palette-lighting-tricks-n64/potatorock.png" alt="Another view of the above potato-shaped rock mesh.">

</figure>
<!-- The result looks pretty convincing if you don't see any UV map seams. -->
<p>In the above example I also did shading in linear space by undoing the gamma correction of the color texture :) In the final demo it wasn’t possible because I split the ambient and direct light terms to be combined by N64’s RDP unit in hardware.</p>
<h3 id="object-space-normal-mapping">Object-space normal mapping</h3>
<p>Usually normal mapping is done in tangent space.
This is way you can use repeating textures and the fine normals can modulate smoothly varying vertex normals.
A tangent-space normal map of a single color represents a smooth surface.</p>
<p>An object-space normal is simpler but more constrained.
Now the normal map’s texels don’t represent deviations from the vertex normals but absolute surface normals instead.
The runtime math becomes simpler – just read a color from a texture – but all surface points now need a unique texel, like in lightmaps.</p>
<!-- ![An early object-space normal mapping experiment in Blender. I reduced the color count of a baked normal map in an image editor and reapplied it back on the object. The results validated that the approach might work.](2025-01-21-object-space-normal-map-compression-comparison.jpg) -->
<figure>
<img src="https://30fps.net/pages/palette-lighting-tricks-n64/baseline_vs_32_palette_objectspace_normalmap.png" alt="An early experiment to validate the approach on a high-res normal map. Left: The original object-space normal map. Right: Compressed to a 32-color palette.">

</figure>

<p>The objects have both a diffuse texture (basecolor * ao) and a normal map.
Both textures actually share the same palette indices that I generated with scikit-learn’s <a href="https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html">K-means clustering</a>.
The images were interpreted as a single six-channel image for that to work.
<!-- It took a lot of tweaking to persuade k-means to behave and weight both textures fairly. --></p>
<p>Below is an example how the compression looks with a tangent-space normal map.</p>
<figure>
<img src="https://30fps.net/pages/palette-lighting-tricks-n64/compression_diagram.png" alt="A roof tile texture compression example. An RGB diffuse texture and a normal map are both compressed to a 16-color palette image in a way that palette indices are shared. Therefore the actual image data has to be stored just once at 4 bits per pixel.">

</figure>
<!-- The objects still need a varying surface color, even though they are shaded with a normal map. -->
<!-- I achieved this by combining a diffuse map and an object-space normal map to a single six-channel image, and fit a palette to *that*. -->
<!-- I had to tweak the colors-vs-normals weights for each texture though. -->
<p>At shading time, which can happen on load or on each frame, each palette color is processed in a for loop.
A single index is used to fetch a normal and a surface diffuse color.
The CPU-side shader code then produces a new RGB color for that index.
The result of the loop is a new palette but with shading applied.</p>
<p>Unfortunately this approach only really works with directional lights.
It’s also difficult to represent any kind of shadows with just the palette alone.
That’s why I started looking into how baked lighting could fit in the to the equation.</p>
<h2 id="baked-directional-ambient-and-sun-light">Baked directional ambient and sun light</h2>
<p>I wanted the demo to have a building with realistic lighting.
Perhaps it was a bit too ambitious😅
After a lot of deliberation, I put ambient and direct sun lighting in vertex color RGB and alpha channels, respectively.
The ambient term is further split into a directional intensity (a greyscale environment map) and color (vertex RGB with a saturation boost).
The sun is a directional light whose visibility is transmitted in vertex alpha.</p>
<p>The shading formula is therefore this:</p>
<pre><code>ambient = vertex_rgb      * grey_irradiance_map(N) 
direct  = vertex_alpha    * sun_color * dot(N, sun_dir)
color   = diffuse_texture * (ambient + direct)</code></pre>
<!-- Ambient color is stored in `vertex_rgb` and sun visibility in `vertex_alpha`. -->
<p>Here’s how the different terms look:</p>
<figure>
<img src="https://30fps.net/pages/palette-lighting-tricks-n64/lighting_comparison.jpg" alt="Lighting decomposition.">

</figure>
<p>Note how the messy “Sun visibility” vertex colors get neatly masked out by the sun (N.L) computation in the bottom right corner.
In the end the ambient and direct terms are summed to get the shaded result below.</p>
<figure>
<img src="https://30fps.net/pages/palette-lighting-tricks-n64/shaded.jpg" alt="Shaded result.">

</figure>
<p>The thing about directional ambient is that even the baked lighting is rough, the details in the textures still make it look pretty high end.
Consider this scene that has just a colored blurred environment map and per-vertex ambient occlusion:</p>
<figure>
<img src="https://30fps.net/pages/palette-lighting-tricks-n64/ibl.png" alt="Image-based ambient lighting. In this image only an ambient sky light is enabled. Also shows the palettes used (top left corner).">

</figure>
<p>It really pops!
I love image-based lighting.</p>
<p>For the blurred environment maps, I used an equirectangular projection for simplicity.
Polyhaven’s HDRIs already use the projection.
Since I precomputed the shading at load time, the complex sampling math wasn’t an issue.</p>
<!-- Consider these messy vertex colors:

![Direct light intensity only.](pillars_vertex_alpha.png)

When these vertex colors are modulated by the surface texture, the shaded normal map, and a directional ambient term, the shading looks pretty convincing:

![Combined ambient and direct light.](pillars_shaded.png)

Here's using a colored environment and per-vertex ambient occlusion: -->
<!-- Conceptually, on each texture texel the renderer samples the irradiance map with the surface normal, multiplies the resulting sky color with the surface color. In this case the texture is then modulated by per-vertex ambient occlusion but in the final demo I also had other bounce light in the vertex color RGB channels. -->
<figure>
<img src="https://30fps.net/pages/palette-lighting-tricks-n64/hdri_plot.png" alt="Visualization of an 64x32 environment map (right) before it gets blurred to an irradiance map. The dot sphere on the left shows the image pixels mapped to unit sphere directions.">

</figure>
<!-- Both ambient and direct light respect normal maps. Ambient uses image-based lighting and the direct light is just a single directional light. Environment lighting is provided by a greyscale irradiance map (think of a blurred cubemap) and is later modulated by vertex colors. -->
<!-- ![The original 3D reconstruction geometry was way too dense.](zumaglia_wireframe2.png)

![A cleaned-up model. I used Instant Meshes for this step. It's really good!](zumaglia_mesh2.png)

The castle model in the demo is based on [a 3D reconstruction by Sketchfab user andxet](https://sketchfab.com/3d-models/zumaglias-castle-bi-italy-8ef740a8ca31498c9e8f73b1c27a3298).
I retopo'd and textured it myself. I'm still very slow working in Blender so the model was left in a pretty rough state in the end.
[Instant Meshes](https://github.com/wjakob/instant-meshes) was a great help in this process. -->
<h2 id="shading-a-larger-model-with-repeating-textures">Shading a larger model with repeating textures</h2>
<p>I designed the original shading algorithm for single objects and only tested it with the <code>potato_rock.obj</code> you saw in the beginning.
For the demo, the castle mesh’s repeating textures posed a problem.
As a workaround, I split the large mesh into submeshes that each conceptually share the same object-space normal map.</p>
<p>The task was done primarily by yours truly manually in Blender, by grouping geometry by material and surface direction.
The computer did its part by calculating a world-to-model matrix based on polygon normals for each group.
That is a pretty much an approximate tangent space.
So I couldn’t escape them in the end!</p>
<p>Each of these groups shares a palette so as a whole their lighting will be correct only in the average sense.</p>
<figure>
<img src="https://30fps.net/pages/palette-lighting-tricks-n64/cube_tangents.png" alt="Tangent-space basis vector visualization for a simple cube. In the final model many polygons that point roughly in the same direction have to share the same tangent space.">

</figure>
<p>The tangent spaces are <em>not</em> interpolated at runtime, which shows up as faceted lighting.
This is perhaps the biggest downside of this technique.</p>
<figure>
<img src="https://30fps.net/pages/palette-lighting-tricks-n64/lighting_facet.png" alt="Lighting isn’t interpolated smoothly on this arch because the tangent spaces are constant over polygons, unlike in proper tangent-space normal mapping.">

</figure>
<!-- The skydome consists of a 32x64 texture that repeats horizontally and a cut sphere with some vertex coloring. I think it looked alright in the end, even though it is blurry.

![The skydome model.](skydome.jpg)

Regarding bloom, it's done on the CPU and composited back as a white quad with an alpha texture. A bit slow and buggy though🙂

![The sky dome with some bloom.](skydomebloom.jpg)

Finally, the white "egg" was supposed to be just a test model that I replace with something else. Well, that didn't happen!

![How the ending of the demo could have been.](cat_statue.jpg)

It's a perfect sphere but due to projection precision issues (!) it got stretched. Later I wanted to put a cat sit on top of it but sadly couldn't make it look right on time.😿 -->
<h2 id="specular-shading">Specular shading</h2>
<!-- Unfortunately many surface points now share a single shaded color. -->
<p>Since many surface points now share the same shaded color, computing point light or specular shading correctly is not possible.
The “palette-space” approach only really works for diffuse directional lights because the shading formulas don’t need a “to camera” vector <span><em>V</em></span>, which depends on the position of the shaded surface point.
Yet still I tried to hack it for the speculars :)</p>
<p>If we approximate the object to be shaded as a sphere, then the point <em>p</em> being shaded is simply <code>p=radius*normal</code>.
We must also accept that the result will look faceted since many surface points share the same palette index.</p>
<figure>
<img src="https://30fps.net/pages/palette-lighting-tricks-n64/lion.jpg" alt="Fresnel shading. The sculpt was approximated as a stretched sphere in lighting calculations.">

</figure>
<p>In the demo, the specular highlights looked a bit funny but still they seemed to fool most people. I count this as a success.</p>
<h2 id="is-this-the-future">Is this the future?</h2>
<p>In the demo I tried to hide the main limitations of the technique: shading discontinuities, only greyscale textures supported (!), no point lights.
So it really only works with elaborate preprocessing.
I’d love to see the shading discontinuity issue solved somehow (Spooky Iluha’s techniques don’t have it) without losing support for both ambient and direct lighting.
I don’t know if it’s possible but that’s what makes this hobby so fun :)</p>
<p>A <a href="https://files.scene.org/view/parties/2025/revision25/wild/castello.zip">PAL-compatible N64 ROM</a> is available but note that it crashes a lot.</p>
<hr>
<p><em>I’m also thinking of writing a book. <a href="https://30fps.net/book">Sign up here</a> if you’re interested.</em></p>



</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Pyrefly: A new type checker and IDE experience for Python (127 pts)]]></title>
            <link>https://engineering.fb.com/2025/05/15/developer-tools/introducing-pyrefly-a-new-type-checker-and-ide-experience-for-python/</link>
            <guid>44013913</guid>
            <pubDate>Sat, 17 May 2025 12:47:40 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://engineering.fb.com/2025/05/15/developer-tools/introducing-pyrefly-a-new-type-checker-and-ide-experience-for-python/">https://engineering.fb.com/2025/05/15/developer-tools/introducing-pyrefly-a-new-type-checker-and-ide-experience-for-python/</a>, See on <a href="https://news.ycombinator.com/item?id=44013913">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>

		<p><span>Today we are announcing an alpha version of </span><a href="https://pyrefly.org/" target="_blank" rel="noopener"><span>Pyrefly</span></a><span>, an open source Python type checker and IDE extension crafted in <a href="https://engineering.fb.com/2021/04/29/developer-tools/rust/" target="_blank" rel="noopener">Rust</a>. Pyrefly is a static type checker that analyzes Python code to ensure type consistency and help you catch errors throughout your codebase before your code runs. It also supports IDE integration and CLI usage to give you flexibility in how you incorporate it into your workflow.&nbsp;</span></p>
<p><span>The open source community is the backbone of the Python language. We are eager to collaborate on Pyrefly with the community and improve Python’s type system and the many libraries that we all rely on.&nbsp;&nbsp;</span></p>
<h2><span>Get started</span></h2>
<p><span>Ready to dive in? </span><a href="https://pyrefly.org/" target="_blank" rel="noopener"><span>The official Pyrefly website</span></a><span> has all the details, but to quickly get started:</span></p>
<ul>
<li aria-level="1"><a href="https://pyrefly.org/en/docs/installation/" target="_blank" rel="noopener"><span>Install</span></a><span> Pyrefly on the command-line: </span><span>pip install pyrefly</span><span>.</span></li>
<li aria-level="1"><span><a href="https://pyrefly.org/en/docs/migrating-to-pyrefly/" target="_blank" rel="noopener">Migrate your existing type checker configuration to Pyrefly</a>.</span></li>
<li aria-level="1"><span>Enhance Your IDE: Download the </span><a href="https://marketplace.visualstudio.com/items?itemName=meta.pyrefly" target="_blank" rel="noopener"><span>Pyrefly extension for VSCode</span></a><span> and enjoy a lightning fast IDE experience from starter projects to monorepos.</span></li>
<li aria-level="1"><span>Leave feedback for us on </span><a href="https://github.com/facebook/pyrefly/issues" target="_blank" rel="noopener"><span>GitHub</span></a><span>.</span></li>
</ul>
<h2><span>Why we built Pyrefly</span></h2>
<p><span>Back in 2017, we embarked on a mission to create a type checker that could handle </span><a href="https://instagram-engineering.com/web-service-efficiency-at-instagram-with-python-4976d078e366" target="_blank" rel="noopener"><span>Instagram’s massive codebase</span></a><span> of typed Python. This mission led to the birth of the </span><a href="https://github.com/facebook/pyre-check" target="_blank" rel="noopener"><span>Pyre</span></a><span> type checker, inspired by the robust designs of </span><a href="https://hacklang.org/" target="_blank" rel="noopener"><span>Hack</span></a><span> and </span><a href="https://flow.org/"><span>Flow</span></a><span>, and written in OCaml to deliver scalable performance.&nbsp;</span></p>
<p><span>Over the years, Pyre served us well, but as the type system evolved and the need for typechecking to drive responsive IDE emerged, it was clear that we needed to take a new approach. We explored alternate solutions and leveraged community tools like </span><a href="https://github.com/Microsoft/pyright" target="_blank" rel="noopener"><span>Pyright</span></a><span> for code navigation. But the need for an extensible type checker that can bring code navigation, checking at scale, and exporting types to other services drove us to start over, creating Pyrefly.&nbsp;</span></p>
<h2><span>The principles behind Pyrefly</span></h2>
<p><span>Today, we’re excited to unveil Pyrefly, a project <a href="https://github.com/facebook/pyrefly" target="_blank" rel="noopener">we’ve been developing openly on </a></span><span>GitHub</span><span>. We invite you to explore our work and try it out on your own project. While a project like Pyrefly is the sum of thousands of technical choices, a few notable principles we’ve followed are:</span></p>
<h3>Performance</h3>
<p><span>We want to shift checks that used to happen later on CI to happening on every single keystroke. That requires checking code at speed (on large codebases we can check 1.8 million lines of code per second!) and careful thought to incrementality and updates. Pyrefly is implemented in Rust and designed for high performance on codebases of all sizes.</span></p>
<h3>IDE first</h3>
<p><span>We want the IDE and command line to share a consistent view of the world, which means crafting abstractions that capture the differences without incurring unnecessary costs. Designing these abstractions from the beginning is much easier than retrofitting them, which we tried with Pyre.</span></p>
<h3>Inference</h3>
<p><span>Some </span><a href="https://engineering.fb.com/2024/12/09/developer-tools/typed-python-2024-survey-meta/" target="_blank" rel="noopener"><span>Python programs are typed</span></a><span>, but many aren’t. We want users to benefit from types even if they haven’t annotated their code – so automatically infer types for returns and local variables and display them in the IDE. What’s more, in the IDE you can even double click to insert these inferred types if you think that would make the program better.</span></p>
<h3>Open source</h3>
<p><span>Python is open source, and hugely popular. The </span><a href="https://typing.python.org/en/latest/spec/" target="_blank" rel="noopener"><span>Python typing specification</span></a><span> is open source, which made Pyrefly vastly easier to develop. Many of the libraries Meta contributes to are open source,( e.g., </span><a href="https://pytorch.org/" target="_blank" rel="noopener"><span>PyTorch</span></a><span>).</span></p>
<p><span>Pyrefly is also open source, </span><a href="https://github.com/facebook/pyrefly/" target="_blank" rel="noopener"><span>available on GitHub</span></a><span> under the </span><a href="https://github.com/facebook/pyrefly/blob/main/LICENSE" target="_blank" rel="noopener"><span>MIT license</span></a><span>, and we encourage </span><a href="https://github.com/facebook/pyrefly/pulls" target="_blank" rel="noopener"><span>pull requests</span></a><span> and </span><a href="https://github.com/facebook/pyrefly/issues" target="_blank" rel="noopener"><span>issue reports</span></a><span>. We also have a </span><a href="https://discord.gg/Cf7mFQtW7W" target="_blank" rel="noopener"><span>Discord channel</span></a><span> for more free flowing discussions. We would love to build a community around Pyrefly.</span></p>
<h2><span>The future of Pyrefly</span></h2>
<p><span>We will work with the Python community to drive the language forward and improve the developer experience. Since the beginning of Pyre, we open sourced our code and contributed a number of PEPs alongside the community of type checker maintainers. We feel we can do more with Pyrefly to help Python developers leverage the benefits of types for developers, library authors, and folks just learning the language.&nbsp;</span></p>
<p><span>Meta has leveraged types in dynamic languages from the beginning and knows the significant benefits it brings to developer productivity and security. We plan to share more of our learnings and tooling with </span><a href="https://engineering.fb.com/2024/12/09/developer-tools/typed-python-2024-survey-meta/" target="_blank" rel="noopener"><span>blogs</span></a><span>, better types in the ecosystem and language enhancements.&nbsp;</span></p>
<p><span>Today we’re releasing Pyrefly as an alpha. At the same time, we’re busy burning down the long-tail of bugs and features aiming to remove the alpha label this Summer. Your feedback is invaluable to get there, so please give it a try and </span><a href="https://github.com/facebook/pyrefly/issues" target="_blank" rel="noopener"><span>report your bugs</span></a><span> or things you think can be improved. Even if Pyrefly isn’t right for your project, we would love to hear how you use types and what you would like to see improved in your editor.</span></p>
<p><span>Join us on the journey as we help illuminate your bugs with Pyrefly. Happy coding! 🐍✨</span></p>
<h2><span>Hear more about Pyrefly&nbsp;</span></h2>
<p><span>Check out the <a href="https://engineering.fb.com/2025/05/15/developer-tools/open-sourcing-pyrefly-a-faster-python-type-checker-written-in-rust" target="_blank" rel="noopener">episode of the Meta Tech Podcast</a> where several team members share their experience developing Pyrefly and technical details for how it works. We also just </span><a href="https://us.pycon.org/2025/schedule/presentation/118/" target="_blank" rel="noopener"><span>talked at PyCon US</span></a><span> about high-performance Python through faster type checking and free threaded execution.</span></p>
<p><span>To learn more about Meta Open Source, visit our </span><a href="https://opensource.fb.com/" target="_blank" rel="noopener"><span>open source site</span></a><span>, subscribe to our </span><a href="https://www.youtube.com/channel/UCCQY962PmHabTjaHv2wJzfQ" target="_blank" rel="noopener"><span>YouTube channel</span></a><span>, or follow us on </span><a href="https://www.facebook.com/MetaOpenSource" target="_blank" rel="noopener"><span>Facebook</span></a><span>, </span><a href="https://www.threads.net/@metaopensource" target="_blank" rel="noopener"><span>Threads</span></a><span>, </span><a href="https://x.com/MetaOpenSource" target="_blank" rel="noopener"><span>X</span></a>,<span> and </span><a href="https://www.linkedin.com/showcase/meta-open-source?fbclid=IwZXh0bgNhZW0CMTEAAR2fEOJNb7zOi8rJeRvQry5sRxARpdL3OpS4sYLdC1_npkEy60gBS1ynXwQ_aem_mJUK6jEUApFTW75Emhtpqw" target="_blank" rel="noopener"><span>LinkedIn</span></a><span>.</span></p>
<h2><span>Acknowledgements&nbsp;</span></h2>
<p><i><span>Pyrefly was created By Meta’s Python Language Tooling Team: Jia Chen, Rebecca Chen, Sam Goldman, David Luo, Kyle Into, Zeina Migeed, Neil Mitchell, Maggie Moss, Conner Nilsen, Aaron Pollack, Teddy Sudol, Steven Troxler, Lucian Wischik, Danny Yang, and Sam Zhou.</span></i></p>

		
	</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Push Ifs Up and Fors Down (254 pts)]]></title>
            <link>https://matklad.github.io/2023/11/15/push-ifs-up-and-fors-down.html</link>
            <guid>44013157</guid>
            <pubDate>Sat, 17 May 2025 09:31:55 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://matklad.github.io/2023/11/15/push-ifs-up-and-fors-down.html">https://matklad.github.io/2023/11/15/push-ifs-up-and-fors-down.html</a>, See on <a href="https://news.ycombinator.com/item?id=44013157">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
      <article>
        <h2>
          Push Ifs Up And Fors Down <time datetime="2023-11-15">Nov 15, 2023</time>
        </h2>
        <p>A short note on two related rules of thumb.</p>
        <section id="Push-Ifs-Up">
          
          <p>
            If there’s an <code>if</code> condition inside a function, consider
            if it could be moved to the caller instead:
          </p>

          <figure>
            <pre><code><span><span>// GOOD</span></span>
<span><span>fn</span> <span>frobnicate</span>(walrus: Walrus) {</span>
<span>    ...</span>
<span>}</span>
<span></span>
<span><span>// BAD</span></span>
<span><span>fn</span> <span>frobnicate</span>(walrus: <span>Option</span>&lt;Walrus&gt;) {</span>
<span>  <span>let</span> <span>walrus</span> = <span>match</span> walrus {</span>
<span>    <span>Some</span>(it) =&gt; it,</span>
<span>    <span>None</span> =&gt; <span>return</span>,</span>
<span>  };</span>
<span>  ...</span>
<span>}</span></code></pre>
          </figure>
          <p>
            As in the example above, this often comes up with preconditions: a
            function might check precondition inside and “do nothing” if it
            doesn’t hold, or it could push the task of precondition checking to
            its caller, and enforce via types (or an assert) that the
            precondition holds. With preconditions especially, “pushing up” can
            become viral, and result in fewer checks overall, which is one
            motivation for this rule of thumb.
          </p>
          <p>
            Another motivation is that control flow and <code>if</code>s are
            complicated, and are a source of bugs. By pushing <code>if</code>s
            up, you often end up centralizing control flow in a single function,
            which has a complex branching logic, but all the actual work is
            delegated to straight line subroutines.
          </p>
          <p>
            <em>If</em> you have complex control flow, better to fit it on a
            screen in a single function, rather than spread throughout the file.
            What’s more, with all the flow in one place it often is possible to
            notice redundancies and dead conditions. Compare:
          </p>

          <figure>
            <pre><code><span><span>fn</span> <span>f</span>() {</span>
<span>  <span>if</span> foo &amp;&amp; bar {</span>
<span>    <span>if</span> foo {</span>
<span></span>
<span>    } <span>else</span> {</span>
<span></span>
<span>    }</span>
<span>  }</span>
<span>}</span>
<span></span>
<span><span>fn</span> <span>g</span>() {</span>
<span>  <span>if</span> foo &amp;&amp; bar {</span>
<span>    <span>h</span>()</span>
<span>  }</span>
<span>}</span>
<span></span>
<span><span>fn</span> <span>h</span>() {</span>
<span>  <span>if</span> foo {</span>
<span></span>
<span>  } <span>else</span> {</span>
<span></span>
<span>  }</span>
<span>}</span></code></pre>
          </figure>
          <p>
            For <code>f</code>, it’s much easier to notice a dead branch than
            for a combination of <code>g</code> and <code>h</code>!
          </p>
          <p>
            A related pattern here is what I call “dissolving enum” refactor.
            Sometimes, the code ends up looking like this:
          </p>

          <figure>
            <pre><code><span><span>enum</span> <span>E</span> {</span>
<span>  <span>Foo</span>(<span>i32</span>),</span>
<span>  <span>Bar</span>(<span>String</span>),</span>
<span>}</span>
<span></span>
<span><span>fn</span> <span>main</span>() {</span>
<span>  <span>let</span> <span>e</span> = <span>f</span>();</span>
<span>  <span>g</span>(e)</span>
<span>}</span>
<span></span>
<span><span>fn</span> <span>f</span>() <span>-&gt;</span> E {</span>
<span>  <span>if</span> condition {</span>
<span>    E::<span>Foo</span>(x)</span>
<span>  } <span>else</span> {</span>
<span>    E::<span>Bar</span>(y)</span>
<span>  }</span>
<span>}</span>
<span></span>
<span><span>fn</span> <span>g</span>(e: E) {</span>
<span>  <span>match</span> e {</span>
<span>    E::<span>Foo</span>(x) =&gt; <span>foo</span>(x),</span>
<span>    E::<span>Bar</span>(y) =&gt; <span>bar</span>(y)</span>
<span>  }</span>
<span>}</span></code></pre>
          </figure>
          <p>
            There are two branching instructions here and, by pulling them up,
            it becomes apparent that it is the exact same condition, triplicated
            (the third time reified as a data structure):
          </p>

          <figure>
            <pre><code><span><span>fn</span> <span>main</span>() {</span>
<span>  <span>if</span> condition {</span>
<span>    <span>foo</span>(x)</span>
<span>  } <span>else</span> {</span>
<span>    <span>bar</span>(y)</span>
<span>  }</span>
<span>}</span></code></pre>
          </figure>
        </section>
        <section id="Push-Fors-Down">
          <h2>
            <a href="#Push-Fors-Down">Push Fors Down </a>
          </h2>
          <p>
            This comes from data oriented school of thought. Few things are few,
            many things are many. Programs usually operate with bunches of
            objects. Or at least the hot path usually involves handling many
            entities. It is the volume of entities that makes the path hot in
            the first place. So it often is prudent to introduce a concept of a
            “batch” of objects, and make operations on batches the base case,
            with a scalar version being a special case of a batched ones:
          </p>

          <figure>
            <pre><code><span><span>// GOOD</span></span>
<span><span>frobnicate_batch</span>(walruses)</span>
<span></span>
<span><span>// BAD</span></span>
<span><span>for</span> <span>walrus</span> <span>in</span> walruses {</span>
<span>  <span>frobnicate</span>(walrus)</span>
<span>}</span></code></pre>
          </figure>
          <p>
            The primary benefit here is performance. Plenty of performance, <a href="http://venge.net/graydon/talks/VectorizedInterpretersTalk-2023-05-12.pdf">in extreme cases</a>.
          </p>
          <p>
            If you have a whole batch of things to work with, you can amortize
            startup cost and be flexible about the order you process things. In
            fact, you don’t even need to process entities in any particular
            order, you can do vectorized/struct-of-array tricks to process one
            field of all entities first, before continuing with other fields.
          </p>
          <p>
            Perhaps the most fun example here is <a href="https://en.wikipedia.org/wiki/Sch%C3%B6nhage%E2%80%93Strassen_algorithm">FFT-based polynomial multiplication</a>: turns out, evaluating a
            polynomial at a bunch of points simultaneously could be done faster
            than a bunch of individual point evaluations!
          </p>
          <p>
            The two pieces of advice about <code>for</code>s and <code>if</code>s even compose!
          </p>

          <figure>
            <pre><code><span><span>// GOOD</span></span>
<span><span>if</span> condition {</span>
<span>  <span>for</span> <span>walrus</span> <span>in</span> walruses {</span>
<span>    walrus.<span>frobnicate</span>()</span>
<span>  }</span>
<span>} <span>else</span> {</span>
<span>  <span>for</span> <span>walrus</span> <span>in</span> walruses {</span>
<span>    walrus.<span>transmogrify</span>()</span>
<span>  }</span>
<span>}</span>
<span></span>
<span><span>// BAD</span></span>
<span><span>for</span> <span>walrus</span> <span>in</span> walruses {</span>
<span>  <span>if</span> condition {</span>
<span>    walrus.<span>frobnicate</span>()</span>
<span>  } <span>else</span> {</span>
<span>    walrus.<span>transmogrify</span>()</span>
<span>  }</span>
<span>}</span></code></pre>
          </figure>
          <p>
            The <code>GOOD</code> version is good, because it avoids repeatedly
            re-evaluating <code>condition</code>, removes a branch from the hot
            loop, and potentially unlocks vectorization. This pattern works on a
            micro level and on a macro level — the good version is the
            architecture of TigerBeetle, where in the data plane we operate on
            batches of objects at the same time, to amortize the cost of
            decision making in the control plane.
          </p>
          <p>
            While performance is perhaps the primary motivation for the <code>for</code> advice, sometimes it helps with expressiveness as well.
            <code>jQuery</code> was quite successful back in the day, and it
            operates on collections of elements. The language of abstract vector
            spaces is often a better tool for thought than bunches of
            coordinate-wise equations.
          </p>
          <p>
            To sum up, push the <code>if</code>s up and the <code>for</code>s
            down!
          </p>
        </section>
      </article>
    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Catalog of Novel Operating Systems (125 pts)]]></title>
            <link>https://github.com/prathyvsh/os-catalog</link>
            <guid>44012615</guid>
            <pubDate>Sat, 17 May 2025 07:19:55 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/prathyvsh/os-catalog">https://github.com/prathyvsh/os-catalog</a>, See on <a href="https://news.ycombinator.com/item?id=44012615">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><p dir="auto"><h2 tabindex="-1" dir="auto">Novel Operating Systems Catalog</h2><a id="user-content-novel-operating-systems-catalog" aria-label="Permalink: Novel Operating Systems Catalog" href="#novel-operating-systems-catalog"></a></p>
<p dir="auto">Catalogue of novel operating systems</p>
<p dir="auto">Somewhere after the fall in popularity of note-taking apps, perhaps recognizing that just note-taking is not enough and the deafening hype of LLMs, there was a sweet period of lull when a lot of people started boldly building new operating systems. This is a catalogue of such operating systems that I have come across. In the past, before the commercialization of computers, we had a plethora of operating systems with unique languages to interact with computers, like AmigaOS, Symbolics, SunOS, MULTICS, Burroughs, Meneut, BeOS PARC, Star, Oberon, Plan9, NeXTSTEP, OS/2, PL/8, Inferno, QNX, RISCOS etc. This spirit can only be glimpsed in pockets, and kudos to all those who keep the fire alive!</p>
<p dir="auto">A thread on it <a href="https://x.com/Prabros/status/1922915943631523899" rel="nofollow">here</a>.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto"><a href="https://100r.co/site/uxn.html" rel="nofollow">UXN</a></h2><a id="user-content-uxn" aria-label="Permalink: UXN" href="#uxn"></a></p>
<p dir="auto">Perhaps the best one to start off this catalog is the UXN/Varvara personal computing stack of <a href="https://100r.co/" rel="nofollow">100 Rabbits</a>. Such a great couple with such a radical vision!</p>
<p dir="auto"><a href="https://github.com/prathyvsh/os-catalog/blob/main/UXN%20logo"><img src="https://github.com/prathyvsh/os-catalog/raw/main/img/uxn-logo.jpg" alt="./img/uxn-logo.jpg"></a></p>
<p dir="auto"><a href="https://github.com/prathyvsh/os-catalog/blob/main/UXN%20screenshot"><img src="https://github.com/prathyvsh/os-catalog/raw/main/img/uxn-screenshot.jpg" alt="./img/uxn-screenshot.jpg"></a></p>
<p dir="auto">They have documented their rationale in these two documents:</p>
<ul dir="auto">
  <li><a href="https://100r.co/site/tools_ecosystem.html" rel="nofollow">Tools Ecosystem</a></li>
  <li><a href="https://100r.co/site/weathering_software_winter.html" rel="nofollow">Weathering Software Winter</a></li>
</ul>
<p dir="auto">Documents related to UXN can be obtained here: <a href="https://github.com/hundredrabbits/awesome-uxn?tab=readme-ov-file">https://github.com/hundredrabbits/awesome-uxn?tab=readme-ov-file</a></p>
<p dir="auto"><h2 tabindex="-1" dir="auto"><a href="https://playb.it/" rel="nofollow">Playbit</a></h2><a id="user-content-playbit" aria-label="Permalink: Playbit" href="#playbit"></a></p>
<p dir="auto">Daring effort from <a href="https://github.com/rsms">Rasmus Andersson</a> and team to reinvent the computer stack.</p>
<p dir="auto">And alpha version available <a href="https://playb.it/alpha/" rel="nofollow">here</a>.</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/prathyvsh/os-catalog/blob/main/img/playbit-screenshot.webp"><img src="https://github.com/prathyvsh/os-catalog/raw/main/img/playbit-screenshot.webp" alt="Screenshot of Playbit"></a></p>

<p dir="auto"><h2 tabindex="-1" dir="auto"><a href="https://folk.computer/" rel="nofollow">Folk.computer</a></h2><a id="user-content-folkcomputer" aria-label="Permalink: Folk.computer" href="#folkcomputer"></a></p>
<p dir="auto">Omar Rizwan and Andreas Cuérvo</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/prathyvsh/os-catalog/blob/main/img/folk-computer.gif"><img src="https://github.com/prathyvsh/os-catalog/raw/main/img/folk-computer.gif" alt="Video of with Folk.computer" data-animated-image=""></a></p>
<p dir="auto"><h2 tabindex="-1" dir="auto"><a href="https://nette.io/" rel="nofollow">Nette.io</a></h2><a id="user-content-netteio" aria-label="Permalink: Nette.io" href="#netteio"></a></p>
<p dir="auto">Nette.io by <a href="https://github.com/qazwsxpawel">Pawel Ceranka</a> positions itself as a research OS for the web.</p>
<p dir="auto"><a href="https://github.com/prathyvsh/os-catalog/blob/main/Nette%20website%20screenshot"><img src="https://github.com/prathyvsh/os-catalog/raw/main/img/nette.png" alt="./img/nette.png"></a></p>
<p dir="auto"><h2 tabindex="-1" dir="auto"><a href="https://github.com/mntmn/interim">Interim</a></h2><a id="user-content-interim" aria-label="Permalink: Interim" href="#interim"></a></p>
<p dir="auto"><a href="https://github.com/prathyvsh/os-catalog/blob/main/Interim%20Logo"><img src="https://github.com/prathyvsh/os-catalog/raw/main/img/interim-logo.png" alt="./img/interim-logo.png"></a></p>
<p dir="auto">Something about Lisp draws people into construct OSes from ground up. Perhaps it is the simplicity of the language that acts as the foundation. Here‘s Interim, one of our favourite minimal OSes constructed with Lisp.</p>
<p dir="auto"><a href="https://github.com/prathyvsh/os-catalog/blob/main/Interim%20Screenshot"><img src="https://github.com/prathyvsh/os-catalog/raw/main/img/interim-screenshot.jpg" alt="./img/interim-screenshot.jpg"></a></p>
<p dir="auto"><h2 tabindex="-1" dir="auto"><a href="https://github.com/froggey/Mezzano">Mezzano</a></h2><a id="user-content-mezzano" aria-label="Permalink: Mezzano" href="#mezzano"></a></p>
<p dir="auto"><a href="https://github.com/prathyvsh/os-catalog/blob/main/Mezzano%20Screenshot"><img src="https://github.com/prathyvsh/os-catalog/raw/main/img/mezzano.png" alt="./img/mezzano.png"></a></p>
<p dir="auto">An OS written in CommonLisp</p>
<p dir="auto"><h2 tabindex="-1" dir="auto"><a href="https://github.com/vygr/ChrysaLisp">ChrysalLisp</a></h2><a id="user-content-chrysallisp" aria-label="Permalink: ChrysalLisp" href="#chrysallisp"></a></p>
<p dir="auto"><a href="https://github.com/prathyvsh/os-catalog/blob/main/ChrysaLisp%20screenshot"><img src="https://github.com/prathyvsh/os-catalog/raw/main/img/chrysalisp-screenshot.png" alt="./img/chrysalisp-screenshot.png"></a></p>
<p dir="auto">ChrysaLisp is amulti-threaded, multi-core, multi-user parallel OS with features such as a GUI, terminal, OO Assembler, class libraries, C-Script compiler, Lisp interpreter, debugger, profiler, vector font engine, and more.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto"><a href="https://ravynos.com/" rel="nofollow">RayvnOS</a></h2><a id="user-content-rayvnos" aria-label="Permalink: RayvnOS" href="#rayvnos"></a></p>
<p dir="auto"><h2 tabindex="-1" dir="auto"><a href="https://www.redox-os.org/" rel="nofollow">RedoxOS</a></h2><a id="user-content-redoxos" aria-label="Permalink: RedoxOS" href="#redoxos"></a></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Ideas</h2><a id="user-content-ideas" aria-label="Permalink: Ideas" href="#ideas"></a></p>
<p dir="auto"><h2 tabindex="-1" dir="auto"><a href="https://desktopneo.com/" rel="nofollow">DesktopNeo</a></h2><a id="user-content-desktopneo" aria-label="Permalink: DesktopNeo" href="#desktopneo"></a></p>
<p dir="auto">DesktopNeo, a rethinking of the desktop interface by <a href="https://www.lennartziburski.com/" rel="nofollow">Lennart Ziburski</a></p>
<p dir="auto"><a href="https://github.com/prathyvsh/os-catalog/blob/main/Screenshot%20of%20Desktop%20Neo"><img src="https://github.com/prathyvsh/os-catalog/raw/main/img/desktop-neo.jpg" alt="./img/desktop-neo.jpg"></a></p>
<p dir="auto"><a href="https://github.com/prathyvsh/os-catalog/blob/main/Another%20screenshot%20of%20Desktop%20Neo"><img src="https://github.com/prathyvsh/os-catalog/raw/main/img/desktop-neo-screenshot.png" alt="./img/desktop-neo-screenshot.png"></a></p>
<p dir="auto"><h2 tabindex="-1" dir="auto"><a href="https://www.mercuryos.com/" rel="nofollow">MercuryOS</a></h2><a id="user-content-mercuryos" aria-label="Permalink: MercuryOS" href="#mercuryos"></a></p>
<p dir="auto">MercuryOS by Jason Yuan is an interesting rethink of the OS based on intensions:</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/prathyvsh/os-catalog/blob/main/img/mercury-screenshot.png"><img src="https://github.com/prathyvsh/os-catalog/raw/main/img/mercury-screenshot.png" alt="./img/mercury-screenshot.png"></a>
  <a target="_blank" rel="noopener noreferrer" href="https://github.com/prathyvsh/os-catalog/blob/main/img/mercury-dark-mode.png"><img src="https://github.com/prathyvsh/os-catalog/raw/main/img/mercury-dark-mode.png" alt="./img/mercury-dark-mode.png"></a></p>
<p dir="auto">Prototype by Rauno Freiberg. <a href="https://github.com/prathyvsh/os-catalog/blob/main/Source">https://x.com/raunofreiberg/status/1666122499401166873</a></p>

<p dir="auto">The team seems to be working on MercuryOS → Makespace.fun → <a href="https://new.computer/" rel="nofollow">New.computer</a></p>
<p dir="auto"><h2 tabindex="-1" dir="auto"><a href="https://x.com/getFreezeframe" rel="nofollow">Freeze.app</a></h2><a id="user-content-freezeapp" aria-label="Permalink: Freeze.app" href="#freezeapp"></a></p>
<p dir="auto">Freeze the desktop interface and then thaw it at will: <a href="https://x.com/getFreezeframe/status/1358805285393948673" rel="nofollow">https://x.com/getFreezeframe/status/1358805285393948673</a></p>
<p dir="auto"><h2 tabindex="-1" dir="auto"><a href="https://azlen.me/stories/worm-os/" rel="nofollow">WormOS</a></h2><a id="user-content-wormos" aria-label="Permalink: WormOS" href="#wormos"></a></p>
<p dir="auto">Interesting article on partitioned rooms by mental space with little bubbles on the edges that act as wormholes into things you want to achieve.</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/prathyvsh/os-catalog/blob/main/img/wormos.png"><img src="https://github.com/prathyvsh/os-catalog/raw/main/img/wormos.png" alt="./img/wormos.png"></a></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Status Unknown</h2><a id="user-content-status-unknown" aria-label="Permalink: Status Unknown" href="#status-unknown"></a></p>
<p dir="auto"><h2 tabindex="-1" dir="auto"><a href="https://bedrock.computer/" rel="nofollow">Bedrock.computer</a></h2><a id="user-content-bedrockcomputer" aria-label="Permalink: Bedrock.computer" href="#bedrockcomputer"></a></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Other lists</h2><a id="user-content-other-lists" aria-label="Permalink: Other lists" href="#other-lists"></a></p>
<ul dir="auto">
  <li><a href="https://github.com/jubalh/awesome-os">AwesomeOS by @jubalh</a></li>
  <li><a href="https://1.anagora.org/node/os" rel="nofollow">Anagora List</a></li>
</ul>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[JavaScript's New Superpower: Explicit Resource Management (248 pts)]]></title>
            <link>https://v8.dev/features/explicit-resource-management</link>
            <guid>44012227</guid>
            <pubDate>Sat, 17 May 2025 05:23:20 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://v8.dev/features/explicit-resource-management">https://v8.dev/features/explicit-resource-management</a>, See on <a href="https://news.ycombinator.com/item?id=44012227">Hacker News</a></p>
<div id="readability-page-1" class="page"><div itemprop="articleBody"><p>The <em>Explicit Resource Management</em> proposal introduces a deterministic approach to explicitly manage the lifecycle of resources like file handles, network connections, and more. This proposal brings the following additions to the language: the <code>using</code> and <code>await using</code> declarations, which automatically calls dispose method when a resource goes out of scope; <code>[Symbol.dispose]()</code> and <code>[Symbol.asyncDispose]()</code> symbols for cleanup operations; two new global objects <code>DisposableStack</code> and <code>AsyncDisposableStack</code> as containers to aggregate disposable resources; and <code>SuppressedError</code> as a new type of error (contain both the error that was most recently thrown, as well as the error that was suppressed) to address the scenario where an error occurs during the disposal of a resource, and potientially masking an existing error thrown from the body, or from the disposal of another resource. These additions enable developers to write more robust, performant, and maintainable code by providing fine-grained control over resource disposal.</p><h2 id="using-and-await-using-declarations" tabindex="-1"><code>using</code> and <code>await using</code> declarations <a href="#using-and-await-using-declarations">#</a></h2><p>The core of the Explicit Resource Management proposal lies in the <code>using</code> and <code>await using</code> declarations. The <code>using</code> declaration is designed for synchronous resources, ensuring that the <code>[Symbol.dispose]()</code> method of a disposable resource is called when the scope in which it's declared exits. For asynchronous resources, the <code>await using</code> declaration works similarly, but ensures that the <code>[Symbol.asyncDispose]()</code> method is called and the result of this calling is awaited, allowing for asynchronous cleanup operations. This distinction enables developers to reliably manage both synchronous and asynchronous resources, preventing leaks and improving overall code quality. The <code>using</code> and <code>await using</code> keywords can be used inside braces <code>{}</code> (such as blocks, for loops and function bodies), and cannot be used in top-levels.</p><p>For example, when working with <a href="https://developer.mozilla.org/en-US/docs/Web/API/ReadableStreamDefaultReader"><code>ReadableStreamDefaultReader</code></a>, it's crucial to call <code>reader.releaseLock()</code> to unlock the stream and allow it to be used elsewhere. However, error handling introduces a common problem: if an error occurs during the reading process, and you forget to call <code>releaseLock()</code> before the error propagates, the stream remains locked. Let's start with a naive example:</p><pre><code><span>let</span> responsePromise <span>=</span> <span>null</span><span>;</span><p><span>async</span> <span>function</span> <span>readFile</span><span>(</span><span>url</span><span>)</span> <span>{</span>  <br>    <span>if</span> <span>(</span><span>!</span>responsePromise<span>)</span> <span>{</span><br>        <span>// Only fetch if we don't have a promise yet</span><br>        responsePromise <span>=</span> <span>fetch</span><span>(</span>url<span>)</span><span>;</span><br>    <span>}</span><br>    <span>const</span> response <span>=</span> <span>await</span> responsePromise<span>;</span><br>    <span>if</span> <span>(</span><span>!</span>response<span>.</span>ok<span>)</span> <span>{</span><br>      <span>throw</span> <span>new</span> <span>Error</span><span>(</span><span><span>`</span><span>HTTP error! status: </span><span><span>${</span>response<span>.</span>status<span>}</span></span><span>`</span></span><span>)</span><span>;</span><br>    <span>}</span><br>    <span>const</span> processedData <span>=</span> <span>await</span> <span>processData</span><span>(</span>response<span>)</span><span>;</span></p><p>    <span>// Do something with processedData</span><br>    <span>...</span><br> <span>}</span></p><p><span>async</span> <span>function</span> <span>processData</span><span>(</span><span>response</span><span>)</span> <span>{</span><br>    <span>const</span> reader <span>=</span> response<span>.</span>body<span>.</span><span>getReader</span><span>(</span><span>)</span><span>;</span><br>    <span>let</span> done <span>=</span> <span>false</span><span>;</span><br>    <span>let</span> value<span>;</span><br>    <span>let</span> processedData<span>;</span></p><p>        <span>while</span> <span>(</span><span>!</span>done<span>)</span> <span>{</span><br>        <span>(</span><span>{</span> done<span>,</span> value <span>}</span> <span>=</span> <span>await</span> reader<span>.</span><span>read</span><span>(</span><span>)</span><span>)</span><span>;</span><br>        <span>if</span> <span>(</span>value<span>)</span> <span>{</span><br>            <span>// Process data and save the result in processedData</span><br>            <span>...</span><br>            <span>// An error is thrown here!</span><br>        <span>}</span><br>    <span>}</span></p><p>        <span>// Because the error is thrown before this line, the stream remains locked.</span><br>    reader<span>.</span><span>releaseLock</span><span>(</span><span>)</span><span>;</span></p><p>    <span>return</span> processedData<span>;</span><br>  <span>}</span></p><p>  <span>readFile</span><span>(</span><span>'https://example.com/largefile.dat'</span><span>)</span><span>;</span></p></code></pre><p>So it is crucial for developers to have <code>try...finally</code> block while using streams and put <code>reader.releaseLock()</code> in <code>finally</code>. This pattern ensures that <code>reader.releaseLock()</code> is always called.</p><pre><code><span>async</span> <span>function</span> <span>processData</span><span>(</span><span>response</span><span>)</span> <span>{</span><br>    <span>const</span> reader <span>=</span> response<span>.</span>body<span>.</span><span>getReader</span><span>(</span><span>)</span><span>;</span><br>    <span>let</span> done <span>=</span> <span>false</span><span>;</span><br>    <span>let</span> value<span>;</span><br>    <span>let</span> processedData<span>;</span><p>        <span>try</span> <span>{</span><br>        <span>while</span> <span>(</span><span>!</span>done<span>)</span> <span>{</span><br>            <span>(</span><span>{</span> done<span>,</span> value <span>}</span> <span>=</span> <span>await</span> reader<span>.</span><span>read</span><span>(</span><span>)</span><span>)</span><span>;</span><br>            <span>if</span> <span>(</span>value<span>)</span> <span>{</span><br>                <span>// Process data and save the result in processedData</span><br>                <span>...</span><br>                <span>// An error is thrown here!</span><br>            <span>}</span><br>        <span>}</span><br>    <span>}</span> <span>finally</span> <span>{</span><br>        <span>// The reader's lock on the stream will be always released.</span><br>        reader<span>.</span><span>releaseLock</span><span>(</span><span>)</span><span>;</span><br>    <span>}</span></p><p>    <span>return</span> processedData<span>;</span><br>  <span>}</span></p><p>  <span>readFile</span><span>(</span><span>'https://example.com/largefile.dat'</span><span>)</span><span>;</span></p></code></pre><p>An alternative to write this code is to create a disposable object <code>readerResource</code>, which has the reader (<code>response.body.getReader()</code>) and the <code>[Symbol.dispose]()</code> method that calls <code>this.reader.releaseLock()</code>. The <code>using</code> declaration ensures that <code>readerResource[Symbol.dispose]()</code> is called when the code block exits, and remembering to call <code>releaseLock</code> is no longer needed because the using declaration handles it. Integration of <code>[Symbol.dispose]</code> and <code>[Symbol.asyncDispose]</code> in web APIs like streams may happen in the future, so developers do not have to write the manual wrapper object.</p><pre><code> <span>async</span> <span>function</span> <span>processData</span><span>(</span><span>response</span><span>)</span> <span>{</span><br>    <span>const</span> reader <span>=</span> response<span>.</span>body<span>.</span><span>getReader</span><span>(</span><span>)</span><span>;</span><br>    <span>let</span> done <span>=</span> <span>false</span><span>;</span><br>    <span>let</span> value<span>;</span><p>    <span>// Wrap the reader in a disposable resource</span><br>    using readerResource <span>=</span> <span>{</span><br>        <span>reader</span><span>:</span> response<span>.</span>body<span>.</span><span>getReader</span><span>(</span><span>)</span><span>,</span><br>        <span>[</span>Symbol<span>.</span>dispose<span>]</span><span>(</span><span>)</span> <span>{</span><br>            <span>this</span><span>.</span>reader<span>.</span><span>releaseLock</span><span>(</span><span>)</span><span>;</span><br>        <span>}</span><span>,</span><br>    <span>}</span><span>;</span><br>    <span>const</span> <span>{</span> reader <span>}</span> <span>=</span> readerResource<span>;</span></p><p>    <span>let</span> done <span>=</span> <span>false</span><span>;</span><br>    <span>let</span> value<span>;</span><br>    <span>let</span> processedData<span>;</span><br>    <span>while</span> <span>(</span><span>!</span>done<span>)</span> <span>{</span><br>        <span>(</span><span>{</span> done<span>,</span> value <span>}</span> <span>=</span> <span>await</span> reader<span>.</span><span>read</span><span>(</span><span>)</span><span>)</span><span>;</span><br>        <span>if</span> <span>(</span>value<span>)</span> <span>{</span><br>            <span>// Process data and save the result in processedData</span><br>            <span>...</span><br>            <span>// An error is thrown here!</span><br>        <span>}</span><br>    <span>}</span><br>    <span>return</span> processedData<span>;</span><br>  <span>}</span><br> <span>// readerResource[Symbol.dispose]() is called automatically.</span></p><p> <span>readFile</span><span>(</span><span>'https://example.com/largefile.dat'</span><span>)</span><span>;</span></p></code></pre><h2 id="disposablestack-and-asyncdisposablestack" tabindex="-1"><code>DisposableStack</code> and <code>AsyncDisposableStack</code> <a href="#disposablestack-and-asyncdisposablestack">#</a></h2><p>To further facilitate managing multiple disposable resources, the proposal introduces <code>DisposableStack</code> and <code>AsyncDisposableStack</code>. These stack-based structures allow developers to group and dispose of multiple resources in a coordinated manner. Resources are added to the stack, and when the stack is disposed, either synchronously or asynchronously, the resources are disposed of in the reverse order they were added, ensuring that any dependencies between them are handled correctly. This simplifies the cleanup process when dealing with complex scenarios involving multiple related resources. Both structures provide methods like <code>use()</code>, <code>adopt()</code>, and <code>defer()</code> to add resources or disposal actions, and a <code>dispose()</code> or <code>asyncDispose()</code> method to trigger the cleanup. <code>DisposableStack</code> and <code>AsyncDisposableStack</code> have <code>[Symbol.dispose]()</code> and <code>[Symbol.asyncDispose]()</code>, respectively, so they can be used with <code>using</code> and <code>await using</code> keywords. They offer a robust way to manage the disposal of multiple resources within a defined scope.</p><p>Let’s take a look at each method and see an example of it:</p><p><code>use(value)</code> adds a resource to the top of the stack.</p><pre><code><span>{</span><br>    <span>const</span> readerResource <span>=</span> <span>{</span><br>        <span>reader</span><span>:</span> response<span>.</span>body<span>.</span><span>getReader</span><span>(</span><span>)</span><span>,</span><br>        <span>[</span>Symbol<span>.</span>dispose<span>]</span><span>(</span><span>)</span> <span>{</span><br>            <span>this</span><span>.</span>reader<span>.</span><span>releaseLock</span><span>(</span><span>)</span><span>;</span><br>            console<span>.</span><span>log</span><span>(</span><span>'Reader lock released.'</span><span>)</span><span>;</span><br>        <span>}</span><span>,</span><br>    <span>}</span><span>;</span><br>    using stack <span>=</span> <span>new</span> <span>DisposableStack</span><span>(</span><span>)</span><span>;</span><br>    stack<span>.</span><span>use</span><span>(</span>readerResource<span>)</span><span>;</span><br><span>}</span><br><span>// Reader lock released.</span></code></pre><p><code>adopt(value, onDispose)</code> adds a non-disposable resource and a disposal callback to the top of the stack.</p><pre><code><span>{</span><br>    using stack <span>=</span> <span>new</span> <span>DisposableStack</span><span>(</span><span>)</span><span>;</span><br>    stack<span>.</span><span>adopt</span><span>(</span><br>      response<span>.</span>body<span>.</span><span>getReader</span><span>(</span><span>)</span><span>,</span> reader <span>=</span> <span>&gt;</span> <span>{</span><br>        reader<span>.</span><span>releaseLock</span><span>(</span><span>)</span><span>;</span><br>        console<span>.</span><span>log</span><span>(</span><span>'Reader lock released.'</span><span>)</span><span>;</span><br>      <span>}</span><span>)</span><span>;</span><br><span>}</span><br><span>// Reader lock released.</span></code></pre><p><code>defer(onDispose)</code> adds a disposal callback to the top of the stack. It's useful for adding cleanup actions that don't have an associated resource.</p><pre><code><span>{</span><br>    using stack <span>=</span> <span>new</span> <span>DisposableStack</span><span>(</span><span>)</span><span>;</span><br>    stack<span>.</span><span>defer</span><span>(</span><span>(</span><span>)</span> <span>=&gt;</span> console<span>.</span><span>log</span><span>(</span><span>"done."</span><span>)</span><span>)</span><span>;</span><br><span>}</span><br><span>// done.</span></code></pre><p><code>move()</code> moves all resources currently in this stack into a new <code>DisposableStack</code>. This can be useful if you need to transfer ownership of resources to another part of your code.</p><pre><code><span>{</span><br>    using stack <span>=</span> <span>new</span> <span>DisposableStack</span><span>(</span><span>)</span><span>;</span><br>    stack<span>.</span><span>adopt</span><span>(</span><br>      response<span>.</span>body<span>.</span><span>getReader</span><span>(</span><span>)</span><span>,</span> reader <span>=</span> <span>&gt;</span> <span>{</span><br>        reader<span>.</span><span>releaseLock</span><span>(</span><span>)</span><span>;</span><br>        console<span>.</span><span>log</span><span>(</span><span>'Reader lock released.'</span><span>)</span><span>;</span><br>      <span>}</span><span>)</span><span>;</span><br>    using newStack <span>=</span> stack<span>.</span><span>move</span><span>(</span><span>)</span><span>;</span><br><span>}</span><br><span>// Here just the newStack exists and the resource inside it will be disposed.</span><br><span>// Reader lock released.</span></code></pre><p><code>dispose()</code> in DisposableStack and <code>asyncDispose()</code> in AsyncDisposableStack dispose the resources within this object.</p><pre><code><span>{</span><br>    <span>const</span> readerResource <span>=</span> <span>{</span><br>        <span>reader</span><span>:</span> response<span>.</span>body<span>.</span><span>getReader</span><span>(</span><span>)</span><span>,</span><br>        <span>[</span>Symbol<span>.</span>dispose<span>]</span><span>(</span><span>)</span> <span>{</span><br>            <span>this</span><span>.</span>reader<span>.</span><span>releaseLock</span><span>(</span><span>)</span><span>;</span><br>            console<span>.</span><span>log</span><span>(</span><span>'Reader lock released.'</span><span>)</span><span>;</span><br>        <span>}</span><span>,</span><br>    <span>}</span><span>;</span><br>    <span>let</span> stack <span>=</span> <span>new</span> <span>DisposableStack</span><span>(</span><span>)</span><span>;</span><br>    stack<span>.</span><span>use</span><span>(</span>readerResource<span>)</span><span>;</span><br>    stack<span>.</span><span>dispose</span><span>(</span><span>)</span><span>;</span><br><span>}</span><br><span>// Reader lock released.</span></code></pre><h2 id="availability" tabindex="-1">Availability <a href="#availability">#</a></h2><p>Explicit Resource Management is shipped in Chromium 134 and V8 v13.8.</p><ul><li><a href="https://chromestatus.com/feature/5071680358842368"><span>Chrome:</span> <span>supported since version <span>134</span></span></a></li><li><a href="https://v8.dev/features/(nightly)"><span>Firefox:</span> <span>supported since version <span>134</span></span></a></li><li><a href="https://bugs.webkit.org/show_bug.cgi?id=248707"><span>Safari:</span> <span>no support</span></a></li><li><span>Node.js:</span> <span>no support</span></li><li><a href="https://github.com/zloirock/core-js#explicit-resource-management"><span>Babel:</span> <span>supported</span></a></li></ul></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[A kernel developer plays with Home Assistant (137 pts)]]></title>
            <link>https://lwn.net/SubscriberLink/1017720/7155ecb9602e9ef2/</link>
            <guid>44011669</guid>
            <pubDate>Sat, 17 May 2025 02:50:15 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://lwn.net/SubscriberLink/1017720/7155ecb9602e9ef2/">https://lwn.net/SubscriberLink/1017720/7155ecb9602e9ef2/</a>, See on <a href="https://news.ycombinator.com/item?id=44011669">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
<p>
Those of us who have spent our lives playing with computers naturally see
the appeal of deploying them though the home for both data acquisition and
automation.  But many of us who have watched the evolution of the
technology industry are increasingly unwilling to entrust critical
household functions to cloud-based servers run by companies that may not
have our best interests at heart.  The Apache-licensed <a href="https://www.home-assistant.io/">Home Assistant</a> project offers a
welcome alternative: locally controlled automation with free software.
This two-part series covers roughly a year of Home Assistant use, starting
with a set of overall observations about the project.
</p><p>
This is not the first time that LWN has looked at this project, of course;
<a href="https://lwn.net/Articles/822350/">this review</a> gives a snapshot of what Home
Assistant looked like five years ago, while <a href="https://lwn.net/Articles/947843/">this 2023 article</a> gives a good overview of the
project's history, governance, and overall direction.  I will endeavor to
not duplicate that material here.
</p><h4>Project health</h4>
<p>
At a first glance, Home Assistant bears some of the hallmarks of a
company-owned project.  The company in question, <a href="https://www.nabucasa.com/">Nabu Casa</a>, was formed around the
project and employs a number of its key developers.  One of the ways in
which the company makes money is with a $65/year subscription service, providing
remote access to Home Assistant servers installed on firewalled residential
networks.  Home Assistant has support for that remote option, and no
others.  It would be interesting to see what would happen to a pull request
adding support for, say, <a href="https://opensprinklershop.de/en/2023/01/22/opensprinkler-fernzugriff-mit-openthings-cloud-otc-token/">OpenThings
Cloud</a> as an alternative.  The fate of that request would say a lot
about how open the project really is.
</p><blockquote>
<b>No slop, all substance: subscribe to LWN today</b>
<p>
LWN has always been about quality over quantity; we need your help
to continue publishing in-depth, reader-focused articles about Linux
and the free-software community. Please subscribe today to support our work
and keep LWN on the air; we are offering <a href="https://lwn.net/Promo/no-slop/claim">a free one-month trial subscription</a> to get you started.
</p></blockquote>
<p>
(For the record, I have bought the Nabu Casa subscription rather than, say,
using WireGuard to make a port available on an accessible system; it is a
hassle-free way to solve the problem and support the development of this
software).
</p><p>
That said, most of the warning signs that accompany a corporate-controlled
project are not present with Home Assistant.  The project's <a href="https://github.com/home-assistant/core/blob/dev/CLA.md">contributor
license agreement</a> is a derivative of the kernel's developer certificate
of origin; contributors retain their copyright on their work.  Since the <a href="https://www.home-assistant.io/blog/2024/04/03/release-20244/">2024.4
release</a>, the Home Assistant core repository has acquired over 17,000
changesets from over 900 contributors.  While a number of Nabu Casa
employees (helpfully listed on <a href="https://www.nabucasa.com/about/">this page</a>) appear in the top ten
contributors, they do not dominate that list.
</p><p>
Home Assistant is clearly an active project with a wide developer base.  In
2024, overall responsibility for this project was transferred to <a href="https://www.openhomefoundation.org/blog/announcing-the-open-home-foundation/">the
newly created Open Home Foundation</a>.  This project is probably here to
stay, and seems unlikely to take a hostile turn in the future.  For a
system that sits at the core of one's home, those are important
characteristics.
</p><h4>Installation and setup</h4>
<p>
Linux users tend to be somewhat spoiled; installing a new application is
typically a matter of a single package-manager command.  Home Assistant
does not really fit into that model.  The first three options on <a href="https://www.home-assistant.io/installation/">the installation
page</a> involve dedicated computers — two of which are sold by Nabu Casa.
For those wanting to install it on a general-purpose computer, the
recommended course is to install the <a href="https://github.com/home-assistant/operating-system">Home Assistant
Operating System</a>, a bespoke Linux distribution that runs Home Assistant
within a Docker container.  There is also a container-based method that can
run on another distribution, but this installation does not support <a href="https://www.home-assistant.io/addons/">the add-ons feature</a>.
</p><p>
Home Assistant, in other words, is not really set up to be just another
application on a Linux system.  If one scrolls far enough, though, one will
find, the instructions to install onto a "normal" Linux system, suitably
guarded with warnings about how it is an "<q>advanced</q>" method.
Of course, that is what I did, putting the software onto an existing system
running Fedora.  The whole thing subsequently broke when a
distribution upgrade replaced Python, but that was easily enough repaired.
As a whole, the installation has worked as expected.
</p><p>
Out of the box, though, a new Home Assistant installation does not do much.
Its job, after all, is to interface with the systems throughout the house,
and every house is different.  While Home Assistant can find some systems
automatically (it found the Brother printer and dutifully informed me that
the device was, inevitably, low on cyan toner), it usually needs to be
told about what is installed in the house.  Thus, the user quickly delves
into the world of "integrations" — the device drivers of Home Assistant.
</p><p>
For each remotely accessible device in the house, there is, hopefully, at
least one integration available that allows Home Assistant to work with it.
Many integrations are packaged with the system itself, and can be found by
way of a simple search screen in the Home Assistant web interface.  A much
larger set is packaged separately, usually in the <a href="https://www.hacs.xyz/">Home Assistant Community Store</a>, or HACS;
it is fair to say that most users will end up getting at least some
integrations from this source.  Setting up HACS requires a few steps and,
unfortunately, requires the user to have a GitHub account for full
integration.  It <i>is</i> possible to install HACS integrations without
that account, but it is a manual process that loses support for features
like update tracking.
</p><p>
Most integrations, at setup time, will discover any of the appropriate
devices on the network — if those devices support that sort of discovery,
of course.  Often, using an integration will require the credentials to log
into the cloud account provided by the vendor of the devices in question.
When possible, integrations mostly strive to operate entirely locally; some
only use the cloud connection for the initial device discovery.  When there
is no alternative, though, integrations will remain logged into the cloud
account and interact with their devices that way; this mode may or may not
be supported (or condoned) by the vendor.  There are, of course, some
vendors that <a href="https://www.bleepingcomputer.com/news/security/haier-hits-home-assistant-plugin-dev-with-takedown-notice/">are
actively hostile</a> to integration with Home Assistant.
</p><p>
As might be expected, the quality of integrations varies widely.  Most of
the integrations I have tried have worked well enough.  The OpenSprinkler
(<a href="https://lwn.net/Articles/940509/">reviewed here</a> in 2023) integration,
instead, thoroughly corrupted the device configuration, exposing me to the
shame of being seen with a less-than-perfect lawn; it was quickly
removed.  It is an especially nice surprise when a device comes with Home
Assistant support provided by the vendor, but that is still a relatively
rare occurrence.  Home Assistant now is in a position similar to Linux
25&nbsp;years ago; many devices are supported, but often in spite of their
vendor, and one has to choose components carefully.
</p><h4>Security</h4>
<p>
Home Assistant sits at the core of the home network; it has access to
sensors that can reveal a lot about the occupants of the home, and it
collects data in a single location.  An installation will be exposed to the
Internet if its owner needs remote access.  There is clearly potential for
a security disaster here.
</p><p>
The project has <a href="https://www.home-assistant.io/security/">a posted
security policy</a> describing the project's stance; it asks for a 90-day
embargo on the reporting of any security issues.  Authors writing about the
project's security are encouraged to run their work past the project "<q>so
we can ensure that all claims are correct</q>".  The security policy
explicitly excludes reports regarding third-party integrations (the core
project cannot fix those, after all).  The project is also uninterested in
any sort of privilege escalation by users who are logged into Home
Assistant, assuming that anybody who has an account is fully trusted.
</p><p>
The project has only issued <a href="https://github.com/home-assistant/core/security/advisories/GHSA-m3pm-rpgg-5wj6">one
security advisory</a> since the beginning of 2024.  There were several in
2023, mostly as the result of <a href="https://github.blog/security/vulnerability-research/securing-our-home-labs-home-assistant-code-review/">a
security audit</a> performed by GitHub.
</p><p>

There is no overall vetting of third-party integrations, which are, in the
end, just more Python code.  So loading an unknown integration is similar
to importing an unknown module from PyPI; it will probably work, but the
potential for trouble is there.  The project has occasionally <a href="https://www.home-assistant.io/blog/2021/01/23/security-disclosure2/">reported
security problems in third-party integrations</a>, but such reports are
rare.  I am unable to find any reports of actively malicious integrations
in the wild, but one seems destined to appear sooner or later.
</p><h4>Actually doing something with Home Assistant</h4>
<p>
The first step for the owner of a new Home Assistant installation is,
naturally, to seek out integrations for the devices installed in the home.
On successful installation and initialization, an integration will add one
or more "devices" to the system, each of which has some number of "sensors"
for data it reports, and possible "controls" to change its operating state.
A heat-pump head, for example, may have sensors for the current temperature
and humidity, and controls for its operating mode, fan speed, vane
direction, and more.
</p><p>
It is worth noting that the setup of these entities seems a bit
non-deterministic at times.  My solar system has 22&nbsp;panels with
inverters, each of which reports nearly a dozen parameters (voltage,
current, frequency, temperature, etc.).  There is no easy way to determine
which panel is reporting, for example, <tt>sensor_amps_12</tt>, especially
since <tt>sensor_frequency_12</tt> almost certainly corresponds to a
<i>different</i> panel.  My experience is that Home Assistant is a system
for people who are willing to spend a lot of time fiddling around with
things to get them to a working state.  Dealing with these sensors was an
early introduction to that; it took some time to figure out the mapping
between names and rooftop positions, then to rename each sensor to
something more helpful.
</p><p>
The next level of fiddling around is setting up dashboards.  Home Assistant
offers a great deal of flexibility in the information and controls it
provides to the user; it is possible to set up screens focused on, say,
energy production or climate control.  Happily, the days when this
configuration had to be done by writing YAML snippets are mostly in the
past at this point; one occasionally still has to dip into YAML, but it
does not happen often.  The interface is not always intuitive,
but it is fairly slick, interactive, and functional.
</p><p>
Another part of Home Assistant that I have not yet played with much
is automations and scenes.  Automations are simple rule-triggered programs
that make changes to some controls.  They can carry out actions like
"turn on the front light when it gets dark" or "play scary music if
somebody rings the doorbell and nobody is home".  Scenes are sets of canned
device configurations.  One might create a scene called "in-laws visiting"
that plays loud punk music, sets the temperature to just above freezing,
disables all voice control, and tunes all of the light bulbs to 6000K, for
example.
</p><p>
The good news is that, unless the fiddling itself is the point (and it can
be a good one), there comes a time when things just work and the fiddling
can stop.  A well-configured Home Assistant instance provides detailed
information about the state of the home — and control where the devices
allow it — to any web browser that can reach it and log in.  There are
(open-source) apps that bring this support to mobile devices in a way that
is nearly indistinguishable from how the web interface works.
</p><p>
All told, it is clear why Home Assistant has a strong and growing
following.  It is an open platform that brings control to an industry that
is doing its best to keep a firm grasp on our homes and the data they
create.  Home Assistant shows that we can do nicely without all of these
fragile, non-interoperable, rug-pull-susceptible cloud systems.  Just like
Linux proved that we can have control over our computers, Home Assistant
shows that we do not have to surrender control over our homes.
</p><p>
This article has gotten long, and is remarkably short on interesting things
that one can actually <i>do</i> with Home Assistant.  There are some
interesting stories to be told along those lines; they will appear shortly
in <a href="https://lwn.net/Articles/1017945/">the second, concluding part</a> of this series.<br clear="all"></p>
               <br clear="all">
               <hr>
            </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Behind Silicon Valley and the GOP’s campaign to ban state AI laws (103 pts)]]></title>
            <link>https://www.bloodinthemachine.com/p/de-democratizing-ai</link>
            <guid>44011654</guid>
            <pubDate>Sat, 17 May 2025 02:46:33 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.bloodinthemachine.com/p/de-democratizing-ai">https://www.bloodinthemachine.com/p/de-democratizing-ai</a>, See on <a href="https://news.ycombinator.com/item?id=44011654">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><article><div dir="auto"><p>Greetings all,</p><p>Today, we dive deep into the GOP’s radical campaign to ban US states from passing any laws that govern AI. Even in a political moment as fraught as ours, this one stands out. We’ll get into:</p><ul><li><p>How a proposal to ban AI lawmaking wound up in the budget reconciliation bill the same week that AI execs took a trip with Trump to Saudi Arabia</p></li><li><p>How the GOP plans to try to sell its state AI ban, according to the GOP</p></li><li><p>A look at what the implications are for AI in general</p></li><li><p>An interview at the end with California Assemblyman Isaac Bryan, author and co-sponsor of some of the AI bills Silicon Valley wants dead</p></li></ul><p>I’m not going to lie, this was a dark week, and a tough one to report through—I meant to publish this on Tuesday, then Wednesday, and the new twists and revelations in how this campaign came together just kept piling up. It took many hours to research, investigate, and write this story. To that end, Blood in the Machine is 100% reader supported, and made possible by paid subscribers. If you find value in this work, and if you can, your support would be immensely appreciated. </p><p><span>On Sunday, May 11th, Republicans added </span><a href="https://www.404media.co/republicans-try-to-cram-ban-on-ai-regulation-into-budget-reconciliation-bill/" rel="">a sweeping amendment</a><span> to the 2025 budget reconciliation bill that would ban all US states from enacting any laws regulating AI for ten years. Reconciliation is a common way for a party to try to push through controversial or unpopular legislation that might not survive a regular Senate vote (budgets can’t be filibustered, and need just a simple majority to pass). Even so, this amendment, put forward by the Kentucky congressman and energy and commerce committee chair Brett Guthrie, managed to shock.</span></p><p><span>The amendment drew admonitory </span><a href="https://www.washingtonpost.com/opinions/2025/05/14/artificial-intelligence-regulation-congress-reconciliation/" rel="">headlines</a><span>, consternation in Democrats, and anger and disbelief on social media. The outcry is well deserved. The bill’s language is not ambiguous. It </span><a href="https://d1dth6e84htgma.cloudfront.net/Subtitle_C_Communications_4e3fbcc3bc.pdf?ref=404media.co" rel="">says</a><span> that “no State or political subdivision thereof may enforce any law or regulation regulating artificial intelligence models, artificial intelligence systems, or automated decision systems during the 10-year period beginning on the date of the enactment of this Act.”</span></p><p><span>Take a minute to absorb what’s being proposed here: No state</span><em> </em><span>may enforce </span><em>any law or regulation</em><span> of AI. A total ban of state lawmaking on what is routinely touted as the most transformative commercial technology of our generation. And because we can safely assume there will be no serious efforts to regulate AI by the GOP-controlled Congress or by a Trump administration intent on </span><a href="https://www.bloodinthemachine.com/p/ai-is-in-its-empire-era" rel="">helping the US AI industry dominate</a><span>, this is, in effect, an effort to ban any lawmaking around AI whatsoever, for the next two to four years, while Republicans have a stranglehold on power.</span></p><p><span>All of this is, needless to say, profoundly undemocratic. Both in approach—the act of sliding a bill with such severe repercussions into the reconciliation process, where it won’t receive a proper public hearing—and intent: to prevent the public from having a vote on how pervasive Silicon Valley technologies are impacting their lives. Worse still, Gutherie’s amendment is the culmination of </span><a href="https://www.politico.com/news/2025/05/12/how-big-tech-is-pitting-washington-against-california-00336484" rel="">a multi-pronged lobbying effort</a><span> from the major AI companies. That effort’s aim, as reported by </span><a href="https://www.politico.com/news/2025/05/12/how-big-tech-is-pitting-washington-against-california-00336484" rel="">Politico</a><span>, was to shut down state laws that might constrain AI firms’ and investors’ ability to profit off of AI products—especially California’s. </span></p><p>AI industry pitchmen are fond of saying that AI is a powerful tool for “democratization.” It has instead become a force for the opposite.</p><p><span>On Tuesday, at about the same time that the proposed language seeking to ban state AI regulation was officially being introduced in Congress, a bevy of tech billionaires including Sam Altman, Elon Musk, Nvidia CEO Jensen Huang and Amazon CEO Andrew Jassy were at lunch with president Trump in Saudi Arabia. There, the tech titans cut billion dollar deals with Gulf State royalty and the Trump Administration. Trump announced a $142 billion defense and AI services sale to Saudi Arabia. DataVolt, a Saudi Arabian company will </span><a href="https://www.ft.com/content/5302d5d2-d375-4327-905c-7b1ad5173105" rel="">spend $20 billion on data centers in the US</a><span>. Amazon is </span><a href="https://techcrunch.com/2025/05/13/aws-enters-into-strategic-partnership-with-saudi-arabia-backed-humain/" rel="">investing $5 billion in Humain</a><span>, Mohammed bin Salman’s AI startup. Nvidia is selling billions of dollars worth of chips to Humain. Meanwhile, OpenAI is mulling a StarGate project in the United Arab Emirates; MGX, the Emirati investment firm, is already a backer of its fledgling Texas data megacenter. </span></p><p>And on and on it goes. I hope this fact escapes no one: While the executives of AI firms are abroad in Saudi Arabia, cutting billion dollar deals to expand their operations with nations boasting some of the worst human rights records in the world, their lobbyists and partners back home are trying to make it impossible to pass any laws governing their AI products at all. </p><p>With states’ rights to legislate AI under assault, I reached out to lawmakers to see how the move in DC was reverberating back home. </p><p>“The tech industry was incubated, cultivated, and continues to grow and innovate here in California,” says Isaac Bryan, a California assemblyman who has authored a state bill that limits the ways AI can be used for surveillance in the workplace—one of the bills that the GOP amendment would ban. (Bryan also happens to represent my district in the CA assembly.) “California deserves the right, and has the expertise, to lead. We’ve been establishing meaningful guardrails and regulations around these advancements so that we center people as we continue to innovate.”</p><p><span>But now there’s a gulf between who gets a say in AI policy, Bryan says, and who doesn’t. “There's the needs that everyday folks have,” he says, “and there's the needs that our tech billionaire class has—and </span><em>those</em><span> are the only ones being addressed.”</span></p><p>Samantha Gordon, a program director at TechEquity, a nonprofit group of tech workers that advocates for housing and labor issues, and that has backed a number of California AI bills, tells me that widening gulf is by design. “This amendment is the direct result of a campaign by Google, Meta, OpenAI, and venture capitalists like Andreessen Horowitz—and their dozens of trade associations—to bulldoze through the public's safety in order to continue to make risky bets on a precarious and potentially hazardous technology,” Gordon says. </p><p><a href="https://www.politico.com/newsletters/digital-future-daily/2024/05/06/exclusive-poll-americans-favor-ai-data-regulation-00156350" rel="">Public polling</a><span> shows </span><a href="https://www.forbes.com/sites/cio/2024/04/18/ai-regulation-has-strong-bipartisan-approval/" rel="">bipartisan support</a><span> for </span><em>more </em><span>regulation of AI, after all, not less. And yet, as Gordon puts it, “if this amendment passes, not a single state in America could protect people from AI systems that unfairly deny their medical care, keep their nursing homes understaffed, revoke their unemployment benefits, or inflate their rent.” It’s part of what she says is a “cynical campaign” the tech industry is waging “to override the will of the public.”</span></p><p><span>Now, there’s a good possibility that this aggressive language won’t survive the Byrd Rule—</span><a href="https://www.congress.gov/crs-product/RL30862" rel="">a law that restricts</a><span> what can be included in the reconciliation process to measures that affect spending levels and revenue—but it might. And GOP leadership, which now counts Silicon Valley insiders and AI bulls like Musk, Andreessen, and David Sacks among its inner circle, may deem it worth the legal challenges. And even if the language does gets stripped we cannot afford to ignore what it tells us: Top Republicans and top players in the AI industry can now move as a united front. The time of AI industry leaders paying lip service to AI as a technology that “benefits all of humanity,” a line that has been withering on the vine for a while now, is gone. In its place is a cold calculus bent on using the technology and </span><a href="https://www.bloodinthemachine.com/p/whats-really-behind-elon-musk-and" rel="">its logic</a><span> to accumulate as much power as possible. </span></p><p>So let’s run down why it is the AI companies are so intent on stopping these state-level bills, why the GOP is so interested in helping them, and how this changes the very way we should think about AI as a technology. </p><p><span>You may have noticed in the above language in the bill goes beyond “AI” and also includes “automated decision systems.” That’s likely because there are two California bills currently under consideration in the state legislature that use the term; AB 1018, </span><a href="https://techequity.us/the-automated-decisions-safety-act-ab-1018/" rel="">the Automated Decisions Safety Act</a><span> and SB7, the </span><a href="https://sd05.senate.ca.gov/news/mcnerney-introduces-no-robo-bosses-act-ensure-human-oversight-ai-workplace" rel="">No Robo Bosses Act</a><span>, which would seek to prevent employers from relying on “automated decision-making systems, to make hiring, promotion, discipline, or termination decisions without human oversight.”</span></p><p><span>The GOP’s new amendments would ban both outright, along with the other </span><a href="https://calmatters.org/economy/technology/2025/03/ai-regulation-after-trump-election/" rel="">30 proposed bills that address AI</a><span> in California. Three of the proposed bills are backed by the California Federation of Labor Unions, including AB 1018, which aims to eliminate algorithmic discrimination and to ensure companies are transparent about how they use AI in workplaces. It requires workers to be told if AI is used in the hiring process, allows them to opt out of AI systems, and to appeal decisions made by AI. The Labor Fed also backs Bryan’s bill, AB 1221, which seeks to prohibit discriminatory surveillance systems like facial recognition, establish worker data protections, and compels employers to notify workers when they introduce new AI surveillance tools. </span></p><p><span>It should be getting clearer why Silicon Valley is intent on halting these bills: One of the key markets—if not </span><em>the </em><span>key market—for AI is as enterprise and workplace software. A top promise is that companies can automate jobs and labor; restricting surveillance capabilities or carving out worker protections promise to put a dent in the AI companies’ bottom lines. Furthermore, AI products and automation software promise a way for managers to </span><em>evade</em><span> accountability—laws that force them to stay accountable defeat the purpose.</span></p><p><span>OpenAI already won a major victory in beating back state level policy earlier this year, after Assemblywoman Diane Papan, who had proposed a bill aimed at preventing nonprofits from restructuring as for-profit companies—which OpenAI was in the process of trying to do—</span><a href="https://www.sfexaminer.com/news/technology/california-bill-barring-openai-for-profit-transition-dead/article_27250d39-7577-47cc-a414-a7b13e5f6ce0.html" rel="">gutted the language of her own bill</a><span> and replaced it with </span><a href="https://garymarcus.substack.com/p/breaking-bill-that-would-have-blocked" rel="">essentially an entirely new one</a><span>. The strange move came after pushback from OpenAI, and just three days after OpenAI closed its deal with SoftBank for $40 billion, a large portion of which is contingent on the removal of that nonprofit structure. It’s almost quaint to think back to 2023, when Sam Altman made a performative show of </span><a href="https://www.nytimes.com/2023/05/16/technology/openai-altman-artificial-intelligence-regulation.html" rel="">asking Congress to regulate his company</a><span>—he’s spent the two years since fighting tooth and nail against every meaningful regulation that would affect his business.</span></p><p><span>The Trump administration, meanwhile, has adopted the industry’s zeal for deregulation; in part, of course, because there’s significant overlap between the industry and the administration. One of Trump’s first actions was to dissolve Biden’s framework for governing AI, and to institute a new set of priorities aimed not at safe, equitable AI but at helping the US AI industry achieve dominance. Vice president, and former venture capitalist, JD Vance used his first speech abroad to call for </span><a href="https://www.bloodinthemachine.com/p/ai-is-in-its-empire-era" rel="">an end to international AI regulations</a><span>. Marc Andreessen, who’s </span><a href="https://www.washingtonpost.com/politics/2025/01/13/andreessen-tech-industry-trump-administration-doge/" rel="">advising the administration on tech policy</a><span>, and who wields nearly as much influence as Musk, has long advocated for less regulation—his fingerprints are all over the Guthrie amendment.</span></p><p><span>In an effort to try to better understand the GOP’s aims here, I called up Guthrie’s office, and spoke on background with a rep on the energy and commerce committee. </span><a href="https://energycommerce.house.gov/posts/chairman-guthrie-delivers-opening-statement-at-full-committee-markup-of-budget-reconciliation-text" rel="">Evidently</a><span>, their plan is to argue that because the Trump administration is modernizing agencies like the Department of Commerce and the Federal Trade Commission with AI, banning states’ ability to regulate AI is a spending-related matter. If, for instance, California passes a law that, say, requires an AI company to comply with transparency laws, and it becomes more expensive as a result, then the federal government will have to spend more on AI services. </span></p><p>This strikes me as an enormous stretch, as such logic could be deployed to ban state lawmaking around just about anything. You could, say, ban states from making laws that seek to regulate the housing market, on the grounds that they might effect the price of maintaining federal buildings, or ban statewide labor laws because they impact the cost of paying federal employees, and so on.</p><p>It seems that the talking points around promoting this amendment will roughly be: </p><p><span>-It will encourage innovation and efficiency, preventing AI companies from having to deal with a patchwork of state laws</span><br><span>-States like Colorado and California that have passed or are preparing to pass AI regulations are not truly prepared to do so</span><br><span>-This effort is actually intended to benefit little tech, not big tech, because any regulations would harm little tech more</span><br><span>-A “light touch” is imperative so we can beat China in the AI race</span></p><p><span>A lot of these ideas can be traced back to Congressional </span><a href="https://web.archive.org/web/20250410155751/https://www.washingtonpost.com/politics/2025/04/10/ai-race-china-energy-congress/" rel="">committee hearings</a><span> held by Gutherie and Ted Cruz in recent months, which were attended by Altman, former Google chief Eric Schmidt, Scale AI CEO Alexandr Wang and others. The notion that the US must “beat” China in the AI race at any cost was a frequent theme, and this was where Schmidt’s now-infamous declaration that AI needs to be given as much energy as possible (and not to worry, AI will solve the climate crisis) was made. It left an impression.</span></p><p><span>“Eric Schmidt said we need to use energy [to develop AI] because it’s going to produce the solutions to climate change,” Brett Gutherie told the </span><em>Washington Post</em><span>, weeks before he introduced his amendment banning state lawmaking on AI. </span></p><div id="youtube2-Ffak4ngCvgE" data-attrs="{&quot;videoId&quot;:&quot;Ffak4ngCvgE&quot;,&quot;startTime&quot;:null,&quot;endTime&quot;:null}" data-component-name="Youtube2ToDOM"><p><iframe src="https://www.youtube-nocookie.com/embed/Ffak4ngCvgE?rel=0&amp;autoplay=0&amp;showinfo=0&amp;enablejsapi=0" frameborder="0" loading="lazy" gesture="media" allow="autoplay; fullscreen" allowautoplay="true" allowfullscreen="true" width="728" height="409"></iframe></p></div><p><span>In the interview, Guthrie argues that “the most existential threat to America” is “losing the battle for AI” to China.</span><span><a data-component-name="FootnoteAnchorToDOM" id="footnote-anchor-1-163495084" href="https://www.bloodinthemachine.com/p/de-democratizing-ai#footnote-1-163495084" target="_self" rel="">1</a></span><span> That, Guthrie says, is why we can’t “go down the path some other continents have,” as Europe has, and adopt even modest regulations on AI or the tech sector. </span></p><p>It’s unclear whether Guthrie and the GOP—or Sam Altman and Eric Schmidt, for that matter—believe there’s an AI race with China of existential proportions, or if it’s simply a useful line to justify calling for limitless investment, and placing AI outside the democratic process. Ultimately it doesn’t matter. It’s serving GOP and Silicon Valley interests in providing the imperative for unfettered AI development, even halfheartedly. </p><p>What’s clear is that the GOP, AI executives, and Gulf State princes all have a common belief in AI—as a means of accumulating capital, undercutting labor, and concentrating power. And the terms of AI development and deployment are on the cusp of being set entirely by oligarchs, billionaires, and their allies in the ruling party. And those parties are intent removing any impediments—like the democratic process—from their pursuit of power and profit. </p><p>“Politicians are letting billionaires call the shots and all of us will be the ones who pay the price,” as TechEquity’s Samantha Gordon put it. Bryan says it might come to people taking to the streets; there’s so much at stake. “As goes California, as goes the country. And so they're trying to get ahead of us,” he says. “But I don't think they've got the expertise, and they certainly don't have the American people behind them in an effort like this.”</p><p><span>Over in the Senate, Ted Cruz </span><a href="https://www.washingtonpost.com/opinions/2025/05/14/artificial-intelligence-regulation-congress-reconciliation/" rel="">has announced</a><span> that he will introduce an amendment like Guthrie’s, where he’ll make the push for the ban on states making their own AI rules to become law. </span></p><p><em>This story was edited by Mike Pearl. Eliza McCullough contributed research.</em></p><p><span>In reporting the above piece, I spoke at length with California state assemblyman Isaac Bryan, the author of the state bill </span><a href="https://calmatters.digitaldemocracy.org/bills/ca_202520260ab1221" rel="">AB 1221: Workplace surveillance tools</a><span>, the co-sponsor of a number of other AI-focused bills, and who also happens to represent my district in Los Angeles. </span></p><p>I thought the full conversation worth sharing, so I’m sharing a lightly edited and abridged version of it below. </p><p><strong>BLOOD IN THE MACHINE: Thanks for taking the time to talk —&nbsp;so, you’ve sponsored some of the bills that are being considered in the California state legislature, to prevent employers from using AI unethically in the workplace, for one. How are you thinking about these bills now, as the GOP is taking aim at your capacity to even pass such laws?</strong></p><p>The reality is we still have a preservation of states' rights and the ability for states to set policy guidelines and regulations, particularly on issues that impact residents of their state disproportionately. The tech industry was incubated, cultivated, and continues to grow and innovate here in California. And I think many of us, all of us, most of us, believe in that innovation, believe in that creativity, believe in that advancement. It's also why California deserves the right, and has the expertise, to lead. We’ve been establishing meaningful guardrails and regulations around these advancements so that we center people as we continue to innovate.</p><p>The challenges of this administration in Washington—they often shoot from the hip and misfire as they have rapidly on several different occasions with several different policy fronts and especially on things related to the economy. I can't think of how many executive orders and how many tariff ideas and how many other things have come from this administration only to be kind of rolled back or changed when they ran into the pragmatism of reality.</p><p>The budget reconciliation process certainly isn't done yet and I know we continue all to an ever-encroaching minority in the House. And so I expect for California's voices to be heard. But I think all states should be deeply concerned about new levels of preemption. That should be a bipartisan kind of conversation about where the federal government can step in, should step in, and where it absolutely should not.</p><p>What we're going to do in California is continue to lead in the ways that we have. Balancing the needs of everyday people, the needs of the emerging industries, the desires of those of us in the state house, along with the goals of the governor, and strike those balances where we can. And if the federal government continues to encroach on that, we'll continue to file lawsuits as we have and have done successfully, both in the past Trump administration and currently.</p><p><strong>There are two levels of audacity here. First, that they would even attempt such a thing — this is a party that has, in the past, quite loudly expressed their belief in states' rights; as recently as last summer. And now to try to do away with them on such a key issue altogether. But then, secondly, to do this in the budget reconciliation bill, to have such a far-reaching and potentially impactful measure put through in reconciliation. That really, to me, without giving this bill a proper hearing, it really underscores how undemocratic this maneuver is, around such an important topic.</strong></p><p>I couldn't agree further. I mean, if there's one thing that the GOP has been consistent about, it has been their hypocrisy. They are for any type of legislative maneuvers, any type of distribution of checks and balances and powers that favor them in any given moment. There's very limited consistency, and we've seen some strict constitutionalists in the GOP absolutely flipped the script to justify the actions of the current administration and leaders in Congress. This is no exception.</p><p>But, you know, this is a moment for those in the federal government who value the preservation of our civic institutions over the preservation of self and self-interest to rise and stand up. And I think you're seeing that kind of consistency from the left. You're seeing that kind of measured and steady hand from the last two Democratic presidents, and hopefully that kind of longer term view of balancing powers and making sure that the American people are heard, the people who are most impacted by these kinds of changes. And in this particular space, on AI, that is Californians. </p><p>Whether the Trump administration likes it or not, I recognize that he took a far-reaching group of billionaire CEOs and particular tech CEOs to Saudi Arabia just the other day to meet with the prince. And I think all of that has important diplomatic motivations, but it's a very strange thing to have people struggling to keep a roof over their head and watching the exorbitant wealth being generated by tech billionaires, and the preservation of that wealth by this administration, supersede the needs of everyday people.</p><p>The richest man in the world, a tech billionaire himself, was serving as a surrogate president for the last several months. I think it's a scary time for folks.</p><p><strong>I could not ignore that irony either. The same day that the amendment to try to effectively wipe out AI regulation in the United States, the CEOs of these companies are in Saudi Arabia inking billion dollar deals.</strong><span> </span></p><p>It's interesting, too. It seems like the only bridges that Trump can build are between tech billionaire CEOs who don't like each other, right? It's not a well-kept secret that Musk and Altman don't like each other. Their views around OpenAI have spilled out into the public, and yet they both seem to find comfort, security, and safety in this current president.</p><p>It's a shame that the everyday folks across this country who are struggling right now and deeply afraid of how these kind of economic decisions will impact or limit their choices as they try to provide food for their kids and keep a roof over their heads and buy new school clothes and backpacks. There's the needs that everyday folks have, and there's the needs that our tech billionaire class has—and those are the only ones being addressed. </p><p><strong>I know it's always a tricky line in California, because the tech sector is largely based here. And it's a key constituency. Does it concern you at all that these tech companies, that this is essentially what they have been lobbying for? That Sam Altman and Google and IBM have been pushing for an exemption to state lawmaking, specifically because I think that they worry about having to comply with rules that might be put forward in places like California?</strong></p><p>Yeah, it's deeply concerning because I think there's no place better positioned to understand the tremendous positive things, both for society, for social living, but also for wealth generation in an industry that pays taxes to the state. Nobody understands that better than California. But we also deal with the harsh and very real realities that as these kinds of innovations take place in a way that displaces workers, with an intentionality of increasing productivity through the laying off of everyday people trying to earn a living, that there's a balance that's got to be struck there. </p><p>And even as we generate new forms of state revenue, and are able to increase the state's wealth through this new industry, we will also have a disproportionate growth in liabilities, as people will need unemployment and health care and other social safety nets because their ability to earn a living has drastically changed during this spike of innovation. So we've got to be mindful of that balance—we also want to make sure that tools don't become predatory, or increase the opportunity for data and information to be leaked.</p><p>It's unconscionable to me the way that the current federal administration has treated people's private and sensitive federal data like some sort of play thing for Elon Musk and Big Balls—only because I can't remember the guy's actual name.</p><p><strong>On the DOGE team.</strong></p><p>Exactly. To me, how much we've allowed for our lives to be captured through these algorithmic systems, and through these innovations, and then to have that data decisively unprotected in this present moment—and so we've got to do almost all of the above in California. And I think when we lead in this sector the kind of decisions we'll make will strike the appropriate balances that allow for others to follow.</p><p>And that's the real fear: As goes California, as goes the country. And so they're trying to get ahead of us. But I don't think they've got the expertise, and they certainly don't have the American people behind them in an effort like this.</p><p><strong>Yeah. These are proposals aimed at limiting some of the harms of AI, making sure that workers don't get steamrolled and can't be surveilled at will, and giving some power back to workers when these tools are used in their workplaces. Now, I think that when a tech company sees that, they see something that’s going to be inconvenient and costly. But can you talk a little bit about why it's important to have things like SB7 or AB 1221, which, I would describe them as hardly radical, but more like common sense proposals in the era of AI, as the technology that stands to affect more workers.</strong></p><p>They're absolutely common sense proposals, and they're working through the legislative process, taking amendments through the process, learning, bringing stakeholders to the table.</p><p><span>It's interesting too, if I was some of these tech CEOs, I actually wouldn't </span><em>want</em><span> this power to, the ability to regulate and make thoughtful decisions in the hands of somebody [Trump] who's decided that thoughtfulness is not a characteristic that they want to exhibit through their leadership.</span></p><p>I mean, he has haphazardly taken a sledgehammer and swung from left to right on a range of issues. And even in these issues, I think he's going to wake up one day and realize that the Teamsters, the only labor union that backed him at the federal level, have a deep interest in not being pushed out by automated trucks, which is a conversation that's been going on here in California, and there's been some back and forth between legislators and the governor on how to land this correctly, even here. But that's another base of a constituency that he will eventually hear from. They probably don't get in through the door as quickly as the billionaire tech CEOs.</p><p>So I think this is California's responsibility. These are the kinds of state rights that we can and should have. We are allowed to govern in the interests that protect our economy and the people who rely on us. To have that preempted or suggested to be preempted in this way surely has got to be unconstitutional and we should do what we can to find out.</p><p><strong>Let’s talk for a second about why you co-sponsored these bills in the first place, and what stands to be lost. </strong></p><p>Our bill, AB 1221, we offered this bill because we don't want to lose the humanity in the workplace.</p><p>There are some AI tools now that, register your emotional feeling for the day, your gait, movement and the way that you walk. They make corrective decisions, recommendations, disciplinary recommendations, all without human intervention. This has gone far beyond cameras in the store to make sure theft doesn't occur. It is very invasive. It has increasing ability to show biased attitudes, biased behaviors that can be harmful to both protected classes and workers more broadly. </p><p>We just want to make sure that these tools are being used responsibly, that workers know which ones are being used on them and that any kind of disciplinary activity or things that impact somebody's ability to keep their job that those decisions are ultimately made by a person—which doesn't sound unreasonable to me. Like I said, it's about preserving the humanity in the workplace.</p><p><strong>You mentioned legal challenges.</strong><span> </span><strong>Hopefully it gets struck down and doesn't pass the Byrd Rule in the Senate, and hopefully. But now we've also seen their colors, their intent, through this maneuver. That they're willing to even attempt this route; something that even just a couple years ago would have been considered audacious and extreme. How do you push back on this?</strong></p><p>We stand up. We make sure our elected officials hear from us. We, hold rallies, hold town halls, hold people accountable, take to the streets when necessary, and defend states that are willing to step up and buck this administration for the good of the American people.</p><p>You know, this is not a partisan issue. It's about putting people first. And that used to be a shared value. But it's not everyday people who met with Saudi oligarchs a week ago, right? You needed a certain net worth to be invited on that trip.And you can't imagine that any kind of conversations that take place in that setting are good for everyday workers trying to keep a roof over their head and food on their tables. But there are more everyday people trying to earn an honest living in this moment than there are tech billionaires, and it's time for those folks to be heard.</p><p><strong>Well, I think that's a great place to leave it. Thanks for your time.</strong></p><p>Absolutely. Thank you.</p><p><strong>OTHER BLOODY STUFF</strong></p><ul><li><p><span>Columbia Journalism Review asked me to </span><a href="https://www.cjr.org/feature-2/how-were-using-ai-tech-gina-chua-nicholas-thompson-emilia-david-zach-seward-millie-tran.php#Tristan%20Lee" rel="">participate in a roundup</a><span> of how journalists are using AI—Spoiler: I am not. The whole thing is worth a read, with smart takes from fellow travelers like Jason Koebler, Khari Johnson, Susie Cagle, and others. </span></p></li><li><p>I was on the Majority Report with Emma Vigeland to talk about my piece on the AI jobs crisis:</p><div id="youtube2-oezqSzpb-h8" data-attrs="{&quot;videoId&quot;:&quot;oezqSzpb-h8&quot;,&quot;startTime&quot;:null,&quot;endTime&quot;:null}" data-component-name="Youtube2ToDOM"><p><iframe src="https://www.youtube-nocookie.com/embed/oezqSzpb-h8?rel=0&amp;autoplay=0&amp;showinfo=0&amp;enablejsapi=0" frameborder="0" loading="lazy" gesture="media" allow="autoplay; fullscreen" allowautoplay="true" allowfullscreen="true" width="728" height="409"></iframe></p></div></li><li><p><span>Friend of the blog </span><a href="https://x.com/tigerbeat" rel="">Steve Rhodes</a><span> sent over this poem, which is worth a read:</span></p><div data-attrs="{&quot;instagram_id&quot;:&quot;DJo6P84OH4J&quot;,&quot;title&quot;:&quot;A post shared by @ssyjuco&quot;,&quot;author_name&quot;:&quot;ssyjuco&quot;,&quot;thumbnail_url&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/__ss-rehost__IG-meta-DJo6P84OH4J.jpg&quot;,&quot;timestamp&quot;:null,&quot;belowTheFold&quot;:true}" data-component-name="InstagramToDOM"><p><a href="https://instagram.com/p/DJo6P84OH4J" target="_blank" rel=""><img src="https://substackcdn.com/image/fetch/w_640,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F__ss-rehost__IG-meta-DJo6P84OH4J.jpg" loading="lazy"></a></p></div></li><li><p>Tune into System Crash this week, where we discuss the above, as well as the new Luddite pope, and a lot more. </p></li><li><p><span>Tune into a live chat on Friday, May 16th, at 10 AM EST / 1 PM PST with Karen Hao, where we’ll talk about her fantastic new book, </span><a href="https://www.penguinrandomhouse.com/books/743569/empire-of-ai-by-karen-hao/" rel="">Empire of AI</a><span>.</span></p></li></ul><p>That’s it for this week. Until next time, thanks for reading —&nbsp;and hammers up. Way up. </p></div></article></div><div id="discussion"><h4>Discussion about this post</h4></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[XTool – Cross-platform Xcode replacement (176 pts)]]></title>
            <link>https://github.com/xtool-org/xtool</link>
            <guid>44011515</guid>
            <pubDate>Sat, 17 May 2025 02:10:36 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/xtool-org/xtool">https://github.com/xtool-org/xtool</a>, See on <a href="https://news.ycombinator.com/item?id=44011515">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><p dir="auto"><h2 tabindex="-1" dir="auto">xtool</h2><a id="user-content-xtool" aria-label="Permalink: xtool" href="#xtool"></a></p>
<p dir="auto">Cross-platform Xcode replacement. Build and deploy iOS apps with SwiftPM on Linux, Windows, and macOS.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Overview</h2><a id="user-content-overview" aria-label="Permalink: Overview" href="#overview"></a></p>
<p dir="auto">xtool is a cross-platform (Linux/WSL/macOS) tool that replicates Xcode functionality with open standards.</p>
<p dir="auto">✅ Build a SwiftPM package into an iOS app</p>
<p dir="auto">✅ Sign and install iOS apps</p>
<p dir="auto">✅ Interact with Apple Developer Services programmatically</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Getting Started</h2><a id="user-content-getting-started" aria-label="Permalink: Getting Started" href="#getting-started"></a></p>
<ol dir="auto">
<li>Follow the guide to install <code>xtool</code>
<ul dir="auto">
<li><a href="https://swiftpackageindex.com/xtool-org/xtool/documentation/xtool/installation-linux" rel="nofollow">Installation (Linux/Windows)</a></li>
<li><a href="https://swiftpackageindex.com/xtool-org/xtool/documentation/xtool/installation-macos" rel="nofollow">Installation (macOS)</a></li>
</ul>
</li>
<li>Create and run your first xtool-powered app by following the <a href="https://swiftpackageindex.com/xtool-org/xtool/tutorials/xtool/first-app" rel="nofollow">tutorial</a>!</li>
</ol>
<p dir="auto"><h2 tabindex="-1" dir="auto">Examples</h2><a id="user-content-examples" aria-label="Permalink: Examples" href="#examples"></a></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Screenshot</h3><a id="user-content-screenshot" aria-label="Permalink: Screenshot" href="#screenshot"></a></p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/xtool-org/xtool/blob/main/Sources/xtool/Documentation.docc/Resources/Cover.png"><img src="https://github.com/xtool-org/xtool/raw/main/Sources/xtool/Documentation.docc/Resources/Cover.png" alt="A screenshot of xtool being invoked from VSCode"></a></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Command line interface</h3><a id="user-content-command-line-interface" aria-label="Permalink: Command line interface" href="#command-line-interface"></a></p>
<div dir="auto" data-snippet-clipboard-copy-content="$ xtool --help
OVERVIEW: Cross-platform Xcode replacement

USAGE: xtool <subcommand>

OPTIONS:
  -h, --help              Show help information.

CONFIGURATION SUBCOMMANDS:
  setup                   Set up xtool for iOS development
  auth                    Manage Apple Developer Services authentication
  sdk                     Manage the Darwin Swift SDK

DEVELOPMENT SUBCOMMANDS:
  new                     Create a new xtool SwiftPM project
  dev                     Build and run an xtool SwiftPM project
  ds                      Interact with Apple Developer Services

DEVICE SUBCOMMANDS:
  devices                 List devices
  install                 Install an ipa file to your device
  uninstall               Uninstall an installed app
  launch                  Launch an installed app

  See 'xtool help <subcommand>' for detailed help."><pre>$ xtool --help
OVERVIEW: Cross-platform Xcode replacement

USAGE: xtool <span>&lt;</span>subcommand<span>&gt;</span>

OPTIONS:
  -h, --help              Show <span>help</span> information.

CONFIGURATION SUBCOMMANDS:
  setup                   Set up xtool <span>for</span> iOS development
  auth                    Manage Apple Developer Services authentication
  sdk                     Manage the Darwin Swift SDK

DEVELOPMENT SUBCOMMANDS:
  new                     Create a new xtool SwiftPM project
  dev                     Build and run an xtool SwiftPM project
  ds                      Interact with Apple Developer Services

DEVICE SUBCOMMANDS:
  devices                 List devices
  install                 Install an ipa file to your device
  uninstall               Uninstall an installed app
  launch                  Launch an installed app

  See <span><span>'</span>xtool help &lt;subcommand&gt;<span>'</span></span> <span>for</span> detailed help.</pre></div>
<p dir="auto"><h3 tabindex="-1" dir="auto">Library</h3><a id="user-content-library" aria-label="Permalink: Library" href="#library"></a></p>
<p dir="auto">xtool includes a library that you can use to interact with Apple Developer Services, iOS devices, and more from your own app. You can use this by adding <code>XKit</code> as a SwiftPM dependency.</p>
<div dir="auto" data-snippet-clipboard-copy-content="// package dependency:
.package(url: &quot;https://github.com/xtool-org/xtool&quot;, .upToNextMinor(from: &quot;1.2.0&quot;))
// target dependency:
.product(name: &quot;XKit&quot;, package: &quot;xtool&quot;)"><pre>// package dependency:
<span>.</span><span>package</span><span>(</span>url<span>:</span> <span>"</span><span>https://github.com/xtool-org/xtool</span><span>"</span><span>,</span> <span>.</span>upToNextMinor<span>(</span>from<span>:</span> <span>"</span><span>1.2.0</span><span>"</span><span>)</span><span>)</span>
// target dependency:
<span>.</span><span>product</span><span>(</span>name<span>:</span> <span>"</span><span>XKit</span><span>"</span><span>,</span> <span>package</span><span>:</span> <span>"</span><span>xtool</span><span>"</span><span>)</span></pre></div>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Wow@Home – Network of Amateur Radio Telescopes (172 pts)]]></title>
            <link>https://phl.upr.edu/wow/outreach</link>
            <guid>44011489</guid>
            <pubDate>Sat, 17 May 2025 02:02:14 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://phl.upr.edu/wow/outreach">https://phl.upr.edu/wow/outreach</a>, See on <a href="https://news.ycombinator.com/item?id=44011489">Hacker News</a></p>
<div id="readability-page-1" class="page"><div tabindex="-1" dir="ltr"><div id="h.f6d8ee3d198bc6f_1" jscontroller="sGwD4d" jsaction="zXBUYb:zTPCnb;zQF9Uc:Qxe3nd;" jsname="F57UId" tabindex="-1"><p role="main" tabindex="0"><h2 id="h.m56w2pmswrzx" dir="ltr"><span>Wow@Home</span></h2></p></div><div id="h.f6d8ee3d198bc6f_27" jscontroller="sGwD4d" jsaction="zXBUYb:zTPCnb;zQF9Uc:Qxe3nd;" jsname="F57UId" tabindex="-1"><p dir="ltr"><span>A </span><span>network of small</span><span> </span><span>radio telescopes</span><span> offers several distinct advantages compared to large professional observatories. These systems are low</span><span>-</span><span>cost an</span><span>d</span><span> can operate autonomously around the clock, making them ideal for continuous monitoring of transient events or long-duration signals that professional telescopes cannot commit to observing full-time.</span></p><p dir="ltr"><span>Their geographic distribution enables global sky coverage and coordinated observations across different time zones, which is especially valuable for </span><span>validating repeating or time-variable signals</span><span>. Coincidence detection across multiple stations helps </span><span>reject local radio frequency interference (RFI)</span><span>, increasing confidence in true astrophysical or technosignature candidates.</span></p><p dir="ltr"><span>These networks are also highly scalable, resilient to single-point failures, and capable of </span><span>rapid response to external alerts</span><span>. Furthermore, they are cost-effective, engaging, and accessible, </span><span>ideal for education, citizen science, and expanding participation in radio astronomy</span><span>.</span></p><p dir="ltr"><span>However, these systems also come with notable limitations when compared to professional telescopes. They have </span><span>significantly lower sensitivity</span><span>, limiting their ability to detect faint or distant sources. Their angular resolution is poor due to smaller dish sizes and wide beamwidths, making </span><span>precise source localization difficult</span><span>.</span></p><p dir="ltr"><span>Calibration can be inconsistent across stations</span><span>, and frequency stability or dynamic range may not match the performance of professional-grade equipment. Additionally, without standardized equipment and protocols, data quality and interoperability can vary across the network. Despite these constraints, when thoughtfully coordinated, such networks can </span><span>provide valuable complementary observations to professional facilities</span><span>.</span></p></div><div id="h.3e7c17c5694c8634_48" jscontroller="sGwD4d" jsaction="zXBUYb:zTPCnb;zQF9Uc:Qxe3nd;" jsname="F57UId" tabindex="-1"><h2 id="h.xu5ifbv830af_l" dir="ltr"><div jscontroller="Ae65rd" jsaction="touchstart:UrsOsc; click:KjsqPd; focusout:QZoaZ; mouseover:y0pDld; mouseout:dq0hvd;fv1Rjc:jbFSOd;CrfLRd:SzACGe;"><p><span>The </span><span>Wow@Home Radio Telescope</span></p></div></h2></div><div id="h.3e7c17c5694c8634_44" jscontroller="sGwD4d" jsaction="zXBUYb:zTPCnb;zQF9Uc:Qxe3nd;" jsname="F57UId" tabindex="-1"><p dir="ltr"><span>This page presents a test of our first </span><span>Wow@Home Radio Telescope</span><span> hardware and software configuration (Figure 1). The system is tested for a network of small radio telescopes designed to emulate, as closely as possible, the observation protocol of the meridian radio telescope </span><span>Big Ear</span><span> used by the </span><span>Ohio SETI project</span><span> in the 1970s. As in the </span><a href="http://www.bigear.org/Wow30th/wow30th.htm" target="_blank"><span>original setup</span></a><span>, we use a </span><span>10 kHz channel width</span><span> and a </span><span>12-second integration time</span><span>. However, our system differs in several ways: it features 256 channels instead of 50, a much larger beam size, but significantly lower sensitivity.</span></p><p dir="ltr"><span>The telescope is </span><span>fixed at a constant elevation</span><span>, pointed south, and scans a specific celestial declination over the course of one or more days using a wide field of view of approximately 25° (HPBW or its beamwidth). As the Earth rotates, this configuration allows the telescope to capture a </span><span>continuous 360° strip of the sky</span><span> at that declination. After completing three or more full-sky passes, the telescope is adjusted to a new elevation to begin scanning a different declination, gradually building up </span><span>full-sky coverage over time</span><span>.</span></p><p dir="ltr"><span>While optimized for </span><span>educational use</span><span>, this configuration also yields valuable data on RFI near the H I line in urban environments, helping us assess the likelihood of RFI mimicking a Wow!-like signal. Additionally, it serves as a practical platform for a </span><span>wide-field search for strong transient events</span><span>, whether of astrophysical origin or potential technosignatures.</span></p><p dir="ltr"><span>For events that persist longer than a day, </span><span>multiple observing passes</span><span> can be used to validate their presence, detect weaker features, improve overall sensitivity, and help distinguish them from RFI. Additionally, </span><span>simultaneous observations</span><span> by two or more telescopes pointed at the same location can further aid in rejecting local interference and confirming the reality of signals that last less than 24 hours.</span></p><p dir="ltr"><span>The Wow@Home Radio Telescope operates autonomously, 24/7, as a meridian-style instrument, conducting a continuous </span><span>all-sky survey</span><span> </span><span>for transient events</span><span>. The hardware required to build these telescopes is both </span><span>inexpensive and widely accessible</span><span>, relying on readily available components. </span><span>The critical element lies in the software</span><span>, which must be capable of analyzing data effectively, whether from a single station or across a coordinated network of telescopes.</span></p><p dir="ltr"><span>Future expansions could include the integration of </span><span>multibeam systems</span><span> to enable simultaneous ON–OFF observations </span><span>to improve sensitivity</span><span>, </span><span>tracking capability</span><span> to perform targeted observations of specific sources, </span><span>multi-site detection</span><span> for signal validation, higher sensitivity, and RFI discrimination, </span><span>interferometric capabilities</span><span> for improved angular resolution, and </span><span>phased array configurations</span><span> to enhance sensitivity and enable electronic beam steering.</span></p></div><div id="h.3e7c17c5694c8634_3" jscontroller="sGwD4d" jsaction="zXBUYb:zTPCnb;zQF9Uc:Qxe3nd;" jsname="F57UId" tabindex="-1"><p><img src="https://lh3.googleusercontent.com/A5qS8iP4yHHZob04FJu8ja9Yf1wfndGkZ_jZD5ng32OkIaWIn0i2yyo47SUkBpoWr64Mxka8U4MbdyZMRsQGyIkk_K8z5--u0LU3PokF8Y1snW6TSvEkKvvducYt9A4RMw=w1280" role="img"></p></div><div id="h.3e7c17c5694c8634_13" jscontroller="sGwD4d" jsaction="zXBUYb:zTPCnb;zQF9Uc:Qxe3nd;" jsname="F57UId" tabindex="-1"><p dir="ltr"><span>Figure 1: </span><span>Components of our first Wow@Home Radio Telescope. The </span><a href="https://github.com/tedcline/ezRA" target="_blank"><span>Easy Radio Astronomy (ezRA)</span></a><span> software is an excellent starter package for getting this configuration up and running for radio astronomy. We plan to test additional configurations in the coming months, including the </span><a href="https://www.crowdsupply.com/krakenrf/discovery-dish" target="_blank"><span>Discovery Dish</span></a><span>, which integrates the frontend into the antenna, and the </span><a href="https://airspy.com/airspy-mini/" target="_blank"><span>Airspy Mini</span></a><span> as the backend, offering a 12-bit ADC for improved dynamic range.</span></p></div><div id="h.3e7c17c5694c8634_30" jscontroller="sGwD4d" jsaction="zXBUYb:zTPCnb;zQF9Uc:Qxe3nd;" jsname="F57UId" tabindex="-1"><p dir="ltr"><span>The </span><span>Wow@Home Software</span><span> is the core of our project. It serves as the data acquisition and analysis platform designed to search for transient events caused by astrophysical phenomena, potential technosignatures, and RFI characterization, using data from any small radio telescope. The software is built on the analysis methods we are developing to detect Wow-like signals in the archive data of professional observatories, as part of our </span><a href="https://phl.upr.edu/wow"><span>Arecibo Wow! Project</span></a><span>. We are currently developing the software in </span><a href="https://www.nv5geospatialsoftware.com/Products/IDL" target="_blank"><span>IDL</span></a><span>, with example outputs shown in Figures 2, 3, and 4. It will later be translated to Python to ensure cross-platform compatibility and broader accessibility.</span></p></div><div jscontroller="sGwD4d" jsaction="zXBUYb:zTPCnb;zQF9Uc:Qxe3nd;" jsname="F57UId" tabindex="-1" id="h.f6d8ee3d198bc6f_8"><div id="h.331927cc8f5c07b8_4"><p><img src="https://lh4.googleusercontent.com/zhgcn2e7vYwMCacemRJOlqkF5VcZcPjS6d8s5sf80ZYdLPNsoSpT0vNAZs7RIVnrGzhbHlboR0S0chVQrL1nGlPG6eynsKj6hH73NCSuuGDFrYGboTSbP8jtSwohUJC2vw=w1280" role="img"></p></div><div id="h.3e7c17c5694c8634_4"><p dir="ltr"><span>Figure </span><span>2</span><span>: </span><span>This is a test run of the Wow@Home Radio Telescope</span><span>. </span><span>The top panel shows the </span><span>relative</span><span> power as a function of time. The </span><span>next</span><span> panel </span><span>is</span><span> the signal-to-noise ratio (SNR). Most RFI here originates from continuum sources, which are relatively easy to filter out. </span><span>The following dynamic spectra images show three different ways to analyze the data, depending on the type of signal of interest. The broadband SNR is suitable for detecting continuum sources, but RFI heavily contaminates it. A second telescope at a different location could be used to cross-correlate astronomical signals. The mediumband SNR is good for highlighting the</span><span> Galactic center transiting </span><span>after</span><span> 6 hours and the Galactic anticenter </span><span>about</span><span> </span><span>12</span><span> hours later. The narrowband SNR is more sensitive to signals oc</span><span>curring in only one channel. </span><span>The horizontal line at channel 224 </span><span>is an injected</span><span> test signal spanning the telescope’s beamwidth. An actual narrowband RFI event is visible near channel 0 after 15 hours.</span></p></div></div><div jscontroller="sGwD4d" jsaction="zXBUYb:zTPCnb;zQF9Uc:Qxe3nd;" jsname="F57UId" tabindex="-1" id="h.f6d8ee3d198bc6f_23"><div id="h.5ad76efa4068bcdf_14"><p><img src="https://lh4.googleusercontent.com/wurJ-qrx1IZw_zkvp20ykcD736bQVpzjvXIlsXw0zhCGxcojIRQMiB-Fv8DJtt_9LSaYWyN1IN8daAZTjlNhwWg905MYGhiU7TKfwLVL3GZ7zC_mIz-T7OOS0t9ro8Qi=w1280" role="img"></p></div><div id="h.f6d8ee3d198bc6f_20"><p dir="ltr"><span>Figure 3: </span><a href="https://www.britannica.com/science/hydrogen-cloud" target="_blank"><span>Neutral Hydrogen</span></a><span> (H I) spectral profile of the Galactic center, extracted from the data in Figure 2 at 6.5 hours. Error bars represent the 1σ uncertainty in each frequency channel.</span></p></div></div><div id="h.3e7c17c5694c8634_36" jscontroller="sGwD4d" jsaction="zXBUYb:zTPCnb;zQF9Uc:Qxe3nd;" jsname="F57UId" tabindex="-1"><p><img src="https://lh6.googleusercontent.com/hMIAxkutySlccvKGun3Pd3CByn7EsvLmj8TkK5o3QYFmBoSQwug-dYthHbmg3DS0Dtn1e9C_INbNOE4iyTW3kj_czt590rnQ-rgksTYY7dY1bQPvL-8StNajaFB2AIkL7Q=w1280" role="img"></p></div><div id="h.3e7c17c5694c8634_40" jscontroller="sGwD4d" jsaction="zXBUYb:zTPCnb;zQF9Uc:Qxe3nd;" jsname="F57UId" tabindex="-1"><p dir="ltr"><span>Figure 4: </span><span>In addition to the modern analysis tools available with today’s radio telescopes, we also aim to incorporate into our software the ability to generate a live preview of the data in the style of the original Ohio State SETI project printouts. This feature is intended to provide historical context and connect current efforts to the legacy of early SETI research. Above is an example using the original Wow! Signal data.</span></p></div></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[A kernel developer plays with Home Assistant (138 pts)]]></title>
            <link>https://lwn.net/SubscriberLink/1017720/7155ecb9602e9ef2/</link>
            <guid>44011381</guid>
            <pubDate>Sat, 17 May 2025 01:36:23 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://lwn.net/SubscriberLink/1017720/7155ecb9602e9ef2/">https://lwn.net/SubscriberLink/1017720/7155ecb9602e9ef2/</a>, See on <a href="https://news.ycombinator.com/item?id=44011381">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
<p>
Those of us who have spent our lives playing with computers naturally see
the appeal of deploying them though the home for both data acquisition and
automation.  But many of us who have watched the evolution of the
technology industry are increasingly unwilling to entrust critical
household functions to cloud-based servers run by companies that may not
have our best interests at heart.  The Apache-licensed <a href="https://www.home-assistant.io/">Home Assistant</a> project offers a
welcome alternative: locally controlled automation with free software.
This two-part series covers roughly a year of Home Assistant use, starting
with a set of overall observations about the project.
</p><p>
This is not the first time that LWN has looked at this project, of course;
<a href="https://lwn.net/Articles/822350/">this review</a> gives a snapshot of what Home
Assistant looked like five years ago, while <a href="https://lwn.net/Articles/947843/">this 2023 article</a> gives a good overview of the
project's history, governance, and overall direction.  I will endeavor to
not duplicate that material here.
</p><h4>Project health</h4>
<p>
At a first glance, Home Assistant bears some of the hallmarks of a
company-owned project.  The company in question, <a href="https://www.nabucasa.com/">Nabu Casa</a>, was formed around the
project and employs a number of its key developers.  One of the ways in
which the company makes money is with a $65/year subscription service, providing
remote access to Home Assistant servers installed on firewalled residential
networks.  Home Assistant has support for that remote option, and no
others.  It would be interesting to see what would happen to a pull request
adding support for, say, <a href="https://opensprinklershop.de/en/2023/01/22/opensprinkler-fernzugriff-mit-openthings-cloud-otc-token/">OpenThings
Cloud</a> as an alternative.  The fate of that request would say a lot
about how open the project really is.
</p><blockquote>
	<b>Like what you are reading?</b>
    		<a href="https://lwn.net/Promo/slink-trial-terse/claim">Try LWN for free</a> for 1 month,
    		no credit card required.
</blockquote>
<p>
(For the record, I have bought the Nabu Casa subscription rather than, say,
using WireGuard to make a port available on an accessible system; it is a
hassle-free way to solve the problem and support the development of this
software).
</p><p>
That said, most of the warning signs that accompany a corporate-controlled
project are not present with Home Assistant.  The project's <a href="https://github.com/home-assistant/core/blob/dev/CLA.md">contributor
license agreement</a> is a derivative of the kernel's developer certificate
of origin; contributors retain their copyright on their work.  Since the <a href="https://www.home-assistant.io/blog/2024/04/03/release-20244/">2024.4
release</a>, the Home Assistant core repository has acquired over 17,000
changesets from over 900 contributors.  While a number of Nabu Casa
employees (helpfully listed on <a href="https://www.nabucasa.com/about/">this page</a>) appear in the top ten
contributors, they do not dominate that list.
</p><p>
Home Assistant is clearly an active project with a wide developer base.  In
2024, overall responsibility for this project was transferred to <a href="https://www.openhomefoundation.org/blog/announcing-the-open-home-foundation/">the
newly created Open Home Foundation</a>.  This project is probably here to
stay, and seems unlikely to take a hostile turn in the future.  For a
system that sits at the core of one's home, those are important
characteristics.
</p><h4>Installation and setup</h4>
<p>
Linux users tend to be somewhat spoiled; installing a new application is
typically a matter of a single package-manager command.  Home Assistant
does not really fit into that model.  The first three options on <a href="https://www.home-assistant.io/installation/">the installation
page</a> involve dedicated computers — two of which are sold by Nabu Casa.
For those wanting to install it on a general-purpose computer, the
recommended course is to install the <a href="https://github.com/home-assistant/operating-system">Home Assistant
Operating System</a>, a bespoke Linux distribution that runs Home Assistant
within a Docker container.  There is also a container-based method that can
run on another distribution, but this installation does not support <a href="https://www.home-assistant.io/addons/">the add-ons feature</a>.
</p><p>
Home Assistant, in other words, is not really set up to be just another
application on a Linux system.  If one scrolls far enough, though, one will
find, the instructions to install onto a "normal" Linux system, suitably
guarded with warnings about how it is an "<q>advanced</q>" method.
Of course, that is what I did, putting the software onto an existing system
running Fedora.  The whole thing subsequently broke when a
distribution upgrade replaced Python, but that was easily enough repaired.
As a whole, the installation has worked as expected.
</p><p>
Out of the box, though, a new Home Assistant installation does not do much.
Its job, after all, is to interface with the systems throughout the house,
and every house is different.  While Home Assistant can find some systems
automatically (it found the Brother printer and dutifully informed me that
the device was, inevitably, low on cyan toner), it usually needs to be
told about what is installed in the house.  Thus, the user quickly delves
into the world of "integrations" — the device drivers of Home Assistant.
</p><p>
For each remotely accessible device in the house, there is, hopefully, at
least one integration available that allows Home Assistant to work with it.
Many integrations are packaged with the system itself, and can be found by
way of a simple search screen in the Home Assistant web interface.  A much
larger set is packaged separately, usually in the <a href="https://www.hacs.xyz/">Home Assistant Community Store</a>, or HACS;
it is fair to say that most users will end up getting at least some
integrations from this source.  Setting up HACS requires a few steps and,
unfortunately, requires the user to have a GitHub account for full
integration.  It <i>is</i> possible to install HACS integrations without
that account, but it is a manual process that loses support for features
like update tracking.
</p><p>
Most integrations, at setup time, will discover any of the appropriate
devices on the network — if those devices support that sort of discovery,
of course.  Often, using an integration will require the credentials to log
into the cloud account provided by the vendor of the devices in question.
When possible, integrations mostly strive to operate entirely locally; some
only use the cloud connection for the initial device discovery.  When there
is no alternative, though, integrations will remain logged into the cloud
account and interact with their devices that way; this mode may or may not
be supported (or condoned) by the vendor.  There are, of course, some
vendors that <a href="https://www.bleepingcomputer.com/news/security/haier-hits-home-assistant-plugin-dev-with-takedown-notice/">are
actively hostile</a> to integration with Home Assistant.
</p><p>
As might be expected, the quality of integrations varies widely.  Most of
the integrations I have tried have worked well enough.  The OpenSprinkler
(<a href="https://lwn.net/Articles/940509/">reviewed here</a> in 2023) integration,
instead, thoroughly corrupted the device configuration, exposing me to the
shame of being seen with a less-than-perfect lawn; it was quickly
removed.  It is an especially nice surprise when a device comes with Home
Assistant support provided by the vendor, but that is still a relatively
rare occurrence.  Home Assistant now is in a position similar to Linux
25&nbsp;years ago; many devices are supported, but often in spite of their
vendor, and one has to choose components carefully.
</p><h4>Security</h4>
<p>
Home Assistant sits at the core of the home network; it has access to
sensors that can reveal a lot about the occupants of the home, and it
collects data in a single location.  An installation will be exposed to the
Internet if its owner needs remote access.  There is clearly potential for
a security disaster here.
</p><p>
The project has <a href="https://www.home-assistant.io/security/">a posted
security policy</a> describing the project's stance; it asks for a 90-day
embargo on the reporting of any security issues.  Authors writing about the
project's security are encouraged to run their work past the project "<q>so
we can ensure that all claims are correct</q>".  The security policy
explicitly excludes reports regarding third-party integrations (the core
project cannot fix those, after all).  The project is also uninterested in
any sort of privilege escalation by users who are logged into Home
Assistant, assuming that anybody who has an account is fully trusted.
</p><p>
The project has only issued <a href="https://github.com/home-assistant/core/security/advisories/GHSA-m3pm-rpgg-5wj6">one
security advisory</a> since the beginning of 2024.  There were several in
2023, mostly as the result of <a href="https://github.blog/security/vulnerability-research/securing-our-home-labs-home-assistant-code-review/">a
security audit</a> performed by GitHub.
</p><p>

There is no overall vetting of third-party integrations, which are, in the
end, just more Python code.  So loading an unknown integration is similar
to importing an unknown module from PyPI; it will probably work, but the
potential for trouble is there.  The project has occasionally <a href="https://www.home-assistant.io/blog/2021/01/23/security-disclosure2/">reported
security problems in third-party integrations</a>, but such reports are
rare.  I am unable to find any reports of actively malicious integrations
in the wild, but one seems destined to appear sooner or later.
</p><h4>Actually doing something with Home Assistant</h4>
<p>
The first step for the owner of a new Home Assistant installation is,
naturally, to seek out integrations for the devices installed in the home.
On successful installation and initialization, an integration will add one
or more "devices" to the system, each of which has some number of "sensors"
for data it reports, and possible "controls" to change its operating state.
A heat-pump head, for example, may have sensors for the current temperature
and humidity, and controls for its operating mode, fan speed, vane
direction, and more.
</p><p>
It is worth noting that the setup of these entities seems a bit
non-deterministic at times.  My solar system has 22&nbsp;panels with
inverters, each of which reports nearly a dozen parameters (voltage,
current, frequency, temperature, etc.).  There is no easy way to determine
which panel is reporting, for example, <tt>sensor_amps_12</tt>, especially
since <tt>sensor_frequency_12</tt> almost certainly corresponds to a
<i>different</i> panel.  My experience is that Home Assistant is a system
for people who are willing to spend a lot of time fiddling around with
things to get them to a working state.  Dealing with these sensors was an
early introduction to that; it took some time to figure out the mapping
between names and rooftop positions, then to rename each sensor to
something more helpful.
</p><p>
The next level of fiddling around is setting up dashboards.  Home Assistant
offers a great deal of flexibility in the information and controls it
provides to the user; it is possible to set up screens focused on, say,
energy production or climate control.  Happily, the days when this
configuration had to be done by writing YAML snippets are mostly in the
past at this point; one occasionally still has to dip into YAML, but it
does not happen often.  The interface is not always intuitive,
but it is fairly slick, interactive, and functional.
</p><p>
Another part of Home Assistant that I have not yet played with much
is automations and scenes.  Automations are simple rule-triggered programs
that make changes to some controls.  They can carry out actions like
"turn on the front light when it gets dark" or "play scary music if
somebody rings the doorbell and nobody is home".  Scenes are sets of canned
device configurations.  One might create a scene called "in-laws visiting"
that plays loud punk music, sets the temperature to just above freezing,
disables all voice control, and tunes all of the light bulbs to 6000K, for
example.
</p><p>
The good news is that, unless the fiddling itself is the point (and it can
be a good one), there comes a time when things just work and the fiddling
can stop.  A well-configured Home Assistant instance provides detailed
information about the state of the home — and control where the devices
allow it — to any web browser that can reach it and log in.  There are
(open-source) apps that bring this support to mobile devices in a way that
is nearly indistinguishable from how the web interface works.
</p><p>
All told, it is clear why Home Assistant has a strong and growing
following.  It is an open platform that brings control to an industry that
is doing its best to keep a firm grasp on our homes and the data they
create.  Home Assistant shows that we can do nicely without all of these
fragile, non-interoperable, rug-pull-susceptible cloud systems.  Just like
Linux proved that we can have control over our computers, Home Assistant
shows that we do not have to surrender control over our homes.
</p><p>
This article has gotten long, and is remarkably short on interesting things
that one can actually <i>do</i> with Home Assistant.  There are some
interesting stories to be told along those lines; they will appear shortly
in <a href="https://lwn.net/Articles/1017945/">the second, concluding part</a> of this series.<br clear="all"></p>
               <br clear="all">
               <hr>
            </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Will AI systems perform poorly due to AI-generated material in training data? (107 pts)]]></title>
            <link>https://cacm.acm.org/news/the-collapse-of-gpt/</link>
            <guid>44010705</guid>
            <pubDate>Fri, 16 May 2025 23:27:24 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://cacm.acm.org/news/the-collapse-of-gpt/">https://cacm.acm.org/news/the-collapse-of-gpt/</a>, See on <a href="https://news.ycombinator.com/item?id=44010705">Hacker News</a></p>
<div id="readability-page-1" class="page"><div lang="en"><section id="sec1"><p id="p-1">Ever since ChatGPT was released to the public in November 2022, people have been using it to generate text, from emails to blog posts to bad poetry, much of which they post online. Since that release, the companies that build the large language models (LLMs) on which such chatbots are based—such as OpenAI’s GPT 3.5, the technology underlying ChatGPT—have also continued to put out newer versions of their models, training them with new text data, some of which they scraped off the Web. That means, inevitably, that some of the training data used to create LLMs did not come from humans, but from the LLMs themselves.</p><p id="p-2">That has led computer scientists to worry about a phenomenon they call model collapse. Basically, model collapse happens when the training data no longer matches real-world data, leading the new LLM to produce gibberish, in a 21st-century version of the classic computer aphorism “garbage in, garbage out.”</p><p id="p-3">LLMs work by learning the statistical distribution of so-called tokens—words or parts of words—within a language by examining billions of sentences garnered from sources including book databases, Wikipedia, and the Common Crawl dataset, a collection of material gathered from the Internet. An LLM, for instance, will figure out how often the word “president” is associated with the word “Obama” versus “Trump” versus “Hair Club for Men.” Then, when prompted by a request, it will produce words that it reasons have the highest probability of meeting that request and of following from previous words. The results bear a credible resemblance to human-written text.</p><p id="p-4">Model collapse is basically a statistical problem, said Sanmi Koyejo, an assistant professor of computer science at Stanford University. When machine-generated text replaces human-generated text, the distribution of tokens no longer matches the natural distribution produced by humans. As a result, the training data for a new round of modeling does not match the real world, and the new model’s output gets worse. “The thing we’re worried about is that the distribution of your data that you end up with, if you’re trying to fit your model, ends up really far from the actual distribution that generated the data,” he said.</p><p id="p-5">The problem arises because whatever text the LLM generates would be, at most, a subsample of the sentences on which it was trained. “Because you generate a finite sample, you have some probability of not sampling them,” said Yarin Gal, an associate professor of machine learning at Oxford University. “Once you don’t sample, then they disappear. They will never appear again. So every time you generate data, you basically start forgetting more and more of the tail events and therefore that leads to the concentration of the higher probability events.” Gal and his colleagues published a study in <i>Nature</i> in July that showed indiscriminate use of what they called ‘recursively generated data’ caused the models to fail.</p><p id="p-6">The problem is not limited to LLMs. Any generative model that is iteratively trained can suffer the same fate if it starts ingesting machine-produced data, Gal says. That includes stable diffusion models that create images, such as Dall-E. The issue also can affect variational autoencoders, which create new data samples by producing variations of their original data. It can apply to Gaussian mixture models, a form of unsupervised machine learning that sorts subpopulations of data into clusters; they are used to analyze customer preferences, predict stock prices, and analyze gene expression.</p><p id="p-7">Collapse is not a danger for models that incorporate synthetic data but only do so once, such as neural networks used to identify cancer in medical images, where synthetic data was used to augment rare or expensive real data. “The main distinction is that model collapse happens when you have multiple steps, where each step depends on the output from the previous step,” Gal said.</p><p id="p-8">The theory that replacing training data with synthetic data will quickly lead to the demise of LLMs is sound, Koyejo said. In practice, however, not all human data gets replaced immediately. Instead, when the generated text is scraped from the Internet, it gets mixed in with human text. “You create synthetic data, you add that to real data, so you now have more data, which is real data plus synthetic data,” he said. What is actually happening, he said, is not data replacement, but data accumulation. That slows the degradation of the dataset.</p><p id="p-9">Simply accumulating data may stop model collapse but can cause other problems if done without thought, said Yunzhen Feng, a Ph.D. student at the Center for Data Science at New York University. As a rule, the performance of neural networks improves as their size increases. Naively mixing real and synthetic data together, however, can slow that improvement. “You can still obtain similar performance, but you need much more data. That means you’re using much more compute and much more money to achieve that,” he said.</p><p id="p-10">One challenge is that there is no easy way to tell whether text found on the Internet is synthetic or human-generated. Though there have been attempts to automatically identify text from LLMs, none have been entirely successful. Research into this problem is ongoing, Gal said.</p></section><section id="sec2"><h2>Solving with curation</h2><p id="p-11">There are ways, however, to make the addition of synthetic data less of a problem.</p><p id="p-12">One approach is to curate the synthetic data to make sure it is of good quality. Some curation happens naturally, Gal said; people do not post everything their chatbot creates to the Internet, weeding out the material that contains false information or simply does not make sense, so that improves the training set.</p><p id="p-13">Curation can also be a deliberate process to make sure high-quality data goes into a training set. Feng, for instance, has experimented with asking the LLM to assess the quality of its own output. LLMs naturally select the words they think have the highest probability of fitting into a context. In doing so, they internally generate a score rating how confident they are that they are pairing the best words together. That same mechanism can be used to assess already generated text to rate its quality, with low-scoring results removed or the highest-scoring result of several attempts selected as the best. The idea is similar to a method used to fine-tune LLMs called reinforcement learning from human feedback (RLHF), in which people provide examples of good results, thereby pushing the models toward producing similar results. In this case, though, the LLM is generating its own feedback.</p><p id="p-14">How well that works varies by case, Feng said. The feedback can be improved by having other LLMs assess the same text and combining the results from different models. Including human assessments also improves the outcomes, as does applying some pre-written rules about what the output should look like. Eliminating lower-quality results from the synthetic data makes the generated data more closely resemble original data, he said. “It’s like you have a distribution of the synthetic data, you have a distribution of the real data, and you want to close the gap between them as much as possible,” he said.</p><p id="p-15">Improving the quality of synthetic data could also help with another challenge LLMs are facing as they try to improve: a dearth of new data on which to train. Scientists from Epoch AI, a research institute that focuses on trends in AI, have predicted the world will run out of new text to train on sometime between 2026 and 2032. With no new data on which to train future generations of LLMs, progress could stagnate. “The interesting question is, can synthetic data lead to not just stagnation but actual improvement in the model?” asked Pablo Villalobos, a staff researcher at Epoch.</p><p id="p-16">With curation of high-quality synthetic data, he said, the question becomes “whether this can be done iteratively so that each model generates better data that is used to train another model in basically the opposite of model collapse, in some virtuous circle.” He is not yet sure whether such improvement is possible, but sees some signs it could be.</p><p id="p-17">Other issues arise from training new models on generated data that do not quite reach the level of model collapse. For instance, Koyejo said, synthetic data could increase the likelihood that LLMs will discriminate against people in minority groups. Because any minority is by definition a smaller part of the data distribution, losing the tails of the distribution could make minorities disappear entirely. “Data tends to anchor on majority subgroups,” he said. “It tends to be good at capturing the most popular themes and less good at capturing tails. So less represented demographics can get erased in various ways.”</p><p id="p-18">While such erasure is something that could happen, he added, the issue has not been well studied. His colleague Diyi Yang, an assistant professor in the natural language processing group at Stanford, said there has been very little research into the question of how model collapse affects diversity issues. “Part of the reason is that, if you think about any existing big models, a lot of the training dynamics or checkpoints of those models actually are not really transparent or publicly available,” she said.</p><p id="p-19">In the end, Gal argued, model collapse is an important consideration, but not the matter of imminent disaster that some news coverage has made it out to be. “It’s a matter for the tech companies who build these models to be aware of how the models are being used and how the models are being trained, in order to avoid training on synthetic data that they themselves generated.”</p><h2 id="FurtherReading">Further Reading</h2><ul id="ref-list1"><li><p><span data-jats-publication-type="other"><em>Shumailov, I., Shumaylov, Z., Zhao, Y., Papernot, N., Anderson, R., and Gal, Y.</em> <br><strong>AI models collapse when trained on recursively generated data, <em><span>Nature</span></em>, 2024, DOI: 10.1038/s41586-024-07566-y </strong><a href="https://www.nature.com/articles/s41586-024-07566-y" data-jats-ext-link-type="uri"><strong>https://www.nature.com/articles/s41586-024-07566-y</strong></a></span></p></li><li><p><span data-jats-publication-type="other"><em>Feng, Y., Dohmatob, E., Yang, P., Charton, and F. Kempe, J.</em> <br><strong>Beyond Model Collapse: Scaling Up with Synthesized Data Requires Reinforcement, <em>arXiv</em>, 2024, </strong><a href="https://doi.org/10.48550/arXiv.2406.07515" data-jats-ext-link-type="uri"><strong>https://doi.org/10.48550/arXiv.2406.07515</strong></a></span></p></li><li><p><span data-jats-publication-type="other"><em>Gerstgrasser, M., Schaeffer, R., Dey, A., et al.</em> <br><strong>Is Model Collapse Inevitable? Breaking the Curse of Recursion by Accumulating Real and Synthetic Data, <em>arXiv</em>, 2024, </strong><a href="https://doi.org/10.48550/arXiv.2404.01413" data-jats-ext-link-type="uri"><strong>https://doi.org/10.48550/arXiv.2404.01413</strong></a></span></p></li><li><p><span data-jats-publication-type="other"><em>Villalobos, P., Ho, A., Sevilla, J., Besiroglu, T., Heim, L., and Hobbhahn, M.</em> <br><strong>Will we run out of data? Limits of LLM scaling based on human-generated data, <em>arXiv</em>, 2022, </strong><a href="https://doi.org/10.48550/arXiv.2211.04325" data-jats-ext-link-type="uri"><strong>https://doi.org/10.48550/arXiv.2211.04325</strong></a></span></p></li><li></li></ul></section></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Moody’s strips U.S. of triple-A credit rating (266 pts)]]></title>
            <link>https://www.ft.com/content/e456ea34-c6ad-43fe-abe9-d4ce781c07b4</link>
            <guid>44009999</guid>
            <pubDate>Fri, 16 May 2025 21:39:15 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.ft.com/content/e456ea34-c6ad-43fe-abe9-d4ce781c07b4">https://www.ft.com/content/e456ea34-c6ad-43fe-abe9-d4ce781c07b4</a>, See on <a href="https://news.ycombinator.com/item?id=44009999">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p><a data-trackable="a11y-skip-to-help" href="https://www.ft.com/accessibility">Accessibility help</a><a data-trackable="a11y-skip-to-navigation" href="#site-navigation">Skip to navigation</a><a data-trackable="a11y-skip-to-content" href="#site-content">Skip to content</a><a data-trackable="a11y-skip-to-footer" href="#site-footer">Skip to footer</a></p><div id="barrier-page"><div data-component="electionsHeader" data-component-unique-name="electionsHeader"><h2>US Politics 2025</h2></div><div data-component="topicHeroOffer" data-component-unique-name="topicsHeroOffer"><div><div data-o-grid-colspan="12 XL6"><p><span></span><span></span><span></span><span>Register to unlock this article</span><span></span></p></div><div data-o-grid-colspan="12 XL5"><p><h2><span><span>Continue reading and get the indispensable White House Watch newsletter for free.</span></span></h2></p></div></div><p><img src="https://www.ft.com/__origami/service/image/v2/images/raw/https://barrier-page-components.s3.eu-west-1.amazonaws.com/assets/images/us_election_2024/hero_banner_us_election_white_house_watch.png?source=next-barrier-page" alt="WhiteHouseWatch"></p></div><div data-component="electionsFeatures" data-component-unique-name="electionsFeatures"><h2>We’ve got you covered</h2><div><div><p><img alt="" src="https://www.ft.com/__origami/service/image/v2/images/raw/https://barrier-page-components.s3.eu-west-1.amazonaws.com/assets/images/us_election_2024/feature_countdown.png?source=next-barrier-page"></p><h3>White House Watch newsletter</h3><p>Sign up for your free, indispensable guide to what Trump’s second term means for Washington, business and the world.</p></div><div><p><img alt="" src="https://www.ft.com/__origami/service/image/v2/images/raw/https://barrier-page-components.s3.eu-west-1.amazonaws.com/assets/images/us_election_2024/feature_hub_2024.png?source=next-barrier-page"></p><h3>Trump tracker: US tariffs</h3><p>As the president threatens a trade war, follow the latest on tariffs and executive orders</p></div><div><p><img alt="" src="https://www.ft.com/__origami/service/image/v2/images/raw/https://barrier-page-components.s3.eu-west-1.amazonaws.com/assets/images/us_election_2024/feature_expert_opinion_analysis.png?source=next-barrier-page"></p><h3>US politics &amp; policy</h3><p>Stay on top of the latest events in US politics with the FT’s trusted and impartial coverage.</p></div><div><p><img alt="" src="https://www.ft.com/__origami/service/image/v2/images/raw/https://barrier-page-components.s3.eu-west-1.amazonaws.com/assets/images/us_election_2024/feature_democracy_2024.png?source=next-barrier-page"></p><h3>Expert Opinion &amp; Analysis</h3><p>Insight and analysis on US politics from commentators such as Ed Luce and James Politi</p></div></div><div><div><p><img alt="" src="https://www.ft.com/__origami/service/image/v2/images/raw/https://barrier-page-components.s3.eu-west-1.amazonaws.com/assets/images/us_election_2024/feature_countdown.png?source=next-barrier-page"></p><h3>White House Watch newsletter</h3><p>Sign up for your free, indispensable guide to what Trump’s second term means for Washington, business and the world.</p></div><div><p><img alt="" src="https://www.ft.com/__origami/service/image/v2/images/raw/https://barrier-page-components.s3.eu-west-1.amazonaws.com/assets/images/us_election_2024/feature_hub_2024.png?source=next-barrier-page"></p><h3>Trump tracker: US tariffs</h3><p>As the president threatens a trade war, follow the latest on tariffs and executive orders</p></div><div><p><img alt="" src="https://www.ft.com/__origami/service/image/v2/images/raw/https://barrier-page-components.s3.eu-west-1.amazonaws.com/assets/images/us_election_2024/feature_expert_opinion_analysis.png?source=next-barrier-page"></p><h3>US politics &amp; policy</h3><p>Stay on top of the latest events in US politics with the FT’s trusted and impartial coverage.</p></div><div><p><img alt="" src="https://www.ft.com/__origami/service/image/v2/images/raw/https://barrier-page-components.s3.eu-west-1.amazonaws.com/assets/images/us_election_2024/feature_democracy_2024.png?source=next-barrier-page"></p><h3>Expert Opinion &amp; Analysis</h3><p>Insight and analysis on US politics from commentators such as Ed Luce and James Politi</p></div></div></div><div id="recommendedOffers-recommendedOffers" data-component="recommendedOffers" data-component-unique-name="recommendedOffers"><p><h2 data-o-grid-colspan="12">Explore more offers.</h2></p><div data-o-grid-colspan="12"><div data-o-grid-colspan="12"><div><p><img src="https://www.ft.com/__origami/service/image/v2/images/raw/https://barrier-page-components.s3.eu-west-1.amazonaws.com/assets/icons/primary_product_icon_digital_edition.svg?source=next-barrier-page&amp;format=svg" alt=""></p><p><h3>FT Digital Edition</h3></p></div><p><span><span>€42.99</span><span> per 3 months</span></span></p><p><span><span>The new FT Digital Edition: today’s FT, cover to cover on any device. This subscription does not include access to ft.com or the FT App.</span></span></p></div><div data-o-grid-colspan="12"><div><p><img src="https://www.ft.com/__origami/service/image/v2/images/raw/https://barrier-page-components.s3.eu-west-1.amazonaws.com/assets/icons/primary_product_icon_standard.svg?source=next-barrier-page&amp;format=svg" alt=""></p><p><h3>Standard Digital</h3></p></div><p><span><span>€45</span><span> per month</span></span></p><p><span><span>Essential digital access to quality FT journalism on any device. Pay a year upfront and save 20%.</span></span></p></div><div data-o-grid-colspan="12"><div><p><img src="https://www.ft.com/__origami/service/image/v2/images/raw/https://barrier-page-components.s3.eu-west-1.amazonaws.com/assets/icons/primary_product_icon_premium.svg?source=next-barrier-page&amp;format=svg" alt=""></p><p><h3>Premium Digital</h3></p></div><p><span><span>€69</span><span> per month</span></span></p><p><span><span>Complete digital access to quality FT journalism with expert analysis from industry leaders. Pay a year upfront and save 20%.</span></span></p></div></div></div><div data-component="subscriptionOptions" data-component-unique-name="subscriptionsOptions" data-o3-theme="inverse"><h2>Explore our full range of subscriptions.</h2><div><div><div><h3>For individuals</h3></div><p>Discover all the plans currently available in your country</p></div><div><div><h3> For multiple readers</h3></div><p>Digital access for organisations. Includes exclusive features and content.</p></div></div><div><p>Check whether you already have access via your <span><a data-trackable="edu-finder" href="https://find-your-subscription.ft.com/?segmentId=a0e9a794-4c6d-bb35-e4dc-8bd409e0f54f&amp;ft-content-uuid=e456ea34-c6ad-43fe-abe9-d4ce781c07b4">university</a></span> or <span><a data-trackable="licence-finder" href="https://subs.enterprise.ft.com/en-gb/licence-finder/?segmentId=9fb23d7d-afe4-12f3-3eaa-ff7a41e9d073&amp;ft-content-uuid=e456ea34-c6ad-43fe-abe9-d4ce781c07b4">organisation.</a></span></p></div></div><div data-component="whyFT" data-component-unique-name="whyFT" data-o3-theme="inverse"><div><h2>Why the FT?</h2><p>See why over a million readers pay to read the Financial Times.</p></div><p><a href="https://subs.ft.com/whytheft?ft-content-uuid=e456ea34-c6ad-43fe-abe9-d4ce781c07b4">Find out why</a></p></div></div></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Getting AI to write good SQL (459 pts)]]></title>
            <link>https://cloud.google.com/blog/products/databases/techniques-for-improving-text-to-sql</link>
            <guid>44009848</guid>
            <pubDate>Fri, 16 May 2025 21:10:12 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://cloud.google.com/blog/products/databases/techniques-for-improving-text-to-sql">https://cloud.google.com/blog/products/databases/techniques-for-improving-text-to-sql</a>, See on <a href="https://news.ycombinator.com/item?id=44009848">Hacker News</a></p>
<div id="readability-page-1" class="page"><div jsname="tx2NYc"><div jsaction="rcuQ6b:npT2md" jscontroller="M0Q3Qb"><p><span>Organizations depend on fast and accurate data-driven insights to make decisions, and SQL is at the core of how they access that data. With Gemini, Google can generate SQL directly from natural language — a.k.a. text-to-SQL. This capability increases developer and analysts’ productivity and empowers non-technical users to interact directly with the data they need.</span></p>
<p><span>Today, you can find text-to-SQL capabilities in many Google Cloud products:</span></p>
<ul>
<li>
<p><strong>BigQuery Studio</strong><span> in the </span><a href="https://cloud.google.com/bigquery/docs/write-sql-gemini#generate_sql_from_a_comment"><span>SQL Editor</span></a><span> and </span><a href="https://cloud.google.com/bigquery/docs/write-sql-gemini#use_the_sql_generation_tool"><span>SQL Generation tool</span></a><span>, and within the </span><strong>Data Canvas</strong><span> </span><a href="https://cloud.google.com/blog/products/data-analytics/using-bigquery-data-canvas-a-deep-dive?e=48754805#:~:text=powered%20by%20Gemini-,2.%20Generate%20SQL,-You%20can%20also"><span>SQL node</span></a></p>
</li>
<li>
<p><span>"Help me code" functionality in </span><a href="https://cloud.google.com/sql/docs/mysql/write-sql-gemini"><strong>Cloud SQL Studio</strong></a><strong> (</strong><span>Postgres, MySQL and SQLServer), </span><strong>AlloyDB Studio</strong><span> and </span><strong>Cloud Spanner Studio</strong></p>
</li>
<li>
<p><strong>AlloyDB AI</strong><span> with its direct natural language interface to the database, currently available as a public preview</span></p>
</li>
<li>
<p><span>Through </span><strong>Vertex AI</strong><span>, which lets you access the Gemini models that are the basis for these products directly</span></p>
</li>
</ul>
<p><span>Recently, powerful large language models (LLMs) like Gemini, with their abilities to reason and synthesize, have driven remarkable advancements in the field of text-to-SQL. In this blog post, the first entry in a series, we explore the technical internals of Google Cloud's text-to-SQL agents. We will cover state-of-the-art approaches to context building and table retrieval, how to do effective evaluation of text-to-SQL quality with LLM-as-a-judge techniques, the best approaches to LLM prompting and post-processing, and how we approach techniques that allows the system to offer virtually certified correct answers.</span></p></div><section><figure><section jscontroller="SCGBie" jsaction="rcuQ6b:npT2md"><img src="https://storage.googleapis.com/gweb-cloudblog-publish/images/1_Text-to-SQL_at_Google_Cloud.max-1800x1800.jpg" alt="https://storage.googleapis.com/gweb-cloudblog-publish/images/1_Text-to-SQL_at_Google_Cloud.max-1800x1800.jpg" jsname="P3Vluc" jsaction="click:HTIlC" loading="lazy"></section></figure><div><p>The ‘Help me code’ feature in Cloud SQL Studio generates SQL from a text prompt</p></div></section><div jsaction="rcuQ6b:npT2md" jscontroller="M0Q3Qb"><h3><strong>The challenges of text-to-SQL technology</strong></h3>
<p><span>Current state-of-the-art LLMs like Gemini 2.5 have reasoning capabilities that make them good at translating complex questions posed in natural language to functioning SQL, complete with joins, filters, aggregations and other difficult concepts.</span></p>
<p><span>To see this in action you can do a simple test in </span><a href="https://cloud.google.com/generative-ai-studio"><span>Vertex AI Studio</span></a><span>. Given the prompt </span><span>"I have a database schema that contains products and orders. Write a SQL query that shows the number of orders for shoes"</span><span>, Gemini produces SQL for a hypothetical schema:</span></p></div><div jsaction="rcuQ6b:npT2md" jscontroller="M0Q3Qb"><p><span>Great, this is a good looking query. But what happens when you move beyond this trivial example, and use Gemini for text-to-SQL against a real world database and on real-world user questions? It turns out that the problem is more difficult. The model needs to be complemented with methods to:&nbsp;</span></p>
<ol>
<li>
<p><span>provide business-specific context</span></p>
</li>
<li>
<p><span>understand user intent</span></p>
</li>
<li>
<p><span>manage differences in SQL dialects</span></p>
</li>
</ol>
<p><span>Let’s take a look at each of these challenges.&nbsp;</span></p>
<p><strong>Problem #1: Provide business-specific context</strong></p>
<p><span>Just like data analysts or engineers, LLMs need significant amounts of knowledge or "context" to generate accurate SQL. The context can be both explicit (what does the schema look like, what are the relevant columns, and what does the data itself look like?) or more implicit (what is the precise semantic meaning of a piece of data? what does it mean for the specific business case?).</span></p>
<p><span>Specialized model training, or fine tuning, is typically not a scalable solution to this problem. Training on the shape of every database or dataset, and keeping up with schema or data changes, is both difficult and cost-prohibitive. Business knowledge and semantics are often not well documented in the first place, and difficult to turn into training data.</span></p>
<p><span>For example, even the best DBA in the world would not be able to write an accurate query to track shoe sales if they didn't know that </span><code>cat_id2 = 'Footwear'</code><span> in a </span><code>pcat_extension</code><span> table means that the product in question is a kind of shoe. The same is true for&nbsp; LLMs.</span></p>
<p><strong>Problem #2: Understanding user intent</strong></p>
<p><span>Natural language is less precise than SQL. An engineer or analyst faced with an ambiguous question can detect that they need more information and go back and ask the right follow-up questions. An LLM, on the other hand, tends to try to give you an answer, and when the question is ambiguous, can be prone to hallucinating.</span></p>
<p><span>Example: Take a question like "What are the best-selling shoes?" Here, one obvious point of ambiguity is what "best selling" actually means in the context of the business or application — the most ordered shoes? The shoe brand that brought in the most money? Further, should the SQL count returned orders? And how many kinds of shoes do you want to see in the report? etc.&nbsp;</span></p>
<p><span>Further, different users need different kinds of answers. If the user is a technical analyst or a developer asking a vague question, giving them a reasonable, but perhaps not 100% correct SQL query is a good starting point. On the other hand, if the user is less technical and does not understand SQL, providing precise, correct SQL is more important. Being able to reply with follow-up questions to disambiguate, explaining the reasoning that went into an answer, and guiding the user to what they are looking for is key.</span></p>
<p><strong>Problem #3: Limits of LLM generation</strong></p>
<p><span>Out of the box, LLMs are particularly good at tasks like creative writing, summarizing or extracting information from documents. But some models can struggle with following precise instructions and getting details exactly right, particularly when it comes to more obscure SQL features. To be able to produce correct SQL, the LLM needs to adhere closely to what can often turn into complex specifications.</span></p>
<p><span>Example: Consider the differences between SQL dialects, which are more subtle than differences between programming languages like Python and Java. As a simple example, if you're using BigQuery SQL, the correct function for extracting a month from a timestamp column is </span><code>EXTRACT(MONTH FROM timestamp_column)</code><span>. But if you are using MySQL, you use </span><code>MONTH(timestamp_column)</code><span>.</span></p>
<h3><strong>Text-to-SQL techniques</strong></h3>
<p><span>At Google Cloud, we’re constantly evolving our text-to-SQL agents to improve their quality. To address the problems listed above, we apply a number of techniques.</span></p>
<div><table><colgroup><col><col></colgroup>
<tbody>
<tr>
<td>
<p><strong>Problem</strong></p>
</td>
<td>
<p><strong>Solutions</strong></p>
</td>
</tr>
<tr>
<td>
<p><span>Understanding schema, data and business concepts</span></p>
</td>
<td>
<ul>
<li>
<p><strong>Intelligent retrieval</strong><span> and ranking of datasets, tables and columns, based on semantic similarity.</span></p>
</li>
<li>
<p><strong>In-context-learning</strong><span> with business specific examples</span></p>
</li>
<li>
<p><span>Data linking and sampling</span></p>
</li>
<li>
<p><span>Semantic layer over raw data. This provides a bridge between complex data structures and the everyday language used by the customer</span></p>
</li>
<li>
<p><span>Usage pattern analysis and query history</span></p>
</li>
</ul>
</td>
</tr>
<tr>
<td>
<p><span>Understanding user intent</span></p>
</td>
<td>
<p><strong>Disambiguation using LLMs</strong></p>
<ul>
<ul>
<li>
<p><span>Entity resolution</span></p>
</li>
</ul>
</ul>
<p><strong>SQL-aware foundation models</strong></p>
</td>
</tr>
<tr>
<td>
<p><span>Limits of LLM generation</span></p>
</td>
<td>
<p><strong>Self-consistency</strong></p>
<p><strong>Validation and rewriting</strong></p>
<ul>
<li>
<p><span>Strong foundation models</span></p>
</li>
<li>
<p><span>In-context-learning with dialect specific examples</span></p>
</li>
<li>
<p><span>Model finetuning</span></p>
</li>
</ul>
</td>
</tr>
</tbody>
</table></div></div><section><figure><section jscontroller="SCGBie" jsaction="rcuQ6b:npT2md"><img src="https://storage.googleapis.com/gweb-cloudblog-publish/images/2_Text-to-SQL_at_Google_Cloud.max-2200x2200.jpg" alt="https://storage.googleapis.com/gweb-cloudblog-publish/images/2_Text-to-SQL_at_Google_Cloud.max-2200x2200.jpg" jsname="P3Vluc" jsaction="click:HTIlC" loading="lazy"></section></figure><div><p>The text-to-SQL architecture</p></div></section><div jsaction="rcuQ6b:npT2md" jscontroller="M0Q3Qb"><p><span>Let’s take a closer look at some of these techniques.</span></p>
<p><strong>SQL-aware models<br></strong><span>Strong LLMs are the foundation of text-to-SQL solutions, and the Gemini family of models has a proven track record of high-quality code and SQL generation. Depending on the particular SQL generation task, we mix and match model versions, including some cases where we employ customized fine-tuning, for example to ensure that models provide sufficiently good SQL for certain dialects.</span></p>
<p><strong>Disambiguation using LLMs<br></strong><span>Disambiguation involves getting the system to respond with a clarifying question when faced with a question that is not clear enough (in the example above of "What is the best selling shoe?" should lead to a follow-up question like "Would you like to see the shoes ordered by order quantity or revenue?" from the text-to-SQL agent). Here we typically orchestrate LLM calls to first try to identify if a question can actually be answered given the available schema and data, and if not, to generate the necessary follow-up questions to clarify the user's intent.</span></p>
<p><strong>Retrieval and in-context-learning<br></strong><span>As mentioned above, providing models with the context they need to generate SQL is critical. We use a variety of indexing and retrieval techniques — first to identify relevant datasets, tables and columns, typically using vector search for multi-stage semantic matching, then to load additional useful context. Depending on the product, this may include things like user-provided schema annotations, examples of similar SQL or how to apply specific business rules, or samples of recent queries that a user has run against the same datasets. All of this data is organized into prompts then passed to the model. Gemini's support for long context windows unlocks new capabilities here by allowing the model to handle large schemas and other contextual information.</span></p>
<p><strong>Validation and reprompting<br></strong><span>Even with a high-quality model, there is still some level of non-determinism or unpredictability involved in LLM-driven SQL generation. To address this we have found that non-AI approaches like query parsing or doing a dry run of the generated SQL complements model-based workflows well. We can get a clear, deterministic signal if the LLM has missed something crucial, which we then pass back to the model for a second pass. When provided an example of a mistake and some guidance, models can typically address what they got wrong.</span></p>
<p><strong>Self-consistency<br></strong><span>The idea of self-consistency is to not depend on a single round of generation, but to generate multiple queries for the same user question, potentially using different prompting techniques or model variants, and picking the best one from all candidates. If several models agree that one answer looks particularly good, there is a greater chance that the final SQL query will be accurate and matches what the user is looking for.</span></p>
<h3><strong>Evaluation and measuring improvements</strong></h3>
<p><span>Improving AI-driven capabilities depends on robust evaluation. The text-to-SQL benchmarks developed in the academic community, like the popular </span><a href="https://bird-bench.github.io/" rel="noopener" target="_blank"><span>BIRD-bench</span></a><span>, have been a very useful baseline to understand model and end-to-end system performance. However, these benchmarks are often lacking when it comes to representing broad real-world schemas and workloads. To address this we have developed our own suite of synthetic benchmarks that augment the baseline in many ways.</span></p>
<p><strong>Coverage:</strong><span> We make sure to have benchmarks that cover a broad list of SQL engines and products, both dialects and engine-specific features. This includes not only queries, but also DDL, DML and other administrative needs, and questions that are representative for common usage patterns, including more complex queries and schemas.</span></p>
<p><strong>Metrics:</strong><span> We combine user metrics and offline eval metrics, and employ both human and automated evaluation, particularly using LLM-as-a-judge techniques, which reduce cost but still allow us to understand performance on ambiguous and unclear tasks.</span></p>
<p><strong>Continuous evals:</strong><span> Our engineering and research teams use evals to quickly be able to test out new models, new prompting techniques and other improvements. It can give us signals quickly to tell if an approach is showing promise and is worth pursuing.</span></p>
<p><span>Taken together, using these techniques are driving the remarkable improvements in text-to-SQL that we are witnessing in our labs, as well as in customers’ environments. As you get ready to incorporate text-to-SQL in your own environments, stay tuned for more deep dives into our text-to-SQL solutions. Try Gemini text-to-SQL in </span><a href="https://cloud.google.com/bigquery/docs/write-sql-gemini#use_the_sql_generation_tool"><span>BigQuery Studio</span></a><span>, </span><a href="https://cloud.google.com/sql/docs/mysql/write-sql-gemini"><span>CloudSQL, AlloyDB and Spanner Studio</span></a><span>, and in </span><a href="https://cloud.google.com/blog/products/databases/alloydb-ai-drives-innovation-from-the-database"><span>AlloyDB AI</span></a><span> today.</span></p></div><section><span>Posted in</span><ul><li><a href="https://cloud.google.com/blog/products/databases" track-metadata-position="body" track-metadata-eventdetail="cloud.google.com/blog/products/databases" track-metadata-module="tag list" track-metadata-module_headline="posted in">Databases</a></li><li><a href="https://cloud.google.com/blog/products/ai-machine-learning" track-metadata-position="body" track-metadata-eventdetail="cloud.google.com/blog/products/ai-machine-learning" track-metadata-module="tag list" track-metadata-module_headline="posted in">AI &amp; Machine Learning</a></li></ul></section></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[ClojureScript 1.12.42 (190 pts)]]></title>
            <link>https://clojurescript.org/news/2025-05-16-release</link>
            <guid>44009464</guid>
            <pubDate>Fri, 16 May 2025 20:20:20 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://clojurescript.org/news/2025-05-16-release">https://clojurescript.org/news/2025-05-16-release</a>, See on <a href="https://news.ycombinator.com/item?id=44009464">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
      
      <p><em>16 May 2025</em><br>
         <em>ClojureScript Team</em></p>

      <div id="preamble">
<p>We’re happy to announce a new release of ClojureScript. If you’re an existing
user of ClojureScript please read over the following release notes carefully.</p>
<p>This release features two significant dependency changes. First, Google Closure
Compiler has been updated to <code>v20250402</code>. This change makes Java 21 a
requirement for ClojureScript. The other significant change is that this release
now depends on the Clojure fork of Google Closure Library. Please read on for
more details about these changes.</p>
<p>For a complete list of fixes, changes, and enhancements to
ClojureScript see
<a href="https://github.com/clojure/clojurescript/blob/master/changes.md#1.12.42">here</a></p>
</div>
<div>
<h2 id="_google_closure_compiler_java_21"><a href="#_google_closure_compiler_java_21"></a>Google Closure Compiler &amp; Java 21</h2>
<div>
<p>Last year we noted that updating Google Closure Compiler would mean losing Java
8 support. Google Closure now requires Java 21. From our perspective this change
doesn’t seem strictly necessary, but Google is a large organization and this
change is likely to due to internal requirements which are hard to influence from
the outside. The general enthusiasm in the Clojure community around adopting more
recent Java releases hopefully softens the overall impact of this change.</p>
<p>So far, the burden of staying current with Google Closure has been manageable.
If for some reason that calculus changes, we could adopt the strategy we have taken
with Google Closure Library.</p>
</div>
</div>
<div>
<h2 id="_clojures_fork_of_google_closure_library"><a href="#_clojures_fork_of_google_closure_library"></a>Clojure’s Fork of Google Closure Library</h2>
<div>
<p>The incredible stability of Google Closure Library started declining around
2019. Google was both trying many things with respect to their internal
JavaScript strategy as well as becoming less concerned about the impact on outside
consumers. Finally, Google stopped contributing to Google Closure Library
last August.</p>
<p>We have forked Google Closure Library (GCL) and taken up maintenance. We backed out a
few years of needless breaking changes and aligned the codebase with the latest
Google Closure Compiler release.</p>
<p>One of the biggest benefits of GCL is that it makes ClojureScript a complete
solution for a variety of JavaScript contexts, not limited to the browser.
Taking on additional dependencies always comes with a cost. One of
ClojureScript’s original value propositions was a rock solid set of readily
available JavaScript tools as dependable as <code>clojure.core</code>.</p>
<p>We are working on restoring that original stability. With this release, you’ll
find that quite a few old ClojureScript libraries work again today as well
as they did <strong>14 years</strong> ago.</p>
<p>ClojureScript is not and never was only just for rich web applications. Even in the
post React-world, a large portion of the web is (sensibly) still using jQuery. If you need
robust DOM manipulation, internationalization, date/time handling, color
value manipulation, mathematics, programmatic animation, browser history management,
accessibility support, graphics, and much more, all without committing to a framework
and without bloating your final JavaScript artifact - ClojureScript is a one
stop shop.</p>
<p>Give it a try!</p>
</div>
</div>
    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Show HN: KVSplit – Run 2-3x longer contexts on Apple Silicon (264 pts)]]></title>
            <link>https://github.com/dipampaul17/KVSplit</link>
            <guid>44009321</guid>
            <pubDate>Fri, 16 May 2025 20:04:58 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/dipampaul17/KVSplit">https://github.com/dipampaul17/KVSplit</a>, See on <a href="https://news.ycombinator.com/item?id=44009321">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><div dir="auto">
<p dir="auto"><h2 tabindex="-1" dir="auto">🚀 KVSplit</h2><a id="user-content--kvsplit" aria-label="Permalink: 🚀 KVSplit" href="#-kvsplit"></a></p>
<p dir="auto"><strong>Differentiated KV Cache Quantization for Apple Silicon</strong></p>
<p dir="auto"><a href="https://github.com/dipampaul17/KVSplit/stargazers"><img src="https://camo.githubusercontent.com/e0303260b83f266fb26524b0c11e415c6878324211d0c8edeae0fc1c0b3c9b2a/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f646970616d7061756c31372f4b5653706c69743f7374796c653d666f722d7468652d6261646765266c6f676f3d676974687562" alt="GitHub Stars" data-canonical-src="https://img.shields.io/github/stars/dipampaul17/KVSplit?style=for-the-badge&amp;logo=github"></a>
<a href="https://github.com/dipampaul17/KVSplit/blob/main/LICENSE"><img src="https://camo.githubusercontent.com/e16a8865515749158a69c1e57a8fb9df3373386b8715b5d087517c0d3d887845/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6c6963656e73652f646970616d7061756c31372f4b5653706c69743f7374796c653d666f722d7468652d6261646765" alt="License" data-canonical-src="https://img.shields.io/github/license/dipampaul17/KVSplit?style=for-the-badge"></a>
<a href="https://github.com/dipampaul17/KVSplit/blob/main"><img src="https://camo.githubusercontent.com/8b0dd35d753ba4b14face3f93eafee56c2a5d58f05ee9eae4c8d00e55ca22e02/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f506c6174666f726d2d4170706c6525323053696c69636f6e2d626c61636b3f7374796c653d666f722d7468652d6261646765266c6f676f3d6170706c65" alt="Platform" data-canonical-src="https://img.shields.io/badge/Platform-Apple%20Silicon-black?style=for-the-badge&amp;logo=apple"></a></p>
<p><a target="_blank" rel="noopener noreferrer" href="https://github.com/dipampaul17/KVSplit/blob/main/plots/kv_cache_memory_usage.png"><img src="https://github.com/dipampaul17/KVSplit/raw/main/plots/kv_cache_memory_usage.png" alt="KV Cache Memory Usage" width="70%"></a>
</p></div>
<p dir="auto"><h2 tabindex="-1" dir="auto">📌 Overview</h2><a id="user-content--overview" aria-label="Permalink: 📌 Overview" href="#-overview"></a></p>
<p dir="auto">Run <strong>larger context windows</strong> and <strong>heavier LLMs</strong> on your Mac by applying different quantization precision to keys vs values in the attention mechanism's KV cache. KVSplit enables you to:</p>
<ul dir="auto">
<li><strong>Reduce memory usage by up to 72%</strong> with minimal quality loss</li>
<li><strong>Run 2-3x longer contexts</strong> in the same memory budget</li>
<li><strong>Maintain or improve inference speed</strong> compared to FP16</li>
<li><strong>Optimize for Apple Silicon</strong> with full Metal support</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Key Findings</h2><a id="user-content-key-findings" aria-label="Permalink: Key Findings" href="#key-findings"></a></p>
<markdown-accessiblity-table><table>
<thead>
<tr>
<th>Configuration</th>
<th>VRAM @ 8K tokens</th>
<th>Tokens/sec</th>
<th>Perplexity Change</th>
</tr>
</thead>
<tbody>
<tr>
<td>FP16 (base)</td>
<td>176.00 MB (100%)</td>
<td>54,360</td>
<td>--</td>
</tr>
<tr>
<td>K8V8 (8-bit)</td>
<td>93.50 MB (47%)</td>
<td>51,503</td>
<td>+0.03%</td>
</tr>
<tr>
<td><strong>K8V4</strong></td>
<td><strong>71.50 MB (41%)</strong></td>
<td><strong>57,438</strong></td>
<td><strong>+0.86%</strong></td>
</tr>
<tr>
<td>K4V8</td>
<td>71.50 MB (41%)</td>
<td>58,690</td>
<td>+6.06%</td>
</tr>
<tr>
<td>K4V4 (4-bit)</td>
<td>49.50 MB (28%)</td>
<td>55,193</td>
<td>+6.15%</td>
</tr>
</tbody>
</table></markdown-accessiblity-table>
<p dir="auto"><h3 tabindex="-1" dir="auto">Memory Savings by Sequence Length</h3><a id="user-content-memory-savings-by-sequence-length" aria-label="Permalink: Memory Savings by Sequence Length" href="#memory-savings-by-sequence-length"></a></p>
<markdown-accessiblity-table><table>
<thead>
<tr>
<th>Configuration</th>
<th>128 tokens</th>
<th>2048 tokens</th>
<th>4096 tokens</th>
<th>8192 tokens</th>
</tr>
</thead>
<tbody>
<tr>
<td>FP16 (baseline)</td>
<td>5.50 MB</td>
<td>44.00 MB</td>
<td>88.00 MB</td>
<td>176.00 MB</td>
</tr>
<tr>
<td>K8V8 (8-bit)</td>
<td>2.92 MB</td>
<td>23.38 MB</td>
<td>46.75 MB</td>
<td>93.50 MB</td>
</tr>
<tr>
<td>K8V4 (mixed)</td>
<td>2.23 MB</td>
<td>17.88 MB</td>
<td>35.75 MB</td>
<td>71.50 MB</td>
</tr>
<tr>
<td>K4V8 (mixed)</td>
<td>2.23 MB</td>
<td>17.88 MB</td>
<td>35.75 MB</td>
<td>71.50 MB</td>
</tr>
<tr>
<td>K4V4 (4-bit)</td>
<td>1.55 MB</td>
<td>12.38 MB</td>
<td>24.75 MB</td>
<td>49.50 MB</td>
</tr>
</tbody>
</table></markdown-accessiblity-table>
<p dir="auto"><h2 tabindex="-1" dir="auto">Features</h2><a id="user-content-features" aria-label="Permalink: Features" href="#features"></a></p>
<ul dir="auto">
<li>Independent quantization of keys and values in the KV cache</li>
<li>Optimized for Apple Silicon with Metal support</li>
<li>Comprehensive benchmarking suite with perplexity measurement</li>
<li>Memory usage and performance analysis tools</li>
<li>Publication-quality visualization tools</li>
<li>Easy setup and usage</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Prerequisites</h2><a id="user-content-prerequisites" aria-label="Permalink: Prerequisites" href="#prerequisites"></a></p>
<ul dir="auto">
<li>macOS (tested on Apple Silicon)</li>
<li>Homebrew package manager</li>
<li>Xcode Command Line Tools</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">⚡ One-Command Installation</h2><a id="user-content--one-command-installation" aria-label="Permalink: ⚡ One-Command Installation" href="#-one-command-installation"></a></p>
<div dir="auto" data-snippet-clipboard-copy-content="# Clone the repository
git clone https://github.com/dipampaul17/KVSplit.git
cd kvsplit

# Run the installer script
chmod +x scripts/install_kvsplit.sh
./scripts/install_kvsplit.sh"><pre><span><span>#</span> Clone the repository</span>
git clone https://github.com/dipampaul17/KVSplit.git
<span>cd</span> kvsplit

<span><span>#</span> Run the installer script</span>
chmod +x scripts/install_kvsplit.sh
./scripts/install_kvsplit.sh</pre></div>
<p dir="auto">The installer will:</p>
<ul dir="auto">
<li>Set up the project structure</li>
<li>Clone and build llama.cpp with Metal support</li>
<li>Configure for differentiated KV cache quantization</li>
<li>Download a small test model (optional)</li>
<li>Set up Python environment for visualization</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">🏎️ Quick Comparison</h2><a id="user-content-️-quick-comparison" aria-label="Permalink: 🏎️ Quick Comparison" href="#️-quick-comparison"></a></p>
<p dir="auto">Want to see the benefits immediately? Run a quick comparison with your model:</p>
<div dir="auto" data-snippet-clipboard-copy-content="# Run quick comparison with different configurations
python scripts/quick_compare.py --model models/your-model.gguf"><pre><span><span>#</span> Run quick comparison with different configurations</span>
python scripts/quick_compare.py --model models/your-model.gguf</pre></div>
<p dir="auto">This will show you a side-by-side comparison of FP16, K8V8, K8V4, K4V8, and K4V4 with memory usage, speed, and quality metrics.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">📊 Impressive Results</h2><a id="user-content--impressive-results" aria-label="Permalink: 📊 Impressive Results" href="#-impressive-results"></a></p>
<p><a target="_blank" rel="noopener noreferrer" href="https://github.com/dipampaul17/KVSplit/blob/main/plots/memory_vs_quality.png"><img src="https://github.com/dipampaul17/KVSplit/raw/main/plots/memory_vs_quality.png" alt="Memory vs Quality" width="50%"></a>
</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">📉 Memory Reduction</h3><a id="user-content--memory-reduction" aria-label="Permalink: 📉 Memory Reduction" href="#-memory-reduction"></a></p>
<markdown-accessiblity-table><table>
<thead>
<tr>
<th>Configuration</th>
<th>VRAM @ 8K tokens</th>
<th>Memory Savings</th>
<th>Quality Impact</th>
</tr>
</thead>
<tbody>
<tr>
<td>FP16 (base)</td>
<td>176.00 MB</td>
<td>—</td>
<td>—</td>
</tr>
<tr>
<td>K8V8 (8-bit)</td>
<td>93.50 MB</td>
<td>47%</td>
<td>+0.03%</td>
</tr>
<tr>
<td><strong>K8V4</strong></td>
<td><strong>71.50 MB</strong></td>
<td><strong>59%</strong></td>
<td><strong>+0.86%</strong></td>
</tr>
<tr>
<td>K4V8</td>
<td>71.50 MB</td>
<td>59%</td>
<td>+6.06%</td>
</tr>
<tr>
<td>K4V4 (4-bit)</td>
<td>49.50 MB</td>
<td>72%</td>
<td>+6.15%</td>
</tr>
</tbody>
</table></markdown-accessiblity-table>
<p dir="auto"><h3 tabindex="-1" dir="auto">📈 Performance Impact</h3><a id="user-content--performance-impact" aria-label="Permalink: 📈 Performance Impact" href="#-performance-impact"></a></p>
<p dir="auto">Using KVSplit doesn't just save memory—it often <strong>improves inference speed</strong> by 5-15%!</p>
<markdown-accessiblity-table><table>
<thead>
<tr>
<th>Configuration</th>
<th>Tokens/sec (8K ctx)</th>
<th>Speedup vs FP16</th>
</tr>
</thead>
<tbody>
<tr>
<td>FP16</td>
<td>54,360</td>
<td>—</td>
</tr>
<tr>
<td>K8V8</td>
<td>51,503</td>
<td>-5.3%</td>
</tr>
<tr>
<td><strong>K8V4</strong></td>
<td><strong>57,438</strong></td>
<td><strong>+5.7%</strong></td>
</tr>
<tr>
<td>K4V8</td>
<td>58,690</td>
<td>+8.0%</td>
</tr>
<tr>
<td>K4V4</td>
<td>55,193</td>
<td>+1.5%</td>
</tr>
</tbody>
</table></markdown-accessiblity-table>
<p dir="auto"><h2 tabindex="-1" dir="auto">🧠 Project Structure</h2><a id="user-content--project-structure" aria-label="Permalink: 🧠 Project Structure" href="#-project-structure"></a></p>
<div data-snippet-clipboard-copy-content="kvsplit/
├── llama.cpp/      # Optimized llama.cpp build
├── models/         # LLM model files
├── scripts/        # Utility scripts
│   ├── benchmark_kvsplit.py    # Comprehensive benchmark tool
│   ├── install_kvsplit.sh      # One-command installer
│   ├── quick_compare.py        # Quick comparison utility
│   ├── capture_memory.sh       # GIF creation for memory visualization
│   └── visualize_results.py    # Generate publication-quality plots
├── results/        # Benchmark results (CSV/JSON)
├── plots/          # Generated visualizations
└── README.md       # This file"><pre><code>kvsplit/
├── llama.cpp/      # Optimized llama.cpp build
├── models/         # LLM model files
├── scripts/        # Utility scripts
│   ├── benchmark_kvsplit.py    # Comprehensive benchmark tool
│   ├── install_kvsplit.sh      # One-command installer
│   ├── quick_compare.py        # Quick comparison utility
│   ├── capture_memory.sh       # GIF creation for memory visualization
│   └── visualize_results.py    # Generate publication-quality plots
├── results/        # Benchmark results (CSV/JSON)
├── plots/          # Generated visualizations
└── README.md       # This file
</code></pre></div>
<p dir="auto"><h2 tabindex="-1" dir="auto">🔬 Scientific Insight</h2><a id="user-content--scientific-insight" aria-label="Permalink: 🔬 Scientific Insight" href="#-scientific-insight"></a></p>
<p><a target="_blank" rel="noopener noreferrer" href="https://github.com/dipampaul17/KVSplit/blob/main/plots/configuration_summary.png"><img src="https://github.com/dipampaul17/KVSplit/raw/main/plots/configuration_summary.png" alt="Configuration Summary" width="80%"></a>
</p>
<p dir="auto">KV cache memory is dominated by storing key and value vectors for each token. Our research has revealed a critical insight: <strong>keys are significantly more sensitive to quantization than values</strong>.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">🔑 Key Findings</h3><a id="user-content--key-findings" aria-label="Permalink: 🔑 Key Findings" href="#-key-findings"></a></p>
<ul dir="auto">
<li><strong>Asymmetric Impact</strong>: Keys require higher precision than values for maintaining quality</li>
<li><strong>Sweet Spot</strong>: K8V4 (8-bit keys, 4-bit values) provides optimal balance
<ul dir="auto">
<li>Only 0.86% perplexity degradation vs. FP16</li>
<li>59% memory reduction</li>
<li>Faster inference than FP16</li>
</ul>
</li>
<li><strong>Confirmation</strong>: K4V8 configuration shows 7x more quality degradation than K8V4, despite using the same total bits</li>
</ul>
<p dir="auto">This asymmetry allows for more efficient memory usage without compromising model quality, enabling longer context windows and larger models on consumer hardware.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">💻 Usage Examples</h2><a id="user-content--usage-examples" aria-label="Permalink: 💻 Usage Examples" href="#-usage-examples"></a></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Running with Different KV Cache Precisions</h3><a id="user-content-running-with-different-kv-cache-precisions" aria-label="Permalink: Running with Different KV Cache Precisions" href="#running-with-different-kv-cache-precisions"></a></p>
<div dir="auto" data-snippet-clipboard-copy-content="# Baseline (FP16)
./llama.cpp/build/bin/llama-cli -m models/your-model.gguf -p &quot;Your prompt&quot; \
  -t 8 --flash-attn

# ⭐ RECOMMENDED: 8-bit keys, 4-bit values (K8V4) 
# Best balance of quality and memory savings
./llama.cpp/build/bin/llama-cli -m models/your-model.gguf -p &quot;Your prompt&quot; \
  -t 8 --flash-attn --kvq 8

# 4-bit keys, 8-bit values (K4V8)
# Shows why key precision matters more than value precision
./llama.cpp/build/bin/llama-cli -m models/your-model.gguf -p &quot;Your prompt&quot; \
  -t 8 --flash-attn --kvq-key 4 --kvq-val 8

# 4-bit keys and values (K4V4)
# Maximum memory savings (72% reduction) with acceptable quality
./llama.cpp/build/bin/llama-cli -m models/your-model.gguf -p &quot;Your prompt&quot; \
  -t 8 --flash-attn --kvq 4"><pre><span><span>#</span> Baseline (FP16)</span>
./llama.cpp/build/bin/llama-cli -m models/your-model.gguf -p <span><span>"</span>Your prompt<span>"</span></span> \
  -t 8 --flash-attn

<span><span>#</span> ⭐ RECOMMENDED: 8-bit keys, 4-bit values (K8V4) </span>
<span><span>#</span> Best balance of quality and memory savings</span>
./llama.cpp/build/bin/llama-cli -m models/your-model.gguf -p <span><span>"</span>Your prompt<span>"</span></span> \
  -t 8 --flash-attn --kvq 8

<span><span>#</span> 4-bit keys, 8-bit values (K4V8)</span>
<span><span>#</span> Shows why key precision matters more than value precision</span>
./llama.cpp/build/bin/llama-cli -m models/your-model.gguf -p <span><span>"</span>Your prompt<span>"</span></span> \
  -t 8 --flash-attn --kvq-key 4 --kvq-val 8

<span><span>#</span> 4-bit keys and values (K4V4)</span>
<span><span>#</span> Maximum memory savings (72% reduction) with acceptable quality</span>
./llama.cpp/build/bin/llama-cli -m models/your-model.gguf -p <span><span>"</span>Your prompt<span>"</span></span> \
  -t 8 --flash-attn --kvq 4</pre></div>
<p dir="auto"><h3 tabindex="-1" dir="auto">Long Context Example (32K)</h3><a id="user-content-long-context-example-32k" aria-label="Permalink: Long Context Example (32K)" href="#long-context-example-32k"></a></p>
<div dir="auto" data-snippet-clipboard-copy-content="# Run with a 32K context (would require ~1.4GB in FP16, only ~400MB with K8V4)
./llama.cpp/build/bin/llama-cli -m models/your-model.gguf \
  -c 32768 -n 4096 -t 8 --flash-attn --kvq 8 \
  -f your-long-document.txt"><pre><span><span>#</span> Run with a 32K context (would require ~1.4GB in FP16, only ~400MB with K8V4)</span>
./llama.cpp/build/bin/llama-cli -m models/your-model.gguf \
  -c 32768 -n 4096 -t 8 --flash-attn --kvq 8 \
  -f your-long-document.txt</pre></div>
<p dir="auto"><h3 tabindex="-1" dir="auto">🚩 Command-Line Arguments</h3><a id="user-content--command-line-arguments" aria-label="Permalink: 🚩 Command-Line Arguments" href="#-command-line-arguments"></a></p>
<markdown-accessiblity-table><table>
<thead>
<tr>
<th>Flag</th>
<th>Description</th>
<th>Recommendation</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>-t 8</code></td>
<td>Number of threads</td>
<td>8 is optimal for most Apple Silicon chips</td>
</tr>
<tr>
<td><code>--flash-attn</code></td>
<td>Enables optimized attention</td>
<td>Recommended for Apple Silicon</td>
</tr>
<tr>
<td><code>--kvq N</code></td>
<td>Sets both key and value bits to N</td>
<td>Use <code>--kvq 8</code> for K8V4 configuration</td>
</tr>
<tr>
<td><code>--kvq-key N</code></td>
<td>Sets key bits only</td>
<td>Key precision has major quality impact</td>
</tr>
<tr>
<td><code>--kvq-val N</code></td>
<td>Sets value bits only</td>
<td>Value precision has minor quality impact</td>
</tr>
<tr>
<td><code>-c N</code></td>
<td>Context size in tokens</td>
<td>Longer contexts benefit more from KVSplit</td>
</tr>
<tr>
<td><code>-n N</code></td>
<td>Number of tokens to generate</td>
<td>Adjust based on your needs</td>
</tr>
<tr>
<td><code>-f FILE</code></td>
<td>Input file</td>
<td>For processing documents</td>
</tr>
<tr>
<td><code>-m MODEL</code></td>
<td>Model path</td>
<td>Path to your .gguf model file</td>
</tr>
</tbody>
</table></markdown-accessiblity-table>
<p dir="auto"><h2 tabindex="-1" dir="auto">📏 Advanced Benchmarking</h2><a id="user-content--advanced-benchmarking" aria-label="Permalink: 📏 Advanced Benchmarking" href="#-advanced-benchmarking"></a></p>
<p dir="auto">For comprehensive performance analysis, use our full benchmark suite:</p>
<div dir="auto" data-snippet-clipboard-copy-content="# Run the full benchmark suite (all configurations and sequence lengths)
python scripts/benchmark_kvsplit.py

# Run a specific configuration test
python scripts/benchmark_kvsplit.py --config K8V4 --seq-len 4096

# Generate publication-quality visualizations
python scripts/visualize_results.py"><pre><span><span>#</span> Run the full benchmark suite (all configurations and sequence lengths)</span>
python scripts/benchmark_kvsplit.py

<span><span>#</span> Run a specific configuration test</span>
python scripts/benchmark_kvsplit.py --config K8V4 --seq-len 4096

<span><span>#</span> Generate publication-quality visualizations</span>
python scripts/visualize_results.py</pre></div>
<p dir="auto">The benchmarking script provides thorough measurements of:</p>
<ul dir="auto">
<li>📊 <strong>Memory Usage</strong>: VRAM and KV cache specifically</li>
<li>⚡ <strong>Performance</strong>: Tokens per second across different sequence lengths</li>
<li>🎯 <strong>Quality</strong>: Perplexity measurement using llama-perplexity</li>
<li>📈 <strong>Scaling</strong>: How memory usage and performance scale with sequence length</li>
</ul>
<p dir="auto">Results are saved in CSV/JSON formats with automatic summary statistics, and the visualization script generates publication-quality plots showing key insights.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">License</h2><a id="user-content-license" aria-label="Permalink: License" href="#license"></a></p>
<p dir="auto">MIT</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">🎬 Visual Memory Savings</h2><a id="user-content--visual-memory-savings" aria-label="Permalink: 🎬 Visual Memory Savings" href="#-visual-memory-savings"></a></p>
<p dir="auto">You can visualize memory savings with our capture tool:</p>
<div dir="auto" data-snippet-clipboard-copy-content="# Capture memory reduction in Activity Monitor
./scripts/capture_memory.sh"><pre><span><span>#</span> Capture memory reduction in Activity Monitor</span>
./scripts/capture_memory.sh</pre></div>

<p dir="auto"><h2 tabindex="-1" dir="auto">🍎 Apple Silicon Optimization</h2><a id="user-content--apple-silicon-optimization" aria-label="Permalink: 🍎 Apple Silicon Optimization" href="#-apple-silicon-optimization"></a></p>
<ul dir="auto">
<li><strong>Metal Performance</strong>: Fully optimized for Apple's Metal framework</li>
<li><strong>Memory Efficiency</strong>: Critical for memory-constrained M1/M2/M3 devices</li>
<li><strong>Activity Monitor</strong>: Use our <code>capture_memory.sh</code> script to visualize real-time memory reductions</li>
<li><strong>Alignment</strong>: 256B page alignment in llama.cpp means actual memory savings might differ slightly from theoretical calculations</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">⭐ Key Features</h2><a id="user-content--key-features" aria-label="Permalink: ⭐ Key Features" href="#-key-features"></a></p>
<ul dir="auto">
<li><strong>Differentiated Precision</strong>: Independent key and value bit precision (K8V4, K4V8, etc)</li>
<li><strong>Apple Silicon Optimization</strong>: Full Metal support for M1/M2/M3 chips</li>
<li><strong>Comprehensive Benchmarking</strong>: Memory, speed, and quality metrics</li>
<li><strong>Publication-Quality Visualization</strong>: Beautiful plots for analysis</li>
<li><strong>Simple User Interface</strong>: One-command install and quick comparison tools</li>
<li><strong>Memory Visualization</strong>: Tools to capture and visualize memory savings</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">🙏 Acknowledgments</h2><a id="user-content--acknowledgments" aria-label="Permalink: 🙏 Acknowledgments" href="#-acknowledgments"></a></p>
<p dir="auto">This project implements ideas from recent research including:</p>
<ul dir="auto">
<li>"More for Keys, Less for Values: Adaptive KV Cache Quantization" (2024)</li>
<li>"Unifying KV Cache Compression for Large Language Models with LeanKV" (2025)</li>
</ul>
<p dir="auto">Additional credits:</p>
<ul dir="auto">
<li><a href="https://github.com/ggerganov/llama.cpp">llama.cpp</a> - Base implementation</li>
<li><a href="https://huggingface.co/TinyLlama" rel="nofollow">TinyLlama</a> - Test model</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Contributing</h2><a id="user-content-contributing" aria-label="Permalink: Contributing" href="#contributing"></a></p>
<p dir="auto">Contributions are welcome! Please open an issue or submit a pull request.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">🧠 Configuration Recommendations</h2><a id="user-content--configuration-recommendations" aria-label="Permalink: 🧠 Configuration Recommendations" href="#-configuration-recommendations"></a></p>
<ul dir="auto">
<li>
<p dir="auto"><strong>Best Overall</strong>: 🌟 <strong>K8V4</strong> 🌟 (8-bit keys, 4-bit values)</p>
<ul dir="auto">
<li>59% memory reduction with only 0.86% quality loss</li>
<li>Improved inference speed (+5.7% vs FP16)</li>
<li>Great balance of quality and efficiency</li>
</ul>
</li>
<li>
<p dir="auto"><strong>Absolute Maximum Memory Savings</strong>: K4V4 (4-bit keys and values)</p>
<ul dir="auto">
<li>72% memory reduction with ~6% quality loss</li>
<li>Good for memory-constrained devices</li>
<li>Acceptable for less sensitive applications</li>
</ul>
</li>
<li>
<p dir="auto"><strong>Best for Very Long Contexts</strong>: K8V4 or K4V4</p>
<ul dir="auto">
<li>Memory savings compound with context length</li>
<li>Run 2-3x longer contexts in the same memory budget</li>
</ul>
</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">🔮 Future Roadmap</h2><a id="user-content--future-roadmap" aria-label="Permalink: 🔮 Future Roadmap" href="#-future-roadmap"></a></p>
<ul>
<li> <strong>Adaptive Precision</strong>: Dynamic precision based on token importance</li>
<li> <strong>Layer-Specific Quantization</strong>: Different precision for different model layers</li>
<li> <strong>Model-Specific Optimizations</strong>: Tailored for Mistral, Phi-3, etc.</li>
<li> <strong>Web Demo</strong>: Interactive testing environment</li>
<li> <strong>Mobile Support</strong>: Adapting for iOS and iPadOS</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">📜 License</h2><a id="user-content--license" aria-label="Permalink: 📜 License" href="#-license"></a></p>
<p dir="auto">MIT</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">🤝 Contributing</h2><a id="user-content--contributing" aria-label="Permalink: 🤝 Contributing" href="#-contributing"></a></p>
<p dir="auto">Contributions are welcome! Please open an issue or submit a pull request.</p>
</article></div></div>]]></description>
        </item>
    </channel>
</rss>