<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0">
    <channel>
        <title>HN100 - Readable Contents</title>
        <link>https://hn.algolia.com/api/v1/search_by_date?tags=%28story,poll%29&amp;numericFilters=points%3E100</link>
        <description>Uses Readability to add bodies to the RSS feed</description>
        <lastBuildDate>Sat, 07 Jun 2025 15:30:02 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>en</language>
        <item>
            <title><![CDATA[Low-Level Optimization with Zig (156 pts)]]></title>
            <link>https://alloc.dev/2025/06/07/zig_optimization</link>
            <guid>44208060</guid>
            <pubDate>Sat, 07 Jun 2025 07:26:24 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://alloc.dev/2025/06/07/zig_optimization">https://alloc.dev/2025/06/07/zig_optimization</a>, See on <a href="https://news.ycombinator.com/item?id=44208060">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
        <div>
            <h2> 2025-06-07 # Optimizations with Zig </h2>
            <p>
                "Beware of the <a href="https://en.wikipedia.org/wiki/Turing_tarpit" target="_blank">Turing tar-pit</a>
                in which everything is possibile, but nothing of interest is easy." - Alan Perlis 1982
            </p>
            <p>
                What is of interest to you? Many things, I am certain. One such topic that I am constantly intrigued by
                is program optimization. Whether you are looking to <a href="https://youtu.be/KzT9I1d-LlQ" target="_blank">compute the largest fibonacci number</a> in one second, or creating the <a href="https://tigerbeetle.com/" target="_blank">fastest financial transaction database</a> ever
                written, or even just <a href="https://gaultier.github.io/blog/lessons_learned_from_a_successful_rust_rewrite.html" target="_blank">rewriting something in rust</a>, you likely know how rewarding optimization can be.
            </p>
            <p>
                Optimization separates the weak from the fast. Optimization isn't a thing of the past. Advances in
                technology shape the form of our programs, but don't negate the need to optimize. Well optimized
                programs save money, enable higher-tier scaling opportunities, and preserve system simplicity. Would you
                rather spend thousands to run shoddy code on autoscaling cloud infrastructure, or write better code,
                letting you use a handful of mid-tier servers at reduced latency and cost?
            </p>
            <p>
                In this article I aim to explain the concept of low-level optimization, and why Zig is particularly well
                suited for it. If you enjoy what you read, please consider <a href="https://www.paypal.com/donate/?business=2Z3H3UQA37LML&amp;no_recurring=0&amp;item_name=Like+what+you+see?+A+dollar+or+two+will+help+me+learn+and+publish+more+-+Thanks!&amp;currency_code=USD" target="_blank">supporting me</a> :)
            </p>
            <p><img src="https://alloc.dev/2025/06/07/servers.webp" width="100%">
        </p></div>

        <div id="trust_compiler">
            <h2> Trust the compiler? </h2>
            <p>
                Some people would say "trust the compiler, it knows best." It sure does, for most low-level situations!
                Optimizing compilers have come a long ways. Increased system resources and <a href="https://blog.vortan.dev/ematching" target="_blank">advances in IR transformations</a> have
                enabled compiler backends like <a href="https://github.com/llvm/llvm-project" target="_blank">LLVM</a>
                to deliver <a href="https://blog.matthieud.me/2020/exploring-clang-llvm-optimization-on-programming-horror/" target="_blank">impressive results</a>.
            </p>
            <p>
                Compilers are complicated beasts. The best optimizing backends will still <a href="https://godbolt.org/#g:!((g:!((g:!((h:codeEditor,i:(filename:'1',fontScale:14,fontUsePx:'0',j:1,lang:zig,selection:(endColumn:2,endLineNumber:7,positionColumn:2,positionLineNumber:7,selectionStartColumn:2,selectionStartLineNumber:7,startColumn:2,startLineNumber:7),source:'const+std+%3D+@import(%22std%22)%3B%0A%0Aexport+fn+makeArray()+%5B*%5Du8+%7B%0A++++var+list+%3D+std.ArrayListUnmanaged(u8).initCapacity(std.heap.c_allocator,+1)+catch+unreachable%3B%0A++++list.appendAssumeCapacity(123)%3B%0A++++return+list.items.ptr%3B%0A%7D'),l:'5',n:'0',o:'Zig+source+%231',t:'0')),k:50.12184508268059,l:'4',m:100,n:'0',o:'',s:0,t:'0'),(g:!((h:compiler,i:(compiler:z0141,filters:(b:'0',binary:'1',binaryObject:'1',commentOnly:'0',debugCalls:'1',demangle:'0',directives:'0',execute:'1',intel:'0',libraryCode:'0',trim:'1',verboseDemangling:'0'),flagsViewOpen:'1',fontScale:14,fontUsePx:'0',j:1,lang:zig,libs:!(),options:'-O+ReleaseFast+-fomit-frame-pointer+-mcpu+x86_64_v3+-lc',overrides:!(),selection:(endColumn:1,endLineNumber:1,positionColumn:1,positionLineNumber:1,selectionStartColumn:1,selectionStartLineNumber:1,startColumn:1,startLineNumber:1),source:1),l:'5',n:'0',o:'+zig+0.14.1+(Editor+%231)',t:'0')),k:49.878154917319414,l:'4',m:100,n:'0',o:'',s:0,t:'0')),l:'2',n:'0',o:'',t:'0')),version:4" target="_blank">generate sub</a>-<a href="https://godbolt.org/#z:OYLghAFBqd5QCxAYwPYBMCmBRdBLAF1QCcAaPECAMzwBtMA7AQwFtMQByARg9KtQYEAysib0QXACx8BBAKoBnTAAUAHpwAMvAFYTStJg1AAvPMFJL6yAngGVG6AMKpaAVxYMJAdlIOAMngMmABy7gBGmMR6AA6oCoS2DM5uHt6ksfE2AgFBoSwRUVwWmFZZDEIETMQEye6eXD6WmNaJFVUEOSHhkXoKldW1qQ0W/R2BXfk9XACUFqiuxMjsHAD0KwDUAOoIAJ7reArrhuuYqtG0eMiE6zQMhJjrfUwED0yuRCzPAuuo0TYseGMmHQ6wiBBexFBez8fgAagBZACkAGYAGKIjQAQQxmNOsWqNwY63oTGiAE1MFVMRAdpSoutXMiAEzTUGoFzrRFeABCOPW/JuCiYIEeAHdCMgEOsIAABJgKCCMpmkdYaaasrm8rECnWqznIgAiKOwnJ5fN1FrwVGltKqnKZAFZ1pJ9UbDaqNWbtRaffy0IJAq4HiAqEL1lwUVrMb6dVyDSdaEpTVGYz7/TYGEH1iGw5JI%2BbU3GC7q46RizquK7jcnyz6rTa6fanVwNBoq%2B61TXvamdenA8HQ0x1kz892Y3GE0nNbXU33MwOw8jR9Ge0Wx7GvAay%2BuBUz2ybpzvLdaaY3EY7na32/HO4eVz2BXOszmh0vkSnV5vJw87w/e7J%2B2zQdnWXB813vEtN23CCdWRfd1mITACAWIkCGIINoN9F0UTdE1EOQ4giSoMQlEwn0SinQ1q1cBhEKYSUmDCegyP5cC2KxPESAIQliUpck6W5U8qhFJVWTCdlaC7CD6yEyFzydbCqI7T0P2PBs7Xk8MrxwnCPSkh8ZNtOSL0kK8wDAXTby9GCe3wlCbhIzBQM/I0j1NVybP5OzCPWNCg2c1jN2LbyiMc5c4w4WZaE4B1eE8DgtFIVBOAALTMR55kWH8mWRHhSAITRItmABrEBJAATgAOkkAA2DRkQ0JkvHK8qauRZFJGkaKOEkOLCqSzheAUEANHywrZjgWAkDQFhojoSJyEoGa5voKJjA0KQihoWgIWGiAwn6sJAiqHZODyo7mGIHYAHkwm0ZoCu4XgZrYQRroYWhToS3gsDCVxgEcMREzOn7ME%2BIxxG%2B0h8EQloADdMGGqHTmad5ljywIXm6xKLjCYgTucLB%2BrQvAWBB0gEeIcSlANMHDGAC4jHGvgDGABRYTwTBRWu6JGHJ/hBBEMR2CkGRBEUFR1Ch3QigMJnTHMXHhsgWZfjKJGAFproAJRKSklCET5aEkjX%2BABAhTfxtgNdiTHIl4VBKeIPAsGViBZiaFo7AgBxBnqXwGHQTo8gKGI4gSAQ/bDzJEmD7pCmKUpWlGKOik9so2mqOPJgTp4BhcOpelGbPQ5mOYFiWCQopivqoeSjh1gV1VKqkFvpVwQgSHtXLpl4R6tHVUhSodDRKodZFWvKrwmVbZEapqh0fG63rSHixL66Gkaxu%2BwfuqZWv18G7eB9mSn4jsSQgA%3D" target="_blank">par code</a> in some cases. In fact, even
                state-of-art compilers will <a href="https://godbolt.org/#z:OYLghAFBqd5QCxAYwPYBMCmBRdBLAF1QCcAaPECAMzwBtMA7AQwFtMQByARg9KtQYEAysib0QXACx8BBAKoBnTAAUAHpwAMvAFYTStJg1AB9U8lJL6yAngGVG6AMKpaAVxYMQAJlIOAMngMmABy7gBGmMQSPgAOqAqEtgzObh7epHEJNgIBQaEsEVFcPpaY1klCBEzEBCnuniWYVtkMldUEuSHhkdEWVTV1aSX9HYFdBT3FAJQWqK7EyOwcAKReAMyByG5YANTLa44KBPiCAHQI%2B9jLGgCC1zcAbqh46Du0qKgxAOKoAELEAOIEFcgQIay8xgIO1UUz2AHZfvcdsidiDBODITs8Hs1gARaH7RG3FE7ADuCDomB2EGx%2B0c%2B3xMPhRJuJJJAHp2TsAJJUaFYhQ7YDETBMAiRHYEBCGHYaVRUBWKqgAVlIOyY6BOwB2XA0ktQWLJc1orzEpKYAE9BSKFK5aAQkWzkZysQx1TsFCwxPRiDsHmJXFT0IH9TtUA9IlR3qSdgBaHY2u02Iyu9VuwI0BiEKnvT6nR1Ol2OMAcQUKGJlPA0UQtdW0aOC8OR6Nht2uBgJYBBV6gzDASIKNUKA1bQzavAKAtsl0PWwGZPaqVUxweivIHYISKYfPEp2G1aIvE6jSEqfLOG4%2B7n3EcGa0TjK3ieDhaUioTgr4fzRZ7dY8UgEJot4zJuGo9BAMwANYgMqGj6JwkhPkBb6cLwCggHBgEvrepBwLAMCICgqAsDElJkBQEBoCRZEoMAXDKlwfB0OKxDoRAYTIWEgTVBanD/lxzDEBaADyYTaGUWH/lRbCCMJDC0Lx2GkFgYSuMAjjeuh3C8FgXpGOISn4CK5QRlpr6YKoZSuOKfG8L295KbQeBhMQPHOFgyEEMQeAsLZpARsQYTxJguKYHpwBOUYQEzFGTDAAoABqeCYKSwkVs%2B/78IIIhiOwUgyIIigqOoSm6IxBhRaYxjmE5YToZAMyfC0WmxsJay8E2AIvJg9UQRYTQSUk9gME4Lj1Ho/hjPkhR6JkiQCIMniMXNLSdNNkz9c0FQjIteilOUAhtDUa3dEUfTtLtjFHO0J0TEUMxfgseV3g%2BSFKe%2BHDQgAHAAbLGP2SEKyDrvRpxcNSuCECQv5rFwUy8FhWhTCBopYFEfXQbB8EcIhpC%2BVjz6vh9aEYQB0W4QREBIFRpE%2BuQlA0zRwMVcAXgaLqTH2gOlAcUpAk8X5/NCaJ4nWH50mMAQckKchKlqRp9Zaf%2BuljgZr5GYNpnIRZVk2dp5CCE0yG1a5QnuUsr5eT5fkBUFSiheFkWgNhMUGPFSUpWljB%2BVlwiiOI%2BW%2B0VajIboPgsyAVU1c5vWNTEzWcK17VvgF3lYLHm2DXYEAOJdvgjbdM3LfE83JGNaTF1kSSFxt%2B0tEdtTl0tmcHa0Iw12d10DE3e3t1Np0SA9cxPYP2OPqQhMdZw31/QDOyjimbNg6ceoQJDRC%2BqssPw2TLso2B6MvTjvD43Bk8oRwJOYdFUEwXBDnJ%2BfxO70jR9eG9ROoS/wH%2BQOQ2SEAA" target="_blank">break language
                    specifications</a> (<a href="https://github.com/llvm/llvm-project/issues/143046" target="_blank">Clang assumes</a> that all loops without side effects will terminate). It is up to
                us to give our compilers as much information as possible, and verify that the compiler is functioning
                correctly. In the case of most low-level languages, you can generally massage your code until the
                compiler realizes it can apply a certain transform. In other situations, it's not so easy.
            </p>
            <p>
                Why are low-level languages generally more performant? You might think the reason is that high level
                languages are doing a lot of extra work, such as garbage collection, string interning, interpreting
                code, etc. While you are right, this isn't *entirely* complete. High level languages lack something that
                low level languages have in great adundance - intent.
            </p>
            <p>
                The verbosity of low level programming languages enable us to produce code that the compiler can reason
                about very well. As an example, consider the following JavaScript code:
            </p>
            <pre><code>function maxArray(x, y) {
    for (let i = 0; i &lt; 65536; i++) {
        x[i] = y[i] &gt; x[i] ? y[i] : x[i];
    }
}</code></pre>
            <p>
                As humans, we can interpret this code as setting the values in <code>x</code> as the maximum of
                <code>x</code> and <code>y</code>. The <a href="https://godbolt.org/#g:!((g:!((g:!((h:codeEditor,i:(filename:'1',fontScale:14,fontUsePx:'0',j:1,lang:javascript,selection:(endColumn:51,endLineNumber:15,positionColumn:51,positionLineNumber:15,selectionStartColumn:51,selectionStartLineNumber:15,startColumn:51,startLineNumber:15),source:'function+maxArray(x,+y)+%7B%0A++++for+(let+i+%3D+0%3B+i+%3C+65536%3B+i%2B%2B)+%7B%0A++++++++x%5Bi%5D+%3D+y%5Bi%5D+%3E+x%5Bi%5D+%3F+y%5Bi%5D+:+x%5Bi%5D%3B%0A++++%7D%0A%7D%0A%0Alet+arrA+%3D+new+Float64Array(65536).fill(1)%3B+//+all+1s%0Alet+arrB+%3D+new+Float64Array(65536).fill(2)%3B+//+all+2s%0A%0A//+Prepare+and+warm+up+the+function%0A%25PrepareFunctionForOptimization(maxArray)%3B%0AmaxArray(arrA,+arrB)%3B++++++//+Fill+type+feedback%0AmaxArray(arrA,+arrB)%3B++++++//+Stabilize+into+monomorphic%0A%25OptimizeFunctionOnNextCall(maxArray)%3B%0AmaxArray(arrA,+arrB)%3B++++++//+Trigger+optimization'),l:'5',n:'0',o:'Javascript+source+%231',t:'0')),k:49.48081530907177,l:'4',m:100,n:'0',o:'',s:0,t:'0'),(g:!((h:compiler,i:(compiler:v8trunk,filters:(b:'0',binary:'1',binaryObject:'1',commentOnly:'0',debugCalls:'1',demangle:'0',directives:'0',execute:'1',intel:'0',libraryCode:'0',trim:'1',verboseDemangling:'0'),flagsViewOpen:'1',fontScale:14,fontUsePx:'0',j:1,lang:javascript,libs:!(),options:'',overrides:!(),selection:(endColumn:1,endLineNumber:1,positionColumn:1,positionLineNumber:1,selectionStartColumn:1,selectionStartLineNumber:1,startColumn:1,startLineNumber:1),source:1),l:'5',n:'0',o:'+v8+(trunk)+(Editor+%231)',t:'0')),header:(),k:50.51918469092824,l:'4',m:100,n:'0',o:'',s:0,t:'0')),l:'2',n:'0',o:'',t:'0')),version:4" target="_blank">generated bytecode</a> for this JavaScript (under V8) is pretty bloated. As a
                comparison, here is what the function might look like if it were written in Zig:
            </p>
            <pre><code>fn maxArray(
    noalias x: *align(64) [65536]f64,
    y: *align(64) const [65536]f64,
) void {
    for (x, y, 0..) |a, b, i| {
        x[i] = if (b &gt; a) b else a;
    }
}</code></pre>
            <p>
                In this more verbose language, we can tell the compiler additional information about our code. For
                example, the optimizer now knows about alignment, aliasing requirements, array sizes, and array element
                types - all at compile-time. Using this information, the compiler generates <a href="https://godbolt.org/#g:!((g:!((g:!((h:codeEditor,i:(filename:'1',fontScale:14,fontUsePx:'0',j:1,lang:zig,selection:(endColumn:2,endLineNumber:8,positionColumn:2,positionLineNumber:8,selectionStartColumn:2,selectionStartLineNumber:8,startColumn:2,startLineNumber:8),source:'export+fn+maxArray(%0A++++noalias+x:+*align(64)+%5B65536%5Df64,%0A++++y:+*align(64)+const+%5B65536%5Df64,%0A)+void+%7B%0A++++for+(x,+y,+0..)+%7Ca,+b,+i%7C+%7B%0A++++++++x%5Bi%5D+%3D+if+(b+%3E+a)+b+else+a%3B%0A++++%7D%0A%7D'),l:'5',n:'0',o:'Zig+source+%231',t:'0')),k:54.464285714285715,l:'4',m:100,n:'0',o:'',s:0,t:'0'),(g:!((h:compiler,i:(compiler:z0141,filters:(b:'0',binary:'1',binaryObject:'1',commentOnly:'0',debugCalls:'1',demangle:'0',directives:'0',execute:'1',intel:'0',libraryCode:'0',trim:'1',verboseDemangling:'0'),flagsViewOpen:'1',fontScale:14,fontUsePx:'0',j:1,lang:zig,libs:!(),options:'-OReleaseFast+-mcpu%3Dznver5+-fomit-frame-pointer',overrides:!(),selection:(endColumn:1,endLineNumber:1,positionColumn:1,positionLineNumber:1,selectionStartColumn:1,selectionStartLineNumber:1,startColumn:1,startLineNumber:1),source:1),l:'5',n:'0',o:'+zig+0.14.1+(Editor+%231)',t:'0')),header:(),k:45.5357142857143,l:'4',m:100,n:'0',o:'',s:0,t:'0')),l:'2',n:'0',o:'',t:'0')),version:4" target="_blank">far superior, even vectorized</a> code.
                If you were wondering, <a href="https://godbolt.org/#g:!((g:!((g:!((h:codeEditor,i:(filename:'1',fontScale:14,fontUsePx:'0',j:1,lang:rust,selection:(endColumn:17,endLineNumber:10,positionColumn:17,positionLineNumber:10,selectionStartColumn:17,selectionStartLineNumber:10,startColumn:17,startLineNumber:10),source:'%23%5Brepr(align(64))%5D%0Apub+struct+Aligned%3CT:+%3FSized%3E(T)%3B%0A%0Apub+fn+max_array(x:+%26mut+Aligned%3C%5Bf64%3B+65536%5D%3E,+y:+%26Aligned%3C%5Bf64%3B+65536%5D%3E)+%7B%0A++++for+(x,+y)+in+x.0.iter_mut().zip(y.0.iter())+%7B%0A++++++++*x+%3D+if+*y+%3E+*x+%7B+*y+%7D+else+%7B+*x+%7D%3B%0A++++%7D%0A%7D%0A%0Apub+fn+main()+%7B%7D'),l:'5',n:'0',o:'Rust+source+%231',t:'0')),k:54.464285714285715,l:'4',m:100,n:'0',o:'',s:0,t:'0'),(g:!((h:compiler,i:(compiler:r1870,filters:(b:'0',binary:'1',binaryObject:'1',commentOnly:'0',debugCalls:'1',demangle:'0',directives:'0',execute:'1',intel:'0',libraryCode:'0',trim:'1',verboseDemangling:'0'),flagsViewOpen:'1',fontScale:14,fontUsePx:'0',j:1,lang:rust,libs:!(),options:'-C+opt-level%3D3+-C+target-cpu%3Dznver5',overrides:!(),selection:(endColumn:1,endLineNumber:1,positionColumn:1,positionLineNumber:1,selectionStartColumn:1,selectionStartLineNumber:1,startColumn:1,startLineNumber:1),source:1),l:'5',n:'0',o:'+rustc+1.87.0+(Editor+%231)',t:'0')),header:(),k:45.5357142857143,l:'4',m:100,n:'0',o:'',s:0,t:'0')),l:'2',n:'0',o:'',t:'0')),version:4" target="_blank">equivalent Rust
                    code</a> generates near identical assembly.
            </p>
            <p>
                So can we *really* trust our compilers? It depends. Are you looking to uncover transformations to triple
                the throughput of your program's performance bottleneck? You should probably look at what the compiler
                is doing, and figure out if there are better ways to express *your intent* to the compiler. You may need
                to tweak the code to get the transforms you want. In the worst cases, you may discover that the compiler
                isn't applying optimal transforms to your code. In these cases, you may need to write inline assembly to
                squeeze out that last drop.
            </p>
            <p>
                But what about high level code? Except for <a href="https://spiral.net/" target="_blank">niche
                    cases</a>, the most we can do is <a href="https://github.com/llvm/llvm-project/tree/main/polly" target="_blank">reason about loops</a>. In other words, compilers cannot change our algorithms and
                optimize our paradigms. Their scope is relatively narrow.
            </p>
        </div>

        <div id="what_about_zig">
            <h2> So where does Zig fall? </h2>
            <p>
                I love Zig for it's verbosity. Due to this verbosity, it's easier to write more performant programs than
                with most other languages. With Zig's <a href="https://ziglang.org/documentation/master/#Builtin-Functions" target="_blank">builtin
                    functions</a>, non-optional <a href="https://ziglang.org/documentation/master/#Pointers" target="_blank">pointers</a>, <a href="https://ziglang.org/documentation/master/#unreachable" target="_blank"><code>unreachable</code> keyword</a>, well chosen <a href="https://ziglang.org/documentation/master/#Illegal-Behavior" target="_blank">illegal
                    behavior</a>, Zig's excellent <a href="https://ziglang.org/documentation/master/#comptime" target="_blank">comptime</a>... LLVM is practically spoon-fed information about our code.
            </p>
            <p>
                It isn't all rainbows though, there are tradeoffs. For example, Rust's memory model allows the compiler
                to always assume that function arguments never alias. You must manually specify this in Zig. If the
                compiler can't tell that your Zig function is always called with non-aliasing arguments, Rust
                functions will outperform the non-annotated Zig functions.
            </p>
            <p>
                If we take in well-annotated LLVM IR as the *only* metric of a language's optimization capability, then
                Zig does well. This is not all that Zig has up it's sleeves. Zig's true optimization superpower lies in
                compile-time execution.
            </p>
            <p><img src="https://alloc.dev/2025/06/07/gotta_go_fast.webp" alt="speedily (poorly) drawn lizard with a space helmet" width="100%">
        </p></div>

        <div id="what_is_comptime">
            <h2> What is <code>comptime</code>? </h2>
            <p>
                <a href="https://ziglang.org/documentation/master/#comptime" target="_blank">Zig's
                    <code>comptime</code></a> is all about code generation. Do you want to use a constant in your code?
                You can generate it at compile-time and the value will be embedded in the produced binary. Do you want
                to avoid writing the same <a href="https://en.wikipedia.org/wiki/Hash_table" target="_blank">hashmap
                    structure</a> for each type of data it can store? <code>comptime</code> has your back. Do you have
                data which is known at compile-time, and want the optimizer to elide code using this data? Yes, <a href="https://www.scottredig.com/blog/bonkers_comptime/" target="_blank">you can do this</a> with
                <code>comptime</code>. Zig's <code>comptime</code> is an example of <a href="https://en.wikipedia.org/wiki/Metaprogramming">metaprogramming</a>.
            </p>
            <p>
                So how is this different from macros? It's pretty nuanced. The purpose of <code>comptime</code> is
                essentially the same as macros. Some macros will modify the raw text of your code, and others can modify
                your program's <a href="https://en.wikipedia.org/wiki/Abstract_syntax_tree" target="_blank">AST</a>
                directly. This allows macros to inline code specific to the types and values of data in your program. In
                Zig, <code>comptime</code> code is just regular code running at compile-time. This code cannot have
                side-effects - such as network IO - and the emulated machine will match the compilation target.
            </p>
            <p>
                The reason that Zig's <code>comptime</code> can match up so well to macros is twofold. Firstly, almost
                all Zig code can be run at compile-time, using <code>comptime</code>. Secondly, at compile-time, all
                types can be inspected, reflected, and generated. This is <a href="https://ziglang.org/documentation/0.14.1/#Generic-Data-Structures" target="_blank">how
                    generics are implemented</a> in Zig.
            </p>
            <p>
                The flexibility of Zig's <code>comptime</code> has resulted in some rather nice improvements in other
                programming languages. For example, Rust has the <a href="https://docs.rs/crabtime/latest/crabtime/" target="_blank">"crabtime"</a> crate, which provides more flexibility and power than standard Rust
                macros. I believe a benefit of <code>comptime</code> over current alternatives lies in how seamlessly
                <code>comptime</code> fits into the Zig language. Unlike C++'s <code>constexpr</code>, you can use
                <code>comptime</code> without needing to learn a new "language" of sorts. C++ <a href="https://youtu.be/bIc5ZxFL198" target="_blank">is improving</a>, but it has a long ways to go
                if it hopes to compete with Zig in this domain.
            </p>
            <p>
                So can Zig's <code>comptime</code> do *everything* macros can? Nope. Token-pasting macros don't have a
                mirror in Zig's comptime. Zig is designed to be easy to read, and macros which modify or create
                variables in a unrelated scopes just don't cut it. Macros can define other macros, macros can alter the
                AST, and macros can implement mini-languages, or DSLs. Zig's <code>comptime</code> can't directly alter
                the AST. If you really want to, you can implement a DSL in Zig. For example, Zig's print function <a href="https://ziglang.org/documentation/master/#Case-Study-print-in-Zig" target="_blank">relies on
                    <code>comptime</code></a> to parse the format string. The print function's format string is a DSL of
                sorts. Based on the format string, Zig's <code>comptime</code> will construct a graph of functions to
                serialize your data. Here are some other examples of <code>comptime</code> DSLs in the wild: the <a href="https://github.com/tigerbeetle/tigerbeetle/blob/main/src/state_machine.zig#L5840" target="_blank">TigerBeetle account testing DSL</a>, <a href="https://github.com/InKryption/comath" target="_blank">comath: comptime math</a>, and <a href="https://github.com/ymndoseijin/zilliam" target="_blank">zilliam</a>, a Geometric Algebra library.
            </p>
            <p>
                Here are more resources for learning about Zig's <code>comptime</code>:
            </p>
            <ul>
                <li> <a href="https://andrewkelley.me/post/string-matching-comptime-perfect-hashing-zig.html" target="_blank">
                        String Matching based on Compile Time Perfect Hashing in Zig - Andrew Kelley </a> </li>
                <li> <a href="https://kristoff.it/blog/what-is-zig-comptime/" target="_blank">
                        What is Zig's Comptime? - Loris Cro </a> </li>
                <li> <a href="https://matklad.github.io/2025/04/19/things-zig-comptime-wont-do.html" target="_blank">
                        Things Zig comptime Won’t Do - matklad </a> </li>
                <li> <a href="https://zig.news/edyu/wtf-is-zig-comptime-and-inline-257b" target="_blank">
                        Zig Comptime - WTF is Comptime (and Inline) - Ed Yu </a> </li>
                <li> <a href="https://ziglang.org/documentation/master/#comptime" target="_blank">
                        Comptime - Zig Language Reference </a> </li>
                <li> <a href="https://zig.guide/language-basics/comptime/" target="_blank">
                        Comptime - zig.guide </a> </li>
            </ul>
        </div>

        <div id="string_comparison">
            <h2> String Comparison with <code>comptime</code>: </h2>
            <p>
                How do you compare two strings? Here's an approach that works in any language:
            </p>
            <pre><code>function stringsAreEqual(a, b) {
    if (a.length !== b.length) return false;
    for (let i = 0; i &lt; a.length; i++)
        if (a[i] !== b[i]) return false;
    return true;
}</code></pre>
            <p>
                We know that two strings aren't equal if their lengths aren't equal. We also know that if one byte isn't
                equal, then the strings as a whole aren't equal. Pretty simple, right? Yes, and the <a href="https://godbolt.org/#g:!((g:!((g:!((h:codeEditor,i:(filename:'1',fontScale:14,fontUsePx:'0',j:1,lang:zig,selection:(endColumn:2,endLineNumber:14,positionColumn:2,positionLineNumber:14,selectionStartColumn:2,selectionStartLineNumber:14,startColumn:2,startLineNumber:14),source:'const+std+%3D+@import(%22std%22)%3B%0A%0Aexport+fn+isHelloWorldA(str:+%5B*%5Dconst+u8,+len:+usize)+bool+%7B%0A++++const+hello+%3D+%22Hello,+world!!%5Cn%22%3B%0A++++if+(len+!!%3D+hello.len)+return+false%3B%0A++++return+staticEqlA(hello.len,+hello.*,+str)%3B%0A%7D%0A%0Afn+staticEqlA(comptime+len:+usize,+comptime+a:+%5Blen%5Du8,+b:+%5B*%5Dconst+u8)+bool+%7B%0A++++for+(0..len)+%7Cidx%7C+%7B%0A++++++++if+(a%5Bidx%5D+!!%3D+b%5Bidx%5D)+return+false%3B%0A++++%7D%0A++++return+true%3B%0A%7D'),l:'5',n:'0',o:'Zig+source+%231',t:'0')),header:(),k:50,l:'4',m:100,n:'0',o:'',s:0,t:'0'),(g:!((h:compiler,i:(compiler:z0141,filters:(b:'0',binary:'1',binaryObject:'1',commentOnly:'0',debugCalls:'1',demangle:'0',directives:'0',execute:'1',intel:'0',libraryCode:'0',trim:'1',verboseDemangling:'0'),flagsViewOpen:'1',fontScale:14,fontUsePx:'0',j:1,lang:zig,libs:!(),options:'-OReleaseFast+-fomit-frame-pointer+-mcpu%3Dznver5+-target+x86_64-linux',overrides:!(),selection:(endColumn:28,endLineNumber:8,positionColumn:28,positionLineNumber:8,selectionStartColumn:28,selectionStartLineNumber:8,startColumn:28,startLineNumber:8),source:1),l:'5',n:'0',o:'+zig+0.14.1+(Editor+%231)',t:'0')),header:(),k:50,l:'4',n:'0',o:'',s:0,t:'0')),l:'2',n:'0',o:'',t:'0')),version:4" target="_blank">generated assembly</a> for this function
                reflects that. There's a slight issue though. We need to load individual bytes from both strings,
                comparing them individually. It would be nice if there was some way to optimize this. We could use SIMD
                here, chunking the input strings and comparing them block by block, but we would still be loading from
                two separate strings. In most cases, we already know one of the strings at compile-time. Can we do
                better? Yes:
            </p>
            <pre><code>fn staticEql(comptime a: []const u8, b: []const u8) bool {
    if (a.len != b.len) return false;
    for (0..a.len) |idx| {
        if (a[idx] != b[idx]) return false;
    }
    return true;
}</code></pre>
            <p>
                The difference here is that one of the strings is required to be known at compile-time. The compiler can
                use this new information to <a href="https://godbolt.org/#g:!((g:!((g:!((h:codeEditor,i:(filename:'1',fontScale:14,fontUsePx:'0',j:1,lang:zig,selection:(endColumn:1,endLineNumber:4,positionColumn:1,positionLineNumber:4,selectionStartColumn:1,selectionStartLineNumber:4,startColumn:1,startLineNumber:4),source:'export+fn+isHello(str:+%5B*%5Dconst+u8,+len:+usize)+bool+%7B%0A++++return+staticEql(%22Hello!!%5Cn%22,+str%5B0..len%5D)%3B%0A%7D%0A%0Afn+staticEql(comptime+a:+%5B%5Dconst+u8,+b:+%5B%5Dconst+u8)+bool+%7B%0A++++if+(a.len+!!%3D+b.len)+return+false%3B%0A++++for+(0..a.len)+%7Cidx%7C+%7B%0A++++++++if+(a%5Bidx%5D+!!%3D+b%5Bidx%5D)+return+false%3B%0A++++%7D%0A++++return+true%3B%0A%7D'),l:'5',n:'0',o:'Zig+source+%231',t:'0')),header:(),k:48.0168776371308,l:'4',n:'0',o:'',s:0,t:'0'),(g:!((h:compiler,i:(compiler:z0141,filters:(b:'0',binary:'1',binaryObject:'1',commentOnly:'0',debugCalls:'1',demangle:'0',directives:'0',execute:'1',intel:'0',libraryCode:'0',trim:'1',verboseDemangling:'0'),flagsViewOpen:'1',fontScale:14,fontUsePx:'0',j:1,lang:zig,libs:!(),options:'-OReleaseFast+-fomit-frame-pointer+-mcpu%3Dznver5+-target+x86_64-linux',overrides:!(),selection:(endColumn:1,endLineNumber:1,positionColumn:1,positionLineNumber:1,selectionStartColumn:1,selectionStartLineNumber:1,startColumn:1,startLineNumber:1),source:1),l:'5',n:'0',o:'+zig+0.14.1+(Editor+%231)',t:'0'),(h:output,i:(compilerName:'zig+trunk',editorid:1,fontScale:14,fontUsePx:'0',j:1,wrap:'1'),l:'5',n:'0',o:'Output+of+zig+0.14.1+(Compiler+%231)',t:'0')),header:(),k:51.983122362869196,l:'4',m:100,n:'0',o:'',s:0,t:'0')),l:'2',n:'0',o:'',t:'0')),version:4" target="_blank">produce
                    improved</a> assembly code:
            </p>
            <pre><code>isHello:
        cmp     rsi, 7
        jne     .LBB0_8
        cmp     byte ptr [rdi], 72
        jne     .LBB0_8
        cmp     byte ptr [rdi + 1], 101
        jne     .LBB0_8
        cmp     byte ptr [rdi + 2], 108
        jne     .LBB0_8
        cmp     byte ptr [rdi + 3], 108
        jne     .LBB0_8
        cmp     byte ptr [rdi + 4], 111
        jne     .LBB0_8
        cmp     byte ptr [rdi + 5], 33
        jne     .LBB0_8
        cmp     byte ptr [rdi + 6], 10
        sete    al
        ret
.LBB0_8:
        xor     eax, eax
        ret</code></pre>
            <p>
                Is this not amazing? We just used <code>comptime</code> to make a function which compares a string
                against <code>"Hello!\n"</code>, and the assembly will run much faster than the naive comparison
                function. It's unfortunately still not perfect. Because we know the length of the expected string at
                compile-time, we can compare much larger sections of text at a time, instead of just byte-by-byte:
            </p>
            <pre><code>const std = @import("std");

fn staticEql(comptime a: []const u8, b: []const u8) bool {
    const block_len = std.simd.suggestVectorLength(u8) orelse @sizeOf(usize);

    // Exit early if the string lengths don't match up
    if (a.len != b.len) return false;

    // Find out how many large "blocks" we can compare at a time
    const block_count = a.len / block_len;
    // Find out how many extra bytes we need to compare
    const rem_count = a.len % block_len;

    // Compare "block_len" bytes of text at a time
    for (0..block_count) |idx| {
        const Chunk = std.meta.Int(.unsigned, block_len * 8);
        const a_chunk: Chunk = @bitCast(a[idx * block_len ..][0..block_len].*);
        const b_chunk: Chunk = @bitCast(b[idx * block_len ..][0..block_len].*);
        if (a_chunk != b_chunk) return false;
    }

    // Compare the remainder of bytes in both strings
    const Rem = std.meta.Int(.unsigned, rem_count * 8);
    const a_rem: Rem = @bitCast(a[block_count * block_len ..][0..rem_count].*);
    const b_rem: Rem = @bitCast(b[block_count * block_len ..][0..rem_count].*);
    return a_rem == b_rem;
}</code></pre>
            <p>
                Ok, so it's a bit more complex than the first example. Is it worth it though? Yep. The <a href="https://godbolt.org/#g:!((g:!((g:!((h:codeEditor,i:(filename:'1',fontScale:14,fontUsePx:'0',j:1,lang:zig,selection:(endColumn:1,endLineNumber:6,positionColumn:1,positionLineNumber:6,selectionStartColumn:1,selectionStartLineNumber:6,startColumn:1,startLineNumber:6),source:'const+std+%3D+@import(%22std%22)%3B%0A%0Aexport+fn+isHelloWorld(str:+%5B*%5Dconst+u8,+len:+usize)+bool+%7B%0A++++return+staticEql(%22Hello,+World!!%5Cn%22,+str%5B0..len%5D)%3B%0A%7D%0A%0Afn+staticEql(comptime+a:+%5B%5Dconst+u8,+b:+%5B%5Dconst+u8)+bool+%7B%0A++++const+block_len+%3D+std.simd.suggestVectorLength(u8)+orelse+@sizeOf(usize)%3B%0A%0A++++//+Exit+early+if+the+string+lengths+don!'t+match+up%0A++++if+(a.len+!!%3D+b.len)+return+false%3B%0A%0A++++//+Find+out+how+many+large+%22blocks%22+we+can+compare+at+a+time%0A++++const+block_count+%3D+a.len+/+block_len%3B%0A++++//+Find+out+how+many+extra+bytes+we+need+to+compare%0A++++const+rem_count+%3D+a.len+%25+block_len%3B%0A%0A++++//+Compare+%22block_len%22+bytes+of+text+at+a+time%0A++++for+(0..block_count)+%7Cidx%7C+%7B%0A++++++++const+Chunk+%3D+std.meta.Int(.unsigned,+block_len+*+8)%3B%0A++++++++const+a_chunk:+Chunk+%3D+@bitCast(a%5Bidx+*+block_len+..%5D%5B0..block_len%5D.*)%3B%0A++++++++const+b_chunk:+Chunk+%3D+@bitCast(b%5Bidx+*+block_len+..%5D%5B0..block_len%5D.*)%3B%0A++++++++if+(a_chunk+!!%3D+b_chunk)+return+false%3B%0A++++%7D%0A%0A++++//+Compare+the+remainder+of+bytes+in+both+strings%0A++++const+Rem+%3D+std.meta.Int(.unsigned,+rem_count+*+8)%3B%0A++++const+a_rem:+Rem+%3D+@bitCast(a%5Bblock_count+*+block_len+..%5D%5B0..rem_count%5D.*)%3B%0A++++const+b_rem:+Rem+%3D+@bitCast(b%5Bblock_count+*+block_len+..%5D%5B0..rem_count%5D.*)%3B%0A++++return+a_rem+%3D%3D+b_rem%3B%0A%7D'),l:'5',n:'0',o:'Zig+source+%231',t:'0')),header:(),k:56.046814044213264,l:'4',n:'0',o:'',s:0,t:'0'),(g:!((h:compiler,i:(compiler:z0141,filters:(b:'0',binary:'1',binaryObject:'1',commentOnly:'0',debugCalls:'1',demangle:'0',directives:'0',execute:'1',intel:'0',libraryCode:'0',trim:'1',verboseDemangling:'0'),flagsViewOpen:'1',fontScale:14,fontUsePx:'0',j:1,lang:zig,libs:!(),options:'-OReleaseFast+-fomit-frame-pointer+-mcpu%3Dznver5+-target+x86_64-linux',overrides:!(),selection:(endColumn:1,endLineNumber:1,positionColumn:1,positionLineNumber:1,selectionStartColumn:1,selectionStartLineNumber:1,startColumn:1,startLineNumber:1),source:1),l:'5',n:'0',o:'+zig+0.14.1+(Editor+%231)',t:'0'),(h:output,i:(compilerName:'zig+trunk',editorid:1,fontScale:14,fontUsePx:'0',j:1,wrap:'1'),l:'5',n:'0',o:'Output+of+zig+0.14.1+(Compiler+%231)',t:'0')),header:(),k:43.953185955786736,l:'4',m:100,n:'0',o:'',s:0,t:'0')),l:'2',n:'0',o:'',t:'0')),version:4" target="_blank">generated assembly</a> is much more optimal. Comparing larger chunks utilizes
                larger registers, and reduces the number of conditional branches in our code:
            </p>
            <pre><code>isHelloWorld:
        cmp     rsi, 14 ; The length of "Hello, World!\n"
        jne     .LBB0_1
        movzx   ecx, word ptr [rdi + 12]
        mov     eax, dword ptr [rdi + 8]
        movabs  rdx, 11138535027311
        shl     rcx, 32 ; Don't compare out-of-bounds data
        or      rcx, rax
        movabs  rax, 6278066737626506568
        xor     rax, qword ptr [rdi]
        xor     rdx, rcx
        or      rdx, rax ; Both chunks must match
        sete    al
        ret
.LBB0_1:
        xor     eax, eax
        ret</code></pre>
            <p>
                If you try to compare much larger strings, you'll notice that this more advanced function will generate
                assembly which uses the larger SIMD registers. Just testing against <code>"Hello, World!\n"</code>
                though, you can tell that we <a href="https://gist.github.com/RetroDev256/660824008ff5526ea785d8b3659c29f2" target="_blank">significantly improved</a> the runtime performance of this function. (<code>a</code>
                was the runtime-only function, <code>b</code> was the same function where one argument was known at
                compile-time, and <code>c</code> was the more advanced function).
            </p>
            <p><img src="https://alloc.dev/2025/06/07/string_bench.webp" width="100%" alt="basic comptime was 45% faster while advanced comptime was 70% faster">
        </p></div>

        <div id="runtime_at_comptime">
            <h2> Runtime-known data at <code>comptime</code>: </h2>
            <p>
                Zig's <code>comptime</code> powers aren't limited to compile-time. You can generate some number of
                procedures at compile-time for simple cases, and dynamically dispatch to the right procedure,
                falling-back to a fully runtime implementation if you don't want to bloat your binary:
            </p>
            <pre><code>fn dispatchFn(runtime_val: u32) void {
    switch (runtime_val) {
        inline 0...100 =&gt; |comptime_val| {
            staticFn(comptime_val);
        },
        else =&gt; runtimeFn(runtime_val),
    }
}

fn staticFn(comptime val: u32) void {
    _ = val; // ...
}

fn runtimeFn(runtime_val: u32) void {
    _ = runtime_val; // ...
}</code></pre>
        </div>

        <div id="conclusion">
            <h2> Conclusion: </h2>
            <p>
                Is <code>comptime</code> useful? I would say so. I use it every time I write Zig code. It fits into the
                language really well, and removes the need for templates, macros, generics, and manual code generation.
                Yes, you can do all of this with other languages, but it isn't nearly as clean. Whenever I use Zig, I
                feel it's easier to write performant code for *actually useful* scenarios. In other words, Zig is not
                the "Turing tar-pit".
            </p>
            <p>
                The possibilities are <a href="https://github.com/RetroDev256/comptime_suffix_automaton" target="_blank">only limited to your imagination</a>. If you are required to use a language without
                good generics, code generation, templates, macros, or comptime at your workplace, I feel sorry for you.
            </p>
            <p>
                Hopefully you enjoyed this article. If you did, please consider <a href="https://www.paypal.com/donate/?business=2Z3H3UQA37LML&amp;no_recurring=0&amp;item_name=Like+what+you+see?+A+dollar+or+two+will+help+me+learn+and+publish+more+-+Thanks!&amp;currency_code=USD" target="_blank">supporting me</a>. On a final note, I think it's time for the language wars to end.
                Turing completeness is all that we need, and the details fade away when we look for the bigger picture.
                Does this mean we can't have favorite languages? No, it does not. People will still mistakenly say "C is
                faster than Python", when the language isn't what they are benchmarking. On that note, enjoy this Zig
                propaganda:
            </p>
            <iframe width="100%" src="https://www.youtube.com/embed/0Ahr2XWymPk" title="Zig Go Brrrrrr" frameborder="0" allowfullscreen=""></iframe>
        </div>
    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Windows 10 spies on your use of System Settings (2021) (111 pts)]]></title>
            <link>https://www.michaelhorowitz.com/Windows10.spying.onsettings.php</link>
            <guid>44208050</guid>
            <pubDate>Sat, 07 Jun 2025 07:24:45 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.michaelhorowitz.com/Windows10.spying.onsettings.php">https://www.michaelhorowitz.com/Windows10.spying.onsettings.php</a>, See on <a href="https://news.ycombinator.com/item?id=44208050">Hacker News</a></p>
<div id="readability-page-1" class="page"><div> 
<p>October 23, 2021 </p> 

<p>I am a big fan of Nir Sofer. He creates <a href="https://www.nirsoft.net/">great utilities</a> for Windows and his software is both free and portable. Recently, he released a new DNS logging program <a href="https://www.nirsoft.net/utils/dns_lookup_view.html">DNSLookupView</a>. </p>

<p>He already had a DNS logging program called  <a href="https://www.nirsoft.net/utils/dns_query_sniffer.html">DNSQuerySniffer</a> but the new program gets its raw data from a different source. 
So, I have been running both of them side by side to see the pros/cons of each.</p> 

<p>And, while logging DNS requests on a Windows 10 machine, I came across some interesting stuff. The computer in question was running Windows 10 Home Edition, service pack 20H2, build 19042.1083, which means it has bug fixes as of July 6, 2021. It was logged on with a local account, not a Microsoft account. It was also locked down as much as possible. Every customization I could find in the Systems Settings was disabled. Many of the  scheduled telemetry tasks were disabled. </p>

<p>With the DNS logging programs running, I happened to look at the System Settings. </p><p>I didn't <i>change</i> anything, just <i>looked</i> at a couple settings.</p> 

 
<p>DNSLookupView showed that this generated two DNS queries, one for <span>www.bing.com</span> and another for 
<span>cxcs.microsoft.net</span>. If you are thinking WTF? So was I. You can see the DNS log below.</p>  
 <!-- centered image -->  
<p><img src="https://www.michaelhorowitz.com/pix/Win10.spies.dns.query.systemsettings.webp" alt="DNSLookupView output" title="DNSLookupView output"><br>
	 <span>&nbsp;DNSLookupView output&nbsp;</span>
</p> 


<p>The queries were made by program <span>C:\Windows\ImmersiveControlPanel\SystemSettings.exe</span>
and the query type was AAAA. The sub-domain <span>www.bing.com</span> resolved to ::ffff:13.107.21.200 and  ::ffff:204.79.197.200. 
The sub-domain <span>cxcs.microsoft.net</span> resolved to ::ffff:96.17.141.116. </p>

<p>Of course a DNS query is not data transmission. Typically, however, it immediately precedes a data transmission. </p>

<p>After seeing this the first time, I ran a second test that looked for data transmission. For this, I used another Nir Sofer program called <a href="https://www.nirsoft.net/utils/tcp_log_view.html">TcpLogView</a> that logs every outbound TCP request. If Windows is phoning home via UDP, I would not see it. 
 
</p><p> Below is the output from TCPLogView for the exact same second (2:30 and 51 seconds) as the DNS queries  for <span>www.bing.com</span> and 
<span>cxcs.microsoft.net</span>. 

</p><p><img src="https://www.michaelhorowitz.com/pix/Win10.spies.tcplogview.systemsettings.webp" alt="TCPLogView showing outbound requests" title="TCPLogView showing outbound requests"><br>
	 <span>&nbsp;TCPLogView showing outbound requests&nbsp;</span>
</p> 
 

<p> We see that program <span>C:\Windows\ImmersiveControlPanel\SystemSettings.exe</span> made two outbound requests. 
The first was to port 443 (HTTPS) at IP address <span>13.107.21.200</span> which is <span>www.bing.com</span>. 
The second was to port 443 at IP address <span>96.17.141.116</span>  which is <span>cxcs.microsoft.net</span>. Twenty seconds after opening a socket to these two computers, <span>SystemSettings.exe</span> closed the socket. </p>

<p><b>Clearly Windows 10 spies on you any time you use the System Settings</b>.</p>
<p>What else are they spying on?</p>
<p>What I did is quite repeatable, feel free to try it yourself. </p> 

<p>NOTE: I did not log the actual data being transmitted between my computer and Microsoft. Since it went to the HTTPS port (443) it was surely encrypted. </p>

<p>I wondered what <span>cxcs.microsoft.net</span> is. When I viewed the site in a browser, it returned <span>2021.1019.1.0</span>, whatever that means. It was not even HTML, just plain text. A search for the sub-domain turned up a <a href="https://www.tenforums.com/network-sharing/168515-what-these-microsoft-domains-used.html">guess</a> that it might be the Microsoft customer experience center. There is/was a <a href="http://www.matthewtennant.net/microsoft">Microsoft Customer Experience Center</a> that went by CXC. I also found it on a  list of domains to be <a href="https://github.com/hl2guide/Filterlist-for-AdGuard-or-PiHole/blob/master/dns_disallowed_domains.txt">blocked</a>  by AdGuard or PiHole. </p> 

<p>As for <span>www.bing.com</span>, it clearly is doing double duty. More than just a search engine, it seems to be used for telemetry too.</p>
 
<p><b>BLOCKING CXCS.MICROSOFT.NET</b></p> 

<p>The first thing I did was block access to <span>cxcs.microsoft.net</span> (more on this below). Then, I ran the above test again to make sure that I could still use the System Settings app. And, another interesting thing turned up.</p>

<p>The System Settings worked without access to <span>cxcs.microsoft.net</span>, but in testing, I saw another DNS request, this time to 
<span>ctldl.windowsupdate.com</span>. I tested it twice and each time program SystemSettings.exe made DNS requests for <span>www.bing.com</span> and 
<span>ctldl.windowsupdate.com</span>. I had already blocked the Windows Update website so the only outbound request it could make was to Bing. </p>
<p>The next day, I tested again and the result is shown below. </p>

<p><img src="https://www.michaelhorowitz.com/pix/win10.spies.test2.webp" alt="DNS Queries tested the next day" title="DNS Queries tested the next day"><br>
	 <span>&nbsp;More DNS Queries the next day&nbsp;</span>
</p> 
<p>This time, program SystemSettings.exe tried to resolve all three servers. Two of them resolved to IP address zero thanks to my router. It made the one outbound TCP request that it could, to
 <span>www.bing.com</span> at 13.107.21.200 (port 443). My testing of the Settings app was very quick. I have to wonder if I looked at many different settings and actually changed a setting or two what other servers Windows would try to phone home to.</p>

<p>Clearly Bing is more than a search engine. It seems to be spying on us. </p>
 
<p><b>DONT SPY ON ME</b></p> 

<p>What are some <a href="https://defensivecomputingchecklist.com/">Defensive Computing</a> responses to this? </p>

<p>Personally, I modify my router (the <a href="https://routersecurity.org/pepwavesurfsoho.php">Pepwave Surf SOHO</a>) to resolve queries to domains that I want to block to zero, which is an invalid IP address. I can do this because the router includes a customizable DNS server.  This, however, is not something people with a consumer router or a router provided by their ISP can do. Pepwave/Peplink routers are for professional use and thus have advanced features such as internal DNS server. </p> 

<p>Another option for techies is <a href="https://pi-hole.net/">Pi Hole</a>, a DNS server that is typically installed on a Raspberry Pi computer that is connected to a router. It too, can force a domain or sub-domain to resolve to an invalid IP address and it should work with any router.</p>

<p>Note however, that when you are using a VPN or Tor (and probably Cloudflare WARP too) any DNS or firewall rules in the router are bypassed. DNS rules are also bypassed when using a web browser configured for Secure DNS. A number of VPNs offer tracker blocking, so maybe some of them block <span>cxcs.microsoft.net</span>. Maybe.</p>

<p>Focusing one computer, rather than an entire network, <a href="https://yogadns.com/">YogaDNS</a> should let you use <a href="https://nextdns.io/">NextDNS</a> system-wide. One big upside to NextDNS is that you can create customized Black/Block lists. If you don't want a computer to ever visit <span>cxcs.microsoft.net</span>, you can block it with NextDNS. I have not tried YogaDNS. </p> 

<p>Another approach for a single computer is to install a firewall that offers outbound control, something akin to Little Snitch on macOS. For inspiration see this August 2021 article by Michael 
  Bazzell: <a href="https://inteltechniques.com/blog/2021/08/03/minimizing-macos-telemetry/">Minimizing macOS Telemetry</a>. </p>
 
<p>Bing is an interesting case as the website is <i>both</i> involved in spying/telemetry and a search engine. Blocking it on the entire LAN means that it can not be used as a search engine, so I went old school and modified the "hosts" file in Windows to force <span>www.bing.com</span> to resolve to a zero IP address. Then, I flushed the Windows DNS cache and went back to the System Settings. </p>
 
<p>This time, however, I stayed in the Settings a bit and went to a few different sections. Previously, I had just gone in and out. The DNS query log is shown below. The spying Windows does is worse than it first appeared. It looks like Windows wants to record every section that you visit. </p>
 
 <p><img src="https://www.michaelhorowitz.com/pix/Win10.spies.test3.webp" alt="Kicking more tires in System Settings" title="Kicking more tires in System Settings"><br>
	 <span>&nbsp;Kicking more tires in System Settings&nbsp;</span>
</p> 

<p>All the DNS requests above resolved to the zero IP address, except for sub-domain <span>ecn.dev.virtualearth.net</span>.  I don't know if this domain was triggered by the Settings app or if Windows did it for unrelated reasons. It came from svchost rather than from SystemSettings.exe. There was an outbound request made to port 443 at 23.78.209.31 which is the IP address of the virtualearth domain.</p> 

<p>The requests to <b>wpad</b> are a bit off-topic. Not shown in these screen shots is  that DNSLookupView also reports the process ID that made the DNS request. This can be used to determine the specific Windows service that is running under the control of svchost.exe. On this particular PC, it was  the WinHTTP Web Proxy Auto-Discovery Service  (WinHTTPAutoProxySvc). According to Microsoft, the service handles the automatic discovery of a proxy configuration via the WPAD (Web Proxy Auto-Discovery) protocol. The service is running, the startup type is Manual and, even logged on as an Administrator, I can not disable it. The service should not be running, the WPAD feature is disabled. You can check this in Windows 10 at Settings -&gt; Network and Internet -&gt; Proxy (in left side vertical column). Fortunately, all the DNS requests are failing. For more on this see <a href="https://www.howtogeek.com/298460/disable-wpad-in-windows-to-stay-safe-on-public-wi-fi-networks/">Disable WPAD in Windows to Stay Safe on Public Wi-Fi Networks</a> by Chris Hoffman (March 2017). Yet another Windows annoyance. </p>
<!-- 
C:\Windows\system32\svchost.exe -k LocalServiceNetworkRestricted -p 
   WinHTTP implements the client HTTP stack and provides developers with a Win32 API and COM Automation component for sending HTTP requests and receiving responses. In addition, WinHTTP provides support for auto-discovering a proxy configuration via its implementation of the Web Proxy Auto-Discovery (WPAD) protocol.
-->
 

<p>The most important point, however, about this trace data is the <i>many</i> requests to both  <span>cxcs.microsoft.net</span> and <span>www.bing.com</span> by SystemSettings.exe. Microsoft <i>really</i> wants to know what we are doing.  </p>
<!-- query sniiffer program only shows A requestes not AAAA. It showed only two requests 
     c:\Windows\System32\Drivers\etc\hosts    ipconfig /flushdns   to flush cache -->
  
<p><b>HOSTS FILE</b></p> 

<p>Of the many defensive steps, modifying the hosts file should probably be avoided. DNS on Windows 10 is more complicated that it used to be with earlier versions of Windows. I suggest blocking
 <span>www.bing.com</span> network-wide (router, Pi Hole). If you wanted to do a Bing search, the simple work-around is to use DuckDuckGo which gets its search results from Bing.</p>
 
<p>To test blocking sub-domains with the hosts file, I configured it to resolve <span>www.bing.com</span> to 1.2.3.44 and <span>nh.craigslist.org</span> to 1.2.3.55. I did this just in case Microsoft treated either the zero IP address or Bing as a special case. After modifying the hosts file, I re-started Windows. Then I tested using the nslookup command, the ping command and a couple web browsers. The results were not what I expected. </p>

<p>The nslookup command returned valid IP addresses for both sub-domains, rather than the dummy IP addresses I put into the hosts file. Beats me why. DNS logging showed that nslookup queried my router for the IP addresses. </p>
<p>A ping to each of the two sub-domains was just the opposite, the ping command tried to contact the dummy IP addresses in the hosts file. </p> 
<p>Some DNS logging, while I was not using the computer, showed that program SearchApp.exe picked up the dummy IP addresses from the hosts file.</p>
   <!--    C:\Windows\SystemApps\Microsoft.Windows.Search_cw5n1h2txyewy\SearchApp.exe    -->  
<p>A number of different web browsers could not display either  <span>www.bing.com</span> or <span>nh.craigslist.org</span> so they were clearly getting their IP addresses from the hosts file. I was surprised that even two browsers using Secure DNS failed to display these pages, which means the Secure DNS servers were not used. Might be a bug. </p> 

  
<p><b>FINISHING UP</b></p> 

 <p>I don't think the spying that Microsoft does on Windows users gets the attention it deserves. </p> 
 
 <p>If you want to stop Microsoft/Windows from spying on you, the programs mentioned here are great at illuminating the servers that Windows communicates with. I block the sub-domains listed below and have not observed an obvious problem. </p><ol>
<li>browser.events.data.msn.com</li>
<li>browser.events.data.microsoft.com</li>
<li>config.edge.skype.com</li>
<li>cxcs.microsoft.net</li>
<li>evoke-windowsservices-tas.msedge.net</li>
<li>self.events.data.microsoft.com</li>
<li>settings-win.data.microsoft.com</li>
<li>settings.data.microsoft.com</li>
<li>umwatson.events.data.microsoft.com</li>
<li>watson.telemetry.microsoft.com</li></ol>

<p>Your mileage may vary :-) It is probably safest to block one or two of these at a time, just in case. </p>
<p>If you do not use any Microsoft services, you may be able to also block 
<span>login.live.com</span>   and <span>login.microsoftonline.com</span>.  The first one has appeared  quite often in my DNS request logs. </p>

 <p>As for me, I am going to try blocking  <span>www.bing.com</span> on my LAN and see if it interferes with anything. </p>
 
 <p>- - - - - - -</p>
 <p>Update:  A few things about that. DNS blocking in the router (or Pi Hole) is bypassed by browsers configured to use Secure DNS. It is also bypassed when using a VPN. Still, this is not all bad as we are mostly concerned with Windows using <span>www.bing.com</span> for telemetry when not using a web browser. On another note, I would have thought that other bing.com sub-domains such as <span>news.bing.com</span> or <span>images.bing.com</span> would not be affected by a DNS block of <span>www.bing.com</span>. Each sub-domain is a free agent. But, it turned out that Microsoft re-writes these to  <span>www.bing.com/news</span> and <span>www.bing.com/images/</span>, so they get blocked too. </p>
 
<p>For what its worth, I have also seen  DNS queries for <span>www.bing.com</span> from an instance of svchost that is running two services, the Base Filtering Engine and the Windows Defender Firewall.   Finally, program SearchApp.exe has also tried to resolve it. The full path was <br>  
C:\Windows\SystemApps\Microsoft.Windows.Search_cw5n1h2txyewy\SearchApp.exe</p>

<p>- - - - - - -</p>
<p>Much more comprehensive, but related, research was done  by Helge Klein in March 2021. He found that Windows  talks to 291 hosts/servers and 2,764 unique IP addresses. See <a href="https://helgeklein.com/blog/windows-os-services-apps-network-connection-target-hosts/">Windows OS, Services &amp; Apps: Network Connection Target Hosts</a>.</p>



  
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[The FAIR Package Manager: Decentralized WordPress infrastructure (148 pts)]]></title>
            <link>https://joost.blog/path-forward-for-wordpress/</link>
            <guid>44207503</guid>
            <pubDate>Sat, 07 Jun 2025 04:45:13 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://joost.blog/path-forward-for-wordpress/">https://joost.blog/path-forward-for-wordpress/</a>, See on <a href="https://news.ycombinator.com/item?id=44207503">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>Back in December, I wrote about the <a href="https://joost.blog/wordpress-leadership/">state of leadership in the WordPress ecosystem</a>. I shared how too much power rests with one person, and how the lack of clear governance puts contributors and businesses alike in difficult positions. That post ended with a call: <em>we need to lead</em>. That wasn’t rhetorical. It was a pivot.</p>



<p>Since then, a lot has happened. Today, I want to share what came next and where it’s going. This is the story of <strong>FAIR</strong>, a project I first named in that same blog post: <strong>Federated and Independent Repositories</strong>. At the time, it was just a placeholder for an idea. Now, it’s real, and it’s running.</p>



<h3 id="h-the-moment-the-ecosystem-shifted">The moment the ecosystem shifted</h3>



<p>In October, Sarah Savage <a href="https://aspirepress.org/a-vision-for-aspirepress-and-a-community-run-org-mirror/">published a post introducing AspirePress</a>, a community-run mirror of the WordPress.org plugin and theme repositories. That post was a spark. It showed what was technically possible, and voiced what many of us had been saying in smaller rooms for years: the ecosystem needed options.</p>



<p>Then, in early December, twenty core contributors <a href="https://www.therepository.email/wordpress-contributors-and-community-leaders-call-for-governance-reform-in-rare-open-letter">wrote an open letter</a> calling for governance reform in WordPress. These weren’t newcomers. They were committers, team leads, people who had spent years helping build the platform. Their message added weight to the concerns many had been feeling in private.</p>



<p>Shortly after, <a href="https://marucchi.com/wordpress-leadership-continued/">Karim Marucchi</a> and I published our thoughts. We proposed a dual-track forward: one that addressed technical issues like centralization, update bottlenecks, and discoverability, and one that introduced a different approach to governance, governance that would be transparent, accountable, and neutral.</p>



<p>That combination turned out to be a catalyst. Even more so after Matt <a href="https://wordpress.org/news/2025/01/jkpress/">blogged about it</a> with a sarcastic post on WordPress.org. People began reaching out, and conversations happening in parallel started to overlap.</p>



<h3 id="h-from-parallel-threads-to-shared-momentum">From parallel threads to shared momentum</h3>



<p>A number of different groups had been working on related problems, each bringing different perspectives, needs, and capabilities. Rather than announce a new umbrella, we focused on connecting the dots between these parallel efforts. We’re not naming everyone in that alignment publicly yet, but it’s safe to say: this <em>is</em> a group of groups, and that’s its strength.</p>



<p>What followed were weeks of collaboration. We mapped out which parts of the ecosystem needed reinforcement. Some things were urgent: plugin update services, the plugin and theme directories, static assets like emojis and avatars, and the dashboard feeds. We began with mirrors and drop-in replacements, but the plan was always broader than that.</p>



<p>We designed a system that could serve as a full distribution layer for WordPress. It would work with the core as it exists today, stay compatible while removing the bottlenecks created by centralization, and be governed independently of .org.</p>



<h3 id="h-fair-federated-independent-and-live">FAIR: federated, independent, and live</h3>



<p>FAIR is now a technical project under the Linux Foundation. Its technology is governed by a community-led Technical Steering Committee (TSC) and built by contributors from across the WordPress ecosystem. The TSC chose three amazing community leaders as their chairs: Carrie Dils, Mika Epstein and Ryan McCue. All together, they built, in a relatively short amount of time, a decentralized package management system, federation-ready mirrors, support for commercial plugins, cryptographic signing, and more.</p>



<p>The goal of FAIR is <strong>not</strong> to fork WordPress. We’re still using the same core software. We’re not building a separate platform. What we are doing is adding a new distribution layer and putting our own governance on top of it. It’s a new path within WordPress, not outside it.</p>



<p>You can still install WordPress from WordPress.org, and that won’t change. But if you want more control over how plugins are delivered, or a system that supports decentralization, FAIR gives you that choice.</p>



<p>This work builds on tools like Composer and package managers from the Linux world, but with a clear focus on usability for real WordPress users. Most people won’t even need to know how it works under the hood. They’ll just know it works.</p>



<h3 id="h-why-this-matters">Why this matters</h3>



<p>When I wrote about WordPress leadership, I meant it. Change in open source doesn’t happen from the outside. It happens when people show up, do the work, and offer a better option. That’s what we’ve done.</p>



<p>FAIR is not a protest. It’s not a fork. It is a <strong>contribution</strong>. It reflects our belief that WordPress deserves better infrastructure and more accountable governance, and that we can build that ourselves, together.</p>



<p>This project is the result of months of collaboration across companies, countries, and communities. Dozens and dozens of people have already worked on it, and more are joining every day.</p>



<p>You can learn more at <strong><a href="https://fair.pm/">fair.pm</a></strong>. There’s plenty of work still ahead, but the foundations are in place, and the path is open.</p>



<p>If you believe in the open web, in WordPress as shared infrastructure, and in a future where the people who contribute get a say in how the platform evolves: join us!</p>



<p>Other people we’ve been collaborating with have blogged as well:</p>



<ul>
<li><a href="https://marucchi.com/introducing-the-fair-package-manager-for-wordpress/">Karim Marucchi</a></li>



<li><a href="https://journal.rmccue.io/488/">Ryan McCue</a></li>



<li><a href="https://siobhanmckeown.com/a-way-forward-with-fair/">Siobhan McKeown</a></li>
</ul>



<p>Plus the <a href="https://www.linuxfoundation.org/press/linux-foundation-announces-the-fair-package-manager-project-for-open-source-content-management-system-stability">press release from the Linux Foundation</a>.</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Getting Past Procrastination (179 pts)]]></title>
            <link>https://spectrum.ieee.org/getting-past-procastination</link>
            <guid>44207095</guid>
            <pubDate>Sat, 07 Jun 2025 03:06:55 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://spectrum.ieee.org/getting-past-procastination">https://spectrum.ieee.org/getting-past-procastination</a>, See on <a href="https://news.ycombinator.com/item?id=44207095">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-headline="Getting Past Procastination"><p><em>This article is crossposted from </em><a href="https://spectrum.ieee.org/zaporizhzhia-nuclear-power-plant" target="_self">IEEE Spectrum</a><em>’s careers newsletter. <a href="https://engage.ieee.org/Career-Alert-Sign-Up.html" rel="noopener noreferrer" target="_blank"><em>Sign up now</em></a><em> to get insider tips, expert advice, and practical strategies, <em><em>written i<em>n partnership with tech career development company <a href="https://jointaro.com/" rel="noopener noreferrer" target="_blank">Taro</a> and </em></em></em>delivered to your inbox for free!</em></em></p><p>Across a decade working at hypergrowth tech companies like <a href="https://spectrum.ieee.org/tag/meta">Meta</a> and <a href="https://spectrum.ieee.org/tag/pinterest">Pinterest</a>, I constantly struggled with procrastination. I’d be assigned an important project, but I simply couldn’t get myself to get started. The source of my distraction varied—I would constantly check my email, read random documentation, or even scroll through my social feeds. But the result was the same: I felt a deep sense of dread that I was not making progress on the things that mattered.</p><p>At the end of the day, time is the only resource that matters. With every minute, you are making a decision about how to spend your life. Most of the ways people spend their time are ineffective. Especially in the tech world, our tasks and tools are constantly changing, so we must be able to adapt. What separates the best engineers from the rest of the pack is that they create systems that allow them to be consistently productive.</p><p>Here’s the core idea that changed my perspective on productivity: <strong>Action leads to motivation</strong>, not the other way around. You should not check your email or scroll <a href="https://spectrum.ieee.org/tag/instagram">Instagram</a> while you wait for motivation to “hit you.” Instead, just start doing something, anything, that makes progress toward your goal, and you’ll find that motivation will follow.</p><p>For example, if I have a high-priority, complex bug-fixing challenge at work, my approach is to decompose the problem into something much simpler. <em>Could I simply</em> add a log statement that prints the value of a relevant variable? My goal at this point is not to solve the bug, it’s simply to take a tiny step forward.</p><p>This creates a powerful <a href="https://spectrum.ieee.org/tag/flywheel">flywheel</a>: you’re productive → you feel good → you’re more productive.</p><p>Unfortunately, many engineers are stuck in the opposite flywheel, a downward spiral of procrastination: you’re unproductive → you feel bad → you’re unproductive.</p><p>The idea that motivation follows naturally from progress lets us lower the activation energy needed to enter the upward spiral. Author and motivational speaker Tony Robbins talks about a related concept that “motion creates emotion.” The actions we take, and even the way we move our body, affect how we feel. Once you realize that you can control your motivation, you can achieve stress-free productivity.</p><p>—Rahul</p><h2><a href="https://connect.ieee.org/NzU2LUdQSC04OTkAAAGaz-6IXRxzqsX3H7CzoAP8tlH0fjowYKuaOpFAGfQg611FSGQCPicCY_Zu77pP38Bq7HyWSAs=" target="_blank">Overcoming Tech Workforce Shortages With IEEE Microcredentials</a></h2><p><span>A shortage of technical workers is coming. Currently, most of these roles require university degrees, but specialized training through focused, skills-based microcredential courses could provide an alternative and expand the workforce. IEEE’s microcredentials program offers credentials that focus on the skills needed to become a technician, electrician, or programmer, regardless of educational background.</span></p><p><a href="https://connect.ieee.org/NzU2LUdQSC04OTkAAAGaz-6IXRxzqsX3H7CzoAP8tlH0fjowYKuaOpFAGfQg611FSGQCPicCY_Zu77pP38Bq7HyWSAs=" target="_blank" title="Read more here.">Read more here.</a></p><h2><a href="https://connect.ieee.org/NzU2LUdQSC04OTkAAAGaz-6IXexwH67TVCzP_FrLeWBMKMZkkLIpLauUNIFTyCH7znsUKaiGLKVc-0hYeTdBLeD5zds=" target="_blank" title="How Software Engineers Actually Use AI">How Software Engineers Actually Use AI</a></h2><p><a href="https://connect.ieee.org/NzU2LUdQSC04OTkAAAGaz-6IXRxzqsX3H7CzoAP8tlH0fjowYKuaOpFAGfQg611FSGQCPicCY_Zu77pP38Bq7HyWSAs=" target="_blank" title="Read more here."></a><span>Amidst conflicting accounts of how <a href="https://spectrum.ieee.org/tag/programmers">programmers</a> use AI on the job, Wired surveyed 730 coders to get more clarity—then used <a href="https://spectrum.ieee.org/tag/chatgpt">ChatGPT</a> to comb through the data, with plenty of help from human editors and fact-checkers. The survey asked coders how much they use AI, their outlook on the technology, and how it has changed their jobs, among other questions.</span></p><p><a href="https://connect.ieee.org/NzU2LUdQSC04OTkAAAGaz-6IXexwH67TVCzP_FrLeWBMKMZkkLIpLauUNIFTyCH7znsUKaiGLKVc-0hYeTdBLeD5zds=" target="_blank" title="Read more here.">Read more here.</a></p><h2><a href="https://connect.ieee.org/NzU2LUdQSC04OTkAAAGaz-6IXn-WPi12NZwyS58w9WNKrtyQ406RJlXcxcYHA9l4y1kUGMCWFQCXUYSqJI-5igxkdCY=" rel="noopener noreferrer" target="_blank" title="Profile: A Knee Injury Launched This VR Pioneer’s Career">Profile: A Knee Injury Launched This VR Pioneer’s Career</a></h2><p>Unlike many engineers, Carolina Cruz-Neira had little interest in technology as a child. Instead, she dreamed of becoming a professional ballerina. But when an injury forced her to pivot, Cruz-Neira found success in computer science, eventually blending her interests in art and science as a pioneer in <a href="https://spectrum.ieee.org/tag/virtual-reality">virtual reality</a>. </p><p><a href="https://connect.ieee.org/NzU2LUdQSC04OTkAAAGaz-6IXn-WPi12NZwyS58w9WNKrtyQ406RJlXcxcYHA9l4y1kUGMCWFQCXUYSqJI-5igxkdCY=" rel="noopener noreferrer" target="_blank" title="Read more here.">Read more here.</a></p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Why are smokestacks so tall? (126 pts)]]></title>
            <link>https://practical.engineering/blog/2025/6/3/why-are-smokestacks-so-tall</link>
            <guid>44206553</guid>
            <pubDate>Sat, 07 Jun 2025 01:06:39 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://practical.engineering/blog/2025/6/3/why-are-smokestacks-so-tall">https://practical.engineering/blog/2025/6/3/why-are-smokestacks-so-tall</a>, See on <a href="https://news.ycombinator.com/item?id=44206553">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-block-type="2" id="item-683f0f4daf60562460eee335" data-layout-label="Post Body" data-type="item" data-updated-on="1748963236708">
  <p><em>[Note that this article is a transcript of the video embedded above.]</em></p><p>“The big black stacks of the Illium Works of the Federal Apparatus Corporation spewed acid fumes and soot over the hundreds of men and women who were lined up before the red-brick employment office.” That’s the first line of one of my favorite short stories, written by Kurt Vonnegut in 1955. It paints a picture of a dystopian future that, thankfully, didn’t really come to be, in part because of those stacks.</p><p>In some ways, air pollution is kind of a part of life. I’d love to live in a world where the systems, materials and processes that make my life possible didn’t come with any emissions, but it’s just not the case... From the time that humans discovered fire, we’ve been methodically calculating the benefits of warmth, comfort, and cooking against the disadvantages of carbon monoxide exposure and particulate matter less than 2.5 microns in diameter… Maybe not in that exact framework, but basically, since the dawn of humanity, we’ve had to deal with smoke one way or another.</p><p>Since, we can’t accomplish much without putting unwanted stuff into the air, the next best thing is to manage how and where it happens to try and minimize its impact on public health. Of course, any time you have a balancing act between technical issues, the engineers get involved, not so much to help decide where to draw the line, but to develop systems that can stay below it. And that’s where the smokestack comes in. Its function probably seems obvious; you might have a chimney in your house that does a similar job. But I want to give you a peek behind the curtain into the Illium Works of the Federal Apparatus Corporation of today and show you what goes into engineering one of these stacks at a large industrial facility. I’m Grady, and this is Practical Engineering.</p><p>We put a lot of bad stuff in the air, and in a lot of different ways. There are roughly 200 regulated hazardous air pollutants in the United States, many with names I can barely pronounce. In many cases, the industries that would release these contaminants are required to deal with them at the source. A wide range of control technologies are put into place to clean dangerous pollutants from the air before it’s released into the environment. One example is coal-fired power plants. Coal, in particular, releases a plethora of pollutants when combusted, so, in many countries, modern plants are required to install control systems. Catalytic reactors remove nitrous oxides. Electrostatic precipitators collect particulates. Scrubbers use lime (the mineral, not the fruit) to strip away sulfur dioxide. And I could go on. In some cases, emission control systems can represent a significant proportion of the costs involved in building and operating a plant. But these primary emission controls aren’t always feasible for every pollutant, at least not for 100 percent removal.</p><p>There’s a very old saying that “the solution to pollution is dilution.” It’s not really true on a global scale. Case in point: There’s no way to dilute the concentration of carbon dioxide in the atmosphere, or rather, it’s already as dilute as it’s going to get. But, it can be true on a local scale. Many pollutants that affect human health and the environment are short-lived; they chemically react or decompose in the atmosphere over time instead of accumulating indefinitely. And, for a lot of chemicals, there are concentration thresholds below which the consequences on human health are negligible. In those cases, dilution, or really dispersion, is a sound strategy to reduce their negative impacts, and so, in some cases, that’s what we do, particularly at major point sources like factories and power plants.</p><p>One of the tricks to dispersion is that many plumes are naturally buoyant. Naturally, I’m going to use my pizza oven to demonstrate this. Not all, but most pollutants we care about are a result of combustion; burning stuff up. So the plume is usually hot. We know hot air is less dense, so it naturally rises. And the hotter it is, the faster that happens. You can see when I first start the fire, there’s not much air movement. But as the fire gets hotter in the oven, the plume speeds up, ultimately rising higher into the air. That’s the whole goal: get the plume high above populated areas where the pollutants can be dispersed to a minimally-harmful concentration. It sounds like a simple solution - just run our boilers and furnaces super hot to get enough buoyancy for the combustion products to disperse.</p><p>The problem with the solution is that the whole reason we combust things is usually to recover the heat. So if you’re sending a lot of that heat out of the system, just because it makes the plume disperse better, you’re losing thermodynamic efficiency. It’s wasteful. That’s where the stack comes in. Let me put mine on and show you what I mean. I took some readings with the anemometers with the stack on and off. The airspeed with the stack on was around double with it off. About a meter per second compared with two. But it’s a little tougher to understand why.</p><p>It’s intuitive that as you move higher in a column of fluid, the pressure goes down (since there’s less weight of the fluid above). The deeper you dive in a pool, the more pressure you feel. The higher you fly in a plane or climb a mountain, the lower the pressure. The slope of that line is proportional to a fluid’s density. You don’t feel much of a pressure difference climbing a set of stairs because air isn’t very dense. If you travel the same distance in water, you’ll definitely notice the difference. So let’s look at two columns of fluid. One is the ambient air and the other is the air inside a stack. Since it’s hotter, the air inside the stack is less dense. Both columns start at the same pressure at the bottom, but the higher you go, the more the pressure diverges.</p><p>It’s kind of like deep sea diving in reverse. In water, the deeper you go into the dense water, the greater the pressure you feel. In a stack, the higher you are in a column of hot air, the more buoyant you feel compared to the outside air. This is the genius of a smoke stack. It creates this difference in pressure between the inside and outside that drives greater airflow for a given temperature. Here’s the basic equation for a stack effect. I like to look at equations like this divided into what we can control and what we can’t. We don’t get to adjust the atmospheric pressure, the outside temperature, and this is just a constant. But you can see, with a stack, an engineer now has two knobs to turn: the temperature of the gas inside and the height of the stack.</p><p>I did my best to keep the temperature constant in my pizza oven and took some airspeed readings. First with no stack. Then with the stock stack. Then with a megastack. By the way, this melted my anemometer; should have seen that coming. Thankfully, I got the measurements before it melted. My megastack nearly doubled the airspeed again at around three-and-a-half meters per second versus the two with just the stack that came with the oven. There’s something really satisfying about this stack effect to me. No moving parts or fancy machinery. Just put a longer pipe and you’ve fundamentally changed the physics of the whole situation. And it’s a really important tool in the environmental engineer’s toolbox to increase airflow upward, allowing contaminants to flow higher into the atmosphere where they can disperse. But this is not particularly revolutionary… unless you’re talking about the Industrial Revolution.</p><p>When you look at all the pictures of the factories in the 19th century, those stacks weren’t there to improve air quality, if you can believe it. The increased airflow generated by a stack just created more efficient combustion for the boilers and furnaces. Any benefits to air quality in the cities were secondary. With the advent of diesel and electric motors, we could use forced drafts, reducing the need for a tall stack to increase airflow. That was kind of the decline of the forests of industrial chimneys that marked the landscape in the 19th century. But they’re obviously not all gone, because that secondary benefit of air quality turned into the primary benefit as environmental rules about air pollution became stricter.</p><p>Of course, there are some practical limits that aren’t taken into account by that equation I showed. The plume cools down as it moves up the stack to the outside, so its density isn’t constant all the way up. I let my fire die down a bit so it wouldn’t melt the thermometer (learned my lesson), and then took readings inside the oven and at the top of the stack. You can see my pizza oven flue gas is around 210 degrees at the top of the mega-stack, but it’s roughly 250 inside the oven. After the success of the mega stack on my pizza oven, I tried the super-mega stack with not much improvement in airflow: about 4 meters per second. The warm air just got too cool by the time it reached the top. And I suspect that frictional drag in the longer pipe also contributed to that as well. So, really, depending on how insulating your stack is, our graph of height versus pressure actually ends up looking like this. And this can be its own engineering challenge. Maybe you’ve gotten back drafts in your fireplace at home because the fire wasn’t big or hot enough to create that large difference in pressure. </p><p>You can see there are a lot of factors at play in designing these structures, but so far, all we’ve done is get the air moving faster. But that’s not the end goal. The purpose is to reduce the concentration of pollutants that we’re exposed to. So engineers also have to consider what happens to the plume once it leaves the stack, and that’s where things really get complicated.</p><p>In the US, we have National Ambient Air Quality Standards that regulate six so-called “criteria” pollutants that are relatively widespread: carbon monoxide, lead, nitrogen dioxide, ozone, particulates, and sulfur dioxide. We have hard limits on all these compounds with the intention that they are met at all times, in all locations, under all conditions. Unfortunately, that’s not always the case. You can go on EPA’s website and look at the so-called “non-attainment” areas for the various pollutants. But we do strive to meet the standards through a list of measures that is too long to go into here. And that is not an easy thing to do.</p><p>Not every source of pollution comes out of a big stationary smokestack where it’s easy to measure and control. Cars, buses, planes, trucks, trains, and even rockets create lots of contaminants that vary by location, season, and time of day. And there are natural processes that contribute as well. Forests and soil microbes release volatile organic compounds that can lead to ozone formation. Volcanic eruptions and wildfires release carbon monoxide and sulfur dioxide. Even dust storms put particulates in the air that can travel across continents. And hopefully you’re seeing the challenge of designing a smoke stack. The primary controls like scrubbers and precipitators get most of the pollutants out, and hopefully all of the ones that can’t be dispersed. But what’s left over and released has to avoid pushing concentrations above the standards. That design has to work within the very complicated and varying context of air chemistry and atmospheric conditions that a designer has no control over.</p><p>Let me show you a demo. I have a little fog generator set up in my garage with a small fan simulating the wind. This isn’t a great example because the airflow from the fan is pretty turbulent compared to natural winds. You occasionally get some fog at the surface, but you can see my plume mainly stays above the surface, dispersing as it moves with the wind.  But watch what happens when I put a building downstream. The structure changes the airflow, creating a downwash effect and pulling my plume with it. Much more frequently you see the fog at the ground level downstream. And this is just a tiny example of how complex the behavior of these plumes can be. Luckily, there’s a whole field of engineering to characterize it.</p><p>There are really just two major transport processes for air pollution. Advection describes how contaminants are carried along by the wind. Diffusion describes how those contaminants spread out through turbulence. Gravity also affects air pollution, but it doesn’t have a significant effect except on heavier-than-air particulates. With some math and simplifications of those two processes, you can do a reasonable job predicting the concentration of any pollutant at any point in space as it moves and disperses through the air. Here’s the basic equation for that, and if you’ll join me for the next 2 hours, we’ll derive this and learn the meaning of each term… Actually, it might take longer than that, so let’s just look at a graphic. You can see that as the plume gets carried along by the wind, it spreads out in what’s basically a bell curve, or gaussian distribution, in the planes perpendicular to the wind direction. </p><p>But even that is a bit too simplified to make any good decisions with, especially when the consequences of getting it wrong are to public health. A big reason for that is atmospheric stability. And this can make things even more complicated, but I want to explain the basics, because the effect on plumes of gas can be really dramatic. You probably know that air expands as it moves upward; there’s less pressure as you go up because there is less air above you. And as any gas expands, it cools down. So there’s this relationship between height and temperature we call the adiabatic lapse rate. It’s about 10 degrees Celsius for every kilometer up or about 28 Fahrenheit for every mile up. But the actual atmosphere doesn’t always follow this relationship. For example, rising air parcels can cool more slowly than the surrounding air. This makes them warmer and less dense, so they keep rising, promoting vertical motion in a positive feedback loop called atmospheric instability. You can even get a temperature inversion where you have cooler air below warmer air, something that can happen in the early morning when the ground is cold. And as the environmental lapse rate varies from the adiabatic lapse rate, the plumes from stacks change.</p><p>In stable conditions, you usually get a coning plume, similar to what our gaussian distribution from before predicts. In unstable conditions, you get a lot of mixing, which leads to a looping plume. And things really get weird for temperature inversions because they basically act like lids for vertical movement. You can get a fanning plume that rises to a point, but then only spreads horizontally. You can also get a trapping plume, where the air gets stuck between two inversions. You can have a lofting plume, where the air is above the inversion with stable conditions below and unstable conditions above. And worst of all, you can have a fumigating plume when there are unstable conditions below an inversion, trapping and mixing the plume toward the ground surface. And if you pay attention to smokestacks, fires, and other types of emissions, you can identify these different types of plumes pretty easily.</p><p>Hopefully you’re seeing now how much goes into this. Engineers have to keep track of the advection and diffusion, wind speed and direction, atmospheric stability, the effects of terrain and buildings on all those factors, plus the pre-existing concentrations of all the criteria pollutants from other sources, which vary in time and place. All that to demonstrate that your new source of air pollution is not going to push the concentrations at any place, at any time, under any conditions, beyond what the standards allow. That’s a tall order, even for someone who loves gaussian distributions. And often the answer to that tall order is an even taller smokestack. But to make sure, we use software. The EPA has developed models that can take all these factors into account to simulate, essentially, what would happen if you put a new source of pollution into the world and at what height.</p><p>So why are smokestacks so tall? I hope you’ll agree with me that it turns out to be a pretty complicated question. And it’s important, right? These stacks are expensive to build and maintain. Those costs trickle down to us through the costs of the products and services we buy. They have a generally negative visual impact on the landscape. And they have a lot of other engineering challenges too, like resonance in the wind. And on the other hand, we have public health, arguably one of the most critical design criteria that can exist for an engineer. It’s really important to get this right. I think our air quality regulations do a lot to make sure we strike a good balance here. There are even rules limiting how much credit you can get for building a stack higher for greater dispersion to make sure that we’re not using excessively tall stacks in lieu of more effective, but often more expensive, emission controls and strategies.</p><p>In a perfect world, none of the materials or industrial processes that we rely on would generate concentrated plumes of hazardous gases. We don’t live in that perfect world, but we are pretty fortunate that, at least in many places on Earth, air quality is something we don’t have to think too much about. And to thank for it, we have a relatively small industry of environmental professionals who do think about it, a whole lot. You know, for a lot of people, this is their whole career; what they ponder from 9-5 every day. Something most of us would rather keep out of mind, they face it head-on, developing engineering theories, professional consensus, sensible regulations, modeling software, and more - just so we can breathe easy.</p>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[I Read All of Cloudflare's Claude-Generated Commits (165 pts)]]></title>
            <link>https://www.maxemitchell.com/writings/i-read-all-of-cloudflares-claude-generated-commits/</link>
            <guid>44205697</guid>
            <pubDate>Fri, 06 Jun 2025 22:35:01 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.maxemitchell.com/writings/i-read-all-of-cloudflares-claude-generated-commits/">https://www.maxemitchell.com/writings/i-read-all-of-cloudflares-claude-generated-commits/</a>, See on <a href="https://news.ycombinator.com/item?id=44205697">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>A few days ago, my CTO <a href="https://www.linkedin.com/in/christopher-ingebrigtsen" target="_blank" rel="noreferrer">Chris</a> shared Cloudflare's open-sourced <a href="https://github.com/cloudflare/workers-oauth-provider" target="_blank" rel="noreferrer">OAuth 2.1 library</a> that was almost entirely written by Claude. </p><p>What caught my attention wasn't just the technical achievement, but that they'd documented their entire creative process. Every prompt, every iteration, every moment of human intervention was preserved in git commit messages—creating what felt like an archaeological record of human-AI collaboration. Reading through their development history was like watching a real-time conversation (and sometimes struggle) between human intuition and artificial intelligence.</p><p>The lead engineer, <a href="https://github.com/kentonv" target="_blank" rel="noreferrer">@kentonv</a>, started as an AI skeptic. <i>"I was trying to validate my skepticism. I ended up proving myself wrong."</i> Two months later, Claude had generated nearly all of the code in what became a production-ready authentication library.</p><p>Kenton included the prompt used to generate code in every commit, which made this exploration possible. As we begin to lean more heavily on AI tools within development, this practice will become increasingly important. Sometimes, the original prompt is more valuable (and easier) to review than the resulting code—especially when an engineer declares an incorrect assumption that the model blindly follows.</p><p>This transparency transforms git history from a record of changes into a record of intent, creating a new form of documentation that bridges human reasoning and machine implementation.
</p><p>Reading through roughly 50 commits revealed some interesting patterns about how this collaboration actually unfolded:</p><p><b>Prompt by example.</b> Their initial prompt was a substantial code block showing exactly how the library would be used by a worker implementing it. This approach eliminated ambiguity about method signatures while revealing practical considerations that abstract specifications often miss. It's the difference between describing a dance and demonstrating the choreography.</p><p><b>"You did X, but we should do Y. pls fix."</b> The most effective prompts followed a consistent pattern: clear context about the current state, explanation of why change was needed, and specific direction forward. No elaborate instructions—just contextual feedback that felt remarkably like correcting a colleague.</p><p><i>Personal note: In my own experience with Cursor, I've found it helpful to include direct links to documentation. I can easily reference `@&lt;pasted_docs_link&gt;` in prompts, though I haven't used Claude Code enough to know an equivalent workflow there.</i></p><p><b>Documentation becomes effortless.</b> A single sentence prompt generated comprehensive schema documentation. I've noticed models excel at these documentation generation tasks—transforming what used to be tedious housekeeping into natural byproducts of development.
</p><p>About 20 commits in, I noticed Kenton had to step in manually. Claude couldn't properly move a class declaration and needed a follow-up commit just to fix the positioning. Later, Claude resorted to using `grep` and `sed` bash commands because its search-and-replace function couldn't handle duplicate code blocks.</p><p>One comment resonated with me: <i>"I really could have done this one faster by hand, oh well."</i></p><p>Around the 40-commit mark, manual commits became frequent—styling, removing unused methods, the kind of housekeeping that coding models still struggle with. It's clear that AI generated &gt;95% of the code, but human oversight was essential throughout.</p><p>If you're working with AI coding tools, a few observations from their approach:</p><p>- <b>Focus on the deliverable.</b> For a backend service, define the public endpoints and their expected behavior. For a CLI tool, show example usage. For a library, demonstrate the integration.</p><p>
- <b>Treat prompts as version-controlled assets.</b> Including prompts in commit messages creates valuable context for future maintenance and debugging.</p><p>
- <b>Expect multi-shot prompting.</b> Almost every feature required multiple iterations and refinements. This isn't a limitation—it's how the collaboration works.</p><p>
- <b>Don't be afraid to get your hands dirty.</b> Some bugs and styling issues are faster to fix manually than to prompt through. Knowing when to intervene is part of the craft.
</p><p>Reading through these commits sparked an idea: what if we treated prompts as the actual source code? Imagine version control systems where you commit the prompts used to generate features rather than the resulting implementation. When models inevitably improve, you could connect the latest version and regenerate the entire codebase with enhanced capability.</p><p>This approach would make business logic inherently self-documenting—anyone who reads English could understand when features were added and why. The prompts would become like genetic instructions, containing the essential information needed to grow the application.</p><p>Of course, this assumes models can achieve strict prompt adherence and requires a <b>very</b> high level of trust in the models. In practice, generated code still needs deployment, testing, and maintenance. But it raises a fascinating question: if all business logic could be contained within self-documenting prompts, might we eventually reach a point where this history of prompt commits itself becomes the "application," running directly in a model? Could we eliminate the intermediary code generation step entirely?</p><p>Anyways, back to the the current state of reality.
</p><p>This OAuth library represents something larger than a technical milestone—it's evidence of a new creative dynamic emerging. One where artificial intelligence handles mechanical implementation while humans provide direction, context, and judgment.</p><p>It's clear there was substantial human involvement throughout this process. We're still far from AI independently implementing libraries of this scope. Almost every feature required multi-shot prompting, some bugs resisted all attempts at automated fixes, certain features would have been completed faster manually, and a human had to create every prompt and provide strategic direction.</p><p>Despite these limitations, AI generated the vast majority of functional code in this library. Claude Code was publicly launched just two weeks ago, and it's already enabling this level of collaboration.</p><p>For now, it's just another tool. But unlike a hammer, this tool is improving itself, learning from every interaction, becoming more capable with each iteration.</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Medieval Africans had a unique process for purifying gold with glass (2019) (112 pts)]]></title>
            <link>https://www.atlasobscura.com/articles/medieval-african-gold</link>
            <guid>44205599</guid>
            <pubDate>Fri, 06 Jun 2025 22:21:39 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.atlasobscura.com/articles/medieval-african-gold">https://www.atlasobscura.com/articles/medieval-african-gold</a>, See on <a href="https://news.ycombinator.com/item?id=44205599">Hacker News</a></p>
<div id="readability-page-1" class="page"><section id="article-body">

            <p><span>When Sam Nixon, an archaeologist </span>with the British Museum, excavated ancient coin molds in Tadmekka, Mali, in 2005, it triggered a several-year exploration of how medieval Africans purified the gold they were using for their currency. Nixon had <a href="https://u7061146.ct.sendgrid.net/wf/click?upn=G62jSYfZdO-2F12d8lSllQB1LMA0487iNLTsD-2BsL-2Br-2FM1ekH9TzmVzi7UAEJ3iWeNQMaCbaI9bmVnA1yfk6nBC2BrTN8h2meP06ZRfBmFZ8yo-3D_sVPgwP-2Bf4iaXgRTGnGI2y6aKLMr-2BfKN0ysmZ-2FrtfI-2B-2FdqAX6JBxkyCSpJ4fgqiO38HPghmlsVzBRS-2BkDHVAVgKxS68KN7md8rgyf4tU7UUGf9cGf5n8OAIN3vZRWWXaE1HMteHN3H0KBWmt6-2BN7E99Da72Fz5I2kp-2BILeIMXzVMhf4Q5ruGz1qZn1cIUwKUCYFZ6oSMJCgvTQHzxoq5TKmlySrqZoX2MlUh4tgKHeDXgpsCwcy9bvACYhQZXiIzzUrWck96qN1vfT2XqSaLOAhw-2BmdgR8h8ECf9MSS21eV4-2F5wghRBiowYOraEZqDURCv6QoawVxUYXxUZx3C8AJ6pdn4li9sY0yb6-2BlUkWs994-3D">found</a> little droplets of highly refined gold left over in the molds—which have been dated to the 11th century—as well as curious fragments of glass. Now scientists have recreated the advanced process behind the purification method they used then.</p>
<p>“This is the first time in the archaeological record that we saw glass being used to be able to refine gold,” says Marc Walton, codirector of the <a href="https://scienceforart.northwestern.edu/">Center for Scientific Studies in the Arts</a>, a collaboration between Northwestern University and the Art Institute of Chicago. “The glass appeared to be material that was [actually] recycled glass materials … so it really shows the industriousness and creativity of the craftsmen, who understood the properties of gold and glass enough to [use them for] this process of refining gold.” The recycled glass materials were remnants of broken vessels. Tadmekka was a town right in the middle of the trans-Saharan caravan route, so Nixon uncovered several types of material culture that had to do with trade, namely molds for “bald dinar,” or coins that hadn’t been stamped with the name of a mint (or a 10th-century equivalent of one).</p>
<figure><img src="https://img.atlasobscura.com/5CACs1ObRmL6vVOATQuJDdCqKsp-aj01CzHwSk9PMf0/rt:fill/w:1200/el:1/q:81/sm:1/scp:1/ar:1/aHR0cHM6Ly9hdGxh/cy1kZXYuczMuYW1h/em9uYXdzLmNvbS91/cGxvYWRzL2Fzc2V0/cy82NzQ2M2NjZC0w/ODFkLTQwNmQtOWE5/Zi05NTNkZjMzMzFh/NTNjNzczMzU3MGM3/ODgwNjM0ZTZfQmxv/Y2sgQ2FyYXZhbnMs/IE1hbnNhIE11c2Eu/anBn.jpg" alt="An illuminated atlas depicts Mansa Musa, emperor of Mali from 1312 to 1337, widely known for his vast vast wealth in the form of gold deposits." width="auto" data-article-image-id="undefined" data-full-size-image="https://assets.atlasobscura.com/article_images/full/65140/image" data-kind="article-image" id="article-image-65140" loading="lazy"><figcaption>An illuminated atlas depicts Mansa Musa, emperor of Mali from 1312 to 1337, widely known for his vast vast wealth in the form of gold deposits. <span>Courtesy The Block Museum of Art</span></figcaption></figure>
<p>According to Walton, Europeans in the 10th and 11th centuries purified their gold through cupellation, a process in which lead is mixed with gold laced with impurities, and then heated in a furnace until the droplets of purer gold can be skimmed off. But in the case of medieval West Africans, “They were taking the ore and other raw materials from the river and mixing it with glass,” says Walton. Since gold is inert, it doesn’t fully dissolve into melted glass, while impurities and other materials do, making this “a really novel way of using recycled glass material.”</p>
<p>Walton’s team at the Center for Scientific Studies in the Arts replicates a lot of ancient technologies. To get to the bottom of how medieval Africans purified gold so successfully, they also made do with what their environment provided them. “We bought gold dust from a chemical supply company and then we mixed it with local Lake Michigan sand and then we made our own synthetic glass,” says Walton. “We heated it up, were able to dissolve the minerals in the sand, and were left behind with gold.”</p>
<p>Though Walton and his team updated this small-scale, bespoke process for purifying gold to modern times and adapted it to their environment in Illinois, their results show that the medieval Malian technique was both clever and sophisticated.</p>






          </section></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Falsehoods programmers believe about aviation (340 pts)]]></title>
            <link>https://flightaware.engineering/falsehoods-programmers-believe-about-aviation/</link>
            <guid>44205590</guid>
            <pubDate>Fri, 06 Jun 2025 22:20:33 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://flightaware.engineering/falsehoods-programmers-believe-about-aviation/">https://flightaware.engineering/falsehoods-programmers-believe-about-aviation/</a>, See on <a href="https://news.ycombinator.com/item?id=44205590">Hacker News</a></p>
<div id="readability-page-1" class="page">





	<section>

		





<main role="main">
	<article>
		<div>

			<section>
				<p>At FlightAware, our software needs to gracefully handle all sorts of weird and wonderful situations. While we as engineers might hope for aviation data to be clean and well-standardized, the real world is messy.</p><p>There are a lot of assumptions one could make when designing data types and schemas for aviation data that turn out to be inaccurate. In the spirit of <a href="https://www.kalzumeus.com/2010/06/17/falsehoods-programmers-believe-about-names/?ref=flightaware.engineering">Patrick McKenzie’s classic piece on names</a>, here are some false assumptions one might make about aviation. While many of these are simply common misconceptions, some of these assumptions have bitten our customers at various points, and others have caused issues in our own systems over the years.</p><p>Together they are illustrative of the situations that Hyperfeed, our flight tracking engine, is responsible for correctly interpreting in order to provide a clean and consistent data feed for our website, apps, and APIs.</p><h2 id="flights">Flights</h2><ul><li>Flights depart from a gate</li><li>Flights that depart from a gate <a href="https://www.flightaware.com/live/flight/AFR1/history/20250514/2040Z/KJFK/LFPG?ref=flightaware.engineering">only leave their gate once</a></li><li>Flights depart within a few hours of the time they were scheduled to</li><li>Flights depart <a href="https://www.flightaware.com/live/flight/PDT5965/history/20250508/2224Z/KCHO/KCLT?ref=flightaware.engineering">within a day</a> of the time they were scheduled to</li><li>Flights have schedules</li><li>Flights take off and land <a href="https://www.flightaware.com/live/flight/N144NE/history/20250518/1747Z/KPSM/L%2042.98589%20-71.12891?ref=flightaware.engineering">at airports</a></li><li>Airplanes (excluding helicopters) take off and land at airports</li><li>Flights are at most <a href="https://www.flightaware.com/live/flight/SIA21/history/20250516/1345Z/KEWR/WSSS?ref=flightaware.engineering">a dozen or so hours long</a></li><li>Okay, they’re at most <a href="https://www.flightaware.com/live/flight/HBAL812/history/20190717/1738Z?ref=flightaware.engineering">a few days long</a></li><li>Flights are identified by a flight number consisting of an airline’s code plus some numbers, like UAL1234</li><li>Flights <a href="https://www.flightaware.com/live/flight/C6031/history/20250521/1752Z/KBID/KFMH?ref=flightaware.engineering">are identified by either</a> an airline flight number like UAL1234, or the aircraft’s registration like N12345, B6459, or FHUVL</li><li>A flight identifier like B6459 is unambiguously either a registration (<a href="https://www.flightaware.com/live/flight/B6459?ref=flightaware.engineering" rel="noreferrer">B–6459</a>), an airline flight number (<a href="https://www.flightaware.com/live/flight/JBU459?ref=flightaware.engineering" rel="noreferrer">B6 459</a>), or something else</li><li>Flights don’t have <a href="https://en.wikipedia.org/wiki/Change_of_gauge_(aviation)?ref=flightaware.engineering">multiple flight numbers</a></li><li>Flights with multiple flight numbers unambiguously have one “main” flight number</li><li>A particular trip’s flight number(s) <a href="https://web.archive.org/web/20230328124705/https://community.southwest.com/t5/Blog/The-Science-behind-Flight-Numbers/ba-p/42760">never change</a></li><li>The flight number shown on your ticket <a href="https://www.eurocontrol.int/service/call-sign-similarity-service?ref=flightaware.engineering">is what the pilots and air traffic control are using</a></li><li>Flights don’t use the code of some entirely unrelated airline in their flight identifier</li><li>No flights use the same flight number within a day</li><li>Surely at least no flights use the same flight number at the same time?</li><li>Okay fine, separate flights from the same major passenger airline that depart within a few minutes of each other would not <a href="https://www.flightaware.com/live/flight/AAL2586/history/20250509/1935Z/TBPB/KCLT?ref=flightaware.engineering">both</a> have the <a href="https://www.flightaware.com/live/flight/AAL2586/history/20250508/1935Z/TBPB/KCLT?ref=flightaware.engineering">same</a> flight number… right?</li></ul><h2 id="airports">Airports</h2><ul><li>Airports <a href="https://en.wikipedia.org/wiki/Atat%C3%BCrk_Airport?ref=flightaware.engineering#Closure">never move</a></li><li>Terminal and gate numbers have a consistent naming scheme</li><li>Each runway is <a href="https://en.wikipedia.org/wiki/Hickam_Air_Force_Base?ref=flightaware.engineering">only used by one</a> airport </li><li>Airports always have two unique identifiers: a 4-letter Civil Aviation Organization (ICAO) code and a 3-letter International Air Transport Association (IATA) code</li><li>Airports always have three unique identifiers: an ICAO, an IATA, and a regionally-administered location code</li><li>The U.S. Department of Transportation <a href="https://www.bts.gov/topics/airlines-and-airports/world-airport-codes?ref=flightaware.engineering">assigns one</a> canonical code to <a href="https://www.faa.gov/air_traffic/flight_info/aeronav/aero_data/loc_id_search/Encodes_Decodes/?ref=flightaware.engineering">each airport</a> it oversees</li><li>No airports have <a href="https://en.wikipedia.org/wiki/EuroAirport_Basel_Mulhouse_Freiburg?ref=flightaware.engineering">multiple IATA codes</a></li><li>The ICAO code for airports in the U.S. <a href="https://www.flightaware.com/live/airport/PANC?ref=flightaware.engineering">always starts with the letter K</a></li><li>For U.S. airports whose ICAO code starts with K, the <a href="https://en.wikipedia.org/wiki/McClellan%E2%80%93Palomar_Airport?ref=flightaware.engineering">last three letters</a> are its IATA code</li><li>You can tell which <a href="https://www.flightaware.com/live/airport/NZIR?ref=flightaware.engineering">geographic region</a> an airport is in from its ICAO code</li><li>Everything that has an IATA code <a href="https://en.wikipedia.org/wiki/List_of_IATA-indexed_railway_stations,_bus_stations_and_ferry_terminals?ref=flightaware.engineering">is an airport</a></li><li>Everything that has an ICAO code <a href="https://en.wikipedia.org/wiki/Jezero_(crater)?ref=flightaware.engineering">is on Earth</a></li><li>Airports have at least one well-known identifier of some sort</li></ul><h2 id="airlines">Airlines</h2><ul><li>No <a href="https://en.wikipedia.org/wiki/SkyJet_Airlines?ref=flightaware.engineering">two</a> airlines <a href="https://en.wikipedia.org/wiki/Euroavia_Airlines?ref=flightaware.engineering">share</a> the same <a href="https://en.wikipedia.org/wiki/Airline_codes?ref=flightaware.engineering#IATA_airline_designator">IATA code</a></li><li>No <a href="https://en.wikipedia.org/wiki/EasyJet_UK?ref=flightaware.engineering">airlines</a> use <a href="https://en.wikipedia.org/wiki/EasyJet_Europe?ref=flightaware.engineering">multiple</a> IATA or ICAO <a href="https://en.wikipedia.org/wiki/EasyJet_Switzerland?ref=flightaware.engineering">codes</a></li><li>You can tell <a href="https://en.wikipedia.org/wiki/Aircraft_lease?ref=flightaware.engineering#Wet_lease">what airline is operating a flight</a> by looking at the physical aircraft</li><li>Airlines assign flight numbers to specific routes</li><li>Airlines only assign flight numbers to <a href="https://en.wikipedia.org/wiki/Codeshare_agreement?ref=flightaware.engineering">flights they operate</a></li><li>Airlines only assign flight numbers to <a href="https://www.flyertalk.com/forum/air-france-frequence-plus/1325488-how-fly-mlh-bsl.html?ref=flightaware.engineering">flights</a></li></ul><h2 id="navigation">Navigation</h2><ul><li>Waypoint names are unique</li><li>There is one <a href="https://en.wikipedia.org/wiki/Altitude?ref=flightaware.engineering#In_aviation">agreed-upon definition of altitude</a></li><li>Flight information from Air Navigation Service Providers is accurate</li><li>Okay, <em>pretty</em> accurate; they wouldn’t indicate that a flight had departed unless it really had</li><li>If they indicate that a flight plan has been cancelled, then that flight definitely isn’t going to operate — it wouldn’t simply be due to someone editing the flight plan</li><li>At least their radar data accurately identifies each aircraft</li><li>Radars with overlapping coverage areas agree on the location of a target they can both see</li><li>If they send us a flight plan with the ICAO identifier of a known airport as the destination, then there must have been some intention of arriving there</li><li>If an aircraft diverts to another destination, it won’t <a href="https://www.flightaware.com/live/flight/AAL1372/history/20250516/1410Z/KMIA/KRIC?ref=flightaware.engineering">divert again</a></li></ul><h2 id="transponders-and-ads-b">Transponders and ADS-B</h2><ul><li>ADS-B messages only come from aircraft</li><li>ADS-B messages only come from aircraft and airport service vehicles</li><li>ADS-B messages only come from vehicles of some kind</li><li>The GPS position in ADS-B messages <a href="https://en.wikipedia.org/wiki/Dilution_of_precision_(navigation)?ref=flightaware.engineering">is accurate</a></li><li>The GPS position in ADS-B messages is accurate <a href="https://en.wikipedia.org/wiki/Spoofing_attack?ref=flightaware.engineering#Global_navigation_satellite_system_spoofing">within some known uncertainty</a> radius</li><li>ADS-B messages always include the correct flight identification</li><li>Transponders are correctly programmed to indicate the aircraft type (helicopter, airplane, balloon, etc)</li><li>You can always determine a aircraft’s registration number from its ADS-B messages</li><li>Transponders are programmed with the correct Mode S address</li><li>All of the transponders on a single aircraft are programmed with the same Mode S address</li><li>Nobody will ever set their flight identification to weird things like NULL</li><li>People will remember to update the transponder when the aircraft’s registration changes</li><li>ADS-B messages are always received exactly as they were transmitted</li><li>No one ever transmits false ADS-B messages</li><li>Transponders never break and rodents never chew through cables</li></ul><hr><p>Thanks to my colleagues who contributed to or reviewed this collection of falsehoods: Mark Duell, Paul Durandt, Karina Elizondo, Matt Higgins, Thomas Kyanko, Nathan Reed, and Amy Szczepanski.</p>
			</section>
			
			

			


			


		<section>
			<a href="https://flightaware.engineering/">Back to home</a>
		</section>

			


		</div>
	</article>
</main>




		

		

	</section>

	

	


	



</div>]]></description>
        </item>
        <item>
            <title><![CDATA[Researchers develop ‘transparent paper’ as alternative to plastics (321 pts)]]></title>
            <link>https://japannews.yomiuri.co.jp/science-nature/technology/20250605-259501/</link>
            <guid>44205282</guid>
            <pubDate>Fri, 06 Jun 2025 21:43:10 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://japannews.yomiuri.co.jp/science-nature/technology/20250605-259501/">https://japannews.yomiuri.co.jp/science-nature/technology/20250605-259501/</a>, See on <a href="https://news.ycombinator.com/item?id=44205282">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="p-article-block" data-state="">


<figure>
<a href="https://japannews.yomiuri.co.jp/attachment/259499/" rel="attachment wp-att-259499"><img decoding="async" src="https://japannews.yomiuri.co.jp/wp-content/uploads/2025/06/transparent-paper1.jpg" alt=""></a>
<figcaption><span>Photos courtesy of JAMSTEC</span><br>Scenery 100 meters away is seen through a sheet of transparent paper which is 0.7 millimeters thick.</figcaption></figure>


<p><i></i>14:47 JST, June 5, 2025</p>

<p>A team of researchers with the Japan Agency for Marine-Earth Science and Technology (JAMSTEC) and other entities have developed thick sheets of transparent paper using cellulose, a material made from plant biomass.</p>
<p>The transparent paper sheets can be broken down by microbes into water and carbon dioxide. Also, they can be used to make containers because they are thicker than conventional cellulose-based materials. The new material is expected to replace plastics for this purpose, as plastics are a source of ocean pollution.</p>
<p>To make the paper sheets, the team used cellulose powder made from fibers found on the surfaces of cotton seeds. They dissolved the powder in a lithium bromide-water solution, mixing it in as they raised it to a high temperature until it became a gel, at which point the material was shaped and dried.</p>
<p>When the researchers shaped the material into cups and straws, they found that it was about as strong as polycarbonate, a type of plastic.</p>
<p>The paper sheets become transparent because they are packed tightly with nanometer-scale (one 1-billionth of a meter) fibers. The concentration of these fibers allows light to pass straight through the sheets without experiencing diffusion.</p>
<p>Even a sheet of the transparent paper that is 0.7 millimeters thick remains flexible, and scenery 100 meters away can be clearly seen through it.</p>
<p>On the assumption that the paper sheets will be washed into the ocean, the team investigated its biodegradability. The researchers sank the paper sheets into the ocean and checked whether microbes there were able to dissolve them.</p>
<p>The results showed that the deeper the place under the sea was, the slower the dissolution progressed, because there are fewer microbes in the deep ocean. But the paper sheets were mostly dissolved within four months even at 757 meters below the surface.</p>
<figure>
<a href="https://japannews.yomiuri.co.jp/attachment/259500/" rel="attachment wp-att-259500"><img decoding="async" src="https://japannews.yomiuri.co.jp/wp-content/uploads/2025/06/transparent-paper2.jpg" alt=""></a>
<figcaption><span>Photos courtesy of JAMSTEC</span><br>A cup and a straw made of the newly developed transparent paper</figcaption></figure>
<p>So far, paper packs have been the most common alternatives to plastic containers.</p>
<p>But business experts have pointed out that consumers are less willing to buy goods in paper packs because they cannot see the contents.</p>
<p>Transparent paper could overcome this problem, but bringing the material to market will require factories with the technology to mass-produce it.</p>
<p>Noriyuki Isobe, a deputy chief researcher for JAMSTEC, said, “If a plant for demonstration experiments of the technology is built, we estimate that the cost to produce the material will be about three times that of ordinary paper, while the volume of CO2 emissions can be kept to about half that of the plastic making process.”</p>
<p>Prof. Masaya Nogi of the University of Osaka, an expert on wooden materials, said, “There have been other types of transparent paper in the past, but the advantage of this over those is that it has been proven to be biodegradable in the deep sea.”</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Supreme Court allows DOGE to access social security data (141 pts)]]></title>
            <link>https://www.nbcnews.com/politics/supreme-court/supreme-court-trump-doge-social-security-data-access-elon-musk-rcna206515</link>
            <guid>44205060</guid>
            <pubDate>Fri, 06 Jun 2025 21:15:17 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.nbcnews.com/politics/supreme-court/supreme-court-trump-doge-social-security-data-access-elon-musk-rcna206515">https://www.nbcnews.com/politics/supreme-court/supreme-court-trump-doge-social-security-data-access-elon-musk-rcna206515</a>, See on <a href="https://news.ycombinator.com/item?id=44205060">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>WASHINGTON — The <a href="https://www.nbcnews.com/politics/supreme-court" target="_blank">Supreme Court</a> on Friday allowed members of the Trump administration's Department of Government Efficiency to access Social Security Administration data.</p><p>The conservative-majority court, with its three liberal justices objecting, granted an emergency application filed by the Trump administration asking the justices to lift an injunction issued by a federal judge in Maryland.</p><p>The <a href="https://www.supremecourt.gov/opinions/24pdf/24a1063_6j37.pdf" target="_blank">unsigned order</a> said that members of the DOGE team assigned to the Social Security Administration should have "access to the agency records in question in order for those members to do their work."</p><p>The lawsuit challenging DOGE’s actions was filed by progressive group Democracy Forward on behalf of two unions — the American Federation of State, County and Municipal Employees, and the American Federation of Teachers — as well as the Alliance for Retired Americans.</p><p>"This is a sad day for our democracy and a scary day for millions of people," the groups said in a statement. "This ruling will enable President Trump and DOGE’s affiliates to steal Americans’ private and personal data."</p><p>The White House praised the ruling.</p><p>"The Supreme Court allowing the Trump Administration to carry out commonsense efforts to eliminate waste, fraud, and abuse and modernize government information systems is a huge victory for the rule of law," White House spokesperson Liz Huston said in a statement.</p><p><a href="https://www.nbcnews.com/politics/trump-administration/live-blog/trump-administration-elon-musk-travel-ban-tariffs-live-updates-rcna210179" target="_blank"><em>Follow live politics coverage here</em></a></p><p>Liberal Justice Ketanji Brown Jackson wrote a dissenting opinion questioning the need for the court to intervene on an emergency basis.</p><p>"In essence, the 'urgency' underlying the government’s stay application is the mere fact that it cannot be bothered to wait for the litigation process to play out before proceeding as it wishes," she added.</p><p>DOGE, set up by billionaire Elon Musk before his falling out with President Donald Trump,<strong> </strong>says it wants to modernize systems and detect waste and fraud at the agency. The data it seeks includes Social Security numbers, medical records, and tax and banking information.</p><p>“These teams have a business need to access the data at their assigned agency and subject the government’s records to much-needed scrutiny,” Solicitor General D. John Sauer wrote in court papers.</p><p>The lawsuit alleged that allowing broader access to the personal information would violate a federal law called the Privacy Act as well as the Administrative Procedure Act.</p><p>"The agency is obligated by the Privacy Act and its own regulations, practices, and procedures to keep that information secure — and not to share it beyond the circle of those who truly need it," the challengers' lawyers wrote in court papers.</p><p>U.S. District Judge Ellen Hollander had ruled that DOGE had no need to access the specific data at issue. The 4th U.S. Circuit Court of Appeals, based in Richmond, Virginia, declined to block Hollander's decision, leading to the Trump administration to file its emergency request at the Supreme Court.</p><p>Social Security Administration Commissioner Frank Bisignano welcomed Friday's ruling by the high court.</p><p>“The Supreme Court’s ruling is a major victory for American taxpayers. The Social Security Administration will continue driving forward modernization efforts, streamlining government systems, and ensuring improved service and outcomes for our beneficiaries,” Bisignano said in a statement.</p><p>In a <a href="https://www.supremecourt.gov/orders/courtorders/060625zr_5426.pdf" target="_blank">separate order</a> issued Friday in another case involving DOGE, the Supreme Court granted another request filed by the Trump administration.</p><p>That decision allows the Trump administration to, for now, shield DOGE from freedom of information requests seeking thousands of pages of material.</p><p>The move <a href="https://www.nbcnews.com/politics/supreme-court/supreme-court-temporarily-allows-trump-administration-shield-doge-docu-rcna208628" target="_blank">formalizes a decision</a> issued by Chief Justice John Roberts on May 23 that temporarily put lower court decisions on hold while the Supreme Court considered what next steps to take. The court also told lower courts to limit the scope of what material could be disclosed.</p><p>It means the government will not have to respond to requests for documents and allow for the deposition of the DOGE administrator, Amy Gleason, as a lower court had ruled, while litigation continues.</p><p>The three liberal justices noted their disagreement with that decision, too.</p><p>A spokesman for Citizens for Responsibility and Ethics in Washington, which filed the lawsuit, said the group was "obviously disappointed" with the decision but "pleased that the court allowed discovery to proceed."</p><p>A Justice Department spokesman did not immediately respond to a request for comment on the order.</p></div><div><div data-activity-map="expanded-byline-article-bottom"><div data-testid="byline-thumbnail"><a href="https://www.nbcnews.com/author/lawrence-hurley-ncpn1298564" tabindex="-1"><picture data-testid="picture"><source media="(min-width: 320px)" srcset="https://media-cldnry.s-nbcnews.com/image/upload/t_focal-60x60,f_avif,q_auto:eco,dpr_2/newscms/2023_08/3595831/lawrence-hurley-byline-jm-1.jpg 2x, https://media-cldnry.s-nbcnews.com/image/upload/t_focal-60x60,f_auto,q_auto:best/newscms/2023_08/3595831/lawrence-hurley-byline-jm-1.jpg 1x"><img loading="lazy" src="https://media-cldnry.s-nbcnews.com/image/upload/t_focal-60x60,f_auto,q_auto:best/newscms/2023_08/3595831/lawrence-hurley-byline-jm-1.jpg" alt="" height="48" width="48"></picture></a></div><p><span data-testid="byline-name"><a href="https://www.nbcnews.com/author/lawrence-hurley-ncpn1298564">Lawrence Hurley</a></span><span><a href="https://x.com/lawrencehurley" target="_blank" rel="noopener noreferrer"><span></span></a><a href="mailto:Lawrence.Hurley@nbcuni.com" target="_blank" rel="noopener noreferrer"><span></span></a></span></p><p>Lawrence Hurley is a senior Supreme Court reporter for NBC News. </p></div><div data-activity-map="expanded-byline-article-bottom"><div data-testid="byline-thumbnail"><a href="https://www.nbcnews.com/author/gary-grumbach-ncpn752571" tabindex="-1"><picture data-testid="picture"><source media="(min-width: 320px)" srcset="https://media-cldnry.s-nbcnews.com/image/upload/t_focal-60x60,f_avif,q_auto:eco,dpr_2/newscms/2024_16/3645263/240415-gary-grumbach-mn-1145.jpg 2x, https://media-cldnry.s-nbcnews.com/image/upload/t_focal-60x60,f_auto,q_auto:best/newscms/2024_16/3645263/240415-gary-grumbach-mn-1145.jpg 1x"><img loading="lazy" src="https://media-cldnry.s-nbcnews.com/image/upload/t_focal-60x60,f_auto,q_auto:best/newscms/2024_16/3645263/240415-gary-grumbach-mn-1145.jpg" alt="" height="48" width="48"></picture></a></div><p><span data-testid="byline-name"><a href="https://www.nbcnews.com/author/gary-grumbach-ncpn752571">Gary Grumbach</a></span><span><a href="https://x.com/GaryGrumbach" target="_blank" rel="noopener noreferrer"><span></span></a><a href="mailto:Gary.Grumbach@nbcuni.com" target="_blank" rel="noopener noreferrer"><span></span></a></span></p><p>Gary Grumbach is a NBC News Legal Affairs Reporter, based in Washington, D.C.</p></div><div><p>Garrett Haake</p><!-- --><p> and </p><!-- --><p>Julia Jester</p><!-- --> <!-- --><p>contributed</p><!-- --><p>.</p></div></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Smalltalk, Haskell and Lisp (102 pts)]]></title>
            <link>https://storytotell.org/smalltalk-haskell-and-lisp</link>
            <guid>44204878</guid>
            <pubDate>Fri, 06 Jun 2025 20:55:34 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://storytotell.org/smalltalk-haskell-and-lisp">https://storytotell.org/smalltalk-haskell-and-lisp</a>, See on <a href="https://news.ycombinator.com/item?id=44204878">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>

        
<p>To assist with job interviews at the <span>NRAO</span> we recently wrote a small<span></span> <span>“</span>contest” program. Without giving away the details, the crux of the problem is to read a file with a list of scans and calculate the amount of time it takes to move the dishes and perform the scans, and report it. Candidates are required to write this in Java, but that restriction does not apply to me, so I of course had to write it in three languages that are not Java: Haskell, Common Lisp and Smalltalk.</p>
<p>I expect to learn something, or at least reinforce an existing fact, by doing these sorts of language games, but this time I really wasn’t expecting what I found. I discovered that my enjoyment of Haskell is pretty much independent of Haskell’s appropriateness or usability. In fact, it is more attributable to the way I feel when using it, in contrast to everything else. Let me show you a piece of code:</p>
<pre><code>-- | Perform a scan and produce the scan log for it.
performScan ∷ Scan → AntennaSimulator ScanLog
performScan scan = wrapDuration $ do
  slewTime     ← moveTo  $ scanPosition scan  -- move to the scan's position
  onSourceTime ← waitFor $ scanLength   scan  -- wait for the scan to finish
  return $ Log scan onSourceTime slewTime </code></pre>
<p>Somehow, the other languages I’ve used to implement this problem never arrived at a piece of code quite this beautiful. Compare to the Smalltalk:</p>
<pre><code>Scan ≫ runWith: anAntenna
runWith: anAntenna
  "Performs this scan on the supplied antenna."
  | onSourceTime slewTime |
  slewTime     := anAntenna moveTo: self position; lastActionDuration.
  onSourceTime := anAntenna wait: self duration; lastActionDuration.
  ^ ScanResult withScan: self slewTime: slewTime onSourceTime: onSourceTime</code></pre>
<p>And the Lisp:</p>
<pre><code>(defun run-scan (antenna scan)
  "Perform a scan on this antenna. Return a scan log."
  (let* ((slew-time       (move-to antenna (scan-position scan)))
     (on-source-time  (delay antenna (scan-length scan)))
     (total-time      (+ on-source-time slew-time)))
    (with-slots (last-duration) antenna
      (setf last-duration total-time)
      (make-scan-log scan slew-time on-source-time))))</code></pre>
<p>Strangely, despite the similarity of the Smalltalk, I still find the Haskell shorter and clearer. It’s also more modular, though it doesn’t look like it. The <code>wrapDuration</code> command arranges for the next set of actions to count as one as far as the <code>lastActionDuration</code> is concerned, but does not interfere with usages of <code>lastActionDuration</code> within the wrapped action. I found this notion essentially impossible to port to the other languages. The Haskell version really feels more like a composable set of actions for driving the antenna, whereas the Smalltalk version never really stopped feeling like an arithmetic fabrication, and the abstraction leaked a lot in the Lisp version</p>
<p>The Lisp code is just atrocious. I was never really able to put my finger on what it is I dislike about Lisp before. It’s this: while Lisp code is often clever and interesting, it also <em>always</em> feels like I’m tricking Lisp into doing what I want, rather than simply expressing what I want.</p>
<p>Neither Lisp nor Smalltalk really lend themselves to my style. I realize now, that my style is to try to break everything down into the smallest piece, a piece that doesn’t look like work. Haskell encourages this style with the <code>where</code> syntax, which encourages you to say, X is really Y + Z, with Y being this and Z being that. This kind of thing:</p>
<pre><code>-- | To calculate the time to move between two positions, we take
-- whichever is larger of the time to turn and the time to change
-- elevation.
timeToMoveTo ∷ Position → Position → Time
timeToMoveTo (sourceAz, sourceEl) (destAz, destEl) = 
  max rotationTime ascensionTime
    where
      rotationTime, ascensionTime ∷ Time
      rotationTime  = timeToRotate sourceAz destAz
      ascensionTime = timeToAscend sourceEl destEl</code></pre>
<p>This almost doesn’t look like code to me. I would expect any programmer to be able to at least get some sense of what’s going on here, even without knowing Haskell. I don’t think I could say the same thing about this piece of Lisp:</p>
<pre><code>(defun time-to-move (from-pos to-pos)
  "Calculates the total time to move between two positions, assuming
rotation and ascension can occur simultaneously."
  (with-accessors ((from-az azimuth) (from-el elevation)) from-pos
    (with-accessors ((to-az azimuth) (to-el elevation)) to-pos
      (max (time-to-rotate from-az to-az) (time-to-ascend from-el to-el)))))</code></pre>
<p>Lispers are fond of talking about the programmability of the syntax and how easy it is to learn. But the syntax is still there, it’s just encoded positionally rather than with auxiliary words and symbols. For example, the words <code>azimuth</code> and <code>elevation</code> in the Lisp code denote accessor functions. The outer <code>with-accessors</code> macro does something akin to this:</p>
<pre><code>timeToMoveFrom: from_pos to: to_pos
  | from_az from_el |
  from_az := from_pos azimuth.
  from_el := from_pos elevation.
  ...</code></pre>
<p>So the Lisp is wonderfully terse, but I still feel like I’m tricking something into doing what I want rather than expressing it. I think it’s possible to write clearer Lisp code, I just don’t know how, and I’ve relied on Haskell long enough now that Haskell’s version of clarity is becoming my own version of clarity.</p>
<p>I think this is part of what makes my love for Haskell irrational. I really can’t with a straight face tell you Haskell is a better language than every other language I know. I can’t imagine anything more frustrating than trying to teach Haskell to pragmatic people who can already program. And my style grows ever more idiosyncratic as I use it more. (I consider most uses of monad transformers a failure of functional decomposition). I never lose sleep wondering if my code is going to break in strange ways in Haskell, even though it would, albeit probably less often.</p>
<p>Another thing which surprised me about this little experiment was discovering the degree to which I am dependent on the compiler or interpreter to program effectively. Despite idolizing Dijkstra and befriending John Shipman and Al Stavely, I’m lousy at analyzing my code before running it. This problem affects me in every language I use, but I think I am worse because I simply have no respect for languages other than Haskell. Haskell will catch me trying to do really absurd things much earlier in the process of my not understanding what I’m doing. With Lisp and Smalltalk, I often managed to write ten or twenty lines of code, code that parsed successfully, before realizing I was doing something completely stupid, designing myself into a corner or worse. I’m sure I could write elegant code with less waste in these languages, but Haskell really forces me to.</p>
<p>A good friend once said, you don’t learn a foreign language for the sake of knowing it, you learn it to read the poetry. And we live in a world in which people meticulously study Hebrew, Greek and Arabic just for the sake of reading some of the best poetry we have. It would be absurd for me to claim that Haskell is always shorter, always more readable, or whatever; there are certain things I just don’t like to go near when I’m using it (like web development). Is this not similar to preferring Biblical Hebrew for religious poetry over humor?</p>
<p>A lot of people use and encourage the teaching of languages like Java and C for their simplicity. My wife endured this in C, and I got to see first-hand that notions like function calling, sequential processing, and boolean logic are not intuitive. If I reach back far enough, they weren’t intuitive to me either, I just crossed those bridges so long ago I seldom think about what it was like before, but I have enough of a glimpse that when obscure languages like Smalltalk and Haskell are discarded from the realm of possibility for teaching beginners, I find it upsetting. I didn’t know anything about category theory before I learned Haskell, and I still know about that much, yet I am able to benefit from writing terse, correct programs in it, programs that I can reason about the performance of. Granted, my understanding of Haskell’s execution model is mostly in terms of how it differs from C and every other language. But I still remember learning things about C’s execution model that
were shocking and amazing, and I still managed to learn and be productive in C before I understood those things, so why would it be different with Haskell?</p>
<p>I want to have a concise, cogent, interesting conclusion to insert here, but there really isn’t one, because programming is not a destination and all of my conclusions are provisional. The more I learn, the more discomfort there is. For one thing, if Haskell is so wonderful, why is there no good dependency management? Why do languages like Lisp and Smalltalk depend so much on manual intervention with mistakes, why can’t they have a strong, inferring type system like Haskell? More importantly to me, why am I never really able to commit to an environment like Smalltalk that embraces code evolution and failure, which seems like something I would like, and why do I like a system that tries so hard to achieve compile-time correctness when I depend on rapid iteration to write code?</p>
<p>One promising option I am looking forward to playing with more is <a href="http://magaloma.blogspot.com/2010/06/autotest-for-pharo.html">Autotest</a>, which detects which tests should be run after each change of the code and runs them, used in conjunction with <span>TDD</span>. This could potentially go much further than strong typing, but what if I’m testing something that has to do I/O? A question for another day.</p>


        <hr>
        


        <hr>
        

        

        
        

      </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Supreme Court Gives Doge Access to Social Security Data (128 pts)]]></title>
            <link>https://www.bloomberg.com/news/articles/2025-06-06/supreme-court-gives-doge-access-to-social-security-data</link>
            <guid>44204767</guid>
            <pubDate>Fri, 06 Jun 2025 20:42:24 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.bloomberg.com/news/articles/2025-06-06/supreme-court-gives-doge-access-to-social-security-data">https://www.bloomberg.com/news/articles/2025-06-06/supreme-court-gives-doge-access-to-social-security-data</a>, See on <a href="https://news.ycombinator.com/item?id=44204767">Hacker News</a></p>
Couldn't get https://www.bloomberg.com/news/articles/2025-06-06/supreme-court-gives-doge-access-to-social-security-data: Error: Request failed with status code 403]]></description>
        </item>
        <item>
            <title><![CDATA[Online sports betting: As you do well, they cut you off (127 pts)]]></title>
            <link>https://doc.searls.com/2025/05/21/online-sports-betting-is-for-losers/</link>
            <guid>44204603</guid>
            <pubDate>Fri, 06 Jun 2025 20:21:11 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://doc.searls.com/2025/05/21/online-sports-betting-is-for-losers/">https://doc.searls.com/2025/05/21/online-sports-betting-is-for-losers/</a>, See on <a href="https://news.ycombinator.com/item?id=44204603">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p><a href="https://flickr.com/photos/docsearls/81417674/in/album-1739217"><img decoding="async" src="https://150108457.v2.pressablecdn.com/wp-content/uploads/2025/05/vegas_020.jpg" alt="" width="70%" height="image" srcset="https://150108457.v2.pressablecdn.com/wp-content/uploads/2025/05/vegas_020.jpg 2560w , https://150108457.v2.pressablecdn.com/wp-content/uploads/2025/05/vegas_020-300x225.jpg 300w , https://150108457.v2.pressablecdn.com/wp-content/uploads/2025/05/vegas_020-1024x768.jpg 1024w , https://150108457.v2.pressablecdn.com/wp-content/uploads/2025/05/vegas_020-768x576.jpg 768w , https://150108457.v2.pressablecdn.com/wp-content/uploads/2025/05/vegas_020-1536x1152.jpg 1536w , https://150108457.v2.pressablecdn.com/wp-content/uploads/2025/05/vegas_020-2048x1536.jpg 2048w " sizes="(max-width: 2560px) 100vw, 2560px"></a></p>
<p>A few decades back my teenage son and I approached Las Vegas at night while traveling south on Interstate 15. When the skyline of the city began sparkling into view, the kid said, “Wow. Think of all the money people have made there!” This was a perfect tease for my response: “Dude, everything you see there was paid for by losers.”</p>
<p>The same is true for online sports betting, only more so. How? Soon as you do well, they cut you off. Sources:</p>
<ul>
<li><a href="https://www.espn.com/sports-betting/story/_/id/24425026/gambling-bookmakers-growing-us-legal-betting-market-allowed-ban-bettors">Won and done? Sportsbooks banning the smart money</a>. By <a href="http://davidpurdum/">@DavidPurdum</a> in ESPN</li>
<li><a href="https://www.wsj.com/business/media/sports-betting-companies-limit-winners-f06ea822">Sports Betting Companies Weed Out Winners. Gamblers Want to Know Why</a>. By <a href="https://www.wsj.com/news/author/katherine-sayre">Katherine Sayre</a> in&nbsp;<a href="https://www.wsj.com/"><em>The Wall Street Journal</em></a></li>
<li><a href="https://nypost.com/2024/07/18/sports/sportsbooks-seem-to-be-the-worst-losers-of-them-all/">Sportsbooks seem to be the worst losers of them all</a>. By <a href="https://nypost.com/author/phil-mushnick/">Phil Mushnick</a> in&nbsp;<a href="https://nypost.com/"><em>The New York Post</em></a></li>
<li><a href="https://www.nepm.org/regional-news/2024-05-22/sports-betting-companies-balk-at-discussing-limits-on-bettors">‘Very disappointing to me’: Sports betting companies balk at discussing limits on bettors</a><br>
By Colin A. Young in New England Public Media</li>
<li>In <a href="https://www.pushkin.fm/podcasts/against-the-rules/episode-5-the-mule">Episode 5</a> and <a href="https://www.pushkin.fm/podcasts/against-the-rules/episode-6-vip">Episode 6</a> of <a href="https://en.wikipedia.org/wiki/Michael_Lewis">Michael Lewis</a>‘ <a href="https://www.pushkin.fm/podcasts/against-the-rules">Against the Rules podcast series on sports gambling</a>, producer <a href="https://www.lidiajeankott.com/">Lydia Jean Kott</a> made a bunch of bad bets that won her the favor of sportsbooks (FanDuel and DraftKings), and then found herself cut off after she started winning (as a “mule” for a skilled sports gambler).</li>
</ul>
<p>Also interesting: While the online sportsbook algorithms are good at spotting gamblers who are smarter than the house, they aren’t so good at spotting and getting help for problem gamblers, which the sportsbook apps are all but (or actually) designed to create—especially out of young people who are quick to rationalize the game of losing money as a form of fun.</p>
<p>In <a href="https://outlier.bet/sports-betting-strategy/positive-ev-betting/why-sportsbooks-limit-your-bets/">Why Sportsbooks Limit Your Bets (And How to Avoid It)</a>, Outlier excuses the practice:</p>
<blockquote><p>Sportsbooks are businesses, and like any business, their primary goal is to make a profit. When a bettor consistently wins, they are essentially taking money from the sportsbook’s bottom line. To mitigate this risk, sportsbooks implement limits on the accounts of profitable bettors. The aim is to encourage more casual, recreational bettors, who are less likely to have an edge and more likely to contribute to the sportsbook’s revenue.</p></blockquote>
<p>Translation: They want people who throw their money away.</p>
<p>I could go on, but instead suggest you dig those two episodes of Against the Rules.</p>
<p>Oh, and here’s a bet: a generation or few from now (though hopefully sooner), we’ll look back on sports gambling everywhere the same way we look back today on smoking and drunk driving everywhere.</p>



</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[United States Digital Service Origins (151 pts)]]></title>
            <link>https://usdigitalserviceorigins.org/</link>
            <guid>44204277</guid>
            <pubDate>Fri, 06 Jun 2025 19:43:01 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://usdigitalserviceorigins.org/">https://usdigitalserviceorigins.org/</a>, See on <a href="https://news.ycombinator.com/item?id=44204277">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
					<p>June 6, 2025</p>
<p><b>Launching an Oral History of the Origins of the United States Digital Service</b></p>
<p><span>Opinions about technology’s role in government are everywhere right now. As founding members of the United States Digital Service, we could easily add our voices alone to the discourse — especially after it was reorganized and renamed to the United States DOGE Service on January 20, 2025.&nbsp;</span></p>
<p><span>While there’s certainly a need to think strategically about the future, it is also critical that we understand and learn from the past, because hard-won lessons shouldn’t be overshadowed by controversy or politics. After all, having a government that functions and delivers on its promises to the public shouldn’t be divisive.</span></p>
<p><span>That’s why over the past several years we have been collecting stories and experiences from our colleagues — the people who laid the foundation for an organization that brought over 700 technologists into government across three presidential administrations. Today we are proud to share the outcome of this work: </span><i><span>an oral history documenting how the United States Digital Service came to exist, and the initial days of building its foundation.&nbsp;&nbsp;</span></i></p>
<p><a href="https://usdigitalserviceorigins.org/"><span>The United States Digital Service Origins</span></a><span> features nearly 50 interviews, spanning 2009 – 2015, with those involved in the founding of the United States Digital Service, as well as some of the first leaders of agency teams and Communities of Practice. Each interview stands on its own, but in the aggregate the interviews piece together a larger story and vision of technology in government, and the realities of creating something new in an environment that is often paralyzed by inertia.&nbsp;</span></p>
<p><span>When we looked across interviews, we were able to identify </span><a href="https://usdigitalserviceorigins.org/insights/"><span>key themes and lessons</span></a><span>, and create a </span><a href="https://usdigitalserviceorigins.org/timeline/"><span>chronology of events</span></a><span> that tell the story of how the U.S. Digital Service was conceptualized and how it came to life. We also identified </span><a href="https://usdigitalserviceorigins.org/quotes/"><span>key quotes</span></a><span> that together provide a sense of the experience itself — what the work was actually like, as well as differing views and reflections on that time.&nbsp;&nbsp;</span></p>
<p><span>This is just one part of a larger story that led to the U.S. Digital Service; it’s one part of a 20+ year civic tech movement that transcends organizations, disciplines, and borders, and is built on the belief that people with experience designing and delivering digital products, combined with institutional knowledge and positioned at the right level of government, can transform public services — and that if we do this right, maybe we can make life even just a little bit better for people.</span></p>
<p><span>That story and movement doesn’t halt when one program — or presidency — comes to an end. Because it’s a body of work that’s bigger than any agency or group of people.&nbsp;</span></p>
<p><span>Whatever form government technology teams take in the coming years, the U.S. Digital Service’s legacy and learnings are relevant. It was not perfect, but it demonstrated what was possible, and those who had an opportunity to serve were forever changed.</span></p>
<p><em><span>– Kathy Pham and Emily Tavoulareas</span></em></p>
				</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[A year of funded FreeBSD development (306 pts)]]></title>
            <link>https://www.daemonology.net/blog/2025-06-06-A-year-of-funded-FreeBSD.html</link>
            <guid>44204224</guid>
            <pubDate>Fri, 06 Jun 2025 19:35:54 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.daemonology.net/blog/2025-06-06-A-year-of-funded-FreeBSD.html">https://www.daemonology.net/blog/2025-06-06-A-year-of-funded-FreeBSD.html</a>, See on <a href="https://news.ycombinator.com/item?id=44204224">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
<p>
I've been maintaining
<a href="https://www.freebsd.org/">FreeBSD</a> on the
<a href="https://aws.amazon.com/ec2/">Amazon EC2</a> platform ever since I
first got it booting in 2010, but in
<a href="https://www.freebsd.org/news/newsflash/#2023-11-17:1">November
2023</a> I added to my responsibilities the role of FreeBSD release
engineering lead — just in time to announce the availability of
FreeBSD 14.0, although Glen Barber did all the release engineering work for
that release.  While I receive a small amount of funding from
<a href="https://antithesis.com/">Antithesis</a> and from my
<a href="https://www.patreon.com/c/cperciva">FreeBSD/EC2 Patreon</a>, it
rapidly became clear that my release engineering duties were competing
with — in fact, out-competing — FreeBSD/EC2 for my available
FreeBSD volunteer hours: In addition to my long list of "features to
implement" stagnating, I had increasingly been saying "huh that's weird...
oh well, no time to investigate that now".  In short, by early 2024 I was
becoming increasingly concerned that I was not in a position to be a good
"owner" of the FreeBSD/EC2 platform.
</p><p>
For several years leading up to this point I had been talking to Amazonians
on and off about the possibility of Amazon sponsoring my FreeBSD/EC2 work;
rather predictably, most of those conversation ended up with my
contacts at Amazon rhyming with "Amazon should definitely sponsor the work
you're doing... but I don't have any money available in <em>my</em> budget
for this".  Finally in April 2024 I found someone with a budget, and after
some discussions around timeline, scope, and process, it was determined
that Amazon would support me for a year via
<a href="https://github.com/sponsors/cperciva">GitHub Sponsors</a>.  I'm
not entirely sure if the year in question was June through May or July
through June — money had to move within Amazon, from Amazon to GitHub,
from GitHub to Stripe, and finally from Stripe into my bank account, so when
I received money doesn't necessarily reflect when Amazon <i>intended</i> to
give me money — but either way the sponsorship either has come to an end
or is coming to an end soon, so I figured now was a good time to write about
what I've done.
</p><p>
Amazon was nominally sponsoring me for 40 hours/month of work on FreeBSD
release engineering and FreeBSD/EC2 development — I made it clear to
them that sponsoring one and not the other wasn't really feasible, especially
given dependencies between the two — and asked me to track how much
time I was spending on things.  In the end, I spent roughly 50 hours/month
on this work, averaging 20 hours/month spent on EC2-specific issues, 20
hours/month making FreeBSD releases happen, and 10 hours/month on other
release engineering related work — although the exact breakdown
varied dramatically from month to month.
</p><p>
Following FreeBSD's
<a href="https://lists.freebsd.org/archives/freebsd-announce/2024-July/000143.html">quarterly
release schedule</a> (which I announced in July 2024, but put together and
presented at the FreeBSD developer summit at
<a href="https://www.bsdcan.org/">BSDCan</a> in May 2024), I managed four
FreeBSD releases during the past year: FreeBSD 13.4, in September 2024;
FreeBSD 14.2, in December 2024; FreeBSD 13.5, in March 2025; and FreeBSD
14.3, currently scheduled for release on June 10th.  The work involved in
manging each of these releases — nagging developers to get their
code into the tree in time, approving (or disapproving!) merge requests,
coordinating with other teams, building and testing images (usually three
Betas, one Release Candidate, and the final Release), writing announcement
text, and fixing any release-building breakage which arose along the way
— mostly happened in the month prior to the release (I refer to the
second month of each calendar quarter as "Beta Month") and ranged from a
low of 33.5 hours (for FreeBSD 13.5) to a high of 79 hours (for FreeBSD
14.2).  As one might imagine, the later in a stable branch you get, the
fewer the number of things there are breaking and the lower the amount of
work required for doing a release; while I wasn't tracking hours when I
managed FreeBSD 14.1, I suspect it took close to 100 hours of release
engineering time, and FreeBSD 15.0 is very likely to be well over that.
</p><p>
On the FreeBSD/EC2 side of things, there were two major features which
Amazon encouraged me to prioritize: The "power driver" for AWS Graviton
instances (aka "how the EC2 API tells the OS to shut down" — without
this, FreeBSD ignores the shutdown signal and a few minutes later EC2
times out and yanks the virtual power cable), and device hotplug on AWS
Graviton instances.  The first of these was straightforward: On Graviton
systems, the "power button" is a GPIO pin, the details of which are
specified via an ACPI _AEI object.  I added code to find those in ACPI and
pass the appropriate configuration through to the driver for the PL061 GPIO
controller; when the GPIO pin is asserted, the controller generates an
interrupt which causes the ACPI "power button" event to be triggered, which
in turn now shuts down the system.  There was one minor hiccup: The ACPI
tables provided by EC2 specify that the GPIO pin in question should be
configured as a "Pull Up" pin, but the PL061 controller in fact doesn't
have any pullup/pulldown resistors; this didn't cause problems on Linux
because Linux silently ignores GPIO configuration failures, but on FreeBSD
we disabled the device after failing to configure it.  I believe this EC2
bug will be fixed in future Graviton systems; but in the mean time I ship
FreeBSD/EC2 AMIs with a new "quirk": <tt>ACPI_Q_AEI_NOPULL</tt>, aka
"Ignore the PullUp flag on GPIO pin specifications in _AEI objects".
</p><p>
Getting hotplug working — or more specifically, getting hot
<i>unplug</i> working, since that's where most of the problems arose
— took considerably more work, largely because there were several
different problems, each presenting on a subset of EC2 instance types:
</p><ul><li>
On some Graviton systems, we leaked a (virtual) IRQ reservation during
PCI attach; this is harmless in most cases, but after attaching and
detaching an EBS volume 67 times we would run out of IRQs and the FreeBSD
kernel would panic.  This IRQ leakage was happening from some "legacy"
PCI interrupt routing code, and in the end I simply added a boot loader
setting to turn that code off in EC2.
</li><li>
On some Graviton systems, the firmware uses PCI device power state as an
indication that the OS has finished using a device and is ready for it
to be "ejected".  This is 100% a bug in EC2, and I believe it will be
fixed in due course; in the mean time, FreeBSD/EC2 AMIs have an ACPI
quirk <tt>ACPI_Q_CLEAR_PME_ON_DETACH</tt> which instructs them to flip
some bits in the PCI power management register before ejecting a
device.
</li><li>
On the newest generation of EC2 instances (both x86 and Graviton),
FreeBSD's nvme driver would panic after a PCIe unplug.  This bug I
didn't need to fix myself, beyond pointing it out to our nvme driver
maintainer.
</li><li>
On some EC2 instances (both x86 and Graviton), we would see a "ghost"
device on the PCI bus after it was ejected; attempts to access the
device would fail, but the "ghost" would block any attempt to attach
new devices.  This turned out to be another EC2 bug: The Nitro firmware
managing the PCI bus operated asynchronously from the firmware managing
the PCI <i>devices</i>, so there was a window of a few ms where a device
had been unplugged but the PCI bus still reported it as being present.
On Linux this is (almost always) not a problem since Linux scans buses
periodically and typically loses the race; but on FreeBSD we immediately
re-scan the PCI bus after detaching a device, so we usually won the race
against the Nitro firmware, causing us to "see" the device which was no
longer present.  My understanding is that this is being fixed
in Nitro to ensure that the PCI bus "knows" about a device detach before
the detach is acknowledged to the operating system; but in the mean time,
FreeBSD/EC2 AMIs have an ACPI quirk <tt>ACPI_Q_DELAY_BEFORE_EJECT_RESCAN</tt>
which adds a 10 ms delay between signalling that a device should be ejected
and rescanning the PCI bus.
</li></ul><p>
In addition to these functionality fixes, I made one "quality of
life" improvement to FreeBSD's hotplug handling: PCIe (used in the latest
generation of EC2 instances) mandates that after the "attention" button
is pressed to request a device eject, there is a 5 second delay —
in case a human standing in front of a machine says "oh no that's the
wrong disk" — and if the button is pressed a second time, the eject
request is cancelled.  This delay is entirely pointless in EC2, where there
is no human physically pressing a button (and no mechanism for pressing a
virtual button a second time) — so I added a boot loader tunable to
adjust that timeout and set it to zero in EC2.  Finally, I put together a
test script: I can now launch an EC2 instance and repeatedly plug and
unplug an EBS volume via the EC2 API, and confirm that FreeBSD sucessfully
attaches and detaches it 300 times in a row.  With luck, this will allow
me to ensure that hotplug is fully operational on future EC2 instance
types — at least, assuming I get access to them before they launch.
</p><p>
While those two were Amazon's top priorities for FreeBSD/EC2 work, they
were by no means the only things I worked on; in fact they only took up
about half of the time I spent on EC2-specific issues.  I did a lot of
work in 2021 and 2022 to speed up the FreeBSD boot process, but among the
"that's weird but I don't have time to investigate right now" issues I
had noticed in late 2023 and early 2024 was that FreeBSD/EC2 instances
sometimes took a surprisingly long time to boot.  I hadn't <i>measured</i>
how long they took, mind you; but as part of the FreeBSD weekly snapshot
process I ran test boots of a few EC2 instance types, and I had needed to
increase the sleep time between launching instances and trying to SSH
into them.
</p><p>
Well, the first thing to do with any sort of performance issues is to
collect data; so I benchmarked boot time on weekly EC2 AMI builds dating
back to 2018 — spinning up over ten thousand EC2 instances in the
process — and started generating FreeBSD
<a href="https://www.daemonology.net/freebsd-ec2-boot-performance/">boot
performance plots</a>.  Collecting new data and updating those plots is
now part of my weekly snapshot testing process; but even without drawing
plots, I could immediately see some issues.  I got to work:
</p><ul><li>
Starting in the first week of 2024, the FreeBSD boot process suddenly got
about 3x slower.  I started bisecting commits, and tracked it down to...
a commit which increased the root disk size from 5 GB to 6 GB.  Why?  Well,
I reached out to some of my friends at Amazon, and it turned out that the
answer was somewhere between "magic" and "you really don't want to know";
but the important part for me was that increasing the root disk size to 8
GB restored performance to earlier levels.
</li><li>
FreeBSD was also taking a long time to boot on Graviton 2 (aka <tt>c6g</tt>
and similar) instances, and after some investigation I tracked this down to
a problem with kernel entropy seeding: If the FreeBSD kernel doesn't have
enough entropy to generate secure random numbers, the boot process stalls
until it collects more entropy — and in a VM, that can take a while.
Now, we have code to obtain entropy via the EFI boot loader — which
effectively means asking the Nitro firmware to give us a secure seed —
but that ran into two problems: First, it wasn't actually running in EC2,
and second, when it did run, it was absurdly slow on the Graviton 2.
<br>
The first problem was easily solved: The entropy seeding request was being
made from the "boot menu" lua code, and (ironically in order to improve
boot performance) we bypass that menu in EC2; moving that to the right place
in the boot loader lua code ensured that it ran regardless of whether the
menu was enabled.  The second problem turned out to be a Graviton 2 issue:
It could provide a small amount of entropy quickly, but took a long time to
provide the 2048 bytes which the FreeBSD boot loader was requesting.  This
large request was due to the way that FreeBSD seeded its entropy system;
32 pools each needed 64 bytes of entropy.  Since this was in no way
<i>cryptographically</i> necessary — the multiple pools exist only as a
protection in case a pool state is leaked — I rewrote the code to make
it possible to take 64 bytes from EFI and use PBKDF2 as an "entropy spreader"
to turn that into the 2048 bytes our entropy-feeding API needed.  This took
the boot time of FreeBSD arm64/base/UFS from ~25 seconds down to ~8
seconds.
</li><li>
I also noticed that ZFS images were taking quite a bit longer to boot than
UFS images — and interestingly, this delta varied depending on the
amount of data on the disk (but not the disk size itself).  I traced this
down to a weird interaction of our filesystem-building code (makefs) and
what happens when you attach a ZFS pool: ZFS performs some filesystem
verification steps which involve traversing the most recent transaction
group, but makefs puts the everything into a single transaction group
— so when EC2 ZFS images booted, they had to read and process metadata
for every single file on disk (hence the number of files on disk affecting
the time taken).  Once I tracked down the issue, I was able to report it
to FreeBSD's makefs guru (Mark Johnston), who solved the problem simply by
recording a higher transaction group on the filesystem — that way,
the single transaction group was not "recent enough" to prompt the ZFS
transaction group verification logic.  ZFS images promptly dropped from ~22
seconds down to ~11 seconds of boot time.
</li><li>
Finally — and this issue was one I caught promptly as a result of
including boot performance in my weekly testing — in December 2024 I
updated the <tt>net/aws-ec2-imdsv2-get</tt> port to support IPv6.  This port
provides a command-line interface to the EC2 Instance MetaData Service, and
is necessary because when Amazon launched "IMDSv2" to paper over (but not
properly fix) the security problem inherent in exposing IAM credentials over
HTTP, they made it impossible to use FreeBSD <tt>fetch(1)</tt> to access the
IMDS.  Unfortunately when IPv6 support was added to
<tt>aws-ec2-imdsv2-get</tt>, two mistakes were made: First, it attempted to
connect on IPv6 first (even though IPv4-only is the default IMDS instance
configuration); and second, it kept the default TCP timeout (75 seconds).
Thanks to my testing, I got this fixed promptly, to attempt IPv4 first and
reduce the timeout to 100 ms — considering that IMDS requests are
serviced without ever leaving the physical system, waiting more than 100 ms
seemed unnecessary!
</li></ul>
<p>
One thing which had long been on my "features to implement" list for
FreeBSD/EC2 but I hadn't found time for earlier was adding more AMI
"flavours": A year ago, we had <tt>base</tt> (the
FreeBSD base system, with minimal additional code installed from the ports
tree to make it "act like an EC2 AMI") and <tt>cloud-init</tt> (as the name
suggests, FreeBSD with Cloud-init installed).  I added two more flavours
of FreeBSD AMI to the roster: <tt>small</tt> AMIs, which are like <tt>base</tt>
except without debug symbols, the LLDB debugger, 32-bit libraries, FreeBSD tests,
or the Amazon SSM Agent or AWS CLI — which collectively reduces the
disk space usage from ~5 GB to ~1 GB while not removing anything which most
people will use — and <tt>builder</tt> AMIs, which are FreeBSD
<a href="https://www.daemonology.net/blog/2015-11-21-FreeBSD-AMI-builder-AMI.html">AMI Builder AMIs</a>,
providing an easy path for users to create customized FreeBSD AMIs.
</p><p>
Of course, with 4 flavours of FreeBSD AMIs — and two filesystems (UFS
and ZFS), two architectures (amd64 and arm64), and three versions of
FreeBSD (13-STABLE, 14-STABLE, and 15-CURRENT) — all of the weekly
snapshot builds were starting to add up; so in May I finally got around to
cleaning up old images (and their associated EBS snapshots).  While I don't
pay for these images — the FreeBSD release engineering AWS account is
sponsored by Amazon — it was still costing <i>someone</i> money; so
when I realized I could get rid of 336 TB of EBS snapshots, I figured it
was worth spending a few hours writing shell scripts.
</p><p>
While most of my time was spent on managing release cycles and maintaining
the FreeBSD/EC2 platform, I did also spend some time on broader release
engineering issues — in fact, part of the design of the "quarterly"
release schedule is that it leaves a few weeks between finishing one
release and starting the next to allow for release engineering work which
can't effectively be done in the middle of a release cycle.  The first
issue I tackled here was parallelizing release building: With a large
number of EC2 AMIs being built, a large proportion of the release build
time was being spent not <i>building</i> but rather <i>installing</i>
FreeBSD into VM images.  I reworked the release code to parallelize this,
but found that it caused sporadic build failures — which were very
hard to isolate, since they only showed up with a complete release build
(which took close to 24 hours) and not with any subset of the build.
After many hours of work I finally tracked the problem down to a single
missing Makefile line: We weren't specifying that a directory should be
created before files were installed into it.  With that fix, I was able
to reduce the release build from ~22 hours down to ~13 hours, and also
"unlock" the ability to add more EC2 AMI flavours (which I couldn't do
earlier since it would have increased the build time too much).
</p><p>
Another general release engineering issue I started tackling was the
problem of build reproducibility — aided by the fact that I had
EC2 to draw upon.  As part of my weekly testing of snapshot images, I
now spin up EC2 instances and have them build their own AMIs — and
then use <a href="https://diffoscope.org/">diffoscope</a> to compare
the disk images they built against the ones they were launched from.
This has already found several issues — including some which
appeared partway through the year and were identified quickly thanks to the
regular testing — of which I've fixed a few and some others I've
passed on to other developers to tackle.
</p><p>
Of course, in addition to the big projects there's also a plethora of smaller
issues to tackle.  Build breakage (weekly snapshot builds are good at finding
this!); reviewing patches to the ENA driver; helping Dave Cottlehuber add
support for building OCI Containers and uploading them to repositories;
teaching my <tt>bsdec2-image-upload</tt> tool to gracefully handle internal
AWS errors; reporting an AWS security issue I stumbled across... some days
everything falls under the umbrella of "other stuff which needs to get done",
but a lot of it is just as important as the larger projects.
</p><p>
So what's next?  Well, I'm still the FreeBSD release engineering lead and the
maintainer of the FreeBSD/EC2 platform — just with rather less time to
devote to this work.  FreeBSD releases will continue to happen — 15.0
should land in December, followed in 2026 by 14.4, 15.1, 14.5, and 15.2
— but I probably won't have time to jump in and fix things as much, so
late-landing features are more likely to get removed from a release rather
than fixed in time for the release; we were
only able to ship OCI Containers starting in FreeBSD 14.2 because I had
funded hours to make sure all the pieces landed intact, and that sort of
involvement won't be possible.  On the EC2 side, now that I have regression
testing of boot performance set up, I'll probably catch any issues which need
to be fixed there; but the rest of my "features to implement" list —
automatically growing filesystems when EBS volumes expand, better automatic
configuration with multiple network interfaces (and network interface hot
plug), rolling "pre-patched" AMIs (right now FreeBSD instances update
themselves when they first boot), putting together a website to help users
generate EC2 <tt>user-data</tt> files (e.g., for installing packages and
launching daemons), returning to my work on FreeBSD/Firecracker and making
it a supported FreeBSD platform, etc. — is likely to stagnate unless
I find more time.
</p><p>
I've been incredibly lucky to get this sponsorship from Amazon; it's far more
than <a href="https://xkcd.com/2347/">most open source developers</a> ever
get.  I wish it wasn't ending; but I'm proud of the work I've done and I'll
always be grateful to Amazon for giving me this opportunity. 
</p>



<p><a href="https://disqus.com/">blog comments powered by <span>Disqus</span></a>
</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Show HN: AI game animation sprite generator (107 pts)]]></title>
            <link>https://www.godmodeai.cloud/ai-sprite-generator</link>
            <guid>44204181</guid>
            <pubDate>Fri, 06 Jun 2025 19:30:29 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.godmodeai.cloud/ai-sprite-generator">https://www.godmodeai.cloud/ai-sprite-generator</a>, See on <a href="https://news.ycombinator.com/item?id=44204181">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><header></header><div><div><div><div><p><img alt="God Mode AI" loading="lazy" width="64" height="64" decoding="async" data-nimg="1" srcset="https://www.godmodeai.cloud/_next/image?url=%2Fimages%2Fgodmode-logo.png&amp;w=64&amp;q=75 1x, https://www.godmodeai.cloud/_next/image?url=%2Fimages%2Fgodmode-logo.png&amp;w=128&amp;q=75 2x" src="https://www.godmodeai.cloud/_next/image?url=%2Fimages%2Fgodmode-logo.png&amp;w=128&amp;q=75"></p></div><h2><span>AI Sprite Generator</span><br>God Mode Unleashed</h2><p>Upload character design and get professional game animation sprites. Create smooth combat animations with strong hit feedback, support all action types. Production-ready sprites in a click.</p></div><div id="upload-area"><p>Start Creating Now</p><div><p>Drop your character image here</p><p>or click to browse files</p><p>Supports: JPG, PNG, GIF, WebP (max 50MB)</p></div></div></div><div><h2>Simple 3 Steps to Create Sprites</h2><div><div><p>1</p><h3>Upload Image</h3><p>Drop your character or describe with text</p></div><div><p>2</p><h3>Select Actions</h3><p>Choose from diverse animation types</p></div><div><p>3</p><h3>Get Sprites</h3><p>Download production-ready animations</p></div></div></div></div><div><h2>See God Mode AI in Action</h2><p>Watch how our AI sprite generator transforms characters into animated sprites</p></div><div><div><h2>Sprite Examples</h2><p>See the quality and variety of sprites generated by God Mode AI</p></div><div><div><div><p><img alt="Panda Warrior Original" loading="lazy" width="192" height="192" decoding="async" data-nimg="1" srcset="https://www.godmodeai.cloud/_next/image?url=%2Fexample-animations%2Fpanda-upload.png&amp;w=256&amp;q=75 1x, https://www.godmodeai.cloud/_next/image?url=%2Fexample-animations%2Fpanda-upload.png&amp;w=384&amp;q=75 2x" src="https://www.godmodeai.cloud/_next/image?url=%2Fexample-animations%2Fpanda-upload.png&amp;w=384&amp;q=75"></p><div><h3>Panda Warrior</h3><p>Original uploaded character</p><p>6 Animations Generated</p></div></div><div><h4>Generated Sprite Animations</h4></div></div><div><div><p><img alt="Female Ninja Original" loading="lazy" width="192" height="192" decoding="async" data-nimg="1" srcset="https://www.godmodeai.cloud/_next/image?url=%2Fexample-animations%2Ffemale-nanja-upload.png&amp;w=256&amp;q=75 1x, https://www.godmodeai.cloud/_next/image?url=%2Fexample-animations%2Ffemale-nanja-upload.png&amp;w=384&amp;q=75 2x" src="https://www.godmodeai.cloud/_next/image?url=%2Fexample-animations%2Ffemale-nanja-upload.png&amp;w=384&amp;q=75"></p><div><h3>Female Ninja</h3><p>Original uploaded character</p><p>6 Animations Generated</p></div></div><div><h4>Generated Sprite Animations</h4></div></div></div><div><h3>Ready to Create Your Own?</h3><p>Upload your character and generate professional sprite animations in minutes</p></div></div><div id="features"><div><h2>Powerful Features</h2><p>Everything you need to create professional game sprites</p></div><div><div><p>Advanced AI creates smooth, professional animations from your images or text descriptions.</p></div><div><p>Complete set of character actions: idle, walk, run, attack, jump, cast, and many more.</p></div><div><p>Transparent backgrounds, perfect center offset, and bounding boxes included for immediate use.</p></div><div><p>From retro pixel art to modern anime styles - choose the perfect look for your game.</p></div><div><p>Train custom actions for your unique gameplay needs. Create personalized animations that match your game's style.</p></div><div><p>Upload existing character art or describe your vision with text - both work perfectly.</p></div></div></div><div><div><h2>Perfect For</h2><p>Trusted by game developers worldwide</p></div><div><div><h3>Indie Developers</h3><p>Create professional sprites without hiring expensive artists. Perfect for solo developers and small teams.</p></div><div><h3>Game Studios</h3><p>10x efficiency and 10x cost savings - no more creating animations for 100 NPCs 100 times. Generate complete character sets instantly for AAA production pipelines.</p></div><div><h3>Artists &amp; Designers</h3><p>Enhance your workflow with AI assistance. Generate base animations and refine them to perfection.</p></div></div></div><div><div><h2>Train Your Own Action Models</h2><p>Create unique animations with as few as <span>5 animation training samples</span> - <span> FREE!</span></p></div><div><div><h3>🎉 FREE Model Training!</h3><p>Train custom action models at no cost. Create personalized animations for your unique game mechanics.</p></div><div><h3><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 15v4a2 2 0 0 1-2 2H5a2 2 0 0 1-2-2v-4"></path><polyline points="17 8 12 3 7 8"></polyline><line x1="12" x2="12" y1="3" y2="15"></line></svg>How It Works</h3><div><p>Start creating your own action models with just <span>5 animation training samples</span>! Upload a few examples of your desired action, and our AI will learn to generate infinite variations in that style. <span>Best of all - training is FREE!</span></p><div><h4>Tips for Good Training:</h4><div><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21.801 10A10 10 0 1 1 17 3.335"></path><path d="m9 11 3 3L22 4"></path></svg><p><span>Start with just <span>5 animation training samples</span> - that's all you need!</span></p></div></div></div></div><div><div><p><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><rect width="18" height="11" x="3" y="11" rx="2" ry="2"></rect><path d="M7 11V7a5 5 0 0 1 10 0v4"></path></svg><h4>Private Model</h4></p><p>Keep your trained action model private for your exclusive use. Perfect for proprietary game mechanics and unique character movements.</p></div><div><p><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="10"></circle><path d="M12 2a14.5 14.5 0 0 0 0 20 14.5 14.5 0 0 0 0-20"></path><path d="M2 12h20"></path></svg><h4>Public Model</h4></p><p>Share your action model with the community and earn money! Perfect for creating popular game mechanics that others can use.</p></div></div><div><p>🆓 FREE</p><p>Custom Training</p><p>Revenue Share</p></div></div></div><div id="pricing"><div><h2>Pay as you go</h2><p>No more subscriptions. Just pay for what you use.</p></div><div><h3>How credits work</h3><div><div><p><span>1</span></p><div><p>1 credit</p><p>1 sprite generation</p></div></div><div><p><span>10</span></p><div><p>10 credits</p><p>Train your own action model</p></div></div></div></div><div><div><div><h3>Starter Pack</h3><p>Perfect for trying out our AI magic</p></div><ul><li><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M20 6 9 17l-5-5"></path></svg><span>20 credits included</span></li><li><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M20 6 9 17l-5-5"></path></svg><span>1 credit = 1 sprite generation</span></li><li><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M20 6 9 17l-5-5"></path></svg><span>Credits never expire</span></li><li><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M20 6 9 17l-5-5"></path></svg><span>No subscription required</span></li><li><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M20 6 9 17l-5-5"></path></svg><span>Pay only for what you use</span></li><li><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M20 6 9 17l-5-5"></path></svg><span>Instant access to all features</span></li><li><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M20 6 9 17l-5-5"></path></svg><span>High-quality AI generations</span></li></ul></div><div><div><h3>Popular Pack</h3><p>Great for regular creators</p></div><ul><li><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M20 6 9 17l-5-5"></path></svg><span>60 credits included</span></li><li><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M20 6 9 17l-5-5"></path></svg><span>1 credit = 1 sprite generation</span></li><li><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M20 6 9 17l-5-5"></path></svg><span>Credits never expire</span></li><li><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M20 6 9 17l-5-5"></path></svg><span>No subscription required</span></li><li><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M20 6 9 17l-5-5"></path></svg><span>Pay only for what you use</span></li><li><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M20 6 9 17l-5-5"></path></svg><span>Instant access to all features</span></li><li><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M20 6 9 17l-5-5"></path></svg><span>High-quality AI generations</span></li></ul></div><div><div><h3>Ultimate Pack</h3><p>Best value for power users</p></div><div><p>Best value</p><p>50% more credits per dollar</p></div><ul><li><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M20 6 9 17l-5-5"></path></svg><span>250 credits included</span></li><li><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M20 6 9 17l-5-5"></path></svg><span>1 credit = 1 sprite generation</span></li><li><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M20 6 9 17l-5-5"></path></svg><span>Credits never expire</span></li><li><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M20 6 9 17l-5-5"></path></svg><span>No subscription required</span></li><li><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M20 6 9 17l-5-5"></path></svg><span>Pay only for what you use</span></li><li><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M20 6 9 17l-5-5"></path></svg><span>Instant access to all features</span></li><li><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M20 6 9 17l-5-5"></path></svg><span>High-quality AI generations</span></li></ul></div></div><p>All prices are one-time payments. No recurring charges.</p><div><div><h3>Enterprise &amp; Game Studios</h3><p>Supercharge your game development with custom AI solutions</p></div><div><div><div><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M20 6 9 17l-5-5"></path></svg><p><span>Train customized AI models specifically for your game development needs</span></p></div><div><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M20 6 9 17l-5-5"></path></svg><p><span>Custom-tailored solutions for your unique art style and character design</span></p></div><div><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M20 6 9 17l-5-5"></path></svg><p><span>10x your animation productivity with our specialized AI tools</span></p></div></div><div><div><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M20 6 9 17l-5-5"></path></svg><p><span>Create 10x more characters with your existing resources</span></p></div><div><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M20 6 9 17l-5-5"></path></svg><p><span>Access to customized AI services and dedicated support</span></p></div></div></div><div><p>Looking to scale your game development? Let's discuss how our AI solutions can transform your workflow.</p></div></div></div><div id="faq"><h2>Frequently Asked Questions</h2><p>Everything you need to know</p></div></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Highly efficient matrix transpose in Mojo (112 pts)]]></title>
            <link>https://veitner.bearblog.dev/highly-efficient-matrix-transpose-in-mojo/</link>
            <guid>44204155</guid>
            <pubDate>Fri, 06 Jun 2025 19:28:29 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://veitner.bearblog.dev/highly-efficient-matrix-transpose-in-mojo/">https://veitner.bearblog.dev/highly-efficient-matrix-transpose-in-mojo/</a>, See on <a href="https://news.ycombinator.com/item?id=44204155">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
    

    
        
    

    
        

        <p>
            <i>
                <time datetime="2025-06-06T17:46Z">
                    06 Jun, 2025
                </time>
            </i>
        </p>
    

    <p>In this blogpost I will step by step show you how to implement a highly efficient transpose kernel for the <code>Hopper</code> architecture using Mojo.
The best kernel archives a bandwidth of <code>2775.49 GB/s</code>, i.e. <code>84.1056%</code>.  The optimisations are the same that I applied to archive a bandwidth of <code>2771.35 GB/s</code> using pure <code>CUDA</code> on the same <code>H100</code> that I use here. That shows that Mojo can archive <code>CUDA</code> like performance on exactly the same task. You may compare the kernels with the previous <a href="https://github.com/simveit/effective_transpose/tree/main">kernels</a> I wrote and read my other <a href="https://veitner.bearblog.dev/making-matrix-transpose-really-fast-on-hopper-gpus/">blogpost</a> as well where I explain the concepts in detail. Here I will only briefly review them and instead focus on the implementation details. For readers without knowledge how to use <code>TMA</code> in Mojo I refer you to my previous <a href="https://veitner.bearblog.dev/use-tma-without-cuda/">blogpost</a> on this topic.</p>
<h2 id="naive-approach">Naive approach</h2><p>Before calling the kernel we need to initialise two <code>TMA descriptors</code>, this concept is similar to <code>cuTensorMapEncodeTiled</code> we can use in <code>CUDA</code>.</p>
<div><pre><span></span><span>var</span> <span>descriptor</span> <span>=</span> <span>create_tma_descriptor</span><span>[</span><span>DType</span><span>.</span><span>float32</span><span>,</span> <span>2</span><span>](</span>
	<span>gmem_dev</span><span>,</span>
	<span>(</span><span>GMEM_HEIGHT</span><span>,</span> <span>GMEM_WIDTH</span><span>),</span>
	<span>(</span><span>GMEM_WIDTH</span><span>,</span> <span>1</span><span>),</span>
	<span>(</span><span>SMEM_HEIGHT</span><span>,</span> <span>SMEM_WIDTH</span><span>),</span>
<span>)</span>
<span>var</span> <span>descriptor_tr</span> <span>=</span> <span>create_tma_descriptor</span><span>[</span><span>DType</span><span>.</span><span>float32</span><span>,</span> <span>2</span><span>](</span>
	<span>gmem_tr_dev</span><span>,</span>
	<span>(</span><span>GMEM_WIDTH</span><span>,</span> <span>GMEM_HEIGHT</span><span>),</span>
	<span>(</span><span>GMEM_HEIGHT</span><span>,</span> <span>1</span><span>),</span>
	<span>(</span><span>SMEM_WIDTH</span><span>,</span> <span>SMEM_HEIGHT</span><span>),</span>
<span>)</span>
</pre></div>
<p>We have two descriptors. Both in row major format, the one the transpose of the other. The corresponding <code>smems</code> in relation of transpose as well.
As a quick reminder he is the algorithm we are going to implement.
We take a tile, perform transpose inside the tile and put it at the opposite position in the matrix, i.e. at the transposed position</p>
<p><img src="https://bear-images.sfo2.cdn.digitaloceanspaces.com/veitner/36.webp" alt="Screenshot 2025-06-06 at 19"></p>
<p>Below is the code that archives that.</p>
<h3 id="load-to-shared-memory">Load to shared memory</h3><div><pre><span></span><span>@__llvm_arg_metadata</span><span>(</span><span>descriptor</span><span>,</span> <span>`</span><span>nvvm</span><span>.</span><span>grid_constant</span><span>`</span><span>)</span>
<span>@__llvm_arg_metadata</span><span>(</span><span>descriptor_tr</span><span>,</span> <span>`</span><span>nvvm</span><span>.</span><span>grid_constant</span><span>`</span><span>)</span>
<span>fn</span> <span>transpose_kernel_naive</span><span>[</span>
    <span>block_size</span><span>:</span> <span>Int</span>
<span>](</span><span>descriptor</span><span>:</span> <span>TMADescriptor</span><span>,</span> <span>descriptor_tr</span><span>:</span> <span>TMADescriptor</span><span>):</span>
    <span>var</span> <span>shmem</span> <span>=</span> <span>stack_allocation</span><span>[</span>
        <span>block_size</span> <span>*</span> <span>block_size</span><span>,</span>
        <span>DType</span><span>.</span><span>float32</span><span>,</span>
        <span>alignment</span><span>=</span><span>1024</span><span>,</span>
        <span>address_space</span> <span>=</span> <span>_GPUAddressSpace</span><span>.</span><span>SHARED</span><span>,</span>
    <span>]()</span>
    <span>var</span> <span>shmem_tr</span> <span>=</span> <span>stack_allocation</span><span>[</span>
        <span>block_size</span> <span>*</span> <span>block_size</span><span>,</span>
        <span>DType</span><span>.</span><span>float32</span><span>,</span>
        <span>alignment</span><span>=</span><span>1024</span><span>,</span>
        <span>address_space</span> <span>=</span> <span>_GPUAddressSpace</span><span>.</span><span>SHARED</span><span>,</span>
    <span>]()</span>
    <span>var</span> <span>mbar</span> <span>=</span> <span>stack_allocation</span><span>[</span>
        <span>1</span><span>,</span> <span>Int64</span><span>,</span> <span>address_space</span> <span>=</span> <span>_GPUAddressSpace</span><span>.</span><span>SHARED</span>
    <span>]()</span>
    <span>var</span> <span>descriptor_ptr</span> <span>=</span> <span>UnsafePointer</span><span>(</span><span>to</span><span>=</span><span>descriptor</span><span>)</span><span>.</span><span>bitcast</span><span>[</span><span>NoneType</span><span>]()</span>
    <span>var</span> <span>descriptor_tr_ptr</span> <span>=</span> <span>UnsafePointer</span><span>(</span><span>to</span><span>=</span><span>descriptor_tr</span><span>)</span><span>.</span><span>bitcast</span><span>[</span><span>NoneType</span><span>]()</span>

    <span>x</span> <span>=</span> <span>block_idx</span><span>.</span><span>x</span> <span>*</span> <span>block_size</span>
    <span>y</span> <span>=</span> <span>block_idx</span><span>.</span><span>y</span> <span>*</span> <span>block_size</span>

    <span>col</span> <span>=</span> <span>thread_idx</span><span>.</span><span>x</span> <span>%</span> <span>block_size</span>
    <span>row</span> <span>=</span> <span>thread_idx</span><span>.</span><span>x</span> <span>//</span> <span>block_size</span>

    <span># LOAD</span>
    <span>if</span> <span>thread_idx</span><span>.</span><span>x</span> <span>==</span> <span>0</span><span>:</span>
        <span>mbarrier_init</span><span>(</span><span>mbar</span><span>,</span> <span>1</span><span>)</span>
        <span>mbarrier_arrive_expect_tx_shared</span><span>(</span><span>mbar</span><span>,</span> <span>block_size</span> <span>*</span> <span>block_size</span> <span>*</span> <span>4</span><span>)</span>
        <span>cp_async_bulk_tensor_shared_cluster_global</span><span>(</span>
            <span>shmem</span><span>,</span> <span>descriptor_ptr</span><span>,</span> <span>mbar</span><span>,</span> <span>Index</span><span>(</span><span>x</span><span>,</span> <span>y</span><span>)</span>
        <span>)</span>
    <span>barrier</span><span>()</span>
    <span>mbarrier_try_wait_parity_shared</span><span>(</span><span>mbar</span><span>,</span> <span>0</span><span>,</span> <span>10000000</span><span>)</span>
</pre></div>
<p>We annotate the descriptors with <code>nvvm.grid_constant</code> similar to what we would do in <code>CUDA</code>.
After allocating the shared memories we define the upper left coordinate of the tile using <code>x</code> and <code>y</code> and get <code>row</code> and <code>column</code> the current thread is responsible fore.
We'll than copy over the tile to the shared memory array.
This kernel archives a bandwidth of <code>1056.08 GB/s</code> which is faster than the <code>875.46 GB/s</code> we archived using <code>CUDA</code>. I believe that to be the reason because we use the <code>PTX</code> api for <code>TMA</code> transfers in Mojo. You can read about the difference between these in the <code>CUDA</code> api in <a href="https://cudaforfun.substack.com/p/outperforming-cublas-on-h100-a-worklog">this excellent blogpost</a>.</p>
<h3 id="compute-transpose-in-shared-memory">Compute transpose in shared memory</h3><div><pre><span></span><span># COMPUTE</span>
<span>shmem_tr</span><span>[</span><span>col</span> <span>*</span> <span>block_size</span> <span>+</span> <span>row</span><span>]</span> <span>=</span> <span>shmem</span><span>[</span><span>row</span> <span>*</span> <span>block_size</span> <span>+</span> <span>col</span><span>]</span>

<span># FENCE</span>
<span>barrier</span><span>()</span>
<span>tma_store_fence</span><span>()</span>
</pre></div>
<p>We compute the transpose using our two arrays. We'll than create a <code>fence</code> to let the <code>TMA</code> know we are finished with computation.</p>
<h3 id="store-to-gmem">Store to gmem</h3><div><pre><span></span><span># STORE</span>
<span>if</span> <span>thread_idx</span><span>.</span><span>x</span> <span>==</span> <span>0</span><span>:</span>
	<span>cp_async_bulk_tensor_global_shared_cta</span><span>(</span>
		<span>shmem_tr</span><span>,</span> <span>descriptor_tr_ptr</span><span>,</span> <span>Index</span><span>(</span><span>y</span><span>,</span> <span>x</span><span>)</span>
	<span>)</span>
	<span>cp_async_bulk_commit_group</span><span>()</span>

<span>cp_async_bulk_wait_group</span><span>[</span><span>0</span><span>]()</span>
</pre></div>
<p>We store the transposed result to the GMEM using the transposed TMA descriptor.</p>
<h2 id="swizzling">Swizzling</h2><p>For a more detailed explanation of what swizzling is and how it works please in my previous <a href="https://veitner.bearblog.dev/making-matrix-transpose-really-fast-on-hopper-gpus/">blogpost on matrix transpose</a> the concept is the same for Mojo. In the repo I link to at the end there is also one program which you can use to understand swizzling yourself.
Only two things need to be adjusted to make swizzling work:</p>
<ul>
<li>The descriptors need to be provided with the appropriate swizzling mode</li>
<li>Inside the kernel we need to use swizzled indices</li>
</ul>
<p>This can be implemented as follows</p>
<div><pre><span></span><span>var</span> <span>descriptor</span> <span>=</span> <span>create_tma_descriptor</span><span>[</span>
	<span>DType</span><span>.</span><span>float32</span><span>,</span> <span>2</span><span>,</span> <span>TensorMapSwizzle</span><span>.</span><span>SWIZZLE_128B</span>
<span>](</span>
	<span>gmem_dev</span><span>,</span>
	<span>(</span><span>GMEM_HEIGHT</span><span>,</span> <span>GMEM_WIDTH</span><span>),</span>
	<span>(</span><span>GMEM_WIDTH</span><span>,</span> <span>1</span><span>),</span>
	<span>(</span><span>SMEM_HEIGHT</span><span>,</span> <span>SMEM_WIDTH</span><span>),</span>
<span>)</span>
<span>var</span> <span>descriptor_tr</span> <span>=</span> <span>create_tma_descriptor</span><span>[</span>
	<span>DType</span><span>.</span><span>float32</span><span>,</span> <span>2</span><span>,</span> <span>TensorMapSwizzle</span><span>.</span><span>SWIZZLE_128B</span>
<span>](</span>
	<span>gmem_tr_dev</span><span>,</span>
	<span>(</span><span>GMEM_WIDTH</span><span>,</span> <span>GMEM_HEIGHT</span><span>),</span>
	<span>(</span><span>GMEM_HEIGHT</span><span>,</span> <span>1</span><span>),</span>
	<span>(</span><span>SMEM_WIDTH</span><span>,</span> <span>SMEM_HEIGHT</span><span>),</span>
<span>)</span>
</pre></div>
<p>We can compute swizzled indices like this:</p>
<div><pre><span></span><span>fn</span> <span>calculate_row_swizzle</span><span>[</span><span>block_size</span><span>:</span> <span>Int</span><span>](</span><span>col</span><span>:</span> <span>Int</span><span>,</span> <span>row</span><span>:</span> <span>Int</span><span>)</span> <span>-&gt;</span> <span>Int</span><span>:</span>
    <span>i16_tr</span> <span>=</span> <span>(</span><span>col</span> <span>*</span> <span>BLOCK_SIZE</span> <span>+</span> <span>row</span><span>)</span> <span>*</span> <span>4</span> <span>&gt;&gt;</span> <span>4</span>
    <span>y16_tr</span> <span>=</span> <span>i16_tr</span> <span>&gt;&gt;</span> <span>3</span>
    <span>x16_tr</span> <span>=</span> <span>i16_tr</span> <span>&amp;</span> <span>7</span>
    <span>x16_swz_tr</span> <span>=</span> <span>y16_tr</span> <span>^</span> <span>x16_tr</span>
    <span>return</span> <span>((</span><span>x16_swz_tr</span> <span>*</span> <span>4</span><span>)</span> <span>&amp;</span> <span>(</span><span>BLOCK_SIZE</span> <span>-</span> <span>1</span><span>))</span> <span>+</span> <span>(</span><span>row</span> <span>&amp;</span> <span>3</span><span>)</span>


<span>fn</span> <span>calculate_col_swizzle</span><span>[</span><span>block_size</span><span>:</span> <span>Int</span><span>](</span><span>col</span><span>:</span> <span>Int</span><span>,</span> <span>row</span><span>:</span> <span>Int</span><span>)</span> <span>-&gt;</span> <span>Int</span><span>:</span>
    <span>i16</span> <span>=</span> <span>(</span><span>row</span> <span>*</span> <span>BLOCK_SIZE</span> <span>+</span> <span>col</span><span>)</span> <span>*</span> <span>4</span> <span>&gt;&gt;</span> <span>4</span>
    <span>y16</span> <span>=</span> <span>i16</span> <span>&gt;&gt;</span> <span>3</span>
    <span>x16</span> <span>=</span> <span>i16</span> <span>&amp;</span> <span>7</span>
    <span>x16_swz</span> <span>=</span> <span>y16</span> <span>^</span> <span>x16</span>
    <span>return</span> <span>((</span><span>x16_swz</span> <span>*</span> <span>4</span><span>)</span> <span>&amp;</span> <span>(</span><span>block_size</span> <span>-</span> <span>1</span><span>))</span> <span>+</span> <span>(</span><span>col</span> <span>&amp;</span> <span>3</span><span>)</span>
</pre></div>
<p>and than use the swizzled indices inside our kernel like so:</p>
<div><pre><span></span><span>col_swizzle</span> <span>=</span> <span>calculate_col_swizzle</span><span>[</span><span>block_size</span><span>](</span><span>col</span><span>,</span> <span>row</span><span>)</span>
<span>row_swizzle</span> <span>=</span> <span>calculate_row_swizzle</span><span>[</span><span>block_size</span><span>](</span><span>col</span><span>,</span> <span>row</span><span>)</span>
<span>...</span>
<span># COMPUTE</span>
<span>shmem_tr</span><span>[</span><span>col</span> <span>*</span> <span>block_size</span> <span>+</span> <span>row_swizzle</span><span>]</span> <span>=</span> <span>shmem</span><span>[</span>
	<span>row</span> <span>*</span> <span>block_size</span> <span>+</span> <span>col_swizzle</span>
<span>]</span>
</pre></div>
<p>Everything else is exactly the same.</p>
<p>This kernel archives <code>1437.55 GB/s</code> compared to the <code>1251.76 GB/s</code> we get in <code>CUDA</code>.</p>
<h2 id="processes-a-batch-of-columns-per-thread">Processes a batch of columns per thread</h2><p>An important and common optimisation one can apply in memory bound kernels is thread coarsening which is essentially putting more work on each thread.
We can modify the previous kernel as follows to do that:</p>
<div><pre><span></span>    <span># COMPUTE</span>
    <span>@parameter</span>
    <span>for</span> <span>i</span> <span>in</span> <span>range</span><span>(</span><span>batch_size</span><span>):</span>
        <span>col_</span> <span>=</span> <span>col</span> <span>+</span> <span>i</span>
        <span>row_</span> <span>=</span> <span>row</span>
        <span>col_swizzle</span> <span>=</span> <span>calculate_col_swizzle</span><span>[</span><span>block_size</span><span>](</span><span>col_</span><span>,</span> <span>row_</span><span>)</span>
        <span>row_swizzle</span> <span>=</span> <span>calculate_row_swizzle</span><span>[</span><span>block_size</span><span>](</span><span>col_</span><span>,</span> <span>row_</span><span>)</span>
        <span>shmem_tr</span><span>[</span><span>col</span> <span>*</span> <span>block_size</span> <span>+</span> <span>row_swizzle</span><span>]</span> <span>=</span> <span>shmem</span><span>[</span>
            <span>row</span> <span>*</span> <span>block_size</span> <span>+</span> <span>col_swizzle</span>
        <span>]</span>
</pre></div>
<p>Note that we launch less threads with this approach (we divide by a factor of <code>batch_size</code>) to account for the fact we are processing multiple columns per thread now.</p>
<p>This kernel archives a bandwidth of <code>2775.49 GB/s</code> compared to the <code>2771.35 GB/s</code> we archived in the equivalent <code>CUDA</code> kernel.</p>
<h2 id="conclusion">Conclusion</h2><p>I hope this blogpost showed you how to archive high performance on a common task in GPU computing using Mojo.
Feel free to contact me on <a href="https://www.linkedin.com/in/simon-veitner-174a681b6/">Linkedin</a> to chat about GPU programming or other topics related to MLSys.</p>
<p>The full code for the blogpost can be find on my <a href="https://github.com/simveit/efficient_transpose_mojo">Github</a>.</p>


    

    
        

        
            


        
    


  </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[The Illusion of Thinking: Understanding the Limitations of Reasoning LLMs [pdf] (258 pts)]]></title>
            <link>https://ml-site.cdn-apple.com/papers/the-illusion-of-thinking.pdf</link>
            <guid>44203562</guid>
            <pubDate>Fri, 06 Jun 2025 18:18:36 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://ml-site.cdn-apple.com/papers/the-illusion-of-thinking.pdf">https://ml-site.cdn-apple.com/papers/the-illusion-of-thinking.pdf</a>, See on <a href="https://news.ycombinator.com/item?id=44203562">Hacker News</a></p>
&lt;Not HTML&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[SaaS is just vendor lock-in with better branding (191 pts)]]></title>
            <link>https://rwsdk.com/blog/saas-is-just-vendor-lock-in-with-better-branding</link>
            <guid>44203494</guid>
            <pubDate>Fri, 06 Jun 2025 18:10:29 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://rwsdk.com/blog/saas-is-just-vendor-lock-in-with-better-branding">https://rwsdk.com/blog/saas-is-just-vendor-lock-in-with-better-branding</a>, See on <a href="https://news.ycombinator.com/item?id=44203494">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
<p>Developers are told "to focus on the product" and let SaaS vendors handle the rest, but integrating third-party services, whether it's auth, queuing, file storage, or image optimization, comes at a cost. Not just in dollars but in time, friction, and mental overhead.</p>
<p>There are five hidden taxes you pay every time you integrate a SaaS into your stack.</p>
<h2>1. The Discovery Tax</h2>
<p>Before you can integrate anything, you first have to figure out what they're actually selling?</p>
<ol>
<li>What problems are they solving?</li>
<li>Is it compatible with your stack?</li>
<li>Is their price sane at your scale? a</li>
<li>Are their docs clear and do they reveal any implementation weirdness?</li>
</ol>
<p>This unpaid research work is usually non-transferable. What you learn about "Uploady" or "MegaQueue" doesn't help you next time when you're evaluating something else. It's also subjective. It's marketing, and does the marketing message resonate with you?</p>
<h2>2. The Sign-Up Tax</h2>
<p>You've decided on a service, and this is the moment when you hand over your email and credit card.</p>
<ol>
<li>Do they support usage-based pricing or only lock-in tiers?</li>
<li>Can your team members access the dashboard, or do you have to pay more for that functionality? Despite only using the service the same amount!</li>
<li>Can you even test the product without hitting a paywall?</li>
</ol>
<p>You're now on the hook, even if you haven't written a single line of code.</p>
<h2>3. The Integration Tax</h2>
<p>Now the real work begins.</p>
<ol>
<li>You read the docs.</li>
<li>You install the libraries</li>
<li>You wire it into your framework.</li>
<li>And figure out the edge cases that the docs don't mention, because docs are marketing!</li>
</ol>
<p>Often you're left fighting your own tooling. They're aiming for the lowest common denominator, and you're bleeding edge. Or the other way around!</p>
<h2>4. The Local Development Tax</h2>
<p>You need the SaaS service to work locally. Does it even offer a local emulator? Can you stub it out in tests? Do you need to tunnel to the cloud just to test one feature?</p>
<p>Now you've got branching configuration logic, one for production, one for staging, one for local… If you're lucky.</p>
<h2>5. The Production Tax</h2>
<p>This is the part where you're "done," except you're not.</p>
<ol>
<li>Can you use this in your staging environment? What about pull request previews?</li>
<li>You need to securely manage the API keys.</li>
<li>Monitoring, logging, and alerting</li>
<li>Wondering why something worked in your laptop but fails in production?</li>
</ol>
<p>You've integrated the service, but now you're on the hook for its reliability in production.</p>
<h2>Conclusion</h2>
<p>The pitch of modern SaaS is "don't reinvent the wheel." But every wheel you bolt on comes with some friction. It's not just a service: It's a contract. It's a dependency. It's a subtle architectural shift, and it comes with taxes.</p>
<p>No matter what choice you make, it's always going to be vendor-locked in. Switching out something, even if it's open source and self-hosted, means that you're rewriting a lot of code.</p>
<p>So, my argument is, don't make those decisions. Just pick a platform. The thing that matters is the software that you want to write, not the framework or the services that it runs on.</p>
<p>Platforms like Cloudflare or Supabase shine. Where your database, queue, image service, and storage all live within the same platform and speak the same language. You avoid paying these taxes repeatedly. You simply pick the product that's already there.</p>
<ol>
<li>No context switching between vendors.</li>
<li>No API key wrangling.</li>
<li>No compatibility hacks or configuration forks.</li>
<li>Just fast, local feeling integrations that work the same in dev and production.</li>
</ol>
<p>It feels like everything is running on the same machine, and in a way it kind of is. That's the hidden superpower of integrated platforms. They collapse the distance between your code and your services. And in doing so, they give you back the one thing no SaaS vendor can sell you: "Flow."</p>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Breakthrough in search for HIV cure leaves researchers 'overwhelmed' (199 pts)]]></title>
            <link>https://www.theguardian.com/global-development/2025/jun/05/breakthrough-in-search-for-hiv-cure-leaves-researchers-overwhelmed</link>
            <guid>44202664</guid>
            <pubDate>Fri, 06 Jun 2025 16:44:12 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.theguardian.com/global-development/2025/jun/05/breakthrough-in-search-for-hiv-cure-leaves-researchers-overwhelmed">https://www.theguardian.com/global-development/2025/jun/05/breakthrough-in-search-for-hiv-cure-leaves-researchers-overwhelmed</a>, See on <a href="https://news.ycombinator.com/item?id=44202664">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="maincontent"><p>A cure for HIV could be a step closer after researchers found a new way to force the virus out of hiding inside human cells.</p><p>The virus’s ability to conceal itself inside certain white blood cells has been one of the main challenges for scientists looking for a cure. It means there is a reservoir of the HIV in the body, capable of reactivation, that neither the immune system nor drugs can tackle.</p><p>Now researchers from the Peter Doherty Institute for Infection and Immunity in Melbourne, have demonstrated a way to make the virus visible, paving the way to fully clear it from the body.</p><p>It is based on mRNA technology, which came to prominence during the Covid-19 pandemic <a href="https://www.theguardian.com/science/2021/nov/01/flu-cancer-hiv-after-covid-success-what-next-for-mrna-vaccines" data-link-name="in body link">when it was used in vaccines</a> made by Moderna and Pfizer/BioNTech.</p><p>In a <a href="https://www.nature.com/articles/s41467-025-60001-2" data-link-name="in body link">paper published in Nature Communications</a>, the researchers have shown for the first time that mRNA can be delivered into the cells where HIV is hiding, by encasing it in a tiny, specially formulated fat bubble. The mRNA then instructs the cells to reveal the virus.</p><p>Globally, there are <a href="https://www.unaids.org/en/resources/fact-sheet" data-link-name="in body link">almost 40 million people</a> living with HIV, who must take medication for the rest of their lives in order to suppress the virus and ensure they do not develop symptoms or transmit it. For many it remains deadly, with UNAids figures suggesting one person died of HIV every minute in 2023.</p><p>It was “previously thought impossible” to deliver mRNA to the type of white blood cell that is home to HIV, said Dr Paula Cevaal, research fellow at the Doherty Institute and co-first author of the study, because those cells did not take up the fat bubbles, or lipid nanoparticles (LNPs), used to carry it.</p><p>The team have developed a new type of LNP that those cells will accept, known as LNP X. She said: “Our hope is that this new nanoparticle design could be a new pathway to an HIV cure.”</p><p>When a colleague first presented test results at the lab’s weekly meeting, Cevaal said, they seemed too good to be true.</p><p>“We sent her back into the lab to repeat it, and she came back the next week with results that were equally good. So we had to believe it. And of course, since then, we’ve repeated it many, many, many more times.</p><p>“We were overwhelmed by how [much of a] night and day difference it was – from not working before, and then all of a sudden it was working. And all of us were just sitting gasping like, ‘wow’.”</p><p>Further research will be needed to determine whether revealing the virus is enough to allow the body’s immune system to deal with it, or whether the technology will need to be combined with other therapies to eliminate HIV from the body.</p><p>The study is laboratory based and was carried out in cells donated by HIV patients. The path to using the technology as part of a cure for patients is long, and would require successful tests in animals followed by safety trials in humans, likely to take years, before efficacy trials could even begin.</p><p>“In the field of biomedicine, many things eventually don’t make it into the clinic – that is the unfortunate truth; I don’t want to paint a prettier picture than what is the reality,” stressed Cevaal. “But in terms of specifically the field of HIV cure, we have never seen anything close to as good as what we are seeing, in terms of how well we are able to reveal this virus.</p><p>“So from that point of view, we’re very hopeful that we are also able to see this type of response in an animal, and that we could eventually do this in humans.”</p><p>Dr Michael Roche of the University of Melbourne and co-senior author of the research, said the discovery could have broader implications beyond HIV, with the relevant white blood cells also involved in other diseases including cancers.</p><p>Dr Jonathan Stoye, a retrovirologist and emeritus scientist at the Francis Crick Institute, who was not involved in the study, said the approach taken by the Melbourne team appeared be a major advance on existing strategies to force the virus out of hiding, but further studies would be needed to determine how best to kill it after that.</p><p>He added: “Ultimately, one big unknown remains. Do you need to eliminate the entire reservoir for success or just the major part? If just 10% of the latent reservoir survives will that be sufficient to seed new infection? Only time will tell.</p><p>“However, that does not detract from the significance of the current study, which represents a major potential advance in delivery of mRNA for therapeutic purposes to blood cells.”</p><p>Prof Tomáš Hanke of the Jenner Institute, University of Oxford, disputed the idea that getting RNA into white blood cells had been a significant challenge. He said the hope that all cells in the body where HIV was hiding could be reached in this way was “merely a dream”.</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Decreasing Gitlab repo backup times from 48 hours to 41 minutes (461 pts)]]></title>
            <link>https://about.gitlab.com/blog/2025/06/05/how-we-decreased-gitlab-repo-backup-times-from-48-hours-to-41-minutes/</link>
            <guid>44201975</guid>
            <pubDate>Fri, 06 Jun 2025 15:43:05 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://about.gitlab.com/blog/2025/06/05/how-we-decreased-gitlab-repo-backup-times-from-48-hours-to-41-minutes/">https://about.gitlab.com/blog/2025/06/05/how-we-decreased-gitlab-repo-backup-times-from-48-hours-to-41-minutes/</a>, See on <a href="https://news.ycombinator.com/item?id=44201975">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-v-67d56f21="" data-v-53094866="" data-v-74bd29c6=""><p>Repository backups are a critical component of any robust disaster recovery strategy. However, as repositories grow in size, the process of creating reliable backups becomes increasingly challenging.  Our own <a href="https://gitlab.com/gitlab-org/gitlab">Rails repository</a> was taking 48 hours to back up — forcing impossible choices between backup frequency and system performance. We wanted to tackle this issue for our customers and for our own users internally.</p>
<p>Ultimately, we traced the issue to a 15-year-old Git function with O(N²) complexity and fixed it with an algorithmic change, <strong>reducing backup times exponentially</strong>. The result: lower costs, reduced risk, and backup strategies that actually scale with your codebase.</p>
<p>This turned out to be a Git scalability issue that affects anyone with large repositories. Here's how we tracked it down and fixed it.</p>
<h2 id="backup-at-scale" tabindex="-1">Backup at scale <a href="#backup-at-scale"><svg width="24" height="24" viewBox="0 0 16 16" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M12.2426 3.75736C11.4615 2.97631 10.1952 2.97631 9.41416 3.75736L7.99995 5.17157C7.60942 5.56209 6.97626 5.56209 6.58573 5.17157C6.19521 4.78105 6.19521 4.14788 6.58573 3.75736L7.99995 2.34314C9.56205 0.781046 12.0947 0.781046 13.6568 2.34314C15.2189 3.90524 15.2189 6.4379 13.6568 8L12.2426 9.41421C11.8521 9.80473 11.2189 9.80473 10.8284 9.41421C10.4379 9.02369 10.4379 8.39052 10.8284 8L12.2426 6.58578C13.0236 5.80473 13.0236 4.5384 12.2426 3.75736Z" fill="#333333"></path><path d="M10.5355 5.4645C10.926 5.85502 10.926 6.48819 10.5355 6.87871L6.87863 10.5356C6.4881 10.9261 5.85494 10.9261 5.46441 10.5356C5.07389 10.145 5.07389 9.51188 5.46441 9.12135L9.12127 5.4645C9.51179 5.07397 10.145 5.07397 10.5355 5.4645Z" fill="#333333"></path><path d="M3.75742 9.41422C2.97637 10.1953 2.97637 11.4616 3.75742 12.2426C4.53847 13.0237 5.8048 13.0237 6.58584 12.2426L8.00006 10.8284C8.39058 10.4379 9.02375 10.4379 9.41427 10.8284C9.8048 11.219 9.8048 11.8521 9.41427 12.2426L8.00006 13.6569C6.43796 15.219 3.9053 15.219 2.3432 13.6569C0.781107 12.0948 0.781107 9.56211 2.3432 8.00001L3.75742 6.5858C4.14794 6.19527 4.78111 6.19527 5.17163 6.5858C5.56216 6.97632 5.56215 7.60948 5.17163 8.00001L3.75742 9.41422Z" fill="#333333"></path></svg></a></h2>
<p>First, let's look at the problem. As organizations scale their repositories and backups grow more complex, here are some of the challenges they can face:</p>
<ul>
<li><strong>Time-prohibitive backups:</strong> For very large repositories, creating a repository backup could take several hours, which can hinder the ability to schedule regular backups.</li>
<li><strong>Resource intensity:</strong> Extended backup processes can consume substantial server resources, potentially impacting other operations.</li>
<li><strong>Backup windows:</strong> Finding adequate maintenance windows for such lengthy processes can be difficult for teams running 24/7 operations.</li>
<li><strong>Increased failure risk:</strong> Long-running processes are more susceptible to interruptions from network issues, server restarts, and system errors, which can force teams to restart the entire very long backup process from scratch.</li>
<li><strong>Race conditions:</strong> Because it takes a long time to create a backup, the repository might have changed a lot during the process, potentially creating an invalid backup or interrupting the backup because objects are no longer available.</li>
</ul>
<p>These challenges can lead to compromising on backup frequency or completeness – an unacceptable trade-off when it comes to data protection. Extended backup windows can force customers into workarounds. Some might adopt external tooling, while others might reduce backup frequency, resulting in potential inconsistent data protection strategies across organizations.</p>
<p>Now, let's dig into how we identified a performance bottleneck, found a resolution, and deployed it to help cut backup times.</p>
<h2 id="the-technical-challenge" tabindex="-1">The technical challenge <a href="#the-technical-challenge"><svg width="24" height="24" viewBox="0 0 16 16" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M12.2426 3.75736C11.4615 2.97631 10.1952 2.97631 9.41416 3.75736L7.99995 5.17157C7.60942 5.56209 6.97626 5.56209 6.58573 5.17157C6.19521 4.78105 6.19521 4.14788 6.58573 3.75736L7.99995 2.34314C9.56205 0.781046 12.0947 0.781046 13.6568 2.34314C15.2189 3.90524 15.2189 6.4379 13.6568 8L12.2426 9.41421C11.8521 9.80473 11.2189 9.80473 10.8284 9.41421C10.4379 9.02369 10.4379 8.39052 10.8284 8L12.2426 6.58578C13.0236 5.80473 13.0236 4.5384 12.2426 3.75736Z" fill="#333333"></path><path d="M10.5355 5.4645C10.926 5.85502 10.926 6.48819 10.5355 6.87871L6.87863 10.5356C6.4881 10.9261 5.85494 10.9261 5.46441 10.5356C5.07389 10.145 5.07389 9.51188 5.46441 9.12135L9.12127 5.4645C9.51179 5.07397 10.145 5.07397 10.5355 5.4645Z" fill="#333333"></path><path d="M3.75742 9.41422C2.97637 10.1953 2.97637 11.4616 3.75742 12.2426C4.53847 13.0237 5.8048 13.0237 6.58584 12.2426L8.00006 10.8284C8.39058 10.4379 9.02375 10.4379 9.41427 10.8284C9.8048 11.219 9.8048 11.8521 9.41427 12.2426L8.00006 13.6569C6.43796 15.219 3.9053 15.219 2.3432 13.6569C0.781107 12.0948 0.781107 9.56211 2.3432 8.00001L3.75742 6.5858C4.14794 6.19527 4.78111 6.19527 5.17163 6.5858C5.56216 6.97632 5.56215 7.60948 5.17163 8.00001L3.75742 9.41422Z" fill="#333333"></path></svg></a></h2>
<p>GitLab's repository backup functionality relies on the <a href="https://git-scm.com/docs/git-bundle"><code>git bundle create</code></a> command, which captures a complete snapshot of a repository, including all objects and references like branches and tags. This bundle serves as a restoration point for recreating the repository in its exact state.</p>
<p>However, the implementation of the command suffered from poor scalability related to reference count, creating a performance bottleneck. As repositories accumulated more references, processing time increased exponentially. In our largest repositories containing millions of references, backup operations could extend beyond 48 hours.</p>
<h3 id="root-cause-analysis" tabindex="-1">Root cause analysis <a href="#root-cause-analysis"><svg width="24" height="24" viewBox="0 0 16 16" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M12.2426 3.75736C11.4615 2.97631 10.1952 2.97631 9.41416 3.75736L7.99995 5.17157C7.60942 5.56209 6.97626 5.56209 6.58573 5.17157C6.19521 4.78105 6.19521 4.14788 6.58573 3.75736L7.99995 2.34314C9.56205 0.781046 12.0947 0.781046 13.6568 2.34314C15.2189 3.90524 15.2189 6.4379 13.6568 8L12.2426 9.41421C11.8521 9.80473 11.2189 9.80473 10.8284 9.41421C10.4379 9.02369 10.4379 8.39052 10.8284 8L12.2426 6.58578C13.0236 5.80473 13.0236 4.5384 12.2426 3.75736Z" fill="#333333"></path><path d="M10.5355 5.4645C10.926 5.85502 10.926 6.48819 10.5355 6.87871L6.87863 10.5356C6.4881 10.9261 5.85494 10.9261 5.46441 10.5356C5.07389 10.145 5.07389 9.51188 5.46441 9.12135L9.12127 5.4645C9.51179 5.07397 10.145 5.07397 10.5355 5.4645Z" fill="#333333"></path><path d="M3.75742 9.41422C2.97637 10.1953 2.97637 11.4616 3.75742 12.2426C4.53847 13.0237 5.8048 13.0237 6.58584 12.2426L8.00006 10.8284C8.39058 10.4379 9.02375 10.4379 9.41427 10.8284C9.8048 11.219 9.8048 11.8521 9.41427 12.2426L8.00006 13.6569C6.43796 15.219 3.9053 15.219 2.3432 13.6569C0.781107 12.0948 0.781107 9.56211 2.3432 8.00001L3.75742 6.5858C4.14794 6.19527 4.78111 6.19527 5.17163 6.5858C5.56216 6.97632 5.56215 7.60948 5.17163 8.00001L3.75742 9.41422Z" fill="#333333"></path></svg></a></h3>
<p>To identify the root cause of this performance bottleneck, we analyzed a flame graph of the command during execution.</p>
<p><img src="https://images.ctfassets.net/r9o86ar0p03f/7IS0wTEb44v1DTFKdklt1s/f8c724b1578ef0c8f550d9237f03dbeb/image1.jpg" alt="Flame graph showing command during execution"></p>
<p>A flame graph displays the execution path of a command through its stack trace. Each bar corresponds to a function in the code, with the bar's width indicating how much time the command spent executing within that particular function.</p>
<p>When examining the flame graph of <code>git bundle create</code> running on a repository with 10,000 references, approximately 80% of the execution time is consumed by the <code>object_array_remove_duplicates()</code> function. This function was introduced to Git in the <a href="https://gitlab.com/gitlab-org/git/-/commit/b2a6d1c686">commit b2a6d1c686</a> (bundle: allow the same ref to be given more than once, 2009-01-17).</p>
<p>To understand this change, it's important to know that <code>git bundle create</code> allows users to specify which references to include in the bundle. For complete repository bundles, the <code>--all</code> flag packages all references.</p>
<p>The commit addressed a problem where users providing duplicate references through the command line – such as <code>git bundle create main.bundle main main</code> - would create a bundle without properly handling the duplicated main reference. Unbundling this bundle in a Git repository would break, because it tries to write the same ref twice. The code to avoid duplication uses nested <code>for</code> loops that iterate through all references to identify duplicates. This O(N²) algorithm becomes a significant performance bottleneck in repositories with large reference counts, consuming substantial processing time.</p>
<h3 id="the-fix-from-o(n%C2%B2)-to-efficient-mapping" tabindex="-1">The fix: From O(N²) to efficient mapping <a href="#the-fix-from-o(n%C2%B2)-to-efficient-mapping"><svg width="24" height="24" viewBox="0 0 16 16" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M12.2426 3.75736C11.4615 2.97631 10.1952 2.97631 9.41416 3.75736L7.99995 5.17157C7.60942 5.56209 6.97626 5.56209 6.58573 5.17157C6.19521 4.78105 6.19521 4.14788 6.58573 3.75736L7.99995 2.34314C9.56205 0.781046 12.0947 0.781046 13.6568 2.34314C15.2189 3.90524 15.2189 6.4379 13.6568 8L12.2426 9.41421C11.8521 9.80473 11.2189 9.80473 10.8284 9.41421C10.4379 9.02369 10.4379 8.39052 10.8284 8L12.2426 6.58578C13.0236 5.80473 13.0236 4.5384 12.2426 3.75736Z" fill="#333333"></path><path d="M10.5355 5.4645C10.926 5.85502 10.926 6.48819 10.5355 6.87871L6.87863 10.5356C6.4881 10.9261 5.85494 10.9261 5.46441 10.5356C5.07389 10.145 5.07389 9.51188 5.46441 9.12135L9.12127 5.4645C9.51179 5.07397 10.145 5.07397 10.5355 5.4645Z" fill="#333333"></path><path d="M3.75742 9.41422C2.97637 10.1953 2.97637 11.4616 3.75742 12.2426C4.53847 13.0237 5.8048 13.0237 6.58584 12.2426L8.00006 10.8284C8.39058 10.4379 9.02375 10.4379 9.41427 10.8284C9.8048 11.219 9.8048 11.8521 9.41427 12.2426L8.00006 13.6569C6.43796 15.219 3.9053 15.219 2.3432 13.6569C0.781107 12.0948 0.781107 9.56211 2.3432 8.00001L3.75742 6.5858C4.14794 6.19527 4.78111 6.19527 5.17163 6.5858C5.56216 6.97632 5.56215 7.60948 5.17163 8.00001L3.75742 9.41422Z" fill="#333333"></path></svg></a></h3>
<p>To resolve this performance issue, we contributed an upstream fix to Git that replaces the nested loops with a map data structure. Each reference is added to the map, which automatically ensures only a single copy of each reference is retained for processing.</p>
<p>This change dramatically enhances the performance of <code>git bundle create</code> and enables much better scalability in repositories with large reference counts. Benchmark testing on a repository with 10,000 references demonstrates a 6x performance improvement.</p>
<pre><code>Benchmark 1: bundle (refcount = 100000, revision = master)
  Time (mean ± σ): 	14.653 s ±  0.203 s	[User: 13.940 s, System: 0.762 s]
  Range (min … max):   14.237 s … 14.920 s	10 runs

Benchmark 2: bundle (refcount = 100000, revision = HEAD)
  Time (mean ± σ):  	2.394 s ±  0.023 s	[User: 1.684 s, System: 0.798 s]
  Range (min … max):	2.364 s …  2.425 s	10 runs

Summary
  bundle (refcount = 100000, revision = HEAD) ran
	6.12 ± 0.10 times faster than bundle (refcount = 100000, revision = master)
</code></pre>
<p>The patch was accepted and <a href="https://gitlab.com/gitlab-org/git/-/commit/bb74c0abbc31da35be52999569ea481ebd149d1d">merged</a> into upstream Git. At GitLab, we backported this fix to ensure our customers could benefit immediately, without waiting for the next Git release.</p>
<h2 id="the-result-dramatically-decreased-backup-times" tabindex="-1">The result: Dramatically decreased backup times <a href="#the-result-dramatically-decreased-backup-times"><svg width="24" height="24" viewBox="0 0 16 16" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M12.2426 3.75736C11.4615 2.97631 10.1952 2.97631 9.41416 3.75736L7.99995 5.17157C7.60942 5.56209 6.97626 5.56209 6.58573 5.17157C6.19521 4.78105 6.19521 4.14788 6.58573 3.75736L7.99995 2.34314C9.56205 0.781046 12.0947 0.781046 13.6568 2.34314C15.2189 3.90524 15.2189 6.4379 13.6568 8L12.2426 9.41421C11.8521 9.80473 11.2189 9.80473 10.8284 9.41421C10.4379 9.02369 10.4379 8.39052 10.8284 8L12.2426 6.58578C13.0236 5.80473 13.0236 4.5384 12.2426 3.75736Z" fill="#333333"></path><path d="M10.5355 5.4645C10.926 5.85502 10.926 6.48819 10.5355 6.87871L6.87863 10.5356C6.4881 10.9261 5.85494 10.9261 5.46441 10.5356C5.07389 10.145 5.07389 9.51188 5.46441 9.12135L9.12127 5.4645C9.51179 5.07397 10.145 5.07397 10.5355 5.4645Z" fill="#333333"></path><path d="M3.75742 9.41422C2.97637 10.1953 2.97637 11.4616 3.75742 12.2426C4.53847 13.0237 5.8048 13.0237 6.58584 12.2426L8.00006 10.8284C8.39058 10.4379 9.02375 10.4379 9.41427 10.8284C9.8048 11.219 9.8048 11.8521 9.41427 12.2426L8.00006 13.6569C6.43796 15.219 3.9053 15.219 2.3432 13.6569C0.781107 12.0948 0.781107 9.56211 2.3432 8.00001L3.75742 6.5858C4.14794 6.19527 4.78111 6.19527 5.17163 6.5858C5.56216 6.97632 5.56215 7.60948 5.17163 8.00001L3.75742 9.41422Z" fill="#333333"></path></svg></a></h2>
<p>The performance gains from this improvement have been nothing short of transformative:</p>
<ul>
<li><strong>From 48 hours to 41 minutes:</strong> Creating a backup of our largest repository (<code>gitlab-org/gitlab</code>) now takes just 1.4% of the original time.</li>
<li><strong>Consistent performance:</strong> The improvement scales reliably across repository sizes.</li>
<li><strong>Resource efficiency:</strong> We significantly reduced server load during backup operations.</li>
<li><strong>Broader applicability:</strong> While backup creation sees the most dramatic improvement, all bundle-based operations that operate on many references benefit.</li>
</ul>
<h2 id="what-this-means-for-gitlab-customers" tabindex="-1">What this means for GitLab customers <a href="#what-this-means-for-gitlab-customers"><svg width="24" height="24" viewBox="0 0 16 16" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M12.2426 3.75736C11.4615 2.97631 10.1952 2.97631 9.41416 3.75736L7.99995 5.17157C7.60942 5.56209 6.97626 5.56209 6.58573 5.17157C6.19521 4.78105 6.19521 4.14788 6.58573 3.75736L7.99995 2.34314C9.56205 0.781046 12.0947 0.781046 13.6568 2.34314C15.2189 3.90524 15.2189 6.4379 13.6568 8L12.2426 9.41421C11.8521 9.80473 11.2189 9.80473 10.8284 9.41421C10.4379 9.02369 10.4379 8.39052 10.8284 8L12.2426 6.58578C13.0236 5.80473 13.0236 4.5384 12.2426 3.75736Z" fill="#333333"></path><path d="M10.5355 5.4645C10.926 5.85502 10.926 6.48819 10.5355 6.87871L6.87863 10.5356C6.4881 10.9261 5.85494 10.9261 5.46441 10.5356C5.07389 10.145 5.07389 9.51188 5.46441 9.12135L9.12127 5.4645C9.51179 5.07397 10.145 5.07397 10.5355 5.4645Z" fill="#333333"></path><path d="M3.75742 9.41422C2.97637 10.1953 2.97637 11.4616 3.75742 12.2426C4.53847 13.0237 5.8048 13.0237 6.58584 12.2426L8.00006 10.8284C8.39058 10.4379 9.02375 10.4379 9.41427 10.8284C9.8048 11.219 9.8048 11.8521 9.41427 12.2426L8.00006 13.6569C6.43796 15.219 3.9053 15.219 2.3432 13.6569C0.781107 12.0948 0.781107 9.56211 2.3432 8.00001L3.75742 6.5858C4.14794 6.19527 4.78111 6.19527 5.17163 6.5858C5.56216 6.97632 5.56215 7.60948 5.17163 8.00001L3.75742 9.41422Z" fill="#333333"></path></svg></a></h2>
<p>For GitLab customers, this enhancement delivers immediate and tangible benefits on how organizations approach repository backup and disaster recovery planning:</p>
<ul>
<li><strong>Transformed backup strategies</strong>
<ul>
<li>Enterprise teams can establish comprehensive nightly schedules without impacting development workflows or requiring extensive backup windows.</li>
<li>Backups can now run seamlessly in the background during nightly schedules, instead of needing to be dedicated and lengthy.</li>
</ul>
</li>
<li><strong>Enhanced business continuity</strong>
<ul>
<li>With backup times reduced from days to minutes, organizations significantly minimize their recovery point objectives (RPO). This translates to reduced business risk – in a disaster scenario, you're potentially recovering hours of work instead of days.</li>
</ul>
</li>
<li><strong>Reduced operational overhead</strong>
<ul>
<li>Less server resource consumption and shorter maintenance windows.</li>
<li>Shorter backup windows mean reduced compute costs, especially in cloud environments, where extended processing time translates directly to higher bills.</li>
</ul>
</li>
<li><strong>Future-proofed infrastructure</strong>
<ul>
<li>Growing repositories no longer force difficult choices between backup frequency and system performance.</li>
<li>As your codebase expands, your backup strategy can scale seamlessly alongside it</li>
</ul>
</li>
</ul>
<p>Organizations can now implement more robust backup strategies without compromising on performance or completeness. What was once a challenging trade-off has become a straightforward operational practice.</p>
<p>Starting with the <a href="https://about.gitlab.com/releases/2025/05/15/gitlab-18-0-released/">GitLab 18.0</a> release, all GitLab customers regardless of their license tier can already fully take advantage of these improvements for their <a href="https://docs.gitlab.com/administration/backup_restore/backup_gitlab/">backup</a> strategy and execution. There is no further change in configuration required.</p>
<h2 id="what's-next" tabindex="-1">What's next <a href="#what's-next"><svg width="24" height="24" viewBox="0 0 16 16" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M12.2426 3.75736C11.4615 2.97631 10.1952 2.97631 9.41416 3.75736L7.99995 5.17157C7.60942 5.56209 6.97626 5.56209 6.58573 5.17157C6.19521 4.78105 6.19521 4.14788 6.58573 3.75736L7.99995 2.34314C9.56205 0.781046 12.0947 0.781046 13.6568 2.34314C15.2189 3.90524 15.2189 6.4379 13.6568 8L12.2426 9.41421C11.8521 9.80473 11.2189 9.80473 10.8284 9.41421C10.4379 9.02369 10.4379 8.39052 10.8284 8L12.2426 6.58578C13.0236 5.80473 13.0236 4.5384 12.2426 3.75736Z" fill="#333333"></path><path d="M10.5355 5.4645C10.926 5.85502 10.926 6.48819 10.5355 6.87871L6.87863 10.5356C6.4881 10.9261 5.85494 10.9261 5.46441 10.5356C5.07389 10.145 5.07389 9.51188 5.46441 9.12135L9.12127 5.4645C9.51179 5.07397 10.145 5.07397 10.5355 5.4645Z" fill="#333333"></path><path d="M3.75742 9.41422C2.97637 10.1953 2.97637 11.4616 3.75742 12.2426C4.53847 13.0237 5.8048 13.0237 6.58584 12.2426L8.00006 10.8284C8.39058 10.4379 9.02375 10.4379 9.41427 10.8284C9.8048 11.219 9.8048 11.8521 9.41427 12.2426L8.00006 13.6569C6.43796 15.219 3.9053 15.219 2.3432 13.6569C0.781107 12.0948 0.781107 9.56211 2.3432 8.00001L3.75742 6.5858C4.14794 6.19527 4.78111 6.19527 5.17163 6.5858C5.56216 6.97632 5.56215 7.60948 5.17163 8.00001L3.75742 9.41422Z" fill="#333333"></path></svg></a></h2>
<p>This breakthrough is part of our ongoing commitment to scalable, enterprise-grade Git infrastructure. While the improvement of 48 hours to 41 minutes for backup creation time represents a significant milestone, we continue to identify and address performance bottlenecks throughout our stack.</p>
<p>We're particularly proud that this enhancement was contributed upstream to the Git project, benefiting not just GitLab users but the broader Git community. This collaborative approach to development ensures that improvements are thoroughly reviewed, widely tested, and available to all.</p>
<blockquote>
<p>Deep infrastructure work like this is how we approach performance at GitLab. Join the GitLab 18 virtual launch event to see what other fundamental improvements we're shipping. <a href="https://about.gitlab.com/eighteen/">Register today!</a></p>
</blockquote>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[4-7-8 Breathing (241 pts)]]></title>
            <link>https://www.breathbelly.com/exercises/4-7-8-breathing</link>
            <guid>44201901</guid>
            <pubDate>Fri, 06 Jun 2025 15:36:00 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.breathbelly.com/exercises/4-7-8-breathing">https://www.breathbelly.com/exercises/4-7-8-breathing</a>, See on <a href="https://news.ycombinator.com/item?id=44201901">Hacker News</a></p>
&lt;Unparsable&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[Meta: Shut Down Your Invasive AI Discover Feed. Now (495 pts)]]></title>
            <link>https://www.mozillafoundation.org/en/campaigns/meta-shut-down-your-invasive-ai-discover-feed-now/</link>
            <guid>44201872</guid>
            <pubDate>Fri, 06 Jun 2025 15:33:10 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.mozillafoundation.org/en/campaigns/meta-shut-down-your-invasive-ai-discover-feed-now/">https://www.mozillafoundation.org/en/campaigns/meta-shut-down-your-invasive-ai-discover-feed-now/</a>, See on <a href="https://news.ycombinator.com/item?id=44201872">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
            
    <p data-block-key="ox7g0"><b>Meta is quietly turning private AI chats into public content — and too many people don’t realize it’s happening.</b></p><p data-block-key="a6hs9">That’s why the Mozilla community is demanding that Meta:</p><ol><li data-block-key="f4j6n"><b>Shut down the Discover feed</b> until real privacy protections are in place.</li><li data-block-key="3q7uu"><b>Make all AI interactions private by default</b> with no public sharing option unless explicitly enabled through informed consent.</li><li data-block-key="ddf8c"><b>Provide full transparency</b> about how many users have unknowingly shared private information.</li><li data-block-key="cfiao"><b>Create a universal, easy-to-use opt-out system</b> for all Meta platforms that prevents user data from being used for AI training.</li><li data-block-key="8j7da"><b>Notify all users whose conversations may have been made public</b>, and allow them to delete their content permanently.</li></ol><p data-block-key="3i8bo">Meta is blurring the line between private and public — and it’s happening at the cost of our privacy. People have the right to know when they’re speaking in public, especially when they believe they’re speaking in private.</p><p data-block-key="9cait">If you agree, add your name to demand Meta shut down its invasive AI feed — and guarantee that no private conversations are made public without clear, explicit, and informed opt-in consent.</p>

        </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[VPN providers in France ordered to block pirate sports IPTV (108 pts)]]></title>
            <link>https://torrentfreak.com/major-vpn-providers-ordered-to-block-pirate-sports-streaming-sites-250516/</link>
            <guid>44201857</guid>
            <pubDate>Fri, 06 Jun 2025 15:31:18 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://torrentfreak.com/major-vpn-providers-ordered-to-block-pirate-sports-streaming-sites-250516/">https://torrentfreak.com/major-vpn-providers-ordered-to-block-pirate-sports-streaming-sites-250516/</a>, See on <a href="https://news.ycombinator.com/item?id=44201857">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
        <p><picture loading="lazy" decoding="async">
<source type="image/webp" srcset="https://torrentfreak.com/images/vpn-divertx1.png.webp">
<img loading="lazy" decoding="async" src="https://torrentfreak.com/images/vpn-divertx1.png" alt="vpn-divertx1" width="250" height="238">
</picture>
Article L. 333-10 of the French Sport Code enables rightsholders to request blocking measures against named pirate sites if they can demonstrate “serious and repeated infringement” of their exploitation rights. </p>
<p>To prevent pirate sites from being accessed on French soil, rightsholders may request that “all proportionate measures” are implemented by <em>any</em> online entity in a position to help. The scope of <a href="https://www.legifrance.gouv.fr/codes/article_lc/LEGIARTI000044247629/">Article L. 333-10</a> was always meant to be broad. </p>
<p>The first logical targets were local ISPs which easily fell within scope. Then, when inevitable circumvention raised its head, utilizing infrastructure beyond the reach of regular ISPs, Article L. 333-10 already had that covered. </p>
<p>Following a Canal+ complaint, use of third party DNS resolvers at Cloudflare and Google <a href="https://torrentfreak.com/google-cloudflare-cisco-will-poison-dns-to-stop-piracy-block-circumvention-240613/">headed to court in 2024</a>. Both were labeled intermediaries by the Court, and under the still unstressed scope of Article L. 333-10, both were considered capable of contributing to the suppression of piracy.</p>
<h2>Canal+ / LFP Target Major VPN Providers</h2>
<p>Having effectively added public DNS resolvers to the French blocking machine, attention turned to the next targets. In February 2025, it emerged that Groupe Canal + and Société d’édition de Canal Plus (SECP) had <a href="https://torrentfreak.com/rightsholders-target-vpn-providers-in-french-court-to-block-piracy-250207/">filed a case</a> in November 2024 against NordVPN, CyberGhost, ProtonVPN, ExpressVPN, and Surfshark.</p>
<p>The Canal companies alleged that “numerous” websites, accessible from within France, illegally streamed matches from various sporting competitions to which they hold the rights. </p>
<p>Since the VPNs’ subscribers were among those viewing the infringing streams, the Canal companies asked the court to compel the providers to implement “all measures likely to prevent access [to the illegal streams] from French territory,” including in all French overseas territories, “by any effective means”.</p>
<p>The VPN providers objected to the application on various grounds. Nord and Surfshark requested a declaration that the Canal companies lacked standing to act; as such, their application should be declared inadmissible. Proton sought a similar declaration while noting that the company lacked the ability to defend the requested blocking measures. The same assertion was made by CyberGhost and Express; all argued that Article L. 333-10 does not apply to VPN providers.</p>
<p>Other objections concerned jurisdiction and whether French law is compatible with EU legislation. CyberGhost and Express suggested a stay of proceedings pending a response from the Court of Justice of the European Union (CJEU) in the Dutch case, <a href="https://uitspraken.rechtspraak.nl/details?id=ECLI:NL:HR:2024:1603">AFS et al</a>.</p>
<h2>Court Rejects VPN Providers’ Objections</h2>
<p>In its decision published Thursday, the Court found that matters concerning the CJEU would have no impact on the current case. Calls for the Canal companies’ application to be declared inadmissible due to a lack of standing, were also dismissed, while a review of the allocation of rights pertaining to the various sports competitions raised no concerns.</p>
<p>Arguments that the VPN providers had no standing to defend themselves, due to Article L. 333-10 of the French Sports Code being inapplicable to VPN providers, fared no better.</p>
<p>The court notes that Article L.333-10 does not impose any restrictions on the targeted entity, adding that VPN providers are expressly covered under the Digital Services Act.</p>
<p>“Blocking such a service for certain domain names means that the provider of this service prevents its users from accessing the disputed domain names when using their VPN tool. Internet users using these virtual private networks would therefore no longer be able to access the disputed sites through this intermediary,” the decision reads.</p>
<p>“Consequently, the defendant companies, in their capacity as providers of virtual private network services, are technical intermediaries capable of contributing to remedying the harm that Groupe Canal+, SECP and Canal+ Rights claim to have suffered.”</p>
<h2>Blocking Order Issued</h2>
<p>The Court’s instructions and the full list of domains can be found below.</p>
<p><em>Update: the French football league LFP obtained a similar order on the same day, we have added a statement at the bottom of this article.</em></p>
<blockquote><p><small><em>[The Court] orders the companies Cyberghost LLC, Cyberghost Srl, Expressco services, Express technologies, Nordvpn (Netherlands), Nordvpn (Republic of Panama), Surfshark Bv, Surfshark Ltd and Proton to implement, at the latest within three days following notification of this decision, all measures necessary to prevent, until the date of the last match of the championship of the Premier League for the 2024/2025 season, currently set for May 25, 2025, access to the websites and IPTV services identified [below] as well as to the IPTV sites and services not yet identified on the date of this decision, from French territory, including in the overseas communities, departments and regions, and/or by their users based on a contract taken out in this territory, by any effective means, and in particular by blocking the following domain names and associated subdomains…..</em></small></p></blockquote>
<p>The cost of blocking will be shared between the parties, with the details to be agreed at a later date. A request by the plaintiffs to compel the VPN providers to publish details of the case on their homepages for publicity purposes, was described as “inappropriate” and rejected by the Court.</p>
<p>The decision reveals that many of the domain names submitted by Canal for blocking, are already subject to blocking measures by French ISPs following notification by telecoms regulator ARCOM. Familiar brands include Footy Bite, Cric HD, Buffstreams, Futbollibre, Rojadirecta, and Crackstreams, among dozens of others. In these cases widespread piracy has already been established, but it appears that in-depth proof of infringement may not be a hard requirement.</p>
<p>“Since the burden of proof should not be unnecessarily complex and costly, the court cannot require the claimants to demonstrate access to the disputed IPTV sites and services by using each of the defendants’ virtual private networks, just as it does not request findings by using each of the internet service providers’ networks when a blocking is requested of them on the basis of Article L. 333-10 of the Sports Code,” the court notes.</p>
<p>Had comprehensive checks been carried out, questions may have been raised over the need to block pirate domains previously seized by the Alliance for Creativity and Entertainment. </p>
<center><picture loading="lazy" decoding="async">
<source type="image/webp" srcset="https://torrentfreak.com/images/ace-sus.png.webp 1237w, https://torrentfreak.com/images/ace-sus-300x149.png.webp 300w" sizes="auto, (max-width: 670px) 100vw, 670px">
<img loading="lazy" decoding="async" src="https://torrentfreak.com/images/ace-sus.png" alt="ace-sus" width="670" height="333" srcset="https://torrentfreak.com/images/ace-sus.png 1237w, https://torrentfreak.com/images/ace-sus-300x149.png 300w, https://torrentfreak.com/images/ace-sus-600x298.png 600w, https://torrentfreak.com/images/ace-sus-150x74.png 150w" sizes="auto, (max-width: 670px) 100vw, 670px">
</picture>
</center>
<p>Other domains from the Canal+ list below were seized by ACE during the last few days, so blocking those domains will be unnecessary too. </p>
<p><em>The decision of the Court of Paris is available <a href="https://torrentfreak.com/images/Canal-Plus-v-VPN-Providers-judgment-250515-FR.pdf">here</a> (pdf, French)</em></p>
<p><em>Update: French football league LFP obtained a similar order on the same day and released the following <a href="https://www.lfp.fr/article/la-lfp-et-lfp-media-obtiennent-des-vp-ns-de-bloquer-les-sites-pirates">statement</a>.</em></p><p><em>“The LFP and LFP Media welcome this decision by the Paris Judicial Court, which is a first in France, if not in the world, and constitutes a major step forward in the fight against piracy by ordering VPN services to implement measures to block pirate sites.”</em></p>
<p><em>aliezstream.pro<br>
antenasport.shop<br>
antenasports.ru<br>
antenasports.shop<br>
antenatv.online<br>
antenatv.store<br>
antennasport.ru<br>
asportv.shop<br>
livetv802.me<br>
toparena.store<br>
emb.ap1357.me<br>
embx224539.ap1357.me<br>
1.qwebplay.xyz<br>
livetv807.me<br>
cdn.livetv807.me<br>
boxtv60.com<br>
infinity-ott.com<br>
vbnl23.com<br>
footy-bite.com<br>
freesportstime.com<br>
livestreamlinks.cc<br>
crichdplayer.com<br>
crichd.sc<br>
crichd.mobi<br>
crichd.sx<br>
crichd.tv<br>
me.crichd.tv<br>
nbabite.to<br>
reddit.nbabite.to<br>
s2watch.link<br>
soccerinhd.com<br>
sportlemons.to<br>
sports-prime.com<br>
topevents.us<br>
buffsports.me<br>
buffstreams.sx<br>
vipbox.sx<br>
vipbox.bz<br>
freestreams-live.top<br>
alkooratv.onlinekora-tv.com<br>
onlinekora-tv.com<br>
futbollibre.ws<br>
kooracity.cc<br>
ok.tvkora-online.com<br>
tvkora-online.com<br>
pirlotv.football<br>
ramsportl .com<br>
roja.football<br>
rojadirectaenvivo.life<br>
rojadirectaenvivo.site<br>
rojadirectatv.at<br>
rojadirectatv.la<br>
rojadirectaz.top<br>
rojadiretta.me<br>
soccerlive.app<br>
thesport.live<br>
futbollibre.ws<br>
hesgoal.watch<br>
hesgoall .net<br>
librefutbol.su<br>
myp2p.tv<br>
myp2ptv.org<br>
myp2px.xyz<br>
partidosenvivo.gratis<br>
pirlotvenhd.org<br>
rojadirecta.org.pl<br>
rojadirectaenhd.live<br>
rojadirectaenhd.net<br>
rojadirectaenvivo.com<br>
rojadirectaenvivo.watch<br>
rojadirecta.tv<br>
rojadirectas.org<br>
rojadirectatv.at<br>
rojadirectatv.top<br>
rojadirectatvhd.site<br>
rojadirectatvs.com<br>
soccerstreams.cc<br>
soccerstreams.unblockedstream.online<br>
streameast.soccer<br>
tarjetarojaonline.org<br>
tarjetarojatv.run<br>
totalsportek.one<br>
totalsportk.org<br>
viperplay.net<br>
viperplay.online<br>
viperplayhd.com<br>
vipleague.app<br>
vipleagues.org<br>
vip-league.net<br>
livetv806.me<br>
rojadirectahdenvivo.com<br>
streamsthunder.tv<br>
rojadirectenvivo.me<br>
methstreams.me<br>
antenasports.ru<br>
asportv.shop<br>
toparena.store<br>
Ishunter.net<br>
tv1337.buzz<br>
livetv.sx<br>
sporttuna.pro<br>
livetv807.me<br>
embx224539.ap1366.me<br>
cdn.livetv807.me<br>
locatedinfain.com<br>
tvhd.tutvlive.info<br>
stream-24.net<br>
speci41eagle.com<br>
vl.methstreams.me<br>
klubsports.fun<br>
weblivehdplay.ru<br>
buddycenters.shop<br>
olalivehdplay.ru<br>
1 qwebplay.xyz<br>
sporttvls.com<br>
eur02024direct.ru<br>
librarywhispering.com<br>
cdn.livetv808.me<br>
watch.sporttuna.pro<br>
sporttuna.sx<br>
sporttuna.online<br>
lewblivehdplay.ru<br>
viwlivehdplay.ru<br>
r365.city<br>
fmytv.com<br>
yalla-shootv.live<br>
sportlemo.net<br>
sportlemon.be<br>
antenasport.site<br>
sportlemons.tv<br>
directatvhd.me<br>
sportlemon.info<br>
koora365.io<br>
sport365.live<br>
live.esportivos.one<br>
sportlemonx.com<br>
mip2p.top<br>
bein-live.tv<br>
yalla-shoot.fun<br>
sportlemone.top<br>
sportlemont.org<br>
thedaddy.to<br>
abbasport.site<br>
h5.365streams.world<br>
stad.yallashoot.vip<br>
crickfree.org<br>
drakulatv.eu<br>
sportlemon.net<br>
footdirect.ru<br>
hdmatch.link<br>
livehd7i.live<br>
noblockaabbdd-xcktb.xyz<br>
sportp2p.com<br>
crichd.info<br>
crichd-player.top<br>
crichd. to<br>
telerium.lol<br>
rojatv.tv<br>
mundialqatar2022tv.tv<br>
hesgoal.website<br>
redditsoccerstreams.one<br>
redditsoccerstreams.watch<br>
soccerbite.net<br>
hesgoal.one<br>
cricfree.be<br>
cricfree.me<br>
cricfree.pw<br>
crickfree.net<br>
cricfrees.com<br>
crackstreams.dev<br>
ronald07.me<br>
draculastream.org<br>
drakulastream.tv<br>
drakulastream.org<br>
drakulastream.xyz<br>
drakula.top<br>
drakula.stream<br>
elitegoltv.run<br>
firstrowl.xyz<br>
thesportsl.org<br>
livetv813.me<br>
sportp2p.com<br>
directatvhd.me<br>
Ishunter.net<br>
antenasport.shop<br>
antenasports.ru<br>
antenasports.shop<br>
ilovetoplay.xyz<br>
hoca2.com<br>
livetv814.me<br>
cdn.livetv814.me<br>
streamingon.org<br>
emb.ap1357.me<br>
livetv815.me<br>
cdn.livetv815.me<br>
noblockaabbdd-xcktb.xyz<br>
embx222304.ap1357.me<br>
tutvlive.info<br>
sporttvls.com<br>
quest4play.xyz<br>
antenasport.online<br>
wfzrbhp.luxevpn.xyz<br>
smart.lionsmart.cc</em></p>

      </div></div>]]></description>
        </item>
    </channel>
</rss>