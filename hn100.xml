<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0">
    <channel>
        <title>HN100 - Readable Contents</title>
        <link>https://hn.algolia.com/api/v1/search_by_date?tags=%28story,poll%29&amp;numericFilters=points%3E100</link>
        <description>Uses Readability to add bodies to the RSS feed</description>
        <lastBuildDate>Sun, 10 Mar 2024 11:00:05 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>en</language>
        <item>
            <title><![CDATA[S3 is files, but not a filesystem (134 pts)]]></title>
            <link>https://calpaterson.com/s3.html</link>
            <guid>39656657</guid>
            <pubDate>Sun, 10 Mar 2024 04:11:26 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://calpaterson.com/s3.html">https://calpaterson.com/s3.html</a>, See on <a href="https://news.ycombinator.com/item?id=39656657">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
        <article>
            
            <p><time datetime="2024-03-05T00:00:00Z">March 2024</time></p>
            <p id="article-description">"Deep" modules, mismatched
            interfaces - and why SAP is so painful</p>
            <figure>
                <picture><source srcset="https://calpaterson.com/images/photo/cal-misc.avif" type="image/avif"> <source srcset="https://calpaterson.com/images/photo/cal-misc.webp" type="image/webp"> <img src="https://calpaterson.com/images/photo/cal-misc.jpeg" alt="a box labelled: CAL'S MISC" height="450" width="800"></picture>
                <figcaption>
                    My very own "object store"
                </figcaption>
            </figure>
            <p>Amazon S3 is the original cloud technology: it came out in 2006.
            "Objects" were popular at the time and S3 was labelled an "object store",
            but everyone really knows that S3 is for files. S3 is a cloud filesystem,
            not an object-whatever.</p>
            <p>I think idea that S3 is <em>really</em> "Amazon Cloud Filesystem" is a
            bit of a load bearing fiction. It's sort of true: S3 can store files. It's
            also a very useful belief in getting people to adopt S3, a fundamentally
            good technology, which otherwise they might not. But it's false: S3 is not
            a filesystem and can't stand in for one.</p>
            <h2>What filesystems are about, and module "depth"</h2>
            <p>The unix file API is pretty straightforward. There are just five basic
            functions. They don't take many arguments.</p>
            <p>Here are (the Python versions of) these five basic functions:</p>
            <div>
                <pre><span></span><span># open a file</span>
<span>open</span><span>(</span><span>filepath</span><span>)</span> <span># returns a `file`</span>

<span># read from that file (moving the position forward)</span>
<span>file</span><span>.</span><span>read</span><span>(</span><span>size</span><span>=</span><span>100</span><span>)</span> <span># returns 100 bytes</span>

<span># write to that file (moving the position forward)</span>
<span>file</span><span>.</span><span>write</span><span>(</span><span>"hello, world"</span><span>)</span>

<span># move the position to byte 94</span>
<span>file</span><span>.</span><span>seek</span><span>(</span><span>94</span><span>)</span>

<span># close the file</span>
<span>file</span><span>.</span><span>close</span><span>()</span>
</pre>
            </div>
            <p>Well, perhaps I should add an asterisk: I am simplifying a bit. There
            are loads more calls than that. But still, those five calls are the
            irreducible nub of the file API. They're all you need to read and write
            files.</p>
            <p>Those five functions handle a lot of concerns:</p>
            <ul>
                <li>buffering</li>
                <li>the page cache</li>
                <li>fragmentation</li>
                <li>permissions</li>
                <li>IO scheduling</li>
                <li>and whatever else</li>
            </ul>
            <p>Even though the file API <em>handles</em> all those concerns, but it
            doesn't <em>expose</em> them to you. A narrow interface handling a large
            number of concerns - that makes the unix file API a "deep" module.</p>
            <figure>
                <img src="https://calpaterson.com/assets/wide-vs-narrow.svg" alt="diagram of a deep vs a shallow diagram">
            </figure>
            <p>Deep modules are great because you can benefit from their features -
            like wear-levelling on SD cards - but without bearing the psychic toll of
            thinking about any of it as you save a jpeg to your phone. Happy days.</p>
            <p>But if the file API is "deep", what sorts of things are "shallow"?</p>
            <p>A shallow module would have a relatively large API surface in proportion
            to what it's handling for you. One hint these days that a module is shallow
            is that the interface to it is YAML. YAML <em>appears</em> to be a mere
            markup language but in practice is a reuseable syntax onto which almost any
            semantics can be plonked.</p>
            <p>Often YAML works as the "Programming language of DevOps" and programming
            languages provide about the widest interface possible. Examine your YAML
            micro-language closely. Does it offer a looping construct? If so, it's
            likely Turing complete.</p>
            <p>But sometimes it is hard to package something up nicely with a bow on
            top. SQL ORMs are inherently a leaky abstraction. You can't use them
            without some understanding of SQL. So being shallow isn't inherently a
            criticism. Sometimes a shallow module is the best that can be done. But all
            else equal, deeper is better.</p>
            <h2>What S3 is about (it is deep too)</h2>
            <p>The unix file API was in place by the early 1970s. The interface has
            been retained and the guts have been re-implemented many times for
            compatibility reasons.</p>
            <p>But Amazon S3 <em>does not</em> reimplement the unix filesystem API.</p>
            <p>It has a wholly different arrangement and the primitives are only partly
            compatible. Here's a brief description of the calls that are analogous to
            the above five basic unix calls:</p>
            <div>
                <pre><span></span><span># Read (part) of an object</span>
<span>GetObject</span><span>(</span><span>Bucket</span><span>,</span> <span>Key</span><span>,</span> <span>Range</span><span>=</span><span>None</span><span>)</span> <span># contents is the HTTP body</span>

<span># Write an (entire) object</span>
<span>PutObject</span><span>(</span><span>Bucket</span><span>,</span> <span>Key</span><span>)</span> <span># send contents as HTTP body</span>

<span># er, that's it!</span>
</pre>
            </div>
            <p>Two functions versus five. That's right, the S3 API is simpler than the
            unix file API. There is one additional concept ("buckets") but I think when
            you net it out, S3's interface-to-functionality ratio is even better than
            the unix file API.</p>
            <p>But something is missing. While you can partially read an object using
            the <code>Range</code> argument to <code>GetObject</code>, <strong>you
            can't overwrite partially</strong>. Overwrites have to be the whole
            file.</p>
            <p>That sounds minor but actually scopes S3 to a subset of the old usecases
            for files.</p>
            <h2>Filesystem software, especially databases, can't be ported to Amazon
            S3</h2>
            <p>Databases of all kinds need a place to put their data. Generally, that
            place has ended up being various files on the filesystem. Postgres
            maintains two or three files per table, plus loads of others for
            bookkeeping. SQLite famously stores everything in a single file. MySQL,
            MongoDB, Elasticsearch - whatever - they all store data in files.</p>
            <p>Crucially, these databases overwhelmingly <em>rely</em> on the ability
            to do partial overwrites. They store data in <a href="https://calpaterson.com/how-a-sql-database-works.html">"pages"</a> (eg 4 or 8 kilobytes long) in
            "heap" files where writes are done page by page. There might be thousands
            of pages in a single file. Pages are overwritten as necessary to store
            whatever data is required. That means partial overwrites are absolutely
            essential.</p>
            <figure>
                <img src="https://calpaterson.com/assets/heap.svg" alt="diagram of a database heap file">
                <figcaption>
                    A heap file is full of pages (and empty slots). Pages are
                    overwritten individually as necessary.
                </figcaption>
            </figure>
            <p>Some software projects start with a dream of storing their data in a
            'simple' way by combining two well tested technologies: Amazon S3 and
            SQLite (or DuckDB). Afterall, what could be simpler and more
            straightforward? Sadly, they go together like oil and water.</p>
            <p>When your SQLite database is kept in S3, each write suddenly becomes a
            total overwrite of the entire database. While S3 can do big writes fast,
            even it isn't fast enough to make that strategy work for any but the
            smallest datasets. And you're jettisoning all the transactional integrity
            that the database authors have painstakingly implemented: rewriting the
            database file each time throws out all that stuff. On S3, the last write
            wins.</p>
            <h2>What S3 is good at and what it is bad at</h2>
            <p>The joy of S3 is that bandwidth ("speed") for reads and writes is
            extremely, extremely high. It's not hard to find examples online of people
            who have written to or read from S3 at over 10 gigabytes per second. In
            fact I once saturated a financial client's office network with a set of S3
            writes.</p>
            <p>But the lack of partial overwrites isn't the only problem. There are a
            few more.</p>
            <p>S3 has no rename or move operation. Renaming is <code>CopyObject</code>
            and then <code>DeleteObject</code>. <code>CopyObject</code> takes linear
            time to the size of the file(s). This comes up fairly often when someone
            has written a lot of files to the wrong place - moving the files back is
            very slow.</p>
            <p>And listing files is slow. While the joy of Amazon S3 is that you can
            read and write at extremely, extremely, high bandwidths, listing out what
            is there is much much slower. Slower than a slow local filesystem.</p>
            <p>But S3 is much lower maintenance than a filesystem. You just name the
            bucket, name the key and the cloud elves will sort out everything else.
            This is worth a lot as setting backups, replicating offsite, provisioning
            (which, remember is for IO ops as well as capacity) is pure drudgework.</p>
            <h2>Module depth is even more important across organisations</h2>
            <p>In retrospect it is not a surprise the S3 was the first popular cloud
            API. If deep APIs are helpful in containing the complexity between
            different modules with a single system (like your computer) they are even
            more helpful in containing the complexity of an interaction between two
            different businesses, where the costs of interacting are so much
            higher.</p>
            <p>Consider a converse example. Traditionally when one business wants to
            get it's computers working with those of another they call it
            "integration". It is a byword for suffering. Imagine you are tasked with
            integrating some Big Entreprise software horror into your organisation.
            Something like SAP. Is SAP a deep module? No. The tragedy of SAP is that
            almost your entire organisation has to understand it. Then you have to
            reconcile it with everything you're doing. At all times. SAP integration
            projects are consequently expensive, massive <a href="https://www.zdnet.com/article/millercoors-sues-hcl-tech-for-100-million-over-failure-to-implement-erp-project/">
            and</a> <a href="https://www.henricodolfing.com/2020/01/project-failure-case-study-leaseplan-sap.html">
            regularly</a> <a href="https://www.henricodolfing.com/2020/05/case-study-lidl-sap-debacle.html">fail</a>.</p>
            <p>There isn't much less complexity in S3 than there is in a SAP
            installation. Amazon named it the "Simple Storage Service" but the amount
            of complexity in S3 is <a href="https://brooker.co.za/blog/2023/03/23/economics.html">pretty
            frightening</a>. Queueing theory, IO contention, sharding, the list of
            problems just goes on and on - in addition too all the stuff I listed above
            that filesystems deal with. (And can you believe they do it all
            on-prem?)</p>
            <p>The "simple" in S3 is a misnomer. S3 is not actually simple. It's
            deep.</p>
            <hr>
            <h2>Contact/etc</h2>
            
            <hr>
            <h2>Other notes</h2>
            <p>I don't mean to suggest in any way via this article that S3 is not
            <a href="https://calpaterson.com/amazon-premium.html">overpriced for what it is</a>. To rephrase a
            famous joke about hedge funds, it often seems like The Cloud is a revenue
            model masquerading as a service model.</p>
            <p>The concept of deep vs shallow modules comes from John Ousterhout's
            <a href="https://www.amazon.co.uk/Philosophy-Software-Design-2nd/dp/173210221X">excellent
            book</a>. The book is effectly a list of ideas on software design. Some are
            real hits with me, others not, but well worth reading overall. Praise for
            making it succinct.</p>
            <p>A few databases are explicitly designed from the start to us the S3 API
            for storage. <a href="https://event.cwi.nl/lsde/papers/p215-dageville-snowflake.pdf">Snowflake
            was.</a> So it's possible - but not transparently. But snowflake is one of
            the few I'm aware of (and they made this decision very early, at least by
            2016). If you know of others - let me know by email.</p>
            <p>It isn't just databases that struggle on S3. Many file formats assume
            that you'll be able to seek around cheaply and are less performant on S3
            than on disk. Zipfiles are a key example.</p>
            <h3>Other stuff about S3 that is a matter for regret</h3>
            <p>I genuinely like S3 so did not want to create the wrong impression by
            including a laundry list of complaints in the middle of the post but anyway
            here are the other major problems I didn't mention above:</p>
            <ol>
                <li>
                    <p>The S3 API is <em>only</em> available as XML. JSON was around in
                    2006 but XML was still dominant and so it's probably not a surprise
                    that Amazon picked XML originally. It is a surprise that Amazon
                    never released a JSON version though - particularly when they made
                    the switch from SOAP to REST, which would have been a good
                    time.</p>
                </li>
                <li>
                    <p>It's also a matter for regret that Amazon <a href="http://doc.s3.amazonaws.com/2006-03-01/AmazonS3.xsd">gave up</a>
                    on maintaining the XSD schema as this is one of the key benefits of
                    XML for APIs. The canonical documentation is just a website
                    now.</p>
                </li>
                <li>
                    <p>Criminally, Amazon - like many cloud service providers - have
                    never produced any kind of local test environment. In Python, the
                    more diligent test with the <a href="https://github.com/getmoto/moto">moto</a> library. moto is
                    maintained by volunteers which is weird given that it's a testing
                    tool for a commercial offering.</p>
                </li>
                <li>
                    <p>Amazon S3 <em>does</em> support checksums. For whatever reason
                    they are not turned on by default. Amazon makes many claims about
                    durability. I haven't heard of people having problems but equally:
                    I've never seen these claims tested. I am at least a bit curious
                    about these claims.</p>
                </li>
                <li>
                    <p>For years Amazon S3 held one other trap for the unwary: eventual
                    consistency. If you read a file, then overwrote it, you might read
                    it back and find it hadn't changed yet. Particularly because it
                    only happened sometimes, for short periods of time, this caused all
                    manner of chaos. Other implementors of S3 didn't copy this property
                    and a few years ago <a href="https://aws.amazon.com/blogs/aws/amazon-s3-update-strong-read-after-write-consistency/">
                    Amazon fixed it in their implementation</a>.</p>
                </li>
            </ol>
        </article>
    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[I'm Betting on Call-by-Push-Value (137 pts)]]></title>
            <link>https://thunderseethe.dev/posts/bet-on-cbpv/</link>
            <guid>39653895</guid>
            <pubDate>Sat, 09 Mar 2024 18:56:56 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://thunderseethe.dev/posts/bet-on-cbpv/">https://thunderseethe.dev/posts/bet-on-cbpv/</a>, See on <a href="https://news.ycombinator.com/item?id=39653895">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>You come upon a function argument at a fork in the road.
If it takes the left road, it’ll evaluate itself and then be passed to its function.
If it takes the right road, it’ll pass itself to the function to be evaluated somewhere down the road (🥁🐍).
Let’s bet on which road will be faster.</p><p>We might suspect this is a rather boring bet.
All we have to do is look down each road and see which one is shorter.
Fortunately for our wager (and to the dismay of theorists everywhere), this is not the case.
We can always construct a situation where evaluating either eagerly or lazily is better.</p><p>Our gamble touches on an age-old question in programming language semantics, to <a href="https://en.wikipedia.org/wiki/Evaluation_strategy#Call_by_value" target="_blank" rel="noopener" data-goatcounter-click="https://en.wikipedia.org/wiki/Evaluation_strategy#Call_by_value" data-goatcounter-title="call-by-value" data-goatcounter-referrer="I'm betting on Call-by-Push-Value">call-by-value</a>
(CBV) or to <a href="https://en.wikipedia.org/wiki/Evaluation_strategy#Call_by_need" target="_blank" rel="noopener" data-goatcounter-click="https://en.wikipedia.org/wiki/Evaluation_strategy#Call_by_need" data-goatcounter-title="call-by-name/call-by-need" data-goatcounter-referrer="I'm betting on Call-by-Push-Value">call-by-name/call-by-need</a>
(CBN).
They are both evaluation strategies that determine the order in which expressions are evaluated.
CBV always evaluates a function argument before passing it to a function (aka eager evaluation).
CBN waits to evaluate function arguments until they are used in the function body (aka lazy evaluation).</p><p>Languages pick one and use it for all their function applications.
Rust, Java, JavaScript, Python, and C/C++ use CBV as their evaluation strategy.
Haskell and…uh… Haskell use CBN for their evaluation strategy.
Alas, whichever call you make, the grass is always greener on the other side.
Our CBV languages introduce little spurts of lazy evaluation (closures, iterators, etc.).
Our CBN language(s) introduce eager evaluation; Haskell extended its language to make data types eager by default.</p><h2 id="call-by-push-value">Call By Push Value
<a href="#call-by-push-value">
<span>Link to heading</span></a></h2><p>Given both CBV and CBN languages end up wanting both eager and lazy evaluation, why are we forced to pick one and forgo the other entirely?
It turns out we’re not, we just didn’t know that yet when we designed all those languages. …<em>Whoops</em>.<br>Levy’s ‘99 paper <a href="https://dl.acm.org/doi/10.5555/645894.671755" target="_blank" rel="noopener" data-goatcounter-click="https://dl.acm.org/doi/10.5555/645894.671755" data-goatcounter-title="Call by Push Value: A Subsuming Paradigm" data-goatcounter-referrer="I'm betting on Call-by-Push-Value">Call by Push Value: A Subsuming Paradigm</a>
introduces a third option, Call-by-Push-Value (CBPV), to the CBV/CBN spectrum.
‘99 is basically the Cretaceous era if we’re releasing JavaScript frameworks, but it’s quite recent for releasing research.
CBPV has just started to penetrate the zeitgeist, and it’s by far the most promising approach to calling-by.</p><p>Before we can talk about why CBPV is cool, we have to talk about what it is.
The big idea of CBPV is to support both CBV and CBN with one set of semantics.
It accomplishes this by distinguishing between values and computations.
The paper provides a nice slogan to capture the intuition: “a computation does, a value is”.</p><p>Great, but what does that actually mean?
Let’s look at a traditional lambda calculus, to provide contrast for our CBPV lambda calculus:</p><div><pre tabindex="0"><code data-lang="hs"><span><span><span>data</span> <span>Type</span> 
</span></span><span><span>    <span>|</span> <span>Int</span>
</span></span><span><span>    <span>|</span> <span>Fun</span> <span>Type</span> <span>Type</span>
</span></span><span><span>
</span></span><span><span><span>data</span> <span>Term</span>
</span></span><span><span>    <span>=</span> <span>Var</span> <span>Text</span>
</span></span><span><span>    <span>|</span> <span>Int</span> <span>Int</span>
</span></span><span><span>    <span>|</span> <span>Fun</span> <span>Text</span> <span>Term</span>
</span></span><span><span>    <span>|</span> <span>App</span> <span>Term</span> <span>Term</span>
</span></span></code></pre></div><p>Depending on how we execute our <code>App</code> term, this can be either CBV or CBN (but not both).
If we evaluate our argument and apply it to our function, that’s CBV.
If we apply our argument to our function unevaluated, that’s CBN.</p><p>However, we have to pick one: either CBV or CBN.
This is due to our values being all mixed up with our computations under one term.
CBN wants <code>App</code> to take a computation, but CBV wants <code>App</code> to take a value.
Because the two are indistinguishable we’re forced to pick one.
Our CBPV lambda calculus fixes this by sundering value and computation in two:</p><div><pre tabindex="0"><code data-lang="hs"><span><span><span>-- Type of values</span>
</span></span><span><span><span>data</span> <span>ValType</span> 
</span></span><span><span>    <span>=</span> <span>Int</span>
</span></span><span><span>    <span>|</span> <span>Thunk</span> <span>CompType</span>
</span></span><span><span>
</span></span><span><span><span>-- Type of computations</span>
</span></span><span><span><span>data</span> <span>CompType</span> 
</span></span><span><span>    <span>=</span> <span>Fun</span> <span>ValType</span> <span>CompType</span> <span>-- !!</span>
</span></span><span><span>    <span>|</span> <span>Return</span> <span>ValType</span>
</span></span><span><span>
</span></span><span><span><span>-- A value term</span>
</span></span><span><span><span>data</span> <span>Value</span>
</span></span><span><span>    <span>=</span> <span>Int</span> <span>Int</span>
</span></span><span><span>    <span>|</span> <span>Var</span> <span>Text</span>
</span></span><span><span>    <span>|</span> <span>Thunk</span> <span>Comp</span>
</span></span><span><span>
</span></span><span><span><span>-- A computation term</span>
</span></span><span><span><span>data</span> <span>Comp</span>
</span></span><span><span>    <span>=</span> <span>Fun</span> <span>Text</span> <span>Comp</span>
</span></span><span><span>    <span>|</span> <span>App</span> <span>Comp</span> <span>Value</span>
</span></span><span><span>    <span>|</span> <span>Return</span> <span>Value</span>
</span></span></code></pre></div><p>With that CPBV has cut the Gordian Knot, cementing its place as ruler of all Applications.
And we love that for them, but wow, it took a lot more stuff to do it (we doubled our line count).
It’s now exceedingly clear what’s a value and what’s a computation.
One surprising thing is that variables are a value.
What if our variable is bound to a computation?
CBPV has decreed: “we don’t have to worry about it” (although to be frank I’m a little worried about it).</p><p>If we look at our new <code>App</code> node, it can also only apply a value.
What a relief, that means we can still pass variables to functions.
But CBN has us pass around unevaluated arguments, the whole point is that they’re computations we haven’t evaluated to a value yet.
How are we going to do that if all our variables are values and all our function arguments are values?
The answer lies in a new <code>Value</code> node: <code>Thunk</code>.</p><p>A <code>Thunk</code> turns a computation into a value.
When we want to apply a computation to a function, we first have to turn it into a value using <code>Thunk</code>.
This detail is what makes CPBV so useful.
Being forced to be explicit about packaging our computations into values increases our ability to reason about work.</p><p>We can see another example of this in our new <code>Comp</code> node: <code>Fun</code>.
<code>Fun</code> can only return a <code>Comp</code>.
We can nest <code>Fun</code> nodes (since they are computations) to create multi argument functions.
But what if we want to return a function from a function?</p><p>For that we make use of our final new node <code>Return</code>.
<code>Return</code> is the compliment of <code>Thunk</code>.
It turns a <code>Value</code> into a <code>Computation</code>.
Using <code>Return</code> we can create a function that returns a function like so:</p><div><pre tabindex="0"><code data-lang="hs"><span><span>(<span>Fun</span> <span>"x"</span> (<span>Return</span> (<span>Thunk</span> (<span>Fun</span> <span>"y"</span> (<span>Return</span> (<span>Var</span> <span>"x"</span>))))))
</span></span></code></pre></div><p>This might seem like pageantry, and for a surface language humans write I’d have to agree.
But in a compiler IR, this distinction allows us to generate much more efficient code.</p><h2 id="the-bet">The Bet
<a href="#the-bet">
<span>Link to heading</span></a></h2><p>Now that we know what CBPV <em>is</em>, we can finally talk about why CBPV <em>is…the future</em>.
We know one big advantage is being explicit about where we turn computations into values (and back).
To help put that in perspective, look at this monstrosity from <a href="https://www.cambridge.org/core/journals/journal-of-functional-programming/article/making-a-fast-curry-pushenter-vs-evalapply-for-higherorder-languages/02447DB613E94DC35ACDCB24DB39F085" target="_blank" rel="noopener" data-goatcounter-click="https://www.cambridge.org/core/journals/journal-of-functional-programming/article/making-a-fast-curry-pushenter-vs-evalapply-for-higherorder-languages/02447DB613E94DC35ACDCB24DB39F085" data-goatcounter-title="Making a fast curry" data-goatcounter-referrer="I'm betting on Call-by-Push-Value">Making a fast curry</a>
required to apply arguments to a function at runtime:</p><p><img src="https://thunderseethe.dev/img/stgApplyNP.png" alt="Code snippet showing function application in making of a fast curry"></p><p>Not only do we have to look up the arity of the function, we have to look up whether we’re calling a function or a closure.
Even worse this all has to be done at runtime.
All these headaches go away with CBPV.
If we see a:</p><div><pre tabindex="0"><code data-lang="hs"><span><span>(<span>Fun</span> <span>"x"</span> (<span>Fun</span> <span>"y"</span> (<span>Return</span> (<span>Var</span> <span>"x"</span>))))
</span></span></code></pre></div><p>we know we have to apply two arguments. If instead we see:</p><div><pre tabindex="0"><code data-lang="hs"><span><span>(<span>Fun</span> <span>"x"</span> (<span>Return</span> (<span>Thunk</span> (<span>Fun</span> <span>"y"</span> (<span>Return</span> (<span>Var</span> <span>"x"</span>))))))
</span></span></code></pre></div><p>we can only apply 1 argument, and then we have a value we have to handle before we can do anymore.
It’s not even a valid term to apply two arguments to this term</p><p>Being explicit about values and computations isn’t solely a helpful optimization.
It opens the door to do new things we couldn’t before.
This is what actually led me to write this article.
I kept seeing otherwise unrelated papers employ CBPV to make their work possible.
Let’s look at those papers to see the different things CPBV can do:</p><ul><li><a href="https://dl.acm.org/doi/10.5555/645894.671755" target="_blank" rel="noopener" data-goatcounter-click="https://dl.acm.org/doi/10.5555/645894.671755" data-goatcounter-title="Algebraic Effects" data-goatcounter-referrer="I'm betting on Call-by-Push-Value">Algebraic Effects</a>
(This one is actually covered in Levy’s paper)</li><li><a href="https://www.cl.cam.ac.uk/~nk480/implicit-polarized-f.pdf" target="_blank" rel="noopener" data-goatcounter-click="https://www.cl.cam.ac.uk/~nk480/implicit-polarized-f.pdf" data-goatcounter-title="Implicit Polarized F: Local Type Inference for Impredicativity" data-goatcounter-referrer="I'm betting on Call-by-Push-Value">Implicit Polarized F: Local Type Inference for Impredicativity</a></li><li><a href="https://www.microsoft.com/en-us/research/uploads/prod/2020/03/kacc.pdf" target="_blank" rel="noopener" data-goatcounter-click="https://www.microsoft.com/en-us/research/uploads/prod/2020/03/kacc.pdf" data-goatcounter-title="Kinds Are Calling Conventions" data-goatcounter-referrer="I'm betting on Call-by-Push-Value">Kinds Are Calling Conventions</a></li></ul><h2 id="algebraic-effects">Algebraic Effects
<a href="#algebraic-effects">
<span>Link to heading</span></a></h2><p>Algebraic effects are concerned with tracking side effects in types.
An issue you encounter immediately upon trying to do this is: where can effects happen?
Functions can do effects sure, that’s easy.
What about records, can they do effects?
Well that seems kind of silly, records are values, so let’s say no.
But what if the record contains a function that does an effect, what then?</p><p>CBPV deftly dispatches these quandaries.
Effects can appear on any computation type, and only on computation types.
Functions return a computation, so they can do effects.
But our records are values, so they can only store other values, not effects.</p><p>If we want to put a function in a record we first have to turn it into a <code>Thunk</code>.
So then our record can’t do effects.
If we want to perform our record’s function’s effects, we first have to turn it back into a computation with <code>Return</code>.
CBPV makes it explicit and clear where (and where not) effects can occur in a program.</p><h2 id="implicit-polarized-system-f">Implicit Polarized (System) F
<a href="#implicit-polarized-system-f">
<span>Link to heading</span></a></h2><p>This one has a daunting name, but it’s really cool.
It’s talking about type inference (a subject <a href="https://thunderseethe.dev/posts/type-inference/">we’re well versed in</a>
).
A timeworn tradeoff for type infer-ers is generic types.
If you allow a generic type to be inferred to be another generic type, your type inference is undecidable.
This puts us in a bind though.
A lot of cool types happen to involve these nested generics (called Rank-2, Rank-N, or Impredicative types), and if we can’t infer them we’re forced to write them down by hand.
Truly, a fate worse than death, so the types go sorely under-utilized.</p><p>This paper makes a dent in that problem by allowing us to infer these types, sometimes.
Sometimes may seem underwhelming, but you have to consider it’s infinitely better than never.
It does this with, you guessed it, CBPV.
As we’ve seen, Call by push value makes it explicit when a function is saturated vs when it returns a closure.</p><p>This turns out to be vital information to have during type inference.
Saturated function calls have all their arguments, and these arguments can provide enough information to infer our function type.
Even when our function type includes nested generics.
That’s quite exciting!
All of a sudden our code requires fewer annotations because we made a smarter choice in language semantics.</p><h2 id="kinds-are-calling-conventions">Kinds Are Calling Conventions
<a href="#kinds-are-calling-conventions">
<span>Link to heading</span></a></h2><p>Kinds Are Calling Conventions is a fascinating paper.
It employs kinds to solve issues that have plagued excessively generic languages since the first beta redux:</p><ul><li>Representation - is my type boxed or unboxed</li><li>Levity - is a generic argument evaluated lazily or eagerly</li><li>Arity - how many arguments does a generic function take before doing real work</li></ul><p>To solve these issues, types are given more sophisticated kinds.
Instead of type <code>Int</code> having kind <code>TYPE</code>, it would have kind <code>TYPE Ptr</code>.
Similarly, we ascribe the type <code>Int -&gt; Int -&gt; Int</code> the kind <code>TYPE Call[Int, Int]</code>.
Denoting that it is a function of arity 2 with its kind.
This is where CBPV enters the story.</p><p>To be able to provide the arity in a type’s kind, we first have to know a function’s arity.
This can be tricky in CBV or CBN languages that freely interchange functions and closures.
Thankfully, CBPV makes it abundantly clear what the arity of any function is, based purely on its type.</p><p>Kinds Are Calling conventions utilizes this to great effect to emit efficient calling code for higher order functions.
The paper also makes use of the fact that CBPV admits both eager and lazy evaluation to track how an argument is evaluated in the kind.
All in service of generating more efficient machine code.
Who could’ve guessed such a theoretical approach would serve such pragmatic goals.</p><p>If I had a nickel for every time CBPV shows up in the wild, I’d have 3 nickels.
That’s not a lot, but it’s weird that it happened 3 times.
Personally, I believe this is because CBPV hits upon a kernel of truth in the universe.
Being explicit about what’s a computation and what’s a value allows us to reason about more properties of our programs.</p><p>Not only does it let us optimize our programs better, but it lets us do new kinds of polymorphism and decide fancier types in finite time.
Given how recent CBPV is in terms of research, I think we’re just seeing start of things you can do with CBPV, and we’ll continue to discover more things moving forward.
I’m doing my part.
You better believe <a href="https://thunderseethe.dev/series/making-a-language/">my language</a>
will be built atop call-by-push-value.</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Bruno: Fast and Git-friendly open-source API client (Postman alternative) (958 pts)]]></title>
            <link>https://www.usebruno.com/</link>
            <guid>39653718</guid>
            <pubDate>Sat, 09 Mar 2024 18:29:56 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.usebruno.com/">https://www.usebruno.com/</a>, See on <a href="https://news.ycombinator.com/item?id=39653718">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>Bruno is a Fast and Git-Friendly Opensource API client, aimed at revolutionizing the status quo represented by Postman, Insomnia and similar tools out there. </p><p>Bruno stores your collections directly in a folder on your filesystem. We use a plain text markup language, Bru, to save information about API requests. </p><p>You can use git or any version control of your choice to collaborate over your API collections. </p><p>Bruno is offline-only. There are no plans to add cloud-sync to Bruno, ever. We value your data privacy and believe it should stay on your device. Read our long-term vision <a href="https://github.com/usebruno/bruno/discussions/269" target="_blank" rel="noreferrer">here</a>.</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Show HN: React Geiger – performance profiling using sound (140 pts)]]></title>
            <link>https://github.com/kristiandupont/react-geiger</link>
            <guid>39653625</guid>
            <pubDate>Sat, 09 Mar 2024 18:15:25 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/kristiandupont/react-geiger">https://github.com/kristiandupont/react-geiger</a>, See on <a href="https://news.ycombinator.com/item?id=39653625">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><p dir="auto"><h2 tabindex="-1" dir="auto">React Geiger</h2><a id="user-content-react-geiger" aria-label="Permalink: React Geiger" href="#react-geiger"></a></p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/kristiandupont/react-geiger/blob/main/logo.png"><img src="https://github.com/kristiandupont/react-geiger/raw/main/logo.png" height="100"></a></p>
<p dir="auto">React Geiger is a tool for "audiolizing" React performance issues. You can have it running in the background and makes little clicks which will point your attention to excessive (slow) component rerenders.</p>
<p dir="auto">Play with it in this <a href="https://playcode.io/1793073" rel="nofollow">playground</a></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Installation</h2><a id="user-content-installation" aria-label="Permalink: Installation" href="#installation"></a></p>

<p dir="auto"><h2 tabindex="-1" dir="auto">Usage</h2><a id="user-content-usage" aria-label="Permalink: Usage" href="#usage"></a></p>
<p dir="auto">You wrap whatever you want to track in the <code>&lt;Geiger&gt;</code> component, and re-renders inside will cause a click if they take longer than the threshold set (default: 50ms).</p>
<p dir="auto">The most basic setup is wrapping your entire app:</p>

<p dir="auto">You can also use it on a sub-tree wherever.</p>
<p dir="auto">The options are:</p>
<div dir="auto" data-snippet-clipboard-copy-content="  profilerId?: string;
  renderTimeThreshold?: number;
  phaseOption?: PhaseOption;
  enabled?: boolean;"><pre>  profilerId?: <span>string</span><span>;</span>
  renderTimeThreshold?: <span>number</span><span>;</span>
  phaseOption?: <span>PhaseOption</span><span>;</span>
  enabled?: <span>boolean</span><span>;</span></pre></div>
<ul dir="auto">
<li><code>profilerId</code> is an id that will be passed on to the <code>React.Profiler</code> component. You probably don't need to change this.</li>
<li><code>renderTimeThreshold</code> is the time in milliseconds that will trigger a click. Default is 50ms. Set to 0 to make any re-render click</li>
<li><code>phaseOption</code> is the phase of the render you want to track, either <code>'mount'</code>, <code>'update'</code> or <code>'both'</code> (which is the default)</li>
<li><code>enabled</code> defaults to true, but you can use this to disable it. Note that it relies on <code>React.Profiler</code> under the hood, which is disabled in production builds per default.</li>
</ul>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/kristiandupont/react-geiger/blob/main/not-great-not-terrible.jpg"><img src="https://github.com/kristiandupont/react-geiger/raw/main/not-great-not-terrible.jpg" alt="Not Great, Not Terrible"></a></p>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[What Monks Know about Focus (188 pts)]]></title>
            <link>https://www.millersbookreview.com/p/jamie-kreiner-how-to-focus</link>
            <guid>39653517</guid>
            <pubDate>Sat, 09 Mar 2024 18:02:06 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.millersbookreview.com/p/jamie-kreiner-how-to-focus">https://www.millersbookreview.com/p/jamie-kreiner-how-to-focus</a>, See on <a href="https://news.ycombinator.com/item?id=39653517">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><article><div dir="auto"><p><span>Books are a waste of time. So says </span><a href="https://open.substack.com/users/6319739-richard-hanania?utm_source=mentions" rel="">Richard Hanania</a><span> in “</span><a href="https://www.richardhanania.com/p/the-case-against-most-books?r=1nizk&amp;utm_campaign=post&amp;utm_medium=web" rel="">The Case Against (Most) Books</a><span>.” He allows that books of historical interest and those by contemporary thought leaders might be valuable, but the vast majority of everything else on the shelf is worthless, especially old books. That’s correct: The classics are garbage.</span></p><p><span>I’ve </span><a href="https://www.millersbookreview.com/p/vital-necessity-of-very-old-books" rel="">addressed Hanania’s argument before</a><span> but return now to note that the best rebuttal are the classics themselves. To that end I offer you </span><em><a href="https://press.princeton.edu/books/hardcover/9780691208084/how-to-focus" rel="">How to Focus: A Monastic Guide for an Age of Distraction</a></em><span>, which serves up several choice selections from John Cassian’s fifth-century monastic guide, the</span><em> Conferences</em><span>, as edited and translated by Jamie Kreiner.</span></p><p><span>The subject of monks struggling to maintain focus represents familiar ground for Kreiner, a professor of history at the University of Georgia. Last year I reviewed her book, </span><em><a href="https://www.millersbookreview.com/p/jamie-kreiner-wandering-mind-distraction?utm_source=%2Fsearch%2FKreiner&amp;utm_medium=reader2" rel="">The Wandering Mind: What Medieval Monks Tell Us about Distraction</a></em><span>.</span></p><p><span>In that book, Kreiner explores the monastic enterprise from late antiquity through the Middle Ages across Europe and the Near East to see how monks managed intense concentration while battling interruptions and distractions. In contrast, with </span><em>How to Focus</em><span> she narrows the scope to just one remarkable text, refreshed for modern readers.</span></p><p><span>Cassian wrote his </span><em>Conferences</em><span> as an older man looking back to a period of youthful experimentation and adventure. </span></p><p>In his twenties, he and his friend Germanus joined a monastery in Bethlehem. The two became fast friends, “inseparable bunkmates,” of such shared intensity and interest “everyone remarked on the equality of our companionship and our sense of purpose. They said that we were one mind and soul in two bodies.”</p><p>The pair wanted to know all the ins and outs of their discipline and decided to travel beyond their local confines to hear from reputed monastic masters. So, for the next decade and a half they traveled the Nile Delta, interviewing the men known as the Desert Fathers, those in monasteries as well as hermits living on their own.</p><p><span>They asked a million questions. The final word count of Cassian’s </span><em>Conferences</em><span> stands at 150,000 words, a remarkably large book for the time. </span><em>How to Focus</em><span>, as Kreiner notes, represents less than 10 percent of that total with her selections geared toward, as the title suggests, the conversations dealing with attention and distraction.</span></p><p>And just what would monks know about that?</p><p>We’re so attuned to our own crises and challenges, we tend to think of them as purely contemporary concerns—especially when we externalize our difficulties and blame our tools, the times, and the like. Desert monastics didn’t have Instagram; ergo, they didn’t have attentional problems.</p><p>Au contraire. While technology has evolved in the last fifteen hundred years, the human brain has not. And few people in the ancient world cared as much about the challenges of attention and distraction as monks. Our reasons might differ today, but we have much to learn nonetheless.</p><p>A repeated complaint from Cassian and Germanus is the difficulty maintaining focus on their prayers. “The mind is always moving and meandering, and it’s torn apart in different directions like it’s drunk,” says Germanus at one point. “It doesn’t even have the power to hold onto or stick with things it finds entertaining!”</p><p>Unfortunately, knowing focus matters fails to engender concentration. “What we know hasn’t helped us attain the steady and stable clarity we’ve been seeking,” he says. “Even when we feel our heart heading straight toward its goals, the mind imperceptibly turns the other way. . . .”</p><p><span>Germanus directed this second comment to Abba Serenus of Scetis, who responded, “The </span><em>nous</em><span> or mind is defined as </span><em>aeikinētos kai polykinētos</em><span>, always and very much on the move.” You might recognize our word </span><em>kinetic</em><span> in the Greek. This bubbling, jumping, flitting mind can only be tamed by training through meditation, memorization, fasting, and other forms of ascetical effort by which “it will become strong enough to drive off the enemy’s stimuli. . . .”</span></p><p>It’s a bit of a relief to realize, no? Why does the mind meander? Because that’s what minds do.</p><p>Monks attempted the radical and difficult practice of pure prayer, to bring their entire mind to bear on the act. Given their intense interest in concentration, and also being prone to endless disruptions in the effort, monks became experts in what we today call metacognition—thinking about thinking.</p><p>“It is impossible for the human mind to empty itself of all thoughts,” says Abba Nestorus, a hermit interviewed by Cassian and Germanus. The question is what kind of thoughts to entertain? Nestorus advises the pair to immerse themselves in sacred reading. “Do it continually—or better, nonstop!—until that constant recitation and reflection saturates your mind and shapes it into a kind of likeness of itself.” </p><p>Nestorus offers three reasons, the third the most profound. First, when a person is engrossed in literature, the mind can stay attuned to its content instead of “toxic thoughts.” Second, an understanding of the text comes not only while reading, but also when we take those thoughts with us into other mental states. We’re able to reflect on what we’ve read when we’ve turned out attentions elsewhere, even when we go to sleep.</p><p><span>Another interviewee, Abba Isaac, also talks about this tricky feature of mental latency, though regarding prayer, not reading. “We should,” he says, “be the sort of person we are in prayer </span><em>before</em><span> it’s time to pray. After all, our state of mind during prayer is unavoidably shaped by the situation prior to the moment.” </span></p><p><span>Attention researcher Gloria Mark—a modern scientist, not a monastic—would affirm Isaac’s point. When we approach any task, as Mark notes in her book, </span><em><a href="https://www.harpercollins.com/products/attention-span-gloria-mark?variant=40346590117922" rel="">Attention Span</a></em><span>, we do so by constructing cognitive frames that marshal the various mental resources required for the activity. When distractions occur, we change frames mid-action. The frustration we feel in getting back on task involves the difficulty in reassembling the cognitive frame we enjoyed prior to the interruption. </span></p><p>Since our mental states persist from one moment to the next, Isaac encourages Cassian and Germanus to hold onto their desired state by preparing for it in advance. It’s a way of ensuring our cognitive frame is strong enough to resist distraction and an approach that applies to a wide variety of intellectual activities.</p><p>I mentioned three reasons from Nestorus for immersive reading but have so far only covered two. What of the third and most mysterious of his reasons?</p><p>Nestorus’s third and most mysterious reason for immersive reading is that sustained engagement deepens our understanding of what we read by the changes wrought in ourselves through the very process of reading. </p><p>“How the scriptures look depends on what the human senses are capable of,” he says. “As our mind is gradually remade through this sustained effort, the shape of the scriptures begins to be remade, too, and it’s as if the beauty born of this more sacred perceptiveness grows as we grow.”</p><p>Our investment in reading changes the book because the book has changed us. And this is where Hanania’s argument fundamentally falls short. If books are merely a means of transferring information, then perhaps, yes, a book is a waste of time. If a summary of its thesis and key points could be presented in a brief article or Substack post, why not just save the hours and read the Substack post? All the more if the information is outdated or questionable for one reason or another.</p><p>But that mistakes what a book is for. A book is a tool. It’s a machine for thinking. And “all machines,” as Thoreau once said, “have their friction.” The time it takes to engage with ideas—whether factual or fictional, emotional or intellectual, accurate or inaccurate, efficient or inefficient—might strike some as a drag. But the time given to working through those ideas, adopting and adapting, developing or discarding, changes our minds, changes us.</p><p>It’s not about the wisdom we glean. It’s about what wisdom we grow.</p><p>What about that more basic plane upon which we engage a book, the information itself? After all, if the book is ancient, is the information even useful? Can the ideas shared in distant philosophical or spiritual contexts translate with any value to our present, secular world? Though the answer depends entirely on the ends to which we put the information, Cassian’s book offers us an answer here as well.</p><p><span>In the first chapter of the </span><em>Conferences</em><span>, Cassian and Germanus visit Abba Moses of Scetis, known to the faithful as St. Moses the Black or St. Moses the Ethiopian, whom they regard as “the sweetest of all those extraordinary flowers” in the desert. They ply Moses with the same sorts of questions they later asked of other fathers: Why is the monastic life so difficult? Curiously, Moses addresses their pleas by talking about short- and long-term goals.</span></p><p><span>“Every acquired skill and every discipline,” says Moses, “has a </span><em>scopos</em><span> and a </span><em>telos</em><span>, some immediate goal and some ultimate goal that is particular to it. Practitioners of any skilled craft will gladly and good-naturedly work through all their fatigue and risks and costs as they keep those goals in mind.”</span></p><p>Moses develops the idea from there and, as someone who professionally spends a lot of time reading modern goal-achievement literature, I can say that his treatment is every bit as useful as the work of scholars working today. Moses’s argument can even help resolve the question of whether we should read the classics.</p><p><span>If you’re Richard Hanania, no. You don’t possess a </span><em>telos</em><span> that would justify the effort. But if you see classics such as John Cassian’s </span><em>Conferences</em><span> as valuable, then most definitely yes. And Jamie Kreiner’s presentation in </span><em><a href="https://press.princeton.edu/books/hardcover/9780691208084/how-to-focus" rel="">How to Focus</a><span> </span></em><span>represents the perfect </span><em>scopos</em><span>. Pick it up and enjoy the thoughts it helps you conjure.</span></p><p>Thanks for reading! If you enjoyed this post, please hit the ❤️ below and share it with your friends.</p><p data-attrs="{&quot;url&quot;:&quot;https://www.millersbookreview.com/p/jamie-kreiner-how-to-focus?utm_source=substack&amp;utm_medium=email&amp;utm_content=share&amp;action=share&quot;,&quot;text&quot;:&quot;Share&quot;,&quot;action&quot;:null,&quot;class&quot;:null}" data-component-name="ButtonCreateButton"><a href="https://www.millersbookreview.com/p/jamie-kreiner-how-to-focus?utm_source=substack&amp;utm_medium=email&amp;utm_content=share&amp;action=share" rel=""><span>Share</span></a></p><p>Not a subscriber? Take a moment and sign up. It’s free for now, and I’ll send you my top-fifteen quotes about books and reading. Thanks again! </p><p>Make sure you also read . . . </p><div data-component-name="DigestPostEmbed"><a href="https://www.millersbookreview.com/p/jamie-kreiner-wandering-mind-distraction" target="_blank" rel="noopener"><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_140,h_140,c_fill,f_webp,q_auto:good,fl_progressive:steep,g_auto/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2dd55a9a-9f9e-4681-9294-e241a526e2c5_3024x3024.jpeg"><img src="https://substackcdn.com/image/fetch/w_140,h_140,c_fill,f_auto,q_auto:good,fl_progressive:steep,g_auto/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2dd55a9a-9f9e-4681-9294-e241a526e2c5_3024x3024.jpeg" sizes="100vw" alt="‘Lead Us Not into Distraction’" width="140" height="140"></picture></div></a></div><div data-component-name="DigestPostEmbed"><a href="https://www.millersbookreview.com/p/dear-abbot-monastic-advice-for-modern" target="_blank" rel="noopener"><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_140,h_140,c_fill,f_webp,q_auto:good,fl_progressive:steep,g_auto/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2Fee116ebe-e8b9-4b88-a2b6-438eb0acee44_2880x2880.jpeg"><img src="https://substackcdn.com/image/fetch/w_140,h_140,c_fill,f_auto,q_auto:good,fl_progressive:steep,g_auto/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2Fee116ebe-e8b9-4b88-a2b6-438eb0acee44_2880x2880.jpeg" sizes="100vw" alt="Dear Abbot: Monastic Advice for Modern Living" width="140" height="140"></picture></div></a></div></div></article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Bypassing Safari 17's advanced audio fingerprinting protection (189 pts)]]></title>
            <link>https://fingerprint.com/blog/bypassing-safari-17-audio-fingerprinting-protection/</link>
            <guid>39653431</guid>
            <pubDate>Sat, 09 Mar 2024 17:54:23 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://fingerprint.com/blog/bypassing-safari-17-audio-fingerprinting-protection/">https://fingerprint.com/blog/bypassing-safari-17-audio-fingerprinting-protection/</a>, See on <a href="https://news.ycombinator.com/item?id=39653431">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>Did you know that browsers can produce audio files you can’t hear, and those audio files can be used to identify web visitors? Apple knows, and the company decided to fight the identification possibility in Safari 17, but their measures don’t fully work.</p>
<h2 id="identifying-with-audio"><a href="#identifying-with-audio" aria-label="identifying with audio permalink"></a>Identifying with audio</h2>
<p>The technique is called audio fingerprinting, and you can learn how it works in our <a href="https://fingerprint.com/blog/audio-fingerprinting/" target="_blank" rel="noopener noreferrer">previous article</a>. In a nutshell, audio fingerprinting uses the browser’s <a href="https://developer.mozilla.org/en-US/docs/Web/API/Web_Audio_API" target="_blank" rel="noopener noreferrer">Audio API</a> to render an audio signal with <a href="https://developer.mozilla.org/en-US/docs/Web/API/OfflineAudioContext" target="_blank" rel="noopener noreferrer">OfflineAudioContext</a> interface, which then transforms into a single number by adding all audio signal samples together. The number is the fingerprint, also called “identifier”.</p>
<p>The audio identifier is stable, meaning it doesn’t change when you clear the cookies or go into incognito mode. This is the key feature of fingerprinting. However, the identifier is not very unique, and many users can have the same identifier.</p>
<p>Audio fingerprinting is a part of <a href="https://github.com/fingerprintjs/fingerprintjs" target="_blank" rel="noopener noreferrer">FingerprintJS</a>, our library with source code available on GitHub.</p>
<p>Fingerprinting is used to identify bad actors when they want to remain anonymous. For example, when they want to sign in to your account or use stolen credit card credentials. Fingerprinting can identify repeat bad actors, allowing you to prevent them from committing fraud. However, many people see it as a privacy violation and therefore don’t like it.</p>
<h2 id="how-safari-17-breaks-audio-fingerprinting"><a href="#how-safari-17-breaks-audio-fingerprinting" aria-label="how safari 17 breaks audio fingerprinting permalink"></a>How Safari 17 breaks audio fingerprinting</h2>
<p>Apple <a href="https://www.apple.com/au/newsroom/2023/06/apple-announces-powerful-new-privacy-and-security-features/" target="_blank" rel="noopener noreferrer">introduced</a> advanced fingerprinting protection in Safari 17. Advanced fingerprinting protection aims to reduce fingerprinting accuracy by limiting available information or adding randomness.</p>
<p>By default, the advanced protection is enabled in private (incognito) mode and disabled in normal mode. It affects both desktop and mobile platforms. Advanced fingerprinting protection also affects <a href="https://developer.mozilla.org/en-US/docs/Web/API/Screen" target="_blank" rel="noopener noreferrer">Screen API</a> and <a href="https://developer.mozilla.org/en-US/docs/Glossary/Canvas" target="_blank" rel="noopener noreferrer">Canvas API</a>, but we’ll focus only on Audio API in this article.</p>
<p>An audio signal produced with the Audio API is an array of numbers representing the signal amplitude at each moment of time (also called “audio samples”). When fingerprinting protection is on, Safari adds a random noise to every sample individually. A noised sample lies between <code>sample*(1-magnitude)</code> and <code>sample*(1+magnitude)</code>, and the distribution is <a href="https://en.wikipedia.org/wiki/Continuous_uniform_distribution" target="_blank" rel="noopener noreferrer">uniform</a>. This is how it’s <a href="https://github.com/WebKit/WebKit/blob/167dc5118a3f6228a19df40e673ea0a6d03b9bec/Source/WebCore/platform/audio/AudioUtilities.cpp#L80" target="_blank" rel="noopener noreferrer">implemented in Safari</a>:</p>
<div data-language="cpp"><pre><code><span>void</span> <span>applyNoise</span><span>(</span><span>float</span><span>*</span> values<span>,</span> size_t numberOfElementsToProcess<span>,</span> <span>float</span> magnitude<span>)</span>
<span>{</span>
    WeakRandom generator<span>;</span>
    <span>for</span> <span>(</span>size_t i <span>=</span> <span>0</span><span>;</span> i <span>&lt;</span> numberOfElementsToProcess<span>;</span> <span>++</span>i<span>)</span>
        values<span>[</span>i<span>]</span> <span>*=</span> <span>1</span> <span>+</span> magnitude <span>*</span> <span>(</span><span>2</span> <span>*</span> generator<span>.</span><span>get</span><span>(</span><span>)</span> <span>-</span> <span>1</span><span>)</span><span>;</span>
<span>}</span></code></pre></div>
<p><em>Note: Safari is being developed actively, so this and the other facts may be outdated when you read the article.</em></p>
<p>All Audio API interfaces that allow reading the audio signal apply noise:</p>
<ul>
<li><a href="https://developer.mozilla.org/en-US/docs/Web/API/AudioWorkletNode" target="_blank" rel="noopener noreferrer">AudioWorkletNode</a> (magnitude 0.001, <a href="https://github.com/WebKit/WebKit/blob/167dc5118a3f6228a19df40e673ea0a6d03b9bec/Source/WebCore/Modules/webaudio/AudioWorkletNode.cpp#L219" target="_blank" rel="noopener noreferrer">source code</a>)</li>
<li><a href="https://developer.mozilla.org/en-US/docs/Web/API/AudioBuffer/getChannelData" target="_blank" rel="noopener noreferrer">AudioBuffer::getChannelData</a> (magnitude 0.001, <a href="https://github.com/WebKit/WebKit/blob/167dc5118a3f6228a19df40e673ea0a6d03b9bec/Source/WebCore/Modules/webaudio/AudioBuffer.cpp#L167" target="_blank" rel="noopener noreferrer">source code</a>)</li>
<li><a href="https://developer.mozilla.org/en-US/docs/Web/API/AudioBuffer/copyFromChannel" target="_blank" rel="noopener noreferrer">AudioBuffer::copyFromChannel</a> (magnitude 0.001, <a href="https://github.com/WebKit/WebKit/blob/167dc5118a3f6228a19df40e673ea0a6d03b9bec/Source/WebCore/Modules/webaudio/AudioBuffer.cpp#L227" target="_blank" rel="noopener noreferrer">source code</a>)</li>
<li><a href="https://developer.mozilla.org/en-US/docs/Web/API/AnalyserNode/getFloatFrequencyData" target="_blank" rel="noopener noreferrer">RealtimeAnalyser::getFloatFrequencyData</a> (magnitude 0.25, <a href="https://github.com/WebKit/WebKit/blob/167dc5118a3f6228a19df40e673ea0a6d03b9bec/Source/WebCore/Modules/webaudio/RealtimeAnalyser.cpp#L181" target="_blank" rel="noopener noreferrer">source code</a>)</li>
</ul>
<p>The noise is different every time it’s applied. As a result, the whole audio fingerprint changes every time it’s calculated in private mode. These changes cause the fingerprint to mismatch in normal and private modes. This breaks the stability; therefore, the fingerprint can’t be used for identification.</p>
<p>The fingerprint fluctuates between <code>124.03516</code> and <code>124.04545</code> in Safari 17 on an M1 MacBook Air. The difference is about 0.008%. That may not sound like much, but further on, we’ll explain why this is a huge difference.</p>
<h2 id="how-we-bypass-safari-17s-advanced-fingerprinting-protection"><a href="#how-we-bypass-safari-17s-advanced-fingerprinting-protection" aria-label="how we bypass safari 17s advanced fingerprinting protection permalink"></a>How we bypass Safari 17’s advanced fingerprinting protection</h2>
<p>The goal is to remove the noise added by Safari. To achieve this, we must improve <a href="https://github.com/fingerprintjs/fingerprintjs/blob/a555410e925e0d6a4d548aa555b6945bd713e9cd/src/sources/audio.ts#L41" target="_blank" rel="noopener noreferrer">our fingerprinting algorithm</a> in 3 steps:</p>
<ol>
<li>Reduce the dispersion of the noise.</li>
<li>Push browser identifier numbers farther apart.</li>
<li>Round the fingerprint to remove the remaining noise.</li>
</ol>
<p>We’ll call this improved algorithm “the new algorithm” throughout the article.</p>
<p>Steps 1 and 2 are necessary because Safari’s range of added noise is much bigger than the difference between fingerprints produced by various browsers. This table shows audio fingerprints produced by some browsers and the percent difference between them and the closest fingerprint from other browsers:</p>
<table>
<thead>
<tr>
<th>Browser</th>
<th>Fingerprint</th>
<th>Difference from the closest browser</th>
</tr>
</thead>
<tbody>
<tr>
<td>MacBook Air 2020, Safari 17.0</td>
<td>124.04345259929687</td>
<td>0.0000023%</td>
</tr>
<tr>
<td>MacBook Pro 2015, Safari 16.6</td>
<td>124.04345808873768</td>
<td>0.0000044%</td>
</tr>
<tr>
<td>iPhone SE, Safari 13.1</td>
<td>35.10893253237009</td>
<td>1.8%</td>
</tr>
<tr>
<td>MacBook Air 2020, Chrome 116</td>
<td>124.04344968475198</td>
<td>0.0000023%</td>
</tr>
<tr>
<td>MacBook Pro 2015, Chrome 116</td>
<td>124.04347657808103</td>
<td>0.000015%</td>
</tr>
<tr>
<td>Galaxy S23, Chrome 114</td>
<td>124.08072766105033</td>
<td>0.030%</td>
</tr>
<tr>
<td>MacBook Pro 2015, Firefox 118</td>
<td>35.749968223273754</td>
<td>0.0000055%</td>
</tr>
<tr>
<td>MacBook Air 2020, Firefox 118</td>
<td>35.74996626004577</td>
<td>0.0000055%</td>
</tr>
<tr>
<td>BrowserStack Windows 8, Firefox 67</td>
<td>35.7383295930922</td>
<td>0.033%</td>
</tr>
</tbody>
</table>
<p>As you can see, the smallest difference is 0.0000023%, much smaller than the Safari noise range (0.008%). Eliminating the noise Safari adds requires rounding down by 1 decimal place, but we can’t round to fewer than 6 decimal places. Otherwise, some browsers from the above table will be indistinguishable. In other words, the fingerprint will have poor uniqueness.</p>
<h3 id="step-1-cutting-through-the-noise"><a href="#step-1-cutting-through-the-noise" aria-label="step 1 cutting through the noise permalink"></a>Step 1: Cutting through the noise</h3>
<p>The base idea for noise reduction is combining many separate audio fingerprints together. Each fingerprint is collected using the same algorithm, so the only difference is the noise added by the browser.</p>
<p>First, let’s take a closer look at the <a href="https://github.com/fingerprintjs/fingerprintjs/blob/a555410e925e0d6a4d548aa555b6945bd713e9cd/src/sources/audio.ts#L41" target="_blank" rel="noopener noreferrer">fingerprinting algorithm</a>. A fingerprint is a sum of 500 audio samples, and each audio sample is added with a random number with a <a href="https://en.wikipedia.org/wiki/Continuous_uniform_distribution" target="_blank" rel="noopener noreferrer">uniform distribution</a>. Therefore, according to <a href="https://en.wikipedia.org/wiki/Central_limit_theorem" target="_blank" rel="noopener noreferrer">the central limit theorem</a>, the fingerprint noise has a <a href="https://en.wikipedia.org/wiki/Normal_distribution" target="_blank" rel="noopener noreferrer">normal distribution</a>. The mean of the distribution is the un-noised fingerprint that we want to find.</p>
<p>The mean can be found using a large number of random samples (don’t confuse this with “audio samples”). This won’t be the true mean, but the more random samples there are, the more precise the result is. Uniform and normal distributions require different methods to find the mean:</p>
<ul>
<li>For a uniform distribution, the most precise formula is <code>(min+max)/2</code>, where <code>min</code> and <code>max</code> are the minimum and the maximum random samples</li>
<li>For a normal distribution, the most precise formula is the average of all the random samples</li>
</ul>
<p>Finding the mean of a uniform noise is much easier than a normally distributed noise. For a given precision, one needs much fewer samples in case of a uniform distribution to guess the mean. This JavaScript code proves the point in practice:</p>
<div data-language="jsx"><pre><code><span>const</span> sessionCount <span>=</span> <span>1000</span>
<span>const</span> desiredMaxError <span>=</span> <span>0.005</span>

<span>const</span> <span>uniformRandom</span> <span>=</span> <span>(</span><span>mean<span>,</span> variance</span><span>)</span> <span>=&gt;</span> <span>{</span>
  width <span>=</span> Math<span>.</span><span>sqrt</span><span>(</span><span>12</span> <span>*</span> variance<span>)</span>
  shift <span>=</span> mean <span>-</span> width <span>/</span> <span>2</span>
  <span>return</span> <span>(</span><span>)</span> <span>=&gt;</span> Math<span>.</span><span>random</span><span>(</span><span>)</span> <span>*</span> width <span>+</span> shift
<span>}</span>
<span>const</span> <span>normalRandom</span> <span>=</span> <span>(</span><span>mean<span>,</span> variance</span><span>)</span> <span>=&gt;</span> <span>{</span>
  <span>// https://en.wikipedia.org/wiki/Box–Muller_transform</span>
  <span>const</span> pi2 <span>=</span> Math<span>.</span><span>PI</span> <span>*</span> <span>2</span>
  <span>const</span> sigma <span>=</span> Math<span>.</span><span>sqrt</span><span>(</span>variance<span>)</span>
  <span>return</span> <span>(</span><span>)</span> <span>=&gt;</span> Math<span>.</span><span>sqrt</span><span>(</span><span>-</span><span>2</span> <span>*</span> Math<span>.</span><span>log</span><span>(</span>Math<span>.</span><span>random</span><span>(</span><span>)</span><span>)</span><span>)</span> <span>*</span> Math<span>.</span><span>cos</span><span>(</span>pi2 <span>*</span> Math<span>.</span><span>random</span><span>(</span><span>)</span><span>)</span> <span>*</span> sigma <span>+</span> mean
<span>}</span>

<span>const</span> <span>averageMeanFind</span> <span>=</span> <span>samples</span> <span>=&gt;</span> samples<span>.</span><span>reduce</span><span>(</span><span>(</span><span>a<span>,</span> b</span><span>)</span> <span>=&gt;</span> a <span>+</span> b<span>)</span> <span>/</span> samples<span>.</span>length
<span>const</span> <span>midRangeMeanFind</span> <span>=</span> <span>samples</span> <span>=&gt;</span> <span>{</span>
  <span>let</span> min <span>=</span> samples<span>[</span><span>0</span><span>]</span>
  <span>let</span> max <span>=</span> samples<span>[</span><span>0</span><span>]</span>
  <span>for</span> <span>(</span><span>let</span> i <span>=</span> <span>1</span><span>;</span> i <span>&lt;</span> samples<span>.</span>length<span>;</span> <span>++</span>i<span>)</span> <span>{</span>
    <span>if</span> <span>(</span>samples<span>[</span>i<span>]</span> <span>&lt;</span> min<span>)</span> <span>{</span>
      min <span>=</span> samples<span>[</span>i<span>]</span>
    <span>}</span> <span>else</span> <span>if</span> <span>(</span>samples<span>[</span>i<span>]</span> <span>&gt;</span> max<span>)</span> <span>{</span>
      max <span>=</span> samples<span>[</span>i<span>]</span>
    <span>}</span>
  <span>}</span>
  <span>return</span> <span>(</span>min <span>+</span> max<span>)</span> <span>/</span> <span>2</span>
<span>}</span>

<span>const</span> <span>findAdequateSampleCount</span> <span>=</span> <span>(</span><span>makeRandom<span>,</span> findMean</span><span>)</span> <span>=&gt;</span> <span>{</span>
  <span>const</span> mean <span>=</span> <span>0</span>
  <span>const</span> variance <span>=</span> <span>1</span>
  <span>const</span> random <span>=</span> <span>makeRandom</span><span>(</span>mean<span>,</span> variance<span>)</span>

  <span>sampleCountLoop</span><span>:</span>
  <span>for</span> <span>(</span><span>let</span> sampleCount <span>=</span> <span>2</span><span>;</span> sampleCount <span>&lt;</span> <span>1e7</span><span>;</span> sampleCount <span>*=</span> <span>2</span><span>)</span> <span>{</span>
    <span>for</span> <span>(</span><span>let</span> session <span>=</span> <span>0</span><span>;</span> session <span>&lt;</span> sessionCount<span>;</span> <span>++</span>session<span>)</span> <span>{</span>
      <span>const</span> samples <span>=</span> <span>[</span><span>...</span><span>Array</span><span>(</span>sampleCount<span>)</span><span>]</span><span>.</span><span>map</span><span>(</span>random<span>)</span>
      <span>const</span> foundMean <span>=</span> <span>findMean</span><span>(</span>samples<span>)</span>
      <span>if</span> <span>(</span>Math<span>.</span><span>abs</span><span>(</span>mean <span>-</span> foundMean<span>)</span> <span>&gt;</span> desiredMaxError<span>)</span> <span>{</span>
        <span>continue</span> sampleCountLoop
      <span>}</span>
    <span>}</span>
    <span>return</span> sampleCount
  <span>}</span>

  <span>return</span> <span>'Too much time to compute'</span>
<span>}</span>

console<span>.</span><span>log</span><span>(</span><span>'Normal needs samples'</span><span>,</span> <span>findAdequateSampleCount</span><span>(</span>normalRandom<span>,</span> averageMeanFind<span>)</span><span>)</span>
console<span>.</span><span>log</span><span>(</span><span>'Uniform needs samples'</span><span>,</span> <span>findAdequateSampleCount</span><span>(</span>uniformRandom<span>,</span> midRangeMeanFind<span>)</span><span>)</span>
<span>// Normal: 524288, uniform: 4096</span>
</code></pre></div>
<p>The old audio fingerprint is more computation-demanding and requires 100 times more fingerprint samples to reduce the noise. So, to reduce the noise in a reasonable time, we changed the fingerprinting algorithm to collect only one audio sample, which has a uniform noise distribution. The exact number of randomized samples needed depends on the rounding precision we need, which will be demonstrated later.</p>
<p>The algorithm change also means new fingerprints aren’t compatible with old fingerprints. Because of the rounding, the audio fingerprint will change, so sticking to the old fingerprint identifiers is useless. Note that you need to use <a href="https://github.com/fingerprintjs/fingerprintjs/blob/5ae80b7b946fcd824e35d033bc44e180334109f6/docs/version_policy.md#how-to-update-without-losing-the-identifiers" target="_blank" rel="noopener noreferrer">a special approach</a> to switch from the old fingerprint to the new one without losing the visitor identities.</p>
<h3 id="getting-many-noised-copies-of-the-same-audio-sample"><a href="#getting-many-noised-copies-of-the-same-audio-sample" aria-label="getting many noised copies of the same audio sample permalink"></a>Getting many noised copies of the same audio sample</h3>
<p>One approach for getting multiple noised copies is <a href="https://github.com/fingerprintjs/fingerprintjs/blob/a555410e925e0d6a4d548aa555b6945bd713e9cd/src/sources/audio.ts#L77" target="_blank" rel="noopener noreferrer">calling</a> <code>getChannelData</code> on the <code>AudioBuffer</code> instance many times. Remember that <a href="https://developer.mozilla.org/en-US/docs/Web/API/AudioBuffer/getChannelData" target="_blank" rel="noopener noreferrer"><code>getChannelData</code></a> returns the audio samples that the fingerprint is calculated from. This approach doesn’t work because noise is <a href="https://github.com/WebKit/WebKit/blob/167dc5118a3f6228a19df40e673ea0a6d03b9bec/Source/WebCore/Modules/webaudio/AudioBuffer.cpp#L336" target="_blank" rel="noopener noreferrer">applied once per</a> each <code>AudioBuffer</code> instance, and <code>getChannelData</code> returns the same signal.</p>
<p>This can be circumvented by creating many <code>AudioBuffer</code> instances by running the whole audio signal generation process many times. For 6,000 noised samples, the fastest time is 7 seconds on an M1 MacBook. For 60,000, Safari can’t even finish the process. This is way too long for a fingerprint. Therefore, this approach is not viable.</p>
<p>A better approach is to make an <code>AudioBuffer</code> instance with the same audio signal on repeat:</p>
<ol>
<li>Render an audio signal as usual, but don’t call <code>getChannelData</code>, because it will <a href="https://github.com/WebKit/WebKit/blob/167dc5118a3f6228a19df40e673ea0a6d03b9bec/Source/WebCore/Modules/webaudio/AudioBuffer.cpp#L167" target="_blank" rel="noopener noreferrer">add noise</a> to the signal.</li>
<li>Create another <code>OfflineAudioContext</code> instance, much longer than the original instance. Use the original signal as a source using an <a href="https://developer.mozilla.org/en-US/docs/Web/API/AudioBufferSourceNode" target="_blank" rel="noopener noreferrer"><code>AudioBufferSourceNode</code></a>.</li>
<li>Make the <code>AudioBufferSourceNode</code> loop the needed piece of the original signal using <code>loop</code>, <code>loopStart</code>, and <code>loopEnd</code>. The piece can be as narrow as a single audio sample.</li>
<li>Render the second (looped) audio context and call <code>getChannelData</code>. The resulting audio signal will consist of the original signal followed by the piece repeating until the end. Safari adds a noise after the looping, so the repeating copy has the same audio samples with different noise applied.</li>
</ol>
<p>Here’s how to implement this approach:</p>
<div data-language="jsx"><pre><code><span>const</span> sampleRate <span>=</span> <span>44100</span>

<span>getClonedPieces</span><span>(</span><span>)</span>

<span>async</span> <span>function</span> <span>getClonedPieces</span><span>(</span><span>)</span> <span>{</span>
  <span>const</span> pieceLength <span>=</span> <span>500</span> <span>// Can be as little as 1</span>
  <span>const</span> cloneCount <span>=</span> <span>1000</span>

  <span>// Rendering the original audio signal</span>
  <span>const</span> baseSignal <span>=</span> <span>await</span> <span>getBaseSignal</span><span>(</span><span>)</span>
  <span>const</span> loopStart <span>=</span> baseSignal<span>.</span>length <span>-</span> pieceLength

  <span>// A new audio context that loops an ending part of the original audio signal</span>
  <span>const</span> context <span>=</span> <span>new</span> <span>OfflineAudioContext</span><span>(</span><span>1</span><span>,</span> loopStart <span>+</span> cloneCount <span>*</span> pieceLength<span>,</span> sampleRate<span>)</span>
  <span>const</span> sourceNode <span>=</span> context<span>.</span><span>createBufferSource</span><span>(</span><span>)</span>
  sourceNode<span>.</span>buffer <span>=</span> baseSignal
  sourceNode<span>.</span>loop <span>=</span> <span>true</span>
  sourceNode<span>.</span>loopStart <span>=</span> <span>(</span>baseSignal<span>.</span>length <span>-</span> pieceLength<span>)</span> <span>/</span> sampleRate
  sourceNode<span>.</span>loopEnd <span>=</span> baseSignal<span>.</span>length <span>/</span> sampleRate
  sourceNode<span>.</span><span>connect</span><span>(</span>context<span>.</span>destination<span>)</span>
  sourceNode<span>.</span><span>start</span><span>(</span><span>)</span>

  <span>// Rendering the new audio context and extracting the looped part</span>
  <span>const</span> signalBuffer <span>=</span> <span>await</span> <span>renderAudio</span><span>(</span>context<span>)</span>
  <span>const</span> clones <span>=</span> signalBuffer<span>.</span><span>getChannelData</span><span>(</span><span>0</span><span>)</span><span>.</span><span>subarray</span><span>(</span>loopStart<span>)</span>
  
  console<span>.</span><span>log</span><span>(</span>clones<span>)</span>
<span>}</span>

<span>async</span> <span>function</span> <span>getBaseSignal</span><span>(</span><span>)</span> <span>{</span>
  <span>const</span> context <span>=</span> <span>new</span> <span>OfflineAudioContext</span><span>(</span><span>1</span><span>,</span> <span>5000</span><span>,</span> sampleRate<span>)</span>

  <span>// Any audio signal...</span>

  <span>return</span> <span>await</span> <span>renderAudio</span><span>(</span>context<span>)</span>
<span>}</span>

<span>function</span> <span>renderAudio</span><span>(</span><span>context</span><span>)</span> <span>{</span>
  <span>// See the implementation at https://github.com/fingerprintjs/fingerprintjs/blob/c411aff111e5c79cdc37608d42632d4a66a8c1dc/src/sources/audio.ts#L147</span>
<span>}</span>
</code></pre></div>
<p>Any number of noised sample copies can be produced in 2 audio renderings.</p>
<p>The code below combines the methods to denoise a single selected audio sample:</p>
<div data-language="jsx"><pre><code><span>const</span> sampleRate <span>=</span> <span>44100</span>

console<span>.</span><span>log</span><span>(</span><span>denoiseAudioSample</span><span>(</span><span>10000</span><span>)</span><span>)</span>

<span>async</span> <span>function</span> <span>denoiseAudioSample</span><span>(</span><span>cloneCount</span><span>)</span> <span>{</span>
  <span>// Rendering the original audio signal</span>
  <span>const</span> baseSignal <span>=</span> <span>await</span> <span>getBaseSignal</span><span>(</span><span>)</span>

  <span>// A new audio context that loops an ending part of the original audio signal</span>
  <span>const</span> context <span>=</span> <span>new</span> <span>OfflineAudioContext</span><span>(</span><span>1</span><span>,</span> baseSignal<span>.</span>length <span>-</span> <span>1</span> <span>+</span> cloneCount<span>,</span> sampleRate<span>)</span>
  <span>const</span> sourceNode <span>=</span> context<span>.</span><span>createBufferSource</span><span>(</span><span>)</span>
  sourceNode<span>.</span>buffer <span>=</span> baseSignal
  sourceNode<span>.</span>loop <span>=</span> <span>true</span>
  sourceNode<span>.</span>loopStart <span>=</span> <span>(</span>baseSignal<span>.</span>length <span>-</span> pieceLength<span>)</span> <span>/</span> sampleRate
  sourceNode<span>.</span>loopEnd <span>=</span> baseSignal<span>.</span>length <span>/</span> sampleRate
  sourceNode<span>.</span><span>connect</span><span>(</span>context<span>.</span>destination<span>)</span>
  sourceNode<span>.</span><span>start</span><span>(</span><span>)</span>

  <span>// Rendering the new audio context</span>
  <span>const</span> signalBuffer <span>=</span> <span>await</span> <span>renderAudio</span><span>(</span>context<span>)</span>

  <span>// Restoring the mean (the audio sample before the noising)</span>
  <span>return</span> <span>getMiddle</span><span>(</span>signalBuffer<span>.</span><span>getChannelData</span><span>(</span><span>0</span><span>)</span><span>.</span><span>subarray</span><span>(</span>baseSignal<span>.</span>length <span>-</span> <span>1</span><span>)</span><span>)</span>
<span>}</span>

<span>async</span> <span>function</span> <span>getBaseSignal</span><span>(</span><span>)</span> <span>{</span>
  <span>const</span> context <span>=</span> <span>new</span> <span>OfflineAudioContext</span><span>(</span><span>1</span><span>,</span> <span>5000</span><span>,</span> sampleRate<span>)</span>

  <span>// Any audio signal...</span>

  <span>return</span> <span>await</span> <span>renderAudio</span><span>(</span>context<span>)</span>
<span>}</span>

<span>function</span> <span>renderAudio</span><span>(</span><span>context</span><span>)</span> <span>{</span>
  <span>// See the implementation at https://github.com/fingerprintjs/fingerprintjs/blob/c411aff111e5c79cdc37608d42632d4a66a8c1dc/src/sources/audio.ts#L147</span>
<span>}</span>

<span>function</span> <span>getMiddle</span><span>(</span><span>samples</span><span>)</span> <span>{</span>
  <span>let</span> min <span>=</span> samples<span>[</span><span>0</span><span>]</span>
  <span>let</span> max <span>=</span> samples<span>[</span><span>0</span><span>]</span>
  <span>for</span> <span>(</span><span>let</span> i <span>=</span> <span>1</span><span>;</span> i <span>&lt;</span> samples<span>.</span>length<span>;</span> <span>++</span>i<span>)</span> <span>{</span>
    <span>if</span> <span>(</span>samples<span>[</span>i<span>]</span> <span>&lt;</span> min<span>)</span> <span>{</span>
      min <span>=</span> samples<span>[</span>i<span>]</span>
    <span>}</span> <span>else</span> <span>if</span> <span>(</span>samples<span>[</span>i<span>]</span> <span>&gt;</span> max<span>)</span> <span>{</span>
      max <span>=</span> samples<span>[</span>i<span>]</span>
    <span>}</span>
  <span>}</span>
  <span>return</span> <span>(</span>min <span>+</span> max<span>)</span> <span>/</span> <span>2</span>
<span>}</span>
</code></pre></div>
<p>At this point, the noise is suppressed, not removed completely. The resulting number is still not stable, but the variance is smaller than that of a raw audio sample.</p>
<p>This table shows how the denoising precision and time in the above code snippet depend on the number of samples (<code>cloneCount</code>):</p>
<table>
<thead>
<tr>
<th>Number of copies</th>
<th>Result range: (max-min)/min in 100 runs</th>
<th>Time on an M1 MacBook</th>
</tr>
</thead>
<tbody>
<tr>
<td>2,048</td>
<td>0.194%</td>
<td>2.0ms</td>
</tr>
<tr>
<td>4,096</td>
<td>0.190%</td>
<td>2.3ms</td>
</tr>
<tr>
<td>8,192</td>
<td>0.000387%</td>
<td>2.6ms</td>
</tr>
<tr>
<td>16,384</td>
<td>0.0000988%</td>
<td>2.9ms</td>
</tr>
<tr>
<td>32,768</td>
<td>0.0000411%</td>
<td>4.0ms</td>
</tr>
<tr>
<td>65,536</td>
<td>0.0000123%</td>
<td>4.1ms</td>
</tr>
<tr>
<td>131,072</td>
<td>0.00000823%</td>
<td>5.2ms</td>
</tr>
<tr>
<td>262,144</td>
<td>0% (the ultimate precision)</td>
<td>7.5ms</td>
</tr>
<tr>
<td>524,288</td>
<td>0%</td>
<td>11.9ms</td>
</tr>
<tr>
<td>1,048,576</td>
<td>0%</td>
<td>20.5ms</td>
</tr>
</tbody>
</table>
<h3 id="step-2-push-browser-identifier-numbers-farther-apart"><a href="#step-2-push-browser-identifier-numbers-farther-apart" aria-label="step 2 push browser identifier numbers farther apart permalink"></a>Step 2: Push browser identifier numbers farther apart</h3>
<p>The times shown in the previous table can be 100 times longer on low-end devices or heavy webpages. The performance of the fingerprinting is important, so the fewer copies there are, the better. However, fewer copies mean bigger result dispersion, so it’s necessary to increase the difference between the audio samples in browsers too. These differences can be achieved by changing the base signal.</p>
<h3 id="audio-nodes-with-heavy-distortion"><a href="#audio-nodes-with-heavy-distortion" aria-label="audio nodes with heavy distortion permalink"></a>Audio nodes with heavy distortion</h3>
<p>After hours of experimenting with all the built-in audio nodes, we found an audio signal generator that gives a much bigger audio sample variance between browsers. The generator is a chain of audio nodes:</p>
<ol>
<li>The initial signal is produced by a square <a href="https://developer.mozilla.org/en-US/docs/Web/API/OscillatorNode" target="_blank" rel="noopener noreferrer">OscillatorNode</a>.</li>
<li>Then, the signal goes through a <a href="https://developer.mozilla.org/en-US/docs/Web/API/DynamicsCompressorNode" target="_blank" rel="noopener noreferrer">DynamicsCompressorNode</a>.</li>
<li>Finally, the signal is processed by a <a href="https://developer.mozilla.org/en-US/docs/Web/API/BiquadFilterNode" target="_blank" rel="noopener noreferrer">BiquadFilterNode</a> of type “allpass”.</li>
</ol>
<p>It is not necessary to know what the audio nodes do in detail. They can be treated as black boxes.</p>
<p>The audio sample number 3396 of the signal has the biggest difference between browsers. The number 3396 was found by simply comparing all samples of the audio signals in different browsers. This is how the signal is implemented in code:</p>
<div data-language="jsx"><pre><code><span>async</span> <span>function</span> <span>getBaseSignal</span><span>(</span><span>)</span> <span>{</span>
  <span>const</span> context <span>=</span> <span>new</span> <span>OfflineAudioContext</span><span>(</span><span>1</span><span>,</span> <span>3396</span><span>,</span> <span>44100</span><span>)</span>

  <span>const</span> oscillator <span>=</span> context<span>.</span><span>createOscillator</span><span>(</span><span>)</span>
  oscillator<span>.</span>type <span>=</span> <span>'square'</span>
  oscillator<span>.</span>frequency<span>.</span>value <span>=</span> <span>1000</span>

  <span>const</span> compressor <span>=</span> context<span>.</span><span>createDynamicsCompressor</span><span>(</span><span>)</span>
  compressor<span>.</span>threshold<span>.</span>value <span>=</span> <span>-</span><span>70</span>
  compressor<span>.</span>knee<span>.</span>value <span>=</span> <span>40</span>
  compressor<span>.</span>ratio<span>.</span>value <span>=</span> <span>12</span>
  compressor<span>.</span>attack<span>.</span>value <span>=</span> <span>0</span>
  compressor<span>.</span>release<span>.</span>value <span>=</span> <span>0.25</span>

  <span>const</span> filter <span>=</span> context<span>.</span><span>createBiquadFilter</span><span>(</span><span>)</span>
  filter<span>.</span>type <span>=</span> <span>'allpass'</span>
  filter<span>.</span>frequency<span>.</span>value <span>=</span> <span>5.239622852977861</span>
  filter<span>.</span><span>Q</span><span>.</span>value <span>=</span> <span>0.1</span>

  oscillator<span>.</span><span>connect</span><span>(</span>compressor<span>)</span>
  compressor<span>.</span><span>connect</span><span>(</span>filter<span>)</span>
  filter<span>.</span><span>connect</span><span>(</span>context<span>.</span>destination<span>)</span>
  oscillator<span>.</span><span>start</span><span>(</span><span>0</span><span>)</span>

  <span>return</span> <span>await</span> <span>renderAudio</span><span>(</span>context<span>)</span>
<span>}</span>

<span>function</span> <span>renderAudio</span><span>(</span><span>context</span><span>)</span> <span>{</span>
  <span>// See the implementation at https://github.com/fingerprintjs/fingerprintjs/blob/c411aff111e5c79cdc37608d42632d4a66a8c1dc/src/sources/audio.ts#L147</span>
<span>}</span>

<span>// The audio sample number 3396 (if counted from 1)</span>
<span>const</span> audioSample <span>=</span> <span>(</span><span>await</span> <span>getBaseSignal</span><span>(</span><span>)</span><span>)</span><span>.</span><span>getChannelData</span><span>(</span><span>0</span><span>)</span><span>.</span><span>at</span><span>(</span><span>-</span><span>1</span><span>)</span></code></pre></div>
<p>The following table shows the resulting audio sample in different browsers:</p>
<table>
<thead>
<tr>
<th>Browser</th>
<th>Audio sample</th>
<th>Difference from the closest browser</th>
</tr>
</thead>
<tbody>
<tr>
<td>MacBook Air 2020, Safari 17.0</td>
<td>0.000059806101489812136</td>
<td>0.0014%</td>
</tr>
<tr>
<td>iPhone 13, Safari 15.4 (BrowserStack)</td>
<td>0.00005980528294458054</td>
<td>0.0014%</td>
</tr>
<tr>
<td>MacBook Pro 2015, Safari 16.6</td>
<td>0.00006429151108022779</td>
<td>0.046%</td>
</tr>
<tr>
<td>MacBook Pro 2015, Chrome 116</td>
<td>0.0000642621744191274</td>
<td>0.046%</td>
</tr>
<tr>
<td>MacBook Air 2020, Chrome 116</td>
<td>0.00006128742097644135</td>
<td>2.42%</td>
</tr>
<tr>
<td>Galaxy S23, Chrome 114</td>
<td>0.0000744499484426342</td>
<td>11.8%</td>
</tr>
<tr>
<td>Acer Chromebook 314, Chrome 117</td>
<td>0.00008321150380652398</td>
<td>10.53%</td>
</tr>
<tr>
<td>iPhone SE, Safari 13.1</td>
<td>0.00011335541057633236</td>
<td>26.6%</td>
</tr>
<tr>
<td>BrowserStack Windows 8, Firefox 67</td>
<td>0.00016917561879381537</td>
<td>0.0063%</td>
</tr>
<tr>
<td>MacBook Air 2020, Firefox 118</td>
<td>0.00016918622714001685</td>
<td>0.0040%</td>
</tr>
<tr>
<td>MacBook Pro 2015, Firefox 118</td>
<td>0.00016919305198825896</td>
<td>0.0040%</td>
</tr>
</tbody>
</table>
<p>Now the smallest difference is 0.0014%, which is much bigger than the original fingerprint (0.0000023%). It means that a much coarser denoising is possible.</p>
<h3 id="step-3-round-the-result"><a href="#step-3-round-the-result" aria-label="step 3 round the result permalink"></a>Step 3: Round the result</h3>
<p>The final step is stabilizing the sample to be used as a fingerprint. The sample range is small but still unstable, which is not suitable for FingerprintJS, because even a tiny change to the sample causes the whole fingerprint to change.</p>
<p>Rounding is a way to stabilize the audio sample. Usually, rounding preserves a specific number of digits after the decimal point. This is not a good choice in this case because, as mentioned at the beginning, the noise is not absolute; it’s relative to the audio sample number. Therefore, some number of <em>significant</em> digits should be preserved during rounding. Significant digits are all digits after the beginning zeros. You can see a rounding implementation on <a href="https://github.com/fingerprintjs/fingerprintjs/blob/c411aff111e5c79cdc37608d42632d4a66a8c1dc/src/sources/audio.ts#L244" target="_blank" rel="noopener noreferrer">GitHub</a>.</p>
<p>The table above shows that 5 significant digits are enough to tell the selected browsers apart. But since we can’t check all browsers in the world and can’t predict how they will change in the future, we use a few more digits, just in case.</p>
<p>The table below shows the number of audio sample copies needed to make the denoising result stable in private mode of Safari 17 after rounding with the given precision:</p>
<table>
<thead>
<tr>
<th>Significant digits</th>
<th># of copies</th>
<th>Time in Safari 17 on an M1 MacBook (warm)</th>
<th>Time in Chrome 116 on an M1 MacBook (warm)</th>
<th>Time in Chrome 114 on Pixel 2 (warm)</th>
</tr>
</thead>
<tbody>
<tr>
<td>6</td>
<td>15,000</td>
<td>3ms</td>
<td>4ms</td>
<td>13ms</td>
</tr>
<tr>
<td>7, but the last is the nearest multiple of 5</td>
<td>30,000</td>
<td>4ms</td>
<td>5ms</td>
<td>15ms</td>
</tr>
<tr>
<td>7, but the last is the nearest even digit</td>
<td>70,000</td>
<td>6ms</td>
<td>7ms</td>
<td>16ms</td>
</tr>
<tr>
<td>7 and more</td>
<td>400,000</td>
<td>12ms</td>
<td>13ms</td>
<td>34ms</td>
</tr>
</tbody>
</table>
<p><em>A ”warm” browser is a browser that has run the given code before. A browser becomes “cold” when it’s restarted. A warm browser produces more stable time measurements.</em></p>
<p>We chose “7, but the last is 0 or 5” as a good balance between the performance and uniqueness. We also increased the number of copies to 40,000 to increase stability.</p>
<p>The rounded number is the final new audio fingerprint that doesn’t change, even when Safari 17’s advanced fingerprinting protection is on. Uniqueness is an important property of fingerprinting. The new fingerprint has the same uniqueness as the old audio fingerprint.</p>
<h2 id="performance"><a href="#performance" aria-label="performance permalink"></a>Performance</h2>
<p>The following table shows the fingerprinting time on a blank page in warm browsers:</p>
<table>
<thead>
<tr>
<th>Browser</th>
<th>Old fingerprint</th>
<th>New fingerprint</th>
</tr>
</thead>
<tbody>
<tr>
<td>MacBook Air 2020, Safari 17.3</td>
<td>2ms</td>
<td>4ms</td>
</tr>
<tr>
<td>MacBook Air 2020, Chrome 120</td>
<td>5ms</td>
<td>8ms</td>
</tr>
<tr>
<td>MacBook Air 2020, Firefox 121</td>
<td>6ms</td>
<td>8ms</td>
</tr>
<tr>
<td>MacBook Pro 2015, Safari 16.6</td>
<td>4ms</td>
<td>6ms</td>
</tr>
<tr>
<td>MacBook Pro 2015, Chrome 120</td>
<td>5ms</td>
<td>7ms</td>
</tr>
<tr>
<td>MacBook Pro 2015, Firefox 121</td>
<td>5ms</td>
<td>7ms</td>
</tr>
<tr>
<td>iPhone 13 mini, Safari 17.3</td>
<td>8ms</td>
<td>12ms</td>
</tr>
<tr>
<td>iPhone SE, Safari 13.1</td>
<td>9ms</td>
<td>17ms</td>
</tr>
<tr>
<td>Acer Chromebook 314, Chrome 120</td>
<td>7ms</td>
<td>13ms</td>
</tr>
<tr>
<td>Galaxy S23, Chrome 120</td>
<td>6ms</td>
<td>8ms</td>
</tr>
<tr>
<td>Galaxy J7 Prime, Chrome 120</td>
<td>33ms</td>
<td>45ms</td>
</tr>
<tr>
<td>Pixel 3, Chrome 120</td>
<td>8ms</td>
<td>15ms</td>
</tr>
<tr>
<td>BrowserStack Windows 11, Chrome 120</td>
<td>5ms</td>
<td>7ms</td>
</tr>
<tr>
<td>BrowserStack Windows 11, Firefox 121</td>
<td>10ms</td>
<td>18ms</td>
</tr>
</tbody>
</table>
<p>Compared to the old fingerprinting algorithm, the performance of the new one degrades 1.5–2 times. Even so, the new fingerprint algorithm takes little time to compute, even on low-end devices.</p>
<p>The browser delegates some work to the OfflineAudioRender thread, freeing the main thread. Therefore, the page stays responsive during most of the audio fingerprint calculation. Web Audio API is not available for <a href="https://developer.mozilla.org/en-US/docs/Web/API/Web_Workers_API" target="_blank" rel="noopener noreferrer">web workers</a>, so we cannot calculate audio fingerprints there.</p>
<p>To improve the performance, the new fingerprint can be used only in Safari 17 while keeping the old algorithm in other browsers. Check whether the current browser is Safari 17 or newer using the user-agent string. Based on that, run either the old or the new fingerprinting algorithm.</p>
<h2 id="how-it-works-in-privacy-focused-browsers"><a href="#how-it-works-in-privacy-focused-browsers" aria-label="how it works in privacy focused browsers permalink"></a>How it Works in Privacy-Focused Browsers</h2>
<p>Privacy-focused browsers like Tor and Brave also make attempts to restrict audio fingerprinting. Web Audio API is completely disabled in Tor, so audio fingerprinting is <a href="https://gitlab.torproject.org/legacy/trac/-/issues/21984" target="_blank" rel="noopener noreferrer">impossible</a>. Brave, however, follows an approach like Safari 17 and adds noise to the audio signal. Our <a href="https://fingerprint.com/blog/audio-fingerprinting/#brave" target="_blank" rel="noopener noreferrer">previous article</a> explains more about Brave’s audio fingerprinting protection.</p>
<p>The Brave noise has an important difference. While Safari adds a random noise for each audio sample individually, Brave makes a random multiplier (called “fudge factor”) once and uses it for all audio samples. That is, all audio samples are multiplied by the same number. The fudge factor persists within a page. It changes only in a new regular or incognito session.</p>
<div data-language="jsx"><pre><code><span>// A pseudo-code to illustrate the difference</span>

<span>const</span> audioSignal <span>=</span> <span>new</span> <span>Float32Array</span><span>(</span><span>/* ... */</span><span>)</span>
<span>const</span> magnitude <span>=</span> <span>0.001</span>

<span>// Safari</span>
<span>for</span> <span>(</span><span>let</span> i <span>=</span> <span>0</span><span>;</span> i <span>&lt;</span> audioSignal<span>.</span>length<span>;</span> i<span>++</span><span>)</span> <span>{</span>
  audioSignal<span>[</span>i<span>]</span> <span>*=</span> <span>random</span><span>(</span><span>1</span> <span>-</span> magnitude<span>,</span> <span>1</span> <span>+</span> magnitude<span>)</span>
<span>}</span>

<span>// Brave</span>
<span>const</span> fudgeFactor <span>=</span> <span>random</span><span>(</span><span>1</span> <span>-</span> magnitude<span>,</span> <span>1</span> <span>+</span> magnitude<span>)</span>
<span>for</span> <span>(</span><span>let</span> i <span>=</span> <span>0</span><span>;</span> i <span>&lt;</span> audioSignal<span>.</span>length<span>;</span> i<span>++</span><span>)</span> <span>{</span>
  audioSignal<span>[</span>i<span>]</span> <span>*=</span> fudgeFactor
<span>}</span>
</code></pre></div>
<p>No matter how many audio sample copies we make, the noise addition will be the same in every copy. The copies won’t be dispersed around the true (before noising) audio sample. Therefore, <a href="#step-1-cutting-through-the-noise">the mathematical denoising method</a> doesn’t work.</p>
<p>Nevertheless, the Brave denoising method described in <a href="https://fingerprint.com/blog/audio-fingerprinting/#brave" target="_blank" rel="noopener noreferrer">the previous article</a> still works. <a href="#step-2-push-browser-identifier-numbers-farther-apart">The method for increasing the difference between fingerprints produced by browsers</a> can also increase the error tolerance.</p>
<h2 id="usage-in-fingerprintjs"><a href="#usage-in-fingerprintjs" aria-label="usage in fingerprintjs permalink"></a>Usage in FingerprintJS</h2>
<p>The new audio fingerprinting algorithm replaced the old one in FingerprintJS. It was first published in version <a href="https://github.com/fingerprintjs/fingerprintjs/releases/tag/v4.2.0" target="_blank" rel="noopener noreferrer">4.2.0</a>. You can see the full code for the audio fingerprint implementation <a href="https://github.com/fingerprintjs/fingerprintjs/blob/c411aff111e5c79cdc37608d42632d4a66a8c1dc/src/sources/audio.ts" target="_blank" rel="noopener noreferrer">in our GitHub repository</a>.</p>
<p>Audio fingerprinting is one of the many signals our <a href="https://github.com/fingerprintjs/fingerprintjs" target="_blank" rel="noopener noreferrer">source-available library</a> uses to generate a browser fingerprint. However, we do not blindly incorporate every signal available in the browser. Instead, we analyze the stability and uniqueness of each signal separately to determine their impact on fingerprint accuracy.</p>
<p>For audio fingerprinting, we found that the signal contributes only slightly to uniqueness but is highly stable, resulting in a slight net increase in fingerprint accuracy.</p>
<p>If you want to learn more about Fingerprint join us on <a href="https://discord.gg/39EpE2neBg" target="_blank" rel="noopener noreferrer">Discord</a> or reach out to us at <a href="mailto:oss-support@fingerprint.com" target="_blank" rel="noopener noreferrer">oss-support@fingerprint.com</a> for support using FingerprintJS.</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[FDA Designates MM120 (LSD) Breakthrough Therapy for Generalized Anxiety Disorder (120 pts)]]></title>
            <link>https://www.businesswire.com/news/home/20240307733599/en/MindMed-Receives-FDA-Breakthrough-Therapy-Designation-and-Announces-Positive-12-Week-Durability-Data-From-Phase-2B-Study-of-MM120-for-Generalized-Anxiety-Disorder</link>
            <guid>39653125</guid>
            <pubDate>Sat, 09 Mar 2024 17:19:44 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.businesswire.com/news/home/20240307733599/en/MindMed-Receives-FDA-Breakthrough-Therapy-Designation-and-Announces-Positive-12-Week-Durability-Data-From-Phase-2B-Study-of-MM120-for-Generalized-Anxiety-Disorder">https://www.businesswire.com/news/home/20240307733599/en/MindMed-Receives-FDA-Breakthrough-Therapy-Designation-and-Announces-Positive-12-Week-Durability-Data-From-Phase-2B-Study-of-MM120-for-Generalized-Anxiety-Disorder</a>, See on <a href="https://news.ycombinator.com/item?id=39653125">Hacker News</a></p>
<div id="readability-page-1" class="page"><div itemprop="articleBody">
        <p>NEW YORK--(<span itemprop="provider publisher copyrightHolder" itemscope="itemscope" itemtype="https://schema.org/Organization" itemid="https://www.businesswire.com"><span itemprop="name"><a referrerpolicy="unsafe-url" rel="nofollow" itemprop="url" href="https://www.businesswire.com/">BUSINESS WIRE</a></span></span>)--<b>Mind Medicine (MindMed) Inc. </b>(NASDAQ: MNMD), (Cboe Canada MMED), (the “Company” or “MindMed”), a clinical stage biopharmaceutical company developing novel product candidates to treat brain health disorders, today announced that FDA has granted breakthrough designation to its MM120 (lysergide d-tartrate) program for the treatment of generalized anxiety disorder (GAD). The Company also announced that its Phase 2b study of MM120 in GAD met its key secondary endpoint, and 12-week topline data demonstrated clinically and statistically significant durability of activity observed through Week 12.

</p>
<blockquote></blockquote>
<p>
MindMed previously announced rapid, clinically meaningful, and statistically significant improvements on the Hamilton Anxiety rating scale (HAM-A) compared to placebo at Week 4, which was the trial’s primary endpoint. MM120 was administered as a single dose in a monitored clinical setting with no additional therapeutic intervention.

</p><p>
“I’ve conducted clinical research studies in psychiatry for over two decades and have seen studies of many drugs under development for the treatment of anxiety. That MM120 exhibited rapid and robust efficacy, solidly sustained for 12 weeks after a single dose, is truly remarkable,” stated David Feifel, MD, PhD, Professor Emeritus of Psychiatry at the University of California, San Diego and Director of the Kadima Neuropsychiatry Institute in La Jolla, California and an investigator in the MM120 study. “These results suggest the potential MM120 has in the treatment of anxiety, and those of us who struggle every day to alleviate anxiety in our patients look forward to seeing results from future Phase 3 trials.”

</p><p>
MM120 100 µg – the dose with optimal clinical activity observed in the trial – demonstrated a 7.7-point improvement over placebo at Week 12 (-21.9 MM120 vs. -14.2 placebo; p&lt;0.003 Cohen’s d=0.81), with a 65% clinical response rate and a 48% clinical remission rate sustained to Week 12. Clinical Global Impressions - Severity (CGI-S) scores on average improved from 4.8 to 2.2 in the 100-µg dose group, representing a two-category shift from ‘markedly ill’ to ‘borderline ill’ at Week 12 (p&lt;0.004). This clinical activity was rapid, observed as early as study day 2, and durable with further improvements observed in mean HAM-A or CGI-S scores between Weeks 4 and 12.

</p><p>
Based on the significant unmet medical need in the treatment of GAD – especially in patients who do not respond to or tolerate currently available medications – along with the initial clinical data from Phase 2b and other research conducted by MindMed, the U.S. Food &amp; Drug Administration (FDA) has designated MM120 for GAD as a breakthrough therapy. The Company plans to hold an End-of-Phase 2 meeting with the FDA in the first half of 2024 and initiate a Phase 3 clinical program in the second half of 2024.

</p><p>
“The FDA’s decision to designate MM120 as a breakthrough therapy for GAD and the durability data from our Phase 2b study provide further validation of the important potential role this treatment can play in addressing the huge unmet need among individuals living with GAD,” said Robert Barrow, Chief Executive Officer and Director of MindMed. “We are committed to bringing MM120 to people living with GAD and delivering on the potential of our pipeline to treat serious brain health disorders.”

</p><p>
In the Phase 2b study, known as MMED008, MM120 was generally well-tolerated with most adverse events rated as mild to moderate, transient and occurring on dosing day, and being consistent with expected acute effects of the study drug. The most common adverse events (at least 10% incidence in the high dose groups) on dosing day included illusion, hallucinations, euphoric mood, anxiety, abnormal thinking, headache, paresthesia, dizziness, tremor, nausea, vomiting, feeling abnormal, mydriasis and hyperhidrosis.

</p><p>
Prior to treatment with MM120, study participants were clinically tapered and then washed out from any anxiolytic or antidepressant treatments and did not receive any form of study-related psychotherapy for the duration of their participation in the study.

</p><p>
“As a clinician and clinical researcher, I applaud the way this study was designed by MindMed to isolate the effect of MM120 by removing confounding variables like additional medications and psychotherapy,” said Reid Robison, MD, Psychiatrist and Chief Clinical Officer at Numinus (TSX:NUMI) who has served as adjunct faculty at the University of Utah for the last 12 years and was an investigator in the MM120 study. “It gives me confidence in the data and the positive results give me hope that this may translate into meaningful benefits for my patients.”

</p><p>
The primary data analyses from MMED008 have been accepted for presentation at the American Psychiatric Association’s annual meeting, which will be held in New York on May 4-8, 2024. The study is also being submitted for publication in a leading medical journal.

</p><p>
<b>Conference Call and Webcast</b>

</p><p>
MindMed management will host a webcast at 8:00 am ET today to discuss the Phase 2b results of MM120 in GAD. The webcast and slides will be accessible live under “News &amp; Events” on the Investors page of the Company’s website at <a referrerpolicy="unsafe-url" target="_blank" href="https://cts.businesswire.com/ct/CT?id=smartlink&amp;url=https%3A%2F%2Fir.mindmed.co%2F&amp;esheet=53906612&amp;newsitemid=20240307733599&amp;lan=en-US&amp;anchor=https%3A%2F%2Fir.mindmed.co%2F&amp;index=1&amp;md5=1f5a9dc24dd37580f1bf69afefc00ac6" rel="nofollow" shape="rect">https://ir.mindmed.co/</a> or by clicking <a referrerpolicy="unsafe-url" target="_blank" href="https://cts.businesswire.com/ct/CT?id=smartlink&amp;url=https%3A%2F%2Fmindmed-2024-virtual-investor-event.open-exchange.net%2Fwelcome&amp;esheet=53906612&amp;newsitemid=20240307733599&amp;lan=en-US&amp;anchor=here&amp;index=2&amp;md5=2c0c2e1c0e28deb77d4cdb54c18f44c3" rel="nofollow" shape="rect">here</a>. A replay of the event will be available on MindMed’s website. The webcast will be archived on the Company’s website for at least 30 days after the conference call.

</p><p>
<b>About Generalized Anxiety Disorder (GAD)</b>

</p><p>
GAD is a common condition associated with significant impairment that adversely affects millions of people. GAD results in fear, persistent anxiety and a constant feeling of being overwhelmed. It is characterized by excessive, persistent, and unrealistic worry about everyday things. Approximately 10% of U.S. adults, representing around 20 million people, currently suffer from GAD, an underdiagnosed and underserved indication that is associated with significant impairment, less accomplishment at work and reduced labor force participation. Despite the significant personal and societal burden of GAD, there has been little innovation in the treatment of GAD in the past several decades, with the last new drug approval occurring in 2004.

</p><p>
<b>About MMED008</b>

</p><p>
MMED008 was a multi-center, parallel, randomized, double-blind, placebo-controlled, dose-optimization study. The trial enrolled 198 participants who were randomized to receive a single administration of MM120 at a dose of 25, 50, 100 or 200 µg or placebo. The full analysis set (FAS) for the trial included 194 subjects, those that had at least one valid post-baseline Hamilton Anxiety rating scale (HAM-A) score. Subjects enrolled in the trial presented with severe GAD symptoms (average baseline HAM-A scores of approximately 30). The study's main objective was to determine the dose-response relationship of four doses of MM120 versus placebo as measured by the change in HAM-A from Baseline to Week 4. The key secondary objective of the study was to determine the dose-response relationship of four doses of MM120 versus placebo as measured by the change in HAM-A from Baseline to Week 8. Secondary objectives, measured up to 12 weeks after the single administration, include assessments of anxiety symptoms, safety and tolerability, and other measures of efficacy and quality of life. More information about the trial is available on the MindMed website (mindmed.co) or on clinicaltrials.gov (NCT05407064).

</p><p>
<b>About MM120</b>

</p><p>
Lysergide is a synthetic ergotamine belonging to the group of classic, or serotonergic, psychedelics, which acts as a partial agonist at human serotonin-2A (5-hydroxytryptamine-2A [5-HT2A]) receptors. MindMed is developing MM120 (lysergide D-tartrate), the tartrate salt form of lysergide, for GAD and is exploring its potential applications in other serious brain health disorders.

</p><p>
<b>About MindMed</b>

</p><p>
MindMed is a clinical stage biopharmaceutical company developing novel product candidates to treat brain health disorders. Our mission is to be the global leader in the development and delivery of treatments that unlock new opportunities to improve patient outcomes. We are developing a pipeline of innovative product candidates, with and without acute perceptual effects, targeting neurotransmitter pathways that play key roles in brain health disorders.

</p><p>
MindMed trades on NASDAQ under the symbol MNMD and on the Cboe Canada (formerly known as the NEO Exchange, Inc.) under the symbol MMED.

</p><p>
<b>Forward-Looking Statements</b>

</p><p>
Certain statements in this news release related to the Company constitute “forward-looking information” within the meaning of applicable securities laws and are prospective in nature. Forward-looking information is not based on historical facts, but rather on current expectations and projections about future events and are therefore subject to risks and uncertainties which could cause actual results to differ materially from the future results expressed or implied by the forward-looking statements. These statements generally can be identified by the use of forward-looking words such as “will”, “may”, “should”, “could”, “intend”, “estimate”, “plan”, “anticipate”, “expect”, “believe”, “potential” or “continue”, or the negative thereof or similar variations. Forward-looking information in this news release includes, but is not limited to, statements regarding anticipated upcoming milestones, and progress of trials and studies; results and timing of and reporting of full data from the Company’s Phase 2b clinical trial of MM120; timing of a potential End-of-Phase-2 meeting with the FDA; timing of the initiation of a potential Phase 3 clinical trial of MM120; and the potential benefits of the Company’s product candidates. There can be no guarantees regarding the results of the potential Phase 3 clinical trial or that, following any such trial, MM120 will receive the necessary regulatory approvals. There are numerous risks and uncertainties that could cause actual results and the Company’s plans and objectives to differ materially from those expressed in the forward-looking information, including history of negative cash flows; limited operating history; incurrence of future losses; availability of additional capital; lack of product revenue; compliance with laws and regulations; difficulty associated with research and development; risks associated with clinical trials or studies; heightened regulatory scrutiny; early stage product development; clinical trial risks; regulatory approval processes; novelty of the psychedelic inspired medicines industry; as well as those risk factors discussed or referred to herein and the risks described in the Company’s Annual Report on Form 10-K for the fiscal year ended December 31, 2023, under headings such as “Special Note Regarding Forward-Looking Statements,” “Risk Factors” and “Management’s Discussion and Analysis of Financial Condition and Results of Operations,” and other filings and furnishings made by the Company with the securities regulatory authorities in all provinces and territories of Canada which are available under the Company’s profile on SEDAR at <span><a referrerpolicy="unsafe-url" target="_blank" href="https://cts.businesswire.com/ct/CT?id=smartlink&amp;url=http%3A%2F%2Fwww.sedar.com&amp;esheet=53906612&amp;newsitemid=20240307733599&amp;lan=en-US&amp;anchor=www.sedar.com&amp;index=3&amp;md5=13bd8d738361aab62249735088c381d7" rel="nofollow" shape="rect">www.sedar.com</a></span> and with the U.S. Securities and Exchange Commission on EDGAR at <span><a referrerpolicy="unsafe-url" target="_blank" href="https://cts.businesswire.com/ct/CT?id=smartlink&amp;url=http%3A%2F%2Fwww.sec.gov&amp;esheet=53906612&amp;newsitemid=20240307733599&amp;lan=en-US&amp;anchor=www.sec.gov&amp;index=4&amp;md5=2253927ea51d3c244d9ba449edb52596" rel="nofollow" shape="rect">www.sec.gov</a></span>. Except as required by law, the Company undertakes no duty or obligation to update any forward-looking statements contained in this release as a result of new information, future events, changes in expectations or otherwise.

</p>
        
      </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[An ALS drug fails, again (124 pts)]]></title>
            <link>https://www.science.org/content/blog-post/als-drug-fails-again</link>
            <guid>39652667</guid>
            <pubDate>Sat, 09 Mar 2024 16:20:51 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.science.org/content/blog-post/als-drug-fails-again">https://www.science.org/content/blog-post/als-drug-fails-again</a>, See on <a href="https://news.ycombinator.com/item?id=39652667">Hacker News</a></p>
Couldn't get https://www.science.org/content/blog-post/als-drug-fails-again: Error: Request failed with status code 403]]></description>
        </item>
        <item>
            <title><![CDATA[Show HN: Schedule iMessage Texts from .txt Files (109 pts)]]></title>
            <link>https://github.com/reidjs/schedule-texts-from-txt</link>
            <guid>39652422</guid>
            <pubDate>Sat, 09 Mar 2024 15:50:12 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/reidjs/schedule-texts-from-txt">https://github.com/reidjs/schedule-texts-from-txt</a>, See on <a href="https://news.ycombinator.com/item?id=39652422">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><p dir="auto"><h2 tabindex="-1" dir="auto">schedule-texts-from-txt</h2><a id="user-content-schedule-texts-from-txt" aria-label="Permalink: schedule-texts-from-txt" href="#schedule-texts-from-txt"></a></p>
<p dir="auto">Schedule iMessage texts from <code>.txt</code> files from your Mac.</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/reidjs/schedule-texts-from-txt/blob/main/mailbox.png"><img src="https://github.com/reidjs/schedule-texts-from-txt/raw/main/mailbox.png" alt="a cute smiling mailbox"></a></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Send your first text</h2><a id="user-content-send-your-first-text" aria-label="Permalink: Send your first text" href="#send-your-first-text"></a></p>
<ol dir="auto">
<li>Clone this repo to your computer.</li>
<li>Verify there is a file in example_scheduled_texts folder named <code>Text myself now.txt</code> that has the word "Hello!" in the file body.</li>
<li>Open <code>SETTINGS.txt</code> and replace the number after <code>myself=...</code> to <strong>your phone number</strong>.</li>
<li>Open your terminal to this project directory and run these commands in order
<ol dir="auto">
<li><code>virtualenv venv</code></li>
<li><code>source activate.sh</code></li>
<li><code>pip install -r requirements.txt</code></li>
<li><code>python send_scheduled_messages.py</code></li>
</ol>
</li>
<li>✅ If everything went well, you should receive an iMessage text that says "Hello!" from yourself</li>
</ol>
<p dir="auto"><h2 tabindex="-1" dir="auto">How it works</h2><a id="user-content-how-it-works" aria-label="Permalink: How it works" href="#how-it-works"></a></p>
<p dir="auto">The script parses the filename, <code>Text {person} {datetime}.txt</code>, to determine who and when to send the text message.</p>
<ol dir="auto">
<li>The first word in the filename must be "Text."</li>
<li>The second word, <code>person</code> must be the persons identifier. This is set in SETTINGS.txt, e.g., <code>bob=1234567890</code> would set <code>bob</code>'s phone number to <code>1234567890</code></li>
<li>Everything following the name, before the .txt, is considered the <code>datetime</code>. Many formats may work for the <code>datetime</code>, but this is the only one I have tested extensively: <code>Month D, YYYY HH:MM(AM|PM)</code>, for example <code>March 9, 2024 7:25AM</code>. You can also use the keyword <code>now</code> as a <code>datetime</code> to send one immediately.</li>
</ol>
<p dir="auto"><h2 tabindex="-1" dir="auto">Help &amp; Feedback</h2><a id="user-content-help--feedback" aria-label="Permalink: Help &amp; Feedback" href="#help--feedback"></a></p>
<p dir="auto">Please create a GitHub issue if you have feedback or need help. Thanks!</p>
<p dir="auto">Made by Reid JS on March 9, 2024</p>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[AI-Generated Data Can Poison Future AI Models (141 pts)]]></title>
            <link>https://www.scientificamerican.com/article/ai-generated-data-can-poison-future-ai-models/</link>
            <guid>39652262</guid>
            <pubDate>Sat, 09 Mar 2024 15:34:12 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.scientificamerican.com/article/ai-generated-data-can-poison-future-ai-models/">https://www.scientificamerican.com/article/ai-generated-data-can-poison-future-ai-models/</a>, See on <a href="https://news.ycombinator.com/item?id=39652262">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p data-block="sciam/paragraph">Thanks to a boom in <a href="https://www.scientificamerican.com/podcast/episode/why-were-worried-about-generative-ai/">generative artificial intelligence</a>, programs that can produce text, computer code, images and music are readily available to the average person. And we’re already using them: AI content is <a href="https://www.wsj.com/articles/chatgpt-already-floods-some-corners-of-the-internet-with-spam-its-just-the-beginning-9c86ea25?mod=tech_lead_pos6&amp;mc_cid=987d4025e9&amp;mc_eid=74dd22853c">taking over the Internet</a>, and text generated by “<a href="https://www.scientificamerican.com/article/what-the-new-gpt-4-ai-can-do/">large language models</a>” is filling hundreds of websites, including CNET and Gizmodo. But as AI developers scrape the Internet, AI-generated content may soon enter the data sets used to <a href="https://www.scientificamerican.com/article/why-we-need-to-see-inside-ais-black-box/">train new models</a> to respond like humans. Some experts say that will inadvertently introduce errors that build up with each succeeding generation of models.</p><p data-block="sciam/paragraph">A growing body of evidence supports this idea. It suggests that a training diet of AI-generated text, even in small quantities, eventually becomes “poisonous” to the model being trained. Currently there are few obvious antidotes. “While it may not be an issue right now or in, let’s say, a few months, I believe it will become a consideration in a few years,” says Rik Sarkar, a computer scientist at the School of Informatics at the University of Edinburgh in Scotland.</p><figure data-original-class="cms-video" data-block="sciam/image"><iframe allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen="" frameborder="0" height="360" loading="lazy" src="https://www.youtube.com/embed/ZWvTr5wKGCA" title="Video player" width="640"></iframe></figure><hr><h2>On supporting science journalism</h2><p>If you're enjoying this article, consider supporting our award-winning journalism by<!-- --> <a href="https://www.scientificamerican.com/getsciam/">subscribing</a>. By purchasing a subscription you are helping to ensure the future of impactful stories about the discoveries and ideas shaping our world today.</p><hr><p data-block="sciam/paragraph">The possibility of AI models tainting themselves may be a bit analogous to a certain 20th-century dilemma. After the first atomic bombs were detonated at World War II’s end, decades of nuclear testing spiced Earth’s atmosphere with a dash of radioactive fallout. When that air entered newly-made steel, it brought elevated radiation with it. For particularly radiation-sensitive steel applications, such as Geiger counter consoles, that fallout poses an obvious problem: it won’t do for a Geiger counter to flag itself. Thus, a rush began for a dwindling supply of low-radiation metal. Scavengers <a href="https://www.theatlantic.com/science/archive/2019/10/search-dark-matter-depends-ancient-shipwrecks/600718/">scoured</a> old shipwrecks to extract scraps of prewar steel. Now some insiders believe a similar cycle is set to repeat in generative AI—with training data instead of steel.</p><p data-block="sciam/paragraph">Researchers can watch AI’s poisoning in action. For instance, start with a language model trained on human-produced data. Use the model to generate some AI output. Then use that output to train a new instance of the model and use the resulting output to train a third version, and so forth. With each iteration, errors build atop one another. The 10th model, prompted to write about historical English architecture, <a href="https://arxiv.org/abs/2305.17493v2">spews out gibberish about jackrabbits</a>.</p><p data-block="sciam/paragraph">“It gets to a point where your model is practically meaningless,” says Ilia Shumailov, a machine learning researcher at the University of Oxford.</p><p data-block="sciam/paragraph">Shumailov and his colleagues call this phenomenon “model collapse.” They observed it in a language model called OPT-125m, as well as a different AI model that generates handwritten-looking numbers and even a simple model that tries to separate two probability distributions. “Even in the simplest of models, it’s already happening,” Shumailov says. “I promise you, in more complicated models, it’s 100 percent already happening as well.”</p><p data-block="sciam/paragraph">In a recent preprint study, Sarkar and his colleagues in Madrid and Edinburgh <a href="https://arxiv.org/abs/2306.06130">conducted a similar experiment</a> with a type of AI image generator called a diffusion model. Their first model in this series could generate recognizable flowers or birds. By their third model, those pictures had devolved into blurs.</p><p data-block="sciam/paragraph">Other tests showed that even a partly AI-generated training data set was toxic, Sarkar says. “As long as some reasonable fraction is AI-generated, it becomes an issue,” he explains. “Now exactly how much AI-generated content is needed to cause issues in what sort of models is something that remains to be studied.”</p><p data-block="sciam/paragraph">Both groups experimented with relatively modest models—programs that are smaller and use fewer training data than the likes of the language model GPT-4 or the image generator Stable Diffusion. It’s possible that larger models will prove more resistant to model collapse, but researchers say there is little reason to believe so.</p><p data-block="sciam/paragraph">The research so far indicates that a model will suffer most at the “tails” of its data—the data elements that are less frequently represented in a model’s training set. Because these tails include data that are further from the “norm,” a model collapse could cause the AI’s output to lose the diversity that researchers say is distinctive about human data. In particular, Shumailov fears this will exacerbate models’ existing biases against marginalized groups. “It’s quite clear that the future is the models becoming more biased,” he says. “Explicit effort needs to be put in order to curtail it.”</p><p data-block="sciam/paragraph">Perhaps all this is speculation, but AI-generated content is already beginning to enter realms that machine-learning engineers rely on for training data. Take language models: even mainstream news outlets <a href="https://www.wired.com/story/cnet-published-ai-generated-stories-then-its-staff-pushed-back/">have begun publishing AI-generated articles</a>, and some Wikipedia editors <a href="https://www.vice.com/en/article/v7bdba/ai-is-tearing-wikipedia-apart">want to use language models</a> to produce content for the site.</p><p data-block="sciam/paragraph">“I feel like we’re kind of at this inflection point where a lot of the existing tools that we use to train these models are quickly becoming saturated with synthetic text,” says Veniamin Veselovskyy, a graduate student at the Swiss Federal Institute of Technology in Lausanne (EPFL).</p><p data-block="sciam/paragraph">There are warning signs that AI-generated data might enter model training from elsewhere, too. Machine-learning engineers have long relied on crowd-work platforms, such as Amazon’s Mechanical Turk, to annotate their models’ training data or to review output. Veselovskyy and his colleagues at EPFL asked Mechanical Turk workers to summarize medical research abstracts. They found that around <a href="https://arxiv.org/abs/2306.07899">a third of the summaries had ChatGPT’s touch</a>.</p><p data-block="sciam/paragraph">The EPFL group’s work, released on the preprint server arXiv.org last month, examined only 46 responses from Mechanical Turk workers, and summarizing is a classic language model task. But the result has raised a specter in machine-learning engineers’ minds. “It is much easier to annotate textual data with ChatGPT, and the results are extremely good,” says Manoel Horta Ribeiro, a graduate student at EPFL. Researchers such as Veselovskyy and Ribeiro have begun considering ways to protect the humanity of crowdsourced data, including tweaking websites such as Mechanical Turk in ways that discourage users from turning to language models and redesigning experiments to encourage more human data.</p><p data-block="sciam/paragraph">Against the threat of model collapse, what is a hapless machine-learning engineer to do? The answer could be the equivalent of prewar steel in a Geiger counter: data known to be free (or perhaps as free as possible) from generative AI’s touch. For instance, Sarkar suggests the idea of employing “standardized” image data sets that would be curated by humans who know their content consists only of human creations and freely available for developers to use.</p><p data-block="sciam/paragraph">Some engineers may be tempted to pry open the Internet Archive and look up content that predates the AI boom, but Shumailov doesn’t see going back to historical data as a solution. For one thing, he thinks there may not be enough historical information to feed growing models’ demands. For another, such data are just that: historical and not necessarily reflective of a changing world.</p><p data-block="sciam/paragraph">“If you wanted to collect the news of the past 100 years and try and predict the news of today, it’s obviously not going to work, because technology’s changed,” Shumailov says. “The lingo has changed. The understanding of the issues has changed.”</p><p data-block="sciam/paragraph">The challenge, then, may be more direct: discerning human-generated data from synthetic content and filtering out the latter. But even if the technology for this existed, it is far from a straightforward task. As Sarkar points out, in a world where Adobe Photoshop <a href="https://www.adobe.com/sensei/generative-ai/firefly.html">allows its users to edit images with generative AI</a>, is the result an AI-generated image—or not?</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[An all-optical general-purpose CPU and optical computer architecture (162 pts)]]></title>
            <link>https://arxiv.org/abs/2403.00045</link>
            <guid>39651926</guid>
            <pubDate>Sat, 09 Mar 2024 14:49:53 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://arxiv.org/abs/2403.00045">https://arxiv.org/abs/2403.00045</a>, See on <a href="https://news.ycombinator.com/item?id=39651926">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="content-inner">
    
    
                
    <p><a href="https://arxiv.org/pdf/2403.00045">Download PDF</a>
    <a href="https://arxiv.org/html/2403.00045v1">HTML (experimental)</a></p><blockquote>
            <span>Abstract:</span>Energy efficiency of electronic digital processors is primarily limited by the energy consumption of electronic communication and interconnects. The industry is almost unanimously pushing towards replacing both long-haul, as well as local chip interconnects, using optics to drastically increase efficiency. In this paper, we explore what comes after the successful migration to optical interconnects, as with this inefficiency solved, the main source of energy consumption will be electronic digital computing, memory and electro-optical conversion. Our approach attempts to address all these issues by introducing efficient all-optical digital computing and memory, which in turn eliminates the need for electro-optical conversions. Here, we demonstrate for the first time a scheme to enable general purpose digital data processing in an integrated form and present our photonic integrated circuit (PIC) implementation. For this demonstration we implemented a URISC architecture capable of running any classical piece of software all-optically and present a comprehensive architectural framework for all-optical computing to go beyond.
    </blockquote>

    <!--CONTEXT-->
    
  </div><div>
      <h2>Submission history</h2><p> From: Leonardo Del Bino [<a href="https://arxiv.org/show-email/211339aa/2403.00045">view email</a>]      <br>    <strong>[v1]</strong>
        Thu, 29 Feb 2024 15:49:25 UTC (2,392 KB)<br>
</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Monodraw (726 pts)]]></title>
            <link>https://monodraw.helftone.com/</link>
            <guid>39651796</guid>
            <pubDate>Sat, 09 Mar 2024 14:33:02 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://monodraw.helftone.com/">https://monodraw.helftone.com/</a>, See on <a href="https://news.ycombinator.com/item?id=39651796">Hacker News</a></p>
<div id="readability-page-1" class="page">

<section id="introduction">

	<header>
		
	</header>

<div>

	<hgroup id="ani_h1">
		<h2 data-wow-duration="1.2s" data-wow-delay=".7s">Powerful ASCII art editor designed for the Mac.</h2>
		<!-- <h2 class="inline-block h1 wow fadeInRight" data-wow-delay="1.6s"> Now in Beta.</h2> -->
	</hgroup>
	
	<p><img src="https://monodraw.helftone.com/static/images/screenshots/shot-robot.png" width="1072"></p>
</div>

</section>

<div>

	<h2>Harness the Power and Simplicity of Plain Text</h2>

	<p>
		Plain text has been around for decades and it's here to stay. <strong>Monodraw</strong> allows you to easily create text-based art (like diagrams, layouts, flow charts) and visually represent algorithms, data structures, binary formats and more. Because it's all just text, it can be easily embedded almost anywhere. Of course, exporting as images is also supported (PNG and SVG).
	</p>

</div>



<div id="section-text-tool">
    <div data-wow-delay="0.2s" data-wow-duration="1.1s">
        <h2><img src="https://monodraw.helftone.com/static/images/diagram-icon.png" width="44px"> Diagrams</h2>
        <p>
          A picture is worth a thousand words. A diagram is probably worth twice as much. Enhance your technical <a href="https://github.com/dguerri/vagrant-ansible-openstack#architecture---network-diagram">documentation</a> (code, specs) with easy to comprehend textual art. Visualisation of data structures, algorithms and data formats plays a crucial role in understanding. You will be reading the code more often than writing it, so why not make it much easier to grasp.
          </p>
      </div>
    <div data-wow-duration="1.1s">
        <p><img src="https://monodraw.helftone.com/static/images/screenshots/shot-diagrams.png">
        </p>
    </div>
  </div>

<!-- Section Text Field -->
<div id="section-text-tool">

    <div data-wow-duration="1.1s" data-wow-delay="0.2s">
      <p><img src="https://monodraw.helftone.com/static/images/screenshots/shot-mind-map.png">
      </p>
    </div>

    <div data-wow-duration="1.1s">
        <h2><img src="https://monodraw.helftone.com/static/images/mindmap-icon.png" width="44px"> Mind Mapping</h2>
        <p>
          Combine the simplicity of plain text with the power of mind mapping. Monodraw gives you the freedom to manage your textual data exactly the way you want. Move text around anywhere in the infinite canvas – no need to be constrained by the linear structure of a text file.
        </p>
      </div>

  </div>
<!-- # End Section  -->

<!-- Section Text Field -->
<div id="section-text-tool">
    <div data-wow-duration="1.1s">
        <h2><img src="https://monodraw.helftone.com/static/images/erdiagram-icon.png" width="44px"> ER Diagrams</h2>
        <p>
			Do you deal with databases? Then you know how useful entity-relationship diagrams can be. Visually describe your data model with a simple ER diagram. Monodraw supports Crow's Foot notation in three different variants to suit your personal preference.
        </p>
      </div>
    <div data-wow-duration="1.1s" data-wow-delay="0.2s">
      <p><img src="https://monodraw.helftone.com/static/images/screenshots/shot-er-diagrams.png">
      </p>
    </div>
  </div>
<!-- # End Section  -->

<!-- Section Text Field -->
<div id="section-text-tool">

    <div data-wow-duration="1.1s" data-wow-delay="0.2s">
      <p><img src="https://monodraw.helftone.com/static/images/screenshots/shot-banners.png">
      </p>
    </div>
    <div data-wow-duration="1.1s">
        <h2><img src="https://monodraw.helftone.com/static/images/banner-icon.png" width="44px"> Banners</h2>
        <p>
      Easily create text banners with just a single click. <a href="http://www.figlet.org/">FIGlet</a> is built into Monodraw and we bundle 148 fonts as standard (custom ones are supported, too). You can interactively resize the text box, change the font and adjust the alignment – no need for a terminal.
        </p>
      </div>
  </div>
<!-- # End Section  -->


<!-- <section class="center bg-white">
<div class=" gradient-7 py1 mx-auto wow fadeIn">
	<h2 class="h1 mt0 thin black fw300">Tools that will extend your capabilities </h2>
	<p class="col-10 h4 mx-auto dark-gray">
		Plain text has been around for decades and it's here to stay.
	</p>
<ul class="table table-fixed center p0">
	<li class="table-cell"><h2 class="h3 h-has-icon"><img class="block mx-auto mb2 2x" src="/static/images/text-field-icon.png" width="44px"/> Text Tool</h2></li>

	<li class="table-cell"><h2 class="h3 h-has-icon"><img class="block mx-auto mb2 2x" src="/static/images/line-icon.png" width="44px"/> Line Tool</h2></li>
	<li class="table-cell"><h2 class="h3 h-has-icon"><img class="block mx-auto mb2 2x" src="/static/images/text-field-icon.png" width="44px"/> Rect Tool</h2></li>
	<li class="table-cell"><h2 class="h3 h-has-icon"><img class="block mx-auto mb2 2x" src="/static/images/text-field-icon.png" width="44px"/> Drawing Tool</h2></li>
</ul>
</div>

</section> -->

<!-- Section Text Field -->
<div id="section-text-tool">
    <div data-wow-duration="1.1s">
        <h2><img src="https://monodraw.helftone.com/static/images/text-field-icon.png" width="44px"> Text Tool</h2>
        <p>
          Monodraw is powered by a custom CoreText-based text engine giving you precise control over the layout. You can adjust the alignment, position, line sweep direction and line movement. Adding a border around your text is only a click away, too.
        </p>
      </div>
    <div data-wow-duration="1.1s" data-wow-delay="0.2s">
      <p><img src="https://monodraw.helftone.com/static/images/screenshots/shot-text-tool.png">
      </p>
    </div>
  </div>
<!-- # End Section  -->

<!-- Section Line Tool  -->
<div>
    <div data-wow-duration="1.1s" data-wow-delay="0.2s">
      <p><img src="https://monodraw.helftone.com/static/images/screenshots/shot-line-tool.png">
      </p>
    </div>
    <div data-wow-duration="1.1s">
        <h2><img src="https://monodraw.helftone.com/static/images/line-icon.png" width="44px">Line Tool</h2>
        <p>
          The line tool makes connecting shapes as easy as pie. Orthogonal and staircase lines are supported with the ability to set a line dash pattern. Attachment points allow you to dynamically attach your lines to other shapes so that you don't have to re-arrange them each time you move things around.
        </p>
      </div>
  </div>
<!-- # End Section  -->

<!-- Section Rectangle -->
<div>
    <div data-wow-duration="1.1s">
        <h2><img src="https://monodraw.helftone.com/static/images/rectangle-icon.png" width="44px">Rect Tool</h2>
        <p>
          The rectangle tool can be used to create all kinds of boxes which are the most commonly used element in text art. Specify border or a background with just a few clicks. Oh, you can add shadows, too! Last but not least, custom attachment points will help you attach your lines at exactly the right place.
        </p>
      </div>
    <div data-wow-duration="1.1s" data-wow-delay="0.2s">
      <p><img src="https://monodraw.helftone.com/static/images/screenshots/shot-rect-tool.png">
      </p>
    </div>
  </div>
<!-- # End Section  -->

<!-- Section Text Field -->
<div id="section-text-tool">

    <div data-wow-duration="1.1s" data-wow-delay="0.2s">
      <p><img src="https://monodraw.helftone.com/static/images/screenshots/monodraw-cli.png">
      </p>
    </div>
    <div data-wow-duration="1.1s">
        <h2><img src="https://monodraw.helftone.com/static/images/terminal-icon.png" width="44px">CLI Included</h2>
        <p>
      Monodraw includes a command-line interface (<a href="#cli-tool-direct-store">Direct version only</a>). For example, you can use it to automatically generate docs when committing by leveraging version control hooks. The tool can also output JSON, for easier programmatic manipulation.
        </p>
      </div>
  </div>
<!-- # End Section  -->

<!-- Section Text Field -->
<div id="section-text-tool">
    <div data-wow-duration="1.1s">
        <h2><img src="https://monodraw.helftone.com/static/images/pencil-icon.png" width="44px">Drawing Tools</h2>
        <p>
			The basic drawing tools that you would expect make their usual appearance. The Pencil, Eraser, Bucket Fill and Picker are all indispensable when it comes to producing textual art. You can also easily overlay any images on the canvas for tracing purposes.
        </p>
      </div>
    <div data-wow-duration="1.1s" data-wow-delay="0.2s">
      <p><img src="https://monodraw.helftone.com/static/images/screenshots/shot-drawing-tools.png">
      </p>
    </div>
  </div>
<!-- # End Section  -->



<!-- Section Powerful features -->
<section>
  <div data-wow-duration="1.1s">
      <h2><img src="https://monodraw.helftone.com/static/images/power-icon.png" width="44px">Powerful Features</h2>
      <p>When it comes to creating text art, Monodraw helps you out by providing the tools you need.</p>
    </div>
  <div>
    <div data-wow-duration="1.1s" data-wow-delay="0.2s">
      <p><img src="https://monodraw.helftone.com/static/images/feat-group-icon.png" width="48px"></p><h2>Groups</h2>
      <p>
        Shapes can be grouped for effortless management. By composing multiple elements to form a single group, duplication and movement become very easy.
      </p>
    </div>
    <div data-wow-duration="1.1s" data-wow-delay="0.4s">
      <p><img src="https://monodraw.helftone.com/static/images/guides-icon.png" width="48px"></p><h2>Guides</h2>
      <p>
        Alignment guides are a life-saver when arranging and sizing your content – no longer do you have to stare at the screen and count the number of characters.
      </p>
    </div>
    <div data-wow-duration="1.1s" data-wow-delay="0.6s">
      <p><img src="https://monodraw.helftone.com/static/images/focus-group-icon.png" width="48px"></p><h2>Focus</h2>
      <p>
        When you need to focus on a particular part of the canvas, the rest of the shapes can be locked or hidden away. You can then zoom in to concentrate on the currently visible elements.
      </p>
    </div>
    <div data-wow-duration="1.1s" data-wow-delay=".8s">
      <p><img src="https://monodraw.helftone.com/static/images/shortcuts-icon.png" width="72px"></p><h2>Shortcuts</h2>
      <p>
        All functionality can be efficiently accessed via shortcuts, so there is no need to take your hands off the keyboard. Tools are quickly accessible with a single keystroke, without the need for a modifier.
      </p>
    </div>
  </div>
  <!-- <div class="container clearfix">
  <div class="col col-6">
  <div class="right p4 feature">
  <h2 class="h1 thin h-has-icon"><img class="left mr2 2x" src="/static/images/power-icon.png" width="44px"/>Powerful Features</h2>
  <p>
  When it comes to creating text art, Monodraw helps you out by providing the tools you need. Shapes can be grouped for effortless management. Alignment guides are a life-saver when you are trying to arrange and size your content &ndash; no longer do you have to stare at the screen and count the number of characters.
</p>
<p>
When you need to focus on a particular part of your drawing, you can just lock or hide the rest of the shapes. All the functionality can be efficiently accessed via shortcuts, so there is no need to take your hands off the keyboard.
</p>
</div>
</div>
<div class="col col-6">
<div class="screenshot ss-right">
<img class="2x" src="/static/images/screenshot.png" width="100%"/>
</div>
</div>
</div> -->
</section>
<!-- # End Section  -->

<!-- Section Powerful features -->
<div id="designed_for_mac">
<div id="" data-wow-duration="1.1s" data-wow-delay="0.2s">
  <pre>                        ◎
                        │
                        │                ┌─────────────────────┐
                 ┌──────◇──────┐         │                     │
                 │             │         │                     │
             ┏■──┘━━━━━━━━━━━━━└──■┓     │                     │
             ┃*━━━━━━━━━━━━━━━━━━━*┃     ◍ Hello, dear friend! │
             ┃┃   ■■■■■   ■■■■■   ┃┃    ╱                      │
             ┃┃   ■■■■■   ■■■■■   ┃┃   ╱                       │
             ┃┃   ■■■■■   ■■■■■   ┃┃  ╱──◍                     │
             ┃*━━━━━━━━━━━━━━━━━━━*┃     └─────────────────────┘
            ◆─────◆─────◆─────◆─────◆
            │╲***╱*╲***╱*╲***╱*╲***╱│
            │*╲*╱***╲*╱***╲*╱***╲*╱*│
            └──◆─────◆─────◆─────◆──┘
             ┗━━━━━━━━━━━━━━━━━━━━━┛
      ┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
┏━━━━━┃///////////////////////////////////┃━━━━━┓
┃ ┌─┐ ┃.▫▫▫▫▫▫▫▫▫▫▫▫▫▫▫▫▫▫▫▫▫▫▫▫▫▫▫▫▫▫▫▫▫.┃ ┌─┐ ┃
┃ │ │ ┃.▫          ╔═════════╗          ▫.┃ │ │ ┃
┗━│ │━┃.▫          ║   ┌┐ ●─○║          ▫.┃━│ │━┛
  │ │ ◎─▣──────────▣   ││  ┌─▣──────────▣─◎ │ │
  │ │ ┃.▫▨▨▨▨▨▨▨▨▨▨└┐ ┌┘└┐┌┘ ║▨▨▨▨▨▨▨▨▨▨▫.┃ │ │
  │ │ ┃.▫◹◹◹◹◹◹◹◹◹◹║└─┘  └┘  ║◸◸◸◸◸◸◸◸◸◸▫.┃ │ │
  │ │ ┃.           ╚═════════╝           .┃ │ │
 ┏━━━┓┃.                                 .┃┏━━━┓
 ┗━━━┛┃.     M  O  N  O  D  R  A  W      .┃┗━━━┛
  │ │ ┃.                                 .┃ │ │
  │ │ ┗━━━━━━///////////////////////━━━━━━┛ │ │
  │ │       ┃◦                     ◦┃       │ │
  │ │       ┃   ┏━━━━━━━━━━━━━━━┓   ┃       │ │
  │ │       ┃◦  ┃▤▤▤▤▤▤▤▤▤▤▤▤▤▤▤┃  ◦┃       │ │
  │ │       ┃   ┗━━━━━━━━━━━━━━━┛   ┃       │ │
┌──◎──┐     ┃◦                     ◦┃     ┌──◎──┐
│ ┌─┐ │ ┏━━━━━━━━━━━━┓◬◬◬◬◬┏━━━━━━━━━━━━┓ │ ┌─┐ │
└─┘ └─┘ ┃            ┃━━━━━┃            ┃ └─┘ └─┘
        ┃            ┃     ┃            ┃
        ┃            ┃     ┃            ┃
        ┃            ┃     ┃            ┃
        ┃            ┃     ┃            ┃
        ┃\\\\\\\\\\\\┃     ┃\\\\\\\\\\\\┃
       ┌──────────────┐   ┌──────────────┐
       │              │   │              │
       └──────────────┘   └──────────────┘
        ┃////////////┃     ┃////////////┃
        ┃            ┃     ┃            ┃
        ┃            ┃     ┃            ┃
        ┃            ┃     ┃            ┃
        ┃            ┃     ┃            ┃
      ┌────────────────┐ ┌────────────────┐
      │                │ │                │
      ┏━━━━━━━━━━━━━━━━┓ ┏━━━━━━━━━━━━━━━━┓
      ┗━━━━━━━━━━━━━━━━┛ ┗━━━━━━━━━━━━━━━━┛
  </pre>
  </div>
<div data-wow-duration="1.1s">
    <h2><img src="https://monodraw.helftone.com/static/images/finder-icon.png" width="44px">Designed for Mac</h2>
    <div><p>
      Monodraw is designed for the Mac from the ground up – everything from the text layout engine to the interface is made to take advantage of macOS. Like all native apps, it just works the way you expect. When you make a mistake, undo is always ready to come to the rescue. Exporting your text art could not get any easier – just copy and paste it into your favourite text editor. </p><p><a href="http://blog.helftone.com/ascii-art-unicode/">Not displaying correctly?</a></p></div>
  </div>
 </div>
<!-- # End Section  -->



<div id="section-faq">
  <p><strong>What are the system requirements?</strong></p>
  <div><p>The app requires macOS 11 Big Sur or later.</p><p>If you're running an older version of macOS, you can download <a href="https://updates.helftone.com/monodraw/downloads/Monodraw-b102.dmg">Monodraw v1.3</a> which requires macOS 10.10 Yosemite or <a href="https://updates.helftone.com/monodraw/downloads/Monodraw-b107.dmg">Monodraw v1.5</a> which requires macOS 10.14 Mojave.</p></div>

  <p><strong>Which versions include the command line tool?</strong></p>
  <p>
    Only the versions which you download directly from our website and purchase from <a href="https://sites.fastspring.com/helftone/product/monodraw">our store</a>. Due to restrictions imposed by the <a href="https://developer.apple.com/app-sandboxing/">App Sandbox</a> on the Mac App Store, the tool cannot be included there.
  </p>

  <!-- <p class="h3 fw300"><strong>How do I keep track of the latest news?</strong></p>
  <p class="mid-gray border-left p2">We're going to be posting updates on our <a href="http://blog.helftone.com/">blog</a> (<a href="http://blog.helftone.com/feed.xml">RSS feed</a>) and you should follow <a href="http://twitter.com/monodraw" title="Monodraw on Twitter" target="_blank">@Monodraw</a>. You can also subscribe to receive updates straight to your email inbox &mdash; scroll to the bottom of the page and sign up.</p> -->

  <p><strong>How do I provide feedback?</strong></p>
  <p>We would love to hear from you — the best way would be to drop us an <a href="mailto:monodraw@helftone.com?subject=Monodraw%20Feedback">email</a>. Alternatively, just tweet us <a href="http://twitter.com/monodraw" title="Monodraw on Twitter" target="_blank">@Monodraw</a>.</p>

  <p><strong>How are you going to use my email? I hate spam.</strong></p>
  <p>We hate spam, too. We would not share your email with any 3rd parties, period. We would only email you if we have important news about Monodraw and our upcoming products, that's it.</p>

  <p><strong>Do you have a Press Kit? </strong></p>
  <p>Of course — you can download it <a href="http://helftone-assets.s3.amazonaws.com/monodraw/Monodraw-Press-Kit.zip">from here</a>.</p>  


  <p><strong>Do you offer Educational Pricing?</strong></p>
  <p>Yes, we do — just <a href="mailto:support@helftone.com?subject=Educational%20Pricing">get in touch</a>.</p>

  <p><strong>Privacy Policy</strong></p>
  <p>We take your privacy very seriously. Monodraw does not collect any data whatsoever.</p>  

</div>




<!-- Popup itself -->













</div>]]></description>
        </item>
        <item>
            <title><![CDATA[4D Knit Dress (208 pts)]]></title>
            <link>https://selfassemblylab.mit.edu/4d-knit-dress</link>
            <guid>39651710</guid>
            <pubDate>Sat, 09 Mar 2024 14:19:19 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://selfassemblylab.mit.edu/4d-knit-dress">https://selfassemblylab.mit.edu/4d-knit-dress</a>, See on <a href="https://news.ycombinator.com/item?id=39651710">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-block-type="2" data-border-radii="{&quot;topLeft&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0},&quot;topRight&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0},&quot;bottomLeft&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0},&quot;bottomRight&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0}}" id="block-d17cc2c97847c0510e48">
  <p>4D Knit Dress: Transforming Style<br>MIT Self-Assembly Lab x Ministry of Supply</p><p>MIT Self-Assembly Lab Team:<br>Sasha Mckinlay, Danny Griffin, Sofia Chen, Lavender Tessmer, Natalie Pearl, Susan Williams, Agnes Parker, Jared Laucks, Skylar Tibbits</p><p>Ministry of Supply Team:<br>Jarlath Mellett, Alessandra Vasi, Ryan Connary, Gihan Amarasiriwardena</p><div><p>Typical garment construction requires a designer to build a 2D pattern, then cut and sew from 2D fabric – yielding excess waste, additional cost/labor and bulky seams that don’t always follow human anatomy. New innovation in 3D knitting – akin to 3D printing - has allowed fabric variation and standardized 3D shaping - however customized shaping of knitted garments to fit anyone’s unique body or style hasn’t been possible.</p><p>4D Knit Dress, combines several technologies – heat-activated yarns, computerized knitting and 6-axis robotic activation to create a garment that is sculpted to create a personalized fit or style. Heat-activated yarns are embedded within a unique knit structure to create controlled transformation, while maintaining softness, stretch and resilience. Using an efficient tubular knitting technique, a 6-axis robotic arm (commonly used in automotive manufacturing) heats specific areas to take-in – mimicking the design process of pinning &amp; tucking used in traditional dress tailoring – transforming the dress in real-time to create a perfect fit or a unique look.</p></div>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[What Cannot be Skipped About the Skiplist (109 pts)]]></title>
            <link>https://arxiv.org/abs/2403.04582</link>
            <guid>39651356</guid>
            <pubDate>Sat, 09 Mar 2024 13:03:16 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://arxiv.org/abs/2403.04582">https://arxiv.org/abs/2403.04582</a>, See on <a href="https://news.ycombinator.com/item?id=39651356">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="content-inner">
    
    
                
    <p><a href="https://arxiv.org/pdf/2403.04582">Download PDF</a></p><blockquote>
            <span>Abstract:</span>Skiplists have become prevalent in systems. The main advantages of skiplists are their simplicity and ease of implementation, and the ability to support operations in the same asymptotic complexities as their tree-based counterparts. In this survey, we explore skiplists and their many variants. We highlight many scenarios of how skiplists are useful and fit well in these usage scenarios. We study several extensions to skiplists to make them fit for more applications, e.g., their use in the multi-dimensional space, network overlaying algorithms, as well as serving as indexes in database systems. Besides, we also discuss systems that adopt the idea of skiplists and apply the probabilistic skip pattern into their designs.
    </blockquote>

    <!--CONTEXT-->
    
  </div><div>
      <h2>Submission history</h2><p> From: Lu Xing [<a href="https://arxiv.org/show-email/2724e7e4/2403.04582">view email</a>]      <br>    <strong>[v1]</strong>
        Thu, 7 Mar 2024 15:29:04 UTC (6,629 KB)<br>
</p></div></div>]]></description>
        </item>
    </channel>
</rss>