<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0">
    <channel>
        <title>HN100 - Readable Contents</title>
        <link>https://hn.algolia.com/api/v1/search_by_date?tags=%28story,poll%29&amp;numericFilters=points%3E100</link>
        <description>Uses Readability to add bodies to the RSS feed</description>
        <lastBuildDate>Tue, 22 Oct 2024 18:30:03 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>en</language>
        <item>
            <title><![CDATA[FTC's rule banning fake online reviews goes into effect (212 pts)]]></title>
            <link>https://abcnews.go.com/US/wireStory/ftcs-rule-banning-fake-online-reviews-effect-115009298</link>
            <guid>41915187</guid>
            <pubDate>Tue, 22 Oct 2024 15:21:58 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://abcnews.go.com/US/wireStory/ftcs-rule-banning-fake-online-reviews-effect-115009298">https://abcnews.go.com/US/wireStory/ftcs-rule-banning-fake-online-reviews-effect-115009298</a>, See on <a href="https://news.ycombinator.com/item?id=41915187">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-testid="prism-article-body"><p><span>WASHINGTON -- </span>A federal rule <a data-testid="prism-linkbase" href="https://apnews.com/article/small-business-fake-reviews-ftc-9858b1b84c2e54c70e55179b10cca3e5">banning fake online reviews</a> is now in effect. </p><p>The Federal Trade Commission <a data-testid="prism-linkbase" href="https://www.ftc.gov/news-events/news/press-releases/2024/08/federal-trade-commission-announces-final-rule-banning-fake-reviews-testimonials">issued the rule</a> in August banning the sale or purchase of online reviews. The rule, which went into effect Monday, allows the agency to seek civil penalties against those who knowingly violate it.</p><p>“Fake reviews not only waste people’s time and money, but also pollute the marketplace and divert business away from honest competitors,” FTC Chair Lina Khan said about the rule in August. She added that the rule will “protect Americans from getting cheated, put businesses that unlawfully game the system on notice, and promote markets that are fair, honest, and competitive.”</p><p>Specifically, the rule bans reviews and testimonials attributed to people who don’t exist or are generated by artificial intelligence, people who don’t have experience with the business or product/services, or misrepresent their experience.</p><p>It also bans businesses from creating or selling reviews or testimonials. Businesses that knowingly buy fake reviews, procure them from company insiders or disseminate fake reviews will be penalized. It also prohibits businesses from using “unfounded or groundless legal threats, physical threats, intimidation, or certain false public accusations.”</p><p>People can report violations at <a data-testid="prism-linkbase" href="https://t.co/k0O6juZImU">https://reportfraud.ftc.gov</a>.</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Computer use, a new Claude 3.5 Sonnet, and Claude 3.5 Haiku (522 pts)]]></title>
            <link>https://www.anthropic.com/news/3-5-models-and-computer-use</link>
            <guid>41914989</guid>
            <pubDate>Tue, 22 Oct 2024 15:02:46 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.anthropic.com/news/3-5-models-and-computer-use">https://www.anthropic.com/news/3-5-models-and-computer-use</a>, See on <a href="https://news.ycombinator.com/item?id=41914989">Hacker News</a></p>
<div id="readability-page-1" class="page"><article><div><p>Today, we’re announcing an <strong>upgraded Claude 3.5 Sonnet</strong>, and a new model, <strong>Claude 3.5 Haiku</strong>. The upgraded Claude 3.5 Sonnet delivers across-the-board improvements over its predecessor, with particularly significant gains in coding—an area where it already led the field. Claude 3.5 Haiku matches the performance of Claude 3 Opus, our prior largest model, on many evaluations for the same cost and similar speed to the previous generation of Haiku.</p><p>We’re also introducing a groundbreaking new capability in public beta: <strong>computer use</strong>. Available <a href="https://docs.anthropic.com/en/docs/build-with-claude/computer-use">today on the API</a>, developers can direct Claude to use computers the way people do—by looking at a screen, moving a cursor, clicking buttons, and typing text. Claude 3.5 Sonnet is the first frontier AI model to offer computer use in public beta. At this stage, it is still <a href="https://www.anthropic.com/news/developing-computer-use">experimental</a>—at times cumbersome and error-prone. We're releasing computer use early for feedback from developers, and expect the capability to improve rapidly over time.</p><p>Asana, Canva, Cognition, DoorDash, Replit, and The Browser Company have already begun to explore these possibilities, carrying out tasks that require dozens, and sometimes even hundreds, of steps to complete. For example, Replit is using Claude 3.5 Sonnet's capabilities with computer use and UI navigation to develop a key feature that evaluates apps as they’re being built for their Replit Agent product.</p><p>The upgraded Claude 3.5 Sonnet is now available for all users. Starting today, developers can build with the computer use beta on the Anthropic API, Amazon Bedrock, and Google Cloud’s Vertex AI. The new Claude 3.5 Haiku will be released later this month.</p><h3>Claude 3.5 Sonnet: Industry-leading software engineering skills</h3><p>The updated <a href="https://www.anthropic.com/claude/sonnet">Claude 3.5 Sonnet</a> shows wide-ranging improvements on industry benchmarks, with particularly strong gains in agentic coding and tool use tasks. On coding, it improves performance on <a href="https://www.swebench.com/">SWE-bench Verified</a> from 33.4% to 49.0%, scoring higher than all publicly available models—including reasoning models like OpenAI o1-preview and specialized systems designed for agentic coding. It also improves performance on <a href="https://github.com/sierra-research/tau-bench">TAU-bench</a>, an agentic tool use task, from 62.6% to 69.2% in the retail domain, and from 36.0% to 46.0% in the more challenging airline domain. The new Claude 3.5 Sonnet offers these advancements at the same price and speed as its predecessor.</p><p>Early customer feedback suggests the upgraded Claude 3.5 Sonnet represents a significant leap for AI-powered coding. GitLab, which tested the model for DevSecOps tasks, found it delivered stronger reasoning (up to 10% across use cases) with no added latency, making it an ideal choice to power multi-step software development processes. Cognition uses the new Claude 3.5 Sonnet for autonomous AI evaluations, and experienced substantial improvements in coding, planning, and problem-solving compared to the previous version. The Browser Company, in using the model for automating web-based workflows, noted Claude 3.5 Sonnet outperformed every model they’ve tested before.</p><p>As part of our continued effort to partner with external experts, joint pre-deployment testing of the new Claude 3.5 Sonnet model was conducted by the US AI Safety Institute (US AISI) and the UK Safety Institute (UK AISI).</p><p>We also evaluated the upgraded Claude 3.5 Sonnet for catastrophic risks and found that the ASL-2 Standard, as outlined in our <a href="https://www.anthropic.com/news/announcing-our-updated-responsible-scaling-policy">Responsible Scaling Policy</a>, remains appropriate for this model.</p><h3>Claude 3.5 Haiku: State-of-the-art meets affordability and speed</h3><p><a href="https://www.anthropic.com/claude/haiku">Claude 3.5 Haiku</a> is the next generation of our fastest model. For the same cost and similar speed to Claude 3 Haiku, Claude 3.5 Haiku improves across every skill set and surpasses even Claude 3 Opus, the largest model in our previous generation, on many intelligence benchmarks. Claude 3.5 Haiku is particularly strong on coding tasks. For example, it scores 40.6% on SWE-bench Verified, outperforming many agents using publicly available state-of-the-art models—including the original Claude 3.5 Sonnet and GPT-4o.</p><p>With low latency, improved instruction following, and more accurate tool use, Claude 3.5 Haiku is well suited for user-facing products, specialized sub-agent tasks, and generating personalized experiences from huge volumes of data—like purchase history, pricing, or inventory records.</p><p>Claude 3.5 Haiku will be made available later this month across our first-party API, Amazon Bedrock, and Google Cloud’s Vertex AI—initially as a text-only model and with image input to follow.</p><h3>Teaching Claude to navigate computers, responsibly</h3><p>With computer use, we're trying something fundamentally new. Instead of making specific tools to help Claude complete individual tasks, we're teaching it <em>general</em> computer skills—allowing it to use a wide range of standard tools and software programs designed for people. Developers can use this nascent capability to automate repetitive processes, <a href="https://www.youtube.com/watch?v=vH2f7cjXjKI">build and test software</a>, and <a href="https://youtu.be/jqx18KgIzAE">conduct open-ended tasks like research</a>.</p><p>To make these general skills possible, we've built an API that allows Claude to perceive and interact with computer interfaces. Developers can integrate this API to enable Claude to translate instructions (e.g., “use data from my computer and online to fill out this form”) into computer commands (e.g. check a spreadsheet; move the cursor to open a web browser; navigate to the relevant web pages; fill out a form with the data from those pages; and so on). On <a href="https://os-world.github.io/">OSWorld</a>, which evaluates AI models' ability to use computers like people do, Claude 3.5 Sonnet scored 14.9% in the screenshot-only category—notably better than the next-best AI system's score of 7.8%. When afforded more steps to complete the task, Claude scored 22.0%.</p><p>While we expect this capability to improve rapidly in the coming months, Claude's current ability to use computers is imperfect. Some actions that people perform effortlessly—scrolling, dragging, zooming—currently present challenges for Claude and we encourage developers to begin exploration with low-risk tasks. Because computer use may provide a new vector for more familiar threats such as spam, misinformation, or fraud, we're taking a proactive approach to promote its safe deployment. We've developed new classifiers that can identify when computer use is being used and whether harm is occurring. You can read more about the research process behind this new skill, along with further discussion of safety measures, in our post on <a href="http://anthropic.com/news/developing-computer-use">developing computer use</a>.</p><h3>Looking ahead</h3><p>Learning from the initial deployments of this technology, which is still in its earliest stages, will help us better understand both the potential and the implications of increasingly capable AI systems.</p><p>We’re excited for you to explore <a href="https://assets.anthropic.com/m/1cd9d098ac3e6467/original/Claude-3-Model-Card-October-Addendum.pdf">our new models</a> and the public beta of computer use—and welcome you to <a href="mailto:feedback@anthropic.com">share your feedback</a> with us. We believe these developments will open up new possibilities for how you work with Claude, and we look forward to seeing what you'll create.</p></div></article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Show HN: Rust Web Framework (232 pts)]]></title>
            <link>https://github.com/levkk/rwf</link>
            <guid>41914544</guid>
            <pubDate>Tue, 22 Oct 2024 14:15:54 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/levkk/rwf">https://github.com/levkk/rwf</a>, See on <a href="https://news.ycombinator.com/item?id=41914544">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><p dir="auto"><h2 tabindex="-1" dir="auto">Rwf ‐ Rust Web Framework</h2><a id="user-content-rwf--rust-web-framework" aria-label="Permalink: Rwf ‐ Rust Web Framework" href="#rwf--rust-web-framework"></a></p>
<p dir="auto">Rwf is a comprehensive framework for building web applications in Rust. Written using the classic MVC  pattern (model-view-controller), Rwf comes standard with everything you need to easily build fast and secure web apps.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Documentation</h2><a id="user-content-documentation" aria-label="Permalink: Documentation" href="#documentation"></a></p>
<p dir="auto">📘 The documentation <strong><a href="https://levkk.github.io/rwf/" rel="nofollow">is available here</a></strong>.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Features overview</h2><a id="user-content-features-overview" aria-label="Permalink: Features overview" href="#features-overview"></a></p>
<ul dir="auto">
<li>✔️ <a href="https://github.com/levkk/rwf/blob/main/examples/quick-start">HTTP server</a></li>
<li>✔️ User-friendly <a href="https://github.com/levkk/rwf/blob/main/examples/orm">ORM</a> to build PostgreSQL queries easily</li>
<li>✔️ <a href="https://github.com/levkk/rwf/blob/main/examples/dynamic-templates">Dynamic templates</a></li>
<li>✔️ <a href="https://github.com/levkk/rwf/blob/main/examples/auth">Authentication</a> &amp; built-in user sessions</li>
<li>✔️ <a href="https://github.com/levkk/rwf/blob/main/examples/middleware">Middleware</a></li>
<li>✔️ <a href="https://github.com/levkk/rwf/blob/main/examples/background-jobs">Background jobs</a> and <a href="https://github.com/levkk/rwf/blob/main/examples/scheduled-jobs">scheduled jobs</a></li>
<li>✔️ Database migrations</li>
<li>✔️ Built-in <a href="https://github.com/levkk/rwf/blob/main/examples/rest">REST framework</a> with JSON serialization</li>
<li>✔️ WebSockets support</li>
<li>✔️ <a href="https://github.com/levkk/rwf/blob/main/examples/static-files">Static files</a> hosting</li>
<li>✔️ Tight integration with <a href="https://turbo.hotwired.dev/" rel="nofollow">Hotwired Turbo</a> for building <a href="https://github.com/levkk/rwf/blob/main/examples/turbo">backend-driven SPAs</a></li>
<li>✔️ Environment-specific configuration</li>
<li>✔️ Logging and metrics</li>
<li>✔️ <a href="https://github.com/levkk/rwf/blob/main/rwf-cli">CLI</a></li>
<li>✔️ WSGI server for <a href="https://github.com/levkk/rwf/blob/main/examples/django">migrating</a> from Django/Flask apps</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Quick start</h2><a id="user-content-quick-start" aria-label="Permalink: Quick start" href="#quick-start"></a></p>
<p dir="auto">To add Rwf to your stack, create a Rust binary application and add <code>rwf</code> and <code>tokio</code> to your dependencies:</p>
<div dir="auto" data-snippet-clipboard-copy-content="cargo add rwf
cargo add tokio@1 --features full"><pre>cargo add rwf
cargo add tokio@1 --features full</pre></div>
<p dir="auto">Building an app is then as simple as:</p>
<div dir="auto" data-snippet-clipboard-copy-content="use rwf::prelude::*;
use rwf::http::Server;

#[derive(Default)]
struct IndexController;

#[async_trait]
impl Controller for IndexController {
    async fn handle(&amp;self, request: &amp;Request) -> Result<Response, Error> {
        Ok(Response::new().html(&quot;<h1>Hey Rwf!</h1>&quot;))
    }
}

#[tokio::main]
async fn main() {
    Server::new(vec![
        route!(&quot;/&quot; => IndexController),
    ])
    .launch(&quot;0.0.0.0:8000&quot;)
    .await
    .unwrap();
}"><pre><span>use</span> rwf<span>::</span>prelude<span>::</span><span>*</span><span>;</span>
<span>use</span> rwf<span>::</span>http<span>::</span><span>Server</span><span>;</span>

<span>#<span>[</span>derive<span>(</span><span>Default</span><span>)</span><span>]</span></span>
<span>struct</span> <span>IndexController</span><span>;</span>

<span>#<span>[</span>async_trait<span>]</span></span>
<span>impl</span> <span>Controller</span> <span>for</span> <span>IndexController</span> <span>{</span>
    <span>async</span> <span>fn</span> <span>handle</span><span>(</span><span>&amp;</span><span>self</span><span>,</span> <span>request</span><span>:</span> <span>&amp;</span><span>Request</span><span>)</span> -&gt; <span>Result</span><span>&lt;</span><span>Response</span><span>,</span> <span>Error</span><span>&gt;</span> <span>{</span>
        <span>Ok</span><span>(</span><span>Response</span><span>::</span><span>new</span><span>(</span><span>)</span><span>.</span><span>html</span><span>(</span><span>"&lt;h1&gt;Hey Rwf!&lt;/h1&gt;"</span><span>)</span><span>)</span>
    <span>}</span>
<span>}</span>

<span>#<span>[</span>tokio<span>::</span>main<span>]</span></span>
<span>async</span> <span>fn</span> <span>main</span><span>(</span><span>)</span> <span>{</span>
    <span>Server</span><span>::</span><span>new</span><span>(</span><span>vec</span><span>!</span><span>[</span>
        route!<span>(</span><span>"/"</span> =&gt; <span>IndexController</span><span>)</span>,
    <span>]</span><span>)</span>
    <span>.</span><span>launch</span><span>(</span><span>"0.0.0.0:8000"</span><span>)</span>
    <span>.</span><span>await</span>
    <span>.</span><span>unwrap</span><span>(</span><span>)</span><span>;</span>
<span>}</span></pre></div>
<p dir="auto"><h2 tabindex="-1" dir="auto">Examples</h2><a id="user-content-examples" aria-label="Permalink: Examples" href="#examples"></a></p>
<p dir="auto">See <a href="https://github.com/levkk/rwf/blob/main/examples">examples</a> for common use cases.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">🚧 Status 🚧</h2><a id="user-content-construction-status-construction" aria-label="Permalink: :construction: Status :construction:" href="#construction-status-construction"></a></p>
<p dir="auto">Rwf is in early development and not ready for production. Many features and documentation are incomplete. Contributions are welcome. Please see <a href="https://github.com/levkk/rwf/blob/main/CONTRIBUTING.md">CONTRIBUTING</a> for guidelines, <a href="https://github.com/levkk/rwf/blob/main/ARCHITECTURE.md">ARCHITECTURE</a> for a tour of the code, and <a href="https://github.com/levkk/rwf/blob/main/ROADMAP.md">ROADMAP</a> for a non-exhaustive list of desired features.</p>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Against /Tmp (139 pts)]]></title>
            <link>https://dotat.at/@/2024-10-22-tmp.html</link>
            <guid>41913610</guid>
            <pubDate>Tue, 22 Oct 2024 12:36:29 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://dotat.at/@/2024-10-22-tmp.html">https://dotat.at/@/2024-10-22-tmp.html</a>, See on <a href="https://news.ycombinator.com/item?id=41913610">Hacker News</a></p>
<div id="readability-page-1" class="page"><article>
  <p>I commented on Lobsters that <a href="https://lobste.rs/s/bzg0kq/how_do_you_deploy_10_seconds#c_ihmo1o"><code>/tmp</code> is usually a bad idea</a>,
which caused some surprise. I suppose <code>/tmp</code> security bugs were common
in the 1990s when I was learning Unix, but they are pretty rare now so
I can see why less grizzled hackers might not be familiar with the
problems.</p>
<p>I guess that’s some kind of success, but sadly the fixes have left
behind a lot of scar tissue because they didn’t address the underlying
problem: <code>/tmp</code> should not exist.</p>
<blockquote>
<p>It’s a bad idea because it’s shared global mutable state that
crosses security boundaries. There’s a ton of complexity at all
levels of unix (filesystems, kernel APIs, libc, shell, admin
scripts) that only exists as a workaround for the dangers caused by
making <code>/tmp</code> shared.</p>
</blockquote>
<h2><a name="sticky-bit" href="#sticky-bit">sticky bit</a></h2>
<p>I think the earliest and lowest-level workaround is the sticky bit.</p>
<p>The sticky bit is mode bit 01000 in <a href="https://pubs.opengroup.org/onlinepubs/9799919799/functions/chmod.html">unix file permissions</a>. It
is printed as the <code>t</code> instead of <code>x</code> in <code>rwt</code>.</p>
<pre><code>    drwxrwxrwt  5 root  wheel  160 Oct 22 10:22 /tmp/
</code></pre>
<p>Originally in the 1970s the sticky bit was invented to speed up
frequently-used programs such as the shell: the sticky bit indicated
to the kernel that the executable file should stick in core. This
functionality was made obsolete in the 1980s by filesystem page
caches.</p>
<p>The sticky bit was re-used to mean something else on directories, to
fix a security problem with <code>/tmp</code>.</p>
<p>On unix, permission to delete a file depends on write access to the
directory containing the file, independent of the file’s own
permissions.</p>
<p>So if a directory (such as <code>/tmp</code>) is world-writable, anyone can delete
any file in it.</p>
<p>This means <code>/tmp</code> was vulnerable to all sorts of accidental or malicious
trouble caused by users deleting each others’ files.</p>
<p>To fix this, <a href="https://man.freebsd.org/cgi/man.cgi?query=sticky">a file in a sticky directory may only be removed or
renamed by a user if the user has write permission for the directory
and the user is the owner of the file, the owner of the directory, or
the super-user</a>.</p>
<h2><a name="tmp-stupidity-in-c" href="#tmp-stupidity-in-c">tmp stupidity in C</a></h2>
<p>POSIX used to provide 5 (five!) ways to create a temporary file, three
of which (most of them!) you must never use and which have
subsequently been deprecated – except the do-not-use footgun <code>tmpnam</code>
which is still part of the C standard. Two more safe functions have
subsequently been added to support more complicated situations.</p>
<ul>
<li>bad: <a href="https://pubs.opengroup.org/onlinepubs/007908799/xsh/mktemp.html">mktemp</a>, <a href="https://pubs.opengroup.org/onlinepubs/007908799/xsh/tempnam.html">tempnam</a>, <a href="https://pubs.opengroup.org/onlinepubs/007908799/xsh/tmpnam.html">tmpnam</a></li>
<li>ok: <a href="https://pubs.opengroup.org/onlinepubs/007908799/xsh/mkstemp.html">mkstemp</a>, <a href="https://pubs.opengroup.org/onlinepubs/007908799/xsh/tmpfile.html">tmpfile</a></li>
<li>new: <a href="https://pubs.opengroup.org/onlinepubs/9799919799/functions/mkstemp.html">mkdtemp, mkostemp, mkstemp</a></li>
</ul>
<p>There are so many dangerous API design problems in these functions!
I’m not going to waste time roasting them in detail because I can
cover the important points by discussing …</p>
<h2><a name="mkstemp-and-mkdtemp" href="#mkstemp-and-mkdtemp">mkstemp and mkdtemp</a></h2>
<p>The purpose of all these functions is to create a temporary file (or
directory) that doesn’t collide with other concurrent activity.</p>
<p>When you are creating a file in <code>/tmp</code> the risk is that another
malicious user can make you open a file under their control. To avoid
this vulnerability, you (or rather <code>mkstemp</code>) must:</p>
<ul>
<li>
<p>Generate an <em>unpredictable</em> filename.</p>
<p>It isn’t enough for the filename to be unique, it must contain
sufficient <em>secure</em> randomness that an adversary can’t win a race
and interfere.</p>
</li>
<li>
<p>Open the file with <code>O_CREAT</code> | <code>O_EXCL</code>.</p>
<p>This ensures the file has not already been created by another
friendly or malicious process, without any time-of-check /
time-of-use vulnerabilities. And it ensures that the file isn’t a
symlink pointing somewhere an attacker wants you to accidentally
corrupt.</p>
</li>
<li>
<p>Retry if open fails with <code>EEXIST</code>.</p>
<p>Together with the randomness in the name, this avoids
denial-of-service vulnerabilites.</p>
</li>
</ul>
<p>This is intricate and not entirely obvious, as you can tell from all
the previous failed attempts at functions that didn’t create temporary
files safely.</p>
<h2><a name="mktemp-in-shell" href="#mktemp-in-shell">mktemp in shell</a></h2>
<p>The <a href="https://man.freebsd.org/cgi/man.cgi?query=mktemp">mktemp(1)</a> command is a wrapper around mkstemp(3). (Slightly
confusingly it isn’t a wrapper around mktemp(3) because that would be
unsafe.) It was introduced by OpenBSD and it’s widely supported though
it isn’t yet in POSIX. Its manual page includes a nice rationale:</p>
<blockquote>
<p>The <code>mktemp</code> utility is provided to allow shell scripts to safely use
temporary files. Traditionally, many shell scripts take the name of
the program with the pid as a suffix and use that as a temporary
file name. This kind of naming scheme is predictable and the race
condition it creates is easy for an attacker to win. A safer, though
still inferior, approach is to make a temporary directory using
the same naming scheme. While this does allow one to guarantee that
a temporary file will not be subverted, it still allows a simple
denial of service attack. For these reasons it is suggested that
<code>mktemp</code> be used instead.</p>
</blockquote>
<p>Although shell scripts can’t avoid using the temporary file by name,
mktemp(1) is safe because it creates the file securely and an attacker
can’t interfere with it after that point. It’s OK to re-open a file
that you know is yours.</p>
<h2><a name="tmp-cleanup" href="#tmp-cleanup">tmp cleanup</a></h2>
<p>The last item on my list of regrets is “admin scripts”, an oblique
reference to <code>/tmp</code> cleanup jobs.</p>
<p>By its nature <code>/tmp</code> tends to accumulate junk, so it was common to have
cron jobs that would delete old files. (Less common now that computers
are much bigger.)</p>
<p>These scripts tended to have problems with time-of-check / time-of-use
vulnerabilities, careless handling of symlinks, and pulling the rug
out from under long-running programs that foolishly used <code>/tmp</code>. (Lots
more reasons these scripts are now less common!)</p>
<h2><a name="tmp-remedy" href="#tmp-remedy">tmp remedy</a></h2>
<p>So I’ve spent dozens of paragraphs outlining bugs and complications
related to <code>/tmp</code>. All of them could have been avoided if <code>/tmp</code> did not
exist, and everything would have been simpler as a result.</p>
<p>So where should temporary files go, if not in <code>/tmp</code>?</p>
<p>There should be per-user temporary directories. In fact, on modern
systems there <em>are</em> per-user temporary directories! But this solution
came several decades too late.</p>
<p>If you have per-user <code>$TMPDIR</code> then temporary filenames can safely be
created using the simple mechanisms described in the mktemp(1)
rationale or used by the old deprecated C functions. There’s no need
to defend against an attacker who doesn’t have sufficient access to
mount an attack! There’s no need for sticky directories because there
aren’t any world-writable directories.</p>
<p>There’s a minor wrinkle that setuid programs would have to be more
careful about how they create temporary files, but setuid programs
have to be more careful about everything.</p>
<h2><a name="tmp-rationale" href="#tmp-rationale">tmp rationale</a></h2>
<p>So why wasn’t per-user <code>$TMPDIR</code> a thing back in the day?</p>
<p>Probably the main reason was path-dependence: <code>/tmp</code> was created and in
wide use before its problems became apparent, at which point it was
difficult to deprecate.</p>
<p>There are reasons 1990-ish-you didn’t want <code>$TMPDIR</code> to be in your
home directory:</p>
<ul>
<li>
<p><code>$HOME</code> might be on NFS so a local <code>$TMPDIR</code> might be faster</p>
</li>
<li>
<p><code>$TMPDIR</code> can be a way to get workspace beyond your disk quota</p>
</li>
<li>
<p><code>$HOME</code> might be on a filesystem that doesn’t support named pipes,
so your X11 and SSH agent sockets live in <code>$TMPDIR</code> instead</p>
</li>
</ul>
<p>The fix, way back when, should have been for login(8) to create a
per-user temporary directory in a sensible place before it drops
privilege, and set <code>$TMPDIR</code> so the user’s shell and child processes
can find it.</p>
<p>Oh well, think of all the excitement we would have missed if insecure
temporary file vulnerabilites had not been a thing!</p>

</article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Tog's Paradox (144 pts)]]></title>
            <link>https://www.votito.com/methods/togs-paradox/</link>
            <guid>41913437</guid>
            <pubDate>Tue, 22 Oct 2024 12:05:41 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.votito.com/methods/togs-paradox/">https://www.votito.com/methods/togs-paradox/</a>, See on <a href="https://news.ycombinator.com/item?id=41913437">Hacker News</a></p>
<div id="readability-page-1" class="page"><div dir="auto">
	
	<p><em>Tog’s Paradox</em> (also known as <em>The Complexity Paradox</em> or <em>Tog’s Complexity
Paradox</em>) is an observation that products aiming to simplify a task for users tend to
inspire new, more complex tasks. It’s one of the key reasons for the
symptom of requirements changing after delivery in enterprise software
products, and for feature creep in consumer products. Tog’s Paradox also
explains why it’s futile to try to completely nail down requirements for a
software product, as the product itself will have an impact on the users,
causing them to demand new functions.</p>

<blockquote>
  <p>[when] we reduce the complexity people experience in a given task, people will take on a more challenging task.</p>

  <p>– Bruce Tognazzini, <a rel="nofollow noopener noreferrer" href="https://www.asktog.com/columns/011complexity.html">The Complexity Paradox</a></p>
</blockquote>

<ul id="markdown-toc">
  <li><a href="#loophole-in-teslers-law" id="markdown-toc-loophole-in-teslers-law">Loophole in Tesler’s law</a></li>
  <li><a href="#examples-of-togs-paradox" id="markdown-toc-examples-of-togs-paradox">Examples of Tog’s Paradox</a></li>
  <li><a href="#togs-paradox-explains-why-requirements-always-change" id="markdown-toc-togs-paradox-explains-why-requirements-always-change">Tog’s paradox explains why requirements always change</a></li>
</ul>

<h2 id="loophole-in-teslers-law">Loophole in Tesler’s law</h2>

<p>Bruce Tognazzini formulated the paradox as a loophole in Tesler’s Law, which
states that inherent complexity in a user task does not change (but it can be
shifted from a user to an application or the other way around). Tognazzini
suggested instead that the task complexity does not stay the same, but increases.</p>

<figure>
<picture><source type="image/webp" srcset="https://www.votito.com/assets/20241005/togs-paradox.webp">
<img loading="lazy" src="https://www.votito.com/assets/20241005/togs-paradox.png" width="1340" height="1500" alt="Tog's Paradox" title="Tog's Paradox"></picture>
<figcaption>A product trying to simplify user tasks invariantly makes users want to perform more complex tasks</figcaption></figure>

<p>The argument follows Tognazzini’s previous observation, called <em>Tog’s Law of
Commuting</em> (published in the 1995 book <a rel="nofollow noopener noreferrer" href="https://amzn.to/3YzNnjT">Tog on Software Design</a>),
which suggests that users have a specific amount of time to complete a task,
and if they can finish the work sooner, they will take on more work to fill the
available time.</p>

<blockquote>
  <p>People will strive to experience an equal or increasing level of complexity in their lives no matter what is done to reduce it.</p>

  <p>– Bruce Tognazzini, <a rel="nofollow noopener noreferrer" href="https://www.asktog.com/columns/011complexity.html">The Complexity Paradox</a></p>
</blockquote>

<p>In spirit, Tog’s Paradox is similar to Jevon’s Paradox (which loosely states
that technological advances which improve the efficiency of using a resource
tend to lead to increased demand) and Parkinson’s law (which states that work
expands so to fill the time available for its completion).</p>

<h2 id="examples-of-togs-paradox">Examples of Tog’s Paradox</h2>

<p>Tog’s Paradox is highly visible in enterprise software development, where
attempts to streamline workflows often lead to more complex requirements over
time. For instance, a CRM system designed to automate customer interactions
might initially focus on basic functions like managing contacts and tracking
communication history. Once users experience the efficiency gains from these
core features, they begin requesting more sophisticated tools—such as
integrations with other software, advanced reporting, or analytics to
further optimize their work. Each new feature brings added complexity to the
system, requiring not only more robust infrastructure but also additional
training and support. This mirrors Tognazzini’s idea that making tasks more
efficient encourages the demand for additional use cases, driving the
complexity of the software.</p>

<p>Tog’s observation is also evident when software increases user productivity but
leads to expanded scope in responsibilities. Consider an HR platform that
automates payroll and performance management, freeing up HR staff from routine
tasks. HR teams will need to justify what they do the rest of the time, and may
use this newfound capacity to take on more strategic roles—such as employee
engagement or talent development initiatives—which eventually demands
additional software functionalities. The software that initially saved time
ends up accommodating these new, more complex tasks, reinforcing the idea that
saved time often gets filled with more work, creating an ongoing cycle of
increasing complexity. This phenomenon is common in enterprise software, where
solving one problem often leads to the creation of new, more intricate
challenges.</p>

<p>The law also plays out in consumer applications. Social media platforms like
Instagram or Twitter/X are another clear example. Initially designed to provide
simple ways to share photos or short messages, these platforms quickly expanded
as users sought additional capabilities, such as live streaming, integrated
shopping, or augmented reality filters. Each of these features added new layers
of complexity to the app, requiring more sophisticated algorithms, larger
databases, and increased development efforts. What began as a relatively
straightforward tool for sharing personal content has transformed into a
multi-faceted platform requiring constant updates to handle new features and
growing user expectations. Tog’s Paradox is evident here, as simplifying one
aspect of the communication and sharing often drives demand for other, more
complex ways of sharing content.</p>

<h2 id="togs-paradox-explains-why-requirements-always-change">Tog’s paradox explains why requirements always change</h2>

<p>Tog’s Paradox reveals why attempts to finalize design requirements are often doomed to fail. The moment a product begins to
solve its users’ core problems efficiently, it sparks a natural progression of
second-order effects. As users save time and effort, they inevitably find new,
more complex tasks to address, leading to feature requests that expand the
scope far beyond what was initially anticipated. This cycle shows that the
product itself actively influences users’ expectations and demands, making it
nearly impossible to fully define design requirements upfront.</p>

<p>This evolving complexity highlights the futility of attempting to lock down
requirements before the product is deployed. No matter how thorough the initial
planning, the reality is that users’ experiences with the product will inspire
new use cases and deeper needs. The product’s initial efficiency gains create a
feedback loop, where users seek more capabilities and push for new features and
workflows. These second-order effects suggest that software development must
embrace flexibility and continuous iteration.</p>

<p>Ultimately, Tog’s Paradox underscores that requirements gathering and user
research are not one-time events, but part of an ongoing process. Attempting to
fully specify requirements beforehand fails to account for how the product
itself will influence user behavior and expectations. To truly meet users’
needs, products must be built with the understanding that the needs will change
due to user interactions with those same products, the complexity will grow, and user
demands will evolve, often in unpredictable ways. Recognizing this dynamic is
key to building successful products that remain responsive and relevant over
time.</p>

<p>Tog’s Paradox has significant implications for user experience design,
particularly in how complexity impacts usability and user satisfaction over
time. As products evolve and add features in response to user demands, the
original simplicity and ease of use can degrade, leading to a more complex and
sometimes overwhelming interface. This presents a core UX challenge: balancing
the need for added functionality with maintaining an intuitive and accessible
user experience.</p>

<p><a rel="nofollow noopener noreferrer" href="https://news.ycombinator.com/item?id=41913437">Comments/Discussion on HN</a></p>
 



<h2>Learn more about the Tog's paradox</h2>
<ul>

	<li>
	
	<a rel="nofollow noopener noreferrer" href="https://www.asktog.com/columns/011complexity.html">The Complexity Paradox</a> by Bruce Tognazzini (1998)
	
	</li>

	<li>
	
	<a rel="nofollow noopener noreferrer" href="https://amzn.to/3YzNnjT">Tog on Software Design</a>, ISBN 978-0201489170 (1995)
	
	</li>

</ul>







<hr>

<nav>
	<span><b>Next article</b>: <a href="https://www.votito.com/methods/umux/">Usability Metric for User Experience</a></span>
	<span><a href="https://www.votito.com/methods/" rel="nofollow">All articles</a></span>
</nav>



	
	
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[MQTT turns 25 (217 pts)]]></title>
            <link>https://andypiper.co.uk/2024/10/22/mqtt-turns-25-heres-how-it-has-endured/</link>
            <guid>41912787</guid>
            <pubDate>Tue, 22 Oct 2024 10:01:24 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://andypiper.co.uk/2024/10/22/mqtt-turns-25-heres-how-it-has-endured/">https://andypiper.co.uk/2024/10/22/mqtt-turns-25-heres-how-it-has-endured/</a>, See on <a href="https://news.ycombinator.com/item?id=41912787">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
<p>It’s October 2024 and I’m sitting here in my creative maker studio, wearing a bright t-shirt that excitedly bellows “MQTT 25”! To my left is a top-end Bambu Lab X1C 3D printer, that uses MQTT internally for communication. On my wall are a variety of connected gadgets that display data or that light up in response to <a href="https://mqtt.org/">MQTT</a> notifications. <a href="https://nodered.org/" data-type="link" data-id="https://nodered.org/">Node-RED</a> is sitting quietly on a Raspberry Pi in the corner, processing MQTT messages as they come and go.</p>



<p>Today is the 25th anniversary of the publication of what <a href="https://stanford-clark.com/MQIpdp/" data-type="link" data-id="https://stanford-clark.com/MQIpdp/">would become the initial MQTT specification</a>.</p>



<figure><img data-recalc-dims="1" decoding="async" width="400" height="385" data-attachment-id="5868" data-permalink="https://andypiper.co.uk/2024/10/22/mqtt-turns-25-heres-how-it-has-endured/screenshot-2024-10-22-at-09-40-46/" data-orig-file="https://i0.wp.com/andypiper.co.uk/wp-content/uploads/2024/10/Screenshot-2024-10-22-at-09.40.46.png?fit=2076%2C2000&amp;ssl=1" data-orig-size="2076,2000" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="Screenshot 2024-10-22 at 09.40.46" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/andypiper.co.uk/wp-content/uploads/2024/10/Screenshot-2024-10-22-at-09.40.46.png?fit=300%2C289&amp;ssl=1" data-large-file="https://i0.wp.com/andypiper.co.uk/wp-content/uploads/2024/10/Screenshot-2024-10-22-at-09.40.46.png?fit=400%2C385&amp;ssl=1" tabindex="0" role="button" src="https://i0.wp.com/andypiper.co.uk/wp-content/uploads/2024/10/Screenshot-2024-10-22-at-09.40.46.png?resize=400%2C385&amp;ssl=1" alt="A picture of Andy Piper sitting in a maker studio, surrounded by virtual balloons. He is wearing a yellow coloured t-shirt with the text MQTT 25 in purple text." srcset="https://i0.wp.com/andypiper.co.uk/wp-content/uploads/2024/10/Screenshot-2024-10-22-at-09.40.46.png?resize=400%2C385&amp;ssl=1 400w, https://i0.wp.com/andypiper.co.uk/wp-content/uploads/2024/10/Screenshot-2024-10-22-at-09.40.46.png?resize=300%2C289&amp;ssl=1 300w, https://i0.wp.com/andypiper.co.uk/wp-content/uploads/2024/10/Screenshot-2024-10-22-at-09.40.46.png?resize=150%2C145&amp;ssl=1 150w, https://i0.wp.com/andypiper.co.uk/wp-content/uploads/2024/10/Screenshot-2024-10-22-at-09.40.46.png?resize=768%2C740&amp;ssl=1 768w, https://i0.wp.com/andypiper.co.uk/wp-content/uploads/2024/10/Screenshot-2024-10-22-at-09.40.46.png?resize=1536%2C1480&amp;ssl=1 1536w, https://i0.wp.com/andypiper.co.uk/wp-content/uploads/2024/10/Screenshot-2024-10-22-at-09.40.46.png?resize=2048%2C1973&amp;ssl=1 2048w, https://i0.wp.com/andypiper.co.uk/wp-content/uploads/2024/10/Screenshot-2024-10-22-at-09.40.46.png?resize=1200%2C1156&amp;ssl=1 1200w" sizes="(max-width: 400px) 100vw, 400px"></figure>



<p>The co-creator of MQTT is my good friend <a href="https://stanford-clark.com/">Andy Stanford-Clark</a>, who <a href="https://mastodon.iow.social/@andysc/113349599886653118" data-type="link" data-id="https://mastodon.iow.social/@andysc/113349599886653118">announced the event</a> on Mastodon:</p>


    <blockquote data-instance="mastodon.iow.social" data-post-id="113349599886653118">
        <p>Happy Birthday, <a href="https://mastodon.iow.social/tags/MQTT" rel="tag">#<span>MQTT</span></a>!<br>25 today 🙂 xxx</p>
        
    </blockquote>


<p>I’m not going to post a complete history of the past two (plus) decades of this technology, but for those just joining… what the heck is MQTT, and… how did I come to be involved?</p>



<h3>Connecting things</h3>



<p>Here’s the tl;dr – MQTT is a network protocol that was originally designed to enable small devices on lightweight or patchy networks (we’re going back to the late 1990s, remember!) to publish and collect / receive data. Say you’re an environmental monitoring device in a far-flung area where there’s occasional network coverage, and you have limited power available – it’s important that you use power and network bandwidth and availability efficiently, to send sensor information (in a minimal, but useful, format) to a larger system. MQTT is a <em>great</em> fit here. It turns out that a highly optimised and efficient protocol like this, also scales up extremely well. As networks got better (faster, more stable, and more widespread), and as we moved through a period of greater access to efficient computing devices for edge-of-network, home automation, and in-your-pocket use cases, MQTT remained highly valuable. The simplicity of the protocol is very powerful.</p>



<h3>What’s my connection?</h3>



<p>In 2001 I got my second full-time job after university, and joined IBM as an IT Specialist – a consultant working with IBM software, primarily on-site with their customers, implementing what we used to call business integration, message queueing, application connectivity, middleware etc.</p>



<p>Within a few years I was pretty experienced within the IBM middleware portfolio – I’d been helping to implement banking payment systems and other projects using “full size” IBM MQ. Around that time, IBM was starting a marketing push around something they would <a href="https://andypiper.co.uk/2010/08/05/mqtt-the-smarter-planet-protocol/" data-type="link" data-id="https://andypiper.co.uk/2010/08/05/mqtt-the-smarter-planet-protocol/">ultimately call Smarter Planet</a>. I’d gravitated towards IBM’s fantastic Hursley Lab as an engineering hub in the UK, the home of MQ and also, the base of Andy Stanford-Clark, who was one of my mentors. A bunch of us from there started to hack with this MQTT thing, which was at that time externally published as a protocol, but little-known or implemented outside of IBM. I became something of an accidental advocate for MQTT, and looking back now, I count that as my first “developer relations / developer advocacy” role, even though it was informal and my day job was something different<sup data-fn="a4ddc54c-fc92-4602-8814-8d2f1f904878"><a href="#a4ddc54c-fc92-4602-8814-8d2f1f904878" id="a4ddc54c-fc92-4602-8814-8d2f1f904878-link">1</a></sup>.</p>



<p>Looking back in this blog, I was <a href="https://andypiper.co.uk/tag/mqtt/" data-type="link" data-id="https://andypiper.co.uk/tag/mqtt/">posting about MQTT</a> regularly back through ~2009-2011, which was really the period where we started to make progress in socialising the protocol beyond smaller IBM implementations. We went from having a small number of message brokers – the enterprise and very expensive IBM WebSphere Message Broker, and the excellent but closed-source microbroker and, also closed-source but freely-available Really Small Message Broker from the labs – to <a href="https://fosstodon.org/@ralight">Roger Light</a>‘s creation of the Open Source <a href="https://mosquitto.org/">Mosquitto</a>, which remains one of the more widely-used free implementations out there<sup data-fn="b505d264-351c-4964-b722-476750d44bf3"><a href="#b505d264-351c-4964-b722-476750d44bf3" id="b505d264-351c-4964-b722-476750d44bf3-link">2</a></sup>. I was one of the folks who had the keys to the MQTT Twitter account and community website, and one of my goals as developer advocate was <a href="https://www.youtube.com/playlist?list=PL950E08D350673410" data-type="link" data-id="https://www.youtube.com/playlist?list=PL950E08D350673410">sharing</a> and promoting all of the cool ways that folks were using the protocol<sup data-fn="03445776-7e30-4460-9d06-13f0857337db"><a href="#03445776-7e30-4460-9d06-13f0857337db" id="03445776-7e30-4460-9d06-13f0857337db-link">3</a></sup>.</p>



<p>In 2011 I was <a href="https://andypiper.co.uk/2012/03/10/paho-gets-started/" data-type="link" data-id="https://andypiper.co.uk/2012/03/10/paho-gets-started/">heavily involved</a> in <a href="https://andypiper.co.uk/2011/11/04/mqtt-goes-free-a-personal-qa/" data-type="link" data-id="https://andypiper.co.uk/2011/11/04/mqtt-goes-free-a-personal-qa/">IBM’s donation of its MQTT implementations to the Eclipse community</a>, as the Eclipse Paho project. After I left IBM in 2012, I continued to have a strong connection, and I played a role on the Paho project through my next job at Cloud Foundry; but, after I joined Twitter in 2014 I needed to step back from formal involvement. That was the time at which MQTT went through formal standardisation, at OASIS and ISO/IEC.</p>



<h3>Success and growth</h3>



<p>It is not my place or part in the story to talk in depth about the different companies that have thrived in the past 15 years, and helped to make MQTT as ubiquitous as it has become, but it is truly one of my most proud personal achievements, helping this technology grow to beyond the walls of IBM – into an open protocol success story. Today, 25 years on, it is in many things and places you may not realise – hobbyists and makers use it, it’s used in (for example) Dyson’s air filters and their associated apps, in 3D printer control systems, in home alerting, and across industry and manufactuing. It’s almost certain that more than one of the apps on your phone right now, is using MQTT somewhere in the stack.</p>



<hr>



<p>Andy Stanford-Clark recently did a “fireside chat” with our friends at HiveMQ. This is worth a look, and a much better place to learn more. HiveMQ also have a podcast series called <a href="https://theunstructuredmessage.buzzsprout.com/" data-type="link" data-id="https://theunstructuredmessage.buzzsprout.com/">The Unstructured Message</a> that you can subscribe to for more!</p>



<figure><p>
<iframe title="Revolutionizing Connectivity: Powerful MQTT Use Cases with Co-Inventor Andy Stanford-Clark" width="500" height="281" src="https://www.youtube.com/embed/JYYo7ycQLu4?feature=oembed" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen=""></iframe>
</p></figure>



<hr>



<h3>A small (but timely) update</h3>



<p>As a small 25th birthday present to the community from me, I thought it was beyond time to forget about the old project account over on X<sup data-fn="bcbb5587-918d-40af-841e-1e765dc53fcf"><a href="#bcbb5587-918d-40af-841e-1e765dc53fcf" id="bcbb5587-918d-40af-841e-1e765dc53fcf-link">4</a></sup>, and move us to a similarly open protocol and standards-based platform – Mastodon!</p>



<p>You can now follow <code><a href="https://fosstodon.org/@mqtt" data-type="link" data-id="https://fosstodon.org/@mqtt">@mqtt@fosstodon.org</a></code>!</p>


    <blockquote data-instance="fosstodon.org" data-post-id="113350454310453855">
        <p>It feels like a long time, but also only yesterday – to celebrate our 25th birthday, we've joined the open social web. This is our first message posted on the Fediverse via ActivityPub!</p>
        <p>
            — MQTT (<a rel="mention" href="https://fosstodon.org/@mqtt">@<span>mqtt</span></a>) 
            <a href="https://fosstodon.org/@mqtt/113350454310453855">2024-10-22T09:51:12.460Z</a>
        </p>
    </blockquote>


<p>Here’s to the next 25 years (or more) of MQTT 🎉</p>



<p>Thanks to everyone – developers, users, enthusiasts across the community – for your support!</p>



<hr>


<ol><li id="a4ddc54c-fc92-4602-8814-8d2f1f904878">One year, this cost me a bad PBC rating – I’d spent too much time on the fun community stuff over my client focus; early career lesson learned. <a href="#a4ddc54c-fc92-4602-8814-8d2f1f904878-link" aria-label="Jump to footnote reference 1">↩︎</a></li><li id="b505d264-351c-4964-b722-476750d44bf3">Roger made mosquitto after hearing Andy Stanford-Clark talk about his connected smart home at the very first OggCamp, in 2009; 10 years from the date the specification was created. <a href="#b505d264-351c-4964-b722-476750d44bf3-link" aria-label="Jump to footnote reference 2">↩︎</a></li><li id="03445776-7e30-4460-9d06-13f0857337db">Weirdly, one of my most popular YouTube videos remains <a href="https://youtu.be/jI-0b6XMM5E" data-type="link" data-id="https://youtu.be/jI-0b6XMM5E">a 2009 clip of using MQTT and PHP together</a>. It’s 15 years old! <a href="#03445776-7e30-4460-9d06-13f0857337db-link" aria-label="Jump to footnote reference 3">↩︎</a></li><li id="bcbb5587-918d-40af-841e-1e765dc53fcf">If you are not off X already, <a href="https://macaw.social/@andypiper/113330414080901624" data-type="link" data-id="https://macaw.social/@andypiper/113330414080901624"><em>please</em> get away from there</a>. <a href="#bcbb5587-918d-40af-841e-1e765dc53fcf-link" aria-label="Jump to footnote reference 4">↩︎</a></li></ol>






</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Don't Publish with IEEE (226 pts)]]></title>
            <link>http://cr.yp.to/writing/ieee.html</link>
            <guid>41912625</guid>
            <pubDate>Tue, 22 Oct 2024 09:27:46 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="http://cr.yp.to/writing/ieee.html">http://cr.yp.to/writing/ieee.html</a>, See on <a href="https://news.ycombinator.com/item?id=41912625">Hacker News</a></p>
<div id="readability-page-1" class="page">
<a href="http://cr.yp.to/djb.html">D. J. Bernstein</a>
<br><a href="http://cr.yp.to/writing.html">Notes on writing papers</a>
<h2>Don't publish with IEEE!</h2>
Before you read this page,
you should understand
(1) authors putting papers online to benefit readers,
(2) commercial publishers using copyright to limit #1,
and
(3) authors dedicating papers to the public domain as one way to stop #2.
I have a
<a href="http://cr.yp.to/bib/online.html">separate page</a>
discussing these issues.
<p>
It turns out that, in response to #3,
IEEE is overriding its scientific referees
and flat-out refusing to accept public-domain papers.
</p><p>
I learned about this from a UIC graduate student
who had submitted a paper to a conference
whose proceedings were to be published by IEEE.
After the paper was accepted,
IEEE notified the student that a copyright transfer was required.
The student declared his intention to put his paper into the public domain.
The IEEE Intellectual Property Rights Manager
threatened the student with non-publication of the paper.
Faced with this pressure,
the student capitulated and, rather than eliminating the copyright,
transferred it to IEEE.
</p><p>
When I heard about this incident,
I asked the IEEE Intellectual Property Rights Manager
to explain his overall assertion
that IEEE refuses to publish public-domain papers:
</p><blockquote>
Works from government authors are in the public domain, and I find it
difficult to believe that IEEE refuses papers from government authors. I
see that the IEEE Copyright Form has a special section for government
authors.
<p>
How many public-domain papers does IEEE actually publish? Surely you
have the exact figures for each year. Is it fair to say that, in fact,
IEEE publishes many public-domain papers?
</p></blockquote>
The IEEE Intellectual Property Rights Manager
refused to answer these questions.
<p>
In his messages to the student,
the IEEE Intellectual Property Rights Manager
had asserted that ``IEEE needs to be the owner of the work ... by assignment.''
Obviously this is impossible for papers in the public domain:
copyright assignment isn't possible when the copyright no longer exists.
I looked at the IEEE Copyright Policies
and found that public-domain papers were clearly exempted
from the copyright-transfer requirement:
</p><blockquote>
Such transfer shall be a necessary requirement for publication,
EXCEPT for material in the public domain.
</blockquote>
(Emphasis added.)
I asked the IEEE Intellectual Property Rights Manager
to explain the contradiction between his statements
and the IEEE Copyright Policies.
His only response was the rather idiotic comment that
``IEEE policy requires authors to submit an IEEE Copyright Form
in order for publication to occur'';
needless to say, the IEEE Copyright Form
is written by the IEEE Intellectual Property Rights office.
<p>
In his messages to the student,
the IEEE Intellectual Property Rights Manager
had explained IEEE's alleged need for being ``the owner of the work''
as follows:
``We can put it into the Xplore database and license it to others as one
of our ongoing electronic distribution of IEEE publications.''
I asked the IEEE Intellectual Property Rights Manager for clarification:
</p><blockquote>
When there is no copyright, IEEE is completely free to do these things.
It can distribute the work as widely as it wants, and it can authorize
others to do so. All IEEE gets out of a copyright is the power to _stop_
the distribution of the information.
<p>
If the public domain is a problem for IEEE, how does IEEE publish works
from government authors? Do you have a better explanation for your
desire to be the owner of the work? Is it fair to say that IEEE actually
_does_ want to stop the distribution of scientific information?
</p></blockquote>
Naturally, the IEEE Intellectual Property Rights Manager refused to respond.
<p>
The IEEE Intellectual Property Rights Manager
had also devoted some effort to trying to fool the student
into believing that papers could not simply be dedicated to the public domain.
I asked for clarification:
</p><blockquote>
The Ninth Circuit Court of Appeals has commented that ``It is well
settled that rights gained under the Copyright Act may be abandoned.''
The standard way to abandon copyright is by a clear written dedication
of the work to the public domain. One example of a public-domain
dedication is http://creativecommons.org/licenses/publicdomain; surely
you're aware of attorney Lawrence Lessig's Creative Commons effort.
<p>
You stated that ``there is a specific legal process for placing your
work in the PD.'' When the student asked you for details, you refused to
answer. Instead you said that you were ``dubious about the idea of
simply declaring one's intention to inject a work into the public
domain,'' and that IEEE needed to be able to ``prove'' its rights.
</p><p>
If IEEE has trouble ``proving'' its rights to publish a public-domain
paper, then how does IEEE ``prove'' alleged authorship of a paper whose
copyright is allegedly transferred, and how does IEEE ``prove'' an
allegation of government employment? Is the ``legal process'' you
mentioned something more than making a clear written dedication of the
work to the public domain? If so, what exactly is the process? Why
didn't you answer the student's question regarding this process? Is it
fair to say that the IEEE Intellectual Property Rights Office is trying
to intimidate authors into transferring copyright to IEEE?
</p></blockquote>
The IEEE Intellectual Property Rights Manager refused to respond.
<p>
The bottom line is that IEEE
is refusing to accept public-domain papers
except from government authors.
IEEE has no justification for this position.
IEEE's action is a blatant attempt to maintain control
over papers that would otherwise have been freely available to the public.
Unfortunately, at least in this student's case,
the attempt succeeded:
a paper that was accepted by IEEE's scientific referees,
and that would have been in the public domain without IEEE's pressure,
is now part of IEEE's copyright portfolio.
</p><p>
Consequently, I am blacklisting IEEE here.
I recommend that authors find another publisher.
Springer, for example,
tacitly (although quite unhappily) allows public-domain papers,
and AMS explicitly does not require a copyright transfer.


</p></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Understanding Gaussians (109 pts)]]></title>
            <link>https://gestalt.ink/gaussians</link>
            <guid>41912160</guid>
            <pubDate>Tue, 22 Oct 2024 07:54:32 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://gestalt.ink/gaussians">https://gestalt.ink/gaussians</a>, See on <a href="https://news.ycombinator.com/item?id=41912160">Hacker News</a></p>
<div id="readability-page-1" class="page"><article>
		<h2 id="understanding-gaussians">Understanding Gaussians</h2>

<p>The <strong>Gaussian distribution</strong>, or <strong>normal distribution</strong> is a key subject in statistics, machine learning, physics, and pretty much any other field that deals with data and probability. It’s one of those subjects, like $\pi$ or Bayes’ rule, that is so fundamental that people treat it like an icon.</p>

<ul id="markdown-toc">
  <li><a href="#understanding-gaussians" id="markdown-toc-understanding-gaussians">Understanding Gaussians</a>    <ul>
      <li><a href="#building-a-geometric-intuition-for-gaussians" id="markdown-toc-building-a-geometric-intuition-for-gaussians">Building a geometric intuition for Gaussians</a>        <ul>
          <li><a href="#the-standard-gaussians" id="markdown-toc-the-standard-gaussians">The standard Gaussians</a></li>
          <li><a href="#a-family-of-gaussians" id="markdown-toc-a-family-of-gaussians">A family of Gaussians</a></li>
          <li><a href="#spherical-diagonal-and-degenerate-gaussians" id="markdown-toc-spherical-diagonal-and-degenerate-gaussians">Spherical, diagonal and degenerate Gaussians</a></li>
          <li><a href="#means" id="markdown-toc-means">Means</a></li>
          <li><a href="#covariances" id="markdown-toc-covariances">(Co)variances</a>            <ul>
              <li><a href="#the-variance-of-n1_s" id="markdown-toc-the-variance-of-n1_s">The variance of $N^1_s$</a></li>
              <li><a href="#the-covariance" id="markdown-toc-the-covariance">The covariance</a></li>
              <li><a href="#of-spherical-diagonal-and-degenerate-gaussians" id="markdown-toc-of-spherical-diagonal-and-degenerate-gaussians">Of spherical, diagonal and degenerate Gaussians</a></li>
            </ul>
          </li>
        </ul>
      </li>
      <li><a href="#fundamental-properties-of-gaussians" id="markdown-toc-fundamental-properties-of-gaussians">Fundamental properties of Gaussians</a>        <ul>
          <li><a href="#linear-transformations" id="markdown-toc-linear-transformations">Linear transformations</a></li>
          <li><a href="#if-you-can-linearly-transform-it-to-a-gaussian-its-a-gaussian" id="markdown-toc-if-you-can-linearly-transform-it-to-a-gaussian-its-a-gaussian">If you can linearly transform it to a Gaussian, it’s a Gaussian</a></li>
          <li><a href="#there-is-always-an-invertible-transformation" id="markdown-toc-there-is-always-an-invertible-transformation">There is always an <em>invertible</em> transformation</a></li>
          <li><a href="#the-sum-of-two-gaussians-is-a-gaussian" id="markdown-toc-the-sum-of-two-gaussians-is-a-gaussian">The sum of two Gaussians is a Gaussian</a></li>
          <li><a href="#chaining-gaussians" id="markdown-toc-chaining-gaussians">Chaining Gaussians</a></li>
          <li><a href="#conditioning-gaussians" id="markdown-toc-conditioning-gaussians">Conditioning Gaussians</a></li>
        </ul>
      </li>
      <li><a href="#deriving-the-density" id="markdown-toc-deriving-the-density">Deriving the density</a></li>
      <li><a href="#sources-and-other-materials" id="markdown-toc-sources-and-other-materials">Sources and other materials</a></li>
      <li><a href="#references" id="markdown-toc-references">References</a></li>
    </ul>
  </li>
</ul>

<p>To start at the beginning: the normal distribution is a <em>probability distribution</em>: a mathematical object that describes a process by which you can <em>sample data</em>. Here is an example. If I measure the height of about 2000 female soldiers in the US army, and plot the results in a histogram, here is what that might look like.</p>

<figure>
  <p><img src="https://gestalt.ink/images/gaussians/soldiers.svg"></p>
  <figcaption>
    <p>The <em>stature</em> (height) of 1986 female soldiers in the US Army. From the ANSUR II dataset [<a href="#references">1</a>].</p>
  </figcaption>
</figure>



<p>You can see that the data is clustered around the <em><span>mean value</span></em>. Another way of saying this is that the distribution has a definite <em>scale</em>. That is, even though people can have all sorts of heights, there are clear limits. You might see somebody who is 1 meter taller than the mean, and it might theoretically be possible to be 2 meters taller than the mean, but that’s it. People will never be 3 or 4 meters taller than the mean, no matter how many people you see.</p>



<p>The definite scale of the height distribution is why we can have doors. We know that heights will fall in a certain range, so we can build for that. There are a few distributions like this with a definite scale, but the Gaussian is the most famous one. You can see in the plot above that it has a kind of “bell” shape—it’s also called <em>the bell curve</em>—which trails off smoothly as we get further from the mean, first slowly and then dropping rapidly, and then flattening out quickly. If we make the bins in our histogram smaller, and increase the sample size so they are still filled up, we can see the shape appear more clearly.</p>

<figure>
  <p><img src="https://gestalt.ink/images/gaussians/soldiers-synth.svg"></p>
  <figcaption>
    <p>Synthetic data for 100 million imagined soldiers from the same distribution as the figure above.</p>
  </figcaption>
</figure>

<p>If you measure more than one thing about your subject, you get <em>multivariate</em> data, and the resulting distribution is called a <em>multivariate distribution</em>. For example, if we take our soldiers, and measure their height and their weight, the data looks like this.</p>

<figure>
  <p><img src="https://gestalt.ink/images/gaussians/soldiers-2d.svg"></p>
  <figcaption>
    <p>A scatter plot of the height and weight of our sample of soldiers.</p>
  </figcaption>
</figure>

<p>This is called <em>a multivariate normal distribution</em>. Like the one-dimensional (<em>uni</em>variate) version, the data is clustered around <span>a central value</span>.</p>

<p>Most descriptions you will read of the Gaussian distribution will focus on the way it is used to describe or approximate the real world: its use as <em>a model</em>. This is a typical statistics approach, and it  comes with a lot of baggage that we will not discuss here.</p>

<p>In this article, we want to focus more on the way Gaussians are used in <em>machine learning</em>. There, we also aim to build a model of our data, but we are often less concerned with the fact of capturing our data in a single Gaussian.  Instead, we use Gaussians as a <em>building block</em>, a small part of a more complex model. For instance, we might add noise from a Gaussian to our data at some point in our algorithm, or we could have a neural network produce the parameters of a Gaussian as part of its prediction. We could combine multiple Gaussians together, in order to create a distribution with multiple peaks. We could even take a sample from a Gaussian and feed it to a neural net, so that the neural net effectively twists and folds the relatively simple shape of the Gaussian into something much more complex.</p>

<figure>
  <p><img src="https://gestalt.ink/images/gaussians/neural-net.png"></p>
  <figcaption>
    <p>100 000 points drawn from a Gaussian distribution and passed through a randomly initialized neural network.</p>
  </figcaption>
</figure>

<p>To use Gaussians in this way requires a solid intuition for how they behave. The best way to do that, I think, is to do away entirely with the symbolic and mathematical foundations, and to derive what Gaussians are, and all their fundamental properties from purely geometric and visual principles. That’s what we’ll do in this article.</p>

<h2 id="building-a-geometric-intuition-for-gaussians">Building a geometric intuition for Gaussians</h2>

<p>If you’re not intimately familiar with Gaussians, you would be forgiven  for thinking of them as one of the most monstrously complicated probability distributions around. After all, when we learn about them, pretty much the first thing we see is their probability density function.</p>

\[N(\x \mid \oc{\bmu}, \bc{\Sig}) = \frac{1}{\sqrt{(2\pi)^{k}|\bc{\Sig}|}}\text{exp} \left( -\frac{1}{2}(\x-\oc{\bmu})^T \bc{\Sig}^{-1} (\x-\oc{\bmu})\right)\]

<p>It’s a beast of a formula, especially if you’re not used to reading such things. Why then, do people like this distribution so much? Why use it as a building block when it is already so complex? Shouldn’t we look for simple building blocks—perhaps something like a uniform distribution, which has a much simpler formula?</p>

<p>Partly, we like the Gaussian because it has nice properties, but partly, we like it because once you get to know it, it’s not so complicated. You just have to let yourself forget about the complicated formula. So we’ll put it out of our minds, and start elsewhere.</p>

<p>The plan is as follows. We will first derive a <em>standard</em> Gaussian. Just one distribution, in one dimension. This makes the formula much simpler. From this, we will define a standard Gaussian in $n$ dimensions, in a straightforward way, which doesn’t require us to extend the formula very much. Then, we will use <em>affine transformations</em>—multiplication by a matrix, and addition of a vector—to define a whole <em>family</em> of Gaussians. Implicitly, this will lead to the formula we use above, but practically, all we need to understand are the basic rules of linear algebra.</p>

<p>We will use this view to derive a bunch of useful properties about Gaussians, and then finally wrap up by showing that the above formula is indeed correct.</p>

<h3 id="the-standard-gaussians">The standard Gaussians</h3>

<p>The first thing we need is the standard Gaussian in one dimension. This is a probability distribution on the real number line: if we sample from it, we can get any real number. The function that describes it is a <em>probability density function</em>: it maps each real number to a probability density. Numbers with higher density are in some sense more likely than numbers with low density.</p>



<p>To come up with the density function, remember the aim we started with: we want the distribution to have a definite scale, some area where almost all of the probability mass is concentrated. We'll put that region around zero on the number line (it seems as good a point as any). This is where the density should peak, and as we move away from zero the density should drop very quickly, so that pretty soon, it's almost zero. One way of achieving this is to have <em>exponential decay</em>. Just like an exponential function $e^x$ blows up very quickly, the negative exponential function $e^{-x}$ drops to zero extremely quickly.</p>



<p>We could use the exponential function, but if we add a square in there, to give us $e^{-x^2}$ we get some nice properties on top of the exponential decay.</p>

<figure>
  <p><img src="https://gestalt.ink/images/gaussians/exp-vs-squaredexp.svg"></p>
</figure>

<p>First of all, the decay far out from zero is even faster, since we’re adding in a square. Second, close to zero, we get a little more probability density on all numbers in that region. The exponential decay really favors only 0, while the squared exponential favours all numbers <em>near zero</em>. Finally, the quared exponential has two <em>inflection points</em>, highlighted in the image with diamonds. These are the points where the decay moves from dropping faster and faster to dropping slower and slower. These inflection points form a nice, natural marking for the <em>scale</em> of the distribution: we can take the interval between the inflection points as the “typical” range of outcomes that we might get if we sample from the distribution. The numbers outside this range are possible, but they’re less likely.</p>



<p>So that’s where the basic bell shape of the distribution comes from: the choice to have the probability density decay squared exponentially. Next, we’ll make a small adjustment to make the function a little more well-behaved. Remember that the inflection points give a us a nice interval to consider the “typical points”. This interval is now a little arbitrary. If we scale the function a bit, we can put the inflection points at $-1$ and $1$, so that the interval containing the bulk of the probaility mass is contained in $(-1, 1)$. This seems like a nice property to have, and as it turns out, it doesn’t make the function much more complex.</p>

<p>First, we need to figure out where the inflection points are. We defined them as the point where the function moves from dropping faster and faster to dropping slower and slower. This behavior, how fast the change in the function changes, is given by the second derivative of the function. Where that is equal to zero, we find an inflection point. The first derivative of $\gc{e^{-x^2}}$ is (using the chain rule) $-2x\gc{e^{-x^2}}$, and the second derivative is (using the product rule) $-2\gc{e^{-x^2}} + 4x^2\gc{e^{-x^2}} = (4x^2 -2)\gc{e^{-x^2}}$. Setting that equal to zero, we get $x^2 = 1/2$, so the inflection points are at</p>

\[x = -\sqrt{\frac{1}{2}}  \;\text{and}\; x = \sqrt{\frac{1}{2}} \p\]

<p>If we want to stretch a function $f(x)$ vertically by a factor of $\rc{y}$, we should multiply its input by $\rc{1/y}$: $f\left(\rc{\frac{1}{y}}x\right)$. That means that if we want to stretch it so that the point $\rc{x}$ ends up at 1—a stretch of $\rc{1/x}$—we should multiply the input by $\rc{x}$</p>

<p>In our case, that means we multiply the input by $\rc{\sqrt{\frac{1}{2}}}$:</p>

\[e^ {-\left(\rc{\sqrt{ \frac{1}{2} }}x\right)^2} = e^{- \frac{1}{2}x}\]

<figure>
  <p><img src="https://gestalt.ink/images/gaussians/move-inflection-pts.svg"></p>
</figure>

<p>So, our function is now $e^{-\frac{1}{2}x^2}$. The extra multiplier of $\frac{1}{2}$ is a small price to pay to put the inflection points at $-1$ and $1$.</p>

<p>With that, we almost have a probability density function. The only problem left is that the rules of probability density functions state that the whole area under the curve should integrate to 1. Put simply, the probability of sampling any number in $(-\infty, \infty)$ should be $1$.</p>

<p>We could check whether it does, and if it doesn’t, we could stretch or squeeze the function vertically until it does. This would require some complicated analysis, which, while fun, is exactly the kind of thing we are trying to avoid. To keep things simple, we will simply assume that the area under the whole curve, from negative to positive infinity, is some finite value.</p>



<p>Whatever the area under the curve $e^{-\frac{1}{2}x^2}$ is, we will call that $\bc{z}$. By the rules of integration, multiplying our function by $1/\bc{z}$ will then yield a function that integrates to 1. Since $\bc{z}$ is a constant, we can say that the scaled function, which we will call $N_s$, is <em>proportional to</em> the unscaled function:</p>

\[N_s(x) \propto e^{-\frac{1}{2}x^2}.\]

<p>That is, $N_s$, which is a proper probability density, is a bit more complicated than $e^{-\frac{1}{2}x^2}$ but all that complexity is in some multiplicative constant. One other trick we can use to simplify things is to focus on the logarithm of the probability density. In our case, we get:</p>

\[\text{ln } N_s(x) \eqplus -\frac{1}{2} x ^2\]

<p>where the symbol $\eqplus$ means that both sides are equal except for some term ($- \text{ln}\,\bc{z}$ in this case) that doesn’t depend on $x$.</p>

<p>With that, we have defined our <em>standard Gaussian</em> in one dimension as precisely as we need. We don’t have the complete functional form of the density,but we don’t need it. We know the function exists, and we know what it looks like. We can now derive the full family of Gaussians.</p>



<p>First, to make the leap to <em>multivariate</em> Gaussians, we define a single multivariate <em>standard</em> Gaussian. In $n$ dimensions, we will call this distribution $N^n_s$. It’s a distribution over vectors $\x$ of $n$ elements.</p>

<p>We define $N^n_s$ by a <em>sampling process</em>. To sample from the multivariate Gaussian in $n$ dimensions, we sample $n$ separate values $x_1$ though $x_n$ from the standard one-dimensional Gaussian $N_s$ (which we’ve just defined) and we concatenate them into a vector $\x$. To say that the random vector $\x$ is <em>distributed according to the standard Gaussian</em> $N^n_s$ in $n$ dimensions, we write $\x \sim N^n_s$. This means that</p>

\[\begin{pmatrix}x_1 \\ \vdots \\x_n\end{pmatrix} \;\text{with}\; x_i \sim N_s \p\]

<p>If $\x$ is distributed according to $N^n_s$, then each individual element of $\x$ is distributed according to $N_s$.</p>

<p>This is a complete definition of the standard Gaussian. We haven’t defined a density function for $N^n_s$, but we’ve defined how to sample from it, which is all we need for a definition. The density function exists <em>implicitly</em>.</p>

<p>We can now ask ourselves what this density function looks like. We can derive the general form very easily from one basic property: that of <em>independence</em>. Since we sample the elements of $\x$ independently—how we sample one does not depend on how we sample the others—the probability density of the whole vector is the probability density of the elements multiplied together:</p>

\[N^n_s(\x) = p(x_1) \cdot p(x_2) \cdot \ldots \cdot p(x_n) \\\]

<p>Now, switching to log-probability densities, we can get a sense of the shape of the function. Remember that $\eqplus$ means equal up to some constant term, so we can remove any terms that don’t depend on elements of $\x$.</p>

\[\begin{align*}
\text{ln }N^n(\x) &amp;= \text{ln }p(x_1) + \ldots + \text{ln }p(x) \\
 &amp;= \text{ln }N_s(x_1) + \ldots + \text{ln }N_s p(x) \\
&amp;\eqplus -\tfrac{1}{2}{x_1}^2 - \ldots -\tfrac{1}{2}{x_n}^2 \\
&amp;= -\tfrac{1}{2}\left({x_1}^2 + \ldots +{x_n}^2\right) \\
&amp;= - \tfrac{1}{2}\| \x \|^2\\
\end{align*}\]

<p>The last line follows from recognizing that the right hand side has become equal to the vector norm without the square root. That is, the  square of the norm: $\|\x\|^2 = {x_1}^2 + \ldots + {x_n}^2$. Taking the logarithm away again, we get</p>

<p>$$
N^n_s(\x) \propto e^{-\tfrac{1}{2}\|\x\|^2} \p 
$$</p>

<p>That is, the probability density at any point $\x$ depends only on the norm of $\x$—how far away from $\zero$ we are. Imagining this in two dimensions to start with, this tells us that all points with the same distance to $\zero$, any set of points that forms a <em>circle</em>, have the same density. The function also tells us that as the norms (and thus the circles) get bigger, the probability density of the points in that circle decays in the same way as the density decays in $N_s$: according to a negative squared exponential.</p>

<p>With that, we have a pretty clear picture of what the standard multivariate Gaussian looks like. It’s rotationally symmetric, since all circles have the same density, and it decays in the same way as the bell shape of $N_s$. Putting this together, tells us that it should look, in two dimensions, like the function of $N_s$ rotated about the origin.</p>

<figure>
<img src="https://gestalt.ink/images/gaussians/bivariate.svg">
</figure>

<p>In two dimensions, the set of all points that have the same density—like one of the <span>the circles</span> in the picture above—is called a <em>contour line</em>. The standard Gaussian is called a <em>spherical</em> distribution because all its contour lines are circles (two-dimensional spheres). In higher dimensions, where things are more difficult to visualize, the same principle holds: the density of $\x$ under $N^n_s$ depens only on the norm of $\x$, so the set of all points with the same density is the set of all points with the same norm, a (hyper)-sphere. These spheres are called the <em>contour surfaces</em> of $N_s^n$. The principle of contour surfaces will be very helpful going forward, in building up an intuition for what general Gaussians look like.</p>

<p>Moving forward, we will drop the <span>superscript</span> from $N^\rc{n}_s$ when the dimensionality is clear from context. Likewise, we will use $N^\rc{1}_s$ to emphasize that we are talking about the one-dimensional Gaussian if necessary.</p>

<h3 id="a-family-of-gaussians">A family of Gaussians</h3>

<p>Next, let’s build the rest of the family. We do this by taking the standard Gaussian $N_s$ in $n$ dimensions, and <em>transforming</em> it linearly. We will start, again, with a sampling process.</p>

<p>We sample an $n$-dimensional vector $\s$ from $N_s$ and apply any linear operation $\x = \bc{\A}\s + \oc{\t}$ with a matrix $\bc{\A} \in \mR^{m \times n}$ and $\oc{\t} \in \mR^m$. This results in a random vector $\x$, since part of this process (the sampling of $\s$) is random.</p>

<p>Now, we <em>define</em> a Gaussian to be any distribution that results from this process, for some choice of $\bc{\A}$ and $\oc{\t}$. We will, refer to such a Gaussian as $N(\bc{\A}, \oc{\t})$.</p>



<p>We have defined how to sample a point from $N(\bc{\A}, \oc{\t})$, so we have fully defined this Gaussian. Obviously, it would be interesting to know what the resulting density function looks like, but that doesn’t need to be <em>its definition</em>. We can work that out from how we defined the sampling process. We’ll try to do that, and to work out some properties of the distribution we have now defined, without getting into the complicated formula for the density function.</p>

<p>For the time being, assume that $\bc{\A}$ is square and invertible, so that no two points are mapped to the same point by $\bc{\A}$.</p>

<p>To help us understand the shape of the density function, we can think back to the contour circles we defined for $N_s$, let's say the one for $\|\x\| = 1$. Each of the points $\x$ in this circle could be sampled from $N_s$ and transformed by $\bc{\A}$ and $\rc{\t}$. What happens to a circle when all its points are transformed by a matrix? It becomes an <em>ellipse</em>. What's more, the relative lengths of vectors are maintained under matrix multiplication—if $\|\a\| &lt; \|\b\|$ then $\|\bc{\A}\a\| &lt; \|\bc{\A}\b\| $—so any point inside the circle (any point with $\|\x\| &lt; 1$) before the transformation is inside the <em>ellipse</em> after the transformation. Any point outside the circle before, is outside the ellipse after.</p>

<figure>
<img src="https://gestalt.ink/images/gaussians/circle-ellipse.svg">
<figcaption>Transforming the standard Gaussian by an affine transformation turns the contour circles into contour ellipses. Any <span>point</span> inside one of the circles before the transformation will be inside the corresponding ellipse after.     
</figcaption>
</figure>

<p>This means that the amount of probability mass captured inside the unit circle before the transformation, is captured inside the corresponding ellipse after the transformation. After all, when we are sampling, these are the same points: if p is the probability of sampling some $\s$ inside the circle before the transformation, then that is the probability of sampling some $\x$ inside the corresponding ellipse.</p>

<p>For higher dimensions, the circles becomes hyper-spheres and the ellipses become ellipsoids, but the basic intuition stays the same.</p>

<p>If $\bc{\A}$ is not square and invertible, the picture is a little more complex. If, for example $\s$ is three-dimensional and $\x$ is two-dimensional, then we are taking all points $\s$ on a sphere, and projecting them down to two dimensions. The result is still an ellipse in two dimensions, but not all points are on the edge of the ellipse anymore. Some are in the interior. This means we no longer have the property that if $\|\a\| &lt; \|b\|$ then $\|\bc{\A}\a\| &lt; \|\bc{\A}\b\| $. However, we will be able to show in a bit that this distribution is equivalent to one defined with a two-dimensional $\s$ and a square, invertible $\bc{\A}$. Thus, this messiness isn't really any cause for concern. We can still call this a Gaussian, and think of it as being mapped from $N_s$ in a neat way that maps contour circles to contour ellipses.</p>

<figure>
<img src="https://gestalt.ink/images/gaussians/2s-to-3d.png">
<figcaption>If the transformation is from three to two dimensions, points on the sphere (left) may end up inside the corresponding ellipse (right).    
</figcaption>
</figure>

<h3 id="spherical-diagonal-and-degenerate-gaussians">Spherical, diagonal and degenerate Gaussians</h3>

<p>Before we move on, it pays to investigate what kind of family members this family of ours has. We’ll look at three special types of Gaussians: spherical, diagonal and degenerate.</p>

<p>The simplest type of Gaussian is the <strong>spherical Gaussian</strong>, also known as an <em>isotropic</em> Gaussian. This is the special case when the contour surfaces, which are spheres before the transformation, are still spheres after the transformation.</p>



<p>This happens only when we expand $\s$ uniformly in all directions. Or, in other words, when we multiply it by a scalar. That is, if</p>

\[\x = \bc{\sigma} \s + \oc{\t}\]

<p>for some scalar $\bc{\sigma}$, then the distribution on $\x$ is a sperical Gaussian. To fit this into the standard affine transformation framework, we can insert an identity matrix and get:</p>

\[\x = \bc{\sigma \I} \s + \oc{\t} \p\]

<p>This shows that the matrix $\bc{ \sigma\I}$—a diagonal matrix with $\bc{\sigma}$ at every point on the diagonal—is the matrix we should multiply by to get a spherical Gaussian.</p>



<p>The spherical Gaussians are particularly simple, and using them will simplify many aspects of the use of Gaussians. In machine learning, you will see them used in, for example, diffusion models.</p>

<p>A class that allows for a bit more variation is the <strong>diagonal Gaussian</strong>. Here, we again use a diagonal matrix in our affine transformation, but we let the diagonal values vary. That is, we define some <em>vector</em> $\bc{\sig}$, and we <a href="https://gestalt.ink/diagonal-matrix">place these values along the diagonal of a matrix</a>. That matrix then becomes our transformation matrix.</p>

<p>A diagonal $\gc{\D}$ matrix represents a particularly simple transformation. The dimension $i$ is simply multiplied by the value $\gc{D}_{ii}$, ignoring whatever happens elsewhere in the matrix.</p>

<p>Visually, the result is that the circles or spheres in the standard normal distribution are stretched into ellipses, but <em>only along the axes</em>. Any ellipse is allowed, but the major axis of the ellipse (the line from tip to tip) has to point along one of the axes of our coordinate system.</p>

<p>Practically, this means that the distribution has zero <em>correlation</em> between the elements of $\x$. If I tell you the value of $x_1$, it carries no information about the value of $x_2$. Study the example of the heights and widths of the soldiers above to see the opposite case: the points are roughly on a diagonal line, so if I tell you that a particular soldier has a certain height, you can make an informed guess about what their weight is likely to be. For that sort of reasoning, you need more than a diagonal Gaussian. You get this by playing non-zero values on the off-diagonal elements of your transformation matrix.</p>

<p>The final special case we will discuss is the <strong>degenerate Gaussian</strong>. This is what happens when, for example, we map a one-dimensional $\s$ to a two-dimensional $\x$.</p>

<figure>
<img src="https://gestalt.ink/images/gaussians/family-members.svg">
</figure>

<p>Since all points $\s$ lie on a line, the resulting points $\x$ can only lie on a line, even though they’re in a two-dimensional space. We’ve decided to call this a Gaussian, and along the line, you will see the familiar bell shape, but it’s fundamentally different from a true two-dimensional Gaussian like $N^2_s$, that fills all of $\mR^2$.</p>

<p>We call this a degenerate Gaussian. If $\x$ has $d$ dimensions, the <em>support</em> of the Gaussian, the set of all points that have non-zero density isn’t the whole of $\mR^d$, in fact, it’s a linear subset of it (a hyperplane).</p>

<p>Within the support, the distribution looks like a normal Gaussian: a bell shape and non-zero probability density everywhere, decaying squared-exponentiallyas we move away from the mean. We call a Gaussian that does have a non-zero density everywhere a <em>non-degenerate</em> Gaussian, or a <em>Gaussian with full support</em>.</p>

<h3 id="means">Means</h3>

<p>Next, let’s look at the properties of these Gaussians. First, the mean $\bar\x$. If we average a bunch of samples from $N_s$ and let the number of samples go to infinity, where do we end up? This is called the <a href="https://gestalt.ink/expectation">expected value</a>: $\bar\x = E_{\x\sim N_s} \x $</p>



<p>The definition of the expected value for continuous functions like these involves an integral, but happily, we don’t need to open it up. We just need to remember some key properties:</p>
<ul>
  <li>Expectation distributes over (vector) sums. That is $E_\x \left (f(\x) + g(\x)\right) = E_\x f(\x) + E_\x g(\x)$.</li>
  <li>If we have (matrix) multiplication or (vector) addition inside the expectation, we can move it outside. That is \(E_\x \oc{\t} + \bc{\A}\x  = \oc{\t} + \bc{\A}E_\x \x\).</li>
</ul>

<p>Let’s start with the mean of the standard Gaussian. You may be able to guess what this should come out to. The density peaks at $\zero$, and the function is radially symmetric around $\zero$. If we think of the mean as the center of mass of the density function, there isn’t really any other point that could qualify.</p>

<p>It’s relatively simple to show that this guess is correct, because the components of $\x$ are independently drawn. If we ask for the $i$-th element of the mean, we need only look at the $i$-th elements of our samples. These are all samples from $N^1_s$, so the mean for that component is the mean of $N^1_s$. $N^1_s$ is symmetric around $0$, so its mean must be $0$.
In short, the mean for the standard Gaussian is the zero vector.</p>



<p>What about the mean of our transformed Gaussian $N(\bc{\A}, \oc{\t})$? If we use the basic poperty that the expectation is a linear function–that is, we can move additions and multiplications outside the expectation)—we can show very simply that the mean is equal to the translation vector in our transformation, $\oc{\t}$:</p>

<p>$$E_{\x\sim N(\bc{\A}, \oc{\t})} \x = {E}_{\s \sim N_s} \oc{\t} + \bc{\A}\s= \oc{\t} + \bc{\A} E_{\s} \x = \oc{\t} + \kc{\zero} \p$$
</p>

<h3 id="covariances">(Co)variances</h3>

<h4 id="the-variance-of-n1_s">The variance of $N^1_s$</h4>

<p>The variance is a measure of how widely the sampled points are spread about the mean. We’ll need to work out the variance of $N^1_s$ first. It is defined as the expected value of the squared distance to the mean: $E (x - \bar x)^2$. Since the mean is $0$, we are just looking for the expected value of $x^2$.</p>

<p>We could solve this by unpacking this expectation into its integral definition and working it out, but that requires a lot of heavy math. Happily, there’s a nifty trick that allows us to minimize the amount of time we need to spend in integral-land. First, let’s see what the integral is that we’re looking for. To make things easier to follow, I’ll give away that the answer is 1, and we’ll work towards that.</p>

<p>First, we’ll call the unscaled density function $f$. That is</p>

\[f(x) = e^{-\tfrac{1}{2} x} \;\text{ and }\; p(x) = \tfrac{1}{\bc{z}}f(x) \p\]

<p>Then, the variance is</p>

\[\begin{align}
E x^2 = \int_{-\infty}^\infty p(x)x^2 dx  = \tfrac{1}{\bc{z}} \gc{\int_{-\infty}^\infty x^2f(x) dx} \p \tag{var}
\end{align}\]

<p>To show that the variance of the standard normal distribution is 1, we need to show that the integral marked <span>in green</span> at the end is equal to $\bc{z}$ (which is the name we gave to the area under the curve of $f$). This is where we can use a trick.</p>

<p>The trick requires us to take the second derivative of $f$. We have</p>

\[\begin{align*}
f'(x) &amp;= -x e^{-\tfrac{1}{2}x^2} \\
f''(x) &amp;= (x^2 - 1) e^{-\tfrac{1}{2}x^2} = \gc{x^2 f(x)} - f(x)\p \\
\end{align*}\]

<p>Note that the function we’re taking the integral for $\gc{x^2f(x)}$ has popped up on the right-hand-side. If we re-arrange this last line, we see</p>

\[\gc{x^2 f(x)} = f''(x) + f(x) \p\]

<p>Filling this into the integral, we get</p>

\[\gc{\int_{-\infty}^\infty x^2f(x) dx} = \int_{-\infty}^\infty f''(x) + f(x) dx = \rc{\int_{-\infty}^\infty f''(x)dx} + \oc{\int_{-\infty}^\infty f(x) dx} \p\]

<p>Now, the <span>first term</span>, $\int_{-\infty}^\infty f''(x)$, is equal to 0. We solve it by taking the antiderivative $f'(x)$ and working out $f'(\infty) - f'(-\infty)$. The derivative of $f$ is 0 at both ends, since the function flattens out towards infinity, so the answer is $0 - 0 = 0$.</p>

<p>That leaves us with <span>the second term</span>, $\int_{-\infty}^\infty f(x) dx$, which is exactly the definition of $\bc{z}$. So we have worked out that</p>

\[\gc{\int_{-\infty}^\infty x^2f(x) dx} = \bc{z}\]

<p>which, if we fill it in above—in equation (var)— shows that the variance of $N^1_s$ is 1.</p>



<p>Now that we know what the variance of the standard, one-dimensional Gaussian is, the hard work is done. The parameters of the rest of the Gaussians follow straightforwardly.</p>

<h4 id="the-covariance">The covariance</h4>

<p>For a multivariate distribution on a vector $\x$ there are many variances to capture. There is first the variance along each dimension $x_i$, but also the <em>co</em>variance of every element $x_i$ with every other element $x_j$.</p>

<p>The <a href="https://gestalt.ink/covariance-matrix">covariance matrix</a> $\bc{\Sig}$ captures all of this. For a random vector $\x$ this is defined as the expected outer product of the deviation from the mean $E  (\bar\x - \x)(\bar\x - \x)^T$. This is a square matrix. It contains all the variances of the individual elements $x_i$ of $\x$ along its diagonal, and it contains all the covariances between elements $x_i$ and $x_j$ on its off-diagonal elements.</p>

<p>Let’s start with the covariance matrix of the standard Gaussian $N_s$. We know that in $\bc{\Sig}$, the diagonal elements are the variances of $x_i$. These are $1$, because we sampled them independently from $N_1$, which has variance 1. The off-diagonal elements are the co-variances between any two of the elements $x_i$ and $x_j$. We know these are $0$, because we sampled each $x_i$ independently. So, in a phrase, the covariance matrix of $N_s$ is the identity matrix $\I$.</p>

<p>Now for the rest of the Gaussians. The covariance matrix of $\x = \bc{\A}\s + \rc{\t}$ is defined as the expected outer product of the vector $\x - \bar\x$, where $\bar\x$ is the mean of $\x$. We already know that $\bar\x = \oc{\t}$, so we are looking for the expected outer product of $\x - \bar\x = \bc{\A}\s \kc{\;+\;\t - \t}$. This gives us.</p>

\[\begin{align*}
E_{\x\sim N(\bc{\A}, \oc{\t})} (\x-\bar\x)(\x-\bar\x)^T  &amp;= E_{\s \sim N_s} (\bc{\A}\s)(\bc{\A}\s)^T \\
&amp; = E \bc{\A}\s\s^T\bc{\A}^T = \bc{\A} (E \s\s^T)\bc{\A}^T\\ 
&amp; = \bc{\A} \I\bc{\A}^T = \bc{\A\A}^T \p 
\end{align*}\]

<p>Note that in the second line we are again using the fact that the expectation is a linear function, so we can take matrix multiplications outside of the expectation (on the left and on the right).</p>

<p>So, to summarize, if we build our Gaussian by transforming $N_s$ with a transformation matrix $\bc{\A}$ and a translation vector $\oc\t$, we end up with a distribution with mean $\oc\t$ and covariance matrix $\bc{\Sig} = \bc{\A\A^T}$.</p>

<p>We can now make the leap from <em>properties</em> to <em>parameters</em>. Instead of identifying a particular Gaussian by the transformation $\bc{\A}, \oc{\t}$ we used to create it, we can identify it by the covariance $\bc{\Sig}$ and mean $\oc\t$ of the resulting distribution.</p>

<p>The Gaussian we get from the transformation $\bc{\A}\x + \oc{\t}$ on the standard normal distribution is called $N(\oc{\bmu}, \bc{\Sig})$, with $\oc{\bmu} = \oc{\t}$ and $\bc{\Sig} = \bc{\A\A}^T$.</p>



<p>This also means that $N_s = N(\oc{\zero}, \bc{\I})$, which is how we’ll refer to it from now on.</p>



<h4 id="of-spherical-diagonal-and-degenerate-gaussians">Of spherical, diagonal and degenerate Gaussians</h4>

<p>It’s worth thinking briefly about what the covariance matrix looks like for the three special categories of Gaussian that we discussed earlier: spherical, diagonal and degenerate.</p>

<p>For the spherical and the diagonal Gaussian, remember that $\bc{\A}$ is a diagonal matrix. This means that the covariance matrix $\bc{\A}\bc{\A}^T$ is equal to $\bc{\A}\bc{\A}$, since $\bc{\A}$ is symmetric, so $\bc{\A}^T = \bc{\A}$. The product of two diagonal matrices is very simple: it is another diagonal matrix, with at each point along the diagonal, the product of the corresponding elements of the two matrices.</p>



<p>The result is that for a spherical Gaussian with standard deviation $\bc{\sigma}$, while $\bc{\A}$ is a diagonal matrix with $\bc{\sigma}$ along the diagonal, the covariance matrix is a diagonal matrix with $\bc{\sigma}^2$ along the diagonal. This is of course, the variance.</p>



<p>Likewise for the diagonal Gaussian, we have the standard deviations along the diagonal of $\bc{\A}$, and their squares, the variances, along the diagonal of the covariance $\bc{\A}\bc{\A}^T$.</p>

<p>In both cases, the covariances (the off-diagonal elements of $\bc{\A}^T\bc{\A}$) are zero. This shows that there is no correlation between the axes: if our Gaussian is diagonal, we cannot predict the value of one dimension from one of the other dimensions.</p>

<p>Finally, let’s look at the degenerate Gaussians. We get a degenerate Gaussian if $\bc{\A}$’s <em><a href="https://gestalt.ink/rank">rank</a></em> is less than the output dimension. Or, put differently, for $\x \in \mR^d$, if it has fewer than $d$ linearly independent columns. If this happens—say there are $k$ linearly independent columns in $\bc{\A}$ and the rest can be expressed as a linear combination of these $k$ columns—then any $\s$ multiplied by $\bc{\A}$ is mapped to a space of dimension $k$, since the multiplication is a linear combination of $k$ vectors.</p>

<p>We can get some insight into the consequences by looking at the <a href="https://gestalt.ink/svd">singular value decomposition</a> (SVD) of $\bc{\A}$.</p>



<p>Let $\bc{\A} = \rc{\U}\gc{\Sig}\rc{\V}^T$ be the full SVD of $\bc{\A}$. If $\bc{\A}$ maps its input into an output of dimension $k$, then the diagonal of $\gc{\Sig}$, containing the singular values, has $k$ non-zero elements.</p>



<p>To see the effect on the covariance matrix, we can fill in the SVD. You may have seen this before: filling in the SVD of $\bc{\A}$ in the Gram matrix, and simplifying, gives us the eigendecomposition of the Gram matrix.</p>

\[\begin{align*}
\bc{\A}\bc{\A}^T &amp;= \rc{\U}\gc{\Sig}\rc{\V}^T\left (\rc{\U}\gc{\Sig}\rc{\V}^T\right)^T \\
&amp;= \rc{\U}\gc{\Sig}\rc{\V}^T\rc{\V}\gc{\Sig}^T\rc{\U}^T \\
&amp;= \rc{\U}\gc{\Sig}\gc{\Sig}^T\rc{\U}^T \\
&amp;= \rc{\U}\gc{\Sig}^2\rc{\U}^T \p
\end{align*}\]

<p>Note that the square of a diagonal matrix like $\gc{\Sig}$ just consists of a diagonal matrix with the squares of the original matrix on the diagonal. That means that $\gc{\Sig}^2$ also has $k$ non-zero values.</p>

<p>What does this tell us? Since this last line is the eigendecomposition, the diagonal values of $\gc{\Sig}^2$ are the <a href="https://gestalt.ink/eigenvalues">eigenvalues</a> of the covariance matrix. They tell us how much the matrix $\bc{\A}\bc{\A}^T$ stretches space along the eigenvectors. If any of the eigenvalues are zero, as they are here, then along those directions, $\bc{\A}\bc{\A}^T$ <em>collapses</em> space. By multiplying with zero, a whole dimension is collapsed into a single point. The result is that $\bc{\A}\bc{\A}^T$ is <em>singular</em>—the opposite of invertible.</p>

<p>So with that slight detour into singular value decompositions, we can characterize the covariance matrices of degenerate Gaussians. A covariance matrix is singular <a href="https://gestalt.ink/iff">if and only if</a> the Gaussian is degenerate. If the Gaussian has full support, its covariance matrix is invertible.</p>

<h2 id="fundamental-properties-of-gaussians">Fundamental properties of Gaussians</h2>

<p>Now that we have built up a geometric view of Gaussians, we can work out pretty much any property we need. Let’s look at some examples. First, we know that linear transformations turn the standard Gaussian into another Gaussian. What happens if we linearly transform other Gaussians?</p>

<h3 id="linear-transformations">Linear transformations</h3>

<p><strong>Linear transformation of Gaussians</strong> Let $\x$ be a random variable with any Gaussian distribution $\x \sim N(\oc{\bmu}, \bc{\Sig})$. Apply to $\x$ any linear operation $\y = \gc{\A}\x + \rc{\t}$ with a matrix $\gc{\A}$ and vector $\rc{\t}$. Then $\y$ has a Gaussian distribution. Specifically,
$$
\y \sim N(\gc{\A}\oc{\bmu} + \rc{\t}, \gc{\A}\bc{\Sig}\gc{\A}^T) \p
$$
</p>
<p><span>Proof.</span> We know, from our construction of the Gaussians, that there is some $\bc{\B}$ and $\oc{\q}$ so that $\x = \bc{\B}\s + \oc{\q}$ with $\s = N(\zero, \I)$ gives us $\x \sim N(\oc{\mu} = \oc{\q}, \bc{\Sig} = \bc{\B\B}^T)$. Filling in this operation into the one from the theorem, we get

$$
\y = \gc{\A}(\bc{\B}\s + \oc{\q}) + \rc{\t} =  \gc{\A}\bc{\B}\s + \bc{\A}\oc{\q} + \rc{\t} \p
$$

This expresses $\y$ as a linear transformation of $\s \sim N(\zero, \I)$ with transformation matrix $\gc{\A}\bc{\B}$ and translation vector $\bc{\A}\oc{\q}+\rc{\t}$, so $\y$ has a Gaussian distribution. Moreover, we know that its parameters are 

$$
\oc{\bmu}_\y = \gc{\A}\oc{\q} + \rc{\t} = \gc{\A}\oc{\bmu} + \rc{\t}
$$

and 

$$
\bc{\Sig}_\y = \gc{\A}\bc{\B}(\gc{\A}\bc{\B})^T = \gc{\A}\bc{\B\B}^T\gc{\A}^T = \gc{\A}\bc{\Sig}\gc{\A}^T \p 
$$
<span></span>
</p>



<p>Note, again,  that this result holds, <strong>even if $\gc{\A}$ is not a square matrix</strong>. This leads directly to some very useful corollaries.</p>

<p><strong>Subvectors of a Gaussian vector are Gaussian.</strong>
If we sample $\x$ from any Gaussian, and select one or more of its elements, the resulting vector $\x'$ is also distributed according to a Gaussian.
</p>
<p><span>Proof.</span> Selecting elements of a vector can be done by a matrix multiplication. For instance, the matrix $(0, 1, 0)$ selects the middle element of a three-dimensional vector.
<span></span></p>

<p><strong>question:</strong> What does this look like if I select two elements? What are the parameters of the resulting distribution. What should I expect the resulting distribution to be if I select <em>all elements</em>? Can you show that this expectation is correct?</p>

<p>One consequence is that if you project a Gaussian onto one of the axes, the result is a univariate Gaussian along that axis. In terms of probability, this corresponds to <em>taking a marginal</em>. For example, if I measure the height and weight in a population of female soldiers, I get a bivariate distribution which is highly correlated (you can predict one measurement from the other pretty well) as we saw above. The above result shows that if I know the combined measurement is Gaussian, then dropping one of the two dimensions automatically results in a Gaussian as well.</p>



<p>With the example in the proof, we sample $\x$ from some Gaussian, and then only look at the distribution on $x_2$, disregarding the rest of the vector. If you followed the definition of marginalization, you would end up with a formula like</p>

\[p(x_2) = \int_{x_1, x_3} N(x_1, x_2, x_3 \mid\oc{\bmu}, \bc{\Sig})\, d x_1x_2\]

<p>for which you would then have to fill in that horrible formula for $N$ and work out the integral. Ultimately, you would end up with the result that $p(x_2)$ is a Gaussian, with some particular parameters, but it would be a lot of work.</p>

<p>This shows the benefit of our geometric construction of the Gaussians. With a little thinking we can almost always leave $N$ be and never open up the box. We just assume that it’s some affine transformation of the standard Gaussian and build up from there.</p>

<h3 id="if-you-can-linearly-transform-it-to-a-gaussian-its-a-gaussian">If you can linearly transform it to a Gaussian, it’s a Gaussian</h3>

<p>We showed above that if you linearly transform a Gaussian, the result is another Gaussian. Next, it’s useful to show that, under some mild assumptions, this also works the other way around. If we are given a distribution $p$ and we can apply a linear transformation to turn it into a Gaussian, then $p$ is also a Gaussian.</p>

<p><strong>Linear transformation <em>to</em> Gaussians</strong> Let $\x \sim p$. If there exists a linear transformation $\y = \gc{\A}\x + \oc{\t}$ so that $\y$ is a Gaussian, then so long as the columns of $\gc{\A}$ are linearly independent, $p$ is Gaussian. 
</p>
<div>
  <p><span>Proof.</span> Since $\y$ is Gaussian, there is a transformation $\y = \bc{\B}\s + \oc{\q}$ with $\s \sim N(\zero, \I)$. This gives us</p><p>

\[\begin{align*}
\gc{\A}\x + \rc{\t} &amp;= \bc{\B}\s + \oc{\q} \\
\gc{\A}\x &amp;= \bc{\B}\s + \oc{\q} - \rc{\t}\\
\gc{\A}^T\gc{\A}\x &amp;= \gc{\A}^T\bc{\B}\s + \gc{\A}^T(\oc{\q} - \rc{\t})\\
\end{align*}\]

  </p><p>The matrix $\gc{\A}^T\gc{\A}$ on the left is called the <a href="https://gestalt.ink/gram-matrix">Gram matrix</a> of $\gc{\A}$. If $\gc{\A}$’s columns are linearly independent, then the Gram matrix is invertible. This means we can multiply both sides by the inverse of the Gram matrix and get</p><p>

\[\x = \gc{\A}^\dagger\bc{\B}\s + \gc{\A}^\dagger(\oc{\q} - \rc{\t}) \;\text{ with }\; \gc{\A}^\dagger = (\gc{\A}^T\gc{\A})^{-1}\gc{\A} \p\]

  </p><p>The right-hand-side, while complicated, is an affine transformation of a standard normally distributed vector $\s$, so $\x$ is Gaussian.
<span></span></p>
</div>



<h3 id="there-is-always-an-invertible-transformation">There is always an <em>invertible</em> transformation</h3>

<p>We have defined a Gaussian as a distribution resulting from <em>any</em> affine transformation $\x = \bc{\A}\s + \oc{\t}$ of standard-normal noise $\s$. Even if a $\bc{\A}$ is low-rank, so that the resulting Gaussian only covers a subspace of the space that $\x$ is embedded in.</p>



<p>Let’s focus on those Gaussians that are not degenerate in this way: assume, for an $n$-dimensional vector $\x$, that the Gaussian $\x = \gc{\A}\s + \rc{\t}$ assigns every point in $\mR^n$ a non-zero probability density.</p>



<p>$\s$ might still be of a higher dimensionality than $\x$, and $\gc{\A}$ may thus still not be invertible. In such a case, we can always find a different parametrization of the same Gaussian using an invertible matrix and an $\s$ with the same dimension as $\x$.</p>

<p><strong>Invertible parametrization</strong> Let $\x = \bc{\A}\s + \oc{\t}$ be any non-degenerate Gaussian. Then, there are parameters $\bc{\B}$, $\oc{\u}$ such that $\bc{\B}$ is square and invertible and $\x = \bc{\B}\s' + \oc{\u}$ with $\s' \sim N(\zero, \I)$ describes the same Gaussian.    
</p>
<div>
  <p><span>Proof.</span> Let $k$ be the dimensionality of $\x$. If $\bc{\A}$ is taller than wide, it can not provide full support, so we may dismiss this case. If $\bc{\A}$ is square and provides full support, then it must be invertible, so we can set $\bc{\A} = \bc{\B}$.</p>

  <p>We are left with the case that $\bc{\A}$ is rectangular and wider than tall. Let $\gc{\A} = \rc{\U}\gc{\Sig}\rc{\V}^T$ be the full singular value decomposition of $\bc{\A}$. This gives us</p><p>

\[\x = \rc{\U}\gc{\Sig}\rc{\V}^T\s + \oc{\t} \p\]

  </p><p>We can rename $\s’ = \rc{\V}^T\s$. Since $\rc{\V}$ is an orthogonal transformation (a combination of rotations and flips), $\s’$ is still standard-normally distributed.</p>

  

  <p>This gives us</p><p>

\[\x = \rc{\U}\gc{\Sig}\s' + \oc{\t} \p\]

  </p><figure>
<img src="https://gestalt.ink/images/gaussians/svd-slice.svg">
<figcaption>(left) A multiplication diagram for the operation above (with the $+ \oc{\t}$ term omitted). (right) The reduced version we will derive below.</figcaption>
</figure>

  <p>From the multiplication diagram, we see that $\gc{\Sig}$ contains a number of zero columns which essentially ignore the corrsponding dimensions of $\s'$. Call $\s'_k$ the vector $\s'$ with these dimensions removed, and call $\gc{\Sig}_k$ the matrix $\gc{\Sig}$ with the corresponding columns removed. This gives us</p><p>

\[\x = \rc{\U}\gc{\Sig}_k\s'_k + \oc{\t} \p\]

  </p><p>We set $\bc{\B} = \rc{\U}\gc{\Sig}_k$ to obtain the required result. Note that the diagonal of $\gc{\Sig}$ must contain all non-zero elements, or we would not have full support, so that it must be invertible. $\rc{\U}$ is also invertible, since it is orthogonal, and multiplying two invertible matrices together results in another invertible matrix.
<span></span></p>
</div>

<p>If all that seems a bit technical, the key idea is that an affine transformation that results in full support on $\mR^k$ must map $k$ dimensions in the input to $k$ dimensions in the output. The rest are dimensions that are ignored (they are in the <em>null space</em> of $\gc{\A}$). The trick is then to isolate just those $k$ dimensions, and to ignore the rest.</p>

<p>The singular value decomposition is just a handy tool to isolate the right dimensions.</p>

<h3 id="the-sum-of-two-gaussians-is-a-gaussian">The sum of two Gaussians is a Gaussian</h3>

<p>Let $\a$ be a vector sampled from one Gaussian and $\b$ be a vector sampled from another Gaussian. Sum them together and return the result $\c = \a + \b$. What is the distribution on $\c$?</p>



<p>It may not surprise you to learn that the result is another Gaussian.</p>

<p>It pays to be careful here. If I give you the probability density functions of two Gaussians, and you create a new probability density function by making a weighted sum of these two densities for a given value $\x$, then the result of that is a mixture-of-Gaussians, which is usually, decidedly <em>not</em> Gaussian. What we are talking about here is <em>sampling</em> from two different Gaussians, and then summing the sampled values.</p>

<p><strong>question</strong>: I am a teacher and my class has students from two different schools in equal proportion, with different mean grades. The probability over the whole class of someone scoring a grade of $6$ is the average of the probability that someone from school 1 scores a $6$ and the probability that someone from school 2 scores a $6$. Is the result necessarily a Gaussian? Consider what the distribution looks like if the mean grades for the two schools are very far apart.</p>

<p><strong>question</strong>: I pair up each student from school 1 with a student from school 2. For one such pair, I test both, and average their grades. What is the distribution on the average I get? Is it Gaussian?</p>

<p>We can prove this property using our geometric construction, but we have to be a little bit more inventive than before. The key is to realize that the <em>concatenation</em> of $\a$ and $\b$ has a Gaussian distribution and that given this concatenation, the sum is just an affine operation.</p>

<p>We’ll first show that the concatenation of two Gaussians yields a Gaussian. This is a very intuitive result, that you may well be willing to accept without proof, but it doesn’t hurt to be rigorous.</p>

<div>
  <p><strong>Lemma. Concatenation of Gaussian variables</strong> Let $\a$ and $\b$ be vectors</p><p>

\[\begin{align*}
\a &amp;\sim N(\oc{\bmu}, \bc{\Sig}) \text{, }\\ 
\b &amp;\sim N(\oc{\bnu}, \bc{\T}) \text{ and }\\
\c &amp;= \begin{pmatrix}\a \\ \b \end{pmatrix} \p 
\end{align*}\]

  </p><p>That is, $\c$ is the concatenation of $\a$ and $\b$. Then $p(\c)$ is Gaussian with mean 
\(\begin{pmatrix}\oc{\bmu} \\ \oc{\bnu} \end{pmatrix}\) 
and covariance 
\(\begin{pmatrix}
\bc{\Sig} &amp; \zero \\
\zero &amp; \bc{\T}
\end{pmatrix} \p\)</p>
</div>
<div>
  <p><span>Proof.</span> First, we rewrite $\a$ and $\b$ as affine transformations of standard normal noise:</p><p>

\[\begin{align*}
\a &amp;= \bc{\A}\s + \oc{\bmu} \\
\b &amp;= \bc{\B}\t + \oc{\bnu} \p
\end{align*}\]

  </p><p>Where $\s$ and $\t$ are standard normal and $\bc{\Sig} = \bc{\A\A}^T$ and $\bc{\T} = \bc{\B\B}^T$. Then, $\c$ can be written as</p><p>

\[\c = \begin{pmatrix}\a \\ \b\end{pmatrix} = \begin{pmatrix} \bc{\A}\s + \oc{\bmu} \\ \bc{\B}\t + \oc{\bnu} \end{pmatrix}  = \bc{\C} \begin{pmatrix} \s \\ \t\end{pmatrix} + \begin{pmatrix} \oc{\bmu} \\ \oc{\bnu}\end{pmatrix}\]

  </p><p>where</p><p>

\[\bc{\C}  = \begin{pmatrix}\bc{\A} &amp; \zero \\ \zero &amp; \bc{\B} \end{pmatrix} \p\]

  </p><p>Now, note that the vector $\begin{pmatrix} \s \\ \t\end{pmatrix}$ consists only of univariate, standard-normal elements. In other words, this vector is a standard-normal sample itself. This means that $\c$ has a Gaussian distribution. From the affine transformation above, we see that its mean is the concatenation of $\oc{\bmu}$ and $\oc{\bnu}$ as required. Its covariance is $\bc{\C\C}^T$, which the following diagram shows is equal to the covariance in the proof statement.</p>

  <figure>
<img src="https://gestalt.ink/images/gaussians/concatenate.svg">
</figure>

  
</div>

<p>Using this lemma, the result for summing Gaussians follows almost directly.</p>

<div>
  <p><strong>Theorem. Sum of Gaussian variables</strong> Let</p><p>

\[\begin{align*}
\a &amp;\sim N(\oc{\bmu}, \bc{\Sig}) \text{, }\\ 
\b &amp;\sim N(\oc{\nu}, \bc{\bTau}) \text{ and }\\
\c &amp;= \a + \b \p 
\end{align*}\]

  </p><p>Then $p(\c) = N(\c \mid \oc{\bmu} + \oc{\bnu}, \bc{\Sig} + \bc{\bTau})$.</p>
</div>
<div>
  <p><span>Proof.</span> Let $k$ be the dimensionality of $\a$ and $\b$.</p>

  <p>Let $\d$ be the concatenation of $\a$ and $\b$. By the lemma above, $\d$ follows a Gaussian distribution.</p>

  <p>To turn two concatenated vectors into the sum of two vectors, we can multiply by the matrix $\begin{pmatrix}\I&amp; \I\end{pmatrix}$—that is, two identity matrices side by side. If we have $\c = \begin{pmatrix}\I &amp; \I\end{pmatrix}\d$, then</p><p>

\[c_i = \sum_\kc{k} \begin{pmatrix}\I &amp; \I\end{pmatrix}_{i\kc{k}} d_i = a_i + b_i \p\]

  </p><p>This shows that $\d$ is Gaussian.</p>

  <p>To work out the parameters, we write out the full operation: concatenation and summation:</p><p>

\[\begin{align*}
\c &amp;= \begin{pmatrix}\I &amp; \I\end{pmatrix} \begin{pmatrix}\bc{\A} &amp; \zero \\ \zero &amp; \bc{\B} \end{pmatrix} \begin{pmatrix} \s \\ \t\end{pmatrix} + \begin{pmatrix}\I &amp; \I\end{pmatrix}\begin{pmatrix} \oc{\bmu} \\ \oc{\bnu}\end{pmatrix} \\ 
&amp;= \begin{pmatrix}\bc{\A}&amp;\bc{\B}\end{pmatrix} \s' +\oc{\bmu} + \oc{\bnu} \p
\end{align*}\]

  </p><p>Which tells us that the mean is $\oc{\bmu} + \oc{\bnu}$ and the covariance matrix is $\begin{pmatrix}\bc{\A} &amp; \bc{\B}\end{pmatrix}\begin{pmatrix}\bc{\A} &amp; \bc{\B}\end{pmatrix}^T = \bc{\A}\bc{\A}^T + \bc{\B}\bc{\B}^T $.
<span></span></p>
</div>

<h3 id="chaining-gaussians">Chaining Gaussians</h3>



<p>Here’s a situation that comes up occasionally. We sample a vector $\a$ from one Gaussian, and then make this the mean of another Gaussian. We then sample $\b$ from the second Gaussian. What’s the distribution on $\b$? If we are given the values of $\a$, it’s a Gaussian, that’s how we defined it. But what about $p(\b)$. That is, what if someone told us only they had followed this process, but they didn’t tell us what the value of $\a$ was? What probabilities would we assign to a given value of $\b$?</p>

<p>An example is trying to saw one plank to the length of another. You measure one plank and then saw the other to the length of your measurement. Both steps have some error: there is some error in how accurately you measure, and some error in how accurately you saw. Both processes are probably Gaussian: if you repeat the measurement or the sawing and plot the results, a bell shape will appear.</p>

<figure>
<img src="https://gestalt.ink/images/gaussians/planks.svg">
<figcaption>We <span>measure</span> plank 1, and then <span>saw</span> plank 2 to the measured length. Both actions have some inaccuracy in the form of Gaussian noise. Can we show that the distribution on <span>the end result</span> (a) is Gaussian and (b) has the length of the first plank as its mean?  
</figcaption>
</figure>

<p>The question now is what distribution we get if we don’t know the measurement. Or, if you like, if we repeat the whole experiment many times. What will the distribution be on the length of plank we saw, combining both the uncertainty in the measuring and in the cutting.</p>

<p>It turns out that this distribution is Gaussian as well. One way to think of this distribution is as a <em>convolution</em> of the two Gaussians we used for sampling. At every point $\x$ in space we place a Gaussian. The probability density is a mixture of all these Gaussians, weighted by how likely we are to put a Gaussian at $\x$. Put differently, the probability $p(\y)$ assigned to some point is a weighted “sum”—or more precisely an integral—of all the Gaussians we could sample in the first step, all weighted by how likely they are to be sampled.</p>

<figure>
<img src="https://gestalt.ink/images/gaussians/convolution.svg">
<figcaption>Imagine placing copies of <span>one Gaussian</span> along the number line at various points. We then take a weighted sum of these Gaussians, where the weight is the density of the point according to <span>a second Gaussian</span>. The result, as the number of points goes to infinity, is the <em>convolution</em> of the two Gaussians. 
</figcaption>
</figure>

<p>We could use this integral to work out the shape of $p(\y)$, but that would require lots of calculus. Instead, we will use our geometric perspective to take a shortcut.</p>

<p><strong>Theorem. Gaussian convolution.</strong> Let 
$$\begin{align*}
\a &amp;\sim N(\oc{\bmu}, \bc{\Sig}) \text{ and }\\ 
\b &amp;\sim N(\a, \bc{\bc \T}) \p 
\end{align*}$$
Then, 
$$p(\b) = N(\oc{\bmu}, \bc{\Sig} + \bc{\bc \T}) \p$$
</p>
<div>
  <p><span>Proof.</span> From our geometric definition, we can rewrite $\a$ as 
\(\a = \bc{\A} \s + \oc{\bmu}\)
with $\s$ a standard-normally distributed vector, and $\bc{\Sig} = \bc{\A\A}^T$. Likewise we can write,
\(\b = \bc{\B} \t + \a\)
with $\t$ a <em>separate</em> standard normally distributed vector and $\bc{\T} = \bc{\B\B}^T$. Note that $\a$ takes the roles of the translation vector in the definition of $\b$.</p>

  <p>In this view, we sample $\s$ and $\t$, and then compute $\a$ and $\b$ from them as regular vectors. That means we can plug the definition of $\a$ into that of $\b$ and get</p><p>

\[\begin{align*}
\b = \bc{\B} \t + \bc{\A} \s + \bmu \p
\end{align*}\]

  </p><p>The first two terms, $\bc{\B} \t + \bc{\A} \s$ form the sum of two zero centered Gaussians. By the result of the previous section, this is equal to a single Gaussian with covariance $\bc{\Sig} + \bc{\bc \T}$.</p>
</div>

<p>In the geometric view, we can say that $\b = \bc{\Y}\u + \oc{\bmu}$, with $\u$ standard normally distributed and $\bc{\Y}\bc{\Y}^T = \bc{\A}\bc{\A}^T + \bc{\bc \B}\bc{\bc \B}^T$</p>

<p>We’ll work out the spherical case specifically as a corollary, since it’s so central to diffusion models.</p>

<p><strong>Corollary. Spherical Gaussian convolution</strong> Define $\a$ and $\b$ as before, but with the constraint that they are spherical Gaussians with scalar standard deviations $\bc{\sigma}$ and $\bc{\tau}$ respectively. Then
$$\begin{align*} 
\b &amp;\sim N(\oc{\bmu}, \bc{\sigma}^2 + \bc{\tau}^2) \;\text{and}\\
\b &amp;= \sqrt{\bc{\sigma}^2 + \bc{\tau}^2}\,\s + \oc{\bmu} 
\end{align*}$$
</p>
<div>
  <p><span>Proof.</span> Take the result from the proof and let $\bc{\Sig}$ be diagonal matrix with every $\bc{\Sigma}_{ii} = \bc{\sigma}$ and likewise for $\bc{\T}$. Then $\bc{\Sig}\bc{\Sig}^T$ is a diagonal matrix with all diagonal elements equal to $\bc{\sigma}^2$ and likewise for $\bc{\T}\bc{\T}^T$ so that</p><p>

\[N(\oc{\bmu}, \bc{\Sig}\bc{\Sig}^T + \bc{\bc \T}\bc{\bc \T}^T) = N(\oc{\bmu}, \bc{\sigma}^2 + \bc{\tau}^2 ) \p\]

  </p><p>For the geometric definition of $\b$, note that the transformation matrix $\bc{\A}$ should have the property that $\bc{\Sig} = \bc{\A\A}^T$. Since $\bc{\Sig}$ is the diagonal matrix $(\bc{\sigma}^2 + \bc{\tau}^2)\I$, we can derive $\bc{\A}$ simply by taking the square root of these diagonal elements</p><p>

\[\bc{\A} = \sqrt{\bc{\sigma}^2 + \bc{\tau}^2}\I \p\]

  </p>
</div>



<h3 id="conditioning-gaussians">Conditioning Gaussians</h3>

<p>What if we want to <em>condition</em> $\x$ on one or more of its values? For instance, we are interested in the distribution $p(\x \mid x_2=3)$ where $\x$ is drawn from a Gaussian. We can show that the result is, again, a Gaussian.</p>

<p>For a real-world example, we can look at our population of female soldiers again. If the combination of their heights and the weights is normally distributed, then what happens if we slice out only those soldiers that are 192cm tall? Do we get a Gaussian distribution on the weights in this subpopulation?</p>

<p>This one is a little more complex to prove. We will start with a lemma showing a single, specific, case. If $\x$ is drawn from the standard normal distribution $N(\zero, \I)$, and we condition on one of the elements having a particular value $c$, then the resulting distribution $p(\x\mid x_i = c)$ is standard normal on the remaining elements of $\x$. This result will require us to open the box and to look at the formula for $N(\zero, \I)$, but as we saw earlier, this formula is relatively straightforward.</p>

<p>With that lemma in place, we can then show our main result: that for <em>any</em> variable $y$ with a Gaussian distribution, conditioning on one of the elements of $\y$ results in another Gaussian. This, we can do entirely by the affine operation trick.</p>

<p><strong>Lemma. Gaussian conditioning</strong> Let $\x \sim N^n(\zero, \I)$. Then for any element $x_i$, and value $c$,
$$
p(\x \mid x_i = \rc{c})
$$
is a standard Gaussian $N^{n-1}(\zero, \I)$ on the remaining elements of $\x$.
</p>
<div>
  <p><span>Proof.</span> To start with, consider how this conditional distribution is defined. In two dimensions, the situation looks like this.</p>

  <figure>
<img src="https://gestalt.ink/images/gaussians/standard-conditional.svg">
</figure>

  <p>The constraint $x_i = \rc{c}$ tells us that we assume that $\x$ is on the red line. The probability density for points that are not on the line becomes zero. The density for points on the line stays the same, but should be rescaled uniformly so that the probability density, if we integrate over the whole line becomes 1.</p>

  

  <p>Extending this to $n$ dimensions, if we condition on one element $x_i$ of $\x$, the result is that the line becomes an $n-1$ dimensional hyperplane orthogonal to the $i$-th axis. For any point in this hyperplane, we take the probability density under $N^n(\zero, \I)$ and rescale it, so that the whole hyperplane integrates to 1.</p>

  <p>This integral sounds like a tricky one to work out. Luckily, we don’t have to. We just assume it exists, and work around it with the “proportional to” trick we saw earlier.</p>

  <p>To make the notation simpler, we will assume, <a href="https://gestalt.ink/wlog">without loss of generality</a>, that $x_i$ is the last element of $\x$, that is $x_n$. We call the vector $\x$ with the $n$-th element removed $\x_{\setminus n}$</p>

  <p>Then if $\x$ has $x_n=\rc{c}$, we have</p>

  <p>$$\begin{align*}
p(\x \mid x_n = \rc{c}) &amp;\propto N^n(\zero, \I) \\
&amp;= \text{exp} - \frac{1}{2} \|\x\|^2 \\
&amp;= \text{exp} - \frac{1}{2} \left ({x_1}^2 + \ldots + {x_{n-1}}^2 + x_n \right) \\
&amp;= \text{exp} - \frac{1}{2} \left ({x_1}^2 + \ldots + {x_{n-1}}^2 + \rc{c}\right) \\
&amp;= \text{exp} - \frac{1}{2} \left ({x_1}^2 + \ldots + {x_{n-1}}^2 \right) \cdot \rc{ \text{exp} - \frac{1}{2} c} \\
&amp;\propto \text{exp} - \frac{1}{2} \left ({x_1}^2 + \ldots + {x_{n-1}}^2 \right) = \text{exp} -\frac{1}{2} \|\x_{\setminus n}\|^2 \\ 
&amp;= N^{n-1}(\x_{\setminus n}\mid \zero, \I) \p
\end{align*}$$</p>

  <p>We see that the probability density that $p(\x \mid x_n =\rc{c})$ assigns to the vector $\x$, if $x_n=\rc{c}$, is proportional to the density that $N^{n-1}(\x_{\setminus n})$ assigns to the first $n-1$  elements of $\x$. Normally, to turn this into a fully determined probability function, we need to figure out what this integrates to and divide by that to turn the $\propto$ into a $=$. However, in this case, we know what the right-hand side integrates to, because $N^{n-1}$ is already a proper probability density function, and we are allowing all possible values for $\x_{\setminus n}$. It integrates to $1$, so we can simply say that</p><p>

\[p(\x \mid x_n = \rc{c}) = N^{n-1}(\zero, \I) \p\]

  </p>
</div>

<p>Why doesn’t this argument hold for Gaussians in general? It’s the “orthogonal” structure of the standard Gaussian. This allows us to remove one dimension, after which we are left simply with a standard Gaussian of one dimension fewer.</p>

<p>However, we can build on this result to show that conditioning in general produces a Gaussian. All we need to do is to show that a conditioned Gaussian can be transformed <em>to</em> a conditioned standard normal Gaussian. Since anything that can be transformed to a Gaussian is itself Gaussian (so long as the transformation has linearly independent columns), this proves the result.</p>

<p><strong>Theorem. Gaussian conditioning.</strong> If we sample $\x$ from any Gaussian with full support, and condition on one of its elements, the resulting distribution, $p(\x \mid x_i = \rc{c})$, is Gaussian. 
</p>
<div>
  <p><span>Proof.</span></p>

  <figure>
<img src="https://gestalt.ink/images/gaussians/conditional-proof.svg">
<figcaption>
      <p>The key idea of the proof. The $\s$’s that result in samples $\x$ that satisfy $x_i = \rc{c}$ form a hyperplane constraint on the standard Gaussian on $\s$. With a simple rotation, which doesn’t affect the density, we translate to the situation of the lemma. We can now say that we can transform <em>from</em> $p(\x \mid x_i = \rc{c})$ to $p(\s \mid s_j = \rc{c}’)$ by an invertible, affine operation. As we showed earlier, this means that $p(\x \mid x_i = \rc{c})$ must be Gaussian.</p>
    </figcaption>
</figure>

  <p>Since $p(\x)$ is Gaussian, there is some invertible $\bc{\A}$ and $\oc{\t}$ so that $\x = \bc{\A}\s + \oc{\t}$ with $\s \sim N(\zero, \I)$. This means that $x_i = \bc{\a}_i\s + \oc{t}_i$, where $\bc{\a}_i$ is the $i$-th row of $\bc{\A}$.</p>

  <p>Our conditioning $x_i = \rc{c}$, gives us $\bc{\a}_i\s + \oc{t}_i = \rc{c}$, a linear constraint on the values of $\s$. Since it’s an extra constraint in one variable, it essentially means that if we know all values of $\s$ except one, say $s_1$, then we can work out what $s_1$ must be. We can show this with some simple re-arranging:</p><p>

\[\begin{align*}
y_i = \rc{c} &amp;= \bc{A}_{i1}s_1 + \ldots + \bc{A}_{in}s_n + \oc{t}_i\\
s_1 &amp;= - \frac{1}{\bc{A}_{i1}} \left (\bc{A}_{i2}s_2 + \ldots + \bc{A}_{in}s_n + \oc{t}_i - \rc{c}\right) \p
\end{align*}\]

  </p><p>The last line represents a constraint on $\s$. We’ll refer to this constraint as $c(\s)$, a boolean function which is true if the constraint holds for $\s$.</p>

  <p>Now, since $c(\s)$ linearly expresses one element of $\s$ in terms of the other $n-1$, the $\s$’s that satisfy it form an $n-1$ dimensional hyperplane. It’s not axis-aligned, as it was in the lemma before, but that can be fixed with a simple rotation. Let $\R$ be an orthogonal matrix such that the transformation</p>

  <p>$\z = \R\s$</p>

  <p>when applied to the hyperplane $c(\s)$ yields a hyperplane orthogonal to the $n$-th axis.</p>

  <p>Since $N(\zero, I)$ is rotationally symmetric, the density of any point $\s$ remains unaffected when it is mapped to $\z$. This tells us that $p(\s \mid c(\s)) = p(\z \mid z_n = \rc{c}’)$ for some value $\rc{c}’$.</p>

  <p>And with that, we can apply our lemma. $p(\z \mid z_n=\rc{c}’)$ is a standard Gaussian, by the lemma. $p(\x \mid c(\x))$ is an orthogonal transformation of it, so also a standard Gaussian, and $p(\x \mid x_k=\rc{c})$ is an affine transformation of that (with an invertible matrix), so also Gaussian.</p>

  
</div>



<p>Finally, if we want to condition on more than one element of $\x$, we could repeat the same proof structure any dimension of hyperplane, but it’s simpler to just apply the theorem multiple times.</p>

<p><strong>Corollary. Gaussian conditioning on multiple elements.</strong> If we sample $\y$ from any Gaussian, and condition on $m$ of its elements, the resulting distribution is Gaussian. 
</p>
<p><span>Proof.</span> Assume we have a Gaussian $p(x_1, \ldots, x_n)$. Conditioning on $x_1$ gives us, by the theorem, a Gaussian $p(x_2 \ldots, x_{n} \mid x_1)$. Since the latter is a Gaussian, we can condition on one of its elements and, by the theorem get another Gaussian $p(x_3, \ldots x_n \mid x_1, x_2)$. We can do this for any number of elements, and in any order we like.</p>

<p><strong>question:</strong> What if you want to know not just whether the conditional a Gaussian is, but <em>which</em> Gaussian? I.e. what are its parameters? How would you proceed? Which elements of the proof would you need to work out in greater detail?</p>

<h2 id="deriving-the-density">Deriving the density</h2>

<p>To finish, we will see where that gargantuan formula comes from. With the picture we have built up, of Gaussians as affine transformations of a single standard Gaussian, it’s not so complex to derive. We just need one small trick.</p>

<p>To build our intuition, let’s look at a very simple 2D Gaussian, as defined by the transformation</p>

\[\x = \bc{\I}\s + \oc{\begin{pmatrix} 1 \\ 1\end{pmatrix} } \p\]

<p>That is, we don’t squish or stretch the standard-normal distribution, we just shift it around a bit, <em>translate</em> it.</p>

<p>To figure out what the density of a particular point $\x$ is, all we need to do is shift it back. For example, if $\x = \begin{pmatrix} 2 \\ 1\end{pmatrix}$, a point one unit above the mean, we can work out the density by shifting this point back to $\begin{pmatrix}1 \\ 0\end{pmatrix}$, the point one unit above the mean of the standard Gaussian. Since the two distributions are just translations of each other, these points will have the same density under their respective distributions.</p>

<figure>
<img src="https://gestalt.ink/images/gaussians/translate.svg">
</figure>

<p>This tells us that we can express the density of our new function in terms of the density function we already have for the standard Gaussian.</p>

\[N(\x \mid \bc{\I}, \oc{\t}) = N(\s - \oc{\t} | \bc{\I}, \oc{\zero})\]

<p>Now, we know the density function of the standard Gaussian, that’s</p>

<p>$$
N(\s \mid \bc{\I}, \zero) = \kc{\frac{1}{z}} \text{exp }{- \tfrac{1}{2}\|\s\|^2} \p
$$</p>

<p>So, if we fill in $\s = \x -\oc{\t}$ we get</p>

<p>$$
N(\x \mid \bc{\I}, \oc{\t}) = \kc{\frac{1}{z}} \text{exp }{- \tfrac{1}{2}\|\x - \oc{\t}\|^2} \p
$$</p>



<p>The idea is simple: for a given Gaussian expressed as a transformation of the standard Gaussian, we transform $\x$ back to the standard Gaussian, by inverting the transformation, and then we just read off the density.</p>

<p>Can we apply the same idea to the transformation matrix $\bc{\A}$? Here we have to be a bit more careful. As we transform by $\bc{\A}$, it may stretch or shrink space. It’s easiest to see what might go wrong in the 1D case:</p>

<figure>
<img src="https://gestalt.ink/images/gaussians/inflation.svg">
</figure>

<p>As you can see, if we have $\bc{\sigma}=\bc{1/2}$, then after we multiply by $1/\bc{\sigma} = \bc{2}$, the whole function blows up as a result. This means that the area under the curve will no longer sum to 1. Luckily, the increase is simply a factor of $\bc{\sigma}$, so if we apply the change of variables naively, all we have to do is divide the result by $1/\bc{\sigma}$ to correct the error.</p>

<p>If you’re not convinced, imagine approximating the bell curve by a series of boxes. Multiplying by $\bc{\sigma}$ stretches each box horizontally but not vertically, so the area goes from height $times$ width to height $\times$ width $\times \bc{\sigma}$. When we sum over all boxes, we can take $\bc{\sigma}$ out of the sum to see that the total area is multiplied once by $\bc{\sigma}$. Now let the width of the boxes shrink to get a better and better approximation.</p>

<p>The same reasoning applies in higher dimensions. Assume we have our Gaussian described in terms of an invertible transformation matrix $\bc{\A}$—which, as we showed before, is always possible.</p>

<p>We can carve up the plane of a 2D Gaussian into squares, and approximate the <em>volume</em> under the surface with a series of rectangular columns on top of these.</p>

<p>The result of transforming the Gaussian by $\bc{\A}$ is that the squares are stretched into parallelograms. Happily, because it’s a linear transformation, every square is stretched into a parallelogram of the same size. We know much the surface area shrinks or increases, because that’s simply the determinant of $\bc{\A}$.</p>

<p>With that, we can establish our formula for the density. We know that $\x = \bc{\A}\s + \oc{\t}$ defines our Gaussian. To work out the density of $\x$, all we need to do is invert our function to find the corresponding $\s$, take its density, and then <span>correct</span> for the amount by which $\bc{\A}$ inflates space by multiplying with $1/|\bc{\A}|$.</p>

<p>$$\begin{align*}
N(\x \mid \bc{\A}, \oc{\t}) &amp;= N(\s = \bc{\A}^{-1}(\x - \oc{t}) \mid \I, \zero) \\
&amp;=  \rc{ \frac{1}{|{\A}|} } \rc{\times} \frac{1}{\bc{z}} \;\text{exp } - \| \bc{\A}^{-1}(\x - \oc{t}) \|^2 \\
&amp;=  \frac{1}{z |\bc{\A}|} \; \text{exp } - (\bc{\A}^{-1}(\x - \oc{t}))^T(\bc{\A}^{-1}(\x - \oc{t})) \\
&amp;=  \frac{1}{z |\bc{\A}|} \; \text{exp } - (\x - \oc{t})^T(\bc{\A}^T\bc{\A})^{-1}(\x - \oc{t}) \\
\end{align*}$$</p>

<p>In the last two lines, we've used the properties that $\|z\|^2 = \z^T\z$, that $\A^{-1}\B^{-1} = (\B\A)^{-1}$ and that $(\A^{-1})^T = (\A^T)^{-1}$.
</p>

<p>This is the density function of a Gaussian expressed in the geometric parameters. If we want to translate this to the more common parametrization in terms of the covariance matrix $\bc{\Sig} = \bc{\A}^T\bc{\A}$, we just need to note that the determinant has the properties that $|\A\B| = |\A|\times |\B|$ and $|\A^T| = |\A|$, so that</p>

\[|\bc{\A}| = |\bc{\A}^T\bc{\A}|^{\frac{1}{2}} = |\bc{\Sig}|^{\frac{1}{2}} \p\]

<p>Filling this in, we get</p>

\[\frac{1}{\bc{z} |\bc{\Sig}|^{\frac{1}{2}} } \;\text{exp } - (\x - \oc{t})^T\bc{\Sig}^{-1}(\x - \oc{t}) \p\]

<p>This is exactly the formula we started with at the top of the article, except that we haven’t bothered to work out $\bc{z} = \int _{-\infty}^\infty \text{exp } -\tfrac{1}{2} |\x|^2$. This is called the <a href="https://gestalt.ink/gaussian-integral">Gaussian integral</a>, and as you can tell from the completed formula, it works out to $(2\pi)^{-\frac{1}{2}d}$, where $d$ is our dimension. We will leave this working out to another article.
</p>

<p>You may ask what happens if $\bc{\A}$ is not invertible. Well, then $\bc{\Sig}$ isn’t either, and the traditional parametrization breaks down. However, in the geometric parametrization, we have some hope left. We know that we still have a Gaussian on our hands, with a bell shape and everything. It’s just that it only covers a linear subset of space.</p>



<h2 id="sources-and-other-materials">Sources and other materials</h2>

<ul>
  <li>We discuss these same topics in our <a href="https://dlvu.github.io/diffusion/">lecture on Diffusion models</a> in the <a href="https://dlvu.github.io/">deep learning course at the Vrije Universiteit Amsterdam</a>. This lecture is a little more compact than this article, which may help you to get the general overview. It comes with videos (or will do soon).</li>
</ul>

<h2 id="references">References</h2>

<p>[1] Paquette, S. (2009). <a href="https://www.openlab.psu.edu/ansur2/">Anthropometric survey (ANSUR) II pilot study</a>: methods and summary statistics. Anthrotch, US Army Natick Soldier Research, Development and Engineering Center.</p>

	   </article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Singapore OKs 4,300km subsea cable for importing electricity from Australia (104 pts)]]></title>
            <link>https://mothership.sg/2024/10/ema-conditional-approval-sun-cable/</link>
            <guid>41912103</guid>
            <pubDate>Tue, 22 Oct 2024 07:44:24 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://mothership.sg/2024/10/ema-conditional-approval-sun-cable/">https://mothership.sg/2024/10/ema-conditional-approval-sun-cable/</a>, See on <a href="https://news.ycombinator.com/item?id=41912103">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
<p><a href="https://mothership.sg/category/news">
News
</a></p>
<p>The project is slated for operation after 2035. </p>

</div><div>
<p><a href="https://bit.ly/3qgqzHg" target="_blank" rel="noopener noreferrer"><img src="https://static.mothership.sg/1/2023/07/telegram-button.png" alt="Telegram" width="700" height="73"></a> </p><p><a href="https://bit.ly/3KjTj94" target="_blank" rel="noopener noreferrer"><img src="https://static.mothership.sg/1/2023/08/wa-button.png" alt="Whatsapp" width="700" height="73"></a> </p><p>The Energy Market Authority (EMA) has granted <a href="https://www.suncable.energy/" target="_blank" rel="noopener noreferrer">Sun Cable</a> Conditional Approval to supply electricity from Australia to Singapore via subsea cables spanning over 4,300 km. </p><p>Approval was granted to import 1.75 gigawatts (GW) of low-carbon electricity from Northern Australia into Singapore, announced Second Minister for Trade and Industry Tan See Leng on Oct. 22 at the Asia Clean Energy Summit during the Singapore International Energy Week (SIEW) 2024. </p>
<p>This will account for approximately nine per cent of Singapore's total electricity needs. </p><p>According to a press release by the EMA, the project was recognised as "technically and commercially viable" based on the proposal and information submitted. </p><p>The imported electricity is expected to harness solar power, and the Conditional Approval will provide the Sun Cable project with the support it needs to continue. </p><p>It is slated for operation after 2035. </p><p>The project has previously been approved by the Australian government as well. </p><h2>Decarbonising the power sector</h2> <p>Currently, the power sector accounts for 40 per cent of Singapore's carbon emissions. </p><p>The approval comes at a time when Singapore is actively trying to decarbonise its energy supply, one way of which is to import low-carbon electricity. </p><p>For one, EMA has a goal of importing 6 GW of low-carbon electricity by 2035. </p><p>EMA had also previously <a href="https://mothership.sg/2024/08/4300km-undersea-australia-singapore-approved/" target="_blank" rel="noopener noreferrer">shared</a> with<em> Mothership</em> that Singapore is on track to import up to 4 GW of low-carbon electricity by 2035, which would make up about 30 per cent of Singapore's electricity supply by then. </p><h2>The pathway from a "Conditional License" to "Conditional Approval"</h2> <p>Before the Sun Cable project can be awarded a Conditional License, it will have to demonstrate fulfillment of EMA's required conditions: </p><ol> <li>Comply with EMA’s technical requirements</li> <li>Achieve a commercially viable price acceptable to customer</li> <li>Secure all requisite approvals of relevant jurisdictions, including countries which the cables will pass through</li> </ol> <p>At present, 2 GW of Conditional Licenses for electricity imports from Indonesia have been granted by the EMA, as well as 3.6 GW with Conditional Approval for electricity imports. </p><p>Of the 3.6 GW, 1.4 GW comes from Indonesia, 1 GW from Cambodia and 1.2 GW from Vietnam. </p><p>The energy is sourced from a mix of solar, hydropower and wind energy plants, said EMA. </p><h2>EMA to continue exploring decarbonisation pathways</h2> <p>EMA will continue to grant Conditional Approval to companies with credible and commercially viable proposals, and will also continue exploring alternative decarbonisation pathways. </p><blockquote><ul></ul> <p>"To keep pace with our energy demand, EMA will continue to engage all companies with credible and commercially viable proposals that can contribute to Singapore’s 2050 net zero ambitions. To ensure adequate supply to meet our future energy needs given growing demand, EMA will continue to grant Conditional Approval to companies with credible and commercially viable proposals. EMA will also continue to explore all decarbonisation pathways for the power sector, including hydrogen, solar, deep geothermal energy, nuclear energy, as well as carbon capture and storage technologies, as part of the energy transition towards a low-carbon future."</p></blockquote> <h2>Related stories</h2> <p><em>Top image from Sun Cable and Canva</em></p>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Guide to Fine-Tuning LLMs (148 pts)]]></title>
            <link>https://arxiv.org/abs/2408.13296</link>
            <guid>41911255</guid>
            <pubDate>Tue, 22 Oct 2024 04:41:33 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://arxiv.org/abs/2408.13296">https://arxiv.org/abs/2408.13296</a>, See on <a href="https://news.ycombinator.com/item?id=41911255">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="content-inner">
    
    
                
    <p><a href="https://arxiv.org/pdf/2408.13296">View PDF</a>
    <a href="https://arxiv.org/html/2408.13296v2">HTML (experimental)</a></p><blockquote>
            <span>Abstract:</span>This report examines the fine-tuning of Large Language Models (LLMs), integrating theoretical insights with practical applications. It outlines the historical evolution of LLMs from traditional Natural Language Processing (NLP) models to their pivotal role in AI. A comparison of fine-tuning methodologies, including supervised, unsupervised, and instruction-based approaches, highlights their applicability to different tasks. The report introduces a structured seven-stage pipeline for fine-tuning LLMs, spanning data preparation, model initialization, hyperparameter tuning, and model deployment. Emphasis is placed on managing imbalanced datasets and optimization techniques. Parameter-efficient methods like Low-Rank Adaptation (LoRA) and Half Fine-Tuning are explored for balancing computational efficiency with performance. Advanced techniques such as memory fine-tuning, Mixture of Experts (MoE), and Mixture of Agents (MoA) are discussed for leveraging specialized networks and multi-agent collaboration. The report also examines novel approaches like Proximal Policy Optimization (PPO) and Direct Preference Optimization (DPO), which align LLMs with human preferences, alongside pruning and routing optimizations to improve efficiency. Further sections cover validation frameworks, post-deployment monitoring, and inference optimization, with attention to deploying LLMs on distributed and cloud-based platforms. Emerging areas such as multimodal LLMs, fine-tuning for audio and speech, and challenges related to scalability, privacy, and accountability are also addressed. This report offers actionable insights for researchers and practitioners navigating LLM fine-tuning in an evolving landscape.
    </blockquote>

    <!--CONTEXT-->
    
  </div><div>
      <h2>Submission history</h2><p> From: Arsalan Shahid [<a href="https://arxiv.org/show-email/b7e5f345/2408.13296">view email</a>]      <br>            <strong><a href="https://arxiv.org/abs/2408.13296v1">[v1]</a></strong>
        Fri, 23 Aug 2024 14:48:02 UTC (13,396 KB)<br>
    <strong>[v2]</strong>
        Mon, 21 Oct 2024 11:10:00 UTC (13,398 KB)<br>
</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Firefox-Passwords-Decryptor: Extracts and decrypts passwords saved in Firefox (105 pts)]]></title>
            <link>https://github.com/Sohimaster/Firefox-Passwords-Decryptor</link>
            <guid>41911176</guid>
            <pubDate>Tue, 22 Oct 2024 04:23:53 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/Sohimaster/Firefox-Passwords-Decryptor">https://github.com/Sohimaster/Firefox-Passwords-Decryptor</a>, See on <a href="https://news.ycombinator.com/item?id=41911176">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><p dir="auto"><h2 tabindex="-1" dir="auto">Firefox Password Decryptor</h2><a id="user-content-firefox-password-decryptor" aria-label="Permalink: Firefox Password Decryptor" href="#firefox-password-decryptor"></a></p>
<p dir="auto">This tool is primarily designed for decrypting and extracting passwords stored in Firefox, offering an in-depth look into the security of saved credentials. It provides additional reconnaissance capabilities such as system info, open ports info, devices info, and Firefox browsing history extraction.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Main Feature: Firefox Passwords Decryption</h2><a id="user-content-main-feature-firefox-passwords-decryption" aria-label="Permalink: Main Feature: Firefox Passwords Decryption" href="#main-feature-firefox-passwords-decryption"></a></p>
<ol dir="auto">
<li>
<p dir="auto"><strong>Global Salt Retrieval</strong>: Initiates by fetching the global salt from the <code>key4.db</code>, laying the groundwork for key generation.</p>
</li>
<li>
<p dir="auto"><strong>Key Generation via PBKDF2</strong>: Employs PBKDF2 with SHA-256 hashing to craft a decryption key from the global salt, a critical step for accessing encrypted data.</p>
</li>
<li>
<p dir="auto"><strong>Parsing ASN.1 Encoded Data</strong>: Analyzes ASN.1 encoded structures to extract encrypted credentials alongside their respective algorithms and initialization vectors (IVs).</p>
</li>
<li>
<p dir="auto"><strong>AES/Triple DES Decryption</strong>: Depending on the specified algorithm, decrypts the credentials using the generated key and IV.</p>
</li>
<li>
<p dir="auto"><strong>Padding Removal</strong>: Strips padding from decrypted data to unveil plaintext usernames and passwords.</p>
</li>
</ol>
<p dir="auto"><h2 tabindex="-1" dir="auto">Additional Features</h2><a id="user-content-additional-features" aria-label="Permalink: Additional Features" href="#additional-features"></a></p>
<ul dir="auto">
<li><strong>System Info</strong>: Gathers system information including hostname, OS, architecture, CPU count, and memory.</li>
<li><strong>Ports Info</strong>: Enumerates open ports and identifies the processes using them.</li>
<li><strong>Devices Info</strong>: Lists connected USB devices.</li>
<li><strong>Firefox Browsing History</strong>: Extracts and displays the user's Firefox browsing history.</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Getting Started</h2><a id="user-content-getting-started" aria-label="Permalink: Getting Started" href="#getting-started"></a></p>
<p dir="auto">Clone the repository and compile the source to begin using the toolkit:</p>
<div dir="auto" data-snippet-clipboard-copy-content="git clone https://github.com/yourusername/Firefox-Passwords-Decryptor.git
cd Firefox-Passwords-Decryptor
go build"><pre>git clone https://github.com/yourusername/Firefox-Passwords-Decryptor.git
<span>cd</span> Firefox-Passwords-Decryptor
go build</pre></div>
<p dir="auto"><h3 tabindex="-1" dir="auto">Command-Line Options</h3><a id="user-content-command-line-options" aria-label="Permalink: Command-Line Options" href="#command-line-options"></a></p>
<p dir="auto">Execute the tool with specific flags to utilize its various features:</p>
<ul dir="auto">
<li><code>-passwords</code>: Decrypt and display Firefox passwords (primary feature).</li>
<li><code>-sysinfo</code>: Retrieve system information.</li>
<li><code>-ports</code>: List information on open ports.</li>
<li><code>-devices</code>: Display connected USB devices.</li>
<li><code>-history</code>: Extract Firefox browsing history.</li>
</ul>
<p dir="auto">For instance, to decrypt Firefox passwords and view system info:</p>
<div dir="auto" data-snippet-clipboard-copy-content="./Firefox-Passwords-Decryptor -passwords -sysinfo

>>>

System info:  {
  &quot;Hostname&quot;: &quot;hostname&quot;,
  &quot;Os&quot;: &quot;linux&quot;,
  &quot;Arch&quot;: &quot;amd64&quot;,
  &quot;CpusCount&quot;: 1,
  &quot;MemoryCount&quot;: 31244
}
Firefox profile path:  /home/username/.mozilla/firefox/filename.default-release
Db path:  /home/username/.mozilla/firefox/filename.default-release/key4.db
Logins:  [
  {
    &quot;Username&quot;: &quot;username&quot;,
    &quot;Password&quot;: &quot;password&quot;,
    &quot;URL&quot;: &quot;example.com&quot;
  },
  {
    &quot;Username&quot;: &quot;username&quot;,
    &quot;Password&quot;: &quot;password&quot;,
    &quot;URL&quot;: &quot;example.com&quot;
  },
]"><pre>./Firefox-Passwords-Decryptor -passwords -sysinfo

&gt;&gt;&gt;

System info:  {
  <span><span>"</span>Hostname<span>"</span></span>: <span><span>"</span>hostname<span>"</span></span>,
  <span><span>"</span>Os<span>"</span></span>: <span><span>"</span>linux<span>"</span></span>,
  <span><span>"</span>Arch<span>"</span></span>: <span><span>"</span>amd64<span>"</span></span>,
  <span><span>"</span>CpusCount<span>"</span></span>: 1,
  <span><span>"</span>MemoryCount<span>"</span></span>: 31244
}
Firefox profile path:  /home/username/.mozilla/firefox/filename.default-release
Db path:  /home/username/.mozilla/firefox/filename.default-release/key4.db
Logins:  [
  {
    <span><span>"</span>Username<span>"</span></span>: <span><span>"</span>username<span>"</span></span>,
    <span><span>"</span>Password<span>"</span></span>: <span><span>"</span>password<span>"</span></span>,
    <span><span>"</span>URL<span>"</span></span>: <span><span>"</span>example.com<span>"</span></span>
  },
  {
    <span><span>"</span>Username<span>"</span></span>: <span><span>"</span>username<span>"</span></span>,
    <span><span>"</span>Password<span>"</span></span>: <span><span>"</span>password<span>"</span></span>,
    <span><span>"</span>URL<span>"</span></span>: <span><span>"</span>example.com<span>"</span></span>
  },
]</pre></div>
<p dir="auto"><h2 tabindex="-1" dir="auto">Dependencies</h2><a id="user-content-dependencies" aria-label="Permalink: Dependencies" href="#dependencies"></a></p>
<p dir="auto">Ensure the following packages are installed:</p>
<div dir="auto" data-snippet-clipboard-copy-content="go get -u github.com/google/gousb github.com/mattn/go-sqlite3 github.com/pkg/errors"><pre>go get -u github.com/google/gousb github.com/mattn/go-sqlite3 github.com/pkg/errors</pre></div>
<p dir="auto"><h2 tabindex="-1" dir="auto">Contributing</h2><a id="user-content-contributing" aria-label="Permalink: Contributing" href="#contributing"></a></p>
<p dir="auto">Feel free to submit pull requests, open issues for discussion, or suggest improvements.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">License</h2><a id="user-content-license" aria-label="Permalink: License" href="#license"></a></p>
<p dir="auto">This project is distributed under the MIT License - see <a href="https://github.com/Sohimaster/Firefox-Passwords-Decryptor/blob/main/LICENSE">LICENSE</a> for details.</p>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[LTESniffer: An Open-Source LTE Downlink/Uplink Eavesdropper (224 pts)]]></title>
            <link>https://github.com/SysSec-KAIST/LTESniffer</link>
            <guid>41910084</guid>
            <pubDate>Tue, 22 Oct 2024 00:42:51 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/SysSec-KAIST/LTESniffer">https://github.com/SysSec-KAIST/LTESniffer</a>, See on <a href="https://news.ycombinator.com/item?id=41910084">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><p dir="auto"><h2 tabindex="-1" dir="auto">LTESniffer - An Open-source LTE Downlink/Uplink Eavesdropper</h2><a id="user-content-ltesniffer---an-open-source-lte-downlinkuplink-eavesdropper" aria-label="Permalink: LTESniffer - An Open-source LTE Downlink/Uplink Eavesdropper" href="#ltesniffer---an-open-source-lte-downlinkuplink-eavesdropper"></a></p>
<p dir="auto"><strong>LTESniffer</strong> is An Open-source LTE Downlink/Uplink Eavesdropper</p>
<p dir="auto">It first decodes the Physical Downlink Control Channel (PDCCH) to obtain the Downlink Control Informations (DCIs) and Radio Network Temporary Identifiers (RNTIs) of all active users. Using decoded DCIs and RNTIs, LTESniffer further decodes the Physical Downlink Shared Channel (PDSCH) and Physical Uplink Shared Channel (PUSCH) to retrieve uplink and downlink data traffic.</p>
<p dir="auto">LTESniffer supports an API with three functions for security applications and research. Many LTE security research assumes
a passive sniffer that can capture privacy-related packets on the air. However, non of the current open-source sniffers satisfy their requirements as they cannot decode protocol packets in PDSCH and PUSCH. We developed a proof-of-concept security API that supports three tasks that were proposed by previous works: 1) Identity mapping, 2) IMSI collecting, and 3) Capability profiling.</p>
<p dir="auto">Please refer to our <a href="https://syssec.kaist.ac.kr/pub/2023/wisec2023_tuan.pdf" rel="nofollow">paper</a> for more details.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">LTESniffer in layman's terms</h2><a id="user-content-ltesniffer-in-laymans-terms" aria-label="Permalink: LTESniffer in layman's terms" href="#ltesniffer-in-laymans-terms"></a></p>
<p dir="auto">LTESniffer is a tool that can capture the LTE wireless messages that are sent between a cell tower and smartphones connected to it. LTESniffer supports capturing the messages in both directions, from the tower to the smartphones, and from the smartphones back to the cell tower.</p>
<p dir="auto">LTESniffer <strong>CANNOT DECRYPT</strong> encrypted messages between the cell tower and smartphones. It can be used for analyzing unencrypted parts of the communication between the cell tower and smartphones. For example, for encrypted messages, it can allow the user to analyze unencrypted parts, such as headers in MAC and physical layers. However, those messages sent in plaintext can be completely analyzable. For example, the broadcast messages sent by the cell tower, or the messages at the beginning of the connection are completely visible.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Ethical Consideration</h2><a id="user-content-ethical-consideration" aria-label="Permalink: Ethical Consideration" href="#ethical-consideration"></a></p>
<p dir="auto">The main purpose of LTESniffer is to support security and analysis research on the cellular network. Due to the collection of uplink-downlink user data, any use of LTESniffer must follow the local regulations on sniffing the LTE traffic. We are not responsible for any illegal purposes such as intentionally collecting user privacy-related information.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Features</h2><a id="user-content-features" aria-label="Permalink: Features" href="#features"></a></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">New Update v2.1.0</h3><a id="user-content-new-update-v210" aria-label="Permalink: New Update v2.1.0" href="#new-update-v210"></a></p>
<ul dir="auto">
<li>Supports recording IQ raw data of subframes to file. Please refer to <code>LTESniffer-record-subframe</code> branch and its <a href="https://github.com/SysSec-KAIST/LTESniffer/tree/LTESniffer-record-subframe">README</a> for more details.</li>
<li>Supports offline decoding using recorded files (<a href="https://github.com/SysSec-KAIST/LTESniffer/tree/LTESniffer-record-subframe">README</a>).</li>
<li>Enable API in the downlink mode (only apply for identity collecting and mapping API)</li>
</ul>
<p dir="auto"><h3 tabindex="-1" dir="auto">New Update v2.0.0</h3><a id="user-content-new-update-v200" aria-label="Permalink: New Update v2.0.0" href="#new-update-v200"></a></p>
<ul dir="auto">
<li>Supports two USRP B-series for uplink sniffing mode. Please refer to <code>LTESniffer-multi-usrp</code> branch and its <a href="https://github.com/SysSec-KAIST/LTESniffer/tree/LTESniffer-multi-usrp">README</a> for more details.</li>
<li>Fixed some bugs.</li>
</ul>
<p dir="auto">LTESniffer is implemented on top of <a href="https://github.com/falkenber9/falcon">FALCON</a> with the help of <a href="https://github.com/srsran/srsRAN_4G">srsRAN</a> library. LTESniffer supports:</p>
<ul dir="auto">
<li>Real-time decoding LTE uplink-downlink control-data channels: PDCCH, PDSCH, PUSCH</li>
<li>LTE Advanced and LTE Advanced Pro, up to 256QAM in both uplink and downlink</li>
<li>DCI formats: 0, 1A, 1, 1B, 1C, 2, 2A, 2B</li>
<li>Transmission modes: 1, 2, 3, 4</li>
<li>FDD only</li>
<li>Maximum 20 MHz base station.</li>
<li>Automatically detect maximum UL/DL modulation schemes of smartphones (64QAM/256QAM on DL and 16QAM/64QAM/256QAM on UL)</li>
<li>Automatically detect physical layer configuration per UE.</li>
<li>LTE Security API: RNTI-TMSI mapping, IMSI collecting, UECapability Profiling.</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Hardware and Software Requirement</h2><a id="user-content-hardware-and-software-requirement" aria-label="Permalink: Hardware and Software Requirement" href="#hardware-and-software-requirement"></a></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">OS Requirement</h3><a id="user-content-os-requirement" aria-label="Permalink: OS Requirement" href="#os-requirement"></a></p>
<p dir="auto">Currently, LTESniffer works stably on Ubuntu 18.04/20.04/22.04.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Hardware Requirement</h3><a id="user-content-hardware-requirement" aria-label="Permalink: Hardware Requirement" href="#hardware-requirement"></a></p>
<p dir="auto">Achieving real-time decoding of LTE traffic requires a high-performance CPU with multiple physical cores, especially during peak hours when the base station has many active users. LTESniffer successfully achieved real-time decoding when deployed on an Intel i7-9700K PC, decoding traffic from a base station with 150 active users.</p>
<p dir="auto"><strong>The following hardware is recommended</strong></p>
<ul dir="auto">
<li>Intel i7 CPU with at least 8 physical cores</li>
<li>At least 16Gb RAM</li>
<li>256 Gb SSD storage</li>
</ul>
<p dir="auto"><h3 tabindex="-1" dir="auto">SDR</h3><a id="user-content-sdr" aria-label="Permalink: SDR" href="#sdr"></a></p>
<p dir="auto">LTESniffer requires different SDR for its uplink and downlink sniffing modes.</p>
<p dir="auto">To sniff only downlink traffic from the base station, LTESniffer is compatible with most SDRs that are supported by the srsRAN library (for example, USRP or BladeRF). The SDR should be connected to the PC via a USB 3.0 port. Also, it should be equipped with GPSDO and two RX antennas to decode downlink messages in transmission modes 3 and 4.</p>
<p dir="auto">On the other hand, to sniff uplink traffic from smartphones to base stations, LTESniffer needs to listen to two different frequencies (Uplink and Downlink) concurrently. To solve this problem, LTESniffer supports two options:</p>
<ul dir="auto">
<li>Using a single USRP X310. USRP X310 has two Local Oscillators (LOs) for 2 RX channels, which can turn each RX channel to a distinct Uplink/Downlink frequency. To use this option, please refer to the <code>main</code> branch of LTESniffer.</li>
<li>Using 2 USRP B-Series. LTESniffer utilizes 2 USRP B-series (B210/B200) for uplink and downlink separately. It achieves synchronization between 2 USRPs by using GPSDO for clock source and time reference. To use this option, please refer to the <code>LTESniffer-multi-usrp</code> branch of LTESniffer and its <a href="https://github.com/SysSec-KAIST/LTESniffer/tree/LTESniffer-multi-usrp">README</a>.</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Installation</h2><a id="user-content-installation" aria-label="Permalink: Installation" href="#installation"></a></p>
<p dir="auto"><strong>Important note: To avoid unexpected errors, please follow the following steps on Ubuntu 18.04/20.04/22.04.</strong></p>
<p dir="auto"><strong>Dependencies</strong></p>
<ul dir="auto">
<li><strong>Important dependency</strong>: <a href="https://github.com/EttusResearch/uhd">UHD</a> library version &gt;= 4.0 must be installed in advance (recommend building from source). The following steps can be used on Ubuntu 18.04. Refer to UHD Manual for full installation guidance.</li>
</ul>
<p dir="auto">UHD dependencies:</p>
<div dir="auto" data-snippet-clipboard-copy-content="sudo apt update
sudo apt-get install autoconf automake build-essential ccache cmake cpufrequtils doxygen ethtool \
g++ git inetutils-tools libboost-all-dev libncurses5 libncurses5-dev libusb-1.0-0 libusb-1.0-0-dev \
libusb-dev python3-dev python3-mako python3-numpy python3-requests python3-scipy python3-setuptools \
python3-ruamel.yaml"><pre>sudo apt update
sudo apt-get install autoconf automake build-essential ccache cmake cpufrequtils doxygen ethtool \
g++ git inetutils-tools libboost-all-dev libncurses5 libncurses5-dev libusb-1.0-0 libusb-1.0-0-dev \
libusb-dev python3-dev python3-mako python3-numpy python3-requests python3-scipy python3-setuptools \
python3-ruamel.yaml</pre></div>
<p dir="auto">Clone and build UHD from source (make sure that the current branch is higher than 4.0)</p>
<div dir="auto" data-snippet-clipboard-copy-content="git clone https://github.com/EttusResearch/uhd.git
cd <uhd-repo-path>/host
mkdir build
cd build
cmake ../
make -j 4
make test
sudo make install
sudo ldconfig"><pre>git clone https://github.com/EttusResearch/uhd.git
<span>cd</span> <span>&lt;</span>uhd-repo-path<span>&gt;</span>/host
mkdir build
<span>cd</span> build
cmake ../
make -j 4
make <span>test</span>
sudo make install
sudo ldconfig</pre></div>
<p dir="auto">Download firmwares for USRPs:</p>
<div dir="auto" data-snippet-clipboard-copy-content="sudo uhd_images_downloader"><pre>sudo uhd_images_downloader</pre></div>
<p dir="auto">We use a <a href="https://www.ettus.com/all-products/10gige-kit/" rel="nofollow">10Gb card</a> to connect USRP X310 to PC, refer to UHD Manual <a href="https://files.ettus.com/manual/page_usrp_x3x0.html" rel="nofollow">[1]</a>, <a href="https://files.ettus.com/manual/page_usrp_x3x0_config.html" rel="nofollow">[2]</a> to configure USRP X310 and 10Gb card interface. For USRP B210, it should be connected to PC via a USB 3.0 port.</p>
<p dir="auto">Test the connection and firmware (for USRP X310 only):</p>
<div dir="auto" data-snippet-clipboard-copy-content="sudo sysctl -w net.core.rmem_max=33554432
sudo sysctl -w net.core.wmem_max=33554432
sudo ifconfig <10Gb card interface> mtu 9000
sudo uhd_usrp_probe"><pre>sudo sysctl -w net.core.rmem_max=33554432
sudo sysctl -w net.core.wmem_max=33554432
sudo ifconfig <span>&lt;</span>10Gb card interface<span>&gt;</span> mtu 9000
sudo uhd_usrp_probe</pre></div>
<ul dir="auto">
<li>srsRAN dependencies:</li>
</ul>
<div dir="auto" data-snippet-clipboard-copy-content="sudo apt-get install build-essential git cmake libfftw3-dev libmbedtls-dev libboost-program-options-dev libconfig++-dev libsctp-dev"><pre>sudo apt-get install build-essential git cmake libfftw3-dev libmbedtls-dev libboost-program-options-dev libconfig++-dev libsctp-dev</pre></div>
<ul dir="auto">
<li>LTESniffer dependencies:</li>
</ul>
<div dir="auto" data-snippet-clipboard-copy-content="sudo apt-get install libglib2.0-dev libudev-dev libcurl4-gnutls-dev libboost-all-dev qtdeclarative5-dev libqt5charts5-dev"><pre>sudo apt-get install libglib2.0-dev libudev-dev libcurl4-gnutls-dev libboost-all-dev qtdeclarative5-dev libqt5charts5-dev</pre></div>
<p dir="auto"><strong>Build LTESniffer from source:</strong></p>
<div dir="auto" data-snippet-clipboard-copy-content="git clone https://github.com/SysSec-KAIST/LTESniffer.git
cd LTESniffer
mkdir build
cd build
cmake ../
make -j 4 (use 4 threads)"><pre>git clone https://github.com/SysSec-KAIST/LTESniffer.git
<span>cd</span> LTESniffer
mkdir build
<span>cd</span> build
cmake ../
make -j 4 (use 4 threads)</pre></div>
<p dir="auto"><h2 tabindex="-1" dir="auto">Usage</h2><a id="user-content-usage" aria-label="Permalink: Usage" href="#usage"></a></p>
<p dir="auto">LTESniffer has 3 main functions:</p>
<ul dir="auto">
<li>Sniffing LTE downlink traffic from the base station</li>
<li>Sniffing LTE uplink traffic from smartphones</li>
<li>Security API</li>
</ul>
<p dir="auto">After building from source, <code>LTESniffer</code> is located in <code>&lt;build-dir&gt;/src/LTESniffer</code></p>
<p dir="auto">Note that before using LTESniffer on the commercial, one should have to check the local regulations on sniffing LTE traffic, as we explained in the <strong>Ethical Consideration</strong>.</p>
<p dir="auto">To figure out the base station and Uplink-Downlink band the test smartphone is connected to, install <a href="https://play.google.com/store/apps/details?id=make.more.r2d2.cellular_z&amp;hl=en&amp;gl=US&amp;pli=1" rel="nofollow">Cellular-Z</a> app on the test smartphone (the app only supports Android). It will show the cell ID and Uplink-Downlink band/frequency to which the test smartphone is connected. Make sure that LTESniffer also connects to the same cell and frequency.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">General downlink sniffing</h3><a id="user-content-general-downlink-sniffing" aria-label="Permalink: General downlink sniffing" href="#general-downlink-sniffing"></a></p>
<p dir="auto">
  <a target="_blank" rel="noopener noreferrer" href="https://github.com/SysSec-KAIST/LTESniffer/blob/main/png/dl_mode_png.png"><img src="https://github.com/SysSec-KAIST/LTESniffer/raw/main/png/dl_mode_png.png" alt="LTESniffer Downlink Mode"></a>
</p>
<div dir="auto" data-snippet-clipboard-copy-content="sudo ./<build-dir>/src/LTESniffer -A 2 -W <number of threads> -f <DL Freq> -C -m 0
example: sudo ./src/LTESniffer -A 2 -W 4 -f 1840e6 -C -m 0
-A: number of antennas
-W: number of threads
-f: downlink frequency
-C: turn on cell search
-m: sniffer mode, 0 for downlink sniffing and 1 for uplink sniffing"><pre>sudo ./<span>&lt;</span>build-dir<span>&gt;</span>/src/LTESniffer -A 2 -W <span>&lt;</span>number of threads<span>&gt;</span> -f <span>&lt;</span>DL Freq<span>&gt;</span> -C -m 0
example: sudo ./src/LTESniffer -A 2 -W 4 -f 1840e6 -C -m 0
-A: number of antennas
-W: number of threads
-f: downlink frequency
-C: turn on cell search
-m: sniffer mode, 0 <span>for</span> downlink sniffing and 1 <span>for</span> uplink sniffing</pre></div>
<p dir="auto">Note: to run <code>LTESniffer</code> with USRP B210 in the downlink mode, add option <code>-a "num_recv_frames=512" </code> to the command line.
This option extends the receiving buffer for USRP B210 to achieve better synchronization.</p>
<div dir="auto" data-snippet-clipboard-copy-content="sudo ./<build-dir>/src/LTESniffer -A 2 -W <number of threads> -f <DL Freq> -C -m 0 -a &quot;num_recv_frames=512&quot;
example: sudo ./src/LTESniffer -A 2 -W 4 -f 1840e6 -C -m 0 -a &quot;num_recv_frames=512&quot;"><pre>sudo ./<span>&lt;</span>build-dir<span>&gt;</span>/src/LTESniffer -A 2 -W <span>&lt;</span>number of threads<span>&gt;</span> -f <span>&lt;</span>DL Freq<span>&gt;</span> -C -m 0 -a <span><span>"</span>num_recv_frames=512<span>"</span></span>
example: sudo ./src/LTESniffer -A 2 -W 4 -f 1840e6 -C -m 0 -a <span><span>"</span>num_recv_frames=512<span>"</span></span></pre></div>
<p dir="auto"><h3 tabindex="-1" dir="auto">General uplink sniffing</h3><a id="user-content-general-uplink-sniffing" aria-label="Permalink: General uplink sniffing" href="#general-uplink-sniffing"></a></p>
<p dir="auto">Note: In the uplink sniffing mode, the test smartphones should be located nearby the sniffer, because the uplink signal power from UE is significantly weaker compared to the downlink signal from the base station.</p>
<p dir="auto">
  <a target="_blank" rel="noopener noreferrer" href="https://github.com/SysSec-KAIST/LTESniffer/blob/main/png/ul_mode_png.png"><img src="https://github.com/SysSec-KAIST/LTESniffer/raw/main/png/ul_mode_png.png" alt="LTESniffer Uplink Mode"></a>
</p>
<div dir="auto" data-snippet-clipboard-copy-content="sudo ./<build-dir>/src/LTESniffer -A 2 -W <number of threads> -f <DL Freq> -u <UL Freq> -C -m 1
example: sudo ./src/LTESniffer -A 2 -W 4 -f 1840e6 -u 1745e6 -C -m 1
-u: uplink frequency"><pre>sudo ./<span>&lt;</span>build-dir<span>&gt;</span>/src/LTESniffer -A 2 -W <span>&lt;</span>number of threads<span>&gt;</span> -f <span>&lt;</span>DL Freq<span>&gt;</span> -u <span>&lt;</span>UL Freq<span>&gt;</span> -C -m 1
example: sudo ./src/LTESniffer -A 2 -W 4 -f 1840e6 -u 1745e6 -C -m 1
-u: uplink frequency</pre></div>
<p dir="auto"><h3 tabindex="-1" dir="auto">Security API</h3><a id="user-content-security-api" aria-label="Permalink: Security API" href="#security-api"></a></p>
<p dir="auto">
  <a target="_blank" rel="noopener noreferrer" href="https://github.com/SysSec-KAIST/LTESniffer/blob/main/png/api_png.png"><img src="https://github.com/SysSec-KAIST/LTESniffer/raw/main/png/api_png.png" alt="LTESniffer API Mode"></a>
</p>
<div dir="auto" data-snippet-clipboard-copy-content="sudo ./<build-dir>/src/LTESniffer -A 2 -W <number of threads> -f <DL Freq> -u <UL Freq> -C -m 1 -z 3
example: sudo ./src/LTESniffer -A 2 -W 4 -f 1840e6 -u 1745e6 -C -m 1 -z 3
-z: 3 for turnning on 3 functions of sniffer, which are identity mapping, IMSI collecting, and UECapability profiling.
    2 for UECapability profiling
    1 for IMSI collecting
    0 for identity mapping"><pre>sudo ./<span>&lt;</span>build-dir<span>&gt;</span>/src/LTESniffer -A 2 -W <span>&lt;</span>number of threads<span>&gt;</span> -f <span>&lt;</span>DL Freq<span>&gt;</span> -u <span>&lt;</span>UL Freq<span>&gt;</span> -C -m 1 -z 3
example: sudo ./src/LTESniffer -A 2 -W 4 -f 1840e6 -u 1745e6 -C -m 1 -z 3
-z: 3 <span>for</span> turnning on 3 functions of sniffer, which are identity mapping, IMSI collecting, and UECapability profiling.
    2 <span>for</span> UECapability profiling
    1 <span>for</span> IMSI collecting
    0 <span>for</span> identity mapping</pre></div>
<p dir="auto"><h3 tabindex="-1" dir="auto">Specify a base station</h3><a id="user-content-specify-a-base-station" aria-label="Permalink: Specify a base station" href="#specify-a-base-station"></a></p>
<p dir="auto">LTESniffer can sniff on a specific base station by using options <code>-I &lt;Phycial Cell ID (PCI)&gt; -p &lt;number of Physical Resource Block (PRB)&gt;</code>. In this case, LTESniffer does not do the cell search but connects directly to the specified cell.</p>
<div dir="auto" data-snippet-clipboard-copy-content="sudo ./<build-dir>/src/LTESniffer -A 2 -W <number of threads> -f <DL Freq> -I <PCI> -p <PRB> -m 0
sudo ./<build-dir>/src/LTESniffer -A 2 -W <number of threads> -f <DL Freq> -u <UL Freq> -I <PCI> -p <PRB> -m 1
example: sudo ./src/LTESniffer -A 2 -W 4 -f 1840e6 -u 1745e6 -I 379 -p 100 -m 1"><pre>sudo ./<span>&lt;</span>build-dir<span>&gt;</span>/src/LTESniffer -A 2 -W <span>&lt;</span>number of threads<span>&gt;</span> -f <span>&lt;</span>DL Freq<span>&gt;</span> -I <span>&lt;</span>PCI<span>&gt;</span> -p <span>&lt;</span>PRB<span>&gt;</span> -m 0
sudo ./<span>&lt;</span>build-dir<span>&gt;</span>/src/LTESniffer -A 2 -W <span>&lt;</span>number of threads<span>&gt;</span> -f <span>&lt;</span>DL Freq<span>&gt;</span> -u <span>&lt;</span>UL Freq<span>&gt;</span> -I <span>&lt;</span>PCI<span>&gt;</span> -p <span>&lt;</span>PRB<span>&gt;</span> -m 1
example: sudo ./src/LTESniffer -A 2 -W 4 -f 1840e6 -u 1745e6 -I 379 -p 100 -m 1</pre></div>
<p dir="auto">The debug mode can be enabled by using option <code>-d</code>. In this case, the debug messages will be printed on the terminal.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Output of LTESniffer</h3><a id="user-content-output-of-ltesniffer" aria-label="Permalink: Output of LTESniffer" href="#output-of-ltesniffer"></a></p>
<p dir="auto">LTESniffer provides pcap files in the output. The pcap file can be opened by WireShark for further analysis and packet trace.
The name of downlink pcap file: <code>sniffer_dl_mode.pcap</code>, uplink pcap file: <code>sniffer_ul_mode.pcap</code>, and API pcap file: <code>api_collector.pcap</code>.
The pcap files are located in the same directory <code>LTESniffer</code> has been executed.
To enable the WireShark to analyze the decoded packets correctly, please refer to the WireShark configuration guide <a href="https://github.com/SysSec-KAIST/LTESniffer/blob/main/pcap_file_example/README.md">here</a>. There are also some examples of pcap files in the link.<br>
<strong>Note:</strong> The uplink pcap file contains both uplink and downlink messages. On the WireShark, use this filter to monitor only uplink messages: <code>mac-lte.direction == 0</code>; or this filter to monitor only downlink messages: <code>mac-lte.direction == 1</code>.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Application Note</h2><a id="user-content-application-note" aria-label="Permalink: Application Note" href="#application-note"></a></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Distance for uplink sniffing</h3><a id="user-content-distance-for-uplink-sniffing" aria-label="Permalink: Distance for uplink sniffing" href="#distance-for-uplink-sniffing"></a></p>
<p dir="auto">The effective range for sniffing uplink is limited in LTESniffer due to the capability of the RF front-end of the hardware (i.e. SDR). The uplink signal power from UE is significantly weaker compared to the downlink signal because UE is a handheld device that optimizes battery usage, while the eNB uses sufficient power to cover a large area. To successfully capture the uplink traffic, LTESniffer can increase the strength of the signal power by i) being physically close to the UE, or ii) improving the signal reception capability with specialized hardware, such as a directional antenna, dedicated RF front-end, and signal amplifier.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">The information displayed on the terminal</h3><a id="user-content-the-information-displayed-on-the-terminal" aria-label="Permalink: The information displayed on the terminal" href="#the-information-displayed-on-the-terminal"></a></p>
<p dir="auto"><strong>Downlink Sniffing Mode</strong></p>
<p dir="auto"><code>Processed 1000/1000 subframes</code>: Number of subframes was processed by LTESniffer last 1 second. There are 1000 LTE subframes per second by design. <br>
<code>RNTI</code>: Radio Network Temporary Identifier of UEs. <br>
<code>Table</code>: The maximum modulation scheme that is used by smartphones in downlink. LTESniffer supports up to 256QAM in the downlink. Refer to our <a href="https://syssec.kaist.ac.kr/pub/2023/wisec2023_tuan.pdf" rel="nofollow">paper</a> for more details. <br>
<code>Active</code>: Number of detected messages of RNTIs. <br>
<code>Success</code>: Number of successfully decoded messages over number of detected messages (<code>Active</code>). <br>
<code>New TX, ReTX, HARQ, Normal</code>: Statistic of new messages and retransmitted messages. This function is in development. <br>
<code>W_MIMO, W_pinfor, Other</code>: Number of messages with wrong radio configuration, only for debugging.</p>
<p dir="auto"><strong>Uplink Sniffing Mode</strong></p>
<p dir="auto"><code>Max Mod</code>: The maximum modulation scheme that is used by smartphones in uplink. It can be 16/64/256QAM depending on the support of smartphones and the configuration of the network. Refer to our <a href="https://syssec.kaist.ac.kr/pub/2023/wisec2023_tuan.pdf" rel="nofollow">paper</a> for more details. <br>
<code>SNR</code>: Signal-to-noise ratio (dB). Low SNR means the uplink signal quality from the smartphone is bad. One possible reason is the smartphone is far from the sniffer. <br>
<code>DL-UL_delay</code>: The average of time delay between downlink signal from the base station and uplink signal from the smartphone. <br>
<code>Other Info</code>: Information only for debugging.</p>
<p dir="auto"><strong>API Mode</strong></p>
<p dir="auto"><code>Detected Identity</code>: The name of detected identity. <br>
<code>Value</code>: The value of detected identity. <br>
<code>From Message</code>: The name of the message that contains the detected identity.</p>

<p dir="auto"><h2 tabindex="-1" dir="auto">Credits</h2><a id="user-content-credits" aria-label="Permalink: Credits" href="#credits"></a></p>
<p dir="auto">We sincerely appreciate the <a href="https://github.com/falkenber9/falcon">FALCON</a> and <a href="https://github.com/srsran/srsRAN_4G">SRS team</a> for making their great softwares available.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Contributor</h2><a id="user-content-contributor" aria-label="Permalink: Contributor" href="#contributor"></a></p>
<p dir="auto">Special thanks to all the contributors who helped us to fix bugs and improve LTESniffer</p>
<ol dir="auto">
<li><a href="https://github.com/cellular777">@cellular777</a></li>
<li><a href="https://www.youtube.com/@cemaxecuter7783" rel="nofollow">Cemaxecuter</a></li>
</ol>
<p dir="auto"><h2 tabindex="-1" dir="auto">BibTex</h2><a id="user-content-bibtex" aria-label="Permalink: BibTex" href="#bibtex"></a></p>
<p dir="auto">Please refer to our <a href="https://syssec.kaist.ac.kr/pub/2023/wisec2023_tuan.pdf" rel="nofollow">paper</a> for more details.</p>
<div dir="auto" data-snippet-clipboard-copy-content="@inproceedings{hoang:ltesniffer,
  title = {{LTESniffer: An Open-source LTE Downlink/Uplink Eavesdropper}},
  author = {Hoang, Dinh Tuan and Park, CheolJun and Son, Mincheol and Oh, Taekkyung and Bae, Sangwook and Ahn, Junho and Oh, BeomSeok and Kim, Yongdae},
  booktitle = {16th ACM Conference on Security and Privacy in Wireless and Mobile Networks (WiSec '23)},
  year = {2023}
}"><pre><span>@inproceedings</span>{<span>hoang:ltesniffer</span>,
  <span>title</span> = <span><span>{</span>{LTESniffer: An Open-source LTE Downlink/Uplink Eavesdropper}<span>}</span></span>,
  <span>author</span> = <span><span>{</span>Hoang, Dinh Tuan and Park, CheolJun and Son, Mincheol and Oh, Taekkyung and Bae, Sangwook and Ahn, Junho and Oh, BeomSeok and Kim, Yongdae<span>}</span></span>,
  <span>booktitle</span> = <span><span>{</span>16th ACM Conference on Security and Privacy in Wireless and Mobile Networks (WiSec '23)<span>}</span></span>,
  <span>year</span> = <span><span>{</span>2023<span>}</span></span>
}</pre></div>
<p dir="auto"><h2 tabindex="-1" dir="auto">FAQ</h2><a id="user-content-faq" aria-label="Permalink: FAQ" href="#faq"></a></p>

<p dir="auto"><strong>Q:</strong> Is it mandatory to use GPSDO with the USRP in order to run LTESniffer? <br>
<strong>A:</strong> GPSDO is useful for more stable synchronization. However, for downlink sniffing mode, LTESniffer still can synchronize with the LTE signal to decode the packets without GPSDO. For uplink sniffing mode, GPSDO is only required when using 2 USRP B-series, as it is the time and clock reference sources for synchrozation between uplink and downlink channels. Another uplink SDR option, using a single USRP X310, does not require GPSDO.</p>
<p dir="auto"><strong>Q:</strong> For downlink traffic, can I use a cheaper SDR? <br>
<strong>A:</strong> Technically, any SDRs supported by srsRAN library such as Blade RF can be used to run LTESniffer in the downlink sniffing mode. However, we only tested the downlink sniffing function of LTESniffer with USRP B210 and X310.</p>
<p dir="auto"><strong>Q:</strong> Is it illegal to use LTESniffer to sniff the LTE traffic? <br>
<strong>A:</strong> You should have to check the local regulations on sniffing (unencrypted) LTE traffic. Another way to test LTESniffer is setting up a personal LTE network by using <a href="https://github.com/srsran/srsRAN_4G">srsRAN</a> - an open-source LTE implementation in a Faraday cage.</p>
<p dir="auto"><strong>Q:</strong> Can LTESniffer be used to view the content of messages between two users? <br>
<strong>A:</strong> One can see only the "unencrypted" part of the messages. Note that the air traffic between the base station and users is mostly encrypted.</p>
<p dir="auto"><strong>Q:</strong> Is there any device identity exposed in plaintext in the LTE network? <br>
<strong>A:</strong> Yes, literature shows that there are multiple identities exposed, such as TMSI, GUTI, IMSI, and RNTI. Please refer to the academic literature for more details. e.g. <a href="https://syssec.kaist.ac.kr/pub/2022/sec22summer_bae.pdf" rel="nofollow">Watching the Watchers: Practical Video Identification Attack in LTE Networks</a></p>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Apple's AirPods Pro hearing health features (243 pts)]]></title>
            <link>https://www.theverge.com/24275178/apple-airpods-pro-hearing-aid-test-protection-preview</link>
            <guid>41909967</guid>
            <pubDate>Tue, 22 Oct 2024 00:25:46 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.theverge.com/24275178/apple-airpods-pro-hearing-aid-test-protection-preview">https://www.theverge.com/24275178/apple-airpods-pro-hearing-aid-test-protection-preview</a>, See on <a href="https://news.ycombinator.com/item?id=41909967">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="content"><p>Apple announced a trio of major new hearing health features for the AirPods Pro 2 in September, including clinical-grade hearing aid functionality, a hearing test, and more robust hearing protection. All three will roll out next week with the release of iOS 18.1, and they <a href="https://www.theverge.com/2024/9/12/24241960/apple-airpods-pro-2-otc-hearing-aid-health-industry">could mark a watershed moment</a> for hearing health awareness. Apple is about to instantly turn the world’s most popular earbuds into an over-the-counter hearing aid.</p><p>That also means we’re about to enter an era where we’ll need to get comfortable with people wearing earbuds at all times. There’s a perception that leaving your earbuds in while talking with other people is rude. Transparency mode in many of today’s earbuds sounds totally natural and lifelike, yet I still constantly remove my buds to show someone they’ve got my undivided attention. That way of thinking has to change when popular earbuds start pulling double duty as hearing aids. It’s a powerful way to reduce the stigma that’s all too common with hearing aids, but this shift will take time.</p><p>Over the last several days, I’ve been able to preview Apple’s hearing health features. At times, the experience has been emotionally intense. I’m someone who grew up with a Discman and iPod basically attached to my hip, and I’ve been to countless concerts over the decades. I also haven’t seen an audiologist since 2018 or so. That’s anything but unusual; Apple says 80 percent of adults in the US haven’t had their hearing checked in at least five years. Putting a test right on your iPhone is a great way to improve that trend.</p><div><div role="button" aria-label="Zoom" tabindex="0"><figure><div><p><span><img alt="Hands-on photos of Apple’s AirPods Pro hearing health features." loading="lazy" decoding="async" data-nimg="fill" sizes="(max-width: 1023px) 100vw, 744px" srcset="https://duet-cdn.vox-cdn.com/thumbor/0x0:2040x1360/376x251/filters:focal(1020x680:1021x681):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25687446/DSC_1034_Enhanced_NR.jpg 376w, https://duet-cdn.vox-cdn.com/thumbor/0x0:2040x1360/384x256/filters:focal(1020x680:1021x681):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25687446/DSC_1034_Enhanced_NR.jpg 384w, https://duet-cdn.vox-cdn.com/thumbor/0x0:2040x1360/415x277/filters:focal(1020x680:1021x681):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25687446/DSC_1034_Enhanced_NR.jpg 415w, https://duet-cdn.vox-cdn.com/thumbor/0x0:2040x1360/480x320/filters:focal(1020x680:1021x681):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25687446/DSC_1034_Enhanced_NR.jpg 480w, https://duet-cdn.vox-cdn.com/thumbor/0x0:2040x1360/540x360/filters:focal(1020x680:1021x681):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25687446/DSC_1034_Enhanced_NR.jpg 540w, https://duet-cdn.vox-cdn.com/thumbor/0x0:2040x1360/640x427/filters:focal(1020x680:1021x681):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25687446/DSC_1034_Enhanced_NR.jpg 640w, https://duet-cdn.vox-cdn.com/thumbor/0x0:2040x1360/750x500/filters:focal(1020x680:1021x681):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25687446/DSC_1034_Enhanced_NR.jpg 750w, https://duet-cdn.vox-cdn.com/thumbor/0x0:2040x1360/828x552/filters:focal(1020x680:1021x681):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25687446/DSC_1034_Enhanced_NR.jpg 828w, https://duet-cdn.vox-cdn.com/thumbor/0x0:2040x1360/1080x720/filters:focal(1020x680:1021x681):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25687446/DSC_1034_Enhanced_NR.jpg 1080w, https://duet-cdn.vox-cdn.com/thumbor/0x0:2040x1360/1200x800/filters:focal(1020x680:1021x681):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25687446/DSC_1034_Enhanced_NR.jpg 1200w, https://duet-cdn.vox-cdn.com/thumbor/0x0:2040x1360/1440x960/filters:focal(1020x680:1021x681):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25687446/DSC_1034_Enhanced_NR.jpg 1440w, https://duet-cdn.vox-cdn.com/thumbor/0x0:2040x1360/1920x1280/filters:focal(1020x680:1021x681):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25687446/DSC_1034_Enhanced_NR.jpg 1920w, https://duet-cdn.vox-cdn.com/thumbor/0x0:2040x1360/2048x1365/filters:focal(1020x680:1021x681):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25687446/DSC_1034_Enhanced_NR.jpg 2048w, https://duet-cdn.vox-cdn.com/thumbor/0x0:2040x1360/2400x1600/filters:focal(1020x680:1021x681):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25687446/DSC_1034_Enhanced_NR.jpg 2400w" src="https://duet-cdn.vox-cdn.com/thumbor/0x0:2040x1360/2400x1600/filters:focal(1020x680:1021x681):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25687446/DSC_1034_Enhanced_NR.jpg" data-old-src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7"></span></p></div></figure></div><p><figcaption><em>The AirPods Pro 2 are transforming into something much more than wireless earbuds.</em></figcaption></p></div><p><h3>Prevention: hearing protection</h3></p><p>Did you know there are people who’ve already been <a href="https://www.tomsguide.com/features/i-used-airpods-pro-instead-of-ear-plugs-at-a-concert-heres-what-happened">replacing earplugs with the AirPods Pro</a> at concerts? Until this fall, Apple had never endorsed such a use case or advertised its earbuds as hearing protection devices. The company knew people were doing it but kept quiet on the subject. </p><p>That’s now changed. With iOS 18.1 and the soon-to-be-released AirPods firmware update, the AirPods Pro 2 will offer hearing protection at all times across noise cancellation, transparency, and adaptive audio modes. There’s no “concert mode” or a specific setting to toggle. You can think of this as an expansion of the loud sound reduction option that was already in place. Hearing protection is on by default, and Apple says “an all-new multiband high dynamic range algorithm” helps to preserve the natural sound of concerts and other live events.</p><div><div role="button" aria-label="Zoom" tabindex="0"><figure><div><p><span><img alt="A chart that shows the effectiveness of Apple’s hearing protection in the AirPods Pro 2." loading="lazy" decoding="async" data-nimg="fill" sizes="(max-width: 1023px) 100vw, 744px" srcset="https://duet-cdn.vox-cdn.com/thumbor/0x0:1812x1736/376x360/filters:focal(906x868:907x869):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25690977/Screenshot_2024_10_21_at_5.08.27_AM.png 376w, https://duet-cdn.vox-cdn.com/thumbor/0x0:1812x1736/384x368/filters:focal(906x868:907x869):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25690977/Screenshot_2024_10_21_at_5.08.27_AM.png 384w, https://duet-cdn.vox-cdn.com/thumbor/0x0:1812x1736/415x398/filters:focal(906x868:907x869):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25690977/Screenshot_2024_10_21_at_5.08.27_AM.png 415w, https://duet-cdn.vox-cdn.com/thumbor/0x0:1812x1736/480x460/filters:focal(906x868:907x869):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25690977/Screenshot_2024_10_21_at_5.08.27_AM.png 480w, https://duet-cdn.vox-cdn.com/thumbor/0x0:1812x1736/540x517/filters:focal(906x868:907x869):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25690977/Screenshot_2024_10_21_at_5.08.27_AM.png 540w, https://duet-cdn.vox-cdn.com/thumbor/0x0:1812x1736/640x613/filters:focal(906x868:907x869):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25690977/Screenshot_2024_10_21_at_5.08.27_AM.png 640w, https://duet-cdn.vox-cdn.com/thumbor/0x0:1812x1736/750x719/filters:focal(906x868:907x869):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25690977/Screenshot_2024_10_21_at_5.08.27_AM.png 750w, https://duet-cdn.vox-cdn.com/thumbor/0x0:1812x1736/828x793/filters:focal(906x868:907x869):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25690977/Screenshot_2024_10_21_at_5.08.27_AM.png 828w, https://duet-cdn.vox-cdn.com/thumbor/0x0:1812x1736/1080x1035/filters:focal(906x868:907x869):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25690977/Screenshot_2024_10_21_at_5.08.27_AM.png 1080w, https://duet-cdn.vox-cdn.com/thumbor/0x0:1812x1736/1200x1150/filters:focal(906x868:907x869):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25690977/Screenshot_2024_10_21_at_5.08.27_AM.png 1200w, https://duet-cdn.vox-cdn.com/thumbor/0x0:1812x1736/1440x1380/filters:focal(906x868:907x869):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25690977/Screenshot_2024_10_21_at_5.08.27_AM.png 1440w, https://duet-cdn.vox-cdn.com/thumbor/0x0:1812x1736/1920x1839/filters:focal(906x868:907x869):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25690977/Screenshot_2024_10_21_at_5.08.27_AM.png 1920w, https://duet-cdn.vox-cdn.com/thumbor/0x0:1812x1736/2048x1962/filters:focal(906x868:907x869):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25690977/Screenshot_2024_10_21_at_5.08.27_AM.png 2048w, https://duet-cdn.vox-cdn.com/thumbor/0x0:1812x1736/2400x2299/filters:focal(906x868:907x869):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25690977/Screenshot_2024_10_21_at_5.08.27_AM.png 2400w" src="https://duet-cdn.vox-cdn.com/thumbor/0x0:1812x1736/2400x2299/filters:focal(906x868:907x869):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25690977/Screenshot_2024_10_21_at_5.08.27_AM.png" data-old-src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7"></span></p></div></figure></div><div><figcaption><em>This chart shows the effectiveness of Apple’s hearing protection across the different sound modes of the AirPods Pro 2.</em></figcaption> <p><cite>Image: Apple</cite></p></div></div><p>Which listening mode you’ll use for concerts comes down to personal preference. I’ve found adaptive audio works well since it lets you customize whether you prefer more noise cancellation or more passthrough. But even in full transparency mode, some level of hearing protection is active. The more noise cancellation that’s applied, the longer you can remain in relatively loud environments. </p><p>There are limits to what the AirPods Pro 2 can handle; Apple’s hearing protection isn’t cut out for extremely loud, sudden noises like gunfire, fireworks, or a jackhammer. Sustained noises over 110dBA are also too much for the earbuds. Some clubs and concerts can definitely exceed that threshold, so be aware.</p><div><div role="button" aria-label="Zoom" tabindex="0"><figure><div><p><span><img alt="A photo of a person wearing Apple’s AirPods Pro 2." loading="lazy" decoding="async" data-nimg="fill" sizes="(max-width: 1023px) 100vw, 744px" srcset="https://duet-cdn.vox-cdn.com/thumbor/0x0:2040x1360/376x251/filters:focal(1020x680:1021x681):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25688614/DSC_1043_Enhanced_NR.jpg 376w, https://duet-cdn.vox-cdn.com/thumbor/0x0:2040x1360/384x256/filters:focal(1020x680:1021x681):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25688614/DSC_1043_Enhanced_NR.jpg 384w, https://duet-cdn.vox-cdn.com/thumbor/0x0:2040x1360/415x277/filters:focal(1020x680:1021x681):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25688614/DSC_1043_Enhanced_NR.jpg 415w, https://duet-cdn.vox-cdn.com/thumbor/0x0:2040x1360/480x320/filters:focal(1020x680:1021x681):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25688614/DSC_1043_Enhanced_NR.jpg 480w, https://duet-cdn.vox-cdn.com/thumbor/0x0:2040x1360/540x360/filters:focal(1020x680:1021x681):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25688614/DSC_1043_Enhanced_NR.jpg 540w, https://duet-cdn.vox-cdn.com/thumbor/0x0:2040x1360/640x427/filters:focal(1020x680:1021x681):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25688614/DSC_1043_Enhanced_NR.jpg 640w, https://duet-cdn.vox-cdn.com/thumbor/0x0:2040x1360/750x500/filters:focal(1020x680:1021x681):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25688614/DSC_1043_Enhanced_NR.jpg 750w, https://duet-cdn.vox-cdn.com/thumbor/0x0:2040x1360/828x552/filters:focal(1020x680:1021x681):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25688614/DSC_1043_Enhanced_NR.jpg 828w, https://duet-cdn.vox-cdn.com/thumbor/0x0:2040x1360/1080x720/filters:focal(1020x680:1021x681):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25688614/DSC_1043_Enhanced_NR.jpg 1080w, https://duet-cdn.vox-cdn.com/thumbor/0x0:2040x1360/1200x800/filters:focal(1020x680:1021x681):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25688614/DSC_1043_Enhanced_NR.jpg 1200w, https://duet-cdn.vox-cdn.com/thumbor/0x0:2040x1360/1440x960/filters:focal(1020x680:1021x681):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25688614/DSC_1043_Enhanced_NR.jpg 1440w, https://duet-cdn.vox-cdn.com/thumbor/0x0:2040x1360/1920x1280/filters:focal(1020x680:1021x681):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25688614/DSC_1043_Enhanced_NR.jpg 1920w, https://duet-cdn.vox-cdn.com/thumbor/0x0:2040x1360/2048x1365/filters:focal(1020x680:1021x681):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25688614/DSC_1043_Enhanced_NR.jpg 2048w, https://duet-cdn.vox-cdn.com/thumbor/0x0:2040x1360/2400x1600/filters:focal(1020x680:1021x681):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25688614/DSC_1043_Enhanced_NR.jpg 2400w" src="https://duet-cdn.vox-cdn.com/thumbor/0x0:2040x1360/2400x1600/filters:focal(1020x680:1021x681):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25688614/DSC_1043_Enhanced_NR.jpg" data-old-src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7"></span></p></div></figure></div><p><figcaption><em>The AirPods Pro aren’t as discreet as some hearing aids, but their popularity and ubiquity could help reduce stigma.</em></figcaption></p></div><p><h3>Awareness: the Apple hearing test</h3></p><p>You’ll need a quiet space when taking Apple’s hearing test. Before getting started, your iPhone will do a quick analysis of ear tip fit and environmental noise to ensure you’re good to go. All of these hearing health features are calibrated for Apple’s stock silicone tips, so if you’re using aftermarket third-party tips (including foam), there’s no guarantee you’ll get the optimal experience. Once the test begins, you just tap the screen whenever you hear any of the three-beep tone sequences. </p><p>There are a few key things to know about Apple’s hearing test. For one, it’s designed so that you can’t predict or game it. The test can play any frequency at any time, so no two are the same. Apple tests your left ear first, and here’s something I wish I’d known going in: it’s <em>completely normal</em> to hear nothing at all for several seconds at a time. It was in those moments, when five, six, or even 10 seconds would pass without an obvious tone sequence, where I’d start feeling pretty anxious.&nbsp;</p><p>My best advice is to avoid wondering if you <em>should</em> be hearing something at a given moment and instead just focus on the tones as they come. Some can be incredibly faint. There are visual cues that let you know the test is still moving along even during silence — the most obvious one being a large circle that animates onscreen throughout the process. (You’ll also notice a progress dial for each ear that fills as you take it.) </p><div><div role="button" aria-label="Zoom" tabindex="0"><figure><div><p><span><img alt="A screenshot of Apple hearing test results." loading="lazy" decoding="async" data-nimg="fill" sizes="(max-width: 1023px) 100vw, 744px" srcset="https://duet-cdn.vox-cdn.com/thumbor/0x0:1290x703/376x205/filters:focal(645x352:646x353):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25690234/IMG_0041.jpg 376w, https://duet-cdn.vox-cdn.com/thumbor/0x0:1290x703/384x209/filters:focal(645x352:646x353):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25690234/IMG_0041.jpg 384w, https://duet-cdn.vox-cdn.com/thumbor/0x0:1290x703/415x226/filters:focal(645x352:646x353):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25690234/IMG_0041.jpg 415w, https://duet-cdn.vox-cdn.com/thumbor/0x0:1290x703/480x262/filters:focal(645x352:646x353):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25690234/IMG_0041.jpg 480w, https://duet-cdn.vox-cdn.com/thumbor/0x0:1290x703/540x294/filters:focal(645x352:646x353):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25690234/IMG_0041.jpg 540w, https://duet-cdn.vox-cdn.com/thumbor/0x0:1290x703/640x349/filters:focal(645x352:646x353):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25690234/IMG_0041.jpg 640w, https://duet-cdn.vox-cdn.com/thumbor/0x0:1290x703/750x409/filters:focal(645x352:646x353):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25690234/IMG_0041.jpg 750w, https://duet-cdn.vox-cdn.com/thumbor/0x0:1290x703/828x451/filters:focal(645x352:646x353):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25690234/IMG_0041.jpg 828w, https://duet-cdn.vox-cdn.com/thumbor/0x0:1290x703/1080x589/filters:focal(645x352:646x353):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25690234/IMG_0041.jpg 1080w, https://duet-cdn.vox-cdn.com/thumbor/0x0:1290x703/1200x654/filters:focal(645x352:646x353):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25690234/IMG_0041.jpg 1200w, https://duet-cdn.vox-cdn.com/thumbor/0x0:1290x703/1440x785/filters:focal(645x352:646x353):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25690234/IMG_0041.jpg 1440w, https://duet-cdn.vox-cdn.com/thumbor/0x0:1290x703/1920x1046/filters:focal(645x352:646x353):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25690234/IMG_0041.jpg 1920w, https://duet-cdn.vox-cdn.com/thumbor/0x0:1290x703/2048x1116/filters:focal(645x352:646x353):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25690234/IMG_0041.jpg 2048w, https://duet-cdn.vox-cdn.com/thumbor/0x0:1290x703/2400x1308/filters:focal(645x352:646x353):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25690234/IMG_0041.jpg 2400w" src="https://duet-cdn.vox-cdn.com/thumbor/0x0:1290x703/2400x1308/filters:focal(645x352:646x353):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25690234/IMG_0041.jpg" data-old-src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7"></span></p></div></figure></div><div><figcaption><em>My left ear has a bit more hearing loss than my right.</em></figcaption> <p><cite>Screenshot: Chris Welch / The Verge</cite></p></div></div><p>I took the test twice several days apart, and my results for both ears fall under “little to no hearing loss.” Having recently turned 40, I’ll take that. The ranges are as follows:</p><div><ul><li>Little to no loss: up to 25dBHL</li><li>Mild loss: 26–40dBHL</li><li>Moderate loss: 41–60dBHL</li><li>Severe loss: 61–80dBHL</li><li>Profound loss: above 80dBHL</li></ul></div><p>I also learned that my left ear has definitely lost a bit more over the years than my right, which is something I’ve never noticed in daily life. The slight difference between my two tests is exactly the margin that Apple expects for people who take it multiple times. Results are stored in the Health app, where you can export individual tests (or all of them) as a PDF. Here’s how one of mine charts out: </p><div><div role="button" aria-label="Zoom" tabindex="0"><figure><div><p><span><img alt="A screenshot of Apple hearing test chart data." loading="lazy" decoding="async" data-nimg="fill" sizes="(max-width: 1023px) 100vw, 744px" srcset="https://duet-cdn.vox-cdn.com/thumbor/0x0:1290x1247/376x363/filters:focal(645x624:646x625):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25690236/IMG_0042.jpg 376w, https://duet-cdn.vox-cdn.com/thumbor/0x0:1290x1247/384x371/filters:focal(645x624:646x625):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25690236/IMG_0042.jpg 384w, https://duet-cdn.vox-cdn.com/thumbor/0x0:1290x1247/415x401/filters:focal(645x624:646x625):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25690236/IMG_0042.jpg 415w, https://duet-cdn.vox-cdn.com/thumbor/0x0:1290x1247/480x464/filters:focal(645x624:646x625):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25690236/IMG_0042.jpg 480w, https://duet-cdn.vox-cdn.com/thumbor/0x0:1290x1247/540x522/filters:focal(645x624:646x625):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25690236/IMG_0042.jpg 540w, https://duet-cdn.vox-cdn.com/thumbor/0x0:1290x1247/640x619/filters:focal(645x624:646x625):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25690236/IMG_0042.jpg 640w, https://duet-cdn.vox-cdn.com/thumbor/0x0:1290x1247/750x725/filters:focal(645x624:646x625):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25690236/IMG_0042.jpg 750w, https://duet-cdn.vox-cdn.com/thumbor/0x0:1290x1247/828x800/filters:focal(645x624:646x625):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25690236/IMG_0042.jpg 828w, https://duet-cdn.vox-cdn.com/thumbor/0x0:1290x1247/1080x1044/filters:focal(645x624:646x625):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25690236/IMG_0042.jpg 1080w, https://duet-cdn.vox-cdn.com/thumbor/0x0:1290x1247/1200x1160/filters:focal(645x624:646x625):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25690236/IMG_0042.jpg 1200w, https://duet-cdn.vox-cdn.com/thumbor/0x0:1290x1247/1440x1392/filters:focal(645x624:646x625):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25690236/IMG_0042.jpg 1440w, https://duet-cdn.vox-cdn.com/thumbor/0x0:1290x1247/1920x1856/filters:focal(645x624:646x625):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25690236/IMG_0042.jpg 1920w, https://duet-cdn.vox-cdn.com/thumbor/0x0:1290x1247/2048x1980/filters:focal(645x624:646x625):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25690236/IMG_0042.jpg 2048w, https://duet-cdn.vox-cdn.com/thumbor/0x0:1290x1247/2400x2320/filters:focal(645x624:646x625):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25690236/IMG_0042.jpg 2400w" src="https://duet-cdn.vox-cdn.com/thumbor/0x0:1290x1247/2400x2320/filters:focal(645x624:646x625):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25690236/IMG_0042.jpg" data-old-src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7"></span></p></div></figure></div><div><figcaption><em>Hearing test results can easily be accessed from the Health app.</em></figcaption> <p><cite>Screenshot: Chris Welch / The Verge</cite></p></div></div><p>You can also import charts from tests you’ve taken with a hearing professional. This data is what’s used to configure the hearing aid feature. </p><p>Apple’s hearing test only takes about five minutes, but it felt like a <em>long</em> five minutes for me and everyone else I’ve let try it. Again, that’s probably because it’s been too long since many of us have done this. The second time wasn’t nearly as stressful. I can definitely see the hearing test having a viral moment on TikTok and other social media, which seems like a great thing for awareness all around. Hearing loss is incredibly common: 1.5 billion people around the world are living with some extent of it, according to the World Health Organization.</p><p><h3>Assistance: AirPods as hearing aids</h3></p><p>For those 18 years and older with mild to moderate hearing loss, the AirPods Pro 2 can now serve as a clinical-grade hearing aid. Once enabled, you can also toggle on a “Media Assist” setting that uses your hearing test results to optimize the sound of music, phone calls, and video content. </p><p>Within the settings menu, you can use sliders to fine-tune the hearing aid feature’s amplification, tone, and balance. These options are also accessible via Control Center on an iPhone, iPad, or Mac. Much like you can slide a finger on the AirPods Pro 2 stem to adjust volume, you can use that same gesture to control amplification when the hearing aid mode is active. You can only use the hearing aid feature when in transparency mode. Apple’s instructions for the hearing aid feature advise that it takes time — in some cases, weeks — for customers to get fully accustomed to the sound. </p><p>Jerry Saltz, the art critic at <em>New York </em>Magazine (and someone with diagnosed hearing loss), took Apple’s hearing test at our Manhattan office and was impressed in his brief time trying out the hearing aid feature. But no one has had access to it long enough for an in-depth assessment. I’m sure there’ll be detailed comparisons between the AirPods Pro 2 and existing OTC devices in the near future once iOS 18.1 is widely available.</p><div><div role="button" aria-label="Zoom" tabindex="0"><figure><div><p><span><img alt="A photo showing the hearing aid setting on an iPhone in a crowded bar." loading="lazy" decoding="async" data-nimg="fill" sizes="(max-width: 1023px) 100vw, 744px" srcset="https://duet-cdn.vox-cdn.com/thumbor/0x0:2040x1360/376x251/filters:focal(1020x680:1021x681):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25690981/DSC_1024_Enhanced_NR_2.jpg 376w, https://duet-cdn.vox-cdn.com/thumbor/0x0:2040x1360/384x256/filters:focal(1020x680:1021x681):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25690981/DSC_1024_Enhanced_NR_2.jpg 384w, https://duet-cdn.vox-cdn.com/thumbor/0x0:2040x1360/415x277/filters:focal(1020x680:1021x681):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25690981/DSC_1024_Enhanced_NR_2.jpg 415w, https://duet-cdn.vox-cdn.com/thumbor/0x0:2040x1360/480x320/filters:focal(1020x680:1021x681):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25690981/DSC_1024_Enhanced_NR_2.jpg 480w, https://duet-cdn.vox-cdn.com/thumbor/0x0:2040x1360/540x360/filters:focal(1020x680:1021x681):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25690981/DSC_1024_Enhanced_NR_2.jpg 540w, https://duet-cdn.vox-cdn.com/thumbor/0x0:2040x1360/640x427/filters:focal(1020x680:1021x681):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25690981/DSC_1024_Enhanced_NR_2.jpg 640w, https://duet-cdn.vox-cdn.com/thumbor/0x0:2040x1360/750x500/filters:focal(1020x680:1021x681):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25690981/DSC_1024_Enhanced_NR_2.jpg 750w, https://duet-cdn.vox-cdn.com/thumbor/0x0:2040x1360/828x552/filters:focal(1020x680:1021x681):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25690981/DSC_1024_Enhanced_NR_2.jpg 828w, https://duet-cdn.vox-cdn.com/thumbor/0x0:2040x1360/1080x720/filters:focal(1020x680:1021x681):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25690981/DSC_1024_Enhanced_NR_2.jpg 1080w, https://duet-cdn.vox-cdn.com/thumbor/0x0:2040x1360/1200x800/filters:focal(1020x680:1021x681):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25690981/DSC_1024_Enhanced_NR_2.jpg 1200w, https://duet-cdn.vox-cdn.com/thumbor/0x0:2040x1360/1440x960/filters:focal(1020x680:1021x681):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25690981/DSC_1024_Enhanced_NR_2.jpg 1440w, https://duet-cdn.vox-cdn.com/thumbor/0x0:2040x1360/1920x1280/filters:focal(1020x680:1021x681):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25690981/DSC_1024_Enhanced_NR_2.jpg 1920w, https://duet-cdn.vox-cdn.com/thumbor/0x0:2040x1360/2048x1365/filters:focal(1020x680:1021x681):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25690981/DSC_1024_Enhanced_NR_2.jpg 2048w, https://duet-cdn.vox-cdn.com/thumbor/0x0:2040x1360/2400x1600/filters:focal(1020x680:1021x681):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25690981/DSC_1024_Enhanced_NR_2.jpg 2400w" src="https://duet-cdn.vox-cdn.com/thumbor/0x0:2040x1360/2400x1600/filters:focal(1020x680:1021x681):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25690981/DSC_1024_Enhanced_NR_2.jpg" data-old-src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7"></span></p></div></figure></div><p><figcaption><em>You can make hearing aid adjustments from the iOS settings menu.</em></figcaption></p></div><p>Being able to use Apple’s $250 earbuds as a hearing aid is a huge deal for those who can benefit from this capability. That’s substantially less expensive than over-the-counter hearing aids from <a href="https://www.jabraenhance.com/product">Jabra</a>, <a href="https://hearing.electronics.sony.com/lp2_hearingaids">Sony</a>, and other brands. But the AirPods won’t be right for everyone. People with more severe hearing loss will still need to seek out other solutions (including those pricier products). And the main tradeoff with the AirPods Pro 2 is battery life: they can last for around six hours with the hearing aid engaged, which doesn’t match what you’ll get from many OTC and prescription hearing aids. </p><p>But this is a big milestone — and it seems inevitable that Samsung, Google, and other tech heavyweights will follow Apple’s lead fairly quickly. I’m all for that, even if it feels strange that hearing aid functionality has become the latest aspect of ecosystem lock-in. We spent a long time bemoaning the loss of the headphone jack. With advancements like this, and earbuds helping to improve so many people’s quality of life, we’re finally starting to see a worthwhile payoff. </p><p><em>Photography by Chris Welch / The Verge</em></p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Wired's Attack on Privacy (111 pts)]]></title>
            <link>https://simplex.chat/blog/20241016-wired-attack-on-privacy.html</link>
            <guid>41909941</guid>
            <pubDate>Tue, 22 Oct 2024 00:20:31 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://simplex.chat/blog/20241016-wired-attack-on-privacy.html">https://simplex.chat/blog/20241016-wired-attack-on-privacy.html</a>, See on <a href="https://news.ycombinator.com/item?id=41909941">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="article">
<p><img src="https://simplex.chat/blog/images/20241016-wired-privacy.jpg" width="330"></p><p>The <a href="https://www.wired.com/story/neo-nazis-flee-telegram-encrypted-app-simplex/">Wired article</a> by David Gilbert focusing on neo-Nazis moving to SimpleX Chat following the Telegram's changes in privacy policy is biased and misleading. By cherry-picking information from <a href="https://www.isdglobal.org/digital_dispatches/neo-nazi-accelerationists-seek-new-digital-refuge-amid-looming-telegram-crackdown/">the report</a> by the Institute for Strategic Dialogue (ISD), Wired fails to mention that SimpleX network design prioritizes privacy in order to protect human rights defenders, journalists, and everyday users who value their privacy — many people feel safer using SimpleX than non-private apps, being protected from strangers contacting them.</p>
<p>Yes, privacy-focused SimpleX network offers encryption and anonymity — that’s the point. To paint this as problematic solely because of who may use such apps misses the broader, critical context.</p>
<p>SimpleX’s true strength lies in protection of <a href="https://simplex.chat/blog/20240416-dangers-of-metadata-in-messengers.html">users' metadata</a>, which can reveal sensitive information about who is communicating, when, and how often. SimpleX protocols are designed to minimize metadata collection. For countless people, especially vulnerable groups, these features can be life-saving. Wired article ignores these essential protections, and overlooks the positive aspects of having such a unique design, as noted in the publication which <a href="https://www.maargentino.com/is-telegrams-privacy-shift-driving-extremists-toward-simplex/">they link to</a>:</p>
<blockquote>
<p><em>“SimpleX also has a significant advantage when it comes to protecting metadata — the information that can reveal who you’re talking to, when, and how often. SimpleX is designed with privacy at its core, minimizing the amount of metadata collected and ensuring that any temporary data necessary for functionality is not retained or linked to identifiable users.”</em></p>
</blockquote>
<p>Both publications referenced by Wired also explore how SimpleX design actually hinders extremist groups from spreading propaganda or building large networks. SimpleX design restricts message visibility and file retention, making it far from ideal for those looking to coordinate large networks. Yet these important qualities are ignored by Wired in favor of fear-mongering about encryption — an argument we've seen before when apps like Signal <a href="https://foreignpolicy.com/2021/03/13/telegram-signal-apps-right-wing-extremism-islamic-state-terrorism-violence-europol-encrypted/">faced similar treatment</a>. Ironically, Wired just a month earlier encouraged its readers to <a href="https://www.wired.com/story/gadget-lab-podcast-657/">adopt encrypted messaging apps</a>, making its current stance even more contradictory.</p>
<p>The vilification of apps that offer critically important privacy, anonymity, and encryption must stop. That a small share of users may abuse these tools doesn’t justify broad criticism. Additionally, the lobbying for client-side scanning, which Wired’s article seems to indirectly endorse, is not only dangerous but goes against fundamental principles of free speech and personal security. We strongly oppose the use of private communications for any kind of monitoring, including AI training, which would undermine the very trust encryption is designed to build.</p>
<p>It’s alarming to see Wired not only criticize SimpleX for its strong privacy protections but also subtly blame the European Court of Human Rights for <a href="https://www.theregister.com/2024/02/15/echr_backdoor_encryption/">upholding basic human rights</a> by rejecting laws that would force encrypted apps to scan and hand over private messages before encryption. Wired writes:</p>
<blockquote>
<p><em>…European Court of Human Rights decision in February of this year ruled that forcing encrypted messaging apps to provide a backdoor to law enforcement was illegal. This decision undermined the EU’s controversial proposal that would potentially force encrypted messaging apps to scan all user content for identifiers of child sexual abuse material.</em></p>
</blockquote>
<p>This commentary is both inappropriate and misguided — it plays into the hands of anti-privacy lobbyists attempting to criminalize access to private communications. Framing privacy and anonymity as tools for criminals ignores the reality that these protections are essential for millions of legitimate users, from activists to journalists, to ordinary citizens. Client-side scanning can't have any meaningful effect on reducing CSAM distribution, instead resulting in increase of crime and abuse when criminals get access to this data.</p>
<p>We need to correct this narrative. The real danger lies not in protecting communication, but in failing to do so. Privacy apps like SimpleX are crucial, not just for those resisting mass surveillance, but for everyone who values the right to communicate without fear of their conversations being monitored or misused. This is a right we must defend and incorporate into law, <a href="https://simplex.chat/blog/20240704-future-of-privacy-enforcing-privacy-standards.html">as we wrote before</a>.</p>
<p>Wired could have stood on the right side of this battle and helped normalize the demand for privacy, genuinely protecting people from criminals and from the exploitation of the increasingly AI-enabled mass surveillance. Instead they chose the path of spreading fear and uncertainty of encrypted messaging and tools that enable privacy and anonymity.</p>
<p>Spreading misinformation about privacy and security undermines trust in the tools that protect us, making it easier to justify more invasive surveillance measures that chip away at our civil liberties.</p>
<p>Wired did not respond to our request for comment.</p>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Learning to Learn (232 pts)]]></title>
            <link>https://kevin.the.li/posts/learning-to-learn/</link>
            <guid>41909827</guid>
            <pubDate>Tue, 22 Oct 2024 00:01:23 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://kevin.the.li/posts/learning-to-learn/">https://kevin.the.li/posts/learning-to-learn/</a>, See on <a href="https://news.ycombinator.com/item?id=41909827">Hacker News</a></p>
<div id="readability-page-1" class="page"><article id="article"> <p>In the past 3 years, I’ve run over 300 interviews.  On average, I meet someone
new every 3-4 days to understand if they’re the right fit for an early-stage
startup.</p>
<p>I especially like capping interviews with an open-ended question that really
lets great candidates shine. One of my favorite now-retired questions is,
<a href="https://www.safegraph.com/blog/why-the-famous-peter-thiel-interview-question-is-so-predictive" rel="noopener" target="_blank">in Thielian fashion</a>:
“What’s something you’ve learned that you believe gives you an edge - something
that you’re almost surprised more people don’t know about?”</p>
<p>One of the all-time best answers I heard was this:</p>
<blockquote>
<p>When you’re starting something new, the most important thing is knowing what to learn.</p>
</blockquote>
<p>Followed by unpacking an optimal learning flow:</p>
<ol>
<li>Very quickly identify what the foundational knowledge is.</li>
<li>Build a personal curriculum to become an expert and avoid the trap of
the <a href="https://daedtech.com/how-developers-stop-learning-rise-of-the-expert-beginner/" rel="noopener" target="_blank">expert beginner</a>.</li>
<li>Sprint hard the first 15-20 hours to impress initial memory, then decelerate to a
more regular pace.</li>
</ol>
<p>#1 and #2 are common structured approaches to learning, whereas #3 is a
relatively novel interpretation of <a href="https://en.wikipedia.org/wiki/Spaced_repetition" rel="noopener" target="_blank">spaced repetition</a>.
But at a more meta level, the unexpected takeaway is the vast majority of people
likely haven’t updated their mental model for learning in decades. (Stop and
think for a moment on the last time you did this.)</p>
<p>Learning to learn is extremely high leverage. 40 hours at 25% efficiency is the
same as 12.5 hours at 80% efficiency. And it turns out that being <em>productively
honest</em> is one of the most effective and kindest things you can do
for yourself.</p> </article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Math is still catching up to the genius of Ramanujan (689 pts)]]></title>
            <link>https://www.quantamagazine.org/srinivasa-ramanujan-was-a-genius-math-is-still-catching-up-20241021/</link>
            <guid>41909564</guid>
            <pubDate>Mon, 21 Oct 2024 23:19:00 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.quantamagazine.org/srinivasa-ramanujan-was-a-genius-math-is-still-catching-up-20241021/">https://www.quantamagazine.org/srinivasa-ramanujan-was-a-genius-math-is-still-catching-up-20241021/</a>, See on <a href="https://news.ycombinator.com/item?id=41909564">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-role="selectable">
    <p><span>O</span>ne afternoon in January 2011, <a href="https://webusers.imj-prg.fr/~hussein.mourtada/">Hussein Mourtada</a> leapt onto his desk and started dancing. He wasn’t alone: Some of the graduate students who shared his Paris office were there, too. But he didn’t care. The mathematician realized that he could finally confirm a sneaking suspicion he’d first had while writing his doctoral dissertation, which he’d finished a few months earlier. He’d been studying special points, called singularities, where curves cross themselves or come to sharp turns. Now he had unexpectedly found what he’d been looking for, a way to prove that these singularities had a surprisingly deep underlying structure. Hidden within that structure were mysterious mathematical statements first written down a century earlier by a young Indian mathematician named Srinivasa Ramanujan. They had come to him in a dream.</p>
<p>Ramanujan brings life to the myth of the self-taught genius. He grew up poor and uneducated and did much of his research while isolated in southern India, barely able to afford food. In 1912, when he was 24, he began to send a series of letters to prominent mathematicians. These were mostly ignored, but one recipient, the English mathematician G.H. Hardy, corresponded with Ramanujan for a year and eventually persuaded him to come to England, smoothing the way with the colonial bureaucracies.</p>
<p>It became apparent to Hardy and his colleagues that Ramanujan could sense mathematical truths — could access entire worlds — that others simply could not. (Hardy, a mathematical giant in his own right, is said to have quipped that his greatest contribution to mathematics was the discovery of Ramanujan.) Before Ramanujan died in 1920 at the age of 32, he came up with thousands of elegant and surprising results, often without proof. He was fond of saying that his equations had been bestowed on him by the gods.</p>
<p>More than 100 years later, mathematicians are still trying to catch up to Ramanujan’s divine genius, as his visions appear again and again in disparate corners of the world of mathematics.</p>

<p>Ramanujan is perhaps most famous for coming up with partition identities, equations about the different ways you can break a whole number up into smaller parts (such as 7 = 5 + 1 + 1). In the 1980s, mathematicians began to find deep and surprising connections between these equations and other areas of mathematics: in statistical mechanics and the study of phase transitions, in knot theory and string theory, in number theory and representation theory and the study of symmetries.</p>
<p>Most recently, they’ve appeared in Mourtada’s work on curves and surfaces that are defined by algebraic equations, an area of study called algebraic geometry. Mourtada and his collaborators have spent more than a decade trying to better understand that link, and to exploit it to uncover rafts of brand-new identities that resemble those Ramanujan wrote down.</p>
<p>“It turned out that these kinds of results have basically occurred in almost every branch of mathematics. That’s an amazing thing,” said <a href="https://people.smp.uq.edu.au/OleWarnaar/">Ole Warnaar</a> of the University of Queensland in Australia. “It’s not just a happy coincidence. I don’t want to sound religious, but the mathematical god is trying to tell us something.”</p>
<h2><strong>New Worlds </strong></h2>
<p>Ramanujan’s mathematical prowess was obvious to those who knew him. Without formal training, he excelled; by the time he was in high school he had devoured advanced, though often outdated, textbooks, and was doing independent research on different kinds of numerical properties and patterns.</p>
<p>In 1904, he was granted a full scholarship to the Government Arts College in Kumbakonam, the small city where he had grown up, in what is now the Indian state of Tamil Nadu. But he ignored all subjects besides math and lost his scholarship within a year. He later enrolled in another university, this time in Madras (now Chennai), the provincial capital some 250 kilometers north. Again he flunked out.</p>
<figure>
    <p><img width="1300" height="1041" src="https://www.quantamagazine.org/wp-content/uploads/2024/10/Ramanujan-Missing-Boy_cr-The-Hindu.webp" alt="" decoding="async" loading="lazy" srcset="https://www.quantamagazine.org/wp-content/uploads/2024/10/Ramanujan-Missing-Boy_cr-The-Hindu.webp 1300w, https://www.quantamagazine.org/wp-content/uploads/2024/10/Ramanujan-Missing-Boy_cr-The-Hindu-520x416.webp 520w, https://www.quantamagazine.org/wp-content/uploads/2024/10/Ramanujan-Missing-Boy_cr-The-Hindu-768x615.webp 768w" sizes="(max-width: 1300px) 100vw, 1300px">    </p>
            <figcaption>
                            <p>After failing out of college, Ramanujan ran away from home, prompting his mother to post a missing-person notice in <em>The Hindu</em>.</p>
            <p><em>The Hindu</em></p>
        </figcaption>
    </figure>

<p>He continued his research on his own for years, often while in poor health. During that time, he tutored students in math to support himself. Eventually he secured a job as a clerk at the Madras Port Trust in 1912. He pursued mathematics on the side and published some of his results in Indian journals.</p>
<p>Hoping to get some of his work into more prestigious and widely read publications, Ramanujan wrote letters to several British mathematicians, enclosing pages of findings for their review. “I have not trodden through the conventional regular course which is followed in a university course,” he wrote, “but I am striking out a new path for myself.” Among the recipients was Hardy, an expert in number theory and analysis at the University of Cambridge.</p>
<figure>
    <p><img width="1400" height="2106" src="https://www.quantamagazine.org/wp-content/uploads/2024/10/FirstLetterLastPage-Courtesy_KenOno.webp" alt="" decoding="async" loading="lazy" srcset="https://www.quantamagazine.org/wp-content/uploads/2024/10/FirstLetterLastPage-Courtesy_KenOno.webp 1400w, https://www.quantamagazine.org/wp-content/uploads/2024/10/FirstLetterLastPage-Courtesy_KenOno-1143x1720.webp 1143w, https://www.quantamagazine.org/wp-content/uploads/2024/10/FirstLetterLastPage-Courtesy_KenOno-346x520.webp 346w, https://www.quantamagazine.org/wp-content/uploads/2024/10/FirstLetterLastPage-Courtesy_KenOno-768x1155.webp 768w, https://www.quantamagazine.org/wp-content/uploads/2024/10/FirstLetterLastPage-Courtesy_KenOno-1021x1536.webp 1021w, https://www.quantamagazine.org/wp-content/uploads/2024/10/FirstLetterLastPage-Courtesy_KenOno-1361x2048.webp 1361w" sizes="(max-width: 1400px) 100vw, 1400px">    </p>
            <figcaption>
                            <p>Ramanujan’s first letter to G.H. Hardy included formulas (5), (6) and (7), strange nested fractions that Hardy said “defeated me completely; I had never seen anything in the least like them before.”</p>
            <p>Courtesy of Ken Ono</p>
        </figcaption>
    </figure>

<p>Hardy was shocked at what he saw. Ramanujan had identified and then solved a number of continued fractions — expressions that can be written as infinite nests of fractions within fractions, such as:</p>
<figure>
    <p><img src="https://www.quantamagazine.org/wp-content/uploads/2024/10/Ramanujan_Equation.svg" alt="" decoding="async" loading="lazy">    </p>
    </figure>

<p>They “defeated me completely; I had never seen anything in the least like them before,” Hardy later wrote. “They must be true because, if they were not true, no one would have had the imagination to invent them.” The formulas, unproved, were so striking that they inspired Hardy to offer Ramanujan a fellowship at Cambridge. In 1914, Ramanujan arrived in England, and for the next five years he studied and collaborated with Hardy.</p>

<p>One of Ramanujan’s first tasks was to prove a general statement about his continued fractions. To do so, he needed to prove two other statements. But he couldn’t. Neither could Hardy, nor could any of the colleagues he reached out to.</p>
<p>It turned out that they didn’t need to. The statements had been proved 20 years earlier by a little-known English mathematician named L.J. Rogers. Rogers wrote poorly, and at the time the proofs were published no one paid any attention. (Rogers was content to do his research in relative obscurity, play piano, garden and apply his spare time to a variety of other pursuits.) Ramanujan uncovered this work in 1917, and the pair of statements later became known as the Rogers-Ramanujan identities.</p>
<p>Amid Ramanujan’s prodigious output, these statements stand out. They have carried through the decades and across nearly all of mathematics. They are the seeds that mathematicians continue to sow, growing brilliant new gardens seemingly wherever they fall.</p>
<p>Ramanujan fell ill and returned to India in 1919, where he died the next year. It would fall to others to explore the world his identities had revealed.</p>
<h2><strong>The Music of the Game</strong></h2>
<p>Hussein Mourtada grew up in the 1980s in Lebanon, in a small city called Baalbek. As a teenager, he didn’t like studying and preferred to play: soccer, billiards, basketball. Math, too. “It looked like a game,” he said. “And I liked playing.”</p>
<p>As an undergraduate at the Lebanese University in Beirut, he studied both law and mathematics, with an eye to a legal career. But he soon found that while he enjoyed the philosophical aspects of law, he did not enjoy it in practice. He turned his attention to math, where he was particularly drawn to the community. As a child, his teachers and classmates were what excited him about going to school, even though he often fell asleep during class. As a budding mathematician, “I had the impression that these are beautiful people,” he said. “They are honest. You need to be honest with yourself to be a mathematician. Otherwise, it doesn’t work.”</p>
<figure>
    <p><img width="1300" height="866" src="https://www.quantamagazine.org/wp-content/uploads/2024/10/HusseinMourtada_crBasmaJaffal.webp" alt="" decoding="async" loading="lazy" srcset="https://www.quantamagazine.org/wp-content/uploads/2024/10/HusseinMourtada_crBasmaJaffal.webp 1300w, https://www.quantamagazine.org/wp-content/uploads/2024/10/HusseinMourtada_crBasmaJaffal-520x346.webp 520w, https://www.quantamagazine.org/wp-content/uploads/2024/10/HusseinMourtada_crBasmaJaffal-768x512.webp 768w" sizes="(max-width: 1300px) 100vw, 1300px">    </p>
            <figcaption>
                            <p>Hussein Mourtada has been bringing Ramanujan’s work into the 21st century.</p>
            <p>Basma Jaffal</p>
        </figcaption>
    </figure>

<p>He moved to France for his doctorate and started to focus on algebraic geometry — the study of algebraic varieties, or shapes cut out by polynomial equations. These are equations that can be written as sums of variables raised to whole-number powers. A line, for instance, is cut out by the equation <em>x</em> + <em>y</em> = 0, a circle by <em>x</em><sup>2</sup> + <em>y</em><sup>2</sup> = 1, a figure eight by <em>x</em><sup>4</sup> = <em>x</em><sup>2</sup> − <em>y</em><sup>2</sup>. While the line and circle are completely smooth, the figure eight has a point where it intersects itself — a singularity.</p>
<p>It’s easy to spot singularities when you’re dealing with shapes that you can draw on a sheet of paper. But higher-dimensional algebraic varieties are far more complicated and impossible to visualize. Algebraic geometers are in the business of understanding their singularities, too.</p>
<p>They’ve developed all sorts of tools to do this. One dates back to the mathematician John Nash, who in the 1960s started studying related objects called arc spaces. Nash would take a point, or singularity, and define infinitely many short trajectories — little arcs — that passed through it. By looking at all these short trajectories together, he could test how smooth his variety was at that point. “If you want to see if it’s smooth, you want to pet it,” said <a href="https://www.lix.polytechnique.fr/Labo/Gleb.POGUDIN/">Gleb Pogudin</a> of the École Polytechnique in France.</p>
<p>In practical terms, an arc space provides an infinite collection of polynomial equations. “This is really the thing that Mourtada is expert in: understanding the meaning of those equations,” said <a href="https://webusers.imj-prg.fr/~bernard.teissier/">Bernard Teissier</a>, a colleague of Mourtada’s at the Institute of Mathematics of Jussieu in Paris. “Because these equations can be very complicated. But they have a certain music to them. There is a lot of structure which governs the nature of these equations, and he’s just the person, I think, who best listens to this music and understands what it means.”</p>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[A near impossible literacy test Louisiana used to suppress the black vote (246 pts)]]></title>
            <link>https://www.openculture.com/2024/10/take-the-near-impossible-literacy-test-louisiana-used-to-suppress-the-black-vote.html</link>
            <guid>41908701</guid>
            <pubDate>Mon, 21 Oct 2024 21:29:33 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.openculture.com/2024/10/take-the-near-impossible-literacy-test-louisiana-used-to-suppress-the-black-vote.html">https://www.openculture.com/2024/10/take-the-near-impossible-literacy-test-louisiana-used-to-suppress-the-black-vote.html</a>, See on <a href="https://news.ycombinator.com/item?id=41908701">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
			<p><img loading="lazy" fetchpriority="high" decoding="async" src="https://cdn8.openculture.com/2024/10/20191430/cf10083a-fc0b-4a26-b321-1f63373f6c22.avif" alt="" width="920" height="1334" srcset="https://cdn8.openculture.com/2024/10/20191430/cf10083a-fc0b-4a26-b321-1f63373f6c22.avif 920w, https://cdn8.openculture.com/2024/10/20191430/cf10083a-fc0b-4a26-b321-1f63373f6c22-248x360.avif 248w, https://cdn8.openculture.com/2024/10/20191430/cf10083a-fc0b-4a26-b321-1f63373f6c22-706x1024.avif 706w, https://cdn8.openculture.com/2024/10/20191430/cf10083a-fc0b-4a26-b321-1f63373f6c22-166x240.avif 166w, https://cdn8.openculture.com/2024/10/20191430/cf10083a-fc0b-4a26-b321-1f63373f6c22-768x1114.avif 768w" sizes="(max-width: 920px) 100vw, 920px" data-old-src="https://www.openculture.com/wp-content/plugins/native-lazyload/assets/images/placeholder.svg"></p>
<p>In William Faulkner’s 1938 nov­el <a href="https://amzn.to/3NwnMlg"><em>The Unvan­quished</em></a>, the implaca­ble Colonel Sar­toris takes dras­tic action to stop the elec­tion of a black Repub­li­can can­di­date to office after the Civ­il War, destroy­ing the bal­lots of black vot­ers and shoot­ing two North­ern car­pet­bag­gers. While such dra­mat­ic means of vot­er sup­pres­sion occurred often enough in the Recon­struc­tion South, tac­tics of elec­toral exclu­sion refined over time, such that by the mid-twen­ti­eth cen­tu­ry the Jim Crow South relied large­ly on near­ly impos­si­ble-to-pass lit­er­a­cy tests to impede free and fair elec­tions.</p>
<p>These tests, <a href="http://www.slate.com/blogs/the_vault/2013/06/28/voting_rights_and_the_supreme_court_the_impossible_literacy_test_louisiana.html">writes Rebec­ca Onion at <em>Slate</em></a>, were “sup­pos­ed­ly applic­a­ble to both white and black prospec­tive vot­ers who couldn’t prove a cer­tain lev­el of edu­ca­tion” (typ­i­cal­ly up to the fifth grade). Yet they were “in actu­al­i­ty dis­pro­por­tion­ate­ly admin­is­tered to black vot­ers.”</p>


<p>Addi­tion­al­ly, many of the tests were rigged so that reg­is­trars could give poten­tial vot­ers an easy or a dif­fi­cult ver­sion, and could score them dif­fer­ent­ly as well. For exam­ple, the <a href="http://www.crmvet.org/info/lithome.htm">Vet­er­ans of the Civ­il Rights Move­ment</a> describes a test admin­is­tered in Alaba­ma that is so entire­ly sub­jec­tive that it mea­sures the registrar’s shrewd­ness and cun­ning more than any­thing else.</p>
<p><img loading="lazy" decoding="async" src="https://cdn8.openculture.com/2024/10/20191516/ae77f4f1-7de5-429f-a21b-7b5d3649a7cd.avif" alt="" width="920" height="1259" srcset="https://cdn8.openculture.com/2024/10/20191516/ae77f4f1-7de5-429f-a21b-7b5d3649a7cd.avif 920w, https://cdn8.openculture.com/2024/10/20191516/ae77f4f1-7de5-429f-a21b-7b5d3649a7cd-263x360.avif 263w, https://cdn8.openculture.com/2024/10/20191516/ae77f4f1-7de5-429f-a21b-7b5d3649a7cd-748x1024.avif 748w, https://cdn8.openculture.com/2024/10/20191516/ae77f4f1-7de5-429f-a21b-7b5d3649a7cd-175x240.avif 175w, https://cdn8.openculture.com/2024/10/20191516/ae77f4f1-7de5-429f-a21b-7b5d3649a7cd-768x1051.avif 768w" sizes="(max-width: 920px) 100vw, 920px" data-old-src="https://www.openculture.com/wp-content/plugins/native-lazyload/assets/images/placeholder.svg"></p>
<p>The test here from Louisiana con­sists of ques­tions so ambigu­ous that no one, what­ev­er their lev­el of edu­ca­tion, can divine a “right” or “wrong” answer to most of them. And yet, as the instruc­tions state, “one wrong answer denotes fail­ure of the test,” an impos­si­ble stan­dard for even a legit­i­mate exam. Even worse, vot­ers had only ten min­utes to com­plete the three-page, 30-ques­tion doc­u­ment. The Louisiana test dates from 1964, the year before the pas­sage of the <a href="https://www.archives.gov/milestone-documents/voting-rights-act">Vot­ing Rights Act</a>, which effec­tive­ly put an end to these bla­tant­ly dis­crim­i­na­to­ry prac­tices.</p>
<p>Learn more about the his­to­ry of Jim Crow vot­er sup­pres­sion at Rebec­ca Onion’s orig­i­nal post <a href="http://www.slate.com/blogs/the_vault/2013/06/28/voting_rights_and_the_supreme_court_the_impossible_literacy_test_louisiana.html">here</a> and an update <a href="http://www.slate.com/blogs/the_vault/2013/07/03/louisiana_literacy_test_update_the_hunt_for_the_original_document.html">here</a>. And <a href="https://www.openculture.com/2014/11/harvard-students-fail-the-literacy-test.html">here you can watch video of Har­vard stu­dents try­ing to take the test</a>.</p>
<p><img loading="lazy" decoding="async" src="https://cdn8.openculture.com/2024/10/20191550/021b9032-9783-42e3-889a-caec9a10384e.avif" alt="" width="920" height="1256" srcset="https://cdn8.openculture.com/2024/10/20191550/021b9032-9783-42e3-889a-caec9a10384e.avif 920w, https://cdn8.openculture.com/2024/10/20191550/021b9032-9783-42e3-889a-caec9a10384e-264x360.avif 264w, https://cdn8.openculture.com/2024/10/20191550/021b9032-9783-42e3-889a-caec9a10384e-750x1024.avif 750w, https://cdn8.openculture.com/2024/10/20191550/021b9032-9783-42e3-889a-caec9a10384e-176x240.avif 176w, https://cdn8.openculture.com/2024/10/20191550/021b9032-9783-42e3-889a-caec9a10384e-768x1048.avif 768w" sizes="(max-width: 920px) 100vw, 920px" data-old-src="https://www.openculture.com/wp-content/plugins/native-lazyload/assets/images/placeholder.svg"></p>
<p>Note: Note: An ear­li­er ver­sion of this post appeared on our site in 2014.</p>
<p><strong>Relat­ed Con­tent:</strong></p>
<p><a title="Permanent Link to Watch Harvard Students Fail the Literacy Test Louisiana Used to Suppress the Black Vote in 1964" href="https://www.openculture.com/2014/11/harvard-students-fail-the-literacy-test.html" rel="bookmark">Watch Har­vard Stu­dents Fail the Lit­er­a­cy Test Louisiana Used to Sup­press the Black Vote in 1964</a></p>
<p><a title="Permanent Link to Philosopher Richard Rorty Chillingly Predicts the Results of the 2016 Election … Back in 1998" href="https://www.openculture.com/2016/11/philosopher-richard-rorty-chillingly-predicts-the-results-of-the-2016-election.html" rel="bookmark">Philoso­pher Richard Rorty Chill­ing­ly Pre­dicts the Results of the 2016 Elec­tion … Back in 1998</a></p>
<p><em><a href="http://about.me/jonesjoshua">Josh Jones</a>&nbsp;is a writer and musi­cian based in Durham, NC. Fol­low him at&nbsp;<a href="https://twitter.com/jdmagness">@jdmagness</a></em></p>
<br>		</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Thought experiments that fray the fabric of space-time (142 pts)]]></title>
            <link>https://www.quantamagazine.org/the-thought-experiments-that-fray-the-fabric-of-space-time-20240925/</link>
            <guid>41908541</guid>
            <pubDate>Mon, 21 Oct 2024 21:08:22 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.quantamagazine.org/the-thought-experiments-that-fray-the-fabric-of-space-time-20240925/">https://www.quantamagazine.org/the-thought-experiments-that-fray-the-fabric-of-space-time-20240925/</a>, See on <a href="https://news.ycombinator.com/item?id=41908541">Hacker News</a></p>
<div id="readability-page-1" class="page">
                    <header>
                
                
                
            
            </header>
        <main>
    
</main>
    
    



<div data-function="toggle" data-name="reset-password">
        <p>Change your password</p>
        <p>Enter your new password</p>
    </div>


















<!-- Google Tag Manager (noscript) -->

<!-- End Google Tag Manager (noscript) -->
			<div id="cookie-notice" role="banner"><p><span id="cn-notice-text">We care about your data, and we'd like to use cookies to give you a smooth browsing experience. Please agree and read more about our <a href="https://www.quantamagazine.org/privacy-policy">privacy policy</a>.</span></p>
				</div>
        



</div>]]></description>
        </item>
        <item>
            <title><![CDATA[Scalene: A high-performance, high-precision CPU, GPU, memory profiler for Python (166 pts)]]></title>
            <link>https://github.com/plasma-umass/scalene</link>
            <guid>41908536</guid>
            <pubDate>Mon, 21 Oct 2024 21:07:15 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/plasma-umass/scalene">https://github.com/plasma-umass/scalene</a>, See on <a href="https://news.ycombinator.com/item?id=41908536">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/plasma-umass/scalene/raw/master/docs/scalene-icon-white.png"><img src="https://github.com/plasma-umass/scalene/raw/master/docs/scalene-icon-white.png" alt="scalene"></a></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Scalene: a Python CPU+GPU+memory profiler with AI-powered optimization proposals</h2><a id="user-content-scalene-a-python-cpugpumemory-profiler-with-ai-powered-optimization-proposals" aria-label="Permalink: Scalene: a Python CPU+GPU+memory profiler with AI-powered optimization proposals" href="#scalene-a-python-cpugpumemory-profiler-with-ai-powered-optimization-proposals"></a></p>
<p dir="auto">by <a href="https://emeryberger.com/" rel="nofollow">Emery Berger</a>, <a href="https://samstern.me/" rel="nofollow">Sam Stern</a>, and <a href="https://github.com/jaltmayerpizzorno">Juan Altmayer Pizzorno</a>.</p>
<p dir="auto"><a href="https://join.slack.com/t/scaleneprofil-jge3234/shared_invite/zt-110vzrdck-xJh5d4gHnp5vKXIjYD3Uwg" rel="nofollow"><img src="https://github.com/plasma-umass/scalene/raw/master/docs/images/slack-logo.png" alt="Scalene community Slack"></a><a href="https://join.slack.com/t/scaleneprofil-jge3234/shared_invite/zt-110vzrdck-xJh5d4gHnp5vKXIjYD3Uwg" rel="nofollow">Scalene community Slack</a></p>
<p dir="auto"><a href="https://pypi.org/project/scalene/" rel="nofollow"><img src="https://camo.githubusercontent.com/999e3fc8d7e5ec0d4b465371620a1cec5dbb7f7fc2591ef1e3e8f7214bea61b0/68747470733a2f2f696d672e736869656c64732e696f2f707970692f762f7363616c656e652e737667" alt="PyPI Latest Release" data-canonical-src="https://img.shields.io/pypi/v/scalene.svg"></a><a href="https://anaconda.org/conda-forge/scalene" rel="nofollow"><img src="https://camo.githubusercontent.com/60d006b383bad7470fc7ca8368cf8097b3de4044938a4714a788975dc43041ab/68747470733a2f2f696d672e736869656c64732e696f2f636f6e64612f762f636f6e64612d666f7267652f7363616c656e65" alt="Anaconda-Server Badge" data-canonical-src="https://img.shields.io/conda/v/conda-forge/scalene"></a> <a href="https://pepy.tech/project/scalene" rel="nofollow"><img src="https://camo.githubusercontent.com/ecdab4442ec9a2ab4cab1de36bebfb0a5c7bc00f288807723ca673522b4b4dc2/68747470733a2f2f7374617469632e706570792e746563682f62616467652f7363616c656e65" alt="Downloads" data-canonical-src="https://static.pepy.tech/badge/scalene"></a><a href="https://anaconda.org/conda-forge/scalene" rel="nofollow"><img src="https://camo.githubusercontent.com/bc3427e8e678f6da977b954640137ecd09fbaee32bc2b1ae169bec0c48bfd304/68747470733a2f2f696d672e736869656c64732e696f2f636f6e64612f642f636f6e64612d666f7267652f7363616c656e653f6c6f676f3d636f6e6461" alt="Anaconda downloads" data-canonical-src="https://img.shields.io/conda/d/conda-forge/scalene?logo=conda"></a> <a href="https://pepy.tech/project/scalene" rel="nofollow"><img src="https://camo.githubusercontent.com/c80e0ee2e6b083f8ec0b720007a5fcca7c7e33a5702f8a91eb8cca4afb65ced7/68747470733a2f2f7374617469632e706570792e746563682f62616467652f7363616c656e652f6d6f6e7468" alt="Downloads" data-canonical-src="https://static.pepy.tech/badge/scalene/month"></a> <a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/577e8524ded7f9bd5610662b971236965fbbebbf1dbcf24dd9b695733fea4809/68747470733a2f2f696d672e736869656c64732e696f2f707970692f707976657273696f6e732f7363616c656e652e7376673f7374796c653d666c61742d737175617265"><img src="https://camo.githubusercontent.com/577e8524ded7f9bd5610662b971236965fbbebbf1dbcf24dd9b695733fea4809/68747470733a2f2f696d672e736869656c64732e696f2f707970692f707976657273696f6e732f7363616c656e652e7376673f7374796c653d666c61742d737175617265" alt="Python versions" data-canonical-src="https://img.shields.io/pypi/pyversions/scalene.svg?style=flat-square"></a><a href="https://marketplace.visualstudio.com/items?itemName=EmeryBerger.scalene" rel="nofollow"><img src="https://camo.githubusercontent.com/f5a0f652c19744c3680c9e621707576452f929653a982cf216b8ad46962c9ae2/68747470733a2f2f696d672e736869656c64732e696f2f76697375616c2d73747564696f2d6d61726b6574706c6163652f762f656d6572796265726765722e7363616c656e653f6c6f676f3d76697375616c73747564696f636f6465" alt="Visual Studio Code Extension version" data-canonical-src="https://img.shields.io/visual-studio-marketplace/v/emeryberger.scalene?logo=visualstudiocode"></a> <a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/9e2d96d0196d22379d49dcc59fc3d72685bcf46c4957b9d3eddab0986a132bac/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6c6963656e73652f706c61736d612d756d6173732f7363616c656e65"><img src="https://camo.githubusercontent.com/9e2d96d0196d22379d49dcc59fc3d72685bcf46c4957b9d3eddab0986a132bac/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6c6963656e73652f706c61736d612d756d6173732f7363616c656e65" alt="License" data-canonical-src="https://img.shields.io/github/license/plasma-umass/scalene"></a></p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/plasma-umass/scalene/raw/master/docs/Ozsvald-tweet.png"><img src="https://github.com/plasma-umass/scalene/raw/master/docs/Ozsvald-tweet.png" alt="Ozsvald tweet"></a></p>
<p dir="auto">(tweet from Ian Ozsvald, author of <a href="https://smile.amazon.com/High-Performance-Python-Performant-Programming/dp/1492055026/ref=sr_1_1?crid=texbooks" rel="nofollow"><em>High Performance Python</em></a>)</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/plasma-umass/scalene/raw/master/docs/semantic-scholar-success.png"><img src="https://github.com/plasma-umass/scalene/raw/master/docs/semantic-scholar-success.png" alt="Semantic Scholar success story"></a></p>
<p dir="auto"><em><strong>Scalene web-based user interface:</strong></em> <a href="http://plasma-umass.org/scalene-gui/" rel="nofollow">http://plasma-umass.org/scalene-gui/</a></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">About Scalene</h2><a id="user-content-about-scalene" aria-label="Permalink: About Scalene" href="#about-scalene"></a></p>
<p dir="auto">Scalene is a high-performance CPU, GPU <em>and</em> memory profiler for
Python that does a number of things that other Python profilers do not
and cannot do.  It runs orders of magnitude faster than many other
profilers while delivering far more detailed information. It is also
the first profiler ever to incorporate AI-powered proposed
optimizations.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">AI-powered optimization suggestions</h3><a id="user-content-ai-powered-optimization-suggestions" aria-label="Permalink: AI-powered optimization suggestions" href="#ai-powered-optimization-suggestions"></a></p>
<blockquote>
<p dir="auto"><strong>Note</strong></p>
<p dir="auto">To enable AI-powered optimization suggestions, you need to enter an <a href="https://openai.com/api/" rel="nofollow">OpenAI key</a> in the box under "Advanced options". <em>Your account will need to have a positive balance for this to work</em> (check your balance at <a href="https://platform.openai.com/account/usage" rel="nofollow">https://platform.openai.com/account/usage</a>).</p>
<a target="_blank" rel="noopener noreferrer nofollow" href="https://user-images.githubusercontent.com/1612723/211639253-ec926b38-3efe-4a20-8514-e10dde94ec01.png"><img width="487" alt="Scalene advanced options" src="https://user-images.githubusercontent.com/1612723/211639253-ec926b38-3efe-4a20-8514-e10dde94ec01.png"></a>
</blockquote>
<p dir="auto">Once you've entered your OpenAI key (see above), click on the lightning bolt (⚡) beside any line or the explosion (💥) for an entire region of code to generate a proposed optimization. Click on a proposed optimization to copy it to the clipboard.</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer nofollow" href="https://user-images.githubusercontent.com/1612723/211639968-37cf793f-3290-43d1-9282-79e579558388.png"><img width="571" alt="example proposed optimization" src="https://user-images.githubusercontent.com/1612723/211639968-37cf793f-3290-43d1-9282-79e579558388.png"></a></p>
<p dir="auto">You can click as many times as you like on the lightning bolt or explosion, and it will generate different suggested optimizations. Your mileage may vary, but in some cases, the suggestions are quite impressive (e.g., order-of-magnitude improvements).</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Quick Start</h3><a id="user-content-quick-start" aria-label="Permalink: Quick Start" href="#quick-start"></a></p>
<p dir="auto"><h4 tabindex="-1" dir="auto">Installing Scalene:</h4><a id="user-content-installing-scalene" aria-label="Permalink: Installing Scalene:" href="#installing-scalene"></a></p>
<div dir="auto" data-snippet-clipboard-copy-content="python3 -m pip install -U scalene"><pre><span>python3 -m pip install -U scalene</span></pre></div>
<p dir="auto">or</p>
<div dir="auto" data-snippet-clipboard-copy-content="conda install -c conda-forge scalene"><pre><span>conda install -c conda-forge scalene</span></pre></div>
<p dir="auto"><h4 tabindex="-1" dir="auto">Using Scalene:</h4><a id="user-content-using-scalene" aria-label="Permalink: Using Scalene:" href="#using-scalene"></a></p>
<p dir="auto">After installing Scalene, you can use Scalene at the command line, or as a Visual Studio Code extension.</p>
<details>
  <summary>
    Using the Scalene VS Code Extension:
  </summary>
<p dir="auto">First, install <a href="https://marketplace.visualstudio.com/items?itemName=EmeryBerger.scalene" rel="nofollow">the Scalene extension from the VS Code Marketplace</a> or by searching for it within VS Code by typing Command-Shift-X (Mac) or Ctrl-Shift-X (Windows). Once that's installed, click Command-Shift-P or Ctrl-Shift-P to open the <a href="https://code.visualstudio.com/docs/getstarted/userinterface" rel="nofollow">Command Palette</a>. Then select <b>"Scalene: AI-powered profiling..."</b> (you can start typing Scalene and it will pop up if it's installed). Run that and, assuming your code runs for at least a second, a Scalene profile will appear in a webview.</p>
<a target="_blank" rel="noopener noreferrer" href="https://private-user-images.githubusercontent.com/1612723/269439965-7e78e3d2-e649-4f02-86fd-0da2a259a1a4.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3Mjk1ODI1MDIsIm5iZiI6MTcyOTU4MjIwMiwicGF0aCI6Ii8xNjEyNzIzLzI2OTQzOTk2NS03ZTc4ZTNkMi1lNjQ5LTRmMDItODZmZC0wZGEyYTI1OWExYTQucG5nP1gtQW16LUFsZ29yaXRobT1BV1M0LUhNQUMtU0hBMjU2JlgtQW16LUNyZWRlbnRpYWw9QUtJQVZDT0RZTFNBNTNQUUs0WkElMkYyMDI0MTAyMiUyRnVzLWVhc3QtMSUyRnMzJTJGYXdzNF9yZXF1ZXN0JlgtQW16LURhdGU9MjAyNDEwMjJUMDczMDAyWiZYLUFtei1FeHBpcmVzPTMwMCZYLUFtei1TaWduYXR1cmU9OWQ3MzdkNjJkMWY2ZDE4OGNmYzU5ZTRjYTZiZTBmZjY5NDFiOWIyODUyOWY2YTJiMDNiYjY2ZTM3NjkwMWQxMSZYLUFtei1TaWduZWRIZWFkZXJzPWhvc3QifQ.HGFPesaVBb9b5fzF6hdUf9_6YE36aFVjhdGPZ4Rszb4"><img width="734" alt="Screenshot 2023-09-20 at 7 09 06 PM" src="https://private-user-images.githubusercontent.com/1612723/269439965-7e78e3d2-e649-4f02-86fd-0da2a259a1a4.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3Mjk1ODI1MDIsIm5iZiI6MTcyOTU4MjIwMiwicGF0aCI6Ii8xNjEyNzIzLzI2OTQzOTk2NS03ZTc4ZTNkMi1lNjQ5LTRmMDItODZmZC0wZGEyYTI1OWExYTQucG5nP1gtQW16LUFsZ29yaXRobT1BV1M0LUhNQUMtU0hBMjU2JlgtQW16LUNyZWRlbnRpYWw9QUtJQVZDT0RZTFNBNTNQUUs0WkElMkYyMDI0MTAyMiUyRnVzLWVhc3QtMSUyRnMzJTJGYXdzNF9yZXF1ZXN0JlgtQW16LURhdGU9MjAyNDEwMjJUMDczMDAyWiZYLUFtei1FeHBpcmVzPTMwMCZYLUFtei1TaWduYXR1cmU9OWQ3MzdkNjJkMWY2ZDE4OGNmYzU5ZTRjYTZiZTBmZjY5NDFiOWIyODUyOWY2YTJiMDNiYjY2ZTM3NjkwMWQxMSZYLUFtei1TaWduZWRIZWFkZXJzPWhvc3QifQ.HGFPesaVBb9b5fzF6hdUf9_6YE36aFVjhdGPZ4Rszb4"></a>
</details>
<details>
<summary>
Commonly used command-line options:
</summary>
<div dir="auto" data-snippet-clipboard-copy-content="scalene your_prog.py                             # full profile (outputs to web interface)
python3 -m scalene your_prog.py                  # equivalent alternative

scalene --cli your_prog.py                       # use the command-line only (no web interface)

scalene --cpu your_prog.py                       # only profile CPU
scalene --cpu --gpu your_prog.py                 # only profile CPU and GPU
scalene --cpu --gpu --memory your_prog.py        # profile everything (same as no options)

scalene --reduced-profile your_prog.py           # only profile lines with significant usage
scalene --profile-interval 5.0 your_prog.py      # output a new profile every five seconds

scalene (Scalene options) --- your_prog.py (...) # use --- to tell Scalene to ignore options after that point
scalene --help                                   # lists all options"><pre><span>scalene your_prog.py                             # full profile (outputs to web interface)</span>
<span>python3 -m scalene your_prog.py                  # equivalent alternative</span>

<span>scalene --cli your_prog.py                       # use the command-line only (no web interface)</span>

<span>scalene --cpu your_prog.py                       # only profile CPU</span>
<span>scalene --cpu --gpu your_prog.py                 # only profile CPU and GPU</span>
<span>scalene --cpu --gpu --memory your_prog.py        # profile everything (same as no options)</span>

<span>scalene --reduced-profile your_prog.py           # only profile lines with significant usage</span>
<span>scalene --profile-interval 5.0 your_prog.py      # output a new profile every five seconds</span>

<span>scalene (Scalene options) --- your_prog.py (...) # use --- to tell Scalene to ignore options after that point</span>
<span>scalene --help                                   # lists all options</span></pre></div>
</details>
<details>
<summary>
Using Scalene programmatically in your code:
</summary>
<p dir="auto">Invoke using <code>scalene</code> as above and then:</p>
<div dir="auto" data-snippet-clipboard-copy-content="from scalene import scalene_profiler

# Turn profiling on
scalene_profiler.start()

# your code

# Turn profiling off
scalene_profiler.stop()"><pre><span>from</span> <span>scalene</span> <span>import</span> <span>scalene_profiler</span>

<span># Turn profiling on</span>
<span>scalene_profiler</span>.<span>start</span>()

<span># your code</span>

<span># Turn profiling off</span>
<span>scalene_profiler</span>.<span>stop</span>()</pre></div>
<div dir="auto" data-snippet-clipboard-copy-content="from scalene.scalene_profiler import enable_profiling

with enable_profiling():
    # do something"><pre><span>from</span> <span>scalene</span>.<span>scalene_profiler</span> <span>import</span> <span>enable_profiling</span>

<span>with</span> <span>enable_profiling</span>():
    <span># do something</span></pre></div>
</details>
<details>
<summary>
Using Scalene to profile only specific functions via <code>@profile</code>:
</summary>
<p dir="auto">Just preface any functions you want to profile with the <code>@profile</code> decorator and run it with Scalene:</p>
<div dir="auto" data-snippet-clipboard-copy-content="# do not import profile!

@profile
def slow_function():
    import time
    time.sleep(3)"><pre><span># do not import profile!</span>

<span>@<span>profile</span></span>
<span>def</span> <span>slow_function</span>():
    <span>import</span> <span>time</span>
    <span>time</span>.<span>sleep</span>(<span>3</span>)</pre></div>
</details>
<p dir="auto"><h4 tabindex="-1" dir="auto">Web-based GUI</h4><a id="user-content-web-based-gui" aria-label="Permalink: Web-based GUI" href="#web-based-gui"></a></p>
<p dir="auto">Scalene has both a CLI and a web-based GUI <a href="http://plasma-umass.org/scalene-gui/" rel="nofollow">(demo here)</a>.</p>
<p dir="auto">By default, once Scalene has profiled your program, it will open a
tab in a web browser with an interactive user interface (all processing is done
locally). Hover over bars to see breakdowns of CPU and memory
consumption, and click on underlined column headers to sort the
columns. The generated file <code>profile.html</code> is self-contained and can be saved for later use.</p>
<p dir="auto"><a href="https://raw.githubusercontent.com/plasma-umass/scalene/master/docs/scalene-gui-example-full.png" rel="nofollow"><img src="https://raw.githubusercontent.com/plasma-umass/scalene/master/docs/scalene-gui-example.png" alt="Scalene web GUI"></a></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Scalene Overview</h2><a id="user-content-scalene-overview" aria-label="Permalink: Scalene Overview" href="#scalene-overview"></a></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Scalene talk (PyCon US 2021)</h3><a id="user-content-scalene-talk-pycon-us-2021" aria-label="Permalink: Scalene talk (PyCon US 2021)" href="#scalene-talk-pycon-us-2021"></a></p>
<p dir="auto"><a href="https://youtu.be/5iEf-_7mM1k" rel="nofollow">This talk</a> presented at PyCon 2021 walks through Scalene's advantages and how to use it to debug the performance of an application (and provides some technical details on its internals). We highly recommend watching this video!</p>
<p dir="auto"><a href="https://youtu.be/5iEf-_7mM1k" title="Scalene presentation at PyCon 2021" rel="nofollow"><img src="https://raw.githubusercontent.com/plasma-umass/scalene/master/docs/images/scalene-video-img.png" alt="Scalene presentation at PyCon 2021"></a></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Fast and Accurate</h3><a id="user-content-fast-and-accurate" aria-label="Permalink: Fast and Accurate" href="#fast-and-accurate"></a></p>
<ul dir="auto">
<li>
<p dir="auto">Scalene is <strong><em>fast</em></strong>. It uses sampling instead of instrumentation or relying on Python's tracing facilities. Its overhead is typically no more than 10-20% (and often less).</p>
</li>
<li>
<p dir="auto">Scalene is <strong>accurate</strong>. We tested CPU profiler accuracy and found that Scalene is among the most accurate profilers, correctly measuring time taken.</p>
</li>
</ul>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/plasma-umass/scalene/raw/master/docs/cpu-accuracy-comparison.png"><img src="https://github.com/plasma-umass/scalene/raw/master/docs/cpu-accuracy-comparison.png" alt="Profiler accuracy"></a></p>
<ul dir="auto">
<li>Scalene performs profiling <strong><em>at the line level</em></strong> <em>and</em> <strong><em>per function</em></strong>, pointing to the functions and the specific lines of code responsible for the execution time in your program.</li>
</ul>
<p dir="auto"><h3 tabindex="-1" dir="auto">CPU profiling</h3><a id="user-content-cpu-profiling" aria-label="Permalink: CPU profiling" href="#cpu-profiling"></a></p>
<ul dir="auto">
<li>Scalene <strong>separates out time spent in Python from time in native code</strong> (including libraries). Most Python programmers aren't going to optimize the performance of native code (which is usually either in the Python implementation or external libraries), so this helps developers focus their optimization efforts on the code they can actually improve.</li>
<li>Scalene <strong>highlights hotspots</strong> (code accounting for significant percentages of CPU time or memory allocation) in red, making them even easier to spot.</li>
<li>Scalene also separates out <strong>system time</strong>, making it easy to find I/O bottlenecks.</li>
</ul>
<p dir="auto"><h3 tabindex="-1" dir="auto">GPU profiling</h3><a id="user-content-gpu-profiling" aria-label="Permalink: GPU profiling" href="#gpu-profiling"></a></p>
<ul dir="auto">
<li>Scalene reports <strong>GPU time</strong> (currently limited to NVIDIA-based systems).</li>
</ul>
<p dir="auto"><h3 tabindex="-1" dir="auto">Memory profiling</h3><a id="user-content-memory-profiling" aria-label="Permalink: Memory profiling" href="#memory-profiling"></a></p>
<ul dir="auto">
<li>Scalene <strong>profiles memory usage</strong>. In addition to tracking CPU usage, Scalene also points to the specific lines of code responsible for memory growth. It accomplishes this via an included specialized memory allocator.</li>
<li>Scalene separates out the percentage of <strong>memory consumed by Python code vs. native code</strong>.</li>
<li>Scalene produces <strong><em>per-line</em> memory profiles</strong>.</li>
<li>Scalene <strong>identifies lines with likely memory leaks</strong>.</li>
<li>Scalene <strong>profiles <em>copying volume</em></strong>, making it easy to spot inadvertent copying, especially due to crossing Python/library boundaries (e.g., accidentally converting <code>numpy</code> arrays into Python arrays, and vice versa).</li>
</ul>
<p dir="auto"><h3 tabindex="-1" dir="auto">Other features</h3><a id="user-content-other-features" aria-label="Permalink: Other features" href="#other-features"></a></p>
<ul dir="auto">
<li>Scalene can produce <strong>reduced profiles</strong> (via <code>--reduced-profile</code>) that only report lines that consume more than 1% of CPU or perform at least 100 allocations.</li>
<li>Scalene supports <code>@profile</code> decorators to profile only specific functions.</li>
<li>When Scalene is profiling a program launched in the background (via <code>&amp;</code>), you can <strong>suspend and resume profiling</strong>.</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Comparison to Other Profilers</h2><a id="user-content-comparison-to-other-profilers" aria-label="Permalink: Comparison to Other Profilers" href="#comparison-to-other-profilers"></a></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Performance and Features</h2><a id="user-content-performance-and-features" aria-label="Permalink: Performance and Features" href="#performance-and-features"></a></p>
<p dir="auto">Below is a table comparing the <strong>performance and features</strong> of various profilers to Scalene.</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer nofollow" href="https://raw.githubusercontent.com/plasma-umass/scalene/master/docs/images/profiler-comparison.png"><img src="https://raw.githubusercontent.com/plasma-umass/scalene/master/docs/images/profiler-comparison.png" alt="Performance and feature comparison"></a></p>
<ul dir="auto">
<li><strong>Slowdown</strong>: the slowdown when running a benchmark from the Pyperformance suite. Green means less than 2x overhead. Scalene's overhead is just a 35% slowdown.</li>
</ul>
<p dir="auto">Scalene has all of the following features, many of which only Scalene supports:</p>
<ul dir="auto">
<li><strong>Lines or functions</strong>: does the profiler report information only for entire functions, or for every line -- Scalene does both.</li>
<li><strong>Unmodified Code</strong>: works on unmodified code.</li>
<li><strong>Threads</strong>: supports Python threads.</li>
<li><strong>Multiprocessing</strong>: supports use of the <code>multiprocessing</code> library -- <em>Scalene only</em></li>
<li><strong>Python vs. C time</strong>: breaks out time spent in Python vs. native code (e.g., libraries) -- <em>Scalene only</em></li>
<li><strong>System time</strong>: breaks out system time (e.g., sleeping or performing I/O) -- <em>Scalene only</em></li>
<li><strong>Profiles memory</strong>: reports memory consumption per line / function</li>
<li><strong>GPU</strong>: reports time spent on an NVIDIA GPU (if present) -- <em>Scalene only</em></li>
<li><strong>Memory trends</strong>: reports memory use over time per line / function -- <em>Scalene only</em></li>
<li><strong>Copy volume</strong>: reports megabytes being copied per second -- <em>Scalene only</em></li>
<li><strong>Detects leaks</strong>: automatically pinpoints lines responsible for likely memory leaks -- <em>Scalene only</em></li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Output</h2><a id="user-content-output" aria-label="Permalink: Output" href="#output"></a></p>
<p dir="auto">If you include the <code>--cli</code> option, Scalene prints annotated source code for the program being profiled
(as text, JSON (<code>--json</code>), or HTML (<code>--html</code>)) and any modules it
uses in the same directory or subdirectories (you can optionally have
it <code>--profile-all</code> and only include files with at least a
<code>--cpu-percent-threshold</code> of time).  Here is a snippet from
<code>pystone.py</code>.</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer nofollow" href="https://raw.githubusercontent.com/plasma-umass/scalene/master/docs/images/sample-profile-pystone.png"><img src="https://raw.githubusercontent.com/plasma-umass/scalene/master/docs/images/sample-profile-pystone.png" alt="Example profile"></a></p>
<ul dir="auto">
<li><strong>Memory usage at the top</strong>: Visualized by "sparklines", memory consumption over the runtime of the profiled code.</li>
<li><strong>"Time Python"</strong>: How much time was spent in Python code.</li>
<li><strong>"native"</strong>: How much time was spent in non-Python code (e.g., libraries written in C/C++).</li>
<li><strong>"system"</strong>: How much time was spent in the system (e.g., I/O).</li>
<li><strong>"GPU"</strong>: (not shown here) How much time spent on the GPU, if your system has an NVIDIA GPU installed.</li>
<li><strong>"Memory Python"</strong>: How much of the memory allocation happened on the Python side of the code, as opposed to in non-Python code (e.g., libraries written in C/C++).</li>
<li><strong>"net"</strong>: Positive net memory numbers indicate total memory allocation in megabytes; negative net memory numbers indicate memory reclamation.</li>
<li><strong>"timeline / %"</strong>: Visualized by "sparklines", memory consumption generated by this line over the program runtime, and the percentages of total memory activity this line represents.</li>
<li><strong>"Copy (MB/s)"</strong>: The amount of megabytes being copied per second (see "About Scalene").</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Scalene</h2><a id="user-content-scalene" aria-label="Permalink: Scalene" href="#scalene"></a></p>
<p dir="auto">The following command runs Scalene on a provided example program.</p>

<details>
 <summary>
  Click to see all Scalene's options (available by running with <code>--help</code>)
 </summary>
<div dir="auto" data-snippet-clipboard-copy-content="    % scalene --help
     usage: scalene [-h] [--outfile OUTFILE] [--html] [--reduced-profile]
                    [--profile-interval PROFILE_INTERVAL] [--cpu-only]
                    [--profile-all] [--profile-only PROFILE_ONLY]
                    [--use-virtual-time]
                    [--cpu-percent-threshold CPU_PERCENT_THRESHOLD]
                    [--cpu-sampling-rate CPU_SAMPLING_RATE]
                    [--malloc-threshold MALLOC_THRESHOLD]
     
     Scalene: a high-precision CPU and memory profiler.
     https://github.com/plasma-umass/scalene
     
     command-line:
        % scalene [options] yourprogram.py
     or
        % python3 -m scalene [options] yourprogram.py
     
     in Jupyter, line mode:
        %scrun [options] statement
     
     in Jupyter, cell mode:
        %%scalene [options]
        code...
        code...
     
     optional arguments:
       -h, --help            show this help message and exit
       --outfile OUTFILE     file to hold profiler output (default: stdout)
       --html                output as HTML (default: text)
       --reduced-profile     generate a reduced profile, with non-zero lines only (default: False)
       --profile-interval PROFILE_INTERVAL
                             output profiles every so many seconds (default: inf)
       --cpu-only            only profile CPU time (default: profile CPU, memory, and copying)
       --profile-all         profile all executed code, not just the target program (default: only the target program)
       --profile-only PROFILE_ONLY
                             profile only code in filenames that contain the given strings, separated by commas (default: no restrictions)
       --use-virtual-time    measure only CPU time, not time spent in I/O or blocking (default: False)
       --cpu-percent-threshold CPU_PERCENT_THRESHOLD
                             only report profiles with at least this percent of CPU time (default: 1%)
       --cpu-sampling-rate CPU_SAMPLING_RATE
                             CPU sampling rate (default: every 0.01s)
       --malloc-threshold MALLOC_THRESHOLD
                             only report profiles with at least this many allocations (default: 100)
     
     When running Scalene in the background, you can suspend/resume profiling
     for the process ID that Scalene reports. For example:
     
        % python3 -m scalene [options] yourprogram.py &amp;
      Scalene now profiling process 12345
        to suspend profiling: python3 -m scalene.profile --off --pid 12345
        to resume profiling:  python3 -m scalene.profile --on  --pid 12345"><pre><span>    % scalene --help</span>
<span>     usage: scalene [-h] [--outfile OUTFILE] [--html] [--reduced-profile]</span>
<span>                    [--profile-interval PROFILE_INTERVAL] [--cpu-only]</span>
<span>                    [--profile-all] [--profile-only PROFILE_ONLY]</span>
<span>                    [--use-virtual-time]</span>
<span>                    [--cpu-percent-threshold CPU_PERCENT_THRESHOLD]</span>
<span>                    [--cpu-sampling-rate CPU_SAMPLING_RATE]</span>
<span>                    [--malloc-threshold MALLOC_THRESHOLD]</span>
<span>     </span>
<span>     Scalene: a high-precision CPU and memory profiler.</span>
<span>     https://github.com/plasma-umass/scalene</span>
<span>     </span>
<span>     command-line:</span>
<span>        % scalene [options] yourprogram.py</span>
<span>     or</span>
<span>        % python3 -m scalene [options] yourprogram.py</span>
<span>     </span>
<span>     in Jupyter, line mode:</span>
<span>        %scrun [options] statement</span>
<span>     </span>
<span>     in Jupyter, cell mode:</span>
<span>        %%scalene [options]</span>
<span>        code...</span>
<span>        code...</span>
<span>     </span>
<span>     optional arguments:</span>
<span>       -h, --help            show this help message and exit</span>
<span>       --outfile OUTFILE     file to hold profiler output (default: stdout)</span>
<span>       --html                output as HTML (default: text)</span>
<span>       --reduced-profile     generate a reduced profile, with non-zero lines only (default: False)</span>
<span>       --profile-interval PROFILE_INTERVAL</span>
<span>                             output profiles every so many seconds (default: inf)</span>
<span>       --cpu-only            only profile CPU time (default: profile CPU, memory, and copying)</span>
<span>       --profile-all         profile all executed code, not just the target program (default: only the target program)</span>
<span>       --profile-only PROFILE_ONLY</span>
<span>                             profile only code in filenames that contain the given strings, separated by commas (default: no restrictions)</span>
<span>       --use-virtual-time    measure only CPU time, not time spent in I/O or blocking (default: False)</span>
<span>       --cpu-percent-threshold CPU_PERCENT_THRESHOLD</span>
<span>                             only report profiles with at least this percent of CPU time (default: 1%)</span>
<span>       --cpu-sampling-rate CPU_SAMPLING_RATE</span>
<span>                             CPU sampling rate (default: every 0.01s)</span>
<span>       --malloc-threshold MALLOC_THRESHOLD</span>
<span>                             only report profiles with at least this many allocations (default: 100)</span>
<span>     </span>
<span>     When running Scalene in the background, you can suspend/resume profiling</span>
<span>     for the process ID that Scalene reports. For example:</span>
<span>     </span>
<span>        % python3 -m scalene [options] yourprogram.py &amp;</span>
<span>      Scalene now profiling process 12345</span>
<span>        to suspend profiling: python3 -m scalene.profile --off --pid 12345</span>
<span>        to resume profiling:  python3 -m scalene.profile --on  --pid 12345</span></pre></div>
</details>
<p dir="auto"><h3 tabindex="-1" dir="auto">Scalene with Jupyter</h3><a id="user-content-scalene-with-jupyter" aria-label="Permalink: Scalene with Jupyter" href="#scalene-with-jupyter"></a></p>
<details>
<summary>
Instructions for installing and using Scalene with Jupyter notebooks
</summary>
<p dir="auto"><a href="https://nbviewer.jupyter.org/github/plasma-umass/scalene/blob/master/docs/scalene-demo.ipynb" rel="nofollow">This notebook</a> illustrates the use of Scalene in Jupyter.</p>
<p dir="auto">Installation:</p>
<div dir="auto" data-snippet-clipboard-copy-content="!pip install scalene
%load_ext scalene"><pre><span>!pip install scalene</span>
<span>%load_ext scalene</span></pre></div>
<p dir="auto">Line mode:</p>
<div dir="auto" data-snippet-clipboard-copy-content="%scrun [options] statement"><pre><span>%scrun [options] statement</span></pre></div>
<p dir="auto">Cell mode:</p>
<div dir="auto" data-snippet-clipboard-copy-content="%%scalene [options]
code...
code..."><pre><span>%%scalene [options]</span>
<span>code...</span>
<span>code...</span></pre></div>
</details>
<p dir="auto"><h2 tabindex="-1" dir="auto">Installation</h2><a id="user-content-installation" aria-label="Permalink: Installation" href="#installation"></a></p>
<details open="">
<summary>Using <code>pip</code> (Mac OS X, Linux, Windows, and WSL2)</summary>
<p dir="auto">Scalene is distributed as a <code>pip</code> package and works on Mac OS X, Linux (including Ubuntu in <a href="https://docs.microsoft.com/en-us/windows/wsl/wsl2-index" rel="nofollow">Windows WSL2</a>) and (with limitations) Windows platforms.</p>
<blockquote>
<p dir="auto"><strong>Note</strong></p>
<p dir="auto">The Windows version currently only supports CPU and GPU profiling, but not memory or copy profiling.</p>
</blockquote>
<p dir="auto">You can install it as follows:</p>

<p dir="auto">or</p>
<div dir="auto" data-snippet-clipboard-copy-content="  % python3 -m pip install -U scalene"><pre><span>  % python3 -m pip install -U scalene</span></pre></div>
<p dir="auto">You may need to install some packages first.</p>
<p dir="auto">See <a href="https://stackoverflow.com/a/19344978/4954434" rel="nofollow">https://stackoverflow.com/a/19344978/4954434</a> for full instructions for all Linux flavors.</p>
<p dir="auto">For Ubuntu/Debian:</p>
<div dir="auto" data-snippet-clipboard-copy-content="  % sudo apt install git python3-all-dev"><pre><span>  % sudo apt install git python3-all-dev</span></pre></div>
</details>
<details>
<summary>Using <code>conda</code> (Mac OS X, Linux, Windows, and WSL2)</summary>
<div dir="auto" data-snippet-clipboard-copy-content="  % conda install -c conda-forge scalene"><pre><span>  % conda install -c conda-forge scalene</span></pre></div>
<p dir="auto">Scalene is distributed as a <code>conda</code> package and works on Mac OS X, Linux (including Ubuntu in <a href="https://docs.microsoft.com/en-us/windows/wsl/wsl2-index" rel="nofollow">Windows WSL2</a>) and (with limitations) Windows platforms.</p>
<blockquote>
<p dir="auto"><strong>Note</strong></p>
<p dir="auto">The Windows version currently only supports CPU and GPU profiling, but not memory or copy profiling.</p>
</blockquote>
</details>
<details>
<summary>On ArchLinux</summary>
<p dir="auto">You can install Scalene on Arch Linux via the <a href="https://aur.archlinux.org/packages/python-scalene-git/" rel="nofollow">AUR
package</a>. Use your favorite AUR helper, or
manually download the <code>PKGBUILD</code> and run <code>makepkg -cirs</code> to build. Note that this will place
<code>libscalene.so</code> in <code>/usr/lib</code>; modify the below usage instructions accordingly.</p>
</details>
<p dir="auto"><h2 tabindex="-1" dir="auto">Frequently Asked Questions</h2><a id="user-content-frequently-asked-questions" aria-label="Permalink: Frequently Asked Questions" href="#frequently-asked-questions"></a></p>
<details>
<summary>
Can I use Scalene with PyTest?
</summary>
<p dir="auto"><strong>A:</strong> Yes! You can run it as follows (for example):</p>
<p dir="auto"><code>python3 -m scalene --- -m pytest your_test.py</code></p>
</details>
<details>
<summary>
Is there any way to get shorter profiles or do more targeted profiling?
</summary>
<p dir="auto"><strong>A:</strong> Yes! There are several options:</p>
<ol dir="auto">
<li>Use <code>--reduced-profile</code> to include only lines and files with memory/CPU/GPU activity.</li>
<li>Use <code>--profile-only</code> to include only filenames containing specific strings (as in, <code>--profile-only foo,bar,baz</code>).</li>
<li>Decorate functions of interest with <code>@profile</code> to have Scalene report <em>only</em> those functions.</li>
<li>Turn profiling on and off programmatically by importing Scalene profiler (<code>from scalene import scalene_profiler</code>) and then turning profiling on and off via <code>scalene_profiler.start()</code> and <code>scalene_profiler.stop()</code>. By default, Scalene runs with profiling on, so to delay profiling until desired, use the <code>--off</code> command-line option (<code>python3 -m scalene --off yourprogram.py</code>).</li>
</ol>
</details>
<details>
<summary>
How do I run Scalene in PyCharm?
</summary>
<p dir="auto"><strong>A:</strong>  In PyCharm, you can run Scalene at the command line by opening the terminal at the bottom of the IDE and running a Scalene command (e.g., <code>python -m scalene &lt;your program&gt;</code>). Use the options <code>--cli</code>, <code>--html</code>, and <code>--outfile &lt;your output.html&gt;</code> to generate an HTML file that you can then view in the IDE.</p>
</details>
<details>
<summary>
How do I use Scalene with Django?
</summary>
<p dir="auto"><strong>A:</strong> Pass in the <code>--noreload</code> option (see <a data-error-text="Failed to load title" data-id="874855968" data-permission-text="Title is private" data-url="https://github.com/plasma-umass/scalene/issues/178" data-hovercard-type="issue" data-hovercard-url="/plasma-umass/scalene/issues/178/hovercard" href="https://github.com/plasma-umass/scalene/issues/178">#178</a>).</p>
</details>
<details>
<summary>
Does Scalene work with gevent/Greenlets?
</summary>
<p dir="auto"><strong>A:</strong> Yes! Put the following code in the beginning of your program, or modify the call to <code>monkey.patch_all</code> as below:</p>
<div dir="auto" data-snippet-clipboard-copy-content="from gevent import monkey
monkey.patch_all(thread=False)"><pre><span>from</span> <span>gevent</span> <span>import</span> <span>monkey</span>
<span>monkey</span>.<span>patch_all</span>(<span>thread</span><span>=</span><span>False</span>)</pre></div>
</details>
<details>
<summary>
How do I use Scalene with PyTorch on the Mac?
</summary>
<p dir="auto"><strong>A:</strong> Scalene works with PyTorch version 1.5.1 on Mac OS X. There's a bug in newer versions of PyTorch (<a data-error-text="Failed to load title" data-id="870369259" data-permission-text="Title is private" data-url="https://github.com/pytorch/pytorch/issues/57185" data-hovercard-type="issue" data-hovercard-url="/pytorch/pytorch/issues/57185/hovercard" href="https://github.com/pytorch/pytorch/issues/57185">pytorch/pytorch#57185</a>) that interferes with Scalene (discussion here: <a data-error-text="Failed to load title" data-id="780743681" data-permission-text="Title is private" data-url="https://github.com/plasma-umass/scalene/issues/110" data-hovercard-type="issue" data-hovercard-url="/plasma-umass/scalene/issues/110/hovercard" href="https://github.com/plasma-umass/scalene/issues/110">#110</a>), but only on Macs.</p>
</details>
<p dir="auto"><h2 tabindex="-1" dir="auto">Technical Information</h2><a id="user-content-technical-information" aria-label="Permalink: Technical Information" href="#technical-information"></a></p>
<p dir="auto">For details about how Scalene works, please see the following paper, which won the Jay Lepreau Best Paper Award at <a href="https://www.usenix.org/conference/osdi23/presentation/berger" rel="nofollow">OSDI 2023</a>: <a href="https://arxiv.org/pdf/2212.07597" rel="nofollow">Triangulating Python Performance Issues with Scalene</a>. (Note that this paper does not include information about the AI-driven proposed optimizations.)</p>
<details>
<summary>
To cite Scalene in an academic paper, please use the following:
</summary>
<div dir="auto" data-snippet-clipboard-copy-content="@inproceedings{288540,
author = {Emery D. Berger and Sam Stern and Juan Altmayer Pizzorno},
title = {Triangulating Python Performance Issues with {S}calene},
booktitle = {{17th USENIX Symposium on Operating Systems Design and Implementation (OSDI 23)}},
year = {2023},
isbn = {978-1-939133-34-2},
address = {Boston, MA},
pages = {51--64},
url = {https://www.usenix.org/conference/osdi23/presentation/berger},
publisher = {USENIX Association},
month = jul
}"><pre>@inproceedings{288540,
author = {Emery D. Berger and Sam Stern and Juan Altmayer Pizzorno},
title = {Triangulating Python Performance Issues with {S}calene},
booktitle = {{17th USENIX Symposium on Operating Systems Design and Implementation (OSDI 23)}},
year = {2023},
isbn = {978-1-939133-34-2},
address = {Boston, MA},
pages = {51--64},
url = {https://www.usenix.org/conference/osdi23/presentation/berger},
publisher = {USENIX Association},
month = jul
}</pre></div>
</details>
<p dir="auto"><h2 tabindex="-1" dir="auto">Success Stories</h2><a id="user-content-success-stories" aria-label="Permalink: Success Stories" href="#success-stories"></a></p>
<p dir="auto">If you use Scalene to successfully debug a performance problem, please <a href="https://github.com/plasma-umass/scalene/issues/58" data-hovercard-type="issue" data-hovercard-url="/plasma-umass/scalene/issues/58/hovercard">add a comment to this issue</a>!</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Acknowledgements</h2><a id="user-content-acknowledgements" aria-label="Permalink: Acknowledgements" href="#acknowledgements"></a></p>
<p dir="auto">Logo created by <a href="https://www.linkedin.com/in/sophia-berger/" rel="nofollow">Sophia Berger</a>.</p>
<p dir="auto">This material is based upon work supported by the National Science
Foundation under Grant No. 1955610. Any opinions, findings, and
conclusions or recommendations expressed in this material are those of
the author(s) and do not necessarily reflect the views of the National
Science Foundation.</p>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Sam's Club CTO to Exit Due to Walmart Relocation Policy (116 pts)]]></title>
            <link>https://www.bloomberg.com/news/articles/2024-10-21/senior-walmart-wmt-executive-to-leave-company-due-to-relocation-policy</link>
            <guid>41908400</guid>
            <pubDate>Mon, 21 Oct 2024 20:50:25 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.bloomberg.com/news/articles/2024-10-21/senior-walmart-wmt-executive-to-leave-company-due-to-relocation-policy">https://www.bloomberg.com/news/articles/2024-10-21/senior-walmart-wmt-executive-to-leave-company-due-to-relocation-policy</a>, See on <a href="https://news.ycombinator.com/item?id=41908400">Hacker News</a></p>
<div id="readability-page-1" class="page"><section>
    <section>
        <h3>Why did this happen?</h3>
        <p>Please make sure your browser supports JavaScript and cookies and that you are not
            blocking them from loading.
            For more information you can review our <a href="https://www.bloomberg.com/notices/tos">Terms of
                Service</a> and <a href="https://www.bloomberg.com/notices/tos">Cookie Policy</a>.</p>
    </section>
    <section>
        <h3>Need Help?</h3>
        <p>For inquiries related to this message please <a href="https://www.bloomberg.com/feedback">contact
            our support team</a> and provide the reference ID below.</p>
        <p>Block reference ID:</p>
    </section>
</section></div>]]></description>
        </item>
        <item>
            <title><![CDATA[T-Mobile, AT&T oppose unlocking rule, claim locked phones are good for users (199 pts)]]></title>
            <link>https://arstechnica.com/tech-policy/2024/10/t-mobile-att-oppose-unlocking-rule-claim-locked-phones-are-good-for-users/</link>
            <guid>41908231</guid>
            <pubDate>Mon, 21 Oct 2024 20:31:07 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://arstechnica.com/tech-policy/2024/10/t-mobile-att-oppose-unlocking-rule-claim-locked-phones-are-good-for-users/">https://arstechnica.com/tech-policy/2024/10/t-mobile-att-oppose-unlocking-rule-claim-locked-phones-are-good-for-users/</a>, See on <a href="https://news.ycombinator.com/item?id=41908231">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="main">
            <article data-id="2057562">
  
  <header>
  <div>
    <div>
      

      

      <p>
        Carriers fight plan to require unlocking of phones 60 days after activation.
      </p>

      
    </div>

          <div>
        <p><img width="1000" height="1000" src="https://cdn.arstechnica.net/wp-content/uploads/2024/10/phone-locking-1000x1000-1729540365.jpg" alt="A smartphone wrapped in a metal chain and padlock" loading="eager" decoding="async" fetchpriority="high" srcset="https://cdn.arstechnica.net/wp-content/uploads/2024/10/phone-locking-1000x1000-1729540365.jpg 1000w, https://cdn.arstechnica.net/wp-content/uploads/2024/10/phone-locking-150x150-1729540364.jpg 150w, https://cdn.arstechnica.net/wp-content/uploads/2024/10/phone-locking-500x500-1729540365.jpg 500w" sizes="(max-width: 1000px) 100vw, 1000px">
        </p>
        
      </div>
      </div>
</header>

  

  
      
    
    <div>
                      
                      
          
<p>T-Mobile and AT&amp;T say US regulators should drop a plan to require unlocking of phones within 60 days of activation, claiming that locking phones to a carrier's network makes it possible to provide cheaper handsets to consumers. "If the Commission mandates a uniform unlocking policy, it is consumers—not providers—who stand to lose the most," T-Mobile alleged in an <a href="https://www.fcc.gov/ecfs/document/1017178290200/1">October 17 filing</a> with the Federal Communications Commission.</p>
<p>The proposed rule has support from consumer advocacy groups who say it will give users more choice and lower their costs. T-Mobile has been criticized for locking phones for up to a year, which makes it impossible to use a phone on a rival's network. T-Mobile claims that with a 60-day unlocking rule, "consumers risk losing access to the benefits of free or heavily subsidized handsets because the proposal would force providers to reduce the line-up of their most compelling handset offers."</p>
<p>If the proposed rule is enacted, "T-Mobile estimates that its prepaid customers, for example, would see subsidies reduced by 40 percent to 70 percent for both its lower and higher-end devices, such as the Moto G, Samsung A15, and iPhone 12," the carrier said. "A handset unlocking mandate would also leave providers little choice but to limit their handset offers to lower cost and often lesser performing handsets."</p>
<p>T-Mobile and other carriers are responding to a call for public comments that began after the FCC approved a <a href="https://docs.fcc.gov/public/attachments/FCC-24-77A1.pdf">Notice of Proposed Rulemaking</a> (NPRM) in a 5–0 vote. The FCC is proposing "to require all mobile wireless service providers to unlock handsets 60 days after a consumer's handset is activated with the provider, unless within the 60-day period the service provider determines the handset was purchased through fraud."</p>

          
                      
                  </div>
                    
        
          
    
    <div>
          
          
<p>When the FCC proposed the 60-day unlocking rule in July 2024, the agency <a href="https://arstechnica.com/tech-policy/2024/07/fcc-blasts-t-mobiles-365-day-phone-locking-proposes-60-day-unlock-rule/">criticized T-Mobile</a> for locking prepaid phones for a year. The NPRM pointed out that "T-Mobile recently increased its locking period for one of its brands, Metro by T-Mobile, from 180 days to 365 days."</p>
<p>T-Mobile's <a href="https://www.t-mobile.com/responsibility/consumer-info/policies/sim-unlock-policy">policy</a> says the carrier will only unlock mobile devices on prepaid plans if "at least 365 days... have passed since the device was activated on the T-Mobile network."</p>
<p>"You bought your phone, you should be able to take it to any provider you want," FCC Chairwoman Jessica Rosenworcel said when the FCC proposed the rule. "Some providers already operate this way. Others do not. In fact, some have recently increased the time their customers must wait until they can unlock their device by as much as 100 percent."</p>
<h2>T-Mobile locking policy more onerous</h2>
<p>T-Mobile executives, who also argue that the FCC lacks authority to impose the proposed rule, met with FCC officials last week to express their concerns.</p>
<p>"T-Mobile is passionate about winning customers for life, and explained how its handset unlocking policies greatly benefit our customers," the carrier said in its post-meeting filing. "Our policies allow us to deliver access to high-speed mobile broadband on a nationwide 5G network <em>via handsets that are free or heavily discounted</em> off the manufacturer's suggested retail price. T-Mobile's unlocking policies are transparent, and there is absolutely no evidence of consumer harm stemming from these policies. T-Mobile's current unlocking policies also help T-Mobile combat handset theft and fraud by sophisticated, international criminal organizations."</p>
<p>For postpaid users, T-Mobile says it allows unlocking of fully paid-off phones that have been active for at least 40 days. But given the 365-day lock on prepaid users, T-Mobile's overall policy is more onerous than those of other carriers. T-Mobile has also <a href="https://arstechnica.com/tech-policy/2024/06/t-mobile-users-enraged-as-un-carrier-breaks-promise-to-never-raise-prices/">faced angry customers</a> because of a recent decision to raise prices on plans that were <a href="https://arstechnica.com/tech-policy/2024/06/t-mobile-users-thought-they-had-a-lifetime-price-lock-guess-what-happened-next/">advertised as having a lifetime price lock</a>.</p>

          
                  </div>
                    
        
          
    
    <div>
          
          
<p>AT&amp;T <a href="https://www.att.com/support/article/wireless/KM1262649/">enables unlocking</a> of paid-off phones after 60 days for postpaid users and after six months for prepaid users. AT&amp;T lodged similar complaints as T-Mobile, saying in an October 7 <a href="https://www.fcc.gov/ecfs/document/1008217166272/1">filing</a> that the FCC's proposed rules would "mak[e] handsets less affordable for consumers, especially those in low-income households," and "exacerbate handset arbitrage, fraud, and trafficking. "</p>
<p>AT&amp;T told the FCC that "requiring providers to unlock handsets before they are paid-off would ultimately harm consumers by creating upward pressure on handset prices and disincentives to finance handsets on flexible terms." If the FCC implements any rules, it should maintain "existing contractual arrangements between customers and providers, ensure that providers have at least 180 days to detect fraud before unlocking a device, and include at least a 24-month period for providers to implement any new rules," AT&amp;T said.</p>
<p>Verizon, which already faces unlocking rules because of <a href="https://arstechnica.com/information-technology/2015/02/wireless-net-neutrality-puts-verizon-and-rivals-on-equal-footing/">requirements imposed on spectrum licenses</a> it owns, automatically <a href="https://www.verizon.com/about/consumer-safety/device-unlocking-policy">unlocks</a> phones after 60 days for prepaid and postpaid users. Among the three major carriers, Verizon is the most amenable to the FCC's new rules.</p>

<h2>Consumer groups: Make Verizon rules industry-wide</h2>
<p>An <a href="https://www.fcc.gov/ecfs/document/1018046516995/1">October 18 filing</a> supporting a strict unlocking rule was submitted by numerous consumer advocacy groups including Public Knowledge, New America's Open Technology Institute, Consumer Reports, the National Consumers League, the National Consumer Law Center, and the National Digital Inclusion Alliance.</p>
<p>"Wireless users are subject to unnecessary restrictions in the form of locked devices, which tie them to their service providers even when better options may be available. Handset locking practices limit consumer freedom and lessen competition by creating an artificial technological barrier to switching providers," the groups said.</p>

          
                  </div>
                    
        
          
    
    <div>
          
          
<p>The groups cited the Verizon rules as a model and urged the FCC to require "that device unlocking is truly automatic—that is, unlocked after the requisite time period without any additional actions of the consumer." Carriers should not be allowed to lock phones for longer than 60 days even when a phone is on a financing plan with outstanding payments, the groups' letter said:</p>
<blockquote><p>Providers should be required to transition out of selling devices without this [automatic unlocking] capability and the industry-wide rule should be the same as the one protecting Verizon customers today: after the expiration of the initial period, the handset must automatically unlock regardless of whether: (1) the customer asks for the handset to be unlocked or (2) the handset is fully paid off. Removing this barrier to switching will make the standard simple for consumers and encourage providers to compete more vigorously on mobile service price, quality, and innovation.</p></blockquote>
<p>In an October 2 <a href="https://www.fcc.gov/ecfs/document/10022636123363/1">filing</a>, Verizon said it supports "a uniform approach to handset unlocking that allows all wireless providers to lock wireless handsets for a reasonable period of time to limit fraud and to enable device subsidies, followed by automatic unlocking absent evidence of fraud."</p>
<p>Verizon said 60 days should be the minimum for postpaid devices so that carriers have time to detect fraud and theft, and that "a longer, 180-day locking period for prepaid is necessary to enable wireless providers to continue offering subsidies that make phones affordable for prepaid customers." Regardless of what time frame the FCC chooses, Verizon said "a uniform unlocking policy that applies to all providers... will benefit both consumers and competition."</p>

          
                  </div>
                    
        
          
    
    <div>

        
        <div>
          
          
<h2>FCC considers impact on phone subsidies</h2>
<p>While the FCC is likely to impose an unlocking rule, one question is whether it will apply when a carrier has provided a discounted phone. The FCC's NPRM asked the public for "comment on the impact of a 60-day unlocking requirement in connection with service providers' incentives to offer discounted handsets for postpaid and prepaid service plans."</p>
<p>The FCC acknowledged Verizon's argument "that providers may rely on handset locking to sustain their ability to offer handset subsidies and that such subsidies may be particularly important in prepaid environments." But the FCC noted that public interest groups "argue that locked handsets tied to prepaid plans can disadvantage low-income customers most of all since they may not have the resources to switch service providers or purchase new handsets."</p>
<p>The public interest groups also note that unlocked handsets "facilitate a robust secondary market for used devices, providing consumers with more affordable options," the NPRM said.</p>
<p>The FCC says it can impose phone-unlocking rules using its legal authority under Title III of the Communications Act "to protect the public interest through spectrum licensing and regulations to require mobile wireless service providers to provide handset unlocking." The FCC said it <a href="https://arstechnica.com/information-technology/2015/02/wireless-net-neutrality-puts-verizon-and-rivals-on-equal-footing/">previously relied</a> on the same Title III authority when it imposed the unlocking rules on 700 MHz C Block spectrum licenses purchased by Verizon.</p>
<p>T-Mobile told the FCC in a <a href="https://www.fcc.gov/ecfs/document/10923041590680/1">filing</a> last month that "none of the litany of Title III provisions cited in the NPRM support the expansive authority asserted here to regulate consumer handsets (rather than telecommunications services)." T-Mobile also said that "the Commission's legal vulnerabilities on this score are only magnified in light of recent Supreme Court precedent."</p>
<p>The Supreme Court recently <a href="https://arstechnica.com/tech-policy/2024/06/scotus-kills-chevron-deference-giving-courts-more-power-to-block-federal-rules/">overturned</a> the 40-year-old <em>Chevron</em> precedent that gave agencies like the FCC judicial deference when interpreting ambiguous laws. The end of <em>Chevron</em> makes it harder for agencies to issue regulations without explicit authorization from Congress. This is a potential problem for the FCC in its fight to revive net neutrality rules, which are currently <a href="https://arstechnica.com/tech-policy/2024/08/fcc-suffers-major-setback-in-attempt-to-defend-net-neutrality-rules/">blocked</a> by a court order pending the outcome of litigation.</p>


          
                  </div>

                  
          <div>
  <div>
          <p><a href="https://arstechnica.com/author/jon-brodkin/"><img src="https://arstechnica.com/wp-content/uploads/2016/05/j.brodkin-11_2.jpg" alt="Photo of Jon Brodkin"></a></p>
  </div>

  <div>
    

    <p>
      Jon is a Senior IT Reporter for Ars Technica. He covers the telecom industry, Federal Communications Commission rulemakings, broadband consumer affairs, court cases, and government regulation of the tech industry.
    </p>
  </div>
</div>


  


  


  
              </div>
  </article>


<div>
    <header>
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 40 26"><defs><clipPath id="most-read_svg__a"><path fill="none" d="M0 0h40v26H0z"></path></clipPath><clipPath id="most-read_svg__b"><path fill="none" d="M0 0h40v26H0z"></path></clipPath></defs><g clip-path="url(#most-read_svg__a)"><g fill="none" clip-path="url(#most-read_svg__b)"><path fill="currentColor" d="M20 2h.8q1.5 0 3 .6c.6.2 1.1.4 1.7.6 1.3.5 2.6 1.3 3.9 2.1.6.4 1.2.8 1.8 1.3 2.9 2.3 5.1 4.9 6.3 6.4-1.1 1.5-3.4 4-6.3 6.4-.6.5-1.2.9-1.8 1.3q-1.95 1.35-3.9 2.1c-.6.2-1.1.4-1.7.6q-1.5.45-3 .6h-1.6q-1.5 0-3-.6c-.6-.2-1.1-.4-1.7-.6-1.3-.5-2.6-1.3-3.9-2.1-.6-.4-1.2-.8-1.8-1.3-2.9-2.3-5.1-4.9-6.3-6.4 1.1-1.5 3.4-4 6.3-6.4.6-.5 1.2-.9 1.8-1.3q1.95-1.35 3.9-2.1c.6-.2 1.1-.4 1.7-.6q1.5-.45 3-.6zm0-2h-1c-1.2 0-2.3.3-3.4.6-.6.2-1.3.4-1.9.7-1.5.6-2.9 1.4-4.3 2.3-.7.5-1.3.9-1.9 1.4C2.9 8.7 0 13 0 13s2.9 4.3 7.5 7.9c.6.5 1.3 1 1.9 1.4 1.3.9 2.7 1.7 4.3 2.3.6.3 1.3.5 1.9.7 1.1.3 2.3.6 3.4.6h2c1.2 0 2.3-.3 3.4-.6.6-.2 1.3-.4 1.9-.7 1.5-.6 2.9-1.4 4.3-2.3.7-.5 1.3-.9 1.9-1.4C37.1 17.3 40 13 40 13s-2.9-4.3-7.5-7.9c-.6-.5-1.3-1-1.9-1.4-1.3-.9-2.8-1.7-4.3-2.3-.6-.3-1.3-.5-1.9-.7C23.3.4 22.1.1 21 .1h-1"></path><path fill="#ff4e00" d="M20 5c-4.4 0-8 3.6-8 8s3.6 8 8 8 8-3.6 8-8-3.6-8-8-8m0 11c-1.7 0-3-1.3-3-3s1.3-3 3-3 3 1.3 3 3-1.3 3-3 3"></path></g></g></svg>
      
    </header>
    <ol>
              <li>
                      <img src="https://cdn.arstechnica.net/wp-content/uploads/2024/10/2025-VW-ID.-Buzz-1-768x432.jpg" alt="Listing image for first story in Most Read: The 2025 VW ID Buzz electric bus delivers on the hype" decoding="async" loading="lazy">
                    
        </li>
                    <li>
                    
        </li>
                    <li>
                    
        </li>
                    <li>
                    
        </li>
                    <li>
                    
        </li>
                  </ol>
</div>


  

  </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[First images from Euclid are in (930 pts)]]></title>
            <link>https://dlmultimedia.esa.int/download/public/videos/2024/10/023/orig-2410_023_AR_EN.mp4</link>
            <guid>41908075</guid>
            <pubDate>Mon, 21 Oct 2024 20:15:36 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://dlmultimedia.esa.int/download/public/videos/2024/10/023/orig-2410_023_AR_EN.mp4">https://dlmultimedia.esa.int/download/public/videos/2024/10/023/orig-2410_023_AR_EN.mp4</a>, See on <a href="https://news.ycombinator.com/item?id=41908075">Hacker News</a></p>
&lt;Not HTML&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[Show HN: Data Formulator – AI-powered data visualization from Microsoft Research (173 pts)]]></title>
            <link>https://github.com/microsoft/data-formulator</link>
            <guid>41907719</guid>
            <pubDate>Mon, 21 Oct 2024 19:42:15 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/microsoft/data-formulator">https://github.com/microsoft/data-formulator</a>, See on <a href="https://news.ycombinator.com/item?id=41907719">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><div dir="auto"><h2 tabindex="-1" dir="auto">
    <a target="_blank" rel="noopener noreferrer" href="https://github.com/microsoft/data-formulator/blob/main/public/favicon.ico"><img src="https://github.com/microsoft/data-formulator/raw/main/public/favicon.ico" alt="Data Formulator icon" width="28"></a> <b>Data Formulator: Create Rich Visualizations with AI</b>
</h2><a id="user-content------data-formulator-create-rich-visualizations-with-ai" aria-label="Permalink: Data Formulator: Create Rich Visualizations with AI" href="#-----data-formulator-create-rich-visualizations-with-ai"></a></div>
<p dir="auto"><a href="https://arxiv.org/abs/2408.16119" rel="nofollow"><img src="https://camo.githubusercontent.com/14d37bb78cfe9f732aa995eed2e8478e8af2ca184600e69d886b267a42045ccb/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f50617065722d61725869763a323430382e31363131392d6233316231622e737667" alt="arxiv" data-canonical-src="https://img.shields.io/badge/Paper-arXiv:2408.16119-b31b1b.svg"></a> 
<a href="https://opensource.org/licenses/MIT" rel="nofollow"><img src="https://camo.githubusercontent.com/6cd0120cc4c5ac11d28b2c60f76033b52db98dac641de3b2644bb054b449d60c/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f4c6963656e73652d4d49542d79656c6c6f772e737667" alt="License: MIT" data-canonical-src="https://img.shields.io/badge/License-MIT-yellow.svg"></a> 
<a href="https://youtu.be/3ndlwt0Wi3c" rel="nofollow"><img src="https://camo.githubusercontent.com/5a9427530b8083884be8c5591fd00ca34a8d9ccf2bd68fa168a57987de68803e/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f596f75547562652d77686974653f6c6f676f3d796f7574756265266c6f676f436f6c6f723d253233464630303030" alt="YouTube" data-canonical-src="https://img.shields.io/badge/YouTube-white?logo=youtube&amp;logoColor=%23FF0000"></a> 
<a href="https://github.com/microsoft/data-formulator/actions/workflows/python-build.yml"><img src="https://github.com/microsoft/data-formulator/actions/workflows/python-build.yml/badge.svg" alt="build"></a></p>
<p dir="auto">Transform data and create rich visualizations iteratively with AI 🪄. Try Data Formulator now in GitHub Codespaces!</p>
<p dir="auto"><a href="https://codespaces.new/microsoft/data-formulator?quickstart=1" rel="nofollow"><img src="https://github.com/codespaces/badge.svg" alt="Open in GitHub Codespaces"></a></p>
<kbd>
  <a href="https://codespaces.new/microsoft/data-formulator?quickstart=1" title="open Data Formulator in GitHub Codespaces" rel="nofollow"><img src="https://github.com/microsoft/data-formulator/raw/main/public/data-formulator-screenshot.png"></a>
</kbd>
<p dir="auto"><h2 tabindex="-1" dir="auto">News 🔥🔥🔥</h2><a id="user-content-news-" aria-label="Permalink: News 🔥🔥🔥" href="#news-"></a></p>
<ul dir="auto">
<li>
<p dir="auto">[10-11-2024] Data Formulator python package released!</p>
<ul dir="auto">
<li>You can now install Data Formulator using Python and run it locally, easily. <a href="#get-started">[check it out]</a>.</li>
<li>Our Codespace configuration is also updated for fast start up ⚡️. <a href="https://codespaces.new/microsoft/data-formulator?quickstart=1" rel="nofollow">[try it now!]</a></li>
<li>New exprimental feature: load an image or a messy text, and ask AI parsing and cleaning it for you(!). <a href="https://github.com/microsoft/data-formulator/pull/31#issuecomment-2403652717" data-hovercard-type="pull_request" data-hovercard-url="/microsoft/data-formulator/pull/31/hovercard">[demo]</a></li>
</ul>
</li>
<li>
<p dir="auto">[10-01-2024] Initial release of Data Formulator, check out our <a href="https://www.microsoft.com/en-us/research/blog/data-formulator-exploring-how-ai-can-help-analysts-create-rich-data-visualizations/" rel="nofollow">[blog]</a> and <a href="https://youtu.be/3ndlwt0Wi3c" rel="nofollow">[video]</a>!</p>
</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Overview</h2><a id="user-content-overview" aria-label="Permalink: Overview" href="#overview"></a></p>
<p dir="auto"><strong>Data Formulator</strong> is an application from Microsoft Research that uses large language models to transform data, expediting the practice of data visualization.</p>
<p dir="auto">Data Formulator is an AI-powered tool for analysts to iteratively create rich visualizations. Unlike most chat-based AI tools where users need to describe everything in natural language, Data Formulator combines <em>user interface interactions (UI)</em> and <em>natural language (NL) inputs</em> for easier interaction. This blended approach makes it easier for users to describe their chart designs while delegating data transformation to AI.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Get Started</h2><a id="user-content-get-started" aria-label="Permalink: Get Started" href="#get-started"></a></p>
<p dir="auto">Play with Data Formulator with one of the following options:</p>
<ul dir="auto">
<li>
<p dir="auto"><strong>Option 1: Install via Python PIP</strong></p>
<p dir="auto">Use Python PIP for an easy setup experience, running locally (recommend: install it in a virtual environment).</p>
<div dir="auto" data-snippet-clipboard-copy-content="# install data_formulator
pip install data_formulator

# start data_formulator
data_formulator 

# alternatively, you can run data formualtor with this command
python -m data_formulator"><pre><span><span>#</span> install data_formulator</span>
pip install data_formulator

<span><span>#</span> start data_formulator</span>
data_formulator 

<span><span>#</span> alternatively, you can run data formualtor with this command</span>
python -m data_formulator</pre></div>
<p dir="auto">Data Formulator will be automatically opened in the browser at <a href="http://localhost:5000/" rel="nofollow">http://localhost:5000</a>.</p>
</li>
<li>
<p dir="auto"><strong>Option 2: Codespaces (5 minutes)</strong></p>
<p dir="auto">You can also run Data Formualtor in codespace, we have everything pre-configured. For more details, see <a href="https://github.com/microsoft/data-formulator/blob/main/CODESPACES.md">CODESPACES.md</a>.</p>
<p dir="auto"><a href="https://codespaces.new/microsoft/data-formulator?quickstart=1" rel="nofollow"><img src="https://github.com/codespaces/badge.svg" alt="Open in GitHub Codespaces"></a></p>
</li>
<li>
<p dir="auto"><strong>Option 3: Working in the developer mode</strong></p>
<p dir="auto">You can build Data Formulator locally if you prefer full control over your development environment and the ability to customize the setup to your specific needs. For detailed instructions, refer to <a href="https://github.com/microsoft/data-formulator/blob/main/DEVELOPMENT.md">DEVELOPMENT.md</a>.</p>
</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Using Data Formulator</h2><a id="user-content-using-data-formulator" aria-label="Permalink: Using Data Formulator" href="#using-data-formulator"></a></p>
<p dir="auto">Once you’ve completed the setup using either option, follow these steps to start using Data Formulator:</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">The basics of data visualization</h3><a id="user-content-the-basics-of-data-visualization" aria-label="Permalink: The basics of data visualization" href="#the-basics-of-data-visualization"></a></p>
<ul dir="auto">
<li>Provide OpenAI keys and select a model (GPT-4o suggested) and choose a dataset.</li>
<li>Choose a chart type, and then drag-and-drop data fields to chart properties (x, y, color, ...) to specify visual encodings.</li>
</ul>
<details open="">
  <summary>
    
    <span aria-label="Video description renewable.mp4">renewable.mp4</span>
    <span></span>
  </summary>

  <video src="https://private-user-images.githubusercontent.com/93549116/362473677-0fbea012-1d2d-46c3-a923-b1fc5eb5e5b8.mp4?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3Mjk1ODI1MDMsIm5iZiI6MTcyOTU4MjIwMywicGF0aCI6Ii85MzU0OTExNi8zNjI0NzM2NzctMGZiZWEwMTItMWQyZC00NmMzLWE5MjMtYjFmYzVlYjVlNWI4Lm1wND9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNDEwMjIlMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjQxMDIyVDA3MzAwM1omWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPTUzODM2NmVjNmUwZWNkZjI0YjFiNWU1NzQyODJmZjUxOGQ3YmU2YmQwNGJkMmNkZWU5ZTUxZmFlODkyOTkxNjQmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0In0.LazVdekJNdzcdrX46rQwPoHJJg_bHUGPuJGBNOFx7Jc" data-canonical-src="https://private-user-images.githubusercontent.com/93549116/362473677-0fbea012-1d2d-46c3-a923-b1fc5eb5e5b8.mp4?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3Mjk1ODI1MDMsIm5iZiI6MTcyOTU4MjIwMywicGF0aCI6Ii85MzU0OTExNi8zNjI0NzM2NzctMGZiZWEwMTItMWQyZC00NmMzLWE5MjMtYjFmYzVlYjVlNWI4Lm1wND9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNDEwMjIlMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjQxMDIyVDA3MzAwM1omWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPTUzODM2NmVjNmUwZWNkZjI0YjFiNWU1NzQyODJmZjUxOGQ3YmU2YmQwNGJkMmNkZWU5ZTUxZmFlODkyOTkxNjQmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0In0.LazVdekJNdzcdrX46rQwPoHJJg_bHUGPuJGBNOFx7Jc" controls="controls" muted="muted">

  </video>
</details>

<p dir="auto"><h3 tabindex="-1" dir="auto">Create visualization beyond the initial dataset (powered by 🤖)</h3><a id="user-content-create-visualization-beyond-the-initial-dataset-powered-by-" aria-label="Permalink: Create visualization beyond the initial dataset (powered by 🤖)" href="#create-visualization-beyond-the-initial-dataset-powered-by-"></a></p>
<ul dir="auto">
<li>You can type names of <strong>fields that do not exist in current data</strong> in the encoding shelf:
<ul dir="auto">
<li>this tells Data Formulator that you want to create visualizions that require computation or transformation from existing data,</li>
<li>you can optionally provide a natural language prompt to explain your intent to clarify your intent (not necessary when field names are self-explanatory).</li>
</ul>
</li>
<li>Click the <strong>Formulate</strong> button.
<ul dir="auto">
<li>Data Formulator will transform data and instantiate the visualization based on the encoding and prompt.</li>
</ul>
</li>
<li>Inspect the data, chart and code.</li>
<li>To create a new chart based on existing ones, follow up in natural language:
<ul dir="auto">
<li>provide a follow up prompt (e.g., <em>``show only top 5!''</em>),</li>
<li>you may also update visual encodings for the new chart.</li>
</ul>
</li>
</ul>
<details open="">
  <summary>
    
    <span aria-label="Video description renewable-pct.mp4">renewable-pct.mp4</span>
    <span></span>
  </summary>

  <video src="https://private-user-images.githubusercontent.com/93549116/362473692-160c69d2-f42d-435c-9ff3-b1229b5bddba.mp4?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3Mjk1ODI1MDMsIm5iZiI6MTcyOTU4MjIwMywicGF0aCI6Ii85MzU0OTExNi8zNjI0NzM2OTItMTYwYzY5ZDItZjQyZC00MzVjLTlmZjMtYjEyMjliNWJkZGJhLm1wND9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNDEwMjIlMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjQxMDIyVDA3MzAwM1omWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPWRjYTY5MDY0YThhZTFiMWFmZTdjODVlYjgwOTc4NWVmYjA2YjY1NzJiNWZjN2ZiZDJjZTZhZjNkMzcwZGQ0YWImWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0In0.HcvvNh0w679raPdJSJveVEJJrJbF_nZFcPk42A5sjjk" data-canonical-src="https://private-user-images.githubusercontent.com/93549116/362473692-160c69d2-f42d-435c-9ff3-b1229b5bddba.mp4?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3Mjk1ODI1MDMsIm5iZiI6MTcyOTU4MjIwMywicGF0aCI6Ii85MzU0OTExNi8zNjI0NzM2OTItMTYwYzY5ZDItZjQyZC00MzVjLTlmZjMtYjEyMjliNWJkZGJhLm1wND9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNDEwMjIlMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjQxMDIyVDA3MzAwM1omWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPWRjYTY5MDY0YThhZTFiMWFmZTdjODVlYjgwOTc4NWVmYjA2YjY1NzJiNWZjN2ZiZDJjZTZhZjNkMzcwZGQ0YWImWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0In0.HcvvNh0w679raPdJSJveVEJJrJbF_nZFcPk42A5sjjk" controls="controls" muted="muted">

  </video>
</details>

<details open="">
  <summary>
    
    <span aria-label="Video description renewable-rank.mp4">renewable-rank.mp4</span>
    <span></span>
  </summary>

  <video src="https://private-user-images.githubusercontent.com/93549116/362473693-c93b3e84-8ca8-49ae-80ea-f91ceef34acb.mp4?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3Mjk1ODI1MDMsIm5iZiI6MTcyOTU4MjIwMywicGF0aCI6Ii85MzU0OTExNi8zNjI0NzM2OTMtYzkzYjNlODQtOGNhOC00OWFlLTgwZWEtZjkxY2VlZjM0YWNiLm1wND9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNDEwMjIlMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjQxMDIyVDA3MzAwM1omWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPTc4MjNmYzNlYTRkNGMyODFhN2JiMzU0ZTg4NWZiODkyODJhNjJjNzI4MzNkOTMyODcxYjQ5YmFkZjMxNjc4OTYmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0In0.h5DVujuxv-F8aPyEiUyMThp9xl88-j-9h6LylXwytUo" data-canonical-src="https://private-user-images.githubusercontent.com/93549116/362473693-c93b3e84-8ca8-49ae-80ea-f91ceef34acb.mp4?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3Mjk1ODI1MDMsIm5iZiI6MTcyOTU4MjIwMywicGF0aCI6Ii85MzU0OTExNi8zNjI0NzM2OTMtYzkzYjNlODQtOGNhOC00OWFlLTgwZWEtZjkxY2VlZjM0YWNiLm1wND9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNDEwMjIlMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjQxMDIyVDA3MzAwM1omWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPTc4MjNmYzNlYTRkNGMyODFhN2JiMzU0ZTg4NWZiODkyODJhNjJjNzI4MzNkOTMyODcxYjQ5YmFkZjMxNjc4OTYmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0In0.h5DVujuxv-F8aPyEiUyMThp9xl88-j-9h6LylXwytUo" controls="controls" muted="muted">

  </video>
</details>

<p dir="auto">Repeat this process as needed to explore and understand your data. Your explorations are trackable in the <strong>Data Threads</strong> panel.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Developers' Guide</h2><a id="user-content-developers-guide" aria-label="Permalink: Developers' Guide" href="#developers-guide"></a></p>
<p dir="auto">Follow the <a href="https://github.com/microsoft/data-formulator/blob/main/DEVELOPMENT.md">developers' instructions</a> to build your new data analysis tools on top of Data Formulator.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Research Papers</h2><a id="user-content-research-papers" aria-label="Permalink: Research Papers" href="#research-papers"></a></p>
<ul dir="auto">
<li><a href="https://arxiv.org/abs/2408.16119" rel="nofollow">Data Formulator 2: Iteratively Creating Rich Visualizations with AI</a></li>
</ul>
<div data-snippet-clipboard-copy-content="@article{wang2024dataformulator2iteratively,
      title={Data Formulator 2: Iteratively Creating Rich Visualizations with AI}, 
      author={Chenglong Wang and Bongshin Lee and Steven Drucker and Dan Marshall and Jianfeng Gao},
      year={2024},
      booktitle={ArXiv preprint arXiv:2408.16119},
}"><pre><code>@article{wang2024dataformulator2iteratively,
      title={Data Formulator 2: Iteratively Creating Rich Visualizations with AI}, 
      author={Chenglong Wang and Bongshin Lee and Steven Drucker and Dan Marshall and Jianfeng Gao},
      year={2024},
      booktitle={ArXiv preprint arXiv:2408.16119},
}
</code></pre></div>
<ul dir="auto">
<li><a href="https://arxiv.org/abs/2309.10094" rel="nofollow">Data Formulator: AI-powered Concept-driven Visualization Authoring</a></li>
</ul>
<div data-snippet-clipboard-copy-content="@article{wang2023data,
  title={Data Formulator: AI-powered Concept-driven Visualization Authoring},
  author={Wang, Chenglong and Thompson, John and Lee, Bongshin},
  journal={IEEE Transactions on Visualization and Computer Graphics},
  year={2023},
  publisher={IEEE}
}"><pre><code>@article{wang2023data,
  title={Data Formulator: AI-powered Concept-driven Visualization Authoring},
  author={Wang, Chenglong and Thompson, John and Lee, Bongshin},
  journal={IEEE Transactions on Visualization and Computer Graphics},
  year={2023},
  publisher={IEEE}
}
</code></pre></div>
<p dir="auto"><h2 tabindex="-1" dir="auto">Contributing</h2><a id="user-content-contributing" aria-label="Permalink: Contributing" href="#contributing"></a></p>
<p dir="auto">This project welcomes contributions and suggestions. Most contributions require you to
agree to a Contributor License Agreement (CLA) declaring that you have the right to,
and actually do, grant us the rights to use your contribution. For details, visit
<a href="https://cla.microsoft.com/" rel="nofollow">https://cla.microsoft.com</a>.</p>
<p dir="auto">When you submit a pull request, a CLA-bot will automatically determine whether you need
to provide a CLA and decorate the PR appropriately (e.g., label, comment). Simply follow the
instructions provided by the bot. You will only need to do this once across all repositories using our CLA.</p>
<p dir="auto">This project has adopted the <a href="https://opensource.microsoft.com/codeofconduct/" rel="nofollow">Microsoft Open Source Code of Conduct</a>.
For more information see the <a href="https://opensource.microsoft.com/codeofconduct/faq/" rel="nofollow">Code of Conduct FAQ</a>
or contact <a href="mailto:opencode@microsoft.com">opencode@microsoft.com</a> with any additional questions or comments.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Trademarks</h2><a id="user-content-trademarks" aria-label="Permalink: Trademarks" href="#trademarks"></a></p>
<p dir="auto">This project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft
trademarks or logos is subject to and must follow
<a href="https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general" rel="nofollow">Microsoft's Trademark &amp; Brand Guidelines</a>.
Use of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.
Any use of third-party trademarks or logos are subject to those third-party's policies.</p>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Software Engineering Body of Knowledge (SWEBOK) v4.0 is out [pdf] (180 pts)]]></title>
            <link>https://ieeecs-media.computer.org/media/education/swebok/swebok-v4.pdf</link>
            <guid>41907412</guid>
            <pubDate>Mon, 21 Oct 2024 19:16:38 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://ieeecs-media.computer.org/media/education/swebok/swebok-v4.pdf">https://ieeecs-media.computer.org/media/education/swebok/swebok-v4.pdf</a>, See on <a href="https://news.ycombinator.com/item?id=41907412">Hacker News</a></p>
&lt;Not HTML&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[Overengineering a way to know if people are in my university's CS lab (172 pts)]]></title>
            <link>https://www.amoses.dev/blog/upl-people-counter/</link>
            <guid>41907360</guid>
            <pubDate>Mon, 21 Oct 2024 19:12:13 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.amoses.dev/blog/upl-people-counter/">https://www.amoses.dev/blog/upl-people-counter/</a>, See on <a href="https://news.ycombinator.com/item?id=41907360">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-astro-cid-w6n32adp=""> <h2 id="a-history-of-upls-cameras"><a href="#a-history-of-upls-cameras">#</a>A History of UPL’s Cameras</h2>
<p>For almost as long as <a href="https://www.upl.cs.wisc.edu/">the Undergraduate Projects Lab</a> at the University of Wisconsin has existed, there’s been a camera of some sort peering at the room. There’s evidence of a system existing even as far back as the 1990s, with a <a href="https://web.archive.org/web/19981202092257/http://www.upl.cs.wisc.edu/cgi-bin/uplcam.html">prehistoric revision of the site</a> mentioning that an old iteration was:</p>
<blockquote>
<p>…a $15 video camera attached to the wall with duct tape, connected to a VCR, connected to a video spigot in a Mac IIcx, running Timed Video Grabber (TVG), and FTPd. Dax, an HP workstation ran a script that would try to FTP the latest image every 60 seconds. Because the clocks would drift, occasionally, the file accesses would collide, and the whole scheme would break.</p>
</blockquote>
<p>Just <em>reading</em> that makes me stare at the camera that now sits perched on top of the arcade cabinet with wonder. What used to be several thousand dollars of equipment is now achievable (with immeasurably better quality) with a $50 webcam plugged into a Raspberry Pi.</p>
<p><img src="https://www.amoses.dev/images/upl-pc/old_upl.png" alt="A grainy image featuring an interior view of the UPL, a triangular-shaped undergraduate lab at UW-Madison.">
    <img src="https://www.amoses.dev/images/upl-pc/new_upl.jpeg" alt="An image featuring the interior of the UPL, a lab at UW-Madison. Students sit at laptops.">
</p>
<p><i>Taken ~25 years apart.</i></p>
<p>I could—and probably will—write an entire other blog post about the intricate history of the UPL, mentioning how <a href="https://web.archive.org/web/20001003051528/http://www.upl.cs.wisc.edu/uplcam/spincam.html">older versions of the website</a> allowed for users to control the tilt and pan of the camera using four stepper motors attached to the camera.</p>
<p>However, the focus of this article is about the two <em>latest</em> iterations of cameras in the UPL.</p>
<h2 id="is-the-upl-open-right-now"><a href="#is-the-upl-open-right-now">#</a>“Is the UPL open right now?”</h2>
<p>I’m sure that any UPL member can testify the horror of arriving to the lab to see a closed door. If you live anywhere off campus, it’s heartbreaking to see your arduous trek to the CS building result in failure.</p>
<p>There’s no doubt that as far back as IRC, members of the UPL messaged each other asking if the lab was open. With the advent of mobile phones, it’s gotten easier to bother your friends—who may not even be in the room!</p>
<p>Well, myself, in collaboration with other UPL members, decided to fix this issue in, perhaps, the most CS-student-esque way possible: an automated system to identify the occupancy of the lab.</p>
<h2 id="people-counting"><a href="#people-counting">#</a>People counting</h2>
<p>The first iteration of the people counting system (as built by <a href="https://github.com/mdberkey">Michael Berkey</a>) utilized a Logitech C920 camera mounted on a vantage point that had a clear view of the room. A Discord bot was set on a 15 minute loop (using discord.py.ext’s <code>@tasks.loop(minutes=15)</code>) to call a YOLOv7 model set to class 0 (detecting people). The bot called the webcam to take an image, then ran it through the model for inference. It returned the number of people in the room (and annotated the image with bounding boxes of where it believed the people to be, for debug purposes).</p>
<p><img src="https://www.amoses.dev/images/upl-pc/camera-over.png" alt="The front side of a C920 webcam.">
    <img src="https://www.amoses.dev/images/upl-pc/camera-peek.png" alt="The back side of a C920 webcam.">
</p>
<p><i>…don’t mind the tape.</i></p>
<p>It then set the name of a channel to the results (either <code>1-person-in-upl</code> or <code>X-people-in-upl</code>), which others could check.</p>
<p><img src="https://www.amoses.dev/images/upl-pc/8-people-in-upl.png" alt="A channel in the UPL Discord reads '8 people in UPL'.">
</p>
<p><i>An example of what the Discord looked like on a day with a semi-busy UPL.</i></p>
<h2 id="switching-to-door-sensing"><a href="#switching-to-door-sensing">#</a>Switching to door sensing</h2>
<p>This worked perfectly for a while — people would check the Discord channel name and see the estimated count of the number of people in the room. If it said “zero people”, they could infer that the UPL wasn’t open.</p>
<p>However, this solution started presenting issues. For one, having people in the room didn’t necessarily indicate that the UPL was open. There could be a meeting, or a separate gathering where the doors were closed and people weren’t allowed inside. This was confusing to people who might have seen “8 people inside the UPL”, only to arrive at the building to see coords having a meeting.</p>
<p>There was also the issue of the model sometimes interpreting the chair in the corner as a person<sup><a href="#user-content-fn-1" id="user-content-fnref-1" data-footnote-ref="" aria-describedby="footnote-label">1</a></sup>:</p>
<p><img src="https://www.amoses.dev/images/upl-pc/upl_empty.png" alt="An image of the empty lab. An annotation on a brown armchair reads 'Person 0', despite no human on the chair.">
</p>
<p><i>Either the model is too sensitive, or the UPL has a friendly ghost!</i></p>
<p>It was around this time that I stumbled upon the homepage of <a href="https://miters.mit.edu/">MITERS</a>, a makerspace at MIT. On their website, they broadcast whether the door to the space is open using a reed switch attached to a Raspberry Pi. Reed switches are small, physical components that are able to detect a magnetic field. If you put one on a doorframe, and then attach a tiny magnet to the door itself, you have an effective way of detecting whether a door is open or closed! I was able to find <a href="https://andrewbirkel.com/projects/MITERS_Door.html">a writeup</a> by a former member of the space on their implementation, but I can’t guarantee that it’s accurate to how it’s set up there currently.</p>
<p>I considered using similar components for a door status checker for the UPL — it wouldn’t have been too much effort to buy WiFi enabled ESP32 modules and off-the-shelf door-mountable reed switches. Then, I would have the chips simply send a POST request with their status every time the door was opened or closed.</p>
<p>I decided against this approach for a few reasons:</p>
<ul>
<li>
<p>The UPL doesn’t really have the equipment to maintain such a system. I don’t know how to solder, and mounting breadboards to the walls doesn’t seem like the most future-proof or aesthetically pleasing solution.</p>
</li>
<li>
<p>If the system were to spontaneously break after I left, it would be difficult to find somebody to fix it. The UPL is mainly a software oriented lab!</p>
</li>
<li>
<p>The WiFi ran by the university (UWNet) requires you to log in with a <a href="https://en.wikipedia.org/wiki/Captive_portal">captive portal</a> to register your device to connect to the network. Without intervention, it will occasionally require you to sign back in to renew your ability to connect<sup><a href="#user-content-fn-2" id="user-content-fnref-2" data-footnote-ref="" aria-describedby="footnote-label">2</a></sup>. While there <em>are</em> some ways to emulate the specific requests a typical browser would use to authenticate with your NetID, it would be a ton of recurring effort (and the login it used would have to be changed as people graduated)!</p>
</li>
</ul>
<p>So, I decided that the sensors themselves would have to act autonomously and simply relay their state to a device elsewhere in the room. Luckily, the Raspberry Pi that ran the code for the people counter was easily repurposed. I installed <a href="https://www.home-assistant.io/">Home Assistant</a>, an open source platform for interfacing with various network connected devices.</p>
<p>There are plenty of devices that track the status of doors, made by companies like Ring and ADT for home security. However, they usually require proprietary hubs to check their status, and don’t offer easily accessible APIs to interface with the device. Luckily, there was a better solution!</p>
<h2 id="zigbee"><a href="#zigbee">#</a>Zigbee!</h2>
<p>Enter Zigbee. It’s a low-rate mesh wireless protocol that allows for smart devices to communicate over a personal area network. A benefit of this is that you’re able to use one hub to communicate with a variety of devices, even those made by different manufacturers. Instead of searching for a particular brand for the door contact sensors, I would just have to find ones that supported the Zigbee protocol. Then I would be able to view their status through Home Assistant’s dashboard.</p>
<p>It’s important to note that Zigbee radios operate independently from WiFi or Bluetooth antennas. If you want to interface with Zigbee devices, you’ll have to pick up a special receiver that can support the protocol. For this project, I picked up <a href="https://www.amazon.com/SONOFF-Universal-Assistant-Zigbee2MQTT-Wireless/dp/B0B6P22YJC/">this one</a> made by SONOFF. Home Assistant’s Zigbee integration is called <a href="https://www.home-assistant.io/integrations/zha/">Zigbee Home Automation</a>, and it supports a variety of Zigbee coordinators (the USB dongles that allow for connections). When you use this integration, Home Assistant automatically creates a Zigbee network that the devices can join.</p>
<p>I decided to use <a href="https://www.amazon.com/Aqara-MCCGQ11LM-Window-Sensor-White/dp/B07D37VDM3/">these Aqara door and window sensors</a> for this project. They had the best reviews out of all of the Zigbee door sensors I looked at, and have a battery life of two years (with an easily replaceable CR1632 cell).</p>
<p>Once the coordinator and sensors arrived, I created a Home Assistant login and installed the ZHA integration. Pairing simply required holding the “reset” button on the sensors until Home Assistant recognized them and added the corresponding entities in the dashboard.</p>

<h2 id="using-the-door-statuses"><a href="#using-the-door-statuses">#</a>Using the door statuses</h2>
<p>Once this was all configured, I had the live statuses of the doors through the Home Assistant dashboard! I’m not going to lie, it was really fun opening and closing the doors repeatedly and seeing the dashboard change in real-time (even if passerby in the CS building probably thought I was crazy).</p>
<p>
    <video src="https://www.amoses.dev/images/upl-pc/doors.mp4" controls="">
    Your browser does not support the video tag.
</video>
</p>
<p><i>It’s so satisfying to watch this happen in real-time.<sup><a href="#user-content-fn-3" id="user-content-fnref-3" data-footnote-ref="" aria-describedby="footnote-label">3</a></sup></i></p>
<p>An important thing to note here is that UWNet provides total access point isolation. None of the devices on the network can see any of the others (for good reason, as it would be a huge security vulnerability for any devices with open ports). If this wasn’t a limitation, I would just have the website directly query the rpi.</p>
<p>My first intuition was to use Home Assistant’s <a href="https://www.home-assistant.io/integrations/rest_command/">RESTful Command</a> integration to send a POST request to my webserver whenever the status of the doors changed. These require you to setup each command ahead of time, in HA’s <code>configuration.yml</code>:</p>
<div><figure><pre data-language="yaml"><code><div><div><p>1</p></div><p><span>rest_command</span><span>:</span></p></div><div><div><p>2</p></div><p><span>  </span><span>door1_opened</span><span>:</span></p></div><div><div><p>3</p></div><p><span>    </span><span>url</span><span>: </span><span>"https://doors.amoses.dev/door1/open"</span></p></div><div><div><p>4</p></div><p><span>    </span><span>method</span><span>: </span><span>POST</span></p></div><div><div><p>5</p></div><p><span>    </span><span>headers</span><span>:</span></p></div><div><div><p>6</p></div><p><span>      </span><span>content-type</span><span>: </span><span>"application/json"</span></p></div><div><div><p>7</p></div><p><span>    </span><span>payload</span><span>: </span><span>'{"door": "door1", "state": "open"}'</span></p></div><div><div><p>8</p></div><p><span>    </span><span>content_type</span><span>: </span><span>"application/json; charset=utf-8"</span></p></div><div><p>9</p></div><div><div><p>10</p></div><p><span>  </span><span>door1_closed</span><span>:</span></p></div><div><div><p>11</p></div><p><span>    </span><span>url</span><span>: </span><span>"https://doors.amoses.dev/door1/close"</span></p></div><div><div><p>12</p></div><p><span>    </span><span>method</span><span>: </span><span>POST</span></p></div><div><div><p>13</p></div><p><span>    </span><span>headers</span><span>:</span></p></div><div><div><p>14</p></div><p><span>      </span><span>content-type</span><span>: </span><span>"application/json"</span></p></div><div><div><p>15</p></div><p><span>    </span><span>payload</span><span>: </span><span>'{"door": "door1", "state": "closed"}'</span></p></div><div><div><p>16</p></div><p><span>    </span><span>content_type</span><span>: </span><span>"application/json; charset=utf-8"</span></p></div><div><p>17</p></div><div><div><p>18</p></div><p><span>  </span><span>door2_opened</span><span>:</span></p></div><div><div><p>19</p></div><p><span>    </span><span>url</span><span>: </span><span>"https://doors.amoses.dev/door2/open"</span></p></div><div><div><p>20</p></div><p><span>    </span><span>method</span><span>: </span><span>POST</span></p></div><div><div><p>21</p></div><p><span>    </span><span>headers</span><span>:</span></p></div><div><div><p>22</p></div><p><span>      </span><span>content-type</span><span>: </span><span>"application/json"</span></p></div><div><div><p>23</p></div><p><span>    </span><span>payload</span><span>: </span><span>'{"door": "door2", "state": "open"}'</span></p></div><div><div><p>24</p></div><p><span>    </span><span>content_type</span><span>: </span><span>"application/json; charset=utf-8"</span></p></div><div><p>25</p></div><div><div><p>26</p></div><p><span>  </span><span>door2_closed</span><span>:</span></p></div><div><div><p>27</p></div><p><span>    </span><span>url</span><span>: </span><span>"https://doors.amoses.dev/door2/close"</span></p></div><div><div><p>28</p></div><p><span>    </span><span>method</span><span>: </span><span>POST</span></p></div><div><div><p>29</p></div><p><span>    </span><span>headers</span><span>:</span></p></div><div><div><p>30</p></div><p><span>      </span><span>content-type</span><span>: </span><span>"application/json"</span></p></div><div><div><p>31</p></div><p><span>    </span><span>payload</span><span>: </span><span>'{"door": "door2", "state": "closed"}'</span></p></div><div><div><p>32</p></div><p><span>    </span><span>content_type</span><span>: </span><span>"application/json; charset=utf-8"</span></p></div></code></pre></figure></div>
<p>…but I very quickly realized that this solution wasn’t the best. For one, when I published <a href="https://github.com/UW-UPL/people-counter-v2/blob/main/home-assistant/configuration.yaml">the source code</a> onto GitHub, some very funny students decided that they would manually simulate the POST requests and change the status of the doors to be inaccurate. That’s what I get for leaving the endpoint unsecured!<sup><a href="#user-content-fn-4" id="user-content-fnref-4" data-footnote-ref="" aria-describedby="footnote-label">4</a></sup></p>
<p>I eventually learned that Home Assistant provides a <a href="https://developers.home-assistant.io/docs/api/rest/">RESTful API</a> directly alongside the web dashboard. If I set that up, I would be able to query the instance for the states of the connected devices.<sup><a href="#user-content-fn-5" id="user-content-fnref-5" data-footnote-ref="" aria-describedby="footnote-label">5</a></sup> All it took was appending an <code>/api/</code> route to the HA URL. I could just use that!</p>
<p>The API has all of its routes authenticated with a bearer token (to most likely mirror the permissions of the frontend, which requires a user login before showing any data). Given that I wanted to display the door status on the UPL’s page, I realized the potential danger in shipping the bearer token with the site. Any crafty user could take it and access any other route on Home Assistant’s API. Given the level of information and control available on HA instances, this could be disastrous.</p>
<p>I made a quick webserver using Express that proxies the request with the bearer token and only serves the relevant door information. Because it displays this separately, the user has no way of seeing or manipulating anything beyond this.</p>
<div><figure><pre data-language="js"><code><details><summary><div><div><p>7 collapsed lines</p></div></div></summary><div><div><p>1</p></div><p><span>const</span><span> </span><span>express</span><span> = </span><span>require</span><span>(</span><span>"express"</span><span>);</span></p></div><div><div><p>2</p></div><p><span>const</span><span> </span><span>axios</span><span> = </span><span>require</span><span>(</span><span>"axios"</span><span>);</span></p></div><div><div><p>3</p></div><p><span>const</span><span> </span><span>cors</span><span> = </span><span>require</span><span>(</span><span>"cors"</span><span>);</span></p></div><div><p>4</p></div><div><div><p>5</p></div><p><span>const</span><span> </span><span>app</span><span> = </span><span>express</span><span>();</span></p></div><div><div><p>6</p></div><p><span>const</span><span> </span><span>PORT</span><span> = </span><span>3500</span><span>;</span></p></div><div><p>7</p></div></details><div><div><p>8</p></div><p><span>const</span><span> </span><span>apiUrl</span><span> = </span><span>"https://HOMEASSISTANT-URL-HERE/api/states"</span><span>;</span></p></div><div><div><p>9</p></div><p><span>const</span><span> </span><span>token</span><span> = </span><span>"Bearer TOKEN-GOES-HERE"</span><span>;</span></p></div><div><p>10</p></div><div><div><p>11</p></div><p><span>app</span><span>.</span><span>use</span><span>(</span><span>cors</span><span>());</span></p></div><div><p>12</p></div><div><div><p>13</p></div><p><span>app</span><span>.</span><span>get</span><span>(</span><span>"/door-status"</span><span>, </span><span>async</span><span> (</span><span>req</span><span>, </span><span>res</span><span>) </span><span>=&gt;</span><span> {</span></p></div><div><div><p>14</p></div><p><span>  </span><span>try</span><span> {</span></p></div><div><div><p>15</p></div><p><span>    </span><span>const</span><span> </span><span>response</span><span> = </span><span>await</span><span> </span><span>axios</span><span>.</span><span>get</span><span>(</span><span>apiUrl</span><span>, {</span></p></div><div><div><p>16</p></div><p><span>      </span><span>headers:</span><span> {</span></p></div><div><div><p>17</p></div><p><span>        </span><span>Authorization:</span><span> </span><span>token</span><span>,</span></p></div><div><div><p>18</p></div><p><span><span>      </span></span><span>},</span></p></div><div><div><p>19</p></div><p><span><span>    </span></span><span>});</span></p></div><div><p>20</p></div><div><div><p>21</p></div><p><span>    </span><span>const</span><span> </span><span>data</span><span> = </span><span>response</span><span>.</span><span>data</span><span>;</span></p></div><div><p>22</p></div><div><div><p>23</p></div><p><span>    </span><span>// grab the items with the appropriate HA entity ids</span></p></div><div><div><p>24</p></div><p><span>    </span><span>const</span><span> </span><span>doors</span><span> = </span><span>data</span><span>.</span><span>filter</span><span>(</span></p></div><div><div><p>25</p></div><p><span><span>      </span></span><span>(</span><span>item</span><span>) </span><span>=&gt;</span></p></div><div><div><p>26</p></div><p><span>        </span><span>item</span><span>.</span><span>entity_id</span><span> === </span><span>"binary_sensor.back"</span><span> ||</span></p></div><div><div><p>27</p></div><p><span>        </span><span>item</span><span>.</span><span>entity_id</span><span> === </span><span>"binary_sensor.front"</span></p></div><div><div><p>28</p></div><p><span><span>    </span></span><span>);</span></p></div><div><p>29</p></div><div><div><p>30</p></div><p><span>    </span><span>// extract status and last updated information</span></p></div><div><div><p>31</p></div><p><span>    </span><span>const</span><span> </span><span>doorStatus</span><span> = </span><span>doors</span><span>.</span><span>map</span><span>((</span><span>door</span><span>) </span><span>=&gt;</span><span> ({</span></p></div><div><div><p>32</p></div><p><span>      </span><span>door:</span><span> </span><span>door</span><span>.</span><span>attributes</span><span>.</span><span>friendly_name</span><span>,</span></p></div><div><div><p>33</p></div><p><span>      </span><span>status:</span><span> </span><span>door</span><span>.</span><span>state</span><span>,</span></p></div><div><div><p>34</p></div><p><span>      </span><span>last_updated:</span><span> </span><span>door</span><span>.</span><span>last_updated</span><span>,</span></p></div><div><div><p>35</p></div><p><span><span>    </span></span><span>}));</span></p></div><div><p>36</p></div><div><div><p>37</p></div><p><span>    </span><span>// send the filtered data as a json response</span></p></div><div><div><p>38</p></div><p><span>    </span><span>res</span><span>.</span><span>json</span><span>(</span><span>doorStatus</span><span>);</span></p></div><div><div><p>39</p></div><p><span><span>  </span></span><span>} </span><span>catch</span><span> (</span><span>error</span><span>) {</span></p></div><div><div><p>40</p></div><p><span>    </span><span>res</span><span>.</span><span>status</span><span>(</span><span>500</span><span>).</span><span>send</span><span>(</span><span>"Error fetching data"</span><span>);</span></p></div><div><div><p>41</p></div><p><span><span>  </span></span><span>}</span></p></div><div><div><p>42</p></div><p><span>});</span></p></div><details><summary><div><div><p>11 collapsed lines</p></div></div></summary><div><p>43</p></div><div><div><p>44</p></div><p><span>// :P</span></p></div><div><div><p>45</p></div><p><span>app</span><span>.</span><span>get</span><span>(</span><span>"/"</span><span>, </span><span>async</span><span> (</span><span>req</span><span>, </span><span>res</span><span>) </span><span>=&gt;</span><span> {</span></p></div><div><div><p>46</p></div><p><span>  </span><span>res</span></p></div><div><div><p>47</p></div><p><span><span>    </span></span><span>.</span><span>status</span><span>(</span><span>200</span><span>)</span></p></div><div><div><p>48</p></div><p><span><span>    </span></span><span>.</span><span>send</span><span>(</span><span>"&lt;html&gt;&lt;body&gt;&lt;b&gt;wow upl door status endpoint 443&lt;/b&gt;&lt;/body&gt;&lt;/html&gt;"</span><span>);</span></p></div><div><div><p>49</p></div><p><span>});</span></p></div><div><p>50</p></div><div><div><p>51</p></div><p><span>app</span><span>.</span><span>listen</span><span>(</span><span>PORT</span><span>, () </span><span>=&gt;</span><span> {</span></p></div><div><div><p>52</p></div><p><span>  </span><span>console</span><span>.</span><span>log</span><span>(</span><span>`Server is running on port </span><span>${</span><span>PORT</span><span>}</span><span>`</span><span>);</span></p></div><div><div><p>53</p></div><p><span>});</span></p></div></details></code></pre></figure></div>
<p>Now, the server will query Home Assistant’s API on your behalf (with the proper bearer token). It’ll return a JSON object of the door statuses and their last change, like so:</p>
<div><figure><pre data-language="json"><code><div><p><span>[</span></p></div><div><p><span><span>  </span></span><span>{</span></p></div><div><p><span>    </span><span>"door"</span><span>: </span><span>"back"</span><span>,</span></p></div><div><p><span>    </span><span>"status"</span><span>: </span><span>"on"</span><span>,</span></p></div><div><p><span>    </span><span>"last_updated"</span><span>: </span><span>"2024-10-12T20:01:54.353657+00:00"</span></p></div><div><p><span><span>  </span></span><span>},</span></p></div><div><p><span><span>  </span></span><span>{</span></p></div><div><p><span>    </span><span>"door"</span><span>: </span><span>"front"</span><span>,</span></p></div><div><p><span>    </span><span>"status"</span><span>: </span><span>"on"</span><span>,</span></p></div><div><p><span>    </span><span>"last_updated"</span><span>: </span><span>"2024-10-12T20:02:10.132178+00:00"</span></p></div><div><p><span><span>  </span></span><span>}</span></p></div><div><p><span>]</span></p></div></code></pre></figure></div>
<p>…and the Discord bot/UPL website can use that to let people know what the status is.</p>
<p><img src="https://www.amoses.dev/images/upl-pc/open_door.png" alt="The UPL website reads 'the doors are open!' with an icon of an open door.">
</p>
<p><i>The UPL website uses a header component which fetches the door status every 15 seconds.</i></p>
<p><img src="https://www.amoses.dev/images/upl-pc/upl_discord.png" alt="A channel in the UPL Discord reads 'UPL doors open'.">
</p>
<p><i>The Discord channel name is an easy way to see the status without opening the site.</i></p>
<h2 id="conclusion"><a href="#conclusion">#</a>Conclusion</h2>
<p>I’m pretty happy with how this project turned out. It’s been really fun developing something that I actually use every day, and I find it pretty special that every time I check if the UPL’s open or not, I’m doing it via something that I made myself.</p>
<br>
<hr>
<br>
<section data-footnotes="">
<ol>
<li id="user-content-fn-1">
<p>I’m sure that you could apply various transformations to the image to mask out that area from detection. But people occasionally sit in it! <a href="#user-content-fnref-1" data-footnote-backref="" aria-label="Back to reference 1">↩</a></p>
</li>
<li id="user-content-fn-2">
<p>If you’ve ever lived in the UW dorms, you’ll know all too well what I’m talking about. Every device without browser access needs to have its MAC address whitelisted by the network system. This authorization expires in six months, so you’ll lose internet access and have to renew. <a href="#user-content-fnref-2" data-footnote-backref="" aria-label="Back to reference 2">↩</a></p>
</li>
<li id="user-content-fn-3">
<p>The UPL has a front and back entrance, hence the wording being “doors” instead of “door”. <a href="#user-content-fnref-3" data-footnote-backref="" aria-label="Back to reference 3">↩</a></p>
</li>
<li id="user-content-fn-4">
<p>Before you try, these endpoints aren’t in use anymore. :P <a href="#user-content-fnref-4" data-footnote-backref="" aria-label="Back to reference 4">↩</a></p>
</li>
<li id="user-content-fn-5">
<p>Keen eyed readers might be asking “what about the AP isolation issue that you just mentioned?!”. Well, I found a fantastic addon for Home Assistant that allows you to access your dashboard (and the API, by extension) when not on the LAN of the pi. It uses Cloudflare tunnels, and you can find <a href="https://github.com/brenner-tobias/addon-cloudflared">its GitHub repository here.</a> <a href="#user-content-fnref-5" data-footnote-backref="" aria-label="Back to reference 5">↩</a></p>
</li>
</ol>
</section> </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Microsoft is introducing hidden APIs to VS Code only enabled for Copilot (357 pts)]]></title>
            <link>https://old.reddit.com/r/ChatGPTCoding/comments/1g8xrub/microsoft_is_introducing_hidden_apis_to_vs_code/</link>
            <guid>41907350</guid>
            <pubDate>Mon, 21 Oct 2024 19:11:29 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://old.reddit.com/r/ChatGPTCoding/comments/1g8xrub/microsoft_is_introducing_hidden_apis_to_vs_code/">https://old.reddit.com/r/ChatGPTCoding/comments/1g8xrub/microsoft_is_introducing_hidden_apis_to_vs_code/</a>, See on <a href="https://news.ycombinator.com/item?id=41907350">Hacker News</a></p>
Couldn't get https://old.reddit.com/r/ChatGPTCoding/comments/1g8xrub/microsoft_is_introducing_hidden_apis_to_vs_code/: Error: Request failed with status code 403]]></description>
        </item>
        <item>
            <title><![CDATA[Please do not write below the line (330 pts)]]></title>
            <link>http://www.bbctvlicence.com/Please%20do%20not%20write%20below%20the%20line.htm</link>
            <guid>41907071</guid>
            <pubDate>Mon, 21 Oct 2024 18:42:16 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="http://www.bbctvlicence.com/Please%20do%20not%20write%20below%20the%20line.htm">http://www.bbctvlicence.com/Please%20do%20not%20write%20below%20the%20line.htm</a>, See on <a href="https://news.ycombinator.com/item?id=41907071">Hacker News</a></p>
<div id="readability-page-1" class="page"><div> <span face="Bell MT"><b><span color="#000000" size="+2" face="Verdana">Please do not write below this line</span></b></span><p><span face="Bell MT"><b><span color="#000000" size="+1" face="Verdana">I have been vexed for some time by the request at the bottom of
						  each letter that I am not to write below the line. </span></b></span></p><img src="http://www.bbctvlicence.com/Please%20do%20not%20write%20below%20this%20line.jpg" width="538" height="25"> 
						<p><span face="Bell MT"><b><span color="#000000" size="+1" face="Verdana">I emailed TVL/BBC on 5 November 2006 to find out why:
						  </span></b></span></p> 
						<div> 
								<p><span size="-1" color="#000000" face="Arial">I've had a letter from TV Licencing and I'm interested in the
										statement at the bottom of the page. It says: 'Please do not write below this
										line'. I would like to know why the letter requests this. The line referred to
										is about half an inch from the bottom edge of the letter. </span><br><span size="-1" color="#000000" face="Arial"></span><br><span size="-1" color="#000000" face="Arial">What will happen if I write there? How would you know? I am not
										asked to return the letter, so why the request?</span></p><span face="Bell MT"><b><span color="#000000" face="Arial">Seven weeks later, on 27 December 2006, I receive
								this from one Kelly Wright: </span></b></span><p><span size="-1" color="#000000" face="Arial">Thank you for contacting us. Unfortunately I am unable to deal
										with your request, as you have not provided your address and licence number. If
										you have moved address I will need both your new and old address. Once I have
										this information I will action your request and send you the appropriate form
										or confirmation.</span></p><br><span face="Bell MT"><b><span color="#000000" face="Arial">I reply: </span></b></span> <p><span size="-1" color="#000000" face="Arial"><span face="Arial"><span size="-1" color="#000000">Dear Ms Wright,
										</span></span><br><span face="Arial"><span size="-1" color="#000000">I do not
										have a licence. The letter was sent to me as part of TVL's routine mail-out. It
										was not solicited by me. Copies of these letters are commonly reproduced on the
										internet [example]. </span></span></span><span size="-1" color="#000000" face="Arial">You will see that these letters say "Please do not
										write below this line". So did the one sent to me. Please explain to me why I
										am not allowed to write below the line. </span></p><br><span face="Bell MT"><b><span color="#000000" face="Arial">Evidently, my question was too taxing for Ms
								Wright, as the next response came from Ruairi Mcclean, on 3 January
								2007:</span></b></span><p><span size="-1" color="#000000" face="Arial">The reason you would receive such letters is because we would have
										no record of a TV licence at your address. </span><br><span size="-1" color="#000000" face="Arial"></span><br><span size="-1" color="#000000" face="Arial">The reason you cannot write below the line is because the letters
										go through a OCR machine, and anything below the line is rejected.</span></p><br><span face="Bell MT"><b><span color="#000000" face="Arial">OCR is an abbreviation for "optical character
								recognition", software that scans documents for editing on a computer.
								</span></b></span><br><span face="Bell MT"><b><span color="#000000" face="Arial"></span></b></span><br><span face="Bell MT"><b><span color="#000000" face="Arial">This explanation surprised me. I did not
								understand why, having sent me a letter, TVL wanted it back to scan and edit;
								why not scan it before sending it to me? I responded on 27 January
								2007:</span></b></span><p><span size="-1" color="#000000" face="Arial"><span face="Arial"><span size="-1" color="#000000">Dear Mr Mcclean
										</span></span><br><span face="Arial"><span size="-1" color="#000000">Thank you
										for your reply. You say that I cannot write below the line because the letter
										will go through a OCR machine, and anything below the line will be rejected.
										</span></span><br><span face="Arial"><span size="-1" color="#000000"></span></span><br><span face="Arial"><span size="-1" color="#000000">I have two further questions:
										</span></span><br></span><br><span size="-1" color="#000000" face="Arial"><span face="Arial"><span size="-1" color="#000000">i) I take it from your reply that
										a TV officer is planning to collect the letter back off me in order to scan it.
										Please tell me what purpose this serves. </span></span><br></span><br><span size="-1" color="#000000" face="Arial"><span face="Arial"><span size="-1" color="#000000">ii) Anything I write above the line would also be
										rejected by the OCR. Why am I allowed to write above the line, if I am not
										allowed to write in the narrow strip beneath it? </span></span></span></p><br><span face="Bell MT"><b><span color="#000000" face="Arial">On 30 January 2007, I received this reply from Cas
								Scott:</span></b></span> <p><span size="-1" color="#000000" face="Arial">A
										Licensing officer may call at your property not to collect the letters but to
										check that you are not watching a TV. </span><span size="-1" color="#000000" face="Arial">You may write above the line but as we advised you
										previously anything written below the line when they go through the OCR machine
										they will be rejected. </span><span size="-1" color="#000000" face="Arial">If you would like to confirm your address I can up date our
										records to advise no Television is being watched. </span></p><br> 
								<p><span size="-1" color="#000000" face="Arial">Dear Ms Scott</span><br><span size="-1" color="#000000" face="Arial">Thank you for confirming that I may write above the line.
										</span><br><span size="-1" color="#000000" face="Arial"></span><br><span size="-1" color="#000000" face="Arial">Please explain why, having sent me the
										letter, you want it back for scanning. Also, please explain how I am to get the
										letter to you.</span></p><br><span face="Bell MT"><b><span color="#000000" face="Arial">6 February 2007: I have a reply from Gary
								Bessell.</span></b></span> <p><span size="-1" color="#000000" face="Arial">Dear Ms Scott </span><br><span size="-1" color="#000000" face="Arial">Without your address we are unable to amend our records to show
										that you are not using TV equipment. </span><span size="-1" color="#000000" face="Arial">To return an enquiry letter to TV licensing,
										simply return it. </span></p><br><span face="Bell MT"><b><span color="#000000" face="Arial">I don't seem to be getting a straight
								answer.</span></b></span><p><span size="-1" color="#000000" face="Arial">Dear Mr Bessell</span><br><span size="-1" color="#000000" face="Arial">Thank you for your reply. The purpose of my query is not to ask
										why you want my address.</span><span size="-1" color="#000000" face="Arial">The information that I am seeking is why you want the letter back
										for scanning. There was nothing on the letter that said I had to return it.
										</span><span size="-1" color="#000000" face="Arial">Please note that I
										am not Miss Scott.</span></p><br><span face="Bell MT"><b><span color="#000000" face="Arial">9 February 2007: a reply from Karen
								Mcallister.</span></b></span><p><span size="-1" color="#000000" face="Arial">The information about returning the letter was not on the letter
										itself but on the envelope. </span><br><span size="-1" color="#000000" face="Arial"></span><br><span size="-1" color="#000000" face="Arial">The only
										reason we ask you to return the letter is to help us update our records,
										however if you could provide us with your address we can update our records
										without you returning the letter. </span></p><br><span face="Bell MT"><b><span color="#000000" face="Arial">Having kept all my TVL/BBC envelopes, I examined
								them to see whether any displayed an instruction that I was to return the
								letter. None did. There was a return address but only was for undelivered
								letters. I resist the temptation to pursue this point.
								</span></b></span><p><span size="-1" color="#000000" face="Arial">Dear Ms Mcallister</span><br><span size="-1" color="#000000" face="Arial">Thank you for your reply. </span><span size="-1" color="#000000" face="Arial">What I still do not understand is why you would
										wish to OCR my letter in order to update the records. </span><span size="-1" color="#000000" face="Arial">Obviously, the number below the line must be very
										important. Please could you explain its purpose. </span></p><br><span face="Bell MT"><b><span color="#000000" face="Arial">A reply from Carl
								Graves:</span></b></span><p><span size="-1" color="#000000" face="Arial">I
										apologise that it has not been made clear to you. An OCR stands for a Optical
										Character System. This machine enables us to deal/process with large volumes of
										information in a relatively short space of time. The OCR machine reads the
										information below the line and updates the corresponding records on our
										computer system many times faster than if manually processed.
										</span><span size="-1" color="#000000" face="Arial">If the information
										below the line is obscured in anyway the OCR machine will be unable to read the
										information effectively. The number below the line is a unique number that
										relates to the specific property that the letter has been sent to. Once this
										number is read by the OCR machine it will automatically update the computer
										records that relate to that letter/property/licence/application.
										</span><span size="-1" color="#000000" face="Arial">I hope the above
										information is helpful. </span></p><span face="Bell MT"><b><span color="#000000" face="Arial">I am still not satisfied. If I send a letter back
								to TVL/BBC, and they scan the number at the bottom, it will generate the same
								information as they have already got; so, what's the point?
								</span></b></span><span face="Bell MT"><b><span color="#000000" face="Arial">Let's compare TVL/BBC's operation to another company. Below is a
								scan of a pre-paid return postcard from a company called Santander, in which I
								am a shareholder, which I am invited to return. </span></b></span><p><img src="http://www.bbctvlicence.com/Please%20do%20not%20write%20-%20Santander.jpg" width="553" height="421"></p><span face="Bell MT"><b><span color="#000000" face="Arial">Santander requests that I do not write below the line but, unlike
								TVL/BBC, there is an explanation which is on the right hand side. It says that
								they will retrieve my personal data (which I have authorised them to hold,
								unlike TVL/BBC) by scanning the barcode and number. This makes perfect
								sense.</span></b></span><span face="Bell MT"><b><span color="#000000" face="Arial">TVL/BBC letters, however, do not ask for its letters to be
								returned, and Cas Scott has said that the letters are not sought by TVL/BBC
								agents who make street visits. Even if they did collect the letters, the number
								at the bottom would duplicate the information that they already hold. So, the
								original question - why we not permitted to write below the line - remains a
								mystery.</span></b></span><br></div> <br> </div></div>]]></description>
        </item>
    </channel>
</rss>