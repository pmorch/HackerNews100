<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0">
    <channel>
        <title>HN100 - Readable Contents</title>
        <link>https://hn.algolia.com/api/v1/search_by_date?tags=%28story,poll%29&amp;numericFilters=points%3E100</link>
        <description>Uses Readability to add bodies to the RSS feed</description>
        <lastBuildDate>Tue, 14 May 2024 11:00:04 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>en</language>
        <item>
            <title><![CDATA[Upstreaming Linux kernel support for the Snapdragon X Elite (145 pts)]]></title>
            <link>https://www.qualcomm.com/developer/blog/2024/05/upstreaming-linux-kernel-support-for-the-snapdragon-x-elite</link>
            <guid>40350408</guid>
            <pubDate>Tue, 14 May 2024 00:56:15 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.qualcomm.com/developer/blog/2024/05/upstreaming-linux-kernel-support-for-the-snapdragon-x-elite">https://www.qualcomm.com/developer/blog/2024/05/upstreaming-linux-kernel-support-for-the-snapdragon-x-elite</a>, See on <a href="https://news.ycombinator.com/item?id=40350408">Hacker News</a></p>
&lt;Unparsable&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[Not an iPad Pro Review: Why iPadOS Still Doesn't Get the Basics Right (134 pts)]]></title>
            <link>https://www.macstories.net/stories/not-an-ipad-pro-review/</link>
            <guid>40349347</guid>
            <pubDate>Mon, 13 May 2024 22:33:01 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.macstories.net/stories/not-an-ipad-pro-review/">https://www.macstories.net/stories/not-an-ipad-pro-review/</a>, See on <a href="https://news.ycombinator.com/item?id=40349347">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
                        
<p id="p2">Let me cut to the chase: sadly, I don’t have a new iPad Pro to review today on MacStories.</p>
<p id="p3">I was able to try one in London last week, and, <a href="https://www.macstories.net/stories/thoughts-and-first-impressions-on-the-new-ipad-pros-from-apples-event-in-london/" rel="noopener noreferrer">as I wrote</a>, I came away impressed with the hardware. However, I didn’t get a chance to use a new iPad Pro over the past six days ahead of today’s review embargo.</p>
<p id="p4">I know that many of you were expecting a deeper look at the iPad Pro on MacStories this week, but that will have to come later. I still plan on upgrading to a 13” iPad Pro myself; I’ve decided I want to return to the larger size after <a href="https://www.macstories.net/stories/macpad-how-i-created-the-hybrid-mac-ipad-laptop-and-tablet-that-apple-wont-make/" rel="noopener noreferrer">a few months with the 11” iPad Pro</a>. If you’re interested in checking out reviews of the new iPad Pros from heavy iPad users like yours truly <em>right now</em>, I highly recommend reading and watching what my friends <a href="https://sixcolors.com/post/2024/05/m4_ipad_pro_review/" rel="noopener noreferrer">Jason Snell</a> and <a href="https://www.youtube.com/watch?v=k4VG-XktLBY" rel="noopener noreferrer">Chris Lawley</a> have prepared.</p>
<p id="p6">Still, as I was thinking about my usage of the iPad and <em>why</em> I enjoy using the device so much despite its limitations, I realized that I have never actually written about all of those “limitations” in a single, comprehensive article. In our community, we often hear about the issues of iPadOS and the obstacles people like me run into when working on the platform, but I’ve been guilty in the past of taking context for granted and assuming that you, dear reader, also know precisely what I’m talking about.</p>
<p id="p7">Today, I will rectify that. Instead of reviewing the new iPad Pro, I took the time to put together a list of all the common problems I’ve run into over the past…<em>checks notes</em>…<strong>12 years of working on the iPad</strong>, before its operating system was even called iPadOS.</p>
<p id="p8">My goal with this story was threefold. First, as I’ve said multiple times, I love my iPad and want the platform to get better. If you care about something or someone, sometimes you have to tell them what’s wrong in order to improve and find a new path forward. I hope this story can serve as a reference for those with the power to steer iPadOS in a different direction in the future.</p>
<p id="p9">Second, lately I’ve seen some people argue on Mastodon and Threads that folks who criticize iPadOS do so because their ultimate goal is to have macOS on iPads, and I wanted to clarify this misunderstanding. While I’m on the record as thinking that a hybrid macOS/iPadOS environment would be terrific (I know, <a href="https://www.macstories.net/stories/macpad-how-i-created-the-hybrid-mac-ipad-laptop-and-tablet-that-apple-wont-make/" rel="noopener noreferrer">because I use it</a>), that is not the point. The reality is that, regardless of whether macOS runs on iPads or not, iPadOS <em>is</em> the ideal OS for touch interactions. But it still gets many basic computing features wrong, and there is plenty of low-hanging fruit for Apple to pick. We don’t need to talk about macOS to cover these issues.</p>
<p id="p10">Lastly, I wanted to provide readers with the necessary context to understand what I mean when I mention the limitations of iPadOS. My iPad setup and workflow have <a href="https://www.macstories.net/ipad/" rel="noopener noreferrer">changed enough times</a> over the years that I think some of you may have lost track of the issues I (and others) have been experiencing. This article is a chance to collect them all in one place.</p>
<p id="p11">Let’s dive in.</p>

<h2>Table of Contents</h2><ul><li><a href="https://www.macstories.net/stories/not-an-ipad-pro-review/#missing-apps"><span>Missing Apps</span></a></li><li><a href="https://www.macstories.net/stories/not-an-ipad-pro-review/#not-so-desktop-class-apps"><span>Not-So-Desktop-Class Apps</span></a></li><li><a href="https://www.macstories.net/stories/not-an-ipad-pro-review/#files-a-slow-unreliable-file-manager"><span>Files: A Slow, Unreliable File Manager</span></a></li><li><a href="https://www.macstories.net/stories/not-an-ipad-pro-review/#audio-limitations"><span>Audio Limitations</span></a></li><li><a href="https://www.macstories.net/stories/not-an-ipad-pro-review/#multitasking-a-fractured-mess"><span>Multitasking: A Fractured Mess</span></a></li><li><a href="https://www.macstories.net/stories/not-an-ipad-pro-review/#spotlight"><span>Spotlight</span></a></li><li><a href="https://www.macstories.net/stories/not-an-ipad-pro-review/#lack-of-background-processes-and-system-wide-utilities"><span>Lack of Background Processes and System-Wide Utilities</span></a><ul><li><a href="https://www.macstories.net/stories/not-an-ipad-pro-review/#to-be-fixed-later-this-year-but-only-for-some"><span>To Be Fixed Later This Year, But Only for Some</span></a></li></ul></li><li><a href="https://www.macstories.net/stories/not-an-ipad-pro-review/#inefficiency-by-a-thousand-cuts"><span>Inefficiency by a Thousand Cuts</span></a></li><li><a href="https://www.macstories.net/stories/not-an-ipad-pro-review/#the-need-for-change"><span>The Need for Change</span></a></li></ul><h2 id="missing-apps">Missing Apps</h2>
<p id="p15">Of all the issues I have with iPadOS, I want to start with a relatively simple one: some apps from macOS and iOS just aren’t available on the platform.</p>
<p id="p16">This was fine when the iPad was a new product and Apple was busy launching <a href="https://en.wikipedia.org/wiki/WatchOS" rel="noopener noreferrer">new OSes</a> and <a href="https://www.macstories.net/news/apples-back-to-the-mac-event-now-available-for-download-streaming/" rel="noopener noreferrer">rethinking</a> their approach to the Mac. However, after 14 years, it’s hard to imagine that the company couldn’t have filled these gaps if they really wanted to.</p>
<p id="p17">Here are the apps from iOS and macOS I find myself missing the most on the iPad:</p>
<ul id="ul18"><li><strong>Calculator.</strong> I honestly think it’s wild that after 14 years, the iPad still doesn’t come with a built-in calculator app and that you have to use Google or <a href="https://apps.apple.com/us/app/pcalc/id284666222" rel="noopener noreferrer">a third-party calculator</a> for basic operations. Fortunately, it sounds like Apple is going to address this issue in the <a href="https://www.macrumors.com/2024/04/23/calculator-app-for-ipad-rumor/" rel="noopener noreferrer">upcoming iPadOS 18 release</a>.</li>
<li><strong>TextEdit.</strong> I can’t tell you how many times I use <a href="https://support.apple.com/guide/textedit/welcome/mac" rel="noopener noreferrer">TextEdit</a> on my Mac as a simple scratchpad for bits of text I copy from different apps and need to keep around in a temporary holding spot. TextEdit is also a capable, minimal, and reliable text editor for plain and rich text files. Any modern computing platform should have a built-in text editor, but iPadOS doesn’t. Given how Microsoft was even able to <a href="https://www.theverge.com/2024/2/8/24066389/microsofts-copilot-ai-can-explain-stuff-to-you-in-notepad" rel="noopener noreferrer">productize Notepad on Windows</a> with modern features, this feels like a missed opportunity for Apple.</li>
<li><strong>Preview.</strong> Another unsung hero of Apple’s desktop apps is <a href="https://support.apple.com/guide/preview/welcome/mac" rel="noopener noreferrer">Preview</a>. Whether you have an image or PDF document that you want to check out in more detail, or perhaps even edit, you can rest assured that Preview for macOS has your back. The iPad feels like the ideal platform for Preview: the device is great for viewing photos or reading PDF documents, and the Apple Pencil would take Preview’s annotation capabilities to the next level. Apple probably wants you to believe that <a href="https://developer.apple.com/documentation/quicklook" rel="noopener noreferrer">Quick Look’s interactive system-wide previews</a> on iOS and iPadOS are a substitute for the Preview app, but that’s not the case. There’s something about Preview’s clarity and ease of use that can’t be replaced by a simple Quick Look preview, and that’s not to mention the features that iPadOS’ Quick Look lacks compared to Preview for macOS.</li>
<li><strong>Journal.</strong> This is <a href="https://www.macstories.net/reviews/apples-journal-app-journaling-for-all/" rel="noopener noreferrer">a recent addition</a>, but I was very surprised when the Journal app was launched on iPhone without an iPad counterpart. Just like with Preview, the iPad feels like the optimal platform for Journal: it’s a device you can grab at the end of the day, while reflecting on the things you’ve done and the places you’ve been, as you unwind and get ready for what’s coming up tomorrow. An iPadOS version of Journal would be a fantastic way to select photos you want to remember, jot down a few thoughts, and quickly interact with Apple’s journaling suggestions. As apps like <a href="https://apps.apple.com/us/app/day-one-journal-private-diary/id1044867788" rel="noopener noreferrer">Day One</a> and <a href="https://apps.apple.com/us/app/everlog-journal/id1519935634" rel="noopener noreferrer">Everlog</a> have shown, an iPad can be a fantastic device for journaling.</li>
</ul>
<p id="p20">The list could go on, especially if we consider that Apple sells “Pro” versions of the iPad that cost thousands of dollars, which would benefit from apps that a lot of professionals typically use on macOS. Where’s <a href="https://en.wikipedia.org/wiki/Terminal_(macOS)" rel="noopener noreferrer">Terminal</a> for iPadOS? Why do we have to manage fonts with an <a href="https://www.macstories.net/stories/ios-and-ipados-13-the-macstories-review/28/#settings" rel="noopener noreferrer">obscure method</a> in Settings instead of the excellent <a href="https://support.apple.com/guide/font-book/welcome/mac" rel="noopener noreferrer">Font Book</a> app? Dare I even say it…where’s <a href="https://developer.apple.com/xcode/" rel="noopener noreferrer">Xcode</a> for iPadOS?</p>
<h2 id="not-so-desktop-class-apps">Not-So-Desktop-Class Apps</h2>
<p id="p21">While I’m on the topic of apps, it’s worth pointing out that the apps that did find their way to iPadOS still pale in comparison to their Mac versions in terms of feature set. Despite Apple’s <a href="https://www.macstories.net/stories/ios-and-ipados-16-the-macstories-overview/" rel="noopener noreferrer">promise of desktop-class apps</a> a couple of years ago, the company’s actual implementation has been erratic at best, with an inconsistent delivery of Mac-like features that haven’t done much to raise the status of iPad apps.</p>
<p id="p22">The simplest way to look at this is that most built-in iPad apps don’t match the functionalities offered by their macOS counterparts <strong>despite iPadOS having all the prerequisites</strong> for them to offer said features. For instance, while Apple shipped smart folders and smart lists in <a href="https://www.macstories.net/stories/ios-and-ipados-15-the-macstories-review/14/" rel="noopener noreferrer">Notes</a> and <a href="https://www.macstories.net/stories/reminders-smart-lists-put-unprecedented-control-in-the-hands-of-users/" rel="noopener noreferrer">Reminders</a>, respectively, Mail for iPadOS still doesn’t let you create smart mailboxes like it does on the Mac; nor does Music for iPadOS support the Mac version’s smart playlists. The same is also true for the Files app and its lack of Finder’s smart folder functionality.</p>

<p id="p24">The issue extends beyond these power-user features and touches all sorts of aspects of other Apple apps for iPadOS. Shortcuts is one of the worst offenders with a complete disregard for the iPad’s multitasking capabilities: Shortcuts for Mac offers powerful actions to find and control windows on-screen; the iPad version does not. Safari for Mac lets you adjust the browser’s toolbar to your liking with the ability to drag buttons around and pin specific extensions; Apple rolled out customizable toolbars for iPadOS two years ago, but they never added support for them in Safari for iPad. On the Mac, there is an excellent <a href="https://support.apple.com/guide/dictionary/welcome/mac" rel="noopener noreferrer">Dictionary</a> app that you can open to search for words, look up their definitions, and more; on the iPad, the “dictionary” is limited to the ‘Look Up’ option of the edit menu, which forces you to always select some text first.<sup id="fnref-75358-OWMLG"><a href="#fn-75358-OWMLG" rel="noopener noreferrer">1</a></sup></p>

<p id="p26">I hear what some of you might say: <em>“The iPad is supposed to be a lighter platform than the Mac!”</em> But the thing is, you can’t have it both ways. You can’t make a big deal of bringing desktop-class experiences to the iPad (rightfully so, given the price of iPad Pro models) and then do so inconsistently and sporadically. If iPadOS is meant to support desktop-class apps, the job <a href="https://youtu.be/k6hZ9KdG1QU?si=PnbTAjcmcbda9Zy4" rel="noopener noreferrer">can’t be done halfway</a>.</p>
<h2 id="files-a-slow-unreliable-file-manager">Files: A Slow, Unreliable File Manager</h2>
<p id="p27">Out of all the apps I’ve mentioned so far, I want to shine a spotlight on Files. It’s a bad product that needs a fundamental rethink from a design and performance perspective.</p>
<p id="p28">Files has only marginally improved <a href="https://www.macstories.net/stories/ios-11-the-macstories-review/14/#files" rel="noopener noreferrer">since its debut in iOS 11</a> (!), and we’re well past the point of arguing that, well, iPads aren’t meant to have a file manager. Apple has offered a native iPad file manager for exactly half the iPad’s lifetime; that ship has sailed. It’s time for Apple to take the Files app seriously, because this version just doesn’t cut it.</p>
<p id="p30">It starts from the very basics. Files is not a reliable app, and it’s certainly not as reliable as Finder on macOS. More often than not, I try to select some files in a location to copy or move them to another, and the operation either gets stuck or canceled. This frequently happens with large files located on external drives, forcing me to fall back to my Mac if I want to quickly and reliably copy something from an SSD and save it on my computer. Basic copy, cut, and move operations should be the cornerstone of a file manager, and the fact that Files still fails at those is inexcusable.</p>
<p id="p31">Compared to Finder, the Files app also feels sluggish, and it often gets stuck in an unresponsive state that requires me to force quit it. This tends to occur with folders stored in iCloud Drive, which never open instantly, the way they do on my Mac. Furthermore, Files’ integration with iCloud Drive is much more aggressive than Finder about purging downloaded copies from my iPad’s local storage, causing me to realize – at random times – that a file I need is no longer available offline.</p>
<p id="p32">Sadly, the list of problems goes on:</p>
<ul id="ul33"><li>Unlike Finder, the Files app doesn’t display transfer speeds when moving or copying something. This becomes especially problematic for large files that I’m moving to or from an external drive, making me second-guess whether the transfer is working at all.</li>
<li>Quick Actions aren’t customizable like they are on macOS (and have been <a href="https://www.macstories.net/stories/macos-mojave-the-macstories-review/2/#quick-actions" rel="noopener noreferrer">since Mojave</a>). Files’ inspector panel does support Quick Actions, but, for some reason, you can’t build your own custom actions using Shortcuts. The technology is literally the same across platforms, but one file manager doesn’t support it all the way.</li>
<li>Speaking of Shortcuts, on the iPad, the app has no concept of “getting the current selection” from the Files app. On the Mac, you can create shortcuts that act on the currently-selected file(s) in Finder; on the iPad, Shortcuts doesn’t offer a matching action for the Files app.</li>
<li>We still cannot create smart folders in Files for iPad. The technology is clearly there –&nbsp;Apple added a similar feature to Notes and Reminders – but the Files app for iPad remains behind on this front too.</li>
</ul>
<p id="p35">I saved the most absurd limitation of Files for last. As of iPadOS 17, it’s still impossible to set default apps for opening specific file types. I shouldn’t even have to explain why this is a ridiculous shortcoming, but here we are. On the iPad, every document you click in Files defaults to showing you a Quick Look preview, and there is no way to tell the system that you want to view the document with another app instead.</p>
<p id="p36">It’s not like <a href="https://www.theverge.com/24153045/macos-default-apps-email-browser-how-to" rel="noopener noreferrer">macOS has a fantastic UI for this</a> (Windows has <a href="https://support.microsoft.com/en-us/windows/change-default-programs-in-windows-e5d82cad-17d1-c53b-3505-f10a32e1894d" rel="noopener noreferrer">a much better system</a> for file defaults), but at least it’s something, and it works. On iPad, the Files team seemingly failed to acknowledge that people may want to use different apps for different document types; instead, iPadOS assumes all the file management you could ever need should be happening inside the Files app. The inability to just say, “I want to open this file in [App XYZ]”, is completely unacceptable.</p>


<p id="p39">After seven years, I’m starting to wonder if maybe it’s time for Apple to scrap the Files project and start over with a new app based on the strong foundation of Finder. We’re well past the point of excusing the Files app for being a young file manager; when you’re spending $3,000 on a high-end iPad Pro with plenty of storage, you want the app to manage that storage to be flawless.</p>
<p id="p40">Files is not that app.</p>
<h2 id="audio-limitations">Audio Limitations</h2>
<p id="p41">There are two key problems with the audio system of iPadOS:</p>
<ul id="ul42"><li>You cannot play multiple audio streams at the same time.</li>
<li>You cannot record your own local audio while on a VoIP call.</li>
</ul><p id="p43"><a href="https://sixcolors.com/post/2019/02/a-week-of-podcasting-with-only-an-ipad-pro/" rel="noopener noreferrer">Jason Snell</a> and I have widely documented these issues over the years, but especially the second one as it relates to podcasting on iPad. Jason was my source of inspiration years ago when I tried to record podcasts on my iPad Pro, and…the setup I concocted was a bit of a mess. Allow me to <a href="https://www.macstories.net/stories/beyond-the-tablet/11/#podcasting-from-ipad-pro" rel="noopener noreferrer">quote myself from five years ago</a>:</p>
<blockquote id="blockquote44"><p>
  To record a podcast, I have a conversation with my co-hosts on Skype. However, we don’t use the audio of the Skype conversation itself as the episode: while we’re talking, each of us records a high-quality local audio track from our microphones; at the end, multiple local tracks are mixed together in one audio file that gives the illusion we’re all talking together in the same room rather than thousands of miles apart over Skype. This right here is, by far, the most important requirement for the way I like to record podcasts: we want to use Skype, but each of us has to record the local track of our own microphone input.</p>
<p>  Secondly – and this is why I’ve been unwilling to try other “solutions” for podcasting on iPad over the years – I want my co-hosts to hear my voice coming through the same microphone I’m using to record my local audio track. To have a good conversation with natural back and forth that yields a good final product, I think it’s important that everyone on Skype gets to hear the other person loud and clear.
</p></blockquote>
<p id="p45">Replace “Skype” with “Zoom”, and nothing has changed since I wrote this in 2019. If I want to record my microphone’s local audio while also being on a call, iPadOS doesn’t let me do it. It’s no surprise that I, Jason, and others ended up going back to a Mac, if only to record podcasts the way we like, which is not even a particularly weird way of recording audio. All I’m asking for is a way to record myself while I’m on a call, and iPadOS 17 still doesn’t support it. Plus, I have to imagine that this issue would apply to other fields of audio production as well, such as musicians collaborating <a href="https://www.grammy.com/news/the-postal-service-give-up-20th-anniversary-jenny-lewis-jimmy-tamborello-interview-death-cab-tour" rel="noopener noreferrer">remotely</a>, folks who want to record screencasts over apps that play audio, and more.</p>
<p id="p46">Furthermore, these audio restrictions are part of a broader conversation regarding iPadOS and multiple audio streams, or the lack thereof. Have you ever found yourself watching a YouTube or Twitch video on mute on your Mac while also listening to podcasts or music on the same computer? It’s nice, right?</p>

<p id="p48">Well, the same scenario is impossible to achieve on iPadOS. Only one media playback session can exist at a time on the iPad, so if you’re listening to music and start a video, the music will pause, and vice versa. This is why iPadOS doesn’t know how to deal with recording audio while also outputting in another VoIP app. Fourteen years on, I find it hard to believe that an iPad has to be limited to playing only one source of audio at a time.</p>
<h2 id="multitasking-a-fractured-mess">Multitasking: A Fractured Mess</h2>
<p id="p49">Where do we even begin here?</p>
<p id="p50">There is no better definition of uneven development than the iPad’s history with multitasking interfaces. As my <a href="https://www.macstories.net/stories/ios-11-the-macstories-review/17/#split-view" rel="noopener noreferrer">annual</a> <a href="https://www.macstories.net/stories/ios-and-ipados-13-the-macstories-review/21/#multitasking-and-multiwindow" rel="noopener noreferrer">reviews</a> <a href="https://www.macstories.net/stories/ios-and-ipados-15-the-macstories-review/6/#multitasking" rel="noopener noreferrer">can</a> <a href="https://www.macstories.net/stories/ios-and-ipados-17-the-macstories-review/6/#stage-manager" rel="noopener noreferrer">confirm</a>, Apple has shown a tendency to release a new iPad multitasking UI every 2-3 years, never refine it, and discard it to make room for another iteration that starts the cycle anew.</p>
<p id="p51">If Apple followed the same approach with the Mac since its launch in 1984, we’d have 13 different multitasking methods today. Instead, macOS has largely been built around the one freeform multi-window approach, with full-screen mode and Stage Manager tacked onto the system over the past decade.</p>
<p id="p52">Stage Manager is Apple’s latest attempt to bring a more flexible and desktop-like multitasking environment to the iPad. As I explained nearly two years ago when it launched, its <a href="https://www.macstories.net/stories/stage-manager-ipados-16-1-review/" rel="noopener noreferrer">original release was a disaster</a>. Apple’s first take on Mac-like multiwindowing was riddled with bugs and performance issues, which, for a while, forced me to revert to traditional multitasking via Split View and Slide Over.</p>

<p id="p54">Fortunately, with last year’s iPadOS 17, Apple <a href="https://www.macstories.net/stories/with-ipados-17-stage-manager-is-finally-moving-in-the-right-direction/" rel="noopener noreferrer">cleaned up</a> several of Stage Manager’s initial stability woes and inconsistencies while also giving pro users more freedom in terms of window placement and touch-based controls. However, Apple’s efforts last year only improved the basic functionality of Stage Manager, leaving several requests unanswered and letting Stage Manager’s foundation languish without the additional flexibility that one would expect from a pro-oriented feature.</p>
<p id="p55">To name a few:</p>
<ul id="ul56"><li><strong>Stage Manager is still limited to four windows at once.</strong> Despite the iPad Pro becoming more and more powerful over time (to the point that the latest iPad Pros are now one M-chip generation ahead of MacBooks), Stage Manager still forces you to work with only four windows shown on-screen at once. Imagine if a 13-inch MacBook Air could only let you see four windows at the same time. And no, Stage Manager for Mac doesn’t have this limitation, proving that – <a href="https://en.wikipedia.org/wiki/System_on_a_chip" rel="noopener noreferrer">SoCs</a> and screen sizes being equal – Apple’s architecture is more than capable of going beyond four simultaneous windows.</li>
<li><strong>It’s still impossible to create presets for often-used window combinations.</strong> iPadOS still doesn’t offer a way to save frequently-used app combinations as “presets” or “pairs” that you can recreate with one click. Shortcuts could be a way to address this limitation, but Shortcuts for iPadOS doesn’t integrate with Stage Manager at all. Android has offered this functionality on phones for several years now. With Stage Manager for iPad, pro users have to manually recreate their workspaces from scratch every time.</li>
<li><strong>Sometimes the wrong window still remains active and receives keyboard input.</strong> This is a bug that keeps getting in the way of my ability to get work done on a daily basis. Sometimes I have, say, Safari and <a href="https://obsidian.md/" rel="noopener noreferrer">Obsidian</a> windows next to each other in Stage Manager, and I <em>think</em> I’ve selected Safari. But when I invoke a specific keyboard shortcut, text input is received by Obsidian’s text editor instead. Imagine if a Mac did this.</li>
<li><strong>It’s still unclear how to spawn a new window for the frontmost app, or how to see all windows from the current app.</strong> On a Mac, if you’re working with a window in front of you (regardless of whether you’re using Stage Manager or not), you have a consistent, universal way of creating a new window for the current app: the menu bar’s File ⇾ New Window menu. On iPad, this is not the case. There is no system-wide keyboard shortcut to create a new window for the app you’re currently using; to create a new window (and only in apps that support multiple windows), you have to long-press its icon in the dock (<em>if</em> you have that app in your dock), select ‘Show All Windows’, and only then can you press a ‘+’ button to create a new window for the app…in a separate workspace. All of these interactions need to be faster, simpler, and better-presented to users instead of being tucked away within long-press menus.</li>
<li><strong>There is no way to quickly preview all windows in the current workspace.</strong> Similarly, iPadOS doesn’t provide users with an easy-to-use, fast way of previewing all windows in the current workspace. The Mac has long offered <a href="https://support.apple.com/lt-lt/guide/mac-help/mh35798/mac" rel="noopener noreferrer">Mission Control</a> as a system to get a bird’s eye view of windows on your desktop, and it works with both Stage Manager and traditional multiwindowing. On the iPad, none of this exists.</li>
</ul><p id="p57">I don’t think any of these features fall under the hypothetical umbrella of “Mac features you shouldn’t wish for on an iPad”. If Apple sells a product called “iPad Pro” that supports desktop-class apps with desktop-class multitasking, that experience should also be, well, desktop-class. But it’s not. Apple did the bare minimum work for Stage Manager, “fixed” it last year to make it at least passable, and never considerably improved it since launch – which, in many ways, is the story of iPad multitasking so far.</p>
<p id="p58">I insist on covering this aspect of the iPad experience because Apple has followed the same path with Split View and Slide Over, which haven’t received substantial updates in years, either. And I know for a fact that, if only time and resources were devoted to it, it wouldn’t have to be this way.</p>
<p id="p59">For a few months earlier this year, I spent some time with a <a href="https://www.oneplus.com/global/open" rel="noopener noreferrer">OnePlus Open</a> foldable. The device is obviously not as pleasant as an iPad, and I fundamentally dislike the Android app ecosystem, but believe me when I say that the folks at OnePlus (objectively, a smaller company than Apple) have shipped a vastly better tiling interface for split-screen multitasking than Apple.</p>
<p id="p60">OnePlus’ ‘Open Canvas’ multitasking UI has several advantages over the iPad’s Split View system. For starters, you can create saved app pairs (which you can pin to your Home Screen), and the same app window can exist either in an app pair or standalone without having to create a new window for the app. This is a major difference from iPadOS’ architecture, where a window can only exist as an “object” in one place at a time. But it goes deeper than that. To take advantage of the Open’s larger screen when unfolded, the Canvas UI lets you add up to three tiles on-screen, and the system automatically zooms out and reflows to either show you a three-column interface or a fluid tiled arrangement with two windows above and one larger window below. It’s incredibly clever and intuitive in practice, and I recommend checking out this video to get a sense of what it looks like:</p>
<p><span><iframe type="text/html" width="640" height="360" src="https://www.youtube.com/embed/1sdA-I9x9R0?version=3&amp;rel=1&amp;fs=1&amp;autohide=2&amp;showsearch=0&amp;showinfo=1&amp;iv_load_policy=1&amp;wmode=transparent" allowfullscreen="true"></iframe></span></p>
<p id="p62">All this to say: iPadOS’ multitasking, even without Stage Manager, could be so much more. I think several iPad users (and I was guilty of this, too) have convinced themselves due to Apple’s pace of updates that we’ve reached the peak of what tablet multitasking should do with Split View and Stage Manager. But look outside Apple’s stance on iPadOS, and you see that is not the case. Once again, I’m not arguing for macOS features on the iPad; I’m saying that, if Apple wanted to, it could design innovative, high-performance, delightful <strong>tablet-first</strong> multitasking systems. Sadly, iPad multitasking tells a very different story.</p>
<p id="p63">I could then go on and mention the fact that iPadOS doesn’t offer APIs or sandboxing exceptions for third-party apps to enhance multitasking (like <a href="https://www.fadel.io/missioncontrolplus" rel="noopener noreferrer">they can on a Mac</a>), but that’s a step too far given the circumstances. Serious iPadOS multitasking changes need to happen from the top down; before wishing for third parties to be able to modify and augment multitasking, we need Apple to address the basics first.</p>
<h2 id="spotlight">Spotlight</h2>
<p id="p64">Speaking of the basics, we need to talk about Spotlight for iPadOS.</p>
<p id="p65">Any respectable, modern computer – especially one from Apple – should have a fast and reliable app launcher. Spotlight for iPad, despite some minor improvements over the last few iPadOS releases, still isn’t that.</p>
<p id="p66">As has been <a href="https://twitter.com/gruber/status/1124427793226510337" rel="noopener noreferrer">documented</a> over the years, the iPad’s version of Spotlight has long been afflicted by slow and inconsistent performance. To this day, sometimes I invoke Spotlight and search for an app that I <em>know</em> I have installed on my iPad, and no results come up. Then I dismiss Spotlight, try again, and the app appears. Other times, Spotlight gets stuck: either I press ⌘ + Space and the search box doesn’t appear, or my keyboard’s arrow keys don’t do anything. Usually, a reboot fixes these issues; sometimes, I have to detach my Magic Keyboard and re-attach it for Spotlight to receive keyboard input again.</p>

<p id="p68">Then there’s the “find my stuff” issue. So many times, I try looking for a document inside an app that supports Spotlight indexing, or perhaps a shortcut from the Shortcuts app, and no results come up. Then I try again after a few hours, and results do appear. It drives me insane, and it’s been like this for years now.</p>

<p id="p70">These issues are compounded by the inability to install Spotlight replacements on iPadOS. Spotlight works well on macOS, but if I don’t like it, I can install a replacement like <a href="https://www.raycast.com/" rel="noopener noreferrer">Raycast</a> or <a href="https://www.alfredapp.com/" rel="noopener noreferrer">Alfred</a> and use a different launcher. The iPad has neither the APIs nor the policies in place for these apps to exist. And so I’m left using a half-baked, inconsistent launcher that mostly encumbers my work, hoping that someday, eventually, it’ll get better.</p>
<h2 id="lack-of-background-processes-and-system-wide-utilities">Lack of Background Processes and System-Wide Utilities</h2>
<p id="p71">And now, allow me to wish for one Mac-like feature that is not even that esoteric if you think about it.</p>
<p id="p72">iPadOS needs to gain support for executing long-running, complex tasks in the background. I’m not referring to Background App Refresh, which is the system that lets apps stay active in short bursts in the background to receive push notifications and other updates. I’m talking about the ability to tap into the power of the M-series chips and the iPad’s RAM to keep specific tasks running in the background while you’re doing something else.</p>
<p id="p73">You don’t need to look far to see where iPadOS is failing in this regard. If you use Apple’s own <a href="https://apps.apple.com/us/app/final-cut-pro-for-ipad/id1631624924" rel="noopener noreferrer">Final Cut Pro for iPad</a> – one of the company’s very showcases of the new iPad Pro – and begin exporting a video, then switch apps for even a <em>second</em>, the export is canceled. If you simply switch workspaces in Stage Manager or accidentally click on an incoming notification, an <a href="https://joe-steel.com/#but-the-pro-apps" rel="noopener noreferrer">entire project’s export will fail</a>:</p>

<p id="p75">From a computer that now comes with the M4 and, in certain configurations, 16&nbsp;GB of RAM, this is absurd. Mac laptops with far less impressive specs have been able to keep tasks running in the background for decades. The iPad, now fourteen years old, can’t.</p>
<p id="p76">This limitation extends beyond the realm of pro apps such as Final Cut or Logic and applies to all kind of software that, due to the nature of iPadOS, is impossible to find on the platform. Because of a mix of technical limitations and policy decisions, it’s still impossible for an application that wants to perform something in the background to exist on iPadOS. From clipboard managers and video encoders to automation utilities and AI-based photo editors, if you want to run a time-consuming task in the background on iPad, you’re out of luck.</p>
<p id="p77">As a result, not only have these limitations fostered an environment in which third-party developers are actively discouraged from bringing true desktop-class experiences to iPad, but existing iPad apps still largely feel like blown-up versions of their iPhone counterparts. After all, if the same limitations are shared between a phone and tablet, what’s an iPad app but an iPhone version dressed up with a larger layout?</p>

<p id="p79">iPadOS’ closed, iPhone-like nature has reverberated throughout other parts of the iPad experience. For instance, system-wide utilities can’t currently exist on the platform. Software such as <a href="https://www.macbartender.com/" rel="noopener noreferrer">customization tools</a>, task managers’ <a href="https://culturedcode.com/things/support/articles/2249437/" rel="noopener noreferrer">quick capture windows</a>,  alternative <a href="https://matthewpalmer.net/rocket/" rel="noopener noreferrer">emoji pickers</a>, and drag-and-drop “shelf” <a href="https://dropoverapp.com/" rel="noopener noreferrer">utilities</a> – apps that do not execute memory-intensive background tasks but still want to run in the background – simply can’t exist on iPadOS.</p>
<p id="p81">Does it have to be this way? Apple is well within their rights to choose how they want iPadOS to behave. But when the operating system powers a computer that is ostensibly sold as a laptop replacement, one has to wonder if Apple is making the correct decisions.</p>
<h3 id="to-be-fixed-later-this-year-but-only-for-some">To Be Fixed Later This Year, But Only for Some</h3>
<p id="p82">I hear you: some of these problems will be addressed later this year thanks to the <a href="https://commission.europa.eu/strategy-and-policy/priorities-2019-2024/europe-fit-digital-age/digital-markets-act-ensuring-fair-and-open-digital-markets_en" rel="noopener noreferrer">Digital Markets Act</a> in Europe, which has been <a href="https://www.macstories.net/news/the-eu-pulls-ipados-into-the-dma-fray/" rel="noopener noreferrer">extended to apply to iPads as well</a>.</p>
<p id="p83">I’m sure that, later in 2024, we’ll see the likes of <a href="https://rileytestut.com/blog/2020/06/17/introducing-clip/" rel="noopener noreferrer">Clip</a> and other side-loaded apps come to iPadOS in Europe. And it’s also obvious that we’ll receive support for non-WebKit browsers to replace Safari. Besides the fact that these changes are going to be limited to Europe, they also aren’t tied to deeper, global policy-related changes on iPadOS. The issues I’ve outlined so far, and the others I’ll continue highlighting below, don’t indicate that Apple is willing to open up the foundation of iPadOS to allow for more flexible, desktop-class computing. These DMA-related changes just mean that Apple is complying with the law and covering the essential functionalities requested by the European Commission.</p>
<p id="p84">Hiding behind the DMA as a hopeful force for structural iPadOS changes is a farce. At the core of the problem lies Apple’s reticence to take iPadOS to the next level, and the DMA has nothing to do with it. The change, once again, needs to happen from the top down.</p>
<h2 id="inefficiency-by-a-thousand-cuts">Inefficiency by a Thousand Cuts</h2>
<p id="p85">If you’ve used iPadOS long enough (the iPad has been my primary computer for 12 years now), I’m sure you’ve run into these: the small bugs, annoyances, and missing features that don’t seem like much in isolation. Considered as a whole, however, they paint a not-too-rosy picture for an operating system that, 14 years into its existence, still lags behind macOS in terms of basic functionalities and problems that have never been addressed. Let me mention just a few examples.</p>
<p id="p86">iPadOS still doesn’t support third-party backup tools or a built-in one like <a href="https://support.apple.com/en-us/104984" rel="noopener noreferrer">Time Machine</a>. If you’re one of those professional users whom Apple caters to, such as a photographer or YouTuber, you know how painful this is. If you want to back up your work, which likely spans multiple terabytes of storage these days, you have two solutions on iPad:</p>
<ul id="ul87"><li>Back up your work manually.</li>
<li>Rely on iCloud backups.</li>
</ul><p id="p88">Sure, you could use iCloud backup, and, as long as your work archive does not exceed 12 TBs, and assuming the Files app works for you, you’ll be okay. But we’d be kidding ourselves if we thought that an online-only backup system with no support for local or off-site storage was the answer for folks whose livelihoods depend on preserving the files they create. Where is Time Machine for iPadOS with support for versioning? Where are third-party APIs to allow for tools like <a href="https://www.backblaze.com/cloud-backup/personal" rel="noopener noreferrer">Backblaze</a> or <a href="https://bombich.com/" rel="noopener noreferrer">Carbon Copy Cloner</a> to exist on iPad?</p>

<p id="p90">We’ve gotten used to this idea that such tools “shouldn’t exist” on iPad because the iPad was meant to be different from the Mac. But then again, we can’t have it both ways: we can’t celebrate the <a href="https://www.apple.com/newsroom/2023/05/apple-brings-final-cut-pro-and-logic-pro-to-ipad/" rel="noopener noreferrer">arrival</a> of Mac apps such as Final Cut and Logic on iPad while also glossing over the lack of flexibility that professional users have on Apple’s other platform. That is, unless we want to live with a delusional idea of “professionals” only needing to back up their files on a Mac because nothing could ever happen when they do the same work on an iPad.</p>
<p id="p91">Platforms may differ, but work is work. You can’t be all loosey-goosey about work only when it’s convenient for your theory that iPadOS should be “lighter”.</p>
<p id="p92">I could then mention the lack of clamshell mode, which is another example of Apple getting an iPad functionality <em>almost</em> right without finishing it. The company rolled out support for external displays with Stage Manager two years ago, and that’s been a fantastic addition to the platform, allowing people to connect an iPad to a monitor and double their workspace. The iPad Pro supports Thunderbolt 4, which implicitly supports <a href="https://www.displayport.org/displayport-over-usb-c/" rel="noopener noreferrer">DisplayPort over USB-C</a>, making the iPad compatible with a wide range of third-party displays too. It’s great.</p>
<p id="p93">The problem is that if you want to use an iPad with an external display at a desk, you have to keep it open and unlocked. Even if you’ve paired an iPad with an external keyboard and mouse or trackpad, you can’t just close the Magic Keyboard on top of the iPad and assume that the external display connection will continue working. This, of course, is not ideal; it forces you to always keep the iPad open somewhere on your desk, with its display turned on and accessible because – this is the worst part – some functionalities such as Control Center cannot be used on the secondary monitor at all.</p>

<p id="p95">Like so many other iPad workflows, there are <a href="https://www.macstories.net/notes/faking-clamshell-mode-with-external-displays-in-ipados-17/" rel="noopener noreferrer">workarounds</a>, but they’re suboptimal, and they’re no replacement for the real functionality that Apple should ship – a functionality that, as you may imagine, has been working fine for ages on the Mac. I’m sure that someone out there will spin this as, “Actually, I like that I need to keep my iPad open”, but I don’t buy it.</p>
<p id="p96">I could then maybe mention how the Home Screen still doesn’t let you place icons freely anywhere you want, or pin specific folders and files for quick access, thus feeling like an enlarged version of the iOS Home Screen. Or maybe I should cover the <a href="https://mastodon.macstories.net/@viticci/110617100820329640" rel="noopener noreferrer">floating keyboard “thingy”</a>, that button that routinely gets in the way of text fields and other UI elements when you’re typing inside apps with a Magic Keyboard. Perhaps I should note that sometimes the Magic Keyboard’s pointer gets stuck, requiring a reboot of the iPad itself?</p>
<p id="p97">You get the idea.</p>
<h2 id="the-need-for-change">The Need for Change</h2>

<p id="p99">You know what’s equally the best and worst part of all this? That I still love the iPad.</p>
<p id="p100">The iPad is the only Apple computer that genuinely feels made for someone like me – a person who loves <a href="https://www.macstories.net/stories/modular-computer/" rel="noopener noreferrer">modularity</a>, freedom, and the mix of touch and keyboard interactions. I share my frustrations because I <em>care</em> about the platform and want it to get better. But at the same time, we need to face reality: the iPad’s operating system isn’t improving at the speed the hardware deserves – that iPad owners who spent thousands of dollars on these machines <em>deserve</em>.</p>
<p id="p101">Something needs to change.</p>
<p id="p102">With <a href="https://www.macstories.net/news/apple-announces-new-11-and-13-ipad-pros/" rel="noopener noreferrer">new iPad Pros</a> nearly upon us, it’s time to admit that iPadOS is not an operating system of the same caliber as Apple’s new hardware. iPadOS has been the victim of erratic updates over the years, with features that were meant to “reimagine” desktop computing only to get not even halfway there and be left to languish for years. Once again, I am <em>not</em> suggesting that the solution is to put macOS on iPad and call it a day. I’m saying that if that’s not in the cards, then Apple should consider all the ways iPadOS is still failing at basic computing tasks. I’d be okay with iPads running iPadOS forever. But if we passively accept that this is as good as an iPad can get, I strongly believe that we’ll play a role in letting Apple squander the greatest computer form factor they’ve ever created.</p>
<p id="p104">I’m tired of hearing apologies that smell of <a href="https://en.wikipedia.org/wiki/Stockholm_syndrome" rel="noopener noreferrer">Stockholm syndrome</a> from iPad users who want to invalidate these opinions and claim that everything is perfect. I’m tired of seeing this cycle start over every two years, with fantastic iPad hardware and the usual (justified), <em>“But it’s the software…”</em> <a href="https://512pixels.net/2024/05/the-problems-never-the-hardware/" rel="noopener noreferrer">line at the end</a>. I’m tired of feeling like my computer is a second-class citizen in Apple’s ecosystem. I’m tired of being told that iPads are perfectly fine if you use Final Cut and Logic, but if you don’t use those apps and ask for more desktop-class features, you’re a weirdo, and you should just get a Mac and shut up. And I’m tired of seeing the best computer Apple ever made not live up to its potential.</p>
<p id="p105">I started using the iPad as my main computer when I was stuck in a hospital bed and couldn’t use a laptop. I kept using it because once you get a taste of that freedom, it’s hard to go back. I will continue using it because none of the alternatives match Apple’s hardware quality, app ecosystem, and pure <strong>delight</strong>. But loving something doesn’t mean ignoring its flaws. And iPadOS is a flawed operating system that still doesn’t get the basics right and, as a result, drags down the entire product line.</p>
<p id="p106">I’m looking forward to the new iPad Pros, but I can’t shake the feeling that the same old iPadOS cycle is about to begin all over again.</p>

            </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Which cognitive psychology findings are solid that I can use to help students? (110 pts)]]></title>
            <link>https://matheducators.stackexchange.com/questions/27839/which-cognitive-psychology-findings-are-solid-that-i-can-use-to-help-my-student</link>
            <guid>40348986</guid>
            <pubDate>Mon, 13 May 2024 21:57:48 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://matheducators.stackexchange.com/questions/27839/which-cognitive-psychology-findings-are-solid-that-i-can-use-to-help-my-student">https://matheducators.stackexchange.com/questions/27839/which-cognitive-psychology-findings-are-solid-that-i-can-use-to-help-my-student</a>, See on <a href="https://news.ycombinator.com/item?id=40348986">Hacker News</a></p>
<div id="readability-page-1" class="page"><div itemprop="text">
<p>There's a highly upvoted answer here claiming that practically no cognitive psychology findings hold up in replication. I don't think that's true at all. Sure, many findings don't hold up, but also, many findings <em>do.</em></p>
<p>For instance: we know that actively solving problems produces more learning than passively watching a video/lecture or re-reading notes. This sort of thing has been tested scientifically, numerous times, and it is completely replicable. It might as well be a law of physics at this point. In fact, a <a href="https://www.pnas.org/doi/10.1073/pnas.1319030111" rel="noreferrer">highly-cited meta-analysis</a> states, verbatim:</p>
<blockquote>
<p>"...[C]alls to increase the number of students receiving STEM degrees could be answered, at least in part, by abandoning traditional lecturing in favor of active learning. … Given our results, it is reasonable to raise concerns about the continued use of traditional lecturing as a control in future experiments."</p>
</blockquote>
<p>So there you go, that's one cognitive psychology finding that holds up: <em>active learning beats passive learning.</em></p>
<p>Another finding: <em>if you don't review information, you forget it.</em> You can actually model this precisely, mathematically, using a <a href="https://en.wikipedia.org/wiki/Forgetting_curve" rel="noreferrer">forgetting curve</a>. I'm not exaggerating when I refer to these things as laws of physics -- the only real difference is that we've gone up several levels of scale and are dealing with noisier stochastic processes.</p>
<p>Okay, but aren't these obvious? Yes, but...</p>
<ul>
<li><p>Yes, but in education, obvious strategies often aren't put into practice. For instance, plenty of classes that still run on a pure lecture format and don't review previously learned unless it's the day before a test.</p>
</li>
<li><p>Yes, but there are plenty of other findings that replicate just as well but are not so obvious.</p>
</li>
</ul>
<p><strong>Here are some less obvious findings.</strong></p>
<ul>
<li>The <a href="https://en.wikipedia.org/wiki/Spacing_effect" rel="noreferrer">spacing effect</a>: more long-term retention occurs when you space out your practice, <em>even if it's the same amount of total practice.</em> As researcher Doug Rohrer <a href="http://uweb.cas.usf.edu/%7Edrohrer/pdfs/Rohrer2009JRME.pdf" rel="noreferrer">states</a>:</li>
</ul>
<blockquote>
<p>"...[T]he spacing effect is arguably one of the largest and most robust findings in learning research, and it appears to have few constraints."</p>
</blockquote>
<ul>
<li><p><em><strong>Note:</strong> There are tons of more detailed scientific references/quotes I want to include, but I'm going to skip them so not to continue blowing up the length of this already-gigantic answer. If you want to see them, <a href="https://www.justinmath.com/books/#the-math-academy-way" rel="noreferrer">here's</a> a draft I'm working on that covers all these findings (and more) with over 300 references and relevant quotes pulled out of those references.</em></p>
</li>
<li><p>A profound consequence of the spacing effect is that the more reviews are completed (with appropriate spacing), the longer the memory will be retained, and the longer one can wait until the next review is needed. This observation gives rise to a systematic method for reviewing previously-learned material called <a href="https://en.wikipedia.org/wiki/Spaced_repetition" rel="noreferrer">spaced repetition</a> (or <a href="https://en.wikipedia.org/wiki/Distributed_practice" rel="noreferrer">distributed practice</a>). A "repetition" is a successful review at the appropriate time.</p>
</li>
<li><p>To maximize the amount by which your memory is extended when solving review problems, it's necessary to avoid looking back at reference material unless you are totally stuck and cannot remember how to proceed. This is called the <a href="https://en.wikipedia.org/wiki/Testing_effect" rel="noreferrer">testing effect</a>, also known as the retrieval practice effect: the best way to review material is to test yourself on it, that is, practice retrieving it from memory, unassisted.</p>
</li>
<li><p>During review, it's also best to spread minimal effective doses of practice across various skills. This is known as <a href="https://en.wikipedia.org/wiki/Varied_practice" rel="noreferrer">mixed practice</a> -- it's the opposite of "blocked" practice, which involves extensive consecutive repetition of a single skill. Blocked practice can give a false sense of mastery and fluency because it allows students to settle into a robotic rhythm of mindlessly applying one type of solution to one type of problem. Mixed practice, on the other hand, creates a "desirable difficulty" that promotes vastly superior retention and generalization, making it a more effective review strategy.</p>
</li>
<li><p>To free up mental processing power, it's critical to practice low-level skills enough that they can be carried out without requiring conscious effort. This is known as <a href="https://en.wikipedia.org/wiki/Automaticity" rel="noreferrer">automaticity</a>. Think of a basketball player who is running, dribbling, and strategizing all at the same time -- if they had to consciously manage every bounce and every stride, they'd be too overwhelmed to look around and strategize. The same is true in math. I wrote more about the importance of automaticity in a recent answer <a href="https://matheducators.stackexchange.com/a/27786/22672">here</a>.</p>
</li>
<li><p>Instructional techniques that promote the most learning in experts, promote the least learning in beginners, and vice versa. This is known as the <a href="https://en.wikipedia.org/wiki/Expertise_reversal_effect" rel="noreferrer">expertise reversal effect</a>. An important consequence is that effective methods of practice for students typically should <em>not</em> emulate what experts do in the professional workplace (e.g., working in groups to solve open-ended problems).</p>
</li>
</ul>
<p><strong>Why haven't these findings transformed education?</strong></p>
<p>In Daniel R. Collins' answer, he states <em>"if there was some magic solution, it would have been implemented large-scale very quickly."</em></p>
<p>That raises the question: if cognitive psychology has found many effective learning strategies (like <a href="https://en.wikipedia.org/wiki/Mastery_learning" rel="noreferrer">mastery learning</a>, <a href="https://en.wikipedia.org/wiki/Spaced_repetition" rel="noreferrer">spaced repetition</a>, <a href="https://en.wikipedia.org/wiki/Testing_effect" rel="noreferrer">the testing effect</a>, and <a href="https://en.wikipedia.org/wiki/Varied_practice" rel="noreferrer">varied practice</a>), then why haven't these learning strategies been implemented large-scale?</p>
<p>Here are a handful of reasons that I'm aware of.</p>
<p><em>1. Leveraging them (at all) requires additional effort from both teachers and students.</em></p>
<p>In some way or another, each strategy increases the intensity of effort required from students and/or instructors, and the extra effort is then converted into an outsized gain in learning.</p>
<p>This theme is so well-documented in the literature that it even has a catchy name: a practice condition that makes the task harder, slowing down the learning process yet improving recall and transfer, is known as a <a href="https://en.wikipedia.org/wiki/Desirable_difficulty" rel="noreferrer">desirable difficulty</a>.</p>
<p>Desirable difficulties make practice more representative of true assessment conditions. Consequently, it is easy for students (and their teachers) to vastly overestimate their knowledge if they do not leverage desirable difficulties during practice, a phenomenon known as the <em>illusion of comprehension.</em></p>
<p>However, the typical teacher is incentivized to maximize the immediate performance and/or happiness of their students, which biases them against introducing desirable difficulties and incentivizes them to promote illusions of comprehension.</p>
<p>Using desirable difficulties exposes the reality that students didn't actually learn as much as they (and their teachers) "felt" they did under less effortful conditions. This reality is inconvenient to students and teachers alike; therefore, it is common to simply believe the illusion of learning and avoid activities that might present evidence to the contrary.</p>
<p><em>2. Leveraging cognitive learning strategies to their fullest extent requires an inhuman amount of effort from teachers.</em></p>
<p>Let's imagine a classroom where these strategies are being used to their fullest extent.</p>
<ul>
<li><p>Every individual student is fully engaged in productive problem-solving, with immediate feedback (including remedial support when necessary), on the specific types of problems, and in the specific types of settings (e.g., with vs without reference material, blocked vs interleaved, timed vs untimed), that will move the needle the most for their personal learning progress at that specific moment in time.</p>
</li>
<li><p>This is happening throughout the entirety of class time, the only exceptions being those brief moments when a student is introduced to a new topic and observes a worked example before jumping into active problem-solving.</p>
</li>
</ul>
<p>Why is this an inhuman amount of work?</p>
<ul>
<li><p>First of all, it's at best extremely difficult, and at worst (and most commonly) impossible, to find a type of problem that is productive for all students in the class. Even if a teacher chooses a type of problem that is appropriate for what they perceive to be the "class average" knowledge profile, it will typically be too hard for many students and too easy for many others (an unproductive use of time for those students either way).</p>
</li>
<li><p>Additionally, to even know the specific problem types that each student needs to work on, the teacher has to separately track each student's progress on each problem type, manage a spaced repetition schedule of when each student needs to review each topic, and continually update each schedule based on the student's performance (which can be incredibly complicated given that each time a student learns or reviews an advanced topic, they're implicitly reviewing many simpler topics, all of whose repetition schedules need to be adjusted as a result, depending on how the student performed). This is an inhuman amount of bookkeeping and computation.</p>
</li>
<li><p>Furthermore, even on the rare occasion that a teacher manages to find a type of problem that is productive for all students in the class, different students will require different amounts of practice to master the solution technique. Some students will catch on quickly and be ready to move on to more difficult problems after solving just a couple problems of the given type, while other students will require many more attempts before they are able to solve problems of the given type successfully on their own. Additionally, some students will solve problems quickly while others will require more time.</p>
</li>
</ul>
<p>In the absence of the proper technology, it is impossible for a single human teacher to deliver an optimal learning experience to a classroom of many students with heterogeneous knowledge profiles, who all need to work on different types of problems and receive immediate feedback on each attempt.</p>
<p><em>3. Most edtech systems do not actually leverage the above findings.</em></p>
<p>If you pick any edtech system off the shelf and check whether it leverages each of the cognitive learning strategies I've described above, you'll probably be surprised at how few it actually uses. For instance:</p>
<ul>
<li><p>Tons of systems don't scaffold their content into bite-sized pieces.</p>
</li>
<li><p>Tons of systems allow students to move on to more material despite not demonstrating knowledge of prerequisite material.</p>
</li>
<li><p>Tons of systems don't do spaced review. (Moreover, tons of systems don't do <em>any</em> review.)</p>
</li>
</ul>
<p>Sometimes a system will appear to leverage some finding, but if you look more closely it turns out that this is actually an illusion that is made possible by cutting corners somewhere less obvious. For instance:</p>
<ul>
<li><p>Tons of systems offer bite-sized pieces of content, <em>but</em> they accomplish this by watering down the content, cherry-picking the simplest cases of each problem type, and skipping lots of content that would reasonably be covered in a standard textbook.</p>
</li>
<li><p>Tons of systems make students do prerequisite lessons before moving on to more advanced lessons, <em>but</em> they don't actually measure tangible mastery on prerequisite lessons. Simply watching a video and/or attempting some problems is <em>not</em> mastery. The student has to actually be getting problems right, and those problems have to be representative of the content covered in the lesson.</p>
</li>
<li><p>Tons of systems claim to help students when they're struggling, <em>but</em> the way they do this is by lowering the bar for success on the learning task (e.g., by giving away hints). Really, what the system needs to do is take actions that are most likely to strengthen a student's area of weakness and empower them to clear the bar fully and independently on their next attempt.</p>
</li>
</ul>
<p>Now, I'm not saying that these issues apply to all edtech systems. I <em>do</em> think edtech is the way forward here -- optimal teaching is an inhuman amount of work, and technology is needed. Heck, I personally developed all the quantitative software behind one <a href="https://mathacademy.com/" rel="noreferrer">system</a> that properly handles the above challenges. All I'm saying is that you can't just take these things at face value. Many edtech systems don't really work from a learning standpoint, just as many psychology findings don't hold up in replication -- but at the same time, some edtech systems <em>do</em> work, shockingly well, just as some cognitive psychology findings <em>do</em> hold up and can be leveraged to massively increase student learning.</p>
<p><em>4. Even if you leverage the above findings, you still have to hold students accountable for learning.</em></p>
<p>Suppose you have the Platonic ideal of an edtech system that leverages all the above cognitive learning strategies to their fullest extent.</p>
<p>Can you just put a student on it and expect them to learn? Heck no! That would only work for exceptionally motivated students.</p>
<p>Most students are not motivated to learn the subject material. They need a responsible adult -- such as a parent or a teacher -- to incentivize them and hold them accountable for their behavior.</p>
<p>I can't tell you how many times I've seen the following situation play out:</p>
<ul>
<li><p>Adult puts a student on an edtech system.</p>
</li>
<li><p>Student goofs off doing other things instead (e.g., watching YouTube).</p>
</li>
<li><p>Adult checks in, realizes the student is not accomplishing anything, and asks the student what's going on.</p>
</li>
<li><p>Student says that the system is too hard or otherwise doesn't work.</p>
</li>
<li><p>Adult might take the student's word at face value. Or, if the adult notices that the student hasn't actually attempted any work and calls them out on it, the scenario repeats with the student putting forth as little effort as possible -- enough to convince the adult that they're trying, but not enough to really make progress.</p>
</li>
</ul>
<p>In these situations, here's what needs to happen:</p>
<ul>
<li><p>The adult needs to sit down next to the student and force them to actually put forth the effort required to use the system properly.</p>
</li>
<li><p>Once it's established that the student is able to make progress by putting forth sufficient effort, the adult needs to continue holding the student accountable for their daily progress. If the student ever stops making progress, the adult needs to sit down next to the student again and get them back on the rails.</p>
</li>
<li><p>To keep the student on the rails without having to sit down next to them all the time, the adult needs to set up an incentive structure. Even little things go a long way, like <em>"if you complete all your work this week then we'll go get ice cream on the weekend,"</em> or <em>"no video games tonight until you complete your work."</em> The incentive has to be centered around something that the student actually cares about, whether that be dessert, gaming, movies, books, etc.</p>
</li>
</ul>
<p>Even if an adult puts a student on an edtech system that is truly optimal, if the adult clocks out and stops holding the student accountable for completing their work every day, then of course the overall learning outcome is going to be worse.</p>
<p><strong>Connecting to mechanics within the brain</strong></p>
<p>Before ending this answer, I want to drive home the point that the cognitive learning strategies discussed here really do connect all the way down to the mechanics of what's going on in the brain.</p>
<p>The goal of mathematical instruction is to increase the quantity, depth, retrievability, and generalizability of mathematical concepts and skills in the student's <a href="https://en.wikipedia.org/wiki/Long-term_memory" rel="noreferrer">long-term memory (LTM)</a>.</p>
<p>At a physical level, that amounts to creating strategic connections between neurons so that the brain can more easily, quickly, accurately, and reliably activate more intricate patterns of neurons. This process is known as <a href="https://en.wikipedia.org/wiki/Memory_consolidation" rel="noreferrer">consolidation</a>.</p>
<p><em>Now, here's the catch:</em> before information can be consolidated into LTM, it has to pass through <a href="https://en.wikipedia.org/wiki/Working_memory" rel="noreferrer">working memory (WM)</a>, which has severely limited capacity. The brain's working memory capacity (WMC) represents the amount of effort that it can devote to activating neural patterns and persistently maintaining their simultaneous activation, a process known as <a href="https://en.wikipedia.org/wiki/Memory_rehearsal" rel="noreferrer">rehearsal</a>.</p>
<p>Most people can only hold about <a href="https://en.wikipedia.org/wiki/The_Magical_Number_Seven,_Plus_or_Minus_Two" rel="noreferrer">7 digits</a> (or more generally 4 <a href="https://en.wikipedia.org/wiki/Chunking_(psychology)" rel="noreferrer">chunks of coherently grouped items</a>) simultaneously and only for about 20 seconds. And that assumes they aren't needing to perform any mental manipulation of those items – if they do, then fewer items can be held due to competition for limited processing resources.</p>
<p>Limited capacity makes WMC a bottleneck in the transfer of information into LTM. When the cognitive load of a learning task exceeds a student's WMC, the student experiences cognitive overload and is not able to complete the task.</p>
<p>Additionally, different students have different WMC, and those with higher WMC are typically going to find it easier to "see the forest for the trees" by learning underlying rules as opposed to memorizing example-specific details. (This is unsurprising given that understanding large-scale patterns requires balancing many concepts simultaneously in WM.)</p>
<p>It's expected that higher-WMC students will more quickly improve their performance on a learning task over the course of exposure, instruction, and practice on the task. However, once a student learns a task to a sufficient level of performance, the impact of WMC on task performance is diminished because the information processing that's required to perform the task has been transferred into long-term memory, where it can be recalled by WM without increasing the actual load placed on WM.</p>
<p>So, for each concept or skill you want to teach:</p>
<ul>
<li><p>it needs to be introduced after the prerequisites have been learned (so that the prerequisite knowledge can be pulled from long-term memory without taxing WM)</p>
</li>
<li><p>it needs to be broken down into bite-sized pieces small enough that no piece overloads any student's WM</p>
</li>
<li><p>each student needs to be given enough practice to achieve mastery on each piece – and that amount of practice may vary depending on the particular student and the particular learning task.</p>
</li>
</ul>
<p>But also, even if you do all the above perfectly, you still have to deal with <em>forgetting.</em> The representations in LTM gradually, over time, decay and become harder to retrieve if they are not used, resulting in forgetting.</p>
<p>The solution to forgetting is <em>review</em> -- and not just passively re-ingesting information, but actively <em>retrieving</em> it, unassisted, from LTM. Each time you successfully actively retrieve fuzzy information from LTM, you physically refresh and deepen the corresponding neural representation in your brain. But that doesn't happen if you just passively re-ingest the information through your senses instead of actively retrieving it from LTM.</p>
<p><strong>Further Reading</strong></p>
<p>I've written extensively on this. See the working draft <a href="https://www.justinmath.com/books/#the-math-academy-way" rel="noreferrer">here</a> for more info and hundreds of scientific citations to back it up.</p>
<p>The citations are from a wide variety of researchers, but there's one researcher in particular who has published a TON of papers relevant to this question/answer in particular, has all (or at least most) of those papers freely available on his personal site, and has a really engaging and "to the point" writing style, so I want to give him a shout-out. His name is Doug Rohrer. You can read his papers here: <a href="http://drohrer.myweb.usf.edu/pubs.htm" rel="noreferrer">drohrer.myweb.usf.edu/pubs.htm</a></p>
<p>Also check out the following:</p>
<ul>
<li><p>Kirschner, P., &amp; Hendrick, C. (2020). <a href="https://www.taylorfrancis.com/books/mono/10.4324/9780429061523/learning-happens-paul-kirschner-carl-hendrick" rel="noreferrer"><em>How Learning Happens: Seminal Works in Educational Psychology and What They Mean in Practice</em></a></p>
</li>
<li><p>Hattie, J., &amp; Yates, G. C. (2013). <a href="https://www.taylorfrancis.com/books/mono/10.4324/9781315885025/visible-learning-science-learn-john-hattie-gregory-yates" rel="noreferrer"><em>Visible Learning and the Science of How We Learn</em></a></p>
</li>
</ul>
<p>In the comments, OpalE suggests another resource that I agree is worth checking out: <a href="https://www.learningscientists.org/" rel="noreferrer">learningscientists.org</a></p>
    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Unitree G1 Humanoid Agent (138 pts)]]></title>
            <link>https://www.unitree.com/g1/</link>
            <guid>40348531</guid>
            <pubDate>Mon, 13 May 2024 21:14:54 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.unitree.com/g1/">https://www.unitree.com/g1/</a>, See on <a href="https://news.ycombinator.com/item?id=40348531">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-v-1f53478b=""><p data-v-1f53478b="">[1] The maximum torque of the joint motors of the whole machine is different. This is the maximum torque of the largest joint motor among them.</p><p data-v-1f53478b="">[2] The maximum load of the arm varies greatly under different arm extension postures.</p><p data-v-1f53478b="">[3] For more information, please read the secondary development manual.</p><p data-v-1f53478b="">[4] For more detailed warranty terms, please read the product warranty brochure.</p><p data-v-1f53478b="">[5] The above parameters may vary in different scenarios and configurations, please subject to actual situation.</p><p data-v-1f53478b="">[6] The humanoid robot has a complex structure and extremely powerful power. Users are asked to keep a sufficient safe distance between the humanoid robot and the humanoid robot.Please use with caution</p><p data-v-1f53478b="">[7] If any change in the appearance of the product, please refer to the actual product.</p><p data-v-1f53478b="">[8] Some sample functions on this page are still being developed and tested, and will be opened to users in the future.</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[DOS game "F-15 Strike Eagle II" reverse engineering/reconstruction war stories (163 pts)]]></title>
            <link>https://neuviemeporte.github.io/category/f15-se2.html</link>
            <guid>40347662</guid>
            <pubDate>Mon, 13 May 2024 20:00:30 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://neuviemeporte.github.io/category/f15-se2.html">https://neuviemeporte.github.io/category/f15-se2.html</a>, See on <a href="https://news.ycombinator.com/item?id=40347662">Hacker News</a></p>
<div id="readability-page-1" class="page">
<main aria-label="Content">
      <div>
  <h2>Posts in category : f15-se2</h2>
  <ul>
    
      <li><a href="https://neuviemeporte.github.io/f15-se2/2024/05/05/ghidra.html">Ghidra to the rescue</a></li>
    
      <li><a href="https://neuviemeporte.github.io/f15-se2/2024/01/07/unlink2.html">More delinking fun</a></li>
    
      <li><a href="https://neuviemeporte.github.io/f15-se2/2023/12/30/unlink.html">First steps in delinking</a></li>
    
      <li><a href="https://neuviemeporte.github.io/f15-se2/2023/11/08/imatching2.html">Trying to think like a compiler, Part 2</a></li>
    
      <li><a href="https://neuviemeporte.github.io/f15-se2/2023/10/06/linking.html">Mixed-language linking misadventures</a></li>
    
      <li><a href="https://neuviemeporte.github.io/f15-se2/2023/09/25/reassembly.html">Reassembling the disassembly</a></li>
    
      <li><a href="https://neuviemeporte.github.io/f15-se2/2023/09/02/compiler3.html">Hunting for the Right Compiler, Part 3</a></li>
    
      <li><a href="https://neuviemeporte.github.io/f15-se2/2023/07/13/imatching.html">Trying to think like a compiler</a></li>
    
      <li><a href="https://neuviemeporte.github.io/f15-se2/2023/07/12/overlays.html">Usage of overlays in F15 SE2</a></li>
    
      <li><a href="https://neuviemeporte.github.io/f15-se2/2023/05/18/compiler2.html">Hunting for the Right Compiler, Part 2</a></li>
    
      <li><a href="https://neuviemeporte.github.io/f15-se2/2023/05/17/compiler.html">Hunting for the Right Compiler, Part 1</a></li>
    
      <li><a href="https://neuviemeporte.github.io/f15-se2/2023/03/23/farcalls.html">Chasing far calls</a></li>
    
      <li><a href="https://neuviemeporte.github.io/f15-se2/2022/12/09/reversing-3.html">What does it take to take an old game apart? (Part 3)</a></li>
    
      <li><a href="https://neuviemeporte.github.io/f15-se2/2022/12/09/reversing-2.html">What does it take to take an old game apart? (Part 2)</a></li>
    
      <li><a href="https://neuviemeporte.github.io/f15-se2/2022/12/09/reversing-1.html">What does it take to take an old game apart? (Part 1)</a></li>
    
      <li><a href="https://neuviemeporte.github.io/f15-se2/2022/12/08/firstlook.html">Having a first look around F-15 SE2</a></li>
    
      <li><a href="https://neuviemeporte.github.io/f15-se2/2022/06/05/origins.html">F-15 Strike Eagle II: The origin story</a></li>
    
  </ul>
</div>
    </main>



</div>]]></description>
        </item>
        <item>
            <title><![CDATA[Show HN: Pi-C.A.R.D, a Raspberry Pi Voice Assistant (261 pts)]]></title>
            <link>https://github.com/nkasmanoff/pi-card</link>
            <guid>40346995</guid>
            <pubDate>Mon, 13 May 2024 19:03:55 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/nkasmanoff/pi-card">https://github.com/nkasmanoff/pi-card</a>, See on <a href="https://news.ycombinator.com/item?id=40346995">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><p dir="auto"><h2 tabindex="-1" dir="auto">Pi-C.A.R.D</h2><a id="user-content-pi-card" aria-label="Permalink: Pi-C.A.R.D" href="#pi-card"></a></p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/nkasmanoff/pi-card/blob/main/assets/assistant.png"><img src="https://github.com/nkasmanoff/pi-card/raw/main/assets/assistant.png" height="300"></a></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Table of Contents</h2><a id="user-content-table-of-contents" aria-label="Permalink: Table of Contents" href="#table-of-contents"></a></p>
<ul dir="auto">
<li><a href="#introduction">Introduction</a></li>
<li><a href="#usage">Usage</a></li>
<li><a href="#hardware">Hardware</a></li>
<li><a href="#setup">Setup</a></li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Introduction</h2><a id="user-content-introduction" aria-label="Permalink: Introduction" href="#introduction"></a></p>
<p dir="auto">Pi-card is an AI powered voice assistant running entirely on a Raspberry Pi. It is capable of doing anything a standard LLM (like ChatGPT) can do in a conversational setting.
In addition, if there is a camera equipped, you can also ask Pi-card to take a photo, describe what it sees, and then ask questions about that image.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Why Pi-card?</h3><a id="user-content-why-pi-card" aria-label="Permalink: Why Pi-card?" href="#why-pi-card"></a></p>
<p dir="auto">Raspberry <strong>Pi</strong> - <strong>C</strong>amera <strong>A</strong>udio <strong>R</strong>ecognition <strong>D</strong>evice.</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/nkasmanoff/pi-card/blob/main/assets/picard-facepalm.jpg"><img src="https://github.com/nkasmanoff/pi-card/raw/main/assets/picard-facepalm.jpg" height="300"></a></p>
<p dir="auto">Please submit an issue or pull request if you can think of a better way to force this ackronym.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">How does it work?</h3><a id="user-content-how-does-it-work" aria-label="Permalink: How does it work?" href="#how-does-it-work"></a></p>
<p dir="auto">Pi-card runs entirely on your Raspberry Pi. Once the main program is run, the system will listen for your wake word. Once your wake word has been said, you are officially in a conversation. Within this conversation you do not need to constantly repeat the wake word. The system will continue to listen for your commands until you say something like "stop", "exit", or "goodbye".</p>
<p dir="auto">The system has a memory of the conversation while you have it, meaning if you want the assistant to repeat something it said, or elaborate on a previous topic, you can do so.</p>
<p dir="auto">While the system is designed to be entirely local, it is also possible to easily connect it to some external APIs or services if you want to enhance the conversation, or give it control to some external devices. To do so is something I am open to improving, but for now it will be done based on specific keywords to trigger the external service. A good example of this is that for camera purposes, the system will activate the camera if you say "take a photo" or "what do you see".</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">How useful is it?</h3><a id="user-content-how-useful-is-it" aria-label="Permalink: How useful is it?" href="#how-useful-is-it"></a></p>
<p dir="auto">The system is designed to be a fun project that can be a <em>somewhat</em> helpful AI assistant. Since everything is done locally, the system will not be as capable, or as fast, as cloud based systems. However, the system is still capable of a lot of improvements to be made.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Why isn't this an app?</h3><a id="user-content-why-isnt-this-an-app" aria-label="Permalink: Why isn't this an app?" href="#why-isnt-this-an-app"></a></p>
<p dir="auto">The main reason for this is that I wanted to create a voice assistant that is completely offline and doesn't require any internet connection. This is because I wanted to ensure that the user's privacy is protected and that the user's data is not being sent to any third party servers.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Usage</h2><a id="user-content-usage" aria-label="Permalink: Usage" href="#usage"></a></p>
<p dir="auto">After downloading the repository, installing the requirements, and following the other setup instructions, you can run the main program by running the following command:</p>

<p dir="auto">Once the program is running, you can start a conversation with the assistant by saying the wake word. The default wake word is "hey assistant", but you can change this in the <code>config.py</code> file.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Hardware</h2><a id="user-content-hardware" aria-label="Permalink: Hardware" href="#hardware"></a></p>
<ul dir="auto">
<li>Raspberry Pi 5 Model B</li>
<li>USB Microphone</li>
<li>Speaker</li>
<li>Camera</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Setup</h2><a id="user-content-setup" aria-label="Permalink: Setup" href="#setup"></a></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Software</h3><a id="user-content-software" aria-label="Permalink: Software" href="#software"></a></p>
<p dir="auto">To keep this system as fast and lean as possible, we use cpp implementations of the audio transcription and vision language models. These are done with the wonderful libraries <a href="https://github.com/ggerganov/whisper.cpp">whipser.cpp</a> for the audio transcription and <a href="https://github.com/ggerganov/llama.cpp">llama.cpp</a> for the vision language model.</p>
<p dir="auto">In both cases, please clone these repositories wherever you like, and add their paths to the <code>config.py</code> file.</p>
<p dir="auto">Once cloned, please go to each repository, and follow the setup instructions to get the models running. Some pointers are given below:</p>
<p dir="auto">For llama.cpp, we are using the vision language model capabilities, which are slightly different from the standard setup. You will need to follow the setup instructions for <a href="https://github.com/ggerganov/llama.cpp/blob/master/examples/llava/README.md">LlaVA</a>, but update the model to be used to be one better suited for this device, <a href="https://github.com/nkasmanoff/pi-card/blob/main/moondream.ai">Moondream2</a></p>
<p dir="auto">To install Moondream, you'll need to go to HuggingFace model hub, and download the model. I did so using python, with the following commands. Once again, make sure the vision model path is added to the <code>config.py</code> file.</p>
<div dir="auto" data-snippet-clipboard-copy-content="from huggingface_hub import snapshot_download
model_id=&quot;vikhyatk/moondream2&quot;
snapshot_download(repo_id=model_id, local_dir=your/local/path, local_dir_use_symlinks=False, revision=&quot;main&quot;)"><pre><span>from</span> <span>huggingface_hub</span> <span>import</span> <span>snapshot_download</span>
<span>model_id</span><span>=</span><span>"vikhyatk/moondream2"</span>
<span>snapshot_download</span>(<span>repo_id</span><span>=</span><span>model_id</span>, <span>local_dir</span><span>=</span><span>your</span><span>/</span><span>local</span><span>/</span><span>path</span>, <span>local_dir_use_symlinks</span><span>=</span><span>False</span>, <span>revision</span><span>=</span><span>"main"</span>)</pre></div>
<p dir="auto">For whisper.cpp, you will need to follow the quick-start guide in the <a href="https://github.com/ggerganov/whisper.cpp?tab=readme-ov-file#quick-start">README</a>.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Hardware</h3><a id="user-content-hardware-1" aria-label="Permalink: Hardware" href="#hardware-1"></a></p>
<p dir="auto">The hardware setup is quite simple. You will need a Raspberry Pi 5 Model B, a USB microphone, a speaker, and a camera.</p>
<p dir="auto">The USB microphone and speaker can be plugged into the Raspberry Pi's USB ports. The camera can be connected to the camera port on the Raspberry Pi.</p>
<p dir="auto">I used the following hardware for my setup:</p>
<ul dir="auto">
<li><a href="https://www.amazon.com/dp/B0CRSNCJ6Y?psc=1&amp;ref=ppx_yo2ov_dt_b_product_details" rel="nofollow">Raspberry Pi 5 Kit</a></li>
<li><a href="https://www.amazon.com/dp/B087PTH787?psc=1&amp;ref=ppx_yo2ov_dt_b_product_details" rel="nofollow">USB Microphone</a></li>
<li><a href="https://www.amazon.com/dp/B075M7FHM1?ref=ppx_yo2ov_dt_b_product_details&amp;th=1" rel="nofollow">Speaker</a></li>
<li><a href="https://www.amazon.com/dp/B012V1HEP4?ref=ppx_yo2ov_dt_b_product_details&amp;th=1" rel="nofollow">Camera</a></li>
<li><a href="https://www.amazon.com/dp/B0716TB6X3?psc=1&amp;ref=ppx_yo2ov_dt_b_product_details" rel="nofollow">Camera Connector</a></li>
</ul>
<p dir="auto">Please note Pi 5's have a new camera port, hence the new camera connector.</p>
<p dir="auto">Feel free to use your own, this is what worked for me.</p>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[mRNA cancer vaccine triggers fierce immune response to fight brain tumors (196 pts)]]></title>
            <link>https://ufhealth.org/news/2024/uf-developed-mrna-vaccine-triggers-fierce-immune-response-to-fight-malignant-brain-tumor</link>
            <guid>40346675</guid>
            <pubDate>Mon, 13 May 2024 18:36:04 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://ufhealth.org/news/2024/uf-developed-mrna-vaccine-triggers-fierce-immune-response-to-fight-malignant-brain-tumor">https://ufhealth.org/news/2024/uf-developed-mrna-vaccine-triggers-fierce-immune-response-to-fight-malignant-brain-tumor</a>, See on <a href="https://news.ycombinator.com/item?id=40346675">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
      



    


<main id="maincontent" data-gtm-scope="Content">
  
<div>
      <header data-gtm-scope="ContentHeader">
        



    <div>
            
            

                  <figure>

                  
  
        <picture>
    <source type="image/webp" srcset="https://ufhealth.org/assets/images/stories/_640x426_crop_center-center_line/Brain-Cancer-Vaccine_Feature.jpg.webp 640w, https://ufhealth.org/assets/images/stories/_860x573_crop_center-center_line/Brain-Cancer-Vaccine_Feature.jpg.webp 860w, https://ufhealth.org/assets/images/stories/_1500x1000_crop_center-center_line/Brain-Cancer-Vaccine_Feature.jpg.webp 1500w, https://ufhealth.org/assets/images/stories/_2432x1621_crop_center-center_line/Brain-Cancer-Vaccine_Feature.jpg.webp 2432w" sizes="100vw">
    <img alt="Researchers discussing the mRNA brain cancer vaccine" width="640" height="426" src="https://ufhealth.org/assets/images/stories/_640x426_crop_center-center_line/Brain-Cancer-Vaccine_Feature.jpg" srcset="https://ufhealth.org/assets/images/stories/_640x426_crop_center-center_line/Brain-Cancer-Vaccine_Feature.jpg 640w, https://ufhealth.org/assets/images/stories/_860x573_crop_center-center_line/Brain-Cancer-Vaccine_Feature.jpg 860w, https://ufhealth.org/assets/images/stories/_1500x1000_crop_center-center_line/Brain-Cancer-Vaccine_Feature.jpg 1500w, https://ufhealth.org/assets/images/stories/_2432x1621_crop_center-center_line/Brain-Cancer-Vaccine_Feature.jpg 2432w" sizes="100vw">
  </picture>
  

                        <figcaption>
          <p>Dr. Elias Sayour, Chong Zhao and Arnav Barpujari discuss the mRNA cancer vaccine developed at the University of Florida.</p>
        </figcaption>
        
                
      </figure>
          </div>
  </header>
</div>


        
    
  
  
  
<section>
                    




<div data-module="InlineVideo">
  <figure data-target="InlineVideo__Preview">
        <img data-lazy="" src="https://img.youtube.com/vi/mgEIofnZTe8/maxresdefault.jpg" data-src="https://img.youtube.com/vi/mgEIofnZTe8/maxresdefault.jpg" width="768" height="432" alt="In a first-ever human clinical trial of four adult patients, an mRNA cancer vaccine developed at UF quickly reprogrammed the immune system to attack glioblastoma, the most aggressive and lethal brain tumor.">

    
  </figure>

    <iframe data-target="InlineVideo__Video" width="768" height="432" data-src="https://www.youtube.com/embed/mgEIofnZTe8?autoplay=1&amp;rel=0&amp;modestbranding=1&amp;enablejsapi=1" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen="">
  </iframe>
</div>


    
          
<div data-gtm-component="GenericContent">
  <p>In a first-ever human clinical trial of four adult patients, an mRNA cancer vaccine developed at the <a href="https://www.ufl.edu/" target="_blank" rel="noreferrer noopener">University of Florida</a> quickly reprogrammed the immune system to attack glioblastoma, the most aggressive and lethal brain tumor.<br></p>
<p>The results mirror those in 10 pet dog patients suffering from naturally occurring brain tumors whose owners approved of their participation, as they had no other treatment options, as well as results from preclinical mouse models. The breakthrough now will be tested in a Phase 1 pediatric clinical trial for brain cancer.</p>
<p> <a href="https://www.sciencedirect.com/science/article/pii/S0092867424003982?via%3Dihub" target="_blank" rel="noreferrer noopener">Reported May 1 in the journal Cell</a>, the discovery represents a potential new way to recruit the immune system to fight notoriously treatment-resistant cancers using an iteration of mRNA technology and lipid nanoparticles, similar to COVID-19 vaccines, but with two key differences: use of a patient’s own tumor cells to create a personalized vaccine, and a newly engineered complex delivery mechanism within the vaccine.</p>
<p>“Instead of us injecting single particles, we’re injecting clusters of particles that are wrapping around each other like onions, like a bag full of onions,” said senior author <a href="https://ufhealth.org/doctors/elias-sayour" target="_blank" rel="noreferrer noopener">Elias Sayour</a>, M.D., Ph.D., a UF Health pediatric oncologist who pioneered the new vaccine, which like other immunotherapies attempts to “educate” the immune system that a tumor is foreign. “And the reason we’ve done that in the context of cancer is these clusters alert the immune system in a much more profound way than single particles would.”<br></p>
</div>


    
          
 
      


  





  

  

    
          
<div data-gtm-component="GenericContent">
  <p>Among the most impressive findings was how quickly the new method, delivered intravenously, spurred a vigorous immune-system response to reject the tumor, said Sayour, principal investigator of the RNA Engineering Laboratory within UF’s <a href="https://braintumors.ufhealth.org/" target="_blank" rel="noreferrer noopener">Preston A. Wells Jr. Center for Brain Tumor Therapy</a> and a <a href="https://cancer.ufl.edu/" target="_blank" rel="noreferrer noopener">UF Health Cancer Center</a> and <a href="https://mbi.ufl.edu/" target="_blank" rel="noreferrer noopener">McKnight Brain Institute</a> investigator who led the multi-institution research team.</p>
<p>“In less than 48 hours, we could see these tumors shifting from what we refer to as ‘cold’ — immune cold, very few immune cells, very silenced immune response — to ‘hot,’ very active immune response,” he said. “That was very surprising given how quick this happened, and what that told us is we were able to activate the early part of the immune system very rapidly against these cancers, and that’s critical to unlock the later effects of the immune response.”</p>
<p>Glioblastoma is among the most devastating diagnoses, with median survival around 15 months. Current standard of care involves surgery, radiation and some combination of chemotherapy.</p>
<p>The new publication is the culmination of promising translational results over seven years of studies, starting in preclinical mouse models and then in a clinical trial of 10 pet dogs that had spontaneously developed terminal brain cancer and had no other treatment options. That trial was conducted with owners’ consent in collaboration with the <a href="https://www.vetmed.ufl.edu/" target="_blank" rel="noreferrer noopener">UF College of Veterinary Medicine</a>. Dogs offer a naturally occurring model for malignant glioma because they are the only other species that develops spontaneous brain tumors with some frequency, said <a href="https://www.vetmed.ufl.edu/profile/carrera-justiz-sheila/" target="_blank" rel="noreferrer noopener">Sheila Carrera-Justiz</a>, D.V.M., a veterinary neurologist at the UF College of Veterinary Medicine who is partnering with Sayour on the clinical trials. Gliomas in dogs are universally terminal, she said.</p>
<p>After treating pet dogs that had spontaneously developed brain cancer with personalized mRNA vaccines, Sayour’s team advanced the research to a small Food and Drug Administration-approved clinical trial designed to ensure safety and test feasibility before expanding to a larger trial.</p>
<p>In a cohort of four patients, genetic material called RNA was extracted from each patient’s own surgically removed tumor, and then messenger RNA, or mRNA — the blueprint of what is inside every cell, including tumor cells — was amplified and wrapped in the newly designed high-tech packaging of biocompatible lipid nanoparticles, to make tumor cells “look” like a dangerous virus when reinjected into the bloodstream and prompt an immune-system response. The vaccine was personalized to each patient with a goal of getting the most out of their unique immune system.</p>
<p>“The demonstration that making an mRNA cancer vaccine in this fashion generates similar and strong responses across mice, pet dogs that have developed cancer spontaneously and human patients with brain cancer is a really important finding, because oftentimes we don’t know how well the preclinical studies in animals are going to translate into similar responses in patients,” said <a href="https://neurosurgery.ufl.edu/faculty-staff/research-faculty/mitchell/" target="_blank" rel="noreferrer noopener">Duane Mitchell</a>, M.D., Ph.D., director of the UF Clinical and Translational Science Institute and the <a href="https://neurosurgery.ufl.edu/research/laboratories/ufbtip-lab/" target="_blank" rel="noreferrer noopener">UF Brain Tumor Immunotherapy Program</a> and a co-author of the paper. “And while mRNA vaccines and therapeutics are certainly a hot topic since the COVID pandemic, this is a novel and unique way of delivering the mRNA to generate these really significant and rapid immune responses that we’re seeing across animals and humans.”<br></p>
</div>


    
          
 
      


  





  

  

    
          
<div data-gtm-component="GenericContent">
  <p>While too early in the trial to assess the clinical effects of the vaccine, the patients either lived disease-free longer than expected or survived longer than expected.</p>
<p>The 10 pet dogs lived a median of 139 days, compared with a median survival of 30 to 60 days typical for dogs with the condition.</p>
<p>The next step, through support from the Food and Drug Administration and the <a href="https://curesearch.org/" target="_blank" rel="noreferrer noopener">CureSearch for Children’s Cancer foundation</a>, will be an expanded Phase I clinical trial to include up to 24 adult and pediatric patients to validate the findings. Once an optimal and safe dose is confirmed, an estimated 25 children would participate in Phase 2, said Sayour, an associate professor in the <a href="https://neurosurgery.ufl.edu/" target="_blank" rel="noreferrer noopener">Lillian S. Wells Department of Neurosurgery</a> and the department of pediatrics in the <a href="https://med.ufl.edu/" target="_blank" rel="noreferrer noopener">UF College of Medicine</a>, part of UF Health.</p>
<p>For the new clinical trial, Sayour’s lab will partner with a multi-institution consortium, the <a href="https://pnoc.us/" target="_blank" rel="noreferrer noopener">Pediatric Neuro-Oncology Consortium</a>, to send the immunotherapy treatment to children’s hospitals across the country. They will do this by receiving an individual patient’s tumor, manufacturing the personalized vaccine at UF and sending it back to the patient’s medical team, said Sayour, co-leader of the Immuno-Oncology and Microbiome research program at the UF Health Cancer Center.</p>
<p>Despite the promising results, the authors said one limitation is continued uncertainty about how best to harness the immune system while minimizing the potential for adverse side effects.</p>
<p>“I am hopeful that this could be a new paradigm for how we treat patients, a new platform technology for how we can modulate the immune system,” said Sayour, the Stop Children's Cancer/Bonnie R. Freeman Professor for Pediatric Oncology Research. “I am hopeful for how this could now synergize with other immunotherapies and perhaps unlock those immunotherapies. We showed in this paper that you actually can have synergy with other types of immunotherapies, so maybe now we can have a combination approach of immunotherapy.”</p>
<p>Sayour and Mitchell hold patents related to the vaccine which are under option to license by iOncologi Inc., a biotech company born as a “spin out” from UF in which Mitchell holds interest.</p>
</div>


    
  

      </section>

        
  
    
    
    



    
<div>

      <section>
    <h2>
      About the author
    </h2>

    
  </section>
  
      <section id="for-the-media">
    <h2>For the media</h2>
    <div>
        <h3>
          Media contact
        </h3>

                          


        
                      </div>
  </section>
</div>

</main>

  </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Amazon S3 will no longer charge for several HTTP error codes (279 pts)]]></title>
            <link>https://aws.amazon.com/about-aws/whats-new/2024/05/amazon-s3-no-charge-http-error-codes/</link>
            <guid>40346597</guid>
            <pubDate>Mon, 13 May 2024 18:30:00 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://aws.amazon.com/about-aws/whats-new/2024/05/amazon-s3-no-charge-http-error-codes/">https://aws.amazon.com/about-aws/whats-new/2024/05/amazon-s3-no-charge-http-error-codes/</a>, See on <a href="https://news.ycombinator.com/item?id=40346597">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="aws-page-content"> 
     <main id="aws-page-content-main" role="main" tabindex="-1"> 
      <section data-page-alert-target="true"> 
        
       <p> Posted On: <span> May 13, 2024</span> </p> 
       <div> 
        <p>Amazon S3 will make a change so unauthorized requests that customers did not initiate are free of charge. With this change, bucket owners will never incur request or bandwidth charges for requests that return an HTTP 403 (Access Denied) error response if initiated from outside their individual AWS account or AWS Organization. To see the full list of error codes that are free of&nbsp;charge, visit <a href="https://docs.aws.amazon.com/AmazonS3/latest/userguide/ErrorCodeBilling.html" target="_blank" rel="noopener">Billing for Amazon S3 error responses</a>. This billing change requires no changes to customer applications and applies to all S3 buckets.</p> 
       </div> 
       <div> 
        <p>These billing changes will apply in all AWS Regions, including the AWS GovCloud Regions and the AWS China Regions. This deployment is starting today and we will post another update in a few weeks when it is completed. To learn more,&nbsp;visit <a href="https://docs.aws.amazon.com/AmazonS3/latest/userguide/ErrorCodeBilling.html" target="_blank" rel="noopener">Billing for Amazon S3 error responses</a> and <a href="https://docs.aws.amazon.com/AmazonS3/latest/API/ErrorResponses.html" target="_blank" rel="noopener">Error Responses</a> in the S3 User Guide.</p> 
       </div> 
        
      </section> 
     </main> 
    </div><div data-lb-comp="modal" data-lb-modal-id="ie-deprecation-msg" data-ie10-deprecation-msg="You are using an outdated browser. Please upgrade to a modern browser to improve your experience."> 
      
     <p>
       AWS support for Internet Explorer ends on 07/31/2022. Supported browsers are Chrome, Firefox, Edge, and Safari. 
      <a href="https://aws.amazon.com/blogs/aws/heads-up-aws-support-for-internet-explorer-11-is-ending/" rel="noopener">Learn more »</a> 
     </p> 
      
    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Apple and Google deliver support for unwanted tracking alerts in iOS and Android (365 pts)]]></title>
            <link>https://www.apple.com/ca/newsroom/2024/05/apple-and-google-deliver-support-for-unwanted-tracking-alerts-in-ios-and-android/</link>
            <guid>40346024</guid>
            <pubDate>Mon, 13 May 2024 17:46:28 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.apple.com/ca/newsroom/2024/05/apple-and-google-deliver-support-for-unwanted-tracking-alerts-in-ios-and-android/">https://www.apple.com/ca/newsroom/2024/05/apple-and-google-deliver-support-for-unwanted-tracking-alerts-in-ios-and-android/</a>, See on <a href="https://news.ycombinator.com/item?id=40346024">Hacker News</a></p>
<div id="readability-page-1" class="page">


	
    







 
<nav id="ac-localnav" lang="en-CA" role="navigation" aria-label="Newsroom" data-analytics-region="local nav" data-sticky="">
	
    
        




    
    
    

</nav>



<main id="main" role="main"> 



<span id="opens-in-new-window">opens in new window</span>
<section>
<article data-analytics-activitymap-region-id="article">









    
    
    









    





    <div>
                
                
                
                    <h2>
                        
    
        Apple and Google deliver support for unwanted tracking alerts in iOS and Android
    

                    </h2>
                
            </div>







    
    
    


    
        
        
        
        
            <figure aria-label="Media, Logos representing Apple and Google.">
                <div>
                        
                         
                            
                        
                        
                        
                        <a href="https://www.apple.com/newsroom/images/2024/05/apple-and-google-deliver-support-for-unwanted-tracking-alerts-in-ios-and-android/article/Apple-Google-partner-tracking-detection-hero.zip" download="" data-analytics-title="download image - Apple-Google-partner-tracking-detection-hero_inline" aria-label="Download media, Logos representing Apple and Google."></a>
                    </div>
            </figure>
        
    










    
    
    


     
     
    
    
        <div>
             
                 <div>
Apple and Google have worked together to create an industry specification — <a href="https://datatracker.ietf.org/doc/draft-detecting-unwanted-location-trackers/01/" target="_blank" rel="nofollow" data-analytics-exit-link="">Detecting Unwanted Location Trackers</a> — for Bluetooth tracking devices that makes it possible to alert users across both iOS and Android if such a device is unknowingly being used to track them. This will help mitigate the misuse of devices designed to help keep track of belongings. Today Apple is implementing this capability in iOS 17.5, and Google is now launching this capability on Android 6.0+ devices.</div>
                 
             
                 <div>
With this new capability, users will now get an “[Item] Found Moving With You” alert on their device if an unknown Bluetooth tracking device is seen moving with them over time, regardless of the platform the device is paired with.</div>
                 
             
                 <div>If a user gets such an alert on their iOS device, it means that someone else’s AirTag, Find My accessory, or other industry specification-compatible Bluetooth tracker is moving with them. It’s possible the tracker is attached to an item the user is borrowing, but if not, iPhone can view the tracker’s identifier, have the tracker play a sound to help locate it, and access instructions to disable it.&nbsp;Bluetooth tag manufacturers including Chipolo, eufy, Jio, Motorola, and Pebblebee have committed that future tags will be compatible.
</div>
                 
             
                 <div>
AirTag and third-party Find My network accessories were designed from the beginning with industry-first privacy and safety protections, and Apple has remained committed to innovating and supplementing these protections to keep consumers safe. This cross-platform collaboration — also an industry first, involving community and industry input — offers instructions and best practices for manufacturers, should they choose to build unwanted tracking alert capabilities into their products. Apple and Google will continue to work with the <a href="https://www.ietf.org/about/introduction/" target="_blank" rel="nofollow" data-analytics-exit-link="">Internet Engineering Task Force</a> via the Detecting Unwanted Location Trackers working group to develop the official standard for this technology.</div>
                 
             
         </div>
 

    
    
    




    
    
        
    


    
    
    



    
    
    






    
















	
	
		















	
	
	

</article>



</section>
</main>


	

</div>]]></description>
        </item>
        <item>
            <title><![CDATA[GPT-4o (2557 pts)]]></title>
            <link>https://openai.com/index/hello-gpt-4o/</link>
            <guid>40345775</guid>
            <pubDate>Mon, 13 May 2024 17:28:00 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://openai.com/index/hello-gpt-4o/">https://openai.com/index/hello-gpt-4o/</a>, See on <a href="https://news.ycombinator.com/item?id=40345775">Hacker News</a></p>
Couldn't get https://openai.com/index/hello-gpt-4o/: Error: Request failed with status code 403]]></description>
        </item>
        <item>
            <title><![CDATA[Show HN: An open source framework for voice assistants (277 pts)]]></title>
            <link>https://github.com/pipecat-ai/pipecat</link>
            <guid>40345696</guid>
            <pubDate>Mon, 13 May 2024 17:21:14 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/pipecat-ai/pipecat">https://github.com/pipecat-ai/pipecat</a>, See on <a href="https://news.ycombinator.com/item?id=40345696">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><p><a target="_blank" rel="noopener noreferrer" href="https://github.com/pipecat-ai/pipecat/blob/main/pipecat.png"><img alt="pipecat" width="300px" height="auto" src="https://github.com/pipecat-ai/pipecat/raw/main/pipecat.png"></a>
</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Pipecat</h2><a id="user-content-pipecat" aria-label="Permalink: Pipecat" href="#pipecat"></a></p>
<p dir="auto"><a href="https://pypi.org/project/pipecat-ai" rel="nofollow"><img src="https://camo.githubusercontent.com/e44c7b97b713ead1309d49f5930eaf97f7045880e0f508d21570d49f607485df/68747470733a2f2f696d672e736869656c64732e696f2f707970692f762f706970656361742d6169" alt="PyPI" data-canonical-src="https://img.shields.io/pypi/v/pipecat-ai"></a> <a href="https://discord.gg/pipecat" rel="nofollow"><img src="https://camo.githubusercontent.com/884b6fbc9b12772324a7011eaee4cc25bc81d85225e7cf866f74b007051511e6/68747470733a2f2f696d672e736869656c64732e696f2f646973636f72642f31323339323834363737313635303536303231" alt="Discord" data-canonical-src="https://img.shields.io/discord/1239284677165056021"></a></p>
<p dir="auto"><code>pipecat</code> is a framework for building voice (and multimodal) conversational agents. Things like personal coaches, meeting assistants, <a href="https://storytelling-chatbot.fly.dev/" rel="nofollow">story-telling toys for kids</a>, customer support bots, <a href="https://www.youtube.com/watch?v=lDevgsp9vn0" rel="nofollow">intake flows</a>, and snarky social companions.</p>
<p dir="auto">Take a look at some example apps:</p>
<p dir="auto">
    <a href="https://github.com/pipecat-ai/pipecat/tree/main/examples/simple-chatbot"><img src="https://github.com/pipecat-ai/pipecat/raw/main/examples/simple-chatbot/image.png" width="280"></a>&nbsp;
    <a href="https://github.com/pipecat-ai/pipecat/tree/main/examples/storytelling-chatbot"><img src="https://github.com/pipecat-ai/pipecat/raw/main/examples/storytelling-chatbot/image.png" width="280"></a>
    <br>
    <a href="https://github.com/pipecat-ai/pipecat/tree/main/examples/translation-chatbot"><img src="https://github.com/pipecat-ai/pipecat/raw/main/examples/translation-chatbot/image.png" width="280"></a>&nbsp;
    <a href="https://github.com/pipecat-ai/pipecat/tree/main/examples/moondream-chatbot"><img src="https://github.com/pipecat-ai/pipecat/raw/main/examples/moondream-chatbot/image.png" width="280"></a>
</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Getting started with voice agents</h2><a id="user-content-getting-started-with-voice-agents" aria-label="Permalink: Getting started with voice agents" href="#getting-started-with-voice-agents"></a></p>
<p dir="auto">You can get started with Pipecat running on your local machine, then move your agent processes to the cloud when you’re ready. You can also add a 📞 telephone number, 🖼️ image output, 📺 video input, use different LLMs, and more.</p>
<div dir="auto" data-snippet-clipboard-copy-content="# install the module
pip install pipecat-ai

# set up an .env file with API keys
cp dot-env.template .env"><pre><span><span>#</span> install the module</span>
pip install pipecat-ai

<span><span>#</span> set up an .env file with API keys</span>
cp dot-env.template .env</pre></div>
<p dir="auto">By default, in order to minimize dependencies, only the basic framework functionality is available. Some third-party AI services require additional dependencies that you can install with:</p>
<div dir="auto" data-snippet-clipboard-copy-content="pip install &quot;pipecat-ai[option,...]&quot;"><pre>pip install <span><span>"</span>pipecat-ai[option,...]<span>"</span></span></pre></div>
<p dir="auto">Your project may or may not need these, so they're made available as optional requirements. Here is a list:</p>
<ul dir="auto">
<li><strong>AI services</strong>: <code>anthropic</code>, <code>azure</code>, <code>fal</code>, <code>moondream</code>, <code>openai</code>, <code>playht</code>, <code>silero</code>, <code>whisper</code></li>
<li><strong>Transports</strong>: <code>local</code>, <code>websocket</code>, <code>daily</code></li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Code examples</h2><a id="user-content-code-examples" aria-label="Permalink: Code examples" href="#code-examples"></a></p>
<ul dir="auto">
<li><a href="https://github.com/pipecat-ai/pipecat/tree/main/examples/foundational">foundational</a> — small snippets that build on each other, introducing one or two concepts at a time</li>
<li><a href="https://github.com/pipecat-ai/pipecat/tree/main/examples/">example apps</a> — complete applications that you can use as starting points for development</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">A simple voice agent running locally</h2><a id="user-content-a-simple-voice-agent-running-locally" aria-label="Permalink: A simple voice agent running locally" href="#a-simple-voice-agent-running-locally"></a></p>
<p dir="auto">Here is a very basic Pipecat bot that greets a user when they join a real-time session. We'll use <a href="https://daily.co/" rel="nofollow">Daily</a> for real-time media transport, and <a href="https://elevenlabs.io/" rel="nofollow">ElevenLabs</a> for text-to-speech.</p>
<div dir="auto" data-snippet-clipboard-copy-content="#app.py

import asyncio
import aiohttp

from pipecat.frames.frames import EndFrame, TextFrame
from pipecat.pipeline.pipeline import Pipeline
from pipecat.pipeline.task import PipelineTask
from pipecat.pipeline.runner import PipelineRunner
from pipecat.services.elevenlabs import ElevenLabsTTSService
from pipecat.transports.services.daily import DailyParams, DailyTransport

async def main():
  async with aiohttp.ClientSession() as session:
    # Use Daily as a real-time media transport (WebRTC)
    transport = DailyTransport(
      room_url=...,
      token=...,
      &quot;Bot Name&quot;,
      DailyParams(audio_out_enabled=True))

    # Use Eleven Labs for Text-to-Speech
    tts = ElevenLabsTTSService(
      aiohttp_session=session,
      api_key=...,
      voice_id=...,
      )

    # Simple pipeline that will process text to speech and output the result
    pipeline = Pipeline([tts, transport.output()])

    # Create Pipecat processor that can run one or more pipelines tasks
    runner = PipelineRunner()

    # Assign the task callable to run the pipeline
    task = PipelineTask(pipeline)

    # Register an event handler to play audio when a
    # participant joins the transport WebRTC session
    @transport.event_handler(&quot;on_participant_joined&quot;)
    async def on_new_participant_joined(transport, participant):
      participant_name = participant[&quot;info&quot;][&quot;userName&quot;] or ''
      # Queue a TextFrame that will get spoken by the TTS service (Eleven Labs)
      await task.queue_frames([TextFrame(f&quot;Hello there, {participant_name}!&quot;), EndFrame()])

    # Run the pipeline task
    await runner.run(task)

if __name__ == &quot;__main__&quot;:
  asyncio.run(main())"><pre><span>#app.py</span>

<span>import</span> <span>asyncio</span>
<span>import</span> <span>aiohttp</span>

<span>from</span> <span>pipecat</span>.<span>frames</span>.<span>frames</span> <span>import</span> <span>EndFrame</span>, <span>TextFrame</span>
<span>from</span> <span>pipecat</span>.<span>pipeline</span>.<span>pipeline</span> <span>import</span> <span>Pipeline</span>
<span>from</span> <span>pipecat</span>.<span>pipeline</span>.<span>task</span> <span>import</span> <span>PipelineTask</span>
<span>from</span> <span>pipecat</span>.<span>pipeline</span>.<span>runner</span> <span>import</span> <span>PipelineRunner</span>
<span>from</span> <span>pipecat</span>.<span>services</span>.<span>elevenlabs</span> <span>import</span> <span>ElevenLabsTTSService</span>
<span>from</span> <span>pipecat</span>.<span>transports</span>.<span>services</span>.<span>daily</span> <span>import</span> <span>DailyParams</span>, <span>DailyTransport</span>

<span>async</span> <span>def</span> <span>main</span>():
  <span>async</span> <span>with</span> <span>aiohttp</span>.<span>ClientSession</span>() <span>as</span> <span>session</span>:
    <span># Use Daily as a real-time media transport (WebRTC)</span>
    <span>transport</span> <span>=</span> <span>DailyTransport</span>(
      <span>room_url</span><span>=</span>...,
      <span>token</span><span>=</span>...,
      <span>"Bot Name"</span>,
      <span>DailyParams</span>(<span>audio_out_enabled</span><span>=</span><span>True</span>))

    <span># Use Eleven Labs for Text-to-Speech</span>
    <span>tts</span> <span>=</span> <span>ElevenLabsTTSService</span>(
      <span>aiohttp_session</span><span>=</span><span>session</span>,
      <span>api_key</span><span>=</span>...,
      <span>voice_id</span><span>=</span>...,
      )

    <span># Simple pipeline that will process text to speech and output the result</span>
    <span>pipeline</span> <span>=</span> <span>Pipeline</span>([<span>tts</span>, <span>transport</span>.<span>output</span>()])

    <span># Create Pipecat processor that can run one or more pipelines tasks</span>
    <span>runner</span> <span>=</span> <span>PipelineRunner</span>()

    <span># Assign the task callable to run the pipeline</span>
    <span>task</span> <span>=</span> <span>PipelineTask</span>(<span>pipeline</span>)

    <span># Register an event handler to play audio when a</span>
    <span># participant joins the transport WebRTC session</span>
    <span>@<span>transport</span>.<span>event_handler</span>(<span>"on_participant_joined"</span>)</span>
    <span>async</span> <span>def</span> <span>on_new_participant_joined</span>(<span>transport</span>, <span>participant</span>):
      <span>participant_name</span> <span>=</span> <span>participant</span>[<span>"info"</span>][<span>"userName"</span>] <span>or</span> <span>''</span>
      <span># Queue a TextFrame that will get spoken by the TTS service (Eleven Labs)</span>
      <span>await</span> <span>task</span>.<span>queue_frames</span>([<span>TextFrame</span>(<span>f"Hello there, <span><span>{</span><span>participant_name</span><span>}</span></span>!"</span>), <span>EndFrame</span>()])

    <span># Run the pipeline task</span>
    <span>await</span> <span>runner</span>.<span>run</span>(<span>task</span>)

<span>if</span> <span>__name__</span> <span>==</span> <span>"__main__"</span>:
  <span>asyncio</span>.<span>run</span>(<span>main</span>())</pre></div>
<p dir="auto">Run it with:</p>

<p dir="auto">Daily provides a prebuilt WebRTC user interface. Whilst the app is running, you can visit at <code>https://&lt;yourdomain&gt;.daily.co/&lt;room_url&gt;</code> and listen to the bot say hello!</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">WebRTC for production use</h2><a id="user-content-webrtc-for-production-use" aria-label="Permalink: WebRTC for production use" href="#webrtc-for-production-use"></a></p>
<p dir="auto">WebSockets are fine for server-to-server communication or for initial development. But for production use, you’ll need client-server audio to use a protocol designed for real-time media transport. (For an explanation of the difference between WebSockets and WebRTC, see <a href="https://www.daily.co/blog/how-to-talk-to-an-llm-with-your-voice/#webrtc" rel="nofollow">this post.</a>)</p>
<p dir="auto">One way to get up and running quickly with WebRTC is to sign up for a Daily developer account. Daily gives you SDKs and global infrastructure for audio (and video) routing. Every account gets 10,000 audio/video/transcription minutes free each month.</p>
<p dir="auto">Sign up <a href="https://dashboard.daily.co/u/signup" rel="nofollow">here</a> and <a href="https://docs.daily.co/reference/rest-api/rooms" rel="nofollow">create a room</a> in the developer Dashboard.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">What is VAD?</h2><a id="user-content-what-is-vad" aria-label="Permalink: What is VAD?" href="#what-is-vad"></a></p>
<p dir="auto">Voice Activity Detection — very important for knowing when a user has finished speaking to your bot. If you are not using press-to-talk, and want Pipecat to detect when the user has finished talking, VAD is an essential component for a natural feeling conversation.</p>
<p dir="auto">Pipecast makes use of WebRTC VAD by default when using a WebRTC transport layer. Optionally, you can use Silero VAD for improved accuracy at the cost of higher CPU usage.</p>
<div dir="auto" data-snippet-clipboard-copy-content="pip install pipecat-ai[silero]"><pre>pip install pipecat-ai[silero]</pre></div>
<p dir="auto">The first time your run your bot with Silero, startup may take a while whilst it downloads and caches the model in the background. You can check the progress of this in the console.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Hacking on the framework itself</h2><a id="user-content-hacking-on-the-framework-itself" aria-label="Permalink: Hacking on the framework itself" href="#hacking-on-the-framework-itself"></a></p>
<p dir="auto"><em>Note that you may need to set up a virtual environment before following the instructions below. For instance, you might need to run the following from the root of the repo:</em></p>
<div dir="auto" data-snippet-clipboard-copy-content="python3 -m venv venv
source venv/bin/activate"><pre>python3 -m venv venv
<span>source</span> venv/bin/activate</pre></div>
<p dir="auto">From the root of this repo, run the following:</p>
<div dir="auto" data-snippet-clipboard-copy-content="pip install -r dev-requirements.txt -r {env}-requirements.txt
python -m build"><pre>pip install -r dev-requirements.txt -r {env}-requirements.txt
python -m build</pre></div>
<p dir="auto">This builds the package. To use the package locally (eg to run sample files), run</p>

<p dir="auto">If you want to use this package from another directory, you can run:</p>
<div dir="auto" data-snippet-clipboard-copy-content="pip install path_to_this_repo"><pre>pip install path_to_this_repo</pre></div>
<p dir="auto"><h3 tabindex="-1" dir="auto">Running tests</h3><a id="user-content-running-tests" aria-label="Permalink: Running tests" href="#running-tests"></a></p>
<p dir="auto">From the root directory, run:</p>
<div dir="auto" data-snippet-clipboard-copy-content="pytest --doctest-modules --ignore-glob=&quot;*to_be_updated*&quot; src tests"><pre>pytest --doctest-modules --ignore-glob=<span><span>"</span>*to_be_updated*<span>"</span></span> src tests</pre></div>
<p dir="auto"><h2 tabindex="-1" dir="auto">Setting up your editor</h2><a id="user-content-setting-up-your-editor" aria-label="Permalink: Setting up your editor" href="#setting-up-your-editor"></a></p>
<p dir="auto">This project uses strict <a href="https://peps.python.org/pep-0008/" rel="nofollow">PEP 8</a> formatting.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Emacs</h3><a id="user-content-emacs" aria-label="Permalink: Emacs" href="#emacs"></a></p>
<p dir="auto">You can use <a href="https://github.com/jwiegley/use-package">use-package</a> to install <a href="https://codeberg.org/ideasman42/emacs-py-autopep8" rel="nofollow">py-autopep8</a> package and configure <code>autopep8</code> arguments:</p>
<div dir="auto" data-snippet-clipboard-copy-content="(use-package py-autopep8
  :ensure t
  :defer t
  :hook ((python-mode . py-autopep8-mode))
  :config
  (setq py-autopep8-options '(&quot;-a&quot; &quot;-a&quot;, &quot;--max-line-length=100&quot;)))"><pre>(<span>use-package</span> py-autopep8
  <span>:ensure</span> <span>t</span>
  <span>:defer</span> <span>t</span>
  <span>:hook</span> ((<span>python-mode</span> <span>.</span> py-autopep8-mode))
  <span>:config</span>
  (<span>setq</span> py-autopep8-options '(<span><span>"</span>-a<span>"</span></span> <span><span>"</span>-a<span>"</span></span>, <span><span>"</span>--max-line-length=100<span>"</span></span>)))</pre></div>
<p dir="auto"><code>autopep8</code> was installed in the <code>venv</code> environment described before, so you should be able to use <a href="https://github.com/ryotaro612/pyvenv-auto">pyvenv-auto</a> to automatically load that environment inside Emacs.</p>
<div dir="auto" data-snippet-clipboard-copy-content="(use-package pyvenv-auto
  :ensure t
  :defer t
  :hook ((python-mode . pyvenv-auto-run)))
"><pre>(<span>use-package</span> pyvenv-auto
  <span>:ensure</span> <span>t</span>
  <span>:defer</span> <span>t</span>
  <span>:hook</span> ((<span>python-mode</span> <span>.</span> pyvenv-auto-run)))
</pre></div>
<p dir="auto"><h3 tabindex="-1" dir="auto">Visual Studio Code</h3><a id="user-content-visual-studio-code" aria-label="Permalink: Visual Studio Code" href="#visual-studio-code"></a></p>
<p dir="auto">Install the
<a href="https://marketplace.visualstudio.com/items?itemName=ms-python.autopep8" rel="nofollow">autopep8</a> extension. Then edit the user settings (<em>Ctrl-Shift-P</em> <code>Open User Settings (JSON)</code>) and set it as the default Python formatter, enable formatting on save and configure <code>autopep8</code> arguments:</p>
<div dir="auto" data-snippet-clipboard-copy-content="&quot;[python]&quot;: {
    &quot;editor.defaultFormatter&quot;: &quot;ms-python.autopep8&quot;,
    &quot;editor.formatOnSave&quot;: true
},
&quot;autopep8.args&quot;: [
    &quot;-a&quot;,
    &quot;-a&quot;,
    &quot;--max-line-length=100&quot;
],"><pre><span>"[python]"</span>: {
    <span>"editor.defaultFormatter"</span>: <span><span>"</span>ms-python.autopep8<span>"</span></span>,
    <span>"editor.formatOnSave"</span>: <span>true</span>
},
<span>"autopep8.args"</span>: [
    <span><span>"</span>-a<span>"</span></span>,
    <span><span>"</span>-a<span>"</span></span>,
    <span><span>"</span>--max-line-length=100<span>"</span></span>
],</pre></div>
<p dir="auto"><h2 tabindex="-1" dir="auto">Getting help</h2><a id="user-content-getting-help" aria-label="Permalink: Getting help" href="#getting-help"></a></p>
<p dir="auto">➡️ <a href="https://discord.gg/pipecat" rel="nofollow">Join our Discord</a></p>
<p dir="auto">➡️ <a href="https://x.com/pipecat_ai" rel="nofollow">Reach us on Twitter</a></p>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Deblur-GS: 3D Gaussian splatting from camera motion blurred images (138 pts)]]></title>
            <link>https://chaphlagical.icu/Deblur-GS/</link>
            <guid>40345654</guid>
            <pubDate>Mon, 13 May 2024 17:16:54 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://chaphlagical.icu/Deblur-GS/">https://chaphlagical.icu/Deblur-GS/</a>, See on <a href="https://news.ycombinator.com/item?id=40345654">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
    <p><img src="https://chaphlagical.icu/Deblur-GS/static/images/teaser.png"></p><h2>
      Deblur-GS reconstruction sharp Gaussian scene from motion blurred images
    </h2>
  </div><div id="BibTeX">
      <h2>BibTeX</h2>
      <pre><code>@article{Chen_deblurgs2024,
        author       = {Wenbo, Chen and Ligang, Liu},
        title        = {Deblur-GS: 3D Gaussian Splatting from Camera Motion Blurred Images},
        journal      = {Proc. ACM Comput. Graph. Interact. Tech. (Proceedings of I3D 2024)},
        year         = {2024},
        volume       = {7},
        number       = {1},
        numpages     = {13},
        location     = {Philadelphia, PA, USA},
        url          = {http://doi.acm.org/10.1145/3651301},
        doi          = {10.1145/3651301},
        publisher    = {ACM Press},
        address      = {New York, NY, USA},
     }</code></pre>
    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Discord has been using ML to determine the gender and age of some of its users (121 pts)]]></title>
            <link>https://twitter.com/DiscordPreviews/status/1790065494432608432</link>
            <guid>40345627</guid>
            <pubDate>Mon, 13 May 2024 17:14:50 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://twitter.com/DiscordPreviews/status/1790065494432608432">https://twitter.com/DiscordPreviews/status/1790065494432608432</a>, See on <a href="https://news.ycombinator.com/item?id=40345627">Hacker News</a></p>
Couldn't get https://twitter.com/DiscordPreviews/status/1790065494432608432: Error: Request failed with status code 400]]></description>
        </item>
        <item>
            <title><![CDATA[Good, Kind, Caring People Became the Bad Guys (128 pts)]]></title>
            <link>https://www.okdoomer.io/thebadguys/</link>
            <guid>40345552</guid>
            <pubDate>Mon, 13 May 2024 17:09:46 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.okdoomer.io/thebadguys/">https://www.okdoomer.io/thebadguys/</a>, See on <a href="https://news.ycombinator.com/item?id=40345552">Hacker News</a></p>
<div id="readability-page-1" class="page"><article>
              <p>There's a story about Zig Ziglar, the motivational speaker and author. Apparently, a woman approached Ziglar at one of his seminars, begging him for help. She said she hated her job. Her boss was horrible. Her coworkers were so mean to her.</p><p>What should she do?</p><p>Well, Zig told her to shut up and stop complaining. He said <em>she </em>sounded like the negative one. He told her to chant aphorisms into a mirror. Low and behold, she was cured.</p><p>I've got bad news for everyone. Zig didn't help this woman. In fact, all he did was reinforce a cognitive bias that psychologists have been studying for decades now.</p><p>This bias explains a lot about the state of the world today.</p><p>But first, a personal story:</p><p>I don't know if you've ever been trapped in a small space with a severe, violent schizophrenic relative.</p><p>It's not fun.</p><p>When I was a teen, my mom started having delusions and hallucinations. She thought the CIA was spying on my dad. She thought a Category 4 Hurricane was going to hit the house. She thought I was a space alien. She thought dinosaurs were real. She tried to make bombs out of old electronics and threw them in our rooms in the middle of the night. And, she tried to seduce my brother.</p><p>All of that happened in the span of ten years.</p><p>I got pretty good at spotting the early red flags. My mom said and did little things that didn't make sense. She started dropping incredibly pessimistic observations or insults into the middle of small talk. She hid things in weird places. She stayed up even later than usual, not watching television, just smoking and drinking black coffee, staring off into the midnight void. The tone of her voice changed.</p><p>So did her body language.</p><p>I tried to warn my dad and brother. Every single time, they ignored me until it was too late. My dad usually got angry at me. He thought I was trying to wreck the family vacation or whatever. More than once, he piled us into the car and drove us to a beach, where my mom's health completely unraveled and I got stuck following her around and trying to keep her from drowning herself.</p><p>So I got used to quietly watching things devolve, until one night she'd get so violent my dad had to call the police.</p><p>The police wouldn't come.</p><p>(At first.)</p><p>They frequently told us they couldn't do anything until <em>after </em>she hurt someone. It didn't matter if she implied she was going to do us harm. It didn't matter if she made us feel unsafe. It didn't matter if a court had ordered her to take medication.</p><p>In the end, she wound wind up hurting one of us. Then the police would arrest her. Then the doctors would evaluate her and determine that, yes, she belonged in a mental health facility, but only for as long as our health insurance provider allowed.</p><p>Social workers started visiting my house. They came to my school. I was called out of class to answer their questions. They asked me if I felt safe in my home. They asked if I was okay.</p><p>I lied.</p><p>I lied because I'd been taught a valuable lesson, one that I didn't truly understand for years. When you complain, people judge you. It doesn't matter what you're complaining about. It doesn't matter what you're protesting or whistle-blowing. It doesn't matter if your life is at stake. It doesn't matter if thousands of lives are at stake. It doesn't matter if the fate of humanity is at stake. Someone's first instinct is to suspect you. It's to accuse you of lying. It's to label you a troublemaker.</p><p>They hear negative words coming out of your mouth. They associate those negative things with you, because you're the person saying them. That's how our primate brains operate. It takes a lot of self-awareness to overcome that, and many people lack it.</p><p>A lot has changed since then.</p><p>Now I'm an adult, watching this kind of thing play out on a global scale every single day with the collapse of public health, the collapse of our democracies, the collapse of global industrial civilization, and the collapse of our ecosystems.</p><p>Now it occurs to me.</p><p>I've already lived through collapse. I've watched the collapse of my mom's mental and physical health. I've watched the collapse of my family. And I've watched the collapse of at least one university.</p><p>You could call me a connoisseur of collapse.</p><p>Over the last few years, I've studied the psychology of denial and cognitive dissonance. There are so many bugs and glitches in the human psyche to explain what's happening and why it takes so much patience, and so much effort, to ever get anyone to take threats seriously or to change their behavior, for any reason.</p><p>Here's one of the most fascinating ones:</p><p>Spontaneous trait transference.</p><p>In the 1990s, psychologists John Skowronski and Donal Carlston began noticing something strange when someone tried to raise concerns about someone else. Instead of believing them, people tended to <a href="https://pubmed.ncbi.nlm.nih.gov/9569648/?ref=okdoomer.io" rel="noreferrer">transfer those negative traits</a> to the person trying to warn them. They confirmed the behavior in four different studies. They defined spontaneous trait transference as when "communicators are perceived as possessing the very traits they describe in others."</p><p>He who smelt it dealt it.</p><p>Am I right?</p><p>As Skowronsky explains, "politicians who allege corruption by their opponents may themselves be perceived as dishonest" and "critics who praise artists may themselves be perceived as talented." If you describe someone as negative, unreliable, or dangerous, people tend to misremember it as a self-description. Yes, they really think you were talking about yourself.</p><p>This tendency reinforces our worst behaviors, ensuring we never criticize those who deserve it while the ones who deserve praise never actually get it. All those fake compliments flying around? These people have probably never heard of spontaneous trait transference, but they've observed it. They've learned that if you pass out worthless attagirls all day long, people mistakenly assume you're a great person.</p><p>And it works.</p><p>Because humans are kind of gullible...</p><p>Rick Brown and John Bassili even found that people can transfer personality traits <a href="https://www.sciencedirect.com/science/article/pii/S0022103101914866?ref=okdoomer.io" rel="noreferrer">to inanimate objects</a> like bananas. They do it without even thinking. Yes, you can condition someone to believe that bananas are evil.</p><p>It gets worse.</p><p>You know the phrase, don't shoot the messenger?</p><p>That actually happens.</p><p>In 2019, a team of psychologists at Harvard led by Leslie John reviewed hundreds of studies and conducted <a href="https://psycnet.apa.org/record/2019-19962-004?ref=okdoomer.io" rel="noreferrer">eleven different experiments</a> to explain why people punish someone for giving them bad news. They learned that the human brain often reaches for the quickest, easiest explanations for negative events in their lives, especially ones that preserve their self-image and group harmony. As Leslie John and her colleagues write, "people are especially prone to attributing agency to others for negative outcomes." They also "attribute agency to those proximal to the event."</p><p>There's nothing more proximal to an event than the first person to tell you what's going on. Once again, our shared psychology discourages us from warning each other about threats.</p><p>And so:</p><p>"Bad news messengers may be prime candidates in recipients' search for antagonists to cast in accounts of unwanted outcomes." Bad news also motivates people to come up with "fallacious" causal explanations "often generated effortlessly, seemingly automatically." They generate these fallacious explanations through poor reasoning "characterized by shallow, unconscious thought."</p><p>That's how we wind up with so many conspiracy theories. They're easier to swallow than the truth.</p><p>They gratify us. </p><p>To sum things up, people tend to attribute the bad news and negative events in their lives to those around them, often their friends and family. They don't do a good job of distinguishing between a threat and someone trying to warn them.</p><p>They get them mixed up.</p><p>That's hardly a useful evolutionary trait, is it?</p><p>And yet, this trait explains so much of what's going on now. It explains why the public gets angry at climate protestors instead of the oil executives who've ruined their future. It explains how university students have somehow wound up as the villains in so many people's eyes, instead of the governments sponsoring and committing genocide. It explains why you can't criticize billionaires or super rich influencers without that incredibly annoying counter claim:</p><p>"You're just jealous."</p><p>It explains why you get pathologized and called everything from a doomer to a snowflake for caring about anything but yourself. It explains why pretending to care looks better.</p><p>It explains why my family didn't listen to me about my mom's mental health. It explains why Zig Ziglar was such an asshole to a woman who begged him for help.</p><p>I've tried to come up with little strategies and workarounds for all of humanity's psychological shortcomings.</p><p>It comes down to this:</p><p>You have to find a way to be the smart, positive, compassionate, mature, respectful, charming one. You have to do that even if everyone around you is acting like a complete idiot.</p><p>You have to achieve a fine balance between sugar coating and blunt honesty. Above all, you have to anticipate that all of your hard work won't achieve the results you want in the short term. It takes a long time. A lot of people won't listen at first.</p><p>You often have to trick people into doing the right thing.</p><p>It's exhausting.</p><p>It takes a lot out of you to be respectful to idiots all day long, especially when many of them go out of their way to do you harm. Despite the audacious tone of my writing (my dry humor is often mistaken for bitterness or anger), I try to remain calm and compassionate when dealing with people right in front of me.</p><p>And of course, a lot of people will call you rude or disrespectful, simply because you don't smile when you talk.</p><p>It's a lot to deal with.</p><p>It's all hard, because every single minute matters now. We don't have decades to change public thought. The plagues are getting worse and more frequent. The climate collapse is accelerating. The weather is getting more extreme. People are dying from heatstroke in the thousands now. They're getting swept away in crash floods. They're getting blown out of their apartments by typhoon-strength winds in the middle of the night. Every disease we ever dealt with is now converging on our weakened immune systems.</p><p>Our politicians spent the last year whining about TikTok while letting yet another zoonotic disease run rampant. The last threads of democracy are unraveling right in front of us.</p><p>It's hard to sound optimistic.</p><p>What's my point?</p><p>I guess, this:</p><p>When I was dealing with my mom's mental health, the worst part wasn't the fear or the violence. It was the solitude. It was lying to social workers. It was pretending everything was fine around my friends. It was waiting for my family to finally see what I saw.</p><p>Those days when my mom was in the mental health facility, those were the best. She was getting the help she needed. My family stopped being angry at me. It was when she came home, when she stopped taking her medicine, and things went back to "normal."</p><p>Those were the worst days.</p><p>It was lonely.</p><p>I hated those long stretches of "normal," when we were all waiting for the next schizophrenic break but wouldn't do anything about it. When I moved out, I was relieved. My mom suffered from schizophrenia for almost 20 more years before she died. At least I didn't have to live in that state of paralysis anymore.</p><p>I could have a life.</p><p>Know this:</p><p>You are seen. You are not working in vain. Millions of people out there are listening to your warnings, even if it often doesn't feel that way. Maybe it's not enough to stop the worst of everything.</p><p>But you're not alone.</p><p>It's something, at least.</p><hr><p><em>Thank you to all the readers who </em><a href="https://www.okdoomer.io/support/" rel="noreferrer"><em>support</em></a><em> this site.</em></p><p><em>It makes a difference.</em></p>
            </article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[OpenAI launches new AI model and desktop version of ChatGPT (107 pts)]]></title>
            <link>https://www.cnbc.com/2024/05/13/openai-launches-new-ai-model-and-desktop-version-of-chatgpt.html</link>
            <guid>40345526</guid>
            <pubDate>Mon, 13 May 2024 17:08:23 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.cnbc.com/2024/05/13/openai-launches-new-ai-model-and-desktop-version-of-chatgpt.html">https://www.cnbc.com/2024/05/13/openai-launches-new-ai-model-and-desktop-version-of-chatgpt.html</a>, See on <a href="https://news.ycombinator.com/item?id=40345526">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="SpecialReportArticle-ArticleBody-6" data-module="ArticleBody" data-test="articleBody-2" data-analytics="SpecialReportArticle-articleBody-6-2"><div id="Placeholder-ArticleBody-Video-107414178" data-test="VideoPlaceHolder" role="region" tabindex="0" data-vilynx-id="7000340046" aria-labelledby="Placeholder-ArticleBody-Video-107414178"><p><img src="https://image.cnbcfm.com/api/v1/image/107414179-17156232921715623289-34511329350-1080pnbcnews.jpg?v=1715623291&amp;w=750&amp;h=422&amp;vtcrop=y" alt="OpenAI unveils new AI model and desktop version of ChatGPT"><span></span><span></span></p></div><div><p><a href="https://www.cnbc.com/2023/05/09/openai-disruptor-50.html">OpenAI</a> on Monday launched a new AI model and desktop version of ChatGPT, along with an updated user interface, the company's latest effort to expand use of its popular chatbot.</p><p>The update brings GPT-4 to everyone, including OpenAI's free users, technology chief Mira Murati said in a livestreamed event. She added that the new model, GPT-4o, is "much faster," with improved capabilities in text, video and audio. OpenAI said it eventually plans to allow users to video chat with ChatGPT.</p><p>"This is the first time that we are really making a huge step forward when it comes to the ease of use," Murati said.</p><p>OpenAI, backed by <span data-test="QuoteInBody" id="SpecialReportArticle-QuoteInBody-2"><a href="https://www.cnbc.com/quotes/MSFT/">Microsoft</a><span><span id="-WatchlistDropdown" data-analytics-id="-WatchlistDropdown"></span></span></span>, has been valued by more than $80 billion by investors. The company, founded in 2015, is under pressure to stay on top of the generative AI market while finding ways to make money as it spends massive sums on processors and infrastructure to build and train its models.</p></div><div id="ArticleBody-InlineImage-107346332" data-test="InlineImage"><p>Sam Altman, CEO of OpenAI, at the Hope Global Forums annual meeting in Atlanta on Dec. 11, 2023.</p><p>Dustin Chambers | Bloomberg | Getty Images</p></div><div><p>The o in GPT-4o stands for omni. The new model allows ChatGPT to handle 50 different languages with improved speed and quality, and it will also be available via OpenAI's API making it possible for developers to begin building applications using the new model today, Murati said.</p><p>She added that GPT-4o is twice as fast as, and half the cost of, GPT-4 Turbo.</p><p>OpenAI team members demonstrated the new model's audio capabilities, for example, asking it to help calm someone down ahead of a public speech. OpenAI researcher Mark Chen said the model is able to "perceive your emotion," adding the model can also handle users interrupting it. The team also asked it to analyze a user's facial expression to comment on the emotions the person may be experiencing.</p><p>"Hey there, what's up? How can I brighten your day today?" ChatGPT's audio mode said when a user greeted it.</p><p>The company plans to test Voice Mode in the coming weeks, with early access for paid subscribers to ChatGPT Plus, according to a <a href="https://openai.com/index/hello-gpt-4o/" target="_blank">blog post</a>. OpenAI also said the new model can respond to users' audio prompts "in as little as 232 milliseconds, with an average of 320 milliseconds, which is similar to&nbsp;human response time&nbsp;in a conversation."</p><p>Chen demonstrated the model's ability to tell a bedtime story and asked it to change the tone of its voice to be more dramatic or robotic. He even asked it to sing the story.</p><p>In addition, OpenAI's new model can function as a translator, even in audio mode, the company said. Chen demonstrated the tool's ability to listen to Murati speaking Italian while he spoke English and to translate into their respective languages as they conversed.</p><p>Team members also demonstrated the model's ability to solve math equations and help write code, positioning it as a stronger competitor to Microsoft's own GitHub Copilot.</p><p>For OpenAI, the launch was one of the company's biggest announcements since the August kickoff of ChatGPT Enterprise, the AI chatbot's business tier. That tool was in development for "under a year" and had the help of more than 20 companies of varying sizes and industries, OpenAI Chief Operating Officer Brad Lightcap told CNBC at the time.</p><p>OpenAI, Microsoft and <span data-test="QuoteInBody" id="SpecialReportArticle-QuoteInBody-4"><a href="https://www.cnbc.com/quotes/GOOGL/">Google</a><span><span id="-WatchlistDropdown" data-analytics-id="-WatchlistDropdown"></span></span></span> are at the helm of a&nbsp;<a href="https://www.cnbc.com/ai-artificial-intelligence/">generative AI</a>&nbsp;gold rush as companies in seemingly every industry race to add AI-powered chatbots and agents to key services to avoid <a href="https://www.cnbc.com/2024/03/30/fomo-drives-tech-heavyweights-to-invest-billions-in-generative-ai-.html">being left behind</a> by competitors. Earlier this month, OpenAI rival Anthropic announced its <a href="https://www.cnbc.com/2024/05/01/anthropic-iphone-ai-app-business-plan-to-compete-with-openai-announced.html">first-ever enterprise offering</a> and a free iPhone app.</p><p>A record $29.1 billion was invested across nearly 700 generative AI deals in 2023, an increase of more than 260% from the prior year, according to PitchBook. The market is&nbsp;predicted to<a href="https://www.bloomberg.com/professional/insights/data/generative-ai-races-toward-1-3-trillion-in-revenue-by-2032/#:~:text=Generative%20AI%20is%20poised%20to,our%20proprietary%20market%2Dsizing%20model." target="_blank">&nbsp;top $1 trillion</a>&nbsp;in revenue within a decade.</p><p>Some in the industry have raised concerns about the speed at which untested new services are coming to market, and academics and ethicists are&nbsp;<a href="https://www.cnbc.com/2023/03/29/elon-musk-other-tech-leaders-pause-training-ai-beyond-gpt-4.html">distressed</a>&nbsp;about the technology's tendency to propagate bias.</p><p>After ChatGPT's launch in November 2022, it broke records at the time as the fastest-growing consumer app in history, and now has about 100 million weekly active users. OpenAI says that&nbsp;<a href="https://www.cnbc.com/2023/11/06/openai-announces-more-powerful-gpt-4-turbo-and-cuts-prices.html">more than 92%</a>&nbsp;of Fortune 500 companies are using the platform.</p><p>Murati said during the Monday event that OpenAI wants to "remove some of the mysticism from the technology."</p><p>"Over the next few weeks, we'll be rolling out these capabilities to everyone," she said.</p><p>The new model will first roll out on Tuesday to customers of ChatGPT Plus and Team, and then to Enterprise later, a <a href="https://openai.com/index/gpt-4o-and-more-tools-to-chatgpt-free/" target="_blank">blog post</a> on Monday said. It will also be available to free users of ChatGPT starting Monday, with usage limits. Users of ChatGPT Plus will have five times more message capacity than free users, and clients of ChatGPT Team and Enterprise will have even greater usage limits.</p><p>Murati concluded the livestreamed event by thanking Nvidia CEO Jensen Huang and his company for providing the necessary graphics processing units (GPUs) to power OpenAI's technology.</p><p>"I just want to thank the incredible OpenAI team, and also thanks to Jensen and the Nvidia team for bringing us the most advanced GPUs to make this demo possible today," she said.</p></div><div id="SpecialReportArticle-RelatedContent-1"><h2>Don’t miss these exclusives from CNBC PRO</h2><div><ul><li><a href="https://www.cnbc.com/2024/05/04/most-of-warren-buffetts-stock-portfolio-is-tied-up-in-just-5-stocks-heres-what-they-are.html"><em>Most of Warren Buffett's stock portfolio is tied up in just 5 stocks. Here's what they are</em></a>&nbsp;</li><li><a href="https://www.cnbc.com/2024/05/02/bank-of-america-says-you-should-buy-this-shoe-stock-before-earnings.html"><em>Bank of America says this shoe stock is an inflation winner and you should buy it before earnings</em></a><em>&nbsp;</em></li><li><a href="https://www.cnbc.com/2024/04/17/-these-are-morgan-stanleys-top-picks-into-quarterly-earnings.html"><em>These are Morgan Stanley's top picks into quarterly earnings</em></a></li><li><a href="https://www.cnbc.com/2024/05/02/tesla-to-face-a-huge-demand-problem-over-price-cuts-says-investor.html"><em>Tesla price cuts could backfire, fund manager says, warning of a 'huge demand problem' on the horizon</em></a></li><li><a href="https://www.cnbc.com/2024/05/01/doublelines-gundlach-sees-one-rate-cut-this-year-as-fed-keeps-up-inflation-fight.html"><em>Gundlach sees one rate cut this year as Fed keeps up inflation fight</em></a></li></ul></div></div></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Unix version control lore: what, ident (108 pts)]]></title>
            <link>https://dotat.at/@/2024-05-13-what-ident.html</link>
            <guid>40344946</guid>
            <pubDate>Mon, 13 May 2024 16:17:20 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://dotat.at/@/2024-05-13-what-ident.html">https://dotat.at/@/2024-05-13-what-ident.html</a>, See on <a href="https://news.ycombinator.com/item?id=40344946">Hacker News</a></p>
<div id="readability-page-1" class="page"><article>
  <p>There are a couple of version control commands that deserve wider
appreciation: <a href="https://pubs.opengroup.org/onlinepubs/9699919799/utilities/what.html">SCCS <code>what</code></a> and <a href="https://www.gnu.org/software/rcs/manual/html_node/ident.html">RCS <code>ident</code></a>. They allow
you to find out what source a binary was built from, without having to
run it – handy if it is a library! They basically scan a file looking
for magic strings that contain version control metadata and print out
what they discover.</p>
<h2 id="keyword-expansion">keyword expansion</h2>
<p>SCCS, RCS, <code>cvs</code>, and <code>svn</code> all have a way to expand keywords in a
file when it is checked out of version control.</p>
<p>The POSIX <a href="https://pubs.opengroup.org/onlinepubs/9699919799/utilities/get.html#tag_20_52_13_02">SCCS <code>get</code></a> documentation describes its runes under
the “identification keywords” heading. The relevant one is <code>%W%</code> which
inserts the magic marker <code>@(#)</code> used by <code>what</code>.</p>
<p><a href="https://www.gnu.org/software/trans-coord/manual/cvs/cvs.html#Keyword-substitution">RCS / <code>cvs</code> / <code>svn</code> keyword substitution</a> uses more descriptive
markers like <code>$Revision$</code>.</p>
<h2 id="a-berkeley-example">a berkeley example</h2>
<p>It was a lonstanding BSD practice to use keyword expansion everywhere.
I first encountered it when I got involved in the Apache httpd project
in the late 1990s – Apache’s CVS repository was hosted on a FreeBSD
box and used a version of FreeBSD’s CVS administrative scripts.</p>
<p>Here’s an example from <a href="https://cgit.freebsd.org/src/tree/lib/libc/resolv/res_send.c?id=e45764721aedfa6460e1767664864bda9457c10e"><code>res_send.c</code></a> in FreeBSD’s libc
resolver.</p>
<pre><code>static const char sccsid[] = "@(#)res_send.c	8.1 (Berkeley) 6/4/93";
static const char rcsid[] = "$Id: res_send.c,v 1.22 2009/01/22 23:49:23 tbox Exp $";
__FBSDID("$FreeBSD$");
</code></pre>
<p>There are geological strata of version control ident strings here:</p>
<ul>
<li>the <code>sccsid</code> from the Berkeley CSRG SCCS repository</li>
<li>the <code>rcsid</code> from ISC’s BIND repository
(<code>tbox</code> was ISC’s tinderbox build / CI system)</li>
<li>the <code>FBSDID</code> from FreeBSD’s <code>cvs</code> and later <code>svn</code> repositories
(which has not been expanded)</li>
</ul>
<h2 id="an-unifdef-example">an unifdef example</h2>
<p>When <code>unifdef</code> was uplifted to git, I wanted to keep its embedded
version control keywords – I have a sentimental liking for this old
tradition. If you’ve installed <a href="https://www.gnu.org/software/cssc/"><code>cssc</code></a>, <code>rcs</code>, and <code>unifdef</code> on
a Debian box, you can run,</p>
<pre><code>    :; sccs what /usr/bin/unifdef
    :; ident /usr/bin/unifdef
</code></pre>
<p>Both of those will produce similar output to</p>
<pre><code>    :; unifdef -V
</code></pre>
<p>On a Mac with the developer command-line tools installed,</p>
<pre><code>    :; what /Library/Developer/CommandLineTools/usr/bin/unifdef
</code></pre>
<p>You get the output twice because it’s a fat binary!</p>
<h2 id="versioning-three-ways">versioning three ways</h2>
<p>In <a href="https://dotat.at/cgi/git/unifdef.git/blob/HEAD:/unifdef.c"><code>unifdef.c</code></a>, the embedded version string looks like,</p>
<pre><code>    static const char copyright[] =
        "@(#) $Version: unifdef-2.12 $\n"
        "@(#) $Date: 2020-02-14 16:49:56 +0000 $\n"
        "@(#) $Author: Tony Finch (dot@dotat.at) $\n"
        "@(#) $URL: http://dotat.at/prog/unifdef $\n"
    ;
</code></pre>
<p>Each line is prefixed with an SCCS magic marker <code>@(#)</code> so that <code>what</code>
can find it, and wrapped in an RCS-style <code>$Keyword$</code> so that <code>ident</code>
can find it. There’s a fairly trivial <code>version()</code> function that
spits out the <code>copyright[]</code> string when you run <code>unifdef -V</code>.</p>
<h2 id="embedding-versions-from-git">embedding versions from git</h2>
<p>My projects have various horrible build scripts for embedding the
version number from <code>git</code>. The basic idea is,</p>
<ul>
<li>
<p>use an annotated or signed tag to mark a release,
i.e. <code>git tag -a</code> or <code>git tag -s</code></p>
</li>
<li>
<p>use <code>git describe</code> to get a version string that includes an
extra number counting commits since the last release</p>
</li>
<li>
<p>maybe use <code>git show --pretty=format:%ai -s HEAD</code> to get a release date</p>
</li>
<li>
<p>stuff the outputs from <code>git</code> into the <code>$Version$</code> and <code>$Date$</code> RCS
keywords</p>
</li>
</ul>
<h2 id="retro-cool">retro cool</h2>
<p>I enjoy keeping this old feature working, even though it isn’t very
useful if no-one knows about it! Maybe if I blog about it, it’ll
become more widespread?</p>

</article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Falcon 2 (223 pts)]]></title>
            <link>https://www.tii.ae/news/falcon-2-uaes-technology-innovation-institute-releases-new-ai-model-series-outperforming-metas</link>
            <guid>40344302</guid>
            <pubDate>Mon, 13 May 2024 15:17:26 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.tii.ae/news/falcon-2-uaes-technology-innovation-institute-releases-new-ai-model-series-outperforming-metas">https://www.tii.ae/news/falcon-2-uaes-technology-innovation-institute-releases-new-ai-model-series-outperforming-metas</a>, See on <a href="https://news.ycombinator.com/item?id=40344302">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="block-mainpagecontent"> <main>  <div> <article><div><ul><li><strong>Next-Gen Falcon 2 Series launches AI Model that is Open-Source, Multilingual, and Multimodal – and is only AI Model with Vision-to-Language Capabilities</strong></li><li><strong>New Falcon 2 11B Outperforms Meta’s Llama 3 8B, and Performs on par with leading Google Gemma 7B Model, as Independently Verified by Hugging Face Leaderboard</strong></li><li><strong>Immediate Plans Include Exploring 'Mixture of Experts' for Enhanced Machine Learning Capabilities</strong></li></ul></div><p><strong>Abu Dhabi-UAE: 13 May, 2024</strong> - <a href="https://www.tii.ae/news/tii.ae">The Technology Innovation Institute</a> (TII), a leading global scientific research center and the applied research pillar of Abu Dhabi’s <a href="https://www.atrc.gov.ae/">Advanced Technology Research Council</a> (ATRC), today launched a second iteration of its renowned large language model (LLM) – Falcon 2. Within this series, it has unveiled two groundbreaking versions: Falcon 2 11B, a more efficient and accessible LLM trained on 5.5 trillion tokens with 11 billion parameters, and Falcon 2 11B VLM, distinguished by its vision-to-language model (VLM) capabilities, which enable seamless conversion of visual inputs into textual outputs. While both models are multilingual, notably, Falcon 2 11B VLM stands out as TII's first multimodal model – and the only one currently in the top tier market that has this image-to-text conversion capability, marking a significant advancement in AI innovation.</p><p>Tested against several prominent AI models in its class among pre-trained models, Falcon 2 11B surpasses the performance of Meta’s newly launched Llama 3 with 8 billion parameters(8B), and performs on par with Google’s Gemma 7B at first place (Falcon 2 11B: 64.28 vs Gemma 7B: 64.29), as independently verified by Hugging Face, a US-based platform hosting an objective evaluation tool and global leaderboard for open LLMs. More importantly, Falcon 2 11B and 11B VLM are both open-source, empowering developers worldwide with unrestricted access. In the near future, there are plans to broaden the Falcon 2 next-generation models, introducing a range of sizes. These models will be further enhanced with advanced machine learning capabilities like 'Mixture of Experts' (MoE), aimed at pushing their performance to even more sophisticated levels.</p><p>All of TII’s AI models released to date have consistently ranked in the top tier globally, as the most powerful open-source LLMs. The new scaled-down and versatile Falcon 2 11B models are set to give TII greater market adoption in the ever-evolving world of generative AI.</p><p>Falcon 2 11B models, equipped with multilingual capabilities, seamlessly tackle tasks in English, French, Spanish, German, Portuguese, and various other languages, enriching their versatility and magnifying their effectiveness across diverse scenarios. Falcon 2 11B VLM, a 2 vision-to-language model, has the capability to identify and interpret images and visuals from the environment, providing a wide range of applications across industries such as healthcare, finance, e-commerce, education, and legal sectors. These applications range from document management, digital archiving, and context indexing to supporting individuals with visual impairments. Furthermore, these models can run efficiently on just one graphics processing unit (GPU), making them highly scalable, and easy to deploy and integrate into lighter infrastructures like laptops and other devices.</p><p><strong>H.E. Faisal Al Bannai</strong>, Secretary General of ATRC and Strategic Research and Advanced Technology Affairs Advisor to the UAE President, said: "With the release of Falcon 2 11B, we've introduced the first model in the Falcon 2 series. While Falcon 2 11B has demonstrated outstanding performance, we reaffirm our commitment to the open-source movement with it, and to the Falcon Foundation. With other multimodal models soon coming to the market in various sizes, our aim is to ensure that developers and entities that value their privacy, have access to one of the best AI models to enable their AI journey."</p><p>Speaking on the model, <strong>Dr. Hakim Hacid</strong>, Executive Director and Acting Chief Researcher of the AI Cross-Center Unit at TII, said: “AI is continually evolving, and developers are recognizing the myriad benefits of smaller, more efficient models. In addition to reducing computing power requirements and meeting sustainability criteria, these models offer enhanced flexibility, seamlessly integrating into edge AI infrastructure, the next emerging megatrend. Furthermore, the vision-to-language capabilities of Falcon 2 open new horizons for accessibility in AI, empowering users with transformative image to text interactions.”</p><p>The versatility of Falcon 2 11B has also led TII to consider working on more exciting GenAI innovations. Among these, will be the adoption of a new type of machine learning capability known as the aforementioned ‘Mixture of Experts’. This method involves amalgamating smaller networks with distinct specializations, ensuring that the most knowledgeable domains collaborate to deliver highly sophisticated and customized responses – almost like having a team of smart helpers who each know something different and work together to predict or make decisions when needed. This approach not only improves accuracy, but it also accelerates decision-making, paving the way for more intelligent and efficient AI systems</p><p>Falcon 2 11B is licenced under TII Falcon License 2.0, the permissive Apache 2.0-based software license which includes an acceptable use policy that promotes the responsible use of AI. More information on the new model can be found at <a href="https://falconllm.tii.ae/">FalconLLM.TII.ae</a></p> </article></div> </main></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Pg_lakehouse: Query Any Data Lake from Postgres (153 pts)]]></title>
            <link>https://github.com/paradedb/paradedb/tree/dev/pg_lakehouse</link>
            <guid>40343131</guid>
            <pubDate>Mon, 13 May 2024 13:29:17 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/paradedb/paradedb/tree/dev/pg_lakehouse">https://github.com/paradedb/paradedb/tree/dev/pg_lakehouse</a>, See on <a href="https://news.ycombinator.com/item?id=40343131">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><div dir="auto"><h2 tabindex="-1" dir="auto">
  <a target="_blank" rel="noopener noreferrer" href="https://github.com/paradedb/paradedb/blob/dev/docs/logo/pg_lakehouse.svg"><img src="https://github.com/paradedb/paradedb/raw/dev/docs/logo/pg_lakehouse.svg" alt="pg_lakehouse" width="500px"></a>
<br>
</h2><a id="user-content---" aria-label="Permalink: " href="#--"></a></div>
<p dir="auto"><h2 tabindex="-1" dir="auto">Overview</h2><a id="user-content-overview" aria-label="Permalink: Overview" href="#overview"></a></p>
<p dir="auto"><code>pg_lakehouse</code> is an extension that transforms Postgres into an analytical query engine over object stores like S3 and table formats like Delta Lake. Queries are pushed down to <a href="https://github.com/apache/datafusion">Apache DataFusion</a>, which delivers excellent analytical performance. Combinations of the following object stores, table formats, and file formats are supported.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Object Stores</h3><a id="user-content-object-stores" aria-label="Permalink: Object Stores" href="#object-stores"></a></p>
<ul>
<li> Amazon S3</li>
<li> S3-compatible object stores (e.g. MinIO)</li>
<li> Local file system</li>
<li> Google Cloud Storage (coming soon)</li>
<li> Azure Blob Storage (coming soon)</li>
</ul>
<p dir="auto">...and potentially any service supported by <a href="https://opendal.apache.org/docs/category/services" rel="nofollow">Apache OpenDAL</a>. See the Development section for instructions on how to <a href="#adding-a-service">add a service</a>.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">File Formats</h3><a id="user-content-file-formats" aria-label="Permalink: File Formats" href="#file-formats"></a></p>
<ul>
<li> Parquet</li>
<li> CSV</li>
<li> JSON</li>
<li> Avro</li>
<li> ORC (coming soon)</li>
</ul>
<p dir="auto"><h3 tabindex="-1" dir="auto">Table Formats</h3><a id="user-content-table-formats" aria-label="Permalink: Table Formats" href="#table-formats"></a></p>
<ul>
<li> Delta Lake</li>
<li> Apache Iceberg (coming soon)</li>
</ul>
<p dir="auto"><code>pg_lakehouse</code> is supported on Postgres 14, 15, and 16. Support for Postgres 12 and 13 is coming soon.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Motivation</h2><a id="user-content-motivation" aria-label="Permalink: Motivation" href="#motivation"></a></p>
<p dir="auto">Today, a vast amount of non-operational data — events, metrics, historical snapshots, vendor data, etc. — is ingested into data lakes like S3. Querying this data by moving it into a cloud data warehouse or operating a new query engine is expensive and time consuming. The goal of <code>pg_lakehouse</code> is to enable this data to be queried directly from Postgres. This eliminates the need for new infrastructure, loss of data freshness, data movement, and non-Postgres dialects of other query engines.</p>
<p dir="auto"><code>pg_lakehouse</code> uses the foreign data wrapper (FDW) API to connect to any object store or table format and the executor hook API to push queries to DataFusion. While other FDWs like <code>aws_s3</code> have existed in the Postgres extension ecosystem, these FDWs suffer from two limitations:</p>
<ol dir="auto">
<li>Lack of support for most object stores, file, and table formats</li>
<li>Too slow over large datasets to be a viable analytical engine</li>
</ol>
<p dir="auto"><code>pg_lakehouse</code> differentiates itself by supporting a wide breadth of stores and formats (thanks to <a href="https://github.com/apache/opendal">OpenDAL</a>) and by being very fast (thanks to <a href="https://github.com/apache/datafusion">DataFusion</a>).</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Getting Started</h2><a id="user-content-getting-started" aria-label="Permalink: Getting Started" href="#getting-started"></a></p>
<p dir="auto">The following example uses <code>pg_lakehouse</code> to query an example dataset of 3 million NYC taxi trips from January 2024, hosted in a public S3 bucket provided by ParadeDB.</p>
<div dir="auto" data-snippet-clipboard-copy-content="CREATE EXTENSION pg_lakehouse;
CREATE FOREIGN DATA WRAPPER s3_wrapper HANDLER s3_fdw_handler VALIDATOR s3_fdw_validator;

-- Provide S3 credentials
CREATE SERVER s3_server FOREIGN DATA WRAPPER s3_wrapper
OPTIONS (bucket 'paradedb-benchmarks', region 'us-east-1', allow_anonymous 'true');

-- Create foreign table
CREATE FOREIGN TABLE trips (
    &quot;VendorID&quot;              INT,
    &quot;tpep_pickup_datetime&quot;  TIMESTAMP,
    &quot;tpep_dropoff_datetime&quot; TIMESTAMP,
    &quot;passenger_count&quot;       BIGINT,
    &quot;trip_distance&quot;         DOUBLE PRECISION,
    &quot;RatecodeID&quot;            DOUBLE PRECISION,
    &quot;store_and_fwd_flag&quot;    TEXT,
    &quot;PULocationID&quot;          REAL,
    &quot;DOLocationID&quot;          REAL,
    &quot;payment_type&quot;          DOUBLE PRECISION,
    &quot;fare_amount&quot;           DOUBLE PRECISION,
    &quot;extra&quot;                 DOUBLE PRECISION,
    &quot;mta_tax&quot;               DOUBLE PRECISION,
    &quot;tip_amount&quot;            DOUBLE PRECISION,
    &quot;tolls_amount&quot;          DOUBLE PRECISION,
    &quot;improvement_surcharge&quot; DOUBLE PRECISION,
    &quot;total_amount&quot;          DOUBLE PRECISION
)
SERVER s3_server
OPTIONS (path 's3://paradedb-benchmarks/yellow_tripdata_2024-01.parquet', extension 'parquet');

-- Success! Now you can query the remote Parquet file like a regular Postgres table
SELECT COUNT(*) FROM trips;
  count
---------
 2964624
(1 row)"><pre>CREATE EXTENSION pg_lakehouse;
CREATE FOREIGN DATA WRAPPER s3_wrapper HANDLER s3_fdw_handler VALIDATOR s3_fdw_validator;

<span><span>--</span> Provide S3 credentials</span>
CREATE SERVER s3_server FOREIGN DATA WRAPPER s3_wrapper
OPTIONS (bucket <span><span>'</span>paradedb-benchmarks<span>'</span></span>, region <span><span>'</span>us-east-1<span>'</span></span>, allow_anonymous <span><span>'</span>true<span>'</span></span>);

<span><span>--</span> Create foreign table</span>
CREATE FOREIGN TABLE trips (
    <span><span>"</span>VendorID<span>"</span></span>              <span>INT</span>,
    <span><span>"</span>tpep_pickup_datetime<span>"</span></span>  <span>TIMESTAMP</span>,
    <span><span>"</span>tpep_dropoff_datetime<span>"</span></span> <span>TIMESTAMP</span>,
    <span><span>"</span>passenger_count<span>"</span></span>       <span>BIGINT</span>,
    <span><span>"</span>trip_distance<span>"</span></span>         <span>DOUBLE PRECISION</span>,
    <span><span>"</span>RatecodeID<span>"</span></span>            <span>DOUBLE PRECISION</span>,
    <span><span>"</span>store_and_fwd_flag<span>"</span></span>    <span>TEXT</span>,
    <span><span>"</span>PULocationID<span>"</span></span>          <span>REAL</span>,
    <span><span>"</span>DOLocationID<span>"</span></span>          <span>REAL</span>,
    <span><span>"</span>payment_type<span>"</span></span>          <span>DOUBLE PRECISION</span>,
    <span><span>"</span>fare_amount<span>"</span></span>           <span>DOUBLE PRECISION</span>,
    <span><span>"</span>extra<span>"</span></span>                 <span>DOUBLE PRECISION</span>,
    <span><span>"</span>mta_tax<span>"</span></span>               <span>DOUBLE PRECISION</span>,
    <span><span>"</span>tip_amount<span>"</span></span>            <span>DOUBLE PRECISION</span>,
    <span><span>"</span>tolls_amount<span>"</span></span>          <span>DOUBLE PRECISION</span>,
    <span><span>"</span>improvement_surcharge<span>"</span></span> <span>DOUBLE PRECISION</span>,
    <span><span>"</span>total_amount<span>"</span></span>          <span>DOUBLE PRECISION</span>
)
SERVER s3_server
OPTIONS (<span>path</span> <span><span>'</span>s3://paradedb-benchmarks/yellow_tripdata_2024-01.parquet<span>'</span></span>, extension <span><span>'</span>parquet<span>'</span></span>);

<span><span>--</span> Success! Now you can query the remote Parquet file like a regular Postgres table</span>
<span>SELECT</span> <span>COUNT</span>(<span>*</span>) <span>FROM</span> trips;
  count
<span><span>--</span>-------</span>
 <span>2964624</span>
(<span>1</span> row)</pre></div>
<p dir="auto">Note that column names must be wrapped in double quotes to preserve uppercase letters. This is because DataFusion is case-sensitive and Postgres' foreign table column names must match the foreign table's column names exactly.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Query Acceleration</h2><a id="user-content-query-acceleration" aria-label="Permalink: Query Acceleration" href="#query-acceleration"></a></p>
<p dir="auto">This extension uses Postgres hooks to intercept and push queries down to DataFusion. In order to enable these hooks, the extension must be added to <code>shared_preload_libraries</code> inside <code>postgresql.conf</code>.</p>
<div dir="auto" data-snippet-clipboard-copy-content="# Inside postgresql.conf
shared_preload_libraries = 'pg_lakehouse'"><pre><span><span>#</span> Inside postgresql.conf</span>
shared_preload_libraries = <span><span>'</span>pg_lakehouse<span>'</span></span></pre></div>
<p dir="auto"><h2 tabindex="-1" dir="auto">Inspecting the Foreign Schema</h2><a id="user-content-inspecting-the-foreign-schema" aria-label="Permalink: Inspecting the Foreign Schema" href="#inspecting-the-foreign-schema"></a></p>
<p dir="auto">The <code>arrow_schema</code> function displays the schema of a foreign table. This function is useful for verifying that the server and table credentials you've provided are valid. If the connection is successful and <code>pg_lakehouse</code> is able to read the foreign data, a table will be returned with the <a href="https://docs.rs/datafusion/latest/datafusion/common/arrow/datatypes/enum.DataType.html" rel="nofollow">Arrow schema</a> of the foreign table. Otherwise, an empty table will be returned or an error will be thrown.</p>
<div dir="auto" data-snippet-clipboard-copy-content="SELECT * FROM arrow_schema(
  server => 's3_server',
  path => 's3://paradedb-benchmarks/yellow_tripdata_2024-01.parquet',
  extension => 'parquet'
);"><pre><span>SELECT</span> <span>*</span> <span>FROM</span> arrow_schema(
  server <span>=&gt;</span> <span><span>'</span>s3_server<span>'</span></span>,
  <span>path</span> <span>=&gt;</span> <span><span>'</span>s3://paradedb-benchmarks/yellow_tripdata_2024-01.parquet<span>'</span></span>,
  extension <span>=&gt;</span> <span><span>'</span>parquet<span>'</span></span>
);</pre></div>
<p dir="auto">You can also use this function to decide what Postgres types to assign to each column of the foreign table. For instance, an Arrow <code>Utf8</code> datatype should map to a Postgres <code>TEXT</code>, <code>VARCHAR</code>, or <code>BPCHAR</code> column. If an incompatible Postgres type is chosen, querying the table will fail.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Connect an Object Store</h2><a id="user-content-connect-an-object-store" aria-label="Permalink: Connect an Object Store" href="#connect-an-object-store"></a></p>
<p dir="auto">To connect your own object store, please refer to the <a href="https://docs.paradedb.com/analytics/object_stores" rel="nofollow">documentation</a>.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Types</h2><a id="user-content-types" aria-label="Permalink: Types" href="#types"></a></p>
<p dir="auto">Some types like <code>date</code>, <code>timestamp</code>, and <code>timestamptz</code> must be handled carefully. Please refer to the <a href="https://docs.paradedb.com/analytics/schema#datetime-types" rel="nofollow">documentation</a>.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Development</h2><a id="user-content-development" aria-label="Permalink: Development" href="#development"></a></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Install Rust</h3><a id="user-content-install-rust" aria-label="Permalink: Install Rust" href="#install-rust"></a></p>
<p dir="auto">To develop the extension, first install Rust via <code>rustup</code>.</p>
<div dir="auto" data-snippet-clipboard-copy-content="curl --proto '=https' --tlsv1.2 -sSf https://sh.rustup.rs | sh
rustup install <version>

rustup default <version>"><pre>curl --proto <span><span>'</span>=https<span>'</span></span> --tlsv1.2 -sSf https://sh.rustup.rs <span>|</span> sh
rustup install <span>&lt;</span>version<span>&gt;</span>

rustup default <span>&lt;</span>version<span>&gt;</span></pre></div>
<p dir="auto">Note: While it is possible to install Rust via your package manager, we recommend using <code>rustup</code> as we've observed inconcistencies with Homebrew's Rust installation on macOS.</p>
<p dir="auto">Then, install the PostgreSQL version of your choice using your system package manager. Here we provide the commands for the default PostgreSQL version used by this project:</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Install Postgres</h3><a id="user-content-install-postgres" aria-label="Permalink: Install Postgres" href="#install-postgres"></a></p>
<div dir="auto" data-snippet-clipboard-copy-content="# macOS
brew install postgresql@16

# Ubuntu
wget --quiet -O - https://www.postgresql.org/media/keys/ACCC4CF8.asc | sudo apt-key add -
sudo sh -c 'echo &quot;deb http://apt.postgresql.org/pub/repos/apt/ $(lsb_release -cs)-pgdg main&quot; > /etc/apt/sources.list.d/pgdg.list'
sudo apt-get update &amp;&amp; sudo apt-get install -y postgresql-16 postgresql-server-dev-16"><pre><span><span>#</span> macOS</span>
brew install postgresql@16

<span><span>#</span> Ubuntu</span>
wget --quiet -O - https://www.postgresql.org/media/keys/ACCC4CF8.asc <span>|</span> sudo apt-key add -
sudo sh -c <span><span>'</span>echo "deb http://apt.postgresql.org/pub/repos/apt/ $(lsb_release -cs)-pgdg main" &gt; /etc/apt/sources.list.d/pgdg.list<span>'</span></span>
sudo apt-get update <span>&amp;&amp;</span> sudo apt-get install -y postgresql-16 postgresql-server-dev-16</pre></div>
<p dir="auto">If you are using Postgres.app to manage your macOS PostgreSQL, you'll need to add the <code>pg_config</code> binary to your path before continuing:</p>
<div dir="auto" data-snippet-clipboard-copy-content="export PATH=&quot;$PATH:/Applications/Postgres.app/Contents/Versions/latest/bin&quot;"><pre><span>export</span> PATH=<span><span>"</span><span>$PATH</span>:/Applications/Postgres.app/Contents/Versions/latest/bin<span>"</span></span></pre></div>
<p dir="auto"><h3 tabindex="-1" dir="auto">Install pgrx</h3><a id="user-content-install-pgrx" aria-label="Permalink: Install pgrx" href="#install-pgrx"></a></p>
<p dir="auto">Then, install and initialize <code>pgrx</code>:</p>
<div dir="auto" data-snippet-clipboard-copy-content="# Note: Replace --pg16 with your version of Postgres, if different (i.e. --pg15, --pg14, etc.)
cargo install --locked cargo-pgrx --version 0.11.3

# macOS arm64
cargo pgrx init --pg16=/opt/homebrew/opt/postgresql@16/bin/pg_config

# macOS amd64
cargo pgrx init --pg16=/usr/local/opt/postgresql@16/bin/pg_config

# Ubuntu
cargo pgrx init --pg16=/usr/lib/postgresql/16/bin/pg_config"><pre><span><span>#</span> Note: Replace --pg16 with your version of Postgres, if different (i.e. --pg15, --pg14, etc.)</span>
cargo install --locked cargo-pgrx --version 0.11.3

<span><span>#</span> macOS arm64</span>
cargo pgrx init --pg16=/opt/homebrew/opt/postgresql@16/bin/pg_config

<span><span>#</span> macOS amd64</span>
cargo pgrx init --pg16=/usr/local/opt/postgresql@16/bin/pg_config

<span><span>#</span> Ubuntu</span>
cargo pgrx init --pg16=/usr/lib/postgresql/16/bin/pg_config</pre></div>
<p dir="auto">If you prefer to use a different version of Postgres, update the <code>--pg</code> flag accordingly.</p>
<p dir="auto">Note: While it is possible to develop using pgrx's own Postgres installation(s), via <code>cargo pgrx init</code> without specifying a <code>pg_config</code> path, we recommend using your system package manager's Postgres as we've observed inconsistent behaviours when using pgrx's.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Adding a Service</h3><a id="user-content-adding-a-service" aria-label="Permalink: Adding a Service" href="#adding-a-service"></a></p>
<p dir="auto"><code>pg_lakehouse</code> uses OpenDAL to integrate with various object stores. As of the time of writing, some — but not all — of the object stores supported by OpenDAL have been integrated.</p>
<p dir="auto">Adding support for a new object store is as straightforward as</p>
<ol dir="auto">
<li>Adding the service feature to <code>opendal</code> in <code>Cargo.toml</code>. For instance, S3 requires <code>services-s3</code>.</li>
<li>Creating a file in the <code>fdw/</code> folder that implements the <code>BaseFdw</code> trait. For instance, <code>fdw/s3.rs</code> implements the S3 FDW.</li>
<li>Registering the FDW in <code>fdw/handler.rs</code>.</li>
</ol>
<p dir="auto"><h3 tabindex="-1" dir="auto">Running Tests</h3><a id="user-content-running-tests" aria-label="Permalink: Running Tests" href="#running-tests"></a></p>
<p dir="auto">We use <code>cargo test</code> as our runner for <code>pg_lakehouse</code> tests. Tests are conducted using <a href="https://github.com/testcontainers/testcontainers-rs">testcontainers</a> to manage testing containers like <a href="https://hub.docker.com/r/localstack/localstack" rel="nofollow">LocalStack</a>. <code>testcontainers</code> will pull any Docker images that it requires to perform the test.</p>
<p dir="auto">You also need a running Postgres instance to run the tests. The test suite will look for a connection string on the <code>DATABASE_URL</code> environment variable. You can set this variable manually, or use <code>.env</code> file with contents like this:</p>
<div data-snippet-clipboard-copy-content="DATABASE_URL=postgres://<username>@<host>:<port>/<database>"><pre lang="text"><code>DATABASE_URL=postgres://&lt;username&gt;@&lt;host&gt;:&lt;port&gt;/&lt;database&gt;
</code></pre></div>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Squarespace to Go Private in $6.9B All-Cash Transaction with Permira (265 pts)]]></title>
            <link>https://investors.squarespace.com/news-events-financials/investor-news/news-details/2024/Squarespace-to-Go-Private-in-6.9B-All-Cash-Transaction-with-Permira/default.aspx</link>
            <guid>40343006</guid>
            <pubDate>Mon, 13 May 2024 13:17:31 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://investors.squarespace.com/news-events-financials/investor-news/news-details/2024/Squarespace-to-Go-Private-in-6.9B-All-Cash-Transaction-with-Permira/default.aspx">https://investors.squarespace.com/news-events-financials/investor-news/news-details/2024/Squarespace-to-Go-Private-in-6.9B-All-Cash-Transaction-with-Permira/default.aspx</a>, See on <a href="https://news.ycombinator.com/item?id=40343006">Hacker News</a></p>
Couldn't get https://investors.squarespace.com/news-events-financials/investor-news/news-details/2024/Squarespace-to-Go-Private-in-6.9B-All-Cash-Transaction-with-Permira/default.aspx: Error: Request failed with status code 403]]></description>
        </item>
        <item>
            <title><![CDATA[Static Chess (394 pts)]]></title>
            <link>https://www.val.town/v/maxm/staticChess</link>
            <guid>40342803</guid>
            <pubDate>Mon, 13 May 2024 12:56:34 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.val.town/v/maxm/staticChess">https://www.val.town/v/maxm/staticChess</a>, See on <a href="https://news.ycombinator.com/item?id=40342803">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p><span></span><span>import</span><span> </span><span>{</span><span> </span><span>Chess</span><span>,</span><span> </span><span>Move</span><span>,</span><span> </span><span>Square</span><span> </span><span>}</span><span> </span><span>from</span><span> </span><span>"npm:chess.js"</span><span>;</span><span></span></p><p><span></span><span>import</span><span> </span><span>minify</span><span> </span><span>from</span><span> </span><span>"npm:css-simple-minifier"</span><span>;</span><span></span></p><p><span></span><span>import</span><span> </span><span>{</span><span> </span><span>renderToString</span><span> </span><span>}</span><span> </span><span>from</span><span> </span><span>"npm:react-dom/server"</span><span>;</span><span></span></p><p><span></span><span>class</span><span> </span><span>StaticChess</span><span> </span><span>{</span><span></span></p><p><span> </span><span>size</span><span> </span><span>=</span><span> </span><span>8</span><span>;</span><span></span></p><p><span> </span><span>rows</span><span> </span><span>=</span><span> </span><span>Array</span><span>.</span><span>from</span><span>(</span><span>{</span><span> </span><span>length</span><span>:</span><span> </span><span>this</span><span>.</span><span>size</span><span> </span><span>}</span><span>,</span><span> </span><span>(</span><span>_</span><span>,</span><span> </span><span>i</span><span>)</span><span> </span><span>=&gt;</span><span> </span><span>i</span><span>)</span><span>;</span><span></span></p><p><span> </span><span>squares</span><span> </span><span>=</span><span> </span><span>Array</span><span>.</span><span>from</span><span>(</span><span>{</span><span> </span><span>length</span><span>:</span><span> </span><span>this</span><span>.</span><span>size</span><span> </span><span>}</span><span>,</span><span> </span><span>(</span><span>_</span><span>,</span><span> </span><span>i</span><span>)</span><span> </span><span>=&gt;</span><span> </span><span>i</span><span>)</span><span>;</span><span></span></p><p><span> </span><span>constructor</span><span>(</span><span>)</span><span> </span><span>{</span><span>}</span><span></span></p><p><span> </span><span>async</span><span> </span><span>fetch</span><span>(</span><span>req</span><span>:</span><span> </span><span>Request</span><span>)</span><span>:</span><span> </span><span>Promise</span><span>&lt;</span><span>Response</span><span>&gt; </span><span>{</span><span></span></p><p><span> </span><span>const</span><span> </span><span>gameInfo</span><span> </span><span>=</span><span> </span><span>parseURL</span><span>(</span><span>req</span><span>.</span><span>url</span><span>)</span><span>;</span><span></span></p><p><span> </span><span>if</span><span> </span><span>(</span><span>gameInfo</span><span> </span><span>===</span><span> </span><span>undefined</span><span>)</span><span> </span><span>{</span><span></span></p><p><span> </span><span>return</span><span> </span><span>new</span><span> </span><span>Response</span><span>(</span><span>"Not Found"</span><span>,</span><span> </span><span>{</span><span> </span><span>status</span><span>:</span><span> </span><span>404</span><span>,</span><span> </span><span>headers</span><span>:</span><span> </span><span>{</span><span> </span><span>"cache-control"</span><span>:</span><span> </span><span>"max-age=86400, public"</span><span> </span><span>}</span><span> </span><span>}</span><span>)</span><span>;</span><span></span></p><p><span> </span><span>}</span><span></span></p><p><span> </span><span>const</span><span> </span><span>game</span><span> </span><span>=</span><span> </span><span>new</span><span> </span><span>Game</span><span>(</span><span>gameInfo</span><span>.</span><span>game</span><span>,</span><span> </span><span>gameInfo</span><span>.</span><span>selected</span><span>)</span><span>;</span><span></span></p><p><span> </span><span>return</span><span> </span><span>new</span><span> </span><span>Response</span><span>(</span><span></span></p><p><span> </span><span>renderToString</span><span>(</span><span></span></p><p><span> </span><span>&lt;</span><span>html</span><span>&gt;</span><span></span></p><p><span> </span><span>&lt;</span><span>head</span><span>&gt;</span><span></span></p><p><span> </span><span>&lt;</span><span>title</span><span>&gt;</span><span>Static Chess</span><span>&lt;/</span><span>title</span><span>&gt;</span><span></span></p><p><span> </span><span>&lt;</span><span>meta</span><span> </span><span>name</span><span>=</span><span>"viewport"</span><span> </span><span>content</span><span>=</span><span>"width=device-width, initial-scale=1.0"</span><span> </span><span>/&gt;</span><span></span></p><p><span> </span><span>&lt;</span><span>link</span><span> </span><span>rel</span><span>=</span><span>"icon"</span><span> </span><span>href</span><span>=</span><span>"https://fav.farm/♟️"</span><span> </span><span>/&gt;</span><span></span></p><p><span> </span><span>&lt;</span><span>style</span><span>&gt;</span><span>{</span><span>minify</span><span>(</span><span>CSS</span><span>)</span><span>}</span><span>&lt;/</span><span>style</span><span>&gt;</span><span></span></p><p><span> </span><span>&lt;/</span><span>head</span><span>&gt;</span><span></span></p><p><span> </span><span>&lt;</span><span>body</span><span>&gt;</span><span></span></p><p><span> </span><span>&lt;</span><span>h1</span><span>&gt;</span><span>Static Chess</span><span>&lt;/</span><span>h1</span><span>&gt;</span><span></span></p><p><span> </span><span>&lt;</span><span>div</span><span>&gt;</span><span></span></p><p><span> </span><span>&lt;</span><span>a</span><span> </span><span>href</span><span>=</span><span>"https://www.val.town/v/maxm/staticChess"</span><span>&gt;</span><span>info</span><span>&lt;/</span><span>a</span><span>&gt;</span><span> - </span><span>&lt;</span><span>a</span><span> </span><span>href</span><span>=</span><span>"/"</span><span>&gt;</span><span>reset</span><span>&lt;/</span><span>a</span><span>&gt;</span><span></span></p><p><span> </span><span>&lt;/</span><span>div</span><span>&gt;</span><span></span></p><p><span> </span><span>&lt;</span><span>div</span><span> </span><span>className</span><span>=</span><span>"board"</span><span>&gt;</span><span></span></p><p><span> </span><span>{</span><span>this</span><span>.</span><span>rows</span><span>.</span><span>map</span><span>(</span><span>row</span><span> </span><span>=&gt;</span><span> </span><span>(</span><span></span></p><p><span> </span><span>&lt;</span><span>div</span><span> </span><span>key</span><span>=</span><span>{</span><span>row</span><span>}</span><span> </span><span>className</span><span>=</span><span>"row"</span><span>&gt;</span><span>{</span><span>this</span><span>.</span><span>squares</span><span>.</span><span>map</span><span>(</span><span>square</span><span> </span><span>=&gt;</span><span> </span><span>game</span><span>.</span><span>squareContent</span><span>(</span><span>row</span><span>,</span><span> </span><span>square</span><span>)</span><span>)</span><span>}</span><span>&lt;/</span><span>div</span><span>&gt;</span><span></span></p><p><span> </span><span>)</span><span>)</span><span>}</span><span></span></p><p><span> </span><span>&lt;/</span><span>div</span><span>&gt;</span><span></span></p><p><span> </span><span>&lt;</span><span>div</span><span> </span><span>className</span><span>=</span><span>"info"</span><span>&gt;</span><span></span></p><p><span> </span><span>{</span><span>game</span><span>.</span><span>selected</span><span></span></p><p><span> </span><span>?</span><span> </span><span>"Click a highted square to move the selected piece, or select a different piece."</span><span></span></p><p><span> </span><span>:</span><span> </span><span>`It is </span><span>${</span><span>{</span><span> </span><span>w</span><span>:</span><span> </span><span>"white"</span><span>,</span><span> </span><span>b</span><span>:</span><span> </span><span>"black"</span><span> </span><span>}</span><span>[</span><span>game</span><span>.</span><span>game</span><span>.</span><span>turn</span><span>(</span><span>)</span><span>]</span><span>}</span><span>'s turn. Click a piece to make a move.`</span><span>}</span><span></span></p><p><span> </span><span>&lt;/</span><span>div</span><span>&gt;</span><span></span></p><p><span> </span><span>&lt;/</span><span>body</span><span>&gt;</span><span></span></p><p><span> </span><span>&lt;/</span><span>html</span><span>&gt;</span><span>,</span><span></span></p><p><span> </span><span>)</span><span>,</span><span></span></p><p><span> </span><span>{</span><span> </span><span>headers</span><span>:</span><span> </span><span>{</span><span> </span><span>"content-type"</span><span>:</span><span> </span><span>"text/html"</span><span>,</span><span> </span><span>"cache-control"</span><span>:</span><span> </span><span>"max-age=86400, public"</span><span> </span><span>}</span><span> </span><span>}</span><span>,</span><span></span></p><p><span> </span><span>)</span><span>;</span><span></span></p><p><span> </span><span>}</span><span></span></p><p><span></span><span>}</span><span></span></p><p><span></span><span>class</span><span> </span><span>Game</span><span> </span><span>{</span><span></span></p><p><span> </span><span>game</span><span>:</span><span> </span><span>Chess</span><span>;</span><span></span></p><p><span> </span><span>selected</span><span>?</span><span>:</span><span> </span><span>string</span><span>;</span><span></span></p><p><span> </span><span>selectable</span><span>:</span><span> </span><span>string</span><span>[</span><span>]</span><span>;</span><span></span></p><p><span> </span><span>board</span><span>;</span><span></span></p><p><span> </span><span>nextMoves</span><span>:</span><span> </span><span>{</span><span> </span><span>[</span><span>key</span><span>:</span><span> </span><span>string</span><span>]</span><span>:</span><span> </span><span>Move</span><span> </span><span>}</span><span>;</span><span></span></p><p><span> </span><span>fen</span><span>:</span><span> </span><span>string</span><span>;</span><span></span></p><p><span> </span><span>constructor</span><span>(</span><span>game</span><span>:</span><span> </span><span>Chess</span><span>,</span><span> </span><span>selected</span><span>?</span><span>:</span><span> </span><span>string</span><span>)</span><span> </span><span>{</span><span></span></p><p><span> </span><span>this</span><span>.</span><span>game</span><span> </span><span>=</span><span> </span><span>game</span><span>;</span><span></span></p><p><span> </span><span>this</span><span>.</span><span>selected</span><span> </span><span>=</span><span> </span><span>selected</span><span>;</span><span></span></p><p><span> </span><span>this</span><span>.</span><span>board</span><span> </span><span>=</span><span> </span><span>game</span><span>.</span><span>board</span><span>(</span><span>)</span><span>;</span><span></span></p><p><span> </span><span>this</span><span>.</span><span>fen</span><span> </span><span>=</span><span> </span><span>game</span><span>.</span><span>fen</span><span>(</span><span>)</span><span>.</span><span>replaceAll</span><span>(</span><span>" "</span><span>,</span><span> </span><span>"_"</span><span>)</span><span>;</span><span></span></p><p><span> </span><span>this</span><span>.</span><span>nextMoves</span><span> </span><span>=</span><span> </span><span>{</span><span>}</span><span>;</span><span></span></p><p><span> </span><span>this</span><span>.</span><span>selectable</span><span> </span><span>=</span><span> </span><span>game</span><span>.</span><span>moves</span><span>(</span><span>{</span><span> </span><span>verbose</span><span>:</span><span> </span><span>true</span><span> </span><span>}</span><span>)</span><span>.</span><span>map</span><span>(</span><span>(</span><span>m</span><span>)</span><span> </span><span>=&gt;</span><span> </span><span>m</span><span>.</span><span>from</span><span>.</span><span>toString</span><span>(</span><span>)</span><span>)</span><span>;</span><span></span></p><p><span> </span><span>if</span><span> </span><span>(</span><span>this</span><span>.</span><span>selected</span><span>)</span><span> </span><span>{</span><span></span></p><p><span> </span><span>var</span><span> </span><span>moves</span><span> </span><span>=</span><span> </span><span>game</span><span>.</span><span>moves</span><span>(</span><span>{</span><span></span></p><p><span> </span><span>square</span><span>:</span><span> </span><span>selected</span><span> </span><span>as</span><span> </span><span>Square</span><span>,</span><span></span></p><p><span> </span><span>verbose</span><span>:</span><span> </span><span>true</span><span>,</span><span></span></p><p><span> </span><span>}</span><span>)</span><span>;</span><span></span></p><p><span> </span><span>for</span><span> </span><span>(</span><span>const</span><span> </span><span>move</span><span> </span><span>of</span><span> </span><span>moves</span><span>)</span><span> </span><span>{</span><span></span></p><p><span> </span><span>this</span><span>.</span><span>nextMoves</span><span>[</span><span>move</span><span>.</span><span>to</span><span>]</span><span> </span><span>=</span><span> </span><span>move</span><span>;</span><span></span></p><p><span> </span><span>}</span><span></span></p><p><span> </span><span>}</span><span></span></p><p><span> </span><span>}</span><span></span></p><p><span> </span><span>squareContent</span><span>(</span><span>row</span><span>:</span><span> </span><span>number</span><span>,</span><span> </span><span>square</span><span>:</span><span> </span><span>number</span><span>)</span><span> </span><span>{</span><span></span></p><p><span> </span><span>const</span><span> </span><span>pos</span><span> </span><span>=</span><span> </span><span>indexToPos</span><span>(</span><span>row</span><span>,</span><span> </span><span>square</span><span>)</span><span>;</span><span></span></p><p><span> </span><span>const</span><span> </span><span>color</span><span> </span><span>=</span><span> </span><span>this</span><span>.</span><span>board</span><span>[</span><span>row</span><span>]</span><span>[</span><span>square</span><span>]</span><span>?.</span><span>color</span><span>;</span><span></span></p><p><span> </span><span>let</span><span> </span><span>className</span><span> </span><span>=</span><span> </span><span>"square"</span><span>;</span><span></span></p><p><span> </span><span>if</span><span> </span><span>(</span><span>color</span><span>)</span><span> </span><span>className</span><span> </span><span>+=</span><span> </span><span>" "</span><span> </span><span>+</span><span> </span><span>color</span><span>;</span><span></span></p><p><span> </span><span>if</span><span> </span><span>(</span><span>this</span><span>.</span><span>selected</span><span>)</span><span> </span><span>{</span><span></span></p><p><span> </span><span>if</span><span> </span><span>(</span><span>this</span><span>.</span><span>nextMoves</span><span>[</span><span>pos</span><span>]</span><span>)</span><span> </span><span>className</span><span> </span><span>+=</span><span> </span><span>" highlight"</span><span>;</span><span></span></p><p><span> </span><span>if</span><span> </span><span>(</span><span>this</span><span>.</span><span>selected</span><span> </span><span>==</span><span> </span><span>pos</span><span>)</span><span> </span><span>className</span><span> </span><span>+=</span><span> </span><span>" selected highlight"</span><span>;</span><span></span></p><p><span> </span><span>}</span><span></span></p><p><span> </span><span>const</span><span> </span><span>squareContent</span><span> </span><span>=</span><span> </span><span>(</span><span>(</span><span>)</span><span> </span><span>=&gt;</span><span> </span><span>{</span><span></span></p><p><span> </span><span>if</span><span> </span><span>(</span><span>this</span><span>.</span><span>selectable</span><span>.</span><span>includes</span><span>(</span><span>pos</span><span>)</span><span>)</span><span> </span><span>{</span><span></span></p><p><span> </span><span>return</span><span> </span><span>&lt;</span><span>a</span><span> </span><span>href</span><span>=</span><span>{</span><span>`/</span><span>${</span><span>this</span><span>.</span><span>fen</span><span>}</span><span>/</span><span>${</span><span>pos</span><span>}</span><span>`</span><span>}</span><span>&gt;</span><span>{</span><span>pieces</span><span>[</span><span>this</span><span>.</span><span>board</span><span>[</span><span>row</span><span>]</span><span>[</span><span>square</span><span>]</span><span>?.</span><span>type</span><span>]</span><span>}</span><span>&lt;/</span><span>a</span><span>&gt;</span><span>;</span><span></span></p><p><span> </span><span>}</span><span></span></p><p><span> </span><span>const</span><span> </span><span>nextMove</span><span> </span><span>=</span><span> </span><span>this</span><span>.</span><span>nextMoves</span><span>[</span><span>pos</span><span>]</span><span>;</span><span></span></p><p><span> </span><span>if</span><span> </span><span>(</span><span>nextMove</span><span> </span><span>!==</span><span> </span><span>undefined</span><span>)</span><span> </span><span>{</span><span></span></p><p><span> </span><span>return</span><span> </span><span>(</span><span></span></p><p><span> </span><span>&lt;</span><span>a</span><span> </span><span>href</span><span>=</span><span>{</span><span>`/</span><span>${</span><span>nextMove</span><span>.</span><span>after</span><span>.</span><span>replaceAll</span><span>(</span><span>" "</span><span>,</span><span> </span><span>"_"</span><span>)</span><span>}</span><span>/`</span><span>}</span><span>&gt;</span><span></span></p><p><span> </span><span>{</span><span>pieces</span><span>[</span><span>this</span><span>.</span><span>board</span><span>[</span><span>row</span><span>]</span><span>[</span><span>square</span><span>]</span><span>?.</span><span>type</span><span>]</span><span>}</span><span></span></p><p><span> </span><span>&lt;/</span><span>a</span><span>&gt;</span><span></span></p><p><span> </span><span>)</span><span>;</span><span></span></p><p><span> </span><span>}</span><span></span></p><p><span> </span><span>return</span><span> </span><span>&lt;</span><span>span</span><span>&gt;</span><span>{</span><span>pieces</span><span>[</span><span>this</span><span>.</span><span>board</span><span>[</span><span>row</span><span>]</span><span>[</span><span>square</span><span>]</span><span>?.</span><span>type</span><span>]</span><span>}</span><span>&lt;/</span><span>span</span><span>&gt;</span><span>;</span><span></span></p><p><span> </span><span>}</span><span>)</span><span>(</span><span>)</span><span>;</span></p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[The USDA's gardening zones shifted, this map shows you what's changed (445 pts)]]></title>
            <link>https://apps.npr.org/plant-hardiness-garden-map/</link>
            <guid>40342578</guid>
            <pubDate>Mon, 13 May 2024 12:32:41 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://apps.npr.org/plant-hardiness-garden-map/">https://apps.npr.org/plant-hardiness-garden-map/</a>, See on <a href="https://news.ycombinator.com/item?id=40342578">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
      <p><img alt="walking pot" src="https://apps.npr.org/plant-hardiness-garden-map/assets/synced/illo/resized/walker.gif">            
      </p>      
      
            
      <div id="sticky-nav">
          <p>2023 USDA map</p>
          <p>2012 USDA map</p>
          
        </div>
      <div id="debugger">
        <h3>Initial Load</h3><p>
          Speed for transfer:<span id="Speedfortransfer"></span>s<br>
          Speed for paint:<span id="Speedforpaint"></span>s<br>
          Total map load:<span id="loadSpeed"></span>s<br>
          Initial tiles transfered:<span id="initTiles"></span></p><h3>Cumulative</h3><p>
          Total tiles requested:   <span id="Totaltilesrequested"></span></p><h3>current slide ID: <span id="slideID"></span></h3>
        <h3>Layers Loaded</h3>
          
      </div>
      

<div id="titlecard" data-type="image">

    <p><img data-src="./assets/synced/illo/resized/walker-title.gif" alt="Animation of a cute azalea plant walking in front of the hardiness map"></p>
    
    

    <div>

      
        
      

      <hr>

      <div>
        <p>Recently, the USDA updated its plant hardiness map for the first time in 11<span>&nbsp;</span>years.</p>
<p>If you’re a gardener — and everybody can be a gardener, even on a balcony or a stoop — this is a big<span>&nbsp;</span>deal!</p>
<p>The updated map opens up new possibilities for home gardeners, but there are limits. Let’s explore how the map has changed and what this means for your<span>&nbsp;</span>garden.</p>

      </div>
    </div>
  
    

    
  </div>

<div id="intro-1" data-type="map" data-maplayer="2012_zones" data-center="[-98.04, 39.507]" data-zoom="3.8">
    

    <p>Enter your city and<span>&nbsp;</span>state:</p>

    

    

    

    
      

    
    
  </div>
<div id="zoomIn" data-type="map" data-maplayer="2012_zones"><p>In 2012, the USDA classified <span data-item="yourPlace"></span>, as Zone <span data-item="oldZone"></span>.
</p><p>Back then, <span data-item="yourPlaceShortPoss"></span> coldest winter temperature was somewhere <span data-item="tempRange-2012"></span> degrees Fahrenheit on average.</p></div>
<div id="intro-3" data-type="map" data-maplayer="2023_zones"> 
  <p>
    In 2023, <span data-item="yourPlaceShort"></span> is still rated as Zone <span data-item="newZone"></span>.
  </p>
  <div><p>
    In 2023, the USDA reclassified <span data-item="yourPlaceShort"></span> as Zone <span data-item="newZone">8a</span>. 
</p><p>Now, the lowest winter temperature is <span data-item="tempRange-2023"></span> degrees Fahrenheit on<span>&nbsp;</span>average.</p>
  </div>
</div>
<div id="intro-4" data-type="map" data-maplayer="temp_diff_layer">
    

    <div> 
  <div><p>
    Even though <span data-item="yourPlaceShortPoss">your area’s</span> zone didn’t change, that doesn’t mean the area hasn’t experienced some change in winter lows. 
</p><p>The new 30-year minimum temperature average was <span data-item="tempDiff"> 3.3 degrees F warmer</span> than the previous average, which spanned 1976 to<span>&nbsp;</span>2005.</p>
  </div>
  <p>
    That’s because the new average minimum temperature in <span data-item="yourPlaceShort"></span> is <span data-item="tempDiff"> 3.3 degrees Fahrenheit  warmer</span> than the previous average, from an earlier period.
  </p>
</div>

    

    

    
      <div>
  
    <h3>Change in lowest winter temperature</h3>
  
  <ul>
    <li data-range="-8"></li>
    <li data-range="-7"></li>
    <li data-range="-6"></li>
    <li data-range="-5"></li>
    <li data-range="-4"></li>
    <li data-range="-3"></li>
    <li data-range="-2"></li>
    <li data-range="-1"></li>
    <li data-range="0"></li>
    <li data-range="1"></li>
    <li data-range="2"></li>
    <li data-range="3"></li>
    <li data-range="4"></li>
    <li data-range="5"></li>
    <li data-range="6"></li>
    <li data-range="7"></li>
    <li data-range="8"></li>
  </ul>
  <div>
    <p>-8</p>
    
    <p>-6</p>
    
    <p>-4</p>
    
    <p>-2</p>
    
    <p>0</p>
    
    <p>+2</p>
    
    <p>+4</p>
    
    <p>+6</p>
    
    <p>+8°&nbsp;F</p>
    <!-- <p class="max">+9&deg;&nbsp;C</p> -->
  </div>

  
    <details>
      <summary>About this data</summary>
      <p>The 2012 USDA hardiness zones were calculated using the average lowest winter temperature for the observation period of 1976-2005. The new zones are calculated using the years 1991-2020. These two observation windows overlap. Colors show the difference between the two 30-year averages for each place on the<span>&nbsp;</span>map.</p>

    </details>
  
  
</div>
    

    
    
  </div>
<div id="intro-5" data-type="map" data-maplayer="temp_diff_layer">
      
      <p>Most of the changes across the country are due to the warming<span>&nbsp;</span>climate.</p>
<p>Winters are warming at a faster pace than other seasons, according to Deke Arndt, director of the National Oceanic and Atmospheric Administration’s National Centers for Environmental<span>&nbsp;</span>Information.</p>
<p>At the same time, an increase in the amount and quality of data collected at weather stations across the country helped to improve the overall accuracy of temperature readings in recent<span>&nbsp;</span>years.</p>

    </div>
<div id="transition-1">
      <p><img data-src="./assets/synced/illo/resized/rake-final.png" width="”600”" height="”400”" alt="””" src="https://apps.npr.org/plant-hardiness-garden-map/assets/synced/illo/resized/rake-final.png">
</p>
<h3>What does your hardiness zone tell<span>&nbsp;</span>you?</h3>
<p>Some people might think their hardiness zone tells them which plants they can grow. In reality, it’s a little more<span>&nbsp;</span>complicated.</p>
<p>Your zone measurement is an average of the coldest yearly temperature in your area over the past 30<span>&nbsp;</span>years.</p>

    </div>
<section id="temperature-chart" data-type="chart">  
  
  


  <div id="chart-1"><p>Here are the coldest temperatures from each winter between 1991 and 2020 in <span data-item="chartPlaceShort"></span>. 
</p><p>Though these temperature estimates differ slightly from the data the USDA used to create the zone map, we’re using them to illustrate how zones are calculated.</p></div>

  <div id="chart-2"><p>The <span>average coldest night</span> over the past 30 years was about <span data-item="avg"></span>.</p></div>

  <div id="chart-3"> 
  <div><p>
    With this average temperature, <span data-item="chartPlace"></span> might be classified as Zone <span data-item="wrongZone"></span>.	
</p><p>However, the temperatures we’re showing here are based on estimates, and in this case they do not align with the more accurate, granular data that the USDA used. The USDA map classifies this area as <span data-item="newZone"></span>.</p>
  </div>
  <p>
    With this average coldest temperature, <span data-item="chartPlaceShort"></span> is classified as Zone <span data-item="chartNewZone"></span>.
  </p>
</div>

</section>
<div id="how-to" data-type="text">
        <p>This measurement, which predicts an area’s coldest temperatures, is only useful for plants that have to survive the<span>&nbsp;</span>winter.</p>
<p>They’re called <b>perennials:</b> You plant them once and they come back after each winter if they’re given the right environment to survive. Think things like trees, shrubs and woody<span>&nbsp;</span>plants.</p>


        

        <p>And for predicting winter plant survival, knowing an area’s hardiness zone is a big help to gardeners, says Todd Rounsaville, a horticulturist with the USDA who was involved with creating the new map. He explains that the hardiness zone “is really one of the best predictors of winter survival and plant survival in general in the<span>&nbsp;</span>landscape.”</p>
<p>He advises gardeners to use the map as one very important tool of many in their risk assessment<span>&nbsp;</span>toolbox.</p>
<p>“Because the USDA map has really become the industry standard for rating things, it’s pretty rare that you will not see a zone rating on a plant, either on the tag or on a website,” he<span>&nbsp;</span>says.</p>
<p>Knowing what your average coldest temperature is helps rightsize your expectations about what might grow in your area. Live in Chicago’s <span>Zone 6a</span>? You can be assured that no citrus plants will survive your winter. Instead, try an apple tree. The apple tree is that kid you grew up with who wore shorts all winter. It needs the cold temperatures to set<span>&nbsp;</span>fruit.</p>
<p>Live in Miami’s <span>Zone 11a</span>? No apples for you. Instead, grow dragon<span>&nbsp;</span>fruit!</p>
<p><img data-src="./assets/synced/illo/resized/plant-tag.png" width="1200" height="674" alt="" src="https://apps.npr.org/plant-hardiness-garden-map/assets/synced/illo/resized/plant-tag.png">
</p>
<h3>What does your hardiness zone <u>not</u> tell you?</h3>
<p>On its own, your hardiness zone can’t tell you exactly what to grow in your<span>&nbsp;</span>area.</p>
<p>For example, parts of these three areas — Juneau, Alaska; Boston, Mass.; and Santa Fe, N.M. — are all in USDA’s <span>Zone<span>&nbsp;</span>7a</span>.</p>


        

        <p>“We know intuitively that the same plants can’t grow in these places,” Rounsaville<span>&nbsp;</span>says.</p>
<p>While Juneau may have relatively temperate winters, it also is extremely wet, averaging over 80 inches of snow a year. Santa Fe, on the other hand, is extremely dry, with much hotter summer temperatures than Juneau. Boston has both temperate winters and summers. It gets a good amount of rain but not nearly enough to sustain Juneau’s rainforest plants. It gets plenty of heat but is colder and wetter in the winter, making it inhospitable for desert dwellers, like cactuses and other<span>&nbsp;</span>succulents.</p>
<p>But all three cities rarely get below zero degrees each winter, so they are classified as the same<span>&nbsp;</span>zone.</p>
<p>So when you hear that your zone has changed, here are some things to keep in<span>&nbsp;</span>mind:</p>
<h4><span><b>1</b> The hardiness map says nothing about your <u>extreme</u> lowest temperature</span></h4>
<p>Just because your average lowest winter temperature has changed, doesn’t mean the temperature will never dip below your hardiness<span>&nbsp;</span>zone.</p>

      </div><section id="temperature-chart-return" data-type="chart" data-center="[-90.244582,38.635699]">  
  
  


  <div id="return-1">
    <p>For example, the average coldest night of the year in <u>St. Louis, Mo.,</u> tends to be around 2º F, meaning that it’s in Zone <span>7a</span>. Because St. Louis has warmed, it moved up from its <span>previous zone rating</span> of <span>6b</span>.</p>

    

    

    
  </div>

  <div id="return-2">

      <p>But notice that this is an <b><u>average</u></b> of the coldest temperature St. Louis gets each<span>&nbsp;</span>winter.</p>
<p>In the past 30 years, the temperature dropped below <span>Zone 7a</span> in at least 11 different<span>&nbsp;</span>years.</p>

    </div>

  <div id="return-3">

      <p>In 2014, the temperature dipped <span><b>three</b> half zones</span> below St. Louis’ hardiness zone, to -10º F.<span>&nbsp;</span>Brrrr!</p>
<p>Many common&nbsp;plants that are hardy down to Zone <span>7</span>, like rosemary, canna lilies or agave, would suffer significant damage or death from those temperatures, especially during a long cold<span>&nbsp;</span>snap.</p>

    </div>

</section>
<div id="limits-of-hardiness" data-type="text">
        <h4><span><b>2</b> The hardiness map says nothing about the <u>frequency</u> of extreme cold<span>&nbsp;</span>weather</span></h4>
<p>Your poor plants have to stay outside all winter, so the duration and frequency of cold weather matters for plant<span>&nbsp;</span>survival.</p>
<p>“If you’re naked and you run through a freezer, it’s not going to kill you,” says Andrew Bunting, vice president of horticulture at the Pennsylvania Horticultural Society. “If you run into the freezer and have to stay there for an extended period of time, it’s probably going to kill<span>&nbsp;</span>you.”</p>
<p><img data-src="./assets/synced/illo/resized/fig.png" width="1200" height="674" alt="" src="https://apps.npr.org/plant-hardiness-garden-map/assets/synced/illo/resized/fig.png">
</p>
<p>If extreme, out-of-zone weather occurs during a quick cold snap, steps can be taken to protect your plants with temporary blankets or other shelters. Pots can be brought<span>&nbsp;</span>inside.</p>
<p>But if the extreme lows persist, tender plants will struggle to survive. Your hardiness zone does not take any of this into<span>&nbsp;</span>account.</p>
<h4><span><b>3</b> The hardiness map can’t tell you if your plants will survive the<span>&nbsp;</span>summer</span></h4>
<p>Summer temperature extremes matter a great deal but are not reflected in the USDA hardiness<span>&nbsp;</span>map.</p>
<p>Let’s look again at <u>Juneau</u> and <u>Santa Fe,</u> much of which are in <span>Zone 7a</span>. Juneau’s all-time high temperature was 90º F in 1975. Summer days in Santa Fe routinely reach the 90s. Some shade- and cool-weather-loving plants like ferns and hostas will thrive in Juneau but struggle mightily in a place like Santa Fe. Likewise, a cactus accustomed to high temperatures would struggle to thrive in the cooler summer temperatures of Juneau, to say nothing of the overwhelming<span>&nbsp;</span>rainfall.</p>
<p>Because of this tricky problem, there have been attempts to create a corresponding map that helps gardeners know which plants might survive summer in their<span>&nbsp;</span>area.</p>
<p>In 1997, the American Horticultural Society released a heat zone map that measured the average number of times per year that the temperature of an area exceeds 86º<span>&nbsp;</span>F.</p>
<p><img data-src="./assets/synced/images/ahs-heat-map.jpg" alt="AHS Heat Zone Map (1997)" src="https://apps.npr.org/plant-hardiness-garden-map/assets/synced/images/ahs-heat-map.jpg">
<span>American Horticultural Society</span> 
</p>
<p>But this map didn’t become well known among gardeners. On a recent visit to a plant nursery outside Washington, D.C., nearly every plant tag had a USDA hardiness zone, but only one, out of the several dozen checked, had the AHS heat zone<span>&nbsp;</span>listed.</p>
<p>Above 86º F, plants from cooler climates rapidly become<span>&nbsp;</span>stressed.</p>
<p>Because of these complexities, more plant survival factors should be included in the 2023 map, says Tony Avent, who runs Juniper Level Botanic Garden and Plant Delights Nursery in Raleigh,<span>&nbsp;</span>N.C.</p>
<p>“If [these metrics] had been factored in, that would have given you a much more applicable map,” says Avent, who was a member of the committee that put together the 2012 version of the<span>&nbsp;</span>map.</p>
<p>“And that’s the part that’s a little<span>&nbsp;</span>disappointing.”</p>
<p><img data-src="./assets/synced/illo/resized/wilting.png" width="1200" height="674" alt="" src="https://apps.npr.org/plant-hardiness-garden-map/assets/synced/illo/resized/wilting.png">
</p>
<p>But including more plant survival factors in the USDA hardiness map runs the risk of creating an overly complicated map and muddying its intended use, Rounsaville<span>&nbsp;</span>says.</p>
<p>“In a perfect world, we could infinitely break down where plants will grow well, but that’s very hard to do and produce a map that is, you know, coherent but at a local resolution,” Rounsaville<span>&nbsp;</span>says.</p>
<p><img data-src="./assets/synced/illo/resized/shovel.png" width="”600”" height="”400”" alt="””" src="https://apps.npr.org/plant-hardiness-garden-map/assets/synced/illo/resized/shovel.png">
</p>
<p>Since the USDA plant hardiness zone can’t tell you everything about how a plant will fare in your garden, it’s a good idea to turn to local plant experts for guidance. Local nurseries and botanical gardens can be great resources for in-depth knowledge of the area and recent warming or cooling<span>&nbsp;</span>trends.</p>
<p>New plant varieties are constantly being bred with improvements such as increased hardiness, bloom count, bloom length or color<span>&nbsp;</span>combinations.</p>
<p>Some nursery owners like Avent enjoy experimenting with these plants.  He and his team grow many varieties of plants — both typical and unconventional — to figure out which plants they can bring to market in<span>&nbsp;</span>Raleigh.</p>
<p>“We live to kill plants,” Avent says. He estimates that they’ve killed over 50,000 plant varieties in his career. Every one they kill, they record in a<span>&nbsp;</span>database.</p>
<p><img data-src="./assets/synced/illo/resized/hose.png" width="”600”" height="”400”" alt="””" src="https://apps.npr.org/plant-hardiness-garden-map/assets/synced/illo/resized/hose.png">
</p>
<h3>If my zone changed, can I plant new things<span>&nbsp;</span>now?</h3>
<p>Maybe, and maybe you already did! It’s possible you or your neighbors may have already noticed some of these climatic changes and have been experimenting with plant varieties that were once unusual for your<span>&nbsp;</span>area.</p>
<p>Keep in mind that the new USDA map is backward looking; it represents changes that have already taken place over the past 30<span>&nbsp;</span>years.</p>
<p>In the <span>7a</span>-<span>7b</span> Philadelphia suburbs, Bunting notes two perennials that he has noticed surviving Philadelphia winters in recent<span>&nbsp;</span>years.</p>
<p>“It used to be [that] if you had a camellia, it was in a little courtyard with lots of protection, maybe even wrapped [in protective cloth] for the winter.” But now, “It’s perfectly hardy. Same with figs. People used to wrap figs. You don’t have to do that<span>&nbsp;</span>anymore.”</p>
<p>Of course, your mileage may vary. As Bunting notes, <i>where</i> you plant a perennial in your yard — whether sheltered or in the open — matters. Some areas get southern exposure and lots of sun, others are behind a house, or under a tree. Every yard has many distinct microclimates, and learning how to harness these subtle differences in your yard can help you plant more ambitious varieties with more<span>&nbsp;</span>confidence.</p>
<p>“Gardeners know that if they’re near paved surfaces or brick and mortar structures, that there’s a lot of radiant heat that those absorb during the day,” Rounsaville says. “And they can really push hardiness zones through the winter to help with plant<span>&nbsp;</span>survival.”</p>
<p><img data-src="./assets/synced/illo/resized/walking-final.png" width="1200" height="674" alt="" src="https://apps.npr.org/plant-hardiness-garden-map/assets/synced/illo/resized/walking-final.png">
</p>

      </div>

<section id="cooperative-extensions" data-type="waterfall">
  <!-- <img
    class="backdrop"
    data-src="./assets/synced/images/"
    alt=""
    style=""
  > -->

  
  <div>
    <p>Aside from local nurseries and botanic gardens, cooperative extension services can be a great place to find local gardening advice. The extension services are <a href="https://www.nifa.usda.gov/land-grant-colleges-and-universities-partner-website-directory">part of a national network</a> of local experts who provide advice on everything from agriculture to<span>&nbsp;</span>gardening.</p>
<p>NPR reached out to services in over 30 areas across the country, and many told us about changes they’ve seen in what they can and can’t plant over the past 15<span>&nbsp;</span>years.</p>

  </div>
  

<div>
    
      <div>
        <p>Phoenix</p>
        <div><p><span>Aleppo pines</span> and <span>Arizona ash</span> have shown increased issues in urban environments.
</p><p>
[Meanwhile,] plants which are strongly limited by freeze events have benefited from over a decade without severe freezes in some parts of the region.</p></div>
        <p>– Michael Chamberland, assistant agent for urban horticulture, University of Arizona, Maricopa County Cooperative Extension</p>
      </div>
    
      <div>
        <p>Amherst, Mass.</p>
        <p>Nurseries in western Massachusetts rarely carried <span>Leyland cypress</span> and <span>Japanese cryptomeria</span> but now offer them. There's nothing that I know of that can no longer be grown here due to the warming trend.</p>
        <p>– Nicholas J. Brazee, extension plant pathologist, University of Massachusetts Amherst</p>
      </div>
    
      <div>
        <p>Omaha, Neb.</p>
        <p>Because [the hardiness map]'s based on averages and we live in a part of the country that <span>experiences significant fluctuations</span> in both temperature and precipitation, we’re advising gardeners not to buy more plants for the increased hardiness zone than they can afford to lose. </p>
        <p>– Dana Freeman, horticulture program coordinator, University of Nebraska Extension, Douglas-Sarpy Counties</p>
      </div>
    
      <div>
        <p>Denver</p>
        <p>One thing I’ve noticed in my horticulture career is that <span>“global weirding”</span> — the continental climate variability we have here — does quite a bit to negate the USDA zone changes. I always want what I can’t have and persistently have things perform well even for a few years, only to be killed in late spring freezes. A plant’s ability to prosper somewhere is based upon more than the average winter temperature (i.e., USDA zone).</p>
        <p>– John Murgel, extension specialist for Douglas County, Colorado State University </p>
      </div>
    
      <div>
        <p>Minneapolis</p>
        <p>With the new USDA Plant Hardiness map, our state now recognizes larger areas of Zone 5A, particularly in urban and southwestern areas, supporting a broader palette of plants such as <span>oakleaf hydrangea</span> and <span>Japanese maple</span> that can be grown in these areas. </p>
        <p>– Brandon Miller, assistant professor; 
Julie Weisenhorn, extension horticulture educator and associate extension professor, University of Minnesota</p>
      </div>
    
      <div>
        <p>San Francisco</p>
        <p>Since our climate is more moderate in climate extremes (highs and lows) vs. say the Midwest or East Coast, these extremes have not necessarily increased significantly other than in duration and period of occurrence. We have noticed that some plants such as <span>Bougainvillea</span>, which might go into winter dormancy and be in jeopardy of damage in a freeze or cold night, no longer go into dormancy and keep many of their leaves throughout the winter.</p>
        <p>– University of California Cooperative Extension Master Gardeners, San Mateo and San Francisco Counties</p>
      </div>
    
      <div>
        <p>Dallas</p>
        <div><p>The climate change as indicated by the new zone map will have an impact in the South, especially if the area continues to experience droughts. This will result in a <span>shorter growing season.</span></p><p>
A greater impact may be noticed as you go north, where the area may experience more insect and disease pressure due to mild winters, and a shift in vegetable crops that can be grown. For example, <span>okra</span> is not common in Michigan but may be easily grown with warmer temperatures. This change may also have more impact on <span>fruiting trees such as apples</span> with a minimum chilling hours requirement that will not be met if temperatures continue to stay warmer in the winters.</p></div>
        <p>– Joe Masabni, extension vegetable specialist, Texas A&amp;M AgriLife Research and Extension Center</p>
      </div>
    
  </div>

</section>
<div id="explore" data-type="map" data-maplayer="2023_zones">
    

    <div>
      
      <p>With that, you have what you need to start a garden. Big or<span>&nbsp;</span>small.</p>
<p>Happy planting!</p>
<p><img data-src="./assets/synced/illo/planted-final.png" width="1200" height="674" alt="" src="https://apps.npr.org/plant-hardiness-garden-map/assets/synced/illo/planted-final.png">
</p>

    </div>

    

    

    

    
    

      <div>
        <p>
          Explore the map
      </p>
      <p>
        Start over with a new location
      </p>

      </div>

    
  </div>


    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[It’s an age of marvels (391 pts)]]></title>
            <link>https://blog.plover.com/tech/its-an-age-of-marvels.html</link>
            <guid>40342188</guid>
            <pubDate>Mon, 13 May 2024 11:49:28 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://blog.plover.com/tech/its-an-age-of-marvels.html">https://blog.plover.com/tech/its-an-age-of-marvels.html</a>, See on <a href="https://news.ycombinator.com/item?id=40342188">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>

   <br>

<span>Sun, 12 May 2024</span>
<p>
<a name="its-an-age-of-marvels" href="https://blog.plover.com/tech/its-an-age-of-marvels.html">It's an age of marvels</a>
<br>
</p><p>As I walk around Philadelphia I often converse with Benjamin Franklin,
to see what he thinks about how things have changed since 1790.
Sometimes he's astounded, other times less so. The things that
astound Franklin aren't always what you might think at first.
Electric streetlamps are a superb invention, and while I think
Franklin would be very pleased to see them, I don't think he would be
surprised.  Better street lighting was something everyone wanted in
Franklin's time, and
<a href="https://www.ushistory.org/franklin/autobiography/page60.htm">this was something very much on Franklin's mind</a>.
It was certainly clear that electricity could be turned into light.
Franklin could have and might have thought up the basic
mechanism of an incandescent bulb himself, although he wouldn't have
been able to make one.</p>

<p>The Internet?  Well, again yes, but no.  The complicated engineering
details are complicated engineering, but again the basic idea is
easily within the reach of the 18th century and is not all that
astounding.  They hadn't figured out Oersted's law yet, which was
crucial, but they certainly knew that you could do something at one
end of a long wire and it would have an effect at the other end, and
had an idea that that might be a way to send messages from one place
to another.  Wikipedia says that as early as 1753 people were thinking
that an electric signal could deflect a ping-pong ball at the
receiving end.  It might have worked!  If you look into the history of
transatlantic telegraph cables you will learn that the earliest
methods were almost as clunky.</p>

<p>Wikipedia itself is more impressive.  The universal encyclopedia has
long been a dream, and now we have one.  It's not always reliable,
but you know what?  Not all of <em>anything</em> is reliable.</p>

<p>An obvious winner, something sure to blow Franklin's mind is “yeah,
we've sent people to the Moon to see what it was like, they left
scientific instruments there and then they came back with rocks and
stuff.”  But that's no everyday thing, it blew everyone's mind when it
happened and it still does.  Some things I tell Franklin make him
goggle and say “We did <em>what</em>?” and I shrug modestly and say yeah,
it's pretty impressive, isn't it.  The Moon thing makes me goggle
right
back. <a href="https://store.theonion.com/products/man-walks-on-moon-front-page-poster-from-the-onions-our-dumb-century"><em>The Onion</em> nailed it</a>.</p>

<p>The really interesting stuff is the <em>everyday</em> stuff that makes
Franklin goggle.  CAT scans, for example.  Ordinary endoscopy will
interest and perhaps impress Franklin, but it won't boggle his mind.
(“Yeah, the doctor sticks a tube up your butt with an electric light
so they can see if your bowel is healthy.”  Franklin nods right
along.)  X-rays are more impressive.  (I wrote a while back about
<a href="https://blog.plover.com/tech/dental-x-rays.html">how long it took dentists to start adopting X-ray technology</a>:
about two weeks.)  But CAT scans are mind-boggling.  Oh yeah, we send
invisible rays at you from all directions, and measure how much each
one was attenuated from passing through your body, and then infer from
that exactly what must be inside and how it is all arranged.  We do
<em>what</em>?  And that's without getting into any of the details of whether
this is done by positron emission or nuclear magnetic resonance
(whatever those are, I have no idea) or something else equally
incomprehensible.  Apparently there really is something to this
quantum physics nonsense.</p>

<p>So far though the most Franklin-astounding thing I've found has been GPS.  The
explanation starts with “well, first we put 32 artificial satellites
in orbit around the Earth…”, which is already astounding, and can
derail the conversation all by itself.  But it just
goes on from there getting more and more astounding:</p>

<p>“…and each one has a  clock on board, accurate to within 40 nanoseconds…”</p>

<p>“…and can communicate the exact time wirelessly to the entire half of
the Earth that it can see…”</p>

<p>“… and because the GPS device <em>also</em> has a perfect clock, it can
compute how far it is from the satellite by comparing the two times
and multiplying by the speed of light…”</p>

<p>“… and because the satellite also tells the GPS device exactly where
it is, the device can determine that it lies on the surface of a sphere
with the satellite at the center, so with messages from three or four
satellites the device can compute its exact location, up to the error
in the clocks and other measurements…”</p>

<p>“…and it fits in my pocket.”</p>

<p>And that's not even getting into the hair-raising complications
introduced by general relativity.  “It's a bit fiddly because time
isn't passing at the same rate for the device as it is for the
satellites, but we were able to work it out.”  What.  The.  Fuck.</p>

<p>Of course not all marvels are good ones.  I sometimes explain to
Franklin that we have gotten so good at fishing&nbsp;— <em>too</em> good&nbsp;— that we
are in real danger of fishing out the oceans.  A marvel,
nevertheless.</p>

<p>A past what-the-fuck was that we know exactly how many cells
there are (959) in a particular little worm, <em>C. elegans</em>, and
<a href="https://www.wormatlas.org/celllineages.html">how each of those cells arises from the division of previous cells</a>,
as the grows from a fertilized egg, and we know what each cell does
and how they are connected, and we know that 302 of those cells are
nerve cells, and how the nerve cells are connected together.  (There
are 6,720 connections.)  The big science news on Friday was that for
the first time
<a href="https://www.science.org/doi/10.1126/science.add9330">we have done this for an insect brain</a>.
It was the <em>drosophila</em> larva, and it has 3016 neurons and 548,000
synapses.</p>

<p>Today I was reading somewhere about how most meteorites are
asteroidal, but some are from the Moon and a few are from Mars.  I
wondered “how do we know that they are from Mars?” but then I couldn't
understand
<a href="https://en.wikipedia.org/wiki/Martian_meteorite">the explanation</a>.
Someday maybe.</p>

<p>And by the way, there are only 277 known Martian meteorites.  So
today's what-the-fuck is: “Yeah, we looked at all the rocks we could
find all over the Earth and we noticed a couple hundred we found lying
around various places looked funny and we figured out they must have
come from Mars.  And when.  And how long they were on Mars before
that.”</p>

<p>Obviously, It's amazing that we know enough about Mars to
be able to say that these rocks are like the ones on Mars. (“Yeah, we
sent some devices there to look around and send back messages about
what it was like.”)  But to me, the deeper and more amazing thing is,
from looking at billions of rocks, we have learned <em>so</em> much about what
rocks are like that we can pick out, from these billions, a couple of
hundred that came to the Earth not merely from elsewhere but
<em>specifically</em> from Mars.</p>

<p>What. The. Fuck.</p>




<p>
<i>[<a href="https://blog.plover.com/tech">Other articles in category /tech</a>] 
<a href="https://blog.plover.com/tech/its-an-age-of-marvels.html">permanent link</a></i>
</p>

<br>

  </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[The danger of the mediocre success when testing startup hypotheses (2023) (150 pts)]]></title>
            <link>https://pivotal.substack.com/p/the-worst-outcome-is-a-mediocre-success</link>
            <guid>40342075</guid>
            <pubDate>Mon, 13 May 2024 11:38:15 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://pivotal.substack.com/p/the-worst-outcome-is-a-mediocre-success">https://pivotal.substack.com/p/the-worst-outcome-is-a-mediocre-success</a>, See on <a href="https://news.ycombinator.com/item?id=40342075">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><article><div dir="auto"><p>One of the wisest and most important pieces of advice I received as a startup founder was this: “The worst outcome is a mediocre success”.</p><p>Now, this is not a simplistic exhortation to go big or go home, hit a home run or strike out, be blindly ambitious, any of those things. It's subtler than that. Let me explain.</p><p>Startups are defined by uncertainty. As a founder, you have to discover almost everything about your business: What is the product? Who are your customers? How will you reach them? How much will they pay? Who are your competitors? How is the industry evolving? The list goes on.</p><p>One common way to answer these questions is, essentially, the scientific method, applied to tech startups. Frame a hypothesis; run an experiment to test the hypothesis; confirm or disprove the hypothesis; learn and iterate and learn and iterate.</p><p>For example, your hypothesis could be that ‘Cold-calling customers will lead to sales’. So you hire a couple of sales reps and tell them to cold-call 100 customers. If you get 30 new sales out of that (a terrific hit rate) — great, the hypothesis is true! You can hire more sales reps and double down on this tactic. And if you get 0 new sales — that’s also great, the hypothesis is false! You can move on to other acquisition tactics like FB ads or SEO or events or whatever. Either way, your experiment worked in confirming or disproving the hypothesis.</p><p><span>The </span><em>worst</em><span> outcome, the very worst outcome, is to get a small but non-zero number of sales — say 1 or 2. Because now you're in a bind. Do you double down or pull the plug? Does cold-calling work or not? Could it be that the method works, but the sales reps aren’t hustling enough, or they’re not following the right script, or they’re not calling the right people? Or maybe the method is flawed and your reps just got lucky? You just don't know!</span></p><p>That’s the danger of the mediocre success. The point of startup experimentation isn't the success itself; it’s the learning that comes with clear-cut success or failure. You don't really care about the sales revenue generated by your first two reps; you care about whether this is a strategy you can scale to dozens and then hundreds of reps, or whether you need to use a completely different strategy. It’s all about the learning. And mediocre successes don’t give you any learning.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff391cd33-db65-4f91-be4a-49311c477d7b_1513x811.jpeg" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff391cd33-db65-4f91-be4a-49311c477d7b_1513x811.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff391cd33-db65-4f91-be4a-49311c477d7b_1513x811.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff391cd33-db65-4f91-be4a-49311c477d7b_1513x811.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff391cd33-db65-4f91-be4a-49311c477d7b_1513x811.jpeg 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff391cd33-db65-4f91-be4a-49311c477d7b_1513x811.jpeg" width="460" height="246.42857142857142" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/f391cd33-db65-4f91-be4a-49311c477d7b_1513x811.jpeg&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:780,&quot;width&quot;:1456,&quot;resizeWidth&quot;:460,&quot;bytes&quot;:151282,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/jpeg&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:false,&quot;topImage&quot;:true,&quot;internalRedirect&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff391cd33-db65-4f91-be4a-49311c477d7b_1513x811.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff391cd33-db65-4f91-be4a-49311c477d7b_1513x811.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff391cd33-db65-4f91-be4a-49311c477d7b_1513x811.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff391cd33-db65-4f91-be4a-49311c477d7b_1513x811.jpeg 1456w" sizes="100vw" fetchpriority="high"></picture></div></a></figure></div><p>Unfortunately, there’s a natural human tendency to hedge our bets — to make design choices in our experiments such that a mediocre success is the most likely outcome. There’s also a tendency to only do experiments that you know in advance will work — but such experiments are not useful: the delta in information is close to zero. For example, and continuing the sales experiment: as a founder, you could do the calls yourself; reach out only to the very best, most qualified prospects; create custom collateral; offer sweetheart pricing. All those actions will increase the chances of closing any one deal, and as a result they’re very tempting, but do they tell you if cold-calling is a viable sales strategy at scale? Nope.</p><p>Making it worse is that we’re all heavily socialized to aim for mediocre success. Schools, universities, large organizations — they don’t want big swings and big misses; they want safety and consistency. A steady 7 is better than 10s interspersed with 0s. This might work well in structured, predictable environments, but in startup-land it’s anathema.</p><p>So when a startup comes to me with an idea for an experiment, the one thing I tell them is: make sure that there’s a well-defined distinction between success and failure. Don't fall in the messy middle. If the hypothesis fails, make sure it fails clearly and unambiguously; if it succeeds, make sure it succeeds equally clearly and unambiguously. And remember that a hypothesis failing means the experiment succeeded; you learned something. That’s what it's all about.</p><p>The worst outcome is a mediocre success!</p><p><em>Toronto, Sep 2023.</em></p><ul><li><p><span>This essay is about </span><em>tactics</em><span>, and specifically about how mediocre successes hamper your learning. There’s a whole separate essay to be written about mediocre success in the realm of </span><em>strategy</em><span>, which is largely about opportunity cost.</span></p></li><li><p>I’m experimenting with the content and form factor of this newsletter. In addition to the long-form, deep-dive pieces that I’ve written so far, I’m going to write some shorter single-topic pieces like this one, hopefully at a higher cadence. I’ll also widen the scope to include more personal anecdotes, tactical nuggets and random riffs. Let’s see how it goes!</p></li><li><p>If you enjoyed this article, please subscribe, and share it with 2-3 other folks who you think might enjoy it as well. I’d like to get more subscribers, and Twitter/X is no longer the reader-content discovery engine it once was.</p></li></ul></div></article></div></div>]]></description>
        </item>
    </channel>
</rss>