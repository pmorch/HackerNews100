<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0">
    <channel>
        <title>HN100 - Readable Contents</title>
        <link>https://hn.algolia.com/api/v1/search_by_date?tags=%28story,poll%29&amp;numericFilters=points%3E100</link>
        <description>Uses Readability to add bodies to the RSS feed</description>
        <lastBuildDate>Sat, 23 Dec 2023 12:00:03 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>en</language>
        <item>
            <title><![CDATA[They want you to forget what a film looks like (159 pts)]]></title>
            <link>https://aftermath.site/true-lies-4k-uhd-blu-ray-james-cameron-peter-jackson-park-road-post</link>
            <guid>38741536</guid>
            <pubDate>Sat, 23 Dec 2023 04:06:42 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://aftermath.site/true-lies-4k-uhd-blu-ray-james-cameron-peter-jackson-park-road-post">https://aftermath.site/true-lies-4k-uhd-blu-ray-james-cameron-peter-jackson-park-road-post</a>, See on <a href="https://news.ycombinator.com/item?id=38741536">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>The most grotesque videos on YouTube fit into a specific category. It consists of old footage run through an AI upscaler. Sometimes the videos are colorized, sometimes they’re interpolated to 60 frames per second. Uniformly they look atrocious, smeary, and garish, unless you don’t know what a film is supposed to look like. Increasingly that’s a lot of people, including, evidently, the people responsible for the latest transfers of the movie <em>True Lies</em> and to a lesser extent <em>Aliens</em>, <em>The Abyss</em>, and <em>Titanic</em>.</p><div><figure><p><iframe title="[4K, 60 fps, color] The lumiere family goes on a trip.1895." width="710" height="399" src="https://www.youtube.com/embed/GHHt-CBKkS0?feature=oembed" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen=""></iframe></p><figcaption>There's like a million of these and they're all uniformly disgusting.</figcaption></figure></div><p>If you have never seen the movie <em>True Lies</em>, you are probably under 30. Arnold Schwarzenegger plays an agent in a government counter-terrorist organization that has to fight bad guys while keeping his dangerous lifestyle a secret from his mousey wife played by Jamie Lee Curtis. It’s a goofy big budget action comedy remake that has some fantastic set pieces, an incredible cast, some great bits, and ages exactly how you would expect from an airheaded action movie from 1994. In James Cameron’s career, It’s probably the weirdest movie James Cameron ever made outside of <em>Piranha II: The Spawning</em>.</p><figure><img alt="" src="https://lede-admin.aftermath.site/wp-content/uploads/sites/55/2023/12/True.Lies_.1994.2160p.MA_.WEB-DL.DDP5_.1.Atmos_.DV_.HDR_.H.265-FLUX-00_43_12.png?w=710" srcset="https://lede-admin.aftermath.site/wp-content/uploads/sites/55/2023/12/True.Lies_.1994.2160p.MA_.WEB-DL.DDP5_.1.Atmos_.DV_.HDR_.H.265-FLUX-00_43_12.png?w=425&amp;quality=75 425w, https://lede-admin.aftermath.site/wp-content/uploads/sites/55/2023/12/True.Lies_.1994.2160p.MA_.WEB-DL.DDP5_.1.Atmos_.DV_.HDR_.H.265-FLUX-00_43_12.png?w=850&amp;quality=75 850w, https://lede-admin.aftermath.site/wp-content/uploads/sites/55/2023/12/True.Lies_.1994.2160p.MA_.WEB-DL.DDP5_.1.Atmos_.DV_.HDR_.H.265-FLUX-00_43_12.png?w=585&amp;quality=75 585w, https://lede-admin.aftermath.site/wp-content/uploads/sites/55/2023/12/True.Lies_.1994.2160p.MA_.WEB-DL.DDP5_.1.Atmos_.DV_.HDR_.H.265-FLUX-00_43_12.png?w=1170&amp;quality=75 1170w, https://lede-admin.aftermath.site/wp-content/uploads/sites/55/2023/12/True.Lies_.1994.2160p.MA_.WEB-DL.DDP5_.1.Atmos_.DV_.HDR_.H.265-FLUX-00_43_12.png?w=710&amp;quality=75 710w, https://lede-admin.aftermath.site/wp-content/uploads/sites/55/2023/12/True.Lies_.1994.2160p.MA_.WEB-DL.DDP5_.1.Atmos_.DV_.HDR_.H.265-FLUX-00_43_12.png?w=1420&amp;quality=75 1420w" sizes="(max-width: 30rem) 425px, (min-width: 30rem) and (max-width: 40rem) 585px, (min-width: 40rem) 710px" loading="lazy"><figcaption>The Folders. The pictures on the left. It all looks bad. Credit: 20th Century Fox.</figcaption></figure><p>I have a deep fondness for this movie in my heart, partially because I played the <a href="https://en.wikipedia.org/wiki/True_Lies_(video_game)" target="_blank" rel="noreferrer noopener">pretty bad Super Nintendo</a> game too much but also because it’s rare to get a comedy where the hero airholes the side of a skyscraper with a harrier jet. So when I saw people posting that it had gotten a garish transfer, I was particularly insulted. But even with prior warning, I was unprepared for how disgusting it looked.</p><figure><img alt="" src="https://lede-admin.aftermath.site/wp-content/uploads/sites/55/2023/12/True.Lies_.1994.2160p.MA_.WEB-DL.DDP5_.1.Atmos_.DV_.HDR_.H.265-FLUX-00_52_09.jpg?w=710" srcset="https://lede-admin.aftermath.site/wp-content/uploads/sites/55/2023/12/True.Lies_.1994.2160p.MA_.WEB-DL.DDP5_.1.Atmos_.DV_.HDR_.H.265-FLUX-00_52_09.jpg?w=425&amp;quality=75 425w, https://lede-admin.aftermath.site/wp-content/uploads/sites/55/2023/12/True.Lies_.1994.2160p.MA_.WEB-DL.DDP5_.1.Atmos_.DV_.HDR_.H.265-FLUX-00_52_09.jpg?w=850&amp;quality=75 850w, https://lede-admin.aftermath.site/wp-content/uploads/sites/55/2023/12/True.Lies_.1994.2160p.MA_.WEB-DL.DDP5_.1.Atmos_.DV_.HDR_.H.265-FLUX-00_52_09.jpg?w=585&amp;quality=75 585w, https://lede-admin.aftermath.site/wp-content/uploads/sites/55/2023/12/True.Lies_.1994.2160p.MA_.WEB-DL.DDP5_.1.Atmos_.DV_.HDR_.H.265-FLUX-00_52_09.jpg?w=1170&amp;quality=75 1170w, https://lede-admin.aftermath.site/wp-content/uploads/sites/55/2023/12/True.Lies_.1994.2160p.MA_.WEB-DL.DDP5_.1.Atmos_.DV_.HDR_.H.265-FLUX-00_52_09.jpg?w=710&amp;quality=75 710w, https://lede-admin.aftermath.site/wp-content/uploads/sites/55/2023/12/True.Lies_.1994.2160p.MA_.WEB-DL.DDP5_.1.Atmos_.DV_.HDR_.H.265-FLUX-00_52_09.jpg?w=1420&amp;quality=75 1420w" sizes="(max-width: 30rem) 425px, (min-width: 30rem) and (max-width: 40rem) 585px, (min-width: 40rem) 710px" loading="lazy"><figcaption>It looks like they airbrushed her soul. Credit: 20th Century FOX</figcaption></figure><p>The transfer of <em>True Lies</em> has a truly vile quality to it, a feeling like someone clandestinely dosed you with LSD just a hair below the threshold. At times it can look passable in motion, but then you notice something out of the corner of your eye: a thick fold of skin, a framed photo of a child, folders that are too thick at the margins, cheeks that look rendered. It’s that familiar dread at the pit of your gut when you spot AI generated imagery, a combination of edges not looking quite right and surfaces that are simultaneously too smooth and too sharp. A crime was committed here, and you can tell.</p><figure><img alt="" src="https://lede-admin.aftermath.site/wp-content/uploads/sites/55/2023/12/Aliens.1986.Special.Edition.2160p.WEB-DL.DDP5_.1.HDR_.H.265-CanadianSupervillainJamesCameron-01_04_14-1.png?w=710" srcset="https://lede-admin.aftermath.site/wp-content/uploads/sites/55/2023/12/Aliens.1986.Special.Edition.2160p.WEB-DL.DDP5_.1.HDR_.H.265-CanadianSupervillainJamesCameron-01_04_14-1.png?w=425&amp;quality=75 425w, https://lede-admin.aftermath.site/wp-content/uploads/sites/55/2023/12/Aliens.1986.Special.Edition.2160p.WEB-DL.DDP5_.1.HDR_.H.265-CanadianSupervillainJamesCameron-01_04_14-1.png?w=850&amp;quality=75 850w, https://lede-admin.aftermath.site/wp-content/uploads/sites/55/2023/12/Aliens.1986.Special.Edition.2160p.WEB-DL.DDP5_.1.HDR_.H.265-CanadianSupervillainJamesCameron-01_04_14-1.png?w=585&amp;quality=75 585w, https://lede-admin.aftermath.site/wp-content/uploads/sites/55/2023/12/Aliens.1986.Special.Edition.2160p.WEB-DL.DDP5_.1.HDR_.H.265-CanadianSupervillainJamesCameron-01_04_14-1.png?w=1170&amp;quality=75 1170w, https://lede-admin.aftermath.site/wp-content/uploads/sites/55/2023/12/Aliens.1986.Special.Edition.2160p.WEB-DL.DDP5_.1.HDR_.H.265-CanadianSupervillainJamesCameron-01_04_14-1.png?w=710&amp;quality=75 710w, https://lede-admin.aftermath.site/wp-content/uploads/sites/55/2023/12/Aliens.1986.Special.Edition.2160p.WEB-DL.DDP5_.1.HDR_.H.265-CanadianSupervillainJamesCameron-01_04_14-1.png?w=1420&amp;quality=75 1420w" sizes="(max-width: 30rem) 425px, (min-width: 30rem) and (max-width: 40rem) 585px, (min-width: 40rem) 710px" loading="lazy"><figcaption>Game over. Credit: 20th Century Fox.</figcaption></figure><p>The transfers of <em>Aliens </em>and <em>The Abyss </em>are markedly less bad than <em>True Lies</em>, but I still have difficulty watching them. The skin looks sterile and waxy with too much film grain removed. Everything looks like it has raytracing on. Both transfers are, however, within acceptable parameters for most normal people.&nbsp;</p><p>The recent transfer of <em>Titanic </em>got a similar treatment, with <a href="https://forum.blu-ray.com/showthread.php?t=302519" target="_blank" rel="noreferrer noopener">similarly mixed reactions online</a>.</p><figure><img alt="" src="https://lede-admin.aftermath.site/wp-content/uploads/sites/55/2023/12/Titanic-1997-2160p-UHD-BluRay-x265-DV-HDR-DDP-7.1-English-Weasley-HONE-01_19_33.png?w=710" srcset="https://lede-admin.aftermath.site/wp-content/uploads/sites/55/2023/12/Titanic-1997-2160p-UHD-BluRay-x265-DV-HDR-DDP-7.1-English-Weasley-HONE-01_19_33.png?w=425&amp;quality=75 425w, https://lede-admin.aftermath.site/wp-content/uploads/sites/55/2023/12/Titanic-1997-2160p-UHD-BluRay-x265-DV-HDR-DDP-7.1-English-Weasley-HONE-01_19_33.png?w=850&amp;quality=75 850w, https://lede-admin.aftermath.site/wp-content/uploads/sites/55/2023/12/Titanic-1997-2160p-UHD-BluRay-x265-DV-HDR-DDP-7.1-English-Weasley-HONE-01_19_33.png?w=585&amp;quality=75 585w, https://lede-admin.aftermath.site/wp-content/uploads/sites/55/2023/12/Titanic-1997-2160p-UHD-BluRay-x265-DV-HDR-DDP-7.1-English-Weasley-HONE-01_19_33.png?w=1170&amp;quality=75 1170w, https://lede-admin.aftermath.site/wp-content/uploads/sites/55/2023/12/Titanic-1997-2160p-UHD-BluRay-x265-DV-HDR-DDP-7.1-English-Weasley-HONE-01_19_33.png?w=710&amp;quality=75 710w, https://lede-admin.aftermath.site/wp-content/uploads/sites/55/2023/12/Titanic-1997-2160p-UHD-BluRay-x265-DV-HDR-DDP-7.1-English-Weasley-HONE-01_19_33.png?w=1420&amp;quality=75 1420w" sizes="(max-width: 30rem) 425px, (min-width: 30rem) and (max-width: 40rem) 585px, (min-width: 40rem) 710px" loading="lazy"><figcaption>Rose lookin' like a Pixar character over here. <em>Credit: 20th Century Fox</em>.</figcaption></figure><p>“Why would you do this?” is a logical question. It’s worth contextualizing who handled these “restorations” – namely Park Road Post, a subsidiary of Peter Jackson’s WingNut Films. They have worked on multiple films in the past, but the two that are most germane here are Jackson’s <em>They Shall Not Grow Old</em> and the 3-part Disney+ documentary <em>The Beatles: Get Back. </em>Both movies recontextualize pre-existing footage and, importantly, do so with an aggressive use of machine learning. <em>They Shall Not Grow Old </em>upscales and colorizes old World War I imagery in an attempt to set the bloodshed in a more modern context, while <em>Get Back</em> recycled footage shot for Michael Lindsay-Hogg’s <em>Let It Be</em>, including moments never before seen by the public, to elucidate the process behind the creation of some of The Beatles’ most iconic songs.&nbsp;</p><p>I understand the intent of using machine learning in both works. In the case of Hogg’s <em>Let it Be</em>,<em> </em>much of the footage was chunky and rough, and they used audio isolation narratively in an interesting way.</p><figure><img alt="" src="https://lede-admin.aftermath.site/wp-content/uploads/sites/55/2023/12/They.Shall_.Not_.Grow_.Old_.2018.1080p.BluRay.Remux_.AVC_.DTS-HD.MA_.5.1-PmP-00_43_55-1.jpg?w=710" srcset="https://lede-admin.aftermath.site/wp-content/uploads/sites/55/2023/12/They.Shall_.Not_.Grow_.Old_.2018.1080p.BluRay.Remux_.AVC_.DTS-HD.MA_.5.1-PmP-00_43_55-1.jpg?w=425&amp;quality=75 425w, https://lede-admin.aftermath.site/wp-content/uploads/sites/55/2023/12/They.Shall_.Not_.Grow_.Old_.2018.1080p.BluRay.Remux_.AVC_.DTS-HD.MA_.5.1-PmP-00_43_55-1.jpg?w=850&amp;quality=75 850w, https://lede-admin.aftermath.site/wp-content/uploads/sites/55/2023/12/They.Shall_.Not_.Grow_.Old_.2018.1080p.BluRay.Remux_.AVC_.DTS-HD.MA_.5.1-PmP-00_43_55-1.jpg?w=585&amp;quality=75 585w, https://lede-admin.aftermath.site/wp-content/uploads/sites/55/2023/12/They.Shall_.Not_.Grow_.Old_.2018.1080p.BluRay.Remux_.AVC_.DTS-HD.MA_.5.1-PmP-00_43_55-1.jpg?w=1170&amp;quality=75 1170w, https://lede-admin.aftermath.site/wp-content/uploads/sites/55/2023/12/They.Shall_.Not_.Grow_.Old_.2018.1080p.BluRay.Remux_.AVC_.DTS-HD.MA_.5.1-PmP-00_43_55-1.jpg?w=710&amp;quality=75 710w, https://lede-admin.aftermath.site/wp-content/uploads/sites/55/2023/12/They.Shall_.Not_.Grow_.Old_.2018.1080p.BluRay.Remux_.AVC_.DTS-HD.MA_.5.1-PmP-00_43_55-1.jpg?w=1420&amp;quality=75 1420w" sizes="(max-width: 30rem) 425px, (min-width: 30rem) and (max-width: 40rem) 585px, (min-width: 40rem) 710px" loading="lazy"><figcaption>They should have called it They Shall Grow Really Weird Looking. <em>Credit: Warner Bros. Pictures</em></figcaption></figure><p>Unfortunately they both look<em> really fucking bad</em>. And they look worse as the years go on.&nbsp;</p><p><em>They Shall Not Grow Old</em> is difficult to stomach, with the soldiers being motion interpolated in a melting, shambolic manner. The digital noise reduction is inconsistent – film grain is present on the skin of soldiers and absent in other places, following their faces like reptilian scales. This is an enthusiastic, clumsy use of a technology on severely damaged footage.&nbsp;</p><figure><img alt="" src="https://lede-admin.aftermath.site/wp-content/uploads/sites/55/2023/12/The.Beatles.Get_.Back_.S01E01.Part_.1.Days_.1-7.2160p.WEB-DL.DDP5_.1.HEVC-TEPES-00_15_55.jpg?w=710" srcset="https://lede-admin.aftermath.site/wp-content/uploads/sites/55/2023/12/The.Beatles.Get_.Back_.S01E01.Part_.1.Days_.1-7.2160p.WEB-DL.DDP5_.1.HEVC-TEPES-00_15_55.jpg?w=425&amp;quality=75 425w, https://lede-admin.aftermath.site/wp-content/uploads/sites/55/2023/12/The.Beatles.Get_.Back_.S01E01.Part_.1.Days_.1-7.2160p.WEB-DL.DDP5_.1.HEVC-TEPES-00_15_55.jpg?w=850&amp;quality=75 850w, https://lede-admin.aftermath.site/wp-content/uploads/sites/55/2023/12/The.Beatles.Get_.Back_.S01E01.Part_.1.Days_.1-7.2160p.WEB-DL.DDP5_.1.HEVC-TEPES-00_15_55.jpg?w=585&amp;quality=75 585w, https://lede-admin.aftermath.site/wp-content/uploads/sites/55/2023/12/The.Beatles.Get_.Back_.S01E01.Part_.1.Days_.1-7.2160p.WEB-DL.DDP5_.1.HEVC-TEPES-00_15_55.jpg?w=1170&amp;quality=75 1170w, https://lede-admin.aftermath.site/wp-content/uploads/sites/55/2023/12/The.Beatles.Get_.Back_.S01E01.Part_.1.Days_.1-7.2160p.WEB-DL.DDP5_.1.HEVC-TEPES-00_15_55.jpg?w=710&amp;quality=75 710w, https://lede-admin.aftermath.site/wp-content/uploads/sites/55/2023/12/The.Beatles.Get_.Back_.S01E01.Part_.1.Days_.1-7.2160p.WEB-DL.DDP5_.1.HEVC-TEPES-00_15_55.jpg?w=1420&amp;quality=75 1420w" sizes="(max-width: 30rem) 425px, (min-width: 30rem) and (max-width: 40rem) 585px, (min-width: 40rem) 710px" loading="lazy"><figcaption>Mods are asleep, post smooth Paul. <em>Credit: Disney+</em></figcaption></figure><p>The same is true to a lesser extent for <em>The Beatles: Get Back</em>. Nobody would begrudge Jackson for color grading and restoring rough footage, but the effect is sterilizing and alien. Hair and fur coats are simultaneously shiny and oily, with gorgon-esque strands that undulate and melt into themselves. The folds of skin and clothing have an unnatural heft. The edges of objects will catch and melt into each other. The grain structure is slightly more natural than in <em>They Shall Not Grow Old</em>, but it looks like it was artificially added over heavily denoised footage. <em>Get Back </em>was wildly successful, in part because the only semi-available version of <em>Let It Be</em> is an atrocious DVD transfer, but also on the strength of 60 hours of unused footage, an unseen insight into an iconic band.</p><figure><img alt="" src="https://lede-admin.aftermath.site/wp-content/uploads/sites/55/2023/12/The-Beatles-Now-And-Then-Official-Music-Video-00.01.52.420.jpeg?w=710" srcset="https://lede-admin.aftermath.site/wp-content/uploads/sites/55/2023/12/The-Beatles-Now-And-Then-Official-Music-Video-00.01.52.420.jpeg?w=425&amp;quality=75 425w, https://lede-admin.aftermath.site/wp-content/uploads/sites/55/2023/12/The-Beatles-Now-And-Then-Official-Music-Video-00.01.52.420.jpeg?w=850&amp;quality=75 850w, https://lede-admin.aftermath.site/wp-content/uploads/sites/55/2023/12/The-Beatles-Now-And-Then-Official-Music-Video-00.01.52.420.jpeg?w=585&amp;quality=75 585w, https://lede-admin.aftermath.site/wp-content/uploads/sites/55/2023/12/The-Beatles-Now-And-Then-Official-Music-Video-00.01.52.420.jpeg?w=1170&amp;quality=75 1170w, https://lede-admin.aftermath.site/wp-content/uploads/sites/55/2023/12/The-Beatles-Now-And-Then-Official-Music-Video-00.01.52.420.jpeg?w=710&amp;quality=75 710w, https://lede-admin.aftermath.site/wp-content/uploads/sites/55/2023/12/The-Beatles-Now-And-Then-Official-Music-Video-00.01.52.420.jpeg?w=1420&amp;quality=75 1420w" sizes="(max-width: 30rem) 425px, (min-width: 30rem) and (max-width: 40rem) 585px, (min-width: 40rem) 710px" loading="lazy"><figcaption>They should have let it be. <em>Credit: WingNut Films Productions Ltd</em>.</figcaption></figure><p>I wish we had stopped Jackson then and there. As my good friend Danielle joked, this was a trial balloon. People praised Jackson for doing this to Lindsay-Hogg’s footage in the name of restoration, and it emboldened him to do worse things. Before the <em>True Lies </em>debacle, the most recent example of this was the aggressively saccharine and confusing <em>Now &amp; Then</em>, a long unfinished demo now finished by Ringo and Paul, edited together with archival footage of younger John and George composited in an a fashion that can be charitably described as tremendously weird.</p><div><figure><p><iframe title="The Beatles - Now And Then (Official Music Video)" width="710" height="399" src="https://www.youtube.com/embed/Opxhh9Oh3rg?feature=oembed" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen=""></iframe></p><figcaption>Every part of this is strange.</figcaption></figure></div><p>Lest I am accused of being a luddite, I firmly believe there are many use cases for this technology. Nvidia’s DLSS and competing variants generally work very well on the games they are trained on. I regularly use <a href="https://nmkd.itch.io/flowframes" target="_blank" rel="noreferrer noopener">Flowframes</a> in the rare case that I need interpolation. I have often used waifu2x and now <a href="https://github.com/chaiNNer-org/chaiNNer" target="_blank" rel="noreferrer noopener">chainner</a> if I need to photoshop a still and my source is bad, and there are databases of countless <a href="https://openmodeldb.info/" target="_blank" rel="noreferrer noopener">AI upscaling models</a>. But the flip side to this is that these technologies are often used in place of proper ingest. “Crap in, crap out” is a truism for a reason. I spend a lot of time regularly capturing VHS and Laserdisc at the highest possible quality for fun, and when I see people who should know better say “Just use Topaz” (a commercial AI upscaler) instead of learning how to correctly ingest footage and deinterlace it, it makes me want to pull out my hair, because it almost uniformly looks bad to anyone who works with video professionally.&nbsp;</p><div><figure><p><iframe title="The Cure - Catch &amp; Why Can't I Be You? (Live) - The Tube (1987)" width="710" height="533" src="https://www.youtube.com/embed/ks1wE_NXWv8?feature=oembed" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen=""></iframe></p><figcaption>This is captured on a home VHS with <a href="https://github.com/oyvindln/vhs-decode">VHS-Decode,</a> and then upscaled to 4k for YouTube and deinterlaced to 50 FPS <a href="http://avisynth.nl/index.php/QTGMC">QTGMC</a>, and as a result it looks great.</figcaption></figure></div><p>When you finally do see a piece of footage transferred well, it can be breathtaking. Good archival practices require a lot of institutional knowledge and labor. It’s an art when done well, and the people who do it care so much about what they do. But the modern application of much of AI is precisely about taking labor out of the equation. Why transfer a tape correctly when we can just have a computer guess badly instead? What if crap goes in, and it doesn’t come out?</p><figure><img alt="" src="https://lede-admin.aftermath.site/wp-content/uploads/sites/55/2023/12/neck.jpg?w=710" srcset="https://lede-admin.aftermath.site/wp-content/uploads/sites/55/2023/12/neck.jpg?w=425&amp;quality=75 425w, https://lede-admin.aftermath.site/wp-content/uploads/sites/55/2023/12/neck.jpg?w=850&amp;quality=75 850w, https://lede-admin.aftermath.site/wp-content/uploads/sites/55/2023/12/neck.jpg?w=585&amp;quality=75 585w, https://lede-admin.aftermath.site/wp-content/uploads/sites/55/2023/12/neck.jpg?w=1170&amp;quality=75 1170w, https://lede-admin.aftermath.site/wp-content/uploads/sites/55/2023/12/neck.jpg?w=710&amp;quality=75 710w, https://lede-admin.aftermath.site/wp-content/uploads/sites/55/2023/12/neck.jpg?w=1420&amp;quality=75 1420w" sizes="(max-width: 30rem) 425px, (min-width: 30rem) and (max-width: 40rem) 585px, (min-width: 40rem) 710px" loading="lazy"><figcaption>Neck. NECK! <em>Credit: 20th Century Fox</em></figcaption></figure><p>What makes all of this worse is that <em>True Lies</em>, as I understand it, did not need to be shoved through the AI wringer. According to <a href="https://thedigitalbits.com/item/true-lies-2023-digital-uhd" target="_blank" rel="noreferrer noopener">The Digital Bits</a>, Park Road Post had a recent 4k scan of <em>True Lies </em>from the original camera negative. Park Road Post’s own website claims they have a Lasergraphics Director 10K film scanner on the premises. So what is the purpose of adding AI to this mix? Why do that to a perfectly fine-looking film? What is gained here, other than to slightly yassify an Arnold film? At this point, maybe they are simply doing it just to say that they did, because the technology is lying around, like a loaded gun with the safety off.</p><p>Nerds who post on blu-ray forums as a rule often need to calm down, and the forum threads I have read about this are no exception, but there are certain cases where a filmmaker is just wrong about how their films should look. Lucas is the infamous notable example, but Cameron is not innocent here in his treatment of his own films. Wong Kar-wai is another notable example, as what he did to <em>Ashes of Time</em> is criminal as was his recent “remasters” of his movies like<em> In The Mood For Love</em>. In certain rare conditions like this, it’s healthy to question if directors have the best interests of their own films in mind, <a href="https://thedigitalbits.com/item/titanic-4k-25thle-uhd-2023" target="_blank" rel="noreferrer noopener">as Cameron himself personally approved of these remasters</a>.</p><figure><img alt="" src="https://lede-admin.aftermath.site/wp-content/uploads/sites/55/2023/12/In.the_.Mood_.for_.Love_.2000.1080p.UHD_.BluRay.DD5_.1.x264-ZQ-00_01_14.png?w=710" srcset="https://lede-admin.aftermath.site/wp-content/uploads/sites/55/2023/12/In.the_.Mood_.for_.Love_.2000.1080p.UHD_.BluRay.DD5_.1.x264-ZQ-00_01_14.png?w=425&amp;quality=75 425w, https://lede-admin.aftermath.site/wp-content/uploads/sites/55/2023/12/In.the_.Mood_.for_.Love_.2000.1080p.UHD_.BluRay.DD5_.1.x264-ZQ-00_01_14.png?w=850&amp;quality=75 850w, https://lede-admin.aftermath.site/wp-content/uploads/sites/55/2023/12/In.the_.Mood_.for_.Love_.2000.1080p.UHD_.BluRay.DD5_.1.x264-ZQ-00_01_14.png?w=585&amp;quality=75 585w, https://lede-admin.aftermath.site/wp-content/uploads/sites/55/2023/12/In.the_.Mood_.for_.Love_.2000.1080p.UHD_.BluRay.DD5_.1.x264-ZQ-00_01_14.png?w=1170&amp;quality=75 1170w, https://lede-admin.aftermath.site/wp-content/uploads/sites/55/2023/12/In.the_.Mood_.for_.Love_.2000.1080p.UHD_.BluRay.DD5_.1.x264-ZQ-00_01_14.png?w=710&amp;quality=75 710w, https://lede-admin.aftermath.site/wp-content/uploads/sites/55/2023/12/In.the_.Mood_.for_.Love_.2000.1080p.UHD_.BluRay.DD5_.1.x264-ZQ-00_01_14.png?w=1420&amp;quality=75 1420w" sizes="(max-width: 30rem) 425px, (min-width: 30rem) and (max-width: 40rem) 585px, (min-width: 40rem) 710px" loading="lazy"><figcaption>Why did you change the color of your movie like this? <em>Credit: Block 2 Pictures</em></figcaption></figure><p>What actually chills my blood more than anything is the thought that a lot of people think this all looks pretty good. You see this mindset at work whenever an AI fetishist posts a stable diffusion image of a woman with 13 fingers, 40 incisors and comically huge breasts. There’s an entire portion of the population that takes overt pleasure in the over-smoothed, perverts that prefer all media to be fast, high frame rate, and scrubbed squeaky clean. The cameras on our phones don’t simply capture images anymore, <a href="https://www.newyorker.com/culture/infinite-scroll/have-iphone-cameras-become-too-smart" target="_blank" rel="noreferrer noopener">they compute them and ‘optimize’ them</a>. It’s Italian Futurism in 4k, a noise reduction death drive. It’s not simply enough for much of digital cinema to look crystal clear and lifeless; the past should be denoised, grain managed and cleaned to conform to that standard. It is expedient and profitable if people don’t remember what film is supposed to look like.</p><figure><img alt="" src="https://lede-admin.aftermath.site/wp-content/uploads/sites/55/2023/12/Aliens.1986.Special.Edition.2160p.WEB-DL.DDP5_.1.HDR_.H.265-CanadianSupervillainJamesCameron-01_16_00.png?w=710" srcset="https://lede-admin.aftermath.site/wp-content/uploads/sites/55/2023/12/Aliens.1986.Special.Edition.2160p.WEB-DL.DDP5_.1.HDR_.H.265-CanadianSupervillainJamesCameron-01_16_00.png?w=425&amp;quality=75 425w, https://lede-admin.aftermath.site/wp-content/uploads/sites/55/2023/12/Aliens.1986.Special.Edition.2160p.WEB-DL.DDP5_.1.HDR_.H.265-CanadianSupervillainJamesCameron-01_16_00.png?w=850&amp;quality=75 850w, https://lede-admin.aftermath.site/wp-content/uploads/sites/55/2023/12/Aliens.1986.Special.Edition.2160p.WEB-DL.DDP5_.1.HDR_.H.265-CanadianSupervillainJamesCameron-01_16_00.png?w=585&amp;quality=75 585w, https://lede-admin.aftermath.site/wp-content/uploads/sites/55/2023/12/Aliens.1986.Special.Edition.2160p.WEB-DL.DDP5_.1.HDR_.H.265-CanadianSupervillainJamesCameron-01_16_00.png?w=1170&amp;quality=75 1170w, https://lede-admin.aftermath.site/wp-content/uploads/sites/55/2023/12/Aliens.1986.Special.Edition.2160p.WEB-DL.DDP5_.1.HDR_.H.265-CanadianSupervillainJamesCameron-01_16_00.png?w=710&amp;quality=75 710w, https://lede-admin.aftermath.site/wp-content/uploads/sites/55/2023/12/Aliens.1986.Special.Edition.2160p.WEB-DL.DDP5_.1.HDR_.H.265-CanadianSupervillainJamesCameron-01_16_00.png?w=1420&amp;quality=75 1420w" sizes="(max-width: 30rem) 425px, (min-width: 30rem) and (max-width: 40rem) 585px, (min-width: 40rem) 710px" loading="lazy"><figcaption>That's too much detail. <em>Credit: 20th Century Fox</em></figcaption></figure><p>I don’t think anyone gets into preservation to destroy film. I believe that everyone involved with this process worked hard and had the best interests of the film in mind, but the exact nature of restoration itself can vary wildly. I believe that some companies get blinded by new tech, get high on their own supply, and that can result in work that is destructive instead of restorative. I don’t know what the solution to this is in the world we live in, outside of decoupling film preservation from the profit motive whenever possible.</p><p>But I am certain about one thing. For a while, much of gaming tried looking like <em>Aliens</em>. Now, <em>Aliens</em> looks like a video game. And that doesn’t sit right with me.</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[How big is YouTube? (543 pts)]]></title>
            <link>https://ethanzuckerman.com/2023/12/22/how-big-is-youtube/</link>
            <guid>38739563</guid>
            <pubDate>Fri, 22 Dec 2023 22:55:19 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://ethanzuckerman.com/2023/12/22/how-big-is-youtube/">https://ethanzuckerman.com/2023/12/22/how-big-is-youtube/</a>, See on <a href="https://news.ycombinator.com/item?id=38739563">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
		
		
	<main id="content" role="main">

	<div>
						<article id="post-6596">
				<div><p>How big is YouTube?</p>
<p>I got interested in this question a few years ago, when I started writing about the <a href="https://theconversation.com/facebook-has-a-misinformation-problem-and-is-blocking-access-to-data-about-how-much-there-is-and-who-is-affected-164838">“denominator problem”</a>. A great deal of social media research focuses on finding unwanted behavior – mis/disinformation, hate speech – on platforms. This isn’t that hard to do: search for “white genocide” or “ivermectin” and count the results. Indeed, a lot of eye-catching research does just this – consider <a href="https://secure.avaaz.org/campaign/en/facebook_threat_health/">Avaaz’s August 2020 report about COVID misinformation</a>. It reports 3.8 billion views of COVID misinfo in a year, which is a very big number. But it’s a numerator without a denominator – Facebook generates dozens or hundreds of views a day for each of its 3 billion users – 3.8 billion views is actually a very small number, contextualized with a denominator.</p>
<p>A few social media platforms have made it possible to calculate denominators. Reddit, for many years, permitted Pushshift to collect all Reddit posts, which means we can calculate what a small fraction of Reddit is focused on meme stocks or crypto, versus conversations about mental health or board gaming. Our <a href="https://redditmap.social/">Redditmap.social platform</a> – primarily built by Virginia Partridge and Jasmine Mangat – is based around the idea of looking at the platform as a whole and understanding how big or small each community is compared to the whole. Alas, Reddit cut off public access to Pushshift this summer, so Redditmap.social can only use data generated early this year. </p>
<p>Twitter was also a good platform for studying denominators, because it created a research API that took a statistical sample of all tweets and gave researchers access to every 10th or 100th one. If you found 2500 tweets about ivermectin a day, and saw 100m tweets through the decahose (which gave researchers 1/10th of tweet volume), you could calculate an accurate denominator (100m x 10) (All these numbers are completely made up.) Twitter has cut off access to these excellent academic APIs and now charges massive amounts of money for much less access, which means that it’s no longer possible for most researchers to do denominator-based work.</p>
<p>Interesting as Reddit and Twitter are, they are much less widely used than YouTube, which is used by virtually all internet users. <a href="https://www.pewresearch.org/internet/2023/12/11/teens-social-media-and-technology-2023/">Pew reports that 93% of teens use YouTube</a> – the closest service in terms of usage is Tiktok with 63% and Snapchat with 60%. While YouTube has a good, well-documented API, there’s no good way to get a random, representative sample of YouTube. Instead, most research on YouTube either studies a collection of videos (all videos on the channels of a selected set of users) or videos discovered via recommendation (start with <a href="https://www.youtube.com/watch?v=dQw4w9WgXcQ">Never Going to Give You Up</a>, objectively the center of the internet, and collect recommended videos.) You can do excellent research with either method, but you won’t get a sample of all YouTube videos and you won’t be able to calculate the size of YouTube.</p>
<p>I brought this problem to Jason Baumgartner, creator of PushShift, and prince of the dark arts of data collection. One of Jason’s skills is a deep knowledge of undocumented APIs, ways of collecting data outside of official means. Most platforms have one or more undocumented APIs, widely used by programmers for that platform to build internal tools. In the case of YouTube, that API is called <a href="https://gizmodo.com/how-project-innertube-helped-pull-youtube-out-of-the-gu-1704946491">“Inner Tube”</a> and its existence is an open secret in programmer communities. Using InnerTube, Jason suggested we do something that’s both really smart and really stupid: guess at random URLs and see if there are videos there.</p>
<p>Here’s how this works: YouTube URLs look like this: <code> https://www.youtube.com/ watch?v=vXPJVwwEmiM</code></p>
<p>That bit after “watch?v=” is an 11 digit string. The first ten digits can be a-z,A-Z,0-9 and _-. The last digit is special, and can only be one of 16 values. Turns out there are 2^64 possible YouTube addresses, an enormous number: 18.4 quintillion. There are lots of YouTube videos, but not that many. Let’s guess for a moment that there are 1 billion YouTube videos – if you picked URLs at random, you’d only get a valid address roughly once every 18.4 billion tries. </p>
<p>We refer to this method as “drunk dialing”, as it’s basically as sophisticated as taking swigs from a bottle of bourbon and mashing digits on a telephone, hoping to find a human being to speak to. Jason found a couple of cheats that makes the method roughly 32,000 times as efficient, meaning our “phone call” connects lots more often. Kevin Zheng wrote a whole bunch of scripts to do the dialing, and over the course of several months, we collected more than 10,000 truly random YouTube videos.</p>
<p>There’s lots you can do once you’ve got those videos. Ryan McCarthy is lead author <a href="https://journalqd.org/article/view/4066">on our paper in the Journal of Quantitative Description</a>, and he led the process of watching a thousand of these videos and hand-coding them, a massive and fascinating task. Kevin wired together his retrieval scripts with a variety of language detection systems, and we now have a defensible – if far from perfect – estimate of what languages are represented on YouTube. We’re starting some experiments to understand how the videos YouTube recommends differ from the “average” YouTube video – YouTube likes recommending videos with at least ten thousand views, while the median YouTube video has 39 views.</p>
<p>I’ll write at some length in the future about what we can learn from a true random sample of YouTube videos. I’ve been doing a lot of thinking about the idea of “the quotidian web”, learning from the bottom half of the long tail of user-generated media so we can understand what most creators are doing with these tools, not just from the most successful influencers. But I’m going to limit myself to the question that started this blog post: how big is YouTube?</p>
<p><img decoding="async" src="https://ethanzuckerman.com/wp-content/2023/12/Screenshot-2023-12-22-at-12.18.43?PM-1.png" alt="" width="1" height="1"><img decoding="async" fetchpriority="high" src="https://ethanzuckerman.com/wp-content/2023/12/tubestats-1024x758.jpg" alt="" width="1024" height="758" srcset="https://ethanzuckerman.com/wp-content/2023/12/tubestats-1024x758.jpg 1024w, https://ethanzuckerman.com/wp-content/2023/12/tubestats-600x444.jpg 600w, https://ethanzuckerman.com/wp-content/2023/12/tubestats-150x111.jpg 150w, https://ethanzuckerman.com/wp-content/2023/12/tubestats-768x569.jpg 768w, https://ethanzuckerman.com/wp-content/2023/12/tubestats.jpg 1334w" sizes="(max-width: 1024px) 100vw, 1024px"></p>
<p>Consider drunk dialing again. Let’s assume you only dial numbers in the 413 area code: 413-000-0000 through 413-999-9999. That’s 10,000,000 possible numbers. If one in 100 phone calls connect, you can estimate that 100,000 people have numbers in the 413 area code. In our case, our drunk dials tried roughly 32k numbers at the same time, and we got a “hit” every 50,000 times or so. Our current estimate for the size of YouTube is 13.325 billion videos – we are now updating this number every few weeks at <a href="https://tubestats.org/">tubestats.org</a>.</p>
<p>Once you’re collecting these random videos, other statistics are easy to calculate. We can look at how old our random videos are and calculate how fast YouTube is growing: we estimate that over 4 billion videos were posted to YouTube just in 2023. We can calculate the mean and median views per video, and show just how long the “long tail” is – videos with 10,000 or more videos are roughly 4% of our data set, though they represent the lion’s share of views of the YouTube platform. </p>
<p>Perhaps the most important thing we did with our set of random videos is to demonstrate a vastly better way of studying YouTube than drunk dialing. We know our method is random because it iterates through the entire possible address space. By comparing our results to other ways of generating lists of YouTube videos, we can declare them “plausibly random” if they generate similar results. Fortunately, one method does – it was <a href="https://dl.acm.org/doi/10.1145/2068816.2068851">discovered by Jia Zhou et. al. in 2011</a>, and it’s far more efficient than our naïve method. (You generate a five character string where one character is a dash – YouTube will autocomplete those URLs and spit out a matching video if one exists.) Kevin now polls YouTube using the “dash method” and uses the results to maintain our dashboard at Tubestats. </p>
<p>We have lots more research coming out from this data set, both about what we’re discovering and about some complex ethical questions about how to handle this data. (Most of the videos we’re discovering were only seen by a few dozen people. If we publish those URLs, we run the risk of exposing to public scrutiny videos that are “public” but whose authors could reasonably expect obscurity. Thus our paper does not include the list of videos discovered.) <a href="https://publicinfrastructure.org/2023/12/21/notes-from-random-youtube-coding/">Ryan has a great introduction to main takeaways from our hand-coding</a>. He and I are both working on longer writing about the weird world of random videos – what can we learn from spending time deep in the long tail? </p>
<p>Perhaps most importantly, we plan to maintain <a href="https://tubestats.org/">Tubestats</a> so long as we can. It’s possible that YouTube will object to the existence of this resource or the methods we used to create it. Counterpoint: I believe that high level data like this should be published regularly for all large user-generated media platforms. These platforms are some of the most important parts of our digital public sphere, and we need far more information about what’s on them, who creates this content and who it reaches.</p>
<p>Many thanks to the Journal for Quantitative Description of publishing such a large and unwieldy paper – it’s 85 pages! Thanks and congratulations to all authors: Ryan McGrady, Kevin Zheng, Rebecca Curran, Jason Baumgartner and myself. And thank you to everyone who’s funded our work: the Knight Foundation has been supporting a wide range of our work on studying extreme speech on social media, and other work in our lab is supported by the Ford Foundation and the MacArthur Foundation.</p>
<p>Finally – I’ve got COVID, so if this post is less coherent than normal, that’s to be expected. Feel free to use the comments to tell me what didn’t make sense and I will try to clear it up when my brain is less foggy.</p>
</div>

			</article>
			
		</div>
</main><!--/.neve-main-->




</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Groqchat (145 pts)]]></title>
            <link>https://chat.groq.com/</link>
            <guid>38739199</guid>
            <pubDate>Fri, 22 Dec 2023 22:03:04 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://chat.groq.com/">https://chat.groq.com/</a>, See on <a href="https://news.ycombinator.com/item?id=38739199">Hacker News</a></p>
&lt;Unparsable&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[Bluesky makes web view public, login no longer required to read posts (265 pts)]]></title>
            <link>https://bsky.app/profile/bsky.app/post/3kh5rbndrjd2x</link>
            <guid>38739130</guid>
            <pubDate>Fri, 22 Dec 2023 21:56:37 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://bsky.app/profile/bsky.app/post/3kh5rbndrjd2x">https://bsky.app/profile/bsky.app/post/3kh5rbndrjd2x</a>, See on <a href="https://news.ycombinator.com/item?id=38739130">Hacker News</a></p>
&lt;Unparsable&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[Shein forces Amazon to lower seller fees (110 pts)]]></title>
            <link>https://www.marketplacepulse.com/articles/shein-forces-amazon-to-lower-seller-fees</link>
            <guid>38739086</guid>
            <pubDate>Fri, 22 Dec 2023 21:50:48 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.marketplacepulse.com/articles/shein-forces-amazon-to-lower-seller-fees">https://www.marketplacepulse.com/articles/shein-forces-amazon-to-lower-seller-fees</a>, See on <a href="https://news.ycombinator.com/item?id=38739086">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
<p>Amazon will lower the transaction fee it charges sellers from 17% to just 5% for under-$15 apparel items as it faces Shein’s competition.</p>
<p>Amazon is the largest clothing retailer in the U.S., including online and offline retail. But it charges sellers a 17% fee plus additional fees for fulfillment and advertising, making it one of the most expensive Amazon categories to sell in. Amazon increased the fee to 17% from 15% five years ago, in April 2018.</p>
<p>Shein is an order of magnitude smaller than Amazon - its GMV this year, more than $40 billion, is less than 10% of Amazon’s. But most of that $40 billion is in clothing, which Shein is best known for and strongest in. Its supply chain, tuned to introduce thousands of new designs daily while dynamically adjusting which products get manufactured, is uniquely fit for clothing. It is the biggest online-native clothing retailer. </p>
<p>Amazon is reacting to Shein by lowering referral fees from 17% to 5% for clothing items priced under $15. For products priced between $15 and $20, it will decrease referral fees from 17% to 10%. Lower fees will allow sellers to lower prices by a few dollars and maintain the same margin. More expensive items remain at 17%. </p>
<p><img src="https://cdn.marketplacepulse.com/articles/641/amazon-clothing-referral-fee-now-vs-starting-2024.png" alt="Amazon Clothing Referral Fee Now vs. Starting 2024"></p>
<p>Shein can’t match Amazon’s one-day or two-day delivery, but it can offer bargain prices while taking a week to deliver. Amazon sells products and fast shipping; Shein sells products and slow shipping. The two are inseparable - <a href="https://www.marketplacepulse.com/articles/amazon-sells-fast-shipping-not-products">the logistics are as much part of the product as the products themselves</a>. However, Amazon’s high fees made the price difference even greater. </p>
<p>Amazon will remain more expensive than Shein even after the fee reduction because the biggest cost is fulfillment. Items stored in domestic Amazon warehouses will always be more costly than Shein’s direct shipments to consumers from China, which also skip inspection and taxation by U.S. Customs. </p>
<p>Shein is not a direct competitor to Amazon in general, but in clothing, it absolutely is. Shein has expanded to more categories, started manufacturing outside China, built domestic warehouses, and <a href="https://www.marketplacepulse.com/articles/amazon-sellers-are-joining-shein-marketplace">launched a marketplace</a> to attract sellers. But its core remains clothing shipped from China. Yet Shein’s critical innovation was dressing the supply chain advantage in an experience that delights. Shein looks and feels like a brand store rather than a random selection of products.</p>
<p>Amazon rarely changes referral fees, and reducing the fee by 70% for items in Shein’s key price segment is unquestionably a response to Shein. But even if fees were to go to zero, Amazon would still have no answer for Shein. For instance, Amazon doesn’t have the tens of millions of followers Shein has on Instagram nor the billions of views Shein hauls have on TikTok. Perhaps Amazon doesn’t need to answer, but the seller fee is not it. Focusing on fees is missing the forest for the trees. </p>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Google search overwhelmed by spam attack (287 pts)]]></title>
            <link>https://www.searchenginejournal.com/google-search-overwhelmed-by-massive-spam-attack/504527/</link>
            <guid>38738619</guid>
            <pubDate>Fri, 22 Dec 2023 21:00:21 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.searchenginejournal.com/google-search-overwhelmed-by-massive-spam-attack/504527/">https://www.searchenginejournal.com/google-search-overwhelmed-by-massive-spam-attack/504527/</a>, See on <a href="https://news.ycombinator.com/item?id=38738619">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="narrow-cont"><p><a href="https://www.searchenginejournal.com/serp-search-engine-results-page-features-guide/377094/">Google’s search results</a> have been hit by a spam attack for the past few days in what can only be described as completely out of control. Many domains are ranking for hundreds of thousands of keywords each, an indication that the scale of this attack could easily reach into the millions of keyword phrases.</p>
<p><strong>Updated:</strong><br>
The spam was initially discovered by Lily Ray:</p>
<blockquote id="tweet-1737499776386576538" data-width="550" data-dnt="true">
<p lang="en" dir="ltr">If you currently Google "craigslist used auto parts," every single result in the top 20 is spam, minus the first two results from Craigslist.</p>
<p>— Lily Ray 😏 (@lilyraynyc) <a href="https://twitter.com/lilyraynyc/status/1737499776386576538?ref_src=twsrc%5Etfw" target="_blank" rel="noopener">December 20, 2023</a></p></blockquote>

<p>Surprisingly, <a href="https://www.searchenginejournal.com/google-what-to-do-about-spammy-links-from-malicious-domains/449215/">many of the domains</a> have only been registered within the past 24-48 hours.</p>
<p>This recently came to my attention from a series of posts by Bill Hartzer (<a href="https://www.linkedin.com/in/bhartzer/" target="_blank" rel="noopener">LinkedIn profile</a>) where he published a <a href="https://www.searchenginejournal.com/link-graphs-and-google-rankings/435688/">link graph</a> generated by the Majestic backlinks tool that exposed the link networks of several of the spam sites.</p>
<p>The link graph that he posted showed scores of websites tightly <a href="https://www.searchenginejournal.com/seo-internal-links-best-practices/214886/">interlinking</a> with each other, which is a fairly typical pattern for <a href="https://www.searchenginejournal.com/link-building-guide/bad-links-risky-tactics/">spammy link</a> networks.</p>
<h3>Screenshot Of Tightly Interlinked Network</h3>
<p><img fetchpriority="high" decoding="async" src="https://www.searchenginejournal.com/wp-content/uploads/2023/12/link-network-65841b421292a-sej.jpg" alt="Google Search Overwhelmed By Massive Spam Attack" width="500" height="618" sizes="(max-width: 500px) 100vw, 500px" data-srcset="https://www.searchenginejournal.com/wp-content/uploads/2023/12/link-network-65841b421292a-sej.jpg 500w, https://www.searchenginejournal.com/wp-content/uploads/2023/12/link-network-65841b421292a-sej-480x593.jpg 480w" data-src="https://www.searchenginejournal.com/wp-content/uploads/2023/12/link-network-65841b421292a-sej.jpg" srcset="https://www.searchenginejournal.com/wp-content/uploads/2023/12/link-network-65841b421292a-sej.jpg 500w, https://www.searchenginejournal.com/wp-content/uploads/2023/12/link-network-65841b421292a-sej-480x593.jpg 480w"><span>Image by Bill Hartzer via Majestic</span></p>
<p>Bill and I talked about the spam sites over Facebook messenger and we both agreed that although the spammers put a lot of work into creating a backlink network, the links weren’t actually responsible for the high rankings.</p>
<p><strong>Bill said:</strong></p>
<blockquote><p>“This, in my opinion, is partly the fault of Google, who appears to be putting more emphasis on content rather than links.”</p></blockquote>
<p>I agree 100% that Google is putting more emphasis on content than links. But my thoughts are that the <a href="https://www.searchenginejournal.com/link-building-guide/bad-links-risky-tactics/">spam links</a> are there so that Googlebot can discover the spam pages and index them, even if just for one or two days.</p>
<p>Once indexed the spam pages are likely exploiting what I consider two loopholes in <a href="https://www.searchenginejournal.com/google-algorithm-history/">Google’s algorithms</a>, which I talk about next.</p>
<h2>Out of Control Spam in Google SERPs</h2>
<p>Multiple sites are ranking for longtail phrases that are somewhat easy to rank, as well as phrases with a local search component, which are also easy to rank.</p>
<p>Longtail phrases are keyword phrases that are used by people but exceedingly rarely. Longtail is a concept that’s been around for almost twenty years and subsequently popularized by a 2006 book called The Long Tail: Why the Future of Business is Selling Less of More.</p>
<p>Spammers are able to rank for these rarely searched phrases because there is little competition for those phrases, which makes it easy to rank.</p>
<p>So if a spammer creates millions of pages of longtail phrases those pages can then rank for hundreds of thousands of keywords every day in a short period of time.</p>
<p>Companies like Amazon use the principle of the longtail to sell hundreds of thousands of individual products a day which is different than selling one product hundreds of thousands of times per day.</p>
<p>That’s what the spammers are exploiting, the ease of ranking for <a href="https://www.searchenginejournal.com/keyword-research/long-tail-keywords/">longtail phrase</a>s.</p>
<p>The second thing that the spammers are exploiting is the loophole that’s inherent in Local Search.</p>
<p>The local search algorithm is not the same as the algorithm for ranking non-local keywords.</p>
<p>The examples that have come to light are variations of Craigslist and related keywords.</p>
<p>Examples are phrases like <em>Craigslist auto parts</em>, <em>Craigslist rooms to rent</em>, <em>Craigslist for sale by owner</em> and thousands of other keywords, most of which don’t use the word Craigslist.</p>
<p>The scale of the spam is huge and it goes far beyond than keywords with the word “Craigslist” in it.</p>
<h2>What The Spam Page Looks Like</h2>
<p>Taking a look at what the spam page looks like is impossible by visiting the pages with a browser.</p>
<p>I tried to see the source code of the sites that rank in Google but all of the spam sites automatically redirect to another domain.</p>
<p>I next entered the spam URL into the <a href="https://www.searchenginejournal.com/w3c-validator-guide/437030/">W3C</a> link checker to visit the website but the W3C bot couldn’t see the site either.</p>
<p>So I changed my browser user agent to identify itself as Googlebot but the spam site still redirected me.</p>
<p>That indicated that the site was not checking if the user agent was Googlebot.</p>
<p>The spam site was checking for Googlebot IP addresses. If the visitor’s IP address matched as belonging to Google then the spam page displayed content to Googlebot.</p>
<p>All other visitors got a redirect to other domains that displayed sketchy content.</p>
<p>In order to see the <a href="https://www.searchenginejournal.com/google-valid-html/258881/">HTML</a> of the website I had to visit with a Google IP address. So I used Google’s Rich Results tester to visit the spam site and record the HTML of the page.</p>
<p>I showed Bill Hartzer how to extract the HTML by using the Rich Results tester and he immediately went off to tweet about it, lol. Dang!</p>
<p>The Rich Results Tester has an option to show the HTML of a webpage. So copied the HTML, pasted it into a text file then saved it it as an HTML file.</p>
<h3>Screenshot Of HTML Provided By Rich Results Tool</h3>
<p><img src="https://www.searchenginejournal.com/wp-content/uploads/2023/12/screenshot-html-658423275fce1-sej.jpg" alt="Google Search Overwhelmed By Massive Spam Attack" data-old-src="data:image/svg+xml,%3Csvg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20400%20473%22%3E%3C/svg%3E" data-src="https://www.searchenginejournal.com/wp-content/uploads/2023/12/screenshot-html-658423275fce1-sej.jpg"></p>
<p>I next edited the HTML file to remove any JavaScript then saved the file again.</p>
<p><strong>I was now able to see what the webpage looks like to Google:</strong></p>
<h3>Screenshot Of Spam Webpage</h3>
<p><img src="https://www.searchenginejournal.com/wp-content/uploads/2023/12/screenshot-of-spam-page-65841ab85e154-sej.png" alt="Screenshot of a spam webpage that ranks in Google" data-old-src="data:image/svg+xml,%3Csvg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20447%20644%22%3E%3C/svg%3E" data-src="https://www.searchenginejournal.com/wp-content/uploads/2023/12/screenshot-of-spam-page-65841ab85e154-sej.png"></p>
<h2>One Domain Ranks For 300,000+ Keywords</h2>
<p>Bill sent me a spreadsheet containing a list of keyword phrases that just one of the spam sites ranked for. One spam site, just one of them, ranked for over 300,000 keyword phrases.</p>
<h3>Screenshot Showing Keywords For One Domain</h3>
<p><img src="https://www.searchenginejournal.com/wp-content/uploads/2023/12/screenshot-spreadsheet-65841a432f6d1-sej.png" alt="Image showing a closeup of a spreadsheet with keyword phrases on it" data-old-src="data:image/svg+xml,%3Csvg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20320%20420%22%3E%3C/svg%3E" data-src="https://www.searchenginejournal.com/wp-content/uploads/2023/12/screenshot-spreadsheet-65841a432f6d1-sej.png"></p>
<p>There were a lot of Craigslist keyword phrases but there were also other longtail phrases, many of which contained a local search element. As I mentioned, it’s easy to rank for longtail phrases, easy to rank for local search phrases and combine the two kinds of phrases and it’s really easy to rank for these keyword phrases.</p>
<h2>Why Does This Spam Technique Work?</h2>
<p><a href="https://www.searchenginejournal.com/local-seo/what-is-local-seo-why-local-search-is-important/">Local search</a> uses a different algorithm than the non-local algorithm. For example, a local site, in general, doesn’t need a lot of links to rank for a query. The pages just need the right kinds of keywords to trigger a local search algorithm and rank it for a geographic area.</p>
<p>So if you search for “Craigslist auto parts” that’s going to trigger the local search algorithm and because it’s longtail it’s not going to take too much to rank it.</p>
<p>This is an ongoing problem for many years. Several years ago a website was able to rank for “Rhinoplasty Plano, Texas” with a site that contained old Roman Latin content and headings in English. Rhinoplasty is a longtail local search and Plano, Texas is a relatively small town. Ranking for that Rhinoplasty keyword phrase was so easy that the latin language website was able to easily rank for it.</p>
<p>Google has known about this spam problem since at least December 19th, as acknowledged in a tweet by Danny Sullivan.</p>
<blockquote id="tweet-1737236506190987682" data-width="550" data-dnt="true">
<p lang="en" dir="ltr">Yes, I already passed that one on to the search team. Here’s a peek. And it’s being looked at. <a href="https://t.co/vJH3EisnXD" target="_blank" rel="noopener">pic.twitter.com/vJH3EisnXD</a></p>
<p>— Google SearchLiaison (@searchliaison) <a href="https://twitter.com/searchliaison/status/1737236506190987682?ref_src=twsrc%5Etfw" target="_blank" rel="noopener">December 19, 2023</a></p></blockquote>

<p>It will be interesting to see if Google finally after all this time figures out a way to combat this kind of spam.</p>
<p><em>Featured Image by Shutterstock/Kateryna Onyshchuk</em></p>



</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Shitflation, shrinkflation, inflation database (119 pts)]]></title>
            <link>https://jonatron.github.io/shitflation/</link>
            <guid>38737113</guid>
            <pubDate>Fri, 22 Dec 2023 18:42:34 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://jonatron.github.io/shitflation/">https://jonatron.github.io/shitflation/</a>, See on <a href="https://news.ycombinator.com/item?id=38737113">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
        <tr>
          <td>Tesco Tomato &amp; Basil Sauce For Meatballs 500G</td>
          <td>July 2014</td>
          <td><b>Tomato (73%)</b>, Tomato Purée (14%), Onion, Lemon Juice From Concentrate, Sugar, Basil (1.5%), Olive Oil, Salt, Garlic Purée, Citric Acid, Oregano, Firming Agent (Calcium Chloride), Black Pepper</td>
          <td>£1.19</td>
          <td><a href="https://web.archive.org/web/20140730081247/http://www.tesco.com/groceries/product/details/?id=277081824">Archive</a></td>
        </tr>
        <tr>
          <td>Tesco Tomato &amp; Basil Sauce For Meatballs 500G</td>
          <td>Dec 2023</td>
          <td>INGREDIENTS: <b>Tomato (46%)</b>, Partially Reconstituted Tomato Purée (43%), Onion, Herbs (1.5%) (Basil, Oregano), Sugar, Cornflour, Salt, Garlic Purée, Concentrated Lemon Juice, Acidity Regulator (Citric Acid), Black Pepper.</td>
          <td>£0.95</td>
          <td><a href="https://web.archive.org/web/20230130072741/https://www.tesco.com/groceries/en-GB/products/277081824">Archive</a></td>
          <td>Shit, De</td>
        </tr>
        <tr>
          <td>Tesco Finest Smoked Wiltshire Cure Back Bacon 240G</td>
          <td>Oct 2014</td>
          <td><b>Pork (90%)</b>, Water, Salt, Preservatives (Sodium Nitrite, Potassium Nitrate)</td>
          <td>£3.29</td>
          <td><a href="https://web.archive.org/web/20141001040111/http://www.tesco.com/groceries/product/details/?id=275174361">Archive</a></td>
        </tr>
        <tr>
          <td>Tesco Finest Smoked Wiltshire Cure Back Bacon 240G</td>
          <td>Dec 2023</td>
          <td>INGREDIENTS: <b>Pork (89%)</b>, Water, Salt, Preservatives (Potassium Nitrate, Sodium Nitrite).</td>
          <td>£3.50</td>
          <td><a href="https://web.archive.org/web/20221204182102/https://www.tesco.com/groceries/en-GB/products/259703198">Archive</a></td>
          <td>Shit, In</td>
        </tr>
        <tr>
          <td>Tesco Chicken Tikka Masala &amp; Pilau Rice 550G</td>
          <td>Dec 2014</td>
          <td>Cooked Pilau Rice, <b>Marinated Chicken (26%)</b>, Onion, Single Cream (Milk) (10%), Tomato Purée, Yogurt (Milk), Rapeseed Oil, Garlic Purée, Ginger Purée, Ground Cashew Nut, Honey, Butter (Milk), Cashew Nut Paste, Spices, Coriander Leaf, Sugar, Tandoori Masala, Salt,

Cooked Pilau Rice contains: Basmati Rice, Water, Rapeseed Oil, Salt, Concentrated Lemon Juice, Cumin Seed, Cardamom Pods, Colour (Curcumin), Cardamom, Ground Bay Leaf,

Marinated Chicken contains: Chicken Breast, Water, Tomato Purée, Ginger Purée, Palm Oil, Soya Oil, Garlic Purée, Yogurt Powder (Milk), Cornflour, Salt, Spices, Green Chilli, Colour (Paprika Extract), Sunflower Oil, Sugar, Cashew Nut Paste contains: Cashew Nut, Rapeseed Oil, Tandoori Masala contains: Paprika, Salt, Coriander Powder, Turmeric Powder, Chilli Powder, Cumin Powder, Cinnamon, Clove Powder, Lemon Oil, Black Pepper, Bay Leaf, Colour (Paprika Extract)
          </td>
          <td>£3.30</td>
          <td><a href="https://web.archive.org/web/20141203131159/http://www.tesco.com/groceries/product/details/?id=262805275">Archive</a></td>
        </tr>
        <tr>
          <td>Tesco Chicken Tikka Masala Meal For 1 500G</td>
          <td>Dec 2023</td>
          <td>INGREDIENTS: Cooked Pilau Rice [
    Water, Basmati Rice, Rapeseed Oil, Salt, Concentrated Lemon Juice, Cumin Seed, Colour (Curcumin), Cardamom Pods, Cardamom Powder, Ground Bay Leaf
], <b>Cooked Marinated Chicken (14%)</b> [
    Chicken, Tomato Purée, Ginger Purée, Garlic Purée, Cornflour, Water, Salt, Soya Oil, Yogurt Powder (Milk), Green Chilli Purée, Palm Oil, Chilli Powder, Yogurt (Milk), Skimmed Milk, Coriander Powder, Cumin Powder, Colour (Paprika Extract), Ginger Powder, Cinnamon, Black Pepper, Mace, Fenugreek, Star Anise, Turmeric, Basil
], Onion,
Onion Bhaji [
    Onion, Gram Flour, Rapeseed Oil, Coriander, Coriander Powder, Cumin Powder, Raising Agents (Disodium Diphosphate, Sodium Bicarbonate), Red Chilli Purée, Cayenne Pepper, Onion Seed, Rice Flour, Salt, Turmeric Powder
], Potato, Single Cream (Milk), Tomato, Tomato Purée, Yogurt (Milk), Rapeseed Oil, Tomato Juice, Garlic Purée, Ginger Purée, Water, Ground (Cashew Nut), Sugar, Honey, Butter (Milk), Coriander Leaf, Coriander Powder, Green Chilli, Cashew Nut, Salt, Cumin Powder, Cornflour, Turmeric, Paprika, Curry Leaves, Cumin Seed, Black Mustard Seed, Black Pepper, Cinnamon, Clove Powder, Chilli Powder, Cardamom Powder, Fennel, Bay Leaf, Lemon Oil, Colour (Paprika Extract).
          </td>
          <td>£4.00</td>
          <td><a href="https://web.archive.org/web/20230324135914/https://www.tesco.com/groceries/en-GB/products/312088891">Archive</a></td>
          <td>Shit, Shrink, In</td>
        </tr>
        <tr>
          <td>Tesco Italian Macaroni Cheese Pasta 450G</td>
          <td>Oct 2014</td>
          <td>Cooked Pasta, Milk, <b>Mature Cheddar Cheese (Milk) (16%)</b>, Water, Single Cream (Milk), Wheat Flour (Wheat Flour, Calcium Carbonate, Iron, Niacin, Thiamin), Rapeseed Oil, Cornflour, Salt, White Pepper, Mustard Powder, Cooked Pasta contains: Durum Wheat Semolina, Water</td>
          <td>£2.30</td>
          <td><a href="https://web.archive.org/web/20141005051457/http://www.tesco.com/groceries/product/details/?id=275153841">Archive</a></td>
        </tr>
        <tr>
          <td>Tesco Macaroni Cheese 400G</td>
          <td>Dec 2023</td>
          <td>INGREDIENTS: Cooked Pasta [Water, Durum Wheat Semolina], Whole Milk, <b>Water, Mature Cheddar Cheese (Milk) (13%), Cornflour</b>, Medium Fat Soft Cheese (Milk), Wheat Flour [Wheat Flour, Calcium Carbonate, Iron, Niacin, Thiamin], Natural Cream Flavouring (contains Milk), Salt, Cheddar Cheese (Milk), Butter (Milk), Mustard Seed, Spirit Vinegar, Whey (Milk).</td>
          <td>£3.25</td>
          <td><a href="https://www.tesco.com/groceries/en-GB/products/310167622">Tesco</a>
            <br><a href="https://jonatron.github.io/shitflation/screenshots/Screenshot%202023-12-22%20at%2019-44-35%20Tesco%20Macaroni%20Cheese%20400G.png">Screenshot</a></td>
          <td>Shit, Shrink, In</td>
        </tr>
        <tr>
          <td>Tesco Mayonnaise 500Ml</td>
          <td>Oct 2014</td>
          <td><b>Vegetable Oil (77%)</b>, Water, <b>Free Range Pasteurised Whole Egg &amp; Egg Yolk (8%)</b>, Lemon Juice From Concentrate, Spirit Vinegar, Salt, Sugar, Stabiliser (Xanthan Gum), Natural Flavouring (contains Mustard)</td>
          <td>£0.89</td>
          <td><a href="https://web.archive.org/web/20141013070251/http://www.tesco.com/groceries/product/details/?id=254884543">Arcive</a></td>
        </tr>
        <tr>
          <td>Tesco Mayonnaise 500Ml</td>
          <td>Dec 2023</td>
          <td><b>Rapeseed Oil (75%)</b>, Water, <b>Pasteurised Egg Yolk (7%)</b>, Spirit Vinegar, Sugar, Salt, <b>Modified Maize Starch</b>, Concentrated Lemon Juice, Flavouring (contains Mustard).</td>
          <td>£1.20</td>
          <td><a href="https://web.archive.org/web/20231123163709/https://www.tesco.com/groceries/en-GB/products/309197604">Archive</a></td>
          <td>Shit, In</td>
        </tr>
        <tr>
          <td>Jacob's Baked Original Mini Cheddars 12x25g</td>
          <td>Jul 2014</td>
          <td>Flour (Wheat Flour, Calcium, Iron, Niacin, Thiamin), Vegetable Oils (Palm, Sunflower), <b>Dried Cheese (12%)</b> (Milk) [Dried Powdered Cheese (Milk), Natural Flavouring, Yeast Extract (Barley)], Sugar, Glucose Syrup, Salt, Dried Whey (Milk), Barley Malt Extract, Raising Agents (Ammonium Bicarbonate, Sodium Bicarbonate), Lactic Acid, Natural Flavourings, May also contain Egg, Soya</td>
          <td>£2.00</td>
          <td><a href="https://web.archive.org/web/20140715052224/http://www.sainsburys.co.uk/shop/gb/groceries/crisps/mini-cheddars-12x25g">Archive</a></td>
        </tr>
        <tr>
          <td>Jacob's Mini Cheddars Original Baked Snacks Multipack 6x23g</td>
          <td>Dec 2023</td>
          <td>Flour (Wheat Flour, Calcium, Iron, Niacin, Thiamin), Vegetable Oils (Palm, Sunflower), <b>Dried Cheese (10%)</b> (Milk) [Dried Powdered Cheese (Milk), Natural Flavouring], Sugar, Glucose Syrup, Salt, Dried Whey (Milk), Barley Malt Extract, Raising Agents (Ammonium Bicarbonate, Sodium Bicarbonate), Acid (Lactic Acid), Natural Flavourings </td>
          <td>£1.75</td>
          <td><a href="https://www.sainsburys.co.uk/gol-ui/product/mcvities-mini-cheddars-7x25g">Sainsbury's</a>
            <br><a href="https://jonatron.github.io/shitflation/screenshots/Screenshot%202023-12-22%20at%2021-37-13%20Jacob's%20Mini%20Cheddars%20Original%20Baked%20Snacks%20Multipack%206x23g%20Sainsbury's.png">Screenshot</a></td>
          <td>Shit, Shrink, In</td>
        </tr>

        <!-- 
        <tr>
          <td>Product</td>
          <td>Date</td>
          <td>Ingredients</td>
          <td>Price</td>
          <td>Link</td>
        </tr>
        <tr>
          <td>Product</td>
          <td>Date</td>
          <td>Ingredients</td>
          <td>Price</td>
          <td>Link</td>
        </tr>
 -->
      </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[WebGPU now available for testing in Safari Technology Preview (204 pts)]]></title>
            <link>https://webkit.org/blog/14879/webgpu-now-available-for-testing-in-safari-technology-preview/</link>
            <guid>38737028</guid>
            <pubDate>Fri, 22 Dec 2023 18:34:53 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://webkit.org/blog/14879/webgpu-now-available-for-testing-in-safari-technology-preview/">https://webkit.org/blog/14879/webgpu-now-available-for-testing-in-safari-technology-preview/</a>, See on <a href="https://news.ycombinator.com/item?id=38737028">Hacker News</a></p>
<div id="readability-page-1" class="page"><article id="post-14879">
            
            

            <div>
                                
                <p><a href="https://developer.mozilla.org/en-US/docs/Web/API/WebGPU_API">WebGPU</a> is a new <a href="https://www.w3.org/TR/webgpu/">standards-compliant</a> API that enables high-performance 3D graphics and general-purpose <a href="https://en.wikipedia.org/wiki/Data_parallelism">computations</a> on the Web. WebGPU programs are written in JavaScript but expose GPU functionality, allowing GPU computing to be used in Web content for the first time. Starting in <a href="https://developer.apple.com/safari/technology-preview/">Safari Technology Preview</a> 185, WebGPU can be enabled for early testing and development.</p>
<p>To enable WebGPU, turn on the “WebGPU”, “GPU Process: DOM Rendering” and “GPU Process: Canvas Rendering” feature flags in the <a href="https://developer.apple.com/documentation/safari-developer-tools/feature-flag-settings">Feature Flags</a> tab in Safari Preferences. If you don’t see the Feature Flags tab, you need to first check “<a href="https://developer.apple.com/documentation/safari-developer-tools/enabling-developer-features">Show features for web developers</a>” in the Advanced tab.</p>
<p>Once you have WebGPU enabled in Safari Technology Preview 185, <a href="https://webgpu.github.io/webgpu-samples/samples/particles">try out this example of WebGPU</a>. It utilizes many of the best features of WebGPU.</p>
<figure><video autoplay="" loop="" muted="" src="https://webkit.org/blog-files/webgpu-particles.mov"></video></figure>
<h2>WebGPU JavaScript API</h2>
<p>The WebGPU API is accessed through JavaScript, similar to WebGL.</p>
<h3>Creating a GPUDevice</h3>
<p>In order to use WebGPU, a device must be created. Resources and pipeline state are created from a <code>GPUDevice</code> instance. To create a device with default limits and features which are supported on all devices supporting WebGPU, we can pass zero parameters to the invocations of <code>requestAdapter</code> and <code>requestDevice</code>.</p>
<pre><code><span>const</span> <span>adapter</span> <span>=</span> <span>await</span> <span>navigator</span>.<span>gpu</span>.<span>requestAdapter</span>();
<span>device</span> <span>=</span> <span>await</span> <span>adapter</span>.<span>requestDevice</span>();
</code></pre>
<h3>Configuring a GPUCanvasContext</h3>
<p>The <code>GPUCanvasContext</code> is an interface that allows you to configure how your content will be displayed in the corresponding <code>HTMLCanvas</code> element on the page.</p>
<pre><code><span>context</span> <span>=</span> <span>canvas</span>.<span>getContext</span>(<span>'webgpu'</span>);
<span>const</span> <span>canvasFormat</span> <span>=</span> <span>"bgra8unorm"</span>;

<span>const</span> <span>contextConfiguration</span> <span>=</span> {
    <span>device</span><span>:</span> <span>device</span>,
    <span>format</span><span>:</span> <span>canvasFormat</span>,
    <span>alphaMode</span><span>:</span> <span>'opaque'</span>,
};
<span>context</span>.<span>configure</span>(<span>contextConfiguration</span>);
</code></pre>
<h3>Creating a GPURenderPipeline</h3>
<p>A <code>GPURenderPipeline</code> or a corresponding <code>GPUComputePipeline</code> are used to configure the pipeline state of the graphics driver. This pipeline state is then used in a <code>GPURenderPassEncoder</code> or <code>GPUComputePassEncoder</code> as later illustrated.</p>
<pre><code><span>const</span> <span>shaderModule</span> <span>=</span> <span>device</span>.<span>createShaderModule</span>({ <span>code</span><span>:</span> <span>wgslSource</span> });
<span>const</span> <span>vertexStageDescriptor</span> <span>=</span> { <span>module</span><span>:</span> <span>shaderModule</span>, <span>entryPoint</span><span>:</span> <span>"vsmain"</span> };
<span>const</span> <span>fragmentStageDescriptor</span> <span>=</span> { <span>module</span><span>:</span> <span>shaderModule</span>, <span>entryPoint</span><span>:</span> <span>"fsmain"</span> };
<span>const</span> <span>renderPipelineDescriptor</span> <span>=</span> {
    <span>layout</span><span>:</span> <span>'auto'</span>,
    <span>vertex</span><span>:</span> <span>vertexStageDescriptor</span>,
    <span>fragment</span><span>:</span> <span>fragmentStageDescriptor</span>,
    <span>primitive</span><span>:</span> {<span>topology</span><span>:</span> <span>"triangle-list"</span> },
};
<span>const</span> <span>renderPipeline</span> <span>=</span> <span>device</span>.<span>createRenderPipeline</span>(<span>renderPipelineDescriptor</span>);
</code></pre>
<h3>Issuing draw calls</h3>
<p>A <code>GPURenderPassEncoder</code> is created to send draw calls to the graphics driver. In the below example, we draw a simple triangle which contains three vertices. A <code>GPURenderPassEncoder</code> can also draw multiple instances of the same geometry or draw from an offset of a vertex buffer.</p>
<pre><code><span>const</span> <span>colorAttachmentDescriptor</span> <span>=</span> {
    <span>view</span><span>:</span> <span>renderAttachment</span>,
    <span>loadOp</span><span>:</span> <span>"clear"</span>,
    <span>storeOp</span><span>:</span> <span>"store"</span>,
    <span>clearColor</span><span>:</span> { <span>r</span><span>:</span> <span>0.15</span>, <span>g</span><span>:</span> <span>0.15</span>, <span>b</span><span>:</span> <span>0.5</span>, <span>a</span><span>:</span> <span>1</span> }
};
<span>const</span> <span>renderPassDescriptor</span> <span>=</span> { <span>colorAttachments</span><span>:</span> [<span>colorAttachmentDescriptor</span>] };
<span>const</span> <span>commandEncoder</span> <span>=</span> <span>device</span>.<span>createCommandEncoder</span>();
<span>const</span> <span>renderPassEncoder</span> <span>=</span> <span>commandEncoder</span>.<span>beginRenderPass</span>(<span>renderPassDescriptor</span>);
<span>renderPassEncoder</span>.<span>setPipeline</span>(<span>renderPipeline</span>);
<span>const</span> <span>vertexBufferSlot</span> <span>=</span> <span>0</span>;
<span>renderPassEncoder</span>.<span>setVertexBuffer</span>(<span>vertexBufferSlot</span>, <span>vertexBuffer</span>, <span>0</span>);
<span>renderPassEncoder</span>.<span>draw</span>(<span>3</span>, <span>1</span>, <span>0</span>, <span>0</span>); <span>// 3 vertices, 1 instance, 0th vertex, 0th instance.
</span><span>renderPassEncoder</span>.<span>end</span>();
<span>const</span> <span>commandBuffer</span> <span>=</span> <span>commandEncoder</span>.<span>finish</span>();
<span>const</span> <span>queue</span> <span>=</span> <span>device</span>.<span>queue</span>;
<span>queue</span>.<span>submit</span>([<span>commandBuffer</span>]);
</code></pre>
<h2>WebGPU Shading Language</h2>
<p>WebGPU introduces WGSL, a platform independent shading language for the web. Here is an example of a WGSL shader source that would be passed in place of <code>wgslSource</code> in the above API call:</p>
<pre><code><span>const</span> <span>wgslSource</span> <span>=</span> `
    <span>struct</span> <span>Vertex</span> {
        @<span>builtin</span>(<span>position</span>) <span>Position</span><span>:</span> <span>vec4</span><span>&lt;</span><span>f32</span><span>&gt;</span>,
        @<span>location</span>(<span>0</span>) <span>color</span><span>:</span> <span>vec4</span><span>&lt;</span><span>f32</span><span>&gt;</span>,
    }

    @<span>vertex</span> <span>fn</span> <span>vsmain</span>(@<span>builtin</span>(<span>vertex_index</span>) <span>VertexIndex</span><span>:</span> <span>u32</span>) <span>-</span><span>&gt;</span> <span>Vertex</span>
    {
        <span>var</span> <span>pos</span><span>:</span> <span>array</span><span>&lt;</span><span>vec2</span><span>&lt;</span><span>f32</span><span>&gt;</span>, <span>3</span><span>&gt;</span> <span>=</span> <span>array</span><span>&lt;</span><span>vec2</span><span>&lt;</span><span>f32</span><span>&gt;</span>, <span>3</span><span>&gt;</span>(
            <span>vec2</span><span>&lt;</span><span>f32</span><span>&gt;</span>( <span>0.0</span>,  <span>0.5</span>),
            <span>vec2</span><span>&lt;</span><span>f32</span><span>&gt;</span>(<span>-</span><span>0.5</span>, <span>-</span><span>0.5</span>),
            <span>vec2</span><span>&lt;</span><span>f32</span><span>&gt;</span>( <span>0.5</span>, <span>-</span><span>0.5</span>));
        <span>var</span> <span>vertex_out</span> <span>:</span> <span>Vertex</span>;
        <span>vertex_out</span>.<span>Position</span> <span>=</span> <span>vec4</span><span>&lt;</span><span>f32</span><span>&gt;</span>(<span>pos</span>[<span>VertexIndex</span>], <span>0.0</span>, <span>1.0</span>);
        <span>vertex_out</span>.<span>color</span> <span>=</span> <span>vec4</span><span>&lt;</span><span>f32</span><span>&gt;</span>(<span>pos</span>[<span>VertexIndex</span>] <span>+</span> <span>vec2</span><span>&lt;</span><span>f32</span><span>&gt;</span>(<span>0.5</span>, <span>0.5</span>), <span>0.0</span>, <span>1.0</span>);
        <span>return</span> <span>vertex_out</span>;
    }

    @<span>fragment</span> <span>fn</span> <span>fsmain</span>(<span>in</span><span>:</span> <span>Vertex</span>) <span>-</span><span>&gt;</span> @<span>location</span>(<span>0</span>) <span>vec4</span><span>&lt;</span><span>f32</span><span>&gt;</span>
    {
        <span>return</span> <span>in</span>.<span>color</span>;
    }
`;
</code></pre>
<h2>Try WebGPU and file bugs!</h2>
<p>We’re very excited to have an early version of WebGPU and WGSL in the latest version of Safari Technology Preview. Please do try it out. Check out the <a href="https://webgpu.github.io/webgpu-samples/samples/helloTriangle">public repository of WebGPU samples</a>. And file bugs or issues you discover at <a href="http://bugs.webkit.org/">bugs.webkit.org</a>.</p>

                            </div>
        </article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Granting Pardon for the Offense of Simple Possession of or Use of Marijuana (303 pts)]]></title>
            <link>https://www.whitehouse.gov/briefing-room/presidential-actions/2023/12/22/a-proclamation-on-granting-pardon-for-the-offense-of-simple-possession-of-marijuana-attempted-simple-possession-of-marijuana-or-use-of-marijuana/</link>
            <guid>38736919</guid>
            <pubDate>Fri, 22 Dec 2023 18:22:44 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.whitehouse.gov/briefing-room/presidential-actions/2023/12/22/a-proclamation-on-granting-pardon-for-the-offense-of-simple-possession-of-marijuana-attempted-simple-possession-of-marijuana-or-use-of-marijuana/">https://www.whitehouse.gov/briefing-room/presidential-actions/2023/12/22/a-proclamation-on-granting-pardon-for-the-offense-of-simple-possession-of-marijuana-attempted-simple-possession-of-marijuana-or-use-of-marijuana/</a>, See on <a href="https://news.ycombinator.com/item?id=38736919">Hacker News</a></p>
<div id="readability-page-1" class="page"><article>
	


	<div>
								


<div><p>&nbsp; &nbsp; &nbsp;In Proclamation 10467 of October 6, 2022 (Granting Pardon for the Offense of Simple Possession of Marijuana), I exercised my authority under the Constitution to pardon individuals who committed or were convicted of the offense of simple possession of marijuana in violation of the Controlled Substances Act and section 48–904.01(d)(1) of the Code of the District of Columbia (D.C. Code).&nbsp; As I have said before, convictions for simple possession of marijuana have imposed needless barriers to employment, housing, and educational opportunities.&nbsp; Through this proclamation, consistent with the grant of Proclamation 10467, I am pardoning additional individuals who may continue to experience the unnecessary collateral consequences of a conviction for simple possession of marijuana, attempted simple possession of marijuana, or use of marijuana.&nbsp; Therefore, acting pursuant to the grant of authority in Article&nbsp;II, Section 2, of the Constitution of the United States, I, Joseph&nbsp;R. Biden Jr., do hereby grant a full, complete, and unconditional pardon to all current United States citizens and lawful permanent residents who, on or before the date of this proclamation, committed or were convicted of the offense of simple possession of marijuana, attempted simple possession of marijuana, or use of marijuana, regardless of whether they have been charged with or prosecuted for these offenses on or before the date of this proclamation, in violation of:</p><p>(1) &nbsp;section 844 of title 21, United States Code, section 846 of title 21, United States Code, and previous provisions in the United States Code that prohibited simple possession of marijuana or attempted simple possession of marijuana;&nbsp;</p><p>(2) &nbsp;section 48-904.01(d)(1) of the D.C. Code and previous provisions in the D.C. Code that prohibited simple possession of marijuana;</p><p>(3) &nbsp;section 48-904.09 of the D.C. Code and previous provisions in the D.C. Code that prohibited attempted simple possession of marijuana; and</p><p>(4) &nbsp;provisions in the Code of Federal Regulations, including as enforced under the United States Code, that prohibit only the simple possession or use of marijuana on Federal properties or installations, or in other locales, as currently or previously codified, including but not limited to 25 C.F.R. 11.452(a); 32 C.F.R. 1903.12(b)(2); 36 C.F.R. 2.35(b)(2); 36 C.F.R. 1002.35(b)(2); 36 C.F.R. 1280.16(a)(1); 36 C.F.R. 702.6(b); 41 C.F.R. 102-74.400(a); 43 C.F.R. 8365.1-4(b)(2); and 50 C.F.R. 27.82(b)(2).</p></div>



<div><p>&nbsp;&nbsp;&nbsp;&nbsp; My intent by this proclamation is to pardon only the offenses of simple possession of marijuana, attempted simple possession of marijuana, or use of marijuana in violation of the Federal and D.C. laws set forth in paragraphs (1) through (3) of this proclamation, as well as the provisions in the Code of Federal Regulations consistent with paragraph (4) of this proclamation, and not any other offenses involving other controlled substances or activity beyond simple possession of marijuana, attempted simple possession of marijuana, or use of marijuana, such as possession of marijuana with intent to distribute or driving offenses committed while under the influence of marijuana. &nbsp;This pardon does not apply to individuals who were non-citizens not lawfully present in the United States at the time of their offense.</p><p>&nbsp;&nbsp;&nbsp;&nbsp; Pursuant to the procedures in Proclamation 10467, the Attorney General, acting through the Pardon Attorney, shall review all properly submitted applications for certificates of pardon and shall issue such certificates of pardon to eligible applicants in due course.&nbsp;</p><p>&nbsp;&nbsp;&nbsp;&nbsp; IN WITNESS WHEREOF, I have hereunto set my hand this twenty-second day of December, in the year of our Lord two&nbsp;thousand&nbsp;twenty-three, and of the Independence of the United&nbsp;States of&nbsp;America the two&nbsp;hundred and forty-eighth.</p></div>



<p>JOSEPH R. BIDEN JR.</p>
			</div>
</article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Database Isolation Is Broken and You Should Care (117 pts)]]></title>
            <link>https://materializedview.io/p/database-isolation-is-broken-you-should-care</link>
            <guid>38736904</guid>
            <pubDate>Fri, 22 Dec 2023 18:20:58 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://materializedview.io/p/database-isolation-is-broken-you-should-care">https://materializedview.io/p/database-isolation-is-broken-you-should-care</a>, See on <a href="https://news.ycombinator.com/item?id=38736904">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><article><div dir="auto"><p><span>This month saw several related posts on </span><a href="https://en.wikipedia.org/wiki/ACID" rel="">ACID</a><span> (</span><a href="https://en.wikipedia.org/wiki/Atomicity_(database_systems)" rel="">atomicity</a><span>, </span><a href="https://en.wikipedia.org/wiki/Consistency_(database_systems)" rel="">consistency</a><span>, </span><a href="https://en.wikipedia.org/wiki/Isolation_(database_systems)" rel="">isolation</a><span>, </span><a href="https://en.wikipedia.org/wiki/Durability_(database_systems)" rel="">durability</a><span>) database properties.</span></p><p><a href="https://www.linkedin.com/in/tony-solomonik-646819121" rel="">Tony Solomonik</a><span> writes a database using </span><a href="https://www.gnu.org/software/bash/" rel="">bash</a><span> in his </span><a href="https://tontinton.com/posts/database-fundementals/" rel="">database fundamentals</a><span> post. bashdb is a fascinating exercise, but also demonstrates why databases are so complex (ACID, fsync, B+ trees, LSMs, consensus, replication, and so on). Start here to brush up on the basics.</span></p><p><span>Next, </span><a href="https://www.linkedin.com/in/gwenshapira/" rel="">Gwen Shapira</a><span>, co-founder of </span><a href="https://www.thenile.dev/" rel="">Nile</a><span> [$], posted </span><a href="https://www.thenile.dev/blog/transaction-isolation-postgres" rel="">Transaction Isolation in Postgres, explained</a><span>. Though the headline says Postres, the post is mostly about database isolation levels (the </span><em>I</em><span> in ACID). She describes the various isolation levels in </span><a href="https://en.wikipedia.org/wiki/SQL-92" rel="">ANSI SQL ‘92</a><span> and shows the edge cases that break them.</span></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4ce5c678-ea0c-4df9-b1d4-20fab2223350_1228x534.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4ce5c678-ea0c-4df9-b1d4-20fab2223350_1228x534.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4ce5c678-ea0c-4df9-b1d4-20fab2223350_1228x534.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4ce5c678-ea0c-4df9-b1d4-20fab2223350_1228x534.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4ce5c678-ea0c-4df9-b1d4-20fab2223350_1228x534.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4ce5c678-ea0c-4df9-b1d4-20fab2223350_1228x534.png" width="1228" height="534" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/4ce5c678-ea0c-4df9-b1d4-20fab2223350_1228x534.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:534,&quot;width&quot;:1228,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:87876,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:false,&quot;topImage&quot;:true,&quot;internalRedirect&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4ce5c678-ea0c-4df9-b1d4-20fab2223350_1228x534.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4ce5c678-ea0c-4df9-b1d4-20fab2223350_1228x534.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4ce5c678-ea0c-4df9-b1d4-20fab2223350_1228x534.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4ce5c678-ea0c-4df9-b1d4-20fab2223350_1228x534.png 1456w" sizes="100vw" fetchpriority="high"></picture></div></a></figure></div><p>Here’s where things get interesting. Gwen’s post says that there are many anomalies that aren’t classified in the ANSI SQL ‘92 spec.</p><blockquote><p><span>It turned out that defining a perfect state and then defining some states where certain anomalies can happen has this fundamental problem: There may be anomalies you didn't think of. So, only three years after the SQL92 standard came out, a very impressive team of researchers published </span><strong><a href="https://arxiv.org/pdf/cs/0701157.pdf" rel="">A Critique of ANSI SQL Isolation Levels (1995)</a></strong><span>. The paper introduced a whole collection of anomalies that weren't specified in the standard, and therefore were technically allowed at the Serializable level.</span></p></blockquote><p>These new-found anomalies make database isolation ambiguous:</p><blockquote><p>Different databases support different isolation levels, and the same isolation level can mean different things in different databases.</p></blockquote><p><span>This leads to the final post I want to call out: </span><a href="https://jepsen.io/analyses/mysql-8.0.34" rel="">Jepsen’s MySQL Analysis</a><span>. Jepsen is a project run by Kyle Kingsbury alter ego, </span><a href="https://aphyr.com/about" rel="">Aphyr</a><span>. Kyle and his team use Jepsen to break databases. Kyle and </span><a href="https://www.linkedin.com/in/peteralvaro/" rel="">Peter Avlaro</a><span>’s latest post finds anomalies with MySQL and Amazon RDS.</span></p><blockquote><p><em><span>We revisit Kleppmann’s 2014 </span><a href="https://github.com/ept/hermitage/blob/master/mysql.md" rel="">Hermitage</a><span> and confirm that MySQL’s Repeatable Read still allows G2-item, G-single, and lost update. Using our transaction consistency checker </span><a href="https://github.com/jepsen-io/elle" rel="">Elle</a><span>, we show that MySQL Repeatable Read also violates internal consistency. Furthermore, it violates Monotonic Atomic View: transactions can observe some of another transaction’s effects, then later fail to observe other effects of that same transaction. We demonstrate violations of ANSI SQL’s requirements for Repeatable Read.</span></em></p></blockquote><p>Similar to Gwen’s post, they find anomalies and that don’t adhere to the ANSI ‘92 spec. But the behavior they find is much worse than PostgreSQL’s behavior. If you’re using MySQL, I recommend reading the post; they provide suggestions to mitigate some of the issues.</p><p>But what caught my eye was their plea to the standards bodies:</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3f8170b4-97c8-4441-8505-e9100af038a6_1590x952.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3f8170b4-97c8-4441-8505-e9100af038a6_1590x952.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3f8170b4-97c8-4441-8505-e9100af038a6_1590x952.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3f8170b4-97c8-4441-8505-e9100af038a6_1590x952.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3f8170b4-97c8-4441-8505-e9100af038a6_1590x952.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3f8170b4-97c8-4441-8505-e9100af038a6_1590x952.png" width="1456" height="872" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/3f8170b4-97c8-4441-8505-e9100af038a6_1590x952.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:872,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:363384,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3f8170b4-97c8-4441-8505-e9100af038a6_1590x952.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3f8170b4-97c8-4441-8505-e9100af038a6_1590x952.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3f8170b4-97c8-4441-8505-e9100af038a6_1590x952.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3f8170b4-97c8-4441-8505-e9100af038a6_1590x952.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p>Jepsen’s call for better isolation levels mirror Gwen’s. ANSI ‘92 database isolation levels clearly aren’t cutting it.</p><p><span>I’m going to assume you, dear reader, are not part of the standards body. So your takeaway from all this is that your database might not behaving as you expect. But do you care? </span><a href="https://twitter.com/justinjaffray/status/1737506863455854750" rel="">This thread</a><span> says you don’t.</span></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F80eb7675-8db0-4379-8616-b88846c3caac_1372x242.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F80eb7675-8db0-4379-8616-b88846c3caac_1372x242.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F80eb7675-8db0-4379-8616-b88846c3caac_1372x242.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F80eb7675-8db0-4379-8616-b88846c3caac_1372x242.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F80eb7675-8db0-4379-8616-b88846c3caac_1372x242.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F80eb7675-8db0-4379-8616-b88846c3caac_1372x242.png" width="1372" height="242" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/80eb7675-8db0-4379-8616-b88846c3caac_1372x242.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:242,&quot;width&quot;:1372,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:67801,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F80eb7675-8db0-4379-8616-b88846c3caac_1372x242.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F80eb7675-8db0-4379-8616-b88846c3caac_1372x242.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F80eb7675-8db0-4379-8616-b88846c3caac_1372x242.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F80eb7675-8db0-4379-8616-b88846c3caac_1372x242.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p><a href="https://twitter.com/justinjaffray" rel="">Justin</a><span>’s point is that we’ve been living with these “broken” databases for a long time and no one noticed.</span></p><blockquote><p>that said: it took one of the greatest software minds of a generation to prove that one of the most popular databases of all time was broken. kind of calls into question whether anyone ever cared about that brokenness!</p></blockquote><p>The problem I have with Justin’s argument is that the kinds of anomalies in these posts are hard for developers to complain about. They manifest themselves only very occasionally, and are very difficult to detect.</p><p>Developers that are bitten by database anomalies often don’t know. Application exhibits some weird anomaly, which developers can’t reproduce. Engineers have to just throw up their hands, shrug, and move on. They never even know the database was the culprit.</p><p><span>I have been personally bitten by bugs like this while working on payments </span><a href="https://go.wepay.com/" rel="">my last job</a><span>. Payments processing is a very precise thing (indeed, it’s the example Gwen uses in her post and is what </span><a href="https://materializedview.io/p/durable-execution-justifying-the-bubble" rel="">durable execution frameworks</a><span> always demonstrate). Our double entry book keeping log would no longer line up, databases would misalign, or some other oddity would occur. Such bugs resulted in hours of fruitless debugging and unhappy customers. These are things that I, and many engineers, most definitely do care about.</span></p><p>Jepsen is doing us a favor by showing us that the almighty OLTP database is in fact fallible. Make sure you understand how your database behaves (as best you can) and act accordingly. Caveat emptor.</p></div></article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Polymers capable of killing bacteria without inducing antibiotic resistance (129 pts)]]></title>
            <link>https://today.tamu.edu/2023/12/21/texas-am-team-develops-polymers-that-can-kill-bacteria/</link>
            <guid>38736798</guid>
            <pubDate>Fri, 22 Dec 2023 18:07:40 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://today.tamu.edu/2023/12/21/texas-am-team-develops-polymers-that-can-kill-bacteria/">https://today.tamu.edu/2023/12/21/texas-am-team-develops-polymers-that-can-kill-bacteria/</a>, See on <a href="https://news.ycombinator.com/item?id=38736798">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
                    <div role="region" aria-label="Texas A&amp;M Team Develops Novel Antibacterial Polymers That Can Kill Bacteria content 0">
<figure><p><img decoding="async" loading="lazy" src="https://today.tamu.edu/wp-content/uploads/2023/12/bacteria-colonies-in-petri-dish-getty-350x233.jpg" alt="A photo of a petri dish with bacteria in a science lab." width="2400" height="1597" srcset="https://today.tamu.edu/wp-content/uploads/2023/12/bacteria-colonies-in-petri-dish-getty-350x233.jpg 350w, https://today.tamu.edu/wp-content/uploads/2023/12/bacteria-colonies-in-petri-dish-getty-1024x681.jpg 1024w, https://today.tamu.edu/wp-content/uploads/2023/12/bacteria-colonies-in-petri-dish-getty-768x511.jpg 768w, https://today.tamu.edu/wp-content/uploads/2023/12/bacteria-colonies-in-petri-dish-getty-1536x1022.jpg 1536w, https://today.tamu.edu/wp-content/uploads/2023/12/bacteria-colonies-in-petri-dish-getty-2048x1363.jpg 2048w, https://today.tamu.edu/wp-content/uploads/2023/12/bacteria-colonies-in-petri-dish-getty-500x333.jpg 500w" sizes="(max-width: 2400px) 100vw, 2400px"></p><div><figcaption><hr><p>Getty Images</p></figcaption></div></figure>

<p>Antibiotic-resistant bacteria have become a rapidly growing threat to public health. Each year, they account for more than 2.8 million infections, according to the U.S. Centers for Disease Control and Prevention. Without new antibiotics, even common injuries and infections harbor the potential to become lethal.</p>
<p>Scientists are now one step closer to eliminating that threat, thanks to a Texas A&amp;M University-led collaboration that has developed a new family of polymers capable of killing bacteria without inducing antibiotic resistance by disrupting the membrane of these microorganisms.</p>
<p>“The new polymers we synthesized could help fight antibiotic resistance in the future by providing antibacterial molecules that operate through a mechanism against which bacteria do not seem to develop resistance,” said&nbsp;<a href="https://www.chem.tamu.edu/faculty/quentin-michaudel/" target="_blank" rel="noopener">Dr. Quentin Michaudel</a>, an assistant professor in the&nbsp;<a href="https://artsci.tamu.edu/chemistry/index.html" target="_blank" rel="noopener">Department of Chemistry</a>&nbsp;and lead investigator in the research,&nbsp;<a href="https://www.pnas.org/doi/10.1073/pnas.2311396120" target="_blank" rel="noopener">published Dec. 11</a>&nbsp;in the&nbsp;<a href="https://www.pnas.org/" target="_blank" rel="noopener"><em>Proceedings of the National Academy of Sciences</em>&nbsp;(PNAS)</a>.</p>
<p>Working at the interface of organic chemistry and polymer science, the&nbsp;<a href="https://www.michaudellab.org/" target="_blank" rel="noopener">Michaudel Laboratory</a> was able to synthesize the new polymer by carefully designing a positively charged molecule that can be stitched many times to form a large molecule made of the same repeating charged motif using a carefully selected catalyst called AquaMet. According to Michaudel, that catalyst proves key, given that it has to tolerate a high concentration of charges and also be water-soluble — a feature he describes as uncommon for this type of process.</p>
<p>After achieving success, the Michaudel Lab put its polymers to the test against two main types of antibiotic-resistant bacteria — E. coli and Staphylococcus aureus (MRSA) — in collaboration with <a href="https://www.umass.edu/engineering/about/directory/jessica-schiffman" target="_blank" rel="noopener">Dr. Jessica Schiffman’s</a> group at the University of Massachusetts Amherst. While awaiting those results, the researchers also tested their polymers’ toxicity against human red blood cells.</p>
</div>
<div role="region" aria-label="Texas A&amp;M Team Develops Novel Antibacterial Polymers That Can Kill Bacteria content 1">
<p>“A common issue with antibacterial polymers is a lack of selectivity between bacteria and human cells when targeting the cellular membrane,” Michaudel explained. “The key is to strike a right balance between effectively inhibiting bacteria growth and killing several types of cells indiscriminately.”</p>
<p>Michaudel credits the multidisciplinary nature of scientific innovation and the generosity of dedicated researchers across the Texas A&amp;M campus and country as factors in his team’s success in determining the perfect catalyst for their molecule assembly.</p>
<p>“This project was several years in the making and would not have been possible without the help of several groups, in addition to our UMass collaborators,” Michaudel said. “For instance, we had to ship some samples to the&nbsp;<a href="https://www.letterilab.com/" target="_blank" rel="noopener">Letteri Lab</a>&nbsp;at the University of Virginia to determine the length of our polymers, which required the use of an instrument that few labs in the country have. We are also tremendously grateful to [biochemistry Ph.D. candidate] Nathan Williams and&nbsp;<a href="https://bcbp.tamu.edu/people/pellois-jean-philippe/" target="_blank" rel="noopener">Dr. Jean-Philippe Pellois</a>&nbsp;here at Texas A&amp;M, who provided their expertise in our assessment of toxicity against red blood cells.”</p>
<p>Michaudel says the team will now focus on improving the activity of its polymers against bacteria — specifically, their selectivity for bacterial cells versus human cells — before moving on to <em>in vivo</em>&nbsp;assays.</p>
<p>“We are in the process of synthesizing a variety of analogs with that exciting goal in mind,” he said.</p>
</div>
<div role="region" aria-label="Texas A&amp;M Team Develops Novel Antibacterial Polymers That Can Kill Bacteria content 2">
<p>The team’s paper, which features Michaudel Lab member and Texas A&amp;M chemistry Ph.D. graduate Dr. Sarah Hancock ’23 as first author, can be <a href="https://www.pnas.org/doi/10.1073/pnas.2311396120" target="_blank" rel="noopener">viewed online</a>&nbsp;along with related figures and captions. Other key contributors from the Michaudel Lab are chemistry graduate student An Tran ’23, postdoctoral scholar Dr. Arunava Maity and former postdoctoral scholar Dr. Nattawut Yuntawattana, who is now an assistant professor of materials science at Kasetsart University in Thailand.</p>
<p>This research was funded primarily by Michaudel’s&nbsp;<a href="https://www.nigms.nih.gov/Research/mechanisms/MIRA/Pages/default.aspx" target="_blank" rel="noopener">National Institutes of Health Maximizing Investigators’ Research Award (MIRA)</a>&nbsp;through the&nbsp;<a href="https://www.nigms.nih.gov/" target="_blank" rel="noopener">National Institute of General Medical Sciences</a>.</p>
<p>A native of La Rochelle, France, Michaudel joined the Texas A&amp;M Chemistry faculty in 2018 and holds a joint appointment in the <a href="https://engineering.tamu.edu/materials/index.html" target="_blank" rel="noopener">Department of Materials Science and Engineering</a>. In addition to an NIH MIRA in 2020, his career honors to date include a 2022 National Science Foundation Faculty Early Career Development (CAREER) Award, a 2022 American Chemical Society Polymeric Materials: Science and Engineering (PMSE) Young Investigator Award and a 2021 Thieme Chemistry Journals Award.</p>
<p>Learn more about Michaudel and his research at <a href="https://www.michaudellab.org/" target="_blank" rel="noopener">michaudellab.org</a>.</p>
</div>

                </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[3500 arrested in global cybercrime crackdown (133 pts)]]></title>
            <link>https://www.scmagazine.com/news/3500-arrested-300m-seized-in-global-cybercrime-crackdown</link>
            <guid>38736525</guid>
            <pubDate>Fri, 22 Dec 2023 17:36:55 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.scmagazine.com/news/3500-arrested-300m-seized-in-global-cybercrime-crackdown">https://www.scmagazine.com/news/3500-arrested-300m-seized-in-global-cybercrime-crackdown</a>, See on <a href="https://news.ycombinator.com/item?id=38736525">Hacker News</a></p>
<div id="readability-page-1" class="page"><article><div><p>Police in 34 countries arrested 3500 people and seized assets worth $300 million in the latest iteration of what has become an annual coordinated global crackdown on cybercrime.</p><p><a href="https://www.interpol.int/en/News-and-Events/News/2023/USD-300-million-seized-and-3-500-suspects-arrested-in-international-financial-crime-operation">According to Interpol</a>, Operation HAECHI IV ran from July to December and targeted seven types of scams: voice phishing, romance scams, online sextortion, investment fraud, money laundering associated with illegal online gambling, business email compromise fraud, and e-commerce fraud.</p><p>As a result of the operation, authorities blocked 82,112 suspicious bank accounts, seizing a total of $199 million in hard currency and a further $101 million worth of virtual assets.</p><p>Interpol’s executive director of police services, Stephen Kavanagh, said the “staggering” sum seized was a clear illustration of the incentives that were driving an explosive growth in transnational organized crime.</p><p>“This represents the savings and hard-earned cash of victims,” he said. “This vast accumulation of unlawful wealth is a serious threat to global security and weakens the economic stability of nations worldwide.”</p><p>Interpol said Operation HAECHI IV involved investigators working together to detect online fraud and freeze associated bank and virtual asset service provider accounts using Interpol’s Global Rapid Intervention of Payments (I-GRIP), a stop-payment mechanism which helps countries work together to block criminal proceeds.</p><p>Interpol helped frontline officers identify 367 virtual asset accounts linked to transnational organized crime. Assets in those accounts have been frozen as local police continue their investigations.</p><h2>Dragnet pulls in more AI-powered crime</h2><p>In one case resulting from the operation, Filipino and Korean authorities worked together to apprehend a “high-profile online gambling criminal” who was arrested in Manila after spending two years on the run from Korea's National Police Agency. The illegal gambling operation the man allegedly ran was dismantled.</p><p>Interpol published two “purple notices” – warnings about emerging digital investment fraud practices – during the operation.</p><p>One alerted police around the world to a new scam detected in Korea involving the sale of non-fungible tokens (NFTs) with promises of huge returns, which turned out to be a <a href="https://www.scmagazine.com/analysis/crypto-nft-losses-believed-to-hit-25-trillion-says-industry-researcher">“rug pull” scam</a> where the developers abruptly abandon a project and investors lose their money.</p><p>The second purple notice warned about the use of AI and deep fake technology to lend credibility to scams by enabling criminals to hide their identities and to pretend to be a family member, friend, or love interests of the person they are attempting to dupe.</p><p>“The UK leg of the operation reported several cases where AI-generated synthetic content was used to deceive, defraud, harass, and extort victims, particularly through impersonation scams, online sexual blackmail, and investment fraud,” Interpol said.</p><p>“Cases also involved the impersonation of people known to the victims through voice cloning technology.”</p><p>Investment fraud, business email compromise and e-commerce fraud accounted for 75 per cent of cases investigated during the operation.</p><h2>Arrests and seizures keep growing</h2><p>A similar operation last year, <a href="https://www.scmagazine.com/brief/interpol-cybercrime-crackdown-leads-to-seizure-of-130m">HAECHI III</a>, netted almost 1000 arrests and $130 million in assets.</p><p>“HAECHI IV’s 200 per cent surge in arrests shows the persistent challenge of cyber-enabled crime, reminding us to stay alert and keep refining our tactics against online fraud, which is why INTERPOL operations like this are so important” Kavanagh said.</p><p>The first operation in the series, HAECHI-I, involved police from nine countries in Asia working together between September 2020 and March 2021 to make 585 arrests and seize $83 million.</p><p>Interpol’s head of National Central Bureau in Korea, Kim Dong Kwon, praised the international policing effort that led to the increased results achieved by HAECHI IV.</p><p>“Despite criminals' endeavors to gain illicit advantages through contemporary trends, they will eventually be apprehended and face due punishment. To accomplish this, Project HAECHI will consistently evolve and expand its scope.”</p><p>As SentinalOne explained in a 2021 <a href="https://www.sentinelone.com/blog/the-good-the-bad-and-the-ugly-in-cybersecurity-week-49-3/">post about HAECHI-II</a>: in Korea, Haechi is a popular mythical animal widely used as a symbol of justice. The countries participating in this year’s operation were: Argentina, Australia, Brunei, Cambodia, Cayman Islands, Ghana, India, Indonesia, Ireland, Japan, Kyrgyzstan, Laos, Liechtenstein, Malaysia, Maldives, Mauritius, Nigeria, Pakistan, Philippines, Poland, Korea, Romania, Seychelles, Singapore, Slovenia, South Africa, Spain, Sweden, Thailand, United Arab Emirates, United Kingdom, United States and Vietnam. Hong Kong also participated.</p></div></article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Ohio gives historical status to building that once housed CompuServe (149 pts)]]></title>
            <link>https://news.wosu.org/2023-12-21/ohio-gives-historical-status-to-building-that-once-housed-internet-service-pioneer-compuserve</link>
            <guid>38736419</guid>
            <pubDate>Fri, 22 Dec 2023 17:28:51 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://news.wosu.org/2023-12-21/ohio-gives-historical-status-to-building-that-once-housed-internet-service-pioneer-compuserve">https://news.wosu.org/2023-12-21/ohio-gives-historical-status-to-building-that-once-housed-internet-service-pioneer-compuserve</a>, See on <a href="https://news.ycombinator.com/item?id=38736419">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>
        Published&nbsp;December 21, 2023 at 3:37 PM EST
    </p>
    <meta content="2023-12-21T20:37:11.743Z">


                                        </div><div>
                                        <p>A central Ohio building that once served as the global headquarters for CompuServe has been recognized with historic marker status by the state.</p><p>At its height in the 1990's, the pioneering tech company — one of the first to offer commercial internet services — was known by the public for online forums that offered news, message boards and data file transfers. The firm also introduced the GIF image format back in 1987.</p><p>“This may be the first historical marker about the internet. Most history is not recognized and celebrated in your lifetime, but this is and its really special,” said Ohio Lt. Governor Jon Husted at a dedication event featuring state officials and former CompuServe CEO Jeff Wilkins. “Today we have a vibrant, growing tech economy in the state of Ohio. And it began right here.”</p><p>The company opened its headquarters in 1973 and, according to records housed at the Columbus Metropolitan Library, closed the location in the summer of 2006. Ohio's historical markers program, started in the 1953, commemorates unique sites that shape state history.</p><p>Wilkins, who co-founded CompuServe in 1969, also attempted settle a classic internet argument over the pronunciation of the GIF image format at the dedication. He recalled a story about how Steve Wilhite, the engineer who helped create the format, once held up a sign at an awards ceremony read: “It’s pronounced JIF.”</p>
                                    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[SymbOS Z80 multitasking operating system (166 pts)]]></title>
            <link>http://www.symbos.de/</link>
            <guid>38736054</guid>
            <pubDate>Fri, 22 Dec 2023 16:57:22 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="http://www.symbos.de/">http://www.symbos.de/</a>, See on <a href="https://news.ycombinator.com/item?id=38736054">Hacker News</a></p>
&lt;Unparsable&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[Schrödinger equation emerges mathematically from classical mechanics (2012) (164 pts)]]></title>
            <link>https://www.researchgate.net/publication/241778960_A_Pseudo-Quantum_Triad_Schrodinger%27s_Equation_the_Uncertainty_Principle_and_the_Heisenberg_Group</link>
            <guid>38735725</guid>
            <pubDate>Fri, 22 Dec 2023 16:29:27 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.researchgate.net/publication/241778960_A_Pseudo-Quantum_Triad_Schrodinger%27s_Equation_the_Uncertainty_Principle_and_the_Heisenberg_Group">https://www.researchgate.net/publication/241778960_A_Pseudo-Quantum_Triad_Schrodinger%27s_Equation_the_Uncertainty_Principle_and_the_Heisenberg_Group</a>, See on <a href="https://news.ycombinator.com/item?id=38735725">Hacker News</a></p>
Couldn't get https://www.researchgate.net/publication/241778960_A_Pseudo-Quantum_Triad_Schrodinger%27s_Equation_the_Uncertainty_Principle_and_the_Heisenberg_Group: Error: Request failed with status code 403]]></description>
        </item>
        <item>
            <title><![CDATA[From Nand to Tetris: Building a Modern Computer from First Principles (583 pts)]]></title>
            <link>https://www.nand2tetris.org</link>
            <guid>38735066</guid>
            <pubDate>Fri, 22 Dec 2023 15:31:59 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.nand2tetris.org">https://www.nand2tetris.org</a>, See on <a href="https://news.ycombinator.com/item?id=38735066">Hacker News</a></p>
<div id="readability-page-1" class="page"><p id="comp-j847hr0w" data-testid="richTextElement"><h6><span><span><span><span><span>And of the book </span></span></span></span><a href="https://www.amazon.com/Elements-Computing-Systems-Building-Principles/dp/0262640686/ref=ed_oe_p" target="_blank"><span><span><span><span><span>The Elements of Computing Systems,&nbsp;</span></span></span></span></span></a><span><span><span><span>By </span></span></span></span><a href="http://www.cs.huji.ac.il/~noam/" target="_blank"><span><span><span><span><span>Noam Nisan</span></span></span></span></span></a><span><span><span><span> and</span><span> </span><a href="http://www.shimonschocken.com/" target="_blank"><span><span>Shimon Schocken</span></span></a><span> (MIT Press)</span></span></span></span></span></h6></p><div id="comp-j847if12" data-testid="richTextElement"><p><span><span><span><span>The site contains all the lectures, project materials and tools necessary for building a general-purpose computer system and a modern software hierarchy from the ground up.<p>

The materials are aimed at students, instructors, and self-learners. Everything is free and open-source, as long as you operate in a non-profit, educational setting.</p></span></span></span></span><br>
&nbsp;</p>

<p><span><span><span><span>The materials also support two on-line courses:&nbsp;</span></span></span><span><span><span><span><a href="https://www.coursera.org/learn/build-a-computer" target="_blank" rel="noreferrer noopener">Nand2Tetris Part</a></span></span></span></span><span><span><span><a href="https://www.coursera.org/learn/build-a-computer" target="_blank" rel="noreferrer noopener"><span><span> </span></span></a></span></span></span><span><span><span><span><a href="https://www.coursera.org/learn/build-a-computer" target="_blank" rel="noreferrer noopener">I</a></span></span><span><span><span> </span></span></span><span>(hardware projects/chapters 1-6), and&nbsp;</span><a href="https://www.coursera.org/learn/nand2tetris2" target="_blank" rel="noreferrer noopener"><span><span>Nand2Tetris Part II</span></span></a></span></span><span><span><span> (software projects/chapters 7-12).</span></span></span></span></p>

<p><span><span>​</span></span></p>

<p><span><span><span><span>Nand to Tetris courses are taught at 400+ universities, high schools, and bootcamps. The students who take them range from high schoolers to Ph.D. students to Google engineers. Here is the&nbsp;</span></span></span><a href="https://drive.google.com/file/d/1EWCOVIcg0-dX0XtL3KwNyra6jzMogXLL/view?usp=sharing" target="_blank" rel="noreferrer noopener"><span><span><span><span>extended course syllabus.</span></span></span></span></a></span></p>

<p><span><span>​</span></span></p>

<p><span><span><span><span><span><span>Instructors:</span></span></span></span></span><span><span><span>&nbsp;For additional course materials, contact <a data-auto-recognition="true" href="mailto:schocken@gmail.com">schocken@gmail.com</a></span></span></span></span></p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[The Hyperloop was always a scam (156 pts)]]></title>
            <link>https://www.disconnect.blog/p/the-hyperloop-was-always-a-scam</link>
            <guid>38734909</guid>
            <pubDate>Fri, 22 Dec 2023 15:16:35 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.disconnect.blog/p/the-hyperloop-was-always-a-scam">https://www.disconnect.blog/p/the-hyperloop-was-always-a-scam</a>, See on <a href="https://news.ycombinator.com/item?id=38734909">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><article><div dir="auto"><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F07c65282-6c6f-4629-87c7-58ed30f27658.heic" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F07c65282-6c6f-4629-87c7-58ed30f27658.heic 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F07c65282-6c6f-4629-87c7-58ed30f27658.heic 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F07c65282-6c6f-4629-87c7-58ed30f27658.heic 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F07c65282-6c6f-4629-87c7-58ed30f27658.heic 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F07c65282-6c6f-4629-87c7-58ed30f27658.heic" width="1200" height="675" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/07c65282-6c6f-4629-87c7-58ed30f27658.heic&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:675,&quot;width&quot;:1200,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:31189,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/heic&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:false,&quot;topImage&quot;:true,&quot;internalRedirect&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F07c65282-6c6f-4629-87c7-58ed30f27658.heic 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F07c65282-6c6f-4629-87c7-58ed30f27658.heic 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F07c65282-6c6f-4629-87c7-58ed30f27658.heic 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F07c65282-6c6f-4629-87c7-58ed30f27658.heic 1456w" sizes="100vw" fetchpriority="high"></picture></div></a><figcaption>Original Hyperloop concept image.</figcaption></figure></div><p><span>Ten years after Elon Musk unveiled the white paper for the vacuum-tube transport system he dubbed the Hyperloop, it’s time to drive the final nail into its coffin. Earlier this year, </span><em>Motley Fool</em><span> was already reporting that </span><a href="https://www.fool.com/investing/2023/02/21/hyperloop-startups-are-dying-a-quiet-death/" rel="">things were tough in Hyperloop world</a><span> with startups “dying a quiet death” as higher interest rates meant investors weren’t going for projects that were clearly never going to pay off.</span></p><p><span>Hyperloop One, previously known as </span><em>Virgin</em><span> Hyperloop One, was struggling too. Last year it finally had to admit its passenger tube dreams were never going to be realized, so it tried to </span><a href="https://www.bbc.com/news/technology-60478719" rel="">convince some Emiratis</a><span> that narrow, low-capacity tubes were actually going to be a great way of moving cargo. Well, they clearly didn’t buy it for long because yesterday </span><em>Bloomberg</em><span> reported Hyperloop One is </span><a href="https://www.bloomberg.com/news/articles/2023-12-21/hyperloop-one-to-shut-down-after-raising-millions-to-reinvent-transit" rel="">finally shutting down</a><span>.</span></p><p>To be clear, Hyperloop One is not associated with Musk himself. After launching in 2014, it was partly funded from Richard Branson’s fortune for a while, but even that didn’t last, with the Virgin branding disappearing along with the vision for transforming passenger transportation. I sincerely hope the media (and investors) take its collapse as a sign to stop giving any oxygen to Hyperloop fantasies. The technology was never really meant to go anywhere. Its main goal was always to stop a better transport future from being realized.</p><p>A decade on, people often forget what was really motivating the Hyperloop when Musk first started pushing it. In the early 2010s, there was a big debate around California’s plan to build a high-speed rail line from Los Angeles to San Francisco, with further extensions to San Diego and Sacramento to follow. Naturally, conservative, automotive, and airline interests were vehemently opposed to a technology that Japan and Europe had been living with for decades arriving on American shores because it threatened their commercial interests.</p><p><span>Elon Musk, as an automaker and (let’s be honest) somewhat of a conservative himself, eagerly adopted the arguments against the bullet train with his own spin. He began incessantly repeating a line he </span><a href="https://www.youtube.com/watch?v=qox_m6jyfmA" rel="">doled out</a><span> at the D11 conference with Walt Mossberg and Kara Swisher in 2013, calling it, “the slowest bullet train in the world and the most expensive bullet train per mile in the world” — and that meant it not only had to be opposed, but that the geniuses in Silicon Valley could surely do better than people who actually had a clue about trains and transportation.</span></p><p><span>When he unveiled the Hyperloop concept, Musk claimed that building it on the Los Angeles to San Francisco route would not only result in a far faster trip, but would also cost only a fraction of the price — as little as $6 billion. That figure was immediately debunked by people who </span><a href="https://web.archive.org/web/20130819164738/http://america.aljazeera.com/articles/2013/8/14/economists-don-tbelievethehyperloop.html" rel="">actually understood what went into building that kind of infrastructure</a><span>, with the real cost estimated around at least $100 billion, even though it would have far less capacity than a high-speed train. But there’s another feature that’s often forgotten: the Hyperloop was also for cars. “You just drive on, and the pod departs,” Musk told </span><em>Bloomberg Businessweek</em><span> in </span><a href="https://web.archive.org/web/20130815003336/http://www.businessweek.com/articles/2013-08-12/revealed-elon-musk-explains-the-hyperloop" rel="">his first interview</a><span> about the idea. That fantasy would show up again in the Boring Company a few years later.</span></p><p><span>In 2013, Musk’s star was rising. He was gracing the covers of magazines and being hailed as our future-builder. He’d already served as inspiration for Robert Downey Jr.’s take on Tony Stark in </span><em>Iron Man</em><span> and was largely seen as a man who could do no wrong. When he chose to speak out against the bullet train, that amplified the campaign against it, further seeping political support from an already challenged project. And that was the point. Musk never had any intention of building the Hyperloop. He only needed it to help kill or substantially delay the high-speed rail project and the alternate vision of sustainable </span><em>collective</em><span> transportation it offered. It threatened his interests as an automaker and his elite vision of “individualized” mobility that simply worked better for him.</span></p><p><span>In 2015, Ashlee Vance’s biography </span><em>Elon Musk: Tesla, SpaceX, and the Quest for a Fantastic Future</em><span> was published to an audience eager to consume a hagiography of the man supposedly saving the world and preparing us to colonize the stars. But buried within those 300-odd pages was an admission that many people seemed to have glossed over. On the subject of the Hyperloop, Vance wrote,</span></p><blockquote><p>Musk told me that the idea originated out of his hatred for California’s proposed high-speed rail system. … He insisted the Hyperloop would cost about $6 billion to $10 billion, go faster than a plane, and let people drive their cars onto a pod and drive out into a new city. At the time, it seemed that Musk had dished out the Hyperloop proposal just to make the public and legislators rethink the high-speed train. He didn’t actually intend to build the thing. … With any luck, the high-speed rail would be canceled. Musk said as much to me during a series of e-mails and phone calls leading up to the announcement.</p></blockquote><p><span>The only thing that could be clearer is if Vance released the e-mails and phone calls he’s referring to, but what he wrote is conclusive enough. Years later, after I </span><a href="https://twitter.com/parismarx/status/1167410460125097990" rel="">began</a><span> </span><a href="https://time.com/6203815/elon-musk-flaws-billionaire-visions/" rel="">resurrecting</a><span> that passage, Vance </span><a href="https://jalopnik.com/did-musk-propose-hyperloop-to-stop-california-high-spee-1849402460" rel="">claimed</a><span> my telling was “vaguely accurate but a disingenuous take on the situation.” Speaking to </span><em>Jalopnik</em><span> in 2022, he said, “I honestly do not think that was the goal of Hyperloop at all. I think if there was a better public transport system, my impression — and I think it’s genuine — is that Elon would be all for it.” Yet that completely contradicted what he had written years earlier, that Musk wanted to make lawmakers “rethink the high-speed train” and hoped it “would be canceled.”</span></p><p><span>Vance’s original telling is more credible because stopping collective transport projects has become a pattern for Musk. He proposed the Hyperloop to imperil high-speed rail. He also proposed the Boring Company tunnel system as his solution to traffic, after previously believing </span><a href="https://la.curbed.com/2018/11/9/18077612/elon-musk-mayor-garcetti-tunnels-boring-company" rel="">double-decker highways</a><span> would do the trick, instead of advocating for much better public transit because he felt transit was “</span><a href="https://www.wired.com/story/elon-musk-awkward-dislike-mass-transit/" rel="">a pain in the ass</a><span>” and suggested it was filled with serial killers. Counter to Vance’s more recent suggestion, there’s no evidence Musk would back better public transit, but plenty that he wanted to stall out plans for improved collective transport altogether. He did exactly that with the Boring Company: selling cities </span><a href="https://nymag.com/intelligencer/2022/08/elon-musks-biggest-boondoggle.html" rel="">useless tunnels</a><span> that were rarely ever built and which often displaced realistic transit plans.</span></p><p data-attrs="{&quot;url&quot;:&quot;https://www.disconnect.blog/p/the-hyperloop-was-always-a-scam?utm_source=substack&amp;utm_medium=email&amp;utm_content=share&amp;action=share&quot;,&quot;text&quot;:&quot;Share&quot;,&quot;action&quot;:null,&quot;class&quot;:null}" data-component-name="ButtonCreateButton"><a href="https://www.disconnect.blog/p/the-hyperloop-was-always-a-scam?utm_source=substack&amp;utm_medium=email&amp;utm_content=share&amp;action=share" rel=""><span>Share</span></a></p><p><span>Unfortunately, the harm of the Hyperloop went far beyond California. Companies claiming they’d realize the bullshit idea claimed to be moving forward with projects in </span><a href="https://indianexpress.com/article/explained/explained-after-hype-over-hyperloop-why-theres-a-question-mark-over-ultra-modern-project-6223864/" rel="">India</a><span>, </span><a href="https://www.thenationalnews.com/uae/transport/virgin-hyperloop-aims-for-2022-test-launch-as-chief-warns-against-rushing-technology-1.842589" rel="">Dubai</a><span>, </span><a href="https://lareleveetlapeste.fr/lhyperloop-delon-musk-un-fiasco-planetaire-qui-a-coute-plus-de-55-millions-deuros-a-la-metropole-de-toulouse/" rel="">France</a><span>, </span><a href="https://edmonton.ctvnews.ca/alta-hyperloop-project-awaiting-government-meeting-committing-to-stop-in-red-deer-1.6217544" rel="">Canada</a><span>, and countless other parts of the world only for every single one to amount to nothing. Sure, some short tubes got built in a few places to try to keep squeezing money out of investors and governments, but the only thing they really achieved was to distract people with fantasies while they could’ve been focused on building something real.</span></p><p><span>In the time since California started talking about high-speed rail and Elon Musk interjected with his fantasy to help sidetrack it, China moved ahead and built a network consisting of 42,000 kilometres (26,000 miles) of track. Europe is continuing to expand its own network, and Japan is building a maglev line that will run at speeds of over 500 km/h (310 mph). The first segment from Tokyo to Nagoya could </span><a href="https://www.japantimes.co.jp/news/2023/12/15/japan/maglev-shinkansen-opening-delayed/" rel="">open by 2027</a><span>. Not to be outdone, China is working on </span><a href="https://www.cnbctv18.com/travel/china-completes-first-operation-of-worlds-fastest-train-that-travels-at-600-kmhour-16321471.htm" rel="">a maglev of its own</a><span> to beat its Japanese rivals.</span></p><p><span>While the Hyperloop deception spread far and wide, nowhere was it stronger than in the United States. As countries around the world </span><a href="https://en.wikipedia.org/wiki/List_of_high-speed_railway_lines" rel="">moved forward</a><span> with real transport improvements, North Americans were distracted by the fantasies of clueless, but self-confident tech moguls. They left people trapped in their cars and denied better options to get around that people in many other parts of the world — even those that are quite a bit poorer — take for granted. Now all they can do is shovel money at automakers to try to power cars with batteries instead of internal combustion engines. They have no vision for a better, less car dependent alternative.</span></p><p>The tech industry’s move into transportation was not only a failure; it was an active campaign to deny the public access to better transit and trains because the billionaires of Silicon Valley don’t personally want to get around that way. The Hyperloop was one part of that, but so were the Boring Company, ride-hailing services, and self-driving cars. The Hyperloop’s failure provides a lesson we’re learning far too late: that Silicon Valley won’t deliver us a better world if they can’t find some way to profit off it. We need to stop falling for their grand deceptions, and tell our media to stop echoing them too.</p></div></article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[LED Industrial Piercing (216 pts)]]></title>
            <link>https://mitxela.com/projects/scaffold</link>
            <guid>38734164</guid>
            <pubDate>Fri, 22 Dec 2023 13:59:03 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://mitxela.com/projects/scaffold">https://mitxela.com/projects/scaffold</a>, See on <a href="https://news.ycombinator.com/item?id=38734164">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="mxmain"><p><a href="https://mitxela.com/projects/hardware"><img onload="this.style.opacity=1;" src="https://mitxela.com/img/titles/mitxela_dot_com-65.png" title="Back to Hardware" alt="Back to Hardware"></a></p><p>22 Dec 2023<br><b>Progress: Complete</b></p><p>
A friend of mine wears a type of jewellery known as an "industrial piercing" or "scaffold" that puts a bar across her ear.</p><p>

Always looking for a new challenge, I had the idea to fill the bar with LEDs.</p><p>

Watch the following video to experience the journey:</p><p>

<iframe width="704" height="396" src="https://www.youtube.com/embed/T3evnh9lT9k" allowfullscreen=""></iframe></p><p>

This is one of a number of projects I rushed through in the run-up to the closure of London Hackspace. I've now not had access to a milling machine for over a year. Drilling holes in the side of a hypodermic needle would be a little tricky without one.</p><h3>Design</h3><p>
Our journey begins in the summer of '22, when I snapped this image of my friend's ear. (With permission!)</p><p>

<img src="https://mitxela.com/img/uploads/metal/scaffold/scaff.jpg" alt="The original piercing the design was based on"></p><p>

Could it be filled with LEDs? Most people laughed in disbelief when I suggested it. But everyone agreed that it would be really cool.</p><p>

You can buy stainless tubing in various sizes, but nothing quite compares to the cheapness and availability of hypodermic needles. A gauge 16 needle is exactly the right diameter, and the sidewall is so thin you couldn't ask for anything better. But needles are quite short, there was some concern that it wouldn't be long enough. I did find a supplier of extra long syringe needles intended for model makers, there might have been something in the right size, but after some careful measurements and calculations I decided the regular needles would be OK.</p><p>

I did the initial design in OpenSCAD, and having rotated the above photo into a scaled and skew-corrected position we could overlay the two in photoshop.</p><p>

<img src="https://mitxela.com/img/uploads/metal/scaffold/overlay.jpg" alt="CAD design overlaid onto rotated photograph in photoshop"></p><p>

That's before the final adjustments to length were made. This mockup was invaluable in getting the LED pitch and offset correct.</p><p>

OpenSCAD continued to be helpful as I came up with the circuit, thinking about how it would fold together and how much tolerance we needed to leave. Just a basic idea is all that's required.</p><p>

<img src="https://mitxela.com/img/uploads/metal/scaffold/openscad.png" alt="OpenSCAD screenshot"></p><p>

The PCB design had to wait until I'd gotten this all exactly right, the distances between the microcontroller and the LEDs is critical.</p><p>

A flex PCB is the way to get the very thinnest board possible. You can technically get 0.2mm FR4 from some board houses but it'll cost about the same as a flex PCB anyway. We could have gone flex-rigid, that would have let us use the thinnest material for the LEDs and a multilayer board for the microcontroller, but it would have cost a lot more and still not allowed us to put a connector above the microcontroller.</p><p>

<img src="https://mitxela.com/img/uploads/metal/scaffold/kicad.png" alt="KiCad PCB design"></p><p>

Once I'd settled on those off-the-shelf earrings as the source of my battery holder, that determined quite a bit about the design too. The metal case is our positive battery terminal, we need to fit a wire that pokes out of the end of the needle to be the ground pin.</p><p>

I did a few redesigns of the board before settling on the above design. Even forgetting the fiddly exact dimensions of how it will go together, routing the LED tracks was tricky. If we had gone for the 0201 LEDs they could have been mounted in the other orientation, with tracks passing directly underneath them. Once I'd checked the dimensions I felt a lot more comfortable with the 0402 LEDs though.</p><h3>Metalwork</h3><p>
Once I'd made the tiny brass V-block arrangement, crossdrilling the needle took barely five minutes. In my mind it was going to be a big deal.</p><p>

<img src="https://mitxela.com/img/uploads/metal/scaffold/metalwork1.jpg" alt="Drilling a 1mm hole in the side of a hypodermic needle"></p><p>

It helps that I used an endmill to drill the holes, not only would a drillbit have wandered on the curved surface but the carbide is simply a lot stiffer. I should really invest in some tiny carbide drills, you might say that HSS just doesn't cut it for me.</p><p>

The dodgy-but-still-kind-of-legit tactic of holding an endmill in the tailstock chuck no doubt made some viewers grimace.</p><p>

<img src="https://mitxela.com/img/uploads/metal/scaffold/metalwork2.jpg" alt="6mm endmill being used to produce a flat-bottomed hole"></p><p>

Drill shanks are soft, chuck jaws are hard. Endmill shanks are also hard which means the jaws won't grip them properly. If I'd had the proper tools I would have used them. Anyway you can't argue with the results, the flat-bottomed hole was spot-on. The 1.6mm through hole was a lot messier, the drillbit really wanted to wander. Again I yearned for all my tooling to be solid tungsten carbide.</p><p>

<img src="https://mitxela.com/img/uploads/metal/scaffold/metalwork3.jpg" alt="Drilling a tiny hole in stainless steel"></p><p>

This formed a decent press-fit onto the needle. I suppose the hollow tube is fairly compressible. Even so, once I'd settled on the right position I gave the needle a healthy smear of cyanoacrylate before pressing it in for the final time. Remember, the battery-holding collet on the other end is going to be mounted and unmounted by simply pulling on this join, so we want it to be very secure.</p><p>

<img src="https://mitxela.com/img/uploads/metal/scaffold/metalwork4.jpg" alt="Needle fitted into endcap"></p><h3>Circuit</h3><p>
When the circuit boards arrived, I soldered one together almost immediately.</p><p>

<img src="https://mitxela.com/img/uploads/metal/scaffold/flex-circuit.jpg" alt="Bare flex PCB"></p><p>

For multiple reasons I then put it aside for about a year. It was only a few months ago that I powered the board up and got some flashing patterns going.</p><p>

<img src="https://mitxela.com/img/uploads/metal/scaffold/circuit1.jpg" alt="Circuit lighting up"></p><p>

The microcontroller is an ATtiny44. If I was designing this today I'd have gone with the CH32V003, which admittedly has worse power consumption, but the benefit of that one-wire debug pin can't be overstated. It now seems so silly that I had to wire up six pins to be able to reflash this thing.</p><p>

Sliding the circuit inside of the case and it already looks nearly finished.</p><p>

<img src="https://mitxela.com/img/uploads/metal/scaffold/circuit2.jpg" alt="Circuit inside of the metal case"></p><p>

The thick black wire protruding from the end needs to be trimmed to form our battery negative terminal. But as you can see the power and ground pins are also exposed on the connector.</p><p>

In terms of software, the LEDs are charlieplexed and the pattern I settled on loops over the course of about three minutes. All in, the software is similar but a fair bit simpler than the <a href="https://mitxela.com/projects/charliestar">charliestar</a>, which does more with less.</p><p>

I will link to the source code at the end of the page.</p><h3>Assembly</h3><p>
I should mention, the cheapo earrings I used for the battery case are <i>not</i> made from stainless. It's nickel-plated brass, which I realised when I started machining it. It does make me think I'd have had an easier time making the end out of brass and nickel-plating it. On the other hand the stainless does look pretty good, and with any luck it'll stay that shiny for a long long time.</p><p>

<img src="https://mitxela.com/img/uploads/metal/scaffold/finished1.jpg" alt="Piercing fully assembled but with battery holder removed"></p><p>

At this stage everything appeared to work perfectly, so I prepared some clear food-grade epoxy to fill the holes.</p><p>

<img src="https://mitxela.com/img/uploads/metal/scaffold/finished2.jpg" alt="Piercing fully assembled and illuminated"></p><p>

I assume the reader has already watched the video, but if not, spoilers: after potting the bar with clear epoxy, one of the LEDs stopped working. Luckily it's on the end, so it's not particularly obvious. As this happened after the epoxy cured, there's no way to repair it.</p><p>

<img src="https://mitxela.com/img/uploads/metal/scaffold/finished3.jpg" alt="Piercing fully assembled and only five LEDs illuminated"></p><p>

I'm not above building another one from scratch except that I'm currently between workshops. That, and I feel like it's been long enough that the next version should also be an improvement, not merely a second attempt.</p><p>

<img src="https://mitxela.com/img/uploads/metal/scaffold/finished-holding.jpg" alt=""></p><h3>End Cap</h3><p>
I originally planned to add one or more end caps to the design. These would connect into that programming header, which doubles as simply GPIO pins of the microcontroller, and offer various interfaces, such as a microphone or an IR receiver.</p><p>

I'm not saying it won't happen in the future but I didn't get round to it yet. And besides, the flashing pattern I settled on seems interesting enough for now.</p><h3>Case</h3><p>
The case was modelled in OpenSCAD too. It's tricky to do fillets and chamfers, and I wasn't trying to make this look amazing or anything. In fact with the dead LED I wanted to wrap this up as quick as possible. But the case turned out great, it holds the device perfectly and you can just chuck it in a pocket without worrying.</p><p>

Here's an embeddiment. The scaffold is stored with the battery pack in the "off" position, i.e. pulled out a little.</p><p>


<model-viewer src="/img/uploads/metal/scaffold/scaffold-case.wrl" start="4.5,5.4,-6.4">
  Sorry, your browser doesn't support embedded 3D models.
</model-viewer></p><p>

The hinges are dressmaker's pins through just-barely-press-fit holes. It's stiff enough that the lid will stay at any angle.</p><p>

<img src="https://mitxela.com/img/uploads/metal/scaffold/finished-with-case.jpg" alt="Illuminated within case"></p><p>

The texture on the lid is from the bed of the 3D printer. It helps parts adhere while they're printing, but it also adds a bit of interest to the end result.</p><p>

<img src="https://mitxela.com/img/uploads/metal/scaffold/case-closed.jpg" alt="3D printed case"></p><h3>Modelled</h3><p>
It's a while since I've had a living, breathing person pose for my macro lens. It is quite tricky to get the exposure right, usually my subjects are inanimate.</p><p>

<img src="https://mitxela.com/img/uploads/metal/scaffold/wearing1.jpg" alt="LED Industrial Piercing being worn"></p><p>

The flash does somewhat drown out the LEDs, makes it look a bit clinical.</p><p>

<img src="https://mitxela.com/img/uploads/metal/scaffold/wearing2.jpg" alt="LED Industrial Piercing being worn"></p><p>

It's entirely possible I'll get some more pictures of the piercing being worn in future. It would be cool to do some long exposures with flash, so that the image is visible but the LEDs leave a trail.</p><h3>Battery Life</h3><p>
I measured the current draw at just over 3mA. LR521 batteries have a capacity of about 10mA at best, which naively implies about 3 hours of battery life. However, the alkaline discharge curve is pretty steep and it's not clear at what voltage the chip will brown-out, it could potentially cut out well before the batteries are depleted.</p><p>

However, there's another battery in the same form factor called SR521, which is a silver-oxide battery. Not only does that give us about double the capacity, the discharge curve is much flatter so with those we should be good for at least five hours.</p><p>

Changing the batteries is about as fiddly as it looks. Really, instead of machining that collet to be bigger, I should have just reduced the diameter of our bar near the end, so the regular battery holders from the cheap earrings could be fitted unmodified. In that case, it would only take a second to swap the whole battery pack, instead of having to unscrew things.</p><h3>Conclusion</h3><p>
Overall, to my surprise, there really wasn't anything particularly challenging in this project, for the most part all the bits I expected to be difficult turned out fine. Evidently the next piercing will need to be smaller and use those 0201 LEDs.</p><p>

<img src="https://mitxela.com/img/uploads/metal/scaffold/finished4.jpg" alt="Piercing fully assembled and illuminated"></p><p>

I'm pretty pleased with the result. I was a bit disheartened when the LED died, as all of my joking about soldering things the hard way using the wrong equipment then sounds a bit pathetic. It's fine if you use terrible equipment and it works regardless, but if you use terrible equipment and it fails to work then you're just a fool.</p><p>

However when I saw the thing being worn I had to admit that it looks really good.</p><p>

<img src="https://mitxela.com/img/uploads/metal/scaffold/wearing3.jpg" alt="LED Industrial Piercing being worn"></p><p>

If you would like to own one of these piercings, you simply can't. At least, it would be even harder for me to make and sell these than most of my inventions as the length has to be sized exactly to the wearer's ear. But, if you're totally loaded and you really want one, get in touch.</p><p>

The source code for the ATtiny is <a href="https://github.com/mitxela/scaffold">here</a>.</p><div>

<p><img src="https://mitxela.com/img/uploads/metal/scaffold/wearing4.jpg" alt="LED Industrial Piercing being worn"></p></div><nav>
<a href="https://mitxela.com/projects/random" title="random project">~</a>
<span typeof="v:Breadcrumb"><a rel="v:url" property="v:title" href="https://mitxela.com/">mitxela.com</a></span> » 
<span typeof="v:Breadcrumb"><a rel="v:url" property="v:title" href="https://mitxela.com/projects">Projects</a></span> » 
<span typeof="v:Breadcrumb"><a rel="v:url" property="v:title" href="https://mitxela.com/projects/hardware">Hardware</a></span> »
<span typeof="v:Breadcrumb"><a rel="v:url" property="v:title" href="https://mitxela.com/projects/scaffold">LED Industrial Piercing</a></span>
<p>Questions? Comments? Check out the <a href="https://mitxela.com/forum">Forum</a>
</p><p><a href="https://mitxela.com/support">Support mitxela.com</a>
</p></nav></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Celebrating the first NES Tetris game crash (122 pts)]]></title>
            <link>https://biggieblog.com/celebrating-the-first-nes-tetris-game-crash/</link>
            <guid>38734106</guid>
            <pubDate>Fri, 22 Dec 2023 13:51:57 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://biggieblog.com/celebrating-the-first-nes-tetris-game-crash/">https://biggieblog.com/celebrating-the-first-nes-tetris-game-crash/</a>, See on <a href="https://news.ycombinator.com/item?id=38734106">Hacker News</a></p>
<div id="readability-page-1" class="page"><article id="post-995">
	

	



	<div>
		

<content>
  <div>

		  <h2>Celebrating the first NES Tetris game crash</h2>
	
	
<p>Today, the NES Tetris community achieved a new milestone.  A player known as “Blue Scuti” reached level 157, a total of 1510 lines deep into the game, placed a piece, and his game crashed.</p>



<p>This post will describe why that is not only interesting, but a thing to be celebrated.</p>



<p>There will be topics in this post that assume you know everything from my <a href="https://biggieblog.com/keeping-classic-tetris-interesting/" target="_blank" rel="noreferrer noopener">previous post about Tetris</a>.  That post has no assumed prior knowledge, so start there and then come back if you want to know more.</p>



<h2>Blue Scuti</h2>



<p>When I wrote that previous post in August 2022, Scuti was only 12 years old.  The minimum age of entry into most competitive Tetris events is 13.  The scene is young, but Scuti is young even by the standards of the scene.  Now age 13, he participated in the 2023 Classic Tetris World Championships, where he took 3rd place.  Accompanied everywhere by his stuffed blue yoshi plush, or other plushies borrowed from his friend “coalbucket”, Scuti established himself very quickly as someone to watch.  He is without a doubt one of the most talented players the game has ever seen, using the rolling playstyle.  <a href="https://www.stwnewspress.com/news/boy-wonder-stillwater-teen-places-third-in-classic-tetris-world-championship/article_0a390482-83ef-11ee-9d4d-b777b200f759.html" target="_blank" rel="noreferrer noopener">An article in his local news gave some additional info</a> about Scuti and his background.</p>



<p>I also attended the Classic Tetris World Championships, where I got 45th.  At the afterparty, I did get a picture with coalbucket (left) and Scuti (center).</p>



<figure><img fetchpriority="high" decoding="async" width="1024" height="768" src="https://i0.wp.com/biggieblog.com/wp-content/uploads/2023/12/PXL_20231016_025250899.jpg?resize=1024%2C768&amp;ssl=1" alt="" srcset="https://i0.wp.com/biggieblog.com/wp-content/uploads/2023/12/PXL_20231016_025250899-scaled.jpg?resize=1024%2C768&amp;ssl=1 1024w, https://i0.wp.com/biggieblog.com/wp-content/uploads/2023/12/PXL_20231016_025250899-scaled.jpg?resize=300%2C225&amp;ssl=1 300w, https://i0.wp.com/biggieblog.com/wp-content/uploads/2023/12/PXL_20231016_025250899-scaled.jpg?resize=768%2C576&amp;ssl=1 768w, https://i0.wp.com/biggieblog.com/wp-content/uploads/2023/12/PXL_20231016_025250899-scaled.jpg?resize=1536%2C1152&amp;ssl=1 1536w, https://i0.wp.com/biggieblog.com/wp-content/uploads/2023/12/PXL_20231016_025250899-scaled.jpg?resize=2048%2C1536&amp;ssl=1 2048w" sizes="(max-width: 1000px) 100vw, 1000px" data-recalc-dims="1"></figure>



<h2>Game Quirks</h2>



<p>Like most old games, NES Tetris doesn’t have safeguards in its game code for every possibility.  Built in 1989 for the Nintendo Entertainment System, it had to prioritize simplicity.  That prompted some assumptions, ones considered pretty safe for decades.</p>



<p>First, the score count stops at 999,999.  This is an entirely reachable score, affectionately called the “maxout.”  Developers either assumed that it was not going to be reached (skill issue), or figured that nobody needed to keep track of score beyond that.</p>



<p>Next, the lookup table of levels stops at 29.  When a player reaches level 30, the byte loaded in to display the level is actually read from a different table entirely, and shows as “00”.  Similar weird values show on all later levels as well.  This is only a display bug, the game still stores levels accurately as a value from 0 to 255.  Level 256, if reached, would be identical to level 0.  Level 29 also marks the final speed increase, which the developers (again due to skill issue) deemed unplayable, so it is fair to believe they considered level 30 to be outside of human ability.</p>



<p>The next interesting display bug comes at level 138.  Here, the lookup table for the colors of each piece, is now indexed incorrectly due to 138-10 being stored as a signed byte, where it gets interpreted as -128.  Code written and forgotten, with quirks that no developer expected would be relevant.  Levels 138 through 255 have unusual color palettes, some of which are exceptionally dark and difficult to play on.  In a somewhat famous video, Greg Cannon’s AI called “StackRabbit” plays through many of these colors, and the names given in this video are now the community names for all of the levels.</p>



<figure><p>
<iframe title="AI BREAKS NES TETRIS! - 102 MILLION and level 237" width="500" height="281" src="https://www.youtube.com/embed/l_KY_EwZEVA?feature=oembed" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen=""></iframe>
</p></figure>



<h3>Game Crash</h3>



<figure><p>
<iframe title="why clearing a single at level 155 crashes nes tetris" width="500" height="281" src="https://www.youtube.com/embed/BpEcjdr_YDo?feature=oembed" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen=""></iframe>
</p><figcaption><mark>Explainer courtesy of HydrantDude</mark></figcaption></figure>



<p>Finally, there is the game crash.  If the player’s score is 999,999, any further points are still added internally.  After adding, the bytes that hold score get reset back to 999,999.  In a wonderful bit of inefficiency, this “add then clamp” routine actually runs many many times, equal to the level the player is on, plus one.  So while a single line clear on level 0 is worth 40 points, and a single line clear on level 99 is worth 4000 points, the latter is actually 40 points, 100 times.  Separately.</p>



<p>The score occupies 3 bytes and singles are the only types of line clear that modify the lowest byte.  Accordingly, single line clears take the most time to add to your score.  At level 155, due to the slow score loop, it becomes possible for the rendering code to take over before the rest of the game logic finishes.  The rendering code assumes that the game logic’s execution pointer is in a known state.  Making it to level 155 violates this assumption, to spectacular and game-crashing results.</p>



<p>A few things need to happen.  First, there needs to be no modifications to the scoring code.  The StackRabbit video above uses a scoring mod, which changes this behavior, which for a long time also led the community to have some misconceptions about the crash.  To get a crash at level 155, you need to actually keep the loathsome 999,999 on your screen while playing long past that score.</p>



<p>Next, you need to hit a crash trigger.  The earliest and most well known crash trigger is “single to enter level 155”, as the level transition coupled with the slow score loop deterministically brings a crash.  If a player misses this trigger, there are <a href="https://docs.google.com/spreadsheets/d/1zAQIo_mnkk0c9e4-hpeDvVxrl9r_HvLSx8V4h4ttmrs/edit?pli=1#gid=0">later opportunities</a>, but most only have a probability of causing a crash.</p>



<h2>What does it look like?</h2>



<figure><video controls="" src="https://biggieblog.com/wp-content/uploads/2023/12/AT-cm_LmewIz4CuLRVlBftzCLL1w.mp4"></video></figure>



<p>In this video, Scuti has already been playing this particular game of NES Tetris for close to 45 minutes.  From level 29 and beyond, the speed shown here is the speed of play.  He is able to think quickly enough and place pieces using a method where multiple fingers each push the controller into his other hand from behind (rolling).</p>



<p>The level display has long since given up, and the score is unhelpful, but other numbers on the screen still have meaning.  The line count E85, is actually 1485.  By dividing by 10 and adding 6, we can find that Scuti is on level 154.  If he gets a single when his line count reads E89, entering level 155, then the game will crash.  But he scores a triple instead, which calculates more quickly due to the base value of a triple being 300.  So he misses the crash.  From E92 and beyond, he doesn’t know if or when when the game will crash and has to plan for anything.</p>



<p>Two levels later, with a line count of F10, the color palette once again becomes dark.  Levels 146, 148, and 151, nicknamed “Dusk”, “Charcoal” and “Australian Outback”, are the only really dark levels necessary to reach the E89 crash trigger, and so these are the ones players practice, or calibrate their display for.  Level 157, “Radium”, isn’t one of the ones that most players practice these days.  Scuti’s play suffers, and the crash has not come yet.</p>



<p>In a tense position, Scuti manages to get a single line to burn, and immediately the game freezes.  The line clears visually, the game updates from F10 to F11 lines, but the pieces stop falling.  A gentle hum replaces the background music.  Inputs do nothing.  Scuti crashed the game.</p>



<h2>History</h2>



<p>In my previous post, I wrote “The only things left to do now on the original cartridge are to literally crash the game, or to perform ridiculous (TAS-only?) crash-avoiding tactics beyond a certain level.”  We’ve now done the first.</p>



<p>Looking at the year and a half since that post, there was a lot of new focus on a game modification that adds a new speed increase at level 39.  </p>



<figure><p>
<iframe title="First Ever Classic Tetris Match Won at the Level 39 SUPER Killscreen" width="500" height="281" src="https://www.youtube.com/embed/QVsqcRf_3uI?feature=oembed" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen=""></iframe>
</p></figure>



<p>This super-killscreen was used in most competitive events since its introduction in 2022, but November 2023 hosted a no-linecap event that got people thinking about crash again.  With lots of spare time, winter break 2023 was when historic movement began.  So let’s look at 3 consecutive very special days in NES Tetris history.</p>



<h3>Tuesday, December 19, 2023</h3>



<p>Fractal161 was the most dedicated to game crash.  He was also the first player to reach level 138 and experience those color palettes on the original rom.  Back from MIT for this year’s winter break, he set up a stream titled “FINAL CRASH GRIND DAY 1”.  The intention was to stream attempts every day, until he achieved this milestone.</p>



<p>That same day, also on winter break, Scuti set a<a href="https://www.youtube.com/watch?v=F0sA2kFM8K4"> new score world record</a>.  Beating EricICX’s long-standing 6.4 million point <a href="https://www.youtube.com/watch?v=SctVBfLjpLg">game</a>, Scuti got 6,609,220.  In this 6.6 million point game, Scuti also played through all 3 challenging levels pre-crash: “Dusk”, “Charcoal” and “Australian Outback”.  He ended up losing the game naturally to a difficult sequence on level 153, but because he was playing on a version of the game that modifies the score, there was not going to be a crash at 155 anyway.</p>



<h3>Wednesday, December 20, 2023</h3>



<p>The very next day, Fractal took back the score world record with 6.7 million.  Because he was going for game crash, he was using the unmodified rom, so his score only showed as 999,999.  It took many people confirming every line clear from the video, to arrive at an exact score of 6,737,260.  With efficient play, Fractal had scored more points before the difficult colors.  His game ended on Dusk, level 146.</p>



<p>On the 20th, Scuti began his own attempts at game crash, providing a battle of titans.</p>



<h3>Thursday, December 21, 2023</h3>



<p>One day later, another player Gerald Freeman joined the race to game crash.  Gerald had a <a href="https://www.youtube.com/watch?v=kRO0znVaL90">game</a> end on level 150 the previous day, with modified score showing 6.2 million, making him the 2nd ever player to pass both Dusk and Charcoal in a run.  Three of the 6 players to have ever reached level 138’s colors, were now playing on the original, unmodified rom, with the same goal.  Crash the game.</p>



<p>That day, Scuti triumphed.  Not on level 155, but level 157.  His final score was recalculated to 6,850,560 points.  Historic milestone, and reclaimed world record.</p>



<p>As I write this, Fractal is adjusting his goal.  He still wants to crash the game, but no longer as the first ever.  Instead, he intends to claim the “best game of classic Tetris ever played” by scoring far more Tetrises on the fastest speed, and still ending with a crash.  Pushing the efficiency harder, to break score world record by <strong>a lot</strong>, multiple millions higher.</p>



<p>It’s an exciting day.</p>



<h2>Who to Watch</h2>



<p>All of the players below have expressed some interest in playing long games of NES Tetris.  Please follow them!  I will also be trying to do cool things in Tetris, but my level PB is 37 which doesn’t put me in any sort of contention for these sorts of achievements.</p>



<p>Blue Scuti: <a href="https://www.twitch.tv/bluescuti">twitch</a> <a href="https://www.youtube.com/channel/UC_BFxKkGiDkj4SvKDLc4r-w">youtube</a></p>



<p>Fractal161: <a href="https://www.twitch.tv/fractal161">twitch</a>, <a href="https://www.youtube.com/channel/UC-5fATPHnwCmelz5_8kEZ-g">youtube</a></p>



<p>Gerald Freeman: <a href="https://www.twitch.tv/gerald_freeman">twitch</a> <a href="https://www.youtube.com/@geraldfreeman2164">youtube</a></p>



<p>Sidnev: <a href="https://www.youtube.com/channel/UC9MrzQ41hbOOOgMoaclEvhw?">youtube</a></p>



<p>Alex Thach: <a href="https://www.youtube.com/@alexthach276">youtube</a></p>



<p>PixelAndy: <a href="https://www.youtube.com/@P1xelAndy">youtube</a></p>



<p>Other content creators who are likely to make a much better video or article about this milestone:</p>



<p>aGameScout: <a href="https://www.youtube.com/@aGameScout">youtube</a></p>



<p>Cobalt: <a href="https://www.youtube.com/@okCobalt">youtube</a></p>



<p>EricICX: <a href="https://www.youtube.com/@EricICX">youtube</a></p>



<p>HydrantDude: <a href="https://www.youtube.com/channel/UCxj-zfOhKarvzLSjt1jjgWA">youtube</a></p>



<p>See you around!</p>
	
	 
		

			
		
	
  </div>
</content>
		
		
<!-- #comments -->
	</div>
</article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Show HN: Heynote – A Dedicated Scratchpad for Developers (878 pts)]]></title>
            <link>https://heynote.com/</link>
            <guid>38733968</guid>
            <pubDate>Fri, 22 Dec 2023 13:33:51 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://heynote.com/">https://heynote.com/</a>, See on <a href="https://news.ycombinator.com/item?id=38733968">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
          <h3>One buffer, many blocks</h3>
          <p>
            At its core, Heynote is a large, persistent text buffer divided into blocks. 
            Creating a new block is as easy as pressing <span><span>⌘</span>-Enter</span>, 
            and pressing <span><span>⌘</span>-A</span> within 
            a block selects the content of just that block.
          </p>
          <p>
            Works great for that Slack message you don't want to accidentally send, a JSON response 
            from an API you're working with, notes from a meeting, your daily to-do list, etc.
          </p>
        </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[In the Long Run, We're All Dad (261 pts)]]></title>
            <link>https://www.astralcodexten.com/p/in-the-long-run-were-all-dad</link>
            <guid>38733440</guid>
            <pubDate>Fri, 22 Dec 2023 12:20:19 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.astralcodexten.com/p/in-the-long-run-were-all-dad">https://www.astralcodexten.com/p/in-the-long-run-were-all-dad</a>, See on <a href="https://news.ycombinator.com/item?id=38733440">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><article><div dir="auto"><p><strong>I.</strong></p><p>In February 2023 I found myself sitting in the waiting room of a San Francisco fertility clinic, holding a cup of my own semen.</p><p>The Bible tells the story of Onan, son of Judah. Onan’s brother died. Tradition dictated that Onan should impregnate his brother’s wife, ensuring that his brother’s line would (in some sense) live on. Onan refused, instead “spilling the seed on the ground”. God smote Onan, starting a 4,000-year-old tradition of religious people getting angry about wasting sperm on anything other than procreative sex.</p><p>Modern academics have a perfectly reasonable explanation for all of this. If Onan had impregnated his brother’s wife, the resulting child would have been the heir to the family fortune. Onan refused so he could keep the fortune for himself and his descendants. So the sin of Onan was greed, not masturbation. All that stuff in the Talmud about how the hands of masturbators should be cut off, or how masturbation helped cause Noah’s Flood (really! Sanhedrin 108b!) is just a coincidence. God hates greed, just like us.</p><p>Modern academics are great, but trusting them feels somehow too convenient. So there in the waiting room, I tried to put myself in the mindset of the rabbis thousands of years ago who thought wasting semen was a such a dire offense.</p><p>The average ejaculation contains about 300 million sperm. There are about 300 million people in the United States. If every sperm in a single ejaculation got to fertilize an egg and incubate in a womb, it would be enough to populate a second America.</p><p>America has about 200 living Nobel Prize winners. 735 billionaires. 1,000,000 doctors, 5,000,000 nurses. 100,000 pilots, 700,000 cops. Also 700,000 drug dealers, 100,000 murderers, and 1,700 NYT journalists.</p><p>That doesn’t necessarily mean my cup contained exactly 100,000 future pilots. If we assume complete genetic determinism, my sperm form a normal distribution around my personal genetic average. I’m terrible at three dimensional reasoning, so let’s say I’m two standard deviations less likely than usual to become a pilot. If my wife is normal on this trait, and we average it out, that means only about 32,000 future pilots in the cup.</p><p><span>On the other hand, I’m better than average at writing. I might be among the top 20,000 most-read authors in the US, so maybe +4SD above average. Again assuming my wife is normal, that suggests even the average kid we have will be a good writer. But imagine an entire America worth of people </span><em>centered</em><span> around being a good writer. The best writer in existing America should be +6SD above average; the best writer among the sperm in the cup is +8SD. 8SD is “best in two quadrillion”. There has never been a writer that good in the whole history of the world. There is a sperm in that cup who could write at an utterly superhuman level, write things none of us could possibly imagine, things so good it’s not even clear you would still call them writing and not some entirely new semi-divine form of art.</span></p><p>There’s also, on priors, some sperm who would shoot up a school. There’s a decent chance of a few who, if given an egg and a womb, would destroy the world, and a few others who would save it. A few hundred might ruin my life so thoroughly that I would commit suicide to escape them. A few dozen might be so great that people would build statues to me just for being their father, the same way some people build statues to St. Joseph.</p><p>The nurse called my name, I handed her the cup, and she took it away to pour into some lab apparatus. Good bye, 200 Nobelists. Good by, 32,000 pilots. Good bye, son who would have destroyed the world. Good bye, daughter who would have saved it. I waited to see if God would smite me. He did not. A few weeks later the clinic called and said there was nothing wrong with my sperm. My fertility problems were just bad luck. I should just keep trying.</p><p><span>There’s an old Jewish joke. How do you make God laugh? Tell Him your plans. 1/10,000 chance of a pilot, because I’m bad at navigation and the base rates are low. 1/10 chance of a doctor, because of all the doctors in my family. I knew it was bogus. Partly because I’m bad at standard deviations and probably got the numbers wrong. But partly because anything can happen. Maybe I was having all this trouble because the lab missed something and I really </span><em>was</em><span> infertile. Maybe my </span><em>wife</em><span> was infertile. Maybe we’d eaten too many microplastics and it was all over. Maybe we’d have a kid, an amazing kid who could have changed everything, but the world would end in 2027 and they’d never get a chance. Still, you’ve got to calculate. One in three million chance of becoming a billionaire. One in thirty thousand chance of committing murder. One out of this. One in that. One one one one one, until you reach semantic satiation on the number “one” and the syllable loses all meaning.</span></p><p>This time God chose to frustrate my calculations even faster and more decisively than usual: He blessed me with twins.</p><p><strong>II.</strong></p><p>Natural selection didn’t design the female body to carry two children. It barely, grudgingly, designed it to carry one. Two is a cruel joke.</p><p>I remember cutting an onion, sometime during month one. My wife asked if they were a different variant from usual, or if they’d gone bad. They hadn’t. It had to be morning sickness. We laughed and hugged each other. This pregnancy thing was starting to feel real!</p><p>A month later - including a hunt through the kitchen to cleanse it of any shred of onion, or anything that had ever touched an onion - we agreed that actually, morning sickness was bad. Two months later, we debated bringing my wife to the ER because she hadn’t eaten anything other than plain saltine crackers in several days. We did manage to avoid the hospital, but it was rough. I’m surprised more people don’t name their children after Zofran®. Women get such positive feelings about it, right when they’re considering baby names. For a girl, you could nickname her Zoe. For a boy, Frank. </p><p><span>And after the morning sickness it was asthma. After the asthma, anemia. After the anemia, hip pain, trouble sleeping, trouble walking, trouble with </span><em>everything</em><span>.</span></p><p>I’ve heard rumors of some women who keep working all through pregnancy, with a smile on their face. Pronatalist influencer Simone Collins says she was taking business calls from her hospital room during the delivery. I think it’s a conspiracy. All the pronatalist influencers get together and say that pregnancy isn’t so bad. Young women believe them, and so the human race survives another generation. </p><p><span>As my wife labored to build our childrens’ physical forms, I toiled to give them their spiritual-semiotic identity. The theory of </span><a href="https://en.wikipedia.org/wiki/Nominative_determinism" rel="">nominative determinism</a><span> posits that a person’s name shapes the course of their future life. Its proponents have collected a mountain of evidence: British chief justice </span><a href="https://en.wikipedia.org/wiki/Igor_Judge,_Baron_Judge" rel="">Igor Judge</a><span>, neurologist </span><a href="https://en.wikipedia.org/wiki/Russell_Brain,_1st_Baron_Brain" rel="">Lord Brain</a><span>, poker champion </span><a href="https://en.wikipedia.org/wiki/Chris_Moneymaker" rel="">Chris Moneymaker</a><span>, investment CEO </span><a href="https://en.wikipedia.org/wiki/Eugene_Profit" rel="">Eugene Profit</a><span>. The Chinese think the </span><a href="https://www.thoughtco.com/number-of-stroke-chinese-names-2278472" rel="">number of strokes</a><span> in the characters that form </span><a href="https://www.bbc.com/worklife/article/20201209-why-some-chinese-believe-a-name-change-could-improve-luck" rel="">a child’s name</a><span> must add up to a lucky number; the Jews believe each letter corresponds to a number, and a person’s name resonates spiritually with all other words whose letters sum to the same amount. </span></p><p><span>Now the statisticians have joined the fray: </span><a href="https://www.experiencedmommy.com/baby-name-salary/" rel="">did you know</a><span> that children with short first names earn over $10,000 more than longer ones? Or that men named "Jim" make 50% more than men named "Isaiah"? Is this causation or confounding? Names indicate whether you are black or white, rich or poor, and whether your parents are traditional or eccentric; what is left after adjusting for this effect? The only paper I’ve seen even begin to address the question is </span><a href="https://www.nber.org/system/files/working_papers/w11195/w11195.pdf" rel="">a sibling-control study by David Figlio</a><span>, who finds that even within families, children with lower-class names perform worse. And you don’t need scientists to know that names affect how other people see you. Just ask Chad, Karen, Tyrone, or the poor doctor I worked with once named Osama (he went by “Sam”).</span></p><p>But also, some people love their names, and other people hate theirs. This was the factor I was least sure about, so I surveyed 1518 blog readers.</p><figure data-component-name="Image3Dynamic"><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9242b644-1695-46be-bc07-6042c379a03b_636x416.png 424w, https://substackcdn.com/image/fetch/w_652,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9242b644-1695-46be-bc07-6042c379a03b_636x416.png 652w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9242b644-1695-46be-bc07-6042c379a03b_636x416.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9242b644-1695-46be-bc07-6042c379a03b_636x416.png 1272w, https://substackcdn.com/image/fetch/w_1304,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9242b644-1695-46be-bc07-6042c379a03b_636x416.png 1304w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9242b644-1695-46be-bc07-6042c379a03b_636x416.png 1456w, https://substackcdn.com/image/fetch/w_1956,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9242b644-1695-46be-bc07-6042c379a03b_636x416.png 1956w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_652,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9242b644-1695-46be-bc07-6042c379a03b_636x416.png" sizes="100vw" alt="Graph of people's name preferences, showing they are happiest with names of rank 501 - 1000" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9242b644-1695-46be-bc07-6042c379a03b_636x416.png 424w, https://substackcdn.com/image/fetch/w_652,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9242b644-1695-46be-bc07-6042c379a03b_636x416.png 652w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9242b644-1695-46be-bc07-6042c379a03b_636x416.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9242b644-1695-46be-bc07-6042c379a03b_636x416.png 1272w, https://substackcdn.com/image/fetch/w_1304,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9242b644-1695-46be-bc07-6042c379a03b_636x416.png 1304w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9242b644-1695-46be-bc07-6042c379a03b_636x416.png 1456w, https://substackcdn.com/image/fetch/w_1956,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9242b644-1695-46be-bc07-6042c379a03b_636x416.png 1956w" width="652"></picture></div></figure><p><span>Here “popularity rank” comes from the </span><a href="https://www.ssa.gov/oact/babynames/" rel="">List Of Most Popular Baby Names</a><span> for the respondent’s birth year - for example, Scott was the 39th most popular boys’ name in 1984, so I am rank 39. I find that people are happiest with names in the 501 - 1000 range (a separate question, which asked people to rate their happiness with their name on a scale of 1 - 5 without reference to whether it was traditional or unusual, got the same result). </span></p><p>What about other considerations?</p><figure data-component-name="Image3Dynamic"><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4c790d21-2ecf-4187-a0ad-96bdf8723bfb_707x339.png 424w, https://substackcdn.com/image/fetch/w_682,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4c790d21-2ecf-4187-a0ad-96bdf8723bfb_707x339.png 682w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4c790d21-2ecf-4187-a0ad-96bdf8723bfb_707x339.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4c790d21-2ecf-4187-a0ad-96bdf8723bfb_707x339.png 1272w, https://substackcdn.com/image/fetch/w_1364,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4c790d21-2ecf-4187-a0ad-96bdf8723bfb_707x339.png 1364w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4c790d21-2ecf-4187-a0ad-96bdf8723bfb_707x339.png 1456w, https://substackcdn.com/image/fetch/w_2046,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4c790d21-2ecf-4187-a0ad-96bdf8723bfb_707x339.png 2046w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_682,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4c790d21-2ecf-4187-a0ad-96bdf8723bfb_707x339.png" sizes="100vw" alt="Another graph of name preferences, showing they are happiest with older names, historical names, or names honoring deceased relatives." srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4c790d21-2ecf-4187-a0ad-96bdf8723bfb_707x339.png 424w, https://substackcdn.com/image/fetch/w_682,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4c790d21-2ecf-4187-a0ad-96bdf8723bfb_707x339.png 682w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4c790d21-2ecf-4187-a0ad-96bdf8723bfb_707x339.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4c790d21-2ecf-4187-a0ad-96bdf8723bfb_707x339.png 1272w, https://substackcdn.com/image/fetch/w_1364,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4c790d21-2ecf-4187-a0ad-96bdf8723bfb_707x339.png 1364w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4c790d21-2ecf-4187-a0ad-96bdf8723bfb_707x339.png 1456w, https://substackcdn.com/image/fetch/w_2046,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4c790d21-2ecf-4187-a0ad-96bdf8723bfb_707x339.png 2046w" width="682"></picture></div></figure><p>I asked people how happy they would be with ten different types of names. </p><p><span>People expressed a strong preference for common older names like John and Mary. Does this contradict the finding above that people with very common names were least happy? Not </span><em>necessarily </em><span>- the common names on the question above included all common names (the #1 most common name for boys born last year is “Liam”), so maybe people like common </span><em>older</em><span> names in particular? But I looked at people in the sample named John, Michael, Mary, and Sarah, and they didn’t differ much from the overall common names category. So people may </span><em>think</em><span> they would like names like these, but actual Johns and Marys wish they were named something a little more unusual.</span></p><p>The least popular categories included “new-fangled name” and “sci-fi / fantasy name”. The most popular were “name honoring a deceased relative”, “name from your ethnic origin”, and “historical figure”. </p><p>So that’s why I decided to name my children Napoleon Herschel Siskind and Hatshepsut Tzeitel Siskind. </p><p>No, seriously, I’m not comfortable telling the Internet my kids’ names. I’ll let them get doxxed the usual way - by the NYT, the first time they express a problematic opinion.</p><p>But I need some way to refer to them online, so their nicknames are Kai and Lyra.</p><p><strong>III.</strong></p><p><span>On December 13, 2023, two surprisal-minimization engines registered an unpredecented spike in surprisal. They were thrust from a sunless sea into a blooming buzzing confusion, flooded with inexplicable data through input channels they didn’t even know they had. The engines heroically tested hyperprior after hyperprior to compress the data into something predictable. Certain patterns quickly emerged. Probability distributions resolved into solid objects. The highest-resolution input channel snapped into place as a two-dimensional surface being projected onto by a three dimensional space. But - a blur of calculations - the three-dimensional nature of space implies that it must be intractably large! And if there are n solid objects in the world, that implies the number of object-object interactions increases as n(n-1)/2, which would quickly become impossible to track. Their hearts sinking, the engines started to worry it will might take </span><em>hours</em><span> before they were fully able to predict every aspect of this new environment. A panic reflex they didn’t know they had kicked in, and they began to cry.</span></p><p><span>Some outside force picked them up, rocked them back and forth. A million inexplicable sense-data, overwhelmed by a single stimulus - a </span><em>rhythmic</em><span> stimulus. The predictability of importance-weighted sense-data shot way up! Kullback-Leibler divergence dropped to near-zero! The panic reflex subsided, and the engines - exhausted by their sudden spurt of computation - shut off to </span><a href="https://www.sciencedirect.com/science/article/abs/pii/S1084952121000318" rel="">renormalize synaptic weights</a><span>.</span></p><p>Soon the engines will discover that things are even worse than they think. Some of their predictions are hard-coded; they will never be able to change them to match the world. Their only hope is to change the world to match their predictions: they are obligate agents. As they grow older, their goal systems will throw up increasingly complicated hard-coded forecasts; food, water, belonging, social status, sex. Their only path towards predictive accuracy will be to obtain all of these things from a hostile world. It’s a lousy deal.</p><p><span>My poor, fragile, little cognitive engines! These, then, will be the twin imperatives of your life: </span><a href="https://slatestarcodex.com/2017/09/05/book-review-surfing-uncertainty/" rel="">surprisal minimization</a><span> and </span><a href="https://slatestarcodex.com/2019/03/20/translating-predictive-coding-into-perceptual-control/" rel="">active inference</a><span>. If your brains are still too small to process such esoteric terms, there are others available. Your father’s ancestors called them </span><em>Torah</em><span> and </span><em>tikkun olam</em><span>; your mother’s ancestors called them Truth and Beauty; your current social sphere calls them Rationality and Effective Altruism. You will learn other names, too: no perspective can exhaust their infinite complexity. Whatever you call them, your lives will be spent in their service, pursuing them even unto that far-off and maybe-mythical point where they blur into One.</span></p><p>If you pursue them only far enough to reduce your own predictive error, it will still be a life well-lived, and nobody will blame you for it. But if you choose, you can take an extra burden upon yourself, improving not just your own models but the broader predictions of the world. You can push forward the frontiers of knowledge, or improve the lot of all humankind. It’s a crazy thing to try, when even your own local predictions are so far from perfect accuracy. I cannot exactly tell you why you should want to do something like this. If you feel it, you feel it; if not, so it goes.</p><p><span>But a parable: when you were born, your mother kissed you.  Along with the kiss came a microdose of </span><a href="https://www.astralcodexten.com/p/defying-cavity-lantern-bioworks-faq" rel="">the BCS3-L1 genetically engineered bacterium</a><span>. Without any teeth to cling to, it fell into the pit of your stomach and died. But she’ll kiss you again and again, transferring a few more BCS3-LI each time. In a few months, one of the colonists will find an incipient tooth and hang on for dear life. It will fight off competitors, wage epic battles that will determine the fate of the mouth for decades to come. It will win, because its genetic enhancements are pretty good. Then, if some smart people got their calculations right, it will do exactly nothing. No tooth decay. No cavities. The teeth will stay safe and clean.</span></p><p>When you get older, I’ll tell you the story behind this. Your mother worked for a company synthesizing genetically engineered tooth bacteria that prevent cavities. She isn’t the kind of person who would push a product on others that she hadn’t tried herself. So she infected herself with the bacterium, fresh out of the lab. Other people in the company did the same. But only she was pregnant. Babies get their mouth bacteria from their mothers. So you might be the first children in the world to grow up without s-mutans-mediated tooth decay.</p><p><span>Tooth decay isn’t the worst thing in the world. As victories go, this is a relatively minor one. I tell it to you only because it is ours. Our drop of water in a vast ocean of victories that have improved the lot of humankind on every continent, for as long as the species lasts. There is nothing that hammers this in like being a new father - nothing like seeing two tiny rudimentary week-old cognitive engines struggle not to fade into the entropic background. Kai, you wouldn’t come out of your mother on your own - the obstetrician used </span><a href="https://en.wikipedia.org/wiki/Vacuum_extraction" rel="">vacuum extraction</a><span> to save your life. Neither of you was a great breastfeeder at first, and if we hadn’t had nurses and bottles and formula, you might not have made it. A few days after your birth, it rained two inches in fifty degree weather; if we didn’t have central heating and space heaters and warm blankets, who knows what would have happened?  In 1800, about 50% of babies died before their fifth birthday. This statistic used to feel like a brute fact. Now I’m noticing all the little cracks that Death could creep in through, if we didn’t have our cornucopia of technologies and our team of vigilant pediatricians.</span></p><figure data-component-name="Image3Dynamic"><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F52a819de-b799-4ab2-88d3-bd873a59158b_825x764.png 424w, https://substackcdn.com/image/fetch/w_580,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F52a819de-b799-4ab2-88d3-bd873a59158b_825x764.png 580w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F52a819de-b799-4ab2-88d3-bd873a59158b_825x764.png 848w, https://substackcdn.com/image/fetch/w_1160,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F52a819de-b799-4ab2-88d3-bd873a59158b_825x764.png 1160w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F52a819de-b799-4ab2-88d3-bd873a59158b_825x764.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F52a819de-b799-4ab2-88d3-bd873a59158b_825x764.png 1456w, https://substackcdn.com/image/fetch/w_1740,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F52a819de-b799-4ab2-88d3-bd873a59158b_825x764.png 1740w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_580,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F52a819de-b799-4ab2-88d3-bd873a59158b_825x764.png" sizes="100vw" alt="A graph showing global child mortality plummeting from 1800 to today" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F52a819de-b799-4ab2-88d3-bd873a59158b_825x764.png 424w, https://substackcdn.com/image/fetch/w_580,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F52a819de-b799-4ab2-88d3-bd873a59158b_825x764.png 580w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F52a819de-b799-4ab2-88d3-bd873a59158b_825x764.png 848w, https://substackcdn.com/image/fetch/w_1160,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F52a819de-b799-4ab2-88d3-bd873a59158b_825x764.png 1160w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F52a819de-b799-4ab2-88d3-bd873a59158b_825x764.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F52a819de-b799-4ab2-88d3-bd873a59158b_825x764.png 1456w, https://substackcdn.com/image/fetch/w_1740,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F52a819de-b799-4ab2-88d3-bd873a59158b_825x764.png 1740w" width="580"></picture></div></figure><p>There are two of you. Back in 1800, statistically, one of you would have made it. I look at you now - such beautiful, fragile cognitive engines - and I cannot bear the thought of losing either one. The statistics for the 21st century suggest I won’t have to.</p><p>I was thinking about this recently, because - well, I feel kind of bad. I instantiated two surprisal-minimization engines - two conscious algorithms designed to feel negative qualia in the presence of hard-to-predict stimuli - on a world ruled by 195 mutually-hostile and frequently-shifting coalitions of over-evolved murder-monkeys, many of whom have nuclear weapons. I cannot quite remember why I thought this would be a good idea. I blame the pronatalist influencer conspiracy. </p><p>But if I have any excuse at all, it’s excessive enthusiasm for this grand project of world-scale surprise minimization and active inference. You are here to benefit from it, to enjoy sensual and intellectual pleasures that our ancestors could never know. And also, if you choose, to continue it, push it forward into a new era. You have already contributed in a tiny way - as guinea pigs - to the conquest of tooth decay. But there are so many other worse sources of prediction error out there. What else might you conquer, my two little surprisal-minimization engines?</p><p><strong>IV.</strong></p><p><span>There is a secret known only to parents of twins, medical residents, and </span><a href="https://guzey.com/theses-on-sleep/" rel="">Alexey Guzey</a><span>: the human body does not actually need sleep. After 31 hours awake, you get </span><a href="https://www.astralcodexten.com/p/sleep-is-the-mate-of-death" rel="">an integer overflow</a><span> in God’s database and go back to being well-rested again. Also you gain the ability to see angels.</span></p><p>This has become the new rhythm of our lives. Changing, nursing, burping, first one child, then the other. Twenty minutes per child, times two children, times once every 2-3 hours; you can do the math. We do everything else - laundry, shopping, cooking, occasionally even napping - in the precious intervals when both babies are asleep.</p><p><span>The </span><a href="https://www.nytimes.com/wirecutter/reviews/snoo-smart-sleeper-what-to-know/" rel="">Snoo </a><span>is a $1500 computerized bassinet that continually assesses babies’ needs and tries to calm them with various soothing noises and automated rocking motions. We got two, both of which have been soundly rejected. The twins insist on sleeping in their carseats, which we’ve grudgingly moved to the nursery. At first I was miffed, but now I see their logic. You’ve got to learn to resist the algorithmic content mills early.</span></p><p>Kai has some baby version of Alien Hand Syndrome. His arms are controlled by a malevolent entity with a grudge against the rest of his body. If we leave them loose, they wave wildly in all directions, and he freaks out. This is apparently a common problem, best solved by heavy swaddling clothes. The malevolent entity struggles against the swaddle and occasionally breaks free, like some 1980s horror movie monster. Every nursing, we must struggle against it and bind it anew before returning him to his carseat.</p><p>Lyra is already an overachiever. She has clearly read all the How To Be A Baby textbooks, learned when crying is appropriate, and only cries at those specific times. She drinks the exact amount of milk recommended on the Baby Age-Appropriate Nursing Chart, then refuses to accept more. I’m worried that if we don’t teach her to think independently soon, she’ll end up somewhere terrible like Harvard.</p><p>I look over at them. They seem so peaceful in their stupid carseats. Let them sleep. Let them nurse as often as they want. They’ll need all their strength for what’s ahead.</p><p><span>Kai. Lyra. You’ll </span><a href="https://www.youtube.com/watch?v=VGDhrH_uLUw" rel="">live to see a million things that man was never meant to see</a><span>. You were born just in time for a high-speed collision with the hinge of history. I’m only 39, I expect to be around when whatever-it-is happens - but if not, you’re our family’s ambassador to the singularity. A thousand generations, from hardy Neolithic farmers to studious Russian rabbis to overprivileged American office workers - they all lived and died so you could be here and experience this, and maybe tilt the course of what’s coming by a couple of micro-degrees.</span></p><p><span>Parents are supposed to teach their children the skills they need to navigate the world. This already feels somewhat obsolete - where are the Google programmers who were taught Python by their fathers, or the Instagram influencers who learned content creation on their mother’s knee? Soon it will be completely hopeless. Where we’re going there are no roads. You’ll have to figure it out by yourself. If I am to pass on anything of value to you, it can only be </span><a href="https://www.lesswrong.com/posts/SXK87NgEPszhWkvQm/mundane-magic" rel="">the ultimate power</a><span>, the technique that forms all other techniques. </span></p><p>I’ve always wondered why I wrote so much. Now I realize I was leaving you bread crumbs.</p><figure data-component-name="Image3Dynamic"><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fae22914c-e8a8-4cbe-b776-c677202b9f6b_683x514.png 424w, https://substackcdn.com/image/fetch/w_550,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fae22914c-e8a8-4cbe-b776-c677202b9f6b_683x514.png 550w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fae22914c-e8a8-4cbe-b776-c677202b9f6b_683x514.png 848w, https://substackcdn.com/image/fetch/w_1100,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fae22914c-e8a8-4cbe-b776-c677202b9f6b_683x514.png 1100w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fae22914c-e8a8-4cbe-b776-c677202b9f6b_683x514.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fae22914c-e8a8-4cbe-b776-c677202b9f6b_683x514.png 1456w, https://substackcdn.com/image/fetch/w_1650,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fae22914c-e8a8-4cbe-b776-c677202b9f6b_683x514.png 1650w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_550,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fae22914c-e8a8-4cbe-b776-c677202b9f6b_683x514.png" sizes="100vw" alt="Me, my wife, and the twins." srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fae22914c-e8a8-4cbe-b776-c677202b9f6b_683x514.png 424w, https://substackcdn.com/image/fetch/w_550,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fae22914c-e8a8-4cbe-b776-c677202b9f6b_683x514.png 550w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fae22914c-e8a8-4cbe-b776-c677202b9f6b_683x514.png 848w, https://substackcdn.com/image/fetch/w_1100,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fae22914c-e8a8-4cbe-b776-c677202b9f6b_683x514.png 1100w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fae22914c-e8a8-4cbe-b776-c677202b9f6b_683x514.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fae22914c-e8a8-4cbe-b776-c677202b9f6b_683x514.png 1456w, https://substackcdn.com/image/fetch/w_1650,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fae22914c-e8a8-4cbe-b776-c677202b9f6b_683x514.png 1650w" width="550"></picture></div></figure><p>Happy holidays, from our family to yours. ACX will return to its normal posting schedule in January.</p><figure data-component-name="Image3Dynamic"><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb382aef1-8cfc-4394-826f-a180c9c6f946_439x115.png 424w, https://substackcdn.com/image/fetch/w_728,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb382aef1-8cfc-4394-826f-a180c9c6f946_439x115.png 728w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb382aef1-8cfc-4394-826f-a180c9c6f946_439x115.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb382aef1-8cfc-4394-826f-a180c9c6f946_439x115.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb382aef1-8cfc-4394-826f-a180c9c6f946_439x115.png 1456w, https://substackcdn.com/image/fetch/w_2184,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb382aef1-8cfc-4394-826f-a180c9c6f946_439x115.png 2184w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_728,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb382aef1-8cfc-4394-826f-a180c9c6f946_439x115.png" sizes="100vw" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb382aef1-8cfc-4394-826f-a180c9c6f946_439x115.png 424w, https://substackcdn.com/image/fetch/w_728,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb382aef1-8cfc-4394-826f-a180c9c6f946_439x115.png 728w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb382aef1-8cfc-4394-826f-a180c9c6f946_439x115.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb382aef1-8cfc-4394-826f-a180c9c6f946_439x115.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb382aef1-8cfc-4394-826f-a180c9c6f946_439x115.png 1456w, https://substackcdn.com/image/fetch/w_2184,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb382aef1-8cfc-4394-826f-a180c9c6f946_439x115.png 2184w" width="728"></picture></div></figure><p>…of 2042.</p></div></article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[How to make LLMs go fast (192 pts)]]></title>
            <link>https://vgel.me/posts/faster-inference/</link>
            <guid>38733384</guid>
            <pubDate>Fri, 22 Dec 2023 12:09:27 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://vgel.me/posts/faster-inference/">https://vgel.me/posts/faster-inference/</a>, See on <a href="https://news.ycombinator.com/item?id=38733384">Hacker News</a></p>
<div id="readability-page-1" class="page"><p>In <a href="https://vgel.me/posts/handmade-transformer">my last post</a>, we made a transformer by hand.
There, we used the classic autoregressive sampler, along the lines of:</p><p>This approach to inference is elegant and cuts to the heart of how LLMs work—they're <em>autoregressive</em>, consuming their own output.
And for our toy model with merely thousands of parameters, it worked completely fine.
Unfortunately, for real models it's far too slow<sup><a href="#generate-too-slow">1</a></sup>.
Why is that, and how can we make it faster?</p><p>This post is a long and wide-ranging survey of a bunch of different ways to make LLMs go brrrr, from better hardware utilization to clever decoding tricks.
It's not completely exhaustive, and isn't the most in-depth treatment of every topic—I'm not an expert on all these things!
But hopefully you'll find the information here a useful jumping off point to learn more about the topics you're interested in.
(I tried to include links to relevant papers and blog posts where applicable.)</p><p>There are two main reasons that inference with the plain autoregressive <code>generate</code> function is slow: an algorithmic one, and a hardware one.</p><p>Algorithmically, <code>generate</code> has to process an increasing number of tokens every cycle, because each cycle we append a new token to the context.
That means to generate 100 tokens from a 10 token prompt, you don't need to run <code>model</code> on only 109 tokens. You need to run it on 10 + 11 + 12 + 13 + ... + 109 = 5,950 tokens!
<small>(The initial prompt can be processed in parallel, which is part of why prompt tokens are usually cheaper in inference APIs.)</small>
It also means that the model <em>slows down</em> as it generates, since each successive token generation has a longer and longer prefix:</p><p>Attention, at least vanilla attention, is also a quadratic algorithm: all tokens attend to all tokens, leading to N² scaling, making everything worse.</p><p>So that's the algorithmic reason.
What's the hardware reason?
Well, it's simple: LLMs are just huge.
Even a relatively small model like gpt2 (117M parameters) is hundreds of megabytes, and all that data has to live in RAM.
RAM is really slow, and modern processors (both CPUs and GPUs) make up for that by having lots of cache close to the processor that's faster to access<sup><a href="#gpu-cache">2</a></sup>.
The details of this differ based on type and model of processor, but the gist is that LLM weights do not fit in cache, so a lot of time is spent waiting to load weights from RAM.
This has some unintuitive effects!
For example, looking at the graph above, operating on 10 tokens isn't necessarily much slower than operating on a single token, even though the activation tensors are 10x larger, because the main time sink is moving the model weights around, not doing calculations!</p><p>As a sidebar, what do we mean exactly when we say <em>slow</em>?
There's a whole zoo of metrics people talk about when it comes to LLM inference:</p><p>Different optimizations affect these metrics differently.
For example, batching improves throughput and better utilizes the hardware, but can increase <abbr title="Time to First Token">TtFT</abbr> and generation latency.</p><p>A straightforward way to speed up inference (especially if you're VC funded :-)) is to just buy better hardware, or if you can't afford that, to take better advantage of the hardware you have.</p><p>If you're buying better hardware, most likely that would be some sort of accelerator—usually a GPU, or sometimes/if you're Google, a <abbr title="Tensor Processing Unit">TPU</abbr>.</p><p>Using an accelerator can produce dramatic speedups (hence the name), but keep in mind that there's a transfer bottleneck between the CPU and the accelerator.
If your model doesn't fit in the accelerator's memory, it will need to be swapped out throughout the forward pass, which can slow things down dramatically.
<small>(This is one of the reasons Apple's M1/M2/M3 chips punch above their weight for inference—they have unified CPU and GPU memory.)</small></p><p>Another thing to keep in mind with both CPU and accelerator inference is whether you're taking full advantage of the hardware—a properly optimized program can squeeze more out of weaker hardware than a poorly optimized one can get out of the best hardware.</p><p>For example, you could write attention in PyTorch as <code>F.softmax(q @ k.T / sqrt(k.size(-1)) + mask) @ v</code>, which will give you correct results.
But if you instead use <code>torch.nn.functional.scaled_dot_product_attention</code>, it will delegate the calculation to <a href="https://arxiv.org/abs/2205.14135">FlashAttention</a> when available, which can produce 3x speedups using a handwritten kernel that better takes advantage of cache.</p><p>A more general version of this is compilers like <code>torch.compile</code>, TinyGrad, and ONNX, which can fuse naive Python code into kernels optimized for your hardware.
For example, I could write the following function:</p><p>Each of these things is slow, and some of the steps require jumping the boundary between Python and native code, which doesn't help.
So what if I compile this function using <code>torch.compile</code>?</p><p>If I go into that debug trace directory and open the <code>output_code.py</code> file there, <code>torch</code> has generated an optimized C++ kernel for my CPU that fuses <code>foo</code> into a single kernel.
<small>(If I had run this with a GPU available, <code>torch</code> would have generated a <a href="https://developer.nvidia.com/blog/easy-introduction-cuda-c-and-c/">CUDA kernel</a> for the GPU instead.)</small></p><p>Note that <code>torch.compile</code> specialized the code above for the specific size of tensor we passed in (<code>(10,)</code>).
If we passed in tensors of many different sizes, <code>torch.compile</code> would instead generate code generic over the size, but having a constant size can enable the compiler to generate better code in some cases (e.g. via loop unrolling or better vectorization).</p><p>This function has <em>data-dependent control flow</em>, meaning we do something different based on the runtime value of a variable.
If we compile this in the same way we compiled <code>foo</code>, we get <em>two</em> graphs (and thus two debug directories):</p><p>The first kernel implements the <code>torch.sin(x) + torch.cos(x)</code> and <code>r.sum() &lt; 0</code> parts of the function:</p><p>And the second kernel implements the <code>return r - torch.tan(x)</code> branch, since this is the branch that was taken with the example input:</p><p>This is called a <em>graph break</em>, and it's not good!
The compiled function is slower due to it, since we have to leave the optimized kernel and return to Python to evaluate the branch.
On top of that, the other branch (<code>return r + torch.tan(x)</code>) hasn't been compiled yet, since it hasn't been taken!
That means it will be compiled on the fly when needed, which could be bad if it happens at an inopportune time (such as in the middle of serving a user request).</p><p>Tools like <code>torch.compile</code> are a great way to optimize your code to get better performance out of your hardware, without dipping down to CUDA to write kernels the old-fashioned way.
<small></small></p><div>
<p>(And if you're curious about the compilers work, <a href="https://bernsteinbear.com/blog/compiling-ml-models/">this post</a> by <a href="https://twitter.com/tekknolagi">@tekknolagi</a> explores compiling models written in <a href="https://github.com/karpathy/micrograd/">micrograd</a> to C for a 1000-7500x speed increase!)</p>
<h2 id="Batching"><a href="#Batching">
  <img src="https://vgel.me/permalink.svg" alt="permalink for Batching">
</a>Batching</h2>
<p>In the unoptimized version of <code>generate</code>, we pass the model a single sequence at once, and at each step ask it to append a token:</p>
<div>
<div>
    <p>llm(</p>
    <table><tbody><tr> <td>Mark</td><td>is</td><td>quick</td><td>.</td><td>He</td><td>moves</td></tr></tbody></table>
    <p>)</p>
    <p>=</p>
    
</div><p>
⬇️
</p><div>
    <p>llm(</p>
    <table><tbody><tr> <td>Mark</td><td>is</td><td>quick</td><td>.</td><td>He</td><td>moves</td><td>quickly</td></tr></tbody></table>
    <p>)</p>
    <p>=</p>
    
</div><p>
⬇️
</p><div>
    <p>llm(</p>
    <table><tbody><tr><td>…</td><td>quick</td><td>.</td><td>He</td><td>moves</td><td>quickly</td><td>.</td></tr></tbody></table>
    <p>)</p>
    <p>=</p>
    
</div>
</div>
<p>To batch generation, we instead pass the model multiple sequences at once, generating a completion for each in the same forward pass.<sup><a href="#jax-vmap">3</a></sup>
This requires the sequences to be padded on either the left or right with filler tokens to equal length.
The padding tokens (which can be anything, I'm using [end] here) are masked in the attention mask so that they don't influence generation.</p>
<div>
<div>
    <p>llm(</p>
    <table>
        <tbody><tr><td>Mark</td><td>is</td><td>quick</td><td>.</td><td>He</td><td>moves</td></tr>
        <tr><td>[end]</td><td>[end]</td><td>The</td><td>Eiffel</td><td>Tower</td><td>is</td></tr>
        <tr><td>[end]</td><td>I</td><td>like</td><td>bananas</td><td>because</td><td>they</td></tr>
    </tbody></table>
    <p>)</p>
    <p>=</p>
    
</div><p>
⬇️
</p><div>
    <p>llm(</p>
    <table>
        <tbody><tr><td>Mark</td><td>is</td><td>quick</td><td>.</td><td>He</td><td>moves</td><td>quickly</td></tr>
        <tr><td>[end]</td><td>[end]</td><td>The</td><td>Eiffel</td><td>Tower</td><td>is</td><td>in</td></tr>
        <tr><td>[end]</td><td>I</td><td>like</td><td>bananas</td><td>because</td><td>they</td><td>have</td></tr>
    </tbody></table>
    <p>)</p>
    <p>=</p>
    
</div><p>
⬇️
</p><div>
    <p>llm(</p>
    <table>
        <tbody><tr><td>Mark</td><td>is</td><td>quick</td><td>.</td><td>He</td><td>moves</td><td>quickly</td><td>.</td></tr>
        <tr><td>[end]</td><td>[end]</td><td>The</td><td>Eiffel</td><td>Tower</td><td>is</td><td>in</td><td>Paris</td></tr>
        <tr><td>[end]</td><td>I</td><td>like</td><td>bananas</td><td>because</td><td>they</td><td>have</td><td>no</td></tr>
    </tbody></table>
    <p>)</p>
    <p>=</p>
    
</div><p>
⬇️
</p><div>
    <p>llm(</p>
    <table>
        <tbody><tr><td>…</td><td>quick</td><td>.</td><td>He</td><td>moves</td><td>quickly</td><td>.</td><td>[end]</td></tr>
        <tr><td>…</td><td>The</td><td>Eiffel</td><td>Tower</td><td>is</td><td>in</td><td>Paris</td><td>,</td></tr>
        <tr><td>…</td><td>like</td><td>bananas</td><td>because</td><td>they</td><td>have</td><td>no</td><td>bones</td></tr>
    </tbody></table>
    <p>)</p>
    <p>=</p>
    
</div>
</div>
<p>Because batching sequences in this way allows the model weights to be used for multiple sequences at once, running the entire batch of sequences together takes less time than running each sequence separately.
For example, on my machine, using GPT-2 to generate a next token for:</p>
<ul>
<li>20 tokens x  1 sequence  = ~70ms</li>
<li>20 tokens x  5 sequences = ~220ms (linear scaling would be ~350ms)</li>
<li>20 tokens x 10 sequences = ~400ms (linear scaling would be ~700ms)</li>
</ul>
<h3 id="Continuous_Batching"><a href="#Continuous_Batching">
  <img src="https://vgel.me/permalink.svg" alt="permalink for Continuous_Batching">
</a>Continuous Batching</h3>
<p>Notice how in the example above, "Mark is quick. He moves quickly." finished before the other sequences, but because the batch as a whole wasn't done, we were forced to continue generating tokens for it ("Random").
This isn't a problem for correctness—we can simply clip the generated sequence to the <code>[end]</code> token—but it is unfortunate, since GPU resources are being used to generate tokens we will just throw away.</p>
<p>Continuous batching fixes this by inserting new sequences into the batch as other sequences complete, after their <code>[end]</code> tokens.
Instead of generating random tokens after the <code>[end]</code> token, a new sequence can be inserted in that row of the batch, with attention masking to prevent the sequence from being influenced by the tokens from the previous sequence in the row.
<small>(Essentially, the prior sequence acts like additional padding.)</small></p>
<div>
<div>
    <p>llm(</p>
    <table>
        <tbody><tr><td>1</td><td>2</td><td>3</td><td>4</td><td></td><td></td></tr>
        <tr><td>[end]</td><td>[end]</td><td>a</td><td>b</td><td></td><td></td></tr>
        <tr><td>[end]</td><td>A</td><td>B</td><td>C</td><td></td><td></td></tr>
    </tbody></table>
    <p>)</p>
    <p>=</p>
    
</div><p>
⬇️
</p><div>
    <p>llm(</p>
    <table>
        <tbody><tr><td>1</td><td>2</td><td>3</td><td>4</td><td>X</td><td></td></tr>
        <tr><td>[end]</td><td>[end]</td><td>a</td><td>b</td><td>c</td><td></td></tr>
        <tr><td>[end]</td><td>A</td><td>B</td><td>C</td><td>D</td><td></td></tr>
    </tbody></table>
    <p>)</p>
    <p>=</p>
    
</div><p>
⬇️
</p><div>
    <p>llm(</p>
    <table>
        <tbody><tr><td>1</td><td>2</td><td>3</td><td>4</td><td>X</td><td>Y</td></tr>
        <tr><td>[end]</td><td>[end]</td><td>a</td><td>b</td><td>c</td><td>α</td></tr>
        <tr><td>[end]</td><td>A</td><td>B</td><td>C</td><td>D</td><td>E</td></tr>
    </tbody></table>
    <p>)</p>
    <p>=</p>
    
</div>
</div>
<h2 id="Shrinking_model_weights"><a href="#Shrinking_model_weights">
  <img src="https://vgel.me/permalink.svg" alt="permalink for Shrinking_model_weights">
</a>Shrinking model weights</h2>
<p>Floating point numbers come in different sizes, and that matters for performance.
Most of the time for regular software (e.g., Javascript numbers and Python floats), we use 64 bit (double precision) IEEE 754 floating point.
Most ML, however, has traditionally used 32 bit (single precision) IEEE 754:</p>
<pre data-lang="python"><code data-lang="python"><span>&gt;&gt;&gt; </span><span>gpt2.transformer.h[</span><span>0</span><span>].attn.c_attn.weight.dtype
</span><span>torch.float32
</span></code></pre>
<p>Models train and infer fine with fp32, and this saves 4 bytes (50%) per parameter, which is huge—a 7B parameter model would take up 56Gb in fp64, and only 28 Gb in fp32.
Remember that large amounts of time during training and inference are spent moving data from RAM to cache and registers—the less data there is to move, the better.
So while fp32 is better than fp64, can we do <em>even better</em>?</p>
<h3 id="16_bit_floats"><a href="#16_bit_floats">
  <img src="https://vgel.me/permalink.svg" alt="permalink for 16_bit_floats">
</a>16 bit floats</h3>
<p>fp16, or half precision, is the obvious next step—another 50% savings!
You have two main options here: fp16, and bfloat16 (short for brain float, since it was developed by Google Brain), which has better range but worse hardware support.</p>
<p>It's easiest to see the distinction with a diagram showing the size of each field:</p>
<div>
<table>
<tbody><tr><th colspan="32">fp32</th></tr>
<tr>
    <th>sign</th>
    <th colspan="8">exponent (8)</th>
    <th colspan="23">fraction (23)</th>
</tr>
<tr>
    <td>0</td>
    <td>1</td> <td>0</td> <td>0</td> <td>0</td> <td>0</td> <td>1</td> <td>1</td> <td>1</td>
    <td>1</td> <td>0</td> <td>1</td> <td>0</td> <td>0</td> <td>1</td> <td>0</td> <td>0</td> <td>0</td> <td>0</td> <td>1</td> <td>0</td> <td>0</td> <td>0</td> <td>1</td> <td>0</td> <td>0</td> <td>0</td> <td>1</td> <td>1</td> <td>1</td> <td>0</td> <td>1</td>
</tr>
<tr><td></td></tr>
<tr><th colspan="16">fp16 (IEEE half)</th></tr>
<tr>
    <th>sign</th>
    <th colspan="5">exponent (5)</th>
    <th colspan="10">fraction (10)</th>
</tr>
<tr>
    <td>0</td>
    <td>1</td> <td>0</td> <td>0</td> <td>0</td> <td>0</td>
    <td>1</td> <td>0</td> <td>1</td> <td>0</td> <td>0</td> <td>1</td> <td>0</td> <td>0</td> <td>0</td> <td>0</td>
</tr>
<tr><td></td></tr>
<tr><th colspan="16">bfp16 (brainfloat)</th></tr>
<tr>
    <th>sign</th>
    <th colspan="8">exponent (8)</th>
    <th colspan="7">fraction (7)</th>
</tr>
<tr>
    <td>0</td>
    <td>1</td> <td>0</td> <td>0</td> <td>0</td> <td>0</td> <td>1</td> <td>1</td> <td>1</td>
    <td>1</td> <td>0</td> <td>1</td> <td>0</td> <td>0</td> <td>1</td> <td>0</td>
</tr>
</tbody></table>
</div>
<p>When reducing the fields of a fp32, fp16 and bfloat16 made different tradeoffs: fp16 tried to balance between range and precision by shrinking both the exponent and fraction fields, whereas bfloat16 preserved the range of fp32 by keeping an 8-bit exponent, while sacrificing precision by shrinking the fraction field smaller than fp16.
<a href="https://x.com/sytelus/status/1713462678226968973">The loss of range can sometimes be a problem for training in fp16</a>, but for inference either works, and fp16 is probably a better choice if your GPU doesn't support bfloat16.</p>
<h3 id="Even_smaller!"><a href="#Even_smaller!">
  <img src="https://vgel.me/permalink.svg" alt="permalink for Even_smaller!">
</a>Even smaller!</h3>
<p>Can we go even smaller? Of course!</p>
<p>One approach is to quantize a model trained in a larger format, like fp16.
The llama.cpp project (and the associated ML library ggml) defines <a href="https://github.com/ggerganov/llama.cpp#quantization">a whole zoo of quantization formats</a> (the README is currently out of date, so make sure to check the <a href="https://github.com/ggerganov/llama.cpp/pull/1684">k-quants PR</a> as well), which can go down to less than 5 bits per weight from an fp32 or fp16 model.</p>
<p>These quantizations work a bit differently than fp16 / bfloat16—there isn't enough room to fit a whole number in that space, so instead the weights are quantized in <em>blocks</em>, where an fp16 acts as the block scale, and then the block of quantized weights are each multiplied against that scale. (In some formats, there's also a min value, and sometimes the scale and min are themselves quantized to still be smaller than fp16—it's complicated! See the k-quants PR for more details about how it's implemented in GGML, and <a href="https://timdettmers.com/2022/08/17/llm-int8-and-emergent-features/">this post</a> for more details about why quantization is challenging.)</p>
<p><a href="https://github.com/TimDettmers/bitsandbytes">bitsandbytes</a> also implements quantization for non-llama.cpp projects.
<small>(I don't have much experience with it personally, though, besides dealing with it as a transitive dependency when it doesn't want to install on Lambda Labs instances :-))</small></p>
<p>However, the smaller you go with quantization of a model trained with wider parameters, the more it can start to affect the model's performance, reducing the quality of responses.
It's best to go with the least amount of quantization that will give you acceptable inference speed.</p>
<p>However, it's also possible to finetune or train models with datatypes smaller than fp16.
For example, you can train quantized low rank adapters with <a href="https://github.com/artidoro/qlora">qLoRA</a>, and <a href="https://arxiv.org/abs/2209.05433">a 2022 paper</a> demonstrated training 175B parameter language models in (simulated) fp8, achieving very similar results to fp16.</p>
<p>Note that, as of 2023, GPUs don't natively support datatypes smaller than fp16, except int8 (8 bit integer).
You can train and infer with int8 to some extent, but most quantization requires converting the weights from the quantized format to another type (like fp16) for calculation, and then back when they're no longer needed, which incurs some performance cost.
This can pay for itself based on how much memory your GPU has and how fast that memory is, but it's worth being aware of—quantization isn't free.</p>
<h2 id="KV_caching"><a href="#KV_caching">
  <img src="https://vgel.me/permalink.svg" alt="permalink for KV_caching">
</a>KV caching</h2>
<p>To explain this one, I'm going to borrow some diagrams from <a href="https://vgel.me/posts/handmade-transformer">my last post</a> about how Transformers work.
If this section feels too quick, please read that post for a (much) more in depth explanation!
This explanation is also based on GPT-2, since it's the model I covered in that post.
Other architectures work slightly differently—I'll explain the relevant differences, but most don't make too much of a difference for understanding KV caching.</p>
<p>Inside a Transformer, the activations run through a feed-forward layer to generate a <code>qkv</code> matrix, where each row corresponds to a token:</p>
<div>
<table>
<tbody><tr>
<th>token</th>
<th>q</th><th>q</th><th>q</th><th>q</th><th>q</th><th>q</th><th>q</th><th>q</th>
<th>k</th><th>k</th><th>k</th><th>k</th><th>k</th><th>k</th><th>k</th><th>k</th>
<th>v</th><th>v</th><th>v</th><th>v</th><th>v</th><th>v</th><th>v</th><th>v</th>
</tr>
<tr>
  <td>token 1</td>
  <td>1024</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td>
  <td>1</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td>
  <td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>1</td>
</tr>
<tr>
  <td>token 2</td>
  <td>1024</td><td>1024</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td>
  <td>0</td><td>1</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td>
  <td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>1</td>
</tr>
<tr>
  <td>token 3</td>
  <td>0</td><td>1024</td><td>1024</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td>
  <td>0</td><td>0</td><td>1</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td>
  <td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>-1</td>
</tr>
<tr>
  <td>token 4</td>
  <td>0</td><td>0</td><td>1024</td><td>1024</td><td>0</td><td>0</td><td>0</td><td>0</td>
  <td>0</td><td>0</td><td>0</td><td>1</td><td>0</td><td>0</td><td>0</td><td>0</td>
  <td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>1</td>
</tr>
<tr>
  <td>token 5</td>
  <td>0</td><td>0</td><td>0</td><td>1024</td><td>1024</td><td>0</td><td>0</td><td>0</td>
  <td>0</td><td>0</td><td>0</td><td>0</td><td>1</td><td>0</td><td>0</td><td>0</td>
  <td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>1</td>
</tr>
</tbody></table>
</div>
<p>Then, the <code>qkv</code> matrix is split into <code>q</code>, <code>k</code>, and <code>v</code>, which are combined with attention like this:</p>
<div>
<p>softmax(</p>
<table>
<tbody><tr><th colspan="8">q</th></tr>
<tr><td>1024</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td> </tr>
<tr> <td>1024</td><td>1024</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td> </tr>
<tr> <td>0</td><td>1024</td><td>1024</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td> </tr>
<tr> <td>0</td><td>0</td><td>1024</td><td>1024</td><td>0</td><td>0</td><td>0</td><td>0</td> </tr>
<tr> <td>0</td><td>0</td><td>0</td><td>1024</td><td>1024</td><td>0</td><td>0</td><td>0</td> </tr>
</tbody></table>
<p>@</p>
<table>
<tbody><tr><th colspan="5">k.T</th></tr>
<tr><td>1</td><td>0</td><td>0</td><td>0</td><td>0</td></tr>
<tr><td>0</td><td>1</td><td>0</td><td>0</td><td>0</td></tr>
<tr><td>0</td><td>0</td><td>1</td><td>0</td><td>0</td></tr>
<tr><td>0</td><td>0</td><td>0</td><td>1</td><td>0</td></tr>
<tr><td>0</td><td>0</td><td>0</td><td>0</td><td>1</td></tr>
<tr><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td></tr>
<tr><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td></tr>
<tr><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td></tr>
</tbody></table>
<p>+ mask)</p>
<p>@</p>
<table>
<tbody><tr><th colspan="8">v</th></tr>
<tr>
  <td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>1</td>
</tr>
<tr>
  <td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>1</td>
</tr>
<tr>
  <td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>-1</td>
</tr>
<tr>
  <td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>1</td>
</tr>
<tr>
  <td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>1</td>
</tr>
</tbody></table>
</div>
<p>To produce a matrix like this:</p>
<div>
<table>
<tbody><tr>
  <td>Result for token 1</td>
  <td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>1</td>
  <td>(1 * 1)</td>
</tr>
<tr>
  <td>Result for token 2</td>
  <td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>1</td>
  <td>(0.5*1 + 0.5*1)</td>
</tr>
<tr>
  <td>Result for token 3</td>
  <td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td>
  <td>(0.5*1 + 0.5*(-1))</td>
</tr>
<tr>
  <td>Result for token 4</td>
  <td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td>
  <td>(0.5*(-1) + 0.5*1)</td>
</tr>
<tr>
  <td>Result for token 5</td>
  <td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>1</td>
  <td>(0.5*1 + 0.5*1)</td>
</tr>
</tbody></table>
</div>
<p>Now depending on where this layer is in the Transformer, these rows might be used (after passing through an MLP) as the input to the next Transformer block, or be the predictions for the next token—but note that there's a row for every token!
That's because Transformers are trained to predict the next token for <em>every single token in the context window!</em></p>
<pre data-lang="python"><code data-lang="python"><span># the gpt2 tokenizer produces 3 tokens for this string
</span><span>&gt;&gt;&gt; </span><span>tokens </span><span>= </span><span>tokenizer(</span><span>" A B C"</span><span>).input_ids
</span><span>&gt;&gt;&gt; </span><span>tokens
</span><span>[</span><span>317</span><span>, </span><span>347</span><span>, </span><span>327</span><span>]
</span><span>
</span><span># if we put that into the model, we get 3 rows of logits
</span><span>&gt;&gt;&gt; </span><span>logits </span><span>= </span><span>gpt2(</span><span>input_ids</span><span>=</span><span>torch.tensor(tokens)).logits.squeeze()
</span><span>&gt;&gt;&gt; </span><span>logits.shape
</span><span>torch.Size([</span><span>3</span><span>, </span><span>50257</span><span>])
</span><span>
</span><span># and if we argmax those, we see the model is predicting a next token
</span><span># for _every_ prompt token!
</span><span>&gt;&gt;&gt; for </span><span>i, y </span><span>in </span><span>enumerate</span><span>(logits.argmax(</span><span>-</span><span>1</span><span>)):
</span><span>...     </span><span>print</span><span>(</span><span>f</span><span>"</span><span>{tokenizer.decode(tokens[:i</span><span>+</span><span>1</span><span>])</span><span>!r</span><span>}</span><span> -&gt; </span><span>{tokenizer.decode(y)</span><span>!r</span><span>}</span><span>"</span><span>)
</span><span>' A' </span><span>-&gt; </span><span>'.'
</span><span>' A B' </span><span>-&gt; </span><span>' C'
</span><span>' A B C' </span><span>-&gt; </span><span>' D'
</span></code></pre>
<p>During training, this behavior is desirable—it means more information is flowing into the Transformer since many tokens are being graded instead of just one.
But usually during inference, all we care about is that bottom row, the prediction for the final token.</p>
<p>So how can we get just that out of a Transformer trained to predict the entire context?
Well, let's go back to the attention calculation.
What if <code>q</code> was only one row—the row corresponding to the last token?</p>
<div>
<p>softmax(</p>
<table>
<tbody><tr><th colspan="8">q</th></tr>
<tr> <td>0</td><td>0</td><td>0</td><td>1024</td><td>1024</td><td>0</td><td>0</td><td>0</td> </tr>
</tbody></table>
<p>@</p>
<table>
<tbody><tr><th colspan="5">k.T</th></tr>
<tr><td>1</td><td>0</td><td>0</td><td>0</td><td>0</td></tr>
<tr><td>0</td><td>1</td><td>0</td><td>0</td><td>0</td></tr>
<tr><td>0</td><td>0</td><td>1</td><td>0</td><td>0</td></tr>
<tr><td>0</td><td>0</td><td>0</td><td>1</td><td>0</td></tr>
<tr><td>0</td><td>0</td><td>0</td><td>0</td><td>1</td></tr>
<tr><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td></tr>
<tr><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td></tr>
<tr><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td></tr>
</tbody></table>
<p>+ mask)</p>
<p>@</p>
<table>
<tbody><tr><th colspan="8">v</th></tr>
<tr>
  <td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>1</td>
</tr>
<tr>
  <td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>1</td>
</tr>
<tr>
  <td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>-1</td>
</tr>
<tr>
  <td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>1</td>
</tr>
<tr>
  <td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>1</td>
</tr>
</tbody></table>
</div>
<p>Then, we'd get this as the attention result—just the result for the last token, exactly like what we want.</p>
<div>
<table>
<tbody><tr>
  <td>Result for token 5</td>
  <td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>1</td>
  <td>(0.5*1 + 0.5*1)</td>
</tr>
</tbody></table>
</div>
<p>So that's great, but to only generate the last row of <code>q</code>, that means we can only run the layer that generates the <code>qkv</code> matrix on a single row as well.
So where do the rest of the rows of <code>k</code> and <code>v</code> come from, since we still need them?
The answer is in the name—KV caching—we reuse them from the previous token generation step!
Inside the model, we save the KV values calculated during attention in each Transformer block.
Then on the next generation, only a single token will be passed in, and the cached KV rows will be stacked on top of the K and V row for the new token to produce the single row Q and multi-row K and V that we want.</p>
<p>Here's an example of KV caching with the HuggingFace <code>transformers</code> API, which actually returns the KV cache by default as part of the model forward pass.
The cache is a tuple with a <code>(k, v)</code> tuple for each layer.
The <code>k</code> and <code>v</code> tensors are each of shape <code>(batch_size, n_head, n_seq, head_size)</code>.</p>
<pre data-lang="python"><code data-lang="python"><span>&gt;&gt;&gt; </span><span>tokens
</span><span>[</span><span>317</span><span>, </span><span>347</span><span>, </span><span>327</span><span>] </span><span># the " A B C" string from before
</span><span>&gt;&gt;&gt; </span><span>key_values </span><span>= </span><span>gpt2(</span><span>input_ids</span><span>=</span><span>torch.tensor(tokens)).past_key_values
</span><span>&gt;&gt;&gt; </span><span>tuple</span><span>(</span><span>tuple</span><span>(x.shape </span><span>for </span><span>x </span><span>in </span><span>t) </span><span>for </span><span>t </span><span>in </span><span>key_values)
</span><span>((torch.Size([</span><span>1</span><span>, </span><span>12</span><span>, </span><span>3</span><span>, </span><span>64</span><span>]), torch.Size([</span><span>1</span><span>, </span><span>12</span><span>, </span><span>3</span><span>, </span><span>64</span><span>])),
</span><span> (torch.Size([</span><span>1</span><span>, </span><span>12</span><span>, </span><span>3</span><span>, </span><span>64</span><span>]), torch.Size([</span><span>1</span><span>, </span><span>12</span><span>, </span><span>3</span><span>, </span><span>64</span><span>])),
</span><span> (torch.Size([</span><span>1</span><span>, </span><span>12</span><span>, </span><span>3</span><span>, </span><span>64</span><span>]), torch.Size([</span><span>1</span><span>, </span><span>12</span><span>, </span><span>3</span><span>, </span><span>64</span><span>])),
</span><span> (torch.Size([</span><span>1</span><span>, </span><span>12</span><span>, </span><span>3</span><span>, </span><span>64</span><span>]), torch.Size([</span><span>1</span><span>, </span><span>12</span><span>, </span><span>3</span><span>, </span><span>64</span><span>])),
</span><span> (torch.Size([</span><span>1</span><span>, </span><span>12</span><span>, </span><span>3</span><span>, </span><span>64</span><span>]), torch.Size([</span><span>1</span><span>, </span><span>12</span><span>, </span><span>3</span><span>, </span><span>64</span><span>])),
</span><span> (torch.Size([</span><span>1</span><span>, </span><span>12</span><span>, </span><span>3</span><span>, </span><span>64</span><span>]), torch.Size([</span><span>1</span><span>, </span><span>12</span><span>, </span><span>3</span><span>, </span><span>64</span><span>])),
</span><span> (torch.Size([</span><span>1</span><span>, </span><span>12</span><span>, </span><span>3</span><span>, </span><span>64</span><span>]), torch.Size([</span><span>1</span><span>, </span><span>12</span><span>, </span><span>3</span><span>, </span><span>64</span><span>])),
</span><span> (torch.Size([</span><span>1</span><span>, </span><span>12</span><span>, </span><span>3</span><span>, </span><span>64</span><span>]), torch.Size([</span><span>1</span><span>, </span><span>12</span><span>, </span><span>3</span><span>, </span><span>64</span><span>])),
</span><span> (torch.Size([</span><span>1</span><span>, </span><span>12</span><span>, </span><span>3</span><span>, </span><span>64</span><span>]), torch.Size([</span><span>1</span><span>, </span><span>12</span><span>, </span><span>3</span><span>, </span><span>64</span><span>])),
</span><span> (torch.Size([</span><span>1</span><span>, </span><span>12</span><span>, </span><span>3</span><span>, </span><span>64</span><span>]), torch.Size([</span><span>1</span><span>, </span><span>12</span><span>, </span><span>3</span><span>, </span><span>64</span><span>])),
</span><span> (torch.Size([</span><span>1</span><span>, </span><span>12</span><span>, </span><span>3</span><span>, </span><span>64</span><span>]), torch.Size([</span><span>1</span><span>, </span><span>12</span><span>, </span><span>3</span><span>, </span><span>64</span><span>])),
</span><span> (torch.Size([</span><span>1</span><span>, </span><span>12</span><span>, </span><span>3</span><span>, </span><span>64</span><span>]), torch.Size([</span><span>1</span><span>, </span><span>12</span><span>, </span><span>3</span><span>, </span><span>64</span><span>])))
</span></code></pre>
<p>If we pass this returned KV cache to a model forward pass, the model will treat the tokens we passed in to generate the cache as present even though we don't provide them again.
Note that we only pass a single token here, and only get a single row of logits back!</p>
<pre data-lang="python"><code data-lang="python"><span>&gt;&gt;&gt; </span><span>new_token </span><span>= </span><span>tokenizer(</span><span>" D"</span><span>).input_ids
</span><span>&gt;&gt;&gt; </span><span>new_token
</span><span>[</span><span>360</span><span>]
</span><span>&gt;&gt;&gt; </span><span>logits </span><span>= </span><span>gpt2(</span><span>input_ids</span><span>=</span><span>torch.tensor(new_token), </span><span>past_key_values</span><span>=</span><span>key_values).logits
</span><span>&gt;&gt;&gt; </span><span>logits.shape
</span><span>torch.Size([</span><span>1</span><span>, </span><span>50257</span><span>])
</span><span>&gt;&gt;&gt; </span><span>tokenizer.decode(logits.argmax(</span><span>-</span><span>1</span><span>))
</span><span>' E'
</span></code></pre>
<p>Compare that to if we only pass the single token <em>without</em> passing <code>past_key_values</code>—we get a completion, but it's not conditioned on those previous tokens that the KV cache was generated from.</p>
<pre data-lang="python"><code data-lang="python"><span>&gt;&gt;&gt; </span><span>tokenizer.decode(gpt2(</span><span>input_ids</span><span>=</span><span>torch.tensor(new_token)).logits.argmax(</span><span>-</span><span>1</span><span>))
</span><span>'.'
</span></code></pre>
<p><small>(Also note that e.g. lit-gpt has a nicer KV cache API that handles the cache for you, instead of needing to pass it around manually :-) )</small></p>
<p>KV caching helps with the algorithmic side of LLM slowness—since we're now only passing in a single token on each step, we don't have to redo <em>everything</em> for each new token.
However, it doesn't completely banish the problem, since the KV cache still grows in size each step, slowing down the attention calculation.
The size of the KV cache can also pose its own, new problem—for example, with a 1,000 token KV cache, even with the smallest GPT-2 there are 18,432,000 values being cached.
If each is an fp32, that's almost 74MB of cache, for a single generation, for a comparatively tiny model!
With modern large models, especially running on a server that needs to handle many simultaneous clients, the KV cache can quickly become unmanageable, so a few techniques have popped up to make it better.</p>
<h3 id="Multi-Query_Attention"><a href="#Multi-Query_Attention">
  <img src="https://vgel.me/permalink.svg" alt="permalink for Multi-Query_Attention">
</a>Multi-Query Attention</h3>
<p>Multi-Query attention is a change to the model architecture that shrinks the size of the KV cache by assigning multiple heads to Q, and only a single head to K and V.
It needs to be trained into the model from the beginning—it's not just an inference-time optimization—but it's worth being aware of if you're trying to choose a model, because models with MQA can support more tokens in the KV cache than models trained with normal attention.
To understand that, first we need to understand multi-head attention, so let's digress into that for a second.</p>
<p>Modern LLMs don't usually perform attention on the entire QKV matrix at once like how I described above—instead, the KQV matrix is split into multiple smaller "heads".
That means instead of how it's shown in the diagram above, it looks more like this:</p>
<div>
<table>
<tbody><tr><th colspan="8">q</th></tr>
<tr><td>0.23</td><td>0.03</td><td>0.1</td><td>0.3</td><td>0.87</td><td>0.84</td><td>0.3</td><td>0.3</td></tr>
<tr><td>0.27</td><td>0.61</td><td>0.7</td><td>0.02</td><td>0.83</td><td>0.94</td><td>0.12</td><td>0.21</td></tr>
<tr><td>0.79</td><td>0.23</td><td>0.03</td><td>0.28</td><td>0.02</td><td>0.47</td><td>0.97</td><td>0.61</td></tr>
<tr><td>0.11</td><td>0.1</td><td>0.3</td><td>1.0</td><td>0.08</td><td>0.88</td><td>0.83</td><td>0.69</td></tr>
<tr><td>0.07</td><td>0.01</td><td>0.16</td><td>0.05</td><td>0.51</td><td>0.54</td><td>0.23</td><td>0.47</td></tr>
</tbody></table>
<p>→</p>
<table>
<tbody><tr><th colspan="2">q/0</th></tr>
<tr><td>0.23</td><td>0.03</td></tr>
<tr><td>0.27</td><td>0.61</td></tr>
<tr><td>0.79</td><td>0.23</td></tr>
<tr><td>0.11</td><td>0.1</td></tr>
<tr><td>0.07</td><td>0.01</td></tr>
</tbody></table>
<table>
<tbody><tr><th colspan="2">q/1</th></tr>
<tr><td>0.1</td><td>0.3</td></tr>
<tr><td>0.7</td><td>0.02</td></tr>
<tr><td>0.03</td><td>0.28</td></tr>
<tr><td>0.3</td><td>1.0</td></tr>
<tr><td>0.16</td><td>0.05</td></tr>
</tbody></table>
<table>
<tbody><tr><th colspan="2">q/2</th></tr>
<tr><td>0.87</td><td>0.84</td></tr>
<tr><td>0.83</td><td>0.94</td></tr>
<tr><td>0.02</td><td>0.47</td></tr>
<tr><td>0.08</td><td>0.88</td></tr>
<tr><td>0.51</td><td>0.54</td></tr>
</tbody></table>
<table>
<tbody><tr><th colspan="2">q/3</th></tr>
<tr><td>0.3</td><td>0.3</td></tr>
<tr><td>0.12</td><td>0.21</td></tr>
<tr><td>0.97</td><td>0.61</td></tr>
<tr><td>0.83</td><td>0.69</td></tr>
<tr><td>0.23</td><td>0.47</td></tr>
</tbody></table>
</div>
<div>
<table>
<tbody><tr><th colspan="8">k</th></tr>
<tr><td>0.08</td><td>0.41</td><td>0.36</td><td>0.1</td><td>0.15</td><td>0.03</td><td>0.95</td><td>0.16</td></tr>
<tr><td>0.7</td><td>0.77</td><td>0.57</td><td>0.9</td><td>0.65</td><td>0.36</td><td>0.58</td><td>0.32</td></tr>
<tr><td>0.77</td><td>0.29</td><td>0.42</td><td>0.58</td><td>0.16</td><td>0.49</td><td>0.17</td><td>0.73</td></tr>
<tr><td>0.94</td><td>0.36</td><td>0.16</td><td>0.03</td><td>0.31</td><td>0.67</td><td>0.81</td><td>0.94</td></tr>
<tr><td>0.76</td><td>1.0</td><td>0.45</td><td>0.94</td><td>0.6</td><td>0.49</td><td>0.68</td><td>0.54</td></tr>
</tbody></table>
<p>→</p>
<table>
<tbody><tr><th colspan="2">k/0</th></tr>
<tr><td>0.08</td><td>0.41</td></tr>
<tr><td>0.7</td><td>0.77</td></tr>
<tr><td>0.77</td><td>0.29</td></tr>
<tr><td>0.94</td><td>0.36</td></tr>
<tr><td>0.76</td><td>1.0</td></tr>
</tbody></table>
<table>
<tbody><tr><th colspan="2">k/1</th></tr>
<tr><td>0.36</td><td>0.1</td></tr>
<tr><td>0.57</td><td>0.9</td></tr>
<tr><td>0.42</td><td>0.58</td></tr>
<tr><td>0.16</td><td>0.03</td></tr>
<tr><td>0.45</td><td>0.94</td></tr>
</tbody></table>
<table>
<tbody><tr><th colspan="2">k/2</th></tr>
<tr><td>0.15</td><td>0.03</td></tr>
<tr><td>0.65</td><td>0.36</td></tr>
<tr><td>0.16</td><td>0.49</td></tr>
<tr><td>0.31</td><td>0.67</td></tr>
<tr><td>0.6</td><td>0.49</td></tr>
</tbody></table>
<table>
<tbody><tr><th colspan="2">k/3</th></tr>
<tr><td>0.95</td><td>0.16</td></tr>
<tr><td>0.58</td><td>0.32</td></tr>
<tr><td>0.17</td><td>0.73</td></tr>
<tr><td>0.81</td><td>0.94</td></tr>
<tr><td>0.68</td><td>0.54</td></tr>
</tbody></table>
</div>
<div>
<table>
<tbody><tr><th colspan="8">v</th></tr>
<tr><td>0.54</td><td>0.8</td><td>0.73</td><td>0.35</td><td>0.97</td><td>0.05</td><td>0.07</td><td>0.45</td></tr>
<tr><td>0.81</td><td>0.42</td><td>0.13</td><td>0.33</td><td>0.6</td><td>0.75</td><td>0.41</td><td>0.36</td></tr>
<tr><td>0.81</td><td>0.47</td><td>0.2</td><td>0.05</td><td>0.63</td><td>0.75</td><td>0.58</td><td>0.66</td></tr>
<tr><td>0.69</td><td>0.89</td><td>0.09</td><td>0.49</td><td>0.49</td><td>0.63</td><td>0.91</td><td>0.88</td></tr>
<tr><td>0.19</td><td>0.39</td><td>0.22</td><td>0.36</td><td>1.0</td><td>0.17</td><td>0.66</td><td>0.02</td></tr>
</tbody></table>
<p>→</p>
<table>
<tbody><tr><th colspan="2">v/0</th></tr>
<tr><td>0.54</td><td>0.8</td></tr>
<tr><td>0.81</td><td>0.42</td></tr>
<tr><td>0.81</td><td>0.47</td></tr>
<tr><td>0.69</td><td>0.89</td></tr>
<tr><td>0.19</td><td>0.39</td></tr>
</tbody></table>
<table>
<tbody><tr><th colspan="2">v/1</th></tr>
<tr><td>0.73</td><td>0.35</td></tr>
<tr><td>0.13</td><td>0.33</td></tr>
<tr><td>0.2</td><td>0.05</td></tr>
<tr><td>0.09</td><td>0.49</td></tr>
<tr><td>0.22</td><td>0.36</td></tr>
</tbody></table>
<table>
<tbody><tr><th colspan="2">v/2</th></tr>
<tr><td>0.97</td><td>0.05</td></tr>
<tr><td>0.6</td><td>0.75</td></tr>
<tr><td>0.63</td><td>0.75</td></tr>
<tr><td>0.49</td><td>0.63</td></tr>
<tr><td>1.0</td><td>0.17</td></tr>
</tbody></table>
<table>
<tbody><tr><th colspan="2">v/3</th></tr>
<tr><td>0.07</td><td>0.45</td></tr>
<tr><td>0.41</td><td>0.36</td></tr>
<tr><td>0.58</td><td>0.66</td></tr>
<tr><td>0.91</td><td>0.88</td></tr>
<tr><td>0.66</td><td>0.02</td></tr>
</tbody></table>
</div>
<p>Then each head is combined in attention as before:</p>
<div>
<p>softmax(</p>
<table>
<tbody><tr><th colspan="2">q/0</th></tr>
<tr><td>0.23</td><td>0.03</td></tr>
<tr><td>0.27</td><td>0.61</td></tr>
<tr><td>0.79</td><td>0.23</td></tr>
<tr><td>0.11</td><td>0.1</td></tr>
<tr><td>0.07</td><td>0.01</td></tr>
</tbody></table>
<p>@</p>
<table>
<tbody><tr><th colspan="5">(k/0).T</th></tr>
<tr><td>0.08</td><td>0.7</td><td>0.77</td><td>0.94</td><td>0.76</td></tr>
<tr><td>0.41</td><td>0.77</td><td>0.29</td><td>0.36</td><td>1.0</td></tr>
</tbody></table>
<p>+ mask)</p>
<p>@</p>
<table>
<tbody><tr><th colspan="2">v/0</th></tr>
<tr><td>0.54</td><td>0.8</td></tr>
<tr><td>0.81</td><td>0.42</td></tr>
<tr><td>0.81</td><td>0.47</td></tr>
<tr><td>0.69</td><td>0.89</td></tr>
<tr><td>0.19</td><td>0.39</td></tr>
</tbody></table>
</div>
<p>The individual small result matrices are then stuck back together to recreate a final result matrix of shape <code>(seq_len, embed_size)</code>, just like the result of vanilla attention.
This process allows each head to be used for a different task (e.g., one head could handle punctuation in acronyms and another French articles), instead of wastefully dedicating an entire Transformer block to a single task.</p>
<p>So what is <a href="https://arxiv.org/abs/1911.02150">Multi-Query Attention</a>?
Instead of Q, K, and V all being split into separate heads, <em>only</em> Q is split.
K and V are smaller, the size of a single head, and that single K and V is shared among all the Q heads.</p>
<div>
<table>
<tbody><tr><th colspan="8">q</th></tr>
<tr><td>0.23</td><td>0.03</td><td>0.1</td><td>0.3</td><td>0.87</td><td>0.84</td><td>0.3</td><td>0.3</td></tr>
<tr><td>0.27</td><td>0.61</td><td>0.7</td><td>0.02</td><td>0.83</td><td>0.94</td><td>0.12</td><td>0.21</td></tr>
<tr><td>0.79</td><td>0.23</td><td>0.03</td><td>0.28</td><td>0.02</td><td>0.47</td><td>0.97</td><td>0.61</td></tr>
<tr><td>0.11</td><td>0.1</td><td>0.3</td><td>1.0</td><td>0.08</td><td>0.88</td><td>0.83</td><td>0.69</td></tr>
<tr><td>0.07</td><td>0.01</td><td>0.16</td><td>0.05</td><td>0.51</td><td>0.54</td><td>0.23</td><td>0.47</td></tr>
</tbody></table>
<p>→</p>
<table>
<tbody><tr><th colspan="2">q/0</th></tr>
<tr><td>0.23</td><td>0.03</td></tr>
<tr><td>0.27</td><td>0.61</td></tr>
<tr><td>0.79</td><td>0.23</td></tr>
<tr><td>0.11</td><td>0.1</td></tr>
<tr><td>0.07</td><td>0.01</td></tr>
</tbody></table>
<table>
<tbody><tr><th colspan="2">q/1</th></tr>
<tr><td>0.1</td><td>0.3</td></tr>
<tr><td>0.7</td><td>0.02</td></tr>
<tr><td>0.03</td><td>0.28</td></tr>
<tr><td>0.3</td><td>1.0</td></tr>
<tr><td>0.16</td><td>0.05</td></tr>
</tbody></table>
<table>
<tbody><tr><th colspan="2">q/2</th></tr>
<tr><td>0.87</td><td>0.84</td></tr>
<tr><td>0.83</td><td>0.94</td></tr>
<tr><td>0.02</td><td>0.47</td></tr>
<tr><td>0.08</td><td>0.88</td></tr>
<tr><td>0.51</td><td>0.54</td></tr>
</tbody></table>
<table>
<tbody><tr><th colspan="2">q/3</th></tr>
<tr><td>0.3</td><td>0.3</td></tr>
<tr><td>0.12</td><td>0.21</td></tr>
<tr><td>0.97</td><td>0.61</td></tr>
<tr><td>0.83</td><td>0.69</td></tr>
<tr><td>0.23</td><td>0.47</td></tr>
</tbody></table>
</div>
<div>
<table>
<tbody><tr><th colspan="2">k</th></tr>
<tr><td>0.08</td><td>0.41</td></tr>
<tr><td>0.7</td><td>0.77</td></tr>
<tr><td>0.77</td><td>0.29</td></tr>
<tr><td>0.94</td><td>0.36</td></tr>
<tr><td>0.76</td><td>1.0</td></tr>
</tbody></table>

<table>
<tbody><tr><th colspan="2">v</th></tr>
<tr><td>0.54</td><td>0.8</td></tr>
<tr><td>0.81</td><td>0.42</td></tr>
<tr><td>0.81</td><td>0.47</td></tr>
<tr><td>0.69</td><td>0.89</td></tr>
<tr><td>0.19</td><td>0.39</td></tr>
</tbody></table>
</div>
<p>You might think this would be a serious problem for the model, but it actually has only a small effect on perplexity.
This table from the MQA paper shows slightly worse results than the Multi-Head Attention baseline, but better than alternatives involving shrinking all the dimensions of MHA.</p>
<p><a href="https://vgel.me/posts/faster-inference/multi_query_table3.png"><img src="https://vgel.me/posts/faster-inference/multi_query_table3.png" alt="dev-PPL of MHA: 29.9, MQA: 30.2, best MHA with smaller dimensions: 30.9"></a></p>
<p>The benefit is, because K and V are so much smaller than in MHA, the KV cache is proportionally smaller as well.
Both LLAMA-2 and Falcon use MQA, for this reason.</p>
<p>Mistral 7B uses a variant called <a href="https://arxiv.org/abs/2305.13245v2">Grouped-Query Attention</a> which is a hybrid between MQA and MHA.
If MHA is <code>Q_heads=K_heads=V_heads=N</code> and MQA is <code>Q_heads=N; K_heads=V_heads=1</code>, then GQA is <code>Q_heads=N; K_heads=V_heads=G</code> where <code>1 &lt; G &lt; N</code>.
GQA claims less effect on perplexity and better training stability than MQA.</p>
<h3 id="PagedAttention"><a href="#PagedAttention">
  <img src="https://vgel.me/permalink.svg" alt="permalink for PagedAttention">
</a>PagedAttention</h3>
<p>The other issue with a large KV cache is that it often needs to be stored as in <em>contiguous</em> tensors, regardless of whether all of the cache is currently in use.
That leads to multiple problems:</p>
<ul>
<li>More space than necessary needs to be allocated up front, since we need to anticipate the maximum size of the KV cache before it's needed.</li>
<li>That reserved space can't be used by other requests, even if it isn't needed <em>yet</em>.</li>
<li>Requests with the same prefix can't share KV cache for that prefix, since they may diverge later.</li>
</ul>
<p><a href="https://arxiv.org/abs/2309.06180">PagedAttention</a> fixes these problems by taking inspiration from how operating systems handle a similar issue with userspace program memory.</p>
<p>Let's take a moment to explore OS paging, as a primer.
Like tensors, programs want to see their memory as a contiguous linear space.
<small>(If I allocate a million-byte array <code>x</code>, I expect the address of <code>x[n + 1]</code> to exactly equal <code>x[n] + 1</code>, no more, no less! Much code depends on this.)</small>
However, physical memory isn't always so forgiving—operating systems have to worry about pesky things like "fragmentation" and <a href="https://vgel.me/posts/mmap-arena-alloc/">"hey you asked for a 1TiB allocation i cannot put this anywhere"</a>.</p>
<p>So the operating system collaborates with hardware, using the <abbr title="Memory Management Unit">MMU</abbr> to map virtual pages to physical pages in a page table.
When you access an address in a userspace program, that address gets translated from your program's address space via the page table <small>(and TLB cache)</small> to a physical address before being read from or written to.
Importantly, that physical address may not exist yet—it may be generated on-demand for a write.
For example, let's map 16 pages of memory in C:</p>
<pre data-lang="c"><code data-lang="c"><span>
</span><span>uint8_t </span><span>*</span><span>pages </span><span>= </span><span>mmap(</span><span>NULL</span><span>, page_size </span><span>* </span><span>16</span><span>,
</span><span>                      PROT_READ </span><span>|</span><span> PROT_WRITE,
</span><span>                      MAP_PRIVATE </span><span>|</span><span> MAP_ANONYMOUS,
</span><span>                      </span><span>-</span><span>1</span><span>, </span><span>0</span><span>);
</span></code></pre>
<p>Now if we <a href="https://stackoverflow.com/a/45500208/1159735">pagemap-dump</a> the running program, we get this list of page addresses and associated <code>pfn</code> physical addresses (along with other metadata):</p>
<pre data-lang="bash"><code data-lang="bash"><span># addr pfn soft-dirty file/shared swapped present library
</span><span>7fbe0fe6a000 0 1 0 0 0 
</span><span>7fbe0fe6b000 0 1 0 0 0 
</span><span>7fbe0fe6c000 0 1 0 0 0 
</span><span>7fbe0fe6d000 0 1 0 0 0 
</span><span>7fbe0fe6e000 0 1 0 0 0 
</span><span>7fbe0fe6f000 0 1 0 0 0 
</span><span>7fbe0fe70000 0 1 0 0 0 
</span><span>7fbe0fe71000 0 1 0 0 0 
</span><span>7fbe0fe72000 0 1 0 0 0 
</span><span>7fbe0fe73000 0 1 0 0 0 
</span><span>7fbe0fe74000 0 1 0 0 0 
</span><span>7fbe0fe75000 0 1 0 0 0 
</span><span>7fbe0fe76000 0 1 0 0 0 
</span><span>7fbe0fe77000 0 1 0 0 0 
</span><span>7fbe0fe78000 0 1 0 0 0 
</span><span>7fbe0fe79000 0 1 0 0 0 
</span></code></pre>
<p>Notice that all the pages have a physical address of zero—they don't exist yet!
This is called <em>memory overcommit</em>.
The kernel doesn't know if we're going to use these pages, so it doesn't bother to set up mappings for them yet.
Trying to read from them will just return an unspecified value (this is part of why reading uninitialized memory is UB in C).</p>
<p>However, if I then touch every page by writing to it...</p>
<pre data-lang="c"><code data-lang="c"><span>// pages is our page_size * 16 map from earlier
</span><span>for </span><span>(</span><span>int</span><span> i </span><span>= </span><span>0</span><span>; i </span><span>&lt;</span><span> page_size </span><span>* </span><span>16</span><span>; i</span><span>++</span><span>) pages[i] </span><span>= </span><span>1</span><span>;
</span></code></pre>
<p>...the dump looks different!</p>
<pre data-lang="bash"><code data-lang="bash"><span># addr pfn soft-dirty file/shared swapped present library
</span><span>7fbe0fe6a000 14a009 1 0 0 1 
</span><span>7fbe0fe6b000 50eca5 1 0 0 1 
</span><span>7fbe0fe6c000 175a5d 1 0 0 1 
</span><span>7fbe0fe6d000 148d85 1 0 0 1 
</span><span>7fbe0fe6e000 2de86c 1 0 0 1 
</span><span>7fbe0fe6f000 13a300 1 0 0 1 
</span><span>7fbe0fe70000 8f25b4 1 0 0 1 
</span><span>7fbe0fe71000 15ae63 1 0 0 1 
</span><span>7fbe0fe72000 6e1d7f 1 0 0 1 
</span><span>7fbe0fe73000 13a307 1 0 0 1 
</span><span>7fbe0fe74000 14a074 1 0 0 1 
</span><span>7fbe0fe75000 14a0a7 1 0 0 1 
</span><span>7fbe0fe76000 7e2662 1 0 0 1 
</span><span>7fbe0fe77000 1ccdc2 1 0 0 1 
</span><span>7fbe0fe78000 2a4f06 1 0 0 1 
</span><span>7fbe0fe79000 2169ef 1 0 0 1 
</span></code></pre>
<p>Now all the pages have a physical address, because they've been written to.
However, note that the physical addresses aren't contiguous like the virtual addresses!
<small>(The largest physical address is 0x7e2662, which is the mapping for virtual address 0x7fbe0fe76000, page #13.)</small>
They're scattered all over the place, wherever they can fit.
And if our C program had only touched e.g. half the pages, only those pages would've been mapped, the others would have remained unmapped.
This means that no physical memory is reserved until the exact moment that it's needed.</p>
<p>I could share only part of this memory with another process.
Imagine I mapped 4 pages of shared memory:</p>
<pre data-lang="bash"><code data-lang="bash"><span>7fbe0fe6a000 14a009
</span><span>7fbe0fe6b000 50eca5 
</span><span>7fbe0fe6c000 175a5d
</span><span>7fbe0fe6d000 148d85
</span></code></pre>
<p>The other process might see these pages as:</p>
<pre data-lang="bash"><code data-lang="bash"><span>5581627bb000 14a009
</span><span>5581627bc000 50eca5
</span><span>5581627bd000 175a5d
</span><span>5581627be000 148d85
</span></code></pre>
<p>The virtual addresses are different, but the physical addresses are identical!
Each program might see these pages interleaved in different contexts, but the underlying data can be stored, deduplicated, in a single place in physical memory.</p>
<p>So how does this apply to PagedAttention?
PagedAttention is the <em>exact same idea</em>—they say so in the paper.<sup><a href="#why-not-mmu">4</a></sup></p>
<p>Instead of pages, we have blocks of KV cache for tokens, and instead of processes accessing those pages, we have LLM requests accessing those blocks of tokens.</p>
<p>At startup, PagedAttention allocates a <em>block table</em> for the request, analogous to the role of a hardware MMU.
This block table starts out empty, with no blocks mapped, just like our C process.</p>
<p>Instead of associating requests with a large tensor of KV cache items, each request only has a comparatively small list of block indices, analogous to virtual addresses in OS paging.
Those indices point at blocks stored in the global block table.
Just like OS pages, they can be out of order, placed wherever they can fit:</p>
<p><a href="https://vgel.me/posts/faster-inference/pagedattention.png"><img src="https://vgel.me/posts/faster-inference/pagedattention.png" alt=""></a></p>
<p>During the attention computation, the PagedAttention kernel walks the request's list of block indices, and goes and fetches those blocks from the global block table to compute attention as normal in the correct order.</p>
<p>Importantly, because the blocks have been decoupled from individual requests, they can be shared, just like the shared memory example in OS paging.
If two requests use the same long prefix (such as k-shot examples for multiple parallel translation tasks, the newest Twitter prompt hack, the chain so far for self-consistency Chain of Thought, etc.), the KV cache blocks for that prefix can be shared by multiple requests, simply by placing the index of that block in the appropriate part of each request's list of block indices.</p>
<p><a href="https://vgel.me/posts/faster-inference/shared-prefix-pagedattention.drawio.png"><img src="https://vgel.me/posts/faster-inference/shared-prefix-pagedattention.drawio.png" alt=""></a></p>
<h2 id="Speculative_Decoding"><a href="#Speculative_Decoding">
  <img src="https://vgel.me/permalink.svg" alt="permalink for Speculative_Decoding">
</a>Speculative Decoding</h2>
<p>To understand speculative decoding, you need to remember three things.</p>
<p>First, running a small number of tokens through a model takes about the same amount of time as running a single token, thanks to memory access overhead:</p>
<p><a href="https://vgel.me/posts/faster-inference/timing_graph.png"><img src="https://vgel.me/posts/faster-inference/timing_graph.png" alt="Log-log plot of time to generate N tokens, in seconds, showing a flat region from 1-10 tokens that then goes linear"></a></p>
<p>Second, LLMs generate a prediction for <em>every</em> token in the context:</p>
<pre data-lang="python"><code data-lang="python"><span>&gt;&gt;&gt; for </span><span>i, y </span><span>in </span><span>enumerate</span><span>(logits.argmax(</span><span>-</span><span>1</span><span>)):
</span><span>...     </span><span>print</span><span>(</span><span>f</span><span>"</span><span>{tokenizer.decode(tokens[:i</span><span>+</span><span>1</span><span>])</span><span>!r</span><span>}</span><span> -&gt; </span><span>{tokenizer.decode(y)</span><span>!r</span><span>}</span><span>"</span><span>)
</span><span>' A' </span><span>-&gt; </span><span>'.'
</span><span>' A B' </span><span>-&gt; </span><span>' C'
</span><span>' A B C' </span><span>-&gt; </span><span>' D'
</span></code></pre>
<p>Finally third, some words are very easy to predict.
For example, after the word "going", you don't need to be GPT-4 to know that the word "to" is an extremely likely next token.</p>
<p>We can take advantage of these facts to optimize generation!
Imagine if, whenever the most recent token was " going", we optimistically tacked " to" onto the end as well before running generation.
Then, after running the model forward, we check if the model's prediction after "going" was indeed "to".
if so, we got a token ("to") for free!
And if not, no sweat, we simply accept the token predicted for "going" instead, with no increased perplexity, since that token is the exact token the model would have generated without our trick.</p>
<pre data-lang="python"><code data-lang="python"><span>def </span><span>generate</span><span>(</span><span>prompt</span><span>: </span><span>str</span><span>, </span><span>tokens_to_generate</span><span>: </span><span>int</span><span>) -&gt; </span><span>str</span><span>:
</span><span>    tokens: </span><span>list</span><span>[</span><span>int</span><span>] </span><span>= </span><span>tokenize(prompt)
</span><span>    GOING, TO </span><span>= </span><span>tokenize(</span><span>" going to"</span><span>)
</span><span>
</span><span>    </span><span>for </span><span>i </span><span>in </span><span>range</span><span>(tokens_to_generate):
</span><span>        </span><span>if </span><span>tokens[</span><span>-</span><span>1</span><span>] </span><span>== </span><span>GOING:
</span><span>          </span><span># do our speculative decoding trick
</span><span>          logits </span><span>= </span><span>model.forward(tokens </span><span>+ </span><span>[TO])
</span><span>          </span><span># the token the model predicts will follow "... going"
</span><span>          going_pred </span><span>= </span><span>argmax(logits[</span><span>-</span><span>2</span><span>, :])
</span><span>          </span><span># the token the model predicts will follow "... going to" 
</span><span>          to_pred </span><span>= </span><span>argmax(logits[</span><span>-</span><span>1</span><span>, :])
</span><span>          </span><span>if </span><span>going_pred </span><span>== </span><span>TO:
</span><span>            </span><span># if our guess was correct, accept "to" and the next token after
</span><span>            tokens </span><span>+= </span><span>[TO, to_pred]
</span><span>          </span><span>else</span><span>:
</span><span>            </span><span># otherwise, accept the real next token
</span><span>            </span><span># (e.g. "for" if the true generation was "going for broke")
</span><span>            tokens </span><span>+= </span><span>[going_pred]
</span><span>        </span><span>else</span><span>:
</span><span>          </span><span># do normal single-token generation
</span><span>          logits </span><span>= </span><span>model.forward(tokens)
</span><span>          tokens </span><span>+= </span><span>[argmax(logits[</span><span>-</span><span>1</span><span>])]
</span><span>
</span><span>    </span><span>return </span><span>detokenize(tokens)
</span></code></pre>
<p>This will absolutely work, and buy you a (minuscule) speed boost.
To improve it, you could imagine making more heuristics: predict "and" after a comma, and "the" after "is".
You could even make multi-word heuristics: if you see "The early bird", why not optimistically add "catches the worm"?
Even if they're doing a twist on the phrase, you could still win "catches" and "the", the entire phrase doesn't need to be accepted.</p>
<p>But wait a second—why make these heuristics by hand?
We're trying to come up with likely completions of a token... that's exactly what language models are good at!
If we use a language model to generate our optimistic tokens, we could pick up even more complex patterns, even ones from the earlier context.</p>
<p>We just need to use a "draft" model that's small enough (and therefore quick enough to run) that it will pay for itself by avoiding passes through the larger "oracle" model.
A good rule of thumb is for this model to be ~1/10 the size of the oracle model.
It should also use the same tokenizer (to avoid needing to detokenize and retokenize the sequence over and over).</p>
<p>Here's what the generate loop looks like with a draft model:</p>
<pre data-lang="python"><code data-lang="python"><span>def </span><span>generate</span><span>(</span><span>prompt</span><span>: </span><span>str</span><span>, </span><span>tokens_to_generate</span><span>: </span><span>int</span><span>, </span><span>n_draft</span><span>: </span><span>int </span><span>= </span><span>8</span><span>) -&gt; </span><span>str</span><span>:
</span><span>    tokens: </span><span>list</span><span>[</span><span>int</span><span>] </span><span>= </span><span>tokenize(prompt)
</span><span>
</span><span>    </span><span>for </span><span>i </span><span>in </span><span>range</span><span>(tokens_to_generate):
</span><span>        </span><span># generate `n_draft` draft tokens in the usual autoregressive way
</span><span>        draft </span><span>= </span><span>tokens[:]
</span><span>        </span><span>for </span><span>_ </span><span>in </span><span>range</span><span>(n_draft):
</span><span>            logits </span><span>= </span><span>draft_model.forward(draft)
</span><span>            draft.append(argmax(logits[</span><span>-</span><span>1</span><span>]))
</span><span>
</span><span>        </span><span># run the draft tokens through the oracle model all at once
</span><span>        logits </span><span>= </span><span>model.forward(draft)
</span><span>        checked </span><span>= </span><span>logits[</span><span>len</span><span>(tokens) </span><span>- </span><span>1 </span><span>:].argmax(</span><span>-</span><span>1</span><span>)
</span><span>
</span><span>        </span><span># find the index of the first draft/oracle mismatch—we'll accept every
</span><span>        </span><span># token before it
</span><span>        </span><span># (the index might be past the end of the draft, if every draft token
</span><span>        </span><span># was correct)
</span><span>        n_accepted </span><span>= </span><span>next</span><span>(
</span><span>            idx </span><span>+ </span><span>1
</span><span>            </span><span>for </span><span>idx, (checked, draft) </span><span>in </span><span>enumerate</span><span>(
</span><span>                </span><span># we add None here because the oracle model generates one extra
</span><span>                </span><span># token (the prediction for the last draft token)
</span><span>                </span><span>zip</span><span>(checked, draft[</span><span>len</span><span>(tokens) :] </span><span>+ </span><span>[</span><span>None</span><span>])
</span><span>            )
</span><span>            </span><span>if </span><span>checked </span><span>!= </span><span>draft
</span><span>        )
</span><span>        tokens.extend(checked[:n_accepted])
</span><span>
</span><span>    </span><span>return </span><span>detokenize(tokens)
</span></code></pre>
<p>Here, I used the above loop with GPT-2-XL (1558M parameters) as the oracle model, and GPT-2-small (124M parameters) as the draft model, with <code>n_draft=8</code>.
The green tokens were generated by the draft model, and the blue tokens are where the draft model was incorrect and the oracle model's corrected token had to be used.</p>
<p><span>What is a tensor in machine learning?</span>
<span>&nbsp;<br></span>
<span>A</span><span> tens</span><span>or</span><span> is</span><span> a</span><span> mathematical</span><span> object</span><span> that</span><span> represents</span><span> a</span><span> set</span><span> of</span><span> data</span><span> points</span><span>.</span><span> It</span><span> is</span><span> a</span><span> mathematical</span><span> object</span><span> that</span><span> can</span><span> be</span><span> used</span><span> to</span><span> represent</span><span> a</span><span> set</span><span> of</span><span> data</span><span> points</span><span>.</span>
</p>
<p>Note how the draft model is particularly good at quickly copying text from the question ("A tensor is a"), completing common N-grams ("It" -&gt; "is a", "can" -&gt; "can be"), and inserting stock phrases ("a set of data points"), so that the oracle model only needs to step in for a few key words.</p>
<p>Here's another example, with the same setup but a different prompt:</p>
<p><span>Index: A B C</span><span> D</span><span> E</span><span> F</span><span> G</span><span> H</span><span> I</span><span> J</span><span> K</span><span> L</span><span> M</span><span> N</span><span> O</span><span> P</span><span> Q</span><span> R</span><span> S</span><span> T</span><span> U</span><span> V</span><span> W</span><span> X</span><span> Y</span><span> Z</span><span>
</span><span>
</span><span>The</span><span> following</span><span> table</span><span> lists</span><span> the</span><span> number</span><span> of</span><span> times</span><span> each</span><span> of</span><span> the</span><span> following</span><span> words</span><span> appear</span><span> in</span><span> the</span><span> text</span><span> of</span><span> the</span><span> book</span><span>.</span><span>
</span><span>
</span><span>Word</span><span> Number</span><span> of</span><span> times</span><span> in</span><span> text</span>
</p>
<p>Here, the draft model does very well in the alphabet part, actually hitting the draft limit several times (the draft model would have correctly predicted "L", for example, but we limit it to 8 tokens at once).
But once it got into the prose generation below, the draft model couldn't keep up as well.</p>
<p>Finally, here's a pathological case:</p>
<p><span>The digits of Pi are 3.14159</span><span>265</span><span>35</span><span>89</span><span>793</span><span>238</span><span>46</span><span>264</span><span>33</span><span>83</span><span>279</span><span>50</span><span>28</span><span>84</span><span>197</span><span>16</span><span>93</span><span>99</span><span>375</span><span>10</span><span>58</span><span>20</span>
</p>
<p>At first the models are in agreement, but fairly quickly they diverge as the draft model becomes inaccurate, and generation becomes unbearably slow.
For every token, 8 draft tokens are generated and then immediately discarded.</p>
<p>This shows that speculative decoding performance can be very context dependent!
If the draft model is well-correlated with the oracle model and the text is easy to predict, you'll get lots of drafted tokens and fast inference.
But if the models aren't correlated, speculative decoding can actually make inference <em>slower</em>, because you're wasting time generating draft tokens that will just be rejected.</p>
<h3 id="Threshold_decoding?"><a href="#Threshold_decoding?">
  <img src="https://vgel.me/permalink.svg" alt="permalink for Threshold_decoding?">
</a>Threshold decoding?</h3>
<p>An approach I came up with to mitigate the issues with using a fixed number of draft tokens is <em>threshold decoding</em>.</p>
<p>Instead of always decoding up to the maximum number of draft tokens, we keep a moving probability threshold, calibrated based on how many tokens are being accepted right now.
Draft tokens are generated until the cumulative probability of the draft so far (based on the draft model logits) falls below this threshold.</p>
<p>For example, if the threshold was 0.5, and the we generated a draft token " the" with a probability of 0.75, we'd keep going.
If the next token, " next", had a probability of 0.5, the cumulative probability 0.375 would be lower than the threshold, so we'd stop and submit the two draft tokens to the oracle.</p>
<p>Then, based on how far into the draft is accepted, the threshold is adjusted up or down to try and calibrate the draft model's confidence with the actual acceptance rate.
Right now this is just done by a simple moving average and some thresholding, but there's probably a more principled way to do it based on real statistics.</p>
<p>This is the code (using my homegrown framework, apologies):</p>
<pre data-lang="python"><code data-lang="python"><span>def </span><span>speculative_threshold</span><span>(
</span><span>    </span><span>prompt</span><span>: </span><span>str</span><span>,
</span><span>    </span><span>max_draft</span><span>: </span><span>int </span><span>= </span><span>16</span><span>,
</span><span>    </span><span>threshold</span><span>: </span><span>float </span><span>= </span><span>0.4</span><span>,
</span><span>    </span><span>threshold_all_correct_boost</span><span>: </span><span>float </span><span>= </span><span>0.1</span><span>,
</span><span>):
</span><span>    tokens </span><span>= </span><span>encoder.encode(prompt)
</span><span>
</span><span>    </span><span># homegrown KV cache setup has an `n_tokens` method that returns the length
</span><span>    </span><span># of the cached sequence, and a `truncate` method to truncate that sequence
</span><span>    </span><span># to a specific token
</span><span>    model_kv </span><span>= </span><span>gpt2.KVCache()
</span><span>    draft_kv </span><span>= </span><span>gpt2.KVCache()
</span><span>
</span><span>    </span><span>while </span><span>True</span><span>:
</span><span>        </span><span># generate up to `max_draft` draft tokens autoregressively, stopping
</span><span>        </span><span># early if we fall below `threshold`
</span><span>        draft </span><span>= </span><span>tokens[:]
</span><span>        drafted_probs </span><span>= </span><span>[]
</span><span>        </span><span>for </span><span>_ </span><span>in </span><span>range</span><span>(max_draft):
</span><span>            logits </span><span>= </span><span>draft_model.forward(draft[draft_kv.n_tokens() :], draft_kv)
</span><span>            next_id </span><span>= </span><span>np.argmax(logits[</span><span>-</span><span>1</span><span>])
</span><span>            next_prob </span><span>= </span><span>gpt2.softmax(logits[</span><span>-</span><span>1</span><span>])[next_id]
</span><span>            </span><span>if not </span><span>len</span><span>(drafted_probs):
</span><span>                drafted_probs.append(next_prob)
</span><span>            </span><span>else</span><span>:
</span><span>                drafted_probs.append(next_prob </span><span>* </span><span>drafted_probs[</span><span>-</span><span>1</span><span>])
</span><span>            draft.append(</span><span>int</span><span>(next_id))
</span><span>            </span><span>if </span><span>drafted_probs[</span><span>-</span><span>1</span><span>] </span><span>&lt; </span><span>threshold:
</span><span>                </span><span>break
</span><span>        n_draft </span><span>= </span><span>len</span><span>(draft) </span><span>- </span><span>len</span><span>(tokens)
</span><span>
</span><span>        </span><span># run draft tokens through the oracle model
</span><span>        logits </span><span>= </span><span>model.forward(draft[model_kv.n_tokens() :], model_kv)
</span><span>        checked </span><span>= </span><span>logits[</span><span>-</span><span>n_draft </span><span>- </span><span>1 </span><span>:].argmax(</span><span>-</span><span>1</span><span>)
</span><span>        n_accepted </span><span>= </span><span>next</span><span>(
</span><span>            idx </span><span>+ </span><span>1
</span><span>            </span><span>for </span><span>idx, (checked, draft) </span><span>in </span><span>enumerate</span><span>(
</span><span>                </span><span>zip</span><span>(checked, draft[</span><span>len</span><span>(tokens) :] </span><span>+ </span><span>[</span><span>None</span><span>])
</span><span>            )
</span><span>            </span><span>if </span><span>checked </span><span>!= </span><span>draft
</span><span>        )
</span><span>        </span><span>yield from </span><span>checked[:n_accepted]
</span><span>        tokens.extend(checked[:n_accepted])
</span><span>
</span><span>        </span><span>if </span><span>n_accepted </span><span>&lt;= </span><span>n_draft:
</span><span>            </span><span># adjust threshold towards prob of last accepted token, if we
</span><span>            </span><span># ignored any draft tokens
</span><span>            threshold </span><span>= </span><span>(threshold </span><span>+ </span><span>drafted_probs[n_accepted </span><span>- </span><span>1</span><span>]) </span><span>/ </span><span>2
</span><span>        </span><span>else</span><span>:
</span><span>            </span><span># otherwise, lower the threshold slightly, we're probably being
</span><span>            </span><span># too conservative
</span><span>            threshold </span><span>-= </span><span>threshold_all_correct_boost
</span><span>        </span><span># clamp to avoid pathological thresholds
</span><span>        threshold </span><span>= </span><span>min</span><span>(</span><span>max</span><span>(threshold, </span><span>0.05</span><span>), </span><span>0.95</span><span>)
</span><span>
</span><span>        </span><span># don't include oracle token in kv cache
</span><span>        model_kv.truncate(</span><span>len</span><span>(tokens) </span><span>- </span><span>1</span><span>)
</span><span>        draft_kv.truncate(</span><span>len</span><span>(tokens) </span><span>- </span><span>1</span><span>)
</span></code></pre>
<p>This table compares threshold decoding to regular speculative decoding with different fixed draft lengths (along with KV caching, to make the comparison fair), for the test prompts I showed earlier.
The rightmost column is a sparkline showing how generation speed changes over time.</p>
<table>
<tbody><tr><th colspan="3">What is a tensor in machine learning?</th></tr>
<tr>
<td>threshold</td>
<td>2.01 tok/s</td>
<td>▁▂▄▅▄▅▅▆▅▄▅▅▅▅▅▆▅▅▅▅▆▅▆▆▅▆▅▆▆▆▅▆▆▆▆▇▆▆▇▇▇▆▆▆▇▇▇▇▇█</td>
</tr>
<tr>
<td>n_draft=1</td>
<td>1.40 tok/s</td>
<td>▁▃▃▅▅▆▆▇▆▆▆▆▇▆▇▇▇▇▆▇▆▆▆▆▆▇▆▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇█</td>
</tr>
<tr>
<td>n_draft=8</td>
<td>1.92 tok/s</td>
<td>▁▂▃▄▅▆▇█▅▃▄▃▄▄▄▄▅▅▄▅▅▄▄▅▄▄▄▄▄▄▅▅▅▅▅▅▅▅▅▅▆▆▆▆▆▆▆▆▆▆</td>
</tr>
<tr>
<td>n_draft=16</td>
<td>1.36 tok/s</td>
<td>▁▂▃▄▅▆▇█▄▃▃▃▃▃▃▄▄▄▄▄▄▄▄▄▃▄▃▃▄▄▄▄▄▄▄▅▅▅▅▅▅▅▆▅▅▅▅▅▆▆</td>
</tr>
<tr><th colspan="3">Index: A B C</th></tr>
<tr>
<td>threshold</td>
<td>1.90 tok/s</td>
<td>▁▁▁▂▂▃▃▄▄▄▅▅▆▆▇▇█▅▅▅▆▆▆▇▆▆▆▆▆▅▅▅▅▅▅▅▅▅▅▄▄▄▄▄▄▄▄▄▄▄</td>
</tr>
<tr><td>n_draft=1</td>
<td>1.32 tok/s</td>
<td>▁▄▃▅▅▆▅▇▆▇▆▇▇▇▇▇▇█▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▆▇▇▆▆▆▆▆▆▆▆</td>
</tr>
<tr><td>n_draft=8</td>
<td>1.49 tok/s</td>
<td>▁▁▂▃▃▄▅▆▆▄▄▅▅▆▆▇▇█▆▆▆▆▇▇▆▆▆▆▇▇▇▆▆▆▆▆▆▆▅▅▅▅▅▄▄▄▄▄▄▄</td>
</tr>
<tr><td>n_draft=16</td>
<td>0.97 tok/s</td>
<td>▁▁▁▂▂▃▃▄▄▄▅▅▆▆▇▇█▅▅▅▅▆▆▆▄▅▅▅▅▅▆▄▅▅▄▄▄▄▄▃▃▃▃▃▂▂▂▂▂▂</td>
</tr>
<tr><th colspan="3">The digits of Pi are 3.14159</th></tr>
<tr>
<td>threshold</td>
<td>0.84 tok/s</td>
<td>▁▄▇▅█▆▅▅▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄</td>
</tr>
<tr><td>n_draft=1</td>
<td>0.85 tok/s</td>
<td>▁▅▅█▇▆▆▆▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅</td>
</tr>
<tr><td>n_draft=8</td>
<td>0.42 tok/s</td>
<td>▁▂▄▆█▅▄▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td>
</tr>
<tr><td>n_draft=16</td>
<td>0.27 tok/s</td>
<td>▁▂▄▆█▅▃▃▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td>
</tr>
</tbody></table>
<p>Note how, for example, n_draft=16 on the "Index: A B C" prompt has a strong start, but falls off hard in the later prose section as it overgenerates incorrect tokens.
Threshold decoding, meanwhile, is able to ramp up to take advantage of the easy alphabet tokens, and then ramp back down to not overgenerate on the harder prose section:</p>
<p><span>Index: A B C</span><span> D</span><span> E</span><span> F</span><span> G</span><span> H</span><span> I</span><span> J</span><span> K</span><span> L</span><span> M</span><span> N</span><span> O</span><span> P</span><span> Q</span><span> R</span><span> S</span><span> T</span><span> U</span><span> V</span><span> W</span><span> X</span><span> Y</span><span> Z</span><span>
</span><span>
</span><span>The</span><span> following</span><span> table</span><span> lists</span><span> the</span><span> number</span><span> of</span><span> times</span><span> each</span><span> of</span><span> the</span><span> following</span><span> words</span><span> appear</span><span> in</span><span> the</span><span> text</span><span> of</span><span> the</span><span> book</span><span>.</span><span>
</span><span>
</span><span>Word</span><span> Number</span>
</p>
<h3 id="Staged_speculative_decoding"><a href="#Staged_speculative_decoding">
  <img src="https://vgel.me/permalink.svg" alt="permalink for Staged_speculative_decoding">
</a>Staged speculative decoding</h3>
<p><a href="https://arxiv.org/abs/2308.04623">Staged speculative decoding</a> adds two improvements to vanilla speculative decoding:</p>
<p>The first is to restructure the draft batch as a tree, instead of a single generation.
This helps because longer draft batches on complex text can quickly diverge from the base model.
It can instead make more sense to do multiple, shorter drafts, branching off from each other, and then verify them all against the oracle model using a specially-crafted attention mask.
Generating multiple draft sequences lets you reuse prior tokens and sample the draft model in batches, further accelerating the process.</p>
<p><small>(You can think of regular speculative decoding as "deep" and using a tree as "wide", which begs the question of how you should prioritize deep v.s. wide for certain text. Maybe an adjustable parameter like in the threshold decoding section would be useful here as well?)</small></p>
<p>The second improvement is to speculatively decode the draft model as well—it's usually a Transformer after all.
This could be a yet-smaller Transformer (they recommend 15-20x smaller than the oracle model), or even a simple N-gram model.</p>
<h3 id="Guided_generation"><a href="#Guided_generation">
  <img src="https://vgel.me/permalink.svg" alt="permalink for Guided_generation">
</a>Guided generation</h3>
<p>Grammar-guided generation lets you constrain a model's output to follow some grammar, giving you output that guaranteed to match some grammar—such as JSON.
At first, this seems unrelated to speed—reliability is nice, but speed too?
That can't be possible!
But it is—let's dig into how it works to see why.</p>
<p>Imagine you're generating JSON with an LLM, and the generation so far is:</p>
<pre data-lang="json"><code data-lang="json"><span>{
</span><span>    </span><span>"key"</span><span>: 
</span></code></pre>
<p>GPT-4 could generate any of 100k+ tokens here, but only a few are actually valid: whitespace, an open bracket, a quote, a digit, <code>null</code>, etc.
During regular (non-guided) generation, you'd just hope the model had learned that properly and didn't generate syntactically invalid JSON.
During guided generation, however, the sampler <em>only</em> samples from those valid tokens, ignoring any others, even if the invalid tokens are more likely.<sup><a href="#openai-json-mode">5</a></sup></p>
<p>Even better, with libraries like <a href="https://github.com/outlines-dev/outlines">Outlines</a> or <a href="https://github.com/1rgs/jsonformer">jsonformer</a>, you can give the guided generation sampler a schema, and it will sample <em>within that schema</em>!
For example, if a key requires a digit, it will only sample digits after that key name.
Note how the returned object in this example exactly matches the Pydantic schema:</p>
<pre data-lang="python"><code data-lang="python"><span>...
</span><span>
</span><span>class </span><span>Weapon</span><span>(</span><span>str</span><span>, </span><span>Enum</span><span>):
</span><span>    sword </span><span>= </span><span>"sword"
</span><span>    axe </span><span>= </span><span>"axe"
</span><span>    mace </span><span>= </span><span>"mace"
</span><span>    spear </span><span>= </span><span>"spear"
</span><span>    bow </span><span>= </span><span>"bow"
</span><span>    crossbow </span><span>= </span><span>"crossbow"
</span><span>
</span><span>
</span><span>class </span><span>Armor</span><span>(</span><span>str</span><span>, </span><span>Enum</span><span>):
</span><span>    leather </span><span>= </span><span>"leather"
</span><span>    chainmail </span><span>= </span><span>"chainmail"
</span><span>    plate </span><span>= </span><span>"plate"
</span><span>
</span><span>class </span><span>Character</span><span>(</span><span>BaseModel</span><span>):
</span><span>    name: constr(</span><span>max_length</span><span>=</span><span>10</span><span>)
</span><span>    age: </span><span>int
</span><span>    armor: Armor
</span><span>    weapon: Weapon
</span><span>    strength: </span><span>int
</span><span>
</span><span>model </span><span>= </span><span>...
</span><span>generator </span><span>= </span><span>outlines.generate.json(model, Character, </span><span>max_tokens</span><span>=</span><span>100</span><span>)
</span><span>print</span><span>(generator(</span><span>"Give me a character description"</span><span>, </span><span>rng</span><span>=</span><span>...</span><span>))
</span><span># {
</span><span>#   "name": "clerame",
</span><span>#   "age": 7,
</span><span>#   "armor": "plate",
</span><span>#   "weapon": "mace",
</span><span>#   "strength": 4171
</span><span># }
</span></code></pre>
<p>This is great for reliability, but what does it have to do with speed?
Well, let's take a look at that response again.
If we compare it to what's required by the schema, barely any tokens are actually ambiguous—for most of the response, the model would only have one possible token to pick from.
In that case, the sampler can just pick that token, bypassing the model entirely!
Only a small fraction of the tokens (the ones highlighted in green) actually required a model forward call:</p>
<p>
{
  "name": "<span>clerame", </span>   <small>(4 ambiguous tokens: <span>cl</span> <span>er</span> <span>ame</span> <span>",\n</span>)</small>
  "age": <span>7, </span>            <small>(2 ambiguous tokens: <span>7</span> <span>,\n</span>)</small>
  "armor": "<span>plate</span>",     <small>(1 ambiguous token:  <span>plate</span>)</small>
  "weapon": "<span>m</span>ace",     <small>(1 ambiguous token:  <span>m</span>)</small>
  "strength": <span>4171 </span>     <small>(3 ambiguous tokens: <span>417</span> <span>1</span> <span>\n</span>)</small>
}
</p>
<p>Between the JSON grammar and the schema, we already know the first 7 tokens are <code>{\n "name": "</code>, so we can automatically add those into the context before the first model call.
Then after the model generates <code>clerame",\n</code>, we know the next tokens up until the model needs to actually generate the age (<code> "age": </code>), so we append those as well before calling the model again to generate <code>7,</code>.
We can keep going that way all the way down.
(We can even save a token by completing <code>m</code> as <code>"mace",</code> because no other weapon starts with <code>m</code>!)</p>
<p>The complete response was 41 tokens, but only 11 of them had to come from the model, the others were automatically inserted and only needed to be processed as prompt tokens, which are much faster.
This is a nice speedup, <em>and</em> more reliable to boot—a win-win.
If you need to generate structured data with LLMs, especially OSS LLMs that can use custom samplers, you should definitely be using a library like Outlines.</p>
<h3 id="Lookahead_decoding"><a href="#Lookahead_decoding">
  <img src="https://vgel.me/permalink.svg" alt="permalink for Lookahead_decoding">
</a>Lookahead decoding</h3>
<p><a href="https://lmsys.org/blog/2023-11-21-lookahead-decoding/">Lookahead decoding</a> is a new approach to speculative decoding that doesn't require a draft model.
Instead, the model itself is used in two branches: a lookahead branch, which predicts and extends candidate N-grams (short sequences of N tokens), and a verification branch, which verifies the candidates.
The lookahead branch is similar to the draft model in regular speculative decoding, and the verification branch has the same role as the oracle model.</p>
<p>But unlike regular speculative decoding, this is all done not just in a single model, but in a single model <em>call</em> using a specially-crafted attention mask:</p>
<p><a href="https://vgel.me/posts/faster-inference/lookahead.png"><img src="https://vgel.me/posts/faster-inference/lookahead.png" alt=""></a></p>
<p>I won't go too in depth because the <a href="https://lmsys.org/blog/2023-11-21-lookahead-decoding/">lmsys blog post</a> announcing lookahead decoding already has some nice animations (even if the explanation is somewhat dense).</p>
<h3 id="Prompt_lookup_decoding"><a href="#Prompt_lookup_decoding">
  <img src="https://vgel.me/permalink.svg" alt="permalink for Prompt_lookup_decoding">
</a>Prompt lookup decoding</h3>
<p><a href="https://twitter.com/apoorv_umang/status/1728831397153104255">Prompt lookup decoding</a> is another technique, where the draft model is replaced by simple string matching over the context.
They claim it works well for tasks like code editing or RAG where the output necessarily contains lots of verbatim copying from the input.
I assume it'd also work well in staged speculative decoding, to speculatively decode a draft model.</p>
<h2 id="Training_time_optimizations"><a href="#Training_time_optimizations">
  <img src="https://vgel.me/permalink.svg" alt="permalink for Training_time_optimizations">
</a>Training time optimizations</h2>
<p>There are a few optimizations that I'm going to speed through since they're not very relevant if you don't have the resources to pretrain a model with them from the start.</p>
<h3 id="Sparse_attention"><a href="#Sparse_attention">
  <img src="https://vgel.me/permalink.svg" alt="permalink for Sparse_attention">
</a>Sparse attention</h3>
<p>Attention is algorithmically slow because it's quadratic: as the sequence grows in length, each of the N tokens needs to attend to each of the N tokens.
Sparse attention attempts to remedy this by calculating less attention.
For example, Mistral 7B uses sliding window attention, where tokens in some layers can only attend to nearby tokens.
<a href="https://arxiv.org/abs/2004.05150">Longformer</a> also explored some interesting sparse attention patterns, like giving all tokens access to specific positions, dilating the sliding window, using different size windows on different layers, and other tricks.
(Longformer predates Mistral, but as far as I can tell Mistral didn't use all the tricks Longformer did—I'm not sure why.)</p>
<p>Sometimes this kind of attention can be finetuned in or bolted on without tuning after the fact, but for the best performance it needs to be trained into the model from the start, like Mistral did.</p>
<h3 id="Non-Transformer_architectures"><a href="#Non-Transformer_architectures">
  <img src="https://vgel.me/permalink.svg" alt="permalink for Non-Transformer_architectures">
</a>Non-Transformer architectures</h3>
<p>Recently there's been a renewed surge of interest in non-Transformer LLMs.
If you're new to the field you may not be familiar with RNNs/LSTMs, but they were the dominant sequence modeling architecture before Attention is All You Need was published and Transformers took off.
Unlike Transformers where the whole context is available to the model at once, RNNs do a linear scan over the sequence, building up a hidden state that models what has come before.
<small>(There are also reversed and bidirectional RNNs, it's a whole thing.)</small></p>
<p>They were outmatched by Transformers due to difficulty scaling them and problems like forgetting, but some recent papers have tried to bring them back, or invent new sub-quadratic architectures that can finally dethrone Transformers.
These include <a href="https://arxiv.org/abs/2305.13048">RWKV</a> (new type of RNN), <a href="https://arxiv.org/abs/2312.00752">Mamba</a> (state-space model), <a href="https://arxiv.org/abs/2302.10866">Hyena</a> (convolutional), and <a href="https://proceedings.neurips.cc/paper_files/paper/2022/file/47e288629a6996a17ce50b90a056a0e1-Paper-Conference.pdf">Recurrent Memory Transformers</a> (use a Transformer for segments, then special memory tokens for global context).
So far the biggest and best models are still Transformers, but that might not be true in the future!</p>
<h2 id="Conclusion"><a href="#Conclusion">
  <img src="https://vgel.me/permalink.svg" alt="permalink for Conclusion">
</a>Conclusion</h2>
<p>Thanks for reading!</p>
<p>Unfortunately I didn't get to cover everything I wanted to in this post, since it's already quite long—I didn't touch on structured sparsity, Mixture of Experts, activation quantization, static vs dynamic quantization, or lots of other great topics.
However, I think the post is a good survey of different areas of LLM optimization.
LLMs are currently slow and hard to run, but the situation is improving all the time—Lookahead Decoding was released while I was writing this post!
It seems likely that in a few years, between better hardware, better training methods, better quantization, more inference optimizations, and the continuing hard work of the open source community, we could be running models than handily beat GPT-4 on consumer hardware, and these techniques and more will be instrumental to making that happen.
(Of course, GPT-5 will probably be out by then... but always upwards!)</p>
<p>Hopefully you found the post useful and/or entertaining.
If you enjoyed it, you may also enjoy:</p>
<ul>
<li><a href="https://vgel.me/posts">My other blog posts</a>, such as <a href="https://vgel.me/posts/handmade-transformer/">I made a transformer by hand (no training!)</a>, <a href="https://vgel.me/posts/tools-not-needed/">GPT-3 will ignore tools when it disagrees with them</a>, <a href="https://vgel.me/posts/gpt4-javascript">Does GPT-4 think better in Javascript?</a> and <a href="https://vgel.me/posts/adversarial-training-data">I'm worried about adversarial training data</a></li>
<li><a href="https://vgel.me/">My other projects and writing</a></li>
<li>My <a href="https://twitter.com/voooooogel/">Twitter</a>, where I post about new blog posts, smaller AI-related thoughts (e.g. <a href="https://twitter.com/voooooogel/status/1730726744314069190">1</a>, <a href="https://twitter.com/voooooogel/status/1733590921605001677">2</a>), whatever fiction I've been reading, and other things.</li>
</ul>
<p>If you have thoughts about this post, please feel free to <a href="https://vgel.me/contact">get in touch</a>!
I love hearing from people who read my posts.</p>
<h3 id="Thanks"><a href="#Thanks">
  <img src="https://vgel.me/permalink.svg" alt="permalink for Thanks">
</a>Thanks</h3>
<p>As always, thanks to everyone who contributed to and reviewed this post:</p>
<ul>
<li><a href="https://twitter.com/wjessup">@wjessup</a> (http://willjessup.com/) for a <em>very</em> thorough review of the draft.</li>
<li><a href="https://twitter.com/tekknolagi">@tekknolagi</a> (https://bernsteinbear.com/) for reviewing and commenting.</li>
<li><a href="https://twitter.com/MF_FOOM">@MF_FOOM</a> for reviewing and commenting (and helping me rent some GPUs for an experiment 🫡 even though the experiment didn't work rip)</li>
<li>@laikhtewari on Discord for great topic suggestions</li>
<li>Everyone on Twitter who liked and commented on the various posts I made while working on this! Really helps.</li>
</ul>
<hr>

<!-- -->

<!-- -->

<!-- -->

<!-- -->



    <ul>
      
        <li><strong>Previous entry:</strong> <a href="https://vgel.me/posts/handmade-transformer/">I made a transformer by hand (no training!)</a></li>
      
      
    </ul>
</div></div>]]></description>
        </item>
    </channel>
</rss>