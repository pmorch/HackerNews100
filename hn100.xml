(ignoring known css parsing error)
<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0">
    <channel>
        <title>HN100 - Readable Contents</title>
        <link>https://hn.algolia.com/api/v1/search_by_date?tags=%28story,poll%29&amp;numericFilters=points%3E100</link>
        <description>Uses Readability to add bodies to the RSS feed</description>
        <lastBuildDate>Mon, 11 Aug 2025 14:30:04 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>en</language>
        <item>
            <title><![CDATA[Wikimedia Foundation Challenges UK Online Safety Act Regulations (160 pts)]]></title>
            <link>https://wikimediafoundation.org/news/2025/08/11/wikimedia-foundation-challenges-uk-online-safety-act-regulations/</link>
            <guid>44863487</guid>
            <pubDate>Mon, 11 Aug 2025 12:38:15 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://wikimediafoundation.org/news/2025/08/11/wikimedia-foundation-challenges-uk-online-safety-act-regulations/">https://wikimediafoundation.org/news/2025/08/11/wikimedia-foundation-challenges-uk-online-safety-act-regulations/</a>, See on <a href="https://news.ycombinator.com/item?id=44863487">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
<p>UPDATE: On Monday, 11 August, the High Court of Justice dismissed the Wikimedia Foundation’s <a href="https://medium.com/wikimedia-policy/wikipedias-nonprofit-host-brings-legal-challenge-to-new-online-safety-act-osa-regulations-0f9153102f29" target="_blank" rel="noreferrer noopener">challenge to the UK’s Online Safety Act</a> (OSA) Categorisation Regulations. While the decision does not provide the immediate legal protections for Wikipedia that we hoped for, the Court’s ruling emphasized the responsibility of <a href="https://www.ofcom.org.uk/" target="_blank" rel="noreferrer noopener">Ofcom</a> and the UK government to ensure Wikipedia is protected as the OSA is implemented.&nbsp;</p>



<p>The judge recognized the “significant value” of Wikipedia, its safety for users, as well as the damages that wrongly-assigned OSA categorisations and duties could have on the human rights of Wikipedia’s volunteer contributors. The Court stressed that this ruling “does not give Ofcom and the Secretary of State a green light to implement a regime that would significantly impede Wikipedia’s operations”,&nbsp; and indicated they could face legal repercussions if they fail to protect Wikipedia and the rights of its users.&nbsp;In order to achieve that outcome, he suggested that Ofcom may need to find a particularly flexible interpretation of the rules in question, or that the rules themselves may need amendment in Parliament.</p>



<p>If the ruling stands, the first categorization decisions from Ofcom are expected this summer. The Foundation will continue to seek solutions to protect Wikipedia and the rights of its users as the OSA continues to be implemented.</p>







<hr>



<p>17 July 2025&nbsp;— Next week, on 22 and 23 July 2025, the High Court of Justice in London will hear the Wikimedia Foundation’s&nbsp;<a href="https://medium.com/wikimedia-policy/wikipedias-nonprofit-host-brings-legal-challenge-to-new-online-safety-act-osa-regulations-0f9153102f29" target="_blank" rel="noreferrer noopener">legal challenge</a>&nbsp;to the&nbsp;<a href="https://www.legislation.gov.uk/uksi/2025/226/regulation/3/made" target="_blank" rel="noreferrer noopener">Categorisation Regulations</a>&nbsp;of the United Kingdom (UK)’s Online Safety Act (OSA).<strong>&nbsp;</strong></p>



<p>The Wikimedia Foundation, the non-profit that operates Wikipedia and other <a href="https://wikimediafoundation.org/our-work/wikimedia-projects/" target="_blank" rel="noreferrer noopener">Wikimedia projects</a>, announced its legal challenge earlier this year, arguing that the regulations endanger Wikipedia and the global community of volunteer contributors who create the information on the site.</p>



<blockquote>
<p>“The Court has an opportunity in this case to set a global precedent for protecting public interest projects online,” said <a href="https://wikimediafoundation.org/profile/stephen-laporte/" target="_blank" rel="noreferrer noopener">Stephen LaPorte</a>, General Counsel at the Wikimedia Foundation. “Wikipedia is the backbone of knowledge on the internet. It’s the only top-ten website operated by a non-profit and one of the highest-quality datasets used in training Large Language Models (LLMs). We trust the Court will protect Wikipedia—a vital encyclopedic resource—from rules crafted for the internet’s riskiest commercial sites and, in doing so, safeguard the open internet for everyone”.</p>
</blockquote>



<p>Information on Wikipedia is written and curated by a global community of nearly 260,000 volunteer contributors. These volunteers set and enforce policies to ensure that information on the platform is fact-based, neutral, and attributed to reliable sources. Over the last 25 years, this human-centered content moderation model has established Wikipedia as an unparalleled resource for reliable information in over 300 languages; its 65 million articles are viewed more than 15 billion times per month worldwide.</p>



<p>The Wikimedia Foundation shares the UK government’s commitment to promoting online environments where everyone can safely participate. The organization is not bringing a general challenge to the OSA as a whole, nor to the existence of the Category 1 duties themselves. Rather, the legal challenge focuses solely on the new Categorisation Regulations that risk imposing Category 1 duties (the OSA’s most stringent obligations) on Wikipedia.</p>



<p>If enforced on Wikipedia, Category 1 demands would undermine the privacy and safety of Wikipedia’s volunteer contributors, expose the encyclopedia to manipulation and vandalism, and divert essential resources from protecting people and improving Wikipedia, one of the world’s most trusted and widely used <a href="https://wikimediafoundation.org/news/2025/02/12/wikipedia-recognized-as-a-digital-public-good/" target="_blank" rel="noreferrer noopener">digital public goods</a>.</p>



<p>For example, the Foundation would be required to verify the identity of many Wikipedia contributors, undermining the privacy that is central to keeping Wikipedia volunteers safe. In addition to being exceptionally burdensome, this requirement—which is just one of several Category 1 demands—could expose contributors to data breaches, stalking, lawsuits, or even imprisonment by authoritarian regimes. Additional details about the concerning impacts of the Category 1 duties on Wikipedia are available in this <a href="https://medium.com/wikimedia-policy/wikipedias-nonprofit-host-brings-legal-challenge-to-new-online-safety-act-osa-regulations-0f9153102f29" target="_blank" rel="noreferrer noopener">blog post</a>.</p>
</div><div>
<p>The Wikimedia Foundation will be joined in the case by longtime UK-based volunteer Wikipedia contributor <a href="https://en.wikipedia.org/wiki/User:Zzuuzz" target="_blank" rel="noreferrer noopener">User:Zzuuzz</a> as a joint claimant. Their voluntary participation highlights what is at stake in this case for the everyday people who read and contribute to Wikimedia projects. It presents the perspective of a Wikipedia volunteer on how the OSA Categorisation Regulations directly threaten the ability of contributors to participate in knowledge sharing on Wikipedia, as well as compromising their rights to privacy, safety, free speech, and association.&nbsp;</p>



<p>The legal challenge is the first to be issued against the OSA’s Categorisation Regulations, as well as the first with a volunteer Wikipedia editor participating as a joint claimant. It follows years of dialogue with regulators and policymakers, in which the Foundation expressed its concerns, as well as <a href="https://www.theyworkforyou.com/lords/?id=2025-02-24a.1524.0&amp;s=wikipedia" target="_blank" rel="noreferrer noopener">warnings from the UK Parliament</a> and <a href="https://wikimedia.org.uk/2023/06/online-safety-bill-open-letter/" target="_blank" rel="noreferrer noopener">civil society</a>.</p>



<blockquote>
<p>“Our concerns on the looming threats to Wikipedia and its contributors remain unaddressed”, said Phil Bradley-Schmieg, Lead Counsel at the Wikimedia Foundation. “We are taking action now to protect Wikipedia’s volunteers, as well as the global accessibility and integrity of free knowledge. We call on the Court to defend the privacy and safety of Wikipedia’s volunteer contributors from flawed legislation”.</p>
</blockquote>



<p>Wikipedia and other Wikimedia projects are safe and important resources through which people across the UK—and the wider world—learn, share knowledge, collaborate, and gain media literacy. Thousands of volunteer Wikipedia contributors are based in the UK, and Wikipedia hosts content from cultural institutions such as the British Library and Wellcome Collection. Content on Wikipedia and other Wikimedia projects was viewed 776 million times last month in the UK alone. Moreover, Wikipedia is used to preserve and promote cultural heritage in the UK, including Indigenous and minority languages such as Welsh. The <a href="https://cy.wikipedia.org/wiki/Hafan?wprov=wppw2" target="_blank" rel="noreferrer noopener">Welsh language version of Wikipedia</a> is the single most popular Welsh language website in the world and is an <a href="https://digitalanddata.blog.gov.wales/2017/08/07/using-technology-to-promote-welsh-language-wikipedia/" target="_blank" rel="noreferrer noopener">official component of the curriculum</a> in Wales.</p>



<p>The hearings at the Royal Courts of Justice (Administrative courts of the King’s Bench Division) in London, are expected to be open to the public. The case reference is AC-2025-LON-001365, and the courtroom location will be announced <a href="https://www.court-tribunal-hearings.service.gov.uk/summary-of-publications?locationId=109" target="_blank" rel="noreferrer noopener">here</a> shortly before the hearing. The Court will issue its decision following the hearing, though the exact timing of the announcement is not known.</p>



<hr>







<p>The personal identity of User:Zzuuzz, the volunteer joining the challenge, will remain confidential and protected by the law and the Foundation.&nbsp;</p>



<p>For media inquiries, please contact <a href="mailto:press@wikimedia.org" target="_blank" rel="noreferrer noopener">press@wikimedia.org</a>.&nbsp;</p>



<p><a href="https://mailchi.mp/wikimedia/global-advocacy-policy-newsletter" target="_blank" rel="noreferrer noopener">Subscribe to our Global Advocacy newsletter</a> to stay informed on this case and other global advocacy updates from the Wikimedia Foundation.&nbsp;</p>



<h2><strong>About the Wikimedia Foundation</strong></h2>



<p>The <a href="https://wikimediafoundation.org/" target="_blank" rel="noreferrer noopener">Wikimedia Foundation</a> is the nonprofit organization that operates Wikipedia and other Wikimedia free knowledge projects. Our vision is a world in which every single human can freely share in the sum of all knowledge. We believe that everyone has the potential to contribute something to our shared knowledge and that everyone should be able to access that knowledge freely. We host Wikipedia and the Wikimedia projects; build software experiences for reading, contributing, and sharing Wikimedia content; support the volunteer communities and partners who make Wikimedia possible. The Wikimedia Foundation is a United States 501(c)(3) tax-exempt organization with offices in San Francisco, California, USA.</p>
</div><div>

			<p><a alt="" tabindex="-1" href="https://wikimediafoundation.org/news/2025/08/11/wikimedia-foundation-challenges-uk-online-safety-act-regulations/">
			<img loading="lazy" decoding="async" width="800" height="600" src="https://wikimediafoundation.org/wp-content/uploads/2025/07/1024px-Royal_Courts_of_Justice_2019-1.jpg?w=800&amp;h=600&amp;crop=1" alt="" srcset="https://wikimediafoundation.org/wp-content/uploads/2025/07/1024px-Royal_Courts_of_Justice_2019-1.jpg?resize=400,300 400w, https://wikimediafoundation.org/wp-content/uploads/2025/07/1024px-Royal_Courts_of_Justice_2019-1.jpg?resize=800,600 800w" sizes="auto, (max-width: 800px) 100vw, 800px">		</a></p><div>
					<h3>
				<a href="https://wikimediafoundation.org/news/2025/08/11/wikimedia-foundation-challenges-uk-online-safety-act-regulations/">
					Wikimedia Foundation Challenges UK Online Safety Act Regulations				</a>
			</h3>
		
					
		
					<p>UPDATE: On Monday, 11 August, the High Court of Justice dismissed the Wikimedia Foundation’s challenge to the UK’s Online Safety Act (OSA) Categorisation Regulations. While the decision does not provide the immediate legal protections for Wikipedia that we hoped for, the Court’s ruling emphasized the responsibility of Ofcom and the UK government to ensure Wikipedia….</p>
		
		

		<p><a href="https://wikimediafoundation.org/news/2025/08/11/wikimedia-foundation-challenges-uk-online-safety-act-regulations/" aria-label="Read more about Wikimedia Foundation Challenges UK Online Safety Act Regulations">
			Read more		</a>
	</p></div>
</div><div>

			<p><a alt="" tabindex="-1" href="https://wikimediafoundation.org/news/2025/07/09/china-block-wikimedia-wipo/">
			<img loading="lazy" decoding="async" width="800" height="600" src="https://wikimediafoundation.org/wp-content/uploads/2025/07/1024px-069_United_Nations_Geneva_World_Intellectual_Property_Organization_WIPO_-_Creative_Commons-1.jpg?w=800&amp;h=600&amp;crop=1" alt="" srcset="https://wikimediafoundation.org/wp-content/uploads/2025/07/1024px-069_United_Nations_Geneva_World_Intellectual_Property_Organization_WIPO_-_Creative_Commons-1.jpg?resize=400,300 400w, https://wikimediafoundation.org/wp-content/uploads/2025/07/1024px-069_United_Nations_Geneva_World_Intellectual_Property_Organization_WIPO_-_Creative_Commons-1.jpg?resize=800,600 800w" sizes="auto, (max-width: 800px) 100vw, 800px">		</a></p><div>
					<h3>
				<a href="https://wikimediafoundation.org/news/2025/07/09/china-block-wikimedia-wipo/">
					For fifth time, China blocks Wikimedia Foundation as permanent observer to the World Intellectual Property Organization (WIPO)				</a>
			</h3>
		
					
		
					<p>On 9 July 2025, the Wikimedia Foundation was denied permanent observer status at the World Intellectual Property Organization (WIPO).</p>
		
		

		<p><a href="https://wikimediafoundation.org/news/2025/07/09/china-block-wikimedia-wipo/" aria-label="Read more about For fifth time, China blocks Wikimedia Foundation as permanent observer to the World Intellectual Property Organization (WIPO)">
			Read more		</a>
	</p></div>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[OpenSSH Post-Quantum Cryptography (149 pts)]]></title>
            <link>https://www.openssh.com/pq.html</link>
            <guid>44863242</guid>
            <pubDate>Mon, 11 Aug 2025 12:01:58 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.openssh.com/pq.html">https://www.openssh.com/pq.html</a>, See on <a href="https://news.ycombinator.com/item?id=44863242">Hacker News</a></p>
<div id="readability-page-1" class="page">
<hr>

<p>
OpenSSH supports a number of cryptographic key agreement algorithms
considered to be safe against attacks from quantum computers. 
We recommend that all SSH connections use these algorithms.
</p>

<p>
OpenSSH has offered post-quantum key agreement (<i>KexAlgorithms</i>)
by default since release 9.0 (2022), initially via the
<tt>sntrup761x25519-sha512</tt> algorithm. More recently, in OpenSSH 9.9,
we have added a second post-quantum key agreement <tt>mlkem768x25519-sha256</tt>
and it was made the default scheme in OpenSSH 10.0.
</p>

<p>
To encourage migration to these stronger algorithms, OpenSSH 10.1 will warn
the user when a non post-quantum key agreement scheme is selected. These
warnings are displayed by default but may be disabled via the
<i>WarnWeakCrypto</i> option in
<a href="https://man.openbsd.org/ssh_config.5">ssh_config(5)</a>.
</p>

<h3>Background</h3>

<p>
A quantum computer (QC) is a device capable of performing computations
with information encoded as quantum states. Such a device could quickly solve
particular problems that are intractable for existing "classical" computers.
</p>

<p>
The mathematics that underpin a number of cryptographic algorithms
are among the problems that quantum computers are believed to be able to
effectively solve. This means that a sufficiently-powerful quantum computer
(a.k.a a "cryptographically-relevant" quantum computer) will be able to break
them. Most affected is the cryptography used for key agreement and digital
signatures, both of which play important roles in SSH.
</p>

<p>
Fortunately, quantum computers of sufficient power to break cryptography
have not been invented yet. Estimates for when a cryptographically-relevant
quantum computer will arrive, based on the rate of progress in the field,
range from 5-20 years, with many observers expecting them to arrive
in the mid-2030s.
</p>

<p>
The entire privacy of an SSH connection depends on cryptographic key agreement.
If an attacker can break the key agreement then they are able to decrypt and
view the entire session. The attacker need not perform this attack in real
time; they may collect encrypted SSH sessions now and then decrypt them later
once they have access to a quantum computer.
This is referred to as a "store now, decrypt later" attack (also as
"harvest now, decrypt later").
</p>

<p>
OpenSSH supports post-quantum cryptography to protect user traffic against
this attack.
</p>

<h2>FAQ</h2>

<dl>
<dt><b>I received a warning from ssh that directed me to this page. What should I do?</b></dt>
<dd>
As mentioned above, OpenSSH 10.1 started warning users when connections use
cryptography that is not safe against quantum computers. If you received such
a warning, it means that the server you connected to did not offer one of the
two post-quantum key agreement algorithms that are being standardised for the
SSH protocol:
<a href="https://datatracker.ietf.org/doc/draft-ietf-sshm-mlkem-hybrid-kex/"><tt>mlkem768x25519-sha256</tt></a> and
<a href="https://datatracker.ietf.org/doc/draft-josefsson-ntruprime-ssh/"><tt>sntrup761x25519-sha512</tt></a>
<p>
The ideal solution is to update the server to use an SSH implementation that
supports at least one of these. OpenSSH versions 9.0 and greater support
</p><tt>sntrup761x25519-sha512</tt> and versions 9.9 and greater support
<tt>mlkem768x25519-sha256</tt>. If your server is already running one of these
versions, then check whether the <i>KexAlgorithms</i> option has disabled
their use.
<p>
If you are unable to update the server and/or you prefer to accept the risk
of continuing to use quantum-unsafe cryptography then the warning may be
silenced via the 
<i>WarnWeakCrypto</i> option in
<a href="https://man.openbsd.org/ssh_config.5">ssh_config(5)</a>.
We recommend doing this selectively, for example:
</p><pre>Match host unsafe.example.com
    WarnWeakCrypto no
</pre>
</dd>
<dt><b>Quantum computers don't exist yet, why go to all this trouble?</b></dt>
<dd>
Because of the "store now, decrypt later" attack mentioned above. Traffic
sent today is at risk of decryption unless post-quantum key agreement is used.
</dd>
<dt><b>What about signature algorithms? You said they were at risk too</b></dt>
<dd>
Yes, most currently-used signature algorithms (including RSA and ECDSA) can be
broken by a quantum computer. However, there is no risk to existing traffic
in this situation (i.e. there is no analogous "store now, decrypt later").
The only urgency for signature algorithms is ensuring that all classical
signature keys are retired in advance of cryptographically-relevant computers
becoming a reality. OpenSSH will add support for post-quantum signature
algorithms in the future.
</dd>
<dt><b>I don't believe we'll ever get quantum computers. This is a waste of time</b></dt>
<dd>
Some people consider the task of scaling existing quantum computers up to the
point where they can tackle cryptographic problems to be practically
insurmountable. This is a possibilty. However, it appears that most of the
barriers to a cryptographically-relevant quantum computer are engineering
challenges rather than underlying physics.
<p>
If we're right about quantum computers being practical, then we will have
protected vast quantities of user data. If we're wrong about it, then all
we'll have done is moved to cryptographic algorithms with stronger mathematical
underpinnings.
</p></dd>
<dt><b>These post-quantum algorithms are new, are we sure they aren't broken?</b></dt>
<dd>
We're wary of this too. Though post-quantum key agreement algorithms have
received a lot of concerted cryptographic attention over the last few years,
it's possible that new attacks might be found.
<p>
To defend against this happening we have selected post-quantum algorithms with
good safety margins, this means that even if they turn out to be weaker than
expected they are still likely to be strong enough to be considered fit for
purpose.
</p><p>
Additionally, all the post-quantum algorithms implemented by OpenSSH are
"hybrids" that combine a post-quantum algorithm with a classical
algorithm. For example </p><tt>mlkem768x25519-sha256</tt> combines ML-KEM, a
post-quantum key agreement scheme, with ECDH/x25519, a classical key agreement
algorithm that was formerly OpenSSH's preferred default. This ensures that the
combined, hybrid algorithm is <i>no worse</i> than the previous best
classical algorithm, even if the post-quantum algorithm turns out to be
completely broken by future cryptanalysis.
</dd>
</dl>

</div>]]></description>
        </item>
        <item>
            <title><![CDATA[GPT-OSS-120B runs on just 8GB VRAM & 64GB+ system RAM (126 pts)]]></title>
            <link>https://old.reddit.com/r/LocalLLaMA/comments/1mke7ef/120b_runs_awesome_on_just_8gb_vram/</link>
            <guid>44862542</guid>
            <pubDate>Mon, 11 Aug 2025 10:02:08 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://old.reddit.com/r/LocalLLaMA/comments/1mke7ef/120b_runs_awesome_on_just_8gb_vram/">https://old.reddit.com/r/LocalLLaMA/comments/1mke7ef/120b_runs_awesome_on_just_8gb_vram/</a>, See on <a href="https://news.ycombinator.com/item?id=44862542">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>Here is the thing, the expert layers run amazing on CPU  (<del>~17T/s</del> 25T/s on a 14900K) and you can force that with this new llama-cpp option: --cpu-moe .</p>

<p>You can offload just the attention layers to GPU  (requiring about 5 to 8GB of VRAM) for fast prefill.</p>

<ul>
<li>KV cache for the sequence</li>
<li>Attention weights &amp; activations</li>
<li>Routing tables</li>
<li>LayerNorms and other “non-expert” parameters</li>
</ul>

<p>No giant MLP weights are resident on the GPU, so memory use stays low.</p>

<p>This yields an amazing snappy system for a 120B model!  Even something like a 3060Ti would be amazing! GPU with BF16 support would be best (RTX3000+) because all layers except the MOE layers (which are mxfp4) are BF16.</p>

<p>64GB of system ram would be minimum, and 96GB would be ideal. (linux uses mmap so will keep the 'hot' experts in memory even if the whole model doesn't fit in memory)</p>

<blockquote>
<p>prompt eval time = 28044.75 ms / 3440 tokens ( 8.15 ms per token, 122.66 tokens per second)</p>

<p>eval time = 5433.28 ms / 98 tokens ( 55.44 ms per token, 18.04 tokens per second)</p>
</blockquote>

<p>with 5GB of vram usage!</p>

<p>Honestly, I think this is the biggest win of this 120B model. This seems an amazing model to run fast for GPU-poor people. You can do this on a 3060Ti and 64GB of system ram is cheap.</p>

<p>edit: with this latest PR: <a href="https://github.com/ggml-org/llama.cpp/pull/15157">https://github.com/ggml-org/llama.cpp/pull/15157</a></p>

<pre><code>~/build/llama.cpp/build-cuda/bin/llama-server \
    -m $LLAMA_MODEL_DIR/gpt-oss-120b-mxfp4-00001-of-00003.gguf \
    --n-cpu-moe 36 \    #this model has 36 MOE blocks. So cpu-moe 36 means all moe are running on the CPU. You can adjust this to move some MOE to the GPU, but it doesn't even make things that much faster.
    --n-gpu-layers 999 \   #everything else on the GPU, about 8GB
    -c 0 -fa \   #max context (128k), flash attention
    --jinja --reasoning-format none \
    --host 0.0.0.0 --port 8502 --api-key "dummy" \



prompt eval time =   94593.62 ms / 12717 tokens (    7.44 ms per token,   134.44 tokens per second)
       eval time =   76741.17 ms /  1966 tokens (   39.03 ms per token,    25.62 tokens per second)
</code></pre>

<p>Hitting above 25T/s with only 8GB VRAM use!</p>

<p>Compared to running 8 MOE layers also on the GPU (about 22GB VRAM used total) :</p>

<pre><code>~/build/llama.cpp/build-cuda/bin/llama-server \
    -m $LLAMA_MODEL_DIR/gpt-oss-120b-mxfp4-00001-of-00003.gguf \
    --n-cpu-moe 28 \
    --n-gpu-layers 999 \
    -c 0 -fa \
    --jinja --reasoning-format none \
    --host 0.0.0.0 --port 8502 --api-key "dummy" \

prompt eval time =   78003.66 ms / 12715 tokens (    6.13 ms per token,   163.01 tokens per second)
       eval time =   70376.61 ms /  2169 tokens (   32.45 ms per token,    30.82 tokens per second)
</code></pre>

<p>Honestly, this 120B is the perfect architecture for running at home on consumer hardware. Somebody did some smart thinking when designing all of this!</p>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Faster substring search with SIMD in Zig (111 pts)]]></title>
            <link>https://aarol.dev/posts/zig-simd-substr/</link>
            <guid>44862414</guid>
            <pubDate>Mon, 11 Aug 2025 09:41:13 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://aarol.dev/posts/zig-simd-substr/">https://aarol.dev/posts/zig-simd-substr/</a>, See on <a href="https://news.ycombinator.com/item?id=44862414">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p><span>Published 10.08.2025</span></p><p>I’ve been learning a lot about low-level programming languages lately, and for a long time there has been one thing that has interested me: SIMD (or ‘single instruction, multiple data’) code. I’ve seen a lot of articles about having massive performance gains by utilizing SIMD and wanted to learn how to do it myself.</p><p>This article is a journey into implementing ~60% faster substring searching compared to Zig’s <code>std.mem.indexOf</code> using a SIMD-friendly algorithm.</p><h2 id="baseline">Baseline
<a href="#baseline">#</a></h2><p>This is the baseline function that we will be comparing against:</p><div><pre tabindex="0"><code data-lang="zig"><span><span><span>fn</span><span> </span><span>find_substr</span><span>(</span><span>needle</span><span>:</span><span> </span><span>[]</span><span>const</span><span> </span><span>u8</span><span>,</span><span> </span><span>haystack</span><span>:</span><span> </span><span>[]</span><span>const</span><span> </span><span>u8</span><span>)</span><span> </span><span>?</span><span>usize</span><span> </span><span>{</span><span>
</span></span></span><span><span><span>    </span><span>return</span><span> </span><span>std</span><span>.</span><span>mem</span><span>.</span><span>indexOf</span><span>(</span><span>u8</span><span>,</span><span> </span><span>haystack</span><span>,</span><span> </span><span>needle</span><span>);</span><span>
</span></span></span><span><span><span></span><span>}</span><span>
</span></span></span></code></pre></div><p>It’s the closest thing to a substring search function from Zig’s standard library. It returns the first index of a subsequence – or <code>null</code> if not found.</p><h2 id="simd-algorithm">SIMD algorithm
<a href="#simd-algorithm">#</a></h2><p>This algorithm is taken directly from Wojciech Muła’s fantastic article <a href="http://0x80.pl/notesen/2016-11-28-simd-strfind.html#algorithm-1-generic-simd">SIMD-friendly algorithms for substring searching</a>, which seems to have the best algorithms for finding substrings in a large body of text.</p><p>Here’s how the algorithm works: say that we want to find the index of the word “blue” (the <code>needle</code>) in “It was a beautiful, bounteous, blue day” (the <code>haystack</code>). First, we extract the first and last character of the <code>needle</code> (‘b’ and ’e’) and store them in a variable.</p><p>Then we will loop through all of the characters in <code>haystack</code>, loading the next 32 characters (bytes) from memory into a SIMD register and comparing each character (byte) in the register with ‘b’. This will result in a mask containing 32 bytes, <code>1</code> if the character is ‘b’ and <code>0</code> in all other cases.</p><p>We will do the same with the last character, but load the characters with an offset (<code>needle.len - 1</code>).</p><blockquote><p>Without the offset, any match that starts in one 32‑byte chunk and ends in the next would be missed. With this method, we can also check for <code>needles</code> that are longer than 32 characters.</p></blockquote><p>The result will be two bit masks, <code>First</code> and <code>Last</code>, where we can use bit-wise AND (<code>Result = First &amp; Last</code>) to figure out potential substring occurrences.</p><p><code>Result</code> will be <code>1</code> only when there is a ‘b’ at index <code>i</code> followed by an ’e’ at index <code>i+3</code>. We still need to check if those positions actually contain the value “blue”, but this still dramatically reduces the number of checks (= individual memory accesses) that are necessary. We’ll see how this works in practice in the next section.</p><h2 id="implementation-in-zig">Implementation in Zig
<a href="#implementation-in-zig">#</a></h2><p>First, to properly use SIMD, let’s assume that the CPU supports AVX2 (Advanced Vector Extensions 2) and has 256-bit wide registers.</p><blockquote><p>All desktop processors less than 10 years old support AVX2, with newer ones also supporting AVX-512 with 512-bit wide registers.</p></blockquote><p>This allows us to use Zig’s <a href="https://ziglang.org/documentation/0.14.1/#Vectors">@Vector</a> function to make a type:</p><div><pre tabindex="0"><code data-lang="zig"><span><span><span>const</span><span> </span><span>Block</span><span> </span><span>=</span><span> </span><span>@Vector</span><span>(</span><span>32</span><span>,</span><span> </span><span>u8</span><span>);</span><span> </span><span>// number of elements, element type (32*8=256)
</span></span></span></code></pre></div><p>By using <code>Block</code>, we are telling the compiler that the operations on this datatype should use SIMD instructions where possible.</p><p>Next, we take the first and last letters of the search word (’needle’) and load them into two SIMD registers, so that every byte of the register is filled with the character. This is handled by another built-in function, <code>@splat</code>:</p><div><pre tabindex="0"><code data-lang="zig"><span><span><span>const</span><span> </span><span>first_letter</span><span>:</span><span> </span><span>Block</span><span> </span><span>=</span><span> </span><span>@splat</span><span>(</span><span>needle</span><span>[</span><span>0</span><span>]);</span><span>
</span></span></span><span><span><span></span><span>const</span><span> </span><span>last_letter</span><span>:</span><span> </span><span>Block</span><span> </span><span>=</span><span> </span><span>@splat</span><span>(</span><span>needle</span><span>[</span><span>needle</span><span>.</span><span>len</span><span> </span><span>-</span><span> </span><span>1</span><span>]);</span><span>
</span></span></span></code></pre></div><p>In the main loop, we check that there is enough characters left in <code>haystack</code> so that we can read the next <code>32 + needle.len</code> characters. Inside the block, we load the blocks that we’re going to compare <code>first_letter</code> and <code>last_letter</code> with.</p><div><pre tabindex="0"><code data-lang="zig"><span><span><span>const</span><span> </span><span>n</span><span> </span><span>=</span><span> </span><span>haystack</span><span>.</span><span>len</span><span>;</span><span>
</span></span></span><span><span><span></span><span>const</span><span> </span><span>k</span><span> </span><span>=</span><span> </span><span>needle</span><span>.</span><span>len</span><span>;</span><span>
</span></span></span><span><span><span></span><span>var</span><span> </span><span>i</span><span>:</span><span> </span><span>usize</span><span> </span><span>=</span><span> </span><span>0</span><span>;</span><span>
</span></span></span><span><span><span></span><span>while</span><span> </span><span>(</span><span>i</span><span> </span><span>+</span><span> </span><span>k</span><span> </span><span>+</span><span> </span><span>32</span><span> </span><span>&lt;=</span><span> </span><span>n</span><span>)</span><span> </span><span>:</span><span> </span><span>(</span><span>i</span><span> </span><span>+=</span><span> </span><span>32</span><span>)</span><span> </span><span>{</span><span>
</span></span></span><span><span><span>    </span><span>const</span><span> </span><span>first_block</span><span>:</span><span> </span><span>Block</span><span> </span><span>=</span><span> </span><span>haystack</span><span>[</span><span>i</span><span>..][</span><span>0</span><span>..</span><span>32</span><span>].</span><span>*</span><span>;</span><span>
</span></span></span><span><span><span>    </span><span>const</span><span> </span><span>last_block</span><span>:</span><span> </span><span>Block</span><span> </span><span>=</span><span> </span><span>haystack</span><span>[</span><span>i</span><span> </span><span>+</span><span> </span><span>k</span><span> </span><span>-</span><span> </span><span>1</span><span> </span><span>..][</span><span>0</span><span>..</span><span>32</span><span>].</span><span>*</span><span>;</span><span>
</span></span></span></code></pre></div><p>Now we can make the comparisons and combine them into a mask:</p><div><pre tabindex="0"><code data-lang="zig"><span><span><span>	</span><span>// ...
</span></span></span><span><span><span></span><span>    </span><span>const</span><span> </span><span>eq_first</span><span> </span><span>=</span><span> </span><span>first_letter</span><span> </span><span>==</span><span> </span><span>first_block</span><span>;</span><span>
</span></span></span><span><span><span>    </span><span>const</span><span> </span><span>eq_last</span><span> </span><span>=</span><span> </span><span>last_letter</span><span> </span><span>==</span><span> </span><span>last_block</span><span>;</span><span>
</span></span></span><span><span><span>    </span><span>var</span><span> </span><span>mask</span><span>:</span><span> </span><span>std</span><span>.</span><span>bit_set</span><span>.</span><span>IntegerBitSet</span><span>(</span><span>32</span><span>)</span><span> </span><span>=</span><span> </span><span>.{</span><span> </span><span>.</span><span>mask</span><span> </span><span>=</span><span> </span><span>@bitCast</span><span>(</span><span>eq_first</span><span> </span><span>&amp;</span><span> </span><span>eq_last</span><span>)</span><span> </span><span>};</span><span>
</span></span></span></code></pre></div><p>Here we can use an <code>IntegerBitSet</code> from Zig’s standard library. We construct it by casting the result of <code>eq_first &amp; eq_last</code> into a 32-bit integer. If the resulting mask is non-zero, there are candidates in the current block.</p><div><pre tabindex="0"><code data-lang="zig"><span><span><span>	</span><span>// ...
</span></span></span><span><span><span></span><span>&nbsp; &nbsp; </span><span>while</span><span> </span><span>(</span><span>mask</span><span>.</span><span>findFirstSet</span><span>())</span><span> </span><span>|</span><span>bitpos</span><span>|</span><span> </span><span>{</span><span>
</span></span></span><span><span><span>        </span><span>if</span><span> </span><span>(</span><span>std</span><span>.</span><span>mem</span><span>.</span><span>eql</span><span>(</span><span>u8</span><span>,</span><span> </span><span>haystack</span><span>[</span><span>i</span><span> </span><span>+</span><span> </span><span>bitpos</span><span> </span><span>+</span><span> </span><span>1</span><span> </span><span>..][</span><span>0</span><span> </span><span>..</span><span> </span><span>k</span><span> </span><span>-</span><span> </span><span>1</span><span>],</span><span> </span><span>needle</span><span>[</span><span>1</span><span>..]))</span><span> </span><span>{</span><span>
</span></span></span><span><span><span>            </span><span>return</span><span> </span><span>i</span><span> </span><span>+</span><span> </span><span>bitpos</span><span>;</span><span>
</span></span></span><span><span><span>        </span><span>}</span><span>
</span></span></span><span><span><span>        </span><span>_</span><span> </span><span>=</span><span> </span><span>mask</span><span>.</span><span>toggleFirstSet</span><span>();</span><span>
</span></span></span><span><span><span>    </span><span>}</span><span>
</span></span></span></code></pre></div><p>The first and last characters of the substring are checked already, so we don’t need to check their equality again.</p><p>Finally, if there are leftover characters, we can fall back to <code>std.mem.IndexOf</code>.</p><div><pre tabindex="0"><code data-lang="zig"><span><span><span>	</span><span>// ...
</span></span></span><span><span><span></span><span>	</span><span>// Fallback to scalar search for the tail
</span></span></span><span><span><span></span><span>	</span><span>if</span><span> </span><span>(</span><span>i</span><span> </span><span>&lt;</span><span> </span><span>n</span><span>)</span><span> </span><span>{</span><span>
</span></span></span><span><span><span>	    </span><span>if</span><span> </span><span>(</span><span>std</span><span>.</span><span>mem</span><span>.</span><span>indexOf</span><span>(</span><span>u8</span><span>,</span><span> </span><span>haystack</span><span>[</span><span>i</span><span>..],</span><span> </span><span>needle</span><span>))</span><span> </span><span>|</span><span>rel_idx</span><span>|</span><span> </span><span>{</span><span>
</span></span></span><span><span><span>	        </span><span>return</span><span> </span><span>i</span><span> </span><span>+</span><span> </span><span>rel_idx</span><span>;</span><span>
</span></span></span><span><span><span>	    </span><span>}</span><span>
</span></span></span><span><span><span>	</span><span>}</span><span>
</span></span></span><span><span><span>	</span><span>return</span><span> </span><span>null</span><span>;</span><span> </span><span>// no substring found
</span></span></span><span><span><span></span><span>}</span><span>
</span></span></span></code></pre></div><h3 id="benchmarks">Benchmarks
<a href="#benchmarks">#</a></h3><p>To properly show the effects of our SIMD algorithm, we’re going to need a large haystack. For this, I’ve chosen to use <a href="https://www.gutenberg.org/ebooks/2701">the entirety Moby Dick</a> in plain text, and a search word ’newsletter’, which appears at the very end of the text.</p><blockquote><p>The code is available <a href="https://github.com/aarol/substr">on GitHub</a></p></blockquote><p>To compile the code, I ran <code>zig build</code> with <code>-Doptimize=ReleaseFast</code>:</p><div><pre tabindex="0"><code data-lang="sh"><span><span>&gt; zig build -Doptimize<span>=</span>ReleaseFast
</span></span></code></pre></div><blockquote><p>Support for <a href="https://github.com/ziglang/zig/pull/24131">bitwise operations on boolean vectors</a> was added in Zig 0.15, which is unreleased as of now (August 2025). If you want to run the code on your system, you need to build Zig from the master branch.</p></blockquote><p>To measure performance and compare against baseline, I’ll use one of my favorite CLI tools, <a href="https://github.com/andrewrk/poop">poop</a>:</p><div><pre tabindex="0"><code data-lang="sh"><span><span>&gt; poop -d <span>10000</span> <span>"./zig-out/bin/substr"</span> <span>"./zig-out/bin/substr --simd"</span>
</span></span></code></pre></div><pre>
<span>Benchmark 1 (6361 runs)</span>: ./zig-out/bin/substr
<span>  measurement          </span><span>mean</span><span> ± </span><span>σ</span><span>            </span><span>min</span><span> … </span><span>max</span><span>           </span><span>outliers</span><span>         delta</span>
  wall_time          <span>1.22</span><span>ms</span> ± <span> 185</span><span>us</span>    <span> 903</span><span>us</span> … <span>5.33</span><span>ms</span>        242 ( 4%)        0%
  peak_rss           <span>1.20</span><span>MB</span> ± <span> 290</span><span>  </span>    <span>1.18</span><span>MB</span> … <span>1.20</span><span>MB</span>          2 ( 0%)        0%
  cpu_cycles         <span>2.15</span><span>M </span> ± <span>40.5</span><span>K </span>    <span>2.10</span><span>M </span> … <span>2.71</span><span>M </span>        312 ( 5%)        0%
  instructions       <span>1.85</span><span>M </span> ± <span>0.75</span><span>  </span>    <span>1.85</span><span>M </span> … <span>1.85</span><span>M </span>         56 ( 1%)        0%
  cache_references   <span>43.8</span><span>K </span> ± <span> 620</span><span>  </span>    <span>38.3</span><span>K </span> … <span>44.9</span><span>K </span>          9 ( 0%)        0%
  cache_misses       <span>19.0</span><span>K </span> ± <span>10.3</span><span>K </span>    <span>4.08</span><span>K </span> … <span>33.6</span><span>K </span>          0 ( 0%)        0%
  branch_misses      <span>48.1</span><span>  </span> ± <span>17.4</span><span>  </span>    <span>  20</span><span>  </span> … <span> 104</span><span>  </span>         97 ( 2%)        0%

<span>Benchmark 2 (10000 runs)</span>: ./zig-out/bin/substr --simd
<span>  measurement          </span><span>mean</span><span> ± </span><span>σ</span><span>            </span><span>min</span><span> … </span><span>max</span><span>           </span><span>outliers</span><span>         delta</span>
  wall_time          <span> 500</span><span>us</span> ± <span>96.9</span><span>us</span>    <span> 397</span><span>us</span> … <span>4.23</span><span>ms</span>        840 ( 8%)        <span>⚡</span><span>- 58.9% ±  0.4%</span>
  peak_rss           <span>1.20</span><span>MB</span> ± <span> 164</span><span>  </span>    <span>1.18</span><span>MB</span> … <span>1.20</span><span>MB</span>          1 ( 0%)          +  0.0% ±  0.0%
  cpu_cycles         <span> 369</span><span>K </span> ± <span>36.1</span><span>K </span>    <span> 340</span><span>K </span> … <span>1.10</span><span>M </span>       <span>1167 (12%)</span>        <span>⚡</span><span>- 82.8% ±  0.1%</span>
  instructions       <span> 578</span><span>K </span> ± <span>0.53</span><span>  </span>    <span> 578</span><span>K </span> … <span> 578</span><span>K </span>          6 ( 0%)        <span>⚡</span><span>- 68.8% ±  0.0%</span>
  cache_references   <span>38.8</span><span>K </span> ± <span> 545</span><span>  </span>    <span>34.1</span><span>K </span> … <span>40.5</span><span>K </span>          6 ( 0%)        <span>⚡</span><span>- 11.4% ±  0.0%</span>
  cache_misses       <span>5.62</span><span>K </span> ± <span>4.97</span><span>K </span>    <span>2.11</span><span>K </span> … <span>27.9</span><span>K </span>       <span>1529 (15%)</span>        <span>⚡</span><span>- 70.3% ±  1.2%</span>
  branch_misses      <span>2.88</span><span>K </span> ± <span>23.4</span><span>  </span>    <span>2.81</span><span>K </span> … <span>3.09</span><span>K </span>        453 ( 5%)        💩<span>+5879.8% ±  1.4%</span>
</pre><p>(Scroll right to see more data)</p><p>As you can see, for a large body of text, the speedup is noticeable: 59% faster with 80% less CPU cycles!</p><blockquote><p>The SIMD version only took 500 microseconds to complete on average, including the overhead of loading the program into memory and printing the result. 500 microseconds is half a millisecond. That’s how fast my laptop searches through a whole book, <strong>200 000 words</strong>, cover to cover. This is why computers are so powerful! How long would it take for a human to do that?</p></blockquote><p>This is quite a large improvement, and proves that the SIMD code is actually working (otherwise the reduction in CPU cycles wouldn’t be so massive). Can we do even better though?</p><h2 id="character-selection">Character selection
<a href="#character-selection">#</a></h2><p>You may notice from the output of <code>poop</code> that the number of branch misses has absolutely blown up – from 48 on average to 2.88k !</p><p>Why does this happen? Well, if you were to count how many times the inner while loop is entered when the mask is non-zero:</p><div><pre tabindex="0"><code data-lang="zig"><span><span><span>    </span><span>var</span><span> </span><span>i</span><span>:</span><span> </span><span>usize</span><span> </span><span>=</span><span> </span><span>0</span><span>;</span><span>
</span></span></span><span><span><span>    </span><span>var</span><span> </span><span>count</span><span>:</span><span> </span><span>usize</span><span> </span><span>=</span><span> </span><span>0</span><span>;</span><span>
</span></span></span><span><span><span>    </span><span>while</span><span> </span><span>(</span><span>i</span><span> </span><span>+</span><span> </span><span>k</span><span> </span><span>+</span><span> </span><span>32</span><span> </span><span>&lt;=</span><span> </span><span>n</span><span>)</span><span> </span><span>:</span><span> </span><span>(</span><span>i</span><span> </span><span>+=</span><span> </span><span>32</span><span>)</span><span> </span><span>{</span><span>
</span></span></span><span><span><span>        </span><span>const</span><span> </span><span>block_first</span><span>:</span><span> </span><span>Block</span><span> </span><span>=</span><span> </span><span>haystack</span><span>[</span><span>i</span><span>..][</span><span>0</span><span>..</span><span>32</span><span>].</span><span>*</span><span>;</span><span>
</span></span></span><span><span><span>        </span><span>const</span><span> </span><span>block_last</span><span>:</span><span> </span><span>Block</span><span> </span><span>=</span><span> </span><span>haystack</span><span>[</span><span>i</span><span> </span><span>+</span><span> </span><span>k</span><span> </span><span>-</span><span> </span><span>1</span><span> </span><span>..][</span><span>0</span><span>..</span><span>32</span><span>].</span><span>*</span><span>;</span><span>
</span></span></span><span><span><span>        </span><span>const</span><span> </span><span>eq_first</span><span> </span><span>=</span><span> </span><span>first</span><span> </span><span>==</span><span> </span><span>block_first</span><span>;</span><span>
</span></span></span><span><span><span>        </span><span>const</span><span> </span><span>eq_last</span><span> </span><span>=</span><span> </span><span>last</span><span> </span><span>==</span><span> </span><span>block_last</span><span>;</span><span>
</span></span></span><span><span><span>        </span><span>var</span><span> </span><span>mask</span><span>:</span><span> </span><span>std</span><span>.</span><span>bit_set</span><span>.</span><span>IntegerBitSet</span><span>(</span><span>32</span><span>)</span><span> </span><span>=</span><span> </span><span>.{</span><span> </span><span>.</span><span>mask</span><span> </span><span>=</span><span> </span><span>@bitCast</span><span>(</span><span>eq_first</span><span> </span><span>&amp;</span><span> </span><span>eq_last</span><span>)</span><span> </span><span>};</span><span>
</span></span></span><span><span><span>        </span><span>while</span><span> </span><span>(</span><span>mask</span><span>.</span><span>findFirstSet</span><span>())</span><span> </span><span>|</span><span>bitpos</span><span>|</span><span> </span><span>{</span><span>
</span></span></span><span><span><span>            </span><span>count</span><span> </span><span>+=</span><span> </span><span>1</span><span>;</span><span>
</span></span></span><span><span><span>            </span><span>if</span><span> </span><span>(</span><span>std</span><span>.</span><span>mem</span><span>.</span><span>eql</span><span>(</span><span>u8</span><span>,</span><span> </span><span>haystack</span><span>[</span><span>i</span><span> </span><span>+</span><span> </span><span>bitpos</span><span> </span><span>+</span><span> </span><span>1</span><span> </span><span>..][</span><span>0</span><span> </span><span>..</span><span> </span><span>k</span><span> </span><span>-</span><span> </span><span>1</span><span>],</span><span> </span><span>needle</span><span>[</span><span>1</span><span>..]))</span><span> </span><span>{</span><span>
</span></span></span><span><span><span>                </span><span>std</span><span>.</span><span>debug</span><span>.</span><span>print</span><span>(</span><span>"found match with count: {}</span><span>\n</span><span>"</span><span>,</span><span> </span><span>.{</span><span>count</span><span>});</span><span>
</span></span></span><span><span><span>                </span><span>return</span><span> </span><span>i</span><span> </span><span>+</span><span> </span><span>bitpos</span><span>;</span><span>
</span></span></span><span><span><span>            </span><span>}</span><span>
</span></span></span><span><span><span>            </span><span>_</span><span> </span><span>=</span><span> </span><span>mask</span><span>.</span><span>toggleFirstSet</span><span>();</span><span>
</span></span></span><span><span><span>        </span><span>}</span><span>
</span></span></span><span><span><span>    </span><span>}</span><span>
</span></span></span></code></pre></div><div><pre tabindex="0"><code data-lang="fallback"><span><span>found match with count: 2792
</span></span></code></pre></div><p>The fact that&nbsp;<code>count</code>&nbsp;is so close to the number of mispredictions suggests that each time the mask is non‑zero we incur a branch miss.</p><p>Unfortunately, there is no obvious way to prevent this with the current algorithm. The state-of-the-art seems to be choosing two bytes in the needle that occur less frequently according to a pre-calculated frequency distribution. This is used in the <a href="https://github.com/BurntSushi/memchr"><code>memchr</code> crate</a> in Rust, as explained by the author in <a href="https://news.ycombinator.com/item?id=44275934">this comment on Hacker News</a>.</p><p>For example, the needle <code>newsletter</code> has the rarest characters <code>w</code> at index <code>2</code> and <code>l</code> at index <code>4</code>.</p><p>The function in <code>memchr</code> can be found <a href="https://github.com/BurntSushi/memchr/blob/3962118774ac511580c5b40fd14323e31629fa52/src/arch/all/packedpair/mod.rs#L163">here</a>. I’ve ported it into Zig, and you can see it <a href="https://github.com/aarol/substr/blob/9392f9557de735929dfb79efa4fc88115341c65d/src/main.zig#L100">here</a>.</p><div><pre tabindex="0"><code data-lang="zig"><span><span><span>    </span><span>const</span><span> </span><span>needle_index_pair</span><span> </span><span>=</span><span> </span><span>find_rarest</span><span>(</span><span>needle</span><span>);</span><span>
</span></span></span><span><span><span>
</span></span></span><span><span><span>    </span><span>const</span><span> </span><span>first_letter</span><span>:</span><span> </span><span>Block</span><span> </span><span>=</span><span> </span><span>@splat</span><span>(</span><span>needle</span><span>[</span><span>needle_index_pair</span><span>[</span><span>0</span><span>]]);</span><span>
</span></span></span><span><span><span>    </span><span>const</span><span> </span><span>first_offset</span><span> </span><span>=</span><span> </span><span>needle_index_pair</span><span>[</span><span>0</span><span>];</span><span>
</span></span></span><span><span><span>    </span><span>const</span><span> </span><span>second_letter</span><span>:</span><span> </span><span>Block</span><span> </span><span>=</span><span> </span><span>@splat</span><span>(</span><span>needle</span><span>[</span><span>needle_index_pair</span><span>[</span><span>1</span><span>]]);</span><span>
</span></span></span><span><span><span>    </span><span>const</span><span> </span><span>second_offset</span><span> </span><span>=</span><span> </span><span>needle_index_pair</span><span>[</span><span>1</span><span>];</span><span>
</span></span></span></code></pre></div><p>The algorithm is the exact same, but the index for <code>first_letter</code> and <code>second_letter</code> now varies according to the pre-calculated frequency distribution.</p><h3 id="benchmarks">Benchmarks
<a href="#benchmarks">#</a></h3><pre>
<span>Benchmark 1 (10000 runs)</span>: ./zig-out/bin/substr --simd
<span>  measurement          </span><span>mean</span><span> ± </span><span>σ</span><span>            </span><span>min</span><span> … </span><span>max</span><span>           </span><span>outliers</span><span>         delta</span>
  wall_time          <span> 472</span><span>us</span> ± <span>62.9</span><span>us</span>    <span> 400</span><span>us</span> … <span>1.62</span><span>ms</span>        735 ( 7%)        0%
  peak_rss           <span>1.20</span><span>MB</span> ± <span>   0</span><span>  </span>    <span>1.20</span><span>MB</span> … <span>1.20</span><span>MB</span>          0 ( 0%)        0%
  cpu_cycles         <span> 376</span><span>K </span> ± <span>44.7</span><span>K </span>    <span> 347</span><span>K </span> … <span>1.46</span><span>M </span>       <span>1213 (12%)</span>        0%
  instructions       <span> 578</span><span>K </span> ± <span>0.54</span><span>  </span>    <span> 578</span><span>K </span> … <span> 578</span><span>K </span>         10 ( 0%)        0%
  cache_references   <span>38.7</span><span>K </span> ± <span> 715</span><span>  </span>    <span>28.3</span><span>K </span> … <span>40.6</span><span>K </span>         96 ( 1%)        0%
  cache_misses       <span>7.37</span><span>K </span> ± <span>5.83</span><span>K </span>    <span>2.78</span><span>K </span> … <span>27.7</span><span>K </span>       <span>1608 (16%)</span>        0%
  branch_misses      <span>2.88</span><span>K </span> ± <span>23.4</span><span>  </span>    <span>2.82</span><span>K </span> … <span>3.08</span><span>K </span>        415 ( 4%)        0%

<span>Benchmark 2 (10000 runs)</span>: ./zig-out/bin/substr --simdv2
<span>  measurement          </span><span>mean</span><span> ± </span><span>σ</span><span>            </span><span>min</span><span> … </span><span>max</span><span>           </span><span>outliers</span><span>         delta</span>
  wall_time          <span> 429</span><span>us</span> ± <span>75.5</span><span>us</span>    <span> 369</span><span>us</span> … <span>3.85</span><span>ms</span>        393 ( 4%)        <span>⚡</span><span>-  9.1% ±  0.4%</span>
  peak_rss           <span>1.20</span><span>MB</span> ± <span>   0</span><span>  </span>    <span>1.20</span><span>MB</span> … <span>1.20</span><span>MB</span>          0 ( 0%)          -  0.0% ±  0.0%
  cpu_cycles         <span> 304</span><span>K </span> ± <span>28.4</span><span>K </span>    <span> 282</span><span>K </span> … <span>1.07</span><span>M </span>       <span>1140 (11%)</span>        <span>⚡</span><span>- 19.2% ±  0.3%</span>
  instructions       <span> 561</span><span>K </span> ± <span>0.52</span><span>  </span>    <span> 561</span><span>K </span> … <span> 561</span><span>K </span>          5 ( 0%)        <span>⚡</span><span>-  2.9% ±  0.0%</span>
  cache_references   <span>38.7</span><span>K </span> ± <span> 610</span><span>  </span>    <span>29.9</span><span>K </span> … <span>40.3</span><span>K </span>         25 ( 0%)          -  0.1% ±  0.0%
  cache_misses       <span>5.21</span><span>K </span> ± <span>3.53</span><span>K </span>    <span>2.57</span><span>K </span> … <span>27.3</span><span>K </span>       <span>1306 (13%)</span>        <span>⚡</span><span>- 29.3% ±  1.8%</span>
  branch_misses      <span>1.07</span><span>K </span> ± <span>14.0</span><span>  </span>    <span>1.02</span><span>K </span> … <span>1.17</span><span>K </span>        275 ( 3%)        <span>⚡</span><span>- 62.8% ±  0.0%</span>

</pre><p>Comparing to the previous SIMD version, the number of branch misses has dropped by 60%, and it’s 9% faster too. Nice!</p><blockquote><p>The number of branch misses is lower, which can cause faster execution, but I suspect that a much bigger impact is the fact that there are less false positives, which means less byte-by-byte memory accesses and comparisons.</p></blockquote><h2 id="avx-512">AVX-512
<a href="#avx-512">#</a></h2><p>Since AMD <a href="https://en.wikipedia.org/wiki/Zen_4">Zen 4 </a>and Intel <a href="https://en.wikipedia.org/wiki/Cannon_Lake_%28microprocessor%29">Cannon Lake</a>, there has been a new SIMD instruction set, AVX-512 with 512-bit instructions – double the size of AVX2. I don’t have a computer that has AVX-512 right now, but I suspect that changing the Zig code to process 64 characters at once would lead to even better results.</p><h2 id="a-smaller-haystack">A smaller haystack
<a href="#a-smaller-haystack">#</a></h2><p>It’s clear that with a very large haystack, the SIMD version is much faster. But what about a tiny input, like less than a hunder characters?</p><p>I did a bit of benchmarking with <code>poop</code>, but I found that I couldn’t accurately measure the speed, since both versions finish extremely very quickly. I decided to use <a href="https://github.com/hendriknielaender/zBench">zBench</a> to do a microbenchmark. I decided to use a snippet from Moby Dick as seen <a href="https://github.com/aarol/substr/blob/main/src/haystack-small.txt">here</a>.</p><pre>+- run test stderr
benchmark              runs     total time     time/run (avg ± σ)    (min ... max)                p75        p99        p995      
-----------------------------------------------------------------------------------------------------------------------------
find_substr            <span>100000   424.368ms      </span><span>4.243us ± 740ns       </span><span>(3.964us ... 107.923us)      </span><span>4.187us    7.075us    7.245us   </span>
find_substr_simd_v2    <span>100000   147.883ms      </span><span>1.478us ± 186ns       </span><span>(1.417us ... 21.354us)       </span><span>1.483us    1.539us    1.548us   </span>
</pre><p>I was surprised to see that even when processing less than a hundred characters, the SIMD algorithm is still faster! The difference between 4μs vs 1μs is extremely small, but it’s slightly faster nonetheless.</p><h2 id="conclusion">Conclusion
<a href="#conclusion">#</a></h2><p>As you can see, SIMD can be used to make substring searching dramatically faster, for both very large and very small strings.</p><p>But if it’s so much better, then why haven’t I made a pull request to change <code>std.mem.indexOf</code> to use SIMD? Well, the reason is that</p><ol><li><code>std.mem.indexOf</code> is generic over element size, and having a size larger than <code>u8</code> makes the algorithm much slower</li><li>The <a href="https://en.wikipedia.org/wiki/Boyer%E2%80%93Moore%E2%80%93Horspool_algorithm">algorithm</a> used in <code>stdmem.indexOf</code> is cross-platform, while the SIMD code wouldn’t be. (not all platforms have SIMD registers at all, Arm has only 128-bit)</li></ol><p>Substring searching is rarely the bottleneck in programs, especially ones written in a fast language like Zig. That’s why I don’t personally think it would be worth it to add it to the standard library.</p><p>Still, it was great to learn about this advanced optimization technique and see some concrete performance measurements from it!</p><p>The full code is available on GitHub <a href="https://github.com/aarol/substr/">here</a>.</p><h2 id="further-reading">Further reading
<a href="#further-reading">#</a></h2><ul><li>SIMD with Zig <a href="https://www.openmymind.net/SIMD-With-Zig/">https://www.openmymind.net/SIMD-With-Zig/</a></li><li>SIMD-friendly algorithms for substring searching: <a href="http://0x80.pl/notesen/2016-11-28-simd-strfind.html">http://0x80.pl/notesen/2016-11-28-simd-strfind.html</a></li><li><code>memchr</code> source code: <a href="https://github.com/BurntSushi/memchr">https://github.com/BurntSushi/memchr</a></li></ul></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Hand-picked selection of articles on AI fundamentals/concepts (146 pts)]]></title>
            <link>https://aman.ai/primers/ai/</link>
            <guid>44862112</guid>
            <pubDate>Mon, 11 Aug 2025 08:59:46 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://aman.ai/primers/ai/">https://aman.ai/primers/ai/</a>, See on <a href="https://news.ycombinator.com/item?id=44862112">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>

  <header>
    
  </header>

  <article>
  <h2 id="overview">Overview</h2>

<ul>
  <li>Here’s a hand-picked selection of articles on AI fundamentals/concepts that cover the entire process of building neural nets to training them to evaluating results.</li>
</ul>

<h2 id="algorithmsarchitecture">Algorithms/Architecture</h2>

<ul>
  <li><a href="https://aman.ai/primers/ai/linear-logistic-regression">Linear and Logistic Regression</a></li>
  <li><a href="https://aman.ai/primers/ai/k-nearest-neighbors">k-Nearest Neighbors</a></li>
  <li><a href="https://aman.ai/primers/ai/clustering">Clustering</a></li>
  <li><a href="https://aman.ai/primers/ai/support-vector-machines">Support Vector Machines (SVM)</a></li>
  <li><a href="https://aman.ai/primers/ai/naive-bayes">Naive Bayes</a></li>
  <li><a href="https://aman.ai/primers/ai/decision-trees-and-ensemble-methods">Decision Trees and Ensemble Methods</a></li>
  <li><a href="https://aman.ai/primers/ai/ml-comp">ML Algorithms Comparative Analysis</a></li>
  <li><a href="https://aman.ai/primers/ai/dl-comp">DL Architectures Comparative Analysis</a></li>
  <li><a href="https://aman.ai/primers/ai/prompt-engineering">Prompt Engineering</a></li>
  <li><a href="https://aman.ai/primers/ai/gan">Generative Adversarial Networks (GANs)</a></li>
  <li><a href="https://aman.ai/primers/ai/diffusion-models">Diffusion Models</a></li>
  <li><a href="https://aman.ai/primers/ai/gnn">Graph Neural Networks</a></li>
  <li><a href="https://aman.ai/primers/ai/attention">Attention</a></li>
  <li><a href="https://aman.ai/primers/ai/separable-convolutions">Separable Convolutions</a></li>
  <li><a href="https://aman.ai/primers/ai/inductive-bias">Inductive Bias</a></li>
  <li><a href="https://aman.ai/primers/ai/cnn">Convolutional Neural Networks</a></li>
  <li><a href="https://aman.ai/primers/ai/reinforcement-learning">Reinforcement Learning</a></li>
  <li><a href="https://aman.ai/primers/ai/mixture-of-experts">Mixture-of-Experts</a></li>
  <li><a href="https://aman.ai/primers/ai/state-space-models">State Space Models</a></li>
  <li><a href="https://aman.ai/primers/ai/agents">Agents</a></li>
  <li><a href="https://aman.ai/primers/ai/flashattention">FlashAttention</a>
<!-- - [Quantization](../ai/quantization) --></li>
  <li><a href="https://aman.ai/primers/ai/model-acceleration">Model Acceleration</a></li>
  <li><a href="https://aman.ai/primers/ai/speculative-decoding">Speculative Decoding</a></li>
  <li><a href="https://aman.ai/primers/ai/cross-validation">Cross Validation</a></li>
</ul>

<h2 id="datatraining">Data/Training</h2>

<ul>
  <li><a href="https://aman.ai/primers/ai/data-sampling">Data Sampling</a></li>
  <li><a href="https://aman.ai/primers/ai/data-imbalance">Data Imbalance</a></li>
  <li><a href="https://aman.ai/primers/ai/standardization-vs-normalization">Standardization vs. Normalization</a></li>
  <li><a href="https://aman.ai/primers/ai/learning-paradigms">Learning Paradigms</a></li>
  <li><a href="https://aman.ai/primers/ai/xavier-init">Xavier Initialization</a></li>
  <li><a href="https://aman.ai/primers/ai/padding-and-packing">Padding and Packing</a></li>
  <li><a href="https://aman.ai/primers/ai/regularization">Regularization</a></li>
  <li><a href="https://aman.ai/primers/ai/gradient-descent-and-backprop">Gradient Descent and Backprop</a></li>
  <li><a href="https://aman.ai/primers/ai/activation-functions">Activation Functions</a></li>
  <li><a href="https://aman.ai/primers/ai/loss">Loss Functions</a></li>
  <li><a href="https://aman.ai/primers/ai/activation">Activation Functions</a></li>
  <li><a href="https://aman.ai/primers/ai/fine-tuning-models">Fine-tuning Models</a></li>
  <li><a href="https://aman.ai/primers/ai/data-split">Splitting Datasets</a></li>
  <li><a href="https://aman.ai/primers/ai/batchnorm">Batchnorm</a></li>
  <li><a href="https://aman.ai/primers/ai/dropout">Dropout</a></li>
  <li><a href="https://aman.ai/primers/ai/double-descent">Double Descent</a></li>
  <li><a href="https://aman.ai/primers/ai/fine-tune-and-eval-BERT">Fine-Tuning and Evaluating BERT</a>
<!-- - [Debugging Deep Learning Projects](../ai/debugging-dl-projects) --></li>
  <li><a href="https://aman.ai/primers/ai/train-val-loss">Training Loss &gt; Validation Loss?</a></li>
  <li><a href="https://aman.ai/primers/ai/svm-kernel-trick">SVM Kernel/Polynomial Trick</a></li>
  <li><a href="https://aman.ai/primers/ai/bias-variance-tradeoff">Bias Variance Tradeoff</a></li>
  <li><a href="https://aman.ai/primers/ai/grad-accum-checkpoint">Gradient Accumulation and Checkpointing</a></li>
  <li><a href="https://aman.ai/primers/ai/parameter-efficient-fine-tuning">Parameter Efficient Fine-Tuning</a></li>
  <li><a href="https://aman.ai/primers/ai/hypernetworks">Hypernetworks</a></li>
  <li><a href="https://aman.ai/primers/ai/distributed-training-parallelism">Distributed Training Parallelism</a></li>
</ul>

<h2 id="speech">Speech</h2>

<ul>
  <li><a href="https://aman.ai/primers/ai/speech-processing">Speech Processing</a></li>
</ul>

<h2 id="vision">Vision</h2>

<ul>
  <li><a href="https://aman.ai/primers/ai/vit">Vision Transformer (ViT)</a></li>
  <li><a href="https://aman.ai/primers/ai/receptive-field">Receptive Field</a></li>
  <li><a href="https://aman.ai/primers/ai/skip-connections">Residual Networks/Skip Connections</a></li>
  <li><a href="https://aman.ai/primers/ai/gpt4o-native-image-generation">GPT-4o Native Image Generation</a></li>
</ul>

<h2 id="nlp">NLP</h2>

<ul>
  <li><a href="https://aman.ai/primers/ai/word-vectors">Word Vectors/Embeddings</a></li>
  <li><a href="https://aman.ai/primers/ai/nlp-tasks">NLP Tasks</a></li>
  <li><a href="https://aman.ai/primers/ai/preprocessing">Preprocessing</a></li>
  <li><a href="https://aman.ai/primers/ai/tokenizer">Tokenization</a></li>
  <li><a href="https://aman.ai/primers/ai/data-sampling">Data Sampling</a></li>
  <li><a href="https://aman.ai/primers/ai/architectures">Neural Architectures</a></li>
  <li><a href="https://aman.ai/primers/ai/attention">Attention</a></li>
  <li><a href="https://aman.ai/primers/ai/transformers">Transformers</a></li>
  <li><a href="https://aman.ai/primers/ai/token-sampling">Token Sampling Methods</a></li>
  <li><a href="https://aman.ai/primers/ai/encoder-vs-decoder-models">Encoder vs. Decoder vs. Encoder-Decoder Models</a>
<!-- - [Language Models](../ai/language-model) --></li>
  <li><a href="https://aman.ai/primers/ai/LLM">Overview of Large Language Models (LLMs)</a></li>
  <li><a href="https://aman.ai/primers/ai/reinforcement-finetuning">Reinforcement Fine-Tuning</a></li>
  <li><a href="https://aman.ai/primers/ai/preference-optimization">Preference Optimization</a></li>
  <li><a href="https://aman.ai/primers/ai/translation">Machine Translation</a></li>
  <li><a href="https://aman.ai/primers/ai/knowledge-graphs">Knowledge Graphs</a></li>
  <li><a href="https://aman.ai/primers/ai/hallucination">Hallucination Detection and Mitigation</a></li>
  <li><a href="https://aman.ai/primers/ai/AIDetect">AI Text Detection Techniques</a></li>
  <li><a href="https://aman.ai/primers/ai/ner">Named Entity Recognition</a></li>
  <li><a href="https://aman.ai/primers/ai/textual-entailment">Textual Entailment</a></li>
  <li><a href="https://aman.ai/primers/ai/RAG">Retrieval Augmented Generation (RAG)</a></li>
  <li><a href="https://aman.ai/primers/ai/context-length-extension">LLM Context Length Extension</a></li>
  <li><a href="https://aman.ai/primers/ai/document-intelligence">Document Intelligence</a>
<!-- - [Personalizing Large Language Models](../ai/personalize-LLMs) --></li>
  <li><a href="https://aman.ai/primers/ai/code-mixing-switching">Code Mixing and Switching</a></li>
  <li><a href="https://aman.ai/primers/ai/LLMOps">Large Language Model Ops (LLMOps)</a></li>
  <li><a href="https://aman.ai/primers/ai/benchmarks">LLM/VLM Benchmarks</a></li>
</ul>

<h2 id="multimodal">Multimodal</h2>

<ul>
  <li><a href="https://aman.ai/primers/ai/VLM">Overview of Vision-Language Models (VLMs)</a></li>
  <li><a href="https://aman.ai/primers/ai/vision-language-models">VLM Architectures</a></li>
  <li><a href="https://aman.ai/primers/ai/computer-control">Computer Control</a></li>
</ul>

<h2 id="models">Models</h2>

<ul>
  <li><a href="https://aman.ai/primers/ai/bert">BERT</a></li>
  <li><a href="https://aman.ai/primers/ai/gpt">GPT</a></li>
  <li><a href="https://aman.ai/primers/ai/CLIP">CLIP</a></li>
  <li><a href="https://aman.ai/primers/ai/meena">Meena</a></li>
  <li><a href="https://aman.ai/primers/ai/chatGPT">ChatGPT</a></li>
  <li><a href="https://aman.ai/primers/ai/GPT-4">GPT-4</a></li>
  <li><a href="https://aman.ai/primers/ai/LLaMA">LLaMA</a></li>
  <li><a href="https://aman.ai/primers/ai/alpaca">Alpaca</a></li>
  <li><a href="https://aman.ai/primers/ai/gemini">Gemini</a></li>
  <li><a href="https://aman.ai/primers/ai/toolformer">Toolformer</a></li>
  <li><a href="https://aman.ai/primers/ai/visualChatGPT">Visual ChatGPT</a></li>
  <li><a href="https://aman.ai/primers/ai/TaskMatrix">TaskMatrix.AI</a></li>
  <li><a href="https://aman.ai/primers/ai/bigbird">BigBird</a></li>
  <li><a href="https://aman.ai/primers/ai/o1">OpenAI o1</a></li>
  <li><a href="https://aman.ai/primers/ai/deepseek-R1">DeepSeek R1</a></li>
  <li><a href="https://aman.ai/primers/ai/deepseek-janus-pro">DeepSeek Janus-Pro</a></li>
  <li><a href="https://aman.ai/primers/ai/gemma-3n">Gemma 3n</a></li>
</ul>

<h2 id="offlineonline-evaluation">Offline/Online Evaluation</h2>

<ul>
  <li><a href="https://aman.ai/primers/ai/evaluation-metrics">Evaluation Metrics</a></li>
  <li><a href="https://aman.ai/primers/ai/f-beta">F-Beta Score</a></li>
  <li><a href="https://aman.ai/primers/ai/ab-testing">A/B Testing</a></li>
</ul>

<h2 id="mlops">MLOps</h2>

<ul>
  <li><a href="https://aman.ai/primers/ai/drift">Data Drift</a></li>
  <li><a href="https://aman.ai/primers/ai/mlops-tooling">MLOps Tooling</a></li>
  <li><a href="https://aman.ai/primers/ai/mlops-testing">MLOps Testing</a></li>
</ul>

<h2 id="on-device-ai">On-Device AI</h2>

<ul>
  <li><a href="https://aman.ai/primers/ai/model-compression">Model Compression</a></li>
  <li><a href="https://aman.ai/primers/ai/pii">Personally Identifiable Information (PII)</a></li>
  <li><a href="https://aman.ai/primers/ai/federated-learning">Federated Learning</a></li>
  <li><a href="https://aman.ai/primers/ai/differential-privacy">Differential Privacy</a></li>
  <li><a href="https://aman.ai/primers/ai/on-device-transformers">On-device Transformers</a></li>
</ul>

<h2 id="project-planning-scheduling-execution">Project Planning, Scheduling, Execution</h2>

<ul>
  <li><a href="https://aman.ai/primers/ai/okr">Objectives and Key Results (OKRs)</a></li>
  <li><a href="https://aman.ai/primers/ai/rice-framework">RICE Framework</a></li>
  <li><a href="https://aman.ai/primers/ai/gantt-charts">Gantt Charts</a></li>
  <li><a href="https://aman.ai/primers/ai/project-management">Project Management</a></li>
</ul>

<h2 id="miscellaneous">Miscellaneous</h2>

<ul>
  <li><a href="https://aman.ai/primers/ai/top-30-papers">Ilya Sutskever’s Top 30</a></li>
  <li><a href="https://aman.ai/primers/ai/model-debugging">Debugging Model Training</a></li>
  <li><a href="https://aman.ai/primers/ai/ml-runtimes">ML Runtimes</a></li>
  <li><a href="https://aman.ai/primers/ai/chain-rule">Chain Rule</a></li>
  <li><a href="https://aman.ai/primers/ai/bayes-theorem">Bayes’ Theorem</a></li>
  <li><a href="https://aman.ai/primers/ai/probability-calibration">Probability Calibration</a></li>
  <li><a href="https://aman.ai/primers/ai/multiclass-vs-multilabel-classification">Multiclass vs. Multilabel Classification</a></li>
  <li><a href="https://aman.ai/primers/ai/matmul">N-Dimensional Tensor Product</a></li>
  <li><a href="https://aman.ai/primers/ai/pytorch-vs-tensorflow">PyTorch vs. TensorFlow</a></li>
  <li><a href="https://aman.ai/primers/ai/ann-similarity-search">Approximate Nearest Neighbors – Similarity Search</a></li>
  <li><a href="https://aman.ai/primers/ai/transferability-estimation">Transferability Estimation</a></li>
  <li><a href="https://aman.ai/primers/ai/tensorboard">TensorBoard</a></li>
  <li><a href="https://aman.ai/primers/ai/cnns-for-text-classification">Convolutional Neural Networks for Text Classification</a></li>
  <li><a href="https://aman.ai/primers/ai/hmm-and-naive-bayes">Relationship between Hidden Markov Models and Naive Bayes</a></li>
  <li><a href="https://aman.ai/primers/ai/maximum-entropy-markov-models-and-logistic-reg">Maximum Entropy Markov Models</a></li>
  <li><a href="https://aman.ai/primers/ai/conditional-random-fields">Conditional Random Fields</a>
<!-- - [Information Retrieval Methods](../ai/info-retrieval-methods) --></li>
</ul>

<h2 id="hyperparameters">Hyperparameters</h2>

<ul>
  <li><a href="https://aman.ai/primers/ai/hyperparameter-tuning">Hyperparameter Tuning</a></li>
  <li><a href="https://aman.ai/primers/ai/hyperparameter-logging">Hyperparameter Logging</a></li>
</ul>

<h2 id="practice">Practice</h2>

<ul>
  <li><a href="https://aman.ai/primers/ai/interview">Interview Questions</a></li>
</ul>

  </article>

</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Google paid a $250K reward for a bug (354 pts)]]></title>
            <link>https://issues.chromium.org/issues/412578726</link>
            <guid>44861106</guid>
            <pubDate>Mon, 11 Aug 2025 05:56:33 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://issues.chromium.org/issues/412578726">https://issues.chromium.org/issues/412578726</a>, See on <a href="https://news.ycombinator.com/item?id=44861106">Hacker News</a></p>
<div id="readability-page-1" class="page"><header><div ng-non-bindable="" data-ogsr-up="" id="gb"><p><a aria-label="Sign in" href="https://accounts.google.com/ServiceLogin?passive=1209600&amp;osid=1&amp;continue=https%3A%2F%2Fissues.chromium.org%2Fissues%2F412578726&amp;followup=https%3A%2F%2Fissues.chromium.org%2Fissues%2F412578726&amp;ec=GAZAkwI" target="_top"><span>Sign in</span></a></p></div></header></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Basic Social Skills Guide (191 pts)]]></title>
            <link>https://www.improveyoursocialskills.com/basic-social-skills-guide</link>
            <guid>44860932</guid>
            <pubDate>Mon, 11 Aug 2025 05:19:01 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.improveyoursocialskills.com/basic-social-skills-guide">https://www.improveyoursocialskills.com/basic-social-skills-guide</a>, See on <a href="https://news.ycombinator.com/item?id=44860932">Hacker News</a></p>
Couldn't get https://www.improveyoursocialskills.com/basic-social-skills-guide: Error: Request failed with status code 521]]></description>
        </item>
        <item>
            <title><![CDATA[Going faster than memcpy (104 pts)]]></title>
            <link>https://squadrick.dev/journal/going-faster-than-memcpy</link>
            <guid>44860847</guid>
            <pubDate>Mon, 11 Aug 2025 04:59:03 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://squadrick.dev/journal/going-faster-than-memcpy">https://squadrick.dev/journal/going-faster-than-memcpy</a>, See on <a href="https://news.ycombinator.com/item?id=44860847">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
      



<p>While profiling <a href="https://github.com/squadrick/shadesmar">Shadesmar</a> a couple of
weeks ago, I noticed that for large binary unserialized messages (&gt;512kB) most
of the execution time is spent doing copying the message (using <code>memcpy</code>)
between process memory to shared memory and back.</p>

<p>I had a few hours to kill last weekend, and I tried to implement a faster way
to do memory copies.</p>

<hr>

<h3 id="autopsy-of-memcpy">Autopsy of memcpy</h3>

<p>Here’s the dumb of <a href="https://perf.wiki.kernel.org/index.php/Main_Page"><code>perf</code></a>
when running pub-sub for messages of sizes between 512kB and 2MB.</p>

<div><pre><code> Children      Self  Shared Object      Symbol
+  99.86%     0.00%  libc-2.27.so       [.] __libc_start_main
+  99.86%     0.00%  [unknown]          [k] 0x4426258d4c544155
+  99.84%     0.02%  raw_benchmark      [.] main
+  98.13%    97.12%  libc-2.27.so       [.] __memmove_avx_unaligned_erms
+  51.99%     0.00%  raw_benchmark      [.] shm::PublisherBin&lt;16u&gt;::publish
+  51.98%     0.01%  raw_benchmark      [.] shm::Topic&lt;16u&gt;::write
+  47.64%     0.01%  raw_benchmark      [.] shm::Topic&lt;16u&gt;::read
</code></pre></div>

<p><code>__memmove_avx_unaligned_erms</code> is an implementation of <code>memcpy</code> for unaligned
memory blocks that uses AVX to copy over 32 bytes at a time. Digging into the
<code>glibc</code> source code, I found this:</p>

<div><pre><code><span>#if IS_IN (libc)
# define VEC_SIZE                32
# define VEC(i)                  ymm##i
# define VMOVNT                  vmovntdq
# define VMOVU                   vmovdqu
# define VMOVA                   vmovdqa
# define SECTION(p)              p##.avx
# define MEMMOVE_SYMBOL(p,s)     p##_avx_##s
</span>
<span># include "memmove-vec-unaligned-erms.S"
#endif
</span></code></pre></div>

<p>Breaking down this function:</p>

<p><code>memmove</code>: <code>glibc</code> implements <code>memcpy</code> as a <code>memmove</code> instead, here’s the
relevant source code:</p>

<div><pre><code><span># define SYMBOL_NAME memcpy
# include "ifunc-memmove.h"
</span>
<span>libc_ifunc_redirected</span> <span>(</span><span>__redirect_memcpy</span><span>,</span> <span>__new_memcpy</span><span>,</span>
		       <span>IFUNC_SELECTOR</span> <span>());</span>
</code></pre></div>

<p>Here’s the difference between the two: With <code>memcpy</code>, the destination cannot
overlap the source at all. With <code>memmove</code> it can. Initially, I wasn’t sure why
it was implemented as <code>memmove</code>. The reason for this will become clearer as
the post proceeds.</p>

<p><code>erms</code>: <em>E</em>nhanced <em>R</em>ep <em>M</em>ov<em>s</em> is a hardware optimization for a loop that
does a simple copy. In simple pseudo-code, this is what the loop implementation
looks like for copying a single byte at a time (<code>REP MOVSB</code>).</p>

<div><pre><code><span>void</span> <span>rep_movsb</span><span>(</span><span>void</span> <span>*</span><span>dest</span><span>,</span> <span>const</span> <span>void</span> <span>*</span><span>src</span><span>,</span> <span>size_t</span> <span>len</span><span>)</span> <span>{</span>
  <span>const</span> <span>uint8_t</span><span>*</span> <span>s</span> <span>=</span> <span>(</span><span>uint8_t</span><span>*</span><span>)</span><span>src</span><span>;</span>
  <span>uint8_t</span><span>*</span> <span>d</span> <span>=</span> <span>(</span><span>uint8_t</span><span>*</span><span>)</span><span>dest</span><span>;</span>

  <span>while</span> <span>(</span><span>len</span><span>--</span><span>)</span>
    <span>*</span><span>d</span><span>++</span> <span>=</span> <span>*</span><span>s</span><span>++</span><span>;</span>

  <span>return</span> <span>dest</span><span>;</span>
<span>}</span>
</code></pre></div>

<p>Since the loop copies data pointer by pointer, it can handle the case of
overlapping data.</p>

<p><code>vec</code>: For the above loop rather than copying around single bytes, it uses x86
vectorized instructions to copy multiple bytes in a single loop iteration
(technically single instruction). <code>vmov*</code> are assembly instructions for AVX
which is the latest instruction set that the CPU on my laptop supports. With
<code>VEC_SIZE = 32</code>, it copies 32 bytes at a time.</p>

<p><code>unaligned</code>: This is a generic version of <code>memmove</code> that can copy between any
pointer locations irrespective of their alignment. Unaligned pointers increase
complexity for the copy loop when using vectorized instructions. The unaligned
preceeding and trailing memory locations must be copied separately before hitting the
optimized loop.</p>

<p><code>memmove-vec-unaligned-erms.S</code> holds the actual implementation in assembly. A
few things that the implementation does:</p>

<ol>
  <li>
    <p>It uses <code>REP MOVS</code> only if the data is greater than 4kB. For smaller values it uses the SSE2 optimization.</p>
  </li>
  <li>For handling <code>unaligned</code> pointers, it uses the following blocks:
    <ul>
      <li>16 to 31: <code>vmovdqu</code></li>
      <li>15 to 8: <code>movq</code></li>
      <li>7 to 4: <code>movl</code></li>
      <li>3 to 2: <code>movzwl</code> and <code>movw</code></li>
    </ul>
  </li>
  <li><code>VMOVNT</code> defined above is for doing non-temporal(NT) moves. NT instructions
are used when there is an overlap between destination and source since
destination may be in cache when source is loaded. Uses <code>prefetcht0</code> to load
data into cache (all levels: t0). In the current iteration, we prefetch the
data for 2 iterations later. The data is copied (via cache) into registers. The
data (via NT) is copied from registers into destination.</li>
</ol>

<div><pre><code><span>L</span><span>(</span><span>loop_large_forward</span><span>):</span>
	<span>; Copy 4 * VEC a time forward with non-temporal stores.</span>
	<span>PREFETCH_ONE_SET</span> <span>(</span><span>1</span><span>,</span> <span>(</span><span>%</span><span>rsi</span><span>),</span> <span>PREFETCHED_LOAD_SIZE</span> <span>*</span> <span>2</span><span>)</span>
	<span>PREFETCH_ONE_SET</span> <span>(</span><span>1</span><span>,</span> <span>(</span><span>%</span><span>rsi</span><span>),</span> <span>PREFETCHED_LOAD_SIZE</span> <span>*</span> <span>3</span><span>)</span>
  <span>; PREFETCH 256b from rsi+256 to rsi+511</span>

	<span>VMOVU</span>	<span>(</span><span>%</span><span>rsi</span><span>),</span> <span>%</span><span>VEC</span><span>(</span><span>0</span><span>)</span>
	<span>VMOVU</span>	<span>VEC_SIZE</span><span>(</span><span>%</span><span>rsi</span><span>),</span> <span>%</span><span>VEC</span><span>(</span><span>1</span><span>)</span>
	<span>VMOVU</span>	<span>(</span><span>VEC_SIZE</span> <span>*</span> <span>2</span><span>)(</span><span>%</span><span>rsi</span><span>),</span> <span>%</span><span>VEC</span><span>(</span><span>2</span><span>)</span>
	<span>VMOVU</span>	<span>(</span><span>VEC_SIZE</span> <span>*</span> <span>3</span><span>)(</span><span>%</span><span>rsi</span><span>),</span> <span>%</span><span>VEC</span><span>(</span><span>3</span><span>)</span>
  <span>; mov 128b from rsi to rsi+127 -&gt; 4 ymm registers (cache)</span>
  <span>; 2 loops later, we hit the prefetched values</span>

	<span>addq</span>	<span>$</span><span>PREFETCHED_LOAD_SIZE</span><span>,</span> <span>%</span><span>rsi</span>  <span>; advance to rsi+128 in next loop</span>
	<span>subq</span>	<span>$</span><span>PREFETCHED_LOAD_SIZE</span><span>,</span> <span>%</span><span>rdx</span>

	<span>VMOVNT</span>	<span>%</span><span>VEC</span><span>(</span><span>0</span><span>),</span> <span>(</span><span>%</span><span>rdi</span><span>)</span>
	<span>VMOVNT</span>	<span>%</span><span>VEC</span><span>(</span><span>1</span><span>),</span> <span>VEC_SIZE</span><span>(</span><span>%</span><span>rdi</span><span>)</span>
	<span>VMOVNT</span>	<span>%</span><span>VEC</span><span>(</span><span>2</span><span>),</span> <span>(</span><span>VEC_SIZE</span> <span>*</span> <span>2</span><span>)(</span><span>%</span><span>rdi</span><span>)</span>
	<span>VMOVNT</span>	<span>%</span><span>VEC</span><span>(</span><span>3</span><span>),</span> <span>(</span><span>VEC_SIZE</span> <span>*</span> <span>3</span><span>)(</span><span>%</span><span>rdi</span><span>)</span>
  <span>; mov 128b from 4 ymm register -&gt; rdi to rdi+127 (no cache)</span>

	<span>addq</span>	<span>$</span><span>PREFETCHED_LOAD_SIZE</span><span>,</span> <span>%</span><span>rdi</span>  <span>; advance to rdi+128 in next loop</span>
	<span>cmpq</span>	<span>$</span><span>PREFETCHED_LOAD_SIZE</span><span>,</span> <span>%</span><span>rdx</span>
	<span>ja</span>	<span>L</span><span>(</span><span>loop_large_forward</span><span>)</span>
</code></pre></div>

<hr>

<h3 id="method-1-basic-rep-movsb">Method 1: Basic REP MOVSB</h3>

<p>Before getting into more exotic implementations, I wanted to first implement a
super simple version of ERSB to see how well it would perform. I used inline
assembly to write out the loop.</p>

<div><pre><code><span>void</span> <span>_rep_movsb</span><span>(</span><span>void</span> <span>*</span><span>d</span><span>,</span> <span>const</span> <span>void</span> <span>*</span><span>s</span><span>,</span> <span>size_t</span> <span>n</span><span>)</span> <span>{</span>
  <span>asm</span> <span>volatile</span><span>(</span><span>"rep movsb"</span>
               <span>:</span> <span>"=D"</span><span>(</span><span>d</span><span>),</span> <span>"=S"</span><span>(</span><span>s</span><span>),</span> <span>"=c"</span><span>(</span><span>n</span><span>)</span>
               <span>:</span> <span>"0"</span><span>(</span><span>d</span><span>),</span> <span>"1"</span><span>(</span><span>s</span><span>),</span> <span>"2"</span><span>(</span><span>n</span><span>)</span>
               <span>:</span> <span>"memory"</span><span>);</span>
<span>}</span>
</code></pre></div>

<p>This does the same as the pseudo-code attached above, but I wrote it in
assembly to prevent any compiler optimization, and rely only on the hardware
ERMS optimization.</p>

<h3 id="alternate-2-aligned-avx">Alternate 2: Aligned AVX</h3>

<p>One of the complexities in <code>glibc</code>’s  implementation is getting it to work for
unaligned pointers. Since I control the memory allocation, I figured I could
recreate the implementation focused solely on aligned pointer and sizes. I’m
using AVX intrinsics for 32-byte vectors (AVX):</p>

<div><pre><code><span>void</span> <span>_avx_cpy</span><span>(</span><span>void</span> <span>*</span><span>d</span><span>,</span> <span>const</span> <span>void</span> <span>*</span><span>s</span><span>,</span> <span>size_t</span> <span>n</span><span>)</span> <span>{</span>
  <span>// d, s -&gt; 32 byte aligned</span>
  <span>// n -&gt; multiple of 32</span>
  <span>auto</span> <span>*</span><span>dVec</span> <span>=</span> <span>reinterpret_cast</span><span>&lt;</span><span>__m256i</span> <span>*&gt;</span><span>(</span><span>d</span><span>);</span>
  <span>const</span> <span>auto</span> <span>*</span><span>sVec</span> <span>=</span> <span>reinterpret_cast</span><span>&lt;</span><span>const</span> <span>__m256i</span> <span>*&gt;</span><span>(</span><span>s</span><span>);</span>
  <span>size_t</span> <span>nVec</span> <span>=</span> <span>n</span> <span>/</span> <span>sizeof</span><span>(</span><span>__m256i</span><span>);</span>
  <span>for</span> <span>(;</span> <span>nVec</span> <span>&gt;</span> <span>0</span><span>;</span> <span>nVec</span><span>--</span><span>,</span> <span>sVec</span><span>++</span><span>,</span> <span>dVec</span><span>++</span><span>)</span> <span>{</span>
    <span>const</span> <span>__m256i</span> <span>temp</span> <span>=</span> <span>_mm256_load_si256</span><span>(</span><span>sVec</span><span>);</span>
    <span>_mm256_store_si256</span><span>(</span><span>dVec</span><span>,</span> <span>temp</span><span>);</span>
  <span>}</span>
<span>}</span>
</code></pre></div>

<p>The logic is identical to the previous <code>REP MOVSB</code> loop instead operating on 32
bytes at a time.</p>

<h3 id="method-3-stream-aligned-avx">Method 3: Stream aligned AVX</h3>

<p><code>_mm256_load_si256</code> and <code>_mm256_store_si256</code> go through the cache, which incurs
additional overhead. AVX instruction set has <code>_stream_</code> load and store
instructions that skip the cache. The performance of this copy is dependant
on:</p>
<ol>
  <li>Quantity of data to copy</li>
  <li>Cache size</li>
</ol>

<p>Non-temporal moves may bog down the performance for smaller copies (that can
fit into L2 cache) compared to regular moves.</p>

<div><pre><code><span>void</span> <span>_avx_async_cpy</span><span>(</span><span>void</span> <span>*</span><span>d</span><span>,</span> <span>const</span> <span>void</span> <span>*</span><span>s</span><span>,</span> <span>size_t</span> <span>n</span><span>)</span> <span>{</span>
  <span>// d, s -&gt; 32 byte aligned</span>
  <span>// n -&gt; multiple of 32</span>
  <span>auto</span> <span>*</span><span>dVec</span> <span>=</span> <span>reinterpret_cast</span><span>&lt;</span><span>__m256i</span> <span>*&gt;</span><span>(</span><span>d</span><span>);</span>
  <span>const</span> <span>auto</span> <span>*</span><span>sVec</span> <span>=</span> <span>reinterpret_cast</span><span>&lt;</span><span>const</span> <span>__m256i</span> <span>*&gt;</span><span>(</span><span>s</span><span>);</span>
  <span>size_t</span> <span>nVec</span> <span>=</span> <span>n</span> <span>/</span> <span>sizeof</span><span>(</span><span>__m256i</span><span>);</span>
  <span>for</span> <span>(;</span> <span>nVec</span> <span>&gt;</span> <span>0</span><span>;</span> <span>nVec</span><span>--</span><span>,</span> <span>sVec</span><span>++</span><span>,</span> <span>dVec</span><span>++</span><span>)</span> <span>{</span>
    <span>const</span> <span>__m256i</span> <span>temp</span> <span>=</span> <span>_mm256_stream_load_si256</span><span>(</span><span>sVec</span><span>);</span>
    <span>_mm256_stream_si256</span><span>(</span><span>dVec</span><span>,</span> <span>temp</span><span>);</span>
  <span>}</span>
  <span>_mm_sfence</span><span>();</span>
<span>}</span>
</code></pre></div>

<p>Exact code as before but using non-temporal moves instead. There’s an extra
<code>_mm_sfence</code> which guarantees that all stores in the preceding loop are 
visible globally.</p>

<h3 id="method-4-stream-aligned-avx-with-prefetch">Method 4: Stream aligned AVX with prefetch</h3>

<p>In the previous method, we skipped the cache entirely. We can squeeze a bit
more performance by prefetching the source data into the cache for the next
iteration in the current iteration. Since all prefetches work on cache-lines
(64-bytes), each loop iteration copies 64-bytes from source to data.</p>

<div><pre><code><span>void</span> <span>_avx_async_pf_cpy</span><span>(</span><span>void</span> <span>*</span><span>d</span><span>,</span> <span>const</span> <span>void</span> <span>*</span><span>s</span><span>,</span> <span>size_t</span> <span>n</span><span>)</span> <span>{</span>
  <span>// d, s -&gt; 64 byte aligned</span>
  <span>// n -&gt; multiple of 64</span>

  <span>auto</span> <span>*</span><span>dVec</span> <span>=</span> <span>reinterpret_cast</span><span>&lt;</span><span>__m256i</span> <span>*&gt;</span><span>(</span><span>d</span><span>);</span>
  <span>const</span> <span>auto</span> <span>*</span><span>sVec</span> <span>=</span> <span>reinterpret_cast</span><span>&lt;</span><span>const</span> <span>__m256i</span> <span>*&gt;</span><span>(</span><span>s</span><span>);</span>
  <span>size_t</span> <span>nVec</span> <span>=</span> <span>n</span> <span>/</span> <span>sizeof</span><span>(</span><span>__m256i</span><span>);</span>
  <span>for</span> <span>(;</span> <span>nVec</span> <span>&gt;</span> <span>2</span><span>;</span> <span>nVec</span> <span>-=</span> <span>2</span><span>,</span> <span>sVec</span> <span>+=</span> <span>2</span><span>,</span> <span>dVec</span> <span>+=</span> <span>2</span><span>)</span> <span>{</span>
    <span>// prefetch the next iteration's data</span>
    <span>// by default _mm_prefetch moves the entire cache-lint (64b)</span>
    <span>_mm_prefetch</span><span>(</span><span>sVec</span> <span>+</span> <span>2</span><span>,</span> <span>_MM_HINT_T0</span><span>);</span>

    <span>_mm256_stream_si256</span><span>(</span><span>dVec</span><span>,</span> <span>_mm256_load_si256</span><span>(</span><span>sVec</span><span>));</span>
    <span>_mm256_stream_si256</span><span>(</span><span>dVec</span> <span>+</span> <span>1</span><span>,</span> <span>_mm256_load_si256</span><span>(</span><span>sVec</span> <span>+</span> <span>1</span><span>));</span>
  <span>}</span>
  <span>_mm256_stream_si256</span><span>(</span><span>dVec</span><span>,</span> <span>_mm256_load_si256</span><span>(</span><span>sVec</span><span>));</span>
  <span>_mm256_stream_si256</span><span>(</span><span>dVec</span> <span>+</span> <span>1</span><span>,</span> <span>_mm256_load_si256</span><span>(</span><span>sVec</span> <span>+</span> <span>1</span><span>));</span>
  <span>_mm_sfence</span><span>();</span>
<span>}</span>
</code></pre></div>

<p>The load from source pointer to register should <strong>not</strong> skip the cache since
that data is explicitly prefetched into the cache, non-stream
<code>_mm256_load_si256</code> must be used instead.</p>

<p>This also unrolls the loop for 2 copies at a time instead of a single copy.
This is to guarantee that each loop iteration’s prefetch coincides the copy.
Prefetch the next 64-bytes and copy the current 64-bytes.</p>

<hr>

<h2 id="alternate-avenues">Alternate avenues</h2>

<h3 id="unrolling">Unrolling</h3>

<p>In the previous section, most of the changes were in the actual underlying
load, store instructions used. Another avenue of exploration is to unroll the
loop for a certain number of iterations. This reduces the number of branch
statements by the factor of unrolling.</p>

<p>In the <code>glibc</code> implementation the unrolling factor is 4 which is what I’ll use
as well. A very simple way to implement this is to increase the alignment 
required by 4x and treat each loop as 4 instructions that copy 4x data.</p>

<p>A more complicated version would be trying to implement an unrolled loop
without increasing alignment size. We’ll need to copy using a regular fully
rolled loop till we hit a pointer location that is aligned to the size expected
by our unrolled loop.</p>

<p>Unrolling the aligned AVX copy:</p>

<div><pre><code><span>void</span> <span>_avx_cpy_unroll</span><span>(</span><span>void</span> <span>*</span><span>d</span><span>,</span> <span>const</span> <span>void</span> <span>*</span><span>s</span><span>,</span> <span>size_t</span> <span>n</span><span>)</span> <span>{</span>
  <span>// d, s -&gt; 128 byte aligned</span>
  <span>// n -&gt; multiple of 128</span>

  <span>auto</span> <span>*</span><span>dVec</span> <span>=</span> <span>reinterpret_cast</span><span>&lt;</span><span>__m256i</span> <span>*&gt;</span><span>(</span><span>d</span><span>);</span>
  <span>const</span> <span>auto</span> <span>*</span><span>sVec</span> <span>=</span> <span>reinterpret_cast</span><span>&lt;</span><span>const</span> <span>__m256i</span> <span>*&gt;</span><span>(</span><span>s</span><span>);</span>
  <span>size_t</span> <span>nVec</span> <span>=</span> <span>n</span> <span>/</span> <span>sizeof</span><span>(</span><span>__m256i</span><span>);</span>
  <span>for</span> <span>(;</span> <span>nVec</span> <span>&gt;</span> <span>0</span><span>;</span> <span>nVec</span> <span>-=</span> <span>4</span><span>,</span> <span>sVec</span> <span>+=</span> <span>4</span><span>,</span> <span>dVec</span> <span>+=</span> <span>4</span><span>)</span> <span>{</span>
    <span>_mm256_store_si256</span><span>(</span><span>dVec</span><span>,</span> <span>_mm256_load_si256</span><span>(</span><span>sVec</span><span>));</span>
    <span>_mm256_store_si256</span><span>(</span><span>dVec</span> <span>+</span> <span>1</span><span>,</span> <span>_mm256_load_si256</span><span>(</span><span>sVec</span> <span>+</span> <span>1</span><span>));</span>
    <span>_mm256_store_si256</span><span>(</span><span>dVec</span> <span>+</span> <span>2</span><span>,</span> <span>_mm256_load_si256</span><span>(</span><span>sVec</span> <span>+</span> <span>2</span><span>));</span>
    <span>_mm256_store_si256</span><span>(</span><span>dVec</span> <span>+</span> <span>3</span><span>,</span> <span>_mm256_load_si256</span><span>(</span><span>sVec</span> <span>+</span> <span>3</span><span>));</span>
  <span>}</span>
<span>}</span>
</code></pre></div>

<h3 id="multithreading">Multithreading</h3>

<p>The operation of copying data is super easy to parallelize across multiple
threads. The total data to be transferred can be segmented into (almost)
equal chunks, and then copied over using one of the above methods. This will
make the copy super-fast especially if the CPU has a large core count.</p>

<hr>

<h2 id="shadesmar-api">Shadesmar API</h2>

<p>To make it easy to integrate custom memory copying logic into the library,
I introduced the concept of <code>Copier</code> in <a href="https://github.com/Squadrick/shadesmar/commit/22dc762ca658d1396f3c00366e80e4f695189df9">this commit</a>.
For a new copying algorithm, an abstract class <code>Copier</code> must be implemented.</p>

<p>Here’s the definition of <code>Copier</code>:</p>

<div><pre><code><span>class</span> <span>Copier</span> <span>{</span>
 <span>public:</span>
  <span>virtual</span> <span>void</span> <span>*</span><span>alloc</span><span>(</span><span>size_t</span><span>)</span> <span>=</span> <span>0</span><span>;</span>
  <span>virtual</span> <span>void</span> <span>dealloc</span><span>(</span><span>void</span> <span>*</span><span>)</span> <span>=</span> <span>0</span><span>;</span>
  <span>virtual</span> <span>void</span> <span>shm_to_user</span><span>(</span><span>void</span> <span>*</span><span>,</span> <span>void</span> <span>*</span><span>,</span> <span>size_t</span><span>)</span> <span>=</span> <span>0</span><span>;</span>
  <span>virtual</span> <span>void</span> <span>user_to_shm</span><span>(</span><span>void</span> <span>*</span><span>,</span> <span>void</span> <span>*</span><span>,</span> <span>size_t</span><span>)</span> <span>=</span> <span>0</span><span>;</span>
<span>};</span>
</code></pre></div>

<p>The original reason for introducing this construct was to allow cross-device
usage, where a custom copier would be implemented to tranfer between CPU and
GPU. E.g.: using <code>cudaMemcpy</code> for Nvidia GPUs.</p>

<p>For a single device use case the implementation of <code>shm_to_user</code> and 
<code>user_to_shm</code> are identical. The implementation of a copier that uses
<code>std::memcpy</code>:</p>

<div><pre><code><span>class</span> <span>DefaultCopier</span> <span>:</span> <span>public</span> <span>Copier</span> <span>{</span>
 <span>public:</span>
  <span>void</span> <span>*</span><span>alloc</span><span>(</span><span>size_t</span> <span>size</span><span>)</span> <span>override</span> <span>{</span> <span>return</span> <span>malloc</span><span>(</span><span>size</span><span>);</span> <span>}</span>

  <span>void</span> <span>dealloc</span><span>(</span><span>void</span> <span>*</span><span>ptr</span><span>)</span> <span>override</span> <span>{</span> <span>free</span><span>(</span><span>ptr</span><span>);</span> <span>}</span>

  <span>void</span> <span>shm_to_user</span><span>(</span><span>void</span> <span>*</span><span>dst</span><span>,</span> <span>void</span> <span>*</span><span>src</span><span>,</span> <span>size_t</span> <span>size</span><span>)</span> <span>override</span> <span>{</span>
    <span>std</span><span>::</span><span>memcpy</span><span>(</span><span>dst</span><span>,</span> <span>src</span><span>,</span> <span>size</span><span>);</span>
  <span>}</span>

  <span>void</span> <span>user_to_shm</span><span>(</span><span>void</span> <span>*</span><span>dst</span><span>,</span> <span>void</span> <span>*</span><span>src</span><span>,</span> <span>size_t</span> <span>size</span><span>)</span> <span>override</span> <span>{</span>
    <span>std</span><span>::</span><span>memcpy</span><span>(</span><span>dst</span><span>,</span> <span>src</span><span>,</span> <span>size</span><span>);</span>
  <span>}</span>
<span>};</span>
</code></pre></div>

<p>I also created an adapter <code>MTCopier</code> that adds multithreading support to other
copiers:</p>

<div><pre><code><span>template</span> <span>&lt;</span><span>class</span> <span>BaseCopierT</span><span>&gt;</span> 
<span>class</span> <span>MTCopier</span> <span>:</span> <span>public</span> <span>Copier</span> <span>{</span>
<span>public:</span>
  <span>explicit</span> <span>MTCopier</span><span>(</span><span>uint32_t</span> <span>threads</span> <span>=</span> <span>std</span><span>::</span><span>thread</span><span>::</span><span>hardware_concurrency</span><span>())</span>
      <span>:</span> <span>base_copier</span><span>(</span><span>base_copier</span><span>),</span> <span>nthreads</span><span>(</span><span>threads</span><span>)</span> <span>{}</span>

  <span>void</span> <span>*</span><span>alloc</span><span>(</span><span>size_t</span> <span>size</span><span>)</span> <span>override</span> <span>{</span> <span>return</span> <span>base_copier</span><span>.</span><span>alloc</span><span>(</span><span>size</span><span>);</span> <span>}</span>

  <span>void</span> <span>dealloc</span><span>(</span><span>void</span> <span>*</span><span>ptr</span><span>)</span> <span>override</span> <span>{</span> <span>base_copier</span><span>.</span><span>dealloc</span><span>(</span><span>ptr</span><span>);</span> <span>}</span>

  <span>void</span> <span>_copy</span><span>(</span><span>void</span> <span>*</span><span>d</span><span>,</span> <span>void</span> <span>*</span><span>s</span><span>,</span> <span>size_t</span> <span>n</span><span>,</span> <span>bool</span> <span>shm_to_user</span><span>)</span> <span>{</span>
    <span>std</span><span>::</span><span>vector</span><span>&lt;</span><span>std</span><span>::</span><span>thread</span><span>&gt;</span> <span>threads</span><span>;</span>
    <span>threads</span><span>.</span><span>reserve</span><span>(</span><span>nthreads</span><span>);</span>

    <span>ldiv_t</span> <span>per_worker</span> <span>=</span> <span>div</span><span>((</span><span>int64_t</span><span>)</span><span>n</span><span>,</span> <span>nthreads</span><span>);</span>

    <span>size_t</span> <span>next_start</span> <span>=</span> <span>0</span><span>;</span>
    <span>for</span> <span>(</span><span>uint32_t</span> <span>thread_idx</span> <span>=</span> <span>0</span><span>;</span> <span>thread_idx</span> <span>&lt;</span> <span>nthreads</span><span>;</span> <span>++</span><span>thread_idx</span><span>)</span> <span>{</span>
      <span>const</span> <span>size_t</span> <span>curr_start</span> <span>=</span> <span>next_start</span><span>;</span>
      <span>next_start</span> <span>+=</span> <span>per_worker</span><span>.</span><span>quot</span><span>;</span>
      <span>if</span> <span>(</span><span>thread_idx</span> <span>&lt;</span> <span>per_worker</span><span>.</span><span>rem</span><span>)</span> <span>{</span>
        <span>++</span><span>next_start</span><span>;</span>
      <span>}</span>
      <span>uint8_t</span> <span>*</span><span>d_thread</span> <span>=</span> <span>reinterpret_cast</span><span>&lt;</span><span>uint8_t</span> <span>*&gt;</span><span>(</span><span>d</span><span>)</span> <span>+</span> <span>curr_start</span><span>;</span>
      <span>uint8_t</span> <span>*</span><span>s_thread</span> <span>=</span> <span>reinterpret_cast</span><span>&lt;</span><span>uint8_t</span> <span>*&gt;</span><span>(</span><span>s</span><span>)</span> <span>+</span> <span>curr_start</span><span>;</span>

      <span>if</span> <span>(</span><span>shm_to_user</span><span>)</span> <span>{</span>
        <span>threads</span><span>.</span><span>emplace_back</span><span>(</span><span>&amp;</span><span>Copier</span><span>::</span><span>shm_to_user</span><span>,</span> <span>&amp;</span><span>base_copier</span><span>,</span> <span>d_thread</span><span>,</span>
                             <span>s_thread</span><span>,</span> <span>next_start</span> <span>-</span> <span>curr_start</span><span>);</span>
      <span>}</span> <span>else</span> <span>{</span>
        <span>threads</span><span>.</span><span>emplace_back</span><span>(</span><span>&amp;</span><span>Copier</span><span>::</span><span>user_to_shm</span><span>,</span> <span>&amp;</span><span>base_copier</span><span>,</span> <span>d_thread</span><span>,</span>
                             <span>s_thread</span><span>,</span> <span>next_start</span> <span>-</span> <span>curr_start</span><span>);</span>
      <span>}</span>
    <span>}</span>
    <span>for</span> <span>(</span><span>auto</span> <span>&amp;</span><span>thread</span> <span>:</span> <span>threads</span><span>)</span> <span>{</span>
      <span>thread</span><span>.</span><span>join</span><span>();</span>
    <span>}</span>
    <span>threads</span><span>.</span><span>clear</span><span>();</span>
  <span>}</span>

  <span>void</span> <span>shm_to_user</span><span>(</span><span>void</span> <span>*</span><span>dst</span><span>,</span> <span>void</span> <span>*</span><span>src</span><span>,</span> <span>size_t</span> <span>size</span><span>)</span> <span>override</span> <span>{</span>
    <span>_copy</span><span>(</span><span>dst</span><span>,</span> <span>src</span><span>,</span> <span>size</span><span>,</span> <span>true</span><span>);</span>
  <span>}</span>

  <span>void</span> <span>user_to_shm</span><span>(</span><span>void</span> <span>*</span><span>dst</span><span>,</span> <span>void</span> <span>*</span><span>src</span><span>,</span> <span>size_t</span> <span>size</span><span>)</span> <span>override</span> <span>{</span>
    <span>_copy</span><span>(</span><span>dst</span><span>,</span> <span>src</span><span>,</span> <span>size</span><span>,</span> <span>false</span><span>);</span>
  <span>}</span>

<span>private:</span>
  <span>BaseCopierT</span> <span>base_copier</span><span>;</span>
  <span>uint32_t</span> <span>nthreads</span><span>;</span>
<span>};</span>
</code></pre></div>

<p>Currently this only works for <code>memcpy</code> and <code>_rep_movsb</code> since the
implementation expects the memory copy to work for unaligned memory.</p>

<hr>

<h2 id="benchmark">Benchmark</h2>

<p>I used Google’s <a href="https://github.com/google/benchmark">Benchmark</a> for timing
the performance of copying data ranging from size of 32kB to 64MB. All the
benchmarks were run on my PC with the following specifications:</p>
<ol>
  <li>AMD Ryzen 7 3700X</li>
  <li>2x8GB DDR4 RAM @ 3600Mhz</li>
</ol>










<h3 id="conclusion">Conclusion</h3>

<p>Stick to <code>std::memcpy</code>. It delivers great performance while also adapting to
the hardware architecture, and makes no assumptions about the memory alignment.</p>

<p>If performance truly matters, then you might want to consider using a more
specific non-genetic implementation with alignment requirements. The streaming
prefetching copy works the best for larger copies (&gt;1MB), but the performance
for small sizes is abyssal, but <code>memcpy</code> matches its performance. For small to
medium sizes Unrolled AVX absolutely dominates, but as for larger messages, it
is slower than the streaming alternatives. The regular <code>RepMovsb</code> is by far the
worst overall performer as excepted.</p>

<p>Unrolling definitely improves performance in most cases by about 5-10%. The
only case where the unrolled version is slower than rolled version is for
<code>AvxCopier</code> with data size of 32B, which the unrolled version is 25% slower.
The rolled version will do a single AVX-256 load/store and a conditional check.
The unrolled version will do 4 AVX-256 load/stores and a conditional check.</p>

<h3 id="code">Code</h3>

<p>Code for all the methods is included in the library conforming to the above
mentioned API. To actively warn about the danger of using these custom copiers
I have named this file <a href="https://github.com/Squadrick/shadesmar/blob/master/include/shadesmar/memory/dragons.h"><code>dragons.h</code></a>,
with an apt message: <em>Here be dragons</em>.</p>


<p><span>
  Written on
  
  May
  24th,
  2020
  by
  
    Dheeraj R Reddy
  
</span>

    </p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Vanishing from Hyundai’s data network (350 pts)]]></title>
            <link>http://techno-fandom.org/~hobbit/cars/ev/offnet.html</link>
            <guid>44860139</guid>
            <pubDate>Mon, 11 Aug 2025 01:55:26 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="http://techno-fandom.org/~hobbit/cars/ev/offnet.html">http://techno-fandom.org/~hobbit/cars/ev/offnet.html</a>, See on <a href="https://news.ycombinator.com/item?id=44860139">Hacker News</a></p>
<div id="readability-page-1" class="page"><div width="85%">
<tbody><tr><td>&nbsp;</td><td>
The Yuppie Button page talks about making lots of light.&nbsp;
Now I needed to do the opposite, by "going dark" -- to vanish completely
from Hyundai's data network, and avoid having the car being tracked
or actively interfered with outside of my control.&nbsp;
See, this is one of the showstopping problems I have with Tesla -- they
*insist* that you have your car online all the time, talking to Tesla's
cloud and sending telematic data.&nbsp;
Thank you, NO.&nbsp;
The <a href="https://www.hyundaiusa.com/bluelink/index.aspx">
range of things</a>
that Hyundai's BlueLink setup is able to
do remotely to someone's car given only a VIN is totally scary.&nbsp;
Not only did I want no parts of that, we all have every right to not
participate in that nonsense if we so choose.
<p>

As a first step I refused to let the dealer sign me for BlueLink, telling
them that I could handle signup later myself if I wanted to.&nbsp;
But I knew there was more to it, since the car as it came was still able
to make a cellular data connection and send information about itself.&nbsp;
The obvious question was to find and disable the cellular communication
facility, or "telematics unit" as it is implemented in many vehicles.
</p></td></tr></tbody></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Tesla remotely deactivates rapper's vehicle for singing about the Cybertruck (211 pts)]]></title>
            <link>https://www.threads.com/@brittainforsenate/post/DNMcEZ9yOxk</link>
            <guid>44859807</guid>
            <pubDate>Mon, 11 Aug 2025 00:56:37 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.threads.com/@brittainforsenate/post/DNMcEZ9yOxk">https://www.threads.com/@brittainforsenate/post/DNMcEZ9yOxk</a>, See on <a href="https://news.ycombinator.com/item?id=44859807">Hacker News</a></p>
&lt;Unparsable&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[Compiling a Lisp: Lambda lifting (138 pts)]]></title>
            <link>https://bernsteinbear.com/blog/compiling-a-lisp-12/</link>
            <guid>44858892</guid>
            <pubDate>Sun, 10 Aug 2025 22:35:03 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://bernsteinbear.com/blog/compiling-a-lisp-12/">https://bernsteinbear.com/blog/compiling-a-lisp-12/</a>, See on <a href="https://news.ycombinator.com/item?id=44858892">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
            <p><span data-nosnippet="">
<em><a href="https://bernsteinbear.com/blog/compiling-a-lisp-0/">first</a></em> – <em><a href="https://bernsteinbear.com/blog/compiling-a-lisp-11/">previous</a></em>
</span></p>

<p><em>EDIT: /u/thunderseethe correctly points out that this is closure conversion,
not lambda lifting, so I have adjusted the post title from “lambda lifting” to
“closure conversion” accordingly. Thanks!</em></p>

<p>I didn’t think this day would come, but I picked up the <a href="https://bernsteinbear.com/assets/img/11-ghuloum.pdf">Ghuloum
tutorial</a> (PDF) again and I got a little bit further. There’s just
one caveat: I have rewritten the implementation in Python. It’s available in
the <a href="https://github.com/tekknolagi/ghuloum">same repo</a> in
<a href="https://github.com/tekknolagi/ghuloum/blob/trunk/compiler.py">compiler.py</a>.
It’s brief, coming in at a little over 300 LOC + tests (compared to the C
version’s 1200 LOC + tests).</p>

<p>I guess there’s another caveat, too, which is that the Python version has no
S-expression reader. But that’s fine: consider it an exercise for you, dear
reader. That’s hardly the most interesting part of the tutorial.</p>

<p>Oh, and I also dropped the instruction encoding. I’m doing text assembly now.
Womp womp.</p>

<p>Anyway, converting the lambdas as required in the paper requires three things:</p>

<ul>
  <li>Keeping track of which variables are bound</li>
  <li>Keeping track of which variables are free in a given lambda</li>
  <li>Keeping a running list of <code>code</code> objects that we create as we recurse</li>
</ul>

<p>We have two forms that can bind variables: <code>let</code> and <code>lambda</code>. This means that
we need to recognize the names in those special expressions and modify the
environment. What environment, you ask?</p>

<h3 id="the-closure-converter">The closure converter</h3>

<p>Well, I have this little <code>LambdaConverter</code> class.</p>

<div><pre><code><span>class</span> <span>LambdaConverter</span><span>:</span>
    <span>def</span> <span>__init__</span><span>(</span><span>self</span><span>):</span>
        <span>self</span><span>.</span><span>labels</span><span>:</span> <span>dict</span><span>[</span><span>str</span><span>,</span> <span>list</span><span>]</span> <span>=</span> <span>{}</span>

    <span>def</span> <span>convert</span><span>(</span><span>self</span><span>,</span> <span>expr</span><span>,</span> <span>bound</span><span>:</span> <span>set</span><span>[</span><span>str</span><span>],</span> <span>free</span><span>:</span> <span>set</span><span>[</span><span>str</span><span>]):</span>
        <span>match</span> <span>expr</span><span>:</span>
            <span>case</span> <span>_</span><span>:</span>
                <span>raise</span> <span>NotImplementedError</span><span>(</span><span>expr</span><span>)</span>

<span>def</span> <span>convert_lambdas</span><span>(</span><span>expr</span><span>):</span>
    <span>conv</span> <span>=</span> <span>LambdaConverter</span><span>()</span>
    <span>expr</span> <span>=</span> <span>conv</span><span>.</span><span>convert</span><span>(</span><span>expr</span><span>,</span> <span>set</span><span>(),</span> <span>set</span><span>())</span>
    <span>labels</span> <span>=</span> <span>[[</span><span>name</span><span>,</span> <span>code</span><span>]</span> <span>for</span> <span>name</span><span>,</span> <span>code</span> <span>in</span> <span>conv</span><span>.</span><span>labels</span><span>.</span><span>items</span><span>()]</span>
    <span>return</span> <span>[</span><span>"labels"</span><span>,</span> <span>labels</span><span>,</span> <span>expr</span><span>]</span>
</code></pre></div>

<p>We keep the same <code>labels</code> dict for the entire recursive traversal of the
program, but we modify <code>bound</code> at each binding site and <code>free</code> only at lambdas.</p>

<p>To illustrate how they are used, let’s fill in some sample expressions: <code>3</code>,
<code>'a</code>, and <code>#t</code>:</p>

<div><pre><code><span>class</span> <span>LambdaConverter</span><span>:</span>
    <span># ...
</span>    <span>def</span> <span>convert</span><span>(</span><span>self</span><span>,</span> <span>expr</span><span>,</span> <span>bound</span><span>,</span> <span>free</span><span>):</span>
        <span>match</span> <span>expr</span><span>:</span>
            <span>case</span> <span>int</span><span>(</span><span>_</span><span>)</span> <span>|</span> <span>Char</span><span>():</span>  <span># bool(_) is implied by int(_)
</span>                <span>return</span> <span>expr</span>
            <span># ...
</span>
<span>class</span> <span>LambdaTests</span><span>(</span><span>unittest</span><span>.</span><span>TestCase</span><span>):</span>
    <span>def</span> <span>test_int</span><span>(</span><span>self</span><span>):</span>
        <span>self</span><span>.</span><span>assertEqual</span><span>(</span><span>convert_lambdas</span><span>(</span><span>3</span><span>),</span> <span>[</span><span>"labels"</span><span>,</span> <span>[],</span> <span>3</span><span>])</span>

    <span>def</span> <span>test_bool</span><span>(</span><span>self</span><span>):</span>
        <span>self</span><span>.</span><span>assertEqual</span><span>(</span><span>convert_lambdas</span><span>(</span><span>True</span><span>),</span> <span>[</span><span>"labels"</span><span>,</span> <span>[],</span> <span>True</span><span>])</span>
        <span>self</span><span>.</span><span>assertEqual</span><span>(</span><span>convert_lambdas</span><span>(</span><span>False</span><span>),</span> <span>[</span><span>"labels"</span><span>,</span> <span>[],</span> <span>False</span><span>])</span>

    <span>def</span> <span>test_char</span><span>(</span><span>self</span><span>):</span>
        <span>self</span><span>.</span><span>assertEqual</span><span>(</span><span>convert_lambdas</span><span>(</span><span>Char</span><span>(</span><span>"a"</span><span>)),</span> <span>[</span><span>"labels"</span><span>,</span> <span>[],</span> <span>Char</span><span>(</span><span>"a"</span><span>)])</span>
</code></pre></div>

<p>Well, okay, sure, we don’t actually need to think about variable names when we
are dealing with simple constants.</p>

<p>So let’s look at variables:</p>

<div><pre><code><span>class</span> <span>LambdaConverter</span><span>:</span>
    <span># ...
</span>    <span>def</span> <span>convert</span><span>(</span><span>self</span><span>,</span> <span>expr</span><span>,</span> <span>bound</span><span>,</span> <span>free</span><span>):</span>
        <span>match</span> <span>expr</span><span>:</span>
            <span># ...
</span>            <span>case</span> <span>str</span><span>(</span><span>_</span><span>)</span> <span>if</span> <span>expr</span> <span>in</span> <span>bound</span><span>:</span>
                <span>return</span> <span>expr</span>
            <span>case</span> <span>str</span><span>(</span><span>_</span><span>):</span>
                <span>free</span><span>.</span><span>add</span><span>(</span><span>expr</span><span>)</span>
                <span>return</span> <span>expr</span>
            <span># ...
</span>
<span>class</span> <span>LambdaTests</span><span>(</span><span>unittest</span><span>.</span><span>TestCase</span><span>):</span>
    <span># ...
</span>    <span>def</span> <span>test_freevar</span><span>(</span><span>self</span><span>):</span>
        <span>self</span><span>.</span><span>assertEqual</span><span>(</span><span>convert_lambdas</span><span>(</span><span>"x"</span><span>),</span> <span>[</span><span>"labels"</span><span>,</span> <span>[],</span> <span>"x"</span><span>])</span>
</code></pre></div>

<p>We don’t want to actually transform the variable uses, just add some metadata
about their uses. If we have some variable <code>x</code> bound by a <code>let</code> or a <code>lambda</code>
expression, we can leave it alone. Otherwise, we need to mark it.</p>

<div><pre><code><span>(</span><span>let</span> <span>((</span><span>x</span> <span>5</span><span>))</span>
  <span>(</span><span>+</span> <span>x</span>        <span>; bound</span>
     <span>y</span><span>))</span>      <span>; free</span>
</code></pre></div>

<p>There’s one irritating special case here which is that we don’t want to
consider <code>+</code> (for example) as a free variable: it is a special language
primitive. So we consider <code>+</code> and the others as always bound.</p>

<div><pre><code><span>class</span> <span>LambdaConverter</span><span>:</span>
    <span># ...
</span>    <span>def</span> <span>convert</span><span>(</span><span>self</span><span>,</span> <span>expr</span><span>,</span> <span>bound</span><span>,</span> <span>free</span><span>):</span>
        <span>match</span> <span>expr</span><span>:</span>
            <span># ...
</span>            <span>case</span> <span>str</span><span>(</span><span>_</span><span>)</span> <span>if</span> <span>expr</span> <span>in</span> <span>BUILTINS</span><span>:</span>
                <span>return</span> <span>expr</span>
            <span># ...
</span>
<span>class</span> <span>LambdaTests</span><span>(</span><span>unittest</span><span>.</span><span>TestCase</span><span>):</span>
    <span># ...
</span>    <span>def</span> <span>test_plus</span><span>(</span><span>self</span><span>):</span>
        <span>self</span><span>.</span><span>assertEqual</span><span>(</span><span>convert_lambdas</span><span>(</span><span>"+"</span><span>),</span> <span>[</span><span>"labels"</span><span>,</span> <span>[],</span> <span>"+"</span><span>])</span>
</code></pre></div>

<p>Armed with this knowledge, we can do our first recursive traversal: <code>if</code>
expressions. Since they have recursive parts and don’t bind any variables, they
are the second-simplest form for this converter.</p>

<div><pre><code><span>class</span> <span>LambdaConverter</span><span>:</span>
    <span># ...
</span>    <span>def</span> <span>convert</span><span>(</span><span>self</span><span>,</span> <span>expr</span><span>,</span> <span>bound</span><span>,</span> <span>free</span><span>):</span>
        <span>match</span> <span>expr</span><span>:</span>
            <span># ...
</span>            <span>case</span> <span>[</span><span>"if"</span><span>,</span> <span>test</span><span>,</span> <span>conseq</span><span>,</span> <span>alt</span><span>]:</span>
                <span>return</span> <span>[</span><span>"if"</span><span>,</span>
                        <span>self</span><span>.</span><span>convert</span><span>(</span><span>test</span><span>,</span> <span>bound</span><span>,</span> <span>free</span><span>),</span>
                        <span>self</span><span>.</span><span>convert</span><span>(</span><span>conseq</span><span>,</span> <span>bound</span><span>,</span> <span>free</span><span>),</span>
                        <span>self</span><span>.</span><span>convert</span><span>(</span><span>alt</span><span>,</span> <span>bound</span><span>,</span> <span>free</span><span>)]</span>
            <span># ...
</span>
<span>class</span> <span>LambdaTests</span><span>(</span><span>unittest</span><span>.</span><span>TestCase</span><span>):</span>
    <span># ...
</span>    <span>def</span> <span>test_if</span><span>(</span><span>self</span><span>):</span>
        <span>self</span><span>.</span><span>assertEqual</span><span>(</span><span>convert_lambdas</span><span>([</span><span>"if"</span><span>,</span> <span>1</span><span>,</span> <span>2</span><span>,</span> <span>3</span><span>]),</span>
                         <span>[</span><span>"labels"</span><span>,</span> <span>[],</span> <span>[</span><span>"if"</span><span>,</span> <span>1</span><span>,</span> <span>2</span><span>,</span> <span>3</span><span>]])</span>
</code></pre></div>

<p>This test doesn’t tell us much yet (other than adding an empty <code>labels</code> and not
raising an exception). But it will soon.</p>

<h3 id="lambda">Lambda</h3>

<p>Let’s think about what <code>lambda</code> does. It’s a bunch of features in a trench
coat:</p>

<ul>
  <li>bind names</li>
  <li>allocate code</li>
  <li>capture outside environment</li>
</ul>

<p>To handle the closure conversion, we have to reason about all three.</p>

<p>First, the lambda binds its parameters as new names. In fact, those are the
<em>only</em> bound variables in a lambda. Consider:</p>



<p><code>x</code> is a free variable in that lambda! We’ll want to transform that lambda
into:</p>

<div><pre><code><span>;                  +-parameters</span>
<span>;                  |  +-freevars</span>
<span>;                  v  v</span>
<span>(</span><span>labels</span> <span>((</span><span>f0</span> <span>(</span><span>code</span> <span>()</span> <span>(</span><span>x</span><span>)</span> <span>x</span><span>)))</span>
  <span>(</span><span>closure</span> <span>f0</span> <span>x</span><span>))</span>
</code></pre></div>

<p>Even if <code>x</code> were bound by some <code>let</code> outside the lambda, it would be free in
the lambda:</p>

<div><pre><code><span>(</span><span>let</span> <span>((</span><span>x</span> <span>5</span><span>))</span>
  <span>(</span><span>lambda</span> <span>()</span> <span>x</span><span>))</span>
</code></pre></div>

<p>That means we don’t thread through the <code>bound</code> parameter to the lambda body; we
don’t care what names are bound <em>outside</em> the lambda.</p>

<p>We also want to keep track of the set of variables that are free inside the
lambda: we’ll need them to create a <code>code</code> form. Therefore, we also pass in a
new set for the lambda body’s <code>free</code> set.</p>

<p>So far, all of this environment wrangling gives us:</p>

<div><pre><code><span>class</span> <span>LambdaConverter</span><span>:</span>
    <span># ...
</span>    <span>def</span> <span>convert</span><span>(</span><span>self</span><span>,</span> <span>expr</span><span>,</span> <span>bound</span><span>,</span> <span>free</span><span>):</span>
        <span>match</span> <span>expr</span><span>:</span>
            <span># ...
</span>            <span>case</span> <span>[</span><span>"lambda"</span><span>,</span> <span>params</span><span>,</span> <span>body</span><span>]:</span>
                <span>body_free</span> <span>=</span> <span>set</span><span>()</span>
                <span>body</span> <span>=</span> <span>self</span><span>.</span><span>convert</span><span>(</span><span>body</span><span>,</span> <span>set</span><span>(</span><span>params</span><span>),</span> <span>body_free</span><span>)</span>
                <span>free</span><span>.</span><span>update</span><span>(</span><span>body_free</span> <span>-</span> <span>bound</span><span>)</span>
                <span># ...
</span>                <span>return</span> <span># ???
</span>            <span># ...
</span></code></pre></div>

<p>There’s also <code>free.update(body_free - bound)</code> in there because any variable
free in a lambda expression is also free in the current expression—well,
except for the variables that are currently bound.</p>

<p>Last, we’ll make a <code>code</code> form and a <code>closure</code> form. The <code>code</code> gets appended
to the global list with a new label and the label gets threaded through to the
<code>closure</code>.</p>

<div><pre><code><span>class</span> <span>LambdaConverter</span><span>:</span>
    <span>def</span> <span>push_label</span><span>(</span><span>self</span><span>,</span> <span>params</span><span>,</span> <span>freevars</span><span>,</span> <span>body</span><span>):</span>
        <span>result</span> <span>=</span> <span>f</span><span>"f</span><span>{</span><span>len</span><span>(</span><span>self</span><span>.</span><span>labels</span><span>)</span><span>}</span><span>"</span>
        <span>self</span><span>.</span><span>labels</span><span>[</span><span>result</span><span>]</span> <span>=</span> <span>[</span><span>"code"</span><span>,</span> <span>params</span><span>,</span> <span>freevars</span><span>,</span> <span>body</span><span>]</span>
        <span>return</span> <span>result</span>

    <span>def</span> <span>convert</span><span>(</span><span>self</span><span>,</span> <span>expr</span><span>,</span> <span>bound</span><span>,</span> <span>free</span><span>):</span>
        <span>match</span> <span>expr</span><span>:</span>
            <span># ...
</span>            <span>case</span> <span>[</span><span>"lambda"</span><span>,</span> <span>params</span><span>,</span> <span>body</span><span>]:</span>
                <span>body_free</span> <span>=</span> <span>set</span><span>()</span>
                <span>body</span> <span>=</span> <span>self</span><span>.</span><span>convert</span><span>(</span><span>body</span><span>,</span> <span>set</span><span>(</span><span>params</span><span>),</span> <span>body_free</span><span>)</span>
                <span>free</span><span>.</span><span>update</span><span>(</span><span>body_free</span> <span>-</span> <span>bound</span><span>)</span>
                <span># vvvv new below this line vvvv
</span>                <span>body_free</span> <span>=</span> <span>sorted</span><span>(</span><span>body_free</span><span>)</span>
                <span>label</span> <span>=</span> <span>self</span><span>.</span><span>push_label</span><span>(</span><span>params</span><span>,</span> <span>body_free</span><span>,</span> <span>body</span><span>)</span>
                <span>return</span> <span>[</span><span>"closure"</span><span>,</span> <span>label</span><span>,</span> <span>*</span><span>body_free</span><span>]</span>
            <span># ...
</span></code></pre></div>

<p>This is finicky! I think my first couple of versions were subtly wrong for
different reasons. Tests help a lot here. For every place in the code where I
mess with <code>bound</code> or <code>free</code> in a recursive call, I tried to have a test that
would fail if I got it wrong.</p>

<div><pre><code><span>class</span> <span>LambdaTests</span><span>(</span><span>unittest</span><span>.</span><span>TestCase</span><span>):</span>
    <span># ...
</span>    <span>def</span> <span>test_lambda_no_params_no_freevars</span><span>(</span><span>self</span><span>):</span>
        <span>self</span><span>.</span><span>assertEqual</span><span>(</span><span>convert_lambdas</span><span>([</span><span>"lambda"</span><span>,</span> <span>[],</span> <span>3</span><span>]),</span>
                         <span>[</span><span>"labels"</span><span>,</span> <span>[</span>
                             <span>[</span><span>"f0"</span><span>,</span> <span>[</span><span>"code"</span><span>,</span> <span>[],</span> <span>[],</span> <span>3</span><span>]],</span>
                         <span>],</span> <span>[</span><span>"closure"</span><span>,</span> <span>"f0"</span><span>]])</span>

    <span>def</span> <span>test_nested_lambda</span><span>(</span><span>self</span><span>):</span>
        <span>self</span><span>.</span><span>assertEqual</span><span>(</span><span>convert_lambdas</span><span>([</span><span>"lambda"</span><span>,</span> <span>[</span><span>"x"</span><span>],</span>
                                       <span>[</span><span>"lambda"</span><span>,</span> <span>[</span><span>"y"</span><span>],</span>
                                        <span>[</span><span>"+"</span><span>,</span> <span>"x"</span><span>,</span> <span>"y"</span><span>]]]),</span>
                         <span>[</span><span>"labels"</span><span>,</span>
                          <span>[[</span><span>"f0"</span><span>,</span> <span>[</span><span>"code"</span><span>,</span> <span>[</span><span>"y"</span><span>],</span> <span>[</span><span>"x"</span><span>],</span> <span>[</span><span>"+"</span><span>,</span> <span>"x"</span><span>,</span> <span>"y"</span><span>]]],</span>
                           <span>[</span><span>"f1"</span><span>,</span> <span>[</span><span>"code"</span><span>,</span> <span>[</span><span>"x"</span><span>],</span> <span>[],</span> <span>[</span><span>"closure"</span><span>,</span> <span>"f0"</span><span>,</span> <span>"x"</span><span>]]]],</span>
                          <span>[</span><span>"closure"</span><span>,</span> <span>"f1"</span><span>]])</span>
    <span># ... and many more, especially interacting with `let`
</span></code></pre></div>

<p>Now let’s talk about the other binder.</p>

<h3 id="let">Let</h3>

<p>Let’s think about what <code>let</code> does by examining a confusing let expression:</p>

<div><pre><code><span>(</span><span>let</span> <span>((</span><span>wolf</span> <span>5</span><span>)</span>
      <span>(</span><span>x</span> <span>wolf</span><span>))</span>
  <span>wolf</span><span>)</span>
</code></pre></div>

<p>In this expression, there are two <code>wolf</code>s. One of them is bound inside the let,
but the other is free inside the let! This is because <code>let</code> evaluates all of
its bindings without access to the bindings as they are being built up (for
that, we would need <code>let*</code>).</p>

<div><pre><code><span>(</span><span>let</span> <span>((</span><span>wolf</span> <span>5</span><span>)</span>   <span>; new binding  &lt;-------------+</span>
      <span>(</span><span>x</span> <span>wolf</span><span>))</span>  <span>; some other variable; free! |</span>
  <span>wolf</span><span>)</span>          <span>; bound to ------------------+</span>
</code></pre></div>

<p>So this must mean that:</p>

<ul>
  <li>we need to convert all of the bindings using the original <code>bound</code> and <code>free</code>,
then</li>
  <li>only for the let body, add the new bindings (and use the original <code>free</code>)</li>
</ul>

<p>Which gives us, in code:</p>

<div><pre><code><span>class</span> <span>LambdaConverter</span><span>:</span>
    <span># ...
</span>    <span>def</span> <span>convert</span><span>(</span><span>self</span><span>,</span> <span>expr</span><span>,</span> <span>bound</span><span>,</span> <span>free</span><span>):</span>
        <span>match</span> <span>expr</span><span>:</span>
            <span># ...
</span>            <span>case</span> <span>[</span><span>"let"</span><span>,</span> <span>bindings</span><span>,</span> <span>body</span><span>]:</span>
                <span>new_bindings</span> <span>=</span> <span>[]</span>
                <span>for</span> <span>name</span><span>,</span> <span>val_expr</span> <span>in</span> <span>bindings</span><span>:</span>
                    <span>new_bindings</span><span>.</span><span>append</span><span>([</span><span>name</span><span>,</span> <span>self</span><span>.</span><span>convert</span><span>(</span><span>val_expr</span><span>,</span> <span>bound</span><span>,</span> <span>free</span><span>)])</span>
                <span>names</span> <span>=</span> <span>{</span><span>name</span> <span>for</span> <span>name</span><span>,</span> <span>_</span> <span>in</span> <span>bindings</span><span>}</span>
                <span>new_body</span> <span>=</span> <span>self</span><span>.</span><span>convert</span><span>(</span><span>body</span><span>,</span> <span>bound</span> <span>|</span> <span>names</span><span>,</span> <span>free</span><span>)</span>
                <span>return</span> <span>[</span><span>"let"</span><span>,</span> <span>new_bindings</span><span>,</span> <span>new_body</span><span>]</span>
            <span># ...
</span>
<span>class</span> <span>LambdaTests</span><span>(</span><span>unittest</span><span>.</span><span>TestCase</span><span>):</span>
    <span># ...
</span>    <span>def</span> <span>test_let</span><span>(</span><span>self</span><span>):</span>
        <span>self</span><span>.</span><span>assertEqual</span><span>(</span><span>convert_lambdas</span><span>([</span><span>"let"</span><span>,</span> <span>[[</span><span>"x"</span><span>,</span> <span>5</span><span>]],</span> <span>"x"</span><span>]),</span>
                         <span>[</span><span>"labels"</span><span>,</span> <span>[],</span> <span>[</span><span>"let"</span><span>,</span> <span>[[</span><span>"x"</span><span>,</span> <span>5</span><span>]],</span> <span>"x"</span><span>]])</span>

    <span>def</span> <span>test_let_lambda</span><span>(</span><span>self</span><span>):</span>
        <span>self</span><span>.</span><span>assertEqual</span><span>(</span><span>convert_lambdas</span><span>([</span><span>"let"</span><span>,</span> <span>[[</span><span>"x"</span><span>,</span> <span>5</span><span>]],</span>
                                       <span>[</span><span>"lambda"</span><span>,</span> <span>[</span><span>"y"</span><span>],</span>
                                        <span>[</span><span>"+"</span><span>,</span> <span>"x"</span><span>,</span> <span>"y"</span><span>]]]),</span>
                         <span>[</span><span>"labels"</span><span>,</span>
                          <span>[[</span><span>"f0"</span><span>,</span> <span>[</span><span>"code"</span><span>,</span> <span>[</span><span>"y"</span><span>],</span> <span>[</span><span>"x"</span><span>],</span> <span>[</span><span>"+"</span><span>,</span> <span>"x"</span><span>,</span> <span>"y"</span><span>]]]],</span>
                          <span>[</span><span>"let"</span><span>,</span> <span>[[</span><span>"x"</span><span>,</span> <span>5</span><span>]],</span> <span>[</span><span>"closure"</span><span>,</span> <span>"f0"</span><span>,</span> <span>"x"</span><span>]]])</span>

    <span>def</span> <span>test_let_inside_lambda</span><span>(</span><span>self</span><span>):</span>
        <span>self</span><span>.</span><span>assertEqual</span><span>(</span><span>convert_lambdas</span><span>([</span><span>"lambda"</span><span>,</span> <span>[</span><span>"x"</span><span>],</span>
                                       <span>[</span><span>"let"</span><span>,</span> <span>[[</span><span>"y"</span><span>,</span> <span>6</span><span>]],</span>
                                        <span>[</span><span>"+"</span><span>,</span> <span>"x"</span><span>,</span> <span>"y"</span><span>]]]),</span>
                         <span>[</span><span>"labels"</span><span>,</span>
                          <span>[[</span><span>"f0"</span><span>,</span> <span>[</span><span>"code"</span><span>,</span> <span>[</span><span>"x"</span><span>],</span> <span>[],</span>
                                   <span>[</span><span>"let"</span><span>,</span> <span>[[</span><span>"y"</span><span>,</span> <span>6</span><span>]],</span>
                                    <span>[</span><span>"+"</span><span>,</span> <span>"x"</span><span>,</span> <span>"y"</span><span>]]]]],</span>
                          <span>[</span><span>"closure"</span><span>,</span> <span>"f0"</span><span>]])</span>

    <span>def</span> <span>test_paper_example</span><span>(</span><span>self</span><span>):</span>
        <span>self</span><span>.</span><span>assertEqual</span><span>(</span><span>convert_lambdas</span><span>([</span><span>"let"</span><span>,</span> <span>[[</span><span>"x"</span><span>,</span> <span>5</span><span>]],</span>
                                         <span>[</span><span>"lambda"</span><span>,</span> <span>[</span><span>"y"</span><span>],</span>
                                          <span>[</span><span>"lambda"</span><span>,</span> <span>[],</span>
                                           <span>[</span><span>"+"</span><span>,</span> <span>"x"</span><span>,</span> <span>"y"</span><span>]]]]),</span>
                         <span>[</span><span>"labels"</span><span>,</span> <span>[</span>
                             <span>[</span><span>"f0"</span><span>,</span> <span>[</span><span>"code"</span><span>,</span> <span>[],</span>
                               <span>[</span><span>"x"</span><span>,</span> <span>"y"</span><span>],</span> <span>[</span><span>"+"</span><span>,</span> <span>"x"</span><span>,</span> <span>"y"</span><span>]]],</span>
                             <span>[</span><span>"f1"</span><span>,</span> <span>[</span><span>"code"</span><span>,</span> <span>[</span><span>"y"</span><span>],</span> <span>[</span><span>"x"</span><span>],</span>
                               <span>[</span><span>"closure"</span><span>,</span> <span>"f0"</span><span>,</span> <span>"x"</span><span>,</span> <span>"y"</span><span>]]],</span>
                           <span>],</span>
                          <span>[</span><span>"let"</span><span>,</span> <span>[[</span><span>"x"</span><span>,</span> <span>5</span><span>]],</span> <span>[</span><span>"closure"</span><span>,</span> <span>"f1"</span><span>,</span> <span>"x"</span><span>]]])</span>
    <span># ... and many more, especially interacting with `lambda`
</span></code></pre></div>

<h3 id="function-calls">Function calls</h3>

<p>Last, and somewhat boringly, we have function calls. The only thing to call out
is again handling these always-bound primitive operators like <code>+</code>, which we
don’t want to have a <code>funcall</code>:</p>

<div><pre><code><span>class</span> <span>LambdaConverter</span><span>:</span>
    <span># ...
</span>    <span>def</span> <span>convert</span><span>(</span><span>self</span><span>,</span> <span>expr</span><span>,</span> <span>bound</span><span>,</span> <span>free</span><span>):</span>
        <span>match</span> <span>expr</span><span>:</span>
            <span># ...
</span>            <span>case</span> <span>[</span><span>func</span><span>,</span> <span>*</span><span>args</span><span>]:</span>
                <span>result</span> <span>=</span> <span>[]</span> <span>if</span> <span>isinstance</span><span>(</span><span>func</span><span>,</span> <span>str</span><span>)</span> <span>and</span> <span>func</span> <span>in</span> <span>BUILTINS</span> <span>else</span> <span>[</span><span>"funcall"</span><span>]</span>
                <span>for</span> <span>e</span> <span>in</span> <span>expr</span><span>:</span>
                    <span>result</span><span>.</span><span>append</span><span>(</span><span>self</span><span>.</span><span>convert</span><span>(</span><span>e</span><span>,</span> <span>bound</span><span>,</span> <span>free</span><span>))</span>
                <span>return</span> <span>result</span>
            <span># ...
</span>
<span>class</span> <span>LambdaTests</span><span>(</span><span>unittest</span><span>.</span><span>TestCase</span><span>):</span>
    <span># ...
</span>    <span>def</span> <span>test_call</span><span>(</span><span>self</span><span>):</span>
        <span>self</span><span>.</span><span>assertEqual</span><span>(</span><span>convert_lambdas</span><span>([</span><span>"f"</span><span>,</span> <span>3</span><span>,</span> <span>4</span><span>]),</span> <span>[</span><span>"labels"</span><span>,</span> <span>[],</span> <span>[</span><span>"funcall"</span><span>,</span> <span>"f"</span><span>,</span> <span>3</span><span>,</span> <span>4</span><span>]])</span>
</code></pre></div>

<p>Now that we have these new <code>funcall</code>, and <code>closure</code> forms we have to compile
them into assembly.</p>

<h3 id="compiling-closure">Compiling <code>closure</code></h3>

<p>Compiling closure forms is very similar to allocating a string or a vector. In
the first cell, we want to put a pointer to the code that backs the closure
(this will be some label like <code>f12</code>). We can get a reference to that using
<code>lea</code>, since it will be a label in the assembly. Then we write it to the heap.</p>

<p>Then for each free variable, we go find out where it’s defined. Since we know
by construction that these are all strings, we don’t need to worry about having
weird recursion issues around keeping track of a moving heap pointer. Instead,
we know it’s always going to be an indirect from the stack or from the current
closure. Then we write that to the heap.</p>

<p>Then, since a closure is an object, we need to give it a tag. So we tag it with
<code>lea</code> because I felt cute. You could also use <code>or</code> or <code>add</code>. We store the
result in <code>rax</code> because that’s our compiler contract.</p>

<p>Last, we bump the heap pointer by the size of the closure.</p>

<div><pre><code><span>def</span> <span>compile_expr</span><span>(</span><span>expr</span><span>,</span> <span>code</span><span>,</span> <span>si</span><span>,</span> <span>env</span><span>):</span>
    <span>match</span> <span>expr</span><span>:</span>
        <span># ...
</span>        <span>case</span> <span>[</span><span>"closure"</span><span>,</span> <span>str</span><span>(</span><span>lvar</span><span>),</span> <span>*</span><span>args</span><span>]:</span>
            <span>comment</span><span>(</span><span>"Get a pointer to the label"</span><span>)</span>
            <span>emit</span><span>(</span><span>f</span><span>"lea rax, </span><span>{</span><span>lvar</span><span>}</span><span>"</span><span>)</span>
            <span>emit</span><span>(</span><span>f</span><span>"mov </span><span>{</span><span>heap_at</span><span>(</span><span>0</span><span>)</span><span>}</span><span>, rax"</span><span>)</span>
            <span>for</span> <span>idx</span><span>,</span> <span>arg</span> <span>in</span> <span>enumerate</span><span>(</span><span>args</span><span>):</span>
                <span>assert</span> <span>isinstance</span><span>(</span><span>arg</span><span>,</span> <span>str</span><span>)</span>
                <span>comment</span><span>(</span><span>f</span><span>"Load closure cell #</span><span>{</span><span>idx</span><span>}</span><span>"</span><span>)</span>
                <span># Just a variable lookup; guaranteed not to allocate
</span>                <span>compile_expr</span><span>(</span><span>arg</span><span>,</span> <span>code</span><span>,</span> <span>si</span><span>,</span> <span>env</span><span>)</span>
                <span>emit</span><span>(</span><span>f</span><span>"mov </span><span>{</span><span>heap_at</span><span>((</span><span>idx</span><span>+</span><span>1</span><span>)</span><span>*</span><span>WORD_SIZE</span><span>)</span><span>}</span><span>, rax"</span><span>)</span>
            <span>comment</span><span>(</span><span>"Tag a closure pointer"</span><span>)</span>
            <span>emit</span><span>(</span><span>f</span><span>"lea rax, </span><span>{</span><span>heap_at</span><span>(</span><span>CLOSURE_TAG</span><span>)</span><span>}</span><span>"</span><span>)</span>
            <span>comment</span><span>(</span><span>"Bump the heap pointer"</span><span>)</span>
            <span>size</span> <span>=</span> <span>align</span><span>(</span><span>WORD_SIZE</span> <span>+</span> <span>len</span><span>(</span><span>args</span><span>)</span><span>*</span><span>WORD_SIZE</span><span>)</span>
            <span>emit</span><span>(</span><span>f</span><span>"add </span><span>{</span><span>HEAP_BASE</span><span>}</span><span>, </span><span>{</span><span>size</span><span>}</span><span>"</span><span>)</span>
        <span># ...
</span></code></pre></div>

<p>So <code>(lambda (x) x)</code> compiles to:</p>

<div><pre><code><span>.intel_syntax</span>
<span>.global</span> <span>scheme_entry</span>

<span>f0:</span>
<span>mov</span> <span>rax</span><span>,</span> <span>[</span><span>rsp</span><span>-</span><span>8</span><span>]</span>
<span>ret</span>

<span>scheme_entry:</span>
<span>#</span> <span>Get</span> <span>a</span> <span>pointer</span> <span>to</span> <span>the</span> <span>label</span>
<span>lea</span> <span>rax</span><span>,</span> <span>f0</span>
<span>mov</span> <span>[</span><span>rsi</span><span>+</span><span>0</span><span>],</span> <span>rax</span>
<span>#</span> <span>Tag</span> <span>a</span> <span>cl</span><span>osure</span> <span>pointer</span>
<span>lea</span> <span>rax</span><span>,</span> <span>[</span><span>rsi</span><span>+</span><span>6</span><span>]</span>
<span>#</span> <span>Bump</span> <span>the</span> <span>heap</span> <span>pointer</span>
<span>add</span> <span>rsi</span><span>,</span> <span>16</span>
<span>ret</span>
</code></pre></div>

<p>and if we had a closure variable, for example <code>(let ((y 5)) (lambda () y))</code>:</p>

<div><pre><code><span>.intel_syntax</span>
<span>.global</span> <span>scheme_entry</span>

<span>f0:</span>
<span>mov</span> <span>rax</span><span>,</span> <span>[</span><span>rdi</span><span>+</span><span>2</span><span>]</span>
<span>ret</span>

<span>scheme_entry:</span>
<span>#</span> <span>Code</span> <span>for</span> <span>y</span>
<span>mov</span> <span>rax</span><span>,</span> <span>20</span>
<span>#</span> <span>Store</span> <span>y</span> <span>on</span> <span>the</span> <span>stack</span>
<span>mov</span> <span>[</span><span>rsp</span><span>-</span><span>8</span><span>],</span> <span>rax</span>
<span>#</span> <span>Get</span> <span>a</span> <span>pointer</span> <span>to</span> <span>the</span> <span>label</span>
<span>lea</span> <span>rax</span><span>,</span> <span>f0</span>
<span>mov</span> <span>[</span><span>rsi</span><span>+</span><span>0</span><span>],</span> <span>rax</span>
<span>#</span> <span>Load</span> <span>cl</span><span>osure</span> <span>cell</span> <span>#</span><span>0</span>
<span>mov</span> <span>rax</span><span>,</span> <span>[</span><span>rsp</span><span>-</span><span>8</span><span>]</span>
<span>mov</span> <span>[</span><span>rsi</span><span>+</span><span>8</span><span>],</span> <span>rax</span>
<span>#</span> <span>Tag</span> <span>a</span> <span>cl</span><span>osure</span> <span>pointer</span>
<span>lea</span> <span>rax</span><span>,</span> <span>[</span><span>rsi</span><span>+</span><span>6</span><span>]</span>
<span>#</span> <span>Bump</span> <span>the</span> <span>heap</span> <span>pointer</span>
<span>add</span> <span>rsi</span><span>,</span> <span>16</span>
<span>ret</span>
</code></pre></div>

<p>One nicety of emitting text assembly is that I can add inline comments very
easily. That’s what my <code>comment</code> function is for: it just prefixes a <code>#</code>.</p>

<p>…wait, hold on, why are we reading from <code>rdi+2</code> for a closure variable? That
doesn’t make any sense, right?</p>

<p>That’s because while we are reading off the closure, we are reading from a
tagged pointer. Since we know the index into the closure and also the tag at
compile-time, we can fold them into one neat indirect.</p>

<div><pre><code><span>def</span> <span>compile_lexpr</span><span>(</span><span>lexpr</span><span>,</span> <span>code</span><span>):</span>
    <span>match</span> <span>lexpr</span><span>:</span>
        <span>case</span> <span>[</span><span>"code"</span><span>,</span> <span>params</span><span>,</span> <span>freevars</span><span>,</span> <span>body</span><span>]:</span>
            <span>env</span> <span>=</span> <span>{}</span>
            <span>for</span> <span>idx</span><span>,</span> <span>param</span> <span>in</span> <span>enumerate</span><span>(</span><span>params</span><span>):</span>
                <span>env</span><span>[</span><span>param</span><span>]</span> <span>=</span> <span>stack_at</span><span>(</span><span>-</span><span>(</span><span>idx</span><span>+</span><span>1</span><span>)</span><span>*</span><span>WORD_SIZE</span><span>)</span>
            <span># vvvv New for closures vvvv
</span>            <span>for</span> <span>idx</span><span>,</span> <span>fvar</span> <span>in</span> <span>enumerate</span><span>(</span><span>freevars</span><span>):</span>
                <span>env</span><span>[</span><span>fvar</span><span>]</span> <span>=</span> <span>indirect</span><span>(</span><span>CLOSURE_BASE</span><span>,</span> <span>(</span><span>idx</span><span>+</span><span>1</span><span>)</span><span>*</span><span>WORD_SIZE</span> <span>-</span> <span>CLOSURE_TAG</span><span>)</span>
            <span># ^^^^ New for closures ^^^^
</span>            <span>compile_expr</span><span>(</span><span>body</span><span>,</span> <span>code</span><span>,</span> <span>si</span><span>=-</span><span>(</span><span>len</span><span>(</span><span>env</span><span>)</span><span>+</span><span>1</span><span>)</span><span>*</span><span>WORD_SIZE</span><span>,</span> <span>env</span><span>=</span><span>env</span><span>)</span>
            <span>code</span><span>.</span><span>append</span><span>(</span><span>"ret"</span><span>)</span>
        <span>case</span> <span>_</span><span>:</span>
            <span>raise</span> <span>NotImplementedError</span><span>(</span><span>lexpr</span><span>)</span>
</code></pre></div>

<p>Now let’s call some closures…!</p>

<h3 id="compiling-funcall">Compiling <code>funcall</code></h3>

<p>I’ll start by showing the code for <code>labelcall</code> because it’s a good stepping
stone toward <code>funcall</code> (nice job, Dr Ghuloum!).</p>

<p>The main parts are:</p>

<ul>
  <li>save space on the stack for the return address</li>
  <li>compile the args onto the stack</li>
  <li>adjusting the stack pointer above the locals</li>
  <li>call</li>
  <li>bringing the stack pointer back</li>
</ul>

<p>I think in my last version (the C version) I did this recursively because
looping felt challenging to do neatly in C with the data structures I had
built but since this is Python and the wild west, we’re looping.</p>

<div><pre><code><span>def</span> <span>compile_expr</span><span>(</span><span>expr</span><span>,</span> <span>code</span><span>,</span> <span>si</span><span>,</span> <span>env</span><span>):</span>
    <span>match</span> <span>expr</span><span>:</span>
        <span># ...
</span>        <span>case</span> <span>[</span><span>"labelcall"</span><span>,</span> <span>str</span><span>(</span><span>label</span><span>),</span> <span>*</span><span>args</span><span>]:</span>
            <span>new_si</span> <span>=</span> <span>si</span> <span>-</span> <span>WORD_SIZE</span>  <span># Save a word for the return address
</span>            <span>for</span> <span>arg</span> <span>in</span> <span>args</span><span>:</span>
                <span>compile_expr</span><span>(</span><span>arg</span><span>,</span> <span>code</span><span>,</span> <span>new_si</span><span>,</span> <span>env</span><span>)</span>
                <span>emit</span><span>(</span><span>f</span><span>"mov </span><span>{</span><span>stack_at</span><span>(</span><span>new_si</span><span>)</span><span>}</span><span>, rax"</span><span>)</span>
                <span>new_si</span> <span>-=</span> <span>WORD_SIZE</span>
            <span># Align to one word before the return address
</span>            <span>si_adjust</span> <span>=</span> <span>abs</span><span>(</span><span>si</span><span>+</span><span>WORD_SIZE</span><span>)</span>
            <span>emit</span><span>(</span><span>f</span><span>"sub rsp, </span><span>{</span><span>si_adjust</span><span>}</span><span>"</span><span>)</span>
            <span>emit</span><span>(</span><span>f</span><span>"call </span><span>{</span><span>label</span><span>}</span><span>"</span><span>)</span>
            <span>emit</span><span>(</span><span>f</span><span>"add rsp, </span><span>{</span><span>si_adjust</span><span>}</span><span>"</span><span>)</span>
        <span># ...
</span></code></pre></div>

<p>A lot of this carries over exactly to <code>funcall</code>, with a couple differences:</p>

<ul>
  <li>save space on the stack for the return address <em>and the closure pointer</em></li>
  <li>compile the function expression, which can be arbitrarily complex and results
in a closure pointer</li>
  <li>save the current closure pointer</li>
  <li>set up the new closure pointer</li>
  <li>call through the new closure pointer</li>
  <li>restore the old closure pointer</li>
</ul>

<p>I think the stack adjustment math was by and away the most irritating thing to
get right here. Oh, and also remembering to untag the closure when trying to
call it.</p>

<div><pre><code><span>def</span> <span>compile_expr</span><span>(</span><span>expr</span><span>,</span> <span>code</span><span>,</span> <span>si</span><span>,</span> <span>env</span><span>):</span>
    <span>match</span> <span>expr</span><span>:</span>
        <span># ...
</span>        <span>case</span> <span>[</span><span>"funcall"</span><span>,</span> <span>func</span><span>,</span> <span>*</span><span>args</span><span>]:</span>
            <span># Save a word for the return address and the closure pointer
</span>            <span>clo_si</span> <span>=</span> <span>si</span> <span>-</span> <span>WORD_SIZE</span>
            <span>retaddr_si</span> <span>=</span> <span>clo_si</span> <span>-</span> <span>WORD_SIZE</span>
            <span>new_si</span> <span>=</span> <span>retaddr_si</span>
            <span># Evaluate arguments
</span>            <span>for</span> <span>arg</span> <span>in</span> <span>args</span><span>:</span>
                <span>compile_expr</span><span>(</span><span>arg</span><span>,</span> <span>code</span><span>,</span> <span>new_si</span><span>,</span> <span>env</span><span>)</span>
                <span>emit</span><span>(</span><span>f</span><span>"mov </span><span>{</span><span>stack_at</span><span>(</span><span>new_si</span><span>)</span><span>}</span><span>, rax"</span><span>)</span>
                <span>new_si</span> <span>-=</span> <span>WORD_SIZE</span>
            <span>compile_expr</span><span>(</span><span>func</span><span>,</span> <span>code</span><span>,</span> <span>new_si</span><span>,</span> <span>env</span><span>)</span>
            <span># Save the current closure pointer
</span>            <span>emit</span><span>(</span><span>f</span><span>"mov </span><span>{</span><span>stack_at</span><span>(</span><span>clo_si</span><span>)</span><span>}</span><span>, </span><span>{</span><span>CLOSURE_BASE</span><span>}</span><span>"</span><span>)</span>
            <span>emit</span><span>(</span><span>f</span><span>"mov </span><span>{</span><span>CLOSURE_BASE</span><span>}</span><span>, rax"</span><span>)</span>
            <span># Align to one word before the return address
</span>            <span>si_adjust</span> <span>=</span> <span>abs</span><span>(</span><span>si</span><span>)</span>
            <span>emit</span><span>(</span><span>f</span><span>"sub rsp, </span><span>{</span><span>si_adjust</span><span>}</span><span>"</span><span>)</span>
            <span>emit</span><span>(</span><span>f</span><span>"call </span><span>{</span><span>indirect</span><span>(</span><span>CLOSURE_BASE</span><span>,</span> <span>-</span><span>CLOSURE_TAG</span><span>)</span><span>}</span><span>"</span><span>)</span>
            <span>emit</span><span>(</span><span>f</span><span>"add rsp, </span><span>{</span><span>si_adjust</span><span>}</span><span>"</span><span>)</span>
            <span>emit</span><span>(</span><span>f</span><span>"mov </span><span>{</span><span>CLOSURE_BASE</span><span>}</span><span>, </span><span>{</span><span>stack_at</span><span>(</span><span>clo_si</span><span>)</span><span>}</span><span>"</span><span>)</span>
        <span># ...
</span></code></pre></div>

<p>So <code>((lambda (x) x) 3)</code> compiles to:</p>

<div><pre><code><span>.intel_syntax</span>
<span>.global</span> <span>scheme_entry</span>

<span>f0:</span>
<span>mov</span> <span>rax</span><span>,</span> <span>[</span><span>rsp</span><span>-</span><span>8</span><span>]</span>
<span>ret</span>

<span>scheme_entry:</span>
<span>#</span> <span>Evaluate</span> <span>arguments</span>
<span>mov</span> <span>rax</span><span>,</span> <span>12</span>
<span>mov</span> <span>[</span><span>rsp</span><span>-</span><span>24</span><span>],</span> <span>rax</span>
<span>#</span> <span>Get</span> <span>a</span> <span>pointer</span> <span>to</span> <span>the</span> <span>label</span>
<span>lea</span> <span>rax</span><span>,</span> <span>f0</span>
<span>mov</span> <span>[</span><span>rsi</span><span>+</span><span>0</span><span>],</span> <span>rax</span>
<span>#</span> <span>Tag</span> <span>a</span> <span>cl</span><span>osure</span> <span>pointer</span>
<span>lea</span> <span>rax</span><span>,</span> <span>[</span><span>rsi</span><span>+</span><span>6</span><span>]</span>
<span>#</span> <span>Bump</span> <span>the</span> <span>heap</span> <span>pointer</span>
<span>add</span> <span>rsi</span><span>,</span> <span>16</span>
<span>#</span> <span>Save</span> <span>the</span> <span>current</span> <span>cl</span><span>osure</span> <span>pointer</span>
<span>mov</span> <span>[</span><span>rsp</span><span>-</span><span>16</span><span>],</span> <span>rdi</span>
<span>mov</span> <span>rdi</span><span>,</span> <span>rax</span>
<span>#</span> <span>Align</span> <span>to</span> <span>one</span> <span>word</span> <span>before</span> <span>the</span> <span>return</span> <span>address</span>
<span>sub</span> <span>rsp</span><span>,</span> <span>8</span>
<span>call</span> <span>[</span><span>rdi</span><span>-</span><span>6</span><span>]</span>
<span>#</span> <span>Rest</span><span>ore</span> <span>stack</span> <span>and</span> <span>cl</span><span>osure</span>
<span>add</span> <span>rsp</span><span>,</span> <span>8</span>
<span>mov</span> <span>rdi</span><span>,</span> <span>[</span><span>rsp</span><span>-</span><span>16</span><span>]</span>
<span>ret</span>
</code></pre></div>

<p>Not bad for a 300 line compiler!</p>

<h3 id="wrapping-up">Wrapping up</h3>

<p>I think that’s all there is for today, folks. We got closures, free variable
analysis, and indirect function calls. That’s pretty good.</p>

<p>Happy hacking!</p>

<h4 id="mini-table-of-contents">Mini Table of Contents</h4>

<ul>































    <li>
        <a href="https://bernsteinbear.com/blog/compiling-a-lisp-0/"><span>Overture</span></a>
        
    </li>



    <li>
        <a href="https://bernsteinbear.com/blog/compiling-a-lisp-1/"><span>The smallest program</span></a>
        
    </li>



    <li>
        <a href="https://bernsteinbear.com/blog/compiling-a-lisp-2/"><span>Integers</span></a>
        
    </li>



    <li>
        <a href="https://bernsteinbear.com/blog/compiling-a-lisp-3/"><span>Booleans, characters, nil</span></a>
        
    </li>



    <li>
        <a href="https://bernsteinbear.com/blog/compiling-a-lisp-4/"><span>Primitive unary functions</span></a>
        
    </li>



    <li>
        <a href="https://bernsteinbear.com/blog/compiling-a-lisp-5/"><span>Primitive binary functions</span></a>
        
    </li>



    <li>
        <a href="https://bernsteinbear.com/blog/compiling-a-lisp-6/"><span>Reader</span></a>
        
    </li>



    <li>
        <a href="https://bernsteinbear.com/blog/compiling-a-lisp-7/"><span>Let</span></a>
        
    </li>



    <li>
        <a href="https://bernsteinbear.com/blog/compiling-a-lisp-8/"><span>If</span></a>
        
    </li>



    <li>
        <a href="https://bernsteinbear.com/blog/compiling-a-lisp-9/"><span>Heap allocation</span></a>
        
    </li>



    <li>
        <a href="https://bernsteinbear.com/blog/compiling-a-lisp-10/"><span>Instruction encoding interlude</span></a>
        
    </li>



    <li>
        <a href="https://bernsteinbear.com/blog/compiling-a-lisp-11/"><span>Labelled procedure calls</span></a>
        
    </li>











































































































    <li>
        <a href="https://bernsteinbear.com/blog/compiling-a-lisp-12/"><span>Closure conversion</span></a>
         <span><i>(this page)</i></span> 
    </li>


</ul>


        </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[I tried coding with AI, I became lazy and stupid (123 pts)]]></title>
            <link>https://thomasorus.com/i-tried-coding-with-ai-i-became-lazy-and-stupid</link>
            <guid>44858641</guid>
            <pubDate>Sun, 10 Aug 2025 21:54:56 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://thomasorus.com/i-tried-coding-with-ai-i-became-lazy-and-stupid">https://thomasorus.com/i-tried-coding-with-ai-i-became-lazy-and-stupid</a>, See on <a href="https://news.ycombinator.com/item?id=44858641">Hacker News</a></p>
<div id="readability-page-1" class="page"><article>
            

<p>Around April 2025, my boss at $dayjob insisted we try AI tools for coding. It wasn't toxic pressure or anything like <em>"20% of your code needs to be AI"</em>, just a concern from him that we could miss on something. I understand why he asked that and I don't blame him. We are in difficult economic period even for software, and we have salaries to pay. If AI can increase productivity or our margins, it should be at least put on the table of negotiations. I am not happy about this coming, but <em>I get it</em>.</p>

<details><summary>My personal stance of AI</summary> I have personal reasons to dislike LLMs. My partner lost their writing job due to chatGPT convincing their manager writers were now useless. A lot of artist friends struggle because of LLMs. We recently had an intern who lost her translator role due to LLMs. And even outside my personal experience, LLMs are based on stolen content, don't respect consent, waste huge amount of electricity and water, and are overall a new weapon for the capitalists in the <a href="https://danmcquillan.org/ai_thatcherism.html">class warfare</a>. </details>

<p>The other reason why I folded comes from a toxic relationship I built with my job when I became a developer. I detailed in a previous blog post how choosing this career came with very high stakes which triggered a shift in my brain that hasn't left me since:</p>

<figure><blockquote cite="/every-web-stack-is-a-product-now">When I started web development seven years ago, I was in survival mode after years of low paying wages and unemployment. It <em>had</em> to work, and for it to work, I <em>had</em> to always learn more, read and listen about web development all the time, monitor the field, socialize as much as possible with my peers. This way, I would not get disposable and lose my job. I would build a network. I would be safe.</blockquote><figcaption>— I, <a href="https://thomasorus.com/every-web-stack-is-a-product-now">In a blog post from 2022</a></figcaption></figure>

<p>10 years and 3 burnouts later, one can tell this mindset, even if it worked out for a while, wasn't sane or desirable. I had managed to put aside this fear of being disposable, but LLMs triggered it back big time. What if AI vendors were right? What if a future company I apply to requires you to use it? Am I going to lose my job? I'm almost 40, what will I do?</p>

<p>So I tried AI. First at my day job, because I wanted answers. But outside fixing TypeScript types errors, generate inaccessible template code, or review my code for errors, I couldn't find a <em>life changing</em> use out of it that all AI influencers talk about. I asked my colleagues about their own experiments, and lots of them came to the same conclusion: it doesn't seem to help me help our clients achieve their goals.</p>

<p>When July came I was starting the image processing part of my new CMS that powers this website. Still stressed I couldn't get a real shot at coding with an LLM, and very tired by different personal events that fogged my brain, I decided it was the right task to try it seriously and get answers.</p>

<p>After setting up everything in VS Code, opening the AI panel, giving access to the codebase and detailing my needs in a prompt, the LLM produced around 200 lines of code. Mostly functions using dependencies to convert, resize, process images. It wasn't perfect but after a few changes, the task was done and it had taken around 30 minutes, far less than if I had made it by hand.</p>

<p>I was impressed. It really felt like I had superpowers! But then I had the idea to audit the code the LLM just produced, like I did at my $dayjob for a Vue application. Feeling that uploading files could be a source of security issues, I asked the same LLM to focus on this specific topic.</p>

<p>It found several dangers: directory traversal attacks, file size limits, system file overwrite, etc. I had no idea the initial code was this unsafe. I had reviewed the code, but without enough experience in backend development, how could I identify issues I didn't know existed? And why, if it knew about all those dangers, did the LLM produced unsafe code in the first place?</p>

<p>When I tried to fix the security issues, I quickly realized how this whole thing was a trap. Since I didn't wrote it, I didn't have a good bird's eye view of the code and what it did. I couldn't make changes quickly, which started to frustrated me. The easiest route was asking the LLM to do the fixes for me, so I did. More code was changed and added. It worked, but again I could not tell if it was good or not.</p>

<p>That's when I stopped the experiment.</p>

<p>I was shocked by how easily I had slipped into this slacker way of programming. The LLM had given me shitty code, made me ignorant about my own code base, and too lazy to try to fix it myself. And at the same time, the whole experience felt smooth, frictionless, empowering. On the moment I felt smarter, more productive, in control. But it was all an illusion.</p>

<p>I knew about this as we had studies showing <a href="https://www.researchgate.net/publication/392560878_Your_Brain_on_ChatGPT_Accumulation_of_Cognitive_Debt_when_Using_an_AI_Assistant_for_Essay_Writing_Task">LLM use makes us dumb</a>, and that <a href="https://metr.org/blog/2025-07-10-early-2025-ai-experienced-os-dev-study/">self-reported productivity gains are false</a>. But experiencing it for myself was a totally different feeling.</p>

<p>It gave me a whole different perspective and answered my initial question: will I get replaced by AI soon?</p>

<p>The answer is no. I don't think AI will take my job anytime soon because it's smarter and more productive than I am. I also don't think AI will make me <a href="https://colton.dev/blog/curing-your-ai-10x-engineer-imposter-syndrome/">10 times more productive</a>. If I lose my job due to AI, it will be because I used it so much it made me lazy and stupid to the point another human has to replace me and I become unemployable.</p>

<p>I shouldn't invest time in AI. I should invest more time studying new things that interest me. That's probably the only way to keep doing this job and, you know, <em>be safe</em>.</p>


            
    

        </article><p><small><strong>Initially published: </strong>08 Aug 2025 at 20:03</small><br>
        <small><strong>Modified and/or rebuilt: </strong>08 Aug 2025 at 20:17</small>
    </p></div>]]></description>
        </item>
        <item>
            <title><![CDATA[1910: The year the modern world lost its mind (323 pts)]]></title>
            <link>https://www.derekthompson.org/p/1910-the-year-the-modern-world-lost</link>
            <guid>44858154</guid>
            <pubDate>Sun, 10 Aug 2025 20:48:46 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.derekthompson.org/p/1910-the-year-the-modern-world-lost">https://www.derekthompson.org/p/1910-the-year-the-modern-world-lost</a>, See on <a href="https://news.ycombinator.com/item?id=44858154">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><article><div dir="auto"><div><p><em>“Automobilism is an illness, a mental illness. This illness has a pretty name: speed... [Man] can no longer stand still, he shivers, his nerves tense like springs, impatient to get going once he has arrived somewhere because it is not somewhere else, somewhere else, always somewhere else.” </em></p><p><em>- Octave Mirbeau, French novelist, 1910</em></p></div><p><em><strong>About today’s piece: When we hear about technological change and social crisis in the 21st century, it is easy to imagine that we are living through a special period of history. But many eras have grappled with the problems that seem to uniquely plague our own. The beginning of the 20th century was a period of speed and technological splendor (the automobile! the airplane! the bicycle!), shattered nerves, mass anxiety, and a widespread sense that the world had been forever knocked off its historical axis: a familiar stew of ideas. I think we can learn a lot about the present by studying historical periods whose challenges rhyme with our own.</strong></em></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/$s_!QkGQ!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5598fa1c-5c64-43cf-800c-35ce14dd1a12_580x394.jpeg" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!QkGQ!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5598fa1c-5c64-43cf-800c-35ce14dd1a12_580x394.jpeg 424w, https://substackcdn.com/image/fetch/$s_!QkGQ!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5598fa1c-5c64-43cf-800c-35ce14dd1a12_580x394.jpeg 848w, https://substackcdn.com/image/fetch/$s_!QkGQ!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5598fa1c-5c64-43cf-800c-35ce14dd1a12_580x394.jpeg 1272w, https://substackcdn.com/image/fetch/$s_!QkGQ!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5598fa1c-5c64-43cf-800c-35ce14dd1a12_580x394.jpeg 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/$s_!QkGQ!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5598fa1c-5c64-43cf-800c-35ce14dd1a12_580x394.jpeg" width="580" height="394" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/5598fa1c-5c64-43cf-800c-35ce14dd1a12_580x394.jpeg&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:394,&quot;width&quot;:580,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:&quot;1910 Model T ad&quot;,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:false,&quot;topImage&quot;:true,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" alt="1910 Model T ad" title="1910 Model T ad" srcset="https://substackcdn.com/image/fetch/$s_!QkGQ!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5598fa1c-5c64-43cf-800c-35ce14dd1a12_580x394.jpeg 424w, https://substackcdn.com/image/fetch/$s_!QkGQ!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5598fa1c-5c64-43cf-800c-35ce14dd1a12_580x394.jpeg 848w, https://substackcdn.com/image/fetch/$s_!QkGQ!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5598fa1c-5c64-43cf-800c-35ce14dd1a12_580x394.jpeg 1272w, https://substackcdn.com/image/fetch/$s_!QkGQ!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5598fa1c-5c64-43cf-800c-35ce14dd1a12_580x394.jpeg 1456w" sizes="100vw" fetchpriority="high"></picture></div></a></figure></div><p>Welcome back to The Sunday Morning Post!</p><p><span>My favorite period of history is the 30- to 40-year span between the end of the 19th century and the early innings of the 20th century. It was an era of incredible change. From </span><em>Abundance</em><span>:</span></p><blockquote><p>Imagine going to sleep in 1875 in New York City and waking up thirty years later. As you shut your eyes, there is no electric lighting, Coca-Cola, basketball, or aspirin. There are no cars or “sneakers.” The tallest building in Manhattan is a church. </p><p>When you wake up in 1905, the city has been remade with towering steel-skeleton buildings called “skyscrapers.” The streets are filled with novelty: automobiles powered by new internal combustion engines, people riding bicycles in rubber-soled shoes—all recent innovations. The Sears catalog, the cardboard box, and aspirin are  new arrivals. People have enjoyed their first sip of Coca-Cola and their first bite of what we now call an American hamburger. The Wright brothers have flown the first airplane. When you passed into slumber, nobody had taken a picture with a Kodak camera or used a machine that made motion pictures, or bought a device to play recorded music. By 1905, we have the first commercial versions of all three—the simple box camera, the cinematograph,  and the phonograph. </p></blockquote><p><span>No book on turn-of-the-century history has influenced me more, or brought me more joy, than </span><em><a href="https://www.amazon.com/Vertigo-Years-Europe-1900-1914/dp/0465020291" rel="">The Vertigo Years: Europe 1900-1914</a></em><span> by Philipp Blom. I think it might be the most underrated history book ever written.</span><span><a data-component-name="FootnoteAnchorToDOM" id="footnote-anchor-1-170457512" href="https://www.derekthompson.org/p/1910-the-year-the-modern-world-lost#footnote-1-170457512" target="_self" rel="">1</a></span><span> In my favorite chapters focusing on the years around 1910</span><span><a data-component-name="FootnoteAnchorToDOM" id="footnote-anchor-2-170457512" href="https://www.derekthompson.org/p/1910-the-year-the-modern-world-lost#footnote-2-170457512" target="_self" rel="">2</a></span><span>, Blom describes how turn-of-the-century technology changed the way people thought about art and human nature and how it contributed to a nervous breakdown across the west. Disoriented by the speed of modern times, Europeans and Americans suffered from record-high rates of anxiety and a sense that our inventions had destroyed our humanity. Meanwhile, some artists channeled this disorientation to create some of the greatest art of all time.</span></p><p><span>In today’s TSMP, I want to share with you my favorite passages and lessons from </span><em>The Vertigo Years</em><span>, most of which come from the chapter on the year 1910. Great history books remind us that while history never repeats itself, its themes never stop rhyming, and we would all do well to listen with open ears. I’ve tried to limit my summary to areas of overlap between the early 1900s and the 2020s, but I’m not going to press the similarities too hard throughout the piece. You’re going to have to recognize them for yourself.</span></p><p>Transportation technology remade the west in a few short decades between the 1880s and 1910. A “bicycle craze” swept America in the 1890s. The Wright Brothers took flight in 1903. The first Model Ts rolled off Ford’s production lines in 1908. In Europe, cars quickly transformed the physical environment. The number of automobiles in France increased from about 3,000 in 1900 to 100,000 by 1914. That year, Ford's factory in Detroit produced and sold more than 300,000 Model Ts. </p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/$s_!3gex!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5698f33e-71a1-4f5b-83c2-5f87dc392f6a_2418x1625.jpeg" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!3gex!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5698f33e-71a1-4f5b-83c2-5f87dc392f6a_2418x1625.jpeg 424w, https://substackcdn.com/image/fetch/$s_!3gex!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5698f33e-71a1-4f5b-83c2-5f87dc392f6a_2418x1625.jpeg 848w, https://substackcdn.com/image/fetch/$s_!3gex!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5698f33e-71a1-4f5b-83c2-5f87dc392f6a_2418x1625.jpeg 1272w, https://substackcdn.com/image/fetch/$s_!3gex!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5698f33e-71a1-4f5b-83c2-5f87dc392f6a_2418x1625.jpeg 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/$s_!3gex!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5698f33e-71a1-4f5b-83c2-5f87dc392f6a_2418x1625.jpeg" width="1456" height="978" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/5698f33e-71a1-4f5b-83c2-5f87dc392f6a_2418x1625.jpeg&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:978,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/$s_!3gex!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5698f33e-71a1-4f5b-83c2-5f87dc392f6a_2418x1625.jpeg 424w, https://substackcdn.com/image/fetch/$s_!3gex!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5698f33e-71a1-4f5b-83c2-5f87dc392f6a_2418x1625.jpeg 848w, https://substackcdn.com/image/fetch/$s_!3gex!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5698f33e-71a1-4f5b-83c2-5f87dc392f6a_2418x1625.jpeg 1272w, https://substackcdn.com/image/fetch/$s_!3gex!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5698f33e-71a1-4f5b-83c2-5f87dc392f6a_2418x1625.jpeg 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption>Model T, Wikimedia Commons</figcaption></figure></div><p><span>Speed was a physical experience, Blom writes, and cultural critics of the early 1900s were confident that it was unnatural for people to move so quickly through space—women, in particular. A woman on a bicycle was a thing to be feared. She signified a high-velocity freedom that was often associated with moral and sexual deviancy. Physicians warned that </span><a href="https://pessimistsarchive.org/list/bicycle/clippings/1897/sc-178" rel="">"diseases of the wheel"</a><span> came by "the almost universal use of the bicycle" and that </span><a href="https://pessimistsarchive.org/list/bicycle/clippings/1897/m-sc-192-184" rel="">"serious evils"</a><span> might befall the youth who rode without restraint.  Moralists condemned women who “pedaled along gleefully, having discarded their corsets and put on more practical clothing, including trousers.” </span></p><p><span>Critics and novelists considered technological speed to be a vice, and they warned that our lust for celerity might turn into literal lust; that cars and bicycles would beckon us into carnal sin. In </span><em>Le surmale</em><span> (1902), the book’s hero wins a 100,000-mile bike race and then celebrates with an act of love-making that makes one character exclaim, “This is not a man, but a machine!” The idea that cars, planes, and bicycles were turning people into “machines” was most entertainingly summarized by a 1905 article in the journal </span><em>Je sais tout</em><span> (“I know all”), which calculated just how tall a human being would have to be to naturally walk at the pace that our new machines traveled. To equal the speed of a bicycle, for example, it was calculated that a person have to be more than 40 feet tall. Blom:</span></p><blockquote><p>Comparisons with other forms of transportation showed that in a fast train, a voyager would be effectively 51 meters tall, while the chauffeur of a racing car would almost dwarf Notre Dame Cathedral in Paris. Technology had created a new race of giants — in both senses of the term — and it changed the experience of space and time itself.</p></blockquote><p>“The growing speed of daily life, of news and work and play was a fetish of artists and industrialists alike,” Blom writes. “Never before had so much social change occurred so quickly.” As daily life sped up, people in the west started to break down.</p><p><span>Around the turn of the century, a nervous disorder first diagnosed in the U.S. gradually made its way across the Atlantic. The doctor George Miller Beard had called it “neurasthenia,” or nervous exhaustion. Europeans sometimes referred to it as “American Nervousness.” According to Beard, the affliction was most common among “the in-door classes of civilized countries” and the sufferers could be found “in nearly every brain-working household.”</span><span><a data-component-name="FootnoteAnchorToDOM" id="footnote-anchor-3-170457512" href="https://www.derekthompson.org/p/1910-the-year-the-modern-world-lost#footnote-3-170457512" target="_self" rel="">3</a></span><span> </span></p><p>As Blom points out, those afflicted tended to be white-collar workers working at the “frontiers of technology,” as “telephone operators, typesetters on new, faster machines, railway workers, engineers, [or] factory workers handling fast machines. One 1893 hospital survey of neurasthenia found that among nearly 600 cases, “there were almost 200 businessmen, 130 civil servants, 68 teachers, 56 students and eleven farmers.” Notably, no manual workers were counted at the clinic. Neurasthenia seemed to disproportionately affect white-collar workers, who were “overwhelmed” by their labor. “Overwork was a common theme in patients’ histories,” Blom writes.</p><p>It is tempting to write off this phenomenon as just another case of the “worried well.” But the scale of the west’s mental health distress in this period was striking. Blom: </p><blockquote><p><strong>In Germany, 40,375 patients were registered in mental hospitals in 1870. The number rose to 115,882 in 1900 and 220,881 in 1910</strong><span>. Over the same period, the proportion of patients admitted to general hospitals for illnesses of the nervous system rose from 44 to 60 percent. While these numbers include those suffering from many and varied mental conditions, not just neurasthenia, they do not include the huge number of sufferers who preferred going for cures or long stays in private sanatoriums, spas or other paramedical establishments in which a doctor would look after the guests — as in the one described by Thomas Mann in </span><em>The Magic Mountain</em><span>. </span></p></blockquote><p>“Artists were fascinated by this accelerated reality and its possibilities,” Blom writes. The novelists, painters, and musicians of the era could not stop talking about the changes they saw around them and their duty to use art to enter into a dialogue with those changes. Blom: </p><blockquote><p>Their view of things was shaped by reading about races in fast machines and in children’s magazines, by over-hearing adult whispers about nervous breakdowns and fast women … their imagination was alert to the fact that an age had ended and a new one — by turns a promise and a menace — was busting onto the scene, visible as yet only in flashes and fragmented visions. </p></blockquote><p>Blom deeply considers three artistic icons of the era: the composer Igor Stravinsky and the painters Vassily Kandinsky and Pablo Picasso. Each sought to make art that felt simultaneously cutting-edge and primal. Each responded to the modern age by reaching for inspiration in the past.</p><p><span>Blom begins with Stravinsky, whose famous orchestral work </span><em>The Rite of Spring</em><span> was inspired by ancient Russian dance rituals. A melange of old folk music and arresting dissonance, the piece’s first performance in Paris 1913 triggered one of the most infamously violent reactions of any concert-hall audience in history. As Blom puts it bluntly, “all hell broke loose”: </span></p><blockquote><p>“During the first two minutes the public remained quiet,' Monteux [a musician] later recalled, “then there were boos and hissing from the upper circle, soon after from the stalls. People sitting next to one another began to hit one another on the head with fists and walking sticks, or whatever else they had to hand. Soon, their anger was turned against the dancers and especially against the orchestra... Everything to hand was thrown at them, but we continued playing. The chaos was complete when members of the audience turned on one another, on anyone supporting the other side. A heavily bejewelled lady was seen slapping her neighbour before storming off, while another one spat in her detractor's face. Fights broke out everywhere and challenges to duels were issued.”</p></blockquote><p><span>Some music critics now consider The Rite of Spring </span><a href="https://www.pittsburgh-theater.com/shows/heinz-hall/pittsburgh-symphony-orchestra-the-rite-of-spring" rel="">“undoubtedly the most famous composition of the early 20th century.”</a></p><p>As classical music disintegrated in the concert halls, visual art was undergoing its own revolution, which may have been technological in origin. For thousands of years before the turn-of-the-century, the ability to perfectly represent nature been a rare skill possessed only by the most talented painters and drawers among us. But the Kodak camera (invented in 1888, with sales accelerating into the 1900s) turned the ability to capture realist images into a consumerist trifle. It cannot be a coincidence that the rise of abstract art coincided so perfectly with the proliferation of cheap camera technology that debased the value of perfect renderings of the natural world. </p><p>In the early 1900s, Vassily Kandinsky, one of the great pioneers of abstract art, pushed back against the mind-blurring speed of modernity. Kandinsky drew inspiration from the shamans of the Ural Mountains and the sound of their drums, according to Blom, and his abstraction sought to capture their primary music in images. By turning sound into image, Kandinsky’s art sought to achieve an act of synesthesia that no Kodak machine could ever match. Other art historians are less certain about what inspired Kandinsky’s first abstract watercolors, which he painted around 1910. All that is certain is that paintings like this one reject any effort to depict the natural world as it might be seen through a retina or camera lens. </p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/$s_!EFhk!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1ad0d5b2-3932-43ea-b9a1-079d91b0f6ae_1592x1259.jpeg" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!EFhk!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1ad0d5b2-3932-43ea-b9a1-079d91b0f6ae_1592x1259.jpeg 424w, https://substackcdn.com/image/fetch/$s_!EFhk!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1ad0d5b2-3932-43ea-b9a1-079d91b0f6ae_1592x1259.jpeg 848w, https://substackcdn.com/image/fetch/$s_!EFhk!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1ad0d5b2-3932-43ea-b9a1-079d91b0f6ae_1592x1259.jpeg 1272w, https://substackcdn.com/image/fetch/$s_!EFhk!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1ad0d5b2-3932-43ea-b9a1-079d91b0f6ae_1592x1259.jpeg 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/$s_!EFhk!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1ad0d5b2-3932-43ea-b9a1-079d91b0f6ae_1592x1259.jpeg" width="1456" height="1151" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/1ad0d5b2-3932-43ea-b9a1-079d91b0f6ae_1592x1259.jpeg&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:1151,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:&quot;Untitled First Abstract Watercolor, 1910–1913, Centre Pompidou, Paris[19]&quot;,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" alt="Untitled First Abstract Watercolor, 1910–1913, Centre Pompidou, Paris[19]" title="Untitled First Abstract Watercolor, 1910–1913, Centre Pompidou, Paris[19]" srcset="https://substackcdn.com/image/fetch/$s_!EFhk!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1ad0d5b2-3932-43ea-b9a1-079d91b0f6ae_1592x1259.jpeg 424w, https://substackcdn.com/image/fetch/$s_!EFhk!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1ad0d5b2-3932-43ea-b9a1-079d91b0f6ae_1592x1259.jpeg 848w, https://substackcdn.com/image/fetch/$s_!EFhk!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1ad0d5b2-3932-43ea-b9a1-079d91b0f6ae_1592x1259.jpeg 1272w, https://substackcdn.com/image/fetch/$s_!EFhk!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1ad0d5b2-3932-43ea-b9a1-079d91b0f6ae_1592x1259.jpeg 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption><span>Untitled </span><em>First Abstract Watercolor</em><span>, 1910–1913</span></figcaption></figure></div><p>Kandinsky is one of my favorite artists. But the critical response to the dawn of abstract painting was about as brutal as it gets. One German review that Blom cites includes the following passage:</p><blockquote><p>Looked at as painting they are the end of art, a prank. But they show a more nefarious side. The modern phrase that the object of art is indifferent, if abused here in a truly malevolent way... What is presented to us breathes the poison breath of the darkest places of vice of the big city and shows the constitution of the artists, which can only be understood in terms of pathology.</p></blockquote><p><span>Around the same time that Kandinsky was putting his mark on abstraction, Pablo Picasso was pioneering his own rejection of purely representative art, with primitivism. Drawing inspiration from African masks and carvings from West Africa, Picasso’s art “did everything to hide its underlying technical and compositional virtuosity,” Blom writes. Picasso’s 1907 classic </span><em>Les Demoiselles d'Avignon</em><span> is “a large canvas of brutal and disturbing bluntness.” While Picasso was indifferent to the actual “significance and symbolism” of the African styles he drew on, Blom writes, critics have said his aim was to represent the “unchanging structure of the human condition” in the face of civilizational change.</span></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/$s_!9OXU!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff78d9057-3f4d-4474-aaa8-9976a71a337a_1280x1326.jpeg" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!9OXU!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff78d9057-3f4d-4474-aaa8-9976a71a337a_1280x1326.jpeg 424w, https://substackcdn.com/image/fetch/$s_!9OXU!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff78d9057-3f4d-4474-aaa8-9976a71a337a_1280x1326.jpeg 848w, https://substackcdn.com/image/fetch/$s_!9OXU!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff78d9057-3f4d-4474-aaa8-9976a71a337a_1280x1326.jpeg 1272w, https://substackcdn.com/image/fetch/$s_!9OXU!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff78d9057-3f4d-4474-aaa8-9976a71a337a_1280x1326.jpeg 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/$s_!9OXU!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff78d9057-3f4d-4474-aaa8-9976a71a337a_1280x1326.jpeg" width="1280" height="1326" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/f78d9057-3f4d-4474-aaa8-9976a71a337a_1280x1326.jpeg&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:1326,&quot;width&quot;:1280,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:&quot;undefined&quot;,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" alt="undefined" title="undefined" srcset="https://substackcdn.com/image/fetch/$s_!9OXU!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff78d9057-3f4d-4474-aaa8-9976a71a337a_1280x1326.jpeg 424w, https://substackcdn.com/image/fetch/$s_!9OXU!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff78d9057-3f4d-4474-aaa8-9976a71a337a_1280x1326.jpeg 848w, https://substackcdn.com/image/fetch/$s_!9OXU!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff78d9057-3f4d-4474-aaa8-9976a71a337a_1280x1326.jpeg 1272w, https://substackcdn.com/image/fetch/$s_!9OXU!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff78d9057-3f4d-4474-aaa8-9976a71a337a_1280x1326.jpeg 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p>Independent of one another, Stravinsky, Kandinsky, and Picasso each reacted to the modern world and “the alienation of the human mind from its own emotions” by pulling pre-modern styles and atavistic images into their art. What we call Modernism today was in most cases a reaction to modernity. It was an effort to excavate something ancient and honest about humanity in an age obsessed with and overrun by novelty.</p><p>Blom closes his chapter “1910: Human Nature Changed” by considering two intellectual giants of the time: the sociologist Max Weber and the psychoanalyst Sigmund Freud, whose International Psychoanalytic Association was founded in 1910. The tension between their theories of human nature are profoundly relevant today.</p><p><span>In his famous work </span><em>The Protestant Ethic and the Spirit of Capitalism</em><span>, Weber, a German sociologist, argued that certain Protestant—especially Calvinist—traditions supported habits that aligned with the development of modern capitalism. He argued that the Protestant tradition of northern European worshippers cultivated a disciplined approach to work, savings, and investment that proved valuable in commerce, while the Calvinist doctrine of divine grace “could lead believers to read worldly success as a possible sign of God’s favor,” as Blom summarizes. Weber believed that Protestantism not only encouraged followers to pour their energies into labor (hence the allusion to </span><em>Work Ethic</em><span> in the book’s title) but also helped create a culture of trade and investment that supported the rise of modern capitalism.</span></p><p><span>“It is easy to see how Freud’s analysis follows on from Weber’s,” Blom writes. To Freud, human nature was at risk of being fully dissolved by capitalism and modern society, like chalk dropped in acid. Beneath the polite masks demanded by modern society, he said, there lurked a more atavistic and instinctual self. Freud saw our psyche as a tug-of-war between the id (our animal urges) and superego (the voice in our head that internalizes society’s rules), with the ego stuck in the middle trying to negotiate an authentic identity in the face of mass inauthenticity. One of Freud’s most fantastic insights was that some people can channel or redirect their most raw and unacceptable urges toward productive and acceptable work. His name for this bit of psychological alchemy was </span><em>sublimation</em><span>. </span></p><p>Modern capitalism, in Freudian terms, was the sublimation of self-interest—or, one might even say, the sublimation of greed. “The suppression of natural urges is a necessary precondition for capitalist success,” Blom writes in summary, “but while it is productive for the group and its wealth, such an approach will eventually exact its revenge on the individual.” By this interpretation, the mass anxiety of the early 1900s—whether you call it neurasthenia, American Nervousness, or Newyorkitis—was price of modernity, technological development, and even capitalism itself.</p><p><span>There is little evidence that Freud and Weber ever debated one another. Yet when you set their theories side by side, it’s hard not to hear a conversation that still shapes much modern commentary. Weber wrote that modern capitalism evolved from religious doctrines that fit our nature, while Freud argued that human nature is unfit for a modern world that distorts and represses our basic urges. </span><em>Are our most impressive inventions the ultimate expression of our humanity, or are they the ultimate threat to it? </em><span>This is the question that every generation must answer for itself, including our own. It is a question equally worthy of the automobile and artificial intelligence. The troubling answer—for Weber and for Freud; for 1910 and for 2025—is: perhaps, both.</span></p></div></article></div><div id="discussion"><h4>Discussion about this post</h4></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[One Million Screenshots (248 pts)]]></title>
            <link>https://onemillionscreenshots.com/?q=random</link>
            <guid>44858067</guid>
            <pubDate>Sun, 10 Aug 2025 20:30:34 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://onemillionscreenshots.com/?q=random">https://onemillionscreenshots.com/?q=random</a>, See on <a href="https://news.ycombinator.com/item?id=44858067">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><a href="https://onemillionscreenshots.com/"><h2><span>One</span><span>Million</span><span>Screenshots</span></h2></a><p>Zoom into the web's top homepages</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Show HN: Bolt – A super-fast, statically-typed scripting language written in C (239 pts)]]></title>
            <link>https://github.com/Beariish/bolt</link>
            <guid>44856935</guid>
            <pubDate>Sun, 10 Aug 2025 17:53:09 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/Beariish/bolt">https://github.com/Beariish/bolt</a>, See on <a href="https://news.ycombinator.com/item?id=44856935">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><p dir="auto"><h2 tabindex="-1" dir="auto">⚡ Bolt</h2><a id="user-content--bolt" aria-label="Permalink: ⚡ Bolt" href="#-bolt"></a></p>
<p dir="auto">A <em>lightweight</em>, <strong>lightning-fast</strong>, type-safe embeddable language for real-time applications.</p>
<div dir="auto" data-snippet-clipboard-copy-content="import print, error, Error from core
import abs, epsilon from math

// The return type of safe_divide is inferred to be `Error | number`
fn safe_divide(a: number, b: number) {
    if abs(b) < epsilon {
        return error(&quot;Cannot divide by zero!&quot;)
    }

    return a / b
}

match let result = safe_divide(10, 5) {
    is Error {
        // The type of result is narrowed in this branch!
        print(&quot;Failed to divide:&quot;, result.what)
    }

    is number {
        print(&quot;The answer is&quot;, result)
    }
}"><pre><span>import</span> <span>print</span><span>,</span> <span>error</span><span>,</span> <span>Error</span> <span>from</span> <span>core</span>
<span>import</span> <span>abs</span><span>,</span> <span>epsilon</span> <span>from</span> <span>math</span>

<span>// The return type of safe_divide is inferred to be `Error | number`</span>
<span>fn</span> <span>safe_divide</span><span>(</span><span>a</span>: <span>number</span><span>,</span> <span>b</span>: <span>number</span><span>)</span><span></span> <span>{</span>
    <span>if</span> <span>abs</span><span>(</span><span>b</span><span>)</span> <span>&lt;</span> <span>epsilon</span> <span>{</span>
        <span>return</span> <span>error</span><span>(</span><span>"Cannot divide by zero!"</span><span>)</span>
    <span>}</span>

    <span>return</span> <span>a</span> <span>/</span> <span>b</span>
<span>}</span>

<span>match</span> <span>let</span> <span>result</span> <span>=</span> <span>safe_divide</span><span>(</span><span>10</span><span>,</span> <span>5</span><span>)</span><span></span> <span>{</span>
    <span>is</span> <span>Error</span> <span>{</span>
        <span>// The type of result is narrowed in this branch!</span>
        <span>print</span><span>(</span><span>"Failed to divide:"</span><span>,</span> <span>result</span><span>.</span><span>what</span><span>)</span>
    <span>}</span>

    <span>is</span> <span>number</span> <span>{</span>
        <span>print</span><span>(</span><span>"The answer is"</span><span>,</span> <span>result</span><span>)</span>
    <span>}</span>
<span>}</span></pre></div>
<p dir="auto"><h2 tabindex="-1" dir="auto">Features</h2><a id="user-content-features" aria-label="Permalink: Features" href="#features"></a></p>
<ul dir="auto">
<li><a href="https://github.com/Beariish/bolt/blob/main/doc/Bolt%20Performance.md">Lightning-fast performance</a>, outperforming other languages in its class</li>
<li>Compact implementation, leaving a minimal impact on build size while remaining consise enough to browse.</li>
<li>Blazingly quick compilation, plow through code at over 500kloc/thread/second. That's 50'000 lines in the blink of an eye.</li>
<li>Ease of embedding, only a handful of lines to get going</li>
<li>Rich type system to catch errors before code is ran, with plenty of support for extending it from native code</li>
<li>Embed-first design, prioritizing inter-language performance and agility</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Links</h2><a id="user-content-links" aria-label="Permalink: Links" href="#links"></a></p>
<ul dir="auto">
<li><strong><a href="https://github.com/Beariish/bolt/blob/main/doc/Bolt%20Programming%20Guide.md">Bolt programming guide</a></strong></li>
<li><strong><a href="https://github.com/Beariish/bolt/tree/main/doc/Bolt%20Standard%20Library">Bolt standard library reference</a></strong></li>
<li><strong><a href="https://github.com/Beariish/bolt/tree/main/doc/Bolt%20Embedding%20Guide.md">Bolt embedding and API reference</a></strong></li>
<li><strong><a href="https://github.com/Beariish/bolt/blob/main/doc/Bolt%20Performance.md">Bolt performance</a></strong></li>
<li><strong><a href="https://github.com/Beariish/bolt/blob/main/doc/Bolt%20Users.md">Notable Bolt users</a></strong></li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Dependencies</h2><a id="user-content-dependencies" aria-label="Permalink: Dependencies" href="#dependencies"></a></p>
<p dir="auto">Bolt only depends on the C standard library as well as <code>libm</code> on Unix-based systems.
Some standard library modules include things like file and system IO, but these can be disabled easily.
By default, Bolt sets up an environment that uses <code>malloc</code>/<code>realloc</code>/<code>free</code>, but this is also easy to configure.
Bolt also embeds my other library <a href="https://github.com/Beariish/picomatch">picomatch</a> for regex parsing</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Minimal embedding example</h2><a id="user-content-minimal-embedding-example" aria-label="Permalink: Minimal embedding example" href="#minimal-embedding-example"></a></p>
<p dir="auto">The <a href="https://github.com/Beariish/bolt/blob/main/bolt-cli/main.c">bolt-cli</a> program provides a very consice example of how to embed bolt an an application, see the <a href="https://github.com/Beariish/bolt/tree/main/doc/Bolt%20Embedding%20Guide.md">Bolt embedding guide</a> for more details.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Language examples</h2><a id="user-content-language-examples" aria-label="Permalink: Language examples" href="#language-examples"></a></p>
<p dir="auto">The <a href="https://github.com/Beariish/bolt/tree/main/examples">examples</a> folder contains a few short examples of ideomatically written bolt code. Check out the <a href="https://github.com/Beariish/bolt/tree/main/tests">tests</a> and <a href="https://github.com/Beariish/bolt/tree/main/benchmarks">benchmarks</a> folders as wel for some more in-depth language overview.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Building</h2><a id="user-content-building" aria-label="Permalink: Building" href="#building"></a></p>
<p dir="auto">Bolt currently only builds on x64. 32-bit architectures are explicitly not supported, arm and riscv are untested.
Running <code>cmake</code> in the root directory of the project will generate a static library for the language, as well as the CLI tool.
For more information and options regarding embedding Bolt in your application, see <code>bt_config.h</code>.
See below for the status of Bolt on each relevant compiler.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Compiler Status</h2><a id="user-content-compiler-status" aria-label="Permalink: Compiler Status" href="#compiler-status"></a></p>
<p dir="auto">Please note that Bolt is <strong>not</strong> yet stable, expect to encounter compiler bugs and crashes. If you do, opening an issue with replicable Bolt code would be much appreciated 😊</p>
<p dir="auto"><a href="https://github.com/Beariish/bolt/actions/workflows/cmake-multi-platform.yml"><img src="https://github.com/Beariish/bolt/actions/workflows/cmake-multi-platform.yml/badge.svg" alt="Build Status"></a></p>
<markdown-accessiblity-table><table>
<thead>
<tr>
<th>Compiler</th>
<th>Status</th>
<th>Reason</th>
</tr>
</thead>
<tbody>
<tr>
<td>MSVC</td>
<td>✅</td>
<td>no issues</td>
</tr>
<tr>
<td>GCC</td>
<td>✅🟨</td>
<td>all functional, some warnings</td>
</tr>
<tr>
<td>Clang</td>
<td>✅🟨</td>
<td>all functional, some warnings</td>
</tr>
</tbody>
</table></markdown-accessiblity-table>
<p dir="auto"><h2 tabindex="-1" dir="auto">Contributing</h2><a id="user-content-contributing" aria-label="Permalink: Contributing" href="#contributing"></a></p>
<p dir="auto">Bolt is a very opinionated project, and any contributions should take the vision into account.</p>
<p dir="auto">Bugfixes are likely to be accepted as long as they're within reason and don't change any expected behaviour. Adding tests in case of regression is very much appreciated as well. A clean run of <code>/tests/all</code> is expected of course.</p>
<p dir="auto">Optimizations may also be accepted for minor versions under similar criteria. A before/after run of <code>/benchmarks/all</code> is expected to evaluate the impact and make sure nothing else regresses. If the specific optimization isn't captured in any existing benchmark, adding one is required.</p>
<p dir="auto">Feature additions will need a lot of consideration, Bolt is very intentionally minimal in its' design and featureset. I highly suggest you submit some kind of proposal or plan before starting any significant work on a feature to review. Use cases, performance, and implementation cost will all be expected to be justified.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">License</h2><a id="user-content-license" aria-label="Permalink: License" href="#license"></a></p>
<p dir="auto">Bolt is licensed under MIT. See LICENSE for more information.</p>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Fight Chat Control (1283 pts)]]></title>
            <link>https://fightchatcontrol.eu/</link>
            <guid>44856426</guid>
            <pubDate>Sun, 10 Aug 2025 16:50:34 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://fightchatcontrol.eu/">https://fightchatcontrol.eu/</a>, See on <a href="https://news.ycombinator.com/item?id=44856426">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
        <!-- Hero Section -->
        <div>
                    <h2>
                        The EU (still) wants to scan <br> your private messages and photos
                    </h2>
                    <p>
                        The "Chat Control" proposal would mandate scanning of <strong>all</strong> private digital communications,
                        including encrypted messages and photos. This threatens <strong>fundamental privacy rights</strong> and digital security
                        for all EU citizens.
                    </p>
                    <div>
                        
                        <div>
                            <h3 id="support-count">15</h3>
                            <p>Member States Supporting</p>
                        </div>
                        <div>
                            <h3 id="undecided-count">9</h3>
                            <p>Member States Undecided</p>
                        </div>
                    </div>
                    
                </div>

        <!-- Overview Section -->
        <div id="overview">
                <h2>You Will Be Impacted</h2>
                <p>
                    Every photo, every message, every file you send will be automatically scanned—without your consent or suspicion. This is not about catching criminals. It is mass surveillance imposed on all 450 million citizens of the European Union.
                </p>
                <div>
                    <div>
                        <p>📱</p>
                        <h3>Mass Surveillance</h3>
                        <p>Every private message, photo, and file scanned automatically: no suspicion required, no exceptions*, even encrypted communications.</p>
                    </div>
                    <div>
                        <p>🔓️</p>
                        <h3>Breaking Encryption</h3>
                        <p>Weakening or breaking end-to-end encryption exposes everyone’s communications—including sensitive financial, medical, and private data—to hackers, criminals, and hostile actors.</p>
                    </div>
                    <div>
                        <p>⚖️</p>
                        <h3>Fundamental Rights</h3>
                        <p>Undermines your fundamental rights to privacy and data protection, as guaranteed by Articles 7 and 8 of the EU Charter—rights considered core to European democratic values.</p>
                    </div>
                    <div>
                        <p>🎯</p>
                        <h3>False Positives</h3>
                        <p>Automated scanners routinely misidentify innocent content, such as vacation photos or private jokes, as illegal, putting ordinary people at risk of false accusations and damaging investigations.</p>
                    </div>
                    <div>
                        <p>👨‍👩‍👧‍👦</p>
                        <h3>Ineffective Child Protection</h3>
                        <p>Child protection experts and organisations, including the UN, warn that mass surveillance fails to prevent abuse and actually makes children less safe—by weakening security for everyone and diverting resources from proven protective measures.</p>
                    </div>
                    <div>
                        <p>🌍</p>
                        <h3>Global Precedent</h3>
                        <p>Creates a dangerous global precedent enabling authoritarian governments, citing EU policy, to roll out intrusive surveillance at home, undermining privacy and free expression worldwide.</p>
                    </div>
                </div>
                
                <p>
                    *EU politicians exempt themselves from this surveillance under "professional secrecy" rules.
                    They get privacy.<br> You and your family do not. Demand fairness.
                </p>
            </div>

        <!-- Member States Section -->
        <div id="member-states">
                <h2>Member State Positions</h2>
                <br>
                
                
            </div>

        <!-- Delegates Section -->
        <div id="delegates">
                <h2>Find Your Representatives</h2>
                <br>
                
                
            </div>

        <!-- Contact Tool Section -->
        <div id="contact-tool">
                <h2>Take Action!<br> Contact Your MEPs</h2>
                <h3>
                    Your privacy and freedoms are at risk.
                    These policies will impact every European—your messages, photos, and private conversations will be scanned without your consent.
                    But we have the power to stop this.
                    Contact your MEPs now with a clear message: NO to mass surveillance.
                    Your voice matters. Make it heard today.
                </h3>
                
            </div>

        <!-- Timeline Section -->
        
    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Diffusion language models are super data learners (205 pts)]]></title>
            <link>https://jinjieni.notion.site/Diffusion-Language-Models-are-Super-Data-Learners-239d8f03a866800ab196e49928c019ac</link>
            <guid>44856101</guid>
            <pubDate>Sun, 10 Aug 2025 16:04:05 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://jinjieni.notion.site/Diffusion-Language-Models-are-Super-Data-Learners-239d8f03a866800ab196e49928c019ac">https://jinjieni.notion.site/Diffusion-Language-Models-are-Super-Data-Learners-239d8f03a866800ab196e49928c019ac</a>, See on <a href="https://news.ycombinator.com/item?id=44856101">Hacker News</a></p>
&lt;Unparsable&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[AOL closes its dial up internet service (160 pts)]]></title>
            <link>https://www.ispreview.co.uk/index.php/2025/08/after-34-years-aol-finally-closes-its-dial-up-internet-service.html</link>
            <guid>44856090</guid>
            <pubDate>Sun, 10 Aug 2025 16:02:17 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.ispreview.co.uk/index.php/2025/08/after-34-years-aol-finally-closes-its-dial-up-internet-service.html">https://www.ispreview.co.uk/index.php/2025/08/after-34-years-aol-finally-closes-its-dial-up-internet-service.html</a>, See on <a href="https://news.ycombinator.com/item?id=44856090">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
		<div id="news_thumbpicture"><picture id="primaryimage"><img width="600" height="600" src="https://www.ispreview.co.uk/wp-content/uploads/2025/08/nggallery_import/aol-uk-homepage-from-the-past-e1754805650657-600x600.webp" alt="aol uk homepage from the past" decoding="async" fetchpriority="high" srcset="https://www.ispreview.co.uk/wp-content/uploads/2025/08/nggallery_import/aol-uk-homepage-from-the-past-e1754805650657-600x600.webp 600w, https://www.ispreview.co.uk/wp-content/uploads/2025/08/nggallery_import/aol-uk-homepage-from-the-past-e1754805650657-300x300.webp 300w, https://www.ispreview.co.uk/wp-content/uploads/2025/08/nggallery_import/aol-uk-homepage-from-the-past-e1754805650657-768x768.webp 768w, https://www.ispreview.co.uk/wp-content/uploads/2025/08/nggallery_import/aol-uk-homepage-from-the-past-e1754805650657-150x150.webp 150w, https://www.ispreview.co.uk/wp-content/uploads/2025/08/nggallery_import/aol-uk-homepage-from-the-past-e1754805650657.webp 1000w" sizes="(max-width: 600px) 100vw, 600px"></picture></div><!-- Article Start --><p>In a somewhat surprising development, mainly because almost everybody assumed it had died a long time ago, <a href="https://www.aol.co.uk/" target="_blank" rel="noopener">AOL</a> (America Online) – one of the very first consumer ISPs in both the USA and UK – recently caused a stir again by announcing that it had “<em>decided to discontinue Dial-up Internet</em>” on 30th September 2025.<span id="more-42747"></span></p>
<p>According to <a href="https://help.aol.com/articles/dial-up-internet-to-be-discontinued" target="_blank" rel="noopener">AOL’s website</a>: “<em>AOL routinely evaluates its products and services and has decided to discontinue Dial-up Internet. This service will no longer be available in AOL plans. As a result, on September 30, 2025 this service and the associated software, the AOL Dialer software and AOL Shield browser, which are optimized for older operating systems and dial-up internet connections, will be discontinued</em>.” But their email service will continue.</p>
<p><strong>NOTE:</strong> Many <a href="https://www.ispreview.co.uk/index.php/link/dialup" target="_blank" rel="" title="dialup" data-chref="https://www.ispreview.co.uk/broadband_dialup.php">dialup</a> ISPs in the UK during 1995 – like AOL – used expensive premium rate numbers, although this did soon gravitate to local call rates and then unmetered via FRIACO. The v90 <a href="https://www.ispreview.co.uk/index.php/link/dialup" target="_blank" rel="" title="dialup" data-chref="https://www.ispreview.co.uk/broadband_dialup.php">dialup</a> standard (56Kbps capable or 0.056Mbps) didn’t arrive until 1998 and by then <a href="https://www.ispreview.co.uk/index.php/link/adsl" target="_blank" rel="" title="digital subscriber line" data-chref="https://www.ispreview.co.uk/broadband_DSL.php">ADSL</a> and cable broadband were just around the corner – ready to revolutionise the market.</p>
<p>The change appears to have been announced within the past few weeks, although it wasn’t picked up more widely until journalist <a href="https://tedium.co/" target="_blank" rel="noopener">Ernie Smith</a> noted it in a post on <a href="https://bsky.app/profile/ernie.tedium.co/post/3lvwjugziec2f" target="_blank" rel="noopener">Bluesky</a>. Just to be clear, the announcement above refers to the USA and Canada. However, we’re fairly confident that what remains of AOL UK (aka – <a href="https://www.ispreview.co.uk/index.php/go/tt" target="_blank" rel="nofollow" title="talktalk">TalkTalk</a>) doesn’t have any legacy dial-up customers left, although we would have said the same about the USA and Canada too, until that announcement dropped (dial-up speeds in 2025 would be practically unusable). ISPreview is currently checking, just to be sure.</p>

<p>In case anybody has forgotten. The original AOL UK experience was somewhat of a walled-garden way of accessing the internet, which forced you to use the company’s own software and restricted your ability to access certain internet services. This had the benefit of simplifying the experience, but AOL later fell behind the curve and ended up being overtaken by rivals.</p>
<p>The <a title="carphone warehouse" href="https://www.ispreview.co.uk/index.php/go/cpw" target="_blank" rel="nofollow">Carphone Warehouse</a> (CPW) ultimately won the auction to buy AOL UK’s Internet access business in 2006 for £370m (note: AOL’s content division became a separate business). At the time, AOL were the UK’s third-largest ISP with around <strong>2.1 million customers</strong> (600,000 on dial-up and 1.5 million with broadband) and were later re-branded to AOL Broadband.</p>
<p>A second big change occurred on 29th March 2010, when CPW and <a href="https://www.ispreview.co.uk/index.php/go/tt" target="_blank" rel="nofollow" title="talktalk">TalkTalk</a> separated (demerged) – the latter became a separate business, which included customers from CPW’s prior acquisitions (e.g. AOL Broadband, Tiscali etc.). Several more years passed until May 2014, when TalkTalk confirmed that AOL Broadband (formerly AOL UK) had stopped taking on new internet and phone customers (<a href="https://www.ispreview.co.uk/index.php/2014/05/uk-isp-aol-broadband-longer-available-new-customers.html">here</a>), although no mention was made of the dial-up base.</p>
<p>We’re certain that plenty of our readers (those now of a certain age group) will have stories to share of the early AOL UK days. Yours truly only used the original service briefly, before promptly switching away as the UK’s then dialup (narrowband) internet market became more competitive, affordable and less restrictive. It’s a service I was glad to forget, but it played an important role.</p>

<p><iframe title="AOL (Sign On - Dial Up)" width="500" height="281" src="https://www.youtube.com/embed/D1UY7eDRXrs?feature=oembed" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen=""></iframe></p>
<!-- CONTENT END 1 -->
<!-- Article End -->
	</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Zig's Lovely Syntax (222 pts)]]></title>
            <link>https://matklad.github.io/2025/08/09/zigs-lovely-syntax.html</link>
            <guid>44855881</guid>
            <pubDate>Sun, 10 Aug 2025 15:33:06 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://matklad.github.io/2025/08/09/zigs-lovely-syntax.html">https://matklad.github.io/2025/08/09/zigs-lovely-syntax.html</a>, See on <a href="https://news.ycombinator.com/item?id=44855881">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
      <article>
        <h2>
          Zig’s Lovely Syntax <time datetime="2025-08-09">Aug 9, 2025</time>
        </h2>
        <p>
          It’s a bit of a silly post, because syntax is the least interesting
          detail about the language, but, still, I can’t stop thinking how Zig
          gets this detail just right for the class of curly-braced languages,
          and, well, now you’ll have to think about that too.
        </p>
        <p>
          On the first glance, Zig looks almost exactly like Rust, because Zig
          borrows from Rust liberally. And I think that Rust has great syntax,
          considering all the semantics it needs to express (see
          <a href="https://matklad.github.io/2023/01/26/rusts-ugly-syntax.html">“Rust’s Ugly Syntax”</a>). But Zig improves on that, mostly by
          leveraging simpler language semantics, but also through some purely
          syntactical tasteful decisions.
        </p>
        <section id="Integer-Literals">
          <h2>
            <a href="#Integer-Literals">Integer Literals </a>
          </h2>
          <p>
            How do you spell a number ninety-two? Easy, <code>92</code>. But
            what type is that? Statically-typed languages often come with
            several flavors of integers: <code>u32</code>, <code>u64</code>,
            <code>u8</code>. And there’s often a syntax for literals of a
            particular types: <code>92u8</code>, <code>92l</code>, <code>92z</code>.
          </p>
          <p>
            Zig doesn’t have suffixes, because, in Zig, all integer literals
            have the same type: <code>comptime_int</code>:
          </p>

          <figure>
            <pre><code><span><span>const</span> an_integer = <span>92</span>;</span>
<span>assert(<span>@TypeOf</span>(an_integer) <span>==</span> <span>comptime_int</span>);</span></code></pre>
          </figure>
          <p>
            The value of an integer literal is known at compile time and is
            coerced to a specific type on assignment
            <span><code>const x: i32 = 92;</code></span>
            or ascription:
            <span><code>@as(i32, 92)</code></span>
          </p>
          <p>
            To emphasize, this is <em>not</em> type inference, this is implicit
            comptime coercion. This does mean that code like
            <span><code>var x = 92;</code></span>
            generally doesn’t work, and requires an explicit type.
          </p>
        </section>
        <section id="String-Literals">
          <h2>
            <a href="#String-Literals">String Literals </a>
          </h2>
          <p>Raw or multiline strings are spelled like this:</p>

          <figure>
            <pre><code><span><span>const</span> raw =</span>
<span>    <span>\\Roses are red</span></span>
<span>    <span>\\  Violets are blue,</span></span>
<span>    <span>\\Sugar is sweet</span></span>
<span>    <span>\\  And so are you.</span></span>
<span>    <span>\\</span></span>
<span>;</span></code></pre>
          </figure>
          <p>
            This syntax doesn’t require a special form for escaping <code>\\</code> itself:
          </p>

          <figure>
            <pre><code><span><span>const</span> still_raw =</span>
<span>    <span>\\const raw =</span></span>
<span>    <span>\\    <span>\\</span>Roses are red</span></span>
<span>    <span>\\    <span>\\</span>  Violets are blue,</span></span>
<span>    <span>\\    <span>\\</span>Sugar is sweet</span></span>
<span>    <span>\\    <span>\\</span>  And so are you.</span></span>
<span>    <span>\\    <span>\\</span></span></span>
<span>    <span>\\;</span></span>
<span>    <span>\\</span></span>
<span>;</span></code></pre>
          </figure>
          <p>
            It nicely dodges indentation problems that plague every other
            language with a similar feature. And, the best thing ever:
            lexically, each line is a separate token. As Zig has only
            line-comments, this means that <code>\n</code> is <em>always</em>
            whitespace. Unlike most other languages, Zig can be correctly lexed
            in a line-by-line manner.
          </p>
          <p>
            Raw strings is perhaps the biggest improvement of Zig over Rust.
            Rust brute-forces the problem with
            <code>r##""##</code> syntax, which does the required job,
            technically, but suffers from the mentioned problems: indentation is
            messy, nesting quotes requires adjusting hashes, unclosed raw
            literal breaks the following lexical structure completely, and
            rustfmt’s formatting of raw strings tends to be rather ugly. On the
            plus side, this syntax at least cannot be expressed by a
            context-free grammar!
          </p>
        </section>
        <section id="Record-Literals">
          <h2>
            <a href="#Record-Literals">Record Literals </a>
          </h2>
          <p>For the record, Zig takes C syntax (not that C would notice):</p>

          <figure>
            <pre><code><span><span>const</span> p: Point = .{</span>
<span>    .x = <span>1</span>,</span>
<span>    .y = <span>2</span>,</span>
<span>}</span></code></pre>
          </figure>
          <p>
            The <code>.{</code> feels weird! It will make sense by the end of
            the post. Here, I want only to note <code>.x = 1</code>
            part, which matches the assignment syntax <code>obj.x = 1</code>.
            This is great! This means that grepping for
            <code>".x ="</code> gives you <em>all</em> instances where a field
            is written to. This is hugely valuable: most of usages are reads,
            but, to understand the flow of data, you only need to consider
            writes. Ability to mechanically partition the entire set of usages
            into majority of boring reads and a few interesting writes does
            wonders for code comprehension.
          </p>
        </section>
        <section id="Prefix-Types">
          <h2>
            <a href="#Prefix-Types">Prefix Types </a>
          </h2>
          <p>
            Where Zig departs from C the most is the syntax for types. C uses a
            needlessly confusing spiral rule. In Zig, all types are prefix:
          </p>

          <figure>
            <pre><code><span><span>u32</span>      <span>// An integer</span></span>
<span>[<span>3</span>]<span>u32</span>   <span>// An array of three integers</span></span>
<span>?[<span>3</span>]<span>u32</span>  <span>// An array of three integers or null</span></span>
<span></span>
<span><span>// A pointer to...</span></span>
<span><span>*</span><span>const</span> ?[<span>3</span>]<span>u32</span></span></code></pre>
          </figure>
          <p>
            While pointer type is prefix, pointer dereference is postfix, which
            is a more natural subject-verb order to read: <span><code>ptr.* = 92;</code></span>
          </p>
        </section>
        <section id="Identifiers">
          <h2>
            <a href="#Identifiers">Identifiers </a>
          </h2>
          <p>
            Zig has general syntax for “raw” identifiers:
            <span><code>@"a name which a space"</code></span>
            It is useful to avoid collisions with keywords, or for exporting a
            symbol whose name is otherwise not a valid Zig identifier. It is a
            bit more to type than Kotlin’s delightful
            <span><code>`a name with a space`</code>,</span> but
            manages to re-use Zig’s syntax for built-ins (<code>@TypeOf</code>)
            and strings.
          </p>
        </section>
        <section id="Functions">
          <h2>
            <a href="#Functions">Functions </a>
          </h2>
          <p>
            Like, Rust, Zig goes for <code>fn foo</code> function declaration
            syntax. This is such a massive improvement over C/Java style
            function declarations: it puts <code>fn</code> token (which is
            completely absent in traditional C family) and function name next to
            each other, which means that textual search for <code>fn name</code>
            allows you to quickly find the function. Then Zig adds a little
            twist. While in Rust we write
          </p>

          <figure>
            <pre><code><span><span>fn</span> <span>add</span>(x: <span>i32</span>, <span>i32</span>) <span>-&gt;</span> <span>i32</span></span></code></pre>
          </figure>
          <p>Zig is</p>

          <figure>
            <pre><code><span><span>fn</span><span> add</span>(x: <span>i32</span>, <span>i32</span>) <span>i32</span></span></code></pre>
          </figure>
          <p>
            The arrow is gone! Now that I’ve used this for some time, I find
            arrow very annoying to type, and adding to the visual noise. Rust
            needs the arrow: Rust has lambdas with an inferred return type, and,
            in a lambda, the return type is optional. So you need some sort of
            an explicit syntax to tell the parser if there is return type:
          </p>

          <figure>
            <pre><code><span>|| expression;</span>
<span>|| <span>-&gt;</span> Type { }</span></code></pre>
          </figure>
          <p>
            And its understandable that lambdas and functions would want to use
            compatible syntax. But Zig doesn’t have lambdas, so it just makes
            the type mandatory. So the main is
          </p>

          <figure>
            <pre><code><span><span>pub</span> <span>fn</span><span> main</span>() <span>void</span> {}</span></code></pre>
          </figure>
          <p>
            Related small thing, but, as name of the type, I think I like <code>void</code> more than <code>()</code>.
          </p>
        </section>
        <section id="Locals">
          <h2>
            <a href="#Locals">Locals </a>
          </h2>
          <p>
            Zig is using <code>const</code> and <code>var</code> for binding
            values to names:
          </p>

          <figure>
            <pre><code><span><span>const</span> mid = lo <span>+</span> <span>@divFloor</span>(hi <span>-</span> lo, <span>2</span>);</span></code></pre>
          </figure>
          <p>
            This is ok, a bit weird after Rust’s, whose <code>const</code> would
            be <code>comptime</code> in Zig, but not really noticeable after
            some months. I do think this particular part is not great, because
            <code>const</code>, the more frequent one, is longer. I think Kotlin
            nails it: <code>val</code>, <code>var</code>, <code>fun</code>. Note
            all three are monosyllable, unlike <code>const</code> and <code>fn</code>! Number of syllables matters more than the number of
            letters!
          </p>
          <p>Like Rust, Zig uses</p>

          <figure>
            <pre><code><span><span>'name'</span> (<span>':'</span> Type)?</span></code></pre>
          </figure>
          <p>syntax for ascribing types, which is better than</p>

          <figure>
            <pre><code><span>Type <span>'name'</span></span></code></pre>
          </figure>
          <p>
            because optional suffixes are easier to parse visually and
            mechanically than optional prefixes.
          </p>
        </section>
        <section id="Conjunction-Is-Control-Flow">
          <h2>
            <a href="#Conjunction-Is-Control-Flow">Conjunction Is Control Flow
            </a>
          </h2>
          <p>
            Zig doesn’t use <code>&amp;&amp;</code> and <code>||</code> and
            spells the relevant operators as <code>and</code> and <code>or</code>:
          </p>

          <figure>
            <pre><code><span><span>while</span> (count &gt; <span>0</span> <span>and</span> ascii.isWhitespace(buffer[count <span>-</span> <span>1</span>])) {</span></code></pre>
          </figure>
          <p>
            This is easier to type and much easier to read, but there’s also a
            deeper reason why they are not sigils. Zig marks any control flow
            with a keyword. And, because boolean operators short-circuit, they
            <em>are</em> control flow! Treating them as normal binary operator
            leads to an entirely incorrect mental model. For bitwise operations,
            Zig of course uses <code>&amp;</code> and <code>|</code>.
          </p>
        </section>
        <section id="Explicit-return">
          <h2>
            <a href="#Explicit-return">Explicit return </a>
          </h2>
          <p>
            Both Zig and Rust have statements and expressions. Zig is a bit more
            statement oriented, and requires explicit returns:
          </p>

          <figure>
            <pre><code><span><span>fn</span><span> add</span>(x: <span>i32</span>, y: <span>i32</span>) <span>i32</span> {</span>
<span>  <span>return</span> x <span>+</span> y;</span>
<span>}</span></code></pre>
          </figure>
          <p>
            Furthermore, because there are no lambdas, scope of return is always
            clear.
          </p>
          <p>
            Relatedly, the value of a block expression is void. A block is a
            list of statements, and doesn’t have an optional expression at the
            end. This removes the semicolon problem — while Rust rules around
            semicolons are sufficiently clear (until you get to macros), there’s
            some constant mental overhead to getting them right all the time.
            Zig is more uniform and mechanical here.
          </p>
          <p>
            If you need a block that yields a value, Zig supports a general
            syntax for breaking out of a labeled block:
          </p>

          <figure>
            <pre><code><span><span>const</span> header_oldest = blk: {</span>
<span>    <span>var</span> oldest: ?<span>usize</span> = <span>null</span>;</span>
<span>    <span>for</span> (headers.slice, <span>0</span>..) <span>|</span><span>*</span>header, i<span>|</span> {</span>
<span>        <span>switch</span> (Headers.dvc_header_type(header)) {</span>
<span>            .blank =&gt; assert(i &gt; <span>0</span>),</span>
<span>            .valid =&gt; oldest = i,</span>
<span>        }</span>
<span>    }</span>
<span>    <span>break</span> :blk <span>&amp;</span>headers.slice[oldest.?];</span>
<span>};</span></code></pre>
          </figure>
        </section>
        <section id="If">
          <h2>
            <a href="#If">If </a>
          </h2>
          <p>
            Rust makes pedantically correct choice regarding <code>if</code>s:
            braces are mandatory:
          </p>

          <figure>
            <pre><code><span><span>if</span> cond1 {</span>
<span>  case_a</span>
<span>} <span>else</span> {</span>
<span>  <span>if</span> cond2 {</span>
<span>    case_b</span>
<span>  } <span>else</span> {</span>
<span>    case_c</span>
<span>  }</span>
<span>}</span></code></pre>
          </figure>
          <p>
            This removes the dreaded “dangling else” grammatical ambiguity.
            While theoretically nice, it makes
            <code>if</code>-expression one-line feel too heavy. It’s not the
            braces, it’s the whitespace around them:
          </p>

          <figure>
            <pre><code><span>if (a) b else c</span>
<span>if a { b } else { c }</span></code></pre>
          </figure>
          <p>
            But the ternary is important! Exploding a simple choice into
            multi-line condition <em>hurts</em>
            readability. Zig goes with traditional choice of making parentheses
            required and braces optional:
          </p>

          <figure>
            <pre><code><span>  .direction = <span>if</span> (prng.boolean()) .ascending <span>else</span> .descending,</span></code></pre>
          </figure>
          <p>
            By itself, this does create a risk of <code>goto: fail;</code> style
            bugs. But in Zig formatter (non-configurable, user-directed) is a
            part of the compiler, and formatting errors that can mask bugs are
            caught during compilation. For example, <code>1 -2</code> is an
            error due to inconsistent whitespace around the minus sign, which
            signals a plausible mixup of infix and binary minus. No such errors
            are currently produced for incorrect indentation (the value add
            there is relatively little, given <code>zig fmt</code>), but this is
            planned.
          </p>
          <p>
            NB: because Rust requires <code>if</code> branches to be blocks, it
            is forced to make <code>{ expr }</code> synonym with
            <code>(expr)</code>. Otherwise, the ternary <code>if</code> would be
            even more unusable! Syntax design is tricky! Whether you need <code>return</code>s and whether you make <code>()</code> or <code>{}</code> mandatory in ifs are not orthogonal!
          </p>
        </section>
        <section id="Loops">
          <h2>
            <a href="#Loops">Loops </a>
          </h2>
          <p>
            Like Python, Zig allows <code>else</code> on loops. Unlike Python,
            loops are expressions, which leads to a nicely readable imperative
            searches:
          </p>

          <figure>
            <pre><code><span><span>pub</span> <span>const</span> Word = <span>for</span> (.{ <span>u8</span>, <span>u16</span>, <span>u32</span>, <span>u64</span>, <span>u128</span>, <span>u256</span> }) <span>|</span>W<span>|</span> {</span>
<span>    <span>if</span> (<span>@bitSizeOf</span>(W) &gt;= bitset_capacity) <span>break</span> W;</span>
<span>} <span>else</span> <span>unreachable</span>;</span></code></pre>
          </figure>
          <p>
            Zig doesn’t have syntactically-infinite loop like Rust’s <code>loop
              {</code> or Go’s <code>for {</code>. Normally I’d consider that a
            drawback, because these loops produce different control flow,
            affecting reachability analysis in the compiler, and I don’t think
            it’s great to make reachability dependent on condition being visibly
            constant. But! As Zig places <code>comptime</code> semantics front
            and center, and the rules for what is and isn’t a comptime constant
            are a backbone of every feature, “anything equivalent to
            <code>while (true)</code>” becomes sufficiently precise.
            Incidentally, these days I tend to write “infinite” loops as
          </p>

          <figure>
            <pre><code><span><span>for</span> (<span>0</span>..safety_bound) <span>|</span>_<span>|</span> {</span>
<span></span>
<span>} <span>else</span> <span>@panic</span>(<span>"loop safety counter exceeded"</span>);</span></code></pre>
          </figure>
          <p>
            Almost always there is an up-front bound for the number of
            iterations until the break, and its worth asserting this bound,
            because debugging crashes is easier than debugging hangs.
          </p>
          <p>
            <code>for</code>, <code>while</code>, <code>if</code>, <code>switch</code>, and <code>catch</code> all use the same Ruby/Rust
            inspired syntax for naming captured values:
          </p>

          <figure>
            <pre><code><span><span>for</span> (slice) <span>|</span>element<span>|</span> {</span>
<span>  use(element);</span>
<span>}</span>
<span></span>
<span><span>while</span> (iterator.next()) <span>|</span>element<span>|</span> {</span>
<span>  use(element);</span>
<span>}</span></code></pre>
          </figure>
          <p>
            I like how the iterator comes first, and then the name of an item
            follows, logically and syntactically.
          </p>
        </section>
        <section id="Clarity-of-Names">
          <h2>
            <a href="#Clarity-of-Names">Clarity of Names </a>
          </h2>
          <p>
            I have a very strong opinion about variable shadowing. It goes both
            ways: I spent hours debugging code which incorrectly tried to use a
            variable that was shadowed by something else, but I also spent hours
            debugging code that accidentally used a variable that should have
            been shadowed! I really don’t know whether on balance it is better
            to forbid or encourage shadowing!
          </p>
          <p>
            Zig of course forbids shadowing, but what’s curious is that it’s
            just one episode of the large crusade against any complexity in name
            resolution. There’s no “prelude”, if you want to use anything from
            std, you need to import it:
          </p>

          <figure>
            <pre><code><span><span>const</span> std = <span>@import</span>(<span>"std"</span>);</span></code></pre>
          </figure>
          <p>
            There are no glob imports, if you want to use an item from std, you
            need to import it:
          </p>

          <figure>
            <pre><code><span><span>const</span> ArrayList = std.ArrayList;</span></code></pre>
          </figure>
          <p>
            Zig doesn’t have inheritance, mixins, argument-dependent lookup,
            extension functions, implicit or traits, so, if you see <code>x.foo()</code>, that <code>foo</code> is guaranteed to be a boring
            method declared on <code>x</code>
            type. Similarly, while Zig has powerful comptime capabilities, it
            <a href="https://matklad.github.io/2025/04/19/things-zig-comptime-wont-do.html">intentionally disallows</a>
            declaring methods at compile time.
          </p>
          <p>
            Like Rust, Zig used to allow a method and a field to share a name,
            because it actually is syntactically clear enough at the call site
            which is which. But then this feature got removed from Zig.
          </p>
          <p>
            More generally, Zig doesn’t have namespaces. There can be only one
            kind of <code>foo</code> in scope, while Rust allows things like
          </p>

          <figure>
            <pre><code><span><span>struct</span> <span>Point</span> { x: <span>i32</span>, y: <span>i32</span> }</span>
<span><span>fn</span> <span>Point</span>(x: <span>i32</span>, y: <span>i32</span>) <span>-&gt;</span> Point { Point { x, y } }</span></code></pre>
          </figure>
          <p>
            I am astonished at the relative lack of inconvenience in Zig’s
            approach. Turns out that <code>foo.bar.baz</code>
            is all the syntax you’ll ever need for accessing things? For the
            historically inclined, see “The module naming situation” thread in
            the
            <a href="https://github.com/brson/rust-dev-archives">rust mailing list archive</a>
            to learn the story of how rust got its <code>std::vec</code> syntax.
          </p>
        </section>
        <section id="Everything-Is-an-Expression">
          <h2>
            <a href="#Everything-Is-an-Expression">Everything Is an Expression
            </a>
          </h2>
          <p>
            The lack of namespaces touches on the most notable (by its absence)
            feature of Zig syntax, which deeply relates to the most profound
            aspect of Zig’s semantics. Everything is an expression. By which I
            mean, there’s no separate syntactic categories of values, types, and
            patterns. Values, types, and patterns are of course different
            things. And usually in the language grammar it is <em>syntactically</em>
            obvious whether a particular text fragment refers to a type or a
            value:
          </p>

          <figure>
            <pre><code><span><span>let</span> <span>PATTERN</span>: TYPE = VALUE;</span></code></pre>
          </figure>
          <p>
            So the standard way is to have separate syntax families for the
            three categories, which need to be internally unambiguous, but <em>can</em> be ambiguous across the categories because the place in
            the grammar dictates the category: when parsing <code>let</code>,
            everything until <code>:</code> is a pattern, stuff between
            <code>:</code> and <code>=</code> is a type, and after <code>=</code> we have a value.
          </p>
          <p>
            There are two problems here. First, there’s a combinatorial
            explosion of sorts in the syntax, because, while three categories
            describe different things, it turns out that they have the same
            general tree-ish shape.
          </p>
          <p>
            The second problem is that it might be hard to maintain category
            separation in the grammar. Rust
            <em>started</em> with the three categories separated by a bright
            line. But then, changes happen. Originally, Rust only allowed
            <span><code>VALUE = VALUE;</code></span>
            syntax for assignment. But today you can also write
            <span><code>PATTERN = VALUE;</code></span>
            to do unpacking like
            <span><code>(a, b) = (b, a);</code></span>
          </p>
          <p>
            Similarly, the turbofish used to move the parser from the value to
            the type mode, but now const parameters are values that can be found
            in the type position!
          </p>
          <p>
            The alternative is not to pick this fight at all. Rather than trying
            to keep the categories separately in the syntax, use the same
            surface syntax to express all three, and categorize later, during
            semantic analysis. In fact, this is already happens in the <span><code>VALUE = VALUE</code></span>
            example — these are different things! One is a place (lvalue) and
            another is a “true” value (rvalue), but we use the same syntax for
            both.
          </p>
          <p>
            I don’t think such syntactic unification necessarily implies
            semantic unification, but Zig does treat everything uniformly, as a
            value with comptime and runtime behavior (for some values, runtime
            behavior may be missing, for others — comptime):
          </p>

          <figure>
            <pre><code><span><span>const</span> E = <span>enum</span> { a, b };</span>
<span></span>
<span><span>pub</span> <span>fn</span><span> main</span>() <span>void</span> {</span>
<span>    <span>const</span> e: <span>if</span> (<span>true</span>) E <span>else</span> <span>void</span> = .a;</span>
<span>    _ = <span>switch</span> (e) {</span>
<span>        (<span>if</span> (<span>true</span>) .a <span>else</span> .b) =&gt; .a,</span>
<span>        (<span>if</span> (<span>true</span>) .b <span>else</span> .a) =&gt; .b,</span>
<span>    };</span>
<span>}</span></code></pre>
          </figure>
          <p>
            The fact that you can write an <code>if</code> where a type goes is
            occasionally useful. But the fact that simple types look like simple
            values syntactically consistently make the language feel
            significantly less busy.
          </p>
        </section>
        <section id="Generics">
          <h2>
            <a href="#Generics">Generics </a>
          </h2>
          <p>
            As a special case of everything being an expression, instances of
            generic types look like this:
            <span><code>ArrayList(u32)</code></span>
          </p>
          <p>
            Just a function call! Though, there’s some resistance to trickery
            involved to make this work. Usually, languages rely on type
            inference to allow eliding generic arguments. That in turn requires
            making argument <em>syntax</em> optional, and that in turn leads to
            separating generic and non-generic arguments into separate parameter
            lists and some introducer sigil for generics, like <code>::&lt;&gt;</code> or
            <code>!()</code>.
          </p>
          <p>
            Zig solves this syntactic challenge in the most brute-force way
            possible. Generic parameters are never inferred, if a function takes
            3 comptime arguments and 2 runtime arguments, it will always be
            called with 5 arguments syntactically. Like with the (absence of)
            importing flourishes, a reasonable reaction would be “wait, does
            this mean that I’ll have to specify the types all the time?” And,
            like with import, in practice this is a non-issue. The trick are
            comptime closures. Consider a generic
            <code>ArrayList</code>:
          </p>

          <figure>
            <pre><code><span><span>fn</span><span> ArrayListType</span>(<span>comptime</span> T: <span>type</span>) <span>type</span> {</span>
<span>    <span>return</span> <span>struct</span> {</span>
<span>        <span>const</span> ArrayList = <span>@This</span>();</span>
<span></span>
<span>        <span>fn</span><span> init</span>(gpa: Allocator) ArrayList {}</span>
<span>        <span>fn</span><span> deinit</span>(list: <span>*</span>ArrayList, gpa: Allocator) <span>void</span> {}</span>
<span>        <span>fn</span><span> push</span>(list: <span>*</span>ArrayList, item: T) <span>!</span><span>void</span> {}</span>
<span>    };</span>
<span>}</span>
<span></span>
<span><span>fn</span><span> usage</span>(gpa: Allocator) <span>!</span><span>void</span> {</span>
<span>    <span>var</span> xs: ArrayListType(<span>u32</span>) = .init(gpa);</span>
<span>    <span>defer</span> xs.deinit(gpa);</span>
<span></span>
<span>    <span>try</span> xs.push(<span>92</span>);</span>
<span>}</span></code></pre>
          </figure>
          <p>
            We have to specify type <code>T</code> when creating an instance of
            an <code>ArrayList</code>. But subsequently, when we are <em>using</em> the array list, we don’t have to specify the type
            parameter again, because the type of
            <code>xs</code> variable already closes over <code>T</code>. This is
            the major truth of object-orienting programming, the truth so
            profound that no one even notices it: in real code, 90% of functions
            are happiest as (non-virtual) methods. And, because of that, the
            annotation burden in real-world Zig programs is low.
          </p>
        </section>
        <section id="Declaration-Literals">
          <h2>
            <a href="#Declaration-Literals">Declaration Literals </a>
          </h2>
          <p>
            While Zig doesn’t have Hindley-Milner constraint-based type
            inference, it relies heavily on one specific way to propagate types.
            Let’s revisit the first <code>comptime_int</code> example:
          </p>

          <figure>
            <pre><code><span><span>const</span> x = <span>if</span> (condition()) <span>1</span> <span>else</span> <span>2</span>;</span></code></pre>
          </figure>
          <p>
            This doesn’t compile: <code>1</code> and <code>2</code> are
            different <code>comptime</code> values, we can’t select between two
            at runtime because they are different. We need to coerce the
            constants to a specific runtime type:
          </p>

          <figure>
            <pre><code><span><span>const</span> x: <span>u32</span> = <span>if</span> (condition()) <span>1</span> <span>else</span> <span>2</span>;</span>
<span></span>
<span><span>const</span> x = <span>@coerceTo</span>(</span>
<span>  <span>u32</span>,</span>
<span>  <span>if</span> (condition()) <span>1</span> <span>else</span> <span>2</span>,</span>
<span>);</span></code></pre>
          </figure>
          <p>
            But this doesn’t kick the can sufficiently far enough and
            essentially reproduces the <code>if</code> with two incompatible
            branches. We need to sink coercion down the branches:
          </p>

          <figure>
            <pre><code><span><span>const</span> x = <span>if</span> (condition())</span>
<span>    <span>@coerceTo</span>(<span>u32</span>, <span>1</span>)</span>
<span><span>else</span></span>
<span>    <span>@coerceTo</span>(<span>u32</span>, <span>2</span>);</span></code></pre>
          </figure>
          <p>
            And that’s exactly how Zig’s “Result Location Semantics” works. Type
            “inference” runs a simple left-to-right tree-walking algorithm,
            which resembles interpreter’s <code>eval</code>. In fact, <code>eval</code> is
            <em>exactly</em> what happens. Zig is not a compiler, it is an
            interpreter. When <code>zig</code> evaluates an expression, it gets:
          </p>
          <ul>
            <li>
              expression’s type (as a Zig value),
            </li>
            <li>
              expression’s value (if it can be evaluated at comptime),
            </li>
            <li>
              code to compute expression’s value otherwise.
            </li>
          </ul>

          <figure>
            <pre><code><span>eval("1 + 2") =</span>
<span>  3</span>
<span></span>
<span>eval("f() + g()") =</span>
<span>  $1 = call 'f'</span>
<span>  $2 = call 'g'</span>
<span>  $3 = add $1, $2</span>
<span></span>
<span>eval("f() + 2") =</span>
<span>  $1 = call 'f'</span>
<span>  $2 = add $1,  imm 2</span></code></pre>
          </figure>
          <p>When interpreting code like</p>

          <figure>
            <pre><code><span>obj.field = if (condition()) 1 else 2;</span></code></pre>
          </figure>
          <p>
            the interpreter passes the result location (<code>obj.field</code>)
            and type down the tree of subexpressions. If branches store result
            directly into object field (there’s a <code>store</code> inside each
            branch, as opposed to one <code>store</code> after the <code>if</code>), and each coerces its comptime constant to the
            appropriate runtime type of the result.
          </p>
          <p>
            This mechanism enables concise <code>.variant</code> syntax for
            specifying enums:
          </p>

          <figure>
            <pre><code><span><span>const</span> E = <span>enum</span> { a, b };</span>
<span></span>
<span><span>fn</span><span> example</span>(e: E) <span>u32</span> {</span>
<span>    <span>return</span> <span>switch</span> (e) {</span>
<span>        .a =&gt; <span>1</span>,</span>
<span>        (<span>if</span> (<span>true</span>) .b <span>else</span> .a) =&gt; <span>2</span>,</span>
<span>    };</span>
<span>}</span></code></pre>
          </figure>
          <p>
            When <code>zig</code> evaluates the switch, it first evaluates the
            scrutinee, and realizes that it has type
            <code>E</code>. When evaluating <code>switch</code> arm, it sets
            result type to <code>E</code> for the condition, and a literal <code>.a</code>
            gets coerced to <code>E</code>. The same happens for the second arm,
            where result type further sinks down the
            <code>if</code>.
          </p>
          <p>
            Result type semantics also explains the leading dot in the record
            literal syntax:
          </p>

          <figure>
            <pre><code><span><span>const</span> p: Point = .{</span>
<span>    .x = <span>1</span>,</span>
<span>    .y = <span>2</span>,</span>
<span>};</span></code></pre>
          </figure>
          <p>
            Syntactically, we just want to disambiguate records from blocks.
            But, semantically, we want to coerce the literal to whatever type we
            want to get out of this expression. In Zig, <code>.whatever</code>
            is a shorthand for <code>@ResultType().whatever</code>.
          </p>
          <p>
            I must confess that <code>.{}</code> did weird me out a lot at first
            during <em>writing</em> code (I don’t mind reading the dot). It’s
            not the easiest thing to type! But that was fixed once I added <code>..</code> snippet, expanding to <code>.{$0}</code>.
          </p>
          <p>
            The benefits to lightweight record literal syntax are huge, as they
            allow for some pretty nice APIs. In particular, you get named and
            default arguments for free:
          </p>

          <figure>
            <pre><code><span><span>fn</span><span> exec</span>(argv: []<span>const</span> <span>u8</span>, options: <span>struct</span> {</span>
<span>    working_directory: ?[]<span>const</span> <span>u8</span> = <span>null</span></span>
<span>}) <span>!</span><span>void</span> {</span>
<span>    <span>// ...</span></span>
<span>}</span>
<span></span>
<span><span>fn</span><span> usage</span>() <span>!</span><span>void</span> {</span>
<span>    <span>try</span> exec(<span>&amp;</span>.{ <span>"git"</span>, <span>"status"</span>}, .{});</span>
<span></span>
<span>    <span>try</span> exec(<span>&amp;</span>.{ <span>"git"</span>, <span>"status"</span>}, .{</span>
<span>        .working_directory = <span>"./src"</span>,</span>
<span>    });</span>
<span>}</span></code></pre>
          </figure>
          <p>
            I don’t really miss the absence of named arguments in Rust, you can
            always design APIs without them. But they are free in Zig, so I use
            them liberally. Syntax wise, we get two features (calling functions
            and initializing objects) for the price of one!
          </p>
        </section>
        <section id="Built-ins">
          <h2>
            <a href="#Built-ins">Built-ins </a>
          </h2>
          <p>
            Finally, the thing that weirds out some people when they see Zig
            code, and makes others reconsider their choice GitHub handles, even
            when they haven’t seen any Zig: <code>@divExact</code> syntax for
            built-in functions.
          </p>
          <p>
            Every language needs to glue “userspace” code with primitive
            operations supported by the compiler. Usually, the gluing is
            achieved by making the standard library privileged and allowing it
            to define intrinsic functions without bodies, or by adding ad-hoc
            operators directly to the language (like Rust’s <code>as</code>).
            And Zig does have a fair amount of operators, like <code>+</code> or
            <code>orelse</code>. But the release valve for a lot of
            functionality are built-in functions in distinct syntactic
            namespace, so Zig separates out <code>@bitCast</code>, <code>@addrSpaceCast</code>, <code>@alignCast</code>, <code>@constCast</code>, <code>@ptrCast</code>, <code>@intCast</code>,
            <code>@floatCast</code>, <code>@volatileCast</code>, <code>@ptrFromInt</code>, and <code>@intFromPtr</code>. There’s no need
            to overload casting when you can give each variant a name.
          </p>
          <p>
            There’s also <span><code>@as(i32, 92)</code></span>
            for type ascription. The types goes first, because the mechanism
            here is result type semantics: <code>@as</code> evaluates the first
            argument as a type, and then uses that as the type for the second
            argument. Curiously, <code>@as</code> I think actually can be
            implemented in the userspace:
          </p>

          <figure>
            <pre><code><span><span>fn</span><span> as</span>(<span>comptime</span> T: <span>type</span>, value: T) T {</span>
<span>    <span>return</span> value;</span>
<span>}</span></code></pre>
          </figure>
          <p>
            In Zig, a type of function parameter may depend on values of
            preceding (comptime) ones!
          </p>
          <p>
            My favorite builtin is <code>@import()</code>. First, it’s the most
            obvious way to import code:
            <span><code>const foo =
                @import("./foo.zig")</code></span>
            Its crystal clear where the file comes from.
          </p>
          <p>
            But, second, it is an instance of reverse syntax sugar. You see,
            import isn’t really a function. You can’t do
          </p>

          <figure>
            <pre><code><span><span>const</span> name = <span>"./foo.zig"</span>;</span>
<span><span>const</span> foo = <span>@import</span>(name);</span></code></pre>
          </figure>
          <p>
            The argument of <code>@import</code> has to be a string,
            syntactically. It really is
            <span><code>import "./path.zig"</code></span>
            syntax, except that the function-call form is re-used, because it
            already has the right shape.
          </p>
          <hr>
          <p>
            So, this is it. Just a bunch of silly syntactical decisions, which
            add up to a language which is positively enjoyable to read. As for
            big lessons, obviously, the less features your language has, the
            less syntax you’ll need. And less syntax is generally good, because
            varied syntactic constructs tend to step on each other toes.
            Languages are not combinations of orthogonal aspects. Features tug
            and pull the language in different directions and their combinations
            might turn to be miraculous features in their own right, or might
            drag the language down.
          </p>
          <p>
            Even with a small feature-set fixed, there’s still a lot of work to
            pick a good concrete syntax: unambiguous to parse, useful to grep,
            easy to read and not to painful to write. A smart thing is of course
            to steal and borrow solutions from other languages, not because of
            familiarity, but because the ruthless natural selection tends to
            weed out poor ideas. But there’s a lot of inertia in languages, so
            there’s no need to fear innovation. If an odd-looking syntax is
            actually good, people will take to it.
          </p>
          <p>
            Is there anything about Zig’s syntax I don’t like? I thought no,
            when starting this post. But in the process of writing it I did
            discover one form that annoys me. It is the while with the increment
            loop:
          </p>

          <figure>
            <pre><code><span><span>var</span> i: <span>u32</span> = <span>0</span>;</span>
<span><span>while</span> (i &lt; <span>10</span>) : (i<span>+=</span><span>1</span>) {</span>
<span>    print(<span>"{d}"</span>, .{i});</span>
<span>}</span></code></pre>
          </figure>
          <p>
            This is two-thirds of a C-style <code>for</code> loop (without the
            declarator), and it sucks for the same reason: control flow jumps
            all other the place and is unrelated to the source code order. We go
            from condition, to the body, to the increment. But in the source
            order the increment is between the condition and the body. In Zig,
            this loop sucks for one additional reason: that <code>:</code>
            separating the increment I think is the single example of control
            flow in Zig that is expressed by a sigil, rather than a keyword.
          </p>
          <p>
            This form used to be rather important, as Zig lacked a counting
            loop. It has
            <span><code>for(0..10) |i|</code></span>
            form now, so I am tempted to call the while-with-increment
            redundant.
          </p>
          <p>Annoyingly,</p>

          <figure>
            <pre><code><span><span>while</span> (condition) {</span>
<span>    <span>defer</span> increment;</span>
<span></span>
<span>    body</span>
<span>}</span></code></pre>
          </figure>
          <p>is <em>almost</em> equivalent to</p>

          <figure>
            <pre><code><span><span>while</span> (condition) : (increment) {</span>
<span>  body</span>
<span>}</span></code></pre>
          </figure>
          <p>
            But not exactly: if <code>body</code> contains a <code>return</code>, <code>break</code> or <code>try</code>, the <code>defer</code> version would run the
            <code>increment</code> one extra time, which is useless and might be
            outright buggy. Oh well.
          </p>
        </section>
      </article>
    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[GPT-OSS vs. Qwen3 and a detailed look how things evolved since GPT-2 (444 pts)]]></title>
            <link>https://magazine.sebastianraschka.com/p/from-gpt-2-to-gpt-oss-analyzing-the</link>
            <guid>44855690</guid>
            <pubDate>Sun, 10 Aug 2025 15:06:07 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://magazine.sebastianraschka.com/p/from-gpt-2-to-gpt-oss-analyzing-the">https://magazine.sebastianraschka.com/p/from-gpt-2-to-gpt-oss-analyzing-the</a>, See on <a href="https://news.ycombinator.com/item?id=44855690">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><article><div dir="auto"><p>OpenAI just released their new open-weight LLMs this week: gpt-oss-120b and gpt-oss-20b, their first open-weight models since GPT-2 in 2019. And yes, thanks to some clever optimizations, they can run locally (but more about this later).</p><p>This is the first time since GPT-2 that OpenAI has shared a large, fully open-weight model. Earlier GPT models showed how the transformer architecture scales. The 2022 ChatGPT release then made these models mainstream by demonstrating concrete usefulness for writing and knowledge (and later coding) tasks. Now they have shared some long-awaited weight model, and the architecture has some interesting details.</p><p>I spent the past few days reading through the code and technical reports to summarize the most interesting details. (Just days after, OpenAI also announced GPT-5, which I will briefly discuss in the context of the gpt-oss models at the end of this article.)</p><p>Below is a quick preview of what the article covers. For easier navigation, I recommend using the Table of Contents on the left of on the article page.</p><ul><li><p>Model architecture comparisons with GPT-2</p></li><li><p>MXFP4 optimization to fit gpt-oss models onto single GPUs</p></li><li><p>Width versus depth trade-offs (gpt-oss vs Qwen3)</p></li><li><p>Attention bias and sinks</p></li><li><p>Benchmarks and comparisons with GPT-5</p></li></ul><p>I hope you find it informative!</p><p>Before we discuss the architecture in more detail, let's start with an overview of the two models, gpt-oss-20b and gpt-oss-120b, shown in Figure 1 below.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/$s_!rlW0!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F83b5bdbd-7b2b-4c52-9783-0d4de94a0e5a_1589x743.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!rlW0!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F83b5bdbd-7b2b-4c52-9783-0d4de94a0e5a_1589x743.png 424w, https://substackcdn.com/image/fetch/$s_!rlW0!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F83b5bdbd-7b2b-4c52-9783-0d4de94a0e5a_1589x743.png 848w, https://substackcdn.com/image/fetch/$s_!rlW0!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F83b5bdbd-7b2b-4c52-9783-0d4de94a0e5a_1589x743.png 1272w, https://substackcdn.com/image/fetch/$s_!rlW0!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F83b5bdbd-7b2b-4c52-9783-0d4de94a0e5a_1589x743.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/$s_!rlW0!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F83b5bdbd-7b2b-4c52-9783-0d4de94a0e5a_1589x743.png" width="1456" height="681" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/83b5bdbd-7b2b-4c52-9783-0d4de94a0e5a_1589x743.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:681,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:243817,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:false,&quot;topImage&quot;:true,&quot;internalRedirect&quot;:&quot;https://magazine.sebastianraschka.com/i/170506328?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F83b5bdbd-7b2b-4c52-9783-0d4de94a0e5a_1589x743.png&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/$s_!rlW0!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F83b5bdbd-7b2b-4c52-9783-0d4de94a0e5a_1589x743.png 424w, https://substackcdn.com/image/fetch/$s_!rlW0!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F83b5bdbd-7b2b-4c52-9783-0d4de94a0e5a_1589x743.png 848w, https://substackcdn.com/image/fetch/$s_!rlW0!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F83b5bdbd-7b2b-4c52-9783-0d4de94a0e5a_1589x743.png 1272w, https://substackcdn.com/image/fetch/$s_!rlW0!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F83b5bdbd-7b2b-4c52-9783-0d4de94a0e5a_1589x743.png 1456w" sizes="100vw" fetchpriority="high"></picture></div></a><figcaption>Figure 1: The two gpt-oss models side by side.</figcaption></figure></div><p><span>If you have looked at recent LLM architecture diagrams before, or read my previous </span><a href="https://magazine.sebastianraschka.com/p/the-big-llm-architecture-comparison" rel="">Big Architecture Comparison</a><span> article, you may notice that there is nothing novel or unusual at first glance. </span></p><div data-component-name="DigestPostEmbed"><a href="https://magazine.sebastianraschka.com/p/the-big-llm-architecture-comparison" rel="noopener" target="_blank"><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!83ox!,w_140,h_140,c_fill,f_webp,q_auto:good,fl_progressive:steep,g_auto/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd72e5a99-1a11-42b7-8831-8f5785ed2bc1_1600x1116.png"><img src="https://substackcdn.com/image/fetch/$s_!83ox!,w_140,h_140,c_fill,f_auto,q_auto:good,fl_progressive:steep,g_auto/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd72e5a99-1a11-42b7-8831-8f5785ed2bc1_1600x1116.png" sizes="100vw" alt="The Big LLM Architecture Comparison" width="140" height="140"></picture></div></a></div><p>This is not surprising, since leading LLM developers tend to use the same base architecture and then apply smaller tweaks. This is pure speculation on my part, but I think this is because</p><ul><li><p>There is significant rotation of employees between these labs.</p></li><li><p><span>We still have not found anything better than the transformer architecture. Even though state space models and text diffusion models exist, as far as I know no one has shown that they perform as well as transformers at this scale. (Most of the comparisons I found focus only on benchmark performance. It is still unclear how well the models handle real-world, multi-turn writing and coding tasks. At the time of writing, the highest-ranking non-purely-transformer-based model on the </span><a href="https://lmarena.ai/leaderboard/text" rel="">LM Arena</a><span> is Jamba, which is a transformer–state space model hybrid, at rank 96.)</span></p></li><li><p>Most of the gains likely come from data and algorithm tweaks rather than from major architecture changes.</p></li></ul><p>That being said, there are still many interesting aspects of their design choices. Some are shown in the figure above (while others are not, but we will discuss them later as well). In the rest of this article, I will highlight these features and compare them to other architectures, one at a time.</p><p>I should also note that I am not affiliated with OpenAI in any way. My information comes from reviewing the released model code and reading their technical reports. If you want to learn how to use these models locally, the best place to start is OpenAI's official model hub pages:</p><ul><li><p><a href="https://huggingface.co/openai/gpt-oss-20b" rel="">https://huggingface.co/openai/gpt-oss-20b</a></p></li><li><p><a href="https://huggingface.co/openai/gpt-oss-120b" rel="">https://huggingface.co/openai/gpt-oss-120b</a></p></li></ul><p>The 20B model can run on a consumer GPU with up to 16 GB of RAM. The 120B model can run on a single H100 with 80 GB of RAM or newer hardware. I will return to this later, as there are some important caveats.</p><p>Before we jump into comparisons between gpt-oss and a more recent architecture, let's hop into the time machine and take a side-by-side look at GPT-2 (Figure 2) to see just how far things have come.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/$s_!AsnD!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff07debb6-0111-4ff4-a0c9-d4ce27120b72_1531x829.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!AsnD!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff07debb6-0111-4ff4-a0c9-d4ce27120b72_1531x829.png 424w, https://substackcdn.com/image/fetch/$s_!AsnD!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff07debb6-0111-4ff4-a0c9-d4ce27120b72_1531x829.png 848w, https://substackcdn.com/image/fetch/$s_!AsnD!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff07debb6-0111-4ff4-a0c9-d4ce27120b72_1531x829.png 1272w, https://substackcdn.com/image/fetch/$s_!AsnD!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff07debb6-0111-4ff4-a0c9-d4ce27120b72_1531x829.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/$s_!AsnD!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff07debb6-0111-4ff4-a0c9-d4ce27120b72_1531x829.png" width="1456" height="788" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/f07debb6-0111-4ff4-a0c9-d4ce27120b72_1531x829.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:788,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:267271,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://magazine.sebastianraschka.com/i/170506328?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff07debb6-0111-4ff4-a0c9-d4ce27120b72_1531x829.png&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/$s_!AsnD!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff07debb6-0111-4ff4-a0c9-d4ce27120b72_1531x829.png 424w, https://substackcdn.com/image/fetch/$s_!AsnD!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff07debb6-0111-4ff4-a0c9-d4ce27120b72_1531x829.png 848w, https://substackcdn.com/image/fetch/$s_!AsnD!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff07debb6-0111-4ff4-a0c9-d4ce27120b72_1531x829.png 1272w, https://substackcdn.com/image/fetch/$s_!AsnD!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff07debb6-0111-4ff4-a0c9-d4ce27120b72_1531x829.png 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption>Figure 2: A side-by-side comparison between gpt-oss-20b and GPT-2 XL 1.5B.</figcaption></figure></div><p><span>Both gpt-oss and GPT-2 are decoder-only LLMs built on the transformer architecture introduced in the </span><a href="https://arxiv.org/abs/1706.03762" rel="">Attention Is All You Need (2017)</a><span> paper. Over the years, many details have evolved.</span></p><p><span>However, these changes are not unique to gpt-oss. And as we will see later, they appear in many other LLMs. Since I discussed many of these aspects in the previous </span><a href="https://magazine.sebastianraschka.com/p/the-big-llm-architecture-comparison" rel="">Big Architecture Comparison</a><span> article, I will try to keep each subsection brief and focused.</span></p><p><a href="https://arxiv.org/abs/1207.0580" rel="">Dropout (2012)</a><span> is a traditional technique to prevent overfitting by randomly "dropping out" (i.e., setting to zero) a fraction of the layer activations or attention scores (Figure 3) during training. However, dropout is rarely used in modern LLMs, and most models after GPT-2 have dropped it (no pun intended).</span></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/$s_!BS-w!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb25371fe-9ee0-4a65-a5aa-46f3bca4b349_845x850.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!BS-w!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb25371fe-9ee0-4a65-a5aa-46f3bca4b349_845x850.png 424w, https://substackcdn.com/image/fetch/$s_!BS-w!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb25371fe-9ee0-4a65-a5aa-46f3bca4b349_845x850.png 848w, https://substackcdn.com/image/fetch/$s_!BS-w!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb25371fe-9ee0-4a65-a5aa-46f3bca4b349_845x850.png 1272w, https://substackcdn.com/image/fetch/$s_!BS-w!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb25371fe-9ee0-4a65-a5aa-46f3bca4b349_845x850.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/$s_!BS-w!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb25371fe-9ee0-4a65-a5aa-46f3bca4b349_845x850.png" width="554" height="557.2781065088758" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/b25371fe-9ee0-4a65-a5aa-46f3bca4b349_845x850.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:850,&quot;width&quot;:845,&quot;resizeWidth&quot;:554,&quot;bytes&quot;:130475,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://magazine.sebastianraschka.com/i/170506328?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb25371fe-9ee0-4a65-a5aa-46f3bca4b349_845x850.png&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/$s_!BS-w!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb25371fe-9ee0-4a65-a5aa-46f3bca4b349_845x850.png 424w, https://substackcdn.com/image/fetch/$s_!BS-w!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb25371fe-9ee0-4a65-a5aa-46f3bca4b349_845x850.png 848w, https://substackcdn.com/image/fetch/$s_!BS-w!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb25371fe-9ee0-4a65-a5aa-46f3bca4b349_845x850.png 1272w, https://substackcdn.com/image/fetch/$s_!BS-w!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb25371fe-9ee0-4a65-a5aa-46f3bca4b349_845x850.png 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption>Figure 3: An illustration of dropout applied to the attention score matrix.</figcaption></figure></div><p>I assume that dropout was originally used in GPT-2 because it was inherited from the original transformer architecture. Researchers likely noticed that it does not really improve LLM performance (I observed the same in my small-scale GPT-2 replication runs). This is likely because LLMs are typically trained for only a single epoch over massive datasets, which is in contrast to the multi-hundred-epoch training regimes for which dropout was first introduced. So, since LLMs see each token only once during training, there is little risk of overfitting.</p><p><span>Interestingly, while Dropout is kind of ignored in LLM architecture design for many years, I found a </span><a href="https://arxiv.org/abs/2505.24788" rel="">2025 research paper</a><span> with small scale LLM experiments (Pythia 1.4B) that confirms that Dropout results in worse downstream performance in these single-epoch regimes.</span></p><p>In transformer-based LLMs, positional encoding is necessary because of the attention mechanism. By default, attention treats the input tokens as if they have no order. In the original GPT architecture, absolute positional embeddings addressed this by adding a learned embedding vector for each position in the sequence (Figure 4), which is then added to the token embeddings.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/$s_!YCov!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F62fdfc32-605c-4065-abc6-689756d53b87_1195x533.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!YCov!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F62fdfc32-605c-4065-abc6-689756d53b87_1195x533.png 424w, https://substackcdn.com/image/fetch/$s_!YCov!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F62fdfc32-605c-4065-abc6-689756d53b87_1195x533.png 848w, https://substackcdn.com/image/fetch/$s_!YCov!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F62fdfc32-605c-4065-abc6-689756d53b87_1195x533.png 1272w, https://substackcdn.com/image/fetch/$s_!YCov!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F62fdfc32-605c-4065-abc6-689756d53b87_1195x533.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/$s_!YCov!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F62fdfc32-605c-4065-abc6-689756d53b87_1195x533.png" width="1195" height="533" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/62fdfc32-605c-4065-abc6-689756d53b87_1195x533.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:533,&quot;width&quot;:1195,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:123823,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://magazine.sebastianraschka.com/i/170506328?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F62fdfc32-605c-4065-abc6-689756d53b87_1195x533.png&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/$s_!YCov!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F62fdfc32-605c-4065-abc6-689756d53b87_1195x533.png 424w, https://substackcdn.com/image/fetch/$s_!YCov!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F62fdfc32-605c-4065-abc6-689756d53b87_1195x533.png 848w, https://substackcdn.com/image/fetch/$s_!YCov!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F62fdfc32-605c-4065-abc6-689756d53b87_1195x533.png 1272w, https://substackcdn.com/image/fetch/$s_!YCov!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F62fdfc32-605c-4065-abc6-689756d53b87_1195x533.png 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption>Figure 4: Illustration of absolute positional embeddings.</figcaption></figure></div><p><span>RoPE (</span><a href="https://arxiv.org/abs/2104.09864" rel="">Rotary Position Embedding</a><span>) introduced a different approach: instead of adding position information as separate embeddings, it encodes position by rotating the query and key vectors in a way that depends on each token's position. (RoPE is an elegant idea but also a bit of a tricky topic to explain. I plan to cover separately in more detail one day.)</span></p><p>While first introduced in 2021, RoPE became widely adopted with the release of the original Llama model in 2023 and has since become a staple in modern LLMs.</p><p>Early GPT architectures used GELU. Why now use Swish over GELU? Swish is considered computationally slightly cheaper, and in my opinion, that all there is to it. Depending on which paper you look at, you will find that one is slightly better than the other in terms of modeling performance. In my opinion, these small differences are probably within a standard error, and your mileage will vary based on hyperparameter sensitivity.</p><p>Activation functions used to be a hot topic of debate until the deep learning community largely settled on ReLU more than a decade ago. Since then, researchers have proposed and tried many ReLU-like variants with smoother curves, and GELU and Swish (Figure 5) are the ones that stuck.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/$s_!WIz6!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8aa7b74a-6520-4124-99fd-6df60b9e0e7e_1407x775.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!WIz6!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8aa7b74a-6520-4124-99fd-6df60b9e0e7e_1407x775.png 424w, https://substackcdn.com/image/fetch/$s_!WIz6!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8aa7b74a-6520-4124-99fd-6df60b9e0e7e_1407x775.png 848w, https://substackcdn.com/image/fetch/$s_!WIz6!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8aa7b74a-6520-4124-99fd-6df60b9e0e7e_1407x775.png 1272w, https://substackcdn.com/image/fetch/$s_!WIz6!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8aa7b74a-6520-4124-99fd-6df60b9e0e7e_1407x775.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/$s_!WIz6!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8aa7b74a-6520-4124-99fd-6df60b9e0e7e_1407x775.png" width="1407" height="775" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/8aa7b74a-6520-4124-99fd-6df60b9e0e7e_1407x775.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:775,&quot;width&quot;:1407,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:237022,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://magazine.sebastianraschka.com/i/170506328?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8aa7b74a-6520-4124-99fd-6df60b9e0e7e_1407x775.png&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/$s_!WIz6!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8aa7b74a-6520-4124-99fd-6df60b9e0e7e_1407x775.png 424w, https://substackcdn.com/image/fetch/$s_!WIz6!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8aa7b74a-6520-4124-99fd-6df60b9e0e7e_1407x775.png 848w, https://substackcdn.com/image/fetch/$s_!WIz6!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8aa7b74a-6520-4124-99fd-6df60b9e0e7e_1407x775.png 1272w, https://substackcdn.com/image/fetch/$s_!WIz6!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8aa7b74a-6520-4124-99fd-6df60b9e0e7e_1407x775.png 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption>Figure 5: Comparison between Swish and GELU activations, which are both smoother versions or ReLU.</figcaption></figure></div><p><span>Early GPT architectures used GELU, which is defined as </span><code>0.5x * [1 + erf(x / sqrt(2))]</code><span>. Here, </span><code>erf</code><span> (short for error function) is the integral of a Gaussian and it is computed using polynomial approximations of the Gaussian integral, which makes it more computationally expensive than simpler functions like the sigmoid used in Swish, where Swish is simply </span><code>x * sigmoid(x)</code><span>.</span></p><p>In practice, Swish is computationally slightly cheaper than GELU, and that's probably the main reason it replaced GELU in most newer models. Depending on which paper we look at, one might be somewhat better in terms of modeling performance. But I'd say these gains are often within standard error, and the winner will depend heavily on hyperparameter tuning.</p><p>Swish is used in most architectures today. However, GELU is not entirely forgotten; for example, Google's Gemma models still use GELU.</p><p><span>What's more notable, though, is that the feed forward module (a small multi-layer perceptron) is replaced by a gated "GLU" counterpart, where GLU stands for gated linear unit and was proposed in a </span><a href="https://arxiv.org/pdf/2002.05202" rel="">2020 paper</a><span>. Concretely, the 2 fully connected layers are replaced by 3 fully connected layers that are used as shown in Figure 6 below.</span></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/$s_!8gzt!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F10a8befb-b407-45a4-b38b-486c3d7a65d6_1005x844.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!8gzt!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F10a8befb-b407-45a4-b38b-486c3d7a65d6_1005x844.png 424w, https://substackcdn.com/image/fetch/$s_!8gzt!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F10a8befb-b407-45a4-b38b-486c3d7a65d6_1005x844.png 848w, https://substackcdn.com/image/fetch/$s_!8gzt!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F10a8befb-b407-45a4-b38b-486c3d7a65d6_1005x844.png 1272w, https://substackcdn.com/image/fetch/$s_!8gzt!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F10a8befb-b407-45a4-b38b-486c3d7a65d6_1005x844.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/$s_!8gzt!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F10a8befb-b407-45a4-b38b-486c3d7a65d6_1005x844.png" width="655" height="550.0696517412936" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/10a8befb-b407-45a4-b38b-486c3d7a65d6_1005x844.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:844,&quot;width&quot;:1005,&quot;resizeWidth&quot;:655,&quot;bytes&quot;:190423,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://magazine.sebastianraschka.com/i/170506328?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F10a8befb-b407-45a4-b38b-486c3d7a65d6_1005x844.png&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/$s_!8gzt!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F10a8befb-b407-45a4-b38b-486c3d7a65d6_1005x844.png 424w, https://substackcdn.com/image/fetch/$s_!8gzt!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F10a8befb-b407-45a4-b38b-486c3d7a65d6_1005x844.png 848w, https://substackcdn.com/image/fetch/$s_!8gzt!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F10a8befb-b407-45a4-b38b-486c3d7a65d6_1005x844.png 1272w, https://substackcdn.com/image/fetch/$s_!8gzt!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F10a8befb-b407-45a4-b38b-486c3d7a65d6_1005x844.png 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption>Figure 6: A comparison between Swish and GELU and their gated counterparts, SwiGLU and GEGLU.</figcaption></figure></div><p><span>At first glance, it may appear that the GEGLU/SwiGLU variants may be better than the regular feed forward layers because there are simply more parameters due to the extra layer. But this is deceiving because in practice, the </span><code>W</code><span> and </span><code>V</code><span> weight layers in SwiGLU/GEGLU are usually chosen to be half the size each of the </span><code>W_1</code><span> layer in a traditional feed forward layer.</span></p><p>To illustrate this better, consider the concrete code implementations of the regular and GLU variants:</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/$s_!_JVz!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc0fc02c2-0a64-4137-b988-c1f7381da13c_1039x776.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!_JVz!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc0fc02c2-0a64-4137-b988-c1f7381da13c_1039x776.png 424w, https://substackcdn.com/image/fetch/$s_!_JVz!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc0fc02c2-0a64-4137-b988-c1f7381da13c_1039x776.png 848w, https://substackcdn.com/image/fetch/$s_!_JVz!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc0fc02c2-0a64-4137-b988-c1f7381da13c_1039x776.png 1272w, https://substackcdn.com/image/fetch/$s_!_JVz!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc0fc02c2-0a64-4137-b988-c1f7381da13c_1039x776.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/$s_!_JVz!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc0fc02c2-0a64-4137-b988-c1f7381da13c_1039x776.png" width="687" height="513.1010587102984" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/c0fc02c2-0a64-4137-b988-c1f7381da13c_1039x776.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:776,&quot;width&quot;:1039,&quot;resizeWidth&quot;:687,&quot;bytes&quot;:267148,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://magazine.sebastianraschka.com/i/170506328?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc0fc02c2-0a64-4137-b988-c1f7381da13c_1039x776.png&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/$s_!_JVz!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc0fc02c2-0a64-4137-b988-c1f7381da13c_1039x776.png 424w, https://substackcdn.com/image/fetch/$s_!_JVz!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc0fc02c2-0a64-4137-b988-c1f7381da13c_1039x776.png 848w, https://substackcdn.com/image/fetch/$s_!_JVz!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc0fc02c2-0a64-4137-b988-c1f7381da13c_1039x776.png 1272w, https://substackcdn.com/image/fetch/$s_!_JVz!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc0fc02c2-0a64-4137-b988-c1f7381da13c_1039x776.png 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption>Figure 7: Regular feed forward module (top) and SwiGLU variant (bottom) next to each other.</figcaption></figure></div><p>So, suppose we have an embedding dimension of 1024. In the regular feed forward case, this would then be</p><ul><li><p>fc1: 1024 × 4096 = 4,194,304</p></li><li><p>fc2: 1024 × 4096 = 4,194,304</p></li></ul><p>That is fc1 + fc2 = 8,388,608 parameters.</p><p>For the GLU variant, we have</p><ul><li><p>fc1: 1024 × 2048 = 2,097,152</p></li><li><p>fc2: 1024 × 2048 = 2,097,152</p></li><li><p>fc3: 2048 × 1024 = 2,097,152</p></li></ul><p>I.e., 3 × 2,097,152 = 6,291,456 weight parameters.</p><p>So, overall, using the GLU variants results in fewer parameters, and they perform better as well. The reason for this better performance is that these GLU variants provide an additional multiplicative interaction, which improves expressivity (the same reason deep &amp; slim neural nets perform better than shallow &amp; wide neural nets, provided they are trained well).</p><p>In addition to upgrading the feed forward module to a SwiGLU, as discussed in the previous section, gpt-oss replaces the single feed forward module with multiple feed forward modules, using only a subset for each token generation step. This approach is known as a Mixture-of-Experts (MoE) and illustrated in Figure 8 below.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/$s_!SYqb!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffa3367a4-914a-49e0-8c94-016969397ab3_1307x640.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!SYqb!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffa3367a4-914a-49e0-8c94-016969397ab3_1307x640.png 424w, https://substackcdn.com/image/fetch/$s_!SYqb!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffa3367a4-914a-49e0-8c94-016969397ab3_1307x640.png 848w, https://substackcdn.com/image/fetch/$s_!SYqb!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffa3367a4-914a-49e0-8c94-016969397ab3_1307x640.png 1272w, https://substackcdn.com/image/fetch/$s_!SYqb!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffa3367a4-914a-49e0-8c94-016969397ab3_1307x640.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/$s_!SYqb!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffa3367a4-914a-49e0-8c94-016969397ab3_1307x640.png" width="1307" height="640" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/fa3367a4-914a-49e0-8c94-016969397ab3_1307x640.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:640,&quot;width&quot;:1307,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:120915,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://magazine.sebastianraschka.com/i/170506328?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffa3367a4-914a-49e0-8c94-016969397ab3_1307x640.png&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/$s_!SYqb!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffa3367a4-914a-49e0-8c94-016969397ab3_1307x640.png 424w, https://substackcdn.com/image/fetch/$s_!SYqb!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffa3367a4-914a-49e0-8c94-016969397ab3_1307x640.png 848w, https://substackcdn.com/image/fetch/$s_!SYqb!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffa3367a4-914a-49e0-8c94-016969397ab3_1307x640.png 1272w, https://substackcdn.com/image/fetch/$s_!SYqb!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffa3367a4-914a-49e0-8c94-016969397ab3_1307x640.png 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption>Figure 8: The feed forward module is replaced by a Mixture-of-Expert (MoE) module.</figcaption></figure></div><p><span>So, replacing </span><em>a single</em><span> feed forward module with </span><em>multiple</em><span> feed forward modules (as done in a MoE setup) substantially increases the model's total parameter count. However, the key trick is that we don't use ("activate") all experts for every token. Instead, a router selects only a small subset of experts per token.</span></p><p><span>Because only a few experts are active at a time, MoE modules are often referred to as </span><em>sparse</em><span>, in contrast to </span><em>dense</em><span> modules that always use the full parameter set. However, the large total number of parameters via an MoE increases the capacity of the LLM, which means it can take up more knowledge during training. The sparsity keeps inference efficient, though, as we don't use all the parameters at the same time.</span></p><p>(Fun fact: In most MoE models, expert weights account for more than 90% of the total model parameters.)</p><p>As mentioned in my previous articles, Grouped Query Attention (GQA) has emerged in recent years as a more compute- and parameter-efficient alternative to Multi-Head Attention (MHA).</p><p>In MHA, each head has its own set of keys and values. GQA reduces memory usage by grouping multiple heads to share the same key and value projections.</p><p>For example, as shown in Figure 9, if there are 2 key–value groups and 4 attention heads, heads 1 and 2 might share one set of keys and values, while heads 3 and 4 share another. This grouping decreases the total number of key and value computations, leading to lower memory usage and improved efficiency — without noticeably affecting modeling performance, according to ablation studies.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/$s_!Kohq!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa2347cc2-3685-4547-b31b-be7bbfb21201_1237x588.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!Kohq!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa2347cc2-3685-4547-b31b-be7bbfb21201_1237x588.png 424w, https://substackcdn.com/image/fetch/$s_!Kohq!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa2347cc2-3685-4547-b31b-be7bbfb21201_1237x588.png 848w, https://substackcdn.com/image/fetch/$s_!Kohq!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa2347cc2-3685-4547-b31b-be7bbfb21201_1237x588.png 1272w, https://substackcdn.com/image/fetch/$s_!Kohq!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa2347cc2-3685-4547-b31b-be7bbfb21201_1237x588.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/$s_!Kohq!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa2347cc2-3685-4547-b31b-be7bbfb21201_1237x588.png" width="637" height="302.7938561034762" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/a2347cc2-3685-4547-b31b-be7bbfb21201_1237x588.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:588,&quot;width&quot;:1237,&quot;resizeWidth&quot;:637,&quot;bytes&quot;:83420,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://magazine.sebastianraschka.com/i/170506328?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa2347cc2-3685-4547-b31b-be7bbfb21201_1237x588.png&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/$s_!Kohq!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa2347cc2-3685-4547-b31b-be7bbfb21201_1237x588.png 424w, https://substackcdn.com/image/fetch/$s_!Kohq!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa2347cc2-3685-4547-b31b-be7bbfb21201_1237x588.png 848w, https://substackcdn.com/image/fetch/$s_!Kohq!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa2347cc2-3685-4547-b31b-be7bbfb21201_1237x588.png 1272w, https://substackcdn.com/image/fetch/$s_!Kohq!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa2347cc2-3685-4547-b31b-be7bbfb21201_1237x588.png 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption>Figure 9: A comparison between MHA and GQA. Here, the group size is 2, where a key and value pair is shared among 2 queries.</figcaption></figure></div><p>So, the core idea behind GQA is to reduce the number of key and value heads by sharing them across multiple query heads. This (1) lowers the model's parameter count and (2) reduces the memory bandwidth usage for key and value tensors during inference since fewer keys and values need to be stored and retrieved from the KV cache.</p><p><span>(If you are curious how GQA looks in code, see my</span><a href="https://github.com/rasbt/LLMs-from-scratch/blob/main/ch05/07_gpt_to_llama/converting-llama2-to-llama3.ipynb" rel=""> GPT-2 to Llama 3 conversion guide</a><span> for a version without KV cache and my KV-cache variant </span><a href="https://github.com/rasbt/LLMs-from-scratch/blob/main/pkg/llms_from_scratch/llama3.py" rel="">here</a><span>.)</span></p><p><span>While GQA is mainly a computational-efficiency workaround for MHA, ablation studies (such as those in the</span><a href="https://arxiv.org/abs/2305.13245" rel=""> original GQA paper</a><span> and the </span><a href="https://arxiv.org/abs/2307.09288" rel="">Llama 2 paper</a><span>) show it performs comparably to standard MHA in terms of LLM modeling performance.</span></p><p><span>Sliding-window attention (Figure 10 below) was first introduced in the </span><a href="https://arxiv.org/abs/2004.05150" rel="">LongFormer paper (2020)</a><span> and later popularized by Mistral. Interestingly, gpt-oss applies it in every second layer. You can think of it as a variation of multi-head attention, or in this case grouped query attention (GQA), where the attention context is restricted to a smaller window, reducing both memory usage and compute costs.</span></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/$s_!wwFe!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe79d48c3-2bdf-41bb-9e18-45f128cd5c01_1600x792.jpeg" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!wwFe!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe79d48c3-2bdf-41bb-9e18-45f128cd5c01_1600x792.jpeg 424w, https://substackcdn.com/image/fetch/$s_!wwFe!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe79d48c3-2bdf-41bb-9e18-45f128cd5c01_1600x792.jpeg 848w, https://substackcdn.com/image/fetch/$s_!wwFe!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe79d48c3-2bdf-41bb-9e18-45f128cd5c01_1600x792.jpeg 1272w, https://substackcdn.com/image/fetch/$s_!wwFe!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe79d48c3-2bdf-41bb-9e18-45f128cd5c01_1600x792.jpeg 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/$s_!wwFe!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe79d48c3-2bdf-41bb-9e18-45f128cd5c01_1600x792.jpeg" width="1456" height="721" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/e79d48c3-2bdf-41bb-9e18-45f128cd5c01_1600x792.jpeg&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:721,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:225815,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/jpeg&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://magazine.sebastianraschka.com/i/170506328?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe79d48c3-2bdf-41bb-9e18-45f128cd5c01_1600x792.jpeg&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/$s_!wwFe!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe79d48c3-2bdf-41bb-9e18-45f128cd5c01_1600x792.jpeg 424w, https://substackcdn.com/image/fetch/$s_!wwFe!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe79d48c3-2bdf-41bb-9e18-45f128cd5c01_1600x792.jpeg 848w, https://substackcdn.com/image/fetch/$s_!wwFe!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe79d48c3-2bdf-41bb-9e18-45f128cd5c01_1600x792.jpeg 1272w, https://substackcdn.com/image/fetch/$s_!wwFe!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe79d48c3-2bdf-41bb-9e18-45f128cd5c01_1600x792.jpeg 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption>Figure 10: Comparison between regular attention (left) and sliding window attention (right).</figcaption></figure></div><p>Concretely, gpt-oss alternates between GQA layers that attend to the full context and GQA layers with a sliding window limited to 128 tokens.</p><p><span>As I discussed in my </span><a href="https://magazine.sebastianraschka.com/p/the-big-llm-architecture-comparison" rel="">previous article</a><span>, </span><a href="https://arxiv.org/abs/2408.00118" rel="">Gemma 2 (2024)</a><span> used a similar 1:1 ratio. </span><a href="https://arxiv.org/abs/2503.19786" rel="">Gemma 3</a><span> earlier this year went much further and shifted to a 5:1 ratio, which means only one full-attention layer for every five sliding-window (local) attention layers.</span></p><p>According to the Gemma ablation studies, sliding-window attention has minimal impact on modeling performance, as shown in the figure below. Note that the window size in Gemma 2 was 4096 tokens, which Gemma 3 reduced to 1024. In gpt-oss, the window is just 128 tokens, which is remarkably small.</p><p><span>And as a fun fact, the </span><a href="https://openai.com/index/introducing-gpt-oss/" rel="">official announcement article</a><span> notes that sliding-window attention was apparently already used in GPT-3:</span></p><blockquote><p>The models use alternating dense and locally banded sparse attention patterns, similar to GPT-3</p></blockquote><p><span>Who knew!? I went back to the original </span><a href="https://arxiv.org/abs/2005.14165" rel="">GPT-3 paper</a><span>, and it was indeed mentioned there:</span></p><blockquote><p>We use the same model and architecture as GPT-2 [ RWC+19 ], including the modified initialization, pre-normalization, and reversible tokenization described therein, with the exception that we use alternating dense and locally banded sparse attention patterns in the layers of the transformer, similar to the Sparse Transformer [ CGRS19 ]. </p></blockquote><p><span>Finally, the last small tweak, coming from GPT-2, is replacing </span><a href="https://arxiv.org/abs/1607.06450" rel="">LayerNorm (2016)</a><span> by </span><a href="https://arxiv.org/abs/1910.07467" rel="">RMSNorm (2019)</a><span>, which has been a common trend in recent years.</span></p><p>Akin to swapping GELU with Swish and SwiGLU, RMSNorm is one of these smaller but sensible efficiency improvements. RMSNorm is similar to LayerNorm in its purpose to normalize layer activations, as shown in Figure 11 below.</p><p>You might recall that not too long ago, BatchNorm was the go-to choice for this task. It has since fallen out of favor, largely because it is harder to parallelize efficiently (due to the mean and variance batch statistics) and performs poorly with small batch sizes.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/$s_!H32R!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4ac713f9-4f15-4104-9b67-eff1c4f29f95_1367x599.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!H32R!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4ac713f9-4f15-4104-9b67-eff1c4f29f95_1367x599.png 424w, https://substackcdn.com/image/fetch/$s_!H32R!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4ac713f9-4f15-4104-9b67-eff1c4f29f95_1367x599.png 848w, https://substackcdn.com/image/fetch/$s_!H32R!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4ac713f9-4f15-4104-9b67-eff1c4f29f95_1367x599.png 1272w, https://substackcdn.com/image/fetch/$s_!H32R!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4ac713f9-4f15-4104-9b67-eff1c4f29f95_1367x599.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/$s_!H32R!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4ac713f9-4f15-4104-9b67-eff1c4f29f95_1367x599.png" width="1367" height="599" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/4ac713f9-4f15-4104-9b67-eff1c4f29f95_1367x599.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:599,&quot;width&quot;:1367,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:274255,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://magazine.sebastianraschka.com/i/170506328?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4ac713f9-4f15-4104-9b67-eff1c4f29f95_1367x599.png&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/$s_!H32R!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4ac713f9-4f15-4104-9b67-eff1c4f29f95_1367x599.png 424w, https://substackcdn.com/image/fetch/$s_!H32R!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4ac713f9-4f15-4104-9b67-eff1c4f29f95_1367x599.png 848w, https://substackcdn.com/image/fetch/$s_!H32R!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4ac713f9-4f15-4104-9b67-eff1c4f29f95_1367x599.png 1272w, https://substackcdn.com/image/fetch/$s_!H32R!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4ac713f9-4f15-4104-9b67-eff1c4f29f95_1367x599.png 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption>Figure 11: A comparison between LayerNorm (left) and RMSNorm (right) for a small linear layer.</figcaption></figure></div><p>As we can see in Figure 11 above, both LayerNorm and RMSNorm scale the layer outputs to be in a reasonable range.</p><p>LayerNorm subtracts the mean and divides by the standard deviation such that the layer outputs have a zero mean and unit variance (variance of 1 and standard deviation of one).</p><p>RMSNorm divides the inputs by the root-mean-square. This doesn't force zero mean and unit variance, but the mean and variance are in a reasonable range: -1 to 1 for the mean and 0 to 1 for the variance. In this particular example shown in Figure 11, the mean is 0.77 and the variance is 0.41.</p><p>Both LayerNorm and RMSNorm stabilize activation scales and improve optimization, but RMSNorm is often preferred in large-scale LLMs because it is cheaper to compute. Unlike LayerNorm, RMSNorm has no bias (shift) term and reduces the expensive mean and variance computations to a single root-mean-square operation. This reduces the number of cross-feature reductions from two to one, which lowers communication overhead on GPUs and improving training efficiency.</p><p>Figure 12 shows what this looks like in code:</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/$s_!m5aM!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fde968991-5068-40d9-87bb-98b887f5f384_919x690.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!m5aM!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fde968991-5068-40d9-87bb-98b887f5f384_919x690.png 424w, https://substackcdn.com/image/fetch/$s_!m5aM!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fde968991-5068-40d9-87bb-98b887f5f384_919x690.png 848w, https://substackcdn.com/image/fetch/$s_!m5aM!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fde968991-5068-40d9-87bb-98b887f5f384_919x690.png 1272w, https://substackcdn.com/image/fetch/$s_!m5aM!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fde968991-5068-40d9-87bb-98b887f5f384_919x690.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/$s_!m5aM!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fde968991-5068-40d9-87bb-98b887f5f384_919x690.png" width="589" height="442.23068552774754" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/de968991-5068-40d9-87bb-98b887f5f384_919x690.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:690,&quot;width&quot;:919,&quot;resizeWidth&quot;:589,&quot;bytes&quot;:259430,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://magazine.sebastianraschka.com/i/170506328?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fde968991-5068-40d9-87bb-98b887f5f384_919x690.png&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/$s_!m5aM!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fde968991-5068-40d9-87bb-98b887f5f384_919x690.png 424w, https://substackcdn.com/image/fetch/$s_!m5aM!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fde968991-5068-40d9-87bb-98b887f5f384_919x690.png 848w, https://substackcdn.com/image/fetch/$s_!m5aM!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fde968991-5068-40d9-87bb-98b887f5f384_919x690.png 1272w, https://substackcdn.com/image/fetch/$s_!m5aM!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fde968991-5068-40d9-87bb-98b887f5f384_919x690.png 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption>Figure 12: Code implementations of LayerNorm and RMSNorm showing that RMSNorm is computationally simpler.</figcaption></figure></div><p>I still think that GPT-2 is an excellent beginner architecture when learning about LLMs. It's simple enough to understand without getting lost in layers of optimization tricks, but still complex enough to give you a solid grasp of how modern transformer models work.</p><p>By starting with GPT-2, you can focus on the fundamentals (attention mechanisms, positional embeddings, normalization, and the overall training pipeline) without being overwhelmed by the extra features and tweaks found in newer architectures.</p><p>In fact, I think it's worth the time to learn about and even implement GPT-2 first before trying to stack newer changes on top. You will not only have an easier time understanding those changes, but you will likely also appreciate them more, because you will get a better understanding of what limitations or problems they try to solve.</p><p><span>For instance, starting with my GPT-2 code I recently implemented the </span><a href="https://github.com/rasbt/LLMs-from-scratch/tree/main/ch05/11_qwen3" rel="">Qwen3 architecture from scratch</a><span>, which is super similar to gpt-oss, which brings us to the next topic: Comparing gpt-oss to a more recent architecture.</span></p><p>Now that we have walked through the evolution from GPT-2 to GPT OSS, we can take the next step and compare GPT OSS to a more recent architecture, Qwen3, which was released three months earlier in May 2025.</p><p>The reason I am selecting Qwen3 here is that it is among the top open-weight models as of the time of writing. Additionally, one of the Qwen3 MoE models is more or less directly comparable to GPT OSS due to its relatively similar overall size in terms of trainable parameters.</p><p>Figure 13 below compares gpt-oss-20b to a Qwen3 model of comparable size.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/$s_!5K75!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F115d3e6c-4c29-459c-a7e1-195bda61963a_1603x816.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!5K75!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F115d3e6c-4c29-459c-a7e1-195bda61963a_1603x816.png 424w, https://substackcdn.com/image/fetch/$s_!5K75!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F115d3e6c-4c29-459c-a7e1-195bda61963a_1603x816.png 848w, https://substackcdn.com/image/fetch/$s_!5K75!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F115d3e6c-4c29-459c-a7e1-195bda61963a_1603x816.png 1272w, https://substackcdn.com/image/fetch/$s_!5K75!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F115d3e6c-4c29-459c-a7e1-195bda61963a_1603x816.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/$s_!5K75!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F115d3e6c-4c29-459c-a7e1-195bda61963a_1603x816.png" width="1456" height="741" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/115d3e6c-4c29-459c-a7e1-195bda61963a_1603x816.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:741,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:268927,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://magazine.sebastianraschka.com/i/170506328?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F115d3e6c-4c29-459c-a7e1-195bda61963a_1603x816.png&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/$s_!5K75!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F115d3e6c-4c29-459c-a7e1-195bda61963a_1603x816.png 424w, https://substackcdn.com/image/fetch/$s_!5K75!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F115d3e6c-4c29-459c-a7e1-195bda61963a_1603x816.png 848w, https://substackcdn.com/image/fetch/$s_!5K75!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F115d3e6c-4c29-459c-a7e1-195bda61963a_1603x816.png 1272w, https://substackcdn.com/image/fetch/$s_!5K75!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F115d3e6c-4c29-459c-a7e1-195bda61963a_1603x816.png 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption>Figure 13: A gpt-oss and Qwen3 model of comparable size side by side.</figcaption></figure></div><p>As we can see, gpt-oss 20B and Qwen3 30B-A3B are very similar in their architecture components. The primary difference here, aside from the dimensions, is that gpt-oss employs sliding window attention, as discussed earlier in section 1.6 (not shown in this figure), whereas Qwen3 does not.</p><p>Let's walk through the noteworthy details one by one in the following subsections.</p><p>If we look at the two models closely, we see that Qwen3 is a much deeper architecture with its 48 transformer blocks instead of 24 (Figure 14).</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/$s_!G1hj!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8681af3c-a62a-4224-94f3-89dcbe8284a0_1704x815.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!G1hj!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8681af3c-a62a-4224-94f3-89dcbe8284a0_1704x815.png 424w, https://substackcdn.com/image/fetch/$s_!G1hj!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8681af3c-a62a-4224-94f3-89dcbe8284a0_1704x815.png 848w, https://substackcdn.com/image/fetch/$s_!G1hj!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8681af3c-a62a-4224-94f3-89dcbe8284a0_1704x815.png 1272w, https://substackcdn.com/image/fetch/$s_!G1hj!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8681af3c-a62a-4224-94f3-89dcbe8284a0_1704x815.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/$s_!G1hj!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8681af3c-a62a-4224-94f3-89dcbe8284a0_1704x815.png" width="1456" height="696" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/8681af3c-a62a-4224-94f3-89dcbe8284a0_1704x815.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:696,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:307435,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://magazine.sebastianraschka.com/i/170506328?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8681af3c-a62a-4224-94f3-89dcbe8284a0_1704x815.png&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/$s_!G1hj!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8681af3c-a62a-4224-94f3-89dcbe8284a0_1704x815.png 424w, https://substackcdn.com/image/fetch/$s_!G1hj!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8681af3c-a62a-4224-94f3-89dcbe8284a0_1704x815.png 848w, https://substackcdn.com/image/fetch/$s_!G1hj!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8681af3c-a62a-4224-94f3-89dcbe8284a0_1704x815.png 1272w, https://substackcdn.com/image/fetch/$s_!G1hj!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8681af3c-a62a-4224-94f3-89dcbe8284a0_1704x815.png 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption>Figure 14: Qwen3 has twice as many transformer blocks as gpt-oss-20b.</figcaption></figure></div><p>On the other hand, gpt-oss is a much wider architecture:</p><ul><li><p>An embedding dimension of 2880 instead of 2048</p></li><li><p>An intermediate expert (feed forward) projection dimension of 5760 instead of 768</p></li></ul><p>It's also worth noting that gpt-oss uses twice as many attention heads, but this doesn't directly increase the model's width. The width is determined by the embedding dimension.</p><p>Does one approach offer advantages over the other given a fixed number of parameters? As a rule of thumb, deeper models have more flexibility but can be harder to train due to instability issues, due to exploding and vanishing gradients (which RMSNorm and shortcut connections aim to mitigate).</p><p>Wider architectures have the advantage of being faster during inference (with a higher tokens/second throughput) due to better parallelization at a higher memory cost.</p><p><span>When it comes to modeling performance, there's unfortunately no good apples-to-apples comparison I am aware of (where parameter size and datasets are kept constant) except for an ablation study in the </span><a href="https://arxiv.org/abs/2408.00118" rel="">Gemma 2 paper (Table 9)</a><span>, which found that for a 9B parameter architecture, a wider setup is slightly better than a deeper setup. Across 4 benchmarks, the wider model achieved a 52.0 average score, and the deeper model achieved a 50.8 average score.</span></p><p>As shown in Figure 14 above, it's also noteworthy that gpt-oss has a surprisingly small number of experts (32 instead of 128), and only uses 4 instead of 8 active experts per token. However, each expert is much larger than the experts in Qwen3.</p><p>This is interesting because the recent trends and developments point towards more, smaller models as being beneficial. This change, at a constant total parameter size, is nicely illustrated in Figure 15 below from the DeepSeekMoE paper.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/$s_!qYc3!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5941e19e-ead5-47be-9c41-8afee9124c6d_1131x609.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!qYc3!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5941e19e-ead5-47be-9c41-8afee9124c6d_1131x609.png 424w, https://substackcdn.com/image/fetch/$s_!qYc3!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5941e19e-ead5-47be-9c41-8afee9124c6d_1131x609.png 848w, https://substackcdn.com/image/fetch/$s_!qYc3!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5941e19e-ead5-47be-9c41-8afee9124c6d_1131x609.png 1272w, https://substackcdn.com/image/fetch/$s_!qYc3!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5941e19e-ead5-47be-9c41-8afee9124c6d_1131x609.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/$s_!qYc3!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5941e19e-ead5-47be-9c41-8afee9124c6d_1131x609.png" width="1131" height="609" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/5941e19e-ead5-47be-9c41-8afee9124c6d_1131x609.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:609,&quot;width&quot;:1131,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:219481,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://magazine.sebastianraschka.com/i/170506328?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5941e19e-ead5-47be-9c41-8afee9124c6d_1131x609.png&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/$s_!qYc3!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5941e19e-ead5-47be-9c41-8afee9124c6d_1131x609.png 424w, https://substackcdn.com/image/fetch/$s_!qYc3!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5941e19e-ead5-47be-9c41-8afee9124c6d_1131x609.png 848w, https://substackcdn.com/image/fetch/$s_!qYc3!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5941e19e-ead5-47be-9c41-8afee9124c6d_1131x609.png 1272w, https://substackcdn.com/image/fetch/$s_!qYc3!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5941e19e-ead5-47be-9c41-8afee9124c6d_1131x609.png 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption><span>Figure 15: An annotated figure from "DeepSeekMoE: Towards Ultimate Expert Specialization in Mixture-of-Experts Language Models", </span><a href="https://arxiv.org/abs/2401.06066" rel="">https://arxiv.org/abs/2401.06066</a></figcaption></figure></div><p>Notably, unlike DeepSeek's models, neither gpt-oss nor Qwen3 uses shared experts, though.</p><p>To be fair, the small number of experts in gpt-oss could be a side effect of the 20B size. Looking at the 120B mode below, they indeed increased the number of experts (and transformer blocks) while keeping everything else fixed, as shown in Figure 16 below.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/$s_!w8-R!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F901e0e7c-c007-4d32-a489-183c633ec44b_1679x837.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!w8-R!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F901e0e7c-c007-4d32-a489-183c633ec44b_1679x837.png 424w, https://substackcdn.com/image/fetch/$s_!w8-R!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F901e0e7c-c007-4d32-a489-183c633ec44b_1679x837.png 848w, https://substackcdn.com/image/fetch/$s_!w8-R!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F901e0e7c-c007-4d32-a489-183c633ec44b_1679x837.png 1272w, https://substackcdn.com/image/fetch/$s_!w8-R!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F901e0e7c-c007-4d32-a489-183c633ec44b_1679x837.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/$s_!w8-R!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F901e0e7c-c007-4d32-a489-183c633ec44b_1679x837.png" width="1456" height="726" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/901e0e7c-c007-4d32-a489-183c633ec44b_1679x837.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:726,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:291088,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://magazine.sebastianraschka.com/i/170506328?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F901e0e7c-c007-4d32-a489-183c633ec44b_1679x837.png&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/$s_!w8-R!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F901e0e7c-c007-4d32-a489-183c633ec44b_1679x837.png 424w, https://substackcdn.com/image/fetch/$s_!w8-R!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F901e0e7c-c007-4d32-a489-183c633ec44b_1679x837.png 848w, https://substackcdn.com/image/fetch/$s_!w8-R!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F901e0e7c-c007-4d32-a489-183c633ec44b_1679x837.png 1272w, https://substackcdn.com/image/fetch/$s_!w8-R!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F901e0e7c-c007-4d32-a489-183c633ec44b_1679x837.png 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption>Figure 16: The two gpt-oss architectures side by side, where the larger 120B model only scales the number of transformer blocks and number of experts.</figcaption></figure></div><p>The boring explanation for the fact that the 20B and 120B models are so similar is probably that the 120B model was the main focus. And the easiest way to create a smaller model was to make it a bit shorter (fewer transformer blocks) and to reduce the number of experts, because that's where most of the parameters are. However, one might speculate whether they started training the 120B model, and then chopped some of the transformer blocks and experts for continued pre-training (instead of starting from random weights).</p><p>In any case, it's because it's quite unusual to only scale those two (transformer blocks and number of experts). For instance, when looking at Qwen3 MoE models of multiple sizes (Figure 17 below), they were scaled more proportionally to each other over many more aspects..</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/$s_!0h6T!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9faf6dc8-5876-47c2-8965-212195773ed9_1120x903.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!0h6T!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9faf6dc8-5876-47c2-8965-212195773ed9_1120x903.png 424w, https://substackcdn.com/image/fetch/$s_!0h6T!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9faf6dc8-5876-47c2-8965-212195773ed9_1120x903.png 848w, https://substackcdn.com/image/fetch/$s_!0h6T!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9faf6dc8-5876-47c2-8965-212195773ed9_1120x903.png 1272w, https://substackcdn.com/image/fetch/$s_!0h6T!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9faf6dc8-5876-47c2-8965-212195773ed9_1120x903.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/$s_!0h6T!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9faf6dc8-5876-47c2-8965-212195773ed9_1120x903.png" width="1120" height="903" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/9faf6dc8-5876-47c2-8965-212195773ed9_1120x903.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:903,&quot;width&quot;:1120,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:210100,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://magazine.sebastianraschka.com/i/170506328?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9faf6dc8-5876-47c2-8965-212195773ed9_1120x903.png&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/$s_!0h6T!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9faf6dc8-5876-47c2-8965-212195773ed9_1120x903.png 424w, https://substackcdn.com/image/fetch/$s_!0h6T!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9faf6dc8-5876-47c2-8965-212195773ed9_1120x903.png 848w, https://substackcdn.com/image/fetch/$s_!0h6T!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9faf6dc8-5876-47c2-8965-212195773ed9_1120x903.png 1272w, https://substackcdn.com/image/fetch/$s_!0h6T!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9faf6dc8-5876-47c2-8965-212195773ed9_1120x903.png 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption>Figure 17: Architecture differences in the various Qwen3 models.</figcaption></figure></div><p>Both gpt-oss and Qwen3 use grouped query attention. The main difference is that gpt-oss restricts the context size via sliding window attention in each second layer, as mentioned earlier.</p><p>However, there's one interesting detail that caught my eye. It seems that gpt-oss uses bias units for the attention weights, as shown in the figure below.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/$s_!U3bl!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1f1c450c-a6ad-4bc7-9e0e-6596c75fadda_1606x486.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!U3bl!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1f1c450c-a6ad-4bc7-9e0e-6596c75fadda_1606x486.png 424w, https://substackcdn.com/image/fetch/$s_!U3bl!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1f1c450c-a6ad-4bc7-9e0e-6596c75fadda_1606x486.png 848w, https://substackcdn.com/image/fetch/$s_!U3bl!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1f1c450c-a6ad-4bc7-9e0e-6596c75fadda_1606x486.png 1272w, https://substackcdn.com/image/fetch/$s_!U3bl!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1f1c450c-a6ad-4bc7-9e0e-6596c75fadda_1606x486.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/$s_!U3bl!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1f1c450c-a6ad-4bc7-9e0e-6596c75fadda_1606x486.png" width="1456" height="441" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/1f1c450c-a6ad-4bc7-9e0e-6596c75fadda_1606x486.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:441,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:176606,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://magazine.sebastianraschka.com/i/170506328?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1f1c450c-a6ad-4bc7-9e0e-6596c75fadda_1606x486.png&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/$s_!U3bl!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1f1c450c-a6ad-4bc7-9e0e-6596c75fadda_1606x486.png 424w, https://substackcdn.com/image/fetch/$s_!U3bl!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1f1c450c-a6ad-4bc7-9e0e-6596c75fadda_1606x486.png 848w, https://substackcdn.com/image/fetch/$s_!U3bl!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1f1c450c-a6ad-4bc7-9e0e-6596c75fadda_1606x486.png 1272w, https://substackcdn.com/image/fetch/$s_!U3bl!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1f1c450c-a6ad-4bc7-9e0e-6596c75fadda_1606x486.png 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption><span>Figure 18: gpt-oss models use bias units in the attention layers. See code example </span><a href="https://github.com/huggingface/transformers/blob/369c99d0cea403b77bd0aef818527106453fd9fc/src/transformers/models/gpt_oss/modular_gpt_oss.py#L228-L243" rel="">here</a><span>.</span></figcaption></figure></div><p><span>I haven't seen these bias units being used since the GPT-2 days, and they are commonly regarded as redundant. Indeed, I found a </span><a href="https://arxiv.org/abs/2302.08626" rel="">recent paper</a><span> that shows mathematically that this is at least true for the key transformation (k_proj). Furthermore, the empirical results show that there is little difference between with and without bias units (see Figure 19 below).</span></p><p><span>Another detail you may have noticed is the definition of </span><code>sinks</code><span> in the code screenshot in Figure 18. In general models, attention sinks are special "always-attended" tokens placed at the start of the sequence to stabilize attention, which is especially useful in long-context scenarios. I.e., if the context gets very long, this special attended token at the beginning is still attended to, and it can learn to store some generally useful information about the entire sequence. (I think it was originally proposed in the </span><a href="https://arxiv.org/abs/2309.17453" rel="">Efficient Streaming Language Models with Attention Sinks</a><span> paper.)</span></p><p><span>In the gpt-oss implementation, </span><em>attention sinks</em><span> are not actual tokens in the input sequence. Instead, they are learned per-head bias logits that are appended to the attention scores (Figure 20). The goal is the same as with the above-mentioned attention sinks, but without modifying the tokenized inputs.</span></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/$s_!Qwo6!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F542cfc4c-ccfb-48b9-b285-ecbe8d2c0e4e_988x684.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!Qwo6!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F542cfc4c-ccfb-48b9-b285-ecbe8d2c0e4e_988x684.png 424w, https://substackcdn.com/image/fetch/$s_!Qwo6!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F542cfc4c-ccfb-48b9-b285-ecbe8d2c0e4e_988x684.png 848w, https://substackcdn.com/image/fetch/$s_!Qwo6!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F542cfc4c-ccfb-48b9-b285-ecbe8d2c0e4e_988x684.png 1272w, https://substackcdn.com/image/fetch/$s_!Qwo6!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F542cfc4c-ccfb-48b9-b285-ecbe8d2c0e4e_988x684.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/$s_!Qwo6!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F542cfc4c-ccfb-48b9-b285-ecbe8d2c0e4e_988x684.png" width="988" height="684" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/542cfc4c-ccfb-48b9-b285-ecbe8d2c0e4e_988x684.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:684,&quot;width&quot;:988,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:202184,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://magazine.sebastianraschka.com/i/170506328?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F542cfc4c-ccfb-48b9-b285-ecbe8d2c0e4e_988x684.png&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/$s_!Qwo6!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F542cfc4c-ccfb-48b9-b285-ecbe8d2c0e4e_988x684.png 424w, https://substackcdn.com/image/fetch/$s_!Qwo6!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F542cfc4c-ccfb-48b9-b285-ecbe8d2c0e4e_988x684.png 848w, https://substackcdn.com/image/fetch/$s_!Qwo6!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F542cfc4c-ccfb-48b9-b285-ecbe8d2c0e4e_988x684.png 1272w, https://substackcdn.com/image/fetch/$s_!Qwo6!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F542cfc4c-ccfb-48b9-b285-ecbe8d2c0e4e_988x684.png 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption><span>Figure 20: The use of attention sinks in gpt-oss; based on the Hugging Face code </span><a href="https://github.com/huggingface/transformers/blame/369c99d0cea403b77bd0aef818527106453fd9fc/src/transformers/models/gpt_oss/modular_gpt_oss.py" rel="">here</a><span>.</span></figcaption></figure></div><p>Lastly, and similar to Qwen3, the gpt-oss models are Apache 2.0 open-source license, which is great (it's the same license that I prefer for my own open-source projects). This means that the models can be distilled into other models or used in commercial products without restriction.</p><p><strong>Open-weight vs. open-source LLMs.</strong><span> This distinction has been debated for years, but it is worth clarifying to avoid confusion about this release and its artifacts. Some model developers release only the model weights and inference code (for example, Llama, Gemma, gpt-oss), while others (for example, OLMo) release everything including training code, datasets, and weights as true open source.</span></p><p><span>By that stricter definition, gpt-oss is an </span><em>open-weight</em><span> model (just like Qwen3) because it includes the weights and inference code but not the training code or datasets. However, the terminology is used inconsistently across the industry.</span></p><p><span>I assume the "oss" in "gpt-oss" stands for </span><em>open source software</em><span>; however, I am positively surprised that OpenAI itself clearly describes gpt-oss as an open-weight model in their official </span><a href="https://openai.com/index/introducing-gpt-oss/" rel="">announcement article</a><span>.</span></p><p>While the previous sections described how the architecture has evolved since GPT-2 and discussed its similarities to Qwen3 (and most other recent models), there are still a few additional but noteworthy details I have not mentioned, yet. These are points that did not fit neatly into the earlier sections but are still worth mentioning.</p><p><span>Unfortunately, there is not much information about the training set sizes and algorithms available. I added the most interesting puzzle pieces from the </span><a href="https://cdn.openai.com/pdf/419b6906-9da6-406c-a19d-1bb078ac7637/oai_gpt-oss_model_card.pdf" rel="">model card report</a><span> (1) and </span><a href="https://openai.com/index/introducing-gpt-oss/" rel="">announcement post</a><span> (2) below:</span></p><blockquote><p>The gpt-oss models were trained using our most advanced pre-training and post-training techniques [...] (1)</p><p>[...] required 2.1million H100-hours to complete, with gpt-oss-20b needing almost 10x fewer. (1)</p><p>[...] including a supervised fine-tuning stage and a high-compute RL stage [...] (2)</p><p>We trained the models on a mostly English, text-only dataset, with a focus on STEM, coding, and general knowledge. (2)</p></blockquote><p><span>So, we know that the gpt-oss models are reasoning models. The training compute of 2.1 million H100 GPU hours is roughly on par with the 2.788 million H800 GPU hours that the ~5.6x larger </span><a href="https://arxiv.org/abs/2412.19437" rel="">DeepSeek V3</a><span> model was trained for. Unfortunately, there is no information about the Qwen3 training time available yet.</span></p><p>Interestingly, the GPT-oss training hour estimate includes both the supervised learning for instruction following and the reinforcement learning for reasoning, whereas DeepSeek V3 is just a pre-trained base model on top of which DeepSeek R1 was trained separately.</p><p>As mentioned in the previous section, the gpt-oss models are reasoning models. However, what's particularly interesting is that they were trained so that users can easily control the degree of reasoning via inference time scaling.</p><p>Concretely, gpt-oss models can receive "Reasoning effort: low/medium/high" instructions as part of their system prompt, which directly affects the response length and accuracy, as shown in Figure 21.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/$s_!LsLL!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1a51339c-49a4-48c6-b60f-ce841b604692_1219x548.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!LsLL!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1a51339c-49a4-48c6-b60f-ce841b604692_1219x548.png 424w, https://substackcdn.com/image/fetch/$s_!LsLL!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1a51339c-49a4-48c6-b60f-ce841b604692_1219x548.png 848w, https://substackcdn.com/image/fetch/$s_!LsLL!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1a51339c-49a4-48c6-b60f-ce841b604692_1219x548.png 1272w, https://substackcdn.com/image/fetch/$s_!LsLL!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1a51339c-49a4-48c6-b60f-ce841b604692_1219x548.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/$s_!LsLL!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1a51339c-49a4-48c6-b60f-ce841b604692_1219x548.png" width="1219" height="548" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/1a51339c-49a4-48c6-b60f-ce841b604692_1219x548.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:548,&quot;width&quot;:1219,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:175317,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://magazine.sebastianraschka.com/i/170506328?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1a51339c-49a4-48c6-b60f-ce841b604692_1219x548.png&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/$s_!LsLL!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1a51339c-49a4-48c6-b60f-ce841b604692_1219x548.png 424w, https://substackcdn.com/image/fetch/$s_!LsLL!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1a51339c-49a4-48c6-b60f-ce841b604692_1219x548.png 848w, https://substackcdn.com/image/fetch/$s_!LsLL!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1a51339c-49a4-48c6-b60f-ce841b604692_1219x548.png 1272w, https://substackcdn.com/image/fetch/$s_!LsLL!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1a51339c-49a4-48c6-b60f-ce841b604692_1219x548.png 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption><span>Figure 21: Response length and quality of gpt-oss models under different reasoning efforts (annotated figure from the </span><a href="https://cdn.openai.com/pdf/419b6906-9da6-406c-a19d-1bb078ac7637/oai_gpt-oss_model_card.pdf" rel="">model card</a><span>)</span></figcaption></figure></div><p>This level of adjustability is useful because it lets us balance cost, compute, and accuracy. For example, if the task is simple, such as answering a straightforward knowledge question or fixing a small typo, we can skip extended reasoning. This saves time and resources while avoiding unnecessarily long responses and verbose reasoning traces.</p><p>It is somewhat unfortunate that OpenAI did not release the base models prior to reinforcement learning-based reasoning training, unlike Qwen3 or OLMo. Base models are particularly valuable starting points for researchers working on reasoning methods (which is one reason I currently like working with Qwen3 Base). My guess is that OpenAI's decision was driven more by industry and production use cases than by research considerations.</p><p><span>Note that the original Qwen3 models also have a toggle for enabling/disabling thinking (reasoning) modes (via a </span><code>enable_thinking=True/False</code><span> setting in the tokenizer that simply adds &lt;think&gt;&lt;/think&gt; tags to disable the reasoning behavior). However, the Qwen3 team updated their models in the last few weeks and moved away from the hybrid model towards dedicated Instruct/Thinking/Coder variants.</span></p><p>The reason was that the hybrid mode resulted in lower performance compared to the individual models:</p><blockquote><p><span>After discussing with the community and reflecting on the matter, we have decided to abandon the hybrid thinking mode. We will now train the Instruct and Thinking models separately to achieve the best possible quality. </span><a href="https://www.actuia.com/en/news/alibaba-launches-qwen3-235b-a22b-instruct-2507-and-breaks-away-from-hybrid-reasoning/?utm_source=chatgpt.com" rel="">Source</a></p></blockquote><p>One interesting surprise is that OpenAI released the gpt-oss models with an MXFP4 quantization scheme for the MoE experts.</p><p>Quantization formats used to be a niche topic, mostly relevant to mobile or embedded AI, but that's changed with the push toward bigger models. In this case, the MXFP4 optimization allows the model to run on single GPU devices.</p><p>Here’s what that looks like in practice:</p><ul><li><p>The large model (think 120B) fits on a single 80GB H100 or newer GPU. Not consumer hardware, but hey, it's much cheaper to rent a 1-H100 machine than a multi-H100 machine. Plus, we don't have to worry about distributing the model across GPUs and adding communication overhead. It's really nice that AMD MI300X cards are supported from day 1 as well!</p></li><li><p>The smaller 20B model even fits into 16 GB of VRAM; the caveat is that it has to be a RTX 50-series GPU or newer to support MXFP4.</p></li></ul><p>Note that the models will also run on older hardware but without MXFP4 support and will thus consume more RAM. Without MXFP4 optimization, the models in bfloat16 will consume more like 48 GB (gpt-oss-20b) and 240 GB (gpt-oss-120b).</p><p>By the way, I can run the gpt-oss-20b model comfortably on my Mac Mini using ollama. It uses about 13.5 Gb or memory, which is really reasonable.</p><p><span>The models are still a bit too new for independent benchmarks. Checking the </span><a href="https://lmarena.ai/leaderboard" rel="">LM Arena leaderboard</a><span>, I found that gpt-oss is not listed, yet. So, Qwen3-Instruct remains the top open-weight model, according to users on the LM Arena, for now (Figure 22).</span></p><p>Looking at a reasoning benchmarks provide in the gpt-oss announcement post, we can see that the gpt-ossmodels are on par with OpenAI's proprietary models as well as Qwen3 (Figure 23).</p><p>However, this should be caveated by the fact that gpt-oss-120b is almost half the size of the Qwen3 A235B-A22B-Thinking-2507 model and can run on a single GPU.</p><p>Benchmark performance, however, does not always reflect real-world usability. In my limited use over the past few days, I have found gpt-oss to be quite capable. That said, as others have observed, it does seem to have a relatively high tendency to hallucinate (a point also mentioned in its model card).</p><p>This may stem from its heavy training focus on reasoning tasks such as math, puzzles, and code, which could have led to some "general knowledge forgetting." Still, because gpt-oss was designed with tool use in mind, this limitation may become less relevant over time. Tool integration in open-source LLMs is still in its early stages, but as it matures, I expect that we increasingly let models consult external sources (like search engines) when answering factual or knowledge-based queries.</p><p>If that happens, it could be sensible to prioritize reasoning capacity over memorization. This is much like in human learning in school (or in life in general), where problem-solving skills often matter more than memorizing facts.</p><p>OpenAI had a busy week and released the long-awaited GPT-5 model shortly after gpt-oss. The GPT-5 release was interesting. And if there's one thing I have to say here, it's that I am really surprised by how good their open-source models really are compared to their best product offering in terms of benchmark performance (Figure 24).</p><p>All in all, even though some people called the release overhyped, I am glad that we have a new set of really strong open weight models that are not too far behind the best proprietary ones. Of course, benchmarks often do not accurately reflect real-world use, and it is still too early to tell based on the limited usage. But I think these are good times for people who like to work with open-weight and local (or privately hosted) models.</p><p><em>This magazine is a personal passion project, and your support helps keep it alive. If you would like to contribute, there are a few great ways:</em></p><ul><li><p><em><strong><a href="https://amzn.to/4fqvn0D" rel="">Grab a copy of my book</a></strong><span>. Build a Large Language Model (From Scratch) walks you through building an LLM step by step, from tokenizer to training.</span></em></p></li></ul><ul><li><p><em><strong><a href="https://www.manning.com/livevideo/master-and-build-large-language-models" rel="">Check out the video course</a></strong><span>. There’s now a 17-hour video course based on the book, available from Manning. It follows the book closely, section by section, and works well both as a standalone or as a code-along resource. The video course is ad-free (unlike the YouTube version) and has a cleaner, more structured format. It also contains 5 additional hours of pre-requisite video material created by Abhinav Kimothi.</span></em></p></li></ul><ul><li><p><em><strong><a href="https://magazine.sebastianraschka.com/subscribe" rel="">Subscribe</a></strong><span>. A paid subscription helps to make my writing sustainable and gives you access to additional contents.</span></em></p></li></ul><p><em>Thanks for reading, and for helping support independent research!</em></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/$s_!wVLk!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffde977ef-c20a-487b-8f1d-5fdbada6585c_1468x885.jpeg" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!wVLk!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffde977ef-c20a-487b-8f1d-5fdbada6585c_1468x885.jpeg 424w, https://substackcdn.com/image/fetch/$s_!wVLk!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffde977ef-c20a-487b-8f1d-5fdbada6585c_1468x885.jpeg 848w, https://substackcdn.com/image/fetch/$s_!wVLk!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffde977ef-c20a-487b-8f1d-5fdbada6585c_1468x885.jpeg 1272w, https://substackcdn.com/image/fetch/$s_!wVLk!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffde977ef-c20a-487b-8f1d-5fdbada6585c_1468x885.jpeg 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/$s_!wVLk!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffde977ef-c20a-487b-8f1d-5fdbada6585c_1468x885.jpeg" width="1456" height="878" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/fde977ef-c20a-487b-8f1d-5fdbada6585c_1468x885.jpeg&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:878,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:&quot;&quot;,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" alt="" title="" srcset="https://substackcdn.com/image/fetch/$s_!wVLk!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffde977ef-c20a-487b-8f1d-5fdbada6585c_1468x885.jpeg 424w, https://substackcdn.com/image/fetch/$s_!wVLk!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffde977ef-c20a-487b-8f1d-5fdbada6585c_1468x885.jpeg 848w, https://substackcdn.com/image/fetch/$s_!wVLk!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffde977ef-c20a-487b-8f1d-5fdbada6585c_1468x885.jpeg 1272w, https://substackcdn.com/image/fetch/$s_!wVLk!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffde977ef-c20a-487b-8f1d-5fdbada6585c_1468x885.jpeg 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div></div></article></div><div id="discussion"><h4>Discussion about this post</h4></div></div>]]></description>
        </item>
    </channel>
</rss>