<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0">
    <channel>
        <title>HN100 - Readable Contents</title>
        <link>https://hn.algolia.com/api/v1/search_by_date?tags=%28story,poll%29&amp;numericFilters=points%3E100</link>
        <description>Uses Readability to add bodies to the RSS feed</description>
        <lastBuildDate>Thu, 12 Sep 2024 15:30:03 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>en</language>
        <item>
            <title><![CDATA[Officer who ignored NYPD's 'courtesy cards' receives $175K settlement (136 pts)]]></title>
            <link>https://apnews.com/article/nypd-courtesy-card-police-misconduct-d5dfdbdad12b01a2cda864f69aa3d1aa</link>
            <guid>41519736</guid>
            <pubDate>Thu, 12 Sep 2024 11:47:21 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://apnews.com/article/nypd-courtesy-card-police-misconduct-d5dfdbdad12b01a2cda864f69aa3d1aa">https://apnews.com/article/nypd-courtesy-card-police-misconduct-d5dfdbdad12b01a2cda864f69aa3d1aa</a>, See on <a href="https://news.ycombinator.com/item?id=41519736">Hacker News</a></p>
Couldn't get https://apnews.com/article/nypd-courtesy-card-police-misconduct-d5dfdbdad12b01a2cda864f69aa3d1aa: Error: timeout of 10000ms exceeded]]></description>
        </item>
        <item>
            <title><![CDATA[SpaceX Astronauts Begin Spacewalk, Putting New Spacesuits to Test (153 pts)]]></title>
            <link>https://www.wsj.com/science/space-astronomy/spacex-launch-polaris-dawn-space-walk-bfed7f84</link>
            <guid>41519623</guid>
            <pubDate>Thu, 12 Sep 2024 11:30:17 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.wsj.com/science/space-astronomy/spacex-launch-polaris-dawn-space-walk-bfed7f84">https://www.wsj.com/science/space-astronomy/spacex-launch-polaris-dawn-space-walk-bfed7f84</a>, See on <a href="https://news.ycombinator.com/item?id=41519623">Hacker News</a></p>
Couldn't get https://www.wsj.com/science/space-astronomy/spacex-launch-polaris-dawn-space-walk-bfed7f84: Error: timeout of 10000ms exceeded]]></description>
        </item>
        <item>
            <title><![CDATA[Kolmogorov-Arnold networks may make neural networks more understandable (139 pts)]]></title>
            <link>https://www.quantamagazine.org/novel-architecture-makes-neural-networks-more-understandable-20240911/</link>
            <guid>41519240</guid>
            <pubDate>Thu, 12 Sep 2024 10:14:05 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.quantamagazine.org/novel-architecture-makes-neural-networks-more-understandable-20240911/">https://www.quantamagazine.org/novel-architecture-makes-neural-networks-more-understandable-20240911/</a>, See on <a href="https://news.ycombinator.com/item?id=41519240">Hacker News</a></p>
Couldn't get https://www.quantamagazine.org/novel-architecture-makes-neural-networks-more-understandable-20240911/: Error: timeout of 10000ms exceeded]]></description>
        </item>
        <item>
            <title><![CDATA[Why Haskell? (178 pts)]]></title>
            <link>https://www.gtf.io/musings/why-haskell</link>
            <guid>41518600</guid>
            <pubDate>Thu, 12 Sep 2024 08:06:27 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.gtf.io/musings/why-haskell">https://www.gtf.io/musings/why-haskell</a>, See on <a href="https://news.ycombinator.com/item?id=41518600">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p data-pos="3:1-10:0">“Impractical”, “academic”, “niche”. These are a few of the reactions I get when someone discovers that my favourite programming language is Haskell, and not only my favourite in some sort of intellectually-masturbatory way, but favourite for building things, real things, mostly involving web servers. Hobby projects would be one thing, but it gets worse: I have actual teams at <a href="https://www.converge.io/">Converge</a> working in Haskell, too.</p>
<p data-pos="11:1-20:0">I find this reaction quite curious: not only can any problem suitable to one general-purpose programming language be tackled in another, but a lot of the new features we see making their way into programming languages like Python, Rust, and Typescript, are either inspired by, or at least more robustly implemented in, Haskell. It seems to me that part of this response is a version of “choose boring technology” (although Haskell is far older than most of the most popular programming languages) twisted to suit another pernicious ideology: that programming is not maths, and that anything that smells of maths should be excised.</p>
<p data-pos="21:1-26:0">This comes up in all sorts of unlikely places in which it would be quite awkward to have to take my interlocutors through all the reasons I think Haskell is probably the best choice for whatever computational problems they are trying to solve themselves (e.g. dinner parties, the pub, etc.) and thus I find myself writing this apologia.</p>
<p data-pos="27:1-33:0">Indeed the remainder of this essay will consist of my attempt to reason around why I think Haskell is probably the best choice<a id="fnref1" href="#fn1" role="doc-noteref"><sup>1</sup></a> for most programmers<a id="fnref2" href="#fn2" role="doc-noteref"><sup>2</sup></a>, especially if one cares about being able to productively write robust software, and even more so if one wants to have fun while doing it (which is a frequently underrated aspect of writing software).</p>
<p data-pos="34:1-41:0">All mainstream, general purpose programming languages are (basically) Turing-complete, and therefore any programme you can write in one you can, in fact, write in another. There is a computational equivalence between them. The main differences are instead in the expressiveness of the languages, the guardrails they give you, and their performance characteristics (although this is possibly more of a runtime/compiler implementation question).</p>
<p data-pos="42:1-46:0">I think that the things that make Haskell great (meaning both more productive and more fun) can be grouped as follows: the things that stop you making mistakes; the things that make you more productive; and the things that help you reason better about your programmes.</p>
<section data-pos="47:1-101:0" id="Unlearning-and-relearning">
<h2 data-pos="47:1-48:0">Unlearning and relearning</h2>
<p data-pos="49:1-57:0">The first thing to say here is that most programmers in the 2020s have been brought up in some sort of imperative<a id="fnref3" href="#fn3" role="doc-noteref"><sup>3</sup></a> paradigm. As a result, the learning curve for a pure, functional language like Haskell will be steep. There are two aspects to this: one is the Haskell language <em>itself</em> which, if you constrain yourself to a simple subset of it, is actually quite easy to learn; and the second is functional programming, which requires a total shift in how the programmer approaches constructing a programme.</p>
<p data-pos="58:1-61:0">This process of unlearning and relearning is incredibly helpful and will make one a better programmer, regardless as to whether one uses Haskell thenceforth. As Alan Perlis writes:</p>
<blockquote data-pos="62:1-64:30">
<p data-pos="62:3-64:30">A language that doesn’t affect the way you think about programming is not worth knowing. ~ Perlisism #19 <a id="fnref4" href="#fn4" role="doc-noteref"><sup>4</sup></a></p>
</blockquote>
<section data-pos="66:1-101:0" id="A-small-note-on-syntax">
<h3 data-pos="66:1-67:0">A small note on syntax</h3>
<p data-pos="68:1-71:0">In the subsequent sections there will be simple snippets of Haskell. Since the syntax is quite distant from C-like syntax with which many readers will be familiar, here is a small guide:</p>
<ul data-pos="72:1-98:0">
<li>
<code>::</code> denotes a type signature (so <code>myThing :: String</code> says I have a name “myThing” and its value is of type <code>String</code>).
</li>
<li>
function calls do not use parentheses, you simply put the arguments, space-separated, after the function name. There are good reasons for this, but they’re beyond the scope of this explainer (so where in one language you may have <code>doSomething(withThis, withThat)</code> in Haskell you have <code>doSomething withThis withThat</code>).
</li>
<li>
lower-case letters in type-signatures are type-variables, and just represent any type (so <code>head :: [a] -&gt; a</code> just takes a list of any type <code>a</code> and returns a single value of the same type <code>a</code>).
</li>
<li>
you will see two types of “forward” arrows: <code>-&gt;</code> and <code>=&gt;</code>. A single arrow <code>-&gt;</code> is used to describe the type of a function: <code>add1 :: Int -&gt;
Int</code> describes a function which takes an integer and returns an integer. A double arrow <code>=&gt;</code> describes constraints on the type variables used, and always come first: <code>add1 :: Num a =&gt; a -&gt; a</code> describes a function which takes any type <code>a</code> which satisfies <code>Num a</code>, and returns a value of the same type.
</li>
<li>
comments start with <code>--</code>.
</li>
<li>
<code>return</code> does not mean what you think it means, it’s just a regular function.
</li>
<li>
<code>do</code> is syntactic sugar allowing you to write things that “look” imperative.
</li>
<li>
There are various ways of assigning values to local names (“variables”) which differ depending on the context. So you can recognise them they either take the form <code>let x = &lt;something&gt; in
&lt;expression&gt;</code> or <code>x &lt;- &lt;something&gt;</code>.
</li>
</ul>
<p data-pos="98:1-101:0">Otherwise the syntax should be fairly easy to parse, if not for a detailed understanding of every aspect, at least to a sufficient level to get the gist of what I am trying to convey.</p>
</section>
</section>
<section data-pos="102:1-611:0" id="Make-fewer-mistakes">
<h2 data-pos="102:1-103:0">Make fewer mistakes</h2>
<p data-pos="104:1-108:0">In many languages, the way one tries to make sure one’s code is “correct” (or, at least, will do the right thing in most circumstances) is through a large number of test cases, some of which may be automated, and some of which may be manual.</p>
<p data-pos="109:1-112:0">Two aspects of Haskell drastically reduce the test-case-writing burden typical in other languages: one is the type system and the other is pure functional programming.</p>
<p data-pos="113:1-121:0">Haskell’s type system is <em>very</em> strong, which is to say that it makes very specific guarantees about the programme, and enforces those guarantees very strictly. The concomitant expressiveness of the language gives the programmer the tools to more precisely and simply express the <em>meaning</em> of the programme within its domain, as well as the general domain of programming. These two properties of the type system, together, reduce the space of possible mistakes, resulting in a more correct programme with much less effort.</p>
<p data-pos="122:1-127:0">So far, so abstract. Some concrete features of the type system that reduce the “error surface” of your programme are: no nullable types; the ability to represent “failable” computations; pattern matching and completeness checks; and the avoidance of “primitive obsession” for free. Let’s take a look at each of those.</p>
<p data-pos="128:1-146:0">The availability of a <code>null</code> (or <code>nil</code> or <code>none</code>) value which can inhabit any (or the majority) of types in a language is often viewed as a convenience, but in practice it has a huge cost. In a language in which one can use such null values, the programmer can never know if the value they are handling is actually of the expected type or if it is null, and therefore is required to check wherever this value is consumed. Programmers can forget things, and the fact that the null value can inhabit many types means that the type system does not help prevent this, leading to errors of the sort “undefined is not a function” or “NoneType object has no attribute &lt;x&gt;”. These, however, are runtime errors, which both means that the programme has failed in its principal task and also that the errors are harder to find as they occur in the wild. Haskell does not have null values. You can define them in a particular data type (for example the <code>Maybe</code> type, which we will come onto shortly) but you have to explicitly define them and explicitly handle them. As such, the error surface available due to this flaw in language design is eliminated, and the programmer no longer has to think about it.</p>
<p data-pos="147:1-163:0">Null values, however, are often used to represent a “failed” computation. For example, what if you are getting the head of an empty list, how do you represent the result? In languages with null values, such functions will often return <code>null</code> in these circumstances. This is a specific case of the more general question of how to deal with computations which may fail. There are many examples: if you are parsing some user input and that input is malformed, this failure to parse is a valid state of your programme, and therefore you need some way to represent it. Similarly, network requests may time out, solvers may fail to find a solution, users may cancel actions, and so on. There are two common solutions, null values (which we have mentioned) and exception handling. Both of these solutions cause a new problem for the programmer: you have to remember to handle them, in the case of exceptions at the call site rather than where you consume the value as with null, and nothing in the type system is going to prevent you forgetting.</p>
<p data-pos="164:1-175:0">Haskell solves the problem of the representation of computations which may fail very differently: explicitly through the type system. There are types in Haskell to represent a computation which may fail, and because this is done in the type system, these are first-class entities and you can pass around your computation-result-which-may-or-may-not-be-a-failure as you like. When it comes to consuming the result of that computation, the type system forces you to reckon with the fact that there may be no result. This prevents a whole class of runtime errors without the mental burden of keeping track of values which may be present or which functions might throw an exception somewhere.</p>
<p data-pos="176:1-183:0">The two most common of these types are <code>Maybe</code> and <code>Either</code>. <code>Maybe</code> represents a computation which may or may not have a result. For example, if you want to get the first element of a list, but you do not know if the list is empty, then you may want to specify that your <code>head</code> function can return either a result or <code>Nothing</code>. Unlike null values, however, you cannot just pretend that the function must have returned a result, as the following snippet should demonstrate:</p>
<pre data-pos="184:1-210:3"><code>safeHead :: [a] -&gt; Maybe a
-- the implementation isn't important here, but I'm including it
-- because it is simple and, for the curious, might be helpful
safeHead [] = Nothing
safeHead (x : _) = Just x

myFavouriteThings = ["raindrops on roses", "whiskers on kittens"]
emptyList = []

faveThing = safeHead myFavouriteThings 
-- ^ but what is the type of this thing? 
-- It's not a string, it's `Maybe String`
-- and the value is, in fact, `Just "raindrops on roses"`

something = safeHead emptyList
-- ^ and what's the type of this thing?
-- again, it's a `Maybe String`, but in this
-- case the value is `Nothing` because the list
-- has no first element!

-- so how can we use the value we have computed?
printTheFirstThing :: [String] -&gt; IO ()
printTheFirstThing myList = case safeHead myList of
  Just something -&gt; putStrLn something
  Nothing -&gt; putStrLn "You don't have any favourite things? How sad."
</code></pre>
<p data-pos="212:1-216:0">In this example you can see that when consuming the result of a computation that might fail, you have to explicitly handle the failure case. There are many ways of doing this, and the pattern matching ( <code>case x of ...</code>) above is just one to which we will come shortly.</p>
<p data-pos="217:1-221:0"><code>Maybe</code> can also be used when you might want a nullable field of a data structure. This is a specific case of a computation which may fail, but is often thought of as distinct. Here is how this would look in Haskell:</p>
<pre data-pos="222:1-228:3"><code>data Person = Person {
  name :: String,
  dob :: Day,
  favouriteThing :: Maybe String
}
</code></pre>
<p data-pos="230:1-234:0">As before, Haskell’s type system will not let you fail to handle the case that <code>favouriteThing</code> might be an empty value, so you will not end up with a runtime error as you might in a language in which you could forget to do so.</p>
<p data-pos="235:1-242:0"><code>Maybe</code> is useful in these situations in which the failure condition is obvious, but it doesn’t give you much resolution on <em>why</em> the computation failed, it only tells you <em>that</em> it has failed. By contrast, an <code>Either a b</code> can contain two values, <code>Left a</code> or <code>Right b</code>. By convention, <code>Left</code> contains a failure value, whereas <code>Right</code> contains a success value, so the type is often given as <code>Either e a</code> where <code>e</code> is for “error” and <code>a</code> is just the result type.</p>
<p data-pos="243:1-247:0">One way in which this could be used is in parsing or validating some user input, in which you may want to tell the user more than just that what they gave you is invalid, but rather in what way it is invalid. To that end you could have a <code>validate</code> function that looked like this:</p>
<pre data-pos="248:1-250:3"><code>validateAddress :: String -&gt; Either AddressParseError ValidAddress
</code></pre>
<p data-pos="252:1-256:0">This gives you the ability to return more helpful errors to the user, which are an expected path in your programme, but it prevents you from failing to handle the failure case, or from treating the failure case like a success case accidentally.</p>
<p data-pos="257:1-263:0">To be clear, this means that we no longer treat known error states as exceptions by throwing them up the call stack <a id="fnref5" href="#fn5" role="doc-noteref"><sup>5</sup></a>, and instead we treat them as potential values for the type of our expression. In turn, this means that we now can have a total description of all the failure modes from the point of the function call <em>down</em> the stack. Consider these two snippets of code:</p>
<pre data-pos="264:1-270:3"><code>def do_something():
  result = get_a_result()
  if result != "a result":
    raise InvalidResultError(result)
  return 42
</code></pre>
<pre data-pos="272:1-279:3"><code>doSomething :: Either InvalidResultError Int
doSomething = 
  let result = getResult
   in if result /= "a result"
        then Left (InvalidResultError result)
        else Right 42
</code></pre>
<p data-pos="281:1-286:0">In the first snippet, we have no idea what possible exceptions may be raised by <code>do_something</code>, partly because we have no way of knowing what exceptions may be raised by <code>get_a_result</code>. By contrast, in the second snippet, we know <em>all</em> of the possible failure states immediately, because they are captured in the type system.</p>
<p data-pos="287:1-297:0">We can generalise this idea of being forced to handle the failure cases by saying that Haskell makes us write <strong>total</strong> functions rather than <strong>partial</strong> functions. This means that we have to handle the entire input domain rather than only part of the input domain, otherwise the compiler will complain at us and, sometimes, point-blank refuse to give us a programme. The easiest way to see how this works is to look at how pattern matching is done in Haskell, using a basic programme which helps us organise our evenings given a chosen option. Instead of implementing the entire programme, here is an extract to illustrate the use of pattern matching.</p>
<pre data-pos="298:1-316:3"><code>data Option =
  NightIn
  | Restaurant VenueName
  | Theatre VenueName EventName

data OrganiserResult = Success | NeedsSeatChoice [Seat] | Failure Reason

organiseMyEvening :: Option -&gt; IO OrganiserResult
organiseMyEvening NightIn = do
  cancelAllPlans 
  return Success
organiseMyEvening (Restaurant venue) = attemptBooking venue
organiseMyEvening (Theatre venue event) = do
  availableSeats &lt;- checkForSeats venue event
  case availableSeats of
    [] -&gt; return (Failure (Reason "there are no seats available, sorry :("))
    seats -&gt; return (NeedsSeatChoice seats)
</code></pre>
<p data-pos="318:1-325:0">In the above example, if we were to add an additional option for what we may want to do with our evening, like going to the cinema, and <em>forget</em> to update the <code>organiseMyEvening</code> function accordingly, the compiler would complain to us until we fix it. Without this completeness check in the type system, we could end up with a runtime error, but with this type of check, we just do not have to worry about whether we have <em>remembered</em> to update all the places in which a given value is used.</p>
<p data-pos="326:1-337:0">The final major way in which Haskell’s type system makes it easy for us to avoid common errors when programming is related to how easy it is to avoid “primitive obsession”. There is a hint in our evening-organising snippet above: our <code>Restaurant</code> and <code>Theatre</code> constructors take a <code>VenueName</code> and <code>EventName</code>. These could, naturally, be represented as plain old strings, and in many languages they are, but Haskell gives us a very simple, zero-cost way of representing them as something with more semantic value, more meaning, than just a string. It may not be obvious why this is a problem worth solving, however. Let’s imagine we represented these as plain old strings, so we would have something like this:</p>
<pre data-pos="338:1-345:3"><code>data Option =
  NightIn
  | Restaurant String
  | Theatre String String -- venue name and event name respectively

checkForSeats :: String -&gt; String -&gt; IO [Seat]
</code></pre>
<p data-pos="347:1-366:0">This is probably <em>ok</em> the first time you write it, although you will need comments, as above, in order to remind yourself which value is which. This is where we come to our first annoyance (although not yet a problem) – the type system doesn’t help us remember what is what, we have to rely on arbitrary comments or documentation (or perhaps variable names) to remember, which is a lot of overhead. The problem comes, however, when using these values, such as in <code>checkForSeats</code>. We could easily mix up the venue name and event name, and we would always return zero seats (because we probably don’t know a theatre called <em>King Lear</em> in London where they are playing Shakespeare’s masterful <em>The National Theatre</em>). This is erroneous behaviour, but is easily done, and the type system will not help us out. “Primitive obsession” is the use of primitives (strings, numbers, booleans, etc.) to represent data, instead of types with more semantic value. The solution is to encode your domain in your type system, which prevents such errors. This can be very cumbersome in many imperative languages, but in Haskell we can simply wrap a value in a <code>newtype</code> and the type system suddenly stops us falling into the trap of using the wrong value. Therefore our code above becomes:</p>
<pre data-pos="367:1-377:3"><code>newtype VenueName = VenueName String
newtype EventName = EventName String

data Option =
  NightIn
  | Restaurant VenueName
  | Theatre VenueName EventName

checkForSeats :: VenueName -&gt; EventName -&gt; IO [Seat]
</code></pre>
<p data-pos="379:1-385:0">Above it is written that this is a “zero-cost” method, which means that unlike the normal way of creating a data structure to wrap around some values, <code>newtypes</code> have exactly the same representation in memory as the type they wrap (with the result that they can only wrap a single type), they therefore only exist at the level of the type system, but have no impact on your programme otherwise.</p>
<p data-pos="386:1-391:0">Thus far we have discussed four features of the type system which help us as programmers to write correct code with minimal mental overhead: the lack of nullable types, representations of “failable” computations, pattern matching and completeness checks, and the avoidance of “primitive obsession”.</p>
<p data-pos="392:1-398:0">Other languages have some of these features (notably Rust, whose type system was inspired by Haskell’s), but most of these other languages lack the second pillar: pure functional programming. There are two aspects of a pure functional language which help us avoid common errors: immutable data and explicit side-effects (which, together, give us purity and referential transparency).</p>
<p data-pos="399:1-413:0">Almost all data in Haskell are immutable. This means that a whole class of errors like data races, or objects changing between write and read, just do not exist. In single-threaded code this is great because you don’t have to think about mutating state anywhere, you just use things like folds or traversals to achieve your goals, but where this really shines is in concurrent code. For concurrent Haskell you do not have to worry about mutices and locks because your data can’t be mutated <em>anyway</em>. That means that if you want to parallelise a computation, you just fork it into different threads and wait for them all to come back without all of the hairy bugs of multi-threaded computations. Even when you do require some sort of shared, mutable state between your threads, the way this is constructed in Haskell (e.g. in the <a href="https://hackage.haskell.org/package/stm"><code>STM</code> library</a>) still avoids the problems solved by locks and mutices in other languages.</p>
<p data-pos="414:1-424:0">Immutability gets you halfway towards eliminating the sorts of errors found in imperative languages, but <em>purity</em> will get us the rest of the way. Haskell functions are pure, in the sense that they do not permit any side-effects, nor do they rely on anything except for the arguments passed into them. There are ways to encode side-effects, for, at some point, any useful programme needs to at least perform <em>some</em> I/O, and there are ways to include things in functions which are not <em>directly</em> passed as arguments (implicit parameters), but the way Haskell is constructed means that these ways do not violate the purity of the language because we use monads to encode these things.</p>
<p data-pos="425:1-431:0">Monads: at first they throw every novice Haskeller into disarray, and then nearly everyone feels the need to write their own monad tutorial. Exactly what monads are and why they are useful is beyond the scope of what we want to talk about here, but the specific benefit we <em>are</em> looking at is how this allows us to encode side-effects and why that is going to help you avoid mistakes when programming.</p>
<p data-pos="432:1-433:0">Let’s look at some functions for a basic online community:</p>
<pre data-pos="434:1-442:3"><code>data Response = Success | Failure FailureReason

sendGreetings :: User -&gt; IO Response

updateUser :: UserId -&gt; User -&gt; IO Response

findNewestUser :: [User] -&gt; Maybe User
</code></pre>
<p data-pos="444:1-451:0">In many imperative languages, the activity of finding the newest user and sending them some sort of greeting might all be done in one function, or a set of deeply nested functions. There would be nothing to stop you, however, making database calls, sending emails, or doing anything else inside the simple <code>findNewestUser</code> function. This can be a nightmare for tracking down bugs and performance issues, as well as preventing tight-coupling between functions.</p>
<p data-pos="452:1-464:0">The functions above take two forms: <code>findNewestUser</code> returns something by now familiar to us, <code>Maybe User</code> – if there is a newest user, it will return it, otherwise it will return <code>Nothing</code>. The other two functions return something we have not yet seen: <code>IO Response</code>. <code>IO</code>, like <code>Maybe</code> wraps another type (in this case: <code>Response</code>) but instead of representing a “failable” computation as <code>Maybe</code> does, it represents any context in which you are permitted to perform I/O actions (like talking to your database or sending emails, as our cases are above). It is not possible to perform I/O outside of the <code>IO</code> monad – your code will not compile – and, furthermore, I/O “colours” all the functions which call it, because if you are calling something which returns <code>IO</code>, then you have to be returning <code>IO</code> as well.</p>
<p data-pos="465:1-474:0">This might look like a lot of bureaucracy, but it actually does two very helpful things: firstly, it immediately tells the programmer “hey, this function performs side-effects in I/O” which means that they don’t have to read the code in order to understand what it does, just the type signature; secondly, it means that you cannot accidentally perform I/O in a function you thought was pure – this, in itself, eliminates whole classes of bugs in which one may think one understands all the dependencies of a function, but actually something is affecting it which you did not realise, because it can perform side-effects.</p>
<p data-pos="475:1-482:0">This is only partially satisfying, however, as wrapping everything that performs side-effects in <code>IO</code> is a bit imprecise in a similar way in which using primitive types for values with higher-level semantics in the domain is also imprecise, and it can cause similar classes of error: there is nothing to say “in this function you can send emails, but you can’t write to the database.” The type-system has helped you <em>a little bit</em> but stopped short of the guardrail we have come to expect by now.</p>
<p data-pos="483:1-493:0">Thankfully, due to two additional language features: namely ad hoc polymorphisms and typeclasses, we can <em>exactly</em> encode the effects we want a function to be permitted to perform, and make it impossible to perform any others. Let’s modify our example to take advantage of this, noting that <code>class X a where</code> means that we are declaring a <em>class</em> <code>X</code> of types with some associated functions for which we have to write concrete implementations. This is similar to interfaces in some languages, or traits in Rust (which were based on Haskell’s typeclasses). In this example, <code>m</code> is just a type variable representing a “2nd order”<a id="fnref6" href="#fn6" role="doc-noteref"><sup>6</sup></a> type (e.g. <code>IO</code> or <code>Maybe</code>).</p>
<pre data-pos="494:1-515:3"><code>data Response = Success | Failure FailureReason

class CanReadUsers m where
  getUsers :: m (Either FailureReason [User])

class CanWriteUsers m where
  updateUser :: UserId -&gt; User -&gt; m Response

class CanSendEmails m where
  sendEmail :: EmailAddress -&gt; Subject -&gt; Body -&gt; m Response

findNewestUser :: [User] -&gt; Maybe User

sendGreetings :: CanSendEmails m =&gt; User -&gt; m Response

greetNewestUser :: (
  CanReadUsers m,
  CanWriteUsers m,
  CanSendEmails m
  ) =&gt; m Response
</code></pre>
<p data-pos="517:1-527:0">We have introduced a new function here <code>greetNewestUser</code> to illustrate how we can compose these <em>constraints</em> on what we are able to do. Our implementation of this would do something like: find all the users, filter for the newest one, send an email, and mark the user as having been greeted. We have encoded these capabilities at the type level for <code>greetNewestUser</code>, whereas we have not for <code>sendGreetings</code>, so it would be impossible, in fact, for <code>sendGreetings</code> to fetch users from the database or to accidentally update the user information in the database<a id="fnref7" href="#fn7" role="doc-noteref"><sup>7</sup></a>. It can <em>only</em> send emails. To finish this example off, let’s see how the implementations of these functions might look:</p>
<pre data-pos="528:1-555:3"><code>-- these would be defined elsewhere, but just so you know the types
joinDate :: User -&gt; Day
emailAddress :: User -&gt; EmailAddress
setAlreadyGreeted :: User -&gt; User
hasBeenGreeted :: User -&gt; Bool
userId :: User -&gt; UserId

findNewestUser users = safeHead (sortOn joinDate users)

sendGreetings user = 
  let subject = Subject "Welcome to the club!"
      body = Body "Remember: don't stare at the guests..."
   in sendEmail (emailAddress user) subject body

greetNewestUser = do
  fetchResult &lt;- getUsers
  case fetchResult of
    Left err -&gt; return (Failure err)
    Right users -&gt; case findNewestUser users of
      Nothing -&gt; return (Failure NoUsers)
      Just user -&gt; if hasBeenGreeted user
        then return (Failure AlreadyGreetedUser)
        else do
          sendGreetings user
          let newUserData = setAlreadyGreeted user
           in updateUser (userId user) newUserData
</code></pre>
<p data-pos="557:1-566:0">While the exact syntax may be unfamiliar, everything in this section has been building up to this point: we represent “failable” computations with data types which encapsulate that they can fail and how they can fail; we use semantically meaningful types to describe our data, rather than primitives; we explicitly handle failure cases rather than being allowed to forget about them; we cannot mutate state so we create new copies of our data with the requisite updates; and we explicitly encode the side-effects we want to perform, rather than just firing them off willy-nilly.</p>
<p data-pos="567:1-574:0">That rounds off the section about the guardrails Haskell puts in place for you as a programmer, both through the strength of its type system and the purity and referential transparency of the language itself. Far from being an imposition on the programmer, this is incredibly freeing as it allows you to spend your mental energy <em>describing</em> your problem and thereby solving it, not worrying about keeping track of all the ways in which your programme could fail.</p>
<section data-pos="575:1-611:0" id="But-language-has-feature-too">
<h3 data-pos="575:1-576:0">But &lt;language&gt; has &lt;feature&gt;, too!</h3>
<p data-pos="577:1-581:0">Some of the features of Haskell above exist, or <em>look like</em> they exist, in other languages. Without trying to talk about every possible language, we can look at some of the common patterns and how they differ, or do not, from those in Haskell.</p>
<p data-pos="582:1-590:0">Pattern matching, for example, has been introduced into many languages. Some of those have the same characteristics as Haskell, like Rust’s pattern matching, which is exhaustive and enforced by the compiler, whereas some are quite different, especially in gradually-typed languages like Typescript and Python, where there is no guarantee that this sort of safety permeates the codebase, and there are often escape-hatches, because you are using optional tools external to the built-in toolchain.</p>
<p data-pos="591:1-595:0">Very few languages make use of higher-order types like <code>Either</code> and <code>Maybe</code> to represent computations which may fail, but Rust is a notable exception which, like Haskell, strongly encourages representing failure in this way.</p>
<p data-pos="596:1-606:0">Subclassing is commonly used in some languages to make it “easy” to avoid primitive obsession, but this is not as strict as Haskell’s <code>newtype</code>s. Python, for example, has a <code>NewType</code> construction, but it has two weaknesses common to this type of implementation: the first is that subclassing means that our <code>VenueName</code> and <code>EventName</code> types can be passed to functions expecting <code>String</code>, because they are not treated as completely different types, and the second is that, unlike in Haskell, you cannot hide the constructors of these types, which means there are certain patterns you cannot fully implement like the parsing pattern (as opposed to validating)<a id="fnref8" href="#fn8" role="doc-noteref"><sup>8</sup></a>.</p>
<p data-pos="607:1-611:0">Finally, while some libraries exist in other languages in order to isolate and control side-effects<a id="fnref9" href="#fn9" role="doc-noteref"><sup>9</sup></a>, they are not enforced as part of the language in the same way, because this would require purity to be built into the language itself.</p>
</section>
</section>
<section data-pos="612:1-738:0" id="The-things-which-make-you-more-productive">
<h2 data-pos="612:1-613:0">The things which make you more productive</h2>
<p data-pos="614:1-620:0">Providing guardrails, for all the reasons listed in the previous section, is a very useful feature of a language, but that alone might make for a very slow experience of building programmes. Haskell has several properties which actually make it <em>more</em> productive to construct such programmes, especially as those programmes grow in complexity (or sheer size).</p>
<p data-pos="621:1-627:0">As before, these properties derive from the two key characteristics of the language<a id="fnref10" href="#fn10" role="doc-noteref"><sup>10</sup></a>: the strength of the type-system and the pure-functional semantics of the language. These two together give us code which is highly declarative, and therefore easily and unambiguously manipulable, as well as a tendency towards heavy concept and code re-use.</p>
<p data-pos="628:1-635:0">Why are these useful? Starting with the former: if our programme is declarative rather than imperative, we can easily understand it ourselves, as well as simply generate other code from it (or documentation), and refactoring becomes a “fearless” activity. Taking the latter, this means that we can “discover” a set of core concepts and continue to build upon them, instead of having to learn disjoint sets of concepts for each domain or library one uses.</p>
<p data-pos="636:1-650:0">It can be hard to explain just how radically these things transform the way one constructs programmes without experiencing them, but to take a small example, the Haskell ecosystem has a tool called <a href="https://hoogle.haskell.org/">“Hoogle”</a> which allows one to search for functions by type signature. Not only by full type signature with concrete types, but, even by partial type signature with type variables instead of actual types. That means that, instead of searching for something which applies a function to each string in a list of strings (<code>(String -&gt; String)
-&gt; [String] -&gt; [String]</code>), one can instead search for something which applies a function to a list of things, returning a list of the results: (<code>(a -&gt; b) -&gt; [a] -&gt; [b]</code>). You can even get the arguments the wrong way around, and Hoogle will still find you the right functions, so <code>[a] -&gt;
(a -&gt; b) -&gt; [b]</code> will give you the same answers (sorted differently) to <code>(a -&gt; b) -&gt; [a] -&gt; [b]</code>!</p>
<p data-pos="651:1-659:0">This works so well because Haskell’s semantics, standard library, and ecosystem all rely heavily on concept re-use. Almost every library builds upon the core set of concepts<a id="fnref11" href="#fn11" role="doc-noteref"><sup>11</sup></a>. This means that if you are wondering how to do something, and you are faced with one library or set of data types, you can probably search for the general pattern of what you want to achieve and you will get what you want. Almost no other ecosystem<a id="fnref12" href="#fn12" role="doc-noteref"><sup>12</sup></a> has something comparable to this.</p>
<p data-pos="660:1-663:0">In order to flesh out this idea of concept generalisation and re-use, let’s consider two examples: functors and monoids. Before we get there, we will start with lists.</p>
<p data-pos="664:1-672:0">A list is Haskell looks like this <code>myList = [1, 2, 3] :: [Int]</code>. You can do various things with lists, like apply a function to each member of the list (<code>map</code>) in order to obtain a new list, or stitch two lists together (<code>[1, 2] &lt;&gt; [3, 4]</code>). In this sense, we have described two properties of lists which we can generalise: a list is a container over which you can apply a function (a “functor”), and a list is an object which has a binary combining operator with an identity value <code>[]</code> (a “monoid”).</p>
<p data-pos="673:1-678:0">Lots of other structures exhibit these properties, for example a list is a functor, but so is a <code>Maybe</code> or an <code>Either</code>, or even a parser! As a result, if you understand the core concept of functors, you have a set of tools which can apply to all sorts of other data structures which you use day-to-day, but with no extra overhead:</p>
<pre data-pos="679:1-688:3"><code>fmap (+ 2) [1, 2, 3] -- [3, 4, 5]
fmap (+ 2) (Just 2) -- Just 4
fmap (+ 2) (Right 5) -- Right 7
number = fmap (+ 2) decimal :: Parser Int
-- parses a string representation of a decimal, adding 2 to it, but
-- the nice thing here is that we don't have to explicitly handle the
-- failure case with our `+ 2` function!
parseMaybe number "4" -- Just 6
</code></pre>
<p data-pos="690:1-696:0">Similarly, there are plenty of monoids lurking about. Obvious examples might be strings, but then, for example, the <a href="https://hackage.haskell.org/package/lucid"><code>Lucid</code> library</a> for writing HTML represents HTML as monoids, which allows you to compose them with the same tools you would use for any other monoid. Once again, you learn a single core concept, and it becomes applicable across a large part of the ecosystem.</p>
<pre data-pos="697:1-701:3"><code>[1, 2] &lt;&gt; [3, 4] -- [1, 2, 3, 4]
"hello" &lt;&gt; " " &lt;&gt; "world" -- "hello world"
myIntro = p_ (i_ "Don't " &lt;&gt; b_ "panic") -- &lt;p&gt;&lt;i&gt;Don't &lt;/i&gt;&lt;b&gt;panic&lt;/b&gt;&lt;/p&gt;
</code></pre>
<p data-pos="703:1-708:0">You can even use this in your own code, and can write simple instances for your own data structures. This vastly reduces the amount of specialised code you have to write – instead, you can simply re-use code and concepts from elsewhere, whether the standard library or an extension to those concepts like bifunctors.</p>
<p data-pos="709:1-715:0">In short: Haskell’s semantics and standard library encourage generalised concepts which, in turn, heavily promote both concept and code reuse, and this has driven the development of the ecosystem in a similar direction. That re-use means that the programmer need only discover the core concepts once, rather than for each library, providing an accelerating rate of learning and a much more efficient use of code.</p>
<p data-pos="716:1-729:0">The final productivity boost to discuss here is “fearless refactoring”, a term often thrown about in the Haskell community, but what does it actually mean? The essential point here is that the intransigence of the compiler makes it a useful ally when refactoring code. In languages with a more forgiving compiler or weaker type-system, refactoring code can introduce new bugs which are only discovered at runtime. When refactoring Haskell, because the type-system gives you the power to express your programme domain correctly, the process normally works as a constant cycle of “change, compile, change, compile” until all the compilation errors are gone, at which point you can be very confident you will not encounter runtime bugs. This reduces the cognitive load on the programmer, making it far faster (and less scary) to make changes to a codebase, whether large or small.</p>
<p data-pos="730:1-738:0">This section goes beyond just providing guardrails, guardrails which are clearly inspiring other language maintainers to introduce them into their languages, to talk about something very fundamental to productivity in programming: composable, re-usable concepts and the ability to “fearlessly” make changes to your programme. These are not simply features which can be added into a language, they are characteristics of it, and they relate to the more abstract notions laid out in the next section.</p>
</section>
<section data-pos="739:1-898:0" id="Reason-about-your-programmes-more-easily">
<h2 data-pos="739:1-740:0">Reason about your programmes more easily</h2>
<p data-pos="741:1-750:0">In general, programming is about telling a machine about some problem domain: the ontology of it, and the logical rules governing it, and then asking it to compute some results. These results should have some sort of meaning we can interpret, which is going to depend on how well we understand what our programme actually <em>means</em>. Additionally, for us to be able to trust the results of the computations we ask of the machine, we need to be confident that we have done a good job describing the problem domain in terms that result in a “good” understanding on the part of the machine.</p>
<p data-pos="751:1-757:0">A programme can have essential complexity or accidental complexity. The essential complexity comes from precisely describing the problem domain, and some domains are more complex than others. The accidental complexity comes from our (in)ability to express the problem domain to the machine. We can refer to these as <em>complexity</em> and <em>complication</em> to differentiate them.</p>
<p data-pos="758:1-764:0">Complications are bad and should be eliminated. They make it hard to reason about our programmes and therefore hard to trust their results. It also makes it hard to write the programmes in the first place, because we have to deal with all these complications. It’s a bit like trying to embroider a tapestry using a Rube Goldberg machine operated with thick mittens: unlikely to give you what you want.</p>
<p data-pos="765:1-776:0">We could look at general purpose programming languages on a scale of how well we are able to express a problem domain to a machine, and therefore to what extent we are able to trust the results of the computations we ask of that machine. Assembly is at one end: it is all about moving bytes between registers and performing arithmetic on them. Yes, you can write anything in Assembly but it is really hard to reason about the results you will get. As we move along the scale towards “high-level” languages we gain a set of abstractions which allow us to forget about the semantics of the lower level (e.g. moving bytes between registers) because they give us new semantics which are closer to those of the problem domain.</p>
<blockquote data-pos="777:1-779:36">
<p data-pos="777:3-779:36">The purpose of abstracting is not to be vague, but to create a new semantic level in which one can be absolutely precise. ~ Dijkstra, 1972<a id="fnref13" href="#fn13" role="doc-noteref"><sup>13</sup></a></p>
</blockquote>
<p data-pos="781:1-788:0">Haskell improves upon most high-level languages in this regard, providing a level of expressivity that allows more precise descriptions of the problem domain, easily intelligible both to the programmer and the machine. Broadly there are three major contributing factors to this (perhaps all of them can fit under the idea of denotation semantics): algebraic data types, parametric and ad hoc polymorphism, and declarative programming.</p>
<p data-pos="789:1-793:0">We can distinguish declarative and imperative programming by saying that declarative programming describes what a computation is supposed to be with respect to the problem domain, whereas imperative programming describes how a computation is to be carried out, step-by-step.</p>
<p data-pos="794:1-801:0">This is useful distinction: in imperative programming the operational semantics of the programme (the steps a machine must execute in order to compute a result) are mixed into the problem domain, making it difficult to reason about the meaning of a programme and, therefore, its correctness. Declarative programming, however, does not bother with defining these execution steps, making programmes much simpler to understand and reason about.</p>
<p data-pos="802:1-812:0">In Haskell, everything is an expression. In fact, your entire programme is a <em>single</em> expression composed of sub-expressions. These sub-expressions have themselves some sort of meaning, as does their composition. This is different to imperative languages, in which is common for there to be many lines of function calls and loops, with, often, deeply nested function calls, but these are not essentially composable. Haskell’s purity forces concise programmes composed of meaningful sub-expressions with no side-effects. This means that it takes far less time to understand the purpose of a given expression, and therefore reason about whether it is correct or not.</p>
<p data-pos="813:1-818:0">As ever, we are back to our two familiar pillars: so far, we have discussed the pure-functional pillar (single expression, compositionality, no side-effects), but the type system gives us the tools for expressing ourselves clearly to the machine (and to ourselves).</p>
<p data-pos="819:1-828:0">In fact, most of the preceding sections touch upon this in one way or another: we have data types for expressing the idea that some computations can fail in well-defined ways; we have sum types like <code>data
Response = Success | Failure FailureReason</code> which allow us to define all the possible values we might get from a function; we have typeclasses we can use as constraints on a function to semantically express what the result is in the most general way (like <code>CanSendEmails</code>); and we have generalised concepts like <code>Functor</code> and <code>Monoid</code> which describe how things behave rather than the steps to implement those behaviours.</p>
<p data-pos="829:1-838:0">Algebraic data types and typeclasses (and other, similar mechanics which deal with various polymorphisms) allow us to construct our own domain-specific languages within Haskell with which to write our programmes, while building upon common, well-established concepts to do so. These are declarative rather than imperative, and therefore are easy to reason about and to understand <em>semantically</em> because you do not have to either weed out the operational semantics (the step-by-step instructions) nor do you have to translate from a layer of primitives into your own domain.</p>
<p data-pos="839:1-843:0">This section has been necessarily abstract, because the idea is hard to communicate if one has not stepped outside the imperative paradigms in which most of modern programming sits. To try to elucidate it somewhat, here is a small sample programme using the concepts discussed above.</p>
<p data-pos="844:1-847:0">This programme is a basic accounting tool: given some initial monetary value, and a set of transactions (either in or out) in a variety of currencies, allow us to calculate the final value of the account.</p>
<pre data-pos="848:1-897:3"><code>-- this is a bit like an "enum" of possible constructors of the type Currency
data Currency = GBP | EUR | USD 

data Money = Money {
  currency :: Currency,
  value :: Double
  }

convert :: Currency -&gt; Money -&gt; Money
convert = -- not interesting to implement here as it is basically a lookup table

zero :: Money
zero = Money GBP 0

-- Ord gives us ways of comparing things (a natural ordering)
instance Ord Money where
  compare m1 m2 
    | currency m1 == currency m2 = compare (value m1) (value m2)
    | otherwise                  = compare m1 (convert (currency m1) m2)

instance Monoid Money where
  m1 &lt;&gt; m2 = Money (currency m1) (convert (currency m1) m2)
  mempty = zero

instance Functor Money where
  fmap f (Money curr val) = Money curr (f val)

data Transaction = In Money | Out Money

instance Functor Transaction where
  fmap f (In m) = In (f m)
  fmap f (Out m) = Out (f m)

normalise :: Transaction -&gt; Transaction
normalise transaction =
  let m = money transaction
   in if m &lt; zero then Out m else In m

instance Monoid Transaction where
  t1 &lt;&gt; In m2 = normalise (fmap (&lt;&gt; m2) t1)
  t1 &lt;&gt; Out m2 = normalise (fmap (&lt;&gt; (fmap (* (-1)) m2)) t1)

apply :: Transaction -&gt; Money -&gt; Money
apply (In m) initial = initial &lt;&gt; m
apply (Out m) initial = initial &lt;&gt; fmap (* (-1)) m

getAccountValue :: Money -&gt; [Transaction] -&gt; Money
getAccountValue startValue transactions = apply (fold transactions) startValue
</code></pre>
</section>
<section data-pos="899:1-1003:73" id="Epilogue">
<h2 data-pos="899:1-900:0">Epilogue</h2>
<p data-pos="901:1-905:0">I love writing in Haskell, and there are many reasons beyond this apologia why that is the case, but I also think it is an excellent choice for general purpose programming for anyone who wants to write robust software confidently and efficiently, and, of course, enjoyably.</p>
<p data-pos="906:1-913:0">I think what makes Haskell unique is the combination of its type system and functional purity – it’s not enough just to have functional programming, much as I love LISPs, nor is it enough just to have the type system, much as Rust seems like a great language. Many languages have bits of these features, but only a few have all of them, and, of those languages (others include Idris, Agda, and Lean), Haskell is the most mature, and therefore has the largest ecosystem.</p>
<p data-pos="914:1-921:0">While other languages are certainly adding features which I have mentioned above, this combination of a strong and expressive type system and pure functional programming is fundamental to the language: other languages without these axiomatic characteristics simply will not be able to implement them (and attempts to build some of these things into non-functional languages with weaker type systems is often extremely awkward and not very useful).</p>
<p data-pos="922:1-928:0">Not everyone has the luxury of choosing their tools in a professional context, whether because there is history in their team or the decisions are made by others. Even in this case, if you never end up using Haskell professionally, it will change how you think about programming, and to invert Alan Perlis’ quotation from the start of this essay: any language which changes how you think about programming is worth learning.</p>
</section>
<section role="doc-endnotes">
<hr>
<ol>
<li id="fn1">
<p data-pos="939:21-943:0">This is, however, not supposed to be an exhaustive list of all the things I think are great about Haskell, but just a subset of the most compelling reasons I recommend it to programmers. The rest they can discover for themselves.<a href="#fnref1" role="doc-backlink">↩︎</a></p>
</li>
<li id="fn2">
<p data-pos="944:26-947:0">Haskell is, of course, not always an appropriate choice. For example, it is never going to replace C or C++ for writing software for micro-controllers.<a href="#fnref2" role="doc-backlink">↩︎</a></p>
</li>
<li id="fn3">
<p data-pos="948:19-949:0">And quite possibly an “object-oriented” paradigm, to boot.<a href="#fnref3" role="doc-backlink">↩︎</a></p>
</li>
<li id="fn4">
<p data-pos="929:17-930:0">Perlis, A., “Epigrams in Programming” (<a href="http://cs.yale.edu/homes/perlis-alan/quotes.html">retrieved 2024-07-07</a>)<a href="#fnref4" role="doc-backlink">↩︎</a></p>
</li>
<li id="fn5">
<p data-pos="965:22-970:0">This is a bit like using <code>goto</code> to manage known failure states, which, I think, would be quite unintuitive if it hadn’t become such a dominant way of managing such failure states. In any case, I think it would make <a href="https://dl.acm.org/doi/10.1145/362929.362947">Dijkstra quite sad</a>.<a href="#fnref5" role="doc-backlink">↩︎</a></p>
</li>
<li id="fn6">
<p data-pos="987:9-996:0">This use of “2nd order” is not idiomatic in Haskell, as this is technically a higher-kinded type, whereas “order” is typically used to refer to functions, but just like a higher-order function is a function which takes another function as its argument, a higher-kinded type is a type which takes another type as an “argument”, and thereby produces a “concrete type”. Diogo Castro’s <a href="https://diogocastro.com/blog/2018/10/17/haskells-kind-system-a-primer/">2018 blog post “Haskell’s kind system – a primer”</a>  has more details on this.<a href="#fnref6" role="doc-backlink">↩︎</a></p>
</li>
<li id="fn7">
<p data-pos="997:9-1000:0">For those who are familiar with the idea, this is a bit like command-query segregation in the imperative world, but enforced by the type system.<a href="#fnref7" role="doc-backlink">↩︎</a></p>
</li>
<li id="fn8">
<p data-pos="971:25-980:0">To expand slightly on this, although it would be worth reading the <a href="https://lexi-lambda.github.io/blog/2019/11/05/parse-don-t-validate/">excellent blog post by Alexis King</a>, this means that instead of <em>validating</em>, i.e. checking a value meets some criterion, we parse an “unknown” value into a “valid” value, and thereby change its type. The result is that you can write functions which are defined to take the “valid” type (e.g. <code>EmailAddress</code>) and which never have to worry that it might be invalid, because you simply cannot forget to verify it, as you can in a “validation” pattern.<a href="#fnref8" role="doc-backlink">↩︎</a></p>
</li>
<li id="fn9">
<p data-pos="981:32-986:0">Christopher Armstrong <a href="https://www.youtube.com/watch?v=D37dc9EoFus">gave an interesting talk at Strange Loop in 2015</a> on his python library, which includes an introduction to the motivation for this sort of pattern. This might be good follow-on content if you are interested.<a href="#fnref9" role="doc-backlink">↩︎</a></p>
</li>
<li id="fn10">
<p data-pos="954:21-958:0">Actually, what is really distinctive about Haskell is that it is a <strong>lazy</strong>, pure, functional language, but laziness can be confusing and is only lightly related to the benefits discussed in this essay, and so I am going to ignore it.<a href="#fnref10" role="doc-backlink">↩︎</a></p>
</li>
<li id="fn11">
<p data-pos="1001:21-1003:73">You can find a big map of how these concepts related to each other by checking out the <a href="https://wiki.haskell.org/Typeclassopedia#Introduction">Typeclassopedia</a>.<a href="#fnref11" role="doc-backlink">↩︎</a></p>
</li>
<li id="fn12">
<p data-pos="961:18-964:0">Unison <em>does</em> have something called Unison Share, but it was written by a Haskeller and directly inspired by Hoogle (and, in fact, Unison is based on Haskell).<a href="#fnref12" role="doc-backlink">↩︎</a></p>
</li>
<li id="fn13">
<p data-pos="935:23-936:0">Dijkstra, E.W., ACM Turing Lecture: “The Humble Programmer”, 1972 (<a href="https://www.cs.utexas.edu/~EWD/transcriptions/EWD03xx/EWD340.html">transcript</a>)<a href="#fnref13" role="doc-backlink">↩︎</a></p>
</li>
</ol>
</section>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Show HN: Konty – A Balsamiq-alternative lo-fi wireframe tool for modern apps (290 pts)]]></title>
            <link>https://konty.app/http://localhost:4321/</link>
            <guid>41517312</guid>
            <pubDate>Thu, 12 Sep 2024 03:31:23 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://konty.app/http://localhost:4321/">https://konty.app/http://localhost:4321/</a>, See on <a href="https://news.ycombinator.com/item?id=41517312">Hacker News</a></p>
<div id="readability-page-1" class="page"> <header>   </header>  <!-- <main class="w-full mx-auto max-w-5xl px-4"> -->  <div>  <div>  <p> Make hand-drawn style wireframes quickly and easily. </p> </div>  <p><img src="https://konty.app/images/hero.png" alt="Hero"> </p>  </div> <div> <h2>
Features
</h2> <div> <div> <h2>Stress-free hand-drawn style</h2> <p> 
Don't spend a lot of time and effort creating low-fidelity wireframes.
        Express and communicate your ideas quickly. A hand-drawn style reduces
        stress on perfection and allows you to express ideas quickly.
  </p> </div> <div> <p><img src="https://konty.app/images/feat-hand.png" alt="Feature 1"> </p> </div> </div> <div> <div> <h2>Mix with various diagrams</h2> <p> 
You can also draw various types of diagrams including Flowchart, UML
        diagrams (Use Case, Class), Entity-Relationship diagram. It help you
        drawing user flows, information architecture, data models and others in
        one place.
  </p> </div> <div> <p><img src="https://konty.app/images/feat-dgms.png" alt="Feature 1"> </p> </div> </div> <div> <div> <h2>Rich shapes, icons and templates</h2> <p> 
Offer almost every UI component shape you need, over 1,500 icons, and a
        variety of templates for web, mobile, desktop and more.
  </p> </div> <div> <p><img src="https://konty.app/images/feat-libs.png" alt="Feature 1"> </p> </div> </div> <div> <div> <h2>Presentation mode</h2> <p> 
Shape could have a link to another page and you can present your
        wireframe like a slide show.
  </p> </div> <div> <p><img src="https://konty.app/images/feat-link.png" alt="Feature 1"> </p> </div> </div> <div> <div> <h2>Mirroring a frame</h2> <p> 
You can create a mirror of a frame. It allows you to create a master
        frame and reuse it in multiple places. When you update the master frame,
        all the mirrors will be updated.
  </p> </div> <div> <p><img src="https://konty.app/images/feat-mirror.png" alt="Feature 1"> </p> </div> </div> <!-- <Feature title="Export and share">
        Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nam ullamcorper
        magna eget ipsum dapibus viverra. Suspendisse odio diam, tempus vitae
        neque et, convallis vehicula felis. Cras porttitor, lorem vel egestas
        molestie, nulla nisl feugiat ex, sit amet hendrerit justo ante in
        tortor.
        <div slot="image" class="bg-slate-100 w-full aspect-video rounded-lg">
        </div>
      </Feature>
      <Feature title="AI assistant without subscription">
        Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nam ullamcorper
        magna eget ipsum dapibus viverra. Suspendisse odio diam, tempus vitae
        neque et, convallis vehicula felis. Cras porttitor, lorem vel egestas
        molestie, nulla nisl feugiat ex, sit amet hendrerit justo ante in
        tortor.
        <div slot="image" class="bg-slate-100 w-full aspect-video rounded-lg">
        </div>
      </Feature> --> </div> <div> <h2>
Subscribe to our newsletter
</h2> <p>
Get notified about new features and future giveaways by subscribing to
        our newsletter 👇
</p> <astro-island uid="Z2t7Dex" component-url="/_astro/subscribe-form.DHOhAvhe.js" component-export="SubscribeForm" renderer-url="/_astro/client.BIGLHmRd.js" props="{}" ssr="" client="only" opts="{&quot;name&quot;:&quot;SubscribeForm&quot;,&quot;value&quot;:&quot;react&quot;}"></astro-island> </div>  <!-- </main> -->  </div>]]></description>
        </item>
        <item>
            <title><![CDATA[NASA Pulls Off Delicate Thruster Swap, Keeping Voyager 1 Mission Alive (201 pts)]]></title>
            <link>https://gizmodo.com/nasa-pulls-off-delicate-thruster-swap-keeping-voyager-1-mission-alive-2000497434</link>
            <guid>41517272</guid>
            <pubDate>Thu, 12 Sep 2024 03:22:34 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://gizmodo.com/nasa-pulls-off-delicate-thruster-swap-keeping-voyager-1-mission-alive-2000497434">https://gizmodo.com/nasa-pulls-off-delicate-thruster-swap-keeping-voyager-1-mission-alive-2000497434</a>, See on <a href="https://news.ycombinator.com/item?id=41517272">Hacker News</a></p>
Couldn't get https://gizmodo.com/nasa-pulls-off-delicate-thruster-swap-keeping-voyager-1-mission-alive-2000497434: Error: timeout of 10000ms exceeded]]></description>
        </item>
        <item>
            <title><![CDATA[The Minneapolis Street Grid: Explained (112 pts)]]></title>
            <link>https://streets.mn/2024/09/11/streets-and-avenues-in-minneapolis/</link>
            <guid>41516627</guid>
            <pubDate>Thu, 12 Sep 2024 00:53:44 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://streets.mn/2024/09/11/streets-and-avenues-in-minneapolis/">https://streets.mn/2024/09/11/streets-and-avenues-in-minneapolis/</a>, See on <a href="https://news.ycombinator.com/item?id=41516627">Hacker News</a></p>
Couldn't get https://streets.mn/2024/09/11/streets-and-avenues-in-minneapolis/: Error: timeout of 10000ms exceeded]]></description>
        </item>
        <item>
            <title><![CDATA[My business card runs Linux (and Ultrix), yours can too (188 pts)]]></title>
            <link>http://dmitry.gr/?r=05.Projects&amp;proj=33.%20LinuxCard</link>
            <guid>41516476</guid>
            <pubDate>Thu, 12 Sep 2024 00:21:07 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="http://dmitry.gr/?r=05.Projects&#x26;proj=33.%20LinuxCard">http://dmitry.gr/?r=05.Projects&#x26;proj=33.%20LinuxCard</a>, See on <a href="https://news.ycombinator.com/item?id=41516476">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>


<p><span>My business card runs Linux (and Ultrix), yours can too</span>

<b>UPDATES:</b>: See "<a href="#fwv2">Version 2</a>"</p><h2>Table of Contents</h2>
<p><img src="http://dmitry.gr/images/linuxCardPromo.jpg" alt="Linux card project cover image"></p><ol type="1"><li><a href="#_TOC_2673ce8234cc65bf5d5e81743172700b">Why?</a></li><li><a href="#_TOC_544a3f85f27429ab93dcd5ab2a63de0f">Parts selection</a></li><li><a href="#_TOC_7c1edc4d9290405fda54122244529141">What to emulate</a><ol type="a"><li><a href="#_TOC_d30f50757568b8cfaf8978a26d616b30">A MIPS primer</a></li><li><a href="#_TOC_1f0bd75c3792469488787400675c6973">What system?</a></li></ol></li><li><a href="#_TOC_7e76ca59b9efa6e1895f99d810e88da7">Let's emulate!</a><ol type="a"><li><a href="#_TOC_2e782d97f878671469dbb579dab9819e">The CPU</a></li><li><a href="#_TOC_d26852db28343b84596911de435caac9">The FPU</a></li><li><a href="#_TOC_5568f04140f1e9c2bde99c4e9c3572c9">The MMU</a><ol type="I"><li><a href="#_TOC_f05af7a1cabb44e206600aa7d7074fbc">MMU basics</a></li><li><a href="#_TOC_68fe0e8a8517a9a939dc8ad830f3ed3f">The MIPS MMU</a></li><li><a href="#_TOC_3ec65a91b377c508e2a27543e95858ca">Emulating the MMU efficiently</a></li></ol></li><li><a href="#_TOC_07a3dd247ad6bec7fe401aa11619959f">Communication</a></li><li><a href="#_TOC_3a173782d010897c6136c3b4ca51cd47">Hypercalls</a></li></ol></li><li><a href="#_TOC_d2ce354603c45f5e016fd47b9b5c8524">Bring on the hardware!</a><ol type="a"><li><a href="#_TOC_30aace8835696930f69a264dd89cbc60">The honeymoon period</a></li><li><a href="#_TOC_da93b232e827178270bfa561bdf6569c">How not to design a DMA unit</a></li><li><a href="#_TOC_e8ee65a238a03dd3a641faaff13a9c8a">Clocks again</a></li><li><a href="#_TOC_f53cca91f548c50486dcd52bac3efe7c">SD card support</a></li><li><a href="#_TOC_03751c187a665141b52e489852323619">Coolness enhancement</a></li></ol></li><li><a href="#_TOC_9ffdb95250e26c7a6b468126ac7c75b0">How it works</a><ol type="a"><li><a href="#_TOC_ddd6939c0f0cff6ceb8b36172e02b459">How a normal DECstation boots</a></li><li><a href="#_TOC_f9ff15b5ee1683a37691730e23cbd714">How uMIPS boots</a></li><li><a href="#_TOC_55a0a5396aa1bd6b11c6eb5f8ce8d62a">How uMIPS runs</a></li><li><a href="#_TOC_d70d07edc2b683000e09c709f5921fe9">Linux changes</a></li></ol></li><li><a href="#_TOC_9a98bd63857ec302fcec693aae109dde">Improving performance</a><ol type="a"><li><a href="#_TOC_0f63c0f6caf6daba30629abfa403d51a">Instruction cache</a></li><li><a href="#_TOC_fc5278bbec5501f7f5fe590fb59653f2">Improving CPU speed</a></li><li><a href="#_TOC_aee3d0e166f8b8358baf265cb949a756">Improving RAM bandwidth</a></li><li><a href="#_TOC_73dc18b03073c31c5a2bbf1264aff327">Dirty hacks specifically for Linux</a></li></ol></li><li><a href="#_TOC_23b5439c727c2e8557d28f8495da9977">How to build and use one</a><ol type="a"><li><a href="#_TOC_c39b56d4489fb2507289e7ae19567b80">Building</a></li><li><a href="#_TOC_b3abf203d70a08b6d9725f0000f27122">Building from source</a></li><li><a href="#_TOC_7b797cff6f4cf0ca82225d6125fe1861">If you are lazy</a></li><li><a href="#_TOC_3f05d6f38862a5b18b2eb4e867a61fb1">Using</a></li></ol></li><li><a href="#_TOC_425d8e1e777a03c3e220dfaac38dbf1f">Version 2</a><ol type="a"><li><a href="#_TOC_2b50d6628ed817de809605854d478f68">Booting Ultrix</a><ol type="I"><li><a href="#_TOC_2375a25fb86a26c24006ed1d6e2c1c47">About Ultrix</a></li><li><a href="#_TOC_c7bce03d32236f53d1a3d5c04e680838">First time booting Ultrix</a></li><li><a href="#_TOC_87e05c1a936fe8f08f1621e5cd10c534">SCSI</a></li><li><a href="#_TOC_938bce276b64c8ccdaeb079ff7a0bd84">LANCE</a></li><li><a href="#_TOC_2b0d0fb2ee0246234fe0fd9845b87021">ESAR</a></li><li><a href="#_TOC_73d22765ace36f09f4bab935a1608d3c">Memory probing &amp; proper PROM API</a></li><li><a href="#_TOC_c6323dca5b2dcaf05756b569ad13f4b9">Ultrix Loader</a></li></ol></li><li><a href="#_TOC_fd2d1e73f2cd71a4c375685b4abed537">Making Ultrix work</a><ol type="I"><li><a href="#_TOC_42badd9e49002a3cefeaaf28867add83">Framebuffer</a></li><li><a href="#_TOC_347b8386b145ced18ea02f2654e00883">Mouse, Keyboard, ... and Tablet</a></li><li><a href="#_TOC_c4d283323af70979073f5cb6145f3a4b">Patches</a></li></ol></li><li><a href="#_TOC_9f744f0f0ac29e818b7b95da8ad8ee40">Improvements in the emulator</a><ol type="I"><li><a href="#_TOC_7c775bb4800b345425796dce2acef3a8">USB improvements</a></li><li><a href="#_TOC_c5f0dc32adc4e66649115eef7e76f94f">More perf improvements</a></li><li><a href="#_TOC_819a79dcd016b5a99e05c56515825abb">Removing the TLB refill fast path</a></li><li><a href="#_TOC_dda4cbe5c1a77c2c9bf66b0adc6fff4b">Cache geometry changes</a></li><li><a href="#_TOC_b3c8fd852368b78e09e2fc8b39a6ea5c">Serial improvements</a></li></ol></li><li><a href="#_TOC_cb1862398dbf0b5f7d8a7dbe73bee626">More Floating Point Unit work</a></li><li><a href="#_TOC_244cd79fc0c2e4a295228e647cf88dbc">A bootloader</a></li><li><a href="#_TOC_7fa430e699655fa918b84f63d1c5fae5">Hardware improvements</a><ol type="I"><li><a href="#_TOC_9183d42e96c2104c27070b535e3793d9">v1.3 hardware</a></li><li><a href="#_TOC_8971392389323f2057db749c94dcba05">And old hardware too</a></li></ol></li><li><a href="#_TOC_1a574cfae32687e675d081616b06e0a0">Building from source (updated)</a><ol type="I"><li><a href="#_TOC_3d2780b0eeabee178926de36f72658b8">The emulator</a></li><li><a href="#_TOC_7213a2cd3fa80d577b3092f614b7fdb3">The loader</a></li></ol></li><li><a href="#_TOC_d8926db6c437f738792454ba22ec2755">Further Updates</a><ol type="I"><li><a href="#_TOC_52cbfe3bfc8a13d03124dd2283d83807">Firmware v2.1.1</a></li><li><a href="#_TOC_6b3ae1897005585fd36708ff39e693cf">Firmware v2.2.0</a></li></ol></li></ol></li><li><a href="#_TOC_3cd960e7edc378fd94d8777b595ea515">In conclusion</a><ol type="a"><li><a href="#_TOC_0407c27180c9b019e644e8ad4c6a9324">Acknowledgements</a></li><li><a href="#_TOC_c20c35ef53bf1b70789ce94e66800147">Downloads</a></li></ol></li><li><a href="#_TOC_7e1e75c32bc9b275daf70df8cba8efb5">Comments...</a></li></ol>







<h2>Why?</h2>
<p><a href="http://dmitry.gr/images/linuxCardWhole.jpg"><img src="http://dmitry.gr/images/linuxCardWholeSmall.jpg" alt="Linux card in action"></a></p><p>A long long time ago (in 2012) I <a href="http://dmitry.gr/?r=05.Projects&amp;proj=07.%20Linux%20on%208bit">ran Linux on an 8-bit AVR</a>. It was kind of a cool record at the time. I do not think anyone has beaten it - nobody's managed to run Linux on a lower-end device than that 8-bit AVR. The main problem was that is was too slow to be practical. The effective speed was 10KHz, the boot time was 6 hours. Cool, but I doubt that any one of those people who built one of those devices based on my design ever waited for the device to boot more than once. It was time to improve it!
</p>
<p>So what could I improve? A number of things. First, I wanted the new design to be speedy enough to boot in a few minutes and reply to commands in seconds. This would make using the device practical and not a test of patience. Second, I wanted it to be easy to assemble for anyone. This meant no components with tight spacing, no components with too many pins, and no components with contacts hidden underneath them. A part of this wish was also that someone could <em>actually</em> assemble one, meaning that I had to select components that are <em>actually</em> buyable in the middle of the current ongoing shortage of, well, everything. Additionally, I wanted the device to be easy to interface with. The original project required a USB-to-serial adapter. This would not do. And, finally, I wanted the whole thing to be cheap and compact enough to serve as my business card.
</p>

<h2>Parts selection</h2>
<p><a href="http://dmitry.gr/images/linuxCardSchem.png"><img src="http://dmitry.gr/images/linuxCardSchemSmall.jpg" alt="Linux card schematics"></a></p><p>Some things were pretty easy to decide on. For storage, for example, microSD is perfect - easy to interface with, widely available, cheap. I picked a simple microSD slot that is easy to solder and easy to buy: <a href="https://octopart.com/1140084168-amphenol-25513977?r=sp">Amphenol 1140084168</a>.
</p>
<p>Some choices were a litle harder, but not too much so. For example, I was surely not going to use DRAM again. It requires too many pins, necessitating more soldering than I would consider acceptable, given that I wanted this device to be easy to assemble. SRAM in megabyte sizes does not really exist. But there is a cool thing called PSRAM. It is basically DRAM, but in easy mode. It itself takes care of all the refreshing and externally acts just like SRAM. Ok, cool, but still that would usually be a lot of pins. Right? Enter "AP Memory" and "ISSI". They make QSPI PSRAM chips in nice SOIC-8 packages. AP Memory has models with <a href="https://octopart.com/search?q=APS1604M-3&amp;currency=USD&amp;specs=0">2MB</a> and <a href="https://octopart.com/search?q=APS6404L-3&amp;currency=USD&amp;specs=0">8MB</a> of RAM per chip, ISSI has them in <a href="https://octopart.com/search?q=IS66WVS1M8BLL&amp;currency=USD&amp;specs=0">1MB</a>, <a href="https://octopart.com/search?q=IS66WVS2M8BLL&amp;currency=USD&amp;specs=0">2MB</a>, and <a href="https://octopart.com/search?q=IS66WVS4M8BLL&amp;currency=USD&amp;specs=0">4MB</a> sizes. I decided to use these. They are available and my code supports them all!
</p>
<p>There were some miscellaneous choices, like which regulator to use. I chose <a href="https://octopart.com/search?q=MIC5317-3.3YM5TR&amp;currency=USD&amp;specs=0">MIC5317-3.3YM5TR</a> due to having worked with it before and it being available in my "random chips" box. It is also easily available to buy.
</p>
<p>The USB connector was also a fun choice. I settled on: none. With the proper PCB thickness, one can lay out the board edge to fit into the end of a USB-C cable. I've seen this done before for micro-USB and figured it could be done for USB-C as well. At the end, though, I did not even need to do it, since <a href="https://github.com/Pinuct/Eagle_PCB_USB_connectors">someone else</a> already saved me the 30 minutes it would have taken. I just had to remember that the board thickness needs to be 0.8mm for this to work. 
</p>
<p>The last choice was the hardest - which microcontroller to use. The criteria were: built-in USB, no more than 32 pins with at least 0.65mm spacing, no pin-less packages, actually available to buy, QSPI support, as fast as possible. I did not get my last two wishes. After much searching and filtering for "in stock", I was forced to settle for an ATSAMD21 series chip, specifically the <a href="https://octopart.com/search?q=ATSAMDA1E16b-a&amp;currency=USD&amp;specs=0">ATSAMDA1E16</a>. It is not fast (specced to 48MHz, I clock it at 90MHz), it has many bugs (especially in its DMA engine), but it can be bought, it is easy to solder, and it'll have to do... <b>UPDATE</b>: another chip is now supported too, see later in this article.
</p>

<h2>What to emulate</h2>
<p><a href="http://dmitry.gr/images/linuxCardWholeBoot.png"><img src="http://dmitry.gr/images/linuxCardWholeBootSmall.png" alt="Linux card boot log"></a></p><p>I could have just taken my old ARM emulator (uARM) and used that. But what's the fun there? I decided to pick a new target. The ideal emulation target will: (1) be a RISC chip so that I have to spend fewer cycles on decoding instructions, (2) have no condition codes (like MIPS) or only set them on demand (like ARM), so that I am not wasting time calculating them every virtual cycle, (3) be 32-bit since 16-bit machines are all funky and 64 bit is a pain to emulate, (4) be known, and (5) have a workable set of GNU tools and Linux userspaces available. This set of requirements actually only leaves a few candidates: PowerPC, ARM, MIPS. I've done ARM, and I had no desire to mess with an endian-switchable CPU, so MIPS it was! This gives rise to the internal name of the project: uMIPS.
</p>
<h3>A MIPS primer</h3>
<p>MIPS is old - one of the original RISC designs. If you are a RISC-V fanboy(/girl/being), MIPS will look familiar - it is where 99.9994% of the initial RISC-V spec was copied from. The original MIPS was a 32-bit design, optimized for ease-of-designing-it. It has (and does not hide) a delay slot, has a lot of registers, including a hard-wired zero register, and does not use condition codes. The original design was R2000, back in 1986, followed soon by the improved R3000 in 1988. These were the last chips implementing the MIPS-I instruction set. MIPS-II was short lived and only included the R6000, which barely saw the light of day. The real successors were the MIPS-III R4000-series chips, released in 1991. These were 64-bit already in 1991! Clearly, the easiest target would be the R2000/R3000 chips with their simple MIPS-I instruction set.
</p>
<p>MIPS-I is a <a href="https://vhouten.home.xs4all.nl/mipsel/r3000-isa.html">rather simple instruction set</a>. So much so that a complete emulator of just the instructions can be written in under 1000 lines of C code without any dirty tricks. The floating point unit is optional, so it can be skipped (for now). The MMU is weird. It is just a TLB that the software must fill manually. This may seem like a rather unusual choice, but in reality it is a clever one, if you're in 1986 and tring to minimize the number of transistors in your chip. Why have a hardware pagetable walker, when you can make the software do it? You may ask how it handles the situation where the code that would do the walking is itself not mapped? Well, a part of physical memory is always hard-mapped at a certain address, and all exception handlers live there. Even if this were not the case, since the software manages the TLB, it would not be hard to reserve an entry for this purpose. The hardware even has support for some "wired" entries that are meant to be permanent. More on all of this later.
</p>
<h3>What system?</h3>
<p>MIPS R2000/R3000 is a processor. A processor does not a complete system make. What system to emulate? I searched around for a cool system and settled on DECstation2100 (or its big brother - DECstation3100). Why bother? It seemed like a simple system that Linux does support. Initially I was not planning to emulate the whole thing. Why? I had no plans to emulate the LANCE network adapter or the SII SCSI adapter. The last part might surprise you, since we will need a disk to use as our root fs. I did later add emulation of both of these parts, to make Ultrix happy.
</p>

<h2>Let's emulate!</h2>
<h3>The CPU</h3>
<p>MIPS is a rather old instruction set, which shows in a few places. The main one is that it attempts to prevent signed overflow. The normal instructions used for addition and subtraction will cause an actual exception if they cause an overflow. This does not map to how CPUs are used today, so nobody cares, but I still had to emulate it. There are "unsigned" versions of the instructions for addition and subtraction that do not do this, which is what all modern compilers will emit on MIPS.
</p>
<p>I wrote an emulator for the CPU in C first, to allow easy testing on my desktop, while the PCBs were being manufactured. It was not fast, nor meant to be, but it did allow for testing. You can see this emulator in <span>cpu.c</span>. Along the way here, I implemented some features of the R4000 CPU optionally. It turned out that to boot Linux compiled with modern compilers, this is necessary, as the compilers assume these instructions exist. Technically this is a bug. Realistically, I am likely the only person to ever notice. So, which features did I need to add? Likely branches (<span>BxxL</span> instructions), conditional traps (<span>Tcc/TccI</span> instructions), and atomics (<span>LL/CC</span> instructions).
</p>
<p>Of course, C is not the language one uses when one wants to go fast. I wrote an emulator in assembly too, targetting ARMv6-M (for the Cortex-M0 MCU I chose). I later added a sprinking of enhancements for ARMv7-M (in case I ever upgrade the project to a fancier CPU). This was tested on a Cortex-M7 and worked well too. The assembly emulator core is contained in <span>cpuAsm.S</span> and the ARMv6-M specific parts are in <span>cpuM0.inc</span>
</p>
<p>I mentioned delay slots earlier. What is a delay slot? Well, back in the day it was considered cool to expose your CPU's pipeline to the world. Just kiding, it was just a way to save some more transistors. Basically, the instruction after a jump will be executed even if the jump happens. This is called the delay slot. A naive way to avoid dealing with this is to place a <span>NOP</span> after each jump instruction. But with a good compiler, the delay slot can be put to a good use in almost all cases. Obviously one cannot place a jump instruction in the delay slot, since the CPU is already jumping somewhere. Doing this is illegal and undefined. An issue arises, however, if the instruction in the delay slot causes an exception of any sort. The CPU will record that the instruction was in the delay slot, and point the exception handler to the <em>jump</em> whose delay slot we're in. There is no way to return to this "in delay slot" state, so the exception handler is expected to take steps to somehow execute the delay-slot instruction and then complete the jump.
</p>

<h3>The FPU</h3>
<p>The DECstation came with an FPU, so that floating point operations would be fast. Back then this was a separate chip, which was optional in a MIPS R2000/R3000 system. Linux, in fact, will more-or-less corectly emulate the FPU if it is not present, but this is slow. I used this mode initially, and even fixed a few bugs in Linux's emulation, but, in the end, I implemented an FPU emulator. This was necessary since it seems like a lot of MIPS binaries I could find all assume the FPU is available and use it freely. I never reimplemented the FPU emulator in assembly, instead calling out to the C FPU emulator when needed. I figure that squeezing a few cycles out of each instruction is meaningless when the actual FPU operation takes hundreds. The code for this is in <span>fpu.c</span>. I include Linux patches to remove FPU emulation support from the kernel. This saves some RAM. Later, I also added support for a "minimal" FPU - it supports the registers but no operations. This is allowed by the spec, since the FPU may refuse to execute any operation it is "not sure it can do perfectly correctly", so any compliant OS must implement a full FPU fallback anyways. Why? This saves 16K of code size in the binary, opening the possibility of running uMIPS on smaller devices yet.
</p>
<h3>The MMU</h3>
<h4>MMU basics</h4>
<p>(this is a <em>very</em> oversimplified summary, feel free to skip if you know this, and do not complain to me that it is not perfectly accurate!)
</p>
<p>Most CPUs access memory using virtual addresses (<span>VA</span>). The hardware works in terms of physical addresses (<span>PA</span>). Ability to map one to the other is the underpinning of memory safety in modern operating systems. The purpose of an <span>MMU</span> (Memory Management Unit) is to translate virtual addresses to physical addresses, to allow for this mapping. Normally this is done using a tree-like structure in RAM, called a <span>pagetable</span>. Most CPUs have a component whose job it is to walk that structure to resolve what physical address a given virtual address maps to. This component is a <span>pagetable walker</span>. In most cases the <span>pagetable</span> has 3 or 4 levels, which means that resolving a <span>VA</span> to a <span>PA</span> requires reading 3 or 4 words from main memory. Clearly you do not want to do 3 useless memory accesses for every useful one. So usually another component is included in an <span>MMU</span> - a <span>TLB</span> (Translation Lookaside Buffer). Basically you can think of a <span>TLB</span> as a cache of some of the current <span>pagetable</span>'s contents. The idea is that before you go off doing those 3-4 memory reads into the <span>pagetables</span>, you can check and see if the <span>TLB</span> has a matching entry. If so, you can skip the <span>pagetable walk</span>.
</p>
<p>Clearly, like any cache, the <span>TLB</span> needs to stay in sync with the things it caches (the current <span>pagetables</span>). So, if the OS changes the <span>pagetables</span>, it needs to flush the <span>TLB</span>, since it might have stale entries. Usually, <span>TLB</span>s expose very little interface to the CPU, so there isn't a way to go read all the entries and remove only the newly-invalid ones. Additionally, this would be slow, so this is not usually done. However, invalidating the entire <span>TLB</span> also has costs - it needs to be re-filled, at the cost of 3-4 memory accesses per entry. This could hurt performance. A solution commonly used is called an <span>ASID</span>.
</p>
<p>What are the four main cases when <span>pagetables</span> might be modified? (1) Adding a new mapping over a virtual address that previously was not mapped to anything, (2) changing permissions on on existing mapping, (3) removing a mapping, and (4) entirely changing the memory map (for example to switch to a completely different process). In case 1, no <span>TLB</span> flush is necessary, since no stale <span>TLB</span> entry can exist. Cases 2 and 3 do indeed require flushing the <span>TLB</span>, but they aren't that common. Case 4 is quite common, though. It is done at every context switch. One might point out that since we're changing the entire memory map, the entire <span>TLB</span> would be invalid, and thus flushing it isn't a problem. This is wrong. Besides mapping userspace things, the <span>MMU</span> also maps various kernel structures, and there is no point penalizing them.
</p>
<p>If we could somehow tag which entries in the <span>TLB</span> go with which process, and temporarily disable them when another process runs, we could avoid a lot of context-swich flushing and the performance costs imposed by it. It would also be cool if we could tag entires that belong to the kernel and are valid in every process. Well, this exact technology exists in many <span>MMU</span>s. The idea here is that each <span>pagetable</span> entry will have a bit marking it as "global" (valid in all memory maps) or not. There should also be a register in the CPU setting the current <span>ASID</span> (Address Space ID). When a <span>TLB</span> entry is populated from the <span>pagetables</span>, the current <span>ASID</span> is recorded in it. When a lookup in the <span>TLB</span> is done, only entries matching the current <span>ASID</span> or those marked "global" will match. Cool!
</p>
<h4>The MIPS MMU</h4>
<p>The idea at the time was to save transistors. Which of the above could be cut? Well, cutting out the <span>TLB</span> guarantees terrible performance in all cases. But that <span>pagetable walker</span>, do we really need it? What if we make the sotware do it? We can add a little bit of assistance, like ability to manage the <span>TLB</span> efficiently, but skip on the <span>pagetable walker</span> hardware. This is what MIPS did. Here is the MIPS virtual address space:
</p>
<p>Addresses               Name   Mapping
0x00000000..0x7fffffff  kuseg  mapped via MMU
0x80000000..0x9fffffff  kseg0  mapped to physical 0x00000000..0x1fffffff, cached if there is a cache, only accessible in priviledged mode
0xa0000000..0xbfffffff  kseg1  mapped to physical 0x00000000..0x1fffffff, not cached, only accessible in priviledged mode
0xc0000000..0xffffffff  kseg2  mapped via MMU, only accessible in priviledged mode
</p>
<p>So, as you can see, some <span>VA</span>s do not map via the <span>MMU</span> at all. This means that code living there is able to run no matter the state of the <span>MMU</span>. Linux and Ultrix, predictably, put the kernel in <span>kseg0</span>. The kernel does, however, need to be able to dynamically map things in as well. <span>kseg2</span> is one gigabyte of address space that is mappable via the <span>MMU</span> that the kernel can use. Memory-mapped devices will usually be accessed via <span>kseg1</span>. The 2 gigabytes at the bottom of the address range(<span>kuseg</span>) are for userspace tasks.
</p>
<p>What entry in a <span>TLB</span> should one replace when one needs to insert a new entry? An obvious answer might be "the one least recently used", but that would require tracking use, which costs transistors too. A simplification is "the one least recently added". This is easy, but it hides a fatal flaw. Imagine your <span>TLB</span> has N entries, and your workload sequentially uses N + 1 addresses, such that each would need a <span>TLB</span> entry. Now you'll always be replacing the entry you're about to need, guaranteeing that you <em>NEVER</em> hit the <span>TLB</span> and do a lot of pointless <span>pagetable</span> walks. How do we avoid this? The simplest method is replace a random entry. Sure, it might be the entry you're about to need, but for an N-entry <span>TLB</span> the chances are 1/N.
</p>
<p>Generating random numbers is slow in software, so MIPS R2000/R3000 provide some help. The CPU has a register called, literally <span>RANDOM</span> which is supposed to be constantly incrementing, every cycle. Since the "when" of "when will you next need a new <span>TLB</span> entry" is not predictable, this is as good as random, and requires very few transistors. The idea is that whenever you need to replace a <span>TLB</span> entry, you use a special instruction <span>TLBWR</span> to write to a random entry. I did not tell you about <span>ASID</span>s by accident either. The MIPS R3000 <span>MMU</span> implements a 6-bit <span>ASID</span>.
</p>
<h4>Emulating the MMU efficiently</h4>
<p>Emulating the R3000 <span>MMU</span> is a bit of a pain. Since any entry can be in any location, the proper way to do a lookup is to check each one. Doing a 64-cycle loop for every memory access is a non-starter speed-wise, of course. I use a hashtable indexed by the virtual address to keep all the TLB entries in buckets for faster checking. Using 128 buckets virtually guarantees that most buckets have zero or one entry in them, permitting much faster lookups. Initially this was a simple table of pointers, but this used too much RAM, so now it is a table of indices.
</p>
<h3>Communication</h3>
<p>The DECstation had a few ways to communicate with the outside world. It had a built-in network card<s>, which I do not emulate</s>. It was optional<s>, and I haven't found a use for it yet</s>. Maybe I will later - it does not look complex. It also had a SCSI controller which one could attach hard disks and other SCSI peripherals to. Emulating this would be a fun challenge, and I'll probably get to it later, but I did not do it now - it was not necessary - I wrote a paravirtualized disk driver for Linux using hypercalls, more on this later. There was also an optional framebuffer card one could install that added support for a monochrome or a color display. Emulating these would also not be too hard, but my business card lacks a display, <s>so I did not do it either</s> - plus I am not even sure that Linux can make a use of it.
</p>
<p>The last method of communications that the DECstation had was <span>DC7085</span> - a serial port controller that is basically a clone of a PDP11-era <a href="https://gunkies.org/wiki/DZ11_asynchronous_serial_line_interface"><span>DZ-11</span></a>. It supports four serial ports at a blistering 9,600bps speed (or any integer division thereof). Each serial port was allocated a purpose, and they were wired to different connectors indicating this purpose. #0 was for the keyboard, #1 for the mouse, #2 for modem, and #3 for printer. To the machine they are all the same, this was just the purpose DEC assigned to them. The stock <span>PROM</span> would use #3 as serial console instead if it did not detect a keyboard at #0, thus it is customary to use #3 as serial console for Linux on the DECstation. My <span>PROM</span> surrogate does not bother looking for or supporting external keyboard, and just defaults to serial console on #3. That being said, since it is cool to allow multiple login sessions, I also export <s>#0</s> #2 as a second virtual serial port, so that you may login from two serial consoles at once, and do two things at once. How cool is that?
</p>
<p>So, how do I export these serial ports? When you connect the card to a computer, it'll show up as a USB composite device comprised of two CDC-ACM virtual serial ports. One of them is port #3, another is port <s>#0</s> #2 on the virtual <span>DZ-11</span>. How will you know which is which? #3 has the boot console printing and will have the initial <span>sh</span> prompt. If you do not see this, try the other one, computers do not always number them in the order I export them.
</p>
<h3>Hypercalls</h3>
<p>In the real world the <span>PROM</span> had to probe the real hardware to detect what was present where. As my <span>PROM</span> is running in an emulator, there is no need for such mess. We can simply request things from the emulator in an agreed-upon way. That way is a <span>hypercall</span> - a special invalid instruction that, if encounted in supervisor mode, the emulator will treat as a request for some kind of service. The instruction I chose is <span>0x4f646776</span>, which is in the <span>COP3</span> (coprocessor 3) decode space that was not allocated to any real purpose in these chips. The calling convention is close to the normal C calling convention on MIPS: parameters are passed in <span>$a0</span>, <span>$a1</span>, <span>$a2</span>, and <span>$a3</span>, return values are in <span>$v0</span> and <span>$v1</span>. The <span>$at</span> register gets the "hypercall number" - the specific service we're requesting.
</p>
<p>A few hypercalls are implemented. #0 is used to get the memory map. The parameter is word index of the memory map to read. Word 0 is "how many bits the memory map bitmap contains", word 1 is "how many bytes of RAM each bit represents", words 2 and on are the bits of the map, up to the total specified in word 0. This can be used to build a memory map that the <span>PROM</span> can furnish to the running OS and allows me to have discontinuous RAM. Linux supports this and I tried it, but did not end up needing it. It is here in case I change my mind and need it again.
</p>
<p>Hypercall #1 outputs a single byte to the debug console (which is the same as <span>DZ-11</span> port 3). This is used by the <span>PROM</span> and <span>mbrboot</span> to output debug strings without needing to have a complete <span>DZ-11</span> driver in there. Hypercall #5 will terminate emulation. This can be used on the PC version of the emulator to quit peacefully.
</p>
<p>Hypercalls #2, #3, and #4 are used for SD card access. #2 will return card size in sectors, #3 will request a read of a given sector to a given physical RAM address and reply with a nonzero value if that worked. #4 will do the same for a card write.
</p>

<h2>Bring on the hardware!</h2>
<h3>The honeymoon period</h3>
<p><a href="http://dmitry.gr/images/linuxCardBoard.png"><img src="http://dmitry.gr/images/linuxCardBoardSmall.jpg" alt="Linux card board layout"></a></p><p>The first revision of this board came up well initially, after I sorted out the mess that is ATSAMD21's clocking system. I appreciate flexibility as much as the next guy, but this thing is <em>TOO</em> flexible. It took a lot longer than I'd care to admit to get this thing running at a sane speed and to enable some peripherals. The docs were too sparse to be of much use, too. Atmel, what happened to you? You used to have the best docs!
</p>
<p>The first revision of the board had two memory chips, each on their own SPI bus, an SD card on an SPI bus, and USB with the proper resistors. The USB was perfect. Unlike everyone and their grandmother (STMicro, I am glaring at you), Atmel did not license annoying Synopsis USB IP. They made their own. It is easy to use, elegant, and works well. Seriously, it just worked. In two days I got the hardware to work and wrote a USB device stack. I tip my hat to the team that worked on the USB controller. That being said, I have concerns. My main issue: USB descriptors aren't small. They are constant. I'd prefer to keep them in flash. I'd prefer to, but cannot. The USB unit uses a built-in DMA unit to read the data to send. This DMA unit <em>CAN</em> access flash, but if you have any flash wait states enabled, it sends garbage. I suspect that Atmel only tested it for reading from RAM, forgot that some memories have wait states, and did not account for that. Keeping all my descriptors in RAM is a colossal waste of RAM, which there is only 8KB of. Remember that tiped hat? I rescind it, Atmel. I had to work around the issue by sending the descriptors one piece at a time (rather than letting the hardware DMA it all automatically) just to save the valuable RAM.
</p>
<p>Using the SPI units directly worked well enough, until I tried to speed them up. Past about 18MHz The received data was garbled (missing a bit or two, all the following bits shifted). No amount of searching found an issue in my code, and all sample code did more or less the same things. My bus analyzer showed no issues. What gives? <a href="https://microchipsupport.force.com/s/article/SPI-max-clock-frequency-in-SAMD-SAMR-devices">THIS GIVES</a> (<a href="https://archive.ph/IJTHU">archived</a>)! I was beyond furious when I found this forum post. Here I was, trying to build a fast device, and my SPI bus was going to be limited to the speed of a tired snail calmly strolling through peanut butter! With some more testing I found that the SPI units will work fine to about 16MHz, which I'll have to live with.
</p>
<p>The SPI units have no FIFOs, so code must manually feed them one byte at a time and read one byte at a time. This means that there is space between bytes on the bus as code wrangles bytes in and out of registers and memory. This is a waste of potential speed. The solution is DMA. Luckily this chip has DMA. Unluckily, it is fucked beyond belief, to a point where I am beginning to suspect that it was designed by a sleep-deprived stark raving lunatic.
</p>
<h3>How not to design a DMA unit</h3>
<p>A normal garden-variety DMA unit has some minimal global configuration, and a few channels, each independent from the rest. Each channel will usually have a source address, a destination address, a length, and some configuration, to store things like transfer chunk size, trigger, interrupt enable bits, etc. Thus it is common in ARM MCUs to have each channel have precisely these 4 32-bit configuration registeres: SRC, DST, LEN, CFG. This is 16 bytes of SRAM per channel. ATSAMD21 has 12 DMA channels, so that would be 192 bytes of config data for the DMA unit as a whole. Not that much. Well, Atmel was having none of this! Instead, the unit itself only has a <em>POINTER</em> to where in the user RAM all this config data lives. For every transfer, the DMA unit will load its internal state for the active channel from this structure in RAM, and then operate on the channel. If another channel's data was already loaded, it will be written out to RAM first. Depending on your experience level, you may already be on your third or fourth "oh, hell fucking no" as you read this...
</p>
<p>Why is this bad? Let's imagine two SPI units being fed by DMA. Each one will have two DMA channels, one for receive, one for transmit. Four channels are active in total. Now what happens as both the SPI units are enabled? Two DMA channels (the transmit ones) will go active and attempt to send a byte. One will go first, then the second. This will generate <em>14</em>(!!) bus transactions to the RAM! Four to read config data for one channel, one to read the byte to send, four to write back this config data, four to read the config data for the next channel, and one more to read the byte to send. So in order to send 2 bytes, the DMA unit did 14 RAM accesses. Not great. But wait...there's more. Let's take a look what happens next, as the SPI units finish sending this byte and clocking in the received byte, but are also ready for the next byte to send! At this point in time, logically only four bytes need to be moved (two from the units into the receive buffers, two from the transmit buffers into the units). Let's see how this plays out. Remember the DMA unit's internal config data is currently loaded to the second transmit channel's. First, it'll have to do 4 writes to write that data out, then 4 reads to load the first receive channel's structures, one write to memory to write the received byte to RAM, 4 writes to write out this channel's structures out, 4 reads to load structures for receive channel number 2, one write of the received byte to RAM, 4 bytes to write out the config structure for this channel back out to RAM, and then the 14 we already discussed to send the next two bytes. That adds up to 36 RAM accesses to simply read two bytes and write two bytes. All this pain, simply to save the transistors on the 192 bytes of SRAM it would have taken for the DMA unit to store all the config data internally.
</p>
<p>So, why is this bad? Let's say our MCU is running at its designed speed of 48MHz, its SPI units running at their designed max speed of 12MHz. At the point the second bytes need to be sent and first received bytes need to be received, we'll need to perform 36 accesses to RAM, but also 4 accesses to the SPI unit. The SPI unit is on an APB bus, which means that any access to it takes at least 4 cycles. This means that in between each sent and received byte we'll need 36 + 4 * 4 = 52 cycles. If the SPI unit runs at 1/4 the CPU speed, then it will send/receive a byte every 8 * 4 = 32 cycles. So every 32 cycles we'll need to do 52 cycles' worth of work. When they do not get enough cycles, the DMA channels give up and stop working... Oops... 
</p>
<p>So, what can be done? I worked out a hybrid method where I send data using CPU writes and receive using DMA. This worked for two channels, but would not work for more. Once I got rev2 boards that had 4 RAM chips, even this failed, as just the 4 receive DMA units starved each other of bandwidth and got cancelled. Why was Atmel so damn stingy with internal SRAM? We'll probably never know. But they could have solved this exact issue simpler than with 192 bytes of SRAM in the DMA unit. Just adding a 4-byte FIFOs into the SPI units would do as well, then each DMA transaction could transfer more than a single byte, alleviating this traffic jam. Sadly, apparently nobody at Atmel has even tried to actually use their chip for anything. Atmel, what happened to you?
</p>
<h3>Clocks again</h3>
<p>My clocking woes were not over yet. This chip has a number of internal oscillators, one of which is supposed to be a rather precise 32KHz oscillator called <span>OSC32K</span>. I wanted to use that as a source clock for a timer to implement my virtual real time clock. Well, despite much pain and many tears, the damn clock would not start... ever. The code should be simple: <span>SYSCTRL-&gt;OSC32K.bit.EN32K = 1; SYSCTRL-&gt;OSC32K.bit.ENABLE = 1; while (!SYSCTRL-&gt;PCLKSR.bit.OSC32KRDY); </span> Yeah... that did not happen. At the end, I decided that I can use a less-precise <span>OSC32KULP</span> to clock my timer. That one did start and I was able to use it. By this point in the project I was worn out, desensitized to this chip's many faults, and completely out of WTFs, so I resigned myself to a slightly imprecise real-time clock and trudged on.
</p>
<h3>SD card support</h3>
<p>Not really much to say about SD card support. Been there, done that, got the t-shirt. My initial code for the prototype used multi-block reads and writes for better card access speed, but in the final prototype I was forced to abandon it since one of the RAM chips on the b2 boards shared the SPI bus with the SD card, so leaving the card selected was not an option. This was not that big a deal since SD access is rarely, if ever, a bottleneck here. Any card up to 2TB is supported.
</p>
<p>In the v2 revision of the board I wired up card detect pin to the MCU. It was not used, but I thought that I might find a use for it. I did not, so in v3 boards it was removed. I also added a card "activity" LED which lights up when card is accessed. It is simply a LED between the card's chip select line and Vcc. Whenever the card is selected, it is on. This LED also surves a second purpose. If at boot time the SD card or SPI SRAMs fail to initialize, it'll blink out an error code to help identify the problem.
</p>
<h3>Coolness enhancement</h3>
<p>Now that the prototype worked and I was doing the layout for the final version, I decided to do some things to make it look cool. I buried all the traces in layers 2 and 3, leaving layers 1 and 4 uninterrupted copper. It loooks super cool! Of course the top layer copper is interrupted for the actual SMT pads, but other than that, it is all perfectly smooth and looks amazing!
</p>

<h2>How it works</h2>
<h3>How a normal DECstation boots</h3>
<p>Normally there is a built in 256KB ROM (called <span>PROM</span> by DEC) at physical address <span>0x1fc00000</span> that contains enough code to show messages onscreen and accept keyboard input, talk to SCSI devices, load files from disk to RAM and jump to them. This <span>PROM</span> also provides a lot of services to the loaded operating system via an array of callbacks. This includes things like console logging, EEPROM-backed environment variables, memory mapping info, etc. This is rather similar to UEFI. Normally this <span>PROM</span> would read the environment variables from EEPROM that would tell it which device to boot, and then load a kernel and boot from that device if all goes well. This emulator does not boot this way
</p>
<h3>How uMIPS boots</h3>
<p>I had no desire to include a large ROM in the emulator, as the flash space in the microcontroller is limited. I also do not have a graphical console or a keyboard per se. That being said, I had to implement a sizeable subset of the <span>PROM</span> somehow, since MIPS Linux uses it. What to do? I decided to come up with my own boot process, which can still work just as well. There is indeed a ROM at <span>0x1fc00000</span>. This is necessary for rebooting to work from Linux. <s>That rom is tiny - 32 bytes</s>. Its source code is found in the "romboot" directory. It <s>merely</s> loads the first sector of the SD card to the start of RAM at <span>0x80000000</span> and jumps to it. The first sector of the SD card contains a standard MBR partition table and up to 446 bytes of code. The code that lives here can found in the "mbrboot" directory. It is also rather simple. It looks through the partition table for a partition with type byte of <span>0xBB</span>. If not found, an error is shown. Else, the partition in its entirety is read into RAM at <span>0x80001000</span>, and then jumped to. This partition can be arbitrarily large, and this is where my implementation of the "PROM" lives. The actual size limit on it is placed by the fact that MIPS Linux expects to be loaded at <span>0x80040000</span>. This is no accident - the first 192K of RAM is reserved for the <span>PROM</span> to use as long as the operating system expects to use <span>PROM</span>'s services. Thus the limit on the loader's size is 188K. 
</p>
<p>My <span>PROM</span> implementation's code can be found in the "loader" directory. It will search the SD card for a partition marked as active, attempt to mount it as FAT12/16/32, and look for a file called "VMLINUX" in the root directory. If found, it will be parsed as an ELF file, properly loaded, and run. Else an error will be shown. As this code has no serious size limits, it implements a proper ability to log to console, printf, and all sort of such creature comforts. As far as <span>PROM</span> services go, it provides console logging, memory mapping info, and reading environment variables, at least enough to make Linux happy. I have not tried to boot other operating systems on uMIPS (yet?).
</p>
<p>The kernel commandline I pass is rather simple: <span>earlyprintk=prom0 console=ttyS3 root=/dev/pvd3 rootfstype=ext4 rw init=/bin/sh</span>. The first parameter provides for early boot logging via the <span>PROM</span> console, which is useful to see. After the kernel is up, it'll use the third serial port for console. Originally for the DECstation that was the printer serial port, but Linux users on DECstation use that for serial console due to that being the easiest port to convert into a simple serial port. The rest just tells the kernel how to boot. I prefer to boot into sh, and then issue <span>exec init</span> myself, thus the <span>init=/bin/sh</span>
</p>
<h3>How uMIPS runs</h3>
<p>After all the optimizations (which I'll detail in a bit) the effective speed of my virtual MIPS R2000/R3000 on this infernal ATSAMD21 chip is around <s>900KHz</s> 1.2MHz. The CPU spends around 8% of its time handling timer interrupts, and thus around <s>0.83MIPS</s> 1.06MIPS of CPU cycles are left for useful work. With this, the kernel takes around 2 minutes to boot and run <span>sh</span>. Executing busybox's <span>init</span> and getting to the login prompt takes another minute. Overall not too bad. Commands reply instantly, or in a few seconds. It takes gcc around 2 minutes to compile a hello world C program, and I estimate that in a few days' time, one could rebuild the kernel on the device itself, copy it to <span>/boot</span>, and reboot into it. Yes, I <s>do intend to try this and time it</s> did do this and it worked!
</p>
<p>The emulated real time clock is actually real time, plus or minus the inaccuracies of ATSAMD21's ultra-low-power 32KHz timer. It is ok enough that you will not notice. Try the <span>uptime</span> command.
</p>
<p>There is just one thing I did not yet address concerning running Linux on uMIPS. The storage. I said that it is an SD card, but surely DECstation had no SD card slot. However, Linux is open source. I simply created my own very simple paravirtualized disk driver which uses a hypercall to talk directly to the emulator and request sectors to be read or written directly into the virtual RAM. To Linux, this looks just like DMA, except instant. The whole implementation of the driver is under 200 lines of code and can be seen in <span>pvd.patch</span>
</p>

<h3>Linux changes</h3>
<p>I made some changes to the kernel to make life easier. They are provided as patches against the 4.4.292 kernel, and as is a working kernel image. Why that version? Because when I started that project, it was an LTS version of the kernel, and since RAM is short, I wanted the smallest possible kernel, so this was preferable to a later version. The config I am using is available in <span>kernel_4.4.292.config</span>. A config for an even smaller kernel (that requires uMIPS to emulate the full FPU) is available in <span>kernel_4.4.292.config_nofpu</span>
</p>
<p>I did a lot of work making the kernel as small as possible. Since Linux does not support paging out pieces of the kernel, every byte of kernel code is one byte fewer available to use for user space. I ruthlessly removed options that were not needed. In the end I got the kernel down to just under 4MB, which is pretty damn good, considering that MIPS instructions are not very dense.
</p>
<p>As part of this work, I made a few code patches. For various reasons (cough..delay slots..cough) the kernel can find itself needing to interpret userspace code, or parse userspace instructions. No matter what kernel configs I gave, the code to handle microMIPS (a future MIPS expansion not known in the days of R2000/R3000) was present. It was wasting space and time trying to handle things that would never happen. The patch <span>useless_exc_code.patch</span> removes this code if the target CPU does not support microMIPS</p>
<p>Before I implemented my FPU emulator, I was using the kernel's FPU emulation code that traps and executes FPU instructions. It had a bug. If compiled for a 32-bit MIPS processor it did not properly emulate some FPU instructions that operate on the double type. I believe this is wrong. It was causing crashes in code compiled for R3000. The patch <span>fpu.patch</span> modifies the kernel's MIPS FPU emulator by adding a config option to enable the full FPU emulation even on MIPS-I chips.
</p>
<p>Due to the differences between the R2000/R3000 and the R4000 the kernel needs to know at build time which CPU it is being built for. If you attempt to run the wrong kind of kernel on the wrong kind of CPU, it only gets far enough to panic about it. Fine, OK, but then why does this flag not affect a lot of TLB-handling code. Both kinds are always compiled in, despite us knowing at build time with 100% certainty that at least half of it will not ever be of any use? The patch <span>tlbex_shrinkify.patch</span> wraps the useless code in checks for the compile-time-selected CPU type and thus removes some kernel code, saving valuable bytes.
</p>
<p>As uMIPS runs with a real real-time clock, I did not want Linux to spend too much time handling timer interrupts. Normally, a 128Hz timer is used on DECstations by Linux. I added options for 64Hz, 32Hz, and 16Hz timer ticks as well. This reduces effective timer resolution, but effectively unloads the virtual CPU from having to spend most of its time handling timer interrupts. The patch <span>clocksrc.patch</span> does this, and the one called <span>kill_clocksrc_warning.patch</span> silences a pointless warning about timer resolution.
</p>
<p>If you do build uMIPS with full FPU emulation, there is aso a patch to remove all of the FPU emulation code from the kernel to save a few KB of RAM: <span>fpu.patch</span>. 
</p>

<h2>Improving performance</h2>
<h3>Instruction cache</h3>
<p><a href="http://dmitry.gr/images/linuxCardCompileTime.png"><img src="http://dmitry.gr/images/linuxCardCompileTimeSmall.jpg" alt="Linux card board layout"></a></p><p>One thing the processor will surely do every cycle is fetch an instruction. This means that every cycle begins with a memory access. For us that is a painful subject thanks to Atmel's errata-ridden SPI unit. And not just that, memory translation also needs to happen, and that also takes time. A good way to avoid both of these problems is a VIVT instruction cache. It'll read instructions 32 bytes at a time, and allow us to hopefully often not need to translate addresses or reach for main memory. I allocated 2KB of RAM to this cache. It is 32 sets of 2 ways of 32 byte lines. Whenever memory mappings change, it needs to be invalidated. I do this automatically and thus the running code on the virtual MIPS CPU does not need to know about it. The measured hit rate while booting Linux is around 95%, which is pretty nice for such a small cache. The geometry was determined experimentally by profiling how long a boot takes with various cache geometries. This one was found to be the best.
</p>
<h3>Improving CPU speed</h3>
<p>ATSAMD21 series is specified to run at 48MHz. In my testing they run perfectly well up to 96MHz, with some specific chips able to hit 110MHz. I found no chip unstable at 96MHz, so I decided to just run at 90MHz, for some safety margin. This immediately got me a pretty serious performance uplift. No, it is not really 100%, since (1) SPI RAM is still limited by the SPI speed limit, and (2) flash memory has wait states which had to increase for the larger speed. But this did give me an honest 65% improvement. Still a good start. Now RAM SPI runs at CPU / 6 = 15MHz.
</p>
<h3>Improving RAM bandwidth</h3>
<p>Since I could not make the RAM SPI units go faster due to Atmel's incompetence, I decided to go wider! I can drive four units at once. Given, there is overhead to each read and write command, but still this is faster than one or two. <s>My code initially supported one, two, or four RAM chips, but for simplification I dropped that support and now only support four-channel RAM.</s> Quite the statement eh? This microcontroller has four-channel RAM! The emulator accesses RAM in increments of 32 bytes. The RAM read/write commands themselves are 4 bytes each. This means that for a single-RAM chip situation, reading 32 bytes takes (4 + 32) * 8 = 288 SPI bits. In dual-channel configuration it'll take (4 + 16) * 8 = 160 SPI bits, since the command is still 4 bytes long, but we only read 16 bytes from each RAM , for a total of 32. For quad-channel RAM, we thus have (4 + 8) * 8 = 96 SPI bits to read 32 bytes. This is a 66% improvement from the single-channel case! In reality the improvement is less, since quad-channel mode cannot use DMA at all, so it is a bit slower. Real-life measurement shows that quad-channel mode is a 50% improvement over the single-channel case. But still, given this damn chip, any improvement is an improvement I'll take.
</p>
<p>But, why are all the RAM acceses 32 bytes in size? Well, as you see RAM accesses are slow. A typical 32-byte access takes 140-ish SPI cycles, which is around 12 microseconds. If every access took that long, my emulated CPU would be limited to no more than 85,000 memory accesses per second. That is too slow to be practical. Something had to be done. I decided on a cache. Sadly, my microcontroller has a very limited amount of RAM, so the cache had to be small. I evaluated various cache geometries, and found that a 20-set 2-way cache with 32-byte lines produced the best performance uplift for the emulator. It gets a 91% hit rate while booting the kernel, which is a pretty good payoff for 1.25KB of RAM. With a hit taking around half a microsecond and a miss taking around 12 microseconds, adding this cache improved the average memory access by 87%! Yes, this is effectively an L2 cache. Now, how many emulators do you know that have an L2 cache to paper over the terrible performance of their chosen host hardware, eh? The cache allocates on reads and writes, except for reads and writes of precisely 32 bytes in size. Those are passed through directly because they are either SD card access DMA or icache fetches that do not need to also be cached in this cache.
</p>
<p>After some more profiling, I rewrote the "hot" part of the memory access code in assembly for some more speed gain. GCC may have come a long way since a decade ago, but it still does not hold a candle up to hand-written assembly. I removed support I had for one and two-channel RAM to simplify the hot path as well. So now you need to populate all four RAM slots for the card to boot. If you populate different RAM sizes, the smallest one will dictate the final usable RAM size. The usable RAM size will always be four times the size of the smallest RAM chip. This isn't a big deal, the DECstation came with 4MB of RAM, and could be outfitted with a maximum of 24MB. This card can be outfitted with 32MB, so you'll be living like a king! That being said, due to the size of the Linux kernel, you're not going to get a successfull Linux boot unless you have at least 6MB of RAM<s>, and uMIPS will refuse to boot if that is the case (eg: if you populate 4x 1MB chip)</s>.
</p>
<h3>Dirty hacks specifically for Linux</h3>
<p><s>Remember how on MIPS the operating system must do its own pagetable walking and filling of the TLB? As you can imagine this happens often. Very often. How could I speed this up without causing any correctness issues? On taking the <span>TLB refill</span> exception, I verify the handler has not changed and matches the expected bytes, if so, I do what it would have done, but in native code, not emulated MIPS. This helps this particular code run quite a bit faster. Correctness is not compromised since this is only done if the handler matches what is expected, byte for byte.</s>
</p>
<p>I also mentioned that due to how delay slots work, if a CPU takes an exception on an instruction in the delay slot, the kernel must be able to completely emulate that instruction, or in other way execute it and then jump to the right place? Linux uses the fact that MIPS has no PC-relative instructions, except jumps, and it is illegal to place a jump in the delay slot. How? Instead of emulating the delay-slot instruction, Linux copies it out to a special page in memory, where it is followed by a trap. Linux then jumps there in user mode to let it execute, catches the trap, and then re-directs execution where it should go. Now, if this sounds like a giant hassle to you, you are right. What can we do? Well, if an instruction in a delay slot causes an actual exception (like an illegal access, or a TLB refill exception, or some such thing), not much can be done. But what we <em>CAN</em> do is not make things worse. uMIPS will not deliver IRQs before executing an instruction in the delay slot of a branch. At worst, this will delay an IRQ by a cycle, which makes no difference to correctness. The benefit is that this sort of instruction copying and juggling can be done less.
</p>

<h2>How to build and use one</h2>
<h3>Building</h3>

<p><a href="http://dmitry.gr/images/linuxCardPcb.jpg"><img src="http://dmitry.gr/images/linuxCardPcb.jpg" alt="Linux card PCBs"></a></p><p>Now, why you really came here. How do you get one? Well, you could try knowing me personally and asking for my business card, I have a few to give out, but other than that, here is how to do it.
</p>
<p>You'll need to order the board from a board fabrication place. I am a fan of <a href="https://jlcpcb.com/">JLPCB</a> and recommend them. The gerber files I provide come in two flavours. One as you see my card exactly, and one without my name and contact info :). This is a four-layer board, the board house will ask you for layer order, it is: GTL, G1, G2, GBL. At least JLPCB has options to also cover the edge connector in gold for better contact, called "gold fingers" and to grind the board edge to 45° for easier insertion. I suggest selecting both of these options - they are free. Remember to set the board thickness to 0.8mm.
</p>
<p>While you wait for the board to arrive, you'll want to order the parts. You'll need four of the same memory chip (I have the links above), an ATSAMDA1E16, an AMPHENOL 11400841 SD card slot, and a MIC5317-3.3YM5TR regulator. You'll also want to (optionally) order an 0603 sized blue or white LED for SD activity light. If you choose to have that LED, you'll also need a 430 ohm resistor in 0603 or 0805 size. Besides that, you'll need in 0603 or 0805 sizes: 2x 5.1Kohm resistor, 1x 1Kohm resistor, 3x 0.1uF capacitor, and 7x 1.0uF capacitor. You will also need an SD card and any SWD programmer capable of programming the ATSAMD chip. There are many out there. Pick your favourite.
</p>
<p>You'll need an SD card as well. 128MB is the bare minimum here if you want to fit the busybox-based rootfs in. To fit the debian or hybrid image I am providing, you'll want at least 512MB. You can write the image to the card using your favourite tool for that. On Linux and MacOS that is probably <span>dd</span>, on windows, <span>Win32DiskImager</span>.
</p>
<p>Once you've assembled the board, program the MCU with the provided binary <span>software/emu/uMIPS.bin</span> and you're done!
</p>
<h3>Building from source</h3>
<p>You'll want to build a few things. You'll need both an ARM (CodeSourcery) and a MIPS GCC toolchain (I used mips-mti-linux I found online). First, build "romboot", "mbrboot", and "loader". Then, build the kernel. I provided the config, patches, etc. Then you'll want to build the emulator. To build for the MCU, use <s><span>make CPU=atsamd21</span></s>(<b>UPDATE</b>: proper target name changed, see updates later in the article). To build for PC, try <span>make CPU=pc</span>. Then you can build the SD card image. You'll want to copy the MBR from one of mine and modify it, then use <span>mkdisk.sh</span> to embed your kernel, mbrboot, and loader. Use a loopback mount to copy in your rootfs.
</p>
<p>If you want to run the emulator on PC, there are a few things to note. First of all, Ctrl^C will kill it :). Second, unlike the MCU version, the PC version does not incorporate the rom loader in the binary, so you'll need to provide a pointer to it on the command line. A typical command line is <span>./uMIPS ../romboot/loader.bin ../disk.wheezy</span>
</p>

<h3>If you are lazy</h3>
<p>For the lazy ones I am trialing selling all the parts and the board together as a kit on tindie. I'll see how this goes. My suspicion is that it'll end up being a giant pain in my ass and not worth the time, but I am giving it a fair shot. <b>EDIT:</b> Apparently not, and not even with a good reason. I quote: <em>Please resubmit for admin approval once you have addressed: Other Reason.</em>. LOL, how about <em>NO</em>? As a sidenote, if anyone knows companies that do this sort of thing for me (sell a kit I designed), please drop me a line <a href="mailto:tips@dmitry.gr">by email</a>. If you are <em>really really</em> lazy, I might consider having a batch of these factory-assembled by JLPCB as well. If you are interested, click <a href="mailto:assembled_requests_linuxcard@dmitry.gr">here</a> and let me know. No promises yet.
</p>
<h3>Using</h3>
<p>I provide a few disk images. The smallest is the busybox-based one (disk.busybox) - it is small, fast, and cool. I built the busybox from source for MIPS-I with as many applets enabled as I could imagine being needed. The second image is a full debian wheezy (last version to support MIPS-I) rootfs. I should warn you that debian's "init" starts around 3000 processes while it boots, so that takes a long time. If you are using the debian disk image (disk.wheezy), I strongly suggest to just mount proc and sys, and do your things in "sh" without running "init", but it will work if you do ... eventually. I also provide a hybrid image (disk.hybrid). It has a busybox shell and init, but has all of the debian binaries, so things not provided by busybox are still there and work, like gcc and vim. This is the "hybrid" image.
</p>
<p>Using the LinuxCard is easy, insert the SD card, connect USB-C to a computer, and open your favourite serial console app (minicom, PuTTY, etc), if you do not see the boot log, try the other virtual serial port (two exist). In case of a boot error, the SD card LED will blink in an infinite pattern, you can see the code for details on what various numbers of blinks mean.
</p>
<p>Once you see the shell prompt, you can play around, or continue boot to login by typing <span>exec init</span>. After this you'll be able to login as "root" with the password of <s>"mips"</s> "mipsmips". There will also be a login prompt on the second serial port as well. So cool!
</p>


<h2>Version 2</h2>


<h3>Booting Ultrix</h3>
<h4>About Ultrix</h4>
<p>Ultrix is the period-correct UNIX for the DECstation2100/3100. The latest version is 4.5 and with some google-fu you can find ISOs of the install media. It supports the DECstation2100/3100 perfectly, and even has an X11-based UI! The goal of the v2 firmware was to properly run Ultrix on the card. This ended up requiring a lot of work. I had to improve emulation accuracy and implement more hardware. But it did work!
</p>
<h4>First time booting Ultrix</h4>
<p>My first attempts were simple - copy the kernel to my "boot" partition and attempt to load it. It would, of course, not find its root filesystem and panic, but I wanted to see how far I would get at all. The first roadblock was an obvious one - the kernel is not in the <span>ELF</span> format that the linux kernel uses and my loader expects. It is in an older format called <span>COFF</span>. I dug up docs and started working on a parser for <span>COFF</span>. After a little work, I was able to load the kernel and let it run, just to see how far it would go. To my susprise, it got far enough to log some messages to the console! It crashed soon after, when it asked my PROM code for an env variable that I did not know about "scsiid0". Not a bad start. At this point I figured that in a week or so I would have Ultrix booting. It took a little longer...
</p>
<p>Ultrix was designed for this machine, and it was designed to support all parts of it. It does not probe for hardware since it knows that a DECstation2100/3100 should have. It assumes that the requisite hardware is there and starts initializing it. This was a problem for me - I still was not emulating the graphics, SCSI, or the network card. Linux has no support for them so I had not bothered.
</p>
<h4>SCSI</h4>
<p>As this was my first time attempting to emulate SCSI, it took a while. SCSI is so over-engineered, the very word "overengineered" does not do justice to just how much so it it. There are messages, commands, statusses, selects and reselects, and oh so very much more. The SCSI chip in the DECstation2100/3100 is a very strange one that DEC designed just for this device. It is called SII or SMII and I found no docs for it other than the official summary in the <a href="http://www.bitsavers.org/pdf/dec/mips/DS3100_Functional_Specification_Rev3.1_Aug1990.pdf">DECstation3100 specification</a>. It was helpful, as it listed the register bits and values. It was a start. Watching the Ultrix kernel try to access it before it gave up and paniced provided some more help, and reading the SCSI-I and SCSI-II specs filled in the rest. After much work it seemed like the kernel was happy enough to try to enumerate the bus. It would try to select each device in order. Progress!
</p>
<p>From there, the next step was to write a virtual SCSI disk. If you haven't dealt with SCSI before, it is rather unlike most sane designs. A sane design would have a host controller be a heavy/expensive/complex machine that talks to cheap simple devices. This makes sense because typically one would have more devices than host controllers. Not here. A SCSI device drives the bus and determines what it does and when. The only thing the host can do is reqest attention from the device. This took a little while to wrap my head around as it is rather backwards. It is actually even more complex since the target device can disconnect from the bus to do things and later reconnect and continue a transaction. It <em>really</em> is quite complex. Luckily, some of that is optional. A device can also reply without disconnecting, and my virtual disk does that. With a lot of work, I was able to figure out the proper state machinery to make Ultrix indeed identify and talk to my virtual SCSI disk. I split the code into two layers. The bottom handles the basics of just being a SCSI device and the top handles actual disk-specific things.</p>
<p>The code later got expanded to support emulating a CDROM too, to allow me to do an Ultrix install from a virtual CDROM. While working on this, I noticed that the bus enumeration is slowing down the boot a lot. The issue is that there is no way to detect that "no device with this ID exists on the bus". One must attempt a select, and then wait for a timeout. This was taking a while since Ultrix implemented a timeout using a loop with a counter (not using the RTC), and at my virtal CPU speed it was taking seconds. The solution was a dummy SCSI device that does reply to some commands enough to be identified and tell the host that it has no media and is of an unknown type. This device is the "SCSI nothing".
</p>
<p>The SII controller has 128KB of SRAM for DMA-ing data to/from devices. The idea is that one schedules the transfer and it goes on at its pace, when done, an interrupt occurs and data can be copied in/out of this memory. On the PC, this is simple - i can allocate 128KB of RAM and be done with it. On the microcontroller, I do not have that much SRAM, so I steal some memory from my external memory for this, and present less than the full amount to the virtual OS. This works fine for Ultrix as it probes the memory amount page-by-page. Linux probes in 4MB increments, but I have a patch <span>allow_64K_memory_multiples.patch</span> that changes it to probe in smaller increments so that this memory stealing does not cost 4MB of usable RAM.
</p>
<p>Linux has no support for SII SCSI controller, so it continues to use the <span>pvd</span> device.
</p>
<h4>LANCE</h4>
<p>The network card in the DECstation2100/3100 is LANCE. It is somewhat documented in the DECstation2100/3100 specification sheet and I implemented it enought to please Ultrix. It never sends or receives any packets (I can add that later), but it does initialize and interrupt as needed. LANCE has a 64KB SRAM buffer for packets. The PC build of uMIPS fully supports this, the "micro" build of uMIPS will just ignore writes and produce zero reads of this area to avoid wasting 64KB of memory. This works well enough to please Ultrix. Linux has no support for LANCE, so I have no idea if it would be ok with this setup.
</p>
<h4>ESAR</h4>
<p>The MAC address for the network card is stored in a on-board EPROM called the "ESAR" (Ethernet Station AddRess). It lives at the same address as the real time clock, except it is wired to the upper byte of every word, while the DS1287 is wired to the bottom byte. This is a weird thing to do but it works. It does mean that some weird things are possible, like reading both the ESAR and the real time clock registers at once with one read. Luckily this is not usualy done. The ESAR data has some checksums and redundancy (so that its correctness is easy to verify). I implemented an ESAR for uMIPS, assigned the ethernet address <span>66:44:22:44:66:22</span> to the device, and provided for all the required redundancy and checksums. Ultrix is satisfied with this.
</p>
<h4>Memory probing &amp; proper PROM API</h4>
<p>While booting Ultrix I notied that it directly probed the amount of RAM in the system. This is strange since Linux simply queried the memory amount from a PROM API that conveniently exists for this. This was actually my mistake since I was emulating a much newer PROM iterface than the real DECstation2100/3100 had, and Linux was happy to use it. The newer standard (called REX) provides the OS a function pointer table with a lot of API. To signal REX support, a magic value is also passed. DECstation2100/3100 predate the REX API and used a different method of providing API to the OS - a table of jumps is placed at known offsets from the start of the PROM in the <span>0xbfcXXXXX</span> address space. This API is also more primitive, and lacks, for example, the ability to tell the OS how much RAM there is. The pieces now fall into place... My only problem is that I do not have an ability to have a huge PROM, as I wrote earlier. I needed another method to offer this API. I decided to indeed have this jump table, but redirect all the jumps to an address in the RAM area reserved for the PROM <span>0x80001000..0x8002ffff</span>. You'll recall that my OS loader loads there. Now it can provide this PROM API, just like it did the REX API. Cool! Testing Linux also shows that it happily uses this API properly as well. It is, of course, now also forced to <em>probe</em> the RAM amount. No big deal. I did find a bona fide bug in the kernel here! While it means (as per comments) to probe for a maximum of 480MB of RAM, but actually only probes for up to 30. The fix is in <span>fix_mem_limit.patch</span>.
</p>
<h4>Ultrix Loader</h4>
<p>At this point, the kernel was loading far enough to panic about not finding the root filesystem, so it was time to figure out a good way to make this work. The problem is that Ultrix uses a completely different partitioning system than the well-familiar MBR I had been using. The Ultrix "disklabel" allows for 8 "partitions" but with some assumptions, like that the first (caled "a") is always the rootfs, the second (called "b") is always swap, the third ("c") always covers the entire disk (yes it does and is expected to overlap others), and another one ("g") is <span>/usr</span>. Now, if this was not fun enough yet, the partition table itself is expected to be <em>inside</em> the rootfs partition, and a whole lot of tools (including the installer) assume that this all starts at sector zero. Fun, eh?
</p>
<p>I spent a lot of time trying to figure out how to make the installer be happy to not start the rootfs at the 0th sector, but this was a lost cause. A large number of scripts involved assume that both the "a" and the "c" partition start at zero. The kernel also has similar assumptions. With some patching, I got it to work with an offset, but this was not a good approach. I decided to see if I could live with how Ultrix does things, instead of trying to force it to do things my way. Even though the rootfs and the partition table both start at the 0th sector, they both reserve some space up front for "boot code". Specifically, the first 16 sectors (8KB) are always free. I decided to simply place my loader there and teach it how to understand the Ultrix disklabel. As part of this work, I refactored the loader into a few pieces. One part was a partition table handler. There is an option for MBR, one for Ultrix, and one for NetBSD disk labels. One of these (build-time determined) is linked in to the loader, as needed. Another module was a binary loader. Two exist: ELF for Linux and NetBSD, and COFF for Ultrix. Same as before, only one is linked into the loader, as needed. The third modue is the filesystem driver. There is one for FAT12/16/32 (used for my Linux boot sequence), one for old UFS (for Ultrix), and one for modern UFS (for NetBSD). Again, just one is linked in, as needed.
</p>
<p>The cool part now is that I can mix and match these pieces as needed to create a loader for the OS I want to boot. The Linux loader is thus <span>FAT + ELF + MBR</span>, for Ultrix, the loader is <span>UFS.old + COFF + Ultrix disklabel</span>, and for NetBSD, it is <span>UFS.new + ELF + NetBSD disklabel</span>. I was too lazy to implement proper CD-booting, so installing Ultrix is a bit weird. I make a disk image with just the installer kernel (extracted from the CD), in a FAT partition, attach the CDROM to the emulator, and then boot. The installer will then re-partition the disk. For this, yet another loader combination is used: <span>FAT + COFF + MBR</span>. The modularity pays for itself!
</p>
<h3>Making Ultrix work</h3>
<p><a href="http://dmitry.gr/images/linuxCardUltrixUiBig.png"><img src="http://dmitry.gr/images/linuxCardUltrixUiSmall.jpg" alt="Ultrix UI fully booted with a graphical paint program and a terminal open"></a>
<a name="_TOC_42badd9e49002a3cefeaaf28867add83"></a></p><h4>Framebuffer</h4>
<p>Once I had the Ultrix kernel booting properly, at least in the PC build of uMIPS, I <em>really</em> wanted to get the GUI working. Who wouldn't‽ The framebuffer came in two varieties for this machine. There was a monochrome one and a 8-bit color one. They both supported hardware cursor as well. I implemented most of the normally-used modes in the cursor hardware, but not any test modes. I emulated both the framebuffer types and they both work! The 8-bit framebuffer can display up to 259 colors onscreen at a time, out of a 24-bit palette. That is not a typo. The display itself can display 256 colors, and the cursor has its own 3-entry palette, which need not use any of the same colors. The resolution is 1024x1024 in memory, and 1024x864 onscreen. The remaining memory is free for the OS to use however it wishes. I steal memory from the main RAM, same as for the SII buffer. 128KB is used for the mono framebuffer, and a whole megabyte for the color one. The palette is also stored in stolen ram (just about a kilobyte).
</p>
<h4>Mouse, Keyboard, ... and Tablet</h4>
<p>Of course, to make this work, I also had to make the keyboard and mouse work. They talk to the DECstation via serial, and the protocol is somewhat known, from various shreds available online. I was able to put together a passable keyboard emulator rather quickly. It is <em>not</em> a dumb keyboard. It has regions of keys, a bell, some lights, and can support differing autorepeat settings per key group. It is actually pretty cool. The mouse is a pretty basic one, with three buttons. I got that working rather quickly. The problem with emulating mice is a well known one - they are relative device, and most OSs apply acceleration to the mouse as you keep moving it to allow for better reach. Now, if you are running another OS, and passing these accelerated movements to it, it will re-accelerate them even more. This ends up being a mess. This is why most virtualization solutions prefer to load an absolute-pointing-device driver into the guest. I was not prepared to hack up Ultrix or find a way to load a different mouse driver in it. But then I noticed that DEC wrote about a "graphical tablet" that they were selling, that hooked up to the mouse port. Could it be that Ultrix supports this? Yup... Ultrix does. I wrote an emulator for the tablet and it worked wondefully - no more over-accelerated mouse for me! Sweet!
</p>
<h4>Patches</h4>
<p>Ultrix assumes that it is booting on a real DECstation2100/3100, and that includes expecting the CPU to have caches. My virtual CPU does not expose caches to the guest OS, and while Linux handles that fine, Ultrix does not. It correctly probes the cache and finds its size as zero. But there is a logic bug in <span>r3_kn01flush_cache</span>, where if the cache size is zero, it gets into an almost-infinite loop. As uMIPS exposes no cache, it makes sense to patch the function away into just a return. There is another function of interest: <span>kn01delay</span>. It is used for short busy-wait delays when dealing with hardware. All of our virtual hardware is instant-fast, and thus no delays are needed. As long as I am patching a kernel, might as well make it faster. There is also a third area of interest - the periodic timer. In Linux, I was able to change the tick to 16Hz, but I cannot build Ultrix from source, so I cannot modify it easily. Ultrix uses a 256Hz tick. At that rate, on uMIPS hardware we'd never get any useful work done while only handling interrupts. I attempted to patch Ultrix to use a 16Hz timer and account for it correctly. This does not work - there are mathematical errors that happen. 64Hz works, but that is still too freqent for the uMIPS hardware to be usefully fast. I ended up patching the init code to set the timer to 16Hz, but accounting code to act like it is 64Hz. This means that "realtime" in Ultrix runs 4x slower than actual real time, but this is not really a big deal. Just keep in mind that a <span>sleep 1</span> will delay 4 seconds and not 1.
</p>
<p>So how does one even apply such patches? How does one find the proper places to patch? I spent a <em>LOT</em> of time learning about the barely-documented symbol format used in the Ultrix kernel. It worked! I made a working parser for it and was able to properly identify the symbols I needed and to patch the places that needed patching. This was good until I realized that while the installer kernel does ship with symbols, the kernel installed for first boot does not (after first boot, the kernel is recompiled again, with options you choose, and that version DOES have symbols). No symbols means that I cannot use them to find the proper locations to patch. I decided on a different method - binary matching. Look for the proper set of bytes in a row, it should be unique in the kernel. If you find just one case - it's the right one. To save space in the loader (as it is limited to 8KB), I compress the "pattern to look for" cleverly. Cool. This is the final approach I used and you can see it in <span>loadUltrix.c</span>.
</p>
<h3>Improvements in the emulator</h3>
<h4>USB improvements</h4>
<p>After a lot of googling, I learned about interface association descriptors. Turns out that without them, windows will not load the USB CDC-ACM drivers for a device. After adding them, Windows would properly load the driver and it would show up as a COM port. I also learned about the peculiar ways that Windows enumerates devices. Sometimes it'll ask for a descriptor, stating that it'll accept 64 bytes, but after receiving just one 8-byte packet it will reset the bus. This was breaking my USB code, and this is now fixed. Windows now properly supports uMIPS and shows it as two COM ports. Sweet!
</p>
<h4>More perf improvements</h4>
<p>At the end of emulating every instruction, the emulator jumps "to the top", fetching a new instruction to execute. In most cases before this a check is done for whether there is an interrupt pending. This jump was done using a <span>BL</span> - the only long-distance branch available on the Cortex-M0. It takes 3 cycles. The check involved loading a byte from memory (2 cycles), checking if it is zero (1 cycle), and jumping to the interrupt exception creation code if so (1 cycle if not - the common case). That means that the entire "jump and begin handling the next instruction" step took 6 cycles. I wanted to make it faster somehow. I decided that if I could free up a register, I could. Some reworking freed <span>r11</span>. There is a parameter you can pass to gcc to tell it to not use a given register in any C code it compiles: <span>--ffixed-r11</span>. Now that this register is not being used by anyone ever, we can do the clever thing. We keep the address of the "load next instruction and execute it" label in it. Now we can jump to it using just <span>bx r11</span>. This takes just 2 cycles - 4 cycles saved per virtual instruction - a significant speed up. But what if we do have a virtual interrupt to report? Whenever we have one to deliver, we just set <span>r11</span> to point to the "report a virtual interrupt" label, and whenever the current virtual instruction is done being emulated, the interrupt will be reported and <span>r11</span> will be reset. There is a bit more machinery needed to make this work, but this is it in general terms, and it does work!
</p>
<p>I also changed how the TLB hash works (from a table of 32-bit pointers to a table of 8-bit indices) to make the table and each entry smaller (from 24 bytes to 16). This saved a bit under a kilobyte of RAM, which I was able to allocate to the L2 cache. It has now grown from 1.25KB to a full 2KB for a measurable perf improvement!
</p>
<h4>Removing the TLB refill fast path</h4>
<p>For Linux, I had implemented a fast-path for the TLB refill code - it executed in native code what he TLB refill handler would do. In my measurements it slightly improved performance. With all the other performance improvements I had implemented, it no longer offered a measurable improvement. Plus, it did not help Ultrix at all, by definition. Removing it saved flash space and removed complexity. Less complexity is always better. It is gone.
</p>
<h4>Cache geometry changes</h4>
<p>Previously, when profiling to find the best L1i geometry, I used the Linux boot process. I decided to try harder. Now I profiled that, gcc compiling some code, a few other Linux binaries, Ultrix boot, and some Ultrix userspace utilities. The result of this investigation was that a direct-mapped L1i is slightly faster than a 2-way L1i cache. The hit rate goes down slightly, but checking only one cache line instead of two speeds up the checking enough to make up for it. I thus reconfigured the cache as a direct-mapped cache.
</p>
<h4>Serial improvements</h4>
<p>Previously, the emulator would wait a fixed 20ms to send a character to the PC before giving up. I changed this to a permanent wait for the main console. This allows the user to not miss any output if they close their terminal. The emulator also shows its version up front, since it will definitely not be missed now. As of firmware v2.1.1, uMIPS also shows the RAM configuration in terms of the number of chips, each chip's size, and the bit width of the per-chip interface.
</p>
<h3>More Floating Point Unit work</h3>
<p>I had already implemented a full virtual FPU, but now I wanted to see how necessary it really was. I knew that Linux would run if I emulated no FPU at all and would emulate it. I wanted to see if Ultrix would. It did not - it crashed with an invalid instruction trap in the kernel. This was not all that surprising. Once again, it was compiled for a particular machine - a machine that had an FPU. Its assumption that an FPU exists was sane. But there was still more to investigate. The MIPS spec says that the FPU may refuse to execute any instruction if it is not sure that it can perform it perfectly accurately. Since the spec is not clear on what that really means, basically any OS running on such a MIPS chip must implement a complete FPU fallback, capable of emulating any FPU instruction. But then why am I hitting an exception?
</p>
<p>The trick is that the FPU must still exist, it must refuse to do math. This is strictly different from not existing at all. I thus implemented a "minimal" FPU. It implements the instructions to identify itself, move data in and out of the floating point registers, and load and store floating point registers to memory. Any attempts to do actual floating point math report a "coprocessor usage exception" which is the proper way for the FPU to refuse to do math. This worked correctly for Ultrix - it now will not crash at boot, all applications that do floating point math still run, with the kernel emulating the math. I checked and Linux also supports this setup. Thus uMIPS now has three FPU configs that it can be built with: <span>full</span>, <span>minimal</span>, and <span>none</span>.
</p>
<h3>A bootloader</h3>
<p>As I handed out more and more of these cards, the update story needed to be improved. Not everyone has a <a href="https://cortexprog.com/">CortexProg</a> lying around to reflash the firmare. I decided to make it simple and require as little user interaction as possible. The bootloader is just under 3K, I allocated 4K of flash to it, and relocated the main firmware to start 4K into the flash. So, how does it work? At boot, the bootloader will minimally initialize the SD card, attempt to find a FAT16 partition on it, see if it contains a properly-sized file called <span>FIRMWARE.BIN</span> on it, and if so, the firmware will be flashed from this file. On error, the error number will be blinked out on the LED, repeatedly. On success, a varying-frequency pattern of the LED will be repeated forever.
</p>
<p>If the card fails to be initialized, if it fails to mount, if the update file does not exist, or if it is not correctly sized, the bootloader will continue to boot the existing firmware, if any exists (some sanity checking is peformed). This means that when you insert a card with my Linux image or the Ultrix image, all will work as expected. Only FAT16 is supported, so some partitioning may be required on larger cards. I can live with that.
</p>

<h3>Hardware improvements</h3>
<h4>v1.3 hardware</h4>
<p><a href="http://dmitry.gr/images/linuxCardSchem2.png"><img src="http://dmitry.gr/images/linuxCardSchemSmall2.jpg" alt="Linux card schematics"></a>
<a href="http://dmitry.gr/images/linuxCardBoard2.png"><img src="http://dmitry.gr/images/linuxCardBoardSmall2.jpg" alt="Linux card board layout"></a></p><p>After reading my original article, a few people wrote in (including in the comments section here, on twitter, and in email) to suggest that maybe I should entirely abandon the shitty SPI units in this chip. Initially I was worried that the SPI unit speed issue was really an IO port speed issue, but a quick test showed that I could toggle a pin at half my CPU clock reliably and get nice square edges. I prototyped bit-banging SPI on the existing board to see what speeds I could attain and it was promising. I then laid out a new board, with different wiring, to allow me to actually use QSPI mode. The images for the new schematics and the layouts are the ones you see here!
</p>
<p>The ATSAMD21 series features a single-cycle IO port. This optional Cortex-M0+ feature is pretty useful for bit-banging. It really is single-cycle-fast. Normal loads and stores take two cycles minimum on a Cortex-M0+, but ones targetting this kind of a unit take just one. That is <em>how</em> I could toggle a pin at half the cpu speed for my test that I had just mentioned.
</p>
<p>With big-banging, the trick is to do as few operations per cycle as possible. Given this, it would be ideal to do minimal bit-twiddling. It would be super-awesome if I could wire up the four QSPI chips to GPIOS numbered 0..15, allowing me to just read/write the bottom 16 bits of the GPIO port for simple access. Alas, this was not meant to be. This chip has no contiguous 16 GPIO pins wired to the physical pins, so I settled for wiring RAM0 to GPIO0..3, RAM1 to GPIO4..7, RAM2 to GPIO8..11, and RAM3 to GPIO14..17. Since I will be driving them all together, the clock and chip select lines are all wired together. After all was said and done, after the assembly was coded, and the dust settled, I was able to get around a 9MHz clock speed on average. Since the command and address are also sent 4-bits-wide, the speed increase is nice. Previously (using hardware SPI) it took around 8 microseconds to read/write 32 bytes, now it took just under 4 microseconds. A nice speedup.
</p>
<p>An astute reader might notice that the first three RAMS <em>ARE</em> on consecutive GPIO pins. Three is not of much use to us, as it is not a power of two, but <em>two</em>... Yes indeed using only two RAMs i can attain faster speeds (but at half the width). The actual time to read/write 32 bytes is around 5 microseconds. Given this, I decided to re-add the previously-removed support for using less than 4 RAMs on the board. And I did. The newest firmware now supports 1, 2, or 4 RAMs populated on the new boards. I then went futher, and re-added this support for the old boards. That is not as well optimized - it is in C, not ASM, but good enough to play with. This will allow assembling these boards cheaper. Plus, Ultrix happily boots and runs in 4MB (it does need 5MB to start the GUI though).
</p>
<h4>And old hardware too</h4>
<p>I did not want to maintain two separate-but-almost-equal branches of code for the older v1.2 hardware and the new v1.3 hardware. There was also no easy way to tell them apart in software from first glance. But a bit more investigation does provide an idea. The wiring for the RAMs is different enough that we can try each way and see if we detect a plausible RAM chip. It helps that not having RAM0 populated is never supported. This is precisely what I did, in fact. I tried both configs and see which produces a valid-looking ID from RAM0. From there, all four RAMs are probed, identified, and a configuration is picked.
</p>
<p>Support for less than 4 populated RAMs raises a few interesting questions. For speed, all RAMs are treated as if they are the same size, so the size of the smallest RAM determines the total amount of available RAM. This is because I stripe the data across them, of course. So, what if RAM0 is populated with 8MB, and RAM1 with 2MB? We could use just RAM0 and get 8MB of RAM or we could use both and get just 4MB, but faster, since more RAMs in parallel is always faster. I decided that more RAM is better than faster RAM, so in case of such conflicts, more RAM is always chosen. When there is a tie, the faster configuration is used, eg: 4MB, 1MB, 1MB, 1MB RAMs populated add up to 4MB in both the x1 and x4 configs. In this case the x4 config will be chosen and all the RAMs will be used.
</p>
<h3>Building from source (updated)</h3>
<h4>The emulator</h4>
<p>A new parameter called <span>FPU</span> is now passed to uMIPS build to specify the FPU type desired. Options are: <span>none</span> - no FPU at all, Ultrix will not like this but it makes the smallest image; <span>minimal</span> - an FPU that can store values but refuses to do math - Ultrix and Linux will support this, it is slightly larger; and <span>full</span> - a full FPU that does all the math - the fastest option that bloats the Cortex-M0 image by 17KB or so.
</p>
<h4>The loader</h4>
<p>To build the proper loader, pass the <span>BUILD</span> parameter to <span>make</span>. The options are <span>linux</span>, <span>ultrix</span>, <span>ultrix_install</span>, or <span>netbsd</span>. The install loader is just for clean installs, which you have no reason to do since I already did it for you. The netbsd one is to attempt boots of NetBSD on this machine, as it is supported by NetBSD. The proper loader needs to be built and integrated into a disk image for a working system.
</p>
<p>The integration step also changed, <span>mkdisk.sh</span> is gone, replaced by a number of different tools, depending on the intended system. They are: <span>mkdisk-linux.sh</span>, <span>mkdisk-netbsd.sh</span>, <span>mkdisk-unix.sh</span>, and <span>mkdisk-unixinstall.sh</span>. Unix here refers to Ultrix, of course. The scripts are small and self explanatory. Open them for more details. They all operate on a disk image called "disk"
</p>
<p>To enable GUI in Ultrix, the env variable "console" needs to be properly set. In <span>loader.c</span>, find it and set it to "0,0" for text mode or "1,0" for console mode.
</p>
<h3>Further Updates</h3>
<h4>Firmware v2.1.1</h4>
<p>In this version, BBQSPI memory access sped up by 11% for 4-chip case, 6% for others. Ram config shown on boot.
</p>
<h4>Firmware v2.2.0</h4>
<p>In this version, the bootloader was updated to better support other ATSAMD21 parts, including those with more flash &amp; RAM. It now also exposes a version byte at offset <span>0x08</span>. The previous bootloader was version <span>0x10</span>, making this one version <span>0x11</span>. The version will be shows on the serial console at boot now.
</p>
<p>Also, as ATSAMDA1E16 is now apparently out of stock everywhere, I added support for <a href="https://octopart.com/atsamd21e17a-au-microchip-77761547?r=sp">ATSAMD21E17A-AU</a>/<a href="https://octopart.com/atsamd21e17a-aut-microchip-77761548?r=sp">ATSAMD21E17A-AUT</a>. The sad news is that this non-automotive part does not overclock nearly as well. It gets unstable much past 76MHz, so I decided to clock it at 72MHz. It does have more RAM (16KB), which allowed me to allocate a lot more memory to L1i and L2 caches. In most measurements, the performance loss due to lower speed is papered over by the gains of larger caches.
</p>
<p>On the topic of performance, I also rewrote the L2 cache code in assembly for speed and size gains. The speed gains are significant. For extra speed, there is now an option to move the actual access functions to RAM (which is faster than flash). This gains an extra 8% speed, but at the cost of using RAM. On the old 8KB-of-RAM parts this is not always worth it, since it necessitates shrinking the L2 from 2KB to 1.625KB to make space. On the new 16KB-of-RAM parts, though, it is we;ll worth it. It should be noted that there are 6 variants of RAM access low level functions as there are 2 possible access types (SERCOM or bit-banged) and 3 possible chip counts (1, 2, or 4). Only the ones you plan to use need to be moved to RAM. Others will still work from flash, if you want to make a universal firmware. The firmware I provide now moves the 4-chip bit-banged functions to RAM for ATSAMD21E17. See <span>RAM_FUNCS_IN_RAM</span> in the Makefile and the contents of <span>spiRamAtsamd21.c</span>
</p>
<p>While moving functions to RAM, it is easy to accidentally use too much RAM and end up with random crashes as stack collides with data. These are a pain to debug, so I decided to improve this experience. As an option in the Makefile, there is now ability to enable <span>STACKGUARD</span>. What does this do? As the very last word in the pre-allocated RAM (and thus the very first one that stack would overflow over) the code will keep a magic cookie, whose value depends on the current <span>ticks.hi</span> value. This value is checked and updated in the <span>SysTick</span> interrupt which happens every 16 million cycles. If the check fails, an error will be blinked out the LED and the execution will be halted. 
</p>
<p>Starting with this version, the proper make incantations are now <span>make CPU=atsamda1e16</span> and <span>make CPU=atsamd21e17</span>
</p>
<p>The downloads have been updated with the new code and binaries for both chip types. They can be updated using the bootloader and an SD card
</p>





<h2>In conclusion</h2>
<h3>Acknowledgements</h3>
<p>I send a great many thanks to my cats for cutely lying under my table to keep me company during the many hours spent on this project. I send a giant, Mount Rushmore-sized middle finger to Atmel for this sorry excuse of a chip. <b>UPDATE:</b> I even gave up on using the SPI units here, so at this point, I send <em>two</em> of those fingers. Thanks for nothing, Atmel.
</p>
<h3>Downloads</h3>
<p>The source code, gerbers, schematics, and all else <em>except</em> the disk images can be downloaded [<a href="http://dmitry.gr/images/LinuxCard2.7z?v=220">here</a>]. The license on my code is simple: free for all non-commercial use, including using as your own business card. For commercial use (like if you wish to sell kits of this project), <a href="mailto:licensing@dmitry.gr">contact me</a>.
</p>
<p>The disk images (<b>updated</b>) are large so I have no desire to host them on my site, so: [<a href="https://drive.google.com/file/d/14fhdW4Vdz4-ZKucB-iIP4MLTk8OLB7dI/view?usp=sharing">Google Drive</a>] or [<a href="https://mega.nz/file/p9ZWzLrK#saRKVlgBthOFE4Cp-6sb2fMTM7JXtuXMlsYQaaWrEAI">MEGA</a>].
</p>


<!--- We do not show this to the user, but ToC system will index this and we'll get a link to comments in the ToC -->






					
					</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Be a Thermostat, Not a Thermometer (215 pts)]]></title>
            <link>https://larahogan.me/blog/be-a-thermostat-not-a-thermometer/</link>
            <guid>41516327</guid>
            <pubDate>Wed, 11 Sep 2024 23:50:25 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://larahogan.me/blog/be-a-thermostat-not-a-thermometer/">https://larahogan.me/blog/be-a-thermostat-not-a-thermometer/</a>, See on <a href="https://news.ycombinator.com/item?id=41516327">Hacker News</a></p>
<div id="readability-page-1" class="page"><article>
		
		<p>Originally posted Apr 4, 2023  
			
				• More resources on <a href="https://larahogan.me/resources/communication-team/">communication &amp; team dynamics</a>
			
		</p>

		
			<p>This post originally appeared in my <a href="https://mailchi.mp/wherewithall/be-a-thermostat-not-a-thermometer">newsletter</a>. <a href="http://eepurl.com/dxUybL">Subscribe</a> to receive it!</p>
		

		<p>As I’ve learned more about how humans interact with one another at work, I’ve been repeatedly reminded that we are very easily influenced by the mood of those around us. It’s usually not even something we do consciously; we just see someone using a different tone of voice or shifting their body language, and something deep in our brain notices it.</p>

<p>If you’ve ever attended a meeting where there were some “weird vibes,” you know what I’m talking about. You couldn’t quite put your finger on it, but something about the energy of the room was <em>off</em>—and that feeling affected you, even if it was super subtle.</p>

<p>We’re wired to spidey sense this stuff; this gut instinct is part of what’s helped us stay safe for millenia. Our amygdalas are constantly on the lookout for threats in our environment that could be bad news. Plus, we tend to infer meaning from those weird vibes. Our brain is trying to make sense of the shift in behavior, so we’ll make some (often subconscious) guesses about what’s truly going on. We often even jump to the assumption that those vibes are about us.</p>

<h2 id="humans-mirror-each-other">Humans mirror each other</h2>

<p>If I’m distracted in our one-on-one because I’ve got some stuff happening out of work that you don’t know about, it’s a recipe for misunderstanding. What you might observe is that I’m not making eye contact, I’m suddenly changing the subject, and my arms are crossed. How does your brain make sense of this? It decides that I’m upset with you—without any other information, it’s the most likely reason, of course. :)</p>

<p>Plus, humans, like most other mammals, mirror each other. When I change my tone or my body language, there’s some likelihood that your tone and body language will change in response. So now we’ve got a compounding situation—I’m having a bad day, so I’m giving off strange vibes, then <em>you’re</em> giving off strange vibes because you’re picking up on my bad day. We leave the one-on-one and go meet with other people, and now <em>they’re</em> picking up on our strange vibes.</p>

<p>This cycle is far more noticeable when someone is <a href="https://en.wikipedia.org/wiki/Amygdala_hijack">amygdala-hijacked</a>. It’s tremendously easy to be caught off guard by someone who is overcome with a surprising emotion, and feel triggered by it ourselves. Again, this is just a normal defense mechanism—there is no judgment here.</p>

<h3 id="noticing-a-change-in-someones-behavior">Noticing a change in someone’s behavior</h3>

<p>It takes a lot of practice to recognize when this pattern of shifting and influencing behavior is happening! But once you start paying attention to people’s patterns of behavior (what words do they use when they’re feeling upset? How does their body language change? Do they get louder or quieter? In what situations are they cracking jokes, and in what situations are they more quiet?) you can develop a stronger spidey sense when someone’s “vibes” are different than usual.</p>

<p>In my video course on <a href="https://courses.wherewithall.com/courses/surprising-emotions">Dealing with Surprising Human Emotions</a>, I talk about how to recognize when someone’s behavior seems off, it’s just a signal—just data—that one of their core needs might be being messed with. (Or maybe they simply didn’t get enough sleep last night, or haven’t had coffee yet today!) You can try to see it as a weather vane that something has gone awry for this person. Because once you can transform these signals into data—and not simply mirror the weird vibes back—you have an opportunity to positively affect what happens next.</p>

<h3 id="thermometer-vs-thermostat">Thermometer vs thermostat</h3>
<p>I like to use the metaphor of a thermometer and a thermostat for this idea. If you’re looking for signals about how someone is feeling, it’s kind of like you’re trying to take their emotional temperature. You’re being a thermometer. When they’re subtly giving off weird vibes—they’re frowning, answering your questions with fewer words than normal, etc.—you’ve noticed that their temperature is different. When their amygdala is hijacked, you might see large changes in their behavior (they’re picking a fight with you, going completely silent, skipping your meeting, etc.)—in the thermometer metaphor, they’re running a fever, and you’re picking up on it.</p>

<p>And since we know that one person’s behavior change can cause <em>others</em> to change their behavior in response, we can think of it like they’re being a thermostat: they’re setting the whole temperature for the room. Even if it’s unintentional on both sides. It’s just how we’re wired: to mirror the “vibes” that someone else is giving off.</p>

<p>Rather than let that cycle play out subconsciously, you have an opportunity to become the thermostat as soon as you notice that another person’s temperature has changed. <strong>You</strong> get to set the new temperature of the room, in a positive and healthy way.</p>

<h2 id="being-the-thermostat">Being the thermostat</h2>

<p>Once you’re able to start noticing when someone’s amygdala-hijacked, or simply that the vibes are <em>off</em>, you can reframe and use “be the thermostat, not the thermometer” for good. Since humans tend to mirror each other, you can <strong>intentionally</strong> change the energy in the room, setting the thermostat to a more comfortable temperature.</p>

<h3 id="naming-whats-happening">Naming what’s happening</h3>

<p>One way to reset the temperature is to say out loud, with your mouthwords, that you’ve noticed that the energy has shifted. Here’s a how-to blog post on <a href="https://larahogan.me/blog/skill-naming-whats-happening/">naming what’s happening in the room</a>.</p>

<p>As I mention in that post, there are a few risks to doing this, so you should use your best judgment on whether or not naming what’s happening in the room would be helpful in the moment. You won’t always get it right! Avoid projecting your feelings onto others, or putting them on the defensive, that would make the temperature of the room even more uncomfortable!</p>

<p>If you’re noticing a major shift in someone’s demeanor, instead of guessing what’s going on for them (like “you seem upset”) ask an open question about what they need or how they’re feeling. This way you’ll know if you need to get your thermostat hat on.</p>

<h3 id="choose-your-tone-and-body-language">Choose your tone and body language</h3>

<p>When naming what’s happening or asking open questions, keep what you say short and sweet, and remember to use a calm tone and open body language. I’ve <a href="https://larahogan.me/blog/when-coaching-questions-dont-work/#check-your-tone-and-body-language">written about this before</a>, but it’s definitely worth recapping here, because this is a huge component of being an effective thermostat!</p>

<ol>
  <li>
    <p>Gently <strong>nod</strong> at the pace they’re talking at, or slightly slower. It shows you’re following and tracking what they’re saying.</p>
  </li>
  <li>
    <p>Make <strong>soft eye contact</strong>. Hard eye contact is intense, eyes wide—it’s a little creepy. Soft eye contact is more like a Tyra Banks “smize”—a subtle relaxing of your facial muscles that shows you’re not ready to pounce as soon as they’re done talking. Don’t worry about keeping constant eye contact. Research shows you can break eye contact every 3 seconds naturally, then connect again, and this still feels attentive and affirming to the other person.</p>
  </li>
  <li>
    <p><strong>Lean in</strong>, but not too much. When we’re uncomfortable, we sometimes unconsciously tip away from the person in whatever way we can. This can send a signal that you’re uncomfortable or trying to get out of this conversation ASAP, or even that you are asserting dominance. Make sure you’re squarely facing the person—or if you’re on video, squarely face the camera—and lean in slightly. Even as little as 1” will do the trick! If I’m on Zoom and sitting at my desk, I like to make sure my elbows or wrists are evenly resting on it.</p>
  </li>
  <li>
    <p>Be intentional about the <strong>tone</strong> that you’re using. You’re responsible for communicating that you want to hear what they have to say, and that you’re here to support them. This intentional choice, in combination with your body language cues, will communicate to the other person that you are actively listening. I’ve found that even a subtle change in my tone—like going a little quieter if the other person has gotten a little louder, or adding a little bit of joy to my voice if they seem unsure or a little bit stressed—can reset the temperature in the room.</p>
  </li>
</ol>

<p>There’s a lot more to say about active listening; you can read more in <a href="https://larahogan.me/blog/actively-listening/">this blog post</a>! Your whole goal here is to set or reset the temperature of the room by modeling it with your tone, body language, and word choice.</p>

<p>This skill of intentionally choosing your body language, tone, and words can help the other person move out of whatever “weird vibes” they were giving off earlier, as they can now start mirroring yours. But if it’s a more drastic scenario, like this person is in an amygdala-hijack mode, this approach can also help them feel more heard, understood, and confident that you are decidedly not mad at them.</p>

<p>Usually, this skill does the trick. You smiled a bit, told a little joke that made them chuckle, nodded at the pace that they spoke to indicate you’re listening, and their mood started to change. You’ve just acted as the thermostat in a healthy, intentional way. But in case this doesn’t work, or if this person is in a more lizard-brain state, read on for some additional tools you can try.</p>

<h3 id="offer-a-break">Offer a break</h3>

<p>If it feels like the other person has been amygdala-hijacked, or if they are decidedly stressed or distracted and you sense that there’s no way that the rational, logical part of their brain will be able to return in the next few minutes, use a <a href="https://larahogan.me/blog/get-feedback-from-colleagues/#what-if-you-need-some-processing-time">back-pocket script</a> to offer a pause in the conversation and a plan to return to it later. Some of my favorites to use are:</p>

<ul>
  <li>“I’m not sure how y’all are feeling, but I think I could use some more processing time on this. Could we reconvene again tomorrow at 2pm?”</li>
  <li>“I know how much we want to come to an agreement on this decision today, but my spidey sense is that we might need some more time to think on it. How about we sleep on it and check in again tomorrow?”</li>
  <li>“I really want to support you on this and make sure you feel good about our next steps. How would you feel about us taking a break now to spend some more time thinking it through, and chat again at 4pm?”</li>
</ul>

<p>You’ll notice that the phrasing is intentionally trying to avoid putting someone else on the spot, or make them feel attacked. Your gentleness can help set the new temperature in the room.</p>

<h3 id="what-i-learnedwhat-ill-do">What I learned/What I’ll do</h3>

<p>If you’ve contributed to a big shift in the temperature by creating or escalating an awkward or tense situation, you have an opportunity to own your role as the thermostat here. Because if you never acknowledge it, you’re going to risk developing a forever-antagonistic relationship with them.</p>

<p>Sure, they should just be a grownup and get over it, right? But this does not happen in practice. (When was the last time you, yourself, actually did that?) People hold on to this stuff! Your life is going to be SO MUCH HARDER if you don’t clear the air after you amygdala-hijack someone.</p>

<p>In the <a href="https://courses.wherewithall.com/courses/surprising-emotions">Dealing With Surprising Human Emotions</a> video course, I talk with Jason Wong about this template that we both learned from Paloma Medina :)</p>

<blockquote>
<p>“What I <strong>learned</strong>…”<br>“What I’ll <strong>do</strong>…”</p>
</blockquote>

<p>For example, “What I learned is that that last email didn’t do a good job explaining the changes, so what I plan to do is start a forum for folks to post their questions and our CEO will answer them every Tuesday.”</p>

<p>When said with heartfelt authenticity, this phrase tells people that their needs, feelings, and concerns are not irrelevant. It allows people’s bellies to relax because their needs have been acknowledged. You can begin the work of recovering from the amygdala hijack, because you’ve reset the temperature in the room.</p>

<p>My parents actually taught me to “be the thermostat, not the thermometer”. It’s not always easy, that’s for sure. But by being aware of these cycles, we’re more likely to remember to use our thermostat power for good (not just give up or bail out when you notice someone else is running hot).</p>

<p>Next time you find yourself in a conversation that’s exuding some <em>off</em> vibes, or even an intense one, if you use a combination of these tools, you’ll be giving others the opportunity to mirror the temperature you set right back to you.</p>

	</article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Reader-LM: Small Language Models for Cleaning and Converting HTML to Markdown (147 pts)]]></title>
            <link>https://jina.ai/news/reader-lm-small-language-models-for-cleaning-and-converting-html-to-markdown/?nocache=1</link>
            <guid>41515730</guid>
            <pubDate>Wed, 11 Sep 2024 22:07:36 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://jina.ai/news/reader-lm-small-language-models-for-cleaning-and-converting-html-to-markdown/?nocache=1">https://jina.ai/news/reader-lm-small-language-models-for-cleaning-and-converting-html-to-markdown/?nocache=1</a>, See on <a href="https://news.ycombinator.com/item?id=41515730">Hacker News</a></p>
Couldn't get https://jina.ai/news/reader-lm-small-language-models-for-cleaning-and-converting-html-to-markdown/?nocache=1: Error: timeout of 10000ms exceeded]]></description>
        </item>
        <item>
            <title><![CDATA[Show HN: How much is 13B euros? (115 pts)]]></title>
            <link>https://howmuchis13billioneuros.com</link>
            <guid>41515649</guid>
            <pubDate>Wed, 11 Sep 2024 21:52:32 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://howmuchis13billioneuros.com">https://howmuchis13billioneuros.com</a>, See on <a href="https://news.ycombinator.com/item?id=41515649">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="main">


<p>There's <strong><a href="https://duckduckgo.com/?t=h_&amp;q=apple+tax+ireland&amp;iar=news&amp;ia=news" target="_blank" rel="noopener noreferrer">a lot of chatter</a></strong> in the Irish news about <strong>13 billion euros</strong> or <strong>14 billions euros</strong>.</p>

<p>Below are some <em>rough numbers</em>.</p>
<p>You can check their formulas and assumptions under the tab for '💬' (top left) 🔍</p>
<p>And, <em>if you want to, <strong>you can change them!</strong></em></p>
<br>
<h2><span></span> is...</h2>

<div>
<p>In <span></span> tablet devices❓</p>
<ul>
<li><span></span> tablets per person in Ireland, or <span></span> tablets per primary school pupil 🧒📲</li>
</ul>
<p>In <span></span> one-off gifts❓</p>
<ul>
<li><span></span>x <span></span> gifts for each person 🎁</li>
</ul>
<p>In <span></span> <a href="https://duckduckgo.com/?q=oasis+tickets+croke+park&amp;t=h_&amp;iar=news&amp;ia=news" target="_blank" rel="noopener noreferrer">Oasis tickets</a>❓</p>
<ul>
<li><span></span> tickets each for every person! 💃<observablehq-loading></observablehq-loading><!--:d72e0d28:-->🕺</li>
<li>Oasis might need to play <span></span> gigs together in <a href="https://crokepark.ie/stadium/about" target="_blank" rel="noopener noreferrer">Croke Park</a> at <span></span> capacity, to honor this volume of tickets 🎶🧑‍🤝‍🧑🎶</li>
</ul>
<p>In <span></span> <a href="https://www.irishtimes.com/life-style/2024/09/06/its-not-even-a-shed-a-summary-of-the-leinster-house-bike-shelter-controversy/" target="_blank" rel="noopener noreferrer">bike sheds</a>❓</p>
<ul>
<li><span></span> bike sheds 🚴</li>
<li>Using <span></span>x 🚴 per shed, these can store <span></span> bikes 🚴</li>
</ul>
<p>In <span></span> <a href="https://www.irishtimes.com/health/2024/02/13/national-childrens-hospital-cost-rises-to-over-2bn-donnelly-confirms/" target="_blank" rel="noopener noreferrer">National Childrens Hospitals</a>❓</p>
<ul>
<li><span></span> National Childrens Hospitals <observablehq-loading></observablehq-loading><!--:49c04f94:-->👧🏻</li>
</ul>
<p>In Special Needs Assistants at a salary of <span></span> pa❓</p>
<ul>
<li><span></span> years of salary at this rate ✨</li>
</ul>
<p>In <span></span> heat pump grants❓</p>
<ul>
<li><span></span>x <span></span> heat pump grants 🔥</li>
</ul>
<p>In <span></span> houses❓</p>
<ul>
<li>Fully funded: <span></span> of them 🏡</li>
<li>Using an average occupancy of <span></span> people: enough on it's own to house <span></span> of the population</li>
</ul>
</div>
<h2>It's a <u>lot</u> of money! 💰</h2>
<p>But for other context, it's <span></span> of Ireland's <span></span> <a href="https://whereyourmoneygoes.gov.ie/en/2023/" target="_blank" rel="noopener noreferrer">total expenditure in 2023</a>, and <span></span> of Ireland's <span></span> national debt <!-- I want to include this link https://www.ntma.ie/business-areas/funding-and-debt-management/statistics but you need to download or link to the spreadsheet for the National Debt figure I am using and not Gross, so I won't do that in my html. -->.</p>
<p>🤷</p>
</div><div id="calculang-info">
<p>These <a href="https://en.wikipedia.org/wiki/Back-of-the-envelope_calculation" target="_blank" rel="noopener noreferrer"><i>back-of-the-envelope calculations</i></a> are made using <a href="https://calculang.dev/" target="_blank" rel="noopener noreferrer">calculang</a>: a language for calculations for <i>flexibility, transparency, and communication</i> about numbers.</p>
<p>You can check (and change!) their formulas and assumptions under the tab for '💬' (top left)</p>
<p><i>This was a little, hurried exercise by <a href="https://calcwithdec.dev/about" target="_blank" rel="noopener noreferrer">Declan</a>.</i> <strong>Please don't use these numbers for budgetary planning</strong> ⚠️</p>
<p><a href="https://github.com/declann/howmuchis13billioneuros.com" target="_blank" rel="noopener noreferrer"><img width="25px" height="25px" src="https://howmuchis13billioneuros.com/_file/brand-git.ae3af407.png"></a> <a href="https://calcwithdec.dev/" target="_blank" rel="noopener noreferrer">CalcWithDec.dev</a></p>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Android now allows apps to block sideloading (137 pts)]]></title>
            <link>https://arstechnica.com/gadgets/2024/09/android-now-allows-apps-to-block-sideloading-and-push-a-google-play-version/</link>
            <guid>41515588</guid>
            <pubDate>Wed, 11 Sep 2024 21:42:39 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://arstechnica.com/gadgets/2024/09/android-now-allows-apps-to-block-sideloading-and-push-a-google-play-version/">https://arstechnica.com/gadgets/2024/09/android-now-allows-apps-to-block-sideloading-and-push-a-google-play-version/</a>, See on <a href="https://news.ycombinator.com/item?id=41515588">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="main">



<article itemscope="" itemtype="http://schema.org/NewsArticle" id="">
      <div>
        <header>
            <h4>
      Only way in now is through the roof    —
</h4>
            
            <h2 itemprop="description">"Select Play Partners" can block unofficial installation of their apps.</h2>
                    </header>
        <div itemprop="articleBody">
                                    
<figure>
  <img src="https://cdn.arstechnica.net/wp-content/uploads/2024/09/get_this_app_play.png" alt="Image from an Android phone, suggesting user " get="" this="" app="" from="" play="" and="" showing="" disjointed="" pieces="" of="" an="" including="" a="" frowning="" emoji-like="" face.="">
      <figcaption><p><a href="https://cdn.arstechnica.net/wp-content/uploads/2024/09/get_this_app_play.png" data-height="635" data-width="753">Enlarge</a> <span>/</span> It's never explained what this collection of app icons quite represents. A disorganized app you tossed together by sideloading? A face that's frowning because it's rolling down a bar held up by app icons? It's weird, but not quite evocative.</p></figcaption>  </figure>

  




<!-- cache hit 5:single/related:0bed4900a3187d6e128db7810b46a4b0 --><!-- empty -->
<p>You might sideload an Android app, or manually install its APK package, if you're using a custom version of Android that doesn't include Google's Play Store. Alternately, the app might be experimental, under development, or perhaps no longer maintained and offered by its developer. Until now, the existence of sideload-ready APKs on the web was something that seemed to be tolerated, if warned against, by Google.</p>
<p>This quiet standstill is being shaken up by a new feature in Google's Play Integrity API. As <a href="https://www.androidauthority.com/play-integrity-sideloading-detection-3480639/">reported by Android Authority</a>, developer tools to push "remediation" dialogs during sideloading <a href="https://io.google/2024/explore/f757438a-844f-4c59-8dd4-9a5580a5e23d/">debuted</a> at Google's I/O conference in May, have begun showing up on users' phones. Sideloaders of apps from the British shop <a href="https://forum.fairphone.com/t/cannot-install-the-new-tesco-clubcard-app-fp4/93824/10">Tesco</a>, fandom app <a href="https://www.reddit.com/r/Beyblade/comments/16bi9hh/getting_this_message_when_i_open_the_bbx_app/">BeyBlade X</a>, and <a href="https://x.com/AssembleDebug/status/1833700730349515062?t=BYKpcVNNC-8z7Jems7j37Q&amp;s=19">ChatGPT</a> have reported "Get this app from Play" prompts, which cannot be worked around. An Android gaming handheld user encountered <a href="https://www.reddit.com/r/retroid/comments/1dihb9d/how_to_bypass_get_this_app_from_play_on_rp2s/">a similarly worded prompt from <em>Diablo Immortal</em></a> on their device three months ago.</p>
<p>Google's Play Integrity API is how apps have previously blocked access when loaded onto phones that are in some way modified from a stock OS with all Google Play integrations intact. Recently, a popular two-factor authentication app <a href="https://arstechnica.com/gadgets/2024/07/loss-of-popular-2fa-tool-puts-security-minded-grapheneos-in-a-paradox/">blocked access on rooted phones</a>, including the security-minded GrapheneOS. Apps can call the Play Integrity API and get back an "integrity verdict," relaying if the phone has a "trustworthy" software environment, has Google Play Protect enabled, and passes other software checks.</p>                                                                        
                                                                                
<p>Graphene has questioned <a href="https://grapheneos.org/articles/attestation-compatibility-guide">the veracity of Google's Integrity API and SafetyNet Attestation systems</a>, recommending instead <a href="https://developer.android.com/training/articles/security-key-attestation">standard Android hardware attestation</a>. Rahman notes that apps do not have to take an all-or-nothing approach to integrity checking. Rather than block installation entirely, apps could call on the API only during sensitive actions, issuing a warning there. But not having a Play Store connection can also deprive developers of metrics, allow for installation on incompatible devices (and resulting bad reviews), and, of course, open the door to paid app piracy.</p>
<figure><img src="https://cdn.arstechnica.net/wp-content/uploads/2024/09/Screenshot-2024-09-11-at-4.38.04%E2%80%AFPM.png" width="1764" height="864"><figcaption><p>Google</p></figcaption></figure>
<h2>“Unknown distribution channels” blocked</h2>
<p>Google's developer video about "Automatic integrity protection" (at the <a href="https://youtu.be/RccJYep2v5I?si=XMuw3sBdM64cC2BH&amp;t=745">12-minute, 24-second mark on YouTube</a>) notes that "select" apps have access to automatic protection. This adds an automatic checking tool to your app and the "strongest version of Google Play's anti-tamper protection." "If users get your protected app from an unknown distribution channel," a slide in the presentation reads, "they'll be prompted to get it from Google Play," available to "select Play Partners."</p>
<p>Last year, Google introduced <a href="https://arstechnica.com/gadgets/2023/10/android-will-now-scan-sideloaded-apps-for-malware-at-install-time/">malware scanning of sideloaded apps</a> at install time. Google and Apple have come out against legislation that would <a href="https://arstechnica.com/tech-policy/2021/08/apple-and-google-seem-spooked-by-bill-requiring-more-app-stores-and-sideloading/">broaden sideloading rights for smartphone owners</a>, citing security and reliability concerns. <a href="https://arstechnica.com/apple/2024/05/apple-must-open-ipados-to-sideloading-within-6-months-eu-says/">European regulators forced Apple</a> earlier this year to allow for sideloading apps and app stores, though with fees and geographical restrictions in place.</p>

                                                </div>
    </div>

<section>
          
    
    <p>
      <section>
        <a href="https://arstechnica.com/author/kevinpurdy">Kevin Purdy</a>
        Kevin is a senior technology reporter at Ars Technica, covering open-source software, PC gaming, home automation, repairability, e-bikes, and tech history. He has previously worked at Lifehacker, Wirecutter, iFixit, and Carbon Switch.      </section>
    </p>

  </section>

  </article>
  </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Feeld dating app – Your nudes and data were publicly available (259 pts)]]></title>
            <link>https://fortbridge.co.uk/research/feeld-dating-app-nudes-data-publicly-available/</link>
            <guid>41515527</guid>
            <pubDate>Wed, 11 Sep 2024 21:32:46 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://fortbridge.co.uk/research/feeld-dating-app-nudes-data-publicly-available/">https://fortbridge.co.uk/research/feeld-dating-app-nudes-data-publicly-available/</a>, See on <a href="https://news.ycombinator.com/item?id=41515527">Hacker News</a></p>
<div id="readability-page-1" class="page"><article id="post-2251">

	<!-- .entry-header -->

	
	<div>

		
<p>This post highlights the critical importance of implementing robust security controls on the back-end of mobile applications. Specifically, our target was Feeld, a dating mobile app. It is just like Tinder, Bumble and others, but on steroids, meaning you can filter by distance, by age, by gender (&gt;10), couples, and by location. For premium users, you can also search by the type of kink, threeways/group scenarios, or the type of relationship you are interested in.</p>



<div>
<p>We reviewed the security controls implemented and discovered the following vulnerabilities. In particular, except for the first one, all others fall under the category of: ‘<a href="https://owasp.org/Top10/A01_2021-Broken_Access_Control/" data-wpel-link="external" rel="external noopener noreferrer">Broken access control</a>‘ according to <a href="https://owasp.org/www-project-top-ten/" data-wpel-link="external" rel="external noopener noreferrer">OWASP Top 10</a>.</p>



<ol>
<li>Disclosure of profile information to non-premium users</li>



<li>Read other people’s messages </li>



<li>Unauthenticated access to other people’s attachments (photos &amp; videos) from their chats</li>



<li>Delete, recover and edit other people’s messages</li>



<li>Update someone else’s profile information</li>



<li>Get a ‘Like’ from any user profile</li>



<li>Send messages in other people’s chat</li>



<li>View other people’s matches</li>
</ol>
</div>



<p>To begin with, let’s first examine how the application looks and its main menus before analyzing the vulnerabilities one by one.</p>







<div><h2>Table of contents</h2><ul><li><a href="#h-feeld-vulnerabilities" data-level="2">Feeld Vulnerabilities</a><ul><li><a href="#h-1-disclosure-of-profile-information-to-non-premium-users" data-level="3">1. Disclosure of profile information to non-premium users</a></li><li><a href="#h-2-read-other-people-s-messages" data-level="3">2. Read other people’s messages</a></li><li><a href="#h-3-unauthenticated-access-to-other-people-s-attachments-photos-amp-videos-from-their-chats" data-level="3">3. Unauthenticated access to other people’s attachments (photos &amp; videos) from their chats</a></li><li><a href="#h-4-delete-recover-and-edit-other-people-s-messages" data-level="3">4. Delete, recover and edit other people’s messages</a></li><li><a href="#h-5-update-someone-else-s-profile-information" data-level="3">5. Update someone else’s profile information</a></li><li><a href="#h-6-get-a-like-from-any-user-profile" data-level="3">6. Get a ‘Like’ from any user profile</a></li><li><a href="#h-7-send-messages-in-other-people-s-chat" data-level="3">7. Send messages in other people’s chat</a></li><li><a href="#h-8-view-other-people-s-matches" data-level="3">8. View other people’s matches</a></li></ul></li><li><a href="#h-see-our-leading-research-insights" data-level="2">See Our Leading Research Insights</a></li><li><a href="#h-conclusion" data-level="2">Conclusion</a></li><li><a href="#h-feeld-disclosure-timeline" data-level="2">Feeld Disclosure Timeline</a></li></ul></div>



<h2 id="h-feeld-vulnerabilities">Feeld Vulnerabilities</h2>



<h3 id="h-1-disclosure-of-profile-information-to-non-premium-users"><strong>1. Disclosure of profile information to non-premium users</strong></h3>



<p><strong>Details</strong>: The ‘basic’ user will no longer need to pay for a ‘premium’ subscription to get a premium benefit.<br>When you, as basic user, go to the ‘Likes’ menu in the mobile app, to see who liked your profile, you only get limited information, such as the name and the blurred photos of the ‘like’ sender, compared to the premium user who gets all the information available about the sender.<br>However, if you use a proxy tool such as Burp to intercept the request and response, you will find in the response all the information available about the ‘like’ senders, just like a premium user.</p>



<p><strong>Reproduction steps</strong>:</p>



<p>1.As basic user, we can go to the ‘Likes’ menu to see who liked or pinged us, as seen below. But beside their names and their blurred photos, we do not have any other information.</p>



<figure><a href="https://fortbridge.co.uk/wp-content/uploads/2024/04/Feeld10.jpg" data-slb-active="1" data-slb-asset="765513499" data-slb-internal="0" data-slb-group="2251" data-wpel-link="internal" rel="follow"><img loading="lazy" decoding="async" width="383" height="865" src="https://fortbridge.co.uk/wp-content/uploads/2024/04/Feeld10.jpg" alt="Feeld vulnerability #1 - Disclosure of profile information to non-premium users
Fig 1.0" srcset="https://fortbridge.co.uk/wp-content/uploads/2024/04/Feeld10.jpg 383w, https://fortbridge.co.uk/wp-content/uploads/2024/04/Feeld10-133x300.jpg 133w" sizes="(max-width: 383px) 100vw, 383px" data-old-src="data:image/svg+xml,%3Csvg%20xmlns='http://www.w3.org/2000/svg'%20viewBox='0%200%20383%20865'%3E%3C/svg%3E" data-lazy-srcset="https://fortbridge.co.uk/wp-content/uploads/2024/04/Feeld10.jpg 383w, https://fortbridge.co.uk/wp-content/uploads/2024/04/Feeld10-133x300.jpg 133w" data-lazy-src="https://fortbridge.co.uk/wp-content/uploads/2024/04/Feeld10.jpg"></a><figcaption>‘Who liked us’ Menu showing minimal info to non-premium users about people who liked our profile, such as Sam</figcaption></figure>



<p>2. However, if we intercept the request in Burp and check the response, as seen below, we will see that we have all the information about the user (age, distance, all their profile photos, streamUserId), including <strong>unauthenticated</strong> access to their profile photos stored on res.cloudinary.com. In addition, using the ‘streamUserId’ value found in the response we can exploit the next vulnerability ‘Read other people’s messages’ and read Sam’s messages.</p>







<h3 id="h-2-read-other-people-s-messages"><strong>2. Read other people’s messages</strong></h3>



<p><strong>Details</strong>: We can read other people’s messages in the chat. In order to do that, we will need to get our victim’s ‘streamUserId’ value, which is disclosed in different API requests.</p>



<p><strong>Reproduction steps</strong>:</p>



<ol>
<li>Go to the ‘Discover profiles’ menu.</li>



<li>Intercept the /graphql request with operationName: ‘DiscoverProfiles’.<br>Get a ‘streamUserId parameter value of the target user from the response, as seen below:</li>
</ol>



<figure><a href="https://fortbridge.co.uk/wp-content/uploads/2024/04/Feeld20.jpg" data-slb-active="1" data-slb-asset="1959970117" data-slb-internal="0" data-slb-group="2251" data-wpel-link="internal" rel="follow"><img loading="lazy" decoding="async" width="966" height="353" src="https://fortbridge.co.uk/wp-content/uploads/2024/04/Feeld20.jpg" alt="Feeld vulnerability #2 - Read other people's messages
Fig 2.0" srcset="https://fortbridge.co.uk/wp-content/uploads/2024/04/Feeld20.jpg 966w, https://fortbridge.co.uk/wp-content/uploads/2024/04/Feeld20-300x110.jpg 300w, https://fortbridge.co.uk/wp-content/uploads/2024/04/Feeld20-768x281.jpg 768w" sizes="(max-width: 966px) 100vw, 966px" data-old-src="data:image/svg+xml,%3Csvg%20xmlns='http://www.w3.org/2000/svg'%20viewBox='0%200%20966%20353'%3E%3C/svg%3E" data-lazy-srcset="https://fortbridge.co.uk/wp-content/uploads/2024/04/Feeld20.jpg 966w, https://fortbridge.co.uk/wp-content/uploads/2024/04/Feeld20-300x110.jpg 300w, https://fortbridge.co.uk/wp-content/uploads/2024/04/Feeld20-768x281.jpg 768w" data-lazy-src="https://fortbridge.co.uk/wp-content/uploads/2024/04/Feeld20.jpg"></a><figcaption>‘StreamUserId’ is: 64ccd281fbaa820001005b4f , and belongs to user ‘Chlo’.</figcaption></figure>



<p>3. Now go to the ‘Message’ menu, and intercept the request to the endpoint: https://chat.stream-io-api.com/channels?user_id=&amp;connection_id=&amp;api_key=y4tp4akjeb49 ,such as the one below:</p>



<figure><a href="https://fortbridge.co.uk/wp-content/uploads/2024/04/Feeld21.jpg" data-slb-active="1" data-slb-asset="1177161627" data-slb-internal="0" data-slb-group="2251" data-wpel-link="internal" rel="follow"><img loading="lazy" decoding="async" width="1020" height="526" src="https://fortbridge.co.uk/wp-content/uploads/2024/04/Feeld21.jpg" alt="Feeld vulnerability #2 - Read other people's messages
Fig 2.1" srcset="https://fortbridge.co.uk/wp-content/uploads/2024/04/Feeld21.jpg 1020w, https://fortbridge.co.uk/wp-content/uploads/2024/04/Feeld21-300x155.jpg 300w, https://fortbridge.co.uk/wp-content/uploads/2024/04/Feeld21-768x396.jpg 768w" sizes="(max-width: 1020px) 100vw, 1020px" data-old-src="data:image/svg+xml,%3Csvg%20xmlns='http://www.w3.org/2000/svg'%20viewBox='0%200%201020%20526'%3E%3C/svg%3E" data-lazy-srcset="https://fortbridge.co.uk/wp-content/uploads/2024/04/Feeld21.jpg 1020w, https://fortbridge.co.uk/wp-content/uploads/2024/04/Feeld21-300x155.jpg 300w, https://fortbridge.co.uk/wp-content/uploads/2024/04/Feeld21-768x396.jpg 768w" data-lazy-src="https://fortbridge.co.uk/wp-content/uploads/2024/04/Feeld21.jpg"></a></figure>



<p>4. Remove all the request parameters except ‘member’:{‘in’:[“&lt;value&gt;”] , and add the victim’s ‘streamUserId’ as &lt;value&gt; , as seen below:</p>



<figure><a href="https://fortbridge.co.uk/wp-content/uploads/2024/04/Feeld22.jpg" data-slb-active="1" data-slb-asset="322052019" data-slb-internal="0" data-slb-group="2251" data-wpel-link="internal" rel="follow"><img loading="lazy" decoding="async" width="965" height="517" src="https://fortbridge.co.uk/wp-content/uploads/2024/04/Feeld22.jpg" alt="Feeld vulnerability #2 - Read other people's messages
Fig 2.2" srcset="https://fortbridge.co.uk/wp-content/uploads/2024/04/Feeld22.jpg 965w, https://fortbridge.co.uk/wp-content/uploads/2024/04/Feeld22-300x161.jpg 300w, https://fortbridge.co.uk/wp-content/uploads/2024/04/Feeld22-768x411.jpg 768w" sizes="(max-width: 965px) 100vw, 965px" data-old-src="data:image/svg+xml,%3Csvg%20xmlns='http://www.w3.org/2000/svg'%20viewBox='0%200%20965%20517'%3E%3C/svg%3E" data-lazy-srcset="https://fortbridge.co.uk/wp-content/uploads/2024/04/Feeld22.jpg 965w, https://fortbridge.co.uk/wp-content/uploads/2024/04/Feeld22-300x161.jpg 300w, https://fortbridge.co.uk/wp-content/uploads/2024/04/Feeld22-768x411.jpg 768w" data-lazy-src="https://fortbridge.co.uk/wp-content/uploads/2024/04/Feeld22.jpg"></a></figure>



<p>5.If we search in the response by “text” we can see the total number of messages to and from our victim ‘Chloe:</p>



<figure><a href="https://fortbridge.co.uk/wp-content/uploads/2024/04/Feeld23.jpg" data-slb-active="1" data-slb-asset="1411059700" data-slb-internal="0" data-slb-group="2251" data-wpel-link="internal" rel="follow"><img loading="lazy" decoding="async" width="780" height="663" src="https://fortbridge.co.uk/wp-content/uploads/2024/04/Feeld23.jpg" alt="Feeld vulnerability #2 - Read other people's messages
Fig 2.3" srcset="https://fortbridge.co.uk/wp-content/uploads/2024/04/Feeld23.jpg 780w, https://fortbridge.co.uk/wp-content/uploads/2024/04/Feeld23-300x255.jpg 300w, https://fortbridge.co.uk/wp-content/uploads/2024/04/Feeld23-768x653.jpg 768w" sizes="(max-width: 780px) 100vw, 780px" data-old-src="data:image/svg+xml,%3Csvg%20xmlns='http://www.w3.org/2000/svg'%20viewBox='0%200%20780%20663'%3E%3C/svg%3E" data-lazy-srcset="https://fortbridge.co.uk/wp-content/uploads/2024/04/Feeld23.jpg 780w, https://fortbridge.co.uk/wp-content/uploads/2024/04/Feeld23-300x255.jpg 300w, https://fortbridge.co.uk/wp-content/uploads/2024/04/Feeld23-768x653.jpg 768w" data-lazy-src="https://fortbridge.co.uk/wp-content/uploads/2024/04/Feeld23.jpg"></a></figure>



<h3 id="h-3-unauthenticated-access-to-other-people-s-attachments-photos-amp-videos-from-their-chats"><strong>3. Unauthenticated access to other people’s attachments (photos &amp; videos) from their chats</strong></h3>



<p><strong>Details</strong>: We can build upon the previous vulnerability ‘Read other people’s messages’ and we can access the attachments (images and videos) that have been shared between users.</p>



<p>There are 2 types of attachments that can be shared in a chat: photos and videos. The ‘Photo’ option can be either replay-able or time-limited to between 5 to 15 seconds after which it is not available to the receiver user anymore. And the ‘Video’ option can be either ‘replay’-able or ‘play-once’ only.</p>



<div>
<div>
<p><strong>The workflow</strong> for uploading and getting other people’s <strong>replay</strong>-able photos is the following:</p>



<ol>
<li><span>Requirements</span> for the attacker: From the ‘Discover Profiles’ menu, obtain the ‘<span>stream_id</span>‘ from the target victim. This ‘stream_id’ is required to access their messages, which contain the uploaded ‘photo_id’s of both the victim and their counterpart.</li>



<li>The user<strong> </strong>uploads through Feeld app a replay-able photo on api.cloudinary.com . The response from api.cloudinary.com returns a ‘photo_id’  .</li>



<li>The photo gets copied to feeld.co and becomes available to authenticated users to the following endpoints:
<ul>
<li>feeld.co/cdn/chat-attachment/&lt;receiver’s_profileId&gt;/&lt;photo_id</li>



<li>feeld.co/cdn/chat-attachment/&lt;sender’s_profileId&gt;/&lt;photo_id&gt;</li>
</ul>
</li>



<li>After some testing the above endpoint can be reduced to the following:
<ul>
<li>feeld.co/cdn/chat-attachment/<strong>x</strong>/&lt;photo_id&gt; <br>//Instead of ‘x’ any combination of minimum 1 character is permissible and the photo is returned to any feeld authenticated user.</li>
</ul>
</li>



<li><span>After some more testing</span>, we discovered the above endpoint but prepended with /v1/ :
<ul>
<li>feeld.co<strong>/v1/</strong>cdn/chat-attachment/x/&lt;photo_id&gt;&nbsp; <br>//This returns the url to the photo stored on api.cloudinary.com that is accessible <strong>unauthenticated</strong></li>
</ul>
</li>
</ol>
</div>



<div>
<p><strong>The</strong> <strong>workflow</strong> for uploading and getting other people’s <strong>time-limited</strong> photos is the following:</p>



<ol>
<li><span>Requirements</span> for the attacker: From the ‘Discover Profiles’ menu, get from the target victim the ‘<span>profileId</span>‘ and the ‘<span>stream_id</span>‘ required to access their messages and their time-limited photos. To get the time limited photos of the victim’s conterpart in a chat, you will have to repeat this step but search instead for the victim’s counterpart ‘profileId’ and ‘stream_id’ as it is not possible to leak these from their chat messages.   </li>



<li>The user<strong> </strong>uploads through Feeld app a 5-to-15 seconds time-limited photo on api.cloudinary.com . The response from api.cloudinary.com returns a ‘photo_id’  .</li>



<li>The photo gets copied to feeld.co and becomes available to authenticated users to the following endpoints:
<ul>
<li>feeld.co/cdn/chat-attachment/&lt;receiver’s_profileId&gt;/&lt;photo_id&gt;  //Once the receiver opened it or this endpoint is acessed, after 5-15seconds the photo gets deleted and is <span>not accessible</span> to this endpoint anymore.</li>



<li>feeld.co/cdn/chat-attachment/&lt;victim’s_profileId&gt;/&lt;photo_id&gt;      //The time limited photo will always <span>be accessible</span> from this endpoint, even after 5-15 seconds, to all authenticated users. The &lt;victim’s_profileId&gt; is mandatory in this case, compared to the case on the left.</li>
</ul>
</li>



<li><span>After some more testing</span>, we discovered the above endpoint but prepended with /v1/ :
<ul>
<li>feeld.co/<strong>v1/</strong>cdn/chat-attachment/&lt;victim_profileId&gt;/&lt;photo_id&gt; <br>//This returns the url to the photo stored on api.cloudinary.com that is accessible <strong>unauthenticated</strong><br></li>
</ul>
</li>
</ol>
</div>
</div>



<div>
<p>To summarize, as an attacker, we can access all of the following <span>unauthenticated</span>:</p>



<ul>
<li>Photos replay-able</li>



<li>Photos time-limited</li>



<li>Videos replay-able</li>



<li>Videos play-once</li>
</ul>
</div>



<p><strong>Reproduction steps</strong>:</p>



<p><strong>Instance 1</strong>: Uploading replay-able photos</p>



<p>1.Let’s upload in our chat, a normal replay-able photo. <br>So, the first request is ‘Generate Upload Credentials’ for uploading on ‘api.cloudinary.com’:</p>



<figure><a href="https://fortbridge.co.uk/wp-content/uploads/2024/04/Feeld30.jpg" data-slb-active="1" data-slb-asset="1259363653" data-slb-internal="0" data-slb-group="2251" data-wpel-link="internal" rel="follow"><img loading="lazy" decoding="async" width="992" height="352" src="https://fortbridge.co.uk/wp-content/uploads/2024/04/Feeld30.jpg" alt="Feeld vulnerability #3 - Unauthenticated access to other people's attachments (photos &amp; videos) from their chats
Fig 3.0" srcset="https://fortbridge.co.uk/wp-content/uploads/2024/04/Feeld30.jpg 992w, https://fortbridge.co.uk/wp-content/uploads/2024/04/Feeld30-300x106.jpg 300w, https://fortbridge.co.uk/wp-content/uploads/2024/04/Feeld30-768x273.jpg 768w" sizes="(max-width: 992px) 100vw, 992px" data-old-src="data:image/svg+xml,%3Csvg%20xmlns='http://www.w3.org/2000/svg'%20viewBox='0%200%20992%20352'%3E%3C/svg%3E" data-lazy-srcset="https://fortbridge.co.uk/wp-content/uploads/2024/04/Feeld30.jpg 992w, https://fortbridge.co.uk/wp-content/uploads/2024/04/Feeld30-300x106.jpg 300w, https://fortbridge.co.uk/wp-content/uploads/2024/04/Feeld30-768x273.jpg 768w" data-lazy-src="https://fortbridge.co.uk/wp-content/uploads/2024/04/Feeld30.jpg"></a></figure>



<p>2. Then we send a photo upload request to api.cloudinary.com using the above generated ‘publicId’ and ‘signature’ values, plus an api_key and timestamp parameters:</p>







<p>3. Next request done is: ‘UploadChatAttachment’ which gets the above unique public_id of the image from api.cloudinary.com and is passed to core.api.feeld.co, as seen below.<br>I suspect this request is to copy the photo from api.cloudinary.com to core.api.feeld.co.</p>



<figure><a href="https://fortbridge.co.uk/wp-content/uploads/2024/04/Feeld33.jpg" data-slb-active="1" data-slb-asset="632059022" data-slb-internal="0" data-slb-group="2251" data-wpel-link="internal" rel="follow"><img loading="lazy" decoding="async" width="1020" height="494" src="https://fortbridge.co.uk/wp-content/uploads/2024/04/Feeld33.jpg" alt="Feeld vulnerability #3 - Unauthenticated access to other people's attachments (photos &amp; videos) from their chats
Fig 3.3" srcset="https://fortbridge.co.uk/wp-content/uploads/2024/04/Feeld33.jpg 1020w, https://fortbridge.co.uk/wp-content/uploads/2024/04/Feeld33-300x145.jpg 300w, https://fortbridge.co.uk/wp-content/uploads/2024/04/Feeld33-768x372.jpg 768w" sizes="(max-width: 1020px) 100vw, 1020px" data-old-src="data:image/svg+xml,%3Csvg%20xmlns='http://www.w3.org/2000/svg'%20viewBox='0%200%201020%20494'%3E%3C/svg%3E" data-lazy-srcset="https://fortbridge.co.uk/wp-content/uploads/2024/04/Feeld33.jpg 1020w, https://fortbridge.co.uk/wp-content/uploads/2024/04/Feeld33-300x145.jpg 300w, https://fortbridge.co.uk/wp-content/uploads/2024/04/Feeld33-768x372.jpg 768w" data-lazy-src="https://fortbridge.co.uk/wp-content/uploads/2024/04/Feeld33.jpg"></a></figure>



<p>4. A unique ‘attachmentID’ parameter will be returned above in the response.<br>This ‘attachmentID’ will be used and passed in the chat, as seen below:</p>



<figure><a href="https://fortbridge.co.uk/wp-content/uploads/2024/04/Feeld34.jpg" data-slb-active="1" data-slb-asset="1597302537" data-slb-internal="0" data-slb-group="2251" data-wpel-link="internal" rel="follow"><img loading="lazy" decoding="async" width="1020" height="598" src="https://fortbridge.co.uk/wp-content/uploads/2024/04/Feeld34.jpg" alt="Feeld vulnerability #3 - Unauthenticated access to other people's attachments (photos &amp; videos) from their chats
Fig 3.4" srcset="https://fortbridge.co.uk/wp-content/uploads/2024/04/Feeld34.jpg 1020w, https://fortbridge.co.uk/wp-content/uploads/2024/04/Feeld34-300x176.jpg 300w, https://fortbridge.co.uk/wp-content/uploads/2024/04/Feeld34-768x450.jpg 768w" sizes="(max-width: 1020px) 100vw, 1020px" data-old-src="data:image/svg+xml,%3Csvg%20xmlns='http://www.w3.org/2000/svg'%20viewBox='0%200%201020%20598'%3E%3C/svg%3E" data-lazy-srcset="https://fortbridge.co.uk/wp-content/uploads/2024/04/Feeld34.jpg 1020w, https://fortbridge.co.uk/wp-content/uploads/2024/04/Feeld34-300x176.jpg 300w, https://fortbridge.co.uk/wp-content/uploads/2024/04/Feeld34-768x450.jpg 768w" data-lazy-src="https://fortbridge.co.uk/wp-content/uploads/2024/04/Feeld34.jpg"></a></figure>



<p>4. Now to get the photo authenticated, as any other user, we make the following request, using the above attachmentID:<br><a href="https://core.api.feeld.co/cdn/chat-attachment/x/c07c3360-c787-4be9-9cd6-b1ef9d06fff4" data-wpel-link="external" rel="external noopener noreferrer">https://core.api.feeld.co/cdn/chat-attachment/x/<strong>c07c3360-c787-4be9-9cd6-b1ef9d06fff4</strong></a><br>Note: In the above request path, initially instead of ‘x’ it was the ‘ProfileId’ guid value of the sender or receiver of the photo, but deleting it works fine, so I just left an ‘x’ for an easier read. </p>



<figure><a href="https://fortbridge.co.uk/wp-content/uploads/2024/04/Feeld36.jpg" data-slb-active="1" data-slb-asset="1321041874" data-slb-internal="0" data-slb-group="2251" data-wpel-link="internal" rel="follow"><img loading="lazy" decoding="async" width="1024" height="527" src="https://fortbridge.co.uk/wp-content/uploads/2024/04/Feeld36.jpg" alt="Feeld vulnerability #3 - Unauthenticated access to other people's attachments (photos &amp; videos) from their chats
Fig 3.5" srcset="https://fortbridge.co.uk/wp-content/uploads/2024/04/Feeld36.jpg 1024w, https://fortbridge.co.uk/wp-content/uploads/2024/04/Feeld36-300x154.jpg 300w, https://fortbridge.co.uk/wp-content/uploads/2024/04/Feeld36-768x395.jpg 768w" sizes="(max-width: 1024px) 100vw, 1024px" data-old-src="data:image/svg+xml,%3Csvg%20xmlns='http://www.w3.org/2000/svg'%20viewBox='0%200%201024%20527'%3E%3C/svg%3E" data-lazy-srcset="https://fortbridge.co.uk/wp-content/uploads/2024/04/Feeld36.jpg 1024w, https://fortbridge.co.uk/wp-content/uploads/2024/04/Feeld36-300x154.jpg 300w, https://fortbridge.co.uk/wp-content/uploads/2024/04/Feeld36-768x395.jpg 768w" data-lazy-src="https://fortbridge.co.uk/wp-content/uploads/2024/04/Feeld36.jpg"></a></figure>



<p>5.Now, to get the same photo but from cloudinary.com, <strong>unauthenticated</strong>, prepend /v1/ to the above request, as seen below, and you will get the ‘url’ pointing to the original photo:<br><a href="https://res.cloudinary.com/threender/image/upload/s--QQjZiJxc--/d4e74e59-430d-403f-b1c5-9c8208472007" data-wpel-link="external" rel="external noopener noreferrer">https://res.cloudinary.com/threender/image/upload/s–QQjZiJxc–/d4e74e59-430d-403f-b1c5-9c8208472007</a></p>



<div>
<figure><a href="https://fortbridge.co.uk/wp-content/uploads/2024/04/Feeld37-1.jpg" data-slb-active="1" data-slb-asset="1698809843" data-slb-internal="0" data-slb-group="2251" data-wpel-link="internal" rel="follow"><img loading="lazy" decoding="async" width="1024" height="243" src="https://fortbridge.co.uk/wp-content/uploads/2024/04/Feeld37-1.jpg" alt="Feeld vulnerability #3 - Unauthenticated access to other people's attachments (photos &amp; videos) from their chats
Fig 3.6" srcset="https://fortbridge.co.uk/wp-content/uploads/2024/04/Feeld37-1.jpg 1024w, https://fortbridge.co.uk/wp-content/uploads/2024/04/Feeld37-1-300x71.jpg 300w, https://fortbridge.co.uk/wp-content/uploads/2024/04/Feeld37-1-768x182.jpg 768w" sizes="(max-width: 1024px) 100vw, 1024px" data-old-src="data:image/svg+xml,%3Csvg%20xmlns='http://www.w3.org/2000/svg'%20viewBox='0%200%201024%20243'%3E%3C/svg%3E" data-lazy-srcset="https://fortbridge.co.uk/wp-content/uploads/2024/04/Feeld37-1.jpg 1024w, https://fortbridge.co.uk/wp-content/uploads/2024/04/Feeld37-1-300x71.jpg 300w, https://fortbridge.co.uk/wp-content/uploads/2024/04/Feeld37-1-768x182.jpg 768w" data-lazy-src="https://fortbridge.co.uk/wp-content/uploads/2024/04/Feeld37-1.jpg"></a></figure>
</div>



<figure><a href="https://fortbridge.co.uk/wp-content/uploads/2024/04/Feeld38-2.jpg" data-slb-active="1" data-slb-asset="2047219883" data-slb-internal="0" data-slb-group="2251" data-wpel-link="internal" rel="follow"><img loading="lazy" decoding="async" width="773" height="647" src="https://fortbridge.co.uk/wp-content/uploads/2024/04/Feeld38-2.jpg" alt="Feeld vulnerability #3 - Unauthenticated access to other people's attachments (photos &amp; videos) from their chats
Fig 3.7" srcset="https://fortbridge.co.uk/wp-content/uploads/2024/04/Feeld38-2.jpg 773w, https://fortbridge.co.uk/wp-content/uploads/2024/04/Feeld38-2-300x251.jpg 300w, https://fortbridge.co.uk/wp-content/uploads/2024/04/Feeld38-2-768x643.jpg 768w" sizes="(max-width: 773px) 100vw, 773px" data-old-src="data:image/svg+xml,%3Csvg%20xmlns='http://www.w3.org/2000/svg'%20viewBox='0%200%20773%20647'%3E%3C/svg%3E" data-lazy-srcset="https://fortbridge.co.uk/wp-content/uploads/2024/04/Feeld38-2.jpg 773w, https://fortbridge.co.uk/wp-content/uploads/2024/04/Feeld38-2-300x251.jpg 300w, https://fortbridge.co.uk/wp-content/uploads/2024/04/Feeld38-2-768x643.jpg 768w" data-lazy-src="https://fortbridge.co.uk/wp-content/uploads/2024/04/Feeld38-2.jpg"></a><figcaption>The photo (with the view from our office), that can be reached unauthenticated.</figcaption></figure>



<p><strong>Instance 2</strong>: Uploading time-limited photos<br><em>Note: There are 2 main differences from the above process. <br>Firstly, when uploading ‘time-limited’ photos, we pass an extra parameter ‘visibilityMilliseconds:15000’. Secondly, for accessing the photo, we use the ‘profileId’ GUID value of the victim that uploaded the photo, rather than the ‘x’ value used in the above path. If their chat counterpart uploaded the ‘time-limited’ photo, we need to return to the ‘Discover profiles’ menu to locate their &lt;profileId&gt; GUID value, which is mandatory for accessing these photos.</em></p>



<p>1. A request will be made to api.cloudinary.com to upload the photo:</p>



<figure><a href="https://fortbridge.co.uk/wp-content/uploads/2024/04/Feeld330.jpg" data-slb-active="1" data-slb-asset="1212955532" data-slb-internal="0" data-slb-group="2251" data-wpel-link="internal" rel="follow"><img loading="lazy" decoding="async" width="985" height="591" src="https://fortbridge.co.uk/wp-content/uploads/2024/04/Feeld330.jpg" alt="Feeld vulnerability #3 - Unauthenticated access to other people's attachments (photos &amp; videos) from their chats
Fig 3.8" srcset="https://fortbridge.co.uk/wp-content/uploads/2024/04/Feeld330.jpg 985w, https://fortbridge.co.uk/wp-content/uploads/2024/04/Feeld330-300x180.jpg 300w, https://fortbridge.co.uk/wp-content/uploads/2024/04/Feeld330-768x461.jpg 768w" sizes="(max-width: 985px) 100vw, 985px" data-old-src="data:image/svg+xml,%3Csvg%20xmlns='http://www.w3.org/2000/svg'%20viewBox='0%200%20985%20591'%3E%3C/svg%3E" data-lazy-srcset="https://fortbridge.co.uk/wp-content/uploads/2024/04/Feeld330.jpg 985w, https://fortbridge.co.uk/wp-content/uploads/2024/04/Feeld330-300x180.jpg 300w, https://fortbridge.co.uk/wp-content/uploads/2024/04/Feeld330-768x461.jpg 768w" data-lazy-src="https://fortbridge.co.uk/wp-content/uploads/2024/04/Feeld330.jpg"></a></figure>



<p>2. Copying the photo from&nbsp;cloudinary.com&nbsp;to&nbsp;core.api.feeld.co:</p>



<figure><a href="https://fortbridge.co.uk/wp-content/uploads/2024/04/Feeld331.png" data-slb-active="1" data-slb-asset="1492252547" data-slb-internal="0" data-slb-group="2251" data-wpel-link="internal" rel="follow"><img loading="lazy" decoding="async" width="1020" height="640" src="https://fortbridge.co.uk/wp-content/uploads/2024/04/Feeld331.png" alt="Feeld vulnerability #3 - Unauthenticated access to other people's attachments (photos &amp; videos) from their chats
Fig 3.9" srcset="https://fortbridge.co.uk/wp-content/uploads/2024/04/Feeld331.png 1020w, https://fortbridge.co.uk/wp-content/uploads/2024/04/Feeld331-300x188.png 300w, https://fortbridge.co.uk/wp-content/uploads/2024/04/Feeld331-768x482.png 768w" sizes="(max-width: 1020px) 100vw, 1020px" data-old-src="data:image/svg+xml,%3Csvg%20xmlns='http://www.w3.org/2000/svg'%20viewBox='0%200%201020%20640'%3E%3C/svg%3E" data-lazy-srcset="https://fortbridge.co.uk/wp-content/uploads/2024/04/Feeld331.png 1020w, https://fortbridge.co.uk/wp-content/uploads/2024/04/Feeld331-300x188.png 300w, https://fortbridge.co.uk/wp-content/uploads/2024/04/Feeld331-768x482.png 768w" data-lazy-src="https://fortbridge.co.uk/wp-content/uploads/2024/04/Feeld331.png"></a></figure>



<p>3.Then, if we read again the chat using the previous vulnerability ‘Read other people’s messages’, we can find the attachmentId to use in order to get the photo:</p>



<figure><a href="https://fortbridge.co.uk/wp-content/uploads/2024/04/Feeld332.jpg" data-slb-active="1" data-slb-asset="1185664239" data-slb-internal="0" data-slb-group="2251" data-wpel-link="internal" rel="follow"><img loading="lazy" decoding="async" width="1016" height="588" src="https://fortbridge.co.uk/wp-content/uploads/2024/04/Feeld332.jpg" alt="Feeld vulnerability #3 - Unauthenticated access to other people's attachments (photos &amp; videos) from their chats
Fig 3.10" srcset="https://fortbridge.co.uk/wp-content/uploads/2024/04/Feeld332.jpg 1016w, https://fortbridge.co.uk/wp-content/uploads/2024/04/Feeld332-300x174.jpg 300w, https://fortbridge.co.uk/wp-content/uploads/2024/04/Feeld332-768x444.jpg 768w" sizes="(max-width: 1016px) 100vw, 1016px" data-old-src="data:image/svg+xml,%3Csvg%20xmlns='http://www.w3.org/2000/svg'%20viewBox='0%200%201016%20588'%3E%3C/svg%3E" data-lazy-srcset="https://fortbridge.co.uk/wp-content/uploads/2024/04/Feeld332.jpg 1016w, https://fortbridge.co.uk/wp-content/uploads/2024/04/Feeld332-300x174.jpg 300w, https://fortbridge.co.uk/wp-content/uploads/2024/04/Feeld332-768x444.jpg 768w" data-lazy-src="https://fortbridge.co.uk/wp-content/uploads/2024/04/Feeld332.jpg"></a></figure>



<p>4.To retrieve the image, we will need the ‘profileId’ guid of our victim that uploaded the photo, which we already have from the ‘Discover Profile’ menu when we have chosen this target victim. <br>Thus, the 2 urls to get the photo authenticated are:</p>



<ul>
<li>https://core.api.feeld.co/cdn/chat-attachment/&lt;receiver’s_profileId&gt;/971a0d2f-f50c-45fc-8a37-4d9002f71e49 . However, 5-15 seconds after accessing this endpoint, the photo at this endpoint will be <span>deleted</span> .</li>



<li>https://core.api.feeld.co/cdn/chat-attachment/&lt;victim’s_profileId&gt;/971a0d2f-f50c-45fc-8a37-4d9002f71e49 . This will always return the photo to authenticated users.</li>
</ul>



<p>5. We can use the following endpoint: <br>https://core.api.feeld.co<strong>/v1/</strong>cdn/chat-attachment/&lt;victim’s_profileId&gt;/971a0d2f-f50c-45fc-8a37-4d9002f71e49 , which will return a url with the photo stored on res.cloudinary.com .</p>



<figure><a href="https://fortbridge.co.uk/wp-content/uploads/2024/04/Photo-Time-Limited-getting-the-res-url.png" data-slb-active="1" data-slb-asset="1058863944" data-slb-internal="0" data-slb-group="2251" data-wpel-link="internal" rel="follow"><img loading="lazy" decoding="async" width="1024" height="357" src="https://fortbridge.co.uk/wp-content/uploads/2024/04/Photo-Time-Limited-getting-the-res-url-1024x357.png" alt="Feeld vulnerability #3 - Unauthenticated access to other people's attachments (photos &amp; videos) from their chats
Fig 3.11" srcset="https://fortbridge.co.uk/wp-content/uploads/2024/04/Photo-Time-Limited-getting-the-res-url-1024x357.png 1024w, https://fortbridge.co.uk/wp-content/uploads/2024/04/Photo-Time-Limited-getting-the-res-url-300x105.png 300w, https://fortbridge.co.uk/wp-content/uploads/2024/04/Photo-Time-Limited-getting-the-res-url-768x268.png 768w, https://fortbridge.co.uk/wp-content/uploads/2024/04/Photo-Time-Limited-getting-the-res-url-1536x536.png 1536w, https://fortbridge.co.uk/wp-content/uploads/2024/04/Photo-Time-Limited-getting-the-res-url-2048x714.png 2048w" sizes="(max-width: 1024px) 100vw, 1024px" data-old-src="data:image/svg+xml,%3Csvg%20xmlns='http://www.w3.org/2000/svg'%20viewBox='0%200%201024%20357'%3E%3C/svg%3E" data-lazy-srcset="https://fortbridge.co.uk/wp-content/uploads/2024/04/Photo-Time-Limited-getting-the-res-url-1024x357.png 1024w, https://fortbridge.co.uk/wp-content/uploads/2024/04/Photo-Time-Limited-getting-the-res-url-300x105.png 300w, https://fortbridge.co.uk/wp-content/uploads/2024/04/Photo-Time-Limited-getting-the-res-url-768x268.png 768w, https://fortbridge.co.uk/wp-content/uploads/2024/04/Photo-Time-Limited-getting-the-res-url-1536x536.png 1536w, https://fortbridge.co.uk/wp-content/uploads/2024/04/Photo-Time-Limited-getting-the-res-url-2048x714.png 2048w" data-lazy-src="https://fortbridge.co.uk/wp-content/uploads/2024/04/Photo-Time-Limited-getting-the-res-url-1024x357.png"></a></figure>



<p>5. The returned url for <strong>unauthenticated</strong> access is:<br> <a href="https://res.cloudinary.com/threender/image/upload/s--7tD2qatw--/540068ee-b41a-431f-b0e9-b7522fefbd5a" data-wpel-link="external" rel="external noopener noreferrer">https://res.cloudinary.com/threender/image/upload/s–7tD2qatw–/540068ee-b41a-431f-b0e9-b7522fefbd5a</a> .</p>



<figure><a href="https://fortbridge.co.uk/wp-content/uploads/2024/04/Feeld334.jpg" data-slb-active="1" data-slb-asset="1513012821" data-slb-internal="0" data-slb-group="2251" data-wpel-link="internal" rel="follow"><img loading="lazy" decoding="async" width="1020" height="690" src="https://fortbridge.co.uk/wp-content/uploads/2024/04/Feeld334.jpg" alt="Feeld vulnerability #3 - Unauthenticated access to other people's attachments (photos &amp; videos) from their chats
Fig 3.12" srcset="https://fortbridge.co.uk/wp-content/uploads/2024/04/Feeld334.jpg 1020w, https://fortbridge.co.uk/wp-content/uploads/2024/04/Feeld334-300x203.jpg 300w, https://fortbridge.co.uk/wp-content/uploads/2024/04/Feeld334-768x520.jpg 768w" sizes="(max-width: 1020px) 100vw, 1020px" data-old-src="data:image/svg+xml,%3Csvg%20xmlns='http://www.w3.org/2000/svg'%20viewBox='0%200%201020%20690'%3E%3C/svg%3E" data-lazy-srcset="https://fortbridge.co.uk/wp-content/uploads/2024/04/Feeld334.jpg 1020w, https://fortbridge.co.uk/wp-content/uploads/2024/04/Feeld334-300x203.jpg 300w, https://fortbridge.co.uk/wp-content/uploads/2024/04/Feeld334-768x520.jpg 768w" data-lazy-src="https://fortbridge.co.uk/wp-content/uploads/2024/04/Feeld334.jpg"></a></figure>



<p>The only thing random in the above url, in case you want to brute-force it, are the 8 characters ‘7tD2qatw‘.</p>



<p>Once the 15 seconds passed and the receiver wants to re-access his time limited photo, he will get the following response: ‘Attachment has expired’.</p>



<figure><a href="https://fortbridge.co.uk/wp-content/uploads/2024/04/Feeld336.jpg" data-slb-active="1" data-slb-asset="1155472623" data-slb-internal="0" data-slb-group="2251" data-wpel-link="internal" rel="follow"><img loading="lazy" decoding="async" width="1022" height="222" src="https://fortbridge.co.uk/wp-content/uploads/2024/04/Feeld336.jpg" alt="Feeld vulnerability #3 - Unauthenticated access to other people's attachments (photos &amp; videos) from their chats
Fig 3.13" srcset="https://fortbridge.co.uk/wp-content/uploads/2024/04/Feeld336.jpg 1022w, https://fortbridge.co.uk/wp-content/uploads/2024/04/Feeld336-300x65.jpg 300w, https://fortbridge.co.uk/wp-content/uploads/2024/04/Feeld336-768x167.jpg 768w" sizes="(max-width: 1022px) 100vw, 1022px" data-old-src="data:image/svg+xml,%3Csvg%20xmlns='http://www.w3.org/2000/svg'%20viewBox='0%200%201022%20222'%3E%3C/svg%3E" data-lazy-srcset="https://fortbridge.co.uk/wp-content/uploads/2024/04/Feeld336.jpg 1022w, https://fortbridge.co.uk/wp-content/uploads/2024/04/Feeld336-300x65.jpg 300w, https://fortbridge.co.uk/wp-content/uploads/2024/04/Feeld336-768x167.jpg 768w" data-lazy-src="https://fortbridge.co.uk/wp-content/uploads/2024/04/Feeld336.jpg"></a></figure>



<p>Below is the difference in behaviour and what the sender of a timed-photo will see on his Android and iOS phone once the 5-15seconds passed. </p>







<p><strong>Instance 3</strong>: Uploading normal video</p>



<p>1.Pick a chat, select ‘upload video’ option, record a video and submit it in the chat.<br>The below requests will be made. <br>The video will be uploaded to <a href="https://us-east.stream-io-cdn.com/" data-wpel-link="external" rel="external noopener noreferrer">us-east.stream-io-cdn.com</a>:</p>



<figure><a href="https://fortbridge.co.uk/wp-content/uploads/2024/04/Feeld320.jpg" data-slb-active="1" data-slb-asset="303831309" data-slb-internal="0" data-slb-group="2251" data-wpel-link="internal" rel="follow"><img loading="lazy" decoding="async" width="1009" height="445" src="https://fortbridge.co.uk/wp-content/uploads/2024/04/Feeld320.jpg" alt="Feeld vulnerability #3 - Unauthenticated access to other people's attachments (photos &amp; videos) from their chats
Fig 3.16" srcset="https://fortbridge.co.uk/wp-content/uploads/2024/04/Feeld320.jpg 1009w, https://fortbridge.co.uk/wp-content/uploads/2024/04/Feeld320-300x132.jpg 300w, https://fortbridge.co.uk/wp-content/uploads/2024/04/Feeld320-768x339.jpg 768w" sizes="(max-width: 1009px) 100vw, 1009px" data-old-src="data:image/svg+xml,%3Csvg%20xmlns='http://www.w3.org/2000/svg'%20viewBox='0%200%201009%20445'%3E%3C/svg%3E" data-lazy-srcset="https://fortbridge.co.uk/wp-content/uploads/2024/04/Feeld320.jpg 1009w, https://fortbridge.co.uk/wp-content/uploads/2024/04/Feeld320-300x132.jpg 300w, https://fortbridge.co.uk/wp-content/uploads/2024/04/Feeld320-768x339.jpg 768w" data-lazy-src="https://fortbridge.co.uk/wp-content/uploads/2024/04/Feeld320.jpg"></a></figure>



<p>2.The url from the response will be passed in the chat, as seen below:</p>



<figure><a href="https://fortbridge.co.uk/wp-content/uploads/2024/04/Feeld321-1.jpg" data-slb-active="1" data-slb-asset="1788911828" data-slb-internal="0" data-slb-group="2251" data-wpel-link="internal" rel="follow"><img loading="lazy" decoding="async" width="1007" height="625" src="https://fortbridge.co.uk/wp-content/uploads/2024/04/Feeld321-1.jpg" alt="Feeld vulnerability #3 - Unauthenticated access to other people's attachments (photos &amp; videos) from their chats
Fig 3.17" srcset="https://fortbridge.co.uk/wp-content/uploads/2024/04/Feeld321-1.jpg 1007w, https://fortbridge.co.uk/wp-content/uploads/2024/04/Feeld321-1-300x186.jpg 300w, https://fortbridge.co.uk/wp-content/uploads/2024/04/Feeld321-1-768x477.jpg 768w" sizes="(max-width: 1007px) 100vw, 1007px" data-old-src="data:image/svg+xml,%3Csvg%20xmlns='http://www.w3.org/2000/svg'%20viewBox='0%200%201007%20625'%3E%3C/svg%3E" data-lazy-srcset="https://fortbridge.co.uk/wp-content/uploads/2024/04/Feeld321-1.jpg 1007w, https://fortbridge.co.uk/wp-content/uploads/2024/04/Feeld321-1-300x186.jpg 300w, https://fortbridge.co.uk/wp-content/uploads/2024/04/Feeld321-1-768x477.jpg 768w" data-lazy-src="https://fortbridge.co.uk/wp-content/uploads/2024/04/Feeld321-1.jpg"></a></figure>



<p>3.If we again read this chat as the attacker, using the previous vulnerability ‘Read other people’s messages’, we can see the url to the video, as seen below:</p>



<figure><a href="https://fortbridge.co.uk/wp-content/uploads/2024/04/Feeld322.jpg" data-slb-active="1" data-slb-asset="162284062" data-slb-internal="0" data-slb-group="2251" data-wpel-link="internal" rel="follow"><img loading="lazy" decoding="async" width="1015" height="587" src="https://fortbridge.co.uk/wp-content/uploads/2024/04/Feeld322.jpg" alt="Feeld vulnerability #3 - Unauthenticated access to other people's attachments (photos &amp; videos) from their chats
Fig 3.18" srcset="https://fortbridge.co.uk/wp-content/uploads/2024/04/Feeld322.jpg 1015w, https://fortbridge.co.uk/wp-content/uploads/2024/04/Feeld322-300x173.jpg 300w, https://fortbridge.co.uk/wp-content/uploads/2024/04/Feeld322-768x444.jpg 768w" sizes="(max-width: 1015px) 100vw, 1015px" data-old-src="data:image/svg+xml,%3Csvg%20xmlns='http://www.w3.org/2000/svg'%20viewBox='0%200%201015%20587'%3E%3C/svg%3E" data-lazy-srcset="https://fortbridge.co.uk/wp-content/uploads/2024/04/Feeld322.jpg 1015w, https://fortbridge.co.uk/wp-content/uploads/2024/04/Feeld322-300x173.jpg 300w, https://fortbridge.co.uk/wp-content/uploads/2024/04/Feeld322-768x444.jpg 768w" data-lazy-src="https://fortbridge.co.uk/wp-content/uploads/2024/04/Feeld322.jpg"></a></figure>



<p>4.The url can be seen in the response. We have to replace ‘\u0026’ for ‘&amp;’ in it.<br>Now we can go to the above url unauthenticated.</p>



<figure><a href="https://fortbridge.co.uk/wp-content/uploads/2024/04/Feeld323.jpg" data-slb-active="1" data-slb-asset="1429746889" data-slb-internal="0" data-slb-group="2251" data-wpel-link="internal" rel="follow"><img loading="lazy" decoding="async" width="839" height="603" src="https://fortbridge.co.uk/wp-content/uploads/2024/04/Feeld323.jpg" alt="Feeld vulnerability #3 - Unauthenticated access to other people's attachments (photos &amp; videos) from their chats
Fig 3.19" srcset="https://fortbridge.co.uk/wp-content/uploads/2024/04/Feeld323.jpg 839w, https://fortbridge.co.uk/wp-content/uploads/2024/04/Feeld323-300x216.jpg 300w, https://fortbridge.co.uk/wp-content/uploads/2024/04/Feeld323-768x552.jpg 768w" sizes="(max-width: 839px) 100vw, 839px" data-old-src="data:image/svg+xml,%3Csvg%20xmlns='http://www.w3.org/2000/svg'%20viewBox='0%200%20839%20603'%3E%3C/svg%3E" data-lazy-srcset="https://fortbridge.co.uk/wp-content/uploads/2024/04/Feeld323.jpg 839w, https://fortbridge.co.uk/wp-content/uploads/2024/04/Feeld323-300x216.jpg 300w, https://fortbridge.co.uk/wp-content/uploads/2024/04/Feeld323-768x552.jpg 768w" data-lazy-src="https://fortbridge.co.uk/wp-content/uploads/2024/04/Feeld323.jpg"></a></figure>



<p><strong>Instance 4</strong>: Uploading ‘play-once’ videos</p>



<p>1.Upload a video, as in instance 3, but set it to ‘play once’.<br>The following requests will be made. The video will be uploaded to: <a href="http://chat.stream-io-api.com/" data-wpel-link="external" rel="external noopener noreferrer">ch</a><a href="https://chat.stream-io-api.com/" data-wpel-link="external" rel="external noopener noreferrer">at.stream-io-api.com</a> as seen below:</p>



<figure><a href="https://fortbridge.co.uk/wp-content/uploads/2024/04/Feeld340.jpg" data-slb-active="1" data-slb-asset="796268359" data-slb-internal="0" data-slb-group="2251" data-wpel-link="internal" rel="follow"><img loading="lazy" decoding="async" width="974" height="622" src="https://fortbridge.co.uk/wp-content/uploads/2024/04/Feeld340.jpg" alt="Feeld vulnerability #3 - Unauthenticated access to other people's attachments (photos &amp; videos) from their chats
Fig 3.20" srcset="https://fortbridge.co.uk/wp-content/uploads/2024/04/Feeld340.jpg 974w, https://fortbridge.co.uk/wp-content/uploads/2024/04/Feeld340-300x192.jpg 300w, https://fortbridge.co.uk/wp-content/uploads/2024/04/Feeld340-768x490.jpg 768w" sizes="(max-width: 974px) 100vw, 974px" data-old-src="data:image/svg+xml,%3Csvg%20xmlns='http://www.w3.org/2000/svg'%20viewBox='0%200%20974%20622'%3E%3C/svg%3E" data-lazy-srcset="https://fortbridge.co.uk/wp-content/uploads/2024/04/Feeld340.jpg 974w, https://fortbridge.co.uk/wp-content/uploads/2024/04/Feeld340-300x192.jpg 300w, https://fortbridge.co.uk/wp-content/uploads/2024/04/Feeld340-768x490.jpg 768w" data-lazy-src="https://fortbridge.co.uk/wp-content/uploads/2024/04/Feeld340.jpg"></a></figure>



<p>2.The returned url will also be sent in the chat in a subsequent request, as seen below:</p>



<figure><a href="https://fortbridge.co.uk/wp-content/uploads/2024/04/Feeld341.jpg" data-slb-active="1" data-slb-asset="1704355951" data-slb-internal="0" data-slb-group="2251" data-wpel-link="internal" rel="follow"><img loading="lazy" decoding="async" width="985" height="572" src="https://fortbridge.co.uk/wp-content/uploads/2024/04/Feeld341.jpg" alt="Feeld vulnerability #3 - Unauthenticated access to other people's attachments (photos &amp; videos) from their chats
Fig 3.21" srcset="https://fortbridge.co.uk/wp-content/uploads/2024/04/Feeld341.jpg 985w, https://fortbridge.co.uk/wp-content/uploads/2024/04/Feeld341-300x174.jpg 300w, https://fortbridge.co.uk/wp-content/uploads/2024/04/Feeld341-768x446.jpg 768w" sizes="(max-width: 985px) 100vw, 985px" data-old-src="data:image/svg+xml,%3Csvg%20xmlns='http://www.w3.org/2000/svg'%20viewBox='0%200%20985%20572'%3E%3C/svg%3E" data-lazy-srcset="https://fortbridge.co.uk/wp-content/uploads/2024/04/Feeld341.jpg 985w, https://fortbridge.co.uk/wp-content/uploads/2024/04/Feeld341-300x174.jpg 300w, https://fortbridge.co.uk/wp-content/uploads/2024/04/Feeld341-768x446.jpg 768w" data-lazy-src="https://fortbridge.co.uk/wp-content/uploads/2024/04/Feeld341.jpg"></a></figure>



<p>3.Now, an attacker can read our chat using the previous vulnerability ‘Read other people’s chat’ and extract this url, as seen below.</p>



<figure><a href="https://fortbridge.co.uk/wp-content/uploads/2024/04/Feeld342.jpg" data-slb-active="1" data-slb-asset="945673805" data-slb-internal="0" data-slb-group="2251" data-wpel-link="internal" rel="follow"><img loading="lazy" decoding="async" width="1020" height="586" src="https://fortbridge.co.uk/wp-content/uploads/2024/04/Feeld342.jpg" alt="Feeld vulnerability #3 - Unauthenticated access to other people's attachments (photos &amp; videos) from their chats
Fig 3.22" srcset="https://fortbridge.co.uk/wp-content/uploads/2024/04/Feeld342.jpg 1020w, https://fortbridge.co.uk/wp-content/uploads/2024/04/Feeld342-300x172.jpg 300w, https://fortbridge.co.uk/wp-content/uploads/2024/04/Feeld342-768x441.jpg 768w" sizes="(max-width: 1020px) 100vw, 1020px" data-old-src="data:image/svg+xml,%3Csvg%20xmlns='http://www.w3.org/2000/svg'%20viewBox='0%200%201020%20586'%3E%3C/svg%3E" data-lazy-srcset="https://fortbridge.co.uk/wp-content/uploads/2024/04/Feeld342.jpg 1020w, https://fortbridge.co.uk/wp-content/uploads/2024/04/Feeld342-300x172.jpg 300w, https://fortbridge.co.uk/wp-content/uploads/2024/04/Feeld342-768x441.jpg 768w" data-lazy-src="https://fortbridge.co.uk/wp-content/uploads/2024/04/Feeld342.jpg"></a></figure>



<p>4.The url can be extracted and \u0026 character replaced with &amp;. Thus, we can watch the video unauthenticated:</p>



<figure><a href="https://fortbridge.co.uk/wp-content/uploads/2024/04/Feeld343.jpg" data-slb-active="1" data-slb-asset="1209286736" data-slb-internal="0" data-slb-group="2251" data-wpel-link="internal" rel="follow"><img loading="lazy" decoding="async" width="754" height="599" src="https://fortbridge.co.uk/wp-content/uploads/2024/04/Feeld343.jpg" alt="Feeld vulnerability #3 - Unauthenticated access to other people's attachments (photos &amp; videos) from their chats
Fig 3.23" srcset="https://fortbridge.co.uk/wp-content/uploads/2024/04/Feeld343.jpg 754w, https://fortbridge.co.uk/wp-content/uploads/2024/04/Feeld343-300x238.jpg 300w" sizes="(max-width: 754px) 100vw, 754px" data-old-src="data:image/svg+xml,%3Csvg%20xmlns='http://www.w3.org/2000/svg'%20viewBox='0%200%20754%20599'%3E%3C/svg%3E" data-lazy-srcset="https://fortbridge.co.uk/wp-content/uploads/2024/04/Feeld343.jpg 754w, https://fortbridge.co.uk/wp-content/uploads/2024/04/Feeld343-300x238.jpg 300w" data-lazy-src="https://fortbridge.co.uk/wp-content/uploads/2024/04/Feeld343.jpg"></a></figure>



<p>The video is available unauthenticated and is replay-able. <br>The receiver of the ‘play-once’ video, will have no knowledge of the attack. He can still see the video, but only once. After he sees the video, it will say ‘video expired’.</p>



<figure><a href="https://fortbridge.co.uk/wp-content/uploads/2024/04/Feeld344.jpg" data-slb-active="1" data-slb-asset="1775204092" data-slb-internal="0" data-slb-group="2251" data-wpel-link="internal" rel="follow"><img loading="lazy" decoding="async" width="927" height="958" src="https://fortbridge.co.uk/wp-content/uploads/2024/04/Feeld344.jpg" alt="Feeld vulnerability #3 - Unauthenticated access to other people's attachments (photos &amp; videos) from their chats
Fig 3.24" srcset="https://fortbridge.co.uk/wp-content/uploads/2024/04/Feeld344.jpg 927w, https://fortbridge.co.uk/wp-content/uploads/2024/04/Feeld344-290x300.jpg 290w, https://fortbridge.co.uk/wp-content/uploads/2024/04/Feeld344-768x794.jpg 768w" sizes="(max-width: 927px) 100vw, 927px" data-old-src="data:image/svg+xml,%3Csvg%20xmlns='http://www.w3.org/2000/svg'%20viewBox='0%200%20927%20958'%3E%3C/svg%3E" data-lazy-srcset="https://fortbridge.co.uk/wp-content/uploads/2024/04/Feeld344.jpg 927w, https://fortbridge.co.uk/wp-content/uploads/2024/04/Feeld344-290x300.jpg 290w, https://fortbridge.co.uk/wp-content/uploads/2024/04/Feeld344-768x794.jpg 768w" data-lazy-src="https://fortbridge.co.uk/wp-content/uploads/2024/04/Feeld344.jpg"></a><figcaption>‘Before’ and ‘after’ the receiver sees the video once. </figcaption></figure>



<h3 id="h-4-delete-recover-and-edit-other-people-s-messages"><strong>4. Delete, recover and edit other people’s messages</strong></h3>



<p><strong>Details</strong>: We discovered that we can recover other people’s messages that were deleted in a chat. In addition, we can edit and delete other people’s messages.<br>In order to do that, we will need the unique ‘messageId’ value of the message that we want to recover. This is easy to get because when we read our victim’s messages, each message has its messageId next to it.</p>



<p><strong>Instance</strong>: <a href="https://chat.stream-io-api.com/messages/f962bef0-6e9b-4f17-9def-6238a7933abf" data-wpel-link="external" rel="external noopener noreferrer">https://chat.stream-io-api.com/messages/f962bef0-6e9b-4f17-9def-6238a7933abf</a> (Methods: DELETE and PUT)</p>



<p><strong>Reproduction steps</strong>:</p>



<p>1.Use a proxy tool such as Burp to intercept the traffic.<br>2.Enter a chat and leave a message to someone:</p>



<figure><a href="https://fortbridge.co.uk/wp-content/uploads/2024/04/Feeld410.jpg" data-slb-active="1" data-slb-asset="1773535641" data-slb-internal="0" data-slb-group="2251" data-wpel-link="internal" rel="follow"><img loading="lazy" decoding="async" width="994" height="612" src="https://fortbridge.co.uk/wp-content/uploads/2024/04/Feeld410.jpg" alt="Feeld vulnerability #4 - Delete, recover and edit other people's messages
Fig 4.0" srcset="https://fortbridge.co.uk/wp-content/uploads/2024/04/Feeld410.jpg 994w, https://fortbridge.co.uk/wp-content/uploads/2024/04/Feeld410-300x185.jpg 300w, https://fortbridge.co.uk/wp-content/uploads/2024/04/Feeld410-768x473.jpg 768w" sizes="(max-width: 994px) 100vw, 994px" data-old-src="data:image/svg+xml,%3Csvg%20xmlns='http://www.w3.org/2000/svg'%20viewBox='0%200%20994%20612'%3E%3C/svg%3E" data-lazy-srcset="https://fortbridge.co.uk/wp-content/uploads/2024/04/Feeld410.jpg 994w, https://fortbridge.co.uk/wp-content/uploads/2024/04/Feeld410-300x185.jpg 300w, https://fortbridge.co.uk/wp-content/uploads/2024/04/Feeld410-768x473.jpg 768w" data-lazy-src="https://fortbridge.co.uk/wp-content/uploads/2024/04/Feeld410.jpg"></a></figure>



<p>3.Delete the message:</p>



<figure><a href="https://fortbridge.co.uk/wp-content/uploads/2024/04/Feeld411.jpg" data-slb-active="1" data-slb-asset="660965136" data-slb-internal="0" data-slb-group="2251" data-wpel-link="internal" rel="follow"><img loading="lazy" decoding="async" width="933" height="576" src="https://fortbridge.co.uk/wp-content/uploads/2024/04/Feeld411.jpg" alt="Feeld vulnerability #4 - Delete, recover and edit other people's messages
Fig 4.1" srcset="https://fortbridge.co.uk/wp-content/uploads/2024/04/Feeld411.jpg 933w, https://fortbridge.co.uk/wp-content/uploads/2024/04/Feeld411-300x185.jpg 300w, https://fortbridge.co.uk/wp-content/uploads/2024/04/Feeld411-768x474.jpg 768w" sizes="(max-width: 933px) 100vw, 933px" data-old-src="data:image/svg+xml,%3Csvg%20xmlns='http://www.w3.org/2000/svg'%20viewBox='0%200%20933%20576'%3E%3C/svg%3E" data-lazy-srcset="https://fortbridge.co.uk/wp-content/uploads/2024/04/Feeld411.jpg 933w, https://fortbridge.co.uk/wp-content/uploads/2024/04/Feeld411-300x185.jpg 300w, https://fortbridge.co.uk/wp-content/uploads/2024/04/Feeld411-768x474.jpg 768w" data-lazy-src="https://fortbridge.co.uk/wp-content/uploads/2024/04/Feeld411.jpg"></a></figure>



<p>4.Now let’s read the chat as the attacker user using the above vulnerability ‘Read other people’s messages’.<br>It will say, ‘This message was deleted’, as seen below:</p>



<figure><a href="https://fortbridge.co.uk/wp-content/uploads/2024/04/Feeld412.png" data-slb-active="1" data-slb-asset="305331711" data-slb-internal="0" data-slb-group="2251" data-wpel-link="internal" rel="follow"><img loading="lazy" decoding="async" width="1020" height="588" src="https://fortbridge.co.uk/wp-content/uploads/2024/04/Feeld412.png" alt="Feeld vulnerability #4 - Delete, recover and edit other people's messages
Fig 4.2" srcset="https://fortbridge.co.uk/wp-content/uploads/2024/04/Feeld412.png 1020w, https://fortbridge.co.uk/wp-content/uploads/2024/04/Feeld412-300x173.png 300w, https://fortbridge.co.uk/wp-content/uploads/2024/04/Feeld412-768x443.png 768w" sizes="(max-width: 1020px) 100vw, 1020px" data-old-src="data:image/svg+xml,%3Csvg%20xmlns='http://www.w3.org/2000/svg'%20viewBox='0%200%201020%20588'%3E%3C/svg%3E" data-lazy-srcset="https://fortbridge.co.uk/wp-content/uploads/2024/04/Feeld412.png 1020w, https://fortbridge.co.uk/wp-content/uploads/2024/04/Feeld412-300x173.png 300w, https://fortbridge.co.uk/wp-content/uploads/2024/04/Feeld412-768x443.png 768w" data-lazy-src="https://fortbridge.co.uk/wp-content/uploads/2024/04/Feeld412.png"></a></figure>



<p>5.Now if we call the same DELETE request, as the attacker, we will get back the original message:</p>



<figure><a href="https://fortbridge.co.uk/wp-content/uploads/2024/04/Feeld413.png" data-slb-active="1" data-slb-asset="956224169" data-slb-internal="0" data-slb-group="2251" data-wpel-link="internal" rel="follow"><img loading="lazy" decoding="async" width="1020" height="590" src="https://fortbridge.co.uk/wp-content/uploads/2024/04/Feeld413.png" alt="Feeld vulnerability #4 - Delete, recover and edit other people's messages
Fig 4.3" srcset="https://fortbridge.co.uk/wp-content/uploads/2024/04/Feeld413.png 1020w, https://fortbridge.co.uk/wp-content/uploads/2024/04/Feeld413-300x174.png 300w, https://fortbridge.co.uk/wp-content/uploads/2024/04/Feeld413-768x444.png 768w" sizes="(max-width: 1020px) 100vw, 1020px" data-old-src="data:image/svg+xml,%3Csvg%20xmlns='http://www.w3.org/2000/svg'%20viewBox='0%200%201020%20590'%3E%3C/svg%3E" data-lazy-srcset="https://fortbridge.co.uk/wp-content/uploads/2024/04/Feeld413.png 1020w, https://fortbridge.co.uk/wp-content/uploads/2024/04/Feeld413-300x174.png 300w, https://fortbridge.co.uk/wp-content/uploads/2024/04/Feeld413-768x444.png 768w" data-lazy-src="https://fortbridge.co.uk/wp-content/uploads/2024/04/Feeld413.png"></a></figure>



<p><strong>Instance 2</strong>: Edit a message, as a different user than the participants in the chat</p>



<p>1.Let’s send a message and intercept the request:</p>



<figure><a href="https://fortbridge.co.uk/wp-content/uploads/2024/04/Feeld420.png" data-slb-active="1" data-slb-asset="1475535291" data-slb-internal="0" data-slb-group="2251" data-wpel-link="internal" rel="follow"><img loading="lazy" decoding="async" width="1020" height="554" src="https://fortbridge.co.uk/wp-content/uploads/2024/04/Feeld420.png" alt="Feeld vulnerability #4 - Delete, recover and edit other people's messages
Fig 4.4" srcset="https://fortbridge.co.uk/wp-content/uploads/2024/04/Feeld420.png 1020w, https://fortbridge.co.uk/wp-content/uploads/2024/04/Feeld420-300x163.png 300w, https://fortbridge.co.uk/wp-content/uploads/2024/04/Feeld420-768x417.png 768w" sizes="(max-width: 1020px) 100vw, 1020px" data-old-src="data:image/svg+xml,%3Csvg%20xmlns='http://www.w3.org/2000/svg'%20viewBox='0%200%201020%20554'%3E%3C/svg%3E" data-lazy-srcset="https://fortbridge.co.uk/wp-content/uploads/2024/04/Feeld420.png 1020w, https://fortbridge.co.uk/wp-content/uploads/2024/04/Feeld420-300x163.png 300w, https://fortbridge.co.uk/wp-content/uploads/2024/04/Feeld420-768x417.png 768w" data-lazy-src="https://fortbridge.co.uk/wp-content/uploads/2024/04/Feeld420.png"></a></figure>



<p>2.And let’s use the previous vulnerability to ‘Read other people’s messages’ as the attacker, in order to find the messageID (‘0fec78e9-0068-48f1-8563-7144474cc7e2’):</p>



<figure><a href="https://fortbridge.co.uk/wp-content/uploads/2024/04/Feeld421.png" data-slb-active="1" data-slb-asset="2054044557" data-slb-internal="0" data-slb-group="2251" data-wpel-link="internal" rel="follow"><img loading="lazy" decoding="async" width="1020" height="562" src="https://fortbridge.co.uk/wp-content/uploads/2024/04/Feeld421.png" alt="Feeld vulnerability #4 - Delete, recover and edit other people's messages
Fig 4.5" srcset="https://fortbridge.co.uk/wp-content/uploads/2024/04/Feeld421.png 1020w, https://fortbridge.co.uk/wp-content/uploads/2024/04/Feeld421-300x165.png 300w, https://fortbridge.co.uk/wp-content/uploads/2024/04/Feeld421-768x423.png 768w" sizes="(max-width: 1020px) 100vw, 1020px" data-old-src="data:image/svg+xml,%3Csvg%20xmlns='http://www.w3.org/2000/svg'%20viewBox='0%200%201020%20562'%3E%3C/svg%3E" data-lazy-srcset="https://fortbridge.co.uk/wp-content/uploads/2024/04/Feeld421.png 1020w, https://fortbridge.co.uk/wp-content/uploads/2024/04/Feeld421-300x165.png 300w, https://fortbridge.co.uk/wp-content/uploads/2024/04/Feeld421-768x423.png 768w" data-lazy-src="https://fortbridge.co.uk/wp-content/uploads/2024/04/Feeld421.png"></a></figure>



<p>3.The victim will receive a notification:</p>



<figure><a href="https://fortbridge.co.uk/wp-content/uploads/2024/04/Feeld422.png" data-slb-active="1" data-slb-asset="1268902116" data-slb-internal="0" data-slb-group="2251" data-wpel-link="internal" rel="follow"><img loading="lazy" decoding="async" width="336" height="724" src="https://fortbridge.co.uk/wp-content/uploads/2024/04/Feeld422.png" alt="Feeld vulnerability #4 - Delete, recover and edit other people's messages
Fig 4.6" srcset="https://fortbridge.co.uk/wp-content/uploads/2024/04/Feeld422.png 336w, https://fortbridge.co.uk/wp-content/uploads/2024/04/Feeld422-139x300.png 139w" sizes="(max-width: 336px) 100vw, 336px" data-old-src="data:image/svg+xml,%3Csvg%20xmlns='http://www.w3.org/2000/svg'%20viewBox='0%200%20336%20724'%3E%3C/svg%3E" data-lazy-srcset="https://fortbridge.co.uk/wp-content/uploads/2024/04/Feeld422.png 336w, https://fortbridge.co.uk/wp-content/uploads/2024/04/Feeld422-139x300.png 139w" data-lazy-src="https://fortbridge.co.uk/wp-content/uploads/2024/04/Feeld422.png"></a></figure>



<p>4.Edit the message as the attacker, using the messageID and the method PUT on the same endpoint:</p>



<figure><a href="https://fortbridge.co.uk/wp-content/uploads/2024/04/Feeld423.png" data-slb-active="1" data-slb-asset="1748257757" data-slb-internal="0" data-slb-group="2251" data-wpel-link="internal" rel="follow"><img loading="lazy" decoding="async" width="1000" height="522" src="https://fortbridge.co.uk/wp-content/uploads/2024/04/Feeld423.png" alt="Feeld vulnerability #4 - Delete, recover and edit other people's messages
Fig 4.7" srcset="https://fortbridge.co.uk/wp-content/uploads/2024/04/Feeld423.png 1000w, https://fortbridge.co.uk/wp-content/uploads/2024/04/Feeld423-300x157.png 300w, https://fortbridge.co.uk/wp-content/uploads/2024/04/Feeld423-768x401.png 768w" sizes="(max-width: 1000px) 100vw, 1000px" data-old-src="data:image/svg+xml,%3Csvg%20xmlns='http://www.w3.org/2000/svg'%20viewBox='0%200%201000%20522'%3E%3C/svg%3E" data-lazy-srcset="https://fortbridge.co.uk/wp-content/uploads/2024/04/Feeld423.png 1000w, https://fortbridge.co.uk/wp-content/uploads/2024/04/Feeld423-300x157.png 300w, https://fortbridge.co.uk/wp-content/uploads/2024/04/Feeld423-768x401.png 768w" data-lazy-src="https://fortbridge.co.uk/wp-content/uploads/2024/04/Feeld423.png"></a></figure>



<p>5.When the victim taps on the notification from the above step 3, he will see the following message set by the attacker in step 4. There will be an ‘edited’ sign below the actual message but there are no signs of who did the edit. In addition, every account name is not unique and the attacker could choose any name possible. </p>



<figure><a href="https://fortbridge.co.uk/wp-content/uploads/2024/04/Feeld424.png" data-slb-active="1" data-slb-asset="820516206" data-slb-internal="0" data-slb-group="2251" data-wpel-link="internal" rel="follow"><img loading="lazy" decoding="async" width="354" height="766" src="https://fortbridge.co.uk/wp-content/uploads/2024/04/Feeld424.png" alt="Feeld vulnerability #4 - Delete, recover and edit other people's messages
Fig 4.8" srcset="https://fortbridge.co.uk/wp-content/uploads/2024/04/Feeld424.png 354w, https://fortbridge.co.uk/wp-content/uploads/2024/04/Feeld424-139x300.png 139w" sizes="(max-width: 354px) 100vw, 354px" data-old-src="data:image/svg+xml,%3Csvg%20xmlns='http://www.w3.org/2000/svg'%20viewBox='0%200%20354%20766'%3E%3C/svg%3E" data-lazy-srcset="https://fortbridge.co.uk/wp-content/uploads/2024/04/Feeld424.png 354w, https://fortbridge.co.uk/wp-content/uploads/2024/04/Feeld424-139x300.png 139w" data-lazy-src="https://fortbridge.co.uk/wp-content/uploads/2024/04/Feeld424.png"></a></figure>



<p><strong>Instance 3</strong>: Delete other people’s messages</p>



<p>1.This is the exact same endpoint as in Instance 1.<br>2.We will delete the above message as the attacker, who is not a participant in the chat, as seen below, using the unique messageID and the method DELETE:</p>



<figure><a href="https://fortbridge.co.uk/wp-content/uploads/2024/04/Feeld430.png" data-slb-active="1" data-slb-asset="1022053670" data-slb-internal="0" data-slb-group="2251" data-wpel-link="internal" rel="follow"><img loading="lazy" decoding="async" width="1020" height="522" src="https://fortbridge.co.uk/wp-content/uploads/2024/04/Feeld430.png" alt="Feeld vulnerability #4 - Delete, recover and edit other people's messages
Fig 4.9" srcset="https://fortbridge.co.uk/wp-content/uploads/2024/04/Feeld430.png 1020w, https://fortbridge.co.uk/wp-content/uploads/2024/04/Feeld430-300x154.png 300w, https://fortbridge.co.uk/wp-content/uploads/2024/04/Feeld430-768x393.png 768w" sizes="(max-width: 1020px) 100vw, 1020px" data-old-src="data:image/svg+xml,%3Csvg%20xmlns='http://www.w3.org/2000/svg'%20viewBox='0%200%201020%20522'%3E%3C/svg%3E" data-lazy-srcset="https://fortbridge.co.uk/wp-content/uploads/2024/04/Feeld430.png 1020w, https://fortbridge.co.uk/wp-content/uploads/2024/04/Feeld430-300x154.png 300w, https://fortbridge.co.uk/wp-content/uploads/2024/04/Feeld430-768x393.png 768w" data-lazy-src="https://fortbridge.co.uk/wp-content/uploads/2024/04/Feeld430.png"></a></figure>



<figure><a href="https://fortbridge.co.uk/wp-content/uploads/2024/04/Feeld431.png" data-slb-active="1" data-slb-asset="1538699948" data-slb-internal="0" data-slb-group="2251" data-wpel-link="internal" rel="follow"><img loading="lazy" decoding="async" width="374" height="818" src="https://fortbridge.co.uk/wp-content/uploads/2024/04/Feeld431.png" alt="Feeld vulnerability #4 - Delete, recover and edit other people's messages
Fig 4.10" srcset="https://fortbridge.co.uk/wp-content/uploads/2024/04/Feeld431.png 374w, https://fortbridge.co.uk/wp-content/uploads/2024/04/Feeld431-137x300.png 137w" sizes="(max-width: 374px) 100vw, 374px" data-old-src="data:image/svg+xml,%3Csvg%20xmlns='http://www.w3.org/2000/svg'%20viewBox='0%200%20374%20818'%3E%3C/svg%3E" data-lazy-srcset="https://fortbridge.co.uk/wp-content/uploads/2024/04/Feeld431.png 374w, https://fortbridge.co.uk/wp-content/uploads/2024/04/Feeld431-137x300.png 137w" data-lazy-src="https://fortbridge.co.uk/wp-content/uploads/2024/04/Feeld431.png"></a></figure>



<h3 id="h-5-update-someone-else-s-profile-information"><strong>5. Update someone else’s profile information</strong></h3>



<p><strong>Details</strong>: We discovered you can update someone else’s profile information, including name, sexuality, age, etc.</p>



<p><strong>Instance</strong>: https://core.api.feeld.co/graphql (“operationName”:”ProfileUpdate”, vulnerable parameter ‘id’)</p>



<p><strong>Reproduction steps</strong>:</p>



<p>1.Let’s login the mobile application as the ‘attacker’ and go to the ‘Profile’ – ‘Edit Profile’ menu.<br>2.Edit 1 thing on the profile such as ‘bio’, save the change, and intercept the /graphql request with operationName: ‘ProfileUpdate’.<br>3.Modify in the intercepted request the ‘id’ parameter and add the id of your victim. In addition, add the parameters that you want to update, such as ‘bio’.</p>



<figure><a href="https://fortbridge.co.uk/wp-content/uploads/2024/04/Feeld50.png" data-slb-active="1" data-slb-asset="2105289218" data-slb-internal="0" data-slb-group="2251" data-wpel-link="internal" rel="follow"><img loading="lazy" decoding="async" width="1020" height="556" src="https://fortbridge.co.uk/wp-content/uploads/2024/04/Feeld50.png" alt="Feeld vulnerability #5 - Update someone else's profile information
Fig 5.0" srcset="https://fortbridge.co.uk/wp-content/uploads/2024/04/Feeld50.png 1020w, https://fortbridge.co.uk/wp-content/uploads/2024/04/Feeld50-300x164.png 300w, https://fortbridge.co.uk/wp-content/uploads/2024/04/Feeld50-768x419.png 768w" sizes="(max-width: 1020px) 100vw, 1020px" data-old-src="data:image/svg+xml,%3Csvg%20xmlns='http://www.w3.org/2000/svg'%20viewBox='0%200%201020%20556'%3E%3C/svg%3E" data-lazy-srcset="https://fortbridge.co.uk/wp-content/uploads/2024/04/Feeld50.png 1020w, https://fortbridge.co.uk/wp-content/uploads/2024/04/Feeld50-300x164.png 300w, https://fortbridge.co.uk/wp-content/uploads/2024/04/Feeld50-768x419.png 768w" data-lazy-src="https://fortbridge.co.uk/wp-content/uploads/2024/04/Feeld50.png"></a></figure>



<h3 id="h-6-get-a-like-from-any-user-profile"><strong>6. Get a ‘Like’ from any user profile</strong></h3>



<p><strong>Details</strong>: We discovered that users could send ‘Likes’ from profile#2 to profile#3 while logged in as profile#1.</p>



<p><strong>Instance</strong>: https://core.api.feeld.co/graphql (OperationName: ProfileLike)</p>



<p><strong>Reproduction steps</strong>:</p>



<p>1.Below is a request to send a normal ‘Like’, from user with profileId ending in ‘…9c’ to profileId ‘…1f’, and the successful response:</p>



<figure><a href="https://fortbridge.co.uk/wp-content/uploads/2024/04/Feeld60.png" data-slb-active="1" data-slb-asset="632449184" data-slb-internal="0" data-slb-group="2251" data-wpel-link="internal" rel="follow"><img loading="lazy" decoding="async" width="1020" height="570" src="https://fortbridge.co.uk/wp-content/uploads/2024/04/Feeld60.png" alt="Feeld vulnerability #6 - Get a 'Like' from a any profile
Fig 6.0" srcset="https://fortbridge.co.uk/wp-content/uploads/2024/04/Feeld60.png 1020w, https://fortbridge.co.uk/wp-content/uploads/2024/04/Feeld60-300x168.png 300w, https://fortbridge.co.uk/wp-content/uploads/2024/04/Feeld60-768x429.png 768w" sizes="(max-width: 1020px) 100vw, 1020px" data-old-src="data:image/svg+xml,%3Csvg%20xmlns='http://www.w3.org/2000/svg'%20viewBox='0%200%201020%20570'%3E%3C/svg%3E" data-lazy-srcset="https://fortbridge.co.uk/wp-content/uploads/2024/04/Feeld60.png 1020w, https://fortbridge.co.uk/wp-content/uploads/2024/04/Feeld60-300x168.png 300w, https://fortbridge.co.uk/wp-content/uploads/2024/04/Feeld60-768x429.png 768w" data-lazy-src="https://fortbridge.co.uk/wp-content/uploads/2024/04/Feeld60.png"></a></figure>



<p>2.Below is the request and response with a reverse like, from ‘…1f’ to ‘…9c’, which errors:</p>



<figure><a href="https://fortbridge.co.uk/wp-content/uploads/2024/04/Feeld61.png" data-slb-active="1" data-slb-asset="317383413" data-slb-internal="0" data-slb-group="2251" data-wpel-link="internal" rel="follow"><img loading="lazy" decoding="async" width="972" height="532" src="https://fortbridge.co.uk/wp-content/uploads/2024/04/Feeld61.png" alt="Feeld vulnerability #6 - Get a 'Like' from a any profile
Fig 6.1" srcset="https://fortbridge.co.uk/wp-content/uploads/2024/04/Feeld61.png 972w, https://fortbridge.co.uk/wp-content/uploads/2024/04/Feeld61-300x164.png 300w, https://fortbridge.co.uk/wp-content/uploads/2024/04/Feeld61-768x420.png 768w" sizes="(max-width: 972px) 100vw, 972px" data-old-src="data:image/svg+xml,%3Csvg%20xmlns='http://www.w3.org/2000/svg'%20viewBox='0%200%20972%20532'%3E%3C/svg%3E" data-lazy-srcset="https://fortbridge.co.uk/wp-content/uploads/2024/04/Feeld61.png 972w, https://fortbridge.co.uk/wp-content/uploads/2024/04/Feeld61-300x164.png 300w, https://fortbridge.co.uk/wp-content/uploads/2024/04/Feeld61-768x420.png 768w" data-lazy-src="https://fortbridge.co.uk/wp-content/uploads/2024/04/Feeld61.png"></a></figure>



<p>3.Now, send a like from a random profile ‘….d3’ to one of our profiles ‘…1f’ , while logged in as user ‘…9c’:</p>



<figure><a href="https://fortbridge.co.uk/wp-content/uploads/2024/04/Feeld62.png" data-slb-active="1" data-slb-asset="61955025" data-slb-internal="0" data-slb-group="2251" data-wpel-link="internal" rel="follow"><img loading="lazy" decoding="async" width="1020" height="416" src="https://fortbridge.co.uk/wp-content/uploads/2024/04/Feeld62.png" alt="Feeld vulnerability #6 - Get a 'Like' from a any profile
Fig 6.2" srcset="https://fortbridge.co.uk/wp-content/uploads/2024/04/Feeld62.png 1020w, https://fortbridge.co.uk/wp-content/uploads/2024/04/Feeld62-300x122.png 300w, https://fortbridge.co.uk/wp-content/uploads/2024/04/Feeld62-768x313.png 768w" sizes="(max-width: 1020px) 100vw, 1020px" data-old-src="data:image/svg+xml,%3Csvg%20xmlns='http://www.w3.org/2000/svg'%20viewBox='0%200%201020%20416'%3E%3C/svg%3E" data-lazy-srcset="https://fortbridge.co.uk/wp-content/uploads/2024/04/Feeld62.png 1020w, https://fortbridge.co.uk/wp-content/uploads/2024/04/Feeld62-300x122.png 300w, https://fortbridge.co.uk/wp-content/uploads/2024/04/Feeld62-768x313.png 768w" data-lazy-src="https://fortbridge.co.uk/wp-content/uploads/2024/04/Feeld62.png"></a></figure>



<p>4.Now get the profile details (ImaginaryName) of that user with profileId ‘…d3’:</p>



<figure><a href="https://fortbridge.co.uk/wp-content/uploads/2024/04/Feeld63.png" data-slb-active="1" data-slb-asset="101406827" data-slb-internal="0" data-slb-group="2251" data-wpel-link="internal" rel="follow"><img loading="lazy" decoding="async" width="1020" height="558" src="https://fortbridge.co.uk/wp-content/uploads/2024/04/Feeld63.png" alt="Feeld vulnerability #6 - Get a 'Like' from a any profile
Fig 6.3" srcset="https://fortbridge.co.uk/wp-content/uploads/2024/04/Feeld63.png 1020w, https://fortbridge.co.uk/wp-content/uploads/2024/04/Feeld63-300x164.png 300w, https://fortbridge.co.uk/wp-content/uploads/2024/04/Feeld63-768x420.png 768w" sizes="(max-width: 1020px) 100vw, 1020px" data-old-src="data:image/svg+xml,%3Csvg%20xmlns='http://www.w3.org/2000/svg'%20viewBox='0%200%201020%20558'%3E%3C/svg%3E" data-lazy-srcset="https://fortbridge.co.uk/wp-content/uploads/2024/04/Feeld63.png 1020w, https://fortbridge.co.uk/wp-content/uploads/2024/04/Feeld63-300x164.png 300w, https://fortbridge.co.uk/wp-content/uploads/2024/04/Feeld63-768x420.png 768w" data-lazy-src="https://fortbridge.co.uk/wp-content/uploads/2024/04/Feeld63.png"></a></figure>



<p>5. Now, let’s check our list of likes in the app to see if we received a like from user ‘Anni’.<br>Given that we have a Premium account, we can view this information in the app. Indeed, we can see that we have received a ‘Like’ from ‘Anni’:</p>



<figure><a href="https://fortbridge.co.uk/wp-content/uploads/2024/04/Feeld64.png" data-slb-active="1" data-slb-asset="868116542" data-slb-internal="0" data-slb-group="2251" data-wpel-link="internal" rel="follow"><img loading="lazy" decoding="async" width="438" height="962" src="https://fortbridge.co.uk/wp-content/uploads/2024/04/Feeld64.png" alt="Feeld vulnerability #6 - Get a 'Like' from a any profile
Fig 6.4" srcset="https://fortbridge.co.uk/wp-content/uploads/2024/04/Feeld64.png 438w, https://fortbridge.co.uk/wp-content/uploads/2024/04/Feeld64-137x300.png 137w" sizes="(max-width: 438px) 100vw, 438px" data-old-src="data:image/svg+xml,%3Csvg%20xmlns='http://www.w3.org/2000/svg'%20viewBox='0%200%20438%20962'%3E%3C/svg%3E" data-lazy-srcset="https://fortbridge.co.uk/wp-content/uploads/2024/04/Feeld64.png 438w, https://fortbridge.co.uk/wp-content/uploads/2024/04/Feeld64-137x300.png 137w" data-lazy-src="https://fortbridge.co.uk/wp-content/uploads/2024/04/Feeld64.png"></a></figure>



<h3 id="h-7-send-messages-in-other-people-s-chat"><strong>7. Send messages in other people’s chat</strong></h3>



<p><strong>Details</strong>: We discovered that we can send messages to other people’s chats, even though we are not a participant in that chat.</p>



<p><strong>Instance</strong>: https://chat.stream-io-api.com/channels/messaging/50dd83b1-9dda-4940-b6bb-04891e9500bd/message&nbsp;(Method: POST)</p>



<p><strong>Reproduction steps</strong>:</p>



<p>1.Use the previous vulnerability ‘Read other people’s messages’ to find the unique channelID where you want to add your message, such as the the one shown below:<br>’50dd83b1-9dda-4940-b6bb-04891e9500bd’:<br>Add this channelID to the request path when you exploit this issue in step2 .</p>



<figure><a href="https://fortbridge.co.uk/wp-content/uploads/2024/04/Feeld70.png" data-slb-active="1" data-slb-asset="2119606139" data-slb-internal="0" data-slb-group="2251" data-wpel-link="internal" rel="follow"><img loading="lazy" decoding="async" width="1020" height="660" src="https://fortbridge.co.uk/wp-content/uploads/2024/04/Feeld70.png" alt="Feeld vulnerability #7 - Send messages in other people's chat
Fig 7.0" srcset="https://fortbridge.co.uk/wp-content/uploads/2024/04/Feeld70.png 1020w, https://fortbridge.co.uk/wp-content/uploads/2024/04/Feeld70-300x194.png 300w, https://fortbridge.co.uk/wp-content/uploads/2024/04/Feeld70-768x497.png 768w" sizes="(max-width: 1020px) 100vw, 1020px" data-old-src="data:image/svg+xml,%3Csvg%20xmlns='http://www.w3.org/2000/svg'%20viewBox='0%200%201020%20660'%3E%3C/svg%3E" data-lazy-srcset="https://fortbridge.co.uk/wp-content/uploads/2024/04/Feeld70.png 1020w, https://fortbridge.co.uk/wp-content/uploads/2024/04/Feeld70-300x194.png 300w, https://fortbridge.co.uk/wp-content/uploads/2024/04/Feeld70-768x497.png 768w" data-lazy-src="https://fortbridge.co.uk/wp-content/uploads/2024/04/Feeld70.png"></a></figure>



<p>2.Send a message to that channel id:</p>



<figure><a href="https://fortbridge.co.uk/wp-content/uploads/2024/04/Feeld71.png" data-slb-active="1" data-slb-asset="1604116117" data-slb-internal="0" data-slb-group="2251" data-wpel-link="internal" rel="follow"><img loading="lazy" decoding="async" width="1020" height="542" src="https://fortbridge.co.uk/wp-content/uploads/2024/04/Feeld71.png" alt="Feeld vulnerability #7 - Send messages in other people's chat
Fig 7.1" srcset="https://fortbridge.co.uk/wp-content/uploads/2024/04/Feeld71.png 1020w, https://fortbridge.co.uk/wp-content/uploads/2024/04/Feeld71-300x159.png 300w, https://fortbridge.co.uk/wp-content/uploads/2024/04/Feeld71-768x408.png 768w" sizes="(max-width: 1020px) 100vw, 1020px" data-old-src="data:image/svg+xml,%3Csvg%20xmlns='http://www.w3.org/2000/svg'%20viewBox='0%200%201020%20542'%3E%3C/svg%3E" data-lazy-srcset="https://fortbridge.co.uk/wp-content/uploads/2024/04/Feeld71.png 1020w, https://fortbridge.co.uk/wp-content/uploads/2024/04/Feeld71-300x159.png 300w, https://fortbridge.co.uk/wp-content/uploads/2024/04/Feeld71-768x408.png 768w" data-lazy-src="https://fortbridge.co.uk/wp-content/uploads/2024/04/Feeld71.png"></a></figure>



<p>3.The victim will receive a notification, as seen below:<br>Tap the notification and you will see the message. The victim cannot verify whether this message comes from the partner they matched with, or from a 3<sup>rd</sup> party, an attacker, like in this case.</p>







<p>The chat displayed on the right, is between 2 users: ‘D’ and ‘Bogdan’. Although, the system shows the notification is coming from user ‘B’ (the attacker’s name), the attacker can change their name on their profile, as this field is editable and not unique.</p>



<h3 id="h-8-view-other-people-s-matches"><strong>8.</strong> <strong>View other people’s matches</strong></h3>



<p><strong>Details</strong>: We can check who did other people match with and their full profile information, such as ‘imaginaryName, age, photos, gender, sexuality, status, data of birth.</p>



<p><strong>Instance</strong>: https://core.api.feeld.co/graphql (“operationName”:”ChatListQuery”, vulnerable parameter: ‘profileId’)</p>



<p><strong>Reproduction steps</strong>:</p>



<p>1. Enter the mobile application and go to the ‘Discover profiles’ menu.<br>2. It will make a request to /graphql with the “operationName”: “ChatListQuery”, as seen below:</p>







<p>3. Change the profileId to that belonging to a victim user, such as: 00ab5791-e42e-58e2-ab51-e30a453d791f. Thus, we can view that account’s matches, as seen below:</p>







<h2 id="h-see-our-leading-research-insights">See Our Leading Research Insights</h2>



<p>For those interested in exploring more security research similar to our study on mobile app vulnerabilities, consider these insightful articles:</p>



<ul>
<li>Firstly, for<strong> API testing research</strong>, check <a href="https://fortbridge.co.uk/research/mass-account-takeover-yunmai/" target="_blank" rel="noreferrer noopener follow" data-wpel-link="internal"><strong>Mass Account Takeover in Yunmai Smart Scale API</strong></a>: This article details a pentest of Yunmai’s Android and iOS smart scale API, revealing several issues, including a chained attack leading to mass account takeover.</li>



<li>Secondly, for<strong> web app pentest research </strong>and a peek into PHP internals<strong>, </strong>check<strong> <a href="https://fortbridge.co.uk/research/multiple-vulnerabilities-in-concrete-cms-part1-rce/" target="_blank" rel="noreferrer noopener follow" data-wpel-link="internal">Multiple Concrete CMS Vulnerabilities (Part 1 – RCE)</a></strong>: This article investigates achieving remote code execution through 2 race conditions vulnerabilities in the file upload functionality in Concrete CMS, providing a detailed examination of potential security risks and mitigation strategies.</li>



<li>Additionally, for<strong> </strong>our<strong> open source contribution to security tools, </strong>check<strong> <a href="https://fortbridge.co.uk/research/add-spf-dmarc-dkim-mx-records-evilginx/" target="_blank" rel="noreferrer noopener follow" data-wpel-link="internal">Phishing Like a Pro: A Guide for Pentesters to Add SPF, DMARC, DKIM, and MX Records to Evilginx</a></strong>: This guide delves into advanced phishing techniques and how to effectively use SPF, DMARC, DKIM, and MX records with Evilginx for penetration testing.</li>
</ul>



<p>Moreover, explore these resources to deepen your understanding of security testing and stay updated on best practices in the field.</p>



<h2 id="h-conclusion">Conclusion</h2>



<p>Firstly, as we continually advance our tools and techniques for effective mobile app security testing, we invite you to explore our <a href="https://fortbridge.co.uk/mobile-application-pentesting/" target="_blank" rel="noreferrer noopener follow" data-wpel-link="internal">mobile application pentesting services</a>. Indeed, at FORTBRIDGE, we take pride in being a leading pentesting provider with a team of senior consultants who have a proven track record in the industry. Moreover, our experts use advanced methodologies and tools to deliver comprehensive, actionable insights tailored to your specific needs. Specifically, we have the <a href="https://fortbridge.co.uk/accreditations/fortbridge-receives-desc-accreditation-for-penetration-testing/" target="_blank" rel="noreferrer noopener follow" data-wpel-link="internal">Dubai Electronic Security Center (DESC)</a> and the <a href="https://fortbridge.co.uk/accreditations/fortbridge-receives-crest-accreditation-for-penetration-testing-services/" target="_blank" rel="noreferrer noopener follow" data-wpel-link="internal">Council of Registered Ethical Security Testers (CREST)</a> accredit our services, and we design them to fortify your mobile applications and ensure the highest quality of security testing.</p>



<p>Furthermore, dive into the latest and most effective security strategies with our expertly crafted solutions. To that end, for detailed information about our offerings or to discuss customized penetration testing strategies, please visit our services page or <a href="https://fortbridge.co.uk/contact/" target="_blank" rel="noreferrer noopener follow" data-wpel-link="internal">contact us</a> today. In conclusion, with FORTBRIDGE, you can be confident in our commitment to safeguarding your organization’s systems and data, helping you stay ahead of evolving cyber threats.</p>



<h2 id="h-feeld-disclosure-timeline">Feeld Disclosure Timeline</h2>



<p>2024/03/08 – The disclosure of all the above issues to Feeld. <br>2024/03/08 – Feeld asked for the testing account details used during testing. <br>2024/04/02 – FORTBRIDGE – we asked for an update.<br>2024/04/02 – Feeld – ‘We are continuing to review the findings. Hence, if you can <strong>hold off</strong> publication … it would be helpful’<br>2024/04/02 – FORTBRIDGE – ‘We’ll hold off publication’.<br>2024/05/28 – FORTBRIDGE – ‘Any update? It’s been almost 3 months’.<br>2024/05/28 – Feeld: ‘we deployed several fixes. Thus, we kindly ask that you <strong>delay</strong> your findings for a maximum of 2 weeks, allowing us to confirm that we have resolved the flags in your report and ensuring that the safety of our Members remains sound’.<br>2024/05/29 – FORTBRIDGE – ‘So, we agree to delay publishing for 2 more weeks’.<br><strong>2024/06/08</strong> – 3 months have passed since the initial disclosure email.<br>2024/06/19 – FORTBRIDGE – we asked for an update.<br>2024/06/20 – Feeld: ‘We appreciate your patience. Meanwhile, the team is cleaning up a few remaining items’.<br><strong>2024/07/08</strong> – 4 months have passed.<br>2024/07/08 – FORTBRIDGE: ‘Have you closed off all of the issues?’.<br>2024/07/15 – Feeld: ‘[…] a few issues still require a more complex set of remediations. […] we appreciate your allowing us time to fully resolve before publishing any of your findings’.<br>2024/08/04 – Feeld: ‘Our teams are actively working to resolve the remaining findings.&nbsp; Please <strong>hold off</strong> publishing until we can confirm that we have resolved these items.’<br><strong>2024/08/08</strong> – 5 months have passed.<br>2024/08/08 – FORTBRIDGE – we asked for an update.<br>2024/08/16 – Feeld: ‘we have implemented the required changes to mitigate the remaining findings’.<br><strong>2024/09/08</strong> – 6 months have passed.<br>2024/09/10 – Blog published.</p>




        <h3>About Post Author</h3>
                        <div id="3_awpa-tab1">
            <p><a href="https://fortbridge.co.uk/author/bogdanfortbridge-co-uk/" data-wpel-link="internal" rel="follow"><img alt="" src="https://secure.gravatar.com/avatar/aa5c7f766f0673ddbd353c34706841bf?s=150&amp;d=mm&amp;r=g" srcset="https://secure.gravatar.com/avatar/aa5c7f766f0673ddbd353c34706841bf?s=300&amp;d=mm&amp;r=g 2x" height="150" width="150" data-old-src="data:image/svg+xml,%3Csvg%20xmlns='http://www.w3.org/2000/svg'%20viewBox='0%200%20150%20150'%3E%3C/svg%3E"></a>
               
               
            </p>
            
        </div>
                <!-- Simple Share Buttons Adder (8.5.0) simplesharebuttons.com -->
		
	</div><!-- .entry-content -->

	<!-- .entry-footer -->

</article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Why Oxide Chose Illumos (145 pts)]]></title>
            <link>https://rfd.shared.oxide.computer/rfd/0026</link>
            <guid>41515447</guid>
            <pubDate>Wed, 11 Sep 2024 21:22:05 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://rfd.shared.oxide.computer/rfd/0026">https://rfd.shared.oxide.computer/rfd/0026</a>, See on <a href="https://news.ycombinator.com/item?id=41515447">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>Static metrics are valuable when engineers can determine interesting values to
collect and store in advance of a problem occurring.  Unfortunately, many of
the most pernicious software defects are not found or fixed by inspecting
existing metrics.</p><p>Complex defects are often not easily reproducible; their presentation may
depend on ambient and difficult to catalogue background activity that is unique
to a specific customer site at a specific time.  In order to chase these
defects to root cause and make a fix, we must be able to ask questions about
the dynamic state of the system and iteratively refine those questions as we
hone in on the problem.  New instrumentation should be able to be enabled on a
running production system without updating or even restarting the software.</p><p>Human intervention in systems can lead to unforced errors, and even the most
cautious engineers engaging in debugging exercises are not immune.  Our
instrumentation must be constructed to be safe to engage on a production system
without fear of a crash, or data corruption, or serious and widely felt
performance degradation.  When not engaged, the machinery for instrumentation
should have no impact on system operation or performance.  In the limit, it
should be safe to accidentally request the enabling of all possible system
instrumentation without prolonged deleterious effect; it is better to report
an error to the debugging engineer than to make the system unsafe.</p><p>Modern systems generally consist of a lot of software, performing operations at
a dizzying rate.  A naive firehose approach to system observability where
post-processing is used to locate the subset of interesting events is unlikely
to broadly useful.  In order to have minimal impact on system performance, and
to answer very specific questions, the instrumentation system should be able to
efficiently and safely filter and aggregate relevant events <em>in-situ</em>; that is,
to make the decision to record the event, and which data to record, at the time
of the event.</p><p>Our hypervisor and other critical services, are likely to be implemented in a
split between user processes and kernel modules.  We will need instrumentation
that can cleanly measure events that occur across more than one process, and
which extend into the kernel.</p><p>In general, we expect to able to gather at least the following information:</p><div data-lineno="767"><ul><li><p>scheduling events; e.g., when a thread or process goes to sleep and why, or
what caused a thread or process to wake up</p></li><li><p>timer-based profiling; e.g., to allow us to sample system state such as the
kernel or user stack at regular intervals, possibly with other constraints
such as limiting to a particular process</p></li><li><p>system call entry and return, both per-process and globally</p></li><li><p>kernel function entry and return, with guard rails that prevent the tracing
of functions that are not safe to trace</p></li><li><p>user process function entry and return, and breakpoint-style tracing of
specific instructions or specific offsets within a function</p></li><li><p>creation of new processes and threads, and termination (planned or
unplanned) of existing processes and threads</p></li><li><p>latency distributions of operations, some of which may be defined as
the composite of multiple system-level operations by the engineer</p></li></ul></div><p>In some cases it can also be desirable to allow instrumentation to take limited
mutating actions against the system, such as invoke a data collector program
once a particular sequence of trace events has been detected.  In order to aid
in the reproduction of races in multi-threaded software, it can be helpful to
be able to inject small delays (hundreds of microseconds) at specific moments
in order to widen a suspected race window.  Facilities that perturb the system
may present a trade-off in safety, and it’s possible we might want to be able
to restrict these separately from general instrumentation in customer
environments.</p><div><h5 data-sectnum="2.5.3.1."><a href="#_comparison_dtrace_ebpf">Comparison: DTrace &amp; eBPF<svg width="16" height="16"><use href="/build/_assets/sprite-KDLRSXCD.svg#link-16"></use></svg></a></h5><div><p>DTrace, as described in
<a href="https://www.usenix.org/legacy/event/usenix04/tech/general/full_papers/cantrill/cantrill_html/index.html">
the 2004 paper</a>, is a system for the dynamic instrumentation of production
systems.  A prominent goal in its construction is to be perfectly safe, and
over more than a decade of use on a wide variety of production workloads it has
proven to be sufficiently robust that engineers and operators need not worry
when enabling it on a critical system.  Many key design decisions stem from
safety as a core goal; e.g., the instruction set for the tracing virtual
machine allows no backwards branches, so infinite loops are impossible by
construction.</p><p>Joyent hosted arbitrary customer workloads within
<a href="https://illumos.org/man/5/zones">zones</a>, an isolation and virtualisation
technology similar in principle to FreeBSD jails or Docker containers.  DTrace
was sufficiently safe that access could be granted to customers to instrument
software running within their container, with only restricted visibility into
global system behaviour.  In addition to raw DTrace access, part of the <em>Cloud
Analytics</em> product was built on top of DTrace instrumentation.  This product
was able to collect statistics both from probes that fired in user applications,
and from the underlying operating system, aggregating them in a live graphical
view.  Finally, countless real production problems were solved by long-running
DTrace enablings distributed throughout the fleet, waiting to log data about
the events leading up to some critical fault, but without otherwise affecting
the regular operation of the system.</p><p>In the more distant past, DTrace was a critical underpinning of the Analytics
feature of the Fishworks appliance at Sun.  Analytics enabled customers to
drill down into fine detail while analysing the performance of the system,
providing more abstract control over DTrace enablings and presenting an
interactive graphical view of the resultant telemetry.</p><p>The Berkeley (née BSD) Packet Filter (BPF) was introduced in 1992, to provide a
safe virtual machine that could be included in the kernel for selective packet
capture.  By allowing the filtering engine to run safely in the kernel, the
performance overhead of copying every packet into a user address space for
filtering could be avoided.  It followed from similar approaches taken in
earlier systems.</p><p>In 2014, an <em>extended</em> BPF (eBPF) was introduced to the mainline Linux kernel
for a variety of uses.  In contrast to many prior approaches, the eBPF virtual
machine makes use of a just-in-time (JIT) compiler to convert eBPF instructions
into native program text as they are loaded into the kernel.  This choice
appears to be the result of an attempt to build one system for two masters:</p><div data-lineno="838"><ol><li><p>Adding new behaviours to the system, even in the data path, where
performance is of paramount performance and programs must run to completion for
system correctness even if they have an outsized impact on the rest of the
system; e.g.,</p><div data-lineno="842"><ul><li><p>filtering, configuring, or redirecting socket connections</p></li><li><p>classifying and shaping network traffic</p></li><li><p>system call security policy, resource, and quota management in cgroups</p></li><li><p>network encapsulation protocol implementation</p></li></ul></div></li><li><p>Tracing and performance measurement of the system; e.g., by allowing eBPF
programs to hook various trace points and events from the perf subsystem</p></li></ol></div><p>The first use case, extending the data path, requires high performance at all
costs.  Without low latency operations, eBPF would not be an attractive target
when implementing new network filtering or encapsulation facilities.  The
robustness and security of eBPF appear to depend fundamentally on a component
called the "verifier", which scans the eBPF program upon load into the kernel.
The verifier attempts to determine (before execution) whether an eBPF program
will do anything unsafe, and seeks to ensure that it will terminate.  There
have been some serious vulnerabilities found in the verifier, and it is not
clear the extent to which it has been proven to work.  Indeed,
<a href="https://github.com/torvalds/linux/blob/master/kernel/bpf/verifier.c">
<code>kernel/<wbr>bpf/<wbr>verifier.c</code></a> is (according to <code>cloc</code>) eight thousand lines of
non-comment C code running in the kernel.
<a href="https://www.openwall.com/lists/oss-security/2020/03/30/3">CVE-2020-8835</a> from
earlier this year is one such example of a security issue in the verifier.</p><p>By contrast, DTrace has a more constrained programming model which has allowed
a more readily verified implementation.  A byte code interpreter is used, with
security checks directly at the site of operations like loads or stores that
allow a D program to impact or observe the rest of the system.  The instruction
format does not allow for backwards branches, so constraining the program
length has a concomitant impact on execution time and thus impact on the
broader system.  Each action is limited in the amount of work it can perform;
e.g., by caps on the number of bytes of string data that will be read or
copied, and by the overall principal buffer size.  Explicit choices have been
made to favour limiting system impact — one could not implement a reliable
auditing or accounting system in DTrace, as the system makes no guarantee that
an enabling won’t be thrown overboard to preserve the correct execution of the
system.</p><p>In addition to issues of implementation complexity and verifier tractability,
there is the small matter of binary size.  The <code>bpftrace</code> tool, analogous on
some level to <code>dtrace</code>, depends directly on the library form of BCC, Clang, and
LLVM.  This puts the directly linked text size (as visible via <code>ldd</code>) at around
160MB, which was more than half of the size of the entire SmartOS RAM disk.
This doesn’t account for other parts of those toolchains that generally come
along for the ride, or the debugging information that is often stripped from
binaries in desktop Linux distributions.  By contrast, <code>dtrace</code> and supporting
libraries run to around 11MB total including CTF.  In 2020, disks, memory, and
network bandwidth, are relatively cheap.  That said, in contexts within the
system where we choose to execute the OS from a pinned RAM image, program text
size may still be an issue.  Lighter debugging infrastructure is easier to
include in more contexts without other trade-offs.</p><p>Finally, the tools in the eBPF ecosystem continue to present a number of
opportunities for improvement with respect to user experience.  A relatively
easy task with DTrace is to trace all system calls being made on the system or
by a particular process, and to then aggregate them by system call type, or
obtain a distribution of the latency of each call, or some combination of those
and other things.  By contrast, on a modern Ubuntu system, repeated attempts to
do the same basic thing resulted in hitting any number of walls; e.g.,</p><div data-lineno="901"><ul><li><p>Probe names have not been selected to be terribly ergonomic; e.g.,
what would in DTrace be <code>syscall::read:entry</code>, where each of the
colon-separated portions of the tuple are available in separate variables, the
analogous probe available to <code>bpftrace</code> appears to be
<code>tracepoint:syscalls:sys_enter_read</code>.  There is only one variable, <code>probe</code>,
which then contains the whole string.  Useful output appears likely to require
post-processing for even simple tracing activities.</p></li><li><p>When trying to enable all <code>tracepoint:syscalls:sys_*</code> probes and
count the distinct <code>probe</code> values, it becomes apparent that enabling probes <em>at
all</em> results in sufficient risk to the system that you may enable at most 500
of them; following the instructions in the scary warning to increase the probe
count results instead in tens of seconds of the system hanging and then pages
of viscous and apparently diagnostic output from the eBPF verifier.</p></li><li><p>Upon further inspection, there is instead a probe site that fires for the
entry of every different kind of system call,
<code>tracepoint:raw_syscalls:sys_enter</code>, though it is difficult to actually use
this effectively: the only way to identify the system call is then by its
number, which though stable is also different per architecture (e.g., even 32-
or 64-bit processes on x86 have different numbers).</p></li></ul></div><p>Conversely, it is possible to cheaply enable tens of thousands of probe sites
with DTrace.  On a current system, there are 236 system calls and something
like 34000 kernel function probe sites, not including a number of other probes.
A simple enabling that counts the firings of all probes over a ten second run
is responsive and easy:</p><div data-lineno="927"><pre># time dtrace -n '::: { @ = count(); }' -c 'sleep 10'
dtrace: description '::: ' matched 80872 probes
dtrace: pid 28326 has exited

         39354882

real    0m18.393s
user    0m0.564s
sys     0m8.115s</pre></div></div></div></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[The first release candidate of FreeCAD 1.0 is out (314 pts)]]></title>
            <link>https://blog.freecad.org/2024/09/10/the-first-release-candidate-of-freecad-1-0-is-out/comment-page-1/#comments</link>
            <guid>41515101</guid>
            <pubDate>Wed, 11 Sep 2024 20:29:52 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://blog.freecad.org/2024/09/10/the-first-release-candidate-of-freecad-1-0-is-out/comment-page-1/#comments">https://blog.freecad.org/2024/09/10/the-first-release-candidate-of-freecad-1-0-is-out/comment-page-1/#comments</a>, See on <a href="https://news.ycombinator.com/item?id=41515101">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
<p>We’ve just pub­lished builds of the first release can­di­date for FreeCAD 1.0. You can down­load them <a href="https://github.com/FreeCAD/FreeCAD-Bundle/releases/tag/1.0rc1">here</a>.</p>



<p>So far, we’ve enjoyed the con­tri­bu­tions of users who are hap­py to be liv­ing on the edge with our week­ly builds and report­ing what­ev­er bugs they run into. That real­ly helped us make the prover­bial edge less&nbsp;edgy.</p>



<p>The inten­tion behind mak­ing release can­di­dates is to give them into the hands of a dif­fer­ent demo­graph­ic — users who usu­al­ly stay away from unsta­ble soft­ware yet are hap­py enough to try very near­ly com­plete soft­ware and report issues they come across.</p>



<p>We are cur­rent­ly down to just 7 release block­ers, but we expect that the release can­di­dates will bump that num­ber up a tad, and that’s a good thing. While we des­per­ate­ly want 1.0 out, deliv­er­ing a real­ly sta­ble release is a big deal for&nbsp;us.</p>



<p>So please down­load the <span>RC1</span>, try it on a real project if you can, give it some real­ly hard time if you must, and report any bugs you see to our <a href="https://github.com/FreeCAD/FreeCAD/issues">issue track­er</a>.</p>



<p>If you are a devel­op­er who is inter­est­ed in con­tribut­ing to the effort, come look at the list of reports, pick an issue that seems triv­ial to resolve, write a fix, and sub­mit a pull request. We have week­ly <a href="https://www.freecad.org/events.php">merge meet­ings</a> on Mon­days to go through the queue of cur­rent­ly open pull requests. You are wel­come to&nbsp;join!</p>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[A MiniGolf game for Palm OS (291 pts)]]></title>
            <link>https://ctrl-c.club/~captain/posts/2024-08-29-holy-smokes-I-Just-released-a-minigolf-game-for-palmos-in-2024.html</link>
            <guid>41514944</guid>
            <pubDate>Wed, 11 Sep 2024 20:10:20 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://ctrl-c.club/~captain/posts/2024-08-29-holy-smokes-I-Just-released-a-minigolf-game-for-palmos-in-2024.html">https://ctrl-c.club/~captain/posts/2024-08-29-holy-smokes-I-Just-released-a-minigolf-game-for-palmos-in-2024.html</a>, See on <a href="https://news.ycombinator.com/item?id=41514944">Hacker News</a></p>
<div id="readability-page-1" class="page"><a href="https://ctrl-c.club/~captain/">Back to overview</a>


<p>This is a follow up to my previous post:</p>
<a href="https://ctrl-c.club/~captain/posts/2024-07-18-retro-coding-journey-into-creating-a-palmos-minigolf-game.html">Retro Coding Like It's 1999: My Journey into creating a Palm OS MiniGolf Game</a>

<p>This summer, I embarked on a side project to create a brand-new Palm OS game, and after less than two months of intermittent coding, I'm excited to announce that it's ready to be released to the public!</p>

<p>Let me present to you "Captain's MiniGolf (v0.6)":</p>
<video controls="" src="https://ctrl-c.club/~captain/posts/images/Captains_MiniGolf_Play_Preview.mp4"><a src="images/Captains_MiniGolf_Play_Preview.mp4">Gameplay video</a></video>

<p>Besides hoping to have created a fun little MiniGolf game, the strong point of the game is that you can create your own levels:</p>
<video controls="" src="https://ctrl-c.club/~captain/posts/images/Captains_MiniGolf_LevelEdit_Preview.mp4"><a src="images/Captains_MiniGolf_LevelEdit_Preview.mp4">Level Creation video</a></video>

<p>The game allows you to create your own levelpack databases. Those can be exported and shared with other users, not sure if anyone is going to create these, but it would be fun to see what courses other users can come up with.</p>

<h2>Play it!</h2>
<p>Don't have a Palm OS device? No problem, thanks to the cloudpilot emulator, you can directly play it from the browser (Get a Palm though, you won't regret it) </p>
<a href="https://quarters.captaintouch.com/captainsminigolf.html">Game download and in-browser emulator</a>

<h2>Coding for Palm is sometimes harder than I remember</h2>
<p>Some things I realized while coding this in C:</p>
<p>- You can really mess things up without the hand-holding that you get in modern programming languages</p>
<p>- Memory leaks happen more often than you think</p>
<p>- Debugging polygon shapes and trajectories can be hard, so having a debug build that visualizes some behind-the-scenes logic is a big help</p>

<p>Programming for an old platform like Palm OS can be difficult because of the lack of documentation, but I used the following 2 reference guides to help me out:</p>
<a href="https://stuff.mit.edu/afs/sipb/user/yonah/docs/Palm%20OS%20Companion.pdf">The Palm OS Programmer Companion (part of the Palm OS SDK) </a>
<a href="https://archive.org/details/palmosprogrammin0000fost">Palm OS Programming Bible</a>
<p>There are also some projects up on GitHub of developers that shared the code for their old Palm OS games.</p>

<h2>Why bother?!</h2>
<p>Palm OS devices have always been close to my heart, from the moment I got my Palm M100 to this very day. The simplicity and elegance of devices that can achieve so much with so little is something we've lost over time. When programming for these devices, you inevitably encounter their limitations, but these constraints encourage you to think creatively and find alternative solutions. Is a function too slow or using too much memory? You have to find another way to do it! </p>
<p>With the excessively performant phones we have today, nobody is going to give it a second look to see if a function could be optimized... cpu makes up for coding mediocrity/laziness. </p>

<h2>Known bugs</h2>
<p>- The ball can get stuck in a wall</p>
<p>- If you create a level that has a closed polygon of walls within the main playing field walls, the game can't color the background/course correctly.</p>

<h2>Improvements</h2>
<p>- Add a delete/move level option</p>
<p>- Add a delete and share levelpack option (can be done now using an external application like FileZ)</p>
<p>- Resolution is now fixed to 160x160 (or 320x320 on Palm OS 5 hi-res devices), this should be made dynamic based on the available screen size.</p>

<h2>Sharing is caring</h2>
<p>I am releasing the full source code (GPL3 license) for this game as well, in the hope that this can inspire or help others to create more games for Palm OS.</p>
<a href="https://github.com/captaintouch/Captains_MiniGolf_PalmOS">Captain's MiniGolf source code</a>

<p>The draft for this article was written on my Palm Zire 72.</p>


<p>You can get in touch through Mastodon:  </p>
<a href="https://social.linux.pizza/@rxpz">@rxpz@social.linux.pizza</a>
<p>Holy smokes, I just released a MiniGolf game for Palm OS in 2024 was published on 2024-08-29</p>

<a href="https://ctrl-c.club/~captain/">Back to the overview</a>

<a href="https://ctrl-c.club/~captain/posts.xml" target="_blank" rel="noopener noreferrer">📰 Subscribe to RSS feed 📰</a><br>
<img src="https://ctrl-c.club/~captain/posts/images/mozilla.gif"><img src="https://ctrl-c.club/~captain/posts/images/alienow.gif"><img src="https://ctrl-c.club/~captain/posts/images/cassette.gif"><img src="https://ctrl-c.club/~captain/posts/images/linux_now.gif"><img src="https://ctrl-c.club/~captain/posts/images/ns-best.gif">
</div>]]></description>
        </item>
        <item>
            <title><![CDATA[Mistral releases Pixtral 12B, its first multimodal model (160 pts)]]></title>
            <link>https://techcrunch.com/2024/09/11/mistral-releases-pixtral-its-first-multimodal-model/</link>
            <guid>41514727</guid>
            <pubDate>Wed, 11 Sep 2024 19:47:10 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://techcrunch.com/2024/09/11/mistral-releases-pixtral-its-first-multimodal-model/">https://techcrunch.com/2024/09/11/mistral-releases-pixtral-its-first-multimodal-model/</a>, See on <a href="https://news.ycombinator.com/item?id=41514727">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
<p id="speakable-summary">French AI startup <a href="https://mistral.ai/" target="_blank" rel="noreferrer noopener nofollow">Mistral</a> has released its first model that can process images as well as text.</p>

<p>Called Pixtral 12B, the 12-billion-parameter model is about 24GB in size. Parameters&nbsp;roughly correspond to a model’s problem-solving skills, and models with more&nbsp;parameters&nbsp;generally perform better than those with fewer parameters.</p>

	
	


<p>Built on one of Mistral’s text models, Nemo 12B, the new model can answer questions about an arbitrary number of images of an arbitrary size given either URLs or images encoded using base64, the binary-to-text encoding scheme. Similar to other multimodal models such as Anthropic’s Claude family and OpenAI’s GPT-4o, Pixtral 12B should — at least in theory — be able to perform tasks like captioning images and counting the number of objects in a photo.</p>

<p>Available via a torrent link on <a href="https://github.com/mistralai/mistral-common/releases/tag/v1.4.0" target="_blank" rel="noreferrer noopener nofollow">GitHub</a> and AI and machine learning development platform <a href="https://huggingface.co/mistral-community/pixtral-12b-240910" target="_blank" rel="noreferrer noopener nofollow">Hugging Face</a>, Pixtral 12B can be downloaded, fine-tuned and used under an Apache 2.0 license without restrictions. (A Mistral spokesperson confirmed the license being applied to Pixtral 12B via email.)</p>

<p>This writer wasn’t able to take Pixtral 12B for a spin, unfortunately — there weren’t any working web demos at the time of publication. In a <a href="https://x.com/sophiamyang/status/1833825577398354077" target="_blank" rel="noreferrer noopener nofollow">post on X</a>, Sophia Yang, head of Mistral developer relations, said Pixtral 12B will be available for testing on Mistral’s chatbot and API-serving platforms, Le Chat and Le Plateforme, soon.</p>

<p>It’s unclear which image data Mistral might have used to develop Pixtral 12B.</p>

<p>Most generative AI models, <a href="https://www.cnbc.com/2024/03/06/gpt-4-researchers-tested-leading-ai-models-for-copyright-infringement.html" target="_blank" rel="noreferrer noopener nofollow">including Mistral’s other models</a>, are trained on vast quantities of public data from around the web, which is often copyrighted. Some model vendors argue that “fair use” rights entitle them to scrape <em>any</em> public data, but many copyright holders disagree, and have filed lawsuits against larger vendors like OpenAI and Midjourney to put a stop to the practice.</p>


<p>Pixtral 12B comes in the wake of Mistral closing a $645 million funding round led by General Catalyst that <a href="https://techcrunch.com/2024/05/09/sources-mistral-ai-raising-at-a-6b-valuation-softbank-not-in-but-dst-is/">valued</a> the company at $6 billion. Just over a year old, Mistral — minority owned by Microsoft — is seen by many in the AI community as Europe’s answer to OpenAI. The younger company’s strategy thus far has involved releasing free “open” models, charging for managed versions of those models, and providing consulting services to corporate customers.</p>

	
	


<p><em>Updated 9/11 at 8:11 a.m. Pacific: Clarified that Pixtral 12B is being made available under an Apache 2.0 license, not Mistral’s standard dev license that carries with it certain restrictions on commercial usage.</em></p>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Noisy neighbor detection with eBPF (229 pts)]]></title>
            <link>https://netflixtechblog.com/noisy-neighbor-detection-with-ebpf-64b1f4b3bbdd</link>
            <guid>41513860</guid>
            <pubDate>Wed, 11 Sep 2024 18:11:31 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://netflixtechblog.com/noisy-neighbor-detection-with-ebpf-64b1f4b3bbdd">https://netflixtechblog.com/noisy-neighbor-detection-with-ebpf-64b1f4b3bbdd</a>, See on <a href="https://news.ycombinator.com/item?id=41513860">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><div><a href="https://netflixtechblog.medium.com/?source=post_page-----64b1f4b3bbdd--------------------------------" rel="noopener follow"><div aria-hidden="false"><p><img alt="Netflix Technology Blog" src="https://miro.medium.com/v2/resize:fill:88:88/1*BJWRqfSMf9Da9vsXG9EBRQ.jpeg" width="44" height="44" loading="lazy" data-testid="authorPhoto"></p></div></a><a href="https://netflixtechblog.com/?source=post_page-----64b1f4b3bbdd--------------------------------" rel="noopener  ugc nofollow"><div aria-hidden="false"><p><img alt="Netflix TechBlog" src="https://miro.medium.com/v2/resize:fill:48:48/1*ty4NvNrGg4ReETxqU2N3Og.png" width="24" height="24" loading="lazy" data-testid="publicationPhoto"></p></div></a></div><p id="3108"><em>By </em><a href="https://www.linkedin.com/in/josefernandezmn/" rel="noopener ugc nofollow" target="_blank"><em>Jose Fernandez</em></a><em>, </em><a href="https://www.linkedin.com/in/sebastien-dabdoub-2a5a0958/" rel="noopener ugc nofollow" target="_blank"><em>Sebastien Dabdoub</em></a><em>, </em><a href="https://www.linkedin.com/in/jason-koch-5692172/" rel="noopener ugc nofollow" target="_blank"><em>Jason Koch</em></a><em>, </em><a href="https://www.linkedin.com/in/artemtkachuk/" rel="noopener ugc nofollow" target="_blank"><em>Artem Tkachuk</em></a></p><p id="8709">The Compute and Performance Engineering teams at Netflix regularly investigate performance issues in our multi-tenant environment. The first step is determining whether the problem originates from the application or the underlying infrastructure. One issue that often complicates this process is the "noisy neighbor" problem. On <a rel="noopener ugc nofollow" target="_blank" href="https://netflixtechblog.com/titus-the-netflix-container-management-platform-is-now-open-source-f868c9fb5436">Titus</a>, our multi-tenant compute platform, a "noisy neighbor" refers to a container or system service that heavily utilizes the server's resources, causing performance degradation in adjacent containers. We usually focus on CPU utilization because it is our workloads’ most frequent source of noisy neighbor issues.</p><p id="f191">Detecting the effects of noisy neighbors is complex. Traditional performance analysis tools such as <a href="https://www.brendangregg.com/perf.html" rel="noopener ugc nofollow" target="_blank">perf</a> can introduce significant overhead, risking further performance degradation. Additionally, these tools are typically deployed after the fact, which is too late for effective investigation.<em> </em>Another challenge is that debugging noisy neighbor issues requires significant low-level expertise and specialized tooling<em>. </em>In this blog post, we'll reveal how we leveraged <a href="https://ebpf.io/" rel="noopener ugc nofollow" target="_blank">eBPF</a> to achieve continuous, low-overhead instrumentation of the Linux scheduler, enabling effective self-serve monitoring of noisy neighbor issues. You’ll learn how Linux kernel instrumentation can improve your infrastructure observability with deeper insights and enhanced monitoring.</p><h2 id="1e46">Continuous Instrumentation of the Linux Scheduler</h2><p id="792f">To ensure the reliability of our workloads that depend on low latency responses, we instrumented the <a href="https://en.wikipedia.org/wiki/Run_queue" rel="noopener ugc nofollow" target="_blank">run queue</a> latency for each container, which measures the time processes spend in the scheduling queue before being dispatched to the CPU. Extended waiting in this queue can be a telltale of performance issues, especially when containers are not utilizing their total CPU allocation. Continuous instrumentation is critical to catching such matters as they emerge, and eBPF, with its hooks into the Linux scheduler with minimal overhead, enabled us to monitor run queue latency efficiently.</p><p id="048b">To emit a run queue latency metric, we leveraged three eBPF hooks: <code>sched_wakeup</code><strong>, </strong><code>sched_wakeup_new</code><strong>,</strong> and <code>sched_switch</code>.</p></div><div><p id="184b">The <code>sched_wakeup</code><strong> </strong>and <code>sched_wakeup_new</code> hooks are invoked when a process changes state from 'sleeping' to 'runnable.' They let us identify when a process is ready to run and is waiting for CPU time. During this event, we generate a timestamp and store it in an eBPF hash map using the process ID as the key.</p><pre><span id="07d3">struct {<br>    __uint(type, BPF_MAP_TYPE_HASH);<br>    __uint(max_entries, MAX_TASK_ENTRIES);<br>    __uint(key_size, sizeof(u32));<br>    __uint(value_size, sizeof(u64));<br>} runq_enqueued SEC(".maps");<p>SEC("tp_btf/sched_wakeup")<br>int tp_sched_wakeup(u64 *ctx)<br>{<br>    struct task_struct *task = (void *)ctx[0];<br>    u32 pid = task-&gt;pid;<br>    u64 ts = bpf_ktime_get_ns();</p><p>    bpf_map_update_elem(&amp;runq_enqueued, &amp;pid, &amp;ts, BPF_NOEXIST);<br>    return 0;<br>}</p></span></pre><p id="5852">Conversely, the <code>sched_switch</code> hook is triggered when the CPU switches between processes. This hook provides pointers to the process currently utilizing the CPU and the process about to take over. We use the upcoming task's process ID (PID) to fetch the timestamp from the eBPF map. This timestamp represents when the process entered the queue, which we had previously stored. We then calculate the run queue latency by simply subtracting the timestamps.</p><pre><span id="012a">SEC("tp_btf/sched_switch")<br>int tp_sched_switch(u64 *ctx)<br>{<br>    struct task_struct *prev = (struct task_struct *)ctx[1];<br>    struct task_struct *next = (struct task_struct *)ctx[2];<br>    u32 prev_pid = prev-&gt;pid;<br>    u32 next_pid = next-&gt;pid;<p>     // fetch timestamp of when the next task was enqueued<br>    u64 *tsp = bpf_map_lookup_elem(&amp;runq_enqueued, &amp;next_pid);<br>    if (tsp == NULL) {<br>        return 0; // missed enqueue<br>    }</p><p>    // calculate runq latency before deleting the stored timestamp<br>    u64 now = bpf_ktime_get_ns();<br>    u64 runq_lat = now - *tsp;</p><p>    // delete pid from enqueued map<br>    bpf_map_delete_elem(&amp;runq_enqueued, &amp;next_pid);<br>    ....</p></span></pre><p id="58f0">One of the advantages of eBPF is its ability to provide pointers to the actual kernel data structures representing processes or threads, also known as tasks in kernel terminology. This feature enables access to a wealth of information stored about a process. We required the process's cgroup ID to associate it with a container for our specific use case. However, the cgroup information in the process struct is safeguarded by an<a href="https://elixir.bootlin.com/linux/v6.6.16/source/include/linux/sched.h#L1225" rel="noopener ugc nofollow" target="_blank"> RCU (Read Copy Update) lock</a>.</p><p id="0a15">To safely access this RCU-protected information, we can leverage <a href="https://docs.kernel.org/bpf/kfuncs.html" rel="noopener ugc nofollow" target="_blank">kfuncs</a> in eBPF. kfuncs are kernel functions that can be called from eBPF programs. There are kfuncs available to lock and unlock RCU read-side critical sections. These functions ensure that our eBPF program remains safe and efficient while retrieving the cgroup ID from the task struct.</p><pre><span id="769f">void bpf_rcu_read_lock(void) __ksym;<br>void bpf_rcu_read_unlock(void) __ksym;<p>u64 get_task_cgroup_id(struct task_struct *task)<br>{<br>    struct css_set *cgroups;<br>    u64 cgroup_id;<br>    bpf_rcu_read_lock();<br>    cgroups = task-&gt;cgroups;<br>    cgroup_id = cgroups-&gt;dfl_cgrp-&gt;kn-&gt;id;<br>    bpf_rcu_read_unlock();<br>    return cgroup_id;<br>}</p></span></pre><p id="ba4a">Once the data is ready, we must package it and send it to userspace. For this purpose, we chose the eBPF <a href="https://nakryiko.com/posts/bpf-ringbuf/" rel="noopener ugc nofollow" target="_blank">ring buffer</a>. It is efficient, high-performing, and user-friendly. It can handle variable-length data records and allows data reading without necessitating extra memory copying or syscalls. However, the sheer number of data points was causing the userspace program to use too much CPU, so we implemented a rate limiter in eBPF to sample the data.</p><pre><span id="5a42">struct {<br>    __uint(type, BPF_MAP_TYPE_RINGBUF);<br>    __uint(max_entries, RINGBUF_SIZE_BYTES);<br>} events SEC(".maps");<p>struct {<br>    __uint(type, BPF_MAP_TYPE_PERCPU_HASH);<br>    __uint(max_entries, MAX_TASK_ENTRIES);<br>    __uint(key_size, sizeof(u64));<br>    __uint(value_size, sizeof(u64));<br>} cgroup_id_to_last_event_ts SEC(".maps");</p><p>struct runq_event {<br>    u64 prev_cgroup_id;<br>    u64 cgroup_id;<br>    u64 runq_lat;<br>    u64 ts;<br>};</p><p>SEC("tp_btf/sched_switch")<br>int tp_sched_switch(u64 *ctx)<br>{<br>    // ....<br>    // The previous code<br>    // ....</p><p>     u64 prev_cgroup_id = get_task_cgroup_id(prev);<br>    u64 cgroup_id = get_task_cgroup_id(next);</p><p>     // per-cgroup-id-per-CPU rate-limiting <br>    // to balance observability with performance overhead<br>    u64 *last_ts = <br>        bpf_map_lookup_elem(&amp;cgroup_id_to_last_event_ts, &amp;cgroup_id);<br>    u64 last_ts_val = last_ts == NULL ? 0 : *last_ts;</p><p>    // check the rate limit for the cgroup_id in consideration<br>    // before doing more work<br>    if (now - last_ts_val &lt; RATE_LIMIT_NS) {<br>        // Rate limit exceeded, drop the event<br>        return 0;<br>    }</p><p>    struct runq_event *event;<br>    event = bpf_ringbuf_reserve(&amp;events, sizeof(*event), 0);</p><p>      if (event) {<br>        event-&gt;prev_cgroup_id = prev_cgroup_id;<br>        event-&gt;cgroup_id = cgroup_id;<br>        event-&gt;runq_lat = runq_lat;<br>        event-&gt;ts = now;<br>        bpf_ringbuf_submit(event, 0);<br>        // Update the last event timestamp for the current cgroup_id<br>        bpf_map_update_elem(&amp;cgroup_id_to_last_event_ts, &amp;cgroup_id,<br>            &amp;now, BPF_ANY);</p><p>    }</p><p>    return 0;<br>}</p></span></pre><p id="cd03">Our userspace application, developed in Go, processes events from the ring buffer to emit metrics to our metrics backend, <a rel="noopener ugc nofollow" target="_blank" href="https://netflixtechblog.com/introducing-atlas-netflixs-primary-telemetry-platform-bd31f4d8ed9a">Atlas</a>. Each event includes a run queue latency sample with a cgroup ID, which we associate with containers running on the host. We categorize it as a system service if no such association is found. When a cgroup ID is associated with a container, we emit a percentile timer Atlas metric (<code>runq.latency</code>) for that container. We also increment a counter metric (<code>sched.switch.out</code>) to monitor preemptions occurring for the container's processes. Access to the <code>prev_cgroup_id</code> of the preempted process allows us to tag the metric with the cause of the preemption, whether it's due to a process within the same container (or cgroup), a process in another container, or a system service.</p><p id="1bb4">It's important to highlight that both the <code>runq.latency</code> metric and the <code>sched.switch.out</code> metrics are needed to determine if a container is affected by noisy neighbors, which is the goal we aim to achieve — relying solely on the <code>runq.latency </code>metric can lead to misconceptions. For example, if a container is at or over its cgroup CPU limit, the scheduler will throttle it, resulting in an apparent spike in run queue latency due to delays in the queue. If we were only to consider this metric, we might incorrectly attribute the performance degradation to noisy neighbors when it's actually because the container is hitting its CPU quota. However, simultaneous spikes in both metrics, mainly when the cause is a different container or system process, clearly indicate a noisy neighbor issue.</p><h2 id="b5da">A Noisy Neighbor Story</h2><p id="8be9">Below is the <code>runq.latency</code> metric for a server running a single container with ample CPU capacity. The 99th percentile averages 83.4µs (microseconds), serving as our baseline. Although there are some spikes reaching 400µs, the latency remains within acceptable parameters.</p></div><div><p id="d6c6">At 10:35, launching <code>container2</code>, which fully utilized all CPUs on the host, caused a significant 131-millisecond spike (131,000 microseconds) in <code>container1</code>'s P99 run queue latency. This spike would be noticeable in the userspace application if it were serving HTTP traffic. If userspace app owners reported an unexplained latency spike, we could quickly identify the noisy neighbor issue through run queue latency metrics.</p></div><div><p id="1ce4">The <code>sched.switch.out</code> metric indicates that the spike was due to increased preemptions by system processes, highlighting a noisy neighbor issue where system services compete with containers for CPU time. Our metrics show that the noisy neighbors were actually system processes, likely triggered by <code>container2</code> consuming all available CPU capacity.</p><h2 id="37af">Optimizing eBPF Code</h2><p id="5964">We developed an open-source eBPF process monitoring tool called <a rel="noopener ugc nofollow" target="_blank" href="https://netflixtechblog.com/announcing-bpftop-streamlining-ebpf-performance-optimization-6a727c1ae2e5">bpftop</a> to measure the overhead of eBPF code in this kernel hot path. Our profiling with <code>bpftop</code> shows that the instrumentation adds less than 600 nanoseconds to each <code>sched_*</code> hook. We conducted a performance analysis on a Java service running in a container, and the instrumentation did not introduce significant overhead. The performance variance with the run queue profiling code active versus inactive was not measurable in milliseconds.</p><p id="2b6f">During our research on how eBPF statistics are measured in the kernel, we identified an opportunity to improve the calculation. We submitted this <a href="https://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git/commit/?id=ce09cbdd988887662546a1175bcfdfc6c8fdd150" rel="noopener ugc nofollow" target="_blank">patch</a>, which was included in the Linux kernel 6.10 release.</p></div><div><p id="3e54">Through trial and error and using <code>bpftop</code>, we identified several optimizations that helped maintain low overhead for our eBPF code:</p><ul><li id="bdd6">We found that <code>BPF_MAP_TYPE_HASH</code> was the most performant for storing enqueued timestamps. Using <code>BPF_MAP_TYPE_TASK_STORAGE</code> resulted in nearly a twofold performance decline. <code>BPF_MAP_TYPE_PERCPU_HASH</code> was slightly less performant than <code>BPF_MAP_TYPE_HASH</code>, which was unexpected and requires further investigation.</li><li id="6517"><code>BPF_MAP_TYPE_LRU_HASH</code> maps are 40–50 nanoseconds slower per operation than regular hash maps. Due to space concerns from PID churn, we initially used them for enqueued timestamps. Ultimately, we settled on <code>BPF_MAP_TYPE_HASH</code> with an increased size to mitigate this risk.</li><li id="0f38">The <code>BPF_CORE_READ</code> helper adds 20–30 nanoseconds per invocation. In the case of raw tracepoints, specifically those that are "BTF-enabled" (<code>tp_btf/*</code>), it is safe and more efficient to access the task struct members directly. Andrii Nakryiko recommends this approach in this <a href="https://nakryiko.com/posts/bpf-core-reference-guide/#btf-enabled-bpf-program-types-with-direct-memory-reads" rel="noopener ugc nofollow" target="_blank">blog post</a>.</li><li id="c6da">The <code>sched_switch</code>, <code>sched_wakeup</code>, and <code>sched_wakeup_new</code> are all triggered for kernel tasks, which are identifiable by their PID of 0. We found monitoring these tasks unnecessary, so we implemented several early exit conditions and conditional logic to prevent executing costly operations, such as accessing BPF maps, when dealing with a kernel task. Notably, kernel tasks operate through the scheduler queue like any regular process.</li></ul><h2 id="d4d7">Conclusion</h2><p id="6194">Our findings highlight the value of low-overhead continuous instrumentation of the Linux kernel with eBPF. We have integrated these metrics into customer dashboards, enabling actionable insights and guiding multitenancy performance discussions. We can also now use these metrics to refine CPU isolation strategies to minimize the impact of noisy neighbors. Additionally, thanks to these metrics, we've gained deeper insights into the Linux scheduler.</p><p id="4565">This work has also deepened our understanding of eBPF technology and underscored the importance of tools like <code>bpftop</code> for optimizing eBPF code. As eBPF adoption increases, we foresee more infrastructure observability and business logic shifting to it. One promising project in this space is <a href="https://github.com/sched-ext/scx" rel="noopener ugc nofollow" target="_blank">sched_ext</a>, which has the potential to revolutionize how scheduling decisions are made and tailored to specific workload needs.</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[SQLite on Rails: The how and why of optimal performance (249 pts)]]></title>
            <link>https://fractaledmind.github.io/2024/04/15/sqlite-on-rails-the-how-and-why-of-optimal-performance/</link>
            <guid>41513648</guid>
            <pubDate>Wed, 11 Sep 2024 17:49:21 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://fractaledmind.github.io/2024/04/15/sqlite-on-rails-the-how-and-why-of-optimal-performance/">https://fractaledmind.github.io/2024/04/15/sqlite-on-rails-the-how-and-why-of-optimal-performance/</a>, See on <a href="https://news.ycombinator.com/item?id=41513648">Hacker News</a></p>
<div id="readability-page-1" class="page"><article>
  <p><img src="https://fractaledmind.github.io/images/wrocloverb-2024/001-alt.png" alt=""></p>

<p>Over the last year or so, I have found myself on a journey to deeply understand how to run Rails applications backed by SQLite performantly and resiliently. In that time, <a href="https://fractaledmind.github.io/2024/04/11/sqlite-on-rails-isolated-connection-pools/">I</a> <a href="https://fractaledmind.github.io/2023/12/11/sqlite-on-rails-improving-concurrency/">have</a> <a href="https://fractaledmind.github.io/2024/01/02/sqlite-quick-tip-multiple-databases/">learned</a> <a href="https://fractaledmind.github.io/2023/09/10/enhancing-rails-sqlite-optimizing-compilation/">various</a> <a href="https://fractaledmind.github.io/2023/09/07/enhancing-rails-sqlite-fine-tuning/">lessons</a> that I want to share with you all now. I want to walk through where the problems lie, why they exist, and how to resolve them.</p>

<p>And to start, we have to start with the reality that…</p>

<p><img src="https://fractaledmind.github.io/images/wrocloverb-2024/022.png" alt=""></p>

<p>Unfortunately, running SQLite on Rails out-of-the-box isn’t viable today. But, with a bit of tweaking and fine-tuning, you can ship a very performant, resilient Rails application with SQLite. And my personal goal for <a href="https://github.com/rails/rails/milestone/87">Rails 8</a> is to make the out-of-the-box experience fully production-ready.</p>

<p>And so, I have spent the last year digging into the details to uncover what the issues are with SQLite on Rails applications as they exist today and how to resolve those issues. So, let me show you everything you need to build a production-ready SQLite-driven Rails application today…</p>

<p><img src="https://fractaledmind.github.io/images/wrocloverb-2024/023.png" alt=""></p>

<p>… Yeah, not too bad, huh? These three commands will set your app up for production success. You will get massive performance improvements, additional SQL features, and point-in-time backups. This is how you build a production-ready SQLite on Rails application today.</p>

<p><img loading="lazy" src="https://fractaledmind.github.io/images/wrocloverb-2024/024.png" alt=""></p>

<p>… And that’s all you need. Thank you. And I could genuinely stop the talk here. You know how and why to run SQLite in production with Rails. Those two gems truly are the headline, and if you take-away only 1 thing from this talk, let it be that slide.</p>

<p>But, given that this is a space for diving deep into complex topics, I want to walk through the exact problems and solutions that these gems package up.</p>

<p><img loading="lazy" src="https://fractaledmind.github.io/images/wrocloverb-2024/025.png" alt=""></p>

<p>To keep this journey practical and concrete, we will be working on <a href="http://github.com/fractaledmind/wrocloverb-2024">a demo app</a> called “Lorem News”. It is a basic Hacker News clone with posts and comments made by users but all of the content is Lorem Ipsum. This codebase will be the foundation for all of our examples and benchmarks.</p>

<p><img loading="lazy" src="https://fractaledmind.github.io/images/wrocloverb-2024/026.png" alt=""></p>

<p>Let’s observe how our demo application performs. We can use the <a href="https://github.com/hatoo/oha"><code>oha</code> load testing CLI</a> and the <a href="https://github.com/fractaledmind/wrocloverb-2024/blob/main/app/controllers/benchmarking_controller.rb">benchmarking routes</a> built into the app to simulate user activity in our app. Let’s start with a simple test where we sent 1 request after another for 5 seconds to our <code>post#create</code> endpoint.</p>

<p><img loading="lazy" src="https://fractaledmind.github.io/images/wrocloverb-2024/027.png" alt=""></p>

<p>Not bad. We see solid RPS and every request is successful. The slowest request is many times slower than the average, which isn’t great, but even that request isn’t above 1 second. I’ve certainly seen worse. Maybe I was wrong to say that the out-of-the-box experience with Rails and SQLite isn’t production-ready as of today.</p>

<p><img loading="lazy" src="https://fractaledmind.github.io/images/wrocloverb-2024/028.png" alt=""></p>

<p>Let’s try the same load test but send 4 concurrent requests in waves for 5 seconds.</p>

<p><img loading="lazy" src="https://fractaledmind.github.io/images/wrocloverb-2024/029.png" alt=""></p>

<p>All of a sudden things aren’t looking as good any more. We see a percentage of our requests are returning 500 error code responses.</p>

<p><img loading="lazy" src="https://fractaledmind.github.io/images/wrocloverb-2024/030.png" alt=""></p>

<p>If we look at our logs, we will see the first major problem that SQLite on Rails applications need to tackle …</p>

<p><img loading="lazy" src="https://fractaledmind.github.io/images/wrocloverb-2024/031.png" alt=""></p>

<p>… the <a href="https://www.sqlite.org/rescode.html#busy"><code>SQLITE_BUSY</code> exception</a>.</p>

<p>In order to ensure only one write operation occurs at a time, SQLite uses a write lock on the database. Only one connection can hold the write lock at a time. If you have multiple connections open to the database, this is the exception that is thrown when one connection attempts to acquire the write lock but another connection still holds it. Without any configuration, a web app with a connection pool to a SQLite database will have numerous errors in trying to respond to requests.</p>

<p><img loading="lazy" src="https://fractaledmind.github.io/images/wrocloverb-2024/032.png" alt=""></p>

<p>As your Rails application is put under more and more concurrent load, you will see a steady increase in the percentage of requests that error with the <code>SQLITE_BUSY</code> exception. What we need is a way to allow write queries to queue up and resolve linearly without immediately throwing an exception.</p>

<p><img loading="lazy" src="https://fractaledmind.github.io/images/wrocloverb-2024/033.png" alt=""></p>

<p>Enter <a href="https://www.sqlite.org/lang_transaction.html#immediate">immediate transactions</a>. Because of the global write lock, SQLite needs different transaction modes for different possible behaviors.</p>

<p><img loading="lazy" src="https://fractaledmind.github.io/images/wrocloverb-2024/034.png" alt=""></p>

<p>Let’s consider this transaction.</p>

<p>By default, SQLite uses a deferred transaction mode. This means that SQLite will not acquire the lock until a write operation is made inside the transaction. For this transaction, this means that the write lock won’t attempt to be acquired until …</p>

<p><img loading="lazy" src="https://fractaledmind.github.io/images/wrocloverb-2024/035.png" alt=""></p>

<p>… this line here, the third operation within the transaction.</p>

<p>In a context where you only have one connection or you have a large amount of transactions that only do read operations, this is great for performance, because it means that SQLite doesn’t have to acquire a lock on the database for every transaction, only for transactions that actually write to the database. The problem is that this is not the context Rails apps are in.</p>

<p><img loading="lazy" src="https://fractaledmind.github.io/images/wrocloverb-2024/036.png" alt=""></p>

<p>In a production Rails application, not only will you have multiple connections to the database from multiple threads, Rails will only wrap database queries that write to the database in a transaction. And, when we write our own explicit transactions, it is essentially a guarantee that we will include a write operation. So, in a production Rails application, SQLite will be working with multiple connections and every transaction will include a write operation. This is the opposite of the context that SQLite’s default deferred transaction mode is optimized for.</p>

<p>Our <code>SQLITE_BUSY</code> exceptions are arising from the fact that when SQLite attempts to acquire the write lock in the middle of a transaction and there is another connection holding the lock, SQLite cannot safely retry that transaction-bound query. Retrying in the middle of a transaction could break the serializable isolation that SQLite guarantees. Thus, when SQLite hits a busy exception when trying to upgrade a transaction, it can’t queue that query to retry acquiring the write lock later; it immediately throws the error and halts that transaction.</p>

<p><img loading="lazy" src="https://fractaledmind.github.io/images/wrocloverb-2024/037.png" alt=""></p>

<p>If we instead begin the transaction by explicitly declaring this an immediate transaction, SQLite will be able to queue this query to retry acquiring the write lock again later. This gives SQLite the ability to serialize the concurrent queries coming in by relying on a basic queuing system, even when some of those queries are wrapped in transactions.</p>

<p>So, how do we ensure that our Rails application makes all transactions immediate? …</p>

<p><img loading="lazy" src="https://fractaledmind.github.io/images/wrocloverb-2024/038.png" alt=""></p>

<p>… As of <a href="https://github.com/sparklemotion/sqlite3-ruby/releases/tag/v1.6.9">version 1.6.9</a>, the <a href="https://github.com/sparklemotion/sqlite3-ruby"><code>sqlite3-ruby</code> gem</a> allows you to configure the default transaction mode. Since Rails passes any top-level keys in your <code>database.yml</code> configuration directly to the <code>sqlite3-ruby</code> database initializer, you can easily ensure that Rails’ SQLite transactions are all run in IMMEDIATE mode.</p>

<p>Let’s make this change in our demo app and re-run our simple load test.</p>

<p><img loading="lazy" src="https://fractaledmind.github.io/images/wrocloverb-2024/039.png" alt=""></p>

<p>With one simple configuration change, our Rails app now handle concurrent load without throwing nearly any 500 errors! Though we do see some errors start to creep in at 16 concurrent requests. This is a signal that something is still amiss.</p>

<p>If we look now at the latency results from our load tests, we will see that this new problem quickly jumps out.</p>

<p><img loading="lazy" src="https://fractaledmind.github.io/images/wrocloverb-2024/040.png" alt=""></p>

<p>As the number of concurrent requests approaches and then surpasses the number of Puma workers our application has, our p99 latency skyrockets. But, interestingly, the actual request time stays stable, even under 3 times the concurrent load of our Puma workers. We will also see that once we start getting some requests taking approximately 5 seconds, we also start getting some 500 <code>SQLITE_BUSY</code> responses as well.</p>

<p><img loading="lazy" src="https://fractaledmind.github.io/images/wrocloverb-2024/041.png" alt=""></p>

<p>If that 5 seconds is ringing a bell, it is because that is precisely what our timeout is set to. It seems that as our application is put under more concurrent load than the number of Puma workers it has, more and more database queries are timing out. This is our next problem to solve.</p>

<p>This timeout option in our <code>database.yml</code> configuration file will be mapped to one of SQLite’s configuration pragmas…</p>

<p><img loading="lazy" src="https://fractaledmind.github.io/images/wrocloverb-2024/042.png" alt=""></p>

<p>SQLite’s <a href="https://www.sqlite.org/pragma.html#pragma_busy_timeout"><code>busy_timeout</code> configuration option</a>. Instead of throwing the <code>BUSY</code> exception immediately, you can tell SQLite to wait up to the timeout number of milliseconds. SQLite will attempt to re-acquire the write lock using a kind of exponential backoff, and if it cannot acquire the write lock within the timeout window, then and only then will the <code>BUSY</code> exception be thrown. This allows a web application to use a connection pool, with multiple connections open to the database, but not need to resolve the order of write operations itself. You can simply push queries to SQLite and allow SQLite to determine the linear order that write operations will occur in. The process will look something like this:</p>

<p><img loading="lazy" src="https://fractaledmind.github.io/images/wrocloverb-2024/043.png" alt=""></p>

<p>Imagine our application sends 4 write queries to the database at the same moment.</p>

<p><img loading="lazy" src="https://fractaledmind.github.io/images/wrocloverb-2024/044.png" alt=""></p>

<p>One of those four will acquire the write lock first and run. The other three will be queued, running the backoff re-acquire logic. Once the first write query completes, …</p>

<p><img loading="lazy" src="https://fractaledmind.github.io/images/wrocloverb-2024/045.png" alt=""></p>

<p>… one of the queued queries will attempt to re-acquire the lock and successfully acquire the lock and start running. The other two queries will continue to stay queued and keep running the backoff re-acquire logic. Again, when the second write query completes, …</p>

<p><img loading="lazy" src="https://fractaledmind.github.io/images/wrocloverb-2024/046.png" alt=""></p>

<p>… another query will have its backoff re-acquire logic succeed and will start running. Our last query is still queued and still running its backoff re-acquire logic.</p>

<p><img loading="lazy" src="https://fractaledmind.github.io/images/wrocloverb-2024/047.png" alt=""></p>

<p>Once the third query completes, our final query can acquire the write lock and run. So long as no query is forced to wait for longer than the timeout duration, SQLite will resolve the linear order of write operations on its own. This queuing mechanism is essential to avoiding <code>SQLITE_BUSY</code> exceptions. But, there is a major performance bottleneck lurking in the details of this feature for Rails applications.</p>

<p><img loading="lazy" src="https://fractaledmind.github.io/images/wrocloverb-2024/048.png" alt=""></p>

<p>Because SQLite is embedded within your Ruby process and the thread that spawns it, care must be taken to release Ruby’s global VM lock (GVL) when the Ruby-to-SQLite bindings execute SQLite’s C code. <a href="https://github.com/sparklemotion/sqlite3-ruby/issues/287#issuecomment-615346313">By design</a>, the <code>sqlite3-ruby</code> gem does not release the GVL when calling SQLite. For the most part, this is a reasonable decision, but for the <code>busy_timeout</code>, it greatly hampers throughput.</p>

<p><img loading="lazy" src="https://fractaledmind.github.io/images/wrocloverb-2024/049.png" alt=""></p>

<p>Instead of allowing another Puma worker to acquire Ruby’s GVL while one Puma worker is waiting for the database query to return, that first Puma worker will continue to hold the GVL even while the Ruby operations are completely idle waiting for the database query to resolve and run. This means that concurrent Puma workers won’t even be able to send concurrent write queries to the SQLite database and SQLite’s linear writes will force our Rails app to process web requests somewhat linearly as well. This radically slows down the throughput of our Rails app.</p>

<p><img loading="lazy" src="https://fractaledmind.github.io/images/wrocloverb-2024/050.png" alt=""></p>

<p>What we want is to allow our Puma workers to be able to process requests concurrently, passing the GVL amongst themselves as they wait on I/O. So, for Rails app using SQLite, this means that we need to unlock the GVL whenever a write query gets queued and is waiting to acquire the SQLite write lock.</p>

<p><img loading="lazy" src="https://fractaledmind.github.io/images/wrocloverb-2024/051.png" alt=""></p>

<p>Luckily, in addition to the <code>busy_timeout</code>, SQLite also provides the lower-level <a href="https://www.sqlite.org/c3ref/busy_handler.html"><code>busy_handler</code> hook</a>. The <code>busy_timeout</code> is nothing more than a specific <code>busy_handler</code> implementation provided by SQLite. Any application using SQLite can provide its own custom <code>busy_handler</code>. The <code>sqlite3-ruby</code> gem is a SQLite driver, meaning that it provides Ruby bindings for the C API that SQLite exposes. Since it provides <a href="https://github.com/sparklemotion/sqlite3-ruby/blob/055da734dafdbb01bb8cf59dbcdb475ea822683f/ext/sqlite3/database.c#L209-L220">a binding for the <code>sqlite3_busy_handler</code> C function</a>, we can write a Ruby callback that will be called whenever a query is queued.</p>

<p><img loading="lazy" src="https://fractaledmind.github.io/images/wrocloverb-2024/052.png" alt=""></p>

<p>Here is a Ruby implementation of the logic you will find in SQLite’s C source for its <code>busy_timeout</code>. Every time this callback is called, it is passed the count of the number of times this query has called this callback. That count is used to determine how long this query should wait to try again to acquire the write lock and how long it has already waited. By using <a href="https://docs.ruby-lang.org/en/master/Kernel.html#method-i-sleep">Ruby’s <code>sleep</code></a>, we can ensure that the GVL is released while a query is waiting to retry acquiring the lock.</p>

<p><img loading="lazy" src="https://fractaledmind.github.io/images/wrocloverb-2024/053.png" alt=""></p>

<p>By ensuring that the GVL is released while queries wait to retry acquiring the lock, we have massively improved our p99 latency even when under concurrent load.</p>

<p>But, there are still some outliers. If we look instead at the p99.99 latency, we will find another steadily increasing graph.</p>

<p><img loading="lazy" src="https://fractaledmind.github.io/images/wrocloverb-2024/054.png" alt=""></p>

<p>Our slowest queries get steadily slower the more concurrent load our application is under. This is another growth curve that we would like to flatten. But, in order to flatten it, we must understand why it is occurring.</p>

<p>The issue is that our Ruby re-implementation of SQLite’s <code>busy_timeout</code> logic penalizes “older queries”. This is going to kill our long-tail performance, as responses will get naturally segmented into the batch that had “young” queries and those that had “old” queries, because SQLite will naturally segment queries into such batches. To explain more clearly what I mean, let’s step through our Ruby <code>busy_timeout</code> logic a couple times.</p>

<p><img loading="lazy" src="https://fractaledmind.github.io/images/wrocloverb-2024/055.png" alt=""></p>

<p>The first time a query is queued and calls this timeout callback, the count is zero.</p>

<p><img loading="lazy" src="https://fractaledmind.github.io/images/wrocloverb-2024/056.png" alt=""></p>

<p>And since 0 is less than 12, we enter the <code>if</code> block.</p>

<p><img loading="lazy" src="https://fractaledmind.github.io/images/wrocloverb-2024/057.png" alt=""></p>

<p>We get the zero-th element in the delays array as our delay, which is 1.</p>

<p><img loading="lazy" src="https://fractaledmind.github.io/images/wrocloverb-2024/058.png" alt=""></p>

<p>We then take the first 0 elements of the delays array, which is an empty array, and sum those numbers together, which in this case sums to 0. This is how long the query has been delayed for already,</p>

<p><img loading="lazy" src="https://fractaledmind.github.io/images/wrocloverb-2024/059.png" alt=""></p>

<p>With our timeout as 5000, 0 + 1 is not greater than 5000, so we fall through to the <code>else</code> block.</p>

<p><img loading="lazy" src="https://fractaledmind.github.io/images/wrocloverb-2024/060.png" alt=""></p>

<p>And we sleep for 1 millisecond before this callback is called again.</p>

<p><img loading="lazy" src="https://fractaledmind.github.io/images/wrocloverb-2024/061.png" alt=""></p>

<p>The tenth time this query calls this timeout callback, the count is, well, 10.</p>

<p><img loading="lazy" src="https://fractaledmind.github.io/images/wrocloverb-2024/062.png" alt=""></p>

<p>10 is still less than 12, so we enter the <code>if</code> block.</p>

<p><img loading="lazy" src="https://fractaledmind.github.io/images/wrocloverb-2024/063.png" alt=""></p>

<p>We get the tenth element in the delays array as our delay, which is 50.</p>

<p><img loading="lazy" src="https://fractaledmind.github.io/images/wrocloverb-2024/064.png" alt=""></p>

<p>We then take the first 10 elements of the delays array, that is the everything in the array up to but not including the tenth element, and sum those numbers together, which in this case sums to 178. This is how long the query has been delayed for already.</p>

<p><img loading="lazy" src="https://fractaledmind.github.io/images/wrocloverb-2024/065.png" alt=""></p>

<p>50 + 178 is still not greater than 5000, so we fall through to the <code>else</code> block.</p>

<p><img loading="lazy" src="https://fractaledmind.github.io/images/wrocloverb-2024/066.png" alt=""></p>

<p>And now we sleep for 50 milliseconds before this callback is called again.</p>

<p><img loading="lazy" src="https://fractaledmind.github.io/images/wrocloverb-2024/067.png" alt=""></p>

<p>Let’s consider the 58th time this query calls this timeout callback.</p>

<p><img loading="lazy" src="https://fractaledmind.github.io/images/wrocloverb-2024/068.png" alt=""></p>

<p>58 is greater than 12, so we fall through to the <code>else</code> block.</p>

<p><img loading="lazy" src="https://fractaledmind.github.io/images/wrocloverb-2024/069.png" alt=""></p>

<p>Once we are past the 12th call to this callback, we will always delay 100 milliseconds.</p>

<p><img loading="lazy" src="https://fractaledmind.github.io/images/wrocloverb-2024/070.png" alt=""></p>

<p>In order to calculate how long this query has already been delayed, we get the sum of the entire delays array and add the 100 milliseconds times however many times beyond 12 the query has retried. In this case, the sum of the entire delays array is 328, 58 minus 12 is 46 and 46 times 100 is 4600. So 4600 plus 328 is 4928. Up to this point, our query has been delayed for 4928 milliseconds.</p>

<p><img loading="lazy" src="https://fractaledmind.github.io/images/wrocloverb-2024/071.png" alt=""></p>

<p>100 + 4928 is 5028, which is indeed greater than 5000, so we enter the <code>if</code> block.</p>

<p><img loading="lazy" src="https://fractaledmind.github.io/images/wrocloverb-2024/072.png" alt=""></p>

<p>And finally we raise the exception.</p>

<p><img loading="lazy" src="https://fractaledmind.github.io/images/wrocloverb-2024/073.png" alt=""></p>

<p>I know that stepping through this code might be a bit tedious, but we all need to be on the same page understanding how SQLite’s <code>busy_timeout</code> mechanism handles queued queries. When I say it penalizes old queries, I mean that it makes them much more likely to become timed out queries under consistent load. To understand why, let’s go back to our queued queries…</p>

<p><img loading="lazy" src="https://fractaledmind.github.io/images/wrocloverb-2024/074.png" alt=""></p>

<p>Let’s track how many retries each query makes from our simple example above.</p>

<p><img loading="lazy" src="https://fractaledmind.github.io/images/wrocloverb-2024/075.png" alt=""></p>

<p>Our three remaining queries have retried once…</p>

<p><img loading="lazy" src="https://fractaledmind.github.io/images/wrocloverb-2024/076.png" alt=""></p>

<p>… and now the remaining two queries are, at best, on their second retry.</p>

<p><img loading="lazy" src="https://fractaledmind.github.io/images/wrocloverb-2024/077.png" alt=""></p>

<p>And our third query is, again at best, on its third retry. On the third retry, the delay is already 10 milliseconds. Let’s imagine that at this moment a new write query is sent to the database.</p>

<p><img loading="lazy" src="https://fractaledmind.github.io/images/wrocloverb-2024/078.png" alt=""></p>

<p>This new query immediately attempts to acquire the write lock, is denied and makes its zeroth call to the <code>busy_timeout</code> callback. It will be told to wait 1 millisecond. Our original query is waiting for 10 milliseconds, so this new query will get to retry again before our older query.</p>

<p><img loading="lazy" src="https://fractaledmind.github.io/images/wrocloverb-2024/079.png" alt=""></p>

<p>While the write lock is still held, our new query is only asked to wait 2 milliseconds next.</p>

<p><img loading="lazy" src="https://fractaledmind.github.io/images/wrocloverb-2024/080.png" alt=""></p>

<p>Even when the count is 2, it is only asked to wait 5 milliseconds. This new query will be allowed to retry to acquire the write lock <strong>three times</strong> before the original query is allowed to retry <em>once</em>.</p>

<p><img loading="lazy" src="https://fractaledmind.github.io/images/wrocloverb-2024/081.png" alt=""></p>

<p>These increasing backoffs greatly penalize older queries, such that any query that has to wait even just 3 retries is now much more likely to never acquire the write lock if there is a steady stream of write queries coming in.</p>

<p><img loading="lazy" src="https://fractaledmind.github.io/images/wrocloverb-2024/082.png" alt=""></p>

<p>So, what if instead of incrementally backing off our retries, we simply had every query retry at the same frequency, regardless of age? Doing so would also mean that we could do away with our <code>delays</code> array and re-think our <code>busy_handler</code> function altogether.</p>

<p><img loading="lazy" src="https://fractaledmind.github.io/images/wrocloverb-2024/083.png" alt=""></p>

<p>And that is precisely <a href="https://github.com/sparklemotion/sqlite3-ruby/pull/456">what we have done</a> in the <code>main</code> branch of the <code>sqlite3-ruby</code> gem. Unfortunately, as of today, this feature is not in a tagged release of the gem, but it should be released relatively soon. This Ruby callback releases the GVL while waiting for a connection using the <code>sleep</code> operation and always sleeps 1 millisecond. These 10 lines of code make a massive difference in the performance of your SQLite on Rails application.</p>

<p>Let’s re-run our benchmarking scripts and see how our p99.99 latency looks now…</p>

<p><img loading="lazy" src="https://fractaledmind.github.io/images/wrocloverb-2024/084.png" alt=""></p>

<p>Voila! We have flattened out the curve. There is still a jump with currency more than half the number of Puma workers we have, but after that jump our long-tail latency flatlines at around half a second.</p>

<p><img loading="lazy" src="https://fractaledmind.github.io/images/wrocloverb-2024/085.png" alt=""></p>

<p>So, when it comes to performance, there are 4 keys that you need to ensure are true of your next SQLite on Rails application…</p>

<p><img loading="lazy" src="https://fractaledmind.github.io/images/wrocloverb-2024/086.png" alt=""></p>

<p>We have covered the first three, but not the last. The <a href="https://www.sqlite.org/wal.html">write-ahead-log</a> allows SQLite to support multiple concurrent readers. The default <a href="https://www.sqlite.org/lockingv3.html">rollback journal mode</a> only allows for one query at a time, regardless of whether it is a read or a write. WAL mode allows for concurrent readers but only one writer at a time.</p>

<p><img loading="lazy" src="https://fractaledmind.github.io/images/wrocloverb-2024/087.png" alt=""></p>

<p>Luckily, <a href="https://github.com/rails/rails/pull/49349">starting with Rails 7.1</a>, Rails applies a better default configuration for your SQLite database. These changes are central to making SQLite work well in the context of a web application. If you’d like to learn more about what each of these configuration options are, why we use the values we do, and how this specific collection of configuration details improve things, I have <a href="https://fractaledmind.github.io/2023/09/07/enhancing-rails-sqlite-fine-tuning/">a blog post</a> that digs into these details.</p>

<p><img loading="lazy" src="https://fractaledmind.github.io/images/wrocloverb-2024/088.png" alt=""></p>

<p>Now, while this isn’t a requirement, there is a fifth lever we can pull to improve the performance of our application. Since we know that SQLite in WAL mode supports multiple concurrent reading connections but only one writing connection at a time, we can recognize that it is possible for the Active Record connection pool to be saturated with writing connections and thus block concurrent reading operations.</p>

<p><img loading="lazy" src="https://fractaledmind.github.io/images/wrocloverb-2024/089.png" alt=""></p>

<p>If your connection pool only has 3 connections, and you receive 5 concurrent queries, what happens if the 3 connections get picked up by three write queries?</p>

<p><img loading="lazy" src="https://fractaledmind.github.io/images/wrocloverb-2024/090.png" alt=""></p>

<p>The remaining read queries have to wait until one of the write queries releases a connection. Ideally, since we are using SQLite in WAL mode, read queries should never need to wait on write queries. In order to ensure this, we will need to create two distinct connection pools—one for reading operation and one for writing operations.</p>

<p><img loading="lazy" src="https://fractaledmind.github.io/images/wrocloverb-2024/091.png" alt=""></p>

<p>We can leverage <a href="https://guides.rubyonrails.org/active_record_multiple_databases.html">Rails’ support for multiple databases</a> to achieve this result. Instead of pointing the reader and writer database configurations to separate databases, we point them at the same single database, and thus simply create two distinct and isolated connection pools with distinct connection configurations.</p>

<p><img loading="lazy" src="https://fractaledmind.github.io/images/wrocloverb-2024/092.png" alt=""></p>

<p>The reader connection pool will only consist of readonly connections…</p>

<p><img loading="lazy" src="https://fractaledmind.github.io/images/wrocloverb-2024/093.png" alt=""></p>

<p>And the writer connection pool will only have one connection.</p>

<p><img loading="lazy" src="https://fractaledmind.github.io/images/wrocloverb-2024/094.png" alt=""></p>

<p>We can then configure our Active Records models to connect to the appropriate connection pool depending on the role.</p>

<p><img loading="lazy" src="https://fractaledmind.github.io/images/wrocloverb-2024/095.png" alt=""></p>

<p>What we want, conceptually, is for our requests to behave essentially like SQLite deferred transactions. Every request should default to using the reader connection pool, but whenever we need to write to the database, we switch to using the writer pool for just that operation. To set that up, we will use Rails’ <a href="https://guides.rubyonrails.org/active_record_multiple_databases.html#activating-automatic-role-switching">automatic role switching feature</a>.</p>

<p><img loading="lazy" src="https://fractaledmind.github.io/images/wrocloverb-2024/096.png" alt=""></p>

<p>By putting this code in an initializer, we will force Rails to set the default database connection for all web requests to be the reading connection pool. We also tweak the delay configuration since we aren’t actually using separate databases, only separate connections, we don’t need to ensure that requests “read your own writes” with a <code>delay</code>.</p>

<p><img loading="lazy" src="https://fractaledmind.github.io/images/wrocloverb-2024/097.png" alt=""></p>

<p>We can then patch the <code>transaction</code> method of the ActiveRecord adapter to force it to connection to the writing database.</p>

<p><img loading="lazy" src="https://fractaledmind.github.io/images/wrocloverb-2024/098.png" alt=""></p>

<p>Taken together, these changes enable our “deferred requests” utilizing isolated connection pools. And when testing against the comment create endpoint, we do see a performance improvement when looking at simple requests per second.</p>

<p><img loading="lazy" src="https://fractaledmind.github.io/images/wrocloverb-2024/099.png" alt=""></p>

<p>So, these are the 5 levels of performance improvement that you should make to your SQLite on Rails application.</p>

<p><img loading="lazy" src="https://fractaledmind.github.io/images/wrocloverb-2024/100.png" alt=""></p>

<p>But, you don’t need to walk through all of these enhancements in your Rails app. As I said at the beginning, you can simply install <a href="https://github.com/fractaledmind/activerecord-enhancedsqlite3-adapter">the enhanced adapter gem</a>.</p>

<p><img loading="lazy" src="https://fractaledmind.github.io/images/wrocloverb-2024/101.png" alt=""></p>

<p>And if you want to use the isolated connection pools, you can simply add this configuration to your application. This is a newer experimental feature, which is why you have to opt into it.</p>

<p><img loading="lazy" src="https://fractaledmind.github.io/images/wrocloverb-2024/110.png" alt=""></p>

<p>And, after all that, we are now done with how to make your SQLite on Rails application performant.</p>

<p><img loading="lazy" src="https://fractaledmind.github.io/images/wrocloverb-2024/115.png" alt=""></p>

<p>In the end, I hope that this exploration of the tools, techniques, and defaults for SQLite on Rails applications has shown you how powerful, performant, and flexible this approach is. Rails is legitimately the best web application framework for working with SQLite today. The community’s growing ecosystem of tools and gems is unparalleled. And today is absolutely the right time to start a SQLite on Rails application and explore these things for yourself.</p>

<p><img loading="lazy" src="https://fractaledmind.github.io/images/wrocloverb-2024/116.png" alt=""></p>

<p>I hope that you now feel confident in the hows (and whys) of optimal performance when running SQLite in production with Rails.</p>

<p>Thank you.</p>

</article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Algorithmic Wage Discrimination (144 pts)]]></title>
            <link>https://columbialawreview.org/content/on-algorithmic-wage-discrimination/</link>
            <guid>41513417</guid>
            <pubDate>Wed, 11 Sep 2024 17:20:24 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://columbialawreview.org/content/on-algorithmic-wage-discrimination/">https://columbialawreview.org/content/on-algorithmic-wage-discrimination/</a>, See on <a href="https://news.ycombinator.com/item?id=41513417">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>

					<div>
				<p>Recent technological developments related to the extraction and processing of data have given rise to concerns about a reduction of privacy in the workplace. For many low-income and subordinated racial minority workforces in the United States, however, on-the-job data collection and algorithmic decisionmaking systems are having a more profound yet overlooked impact: These technologies are fundamentally altering the experience of labor and undermining economic stability and job mobility. Drawing on a multi-year, first-of-its-kind ethnographic study of organizing on-demand workers, this Article examines the historical rupture in wage calculation, coordination, and distribution arising from the logic of informational capitalism: the use of granular data to produce unpredictable, variable, and personalized hourly pay.</p>
<p>The Article constructs a novel framework rooted in worker on-the- job experiences to understand the ascent of digitalized variable pay practices, or the importation of price discrimination from the consumer context to the labor context—what this Article identifies as algorithmic wage discrimination. Across firms, the opaque practices that constitute algorithmic wage discrimination raise fundamental questions about the changing nature of work and its regulation. What makes payment for labor in platform work fair? How does algorithmic wage discrimination affect the experience of work? And how should the law intervene in this moment of rupture? Algorithmic wage discrimination runs afoul of both longstanding precedent on fairness in wage setting and the spirit of equal pay for equal work laws. For workers, these practices produce unsettling moral expectations about work and remuneration. The Article proposes a nonwaivable restriction on these practices.</p>
<p><span>The full text of this Article can be found by clicking the PDF link to the left.</span></p>
			</div> <!--.content-abstract-->
		
					<p>* Professor of Law, University of California, Irvine; Postdoctoral Fellow, Stanford University; Ph.D. 2014, University of California at Berkeley; J.D. 2006, University of California at Berkeley School of Law; B.A. 2003, Stanford University. I thank Aziza Ahmed, Amna Akbar, Abbye Atkinson, Aslı Bâli, Corinne Blalock, James Brandt, Raúl Carillo, Angela Harris, Amy Kapczynski, K-Sue Park, Fernando Rojas, Karen Tani, and Noah Zatz, all of whom offered comments on an early conceptualization of this Article. I am also grateful to Yochai Benkler, Scott Cummings, Sam Harnett, Sarah Myers West, Aziz Rana, Aaron Shapiro, and Meredith Whittaker, who provided critical feedback on drafts and to the brilliant editors of the <em>Columbia Law Review</em>, especially Zakiya Williams Wells. This Article was written at the Center for Advanced Study in the Behavioral Sciences at Stanford University, where I was a fellow from 2022 to 2023. I dedicate it to John Crew, a wonderful mentor and dear friend whose lifelong dedication to justice and fairness shaped both my understandings of and ways of being in this world and who died while I was writing it.</p><!--.content-author-info-->
				
	</div><div id="">

				<h2>INTRODUCTION</h2>
<p>Over the past two decades, technological developments have ushered in extreme levels of workplace monitoring and surveillance across many sectors.
<cite>
	<span>
		<span>1</span>
		See, e.g., Ifeoma Ajunwa, Kate Crawford &amp; Jason Schultz, Limitless Worker Surveillance, 105 Calif. L. Rev. 735, 738–39 (2017) [hereinafter Ajunwa et al., Limitless Worker Surveillance]; Matthew T. Bodie, The Law of Employee Data: Privacy, Property, Governance, 97 Ind. L.J. 707, 712–17 (2022); Brishen Rogers, The Law and Political Economy of Workplace Technological Change, 55 Harv. C.R.-C.L. L. Rev. 531, 535–36 (2020).	</span>
	
</cite>
 These automated systems record and quantify workers’ movement or activities, their personal habits and attributes, and even sensitive biometric information about their stress and health levels.
<cite>
	<span>
		<span>2</span>
		See Ifeoma Ajunwa, Kate Crawford &amp; Joel S. Ford, Health and Big Data: An Ethical Framework for Health Information Collection by Corporate Wellness Programs, 44 J.L. Med. &amp; Ethics 474, 474–75, 477–78 (2016) (describing the comprehensive data collection practices and capacities of worker wellness programs).	</span>
	
</cite>
 Employers then feed amassed datasets on workers’ lives into machine learning systems to make hiring determinations, to influence behavior, to increase worker productivity, to intuit potential workplace problems (including worker organizing), and, as this Article highlights, to determine worker pay.
<cite>
	<span>
		<span>3</span>
		See, e.g., Annette Bernhardt, Linda Kresge &amp; Reem Suleiman, Berkeley Lab. Ctr., Data and Algorithms at Work: The Case for Workers’ Technology Rights 6, 15–17 (2021), https://laborcenter.berkeley.edu/wp-content/uploads/2021/11/Data-and-Algorithms-at-Work.pdf [https://perma.cc/TC3U-458E]. As employment law scholar Matthew Bodie has written in reference the role of data extraction at work under systems of informational capitalism:<br>
Workers find themselves on the wrong end of this data revolution. They are the producers of data, but the data flows seamlessly from their work and personal experience to corporate repositories. Employers can capture the data, aggregate it into meaningful pools, analyze it, and use it to further productivity. Individual employees cannot tap into that value, nor can independent contractors. They are trapped: the more data they provide, the more powerful their employers become.<br>
Bodie, supra note 1, at 736.	</span>
	
</cite>
</p>
<p>To date, policy concerns about growing technological surveillance in the workplace have largely mirrored the apprehensions articulated by consumer advocates. Scholars and advocates have raised concerns about the growing limitations on worker privacy and autonomy, the potential for society-level discrimination to seep into machine learning systems, and a general lack of transparency on workplace rules.
<cite>
	<span>
		<span>4</span>
		See generally Bernhardt et al., supra note 3 (arguing that data-driven technologies harm workers through discrimination and work intensification at the expense of safety, depriving workers of their autonomy and dignity); Ajunwa et al., Limitless Worker Surveillance, supra note 1 (“[T]here has been a shift in focus from collecting personally identifying information, such as health records, to wholly acquiring unprotected and largely unregulated proxies and metadata, such as wellness information, search queries, social media activity, and outputs of predictive ‘big data’ analytics.”); Bodie, supra note 1 (“As the data collected in this new environment has become increasingly individualized, the line between person as individual and person as employee has become significantly blurred.”); Rogers, supra note 1 (“[L]abor and employment laws . . . and the broader political economy of work that they help sustain, also encourage employers to use new technologies to exert power over workers.”). Labor law scholars Antonio Aloisi and Valerio De Stefano have argued convincingly in a comprehensive review of technology, law, and work that concerns about the supposed “disappearance of work” lost to algorithmic intelligence are less urgent than the myriad challenges raised by the incipient practices of algorithmic management at work. These nascent practices, they argue, have intensified any number of problems including the devaluation of work, the maldistribution of risks and privileges, the health and safety of workers, the assault on dignity, and of course, the destruction of individual and collective worker privacy. Antonio Aloisi &amp; Valerio De Stefano, Your Boss Is an Algorithm: Artificial Intelligence, Platform Work and Labour 9, 23–24, 98–101, 104–05 (2022).	</span>
	
</cite>
 For example, in October 2022, the White House Office of Science and Technology Policy released a non-legally-binding handbook identifying five principles that “should guide the design, use, and deployment of automated systems to protect the American public in the age of artificial intelligence.”
<cite>
	<span>
		<span>5</span>
		White House Off. of Sci. &amp; Tech. Pol’y, Blueprint for an AI Bill of Rights 3 (2022), https://www.whitehouse.gov/wp-content/uploads/2022/10/Blueprint-for-an-AI-Bill-of-Rights.pdf [https://perma.cc/A62C-TV47].	</span>
	
</cite>
 These principles called for automated systems that (1) were safe and effective, (2) protect individuals from discrimination, (3) offer users control over how their data is used, (4) provide notice and explanation that an automated system is being used, and (5) allow users access to a person who can remedy any problems they encounter.
<cite>
	<span>
		<span>6</span>
		Id. at 5–7.	</span>
	
</cite>
 The<em> Blueprint for an AI Bill of Rights</em> (hereinafter <em>Blueprint</em>) specified that these enumerated rights extended to “[e]mployment-related systems [such as] . . . workplace algorithms that inform all aspects of the terms and conditions of employment including, but not limited to, <em>pay</em> or <em>promotion</em>, hiring or termination algorithms, virtual or augmented reality workplace training programs, and electronic workplace surveillance and management systems.”
<cite>
	<span>
		<span>7</span>
		Id. at 53 (emphasis added).	</span>
	
</cite>
</p>
<p>Under each principle, the <em>Blueprint</em> provides “illustrative examples” of the kinds of harms that the principle is meant to address. One such example, used to specify what defines unsafe and ineffective automation in the workplace, involves an unnamed company that has installed AI-powered cameras in their delivery vans to monitor workers’ driving habits, ostensibly for “safety reasons.” The <em>Blueprint</em> states that the system “<em>incorrectly</em> penalized drivers when other cars cut them off . . . . As a result, drivers were <em>incorrectly</em> ineligible to receive a bonus.”
<cite>
	<span>
		<span>8</span>
		Id. at 17 (emphasis added) (citing Lauren Kaori Gurley, Amazon’s AI Cameras Are Punishing Drivers for Mistakes They Didn’t Make, Vice (Sept. 20, 2021), https://www.vice.com/en/article/88npjv/amazons-ai-cameras-are-punishing-drivers-for-mistakes-they-didnt-make [https://perma.cc/HSF4-EG4M]).	</span>
	
</cite>
 Thus, the specific harm identified is a mistaken calculation by an automated variable pay system developed by the company.</p>
<p>What the <em>Blueprint</em> does not specify, however, is that the company in question—Amazon—does not directly employ the delivery workers. Rather, the company contracts with Delivery Service Providers (DSPs), small businesses that Amazon helps to establish. In this putative nonemployment arrangement, Amazon does not provide to the DSP drivers workers’ compensation, unemployment insurance, health insur-ance, or the protected right to organize. Nor does it guarantee individual DSPs or their workers minimum wage or overtime compensation.
<cite>
	<span>
		<span>9</span>
		As economist Brian Callaci explains, since the DSPs legally employ the delivery drivers, the DSPs, rather than Amazon, bear “liability for accidents or workplace safety,” and DSP drivers, classified as Amazon’s contractors, “do not fall under Amazon’s $15 an hour minimum wage.” Brian Callaci, Entrepreneurship, Amazon Style, Am. Prospect (Sept. 27, 2021), https://prospect.org/api/content/1923a910-1d7c-11ec-8dbf-1244d5f7c7c6/ [https://perma.cc/‌AV2H-59YA]. Meanwhile, Amazon’s contracts with DSPs “[restrict] the wages the DSP can offer” drivers and mandate that drivers remain nonunion by stipulating that “they serve as at-will employees.” Id. If the drivers unionize, “Amazon can terminate the contract and find a new DSP, which is much easier than fighting a union campaign itself.” Id.	</span>
	
</cite>
 Instead, DSPs receive a variable hourly rate based on fluctuations in demand and routes, along with “bonuses” based on a quantified digital evaluation of on-the-job behavior, including “service, safety, [and] client experience.”
<cite>
	<span>
		<span>10</span>
		How Are Amazon DSPs Paid?, Route Consultant, https://www.routeconsultant.com/industry-insights/how-are-amazon-dsps-paid [https://perma.cc/684P-WLKB] (last visited Aug. 14, 2023). The scorecards that determine “bonuses” are calculated in constantly changing ways. The DSP scorecards I reviewed include four categories: safety and compliance, reliability, quality, and team. The “scores” for these categories—and for each driver employed by the DSP—are determined algorithmically. See also Peak Delivery Driver, Amazon DSP Scorecard Deep Dive, YouTube, at 1:04–1:58, 2:48–3:10 (Sept. 10, 2021), https://www.youtube.com/watch?v=-mBOYfBZs9I (on file with the <em>Columbia Law Review</em>). The example in the <em>Blueprint</em>, for instance, lowered the score enough to undermine the DSP’s ability to get a bonus. White House Off. of Sci. &amp; Tech. Pol’y, supra note 5, at 17. By contrast, Amazon is guaranteed the data it wants from the DSPs (they cannot reject the use of cameras, for example)—not just while the DSP is servicing Amazon but also for three years afterward. In addition to using such data to calculate bonuses, Amazon can also use it to terminate contracts, terminate specific “underperforming” workers, and punish DSPs with fees. Josh Eidelson &amp; Matt Day, Drivers Don’t Work for Amazon but Company Has Lots of Rules for Them, Det. News (May 5, 2021), https://www.detroitnews.com/story/business/2021/05/05/drivers-dont-work-amazon-but-company-has-lots-rules-them/4955413001/ [https://perma.cc/7REA-NKRU].	</span>
	
</cite>
 DSPs, while completely reliant on Amazon for business, must hire a team of drivers as employees.
<cite>
	<span>
		<span>11</span>
		When a DSP hires other drivers, it may appear more like a company that is legally separate from Amazon. This may protect Amazon from unionization efforts and downstream liability that it may otherwise incur based on allegations that the DSPs are its employees, not contractors. Callaci, supra note 9. It appears FedEx was the first delivery company to use this tactic after redrafting its contracts with drivers in response to Alexander v. FedEx Ground Package Sys., Inc., 765 F.3d 981 (9th Cir. 2014), the Ninth Circuit decision that held that its drivers were employees, not independent contractors. Rather than changing the drivers’ status in response to the decision, FedEx drafted its contracts to make the drivers appear more like independent contractors. V.B. Dubal, Winning the Battle, Losing the War?: Assessing the Impact of Misclassification Litigation on Workers in the Gig Economy, 2017 Wis. L. Rev. 739, 791–92. This included mandating that the drivers purchase more service areas, which in turn made drivers hire others to complete the deliveries. Id.	</span>
	
</cite>
 These Amazon-created and -controlled small businesses rely heavily on their automated “bonuses” to pay for support, repairs, and driver wages.
<cite>
	<span>
		<span>12</span>
		Lauren Kaori Gurley, Amazon’s AI Cameras Are Punishing Drivers for Mistakes They Didn’t Make, Vice (Sept. 20, 2021), https://www.vice.com/en/article/88npjv/amazons-ai-cameras-are-punishing-drivers-for-mistakes-they-didnt-make [https://perma.cc/HSF4-EG4M].	</span>
	
</cite>
 As one DSP owner–worker complained to an investigator, “Amazon uses these [AI surveillance] cameras allegedly to make sure they have a safer driving workforce, but they’re actually using them not to pay [us] . . . . They just take our money and expect that to motivate us to figure it out.”
<cite>
	<span>
		<span>13</span>
		Id. (internal quotation marks omitted) (quoting the owner of a Washington-based Amazon delivery company).	</span>
	
</cite>
</p>
<p>Presented with this additional information, we should ask again: What exactly is the harm of this automated system? Is it, as the <em>Blueprint</em> states, the algorithm’s <em>mistake</em>, which prevented the worker from getting his bonus? Or is it the structure of Amazon’s payment system, rooted in evasion of employment law, data extraction from labor, and digitalized control?</p>
<p>Amazon’s automated control structure and payment mechanisms represent an emergent and undertheorized firm technique arising from the logic of informational capitalism: the use of <em>algorithmic wage discrimination</em> to maximize profits and to exert control over worker behavior.
<cite>
	<span>
		<span>14</span>
		“Informational capitalism” or “information capitalism” as a descriptor of the contemporary digital-age world system is generally attributed to sociologist Manuel Castells. Castells first introduced the term in his three-volume study, <em>The Information Age</em>, published between 1996 and 1998. In describing a shift from industrial capitalism to information capitalism, Castells wrote in Volume I, “A technological revolution, centered around information technologies, is reshaping, at accelerated pace, the material basis of society. Economies throughout the world have become globally interdependent, introducing a new form of relationship between economy, state, and society, in a system of variable geometry.” Manuel Castells, The Rise of Network Society 1 (2d ed. 2000). In legal scholarship, Julie Cohen uses the term “informational capitalism” to explore the relationships between political, legal, and economic institutions amidst the propertized expansion of data and information exchange. See generally Julie Cohen, Between Truth and Power: The Legal Constructions of Informational Capitalism (2019).	</span>
	
</cite>
 “Algorithmic wage discrimination” refers to a practice in which individual workers are paid different hourly wages—calculated with ever-changing formulas using granular data on location, individual behavior, demand, supply, or other factors—for broadly similar work. As a wage-pricing technique, algorithmic wage discrimination encompasses not only digitalized payment for completed work but, critically, digitalized decisions to allocate work, which are significant determinants of hourly wages and levers of firm control. These methods of wage discrimination have been made possible through dramatic changes in cloud computing and machine learning technologies in the last decade.
<cite>
	<span>
		<span>15</span>
		Zephyr Teachout has created a useful taxonomy of five different forms of “personalized wages” that have recently emerged in the labor market: (1) extreme Taylorism, in which “[h]igh degrees of surveillance [result in] . . . rewarding productivity”; (2) gamification, in which employers use psychological tools to incentivize task completion; (3) behavioral price discrimination, in which workers get paid more if they make certain lifestyle choices, like exercising, which can be tracked through fitness apps; (4) dynamic labor pricing, which, she argues, is based primarily on demand; and (5) experimentation, in which firms test “assumptions about what will lead to the firm gathering the highest output for the wages it pays.” Zephyr Teachout, Algorithmic Personalized Wages, 51 Pol. &amp; Soc’y 436, 437, 442–44 (2023) [hereinafter Teachout, Algorithmic Personalized Wages].<br>
In all these instances, wages are rooted in data extracted from labor. My data indicate the potential to further simplify this taxonomy to two main ways of thinking about algorithmic wage discrimination: (1) wages based on productivity analysis alone (most evident in the employment context), and (2) wages based on productivity, supply, demand, and other personalized data used to minimize labor costs. This second form of algorithmic wage discrimination appears most commonly in on-demand work that treats workers like independent contractors.	</span>
	
</cite>
</p>
<p>Though firms have relied upon performance-based variable pay for some time (e.g., the use of bonuses and commission systems to influence worker behavior),
<cite>
	<span>
		<span>16</span>
		Nonalgorithmic variable payment systems with transparent payment structures are more familiar to many people. See, e.g., United Farm Workers (@UFWupdates), Twitter (Oct. 15, 2022), https://twitter.com/UFWupdates/status/1577795973476220930 (on file with the <em>Columbia Law Review</em>) (showing how California companies use a variable bonus system for some farmers’ pay). They are, nonetheless, controversial. Some critics in the human relations and management literature point to variable pay mechanisms as a contributor to income gaps by gender and race. See, e.g., Emilio J. Castilla, Gender, Race, and Meritocracy in Organizational Careers, 13 Am. J. Socio. 1479, 1502–17 (2008) (finding variable salary bias in salary increases and promotions on the basis of gender, race, and nationality). Others suggest variable pay has psychological costs for workers and other unforeseen consequences. See, e.g., Annette Cox, The Outcomes of Variable Pay Systems: Tales of Multiple Costs and Unforeseen Consequences, 16 Int’l J. Hum. Res. Mgmt. 1475, 1483–93 (2005) (discussing unexpected costs to both employers and employees resulting from variable salary systems).	</span>
	
</cite>
 my research on the on-demand ride hail industry suggests that algorithmic wage discrimination raises a new and distinctive set of concerns. In contrast to more traditional forms of variable pay, algorithmic wage discrimination—whether practiced through Amazon’s “bonuses” and scorecards or Uber’s work allocation systems, dynamic pricing, and wage incentives—arises from (and may function akin to) the practice of “price discrimination,” in which individual consumers are charged as much as a firm determines they may be willing to pay.
<cite>
	<span>
		<span>17</span>
		To date, scholars and analysts who have written about what this Article terms “algorithmic wage discrimination” have predominantly adopted the language of pricing, though they describe wage and not product pricing. For example, in her 2021 Enlund Lecture at DePaul University School of Law, Professor Zephyr Teachout referenced some of these practices as “labor price discrimination.” Zephyr Teachout, Professor, Fordham Univ. Sch. of L., Enlund Lecture at DePaul University School of Law (Apr. 15, 2021). Niels van Doorn, in an article analyzing the pay structures of on-demand Deliveroo riders in Berlin, describes “the algorithmic price-setting power of food delivery platforms,” which he understands as a “monopsonistic power that is not only market-making but also potentially livelihood-taking.” Niels van Doorn, At What Price? Labour Politics and Calculative Power Struggles in On-Demand Food Delivery, 14 Work Org. Lab. &amp; Globalisation, no. 1, 2020, at 136, 138. But adopting the language of “pricing” for wage setting is politically and legally consequential. Since at least the rise of neoliberalism, price controls in the United States (and elsewhere) have been highly disfavored as economic interferences in the “free market,” raising conservative critiques of socialism and “planned economies.” See Benjamin C. Waterhouse, Lobbying America: The Politics of Business From Nixon to NAFTA 10623, 13239 (2013) (describing how American businesses rejected government price setting in the Nixon, Ford, and Carter administrations). Wage controls in the form of minimum-wage and overtime laws, on the other hand, have been contested but culturally naturalized as a necessary (or at least, accepted) part of economic regulation. See Amina Dunn, Most Americans Support a $15 Federal Minimum Wage, Pew Rsch. Ctr. (Apr. 22, 2021), https://www.pewresearch.org/short-reads/2021/04/22/most-americans-support-a-15-federal-minimum-wage/ [https://perma.cc/CX5Z-YX9Z] (surveying support for minimum-wage laws across the United States). In this sense, conceptualizing the digitalized wages received by workers not as firm price determinations but as firm wage determinations is a critical political—and legal—corrective.	</span>
	
</cite>
 As a labor management practice, algorithmic wage discrimination allows firms to personalize and differentiate wages for workers in ways unknown to them, paying them to behave in ways that the firm desires, perhaps for as little as the system determines that the workers may be willing to accept.
<cite>
	<span>
		<span>18</span>
		See infra Part II.	</span>
	
</cite>
 Given the information asymmetry between workers and firms, companies can calculate the exact wage rates necessary to incentivize desired behaviors, while workers can only guess how firms determine their wages.
<cite>
	<span>
		<span>19</span>
		See Aaron Shapiro, Dynamic Exploits: Calculative Asymmetries in the On-Demand Economy, 35 New Tech. Work &amp; Emp. 162, 162–63 (2020) [hereinafter Shapiro, Dynamic Exploits: Calculative Asymmetries] (arguing that “independent service providers” for “on-demand service platforms” are workers and not independent contractors because the platforms set wages and “exhibit substantial information asymmetries”). Uber, for its part, has stated that “suggestions that Uber offers variable pricing based on user-profiling is completely unfounded and factually incorrect.” Cansu Safak &amp; James Farrar, Worker Info Exch., Managed by Bots: Data-Driven Exploitation in the Gig Economy 26 (2021), https://5b88ae42-7f11-4060-85ff-4724bbfed648.usrfiles.com/ugd/5b88ae_8d720d54443543e2a928267d354acd90.pdf [https://perma.cc/TLV3-R2EE] (internal quotation marks omitted) (quoting Letter from Uber Data Protection and Cybersecurity Team to Cansu Safak (Dec. 3, 2021), https://5b88ae42-7f11-4060-85ff-4724bbfed648.usrfiles.com/ugd/5b88ae_f12953beac7e4fd9b6057375cce212b5.pdf [https://perma.cc/LL6M-KVGV]). We have no way to judge the accuracy of this statement.<br>
Since a draft of this Article was posted online, Uber drivers have adopted the term “algorithmic wage discrimination,” testified to how it reflects how they are paid, and documented how they are offered different base pay for the exact same ride when sitting next to each other. See, e.g., The RideShare Guy, The Age of Algorithmic Wage Discrimination for Uber &amp; Lyft Drivers and More?!, YouTube, at 2:16 (Apr. 16, 2023), https://www.youtube.com/watch?v=MfFujB0IY6A (on file with the <em>Columbia Law Review</em>); The RideShare Guy, MORE Algorithmic Wage Discrimination?? Show Me The Money Club, YouTube, at 6:25, 1:01:03 ( June 20, 2023), https://www.youtube.com/watch?v=8mwzsB41-f4 (on file with the <em>Columbia Law Review</em>).	</span>
	
</cite>
</p>
<p>The <em>Blueprint</em> example underscores how algorithmic wage discrimination can be “ineffective” and rife with calculated mistakes that are difficult to ascertain and correct. But algorithmic wage discrimination also creates a labor market in which people who are doing the same work, with the same skill, for the same company, at the same time may receive different hourly pay.
<cite>
	<span>
		<span>20</span>
		See infra Part II.	</span>
	
</cite>
 Digitally personalized wages are often determined through obscure, complex systems that make it nearly impossible for workers to predict or understand their constantly changing, and frequently declining, compensation.
<cite>
	<span>
		<span>21</span>
		See infra Part II.	</span>
	
</cite>
</p>
<p>Drawing on anthropologist Karl Polanyi’s notion of <em>embeddedness</em>—the idea that social relations are embedded in economic systems
<cite>
	<span>
		<span>22</span>
		In 1957, Karl Polanyi wrote,<br>
Instead of economy being embedded in social relations, social relations are embedded in the economic system. The vital importance of the economic factor to the existence of society precludes any other result. For once the economic system is organized in separate institutions, based on specific motives and conferring a special status, society must be shaped in such a manner as to allow that system to function according to its own laws.<br>
Karl Polanyi, <em>The Great Transformation</em>: The Political and Economic Origins of Our Time 60 (Beacon Press 2001) (1944). One interpretation of this important excerpt, as used in this Article, is that Polanyi was referring to the ways in which society adapts to and reorganizes itself “by demanding new social institutions that can constrain market forces and compensate for market failures.” Bob Jessop &amp; Ngai-Ling Sum, Polanyi: Classical Moral Economist or Pioneer Cultural Political Economist?, 44 Östereichische Zeitschrift für Soziologie 153, 158 (2019). This, in essence, is what he calls the “embedded economy”: that in order to prevent a “Hobbesian war of all against all,” a market society must limit—through law, politics, and morality—the range of legitimate activities of economic actors motivated by material gain. Fred Block, Karl Polanyi and the Writing of The Great Transformation, 32 Theory &amp; Soc’y 275, 297 (2003).	</span>
	
</cite>
—this Article excavate the norms around payment that constitute what one might consider a moral economy of work to help situate this contemporary rupture in wages.
<cite>
	<span>
		<span>23</span>
		Various disciplines, including political theory, anthropology, and sociology, have explored the notion of “moral economy” in relationship to labor and work as a way to think about and assess various systems of economic distribution and their impacts on everyday life. See, e.g., William Greider, The Soul of Capitalism 39 (2003) (“The logic of capitalism is ingeniously supple and complete, self-sustaining and forward-looking. Except for one large incapacity: As a matter of principle, it cannot take society’s interests into account.”); James Bernard Murphy, The Moral Economy of Labor 42 (1993) (applying moral reason to the social division of labor and technology); Sharon C. Bolton, Maeve Houlihan &amp; Knut Laaser, Contingent Work and Its Contradictions: Towards a Moral Economy Framework, 111 J. Bus. Ethics 121, 123–124 (2012); Sharon C. Bolton &amp; Knut Laaser, Work, Employment and Society Through the Lens of Moral Economy, 27 Work Emp. &amp; Soc’y 508, 509 (2013) (using a moral economic approach in a sociological inquiry); Marion Fourcade, Philippe Steiner, Wolfgang Streeck &amp; Cornelia Woll, Moral Categories in the Financial Crisis 2 (Max Planck Sciences Po Ctr. on Coping With Instability in Mkt. Societies (MaxPo) Discussion Paper, Working Paper No. 13/1, 2013), https://www.econstor.eu/bitstream/10419/104613/1/757489362.pdf [https://perma.cc/4ZJ4-QYC4] (analyzing the reconfiguration of the moral economy surrounding income inequality in France following the 2008 financial crisis).	</span>
	
</cite>
 Although the United States–based system of work is largely regulated through contracts and strongly defers to the managerial prerogative,
<cite>
	<span>
		<span>24</span>
		See Gali Racabi, Abolish the Employer Prerogative, Unleash Work Law, 43 Berkeley J. Emp. &amp; Lab. L. 79, 82 (2022) (“The employer [or managerial] prerogative is the default governance rule in the workplace . . . .”). This legal deference to the managerial prerogative is controversial in the scholarly literature. See, e.g., id. at 138 (“[P]erhaps the employer prerogative’s most sinister effect is convincing work law movements, scholars, and activists that it is a state of nature, a necessary theoretical benchmark for both pragmatic and normative discussions of work law. It is not.”).	</span>
	
</cite>
 two restrictions on wages have emerged from social and labor movements: minimum-wage laws and antidiscrimination laws. Respectively, these laws set a price floor for the purchase of labor relative to time and prohibit identity-based discrimination in the terms, con-ditions, and privileges of employment, requiring firms to provide equal pay for equal work.
<cite>
	<span>
		<span>25</span>
		At the federal level, the Fair Labor Standards Act of 1938, 29 U.S.C. §§ 201–219 (2018), establishes a national floor for minimum-wage and overtime. Id. at §§ 203, 206, 207. The central federal laws that prohibit wage discrimination based on protected identities or classes are the Equal Pay Act, 29 U.S.C. § 206(d) (requiring that men and women in the same workplace be given equal pay for equal work); Title VII of the Civil Rights Act of 1964, 42 U.S.C. § 2000e-2 (2018) (prohibiting employment discrimination based on race, color, religion, sex, and national origin); the Age Discrimination in Employment Act, 29 U.S.C. §§ 623, 631 (prohibiting employment discrimination based on age for workers older than forty); and the Americans with Disabilities Act, 42 U.S.C. § 12112 (prohibiting employment discrimination based on disability).	</span>
	
</cite>
 Both sets of wage laws can be understood as forming a core moral foundation for most work regulation in the United States. In turn, certain ideals of fairness have become embedded in cultural and legal expectations about work. Part I examines how recently passed laws in California and Washington State, which specifically legalize algorithmic wage discrimination for certain firms, compare with and destabilize more than a century of legal and social norms around fair pay.</p>
<p>Part II draws on first-of-its-kind, long-term ethnographic research to understand the everyday, grounded experience of workers earning through and experiencing algorithmic wage discrimination. Specifically, Part II analyzes the experiences of on-demand ride-hail drivers in California before and after the passage of an important industry-initiated law, Proposition 22, which legalized this form of variable pay. This Part illuminates workers’ experiences under compensation systems that make it difficult for them to predict and ascertain their hourly wages. Then, Part II examines the practice of algorithmic wage discrimination in rela-tionship to workers’ on-the-job meaning making and their moral interpretations of their wage experiences.
<cite>
	<span>
		<span>26</span>
		The social construction of meaning is a central concern of sociologists and anthropologists who seek to account for the variability and diversity of human understandings and experiences. Compare Michèle Lamont, Meaning-Making in Cultural Sociology: Broadening Our Agenda, 29 Contemp. Socio. 602, 603–05 (2000) (offering a detailed taxonomy of sociological literature that takes up how people make sense of their worlds through their experiences of race, ethnicity, immigration, and inequality), with Richard A. Posner, Economic Analysis of Law 3–4 (9th ed. 2014) (describing rationality as grounded within self-interested economic maximization of scarce resources).	</span>
	
</cite>
 Though many drivers are attracted to on-demand work because they long to be free from the rigid scheduling structures of the Fordist work model,
<cite>
	<span>
		<span>27</span>
		Philosopher Antonio Gramsci used the term “Fordism” to refer to an emergent system of material production—routine, intensified labor—under the regime of Ford. But due in large part to corresponding political and economic forces, namely the laws and policies passed in response to upheaval during the Great Depression, the Fordist work structure in much of the mid-twentieth century often corresponded to an hourly (living) wage and a forty-hour work week. See Antonio Gramsci, Americanism and Fordism, in Selections From the Prison Notebooks of Antonio Gramsci 561, 56163 (Quentin Hoare &amp; Geoffrey Nowell Smith eds. and trans., 1999). For more on the demise of Fordism, see generally Luc Boltanski &amp; Ève Chiapello, The New Spirit of Capitalism (2007).	</span>
	
</cite>
 they still largely conceptualize their labor through the lens of that model’s payment structure: the hourly wage.
<cite>
	<span>
		<span>28</span>
		See Michael Dunn, Making Gigs Work: Digital Platforms, Job Quality and Worker Motivations, 35 New Tech. Work &amp; Emp. 232, 238–39, 241–42 (2020) (discussing the motivations of gig workers, including flexible work hours, despite often needing to maintain the same work structures as traditional employment). It should be noted that nothing about employment status necessitates an inflexible work schedule. This is a business decision associated with, not mandated by, employment. For a discussion of the history of businesses contesting the legal rules defining employment status to avoid legal responsibility for basic employment safeguards, see Veena B. Dubal, Wage Slave or Entrepreneur?: Contesting the Dualism of Legal Worker Identities, 105 Calif. L. Rev. 65, 86–88 (2017) [hereinafter Dubal, Wage Slave or Entrepreneur?]. Notably, the passage of California’s AB5 law made it much harder to misclassify workers in this way. See Hannah Johnston, Ozlem Ergun, Juliet Schor &amp; Lidong Chen, Is Employment Status Compatible With the On-Demand Platform Economy? Evidence From a Natural Experiment 6 (2021) (unpublished report) (on file with the <em>Columbia Law Review</em>). When at least one labor platform company, called Bring Your Package, went on to hire their previously contracted workers in anticipation of AB5 restrictions, this transition did not precipitate any reduction in workers’ desired scheduling flexibility nor in firm efficiency. See id. at 14, 24, 26–27.	</span>
	
</cite>
 Workers find that, in contrast to more standard wage dynamics, being directed by and paid through an app involves opacity, deception, and manipulation.
<cite>
	<span>
		<span>29</span>
		These findings comport with research findings from across sociology, communications studies, and media studies literatures on algorithmic management. See, e.g., Antonio Aloisi, Platform Work in Europe: Lessons Learned, Legal Developments and Challenges Ahead, 13 Euro. Lab. L.J. 4, 10–11 (2022) (discussing how platform manage-ment tends to unfold in misleading, opaque ways); Rafael Grohmann, Gabriel Pereira, Abel Guerra, Ludmila Costhek Abilio, Bruno Moreschi &amp; Amanda Jurno, Platform Scams: Brazilian Workers’ Experiences of Dishonest and Uncertain Algorithmic Management, 24 New Media &amp; Soc’y 1611, 1614 tbl.1 (2022) (presenting case studies of the types of dishonesty and deception that workers experience in platform work); Elke Schüßler, Will Attwood-Charles, Stefan Kirchner &amp; Juliet B. Schor, Between Mutuality, Autonomy and Domination: Rethinking Digital Platforms as Contested Relational Structures, 19 Socio-Econ. Rev. 1217, 1224 (2021) (outlining common theories of the position of power that platforms hold over their workers); Steven Vallas &amp; Juliet B. Schor, What Do Platforms Do? Understanding the Gig Economy, 46 Ann. Rev. Socio. 273, 279–81 (2020) (conducting a literature review of the predominant sociological views of platform work, which often conceptualize this work as an extension of existing neoliberal models of work without any of the worker protections); Daniel Susser, Beate Roessler &amp; Helen Nissenbaum, Technology, Autonomy, and Manipulation, 8 Internet Pol’y Rev., no. 2, 2019, at 1, 8 (explaining how gig economy services covertly influence an individual’s decision-making through “online manipulation”).	</span>
	
</cite>
 Those who are most economically dependent on income from on-demand work frequently describe their experience of algorithmic wage discrimination through the lens of gambling.
<cite>
	<span>
		<span>30</span>
		See infra section II.B.	</span>
	
</cite>
 As a normative matter, this Article contends that workers laboring for firms (especially large, well-financed ones like Uber, Lyft, and Amazon) should not be subject to the kind of risk and uncertainty associated with gambling as a condition of their work. In addition to the salient constraints on autonomy and threats to privacy that accompany the rise of on-the-job data collection, algorithmic wage discrimination poses significant problems for worker mobility, worker security, and worker collectivity, both on the job and outside of it. Because the on-demand workforces that are remunerated through algorithmic wage discrimination are primarily made up of immigrants and racial minority workers, these harmful economic impacts are also necessarily racialized.
<cite>
	<span>
		<span>31</span>
		In the United States, such work is conducted primarily by immigrants and subordinated minorities. Lyft estimates that 73% of their U.S. workforce identify as racial minorities. Lyft, Economic Impact Report 5 (2022), https://s27.q4cdn.com/263799617/<br>
files/doc_downloads/esg/Lyft-Economic-Impact-Report-2022.pdf [https://perma.cc/8BUG-NGAV]. One study estimates that in the San Francisco Bay Area in 2019, immigrants and people of color composed 78% of Uber and Lyft drivers, most of whom relied on these jobs as their primary source of income. Chris Benner, Erin Johansson, Kung Feng &amp; Hays Witt, UC Santa Cruz Inst. for Soc. Transformation, On-Demand and On-The-Edge: Ride-Hailing and Delivery Workers in San Francisco, Executive Summary 2 (2020), https://transform.ucsc.edu/wpcontent/uploads/2020/05/OnDemandOntheEdge_ExecSum.pdf [https://perma.cc/DFH8-7VSY]. In addition to the nationwide Lyft data, we know that in New York City, 90% of ride-hail drivers are immigrants, and in Seattle, ride-hail drivers are 50% Black and “nearly three times more likely to be immigrants than all Kings County workers.” James A. Parrott &amp; Michael Reich, Ctr. on Wage &amp; Emp. Dynamics &amp; New Sch. Ctr. for N.Y.C. Affs., A Minimum Compensation Standard for Seattle TNC Drivers 23 (2020), https://irle.berkeley.edu/files/2020/07/Parrott-Reich-Seattle-Report_July-2020.pdf [https://perma.cc/QA9F-FV47] [hereinafter Parrott &amp; Reich, Minimum Compensation Standard]; Ginia Bellafante, Uber and the False Hopes of the Sharing Economy, N.Y. Times (Aug. 9, 2018), https://www.nytimes.com/2018/08/09/nyregion/uber-nyc-vote-drivers-ride-sharing.html (on file with the <em>Columbia Law Review</em>).	</span>
	
</cite>
</p>
<p>Finally, Part III explores how workers and worker advocates have used existing data privacy laws and cooperative frameworks to address or at least to minimize the harms of algorithmic wage discrimination. In addition to mobilizing against violations of minimum-wage, overtime, and vehicle reimbursement laws, workers in California—drawing on the knowledge and experience of their coworkers in the United Kingdom—have developed a sophisticated understanding of the laws governing data at work.
<cite>
	<span>
		<span>32</span>
		See infra Part III.	</span>
	
</cite>
 In the United Kingdom, a self-organized group of drivers, the App Drivers &amp; Couriers Union, has not only successfully sued Uber to establish their worker status
<cite>
	<span>
		<span>33</span>
		Kate Duffy &amp; Theo Golden, Uber Just Lost a Major Legal Battle Over Whether Its UK Drivers Count as Workers and Are Entitled to Minimum Wage, Bus. Insider (Feb. 19, 2021), https://www.businessinsider.com/uber-driver-lost-uk-legal-battle-court-worker-rights<br>
-employment-2021-2 [https://perma.cc/CT27-K2ZP].	</span>
	
</cite>
 but also used the General Data Protection Regulation (GDPR) to lay claim to a set of positive rights concerning the data and algorithms that determine their pay.
<cite>
	<span>
		<span>34</span>
		Jeffrey Brown, In New European Lawsuit, Uber Drivers Claim Company’s Algorithm Fired Them, Geo. L. Tech. Rev. Legal Impressions (Nov. 2020), https://georgetownlawtechreview.org/in-new-european-lawsuit-uber-drivers-claim-companys-algorithm-fired-them/GLTR-11-2020/ [https://perma.cc/887P-RET4] (“The GDPR . . . imposes obligations on companies which collect personal information if that data is related to EU consumers, regardless of the consumer’s physical location in the world. Under Article 22, individuals have ‘the right not to be subject to a decision based solely on automated processing.’” (quoting Council Regulation 2016/679, art. 22, 2016 O.J. (L 119) 1 (EU))).	</span>
	
</cite>
 As a GDPR-like law went into effect in California in 2023, drivers there are positioned to do the same.
<cite>
	<span>
		<span>35</span>
		Cal. Civ. Code § 1798.100 (2023) (imposing limits on businesses’ collection of consumer personal information and requiring notice of the purposes behind data collection).	</span>
	
</cite>
 Other workers in both the United States and Europe have responded by creating “data cooperatives” to fashion some transparency around the data extracted from their labor, to attempt to understand their wages, and to assert ownership over the data they collect at work.
<cite>
	<span>
		<span>36</span>
		See infra section III.B.	</span>
	
</cite>
 In addition to examining both approaches to addressing algorithmic wage discrim-ination, this Article argues that the constantly changing nature of machine learning technologies and the asymmetrical power dynamics of the digitalized workplace minimize the impact of these attempts at trans-parency and may not mitigate the objective or subjective harms of algorithmic wage discrimination. Considering the potential for this form of discrimination to spread into other sectors of work, this Article proposes instead an approach that addresses the harms directly: a narrowly structured, nonwaivable peremptory ban on the practice.</p>
<p>While this Article is focused on algorithmic wage discrimination as a labor management practice in “on-demand” or “gig work” sectors, where workers are commonly treated as “independent contractors” without protections, its significance is not limited to that domain. So long as this practice does not run afoul of minimum-wage or antidiscrimination laws, nothing in the laws of work makes this form of digitalized variable pay illegal.
<cite>
	<span>
		<span>37</span>
		See supra note 25. Antitrust laws, however, are a more promising way to address these practices when and if workers are classified as independent contractors. Part III discusses a California lawsuit filed in 2022 by Rideshare Drivers United workers against Uber alleging that the company’s payment structures amount to price fixing and that it is violating state antifraud laws.	</span>
	
</cite>
 As Professor Zephyr Teachout argues, “Uber drivers’ experiences should be understood not as a unique feature of contract work, but as a preview of a new form of wage setting for large employers . . . .”
<cite>
	<span>
		<span>38</span>
		See Teachout, Algorithmic Personalized Wages, supra note 15, at 437.	</span>
	
</cite>
 The core motivations of labor platform firms to adopt algorithmic wage discrimination—labor control and wage uncertainty—apply to many other forms of work. Indeed, extant evidence suggests that algorithmic wage discrimination has already seeped into the healthcare and engineering sectors, impacting how porters, nurses, and nurse practitioners are paid.
<cite>
	<span>
		<span>39</span>
		For example, a company that brands itself “Uber for Hospitals” has developed AI staffing software for hospitals. This software uses “smart technology” to allocate work tasks and to judge the performance of porters, nurses, and nurse practitioners. See Nicky Godding, Oxford Tech Raises £9 Million for ‘Uber for Hospitals’ AI Platform, Bus. Mag. (May 21, 2020), https://thebusinessmagazine.co.uk/technology-innovation/oxford-tech-raises-9-million-for-uber-for-hospitals-ai-platform/ [https://perma.cc/8593-M9U7] (“Hospitals can use [this technology] to assign tasks to healthcare teams based on their location. . . . This helps to ensure . . . full visibility of vulnerable patient movement between departments, and connects porters directly with staff . . . .”). The technology company’s “performance analysis” may then be used to determine the pay for these healthcare workers. Id.<br>
IBM Japan is also using digital surveillance systems to help set wages for their workers. In 2019, the company introduced human relations software created by Watson to use as a “compensation advisor.” The Japan Metal, Manufacturing, Information and Telecommunication Workers’ Union ( JMITU), which represents IBM Japan workers, requested disclosure of the data the Watson AI acquired and used, an explanation for how it was evaluating workers, and how these evaluations were involved in the wage-setting process. IBM Japan refused to disclose the information. JMITU subsequently lodged a complaint with the Tokyo Labor Relations Commission. The union argues that the software is being used to unfairly target union members. According to one report, “[i]n awarding summer bonuses in June 2019, the individual performance rate assessed by the company was only 63.6% on average for union members, compared to an average of 100% for all [other] employees. In addition, an exceptional 0% assessment was made for many union members.” Hozumi Masashi (ほづみ まさし), AIによる賃金査定にどう向き合うか: 日本IBM事件(不当労働行為救済申立) の報告 [How to Face AI-Based Wage Assessments: Report on the IBM Japan Case (Unfair Labor Practice Relief Petition)], 338 季刊 労·働者の権利[Worker Rights Quarterly], no. 10, 2020, at 101, 102.	</span>
	
</cite>
 If left unaddressed, the practice will continue to be normalized in other employment sectors, including retail, restaurant, and computer science, producing new cultural norms around compensation for low-wage work.
<cite>
	<span>
		<span>40</span>
		See, e.g., Min Kyung Lee, Daniel Kusbit, Evan Metsky &amp; Laura Dabbish, Working With Machines: The Impact of Algorithmic and Data-Driven Management on Workers, in CHI 15: Proceedings of the 33rd Annual CHI Conference on Human Factors in Computing Systems 1603, 1603–04 (2015) (discussing how algorithms used across industries can produce new norms of allocation of, evaluation of, and compensation for work). Companies across the world use wage algorithms in both contracting and permanent employment settings to incentivize certain behaviors. Technology capitalists have foreshadowed its growth. See, e.g., Shawn Carolan, Opinion, What Proposition 22 Now Makes Possible, The Info. (Nov. 10, 2020), https://www.theinformation.com/articles/what-proposition-22-now-makes-possible (on file with the <em>Columbia Law Review</em>) (predicting increased venture capitalist investment in “all sorts of industries” after the passage of Proposition 22). As Tarleton Gillespie has warned regarding the power of algorithms, “[t]here is a case to be made that the working logics of these algorithms not only shape user practices, but also lead users to internalize their norms and priorities.” Tarleton Gillespie, The Relevance of Algorithms, in Media Technologies: Essays on Communication, Materiality, and Society 167, 187 (Tarleton Gillespie, Pablo J. Boczkowski &amp; Kirsten A. Foot eds., 2014).	</span>
	
</cite>
 The on-demand sector thus serves as an important and portentous site of forthcoming conflict over longstanding moral and political ideas about work and wages.</p>

		</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[A Uruguayan company teaches people how to turn regular cars into EVs (159 pts)]]></title>
            <link>https://restofworld.org/2024/electric-vehicle-conversions-uruguay/</link>
            <guid>41512774</guid>
            <pubDate>Wed, 11 Sep 2024 16:15:16 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://restofworld.org/2024/electric-vehicle-conversions-uruguay/">https://restofworld.org/2024/electric-vehicle-conversions-uruguay/</a>, See on <a href="https://news.ycombinator.com/item?id=41512774">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
				<!-- Article Start -->
				
<p>In 2010, Uruguayan president-elect José Mujica made headlines for the bright blue mini-truck he rode to his inauguration ceremony.</p>



<p>The vehicle, which looked like any ordinary pickup truck, was used to convey a message: Uruguay was serious about its quest to become more environmentally friendly. The gas-powered four-wheeler had been transformed into an electric vehicle by Organización Autolibre, a local retrofitting company.</p>



<p>Viral press coverage of the ceremony put the company in the spotlight, sparking interest from EV enthusiasts inside and outside Uruguay who wanted to convert their gas-guzzling vehicles into economical EVs.&nbsp;</p>



<p>“This news coverage in many media outlets across Latin America gave a lot of visibility to this technology, and to this day we tour the region every year across Peru, Mexico, Argentina,” Gabriel González Barrios, founder and CEO of Organización Autolibre, told <em>Rest of World</em>. “The same distributors of Autolibre systems permanently invite us to train the necessary technicians to generate the local ecosystem for the local development of this industry.”</p>


	<figure>
		<div>
		<p><img src="https://149346090.v2.pressablecdn.com/wp-content/uploads/2024/08/Electric-Cars-Bog-25-40x27.jpg" data-src="https://149346090.v2.pressablecdn.com/wp-content/uploads/2024/08/Electric-Cars-Bog-25-768x432.jpg" data-srcset="https://149346090.v2.pressablecdn.com/wp-content/uploads/2024/08/Electric-Cars-Bog-25-400x267.jpg 400w, https://149346090.v2.pressablecdn.com/wp-content/uploads/2024/08/Electric-Cars-Bog-25-600x400.jpg 600w, https://149346090.v2.pressablecdn.com/wp-content/uploads/2024/08/Electric-Cars-Bog-25-1000x667.jpg 1000w, https://149346090.v2.pressablecdn.com/wp-content/uploads/2024/08/Electric-Cars-Bog-25-1600x1067.jpg 1600w, https://149346090.v2.pressablecdn.com/wp-content/uploads/2024/08/Electric-Cars-Bog-25-2800x1867.jpg 2800w, " sizes="(max-width: 640px) 100vw, calc(100vw - 40px)" alt="A photo of a mechanical car repair garage. There are two yello vehicles with their parts disassemebled.">
		
		</p>
		</div>
				<figcaption itemprop="caption description">
		
		
		</figcaption>
	</figure>


<p>Over the years, González Barrios and his team at Organización Autolibre have helped convert thousands of traditional vehicles into e-cars across 14 Latin American countries. The company trains individuals and mechanics through online courses, and supervises conversions for corporate fleets. So far, at least 40 companies have used Organización Autolibre’s services, González Barrios said. While some countries have flagged concerns about the safety of retrofitting vehicles, González Barrios said his company is leading efforts to make it a safer and standardized practice across Latin America.</p>



<p>“We want to show it’s an industrialized process,” Andrés García, the owner of a retrofitting shop in Bogotá, Colombia, which works with Autolibre, told <em>Rest of World</em>. “This is not for hobbyists or people who are inexperienced.”</p>



<p>González Barrios had the idea for the company in 2006 after watching the Al Gore-produced climate change documentary <em>An Inconvenient Truth</em>. A distributor of chemical products for gas-fueled vehicles at the time, he was inspired to address environmental concerns from his corner of the world.</p>



<p>“We decided to change the combustion engines of our own vehicles into zero-emission electric ones,” said González Barrios<em>. </em>The experiment was successful and affordable, and led him to found Organización Autolibre.</p>


	<figure>
		<div>
			<ul>
				<li>	<figure>
		<div>
		<p><img src="https://149346090.v2.pressablecdn.com/wp-content/uploads/2024/08/Electric-Cars-Bog-32-40x27.jpg" data-src="https://149346090.v2.pressablecdn.com/wp-content/uploads/2024/08/Electric-Cars-Bog-32-768x432.jpg" data-srcset="https://149346090.v2.pressablecdn.com/wp-content/uploads/2024/08/Electric-Cars-Bog-32-400x267.jpg 400w, https://149346090.v2.pressablecdn.com/wp-content/uploads/2024/08/Electric-Cars-Bog-32-600x400.jpg 600w, https://149346090.v2.pressablecdn.com/wp-content/uploads/2024/08/Electric-Cars-Bog-32-1000x667.jpg 1000w, https://149346090.v2.pressablecdn.com/wp-content/uploads/2024/08/Electric-Cars-Bog-32-1600x1067.jpg 1600w, https://149346090.v2.pressablecdn.com/wp-content/uploads/2024/08/Electric-Cars-Bog-32-2800x1867.jpg 2800w, " sizes="(max-width: 640px) calc(100vw + 40px), 50vw" alt="A photograph of a man in a khaki green polo shirt and sunglasses with his arms folded leans against a blue Fiat car parked in the middle of a lane.">
		
		</p>
		</div>
				<figcaption itemprop="caption description">
		
		
		</figcaption>
	</figure></li><li>	<figure>
		<div>
		<p><img src="https://149346090.v2.pressablecdn.com/wp-content/uploads/2024/08/Electric-Cars-Bog-45-40x27.jpg" data-src="https://149346090.v2.pressablecdn.com/wp-content/uploads/2024/08/Electric-Cars-Bog-45-768x432.jpg" data-srcset="https://149346090.v2.pressablecdn.com/wp-content/uploads/2024/08/Electric-Cars-Bog-45-400x267.jpg 400w, https://149346090.v2.pressablecdn.com/wp-content/uploads/2024/08/Electric-Cars-Bog-45-600x400.jpg 600w, https://149346090.v2.pressablecdn.com/wp-content/uploads/2024/08/Electric-Cars-Bog-45-1000x667.jpg 1000w, https://149346090.v2.pressablecdn.com/wp-content/uploads/2024/08/Electric-Cars-Bog-45-1600x1067.jpg 1600w, https://149346090.v2.pressablecdn.com/wp-content/uploads/2024/08/Electric-Cars-Bog-45-2800x1867.jpg 2800w, " sizes="(max-width: 640px) calc(100vw + 40px), 50vw" alt="A photograph of a person wearing a polo khaki tshirt holding a printed photo showing a a cream car parked on the street.">
		
		</p>
		</div>
				<figcaption itemprop="caption description">
		
		
		</figcaption>
	</figure></li>
			</ul>
		</div>
		<figcaption>Andres García’s Fiat racing car required a special permit for classic gas-engine cars, since they produce high CO2 emissions. He now drives the retrofitted car with no restrictions.</figcaption>
	</figure>


<p>González Barrios initially used some American EV kits to retrofit vehicles, but when those became too costly, Autolibre partnered with Zhuhai Enpower Electric, a Chinese electric power system company.</p>



<p>Over the last few years, as the popularity of EVs has grown, so has interest in retrofitting regular vehicles, Bruno González, head of sales at Autolibre, told<em> Rest of World. </em>In 2011, the company retrofitted a fleet of delivery vans for Bimbo, the largest bread-making company in the world. Bimbo did not respond to questions from <em>Rest of World.</em></p>



<p>In its <a href="https://mobilityportal.lat/wp-content/uploads/2021/09/Informe-retrofit-ALAMOS.pdf">2020 report</a> about the practice, the Latin American Association of Sustainable Mobility revealed that at least 145 retrofitted vehicles had been officially registered.</p>



<p>The Latin American Retrofit Association, co-founded by González Barrios, now has more than 30 members across the region. All are either distributors of EV retrofit kits or have workshops specializing in the process.</p>


	<figure>
		<div>
			<ul>
				<li>	<figure>
		<div>
		<p><img src="https://149346090.v2.pressablecdn.com/wp-content/uploads/2024/08/Electric-Cars-Bog-23-40x27.jpg" data-src="https://149346090.v2.pressablecdn.com/wp-content/uploads/2024/08/Electric-Cars-Bog-23-768x432.jpg" data-srcset="https://149346090.v2.pressablecdn.com/wp-content/uploads/2024/08/Electric-Cars-Bog-23-400x267.jpg 400w, https://149346090.v2.pressablecdn.com/wp-content/uploads/2024/08/Electric-Cars-Bog-23-600x400.jpg 600w, https://149346090.v2.pressablecdn.com/wp-content/uploads/2024/08/Electric-Cars-Bog-23-1000x667.jpg 1000w, https://149346090.v2.pressablecdn.com/wp-content/uploads/2024/08/Electric-Cars-Bog-23-1600x1067.jpg 1600w, https://149346090.v2.pressablecdn.com/wp-content/uploads/2024/08/Electric-Cars-Bog-23-2800x1867.jpg 2800w, " sizes="(max-width: 640px) calc(100vw + 40px), 50vw" alt="A photo of a hand pointing towards a technical drawing of a car">
		
		</p>
		</div>
				<figcaption itemprop="caption description">
		
		
		</figcaption>
	</figure></li><li>	<figure>
		<div>
		<p><img src="https://149346090.v2.pressablecdn.com/wp-content/uploads/2024/08/Electric-Cars-Bog-1-40x27.jpg" data-src="https://149346090.v2.pressablecdn.com/wp-content/uploads/2024/08/Electric-Cars-Bog-1-768x432.jpg" data-srcset="https://149346090.v2.pressablecdn.com/wp-content/uploads/2024/08/Electric-Cars-Bog-1-400x267.jpg 400w, https://149346090.v2.pressablecdn.com/wp-content/uploads/2024/08/Electric-Cars-Bog-1-600x400.jpg 600w, https://149346090.v2.pressablecdn.com/wp-content/uploads/2024/08/Electric-Cars-Bog-1-1000x667.jpg 1000w, https://149346090.v2.pressablecdn.com/wp-content/uploads/2024/08/Electric-Cars-Bog-1-1600x1067.jpg 1600w, https://149346090.v2.pressablecdn.com/wp-content/uploads/2024/08/Electric-Cars-Bog-1-2800x1867.jpg 2800w, " sizes="(max-width: 640px) calc(100vw + 40px), 50vw" alt="A photo of an electric engine placed on a workshop table in front of mechanical tools.">
		
		</p>
		</div>
				<figcaption itemprop="caption description">
		
		
		</figcaption>
	</figure></li><li>	<figure>
		<div>
		<p><img src="https://149346090.v2.pressablecdn.com/wp-content/uploads/2024/08/Electric-Cars-Bog-19-40x27.jpg" data-src="https://149346090.v2.pressablecdn.com/wp-content/uploads/2024/08/Electric-Cars-Bog-19-768x432.jpg" data-srcset="https://149346090.v2.pressablecdn.com/wp-content/uploads/2024/08/Electric-Cars-Bog-19-400x267.jpg 400w, https://149346090.v2.pressablecdn.com/wp-content/uploads/2024/08/Electric-Cars-Bog-19-600x400.jpg 600w, https://149346090.v2.pressablecdn.com/wp-content/uploads/2024/08/Electric-Cars-Bog-19-1000x667.jpg 1000w, https://149346090.v2.pressablecdn.com/wp-content/uploads/2024/08/Electric-Cars-Bog-19-1600x1067.jpg 1600w, https://149346090.v2.pressablecdn.com/wp-content/uploads/2024/08/Electric-Cars-Bog-19-2800x1867.jpg 2800w, " sizes="(max-width: 640px) calc(100vw + 40px), 50vw" alt="A photo of a close up of an electric vehicle charging port with a hand holding a mechanical tool.">
		
		</p>
		</div>
				<figcaption itemprop="caption description">
		
		
		</figcaption>
	</figure></li>
			</ul>
		</div>
		
	</figure>


<p>Retrofitting electric vehicles has been practiced worldwide for over 30 years, with countries like <a href="https://www.apev.jp/en/guide/pdf/Draft-guidelines_201104_E">Japan</a> and <a href="https://www.infrastructure.gov.au/sites/default/files/migrated/vehicles/vehicle_regulation/bulletin/files/NCOP14_Guidelines_Electric_Drive_01Jan2011.pdf">Australia</a> establishing national guidelines for the process. A report from the Latin American Association of Sustainable Mobility lists 21 companies that currently sell EV retrofit kits for different vehicles across the world.</p>



<p>The biggest incentive to retrofit a vehicle is its affordability, said González Barrios. Most new EVs available in Latin America remain out of reach for regular car users. One of the most popular models, the electric Renault Kwid, costs around $18,100. Converting an existing gas or diesel-engine car into an electric vehicle using Autolibre’s process starts at $6,000.</p>



<p>Since the practice is largely a DIY process, there are no official statistics on the retrofitting industry in Latin America. Many retrofitting jobs are done “by tinkerers who seek to extend the life of their petrol cars since they can’t afford a new electric one,” Adolfo Rojas, president of the Association of Entrepreneurs to Promote Electric Vehicles in Peru, told <em>Rest of World.</em></p>



<p>The retrofitting process requires skilled EV technicians to remove the engine, gas tank, exhaust, and other components within a regular car, and fit the electric motor, batteries, on-board charger, and computer into the empty space. Weight has to be carefully distributed so the car doesn’t tilt to one side. The original electrical components — such as airbags and sensors — must function properly, and the battery shouldn’t overheat. Autolibre Academy, the company’s educational branch, offers <a href="https://academia.autolibreelectrico.com/">online courses</a> on these basic skills to any EV enthusiast interested in retrofitting, González said<em>.</em></p>



<p>But Rojas said there are risks associated with the retrofitting process.</p>



<p>Retrofitting kits, many of which are available on online marketplaces like <a href="https://www.alibaba.com/product-detail/Factory-Price-Car-Retrofiting-Parts-Face_1600698207503.html?spm=a2700.7724857.0.0.7d67iKnTiKnTz7">Alibaba</a> or <a href="https://listado.mercadolibre.com.mx/kit-conversion-auto-electrico#!messageGeolocation">MercadoLibre</a>, often don’t guarantee a “minimum level of safety and quality for the retrofit unit,” Rojas said.</p>



<p>Once they’ve been modified, retrofitted vehicles must get government permits that allow them to be on the road in specific countries, according to retrofitting experts.</p>


	<figure>
		<div>
		<p><img src="https://149346090.v2.pressablecdn.com/wp-content/uploads/2024/08/Electric-Cars-Bog-37-40x27.jpg" data-src="https://149346090.v2.pressablecdn.com/wp-content/uploads/2024/08/Electric-Cars-Bog-37-768x432.jpg" data-srcset="https://149346090.v2.pressablecdn.com/wp-content/uploads/2024/08/Electric-Cars-Bog-37-400x267.jpg 400w, https://149346090.v2.pressablecdn.com/wp-content/uploads/2024/08/Electric-Cars-Bog-37-600x400.jpg 600w, https://149346090.v2.pressablecdn.com/wp-content/uploads/2024/08/Electric-Cars-Bog-37-1000x667.jpg 1000w, https://149346090.v2.pressablecdn.com/wp-content/uploads/2024/08/Electric-Cars-Bog-37-1600x1067.jpg 1600w, https://149346090.v2.pressablecdn.com/wp-content/uploads/2024/08/Electric-Cars-Bog-37-2800x1867.jpg 2800w, " sizes="(max-width: 640px) 100vw, (max-width: 992px) calc(100vw - 40px), (max-width: 1140px) calc(100vw - 290px), calc(100vw - ((100vw - 640px)/2))" alt="A photo of an opened car boot with electric battery components inside.">
		
		</p>
		</div>
				<figcaption itemprop="caption description">
		
		
		</figcaption>
	</figure>


<p>In 2021, the Chilean transport ministry passed legislation <a href="https://www.subtrans.gob.cl/wp-content/uploads/2021/12/Reglamento-Transformaci%C3%B3n-de-Veh%C3%ADculos-a-El%C3%A9ctricos-10_12_21-a-consulta-p%C3%BAblica-1.pdf">banning</a> the retrofitting of all used passenger vehicles. “Retrofits were being done, but keeping the car’s safety level was being overlooked,” Rodrigo Salcedo, president of Chile’s Electric Vehicle Association, told<em> Rest of World.</em> A safety compliance regulation is being prepared by the transport and energy ministries.</p>



<p>In Colombia, where retrofitted vehicles face no legal impediment, some are arguing for tighter controls.</p>



<p>García, from the car shop in Bogotá, said he is working with fellow retrofitting experts and enthusiasts to lobby for specific regulation, including meeting with the Colombian transport department and <a href="https://www.sena.edu.co/es-co/Paginas/default.aspx">SENA,</a> the country’s professional and technical training service. He said his company sells retrofit kits exclusively to certified technicians.</p>



<p>Jairo Novoa, one of García’s customers, retrofitted a 1981 BMW. He told<em> Rest of World</em> the process made sense for an old car like his because spare or repair parts are expensive and hard to find.</p>



<p>Although most of Colombia’s more than 11,000 electric vehicles are brand-new, retrofitted ones “do not need to envy” them, said Novoa. Except maybe, “really expensive ones like Tesla.”</p>
				<!-- Article End -->
							</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Uber drivers in Kenya are ignoring the app and charging their own rates (108 pts)]]></title>
            <link>https://restofworld.org/2024/kenya-uber-rate-cards-fuel-prices/</link>
            <guid>41512328</guid>
            <pubDate>Wed, 11 Sep 2024 15:30:48 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://restofworld.org/2024/kenya-uber-rate-cards-fuel-prices/">https://restofworld.org/2024/kenya-uber-rate-cards-fuel-prices/</a>, See on <a href="https://news.ycombinator.com/item?id=41512328">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
				<!-- Article Start -->
				
<p>Charles has been an Uber driver in Nairobi for nearly five<strong> </strong>years. But the last couple of years have been hard, with stubbornly high gas prices eating into&nbsp;his earnings.</p>



<p>In May, however, Charles, who requested to be identified only by his first name as he feared retribution from Uber, found a solution. Kenya’s Organization of Online Drivers (OOD) — a registered union with about 15,000 members — had begun circulating its own suggested fare chart, with rates that were at least 50% higher than Uber’s official fare. It allowed ride-hailing drivers to demand better rates from customers.&nbsp;</p>



<p>Charles now charges passengers according to the union’s chart, prioritizing customers who choose alternative payment methods like cash, or mobile banking services like M-Pesa that the app allows in Kenya. He actively turns down customers who want to pay using the Uber app.</p>



<figure><blockquote><p>“We tried to talk to Uber about adjusting the prices, but it was in vain, so we decided to take matters into our own hands to provoke them.”</p></blockquote></figure>



<p>He strategically places the laminated fare chart behind the driver’s seat and on the dashboard so customers are aware of the rates. Haggling with customers is not new for drivers in Kenya, but the union rate card has somewhat formalized higher fares, Charles told <em>Rest of World</em>. “We had been negotiating with customers before but this rate card became a unifying factor,” he said.</p>



<p>Initially, customers resisted paying the higher fares, Charles said. But now that the OOD’s rate card has become a common sight across Nairobi, passengers are used to it.</p>



<p>Justin Nyaga, chairperson of OOD, believes this is a form of peaceful pushback against Uber. “We tried to talk to Uber about adjusting the prices, but it was in vain, so we decided to take matters into our own hands to provoke them into discussing our terms and conditions,” he told <em>Rest of World</em>.</p>



<p>In a statement to <em>Rest of World</em>, Imran Manji, Uber’s head of East and South Africa, said that requesting payments above the app’s estimated rates is against company’s guidelines. “We encourage all riders to report such instances. We are currently reviewing the incidents reported to us,” Manji said.</p>



<p>Uber drivers around the world have complained about low earnings from the app, according to a <a href="https://fair.work/wp-content/uploads/sites/17/2024/02/Fairwork-Annual-Report-2023.pdf">2023 report</a> by Fairwork, a project at the Oxford Internet Institute that researches platform work. It seems natural for Kenyan taxi drivers to organize themselves and take this step so they can make a decent living, Mark Graham, director of Fairwork, told <em>Rest of World.</em></p>



<p>“At the end of the day, these drivers need to cover the costs of their vehicles, fuel, and insurance,” Graham said. “They often have families to support and are frequently the primary breadwinners. If they find that Uber isn’t providing fair earnings, it’s only natural that they would organize collectively to ensure they can make a decent living.”</p>


	<figure>
		<div>
		<p><img src="https://149346090.v2.pressablecdn.com/wp-content/uploads/2024/08/Screenshot-StephanieWangari-20240827-40x57.jpg" data-src="https://149346090.v2.pressablecdn.com/wp-content/uploads/2024/08/Screenshot-StephanieWangari-20240827-600x1066.jpg" data-srcset="https://149346090.v2.pressablecdn.com/wp-content/uploads/2024/08/Screenshot-StephanieWangari-20240827-400x566.jpg 400w, https://149346090.v2.pressablecdn.com/wp-content/uploads/2024/08/Screenshot-StephanieWangari-20240827-600x848.jpg 600w, https://149346090.v2.pressablecdn.com/wp-content/uploads/2024/08/Screenshot-StephanieWangari-20240827-1000x1414.jpg 1000w, https://149346090.v2.pressablecdn.com/wp-content/uploads/2024/08/Screenshot-StephanieWangari-20240827-1600x2262.jpg 1600w, https://149346090.v2.pressablecdn.com/wp-content/uploads/2024/08/Screenshot-StephanieWangari-20240827.jpg 1652w, " sizes="(max-width: 640px) 100vw, 600px" alt="">
		
		</p>
		</div>
				<figcaption itemprop="caption description">
		
		<span itemprop="copyrightHolder">Stephanie Wangari</span>
		</figcaption>
	</figure>


<p>In September 2022, the Kenyan government <a href="https://www.aljazeera.com/news/2022/9/15/kenyas-new-president-scraps-petrol-subsidy">removed its subsidy</a> on fuel, which caused <a href="https://www.thecitizen.co.tz/tanzania/news/east-africa-news/more-pain-at-the-pump-for-kenyan-motorists-as-fuel-prices-hit-new-records-4369470">gas prices</a> in the country to reach a record high. Even after the subsidy was reintroduced a year later, fuel costs in Kenya have <a href="https://www.businessdailyafrica.com/bd/economy/fuel-prices-drop-by-sh1-in-epra-may-review-4622820">remained high</a>.</p>



<p>Uber’s fares have not kept up with this rise in costs, making it hard for drivers to make ends meet,<strong> </strong>Nyaga said. When Uber entered Kenya in January 2015, a liter of gas cost 94.06 shillings (73 cents). This month, it cost roughly twice as much.</p>



<p>Kenyan taxi drivers have held several protests in the past two years, seeking a reduction in commissions that Uber charges. In <a href="https://www.businessdailyafrica.com/bd/corporate/companies/uber-cuts-commissions-to-18pc-4003712">October 2022</a>, the company reduced its commission in Kenya from 25% to 18% following protests. This July, taxi drivers in Nairobi held a five-day protest against low earnings, and requested Uber and Bolt to increase fares.</p>



<p>“Uber regularly makes pricing updates to ensure that drivers continue to have the opportunity to maximize their earnings while driving on the Uber app and at the same time, remaining at an affordable price point for riders,” an Uber spokesperson told<em> Rest of World</em> in a statement. Uber last increased fares on August 19, the spokesperson said. “This was done after a careful review of market conditions as well as after receiving feedback from drivers during regular roundtable discussions.”</p>



<p>The OOD rate card is a boon to drivers, but passengers have been increasingly frustrated.&nbsp;</p>



<p>Duncan Ndung’u, a businessman in Nairobi who uses Uber frequently to take his children to school, told <em>Rest of World</em> he spends several minutes before each ride in heated negotiations with drivers</p>



<p>“Once I make a request, the drivers ask if I’m willing to pay more than the app suggests, and when I decline, I have to request two or three times before finding someone willing to shuttle me. It is time-wasting,” Ndung’u said. “Nothing frustrates me more than requesting a ride, only to spend the entire journey negotiating with the driver about adding something on top of what the app suggests.”</p>



<p>He said his Uber rating has fallen in recent days and suspects it is probably because of his refusal to give in to the drivers’ demands.</p>



<p>In the coastal city of Mombasa, several taxi drivers have moved to apps like Little Cab and Yego, which charge passengers higher fares and have lower commissions for drivers, Eric Mbaabu, organizing secretary for the All Coast Online Digital Cabs Accord, told <em>Rest of World</em>. The shift happened after various unions were unsuccessful in reaching out to Uber over pricing issues, he said. In June 2023, 600 drivers in Mombasa <a href="https://cloud.kbc.co.ke/coast-taxi-drivers-sign-agreement-with-taxi-app-yego/">reportedly signed an agreement</a> with Yego, which promised them lower commissions.</p>
				<!-- Article End -->
							</div></div>]]></description>
        </item>
    </channel>
</rss>