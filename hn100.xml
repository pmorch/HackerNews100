<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0">
    <channel>
        <title>HN100 - Readable Contents</title>
        <link>https://hn.algolia.com/api/v1/search_by_date?tags=%28story,poll%29&amp;numericFilters=points%3E100</link>
        <description>Uses Readability to add bodies to the RSS feed</description>
        <lastBuildDate>Sun, 07 Jul 2024 03:30:01 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>en</language>
        <item>
            <title><![CDATA[How to think in writing (127 pts)]]></title>
            <link>https://www.henrikkarlsson.xyz/p/writing-to-think</link>
            <guid>40892298</guid>
            <pubDate>Sat, 06 Jul 2024 18:44:46 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.henrikkarlsson.xyz/p/writing-to-think">https://www.henrikkarlsson.xyz/p/writing-to-think</a>, See on <a href="https://news.ycombinator.com/item?id=40892298">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><article><div dir="auto"><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd8bec767-3242-4428-a281-0cdc3182ff75_750x587.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd8bec767-3242-4428-a281-0cdc3182ff75_750x587.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd8bec767-3242-4428-a281-0cdc3182ff75_750x587.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd8bec767-3242-4428-a281-0cdc3182ff75_750x587.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd8bec767-3242-4428-a281-0cdc3182ff75_750x587.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd8bec767-3242-4428-a281-0cdc3182ff75_750x587.png" width="750" height="587" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/d8bec767-3242-4428-a281-0cdc3182ff75_750x587.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:587,&quot;width&quot;:750,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:false,&quot;topImage&quot;:true,&quot;internalRedirect&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd8bec767-3242-4428-a281-0cdc3182ff75_750x587.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd8bec767-3242-4428-a281-0cdc3182ff75_750x587.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd8bec767-3242-4428-a281-0cdc3182ff75_750x587.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd8bec767-3242-4428-a281-0cdc3182ff75_750x587.png 1456w" sizes="100vw" fetchpriority="high"></picture></div></a></figure></div><blockquote><p><em>The reason I've spent so long establishing this rather obvious point [that writing helps you refine your thinking] is that it leads to another that many people will find shocking. If writing down your ideas always makes them more precise and more complete, then no one who hasn't written about a topic has fully formed ideas about it. And someone who never writes has no fully formed ideas about anything nontrivial.</em></p><p><em>It feels to them as if they do, especially if they're not in the habit of critically examining their own thinking. Ideas can feel complete. It's only when you try to put them into words that you discover they're not. So if you never subject your ideas to that test, you'll not only never have fully formed ideas, but also never realize it.</em></p><p>—Paul Graham</p></blockquote><p>When I sit down to write, the meadow is still sunk in darkness, and above it, satellites pass by, one after the other. My thoughts are flighty and shapeless; they morph as I approach them. But when I type, it is as if I pin my thoughts to the table. I can examine them.</p><p><span>But it is hard to do it right. Not all writing helps me </span><em>think</em><span>. Most kinds of writing are rather weak, or even counterproductive, in this regard. You have to approach it in the right way.</span></p><p>Until last fall, I had not seen anyone properly articulate the mental moves that make writing a powerful tool for thought. Writing advice is usually focused on more superficial parts of the craft. Whatever I knew about thinking on the page, I had picked up through trial and error and conversations with other writers.</p><p><span>But then I read Imre Lakatos’s </span><em><a href="https://dl1.cuni.cz/pluginfile.php/730446/mod_resource/content/2/Imre%20Lakatos%3B%20Proofs%20and%20Refutations.pdf" rel="">Proofs and Refutations</a></em><span>. It is not, at first glance, a book about writing. It is a book of mathematical philosophy. By a Hungarian Stalinist, no less. But it is, if you read it sideways, a profound exploration of the act of writing. This shouldn’t be a surprise. Mathematics is, after all, a subset of writing—it is a way of crafting a language that helps you express and improve thoughts. The main difference, compared to prose writers and poets, is that mathematicians are more rigorous, precise. Because of this precision, reading Lakatos gave me a clearer and more precise understanding of what I do, or strive to do, as I sit down each morning and wrestle with my thoughts.&nbsp;</span></p><p>What follows is a series of meditations about thinking through writing provoked by, but not faithful to, Lakatos’s book. I’ve divided it into two parts. The first part covers the basic mental models that are useful to most people (if you write a diary, for example, and want to get clarity about things in your life). The next part goes into more complex patterns of thinking which I suspect is mostly useful if you do research or engage in some other kind of deep creative work.</p><p>A warning. If you aim to write and publish stuff, this essay might tie you up in knots. It is about thinking, not about crafting beauty or finishing things in a finite time.</p><blockquote><p><em><span>There is a crack, a crack in everything</span><br><span>That’s how the light gets in.</span><br><span>—</span></em><span>Leonard Cohen, “Anthem”</span></p></blockquote><p><span>In </span><a href="https://www.dwarkeshpatel.com/p/patrick-collison" rel="">a recent interview with Dwarkesh</a><span>, Patrick Collison explained the value of writing using a metaphor I enjoyed:</span></p><blockquote><p><span>Bruno Latour spoke about how he thinks the printing revolution, like Gutenberg’s, partially caused the scientific revolution by </span><em>making knowledge more rigid.</em><span> Before, if some observation didn’t match some claim, you could always shrug and be like: “Well, the person who transcribed that thing made a mistake.” So </span><em>by making things more rigid, it’s easier to break them. </em><span>[Emphasis mine.]&nbsp;</span></p></blockquote><p><span>Good thinking is about pushing past your current understanding and reaching the thought behind the thought. This often requires breaking old ideas, which is much easier to do when the ideas are as rigid as they get on the page. In a fluid medium like thought or conversation, you can always go, “Well, I didn’t mean it like </span><em>that</em><span>” or rely on the fact that your short-term memory is too limited for you to notice the contradiction between what you are saying now and what you said 12 minutes ago.</span></p><p>When I write, I get to observe the transition from this fluid mode of thinking to the rigid. As I type, I’m often in a fluid mode—writing at the speed of thought. I feel confident about what I’m saying. But as soon as I stop, the thoughts solidify, rigid on the page, and, as I read what I’ve written, I see cracks spreading through my ideas. What seemed right in my head fell to pieces on the page.</p><p><span>Seeing your ideas crumble can be a frustrating experience, but </span><em>it is the point</em><span> if you are writing to think. You want it to break. It is in the cracks the light shines in.</span></p><p>When I write, I push myself to make definite positive claims. Ambiguity allows thought to remain fluid on the page, floating into a different meaning when put under pressure. This makes it harder to push your thinking deeper. By making clear and sharp claims, I reveal my understanding so that I—or the person I’m writing to—can see the state of my knowledge and direct their feedback to the point where it will help my thinking improve.</p><p>This is valuable to do even in areas where you know way too little to “warrant” an opinion. I met a Japanese linguist in the harbor yesterday and talked about the relationship between the Chinese and the Japanese writing systems. This is a topic I had thought about for about twenty seconds before this. “So,” I said after two minutes, “this is a stupid question, but is the relationship between China and Japan like that between Ancient Greece and the Roman Empire?” This is, as it turns out, not a good analogy. But by spelling out my naive understanding, I gave the linguist a good area to work on when he laid out a richer model of the flow of cultural influence in East Asia.</p><p><span>In the terminology of mathematics, what I did here (and in my writing) was to “make a conjecture,” a qualified guess based on limited information. A hypothesis. The mathematician Alexander Grothendieck, whom Johanna and I </span><a href="https://www.henrikkarlsson.xyz/p/good-ideas" rel="">have written about elsewhere</a><span>, would always summarize his first impression of a new situation with a conjecture, proclaiming with irrepressible enthusiasm, “It must be true!” Ten seconds later, someone would come up with a counterexample that proved him wrong. But being right wasn’t the point: getting a better understanding was. And he would immediately throw out a new conjecture. (Holden Karnofsky has a blog post about using this technique to </span><a href="https://www.cold-takes.com/learning-by-writing/" rel="">learn through writing</a><span>.)</span></p><p>Forcing the diffuse ideas and impressions in your head into a definite statement is an art form. You have to grab hold of what is floating and make it rigid and sharp. It can feel almost embarrassing–revealing your ignorance with as much vulnerability as possible.</p><p><span>And it is only the first step. Once you have made your thoughts definite, clear, concrete, sharp, and rigid, you also want to </span><em>unfold </em><span>them.</span></p><p><span>By unfolding I mean “interrogating the conclusion to come up with an explanation of why it </span><em>could </em><span>be true.” What premises and reasoning chains leads to this conclusion? The explanation isn’t meant to prove that your conclusion was right. It is just a way of unpacking it.</span></p><p>By unfolding a claim into an explanation, you spread it on a “wider front” (to borrow a metaphor from Lakatos), so that the criticism has more targets.</p><p><span>I used this tactic in the food store yesterday. Maud, our six-year-old, told me we had to get a pink miniature plastic teapot. I couldn’t come up with a compassionate counterargument, so I said, “Why do you think a plastic teapot is so great?” And she said, “Because it is </span><em>so</em><span> beautiful. And I need one in plastic so it doesn’t break. I would use it all the time.” This brought a smile to my face. See—trying to prove her point, she had given me three times as many claims to attack!</span></p><p>Since the goal is to find flaws in our guesses (so that we can change our minds, refine our mental models and our language, and be more right) unfolding a claim through an explanation is progress. Even if the explanation is wrong.</p><blockquote><p><span>You are interested only in proofs which ‘prove’ what they have set out to prove. I am interested in proofs even if they do not accomplish their intended task. Columbus did not reach India but he discovered something interesting.</span><br><span>—Lakatos</span></p></blockquote><p><span>Let me take another example. Before Maud was born, Johanna and I worked as teachers in Sweden. The first conclusion we drew from that experience was that we didn’t want to submit our kids to what we had observed. This way of formulating it (“Not </span><em>that</em><span>”) is a bit vague as it only defines where not to look for the solution. It is useful to also attempt a positive formulation. If I were to reconstruct the positive version of our conclusion back then, it was something like, “We need to find (or start) a school where our daughter can pursue her interests at her pace.”</span></p><p>There are several subtle problems with this conclusion. But the point is—these problems didn’t come into view until we had unfolded and probed our original position.&nbsp;</p><p>The way we unfolded and improved our conclusion back then was more haphazard than it would have been today. We just talked about it aimlessly, read randomly, and made small notes. This cost us time and caused confusion. These days, I would instead unfold a conclusion like this as a series of bullet points where I spell out the intuition behind my claim in a series of premises. In the case of Maud’s education, this would have looked something like this (note that this is not my current understanding but a reconstruction of what I thought eight years ago):</p><ul><li><p>People have an intrinsic motivation to learn and it is important to not undermine that, which schools do&nbsp;</p></li><li><p>It is better to go deep on a few topics that you are passionate about rather than have a superficial understanding of a broad range of subjects you care little about</p></li><li><p>But you need to attend a school so you get socialized</p></li><li><p>Hence, we need to find a school that allows self-directed learning</p></li></ul><p>Once I unfold my understanding in writing, I often see holes right away. I start correcting myself and discarding ideas already while typing. I cut ideas that are obviously flawed. I rewrite what feels ambiguous to make it sharper–more precise, concrete, unhedged, and true to my understanding.</p><p>The flaws I see immediately, however, are only the more superficial flaws. The deeper patterns take a longer time to emerge—because they are further from my established thoughts and so are harder to articulate.</p><p>Often, they occur first as subtle emotional cues. As I reread a passage, I notice a slight tension across my chest or my eyes fog over. For some reason, it doesn’t feel right. There is something wrong here.&nbsp;</p><p>These subtle feelings are easy to dismiss (“Eh, words are slippery, I mean something slightly different . . . there is no reason to obsess about this”). But in my experience, it is these subtler problems that tend to open a path beyond my current understanding. I learned this from my wife, Johanna, who will often sit with a draft for several hours, not writing or editing, but simply articulating why something feels off to her. Our best essays have come out of the things she surfaced during those sessions.</p><p>For this reason, I suspect that many of my friends who write and publish rapidly are shortchanging themselves. They generate texts filled with hidden doors and move on before they’ve opened them.&nbsp;</p><p>I tend to go through my list of premises and assumptions and ask follow-up questions to myself, to further unfold my conclusion. To continue the example from above, I would take one of the premises and unfold it like this:</p><ul><li><p>But you need a school so you get socialized</p><ul><li><p><em>Curious: why?</em></p></li><li><p>Kids will get depressed and struggle to navigate workplaces, and so on, if they haven’t been exposed to society</p><ul><li><p><em>Where can I read more about this? Are there any good studies?</em></p></li></ul></li><li><p>Being in something like a school is important because humans are social animals. We pick up most of our skills and norms and so on by being immersed in a peer group</p><ul><li><p><em>And what follows from this?</em></p></li><li><p><em>If we are shaped by our peer group, what would the ideal peer group look like?</em></p></li></ul></li></ul></li></ul><p><span>The emotional tone of these questions is, in my head, lovingly curious; I’m not trying to</span><em> </em><span>put myself down. I’m trying not to kill ideas. I want to help them evolve and spill forth more insight. Often this dialogue ends with me changing my mind about several premises and coming to a different conclusion, but the original idea remains the seed—no less valuable for having been proven wrong. It takes creativity and boldness to leap out and form a conclusion, and the part that criticizes must understand how dependent it is on the part that throws ideas at the wall. It is often easier to criticize than it is to synthesize a new position.</span></p><p>The sun is above the horizon now, the satellites hid behind a thin layer of orange and pink. A hare raises on his hind legs in the middle of the meadow looking around. I tap the glass and watch his ears turn my way.</p><p>Now that I have spelled out my position and fixed the obvious flaws, I start probing myself more seriously to see if I can get the argument to break down.</p><p><span>If one of the premises I have unfolded is a factual claim, I’ll spend a few minutes skimming research in the area to see how well my position holds up. “Oh, it turns out that most homeschooled kids do </span><em>not</em><span> have any problems with socialization!” I realized when doing this in relation to Maud’s education. (Though it didn’t take me a few minutes, it took me years in this case. Partly because we were unsystematic, partly because homeschooling is illegal and taboo in Sweden and this had worked itself into my body so that I felt revulsion each time I probed that assumption.) In this case, looking at studies and statistics helped remove several needless assumptions. We changed our conclusion (we left Sweden and now homeschool Maud and her sister).</span></p><p><span>But often the type of problem I like to think about is too personal and messy and qualitative to be resolved cleanly through a statistically significant study. What I do in these situations instead is to consider </span><em>counterexamples</em><span>.</span></p><p><span>I like to visualize concrete situations when I make an argument (in the notes for this essay, for example, I continually compare what I say against past writing projects). This makes it easier for me to think clearly. I am tied back into a lived reality, which is rigid, and do not float off into theory, where I have a solid track record of fooling myself. When I have a concrete situation in mind, I can ask myself, “What is a situation where the opposite happened? Why was that?” I can list the characteristics of the situation that inform my conclusion and then systematically look for cases that have other characteristics. In “</span><a href="https://www.henrikkarlsson.xyz/p/childhoods" rel="">Childhoods of exceptional people</a><span>,” for example, I wrote about parenting from the perspective of concrete biographies. The sample was unsystematic. But once I had extracted what I thought were the common patterns, I asked myself, “So whom does this </span><em>not</em><span> apply to?” Then I added the people that came to mind to the sample and ended up with a distribution that was good enough for my purposes.</span></p><p>Counterexamples are useful in two ways. Either you find a counterexample that a) proves one of the premises wrong but b) does not change your mind about the conclusion. Lakatos calls this a local (and non-global) counterexample. This means there is something wrong with your unfolding. Perhaps you need to change that part of the explanation? Or perhaps you can simply drop it, making the mental model simpler and more general? Local counterexamples help you improve your explanation and get a better understanding.&nbsp;</p><p><span>There is a scene in the last season of </span><em>Breaking Bad</em><span> that illustrates this. The main character, whatever his name was, is a teacher that starts a meth lab. This can be thought of as his conclusion (“I should get into the meth business”) and when asked to defend this decision he unfolds the claim by saying, “I need to support my family.” This is false. There are better ways for him to do that (he has an old friend who offers him money). That is a local counterexample. In the final season, he admits to himself: “I did it because it made me feel alive.” This doesn’t change his conclusion (he does not change his mind about the meth) but it gives him a deeper and more correct understanding of himself.</span></p><p><span>Other times, the counterexample you find undermines the whole idea—a </span><em>global counterexample</em><span>. You unfold your conclusion and discover that one of the premises does not hold up, and there is no way to patch it. The fracture spreads right up to the conclusion. Now—this is what we have been longing for—there is a big hole of confusion where before there was a mental model. It is time to replace it with something more subtle and deep that incorporates the critique.</span></p><p>How to do this, and do it in the most interesting way possible, is the topic of the next part (which I have no idea when I’ll finish).</p><p><em>If you liked this, you might enjoy this one too:</em></p></div></article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Why privacy is important, and having "nothing to hide" is irrelevant (2016) (172 pts)]]></title>
            <link>https://robindoherty.com/2016/01/06/nothing-to-hide.html</link>
            <guid>40892259</guid>
            <pubDate>Sat, 06 Jul 2024 18:38:43 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://robindoherty.com/2016/01/06/nothing-to-hide.html">https://robindoherty.com/2016/01/06/nothing-to-hide.html</a>, See on <a href="https://news.ycombinator.com/item?id=40892259">Hacker News</a></p>
<div id="readability-page-1" class="page"><article>
            <p>
              The governments of
              <a href="https://robindoherty.com/2015/10/07/your-digital-privacy-ends-this-time-next-week.html">Australia</a>,
              <a href="https://lawfareblog.com/german-bundestag-passes-new-data-retention-law">Germany</a>, the
              <a href="http://www.theguardian.com/world/2015/nov/05/mass-snooping-and-more-the-measures-in-theresa-mays-bill">UK</a>
              and the
              <a href="https://medium.com/@RonWyden/this-bill-won-t-protect-you-from-hackers-6aff1d250f67">US</a>
              are destroying your privacy. Some people don’t see the problem…
            </p>

            <h4 id="i-have-nothing-to-hide-so-why-should-i-care">
              “I have nothing to hide, so why should I care?”
            </h4>

            <p>
              It doesn’t matter if <em>you</em> have <em>“nothing to hide”</em>.
              Privacy is a right granted to individuals that underpins the
              freedoms of expression, association and assembly; all of which are
              essential for a free, democratic society.
            </p>

            <p>
              The statement from
              <a href="http://www.smh.com.au/digital-life/digital-life-news/metadata-retention-those-with-nothing-to-hide-have-nothing-to-fear-says-australian-federal-police-assistant-commissioner-tim-morris-20150222-13ljzi.html">some</a>
              <a href="https://www.youtube.com/watch?v=lWam4EWI48M">politicians</a>
              that “if you have nothing to hide then you have nothing to fear”
              purposefully misframes the whole debate.
            </p>

            <p>This affects all of us. We must care.</p>

            <blockquote>
              <p>
                Arguing that you don’t care about the right to privacy because
                you have nothing to hide is no different than saying you don’t
                care about free speech because you have nothing to say.
              </p>
            </blockquote>

            <p><em>– Edward Snowden</em></p>

            <h2 id="privacy-and-freedom">Privacy and freedom</h2>

            <p>Loss of privacy leads to loss of freedom.</p>

            <p>
              Your
              <a href="https://en.wikipedia.org/wiki/Freedom_of_expression">freedom of expression</a>
              is threatened by the surveillance of your internet usage – thought
              patterns and intentions can be extrapolated from your website
              visits (rightly or wrongly), and the knowledge that you are being
              surveilled can make you less likely to research a particular
              topic. You lose that perspective, and your thought can be pushed
              in one direction as a result. Similarly, when the things you write
              online, or communicate privately to others, are surveilled, and
              you <a href="#self-censorship">self-censor</a> as a result, the
              rest of us lose your perspective, and the development of further
              ideas is stifled.
            </p>

            <p>
              Your
              <a href="https://en.wikipedia.org/wiki/Freedom_of_association">freedom of association</a>
              is threatened by the surveillance of your communications online
              and by phone, and your
              <a href="https://en.wikipedia.org/wiki/Freedom_of_assembly">freedom of assembly</a>
              is threatened by the tracking of your location by your mobile
              phone. Can we afford to risk the benefits of free association, the
              social change brought by activists and campaigners, or the right
              to protest?
            </p>

            <p>
              These freedoms are being eroded, right now. The effects will
              worsen over time, as each failure to exercise our freedom builds
              upon the last, and as more people experience the
              <a href="#personal-chilling">chilling effects</a>.
            </p>

            <h3 id="aggregation"><a name="aggregation"></a>Aggregation</h3>

            <p>
              Bits of information that you might not feel the need to hide can
              be <em>aggregated</em> into a telling profile, which might include
              things that you actually do want to conceal.
            </p>

            <p>
              In the case of data retention in Australia, we have given away our
              rights to privacy, and now share a constant stream of:
            </p>

            <ul>
              <li><strong>where</strong> we go,</li>
              <li><strong>who</strong> we contact and when,</li>
              <li>and <strong>what we do</strong> on the internet.</li>
            </ul>

            <p>
              With just a small portion of this
              <a href="http://www.abc.net.au/news/2015-08-16/metadata-retention-privacy-phone-will-ockenden/6694152">data</a>, off-the-shelf software and their own spare time, ABC News
              readers found
              <a href="http://www.abc.net.au/news/2015-08-24/metadata-what-you-found-will-ockenden/6703626">Will Ockenden’s home, workplace and parents’ home</a>.
            </p>

            <p>
              The intrusion becomes all the more spectacular when you consider
              the data across a whole population, the
              <a href="https://www.rt.com/usa/snowden-leak-black-budget-176/">massive budgets</a>
              of the
              <a href="https://www.privacyinternational.org/node/51">Five Eyes</a>
              intelligence agencies, and the constant progress of artificial
              intelligence and big data analytics.
            </p>

            <p>
              Your interactions with the world around you can reveal your
              political and religious beliefs, your desires, sympathies and
              convictions, and things about yourself that you
              <a href="http://www.businessinsider.com.au/the-incredible-story-of-how-target-exposed-a-teen-girls-pregnancy-2012-2">aren’t even aware of</a>
              (and they might be wrong too).
            </p>

            <p>
              Given enough data and time, your behaviour might even be
              <a href="http://qz.com/527008/an-algorithm-can-predict-human-behavior-better-than-humans/">predicted</a>.
            </p>

            <h3 id="personal-chilling-effects">
              <a name="personal-chilling"></a>Personal chilling effects
            </h3>

            <p>
              When you understand the fullness of the picture that mass
              surveillance paints of you, you begin to change your behaviour –
              you avoid exercising certain freedoms.
            </p>

            <p>You might think twice about:</p>

            <ul>
              <li>
                <p>
                  <strong>contacting</strong> or meeting people (exercising your
                  freedom of association) who you think might become “persons of
                  interest” to the state, or that you think the algorithms might
                  determine as such in the future, since you know that your
                  association with them is retained for <em>at least</em> two
                  years and may be analysed,
                </p>
              </li>
              <li>
                <p>
                  <strong>congregating</strong> in the same location as a group
                  of those people (exercising your freedom of assembly). Would
                  you attend a protest march calling for action on climate
                  change, knowing that you would forever be linked to what the
                  Australian government calls a
                  <a href="http://www.theguardian.com/australia-news/2015/sep/10/green-lawfare-voters-feel-coalition-is-trying-to-silence-environment-groups">“vigilantist” movement of “economic saboteurs”</a>?
                </p>
              </li>
              <li>
                <p>
                  <strong>participating</strong> in any activity that might make
                  you look bad in the data – even if you know that you are
                  innocent. This could mean avoiding writing about a particular
                  topic online, or visiting a particular website, or buying a
                  particular book – exercising your freedom of expression.
                </p>
              </li>
            </ul>

            <h3 id="societal-chilling-effects">
              <a name="societal-chilling"></a>Societal chilling effects
            </h3>

            <p>
              The combined result of these second thoughts across the population
              is a chilling effect on many of the activities that are key to a
              well-functioning democracy – activism, journalism, and political
              dissent, among others.
            </p>

            <p>
              We all benefit from progress that occurs when activists,
              journalists and society as a whole are able to freely engage in
              political discourse and dissent. Many of the positive changes of
              the last century were only possible because of these freedoms. For
              example, the
              <a href="https://en.wikipedia.org/wiki/Australian_referendum,_1967_(Aboriginals)">1967 referendum</a>
              on including indigenous Australians in the census, and allowing
              the federal government to make laws specifically benefiting
              indigenous races, was only made possible by sustained activism
              throughout the 1950s and 60s.
            </p>

            <p>
              Unfortunately, we are already <a name="self-censorship"></a><strong>self-censoring</strong>.
              <a href="https://www.pen.org/sites/default/files/Chilling%20Effects_PEN%20American.pdf">A 2013 survey of US writers</a>
              found that after the revelations of the NSA’s mass surveillance
              regime, 1 in 6 had avoided writing on a topic they thought would
              subject them to surveillance, and a further 1 in 6 had seriously
              considered doing so.
            </p>

            <blockquote>
              <p>
                Ask yourself: at every point in history, who suffers the most
                from unjustified surveillance? It is not the privileged, but the
                vulnerable. Surveillance is not about safety, it’s about power.
                It’s about control.
              </p>
            </blockquote>

            <p><em>– Edward Snowden</em></p>

            <h3 id="misuse--misappropriation">
              <a name="misappropriation"></a>Misuse &amp; misappropriation
            </h3>

            <p>
              By creating databases and systems of easy access to such a great
              volume of personally revealing information, we increase the scope
              of mass surveillance, and therefore the scope for infringements
              upon our human rights.
            </p>

            <p>
              East Germany is the most extreme example of a surveillance state
              in history. The Stasi – its infamous security agency – employed
              90,000 spies and had a network of at least 174,000 informants. The
              Stasi kept meticulous files on hundreds of thousands of innocent
              citizens and used this information to psychologically harrass,
              blackmail and discredit people who became dissenters. But that was
              before the internet. Reflecting on the NSA’s current systems of
              mass surveillance, a former Stasi lieutenant colonel
              <a href="http://www.mcclatchydc.com/news/nation-world/national/article24750439.html">said</a>:
              <strong>“for us, this would have been a dream come true”</strong>.
            </p>

            <p>
              Even aside from the risk of systematic state misbehaviour, in
              Australia we know that the
              <a href="https://robindoherty.com/2015/10/07/your-digital-privacy-ends-this-time-next-week.html#snoopers">2500 snoopers</a>
              who have unrestricted access to your data are subject to
              <a href="http://www.watoday.com.au/wa-news/wa-policeman-charged-over-disclosing-ben-cousins-secrets-to-journalist-girlfriend-20150423-1mrjhd.html">“professional curiosity”</a>,
              <a href="http://www.couriermail.com.au/news/queensland/police-under-fire-for-probing-phone-records-of-their-own-in-8216disturbing8217-breach-of-officers8217-privacy/story-fnihsrf2-1226706966590">fallible morals</a>, and are only human, so will make mistakes and become victims of
              social engineering, blackmail or bribery.
            </p>

            <p>
              This is most dangerous for the most vulnerable people. For
              example, if you have an angry or violent ex-partner, you could be
              put in mortal danger by them getting their hands on this much
              detail about your life.
            </p>

            <h4 id="risk-taking">Risk taking</h4>

            <p>
              Our “digital lives” are an accurate reflection of our actual
              lives. Our phone records expose where we go and who we talk to,
              and our internet usage can expose almost everything about
              ourselves and what we care about.
            </p>

            <p>
              Even if we trust the motives of our current governments, and every
              person with authorised access to our data, we are taking an
              incredible risk. The systems of surveillance that we entrench now
              may be misappropriated and misused at any time by future
              governments, foreign intelligence agencies, double agents, and
              opportunistic hackers.
            </p>

            <p>The more data we have, the more devastating its potential.</p>

            <h3 id="gradual-erosion">
              <a name="gradual-erosion"></a>Gradual erosion
            </h3>

            <p>
              Each system of surveillance and intrusion that we introduce erodes
              our privacy and pushes us one step further away from a free
              society.
            </p>

            <p>
              While you may not have noticed the impact yet, your privacy has
              already been eroded. If we continue along our current path,
              building more powers into our systems of surveillance, what was
              once your private life will be whittled away to nothing, and the
              freedoms that we have taken for granted will cease to exist.
            </p>

            <p>
              As technology advances, we are presented with a choice – will it
              to continue to offer an overall benefit to society, or will we
              allow it to be used as a tool for total intrusion into our lives?
            </p>

            <blockquote>
              <p>
                Privacy is rarely lost in one fell swoop. It is usually eroded
                over time, little bits dissolving almost imperceptibly until we
                finally begin to notice how much is gone.
              </p>
            </blockquote>

            <p>
              <em>–
                <a href="https://web.archive.org/web/20151116013709/http://chronicle.com/article/Why-Privacy-Matters-Even-if/127461/">Why Privacy Matters Even if You Have ‘Nothing to Hide’</a>, Daniel J. Solove</em>
            </p>

            <h2 id="what-next">What next?</h2>

            <p>
              The governments of Australia, New Zealand, Canada, the US and
              others are poised to take a big step in the wrong direction with
              the Trans-Pacific Partnership (TPP). The EFF
              <a href="https://www.eff.org/deeplinks/2015/12/how-tpp-will-affect-you-and-your-digital-rights">explains</a>
              why the TPP is a huge threat to your privacy and other rights.
            </p>

            <ul>
              <li>
                <p>
                  <strong>Take action</strong> – if you are a technologist, join
                  Hack for Privacy and fight back against mass surveillance –
                  <a href="https://hackforprivacy.org/">hackforprivacy.org</a>.
                </p>
              </li>
              <li>
                <p>
                  <strong>Spread the privacy mindset</strong> – we must foster
                  understanding of this issue in order to protect ourselves from
                  harmful laws and fight against future invasions of privacy.
                  Please help spread the knowledge, discuss this article with a
                  friend, tweet it, share it, etc.
                </p>
              </li>
              <li>
                <p>
                  <strong>Protect yourself</strong> – protect your own data from
                  mass surveillance. This
                  <a href="http://www.theguardian.com/commentisfree/2013/sep/05/government-betrayed-internet-nsa-spying">increases the cost</a>
                  of mass surveillance and helps others too. Read
                  <a href="https://robindoherty.com/2015/10/07/your-digital-privacy-ends-this-time-next-week.html">my advice on protecting your data from retention in
                    Australia</a>, the EFF’s
                  <a href="https://ssd.eff.org/">Surveillance Self-Defense Guide</a>, and
                  <a href="http://www.tcij.org/node/1016">Information Security for Journalists</a>.
                </p>
              </li>
            </ul>

            <hr>

            <p>
              <em>Translations of this article are available in:
                <a href="http://www.seanhall.it/blog/2016/01/14/perche-la-privacy-e-importante-e-non-avere-nulla-da-nascondere-e-irrilevante">Italian</a>
                and
                <a href="https://robindoherty.com/de/2016/02/01/nichts-zu-verbergen.html">German</a>.</em>
            </p>

            <hr>

            
          </article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Kivy – a cross platform Python UI framework (141 pts)]]></title>
            <link>https://kivy.org</link>
            <guid>40891446</guid>
            <pubDate>Sat, 06 Jul 2024 16:27:48 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://kivy.org">https://kivy.org</a>, See on <a href="https://news.ycombinator.com/item?id=40891446">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
          <p><b>Kivy</b> is released under the <b>MIT License</b>, is
          <b>100%</b> free to use, and is professionally developed, backed and
          maintained.</p><p>
          <b>Companies</b> and <b>individuals</b> are using Kivy for their
          projects <b>every day</b>.
        </p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[First anode-free sodium solid-state battery (295 pts)]]></title>
            <link>https://pme.uchicago.edu/news/uchicago-prof-shirley-mengs-laboratory-energy-storage-and-conversion-creates-worlds-first</link>
            <guid>40891252</guid>
            <pubDate>Sat, 06 Jul 2024 16:01:25 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://pme.uchicago.edu/news/uchicago-prof-shirley-mengs-laboratory-energy-storage-and-conversion-creates-worlds-first">https://pme.uchicago.edu/news/uchicago-prof-shirley-mengs-laboratory-energy-storage-and-conversion-creates-worlds-first</a>, See on <a href="https://news.ycombinator.com/item?id=40891252">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="app">
  <header>
    <uc-skip-nav text="Skip to main content"></uc-skip-nav>
            


        
<uc-masthead :nav-data="{
  &quot;items&quot;:
    [{&quot;section&quot;:&quot;About&quot;,&quot;link&quot;:&quot;/about&quot;,&quot;target&quot;:&quot;_self&quot;,&quot;text&quot;:&quot;The Pritzker School of Molecular Engineering integrates science and engineering to address global challenges from the molecular level up.&quot;,&quot;groups&quot;:null,&quot;links&quot;:[{&quot;title&quot;:&quot;A Welcome from Dean Mason&quot;,&quot;url&quot;:&quot;/about/welcome-dean-mason&quot;,&quot;target&quot;:&quot;_self&quot;},{&quot;title&quot;:&quot;Mission and Vision&quot;,&quot;url&quot;:&quot;/about/mission-and-vision&quot;,&quot;target&quot;:&quot;_self&quot;},{&quot;title&quot;:&quot;How We\u0027re Organized \u2014 by Themes&quot;,&quot;url&quot;:&quot;https://pme.uchicago.edu/research/themes&quot;,&quot;target&quot;:&quot;&quot;},{&quot;title&quot;:&quot;Our Degree Programs&quot;,&quot;url&quot;:&quot;/academics&quot;,&quot;target&quot;:&quot;_self&quot;},{&quot;title&quot;:&quot;Equity, Diversity, &amp; Inclusion&quot;,&quot;url&quot;:&quot;/equity-diversity-inclusion&quot;,&quot;target&quot;:&quot;&quot;},{&quot;title&quot;:&quot;Leadership&quot;,&quot;url&quot;:&quot;/about/leadership&quot;,&quot;target&quot;:&quot;_self&quot;},{&quot;title&quot;:&quot;Faculty Accolades&quot;,&quot;url&quot;:&quot;/about/faculty-accolades&quot;,&quot;target&quot;:&quot;&quot;},{&quot;title&quot;:&quot;PME Facilities&quot;,&quot;url&quot;:&quot;/about/facilities&quot;,&quot;target&quot;:&quot;_self&quot;},{&quot;title&quot;:&quot;Argonne National Laboratory Facilities&quot;,&quot;url&quot;:&quot;https://www.anl.gov/national-scientific-user-facilities&quot;,&quot;target&quot;:&quot;_blank&quot;},{&quot;title&quot;:&quot;Partners&quot;,&quot;url&quot;:&quot;/about/partners&quot;,&quot;target&quot;:&quot;_self&quot;}]},{&quot;section&quot;:&quot;Research&quot;,&quot;link&quot;:&quot;/research/themes&quot;,&quot;target&quot;:&quot;_self&quot;,&quot;text&quot;:&quot;PME is organized  into problem-solving interdisciplinary themes focused on some of humanity\u0027s biggest challenges. Unlike traditional schools with departments, we work together to drive impact.&quot;,&quot;groups&quot;:[{&quot;group&quot;:&quot;Themes&quot;,&quot;links&quot;:[{&quot;title&quot;:&quot;Immunoengineering&quot;,&quot;url&quot;:&quot;/themes/immunoengineering&quot;,&quot;target&quot;:&quot;&quot;},{&quot;title&quot;:&quot;Materials Systems for Sustainability and Health&quot;,&quot;url&quot;:&quot;/themes/materials-systems-sustainability-and-health&quot;,&quot;target&quot;:&quot;&quot;},{&quot;title&quot;:&quot;Quantum Engineering&quot;,&quot;url&quot;:&quot;https://pme.uchicago.edu/quantum-uchicago&quot;,&quot;target&quot;:&quot;&quot;},{&quot;title&quot;:&quot;Arts, Sciences, and Technology&quot;,&quot;url&quot;:&quot;/themes/arts-sciences-and-technology&quot;,&quot;target&quot;:&quot;&quot;}]},{&quot;group&quot;:&quot;Other Areas of Focus&quot;,&quot;links&quot;:[{&quot;title&quot;:&quot;Artificial Intelligence&quot;,&quot;url&quot;:&quot;/ai-pme&quot;,&quot;target&quot;:&quot;&quot;}]}],&quot;links&quot;:[]},{&quot;section&quot;:&quot;Academics&quot;,&quot;link&quot;:&quot;/academics&quot;,&quot;target&quot;:&quot;_self&quot;,&quot;text&quot;:&quot;At the Pritzker School of Molecular Engineering, we are scientists, engineers, and above all, global-problem solvers.&quot;,&quot;groups&quot;:null,&quot;links&quot;:[{&quot;title&quot;:&quot;PhD Programs&quot;,&quot;url&quot;:&quot;/academics/phd-programs&quot;,&quot;target&quot;:&quot;_self&quot;},{&quot;title&quot;:&quot;Master\u0027s Program&quot;,&quot;url&quot;:&quot;/academics/masters-of-engineering&quot;,&quot;target&quot;:&quot;_self&quot;},{&quot;title&quot;:&quot;Undergraduate Program&quot;,&quot;url&quot;:&quot;/academics/undergraduate-program-molecular-engineering&quot;,&quot;target&quot;:&quot;_self&quot;},{&quot;title&quot;:&quot;Non-degree Visiting Student Research&quot;,&quot;url&quot;:&quot;/academics/non-degree-visiting-student-research&quot;,&quot;target&quot;:&quot;_self&quot;},{&quot;title&quot;:&quot;Dean of Students Office&quot;,&quot;url&quot;:&quot;/current-phd-students/pme-dean-students-office&quot;,&quot;target&quot;:&quot;&quot;},{&quot;title&quot;:&quot;Educational Outreach&quot;,&quot;url&quot;:&quot;/academics/educational-outreach&quot;,&quot;target&quot;:&quot;_self&quot;},{&quot;title&quot;:&quot;Postdoctoral Researchers&quot;,&quot;url&quot;:&quot;/academics/postdoctoral-researchers&quot;,&quot;target&quot;:&quot;&quot;}]},{&quot;section&quot;:&quot;People&quot;,&quot;link&quot;:&quot;/people&quot;,&quot;target&quot;:&quot;&quot;,&quot;text&quot;:&quot;We select our community purposefully, bringing together individuals committed to finding solutions to pressing world issues.&quot;,&quot;groups&quot;:null,&quot;links&quot;:[{&quot;title&quot;:&quot;Meet the Dean&quot;,&quot;url&quot;:&quot;https://pme.uchicago.edu/faculty/nadya-mason&quot;,&quot;target&quot;:&quot;_self&quot;},{&quot;title&quot;:&quot;Leadership&quot;,&quot;url&quot;:&quot;/about/leadership&quot;,&quot;target&quot;:&quot;_self&quot;},{&quot;title&quot;:&quot;CASE members&quot;,&quot;url&quot;:&quot;/people/case-directory&quot;,&quot;target&quot;:&quot;&quot;},{&quot;title&quot;:&quot;Faculty&quot;,&quot;url&quot;:&quot;/people/faculty-directory&quot;,&quot;target&quot;:&quot;_self&quot;},{&quot;title&quot;:&quot;Fellows&quot;,&quot;url&quot;:&quot;/people/fellows-directory&quot;,&quot;target&quot;:&quot;_self&quot;},{&quot;title&quot;:&quot;NSF\u2019s ChemMatCARS&quot;,&quot;url&quot;:&quot;/people/nsfs-chemmatcars-directory&quot;,&quot;target&quot;:&quot;&quot;},{&quot;title&quot;:&quot;Senior Instructional Professors&quot;,&quot;url&quot;:&quot;/people/senior-instructional-professors-sip-directory&quot;,&quot;target&quot;:&quot;_self&quot;},{&quot;title&quot;:&quot;Staff&quot;,&quot;url&quot;:&quot;/people/staff-directory&quot;,&quot;target&quot;:&quot;_self&quot;}]},{&quot;section&quot;:&quot;Lab Groups&quot;,&quot;link&quot;:&quot;/lab-groups-directory&quot;,&quot;target&quot;:&quot;&quot;,&quot;text&quot;:&quot;&quot;,&quot;groups&quot;:null,&quot;links&quot;:[]},{&quot;section&quot;:&quot;News&quot;,&quot;link&quot;:&quot;/news&quot;,&quot;target&quot;:&quot;&quot;,&quot;text&quot;:&quot;&quot;,&quot;groups&quot;:null,&quot;links&quot;:[]},{&quot;section&quot;:&quot;Events&quot;,&quot;link&quot;:&quot;/events&quot;,&quot;target&quot;:&quot;&quot;,&quot;text&quot;:&quot;Stay up to date with the latest events at the University of Chicago\u0027s Pritzker School of Molecular Engineering&quot;,&quot;groups&quot;:null,&quot;links&quot;:[]},{&quot;section&quot;:&quot;Give&quot;,&quot;link&quot;:&quot;/make-gift&quot;,&quot;target&quot;:&quot;&quot;,&quot;text&quot;:&quot;&quot;,&quot;groups&quot;:null,&quot;links&quot;:[]},{&quot;section&quot;:&quot;Apply Now&quot;,&quot;link&quot;:&quot;https://pme.uchicago.edu/academics/apply-now&quot;,&quot;target&quot;:&quot;_self&quot;,&quot;text&quot;:&quot;&quot;,&quot;groups&quot;:null,&quot;links&quot;:[]}],
  &quot;switcher&quot;: [
    {&quot;type&quot;:&quot;prospect&quot;,&quot;links&quot;:[{&quot;title&quot;:&quot;Current Students&quot;,&quot;url&quot;:&quot;/current-students&quot;,&quot;target&quot;:&quot;&quot;},{&quot;title&quot;:&quot;Industry&quot;,&quot;url&quot;:&quot;https://pme.uchicago.edu/industry&quot;,&quot;target&quot;:&quot;&quot;},{&quot;title&quot;:&quot;Publications&quot;,&quot;url&quot;:&quot;/publications&quot;,&quot;target&quot;:&quot;&quot;},{&quot;title&quot;:&quot;Intranet&quot;,&quot;url&quot;:&quot;https://uchicagoedu.sharepoint.com/sites/PME&quot;,&quot;target&quot;:&quot;_blank&quot;}]}
  ],
  &quot;site_logo&quot;: &quot;/sites/default/files/2021-06/pme_logo_color_rgb_v3.png&quot;,
  &quot;site_logo_alt&quot;: &quot;Pritzker School of Molecular Engineering&quot;
}">
</uc-masthead>

    
  </header>
  <main>
      





<section>
  <article>
    

        

<header>
      
  
<a href="https://pme.uchicago.edu/taxonomy/term/26/edit">
  News
</a>


    
  
      <p>
  <time datetime="2024-07-03T12:00:00Z">July 3, 2024</time>

</p>
  </header>

          
    
          
    
    <uc-share-links></uc-share-links>

          
    
    
    
    
                        <div>
            <p>UChicago Pritzker Molecular Engineering <a href="https://pme.uchicago.edu/faculty/y-shirley-meng">Prof. Y. Shirley Meng’s</a> <a href="https://lescmeng.ai/">Laboratory for Energy Storage and Conversion</a> has created the world’s first anode-free sodium solid-state battery.</p>
<p>With this research, the LESC – a collaboration between the UChicago Pritzker School of Molecular Engineering and the <a href="https://ne.ucsd.edu/">University of California San Diego’s Aiiso Yufeng Li Family Department of Chemical and Nano Engineering</a> – has brought the reality of inexpensive, fast-charging, high-capacity batteries for electric vehicles and grid storage closer than ever.</p>
<p>“Although there have been previous sodium, solid-state, and anode-free batteries, no one has been able to successfully combine these three ideas until now,” said UC San Diego PhD candidate Grayson Deysher, first author of a new paper outlining the team’s work.</p>
<p>The paper, <a href="https://www.nature.com/articles/s41560-024-01569-9">published today in <em>Nature Energy</em></a>, demonstrates a new sodium battery architecture with stable cycling for several hundred cycles. By removing the anode and using inexpensive, abundant sodium instead of lithium, this new form of battery will be more affordable and environmentally friendly to produce. Through its innovative solid-state design, the battery also will be safe and powerful.</p>
<p>This work is both an advance in the science and a necessary step to fill the battery scaling gap needed to transition the world economy off of fossil fuels.</p>
<p>“To keep the United States running for one hour, we must produce one terawatt hour of energy,” Meng said. “To accomplish our mission of decarbonizing our economy, we need several hundred terawatt hours of batteries. We need more batteries, and we need them fast.”</p>
<p><strong>Sustainability and sodium</strong></p>
<p>The lithium commonly used for batteries isn’t that common. It makes up about 20 parts per million of the Earth’s crust, compared to sodium, which makes up 20,000 parts per million.</p>
<p>This scarcity, combined with the surge in demand for the lithium-ion batteries for laptops, phones and EVs, have sent prices skyrocketing, putting the needed batteries further out of reach.</p>

          </div>
        
                  
                                <div>
            <p>Lithium deposits are also concentrated. The “Lithium Triangle” of Chile, Argentina and Bolivia holds more than 75% of the world’s lithium supply, with other deposits in Australia, North Carolina and Nevada. This benefits some nations over others in the decarbonization needed to fight climate change.</p>
<p>“Global action requires working together to access critically important materials,” Meng said.</p>
<p>Lithium extraction is also environmentally damaging, whether from the industrial acids used to break down mining ore or the more common brine extraction that pumps massive amounts of water to the surface to dry.</p>
<p>Sodium, common in ocean water and soda ash mining, is an inherently more environmentally friendly battery material. The LESC research has made it a powerful one as well.</p>
<p><strong>Innovative architecture</strong></p>
<p>To create a sodium battery with the energy density of a lithium battery, the team needed to invent a new sodium battery architecture.</p>
<p>Traditional batteries have an anode to store the ions while a battery is charging. While the battery is in use, the ions flow from the anode through an electrolyte to a current collector (cathode), powering devices and cars along the way.</p>
<p>Anode-free batteries remove the anode and store the ions on an electrochemical deposition of alkali metal directly on the current collector. This approach enables higher cell voltage, lower cell cost, and increased energy density, but brings its own challenges.</p>
<p>“In any anode-free battery there needs to be good contact between the electrolyte and the current collector,” Deysher said. “This is typically very easy when using a liquid electrolyte, as the liquid can flow everywhere and wet every surface. A solid electrolyte cannot do this.”</p>
<p>However, those liquid electrolytes create a buildup called solid electrolyte interphase while steadily consuming the active materials, reducing the battery’s usefulness over time.</p>
<p><strong>A solid that flows</strong></p>
<p>The team took a novel, innovative approach to this problem. Rather than using an electrolyte that surrounds the current collector, they created a current collector that surrounds the electrolyte.</p>
<p>They created their current collector out of aluminum powder, a solid that can flow like a liquid.</p>

          </div>
        
                  
                                <div>
            <p>During battery assembly the powder was densified under high pressure to form a solid current collector while maintaining a liquid-like contact with the electrolyte, enabling the low-cost and high-efficiency cycling that can push this game-changing technology forward.</p>
<p>“Sodium solid-state batteries are usually seen as a far-off-in-the-future technology, but we hope that this paper can invigorate more push into the sodium area by demonstrating that it can indeed work well, even better than the lithium version in some cases,” Deysher said.</p>
<p>The ultimate goal? Meng envisions an energy future with a variety of clean, inexpensive battery options that store renewable energy, scaled to fit society’s needs.</p>
<p>Meng and Deysher have filed a patent application for their work through UC San Diego’s Office of Innovation and Commercialization.</p>
<p><em>Citation: “Design principles for enabling an anode-free sodium all-solid-state battery,” Deysher et al, </em>Nature Energy, <em>July 3, 2024. DOI: </em><a href="https://www.nature.com/articles/s41560-024-01569-9"><em>10.1038/s41560-024-01569-9</em></a></p>
<p><em>Funding: Funding to support this work was provided by the National Science Foundation through the Partnerships for Innovation (PFI) grant no. 2044465</em></p>

          </div>
        
              
    


        <uc-share-links></uc-share-links>
    
  </article>
</section>







  </main>
  





</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Teaching general problem-solving skills is not a substitute for teaching math [pdf] (193 pts)]]></title>
            <link>https://www.ams.org/notices/201010/rtx101001303p.pdf</link>
            <guid>40890847</guid>
            <pubDate>Sat, 06 Jul 2024 15:07:30 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.ams.org/notices/201010/rtx101001303p.pdf">https://www.ams.org/notices/201010/rtx101001303p.pdf</a>, See on <a href="https://news.ycombinator.com/item?id=40890847">Hacker News</a></p>
&lt;Not HTML&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[Anxious Generation – How Safetyism and Social Media Are Damaging the Kids (115 pts)]]></title>
            <link>https://matija.eu/posts/anxious-generation-safetyism-social-media/</link>
            <guid>40890534</guid>
            <pubDate>Sat, 06 Jul 2024 14:02:15 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://matija.eu/posts/anxious-generation-safetyism-social-media/">https://matija.eu/posts/anxious-generation-safetyism-social-media/</a>, See on <a href="https://news.ycombinator.com/item?id=40890534">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>I just finished reading this book the other day. Written by Jonathan Haidt, <a href="https://www.amazon.com/Anxious-Generation-Rewiring-Childhood-Epidemic/dp/0593655036">"Anxious Generation"</a> deals with a concept he calls the "Great Rewiring." Essentially, he proposes that two forces at play nowadays have led (and continue to lead) an entire generation to significantly higher rates of mental illnesses.</p>
<p>One of these forces is unfettered access to social media. He draws a distinct line between the generic overuse of computers or the internet and the use of social media. The former began back in the 90s (or maybe even the 80s), but no dramatic increases in mental illness occurred back then. In those days, you'd be spending too much time in front of your screen, and the worst that could happen was you'd worsen your eyesight, end up somewhat socially inept, or even get a job in the industry once you grew up. He highlights that this was mostly observed among boys rather than girls, and the effects weren't as negative.</p>
<p>Nowadays, this is no longer the case. It's actually girls who bear the brunt of the effect, and the impact is far from harmless. He convincingly connects the dots between the social validation loop masterfully (ab)used by all modern social media, the early age that social media starts being used (early teens), and the statistics that show the incidence of mental illnesses just as platforms like Instagram started their growth in the 2010s.</p>
<p>The mind gets stuck in a loop where it continuously seeks validation and requires us to put up an almost perfect appearance for others. There's no way this is healthy for a young mind to go through. He even shows research indicating that school-aged kids might actually be taking a toll in cognitive performance due to this. A study done on kids taking exams shows there's an actual difference between taking the test without your phone in the room, with your phone in your pocket, and with your phone on the table right in front of you. You can guess which produces the best results and which produces the worst, as their minds are frankly always on standby, wondering what's going on in their social network.</p>
<p>In addition to social media use, he also talks about the other big issue that leads to his Great Rewiring: helicopter parenting.</p>
<p>The net effect of this is that kids have far more extended boundaries set on them (except on their phones!). For example, nowadays, parents expect their children to be free to go and do groceries alone or play outside without adult supervision only at around the age of 10 to 12 (if not even higher). Gen X, in his research, remembers this as having happened for them around ages 6, 7, or 8. On one hand, I feel like this claim rings true; on the other, I'm also wondering if there might be a case of some <a href="https://en.m.wikipedia.org/wiki/Rosy_retrospection">rosy retrospection</a> or wishful thinking.</p>
<p>Far from stopping there, he mentions other significant societal efforts that are thwarting children's growth, such as having playgrounds where kids don't exhibit any risk of harming themselves. Instead of preparing the kids and making them capable of (literally in this case) tackling obstacles, we're removing obstacles and coddling them.</p>
<p>Kids also become overprotected in other ways, such as not hearing other views or not being able to handle opposing views. No wonder academia is nowadays the exact opposite of free speech and the scientific method.</p>
<p>The trend of not keeping tabs on what kids are doing <em>online</em> (as opposed to offline, where the boundaries are much stricter) leads to what he deems a phone-based childhood. The kids are growing up playing with their phones rather than playing outside with other kids, learning the ropes of, well—life.</p>
<p>He suggests solving these regressions by reverting some societal safetyisms. He comes up with what he calls a "Ladder from Childhood to Adulthood" and presents checkpoints of what type of behavior one ought to expect their child to exhibit. For example, at six, the child should have a certain level of household responsibility. At eight, they might not require adult supervision to play outside, and maybe they get a dumbphone to stay in touch. Within this framework, the use of social media should come only at age 16, unlike the current state where it's supposedly age 13+ but in reality, this is not enforced at all.</p>
<p>Apart from these two major angles of the Great Rewiring, he also touches on the spiritual aspects (albeit he's an atheist), but that's not that important. He's somewhat activist about all of this, so he has a website dedicated to it all <a href="https://www.anxiousgeneration.com/">over here</a> (not just a book commercial).</p>
<p>All in all, it's a fairly interesting book, and I'll probably take a look at some of his other writings.</p>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Properly testing concurrent data structures (156 pts)]]></title>
            <link>https://matklad.github.io/2024/07/05/properly-testing-concurrent-data-structures.html</link>
            <guid>40890035</guid>
            <pubDate>Sat, 06 Jul 2024 12:29:18 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://matklad.github.io/2024/07/05/properly-testing-concurrent-data-structures.html">https://matklad.github.io/2024/07/05/properly-testing-concurrent-data-structures.html</a>, See on <a href="https://news.ycombinator.com/item?id=40890035">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
  <article>


<p><span>There</span>’<span>s a fascinating Rust library, </span><a href="https://github.com/tokio-rs/loom"><span>loom</span></a><span>, which can be used to</span>
<span>thoroughly test lock-free data structures. I always wanted to learn how it works. I still do! But</span>
<span>recently I accidentally implemented a small toy which, I think, contains some of the loom</span>’<span>s ideas,</span>
<span>and it seems worthwhile to write about that. The goal here isn</span>’<span>t to teach you what you should be</span>
<span>using in practice (if you need that, go read loom</span>’<span>s docs), but rather to derive a couple of neat</span>
<span>ideas from first principles.</span></p>
<section id="One-Two-Three-Two">

    <h2>
    <a href="#One-Two-Three-Two"><span>One, Two, Three, Two</span> </a>
    </h2>
<p><span>As usual, we need the simplest possible model program to mess with. The example we use comes from</span>
<a href="https://stevana.github.io/the_sad_state_of_property-based_testing_libraries.html"><span>this excellent article</span></a><span>.</span>
<span>Behold, a humble (and broken) concurrent counter:</span></p>

<figure>


<pre><code><span><span>use</span> std::sync::atomic::{</span>
<span>  AtomicU32,</span>
<span>  Ordering::SeqCst,</span>
<span>};</span>
<span></span>
<span><span>#[derive(Default)]</span></span>
<span><span>pub</span> <span>struct</span> <span>Counter</span> {</span>
<span>  value: AtomicU32,</span>
<span>}</span>
<span></span>
<span><span>impl</span> <span>Counter</span> {</span>
<span>  <span>pub</span> <span>fn</span> <span>increment</span>(&amp;<span>self</span>) {</span>
<span>    <span>let</span> <span>value</span> = <span>self</span>.value.<span>load</span>(SeqCst);</span>
<span>    <span>self</span>.value.<span>store</span>(value + <span>1</span>, SeqCst);</span>
<span>  }</span>
<span></span>
<span>  <span>pub</span> <span>fn</span> <span>get</span>(&amp;<span>self</span>) <span>-&gt;</span> <span>u32</span> {</span>
<span>    <span>self</span>.value.<span>load</span>(SeqCst)</span>
<span>  }</span>
<span>}</span></code></pre>

</figure>
<p><span>The bug is obvious here </span>—<span> the increment is not atomic. But what is the best test we can write to</span>
<span>expose it?</span></p>
</section>
<section id="Trivial-Test">

    <h2>
    <a href="#Trivial-Test"><span>Trivial Test</span> </a>
    </h2>
<p><span>The simplest idea that comes to mind is to just hammer the same counter from multiple threads and</span>
<span>check the result at the end;</span></p>

<figure>


<pre><code><span><span>#[test]</span></span>
<span><span>fn</span> <span>threaded_test</span>() {</span>
<span>  <span>let</span> <span>counter</span> = Counter::<span>default</span>();</span>
<span></span>
<span>  <span>let</span> <span>thread_count</span> = <span>100</span>;</span>
<span>  <span>let</span> <span>increment_count</span> = <span>100</span>;</span>
<span></span>
<span>  std::thread::<span>scope</span>(|scope| {</span>
<span>    <span>for</span> <span>_</span> <span>in</span> <span>0</span>..thread_count {</span>
<span>      scope.<span>spawn</span>(|| {</span>
<span>        <span>for</span> <span>_</span> <span>in</span> <span>0</span>..increment_count {</span>
<span>          counter.<span>increment</span>()</span>
<span>        }</span>
<span>      });</span>
<span>    }</span>
<span>  });</span>
<span></span>
<span>  <span>assert_eq!</span>(counter.<span>get</span>(), thread_count * increment_count);</span>
<span>}</span></code></pre>

</figure>
<p><span>This fails successfully:</span></p>

<figure>


<pre><code><span>thread 'counter::trivial' panicked:</span>
<span>assertion `left == right` failed</span>
<span>  left: 9598</span>
<span> right: 10000</span></code></pre>

</figure>
<p><span>But I wouldn</span>’<span>t call this test satisfactory </span>—<span> it very much depends on the timing, so you can</span>’<span>t</span>
<span>reproduce it deterministically and you can</span>’<span>t debug it. You also can</span>’<span>t minimize it </span>—<span> if you reduce</span>
<span>the number of threads and increments, chances are the test passes by luck!</span></p>
</section>
<section id="PBT">

    <h2>
    <a href="#PBT"><span>PBT</span> </a>
    </h2>
<p><span>Of course the temptation is to apply property based testing here! The problem </span><em><span>almost</span></em><span> fits: we have</span>
<span>easy-to-generate input (the sequence of increments spread over several threads), a good property to</span>
<span>check (result of concurrent increments is identical to that of sequential execution) and the desire</span>
<span>to minimize the test.</span></p>
<p><span>But just how can we plug threads into a property-based test?</span></p>
<p><span>PBTs are great for testing state machines. You can run your state machine through a series of steps</span>
<span>where at each step a PBT selects an arbitrary next action to apply to the state:</span></p>

<figure>


<pre><code><span><span>#[test]</span></span>
<span><span>fn</span> <span>state_machine_test</span>() {</span>
<span>  arbtest::<span>arbtest</span>(|rng| {</span>
<span>    <span>// This is our state machine!</span></span>
<span>    <span>let</span> <span>mut </span><span>state</span>: <span>i32</span> = <span>0</span>;</span>
<span></span>
<span>    <span>// We'll run it for up to 100 steps.</span></span>
<span>    <span>let</span> <span>step_count</span>: <span>usize</span> = rng.<span>int_in_range</span>(<span>0</span>..=<span>100</span>)?;</span>
<span></span>
<span>    <span>for</span> <span>_</span> <span>in</span> <span>0</span>..step_count {</span>
<span>      <span>// At each step, we flip a coin and</span></span>
<span>      <span>// either increment or decrement.</span></span>
<span>      <span>match</span> *rng.<span>choose</span>(&amp;[<span>"inc"</span>, <span>"dec"</span>])? {</span>
<span>        <span>"inc"</span> =&gt; state += <span>1</span>,</span>
<span>        <span>"dec"</span> =&gt; state -= <span>1</span>,</span>
<span>        _ =&gt; <span>unreachable!</span>(),</span>
<span>      }</span>
<span>    }</span>
<span>    <span>Ok</span>(())</span>
<span>  });</span>
<span>}</span></code></pre>

</figure>
<p><span>And it </span><em><span>feels</span></em><span> like we should be able to apply the same technique here. At every iteration, pick a</span>
<span>random thread and make it do a single step. If you can step the threads manually, it should be easy</span>
<span>to maneuver one thread in between load&amp;store of a different thread.</span></p>
<p><span>But we can</span>’<span>t step through threads! Or can we?</span></p>
</section>
<section id="Simple-Instrumentation">

    <h2>
    <a href="#Simple-Instrumentation"><span>Simple Instrumentation</span> </a>
    </h2>
<p><span>Ok, let</span>’<span>s fake it until we make it! Let</span>’<span>s take a look at the buggy increment method:</span></p>

<figure>


<pre><code><span><span>pub</span> <span>fn</span> <span>increment</span>(&amp;<span>self</span>) {</span>
<span>  <span>let</span> <span>value</span> = <span>self</span>.value.<span>load</span>(SeqCst);</span>
<span>  <span>self</span>.value.<span>store</span>(value + <span>1</span>, SeqCst);</span>
<span>}</span></code></pre>

</figure>
<p><span>Ideally, we</span>’<span>d love to be able to somehow </span>“<span>pause</span>”<span> the thread in-between atomic operations. Something</span>
<span>like this:</span></p>

<figure>


<pre><code><span><span>pub</span> <span>fn</span> <span>increment</span>(&amp;<span>self</span>) {</span>
<span>  <span>pause</span>();</span>
<span>  <span>let</span> <span>value</span> = <span>self</span>.value.<span>load</span>(SeqCst);</span>
<span>  <span>pause</span>();</span>
<span>  <span>self</span>.value.<span>store</span>(value + <span>1</span>, SeqCst);</span>
<span>  <span>pause</span>();</span>
<span>}</span>
<span></span>
<span><span>fn</span> <span>pause</span>() {</span>
<span>    <span>// ¯\_(ツ)_/¯</span></span>
<span>}</span></code></pre>

</figure>
<p><span>So let</span>’<span>s start with implementing our own wrapper for </span><code>AtomicU32</code><span> which includes calls to pause.</span></p>

<figure>


<pre><code><span><span>use</span> std::sync::atomic::Ordering;</span>
<span></span>
<span><span>struct</span> <span>AtomicU32</span> {</span>
<span>  inner: std::sync::atomic::AtomicU32,</span>
<span>}</span>
<span></span>
<span><span>impl</span> <span>AtomicU32</span> {</span>
<span>  <span>pub</span> <span>fn</span> <span>load</span>(&amp;<span>self</span>, ordering: Ordering) <span>-&gt;</span> <span>u32</span> {</span>
<span>    <span>pause</span>();</span>
<span>    <span>let</span> <span>result</span> = <span>self</span>.inner.<span>load</span>(ordering);</span>
<span>    <span>pause</span>();</span>
<span>    result</span>
<span>  }</span>
<span></span>
<span>  <span>pub</span> <span>fn</span> <span>store</span>(&amp;<span>self</span>, value: <span>u32</span>, ordering: Ordering) {</span>
<span>    <span>pause</span>();</span>
<span>    <span>self</span>.inner.<span>store</span>(value, ordering);</span>
<span>    <span>pause</span>();</span>
<span>  }</span>
<span>}</span>
<span></span>
<span><span>fn</span> <span>pause</span>() {</span>
<span>  <span>// still no idea :(</span></span>
<span>}</span></code></pre>

</figure>
</section>
<section id="Managed-Threads-API">

    <h2>
    <a href="#Managed-Threads-API"><span>Managed Threads API</span> </a>
    </h2>
<p><span>One rule of a great API design is that you start by implement a single </span><em><span>user</span></em><span> of an API, to</span>
<span>understand how the API should </span><em><span>feel</span></em><span>, and only then proceed to the actual implementation.</span></p>
<p><span>So, in the spirit of faking, let</span>’<span>s just write a PBT using these pausable, managed threads, even if</span>
<span>we still have no idea how to actually implement pausing.</span></p>
<p><span>We start with creating a counter and two managed threads. And we probably want to pass a reference</span>
<span>to the counter to each of the threads:</span></p>

<figure>


<pre><code><span><span>let</span> <span>counter</span> = Counter::<span>default</span>();</span>
<span><span>let</span> <span>t1</span> = managed_thread::<span>spawn</span>(&amp;counter);</span>
<span><span>let</span> <span>t2</span> = managed_thread::<span>spawn</span>(&amp;counter);</span></code></pre>

</figure>
<p><span>Now, we want to step through the threads:</span></p>

<figure>


<pre><code><span><span>while</span> !rng.<span>is_empty</span>() {</span>
<span>  <span>let</span> <span>coin_flip</span>: <span>bool</span> = rng.<span>arbitrary</span>()?;</span>
<span>  <span>if</span> t1.<span>is_paused</span>() {</span>
<span>    <span>if</span> coin_flip {</span>
<span>      t1.<span>unpause</span>();</span>
<span>    }</span>
<span>  } <span>else</span> <span>if</span> t2.<span>is_paused</span>() {</span>
<span>    <span>if</span> coin_flip {</span>
<span>      t2.<span>unpause</span>();</span>
<span>    }</span>
<span>  }</span>
<span>}</span></code></pre>

</figure>
<p><span>Or, refactoring this a bit to semantically compress:</span></p>

<figure>


<pre><code><span><span>let</span> <span>counter</span> = Counter::<span>default</span>();</span>
<span><span>let</span> <span>t1</span> = managed_thread::<span>spawn</span>(&amp;counter);</span>
<span><span>let</span> <span>t2</span> = managed_thread::<span>spawn</span>(&amp;counter);</span>
<span><span>let</span> <span>threads</span> = [t1, t2];</span>
<span></span>
<span><span>while</span> !rng.<span>is_empty</span>() {</span>
<span>  <span>for</span> <span>t</span> <span>in</span> &amp;<span>mut</span> threads {</span>
<span>    <span>if</span> t.<span>is_paused</span>() &amp;&amp; rng.<span>arbitrary</span>()? {</span>
<span>      t.<span>unpause</span>()</span>
<span>    }</span>
<span>  }</span>
<span>}</span></code></pre>

</figure>
<p><span>That is, on each step of our state machine, we loop through all threads and unpause a random subset</span>
<span>of them.</span></p>
<p><span>But besides pausing and unpausing, we need our threads to actually </span><em><span>do</span></em><span> something, to increment the</span>
<span>counter. One idea is to mirror the </span><code>std::spawn</code><span> API and pass a closure in:</span></p>

<figure>


<pre><code><span><span>let</span> <span>t1</span> = managed_thread::<span>spawn</span>({</span>
<span>  <span>let</span> <span>counter</span> = &amp;counter;</span>
<span>  <span>move</span> || {</span>
<span>    <span>for</span> <span>_</span> <span>in</span> <span>0</span>..<span>100</span> {</span>
<span>      counter.<span>increment</span>();</span>
<span>    }</span>
<span>  }</span>
<span>});</span></code></pre>

</figure>
<p><span>But as these are managed threads, and we want to control them from our tests, lets actually go all</span>
<span>the way there and give the controlling thread an ability to change the code running in a managed</span>
<span>thread. That is, we</span>’<span>ll start managed threads without a </span>“<span>main</span>”<span> function, and provide an API to</span>
<span>execute arbitrary closures in the context of this by-default inert thread (</span><a href="https://joearms.github.io/published/2013-11-21-My-favorite-erlang-program.html"><span>universal</span>
<span>server</span></a><span> anyone?):</span></p>

<figure>


<pre><code><span><span>let</span> <span>counter</span> = Counter::<span>default</span>();</span>
<span></span>
<span><span>// We pass the state, &amp;counter, in, but otherwise the thread is inert.</span></span>
<span><span>let</span> <span>t</span> = managed_thread::<span>spawn</span>(&amp;counter);</span>
<span></span>
<span><span>// But we can manually poke it:</span></span>
<span>t.<span>submit</span>(|thread_state: &amp;Counter| thread_state.<span>increment</span>());</span>
<span>t.<span>submit</span>(|thread_state: &amp;Counter| thread_state.<span>increment</span>());</span></code></pre>

</figure>
<p><span>Putting everything together, we get a nice-looking property test:</span></p>

<figure>


<pre><code><span><span>#[cfg(test)]</span></span>
<span><span>use</span> managed_thread::AtomicU32;</span>
<span><span>#[cfg(not(test))]</span></span>
<span><span>use</span> std::sync::atomic::AtomicU32;</span>
<span></span>
<span><span>#[derive(Default)]</span></span>
<span><span>pub</span> <span>struct</span> <span>Counter</span> {</span>
<span>  value: AtomicU32,</span>
<span>}</span>
<span></span>
<span><span>impl</span> <span>Counter</span> {</span>
<span>  <span>// ...</span></span>
<span>}</span>
<span></span>
<span><span>#[test]</span></span>
<span><span>fn</span> <span>test_counter</span>() {</span>
<span>  arbtest::<span>arbtest</span>(|rng| {</span>
<span>    <span>// Our "Concurrent System Under Test".</span></span>
<span>    <span>let</span> <span>counter</span> = Counter::<span>default</span>();</span>
<span></span>
<span>    <span>// The sequential model we'll compare the result against.</span></span>
<span>    <span>let</span> <span>counter_model</span>: <span>u32</span> = <span>0</span>;</span>
<span></span>
<span>    <span>// Two managed threads which we will be stepping through</span></span>
<span>    <span>// manually.</span></span>
<span>    <span>let</span> <span>t1</span> = managed_thread::<span>spawn</span>(&amp;counter);</span>
<span>    <span>let</span> <span>t2</span> = managed_thread::<span>spawn</span>(&amp;counter);</span>
<span>    <span>let</span> <span>threads</span> = [t1, t2];</span>
<span></span>
<span>    <span>// Bulk of the test: in a loop, flip a coin and advance</span></span>
<span>    <span>// one of the threads.</span></span>
<span>    <span>while</span> !rng.<span>is_empty</span>() {</span>
<span>      <span>for</span> <span>t</span> <span>in</span> &amp;<span>mut</span> [t1, t2] {</span>
<span>        <span>if</span> rng.<span>arbitrary</span>() {</span>
<span>          <span>if</span> t.<span>is_paused</span>() {</span>
<span>            t.<span>unpause</span>()</span>
<span>          } <span>else</span> {</span>
<span>            <span>// Standard "model equivalence" property: apply</span></span>
<span>            <span>// isomorphic actions to the system and its model.</span></span>
<span>            t.<span>submit</span>(|c| c.<span>increment</span>());</span>
<span>            counter_model += <span>1</span>;</span>
<span>          }</span>
<span>        }</span>
<span>      }</span>
<span>    }</span>
<span></span>
<span>    <span>for</span> <span>t</span> <span>in</span> threads {</span>
<span>      t.<span>join</span>();</span>
<span>    }</span>
<span></span>
<span>    <span>assert_eq!</span>(counter_model, counter.<span>get</span>());</span>
<span></span>
<span>    <span>Ok</span>(())</span>
<span>  });</span>
<span>}</span></code></pre>

</figure>
<p><span>Now, if only we could make this API work</span>…<span> Remember, our </span><code>pause</code><span> implementation is a shrug emoji!</span></p>
<p><span>At this point, you might be mightily annoyed at me for this rhetorical device where I pretend that I</span>
<span>don</span>’<span>t know the answer. No need for annoyance </span>—<span> when writing this code for the first time, I traced</span>
<span>exactly these steps </span>—<span> I realized that I need a </span>“<span>pausing </span><code>AtomicU32</code>”<span> so I did that (with dummy</span>
<span>pause calls), then I played with the API I </span><em><span>wanted</span></em><span> to have, ending at roughly this spot, without</span>
<span>yet knowing how I would make it work or, indeed, if it is possible at all.</span></p>
<p><span>Well, if I am being honest, there is a bit of up-front knowledge here. I don</span>’<span>t think we can avoid</span>
<span>spawning real threads here, unless we do something really cursed with inline assembly. When</span>
<em><span>something</span></em><span> calls that </span><code>pause()</code><span> function, and we want it to stay paused until further notice, that</span>
<span>just has to happen in a thread which maintains a stack separate from the stack of our test. And, if</span>
<span>we are going to spawn threads, we might as well spawn scoped threads, so that we can freely borrow</span>
<span>stack-local data. And to spawn a scope thread, you need a</span>
<a href="https://doc.rust-lang.org/stable/std/thread/struct.Scope.html"><code>Scope</code></a><span> parameter. So in reality</span>
<span>we</span>’<span>ll need one more level of indentation here:</span></p>

<figure>


<pre><code><span>    std::thread::<span>scope</span>(|scope| {</span>
<span>      <span>let</span> <span>t1</span> = managed_thread::<span>spawn</span>(scope, &amp;counter);</span>
<span>      <span>let</span> <span>t2</span> = managed_thread::<span>spawn</span>(scope, &amp;counter);</span>
<span>      <span>let</span> <span>threads</span> = [t1, t2];</span>
<span>      <span>while</span> !rng.<span>is_empty</span>() {</span>
<span>        <span>for</span> <span>t</span> <span>in</span> &amp;<span>mut</span> [t1, t2] {</span>
<span>          <span>// ...</span></span>
<span>        }</span>
<span>      }</span>
<span>    });</span></code></pre>

</figure>
</section>
<section id="Managed-Threads-Implementation">

    <h2>
    <a href="#Managed-Threads-Implementation"><span>Managed Threads Implementation</span> </a>
    </h2>
<p><span>Now, the fun part: how the heck are we going to make pausing and unpausing work? For starters, there</span>
<span>clearly needs to be some communication between the main thread (</span><code>t.unpause()</code><span>) and the managed</span>
<span>thread (</span><code>pause()</code><span>). And, because we don</span>’<span>t want to change </span><code>Counter</code><span> API to thread some kind of</span>
<span>test-only context, the context needs to be smuggled. So </span><code>thread_local!</code><span> it is. And this context</span>
<span>is going to be shared between two threads, so it must be wrapped in an </span><code>Arc</code><span>.</span></p>

<figure>


<pre><code><span><span>struct</span> <span>SharedContext</span> {</span>
<span>  <span>// 🤷</span></span>
<span>}</span>
<span></span>
<span>thread_local! {</span>
<span>  <span>static</span> INSTANCE: RefCell&lt;<span>Option</span>&lt;Arc&lt;SharedContext&gt;&gt;&gt; =</span>
<span>    RefCell::<span>new</span>(<span>None</span>);</span>
<span>}</span>
<span></span>
<span><span>impl</span> <span>SharedContext</span> {</span>
<span>  <span>fn</span> <span>set</span>(ctx: Arc&lt;SharedContext&gt;) {</span>
<span>    INSTANCE.<span>with</span>(|it| *it.<span>borrow_mut</span>() = <span>Some</span>(ctx));</span>
<span>  }</span>
<span></span>
<span>  <span>fn</span> <span>get</span>() <span>-&gt;</span> <span>Option</span>&lt;Arc&lt;SharedContext&gt;&gt; {</span>
<span>    INSTANCE.<span>with</span>(|it| it.<span>borrow</span>().<span>clone</span>())</span>
<span>  }</span>
<span>}</span></code></pre>

</figure>
<p><span>As usual when using </span><code>thread_local!</code><span> or </span><code>lazy_static!</code><span>, it is convenient to immediately wrap it into</span>
<span>better typed accessor functions. And, given that we are using an </span><code>Arc</code><span> here anyway, we can</span>
<span>conveniently escape </span><code>thread_local</code>’<span>s </span><code>with</code><span> by cloning the </span><code>Arc</code><span>.</span></p>
<p><span>So now we finally can implement the global </span><code>pause</code><span> function (or at least can kick the proverbial can</span>
<span>a little bit farther):</span></p>

<figure>


<pre><code><span><span>fn</span> <span>pause</span>() {</span>
<span>  <span>if</span> <span>let</span> <span>Some</span>(ctx) = SharedContext::<span>get</span>() {</span>
<span>    ctx.<span>pause</span>()</span>
<span>  }</span>
<span>}</span>
<span></span>
<span><span>impl</span> <span>SharedContext</span> {</span>
<span>  <span>fn</span> <span>pause</span>(&amp;<span>self</span>) {</span>
<span>    <span>// 😕</span></span>
<span>  }</span>
<span>}</span></code></pre>

</figure>
<p><span>Ok, what to do next? We somehow need to coordinate the control thread and the managed thread. And we</span>
<span>need some sort of notification mechanism, so that the managed thread knows when it can continue. The</span>
<span>most brute force solution here is a pair of a mutex protecting some state and a condition variable.</span>
<span>Mutex guards the state that can be manipulated by either of the threads. Condition variable can be</span>
<span>used to signal about the changes.</span></p>

<figure>


<pre><code><span><span>struct</span> <span>SharedContext</span> {</span>
<span>  state: Mutex&lt;State&gt;,</span>
<span>  cv: Condvar,</span>
<span>}</span>
<span></span>
<span><span>struct</span> <span>State</span> {</span>
<span>  <span>// 🤡</span></span>
<span>}</span></code></pre>

</figure>
<p><span>Okay, it looks like I am running out of emojies here. There</span>’<span>s no more layers of indirection or</span>
<span>infrastructure left, we need to write some real code that actually does do that pausing thing. So</span>
<span>let</span>’<span>s say that the state is tracking, well, the state of our managed thread, which can be either</span>
<span>running or paused:</span></p>

<figure>


<pre><code><span><span>#[derive(PartialEq, Eq, Default)]</span></span>
<span><span>enum</span> <span>State</span> {</span>
<span>  <span>#[default]</span></span>
<span>  Running,</span>
<span>  Paused,</span>
<span>}</span></code></pre>

</figure>
<p><span>And then the logic of the pause function </span>—<span> flip the state from </span><code>Running</code><span> to </span><code>Paused</code><span>, notify the</span>
<span>controlling thread that we are </span><code>Paused</code><span>, and wait until the controlling thread flips our state back</span>
<span>to </span><code>Running</code><span>:</span></p>

<figure>


<pre><code><span><span>impl</span> <span>SharedContext</span> {</span>
<span>  <span>fn</span> <span>pause</span>(&amp;<span>self</span>) {</span>
<span>    <span>let</span> <span>mut </span><span>guard</span> = <span>self</span>.state.<span>lock</span>().<span>unwrap</span>();</span>
<span>    <span>assert_eq!</span>(*guard, State::Running);</span>
<span>    *guard = State::Paused;</span>
<span>    <span>self</span>.cv.<span>notify_all</span>();</span>
<span>    <span>while</span> *guard == State::Paused {</span>
<span>      guard = <span>self</span>.cv.<span>wait</span>(guard).<span>unwrap</span>();</span>
<span>    }</span>
<span>    <span>assert_eq!</span>(*guard, State::Running);</span>
<span>  }</span>
<span>}</span></code></pre>

</figure>
<p><span>Aside: Rust</span>’<span>s API for condition variables is beautiful. Condvars are tricky, and I didn</span>’<span>t really</span>
<span>understood them until seeing the signatures of Rust functions. Notice how the </span><code>wait</code><span> function</span>
<em><span>takes</span></em><span> a mutex guard as an argument, and returns a mutex guard. This protects you from the logical</span>
<span>races and guides you towards the standard pattern of using condvars:</span></p>
<p><span>First, you lock the mutex around the shared state. Then, you inspect whether the state is what you</span>
<span>need. If that</span>’<span>s the case, great, you do what you wanted to do and unlock the mutex. If not, then,</span>
<em><span>while still holding the mutex</span></em><span>, you </span><em><span>wait</span></em><span> on the condition variable. Which means that the</span>
<span>mutex gets unlocked, and other threads get the chance to change the shared state. When they do</span>
<span>change it, and notify the condvar, your thread wakes up, and it gets the locked mutex back (but the</span>
<span>state now is different). Due to the possibility of spurious wake-ups, you need to double check the</span>
<span>state and be ready to loop back again to waiting.</span></p>
<p><span>Naturally, there</span>’<span>s a helper that encapsulates this whole pattern:</span></p>

<figure>


<pre><code><span><span>impl</span> <span>SharedContext</span> {</span>
<span>  <span>fn</span> <span>pause</span>(&amp;<span>self</span>) {</span>
<span>    <span>let</span> <span>mut </span><span>guard</span> = <span>self</span>.state.<span>lock</span>().<span>unwrap</span>();</span>
<span>    <span>assert_eq!</span>(*guard, State::Running);</span>
<span>    *guard = State::Paused;</span>
<span>    <span>self</span>.cv.<span>notify_all</span>();</span>
<span>    guard = <span>self</span></span>
<span>      .cv</span>
<span>      .<span>wait_while</span>(guard, |state| *state == State::Paused)</span>
<span>      .<span>unwrap</span>();</span>
<span>    <span>assert_eq!</span>(*guard, State::Running)</span>
<span>  }</span>
<span>}</span></code></pre>

</figure>
<p><span>Ok, this actually does look like a reasonable implementation of </span><code>pause</code><span>. Let</span>’<span>s move on to</span>
<code>managed_thread::spawn</code><span>:</span></p>

<figure>


<pre><code><span><span>fn</span> <span>spawn</span>&lt;<span>'scope</span>, T: <span>'scope</span> + <span>Send</span>&gt;(</span>
<span>  scope: &amp;Scope&lt;<span>'scope</span>, <span>'_</span>&gt;,</span>
<span>  state: T,</span>
<span>) {</span>
<span>  <span>// ? ? ?? ??? ?????</span></span>
<span>}</span></code></pre>

</figure>
<p><span>There</span>’<span>s a bunch of stuff that needs to happen here:</span></p>
<ul>
<li>
<span>As we have established, we are going to spawn a (scoped) thread, so we need the </span><code>scope</code><span> parameter</span>
<span>with its three lifetimes. I don</span>’<span>t know how it works, so I am just going by the docs here!</span>
</li>
<li>
<span>We are going to return some kind of handle, which we can use to pause and unpause our managed</span>
<span>thread. And that handle is going to be parametrized over the same </span><code>'scope</code><span> lifetime, because it</span>’<span>ll</span>
<span>hold onto the actual join handle.</span>
</li>
<li>
<span>We are going to pass the generic state to our new thread, and that state needs to be </span><code>Send</code><span>, and</span>
<span>bounded by the same lifetime as our scoped thread.</span>
</li>
<li>
<span>Inside, we are going to spawn a thread for sure, and we</span>’<span>ll need to setup the </span><code>INSTANCE</code><span> thread</span>
<span>local on that thread.</span>
</li>
<li>
<span>And it would actually be a good idea to stuff a reference to that </span><code>SharedContext</code><span> into the handle</span>
<span>we return.</span>
</li>
</ul>
<p><span>A bunch of stuff, in other words. Let</span>’<span>s do it:</span></p>

<figure>


<pre><code><span><span>struct</span> <span>ManagedHandle</span>&lt;<span>'scope</span>&gt; {</span>
<span>  inner: std::thread::ScopedJoinHandle&lt;<span>'scope</span>, ()&gt;,</span>
<span>  ctx: Arc&lt;SharedContext&gt;,</span>
<span>}</span>
<span></span>
<span><span>fn</span> <span>spawn</span>&lt;<span>'scope</span>, T: <span>'scope</span> + <span>Send</span>&gt;(</span>
<span>  scope: &amp;<span>'scope</span> Scope&lt;<span>'scope</span>, <span>'_</span>&gt;,</span>
<span>  state: T,</span>
<span>) <span>-&gt;</span> ManagedHandle&lt;<span>'scope</span>&gt; {</span>
<span>  <span>let</span> <span>ctx</span>: Arc&lt;SharedContext&gt; = <span>Default</span>::<span>default</span>();</span>
<span>  <span>let</span> <span>inner</span> = scope.<span>spawn</span>({</span>
<span>    <span>let</span> <span>ctx</span> = Arc::<span>clone</span>(&amp;ctx);</span>
<span>    <span>move</span> || {</span>
<span>      SharedContext::<span>set</span>(ctx);</span>
<span>      <span>drop</span>(state); <span>// <span>TODO:</span> ¿</span></span>
<span>    }</span>
<span>  });</span>
<span>  ManagedHandle { inner, ctx }</span>
<span>}</span></code></pre>

</figure>
<p><span>The essentially no-op function we spawn looks sus. We</span>’<span>ll fix later! Let</span>’<span>s try to implement</span>
<code>is_paused</code><span> and </span><code>unpause</code><span> first! They should be relatively straightforward. For </span><code>is_paused</code><span>, we just</span>
<span>need to lock the mutex and check the state:</span></p>

<figure>


<pre><code><span><span>impl</span> <span>ManagedHandle</span>&lt;<span>'_</span>&gt; {</span>
<span>  <span>pub</span> <span>fn</span> <span>is_paused</span>(&amp;<span>self</span>,) <span>-&gt;</span> <span>bool</span> {</span>
<span>    <span>let</span> <span>guard</span> = <span>self</span>.ctx.state.<span>lock</span>().<span>unwrap</span>();</span>
<span>    *guard == State::Paused</span>
<span>  }</span>
<span>}</span></code></pre>

</figure>
<p><span>For </span><code>unpause</code><span>, we should additionally flip the state back to </span><code>Running</code><span> and notify the other thread:</span></p>

<figure>


<pre><code><span><span>impl</span> <span>ManagedHandle</span>&lt;<span>'_</span>&gt; {</span>
<span>  <span>pub</span> <span>fn</span> <span>unpause</span>(&amp;<span>self</span>) {</span>
<span>    <span>let</span> <span>mut </span><span>guard</span> = <span>self</span>.ctx.state.<span>lock</span>().<span>unwrap</span>();</span>
<span>    <span>assert_eq!</span>(*guard, State::Paused);</span>
<span>    *guard = State::Running;</span>
<span>    <span>self</span>.ctx.cv.<span>notify_all</span>();</span>
<span>  }</span>
<span>}</span></code></pre>

</figure>
<p><span>But I think that</span>’<span>s not quiet correct. Can you see why?</span></p>
<p><span>With this implementation, after </span><code>unpause</code><span>, the controlling and the managed threads will be running</span>
<span>concurrently. And that can lead to non-determinism, the very problem we are trying to avoid here! In</span>
<span>particular, if you call </span><code>is_paused</code><span> </span><em><span>right</span></em><span> after you </span><code>unpause</code><span> the thread, you</span>’<span>ll most likely get</span>
<code>false</code><span> back, as the other thread will still be running. But it might also hit the </span><em><span>next</span></em><span> </span><code>pause</code>
<span>call, so, depending on timing, you might also get </span><code>true</code><span>.</span></p>
<p><span>What we want is actually completely eliminating all unmanaged concurrency. That means that at any</span>
<span>given point in time, only one thread (controlling or managed) should be running. So the right</span>
<span>semantics for </span><code>unpause</code><span> is to unblock the managed thread, and then block the controlling thread</span>
<span>until the managed one hits the next pause!</span></p>

<figure>


<pre><code><span><span>impl</span> <span>ManagedHandle</span>&lt;<span>'_</span>&gt; {</span>
<span>  <span>pub</span> <span>fn</span> <span>unpause</span>(&amp;<span>self</span>) {</span>
<span>    <span>let</span> <span>mut </span><span>guard</span> = <span>self</span>.ctx.state.<span>lock</span>().<span>unwrap</span>();</span>
<span>    <span>assert_eq!</span>(*guard, State::Paused);</span>
<span>    *guard = State::Running;</span>
<span>    <span>self</span>.ctx.cv.<span>notify_all</span>();</span>
<span>    guard = <span>self</span></span>
<span>      .ctx</span>
<span>      .cv</span>
<span>      .<span>wait_while</span>(guard, |state| *state == State::Running)</span>
<span>      .<span>unwrap</span>();</span>
<span>  }</span>
<span>}</span></code></pre>

</figure>
<p><span>At this point we can spawn a managed thread, pause it and resume. But right now it doesn</span>’<span>t do</span>
<span>anything. Next step is implementing that idea where the controlling thread can directly send an</span>
<span>arbitrary closure to the managed one to make it do something:</span></p>

<figure>


<pre><code><span><span>impl</span>&lt;<span>'scope</span>&gt; ManagedHandle&lt;<span>'scope</span>&gt; {</span>
<span>  <span>pub</span> <span>fn</span> <span>submit</span>&lt;F: FnSomething&gt;(&amp;<span>self</span>, f: F)</span>
<span>}</span></code></pre>

</figure>
<p><span>Let</span>’<span>s figure this </span><code>FnSomething</code><span> bound! We are going to yeet this </span><code>f</code><span> over to the managed thread and</span>
<span>run it there once, so it is </span><code>FnOnce</code><span>. It is crossing thread-boundary, so it needs to be </span><code>+ Send</code><span>.</span>
<span>And, because we are using scoped threads, it </span><em><span>doesn</span>’<span>t</span></em><span> have to be </span><code>'static</code><span>, just </span><code>'scope</code><span> is</span>
<span>enough. Moreover, in that managed thread the </span><code>f</code><span> will have exclusive access to thread</span>’<span>s state, </span><code>T</code><span>.</span>
<span>So we have:</span></p>

<figure>


<pre><code><span><span>impl</span>&lt;<span>'scope</span>&gt; ManagedHandle&lt;<span>'scope</span>&gt; {</span>
<span>  <span>pub</span> <span>fn</span> <span>submit</span>&lt;F: <span>FnOnce</span>(&amp;<span>mut</span> T) + <span>Send</span> + <span>'scope</span>&gt;(<span>self</span>, f: F)</span>
<span>}</span></code></pre>

</figure>
<p><span>Implementing this is a bit tricky. First, we</span>’<span>ll need some sort of the channel to actually move the</span>
<span>function. Then, similarly to the </span><code>unpause</code><span> logic, we</span>’<span>ll need synchronization to make sure that the</span>
<span>control thread doesn</span>’<span>t resume until the managed thread starts running </span><code>f</code><span> and hits a pause (or maybe</span>
<span>completes </span><code>f</code><span>). And we</span>’<span>ll also need a new state, </span><code>Ready</code><span>, because now there are two different</span>
<span>reasons why a managed thread might be blocked </span>—<span> it might wait for an </span><code>unpause</code><span> event, or it might</span>
<span>wait for the next </span><code>f</code><span> to execute. This is the new code:</span></p>

<figure>


<pre><code><span><span>#[derive(Default)]</span></span>
<span><span>enum</span> <span>State</span> {</span>
<span>  <span>#[default]</span></span>
<span>  Ready,</span>
<span>  Running,</span>
<span>  Paused,</span>
<span>}</span>
<span></span>
<span><span>struct</span> <span>ManagedHandle</span>&lt;<span>'scope</span>, T&gt; {</span>
<span>  inner: std::thread::ScopedJoinHandle&lt;<span>'scope</span>, ()&gt;,</span>
<span>  ctx: Arc&lt;SharedContext&gt;,</span>
<span>  sender: mpsc::Sender&lt;<span>Box</span>&lt;<span>dyn</span> <span>FnOnce</span>(&amp;<span>mut</span> T) + <span>'scope</span> + <span>Send</span>&gt;&gt;,</span>
<span>}</span>
<span></span>
<span><span>pub</span> <span>fn</span> <span>spawn</span>&lt;<span>'scope</span>, T: <span>'scope</span> + <span>Send</span>&gt;(</span>
<span>  scope: &amp;<span>'scope</span> Scope&lt;<span>'scope</span>, <span>'_</span>&gt;,</span>
<span>  <span>mut</span> state: T,</span>
<span>) <span>-&gt;</span> ManagedHandle&lt;<span>'scope</span>, T&gt; {</span>
<span>  <span>let</span> <span>ctx</span>: Arc&lt;SharedContext&gt; = <span>Default</span>::<span>default</span>();</span>
<span>  <span>let</span> (sender, receiver) =</span>
<span>    mpsc::channel::&lt;<span>Box</span>&lt;<span>dyn</span> <span>FnOnce</span>(&amp;<span>mut</span> T) + <span>'scope</span> + <span>Send</span>&gt;&gt;();</span>
<span>  <span>let</span> <span>inner</span> = scope.<span>spawn</span>({</span>
<span>    <span>let</span> <span>ctx</span> = Arc::<span>clone</span>(&amp;ctx);</span>
<span>    <span>move</span> || {</span>
<span>      SharedContext::<span>set</span>(Arc::<span>clone</span>(&amp;ctx));</span>
<span></span>
<span>      <span>for</span> <span>f</span> <span>in</span> receiver {</span>
<span>        <span>f</span>(&amp;<span>mut</span> state);</span>
<span></span>
<span>        <span>let</span> <span>mut </span><span>guard</span> = ctx.state.<span>lock</span>().<span>unwrap</span>();</span>
<span>        <span>assert_eq!</span>(*guard, State::Running);</span>
<span>        *guard = State::Ready;</span>
<span>        ctx.cv.<span>notify_all</span>()</span>
<span>      }</span>
<span>    }</span>
<span>  });</span>
<span>  ManagedHandle { inner, ctx, sender }</span>
<span>}</span>
<span></span>
<span><span>impl</span>&lt;<span>'scope</span>, T&gt; ManagedHandle&lt;<span>'scope</span>, T&gt; {</span>
<span>  <span>pub</span> <span>fn</span> <span>submit</span>&lt;F: <span>FnOnce</span>(&amp;<span>mut</span> T) + <span>Send</span> + <span>'scope</span>&gt;(&amp;<span>self</span>, f: F) {</span>
<span>    <span>let</span> <span>mut </span><span>guard</span> = <span>self</span>.ctx.state.<span>lock</span>().<span>unwrap</span>();</span>
<span>    <span>assert_eq!</span>(*guard, State::Ready);</span>
<span>    *guard = State::Running;</span>
<span>    <span>self</span>.sender.<span>send</span>(<span>Box</span>::<span>new</span>(f)).<span>unwrap</span>();</span>
<span>    guard = <span>self</span></span>
<span>      .ctx</span>
<span>      .cv</span>
<span>      .<span>wait_while</span>(guard, |state| *state == State::Running)</span>
<span>      .<span>unwrap</span>();</span>
<span>  }</span>
<span>}</span></code></pre>

</figure>
<p><span>The last small piece of the puzzle is the </span><code>join</code><span> function. It</span>’<span>s </span><em><span>almost</span></em><span> standard! First we close</span>
<span>our side of the channel. This serves as a natural stop signal for the other thread, so it exits.</span>
<span>Which in turn allows us to join it. The small wrinkle here is that the thread might be paused when</span>
<span>we try to join it, so we need to unpause it beforehand:</span></p>

<figure>


<pre><code><span><span>impl</span>&lt;<span>'scope</span>, T&gt; ManagedHandle&lt;<span>'scope</span>, T&gt; {</span>
<span>  <span>pub</span> <span>fn</span> <span>join</span>(<span>self</span>) {</span>
<span>    <span>while</span> <span>self</span>.<span>is_paused</span>() {</span>
<span>      <span>self</span>.<span>unpause</span>();</span>
<span>    }</span>
<span>    <span>drop</span>(<span>self</span>.sender);</span>
<span>    <span>self</span>.inner.<span>join</span>().<span>unwrap</span>();</span>
<span>  }</span>
<span>}</span></code></pre>

</figure>
<p><span>That</span>’<span>s it! Let</span>’<span>s put everything together!</span></p>
<p><span>Helper library, </span><code>managed_thread.rs</code><span>:</span></p>

<figure>


<pre><code><span><span>use</span> std::{</span>
<span>  cell::RefCell,</span>
<span>  sync::{atomic::Ordering, mpsc, Arc, Condvar, Mutex},</span>
<span>  thread::Scope,</span>
<span>};</span>
<span></span>
<span><span>#[derive(Default)]</span></span>
<span><span>pub</span> <span>struct</span> <span>AtomicU32</span> {</span>
<span>  inner: std::sync::atomic::AtomicU32,</span>
<span>}</span>
<span></span>
<span><span>impl</span> <span>AtomicU32</span> {</span>
<span>  <span>pub</span> <span>fn</span> <span>load</span>(&amp;<span>self</span>, ordering: Ordering) <span>-&gt;</span> <span>u32</span> {</span>
<span>    <span>pause</span>();</span>
<span>    <span>let</span> <span>result</span> = <span>self</span>.inner.<span>load</span>(ordering);</span>
<span>    <span>pause</span>();</span>
<span>    result</span>
<span>  }</span>
<span></span>
<span>  <span>pub</span> <span>fn</span> <span>store</span>(&amp;<span>self</span>, value: <span>u32</span>, ordering: Ordering) {</span>
<span>    <span>pause</span>();</span>
<span>    <span>self</span>.inner.<span>store</span>(value, ordering);</span>
<span>    <span>pause</span>();</span>
<span>  }</span>
<span>}</span>
<span></span>
<span><span>fn</span> <span>pause</span>() {</span>
<span>  <span>if</span> <span>let</span> <span>Some</span>(ctx) = SharedContext::<span>get</span>() {</span>
<span>    ctx.<span>pause</span>()</span>
<span>  }</span>
<span>}</span>
<span></span>
<span><span>#[derive(Default)]</span></span>
<span><span>struct</span> <span>SharedContext</span> {</span>
<span>  state: Mutex&lt;State&gt;,</span>
<span>  cv: Condvar,</span>
<span>}</span>
<span></span>
<span><span>#[derive(Default, PartialEq, Eq, Debug)]</span></span>
<span><span>enum</span> <span>State</span> {</span>
<span>  <span>#[default]</span></span>
<span>  Ready,</span>
<span>  Running,</span>
<span>  Paused,</span>
<span>}</span>
<span></span>
<span>thread_local! {</span>
<span>  <span>static</span> INSTANCE: RefCell&lt;<span>Option</span>&lt;Arc&lt;SharedContext&gt;&gt;&gt; =</span>
<span>    RefCell::<span>new</span>(<span>None</span>);</span>
<span>}</span>
<span></span>
<span><span>impl</span> <span>SharedContext</span> {</span>
<span>  <span>fn</span> <span>set</span>(ctx: Arc&lt;SharedContext&gt;) {</span>
<span>    INSTANCE.<span>with</span>(|it| *it.<span>borrow_mut</span>() = <span>Some</span>(ctx));</span>
<span>  }</span>
<span></span>
<span>  <span>fn</span> <span>get</span>() <span>-&gt;</span> <span>Option</span>&lt;Arc&lt;SharedContext&gt;&gt; {</span>
<span>    INSTANCE.<span>with</span>(|it| it.<span>borrow</span>().<span>clone</span>())</span>
<span>  }</span>
<span></span>
<span>  <span>fn</span> <span>pause</span>(&amp;<span>self</span>) {</span>
<span>    <span>let</span> <span>mut </span><span>guard</span> = <span>self</span>.state.<span>lock</span>().<span>unwrap</span>();</span>
<span>    <span>assert_eq!</span>(*guard, State::Running);</span>
<span>    *guard = State::Paused;</span>
<span>    <span>self</span>.cv.<span>notify_all</span>();</span>
<span>    guard = <span>self</span></span>
<span>      .cv</span>
<span>      .<span>wait_while</span>(guard, |state| *state == State::Paused)</span>
<span>      .<span>unwrap</span>();</span>
<span>    <span>assert_eq!</span>(*guard, State::Running)</span>
<span>  }</span>
<span>}</span>
<span></span>
<span><span>pub</span> <span>struct</span> <span>ManagedHandle</span>&lt;<span>'scope</span>, T&gt; {</span>
<span>  inner: std::thread::ScopedJoinHandle&lt;<span>'scope</span>, ()&gt;,</span>
<span>  sender: mpsc::Sender&lt;<span>Box</span>&lt;<span>dyn</span> <span>FnOnce</span>(&amp;<span>mut</span> T) + <span>'scope</span> + <span>Send</span>&gt;&gt;,</span>
<span>  ctx: Arc&lt;SharedContext&gt;,</span>
<span>}</span>
<span></span>
<span><span>pub</span> <span>fn</span> <span>spawn</span>&lt;<span>'scope</span>, T: <span>'scope</span> + <span>Send</span>&gt;(</span>
<span>  scope: &amp;<span>'scope</span> Scope&lt;<span>'scope</span>, <span>'_</span>&gt;,</span>
<span>  <span>mut</span> state: T,</span>
<span>) <span>-&gt;</span> ManagedHandle&lt;<span>'scope</span>, T&gt; {</span>
<span>  <span>let</span> <span>ctx</span>: Arc&lt;SharedContext&gt; = <span>Default</span>::<span>default</span>();</span>
<span>  <span>let</span> (sender, receiver) =</span>
<span>    mpsc::channel::&lt;<span>Box</span>&lt;<span>dyn</span> <span>FnOnce</span>(&amp;<span>mut</span> T) + <span>'scope</span> + <span>Send</span>&gt;&gt;();</span>
<span>  <span>let</span> <span>inner</span> = scope.<span>spawn</span>({</span>
<span>    <span>let</span> <span>ctx</span> = Arc::<span>clone</span>(&amp;ctx);</span>
<span>    <span>move</span> || {</span>
<span>      SharedContext::<span>set</span>(Arc::<span>clone</span>(&amp;ctx));</span>
<span>      <span>for</span> <span>f</span> <span>in</span> receiver {</span>
<span>        <span>f</span>(&amp;<span>mut</span> state);</span>
<span>        <span>let</span> <span>mut </span><span>guard</span> = ctx.state.<span>lock</span>().<span>unwrap</span>();</span>
<span>        <span>assert_eq!</span>(*guard, State::Running);</span>
<span>        *guard = State::Ready;</span>
<span>        ctx.cv.<span>notify_all</span>()</span>
<span>      }</span>
<span>    }</span>
<span>  });</span>
<span>  ManagedHandle { inner, ctx, sender }</span>
<span>}</span>
<span></span>
<span><span>impl</span>&lt;<span>'scope</span>, T&gt; ManagedHandle&lt;<span>'scope</span>, T&gt; {</span>
<span>  <span>pub</span> <span>fn</span> <span>is_paused</span>(&amp;<span>self</span>) <span>-&gt;</span> <span>bool</span> {</span>
<span>    <span>let</span> <span>guard</span> = <span>self</span>.ctx.state.<span>lock</span>().<span>unwrap</span>();</span>
<span>    *guard == State::Paused</span>
<span>  }</span>
<span></span>
<span>  <span>pub</span> <span>fn</span> <span>unpause</span>(&amp;<span>self</span>) {</span>
<span>    <span>let</span> <span>mut </span><span>guard</span> = <span>self</span>.ctx.state.<span>lock</span>().<span>unwrap</span>();</span>
<span>    <span>assert_eq!</span>(*guard, State::Paused);</span>
<span>    *guard = State::Running;</span>
<span>    <span>self</span>.ctx.cv.<span>notify_all</span>();</span>
<span>    guard = <span>self</span></span>
<span>      .ctx</span>
<span>      .cv</span>
<span>      .<span>wait_while</span>(guard, |state| *state == State::Running)</span>
<span>      .<span>unwrap</span>();</span>
<span>  }</span>
<span></span>
<span>  <span>pub</span> <span>fn</span> <span>submit</span>&lt;F: <span>FnOnce</span>(&amp;<span>mut</span> T) + <span>Send</span> + <span>'scope</span>&gt;(&amp;<span>self</span>, f: F) {</span>
<span>    <span>let</span> <span>mut </span><span>guard</span> = <span>self</span>.ctx.state.<span>lock</span>().<span>unwrap</span>();</span>
<span>    <span>assert_eq!</span>(*guard, State::Ready);</span>
<span>    *guard = State::Running;</span>
<span>    <span>self</span>.sender.<span>send</span>(<span>Box</span>::<span>new</span>(f)).<span>unwrap</span>();</span>
<span>    guard = <span>self</span></span>
<span>      .ctx</span>
<span>      .cv</span>
<span>      .<span>wait_while</span>(guard, |state| *state == State::Running)</span>
<span>      .<span>unwrap</span>();</span>
<span>  }</span>
<span></span>
<span>  <span>pub</span> <span>fn</span> <span>join</span>(<span>self</span>) {</span>
<span>    <span>while</span> <span>self</span>.<span>is_paused</span>() {</span>
<span>      <span>self</span>.<span>unpause</span>();</span>
<span>    }</span>
<span>    <span>drop</span>(<span>self</span>.sender);</span>
<span>    <span>self</span>.inner.<span>join</span>().<span>unwrap</span>();</span>
<span>  }</span>
<span>}</span></code></pre>

</figure>
<p><span>System under test, not-exactly-atomic counter:</span></p>

<figure>


<pre><code><span><span>use</span> std::sync::atomic::Ordering::SeqCst;</span>
<span></span>
<span><span>#[cfg(test)]</span></span>
<span><span>use</span> managed_thread::AtomicU32;</span>
<span><span>#[cfg(not(test))]</span></span>
<span><span>use</span> std::sync::atomic::AtomicU32;</span>
<span></span>
<span><span>#[derive(Default)]</span></span>
<span><span>pub</span> <span>struct</span> <span>Counter</span> {</span>
<span>  value: AtomicU32,</span>
<span>}</span>
<span></span>
<span><span>impl</span> <span>Counter</span> {</span>
<span>  <span>pub</span> <span>fn</span> <span>increment</span>(&amp;<span>self</span>) {</span>
<span>    <span>let</span> <span>value</span> = <span>self</span>.value.<span>load</span>(SeqCst);</span>
<span>    <span>self</span>.value.<span>store</span>(value + <span>1</span>, SeqCst);</span>
<span>  }</span>
<span></span>
<span>  <span>pub</span> <span>fn</span> <span>get</span>(&amp;<span>self</span>) <span>-&gt;</span> <span>u32</span> {</span>
<span>    <span>self</span>.value.<span>load</span>(SeqCst)</span>
<span>  }</span>
<span>}</span></code></pre>

</figure>
<p><span>And the test itself:</span></p>

<figure>


<pre><code><span><span>#[test]</span></span>
<span><span>fn</span> <span>test_counter</span>() {</span>
<span>  arbtest::<span>arbtest</span>(|rng| {</span>
<span>    eprintln!(<span>"begin trace"</span>);</span>
<span>    <span>let</span> <span>counter</span> = Counter::<span>default</span>();</span>
<span>    <span>let</span> <span>mut </span><span>counter_model</span>: <span>u32</span> = <span>0</span>;</span>
<span></span>
<span>    std::thread::<span>scope</span>(|scope| {</span>
<span>      <span>let</span> <span>t1</span> = managed_thread::<span>spawn</span>(scope, &amp;counter);</span>
<span>      <span>let</span> <span>t2</span> = managed_thread::<span>spawn</span>(scope, &amp;counter);</span>
<span>      <span>let</span> <span>mut </span><span>threads</span> = [t1, t2];</span>
<span></span>
<span>      <span>while</span> !rng.<span>is_empty</span>() {</span>
<span>        <span>for</span> (tid, t) <span>in</span> threads.<span>iter_mut</span>().<span>enumerate</span>() {</span>
<span>          <span>if</span> rng.<span>arbitrary</span>()? {</span>
<span>            <span>if</span> t.<span>is_paused</span>() {</span>
<span>              eprintln!(<span>"{tid}: unpause"</span>);</span>
<span>              t.<span>unpause</span>()</span>
<span>            } <span>else</span> {</span>
<span>              eprintln!(<span>"{tid}: increment"</span>);</span>
<span>              t.<span>submit</span>(|c| c.<span>increment</span>());</span>
<span>              counter_model += <span>1</span>;</span>
<span>            }</span>
<span>          }</span>
<span>        }</span>
<span>      }</span>
<span></span>
<span>      <span>for</span> <span>t</span> <span>in</span> threads {</span>
<span>        t.<span>join</span>();</span>
<span>      }</span>
<span>      <span>assert_eq!</span>(counter_model, counter.<span>get</span>());</span>
<span></span>
<span>      <span>Ok</span>(())</span>
<span>    })</span>
<span>  });</span>
<span>}</span></code></pre>

</figure>
<p><span>Running it identifies a failure:</span></p>

<figure>


<pre><code><span>---- test_counter stdout ----</span>
<span>begin trace</span>
<span>0: increment</span>
<span>1: increment</span>
<span>0: unpause</span>
<span>1: unpause</span>
<span>1: unpause</span>
<span>0: unpause</span>
<span>0: unpause</span>
<span>1: unpause</span>
<span>0: unpause</span>
<span>0: increment</span>
<span>1: unpause</span>
<span>0: unpause</span>
<span>1: increment</span>
<span>0: unpause</span>
<span>0: unpause</span>
<span>1: unpause</span>
<span>0: unpause</span>
<span>thread 'test_counter' panicked at src/lib.rs:56:7:</span>
<span>assertion `left == right` failed</span>
<span>  left: 4</span>
<span> right: 3</span>
<span></span>
<span>arbtest failed!</span>
<span>    Seed: 0x4fd7ddff00000020</span></code></pre>

</figure>
<p><span>Which </span>…<span> is something we got like 5% into this article already, with normal threads! But there</span>’<span>s</span>
<span>more to this failure. First, it is reproducible. If I specify the same seed, I get the </span><em><span>exact</span></em><span> same</span>
<span>interleaving:</span></p>

<figure>


<pre><code><span><span>#[test]</span></span>
<span><span>fn</span> <span>test_counter</span>() {</span>
<span>  arbtest::<span>arbtest</span>(|rng| {</span>
<span>    eprintln!(<span>"begin trace"</span>);</span>
<span>    ...</span>
<span>  })</span>
<span>    .<span>seed</span>(<span>0x71aafcd900000020</span>);</span>
<span>}</span></code></pre>

</figure>
<p><span>And this is completely machine independent! If </span><em><span>you</span></em><span> specify this seed, you</span>’<span>ll get exact same</span>
<span>interleaving. So, if I am having trouble debugging this, I can DM you this hex in Zulip, and</span>
<span>you</span>’<span>ll be able to help out!</span></p>
<p><span>But there</span>’<span>s more </span>—<span> we don</span>’<span>t need to debug this failure, we can minimize it!</span></p>

<figure>


<pre><code><span><span>#[test]</span></span>
<span><span>fn</span> <span>test_counter</span>() {</span>
<span>  arbtest::<span>arbtest</span>(|rng| {</span>
<span>    eprintln!(<span>"begin trace"</span>);</span>
<span>    ...</span>
<span>  })</span>
<span>    .<span>seed</span>(<span>0x71aafcd900000020</span>)</span>
<span>    .<span>minimize</span>();</span>
<span>}</span></code></pre>

</figure>
<p><span>This gives me the following minimization trace:</span></p>

<figure>


<pre><code><span>begin trace</span>
<span>0: increment</span>
<span>1: increment</span>
<span>0: unpause</span>
<span>1: unpause</span>
<span>1: unpause</span>
<span>0: unpause</span>
<span>0: unpause</span>
<span>1: unpause</span>
<span>0: unpause</span>
<span>0: increment</span>
<span>1: unpause</span>
<span>0: unpause</span>
<span>1: increment</span>
<span>0: unpause</span>
<span>0: unpause</span>
<span>1: unpause</span>
<span>0: unpause</span>
<span>seed 0x4fd7ddff00000020, seed size 32, search time 106.00ns</span>
<span></span>
<span>begin trace</span>
<span>0: increment</span>
<span>1: increment</span>
<span>0: unpause</span>
<span>0: unpause</span>
<span>1: unpause</span>
<span>0: unpause</span>
<span>1: unpause</span>
<span>0: unpause</span>
<span>1: unpause</span>
<span>1: unpause</span>
<span>1: increment</span>
<span>seed 0x540c0c1c00000010, seed size 16, search time 282.16µs</span>
<span></span>
<span>begin trace</span>
<span>0: increment</span>
<span>1: increment</span>
<span>0: unpause</span>
<span>1: unpause</span>
<span>1: unpause</span>
<span>1: unpause</span>
<span>seed 0x084ca71200000008, seed size 8, search time 805.74µs</span>
<span></span>
<span>begin trace</span>
<span>0: increment</span>
<span>1: increment</span>
<span>0: unpause</span>
<span>1: unpause</span>
<span>seed 0x5699b19400000004, seed size 4, search time 1.44ms</span>
<span></span>
<span>begin trace</span>
<span>0: increment</span>
<span>1: increment</span>
<span>0: unpause</span>
<span>1: unpause</span>
<span>seed 0x4bb0ea5c00000002, seed size 2, search time 4.03ms</span>
<span></span>
<span>begin trace</span>
<span>0: increment</span>
<span>1: increment</span>
<span>0: unpause</span>
<span>1: unpause</span>
<span>seed 0x9c2a13a600000001, seed size 1, search time 4.31ms</span>
<span></span>
<span>minimized</span>
<span>seed 0x9c2a13a600000001, seed size 1, search time 100.03ms</span></code></pre>

</figure>
<p><span>That is, we ended up with this tiny, minimal example:</span></p>

<figure>


<pre><code><span><span>#[test]</span></span>
<span><span>fn</span> <span>test_counter</span>() {</span>
<span>  arbtest::<span>arbtest</span>(|rng| {</span>
<span>    eprintln!(<span>"begin trace"</span>);</span>
<span>    ...</span>
<span>  })</span>
<span>    .<span>seed</span>(<span>0x9c2a13a600000001</span>);</span>
<span>}</span></code></pre>

</figure>

<figure>


<pre><code><span>begin trace</span>
<span>0: increment</span>
<span>1: increment</span>
<span>0: unpause</span>
<span>1: unpause</span></code></pre>

</figure>
<p><span>And </span><em><span>this</span></em><span> is how you properly test concurrent data structures.</span></p>
</section>
<section id="Postscript">

    <h2>
    <a href="#Postscript"><span>Postscript</span> </a>
    </h2>
<p><span>Of course, this is just a toy. But you can see some ways to extend it. For example, right now our</span>
<code>AtomicU32</code><span> just delegates to the real one. But what you </span><em><span>could</span></em><span> do instead is, for each atomic, to</span>
<span>maintain a set of values written and, on read, return an </span><em><span>arbitrary</span></em><span> written value consistent with a</span>
<span>weak memory model.</span></p>
<p><span>You could also be smarter with exploring interleavings. Instead of interleaving threads at random,</span>
<span>like we do here, you can try to apply model checking approaches and prove that you have considered</span>
<span>all meaningfully different interleavings.</span></p>
<p><span>Or you can apply the approach from </span><a href="https://matklad.github.io/2021/11/07/generate-all-the-things.html"><em><span>Generate All The</span>
<span>Things</span></em></a><span> and exhaustively</span>
<span>enumerate </span><em><span>all</span></em><span> interleavings for up to, say, five increments. In fact, why don</span>’<span>t we just do this?</span></p>
<p><code>$ cargo add exhaustigen</code></p>

<figure>


<pre><code><span><span>#[test]</span></span>
<span><span>fn</span> <span>exhaustytest</span>() {</span>
<span>  <span>let</span> <span>mut </span><span>g</span> = exhaustigen::Gen::<span>new</span>();</span>
<span>  <span>let</span> <span>mut </span><span>interleavings_count</span> = <span>0</span>;</span>
<span></span>
<span>  <span>while</span> !g.<span>done</span>() {</span>
<span>    interleavings_count += <span>1</span>;</span>
<span>    <span>let</span> <span>counter</span> = Counter::<span>default</span>();</span>
<span>    <span>let</span> <span>mut </span><span>counter_model</span>: <span>u32</span> = <span>0</span>;</span>
<span></span>
<span>    <span>let</span> <span>increment_count</span> = g.<span>gen</span>(<span>5</span>) <span>as</span> <span>u32</span>;</span>
<span>    std::thread::<span>scope</span>(|scope| {</span>
<span>      <span>let</span> <span>t1</span> = managed_thread::<span>spawn</span>(scope, &amp;counter);</span>
<span>      <span>let</span> <span>t2</span> = managed_thread::<span>spawn</span>(scope, &amp;counter);</span>
<span></span>
<span>      <span>'outer</span>: <span>while</span> t1.<span>is_paused</span>()</span>
<span>        || t2.<span>is_paused</span>()</span>
<span>        || counter_model &lt; increment_count</span>
<span>      {</span>
<span>        <span>for</span> <span>t</span> <span>in</span> [&amp;t1, &amp;t2] {</span>
<span>          <span>if</span> g.<span>flip</span>() {</span>
<span>            <span>if</span> t.<span>is_paused</span>() {</span>
<span>              t.<span>unpause</span>();</span>
<span>              <span>continue</span> <span>'outer</span>;</span>
<span>            }</span>
<span>            <span>if</span> counter_model &lt; increment_count {</span>
<span>              t.<span>submit</span>(|c| c.<span>increment</span>());</span>
<span>              counter_model += <span>1</span>;</span>
<span>              <span>continue</span> <span>'outer</span>;</span>
<span>            }</span>
<span>          }</span>
<span>        }</span>
<span>        <span>return</span> <span>for</span> <span>t</span> <span>in</span> [t1, t2] {</span>
<span>          t.<span>join</span>()</span>
<span>        };</span>
<span>      }</span>
<span></span>
<span>      <span>assert_eq!</span>(counter_model, counter.<span>get</span>());</span>
<span>    });</span>
<span>  }</span>
<span>  eprintln!(<span>"interleavings_count = {:?}"</span>, interleavings_count);</span>
<span>}</span></code></pre>

</figure>
<p><span>The shape of the test is more or less the same, except that we need to make sure that there are no</span>
“<span>dummy</span>”<span> iterations, and that we always either unpause a thread or submit an increment.</span></p>
<p><span>It finds the same bug, naturally:</span></p>

<figure>


<pre><code><span>thread 'exhaustytest' panicked at src/lib.rs:103:7:</span>
<span>assertion `left == right` failed</span>
<span>  left: 2</span>
<span> right: 1</span></code></pre>

</figure>
<p><span>But the cool thing is, if we fix the issue by using atomic increment, </span>…</p>

<figure>


<pre><code><span><span>impl</span> <span>AtomicU32</span> {</span>
<span>  <span>pub</span> <span>fn</span> <span>fetch_add</span>(</span>
<span>    &amp;<span>self</span>,</span>
<span>    value: <span>u32</span>,</span>
<span>    ordering: Ordering,</span>
<span>  ) <span>-&gt;</span> <span>u32</span> {</span>
<span>    <span>pause</span>();</span>
<span>    <span>let</span> <span>result</span> = <span>self</span>.inner.<span>fetch_add</span>(value, ordering);</span>
<span>    <span>pause</span>();</span>
<span>    result</span>
<span>  }</span>
<span>}</span>
<span></span>
<span><span>impl</span> <span>Counter</span> {</span>
<span>  <span>pub</span> <span>fn</span> <span>increment</span>(&amp;<span>self</span>) {</span>
<span>    <span>self</span>.value.<span>fetch_add</span>(<span>1</span>, SeqCst);</span>
<span>  }</span>
<span>}</span></code></pre>

</figure>
<p>…<span> we can get a rather specific correctness statements out of our test, that </span><em><span>any</span></em><span> sequence of at</span>
<span>most five increments is correct:</span></p>

<figure>


<pre><code><span><span>$</span> t cargo t -r -- exhaustytest --nocapture</span>
<span><span>running 1 test</span></span>
<span><span>all 81133 interleavings are fine!</span></span>
<span><span>test exhaustytest ... ok</span></span>
<span><span></span></span>
<span><span>real 8.65s</span></span>
<span><span>cpu  8.16s (2.22s user + 5.94s sys)</span></span>
<span><span>rss  63.91mb</span></span></code></pre>

</figure>
<p><span>And the last small thing. Recall that our PBT minimized the first sequence it found </span>…<span>:</span></p>

<figure>


<pre><code><span>begin trace</span>
<span>0: increment</span>
<span>1: increment</span>
<span>0: unpause</span>
<span>1: unpause</span>
<span>1: unpause</span>
<span>0: unpause</span>
<span>0: unpause</span>
<span>1: unpause</span>
<span>0: unpause</span>
<span>0: increment</span>
<span>1: unpause</span>
<span>0: unpause</span>
<span>1: increment</span>
<span>0: unpause</span>
<span>0: unpause</span>
<span>1: unpause</span>
<span>0: unpause</span>
<span>thread 'test_counter' panicked at src/lib.rs:56:7:</span>
<span>assertion `left == right` failed</span>
<span>  left: 4</span>
<span> right: 3</span>
<span></span>
<span>arbtest failed!</span>
<span>    Seed: 0x4fd7ddff00000020</span></code></pre>

</figure>
<p>…<span> down to just</span></p>

<figure>


<pre><code><span>begin trace</span>
<span>0: increment</span>
<span>1: increment</span>
<span>0: unpause</span>
<span>1: unpause</span>
<span>thread 'test_counter' panicked at src/lib.rs:57:7:</span>
<span>assertion `left == right` failed</span>
<span>  left: 2</span>
<span> right: 1</span>
<span></span>
<span>arbtest failed!</span>
<span>    Seed: 0x9c2a13a600000001</span></code></pre>

</figure>
<p><span>But we never implemented shrinking! How is this possible? Well, strictly speaking, this is out of</span>
<span>scope for this post. And I</span>’<span>ve already described this</span>
<a href="https://tigerbeetle.com/blog/2023-03-28-random-fuzzy-thoughts"><span>elsewhere</span></a><span>. And, at 32k, this is the</span>
<span>third-longest post on this blog. And it</span>’<span>s 3AM here in Lisbon right now. But of course I</span>’<span>ll explain!</span></p>
<p><span>The trick is the simplified </span><a href="https://hypothesis.works/articles/compositional-shrinking/"><span>hypothesis</span>
<span>approach</span></a><span>. The</span>
<a href="https://docs.rs/arbtest/latest/arbtest/"><span>arbtest</span></a><span> PBT library we in this post is based on a</span>
<span>familiar interface of a PRNG:</span></p>

<figure>


<pre><code><span>arbtest::<span>arbtest</span>(|rng| {</span>
<span>  <span>let</span> <span>random_int</span>: <span>usize</span> = rng.<span>int_in_range</span>(<span>0</span>..=<span>100</span>)?;</span>
<span>  <span>let</span> <span>random_bool</span>: <span>bool</span> = rng.<span>arbitrary</span>()?;</span>
<span>  <span>Ok</span>(())</span>
<span>});</span></code></pre>

</figure>
<p><span>But there</span>’<span>s a twist! This is a </span><em><span>finite</span></em><span> PRNG. So, if you ask it to flip a coin it can give you</span>
<span>heads. And next time it might give you tails. But if you continue asking it for more, at some point</span>
<span>it</span>’<span>ll give you </span><span><code>Err(OutOfEntropy)</code><span>.</span></span></p>
<p><span>That</span>’<span>s why all these </span><code>?</code><span> and the outer loop of</span>
<span><code>while !rng.is_empty() {</code><span>.</span></span></p>
<p><span>In other words, as soon as the test runs out of entropy, it short-circuits and completes. And that</span>
<span>means that by reducing the amount of entropy available the test becomes shorter, and this works</span>
<span>irrespective of how complex is the logic inside the test!</span></p>
<p><span>And </span>“<span>entropy</span>”<span> is a big scary word here, what actually happens is that the PRNG is just an </span><code>&amp;mut
&amp;[u8]</code><span> inside. That is, a slice of random bytes, which is shortened every time you ask for a random</span>
<span>number. And the shorter the initial slice, the simpler the test gets. Minimization can be this</span>
<span>simple!</span></p>
<p><span>You can find source code for this article at</span>
<a href="https://github.com/matklad/properly-concurrent">https://github.com/matklad/properly-concurrent</a></p>
</section>
</article>
  </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Apple okays Epic Games marketplace app in Europe (149 pts)]]></title>
            <link>https://www.reuters.com/technology/epic-games-says-apple-stalling-launch-its-game-store-europe-2024-07-05/</link>
            <guid>40888461</guid>
            <pubDate>Sat, 06 Jul 2024 06:07:15 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.reuters.com/technology/epic-games-says-apple-stalling-launch-its-game-store-europe-2024-07-05/">https://www.reuters.com/technology/epic-games-says-apple-stalling-launch-its-game-store-europe-2024-07-05/</a>, See on <a href="https://news.ycombinator.com/item?id=40888461">Hacker News</a></p>
Couldn't get https://www.reuters.com/technology/epic-games-says-apple-stalling-launch-its-game-store-europe-2024-07-05/: Error: Request failed with status code 401]]></description>
        </item>
        <item>
            <title><![CDATA[Build and train GPT-2 from scratch using PyTorch (135 pts)]]></title>
            <link>https://differ.blog/p/here-s-how-you-can-build-and-train-gpt-2-from-scratch-using-pytorch-ace4ba</link>
            <guid>40888090</guid>
            <pubDate>Sat, 06 Jul 2024 04:03:03 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://differ.blog/p/here-s-how-you-can-build-and-train-gpt-2-from-scratch-using-pytorch-ace4ba">https://differ.blog/p/here-s-how-you-can-build-and-train-gpt-2-from-scratch-using-pytorch-ace4ba</a>, See on <a href="https://news.ycombinator.com/item?id=40888090">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>Are you tired of always using ChatGPT and curious about how to build your own language model? Well, you’re in the right place! Today, we’re going to create GPT-2&nbsp;, a powerful language model developed by OpenAI, from scratch that can generate human-like text by predicting the next word in a sequence.</p>
<p>To dive deeper into the theory and architecture of GPT-2, I highly recommend reading <a rel="noopener noreferrer nofollow ugc" href="https://jalammar.github.io/illustrated-gpt2/">The Illustrated GPT-2</a> by Jay Alammar. This article provides an excellent visual and intuitive explanation of GPT-2 and its inner workings. I’ll be referring to some of the visuals from the article to explain things better.</p>
<blockquote>
<p>I have tried to make this as simpler as possible. Anyone with any level of Python or machine learning can follow along and build the model.</p>
</blockquote>
<h3>Resources</h3>
<p>This project will take you through all the steps for building a simple GPT-2 model and train on bunch of Taylor Swift and Ed Sheeran songs. We’ll see what it will come up at the end&nbsp;:).</p>
<p>The dataset and source codes for this article will be available in <a rel="noopener noreferrer nofollow ugc" href="https://medium.com/r?url=https%3A%2F%2Fgithub.com%2Fajeetkharel%2Fgpt2-from-scratch">Github</a>.</p>
<blockquote>
<p>I’ll also add a Jupyter Notebook which replicates this article so you can follow along with running code and understanding side-by-side.</p>
</blockquote>
<h3>Building GPT-2 Architecture</h3>
<p>We will take this project step-by-step by continuously improving a bare-bone model and adding layers based on the original <a rel="noopener noreferrer nofollow ugc" href="https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf">GPT-2</a> implementation.</p>
<p>Here are the steps we will follow:</p>
<ol>
<li><strong>Building a custom Tokenizer</strong></li>
<li><strong>Building a Data Loader</strong></li>
<li><strong>Train a simple language model</strong></li>
<li><strong>Implement GPT-2 architecture (part 2)</strong> <a rel="noopener noreferrer nofollow ugc" href="https://medium.com/@mramitkharel/heres-how-you-can-build-and-train-gpt-2-from-scratch-using-pytorch-part-2-9b41d15baf62">🔗</a></li>
</ol>
<p>This project is divided into two parts, the first one goes through the basics of language modelling and <a rel="noopener noreferrer nofollow ugc" href="https://medium.com/@mramitkharel/heres-how-you-can-build-and-train-gpt-2-from-scratch-using-pytorch-part-2-9b41d15baf62">Part 2</a> jumps straight into GPT-2 implementation. I suggest you to follow along with the article and build it yourself which makes learning GPT-2 more interesting and fun.</p>
<blockquote>
<p>Note: This whole project will be done in a single python file so it will be easy for you to follow along block by block.</p>
</blockquote>
<p><strong>Final Model:</strong></p>
<p><img src="https://cdn-images-1.medium.com/max/800/1*35AaHBa5imxVIbbjByIE2Q.png" alt=""></p>
<p><strong>Final Model output:</strong></p>
<blockquote>
<p>Your summer has a matter likely you trying
I wish you would call
Oh-oh,
I'll be a lot of everyoneI just walked
You're sorry"Your standing in love out,
And something would wait forever bring 'Don't you think about the storyIf you're perfectly
I want your beautiful
You had sneak for you make me
This ain't think that it wanted you this enough for lonely thing
It's a duchess and I did nothin' home was no head
Oh, but you left me
Was all the less pair of the applause
Honey, he owns me now
But've looks for us?"
If I see you'll be alright
You understand, a out of theWait for me I can't call
Everything
Oh, no words don't read about me
You should've been so
You're doing what you so tired,
If you, you got perfect fall</p>
</blockquote>
<p>Like the song? Then let’s get building..</p>
<h3><strong>1. Building a custom Tokenizer</strong></h3>
<p>Language models don’t see text like us. Instead they recognize sequence of numbers as tokens of specific text. So, the first step is to import our data and build our own character level Tokenizer.</p>
<pre><code>data_dir = <span>"data.txt"</span>
text = <span>open</span>(data_dir, <span>'r'</span>).<span>read</span>() # load all the data <span>as</span> simple string

# <span>Get</span> all unique characters <span>in</span> the text <span>as</span> vocabulary
chars = <span>list</span>(<span>set</span>(text))
vocab_size = <span>len</span>(chars)
</code></pre>
<p>Example:</p>
<p><img src="https://cdn-images-1.medium.com/max/800/1*34WkqssQKHKpdO1yTH0n-g.png" alt=""></p>
<p>If you see the output above, we have a list of all unique characters extracted from the text data in the initialization process. Character tokenization is basically using the index position of characters from the vocabulary and mapping it to corresponding character in the input text.</p>
<pre><code># build the character level tokenizer
chr_to_idx = {<span>c</span>:i <span>for</span> i, c <span>in</span> <span>enumerate</span>(chars)}
idx_to_chr = {<span>i</span>:c <span>for</span> i, c <span>in</span> <span>enumerate</span>(chars)}

def <span>encode</span>(<span>input_text</span>: str) -&gt; list[int]:
    <span>return</span> [chr_to_idx[t] <span>for</span> t <span>in</span> input_text]

def <span>decode</span>(<span>input_tokens</span>: list[int]) -&gt; <span>str</span>:
    <span>return</span> <span>""</span>.<span>join</span>([idx_to_chr[i] <span>for</span> i <span>in</span> input_tokens])
</code></pre>
<p>Example:</p>
<p><img src="https://cdn-images-1.medium.com/max/800/1*adFjPqc2Ks1MY2uXuB9DGQ.png" alt=""></p>
<p>Convert our text data into tokens:</p>
<p><strong>Installation</strong>:</p>
<p><code>pip install torch</code></p>
<p><strong>Code</strong>:</p>
<pre><code><span>import</span> torch
# use cpu or gpu based on your system
device = <span>"cpu"</span>
<span>if</span> torch.<span>cuda</span>.<span>is_available</span>():
    device = <span>"cuda"</span>

# convert our text data into tokenized tensor
data = torch.<span>tensor</span>(<span>encode</span>(text), dtyppe=torch.<span>long</span>, device=device)
</code></pre>
<p>Now, we have the tokenized tensor <code>data</code> where each characters in the text is converted to the respective tokens.</p>
<p><strong>So far:</strong></p>
<pre><code><span>import</span> torch

data_dir = <span>"data.txt"</span>
text = <span>open</span>(data_dir, <span>'r'</span>).<span>read</span>() # load all the data <span>as</span> simple string

# <span>Get</span> all unique characters <span>in</span> the text <span>as</span> vocabulary
chars = <span>list</span>(<span>set</span>(text))
vocab_size = <span>len</span>(chars)

# build the character level tokenizer
chr_to_idx = {<span>c</span>:i <span>for</span> i, c <span>in</span> <span>enumerate</span>(chars)}
idx_to_chr = {<span>i</span>:c <span>for</span> i, c <span>in</span> <span>enumerate</span>(chars)}

def <span>encode</span>(<span>input_text</span>: str) -&gt; list[int]:
    <span>return</span> [chr_to_idx[t] <span>for</span> t <span>in</span> input_text]

def <span>decode</span>(<span>input_tokens</span>: list[int]) -&gt; <span>str</span>:
    <span>return</span> <span>""</span>.<span>join</span>([idx_to_chr[i] <span>for</span> i <span>in</span> input_tokens])


# convert our text data into tokenized tensor
data = torch.<span>tensor</span>(<span>encode</span>(text), dtyppe=torch.<span>long</span>, device=device)
</code></pre>
<h3><strong>2. Building a Data&nbsp;Loader</strong></h3>
<p>Now, before building our model, we have to define how we are going to feed the data into the model for training and what the data looks like in terms of dimensions and batch size.</p>
<p>Let’s define our data loader as below:</p>
<pre><code>train_batch_size = <span>16</span>  # training batch size
eval_batch_size = <span>8</span>  # evaluation batch size
context_length = <span>256</span>  # number <span>of</span> tokens processed <span>in</span> a single batch
train_split = <span>0.8</span>  # percentage <span>of</span> data to use <span>from</span> total data <span>for</span> training

# split data into trian and <span>eval</span>
n_data = <span>len</span>(data)
train_data = data[:<span>int</span>(n_data * train_split)]
eval_data = data[<span>int</span>(n_data * train_split):]


<span>class</span> <span>DataLoader</span>:
    def <span>__init__</span>(self, tokens, batch_size, context_length) -&gt; <span>None</span>:
        self.<span>tokens</span> = tokens
        self.<span>batch_size</span> = batch_size
        self.<span>context_length</span> = context_length

        self.<span>current_position</span> = <span>0</span>

    def <span>get_batch</span>(self) -&gt; torch.<span>tensor</span>:
        b, c = self.<span>batch_size</span>, self.<span>context_length</span>

        start_pos = self.<span>current_position</span>
        end_pos = self.<span>current_position</span> + b * c + <span>1</span>

        # <span>if</span> the batch exceeds total length, get the data till last token
        # and take remaining <span>from</span> starting token to avoid always excluding some data
        add_data = -<span>1</span> # n, <span>if</span> length exceeds and we need <span>`n`</span> additional tokens <span>from</span> start
        <span>if</span> end_pos &gt; <span>len</span>(self.<span>tokens</span>):
            add_data = end_pos - <span>len</span>(self.<span>tokens</span>) - <span>1</span>
            end_pos = <span>len</span>(self.<span>tokens</span>) - <span>1</span>

        d = self.<span>tokens</span>[<span>start_pos</span>:end_pos]
        <span>if</span> add_data != -<span>1</span>:
            d = torch.<span>cat</span>([d, self.<span>tokens</span>[:add_data]])
        x = (d[:-<span>1</span>]).<span>view</span>(b, c)  # inputs
        y = (d[<span>1</span>:]).<span>view</span>(b, c)  # targets

        self.<span>current_position</span> += b * c # set the next position
        <span>return</span> x, y

train_loader = <span>DataLoader</span>(train_data, train_batch_size, context_length)
eval_loader = <span>DataLoader</span>(eval_data, eval_batch_size, context_length)
</code></pre>
<p>Example:</p>
<p><img src="https://cdn-images-1.medium.com/max/800/1*GMpC_jFxFpk_1xK19YvbrA.png" alt=""></p>
<p>Now we have our own customized data loader for both training and evaluation. The loader has a <code>get_batch</code> function which returns batches of <code>batch_size * context_length</code>.</p>
<p>If you are wondering why <code>x</code> is from <code>start</code> to <code>end</code> and <code>y</code> is from <code>start+1</code> to <code>end+1</code>, it’s because the main task for this model will be to predict next sequence given the previous. So there will be an extra token in <code>y</code> for it to predict the (n+1) token given last n tokens of <code>x</code>. If it sounds complicated look at the below visual:</p>
<p><img src="https://cdn-images-1.medium.com/max/800/0*jTrSzRD-KGPs3v5E.gif" alt=""><em>Figure 2: GPT-2 Input &amp; Output flow from “The Illustrated GPT-2” by Jay&nbsp;Alammar.</em></p>
<h3><strong>3. Train a simple language&nbsp;model</strong></h3>
<p>Now we are ready to build and train a simple language model using the data we have just loaded.</p>
<p>For this section, we will keep it very simple and implement a simple Bi-Gram Model where given the last token predict the next token. As you can see below we will be using just the Embedding layer while ignoring the main decoder block.</p>
<p>An Embedding layer represents <code>n = d_model</code> unique properties of all the characters in our vocabulary and based on which the layer pops out the property using the token index or in our case the index of our character in the vocabulary.</p>
<p>You will be amazed how well the model will behave just by using the Embeddings. And we will be improving the model step by step by adding more layers, so sit tight and follow along.</p>
<p><img src="https://cdn-images-1.medium.com/max/800/1*9cYT2nBANRzBr3vqQVLBsw.png" alt=""></p>
<p><strong>Initialization</strong>:</p>
<h2>used to define size of embeddings</h2>
<p>d_model = vocab_size </p>
<p>The embedding dimension or <code>d_model</code> is <code>vocab_size</code> currently because the final output has to map to the logits for each character in vocab to calculate their probabilities. Later on we will introduce a <code>Linear</code> layer which will map <code>d_model</code> to <code>vocab_size</code> and then we can have a custom embedding_dimension.</p>
<p><strong>Model</strong>:</p>
<pre><code><span>import</span> torch.<span>nn</span> <span>as</span> nn
<span>import</span> torch.<span>nn</span>.<span>functional</span> <span>as</span> F

<span>class</span> <span>GPT</span>(nn.<span>Module</span>):
    def <span>__init__</span>(self, vocab_size, d_model):
        <span>super</span>().<span>__init__</span>()
        self.<span>wte</span> = nn.<span>Embedding</span>(vocab_size, d_model) # word token embeddings
    
    def <span>forward</span>(self, inputs, targets = <span>None</span>):
        logits = self.<span>wte</span>(inputs) # dim -&gt; batch_size, sequence_length, d_model
        loss = <span>None</span>
        <span>if</span> targets != <span>None</span>:
            batch_size, sequence_length, d_model = logits.<span>shape</span>
            # to calculate loss <span>for</span> all token embeddings <span>in</span> a batch
            # kind <span>of</span> a requirement <span>for</span> cross_entropy
            logits = logits.<span>view</span>(batch_size * sequence_length, d_model)
            targets = targets.<span>view</span>(batch_size * sequence_length)
            loss = F.<span>cross_entropy</span>(logits, targets)
        <span>return</span> logits, loss
    
    def <span>generate</span>(self, inputs, max_new_tokens):
        # <span>this</span> will store the model outputs along <span>with</span> the initial input sequence
        # make a copy so that it doesn<span>'t interfare with model 
        for _ in range(max_new_tokens):
            # we only pass targets on training to calculate loss
            logits, _ = self(inputs)  
            # for all the batches, get the embeds for last predicted sequence
            logits = logits[:, -1, :] 
            probs = F.softmax(logits, dim=1)            
            # get the probable token based on the input probs
            idx_next = torch.multinomial(probs, num_samples=1) 
            
            inputs = torch.cat([inputs, idx_next], dim=1)
        # as the inputs has all model outputs + initial inputs, we can use it as final output
        return inputs

m = GPT(vocab_size=vocab_size, d_model=d_model).to(device)
</span></code></pre>
<p>We have now successfully defined our model with just one <code>Embedding</code> layer and <code>Softmax</code> for token generation. Let’s see how our model behaves when given some input characters.</p>
<p><img src="https://cdn-images-1.medium.com/max/800/1*dQpkxEARkvXbpJpI7dCRYA.png" alt=""></p>
<p>😄 Pretty interesting!! But we are not quite there yet.</p>
<p>Now the final step is to train our model and give it some knowledge about the characters. Let’s setup our optimizer. We will use a simple <code>AdamW</code> optimizer for now with <code>0.001</code> learning rate. We will go through improving the optimization in later sections.</p>
<pre><code>lr = <span>1e-3</span>
optim = torch.<span>optim</span>.<span>AdamW</span>(m.<span>parameters</span>(), lr=lr)
<span>Below</span> is a very simple training loop.
epochs = <span>5000</span>
eval_steps = <span>1000</span> # perform evaluation <span>in</span> every n steps
<span>for</span> ep <span>in</span> <span>range</span>(epochs):
    xb, yb = train_loader.<span>get_batch</span>()

    logits, loss = <span>m</span>(xb, yb)
    optim.<span>zero_grad</span>(set_to_none=<span>True</span>)
    loss.<span>backward</span>()
    optim.<span>step</span>()

    <span>if</span> ep % eval_steps == <span>0</span> or ep == epochs-<span>1</span>:
        m.<span>eval</span>()
        <span>with</span> torch.<span>no_grad</span>():
            xvb, yvb = eval_loader.<span>get_batch</span>()
            _, e_loss = <span>m</span>(xvb, yvb)

            <span>print</span>(f<span>"Epoch: {ep}tlr: {lr}ttrain_loss: {loss}teval_loss: {e_loss}"</span>)
        m.<span>train</span>() # back to training mode
</code></pre>
<p>Let’s run:</p>
<p><img src="https://cdn-images-1.medium.com/max/800/1*ikOrVlB0KHzrTWTpOLi9Lw.png" alt=""></p>
<p>So we got a pretty good loss result. But we are not there yet. As you can see, the error decreased by a higher amount until epoch 2000 and not much improvements afterwards. It’s because the model doesn’t yet have much brain power (or layers/neural networks) and it’s just comparing embedding of one character with another.</p>
<p>The output now looks like below:</p>
<p><img src="https://cdn-images-1.medium.com/max/800/1*fEEJXUZrhAIORdD0tXk_wA.png" alt=""></p>
<p>😮 OK!! Not very pleasing but definitely some improvements than the first generation which was without any training (Obviously). The model is starting to know how the songs are formatted and the lines and everything which is pretty impressive.</p>
<p>Now, as this article is getting too longer, I will add rest of the sections in the Part 2 below:</p>

<p>Thanks for reading the article. I hope you learned something new. If you have any questions/feedback, feel free to leave a comment.</p>
<h3>References</h3>
<p><em>Automatic Arabic Poem Generation with GPT-2 — Scientific Figure on ResearchGate. Available from:</em> <a rel="noopener noreferrer nofollow ugc" href="https://www.researchgate.net/figure/GPT-2-architecture-Heilbron-et-al-2019_fig1_358654229"><em>https://www.researchgate.net/figure/GPT-2-architecture-Heilbron-et-al-2019_fig1_358654229</em></a></p>
<p><em>Alammar, J (2018). The Illustrated GPT-2 [Blog post]. Retrieved from</em> <a rel="noopener noreferrer nofollow ugc" href="https://jalammar.github.io/illustrated-gpt2/">https://jalammar.github.io/illustrated-gpt2/</a></p></div></div>]]></description>
        </item>
    </channel>
</rss>