<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0">
    <channel>
        <title>HN100 - Readable Contents</title>
        <link>https://hn.algolia.com/api/v1/search_by_date?tags=%28story,poll%29&amp;numericFilters=points%3E100</link>
        <description>Uses Readability to add bodies to the RSS feed</description>
        <lastBuildDate>Wed, 17 Jan 2024 16:00:27 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>en</language>
        <item>
            <title><![CDATA[Why I Like Obsidian (157 pts)]]></title>
            <link>https://www.ddanieltan.com/posts/obsidian/</link>
            <guid>39027154</guid>
            <pubDate>Wed, 17 Jan 2024 13:11:27 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.ddanieltan.com/posts/obsidian/">https://www.ddanieltan.com/posts/obsidian/</a>, See on <a href="https://news.ycombinator.com/item?id=39027154">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="quarto-content">
<!-- sidebar -->
  

<!-- margin-sidebar -->
    
<!-- main -->
<main id="quarto-document-content">



<section id="the-start-of-my-obsidian-journey">
<h2>The start of my <a href="https://obsidian.md/">Obsidian</a> journey</h2>
<p>21 January 2021, that’s the date I created in my first note in my <a href="https://obsidian.md/">Obsidian</a> vault. Back then, it was just an experiment in my ongoing quest to find the perfect note-taking tool. <a href="https://evernote.com/">Evernote</a>, <a href="https://roamresearch.com/">Roam Research</a>, and even the good ol’ <a href="https://www.zebrapen.com/blogs/enlightened-writing/4-reasons-to-keep-writing-with-a-pen-and-paper">pen and paper</a> took their turns on my desk. But ever since I started using Obsidian, I never used another tool again. Since that day, Obsidian has been my constant companion through learning adventures, career changes, study sessions, and moments of personal reflection. What started as a simple app transformed into an indispensable tool, the backstage manager of my life, keeping everything in check.</p>
</section>
<section id="obsidians-features">
<h2>Obsidian’s Features</h2>
<p>What really makes Obsidian unique? I think it boils down to 3 features:</p>
<section id="obsidian-allows-structure-to-grow-organically">
<h2 data-anchor-id="obsidian-allows-structure-to-grow-organically">1. Obsidian allows structure to grow organically</h2>
<p>Most note-taking tools require you to decide upon the structure of your note before you record the note. For example, say I am taking notes while going through one of deeplearning.ai’s excellent <a href="https://www.deeplearning.ai/courses/generative-ai-with-llms/">Generative AI LLM courses</a>, before writing down my notes, I might have to decide on my folder structure.</p>
<p>Perhaps I should structure my notes by their topic</p>
<pre><code>├── Instruction Fine Tuning
│&nbsp;&nbsp; ├── Single Task
│&nbsp;&nbsp; └── Multi Task
├── Parameter Efficient Fine Tuning (PEFT)
├── Low Rank Adaption of LLMs (LoRA)
├── Model Evaluation Metrics
 &nbsp;&nbsp; └── LLM benchmarks</code></pre>
<p>Or perhaps I should structure my notes based by date so I can track my learning progress over time</p>
<pre><code>├── 2024-01-01
├── 2024-01-02
│&nbsp;&nbsp; └── Instruction Fine Tuning
├── 2024-01-03
│&nbsp;&nbsp; └── Parameter Efficient Fine Tuning (PEFT)
├── 2024-01-04
│&nbsp;&nbsp; └── Low Rank Adaption of LLMs (LoRA)
└── 2024-01-05</code></pre>
<p>Either way could be valid, depending on how I plan to revise and reorganise my notes in the future. But having to decide upon a structure upfront before I get the chance to start recording my notes never feels good.</p>
<section id="internal-links">
<h3 data-anchor-id="internal-links">Internal Links</h3>
<p>Obsidian offers a powerful feature to solve this problem - <a href="https://help.obsidian.md/Linking+notes+and+files/Internal+links">Internal Links</a>. Here is an example of Internal Links in action: <img src="https://www.ddanieltan.com/posts/obsidian/internal-link-example.png"></p>
<p>By wrapping any idea/noun/topic in double <code>[[]]</code> brackets Obsidian creates a dedicated new notes for that item, as well as the connection between the note I am currently taking and the one that was just created. In this short paragraph, I have linked both a date <code>2024-01-02</code> and several key topics e.g.&nbsp;(<code>In-context learning</code>), which means that when I want to revisit or reorganise my notes, I can do so either by topic or by date.</p>

<div><p>With internal links, I enjoy keeping my notes in a flat hierachy but I have seen vaults that still prefer folders. Obsidian supports both preferences equally well 🙂</p></div><p>This flexibility enables me to build my vault out with an organic structure that adapts to my learning needs.</p>
</section>
</section>
<section id="obsidian-helps-me-link-atomic-ideas-into-greater-understanding">
<h2 data-anchor-id="obsidian-helps-me-link-atomic-ideas-into-greater-understanding">2. Obsidian helps me link atomic ideas into greater understanding</h2>
<p>Knowledge is a mosaic crafted from interconnected ideas.</p>
<div>
<ul role="tablist"><li role="presentation"><a id="tabset-1-1-tab" data-bs-toggle="tab" data-bs-target="#tabset-1-1" role="tab" aria-controls="tabset-1-1" aria-selected="true">What my vault started as</a></li><li role="presentation"><a id="tabset-1-2-tab" data-bs-toggle="tab" data-bs-target="#tabset-1-2" role="tab" aria-controls="tabset-1-2" aria-selected="false">What my vault grew into</a></li></ul>
<div>
<div id="tabset-1-1" role="tabpanel" aria-labelledby="tabset-1-1-tab">
<p><img src="https://www.ddanieltan.com/posts/obsidian/graph1.png"></p>
<p>Here is the <a href="https://help.obsidian.md/Plugins/Graph+view">graph view</a> of my Obsidian vault in my 1st year. Every node in this graph is a single note. It is clear that when I was first starting out, I was writing down disparate, unconnected notes. I was probably writing down a lot of interesting ideas, but I had not taken the time to connect these ideas to gain a broader understanding.</p>
</div>
<div id="tabset-1-2" role="tabpanel" aria-labelledby="tabset-1-2-tab">
<p><img src="https://www.ddanieltan.com/posts/obsidian/graph2.png"></p>
<p>As I continued using Obsidian, I started to link notes together. This could be done either when the notew was first created, or when I scheduled time to review these notes again. Soon, I saw clusters of ideas organically forming into broader topics. My <a href="https://www.ddanieltan.com/posts/2022scala/">deep dive into Scala</a>, my <a href="https://www.ddanieltan.com/posts/2023-causal-inference/">exploration of Causal Inference</a> and a whole variety of subjects that interest me came out of these clusters. And today, I have a nice graphical topology of sorts that represents my learning journey and keeps me motivated.</p>
</div>
</div>
</div>
</section>
<section id="obsidians-choice-to-work-with-plain-text-files-make-it-future-proof">
<h2 data-anchor-id="obsidians-choice-to-work-with-plain-text-files-make-it-future-proof">3. Obsidian’s choice to work with plain text files make it future-proof</h2>
<p>Lastly, my favourite feature of Obsidian is that by deafault, it chooses to work with plain text files. This simple decision means that Obsidian notes:</p>
<ul>
<li>Can be used offline</li>
<li>Can be editted with any text editor</li>
<li>Can be viewed with a variety of text readers</li>
<li>Can be easily synced on iCloud, Dropbox or using git</li>
<li>Is yours, forever!</li>
</ul>
<p>Obsidian’s CEO Steph Ango wrote a <a href="https://stephango.com/file-over-app">dedicated blog post on this philosophy</a> that went deeper into this philosophy. He shared that <em>all software is ephemeral and Obsidian wants to give people ownership over their own data</em>, which is an approach that builds trust for its users.</p>
<!--
Adapted from Tom Mock's blog: https://github.com/jthomasmock/themockup-blog/blob/master/_R/footer.qmd
-->
</section>

</section>

<div id="quarto-appendix"><section id="appendix"><h2>Appendix</h2><div>

<div id="callout-1">
<pre><code>─ Session info ───────────────────────────────────────────────────────────────
 setting  value
 version  R version 4.3.2 (2023-10-31)
 os       macOS Sonoma 14.1.2
 system   x86_64, darwin20
 ui       X11
 language (EN)
 collate  en_US.UTF-8
 ctype    en_US.UTF-8
 tz       Asia/Singapore
 date     2024-01-17
 pandoc   3.1.10 @ /usr/local/bin/ (via rmarkdown)
 quarto   1.3.450

─ Packages ───────────────────────────────────────────────────────────────────
 package     * version date (UTC) lib source
 sessioninfo * 1.2.2   2021-12-06 [1] CRAN (R 4.3.0)

 [1] /Users/ddanieltan/Code/ddanieltan.com/renv/library/R-4.3/x86_64-apple-darwin20
 [2] /Users/ddanieltan/Library/Caches/org.R-project.R/R/renv/sandbox/R-4.3/x86_64-apple-darwin20/84ba8b13

──────────────────────────────────────────────────────────────────────────────</code></pre>
</div>
<blockquote>
The statistician cannot evade the responsibility for understanding the process he applies or recommends – Ronald Fisher
</blockquote>


</div></section><section><h2>Reuse</h2></section><section><h2>Citation</h2><div><p>BibTeX citation:</p><pre><code>@online{tan2024,
  author = {Tan, Daniel},
  title = {Why {I} Like {Obsidian}},
  date = {2024-01-17},
  url = {https://www.ddanieltan.com/posts/obsidian},
  langid = {en}
}
</code></pre><p>For attribution, please cite this work as:</p><div id="ref-tan2024" role="listitem"><p>
Tan, Daniel. 2024. <span>“Why I Like Obsidian.”</span> January 17, 2024.
<a href="https://www.ddanieltan.com/posts/obsidian">https://www.ddanieltan.com/posts/obsidian</a>.
</p></div></div></section></div></main> <!-- /main -->

</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[The Rust project has a burnout problem (164 pts)]]></title>
            <link>https://jyn.dev/2024/01/16/the-rust-project-has-a-burnout-problem.html</link>
            <guid>39026855</guid>
            <pubDate>Wed, 17 Jan 2024 12:34:49 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://jyn.dev/2024/01/16/the-rust-project-has-a-burnout-problem.html">https://jyn.dev/2024/01/16/the-rust-project-has-a-burnout-problem.html</a>, See on <a href="https://news.ycombinator.com/item?id=39026855">Hacker News</a></p>
<div id="readability-page-1" class="page"><div aria-label="Content">
        <article itemscope="" itemtype="http://schema.org/BlogPosting">

  

  <div itemprop="articleBody">
    <p><img src="https://jyn.dev/assets/burned%20out%20rust%20club.png" alt="a melting, smiling, ferris. it's surrounded by the cursive text &quot;burned out rust kid club&quot;."></p>

<p>the number of people who have left the rust project due to burnout is shockingly high. the number of people in the project who are close to burnout is also shockingly high.</p>

<p>this post is about myself, but it’s not just about myself. i’m not going to name names because either you know what i’m talking about, in which case you know <em>at least</em> five people matching this description, or you don’t, in which case sorry but you’re not the target audience. consider, though, that the project has been around for 15 years, and compare that to the average time a maintainer has been active …</p>

<h2 id="what-does-this-look-like">what does this look like</h2>

<p>(i apologize in advance if this story does not match your experience; hopefully the suggestions on what to do about burnout will still be helpful to you.)</p>

<p>the pattern usually goes something like this:</p>
<ul>
  <li>you want to work on rust. you go to look at the issue tracker. you find something <em>you</em> care about, since the easy/mentored issues are taken. it’s hard to find a mentor because all the experienced people are overworked and burned out, so you end up doing a lot of the work independently.</li>
</ul>

<p>guess what you’ve already learned at this point: work in this project doesn’t happen unless <em>you personally</em> drive it forward. that issue you fixed was opened for years; the majority of issues you will work on as you start will have been open for months.</p>

<ul>
  <li>you become a more active contributor. the existing maintainer is too burned out to do regular triage, so you end up going through the issue backlog (usually, you’re the first person to have done so in years). this reinforces the belief work doesn’t happen unless <em>you</em> do it <em>personally</em>.</li>
  <li>the existing maintainer recognizes your work and turns over a lot of the responsibilities to you, especially reviews. new contributors make PRs. they make silly simple mistakes due to lack of experience; you point them out and they get fixed. this can be fun, for a time. what it’s teaching you is that <em>you personally</em> are responsible for catching mistakes.</li>
  <li>you get tired. you’ve been doing this for a while. people keep making the same mistakes, and you’re afraid to trust other reviewers; perhaps you’re the <em>only</em> reviewer, or other reviewers have let things slip before and you don’t trust their judgement as much as you used to. perhaps you’re assigned too many PRs and you can’t keep up. you haven’t worked on the things you <em>want</em> to work on in weeks, and no one else is working on them because you said you were going to (“they won’t happen unless <em>you do them personally</em>”, a voice says). you want a break, but you have a voice in the back of your head: “the project would be worse without you”.</li>
</ul>

<p>i’m going to stop here; i think everyone gets the idea.</p>

<h2 id="what-can-i-do-about-it">what can i do about it</h2>

<p>“it won’t get done if i don’t do it” and “i need to review everything or stuff will slip through” is exactly the mindset of my own burnout from rust. it doesn’t matter if it’s true, it will cause you pain. if the project cannot survive without <em>you personally</em> putting in unpaid overtime, perhaps it does not deserve to survive.</p>

<p>if you are paid to work on rust, you likely started as an unpaid contributor and got the job later. <em>treat it like a job now</em>. do not work overtime; do not volunteer at every turn; do not work on things far outside your job description.</p>

<p>the best way to help the project is to keep contributing for it for years. to do that, you have to avoid burning out, which means you have to <em>treat yourself well</em>.</p>

<h2 id="what-can-team-leads-do-about-it">what can team leads do about it</h2>

<p>have documentation for “what to do about burnout”; give it just as much priority as technical issues or moderation conflicts.</p>

<p>rotate responsibilities. don’t have the same person assigned to the majority of PRs. if they review other people’s PRs unsolicited, talk to them 1-1 about why they feel the need to do so. if someone is assigned to the review queue and never reviews PRs, talk to them; take them off the queue; give them a vacation or different responsibilities as appropriate.</p>

<p>ask people why they leave. i know at least one person whose burnout story does not match the one in this post. i am sure there are others. you cannot solve a problem if you don’t understand what causes it.</p>

<p><em>take these problems seriously</em>. <a href="https://jyn.dev/2023/12/04/How-to-maintain-an-open-source-project.html">prioritize growing the team and creating a healthy environment over solving technical issues</a>. <strong>the issues will still be there in a few months; your people may not be</strong>.</p>

<h2 id="what-can-the-rust-project-do-about-it">what can the rust project do about it</h2>

<p>one thing bothering me as i wrote this post is how much of this still falls on individuals within the project. i don’t think this is an individual problem; i think it is a cultural, organizational, and resource problem. i may write more about this once i have concrete ideas about what the project could do.</p>

<h2 id="be-well-be-kind-to-each-other-i-love-you">be well. be kind to each other. i love you.</h2>

<p>remember:</p>

<blockquote><p lang="en" dir="ltr">EMPATHY WITHOUT BOUNDARIES IS SELF DESTRUCTION <a href="https://t.co/HbBwEj4hc3">pic.twitter.com/HbBwEj4hc3</a></p>— 𖤐ARCH BUDZAR𖤐 (@ArchBudzar) <a href="https://twitter.com/ArchBudzar/status/1313572660048269315?ref_src=twsrc%5Etfw">October 6, 2020</a></blockquote>


<h3 id="acknowledgements">acknowledgements</h3>

<p>thank you <strong>@QuietMisdreavus</strong> for the <em>burned out rust kid club</em> art.</p>

<p>thank you <strong>@Gankra</strong>, <strong>@QuietMisdreavus</strong>, <strong>@alercah</strong>, <strong>@ManishEarth</strong>, <strong>@estebank</strong>, <strong>@workingjubilee</strong> and <strong>@yaahc</strong> for discussion and feedback on early drafts of this post. any errors are my own.</p>

  </div>

  
</article>

      </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Willow Protocol (171 pts)]]></title>
            <link>https://willowprotocol.org/</link>
            <guid>39026791</guid>
            <pubDate>Wed, 17 Jan 2024 12:27:50 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://willowprotocol.org/">https://willowprotocol.org/</a>, See on <a href="https://news.ycombinator.com/item?id=39026791">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><h2 id="willow"><a href="#willow"><img src="https://willowprotocol.org/emblem.png" alt="A Willow emblem: a stylised drawing of a Willow’s branch tipping into a water surface, next to a hand-lettered display of the word &quot;Willow&quot;."></a></h2><p>A protocol for peer-to-peer data stores. The best parts? Fine-grained permissions, a keen approach to privacy, destructive edits, and a dainty bandwidth and memory footprint.</p><nav><ul><li><a href="https://willowprotocol.org/more/why/index.html#why_willow">Why did we make Willow?</a></li><li><a href="https://willowprotocol.org/specs/data-model/index.html#data_model">If you read only one specification, let it be this one</a></li><li><a href="https://willowprotocol.org/specs/index.html#specifications">All of the specifications</a></li></ul></nav><p><span><img src="https://willowprotocol.org/assets/4cd25e0cf24e98306961263f67eaf4bb8559d7dd78a809ec30f46f1246df5a31.png" alt="An anthropomorphic computer smiles and shrugs while a series of comical connectivity issues threaten its ethernet cable: a mouse nibbles through the cable, an axe chops it up, and an anvil falls toward it at high velocity."></span>Data storage which never goes offline. You get always-available storage for arbitrary data (e.g. text, media). You can have as many of these stores as you want, keyed to different namespaces. When stores from different devices belong to the same namespace, they deterministically sync with each other.</p><p><span><img src="https://willowprotocol.org/assets/8c8fd1d157e9ac8852da067d53fca38a81a1861c28741e25c843154ba62f56e6.png" alt="A cartoonish troll tries to spy on a person enjoying themselves with a paper airplane, but a solid brick wall blocks the troll’s line of sight. The trool is deeply unhappy about this circumstance."></span>Private and end-to-end encrypted. Other users can't find out what you’re interested in unless they already know about it themselves. And if they get that far, they still have to be able to decrypt synced data to make any sense of it.</p><p><span><img src="https://willowprotocol.org/assets/b9eae2757e3e084c2bacd38c17889546b0103ebee170a5bbc36ad34af485f784.png" alt="Three stylised paper files hang off a tree branch. The branch is being cut off near its base by a pair of hedge clippers, in a way that all files will be pruned of the tree."></span>Total erasure of data. Distributed systems use tombstones to communicate deletes, but even these leave metadata behind. Prefix pruning deletes many entries and all of their metadata in their entirety, leaving a single tombstone in their place.</p><p><span><img src="https://willowprotocol.org/assets/62e12e8abea68a8ed9e5b499c8b2a341b3f226886083eb43827260ac0be0b7b7.png" alt="Two stylised admission tickets. One says &quot;Admin&quot;, the other says &quot;Aug 1st to Sep 3rd&quot;."></span>Fine grained capabilities. Restrict read and write access by semantically meaningful ranges of data, or time range. Use your favourite existing capability system, or try our Meadowcap system.</p><p><span><img src="https://willowprotocol.org/assets/d51b7ffdeb5e8fdc67bff2c3c5ff0634a8f436a9c13c15edad718e8e23fc116e.png" alt="A cake with a single slice being removed. The selected slice has a strawberry on top. Hmm, strawberry cake..."></span>Partial sync. Have a lot of data, but don't want to sync the whole thing to a particular device? Choose which data to replicate by what, when, or who.</p><p><span><img src="https://willowprotocol.org/assets/18ff2330446d72c3599af5d81ea5ade441394228cb42a2c38882e147f37d7159.png" alt="A pencil overwriting a sequence of bits (zeros and ones), leaving no trace of the overwritten bits."></span>Destructive edits. When you update a value, the old values and associated metadata are overwritten.</p><p><span><img src="https://willowprotocol.org/assets/ce762771ec9b6d6173130de34873d79e7c08928a5abd3a7d90962ef043913993.png" alt="A cartoon foot cartoonishly kicking a cartoon file out of a cartoon door."></span>Locally delete data you don’t want to store, even if it was authored by someone else.</p><p><span><img src="https://willowprotocol.org/assets/920e71ff29fac0aa9c7991e33c533768796a62dd1e3556854d3b51a48aa70a89.png" alt="Five ants carry zeros and ones off to the right. The numbers are about as large as the hard-working insects."></span>Peers can communicate resource budgets, so devices with very limited memory can sync too.</p><p><span><img src="https://willowprotocol.org/assets/50235dcc48ae3058df05671ca72154afd9f7e335bb3bac4bf18e26ab27b05fcd.png" alt="The pronoun &quot;I&quot;, followed by a heart, followed by two crossed-out names of hash functions (&quot;MD5&quot; and &quot;SHA256), followed by the hash function of choice: &quot;BLAKE3&quot;."></span>You choose the transport and cryptographic primitives suited to your use-case.</p><p><span><img src="https://willowprotocol.org/assets/fc24ed9df75d9ecd70ba971a4ca686af30b8a3eb48831a57e5b49f960f59ed84.png" alt="A happy little smiley face holding a laptop in one hand and a phone in the other hand. Yay."></span>Authors can write from multiple devices concurrently. Yay.</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Flowblade: Open-Source Video Editor (195 pts)]]></title>
            <link>https://jliljebl.github.io/flowblade/</link>
            <guid>39023565</guid>
            <pubDate>Wed, 17 Jan 2024 05:14:51 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://jliljebl.github.io/flowblade/">https://jliljebl.github.io/flowblade/</a>, See on <a href="https://news.ycombinator.com/item?id=39023565">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>

<p>
User quotes
</p>

<p>
"I found flowblade and I love it.  I edit movies 2 to 3 hours in length with many video clips.  What is a great help is the way the clips snap together, especially when I am deleting and editing out bad video and moving video around.", dnsXXXXX
</p>

<p>
"We - that is a group of German students and me - are currently shooting a movie for a school project, and after trying a few solutions we ended
up choosing Flowblade.",".. From then it worked well - no more crashes! A lot of filters I
actually thought flowblade was simply lacking are available now. Thank you for your support - Flowblade rocks!", R.W, Germany
</p>

<p>
"I'm a professional video editor and director who is working from more than 15 years for the main broadcasters here in Italy...[Flowblade is] already showing it's great potential because it looks like it has been thought by someone who knows what editors need for real.", M.S, Italy
</p>

</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[BBC BASIC raytracer in 432 characters (152 pts)]]></title>
            <link>https://mastodon.me.uk/@bbcmicrobot/111762132859648345</link>
            <guid>39023056</guid>
            <pubDate>Wed, 17 Jan 2024 03:53:50 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://mastodon.me.uk/@bbcmicrobot/111762132859648345">https://mastodon.me.uk/@bbcmicrobot/111762132859648345</a>, See on <a href="https://news.ycombinator.com/item?id=39023056">Hacker News</a></p>
&lt;Unparsable&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[Show HN: I made a website to find best bus seat to avoid the sun while traveling (686 pts)]]></title>
            <link>https://sitinshade.com</link>
            <guid>39022693</guid>
            <pubDate>Wed, 17 Jan 2024 02:59:17 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://sitinshade.com">https://sitinshade.com</a>, See on <a href="https://news.ycombinator.com/item?id=39022693">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
    
      <p> Find Best Bus Seat to Minimize Sun Exposure While Traveling</p>
      
      
      
        <div>
          <div>
            <p><label>Date</label></p>
          </div>
          
          <div>
            <p><label>Time</label></p>
          </div>
        </div>
        
        
        
        <div>
            <p><label>Timezone</label></p><div>
              <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512">
              <path d="M233.4 406.6c12.5 12.5 32.8 12.5 45.3 0l192-192c12.5-12.5 12.5-32.8 0-45.3s-32.8-12.5-45.3 0L256 338.7 86.6 169.4c-12.5-12.5-32.8-12.5-45.3 0s-12.5 32.8 0 45.3l192 192z"></path>
            </svg>
          </div>
        </div>

    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Browsh: The modern text-based web browser (134 pts)]]></title>
            <link>https://www.brow.sh/</link>
            <guid>39022535</guid>
            <pubDate>Wed, 17 Jan 2024 02:38:51 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.brow.sh/">https://www.brow.sh/</a>, See on <a href="https://news.ycombinator.com/item?id=39022535">Hacker News</a></p>
<div id="readability-page-1" class="page"><div itemprop="text" id="main" role="main">
      <div>
        <p><strong>Browsh is a fully-modern text-based browser.</strong> It renders
          anything that a modern browser can; HTML5, CSS3, JS, video and even WebGL.
          Its main purpose is to be run on a remote server and accessed via SSH/Mosh
          or the in-browser HTML service in order to significantly reduce bandwidth and thus both
          increase browsing speeds and decrease bandwidth costs.
        </p>
        <p><img src="https://www.brow.sh/assets/images/browsh-tty.jpg" alt="browsh in the tty">
        </p>
        <p><img src="https://www.brow.sh/assets/images/browsh-html.jpg" alt="browsh in the browser">
        </p>
      </div>


      

      <div>
        <div>
          <h2>Download (v1.8.0)</h2><p>
          Browsh is available as a single static binary on all major platforms.
          The only dependency is a recent 57+ version of Firefox.
          <br>
          <a href="https://www.brow.sh/downloads">Latest version</a> |
          <a href="https://github.com/browsh-org/browsh/releases">Releases archive</a>
          <br>
          <small>
          A Docker image is also
          available: <br> <code>docker run -it browsh/browsh</code>
          </small>
        </p></div>
        <div>
          <h2>Live SSH Demo</h2>
          <p><strong>Temporarily offline</strong></p><p>
            Just point your SSH client to <em>brow.sh</em>, eg;
            <code>ssh brow.sh</code>. No auth needed. The service
            is for demonstration only, sessions last 5 minutes and are logged.<br>
            <small>
              Note that SSH is actually a very inefficient protocol, for best results install
              Browsh on your own server along with <a href="https://mosh.org/">Mosh</a>.
            </small>
          </p>
        </div>
        <div>
          <h2>In-browser Services</h2>
          <p><strong>Temporarily offline</strong></p><div>
            <ul>
              <li>
                <a href="https://html.brow.sh/" target="_blank">html.brow.sh</a>
                Uses very basic graphics and HTML anchor tags.
                <small>Although this service may appear similar to the terminal client it does not yet have feature parity.</small>
              </li>
              <li>
                <a href="https://text.brow.sh/" target="_blank">text.brow.sh</a> Uses nothing but pure text,
                better for usage with <code>curl</code>, for instance.
              </li>
            </ul>
          </div>
        </div>
      </div>

      

      <div>
        <p id="youtube-showcase">
          <iframe src="https://www.youtube.com/embed/zqAoBD62gvo" allow="autoplay; encrypted-media" allowfullscreen=""></iframe>
        </p>

        <div id="open-blurb">
          <h2>Donate</h2><p>
          Browsh is currently maintained and funded by
          <a href="http://tombh.co.uk/" target="_blank">one person</a>.
          If you'd like to see Browsh continue to help those with slow and/or
          expensive Internet, please consider <a href="https://www.brow.sh/donate">donating</a>.
          </p>
        </div>
      </div>
    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Judge blocks JetBlue from acquiring Spirit Airlines (126 pts)]]></title>
            <link>https://www.nytimes.com/2024/01/16/business/jetblue-spirit-airlines-ruling-merger.html</link>
            <guid>39022274</guid>
            <pubDate>Wed, 17 Jan 2024 02:09:43 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.nytimes.com/2024/01/16/business/jetblue-spirit-airlines-ruling-merger.html">https://www.nytimes.com/2024/01/16/business/jetblue-spirit-airlines-ruling-merger.html</a>, See on <a href="https://news.ycombinator.com/item?id=39022274">Hacker News</a></p>
Couldn't get https://www.nytimes.com/2024/01/16/business/jetblue-spirit-airlines-ruling-merger.html: Error: Request failed with status code 403]]></description>
        </item>
        <item>
            <title><![CDATA[OpenAI drops ban on military tools to partner with The Pentagon (318 pts)]]></title>
            <link>https://www.semafor.com/article/01/16/2024/openai-is-working-with-the-pentagon-on-cybersecurity-projects</link>
            <guid>39020778</guid>
            <pubDate>Tue, 16 Jan 2024 23:37:54 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.semafor.com/article/01/16/2024/openai-is-working-with-the-pentagon-on-cybersecurity-projects">https://www.semafor.com/article/01/16/2024/openai-is-working-with-the-pentagon-on-cybersecurity-projects</a>, See on <a href="https://news.ycombinator.com/item?id=39020778">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><div><div><p><img alt="icon" loading="lazy" width="26" height="18" decoding="async" data-nimg="1" src="https://www.semafor.com/_next/static/media/2circles.337bd7ed.svg"></p><h3>Semafor Signals</h3></div><div><p>Insights from Wired, The Wall Street Journal, and The Information</p><p><img alt="Arrow Down" loading="lazy" width="49" height="49" decoding="async" data-nimg="1" src="https://www.semafor.com/_next/static/media/blue-arrow-down.388f0335.svg"></p></div></div><div><div><p><img src="https://www.semafor.com/_next/static/media/thenews@2x.f322bda2.png" alt="Title icon" width="20" height="16"></p><h3>The News</h3></div><div><p>OpenAI is working with the Pentagon on software projects, including ones related to cybersecurity, the company said Tuesday, in a dramatic change from its previous ban on providing its artificial intelligence technology to militaries.</p><p>The ChatGPT creator is also in discussions with the U.S. government about developing tools to reduce veteran suicides, Anna Makanju, the company’s vice president of global affairs, said at the World Economic Forum — but added that it will retain its ban on developing weapons.</p><p>Last week, OpenAI removed language in its usage policy that would ban its AI from being used in “military and warfare” applications, sparking <a href="https://theintercept.com/2024/01/12/open-ai-military-ban-chatgpt/?utm_source=substack&amp;utm_medium=email" rel="no-referrer">alarm among AI safety advocates.</a></p></div></div><div><div><p><img alt="icon" loading="lazy" width="20" height="16" decoding="async" data-nimg="1" src="https://www.semafor.com/_next/static/media/2circles-black.0f46bdfb.svg"></p><h3>SIGNALS</h3></div><p><strong>Semafor Signals:</strong> Global insights on today's biggest stories.</p><div><h3>Silicon Valley has changed its mind about collaborating with the Pentagon</h3><div><p>Wired</p><!-- --><p>, </p><!-- --><p>The Wall Street Journal</p><!-- --><p>, </p><!-- --><p>Semafor’s Technology Editor Reed Albergotti</p></div><p>Silicon Valley has softened its stance on collaborating with the U.S. military in recent years. In 2018, thousands of Google employees protested a Pentagon project, fearing technology they developed could be used for lethal purposes. That proved to be the <a href="https://www.wired.com/story/3-years-maven-uproar-google-warms-pentagon/" rel="noopener" target="_blank">high water mark of Silicon Valley opposition</a> to the Department of Defense, with Google since earning hundreds of millions from its defense contracts. The Pentagon has made <a href="https://www.wsj.com/amp/articles/pentagon-woos-silicon-valley-to-join-ranks-of-arms-makers-38b1d4c0" rel="noopener" target="_blank">a concerted effort</a> in recent years to win over Silicon Valley startups in order to develop new weapons technology and integrate advanced tools into the department’s operations. U.S.-China tensions and Russia’s war in Ukraine have also served to dispel many of the qualms entrepreneurs once had about military collaboration. “What’s emerged lately is a kind of <a href="https://www.semafor.com/article/01/03/2024/defense-tech-is-having-its-moment-in-silicon-valley" rel="noopener" target="_blank">techno-patriotism in Silicon Valley</a>,” wrote Semafor’s technology editor Reed Albergotti.</p></div><div><h3>AI may remake the military, but could come with profound risks</h3><div><p>Wired</p><!-- --><p>, </p><!-- --><p>Vox</p><!-- --><p>, </p><!-- --><p>Foreign Policy</p></div><p>Defense experts have been bullish about the impact AI will have on the military. Former Google CEO Eric Schmidt, now a prominent defense industry figure, has compared the arrival of AI to the <a href="https://www.wired.com/story/eric-schmidt-is-building-the-perfect-ai-war-fighting-machine/#:~:text=%E2%80%9CEinstein%20wrote%20a%20letter%20to,distributed%20systems%20are%20that%20powerful.%E2%80%9D" rel="noopener" target="_blank">advent of nuclear weapons</a>, Wired reported. “Einstein wrote a letter to Roosevelt in the 1930s saying that there is this new technology — nuclear weapons — that could change war, which it clearly did. I would argue that [AI-powered] autonomy and decentralized, distributed systems are that powerful,” Schmidt said. But advocacy groups have warned that integrating AI into warfare could come with profound risks given AI’s tendency to “hallucinate” — make up fake information and pass it off as real — which could have far higher stakes if AI-powered systema wwew integrated into <a href="https://www.vox.com/future-perfect/2023/11/28/23972547/the-militarized-ai-risk-thats-bigger-than-killer-robots" rel="noopener" target="_blank">command and control systems</a>. The Arms Control Association has warned that the rush to “​​exploit emerging technologies for military use has accelerated at a much faster pace than efforts to assess <a href="https://foreignpolicy.com/2023/04/11/ai-arms-race-artificial-intelligence-chatgpt-military-technology/" rel="noopener" target="_blank">the dangers they pose</a>.”</p></div><div><h3>OpenAI rules are unclear about scope of possible military deals</h3><div><p>The Information</p><!-- --><p>, </p><!-- --><p>Euromaidan Press</p><!-- --><p>, </p><!-- --><p>The Economist</p></div><p>Although OpenAI has ruled out developing weapons, its new policy would likely allow it to provide <a href="https://www.theinformation.com/articles/chatgpt-coming-to-an-army-near-you" rel="noopener" target="_blank">AI software to the Department of Defense</a> for uses such as helping analysts interpret data or write code, The Information reported. But as the war in Ukraine has shown, the divide between data crunching and warfare may not be as clear-cut as OpenAI would like. Ukraine has developed and imported software <a href="https://euromaidanpress.com/2023/08/09/ground-and-marine-drones-ai-and-javelin-training-simulator-how-ukrainian-military-technology-advancements-bring-country-closer-to-victory/" rel="noopener" target="_blank">to analyze large data</a>, which has allowed its artillery operators to be rapidly notified of Russian targets in the area and dramatically speed up <a href="https://www.economist.com/special-report/2023/07/03/the-war-in-ukraine-shows-how-technology-is-changing-the-battlefield" rel="noopener" target="_blank">the pace at which they can fire</a>. Meanwhile, The Information warned that the change in policy could be enough to reignite the debate over AI safety at OpenAI that contributed to Sam Altman’s brief firing as CEO.</p></div></div><div><p><img src="https://www.semafor.com/_next/static/media/semafor-logo-small.cc0a7c9c.svg" width="19" height="19" alt="Semafor Logo"></p></div></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA["Amazonian dark earth" was the work of ancient humans (155 pts)]]></title>
            <link>https://www.bbc.com/future/article/20240116-the-dark-earth-revealing-the-amazons-secrets</link>
            <guid>39020600</guid>
            <pubDate>Tue, 16 Jan 2024 23:21:16 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.bbc.com/future/article/20240116-the-dark-earth-revealing-the-amazons-secrets">https://www.bbc.com/future/article/20240116-the-dark-earth-revealing-the-amazons-secrets</a>, See on <a href="https://news.ycombinator.com/item?id=39020600">Hacker News</a></p>
<div id="readability-page-1" class="page"><section id="futurearticle20240116-the-dark-earth-revealing-the-amazons-secrets"><div id="headline-futurearticle20240116-the-dark-earth-revealing-the-amazons-secrets"><picture><source media="(min-width:1200px)" srcset="https://ychef.files.bbci.co.uk/1600x900/p0h5bx6q.webp" type="image/webp"><source media="(min-width:1200px)" srcset="https://ychef.files.bbci.co.uk/1600x900/p0h5bx6q.jpg" type="image/jpeg"><source media="(min-width:880px)" srcset="https://ychef.files.bbci.co.uk/1280x720/p0h5bx6q.webp" type="image/webp"><source media="(min-width:880px)" srcset="https://ychef.files.bbci.co.uk/1280x720/p0h5bx6q.jpg" type="image/jpeg"><source media="(min-width:576px)" srcset="https://ychef.files.bbci.co.uk/976x549/p0h5bx6q.webp" type="image/webp"><source media="(min-width:576px)" srcset="https://ychef.files.bbci.co.uk/976x549/p0h5bx6q.jpg" type="image/jpeg"><source media="(min-width:224px)" srcset="https://ychef.files.bbci.co.uk/624x351/p0h5bx6q.webp" type="image/webp"><source media="(min-width:224px)" srcset="https://ychef.files.bbci.co.uk/624x351/p0h5bx6q.jpg" type="image/jpeg"><img loading="lazy" draggable="false" title="The roots of a strangler fig in the Amazon (Credit: Alamy)" src="https://ychef.files.bbci.co.uk/976x549/p0h5bx6q.jpg" alt="The roots of a strangler fig in the Amazon (Credit: Alamy)" id=""></picture></div><div><article><div><p>Amid the discovery of a lost city in the Amazon rainforest, scientists are uncovering a different kind of relic underground – one that's still being used today.</p><div><p>D</p><div><p>Deep within the Amazon, Mark Robinson was up to his knees in buried treasure.</p>
<p>Together with an international team of scientists, Robinson was on an expedition to a remote patch of forest in Iténez, northwest Bolivia, close to the border with Brazil. Getting there had not been easy. To avoid a 10-hour boat ride, they took a hair-raising flight to the nearest village, Versalles, where the plane had to circle back over a grass runway to avoid landing on a herd of grazing animals. Then came a long trek through thick rainforest, navigating over gnarled roots and past marauding armies of ants. "It's hot, it's humid, you're getting bitten constantly," says Robinson, a senior lecturer in archaeology at the University of Exeter.</p>
<p>The journey, however, was worth it. The researchers had an important mission: they were searching for "Amazonian dark earth" (ADE), sometimes known as "black gold" or terra preta.</p>
<p>This layer of charcoal-black soil, which can be up to <a href="https://onlinelibrary.wiley.com/doi/full/10.1002/gea.21839">3.8m (12.5ft) thick</a>, is <a href="https://esajournals.onlinelibrary.wiley.com/doi/10.1002/ecs2.2035">found in patches across the Amazon basin</a>. It is intensely fertile – rich in decaying organic matter and nutrients essential for growing crops, such as <a href="https://link.springer.com/article/10.1007/s001140000193">nitrogen, potassium and phosphorus</a>. But unlike the thin, sandy soils typical of the rainforest, this layer was not deposited naturally – it was the work of ancient humans.</p>
<p>This rich soil is a relic from a very different time – an era when indigenous groups formed a thriving network of settlements across this rainforest world.&nbsp;</p>
<p>In January 2024, scientists announced the rediscovery of a long-vanished "garden" city. Hidden beneath the foliage of the rainforest in Ecuador's Upano valley was <a href="https://www.science.org/doi/10.1126/science.adi6317">a 2,000 year-old urban centre</a>, complete with plazas, streets and ceremonial platforms. (<em>Read more from BBC News about </em><a href="https://www.bbc.com/news/science-environment-67940671"><em>the lost city found in the Amazon</em></a><em>.</em>) The discovery has raised questions about whether there may be other ancient settlements concealed in the Amazon. And this is where ADE comes in.&nbsp;</p>
<p>It's thought that the garden city could only support so many people because of the region's fertile volcanic soil. But elsewhere in the Amazon, indigenous communities relied on ADE to improve the productivity of their land. Now there's growing interest in the lessons their methods may hold for societies today, from improving crop yields to beating climate change.</p></div></div><div id="future/article/20240116-the-dark-earth-revealing-the-amazons-secrets-p0h5bsnz"><picture><source media="(min-width:1200px)" srcset="https://ychef.files.bbci.co.uk/1600x900/p0h5bsnz.webp" type="image/webp"><source media="(min-width:1200px)" srcset="https://ychef.files.bbci.co.uk/1600x900/p0h5bsnz.jpg" type="image/jpeg"><source media="(min-width:880px)" srcset="https://ychef.files.bbci.co.uk/1280x720/p0h5bsnz.webp" type="image/webp"><source media="(min-width:880px)" srcset="https://ychef.files.bbci.co.uk/1280x720/p0h5bsnz.jpg" type="image/jpeg"><source media="(min-width:576px)" srcset="https://ychef.files.bbci.co.uk/976x549/p0h5bsnz.webp" type="image/webp"><source media="(min-width:576px)" srcset="https://ychef.files.bbci.co.uk/976x549/p0h5bsnz.jpg" type="image/jpeg"><source media="(min-width:224px)" srcset="https://ychef.files.bbci.co.uk/624x351/p0h5bsnz.webp" type="image/webp"><source media="(min-width:224px)" srcset="https://ychef.files.bbci.co.uk/624x351/p0h5bsnz.jpg" type="image/jpeg"><img loading="lazy" draggable="false" title="Amazonian dark earth is packed with ancient artefacts, such as pottery and fossilised seeds (Credit: Mark Robinson)" src="https://ychef.files.bbci.co.uk/976x549/p0h5bsnz.jpg" alt="Amazonian dark earth is packed with ancient artefacts, such as pottery and fossilised seeds (Credit: Mark Robinson)" id=""></picture><div><p>Amazonian dark earth is packed with ancient artefacts, such as pottery and fossilised seeds (Credit: Mark Robinson)</p></div></div><div><p><strong>A hidden influence</strong></p>
<p>Surrounded by the smells and sounds of the rainforest at Versalles, in the remoteness of the Amazon, Robinson says it would be tempting to think that you're in a pristine wilderness. But this is not the case.</p>
<p>"The more we find out, [it becomes clear that] it's not necessarily primary forest," says Robinson. "Everywhere we look, although it seems like a really arduous trip to us, and that we're in the most remote place, we just find evidence of past communities everywhere."</p>
<p>In 2017, research revealed that <a href="https://www.science.org/doi/10.1126/science.aal0157#:~:text=Plants%20domesticated%20by%20pre%2DColumbian,and%20richness%20of%20domesticated%20species.">domesticated trees</a> are five times more likely to be dominant in the Amazon than non-domesticated ones – with more appearing the closer you get to ancient settlements. Though today many of the Amazon’s indigenous communities have vanished, wiped out by Western colonists and the diseases they carried, their farming practices continue to shape the rainforest.</p>
<p>Another crucial element of this hidden influence is ADE, which is widespread. "This is the fascinating thing – it really is pan-Amazonian, we are finding it everywhere," says Robinson.</p>
<p>This <a href="https://eos.org/features/the-nutrient-rich-legacy-in-the-amazons-dark-earths">precious layer</a> contains a potent <a href="https://acsess.onlinelibrary.wiley.com/doi/abs/10.2136/sssaspecpub63.2014.0035.5">blend of inorganic material</a>, including ash, pottery, bone and shells, together with organic matter such as food scraps, manure, and urine. It's simultaneously a treasure trove of ancient rubbish that's extremely exciting to archaeologists like Robinson, and a functional part of the Amazonian soil – one that continues to enrich the rainforest and allow indigenous communities to farm there today.</p></div><div id="future/article/20240116-the-dark-earth-revealing-the-amazons-secrets-p0h5bq99"><picture><source media="(min-width:1200px)" srcset="https://ychef.files.bbci.co.uk/1600x900/p0h5bq99.webp" type="image/webp"><source media="(min-width:1200px)" srcset="https://ychef.files.bbci.co.uk/1600x900/p0h5bq99.jpg" type="image/jpeg"><source media="(min-width:880px)" srcset="https://ychef.files.bbci.co.uk/1280x720/p0h5bq99.webp" type="image/webp"><source media="(min-width:880px)" srcset="https://ychef.files.bbci.co.uk/1280x720/p0h5bq99.jpg" type="image/jpeg"><source media="(min-width:576px)" srcset="https://ychef.files.bbci.co.uk/976x549/p0h5bq99.webp" type="image/webp"><source media="(min-width:576px)" srcset="https://ychef.files.bbci.co.uk/976x549/p0h5bq99.jpg" type="image/jpeg"><source media="(min-width:224px)" srcset="https://ychef.files.bbci.co.uk/624x351/p0h5bq99.webp" type="image/webp"><source media="(min-width:224px)" srcset="https://ychef.files.bbci.co.uk/624x351/p0h5bq99.jpg" type="image/jpeg"><img loading="lazy" draggable="false" title="Amazonian dark earth has been estimated to cover 0.1%-10% of the Amazon basin (Credit: Getty Images)" src="https://ychef.files.bbci.co.uk/976x549/p0h5bq99.jpg" alt="Amazonian dark earth has been estimated to cover 0.1%-10% of the Amazon basin (Credit: Getty Images)" id=""></picture><div><p>Amazonian dark earth has been estimated to cover 0.1%-10% of the Amazon basin (Credit: Getty Images)</p></div></div><div><p>"They really are a goldmine," says Robinson. Along with fossilised seeds and ceramic artefacts dating back thousands of years, there are microscopic clues to what the rainforest may have been like thousands of years ago. One example is faecal spherulites: tiny <a href="https://www.sciencedirect.com/science/article/pii/S0305440317301681">crystals found in animal dung</a> that hint at the kinds of animals that once roamed across the landscape – and defecated in it.&nbsp;</p>
<p><strong>A living history</strong></p>
<p>ADE first piqued the interest of Westerners in the 1870s, when several scientists independently noticed black layers of soil that contrasted with the <a href="https://cdn.hackaday.io/files/20931895511904/Black%20Soil%20Green%20Rice.pdf">pale or reddish kind</a> that surrounded them. One early explorer <a href="https://books.google.com/books?hl=en&amp;lr=&amp;id=xGotEAAAQBAJ&amp;oi=fnd&amp;pg=PA129&amp;dq">described it as a</a> "fine, black loam", and noted that "strewn over it everywhere we find fragments of Indian pottery, so abundant in some places that they almost cover the ground".</p>
<p>However, how ADEs were created has been something of a mystery. Scientists have questioned whether these soils were produced by accident – the product of generations of indigenous people discarding rubbish – or via an intentional process to enrich the rainforest and make its ground more suitable for farming.</p>
<p>In 2023, an international team of scientists weighed in. By combining an analysis of the structure and composition of ADEs with observations of, and interviews with, the indigenous community at Kuikuro – in the southeastern Amazon, in central Brazil – the researchers concluded that these layers of soil were indeed <a href="https://www.science.org/doi/10.1126/sciadv.adh8499">made on purpose</a>.</p></div><div id="future/article/20240116-the-dark-earth-revealing-the-amazons-secrets-p0h5bttv"><picture><source media="(min-width:1200px)" srcset="https://ychef.files.bbci.co.uk/1600x900/p0h5bttv.webp" type="image/webp"><source media="(min-width:1200px)" srcset="https://ychef.files.bbci.co.uk/1600x900/p0h5bttv.jpg" type="image/jpeg"><source media="(min-width:880px)" srcset="https://ychef.files.bbci.co.uk/1280x720/p0h5bttv.webp" type="image/webp"><source media="(min-width:880px)" srcset="https://ychef.files.bbci.co.uk/1280x720/p0h5bttv.jpg" type="image/jpeg"><source media="(min-width:576px)" srcset="https://ychef.files.bbci.co.uk/976x549/p0h5bttv.webp" type="image/webp"><source media="(min-width:576px)" srcset="https://ychef.files.bbci.co.uk/976x549/p0h5bttv.jpg" type="image/jpeg"><source media="(min-width:224px)" srcset="https://ychef.files.bbci.co.uk/624x351/p0h5bttv.webp" type="image/webp"><source media="(min-width:224px)" srcset="https://ychef.files.bbci.co.uk/624x351/p0h5bttv.jpg" type="image/jpeg"><img loading="lazy" draggable="false" title="While the Iténez rainforest is still inhabited today, much of the Amazon basin was depopulated after the arrival of Western colonists (Credit: Mark Robinson)" src="https://ychef.files.bbci.co.uk/976x549/p0h5bttv.jpg" alt="While the Iténez rainforest is still inhabited today, much of the Amazon basin was depopulated after the arrival of Western colonists (Credit: Mark Robinson)" id=""></picture><div><p>While the Iténez rainforest is still inhabited today, much of the Amazon basin was depopulated after the arrival of Western colonists (Credit: Mark Robinson)</p></div></div><div><p>The age and distribution of these soil deposits tell the story of the rise and fall of ancient indigenous civilisations across the Amazon. While the oldest layers of these black soils are around <a href="https://www.pnas.org/doi/10.1073/pnas.2022213118">5,000 years old</a>, "we see a lot more [evidence of ADEs being produced] about 4,000 years ago", says Robinson. "There's a lot more activity, a lot of cultural changes."</p>
<p>It's not until around 2,000 years ago, however, that they reach their peak, says Robinson. That's the average age of the black deposits that are found over a wide area across the Amazon basin. At this point, communities were larger and formed vast networks. However, the settlements where people produced ADE were typically not on the same scale as the recently rediscovered city in Ecuador.</p>
<p>One reason for this could be the power of ADE itself. Within the setting of an abundant jungle habitat, enriched by indigenous people with everything they need – fruiting trees and rich soil for growing crops – Robinson believes that there may have been no need for people to turn to larger-scale agriculture. "So [it's possible that] you don't really need the extra hierarchical level [that tends to develop in mass settlements]," says Robinson.</p>
<p>But by around 500 years ago, something is clearly very wrong. "That's when we really see it [ADE production] drop off," says Robinson.</p>
<p>This is thought to reflect the aftermath of Christopher Columbus' arrival on South American soil on 1 August 1498. When he plunged the red and gold flag of Spain into the ground on the Paria Peninsula in Venezuela, it marked the beginning of a "great dying". It's been estimated that 56 million indigenous people were killed across the Americas by 1600 – so many, it <a href="https://www.sciencedirect.com/science/article/pii/S0277379118307261">cooled the Earth</a>'s climate.</p></div><div id="future/article/20240116-the-dark-earth-revealing-the-amazons-secrets-p0h5byhz"><picture><source media="(min-width:1200px)" srcset="https://ychef.files.bbci.co.uk/1600x900/p0h5byhz.webp" type="image/webp"><source media="(min-width:1200px)" srcset="https://ychef.files.bbci.co.uk/1600x900/p0h5byhz.jpg" type="image/jpeg"><source media="(min-width:880px)" srcset="https://ychef.files.bbci.co.uk/1280x720/p0h5byhz.webp" type="image/webp"><source media="(min-width:880px)" srcset="https://ychef.files.bbci.co.uk/1280x720/p0h5byhz.jpg" type="image/jpeg"><source media="(min-width:576px)" srcset="https://ychef.files.bbci.co.uk/976x549/p0h5byhz.webp" type="image/webp"><source media="(min-width:576px)" srcset="https://ychef.files.bbci.co.uk/976x549/p0h5byhz.jpg" type="image/jpeg"><source media="(min-width:224px)" srcset="https://ychef.files.bbci.co.uk/624x351/p0h5byhz.webp" type="image/webp"><source media="(min-width:224px)" srcset="https://ychef.files.bbci.co.uk/624x351/p0h5byhz.jpg" type="image/jpeg"><img loading="lazy" draggable="false" title="Taking inspiration from ancient methods such as Amazonian dark earth, today many companies are producing biochar as a way to lock away carbon and improve soil (Credit: Alamy)" src="https://ychef.files.bbci.co.uk/976x549/p0h5byhz.jpg" alt="Taking inspiration from ancient methods such as Amazonian dark earth, today many companies are producing biochar as a way to lock away carbon and improve soil (Credit: Alamy)" id=""></picture><div><p>Taking inspiration from ancient methods such as Amazonian dark earth, today many companies are producing biochar as a way to lock away carbon and improve soil (Credit: Alamy)</p></div></div><div><p><strong>A carbon sink</strong></p>
<p>Though many of the ancient inhabitants of the Amazon have long since vanished, their legacy remains. Intriguingly, not all of the ADEs they left behind have the same make-up – in fact, they vary widely, depending on the specific ingredients used in different locations.</p>
<p>"But the basic mechanism for creating the soils and enriching them seems to be similar," says Robinson. "They [indigenous people] are directly investing into the soils, starting with their own waste products," he says. The base is mostly composed of food scraps, with added faeces and charcoal. And it is the latter that is attracting increasing attention.</p>
<p>It turns out that not only are ADEs extraordinarily rich in nutrients, but they are powerful carbon sinks – with up to 7.5 times more carbon within <a href="https://www.css.cornell.edu/faculty/lehmann/research/terra%20preta/terrapretamain.html">compared to the surrounding soils</a>. As ADEs accumulate, the carbon becomes trapped underground, where it remains stable for hundreds of years – locking it away and delaying its entry into the atmosphere.</p>
<p>It's not clear why the carbon within ADEs behaves this way, but scientists suspect that it has something to do with "<a href="https://www.css.cornell.edu/faculty/lehmann/research/terra%20preta/terrapretamain.html">black carbon</a>", also known as "biochar". This key ingredient is made from organic material that has been turned to almost pure carbon at high temperatures, in the presence of little oxygen. The process doesn't emit as much carbon dioxide as charcoal production, but leads to a fine, crumbly black product that has been found in ADEs across the Amazon. (<a href="https://www.bbc.com/future/article/20200206-can-charcoal-cut-cows-methane-to-fight-climate-change"><em>Read more about biochar from the BBC</em></a>).</p></div><div id="future/article/20240116-the-dark-earth-revealing-the-amazons-secrets-p0h5bs2q"><picture><source media="(min-width:1200px)" srcset="https://ychef.files.bbci.co.uk/1600x900/p0h5bs2q.webp" type="image/webp"><source media="(min-width:1200px)" srcset="https://ychef.files.bbci.co.uk/1600x900/p0h5bs2q.jpg" type="image/jpeg"><source media="(min-width:880px)" srcset="https://ychef.files.bbci.co.uk/1280x720/p0h5bs2q.webp" type="image/webp"><source media="(min-width:880px)" srcset="https://ychef.files.bbci.co.uk/1280x720/p0h5bs2q.jpg" type="image/jpeg"><source media="(min-width:576px)" srcset="https://ychef.files.bbci.co.uk/976x549/p0h5bs2q.webp" type="image/webp"><source media="(min-width:576px)" srcset="https://ychef.files.bbci.co.uk/976x549/p0h5bs2q.jpg" type="image/jpeg"><source media="(min-width:224px)" srcset="https://ychef.files.bbci.co.uk/624x351/p0h5bs2q.webp" type="image/webp"><source media="(min-width:224px)" srcset="https://ychef.files.bbci.co.uk/624x351/p0h5bs2q.jpg" type="image/jpeg"><img loading="lazy" draggable="false" title="In the village of Versalles, the indigenous Itonama community still use the fertile Amazonian dark soil to grow crops (Credit: Mark Robinson)" src="https://ychef.files.bbci.co.uk/976x549/p0h5bs2q.jpg" alt="In the village of Versalles, the indigenous Itonama community still use the fertile Amazonian dark soil to grow crops (Credit: Mark Robinson)" id=""></picture><div><p>In the village of Versalles, the indigenous Itonama community still use the fertile Amazonian dark soil to grow crops (Credit: Mark Robinson)</p></div></div><div><p>Now businesses are attempting to capitalise on this ancient method, in a quest to help farmers to improve their soil and combat climate change at the same time. Take Carbon Gold, a company that produces biochar for use as an organic, peat-free planting aid. Founded in 2007 by the creator of a chocolate brand, the company based its methods on Mayan cacao farmers from Belize – who have also been using biochar for millennia.</p>
<p>In addition to locking away carbon, "biochar improves <a href="https://chembioagro.springeropen.com/articles/10.1186/s40538-020-00204-5">structure</a>, aeration, <a href="https://www.mdpi.com/2073-4395/12/2/311">water-holding capacity</a>, and <a href="https://www.nature.com/articles/s41467-017-01123-0">nutrient retention</a>" that can support healthy plant growth, says Sue Rawlings, the managing director at Carbon Gold. Today the company's clients include organic growers, gardeners, sports stadiums, premier league football clubs, major race and golf courses and Royal Parks and gardens in the UK, she says.</p>
<p>For his part, Robinson thinks copying the methods of ancient indigenous people in the Amazon is going to be essential for future generations. He points to the predictions that by 2050, <a href="https://www.jcu.edu.au/state-of-the-tropics/why-do-the-tropics-matter">around half of the world's population</a> will live in the tropics – with large amounts of migration into tropical forests.</p>
<p>"Finding ways for communities to be more sustainable in them, I think it's essential," he says. "And there are things we can learn from the past about this. I think we're just on the cusp of understanding this."</p>
<p>--</p>
<p><em>If you liked this story,&nbsp;</em><a href="https://cloud.email.bbc.com/SignUp10_08?&amp;at_bbc_team=studios&amp;at_medium=Onsite&amp;at_objective=acquisition&amp;at_ptr_name=bbc.com&amp;at_link_origin=featuresarticle&amp;at_campaign=essentiallist&amp;at_campaign_type=owned"><em>sign up for The Essential List newsletter</em></a><em>&nbsp;– a handpicked selection of features, videos and can't-miss news delivered to your inbox every Friday.</em></p>
<p><em>Join one million Future fans by liking us on </em><a href="https://www.facebook.com/BBCFuture/"><em>Facebook</em></a><em>, or follow us on </em><a href="https://twitter.com/BBC_Future"><em>Twitter</em></a><em> or </em><a href="https://www.instagram.com/bbcfuture_official/"><em>Instagram</em></a><em>.</em></p></div></div></article></div>;</section></div>]]></description>
        </item>
        <item>
            <title><![CDATA[US developers can offer non-app store purchasing, Apple still collect commission (710 pts)]]></title>
            <link>https://www.macrumors.com/2024/01/16/us-app-store-alternative-purchase-option/</link>
            <guid>39020365</guid>
            <pubDate>Tue, 16 Jan 2024 22:58:43 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.macrumors.com/2024/01/16/us-app-store-alternative-purchase-option/">https://www.macrumors.com/2024/01/16/us-app-store-alternative-purchase-option/</a>, See on <a href="https://news.ycombinator.com/item?id=39020365">Hacker News</a></p>
<div id="readability-page-1" class="page"><div role="main" id="maincontent"><article expanded="true"><div data-io-article-url="/2024/01/16/us-app-store-alternative-purchase-option/"><p>Apple is making major changes to its U.S. iOS <a href="https://www.macrumors.com/guide/app-store/">App Store</a> policies, and developers are now able to direct customers to a non-App Store purchasing option for digital goods. Apple is allowing apps to feature a single link to a developer website that leads to an in-app purchase alternative, but Apple plans to continue to collect a 12 to 27 percent commission on content bought this way.</p>
<p><img src="https://images.macrumors.com/t/MuYr2cyAEYdr0CMrrvuycUPK5vs=/400x0/article-new/2022/01/iOS-App-Store-General-Feature-JoeBlue.jpg?lossy" srcset="https://images.macrumors.com/t/MuYr2cyAEYdr0CMrrvuycUPK5vs=/400x0/article-new/2022/01/iOS-App-Store-General-Feature-JoeBlue.jpg?lossy 400w,https://images.macrumors.com/t/j81xjhvPhb1xAaD6jc-kW3SoaHc=/800x0/article-new/2022/01/iOS-App-Store-General-Feature-JoeBlue.jpg?lossy 800w,https://images.macrumors.com/t/6S1CCkPCfv7Bu5OKPv07871bKhY=/1600x0/article-new/2022/01/iOS-App-Store-General-Feature-JoeBlue.jpg 1600w,https://images.macrumors.com/t/2D83fOzXH1a-mo51oJaSX0SjmzQ=/2500x0/filters:no_upscale()/article-new/2022/01/iOS-App-Store-General-Feature-JoeBlue.jpg 2500w" sizes="(max-width: 900px) 100vw, 697px" alt="iOS App Store General Feature JoeBlue" width="2250" height="1266"><br>Apple's update and the backstory that led to it are a bit complicated, but what <a href="https://www.macrumors.com/guide/iphone/">iPhone</a> and <a href="https://www.macrumors.com/roundup/ipad/">iPad</a> users need to know is that some apps in the U.S. storefront will soon feature a link to their website where subscriptions and other content can be purchased outside of the ‌App Store‌ in-app purchase system, likely with a discounted price.</p>
<p>Developers who want to offer this option will need to apply for a StoreKit External Purchase Link Entitlement, as Apple has outlined in both updated ‌App Store‌ Review Guidelines and the statement of compliance submitted to the Northern California U.S. District Court. With a Link Entitlement, a developer is able to direct a user to an out-of-app purchasing mechanism using an external purchase link. From Apple's modified ‌App Store‌ rules:<br>
</p>
<blockquote><p>Developers may apply for an entitlement to provide a link in their app to a website the developer owns or maintains responsibility for in order to purchase such items. Learn more about the entitlement. In accordance with the entitlement agreement, the link may inform users about where and how to purchase those in-app purchase items, and the fact that such items may be available for a comparatively lower price. The entitlement is limited to use only in the iOS or iPadOS App Store on the United States storefront. In all other storefronts, apps and their metadata may not include buttons, external links, or other calls to action that direct customers to purchasing mechanisms other than in-app purchase.</p>
<p>If your app engages in misleading marketing practices, scams, or fraud in relation to the entitlement, your app will be removed from the App Store and you may be removed from the Apple Developer Program.</p></blockquote>
<p>There are several requirements that developers need to adhere to maintain the privacy and security of the ‌App Store‌ ecosystem, and notably, Apple will collect a commission on purchases made using these Entitlement Links. Rather than 30 percent, Apple will collect a 27 percent fee on user purchases or year-one subscriptions made through the link. On the second year of a subscription, the commission fee drops to 12 percent, which is three percentage points lower than the 15 percent fee that Apple collects from second-year or longer subscriptions made through the in-app purchase system. Apps that participate in the ‌App Store‌ Small Business Program will be charged a 12 percent commission rate.</p>
<p>The commission will apply to transactions for digital goods and services that take place on a developers website within seven days after a user taps through an External Purchase Link to an external website. </p>
<p>Several key points about Entitlement Links are listed below.</p>
<ul>
<li>All links to outside purchasing methods must use the Entitlement Link system, and developers must apply and get Apple's approval.</li>
<li>Developers are permitted to have a single plain link on <strong>one</strong> screen of an app. The link can be at a sign-in screen, in user settings, or elsewhere, but it can only be in one place. The single location may not be an interstitial, modal, or pop-up.</li>
<li>The link can mention the specific price of content on a website, or that content is discounted on the website from the ‌App Store‌ price. Comparisons are allowed.</li>
<li>Links cannot be placed directly on an in-app purchase screen or in the in-app purchase flow.</li>
<li>Developers need to certify that the third-party payment service provider they are using for out-of-app purchasing meets industry standards for payment processors, and that they will offer users processes for managing subscriptions, requesting refunds, and disputing unauthorized transactions.</li>
<li>Apps that participate in Apple's Video Partner Program or News Partner Program are not eligible for Link Entitlement.</li>
<li>Apps that use the StoreKit External Purchase Link must continue to offer in-app purchases as an option.</li>
<li>‌App Store‌ pages are not able to include information about purchasing on a website or a link to a website.</li>
<li>Digital purchases that are sold on an app's website through the Entitlement Link must be available for use in that app.</li>
<li>The StoreKit External Purchase Link cannot discourage users from making in-app purchases or mimic an in-app purchase.</li>
<li>Links must open a new window in the default browser of the device, and are not able to open a web view.</li>
<li>No redirecting, intermediate links, or URL tracking parameters are allowed.</li>
<li>Developers are required to provide a periodic accounting of qualifying out-of-app purchases, and Apple has a right to audit developers' accounting to ensure compliance with their commission obligations and to charge interest and offset payments.</li>
</ul>
<p>The Link Entitlement process and the ‌App Store‌ changes are applicable only in the U.S. ‌App Store‌. Apps for all other storefronts are not able to include buttons, external links, or calls to action that direct customers to alternative purchasing options.</p>
<p><img src="https://images.macrumors.com/t/fPe_AZQfPf2CAhnY60W6NBJ1tBA=/400x0/article-new/2024/01/apple-external-link-examples.jpg?lossy" srcset="https://images.macrumors.com/t/fPe_AZQfPf2CAhnY60W6NBJ1tBA=/400x0/article-new/2024/01/apple-external-link-examples.jpg?lossy 400w,https://images.macrumors.com/t/Zhg-_b1Z5Havuv-49n69jJbaTXk=/800x0/article-new/2024/01/apple-external-link-examples.jpg?lossy 800w,https://images.macrumors.com/t/xJdNK9YgbXaKeoLrQArGAG_xxD0=/1600x0/article-new/2024/01/apple-external-link-examples.jpg 1600w,https://images.macrumors.com/t/0BqXjjWqbh3lgY14hXDIAoMftOg=/2500x0/filters:no_upscale()/article-new/2024/01/apple-external-link-examples.jpg 2500w" sizes="(max-width: 900px) 100vw, 697px" alt="apple external link examples" width="1635" height="920" data-old-src="https://images.macrumors.com/images-new/1x1.trans.gif" data-src="https://images.macrumors.com/t/fPe_AZQfPf2CAhnY60W6NBJ1tBA=/400x0/article-new/2024/01/apple-external-link-examples.jpg?lossy" data-srcset="https://images.macrumors.com/t/fPe_AZQfPf2CAhnY60W6NBJ1tBA=/400x0/article-new/2024/01/apple-external-link-examples.jpg?lossy 400w,https://images.macrumors.com/t/Zhg-_b1Z5Havuv-49n69jJbaTXk=/800x0/article-new/2024/01/apple-external-link-examples.jpg?lossy 800w,https://images.macrumors.com/t/xJdNK9YgbXaKeoLrQArGAG_xxD0=/1600x0/article-new/2024/01/apple-external-link-examples.jpg 1600w,https://images.macrumors.com/t/0BqXjjWqbh3lgY14hXDIAoMftOg=/2500x0/filters:no_upscale()/article-new/2024/01/apple-external-link-examples.jpg 2500w"><em></em></p><p><em>Examples of how Entitlement Links can be used in apps</em></p><p>Apple will provide an in-app warning to customers to let them know that they are leaving the ‌App Store‌ ecosystem to make a purchase on an external website and that ‌App Store‌ protections will not be available.</p>
<p><img src="https://images.macrumors.com/t/luUzlhnSjO9bR4tK8OwC1qc0z6c=/400x0/article-new/2024/01/apple-external-app-store-warning.jpg?lossy" srcset="https://images.macrumors.com/t/luUzlhnSjO9bR4tK8OwC1qc0z6c=/400x0/article-new/2024/01/apple-external-app-store-warning.jpg?lossy 400w,https://images.macrumors.com/t/VUmHxkAoOP8GY3Aobo_o6Q9PILc=/800x0/article-new/2024/01/apple-external-app-store-warning.jpg?lossy 800w,https://images.macrumors.com/t/gb-PABu893tLV5VX96yj86mCD0M=/1600x0/article-new/2024/01/apple-external-app-store-warning.jpg 1600w,https://images.macrumors.com/t/PHds6_lxLxrM2XJyaUe5x7Mux88=/2500x0/filters:no_upscale()/article-new/2024/01/apple-external-app-store-warning.jpg 2500w" sizes="(max-width: 900px) 100vw, 697px" alt="apple external app store warning" width="2030" height="1142" data-old-src="https://images.macrumors.com/images-new/1x1.trans.gif" data-src="https://images.macrumors.com/t/luUzlhnSjO9bR4tK8OwC1qc0z6c=/400x0/article-new/2024/01/apple-external-app-store-warning.jpg?lossy" data-srcset="https://images.macrumors.com/t/luUzlhnSjO9bR4tK8OwC1qc0z6c=/400x0/article-new/2024/01/apple-external-app-store-warning.jpg?lossy 400w,https://images.macrumors.com/t/VUmHxkAoOP8GY3Aobo_o6Q9PILc=/800x0/article-new/2024/01/apple-external-app-store-warning.jpg?lossy 800w,https://images.macrumors.com/t/gb-PABu893tLV5VX96yj86mCD0M=/1600x0/article-new/2024/01/apple-external-app-store-warning.jpg 1600w,https://images.macrumors.com/t/PHds6_lxLxrM2XJyaUe5x7Mux88=/2500x0/filters:no_upscale()/article-new/2024/01/apple-external-app-store-warning.jpg 2500w"><br>According to Apple's statement filed with the court, the requirements surrounding links are aimed at minimizing "fraud, scams, and confusion," while also providing developers with an opportunity to "entice users to other platforms" and give customers a choice between non-App Store purchasing and in-app purchases.</p>
<p>The changes today stem from Apple's 2021 legal battle with <a href="https://www.macrumors.com/guide/epic-games/">Epic Games</a>. Apple won the dispute and the court did not find that Apple had violated U.S. antitrust law, but Apple was at the time ordered to remove "anti-steering" rules preventing developers from informing customers about alternatives to in-app purchases. That order has been on hold <a href="https://www.macrumors.com/2023/07/17/app-store-rule-change-epic-games-delayed/">during the appeals process</a>, but the appeals process ended today.</p>
<p>Both Apple and ‌Epic Games‌ <a href="https://www.macrumors.com/2023/09/28/apple-epic-appeal-supreme-court/">had appealed</a> to the United States Supreme Court, but the Supreme Court <a href="https://www.macrumors.com/2024/01/16/supreme-court-declines-to-hear-apple-vs-epic-case/">declined to hear the case</a>. That means the initial ruling and the appeals court ruling that agreed with it are permanent, and Apple now has to comply with the part of that order that required it to change the ‌App Store‌ rules.</p>
<p>The anti-steering rule was two-pronged, requiring Apple to allow for links to in-app purchase alternatives and to allow developers to communicate with customers outside of the ‌App Store‌ through email and other contact information collected in the app. The outside communication part of the order was already satisfied with a change that Apple made to the ‌App Store‌ rules in 2021 to <a href="https://www.macrumors.com/2021/08/26/app-store-changes-developer-lawsuit-settlement/">settle a class-action developer lawsuit</a>.</p>
<p>Apple has already been allowing developers to use communication methods like email to inform customers about payment methods available outside of iOS apps, and Apple makes it clear in its messaging today that there are no limits on developers' out-of-app communications with users. The full statements that Apple provided to the court have been obtained by <em>MacRumors</em> and can be read below. </p>


<p><b>Update:</b> ‌Epic Games‌ CEO Tim Sweeney criticized Apple's ‌App Store‌ changes and said that Epic plans to contest Apple's "bad-faith compliance plan" in District Court. </p>
<div>
<blockquote data-lang="en" data-script="//platform.twitter.com/widgets.js">
<p lang="en" dir="ltr">A quick summary of glaring problems we've found so far:
1) Apple has introduced an anticompetitive new 27% tax on web purchases. Apple has never done this before, and it kills price competition. Developers can't offer digital items more cheaply on the web after paying a… <a href="https://t.co/YkHuapG7xa">pic.twitter.com/YkHuapG7xa</a>
— Tim Sweeney (@TimSweeneyEpic) <a href="https://twitter.com/TimSweeneyEpic/status/1747408148799881390?ref_src=twsrc%5Etfw">January 16, 2024</a></p></blockquote>
</div><br>
</div></article><p><h2>Popular Stories</h2></p><div><h3><a href="https://www.macrumors.com/2024/01/13/new-ios-features-coming-2024/">Apple Plans to Release These 8 New iOS Features This Year</a></h3><p>The calendar has turned to 2024, and there are many new iOS 17 and iOS 18 features that are expected to launch throughout the year. Below, we have recapped eight new iOS features expected in 2024, including Stolen Device Protection, collaborative Apple Music playlists, AirPlay on hotel room TVs, app sideloading in the EU, next-generation CarPlay, roadside assistance via satellite outside of...</p></div><div><h3><a href="https://www.macrumors.com/2024/01/15/apple-vision-pro-virtual-keyboard-criticism/">Apple Vision Pro Virtual Keyboard Blasted As 'Complete Write-Off'</a></h3><p>Monday January 15, 2024 3:25 am PST by <a href="https://www.macrumors.com/author/tim-hardwick/" rel="author">Tim Hardwick</a></p><p>Anyone paying attention to Apple's Vision Pro headset unveiling at WWDC 2023 will have seen its virtual keyboard demo. The keyboard floats in mid-air, allowing you to input text in your spatial computing environment while wearing the device. However, anyone planning to ditch their physical keyboard may want to hold onto it for a little while longer: According to Bloomberg's Mark Gurman, the ...</p></div><div><h3><a href="https://www.macrumors.com/2024/01/15/app-store-to-be-split-in-two/">App Store to Be 'Split in Two' Ahead of EU iPhone Sideloading Deadline</a></h3><p>Apple is preparing to split the App Store "in two" in the coming weeks ahead of European Union requirements that will force Apple to enable app sideloading in the region, Bloomberg's Mark Gurman reports. In the latest edition of his "Power On" newsletter, Gurman explained that Apple is gearing up to make changes to the App Store in the EU to comply with the region's impending Digital Markets ...</p></div><div><h3><a href="https://www.macrumors.com/2024/01/10/ios-17-3-coming-soon/">iOS 17.3 Coming Soon With These Two New Features for Your iPhone</a></h3><p>iOS 17.3 has been in beta testing since mid-December, and the upcoming software update includes two new features for the iPhone so far. Apple seeded the third beta of iOS 17.3 this week. The update should be coming soon, with a release likely later this month. Below, we provide additional details about the new features in iOS 17.3 so far. Stolen Device Protection Earlier this year, T...</p></div><div><h3><a href="https://www.macrumors.com/2024/01/12/iphone-16-capture-button-leak/">iPhone 16 Leak Reveals This All-New Button</a></h3><p>Following the addition of the Action button on the iPhone 15 Pro models, all iPhone 16 models may feature yet another all-new button. Apple plans to add a so-called "Capture" button to all iPhone 16 models, according to pre-production information obtained by MacRumors. The button would be located below the power button on the right side of the device, where the mmWave antenna window is...</p></div><div><h3><a href="https://www.macrumors.com/2024/01/16/here-are-all-the-new-m3-apple-macs-expected-in-2024/">Here Are All the New M3 Apple Macs Expected This Year</a></h3><p>Tuesday January 16, 2024 2:26 am PST by <a href="https://www.macrumors.com/author/tim-hardwick/" rel="author">Tim Hardwick</a></p><p>Apple in 2023 launched an M3-powered 24-inch iMac, as well as new 14-inch and 16-inch MacBook Pro models with M3 series chips. But the rest of Apple's Mac lineup is still to be updated to the latest M3 processors. Now that 2023 is over, attention naturally turns to the other Macs in the company's lineup and where they fit into Apple's M3 roadmap for the year ahead. Here's what the latest...</p></div><div><h3><a href="https://www.macrumors.com/2024/01/14/iphone-16-and-16-plus-8gb-ram-rumor/">iPhone 16 and iPhone 16 Plus Rumored to Feature Increased 8GB RAM and Wi-Fi 6E Support</a></h3><p>Apple's next-generation iPhone 16 and iPhone 16 Plus models will both feature 8GB of RAM, an increase over the 6GB of RAM in the iPhone 15 and iPhone 15 Plus, according to information shared today by technology analyst Jeff Pu. In a research note with investment firm Haitong International Securities, Pu reiterated his belief that all iPhone 16 models will be equipped with 8GB of RAM....</p></div></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Fluorite lenses: Corrective capabilities beyond ordinary optical glass (306 pts)]]></title>
            <link>https://global.canon/en/c-museum/special/exhibition2.html</link>
            <guid>39020258</guid>
            <pubDate>Tue, 16 Jan 2024 22:50:21 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://global.canon/en/c-museum/special/exhibition2.html">https://global.canon/en/c-museum/special/exhibition2.html</a>, See on <a href="https://news.ycombinator.com/item?id=39020258">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>

          <h3>Fluorite lenses: Corrective capabilities beyond the limits of ordinary optical glass</h3>

          <p><img src="https://global.canon/ja/c-museum/common/img/special/exhibition2-1.jpg" alt="Fluorite lenses: Corrective capabilities beyond the limits of ordinary optical glass"></p><p>One crucial material that supports the high image quality characteristic of Canon lenses is fluorite, which is a crystallized form of calcium fluoride. It has long been known that using a fluorite lens in conjunction with a
            glass lens can reduce chromatic aberration to extremely low levels. However, natural fluorite occurs in small sizes that are suitable only for use in small optical equipment such as the object lenses of microscopes. Canon, in its pursuit
            of progress in imaging capabilities, was keen on utilizing fluorite in its photography lenses, set out to develop its own technology for forming large, high-purity artificial fluorite crystals using fluorite ore as a raw material. In May
            1969, the <a href="https://global.canon/en/c-museum/product/fl117.html">FL-F300mm f/5.6</a>—the world’s first consumer telephoto lens to employ fluorite lens elements—was released.</p>

          
          <h4>How fluorite lens elements correct chromatic aberration</h4>
          <p>You might have noticed the outlines of your subjects tinted in a way that resembles a color fringe. This is chromatic aberration, and it can also take the form of a haziness throughout the entire image. As this prevents the subject
              from being rendered accurately, it needs to be corrected so that the image quality is sharp, clear, and faithful to the scene.</p>
          <div>
            <p><img src="https://global.canon/ja/c-museum/common/img/special/exhibition2-2.jpg" alt="The purple fringing at the edges if these branches is a form of chromatic aberration."></p><p>The purple fringing at the edges if these branches is a form of chromatic aberration.</p>
          </div>
          
          <p>Chromatic aberration happens because when light passes through a glass surface, the different-colored waves within it (red, green, blue, etc.) are refracted at different angles due to their different lengths, with each
            color converging at a different focal point. Such aberration is usually corrected by using a combination of concave and convex lenses, which refract the light in opposite directions to each other.</p>
          <p><img src="https://global.canon/ja/c-museum/common/img/special/exhibition2-3_en.jpg" alt="Chromatic aberration correction using concave and convex glass lenses">
          </p>
          <p><img src="https://global.canon/ja/c-museum/common/img/special/exhibition2-4_en.jpg" alt="Chromatic aberration correction using concave and convex glass lenses">
          </p>
          
          <p><strong>Chromatic aberration correction using concave and convex glass lenses</strong></p>
          <p>However, it is not possible to correct the chromatic aberration on all wavelengths by using ordinary glass. The chromatic aberration, which occurs on certain wavelengths such as red, is called residual chromatic
            aberration. As the refractive index of the wavelengths differ depending on the type of optical glass, there are limitations to how much residual chromatic aberration can be mitigated depending on the combination and properties of the
            optical glass. </p>
          <p>This is where fluorite comes in handy. As fluorite is a fundamentally different material than conventional optical glass, it can be used in combination with glass to correct chromatic aberration more effectively. It is
            particularly effective on telephoto lenses, where the long focal length exacerbates chromatic aberration.</p>

          <p><img src="https://global.canon/ja/c-museum/common/img/special/exhibition2-5_en.jpg" alt="Chromatic aberration correction using a convex fluorite lens and a concave glass lens">
          </p>
          <p><img src="https://global.canon/ja/c-museum/common/img/special/exhibition2-6_en.jpg" alt="Chromatic aberration correction using a convex fluorite lens and a concave glass lens">
          </p>
          
          <p><strong>Chromatic aberration correction using a convex fluorite lens and a concave glass lens</strong></p>
          <p>Canon’s fluorite lens elements incorporate natural fluorite as a raw material, endowing the lenses with low-refractive, low-dispersion properties not possible with glass lenses. Fluorite lenses are also unique in their
            extraordinary partial dispersion tendencies: the red to green wavelengths are dispersed with the same tendencies as glass, but the green to blue wavelengths are dispersed more than glass. Using a convex fluorite lens element alongside a
            high-dispersion glass concave lens element therefore eliminates residual chromatic aberration, making possible a lens that produces clear, sharp, and high-quality images.</p>
          <div>
            <p><img src="https://global.canon/ja/c-museum/common/img/special/exhibition2-7_en.png" alt="Refraction and dispersion">
            </p>
            <div>
              <h5><strong>Refraction and dispersion</strong></h5>
              <p>‘Refraction’ is the phenomenon in which light changes direction when it passes through the surface of a material such as glass. The degree of the directional change is called the ‘refractive index’. As the refractive
                index varies depending on the color constituents (wavelengths) of the light, each color bends in a different direction. This is known as color dispersion. On optical glass, dispersion occurs at a fixed proportion regardless of the
                wavelength, whereas on fluorite, dispersion occurs at different proportions for different wavelengths and is known as ‘extraordinary partial dispersion’. </p>
            </div>
            
          </div>
          
          <h4>The emergence of fluorite lenses and how they improve the image quality of telephoto lenses</h4>
          <p>Fluorite lenses transcend traditional limitations to reduce chromatic aberration to an extremely low level. These lenses have their origins the Canon F Project, which started in August 1966. Canon’s lens developers strongly
            believed that to create a lens that was performed better than existing lenses, it was first necessary to create a new material, and it was this conviction that drive them to establish the technology for producing artificial fluorite
            crystals to use in camera lenses.</p>
          <p>The challenge in producing artificial fluorite crystals lay in the crystallization. The term “glass” was originally used to describe the state of a material. Being non-crystalline, it consists of atoms fixed in random
            positions--simply melting glass allows it to be processed into different shapes. On the other hand, fluorite is a crystalline substance, and its constituent atoms must be arranged in a specific configuration for it to crystalize. </p>
          <p>To pulverize natural fluorite, purify it, and then restore the exact atomic structure to recrystallize it into a size large enough for application in a camera lens is a near-impossible feat. Therefore, the developers had to
            ensure a precisely controlled vacuum environment where the temperature is kept at least 1,000℃, so they designed an apparatus to artificially produce large crystals with high purity In 1968, two years after the F project started, Canon
            finally managed to overcome these obstacles to successfully form artificial fluorite crystals large enough to be used in camera lenses. </p>
          <div>
            <p><img src="https://global.canon/ja/c-museum/common/img/special/exhibition2-8.jpg" alt="natural fluorite crystals">
            </p>
            <p>The stones on the left are natural fluorite crystals. They are green and purple due to impurities within the crystals. At the middle is an artificial fluorite crystal ingot produced by Canon.<br>When heated, natural
                fluorite glows with a dreamy light that resembles that of fireflies, which is said to be the reason behind its Japanese name, <i>hotaru ishi</i>, literally “firefly stone”. Just like how fireflies require clean water to live, only
                pure fluorite can “shine” as photography lenses.</p>
            
          </div>
          
          <p>Another challenge in fluorite lens production is its polishing. As fluorite is softer and more delicate than glass, the same methods used for polishing glass are not suitable. Therefore, Canon developed a special technology
            for polishing fluorite lenses that requires up to four times longer than the time needed to polish ordinary optical glass. This technology was successfully commercialized the following year, in May 1969.
          </p>
          <p><img src="https://global.canon/ja/c-museum/common/img/special/exhibition2-9.jpg" alt="The first lens to employ fluorite lens elements was the FL-F300mm f/5.6 (released in 1969). The bright green line, meant to evoke the image of the glow of fluorite, indicates that the lens features a fluorite lens element."></p><p>The first lens to employ fluorite lens elements was the FL-F300mm f/5.6 (released in 1969). The bright green line, meant to evoke the image of the glow of fluorite, indicates that the lens features a fluorite
            lens element.</p>
          <p>The FL-F300mm f/5.6, whose successful commercialization was achieved by transcending many challenges, was highly acclaimed for is vivid, high-contrast rendering, and became widely used in such professions as
            photojournalism. </p>
          <p>Canon subsequently improved on is high temperature vacuum, temperature control, and polishing technologies, enabling the use of fluorite lens elements in many more lenses. Canon remains committed to pursuing the highest
            image quality for its telephoto lenses. </p>

          <h4>The fluorite lens production process</h4>
          <p>While the grinding and polishing processes may seem identical for all kinds of optical glass, each stage of the fluorite lens production process requires slow, meticulous attention to detail.</p>
          <div>

            <p><b>1. Raw materials</b><br>The raw material for fluorite lenses is naturally occurring fluorite ore.</p>
            <p><img src="https://global.canon/ja/c-museum/common/img/special/exhibition2-10.jpg" alt="1. Raw materials">
            </p>
            
            <p><b>2. Pulverization and refinement</b><br>The raw fluorite is pulverized and refined to remove impurities before being poured into a graphite crucible that does not melt easily.</p>
            <p><img src="https://global.canon/ja/c-museum/common/img/special/exhibition2-11.jpg" alt="2. Pulverization and refinement">
            </p>
            
            <p><b>3. Crystallization </b><br>The crucible is placed in a crystal-growing apparatus with a heater on top and heated to 1,400℃. After the raw fluorite has melted, the crucible is gradually lowered, allowing crystallization to occur
                starting from the bottom of the crucible.</p>
            <p><img src="https://global.canon/ja/c-museum/common/img/special/exhibition2-12_en.png" alt="3. Crystallization ">
            </p>
            
            <p><b>4. Annealing</b><br>The annealing process removes strains that occur inside the crystals formed. Strain that lead to cracks are removed by heating the crystals to a high temperature insufficient to melt them, and then slowly
                cooling them to room temperature over a long period of several weeks.</p>
            <p><img src="https://global.canon/ja/c-museum/common/img/special/exhibition2-13.jpg" alt="4. Annealing">
            </p>
            
            <p><b>5. Trimming and rough processing</b><br>The unnecessary parts of the crystal surface are trimmed off, and the crystal is rough-processed to the required size. The interior of the crystal is inspected for anomalies. </p>
            <p><img src="https://global.canon/ja/c-museum/common/img/special/exhibition2-14.jpg" alt="5. Trimming and rough processing">
            </p>
            
            <p><b>6. Grinding</b><br>The top and bottom surfaces of the crystal are ground into a spherical shape with a surface that resembles frosted glass.</p>
            <p><img src="https://global.canon/ja/c-museum/common/img/special/exhibition2-15.jpg" alt="6. Grinding">
            </p>
            
            <p><b>7. Polishing</b><br>The surfaces of the crystal are polished with a pellet made from coagulated polish until they are semi-transparent and meet the specified dimensions. Finally, a special polish is used to remove fine scratches.
              </p>
            <p><img src="https://global.canon/ja/c-museum/common/img/special/exhibition2-16.jpg" alt="7. Polishing">
            </p>
            
            <p><b>8. Vapor deposition</b><br>Coating material is heat-evaporated under high vacuum conditions up to one-millionth to one-hundred millionth of one unit of atmospheric pressure. This forms a thin film over the polished lens. </p>
            <p><img src="https://global.canon/ja/c-museum/common/img/special/exhibition2-17_en.png" alt="8. Vapor deposition">
            </p>
            
            <p><b>9. Completion</b><br>An experienced technician inspects purity using an interferometer. Only lens elements that pass the inspection are sent to be assembled in lenses. </p>
            <p><img src="https://global.canon/ja/c-museum/common/img/special/exhibition2-18.jpg" alt="9. Completion">
            </p>
            
          </div>
          <h4>Lenses that employ fluorite lens elements <span>(as at May 2021)</span></h4>
          <div>
            <p>Since the FL-F300mm, Canon has produced 39 more lenses that employ fluorite lens elements. As fluorite lens elements not only correct chromatic aberration but also contribute to reducing the size and weight of products,
                they are proactively used in large telephoto lenses.<br>These lenses are much beloved by many photographers who demand high image quality at super telephoto focal lengths, including not only professional sports photographers and
                photojournalists, but also enthusiasts who photograph subjects such as wild birds, trains and aircraft.</p>
            <div>
              <p><img src="https://global.canon/ja/c-museum/common/img/special/exhibition2-19.jpg" alt="RF600mm F4 L IS USM (released in 2021)"></p><p>RF600mm F4 L IS USM (released in 2021)</p>
            </div>

            
          </div>
          <h4>RF lenses that employ fluorite lens elements <span>(as at May 2021)</span></h4>
          


        </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Stable Code 3B: Coding on the Edge (291 pts)]]></title>
            <link>https://stability.ai/news/stable-code-2024-llm-code-completion-release</link>
            <guid>39019532</guid>
            <pubDate>Tue, 16 Jan 2024 21:40:15 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://stability.ai/news/stable-code-2024-llm-code-completion-release">https://stability.ai/news/stable-code-2024-llm-code-completion-release</a>, See on <a href="https://news.ycombinator.com/item?id=39019532">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-block-type="2" data-border-radii="{&quot;topLeft&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0},&quot;topRight&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0},&quot;bottomLeft&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0},&quot;bottomRight&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0}}" id="block-yui_3_17_2_1_1705434376921_5483">
  <p>Today, we announce our first Large Language Model release of 2024: <a href="https://huggingface.co/stabilityai/stable-code-3b" target="_blank"><span>Stable Code 3B</span></a>. This new LLM is a follow-up to our previously released <a href="https://stability.ai/news/stablecode-llm-generative-ai-coding"><span>Stable Code Alpha 3B</span></a> and the first major Stable Code release, offering a new state-of-the-art model designed for code completion with multiple additional capabilities.&nbsp;</p><p>Compared to CodeLLaMA 7b, Stable Code 3B is 60% smaller while featuring a similar high-level performance across programming languages. Based on our pre-existing <a href="https://huggingface.co/stabilityai/stablelm-3b-4e1t" target="_blank"><span>Stable LM 3B</span></a> foundational model trained on 4 trillion tokens of natural language data, Stable Code was further trained on software engineering-specific data, including code. The model's compact size allows it to be run privately on the edge in real-time on modern laptops, even those without a dedicated GPU.</p><p>Stable Code 3B offers more features and significantly better performance across multiple languages with additional benefits such as support for Fill in the Middle capabilities (FIM) and expanded context size. Stable Code as a base is trained on sequences of up to 16,384 tokens but follows a similar approach to CodeLlama with the implementation of Rotary Embeddings, optionally allowing modification of the rotary base up to 1,000,000, further expanding the model’s context length up to 100k tokens.</p><p>Stable Code is trained on 18 programming languages (selected based on the <a href="https://survey.stackoverflow.co/2023/#section-most-popular-technologies-programming-scripting-and-markup-languages" target="_blank"><span>2023 StackOverflow Developer Survey</span></a>) and demonstrates state-of-the-art performance (compared to models of similar size) on the MultiPL-E metrics across multiple programming languages tested.</p><p><strong>Performance Comparison</strong></p>
</div><div data-block-type="2" data-border-radii="{&quot;topLeft&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0},&quot;topRight&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0},&quot;bottomLeft&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0},&quot;bottomRight&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0}}" id="block-yui_3_17_2_1_1705430802082_7238">
  <p><strong>Training Insights</strong></p><p>Our training pipeline consists of a multi-stage process similar to Codellama. We start with an LM pre-trained on natural language data, in this case, <a href="https://huggingface.co/stabilityai/stablelm-3b-4e1t" target="_blank"><span>StableLM-3B-4e1t</span></a>, followed up with unsupervised fine-tuning on multiple code and code-related datasets, including CommitPack, GitHub Issues, StarCoder &amp; other Math datasets. In the second step, we further fine-tune the model with longer sequences of 16,384 tokens with the base modification suggested in CodeLLama. The new stable-code model also supports Flash Attention 2 and is available for use.</p><p>Further references to the data and model can be found in our <a href="https://huggingface.co/stabilityai/stable-code-3b" target="_blank"><span>model card</span></a>. We will release a full technical report with additional details and ablations to be more transparent and open to the community.</p><p><strong>Commercial Applications</strong></p><p>Stay updated on our progress by signing up for our newsletter, and learn more about commercial applications by contacting us here.&nbsp;</p><p>Follow us on <a href="https://twitter.com/stabilityai" target="_blank"><span>Twitter</span></a>, <a href="https://www.instagram.com/stability.ai/" target="_blank"><span>Instagram</span></a>, <a href="https://www.linkedin.com/company/66318622/"><span>LinkedIn</span></a>, and join our <a href="https://discord.gg/stablediffusion"><span>Discord Community</span></a>.</p>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Post-mortem for last week's incident at Kagi (303 pts)]]></title>
            <link>https://status.kagi.com/issues/2024-01-12-kagi-down-on-some-regions/</link>
            <guid>39019119</guid>
            <pubDate>Tue, 16 Jan 2024 21:04:39 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://status.kagi.com/issues/2024-01-12-kagi-down-on-some-regions/">https://status.kagi.com/issues/2024-01-12-kagi-down-on-some-regions/</a>, See on <a href="https://news.ycombinator.com/item?id=39019119">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p><small>January 12, 2024 at 4:30 PM UTC</small></p><p><small><a href="https://status.kagi.com/affected/us-east4/">us-east4</a>
<a href="https://status.kagi.com/affected/us-west2/">us-west2</a>
<a href="https://status.kagi.com/affected/europe-west2/">europe-west2</a>
<a href="https://status.kagi.com/affected/asia-east2/">asia-east2</a>
<a href="https://status.kagi.com/affected/us-central1/">us-central1</a>
<a href="https://status.kagi.com/affected/europe-west4/">europe-west4</a>
<a href="https://status.kagi.com/affected/asia-southeast1/">asia-southeast1</a>
<a href="https://status.kagi.com/affected/australia-southeast1/">australia-southeast1</a>
<a href="https://status.kagi.com/affected/southamerica-east1/">southamerica-east1</a></small></p><p><strong>Resolved after
6h
50m of downtime.</strong>
<span>January 12, 2024 at 11:20 PM UTC</span></p><hr><p><em>Investigating</em> - We are experiencing issues following a deploy. Team is working on resolving this..
<span>(16:45 UTC — Jan 12)</span></p><p><em>Monitoring</em> - We have are reverting a configuration change that we believe to be the culprit, and are continuing to monitor as service is coming back to full health.
<span>(18:30 UTC — Jan 12)</span></p><p><em>Update</em> - In order to fully restore stability we will need to pause traffic momentarily. We will be redirecting users to this page while we restore load to the service in a controlled manner. We will follow up with further details as the situation progresses.
<span>(20:26 UTC — Jan 12)</span></p><p><em>Monitoring</em> - Traffic has been restored and we are continuing to monitor the service as it comes back to full health.
<span>(21:14 UTC — Jan 12)</span></p><p><em>Resolved</em> - All services are now operating as normal. Thank you for your patience while we resolved this issue.</p><h2 id="post-mortem">Post-Mortem</h2><p>Hi all,</p><p>This is Zac, the Tech Lead at Kagi.
I’m going to be sharing below a more in-depth post-mortem of our service interruption last week.
Assisting me in responding to this incident was Seth, one of our senior engineers, and Luan, our DevOps engineer.</p><p>This will be fairly technical, you can skip to “Next Steps” for takeaways.
The summary is that we were the target of some actors misusing the service, exploting a pathological case in our infrastructure, and we immediately released mitigations and are working on improvements in several areas of our code and communications.</p><h2 id="timeline">Timeline</h2><p>On January 12th, approx. 5:30PM UTC, the team became aware of an infrastructure issue occurring by way of our internal monitoring and user reports of issues.
The nature of the issue was causing slow loading or complete page timeouts for users in various regions.</p><p>This incident unfortunately took us quite some time to resolve - we deeply thank our users for their patience, and the opportunity to give some background as to what was going on, and how we plan to move forward.</p><p>At first, by what turned out to be a complete coincidence, the incident occurred at precisely the same time that we were performing an infrastructure upgrade to our VMs with additional RAM resources.
Our monitoring was reporting both high latency and issues with our application’s database connection pool.
While no code changes were part of this upgrade, and “old” instances were reporting some issues as well, we decided the first course of action was to revert this change.</p><p>This revert completed at around 6:50 PM, but as we continued to monitor the issue persisted.
Meanwhile, we had been inspecting the behavior of our application’s database connection pools, which were saturated with connections to our primary DB instance.
It was unclear what the exact cause of this was yet, but what was clear is that the total number of connections being established globally to our primary exceeded its maximum configured connection limit.</p><p>Our next move was to evaluate if we had somehow caught ourselves in a “spiral” of exhausting our maximum connections, wherein any instance that we would replace would simply get its connection pool exhausted again, queuing for access to the primary.
In several steps we tried replacing a few instances to see what effect reducing the congestion would have.
We also were making progress on evaluating various parts of the databases internal health and query performance.</p><p>With mild signs that cycling some instances was helping, at 9:30PM UTC we decided to pause all user traffic by redirecting users to our status page to give the database a break and completely reset all connection pools in one shot.
We installed the redirect and issued a restart of all nodes.
Once all appeared stable, we started letting traffic back in again.
Unfortunately, the issue persisted.</p><p>While looking at the database state, it became clear to our engineers that the root cause was in fact high contention on rows in the users table.
This contention caused a steep increase in write latency, which in turn put backpressure on our application’s connection pool, causing it to eventually exhaust all available connections as writes were taking too long to complete.
The writes were all stemming from one instance that would eventually starve the rest of our global instances of access to our primary, thus causing disruption in all other regions.</p><p>This didn’t exactly come as a surprise to us, as for the entirety of Kagi’s life so far we have actually used the cheapest, single-core database available to us on GCP!
To this day we’ve yet to exceed 50% load capacity on it, which we’ve worked hard to achieve.
This has always carried the risk of the database being relatively easy to knock over, but we have so far kept load and latency under control with redundancy, distributed read-replicas, and high scrutiny over the SQL we write.</p><p>We then took steps to identify the bad actors, where we found accounts created within 24hrs and over 60,000 searches performed in short time period from a single user facing account.
While we do offer unlimited searches to our users, such volume was a clear abuse of our platform, against our terms of use.
As such, we removed searching capability from the offending accounts.</p><p>At the same time, we issued a hotfix that disabled the particular writes that were causing high contention, in addition to some upgrades of our database driver, which included several relevant bug fixes to connection pool health.
This would help ensure that immediately the same pathological case could not be exploited again.</p><p>By midnight, the issue was fully resolved.
The team continued to closely monitor for any signals that the actors were returning, but they did not.</p><p>We were later in contact with an account that we blocked who claimed they were using their account to perform automated scraping of our results, which is not something our terms allow for.</p><h2 id="next-steps">Next Steps</h2><p>There’s a lot we took away from this incident, and we have immediate plans already in motion to make our system more robust to this type of abuse, as well as our communication processes around incidents.</p><p>First, regretfully we were not very prompt in updating our status page.
We owe it to our customers to be quick and transparent about any issues going on that might affect access to the product that they pay for.
To address this, we are moving to a status page platform to one that will more easily allow us expose some of our automated internal monitoring to users.
This way users have an idea of the platform’s health in real-time, even if our small team of engineers has their hands full to immediately post an update (which was not a very fast process to begin with, even if we were on top of it).
We should have this available by next week.</p><p>Secondly, we have directly mitigated the queries causing issues under load.
With this issue in mind, we are also running load tests to learn about any other similar deficiencies that may still exist, and what to avoid in the future.
We are also installing some additional monitoring to more quickly point us to the right place in our infra, and hopefully not waste as much time chasing a false-flag as we did this time.</p><p>Lastly, we are performing a re-upping of our systems set up to detect this kind of abuse of our terms, which were clearly too lax.
Besides any potential performance impact, this vector also directly costs us money as we pay for each search.
To protect our finances, and all of our subscribers continued access to unlimited (organic) searches, we need to set some automated limits to help us enforce this.
From analyzing our user’s usage, we have picked some limits that no good-faith user of Kagi should reasonably hit.</p><p>These new limits should already be in place by the time of this post, and we will monitor their impact and continue to tune them as needed.
If you believe you find yourself wrongly unable to access Kagi, please reach out to <a href="mailto:support@kagi.com">support@kagi.com</a>.</p><p>Thank you so much for bearing with us through this incident!
Please look forward to a more robust service as we implement these things, and as usual, more features &amp; improvements are on their way.</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[6174 (470 pts)]]></title>
            <link>https://en.wikipedia.org/wiki/6174</link>
            <guid>39018769</guid>
            <pubDate>Tue, 16 Jan 2024 20:35:37 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://en.wikipedia.org/wiki/6174">https://en.wikipedia.org/wiki/6174</a>, See on <a href="https://news.ycombinator.com/item?id=39018769">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
							

						<p>From Wikipedia, the free encyclopedia</p>
					</div><div lang="en" dir="ltr" id="mw-content-text"><p>The number <b>6174</b> is known as <b>Kaprekar's constant</b><sup id="cite_ref-1"><a href="#cite_note-1">[1]</a></sup><sup id="cite_ref-Kaprekar1955_2-0"><a href="#cite_note-Kaprekar1955-2">[2]</a></sup><sup id="cite_ref-Kaprekar1980_3-0"><a href="#cite_note-Kaprekar1980-3">[3]</a></sup> after the <a href="https://en.wikipedia.org/wiki/India" title="India">Indian</a> <a href="https://en.wikipedia.org/wiki/Mathematician" title="Mathematician">mathematician</a> <a href="https://en.wikipedia.org/wiki/D._R._Kaprekar" title="D. R. Kaprekar">D. R. Kaprekar</a>. This number is renowned for the following rule:
</p>
<ol><li>Take any four-digit number, using at least two different digits (leading zeros are allowed).</li>
<li>Arrange the digits in descending and then in ascending order to get two four-digit numbers, adding leading zeros if necessary.</li>
<li>Subtract the smaller number from the bigger number.</li>
<li>Go back to step 2 and repeat.</li></ol>
<p>The above process, known as <a href="https://en.wikipedia.org/wiki/Kaprekar%27s_routine" title="Kaprekar's routine">Kaprekar's routine</a>, will always reach its <a href="https://en.wikipedia.org/wiki/Fixed_point_(mathematics)" title="Fixed point (mathematics)">fixed point</a>, 6174, in at most 7 iterations.<sup id="cite_ref-mathworld_4-0"><a href="#cite_note-mathworld-4">[4]</a></sup> Once 6174 is reached, the process will continue yielding 7641 – 1467 = 6174. For example, choose 1459:
</p>
<div>
<ul><li>9541 – 1459 = 8082</li>
<li>8820 – 0288 = 8532</li>
<li>8532 – 2358 = 6174</li>
<li>7641 – 1467 = <b>6174</b></li></ul>
</div><p>
The only four-digit numbers for which Kaprekar's routine does not reach 6174 are <a href="https://en.wikipedia.org/wiki/Repdigit" title="Repdigit">repdigits</a> such as 1111, which give the result <a href="https://en.wikipedia.org/wiki/0_(number)" title="0 (number)">0000</a> after a single iteration. All other four-digit numbers eventually reach 6174 if leading zeros are used to keep the number of digits at 4. For numbers with three identical numbers and a fourth number that is one number higher or lower (such as 2111), it is essential to treat 3-digit numbers with a leading zero; for example: 2111 – 1112 = 0999; 9990 – 999 = 8991; 9981 – 1899 = 8082; 8820 – 288 = 8532; 8532 – 2358 = 6174.<sup id="cite_ref-5"><a href="#cite_note-5">[5]</a></sup></p><table><tbody><tr><th colspan="2"><table><tbody><tr>
<td>← 6173</td>
<td>6174</td>
<td>6175 →</td>
</tr></tbody></table></th></tr><tr><td colspan="2"><div><ul><li><a href="https://en.wikipedia.org/wiki/List_of_numbers" title="List of numbers">List of numbers</a></li><li><a href="https://en.wikipedia.org/wiki/Integer" title="Integer">Integers</a></li></ul></div><div><p><a href="https://en.wikipedia.org/wiki/Negative_number" title="Negative number">←</a> <a href="https://en.wikipedia.org/wiki/0" title="0">0</a> <a href="https://en.wikipedia.org/wiki/1000_(number)" title="1000 (number)">1k</a> <a href="https://en.wikipedia.org/wiki/2000_(number)" title="2000 (number)">2k</a> <a href="https://en.wikipedia.org/wiki/3000_(number)" title="3000 (number)">3k</a> <a href="https://en.wikipedia.org/wiki/4000_(number)" title="4000 (number)">4k</a> <a href="https://en.wikipedia.org/wiki/5000_(number)" title="5000 (number)">5k</a> <a href="https://en.wikipedia.org/wiki/6000_(number)" title="6000 (number)">6k</a> <a href="https://en.wikipedia.org/wiki/7000_(number)" title="7000 (number)">7k</a> <a href="https://en.wikipedia.org/wiki/8000_(number)" title="8000 (number)">8k</a> <a href="https://en.wikipedia.org/wiki/9000_(number)" title="9000 (number)">9k</a> <a href="https://en.wikipedia.org/wiki/10000_(number)" title="10000 (number)">→</a></p></div></td></tr><tr><th scope="row"><a href="https://en.wikipedia.org/wiki/Cardinal_numeral" title="Cardinal numeral">Cardinal</a></th><td>six thousand one hundred seventy-four</td></tr><tr><th scope="row"><a href="https://en.wikipedia.org/wiki/Ordinal_numeral" title="Ordinal numeral">Ordinal</a></th><td>6174th<br>(six thousand one hundred seventy-fourth)</td></tr><tr><th scope="row"><a href="https://en.wikipedia.org/wiki/Factorization" title="Factorization">Factorization</a></th><td>2 × 3<sup>2</sup> × 7<sup>3</sup></td></tr><tr><th scope="row"><a href="https://en.wikipedia.org/wiki/Divisor" title="Divisor">Divisors</a></th><td>1, 2, 3, 6, 7, 9, 14, 18, 21, 42, 49, 63, 98, 126, 147, 294, 343, 441, 686, 882, 1029, 2058, 3087, 6174</td></tr><tr><th scope="row"><a href="https://en.wikipedia.org/wiki/Greek_numerals" title="Greek numerals">Greek numeral</a></th><td>,ϚΡΟΔ´</td></tr><tr><th scope="row"><a href="https://en.wikipedia.org/wiki/Roman_numerals" title="Roman numerals">Roman numeral</a></th><td><span>V</span>MCLXXIV, or <span>VI</span>CLXXIV</td></tr><tr><th scope="row"><a href="https://en.wikipedia.org/wiki/Binary_number" title="Binary number">Binary</a></th><td>1100000011110<sub>2</sub></td></tr><tr><th scope="row"><a href="https://en.wikipedia.org/wiki/Ternary_numeral_system" title="Ternary numeral system">Ternary</a></th><td>22110200<sub>3</sub></td></tr><tr><th scope="row"><a href="https://en.wikipedia.org/wiki/Senary" title="Senary">Senary</a></th><td>44330<sub>6</sub></td></tr><tr><th scope="row"><a href="https://en.wikipedia.org/wiki/Octal" title="Octal">Octal</a></th><td>14036<sub>8</sub></td></tr><tr><th scope="row"><a href="https://en.wikipedia.org/wiki/Duodecimal" title="Duodecimal">Duodecimal</a></th><td>36A6<sub>12</sub></td></tr><tr><th scope="row"><a href="https://en.wikipedia.org/wiki/Hexadecimal" title="Hexadecimal">Hexadecimal</a></th><td>181E<sub>16</sub></td></tr></tbody></table>
<meta property="mw:PageProp/toc">
<h2><span id="Other_.22Kaprekar.27s_constants.22"></span><span id="Other_&quot;Kaprekar's_constants&quot;">Other "Kaprekar's constants"</span><span><span>[</span><a href="https://en.wikipedia.org/w/index.php?title=6174&amp;action=edit&amp;section=1" title="Edit section: Other &quot;Kaprekar's constants&quot;"><span>edit</span></a><span>]</span></span></h2>

<p>There can be analogous fixed points for digit lengths other than four; for instance, if we use 3-digit numbers, then most sequences (i.e., other than repdigits such as 111) will terminate in the value <a href="https://en.wikipedia.org/wiki/495_(number)" title="495 (number)">495</a> in at most 6 iterations. Sometimes these numbers (495, 6174, and their counterparts in other digit lengths or in bases other than 10) are called "Kaprekar constants".
</p>
<h2><span id="Other_properties">Other properties</span><span><span>[</span><a href="https://en.wikipedia.org/w/index.php?title=6174&amp;action=edit&amp;section=2" title="Edit section: Other properties"><span>edit</span></a><span>]</span></span></h2>
<ul><li>6174 is a 7-<a href="https://en.wikipedia.org/wiki/Smooth_number" title="Smooth number">smooth number</a>, i.e. none of its prime factors are greater than 7.</li>
<li>6174 can be written as the sum of the first three powers of 18:
<ul><li>18<sup>3</sup> + 18<sup>2</sup> + 18<sup>1</sup> = 5832 + 324 + 18 = 6174, and coincidentally, 6 + 1 + 7 + 4 = 18.</li></ul></li>
<li>The sum of squares of the prime factors of 6174 is a square:
<ul><li>2<sup>2</sup> + 3<sup>2</sup> + 3<sup>2</sup> + 7<sup>2</sup> + 7<sup>2</sup> + 7<sup>2</sup> = 4 + 9 + 9 + 49 + 49 + 49 = 169 = 13<sup>2</sup></li></ul></li></ul>
<h2><span id="References">References</span><span><span>[</span><a href="https://en.wikipedia.org/w/index.php?title=6174&amp;action=edit&amp;section=3" title="Edit section: References"><span>edit</span></a><span>]</span></span></h2>
<div><ol>
<li id="cite_note-1"><span><b><a href="#cite_ref-1">^</a></b></span> <span><cite id="CITEREFNishiyama2006"><a href="https://en.wikipedia.org/wiki/Yutaka_Nishiyama" title="Yutaka Nishiyama">Nishiyama, Yutaka</a> (March 2006). <a rel="nofollow" href="http://plus.maths.org/issue38/features/nishiyama/index.html">"Mysterious number 6174"</a>. <i><a href="https://en.wikipedia.org/wiki/Plus_Magazine" title="Plus Magazine">Plus Magazine</a></i>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=unknown&amp;rft.jtitle=Plus+Magazine&amp;rft.atitle=Mysterious+number+6174&amp;rft.date=2006-03&amp;rft.aulast=Nishiyama&amp;rft.aufirst=Yutaka&amp;rft_id=http%3A%2F%2Fplus.maths.org%2Fissue38%2Ffeatures%2Fnishiyama%2Findex.html&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3A6174"></span></span>
</li>
<li id="cite_note-Kaprekar1955-2"><span><b><a href="#cite_ref-Kaprekar1955_2-0">^</a></b></span> <span><cite id="CITEREFKaprekar_DR1955">Kaprekar DR (1955). "An Interesting Property of the Number 6174". <i><a href="https://en.wikipedia.org/wiki/Scripta_Mathematica" title="Scripta Mathematica">Scripta Mathematica</a></i>. <b>15</b>: 244–245.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Scripta+Mathematica&amp;rft.atitle=An+Interesting+Property+of+the+Number+6174&amp;rft.volume=15&amp;rft.pages=244-245&amp;rft.date=1955&amp;rft.au=Kaprekar+DR&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3A6174"></span></span>
</li>
<li id="cite_note-Kaprekar1980-3"><span><b><a href="#cite_ref-Kaprekar1980_3-0">^</a></b></span> <span><cite id="CITEREFKaprekar_DR1980">Kaprekar DR (1980). "On Kaprekar Numbers". <i>Journal of Recreational Mathematics</i>. <b>13</b> (2): 81–82.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Journal+of+Recreational+Mathematics&amp;rft.atitle=On+Kaprekar+Numbers&amp;rft.volume=13&amp;rft.issue=2&amp;rft.pages=81-82&amp;rft.date=1980&amp;rft.au=Kaprekar+DR&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3A6174"></span></span>
</li>
<li id="cite_note-mathworld-4"><span><b><a href="#cite_ref-mathworld_4-0">^</a></b></span> <span><a href="https://en.wikipedia.org/wiki/Kaprekar%27s_routine#CITEREFHanover2017" title="Kaprekar's routine">Hanover 2017</a>, p. 1, Overview.</span>
</li>
<li id="cite_note-5"><span><b><a href="#cite_ref-5">^</a></b></span> <span><cite><a rel="nofollow" href="https://www.cut-the-knot.org/Curriculum/Arithmetic/Kaprekar.shtml">"Kaprekar's Iterations and Numbers"</a>. <i>www.cut-the-knot.org</i><span>. Retrieved <span>2022-09-21</span></span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=unknown&amp;rft.jtitle=www.cut-the-knot.org&amp;rft.atitle=Kaprekar%27s+Iterations+and+Numbers&amp;rft_id=https%3A%2F%2Fwww.cut-the-knot.org%2FCurriculum%2FArithmetic%2FKaprekar.shtml&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3A6174"></span></span>
</li>
</ol></div>
<h2><span id="External_links">External links</span><span><span>[</span><a href="https://en.wikipedia.org/w/index.php?title=6174&amp;action=edit&amp;section=4" title="Edit section: External links"><span>edit</span></a><span>]</span></span></h2>

<ul><li><cite id="CITEREFBowley">Bowley, Roger. <a rel="nofollow" href="https://www.youtube.com/watch?v=d8TRcZklX_Q">"6174 is Kaprekar's Constant"</a>. <i>Numberphile</i>. <a href="https://en.wikipedia.org/wiki/University_of_Nottingham" title="University of Nottingham">University of Nottingham</a>: <a href="https://en.wikipedia.org/wiki/Brady_Haran" title="Brady Haran">Brady Haran</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=unknown&amp;rft.jtitle=Numberphile&amp;rft.atitle=6174+is+Kaprekar%27s+Constant&amp;rft.aulast=Bowley&amp;rft.aufirst=Roger&amp;rft_id=https%3A%2F%2Fwww.youtube.com%2Fwatch%3Fv%3Dd8TRcZklX_Q&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3A6174"></span></li>
<li><a rel="nofollow" href="http://www.panix.com/~baldwin/kaprekar.pl">Sample (Perl) code to walk any four-digit number to Kaprekar's Constant</a></li>
<li><a rel="nofollow" href="http://www.panix.com/~baldwin/kaprekar.py">Sample (Python) code to walk any four-digit number to Kaprekar's Constant</a></li>
<li><a rel="nofollow" href="https://gist.github.com/dakonr/10865d113dd2ca6a7dd3d3fcdb169c13#file-kaprekar_constant-c">Sample (C) code to walk the first 10000 numbers and their steps to Kaprekar's Constant</a></li></ul>
<!-- 
NewPP limit report
Parsed by mw1364
Cached time: 20240108183432
Cache expiry: 2592000
Reduced expiry: false
Complications: [vary‐revision‐sha1, show‐toc]
CPU time usage: 0.261 seconds
Real time usage: 0.360 seconds
Preprocessor visited node count: 1434/1000000
Post‐expand include size: 26512/2097152 bytes
Template argument size: 1448/2097152 bytes
Highest expansion depth: 14/100
Expensive parser function count: 3/500
Unstrip recursion depth: 1/20
Unstrip post‐expand size: 18080/5000000 bytes
Lua time usage: 0.156/10.000 seconds
Lua memory usage: 4900527/52428800 bytes
Number of Wikibase entities loaded: 0/400
-->
<!--
Transclusion expansion time report (%,ms,calls,template)
100.00%  334.287      1 -total
 47.53%  158.872      1 Template:Infobox_number
 30.39%  101.591      1 Template:Infobox_number/box
 29.60%   98.944      1 Template:Infobox
 28.58%   95.525      1 Template:Reflist
 22.44%   75.016      3 Template:Cite_web
 16.07%   53.705      1 Template:Short_description
 14.14%   47.272      1 Template:Commons_category
 13.68%   45.723      1 Template:Infobox_number/range
 13.49%   45.097      1 Template:Sister_project
-->

<!-- Saved in parser cache with key enwiki:pcache:idhash:52724119-0!canonical and timestamp 20240108183431 and revision id 1186034556. Rendering was triggered because: page-view
 -->
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Web AI Model Testing: WebGPU, WebGL, and Headless Chrome (190 pts)]]></title>
            <link>https://developer.chrome.com/blog/supercharge-web-ai-testing</link>
            <guid>39017607</guid>
            <pubDate>Tue, 16 Jan 2024 19:16:12 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://developer.chrome.com/blog/supercharge-web-ai-testing">https://developer.chrome.com/blog/supercharge-web-ai-testing</a>, See on <a href="https://news.ycombinator.com/item?id=39017607">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>

  
    




<div>
        
          <p><img alt="François Beaufort" src="https://web.dev/images/authors/beaufortfrancois.jpg" decoding="async" height="64" loading="lazy" width="64"></p>
      </div>

<p>Great news! You've built a cool <a href="https://goo.gle/made-with-tfjs">Web AI application</a> 
that runs machine learning models directly on a user's device. It runs entirely
on the client-side web browser, without relying on the cloud. This on-device
design enhances user privacy, boosts performance, and reduces costs
significantly.</p>

<p>However, there's a hurdle. Your
<a href="https://www.tensorflow.org/js">TensorFlow.js</a> model can operate on
both CPUs (WebAssembly) and more powerful GPUs (through
<a href="https://developer.mozilla.org/docs/Web/API/WebGL_API">WebGL</a> and
<a href="https://developer.chrome.com/blog/webgpu-release">WebGPU</a>). The question is:
<strong>how can you consistently automate browser testing with the selected hardware?</strong></p>

<p>Maintaining consistency is crucial for comparing machine learning model
performance over time as you iterate and improve them, prior to deployment for
real-world users to use on their device.</p>

<p>Setting up a consistent testing environment with GPUs can be harder than
expected. In this blog post, we'll share the problems we faced and how we solved
them, so you can improve your application's performance.</p>

<p>This isn't just for Web AI developers! If you're working on web gaming or
graphics, this post is valuable for you, too.</p>

<h2 id="what's-automation" data-text="What's in our automation toolbox" tabindex="-1">What's in our automation toolbox</h2>

<p>Here is what we are using:</p>

<ul>
<li><strong>Environment</strong>: A Linux-based Google Colab
<a href="https://colab.google/notebooks/">notebook</a> connected to an NVIDIA
T4 or V100 GPU. You can use other cloud platforms, such as Google Cloud
(GCP), if preferred.</li>
<li><strong>Browser</strong>: Chrome supports <a href="https://developer.chrome.com/docs/web-platform/webgpu">WebGPU</a>,
a powerful <a href="https://developer.chrome.com/blog/from-webgl-to-webgpu">successor to WebGL</a>, that
brings the advancements of modern GPU APIs to the web.</li>
<li><strong>Automation</strong>: <a href="https://developer.chrome.com/docs/puppeteer">Puppeteer</a> is a Node.js library that lets
you control browsers programmatically with JavaScript. With Puppeteer, we can
automate Chrome in headless mode, which means the browser runs without a
visible interface, on a server. We are using the improved
<a href="https://developer.chrome.com/docs/chromium/new-headless">new headless mode</a>, not the
<a href="https://developer.chrome.com/blog/headless-chrome">legacy</a> form.</li>
</ul>



<h2 id="verify-environment" data-text="Verify the environment" tabindex="-1">Verify the environment</h2>

<p>The best way to check whether hardware acceleration is turned on in Chrome is to
type <code translate="no" dir="ltr">chrome://gpu</code> into the address bar. You can
programmatically <a href="https://github.com/jasonmayes/headless-chrome-nvidia-t4-gpu-support/blob/main/examples/puppeteer/jPuppet.js">perform the equivalent with Puppeteer</a> 
with <code translate="no" dir="ltr">console.log</code> or save the full report as PDF to check manually:</p>
<pre translate="no" dir="ltr"><code translate="no" dir="ltr">/* Incomplete example.js */
import puppeteer from 'puppeteer';

// Configure launch parameters: Expands later
const browser = await puppeteer.launch({
  headless: 'new',
  args:  ['--no-sandbox']
});

const page = await browser.newPage();
await page.goto('chrome://gpu');

// Verify: log the WebGPU status or save the GPU report as PDF
const txt = await page.waitForSelector('text/WebGPU');
const status = await txt.evaluate(g =&gt; g.parentElement.textContent);
console.log(status);
await page.pdf({ path: './gpu.pdf' });

await browser.close();
</code></pre>
<p>Open <code translate="no" dir="ltr">chrome://gpu</code> and you should have the following results:</p>

<table>
  <tbody><tr>
    <th colspan="2">Graphics feature status</th>
  </tr>
  <tr>
    <td>OpenGL:</td>
    <td><span>Disabled</span></td>
  </tr>
  <tr>
    <td>Vulkan:</td>
    <td><span>Disabled</span></td>
  </tr>
  <tr>
    <td>WebGL:</td>
    <td><span>Software only, hardware acceleration unavailable.</span></td>
  </tr>
  <tr>
    <td>WebGL2:</td>
    <td><span>Software only, hardware acceleration unavailable.</span></td>
  </tr>
  <tr>
    <td>WebGPU:</td>
    <td><span>Disabled</span></td>
  </tr>
  <tr>
    <td colspan="2">
      <p><b>Problems detected.</b><br>WebGPU has been disabled via blocklist or the command line.</p>
    </td>
  </tr>
</tbody></table>

<p>Not a great start. It's fairly clear that hardware detection was failing.
<strong>WebGL, WebGL2, and WebGPU are essentially disabled or software only</strong>. We
aren't alone in this problem - there are numerous discussions online of people
in a similar situation, including on the official Chrome support channels
(<a href="https://bugs.chromium.org/p/chromium/issues/detail?id=1459930">1</a>),
(<a href="https://bugs.chromium.org/p/chromium/issues/detail?id=765284">2</a>).</p>

<h2 id="enable-webgpu" data-text="Enable WebGPU and WebGL support" tabindex="-1">Enable WebGPU and WebGL support</h2>

<p>By default, Headless Chrome
<a href="https://bugs.chromium.org/p/chromium/issues/detail?id=1416283">disables GPU</a>.
To enable it on Linux, apply all of the following flags when launching Headless
Chrome:</p>

<ul>
<li><strong><code translate="no" dir="ltr">--no-sandbox</code></strong> flag disables <a href="https://developer.chrome.com/docs/puppeteer/troubleshooting#set_up_a_chrome_linux_sandbox">Chrome's security sandbox</a>, which isolates the
browser process from the rest of the system. Running Chrome as root without
this sandbox is not supported.</li>
<li><strong><code translate="no" dir="ltr">--headless=new</code></strong> flag runs Chrome with the new and improved
<a href="https://developer.chrome.com/docs/chromium/new-headless">headless mode</a>, without any visible UI.</li>
<li><strong><code translate="no" dir="ltr">--use-angle=vulkan</code></strong> flag tells Chrome to use the
<a href="https://chromium.googlesource.com/angle/angle/+/HEAD/src/libANGLE/renderer/vulkan/README.md">Vulkan backend</a> 
for <a href="https://chromium.googlesource.com/angle/angle/">ANGLE</a>, which
translates OpenGL ES 2/3 calls to Vulkan API calls.</li>
<li><strong><code translate="no" dir="ltr">--enable-features=Vulkan</code></strong> flag enables Vulkan graphics backend for
compositing and rasterization in Chrome.</li>
<li><strong><code translate="no" dir="ltr">--disable-vulkan-surface</code></strong> flag disables the <code translate="no" dir="ltr">VK_KHR_surface</code> vulkan
instance extension. Instead of using a swapchain,
<a href="https://en.wikipedia.org/wiki/Bit_blit">Bit blit</a> is used for the
present render result on screen.</li>
<li><strong><code translate="no" dir="ltr">--enable-unsafe-webgpu</code></strong> flag enables the experimental WebGPU API in
Chrome on Linux and disables the adapters blocklist.</li>
</ul>

<p>Now we combine all the changes we have made so far. Here is the complete script.</p>
<pre translate="no" dir="ltr"><code translate="no" dir="ltr">/* Complete example.js */
import puppeteer from 'puppeteer';

// Configure launch parameters
const browser = await puppeteer.launch({
  headless: 'new',
  args: [
    '--no-sandbox',
    '--headless=new',
    '--use-angle=vulkan',
    '--enable-features=Vulkan',
    '--disable-vulkan-surface',
    '--enable-unsafe-webgpu',
  ]
});

const page = await browser.newPage();
await page.goto('chrome://gpu');

// Verify: log the WebGPU status or save the GPU report as PDF
const txt = await page.waitForSelector('text/WebGPU');
const status = await txt.evaluate(g =&gt; g.parentElement.textContent);
console.log(status);
await page.pdf({path: './gpu.pdf'});

await browser.close();
</code></pre>
<p>Run the script again. No WebGPU problems are detected and the value changes from
disabled to software only.</p>

<table>
  <tbody><tr>
    <th colspan="2">Graphics feature status</th>
  </tr>
  <tr>
    <td>OpenGL:</td>
    <td><span>Disabled</span></td>
  </tr>
  <tr>
    <td>Vulkan:</td>
    <td><span>Disabled</span></td>
  </tr>
  <tr>
    <td>WebGL:</td>
    <td><span>Software only, hardware acceleration unavailable.</span></td>
  </tr>
  <tr>
    <td>WebGL2:</td>
    <td><span>Software only, hardware acceleration unavailable.</span></td>
  </tr>
  <tr>
    <td>WebGPU:</td>
    <td><span>Software only, hardware acceleration unavailable.</span></td>
  </tr>
</tbody></table>

<p>However, hardware acceleration is still unavailable, the NVIDIA T4 GPU isn't
detected.</p>

<h2 id="install-drivers" data-text="Install the correct GPU drivers" tabindex="-1">Install the correct GPU drivers</h2>

<p>We investigated more closely the output of <code translate="no" dir="ltr">chrome://gpu</code>, with some GPU experts
on the Chrome team. We found issues with the <a href="https://github.com/googlecolab/colabtools/issues/3556#issuecomment-1499397023">default drivers installed on the
Linux Colab</a> 
instance, causing issues with Vulkan, leading to Chrome unable to detect the
NVIDIA T4 GPU at the <code translate="no" dir="ltr">GL_RENDERER</code> level as shown in the following output. This
causes problems with Headless Chrome.</p>

<table>
  <caption>The default output doesn't detect NVIDIA T4 GPU.</caption>
  <tbody><tr>
    <th colspan="2">Driver information</th>
  </tr>
  <tr>
    <td>GL_RENDERER</td>
    <td>ANGLE (Google, Vulkan 1.3.0 (SwiftShader Device (Subzero) (0x0000C0DE)), SwiftShader driver-5.0.0)</td>
  </tr>
</tbody></table>

<p>Installing the correct drivers that were compatible therefore fixes the issue.</p>

<table>
  <caption>Updated output after drivers are installed.</caption>
  <tbody><tr>
    <th colspan="2">Driver information</th>
  </tr>
  <tr>
    <td>GL_RENDERER</td>
    <td>ANGLE (NVIDIA Corporation, Tesla T4/PCIe/SSE2, OpenGL ES 3.2 NVIDIA 525.105.17)</td>
  </tr>
</tbody></table>

<p>To install the correct drivers, run the following commands during setup. The
last two lines help you to log the outputs of what NVIDIA drivers detects along
with <code translate="no" dir="ltr">vulkaninfo</code>.</p>

<pre translate="no" dir="ltr"><code translate="no" dir="ltr">apt-get install -y vulkan-tools libnvidia-gl-525
</code>
<code translate="no" dir="ltr">// Verify the NVIDIA drivers detects along with vulkaninfo</code>
<code translate="no" dir="ltr">nvidia-smi</code>
<code translate="no" dir="ltr">vulkaninfo --summary</code>
</pre>

<p>Now run the script again and we get the following result. 🎉</p>

<table>
  <tbody><tr>
    <th colspan="2">Graphics feature status</th>
  </tr>
  <tr>
    <td>OpenGL:</td>
    <td><span>Enabled</span></td>
  </tr>
  <tr>
    <td>Vulkan:</td>
    <td><span>Enabled</span></td>
  </tr>
  <tr>
    <td>WebGL:</td>
    <td><span>Hardware accelerated but at reduced performance.</span></td>
  </tr>
  <tr>
    <td>WebGL2:</td>
    <td><span>Hardware accelerated but at reduced performance.</span></td>
  </tr>
  <tr>
    <td>WebGPU:</td>
    <td><span>Hardware accelerated but at reduced performance.</span></td>
  </tr>
</tbody></table>

<p>By using the correct drivers and flags when running Chrome, we now have WebGPU
and WebGL support using the shiny, new headless mode.</p>

<h2 id="investigation" data-text="Behind the scenes: Our team's investigation" tabindex="-1">Behind the scenes: Our team's investigation</h2>

<p>After much research, we didn't find working methods for the environment we
needed to execute in Google Colab, although there were some
<a href="https://mirzabilal.com/how-to-enable-hardware-acceleration-on-chrome-chromium-puppeteer-on-aws-in-headless-mode">hopeful posts</a> 
that worked in other environments, which was promising. Ultimately, we weren't
able to replicate their success in the Colab NVIDIA T4 environment, as we had 2
key issues:</p>

<ol>
<li>Some combinations of flags allow detection of the GPU, but don't allow you to
actually use the GPU.</li>
<li>Examples of working solutions by third parties used the old Chrome headless
version, which at some point will be deprecated in favor of the
<a href="https://developer.chrome.com/docs/chromium/new-headless">new version</a>. We needed a solution
that worked with the new Headless Chrome to be better future proofed.</li>
</ol>

<p>We confirmed the under utilization of the GPU by running an
<a href="https://tensorflowjs-fashion-mnist-classifier.glitch.me/">example TensorFlow.js web page for image recognition</a>,
whereby we trained a model to recognize clothing samples (sort of like a "hello
world" of machine learning).</p>

<p>On a regular machine, 50 training cycles (known as epochs) should run in less
than 1 second each. Calling Headless Chrome in its default state, we could log
the JavaScript console output to the Node.js server-side command line to see how
fast these training cycles were actually taking.</p>

<p>As expected, each training epoch took much longer than expected (several
seconds), which suggests Chrome has fallen back to plain old JS CPU execution
instead of utilizing the GPU:</p>

<figure>
  <img src="https://developer.chrome.com/blog/supercharge-web-ai-testing/image/initial_epoch_timing.gif" alt="The training epochs move at a slower cadence.">
  <figcaption><b>Figure 1</b>: Real-time capture showing how long each training epoch took to execute (seconds).</figcaption>
</figure>

<p>After fixing the drivers and using the right combination of flags for Headless
Chrome, rerunning the TensorFlow.js training example results in much faster
training epochs.</p>

<figure>
  <img src="https://developer.chrome.com/blog/supercharge-web-ai-testing/image/epoch_timing_with_headless.gif" alt="There's an increase in speed for epochs..">
  <figcaption><b>Figure 2</b>: Real-time capture showing the speed up of epochs.</figcaption>
</figure>

<h2 id="summary" data-text="Summary" tabindex="-1">Summary</h2>

<p><a href="https://www.youtube.com/watch?v=r7hOoCY6uGo">Web AI has grown exponentially</a> 
since its creation in 2017. With browser technologies such as WebGPU, WebGL, and
<a href="https://webassembly.org/">WebAssembly</a>, a machine learning model's
mathematical operations can be further accelerated on the client side.</p>

<p>As of 2023 TensorFlow.js and MediaPipe Web crossed over 1 billion downloads of
models and libraries—a historic milestone and a sign of how web
developers and engineers are shifting to embrace <a href="https://goo.gle/made-with-tfjs">AI in their next generation
web apps to make some truly incredible solutions</a>.</p>

<p>With great success in usage comes great responsibility. At this level of usage
in production systems, the need arises for testing client-side, browser-based AI
models in a true browser environment, while also being scalable, automatable,
and within a known standardized hardware setup.</p>



<p>By harnessing the combined power of the new Headless Chrome and Puppeteer, you
can confidently test such workloads in a standardized and replicable
environment, ensuring consistent and reliable results.</p>

<h2 id="wrap" data-text="Wrap up" tabindex="-1">Wrap up</h2>

<p>A <a href="https://developer.chrome.com/docs/web-platform/webgpu/colab-headless">step-by-step guide</a> is available in
our documentation, so you can try out the complete setup yourself.</p>

<p>If you found this useful, drop a shout out over on
<a href="https://www.linkedin.com/in/WebAI">LinkedIn</a>,
<a href="https://twitter.com/jason_mayes">X (formerly Twitter)</a>, or whatever
social network you use using hashtag <strong>#WebAI</strong>. It would be great to hear any
feedback you have so we know to write more stuff like this in the future.</p>

<p><a href="https://github.com/jasonmayes/headless-chrome-nvidia-t4-gpu-support">Add a star on the Github repo</a> 
to receive any future updates.</p>

<h2 id="acknowledgements" data-text="Acknowledgements" tabindex="-1">Acknowledgements</h2>

<p>A huge thank you to everyone on the Chrome team who helped debug the driver and
WebGPU issues we faced in this solution, with a special thanks to
<a href="https://jec.fish/">Jecelyn Yeen</a> and
<a href="https://heyawhite.com/">Alexandra White</a> for helping to wordsmith
this blog post. Thanks to Yuly Novikov, Andrey Kosyakov, and
<a href="https://mastodon.online/@orkon">Alex Rudenko</a> who were instrumental
in creating the final, working solution.</p>

  

  
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Infowars and Goop sell the same exact pseudoscientific "wellness" products (164 pts)]]></title>
            <link>https://qz.com/1010684/all-the-wellness-products-american-love-to-buy-are-sold-on-both-infowars-and-goop</link>
            <guid>39017350</guid>
            <pubDate>Tue, 16 Jan 2024 19:01:16 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://qz.com/1010684/all-the-wellness-products-american-love-to-buy-are-sold-on-both-infowars-and-goop">https://qz.com/1010684/all-the-wellness-products-american-love-to-buy-are-sold-on-both-infowars-and-goop</a>, See on <a href="https://news.ycombinator.com/item?id=39017350">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>There are two Americas, we’ve been told.</p><p><span><a data-ga="[[&quot;Embedded Url&quot;,&quot;External link&quot;,&quot;https://www.nytimes.com/interactive/2016/12/26/upshot/duck-dynasty-vs-modern-family-television-maps.html&quot;,{&quot;metric25&quot;:1}]]" href="https://www.nytimes.com/interactive/2016/12/26/upshot/duck-dynasty-vs-modern-family-television-maps.html" target="_blank" rel="noopener noreferrer">There’s</a></span> <em>Duck Dynasty</em> America and <em>Modern Family</em> America.<span><a data-ga="[[&quot;Embedded Url&quot;,&quot;External link&quot;,&quot;https://qz.com/836813&quot;,{&quot;metric25&quot;:1}]]" href="https://qz.com/836813" target="_blank" rel="noopener noreferrer"> There’s</a></span> “gosh” America and “dope” America. Sometimes, though, Americans unite around a common idea. Like the healing powers of eleuthero root, cordyceps mushrooms, and “nascent iodine.”</p><p>Near the end of <span><a data-ga="[[&quot;Embedded Url&quot;,&quot;External link&quot;,&quot;https://www.nytimes.com/2017/05/25/magazine/how-amanda-chantal-bacon-perfected-the-celebrity-wellness-business.html&quot;,{&quot;metric25&quot;:1}]]" href="https://www.nytimes.com/2017/05/25/magazine/how-amanda-chantal-bacon-perfected-the-celebrity-wellness-business.html" target="_blank" rel="noopener noreferrer">a profile</a></span> of Amanda Chantal Bacon, founder of the “wellness” brand Moon Juice, the <em>New York Times Magazine</em> noted that many of the alternative-medicine ingredients in her products are sold—with very different branding—on the Infowars store. That’s the site run by Alex Jones, the radio show host and conspiracy theorist who<span><a data-ga="[[&quot;Embedded Url&quot;,&quot;External link&quot;,&quot;https://www.washingtonpost.com/lifestyle/style/how-alex-jones-conspiracy-theorist-extraordinaire-got-donald-trumps-ear/2016/11/17/583dc190-ab3e-11e6-8b45-f8e493f06fcd_story.html&quot;,{&quot;metric25&quot;:1}]]" href="https://www.washingtonpost.com/lifestyle/style/how-alex-jones-conspiracy-theorist-extraordinaire-got-donald-trumps-ear/2016/11/17/583dc190-ab3e-11e6-8b45-f8e493f06fcd_story.html" target="_blank" rel="noopener noreferrer"> has said</a></span> that both the shooting at Sandy Hook Elementary School and the Boston Marathon bombing were staged. Moon Juice is<span><a data-ga="[[&quot;Embedded Url&quot;,&quot;External link&quot;,&quot;https://shop.goop.com/shop/collection/brands/moon-juice&quot;,{&quot;metric25&quot;:1}]]" href="https://shop.goop.com/shop/collection/brands/moon-juice" target="_blank" rel="noopener noreferrer"> frequently recommended</a></span> by Gwyneth Paltrow’s wellness blog, Goop; &nbsp;it’s a favorite of Hollywood celebrities and others who can afford things like $25 “<span><a data-ga="[[&quot;Embedded Url&quot;,&quot;External link&quot;,&quot;https://www.moonjuiceshop.com/products/cashews&quot;,{&quot;metric25&quot;:1}]]" href="https://www.moonjuiceshop.com/products/cashews" target="_blank" rel="noopener noreferrer">activated cashews</a></span>.” Infowars, on the other hand, is a dark corner of the American right, heavy on guns, light on government intervention, and still very mad at Obama.</p><p>We at Quartz have created a compendium, from Ashwagandha to zizyphus, of the magical healing ingredients both sides of the political spectrum are buying, and how they are presented to each. We looked at the ingredients used in products sold on the<span><a data-ga="[[&quot;Embedded Url&quot;,&quot;External link&quot;,&quot;https://www.infowarsstore.com/&quot;,{&quot;metric25&quot;:1}]]" href="https://www.infowarsstore.com/" target="_blank" rel="noopener noreferrer"> Infowars store</a></span>, and compared them to products on the wellness shops<span><a data-ga="[[&quot;Embedded Url&quot;,&quot;External link&quot;,&quot;https://www.moonjuiceshop.com/&quot;,{&quot;metric25&quot;:1}]]" href="https://www.moonjuiceshop.com/" target="_blank" rel="noopener noreferrer"> Moon Juice</a></span> and&nbsp;<span><a data-ga="[[&quot;Embedded Url&quot;,&quot;External link&quot;,&quot;https://shop.goop.com/shop&quot;,{&quot;metric25&quot;:1}]]" href="https://shop.goop.com/shop" target="_blank" rel="noopener noreferrer">Goop</a></span>. &nbsp;All make similar claims about the health benefits of these ingredients, but what gets called “Super Male Vitality” by Infowars is branded as “Sex Dust” by Moon Juice.</p><h2 id="h24646"><a id=""></a>Ashwagandha</h2><p>Ashwagandha is an herb commonly used in Indian Ayurvedic medicine. In the wellness world it is purported to have all kinds of benefits—<span><a data-ga="[[&quot;Embedded Url&quot;,&quot;External link&quot;,&quot;https://draxe.com/ashwagandha-proven-to-heal-thyroid-and-adrenals/&quot;,{&quot;metric25&quot;:1}]]" href="https://draxe.com/ashwagandha-proven-to-heal-thyroid-and-adrenals/" target="_blank" rel="noopener noreferrer">everything from</a></span> reducing stress to preventing cancer. According to Goop, it “tonifies the immune system,” whatever that means. (The Oxford Dictionary <span><a data-ga="[[&quot;Embedded Url&quot;,&quot;External link&quot;,&quot;https://en.oxforddictionaries.com/definition/tonify&quot;,{&quot;metric25&quot;:1}]]" href="https://en.oxforddictionaries.com/definition/tonify" target="_blank" rel="noopener noreferrer">says</a></span> that means “increase the available energy of (a bodily part or system).”) Infowars says it is “rejuvenative.” Animal studies in the lab <span><a data-ga="[[&quot;Embedded Url&quot;,&quot;External link&quot;,&quot;https://www.mskcc.org/cancer-care/integrative-medicine/herbs/ashwagandha&quot;,{&quot;metric25&quot;:1}]]" href="https://www.mskcc.org/cancer-care/integrative-medicine/herbs/ashwagandha" target="_blank" rel="noopener noreferrer">suggest</a></span>&nbsp;Ashwagandha may be effective for treating cancer, diabetes, and somehow, both reducing fatigue and as a sedative, but these effects have not been thoroughly tested on humans.</p><p><strong>Goop: Recommended in </strong><span><a data-ga="[[&quot;Embedded Url&quot;,&quot;External link&quot;,&quot;http://goop.com/magic-potions-for-clarity-beauty-and-energy/&quot;,{&quot;metric25&quot;:1}]]" href="http://goop.com/magic-potions-for-clarity-beauty-and-energy/" target="_blank" rel="noopener noreferrer"><strong>Magic Potions for Clarity, Beauty, and Energy</strong></a></span></p><blockquote data-type="BlockQuote"><p>Tonifies the immune system; inspires vigor and strength; relieves mental, emotional, and physical stress; and harmonizes mind, body, and spirit.</p></blockquote><p><strong>Infowars:&nbsp;</strong><span><a data-ga="[[&quot;Embedded Url&quot;,&quot;External link&quot;,&quot;https://www.infowarsstore.com/heirloom-organics-professional-medicine-pack.html&quot;,{&quot;metric25&quot;:1}]]" href="https://www.infowarsstore.com/heirloom-organics-professional-medicine-pack.html" target="_blank" rel="noopener noreferrer"><strong>Heirloom Organics Professional Medicine Pack</strong></a></span></p><blockquote data-type="BlockQuote"><p>Ashwagandha is regarded as one of the great rejuvenative [sic] herbs of India. According to Ayurveda, the traditional healing system of India, the root of this low-growing shrub is said to be effective for a host of debilitated [sic] conditions, including general weakness, impotence, infertility, and others. Ashwaganda is sometimes described as Indian Ginseng for the significance of this botanical in Indian pharmacopoeia.</p></blockquote><h2 id="h24647"><a id=""></a>Bacopa</h2><p>Also an Ayurvedic herb, said to reduce stress, improve memory, and treat epilepsy, among other purported benefits. Goop uses bacopa in a supplement pack called “Why am I so Effing Tired;” Infowars sticks it in its “Brain Force Plus.” The science, based on animal studies, <span><a data-ga="[[&quot;Embedded Url&quot;,&quot;External link&quot;,&quot;https://www.uofmhealth.org/health-library/hn-4019005&quot;,{&quot;metric25&quot;:1}]]" href="https://www.uofmhealth.org/health-library/hn-4019005" target="_blank" rel="noopener noreferrer">shows</a></span>&nbsp;some preliminary—but contradictory—evidence of improvements to memory and brain function. There is minimal support for the claims about epilepsy and anxiety.</p><figure data-id="e7e1777b17caddcb17d1d8480c011a7f" data-recommend-id="image://e7e1777b17caddcb17d1d8480c011a7f" data-format="jpg" data-width="1600" data-height="900" data-lightbox="false" data-recommended="false" data-hide="false" contenteditable="false" draggable="false"><div contenteditable="false" data-link-reference="" data-link-target="" data-syndicationrights="true" data-imagerights="other-license" data-notes="" data-hide="false" data-hidecredit="false"><div><picture><source media="(max-width: 37.31em)" type="image/jpeg" srcset="data:image/gif;base64,R0lGODlhAQABAAAAACH5BAEKAAEALAAAAAABAAEAAAICTAEAOw==" data-srcset="https://i.kinja-img.com/image/upload/c_fit,q_60,w_645/e7e1777b17caddcb17d1d8480c011a7f.jpg"><source media="(min-width: 37.37em)" type="image/jpeg" srcset="data:image/gif;base64,R0lGODlhAQABAAAAACH5BAEKAAEALAAAAAABAAEAAAICTAEAOw==" data-srcset="https://i.kinja-img.com/image/upload/c_fit,q_60,w_645/e7e1777b17caddcb17d1d8480c011a7f.jpg"><img alt="Image for article titled All the “wellness” products Americans love to buy are sold on both Infowars and Goop" data-chomp-id="e7e1777b17caddcb17d1d8480c011a7f" data-format="jpg" data-alt="Image for article titled All the “wellness” products Americans love to buy are sold on both Infowars and Goop" data-anim-src="" data-src="https://i.kinja-img.com/image/upload/c_fit,q_60,w_645/e7e1777b17caddcb17d1d8480c011a7f.jpg" src="https://i.kinja-img.com/image/upload/c_fit,q_60,w_645/e7e1777b17caddcb17d1d8480c011a7f.jpg"></picture></div><p><figcaption>Image<!-- -->: <!-- -->Goop/Infowars</figcaption></p></div><span data-id="e7e1777b17caddcb17d1d8480c011a7f" data-recommend-id="image://e7e1777b17caddcb17d1d8480c011a7f" data-format="jpg" data-width="1600" data-height="900" data-lightbox="false" data-recommended="false" data-hide="false"></span></figure><p><strong>Goop:&nbsp;</strong><span><a data-ga="[[&quot;Embedded Url&quot;,&quot;External link&quot;,&quot;https://shop.goop.com/shop/products/why-am-i-so-effing-tired&quot;,{&quot;metric25&quot;:1}]]" href="https://shop.goop.com/shop/products/why-am-i-so-effing-tired" target="_blank" rel="noopener noreferrer"><strong>Why am I so effing tired</strong></a></span></p><blockquote data-type="BlockQuote"><p>Formulated with a variety of vitamins (including a high dose of the B’s) and supplements—many sourced from ancient Ayurveda—this helps re-balance an overtaxed system. Replenishing the nutrients you may be lacking may improve energy levels and diminish stress.</p></blockquote><p><strong>Infowars:&nbsp;</strong><span><a data-ga="[[&quot;Embedded Url&quot;,&quot;External link&quot;,&quot;https://www.infowarsstore.com/brain-force.html&quot;,{&quot;metric25&quot;:1}]]" href="https://www.infowarsstore.com/brain-force.html" target="_blank" rel="noopener noreferrer"><strong>Brain Force Plus</strong></a></span></p><blockquote data-type="BlockQuote"><p>Top scientists and researchers agree: we are being hit by toxic weapons in the food and water supply that are making us fat, sick, and stupid. It’s time to fight back with Brain Force Plus, the next generation of advanced neural activation.</p></blockquote><h2 id="h24648"><a id=""></a>Chaga mushroom</h2><p>Most of these wellness sites provide a long list of potential benefits from ingredients like the chaga mushroom. <span><a data-ga="[[&quot;Embedded Url&quot;,&quot;External link&quot;,&quot;https://www.globalhealingcenter.com/natural-health/chaga-mushroom-the-immune-boosting-superfood/&quot;,{&quot;metric25&quot;:1}]]" href="https://www.globalhealingcenter.com/natural-health/chaga-mushroom-the-immune-boosting-superfood/" target="_blank" rel="noopener noreferrer">One site</a></span> promises that&nbsp;<em>Inonotus obliquus </em>offers&nbsp;immune system support, “soothing properties,” blood pressure normalization, “DNA damage protection,” and a few more unbelievable health benefits. Moon Juice calls the mushroom a “joy promoter.” Studies on animals <span><a data-ga="[[&quot;Embedded Url&quot;,&quot;External link&quot;,&quot;https://www.mskcc.org/cancer-care/integrative-medicine/herbs/chaga-mushroom&quot;,{&quot;metric25&quot;:1}]]" href="https://www.mskcc.org/cancer-care/integrative-medicine/herbs/chaga-mushroom" target="_blank" rel="noopener noreferrer">have shown</a></span>&nbsp;that chaga can “inhibit cancer progression” and “activate some types of immune cells,” but the consensus is that studies in humans are needed.</p><p><strong>Moon Juice:&nbsp;</strong><span><a data-ga="[[&quot;Embedded Url&quot;,&quot;External link&quot;,&quot;https://www.moonjuiceshop.com/products/chaga&quot;,{&quot;metric25&quot;:1}]]" href="https://www.moonjuiceshop.com/products/chaga" target="_blank" rel="noopener noreferrer"><strong>Chaga</strong></a></span></p><blockquote data-type="BlockQuote"><p>Our Chaga mushrooms contain bio-active beta glucans [these have been shown to live up to <span><a data-ga="[[&quot;Embedded Url&quot;,&quot;External link&quot;,&quot;https://www.uofmhealth.org/health-library/hn-2805007#hn-2805007-uses&quot;,{&quot;metric25&quot;:1}]]" href="https://www.uofmhealth.org/health-library/hn-2805007#hn-2805007-uses" target="_blank" rel="noopener noreferrer">some</a></span> of their promises]<strong>&nbsp;</strong>to support the body’s innate defense systems.</p></blockquote><p><strong>Infowars:&nbsp;</strong><span><a data-ga="[[&quot;Embedded Url&quot;,&quot;External link&quot;,&quot;https://www.infowarsstore.com/health-and-wellness/infowars-life/caveman-paleo-formula.html&quot;,{&quot;metric25&quot;:1}]]" href="https://www.infowarsstore.com/health-and-wellness/infowars-life/caveman-paleo-formula.html" target="_blank" rel="noopener noreferrer"><strong>Caveman True Paleo Formula</strong></a></span></p><blockquote data-type="BlockQuote"><p>The Ultimate In True Paleo Nutrition with Bone Broth, Turmeric Root, Chaga Mushroom, Bee Pollen, and other Ancient Supernutrients [sic, entire sentence].</p></blockquote><h2 id="h24649"><a id=""></a>Colloidal Silver</h2><p>Apparently colloidal silver is “a suspension of tiny silver particles in a liquid.” On Goop, Gwyneth Paltrow <span><a data-ga="[[&quot;Embedded Url&quot;,&quot;External link&quot;,&quot;http://goop.com/fly-better-tricks-for-better-plane-trips/&quot;,{&quot;metric25&quot;:1}]]" href="http://goop.com/fly-better-tricks-for-better-plane-trips/" target="_blank" rel="noopener noreferrer">writes</a></span>, “They say that active silver keeps germs at bay so I spray this in the air around me when I sit down.” The US National Institutes of Health, on the other hand, <span><a data-ga="[[&quot;Embedded Url&quot;,&quot;External link&quot;,&quot;https://nccih.nih.gov/health/silver&quot;,{&quot;metric25&quot;:1}]]" href="https://nccih.nih.gov/health/silver" target="_blank" rel="noopener noreferrer">says directly</a></span> that “claims made about the health benefits of taking colloidal silver aren’t backed up by studies,” adding, “colloidal silver can cause serious side effects.”</p><p><strong>Higher Nature:&nbsp;</strong><span><a data-ga="[[&quot;Embedded Url&quot;,&quot;External link&quot;,&quot;https://www.highernature.co.uk/Products/Colloidal-Silver-Spray&quot;,{&quot;metric25&quot;:1}]]" href="https://www.highernature.co.uk/Products/Colloidal-Silver-Spray" target="_blank" rel="noopener noreferrer"><strong>Colloidal Silver</strong></a></span><strong>&nbsp;(recommended by Goop)</strong></p><blockquote data-type="BlockQuote"><p>Our Colloidal silver contains pure, medical grade silver, which has been used for many centuries. It is produced using electro-controlled technology and pure water from a 9-stage water purification process, to achieve a very small particle size (0.0006 to 0.005 microns). This small particle size is important because it provides a much greater surface area and therefore more of an effective liquid.</p></blockquote><p><strong>Infowars:&nbsp;</strong><span><a data-ga="[[&quot;Embedded Url&quot;,&quot;External link&quot;,&quot;https://www.infowarsstore.com/silver-bullet-40-off.html&quot;,{&quot;metric25&quot;:1}]]" href="https://www.infowarsstore.com/silver-bullet-40-off.html" target="_blank" rel="noopener noreferrer"><strong>Silver Bullet</strong></a></span></p><blockquote data-type="BlockQuote"><p>The Infowars Life Silver Bullet Colloidal Silver is finally here following Alex’s extensive search for a powerful colloidal silver product that is both free of artificial additives and utilizes high quality processes to ensure for [sic] a truly unique product that has applications for both preparedness and regular use.</p></blockquote><h2 id="h24650"><a id=""></a>Cordyceps mushroom</h2><p>Another obscure fungus, this one used in traditional Chinese medicine. It <span><a data-ga="[[&quot;Embedded Url&quot;,&quot;External link&quot;,&quot;https://draxe.com/cordyceps/&quot;,{&quot;metric25&quot;:1}]]" href="https://draxe.com/cordyceps/" target="_blank" rel="noopener noreferrer">is purported</a></span> to “increase immune function,” act as a natural aphrodisiac, and improve stamina.&nbsp;According to Goop, it’s “an important Yang tonic,” which means it <span><a data-ga="[[&quot;Embedded Url&quot;,&quot;External link&quot;,&quot;http://goop.com/the-grandmothers-on-why-the-universe-is-out-of-whack/&quot;,{&quot;metric25&quot;:1}]]" href="http://goop.com/the-grandmothers-on-why-the-universe-is-out-of-whack/" target="_blank" rel="noopener noreferrer">provides</a></span> “masculine energy.” There is some <span><a data-ga="[[&quot;Embedded Url&quot;,&quot;External link&quot;,&quot;https://www.uofmhealth.org/health-library/hn-3936006#hn-3936006-uses&quot;,{&quot;metric25&quot;:1}]]" href="https://www.uofmhealth.org/health-library/hn-3936006#hn-3936006-uses" target="_blank" rel="noopener noreferrer">preliminary evidence</a></span> for the immune system thing, but other claims <span><a data-ga="[[&quot;Embedded Url&quot;,&quot;External link&quot;,&quot;https://www.mskcc.org/cancer-care/integrative-medicine/herbs/cordyceps&quot;,{&quot;metric25&quot;:1}]]" href="https://www.mskcc.org/cancer-care/integrative-medicine/herbs/cordyceps" target="_blank" rel="noopener noreferrer">are unproven</a></span>. Goop sells cordyceps as a dietary supplement; Infowars infuses them into its “Wake Up America” coffee.</p><figure data-id="c9abee5e13d318d785b7c08c8661985e" data-recommend-id="image://c9abee5e13d318d785b7c08c8661985e" data-format="jpg" data-width="1600" data-height="900" data-lightbox="false" data-recommended="false" data-hide="false" contenteditable="false" draggable="false"><div contenteditable="false" data-link-reference="" data-link-target="" data-syndicationrights="true" data-imagerights="other-license" data-notes="" data-hide="false" data-hidecredit="false"><div><picture><source media="(max-width: 37.31em)" type="image/jpeg" srcset="data:image/gif;base64,R0lGODlhAQABAAAAACH5BAEKAAEALAAAAAABAAEAAAICTAEAOw==" data-srcset="https://i.kinja-img.com/image/upload/c_fit,q_60,w_645/c9abee5e13d318d785b7c08c8661985e.jpg"><source media="(min-width: 37.37em)" type="image/jpeg" srcset="data:image/gif;base64,R0lGODlhAQABAAAAACH5BAEKAAEALAAAAAABAAEAAAICTAEAOw==" data-srcset="https://i.kinja-img.com/image/upload/c_fit,q_60,w_645/c9abee5e13d318d785b7c08c8661985e.jpg"><img alt="Image for article titled All the “wellness” products Americans love to buy are sold on both Infowars and Goop" data-chomp-id="c9abee5e13d318d785b7c08c8661985e" data-format="jpg" data-alt="Image for article titled All the “wellness” products Americans love to buy are sold on both Infowars and Goop" data-anim-src="" data-src="https://i.kinja-img.com/image/upload/c_fit,q_60,w_645/c9abee5e13d318d785b7c08c8661985e.jpg" src="https://i.kinja-img.com/image/upload/c_fit,q_60,w_645/c9abee5e13d318d785b7c08c8661985e.jpg"></picture></div><p><figcaption>Image<!-- -->: <!-- -->Goop/Infowars</figcaption></p></div><span data-id="c9abee5e13d318d785b7c08c8661985e" data-recommend-id="image://c9abee5e13d318d785b7c08c8661985e" data-format="jpg" data-width="1600" data-height="900" data-lightbox="false" data-recommended="false" data-hide="false"></span></figure><p><strong>Goop:&nbsp;</strong><span><a data-ga="[[&quot;Embedded Url&quot;,&quot;External link&quot;,&quot;https://shop.goop.com/shop/products/cordyceps?taxon_id=751&quot;,{&quot;metric25&quot;:1}]]" href="https://shop.goop.com/shop/products/cordyceps?taxon_id=751" target="_blank" rel="noopener noreferrer"><strong>Sun Potion</strong></a></span></p><blockquote data-type="BlockQuote"><p>Organic, USA-grown cordyceps mushroom and is [sic] an important Yang tonic. May support the oxygenation of the whole body, mental power, muscle tone, sexual energy, and immune function. Mix 1/2 teaspoon (2 grams) in warm water or tea 1-2 times daily. Great added to soups, smoothies, raw chocolate, and anytime you are looking to activate fortitude, sensuality, and endurance.</p></blockquote><p><strong>Infowars:&nbsp;</strong><span><a data-ga="[[&quot;Embedded Url&quot;,&quot;External link&quot;,&quot;http://www.infowarsshop.com/Immune-Support-Blend-100-Organic-Coffee_p_1174.html&quot;,{&quot;metric25&quot;:1}]]" href="http://www.infowarsshop.com/Immune-Support-Blend-100-Organic-Coffee_p_1174.html" target="_blank" rel="noopener noreferrer"><strong>Wake Up America&nbsp;Immune Support Blend 100% Organic Coffee</strong></a></span></p><blockquote data-type="BlockQuote"><p>Certain strands of mushroom such as Cordyceps and Reishi have a history of medicinal use spanning millennia in countries such as China, Tibet, and Japan. Throughout history these are [sic] some of the most expensive herbal raw materials in the world. Only recently has western medicine begun to research all the potential medical benefits of medicinal mushrooms. The cutting-edge Wake Up America! Immune Support Blend brings ancient Asian wisdom together with modern technology.</p></blockquote><h2 id="h24651"><a id=""></a>Eleuthero root</h2><p>There is some preliminary evidence that eleuthero, another ingredient in traditional Chinese medicine, <span><a data-ga="[[&quot;Embedded Url&quot;,&quot;External link&quot;,&quot;https://www.uofmhealth.org/health-library/hn-2084007#hn-2084007-uses&quot;,{&quot;metric25&quot;:1}]]" href="https://www.uofmhealth.org/health-library/hn-2084007#hn-2084007-uses" target="_blank" rel="noopener noreferrer">has various benefits</a></span>, such as reducing fatigue and stress, and improving immune functions. Both Moon Juice and Infowars sell it blended with a bunch of other herbal medicines, though, so it would be difficult to isolate eleuthero’s possible positive effects. Moon Juice says it can help “fuel your physical and entrepreneurial feats.” Also, this is the one case Quartz found where the Moon Juice product sounds more hardcore than the Infowars version.</p><p><strong>Moon Juice:&nbsp;</strong><span><a data-ga="[[&quot;Embedded Url&quot;,&quot;External link&quot;,&quot;https://www.moonjuiceshop.com/products/power-dust&quot;,{&quot;metric25&quot;:1}]]" href="https://www.moonjuiceshop.com/products/power-dust" target="_blank" rel="noopener noreferrer"><strong>Power Dust</strong></a></span></p><blockquote data-type="BlockQuote"><p>Power Dust® is an elite blend of adaptogenic superherbs and supermushrooms that help combat [sic] the effects of stress to fuel your physical and entrepreneurial feats.</p></blockquote><p><strong>Infowars:&nbsp;</strong><span><a data-ga="[[&quot;Embedded Url&quot;,&quot;External link&quot;,&quot;http://www.infowarsshop.com/Relax-De-Stress-Herbal-Extract_p_1094.html&quot;,{&quot;metric25&quot;:1}]]" href="http://www.infowarsshop.com/Relax-De-Stress-Herbal-Extract_p_1094.html" target="_blank" rel="noopener noreferrer"><strong>Relax &amp; De-Stress Herbal Extract</strong></a></span></p><blockquote data-type="BlockQuote"><p>Relax &amp; De-Stress Herbal Extract is a [sic] herbal tincture great for relaxing and supporting the nervous system while aiding in maintaining [sic] a healthy heart and adrenals [sic] gland function.</p></blockquote><h2 id="h24652"><a id=""></a>Eyebright herb</h2><p>The two sides of our herbal medicine spectrum seem to have come to different conclusions about what “eyebright” does for the eyes. Infowars sells it in a supplement called “Occu Power,” which makes your eyes “healthy.” Goop sells it as an ingredient in eye makeup. There is <span><a data-ga="[[&quot;Embedded Url&quot;,&quot;External link&quot;,&quot;https://www.uofmhealth.org/health-library/hn-2087000#hn-2087000-uses&quot;,{&quot;metric25&quot;:1}]]" href="https://www.uofmhealth.org/health-library/hn-2087000#hn-2087000-uses" target="_blank" rel="noopener noreferrer">no scientific evidence</a></span> for its purported eye health benefits.</p><figure data-id="3630675af9e35b2803a4530cc9ff7804" data-recommend-id="image://3630675af9e35b2803a4530cc9ff7804" data-format="jpg" data-width="1600" data-height="900" data-lightbox="false" data-recommended="false" data-hide="false" contenteditable="false" draggable="false"><div contenteditable="false" data-link-reference="" data-link-target="" data-syndicationrights="true" data-imagerights="other-license" data-notes="" data-hide="false" data-hidecredit="false"><div><picture><source media="(max-width: 37.31em)" type="image/jpeg" srcset="data:image/gif;base64,R0lGODlhAQABAAAAACH5BAEKAAEALAAAAAABAAEAAAICTAEAOw==" data-srcset="https://i.kinja-img.com/image/upload/c_fit,q_60,w_645/3630675af9e35b2803a4530cc9ff7804.jpg"><source media="(min-width: 37.37em)" type="image/jpeg" srcset="data:image/gif;base64,R0lGODlhAQABAAAAACH5BAEKAAEALAAAAAABAAEAAAICTAEAOw==" data-srcset="https://i.kinja-img.com/image/upload/c_fit,q_60,w_645/3630675af9e35b2803a4530cc9ff7804.jpg"><img alt="Image for article titled All the “wellness” products Americans love to buy are sold on both Infowars and Goop" data-chomp-id="3630675af9e35b2803a4530cc9ff7804" data-format="jpg" data-alt="Image for article titled All the “wellness” products Americans love to buy are sold on both Infowars and Goop" data-anim-src="" data-src="https://i.kinja-img.com/image/upload/c_fit,q_60,w_645/3630675af9e35b2803a4530cc9ff7804.jpg" src="https://i.kinja-img.com/image/upload/c_fit,q_60,w_645/3630675af9e35b2803a4530cc9ff7804.jpg"></picture></div><p><figcaption>Image<!-- -->: <!-- -->Goop/Infowars</figcaption></p></div><span data-id="3630675af9e35b2803a4530cc9ff7804" data-recommend-id="image://3630675af9e35b2803a4530cc9ff7804" data-format="jpg" data-width="1600" data-height="900" data-lightbox="false" data-recommended="false" data-hide="false"></span></figure><p><strong>Goop:&nbsp;</strong><span><a data-ga="[[&quot;Embedded Url&quot;,&quot;External link&quot;,&quot;https://shop.goop.com/shop/products/mesmerize-eye-shimmer&quot;,{&quot;metric25&quot;:1}]]" href="https://shop.goop.com/shop/products/mesmerize-eye-shimmer" target="_blank" rel="noopener noreferrer"><strong>Vapour Beauty’s Mesmerize Eye Shimmer</strong></a></span></p><blockquote data-type="BlockQuote"><p>This is a sheer, modern wash of gleamy color that’s as brilliant all over the lid as it is when used as a translucent, smoky touch of liner. Made with organic chrysanthemum, eyebright, and horsetail herb—the blend is Vapour’s famous Herbal Eyebright complex—the creamy stick is hydrating and packed with antioxidants to treats [sic] the delicate eye area, soothing inflammation and stimulating circulation.</p></blockquote><p><strong>Infowars:&nbsp;</strong><span><a data-ga="[[&quot;Embedded Url&quot;,&quot;External link&quot;,&quot;https://www.infowarsstore.com/health-and-wellness/wellness/occu-power.html&quot;,{&quot;metric25&quot;:1}]]" href="https://www.infowarsstore.com/health-and-wellness/wellness/occu-power.html" target="_blank" rel="noopener noreferrer"><strong>Occu Power</strong></a></span></p><blockquote data-type="BlockQuote"><p>Occu-Power by Infowars Life is a new formulation specifically designed to nutritionally assist the natural function of healthy eyes. Arguably the most important sense, sight is the primary input to the brain. Combining key ingredients like astaxanthin, lutein, and Eyebright herb extract, Occu-Power is a long awaited ‘super formula’ now available exclusively through the Infowars Life line.</p></blockquote><h2 id="h24653"><a id=""></a>Maca</h2><p>Maca is supposed to increase sex drive and male fertility. The Sloan Kettering Cancer Center <span><a data-ga="[[&quot;Embedded Url&quot;,&quot;External link&quot;,&quot;https://www.mskcc.org/cancer-care/integrative-medicine/herbs/maca&quot;,{&quot;metric25&quot;:1}]]" href="https://www.mskcc.org/cancer-care/integrative-medicine/herbs/maca" target="_blank" rel="noopener noreferrer">says</a></span>&nbsp;”the evidence to support the use of maca in improving sexual function is limited,” and that “more studies are warranted.” Both Moon Juice and Infowars sell it as an ingredient in products targeting an increased libido. (Although, at Moon Juice, it is an ingredient in both “Sex Dust” and “Brain Dust.”)</p><p><strong>Moon Juice:&nbsp;</strong><span><a data-ga="[[&quot;Embedded Url&quot;,&quot;External link&quot;,&quot;https://shop.goop.com/shop/products/sex-dust&quot;,{&quot;metric25&quot;:1}]]" href="https://shop.goop.com/shop/products/sex-dust" target="_blank" rel="noopener noreferrer"><strong>Sex Dust</strong></a></span></p><blockquote data-type="BlockQuote"><p>Sex Dust™ is a lusty edible formula alchemized to ignite and excite sexy energy in and out of the bedroom.</p></blockquote><p><strong>Infowars:&nbsp;</strong><span><a data-ga="[[&quot;Embedded Url&quot;,&quot;External link&quot;,&quot;http://www.infowarsshop.com/Super-Male-Vitality-_p_1227.html&quot;,{&quot;metric25&quot;:1}]]" href="http://www.infowarsshop.com/Super-Male-Vitality-_p_1227.html" target="_blank" rel="noopener noreferrer"><strong>Super Male Vitality</strong></a></span></p><blockquote data-type="BlockQuote"><p>As men age, they may often experience a slow-down in vitality, energy, and overall wellness. Super Male Vitality is specifically designed to assist the body in regulating proper balance to create superior vitality in males, and has been used by Alex Jones in order to maximize vitality when working up to 12 hours a day or more in the fight for freedom.</p></blockquote><h2 id="h24654"><a id=""></a>Nascent iodine</h2><p>We couldn’t find any reliable scientific information on “nascent iodine.” Normal, run-of-the-mill iodine is an important thing to have in your body, but most people <span><a data-ga="[[&quot;Embedded Url&quot;,&quot;External link&quot;,&quot;https://ods.od.nih.gov/factsheets/Iodine-Consumer/&quot;,{&quot;metric25&quot;:1}]]" href="https://ods.od.nih.gov/factsheets/Iodine-Consumer/" target="_blank" rel="noopener noreferrer">get enough</a></span> from foods that are rich in the mineral. The “nascent” stuff is <span><a data-ga="[[&quot;Embedded Url&quot;,&quot;External link&quot;,&quot;https://www.globalhealingcenter.com/natural-health/what-is-nascent-iodine/&quot;,{&quot;metric25&quot;:1}]]" href="https://www.globalhealingcenter.com/natural-health/what-is-nascent-iodine/" target="_blank" rel="noopener noreferrer">supposed to</a></span> have an “electromagnetic charge” that makes it easier to digest.</p><p>Putting the question of the veracity of those claims to the side, ConsumerLab, an independent group that tests and vets dietary supplements, says that iodine in this “charged” form <span><a data-ga="[[&quot;Embedded Url&quot;,&quot;External link&quot;,&quot;http://www.consumerlab.com/answers/I+have+heard+that+nascent+iodine+may+be+better+than+regular+iodine+or+potassium+iodide+for+thyroid+support.+Is+this+true,+and+what+exactly+is+nascent+iodine%3F/nascent_iodine/&quot;,{&quot;metric25&quot;:1}]]" href="http://www.consumerlab.com/answers/I+have+heard+that+nascent+iodine+may+be+better+than+regular+iodine+or+potassium+iodide+for+thyroid+support.+Is+this+true,+and+what+exactly+is+nascent+iodine%3F/nascent_iodine/" target="_blank" rel="noopener noreferrer">cannot even exist</a></span> in liquid form, which is how Alex Jones and company distribute it.&nbsp;What’s more, the amount of nascent iodine wellness websites suggest you take—in order to supposedly maintain a healthy thyroid, usually—is way beyond normal levels for the normal iodine we usually get from foods.&nbsp;The US National Institutes of Health says that “adults should avoid prolonged use of doses higher than 1,100 micrograms per day.” Infowars and the “<span><a data-ga="[[&quot;Embedded Url&quot;,&quot;External link&quot;,&quot;https://www.globalhealingcenter.com/nascent-iodine-detoxadine.html?icn_ghc=ddt1_1_052412_winix&amp;ici_ghc=ddnha#description&quot;,{&quot;metric25&quot;:1}]]" href="https://www.globalhealingcenter.com/nascent-iodine-detoxadine.html?icn_ghc=ddt1_1_052412_winix&amp;ici_ghc=ddnha#description" target="_blank" rel="noopener noreferrer">Global Healing Center</a></span>” both recommend 1,950 micrograms a day.</p><p><strong>Goop: No particular product, but recommends nascent iodine in </strong><span><a data-ga="[[&quot;Embedded Url&quot;,&quot;External link&quot;,&quot;http://goop.com/why-we-shouldnt-dismiss-iodine/&quot;,{&quot;metric25&quot;:1}]]" href="http://goop.com/why-we-shouldnt-dismiss-iodine/" target="_blank" rel="noopener noreferrer"><strong>a Q&amp;A published on the site</strong></a></span></p><p><strong>Infowars:&nbsp;</strong><span><a data-ga="[[&quot;Embedded Url&quot;,&quot;External link&quot;,&quot;https://www.infowarsstore.com/survival-shield-x-2-nascent-iodine.html&quot;,{&quot;metric25&quot;:1}]]" href="https://www.infowarsstore.com/survival-shield-x-2-nascent-iodine.html" target="_blank" rel="noopener noreferrer"><strong>Survival Shield X-2</strong></a></span></p><blockquote data-type="BlockQuote"><p>Experience the benefits of next level proprietary nascent iodine, developed using our Thermodynamic Pressure Sensitive High Energy Sound Pulse Nano-Emulsion Technology [sic] that allows for a highly unique nascent iodine that is both concentrated and free of unwanted additives and genetically modified ingredients.</p></blockquote><h2 id="h24655"><a id=""></a>Reishi mushrooms</h2><p>More fungi with a load of <span><a data-ga="[[&quot;Embedded Url&quot;,&quot;External link&quot;,&quot;https://draxe.com/reishi-mushroom/&quot;,{&quot;metric25&quot;:1}]]" href="https://draxe.com/reishi-mushroom/" target="_blank" rel="noopener noreferrer">supposed</a></span> health benefits. Current research shows there’s <span><a data-ga="[[&quot;Embedded Url&quot;,&quot;External link&quot;,&quot;https://www.mskcc.org/cancer-care/integrative-medicine/herbs/reishi-mushroom&quot;,{&quot;metric25&quot;:1}]]" href="https://www.mskcc.org/cancer-care/integrative-medicine/herbs/reishi-mushroom" target="_blank" rel="noopener noreferrer">no scientific evidence to support the claims that</a></span>&nbsp;reishi mushrooms can treat fatigue or increase stamina. There is a small amount of inconclusive scientific support for claims that the fungi improve immune system function, reduce inflammation, and lower cholesterol. Reishi mushrooms are another ingredient in Infowars’ Wake Up America coffee.</p><p><strong>Moon Juice:&nbsp;</strong><span><a data-ga="[[&quot;Embedded Url&quot;,&quot;External link&quot;,&quot;https://www.moonjuiceshop.com/products/spirit-dust&quot;,{&quot;metric25&quot;:1}]]" href="https://www.moonjuiceshop.com/products/spirit-dust" target="_blank" rel="noopener noreferrer"><strong>Spirit Dust</strong></a></span></p><blockquote data-type="BlockQuote"><p>Spirit Dust® is an uplifting blend of adaptogenic superherbs and supermushrooms that help [sic] combat the effects of stress to expand peaceful awareness and align you with bliss.</p></blockquote><p><strong>Infowars:&nbsp;</strong><span><a data-ga="[[&quot;Embedded Url&quot;,&quot;External link&quot;,&quot;http://www.infowarsshop.com/Immune-Support-Blend-100-Organic-Coffee_p_1174.html&quot;,{&quot;metric25&quot;:1}]]" href="http://www.infowarsshop.com/Immune-Support-Blend-100-Organic-Coffee_p_1174.html" target="_blank" rel="noopener noreferrer"><strong>Wake Up America&nbsp;Immune Support Blend 100% Organic Coffee</strong></a></span></p><blockquote data-type="BlockQuote"><p>Prized for thousands of years for their culinary and medicinal properties, mushrooms are more than just a low kilojoule, low sodium and high-fiber ingredient for pasta and pizzas. One of the many conditions that certain species of mushrooms have been found to heal is human papilloma virus infections, which are feared due to their association with certain cancers.</p></blockquote><h2 id="h24656"><a id=""></a>Selenium</h2><p>Selenium is an element found in trace amounts in all animals, including humans, and is required&nbsp;for cellular function. Infowars says the element “supports a healthy thyroid gland, supports the immune system, is essential for metabolic pathways, and much more.” Some of these claims are&nbsp;<span><a data-ga="[[&quot;Embedded Url&quot;,&quot;External link&quot;,&quot;https://www.uofmhealth.org/health-library/hn-2908005#hn-2908005-uses&quot;,{&quot;metric25&quot;:1}]]" href="https://www.uofmhealth.org/health-library/hn-2908005#hn-2908005-uses" target="_blank" rel="noopener noreferrer">backed by evidence</a></span>, though many are not. The <span><a data-ga="[[&quot;Embedded Url&quot;,&quot;External link&quot;,&quot;https://draxe.com/selenium-benefits/&quot;,{&quot;metric25&quot;:1}]]" href="https://draxe.com/selenium-benefits/" target="_blank" rel="noopener noreferrer">claim</a></span> that selenium defends against cancer is also dangerously inconclusive. “Clinical trials show that selenium may not help prevent cancer; it may actually increase the risk of aggressive and secondary cancers,” <span><a data-ga="[[&quot;Embedded Url&quot;,&quot;External link&quot;,&quot;https://www.mskcc.org/cancer-care/integrative-medicine/herbs/selenium&quot;,{&quot;metric25&quot;:1}]]" href="https://www.mskcc.org/cancer-care/integrative-medicine/herbs/selenium" target="_blank" rel="noopener noreferrer">writes</a></span> Sloan Kettering Cancer Center.</p><figure data-id="d4a74e667448ad7aeec425ed3c811fba" data-recommend-id="image://d4a74e667448ad7aeec425ed3c811fba" data-format="jpg" data-width="1600" data-height="900" data-lightbox="false" data-recommended="false" data-hide="false" contenteditable="false" draggable="false"><div contenteditable="false" data-link-reference="" data-link-target="" data-syndicationrights="true" data-imagerights="other-license" data-notes="" data-hide="false" data-hidecredit="false"><div><picture><source media="(max-width: 37.31em)" type="image/jpeg" srcset="data:image/gif;base64,R0lGODlhAQABAAAAACH5BAEKAAEALAAAAAABAAEAAAICTAEAOw==" data-srcset="https://i.kinja-img.com/image/upload/c_fit,q_60,w_645/d4a74e667448ad7aeec425ed3c811fba.jpg"><source media="(min-width: 37.37em)" type="image/jpeg" srcset="data:image/gif;base64,R0lGODlhAQABAAAAACH5BAEKAAEALAAAAAABAAEAAAICTAEAOw==" data-srcset="https://i.kinja-img.com/image/upload/c_fit,q_60,w_645/d4a74e667448ad7aeec425ed3c811fba.jpg"><img alt="Image for article titled All the “wellness” products Americans love to buy are sold on both Infowars and Goop" data-chomp-id="d4a74e667448ad7aeec425ed3c811fba" data-format="jpg" data-alt="Image for article titled All the “wellness” products Americans love to buy are sold on both Infowars and Goop" data-anim-src="" data-src="https://i.kinja-img.com/image/upload/c_fit,q_60,w_645/d4a74e667448ad7aeec425ed3c811fba.jpg" src="https://i.kinja-img.com/image/upload/c_fit,q_60,w_645/d4a74e667448ad7aeec425ed3c811fba.jpg"></picture></div><p><figcaption>Image<!-- -->: <!-- -->Goop/Infowars</figcaption></p></div><span data-id="d4a74e667448ad7aeec425ed3c811fba" data-recommend-id="image://d4a74e667448ad7aeec425ed3c811fba" data-format="jpg" data-width="1600" data-height="900" data-lightbox="false" data-recommended="false" data-hide="false"></span></figure><p><strong>Goop:&nbsp;</strong><span><a data-ga="[[&quot;Embedded Url&quot;,&quot;External link&quot;,&quot;https://shop.goop.com/shop/products/balls-in-the-air&quot;,{&quot;metric25&quot;:1}]]" href="https://shop.goop.com/shop/products/balls-in-the-air" target="_blank" rel="noopener noreferrer"><strong>Balls in the Air</strong></a></span></p><blockquote data-type="BlockQuote"><p>This antioxidant-rich (beta-carotene, vitamin C, and vitamin E) regimen plays defense so you can play offense, helping to unburden inflammation in the body, ensuring that all systems operate at full capacity. Formulated with a blend of building blocks that boosts the body’s production of glutathione—the master detoxifier—this regimen is designed for women who function at an intense pace, and want to keep it that way.</p></blockquote><p><strong>Infowars:&nbsp;</strong><span><a data-ga="[[&quot;Embedded Url&quot;,&quot;External link&quot;,&quot;https://www.infowarsstore.com/bio-true-selenium.html&quot;,{&quot;metric25&quot;:1}]]" href="https://www.infowarsstore.com/bio-true-selenium.html" target="_blank" rel="noopener noreferrer"><strong>Bio-True Selenium</strong></a></span></p><blockquote data-type="BlockQuote"><p>Selenium is an essential trace mineral that supports a healthy thyroid gland, supports the immune system, is essential for metabolic pathways, and much more. Selenium even plays a role in the natural function of reproductive health, DNA production, and eyesight. As a powerful antioxidant, selenium helps fight free radicals and may even be considered a super antioxidant’ [sic] because of the way in which it may support other antioxidants.</p></blockquote><h2 id="h24657"><a id=""></a>Shilajit</h2><p>The science on this tar-like substance found in the Himalayas is scant. <span><a data-ga="[[&quot;Embedded Url&quot;,&quot;External link&quot;,&quot;https://www.ncbi.nlm.nih.gov/pubmed/26395129&quot;,{&quot;metric25&quot;:1}]]" href="https://www.ncbi.nlm.nih.gov/pubmed/26395129" target="_blank" rel="noopener noreferrer">One study</a></span> found it increased testosterone levels. Another said it might have some benefit in helping control Alzheimer’s, but <span><a data-ga="[[&quot;Embedded Url&quot;,&quot;External link&quot;,&quot;https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3296184/&quot;,{&quot;metric25&quot;:1}]]" href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3296184/" target="_blank" rel="noopener noreferrer">adds that</a></span> “more investigations at the basic biological level as well as clinical trials are necessary.” Alex Jones sells it in his Z-Shield drops, with “proprietary science” behind them, saying it defends us from the “toxic substances” that “bombard” us. Over at Moon Juice, it is yet another ingredient in Brain Dust.</p><p><strong>Moon Juice:&nbsp;</strong><span><a data-ga="[[&quot;Embedded Url&quot;,&quot;External link&quot;,&quot;https://www.moonjuiceshop.com/products/brain-dust&quot;,{&quot;metric25&quot;:1}]]" href="https://www.moonjuiceshop.com/products/brain-dust" target="_blank" rel="noopener noreferrer"><strong>Brain Dust</strong></a></span></p><blockquote data-type="BlockQuote"><p>Brain Dust® is an enlightening blend of adaptogenic superherbs and supermushrooms that help combat [sic] the effects of stress to align you with the cosmic flow for great achievement.</p></blockquote><p><strong>Infowars:&nbsp;</strong><span><a data-ga="[[&quot;Embedded Url&quot;,&quot;External link&quot;,&quot;http://www.infowarsshop.com/Z-Shield_p_2053.html&quot;,{&quot;metric25&quot;:1}]]" href="http://www.infowarsshop.com/Z-Shield_p_2053.html" target="_blank" rel="noopener noreferrer"><strong>Z-Shield</strong></a></span></p><blockquote data-type="BlockQuote"><p>More than four years ago, our team of doctors, chemists, master herbalists and nutraceutical experts set out to develop a toxic metal and chemical defense support formula that didn’t cut any corners. Now, after years of deep research and the development of new proprietary processing technology, our team is proud to announce the launch of Z-Shield: The next big game-changer in the Infowars Life line of super high-quality formulations. Z-Shield is designed to help you fight back with natural ingredients that don’t hold back.</p></blockquote><h2 id="h24658"><a id=""></a>Zizyphus (sometimes referred to as “ziziphus”)</h2><p>Infowars calls this a “nourishing tonifier” and recommends using it to calm your kids down. Both Infowars and Moon Juice market it as a sedative, though there are “no human studies on the sedative or anxiety-reducing effects of Jujube” (though some studies on rats have shown promise), <span><a data-ga="[[&quot;Embedded Url&quot;,&quot;External link&quot;,&quot;https://examine.com/supplements/ziziphus-jujuba/&quot;,{&quot;metric25&quot;:1}]]" href="https://examine.com/supplements/ziziphus-jujuba/" target="_blank" rel="noopener noreferrer">according to</a></span> Examine.com, an online encyclopedia that analyzes evidence on supplements. Getting a good night’s sleep may remain a zizyphean task.</p><p><strong>Moon Juice:&nbsp;</strong><span><a data-ga="[[&quot;Embedded Url&quot;,&quot;External link&quot;,&quot;https://www.moonjuiceshop.com/products/dream-dust&quot;,{&quot;metric25&quot;:1}]]" href="https://www.moonjuiceshop.com/products/dream-dust" target="_blank" rel="noopener noreferrer"><strong>Dream Dust</strong></a></span></p><blockquote data-type="BlockQuote"><p>Dream Dust® is a tranquil blend of adaptogenic superherbs that help combat the effects of stress to soothe your tension for deep, nocturnal rest.</p></blockquote><p><strong>Infowars:&nbsp;</strong><span><a data-ga="[[&quot;Embedded Url&quot;,&quot;External link&quot;,&quot;https://www.infowarsstore.com/health-and-wellness/wellness/child-ease.html&quot;,{&quot;metric25&quot;:1}]]" href="https://www.infowarsstore.com/health-and-wellness/wellness/child-ease.html" target="_blank" rel="noopener noreferrer"><strong>Child Ease</strong></a></span></p><blockquote data-type="BlockQuote"><p>Children today live in a stressful world. Over-stimulation can affect their behavior and concentration.&nbsp;Child Ease™&nbsp;by Infowars Life™&nbsp;is a special blend of herbs that has been specifically designed to soothe the mind and bodies of children. Our new formula uses soothing botanicals like chamomile, lemon balm, and catnip, with the nourishing tonifiers hawthorn, zizyphus, gotu kola extract, and amla. We have even added additional herbs and key nutrients that have been traditionally used by cultures around the world.</p></blockquote><p>____________________________________________________________________________</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[LeftoverLocals: Listening to LLM responses through leaked GPU local memory (129 pts)]]></title>
            <link>https://blog.trailofbits.com/2024/01/16/leftoverlocals-listening-to-llm-responses-through-leaked-gpu-local-memory/</link>
            <guid>39016405</guid>
            <pubDate>Tue, 16 Jan 2024 17:58:31 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://blog.trailofbits.com/2024/01/16/leftoverlocals-listening-to-llm-responses-through-leaked-gpu-local-memory/">https://blog.trailofbits.com/2024/01/16/leftoverlocals-listening-to-llm-responses-through-leaked-gpu-local-memory/</a>, See on <a href="https://news.ycombinator.com/item?id=39016405">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="page" role="main">

			
				
<article id="post-105881">
	<!-- .entry-header -->

	<div>
		<p><em>By Tyler Sorensen and Heidy Khlaaf</em></p>
<p>We are disclosing LeftoverLocals: a vulnerability that allows recovery of data from GPU local memory created by another process on Apple, Qualcomm, AMD, and Imagination GPUs. LeftoverLocals impacts the security posture of GPU applications as a whole, with particular significance to LLMs and ML models <a href="https://twitter.com/AMD/status/1744831880241750112">run on impacted GPU platforms</a>. By recovering local memory—an optimized GPU memory region—we were able to build a PoC where an attacker can listen into another user’s interactive LLM session (e.g., llama.cpp) across process or container boundaries, as shown below:</p>

<p><strong>Figure 1:</strong> An illustration of how LeftoverLocals can be used to implement an attack on an interactive LLM chat session. The LLM user (left) queries the LLM, while a co-resident attacker (right) can listen to the LLM response.</p>
<p>LeftoverLocals can leak ~5.5 MB per GPU invocation on an AMD Radeon RX 7900 XT which, when running a 7B model on llama.cpp, adds up to ~181 MB for each LLM query. This is enough information to reconstruct the LLM response with high precision. The vulnerability highlights that many parts of the ML development stack have unknown security risks and have not been rigorously reviewed by security experts.</p>
<div id="attachment_105917"><p><img fetchpriority="high" decoding="async" aria-describedby="caption-attachment-105917" data-attachment-id="105917" data-permalink="https://blog.trailofbits.com/2024/01/16/leftoverlocals-listening-to-llm-responses-through-leaked-gpu-local-memory/figfig1/" data-orig-file="https://i0.wp.com/blog.trailofbits.com/wp-content/uploads/2024/01/figfig1.png?fit=1090%2C772&amp;ssl=1" data-orig-size="1090,772" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="figfig1" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/blog.trailofbits.com/wp-content/uploads/2024/01/figfig1.png?fit=300%2C212&amp;ssl=1" data-large-file="https://i0.wp.com/blog.trailofbits.com/wp-content/uploads/2024/01/figfig1.png?fit=690%2C489&amp;ssl=1" src="https://i0.wp.com/blog.trailofbits.com/wp-content/uploads/2024/01/figfig1.png?resize=543%2C385&amp;ssl=1" alt="" width="543" height="385" srcset="https://i0.wp.com/blog.trailofbits.com/wp-content/uploads/2024/01/figfig1.png?resize=1024%2C725&amp;ssl=1 1024w, https://i0.wp.com/blog.trailofbits.com/wp-content/uploads/2024/01/figfig1.png?resize=300%2C212&amp;ssl=1 300w, https://i0.wp.com/blog.trailofbits.com/wp-content/uploads/2024/01/figfig1.png?resize=768%2C544&amp;ssl=1 768w, https://i0.wp.com/blog.trailofbits.com/wp-content/uploads/2024/01/figfig1.png?w=1090&amp;ssl=1 1090w" sizes="(max-width: 543px) 100vw, 543px" data-recalc-dims="1"></p><p id="caption-attachment-105917"><strong>Figure 2:</strong> LeftoverLocals logo: what leftover data is your ML model leaving for another user to steal?</p></div>
<p>This vulnerability is tracked by <a href="https://kb.cert.org/vuls/id/446598">CVE-2023-4969</a>. It was discovered by Tyler Sorensen as part of his work within the ML/AI Assurance team. Tyler Sorensen is also an assistant professor at UCSC. Since September 2023, we have been working with CERT Coordination Center on a large coordinated disclosure effort involving all major GPU vendors, including: NVIDIA, Apple, AMD, Arm, Intel, Qualcomm, and Imagination.</p>
<p>As of writing, the status of the impacted vendors, Apple, AMD, and Qualcomm are as follows:</p>
<ul>
<li><strong>Apple</strong>: Despite multiple efforts to establish contact through CERT/CC, we only received a response from Apple on January 13, 2024. We re-tested the vulnerability on January 10 where it appears that some devices have been patched, i.e., Apple iPad Air 3rd G (A12). However, the issue still appears to be present on the Apple MacBook Air (M2). Furthermore, the recently released Apple iPhone 15 does not appear to be impacted as previous versions have been. <span>Apple has confirmed that the A17 and M3 series </span><span>processors</span><span> contain fixes</span>, but we have not been notified of the specific patches deployed across their devices.</li>
<li><strong>AMD</strong>: We have confirmed with AMD that their devices remain impacted, although they continue to investigate potential mitigation plans. Their statement on the issue can be read <a href="https://www.amd.com/en/resources/product-security/bulletin/amd-sb-6010.html">here</a>.</li>
<li><strong>Qualcomm</strong>: We received notice that there is a patch to Qualcomm firmware <a href="https://lore.kernel.org/linux-firmware/20240111114032.126035-1-quic_akhilpo@quicinc.com/T/#u">v2.07</a> that addresses LeftoverLocals for some devices. However, there may still be other devices impacted at this time. A Qualcomm representative has provided the following comment: <em>“Developing technologies that endeavor to support robust security and privacy is a priority for Qualcomm Technologies. We commend Dr. Tyler Sorensen and Dr. Heidy Khlaaf from the AI/ML Assurance group at Trail of Bits for using coordinated disclosure practices and are in the process of providing security updates to our customers. We encourage end users to apply security updates as they become available from their device makers.”</em></li>
<li><strong>Imagination: <span>Despite not observing LeftoverLocals ourselves across the Imagination GPUs that we tested, Google has confirmed that some Imagination GPUs are indeed impacted. Imagination <a href="https://www.imaginationtech.com/gpu-driver-vulnerabilities/">released a fix</a> in their latest DDK release, 23.3, made available to customers in December 2023.</span></strong></li>
</ul>
<p>Further details are discussed in “Coordinated disclosure,” and a list of tested and impacted devices can be found in “Testing GPU platforms for LeftoverLocals.” Other vendors have provided us the following details:</p>
<ul>
<li><strong>NVIDIA</strong>: confirmed that their devices are not currently impacted. One reason for this could be that researchers have explored various memory leaks on NVIDIA GPUs <a href="https://arxiv.org/abs/1305.7383">previously</a>, and thus, they are aware of these types of issues.</li>
<li><strong>ARM</strong>: also confirmed that their devices are not currently impacted.</li>
</ul>
<p>While we did not hear a response from these vendors, we tested at least one GPU from them and did not observe that they were impacted: <strong>Intel</strong>.</p>
<h3>Exploit brief</h3>
<p>GPUs were initially developed to accelerate graphics computations. In this domain, performance is critical, and previously uncovered security issues have generally not had any significant consequences on applications. Historically, this entailed that GPU hardware and software stacks iterated rapidly, with frequent major architecture and programming model changes. This has led to complex system stacks and vague specifications. For example, while CPU ISAs have volumes of documentation, NVIDIA simply provides a few short <a href="https://docs.nvidia.com/cuda/cuda-binary-utilities/index.html#instruction-set-reference">tables</a>. This type of vague specification has led to alarming issues, both <a href="https://users.soe.ucsc.edu/~tsorensen/files/asplos2015.pdf">previously</a> and currently, as LeftoverLocals exemplifies.</p>
<h4>Exploitation requirements</h4>
<p>This is a co-resident exploit, meaning that a threat actor’s avenue of attack could be implemented as another application, app, or user on a shared machine. The attacker only requires the ability to run GPU compute applications, e.g., through OpenCL, Vulkan, or Metal. These frameworks are well-supported and typically do not require escalated privileges. Using these, the attacker can read data that the victim has left in the GPU local memory simply by writing a GPU kernel that dumps uninitialized local memory. These attack programs, as our code demonstrates, can be less than 10 lines of code. Implementing these attacks is thus not difficult and is accessible to amateur programmers (at least in obtaining stolen data). We note that it appears that browser GPU frameworks (e.g., WebGPU) are not currently impacted, as they insert dynamic memory checks into GPU kernels.</p>
<p>Unless the user inspects the application’s low-level GPU source-code, it is not possible for them to uncover if their application is utilizing GPU local memory; this matter is further complicated as the GPU code is often hidden deep in library calls, at low levels of deep software stacks (e.g., for ML). Overall, there are very limited ways to observe that an attacker is currently stealing data, or has stolen data. This attack hinges on the attacker reading uninitialized memory on the GPU, and while this is technically undefined behavior, it is not currently checked dynamically, or logged. Any additional defenses would be quite invasive, e.g., performing code analysis on GPU kernels to check for undefined behavior.</p>
<p>We have <a href="https://github.com/trailofbits/LeftoverLocalsRelease">released a PoC</a> that exploits this vulnerability, and the sections below describe how it works.</p>
<h4>User mitigations</h4>
<p>Given the lack of comprehensive patches across impacted GPU vendors, LeftoverLocals can be defended by modifying the source code of all GPU kernels that use local memory. Before the kernel ends, the GPU threads should clear memory (e.g., store 0s) to any local memory memory locations that were used in the kernel. Additionally, the users should ensure the compiler doesn’t remove these memory-clearing instructions away (e.g., by annotating their local memory as volatile), as the compiler may detect that the cleared memory is not used later in the kernel. This is difficult to verify because GPU binaries are typically not stored explicitly, and there are very few GPU binary analysis tools. Because of reasons like this, we note that this mitigation may be difficult for many users, and we discuss this further in “Mitigations” below.</p>
<h2>The vulnerability: LeftoverLocals</h2>
<p>In this section we describe the vulnerability, named LeftoverLocals, and the corresponding exploit in more detail. We then detail our testing campaign across a wide variety of GPU devices, which found that GPUs from AMD, Apple, and Qualcomm are vulnerable to LeftoverLocals. For those unfamiliar with GPU architecture and terminology, we provide a more in-depth level-setter in “Background: How GPUs work.” We also note that while GPU memory leaks are not <a href="https://arxiv.org/abs/1305.7383">new</a> (a further discussion follows below), LeftoverLocals has demonstrated both deeper impact and wider breadth than previously discovered vulnerabilities.</p>
<p>At a high level, we found that several GPU frameworks do not sufficiently isolate memory in the same way that it is traditionally expected in CPU-based frameworks. We have observed that on impacted GPUs, it is possible for one kernel—potentially from another user that is co-resident on the same machine—to observe values in local memory that were written by another kernel. Thus, an attacker who has access to a shared GPU through its programmable interface (e.g., OpenCL) can steal memory from other users and processes, violating traditional process isolation properties. This data leaking can have severe security consequences, especially given the rise of ML systems, where local memory is used to store model inputs, outputs, and weights.</p>
<p>Previous <a href="https://arxiv.org/abs/1305.7383">academic work</a> showed that NVIDIA GPUs leaked memory across processes through a variety of memory regions, including local memory. However, they examined only GPUs from NVIDIA (and the results from this paper may be part of the reason why we didn’t observe LocalLeftovers on NVIDIA GPUs). They also did not discuss the impact on widely deployed use-cases, such as ML. Other works have shown how GPUs leak graphics data, and that a co-resident attacker can reconstruct partial visual information from another process (see some examples documented <a href="https://ieeexplore.ieee.org/document/6956554">here</a>, <a href="https://arxiv.org/pdf/1605.06610.pdf">here</a>, and <a href="https://www.hertzbleed.com/gpu.zip/">here</a>). Despite these prior works, LeftoverLocals shows that many GPUs remain vulnerable to local memory leaks and that this vulnerability can be exploited in co-resident attacks on important ML applications.</p>
<p>Overall, this vulnerability can be illustrated using two simple programs: a Listener and a Writer, where the writer stores canary values in local memory, while a listener reads uninitialized local memory to check for the canary values. The Listener repeatedly launches a GPU kernel that reads from uninitialized local memory. The Writer repeatedly launches a GPU kernel that writes canary values to local memory. Below, we demonstrate how each of these operations is carried out.</p>
<p><strong><em>The Listener</em></strong>: The Listener launches a GPU kernel that reads from uninitialized local memory and stores the result in a persistent main memory region (i.e., global memory). This can be accomplished with the OpenCL kernel below:</p>
<pre><span>__kernel</span> void listener(__global <span>volatile</span> int *<span>dump</span>) {
  local <span>volatile</span> int lm[LM_SIZE];
  for (int i = <span>get_local_id(0)</span>; i &lt; LM_SIZE; i+= <span>get_local_size(0)</span>) {
    <span>dump</span>[((LM_SIZE * <span>get_group_id(0)</span>) + i)] = lm[i];
  }
}</pre>
<p>The keyword <span><code>__kernel</code></span> denotes that this is the GPU kernel function. We pass a global memory array <span><code>dump</code></span> to the function. Whatever the kernel writes to this array can be read later by the CPU. We statically declare a local memory array lm with a predefined size LM_SIZE (which we set to be the max size of local memory for each GPU we test). This program technically contains undefined behavior, as it reads from uninitialized local memory. Because of this, we use the <span><code>volatile</code></span> qualifier to suppress aggressive compiler optimizations that might optimize away the memory accesses. In fact, our code contains a few more code patterns included to further stop the compiler from optimizing away our memory dump. This process is more of a trial-and-error process than a science.</p>
<p>For each loop iteration, the invocation (thread) is read from a location in local memory, and that location is dumped to a unique location in the <span><code>dump</code></span> array. The only tricky part of this code is the indexing, because local memory is disjointed across workgroups, so workgroup local IDs need to be mapped to a unique global ID in <span><code>dump</code></span>. The process utilizes <span>built-in identifiers</span> to achieve this, which are documented <a href="https://registry.khronos.org/OpenCL/specs/3.0-unified/html/OpenCL_C.html#work-item-functions">here</a>. At the end of the kernel, dump contains every value that was stored in local memory when the listener kernel started executing. Because dump is in the global memory region, it can be examined by the CPU host code to check for canary values.</p>
<p><strong><em>The Writer</em></strong>: On the other hand, the Writer launches a kernel that writes a canary value to local memory (for example, this work uses the value 123). We show an example of the OpenCL kernel code below:</p>
<pre>__kernel void writer(__global volatile int *<span>canary</span>) {
  local volatile int lm[LM_SIZE];
  for (uint i = get_local_id(0); i &lt; LM_SIZE; i+=get_local_size(0)) {
    lm[i] = <span>canary</span>[i];
  }
}
</pre>
<p>This code is very similar to the Listener, except that rather than dumping local memory, we are writing a value. In this case, we are writing a value from an array <span><code>canary</code></span>. We use an extra array so that the compiler does not optimize away the memory write (as it is prone to do with constant values). At the end of the kernel, the writer has filled all available local memory with the <span>canary</span> values.</p>
<p>The CPU programs for both the Listener and the Writer launch their respective kernels repeatedly. In the case of the listener, at each iteration, the CPU analyzes the values observed in the local memory and checks for the canary value. On a server, these two programs can be run by different users or in different Docker containers. On a mobile device, these routines can be run in different apps. The apps can be swapped in and out of focus to alternate reading and writing. <em>If the Listener can reliably read the canary values, then we say that the platform is vulnerable to LeftoverLocals.</em></p>
<p>The following animation shows how the listener and writer interact, and how the listener may observe values from the writer if local memory is not cleared.</p>

<p><strong>Figure 3:</strong> A Listener and a Writer processes, where the writer stores canary values in local memory, while a listener reads uninitialized local memory to check for the canary values</p>
<h3>Listening to LLM responses</h3>
<p>In this section, we provide an overview of how LeftoverLocals can be exploited by a malicious actor (an attacker) to listen to another user’s (the victim) LLM responses on a multi-tenant GPU machine, followed by a detailed description of the PoC.</p>
<p>At a high level, both actors are executed as co-resident processes. The attack process implements the listener described above, with the additional steps of comparing the stolen values to various fingerprints. The victim process is unknowingly the writer, where instead of canary values, the values being written are sensitive components of an interactive LLM chat session. The attack ultimately follows two steps:</p>
<ul>
<li>The attack process fingerprints the model that the victim process is using by repeatedly dumping (i.e., listening) to the leftover local memory, which, in this scenario, consists of sensitive components of linear algebra operations used by the victim in the LLM model architecture.</li>
<li>The attacker then repeatedly listens to the victim’s process again, specifically seeking for an LLM to execute the output layer, which can be identified using weights or memory layout patterns from the earlier fingerprinting.</li>
</ul>
<p>Note that the output layer is a matrix-vector multiplication with two inputs: the model weights, and the layer input—in other words, the values derived from the user input that propagated through the earlier levels of the deep neural network (DNN). Given that the model weights of the output layer are too large to comprehensively steal, an attacker can inspect available open-source models to fully obtain the weights through the exposed model fingerprint. We found that the second input to the last layer (i.e., the layer input) is subsequently small enough to fit into local memory. Thus, the entire layer input can be stolen, and the attacker can reproduce the final layer computation to uncover the final result of the DNN.</p>
<div id="attachment_105919"><p><img decoding="async" aria-describedby="caption-attachment-105919" data-attachment-id="105919" data-permalink="https://blog.trailofbits.com/2024/01/16/leftoverlocals-listening-to-llm-responses-through-leaked-gpu-local-memory/figfig2/" data-orig-file="https://i0.wp.com/blog.trailofbits.com/wp-content/uploads/2024/01/figfig2.png?fit=1999%2C909&amp;ssl=1" data-orig-size="1999,909" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="figfig2" data-image-description="" data-image-caption="<p>Figure 4: Steps of the PoC exploit whereby an attacker process can uncover data to listen to another user’s interactive LLM session with high fidelity</p>
" data-medium-file="https://i0.wp.com/blog.trailofbits.com/wp-content/uploads/2024/01/figfig2.png?fit=300%2C136&amp;ssl=1" data-large-file="https://i0.wp.com/blog.trailofbits.com/wp-content/uploads/2024/01/figfig2.png?fit=690%2C314&amp;ssl=1" src="https://i0.wp.com/blog.trailofbits.com/wp-content/uploads/2024/01/figfig2.png?resize=690%2C314&amp;ssl=1" alt="" width="690" height="314" srcset="https://i0.wp.com/blog.trailofbits.com/wp-content/uploads/2024/01/figfig2.png?resize=1024%2C466&amp;ssl=1 1024w, https://i0.wp.com/blog.trailofbits.com/wp-content/uploads/2024/01/figfig2.png?resize=300%2C136&amp;ssl=1 300w, https://i0.wp.com/blog.trailofbits.com/wp-content/uploads/2024/01/figfig2.png?resize=768%2C349&amp;ssl=1 768w, https://i0.wp.com/blog.trailofbits.com/wp-content/uploads/2024/01/figfig2.png?resize=1536%2C698&amp;ssl=1 1536w, https://i0.wp.com/blog.trailofbits.com/wp-content/uploads/2024/01/figfig2.png?resize=1200%2C546&amp;ssl=1 1200w, https://i0.wp.com/blog.trailofbits.com/wp-content/uploads/2024/01/figfig2.png?w=1999&amp;ssl=1 1999w, https://i0.wp.com/blog.trailofbits.com/wp-content/uploads/2024/01/figfig2.png?w=1380&amp;ssl=1 1380w" sizes="(max-width: 690px) 100vw, 690px" data-recalc-dims="1"></p><p id="caption-attachment-105919"><strong>Figure 4:</strong> Steps of the PoC exploit whereby an attacker process can uncover data to listen to another user’s interactive LLM session with high fidelity</p></div>
<p>We note that this is a fairly straightforward attack, and with further creativity and ingenuity, a threat actor may be able to construct further complex and sophisticated malicious scenarios that may compromise ML applications in more severe ways. Below we provide a detailed description of the PoC, and the configuration and testing carried out on various GPU platforms to uncover their susceptibility to LeftoverLocals.</p>
<p><strong><em>Our configuration</em></strong>: We outline our configuration in the table below. Our attack builds on the llama.cpp LLM due to its simplicity and variety of support for GPU acceleration. In our example we use a large discrete GPU that we found to be susceptible to LeftoverLocals: the AMD Radeon RX 7900 XT. We configure llama.cpp to use OpenCL for GPU acceleration, which uses the CLBLAST linear algebra library. We use the wizardLM-7B.ggmlv3.q5_0.bin model, which <a href="https://huggingface.co/TheBloke/wizardLM-7B-GGML/tree/main">can be obtained</a> from Hugging Face. This model was selected due to its reasonable size, which enabled rapid prototyping and analysis; however, this attack is transferable to many different models. In our threat model, we assume that the victim is using the LLM in an interactive chat session.</p>
<p><img decoding="async" data-attachment-id="105921" data-permalink="https://blog.trailofbits.com/2024/01/16/leftoverlocals-listening-to-llm-responses-through-leaked-gpu-local-memory/figfig3/" data-orig-file="https://i0.wp.com/blog.trailofbits.com/wp-content/uploads/2024/01/figfig3.png?fit=1258%2C504&amp;ssl=1" data-orig-size="1258,504" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="figfig3" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/blog.trailofbits.com/wp-content/uploads/2024/01/figfig3.png?fit=300%2C120&amp;ssl=1" data-large-file="https://i0.wp.com/blog.trailofbits.com/wp-content/uploads/2024/01/figfig3.png?fit=690%2C276&amp;ssl=1" src="https://i0.wp.com/blog.trailofbits.com/wp-content/uploads/2024/01/figfig3.png?resize=600%2C241&amp;ssl=1" alt="" width="600" height="241" srcset="https://i0.wp.com/blog.trailofbits.com/wp-content/uploads/2024/01/figfig3.png?resize=1024%2C410&amp;ssl=1 1024w, https://i0.wp.com/blog.trailofbits.com/wp-content/uploads/2024/01/figfig3.png?resize=300%2C120&amp;ssl=1 300w, https://i0.wp.com/blog.trailofbits.com/wp-content/uploads/2024/01/figfig3.png?resize=768%2C308&amp;ssl=1 768w, https://i0.wp.com/blog.trailofbits.com/wp-content/uploads/2024/01/figfig3.png?resize=1200%2C481&amp;ssl=1 1200w, https://i0.wp.com/blog.trailofbits.com/wp-content/uploads/2024/01/figfig3.png?w=1258&amp;ssl=1 1258w" sizes="(max-width: 600px) 100vw, 600px" data-recalc-dims="1"></p>
<p><strong><em>Modification</em></strong>: The attack requires an optimized GPU implementation of matrix-vector multiplication. We found that the current matrix-vector multiplication in llama.cpp (which does not call into CLBLAST) is not implemented in an optimized idiomatic way. It stores partial dot product results in local memory and then combines them at the end. While there is a more complex approach using linear algebra to achieve our same results, for the simplicity of our PoC and demonstration, we replace the llama.cpp matrix-vector multiplication with our own that is more idiomatic (following best GPU programming programming practices).</p>
<p><strong><em>Step 1—Fingerprinting the model</em></strong>: An attacker can fingerprint a model if it can listen to several inference queries from the victim. In our configuration, the GPU contains roughly 5MB of local memory. The model has roughly 33 layers, each of them consisting of a matrix multiplication operation. Matrix multiplication is often optimized on GPUs by using tiling: an approach that subdivides the matrices into small matrices, performs the multiplication, and then combines the results (as <a href="https://cnugteren.github.io/tutorial/pages/page4.html">detailed here</a>). In many optimized libraries, including CLBLAST, local memory is used to cache the smaller matrices. Thus, for every layer, the attacker can steal ~2.5MB of weights, and ~2.5MB of the inputs. While this is a significant amount of data, we note that it is not enough to reconstruct the entire computation. Many of these layers have weights and inputs that are 100s of MB large.</p>
<p>However, for a whole inference computation (33 layers), the attacker can steal around 80MB of the weights, which is sufficient to fingerprint the model (assuming the user is using an open-source model, such as one that can be found on Hugging Face). Given this, we assume that it is a straightforward task to fingerprint the model, and thus for the attacker to obtain the full model being used by the victim.</p>
<p><strong><em>Step 2—Listening to the LLM output</em></strong>: The attacker can then turn their attention to the output layer of the DNN. In our configuration, we found that the output layer is a matrix-vector multiplication, rather than a matrix-matrix multiplication. The weights matrix is large (~128MB), but the input vector is quite small (~4KB). However, given that the attacker has fingerprinted the model in step 1, the attacker does not need to comprehensively steal the weights as they are available from the fingerprinted model.</p>
<p>Matrix-vector multiplication has a different GPU implementation than matrix-matrix multiplication. In the case where the input vector fits in local memory, the most performant implementation is often to cache the input vector in local memory, as it is used repeatedly (i.e., for repeated dot products). Because the input vector is stored entirely in local memory, the attacker can steal this entire vector. In determining whether the attacker has found local memory from the output layer, we discovered that the attacker could simply look for 4KB of floating point values with zeros on either side. In our testing, this unique fingerprint was associated with the output layer nearly every single time. For different models and different GPUs, this fingerprint will likely have to be recalibrated.</p>
<p><strong><em>Putting it together</em></strong>: With an attacker in possession of both the weights and the input vector, they can perform the final computation and obtain the result of the inference. This allows the attacker to reproduce the output of the victim’s LLM chat session with high fidelity, as demonstrated in the introduction. In practice, we tuned the attacker to dump the local memory very efficiently (that is, by using only a small number of threads and requiring a small amount of memory). This allows the attacker to listen to long chat queries with only a small number of noticeable artifacts. Some of the artifacts observed include:</p>
<ul>
<li><em>Duplicate tokens</em>: This occurs when the attacker steals the same output layer twice due to circumstances such as the attacker process being scheduled twice in a row, thus the LLM was not scheduled to compute its next token.</li>
<li><em>Missing tokens</em>: This occurs when the attacker kernel isn’t scheduled at the right time, i.e., immediately after the output layer computation kernel.</li>
<li><em>Incorrect tokens</em> outputted occurring due to:</li>
<li>the attacker mis-identifying a stolen set of data to be the last layer. In this case, it will print a junk token.</li>
<li>Production of a token that is “close” to the original output, even if it is not exact. That is, the attacker may be unable to steal the exact token embedding at the target layer. This results in a corrupted token embedding which, when decoded, is semantically similar (in the word2vec sense) to the original token. As an example, in the GIF provided at the beginning, the attacker extracts the incorrect word “Facebook”, which is semantically similar to other Named Entities tokens (like “Google”, and “Amazon”) in the generated text.</li>
</ul>
<p>Despite these discrepant artifacts, the stolen text is more than sufficient to uncover the LLM response. Additionally, the attacker can be further tuned by, for example, having multiple threads launch the listener kernel or by having a more precise fingerprint of the last layer.</p>
<h3>Testing GPU platforms for LeftoverLocals</h3>
<p>Given the diversity of the devices we tested, there exists several applications that can test for LeftoverLocals written in a variety of frameworks:</p>
<ul>
<li><strong>Vulkan Command Line</strong>: A command line application using Vulkan. The kernel is written in OpenCL and compiled to SPIR-V using <a href="https://github.com/google/clspv">clspv</a>. It uses a simple Vulkan wrapper called <a href="https://github.com/ucsc-chpl/easyvk">EasyVK</a>.</li>
<li><strong>OpenCL Command Line</strong>: A command line application that uses the OpenCL framework.</li>
<li><strong>Apple App</strong>: An Apple app that can be deployed on iOS or Mac OS. It targets the GPU using Apple’s Metal framework.</li>
<li><strong>Android App</strong>: An Android app that uses Vulkan to target mobile GPUs. The code uses Vulkan’s C API (through EasyVK again) using JNI. The kernels are the same as in the Vulkan command line app: they are written in OpenCL and compiled to SPIR-V using clspv.</li>
</ul>
<p>Using the above programs, we tested 11 devices spanning seven GPU vendors (and multiple GPU frameworks in some cases). We observed LeftoverLocals on devices from three of the vendors (Apple, Qualcomm, and AMD). The amount of memory leaked depends on the size of the GPU. Larger GPUs contain more physical memory, and thus, leak more data. For the larger GPUs (e.g., an AMD Radeon RX 7900 XT), we found that we can leak over ~5MB per kernel. The following tables outlines the system info for the GPUs we were able to observe LeftoverLocals (QC refers to Qualcomm):</p>
<p><img loading="lazy" decoding="async" data-attachment-id="105923" data-permalink="https://blog.trailofbits.com/2024/01/16/leftoverlocals-listening-to-llm-responses-through-leaked-gpu-local-memory/figfig4/" data-orig-file="https://i0.wp.com/blog.trailofbits.com/wp-content/uploads/2024/01/figfig4.png?fit=1576%2C404&amp;ssl=1" data-orig-size="1576,404" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="figfig4" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/blog.trailofbits.com/wp-content/uploads/2024/01/figfig4.png?fit=300%2C77&amp;ssl=1" data-large-file="https://i0.wp.com/blog.trailofbits.com/wp-content/uploads/2024/01/figfig4.png?fit=690%2C177&amp;ssl=1" src="https://i0.wp.com/blog.trailofbits.com/wp-content/uploads/2024/01/figfig4.png?resize=690%2C177&amp;ssl=1" alt="" width="690" height="177" srcset="https://i0.wp.com/blog.trailofbits.com/wp-content/uploads/2024/01/figfig4.png?resize=1024%2C262&amp;ssl=1 1024w, https://i0.wp.com/blog.trailofbits.com/wp-content/uploads/2024/01/figfig4.png?resize=300%2C77&amp;ssl=1 300w, https://i0.wp.com/blog.trailofbits.com/wp-content/uploads/2024/01/figfig4.png?resize=768%2C197&amp;ssl=1 768w, https://i0.wp.com/blog.trailofbits.com/wp-content/uploads/2024/01/figfig4.png?resize=1536%2C394&amp;ssl=1 1536w, https://i0.wp.com/blog.trailofbits.com/wp-content/uploads/2024/01/figfig4.png?resize=1200%2C308&amp;ssl=1 1200w, https://i0.wp.com/blog.trailofbits.com/wp-content/uploads/2024/01/figfig4.png?w=1576&amp;ssl=1 1576w, https://i0.wp.com/blog.trailofbits.com/wp-content/uploads/2024/01/figfig4.png?w=1380&amp;ssl=1 1380w" sizes="(max-width: 690px) 100vw, 690px" data-recalc-dims="1"></p>
<p>For some devices, specifically those from Arm, we were not able to observe the canary value from the Writer in the Listener, but we did observe non-zero data. Representatives from Arm reviewed our observations and concluded that although these values are not zero, they are not from a memory leak.</p>
<p><img loading="lazy" decoding="async" data-attachment-id="105924" data-permalink="https://blog.trailofbits.com/2024/01/16/leftoverlocals-listening-to-llm-responses-through-leaked-gpu-local-memory/figfig5/" data-orig-file="https://i0.wp.com/blog.trailofbits.com/wp-content/uploads/2024/01/figfig5.png?fit=1508%2C216&amp;ssl=1" data-orig-size="1508,216" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="figfig5" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/blog.trailofbits.com/wp-content/uploads/2024/01/figfig5.png?fit=300%2C43&amp;ssl=1" data-large-file="https://i0.wp.com/blog.trailofbits.com/wp-content/uploads/2024/01/figfig5.png?fit=690%2C99&amp;ssl=1" src="https://i0.wp.com/blog.trailofbits.com/wp-content/uploads/2024/01/figfig5.png?resize=690%2C99&amp;ssl=1" alt="" width="690" height="99" srcset="https://i0.wp.com/blog.trailofbits.com/wp-content/uploads/2024/01/figfig5.png?resize=1024%2C147&amp;ssl=1 1024w, https://i0.wp.com/blog.trailofbits.com/wp-content/uploads/2024/01/figfig5.png?resize=300%2C43&amp;ssl=1 300w, https://i0.wp.com/blog.trailofbits.com/wp-content/uploads/2024/01/figfig5.png?resize=768%2C110&amp;ssl=1 768w, https://i0.wp.com/blog.trailofbits.com/wp-content/uploads/2024/01/figfig5.png?resize=1200%2C172&amp;ssl=1 1200w, https://i0.wp.com/blog.trailofbits.com/wp-content/uploads/2024/01/figfig5.png?w=1508&amp;ssl=1 1508w, https://i0.wp.com/blog.trailofbits.com/wp-content/uploads/2024/01/figfig5.png?w=1380&amp;ssl=1 1380w" sizes="(max-width: 690px) 100vw, 690px" data-recalc-dims="1"></p>
<p>Additionally, we tested some GPUs from NVIDIA, Intel, and Imagination. For these devices, we observed only zeros in local memory, and thus did not observe LeftoverLocals. <span>It is unclear if all their devices are not impacted. For example, although we did not observe the issue on our </span><span>Imagination device, Google notified us that they were able to observe it on other Imagination devices.</span></p>
<p><img loading="lazy" decoding="async" data-attachment-id="105925" data-permalink="https://blog.trailofbits.com/2024/01/16/leftoverlocals-listening-to-llm-responses-through-leaked-gpu-local-memory/figfig6/" data-orig-file="https://i0.wp.com/blog.trailofbits.com/wp-content/uploads/2024/01/figfig6.png?fit=1596%2C272&amp;ssl=1" data-orig-size="1596,272" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="figfig6" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/blog.trailofbits.com/wp-content/uploads/2024/01/figfig6.png?fit=300%2C51&amp;ssl=1" data-large-file="https://i0.wp.com/blog.trailofbits.com/wp-content/uploads/2024/01/figfig6.png?fit=690%2C118&amp;ssl=1" src="https://i0.wp.com/blog.trailofbits.com/wp-content/uploads/2024/01/figfig6.png?resize=690%2C118&amp;ssl=1" alt="" width="690" height="118" srcset="https://i0.wp.com/blog.trailofbits.com/wp-content/uploads/2024/01/figfig6.png?resize=1024%2C175&amp;ssl=1 1024w, https://i0.wp.com/blog.trailofbits.com/wp-content/uploads/2024/01/figfig6.png?resize=300%2C51&amp;ssl=1 300w, https://i0.wp.com/blog.trailofbits.com/wp-content/uploads/2024/01/figfig6.png?resize=768%2C131&amp;ssl=1 768w, https://i0.wp.com/blog.trailofbits.com/wp-content/uploads/2024/01/figfig6.png?resize=1536%2C262&amp;ssl=1 1536w, https://i0.wp.com/blog.trailofbits.com/wp-content/uploads/2024/01/figfig6.png?resize=1200%2C205&amp;ssl=1 1200w, https://i0.wp.com/blog.trailofbits.com/wp-content/uploads/2024/01/figfig6.png?w=1596&amp;ssl=1 1596w, https://i0.wp.com/blog.trailofbits.com/wp-content/uploads/2024/01/figfig6.png?w=1380&amp;ssl=1 1380w" sizes="(max-width: 690px) 100vw, 690px" data-recalc-dims="1"></p>
<p>The following YouTube video demonstrates the different interfaces and examples of LocalLeftovers—namely the LLM PoC attack, covert communication channels, and searching for canary values—on a few different platforms using a few different applications.</p>
<p><iframe loading="lazy" title="YouTube video player" src="https://www.youtube.com/embed/g2A7GvbnItg?si=w0tvRgk2Kn1YdcX7" width="560" height="315" frameborder="0" allowfullscreen="allowfullscreen"></iframe></p>
<p><strong><em>Vulnerable environments</em></strong>: An attack program must be co-resident on the same machine and must be “listening” at the same time that the victim is running a sensitive application on the GPU. This could occur in many scenarios: for example, if the attack program is co-resident with the victim on a shared cloud computer with a GPU. On a mobile device, the attack could be implemented in an app or a library. Listening can be implemented efficiently, and thus can be done repeatedly and constantly with almost no obvious performance degradation.</p>
<p>Next, we briefly discuss other environments where GPUs are either deployed or where an attacker might have access to sensitive information. Although it appears that some current systems (e.g., WebGPU) are not currently impacted, the ever-growing prevalence of ML and the diversity of modern GPUs mean that the next iteration of these systems (or other near-future systems) may be severely compromised by these types of vulnerabilities.</p>
<ul>
<li><strong>Cloud providers</strong>: Cloud providers (e.g., AWS and Azure) are unlikely to provide shared GPU instances, especially if users have dedicated access to the GPU machine. In other cases, GPUs could be shared using very conservative GPU VM technology (such as NVIDIA’s vGPU or MxGPU), which physically partitions the GPU and therefore prevents users from sharing GPU resources (e.g., local memory). Given this, many current cloud GPU systems may not currently be vulnerable to LeftoverLocals; however, we do not have conclusive evidence to determine this given the general lack of visibility into the specification and implementation of these systems. We note that we have observed LeftoverLocals on multi-user Linux servers, as well as on desktop (Windows and Mac) systems through traditional multi-processing. This includes Docker containers on these systems.</li>
<li><strong>Mobile applications</strong>: In our experiments and explorations in the mobile domain, we were able to run concurrent GPU processes (from different apps on iOS or Android) only in very specific instances. That is, we were not able to run a GPU process (e.g., from a malicious listener app) in the background while other apps (e.g., the victim) were run in the foreground. As with our analysis of cloud providers, we were unable to find clear documentation that explicitly detailed these constraints, and so we cannot definitively claim whether they are vulnerable. However, as seen in the video above, LeftoverLocals can be exploited either when a malicious listener app is run side-by-side with a victim app, or if the malicious listener app is quickly swapped from the background into the foreground from a victim app.</li>
<li><strong>Remote attacks</strong>: We preliminarily investigated the possibility of attacks originating from websites (e.g., those hosted by a remote attacker). To our knowledge, web applications do not have the low-level features required to listen to local memory using GPU graphics frameworks, such as WebGL. We note that the new WebGPU framework does provide low-level capabilities that allow a webpage to access local memory. Conservatively, WebGPU initializes and performs dynamic array bounds checking on local memory (and global memory), which mitigates this vulnerability. However, these checks cause significant overhead, as documented in discussions like <a href="https://github.com/gpuweb/gpuweb/issues/1202">this one</a>. To test this further, our code repo contains a simple listener in WebGPU. As expected, we have only observed zeros in local memory, even on devices that are vulnerable to LeftoverLocals through other frameworks. However, GPU compilers are <a href="https://medium.com/@afd_icl/crashes-hangs-and-crazy-images-by-adding-zero-689d15ce922b">known to be fragile</a>, and it is not difficult to imagine finding a compiler bug that could somehow bypass these checks (especially using fuzzing techniques). Our position is that LocalLeftovers should be addressed at a lower level (e.g., the driver).</li>
</ul>
<p><strong>How GPU vendors can resolve this vulnerability</strong>: To defend against LocalLeftovers, GPUs should clear their local memory between kernel calls. While this could cause some performance overhead, our experiments show that many GPU vendors (e.g., NVIDIA, Intel) currently appear to provide this functionality. It even appears that some of this functionality is provided for impacted GPUs. For example, Mesa drivers <a href="https://github.com/Mesa3D/mesa/blob/957009978ef6d7121fc0d710d03bc20097d4d46b/src/amd/vulkan/radv_shader.c#L709">for AMD GPUs</a> clears local memory after a compute kernel launch. However, this approach has a fundamental flaw that makes it vulnerable to LeftoverLocals: this memory wipe is done with a separate kernel, thus, the GPU kernel queue may contain a malicious listener between the computation kernel and the local memory wipe, allowing the listener to steal memory. Instead, the computation kernel and the local memory wipe need to occur atomically, i.e., without allowing any other kernel to be interleaved between them. Otherwise, a user may attempt to preemptively defend themselves against LeftoverLocals as described in the next section.</p>
<p><strong>Mitigations</strong>: In light of a lack of comprehensive patches across impacted GPU vendors, LeftoverLocals can be defended by modifying the source code of all GPU kernels that use local memory. As we’ve previously noted, before the kernel ends, the GPU threads should store 0 to any local memory locations that were used in the kernel. Given that GPU tasks are typically interleaved at the kernel boundary, this will prevent another user from being able to read leftover values. We note that this mitigation may be difficult for many users, especially because GPU code is often buried deep in complex software stacks (e.g., for ML). Furthermore, the GPU code may be part of a highly optimized library (e.g., ML linear algebra routines). In these cases, it is very difficult to identify how local memory is used, and even more difficult to modify the kernel to zero it out. It may be possible to augment a compiler to add this functionality, similar to how WebGPU handles GPU memory accesses (described above). These mitigations do have a performance overhead that should be taken into account. Another blunt mitigation involves simply avoiding multi-tenant GPU environments.</p>
<h2>Impact on LLMs and GPU platforms</h2>
<h3>LLM security</h3>
<p>Our PoC attack examines only one application: an interactive open-source LLM session. However, with a little creativity, attackers could likely target many GPU applications, including those used within privacy-sensitive domains. Our motivation stems from the recent increased use and support of open-source models, often accompanied by claims that their “openness” inherently entails safety and security through transparency. A recent article in <a href="https://www.nature.com/articles/d41586-023-03803-y">Nature</a> even alleges that only open-source generative AI models can “safely” revolutionize health care, a safety-critical domain. Yet, even if open-source models provide the opportunity to be rigorously audited and assessed (<a href="https://blog.trailofbits.com/2023/11/15/assessing-the-security-posture-of-a-widely-used-vision-model-yolov7/">which they have yet to be</a>), their deployment still hinges on a closed-source stack (i.e., GPUs). And as demonstrated by LeftoverLocals, open-source LLMs are particularly susceptible to our vulnerability given our ability to fingerprint these models to obtain remaining weights as needed. Indeed, we have already observed announcements regarding the deployment of open-source models in collaboration with impacted GPU vendors, including <a href="https://twitter.com/AMD/status/1744831880241750112">Hugging Face’s collaboration with AMD</a>, <a href="https://www.lamini.ai/blog/lamini-amd-paving-the-road-to-gpu-rich-enterprise-llms">Lamini’s deployment on AMD GPUs</a>, and the <a href="https://www.qualcomm.com/news/releases/2023/07/qualcomm-works-with-meta-to-enable-on-device-ai-applications-usi">Qualcomm and Meta partnership</a> for edge devices.</p>
<p>Generally, the introduction of ML poses new attack surfaces that traditional threat models do not account for, and that can lead to implicit and explicit access to data, model parameters, or resulting outputs, increasing the overall attack surface of the system. It is crucial to identify and taxonomize novel classes of failure modes that directly impact ML models, in addition to novel threats that can compromise the ML Ops pipeline, as we have demonstrated with LeftoverLocals. We discuss GPU-specific threat implications in the following section.</p>
<h3>GPU providers, applications, and vendors</h3>
<p>While many platforms are not currently impacted (see Vulnerable environments), we emphasize that the GPU compute landscape is evolving rapidly. As some examples: <a href="https://cloud-gpus.com/">a growing number of GPU cloud providers</a> have various policies and available configurations; and GPU programming frameworks, such as Vulkan and Metal, are well-supported on mainstream platforms, and can be used in apps without requiring extra privileges. While these developments are exciting, they increase the threat potential of GPU vulnerabilities, as LeftoverLocals illustrates. As far as we are aware, there is no unified security specification for how GPUs are required to handle sensitive data, and no portable test suite to check if systems are vulnerable to simple memory leaks, like LeftoverLocals. Thus, GPU compute environments should be rigorously scrutinized when used for processing any type of sensitive data.</p>
<p>As mentioned above, while we focus on LLM applications, GPU local memory is one of the first tools that a GPU developer uses when optimizing an application. Although other attacks would likely require analyzing the victim’s GPU kernel code to identify local memory usage, other attacks are likely possible in GPU compute domains, such as image processing and scientific computing. It will likely be increasingly difficult for users to detect and defend against these attacks since it’s unlikely they will know if their application is vulnerable to LeftoverLocals; this would require knowing the details of the exact GPU kernel code, which are often hidden away in highly optimized linear algebra libraries (e.g., <a href="https://github.com/CNugteren/CLBlast">CLBLAST</a>). Additionally, an overall lack of specification in up-and-coming GPU platforms makes it difficult to determine whether the compiler or runtime will use impacted memory regions without the user knowing. For example, Apple GPUs have a new caching mechanism, called <a href="https://www.digitaltrends.com/computing/apple-dynamic-caching-explained/">dynamic caching</a>, that does not have a clear specification regarding if local memory regions are being used for other purposes.</p>
<h2>Coordinated disclosure</h2>
<p>Since September 2023, we have been working CERT/CC on a large coordinated disclosure involving all major GPU vendors, including NVIDIA, Apple, AMD, Arm, Intel, Qualcomm, and Imagination. Trail of Bits provided vendors a total of 125 days to test their products and provide remediations. The coordination gradually grew to include software stakeholders, including Google, Microsoft, and others, which allowed us to understand how LocalLeftovers impacts privacy requirements and impact at different stages in the ML supply chain. Apple did not respond or engage with us regarding the disclosure.</p>
<p>A high-level timeline of the disclosure is provided below:</p>
<ul>
<li>September 8, 2023: Trail of Bits submitted report to the CERT/CC</li>
<li>September 11, 2023: CERT/CC acknowledged the submission of LeftoverLocals and began the process of vendor outreach and CVE assignment with a preliminary disclosure date of December 11, 2023</li>
<li>September 14, 2023: AMD acknowledged the CERT disclosure</li>
<li>September 15, 2023: Qualcomm acknowledged the CERT disclosure</li>
<li>September 22, 2023: The case report was shared with Khronos and OpenCL working group</li>
<li>September 29, 2023: NVIDIA acknowledged disclosure and confirmed they were not affected by the vulnerability</li>
<li>November 22, 2023: ToB extended release of embargo to January 16, 2024 to accommodate for vendor requests for further time</li>
<li>January 11, 2024: We received a notice that Qualcomm provided a patch to their firmware that addresses this issue only for some of their devices. Additionally, Google noted that ChromeOS Stable 120 and LTS 114 will be released on January 16 to include AMD and Qualcomm mitigations.</li>
<li><span>January 13, 2024: Apple confirmed that the A17 and M3 series processors contain fixes to the vulnerability.</span></li>
<li><span>January 14, 2024: Google notified us that they observed that that some Imagination GPUs are impacted.</span></li>
<li>January 16, 2024: Embargo lift and public disclosure of LeftoverLocals</li>
</ul>
<h2>Moving forward</h2>
<p>Now that GPUs are being used in a wide range of applications, including privacy sensitive applications, we believe that the wider GPU systems community (vendors, researchers, developers) must work towards hardening the GPU system stack and corresponding specifications. This should be accomplished through robust, holistic specifications that describe both GPU programs’ behavior and how GPU devices integrate with the rest of the system stack (e.g., the OS or hypervisor). Furthermore, these specifications should be rigorously tested to account for the diversity of GPU systems and safety requirements of diverse application domains. Looking forward, a wide variety of <a href="https://www.theinformation.com/articles/the-twelve-startups-battling-for-a-slice-of-nvidias-pie?utm_source=ti_app">new AI chips</a> are being developed and will require rigorous security analysis.</p>
<p>There are positive developments in this direction. For example, AMD’s <a href="https://www.amd.com/en/products/software/rocm.html">ROCm</a> stack is open, and thus available for independent rigorous evaluation, and the Khronos Group has <a href="https://www.khronos.org/syclsc">safety critical specification groups</a>. Additionally, cross-vendor programming frameworks, such as Vulkan, have been incredibly useful for writing portable test suites, as opposed to single-vendor programming frameworks.</p>
<p>While GPU security and privacy guarantees are scattered and scarce, the Vulkan <a href="https://registry.khronos.org/vulkan/specs/1.3-extensions/html/vkspec.html#fundamentals-validusage">specification</a> outlines a reasonable definition of security for GPU platforms to adhere to—a definition that several platforms clearly violate, as our results show:</p>
<blockquote><p><em>… implementations must ensure that […] an application does not affect the integrity of the operating system[…]. In particular, any guarantees made by an operating system about whether memory from one process can be visible to another process or not must not be violated by a Vulkan implementation for any memory allocation.</em></p></blockquote>
<p><span>Given the role of Khronos specifications in this result, we included the Khronos Group in the coordinated disclosure. They connected us with representatives of various impacted vendors, and engaged in fruitful discussions about security specifications and testing. Prior to the release, Khronos released this statement in support of this work:</span></p>
<blockquote><p><i><span>Khronos welcomes the work by Tyler Sorensen and Trail of Bits to increase security around the usage of Khronos APIs and have been working closely with them for several months to ensure that API implementers are aware and able to act on any issues. Khronos is also diligently exploring additional actions relating to API specifications, conformance testing, and platform vendor cooperation to continually strengthen safety and security when using Khronos compute and rendering APIs. – Neil Trevett, Khronos President</span></i></p></blockquote>
<p>With the dust settling, our position is the following: given the wide diversity of GPUs and their critical importance in enabling machine learning applications, these devices, and their ecosystems, are in need of (1) a detailed threat model that considers the various types of data processed on GPUs and how this data might be compromised; (2) an exploration of the GPU execution stack to determine where and how GPU security properties should be specified and implemented; and (3) significant testing and auditing to fortify GPU ecosystem, which is the computational foundation of machine learning.</p>
<p><em><span>For full transparency, we note that Tyler Sorensen has been an invited member of the Khronos group (sponsored by Google) since 2019, and participates in the memory model technical specification group.</span></em></p>
<p><strong>Acknowledgements</strong>: We thank Max Ammann, Dominik Czarnota, Kelly Kaoudis, Jay Little, and Adelin Travers for their insightful comments and feedback on the vulnerability, PoC, and throughout the disclosure process. We also thank the Khronos Group for discussing technical specification details with us, and providing an avenue for us to engage with many vendors. We thank CERT/CC, specifically Vijay Sarvepalli and Ben Koo, for organizing the coordinated disclosure, especially considering the potential breadth of the vulnerability. Thanks to Adam Sorensen and Trent Brunson for helping create the vulnerability logo. <span>Finally, t</span><span>hank you to everyone who engaged with us on this issue. T</span><span>his was a large project and we had </span><span>discussions</span><span> with many people who provided valuable insights and perspectives.</span></p>
<h2>Background: How GPUs work</h2>
<p>GPUs are massively parallel, throughput-oriented co-processors. While originally designed to accelerate graphics workloads, their design, which balances flexible programming and high computational throughput, has been highly effective in a variety of applications. Perhaps the most impactful current application domain is machine learning, where GPUs are the computational workhorse and achieve nearly all major results in this area.</p>
<p>GPUs are not only in large servers; they are in our phones, our tablets, and our laptops. These GPUs come from a variety of vendors, with almost all major hardware vendors (Apple, AMD, Arm, Qualcomm, Intel, and Imagination) producing their own GPU architecture. These GPUs are increasingly used for ML tasks, especially because doing ML locally can preserve users’ privacy, achieve lower latency, and reduce computational burdens on service providers.</p>
<p><strong>GPU architecture</strong>: GPU architecture has a parallel, hierarchical structure. At the top level, a GPU is made up of Compute Units (sometimes called Streaming Multiprocessors in NVIDIA literature). Large, discrete GPUs contain many compute units, and smaller, mobile GPUs have fewer. For example, the large AMD Radeon RX 7900 XT discrete GPU has 84 compute units, while the mobile Qualcomm Adreno 740 GPU has 8. All compute units have access to global memory. On discrete GPUs, global memory is implemented using VRAM; on integrated GPUs, global memory simply uses the CPU’s main memory.</p>
<p>Compute units encapsulate both compute and memory components. Compute units contain an array of processing elements; these simple cores are the fundamental units of computation and execute a stream of GPU instructions. In terms of memory, compute units often contain a cache for global memory, but they also contain a special region of memory called local memory. This is an optimized memory region that is shared only across processing elements in the same compute unit. This memory can be accessed with significantly less latency than global memory, but also has much smaller capacity. Different GPUs have varying amounts of local memory, typically ranging from 16KB to 64KB. For example, the AMD Radeon RX 7900 XT GPU has 84 compute units and a local memory size of 64KB; thus, the total amount of local memory on the GPU is ~5MB. Local memory is a software-managed cache: the program executing on the processing elements is responsible for loading values into local memory (e.g., values that will be repeatedly used from global memory).</p>
<p><img loading="lazy" decoding="async" data-attachment-id="105927" data-permalink="https://blog.trailofbits.com/2024/01/16/leftoverlocals-listening-to-llm-responses-through-leaked-gpu-local-memory/figfig7/" data-orig-file="https://i0.wp.com/blog.trailofbits.com/wp-content/uploads/2024/01/figfig7.png?fit=1384%2C786&amp;ssl=1" data-orig-size="1384,786" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="figfig7" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/blog.trailofbits.com/wp-content/uploads/2024/01/figfig7.png?fit=300%2C170&amp;ssl=1" data-large-file="https://i0.wp.com/blog.trailofbits.com/wp-content/uploads/2024/01/figfig7.png?fit=690%2C392&amp;ssl=1" src="https://i0.wp.com/blog.trailofbits.com/wp-content/uploads/2024/01/figfig7.png?resize=604%2C343&amp;ssl=1" alt="" width="604" height="343" srcset="https://i0.wp.com/blog.trailofbits.com/wp-content/uploads/2024/01/figfig7.png?resize=1024%2C582&amp;ssl=1 1024w, https://i0.wp.com/blog.trailofbits.com/wp-content/uploads/2024/01/figfig7.png?resize=300%2C170&amp;ssl=1 300w, https://i0.wp.com/blog.trailofbits.com/wp-content/uploads/2024/01/figfig7.png?resize=768%2C436&amp;ssl=1 768w, https://i0.wp.com/blog.trailofbits.com/wp-content/uploads/2024/01/figfig7.png?resize=1200%2C682&amp;ssl=1 1200w, https://i0.wp.com/blog.trailofbits.com/wp-content/uploads/2024/01/figfig7.png?w=1384&amp;ssl=1 1384w" sizes="(max-width: 604px) 100vw, 604px" data-recalc-dims="1"></p>
<p><strong>GPU execution model</strong>: A GPU program, called a (GPU) kernel, is written in a shader language. Common examples are SPIR-V (Vulkan), OpenCL C, (OpenCL), and Metal Shading Language (Metal). These kernels specify a single entry point function, called the kernel function, which is executed by many invocations (i.e., GPU threads). Invocations have unique built-in identifiers (such as a global ID), which can be used to index a unique data element in a data-parallel program. Invocations are further partitioned into workgroups. Each workgroup is mapped to a compute unit (although many workgroups may execute on the same compute unit, depending on resource requirements). All invocations have access to the same global memory, but only invocations in the same workgroup will share the same local memory.</p>
<p>Applications that use the GPU often launch many short-running kernels. These kernels often correspond to basic operations, such as matrix multiplication or convolution. Kernels can then be executed in sequence; for example, each layer in a deep neural network will be a kernel execution. Local memory is statically allocated at each kernel launch and is not specified to persist across kernel calls.</p>
<p>Platforms generally do not time-multiplex different GPU kernels. That is, if multiple kernels are launched simultaneously (e.g., by different users), the GPU will execute one kernel to competition before the next kernel starts. Because GPU kernels are typically short running, sharing GPU resources at kernel boundaries saves expensive preemption overhead while also maintaining acceptable latency in practice.</p>
<p><strong>Terminology</strong>: Because this blog post focuses on portable GPU computing, it uses OpenCL GPU terminology. For readers more familiar with GPU terminology from a different framework (e.g., CUDA or Metal), we provide the following translation table:</p>
<p><img loading="lazy" decoding="async" data-attachment-id="105930" data-permalink="https://blog.trailofbits.com/2024/01/16/leftoverlocals-listening-to-llm-responses-through-leaked-gpu-local-memory/figfig8/" data-orig-file="https://i0.wp.com/blog.trailofbits.com/wp-content/uploads/2024/01/figfig8.png?fit=1256%2C288&amp;ssl=1" data-orig-size="1256,288" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="figfig8" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/blog.trailofbits.com/wp-content/uploads/2024/01/figfig8.png?fit=300%2C69&amp;ssl=1" data-large-file="https://i0.wp.com/blog.trailofbits.com/wp-content/uploads/2024/01/figfig8.png?fit=690%2C158&amp;ssl=1" src="https://i0.wp.com/blog.trailofbits.com/wp-content/uploads/2024/01/figfig8.png?resize=611%2C140&amp;ssl=1" alt="" width="611" height="140" srcset="https://i0.wp.com/blog.trailofbits.com/wp-content/uploads/2024/01/figfig8.png?resize=1024%2C235&amp;ssl=1 1024w, https://i0.wp.com/blog.trailofbits.com/wp-content/uploads/2024/01/figfig8.png?resize=300%2C69&amp;ssl=1 300w, https://i0.wp.com/blog.trailofbits.com/wp-content/uploads/2024/01/figfig8.png?resize=768%2C176&amp;ssl=1 768w, https://i0.wp.com/blog.trailofbits.com/wp-content/uploads/2024/01/figfig8.png?resize=1200%2C275&amp;ssl=1 1200w, https://i0.wp.com/blog.trailofbits.com/wp-content/uploads/2024/01/figfig8.png?w=1256&amp;ssl=1 1256w" sizes="(max-width: 611px) 100vw, 611px" data-recalc-dims="1"></p>

			</div><!-- .entry-content -->

	
</article><!-- #post-105881 -->
						<!-- #nav-below -->
		
					<!-- #comments .comments-area -->

			
			</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[On being listed as an artist whose work was used to train Midjourney (713 pts)]]></title>
            <link>https://catandgirl.com/4000-of-my-closest-friends/</link>
            <guid>39016395</guid>
            <pubDate>Tue, 16 Jan 2024 17:58:00 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://catandgirl.com/4000-of-my-closest-friends/">https://catandgirl.com/4000-of-my-closest-friends/</a>, See on <a href="https://news.ycombinator.com/item?id=39016395">Hacker News</a></p>
<div id="readability-page-1" class="page"><div href="" rel="bookmark" title="Permanent Link to Generation Gaps">
          <h2>4,000 of My Closest Friends</h2>
        </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[5 minutes of coding yields a 6%+ boost to Linux I/O performance (119 pts)]]></title>
            <link>https://www.phoronix.com/news/Linux-Caching-Time-Block-IO</link>
            <guid>39016337</guid>
            <pubDate>Tue, 16 Jan 2024 17:53:17 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.phoronix.com/news/Linux-Caching-Time-Block-IO">https://www.phoronix.com/news/Linux-Caching-Time-Block-IO</a>, See on <a href="https://news.ycombinator.com/item?id=39016337">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
<p><img alt="LINUX STORAGE" src="https://www.phoronix.com/assets/categories/linuxstorage.webp" width="100" height="100"></p><p>
IO_uring creator and Linux block subsystem maintainer Jens Axboe spent about five minutes working on two patches to implement caching for issue-side time querying in the block layer and can yield 6% or more better I/O performance.
</p><p>
Axboe <a href="https://twitter.com/axboe/status/1747016366891442220">shared</a> about his latest interesting Linux I/O performance optimization, "<em>Something I've had in the back of my mind for years, and finally did it today. Which is kind of sad, since it was literally a 5 min job, yielding a more than 6% improvement. Would likely be even larger on a full scale distro style kernel config.</em>"
</p><p>
Axboe explained he typically disables iostats when testing due to the performance overhead of the time querying by default. But when providing some basic caching for the issue-side time querying, he's seeing around a 6% boost to IOPS and for a more bloated Linux distribution vendor kernel the gains are likely more significant.
</p><p><img src="https://www.phoronix.net/image.php?id=optane-linux-raid&amp;image=optane_raid_2_med" alt="Intel Optane storage"></p>
<p>He detailed in the <a href="https://lore.kernel.org/linux-block/20240115215840.54432-1-axboe@kernel.dk/">RFC patch series</a>:
</p><blockquote>"Querying the current time is the most costly thing we do in the block layer per IO, and depending on kernel config settings, we may do it many times per IO.
<p>
None of the callers actually need nsec granularity. Take advantage of that by caching the current time in the plug, with the assumption here being that any time checking will be temporally close enough that the slight loss of precision doesn't matter.
</p><p>
If the block plug gets flushed, eg on preempt or schedule out, then we invalidate the cached clock.
<br>...
<br>which is more than a 6% improvement in performance. Looking at perf diff, we can see a huge reduction in time overhead:
</p><p>
    10.55%     -9.88%  [kernel.vmlinux]  [k] read_tsc
<br>     1.31%     -1.22%  [kernel.vmlinux]  [k] ktime_get
</p><p>
Note that since this relies on blk_plug for the caching, it's only applicable to the issue side. But this is where most of the time calls happen anyway. It's also worth nothing that the above testing doesn't enable any of the higher cost CPU items on the block layer side, like wbt, cgroups, iocost, etc, which all would add additional time querying. IOW, results would likely look even better in comparison with those enabled, as distros would do."</p></blockquote>
<p>A nice win and hopefully this continues to pan out and prove useful for upstreaming with the Linux v6.9 cycle in a few months,</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Bees have an internal sense of time (2022) (113 pts)]]></title>
            <link>https://greenrosechemistry.com/how-scientists-proved-that-bees-can-perceive-time/</link>
            <guid>39016040</guid>
            <pubDate>Tue, 16 Jan 2024 17:31:31 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://greenrosechemistry.com/how-scientists-proved-that-bees-can-perceive-time/">https://greenrosechemistry.com/how-scientists-proved-that-bees-can-perceive-time/</a>, See on <a href="https://news.ycombinator.com/item?id=39016040">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
                        <p>Did you know that bees can perceive time? We don’t mean simple circadian rhythm stuff, like knowing when the sun is up. They can actually internally track the hours going by, which is presumably helpful when gathering pollen.</p>
<p>But how could we possibly know this? Well, like any science experiment, it started with a weird observation, and then scientists got curious and started guessing and testing how it works. A <a href="https://www.youtube.com/watch?v=xlGuBT5GT10">video on Tom Lun’s YouTube channel</a> brilliantly tells the story of in short format, but if you don’t feel like watching a video, read on!</p>
<h3>How to train a bee</h3>
<p>The investigation started back in 1929, when Ingeborg Beling, a German chronobiologist (no, really, it’s an <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5785655/">actual field</a>!) trained honeybees to come out of their hive at a specific time of day by putting sugar water out at that time. Unfortunately her 1929 paper is <a href="https://link.springer.com/article/10.1007/BF00340159">behind a paywall</a>, and also in German, but Beling’s groundbreaking work found that the bees’ memory of time (<em>Zeitgedächtnis</em>, if you want to really impress your friends) was quite precise and dependable, but only on a 24-hour cycle. Trying to train them to come out every 19 hours was no good, and every other day didn’t work either.</p>
<p>“Ah,” you may be thinking, “but that’s easy, there’s so many cues like sunshine and warmth to tell you the time!” So naturally the next step was to repeat the experiment in a dark, temperature-controlled room. Other factors eliminated were humidity, which is fairly easy to control, and air ionisation, which required using radioactive substances. Finally, O. Wahl decided to rule out cosmic radiation by <a href="https://psycnet.apa.org/record/1934-00291-001">repeating the experiment</a> 180 metres below ground, in a salt mine. Even in a salt mine, the bees were dependable timekeepers.</p>
<h3>Research intensifies</h3>
<p>That might seem like enough proof for most people, but not for chronobiologists. They carried out <a href="http://symposium.cshlp.org/content/25/361">more experiments</a>, raising bees in a special incubator to try and disrupt their 24-hour rhythm, speeding up or slowing down their metabolism with chemicals, and even cooling them down to near-freezing temperatures. This last one did actually have an effect–cooling a bee down for about five hours will make them 3-6 hours late for their sugar water appointment. So I guess now we know how to embarrass a bee.</p>
<h3>Bees get jet lag</h3>
<p>The final piece of proof was the most convincing. Researchers reasoned that if bees were relying on external factors for their 24-hour timekeeping, those factors must be linked to the earth’s 24-hour rotational period. Changing the bees’ longitude (or time zone, as most people call it) would prove whether they were relying on external factors or their internal sense of time. Max Renner, yet another German chronobiologist, <a href="http://symposium.cshlp.org/content/25/361">settled the matter</a> once and for all by training some bees in Paris, then flying them on a red-eye to New York. He found that the bees were jet-lagged, searching for food at their normal Paris time. He followed it up by showing that if the experiments were done outside, the bees would gradually recover from jet lag, just like humans, using the sun to reset their internal clocks!</p>
<p>So, in short, we now know that bees have an internal sense of time, and also that chronobiology conferences are probably pretty wild.</p>
<p><img fetchpriority="high" decoding="async" src="https://greenrosechemistry.com/wp-content/uploads/2022/03/bees-4812811_1920-1024x254.jpg" alt="bees" width="1024" height="254" srcset="https://greenrosechemistry.com/wp-content/uploads/2022/03/bees-4812811_1920-1024x254.jpg 1024w, https://greenrosechemistry.com/wp-content/uploads/2022/03/bees-4812811_1920-300x75.jpg 300w, https://greenrosechemistry.com/wp-content/uploads/2022/03/bees-4812811_1920-768x191.jpg 768w, https://greenrosechemistry.com/wp-content/uploads/2022/03/bees-4812811_1920-1536x382.jpg 1536w, https://greenrosechemistry.com/wp-content/uploads/2022/03/bees-4812811_1920.jpg 1920w" sizes="(max-width: 1024px) 100vw, 1024px"></p>
<h3>Why do we care?</h3>
<p>These studies show that bees (and insects as a whole) have better cognitive abilities than we previously thought. Bees have tiny brains – only <a href="https://www.imperial.ac.uk/news/171050/bee-brains-have-never-seen-them/">2 cubic millimetres</a>, which is about 0.0002% the size of a human brain. We’re now learning that they can do more with that than we expected. Studies have suggested that time measuring could be a marker for more complex cognitive functions in the species, so now we’re interested in what else we can discover about bees.</p>
<p>In terms of timekeeping, we’ve learned that <a href="https://www.cell.com/current-biology/comments/S0960-9822(06)01843-4">bees can measure short intervals</a> of 6-36 seconds, and may even keep track of multiple different timings at once. That’s more than most humans can do!</p>
<h3>How does it work?</h3>
<p>The short answer is, we don’t know yet, and the bees aren’t telling. A number of potential timekeeping methods have been suggested: counting heartbeats, <a href="https://news.vanderbilt.edu/2021/05/20/research-snapshot-bees-can-tell-time-by-temperature-vanderbilt-research-finds/">temperature cycles in the hive</a>, or even simply an innate sense of time passing. It may be that the short and long timing functions depend on two different mechanisms. This area doesn’t get a huge amount of research funding, so it may be some time before scientists figure it out.</p>
<h3>Further reading</h3>
<p>‘<a href="https://www.pnas.org/doi/10.1073/pnas.1408039111" target="_blank" rel="noopener"><em>Way-finding in displaced clock-shifted bees prove bees use a cognitive map</em></a>’ by J.F. Cheeseman <em>et al</em>., <em>P.N.A.S.</em>, 2014</p>

                    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Understanding x86_64 Paging (150 pts)]]></title>
            <link>https://zolutal.github.io/understanding-paging/</link>
            <guid>39015377</guid>
            <pubDate>Tue, 16 Jan 2024 16:44:30 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://zolutal.github.io/understanding-paging/">https://zolutal.github.io/understanding-paging/</a>, See on <a href="https://news.ycombinator.com/item?id=39015377">Hacker News</a></p>
<div id="readability-page-1" class="page"><section itemprop="text">
        
        <p>I’ve spent quite a lot of time messing with x86_64 page tables, understanding address translation is not easy and when I started learning about it I felt like a lot of the material out there on how it works was hard for me to wrap my head around. So in this blog post I am going to attempt to provide a kind of “what I wish I had when learning about paging”.</p>

<p>Quick note, I’ll only be discussing paging in the context of PML4 (Page Map Level 4) since it’s currently the dominant x86_64 paging scheme and probably will be for a while still.</p>

<h2 id="environment">environment</h2>

<p>Its not necessary, but I recommend that you have a Linux kernel debugging setup with QEMU + gdb prepared to follow along with. If you’ve never done this, maybe give this repo a shot: <a href="https://github.com/deepseagirl/easylkb">easylkb</a> (I’ve never used it, but I’ve heard good things) or if you want to avoid having to setup the environment yourself, the practice mode on any of the Kernel Security challenges on <a href="https://pwn.college/">pwn.college</a> would also work (<code>vm connect</code> and <code>vm debug</code> are the commands to know).</p>

<p>I suggest this because I think running the same commands I am on your own and being able to perform a page walk based on what you can see in gdb is a good test of understanding.</p>

<h2 id="wtf-is-a-page">wtf is a page</h2>

<p>On x86_64 a page is a 0x1000 byte slice of memory which is 0x1000 byte aligned.</p>

<p>This is the reason why if you ever look at /proc/&lt;pid&gt;/maps you see that all the address ranges will start and end with an address ending with 0x000 because the minimum size of a memory mapping on x86_64 is page size (0x1000 bytes) and pages are required to be ‘page aligned’ (the last 12 bits must be zero).</p>

<p>A ‘Virtual Page’ will be resolved to a single ‘Physical Page’ (aka ‘Page Frame’) by your MMU though many Virtual Pages may refer to the same Physical Page.</p>

<h2 id="what-is-in-a-virtual-address">what is in a virtual address</h2>

<p>PML4, as one might guess, has four level of paging structures, these paging structures are called ‘Page Tables’. A page table is a page-sized memory region which contains 512 8-byte page table entries. Each entry of a page table will refer to either the next level page table or to the final physical address a virtual address resolves to.</p>

<p>The entry from a page table that is used for address translation is based on the virtual address of the memory access. With 512 entries per level, that means 9-bits of the virtual address are used at every level to index into the corresponding page table.</p>

<p>Say we have an address like this:</p>

<p><code>0x7ffe1c9c9000</code></p>

<p>The last 12 bits of this address represent the offset within the physical page:</p>

<p><code>0x7ffe1c9c9000 &amp; 0xfff = 0x0</code></p>

<p>This means that once we determine the physical address of the page this virtual address resolves to, we will add zero to the result to get the final physical address.</p>

<p>After the last 12 bits, which is again just the offset within the final page, a virtual address is comprised of indicies into the page tables. As mentioned each level of paging uses 9 bits of the virtual address, so the lowest level of the paging structures, a Page Table, is indexed by the next 9 bits of the address (by bit masking with <code>&amp; 0x1ff</code> on the shifted value). For the following levels we just need to shift right by another nine bits each time and again mask off the lower nine bits as our index. Doing this for the address above gives us these indicies:</p>

<div><pre><code>Level 1, Page Table (PT):
Index = (0x7ffe1c9c9000 &gt;&gt; 12) &amp; 0x1ff = 0x1c9

Level 2, Page Middle Directory (PMD):
Index = (0x7ffe1c9c9000 &gt;&gt; 21) &amp; 0x1ff = 0x0e4

Level 3, Page Upper Directory (PUD):
Index = (0x7ffe1c9c9000 &gt;&gt; 30) &amp; 0x1ff = 0x1f8

Level 4, Page Global Directory (PGD):
Index = (0x7ffe1c9c9000 &gt;&gt; 39) &amp; 0x1ff = 0x0ff
</code></pre></div>

<h2 id="all-your-base">all your base</h2>

<p>Now that we know how to index into page tables and vaguely what they contain, where actually are they???</p>

<p>Well each thread of your CPU has a page table base register called <code>cr3</code>.</p>

<p><code>cr3</code> holds the physical address of the highest level of the paging structure, aka the Page Global Directory (PGD).</p>

<p>From gdb, when debugging the kernel, you can read the contents of <code>cr3</code> like this:</p>

<div><pre><code>gef➤  p/x $cr3
$1 = 0x10d664000
</code></pre></div>

<p>The <code>cr3</code> register can hold some additional information besides just the PGD address depending on what processor features are in use, so a more general way of getting the physical address of the PGD from the <code>cr3</code> register is to mask off the lower 12 bits of its contents like so:</p>

<div><pre><code>gef➤  p/x $cr3 &amp; ~0xfff
$2 = 0x10d664000
</code></pre></div>

<h2 id="page-table-entries">page table entries</h2>

<p>Lets look at what is at that physical address we got from <code>cr3</code> in gdb. The <code>monitor xp/...</code> command that is exposed to gdb by the QEMU Monitor lets us print out the physical memory of the vm and doing <code>monitor xp/512gx ...</code> will print the entire contents, all 512 entries, of the PGD referred to by <code>cr3</code>:</p>

<div><pre><code>gef➤  monitor xp/512gx 0x10d664000
...
000000010d664f50: 0x0000000123fca067 0x0000000123fc9067
000000010d664f60: 0x0000000123fc8067 0x0000000123fc7067
000000010d664f70: 0x0000000123fc6067 0x0000000123fc5067
000000010d664f80: 0x0000000123fc4067 0x0000000123fc3067
000000010d664f90: 0x0000000123fc2067 0x000000000b550067
000000010d664fa0: 0x000000000b550067 0x000000000b550067
000000010d664fb0: 0x000000000b550067 0x0000000123fc1067
000000010d664fc0: 0x0000000000000000 0x0000000000000000
000000010d664fd0: 0x0000000000000000 0x0000000000000000
000000010d664fe0: 0x0000000123eab067 0x0000000000000000
000000010d664ff0: 0x000000000b54c067 0x0000000008c33067
</code></pre></div>

<p>This produces a lot of output and most of it is zero, so I’m only including the tail of the output here.</p>

<p>This output probably doesn’t mean much to you yet, but we can observe some patterns in the data, lots of the 8-byte entries end in <code>0x67</code>, for example.</p>

<h2 id="decoding-a-pgd-entry">decoding a PGD entry</h2>

<p>From the PGD output above, lets take the PGD entry at <code>0x000000010d664f50</code> with value <code>0x0000000123fca067</code> as an example to see how to decode an entry.</p>

<p>and lets do this with the binary representation of that entry’s value:</p>

<div><pre><code>gef➤  p/t 0x0000000123fca067
$6 = 100100011111111001010000001100111
</code></pre></div>

<p>Here is a little diagram to show what each bit in the entry represents:</p>

<div><pre><code>~ PGD Entry ~                                                   Present ──────┐
                                                            Read/Write ──────┐|
                                                      User/Supervisor ──────┐||
                                                  Page Write Through ──────┐|||
                                               Page Cache Disabled ──────┐ ||||
                                                         Accessed ──────┐| ||||
                                                         Ignored ──────┐|| ||||
                                                       Reserved ──────┐||| ||||
┌─ NX          ┌─ Reserved                             Ignored ──┬──┐ |||| ||||
|┌───────────┐ |┌──────────────────────────────────────────────┐ |  | |||| ||||
||  Ignored  | ||               PUD Physical Address           | |  | |||| ||||
||           | ||                                              | |  | |||| ||||
0000 0000 0000 0000 0000 0000 0000 0001 0010 0011 1111 1100 1010 0000 0110 0111
       56        48        40        32        24        16         8         0
</code></pre></div>

<p>and here’s a key for what each of those label mean:</p>

<ul>
  <li>NX (Not Executable) – if this bit is set, no memory mapping that is a descendant of this PGD entry will be executable.</li>
  <li>Reserved – these values must be zero.</li>
  <li>PUD Physical Address – the physical address of the PUD associated with this PGD entry.</li>
  <li>Accessed –  If any pages referred to by this entry or its descendants, this bit will be set by the MMU, and can be cleared by the OS.</li>
  <li>Page Cache Disabled (PCD) – pages descendant of this PGD entry should not enter the CPU’s cache hierarchy, sometimes also called the ‘Uncacheable’ (UC) bit.</li>
  <li>Page Write Through (WT) – writes to pages descendant of this PGD entry should immediately write to RAM rather than buffering writes to CPU cache before eventually updating RAM.</li>
  <li>User/Supervisor – if this bit is unset, pages descendant of this PGD cannot be accessed unless in supervisor mode.</li>
  <li>Read/Write – if this bit is unset, pages descendant of this PGD cannot be written to.</li>
  <li>Present – if this bit is unset then the processor will not use this entry for address translation and none of the other bits will apply.</li>
</ul>

<p>The bits that we really care about here are the the Present bit, the ones representing the physical address of the next level of the paging structures, the PUD Physical Address bits, and the permission bits: NX, User/Supervisor, and Read/Write.</p>

<ul>
  <li>The Present bit is super important because without it set the rest of the entry is ignored.</li>
  <li>The PUD Physical Address lets us continue page walking by telling us where the physical address of the next level of the paging structures is at.</li>
  <li>The Permission bits all apply to pages which are descendants of the PGD entry and determine how those pages are able to be accesssed.</li>
</ul>

<p>The remaining bits are not as important for our purposes:</p>
<ul>
  <li>The Accessed bit is set if the entry was used in translating a memory access, its not important for page walking.</li>
  <li>Page Cache Disabled and Page Write Through are not used for normal memory mappings and do not affect page translation or permissions so lets ignore them.</li>
</ul>

<p>So decoding this entry, we learn:</p>

<p>The PUD is Present:</p>
<div><pre><code>gef➤  p/x 0x0000000123fca067 &amp; 0b0001
$18 = 0x1
</code></pre></div>
<p>The mappings in the PUD and below may be able to be Writable:</p>
<div><pre><code>gef➤  p/x 0x0000000123fca067 &amp; 0b0010
$19 = 0x2
</code></pre></div>
<p>The mappings in the PUD and below may be able to be User accessible:</p>
<div><pre><code>gef➤  p/x 0x0000000123fca067 &amp; 0b0100
$20 = 0x4
</code></pre></div>
<p>The PUD’s physical address ( bits (51:12] ) is <code>0x123fca000</code>:</p>
<div><pre><code>gef➤  p/x 0x0000000123fca067 &amp; ~((1ull&lt;&lt;12)-1) &amp; ((1ull&lt;&lt;51)-1)
$21 = 0x123fca000
</code></pre></div>
<p>The mappings in the PUD and below may be able to be Executable:</p>
<div><pre><code>gef➤  p/x 0x0000000123fca067 &amp; (1ull&lt;&lt;63)
$22 = 0x0
</code></pre></div>

<h2 id="decoding-entries-for-all-levels">decoding entries for all levels</h2>

<p>Now that we’ve seen how to decode a PGD entry, decoding the rest of the levels aren’t so much different, at least in the common case.</p>

<p>For all of these diagrams ‘X’ means the bit can be either zero or one, otherwise, if a bit is set to a specific value then that value is either required by the architecture or by the specific encoding shown by the diagram.</p>

<h3 id="pgd">PGD</h3>

<div><pre><code>~ PGD Entry ~                                                   Present ──────┐
                                                            Read/Write ──────┐|
                                                      User/Supervisor ──────┐||
                                                  Page Write Through ──────┐|||
                                               Page Cache Disabled ──────┐ ||||
                                                         Accessed ──────┐| ||||
                                                         Ignored ──────┐|| ||||
                                                       Reserved ──────┐||| ||||
┌─ NX          ┌─ Reserved                             Ignored ──┬──┐ |||| ||||
|┌───────────┐ |┌──────────────────────────────────────────────┐ |  | |||| ||||
||  Ignored  | ||               PUD Physical Address           | |  | |||| ||||
||           | ||                                              | |  | |||| ||||
XXXX XXXX XXXX 0XXX XXXX XXXX XXXX XXXX XXXX XXXX XXXX XXXX XXXX XXXX 0XXX XXXX
       56        48        40        32        24        16         8         0
</code></pre></div>

<p>This one we’ve already seen, I described it in detail in the previous section, but here it is without that specific PGD entry filled in.</p>

<h3 id="pud">PUD</h3>

<div><pre><code>~ PUD Entry, Page Size unset ~                                  Present ──────┐
                                                            Read/Write ──────┐|
                                                      User/Supervisor ──────┐||
                                                  Page Write Through ──────┐|||
                                               Page Cache Disabled ──────┐ ||||
                                                         Accessed ──────┐| ||||
                                                         Ignored ──────┐|| ||||
                                                      Page Size ──────┐||| ||||
┌─ NX          ┌─ Reserved                             Ignored ──┬──┐ |||| ||||
|┌───────────┐ |┌──────────────────────────────────────────────┐ |  | |||| ||||
||  Ignored  | ||               PMD Physical Address           | |  | |||| ||||
||           | ||                                              | |  | |||| ||||
XXXX XXXX XXXX 0XXX XXXX XXXX XXXX XXXX XXXX XXXX XXXX XXXX XXXX XXXX 0XXX XXXX
       56        48        40        32        24        16         8         0
</code></pre></div>

<p>As you can see the diagram above for the PUD is very similar to the one for the PGD, the only difference is the introduction of the ‘Page Size’ bit. The Page Size bit being set changes how we need to interpret a PUD entry quite a lot. For this diagram we are assuming it is unset, which is the most common case.</p>

<h3 id="pmd">PMD</h3>

<div><pre><code>~ PMD Entry, Page Size unset ~                                  Present ──────┐
                                                            Read/Write ──────┐|
                                                      User/Supervisor ──────┐||
                                                  Page Write Through ──────┐|||
                                               Page Cache Disabled ──────┐ ||||
                                                         Accessed ──────┐| ||||
                                                         Ignored ──────┐|| ||||
                                                      Page Size ──────┐||| ||||
┌─ NX          ┌─ Reserved                             Ignored ──┬──┐ |||| ||||
|┌───────────┐ |┌──────────────────────────────────────────────┐ |  | |||| ||||
||  Ignored  | ||                PT Physical Address           | |  | |||| ||||
||           | ||                                              | |  | |||| ||||
XXXX XXXX XXXX 0XXX XXXX XXXX XXXX XXXX XXXX XXXX XXXX XXXX XXXX XXXX 0XXX XXXX
       56        48        40        32        24        16         8         0
</code></pre></div>

<p>Again, the PMD diagram is very similar to the previous diagram, and like with the PUD entry, we are ignoring the Page Size bit for now.</p>

<h3 id="pt">PT</h3>

<div><pre><code>~ PT Entry ~                                                    Present ──────┐
                                                            Read/Write ──────┐|
                                                      User/Supervisor ──────┐||
                                                  Page Write Through ──────┐|||
                                               Page Cache Disabled ──────┐ ||||
                                                         Accessed ──────┐| ||||
┌─── NX                                                    Dirty ──────┐|| ||||
|┌───┬─ Memory Protection Key              Page Attribute Table ──────┐||| ||||
||   |┌──────┬─── Ignored                               Global ─────┐ |||| ||||
||   ||      | ┌─── Reserved                          Ignored ───┬─┐| |||| ||||
||   ||      | |┌──────────────────────────────────────────────┐ | || |||| ||||
||   ||      | ||            4KB Page Physical Address         | | || |||| ||||
||   ||      | ||                                              | | || |||| ||||
XXXX XXXX XXXX 0XXX XXXX XXXX XXXX XXXX XXXX XXXX XXXX XXXX XXXX XXXX XXXX XXXX
       56        48        40        32        24        16         8         0
</code></pre></div>

<p>At the Page Table entry things get more interesting, there are some new fields/attributes that weren’t there in the previous levels.</p>

<p>Those new fields/attributes are:</p>

<ul>
  <li>Memory Protection Key (MPK or PK): This is an x86_64 extension that allows assigning a 4-bit keys to pages which can be used to configure memory permissions for all pages with that key.</li>
  <li>Global: This has to do with how the TLB (Translation Lookaside Buffer, the MMU’s cache for virtual to physical address translations) caches the translation for th page, this bit being set means the page will not be flushed from the TLB on context switch, this is commonly enabled on Kernel pages to reduce TLB misses.</li>
  <li>Page Attribute Table (PAT): If set, the MMU should consult the Page Attribute Table MSR when determining whether the ‘Memory Type’ of the page, e.g. whether this page is ‘Uncacheable’, ‘Write Through’, or one of a few other memory types.</li>
  <li>Dirty: This bit is similar to the accessed bit, it gets set by the MMU if this page was written to and must be reset by the OS.</li>
</ul>

<p>None of these actually affect the address translation itself, but the configuration of the Memory Protection Key can mean that the expected memory access permissions for the page referred to by this entry may be stricter than what is encoded by the entry itself.</p>

<p>Unlike the previous levels, since this is the last level, the entry holds the final physical address of the page associated with the virtual address we are translating. Once you apply a bit-mask to get the physical address bytes and add the last 12 bits of the original virtual address (the offset within the page), you have your physical address!</p>

<p>Hopefully, this doesn’t seem so bad, the general case of page walking is just a few steps:</p>
<ul>
  <li>Convert the virtual address to indicies and a page offset by shifting the address and applying bitmasks</li>
  <li>Read <code>cr3</code> to get the physical address of the PGD</li>
  <li>For each level until the last:
    <ul>
      <li>Use the indicies calculated from the virtual address to know what entry from the page table to use</li>
      <li>Apply a bitmask to the entry to get the physical address of the next level</li>
    </ul>
  </li>
  <li>On the final level, again find the entry corresponding with the index from the virtual address</li>
  <li>Apply a bitmask to get the physical address of the page associated with the virtual address</li>
  <li>Add offset within the page from the virtual address to the page’s physical address</li>
  <li>Done!</li>
</ul>

<h2 id="hugeify">hugeify</h2>

<p>As mentioned, the previous diagrams for the PUD and PMD are for the common case, when the Page Size bit is not set.</p>

<p>So, what about when it is set?</p>

<p>When it is set that is effectively telling the MMU, pack it up, we’re done here, don’t keep page walking, the current entry holds the physical address of the page we are looking for.</p>

<p>But there is a bit more to it than that, the physical address of the page in entries where the Page Size bit is set isn’t for a normal 4KB (0x1000 byte) page, it is a ‘Huge Page’ which comes in two variants: 1GB Huge Pages and 2MB Huge Pages.</p>

<p>When a PUD entry has the Page Size bit set then it refers to a 1GB Huge Page, and when a PMD has the Page Size bit set it refers to a 2MB Huge Page.</p>

<p>But where do the 1GB and 2MB numbers come from?</p>

<p>Each page table level holds up to 512 entries, that means a PT can refer to at most 512 pages and <code>512 * 4KB = 2MB</code>. So a Huge Page at the PMD level effectively means that the entry refers to a page that is the same size as a full PT.</p>

<p>Extending this to the PUD level, we just multiply by 512 again to get the size of a full PMD that has full PTs: <code>512 * 512 * 4KB = 1GB</code>.</p>

<h3 id="huge-page-pud">Huge Page PUD</h3>

<div><pre><code>~ PUD Entry, Page Size set ~                                     Present ─────┐
                                                             Read/Write ─────┐|
                                                       User/Supervisor ─────┐||
                                                   Page Write Through ─────┐|||
                                                Page Cache Disabled ─────┐ ||||
                                                          Accessed ─────┐| ||||
                                                            Dirty ─────┐|| ||||
┌─── NX                                                Page Size ─────┐||| ||||
|┌───┬─── Memory Protection Key                         Global ─────┐ |||| ||||
||   |┌──────┬─── Ignored                             Ignored ───┬─┐| |||| ||||
||   ||      | ┌─── Reserved           Page Attribute Table ───┐ | || |||| ||||
||   ||      | |┌────────────────────────┐┌───────────────────┐| | || |||| ||||
||   ||      | || 1GB Page Physical Addr ||      Reserved     || | || |||| ||||
||   ||      | ||                        ||                   || | || |||| ||||
XXXX XXXX XXXX 0XXX XXXX XXXX XXXX XXXX XX00 0000 0000 0000 000X XXXX 1XXX XXXX
       56        48        40        32        24        16         8         0
</code></pre></div>

<p>When the page size bit is set notice that the PUD entry looks more like a PT entry than a normal PUD entry, which makes sense because it is also referring to a page rather than a page table.</p>

<p>There are some distinctions from a PT entry though:</p>
<ol>
  <li>The Page Size bit is where the Page Attribute Table (PAT) bit is at on a PT, so the PAT bit is relocated to bit 12.</li>
  <li>The physical address of a 1GB Huge Page is required to have 1GB alignment in physical memory, this is why the new reserved bits exist and why bit 12 is able to be repurposed as the PAT bit.</li>
</ol>

<p>Overall, not too much new here, the only other differences when dealing with huge pages really is that a different bitmask needs to be applied to the address to get the bits for the physical address of the page, also the 1GB alignment means when calculating the physical address of a virtual address within the page we need to use a mask based on 1GB alignment instead of 4KB alignment.</p>

<h3 id="huge-page-pmd">Huge Page PMD</h3>

<div><pre><code>~ PMD Entry, Page Size set ~                                     Present ─────┐
                                                             Read/Write ─────┐|
                                                       User/Supervisor ─────┐||
                                                   Page Write Through ─────┐|||
                                                Page Cache Disabled ─────┐ ||||
                                                          Accessed ─────┐| ||||
                                                            Dirty ─────┐|| ||||
┌─── NX                                                Page Size ─────┐||| ||||
|┌───┬─── Memory Protection Key                         Global ─────┐ |||| ||||
||   |┌──────┬─── Ignored                             Ignored ───┬─┐| |||| ||||
||   ||      | ┌─── Reserved         Page Attribute Table ─────┐ | || |||| ||||
||   ||      | |┌───────────────────────────────────┐┌────────┐| | || |||| ||||
||   ||      | ||     2MB Page Physical Address     ||Reserved|| | || |||| ||||
||   ||      | ||                                   ||        || | || |||| ||||
XXXX XXXX XXXX 0XXX XXXX XXXX XXXX XXXX XXXX XXXX XXX0 0000 000X XXXX 1XXX XXXX
       56        48        40        32        24        16         8         0
</code></pre></div>

<p>This is very similar to the PUD entry with the Page Size bit set, the only thing that has changed is that since the alignment is smaller for the 2MB pages at this level, there are less reserved bits set.</p>

<p>The 2MB alignment means the offset within the huge page should be calculated using a mask based on 2MB alignment.</p>

<h2 id="going-for-a-walk">going for a walk</h2>

<p>So the last section was a lot of diagrams, in this section lets look at how to actually do a page walk manually in gdb.</p>

<h3 id="preparation">preparation</h3>

<p>With a booted up vm and gdb attached I first will pick an address to do a page walk on, as an example I’ll use the current stack pointer while running in the kernel:</p>

<div><pre><code>gef➤  p/x $rsp
$42 = 0xffffffff88c07da8
</code></pre></div>

<p>Now we have the address we are going to walk, lets also get the physical address of the PGD from <code>cr3</code>:</p>

<div><pre><code>gef➤  p/x $cr3 &amp; ~0xfff
$43 = 0x10d664000
</code></pre></div>

<p>I’ll use this little python function to extract the page table offsets from the virtual address:</p>

<div><pre><code><span>def</span> <span>get_virt_indicies</span><span>(</span><span>addr</span><span>):</span>
    <span>pageshift</span> <span>=</span> <span>12</span>
    <span>addr</span> <span>=</span> <span>addr</span> <span>&gt;&gt;</span> <span>pageshift</span>
    <span>pt</span><span>,</span> <span>pmd</span><span>,</span> <span>pud</span><span>,</span> <span>pgd</span> <span>=</span> <span>(((</span><span>addr</span> <span>&gt;&gt;</span> <span>(</span><span>i</span><span>*</span><span>9</span><span>))</span> <span>&amp;</span> <span>0x1ff</span><span>)</span> <span>for</span> <span>i</span> <span>in</span> <span>range</span><span>(</span><span>4</span><span>))</span>
    <span>return</span> <span>pgd</span><span>,</span> <span>pud</span><span>,</span> <span>pmd</span><span>,</span> <span>pt</span>
</code></pre></div>

<p>which outputs this:</p>
<div><pre><code><span>In</span> <span>[</span><span>2</span><span>]:</span> <span>get_virt_indicies</span><span>(</span><span>0xffffffff88c07da8</span><span>)</span>
<span>Out</span><span>[</span><span>2</span><span>]:</span> <span>(</span><span>511</span><span>,</span> <span>510</span><span>,</span> <span>70</span><span>,</span> <span>7</span><span>)</span>
</code></pre></div>

<h3 id="pgd-1">PGD</h3>

<p>The index we got for the PGD based on the virtual address was 511, multiplying 511 by 8 will let us get the byte offset into the PGD that the PGD entry for our virtual address starts at:</p>

<div><pre><code>gef➤  p/x 511*8
$44 = 0xff8
</code></pre></div>

<p>adding that offset to the PGD’s physical address gets us the physical address of the PGD entry:</p>

<div><pre><code>gef➤  p/x 0x10d664000+0xff8
$45 = 0x10d664ff8
</code></pre></div>

<p>and reading the physical memory at that address gets us the PGD entry itself:</p>
<div><pre><code>gef➤  monitor xp/gx 0x10d664ff8
000000010d664ff8: 0x0000000008c33067
</code></pre></div>

<p>Looks like the entry has the last three bits (present, user, and writeable) set, and the top bit (NX) is unset, meaning there aren’t any restrictions so far on the permissions of the pages associated with this virtual address.</p>

<p>Masking the bits [12, 51) gives us the physical address of the PUD:</p>

<div><pre><code>gef➤  p/x 0x0000000008c33067 &amp; ~((1&lt;&lt;12)-1) &amp; ((1&lt;&lt;51) - 1)
$46 = 0x8c33000
</code></pre></div>

<h3 id="pud-1">PUD</h3>

<p>The index we got for the PUD based on the virtual address was 510, multiplying 510 by 8 will let us get the byte offset into the PUD that the PUD entry for our virtual address starts at:</p>

<div><pre><code>gef➤  p/x 510*8
$47 = 0xff0
</code></pre></div>

<p>adding that offset to the PUD’s physical address gets us the physical address of the PUD entry:</p>

<div><pre><code>gef➤  p/x 0x8c33000+0xff0
$48 = 0x8c33ff0
</code></pre></div>

<p>and reading the physical memory at that address gets us the PUD entry itself:</p>
<div><pre><code>gef➤  monitor xp/gx 0x8c33ff0
0000000008c33ff0: 0x0000000008c34063
</code></pre></div>

<p>At this level we need to start paying attention to the Size Bit (bit 7), because if it is a 1GB page we would stop our page walk here.</p>

<div><pre><code>gef➤  p/x 0x0000000008c34063 &amp; (1&lt;&lt;7)
$49 = 0x0
</code></pre></div>

<p>Seems it is unset on this entry so we will continue page walking.</p>

<p>Notice also that the PUD entry ends in 0x3 and not 0x7 like the previous level, the bottom two bits (present, writeable) are still set but the third bit, the user bit is now unset. That means that usermode accesses to pages belonging to this PUD entry will result in a page fault due to the failed permission check on the access.</p>

<p>The NX bit is still unset, so pages belonging to this PUD can still be executable.</p>

<p>Masking the bits [12, 51) gives us the physical address of the PMD:</p>

<div><pre><code>gef➤  p/x 0x0000000008c34063 &amp; ~((1ull&lt;&lt;12)-1) &amp; ((1ull&lt;&lt;51)-1)
$50 = 0x8c34000
</code></pre></div>

<h3 id="pmd-1">PMD</h3>

<p>The index we got for the PMD based on the virtual address was 70, multiplying 70 by 8 will let us get the byte offset into the PMD that the PMD entry for our virtual address starts at:</p>

<div><pre><code>gef➤  p/x 70*8
$51 = 0x230
</code></pre></div>

<p>adding that offset to the PMD’s physical address gets us the physical address of the PMD entry:</p>

<div><pre><code>gef➤  p/x 0x8c34000+0x230
$52 = 0x8c34230
</code></pre></div>

<p>and reading the physical memory at that address gets us the PMD entry itself:</p>
<div><pre><code>gef➤  monitor xp/gx 0x8c34230
0000000008c34230: 0x8000000008c001e3
</code></pre></div>

<p>Again, at this level we need paying attention to the Size Bit, because if it is a 2MB page we will stop our page walk here.</p>

<div><pre><code>gef➤  p/x 0x8000000008c001e3 &amp; (1&lt;&lt;7)
$53 = 0x80
</code></pre></div>

<p>Looks like our virtual address refers to a 2MB Huge Page! so the physical address in this PMD entry is the physical address of that Huge Page.</p>

<p>Also, looking at the permission bits, looks like the page is still present and writeable and the user bit is still unset, so this page is only accessible from supervisor mode (ring-0).</p>

<p>Unlike the previous levels, the top bit, the NX bit, is set:</p>

<div><pre><code>gef➤  p/x 0x8000000008c001e3 &amp; (1ull&lt;&lt;63)
$54 = 0x8000000000000000
</code></pre></div>

<p>So this Huge Page is not executable memory.</p>

<p>Applying a bitmask on bits [21:51) gets us the physical address of the huge page:</p>

<div><pre><code>gef➤  p/x 0x8000000008c001e3 &amp; ~((1ull&lt;&lt;21)-1) &amp; ((1ull&lt;&lt;51)-1)
$56 = 0x8c00000
</code></pre></div>

<p>Now we need to apply a mask to the virtual address based on 2MB page alignment to get the offset into the Huge Page.</p>

<p>2MB is equivalent to <code>1&lt;&lt;21</code> so applying a bitmask of <code>(1ull&lt;&lt;21)-1</code> will get us the offset:</p>

<div><pre><code>gef➤  p/x 0xffffffff88c07da8 &amp; ((1ull&lt;&lt;21)-1)
$57 = 0x7da8
</code></pre></div>

<p>Now adding this offset to the base address of the 2MB Huge Page will get us the physical address associated with the virtual address we started with:</p>

<div><pre><code>gef➤  p/x 0x8c00000 + 0x7da8
$58 = 0x8c07da8
</code></pre></div>

<p>Looks like the Virtual Address: <code>0xffffffff88c07da8</code> has a Physical Address of: <code>0x8c07da8</code>!</p>

<h3 id="checking-ourselves">checking ourselves</h3>

<p>There are a few ways to test that we page walked correctly, an easy check is to just dump the memory at the virtual and physical address and compare them, if they look the same we were probably right:</p>

<p>Physical:</p>
<div><pre><code>gef➤  monitor xp/10gx 0x8c07da8
0000000008c07da8: 0xffffffff810effb6 0xffffffff88c07dc0
0000000008c07db8: 0xffffffff810f3685 0xffffffff88c07de0
0000000008c07dc8: 0xffffffff8737dce3 0xffffffff88c3ea80
0000000008c07dd8: 0xdffffc0000000000 0xffffffff88c07e98
0000000008c07de8: 0xffffffff8138ab1e 0x0000000000000000
</code></pre></div>

<p>Virtual:</p>
<div><pre><code>gef➤  x/10gx 0xffffffff88c07da8
0xffffffff88c07da8:	0xffffffff810effb6	0xffffffff88c07dc0
0xffffffff88c07db8:	0xffffffff810f3685	0xffffffff88c07de0
0xffffffff88c07dc8:	0xffffffff8737dce3	0xffffffff88c3ea80
0xffffffff88c07dd8:	0xdffffc0000000000	0xffffffff88c07e98
0xffffffff88c07de8:	0xffffffff8138ab1e	0x0000000000000000
</code></pre></div>

<p>Looks good to me!</p>

<p>Another way to check is using the <code>monitor gva2gpa</code> (guest virtual address to guest physical address) command exposed to gdb by the QEMU Monitor:</p>

<div><pre><code>gef➤  monitor gva2gpa 0xffffffff88c07da8
gpa: 0x8c07da8
</code></pre></div>

<p>Assuming QEMU is doing address translation correctly (probably a fair assumption), then looks like we have double confirmation that our page walk was successful!</p>

<h2 id="wrapping-up">wrapping up</h2>

<p>Hopefully by the end of this you have a pretty solid understanding of how paging works on x86_64 systems. I wanted to pack a lot of information into the post so it took some thought to figure out how to organize all of it and I’m still not sure if this was a great way to go about it.</p>

<p>Anyways, I think paging is pretty neat and I think its one of those things where once you get it you’ve got it, but getting to that point can take some time and some screwing around in gdb.</p>

<p>I’d also like to mention that the inspiration for the diagrams of the various page table entries I made for this post came from the documentation of the <a href="https://github.com/jart/blink/">blink</a> project: <a href="https://github.com/jart/blink/blob/46d82a0ced97c0df1fc645c5d81a88f0d142fbfd/blink/machine.h#L61">blink/machine.h</a>.</p>

<p>Thanks for reading!</p>

        
      </section></div>]]></description>
        </item>
        <item>
            <title><![CDATA[A NetBSD/amd64 guest can now boot in 18ms (107 pts)]]></title>
            <link>https://old.reddit.com/r/BSD/comments/197vfmq/a_netbsdamd64_guest_can_now_boot_in_40ms_details/</link>
            <guid>39015036</guid>
            <pubDate>Tue, 16 Jan 2024 16:15:33 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://old.reddit.com/r/BSD/comments/197vfmq/a_netbsdamd64_guest_can_now_boot_in_40ms_details/">https://old.reddit.com/r/BSD/comments/197vfmq/a_netbsdamd64_guest_can_now_boot_in_40ms_details/</a>, See on <a href="https://news.ycombinator.com/item?id=39015036">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>I've been working the past 3 months into catching up with Colin Percival's work on FreeBSD with the Firecracker hypervisor.<br>
The result is that NetBSD/amd64 can now boot in PVH mode, i.e. directly into the kernel using qemu's -kernel flag or the PVH-enabled version of AWS's Firecracker.<br>
I then added support for MMIO kernel command line parameters, so NetBSD now supports memory mapped backed devices like ld(4) or vioif(4).
Last but not least, I've been tracking boot time until reaching 40ms from the assembly entry point to handling over to userland.</p>

<p>The latest branch with performances is here: <a href="https://github.com/NetBSDfr/NetBSD-src/tree/perf">https://github.com/NetBSDfr/NetBSD-src/tree/perf</a>
The experimental branch with "only" PVH and MMIO support is here: <a href="https://github.com/NetBSDfr/NetBSD-src/tree/mmio_cmdline">https://github.com/NetBSDfr/NetBSD-src/tree/mmio_cmdline</a></p>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Benchmarks and comparison of LLM AI models and API hosting providers (138 pts)]]></title>
            <link>https://artificialanalysis.ai</link>
            <guid>39014985</guid>
            <pubDate>Tue, 16 Jan 2024 16:11:31 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://artificialanalysis.ai">https://artificialanalysis.ai</a>, See on <a href="https://news.ycombinator.com/item?id=39014985">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><h3>Subscribe to our newsletter</h3><form><label for="email">Email address</label></form></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[TinyML: Ultra-low power Machine Learning (346 pts)]]></title>
            <link>https://www.ikkaro.net/what-tinyml-is/</link>
            <guid>39014866</guid>
            <pubDate>Tue, 16 Jan 2024 16:03:25 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.ikkaro.net/what-tinyml-is/">https://www.ikkaro.net/what-tinyml-is/</a>, See on <a href="https://news.ycombinator.com/item?id=39014866">Hacker News</a></p>
<div id="readability-page-1" class="page"><div itemprop="text">
			

<figure><img fetchpriority="high" decoding="async" width="1024" height="320" src="https://www.ikkaro.net/wp-content/uploads/2024/01/what-is-tinyml-1024x320.jpg" alt="What TinyML is" srcset="https://www.ikkaro.net/wp-content/uploads/2024/01/what-is-tinyml-1024x320.jpg 1024w, https://www.ikkaro.net/wp-content/uploads/2024/01/what-is-tinyml-300x94.jpg 300w, https://www.ikkaro.net/wp-content/uploads/2024/01/what-is-tinyml-768x240.jpg 768w, https://www.ikkaro.net/wp-content/uploads/2024/01/what-is-tinyml-1536x480.jpg 1536w, https://www.ikkaro.net/wp-content/uploads/2024/01/what-is-tinyml.jpg 1920w" sizes="(max-width: 1024px) 100vw, 1024px"></figure>



<p>TinyML or Tiny Machine Learning refers to the <strong>use of Machine Learning in microcontrollers</strong>. In systems that unlike those used in traditional ML have few resources, are systems that have little CPU, little RAM and extremely low power consumption in the order of magnitude of milliwatts or microwatts.</p>



<p>Its official website is the <a href="https://www.tinyml.org/" target="_blank" rel="noreferrer noopener">TinyML Foundation</a>.</p>



<p>What is done is to reduce large models for use with equipment with very few resources and microcontrollers. The preferred field of the Makers.</p>




<p>I have started a series of 3 courses offered by Harvard for free</p>



<ol>
<li><em><a href="https://www.edx.org/learn/machine-learning/harvard-university-fundamentals-of-tinyml" target="_blank" rel="noreferrer noopener">Fundamentals of TinyML</a></em> (What do I build, what for and what are the problems)</li>



<li><em>Applications of TinyML</em> (data-driven, bias, etc) </li>



<li><em>Deploying TinyML</em> (where do we put our models, security and privacy)</li>
</ol>



<p>The following notes are from the first Fundamentals of TinyML where they explain what it is, when it is applied, the different techniques that are used, etc, etc.</p>



<p>Embedded systems using microcontrollers cannot work with the large models, as they have memories up to 256kB. Here are some examples of operating systems that can be used with microcontrollers</p>



<ul>
<li><a href="https://www.freertos.org/index.html" target="_blank" rel="noreferrer noopener">FreeRTOS</a></li>



<li><a href="https://os.mbed.com/mbed-os/" target="_blank" rel="noreferrer noopener">Mbed OS</a></li>
</ul>



<p><a href="https://www.ikkaro.net/machine-learning/" target="_blank" rel="noreferrer noopener">Machine Learning</a> consists of algorithms that search for patterns in data.</p>



<p>With TinyML, techniques are used to compress these algorithms so that they remain effective in finding patterns in data.</p>



<p>There are 5 quintillion bytes of data produced daily by IoT and only less than 1% is analyzed.</p>



<h2><span id="Algorithm_compression_techniques">Algorithm compression techniques</span></h2>



<p>Some algorithm compression techniques are:</p>



<h3><span id="Pruning">Pruning</span></h3>



<p><strong>Pruning Synapsis:</strong> We remove network connections from the model. Sometimes it can decrease the accuracy.</p>



<p><strong>Pruning Neurons:</strong> We can also eliminate entire neurons from our model which reduces the computational demand of the network.</p>



<h3><span id="Quantization">Quantization</span></h3>



<p>It consists of discretizing the values within a small range. For example if we discretize a float within the range -128 to 127 we only have to traverse 256 values. Going from a float point value that is stored in 4 bytes to an integer value that is stored in 1 byte implies a x4 reduction in size.</p>



<p>Quantization is going to be critical in TinyML due to the limited resources available.</p>



<h3><span id="Knowledge_distillation">Knowledge distillation</span></h3>



<p>Apply our knowledge and know how to make the model small.</p>



<h2><span id="Tools">Tools</span></h2>



<p>We use Tensor Flow Lite. While tensorFlow is focused on ML Researcher, Tensor Flow Lite is for Application Developer.</p>



<h2><span id="Uses_of_TinyML">Uses of TinyML</span></h2>



<p>Although they are not cited, of course being on this website we can find <strong>uses of TinyML dedicated to the DIY, Maker and Hacker world</strong>.</p>



<h3><span id="Uses_of_TinyML_in_Industry">Uses of TinyML in Industry</span></h3>



<p>In Industry, in maintenance, to warn us when there are vibrations that indicate that there will be breakage, etc, etc. increases efficiency and reduces costs. The negative points are the accuracy that can give us false alarms. In case of false alarm whose responsibility is the operator or the system.</p>



<h3><span id="TinyML_in_the_environment">TinyML in the environment</span></h3>



<p>Instead of collecting data that then has to be processed, with TinyML we have real-time answers about changes in the environment, for example in the life of wild animals.</p>



<h3><span id="TinyML_for_humans">TinyML for humans</span></h3>



<p>Helps people with disabilities to perform more tasks without having to use their hands. Improving the UI and UX of applications to make them easier to use.</p>



<p>We build technology to improve our experience as humans. Technology has to help people</p>



<h2><span id="Risks_and_downsides">Risks and downsides</span></h2>



<ul>
<li>Will it work well across all population groups? </li>



<li>Is the privacy of our data assured? </li>



<li>Can we protect this data?</li>
</ul>



<p>We have to create technology based on human-centered AI. Design, development and deployment</p>
<!-- AI CONTENT END 1 -->
		</div></div>]]></description>
        </item>
    </channel>
</rss>