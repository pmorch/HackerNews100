<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0">
    <channel>
        <title>HN100 - Readable Contents</title>
        <link>https://hn.algolia.com/api/v1/search_by_date?tags=%28story,poll%29&amp;numericFilters=points%3E100</link>
        <description>Uses Readability to add bodies to the RSS feed</description>
        <lastBuildDate>Wed, 19 Feb 2025 15:30:03 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>en</language>
        <item>
            <title><![CDATA[Doge Claimed It Saved $8B in One Contract. It Was $8M (120 pts)]]></title>
            <link>https://www.nytimes.com/2025/02/18/upshot/doge-contracts-musk-trump.html</link>
            <guid>43101757</guid>
            <pubDate>Wed, 19 Feb 2025 13:12:34 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.nytimes.com/2025/02/18/upshot/doge-contracts-musk-trump.html">https://www.nytimes.com/2025/02/18/upshot/doge-contracts-musk-trump.html</a>, See on <a href="https://news.ycombinator.com/item?id=43101757">Hacker News</a></p>
Couldn't get https://www.nytimes.com/2025/02/18/upshot/doge-contracts-musk-trump.html: Error: Request failed with status code 403]]></description>
        </item>
        <item>
            <title><![CDATA[Debugging Hetzner: Uncovering failures with powerstat, sensors, and dmidecode (132 pts)]]></title>
            <link>https://www.ubicloud.com/blog/debugging-hetzner-uncovering-failures-with-powerstat-sensors-and-dmidecode</link>
            <guid>43101430</guid>
            <pubDate>Wed, 19 Feb 2025 12:40:58 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.ubicloud.com/blog/debugging-hetzner-uncovering-failures-with-powerstat-sensors-and-dmidecode">https://www.ubicloud.com/blog/debugging-hetzner-uncovering-failures-with-powerstat-sensors-and-dmidecode</a>, See on <a href="https://news.ycombinator.com/item?id=43101430">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-animation="default" data-collapse="medium" data-duration="400" data-easing="ease" data-easing2="ease" data-doc-height="1" role="banner"><div><p>EuroGPT Enterprise is open source, runs in Europe, and keeps your data private. <a href="https://www.ubicloud.com/use-cases/eurogpt-enterprise">Try it now</a></p></div><div><p><a href="https://www.ubicloud.com/"><img src="https://cdn.prod.website-files.com/64f9d9b4e737e7b37d4e39a4/64fe48116c52fe1a51e17279_ubicolud%20logo.png" loading="lazy" alt=""></a></p></div></div><div id="w-node-decdb48f-56e8-4c35-c577-932285e9b439-0c072a00"><p>February 17, 2025 · 5 min read</p><div><p><img src="https://cdn.prod.website-files.com/64f9d9b4e737e7b37d4e39a4/669a6467d686c690fa7e7ac6_Burak.jpg" loading="lazy" sizes="40px" srcset="https://cdn.prod.website-files.com/64f9d9b4e737e7b37d4e39a4/669a6467d686c690fa7e7ac6_Burak-p-500.jpg 500w, https://cdn.prod.website-files.com/64f9d9b4e737e7b37d4e39a4/669a6467d686c690fa7e7ac6_Burak.jpg 512w" alt="Burak Yucesoy"></p><div><p>Burak Yucesoy</p><p>Principal Software Engineer</p></div></div><div><p>At Ubicloud, we build software that turns bare metal providers into cloud platforms. One of the providers we like is Hetzner because of their affordable and reliable servers.</p><p>About a year ago, Hetzner launched the AX162 server line. It offers better performance and a lower price than its predecessor, AX161. We were very excited to adopt it, but we soon encountered serious reliability issues. We observed that the new servers were 16 times more likely to crash. After months of debugging and working with Hetzner, the solution only came after several hardware updates. Although the journey was painful, we learned a lot from it and wanted to share our experience.</p></div><div id="some-terminology"><h3>What Happened?</h3><div><p>Three weeks after purchasing our first AX162 server, one of the servers crashed. We checked the system logs and found NULL bytes. These usually mean there was an abrupt failure, like a power loss, which stopped the system from finishing its writing process. Hetzner performed a hardware check but found nothing unusual. A week later, we experienced another crash, followed by several more over the next few days.</p><p>In the days that followed, the crash frequency increased. For each crash, Hetzner checked the hardware. Sometimes, they found a defect and replaced the server. Other times, they found nothing unusual. We contacted Hetzner about the frequent crashes, but it was hard to find a clear cause.</p><p>At this point, we observed a few interesting patterns:</p></div><ul role="list"><li>All crashes occurred on AX162 servers.</li><li><p>There were two types of crashes:</p><ul role="list"><li>The server comes back online after a manual restart.</li><li>The server wouldn't respond to restart requests or diagnostic codes sent by Hetzner engineers. Hetzner would replace the server in these cases.</li></ul></li><li>The servers usually run smoothly for an extended period. However, once a server experiences its first crash, further crashes become more likely. After the server experiences the first type of crash several times, it would eventually have the second type of crash and be replaced.</li></ul></div><div><h3 id="Red-Hat-Reference-Architecture">Initial Investigations</h3><p>We started testing different ideas to find out what caused the crashes.</p><div><h4>System Load</h4><p>We considered the possibility of increased load on the machine causing issues. The AX162 machines come with 96 vCPUs, and we had workloads that utilized all of them at the same time. Consistent high load, for example, could lead to increased temperatures and unexpected issues. However, when we reviewed the load levels at the times of crashes, we found several instances where crashes occurred even under low or no load.</p></div><div><h4>Temperature</h4><p>We wanted to check if there is a correlation between high temperatures and crashes. It is possible to collect the temperature of various components in the system with sensors command.</p><div><pre><code>$&gt; sensors
coretemp-isa-0000
Adapter: ISA adapter
Package id 0:  +51.0°C  (high = +100.0°C, crit = +100.0°C)
Core 0:        +45.0°C  (high = +100.0°C, crit = +100.0°C)
Core 4:        +46.0°C  (high = +100.0°C, crit = +100.0°C)
Core 8:        +51.0°C  (high = +100.0°C, crit = +100.0°C)
Core 9:        +51.0°C  (high = +100.0°C, crit = +100.0°C)
Core 10:       +51.0°C  (high = +100.0°C, crit = +100.0°C)
Core 11:       +51.0°C  (high = +100.0°C, crit = +100.0°C)
Core 12:       +49.0°C  (high = +100.0°C, crit = +100.0°C)
Core 13:       +49.0°C  (high = +100.0°C, crit = +100.0°C)
Core 14:       +49.0°C  (high = +100.0°C, crit = +100.0°C)
Core 15:       +49.0°C  (high = +100.0°C, crit = +100.0°C)</code></pre></div><p>We wrote a simple cron job to collect temperature data. When the servers crashed again, we checked the data. The temperature levels were not significantly higher than average at the time of the crashes.</p></div><div><h4>Faulty Components</h4><p>Commands like <span>lshw</span> and <span>dmidecode</span> are useful to gather information regarding hardware parts, including model and serial numbers.</p><div><pre><code>$&gt; dmidecode -t 2
# dmidecode 3.3
Getting SMBIOS data from sysfs.
SMBIOS 3.3.0 present.
Handle 0x0200, DMI type 2, 8 bytes
Base Board Information
        Manufacturer: Dell Inc.
        Product Name: 0H3K7P
        Version: A08
        Serial Number: .51R1H04.MXWSJ0039D004Z.</code></pre></div><p>We compared the components of AX162 servers that had crashed with those that hadn’t. We found no significant differences. We even checked how serial numbers increase, because we thought older components might fail more often. But crashes happened even in servers with the latest serial numbers.</p></div><div><h4>Power Consumption</h4><div><p>Power, rather than space, often limits data center expansion. To increase the number of machines under power constraints, data center operators usually cap power use per machine. However, this can cause motherboards to degrade more quickly. Although we didn’t know if Hetzner was limiting power consumption, the symptoms suggested this might be a factor. Repeated server crashes after a long period of stability usually mean the hardware is wearing out. We also eliminated all other hypotheses we had one by one, which only left power limiting as a strong hypothesis.</p><p>With the <span>powerstat</span> tool, we measured the maximum power consumption over a long period.</p></div><div><pre><code>$&gt; powerstat -R
  Time   User Nice  Sys  Idle   IO Run Ctxt/s  IRQ/s Fork Exec Exit  Watts
14:17:15  3.1  0.0  0.0  96.9  0.0   5    430   1593    0    0    0 166.54 
14:17:16  3.1  0.0  0.0  96.9  0.0   5    425   1638    1    1    1 166.51 
14:17:17  3.1  0.0  0.0  96.9  0.0   5    570   1737    0    0    0 166.50 
14:17:18  3.1  0.0  0.0  96.9  0.0   5    609   1787    0    0    0 166.48 
14:17:19  3.1  0.0  0.0  96.9  0.0   5    469   1662    0    0    0 166.49 
...
</code></pre></div><p>We then compared our measurements with the advertised amounts.</p><div><table><thead><tr><th>Model</th><th>Advertised Max. Power Consumption (Watt)</th><th>Measured Max. Power Consumption (Watt)</th></tr></thead><tbody><tr><td>AX161</td><td><p>147 (<a href="https://web.archive.org/web/20240223142827/https://www.hetzner.com/dedicated-rootserver/matrix-ax/" target="_blank">1</a>)</p></td><td>168</td></tr><tr><td>AX162</td><td><p>408 (<a href="https://web.archive.org/web/20240228172003/https://www.hetzner.com/dedicated-rootserver/matrix-ax/" target="_blank">2</a>)</p></td><td>266</td></tr><tr></tr></tbody></table></div><p>Based on these numbers, we suspected that Hetzner might indeed be limiting power usage.</p></div><div><h4>Data Collection on Crash Rates and Comparison</h4><p>Although we were observing an increased crash rate, we wanted to support this observation with data. A common way to measure hardware reliability is the Annualized Failure Rate (AFR). It's like the annual run rate, but for component failures. The formula for AFR is:</p><p><img src="https://cdn.prod.website-files.com/64f9d9b4e737e7b37d4e39a4/67ab440611daa766d7af72f7_AFR.png" loading="lazy" sizes="(max-width: 479px) 93vw, (max-width: 767px) 97vw, (max-width: 991px) 94vw, (max-width: 1439px) 58vw, 748.703125px" srcset="https://cdn.prod.website-files.com/64f9d9b4e737e7b37d4e39a4/67ab440611daa766d7af72f7_AFR-p-500.png 500w, https://cdn.prod.website-files.com/64f9d9b4e737e7b37d4e39a4/67ab440611daa766d7af72f7_AFR-p-800.png 800w, https://cdn.prod.website-files.com/64f9d9b4e737e7b37d4e39a4/67ab440611daa766d7af72f7_AFR-p-1080.png 1080w, https://cdn.prod.website-files.com/64f9d9b4e737e7b37d4e39a4/67ab440611daa766d7af72f7_AFR.png 1186w" alt="afr calculation"></p><p>AFR has its own limitations, but it is simple enough to give us a starting point, so we decided to use it. Here are our initial measurements:</p><div><table><thead><tr><th>Model</th><th>Total Failure Count</th><th>Total Days in Service</th><th>Annual Failure Rate</th></tr></thead><tbody><tr><td>AX161</td><td><p>11</p></td><td>3784</td><td>1.06</td></tr><tr><td>AX162</td><td><p>34</p></td><td>737</td><td>16.84</td></tr><tr></tr></tbody></table></div><p>Our observations indicated that AX162 servers are 16 times more likely to experience a failure compared to other models. The data also backed up our first finding: after a server crashes once, it is very likely to crash again. In fact, 80% of servers that crashed once had a second crash within 24 hours</p></div></div><div id="aws-firecracker"><h3>Observing Stability with New Hardware</h3><div><p>We submitted a detailed support ticket with the additional data on power limiting and annualized failure rates. Hetzner didn’t confirm or deny the possibility of power limiting but informed us that they had identified a defect in a batch of motherboards. They had recently received a new batch and recommended replacing the motherboards in our affected servers. Normally, replacing a big part of our fleet can disrupt customer workloads. However, we had already moved most critical tasks from the AX162 servers because they kept crashing, so replacing them was manageable.</p><p>We replaced the motherboards but kept critical workloads off the AX162 servers. We weren't sure the issue was fully resolved. Based on prior experience, we knew that servers appearing stable could still begin to crash frequently even after a month. Thus, we decided to monitor them carefully over an extended period.</p><p>At first, we saw no crashes. Then, after two weeks, servers with the new motherboards started crashing as well.</p></div><div><table><thead><tr><th>Model</th><th>Total Failure Count</th><th>Total Days in Service</th><th>Annual Failure Rate</th></tr></thead><tbody><tr><td>AX161</td><td><p>11</p></td><td>3784</td><td>1.06</td></tr><tr><td>AX162</td><td><p>34</p></td><td>737</td><td>16.84</td></tr><tr><td>AX162 -v2</td><td><p>11</p></td><td>758</td><td>5.30</td></tr></tbody></table></div><div><p>AX162 servers with new motherboards crashed less frequently, but the crash rate was still high. After contacting Hetzner again, we learned of an even newer version of the motherboard with improved reliability. We migrated our servers to this latest version and began monitoring reliability.</p><p>After monitoring the new servers for several months, we concluded that the crash issue is indeed resolved. Additionally, the AFR of these servers is now even better than that of the AX161 servers.</p></div><div><table><thead><tr><th>Model</th><th>Total Failure Count</th><th>Total Days in Service</th><th>Annual Failure Rate</th></tr></thead><tbody><tr><td>AX161</td><td><p>11</p></td><td>3784</td><td>1.06</td></tr><tr><td>AX162</td><td><p>34</p></td><td>737</td><td>16.84</td></tr><tr><td>AX162 -v2</td><td><p>11</p></td><td>758</td><td>5.30</td></tr><tr><td>AX162 -v3</td><td><p>4</p></td><td>3738</td><td>0.39</td></tr></tbody></table></div></div><div id="ubicloud-compute"><h3>Process Improvements</h3><p>Adopting a new line of servers early on can come with unforeseen issues. We were quick to adopt the new servers because their specs were exciting. Also Hetzner’s decision to discontinue the AX161 model suggested the new line was production-ready. Looking back, waiting six months could have helped us avoid many issues. Early adopters usually find problems that get fixed later. Moving forward, we will make the following changes:</p><ul role="list"><li>We will conduct a thorough vetting of future server models.</li><li>We will introduce new hardware gradually, beginning with non-critical workloads.</li><li>We will add more bare metal providers to distribute the risk. In fact, we already support two more bare metal providers; Leaseweb and Latitude. We are also working on adding the fourth one.</li></ul><p>We hope our lessons offer valuable insights to others navigating similar issues. As we develop a solid, open-source alternative to traditional cloud providers, these experiences motivate us to keep improving. We aim to deliver cloud solutions that are both reliable and adaptable.</p></div></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[A few hours ago, the US has turned into a de-facto dictatorship (139 pts)]]></title>
            <link>https://old.reddit.com/r/law/comments/1isvzgu/the_full_executive_order_is_out_this_is_the</link>
            <guid>43099826</guid>
            <pubDate>Wed, 19 Feb 2025 08:15:14 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://old.reddit.com/r/law/comments/1isvzgu/the_full_executive_order_is_out_this_is_the">https://old.reddit.com/r/law/comments/1isvzgu/the_full_executive_order_is_out_this_is_the</a>, See on <a href="https://news.ycombinator.com/item?id=43099826">Hacker News</a></p>
Couldn't get https://old.reddit.com/r/law/comments/1isvzgu/the_full_executive_order_is_out_this_is_the: Error: Request failed with status code 403]]></description>
        </item>
        <item>
            <title><![CDATA[Ensuring Accountability for All Agencies (171 pts)]]></title>
            <link>https://www.whitehouse.gov/presidential-actions/2025/02/ensuring-accountability-for-all-agencies/</link>
            <guid>43098705</guid>
            <pubDate>Wed, 19 Feb 2025 04:49:46 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.whitehouse.gov/presidential-actions/2025/02/ensuring-accountability-for-all-agencies/">https://www.whitehouse.gov/presidential-actions/2025/02/ensuring-accountability-for-all-agencies/</a>, See on <a href="https://news.ycombinator.com/item?id=43098705">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>


<main>
	<div>




<p>&nbsp;By the authority vested in me as President by the Constitution and the laws of the United States of America, it is hereby ordered: &nbsp;</p>



<p>&nbsp; &nbsp; &nbsp;<span>Section</span>&nbsp;<span>1</span>. &nbsp;<span>Policy and Purpose</span>. &nbsp;The Constitution vests all executive power in the President and charges him with faithfully executing the laws. &nbsp;Since it would be impossible for the President to single-handedly perform all the executive business of the Federal Government, the Constitution also provides for subordinate officers to assist the President in his executive duties. &nbsp;In the exercise of their often-considerable authority, these executive branch officials remain subject to the President’s&nbsp;ongoing supervision and control. &nbsp;The President in turn is regularly elected by and accountable to the American people. &nbsp;This is one of the structural safeguards, along with the separation of powers between the executive and legislative branches, regular elections for the Congress, and an independent judiciary whose judges are appointed by the President by and with the advice and consent of the Senate, by which the Framers created a Government accountable to the American people.  &nbsp;</p>



<p>&nbsp; &nbsp; &nbsp;However, previous administrations have allowed so-called “independent regulatory agencies” to operate with minimal Presidential supervision. &nbsp;These regulatory agencies currently exercise substantial executive authority without sufficient accountability to the President, and through him, to the American people. &nbsp;Moreover, these regulatory agencies have been permitted to promulgate significant regulations without review by the President. &nbsp;</p>



<p>&nbsp; &nbsp; &nbsp;These practices undermine such regulatory agencies’ accountability to the American people and prevent a unified and coherent execution of Federal law. &nbsp;For the Federal Government to be truly accountable to the American people, officials who wield vast executive power must be supervised and controlled by the people’s&nbsp;elected President.  &nbsp;</p>



<p>&nbsp; &nbsp; &nbsp;Therefore, in order to improve the administration of the executive branch and to increase regulatory officials’ accountability to the American people, it shall be the policy of&nbsp;the executive branch to ensure Presidential supervision and control of the entire executive branch. &nbsp;Moreover, all executive departments and agencies, including so-called independent agencies, shall submit for review all proposed and&nbsp;final significant regulatory actions to the Office of Information and Regulatory Affairs (OIRA) within the Executive Office of the President before publication in the&nbsp;<em>Federal Register</em>.&nbsp;</p>



<p>&nbsp; &nbsp; &nbsp;<span>Sec</span>.&nbsp;<span>2</span>. &nbsp;<span>Definitions</span>. &nbsp;For the purposes of this order:</p>



<p>&nbsp; &nbsp; &nbsp;(a) &nbsp;The term “employees” shall have the meaning given that term in section 2105 of title 5, United States Code.  &nbsp;</p>



<p>&nbsp; &nbsp; &nbsp;(b) &nbsp;The term “independent regulatory agency” shall have the meaning given that term in section 3502(5) of title 44, United States Code. &nbsp;This order shall not apply to the Board of Governors of the Federal Reserve System or to the Federal Open Market Committee in its conduct of monetary policy. &nbsp;This order shall apply to the Board of Governors of the Federal Reserve System only in connection with its conduct and authorities directly related to its supervision and regulation of financial institutions. &nbsp;</p>



<p>&nbsp; &nbsp; &nbsp;(c) &nbsp;The term “independent regulatory agency chairman” shall mean, with regard to a multi-member independent regulatory agency, the chairman of such agency, and shall mean, with regard to a single-headed independent regulatory agency, such agency’s&nbsp;chairman, director, or other presiding officer.   &nbsp;</p>



<p>&nbsp; &nbsp; &nbsp;(d) &nbsp;The term “head” of an independent regulatory agency shall mean those appointed to supervise independent regulatory agencies and in whom the agencies’ authorities are generally vested, encompassing the chairman, director, or other presiding officer, and, as applicable, other members, commissioners, or similar such officials with responsibility for supervising such agencies.  &nbsp;</p>



<p>&nbsp; &nbsp; &nbsp;Sec.&nbsp;3. &nbsp;OIRA Review of Agency Regulations. &nbsp;(a) &nbsp;Section 3(b) of Executive Order 12866 of September 30, 1993 (“Regulatory Planning and Review”), as amended, is hereby amended to read as follows:  &nbsp;</p>



<p>&nbsp; &nbsp; &nbsp;“(b)&nbsp; “Agency,” unless otherwise indicated, means any authority of the United States that is an “agency” under 44&nbsp;U.S.C. 3502(1), and shall also include the Federal Election Commission. &nbsp;This order shall not apply to the Board of Governors of the Federal Reserve System or to the Federal Open Market Committee in its conduct of monetary policy. &nbsp;This order shall apply to the Board of Governors of the Federal Reserve System only in connection with its conduct and authorities directly related to its supervision and regulation of financial institutions.”.</p>



<p>&nbsp; &nbsp; &nbsp;(b) &nbsp;The Director of the Office of Management and Budget (OMB) shall provide guidance on implementation of this order to the heads of executive departments and agencies newly submitting regulatory actions under section 3(b) of Executive Order 12866. &nbsp;Agency submissions by independent regulatory agencies under such section shall commence within the earlier of 60 days from the date of this order, or completion of such implementation guidance. &nbsp;</p>



<p>&nbsp; &nbsp; &nbsp;<span>Sec</span>.&nbsp;<span>4</span>. &nbsp;<span>Performance Standards and Management Objectives</span>. &nbsp;The Director of OMB shall establish performance standards and management objectives for independent agency heads, as appropriate and consistent with applicable law, and report periodically to the President on their performance and efficiency in attaining such standards and objectives.&nbsp;</p>



<p>&nbsp; &nbsp; &nbsp;<span>Sec</span>.&nbsp;<span>5</span>. &nbsp;<span>Apportionments for Independent Regulatory Agencies</span>. &nbsp;The Director of OMB shall, on an ongoing basis:  &nbsp;</p>



<p>&nbsp; &nbsp; &nbsp;(a) &nbsp;review independent regulatory agencies’ obligations for consistency with the President’s policies and priorities; and  &nbsp;</p>



<p>&nbsp; &nbsp; &nbsp;(b) &nbsp;consult with independent regulatory agency chairmen and adjust such agencies’ apportionments by activity, function, project, or object, as necessary and appropriate, to advance the President’s&nbsp;policies and priorities. &nbsp;Such adjustments to apportionments may prohibit independent regulatory agencies from expending appropriations on particular activities, functions, projects, or objects, so long as such restrictions are consistent with law.&nbsp;</p>



<p>&nbsp; &nbsp; &nbsp;<span>Sec</span>.&nbsp;<span>6</span>. &nbsp;<span>Additional Consultation with the Executive Office of the President</span>. &nbsp;(a) &nbsp;Subject to subsection (b), independent regulatory agency chairmen shall regularly consult with and coordinate policies and priorities with the directors of OMB, the White House Domestic Policy Council, and the White House National Economic Council.  </p>



<p>&nbsp; &nbsp; &nbsp;(b) &nbsp;The heads of independent regulatory agencies shall establish a position of White House Liaison in their respective agencies. &nbsp;Such position shall be in grade 15 of the General Schedule and shall be placed in Schedule C of the excepted service.  </p>



<p>&nbsp; &nbsp; &nbsp;(c) &nbsp;Independent regulatory agency chairmen shall submit agency strategic plans developed pursuant to the Government Performance and Results Act of 1993&nbsp;to the Director of OMB for clearance prior to finalization.&nbsp;</p>



<p>&nbsp; &nbsp; &nbsp;&nbsp;<span>Sec</span>.&nbsp;<span>7</span>. &nbsp;<span>Rules of Conduct Guiding Federal Employees’ Interpretation of the Law</span>. The President and the Attorney General, subject to the President’s supervision and control, shall provide authoritative interpretations of law for the executive branch. &nbsp;The President and the Attorney General’s opinions on questions of law are controlling on all employees in the conduct of their official duties.&nbsp;&nbsp;No employee of the executive branch acting in their official capacity may advance an interpretation of the law as the position of the United States that contravenes the President or the Attorney General’s opinion on a matter of law, including but not limited to the issuance of regulations, guidance, and positions advanced in litigation, unless authorized to do so by the President or in writing by the Attorney General.&nbsp;</p>



<p>&nbsp; &nbsp; &nbsp;&nbsp;<span>Sec</span>.&nbsp;<span>8</span>. &nbsp;<span>General Provisions</span>. &nbsp;(a) &nbsp;If any provision of this order, or the application of any provision to any person or circumstance, is held to be invalid, the remainder of this order and the application of its provisions to any other persons or circumstances shall not be affected thereby. &nbsp;</p>



<p>&nbsp; &nbsp; &nbsp;(b) &nbsp;Nothing in this order shall be construed to impair or otherwise affect: &nbsp;</p>



<p>&nbsp; &nbsp; &nbsp;(i) &nbsp;&nbsp;the authority granted by law to an executive department, agency, or the head thereof; or&nbsp;</p>



<p>&nbsp; &nbsp; &nbsp;(ii) &nbsp;the functions of the Director of the Office of Management and Budget relating to budgetary, administrative, or legislative proposals. &nbsp;</p>



<p>&nbsp; &nbsp; &nbsp;(c) &nbsp;This order shall be implemented consistent with applicable law and subject to the availability of appropriations. &nbsp;</p>



<p>&nbsp; &nbsp; &nbsp;(d) &nbsp;This order is not intended to, and does not, create any right or benefit, substantive or procedural, enforceable at law or in equity by any party against the United States, its departments, agencies, or entities, its officers, employees, or agents, or any other person. &nbsp;</p>
</div>
</main>



</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Thoughts on Daylight Computer (179 pts)]]></title>
            <link>https://jon.bo/posts/daylight-computer-1/</link>
            <guid>43098318</guid>
            <pubDate>Wed, 19 Feb 2025 03:41:48 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://jon.bo/posts/daylight-computer-1/">https://jon.bo/posts/daylight-computer-1/</a>, See on <a href="https://news.ycombinator.com/item?id=43098318">Hacker News</a></p>
Couldn't get https://jon.bo/posts/daylight-computer-1/: Error: timeout of 10000ms exceeded]]></description>
        </item>
        <item>
            <title><![CDATA[Implementing LLaMA3 in 100 Lines of Pure Jax (108 pts)]]></title>
            <link>https://saurabhalone.com/blogs/llama3/web</link>
            <guid>43097932</guid>
            <pubDate>Wed, 19 Feb 2025 02:37:10 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://saurabhalone.com/blogs/llama3/web">https://saurabhalone.com/blogs/llama3/web</a>, See on <a href="https://news.ycombinator.com/item?id=43097932">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
            <article>
                <h2>Implementing LLaMA3 in 100 Lines of Pure Jax</h2>
                
                

                

                <div>

                    <p>In this post, we'll implement <strong>llama3</strong> from scratch using pure <strong>jax</strong> in just 100 lines of code. Why jax? Because I think it has good aesthetics. Also jax looks like a NumPy wrapper but it has some cool features like <strong>xla</strong>; a linear algebra accelerator, <strong>jit</strong>, <strong>vmap</strong>, <strong>pmap</strong> etc., which makes your training go brr brr.</p>
                    <p>Jax is one of the first libraries which strongly focuses on the soul of <strong>pure functional programming</strong> which makes it more cool.<sup>1</sup></p>
                    <p><strong>Note :</strong></p>
                    <div>
                            <ul>
                                <li>This post assumes familiarity with Python and basic understanding of transformer architectures.</li>
                                <li>This implementation is  for educational purposes, which means it is not for any production stuff but it covers all components of the model.<sup>2</sup></li>
                                <li>If you don't wanna read this amazing blog post then you can check out all the code at.<sup>3</sup></li>
                            </ul>
                        </div>

                    <p><img src="https://saurabhalone.com/blogs/llama3/images/newllama.png" alt="Llama architecture">
                        <img src="https://saurabhalone.com/blogs/llama3/images/llamadark.png" alt="Llama architecture">
                    </p>
                </div>


                <div>
                    <h2>Table of Contents</h2>
                    <nav>
                        <ul>
                            <li><a href="#section-1">1. LLaMA3</a></li>
                            <li><a href="#section-2">2. Model Weights Initialization</a></li>
                            <li><a href="#section-3">3. Tokenization</a></li>
                            <li><a href="#section-4">4. Embeddings</a></li>
                            <li><a href="#section-5">5. Root Mean Square Layer Normalization</a></li>
                            <li><a href="#section-6">6. Rotary Positional Encoding</a></li>
                            <li><a href="#section-7">7. Group-Query Attention</a></li>
                            <li><a href="#section-8">8. Transformer-Block</a></li>
                            <li><a href="#section-9">9. Forward-Pass</a></li>
                            <li><a href="#section-10">10. Dataset</a></li>
                            <li><a href="#section-11">11. Loss Function</a></li>
                            <li><a href="#update function">12. Update Function</a></li>
                            <li><a href="#training-loop">13. Training-Loop</a></li>
                            <li><a href="#results">14. Results</a></li>
                        </ul>
                    </nav>
                </div>


                

                <div>
                    <h2 id="section-1">LLaMA3</h2>
                    <p> 
                        At its core, LLaMA 3 is a decoder only transformer language model that generates text one token at a time, 
                        building on previous tokens to predict what comes next ; like completing a sentence word by word.
                   </p>
                    <p>
                        So lets fucking go !! we're doing it, get your diet coke !! First, we will begin with setting up device<sup>5</sup> and configuring the model.
                    </p>
                </div>



<div>
    <pre><code><span># Configure JAX to use GPU and prevent memory preallocation</span>
<span>os</span>.<span>environ</span>[<span>'JAX_PLATFORM_NAME'</span>] = <span>'gpu'</span>
<span>os</span>.<span>environ</span>[<span>'XLA_PYTHON_CLIENT_PREALLOCATE'</span>] = <span>'false'</span>
<span>print</span>(<span>"JAX devices:"</span>, <span>jax</span>.<span>devices</span>())</code></pre>
</div>

<p> So these are the hyperparameter we need to train approximately 2 million parameters model.</p>

<div>
    <pre><code><span># Define model hyperparameters</span>
<span>args</span> = <span>ModelArgs</span>(
    <span>vocab_size</span>=<span>enc</span>.<span>n_vocab</span>,    <span># Size of vocabulary</span>
    <span>dim</span>=<span>256</span>,                <span># Embedding dimension</span>
    <span>n_layers</span>=<span>6</span>,            <span># Number of transformer layers</span>
    <span>n_heads</span>=<span>8</span>,             <span># Number of attention heads</span>
    <span>n_kv_heads</span>=<span>4</span>,          <span># Number of key-value heads for GQA</span>
    <span>max_seq_len</span>=<span>512</span>,       <span># Maximum sequence length</span>
    <span>norm_eps</span>=<span>1e-5</span>          <span># Normalization epsilon</span>
)</code></pre></div>

    
<div>
    <h2 id="section-2">Model Weights Initialization</h2>


    
    
<p>
  In pure JAX, we don't use classes like in PyTorch. We use only pure fucntions why ? cause it makes our code more predictable and easier to parallelize.
   
  A pure function always returns the same output for the same input and doesn’t cause any side effects.<sup>6</sup> For example, if you call F(x), you'll always get the same y.
</p>

<p>
  Since we aren’t using a framework like PyTorch’s <strong>nn.Module</strong> to automatically track parameters, we must initialize and update our weights manually.
</p>

<p>
  Handling randomness is also different. Instead of relying on a single global seed as in NumPy or PyTorch, in <strong>jax</strong> we need to manage randomness with explicit pseudo-random number generator (PRNG) keys. Each random operation gets its own unique key, which is derived by splitting a parent key. This will help in reproducibility and parallelism.
</p>

<p>For example, below you can see we are creating a key and splitting it into sub keys and then providing that key to the function which involves the randomness.</p>

</div>


<div>
    <pre><code><span># Generate and split random keys for reproducibility</span>
key = jax.random.PRNGKey(<span>42</span>)

<span># Create a new subkey for random operations</span>
key, subkey = jax.random.split(key)

<span># Initialize random weights using the subkey</span>
weights = jax.random.normal(subkey, (<span>784</span>, <span>512</span>))
</code></pre>
</div>           




<p>
        Now lets start with our Model Weights Initialization, first we create the random values for our parameters with normal ditribuition.
        </p>


<div>
    <pre><code><span># Initialize weights with optional scaling</span>
<span>def</span> <span>init_weight</span>(key, shape, scale=<span>None</span>):
    <span># Calculate default scale if none provided</span>
    scale = <span>1.0</span> / math.sqrt(shape[0]) <span>if</span> scale <span>is</span> <span>None</span> <span>else</span> scale
    <span># Return scaled normal distribution</span>
    <span>return</span> jax.random.normal(key, shape) * scale
</code></pre></div>



<div>
    <p>
        Next, we'll identify all the learnable parameters of our model(llama3), assign each a unique key to ensure reproducibility, and apply the initialization process to them.
   </p>
   <p>
        Since weights are essentially numbers stored in arrays, we can use dictionaries to manage them as key-value pairs. </p>
    <p>  First we will start with attention module which has four trainable parameters.
   </p> 
</div>

<div>
    <pre><code><span># Initialize attention weights for multi-head attention</span>
<span>def</span> <span>init_attention_weights</span>(<span>key</span>, <span>dim</span>, <span>n_heads</span>, <span>n_kv_heads</span>):
    <span># Split key for each weight matrix</span>
    <span>keys</span> = jax.random.split(<span>key</span>, <span>4</span>)
    <span>head_dim</span> = <span>dim</span> // <span>n_heads</span>
    <span># Return dictionary of weight matrices</span>
    <span>return</span> {
    <span>'wq'</span>: init_weight(<span>keys</span>[<span>0</span>], (<span>dim</span>, <span>n_heads</span> <span>head_dim</span>)),  <span># Query weights</span>
    <span>'wk'</span>: init_weight(<span>keys</span>[<span>1</span>], (<span>dim</span>, <span>n_kv_heads</span> <span>head_dim</span>)),  <span># Key weights</span>
    <span>'wv'</span>: init_weight(<span>keys</span>[<span>2</span>], (<span>dim</span>, <span>n_kv_heads</span> <span>head_dim</span>)),  <span># Value weights</span>
    <span>'wo'</span>: init_weight(<span>keys</span>[<span>3</span>], (<span>n_heads</span> <span>head_dim</span>, <span>dim</span>))    <span># Output projection</span>
    }</code></pre>
    </div>

<p>Next we have our Feed-forward network which has 3 trainable parameters.</p>
<div>
<pre><code><span># Initialize feed-forward network weights</span>
<span>def</span> <span>init_ffn_weights</span>(key, dim):
    <span># Split key into three for each weight matrix</span>
    keys = jax.random.split(key, <span>3</span>)
    <span>return</span> {
        <span>'w1'</span>: <span>init_weight</span>(keys[<span>0</span>], (dim, <span>4</span> * dim)),  <span># First projection</span>
        <span>'w2'</span>: <span>init_weight</span>(keys[<span>1</span>], (<span>4</span> * dim, dim)),  <span># Output projection</span>
        <span>'w3'</span>: <span>init_weight</span>(keys[<span>2</span>], (dim, <span>4</span> * dim))   <span># Gate projection</span>
    }
</code></pre>
</div>
<p>Then we combine our weights into transformer block, adding two additional parameters for two layers of RMSNorm.</p>

<div>
    <pre><code><span># Initialize a complete transformer block</span>
<span>def</span> <span>init_transformer_block</span>(<span>key</span>, <span>dim</span>, <span>n_heads</span>, <span>n_kv_heads</span>):
    <span># Split key for each component</span>
    <span>keys</span> = jax.random.split(<span>key</span>, <span>4</span>)
    <span>return</span> {
    <span>'attention'</span>: init_attention_weights(<span>keys</span>[<span>0</span>], <span>dim</span>, <span>n_heads</span>, <span>n_kv_heads</span>),  <span># Self-attention</span>
    <span>'ffn'</span>: init_ffn_weights(<span>keys</span>[<span>1</span>], <span>dim</span>),  <span># Feed-forward network</span>
    <span>'attention_norm'</span>: init_weight(<span>keys</span>[<span>2</span>], (<span>dim</span>,), scale=<span>1.0</span>),  <span># Pre-attention norm</span>
    <span>'ffn_norm'</span>: init_weight(<span>keys</span>[<span>3</span>], (<span>dim</span>,), scale=<span>1.0</span>)  <span># Pre-ffn norm</span>
    }</code></pre>
    </div>

    <p>Finally we assemble <strong>Model's Weights Initialization</strong> in one place.</p>

    
    <div>
        <pre><code><span># Initialize complete model parameters</span>
<span>def</span> <span>init_model_params</span>(<span>key</span>, <span>vocab_size</span>, <span>dim</span>, <span>n_layers</span>, <span>n_heads</span>, <span>n_kv_heads</span>):
    <span># Split keys for different components</span>
    <span>keys</span> = jax.random.split(<span>key</span>, <span>4</span>)
    <span>params</span> = {
        <span>'token_embedding'</span>: init_weight(<span>keys</span>[<span>0</span>], (<span>vocab_size</span>, <span>dim</span>)),  <span># Token embeddings</span>
        <span>'norm_f'</span>: init_weight(<span>keys</span>[<span>1</span>], (<span>dim</span>,), scale=<span>1.0</span>),  <span># Final normalization</span>
        <span>'output'</span>: init_weight(<span>keys</span>[<span>2</span>], (<span>dim</span>, <span>vocab_size</span>))  <span># Output projection</span>
    }
    <span># Initialize transformer blocks</span>
    <span>block_keys</span> = jax.random.split(<span>keys</span>[<span>3</span>], <span>n_layers</span>)
    <span>params</span>[<span>'blocks'</span>] = [
        init_transformer_block(<span>k</span>, <span>dim</span>, <span>n_heads</span>, <span>n_kv_heads</span>)
        <span>for</span> <span>k</span> <span>in</span> <span>block_keys</span>
    ]
    <span>return</span> <span>params</span></code></pre>
    </div>


<div>
    <h2 id="section-3">Tokenization</h2>

    <p>
     Tokenization means dividing the text into words and subwords (tokens). 
        
      We will be using <bold>Byte Pair Encoding (BPE)</bold> for training our model (BPE was used in training Llama 3).<sup>7</sup>
    I will not build bpe from scratch we will use <bold>tiktoken</bold> library by openai for bpe.</p>
</div>





<div><pre><code><span>import</span> jax.numpy <span>as</span> jnp
<span>import</span><span></span> tiktoken

<span># Load GPT-2 BPE encoding</span>
enc = tiktoken.get_encoding(<span>"gpt2"</span>)


<span># reading a line from</span> 
<span>with</span> <span>open</span>(<span>'../shakespeare.txt'</span>, <span>'r'</span>) <span>as</span> f:
    text = f.readlines()[<span>0</span>]  <span># Take the first line</span>

<span># Encode the text into token IDs</span>
tokens = enc.encode(text)
data = jnp.array(tokens, dtype=jnp.int32)  <span># Store as JAX array</span>

<span># Decode back to text</span>
decoded_text = enc.decode(tokens)

<span>print</span>(<span>"original Text:"</span>, text.strip())
<span>print</span>(<span>"encoded Tokens:"</span>, tokens)
<span>print</span>(<span>"decoded Text:"</span>, decoded_text)

<span>## Ouput ##

# Original Text: From fairest creatures we desire increase,
# Encoded Tokens: [220, 3574, 37063, 301, 8109, 356, 6227, 2620, 11, 198]
# Decoded Text:   From fairest creatures we desire increase,</span></code></pre>
</div> 


<div>
    <h2 id="section-4">Embeddings</h2>

    <p>
        We cannot provide tokens directly to a model because tokens are discrete, while neural networks operate on continuous numerical data this is important for performing mathematical operations. Therefore, we use an embedding layer to convert the discrete tokens into a continuous vector space. These embeddings also help encode the semantic and syntactic relationships between tokens.
   </p>
        
   <p><img src="https://saurabhalone.com/blogs/llama3/images/lemb.png" alt="Llama architecture">
            <img src="https://saurabhalone.com/blogs/llama3/images/demb.png" alt="Llama architecture">
   </p> 

   <p> 
       There are two types of embeddings: static and dynamic. We use dynamic embeddings to train LLMs. Why? Because static embeddings work well for finding similarities between words and representing them in a similar vector space, as seen in the first image. 
   </p>
   <p>However, they suffer from semantic ambiguity, as shown in the second image.  
       This is where <b>Self-Attention</b> comes in, it refines these embeddings to incorporate context. So, we start with random embeddings and update them according to the context.  
   </p>
</div>


<div><pre><code><span># Converting the input tokens into embeddings</span>

h = params[<span>"token_embedding"</span>][inputs]

<span># token_embedding is a matrix of shape (vocab_size, dim).</span>
<span># inputs are token IDs (integers).</span>
<span># token_embedding is a matrix of shape (vocab_size, dim).</span></code></pre>
</div>


<div>
    <h2 id="section-5">Root Mean Square Layer Normalization</h2>
    <p>
        RMS normalization is an important layer in llama3 models. It helps keep the training stable by making sure that the numbers in the network don’t become too high or too low. This balance is very important, especially in deep networks.
      </p>
        <p><img src="https://saurabhalone.com/blogs/llama3/images/rsmnorm.png" alt="Llama architecture">
     <img src="https://saurabhalone.com/blogs/llama3/images/rsmnorm.png" alt="Llama architecture">
</p>
</div> 


<div><pre><code><span># RMS Norm function for stabilizing training</span>
<span>def</span> <span>rms_norm</span>(x, weight, eps=<span>1e-5</span>):
    <span># Calculate variance across last dimension</span>
    variance = jnp.mean(jnp.square(x), axis=-<span>1</span>, keepdims=<span>True</span>)                    
    <span># Normalize and scale</span>
    <span>return</span> x * weight * jnp.reciprocal(jnp.sqrt(variance + eps))
</code></pre></div>
    

<div>
    <h2 id="section-6">Rotary Positional Encoding</h2>
    <p>
  Transformers don't naturally know the order of tokens, so we need to add some position info. In llama3 to solve this we have ROPE. It does this by “rotating” the query and key vectors based on their position.<sup>8</sup>
</p>

<p><img src="https://saurabhalone.com/blogs/llama3/images/rope.png" alt="Llama architecture">
    <img src="https://saurabhalone.com/blogs/llama3/images/rope2.png" alt="Llama architecture">
</p>

<p><strong>How It Works: </strong></p>


<p>

    <strong>Precompute Rotation Factors:</strong>
    First we create a table of rotation factors using a range of frequencies. This means each token gets its own unique rotation angle.
  </p>
</div>


   

<div>
    <pre><code><span># Compute rotary position embeddings</span>
<span>def</span> <span>precompute_freqs_cis</span>(<span>dim</span>: <span>int</span>, <span>end</span>: <span>int</span>, <span>theta</span>: <span>float</span> = <span>10000.0</span>):
    <span># Generate frequency bands</span>
    <span>freqs</span> = <span>1.0</span> / (<span>theta</span> ** (jnp.arange(<span>0</span>, <span>dim</span> // <span>2</span>, dtype=jnp.float32) / <span>dim</span>))
    <span># Generate position indices</span>
    <span>t</span> = jnp.arange(<span>end</span>, dtype=jnp.float32)
    <span># Compute outer product</span>
    <span>freqs</span> = jnp.outer(<span>t</span>, <span>freqs</span>)
    <span># Convert to complex exponential</span>
    <span>return</span> jnp.complex64(jnp.exp(<span>1j</span> * <span>freqs</span>))</code></pre>
</div>


<div>
<p><strong>Apply the Rotation:</strong></p>
<p>
    <strong>Pair Up Features:</strong>  
     we reshape the vectors so that every two numbers form a pair; imagine them as the real and imaginary parts of a complex number.
  </p>
  <p>
    <strong>Rotate:</strong>  
    We multiply these complex numbers by our precomputed rotation factors. This rotates each pair in the complex plane.
  </p>
  <p>
    <strong>Convert Back:</strong>  
    Finally, we split the rotated complex numbers back into their real and imaginary parts to restore the original shape.
  </p>
<p>
    <strong>Math Behind It:</strong>
    For each pair \((x_{2i}, x_{2i+1})\), the rotation is given by:
    <br>
    \[
    \begin{pmatrix} x'_{2i} \\ x'_{2i+1} \end{pmatrix} =
    \begin{pmatrix} \cos(\theta_i) &amp; -\sin(\theta_i) \\ \sin(\theta_i) &amp; \cos(\theta_i) \end{pmatrix}
    \begin{pmatrix} x_{2i} \\ x_{2i+1} \end{pmatrix}
    \]
    where \(\theta_i\) is the rotation angle for that token.
    In short, ROPE embeds positional information directly into the token features by rotating them. This way attention module gets the info about token order without extra position vectors.
  </p>
  
  <!-- Optional: Include MathJax for rendering math formulas -->
  
</div>  
                            
<div>
    <pre><code><span># Apply rotary embeddings to queries and keys</span>
<span>def</span> <span>apply_rotary_emb</span>(<span>xq</span>, <span>xk</span>, <span>freqs_cis</span>):
    <span># Reshape inputs for complex multiplication</span>
    xq_r, xk_r = jnp.reshape(<span>xq</span>, (*<span>xq</span>.shape[:-1], -<span>1</span>, <span>2</span>)),    
    jnp.reshape(<span>xk</span>, (*<span>xk</span>.shape[:-1], -<span>1</span>, <span>2</span>))
    
    <span># Convert to complex numbers</span>
    xq_complex = jnp.complex64(xq_r[..., <span>0</span>] + <span>1j</span> * xq_r[..., <span>1</span>])
    xk_complex = jnp.complex64(xk_r[..., <span>0</span>] + <span>1j</span> * xk_r[..., <span>1</span>])
    
    <span># Reshape frequencies for broadcasting</span>
    freqs_cis = jnp.reshape(<span>freqs_cis</span>, (<span>1</span>, <span>freqs_cis</span>.shape[<span>0</span>], <span>1</span>, <span>freqs_cis</span>.shape[<span>1</span>]))
    
    <span># Apply rotation through complex multiplication</span>
    xq_out = xq_complex * <span>freqs_cis</span>
    xk_out = xk_complex * <span>freqs_cis</span>
    
    <span># Convert back to real numbers and reshape</span>
    <span>xq</span> = jnp.stack([jnp.real(xq_out), jnp.imag(xq_out)], axis=-<span>1</span>).reshape(<span>xq</span>.shape)
    <span>xk</span> = jnp.stack([jnp.real(xk_out), jnp.imag(xk_out)], axis=-<span>1</span>).reshape(<span>xk</span>.shape)
    
    <span>return</span> <span>xq</span>, <span>xk</span>
</code></pre>
</div>

<div>
    <h2 id="section-7">Group-Query Attention</h2>

    <p>
      Now it's time for attention. Grouped Query Attention (GQA) is an optimized version of Multi-Head Attention that improves efficiency by sharing key and value representations among multiple query heads. This reduces computational overhead and memory usage, enabling faster inference and better scaling for transformer models.
    At it's core, it's just self-attention but with some modification.
    </p>

    
    
    <p><strong>Scaled Dot-Product Attention:</strong></p>
    <p>
    \[
    A = \text{softmax} \left( \frac{Q K^T}{\sqrt{d_h}} \right) V
    \]
    </p>





<div><pre><code><span># Attention mechanism with grouped-query attention</span>
<span>def</span> <span>attention</span>(<span>params, x, mask, freqs_cis, n_heads, n_kv_heads, cache=None, position=0</span>):
    <span># Get input dimensions</span>
    <span>B</span>, <span>T</span>, <span>C</span> = <span>x</span>.<span>shape</span>
    <span>head_dim</span> = <span>C</span> // <span>n_heads</span>
    
    <span># Project inputs to queries, keys, and values</span>
    <span>q</span> = <span>jnp</span>.<span>dot</span>(<span>x</span>, <span>params</span>[<span>'wq'</span>]).<span>reshape</span>(<span>B</span>, <span>T</span>, <span>n_heads</span>, <span>head_dim</span>)
    <span>k</span> = <span>jnp</span>.<span>dot</span>(<span>x</span>, <span>params</span>[<span>'wk'</span>]).<span>reshape</span>(<span>B</span>, <span>T</span>, <span>n_kv_heads</span>, <span>head_dim</span>)
    <span>v</span> = <span>jnp</span>.<span>dot</span>(<span>x</span>, <span>params</span>[<span>'wv'</span>]).<span>reshape</span>(<span>B</span>, <span>T</span>, <span>n_kv_heads</span>, <span>head_dim</span>)
    
    <span># Apply rotary embeddings</span>
    <span>q</span>, <span>k</span> = <span>apply_rotary_emb</span>(<span>q</span>, <span>k</span>, <span>freqs_cis</span>[<span>position</span>:<span>position</span> + <span>T</span>])
    
    <span># Handle cache for inference</span>
    <span>if</span> <span>cache</span> <span>is not None</span>:
        <span>k</span> = <span>jnp</span>.<span>concatenate</span>([<span>cache</span>[0], <span>k</span>], <span>axis</span>=-<span>1</span>])
        <span>v</span> = <span>jnp</span>.<span>concatenate</span>([<span>cache</span>[1], <span>v</span>], <span>axis</span>=-<span>1</span>])
    <span>new_cache</span> = (<span>k</span>, <span>v</span>)
    
    <span># Repeat k/v heads for grouped-query attention</span>
    <span>k</span> = <span>repeat_kv</span>(<span>k</span>, <span>n_heads</span> // <span>n_kv_heads</span>)
    <span>v</span> = <span>repeat_kv</span>(<span>v</span>, <span>n_heads</span> // <span>n_kv_heads</span>)
    
    <span># Compute attention scores and apply attention</span>
    <span>q</span>, <span>k</span>, <span>v</span> = <span>map</span>(<span>lambda</span> <span>x</span>: <span>x</span>.<span>transpose</span>(0, 2, 1, 3), (<span>q</span>, <span>k</span>, <span>v</span>))
    <span>scores</span> = <span>jnp</span>.<span>matmul</span>(<span>q</span>, <span>k</span>.<span>transpose</span>(0, 1, 3, 2)) / <span>math</span>.<span>sqrt</span>(<span>head_dim</span>)
    
    <span># Apply attention mask if provided</span>
    <span>if</span> <span>mask</span> <span>is not None</span>:
        <span>scores</span> = <span>scores</span> + <span>mask</span>[:, :, :<span>T</span>, :<span>T</span>]
    
    <span># Compute attention weights and final output</span>
    <span>scores</span> = <span>jax</span>.<span>nn</span>.<span>softmax</span>(<span>scores</span>, <span>axis</span>=-1)
    <span>output</span> = <span>jnp</span>.<span>matmul</span>(<span>scores</span>, <span>v</span>)
    <span>output</span> = <span>output</span>.<span>transpose</span>(0, 2, 1, 3).<span>reshape</span>(<span>B</span>, <span>T</span>, -1)
    
    <span>return</span> <span>jnp</span>.<span>dot</span>(<span>output</span>, <span>params</span>[<span>'wo'</span>]), <span>new_cache</span>
</code></pre></div>

<p><strong>KV-cache : </strong>It stores previously computed key (K) and value (V) tensors from past tokens. We can cache this kv-cache during inference.</p>


<p><img src="https://saurabhalone.com/blogs/llama3/images/lightkv.png" alt="Llama architecture">
    <img src="https://saurabhalone.com/blogs/llama3/images/darkkv.png" alt="Llama architecture">
</p>

<div><pre><code></code><span>if</span> <span>cache</span> <span>is not None</span>:
    <span>k</span> = <span>jnp</span>.<span>concatenate</span>([<span>cache</span>[0], <span>k</span>], <span>axis</span>=-<span>1</span>)  
    <span>v</span> = <span>jnp</span>.<span>concatenate</span>([<span>cache</span>[1], <span>v</span>], <span>axis</span>=-<span>1</span>)  
<span>new_cache</span> = (<span>k</span>, <span>v</span>)  
</pre></div>

</div>



<div>
    <h2 id="section-8">Feed-forward</h2>

    <p>This is simple feed-forward network with <strong>SiLU</strong> activation function. </p>
    

<div>
    <pre><code><span>def</span> <span>feed_forward</span>(<span>params</span>, <span>x</span>):
    
    <span>w3_</span> = <span>jnp</span>.<span>dot</span>(<span>x</span>, <span>params</span>[<span>'w3'</span>])

    <span># SwiGLU(a,b)=SiLU(a)⊙b 
    <span>activated</span> = <span>jax</span>.<span>nn</span>.<span>silu</span>(<span>w3_</span>)
    
    
    <span>w1_</span> = <span>jnp</span>.<span>dot</span>(<span>x</span>, <span>params</span>[<span>'w1'</span>])
    
    
    <span>combined</span> = <span>activated</span> * <span>w1_</span>
    
    <span># Final output projection with w2</span>
    <span>output</span> = <span>jnp</span>.<span>dot</span>(<span>combined</span>, <span>params</span>[<span>'w2'</span>])
    
    <span>return</span> <span>output</span>
</span></code></pre>
</div>




</div>

<div>
    <h2 id="section-9">Transformer-block</h2>

    <p>
        This is where all the important components come together in the transformer block. We unpack the pre-initialized weights and distribute them to their respective layers. The transformer blocks include attention, normalization, feed-forward processing layers and residual connections.
      </p>
    
</div>

<div>
    <pre><code><span># Transformer block implementation</span>
<span>def</span> transformer_block(<span>params</span>, <span>x</span>, <span>mask</span>, <span>freqs_cis</span>, <span>n_heads</span>, <span>n_kv_heads</span>, <span>cache</span>=<span>None</span>, <span>position</span>=<span>0</span>):
    <span># Apply attention with normalization and residual connection</span>
    <span>attn_output</span>, <span>new_cache</span> = attention(
        <span>params</span>[<span>'attention'</span>],
        rms_norm(<span>x</span>, <span>params</span>[<span>'attention_norm'</span>]),
        <span>mask</span>,
        <span>freqs_cis</span>,
        <span>n_heads</span>,
        <span>n_kv_heads</span>,
        <span>cache</span>,
        <span>position</span>
    )
    
    <span># First residual connection</span>
    <span>h</span> = <span>x</span> + <span>attn_output</span>
    
    <span># Apply feed-forward network with normalization and residual</span>
    <span>ffn_output</span> = feed_forward(<span>params</span>[<span>'ffn'</span>], rms_norm(<span>h</span>, <span>params</span>[<span>'ffn_norm'</span>]))
    
    <span># Second residual connection</span>
    <span>out</span> = <span>h</span> + <span>ffn_output</span>
    
    <span>return</span> <span>out</span>, <span>new_cache</span></code></pre>
</div>

<div>
    <h2 id="section-10">Forward-Pass</h2>

    <p> The forward pass takes your data through the entire model from converting input tokens into embeddings, through a series of transformer blocks, and finally to the output layer. In other words, it connects all the layers (embedding, transformers, and output) to produce the final predictions.</p></div>




<div>
    <pre><code><span># Forward pass through the entire model</span>
<span>def</span> <span>model_forward</span>(<span>params</span>, <span>inputs</span>, <span>config</span>, <span>cache</span>=<span>None</span>, <span>position</span>=<span>0</span>):
    <span># Get batch dimensions</span>
    <span>B</span>, <span>T</span> = <span>inputs</span>.shape
    
    <span># Convert input tokens to embeddings</span>
    <span>h</span> = <span>params</span>[<span>'token_embedding'</span>][<span>inputs</span>]
    
    <span># Compute freqs_cis for this forward pass</span>
    <span>freqs_cis</span> = <span>precompute_freqs_cis</span>(<span>config</span>.<span>dim</span> // <span>config</span>.<span>n_heads</span>, <span>config</span>.<span>max_seq_len</span>)
    
    <span># Create causal mask</span>
    <span>mask</span> = <span>jnp</span>.<span>tril</span>(<span>jnp</span>.<span>ones</span>((<span>config</span>.<span>max_seq_len</span>, <span>config</span>.<span>max_seq_len</span>)))
    <span>mask</span> = <span>jnp</span>.<span>where</span>(<span>mask</span> == <span>0</span>, -<span>1e9</span>, <span>0.0</span>)
    <span>mask</span> = <span>mask</span>.<span>astype</span>(<span>h</span>.<span>dtype</span>)
    <span>mask</span> = <span>mask</span>[<span>None</span>, <span>None</span>, :, :]

    <span># Process through transformer blocks</span>
    <span>new_caches</span> = []
    <span>for</span> <span>i</span>, <span>block</span> <span>in</span> <span>enumerate</span>(<span>params</span>[<span>'blocks'</span>]):
        <span>layer_cache</span> = <span>cache</span>[<span>i</span>] <span>if</span> <span>cache</span> <span>is not</span> <span>None</span> <span>else</span> <span>None</span>
        <span>h</span>, <span>layer_cache</span> = <span>transformer_block</span>(
            <span>block</span>, <span>h</span>, <span>mask</span>, <span>freqs_cis</span>,
            <span>config</span>.<span>n_heads</span>, <span>config</span>.<span>n_kv_heads</span>,
            <span>layer_cache</span>, <span>position</span>, training=<span>False</span>)
        <span>new_caches</span>.<span>append</span>(<span>layer_cache</span>)

    <span># Final normalization and output projection</span>
    <span>h</span> = <span>rms_norm</span>(<span>h</span>, <span>params</span>[<span>'norm_f'</span>])
    <span>logits</span> = <span>jnp</span>.<span>dot</span>(<span>h</span>, <span>params</span>[<span>'output'</span>])
    
    <span>return</span> <span>logits</span>, <span>new_caches</span></code></pre>
</div>





          <div>
              <h2 id="section-11">Dataset</h2>
          
              <p>Now the model part is complete so its time to train our model on shakespeare dataset. First we will read our data from <strong>.txt</strong> file then we will encode our data with bpe and then convert it into jax array.</p>
          </div>

<div>
<pre><code><span># Initialize tokenizer and load data</span>
<span>enc</span> = <span>tiktoken.get_encoding</span>(<span>"gpt2"</span>)

<span># Read text file</span>
<span>with</span> <span>open</span>(<span>'shakespeare.txt'</span>, <span>'r'</span>) <span>as</span> <span>f</span>:
    <span>text</span> = <span>f.read</span>()

<span># Convert text to token IDs</span>
<span>tokens</span> = <span>enc.encode</span>(<span>text</span>)
<span># Convert to JAX array</span>
<span>data</span> = <span>jnp.array</span>(<span>tokens</span>)
</code></pre> 
</div>

<div>
    <h3 id="section-1">Get Batches</h3>
    <p>The get_batch function creates training batches from our Shakespeare dataset. We need to feed our model with chunks of data. For each batch, we randomly select starting positions in the text, this way the model sees a variety of contexts. </p>
    <p>Now, here's where JAX's cool vmap feature comes into play. Instead of writing a loop to extract each chunk, we use vmap to automate.</p>
    <p><strong>How does it work ?</strong></p>
    <p> vmap is like a vectorized loop; it takes a function that processes a single index (using <strong>lax.dynamic_slice </strong> to get a sequence of tokens) and applies it to every element in our array of indices. This means our input sequences (x) and corresponding target sequences (y, which are shifted by one token for next-word prediction) are created in one go.</p>

<div>
    <pre><code><span>def</span> <span>get_batch</span>(<span>key</span>, <span>data</span>, <span>batch_size</span>, <span>seq_len</span>):
    <span># Generate random starting indices</span>
    <span>ix</span> = <span>random</span>.<span>randint</span>(<span>key</span>, (<span>batch_size</span>,), <span>0</span>, <span>len</span>(<span>data</span>) - <span>seq_len</span>)
    
    <span># Vectorized operation to get input and target sequences</span>
    <span>x</span> = <span>vmap</span>(<span>lambda</span> <span>i</span>: <span>lax</span>.<span>dynamic_slice</span>(<span>data</span>, (<span>i</span>,), (<span>seq_len</span>,)))(<span>ix</span>)
    <span>y</span> = <span>vmap</span>(<span>lambda</span> <span>i</span>: <span>lax</span>.<span>dynamic_slice</span>(<span>data</span>, (<span>i</span> + <span>1</span>,), (<span>seq_len</span>,)))(<span>ix</span>)
    
    <span>return</span> <span>x</span>, <span>y</span>
</code></pre>
</div>
    

   



</div>



            <div>
                                    <h2 id="section-13">Loss Function</h2>
                                    <p>This function computes the cross-entropy loss for a batch during training. It first performs a forward pass using the model to generate logits for the input data. Then, it reshapes both the logits and targets to merge the batch and sequence dimensions. After applying the log softmax to the logits, it extracts the log probabilities corresponding to the correct target tokens and computes their negative mean as the final loss value.</p>
            
                                    
            
            <p>The cross-entropy loss is defined as:</p>
            <p>
             \[
            \mathcal{L} = -\frac{1}{N} \sum_{i=1}^{N} \log P(y_i)
            \]
            </p>
            
            <p>Where:</p>
            <div>
               
            <ul>
              <li>\( P(y_i) \) is the probability of the correct class, calculated using the softmax function:</li>
            </ul>
            </div>
            
            <p>
                \[
            P(y_i) = \frac{e^{z_i}}{\sum_{j} e^{z_j}}
            \]
            </p>


<div>
    <pre><code><span># Compute cross-entropy loss</span>
<span>def</span> <span>compute_loss</span>(<span>params</span>, <span>batch</span>):
    <span># Split batch into inputs and targets</span>
    <span>inputs</span>, <span>targets</span> = <span>batch</span>
    <span># Forward pass to get logits</span>
    <span>logits</span>, = model_forward(<span>params</span>, <span>inputs</span>, <span>config</span>)
    <span># Reshape for loss computation</span>
    <span>logits</span> = <span>logits</span>.reshape(-<span>1</span>, <span>config</span>.vocab_size)
    <span>targets</span> = <span>targets</span>.reshape(-<span>1</span>)
    <span># Calculate negative log likelihood</span>
    <span>loss</span> = -jnp.mean(jnp.take_along_axis(jax.nn.log_softmax(<span>logits</span>),
    <span>targets</span>[:, <span>None</span>], axis=<span>1</span>))
    <span>return</span> <span>loss</span></code></pre>
    </div>

       
       <div>
       <h2 id="section-14">Update function</h2>
       
       <p>Now we need to write a function to update our weights. For simplicity, we're using Stochastic Gradient Descent (SGD) here, though you can also use Adam or AdamW for faster convergence.
       </p>
       
       <p>In the code, you'll notice the <strong>@jax.jit</strong> decorator. This is one of the features that sets <strong>jax</strong> apart. JIT (Just-In-Time) compilation speeds up execution by converting your Python code into optimized machine code.</p>
        
       <p><strong>How does it work ?</strong></p>

       <p>When you decorate a function with JAX’s jit, it changes how the function executes. Normally, when you call a function, Python runs it line by line. For example, if you have:
       </p>
<div> <pre><code><span>def</span> <span>sqr</span>(<span>x</span>): 
    <span>print</span>(<span>"HI jiited"</span>)<span> # side effect</span> 
    <span>return</span> <span>x</span> * <span>x</span>

<span>print</span>(<span>sqr</span>(<span>2</span>)) 
<span>print</span>(<span>sqr</span>(<span>3</span>)) 
<span>print</span>(<span>sqr</span>(<span>4</span>))</code></pre>
</div>


    <p>Every time you call sqr, it prints "HI jiited" and then returns the square of the number. However, when you add the @jax.jit decorator:</p>


<div> <pre><code><span>@</span><span>jax</span>.<span>jit</span>
<span>def</span> <span>sqr</span>(<span>x</span>): 
    <span>print</span>(<span>"HI jiited"</span>)<span> # side effect</span>  
    <span>return</span> <span>x</span> * <span>x</span>

<span>print</span>(<span>sqr</span>(<span>2</span>)) 
<span>print</span>(<span>sqr</span>(<span>3</span>)) 
<span>print</span>(<span>sqr</span>(<span>4</span>))</code></pre>
</div>

<p><strong>Jax </strong>first traces your function to build an optimized computation graph. This tracing happens the first time the function is called and converts the Python code into machine code.</p>

<p>Because of this tracing, any side effects like the print statement; are only executed during the initial tracing. Once the function is compiled, other remaining    calls use the compiled version, and you might not see the print output every time.</p>








<div>
<pre><code><span>@</span><span>jax</span>.<span>jit</span>
<span>def</span> <span>update_step</span>(<span>params</span>, <span>batch</span>):
    <span># Compute both loss and gradients in a single pass using value_and_grad</span>
    <span># This is more efficient than computing them separately</span>
    <span>loss</span>, <span>grads</span> = <span>jax.value_and_grad</span>(<span>compute_loss</span>)(<span>params</span>, <span>batch</span>)

    <span># Update parameters using gradient descent</span>
    <span># jax.tree.map applies the update rule to each parameter in the model</span>
    <span># The lambda function implements: p_new = p_old - learning_rate * gradient</span>
    <span>params</span> = <span>jax.tree.map</span>(
        <span>lambda</span> <span>p</span>, <span>g</span>: <span>p</span> - <span>config.learning_rate</span> * <span>g</span>,
        <span>params</span>,
        <span>grads</span>
    )

    <span># Return updated parameters and the loss value for monitoring training</span>
    <span>return</span> <span>params</span>, <span>loss</span></code></pre>
</div>
<p>In our <strong>update_step</strong> function, <strong>@jax.jit</strong> compiles the code. The function computes loss and gradients simultaneously with <strong>jax.value_and_grad</strong>, updates the parameters using gradient descent with help of <strong>jax.tree.map</strong>, and returns the updated parameters and loss.</p>

</div>


             
              


              <div>
                 <h2 id="section-15">Trainig-Loop</h2>
                 <p>Finally, its time to train our 2 million parameter model on shakespeare dataset. 
                    We first prepare batches using the <strong>get_batch</strong> which splits our data into batches so we can train faster with 
                     our limited compute.
                 </p>
             </div>

<div>
 <pre><code><span>for</span> <span>epoch</span> <span>in</span> <span>range</span>(<span>num_epochs</span>):
 
   
   <span>epoch_loss</span> = <span>0.0</span>

   <span>for</span> <span>step</span> <span>in</span> <span>range</span>(<span>steps_per_epoch</span>):
   
      <span># Generate new random keys for reproducibility</span>
      <span>key</span>, <span>batch_key</span> = <span>random.split</span>(<span>key</span>)
      
      <span># Sample random batch of sequences</span>
      <span>batch</span> = <span>get_batch</span>(<span>batch_key</span>, <span>data</span>, <span>config.batch_size</span>, <span>config.max_seq_len</span>)
      
      <span># Forward pass, compute loss and update parameters</span>
      <span>params_state</span>, <span>loss</span> = <span>update_step</span>(<span>params_state</span>, <span>batch</span>)
     
      <span># loss for epoch average</span>
      <span>epoch_loss</span> += <span>loss</span>
      
   
      <span>if</span> <span>step</span> % <span>100</span> == <span>0</span>:
            <span>print</span>(<span>f"epoch {epoch + 1}, step {step}/{steps_per_epoch}: loss = {loss:.4f}"</span>)
      

  <span>avg_epoch_loss</span> = <span>epoch_loss</span> / <span>steps_per_epoch</span>
     
 
  <span>epoch_losses</span>.<span>append</span>(<span>avg_epoch_loss</span>)
      
  
  <span>print</span>(<span>f"\nepoch {epoch + 1} | average loss: {avg_epoch_loss:.4f}"</span>)</code></pre>


</div>


<p><img src="https://saurabhalone.com/blogs/llama3/images/train.png" alt="Llama architecture">
    <img src="https://saurabhalone.com/blogs/llama3/images/train.png" alt="Llama architecture">
</p>
  




               




<hr>
<div>
    <h3>Thank you for reading this far !! </h3>
    <h3>You can support me :</h3>
    <div>
        <ul>
            <li><a href="https://x.com/saurabhalonee">Twitter</a></li>
            <li><a href="https://buymeacoffee.com/saurabhaloq">Buy-me-Coffe</a></li>
            <li><a href="https://github.com/saurabhaloneai">Github</a></li>
        </ul>
    </div>
</div>












    



</div></article>

</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[USDA fired officials working on bird flu, now trying to rehire them (163 pts)]]></title>
            <link>https://www.nbcnews.com/politics/doge/usda-accidentally-fired-officials-bird-flu-rehire-rcna192716</link>
            <guid>43097709</guid>
            <pubDate>Wed, 19 Feb 2025 02:04:35 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.nbcnews.com/politics/doge/usda-accidentally-fired-officials-bird-flu-rehire-rcna192716">https://www.nbcnews.com/politics/doge/usda-accidentally-fired-officials-bird-flu-rehire-rcna192716</a>, See on <a href="https://news.ycombinator.com/item?id=43097709">Hacker News</a></p>
Couldn't get https://www.nbcnews.com/politics/doge/usda-accidentally-fired-officials-bird-flu-rehire-rcna192716: Error: timeout of 10000ms exceeded]]></description>
        </item>
        <item>
            <title><![CDATA[Parsing JSON in 500 lines of Rust (106 pts)]]></title>
            <link>https://www.krish.gg/blog/json-parser-in-rust</link>
            <guid>43096975</guid>
            <pubDate>Wed, 19 Feb 2025 00:25:10 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.krish.gg/blog/json-parser-in-rust">https://www.krish.gg/blog/json-parser-in-rust</a>, See on <a href="https://news.ycombinator.com/item?id=43096975">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>Last semester at university, I took a course called "Syntax-Based Tools and Compilers". It focused on building a scanner, parser, compiler, and so on for a language called <a href="https://en.wikipedia.org/wiki/PL/0">PL0</a>. We used Python in the course, but I was really interested in learning Rust at the time.</p>
<p>So, I decided to embark on a side project (yes, another one!). This time, I wanted to build a JSON parser in Rust. My goal was to test the skills I gained in the course and finally dive into a Rust project, something I'd been putting off for three years.</p>
<h2 id="the-plan">The Plan</h2>
<p>I find that there is no better way to learn programming than to just start building. So that was my plan. I found the <a href="https://www.json.org/json-en.html">JSON specification</a> and started reading. This spec has some really nice diagrams that help visualize the structure of a JSON document.</p>
<p>There are many ways to create a "parser". I could validate, scan, tokenize, and then finally parse the JSON. But I wanted to keep it simple, so I ingored everything and just focused on "parsing" the JSON from a raw text file/string into a Rust enum that represents the JSON structure.</p>
<p>There are tools that can take grammars and autogenerate top-down or bottom-up parsers, but my implementation is something that is considered a <strong>hand-written parser</strong>. It's a flexible method that is not bound to very strict rules or implemenation details, allowing me to make changes easily.</p>
<h2 id="the-implementation">The Implementation</h2>
<h3 id="how-do-we-represent-json-in-rust">How do we represent JSON in Rust?</h3>
<p>To store this parsed JSON, I need some way to represent the data in Rust.</p>
<p>I started by creating the general enum <code>JSONValue</code> that would represenet the "tree" structure of the JSON document. Each "node" can be of many types - string, number, object, array, boolean, or null. At the root, you have one node that is the JSON object.</p>
<p>I ended up with the following enum:</p>
<pre><code><span><span>#[derive(Debug, Clone, PartialEq)]</span>
</span><span><span>enum</span> <span>JSONValue</span> <span>{</span>
</span><span>    <span>Null</span><span>,</span>
</span><span>    <span>True</span><span>,</span>
</span><span>    <span>False</span><span>,</span>
</span><span>    <span>Number</span><span>(</span><span>f64</span><span>)</span><span>,</span>
</span><span>    <span>String</span><span>(</span><span>String</span><span>)</span><span>,</span>
</span><span>    <span>Array</span><span>(</span><span>Vec</span><span>&lt;</span><span>JSONValue</span><span>&gt;</span><span>)</span><span>,</span>
</span><span>    <span>Object</span><span>(</span><span>HashMap</span><span>&lt;</span><span>String</span><span>,</span> <span>JSONValue</span><span>&gt;</span><span>)</span><span>,</span>
</span><span><span>}</span>
</span></code></pre>
<h3 id="what-about-errors">What about errors?</h3>
<p>Another thing to note is that parsing is a process that can fail - the source text may have syntax errors and the parser should be able to handle them. So, I decided to return a <code>Result</code> type from the parser. If the parsing is successful, it will return the parsed JSON value. If not, it will return an error.</p>
<pre><code><span><span>enum</span> <span>JSONParseError</span> <span>{</span>
</span><span>    <span>Error</span><span>(</span><span>usize</span><span>)</span><span>,</span>
</span><span>    <span>NotFound</span><span>,</span>
</span><span>    <span>UnexpectedChar</span><span>(</span><span>usize</span><span>)</span><span>,</span>
</span><span>    <span>MissingClosing</span><span>(</span><span>usize</span><span>)</span><span>,</span>
</span><span><span>}</span>
</span></code></pre>
<p>I used this enum to represent different types of errors that can occur during parsing. Note that some of these errors have an associated <code>usize</code> value; this value is the remaining length of the input string when the error occurred. This lets me know how much of the input string was consumed before the error happened, so I can print better error messages. The <code>NotFound</code> error is more of an internal error that I used to indicate that the parser couldn't find the expected element in the input string.</p>
<h3 id="the-json-value">The JSON "value"</h3>
<p>As per the JSON spec, everything starts as an element - which is a value surrounded by whitespace. This value can be of the following types:</p>
<ul>
<li>object</li>
<li>array</li>
<li>string</li>
<li>number</li>
<li>"true"</li>
<li>"false"</li>
<li>"null"</li>
</ul>
<h4 id="simple-values">Simple Values</h4>
<p>I wanted to start with the simplest values first, and then build up to the more complex ones. So, I started with the <code>null</code> value. A simple function for that looks like this:</p>
<pre><code><span><span>fn</span> <span>null</span><span>(</span>src<span>:</span> <span>&amp;</span><span>str</span><span>)</span> <span>-&gt;</span> <span>Result</span><span>&lt;</span><span>(</span><span>&amp;</span><span>str</span><span>,</span> <span>JSONValue</span><span>)</span><span>,</span> <span>JSONParseError</span><span>&gt;</span> <span>{</span>
</span><span>    <span>match</span> src<span>.</span><span>strip_prefix</span><span>(</span><span>"null"</span><span>)</span> <span>{</span>
</span><span>        <span>Some</span><span>(</span>rest<span>)</span> <span>=&gt;</span> <span>Ok</span><span>(</span><span>(</span>rest<span>,</span> <span>JSONValue</span><span>::</span><span>Null</span><span>)</span><span>)</span><span>,</span>
</span><span>        <span>None</span> <span>=&gt;</span> <span>Err</span><span>(</span><span>JSONParseError</span><span>::</span><span>NotFound</span><span>)</span><span>,</span>
</span><span>    <span>}</span>
</span><span><span>}</span>
</span></code></pre>
<p>In this code, we simply check if the input string starts with "null". If it does, we return the remaining string and the <code>JSONValue::Null</code>. If not, we return an error indicating that the expected value was not found.</p>
<p>I followed a similar approach for the <code>true</code> and <code>false</code> values; just replace "null" with "true" or "false".</p>
<h4 id="strings">Strings</h4>
<p>Parsing strings in a string of JSON sounds simple - just find the opening and closing quotes and return the string in between. But it's not that simple. Strings can contain escape sequences like <code>\"</code>, <code>\\</code>, <code>\n</code>, and so on. So there is careful handling required to parse strings correctly.</p>
<p>To parse a string, the code starts by looking for the opening quotation <code>"</code>. After finding it, it reads characters until it finds the closing quotation <code>"</code>. However, you can escape the closing quotation, so the parser maintains a flag to check if the last character was an escape character <code>\</code>. If it was, the parser handles the next character differently, making sure we don't stop parsing prematurely.</p>
<p>This is part of the code that parses strings:</p>
<pre><code><span><span>fn</span> <span>string</span><span>(</span><span>mut</span> src<span>:</span> <span>&amp;</span><span>str</span><span>)</span> <span>-&gt;</span> <span>Result</span><span>&lt;</span><span>(</span><span>&amp;</span><span>str</span><span>,</span> <span>JSONValue</span><span>)</span><span>,</span> <span>JSONParseError</span><span>&gt;</span> <span>{</span>
</span><span>    <span>// make sure we start with a quote</span>
</span><span>    <span>match</span> src<span>.</span><span>strip_prefix</span><span>(</span><span>"\""</span><span>)</span> <span>{</span>
</span><span>        <span>Some</span><span>(</span>rest<span>)</span> <span>=&gt;</span> src <span>=</span> rest<span>,</span>
</span><span>        <span>None</span> <span>=&gt;</span> <span>return</span> <span>Err</span><span>(</span><span>JSONParseError</span><span>::</span><span>NotFound</span><span>)</span><span>,</span>
</span><span>    <span>}</span><span>;</span>
</span><span>
</span><span>    <span>let</span> <span>mut</span> result<span>:</span> <span>String</span> <span>=</span> <span>""</span><span>.</span><span>to_string</span><span>(</span><span>)</span><span>;</span>
</span><span>    <span>let</span> <span>mut</span> escaping <span>=</span> <span>false</span><span>;</span> <span>// the flag</span>
</span><span>    <span>let</span> <span>mut</span> chars <span>=</span> src<span>.</span><span>chars</span><span>(</span><span>)</span><span>;</span> <span>// iterator</span>
</span><span>
</span><span>    <span>loop</span> <span>{</span>
</span><span>        <span>let</span> c <span>=</span> <span>match</span> chars<span>.</span><span>next</span><span>(</span><span>)</span> <span>{</span>
</span><span>            <span>Some</span><span>(</span>c<span>)</span> <span>=&gt;</span> c<span>,</span>
</span><span>            <span>None</span> <span>=&gt;</span> <span>return</span> <span>Err</span><span>(</span>
</span><span>                <span>JSONParseError</span><span>::</span><span>MissingClosing</span><span>(</span>src<span>.</span><span>len</span><span>(</span><span>)</span><span>)</span>
</span><span>            <span>)</span><span>,</span>
</span><span>        <span>}</span><span>;</span>
</span><span>
</span><span>        <span>// if we have the \, then we are escaping</span>
</span><span>        <span>if</span> c <span>==</span> <span>'\\'</span> <span>&amp;&amp;</span> <span>!</span>escaping <span>{</span>
</span><span>            escaping <span>=</span> <span>true</span><span>;</span>
</span><span>        <span>}</span>
</span><span>        <span>// non-escaping closing quote</span>
</span><span>        <span>else</span> <span>if</span> c <span>==</span> <span>'"'</span> <span>&amp;&amp;</span> <span>!</span>escaping <span>{</span>
</span><span>            <span>break</span><span>;</span>
</span><span>        <span>}</span> <span>else</span> <span>if</span> escaping <span>{</span>
</span><span>            <span>// special escape sequences</span>
</span><span>            <span>match</span> c <span>{</span>
</span><span>                <span>// quotation mark</span>
</span><span>                <span>'"'</span> <span>=&gt;</span> result<span>.</span><span>push</span><span>(</span><span>'"'</span><span>)</span><span>,</span>
</span><span>                <span>...</span> <span>// other escape sequences</span>
</span><span>                _ <span>=&gt;</span> <span>{</span>
</span><span>                    <span>// can't escape whatever this is</span>
</span><span>                    <span>return</span> <span>Err</span><span>(</span><span>JSONParseError</span><span>::</span><span>UnexpectedChar</span><span>(</span>
</span><span>                        chars<span>.</span><span>count</span><span>(</span><span>)</span>
</span><span>                    <span>)</span><span>)</span><span>;</span>
</span><span>                <span>}</span>
</span><span>            <span>}</span>
</span><span>            escaping <span>=</span> <span>false</span><span>;</span>
</span><span>        <span>}</span> <span>else</span> <span>{</span>
</span><span>            result<span>.</span><span>push</span><span>(</span>c<span>)</span><span>;</span>
</span><span>        <span>}</span>
</span><span>    <span>}</span>
</span><span>
</span><span>    <span>Ok</span><span>(</span><span>(</span>chars<span>.</span><span>as_str</span><span>(</span><span>)</span><span>,</span> <span>JSONValue</span><span>::</span><span>String</span><span>(</span>result<span>)</span><span>)</span><span>)</span>
</span><span><span>}</span>
</span></code></pre>
<h4 id="numbers">Numbers</h4>
<p>In normal programming languages, we often have multiple data types to represent numbers, such as integers of different sizes, floating-point numbers, etc. In JSON, there is only one type of number - an arbitary value that can either be an integer, floating-point number, or a number in scientific notation.</p>
<p>For my parser, each number is represented as a <code>f64</code> (floating-point number). This is a simple way to represent numbers in Rust, but it does not support the full arbitrary precision that JSON allows. This is a limitation of my parser, but it's one that I'm willing to accept for now.</p>
<!-- json_number.png -->
<p><a href="https://www.krish.gg/assets/json-parser-in-rust/json_number.png"><img src="https://www.krish.gg/assets/json-parser-in-rust/json_number.png" alt="Number"></a></p>
<p>A number in JSON is made up of many parts: the integer, the fraction, and the exponent. The parser reads these parts and constructs a <code>f64</code> from them. There are also some edge cases to consider, like leading zeros, negative numbers, and so on.</p>
<p>I won't go into the full implementation here, but I have functions to parse each of those 3 parts, and I combine them to parse the full number.</p>
<pre><code><span>
</span><span><span>fn</span> <span>number</span><span>(</span><span>mut</span> src<span>:</span> <span>&amp;</span><span>str</span><span>)</span> <span>-&gt;</span> <span>Result</span><span>&lt;</span><span>(</span><span>&amp;</span><span>str</span><span>,</span> <span>JSONValue</span><span>)</span><span>,</span> <span>JSONParseError</span><span>&gt;</span> <span>{</span>
</span><span>    <span>let</span> <span>mut</span> result<span>;</span>
</span><span>    <span>let</span> negative<span>;</span>
</span><span>
</span><span>    <span>match</span> <span>integer</span><span>(</span>src<span>)</span> <span>{</span>
</span><span>        <span>Ok</span><span>(</span><span>(</span>rest<span>,</span> num<span>)</span><span>)</span> <span>=&gt;</span> <span>{</span>
</span><span>            result <span>=</span> num<span>.</span><span>abs</span><span>(</span><span>)</span> <span>as</span> <span>f64</span><span>;</span>
</span><span>            negative <span>=</span> num<span>.</span><span>is_negative</span><span>(</span><span>)</span><span>;</span>
</span><span>            src <span>=</span> rest<span>;</span>
</span><span>        <span>}</span>
</span><span>        <span>Err</span><span>(</span>e<span>)</span> <span>=&gt;</span> <span>return</span> <span>Err</span><span>(</span>e<span>)</span><span>,</span>
</span><span>    <span>}</span><span>;</span>
</span><span>
</span><span>    <span>match</span> <span>fraction</span><span>(</span>src<span>)</span> <span>{</span>
</span><span>        <span>Ok</span><span>(</span><span>(</span>rest<span>,</span> frac<span>)</span><span>)</span> <span>=&gt;</span> <span>{</span>
</span><span>            result <span>+=</span> frac<span>;</span>
</span><span>            src <span>=</span> rest<span>;</span>
</span><span>        <span>}</span>
</span><span>        <span>Err</span><span>(</span><span>JSONParseError</span><span>::</span><span>NotFound</span><span>)</span> <span>=&gt;</span> <span>{</span><span>}</span>
</span><span>        <span>Err</span><span>(</span>e<span>)</span> <span>=&gt;</span> <span>return</span> <span>Err</span><span>(</span>e<span>)</span><span>,</span>
</span><span>    <span>}</span>
</span><span>
</span><span>    <span>match</span> <span>exponent</span><span>(</span>src<span>)</span> <span>{</span>
</span><span>        <span>Ok</span><span>(</span><span>(</span>rest<span>,</span> exponent<span>)</span><span>)</span> <span>=&gt;</span> <span>{</span>
</span><span>            src <span>=</span> rest<span>;</span>
</span><span>
</span><span>            <span>let</span> multipier <span>=</span> <span>10_f64</span><span>.</span><span>powf</span><span>(</span>exponent <span>as</span> <span>f64</span><span>)</span><span>;</span>
</span><span>            result <span>*=</span> multipier<span>;</span>
</span><span>        <span>}</span>
</span><span>        <span>Err</span><span>(</span><span>JSONParseError</span><span>::</span><span>NotFound</span><span>)</span> <span>=&gt;</span> <span>{</span><span>}</span>
</span><span>        <span>Err</span><span>(</span>e<span>)</span> <span>=&gt;</span> <span>return</span> <span>Err</span><span>(</span>e<span>)</span><span>,</span>
</span><span>    <span>}</span>
</span><span>
</span><span>    <span>if</span> negative <span>{</span>
</span><span>        result <span>*=</span> <span>-</span><span>1.0</span><span>;</span>
</span><span>    <span>}</span>
</span><span>
</span><span>    <span>Ok</span><span>(</span><span>(</span>src<span>,</span> <span>JSONValue</span><span>::</span><span>Number</span><span>(</span>result<span>)</span><span>)</span><span>)</span>
</span><span><span>}</span>
</span></code></pre>
<h4 id="lists-objects">Lists, Objects</h4>
<p>Both arrays and objects are collections of values. Arrays are ordered lists of values, while objects are unordered collections of key-value pairs. The parser needs to handle both of these types.</p>
<p>If looking at each of these syntactically, each of these is a collection with elements seperated by commas. For each of these, the parser needs to be able to handle 3 different cases:</p>
<ul>
<li>no elements</li>
<li>one element</li>
<li>multiple elements</li>
</ul>
<p>The case of no elements is simple - just find a pair of brackets with whitespace in between.</p>
<p>For the other two cases, we can enter a loop that keeps reading elements as long as the element has a comma after it. This is a simple way to parse these collections. It is still important to note that we cannot skip over elements that are not valid JSON values, so appropriate error handling is required.</p>
<p>Here is what the code for handling the last two cases looks like:</p>
<pre><code><span><span>fn</span> <span>elements</span><span>(</span><span>mut</span> src<span>:</span> <span>&amp;</span><span>str</span><span>)</span> <span>-&gt;</span> <span>Result</span><span>&lt;</span><span>(</span><span>&amp;</span><span>str</span><span>,</span> <span>Vec</span><span>&lt;</span><span>JSONValue</span><span>&gt;</span><span>)</span><span>,</span> <span>JSONParseError</span><span>&gt;</span> <span>{</span>
</span><span>    <span>let</span> <span>mut</span> values <span>=</span> <span>vec!</span><span>[</span><span>]</span><span>;</span>
</span><span>
</span><span>    <span>loop</span> <span>{</span>
</span><span>        <span>match</span> <span>element</span><span>(</span>src<span>)</span> <span>{</span>
</span><span>            <span>Ok</span><span>(</span><span>(</span>rest<span>,</span> v<span>)</span><span>)</span> <span>=&gt;</span> <span>{</span>
</span><span>                src <span>=</span> rest<span>;</span>
</span><span>                values<span>.</span><span>push</span><span>(</span>v<span>)</span><span>;</span>
</span><span>            <span>}</span>
</span><span>            <span>Err</span><span>(</span>e<span>)</span> <span>=&gt;</span> <span>return</span> <span>Err</span><span>(</span>e<span>)</span><span>,</span>
</span><span>        <span>}</span>
</span><span>
</span><span>        <span>// now we wanna consume the first character of src</span>
</span><span>        <span>// if it is a comma, or break otherwise</span>
</span><span>        <span>if</span> src<span>.</span><span>chars</span><span>(</span><span>)</span><span>.</span><span>next</span><span>(</span><span>)</span> <span>==</span> <span>Some</span><span>(</span><span>','</span><span>)</span> <span>{</span>
</span><span>            src <span>=</span> <span>&amp;</span>src<span>[</span><span>1</span><span>..</span><span>]</span><span>;</span>
</span><span>        <span>}</span> <span>else</span> <span>{</span>
</span><span>            <span>break</span><span>;</span>
</span><span>        <span>}</span>
</span><span>    <span>}</span>
</span><span>
</span><span>    <span>Ok</span><span>(</span><span>(</span>src<span>,</span> values<span>)</span><span>)</span>
</span><span><span>}</span>
</span></code></pre>
<p>Again, this isn't the full implementation, but it gives you an idea of how the parser handles these collections.</p>
<h3 id="putting-the-parser-together">Putting the parser together</h3>
<p>After building all these pieces, we now come to the root of the parser. When we see JSON used in APIs, it's often used for passing objects around. However, the root JSON object can actually be any of the types we've discussed - a string, number, object, array, boolean, or null.</p>
<p>So, the parser starts by checking which of these types the root JSON value is, and then calls the appropriate function to parse it. This happens in a specific order, as per the JSON spec.</p>
<pre><code><span><span>// the surrounding whitespace has</span>
</span><span><span>// already been stripped</span>
</span><span>
</span><span><span>fn</span> <span>value</span><span>(</span>src<span>:</span> <span>&amp;</span><span>str</span><span>)</span> <span>-&gt;</span> <span>Result</span><span>&lt;</span><span>(</span><span>&amp;</span><span>str</span><span>,</span> <span>JSONValue</span><span>)</span><span>,</span> <span>JSONParseError</span><span>&gt;</span> <span>{</span>
</span><span>    <span>match</span> <span>object</span><span>(</span>src<span>)</span> <span>{</span>
</span><span>        <span>Ok</span><span>(</span>res<span>)</span> <span>=&gt;</span> <span>return</span> <span>Ok</span><span>(</span>res<span>)</span><span>,</span>
</span><span>        <span>Err</span><span>(</span><span>JSONParseError</span><span>::</span><span>NotFound</span><span>)</span> <span>=&gt;</span> <span>{</span><span>}</span> <span>// if not found, that ok</span>
</span><span>        <span>Err</span><span>(</span>e<span>)</span> <span>=&gt;</span> <span>return</span> <span>Err</span><span>(</span>e<span>)</span><span>,</span>
</span><span>    <span>}</span>
</span><span>
</span><span>    <span>match</span> <span>array</span><span>(</span>src<span>)</span> <span>{</span>
</span><span>        <span>Ok</span><span>(</span>res<span>)</span> <span>=&gt;</span> <span>return</span> <span>Ok</span><span>(</span>res<span>)</span><span>,</span>
</span><span>        <span>Err</span><span>(</span><span>JSONParseError</span><span>::</span><span>NotFound</span><span>)</span> <span>=&gt;</span> <span>{</span><span>}</span> <span>// if not found, that ok</span>
</span><span>        <span>Err</span><span>(</span>e<span>)</span> <span>=&gt;</span> <span>return</span> <span>Err</span><span>(</span>e<span>)</span><span>,</span>            <span>// if any other error, propogate it up</span>
</span><span>    <span>}</span>
</span><span>
</span><span>    <span>match</span> <span>string</span><span>(</span>src<span>)</span> <span>{</span>
</span><span>        <span>Ok</span><span>(</span>res<span>)</span> <span>=&gt;</span> <span>return</span> <span>Ok</span><span>(</span>res<span>)</span><span>,</span>
</span><span>        <span>Err</span><span>(</span><span>JSONParseError</span><span>::</span><span>NotFound</span><span>)</span> <span>=&gt;</span> <span>{</span><span>}</span> <span>// if not found, that ok</span>
</span><span>        <span>Err</span><span>(</span>e<span>)</span> <span>=&gt;</span> <span>return</span> <span>Err</span><span>(</span>e<span>)</span><span>,</span>            <span>// if any other error, propogate it up</span>
</span><span>    <span>}</span>
</span><span>
</span><span>    <span>match</span> <span>number</span><span>(</span>src<span>)</span> <span>{</span>
</span><span>        <span>Ok</span><span>(</span>res<span>)</span> <span>=&gt;</span> <span>return</span> <span>Ok</span><span>(</span>res<span>)</span><span>,</span>
</span><span>        <span>Err</span><span>(</span><span>JSONParseError</span><span>::</span><span>NotFound</span><span>)</span> <span>=&gt;</span> <span>{</span><span>}</span> <span>// if not found, that ok</span>
</span><span>        <span>Err</span><span>(</span>e<span>)</span> <span>=&gt;</span> <span>return</span> <span>Err</span><span>(</span>e<span>)</span><span>,</span>            <span>// if any other error, propogate it up</span>
</span><span>    <span>}</span>
</span><span>
</span><span>    <span>match</span> <span>bool</span><span>(</span>src<span>)</span> <span>{</span>
</span><span>        <span>Ok</span><span>(</span>res<span>)</span> <span>=&gt;</span> <span>return</span> <span>Ok</span><span>(</span>res<span>)</span><span>,</span>
</span><span>        <span>Err</span><span>(</span><span>JSONParseError</span><span>::</span><span>NotFound</span><span>)</span> <span>=&gt;</span> <span>{</span><span>}</span> <span>// if not found, that ok</span>
</span><span>        <span>Err</span><span>(</span>e<span>)</span> <span>=&gt;</span> <span>return</span> <span>Err</span><span>(</span>e<span>)</span><span>,</span>            <span>// if any other error, propogate it up</span>
</span><span>    <span>}</span><span>;</span>
</span><span>
</span><span>    <span>match</span> <span>null</span><span>(</span>src<span>)</span> <span>{</span>
</span><span>        <span>Ok</span><span>(</span>res<span>)</span> <span>=&gt;</span> <span>return</span> <span>Ok</span><span>(</span>res<span>)</span><span>,</span>
</span><span>        <span>Err</span><span>(</span><span>JSONParseError</span><span>::</span><span>NotFound</span><span>)</span> <span>=&gt;</span> <span>{</span><span>}</span> <span>// if not found, that ok</span>
</span><span>        <span>Err</span><span>(</span>e<span>)</span> <span>=&gt;</span> <span>return</span> <span>Err</span><span>(</span>e<span>)</span><span>,</span>            <span>// if any other error, propogate it up</span>
</span><span>    <span>}</span><span>;</span>
</span><span>
</span><span>    <span>Err</span><span>(</span><span>JSONParseError</span><span>::</span><span>NotFound</span><span>)</span>
</span><span><span>}</span>
</span></code></pre>
<p>This is just a simple flow - just try to parse the root JSON value as each of the types in order. If one of them succeeds, return the result. If none of them succeed, return an error.</p>
<p>With this, the parser is complete. It can parse a JSON string into a Rust <code>JSONValue</code> enum in just 500 lines of code. Here's a gist of just this implementation: <a href="https://gist.github.com/Krish120003/369a892ba7189d3b91b91845e60a1ffa">https://gist.github.com/Krish120003/369a892ba7189d3b91b91845e60a1ffa</a></p>
<h2 id="testing-and-performance">Testing and Performance</h2>
<p>I wrote a few unit tests to make sure the parser works as expected. There are a common set of benchmark files for JSON parsers available <a href="https://github.com/serde-rs/json-benchmark/tree/master/data">here</a>. I used the <code>canada.json</code> and <code>twitter.json</code> files to test the parser. The parser was able to parse these files correctly, so I was happy with the results. The code for testing exceeds the 500 lines, so I didn't include it in the gist.</p>
<p>For performance testing, I found a nice graph on the <a href="https://github.com/ibireme/yyjson">yyjson github</a> that details JSON reader speeds for different JSON parsers. On <code>canada.json</code>, all the parsers achieve a speed under 1 GB/s. My parser was not at all optimized for performance, so I didn't expect it to be fast. Still, I decided to run a very crude benchmark to see how it compared to other parsers.</p>
<pre><code><span><span>let</span> big_file <span>=</span> <span>std<span>::</span>fs<span>::</span></span><span>read_to_string</span><span>(</span><span>"canada.json"</span><span>)</span><span>.</span><span>expect</span><span>(</span><span>"Could not read file"</span><span>)</span><span>;</span>
</span><span>
</span><span><span>// how many bytes of data?</span>
</span><span><span>let</span> num_bytes <span>=</span> big_file<span>.</span><span>len</span><span>(</span><span>)</span><span>;</span>
</span><span>
</span><span><span>let</span> mul <span>=</span> <span>1000</span><span>;</span>
</span><span><span>let</span> bytes_to_parse <span>=</span> num_bytes <span>*</span> mul<span>;</span>
</span><span>
</span><span><span>let</span> start_time <span>=</span> <span>std<span>::</span>time<span>::</span></span><span>Instant</span><span>::</span><span>now</span><span>(</span><span>)</span><span>;</span>
</span><span><span>for</span> _ <span>in</span> <span>0</span><span>..</span>mul <span>{</span>
</span><span>    <span>let</span> _ <span>=</span> <span>parse</span><span>(</span>big_file<span>.</span><span>as_str</span><span>(</span><span>)</span><span>)</span><span>;</span>
</span><span><span>}</span>
</span><span><span>let</span> end_time <span>=</span> <span>std<span>::</span>time<span>::</span></span><span>Instant</span><span>::</span><span>now</span><span>(</span><span>)</span><span>;</span>
</span><span>
</span><span><span>let</span> bps <span>=</span> bytes_to_parse <span>as</span> <span>f64</span> <span>/</span> <span>(</span>end_time <span>-</span> start_time<span>)</span><span>.</span><span>as_secs_f64</span><span>(</span><span>)</span><span>;</span>
</span><span>
</span><span><span>let</span> mbs <span>=</span> <span>(</span>bytes_to_parse <span>as</span> <span>f64</span><span>)</span> <span>/</span> <span>(</span><span>1_000_000.0</span><span>)</span><span>;</span>
</span><span><span>let</span> mbps <span>=</span> mbs <span>/</span> <span>(</span>end_time <span>-</span> start_time<span>)</span><span>.</span><span>as_secs_f64</span><span>(</span><span>)</span><span>;</span>
</span><span>
</span><span><span>let</span> gbs <span>=</span> <span>(</span>bytes_to_parse <span>as</span> <span>f64</span><span>)</span> <span>/</span> <span>(</span><span>1_000_000_000.0</span><span>)</span><span>;</span>
</span><span><span>let</span> gbps <span>=</span> gbs <span>/</span> <span>(</span>end_time <span>-</span> start_time<span>)</span><span>.</span><span>as_secs_f64</span><span>(</span><span>)</span><span>;</span>
</span><span>
</span><span><span>println!</span><span>(</span><span>"Parsing speed: {:.2} Bytes/s"</span><span>,</span> bps<span>)</span><span>;</span>
</span><span><span>println!</span><span>(</span><span>"Parsing speed: {:.2} MB/s"</span><span>,</span> mbps<span>)</span><span>;</span>
</span><span><span>println!</span><span>(</span><span>"Parsing speed: {:.2} GB/s"</span><span>,</span> gbps<span>)</span><span>;</span>
</span></code></pre>
<p>I ran the parser on the <code>canada.json</code> file and compared it to the other parsers. With this crude benchmark, my parser was able to parse the file at a speed of around:</p>
<pre><code><span>Parsing speed: 52014622.29 Bytes/s
</span><span>Parsing speed: 52.01 MB/s
</span><span>Parsing speed: 0.05 GB/
</span></code></pre>
<p>This is not a good speed. But it's still fast enough to parse a large JSON file in under a second. I'm happy with the results, considering I didn't optimize for performance at all. Maybe some day I'll come back and try to make it faster.</p>
<h2 id="pretty-errors">Pretty Errors</h2>
<p>Finally, I wanted to make the error messages more readable. Right now, the errors are just enums with a number associated with them. I wanted to make them more human-readable, kind of like Python errors; I wanted to know which specific location in the input string caused the error, and I wanted to print surrounding context to help "debug" the issue.</p>
<p>So, after a bit of tinkering, I was able to use the <code>usize</code> values associated with the errors to print out the error message with the surrounding context. This made it much easier to debug issues with the parser.</p>
<p>The approach behind this was to use the size of the leftover source at the time of the error to compute the line number and column number of the error. This was then used to print out the error message with the surrounding context. I also added a pretty arrow to point to the exact location of the error.</p>
<pre><code><span><span>Error</span><span>:</span> <span>UnexpectedChar</span><span>(</span><span>76</span><span>)</span>
</span><span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span>
</span><span>  <span>"age"</span><span>:</span> <span>30</span><span>,</span>
</span><span>  <span>"cars"</span><span>:</span> <span>[</span><span>"Ford \e This has an invalid escape"</span><span>,</span> <span>"BMW"</span><span>,</span> <span>"Fiat"</span><span>]</span><span>,</span>
</span><span>                   <span>^</span>
</span><span>                   <span>|</span>
</span><span>                   <span>|</span>
</span><span><span>Error</span><span>:</span> <span>Unexpected</span> <span>Character</span> on <span>Line</span> <span>4</span> <span>Char</span> <span>19</span>
</span></code></pre>
<p>This is a nice way to show the error, and it helped me debug the parser when I was testing it.</p>
<h2 id="confusions">Confusions</h2>
<p>I understand most of what is happening in the parser, but I am very confused by a certain phenomenon. When I run the parser on <code>twitter.json</code> by doing <code>cargo run --release</code>, the parser runs at about 60 MB/s.</p>
<p>But when I run the parser on <code>twitter.json</code> by doing <code>sudo cargo run --release</code>, the parser runs at about 100+ MB/s. I have no idea why this is happening. Using sudo is significantly increasing the speed for my parser. If you have any idea, please let me know.</p>
<blockquote><p lang="en" dir="ltr">someone needs to explain this to me <a href="https://t.co/IPTEbCM50C">pic.twitter.com/IPTEbCM50C</a></p>— Krish (@n0tkr1sh) <a href="https://twitter.com/n0tkr1sh/status/1794786108225827309?ref_src=twsrc%5Etfw">May 26, 2024</a></blockquote> 
<h2 id="the-end">The End</h2>
<p>This was a fun project to work on. I learned a lot about Rust, parsers, and JSON. I also learned how to write a parser from scratch, which was a great experience. I'm happy with the results, and I'm glad I finally sat down to learn Rust.</p>
<p>The final code is about 800 lines, with all the tests, the benchmark, and the pretty error messages. You can find the full code on my GitHub: <a href="https://github.com/Krish120003/jsonparser/">https://github.com/Krish120003/jsonparser/</a>.</p>
<p>The JSON spec I used is available at <a href="https://www.json.org/json-en.html">https://www.json.org/json-en.html</a>.</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Meta announces LlamaCon, its first generative AI dev conference on April 29 (117 pts)]]></title>
            <link>https://www.meta.com/blog/connect-2025-llamacon-save-the-date/</link>
            <guid>43096922</guid>
            <pubDate>Wed, 19 Feb 2025 00:18:52 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.meta.com/blog/connect-2025-llamacon-save-the-date/">https://www.meta.com/blog/connect-2025-llamacon-save-the-date/</a>, See on <a href="https://news.ycombinator.com/item?id=43096922">Hacker News</a></p>
Couldn't get https://www.meta.com/blog/connect-2025-llamacon-save-the-date/: Error: timeout of 10000ms exceeded]]></description>
        </item>
        <item>
            <title><![CDATA[Alice Hamilton waged a one-woman campaign to get the lead out of everything (292 pts)]]></title>
            <link>https://www.smithsonianmag.com/innovation/how-alice-hamilton-waged-one-woman-campaign-get-lead-out-everything-180985960/</link>
            <guid>43096422</guid>
            <pubDate>Tue, 18 Feb 2025 23:22:22 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.smithsonianmag.com/innovation/how-alice-hamilton-waged-one-woman-campaign-get-lead-out-everything-180985960/">https://www.smithsonianmag.com/innovation/how-alice-hamilton-waged-one-woman-campaign-get-lead-out-everything-180985960/</a>, See on <a href="https://news.ycombinator.com/item?id=43096422">Hacker News</a></p>
Couldn't get https://www.smithsonianmag.com/innovation/how-alice-hamilton-waged-one-woman-campaign-get-lead-out-everything-180985960/: Error: timeout of 10000ms exceeded]]></description>
        </item>
        <item>
            <title><![CDATA[HP Acquires Humane's AI Software (166 pts)]]></title>
            <link>https://humane.com/media/humane-hp</link>
            <guid>43095811</guid>
            <pubDate>Tue, 18 Feb 2025 22:15:05 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://humane.com/media/humane-hp">https://humane.com/media/humane-hp</a>, See on <a href="https://news.ycombinator.com/item?id=43095811">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-oldscrollwrapper="true"><p><strong>Palo Alto, CA, February 18, 2025</strong> – HP Inc. (NYSE: HPQ) announced a definitive agreement to acquire key AI capabilities from Humane, including their AI-powered platform Cosmos, highly skilled technical talent, and intellectual property with more than 300 patents and patent applications. The acquisition advances HP’s transformation into a more experience-led company.</p><p>"This investment will rapidly accelerate our ability to develop a new generation of devices that seamlessly orchestrate AI requests both locally and in the cloud," said Tuan Tran, President of Technology and Innovation at HP. "Humane’s AI platform Cosmos, backed by an incredible group of engineers, will help us create an intelligent ecosystem across all HP devices from AI PCs to smart printers and connected conference rooms. This will unlock new levels of functionality for our customers and deliver on the promises of AI."</p><p>The acquisition brings a highly skilled group of Humane engineers, architects, and product innovators to HP’s Technology and Innovation Organization. They will form HP IQ, HP’s new AI innovation lab focused on building an intelligent ecosystem across HP’s products and services for the future of work.</p><p>“We’re excited to join HP at such a pivotal moment in the industry and help shape the future of intelligent experiences,” said Bethany Bongiorno and Imran Chaudhri, Co-founders of Humane. “HP’s scale, global reach, and operational excellence—combined with our design-led approach, integration technology, and engineering expertise—will redefine workforce productivity.”</p><p>HP is committed to reinventing the future of work through technology, delivering experiences that empower organizations and employees to thrive in today's dynamic environment.</p><p>The $116 million transaction is expected to close at the end of this month.&nbsp;</p><p><strong>About HP&nbsp;</strong></p><p>HP Inc. (NYSE: HPQ) is a global technology leader and creator of solutions that enable people to bring their ideas to life and connect to the things that matter most. Operating in more than 170 countries, HP delivers a wide range of innovative and sustainable devices, services and subscriptions for personal computing, printing, 3D printing, hybrid work, gaming, and more. For more information, please visit: <a href="http://www.hp.com/" target="_blank" rel="noreferrer"><strong>http://www.hp.com</strong></a>.</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[A year of uv: pros, cons, and should you migrate (625 pts)]]></title>
            <link>https://www.bitecode.dev/p/a-year-of-uv-pros-cons-and-should</link>
            <guid>43095157</guid>
            <pubDate>Tue, 18 Feb 2025 21:09:19 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.bitecode.dev/p/a-year-of-uv-pros-cons-and-should">https://www.bitecode.dev/p/a-year-of-uv-pros-cons-and-should</a>, See on <a href="https://news.ycombinator.com/item?id=43095157">Hacker News</a></p>
Couldn't get https://www.bitecode.dev/p/a-year-of-uv-pros-cons-and-should: Error: timeout of 10000ms exceeded]]></description>
        </item>
        <item>
            <title><![CDATA[Kafka at the low end: how bad can it get? (111 pts)]]></title>
            <link>https://broot.ca/kafka-at-the-low-end.html</link>
            <guid>43095070</guid>
            <pubDate>Tue, 18 Feb 2025 21:01:02 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://broot.ca/kafka-at-the-low-end.html">https://broot.ca/kafka-at-the-low-end.html</a>, See on <a href="https://news.ycombinator.com/item?id=43095070">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
<p>There is oft-quoted advice that Kafka does poorly as a job queue. I’ve experienced
this myself, and I wanted to formalize it a bit.</p>

<p>I’ll use the common architecture of a Web application submitting
background jobs to workers via Kafka (for example, to generate a PDF of some
report).  Except for the use of Kafka in this role, this is common in Web
applications, and (speaking from experience!) when Kafka is already deployed,
there is an impulse to use it instead of deploying yet-another queue system.</p>

<p>Note: when <a href="https://www.confluent.io/blog/queues-on-kafka/">Queues for Kafka (KIP-932)</a> becomes
a thing, a lot of these concerns go away. I look forward to it!</p>

<p>What I want to characterize here is the worst-case “unfairness” of jobs being
assigned to workers. There are many other reasons to not use Kafka as a job
queue, but this unfairness is (in my view) the strongest reason. In most
queues, you put work into the queue and every worker… well, <em>works</em> until all
the work is done. It sound obvious, but that’s the raison d’être for these
things! When (mis-)using Kafka as a queue, this is not the case: work can get
unfairly assigned to one worker, even if other workers have nothing to do. So,
how many jobs can <em>one</em> worker be assigned, before <em>any other</em> worker is
given work? This can be worked out with this formula:</p>

<div><pre><code>WorstCaseJobsPerConsumer = (Partitions / Consumers) * Producers
</code></pre></div>

<p>To work an example, say you have a topic with 16 partitions, because you would like to
be able to scale up to 16 consumers at peak times, but, at your current load you only
predict you need 4 consumers processing jobs. Further, say you have 5 producers
(Web application servers, here - Gunicorn processes, Kubernetes pods, whatever)
that receive an API call and put a job onto this Kafka topic. Imagine these Web
workers are behind a load balancer which routes API calls in a round-robin
fashion to each of those 5 Web workers. Pretty typical architecture right?</p>

<p><img src="https://broot.ca/img/kafka.png"></p>

<p>Plugging these numbers in, we get <code>(16 / 4) * 5 == 20</code>: that means, if you’re unlucky, the
next <em>20 jobs</em> coming along could <em>all</em> be routed to a single consumer, and that
consumer has to churn away at those 20 jobs while its 3 counterparts will sit idle. How this would
happen is by the following somewhat unlucky sequence of events:</p>

<ul>
  <li>Before any API calls are made, the 4 worker processes start up, and each take 4 of the 16 topic’s partitions, so that
the partitions are fairly shared.</li>
  <li>20 API calls are made by clients.</li>
  <li>The load balancer round-robins these 20 requests, giving 4 requests each to the 5 Web workers</li>
  <li>Each of these Web workers puts those 4 records onto 4 of the topic’s partitions in a round-robin fashion. And, because they
do not coordinate this, they might choose the same 4 partitions, which happen to all land on a single consumer.</li>
</ul>

<p>This exact sequence of events is rare, but milder variations of this happen
constantly when Kafka is used this way, at a low volume - such as only half, or
three-quarters of your workers being busy, while the remainder are idle <em>and
there’s work queued, just sitting there</em>.</p>

<p>To decide if this matters to your application, think about your peak periods
and how many jobs might be created in that period, and what the latency
expectations are for those jobs. If it’s a small internal application used by,
say, 15 users, and they all (in the same instant) request 1 job that takes 5
minutes to run, then those 15 jobs can land on the same consumer and the queue
takes 75 minutes to clear, leading to some of those users being very unhappy.
This doesn’t <em>always</em> happen, but it <em>can</em>.
On the other hand, if you have 200 users each requesting 1 job and those jobs
take 1 second to run, these 200 jobs will be much more fairly distributed and
all workers will be contributing to clearing that queue.  So where, exactly, is
this cutoff? As a rule of thumb, if you have <em>at least</em>
<code>WorstCaseJobsPerConsumer * Consumers</code> jobs in-flight in your peak period (in
the above example, this is <code>20 * 4 == 80 jobs</code>), then you can be sure that all
your workers are doing <em>some</em> work, because there are enough jobs to overcome
the aforementioned worst-case behaviour. If there are fewer jobs than this,
you run the risk that some workers will not be pulling their weight.</p>

<p>Please note that I’m completely ignoring varying job run times. That makes this
problem significantly worse, because again, work is assigned to workers on a
record-by-record basis, irrespective of how long those jobs take. A long job
will block a short job and there’s nothing you can do about it.</p>

<p>I am <em>not</em> trying to say Kafka is a bad tool - what I am saying is <em>it was not
designed for such a low volume</em>. It was designed for exactly the opposite
(millions or billions of records) where a conventional single-node message
broker <em>simply cannot keep up</em>. It strips away a lot of the <em>very useful</em>
features of these conventional brokers in order to go faster. If you don’t
<em>need</em> that speed, you are losing a lot in that trade-off!</p>

<p>In conclusion: the oft-quoted wisdom is right; Kafka is not a good job queue,
especially not at particularly low volumes, at least until <a href="https://www.confluent.io/blog/queues-on-kafka/">Queues for Kafka
(KIP-932)</a> comes along.</p>

</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[South Korean regulator accuses DeepSeek of sharing user data with ByteDance (228 pts)]]></title>
            <link>https://www.bbc.com/news/articles/c4gex0x87g4o</link>
            <guid>43094651</guid>
            <pubDate>Tue, 18 Feb 2025 20:29:16 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.bbc.com/news/articles/c4gex0x87g4o">https://www.bbc.com/news/articles/c4gex0x87g4o</a>, See on <a href="https://news.ycombinator.com/item?id=43094651">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-component="text-block"><p>South Korea has accused Chinese AI startup DeepSeek of sharing user data with the owner of TikTok in China.<!-- --></p><p>"We confirmed DeepSeek communicating with ByteDance," the South Korean data protection regulator told <!-- --><a target="_blank" href="https://en.yna.co.kr/view/AEN20250218005300315">Yonhap News Agency.<!-- --></a></p><p>The country had already <!-- --><a target="_self" href="https://www.bbc.co.uk/news/articles/clyzym0vn8go">removed DeepSeek from app stores<!-- --></a> over the weekend over data protection concerns.<!-- --></p><p>The Chinese app <!-- --><a target="_self" href="https://www.bbc.co.uk/news/articles/c5yv5976z9po">caused shockwaves<!-- --></a> in the AI world in January, wiping billions off global stock markets over claims its new model was trained at a much lower cost than US rivals such as ChatGPT.<!-- --></p></div><div data-component="text-block"><p>Since then, multiple countries have warned that user data may not be properly protected, and in February a US cybersecurity company <!-- --><a target="_blank" href="https://securityscorecard.com/blog/a-deep-peek-at-deepseek/#bytedance-code-implications:~:text=ByteDance%20Code%20Implications">alleged potential data sharing<!-- --></a> between DeepSeek and ByteDance.<!-- --></p><p>DeepSeek's apparent overnight impact saw it shoot to the top of App Store charts in the UK, US and many other countries around the world - although it now sits far below ChatGPT in UK rankings.<!-- --></p><p>In South Korea, it had been downloaded over a million times before being pulled from Apple and Google's App Stores on Saturday evening. <!-- --></p><p>Existing users can still access the app and use it on a web browser.<!-- --></p><p>The data regulator, the Personal Information Protection Commission (PIPC), told South Korea's Yonhap News Agency that despite finding a link between DeepSeek and ByteDance, it was "yet to confirm what data was transferred and to what extent".<!-- --></p><p>Critics of the Chinese state have long argued its National Intelligence Law <!-- --><a target="_self" href="https://www.bbc.co.uk/news/technology-65019279">allows the government<!-- --></a> to access any data it wants from Chinese companies.<!-- --></p><p>However, ByteDance, headquartered in Beijing, is owned by a number of global investors - <!-- --><a target="_self" href="https://www.bbc.co.uk/news/technology-64797355#:~:text=Article%20seven%20of%20China's%20National,TikTok%2C%20but%20all%20Chinese%20companies.&amp;text=Why%20does%20the%20US%20want%20to%20ban%20TikTok%3F">and others say<!-- --></a> the same law allows for the protection of private companies and personal data.<!-- --></p><p>Fears over user data being sent to China was one of the reasons the US Supreme Court upheld a ban on TikTok, which is owned by ByteDance. <!-- --></p><p>The US ban is <!-- --><a target="_self" href="https://www.bbc.co.uk/news/articles/c4g91kyjw07o">on hold until 5 April<!-- --></a> as President Donald Trump attempts to broker a resolution.<!-- --></p></div><div data-component="text-block"><p>Cybersecurity company Security Scorecard <!-- --><a target="_blank" href="https://securityscorecard.com/blog/a-deep-peek-at-deepseek/">published a blog<!-- --></a> on DeepSeek on 10 February which suggested "multiple direct references to ByteDance-owned" services.<!-- --></p><p>"These references suggest deep integration with ByteDance's analytics and performance monitoring infrastructure," it said in its review of DeepSeek's Android app.<!-- --></p><p>Security Scorecard expressed concern that along with privacy risks, DeepSeek "user behaviour and device metadata [are] likely sent to ByteDance servers".<!-- --></p><p>It also found data "being transmitted to domains linked to Chinese state-owned entities".<!-- --></p><p>On Monday, <!-- --><a target="_blank" href="https://www.pipc.go.kr/eng/user/ltn/new/noticeDetail.do">South Korea's PIPC said<!-- --></a> it "found out traffic generated by third-party data transfers and insufficient transparency in DeepSeek's privacy policy".<!-- --></p><p>It said DeepSeek was cooperating with the regulator, and acknowledged it had failed to to take into account South Korean privacy laws.<!-- --></p><p>But the regulator advised users "exercise caution and avoid entering personal information into the chatbot".<!-- --></p><p>South Korea has already followed a number of countries such as Australia and Taiwan in banning DeepSeek from government devices. <!-- --></p><p>The BBC has contacted the PIPC, ByteDance and DeepSeek's parent company, High Flyer, for a response.<!-- --></p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[AWS paywalling select knowledge base articles, requiring Premium Support plan (189 pts)]]></title>
            <link>https://repost.aws/knowledge-center/eks-api-server-unauthorized-error</link>
            <guid>43094467</guid>
            <pubDate>Tue, 18 Feb 2025 20:15:24 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://repost.aws/knowledge-center/eks-api-server-unauthorized-error">https://repost.aws/knowledge-center/eks-api-server-unauthorized-error</a>, See on <a href="https://news.ycombinator.com/item?id=43094467">Hacker News</a></p>
Couldn't get https://repost.aws/knowledge-center/eks-api-server-unauthorized-error: Error: Request failed with status code 403]]></description>
        </item>
        <item>
            <title><![CDATA[Valve releases Team Fortress 2 game code (1617 pts)]]></title>
            <link>https://github.com/ValveSoftware/source-sdk-2013/commit/0759e2e8e179d5352d81d0d4aaded72c1704b7a9</link>
            <guid>43094260</guid>
            <pubDate>Tue, 18 Feb 2025 19:57:17 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/ValveSoftware/source-sdk-2013/commit/0759e2e8e179d5352d81d0d4aaded72c1704b7a9">https://github.com/ValveSoftware/source-sdk-2013/commit/0759e2e8e179d5352d81d0d4aaded72c1704b7a9</a>, See on <a href="https://news.ycombinator.com/item?id=43094260">Hacker News</a></p>
<div id="readability-page-1" class="page"><div itemscope="" itemtype="http://schema.org/SoftwareSourceCode" data-commit-hovercards-enabled="" data-discussion-hovercards-enabled="" data-issue-and-pr-hovercards-enabled="" data-project-hovercards-enabled="">
    <main id="js-repo-pjax-container">
      
  

    
    

    






  
  

  



<turbo-frame id="repo-content-turbo-frame" target="_top" data-turbo-action="advance">
    <div id="repo-content-pjax-container">
  <p>
  <h2>Commit</h2>
</p>

<p><a href="https://github.com/ValveSoftware/source-sdk-2013/commit/0759e2e8e179d5352d81d0d4aaded72c1704b7a9" data-hotkey="y">Permalink</a></p>


<div>
  <div>
    

    <p><a id="browse-at-time-link" href="https://github.com/ValveSoftware/source-sdk-2013/tree/0759e2e8e179d5352d81d0d4aaded72c1704b7a9" rel="nofollow">Browse files</a></p><tool-tip id="tooltip-068139a0-98a5-40d9-a6e4-e3e722c5cd8a" for="browse-at-time-link" popover="manual" data-direction="ne" data-type="description" data-view-component="true">Browse the repository at this point in the history</tool-tip>
  </div>


  <div>
  <include-fragment src="/ValveSoftware/source-sdk-2013/branch_commits/0759e2e8e179d5352d81d0d4aaded72c1704b7a9" id="async-branches-list">
    <div>
      
      <ul>
        <li>Loading branch information<span></span></li>
      </ul>
    </div>
</include-fragment></div>


  
</div>


  


  <diff-layout>
    
        </diff-layout>


</div>

</turbo-frame>


    </main>
  </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[My LLM codegen workflow (436 pts)]]></title>
            <link>https://harper.blog/2025/02/16/my-llm-codegen-workflow-atm/</link>
            <guid>43094006</guid>
            <pubDate>Tue, 18 Feb 2025 19:33:32 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://harper.blog/2025/02/16/my-llm-codegen-workflow-atm/">https://harper.blog/2025/02/16/my-llm-codegen-workflow-atm/</a>, See on <a href="https://news.ycombinator.com/item?id=43094006">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p><em>tl:dr; Brainstorm spec, then plan a plan, then execute using LLM codegen. Discrete loops. Then magic. ✩₊˚.⋆☾⋆⁺₊✧</em></p><p>I have been building so many small products using LLMs. It has been fun, and useful. However, there are pitfalls that can waste so much time. A while back a friend asked me how I was using LLMs to write software. I thought “oh boy. how much time do you have!” and thus this post.</p><p>(p.s. if you are an AI hater - scroll to the end)</p><p>I talk to many dev friends about this, and we all have a similar approach with various tweaks in either direction.</p><p>Here is my workflow. It is built upon my own work, conversations with friends (thx <a href="https://www.nikete.com/">Nikete</a>, <a href="https://nocruft.com/">Kanno</a>, <a href="https://fsck.com/">Obra</a>, <a href="https://github.com/KristopherKubicki">Kris</a>, and <a href="https://thinks.lol/">Erik</a>), and following many best practices shared on the various terrible internet <a href="https://news.ycombinator.com/">bad</a> <a href="https://twitter.com/">places</a>.</p><p>This is working well <strong>NOW</strong>, it will probably not work in 2 weeks, or it will work twice as well. ¯\_(ツ)_/¯</p><h2 id="lets-go">Let’s go</h2><figure role="group" aria-describedby="caption-I always find these AI-generated images to be suspect. Say hi to my juggalo coding robot angel!"><img title="" loading="lazy" decoding="async" src="https://harper.blog/images/posts/llm-coding-robot.webp" alt="Juggalo Robot" width="" height=""><figcaption id="caption-I always find these AI-generated images to be suspect. Say hi to my juggalo coding robot angel!">I always find these AI-generated images to be suspect. Say hi to my juggalo coding robot angel!</figcaption></figure><p>There are many paths for doing dev, but my case is typically one of two:</p><ul><li>Greenfield code</li><li>Legacy modern code</li></ul><p>I will show you my process for both paths</p><h2 id="greenfield">Greenfield</h2><p>I find the following process works well for greenfield development. It provides a robust planning and documentation approach, and allows you to execute easily in small steps.</p><figure role="group" aria-describedby="caption-Technically, there is a green field on the right. Leica Q, 5/14/2016"><img title="" loading="lazy" decoding="async" src="https://harper.blog/images/posts/greenfield.jpg" alt="Green field" width="" height=""><figcaption id="caption-Technically, there is a green field on the right. Leica Q, 5/14/2016">Technically, there is a green field on the right. Leica Q, 5/14/2016</figcaption></figure><h3 id="step-1-idea-honing">Step 1: Idea honing</h3><p>Use a conversational LLM to hone in on an idea (I use ChatGPT 4o / o3 for this):</p><pre tabindex="0"><code data-lang="prompt">Ask me one question at a time so we can develop a thorough, step-by-step spec for this idea. Each question should build on my previous answers, and our end goal is to have a detailed specification I can hand off to a developer. Let’s do this iteratively and dig into every relevant detail. Remember, only one question at a time.

Here’s the idea:

&lt;IDEA&gt;</code></pre><p>At the end of the brainstorm (it will come to a natural conclusion):</p><pre tabindex="0"><code data-lang="prompt">Now that we’ve wrapped up the brainstorming process, can you compile our findings into a comprehensive, developer-ready specification? Include all relevant requirements, architecture choices, data handling details, error handling strategies, and a testing plan so a developer can immediately begin implementation.</code></pre><p>This will output a pretty solid and straightforward spec that can be handed off to the planning step. I like to save it as <code>spec.md</code> in the repo.</p><blockquote><p>You can use this spec for a number of things. We are doing codegen here, but I have used it to bolster ideas by asking a reasoning model to poke holes in the idea (must go deeper!), to generate a white paper, or to generate a business model. You can pop it into deep research and get a 10k word supporting document in return.</p></blockquote><h3 id="step-2-planning">Step 2: Planning</h3><p>Take the spec and pass it to a proper reasoning model (<code>o1*</code>, <code>o3*</code>, <code>r1</code>):</p><p>(This is the TDD prompt)</p><pre tabindex="0"><code data-lang="prompt">Draft a detailed, step-by-step blueprint for building this project. Then, once you have a solid plan, break it down into small, iterative chunks that build on each other. Look at these chunks and then go another round to break it into small steps. Review the results and make sure that the steps are small enough to be implemented safely with strong testing, but big enough to move the project forward. Iterate until you feel that the steps are right sized for this project.

From here you should have the foundation to provide a series of prompts for a code-generation LLM that will implement each step in a test-driven manner. Prioritize best practices, incremental progress, and early testing, ensuring no big jumps in complexity at any stage. Make sure that each prompt builds on the previous prompts, and ends with wiring things together. There should be no hanging or orphaned code that isn't integrated into a previous step.

Make sure and separate each prompt section. Use markdown. Each prompt should be tagged as text using code tags. The goal is to output prompts, but context, etc is important as well.

&lt;SPEC&gt;</code></pre><p>(This is the non-tdd prompt)</p><pre tabindex="0"><code data-lang="prompt">Draft a detailed, step-by-step blueprint for building this project. Then, once you have a solid plan, break it down into small, iterative chunks that build on each other. Look at these chunks and then go another round to break it into small steps. review the results and make sure that the steps are small enough to be implemented safely, but big enough to move the project forward. Iterate until you feel that the steps are right sized for this project.

From here you should have the foundation to provide a series of prompts for a code-generation LLM that will implement each step. Prioritize best practices, and incremental progress, ensuring no big jumps in complexity at any stage. Make sure that each prompt builds on the previous prompts, and ends with wiring things together. There should be no hanging or orphaned code that isn't integrated into a previous step.

Make sure and separate each prompt section. Use markdown. Each prompt should be tagged as text using code tags. The goal is to output prompts, but context, etc is important as well.

&lt;SPEC&gt;</code></pre><p>It should output a prompt plan that you can execute with aider, cursor, etc. I like to save this as <code>prompt_plan.md</code> in the repo.</p><p>I then have it output a <code>todo.md</code> that can be checked off.</p><pre tabindex="0"><code data-lang="prompt">Can you make a `todo.md` that I can use as a checklist? Be thorough.</code></pre><p>You can save it as <code>todo.md</code> in the repo.</p><p>Your codegen tool should be able to check off the <code>todo.md</code> while processing. This is good for keeping state across sessions.</p><h4 id="yay-plan">Yay. Plan!</h4><p>Now you have a robust plan and documentation that will help you execute and build your project.</p><p>This entire process will take maybe <strong>15 minutes</strong>. It is pretty quick. Wild tbh.</p><h3 id="step-3-execution">Step 3: Execution</h3><p>There are so many options available for execution. The success really depends on how well step 2 went.</p><p>I have used this workflow with <a href="https://githubnext.com/projects/copilot-workspace">github workspace</a>, <a href="https://aider.chat/">aider</a>, <a href="https://www.cursor.com/">cursor</a>, <a href="https://github.com/Doriandarko/claude-engineer">claude engineer</a>, <a href="https://sweep.dev/">sweep.dev</a>, <a href="https://chatgpt.com/">chatgpt</a>, <a href="https://claude.ai/">claude.ai</a>, etc. It works pretty well with all the tools I have tried, and I imagine it will work well with any codegen tool.</p><p>I, however, prefer <strong>raw</strong> claude and aider:</p><h3 id="claude">Claude</h3><p>I essentially pair program with <a href="https://claude.ai/">claude.ai</a> and just drop each prompt in iteratively. I find that works pretty well. The back and forth can be annoying, but it largely works.</p><p>I am in charge of the initial boilerplate code, and making sure tooling is set up correctly. This allows for some freedom, choice, and guidance in the beginning. Claude has a tendency to just output react code - and having a solid foundation with the language, style, and tooling of your choice will help quite a bit.</p><p>I will then use a tool like <a href="https://github.com/yamadashy/repomix">repomix</a> to iterate when things get stuck (more about that later).</p><p>The workflow is like this:</p><ul><li>set up the repo (boilerplate, uv init, cargo init, etc)</li><li>paste in prompt into claude</li><li>copy and paste code from claude.ai into IDE</li><li>run code, run tests, etc</li><li>…</li><li>if it works, move on to next prompt</li><li>if it doesn’t work, use repomix to pass the codebase to claude to debug</li><li>rinse repeat ✩₊˚.⋆☾⋆⁺₊✧</li></ul><h3 id="aider">Aider</h3><p><a href="https://aider.chat/">Aider</a> is fun and weird to use. I find that it slots in well to the output of step 2. I can get really far with very little work.</p><p>The workflow is essentially the same as above but instead of pasting into claude, I am pasting the prompts into aider.</p><p>Aider will then “just do it” and I get to play <a href="https://orteil.dashnet.org/cookieclicker/">cookie clicker</a>.</p><blockquote><p>An aside: Aider does really great benchmarking of new models for codegen in their <a href="https://aider.chat/docs/leaderboards/">LLM leaderboards</a>. I find it to be a really great resource for seeing how effective new models are.</p></blockquote><p>Testing is nice with aider, because it can be even more hands off as aider will run the test suite and debug things for you.</p><p>The workflow is like this:</p><ul><li>set up the repo (boilerplate, uv init, cargo init, etc)</li><li>start aider</li><li>paste prompt into aider</li><li>watch aider dance ♪┏(・o･)┛♪</li><li>aider will run tests, or you can run app to verify</li><li>if it works, move on to next prompt</li><li>if it doesn’t work, Q&amp;A with aider to fix</li><li>rinse repeat ✩₊˚.⋆☾⋆⁺₊✧</li></ul><h3 id="results">Results</h3><p>I have built so so many things using this workflow: scripts, expo apps, rust cli tools, etc. It has worked across programming languages, and contexts. I do like it.</p><p>If you have a small or large project that you are procrastinating on, I would recommend giving it a shot. You will be surprised how far you can get in a short amount of time.</p><p>My hack to-do list is empty because I built everything. I keep thinking of new things and knocking them out while watching a movie or something. For the first time in years, I am spending time with new programming languages and tools. This is pushing me to expand my programming perspective.</p><h2 id="non-greenfield-iteration-incrementally">Non-greenfield: Iteration, incrementally</h2><p>Sometimes you don’t have greenfield, and instead need to iterate or do increment work on an established code base.</p><figure role="group" aria-describedby="caption-This is not a green field. A random photo from my grandfather’s camera - somewhere in Uganda in the 60s"><img title="" loading="lazy" decoding="async" src="https://harper.blog/images/posts/brownfield.jpg" alt="a brown field" width="" height=""><figcaption id="caption-This is not a green field. A random photo from my grandfather’s camera - somewhere in Uganda in the 60s">This is not a green field. A random photo from my grandfather’s camera - somewhere in Uganda in the 60s</figcaption></figure><p>For this I have a slightly different method. It is similar to above, but a bit less “planning based.” The planning is done per task, not for the entire project.</p><h3 id="get-context">Get context</h3><p>I think everyone who is knee-deep in AI dev has a different tool for this, but you need something to grab your source code and efficiently jam it into the LLM.</p><p>I currently use a tool called <a href="https://github.com/yamadashy/repomix">repomix</a>. I have a task collection defined in my global <code>~/.config/mise/config.toml</code> that allows me to do various things with my code base (<a href="https://mise.jdx.dev/">mise rules</a>).</p><p>Here is the LLM task list:</p><div><pre tabindex="0"><code data-lang="shell"><span><span>LLM:clean_bundles           Generate LLM bundle output file using repomix
</span></span><span><span>LLM:copy_buffer_bundle      Copy generated LLM bundle from output.txt to system clipboard <span>for</span> external use
</span></span><span><span>LLM:generate_code_review    Generate code review output from repository content stored in output.txt using LLM generation
</span></span><span><span>LLM:generate_github_issues  Generate GitHub issues from repository content stored in output.txt using LLM generation
</span></span><span><span>LLM:generate_issue_prompts  Generate issue prompts from repository content stored in output.txt using LLM generation
</span></span><span><span>LLM:generate_missing_tests  Generate missing tests <span>for</span> code in repository content stored in output.txt using LLM generation
</span></span><span><span>LLM:generate_readme         Generate README.md from repository content stored in output.txt using LLM generation</span></span></code></pre></div><p>I generate an <code>output.txt</code> that has the context from my code base. If I am blowing through tokens, and it is too big - I will edit the generate command to ignore parts of the code base that are not germane to this task.</p><blockquote><p>One thing really nice about <code>mise</code> is that the tasks can be redefined and overloaded in the working directory’s <code>.mise.toml</code>. I can use a different tool to dump/pack the code, and as long as it generates an <code>output.txt</code> I can use my LLM tasks. This is helpful when various codebases differ so much. I regularly override the <code>repomix</code> step to include broader ignore patterns, or just use a more effective tool to do the packing.</p></blockquote><p>Once the output.txt is generated, I pass it to the <a href="https://github.com/simonw/LLM">LLM</a> command to do various transformations and then save those as a markdown file.</p><p>Ultimately, the mise task is running this: <code>cat output.txt | LLM -t readme-gen &gt; README.md</code> or <code>cat output.txt | LLM -m claude-3.5-sonnet -t code-review-gen &gt; code-review.md</code>. This isn’t super complicated. the <code>LLM</code> command is doing the heavy lifting (supporting different models, saving keys, and using prompt templates).</p><p>For example, if I need a quick review and fix of test coverage I would do the following:</p><h4 id="claude-1">Claude</h4><ul><li>go to the directory where the code lives</li><li>run <code>mise run LLM:generate_missing_tests</code></li><li>look at the generated markdown file (<code>issue-prompts.md</code>)</li><li>grab the full context for the code: <code>mise run LLM:copy_buffer_bundle</code></li><li>paste that into claude along with the first missing test “issue”</li><li>copy the generated code from claude into my ide.</li><li>…</li><li>run tests</li><li>rinse repeat ✩₊˚.⋆☾⋆⁺₊✧</li></ul><h4 id="aider-1">Aider</h4><ul><li>go to the directory where the code lives</li><li>run aider (always make sure you are on a new branch for aider work)</li><li>run <code>mise run LLM:generate_missing_tests</code></li><li>look at the generated markdown file (<code>issue-prompts.md</code>)</li><li>paste the first missing test “issue” into aider</li><li>watch aider dance ♪┏(・o･)┛♪</li><li>…</li><li>run tests</li><li>rinse repeat ✩₊˚.⋆☾⋆⁺₊✧</li></ul><p>This is a pretty good way to incrementally improve a code base. It has been super helpful to do small amounts of work in a big code base. I have found that I can do any sized tasks with this method.</p><h3 id="prompt-magic">Prompt magic</h3><p>These quick hacks work super well to dig into places where we can make a project more robust. It is super quick, and effective.</p><p>Here are some of my prompts that I use to dig into established code bases:</p><h4 id="code-review">Code review</h4><pre tabindex="0"><code data-lang="prompt">You are a senior developer. Your job is to do a thorough code review of this code. You should write it up and output markdown. Include line numbers, and contextual info. Your code review will be passed to another teammate, so be thorough. Think deeply  before writing the code review. Review every part, and don't hallucinate.</code></pre><h4 id="github-issue-generation">GitHub Issue generation</h4><p>(I need to automate the actual issue posting!)</p><pre tabindex="0"><code data-lang="prompt">You are a senior developer. Your job is to review this code, and write out the top issues that you see with the code. It could be bugs, design choices, or code cleanliness issues. You should be specific, and be very good. Do Not Hallucinate. Think quietly to yourself, then act - write the issues. The issues will be given to a developer to executed on, so they should be in a format that is compatible with github issues</code></pre><h4 id="missing-tests">Missing tests</h4><pre tabindex="0"><code data-lang="prompt">You are a senior developer. Your job is to review this code, and write out a list of missing test cases, and code tests that should exist. You should be specific, and be very good. Do Not Hallucinate. Think quietly to yourself, then act - write the issues. The issues  will be given to a developer to executed on, so they should be in a format that is compatible with github issues</code></pre><p>These prompts are pretty <em>old and busted</em> (“boomer prompts” if I may). They need some refactoring. If you have ideas to make them better lmk.</p><h2 id="skiing-ᨒ-𖠰ᨒ-𖠰">Skiing ᨒ↟ 𖠰ᨒ↟ 𖠰</h2><p>When I describe this process to people I say “you have to aggressively keep track of what’s going on because you can easily get ahead of yourself.”</p><p>For some reason I say “over my skies” a lot when talking about LLMs. I don’t know why. It resonates with me. Maybe it’s because it is beautiful smooth powder skiing, and then all of a sudden you are like “WHAT THE FUCK IS GOING ON!,” and are completely lost and suddenly fall off a cliff.</p><p>I find that using a <strong>planning step</strong> (ala the Greenfield process above) can help keep things under control. At least you will have a doc you can double-check against. I also do believe that testing is helpful - especially if you are doing wild style aider coding. Helps keep things good, and tight.</p><p>Regardless, I still do find myself <strong>over my skies</strong> quite a bit. Sometimes a quick break or short walk will help. In this regard it is a normal problem-solving process, but accelerated to a breakneck speed.</p><blockquote><p>We will often ask the LLM to include ridiculous things in our not very ridiculous code. For instance, we asked it to create a lore file and then reference the lore in the user interface. This is for python cli tools. Suddenly there is lore, glitchy interfaces, etc. All to manage your cloud functions, your todo list or whatever. The sky is the limit.</p></blockquote><h2 id="i-am-so-lonely-">I am so lonely (｡•́︿•̀｡)</h2><p>My main complaint about these workflows is that it is largely a solo endeavor - i.e. the interfaces are all <em>single player mode</em>.</p><p>I have spent years coding by myself, years coding as a pair, and years coding in a team. It is always better with people. These workflows are not easy to use as a team. The bots collide, the merges are horrific, the context complicated.</p><p>I really want someone to solve this problem in a way that makes coding with an LLM a multiplayer game. Not a solo hacker experience. There is so much opportunity to fix this and make it amazing.</p><p>GET TO WORK!</p><h2 id="ⴵ-time-ⴵ">ⴵ Time ⴵ</h2><p>All this codegen has accelerated the amount of code that I as a single person am able to generate. However, there is a weird side effect. I find myself having a huge amount of “downtime” while waiting for the LLM to finish burning its tokens.</p><figure role="group" aria-describedby="caption-I remember this like it was yesterday"><img title="" loading="lazy" decoding="async" src="https://harper.blog/images/posts/apple-print-shop-printing.png" alt="Printing" width="" height=""><figcaption id="caption-I remember this like it was yesterday">I remember this like it was yesterday</figcaption></figure><p>I have changed how I work enough to start incorporating some practice that will try and eat the waiting time:</p><ul><li>I start the “brainstorming” process for another project</li><li>I listen to records</li><li>I play <a href="https://orteil.dashnet.org/cookieclicker/">cookie clicker</a></li><li>I talk with friends and robots</li></ul><p>It is awesome to be able to hack like this. Hack Hack Hack. I can’t think of another time I have been this productive in code.</p><h2 id="haterade--_-">Haterade ╭∩╮( •̀_•́ )╭∩╮</h2><p>A lot of my friends are like “fuck LLMs. They are terrible at everything.” I don’t mind this POV. I don’t share it, but I think it is important to be skeptical. There are an awful lot of reasons to hate AI. My main fear is about power consumption and the environmental impact. But… the code must flow. Right… sigh.</p><p>If you are open to learning more, but don’t want to dig in and become a cyborg programmer - my recommendation is not to change your opinion, but to read Ethan Mollick’s book about LLMs and how they can be used: <a href="https://www.penguinrandomhouse.com/books/741805/co-intelligence-by-ethan-mollick/"><strong>Co-Intelligence: Living and Working with AI.</strong></a></p><p>It does a good job of explaining the benefits without being a tech anarcho-capitalist bro type tome. I found it very helpful, and have had so many good and nuanced conversations with friends who have read it. Highly recommended.</p><p>If you are skeptical, but a bit curious - feel free to hit me up and let’s talk through all this madness. I can show you how we use LLMs, and maybe we could build something together.</p><p><em>thanks to <a href="https://derek.broox.com/">Derek</a>, <a href="https://nocruft.com/">Kanno</a>, <a href="https://fsck.com/">Obra</a>, and <a href="https://thinks.lol/">Erik</a> for taking a look at this post and suggesting edits. I appreciate it.</em></p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Nuclear fusion: WEST beats the world record for plasma duration (396 pts)]]></title>
            <link>https://www.cea.fr/english/Pages/News/nuclear-fusion-west-beats-the-world-record-for-plasma-duration.aspx</link>
            <guid>43093939</guid>
            <pubDate>Tue, 18 Feb 2025 19:26:33 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.cea.fr/english/Pages/News/nuclear-fusion-west-beats-the-world-record-for-plasma-duration.aspx">https://www.cea.fr/english/Pages/News/nuclear-fusion-west-beats-the-world-record-for-plasma-duration.aspx</a>, See on <a href="https://news.ycombinator.com/item?id=43093939">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="ctl00_PlaceHolderMain_DisplayModePanel_top">
		
                <div>
                    <p id="ctl00_PlaceHolderMain_DisplayModePanel_top_TaxonomyFields_TaxonomyFieldsControl"><span><a href="https://www.cea.fr/english/Pages/SPECIAL-PAGES/Local-search.aspx?k=Press%20release">Press release</a></span> | <a href="https://www.cea.fr/english/Pages/SPECIAL-PAGES/Local-search.aspx?k=Energies">Energies</a> | <a href="https://www.cea.fr/english/Pages/SPECIAL-PAGES/Local-search.aspx?k=Fusion%20through%20magnetic%20containment">Fusion through magnetic containment</a> | <a href="https://www.cea.fr/english/Pages/SPECIAL-PAGES/Local-search.aspx?k=Nuclear%20fusion">Nuclear fusion</a></p>

                    
                    
                    
                    <p><img src="https://www.cea.fr/english/PublishingImages/thumbnails/photo-plasma.jpg" id="ctl00_PlaceHolderMain_DisplayModePanel_top_PeoplePicture_ImagetteField" width="218" height="138">
    <small id="ctl00_PlaceHolderMain_DisplayModePanel_top_PeoplePicture_SmallCredits">© CEA</small>
</p>

                    <div>
                        <p>​On 12 February, the CEA’s WEST machine was able to maintain a plasma for more than 22 minutes. In doing so, it smashed the previous record for plasma duration achieved with a tokamak. This leap forward demonstrates how our knowledge of plasmas and technological control of them over longer periods is becoming more mature, and offers hope that fusion plasmas can be stabilised for greater amounts of time in machines such as ITER.</p>
                    </div>
                </div><!--.cartouche-->

                <div id="bando-infos">
                    <p><em>
                    
                            Published on&nbsp;18 February 2025                      
                    
                    </em></p>


                </div><!--#bando-infos-->

                <div>
                    <div id="ctl00_PlaceHolderMain_DisplayModePanel_top_ctl02__ControlWrapper_RichHtmlField" aria-labelledby="ctl00_PlaceHolderMain_DisplayModePanel_top_ctl02_label"><p>​<strong>1,337 seconds: that was how long WEST, a tokamak run from the CEA Cadarache site in southern France&nbsp;and one of the EUROfusion consortium medium size Tokamak facilities, was able to maintain a plasma for on 12 February. </strong>This was a 25% improvement on the previous record time achieved with EAST, in China, a few weeks previously.</p><p><img src="https://www.cea.fr/english/PublishingImages/Pages/News/nuclear-fusion-west-beats-the-world-record-for-plasma-duration/plasma-record-image.jpg" alt="The plasma record reached a temperature of 50 million degrees." title="The plasma record reached a temperature of 50 million degrees. © CEA" id="img_o7r12ppr">&nbsp;</p><p>The plasma record reached a temperature of 50 million degrees. © CEA</p><p>Reaching durations such as these is <strong>a crucial milestone for machines like Iter</strong>, which will need to maintain fusion plasmas for several minutes. The end goal is to control the plasma, which is naturally unstable, while ensuring that all plasma-facing components are able to withstand its radiation without malfunctioning or polluting it.</p><p><img src="https://www.cea.fr/english/PublishingImages/Pages/News/nuclear-fusion-west-beats-the-world-record-for-plasma-duration/Vue-west-hall-tore.jpg" alt="WEST, the tokamak run by the CEA " title="WEST, the tokamak run by the CEA  © L. Godart/CEA" id="img_94elihk3">&nbsp;</p><p>WEST, the tokamak run by the CEA  © L. Godart/CEA</p><p>This is what CEA researchers intend to achieve and what explains the current record. Over the coming months, the WEST team will double down on its efforts to achieve very long plasma durations – up to several hours combined – but also to heat the plasma to even higher temperatures with a view to approaching the conditions expected in fusion plasmas.</p><p><strong>WEST is a CEA facility that benefits from the commission’s decades of experience in the use of tokamaks to study plasmas</strong>. It welcomes researchers from around the world, who make use of its key characteristics that allow long-duration plasmas, particularly its superconducting coils and actively cooled components. WEST is one facet of an international movement comprising other major experiments in which CEA researchers are heavily involved, such as JET, the Joint European Torus tokamak in the United Kingdom (closed in late 2023), which holds the record for fusion energy, JT-60SA in Japan, EAST in China, and KSTAR in South Korea, not to mention the flagship machine that is ITER.<br></p><blockquote><br>“WEST has achieved a new key technological milestone by maintaining hydrogen plasma for more than twenty minutes through the injection of 2 MW of heating power. Experiments will continue with increased power. This excellent result allows both WEST and the French community to lead the way for the future use of ITER.”, comment Anne-Isabelle Etienvre, Director of Fundamental Research at the CEA.<br></blockquote><h2>What is fusion used for?</h2><p>Nuclear fusion is a technology with the ultimate goal of controlling naturally unstable plasma. It uses even fewer resources and less fuel than fission, which was already very concentrated, and does not produce long-lived radioactive waste.&nbsp;&nbsp; &nbsp;</p><p>Of the various possible techniques for generating energy, the most advanced is magnetic confinement fusion , where plasma is held in a torus by an intense magnetic field and heated until the hydrogen nuclei fuse. Confinement fusion has been shown by JET to produce fusion power of 15 MW for several seconds.</p><p>France, home to both WEST and ITER, is well-placed to house the first prototype nuclear fusion reactor. Nuclear fusion is a source of energy that exploits nuclear reactions, with many possible complementary aspects with nuclear fission energy and associated techniques relating to neutrons and matter, which are well understood.</p><div><p>Nevertheless, given the infrastructure needed to produce this energy on a large scale, it is unlikely that fusion technology will make a significant contribution to achieving net-zero carbon emissions by 2050. For this, several technological sticking points need to be overcome, and the economic feasibility of this form of energy production must still be demonstrated.</p></div></div>
            
                     

                    <p id="ctl00_PlaceHolderMain_DisplayModePanel_top_GoToLink1_GoTopParagraph"><a href="#top" title="Go to the top of the page">Top page</a></p>
   
                            
                    
                    <!--#navmore-->


                    
                    <p id="navtags"><h2>Keywords&nbsp;:&nbsp;<a href="https://www.cea.fr/english/Pages/SPECIAL-PAGES/Local-search.aspx?k=fusion">fusion</a> | <a href="https://www.cea.fr/english/Pages/SPECIAL-PAGES/Local-search.aspx?k=plasma">plasma</a></h2></p><!--#navtags-->
                
                </div>
            
	</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Moving on from 18F (347 pts)]]></title>
            <link>https://ethanmarcotte.com/wrote/leaving-18f/</link>
            <guid>43093859</guid>
            <pubDate>Tue, 18 Feb 2025 19:18:43 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://ethanmarcotte.com/wrote/leaving-18f/">https://ethanmarcotte.com/wrote/leaving-18f/</a>, See on <a href="https://news.ycombinator.com/item?id=43093859">Hacker News</a></p>
<div id="readability-page-1" class="page"><article id="content" tabindex="-1">

      






    

    

    <div>
      <p><strong>Note:</strong> This post gets into the last few weeks of American politics. If that’s not your cup of tea, or if that’s a stressful topic for you, please feel free to skip this one. (Also, it’s a bit long. Sorry about that.)</p>
<hr>
<p>Last week, I finished my tenure as <a href="https://ethanmarcotte.com/wrote/18f/">a designer at 18F</a>.</p>
<p>I want to state up front: I’m not leaving under a “<a href="https://www.afge.org/article/afge-cautions-feds-not-to-be-tricked-into-resigning-you-might-not-get-paid/">deferred resignation</a>”. I also wasn’t laid off. (Though it’s possible I almost was; more on that later.) Instead, I resigned from my position as a product designer, submitting two weeks’ notice…well, two weeks ago.</p>
<p>Before I get into any of that, I’d like to write a bit about 18F, and why it was so hard to leave.</p>
<hr>
<p>While I was writing this post, I thought I’d revisit <a href="https://ethanmarcotte.com/wrote/18f/">what I wrote when I joined 18F last May</a>:</p>
<blockquote>
<ol>
<li>Every single person I’ve met this week — and I’ve met quite a few! — has been smart, kind, <em>and</em> really happy to be working where they do. As someone new to the organization, that’s so encouraging to see.</li>
<li>It’s, like, remarkably energizing to be around people who are really (really, really) passionate about making digital services work better for people.</li>
</ol>
</blockquote>
<p>Honestly, that holds up. Because really, the thread here is the people working at 18F, and the culture they’d built: I really, <em>really</em> liked showing up for work each morning. Everyone I met at 18F was inviting and kind, and excited to talk about what they were working on. (And just as crucially, what they did <em>outside</em> work.)</p>
<p>And my goodness, they were helpful — which, as a new kid joining the team, I’m always going to remember. Here’s one example: during my first month, I was grousing about some weird little computer issue, and a random coworker just offered to hop on a call to look at it with me. They hadn’t dealt with the issue before, and they definitely hadn’t dealt with <em>me</em> before, but they thought they might help a coworker out. And that impulse — <em>maybe I can help someone out</em> — sums up so many of my interactions with everyone at 18F. They were, and are, a remarkable group of people.</p>
<p>At the same time, I was proud of the work I was doing. Alongside my coworkers at 18F, I worked with client teams to help them define requirements, refine their designs, and build better products. I even got asked to pitch in on a small branding project, and I’d be the last person to call myself a brand designer. But I mention that because I was often asked to stretch myself, and every single time I felt safe trying something new — safe, and supported by my team. I can count on one hand the number of times over my career that I’ve felt that kind of safety at work. I doubt that’s true of every job in government, but I know it was true for me at 18F.</p>
<p>I know it sounds pat, but 18F was one of the best places I’ve ever worked. Until it wasn’t, and I felt I had to leave.</p>
<hr>
<p>Before I dive in, here are a couple points that’ll become relevant:</p>
<ul>
<li>I was considered a <em>probationary</em> employee because I’d been employed by the government for less than a year. <a href="https://federalnewsnetwork.com/workforce-rightsgovernance/2025/02/what-are-the-rules-for-probationary-periods-and-federal-employees/">Probationary employees</a> don’t have most of <a href="https://www.mspb.gov/">the protections afforded to “full” employees</a>, and can be dismissed more easily.</li>
<li>Due to some idiosyncrasies of how our roles were defined, many (most?) people in my organization were legally not eligible to join a union.</li>
</ul>
<p>So. After last year’s election, I was trying to decide whether or not I could stay at the job. A far-right candidate had won the election<sup><a href="#fn-margins" id="fnref-fn-margins">1</a></sup>, and was threatening to <a href="https://en.wikipedia.org/wiki/Project_2025">reshape the government</a> into something more partisan, more regressive, and more autocratic. My job involved putting rectangles on screens, and couldn’t have been further from any kind of political influence or impact. But despite that, I didn’t know if I could let myself be part of that government, even in a small way. (Also, <a href="https://ethanmarcotte.com/wrote/catalog/">as you might have guessed</a>: I was panicking.)</p>
<p>During that time, a friend suggested that while things were calm at work, I should write down some lines I wouldn’t want to cross: things I’d want to watch out for that, if they materialized, might be a reason to leave. This was wonderful advice, and I’m grateful to them for it. Equipped with a plan, even a small one, I started thinking through what my lines would be.</p>
<p>I’ll spare you the whole list, but I’ll share three of the entries.</p>
<ol>
<li>First, I need to work remotely. If the incoming administration made good on its promise to end teleworking for federal workers, I’d likely have to find another job. (This is, of course, <a href="https://finance.yahoo.com/news/quarter-bosses-admit-return-office-104103939.html">why “return to office” policies happen</a>.)</li>
<li>The second line was whether I’d be asked to work on a project that could kill or surveil people. I know precisely what governments are capable of — for good and for ill. But one of the things that drew me to the work at 18F was that I understood they tried to weigh individual workers’ preferences when projects were staffed. I figured if that ever changed, and I was asked to work on something I was morally opposed to, it’d be time to leave.</li>
<li>The third was being asked to meet with someone who didn’t work for the government, and being asked to discuss what I did for work.</li>
</ol>
<p>The first two were things I looked into when I was first interviewing at 18F: some of the basic criteria I was screening potential employers for. The third was driven at least in part by the election, and by the billionaire they were putting in charge of “government tech modernization.” I expected that if things went south, he’d just try to run the same horrible <a href="https://web.archive.org/web/20221102222024/https://www.washingtonpost.com/technology/2022/10/29/elon-musk-twitter-takeover/#:~:text=The%20note%20continued%3A%20%E2%80%9CPlease%20come%20prepared%20with%20code%20as%20a%20backup%20to%20review%20on%20your%20own%20machines%20with%20Elon.%E2%80%9D%20Later%2C%20people%20inside%20the%20company%20reported%20that%20Tesla%20engineers%20were%20in%20fact%20reviewing%20the%20code.">Twitter layoffs handbook</a>, and bring in employees from his other companies to rank — and cull — workers.</p>
<p>But it wasn’t just about that. Many things started happening to the federal government after the inauguration, none of them good. While the administration was conducting its vicious rollback of civil liberties and publicly funded research, <a href="https://www.bbc.com/news/articles/c23vkd57471o#:~:text=Despite%20its%20full%20name%2C%20Doge%20is%20not%20an%20official%20government%20department%2C%20which%20would%20have%20had%20to%20be%20established%20by%20an%20act%20of%20Congress.">this billionaire’s so-called “department”</a> was sweeping through <a href="https://www.wired.com/story/elon-musk-lackeys-office-personnel-management-opm-neuralink-x-boring-stalin/">various federal agencies</a>,  pushing aside career civil servants and the law to <a href="https://abcnews.go.com/US/judge-decide-block-doge-accessing-sensitive-labor-department/story?id=118575362">hoover up</a> <a href="https://www.nbcnews.com/politics/doge/doge-affiliated-employee-accessed-irs-system-sensitive-taxpayer-inform-rcna192423">radioactively</a> <a href="https://www.cnn.com/2025/02/17/politics/doge-irs-taxpayer-data/index.html">sensitive data</a> — <em>our</em> data, bought and paid for with <em>our</em> tax dollars, I should add.<sup><a href="#fn-conflicts" id="fnref-fn-conflicts">2</a></sup> And from what I’d read the group was acting on <a href="https://www.washingtonpost.com/business/2025/02/04/elon-musk-government-legal-doge/">dubious legal authority</a>, and with even less <a href="https://oversightdemocrats.house.gov/sites/evo-subsites/democrats-oversight.house.gov/files/evo-media-document/2025-02-06.Dem%20Members%20to%20IGs%20re%20Musk.pdf">oversight</a> or <a href="https://www.404media.co/doge-employees-ordered-to-stop-using-slack-while-agency-transitions-to-a-records-system-not-subject-to-foia/">transparency</a>. I didn’t want to sit down with anyone involved in that, and pretend like any part of their work was lawful, legitimate, or moral.</p>
<p>Anyway. The list was a tremendous help; I’ll always be grateful to the friend who suggested it. But given the speed at which government typically moves, I assumed I’d have several months before I’d have to wrestle with any of these questions. If not longer.</p>
<p>(I know, I know. I’m in the future, too.)</p>
<p>A few weeks ago, a member of <a href="https://www.gsa.gov/about-us/newsroom/news-releases/gsa-announces-new-commissioners-tts-director-and-general-counsel-01242025">the new leadership</a> announced they’d be reaching out to workers to discuss their recent “technical wins”, in order to better understand how the organization worked. The stress on “<em>technical</em> wins” to a <a href="https://experience.dropbox.com/resources/cross-functional-teams">cross-functional organization</a> felt significant to me; it also felt significant that most of the sessions seemed to be getting scheduled with folks who’d only recently joined government — probationary employees.</p>
<p>Just to state the obvious, this isn’t what you do when you want to understand how your organization works; it <em>is</em> what you do when you’re preparing to slash the size of your workforce. As you might imagine, this caused no small amount of panic across the agency, including within 18F. The new leadership hadn’t communicated these plans to anyone before making their announcement, which left 18F’s own leaders and supervisors frantically working to fill in the information void.</p>
<p>Shortly after the announcement, I started hearing about folks who’d had their meetings, but that they didn’t meet with the director who said they’d be conducting the interviews. Instead, they found themselves on a call with people who wouldn’t say where they worked in government; in a few cases, some people wouldn’t disclose their last names, or any part of their names.</p>
<p>And while I was watching these reports trickle in, I got a calendar invitation for my own interview. From the first email announcing the meetings, I figured one of my lines was in danger of being crossed; I just figured I’d have more time.</p>
<p>With only a few hours before my interview, I did a quick overview of my options. It looked like this:</p>
<ol>
<li>I could do the interview.</li>
<li>I could refuse to do the interview.</li>
<li>I could delay: call out sick, take a personal day, whatever.</li>
<li>I could resign.</li>
</ol>
<p>The first item wasn’t really an option, as sitting down with this “department” wasn’t something I could let myself do. Refusing to participate would’ve likely been seen as insubordination by a probationary hire; delaying would’ve just been, well, delaying the inevitable. (It also could have been seen as insubordination.) My math would’ve been different if I wasn’t probationary or, even better, if I’d been allowed to join a union. But given my lack of labor protections, and the options available before me, leaving 18F — withholding my labor — felt like my best and only option. I called a meeting with my supervisor, and gave two weeks’ notice.</p>
<p>In a terrible coda, a large number of <a href="https://www.npr.org/2025/02/13/nx-s1-5296928/layoffs-trump-doge-education-energy">probationary employees were summarily let go</a> at <a href="https://fedscoop.com/gsa-looks-to-terminate-probationary-employees/">my agency</a> just before my last day.</p>
<hr>
<p>Leaving was the right call for me, but I’ll never feel good about the decision. I mean, there’s the grief angle: up until about a month ago, I was working on projects that felt like they mattered, and working alongside people who cared about helping government services work better for the public. A few months ago, I would’ve told you I’d like to stay there for years, which is not something I’ve said about any other place I’ve ever worked. I am incredibly sad to leave this job.</p>
<p>And look, being able to leave is, flatly, a privileged option: I can’t not work forever, but I <em>can</em> not work for a little bit. Most of my coworkers didn’t have that option. Some had just bought a house; some returned from parental leave, only to learn they might be losing the jobs they’d counted on to support their families.</p>
<p>I’m also angry at what was taken from me. At what’s <em>being</em> taken from all of us. I’ve watched a wonderful job, a wonderful place to work, a wonderful <em>team</em> get pulled apart by rich men in ill-fitting suits, each of them parroting the same talking points around “realignment” and “right-sizing”.<sup><a href="#fn-datalake" id="fnref-fn-datalake">3</a></sup></p>
<p>But what’s happening right now is not about “government efficiency,” nor is it about “cost-cutting.” I would gently urge you to look at the net worth of the people who are telling you otherwise. After all, there is no financial analysis; no review of possible downsides, no weighing of potential negative impacts. There is no discussion of <em>what could happen if our math is wrong?</em> Or even more importantly, no consideration of <em>who might be harmed?</em></p>
<p>Instead, as <a href="https://www.anildash.com/">Anil Dash</a> predicted, the billionaire’s so-called “efficiency“ “department” is best understood as a sprawling form of <a href="https://www.anildash.com/2025/01/04/DOGE-procurement-capture/">procurement capture</a>, in which a group of impossibly rich individuals are trampling over the regulations — and the federal workers — that stand between them and a deep, deep <a href="https://newrepublic.com/article/191506/musk-bezos-pichai-zuckerberg-microsoft-trump-climate">revenue</a> <a href="https://www.technologyreview.com/2024/12/04/1107897/openais-new-defense-contract-completes-its-military-pivot/">stream</a>: <a href="https://www.wired.com/story/elon-musk-lieutenant-gsa-ai-agency/">your tax dollars</a>. And as they do, they’re making an explicitly fascist move to roll back rights for every marginalized community in the country — for anyone who doesn’t look like them, or who stands in their way.</p>
<p>So, yes. This is a wholesale attack on the American safety net, led by billionaires and far-right politicians who are frighteningly comfortable with fascism and autocracy. The last month has been called a coup by <a href="https://www.usatoday.com/story/news/politics/2025/02/03/dems-elon-musk-doge-takeover-treasury/78187978007/">politicians</a>, <a href="https://www.techpolicy.press/anatomy-of-an-ai-coup/">researchers</a>, and <a href="https://therevolvingdoorproject.org/tracking-the-doge-treasury-raid/">watchdogs</a> alike. I don’t want to diminish the harm these people will do — the harm they are doing. I also don’t want to downplay the terror of this moment, because lord knows I fucking feel it.</p>
<p>At the same time: what’s happening right now is <em>also</em> a labor story.</p>
<p>If the American government is slow-moving, it’s because rapid change is deadly when you’re talking about healthcare, social security checks, market regulations, food safety, or any of the other countless critical functions it performs. Those federal agencies are, quite simply, infrastructure. And as <a href="http://debcha.org/">Deb Chachra</a> showed in <a href="https://www.penguinrandomhouse.com/books/612711/how-infrastructure-works-by-deb-chachra/">her excellent book</a>, infrastructure is how a society invests in its future: in its ongoing economic, societal, and political stability.</p>
<p>In government, that infrastructure is built by laws, policies, and regulations. But regulations alone do not infrastructure make. Regulations require <em>workers</em> to become infrastructure: those workers who labor to understand new policies, how best to enact them, and then work to make them legible and understandable to the American public — and, yes, to enforce them. Without those federal workers, and their labor, these systems fall apart. And the architects of this assault on the federal workforce are keenly aware of that fact.</p>
<p>The last month has, flatly, been hell. But even so, I wouldn’t trade away my time at 18F for anything. It was a fantastic place to work, filled with genuine, hard-working people who cared for that work <em>and</em> for each other. Even when things got rough, I saw the leaders of 18F scramble to answer their team’s questions; I saw coworkers reaching out to support each other in countless little ways. All while ensuring they got their project work in on time. I saw something wonderful at work, <em>in</em> my work. I’m always going to be grateful for that, and to my coworkers.</p>
<hr>
<h2 id="resources" tabindex="-1">Resources</h2>
<ul>
<li>Wired has some <a href="https://www.wired.com/story/doge-tts-fired/">good coverage on the layoffs I described above</a>, and <a href="https://www.wired.com/tag/elon-musk/">on the billionaire coup more generally</a>.</li>
<li>The <a href="https://www.instagram.com/workingfamilies/p/DGLZz2CP9bH/">Working Families Party</a> and <a href="https://emilyinyourphone.substack.com/p/everything-you-need-to-know-about">Emily Amick</a> both had some great primers on what it means to call your members of Congress, if that’s a thing you’re able to do.</li>
<li>If you’re looking for other ways to get engaged, <a href="https://bsky.app/profile/prisonculture.bsky.social">Mariame Kaba</a> has pulled together a massive list of <a href="https://docs.google.com/document/d/1OSWxykA1WHOi0vTPLAJDaCeVhR3uSfh7PhlCj4t4yT0/edit?tab=t.0">actions that are not protesting or voting</a>.</li>
</ul>
<hr>

    </div><!-- /end .post-content -->

    

    
    <!-- /end .post-footer -->
    


  </article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Pi-hole v6 (495 pts)]]></title>
            <link>https://pi-hole.net/blog/2025/02/18/introducing-pi-hole-v6/</link>
            <guid>43093328</guid>
            <pubDate>Tue, 18 Feb 2025 18:31:36 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://pi-hole.net/blog/2025/02/18/introducing-pi-hole-v6/">https://pi-hole.net/blog/2025/02/18/introducing-pi-hole-v6/</a>, See on <a href="https://news.ycombinator.com/item?id=43093328">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
      <p><img width="1024" height="1024" src="https://wp-cdn.pi-hole.net/wp-content/uploads/2024/08/v6-png.avif" alt="" decoding="async" fetchpriority="high" srcset="https://wp-cdn.pi-hole.net/wp-content/uploads/2024/08/v6-png.avif 1024w, https://wp-cdn.pi-hole.net/wp-content/uploads/2024/08/v6-150x150.png 150w, https://wp-cdn.pi-hole.net/wp-content/uploads/2024/08/v6-300x300.png 300w, https://wp-cdn.pi-hole.net/wp-content/uploads/2024/08/v6-768x768.png 768w, https://wp-cdn.pi-hole.net/wp-content/uploads/2024/08/v6-100x100.png 100w" sizes="(max-width: 1024px) 100vw, 1024px" data-has-transparency="false" data-dominant-color="211f1f"></p>
<p>We’re excited to announce the general release of Pi-hole v6!</p>
<h3>At a glance: What’s New in Pi-hole v6?</h3>
<h4>1. <strong>Embedded Web Server and REST API</strong></h4>
<p>We’ve integrated a new REST API and embedded web server directly into the <code>pihole-FTL</code> binary. This eliminates the need for <code>lighttpd</code> and <code>PHP</code>, reducing the installation footprint and boosting performance. The new API also offers server-side pagination for the query log, ensuring a faster and more responsive interface.</p>
<p>As <code>lua</code> has been embedded into the <code>pihole-FTL</code> binary for <a href="https://github.com/pi-hole/FTL/pull/913">some time now</a>, we have been able to leverage this to rewrite the web interface.<strong><code></code></strong></p>
<h4>2. <strong>Advanced Filtering and Allowlists</strong></h4>
<p>Pi-hole v6 introduces support for subscribed allowlists (Otherwise known as “Antigravity”). These lists work in much the same way as blocklists, but they&nbsp;<em>allow&nbsp;</em>domains instead of&nbsp;<em>denying</em> them</p>
<h4>3. <strong>Consolidated Configuration Files</strong></h4>
<p>We’ve streamlined configuration management by consolidating multiple settings files into a single, richly commented <code>toml</code> file, making it easier to manage and understand your settings. If you are migrating from v5, your existing configurations will be migrated automatically into this file. It can be found at <code>/etc/pihole/pihole.toml</code></p>
<p>Configuration can be set in multiple ways:</p>
<ul>
<li>Directly editing the <code>toml</code> file</li>
<li>Via the command line, e.g <code>pihole-FTL --config dns.upstreams 8.8.8.8</code></li>
<li>Using the API</li>
<li>Via the web interface (which uses the API 😉)</li>
<li>Via environment variables named, e.g <code>FTLCONF_dns_upstreams=8.8.8.8</code></li>
</ul>
<p>If setting via environment variables, it should be noted that this effectively makes the setting read-only, as the environment variable will always force the value to match itself. This is the preferred way to configure FTL in the docker container.</p>
<h4>4. <strong>Redesigned User Interface</strong></h4>
<p>The web interface has been completely overhauled with settings split into Basic and Expert modes. This allows users to customize their experience based on their comfort level and needs.</p>
<h4>5. <strong>HTTPS Support</strong></h4>
<p>Pi-hole v6 includes native HTTPS support, with options to provide your own certificates or use auto-generated ones.</p>
<h4>6. <strong>Docker</strong></h4>
<p>Additionally, the Docker image is now based on Alpine, significantly reducing the image size and opening up possibilities for future system support.</p>
<h3>Upgrading and Getting Started</h3>
<h4>Bare Metal</h4>
<p>Upgrading to Pi-hole v6&nbsp; should be straightforward. For existing users, we recommend backing up your current configuration before proceeding, as the upgrade is strictly a one-way operation.</p>
<p>During the upgrade operation, you will be presented with a dialog box asking if you wish to disable <code>lighttpd</code>. Doing so is probably appropriate for most users – unless you are using it to host web pages&nbsp;<em>other than</em> Pi-hole’s, in which case you may choose to keep it enabled. With <code>lighttpd</code> disabled, <code>pihole-FTL</code> will attempt to bind to ports 80 for HTTP and 443 for HTTPS. If there is any conflict on these ports, then it will revert to port 8080 for HTTP.</p>
<h4>Docker</h4>
<p>The docker image has undergone a complete rewrite from the ground up, and is now based on Alpine rather than Debian.&nbsp;The same migration scripts that run on bare metal will also run on Docker – your configurations will be migrated to the new format.</p>
<p>The exception to this is environment variables. You can start the container with the old variables in place but don’t expect them to work! It is recommended to read the docker section of our <a href="https://docs.pi-hole.net/docker/">docs page</a> before upgrading.</p>
<h3>Join the Community</h3>
<p>Pi-hole thrives thanks to our vibrant and supportive community. Whether you’re looking to share your experience, get advice, or stay informed about the latest updates, there’s a place for you. Join the conversation on our <a href="https://discourse.pi-hole.net/" target="_new" rel="noreferrer noopener">official forum</a> or connect with fellow users on our <a href="https://www.reddit.com/r/pihole/" target="_new" rel="noreferrer noopener">subreddit</a>. We look forward to welcoming you!</p>
<h3>Thank You for Your Support</h3>
<p>We want to express our heartfelt thanks to everyone who has supported Pi-hole throughout the years.</p>
<p>Your community contributions and donations are the lifeblood of this project, allowing us to maintain and continually improve Pi-hole while keeping it free for everyone. If you’d like to contribute to our ongoing efforts, please consider donating through our official <a href="https://pi-hole.net/donate" target="_new" rel="noreferrer noopener">donation page</a>. Every contribution, big or small, makes a significant difference in helping us deliver the best project that we can.</p>
<p>Thank you for being part of the Pi-hole community!</p>
    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Among top researchers 10% publish at unrealistic levels, analysis finds (180 pts)]]></title>
            <link>https://www.chemistryworld.com/news/among-worlds-top-researchers-10-publish-at-unrealistic-levels-analysis-finds/4020962.article</link>
            <guid>43093155</guid>
            <pubDate>Tue, 18 Feb 2025 18:16:22 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.chemistryworld.com/news/among-worlds-top-researchers-10-publish-at-unrealistic-levels-analysis-finds/4020962.article">https://www.chemistryworld.com/news/among-worlds-top-researchers-10-publish-at-unrealistic-levels-analysis-finds/4020962.article</a>, See on <a href="https://news.ycombinator.com/item?id=43093155">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>About 10% of the most influential researchers worldwide in various scientific fields, including chemistry, are achieving ‘implausibly high’ publication and new co-author rates. Many are producing hundreds of papers each year and gaining hundreds to thousands of new collaborators annually, <a href="https://www.tandfonline.com/doi/full/10.1080/08989621.2024.2445280">according to recent analysis by researchers at King Fahd University of Petroleum and Minerals in Saudi Arabia</a>.</p>
<p>Geoscientists <a href="https://cpg.kfupm.edu.sa/bio/dr-peter-mora/">Peter Mora</a> and <a href="https://cpg.kfupm.edu.sa/bio/dr-simone-pilia/">Simone Pilia</a> found that an increasing number of researchers – about 20,000 out of the roughly 200,000 scientists on <a href="https://top2percentscientists.com/stanford-elsevier-top-scientists-list-2024/">Stanford’s ‘Top 2%’ researcher list</a> – are producing ‘anomalously high publication and co-authorship metrics’, indicating efforts to inflate their publication metrics. Roughly 1000 of these individuals have worked in academia for a decade or less, which<em> </em>indicates that these practices are emerging even at early career stages, the authors of the analysis noted. They argue that early-career researchers ‘exemplify the systemic incentive structures that encourage metric inflation across career stages’.</p>
<p>Mora and Pilia also examined the publication and co-authorship rates of 462 Nobel laureates in chemistry, physics, medicine and economics, and found similar results in terms of publication and new co-author rates, which tail off above threshold values of around 20 publications per year and 35 new co-authors annually.</p>
<p>Looking ahead, they argue that further research is needed to better understand the conditions where the quantity of research might affect its quality. However, they suggest that at ‘excessive rates’ these cases likely result from ‘paper pumping’ and low quality or unethical co-author practices such as co-author and citation networks. ‘In our opinion, it is difficult to believe that the majority of authors with excessive rates are able to consistently produce large quantities of high quality or groundbreaking research, with their input to each paper being substantial and within norms of what constitutes co-authorship,’ the researchers conclude. Mora and Pilia propose renormalising research metrics to remove the incentive for researchers to prioritise quantity or resort to unethical practices to boost their metrics.</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Try thinking and learning without working memory (2008) (151 pts)]]></title>
            <link>https://sharpbrains.com/blog/2008/05/25/try-thinking-and-learning-without-working-memory/</link>
            <guid>43092386</guid>
            <pubDate>Tue, 18 Feb 2025 17:21:31 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://sharpbrains.com/blog/2008/05/25/try-thinking-and-learning-without-working-memory/">https://sharpbrains.com/blog/2008/05/25/try-thinking-and-learning-without-working-memory/</a>, See on <a href="https://news.ycombinator.com/item?id=43092386">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-node="l0pa2oxb95cf">
		

<p>Imag­ine dial­ing a phone num­ber by hav­ing to look up each dig­it one at a time in the phone book. Nor­mal­ly, you look up the num­ber and remem­ber all sev­en dig­its long enough to get it dialed. Even with one dig­it at a time, you would have to remem­ber each dig­it long enough to get it dialed. What if your brain could not even do that! We call this kind of remem­ber­ing, “work­ing mem­o­ry,” because that is what the brain works with. Work­ing mem­o­ry is crit­i­cal to every­day living.</p>
<p><span id="more-1376"></span>Con­scious thought involves mov­ing a suc­ces­sion of items through what seems like a vir­tu­al scratch-pad. Think of it like stream­ing audio/video, where “thought bites” move on to the scratch pad where they are fed into a thought process and then moved off the scratch pad to make room for the next thought bite.</p>
<p>We think with what is in work­ing or “scratch pad” mem­o­ry. What we know, stored in reg­u­lar mem­o­ry, is brought onto the scratch pad in suc­ces­sive stages, each involv­ing sub­ject­ing the knowl­edge to analy­sis, inte­gra­tion into the cur­rent con­text, and cre­ative re-orga­ni­za­tion via our think­ing process­es (“thought engine”). The ani­mat­ed ver­sion of this graph­ic shows item 1 mov­ing on to the scratch pad and then sent on to the “thought engine.” This is fol­lowed by item 2, then 3,&nbsp;etc.</p>
<p>Con­scious think­ing thus requires the abil­i­ty to hold infor­ma­tion “on line” long enough to use it in think­ing. Con­scious thought thus seems to be a seri­al­ly ordered process of mov­ing thought bites on to and off of the scratch pad.<img loading="lazy" decoding="async" id="image1392" src="https://sharpbrains.com/wp-content/uploads/2008/06/howwethinnk1allnumbers.jpg" alt="working memory thinking" width="453" height="292"></p>
<p><strong>Uncon­scious Thinking<br>
</strong></p>
<p>What about uncon­scious thought … the kind that occurs when you are not pay­ing atten­tion? We know that the sub­con­scious mind is pro­cess­ing infor­ma­tion (i.e. “think­ing”) all the time, even while we sleep. The evi­dence for this kind of “sleep learn­ing” is incon­tro­vert­ible and sum­ma­rized in my mem­o­ry improve­ment book (see http://thankyoubrain.com). Sub­con­scious think­ing and its relat­ed mem­o­ries may not involve a scratch pad of work­ing mem­o­ry. Sub­con­scious think­ing could occur as mul­ti­ple par­al­lel process­es and may be more non-lin­ear than con­scious thought. How­ev­er, in the case of dream sleep, which I regard as a form of con­scious­ness, those dreams that I hap­pen to remem­ber do seem to be based on seri­al­ly ordered “thought bites.”</p>
<p>A recent study, not explic­it­ly con­cern­ing mem­o­ry, sheds some impor­tant light both on how we think and on the role of work­ing mem­o­ry in thought. In this study, the researchers exam­ined how peo­ple make a cor­rect choice. Researchers com­pared the qual­i­ty of deci­sions formed from con­scious ver­sus uncon­scious think­ing with that result­ing from uncon­scious think­ing. Here is how they stud­ied this issue. In one study, sub­jects were giv­en infor­ma­tion about the attrib­ut­es of four hypo­thet­i­cal cars, and they were to decide which was the best car, based on the attrib­ut­es assigned to each car. Analy­sis con­di­tions were either sim­ple (based on only four attrib­ut­es) or com­plex (based on 12 attrib­ut­es). After read­ing about the attrib­ut­es, sub­jects were assigned to one of two groups: con­scious analy­sis or to an uncon­scious thought con­di­tion. In the con­scious con­di­tion, they thought about the attrib­ut­es for four min­utes before mak­ing a choice. In the uncon­scious con­di­tion, sub­jects were told they would have to make a choice in four min­utes, but they were dis­tract­ed dur­ing that time by being required to solve anagrams.</p>
<p>Their “think­ing” about the prob­lem was thus not allowed to be conscious.</p>
<p>Not sur­pris­ing­ly, when only four attrib­ut­es were involved, sub­jects in the con­scious-thought con­di­tion made the best choice of car. But when the com­plex con­di­tion of 12 attrib­ut­es, results reversed. The best car was cho­sen most reli­ably in the uncon­scious-thought condition.</p>
<p>In a sec­ond study, one change was made. Instead of choos­ing the best car, sub­jects were asked about their atti­tudes toward the four cars. Again, con­scious thinkers made the clear­est dis­tinc­tions among the cars when only four attrib­ut­es were con­sid­ered, but the oppo­site occurred when 12 attrib­ut­es had to be considered.</p>
<p>In anoth­er exper­i­ment, two stores were select­ed, one that sold com­pli­cat­ed items like fur­ni­ture and the oth­er a depart­ment store that sold sim­ple prod­ucts. As peo­ple left the store, peo­ple were asked ques­tions about what they bought, why they bought it, how cost­ly was it, and how much they thought about mak­ing the choice. The buy­ers were cat­e­go­rized as either “thinkers” (those who spent a lot of time con­scious­ly mak­ing a deci­sion) and “impulse buy­ers” (who did not spend much time con­scious­ly think­ing about their choice). Sev­er­al weeks lat­er, these same peo­ple were called to check on how sat­is­fied they were with the pur­chase. As expect­ed, more post-choice sat­is­fac­tion was found in the con­scious thinker group, but only for the sim­ple items in the depart­ment store. For the com­plex choic­es in the fur­ni­ture store, the uncon­scious thinkers expressed the most sat­is­fac­tion with their purchases.</p>
<p>What all this says is that sim­ple deci­sions are best made by care­ful con­scious thought. But for com­pli­cat­ed deci­sions, the best choic­es may result from “delib­er­a­tion with­out pay­ing atten­tion,” that is let­ting the think­ing be done by the uncon­scious mind. I inter­pret these results to reflect the depen­dence of con­scious thought on scratch-pad mem­o­ry and the rel­a­tive inde­pen­dence of sub­con­scious thought on scratch-pad mem­o­ry. Con­scious thought is very effec­tive as long as it can work on infor­ma­tion that it can hold on-line in work­ing mem­o­ry. But work­ing mem­o­ry has lim­it­ed capac­i­ty. There­fore it can­not be very effec­tive when the amount of infor­ma­tion need­ed for high-qual­i­ty thought exceeds the car­ry­ing capac­i­ty of work­ing memory.</p>
<p>The corol­lary of this new evi­dence about work­ing mem­o­ry and think­ing process­es is that if we had a big­ger work­ing mem­o­ry, we might think better.</p>
<p><strong>Work­ing Mem­o­ry Load Affects Pay­ing Attention<br>
</strong></p>
<p>Pay­ing atten­tion is pre-req­ui­site to learn­ing. The abil­i­ty to pay atten­tion seems to be affect­ed by how much infor­ma­tion (load) is being car­ried in work­ing mem­o­ry. These prin­ci­ples have been elu­ci­dat­ed in human exper­i­ments that test­ed the assump­tion that attend­ing to rel­e­vant details in a learn­ing sit­u­a­tion requires that the details be held in work­ing mem­o­ry. Hav­ing oth­er, non-rel­e­vant, infor­ma­tion in work­ing mem­o­ry at the same time serves as a dis­trac­tion, low­er­ing atten­tion and inter­fer­ing with mem­o­ry formation.</p>
<p>In this exper­i­ment, par­tic­i­pants per­formed an atten­tion task that required them to ignore pic­tures of dis­tracter faces while hold­ing in work­ing mem­o­ry a string of dig­its that were in the same order (low mem­o­ry load) or dif­fer­ent order (high mem­o­ry order) on every tri­al. The test thus was one of mul­ti-task­ing, one task being hold­ing the dig­its in work­ing mem­o­ry and the oth­er task being iden­ti­fy­ing whether a name flashed on the screen was that of a famous politi­cian or a pop star, while a con­tra­dic­to­ry face was pro­ject­ed. For exam­ple, the name Mick Jag­ger would have the face of Bill Clin­ton super­im­posed, and the task was to know that Mick Jag­ger is a pop star, not a politician.</p>
<p>The atten­tion per­for­mance degrad­ed severe­ly with high work­ing-mem­o­ry load. That is, the dis­tract­ing faces cre­at­ed con­fu­sion when sub­jects were also required to hold mixed-order dig­its in work­ing mem­o­ry at the same&nbsp;time.</p>
<p>The point is sim­ple. It is hard to think about two com­pli­cat­ed things at once. The grow­ing trend, espe­cial­ly among young peo­ple, to mul­ti-task may seem won­der­ful. But actu­al­ly, mul­ti-task­ing is most like­ly to inter­fere with focused atten­tion and, in turn, degrade mem­o­ry for­ma­tion, recall, and think­ing quality.</p>
<p><strong>Train­ing Work­ing Mem­o­ry and&nbsp;IQ<br>
</strong></p>
<p>Stud­ies have shown that it is pos­si­ble to train ADHD chil­dren to have bet­ter work­ing mem­o­ries. This led researchers in Japan to try to devel­op a sim­ple work­ing mem­o­ry train­ing method and to test whether this method can increase the work­ing mem­o­ry capac­i­ty and whether this has any effect on a child’s IQ. Chil­dren ages 6–8 were trained 10 min­utes a day each day for two months. The train­ing task to expand work­ing mem­o­ry capac­i­ty con­sist­ed of pre­sent­ing a dig­it or a word item for a sec­ond, with one-sec­ond inter­vals between items. For exam­ple, a sequence might be 5, 8, 4, 7, with one-sec­ond inter­vals between each dig­it. Test for recall could take the form of “Where in the sequence was the 4?” or “What was the third item?” Thus stu­dents had to prac­tice hold­ing the item sequence in work­ing mem­o­ry. With prac­tice, the train­ers increased the num­ber of items from 3 to&nbsp;8.</p>
<p>After train­ing, researchers test­ed the chil­dren on anoth­er work­ing mem­o­ry task. Scores on this test indi­cat­ed that work­ing mem­o­ry cor­re­lat­ed with IQ test scores. That is, chil­dren with bet­ter work­ing mem­o­ry abil­i­ty also had high­er IQs. When first graders were test­ed for intel­li­gence, the data showed that intel­li­gence scores increased dur­ing the year by 6% in con­trols, but increased by 9% in the group that had been giv­en the mem­o­ry train­ing. The mem­o­ry train­ing effect was even more evi­dent in the sec­ond graders, with a 12% gain in intel­li­gence score in the mem­o­ry trained group, com­pared with a 6% gain in con­trols. As might be expect­ed, the low­er IQ chil­dren showed the great­est gain from mem­o­ry training.</p>
<p>So in con­clu­sion, it seems that work­ing mem­o­ry capac­i­ty can be increased by train­ing and that such train­ing can even raise IQ, at least in young children.</p>
<p><strong>Ben­e­fits of Increas­ing Work­ing Memory</strong></p>
<p>Accu­mu­lat­ing evi­dence seems to indi­cate that work­ing mem­o­ry, with prop­er train­ing, can be improved in any­one, even adults. I recent­ly found a research report in which last­ing improve­ments in brain func­tion were pro­duced in healthy adults by only five weeks of prac­tice on three work­ing-mem­o­ry tasks that involved the loca­tion of objects in space. Sub­jects per­formed 90 tri­als per day on a train­ing reg­i­men (CogMed). MRI scans showed increased activ­i­ty in the cor­ti­cal areas that were involved in pro­cess­ing the visu­al stim­uli. Brain activ­i­ty increas­es in these areas appeared with­in the first week and grew over&nbsp;time.</p>
<p>Sim­i­lar results have been report­ed by oth­er inves­ti­ga­tors. In a few cas­es, where dif­fer­ent kinds of stim­uli were used, mem­o­ry train­ing induced a decrease of brain activ­i­ty in cer­tain areas, which is inter­pret­ed to indi­cate that the trained brain did not have to work as hard. While we clear­ly don’t under­stand things very well, it seems clear that work­ing mem­o­ry train­ing not only improves mem­o­ry capa­bil­i­ty but also caus­es last­ing changes in the&nbsp;brain.</p>
<p><strong>Help Your Work­ing-Mem­o­ry Capacity<br>
</strong></p>
<p>I just read a fas­ci­nat­ing book on increas­ing teacher aware­ness of the impor­tance of work­ing-mem­o­ry capac­i­ty for teach­ing and learn­ing strate­gies. Many young­sters have work­ing mem­o­ry lim­i­ta­tions, and they usu­al­ly do not grow out of them. This is a major and seri­ous cause of low grades, poor learn­ing skills, poor con­fi­dence, and life-long dimin­ished moti­va­tion to&nbsp;learn.</p>
<p>Lim­it­ed work­ing-mem­o­ry capac­i­ty impairs the abil­i­ty to think and solve prob­lems. I was told once by a mid­dle-school teacher that her “spe­cial needs” stu­dents could do the same math as reg­u­lar stu­dents, but they just can’t remem­ber all the steps. This clear­ly reflects a lim­it­ed work­ing-mem­o­ry capac­i­ty. If the demands made on work­ing mem­o­ry could be less­ened, bet­ter think­ing could result.</p>
<p>Cer­tain strate­gies can help to reduce the load on work­ing mem­o­ry. Teach­ers should mod­el and stu­dents should employ the fol­low­ing devices:</p>
<ul>
<li>Pro­vide help, cues, mnemon­ics, reminders.</li>
<li>KISS (Keep It Sim­ple, Stupid!)(example: use short, sim­ple sen­tences, present much of the instruc­tion as pictures/diagrams).</li>
<li>Don’t present so much infor­ma­tion. Less can be&nbsp;more.</li>
<li>Facil­i­tate rehearsal, using only rel­e­vant infor­ma­tion and no distractors.</li>
<li>Get engaged, by tak­ing notes, and cre­at­ing dia­grams and con­cept&nbsp;maps.</li>
<li>Attach mean­ing from what is already known. (The more you know, the more you can&nbsp;know).</li>
<li>Orga­nize infor­ma­tion in small categories.</li>
<li>Break down tasks into small chunks. Mas­ter each chunk sequen­tial­ly, one at a&nbsp;time.</li>
</ul>
<p>Doing these things not only helps the think­ing process, but will also pro­mote the for­ma­tion of last­ing mem­o­ries. The process of con­vert­ing work­ing mem­o­ry into per­ma­nent form is called con­sol­i­da­tion, and I will explain that next&nbsp;time.</p>
<p><img decoding="async" id="image1334" src="https://sharpbrains.com/wp-content/uploads/2008/04/klemm12001_001.thumbnail.jpg" alt="Bill Klemm" height="96">— W. R. (Bill) Klemm, D.V.M., Ph.D. Sci­en­tist, pro­fes­sor, author, speak­er As a pro­fes­sor of Neu­ro­science at Texas A<span>&amp;</span>M Uni­ver­si­ty, Bill has taught about the brain and behav­ior at all lev­els, from fresh­men, to seniors, to grad­u­ate stu­dents to post-docs. His recent books include <em><a href="http://thankyoubrain.com/" target="_blank"><strong><span>Thank You Brain For All You Remem­ber</span></strong></a></em> and <em><a href="http://neurosciideas.com/" target="_blank"><strong><span>Core Ideas in Neu­ro­science</span></strong></a>.</em></p>
<p>Relat­ed arti­cles on Work­ing Mem­o­ry Training</p>
<blockquote><p>- <a title="Permanent Link to Can Intelligence Be Trained? Martin Buschkuehl shows how" href="https://sharpbrains.com/blog/2008/05/13/can-intelligence-be-trained-martin-buschkuehl-shows-how/" rel="bookmark"><span>Can Intel­li­gence Be Trained? Mar­tin Buschkuehl shows&nbsp;how</span></a></p>
<p>- <a title="Permanent Link to Working Memory Training and RoboMemo: Interview with Dr. Torkel Klingberg" href="https://sharpbrains.com/blog/2006/09/25/working-memory-training-and-robomemo-interview-with-dr-torkel-klingberg/" rel="bookmark"><span>Work­ing Mem­o­ry Train­ing: Inter­view with Dr. Torkel Klingberg</span></a></p>
<p>- <a title="Permanent Link to Working Memory Training for Adults" href="https://sharpbrains.com/blog/2008/04/22/working-memory-training-for-adults/" rel="bookmark"><span>Work­ing Mem­o­ry Train­ing for Adults</span></a></p></blockquote>
<p>Sources</p>
<blockquote><p>1. Repovs, G and Bres­janac, M. 2006. Cog­ni­tive neu­ro­science of work­ing mem­o­ry: a pro­logue. Neu­ro­science. 139: 1–3.</p>
<p>2. Dijk­ster­huis, A. et al. 2006. On mak­ing the right choice: the delib­er­a­tion-with­out-atten­tion effect. Sci­ence. 311: 1005–1007.</p>
<p>3. Waji­ma, Kayo, and Sawaguchi, T. 2005. The effect of work­ing mem­o­ry train­ing on gen­er­al intel­li­gence in chil­dren. Soci­ety for Neu­ro­science Abstracts. Abstract 772.11.</p>
<p>4. de Fock­ert, J. W. et al. 2001. The role of work­ing mem­o­ry in visu­al selec­tive atten­tion. Sci­ence. 291: 1803–1806.</p>
<p>5. Ole­sen, P. J., West­er­berg, H., and King­berg, T. 2004. Increased pre­frontal and pari­etal activ­i­ty after train­ing of work­ing mem­o­ry. Nature Neu­ro­science. 7: 75–79.</p>
<p>6. Gath­er­cole, Susan E., and Alloway, Tra­cy P. 2008. Work­ing Mem­o­ry and Learn­ing. Sage Pub­li­ca­tions, 124&nbsp;pages.</p>
<p>7. Gath­er­cole, Susan E., and Alloway, Tra­cy P. 2008. Work­ing mem­o­ry and learn­ing. Sage Pub­li­ca­tions, . 124&nbsp;pages.</p></blockquote>
	</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[One year after switching from Java to Go (225 pts)]]></title>
            <link>https://glasskube.dev/blog/from-java-to-go/</link>
            <guid>43092003</guid>
            <pubDate>Tue, 18 Feb 2025 16:55:22 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://glasskube.dev/blog/from-java-to-go/">https://glasskube.dev/blog/from-java-to-go/</a>, See on <a href="https://news.ycombinator.com/item?id=43092003">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="__blog-post-container"><blockquote>
<p>I always told people memory is cheap, black magic is OK and efficiency doesn't matter in most cases, but boy, how wrong was I...</p>
</blockquote>
<p>My Java journey started back in 2011 (14 years ago - wow) when I started studying computer science at the Vienna University of Technology.
Using editors like <a href="https://www.jedit.org/" target="_blank" rel="noopener noreferrer">jEdit</a> and compiling my Java programs by hand with the command line.
My first "major" applications were Java Swing GUI applications and "old school" web applications with JSP and Servlets.</p>
<p>Professionally, I started writing Java code in 2016 when I joined a company that was developing a Java-based web application.
It used the classic stack: Java, Spring Boot, Hibernate, and a PostgreSQL database.
I always loved all the Spring features like Dependency Injection, Spring Security, and Spring Data JPA.
Yes, the application took half a minute to boot up and consumed hundreds of megabytes of RAM instantly, but who cares? Memory is cheap, right?</p>
<p>Over the next few years, I always picked Java (and later on Kotlin) for new projects due to the familiarity and the vast ecosystem,
and being afraid of slower development speed in other languages like Go or Rust due to the initial learning effort.
We even went so far as writing infrastructure code for Kubernetes clusters that
<a href="https://github.com/glasskube/operator" target="_blank" rel="noopener noreferrer">automatically provision apps in Kotlin</a>.
Although it is still used in production, I wish we had written it in Go from the beginning.</p>
<p>In hindsight, this decision seems to be obvious, but having already invested so much time and experience, the decision wasn't so clear
back then. As a rule of thumb to ensure productivity and efficiency, I like to follow this credo.</p>
<blockquote>
<p><strong>Either do a project in an unfamiliar domain or with an unfamiliar tech stack.</strong></p>
</blockquote>
<p>As this was our first project in the Kubernetes domain, we didn't want to also introduce execution risk in addition to
market/product risk.</p>
<p>But it became pretty clear early on why most of the current tooling is written in Go. Other operators barely consumed any memory or CPU.
Our operator, with additional tooling, and our stack consumes way over 2 GBs of RAM even when idle.</p>
<p>In 2024, we decided to rewrite our first Kotlin-based "package operator" into a more generic and extensible package manager.
Being already familiar with the domain, the Kubernetes API, and the operator pattern, we chose Go as the programming language.</p>
<p>After an actually not-so-steep learning curve, we were able to accelerate faster and faster and nearly match the development
speed we would have had writing Kotlin.</p>
<p>As a company that is constantly evolving and launching new tools in the DevTool space, we decided in late 2024 to launch
Distr – An Open Source Software Distribution platform, helping software vendors deploy into customer-controlled environments.</p>
<p>As we had familiarized ourselves with the Go stack and had experience in the software distribution domain,
we decided to choose Golang for Distr, which will be our first web application in Go.</p>
<p>The first couple of times I started the web server, I waited a few seconds until it would "boot up"
because I didn't see any new logs, but I realized it was already running and ready to accept connections.</p>
<h2 id="compile--startup-time">Compile &amp; Startup Time<a href="#compile--startup-time" aria-label="Direct link to Compile &amp; Startup Time" title="Direct link to Compile &amp; Startup Time">​</a></h2>
<h3 id="compilation">Compilation<a href="#compilation" aria-label="Direct link to Compilation" title="Direct link to Compilation">​</a></h3>
<p>Java's JIT (Just-In-Time) compiler and Go's AOT (Ahead-Of-Time) compiler are definitely two very different approaches
and therefore hard to compare.
Where Java supports incremental builds hot code reloading which also allow for faster startup times in development.
But this comes at a cost of using Gradle / Maven which definitely have an appetite for memory themselves and are
sadly still far from providing an excellent developer experience.</p>
<p>Go, on the other hand, compiles the whole application into a single binary, which requires a full rebuild on every change.</p>
<h3 id="startup-time">Startup Time<a href="#startup-time" aria-label="Direct link to Startup Time" title="Direct link to Startup Time">​</a></h3>
<p>Starting a real-world but still super light Spring Boot application outputs round about the following:</p>
<div><pre tabindex="0"><code><span><span>...Started CoreApplicationKt in 8.201 seconds (process running for 8.726)</span><br></span></code></pre></div>
<p>Where the Go web server is ready to accept connections in <strong>less than 100 milliseconds</strong>.</p>
<p>In the course of a developer's years, if I only restart the server two times an hour,
this saves me an additional day (!) of development time per year.</p>
<p>This can also get critical in a scenario where all replicas are scaled down to zero and need to be started up again
to accept requests as quickly as possible.</p>
<p>I mean, who doesn't like speed? I sure do. 😎</p>
<h2 id="frameworks-and-libraries">Frameworks and Libraries<a href="#frameworks-and-libraries" aria-label="Direct link to Frameworks and Libraries" title="Direct link to Frameworks and Libraries">​</a></h2>
<p>In the Java ecosystem, most frameworks exist covering a whole part of the stack from the web server to the
database layer.</p>
<p>Go is different, having smaller libraries, for example, web server, router, database, which don't necessarily
belong together but play well together, giving you the ability to pick the libraries you like instead of using
an all-in-one solution like Spring or Quarkus. But with the landscape being more fragmented, you will see projects
differ more than your traditional Java EE or Spring Boot application.</p>
<h2 id="dependency-injection--context">Dependency Injection &amp; Context<a href="#dependency-injection--context" aria-label="Direct link to Dependency Injection &amp; Context" title="Direct link to Dependency Injection &amp; Context">​</a></h2>
<p>Coming from Spring Framework, I was used to annotating my services with <code>@Service</code>, and I could happily use them in
other services. This is also how Angular works. But Go is different.</p>
<p>Although singleton services even exist in the standard library e.g. <code>http.DefaultClient</code> or <code>base64.StdEncoding</code> dependency
injection is not a real thing as runtime reflections is much more limited in Go when compared to Java.</p>
<p>But there are obviously work around solutions in the Go ecosystem.
It uses the <code>Context ctx</code>, which we pass around functions in order to juggle data around in the application.</p>
<p>Having less <em>black magic</em> around dependency injection is definitely a general theme in Go and probably also a main reason
for its popularity.</p>
<h2 id="debugging--ide-support">Debugging &amp; IDE Support<a href="#debugging--ide-support" aria-label="Direct link to Debugging &amp; IDE Support" title="Direct link to Debugging &amp; IDE Support">​</a></h2>
<p>In the beginning, I was very skeptical about the debugging capabilities of Go-based applications, but debugging applications,
setting breakpoints, and evaluating expressions is as convenient as with applications that run in the JVM.</p>
<p>So, IDE support is definitely comparable.</p>
<p>But when it comes to exceptions and stack traces, I've had better experiences with the Java ecosystem. I personally
think that IDE support (at least in IntelliJ) is slightly superior, as it hides traces from frameworks and
creates hyperlinks to the source files inside the scope, where you can quickly browse into them. I haven't seen IntelliJ
doing this with Go.</p>
<blockquote>
<p>Debugging and IDE support, which are crucial for developer experience, are mostly on par, but from my experiences,
stack traces being more tightly coupled to your sources in the Java ecosystem are better.</p>
</blockquote>

<p>Releasing is always related to dependency management and application bundling. In the Java world, there are Gradle
and Maven, with Maven being the established player in the field and Gradle the "newer" version. Although the Java
ecosystem is already that old, there are basically no new tools around.</p>
<p>In the Go ecosystem, I think that there are fewer libraries that do the same thing. There is often one good solution
for dependency management, and GoReleaser is a great tool.</p>
<h2 id="summary">Summary<a href="#summary" aria-label="Direct link to Summary" title="Direct link to Summary">​</a></h2>
<p>Looking back, switching from Java/Kotlin to Go felt like a big step at first, but in hindsight, it was the obvious choice.
The initial learning curve wasn’t as steep as expected, and the benefits—blazing-fast startup times, lower resource consumption,
and a more lightweight ecosystem—became apparent pretty quickly.</p>
<p>Of course, Java still has its strengths, and for certain projects, it remains a solid choice. But for cloud-native applications,
Kubernetes tooling, and our self-hostable software distribution platform, Go just feels like the right tool for the job.</p>
<p>Will I ever write Java again? Probably. But for now, I’m enjoying the speed, simplicity, and flexibility of Go.</p>
<p>And honestly, I don’t miss all of Java's and Spring's black magic under the hood. 😄</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Svelte 5 is not JavaScript (178 pts)]]></title>
            <link>https://hodlbod.npub.pro/post/1739830562159/</link>
            <guid>43091596</guid>
            <pubDate>Tue, 18 Feb 2025 16:29:46 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://hodlbod.npub.pro/post/1739830562159/">https://hodlbod.npub.pro/post/1739830562159/</a>, See on <a href="https://news.ycombinator.com/item?id=43091596">Hacker News</a></p>
<div id="readability-page-1" class="page"><div event="naddr1qqxnzden8yurxvp4xcerzdfeqgsf03c2gsmx5ef4c9zmxvlew04gdh7u94afnknp33qvv3c94kvwxgsrqsqqqa28ae4fzd"><p>For the last couple of weeks, I've been dealing with the fallout of upgrading a web application to Svelte 5. Complaints about framework churn and migration annoyances aside, I've run into some interesting issues with the migration. So far, I haven't seen many other people register the same issues, so I thought it might be constructive for me to articulate them myself.</p>
    <p>I'll try not to complain too much in this post, since I'm grateful for the many years of Svelte 3/4 I've enjoyed. But I don't think I'll be choosing Svelte for any new projects going forward. I hope my reflections here will be useful to others as well.</p>
    <p>If you're interested in reproductions for the issues I mention here, you can find them below.</p>
    <ul>
    <li><a href="https://github.com/sveltejs/svelte/issues/15327">Can't save state to indexeddb</a></li>
    <li><a href="https://github.com/sveltejs/svelte/issues/15327">Component unmount results in undefined variables in closures</a></li>
    </ul>
    <h2>The Need for Speed</h2>
    <p>To start with, let me just quickly acknowledge what the Svelte team is trying to do. It seems like most of the substantial changes in version 5 are built around "deep reactivity", which allows for more granular reactivity, leading to better performance. Performance is good, and the Svelte team has always excelled at reconciling performance with DX.</p>
    <p>In previous versions of Svelte, the main way this was achieved was with the Svelte compiler. There were many ancillary techniques involved in improving performance, but having a framework compile step gave the Svelte team a lot of leeway for rearranging things under the hood without making developers learn new concepts. This is what made Svelte so original in the beginning.</p>
    <p>At the same time, it resulted in an even more opaque framework than usual, making it harder for developers to debug more complex issues. To make matters worse, the compiler had bugs, resulting in errors which could only be fixed by blindly refactoring the problem component. This happened to me personally at least half a dozen times, and is what ultimately pushed me to migrate to Svelte 5.</p>
    <p>Nevertheless, I always felt it was an acceptable trade-off for speed and productivity. Sure, sometimes I had to delete my project and port it to a fresh repository every so often, but the framework was truly a pleasure to use.</p>
    
    <p>Svelte 5 doubled down on this tradeoff — which makes sense, because it's what sets the framework apart. The difference this time is that the abstraction/performance tradeoff did not stay in compiler land, but intruded into runtime in two important ways:</p>
    <ul>
    <li>The use of proxies to support deep reactivity</li>
    <li>Implicit component lifecycle state</li>
    </ul>
    <p>Both of these changes improved performance <em>and</em> made the API for developers look slicker. What's not to like? Unfortunately, both of these features are classic examples of a <a href="https://www.joelonsoftware.com/2002/11/11/the-law-of-leaky-abstractions/">leaky abstraction</a>, and ultimately make things <em>more</em> complex for developers, not less.</p>
    <h2>Proxies are not objects</h2>
    <p>The use of proxies seems to have allowed the Svelte team to squeeze a little more performance out of the framework, without asking developers to do any extra work. Threading state through multiple levels of components without provoking unnecessary re-renders in frameworks like React is an infamously difficult chore.</p>
    <p>Svelte's compiler avoided some of the pitfalls associated with virtual DOM diffing solutions, but evidently there was still enough of a performance gain to be had to justify the introduction of proxies. The Svelte team also <a href="https://svelte.dev/blog/runes">seems to argue</a> that their introduction represents an improvement in developer experience:</p>
    <blockquote>
    <p>we... can maximise both efficiency and ergonomics.</p>
    </blockquote>
    <p>Here's the problem: Svelte 5 <em>looks</em> simpler, but actually introduces <em>more</em> abstractions.</p>
    <p>Using proxies to monitor array methods (for example) is appealing because it allows developers to forget all the goofy heuristics involved with making sure state was reactive and just <code>push</code> to the array. I can't count how many times I've written <code>value = value</code> to trigger reactivity in svelte 4.</p>
    <p>In Svelte 4, developers had to understand how the Svelte compiler worked. The compiler, being a leaky abstraction, forced its users to know that assignment was how you signaled reactivity. In svelte 5, developers can just "forget" about the compiler!</p>
    <p>Except they can't. All the introduction of new abstractions really accomplishes is the introduction of more complex heuristics that developers have to keep in their heads in order to get the compiler to act the way they want it to.</p>
    <p>In fact, this is why after years of using Svelte, I found myself using Svelte stores more and more often, and reactive declarations less. The reason being that Svelte stores are <em>just javascript</em>. Calling <code>update</code> on a store is <em>simple</em>, and being able to reference them with a <code>$</code> was just a nice bonus — nothing to remember, and if I mess up the compiler yells at me.</p>
    <p>Proxies introduce a similar problem to reactive declarations, which is that they look like one thing but act like another on the edges.</p>
    <p>When I started using Svelte 5, everything worked great — until <a href="https://github.com/sveltejs/svelte/issues/15327">I tried to save a proxy to indexeddb</a>, at which point I got a <code>DataCloneError</code>. To make matters worse, it's impossible to reliably tell if something is a <code>Proxy</code> without <code>try/catch</code>ing a structured clone, which is a performance-intensive operation.</p>
    <p>This forces the developer to remember what is and what isn't a Proxy, calling <code>$state.snapshot</code> every time they pass a proxy to a context that doesn't expect or know about them. This obviates all the nice abstractions they gave us in the first place.</p>
    <h2>Components are not functions</h2>
    <p>The reason virtual DOM took off way back in 2013 was the ability to model your application as composed functions, each of which takes data and spits out HTML. Svelte retained this paradigm, using a compiler to sidestep the inefficiencies of virtual DOM and the complexities of lifecycle methods.</p>
    <p>In Svelte 5, component lifecycles are back, react-hooks style.</p>
    <p>In React, hooks are an abstraction that allows developers to avoid writing all the stateful code associated with component lifecycle methods. Modern React tutorials universally recommend using hooks instead, which rely on the framework invisibly synchronizing state with the render tree.</p>
    <p>While this does result in cleaner code, it also requires developers to tread carefully to avoid breaking the assumptions surrounding hooks. Just try accessing state in a <code>setTimeout</code> and you'll see what I mean.</p>
    <p>Svelte 4 had a few gotchas like this — for example, async code that interacts with a component's DOM elements has to keep track of whether the component is unmounted. This is pretty similar to the kind of pattern you'd see in old React components that relied on lifecycle methods.</p>
    <p>It seems to me that Svelte 5 has gone the React 16 route by adding implicit state related to component lifecycles in order to coordinate state changes and effects.</p>
    <p>For example, here is an excerpt from the documentation for <a href="https://svelte.dev/docs/svelte/$effect">$effect</a>:</p>
    <blockquote>
    <p>You can place $effect anywhere, not just at the top level of a component, as long as it is called during component initialization (or while a parent effect is active). It is then tied to the lifecycle of the component (or parent effect) and will therefore destroy itself when the component unmounts (or the parent effect is destroyed).</p>
    </blockquote>
    <p>That's very complex! In order to use <code>$effect</code>... effectively (sorry), developers have to understand how state changes are tracked. The <a href="https://svelte.dev/docs/svelte/lifecycle-hooks">documentation for component lifecycles</a> claims:</p>
    <blockquote>
    <p>In Svelte 5, the component lifecycle consists of only two parts: Its creation and its destruction. Everything in-between — when certain state is updated — is not related to the component as a whole; only the parts that need to react to the state change are notified. This is because under the hood the smallest unit of change is actually not a component, it’s the (render) effects that the component sets up upon component initialization. Consequently, there’s no such thing as a “before update”/"after update” hook.</p>
    </blockquote>
    <p>But then goes on to introduce the idea of <code>tick</code> in conjunction with <code>$effect.pre</code>. This section explains that "<code>tick</code> returns a promise that resolves once any pending state changes have been applied, or in the next microtask if there are none."</p>
    <p>I'm sure there's some mental model that justifies this, but I don't think the claim that a component's lifecycle is only comprised of mount/unmount is really helpful when an addendum about state changes has to come right afterward.</p>
    <p>The place where this really bit me, and which is the motivation for this blog post, is when state gets coupled to a component's lifecycle, even when the state is passed to another function that doesn't know anything about svelte.</p>
    <p>In my application, I manage modal dialogs by storing the component I want to render alongside its props in a store and rendering it in the <code>layout.svelte</code> of my application. This store is also synchronized with browser history so that the back button works to close them. Sometimes, it's useful to pass a callback to one of these modals, binding caller-specific functionality to the child component:</p>
    <pre><code>const {value} = $props()
    const callback = () =&gt; console.log(value)
    const openModal = () =&gt; pushModal(MyModal, {callback})
    </code></pre>
    <p>This is a fundamental pattern in javascript. Passing a callback is just one of those things you do.</p>
    <p>Unfortunately, if the above code lives in a modal dialog itself, the caller component gets unmounted before the callback gets called. In Svelte 4, this worked fine, but in Svelte 5 <code>value</code> gets updated to <code>undefined</code> when the component gets unmounted. <a href="https://github.com/sveltejs/svelte/issues/15325">Here's a minimal reproduction</a>.</p>
    <p>This is only one example, but it seems clear to me that <em>any</em> prop that is closed over by a callback function that lives longer than its component will be undefined when I want to use it —&nbsp;with no reassignment existing in lexical scope.</p>
    <p>This just isn't how javascript works. And I think the reason Svelte works this way is that it is attempting to re-invent garbage collection. Because <code>value</code> is a prop of the component, it apparently has to be cleaned up at the end of the component's lifecycle. I'm sure there's a good engineering reason for this, but it is surprising.</p>
    <h2>Conclusion</h2>
    <p>Easy things are nice, but as Rich Hickey says, <a href="https://www.infoq.com/presentations/Simple-Made-Easy/">easy things are not always simple</a>. And like Joel Spolsky, I don't like being surprised. Svelte has always been full of magic, but with the latest release I think the cognitive overhead of reciting incantations has finally outweighed the power it confers.</p>
    <p>My point in this post is not to dunk on the Svelte team. I know lots of people like Svelte 5 (and react hooks). The point I'm trying to make is that there is a tradeoff between doing things for the user, and giving the user agency. Good software is built on understanding, not cleverness.</p>
    <p>I also think this is an important lesson to remember as AI-assisted coding becomes increasingly popular. Don't choose tools that alienate you from your work. Choose tools that leverage the wisdom you've already accumulated, and which help you to cultivate a deeper understanding of the discipline.</p>
    <p>Thank you to Rich Harris and team for many years of pleasant development. I hope that (if you read this) it's not <em>so</em> full of inaccuracies as to be unhelpful as user feedback.</p>
    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[File Pilot: A file explorer built for speed with a modern, robust interface (186 pts)]]></title>
            <link>https://filepilot.tech/</link>
            <guid>43091466</guid>
            <pubDate>Tue, 18 Feb 2025 16:24:01 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://filepilot.tech/">https://filepilot.tech/</a>, See on <a href="https://news.ycombinator.com/item?id=43091466">Hacker News</a></p>
Couldn't get https://filepilot.tech/: Error: timeout of 10000ms exceeded]]></description>
        </item>
        <item>
            <title><![CDATA[By the end of today, NASA's workforce will be about 10 percent smaller (138 pts)]]></title>
            <link>https://arstechnica.com/space/2025/02/by-the-end-of-today-nasas-workforce-will-be-about-10-percent-smaller/</link>
            <guid>43090862</guid>
            <pubDate>Tue, 18 Feb 2025 15:47:58 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://arstechnica.com/space/2025/02/by-the-end-of-today-nasas-workforce-will-be-about-10-percent-smaller/">https://arstechnica.com/space/2025/02/by-the-end-of-today-nasas-workforce-will-be-about-10-percent-smaller/</a>, See on <a href="https://news.ycombinator.com/item?id=43090862">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="app">
    <p><a href="#main">
  Skip to content
</a></p>



<main id="main">
            <article data-id="2076542">
  
  <header>
  <div>
    <div>
      

      

      <p>
        A dark and painful day at a space agency that brings so much light and joy to the world.
      </p>

      
    </div>

    <div>
    
    <p>
      In this illustration, NASA’s OSIRIS-REx spacecraft collects a sample from the asteroid Bennu. The agency's superpower is its capacity to dazzle us.

              <span>
          Credit:

          
          NASA/Goddard/University of Arizona

                  </span>
          </p>
  </div>
  </div>
</header>


  

  
      
    
    <div>
                      
                      
          
<p>Spread across NASA's headquarters and 10 field centers, which dot the United States from sea to sea, the space agency has had a workforce of nearly <a href="https://www.nasa.gov/organization/#:~:text=The%20agency's%20organization%20comprises%20a,for%20the%20benefit%20of%20humanity.">18,000 civil servants</a>.</p>
<p>However, by the end of today, that number will have shrunk by about 10 percent since the beginning of the second Trump administration four weeks ago. And the world's preeminent space agency may still face significant additional cuts.</p>
<p>According to sources, about 750 employees at NASA accepted the "fork in the road" offer to take deferred resignation from the space agency later this year. This sounds like a lot of people, but generally about 1,000 people leave the agency every year, so effectively, many of these people might just be getting paid to leave jobs they were already planning to exit from.</p>
<p>The culling of "probationary" employees will be more impactful. As it has done at other federal agencies, the Trump administration is generally firing federal employees who are in the "probationary" period of their employment, which includes new hires within the last one or two years or long-time employees who have moved into or been promoted into a new position. About 1,000 or slightly more employees at NASA were impacted by these cuts.</p>
<p>Adding up the deferred resignations and probationary cuts, the Trump White House has now trimmed about 10 percent of the agency's workforce.</p>
<p>However, the cuts may not stop there. Two sources told Ars that directors at the agency's field centers have been told to prepare options for a "significant" reduction in force in the coming months. The scope of these cuts has not been defined, and it's possible they may not even happen, given that the White House must negotiate budgets for NASA and other agencies with the US Congress. But this directive for further reductions in force casts more uncertainty on an already demoralized workforce and signals that the Trump administration would like to make further cuts.</p>

          
                      
                  </div>
                    
        
          
    
    <div>
          
          
<h2>An awful week</h2>
<p>Job losses are always terrible. This will be a dark and painful day at a space agency that brings so much light and joy to the world. Many of the probationary employees are just starting out their careers and were likely thrilled to land a job at NASA to explore the universe. And then all of that youthful energy and hope was extinguished this week.</p>
<p>It's possible to view these losses through a couple of lenses.</p>
<p>Yes, NASA is clearly losing some capability with these latest cuts. Many of these hires were likely being counted on to bring new energy into the space agency and become its future discoverers and leaders. And their jobs are being sacrificed for no clear purpose. Is it to increase funding for the military? Is it to pay for tax cuts for the rich? There is a lot of anger that the relatively thin budget line of NASA—less than one-half of 1 percent of the federal budget—is being sliced for such purposes.</p>

<p>There is also frustration at the indiscriminate nature of the cuts. The Trump White House and the Department of Government Efficiency, spearheaded by Elon Musk, have taken a meat-cleaver approach by firing a lot of people at the same time, and probably not the right people, through a messy and painful process. This is not dissimilar to job cuts during corporate mergers or bankruptcies. It's the fastest possible way to make cuts. There is no empathy, and it is a brutal process.</p>
<h2>Are cuts needed?</h2>
<p>It is also clear that, as within other federal agencies, there is significant "bloat" in NASA's budget. In some areas, this is plain to see, with the space agency having spent in excess of $3 billion a year over the last decade "developing" a heavy lift rocket, the Space Launch System, which used components from the Space Shuttle and costs an extraordinary amount of money to fly. In the meantime, the private launch industry has been running circles around NASA. Similarly, consider the Orion spacecraft. This program is now two decades old, at a cost of $1 billion a year, and the vehicle has never flown humans into space.</p>

          
                  </div>
                    
        
          
    
    <div>

        
        <div>
          
          
<p>One could go on. Much of the space community has been puzzled as to why NASA has been spending on the order of half a billion dollars to develop a Lunar Gateway in an odd orbit around the Moon. It remains years away from launching, and if it ever does fly, it would increase the energy needed to reach the surface of the Moon. The reason, according to multiple sources at the agency when the Gateway was conceived, is that the lunar space station would offer jobs to the current flight controllers operating the International Space Station, which is due to retire in 2030.</p>
<p>In recent years, NASA has been in the midst of a difficult transition. The agency deserves a lot of credit for nurturing a commercial space industry that now is the envy of the world. But as part of this, NASA has been moving away from owning and operating its rockets, spacecraft, and other hardware and buying services from this commercial space industry. This transition from traditional space to commercial space marks an important step for NASA to remain on the cutting edge of exploration and science rather than trying to compete with US industry.</p>
<p>But it is also a painful step.</p>
<p>The key is ensuring that any future cuts at NASA are not indiscriminate. If and when Jared Isaacman is confirmed by the US Senate as the next NASA administrator, it will be up to him and his team to make the programmatic decisions about which parts of the agency are carrying their weight and which are being carried, which investments carry NASA into the future, and which ones drag it into the past. If these future cuts are smart and position NASA for the future, this could all be worth it. If not, then the beloved agency that dares to explore may never recover.</p>


          
                  </div>

                  
          






  <div>
  <div>
          <p><a href="https://arstechnica.com/author/ericberger/"><img src="https://arstechnica.com/wp-content/uploads/2016/05/e.berger-45959.jpg" alt="Photo of Eric Berger"></a></p>
  </div>

  <div>
    

    <p>
      Eric Berger is the senior space editor at Ars Technica, covering everything from astronomy to private space to NASA policy, and author of two books: <a href="https://www.harpercollins.com/products/liftoff-eric-berger?variant=32126620205090"><i>Liftoff</i></a>, about the rise of SpaceX; and <a href="https://benbellabooks.com/shop/reentry/"><i>Reentry</i></a>, on the development of the Falcon 9 rocket and Dragon. A certified meteorologist, Eric lives in Houston.
    </p>
  </div>
</div>


  <p>
    <a href="https://arstechnica.com/space/2025/02/by-the-end-of-today-nasas-workforce-will-be-about-10-percent-smaller/#comments" title="191 comments">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 80 80"><defs><clipPath id="bubble-zero_svg__a"><path fill="none" stroke-width="0" d="M0 0h80v80H0z"></path></clipPath><clipPath id="bubble-zero_svg__b"><path fill="none" stroke-width="0" d="M0 0h80v80H0z"></path></clipPath></defs><g clip-path="url(#bubble-zero_svg__a)"><g fill="currentColor" clip-path="url(#bubble-zero_svg__b)"><path d="M80 40c0 22.09-17.91 40-40 40S0 62.09 0 40 17.91 0 40 0s40 17.91 40 40"></path><path d="M40 40 .59 76.58C-.67 77.84.22 80 2.01 80H40z"></path></g></g></svg>
    191 Comments
  </a>
      </p>
              </div>
  </article>


  


  


  <div>
    <header>
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 40 26"><defs><clipPath id="most-read_svg__a"><path fill="none" d="M0 0h40v26H0z"></path></clipPath><clipPath id="most-read_svg__b"><path fill="none" d="M0 0h40v26H0z"></path></clipPath></defs><g clip-path="url(#most-read_svg__a)"><g fill="none" clip-path="url(#most-read_svg__b)"><path fill="currentColor" d="M20 2h.8q1.5 0 3 .6c.6.2 1.1.4 1.7.6 1.3.5 2.6 1.3 3.9 2.1.6.4 1.2.8 1.8 1.3 2.9 2.3 5.1 4.9 6.3 6.4-1.1 1.5-3.4 4-6.3 6.4-.6.5-1.2.9-1.8 1.3q-1.95 1.35-3.9 2.1c-.6.2-1.1.4-1.7.6q-1.5.45-3 .6h-1.6q-1.5 0-3-.6c-.6-.2-1.1-.4-1.7-.6-1.3-.5-2.6-1.3-3.9-2.1-.6-.4-1.2-.8-1.8-1.3-2.9-2.3-5.1-4.9-6.3-6.4 1.1-1.5 3.4-4 6.3-6.4.6-.5 1.2-.9 1.8-1.3q1.95-1.35 3.9-2.1c.6-.2 1.1-.4 1.7-.6q1.5-.45 3-.6zm0-2h-1c-1.2 0-2.3.3-3.4.6-.6.2-1.3.4-1.9.7-1.5.6-2.9 1.4-4.3 2.3-.7.5-1.3.9-1.9 1.4C2.9 8.7 0 13 0 13s2.9 4.3 7.5 7.9c.6.5 1.3 1 1.9 1.4 1.3.9 2.7 1.7 4.3 2.3.6.3 1.3.5 1.9.7 1.1.3 2.3.6 3.4.6h2c1.2 0 2.3-.3 3.4-.6.6-.2 1.3-.4 1.9-.7 1.5-.6 2.9-1.4 4.3-2.3.7-.5 1.3-.9 1.9-1.4C37.1 17.3 40 13 40 13s-2.9-4.3-7.5-7.9c-.6-.5-1.3-1-1.9-1.4-1.3-.9-2.8-1.7-4.3-2.3-.6-.3-1.3-.5-1.9-.7C23.3.4 22.1.1 21 .1h-1"></path><path fill="#ff4e00" d="M20 5c-4.4 0-8 3.6-8 8s3.6 8 8 8 8-3.6 8-8-3.6-8-8-8m0 11c-1.7 0-3-1.3-3-3s1.3-3 3-3 3 1.3 3 3-1.3 3-3 3"></path></g></g></svg>
      
    </header>
    <ol>
              <li>
                      <a href="https://arstechnica.com/gadgets/2025/02/x-is-reportedly-blocking-links-to-secure-signal-contact-pages/">
              <img src="https://cdn.arstechnica.net/wp-content/uploads/2025/02/GettyImages-1231496453-Large-768x432.jpeg" alt="Listing image for first story in Most Read: X is reportedly blocking links to secure Signal contact pages" decoding="async" loading="lazy">
            </a>
                    
        </li>
                    <li>
                    
        </li>
                    <li>
                    
        </li>
                    <li>
                    
        </li>
                    <li>
                    
        </li>
                  </ol>
</div>


  

  </main>





  </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Migraine is more than a headache – a rethink offers hope (144 pts)]]></title>
            <link>https://www.nature.com/articles/d41586-025-00456-x</link>
            <guid>43090857</guid>
            <pubDate>Tue, 18 Feb 2025 15:47:25 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.nature.com/articles/d41586-025-00456-x">https://www.nature.com/articles/d41586-025-00456-x</a>, See on <a href="https://news.ycombinator.com/item?id=43090857">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-test="access-teaser"> <p>Andrea West remembers the first time she heard about a new class of migraine medication that could end her decades of pain. It was 2021 and she heard a scientist on the radio discussing the <a href="https://www.nature.com/articles/d41586-020-02862-9" data-track="click" data-label="https://www.nature.com/articles/d41586-020-02862-9" data-track-category="body text link">promise of gepants</a>, a class of drug that for the first time seemed to prevent migraine attacks. West followed news about these drugs closely, and when she heard last year that atogepant was approved for use in the United Kingdom, she went straight to her physician. </p><p>West had endured migraines for 70 years. Since she started taking the drug, she hasn’t had one. “It’s marvellous stuff. It’s genuinely changed my life,” she says.</p><p>For ages, the perception of migraine has been one of suffering with little to no relief. In ancient Egypt, physicians strapped clay crocodiles to people’s heads and prayed for the best. And as late as the seventeenth century, surgeons bored holes into people’s skulls — some have suggested — to let the migraine out. The twentieth century brought much more effective treatments, but they did not work for a significant fraction of the roughly one billion people who experience migraine worldwide.</p><article data-label="Related"><a href="https://www.nature.com/articles/d41586-024-02222-x" data-track="click" data-track-label="recommended article"><img alt="" src="https://media.nature.com/w400/magazine-assets/d41586-025-00456-x/d41586-025-00456-x_50648956.jpg"><p>What causes migraines? Study of ‘brain blackout’ offers clues</p></a></article><p>Now there is a new sense of progress running through the field, brought about by developments on several fronts. Medical advances in the past few decades — including the <a href="https://www.nature.com/articles/nbt0718-559b" data-track="click" data-label="https://www.nature.com/articles/nbt0718-559b" data-track-category="body text link">approval of gepants and related treatment</a>s — have redefined migraine as “a treatable and manageable condition”, says Diana Krause, a neuropharmacologist at the University of California, Irvine. </p><p>At the same time, research is leading to a better understanding about the condition — and pointing to directions for future work. Studies have shown, for example, that <a href="https://www.nature.com/articles/d41586-020-02861-w" data-track="click" data-label="https://www.nature.com/articles/d41586-020-02861-w" data-track-category="body text link">migraine is a broad phenomenon</a> that originates in the brain and can manifest in many debilitating symptoms, including light sensitivities and <a href="https://www.nature.com/articles/d41586-020-02863-8" data-track="click" data-label="https://www.nature.com/articles/d41586-020-02863-8" data-track-category="body text link">aura</a>, brain fog and fatigue. “I used to think that disability travels with pain, and it’s only when the pain gets severe that people are impaired. That’s not only false, but we have treatments to do something about it,” says Richard Lipton, a neurologist at the Albert Einstein College of Medicine in New York City.</p><p>Researchers are trying to discover what triggers a <a href="https://www.nature.com/articles/d41586-024-02222-x" data-track="click" data-label="https://www.nature.com/articles/d41586-024-02222-x" data-track-category="body text link">migraine-prone brain to flip into a hyperactive state</a>, causing a full-blown attack, or for that matter, what makes a brain prone to the condition. A new and broader approach to research and treatment is needed, says Arne May, a neurologist at the University Medical Center Hamburg–Eppendorf in Germany. To stop migraine completely and not just headache pain, he says, “we need to create new frameworks to understand how the brain activates the whole system of migraine”.</p><h2><b>Wonder drugs?</b></h2><p>When May started researching migraine in the 1990s, the leading hypotheses were that migraine was either a psychological issue or a vascular headache disorder, with throbbing pain caused by dilation of blood vessels. The psychological associations came with stigma, May says. “No one believed people who had migraine, they just thought they didn’t want to work. Nearly all of my patients at that time had to see a psychologist or psychiatrist.” The field, Krause says, is still recovering from these ideas. Most clinicians have abandoned the idea that the problem is psychological, but the notion that migraine is akin to a particularly bad headache persists even now.</p><p>A lot changed in the 1990s, when May and others began conducting brain scans of people with migraine. The researchers saw for the first time that brain regions were activated during headache attacks, showing that it was more than just a vascular issue<sup><a href="#ref-CR1" data-track="click" data-action="anchor-link" data-track-label="go to reference" data-track-category="references">1</a></sup>. “From that point on, a lot of things changed. It was the very first time someone could point to migraine and say it’s a biological disease,” says May.</p><p>Researchers found that changes in the brain’s activity start appearing at what’s known as the premonitory phase, which begins hours to days before an attack (see ‘Migraine is cyclical’). The premonitory phase is characterized by a swathe of symptoms, including nausea, food cravings, faintness, fatigue and yawning. That’s often followed by a days-long migraine attack phase, which comes with overwhelming headache pain and other physical and psychological symptoms. After the attack subsides, the postdrome phase has its own associated set of symptoms that include depression, euphoria and fatigue. An interictal phase marks the time between attacks and can involve symptoms as well.</p><figure><picture><source type="image/webp" srcset="https://media.nature.com/lw767/magazine-assets/d41586-025-00456-x/d41586-025-00456-x_50649324.png?as=webp 767w, https://media.nature.com/lw319/magazine-assets/d41586-025-00456-x/d41586-025-00456-x_50649324.png?as=webp 319w" sizes="(max-width: 319px) 319px, (min-width: 1023px) 100vw,  767px"><img alt="Migraine is cyclical: A line chart showing the phases of a migraine attack. Symptoms typically ramp up after trigger events that researchers are trying to understand." loading="lazy" src="https://media.nature.com/lw767/magazine-assets/d41586-025-00456-x/d41586-025-00456-x_50649324.png"><figcaption><p><span>Source: N. Karsan &amp; P. J. Goadsby <i>Nature Rev. Neurol</i>. <b>14</b>, 699–710 (2018).</span></p></figcaption></picture></figure><p>But the type, severity and causes of migraine symptoms can differ between people. Dom Horton, who is 53 and an editor in the United Kingdom, never gets headaches. But he experiences other migraine symptoms all the time. “Constant dizziness and a swimming mind are always present,” he says, and they sometimes build to a severity that prevents him from leaving his house. Fiona Gartside, a 60-year-old veterinary surgeon in Scotland, experiences sensitivities to noise, light and movement, overwhelming exhaustion and headaches that get so severe that she occasionally loses consciousness, “which is a relief”, she says. Migraine can even drive full-blown visual hallucinations similar to the ‘reflections of the living light’ painted by Hildegard von Bingen, a twelfth-century abbess who was thought to have experienced a condition that is now called migraine with aura.</p><p>Despite the variety of symptoms, it was research into normal, non-migraine associated headaches that led to revolutionary treatments for migraine. Gepant drugs and a handful of monoclonal antibodies have been designed to block activity of the calcitonin gene-related peptide (CGRP). They came from decades of research on the role of CGRP in headache and are a real “bench to bedside success story”, according to Peter Goadsby, a neurologist at King’s College London, who pioneered the research along with Lars Edvinsson, a neuroscientist at Lund University, Sweden, and collaborators in the 1980s.</p><article data-label="Related"><a href="https://www.nature.com/articles/d41586-023-00869-6" data-track="click" data-track-label="recommended article"><img alt="" src="https://media.nature.com/w400/magazine-assets/d41586-025-00456-x/d41586-025-00456-x_24970190.jpg"><p>Chronic pain can be treated — so why are millions still suffering?</p></a></article><p>Headache begins when sensory nerves called nociceptors in the meninges become sensitized, sending information to the brain to evoke pain. Goadsby’s work showed that CGRP is a key factor in sensitizing these nociceptors. Clinical trials of drugs that block the peptide in people with migraine proved effective both in alleviating headache and sometimes in preventing attacks from starting<sup><a href="#ref-CR2" data-track="click" data-action="anchor-link" data-track-label="go to reference" data-track-category="references">2</a></sup>. Goadsby says it’s been stunning to see the completeness of people’s responses to gepants. “Patients come back and literally cry,” he says. “They’d forgotten before what normal was.”</p><p>From the successes of CGRP blockers, it’s tempting to view CGRP as a ‘factor X’ of migraine. Yet it’s clear that other elements are at play. CGRP blockers work only for a subset of people, as few as one in five according to some studies<sup><a href="#ref-CR3" data-track="click" data-action="anchor-link" data-track-label="go to reference" data-track-category="references">3</a></sup>. And for those who do respond well to the drugs, some migraine symptoms often persist. West, for example, still has bouts of nausea even though the drug she takes, atogepant, stops her migraine attacks. And although atogepant has minimized Gartside’s symptoms, migraine still dominates her life. “There is a constant juggle between prevention, medication, trigger avoidance, fatigue, fear and anticipation of attacks,” she says.</p><h2><b>Migraine in the brain</b></h2><p>Goadsby says the mixed results of CGRP blockers show a huge gap in the biological understanding of migraine. “This tells us there are other frameworks of migraine that need to be discovered, and other pathways,” he says. May agrees. He thinks the field needs a radical change in thinking to find new mechanisms of migraine. “We’re focusing too much on migraine as a headache disease,” he says. “The thinking for most people stops at CGRP, but CGRP isn’t the only answer.” The problem, he says, is that scientists don’t fully understand what a migraine attack looks like in the brain.</p><p>Studies in the past seven years or so have solidified the hypothalamus as a centre of the condition<sup><a href="#ref-CR4" data-track="click" data-action="anchor-link" data-track-label="go to reference" data-track-category="references">4</a></sup>. “It must involve the limbic system, of which the hypothalamus is the king,” May says. The limbic system is a group of interconnected brain structures that process sensory information and regulate emotions. Studies that scanned the brains of people with migraine every few days for several weeks showed that hypothalamic connectivity to various parts of the brain increases just before a migraine attack begins, then collapses during the headache phase<sup><a href="#ref-CR5" data-track="click" data-action="anchor-link" data-track-label="go to reference" data-track-category="references">5</a></sup>.</p><article data-label="Related"><a href="https://www.nature.com/articles/d41586-025-00274-1" data-track="click" data-track-label="recommended article"><img alt="" src="https://media.nature.com/w400/magazine-assets/d41586-025-00456-x/d41586-025-00456-x_50648958.jpg"><p>US drug agency approves potent painkiller — the first non-opioid in decades</p></a></article><p>May and others think that the hypothalamus loses control over the limbic system about two days before the attack begins, and it results in changes to conscious experiences that might explain symptoms such as light- and sound-sensitivity, or cognitive impairments. At the same time, the breakdown of hypothalamic control puts the body’s homeostatic balance out of kilter, which explains why symptoms such as fatigue, nausea, yawning and food cravings are common when a migraine is building up, says Krause.</p><p>Goadsby agrees that the hypothalamus is important, but thinks it’s more complex than simply a loss of control. He hypothesizes that an attack could begin when any part of a ‘migraine network’, potentially including the hypothalamus, thalamus and limbic system, is overstimulated. Researchers have yet to pinpoint precisely which brain regions are part of the network, or the “exact order of batting” of when these regions are activated during an attack, Goadsby says.</p><h2><b>Predispositions and triggers</b></h2><p>Migraine researchers now talk of a hypothetical ‘migraine threshold’ in which environmental or physiological triggers tip brain activity into a dysregulated state.</p><p>The list of potential triggers is extensive. West’s migraines are closely linked to certain foods and to hunger, stress and hormonal changes. She used to get terrible headaches with her period, then after menopause they developed into full-blown three-day migraines. More than half of women with migraine experience attacks every month during menstruation. And migraine is also <a href="https://www.nature.com/articles/d41586-019-00895-3" data-track="click" data-label="https://www.nature.com/articles/d41586-019-00895-3" data-track-category="body text link">three times more prevalent in women</a> than in men; it’s the number one debilitating issue for cisgender women in their reproductive years, and it seems equally prevalent in transgender women taking hormone replacement therapy. </p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Tariffs result in 10% laptop price hike in U.S. says Acer CEO (276 pts)]]></title>
            <link>https://www.tomshardware.com/laptops/acer-ceo-10pc-price-rise-tariffs</link>
            <guid>43090684</guid>
            <pubDate>Tue, 18 Feb 2025 15:33:37 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.tomshardware.com/laptops/acer-ceo-10pc-price-rise-tariffs">https://www.tomshardware.com/laptops/acer-ceo-10pc-price-rise-tariffs</a>, See on <a href="https://news.ycombinator.com/item?id=43090684">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="article-body">
<p>Acer Laptop prices in the U.S. are set to rise by 10% from March 2025, commented CEO Jason Chen in an interview with <a data-analytics-id="inline-link" href="https://www.telegraph.co.uk/business/2025/02/16/computer-giant-to-raise-prices-by-10pc-in-response-to-trump/" data-url="https://www.telegraph.co.uk/business/2025/02/16/computer-giant-to-raise-prices-by-10pc-in-response-to-trump/" target="_blank" referrerpolicy="no-referrer-when-downgrade" data-hl-processed="none">The Telegraph</a>. Chen states that the price hike is a direct result of the Trump administration's <a data-analytics-id="inline-link" href="https://www.tomshardware.com/pc-components/tech-enthusiasts-brace-for-trump-tariff-price-hikes-on-new-components" data-before-rewrite-localise="https://www.tomshardware.com/pc-components/tech-enthusiasts-brace-for-trump-tariff-price-hikes-on-new-components">incoming tariffs</a>.</p><p>"We will have to adjust the end user price to reflect the tariff," Chen said to The Telegraph. "We think 10% probably will be the default price increase because of the import tax. It's very straightforward," the Acer CEO continued.</p><p>The decision to increase prices was reportedly confirmed last week, with the Taiwanese tech giant remaining unaffected by tariffs on products that left China before February. Therefore, stock hitting U.S. channels afterward will be subject to increased tariffs.</p><p>Acer's most expensive laptop on sale, the <a data-analytics-id="inline-link" href="https://www.tomshardware.com/news/acer-predator-triton-17-x-hands-on" data-before-rewrite-localise="https://www.tomshardware.com/news/acer-predator-triton-17-x-hands-on">Acer Predator Triton</a> 17-inch gaming laptop, is currently $3,799 at Best Buy. But, next month that price is set to increase to $4,178 if increases are passed directly to consumers. It's unclear if the blanket price rise will also affect products currently sitting on shelves, but it's likely that as new stock arrives, older stock will also be subject to the price increase.</p><p>Chen notes that the incoming tariff may offer an 'excuse' for others in the segment to raise prices by more than 10%. No other PC manufacturer has made a public statement regarding the incoming tariffs and inevitable price rises.</p><figure data-bordeaux-image-check=""><div><p><picture><source type="image/webp" srcset="https://cdn.mos.cms.futurecdn.net/9ZYDEpNG4ZUa8GxJsXGLRF-320-80.jpg.webp 320w, https://cdn.mos.cms.futurecdn.net/9ZYDEpNG4ZUa8GxJsXGLRF-480-80.jpg.webp 480w, https://cdn.mos.cms.futurecdn.net/9ZYDEpNG4ZUa8GxJsXGLRF-650-80.jpg.webp 650w, https://cdn.mos.cms.futurecdn.net/9ZYDEpNG4ZUa8GxJsXGLRF-970-80.jpg.webp 970w, https://cdn.mos.cms.futurecdn.net/9ZYDEpNG4ZUa8GxJsXGLRF-1024-80.jpg.webp 1024w, https://cdn.mos.cms.futurecdn.net/9ZYDEpNG4ZUa8GxJsXGLRF-1200-80.jpg.webp 1200w" sizes="(min-width: 1000px) 970px, calc(100vw - 40px)"><img src="https://cdn.mos.cms.futurecdn.net/9ZYDEpNG4ZUa8GxJsXGLRF-320-80.jpg" alt="Acer Predator Triton 17 X" srcset="https://cdn.mos.cms.futurecdn.net/9ZYDEpNG4ZUa8GxJsXGLRF-320-80.jpg 320w, https://cdn.mos.cms.futurecdn.net/9ZYDEpNG4ZUa8GxJsXGLRF-480-80.jpg 480w, https://cdn.mos.cms.futurecdn.net/9ZYDEpNG4ZUa8GxJsXGLRF-650-80.jpg 650w, https://cdn.mos.cms.futurecdn.net/9ZYDEpNG4ZUa8GxJsXGLRF-970-80.jpg 970w, https://cdn.mos.cms.futurecdn.net/9ZYDEpNG4ZUa8GxJsXGLRF-1024-80.jpg 1024w, https://cdn.mos.cms.futurecdn.net/9ZYDEpNG4ZUa8GxJsXGLRF-1200-80.jpg 1200w" sizes="(min-width: 1000px) 970px, calc(100vw - 40px)" loading="lazy" data-original-mos="https://cdn.mos.cms.futurecdn.net/9ZYDEpNG4ZUa8GxJsXGLRF.jpg" data-pin-media="https://cdn.mos.cms.futurecdn.net/9ZYDEpNG4ZUa8GxJsXGLRF.jpg"></picture></p></div><figcaption itemprop="caption description"><span itemprop="copyrightHolder">(Image credit: Tom's Hardware)</span></figcaption></figure><p>Acer shifted the assembly of its desktop PCs away from China during Trump's previous term and says that it is "looking at different supply chains beyond China", with U.S. production "one of the options" being considered.</p><p>The Consumer Technology Association claims that 80% of U.S. laptop imports currently come from China and that the incoming tariff could set U.S. customers back a collective $143 billion, hurting sales. Moreover, the benefits to U.S. industry may be meager, with U.S. production forecast to rise by only 8% and prices potentially rising by up to 45%.</p><div data-hydrate="true" id="slice-container-newsletterForm-articleInbodyContent-P4sFCQv7kMAXmCkp7o3CnZ"><section><p>Get Tom's Hardware's best news and in-depth reviews, straight to your inbox.</p></section></div><p>A <a data-analytics-id="inline-link" href="https://www.digitimes.com/news/a20250218VL202/acer-notebooks-price-us-jason-chen.html#:~:text=Acer%20CEO%20Jason%20Chen%20confirmed,%2C%20including%20manufacturing" data-url="https://www.digitimes.com/news/a20250218VL202/acer-notebooks-price-us-jason-chen.html#:~:text=Acer%20CEO%20Jason%20Chen%20confirmed,%2C%20including%20manufacturing" target="_blank" referrerpolicy="no-referrer-when-downgrade" data-hl-processed="none">Digitimes</a> senior supply chain analyst further suggests that the US's proposed 100% tariff on semiconductors could mean that companies such as Nvidia, AMD, and Apple could all face pricing challenges ahead. This is supposedly due to the lack of overseas manufacturing, though the U.S. government is <a data-analytics-id="inline-link" href="https://www.tomshardware.com/tech-industry/us-govt-pushing-tsmc-and-intel-to-create-joint-venture-in-the-us-report" data-before-rewrite-localise="https://www.tomshardware.com/tech-industry/us-govt-pushing-tsmc-and-intel-to-create-joint-venture-in-the-us-report">pushing TSMC and Intel</a> to bolster local manufacturing.</p><p>Until Acer manages to find a U.S.-based provider for its laptops, the real cost of the tariffs is likely to be passed onto the consumer. With other manufacturers still silent, if Acer's approach is anything to go by, it sets an ill-portent for U.S. consumers.</p>
</div></div>]]></description>
        </item>
    </channel>
</rss>