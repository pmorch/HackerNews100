<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0">
    <channel>
        <title>HN100 - Readable Contents</title>
        <link>https://hn.algolia.com/api/v1/search_by_date?tags=%28story,poll%29&amp;numericFilters=points%3E100</link>
        <description>Uses Readability to add bodies to the RSS feed</description>
        <lastBuildDate>Sun, 17 Dec 2023 23:00:05 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>en</language>
        <item>
            <title><![CDATA[3M knew its chemicals were harmful decades ago, but didn't tell the public (357 pts)]]></title>
            <link>https://minnesotareformer.com/2022/12/15/toxic-3m-knew-its-chemicals-were-harmful-decades-ago-but-didnt-tell-the-public-government/</link>
            <guid>38675616</guid>
            <pubDate>Sun, 17 Dec 2023 19:30:47 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://minnesotareformer.com/2022/12/15/toxic-3m-knew-its-chemicals-were-harmful-decades-ago-but-didnt-tell-the-public-government/">https://minnesotareformer.com/2022/12/15/toxic-3m-knew-its-chemicals-were-harmful-decades-ago-but-didnt-tell-the-public-government/</a>, See on <a href="https://news.ycombinator.com/item?id=38675616">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="dataContent">
                                    <p><em>This is part 2 of 2. <a target="_blank" href="https://minnesotareformer.com/2022/12/14/there-must-be-something-in-the-water/">Read part 1</a>, about East Metro residents who wonder if 3M chemicals made them sick.&nbsp;</em></p>
<p><span>3M toxicologist Richard Purdy did a</span><a target="_blank" href="https://www.ag.state.mn.us/Office/Cases/3M/docs/PTX/PTX1523.pdf"> <span>study</span></a><span> in 1998 to see whether any of the company’s perfluorochemicals showed up in the blood of eagles and albatrosses.</span></p>
<p><span>That seemed unlikely, given the birds’ diet consists mostly of fish. So Purdy was surprised and disturbed when he found levels in their blood similar to those found in human blood. It even showed up in bald eagle nestlings whose only food was fish their parents fed them from remote lakes.</span></p>
<p><span>That indicated what Purdy</span><a target="_blank" href="https://www.ag.state.mn.us/Office/Cases/3M/docs/PTX/PTX1001.pdf"> <span>later called</span></a><span> “widespread environmental contamination” — the likelihood the manmade, toxic chemicals were moving through the food chain and accumulating in animals.</span></p>
<p><span>Purdy warned 3M that if wild birds’ blood contained the chemicals, then fish-eating mammals — like otters, mink, porpoise and seals —&nbsp;could have it, too. A study of rats found they had significant levels of a 3M chemical in their livers, likely from eating fishmeal.&nbsp;</span></p>
<p><span>He told company officials in an</span><a target="_blank" href="https://www.ag.state.mn.us/Office/Cases/3M/docs/PTX/PTX1533.pdf"> <span>email</span></a><span> there was a significant risk of ecological harm, which should be reported to the EPA.</span></p>
<p><span>In response, 3M managers dispersed the team collecting the data, Purdy alleged.</span></p>
<p><span>Purdy resigned in 1999 and sent his </span><a target="_blank" href="https://www.ag.state.mn.us/Office/Cases/3M/docs/PTX/PTX1001.pdf"><span>resignation letter</span></a><span> to the EPA, informing them that while 3M had disclosed to the EPA that a chemical called PFOS “had been found in the blood of animals,” it didn’t mention that it was found in the blood of eaglets.</span></p>
<p><span>The EPA began investigating the chemicals that year. But by then, 3M had reaped billions of dollars in profits from chemicals that the company had been warned were harming the environment and risking human health.&nbsp;</span></p>
<p><span>The per-and polyfluoroalkyl substances (PFAS) had spread — through groundwater and products like Scotchgard stain repellent,</span> <span>Teflon cookware, food wrapping and fire retardant — and were showing up in the blood of people and animals in every corner of the world. They were in nearly every living thing, from house dust to human blood, in wildlife in the Arctic circle and drinking water, rivers, streams and breast milk.&nbsp;</span></p>
<p><span>Purdy’s warnings were clear, as revealed by former Attorney General Attorney General Lori Swanson, who sued 3M in 2010, alleging the company failed for decades to report that its chemicals could be toxic to humans, animals and the environment, keeping information from regulators and scientists to protect its lucrative revenue stream.&nbsp;</span></p>
<p><span>The morning the case was set to go to trial in 2018, after 22 hours of negotiation, 3M and the state settled. 3M agreed to pay $850 million to help provide Minnesotans clean drinking water.&nbsp;</span></p>
<p><span>The settlement with Minnesota is the third largest natural resource damage settlement in U.S. history, behind the Deepwater Horizon and Exxon Valdez oil spills.&nbsp;</span></p>
<p><span>But it amounted to just 2.6% of 3M’s nearly $33 billion in revenue in 2018.&nbsp;</span></p>
<p><span>The company admitted nothing, and maintains to this day that its chemicals have no adverse health or environmental consequences.&nbsp;</span></p>
<p><span>3M spokesman Grant Thompson said in an email that 3M’s position reflects the weight of&nbsp; </span><span>scientific evidence from decades of research showing exposure to PFOA and PFOS at current and historical levels found in people and the environment has not been shown to cause adverse health effects.</span></p>
<blockquote data-secret="Vvx1EzNtcF"><p><a target="_blank" href="https://minnesotareformer.com/2022/12/14/there-must-be-something-in-the-water/">There must be something in the water</a></p></blockquote>

<p><span>Still, 3M’s settlement with the state of Minnesota is likely the beginning — not the end — of the company’s legal, regulatory and political challenges stemming from both the invention and dumping of the chemicals. 3M and other companies that made the chemicals may have to pay out billions for the damage they caused the environment and people.&nbsp;</span></p>
<p><span>During a 2019 congressional hearing, U.S. Rep. Harley Rouda of California</span><a target="_blank" href="https://www.govinfo.gov/content/pkg/CHRG-116hhrg37952/html/CHRG-116hhrg37952.htm"> <span>called</span></a><span> the contamination of Americans’ drinking water, groundwater, air and food supplies a national emergency.</span></p>
<p><span>“These companies got away with poisoning people for more than a half century,” Rouda said.</span></p>
<p><span>In August, the EPA proposed designating two perfluorochemicals as hazardous substances under the Superfund law, which would spark federal cleanup standards and could put chemical companies on the hook for billions in cleanup costs.&nbsp;</span></p>
<p><span>The EPA also published new drinking water health advisory levels for several perfluorochemical compounds and plans to propose a national drinking water perfluorochemical regulation soon.</span></p>
<p><span>A federal judge in Charleston, S.C., also dealt the company a blow in September, </span><a target="_blank" href="https://news.bloomberglaw.com/environment-and-energy/3m-loses-government-contractor-defense-in-pfas-litigation"><span>denying</span></a><span> 3M’s request for government contractor immunity in a mass tort case alleging 3M and other companies’ firefighting foam are linked to health problems.</span></p>
<p><span>Judge Richard ​​Gergel said 3M conducted over 1,000 studies of perfluorochemicals’ effect on human health and the environment, the results of which should have been disclosed to the EPA.&nbsp;</span></p>
<p><span>He wrote that 3M and other chemical manufacturers “had significantly greater knowledge than the government about the properties and risks associated with their products and knowingly withheld highly material information from the government.”</span></p>
<p><span>Closer to 3M’s Minnesota headquarters, some sickened residents in the East Metro — where groundwater was contaminated with 3M chemicals — say they’re working with attorneys on a lawsuit.&nbsp;</span></p>
<p><span>David Sunding, a University of California Berkeley professor, published a 2017 report saying Washington County residents who lived in areas where groundwater was contaminated with 3M chemicals had elevated rates of bladder, breast, kidney and prostate cancers, as well as leukemia and non-Hodgkin’s lymphoma.</span></p>
<p><span>3M disputes that, pointing to a 2018 Minnesota health department </span><a target="_blank" href="https://www.health.state.mn.us/data/mcrs/docs/rpteastmetro.pdf"><span>report</span></a><span> showing that the overall cancer rate in Washington County was “virtually identical” to the statewide average, despite chemical contamination.&nbsp;&nbsp;</span></p>
<p><span>Given the stakes of the litigation, the future of the company — which employs 7,000 people at its massive Maplewood campus and about 13,500 statewide — will hinge in part on how it confronts its own history with these toxic chemicals.&nbsp;</span></p>
<p><span>A recent </span><i><span>Bloomberg</span></i><span> analysis </span><a target="_blank" href="https://news.bloomberglaw.com/litigation/3m-combat-earplug-fight-at-crossroads-as-court-strategies-falter"><span>estimated</span></a><span> 3M liabilities for the mass torts case and another over defective earplugs&nbsp;could reach $30 billion, or nearly half of market cap.&nbsp;</span></p>
    <h4>
What they knew, when they knew it
</h4>

	
<p><span>A key problem in any 3M defense: Despite the flurry of recent legal, regulatory and political activity, the chemicals’ dangers have been known —&nbsp;and known to 3M —&nbsp;for decades.&nbsp;</span></p>
<p><span>As early as the 1950s, 3M and DuPont scientists began discovering that the chemicals were accumulating in the bodies of humans and animals.&nbsp;</span></p>
<p><span>After compiling 27 million pages of documents and deposing about 200 witnesses in seven years, Minnesota’s former attorney general, Swanson, didn’t just walk away after settling with 3M. She released </span><a target="_blank" href="https://www.ag.state.mn.us/Office/Cases/3M/StatesExhibits.asp"><span>thousands of internal 3M documents</span></a><span>.&nbsp;</span></p>
<p><span>The </span><i><span>Reformer</span></i><span> reviewed the documents, which</span><span> show that company officials were repeatedly warned that the chemicals were accumulating in the environment and detected in the blood of humans and animals, while showing worrisome signs of toxicity.&nbsp;</span></p>
<p><span>Time and again, the company found reasons to delay a full accounting to government regulators, Minnesota communities, and even its own workers. Like tobacco companies’ tardy admission about its cancer-causing drug and the NFL’s approach to concussions, 3M </span><span>ignored, delayed, minimized and obscured research that raised red flags about the chemicals.&nbsp;</span></p>
<p><span>Internal 3M documents show:&nbsp;</span></p>
<ul>
<li><span> In the 1950s, 3M animal studies consistently found its PFAS chemicals were toxic.</span></li>
<li><span> By the early 1960s, 3M knew the chemicals didn’t degrade in the environment.</span></li>
<li><span> 3M knew by the 1970s its chemicals were widely present in the blood of the general U.S. population.</span></li>
<li><span> A 1970</span><a target="_blank" href="https://www.ag.state.mn.us/Office/Cases/3M/docs/PTX/PTX1083.pdf"> <span>study</span></a><span> of fish had to be abandoned “to avoid severe stream pollution” and because all the fish died. After being exposed to a chemical, the fish couldn’t stay upright and kept crashing into the fish tank and dying.</span></li>
<li><span> By 1976, 3M knew the chemicals were in its plant workers’ blood at higher levels than normal.</span></li>
<li><span> A study of a chemical’s effect on 20 rhesus monkeys in 1978</span><a target="_blank" href="https://www.ag.state.mn.us/Office/Cases/3M/docs/PTX/PTX1193.pdf"> <span>had to be aborted</span></a><span> after 20 days because all the exposed monkeys&nbsp; died.</span></li>
<li><span> In 1979, a 3M scientist warned that perfluorochemicals posed a cancer risk because they are “known to persist for a long time in the body and thereby give long-term chronic exposure.”</span></li>
<li><span> In 1979, 3M lawyers</span><a target="_blank" href="https://www.ag.state.mn.us/Office/Cases/3M/docs/PTX/PTX2534.pdf"> <span>advised</span></a><span> the company to conceal a 3M chemical compound found in human blood.</span></li>
<li><span> In 1983, 3M scientists</span><a target="_blank" href="https://www.ag.state.mn.us/Office/Cases/3M/docs/PTX/PTX1282.pdf"> <span>concluded</span></a><span> that concerns about its chemicals “give rise to legitimate questions about the persistence, accumulation potential, and ecotoxicity of fluorochemicals in the environment.”</span></li>
<li><span>Purdy wrote in his </span><a target="_blank" href="https://www.ag.state.mn.us/Office/Cases/3M/docs/PTX/PTX1001.pdf"><span>resignation letter</span></a><span> that in the 1990s, 3M told researchers not to write down their thoughts or have email discussions because of how their “speculations” might be viewed in legal discovery.</span></li>
<li><span> 3M told employees to mark documents as “attorney-client privileged” regardless of whether attorneys were involved, the state alleged, and minutes of meetings were edited to omit references to health hazards.</span></li>
<li><span> In 1997, 3M gave DuPont a “material safety data sheet” — which lays out potential hazards — for a chemical. It read, “Warning: contains a chemical which can cause cancer,” citing 1983 and 1993 studies by 3M and DuPont. But 3M removed the label that same year and continued to sell the products for decades without warning.</span></li>
</ul>
<p><span>Thompson, the 3M spokesman, said the documents released by Swanson portray an “incomplete and misleading story that distorts the full record regarding 3M’s PFAS stewardship and who we are as a company.”&nbsp;</span></p>
<p><span>He said 3M disclosed many studies to the EPA over the course of decades, including on the chemicals’ toxicity and “the materials produced and discussed with EPA addressed relevant information and issues.”&nbsp;</span></p>
	
    

	
    <h4>‘The wildest hellcat’</h4>

	
<p><span>3M’s man-made, toxic chemicals can be traced back to World War II, and the U.S. race to develop atomic weapons in the top-secret Manhattan Project.</span></p>
<p><span>Scientists used fluorine gas to separate uranium, and discovered that when fluorine weds with carbon, the bonds are almost impossible to break.</span></p>
<p><span>After the war, some of the Manhattan Project scientists were hired by the Minnesota Mining and Manufacturing Company (3M), which bought the patent to develop perfluorochemicals, according to a 3M</span><a target="_blank" href="https://www.ag.state.mn.us/Office/Cases/3M/docs/PTX/PTX1365.pdf"> <span>book</span></a><span> celebrating the company’s history of chemical engineering, called “A Chemical History of 3M.”&nbsp;</span></p>
<p><span>Figuring out how to handle fluorine was a major hurdle for the scientists.</span></p>
<p><span>“In its pure, uncontrolled state — fortunately never found in nature — it is one of the most active, most dangerous elements known to man,” the book says. “The greenish-yellow gas will burn steel, water and even asbestos, which earned it a nickname — the wildest hellcat. Strangely, its wildness contributes to fluorine’s unique stability when it is combined with certain compounds.”</span></p>
<p><span>When combined with carbon, the resulting fluorochemical can repel water and oil and withstand fire, which had obvious commercial potential.</span></p>
<p><span>3M began manufacturing chemicals in Minnesota in the 1950s, and for the next 50 years they were used to make stain repellents,</span> <span>Teflon and other waterproof and fireproof products.</span></p>
<figure id="attachment_19468"><a href="http://minnesotareformer.com/wp-content/uploads/2022/12/Screen-Shot-2022-12-14-at-3.05.16-PM.png" target="_blank" data-slb-active="1" data-slb-asset="355035654" data-slb-internal="0" data-slb-group="19467"><img decoding="async" src="http://minnesotareformer.com/wp-content/uploads/2022/12/Screen-Shot-2022-12-14-at-3.05.16-PM.png" alt="" width="818" height="570" srcset="https://minnesotareformer.com/wp-content/uploads/2022/12/Screen-Shot-2022-12-14-at-3.05.16-PM.png 818w, https://minnesotareformer.com/wp-content/uploads/2022/12/Screen-Shot-2022-12-14-at-3.05.16-PM-300x209.png 300w, https://minnesotareformer.com/wp-content/uploads/2022/12/Screen-Shot-2022-12-14-at-3.05.16-PM-768x535.png 768w" sizes="(max-width: 818px) 100vw, 818px"></a><figcaption><i></i>  This 1961 3M Scotchgard ad that ran in LIFE magazine was going to be an exhibit in the state’s lawsuit against 3M. Courtesy state of Minnesota</figcaption></figure>
<p><span>By the 1990s, the chemicals</span><a target="_blank" href="https://www.ag.state.mn.us/Office/Cases/3M/docs/PTX/PTX1386.pdf"> <span>were in many consumer products</span></a><span>, such as window cleaners, floor waxes and polishes, fabric and leather protective coatings and carpet and upholstery treatments.</span></p>
<p><span>The products were a huge success, and the company was making almost a half a billion dollars per year off them by 2000, when it began —&nbsp;at the EPA’s urging —&nbsp;to phase out production of the chemical used to make Scotchgard. Production of other chemicals continued.&nbsp;</span></p>
<p><span>But the chemicals wouldn’t go away easily: They don’t break down in the environment, and they accumulate in the human body.</span></p>
    <h4>
3M employee: We pled ignorance
</h4>

	
<p><span>In 1975, a Florida professor called 3M after he and two colleagues discovered a fluorine chemical in human blood samples from Texas and New York.</span></p>
<p><span>The scientists suspected the source might be 3M chemicals used in household items such as Teflon cookware and Scotchgard.</span></p>
<p><span>Donald Taves, a researcher at the University of Rochester, first reported in the scientific journal </span><i><span>Nature</span></i><span> in 1968 that the general population had been exposed to the compounds. Then Taves discovered his own blood contained it, according to a 3M </span><a target="_blank" href="https://www.ag.state.mn.us/Office/Cases/3M/docs/PTX/PTX1118.pdf"><span>document</span></a><span> marked “confidential,” obtained in the Minnesota attorney general’s lawsuit.</span></p>
<p><span>Taves was working with Warren Guy and Wallace Brey at the University of Florida on a research paper.&nbsp;</span></p>
<p><span>3M chemist G.H. Crawford took the phone call from Taves, and admitted nothing. He</span><a target="_blank" href="https://www.ag.state.mn.us/Office/Cases/3M/docs/PTX/PTX1118.pdf"> <span>wrote</span></a><span> in a confidential interoffice memo: “We (pleaded) ignorance but advised him that Scotchgard was a polymeric material not a F.C. acid.”</span></p>
<p><span>(In fact, by this point, the company knew its chemicals accumulated in the human body and were toxic, Swanson </span><a target="_blank" href="https://docs.house.gov/meetings/GO/GO28/20190910/109902/HHRG-116-GO28-Wstate-SwansonL-20190910.pdf"><span>told</span></a><span> a congressional committee. Moreover, Swanson added, 3M refused to identify the chemicals in its products, which for a generation thwarted the scientific community’s understanding of their health impacts.)&nbsp;</span></p>
<figure id="attachment_19466"><a href="http://minnesotareformer.com/wp-content/uploads/2022/12/3m-nocrop.jpg" target="_blank" data-slb-active="1" data-slb-asset="1522226148" data-slb-internal="0" data-slb-group="19467"><img decoding="async" loading="lazy" src="http://minnesotareformer.com/wp-content/uploads/2022/12/3m-nocrop-1024x575.jpg" alt="" width="1024" height="575" srcset="https://minnesotareformer.com/wp-content/uploads/2022/12/3m-nocrop-1024x575.jpg 1024w, https://minnesotareformer.com/wp-content/uploads/2022/12/3m-nocrop-300x168.jpg 300w, https://minnesotareformer.com/wp-content/uploads/2022/12/3m-nocrop-768x431.jpg 768w, https://minnesotareformer.com/wp-content/uploads/2022/12/3m-nocrop-1536x863.jpg 1536w, https://minnesotareformer.com/wp-content/uploads/2022/12/3m-nocrop.jpg 2048w" sizes="(max-width: 1024px) 100vw, 1024px"></a><figcaption><i></i>  3M still manufactures perfluorochemicals in Cottage Grove, as well as Cordova, Ill., Decatur, Ala., Zwijndrecht, Belgium, and Gendorf, Germany. Photo by Chad Davis</figcaption></figure>
<p><span>Crawford, the 3M scientist, suggested Guy get blood samples from “uncivilized areas” such as New Guinea “where they don’t use too much Teflon cookware or Scotchgard.”</span></p>
<p><span>He told his colleagues that the chemical 3M sold to DuPont to make Teflon cookware was the “least unlikely” explanation, but he didn’t tell Guy that. Crawford wrote that he “adopted a position of scientific curiosity and desire to assist in any way possible” and told Guy that 3M’s people might be able to “clarify” his study findings.&nbsp;</span></p>
<p><span>Another</span><a target="_blank" href="https://www.ag.state.mn.us/Office/Cases/3M/docs/PTX/PTX2771.pdf"> <span>internal document</span></a><span> shows Guy, the university researcher, also talked to a 3M employee identified as J.D. LaZerte about his quest to track down the source of chemicals in human blood.</span></p>
<p><span>LaZerte</span><a target="_blank" href="https://www.ag.state.mn.us/Office/Cases/3M/docs/PTX/PTX2771.pdf"> <span>wrote</span></a><span> in an internal document that he told Guy not to speculate.</span></p>
<p><span>Taves, Guy and Brey later</span><a target="_blank" href="https://www.ag.state.mn.us/Office/Cases/3M/docs/PTX/PTX1121.pdf"> <span>discovered</span></a><span> plasma from blood banks in five cities suggested “widespread contamination of human tissues with trace amounts of organic fluorocompounds derived from commercial products” such as floor waxes, wax paper, leather and fabric conditioning agents.</span></p>
<p><span>After getting the phone calls from researchers, 3M began analyzing its fluorine compounds. Within weeks,</span><a target="_blank" href="https://www.ag.state.mn.us/Office/Cases/3M/docs/PTX/PTX1123.pdf"><span> they found a compound that was </span><span>a likely match</span></a><span>.&nbsp;</span></p>
<p><span>By late 1975, 3M sent employees to see Guy and Taves at the University of Rochester, where they agreed to try to isolate and identify fluorochemicals in blood.</span></p>
<p><span>In 1976, the company began sampling employees’ blood.&nbsp;</span></p>
<p><span>Tests </span><a target="_blank" href="https://www.ag.state.mn.us/Office/Cases/3M/docs/PTX/PTX1144.pdf"><span>show</span></a><span>ed</span><span> workers at 3M’s Cottage Grove plant called Chemolite had up to 1,000 times the normal amount of fluorochemicals in their blood.</span></p>
<p><span>In plant after plant, elevated levels were found, from Decatur, Alabama, to Antwerp, Belgium.</span></p>
<p><span>Gergel, the federal judge in South Carolina, wrote in his recent ruling that although 3M helped Guy and Taves identify the compound found in blood, the company</span><span> told no one else outside 3M for nearly a <a target="_blank" href="https://www.ag.state.mn.us/Office/Cases/3M/docs/PTX/PTX1691.pdf">quarter century</a>, </span><span>despite the company’s legal duty to alert the EPA about potential harm to human health and the environment.</span></p>
<p><span>The judge cited a potential culprit: 3M lawyers, who urged 3M’s lab not to release the true identity of the compound (PFOS), according to an </span><a target="_blank" href="https://www.ag.state.mn.us/Office/Cases/3M/docs/PTX/PTX2534.pdf"><span>internal 3M documen</span></a><span>t.</span></p>
<p><span>Gergel said it would be reasonable to infer that the company knowingly withheld information that PFOS was in the blood of the general population and sought to discredit independent scientific work that would have disclosed this.</span></p>
<p><span>“3M did more than simply stay silent despite the company’s knowledge that the mystery compound was PFOS,” Gergel wrote.&nbsp;</span></p>
<p><span>The company went even further in its effort to obfuscate, the judge charged. In 1981, an author of an 1976 internal 3M report that confirmed that the unidentified chemical was in fact PFOS published an article in the same scientific journal as Guy and Taves stating that the mystery compound was not man-made but was a naturally occurring substance.&nbsp;</span></p>
    <h4>
DuPont asks 3M for ‘defensive information’
</h4>

	
<p><span>One of 3M’s biggest customers was DuPont, for which it produced chemicals to make Teflon products.</span></p>
<p><span>But by late 1975, DuPont was concerned about the possible toxic effects of Teflon and asked 3M for “defensive information” after a rat study found “sub-acute toxicity,” according to a</span><a target="_blank" href="https://www.ag.state.mn.us/Office/Cases/3M/docs/PTX/PTX1124.pdf"> <span>3M document</span></a><span>.&nbsp;</span></p>
<p><span>After a 1979 meeting between 3M and DuPont, a 3M committee decided its data on the chemicals in workers’ blood samples wasn’t important enough to notify the EPA. Minutes from the meeting said DuPont asked if 3M had done any “chronic studies” on fluorochemicals or planned any in the future. The answer was no, they wouldn’t do such studies unless forced to by regulators.</span></p>
<p><span>3M told DuPont that because they’d seen no adverse human health effects and no widespread potential for the chemicals to accumulate, they did not need to notify the EPA, according to a report by Philippe Grandjean, a Dutch scientist who provided expert testimony for the state of Minnesota in its case against 3M.</span></p>
<p><span>“3M either closed its eyes to the evidence, or chose purposefully not to find it, or being generous to 3M, it seems possible that 3M may have mistakenly relied on the absence of evidence, despite the old dictum that ‘the absence of evidence is not evidence of absence,’ which later became famous in U.S. politics,” Grandjean wrote.</span></p>
<figure id="attachment_19469"><a href="http://minnesotareformer.com/wp-content/uploads/2022/12/Screen-Shot-2022-12-14-at-3.00.50-PM.png" target="_blank" data-slb-active="1" data-slb-asset="31343240" data-slb-internal="0" data-slb-group="19467"><img decoding="async" loading="lazy" src="http://minnesotareformer.com/wp-content/uploads/2022/12/Screen-Shot-2022-12-14-at-3.00.50-PM.png" alt="" width="462" height="596" srcset="https://minnesotareformer.com/wp-content/uploads/2022/12/Screen-Shot-2022-12-14-at-3.00.50-PM.png 462w, https://minnesotareformer.com/wp-content/uploads/2022/12/Screen-Shot-2022-12-14-at-3.00.50-PM-233x300.png 233w" sizes="(max-width: 462px) 100vw, 462px"></a><figcaption><i></i>  This 1961 Scotchgard ad in LIFE magazine was going to be an exhibit in the state’s lawsuit against 3M. Courtesy state of Minnesota</figcaption></figure>
    <h4>
Employees notified of chemicals in blood
&nbsp;
</h4>

	
<p><span>In 1978, 3M began notifying chemical workers that trace amounts of chemicals were found in the blood of employees at the Cottage Grove, Decatur and Cordova</span> <span>plants.</span></p>
<p><span>“There did not appear to be any significant grouping of abnormalities,” according to</span><a target="_blank" href="https://www.ag.state.mn.us/Office/Cases/3M/docs/PTX/PTX1168.pdf"><span> confidential </span><span>meeting minutes </span></a><span>of 3M’s Fluorochemicals Technical Review Committee.</span></p>
<p><span>The committee discussed the potential carcinogenicity of the chemicals, and whether to notify workers and “the appropriate government agency,” given studies showing a PFAS compound was toxic in animals and a 1979</span><a target="_blank" href="https://www.ag.state.mn.us/Office/Cases/3M/docs/PTX/PTX1199.pdf"> <span>report on toxicity studies</span></a><span> on monkeys and rats found PFOS was “certainly more toxic than anticipated.”&nbsp;&nbsp;</span></p>
<p><span>But because there was “no evidence of ill effects,” the committee decided it didn’t constitute a substantial risk based on EPA guidelines pertaining to the Toxic Substances Control Act, which regulates chemicals.</span></p>
<p><span>The committee decided to keep exposure to all fluorochemicals to a minimum in all factory operations, and look into monitoring employee urine.</span></p>
<p><span>But it was becoming increasingly clear that several of the chemicals were toxic. Soon after,</span><a target="_blank" href="https://www.ag.state.mn.us/Office/Cases/3M/docs/PTX/PTX1179.pdf"> <span>a 3M study</span></a><span> of two chemicals found they were “likely to persist in the environment for extended periods.”</span></p>
<p><span>“Because of the apparent persistence of these fluorochemicals in the body, the most important question remains possible long-term effects,” the report said.</span></p>
    <h4>
Prominent toxicologist warns ‘we could have a serious problem’
</h4>

	
<p><span>In the spring of 1979, 3M officials met at the Hilton Hotel in San Francisco to talk about their fluorochemical studies and the future.</span></p>
<p><span>They also heard from toxicologist Harold Hodge, a professor from the University of California, which</span><a target="_blank" href="https://oac.cdlib.org/view?docId=hb5f59n9gs&amp;doc.view=frames&amp;chunk.id=div00015&amp;toc.depth=1&amp;toc.id="> <span>dubbed</span></a><span> him “the dean of American toxicology.”</span></p>
<p><span>An epidemiology study was being done on 3,500 people, but so far there were no “unusual” causes of death.</span></p>
<p><span>Hodge recommended the company study the carcinogenicity of its chemicals.</span></p>
<p><span>A week later, Hodge asked that 3M add to the</span><a target="_blank" href="https://www.ag.state.mn.us/Office/Cases/3M/docs/PTX/PTX1204.pdf"> <span>meeting</span></a><span> minutes that it was of “utmost importance” that the company study whether a certain chemical was present in humans, at what level, and the degree of its persistence.</span></p>
<p><span>“If the levels are high and widespread and the half-life is long, we could have a serious problem,” Hodge warned.</span></p>
<p><span>Months later, 3M scientist M.T. Case</span><a target="_blank" href="https://www.ag.state.mn.us/Office/Cases/3M/docs/PTX/PTX1212.pdf"> <span>expressed similar concerns</span></a><span> — as “responsible 3M scientists” — about the lack of chronic toxicity data more than one year after the rat studies were done.</span></p>
<p><span>“I believe it is paramount to begin now an assessment of the potential (if any) of long term (carcinogenic) effects for these compounds which are known to persist for a long time in the body and thereby give long term chronic exposure,” Case wrote in a memo.</span></p>
    <h4>‘3M will likely be embarrassed’</h4>

	
<p><span>Other 3M employees were trying to persuade the company to come clean.</span></p>
<p><span>After a California company bought firefighting foam from 3M, it later learned that 3M chemist Eric Reiner told the client that the foam wasn’t biodegradable, contrary to 3M’s advertising claims.&nbsp;</span></p>
<p><span>Furious, the client </span><a target="_blank" href="https://docs.house.gov/meetings/GO/GO28/20190910/109902/HHRG-116-GO28-Wstate-SwansonL-20190910.pdf"><span>wrote</span></a><span> to 3M in 1988, demanding an explanation.</span></p>
<p><span>Reiner </span><a target="_blank" href="https://www.ag.state.mn.us/Office/Cases/3M/docs/PTX/PTX1351.pdf"><span>implored</span></a><span> company officials to do tests on the biodegradability of the chemicals, calling out those responsible in an internal memo.</span></p>
<p><span>“I don’t think it is in 3M’s long-term interest to perpetuate the myth that these fluorochemical surfactants are biodegradable,” he wrote. “It is probable that this misconception will eventually be discovered, and when that happens, 3M will likely be embarrassed, and we and our customers may be fined and forced to immediately withdraw products from the market.”</span></p>
<p><span>Three years later, company officials were still debating whether to study the environmental effects of fluorochemicals. A</span><a target="_blank" href="https://www.ag.state.mn.us/Office/Cases/3M/docs/PTX/PTX1372.pdf"> <span>draft proposal</span></a><span> for a study of long-term effects noted the problem with previous studies was there’s rarely a single fluorochemical in the product, making generalizations difficult.</span></p>
<p><span>“Perhaps the most important conclusion from previous studies is the stability of fluorochemicals although stability is one of the most desirable properties fluorochemicals possess,” it said. “For many applications, from an environmental perspective, stability connotes persistence which can be the cause of concern especially when coupled with other properties… taken together, stability, the tendency to bioaccumulate, and biological activity are a potentially troublesome combination.”</span></p>
    <h4>
3M vice president delays reporting to EPA
</h4>

	
<p><span>By the mid-1990s, that “potentially troublesome combination” was becoming a threat to 3M.</span></p>
<p><span>The company’s Toxic Substances Control Act committee recommended in 1998 that 3M notify the EPA and FDA that the chemicals were widely found in human blood.</span></p>
<p><span>A “</span><a target="_blank" href="https://www.ag.state.mn.us/Office/Cases/3M/docs/PTX/PTX1488.pdf"><span>communications plan</span></a><span>” included steps for an “orderly exit” from the market.</span></p>
<p><span>But one month later, 3M Group Vice President Charles Reich</span><a target="_blank" href="https://www.ag.state.mn.us/Office/Cases/3M/docs/PTX/PTX1496.pdf"><span> told</span></a><span> the committee he decided instead to do a review with a “wider spectrum” of internal and external experts.&nbsp;</span></p>
<p><span>“I have concluded that 3M is not presently in possession of information that would be new to EPA and that reasonably supports a conclusion that suggests a substantial risk of injury to human health or the environment,” he wrote.</span></p>
<p><span>This, despite decades of research suggesting otherwise.</span></p>
<p><span>3M finally </span><a target="_blank" href="https://static.ewg.org/reports/2020/pfas-epa-timeline/1998_3M-Alerts-EPA.pdf"><span>notified</span></a><span> the EPA in May 1998 that a fluorochemical (PFOS) was found in the general population’s blood at “very low” levels. The company said its studies of 3M workers found “no adverse effects,” saying, “3M does not believe that any reasonable basis exists to conclude that PFOS presents a substantial risk of injury to health or the environment.”</span></p>
<p><span>Judge Gergel recently noted that despite those assurances, 3M’s manager of corporate </span><span>toxicology, John Butenhoff, urged 3M in 1998 to replace “PFOS-based chemistry as these compounds [are] VERY persistent and thus insidiously toxic.”&nbsp;</span></p>
<p><span>Butenhoff calculated a “safe” level of PFOS in human blood at a little more than 1 part per billion. But 3M’s </span><i><span>own studies</span></i><span> from roughly the same period found that PFOS concentrations in the blood of the general public were in the range of 30 parts per billion.&nbsp;</span></p>
<p><span>Gergel said Butenhoff’s findings were never reported to the EPA and were revealed only during discovery in the firefighting foam litigation.&nbsp;</span></p>
    <h4>
‘This chemical is more stable than many rocks’
</h4>

	
<p><span>By 1998, 3M toxicologist</span> <span>Richard Purdy, the one studying chemicals in eagles and albatrosses, was growing increasingly concerned about those studies of wild birds.&nbsp;</span></p>
<p><span>On Dec. 3, 1998, Purdy said in an</span><a target="_blank" href="https://www.ag.state.mn.us/Office/Cases/3M/docs/PTX/PTX1533.pdf"> <span>email</span></a><span> there was a significant risk of ecological harm, which should be reported to the EPA, warning, “The levels we are seeing in eagles and other biota is likely to climb each year.”</span></p>
<p><span>He wasn’t alone.</span></p>
<p><span>In March 1999, a 3M worker </span><a target="_blank" href="https://www.ag.state.mn.us/Office/Cases/3M/docs/PTX/PTX1003.pdf"><span>emailed</span></a><span> several colleagues and 3M’s general counsel, Thomas J. DiPasquale, questioning why three months had passed since a committee had reviewed Purdy’s hypothesis on food chain contamination.</span></p>
<p><span>DiPasquale wasn’t in a hurry, though.</span></p>
<p><span>“I’m not sure there is a need to support or refute the hypothesis within any particular time frame,” he replied.</span></p>
<p><span>Purdy, who was on the email chain,</span><a target="_blank" href="https://www.ag.state.mn.us/Office/Cases/3M/docs/PTX/PTX1003.pdf"> <span>retorted</span></a><span>: “Plan! That is the same stalling technique you have been using for the last year.”</span></p>
<p><span>“There is a high probability that PFOS is killing marine mammals and you want another plan when we could have had data to support the risk assessment long ago,” Purdy wrote. “You were given a plan in 1983. Again in the early 90s. And you authorized no testing.”</span></p>
<figure id="attachment_19470"><a href="http://minnesotareformer.com/wp-content/uploads/2022/12/Screen-Shot-2022-12-14-at-3.03.07-PM.png" target="_blank" data-slb-active="1" data-slb-asset="1682286043" data-slb-internal="0" data-slb-group="19467"><img decoding="async" loading="lazy" src="http://minnesotareformer.com/wp-content/uploads/2022/12/Screen-Shot-2022-12-14-at-3.03.07-PM.png" alt="" width="634" height="510" srcset="https://minnesotareformer.com/wp-content/uploads/2022/12/Screen-Shot-2022-12-14-at-3.03.07-PM.png 634w, https://minnesotareformer.com/wp-content/uploads/2022/12/Screen-Shot-2022-12-14-at-3.03.07-PM-300x241.png 300w" sizes="(max-width: 634px) 100vw, 634px"></a><figcaption><i></i>  This undated photograph showing open burning of drums in a landfill was an exhibit in the state lawsuit against 3M. Courtesy state of Minnesota.</figcaption></figure>
<p><span>Meanwhile, his preliminary research indicated adult eagles had 50 times as much PFOS in their plasma as the eaglets.</span></p>
<p><span>“For 20 years the division has been stalling the collection of data needed for evaluating the environmental impact of fluorochemicals,” Purdy wrote. “PFOS is the most onerous pollutant since PCB and you want to avoid collecting data that indicates that it is probably worse. I am outraged.”</span></p>
<p><span>Two days later, Purdy resigned, and forwarded his</span><a target="_blank" href="https://www.ag.state.mn.us/Office/Cases/3M/docs/PTX/PTX1001.pdf"> <span>resignation letter</span></a><span> to the EPA.</span></p>
<p><span>“I have continually met roadblocks, delays, and indecision. For weeks on end I have received assurances that my samples would be analyzed soon — never to see results. There are always excuses and little is accomplished,” he wrote.</span></p>
<p><span>3M continued to make the chemicals after Purdy warned they were spreading through the food chain and harming sea mammals.</span></p>
<p><span>“This chemical is more stable than many rocks,” he wrote. “And the chemicals the company is considering for replacement are just as stable and biologically available. The risk assessment I performed was simple, and not worst case.”</span></p>
<p><span>3M told the people working on the fluorochemical project not to write down their thoughts or have email discussions because of how their speculation could be viewed in potential litigation, Purdy alleged.</span></p>
<p><span>“For me it is unethical to be concerned with markets, legal defensibility and image over environmental safety,” he wrote.</span></p>
<p><span>Purdy did not respond to a request for comment, but his view of 3M’s behavior seemed to soften over time. In an interview with MPR from his Wisconsin farm in 2005, he spoke “with pride” about the company’s investment in science and chemicals.&nbsp;</span></p>
<p><span>“3M is like somebody who ran the stop sign, got through the stop sign, ‘Oh my God,’ and stopped,” he was quoted saying.</span></p>
    <h4>
3M begins working to ‘command the science’
</h4>

	
<p><span>With the EPA on notice, the agency pressured 3M to stop manufacturing the compound used in Scotchgard (PFOS) in the U.S. in 2000. Six years later, the EPA fined the company for not turning over hundreds of reports on the chemicals’ toxicity.</span></p>
<p><span>The EPA said 3M’s own data indicated its chemicals didn’t break down and could pose a long-term threat to human health and the environment.</span></p>
<p><span>Still, the Minnesota Pollution Control Agency didn’t begin investigating the chemicals for two years, according to</span><a target="_blank" href="https://www.ag.state.mn.us/Office/Cases/3M/docs/PTX/PTX2004.pdf"> <span>MPR</span></a><span>, which reported that all the agency had on file for 3M’s Cottage Grove plant in 2001 was a press clipping headlined “Scotchgard sticks in the environment.”</span></p>
<p><span>Once 3M had finally alerted regulators, the company worked on a</span><a target="_blank" href="https://www.ag.state.mn.us/Office/Cases/3M/docs/PTX/PTX1587.pdf"> <span>communications plan</span></a><span>.&nbsp;</span></p>
<p><span>The first goal: “Protect and enhance 3M’s reputation.”</span></p>
<p><span>Indeed, its primary concern seemed to be controlling the narrative around the science. The plan included a list of “high-priority” candidates to be spokespersons for the company, including Michigan State University professor John Giesy, a 3M advisor on environmental studies. 3M employee Dale Bacon said he would gauge Giesy’s interest.</span></p>
<p><span>3M wanted to get scientific papers on their chemicals published before others, according to</span><a target="_blank" href="https://www.ag.state.mn.us/Office/Cases/3M/docs/PTX/PTX1740.pdf"> <span>internal emails</span></a><span>.</span></p>
<p><span>A 2003</span><a target="_blank" href="https://www.ag.state.mn.us/Office/Cases/3M/docs/PTX/PTX2604.pdf"> <span>internal memo</span></a><span> showed 3M looking to fund outside research using 3M “grant” money, particularly with people who would be influential in risk assessment and “other science policy matters.”</span></p>
<p><span>Among their action items: Develop a list of 3M and “industry-preferred” nominees for science advisory panels.</span></p>
<p><span>Giesy was the ideal candidate. He was editor of more than half the academic journals about PFAS and considered an independent expert.&nbsp;</span></p>
<p><span>3M went on to pay Giesy to review and share studies with 3M before they were published, Minnesota alleged in its lawsuit against 3M.</span></p>
<p><span>It began when Giesy</span><a target="_blank" href="https://www.ag.state.mn.us/Office/Cases/3M/docs/PTX/PTX1740.pdf"> <span>emailed</span></a><span> 3M officials in August 2000 informing the company he had a draft manuscript ready and wanted to submit it to </span><i><span>Science</span></i><span> before others beat him to it.</span></p>
<p><span>“I think it is important to publish our work before theirs,” Giesy wrote. “Otherwise, it looks like we (ie 3M) was pressured into the investigations they have done and subsequent release of the data.”</span></p>
<p><span>A 3M official warned his colleagues that publishing the paper “could set off a chain reaction of speculation that could reopen the issue with the media and move it back to a health story; something up-to-now we have avoided.”</span></p>
<p><span>Instead, the 3M official wrote that the company should keep “our” scientific publications “in the right order as we had already agreed,” noting he presumed Giesy’s work was done under contract with 3M and was only publishable “if and when we agree.”</span></p>
<p><span>The official added, however: “We also can’t dilly dally around either. It will take a great deal of sensitivity and people skills to bring Dr. Giesy around to our thinking on this and to be sure he doesn’t misinterpret our position as trying to hide the winnie. We just want the winnie in the bun, complete with mustard and ketchup.”</span></p>
<p><span>3M went on to develop a campaign to “command the science” and create “defensive barriers to litigation,” the state alleged in its lawsuit, by selectively funding outside research and editing scientific papers before they were published.</span></p>
<p><span>“The company, unfortunately, engaged in a campaign to hide its own studies and to, in fact, shape the science through the funding of these other studies,” Swanson told Congress.</span></p>
<p><span>Giesy explained how it worked in a</span><a target="_blank" href="https://www.ag.state.mn.us/Office/Cases/3M/docs/PTX/PTX2204.pdf"> <span>March 2008 email</span></a><span> to 3M Laboratory Manager William Reagen: He edited a lot of PFAS papers for scientific journals, but in his 3M billings, he listed the work as “literature searches” on timesheets “so that there was no paper trail to 3M.”</span></p>
<p><span>“Some journals will allow this, but others, for conflict of interest issues, will not allow an industry to review a paper about one of their products,” he wrote. “That is where I came in for Dale (Bacon, the 3M employee).”</span></p>
<p><span>Giesy</span><a target="_blank" href="https://www.ag.state.mn.us/Office/Cases/3M/docs/PTX/PTX2209.pdf"> <span>said</span></a><span> in a later email “Dale (Bacon) had me doing things to keep a finger on the pulse of things going on around the world, especially to try to keep bad papers out of the literature.”</span></p>
<p><span>The state lawsuit alleged 3M paid Giesy at least $2 million, and that he had a net worth of about $20 million despite working at public universities most of his career.</span></p>
<p><span>3M</span><a target="_blank" href="https://www.ag.state.mn.us/Office/Cases/3M/docs/PTX/PTX2773.pdf"> <span>records</span></a><span> show he was first paid by the company in 1993. Beginning in 1998, Entrix, Inc. — Giesy’s environmental consulting company — was paid nearly $1.7 million for his work through 2009, at a rate of $275 an hour, according to one</span><a target="_blank" href="https://www.ag.state.mn.us/Office/Cases/3M/docs/PTX/PTX2102.pdf"> <span>billing</span></a><span>.</span></p>
<p><span>By 2008, the arrangement appeared to be ending. In an email, Giesy offered some closing words:&nbsp;</span></p>
<p><span>“My personal advise (sic) is that you want to keep ‘bad’ papers out of the literature, otherwise in litigation situations they can be a large obstacle to refute,” he wrote. “Judges seem to be of the opinion that if information is in the peer-reviewed, open literature, it is accurate.”</span></p>
<p><span>Giesy — who now works at the University of Saskatchewan — did not respond to multiple requests for comment, but in the past he has</span><a target="_blank" href="https://www.cbc.ca/news/canada/saskatoon/u-of-s-professor-denies-suppressing-toxic-pollution-research-for-3m-1.4554634"> <span>denied</span></a><span> any wrongdoing. He said he was only trying to keep mistakes out of the literature — and accused Swanson of trying to smear his reputation because he refused to be an expert for the state.</span></p>
<p><span>“The documents speak for themselves,” Swanson said in an interview.</span></p>
    <h4>
Goal: ‘Sell PFCs as long and as broadly as we can’
</h4>

	
<p><span>For more than a quarter century, 3M has known its fluorochemicals could have devastating consequences for the company’s long-term financial health.&nbsp;</span></p>
<p><span>A 1995 internal</span><a target="_blank" href="https://www.ag.state.mn.us/Office/Cases/3M/docs/PTX/PTX1445.pdf"> <span>strategic planning document</span></a><span> said “obstacle No. 1” to 3M’s major vision in its chemical business was “the persistence of fluorochemicals,” and “environmental, health, safety and regulatory issues and trends that threaten to limit our business.”</span></p>
<p><span>Among the “key actions” listed: “Continue to maintain regulatory approval to sell PFCs as long and as broadly as we can.”</span></p>
<p><span>It’s easy to understand why they were so committed to the chemicals, despite the massive risks: $500 million per year in revenue, year after year after year.&nbsp;</span></p>
<p><span>“Unfortunately, it succeeded for more than 50 years,” Swanson told Congress. “And now states and local governments around the nation are grappling with the consequences.”</span></p>
<p><span>To this day, 3M still manufactures perfluorochemicals in Cottage Grove, as well as Cordova, Ill., Decatur, Ala., Zwijndrecht, Belgium, and Gendorf, Germany.</span></p>
                                </div><div>
                                        
                                        <p>Our stories may be republished online or in print under Creative Commons license CC BY-NC-ND 4.0. We ask that you edit only for style or to shorten, provide proper attribution and link to our web site. Please see our republishing guidelines for use of photos and graphics.</p>
                                    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[AMD's CDNA 3 Compute Architecture (118 pts)]]></title>
            <link>https://chipsandcheese.com/2023/12/17/amds-cdna-3-compute-architecture/</link>
            <guid>38675258</guid>
            <pubDate>Sun, 17 Dec 2023 18:51:43 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://chipsandcheese.com/2023/12/17/amds-cdna-3-compute-architecture/">https://chipsandcheese.com/2023/12/17/amds-cdna-3-compute-architecture/</a>, See on <a href="https://news.ycombinator.com/item?id=38675258">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
<p>AMD has a long history of vying for GPU compute market share. Ever since Nvidia got first dibs with their Tesla architecture, AMD has been playing catch up. Terascale 3 moved from VLIW5 to VLIW4 to improve execution unit utilization in compute workloads. GCN replaced Terascale and emphasized consistent performance for both GPGPU and graphics applications. Then, AMD diverged their GPU architecture development into separate CDNA and RDNA lines specialized for compute and graphics respectively. </p>
<div>
<figure><a href="https://chipsandcheese.com/?attachment_id=24552"><img decoding="async" width="664" height="411" data-attachment-id="24552" data-permalink="https://chipsandcheese.com/2023/12/17/amds-cdna-3-compute-architecture/amd_gpu_lines/" data-orig-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/12/amd_gpu_lines.jpg?fit=664%2C411&amp;ssl=1" data-orig-size="664,411" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="amd_gpu_lines" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/12/amd_gpu_lines.jpg?fit=664%2C411&amp;ssl=1" data-large-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/12/amd_gpu_lines.jpg?fit=664%2C411&amp;ssl=1" src="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/12/amd_gpu_lines.jpg?resize=664%2C411&amp;ssl=1" alt="" data-recalc-dims="1"></a></figure></div>
<p>CDNA 2 finally brought AMD notable success. MI250X and MI210 GPUs won several supercomputer contracts including ORNL’s Frontier, which holds first place on November 2023’s TOP500 list. But while CDNA2 delivered solid and cost efficient FP64 compute, H100 had better AI performance and offered a larger unified GPU.</p>
<p>CDNA 3 looks to close those gaps by bringing forward everything AMD has to offer. The company’s experience in advanced packaging technology is on full show, with MI300X getting a sophisticated chiplet setup. Together with Infinity Fabric components, advanced packaging lets MI300X scale to compete with Nvidia’s largest GPUs. On the memory side, Infinity Cache from the RDNA line gets pulled into the CDNA world to mitigate bandwidth issues. But that doesn’t mean MI300X is light on memory bandwidth. It still gets a massive HBM setup, giving it the best of both worlds. Finally, CDNA 3’s compute architecture gets significant generational improvements to boost throughput and utilization.</p>
<h2>GPU Layout</h2>
<p>AMD has a tradition of using chiplets to cheaply scale core counts in their Ryzen and Epyc CPUs. MI300X uses a similar strategy at a high level, with compute split off onto Accelerator Complex Dies, or XCDs. XCDs are analogous to CDNA 2 or RDNA 3’s Graphics Compute Dies (GCDs) or Ryzen’s Core Complex Dies (CCDs). AMD likely changed the naming because CDNA products lack the dedicated graphics hardware present in the RDNA line.</p>
<div>
<figure><a href="https://chipsandcheese.com/?attachment_id=24592"><img decoding="async" width="688" height="384" data-attachment-id="24592" data-permalink="https://chipsandcheese.com/2023/12/17/amds-cdna-3-compute-architecture/mi300_xcd_slide/" data-orig-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/12/mi300_xcd_slide.jpg?fit=1275%2C711&amp;ssl=1" data-orig-size="1275,711" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="mi300_xcd_slide" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/12/mi300_xcd_slide.jpg?fit=1275%2C711&amp;ssl=1" data-large-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/12/mi300_xcd_slide.jpg?fit=688%2C384&amp;ssl=1" src="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/12/mi300_xcd_slide.jpg?resize=688%2C384&amp;ssl=1" alt="" srcset="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/12/mi300_xcd_slide.jpg?w=1275&amp;ssl=1 1275w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/12/mi300_xcd_slide.jpg?resize=768%2C428&amp;ssl=1 768w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/12/mi300_xcd_slide.jpg?resize=1200%2C669&amp;ssl=1 1200w" sizes="(max-width: 688px) 100vw, 688px" data-recalc-dims="1"></a></figure></div>
<p>Each XCD contains a set of cores and a shared cache. Specifically, every XCD physically has 40 CDNA 3 Compute Units, with 38 of these being enabled per XCD on the MI300X. A 4 MB L2 cache sits on the XCD as well, and serves all of the die’s CUs. MI300X has eight XCDs, giving it 304 total Compute Units. </p>
<div>
<figure><a href="https://chipsandcheese.com/?attachment_id=24645"><img decoding="async" width="635" height="499" data-attachment-id="24645" data-permalink="https://chipsandcheese.com/2023/12/17/amds-cdna-3-compute-architecture/mi300x_drawio/" data-orig-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/12/mi300x_drawio.jpg?fit=635%2C499&amp;ssl=1" data-orig-size="635,499" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="mi300x_drawio" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/12/mi300x_drawio.jpg?fit=635%2C499&amp;ssl=1" data-large-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/12/mi300x_drawio.jpg?fit=635%2C499&amp;ssl=1" src="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/12/mi300x_drawio.jpg?resize=635%2C499&amp;ssl=1" alt="" data-recalc-dims="1"></a></figure></div>
<p>That’s a large increase over the MI250X’s 220 CUs. Even better, MI300X can expose all of those CUs as a single GPU. On MI250X, a programmer would have to manually split up work across the two GPUs because each has a separate pool of memory.</p>
<div>
<figure><a href="https://chipsandcheese.com/?attachment_id=24647"><img loading="lazy" decoding="async" width="553" height="418" data-attachment-id="24647" data-permalink="https://chipsandcheese.com/2023/12/17/amds-cdna-3-compute-architecture/mi250x_drawio/" data-orig-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/12/mi250x_drawio.jpg?fit=553%2C418&amp;ssl=1" data-orig-size="553,418" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="mi250x_drawio" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/12/mi250x_drawio.jpg?fit=553%2C418&amp;ssl=1" data-large-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/12/mi250x_drawio.jpg?fit=553%2C418&amp;ssl=1" src="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/12/mi250x_drawio.jpg?resize=553%2C418&amp;ssl=1" alt="" srcset="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/12/mi250x_drawio.jpg?w=553&amp;ssl=1 553w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/12/mi250x_drawio.jpg?resize=200%2C150&amp;ssl=1 200w" sizes="(max-width: 553px) 100vw, 553px" data-recalc-dims="1"></a></figure></div>
<p>Nvidia’s H100 consists of 132 Streaming Multiprocessors (SMs) and also presents them to programmers as a big unified GPU. H100 takes a conventional approach by implementing all of that compute on a large monolithic die. Even with everything on the same die, H100 is too large to give all of its SMs equal access to cache. So, H100 splits the L2 into two instances. A single SM can use all 50 MB of L2, but access to more than 25 MB will incur a performance penalty.</p>
<div>
<figure><a href="https://chipsandcheese.com/?attachment_id=24646"><img loading="lazy" decoding="async" width="688" height="337" data-attachment-id="24646" data-permalink="https://chipsandcheese.com/2023/12/17/amds-cdna-3-compute-architecture/h100_drawio/" data-orig-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/12/h100_drawio.jpg?fit=787%2C386&amp;ssl=1" data-orig-size="787,386" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="h100_drawio" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/12/h100_drawio.jpg?fit=787%2C386&amp;ssl=1" data-large-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/12/h100_drawio.jpg?fit=688%2C337&amp;ssl=1" src="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/12/h100_drawio.jpg?resize=688%2C337&amp;ssl=1" alt="" srcset="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/12/h100_drawio.jpg?w=787&amp;ssl=1 787w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/12/h100_drawio.jpg?resize=768%2C377&amp;ssl=1 768w" sizes="(max-width: 688px) 100vw, 688px" data-recalc-dims="1"></a></figure></div>
<p>Still, Nvidia’s strategy makes more efficient use of cache capacity than MI300X’s. A MI300X XCD doesn’t use L2 capacity on other XCDs for caching, just as CCDs on Epyc/Ryzen don’t allocate into each other’s L3 caches.</p>
<p>Intel’s Ponte Vecchio (PVC) compute GPUs make for a very interesting comparison. PVC places its basic compute building blocks in dies called Compute Tiles, which are roughly analogous to CDNA 3’s XCDs. Similarly, PVC’s Base Tile serves a similar function to CDNA 3’s IO dies. Both contain a large last level cache and HBM memory controllers. Like MI300X, a Ponte Vecchio card can be exposed as a single GPU with a unified memory pool.</p>
<div>
<figure><a href="https://chipsandcheese.com/?attachment_id=24652"><img loading="lazy" decoding="async" width="451" height="339" data-attachment-id="24652" data-permalink="https://chipsandcheese.com/2023/12/17/amds-cdna-3-compute-architecture/pvc_drawio/" data-orig-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/12/pvc_drawio.jpg?fit=451%2C339&amp;ssl=1" data-orig-size="451,339" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="pvc_drawio" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/12/pvc_drawio.jpg?fit=451%2C339&amp;ssl=1" data-large-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/12/pvc_drawio.jpg?fit=451%2C339&amp;ssl=1" src="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/12/pvc_drawio.jpg?resize=451%2C339&amp;ssl=1" alt="" srcset="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/12/pvc_drawio.jpg?w=451&amp;ssl=1 451w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/12/pvc_drawio.jpg?resize=400%2C300&amp;ssl=1 400w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/12/pvc_drawio.jpg?resize=200%2C150&amp;ssl=1 200w" sizes="(max-width: 451px) 100vw, 451px" data-recalc-dims="1"></a></figure></div>
<p>However, there are important differences. Ponte Vecchio’s Compute Tiles are smaller with only eight Xe Cores, compared to 38 Compute Units on a CDNA 3 XCD. Instead of using a Compute Tile wide cache, Intel uses larger L1 caches to reduce cross-die traffic demands. Using a two-stack Ponte Vecchio part as a unified GPU presents challenges too. The EMIB bridge between the two stacks <a href="https://www.intel.com/content/www/us/en/developer/articles/technical/intel-data-center-gpu-max-series-overview.html#gs.1p3uqo">only offers 230 GB/s of bandwidth</a>, which isn’t enough to fully utilize HBM bandwidth if accesses are striped across all memory controllers. To address this, Intel has APIs that can let programs work with the GPU in a NUMA configuration. </p>
<p>In terms of physical construction, PVC and CDNA 3’s designs have different challenges. CDNA 3’s ability to present a unified memory pool with HBM requires high bandwidth between the IO dies. PVC gets by with a relatively low bandwidth EMIB link. But PVC’s design gets complicated because it uses four die types with different process nodes and foundries. AMD only uses two die types in MI300X, and both nodes (6 nm and 5 nm) are from TSMC.</p>
<h2>Tackling the Bandwidth Problem</h2>
<p>Compute has been outpacing memory for decades. Like CPUs, GPUs have countered this with increasingly sophisticated caching strategies. CDNA 2 used a conventional two-level cache hierarchy with a 8 MB L2, relying on HBM2e to keep the execution units fed. But even with HBM2e, MI250X was more bandwidth starved than Nvidia’s H100. If AMD simply added more compute, bandwidth starvation could be come a serious issue. So, AMD took a leaf out of RDNA(2)’s book and added an “Infinity Cache”.</p>
<p>Much like the consumer RDNA GPUs, MI300’s Infinity Cache is what the technical documentation calls Memory Attached Last Level (MALL), which is a fancy way to say that the last level cache level is a memory side cache. Compared to L1 and L2 caches that are closer to the Compute Units, the Infinity Cache is attached to the memory controllers. All memory traffic passes through the Infinity Cache regardless of what block it’s coming from. That includes IO traffic, so communications between peer GPUs can benefit from Infinity Cache bandwidth. Because the Infinity Cache always has the most up to date view of DRAM contents, It doesn’t have to handle snoops or other cache maintenance operations.</p>
<div>
<figure><a href="https://chipsandcheese.com/polaris_l2_clients/"><img loading="lazy" decoding="async" width="688" height="385" data-attachment-id="24743" data-permalink="https://chipsandcheese.com/polaris_l2_clients/" data-orig-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/12/polaris_l2_clients.png?fit=1275%2C713&amp;ssl=1" data-orig-size="1275,713" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="polaris_l2_clients" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/12/polaris_l2_clients.png?fit=1275%2C713&amp;ssl=1" data-large-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/12/polaris_l2_clients.png?fit=688%2C385&amp;ssl=1" src="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/12/polaris_l2_clients.png?resize=688%2C385&amp;ssl=1" alt="" srcset="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/12/polaris_l2_clients.png?w=1275&amp;ssl=1 1275w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/12/polaris_l2_clients.png?resize=768%2C429&amp;ssl=1 768w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/12/polaris_l2_clients.png?resize=1200%2C671&amp;ssl=1 1200w" sizes="(max-width: 688px) 100vw, 688px" data-recalc-dims="1"></a><figcaption>From AMD’s presentation on their RDNA architecture. L2 slices may be associated with memory controllers, but the L2 is not a memory side cache because many agents can write to DRAM without going through L2</figcaption></figure></div>
<p>But because a memory side cache is farther away from compute, it generally suffers from higher latency. Therefore, AMD has multi-megabyte L2 caches on both CDNA 3 and RDNA 2 to insulate compute from the lower performance of a memory side cache.</p>
<p>Like RDNA 2, CDNA 3’s Infinity Cache is 16-way set associative. However, CDNA 3’s implementation is more optimized for bandwidth than capacity. It’s composed of 128 slices, each with 2 MB of capacity and 64 bytes per cycle of read bandwidth. All of the slices together can deliver 8192 bytes per cycle, which is good for 17.2 TB/s at 2.1 GHz.</p>
<div>
<figure><a href="https://chipsandcheese.com/cdna3_cache_hierarchy_compared/"><img loading="lazy" decoding="async" width="688" height="554" data-attachment-id="24577" data-permalink="https://chipsandcheese.com/cdna3_cache_hierarchy_compared/" data-orig-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/12/cdna3_cache_hierarchy_compared.jpg?fit=920%2C741&amp;ssl=1" data-orig-size="920,741" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="cdna3_cache_hierarchy_compared" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/12/cdna3_cache_hierarchy_compared.jpg?fit=920%2C741&amp;ssl=1" data-large-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/12/cdna3_cache_hierarchy_compared.jpg?fit=688%2C554&amp;ssl=1" src="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/12/cdna3_cache_hierarchy_compared.jpg?resize=688%2C554&amp;ssl=1" alt="" srcset="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/12/cdna3_cache_hierarchy_compared.jpg?w=920&amp;ssl=1 920w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/12/cdna3_cache_hierarchy_compared.jpg?resize=768%2C619&amp;ssl=1 768w" sizes="(max-width: 688px) 100vw, 688px" data-recalc-dims="1"></a></figure></div>
<p>For comparison, RDNA 2’s 128 MB Infinity Cache can provide 1024 bytes per cycle across all slices, giving it 2.5 TB/s of theoretical bandwidth at 2.5 GHz.<a href="https://www.flickr.com/photos/130561288@N04/51703830661/"> Die shots</a> suggest each Infinity Cache slice has 4 MB of capacity and provides 32B/cycle. RDNA 2 therefore uses bigger slices, fewer of them and has less bandwidth from each slice.</p>
<p>MI300X’s focus on bandwidth means workloads with lower compute density can still enjoy decent performance if they can get enough Infinity Cache hits. That should make CDNA 3’s execution units easier to feed even though the main memory bandwidth to compute ratio hasn’t changed much and remains behind Nvidia’s.</p>
<div>
<figure><a href="https://chipsandcheese.com/cdna3_ic_roofline/"><img loading="lazy" decoding="async" width="688" height="382" data-attachment-id="24579" data-permalink="https://chipsandcheese.com/cdna3_ic_roofline/" data-orig-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/12/cdna3_ic_roofline.png?fit=796%2C442&amp;ssl=1" data-orig-size="796,442" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="cdna3_ic_roofline" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/12/cdna3_ic_roofline.png?fit=796%2C442&amp;ssl=1" data-large-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/12/cdna3_ic_roofline.png?fit=688%2C382&amp;ssl=1" src="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/12/cdna3_ic_roofline.png?resize=688%2C382&amp;ssl=1" alt="" srcset="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/12/cdna3_ic_roofline.png?w=796&amp;ssl=1 796w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/12/cdna3_ic_roofline.png?resize=768%2C426&amp;ssl=1 768w" sizes="(max-width: 688px) 100vw, 688px" data-recalc-dims="1"></a><figcaption>MI250X figures are for a single GCD</figcaption></figure></div>
<p>If we construct a roofline model for MI300X using Infinity Cache’s theoretical bandwidth, we can achieve full FP64 throughput with 4.75 FLOPs per byte loaded. It’s a massive improvement over DRAM, which would require 14.6 to 15 FLOPs per byte loaded.</p>
<h4>Possible Challenges with Cross-Die Bandwidth</h4>
<p>MI300X’s Infinity Fabric spans four IO dies, each of which connects to two HBM stacks and associated cache partitions. However, the bandwidth of the die to die connections may limit achieving full Infinity Cache bandwidth when MI300X operates as a single logical GPU with a unified memory pool. If memory accesses are striped evenly across the memory controllers (and thus cache partitions), as is typical for most GPU designs, the available die-to-die bandwidth may prevent applications from reaching theoretical Infinity Cache bandwidth.</p>
<div>
<figure><a href="https://chipsandcheese.com/?attachment_id=24650"><img loading="lazy" decoding="async" width="688" height="389" data-attachment-id="24650" data-permalink="https://chipsandcheese.com/?attachment_id=24650" data-orig-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/12/image-1-1.jpg?fit=1976%2C1117&amp;ssl=1" data-orig-size="1976,1117" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="image-1-1" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/12/image-1-1.jpg?fit=1976%2C1117&amp;ssl=1" data-large-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/12/image-1-1.jpg?fit=688%2C389&amp;ssl=1" src="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/12/image-1-1.jpg?resize=688%2C389&amp;ssl=1" alt="" srcset="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/12/image-1-1.jpg?w=1976&amp;ssl=1 1976w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/12/image-1-1.jpg?resize=768%2C434&amp;ssl=1 768w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/12/image-1-1.jpg?resize=1536%2C868&amp;ssl=1 1536w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/12/image-1-1.jpg?resize=1200%2C678&amp;ssl=1 1200w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/12/image-1-1.jpg?resize=1600%2C904&amp;ssl=1 1600w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/12/image-1-1.jpg?resize=1320%2C746&amp;ssl=1 1320w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/12/image-1-1.jpg?w=1376&amp;ssl=1 1376w" sizes="(max-width: 688px) 100vw, 688px" data-recalc-dims="1"></a></figure></div>
<p>First, let’s focus on a single IO die partition. It has 2.7 TB/s of ingress bandwidth along two edges adjacent to other IO dies. Its two XCDs can get 4.2 TB/s of Infinity cache bandwidth. If L2 miss requests are evenly striped across the dies, 3/4 of that bandwidth, or 3.15 TB/s, must come from peer dies. Since 3.15 TB/s is greater than 2.7 TB/s, cross-die bandwidth will limit achievable cache bandwidth.</p>
<div>
<figure><a href="https://chipsandcheese.com/?attachment_id=24840"><img loading="lazy" decoding="async" width="688" height="404" data-attachment-id="24840" data-permalink="https://chipsandcheese.com/2023/12/17/amds-cdna-3-compute-architecture/mi300x_ic_bw_singlepart/" data-orig-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/12/mi300x_ic_bw_singlepart.png?fit=912%2C536&amp;ssl=1" data-orig-size="912,536" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="mi300x_ic_bw_singlepart" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/12/mi300x_ic_bw_singlepart.png?fit=912%2C536&amp;ssl=1" data-large-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/12/mi300x_ic_bw_singlepart.png?fit=688%2C404&amp;ssl=1" src="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/12/mi300x_ic_bw_singlepart.png?resize=688%2C404&amp;ssl=1" alt="" srcset="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/12/mi300x_ic_bw_singlepart.png?w=912&amp;ssl=1 912w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/12/mi300x_ic_bw_singlepart.png?resize=768%2C451&amp;ssl=1 768w" sizes="(max-width: 688px) 100vw, 688px" data-recalc-dims="1"></a></figure></div>
<p>We can add the die in the opposite corner without any differences because all of its required die-to-die bandwidth goes in the opposite direction. MI300X has bidirectional die-to-die links.</p>
<div>
<figure><a href="https://chipsandcheese.com/?attachment_id=24841"><img loading="lazy" decoding="async" width="688" height="398" data-attachment-id="24841" data-permalink="https://chipsandcheese.com/2023/12/17/amds-cdna-3-compute-architecture/mi300x_ic_bw_opposite_corners/" data-orig-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/12/mi300x_ic_bw_opposite_corners.png?fit=912%2C527&amp;ssl=1" data-orig-size="912,527" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="mi300x_ic_bw_opposite_corners" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/12/mi300x_ic_bw_opposite_corners.png?fit=912%2C527&amp;ssl=1" data-large-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/12/mi300x_ic_bw_opposite_corners.png?fit=688%2C398&amp;ssl=1" src="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/12/mi300x_ic_bw_opposite_corners.png?resize=688%2C398&amp;ssl=1" alt="" srcset="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/12/mi300x_ic_bw_opposite_corners.png?w=912&amp;ssl=1 912w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/12/mi300x_ic_bw_opposite_corners.png?resize=768%2C444&amp;ssl=1 768w" sizes="(max-width: 688px) 100vw, 688px" data-recalc-dims="1"></a></figure></div>
<p>If all dies demand maximum Infinity Cache bandwidth in a unified configuration, things get more complex. Extra cross-die bandwidth is consumed because transfers between dies in opposite corners require two hops, and that’ll cut into ingress bandwidth available for each die.</p>
<p>While MI300X was engineered to act like one big GPU, splitting MI300X into multiple NUMA domains could give higher combined Infinity Cache bandwidth. It’s possible that AMD will have an API that will transparently split up programs among the different IO dies. Additionally, the likelihood of bandwidth issues would be minimized by high L2 hit rates, which would help avoid those bottlenecks. And in cases where the Infinity Cache hit rate are low, the MI300X’s die-to-die links are sufficiently robust and offer ample bandwidth to smoothly handle HBM traffic.</p>
<h3> Cross-XCD Coherency</h3>
<p>Even though the Infinity Cache doesn’t have to worry about coherency, the L2 caches do. Ordinary GPU memory accesses follow a relaxed coherency model, but programmers can use atomics to enforce ordering between threads. Memory accesses on AMD GPUs can also be marked with a GLC bit (Global Level Coherent). Those mechanisms still have to work if AMD wants to expose MI300X as a single big GPU, rather than a multi-GPU configuration as MI250X had done.</p>
<div>
<figure><a href="https://chipsandcheese.com/fah_atomics/"><img loading="lazy" decoding="async" width="688" height="256" data-attachment-id="24617" data-permalink="https://chipsandcheese.com/fah_atomics/" data-orig-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/12/fah_atomics.png?fit=1007%2C375&amp;ssl=1" data-orig-size="1007,375" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="fah_atomics" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/12/fah_atomics.png?fit=1007%2C375&amp;ssl=1" data-large-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/12/fah_atomics.png?fit=688%2C256&amp;ssl=1" src="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/12/fah_atomics.png?resize=688%2C256&amp;ssl=1" alt="" srcset="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/12/fah_atomics.png?w=1007&amp;ssl=1 1007w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/12/fah_atomics.png?resize=768%2C286&amp;ssl=1 768w" sizes="(max-width: 688px) 100vw, 688px" data-recalc-dims="1"></a><figcaption>Snippet of RDNA 2 code from Folding at Home, showing use of global memory atomics</figcaption></figure></div>
<p>On prior AMD GPUs, atomics and coherent accesses were handled at L2. Loads with the GLC bit set would bypass L1 caches, and thus get the most up-to-date copy of data from L2. That doesn’t work with MI300X because the most up-to-date copy of a cacheline could be on another XCD’s L2 cache. AMD could make coherent accesses bypass L2, but that would lower performance. That may have worked for a gaming GPU where coherent accesses aren’t too important. But AMD wants MI300X to perform well with compute workloads, and needs MI300A (the APU variant) to efficiently share data between the CPU and GPU. That’s where Infinity Fabric comes in.</p>
<div>
<figure><a href="https://chipsandcheese.com/?attachment_id=24583"><img loading="lazy" decoding="async" width="688" height="385" data-attachment-id="24583" data-permalink="https://chipsandcheese.com/2023/12/17/amds-cdna-3-compute-architecture/cdna3_cs/" data-orig-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/12/cdna3_cs.jpg?fit=1533%2C857&amp;ssl=1" data-orig-size="1533,857" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="cdna3_cs" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/12/cdna3_cs.jpg?fit=1533%2C857&amp;ssl=1" data-large-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/12/cdna3_cs.jpg?fit=688%2C385&amp;ssl=1" src="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/12/cdna3_cs.jpg?resize=688%2C385&amp;ssl=1" alt="" srcset="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/12/cdna3_cs.jpg?w=1533&amp;ssl=1 1533w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/12/cdna3_cs.jpg?resize=768%2C429&amp;ssl=1 768w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/12/cdna3_cs.jpg?resize=1200%2C671&amp;ssl=1 1200w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/12/cdna3_cs.jpg?resize=1320%2C738&amp;ssl=1 1320w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/12/cdna3_cs.jpg?w=1376&amp;ssl=1 1376w" sizes="(max-width: 688px) 100vw, 688px" data-recalc-dims="1"></a><figcaption>CM = Coherent Master. CS = Coherent Slave</figcaption></figure></div>
<p>Like Infinity Fabric on Ryzen, CDNA 3 has Coherent Masters (CMs) where the XCDs connect to the IO dies. Coherent Slaves (CS) sit at each memory controller alongside Infinity Cache (IC) slices. We can infer how these work via Ryzen documentation, which shows Coherent Slaves have a probe filter and hardware for handling atomic transactions. MI300X likely has a similar CS implementation.</p>
<div>
<figure><a href="https://chipsandcheese.com/?attachment_id=24585"><img loading="lazy" decoding="async" width="688" height="501" data-attachment-id="24585" data-permalink="https://chipsandcheese.com/2023/12/17/amds-cdna-3-compute-architecture/zen_ppr_cs/" data-orig-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/12/zen_ppr_cs.png?fit=736%2C536&amp;ssl=1" data-orig-size="736,536" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="zen_ppr_cs" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/12/zen_ppr_cs.png?fit=736%2C536&amp;ssl=1" data-large-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/12/zen_ppr_cs.png?fit=688%2C501&amp;ssl=1" src="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/12/zen_ppr_cs.png?resize=688%2C501&amp;ssl=1" alt="" data-recalc-dims="1"></a><figcaption>From AMD’s Zen PPR, showing error reporting available at the Coherent Slave (CS).</figcaption></figure></div>
<p>If a coherent write shows up at the CS, it has to ensure any thread doing a coherent read will observe that write regardless of where that thread is running on the GPU. That means any XCD with the line cached will have to reload it from Infinity Cache to get the most up to date data. Naively, the CS would have to probe L2 caches across all XCDs because any of them could have the corresponding data cached. The probe filter helps avoid this by tracking which XCDs actually have the line cached, thus avoiding unnecessary probe traffic. CDNA 3’s whitepaper says the snoop filter (another name for a probe filter) is large enough to cover multiple XCD L2 caches. I certainly believe them because MI300X has 32 MB of L2 across all eight XCDs. Even consumer Ryzen parts can have more CCD-private cache for the probe filter to cover.</p>
<p>Thanks to CPU-like Infinity Fabric components like CS and CM, a XCD can have a private write-back L2 cache capable of handling intra-die coherent accesses without going across the IO die fabric. AMD could have gone for a naive solution where coherent operations and atomics go straight to the Infinity Cache, bypassing L2. Such a solution would save engineering effort and create a simpler design at the cost of lower performance for coherent operations. Evidently, AMD thought optimizing atomics and coherent accesses was important enough to go the extra mile.</p>
<blockquote>
<p>To ensure coherence of local memory writes of CUs in different agents a <code>buffer_wbl2 sc1</code> is required. It will writeback dirty L2 cache lines.</p>
<p>To ensure coherence of local memory reads of CUs in different agents a <code>buffer_inv sc0 sc1</code> is required. It will invalidate non-local L2 cache<br>lines if configured to have multiple L2 caches.</p>
<cite><a href="https://github.com/llvm/llvm-project/blob/main/llvm/docs/AMDGPUUsage.rst">LLVM Documentation</a> for the GFX942 Target</cite></blockquote>
<p>However, CDNA 3 within the XCD still works a lot like prior GPUs. Evidently normal memory writes will not automatically invalidate written lines from peer caches as in CPUs. Instead, code must explicitly tell the L2 to write back dirty lines and have peer L2 caches invalidate non-local L2 lines.</p>
<h3>L2 Cache</h3>
<p>Closer to the Compute Units, each MI300X XCD packs a 4 MB L2 cache. The L2 is a more traditional GPU cache, and is built from 16 slices. Each 256 KB slice can provide 128 bytes per cycle of bandwidth. At 2.1 GHz, that’s good for 4.3 TB/s. As the last level of cache on the same die as the Compute Units, the L2 plays an important role in acting as a backstop for L1 misses.</p>
<div>
<figure><a href="https://chipsandcheese.com/?attachment_id=24664"><img loading="lazy" decoding="async" width="651" height="396" data-attachment-id="24664" data-permalink="https://chipsandcheese.com/2023/12/17/amds-cdna-3-compute-architecture/mi300x_l2_roofline-1/" data-orig-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/12/mi300x_l2_roofline-1.png?fit=651%2C396&amp;ssl=1" data-orig-size="651,396" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="mi300x_l2_roofline-1" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/12/mi300x_l2_roofline-1.png?fit=651%2C396&amp;ssl=1" data-large-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/12/mi300x_l2_roofline-1.png?fit=651%2C396&amp;ssl=1" src="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/12/mi300x_l2_roofline-1.png?resize=651%2C396&amp;ssl=1" alt="" data-recalc-dims="1"></a></figure></div>
<p>Compared to H100 and MI250X, MI300X has a higher L2 bandwidth to compute ratio. Because each XCD comes with a L2, L2 bandwidth naturally scales as a CDNA 3 product comes with more XCDs. In other words, MI300X’s L2 arrangement avoids the problem of getting a single cache hooked up to a lot of Compute Units and maintain a ton of bandwidth.</p>
<p>PVC’s L2 is a clear contrast. As Intel adds more Compute Tiles, the Base Tile’s shared L2 gets increasing bandwidth demands. From a cache design standpoint, PVC’s configuration is simpler because the L2 acts as a single point of coherency and a backstop for L1 misses. But it can’t offer as much bandwidth as MI300X’s L2. MI300X also likely enjoys better L2 latency, making it easier for applications to utilize cache bandwidth.</p>
<h3>L1 Cache</h3>
<p>CDNA 3’s focus on high cache bandwidth continues to the L1. In a move that matches RDNA, CDNA 3 sees its L1 throughput increased from 64 to 128 bytes per cycle. CDNA 2 increased per-CU vector throughput to 4096 bits per cycle compared to 2048 in GCN, so CDNA 3’s doubled L1 throughput helps maintain the same compute to L1 bandwidth ratio as GCN.</p>
<div>
<figure><a href="https://chipsandcheese.com/?attachment_id=24667"><img loading="lazy" decoding="async" width="651" height="395" data-attachment-id="24667" data-permalink="https://chipsandcheese.com/2023/12/17/amds-cdna-3-compute-architecture/mi300x_l1_roofline/" data-orig-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/12/mi300x_l1_roofline.png?fit=651%2C395&amp;ssl=1" data-orig-size="651,395" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="mi300x_l1_roofline" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/12/mi300x_l1_roofline.png?fit=651%2C395&amp;ssl=1" data-large-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/12/mi300x_l1_roofline.png?fit=651%2C395&amp;ssl=1" src="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/12/mi300x_l1_roofline.png?resize=651%2C395&amp;ssl=1" alt="" data-recalc-dims="1"></a></figure></div>
<p>Besides higher bandwidth, CDNA 3 increases L1 capacity from 16 to 32 KB. It’s a move that again mirrors developments in the RDNA line, where RDNA 3 received a similar size boost for its first level cache. Higher hitrates from the larger cache would lower average memory access latency, improving execution unit utilization. Transferring data from L2 and beyond costs power, so higher hitrate can help power efficiency too.</p>
<p>While CDNA 3 improves first level caching, Ponte Vecchio is still the champion in that category. Each Xe Core in PVC can deliver 512 bytes per cycle, giving Intel a very high L1 bandwidth to compute ratio. The L1 is large as well at 512 KB. Memory bound kernels that fit in L1 will do very well on Intel’s architecture. However, Ponte Vecchio lacks a mid-level cache at the Compute Tile level, and could face a harsh performance cliff as data spills out of L1.</p>
<h2>Scheduling and Execution Units</h2>
<p>A complex chiplet setup and modified cache hierarchy let AMD present MI300X as a single GPU, thus addressing one of MI250X’s biggest weaknesses. But AMD didn’t settle with that. They also made iterative improvements to the core Compute Unit architecture, addressing CDNA 2’s difficulties with utilizing its FP32 units.</p>
<div>
<figure><a href="https://chipsandcheese.com/cdna3_cu/"><img loading="lazy" decoding="async" width="688" height="175" data-attachment-id="24812" data-permalink="https://chipsandcheese.com/cdna3_cu/" data-orig-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/12/cdna3_cu.png?fit=692%2C176&amp;ssl=1" data-orig-size="692,176" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="cdna3_cu" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/12/cdna3_cu.png?fit=692%2C176&amp;ssl=1" data-large-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/12/cdna3_cu.png?fit=688%2C175&amp;ssl=1" src="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/12/cdna3_cu.png?resize=688%2C175&amp;ssl=1" alt="" data-recalc-dims="1"></a><figcaption>From the CDNA 3 whitepaper</figcaption></figure></div>
<p>When CDNA 2 shifted to handling FP64 natively, AMD provided double rate FP32 via packed execution. The compiler would have to pack two FP32 values into adjacent registers and perform the same instruction on both. Often, the compiler struggled to pull this off unless programmers explicitly used vectors.</p>
<p>CDNA 3 gets around this with a more flexible dual issue mechanism. Most likely, this is an extension of GCN’s multi-issue capability rather than RDNA 3’s VOPD/wave64 method. Each cycle, the CU scheduler selects one of the four SIMDs and checks whether any of its threads are ready to execute. If multiple threads are ready, GCN could select up to five of them to send to execution units. Of course a GCN SIMD only has a single 16-wide vector ALU, so GCN would have to select threads with different instruction types ready to multi-issue. For example, a scalar ALU instruction can issue alongside a vector ALU one.</p>
<div>
<figure><img loading="lazy" decoding="async" width="688" height="280" data-attachment-id="24753" data-permalink="https://chipsandcheese.com/2023/12/17/amds-cdna-3-compute-architecture/mi300x_multi_issue_occupancy/" data-orig-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/12/mi300x_multi_issue_occupancy.jpg?fit=1400%2C570&amp;ssl=1" data-orig-size="1400,570" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="mi300x_multi_issue_occupancy" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/12/mi300x_multi_issue_occupancy.jpg?fit=1400%2C570&amp;ssl=1" data-large-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/12/mi300x_multi_issue_occupancy.jpg?fit=688%2C280&amp;ssl=1" src="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/12/mi300x_multi_issue_occupancy.jpg?resize=688%2C280&amp;ssl=1" alt="" srcset="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/12/mi300x_multi_issue_occupancy.jpg?w=1400&amp;ssl=1 1400w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/12/mi300x_multi_issue_occupancy.jpg?resize=768%2C313&amp;ssl=1 768w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/12/mi300x_multi_issue_occupancy.jpg?resize=1200%2C489&amp;ssl=1 1200w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/12/mi300x_multi_issue_occupancy.jpg?resize=1320%2C537&amp;ssl=1 1320w" sizes="(max-width: 688px) 100vw, 688px" data-recalc-dims="1"></figure></div>
<p>An alternative approach would be to take advantage of wave64’s wider width and let a thread complete two vector instructions over four cycles. However, doing so would break GCN’s model of handling VALU instructions in multiples of 4 clock cycles. CDNA 3 is still more closely related to GCN than RDNA is, and reusing GCN’s multi-issue strategy is a sensible move. AMD also could have used RDNA 3’s VOPD mechanism, where a special instruction format can contain two operations. While that method could increase per-thread performance, relying on the compiler to find dual issue pairs could be hit or miss.</p>
<div>
<figure><a href="https://chipsandcheese.com/gcn_cu_scheduler/"><img loading="lazy" decoding="async" width="688" height="381" data-attachment-id="24655" data-permalink="https://chipsandcheese.com/gcn_cu_scheduler/" data-orig-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/12/gcn_cu_scheduler.png?fit=1032%2C571&amp;ssl=1" data-orig-size="1032,571" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="gcn_cu_scheduler" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/12/gcn_cu_scheduler.png?fit=1032%2C571&amp;ssl=1" data-large-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/12/gcn_cu_scheduler.png?fit=688%2C381&amp;ssl=1" src="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/12/gcn_cu_scheduler.png?resize=688%2C381&amp;ssl=1" alt="" srcset="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/12/gcn_cu_scheduler.png?w=1032&amp;ssl=1 1032w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/12/gcn_cu_scheduler.png?resize=768%2C425&amp;ssl=1 768w" sizes="(max-width: 688px) 100vw, 688px" data-recalc-dims="1"></a><figcaption>From an old AMD presentation</figcaption></figure></div>
<p>Instead of relying on the compiler, CDNA 3’s dual issue approach likely pushes responsibility to the programmer to expose more thread level parallelism via larger dispatch sizes. If a SIMD has more threads in flight, it’ll have a better chance of finding two threads with FP32 instructions ready to execute. At minimum, a SIMD will need two threads active to achieve full FP32 throughput. In practice CDNA 3 will need much higher occupancy to achieve good FP32 utilization. GPUs use in-order execution so individual threads will often be blocked by memory or execution latency. Keeping one set of execution units fed can be difficult even at full occupancy.</p>
<div>
<figure><a href="https://chipsandcheese.com/?attachment_id=24752"><img loading="lazy" decoding="async" width="688" height="480" data-attachment-id="24752" data-permalink="https://chipsandcheese.com/2023/12/17/amds-cdna-3-compute-architecture/mi300x_multi_issue/" data-orig-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/12/mi300x_multi_issue.jpg?fit=997%2C695&amp;ssl=1" data-orig-size="997,695" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="mi300x_multi_issue" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/12/mi300x_multi_issue.jpg?fit=997%2C695&amp;ssl=1" data-large-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/12/mi300x_multi_issue.jpg?fit=688%2C480&amp;ssl=1" src="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/12/mi300x_multi_issue.jpg?resize=688%2C480&amp;ssl=1" alt="" srcset="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/12/mi300x_multi_issue.jpg?w=997&amp;ssl=1 997w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/12/mi300x_multi_issue.jpg?resize=768%2C535&amp;ssl=1 768w" sizes="(max-width: 688px) 100vw, 688px" data-recalc-dims="1"></a></figure></div>
<p>Therefore, AMD has dramatically increased the number of threads each CDNA 3 SIMD can track <a href="https://gpuopen.com/learn/amd-lab-notes/amd-lab-notes-register-pressure-readme/">from 8</a> to 24. If a programmer can take advantage of this, CDNA 3 will be better positioned to multi-issue. But this can be difficult. AMD did not mention an increase in vector register file capacity, which often limits how many threads a SIMD can have in flight. The vector register file can hold state for more threads if each thread uses fewer registers, so CDNA 3’s multi-issue capability may work best for simple kernels with few live variables.</p>
<p>Register file bandwidth presents another challenge for dual issue. CDNA 2’s packed FP32 execution didn’t require extra reads from the vector register file because it took advantage of wider register file ports needed to deliver 64-bit values. But separate instructions can reference different registers and require more reads from the register file. Adding more register file ports would be expensive, so CDNA 3 “generationally improves the source caching to provide better re-use and bandwidth amplification so that each vector register read can support more downstream vector or matrix operations”<sup>1</sup>. Most likely, AMD is using a larger register cache to mitigate port conflicts and keep the execution units fed.</p>
<h2>Matrix Operations</h2>
<p>Matrix multiplication has become increasingly important as machine learning picks up. Nvidia invested heavily in this area, adding matrix multiplication units (tensor cores) to their Volta and Turing architectures years ago. AMD’s CDNA architecture added matrix multiply support, but contemporary Nvidia architectures invested more heavily in matrix multiplication throughput. This especially applies to lower precision data types like FP16, which are often used in AI.</p>
<figure><table><tbody><tr><td></td><td>Matrix FP16 FMAs/Clk</td><td>Rate Relative to Packed FP16</td></tr><tr><td>AMD MI100 (CDNA) Compute Unit</td><td>512</td><td>4x</td></tr><tr><td>AMD MI250X (CDNA 2) Compute Unit</td><td>512</td><td>4x</td></tr><tr><td>AMD MI300X (CDNA 3) Compute Unit</td><td>1024</td><td>8x</td></tr><tr><td>Nvidia V100 Streaming Multiprocessor</td><td>512</td><td>4x<sup>4</sup></td></tr><tr><td>Nvidia A100 Streaming Multiprocessor</td><td>1024</td><td>4x</td></tr><tr><td>Nvidia H100 Streaming Multiprocessor</td><td>2048</td><td>8x</td></tr></tbody></table></figure>
<p>MI300X plays catch up by doubling per-CU matrix throughput compared to prior CDNA generations. On top of that, MI300X’s chiplet design allows a massive number of CUs. But Nvidia’s higher per-SM matrix performance still makes it a force to be reckoned with. Therefore, CDNA 3 continues AMD’s trend of hitting Nvidia hard from the vector FP64 performance side while maintaining strong AI performance in isolation.</p>
<h2>Instruction Cache</h2>
<p>Besides handling memory accesses requested by instructions, a Compute Unit has to fetch the instructions themselves from memory. GPUs traditionally had an easier time with instruction delivery because GPU code tends to be simple and not occupy a lot of memory. In the DirectX 9 era, Shader Model 3.0 <a href="https://learn.microsoft.com/en-us/windows/win32/direct3dhlsl/dx9-graphics-reference-asm-ps-3-0">even imposed limits on code size</a>. As GPUs evolved to take on compute, AMD rolled out their GCN architecture with 32 KB instruction caches. Today, CDNA 2 and RDNA GPUs continue to use 32 KB instruction caches.</p>
<p>CDNA 3 increases instruction cache capacity to 64 KB. Associativity doubles too, from 4-way to 8-way. That means higher instruction cache hitrates for CDNA 3 with bigger, more complex kernels. I suspect AMD is targeting CPU code naively ported to GPUs. Complex CPU code can be <a href="https://streamhpc.com/blog/2018-03-14/selecting-applications-suitable-for-porting-to-the-gpu/">punishing on GPUs</a>, since they can’t hide instruction cache miss latency with long distance instruction prefetching and accurate branch prediction. Higher instruction cache capacity helps contain larger kernels, while increased associativity helps avoid conflict misses.</p>
<p>Like CDNA 2, each CDNA 3 instruction cache instance services <a href="https://elixir.bootlin.com/linux/latest/source/drivers/gpu/drm/amd/amdkfd/kfd_crat.c#L328">two Compute Units</a>. GPU kernels are usually launched with large enough work sizes to fill many Compute Units, so sharing the instruction cache is a good way to efficiently use SRAM storage. I suspect AMD didn’t share the cache across even more Compute Units because a single cache instance may struggle to satisfy instruction bandwidth demands.</p>
<h2>Final Words</h2>
<p>CDNA 3’s whitepaper says that “the greatest generational changes in the AMD CDNA 3 architecture lie in the memory hierarchy” and I would have to agree. While AMD improved the Compute Unit’s low precision math capabilities compared to CDNA 2, the real improvement was the addition of the Infinity Cache.</p>
<p>MI250X’s primary issue was that it wasn’t really one GPU. It was two GPUs sharing the same package which only has 200 Gigabyte per second per direction between the GCDs. In AMD’s assessment that 200 Gigabyte per second per direction was not enough to have the MI250X show up as one GPU which is why AMD significantly increased the die to die bandwidth.</p>
<div>
<figure><img loading="lazy" decoding="async" width="688" height="389" data-attachment-id="24650" data-permalink="https://chipsandcheese.com/?attachment_id=24650" data-orig-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/12/image-1-1.jpg?fit=1976%2C1117&amp;ssl=1" data-orig-size="1976,1117" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="image-1-1" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/12/image-1-1.jpg?fit=1976%2C1117&amp;ssl=1" data-large-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/12/image-1-1.jpg?fit=688%2C389&amp;ssl=1" src="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/12/image-1-1.jpg?resize=688%2C389&amp;ssl=1" alt="" srcset="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/12/image-1-1.jpg?w=1976&amp;ssl=1 1976w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/12/image-1-1.jpg?resize=768%2C434&amp;ssl=1 768w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/12/image-1-1.jpg?resize=1536%2C868&amp;ssl=1 1536w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/12/image-1-1.jpg?resize=1200%2C678&amp;ssl=1 1200w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/12/image-1-1.jpg?resize=1600%2C904&amp;ssl=1 1600w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/12/image-1-1.jpg?resize=1320%2C746&amp;ssl=1 1320w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/12/image-1-1.jpg?w=1376&amp;ssl=1 1376w" sizes="(max-width: 688px) 100vw, 688px" data-recalc-dims="1"><figcaption>For this image, I am considering North-South as the vertical axis and East-West as the horizontal axis</figcaption></figure></div>
<p>AMD increased the total East-West bandwidth to 2.4TB/sec per direction which is a 12 fold increase from MI250X. And the total North-South bandwidth is an even higher 3.0TB/sec per direction. With these massive bandwidth increases, AMD was able to make the MI300 appear as one large, unified accelerator instead of as 2 separate accelerators like MI250X. </p>
<p>4.0 TB/s of total ingress bandwidth for one die may not seem like enough if both XCD needs all available memory bandwidth. However, both XCDs combined can only access up to 4.2TB/s of bandwidth from the IO die so realistically the 4.0TB/s of ingress bandwidth is a non-issue. What the maximum of 4.0TB/s of ingress bandwidth does mean is that a single IO die can’t take advantage of all 5.3TB/s of memory bandwidth. </p>
<p>This is similar to desktop Ryzen 7000 parts where one CCD can’t take full advantage of DDR5 bandwidth due to Infinity Fabric limits. However this is likely to be a non-issue on MI300X because the bandwidth demands will be highest with all dies in play. In that case, each die will consume about 1.3 TB/s of bandwidth and getting 3/4 of that over cross-die links won’t be a problem. </p>
<div>
<figure><img loading="lazy" decoding="async" width="688" height="387" data-attachment-id="24792" data-permalink="https://chipsandcheese.com/2023/12/17/amds-cdna-3-compute-architecture/image-1-2-2/" data-orig-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/12/image-1-2.jpg?fit=1495%2C840&amp;ssl=1" data-orig-size="1495,840" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="image-1-2" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/12/image-1-2.jpg?fit=1495%2C840&amp;ssl=1" data-large-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/12/image-1-2.jpg?fit=688%2C387&amp;ssl=1" src="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/12/image-1-2.jpg?resize=688%2C387&amp;ssl=1" alt="" srcset="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/12/image-1-2.jpg?w=1495&amp;ssl=1 1495w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/12/image-1-2.jpg?resize=1280%2C720&amp;ssl=1 1280w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/12/image-1-2.jpg?resize=768%2C432&amp;ssl=1 768w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/12/image-1-2.jpg?resize=1200%2C674&amp;ssl=1 1200w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/12/image-1-2.jpg?resize=1320%2C742&amp;ssl=1 1320w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/12/image-1-2.jpg?w=1376&amp;ssl=1 1376w" sizes="(max-width: 688px) 100vw, 688px" data-recalc-dims="1"></figure></div>
<p>But MI300 isn’t just a GPGPU part, it also has an APU part as well, which is in my opinion is the more interesting of the two MI300 products. AMD’s first ever APU, Llano, was released in 2011 which was based on AMD’s K10.5 CPU paired with a Terascale 3 GPU. Fast forward to 2023 and for their first “big iron” APU, the MI300A, AMD paired 6 of their CDNA3 XCDs with 24 Zen 4 cores all while reusing the same base die. This allows for the CPU and the GPU to shared the same memory address space which removes the need to copy data over an external bus to keep the CPU and GPU coherent with each other. </p>
<p>We look forward to what AMD could do with future “big iron” APUs as well as their future GPGPU line up. Maybe they’ll have specialized CCDs with wider vector units or maybe they’ll have networking on their base die that can directly connect to the xGMI switches that Broadcom have said to be making. Regardless of what future Instinct products look like, we are excited to both be looking forward to those products as well as testing the MI300 series.</p>
<p>We would like to thank AMD for inviting Chips and Cheese to the MI300 launch event. We were able to ask a lot of questions and gain some extra information without which this article would have been much shorter.</p>
<p>If you like our articles and journalism, and you want to support us in our endeavors, then consider heading over to our&nbsp;<a href="https://www.patreon.com/ChipsandCheese">Patreon</a>&nbsp;or our&nbsp;<a href="https://www.paypal.com/donate/?hosted_button_id=4EMPH66SBGVSQ">PayPal</a>&nbsp;if you want to toss a few bucks our way. If you would like to talk with the Chips and Cheese staff and the people behind the scenes, then consider joining our&nbsp;<a href="https://discord.gg/TwVnRhxgY2">Discord</a>.</p>
<h2>References</h2>
<ol>
<li><a href="https://www.amd.com/content/dam/amd/en/documents/instinct-tech-docs/white-papers/amd-cdna-3-white-paper.pdf">CDNA 3 Whitepaper</a></li>
<li><a href="https://www.amd.com/content/dam/amd/en/documents/instinct-business-docs/white-papers/amd-cdna2-white-paper.pdf">CDNA 2 Whitepaper</a></li>
<li><a href="https://www.amd.com/content/dam/amd/en/documents/instinct-business-docs/white-papers/amd-cdna-white-paper.pdf">CDNA Whitepaper</a></li>
<li><a href="https://images.nvidia.com/content/volta-architecture/pdf/volta-architecture-whitepaper.pdf">Volta Whitepaper</a></li>
<li><a href="https://images.nvidia.com/aem-dam/en-zz/Solutions/data-center/nvidia-ampere-architecture-whitepaper.pdf">Nvidia A100 Whitepaper</a></li>
<li><a href="https://resources.nvidia.com/en-us-tensor-core/gtc22-whitepaper-hopper">Nvidia H100 Whitepaper</a></li>
<li><a href="https://www.intel.com/content/www/us/en/developer/articles/technical/intel-data-center-gpu-max-series-overview.html#gs.1p3uqo">Intel Data Center GPU Max Series Technical Overview</a></li>
</ol>

<div data-post_id="10949" data-instance_id="1" data-additional_class="pp-multiple-authors-layout-boxed.multiple-authors-target-the-content" data-original_class="pp-multiple-authors-boxes-wrapper pp-multiple-authors-wrapper box-post-id-10949 box-instance-id-1">

<ul>
<li>
<p><img alt="clamchowder" src="https://secure.gravatar.com/avatar/7c39d2e6d35e77c8fd15c4b2d9ce4e64?s=80&amp;d=identicon&amp;r=g" srcset="https://secure.gravatar.com/avatar/7c39d2e6d35e77c8fd15c4b2d9ce4e64?s=160&amp;d=identicon&amp;r=g 2x" height="80" width="80"> </p>

</li>
<li>
<p><img alt="Cheese" src="https://secure.gravatar.com/avatar/eb262496276a5c8c0a375be578f81db9?s=80&amp;d=identicon&amp;r=g" srcset="https://secure.gravatar.com/avatar/eb262496276a5c8c0a375be578f81db9?s=160&amp;d=identicon&amp;r=g 2x" height="80" width="80"> </p>

</li>
</ul>
</div>





</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[microsoft/promptbase: All things prompt engineering (157 pts)]]></title>
            <link>https://github.com/microsoft/promptbase</link>
            <guid>38673954</guid>
            <pubDate>Sun, 17 Dec 2023 16:36:22 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/microsoft/promptbase">https://github.com/microsoft/promptbase</a>, See on <a href="https://news.ycombinator.com/item?id=38673954">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-target="readme-toc.content">
            <article itemprop="text"><h2 tabindex="-1" dir="auto">promptbase</h2>
<p dir="auto"><code>promptbase</code> is an evolving collection of resources, best practices, and example scripts for eliciting the best performance from foundation models like <code>GPT-4</code>. We currently host scripts demonstrating the <a href="https://arxiv.org/abs/2311.16452" rel="nofollow"><code>Medprompt</code> methodology</a>, including examples of how we further extended this collection of prompting techniques ("<code>Medprompt+</code>") into non-medical domains:</p>
<table>
<thead>
<tr>
<th>Benchmark</th>
<th>GPT-4 Prompt</th>
<th>GPT-4 Results</th>
<th>Gemini Ultra Results</th>
</tr>
</thead>
<tbody>
<tr>
<td>MMLU</td>
<td>Medprompt+</td>
<td>90.10%</td>
<td>90.04%</td>
</tr>
<tr>
<td>GSM8K</td>
<td>Zero-shot</td>
<td>95.3%</td>
<td>94.4%</td>
</tr>
<tr>
<td>MATH</td>
<td>Zero-shot</td>
<td>68.4%</td>
<td>53.2%</td>
</tr>
<tr>
<td>HumanEval</td>
<td>Zero-shot</td>
<td>87.8%</td>
<td>74.4%</td>
</tr>
<tr>
<td>BIG-Bench-Hard</td>
<td>Few-shot + CoT</td>
<td>89.0%</td>
<td>83.6%</td>
</tr>
<tr>
<td>DROP</td>
<td>Zero-shot + CoT</td>
<td>83.7%</td>
<td>82.4%</td>
</tr>
<tr>
<td>HellaSwag</td>
<td>10-shot</td>
<td>95.3%</td>
<td>87.8%</td>
</tr>
</tbody>
</table>
<p dir="auto">In the near future, <code>promptbase</code> will also offer further case studies and structured interviews around the scientific process we take behind prompt engineering. We'll also offer specialized deep dives into specialized tooling that accentuates the prompt engineering process. Stay tuned!</p>
<h2 tabindex="-1" dir="auto"><code>Medprompt</code> and The Power of Prompting</h2>
<details>
<summary>
    <em>"Can Generalist Foundation Models Outcompete Special-Purpose Tuning? Case Study in Medicine" (H. Nori, Y. T. Lee, S. Zhang, D. Carignan, R. Edgar, N. Fusi, N. King, J. Larson, Y. Li, W. Liu, R. Luo, S. M. McKinney, R. O. Ness, H. Poon, T. Qin, N. Usuyama, C. White, E. Horvitz 2023)</em>
</summary>
<br>
<pre><p dir="auto">@article{nori2023can,
title={Can Generalist Foundation Models Outcompete Special-Purpose Tuning? Case Study in Medicine},
author={Nori, Harsha and Lee, Yin Tat and Zhang, Sheng and Carignan, Dean and Edgar, Richard and Fusi, Nicolo and King, Nicholas and Larson, Jonathan and Li, Yuanzhi and Liu, Weishung and others},
journal={arXiv preprint arXiv:2311.16452},
year={2023}
}
</p></pre>
<a href="https://arxiv.org/pdf/1909.09223.pdf" rel="nofollow">Paper link</a>
</details>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/microsoft/promptbase/blob/main/images/medprompt_radar.png"><img src="https://github.com/microsoft/promptbase/raw/main/images/medprompt_radar.png" alt=""></a></p>
<p dir="auto">In a recent <a href="https://arxiv.org/abs/2311.16452" rel="nofollow">study</a>, we showed how the composition of several prompting strategies into a method that we refer to as <code>Medprompt</code> can efficiently steer generalist models like GPT-4 to achieve top performance, even when compared to models specifically finetuned for medicine. <code>Medprompt</code> composes three distinct strategies together -- including dynamic few-shot selection, self-generated chain of thought, and choice-shuffle ensembling -- to elicit specialist level performance from GPT-4. We briefly describe these strategies here:</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/microsoft/promptbase/blob/main/images/medprompt_sa_graphic.png"><img src="https://github.com/microsoft/promptbase/raw/main/images/medprompt_sa_graphic.png" alt=""></a></p>
<ul dir="auto">
<li>
<p dir="auto"><strong>Dynamic Few Shots</strong>: Few-shot learning -- providing several examples of the task and response to a foundation model -- enables models quickly adapt to a specific domain and
learn to follow the task format. For simplicity and efficiency, the few-shot examples applied in prompting for a particular task are typically fixed; they are unchanged across test examples. This necessitates that the few-shot examples selected are broadly representative and relevant to a wide distribution of text examples. One approach to meeting these requirements is to have domain experts carefully hand-craft exemplars. Even so, this approach cannot guarantee that the curated, fixed few-shot examples will be appropriately representative of every test example. However, with enough available data, we can select <em>different</em> few-shot examples for different task inputs. We refer to this approach as employing dynamic few-shot examples. The method makes use of a mechanism to identify examples based on their similarity to the case at hand. For Medprompt, we did the following to identify representative few shot examples: Given a test example, we choose k training examples that are semantically similar using a k-NN clustering in the embedding space. Specifically, we first use OpenAI's <code>text-embedding-ada-002</code> model to embed candidate exemplars for few-shot learning. Then, for each test question x, we retrieve its nearest k neighbors x1, x2, ..., xk from the training set (according to distance in the embedding space of text-embedding-ada-002). These examples -- the ones most similar in embedding space to the test question -- are ultimately registered in the prompt.</p>
</li>
<li>
<p dir="auto"><strong>Self-Generated Chain of Thought (CoT)</strong>: Chain-of-thought (CoT) uses natural language statements, such as “Let’s think step by step,” to explicitly encourage the model to generate a series of intermediate reasoning steps. The approach has been found to significantly improve the ability of foundation models to perform complex reasoning. Most approaches to chain-of-thought center on the use of experts to manually compose few-shot examples with chains of thought for prompting. Rather than rely on human experts, we pursued
a mechanism to automate the creation of chain-of-thought examples. We found that we could simply ask GPT-4 to generate chain-of-thought for the training examples, with appropriate guardrails for reducing risk of hallucination via incorrect reasoning chains.</p>
</li>
<li>
<p dir="auto"><strong>Majority Vote Ensembling</strong>: <a href="https://en.wikipedia.org/wiki/Ensemble_learning" rel="nofollow">Ensembling</a> refers to combining the output of several algorithms together to yield better predictive performance than any individual algorithm. Frontier models like <code>GPT-4</code> benefit from ensembling of their own outputs. A simple technique is to have a variety of prompts, or a single prompt with varied <code>temperature</code>, and report the most frequent answer amongst the ensemble constituents. For multiple choice questions, we employ a further trick that increases the diversity of the ensemble called <code>choice-shuffling</code>, where we shuffle the relative order of the answer choices before generating each reasoning
path. We then select the most consistent answer, i.e., the one that is least sensitive to choice shuffling, which increases the robustness of the answer.</p>
</li>
</ul>
<p dir="auto">The combination of these three techniques led to breakthrough performance in Medprompt for medical challenge questions. Implementation details of these techniques can be found here: <a href="https://github.com/microsoft/promptbase/tree/main/src/promptbase/mmlu">https://github.com/microsoft/promptbase/tree/main/src/promptbase/mmlu</a></p>
<h2 tabindex="-1" dir="auto"><code>Medprompt+</code> | Extending the power of prompting</h2>
<p dir="auto">Here we provide some intuitive details on how we extended the <code>medprompt</code> prompting framework to elicit even stronger out-of-domain performance on the MMLU (Measuring Massive Multitask Language Understanding) benchmark.  MMLU was established as a test of general knowledge and reasoning powers of large language models.  The complete MMLU benchmark contains tens of thousands of challenge problems of different forms across 57 areas from basic mathematics to United States history, law, computer science, engineering, medicine, and more.</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/microsoft/promptbase/blob/main/images/mmlu_accuracy_ablation.png"><img src="https://github.com/microsoft/promptbase/raw/main/images/mmlu_accuracy_ablation.png" alt=""></a></p>
<p dir="auto">We found that applying Medprompt without modification to the whole MMLU achieved a score of 89.1%. Not bad for a single policy working across a great diversity of problems!  But could we push Medprompt to do better?  Simply scaling-up MedPrompt can yield further benefits. As a first step, we increased the number of ensembled calls from five to 20.  This boosted performance to 89.56%.</p>
<p dir="auto">On working to push further with refinement of Medprompt, we noticed that performance was relatively poor for specific topics of the MMLU. MMLU contains a great diversity of types of questions, depending on the discipline and specific benchmark at hand. How might we push GPT-4 to perform even better on MMLU given the diversity of problems?</p>
<p dir="auto">We focused on extension to a portfolio approach based on the observation that some topical areas tend to ask questions that would require multiple steps of reasoning and perhaps a scratch pad to keep track of multiple parts of a solution. Other areas seek factual answers that follow more directly from questions. Medprompt employs “chain-of-thought” (CoT) reasoning, resonating with multi-step solving.  We wondered if the sophisticated Medprompt-classic approach might do less well on very simple questions and if the system might do better if a simpler method were used for the factual queries.</p>
<p dir="auto">Following this argument, we found that we could boost the performance on MMLU by extending MedPrompt with a simple two-method prompt portfolio. We add to the classic Medprompt a set of 10 simple, direct few-shot prompts soliciting an answer directly without Chain of Thought. We then ask GPT-4 for help with deciding on the best strategy for each topic area and question. As a screening call, for each question we first ask GPT-4:</p>
<div data-snippet-clipboard-copy-content="# Question
{{ question }}
 
# Task
Does answering the question above require a scratch-pad?
A. Yes
B. No"><pre><code># Question
{{ question }}
 
# Task
Does answering the question above require a scratch-pad?
A. Yes
B. No
</code></pre></div>
<p dir="auto">If GPT-4 thinks the question does require a scratch-pad, then the contribution of the Chain-of-Thought component of the ensemble is doubled. If it doesn't, we halve that contribution (and let the ensemble instead depend more on the direct few-shot prompts). Dynamically leveraging the appropriate prompting technique in the ensemble led to a further +0.5% performance improvement across the MMLU.</p>
<p dir="auto">We note that Medprompt+ relies on accessing confidence scores (logprobs) from GPT-4. These are not publicly available via the current API but will be enabled for all in the near future.</p>
<h2 tabindex="-1" dir="auto">Running Scripts</h2>
<blockquote>
<p dir="auto">Note: Some scripts hosted here are published for reference on methodology, but may not be immediately executable against public APIs. We're working hard on making the pipelines easier to run "out of the box" over the next few days, and appreciate your patience in the interim!</p>
</blockquote>
<p dir="auto">First, clone the repo and install the promptbase package:</p>

<p dir="auto">Next, decide which tests you'd like to run. You can choose from:</p>
<ul dir="auto">
<li>bigbench</li>
<li>drop</li>
<li>gsm8k</li>
<li>humaneval</li>
<li>math</li>
<li>mmlu</li>
</ul>
<p dir="auto">Before running the tests, you will need to download the datasets from the original sources (see below) and place them in the <code>src/promptbase/datasets</code> directory.</p>
<p dir="auto">After downloading datasets and installing the promptbase package, you can run a test with:</p>
<p dir="auto"><code>python -m promptbase dataset_name</code></p>
<p dir="auto">For example:</p>
<p dir="auto"><code>python -m promptbase gsm8k</code></p>
<h2 tabindex="-1" dir="auto">Dataset Links</h2>
<p dir="auto">To run evaluations, download these datasets and add them to /src/promptbase/datasets/</p>
<ul dir="auto">
<li>MMLU: <a href="https://github.com/hendrycks/test">https://github.com/hendrycks/test</a></li>
<li>HumanEval: <a href="https://huggingface.co/datasets/openai_humaneval" rel="nofollow">https://huggingface.co/datasets/openai_humaneval</a></li>
<li>DROP: <a href="https://allenai.org/data/drop" rel="nofollow">https://allenai.org/data/drop</a></li>
<li>GSM8K: <a href="https://github.com/openai/grade-school-math">https://github.com/openai/grade-school-math</a></li>
<li>MATH: <a href="https://huggingface.co/datasets/hendrycks/competition_math" rel="nofollow">https://huggingface.co/datasets/hendrycks/competition_math</a></li>
<li>Big-Bench-Hard: <a href="https://github.com/suzgunmirac/BIG-Bench-Hard">https://github.com/suzgunmirac/BIG-Bench-Hard</a>
The contents of this repo need to be put into a directory called <code>BigBench</code> in the <code>datasets</code> directory</li>
</ul>
<h2 tabindex="-1" dir="auto">Other Resources:</h2>
<p dir="auto">Medprompt Blog: <a href="https://www.microsoft.com/en-us/research/blog/the-power-of-prompting/" rel="nofollow">https://www.microsoft.com/en-us/research/blog/the-power-of-prompting/</a></p>
<p dir="auto">Medprompt Research Paper: <a href="https://arxiv.org/abs/2311.16452" rel="nofollow">https://arxiv.org/abs/2311.16452</a></p>
<p dir="auto">Medprompt+: <a href="https://www.microsoft.com/en-us/research/blog/steering-at-the-frontier-extending-the-power-of-prompting/" rel="nofollow">https://www.microsoft.com/en-us/research/blog/steering-at-the-frontier-extending-the-power-of-prompting/</a></p>
<p dir="auto">Microsoft Introduction to Prompt Engineering: <a href="https://learn.microsoft.com/en-us/azure/ai-services/openai/concepts/prompt-engineering" rel="nofollow">https://learn.microsoft.com/en-us/azure/ai-services/openai/concepts/prompt-engineering</a></p>
<p dir="auto">Microsoft Advanced Prompt Engineering Guide: <a href="https://learn.microsoft.com/en-us/azure/ai-services/openai/concepts/advanced-prompt-engineering?pivots=programming-language-chat-completions" rel="nofollow">https://learn.microsoft.com/en-us/azure/ai-services/openai/concepts/advanced-prompt-engineering?pivots=programming-language-chat-completions</a></p>
</article>
          </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[BrainGPT turns thoughts into text (217 pts)]]></title>
            <link>https://www.iflscience.com/new-mind-reading-braingpt-turns-thoughts-into-text-on-screen-72054</link>
            <guid>38673854</guid>
            <pubDate>Sun, 17 Dec 2023 16:22:05 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.iflscience.com/new-mind-reading-braingpt-turns-thoughts-into-text-on-screen-72054">https://www.iflscience.com/new-mind-reading-braingpt-turns-thoughts-into-text-on-screen-72054</a>, See on <a href="https://news.ycombinator.com/item?id=38673854">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><section><header><div><p><img alt="clock" title="clock" width="16" height="16" src="https://assets.iflscience.com/svg/_clock.svg"><span>PUBLISHED</span><time data-published-date="2023-12-17T09:56:04Z" date-time="2023-12-17T09:56:04Z"></time></p></div><h2>New Mind-Reading "BrainGPT" Turns Thoughts Into Text On Screen</h2><h2>It offers new hope to people unable to communicate in other ways.</h2></header><figure><picture><source media="(min-width: 1000px)" srcset="https://assets.iflscience.com/assets/articleNo/72054/aImg/72811/braingpt-l.webp" type="image/webp"><source media="(min-width: 1000px)" srcset="https://assets.iflscience.com/assets/articleNo/72054/aImg/72811/braingpt-l.jpg" type="image/jpeg"><source media="(max-width: 567px)" srcset="https://assets.iflscience.com/assets/articleNo/72054/aImg/72811/braingpt-m.webp" type="image/webp"><source media="(max-width: 567px)" srcset="https://assets.iflscience.com/assets/articleNo/72054/aImg/72811/braingpt-m.jpg" type="image/jpeg"><img src="https://assets.iflscience.com/assets/articleNo/72054/aImg/72811/braingpt-m.jpg" alt="Thought signals in the brain." title="&quot;BrainGPT&quot;" type="image/jpeg" loading="lazy"></picture><figcaption><p>The researchers can decode information in the brain without invasive technology.&nbsp;</p><p>Image Credit: Chaikom/Shutterstock.com</p></figcaption></figure><article><div><p id="isPasted">“Mind reading” may be about to become a reality – and in the most literal sense possible, as a new breakthrough from researchers at the University of Technology Sydney’s GrapheneX-UTS Human-centric Artificial Intelligence Centre sees thoughts transformed into words on a screen.</p><p>“This research represents a pioneering effort in translating raw EEG waves directly into language, marking a significant breakthrough in the field,” <a href="https://www.uts.edu.au/news/tech-design/portable-non-invasive-mind-reading-ai-turns-thoughts-text" target="_blank">said</a> <a href="https://profiles.uts.edu.au/Chin-Teng.Lin" target="_blank" rel="noopener noreferrer">Ching-Ten Lin</a>, Distinguished Professor at the UTS School of Computer Science and Director of the GrapheneX-UTS HAI Centre.</p><p>“It is the first to incorporate discrete encoding techniques in the brain-to-text translation process, introducing an innovative approach to neural decoding,” Lin, who led the research, explained. “The integration with large language models is also opening new frontiers in neuroscience and AI.”</p><p>In a study that has been selected as a spotlight paper at the NeurIPS conference, an annual meeting of researchers in artificial intelligence and machine learning, participants silently read passages of text while an AI model called DeWave – using only their brainwaves as input – projected those words onto a screen.&nbsp;</p><p><span contenteditable="false" draggable="true"><iframe width="560" height="315" src="https://www.youtube.com/embed/crJst7Yfzj4?si=pyhjs0xS8pSsRE65" title="YouTube video player" frameborder="0" allowfullscreen=""></iframe></span><br></p><p>While it’s <a href="https://www.iflscience.com/ai-brain-activity-decoder-can-translate-thoughts-into-written-words-68686" target="_blank">not the first</a> technology to be able to <a href="https://www.iflscience.com/write-book-your-mind-29055" target="_blank">translate brain signals into language</a>, it’s the only one so far to require neither <a href="https://www.iflscience.com/scientists-peek-inside-a-persons-minds-eye-by-reading-their-brain-waves-63037" target="_blank">brain implants</a> nor access to <a href="https://www.iflscience.com/new-brain-scanning-algorithm-can-read-your-thoughts-65945" target="_blank">a full-on MRI machine</a>. It also has an edge on predecessors that require additional input such as eye-tracking software, the researchers say, as the new technology can be used with or without such extras.</p><p>Instead, users need only to wear a cap that records their brain activity via electroencephalogram (EEG) – much more practical and convenient than an eye-tracker (not to mention an MRI machine). That meant the signal was a bit noisier than information gained from implants, the researchers admitted – though even then, the tech performed pretty well in trials. Accuracy measurements using the BLEU algorithm – a way to evaluate the similarity of an original text to a machine-translated output by giving it a score between 0 and 1 – put the new tech at about 0.4.&nbsp;</p><p>That, admittedly, isn’t as good as some of the other options that depend on these more invasive methods. “The model is more adept at matching verbs than nouns,” explained Yiqun Duan, first author on the paper accompanying the research – and “when it comes to nouns, we saw a tendency towards synonymous pairs rather than precise translations, such as ‘the man’ instead of ‘the author’.”&nbsp;</p><p>“We think [these errors are] because when the brain processes these words, semantically similar words might produce similar brain wave patterns,” Duan said.&nbsp;</p><p>But the researchers believe they can improve this accuracy up to 0.9 – a level comparable with traditional language translation programs. They already have an advantage, they suspect, due to carrying out their tests on 29 participants – it may not sound like a lot, but it’s an order of magnitude higher than many other decoding tech trials.</p><p>“Despite the challenges, our model yields meaningful results,” Duan said, “aligning keywords and forming similar sentence structures.”</p><p>The results were shown at the <a href="https://neurips.cc/virtual/2023/index.html" target="_blank">NeurIPS conference</a> and a preprint can be found on <a href="https://arxiv.org/abs/2309.14030v2" target="_blank">ArXiV</a>. It has yet to be peer reviewed.</p></div></article><div><hr><div><h3>ARTICLE POSTED IN</h3></div></div></section><br><div><header><img alt="technology" title="technology" width="16" height="16" src="https://assets.iflscience.com/svg/label/_technology.svg"><h2>More Technology Stories</h2></header></div></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[US nuclear-fusion lab enters new era: achieving 'ignition' over and over (157 pts)]]></title>
            <link>https://www.nature.com/articles/d41586-023-04045-8</link>
            <guid>38673654</guid>
            <pubDate>Sun, 17 Dec 2023 15:55:34 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.nature.com/articles/d41586-023-04045-8">https://www.nature.com/articles/d41586-023-04045-8</a>, See on <a href="https://news.ycombinator.com/item?id=38673654">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
                    <p>In December 2022, after <a href="https://www.nature.com/articles/d41586-022-02022-1" data-track="click" data-label="https://www.nature.com/articles/d41586-022-02022-1" data-track-category="body text link">more than a decade of effort and frustration</a>, scientists at the US National Ignition Facility (NIF) <a href="https://www.nature.com/articles/d41586-022-04440-7" data-track="click" data-label="https://www.nature.com/articles/d41586-022-04440-7" data-track-category="body text link">announced that they had set a world record</a> by producing a fusion reaction that released more energy than it consumed — a phenomenon known as ignition. They have now proved that the feat was no accident by replicating it again and again, and the administration of US President Joe Biden is looking to build on this success by establishing a trio of US research centres to help advance the science.</p><article data-label="Related">
  <a href="https://www.nature.com/articles/d41586-023-03923-5" data-track="click" data-track-label="recommended article"><img alt="" src="https://media.nature.com/w400/magazine-assets/d41586-023-04045-8/d41586-023-04045-8_26526928.jpg"><p>Nuclear-fusion breakthrough: this physicist helped to achieve the first-ever energy gain</p></a>
 </article><p>The stadium-sized laser facility, housed at the Lawrence Livermore National Laboratory (LLNL) in California, has unequivocally achieved its goal of ignition in four out of its last six attempts, creating a reaction that generates pressures and temperatures greater than those that occur inside the Sun.</p><p>“I’m feeling pretty good,” says Richard Town, a physicist who heads the lab’s inertial-confinement fusion science programme at the LLNL. “I think we should all be proud of the achievement.”</p><p>The NIF was designed not as a power plant, but as a facility to recreate and study the reactions that occur during thermonuclear detonations after the United States halted underground weapons testing in 1992. The higher fusion yields are already being used to advance nuclear-weapons research, and have also fuelled enthusiasm about fusion as a limitless source of clean energy. US secretary of state John Kerry called for new international partnerships to advance fusion energy at the COP28 climate summit in Dubai last week, and the US Department of Energy (DOE), which oversees the NIF, followed up by announcing the new research hubs, to be led by Lawrence Livermore, the University of Rochester in New York and Colorado State University in Fort Collins.</p><p>Building the NIF was “a leap of faith” for many, and its success has had a real impact on the fusion community, as well as on public perception, says Saskia Mordijck, a physicist at the College of William and Mary in Willamsburg, Virginia. “In that sense, what is important is that scientists said they could do something, and then they actually did do something.”</p><h2>Hot shots</h2><p>The NIF works by firing 192 laser beams at a frozen pellet of the hydrogen isotopes deuterium and tritium that is housed in a diamond capsule suspended inside a gold cylinder. The resulting implosion causes the isotopes to fuse, creating helium and copious quantities of energy. On 5 December 2022, those fusion reactions for the first time generated more energy — roughly 54% more — than the laser beams delivered to the target.</p><p>The facility set a new record on 30 July when its beams delivered the same amount of energy to the target — 2.05 megajoules — but, this time, the implosion generated 3.88 megajoules of fusion energy, an 89% increase over the input energy. Scientists at the laboratory achieved ignition during two further attempts in October (see ‘A year of progress’). And the laboratory’s calculations suggest that two others in June and September generated slightly more energy than the lasers provided, but not enough to confirm ignition.</p><figure>
 <picture>
  <source type="image/webp" srcset="https://media.nature.com/lw767/magazine-assets/d41586-023-04045-8/d41586-023-04045-8_26532550.jpg?as=webp 767w, https://media.nature.com/lw319/magazine-assets/d41586-023-04045-8/d41586-023-04045-8_26532550.jpg?as=webp 319w" sizes="(max-width: 319px) 319px, (min-width: 1023px) 100vw,  767px">
  <img alt="A year of progress: Timeline of 'ignition' experiments conducted by the US National Ignition Facility since December 2022." loading="lazy" src="https://media.nature.com/lw767/magazine-assets/d41586-023-04045-8/d41586-023-04045-8_26532550.jpg">
  <figcaption>
   <p><span>Source: Lawrence Livermore National Laboratory</span></p>
  </figcaption>
 </picture>
</figure><p>For many scientists, the results confirm that the laboratory is now operating in a new regime: researchers can repeatedly hit a goal they’ve been chasing for more than a decade. Tiny variations in the laser pulses or minor defects in the diamond capsule can still allow energy to escape, making for an imperfect implosion, but the scientists now better understand the main variables at play and how to manipulate them.</p><p>“Even when we have these issues, we can still get more than a megajoule of fusion energy, which is good,” says <a href="https://www.nature.com/articles/d41586-023-03923-5" data-track="click" data-label="https://www.nature.com/articles/d41586-023-03923-5" data-track-category="body text link">Annie Kritcher</a>, the NIF’s lead designer on this series of experiments.</p><h2>New hubs</h2><p>It’s a long way from there to providing fusion energy to the power grid, however, and the NIF, although currently home to the world’s largest laser, is not well-suited for that task. The facility’s laser system is enormously inefficient, and more than 99% of the energy that goes into a single ignition attempt is lost before it can reach the target.</p><p>Developing more efficient laser systems is one goal of the DOE’s new inertial-fusion-energy research programme. This month, the agency announced US$42 million over four years to establish three new research centres — each involving a mix of national laboratories, university researchers and industry partners — that will work towards this and other advances.</p><article data-label="Related">
  <a href="https://www.nature.com/articles/d41586-022-04440-7" data-track="click" data-track-label="recommended article"><img alt="" src="https://media.nature.com/w400/magazine-assets/d41586-023-04045-8/d41586-023-04045-8_25275218.jpg"><p>Nuclear-fusion lab achieves ‘ignition’: what does it mean?</p></a>
 </article><p>This investment is the first coordinated effort to develop not just the technologies, but also the workforce for a future laser-fusion industry, says Carmen Menoni, a physicist who is heading up the hub at Colorado State University.</p><p>So far, most government investments in fusion-energy research have gone towards devices known as tokamaks, which use magnetic fields inside a doughnut-shaped ‘torus’ to confine fusion reactions. This is the approach under development at ITER, an international partnership to build the world’s largest fusion facility near Saint-Paul-lez-Durance, France. Tokamaks have also been the focus of many fusion investments in the private sector, but dozens of companies are pursuing other approaches, such as laser fusion.</p><p>The timing for a dedicated laser-fusion programme is right, says Menoni, and the decision to pursue it wouldn’t have happened without the NIF’s recent success. “We now know it will work,” she says. “What will take time is to develop the technology to a level where we can build a power plant.”</p><p>Back at the NIF, Kritcher’s latest series of experiments features a 7% boost in laser energy, which should, in theory, lead to even larger yields. The first experiment in this series was one of the successful ignitions, on 30 October. Although it didn’t break the record, an input of 2.2 megajoules of laser energy yielded an output of 3.4 megajoules of fusion energy.</p><p>Kritcher chalks up the fact that it didn’t break the record for energy yield to growing pains with the new laser configuration, which is designed to squeeze more energy into the same gold cylinder. Before moving to a larger cylinder, Kritcher says her team is going to focus on changes to the laser pulse that could produce a more symmetrical implosion. “We’ve got four experiments next year,” she says. “Let’s see.”</p>
                </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[The Final Speech from The Great Dictator (336 pts)]]></title>
            <link>https://www.charliechaplin.com/en/articles/29-the-final-speech-from-the-great-dictator-</link>
            <guid>38673292</guid>
            <pubDate>Sun, 17 Dec 2023 15:03:15 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.charliechaplin.com/en/articles/29-the-final-speech-from-the-great-dictator-">https://www.charliechaplin.com/en/articles/29-the-final-speech-from-the-great-dictator-</a>, See on <a href="https://news.ycombinator.com/item?id=38673292">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
<article>




<p>
  <iframe width="640" height="480" src="https://www.youtube.com/embed/J7GY1Xg6X20" frameborder="0" allowfullscreen=""></iframe>
</p>


<p>I’m sorry, but I don’t want to be an emperor. That’s not my business. I don’t want to rule or conquer anyone. I should like to help everyone - if possible - Jew, Gentile - black man - white. We all want to help one another. Human beings are like that. We want to live by each other’s happiness - not by each other’s misery. We don’t want to hate and despise one another. In this world there is room for everyone. And the good earth is rich and can provide for everyone. The way of life can be free and beautiful, but we have lost the way.</p>

<p>Greed has poisoned men’s souls, has barricaded the world with hate, has goose-stepped us into misery and bloodshed. We have developed speed, but we have shut ourselves in. Machinery that gives abundance has left us in want. Our knowledge has made us cynical. Our cleverness, hard and unkind. We think too much and feel too little. More than machinery we need humanity. More than cleverness we need kindness and gentleness. Without these qualities, life will be violent and all will be lost…</p>

<p>The aeroplane and the radio have brought us closer together. The very nature of these inventions cries out for the goodness in men - cries out for universal brotherhood - for the unity of us all. Even now my voice is reaching millions throughout the world - millions of despairing men, women, and little children - victims of a system that makes men torture and imprison innocent people.</p>

<figure><img src="https://photo.charliechaplin.com/images/photos/0000/0225/gd_p_140_big.jpg" alt=""><figcaption></figcaption></figure>

<p>To those who can hear me, I say - do not despair. The misery that is now upon us is but the passing of greed - the bitterness of men who fear the way of human progress. The hate of men will pass, and dictators die, and the power they took from the people will return to the people. And so long as men die, liberty will never perish…</p>

<p>Soldiers! don’t give yourselves to brutes - men who despise you - enslave you - who regiment your lives - tell you what to do - what to think and what to feel! Who drill you - diet you - treat you like cattle, use you as cannon fodder. Don’t give yourselves to these unnatural men - machine men with machine minds and machine hearts! You are not machines! You are not cattle! You are men! You have the love of humanity in your hearts! You don’t hate! Only the unloved hate - the unloved and the unnatural! Soldiers! Don’t fight for slavery! Fight for liberty!</p>

<p>In the 17th Chapter of St Luke it is written: “the Kingdom of God is within man” - not one man nor a group of men, but in all men! In you! You, the people have the power - the power to create machines. The power to create happiness! You, the people, have the power to make this life free and beautiful, to make this life a wonderful adventure.</p>

<p>Then - in the name of democracy - let us use that power - let us all unite. Let us fight for a new world - a decent world that will give men a chance to work - that will give youth a future and old age a security. By the promise of these things, brutes have risen to power. But they lie! They do not fulfil that promise. They never will!</p>

<p>Dictators free themselves but they enslave the people! Now let us fight to fulfil that promise! Let us fight to free the world - to do away with national barriers - to do away with greed, with hate and intolerance. Let us fight for a world of reason, a world where science and progress will lead to all men’s happiness. Soldiers! in the name of democracy, let us all unite!</p>

<p><em>Final speech from The Great Dictator Copyright © Roy Export S.A.S. All rights reserved</em></p>

<hr>

<p><a href="https://www.charliechaplin.com/en/films/7-The-Great-Dictator">The Great Dictator</a> was Chaplin’s first film with dialogue.    Chaplin plays both a little Jewish barber, living in the ghetto, and Hynkel, the dictator ruler of Tomainia. In his autobiography Chaplin quotes himself as having said: “One doesn’t have to be a Jew to be anti Nazi.  All one has to be is a normal decent human being.”</p>

<p>Chaplin and Hitler were born within a week of one another.  “There was something uncanny in the resemblance between the Little Tramp and Adolf Hitler, representing opposite poles of humanity, ” writes Chaplin biographer David Robinson, reproducing an unsigned article from The Spectator dated 21st April 1939:<br>
“Providence was in an ironical mood when, fifty years ago this week, it was ordained that Charles Chaplin and Adolf Hitler should make their entry into the world within four days of each other….Each in his own way has expressed the ideas, sentiments, aspirations of the millions of struggling citizens ground between the upper and the lower millstone of society. (…) Each has mirrored the same reality – the predicament of the “little man” in modern society.  Each is a distorting mirror, the one for good, the other for untold evil.”</p>

<p>Chaplin spent many months drafting and re-writing the speech for the end of the film, a call for peace from the barber who has been mistaken for Hynkel. Many people criticized the speech, and thought it was superfluous to the film.  Others found it uplifting. Regrettably Chaplin’s words are as relevant today as they were in 1940.</p>

<h4 id="transcript-of-charlie-chaplins-final-speech-in-the-great-dictatorenfilms7-the-great-dictator">Transcript of Charlie Chaplin’s Final Speech in <a href="https://www.charliechaplin.com/en/films/7-The-Great-Dictator">The Great Dictator</a>
</h4>



</article>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Evaluating New Software Forges (133 pts)]]></title>
            <link>https://notgull.net/finding-a-forge/</link>
            <guid>38672386</guid>
            <pubDate>Sun, 17 Dec 2023 12:26:17 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://notgull.net/finding-a-forge/">https://notgull.net/finding-a-forge/</a>, See on <a href="https://news.ycombinator.com/item?id=38672386">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
    <p>What options <em>are</em> there other than <a href="https://github.com/">GitHub</a>?</p>

<p>Oh boy, I sure do love contributing to open source software on the largest software forge in the world! I hope they haven’t started down the slow and painful process of <a href="https://en.wikipedia.org/wiki/Enshittification">enshittification</a> by following vague, ill-defined industry trends!</p>

<p><img src="https://notgull.net/images/github-home.png" alt="[GitHub]'s home page"></p>

<p>Wait, what’s that? Computer! Enhance!</p>

<p><img src="https://notgull.net/images/github-ai.png" alt="The same image but zoomed in on AI"></p>

<p>Well, I guess I’m ready to find a new forge!</p>

<h2 id="software-host-hellscape">Software Host Hellscape</h2>

<p>In all seriousness, I’ve been looking to move off of <a href="https://github.com/">GitHub</a> for a while now. Let me be clear, <a href="https://github.com/">GitHub</a> is still far and away the best website for open source discovery. Not to mention, its CI offerings are very nice, especially for something free. Yes, there are better paid CI offerings, but for something that costs zero dollars I’ve found it incredibly useful.</p>

<p>However, one thing has made me skeptical of <a href="https://github.com/">GitHub</a> is its “<a href="https://github.com/features/copilot">Copilot</a>” offering. I’ll admit, I was in the beta program for Copilot, and found it really neat. Being able to write large amounts of code from small comments was very nice, even if it was really bad practice.</p>

<p>Then I found out it was <a href="https://www.zdnet.com/article/is-github-copilots-code-legal-ethically-right/">training on GPL-licensed data</a>, which left a pretty bad taste in my mouth. In addition to the fact that I’m increasingly uncomfortable with hosting my free software on a closed source forge, run by Microsoft.</p>

<p>Let’s take a look at everybody else.</p>

<h2 id="gitlab-gauss">GitLab Gauss</h2>

<p><a href="https://about.gitlab.com/">GitLab</a> is the original <a href="https://github.com/">GitHub</a> competitor. The Linux to Microsoft’s Windows, or the MariaDB to Oracle’s MySQL. This has made it the most popular <a href="https://github.com/">GitHub</a> competitor by far, by virtue of people vocally quitting <a href="https://github.com/">GitHub</a> in favor of [GitLab].</p>

<p>Unfortunately, I didn’t really consider GitLab when I was finding a place to move. First of all, <a href="https://about.gitlab.com/blog/2016/07/20/gitlab-is-open-core-github-is-closed-source/">they aren’t actually open source</a>. They’re “open core”, which, I admit, is better than closed source. However, like I said, I’m uncomfortable building free software on infrastructure that isn’t.</p>

<p>I know I can download GitLab and set it up on my own server. However, I’m a software developer, not a sysadmin. I want to spend my time developing software, not putting out fires and paying AWS bills for the rest of time.</p>

<p>Also, GitLab has adopted the unfortunate strategy of “following along with whatever <a href="https://github.com/">GitHub</a> does”. They’ve tried to jump onto the bandwagon so frequently, they’ve gotten splinters. For instance, what happens when we check their homepage?</p>

<p><img src="https://notgull.net/images/gitlab-home.png" alt="GitLab's home page"></p>

<p>Computer! Do the thing again!</p>

<p><img src="https://notgull.net/images/gitlab-ai.png" alt="Same image but with &quot;AI&quot; zoomed in"></p>

<p>Good golly, it’s even the same wording! Yeah, I’ll pass.</p>

<h2 id="sourcehut-scramble"><a href="https://sr.ht/">SourceHut</a> Scramble</h2>

<p><a href="https://sr.ht/"><code>sr.ht</code></a> takes the opposite approach as GitLab. Instead of trying to follow along with <a href="https://github.com/">GitHub</a>’s trends, it’s elected to do go in the other direction. Whenever <a href="https://github.com/">GitHub</a> does something, <a href="https://sr.ht/">SourceHut</a> does the exact opposite.</p>

<p>Pull requests? Too centralized, let’s construct a suitable code contribution system around email. Discussion? Why not IRC, it’s been around since the Bronze Age. Get rid of Mercurial support? Not interested.</p>

<p>I really like <a href="https://sr.ht/">SourceHut</a>. When you go to their homepage, they’re not showing off their fancy CSS effects or telling you about their AI offerings. They give you a simple user interface and some of the projects they host.</p>

<p><img src="https://notgull.net/images/sourcehut-home.png" alt="sr.ht's homepage"></p>

<p>There was also much to impress me. Their CI offerings are better than <a href="https://github.com/">GitHub</a>, which alone justified me paying the humble $2/month price tag. Rather than needing a complicated YAML file to run a CI system, it’s just cloning Git repos and running commands. It’s delightfully simple yet powerful. Having native BSD and Plan9 runners doesn’t quite make up for its inability to run Windows, but I’m sure I can work around that.</p>

<p>Not to mention, <a href="https://sr.ht/">SourceHut</a> has the <em>second</em> best repository discovery system. When I go to <code>sr.ht</code>’s “explore” tab, I’m immediately greeted by a slew of interesting projects. Whether it’s a <a href="https://git.sr.ht/~crc_/retroforth">powerful Forth dialect</a> that brings a lot of genuinely exciting ideas to the table, or a <a href="https://sr.ht/~mcf/cproc/">tiny C11 compiler</a> written in simple ANSI C, I’m always amazed whenever I open up that tab.</p>

<p>I liked it so much that <a href="https://hachyderm.io/@notgull/111207497257139313">I announced that I was moving my personal projects to SourceHut</a>. However, after moving my <a href="https://crates.io/crates/theo"><code>theo</code></a> project to <a href="https://sr.ht/">SourceHut</a>, I found myself dissatisfied with a few things.</p>

<p>For one, the email-based workflow was a lot clunkier than I expected. In theory, building code contribution on top of a standard protocol that’s been around since the 60’s sounds like a good idea. In practice, it’s <a href="https://social.treehouse.systems/@marcan/109863991681394714">a lot clunkier</a> than you’d expect, especially since most modern email clients are simply not built to read and write code.</p>

<p>After trying it for myself I can see that it might turn off a lot of people from contributing. I’m already losing a lot of potential contributors by moving off of <a href="https://github.com/">GitHub</a>. I don’t need those remaining contributors to also be turned off by a workflow completely different than what they’re probably used to.</p>

<p>Still, I can imagine this workflow working for many people, especially ones who already have a decent setup for email-based projects like Linux.</p>

<h2 id="codeberg">Codeberg</h2>

<p><a href="https://codeberg.org/">Codeberg</a> is a public instance of <a href="https://forgejo.org/">Forgejo</a>, which is in turn a fork of <a href="https://about.gitea.com/">Gitea</a>. It’s got a pretty nice interface similar enough to <a href="https://github.com/">GitHub</a>’s. It’s got the familiar pull-request-based contribution interface. The CI is <em>good enough</em>, I suppose. Docker containers aren’t the best CI environment, but I can certainly think of worse.</p>

<p><img src="https://notgull.net/images/codeberg-home.png" alt="Codeberg's home page"></p>

<p>So what’s not to like?</p>

<p>My main problem is that <a href="https://codeberg.org/">Codeberg</a> has a very limited CI capacity, and I have a lot of projects that require significant testing. <a href="https://crates.io/crates/theo"><code>theo</code></a>, for instance, requires these things to be tested:</p>

<ul>
  <li>Make sure it compiles on Windows, Mac, Linux, WASM, Redox, and whatever oblique platform people run Rust on nowadays.</li>
  <li>Check the various different backends that <a href="https://crates.io/crates/theo"><code>theo</code></a> supports: pure software rendering, <a href="https://crates.io/crates/wgpu"><code>wgpu</code></a> and OpenGL. Not to mention all of the different interfaces to OpenGL, so <code>wgl</code>, GLX, EGL…</li>
  <li>Check formatting and linting.</li>
</ul>

<p>…which doesn’t even cover testing. For rendering frameworks like <a href="https://crates.io/crates/theo"><code>theo</code></a>, you want to have some pre-defined rendering programs that render your code to images. That way, you can regenerate these images and compare against an existing set of images in order to check for regressions.</p>

<p>This isn’t a practical concern, although it really should be. It’s a moral concern. You have an organization like <a href="https://codeberg.org/">Codeberg</a>, donating a significant amount of time and resources to try to make a positive difference in the world of software. Now, here I am, sucking up all of those compute resources for my insignificant little projects.</p>

<p>Of course, while pondering this moral concern, I realized that I’ve locked myself into a <a href="https://en.wikipedia.org/wiki/Catch-22_(logic)">Catch-22</a>. I can’t use any independent project’s CI because of my concerns that I would drain too many of their resources. On the other hand, I can’t use any large company’s CI because I don’t want to host my project with a large company. I can’t self host, because that would be a pain.</p>

<p>Would it?</p>

<h2 id="self-hosting-gitea">Self-Hosting Gitea</h2>

<p>I said I didn’t want to self-host. I worked in IT for two years, so I’ve already gotten my fill of fighting with both servers and people.</p>

<p>However, in a Discord I’m in, an acquaintance of mine talked about how they set up <a href="https://about.gitea.com/">Gitea</a> and <a href="https://www.drone.io/">Drone</a> CI on a school Kubernetes cluster they had access to. I mentioned my predicaments in finding a forge service, and they said that it was only two configuration files.</p>

<p>That tempted me. Not enough to deal with the absolute nightmare that is Kubernetes, but enough for me to rent out a couple of <a href="https://en.wikipedia.org/wiki/Timeline_of_Amazon_Web_Services">Lightsail</a> servers to experiment.</p>

<p>I’d like to say that I set up the entire thing in a weekend, but it wasn’t that simple. Sure, it was easy enough to install <a href="https://about.gitea.com/">Gitea</a> and <a href="https://www.drone.io/">Drone</a> in Docker containers. Sure, it wasn’t too hard after that to set up my DNS records to forward <a href="https://src.notgull.net/">src.notgull.net</a> to that new serer. Yeah, it’s probably hacky to set up my CI system on a public cloud, but as long as I keep people from abusing it that aren’t me it shouldn’t be <em>too</em> bad, right?</p>

<p>Of course, I forget my crucial weakness: my perfectionism. Sure, my site looks pretty good… but it looks a bit ugly. Let’s play around with themes for three days until I find one I like. Oh, the logo doesn’t look good with my new theme. Let’s convert the MS paint drawing I call my avatar into SVG and then set it up to be this site’s logo. Oh, Gitea has a weird templating system; let’s compile it from scratch!</p>

<p><img src="https://notgull.net/images/srcnotgull-home.png" alt="src.notgull.net home"></p>

<p>At the end of this week(!)long process, I had a somewhat functional <a href="https://src.notgull.net/">Git forge</a>, with the following features:</p>

<ul>
  <li>A decent Linux <a href="https://www.drone.io/">Drone</a> CI setup.</li>
  <li>A <a href="https://docs.renovatebot.com/">Dependabot clone</a> for automatic updates.</li>
  <li>A system that allows everyone to create an account and open issues, but doesn’t allow people to create repos or mess with the CI.
    <ul>
      <li>If you want to create a repo for PR’s sake, email me at dev at notgull dot net and I can set you up.</li>
    </ul>
  </li>
  <li>A mirror to <a href="https://github.com/">GitHub</a>, so I can still take advantage of their code discovery.
    <ul>
      <li>Yeah yeah, I know, but keeping an arm’s length from <a href="https://github.com/">GitHub</a> is a win in my book.</li>
    </ul>
  </li>
</ul>

<p>I’ve uploaded a lot of my code to there, and it seems to be working well so far. I have to admit, it is somewhat liberating to have full control of how your code is forged.</p>

<h2 id="parting-shots">Parting Shots</h2>

<p>I still consider this to be in the “experimental” stage. If it ends up being too inconvenient or too expensive, I’ll probably move it somewhere else. Still, having my own space for code that I can do whatever I want with is very nice. Let’s hope it keeps working well and this blog post doesn’t age like milk.</p>

<blockquote>
  <p>All photos are from public websites and fall under free use, as this is a review.</p>
</blockquote>


  </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[The origin of the law of torture: A cautionary tale (115 pts)]]></title>
            <link>https://daviddfriedman.substack.com/p/torture</link>
            <guid>38670866</guid>
            <pubDate>Sun, 17 Dec 2023 06:40:28 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://daviddfriedman.substack.com/p/torture">https://daviddfriedman.substack.com/p/torture</a>, See on <a href="https://news.ycombinator.com/item?id=38670866">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><article><div dir="auto"><p><span>People in the past worried about convicting the innocent too. In the early Middle Ages they had a solution: Let God judge. A defendant could be subjected to an ordeal such as plunging his hand into boiling water, carrying a red hot iron, being dumped bound into water. Various passages in the Bible were interpreted to imply that God would reveal guilt (hand injured or body sank) or innocence (not injured, floated). Since God was omniscient it was an approach that guaranteed a correct verdict.</span><span><a data-component-name="FootnoteAnchorToDOM" id="footnote-anchor-1-139789134" href="https://daviddfriedman.substack.com/p/torture#footnote-1-139789134" target="_self" rel="">1</a></span></p><p>The use of ordeals was eventually abandoned on theological grounds. A more careful examination of the biblical passages found little support for it and it could be viewed as an attempt by humans to compel God to serve them, religiously dubious. In 1215 the fourth Lateran council rejected the religious legitimacy of judicial ordeals and banned priests from participating in them. Over the next few decades most European countries abandoned their use.</p><p>That left medieval judicial systems with the problem of finding another way of being certain a defendant was guilty. The solution was to impose a very high standard of proof,&nbsp; evidence “clear as the noonday sun.” Conviction required either two unimpeachable eyewitnesses to the crime or a voluntary confession. Circumstantial evidence, however strong, was insufficient.</p><blockquote><p><span>In the history of Western culture no legal system has ever made a more valiant effort to perfect its safeguards and thereby to exclude completely the possibility of mistaken conviction. But the Europeans learned in due course the inevitable lesson. They had set the level of safeguard too high. They had constructed a system of proof that could as a practical matter be effective only in cases involving overt crime or repentant criminals. Because society cannot long tolerate a legal system that lacks the capacity to convict unrepentant persons who commit clandestine crimes, something had to be done … .(</span><a href="http://digitalcommons.law.yale.edu/fss_papers/543" rel="">Langbein 1978</a><span>)</span></p></blockquote><p>The solution was the law of torture. Once the court had half-proof, one eyewitness or the equivalent in circumstantial evidence, the defendant could be tortured into confessing. A confession under torture was not voluntary so did not count, but that problem could be dealt with. Stop the torture and the next day ask the defendant if he is still willing to confess. Since he is now not being tortured the confession is voluntary. If he doesn’t confess, torture him again.</p><p>John Langbein, my source for this account, offers a parallel story in modern law. Two hundred years ago, jury trials were short:</p><blockquote><p>In the Old Bailey in the 1730s we know that the court routinely processed between twelve and twenty jury trials for felony in a single day. A single jury would be impaneled and would hear evidence in numerous unrelated cases before retiring to formulate verdicts in all. Lawyers were not employed in the conduct of ordinary criminal trials, either for the prosecution or the defense. The trial judge called the witnesses (whom the local justice of the peace had bound over to appear), and the proceeding transpired as a relatively unstructured “altercation” between the witnesses and the accused. In the 1790s, when the Americans were constitutionalizing English jury trial, it was still rapid and efficient. “The trial of Hardy for high treason in 1794 was the first that ever lasted more than one day, and the court seriously considered whether it had any power to adjourn… .”</p></blockquote><p><span>Over the years since trials have become longer and much more complicated, at least in part to reduce the risk of convicting the wrong person. Patricia Hearst’s trial for bank robbery lasted forty days. That was unusually long, but the average felony jury trial in Los Angeles in 1968 took 7.2 days, more than a hundred times the length of a felony trial in the Old Bailey in the 1730’s. If every felony conviction in the U.S. took that long, felony trials alone would require the full time efforts of more than the total number of judges in the state and federal systems</span><span><a data-component-name="FootnoteAnchorToDOM" id="footnote-anchor-2-139789134" href="https://daviddfriedman.substack.com/p/torture#footnote-2-139789134" target="_self" rel="">2</a></span><span> and close to a million jurors, court attendants, and the like. Not impossible but very expensive.</span></p><p>The American legal system found a less expensive alternative. Like its medieval predecessor, it substituted confession for trial. The medieval confession was motivated by the threat of torture. The modern version, a plea bargain, is motivated by the threat of a much more severe sentence if the defendant insists on a trial and is convicted. Like the medieval version, it preserves the form — every felony defendant has the right to a jury trial, a lawyer, and all the paraphernalia of the modern law of criminal defense — but not the substance. Conviction after a lengthy and careful jury trial is arguably evidence of guilt beyond a reasonable doubt. The willingness to accept a sentence of a year, possibly a year already served while awaiting trial, instead of the risk of ten years if convicted is not. Currently in the U.S. about ninety-seven percent of felony convictions are the result of plea bargains, three percent of jury trials.</p><p>Under both Athenian and Roman law, slave testimony could only be taken under torture. Presumably the theory was that slaves were interrogated to get evidence against their owners, the owner had ways of putting pressure on the slave, so torture was needed to get the slave to tell the truth. In Imperial Chinese law, not only the defendant but also witnesses could be tortured.</p><p>Our main source of information on Athenian law consists of orations written by professional orators to be memorized and delivered by a party to a law suit. There is a surviving oration which claims that slave testimony under torture is perfectly reliable, that there has never been a case where it turned out to be false. There is another oration making the obvious argument on the other side, that such testimony is worthless since the slave will say whatever the torturer wants him to say.</p><p>They were both written by the same orator.</p><p>People in other legal systems that used torture were also aware of the problem. There is a collection of Chinese cases compiled in the 13th century for the use of magistrates. Many of them are cases where a clever judge realizes that an innocent person has been forced to confess under torture and figures out who is really guilty.</p><p>That raises an obvious question: If they saw the problem with torture, why did they continue to employ it? One answer is that extracting information might only have been an excuse, that the real purpose was to punish someone without having to first convict him. That is a possible explanation in some contexts. But it does not explain contexts where the person being tortured was not the suspect but a potential witness.</p><p>A second possible explanation is the belief that a competent interrogator could distinguish a real confession from a fake one. That strikes me as the most likely explanation in the Roman and Athenian cases, where it was the defendant's slave, not the defendant, who was being interrogated.</p><p>A third explanation is that torture might produce information that could be checked. That is the situation in the hypothetical cases sometimes offered in defense of the use of torture: The suspect is being forced to say where the kidnap victim or the time bomb is concealed. More plausibly, where the loot is hidden.</p><p>An example of this approach occurs in the law of the Visigoths, the earliest of the surviving Germanic law codes. Before a suspect could be tortured the accuser had to provide the judge with details of the crime that an innocent defendant would not have known. The defendant's confession was only accepted if it matched the details; if the accuser had made the details public, the defendant could not be tortured. How satisfactory the system was for the defendant would depend on how severe the torture was and how much permanent damage it might do to him but it at least was a way of distinguishing a true confession from a false one. The same approach is used in modern law enforcement, where a confession is validated by the fact that it contains information only a guilty defendant would have.</p><p>Both the Visigothic and the modern versions depend on the honesty of the people conducting the interrogation. A policeman who extracts a confession by either physical pressure or the threat of additional charges can make it more convincing by leaking the relevant information to the defendant in the course of the interrogation. That is an argument for recording all interrogations and making the recordings available to the defense, an option not available to the Visigoths. I do not know if they employed the period equivalent of neutral witnesses to the interrogation.</p><p>The problem with arguments for the use of torture, ancient and modern, is that although one can imagine situations where it would be justified, once legal it is not likely to be limited to such situations. </p></div></article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[The Apollo Syndrome (114 pts)]]></title>
            <link>https://www.teamtechnology.co.uk/tt/t-articl/apollo.htm</link>
            <guid>38670760</guid>
            <pubDate>Sun, 17 Dec 2023 06:16:40 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.teamtechnology.co.uk/tt/t-articl/apollo.htm">https://www.teamtechnology.co.uk/tt/t-articl/apollo.htm</a>, See on <a href="https://news.ycombinator.com/item?id=38670760">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="content">
<p>
<a href="https://www.metarasa.com/team-dynamics-assessment/questionnaire/">
  <video controls="" autoplay="" muted="">
    <source src="https://www.teamtechnology.co.uk/tdasm.mp4" type="video/mp4">
    Your browser does not support HTML5 video.
  </video>
</a>
</p>






<p>
This page describes 'The Apollo Syndrome', a phenomenon discovered by Dr Meredith
Belbin where teams of highly capable individuals can, collectively, perform
badly. </p>
<p>Dr Meredith Belbin is one of the original 'gurus' of Team Building.
In his first book on Management Teams (Belbin, 1981) he reported some unexpectedly
poor results with teams formed of people who had sharp, analytical minds
and high mental ability - he called this the Apollo Syndrome. </p>
<p>His criteria for selecting these teams have elements in common with
criteria for selecting IT, academic or scientific staff - using ability and aptitude tests to select
those with high analytical skills. The initial perception of Belbin's Apollo
teams was that they were bound to win in the team competitions. However,
the results were quite the reverse, and the Apollo teams often finished
near the bottom of eight teams. </p>
<p>This failure seemed to be due to certain flaws in the way the team operated:
</p>
<ul>
<li>They spent excessive time in abortive or destructive debate, trying
to persuade other team members to adopt their own view, and demonstrating
a flair for spotting weaknesses in others' arguments. This led to the discussion
equivalent of '<a href="#The Deadly Embrace">the deadly embrace</a>'. </li>
<li>They had difficulties in their decision making, with little coherence
in the decisions reached (several pressing and necessary jobs were often
omitted). </li>
<li>Team members tended to act along their own favourite lines without
taking account of what fellow members were doing, and the team proved difficult
to manage. </li>
<li>In some instances, teams recognised what was happening but over compensated
- they avoided confrontation, which equally led to problems in decision
making. </li>
</ul>
<h3><b>How Apollo teams succeed</b></h3>
<p>There were successful Apollo teams, however, that were characterised
by </p>
<ul>
<li>the absence of highly dominant individuals, and </li>
<li>a particular style of leadership. </li>
</ul>
<p>Successful leaders were suspicious and sceptical people who sought to
impose some shape or pattern on group discussion, and on the outcome of
group activities. They focused attention on the setting of objectives and
priorities, and shaping the way team effort was applied. Rather than 'drawing
out' team members, the successful leaders were tough, discriminating people
who could both hold their ground in any company, yet not dominate the group.
</p>
<p>A key lesson from Belbin's work is that putting together a team of the
cleverest individuals does not necessarily produce the best results, and
the team needs to be designed ensuring that there is a blend of team roles.
</p>
<h3><b>Apollo Syndrome (Version 2)</b></h3>
<p>The term 'Apollo Syndrome' has also been used to describe the condition
where someone has an overly important view of their role within a team.
It is based on the (supposed) claim of someone to have played a vital role
in the success of NASA's Apollo missions to the Moon, where scientists
had to work all through the night on many occasions, battling against fatigue.
One person claimed a vital role to the whole programme - by making the
coffee that kept them awake! </p>
<p>Perhaps a 'Double Apollo' is where a team is composed of highly capable
people, that achieves little, but claims great success! </p>
<a name="The Deadly Embrace"><h4><b>Definition of The Deadly Embrace</b></h4>
This is a term used in computing some years ago to signify a problem
between two computer programs - where each prevents the other from making
progress.
<blockquote><i>What happens is that Program A takes exclusive control of record
1, and program B takes record 2. Program A then tries to get exclusive
access to record 2, but as this is under exclusive control of the other
program, it can't. The program then waits until record 2 is released. Meanwhile,
program B tries to get exclusive control of record 1, but can't, as it
is under the exclusive control of program A. Program B waits until record
1 is released. Therefore, neither program can make any progress because
it is waiting for the other program to give way. A similar situation can
occur in discussions if each person is trying to get the other to concede
the flaws in his/her argument, without conceding the flaws in his own.
The way out of this situation is to look for the points of agreement, rather
than trying to spot flaws. </i>
</blockquote>
<h4><b>References</b></h4>
<p>Management Teams - Why They Succeed or Fail, (Belbin, 1981), ISBN: 0-7506-0253-8
</p>

<br>

</a><p><a name="The Deadly Embrace">©<span id="copydate">2013</span> Team Technology. </a><a href="https://www.teamtechnology.co.uk/cookies.php">Privacy policy and cookies</a>.</p>


</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[AI bots are now outperforming humans in solving CAPTCHAs (341 pts)]]></title>
            <link>https://arxiv.org/abs/2307.12108</link>
            <guid>38670465</guid>
            <pubDate>Sun, 17 Dec 2023 05:10:31 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://arxiv.org/abs/2307.12108">https://arxiv.org/abs/2307.12108</a>, See on <a href="https://news.ycombinator.com/item?id=38670465">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="content-inner">
    
    
                
    <p><a href="https://arxiv.org/pdf/2307.12108.pdf">Download PDF</a></p><blockquote>
            <span>Abstract:</span>For nearly two decades, CAPTCHAs have been widely used as a means of protection against bots. Throughout the years, as their use grew, techniques to defeat or bypass CAPTCHAs have continued to improve. Meanwhile, CAPTCHAs have also evolved in terms of sophistication and diversity, becoming increasingly difficult to solve for both bots (machines) and humans. Given this long-standing and still-ongoing arms race, it is critical to investigate how long it takes legitimate users to solve modern CAPTCHAs, and how they are perceived by those users.
<br>In this work, we explore CAPTCHAs in the wild by evaluating users' solving performance and perceptions of unmodified currently-deployed CAPTCHAs. We obtain this data through manual inspection of popular websites and user studies in which 1,400 participants collectively solved 14,000 CAPTCHAs. Results show significant differences between the most popular types of CAPTCHAs: surprisingly, solving time and user perception are not always correlated. We performed a comparative study to investigate the effect of experimental context -- specifically the difference between solving CAPTCHAs directly versus solving them as part of a more natural task, such as account creation. Whilst there were several potential confounding factors, our results show that experimental context could have an impact on this task, and must be taken into account in future CAPTCHA studies. Finally, we investigate CAPTCHA-induced user task abandonment by analyzing participants who start and do not complete the task.
    </blockquote>

    <!--CONTEXT-->
    
  </div><div>
      <h2>Submission history</h2><p> From: Yoshimichi Nakatsuka [<a href="https://arxiv.org/show-email/59287c22/2307.12108">view email</a>]      <br>    <strong>[v1]</strong>
        Sat, 22 Jul 2023 15:36:13 UTC (1,815 KB)<br>
</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Hasbro laying off Wizards of the Coast staff is baffling (194 pts)]]></title>
            <link>https://www.geekwire.com/2023/hasbro-laying-off-wizards-of-the-coast-staff-is-baffling-and-could-lead-to-a-brain-drain/</link>
            <guid>38668906</guid>
            <pubDate>Sat, 16 Dec 2023 23:42:21 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.geekwire.com/2023/hasbro-laying-off-wizards-of-the-coast-staff-is-baffling-and-could-lead-to-a-brain-drain/">https://www.geekwire.com/2023/hasbro-laying-off-wizards-of-the-coast-staff-is-baffling-and-could-lead-to-a-brain-drain/</a>, See on <a href="https://news.ycombinator.com/item?id=38668906">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
                
<figure><img fetchpriority="high" decoding="async" width="630" height="473" src="https://cdn.geekwire.com/wp-content/uploads/2019/10/wizards2-630x473.jpg" alt="" srcset="https://cdn.geekwire.com/wp-content/uploads/2019/10/wizards2-630x473.jpg 630w, https://cdn.geekwire.com/wp-content/uploads/2019/10/wizards2-768x576.jpg 768w, https://cdn.geekwire.com/wp-content/uploads/2019/10/wizards2-1260x945.jpg 1260w" sizes="(max-width: 630px) 100vw, 630px"><figcaption>Wizards of the Coast was founded in Renton, Wash., in 1990. (GeekWire File Photo / Kurt Schlosser) </figcaption></figure>



<p>A new round of layoffs at Hasbro has impacted its subsidiary Wizards of the Coast, despite Wizards’ strong recent performance and its status as a linchpin for Hasbro’s overall revenue stream. The result could spell trouble for both companies as they head into 2024.</p>



<p>Hasbro announced Monday that it would <a href="https://www.geekwire.com/2023/new-round-of-layoffs-at-hasbro-impacts-wizards-of-the-coast-read-ceos-memo-to-staff/">eliminate an additional 1,100 workers</a> throughout its global operations, as part of what CEO Chris Cocks called a “strategic transformation.” This was on top of previously announced cuts in January. </p>



<p>It was initially unclear as to whether those layoffs would hit Renton, Wash.-based Wizards of the Coast, which seemed like it might’ve been protected given its value to Hasbro’s portfolio.</p>



<p>As has been the case <a href="https://www.geekwire.com/2022/hasbro-shareholder-launches-campaign-to-spin-off-wizards-of-the-coast-business-a-hidden-gem/">for the last couple of years</a>, Hasbro’s gaming endeavors, led by Wizards of the Coast, have been the only consistently profitable part of the company. In its <a href="https://investor.hasbro.com/news-releases/news-release-details/hasbro-reports-third-quarter-2023-financial-results">most recent earnings report</a>, Hasbro wrote that its toy and entertainment segments are both losing money, while its Gaming segment’s revenue grew by 40% in Q3 2023.</p>



<p>Despite that success, reports this week indicate that at least 20 employees of Wizards of the Coast and its own subsidiaries had been laid off.</p>



<p>Those affected, as per a list assembled <a href="https://twitter.com/CHofferCBus/status/1734947730491932929">by ComicBook.com’s Christian Hoffer</a>, include Mike Mearls, <em>Magic: The Gathering </em>director and former <em>Dungeons &amp; Dragons </em>creative director; Amy Dallen, <em>D&amp;D Beyond </em>host and producer; Eytan Bernstein, <em>D&amp;D </em>senior development editor; Larry Frum, senior communications manager; and Bree Heiss, <em>D&amp;D </em>art director. Some employees also opted, according to an internal memo from Cocks, to voluntarily accept early retirement.</p>



<p>At time of writing, it’s unclear why Hasbro’s chosen to lay off employees at the single strongest company in its portfolio. This year, Wizards debuted a critically if not commercially successful major motion picture, earned a Game of the Year trophy at the 2023 Game Awards, and was consistently profitable, but Hasbro’s <em>still</em> sacking its employees. It’s the sort of math that only makes sense if you’ve got shareholders to placate.</p>



<figure><a href="https://cdn.geekwire.com/wp-content/uploads/2023/12/Hasbro-Q3-2023-Report.jpg"><img decoding="async" width="630" height="462" src="https://cdn.geekwire.com/wp-content/uploads/2023/12/Hasbro-Q3-2023-Report-630x462.jpg" alt="" srcset="https://cdn.geekwire.com/wp-content/uploads/2023/12/Hasbro-Q3-2023-Report-630x462.jpg 630w, https://cdn.geekwire.com/wp-content/uploads/2023/12/Hasbro-Q3-2023-Report-768x564.jpg 768w, https://cdn.geekwire.com/wp-content/uploads/2023/12/Hasbro-Q3-2023-Report.jpg 913w" sizes="(max-width: 630px) 100vw, 630px"></a><figcaption>Hasbro’s official Q3 report, released Oct. 26, shows rising revenues for Wizards of the Coast at a point in time when its other departments are slowing down. (Hasbro filing)</figcaption></figure>



<p>The dismissals at Wizards <a href="https://www.geekwire.com/2023/the-state-of-video-games-big-releases-bigger-layoffs-and-an-imminent-crisis-point/">play into a disturbing trend</a> across game development and related fields throughout 2023. Over 9,000 developers worldwide have been dismissed over the course of the year, as per independent tracker <a href="http://videogamelayoffs.com/">VideoGameLayoffs</a>. Since there are a couple of studio closures that aren’t yet on VGL’s list, that number could easily break the 10,000 mark by New Year’s Eve.</p>



<p>The traditional impact of these sorts of layoffs on video game studios has been to tank morale and destabilize the  business. If <em>overwhelming success</em> can’t protect your company from cuts, then the smart response from workers is to take your talents elsewhere. We’ve seen that happen in the Pacific Northwest with companies like Bungie, 343 Industries, and Amazon.</p>



<p>On the other hand, Wizards of the Coast is in a unique position in its industry. While it does handle a lot of video game development right now — it just debuted the first project from its Texas studio Archetype at this year’s Game Awards — Wizards dominates both collectible card games and tabletop RPGs. It’s got competitors, but most of them have a fraction of the audience and visibility of Wizards’ core franchises. Dissatisfied employees would seem to have nowhere to go but down.</p>



<p>As of this year, however, that may be changing, due to ongoing fallout from <a href="https://www.geekwire.com/2023/analysis-nobody-could-dethrone-dungeons-dragons-except-wizards-of-the-coast-and-it-may-have-done-just-that/">a recent controversy at Wizards</a>.</p>



<p>For those who are coming in late: a leaked report in January suggested that Wizards was looking into ways to abolish the Open Gaming License, a public copyright notice that it adopted in 2000 to allow third-party developers to make new material for <em>Dungeons &amp; Dragons</em>. After a couple of weeks of controversy, Wizards <a href="https://www.geekwire.com/2023/wizards-of-the-coast-reverses-stance-on-controversial-licensing-revision-following-outcry/">reversed course on that decision</a>, but not before it triggered a series of reactions from its competitors in the tabletop gaming space.</p>



<p>Most notably, Redmond, Wash.-based Paizo Publishing announced it was spearheading a coalition of companies that would create a new, independent licensing agreement. The result, the Open RPG Creative License (ORC), <a href="https://paizo.com/community/blog/v5748dyo6sico">was finalized in June</a> and published on the website of Seattle legal firm Azora Law.</p>



<p>Paizo is an independent shop that’s built its own tabletop business around the <em>Pathfinder </em>series, which itself began as a heavily modified version of an earlier version of <em>D&amp;D</em>. While <em>D&amp;D </em>has a massive lead in both player population and brand recognition, <em>Pathfinder</em> is <a href="https://www.xfire.com/dungeons-and-dragons-ogl-controversy-paizo-pathfinder-chaosium-cthulhu-rpg-sales/">one of several competing products that got a huge boost in visibility</a> after Wizards’ short-lived attempt to revoke the OGL. Paizo’s workforce also <a href="https://unitedpaizoworkers.org/">unionized in June</a>.</p>



<p>For the last year, Paizo has been going out of its way to make itself look like a preferable alternative to Wizards of the Coast, for both players and professionals. Its audience is growing rapidly, thanks to dissatisfied <em>D&amp;D </em>fans switching systems, and now it’s a union shop in the same state as Wizards.</p>



<p>With Hasbro’s most recent round of layoffs, there’s a real possibility we could see a brain drain from Wizards to Paizo and other independent shops in the Washington area, such as Kobold Press.</p>



<p>Wizards already has a packed schedule for <em>D&amp;D</em>’s 50th anniversary in 2024, with a big rules update and several new projects in the wings. It’s positioned to have a good year. If Hasbro continues to treat Wizards this carelessly, however, it could bring <em>D&amp;Ds (</em>and<em> Magic</em>’s) current boom period to a sudden stop.</p>
              </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Possible to detect an industrial civilization in geological record? (2018) (144 pts)]]></title>
            <link>https://www.cambridge.org/core/journals/international-journal-of-astrobiology/article/silurian-hypothesis-would-it-be-possible-to-detect-an-industrial-civilization-in-the-geological-record/77818514AA6907750B8F4339F7C70EC6</link>
            <guid>38668884</guid>
            <pubDate>Sat, 16 Dec 2023 23:40:04 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.cambridge.org/core/journals/international-journal-of-astrobiology/article/silurian-hypothesis-would-it-be-possible-to-detect-an-industrial-civilization-in-the-geological-record/77818514AA6907750B8F4339F7C70EC6">https://www.cambridge.org/core/journals/international-journal-of-astrobiology/article/silurian-hypothesis-would-it-be-possible-to-detect-an-industrial-civilization-in-the-geological-record/77818514AA6907750B8F4339F7C70EC6</a>, See on <a href="https://news.ycombinator.com/item?id=38668884">Hacker News</a></p>
<div id="readability-page-1" class="page"><div lang="en" data-v-2fa8b348="" id="sec0" data-v-0f2322a2=""><h2>Abstract</h2>  <div><p>If an industrial civilization had existed on Earth many millions of years prior to our own era, what traces would it have left and would they be detectable today? We summarize the likely geological fingerprint of the Anthropocene, and demonstrate that while clear, it will not differ greatly in many respects from other known events in the geological record. We then propose tests that could plausibly distinguish an industrial cause from an otherwise naturally occurring climate event.</p></div> </div><div id="content-container" data-v-0f2322a2=""><div><div><div data-magellan-destination="sec1" id="sec1"><h2> Introduction</h2><p> The search for life elsewhere in the universe is a central occupation of astrobiology and scientists have often looked to Earth analogues for extremophile bacteria, life under varying climate states and the genesis of life itself. A subset of this search is the prospect for intelligent life, and then a further subset is the search for civilizations that have the potential to communicate with us. A common assumption is that any such civilization must have developed industry of some sort. In particular, the ability to harness those industrial processes to develop radio technologies capable of sending or receiving messages. In what follows, however, we will define industrial civilizations here as the ability to harness external energy sources at global scales.</p><p> One of the key questions in assessing the likelihood of finding such a civilization is an understanding of how often, given that life has arisen and that some species are intelligent, does an industrial civilization develop? Humans are the only example we know of, and our industrial civilization has lasted (so far) roughly 300 years (since, for example, the beginning of mass production methods). This is a small fraction of the time we have existed as a species, and a tiny fraction of the time that complex life has existed on the Earth's land surface (~400 million years ago, Ma). This short time period raises the obvious question as to whether this could have happened before. We term this the ‘Silurian hypothesis’<a href="#fn1"><span>Footnote </span><sup>1</sup></a>.</p><p> While much idle speculation and late night chatter has been devoted to this question, we are unaware of previous serious treatments of the problem of detectability of prior terrestrial industrial civilizations in the geologic past. Given the vast increase in work surrounding exoplanets and questions related to detection of life, it is worth addressing the question more formally and in its astrobiological setting. We note also the recent work of Wright (<a href="#ref121"><span>Reference Wright</span>2017</a>) which addressed aspects of the problem and previous attempts to assess the likelihood of solar system non-terrestrial civilization such as Haqq-Misra &amp; Kopparapu (<a href="#ref42"><span>Reference Haqq-Misra and Kopparapu</span>2012</a>). This paper is an attempt to remedy the gap in a way that also puts our current impact on the planet into a broader perspective. We first note the importance of this question to the well-known Drake equation. Then we address the likely geologic consequences of human industrial civilization and then compare that fingerprint to potentially similar events in the geologic record. Finally, we address some possible research directions that might improve the constraints on this question.</p><div data-magellan-destination="sec1-1" id="sec1-1"><h3> Relevance to the Drake equation</h3><p> The Drake equation is the well-known framework for estimating of the number of active, communicative extraterrestrial civilizations in the Milky Way galaxy (Drake, <a href="#ref28"><span>Reference Drake</span>1961</a>, <a href="#ref29"><span>Reference Drake, Mamikunian and Briggs</span>1965</a>). The number of such civilizations, <em>N</em>, is assumed to be equal to the product of; the average rate of star formation, <em>R</em>*, in our Galaxy; the fraction of formed stars, <em>f</em> <sub>p</sub>, that have planets; the average number of planets per star, <em>n</em> <sub><em>e</em></sub>, that can potentially support life; the fraction of those planets, <em>f</em> <sub>l</sub>, that actually develop life; the fraction of planets bearing life on which intelligent, civilized life, <em>f</em> <sub>i</sub>, has developed; the fraction of these civilizations that have developed communications, <em>f</em> <sub>c</sub>, i.e., technologies that release detectable signs into space, and the length of time, <em>L</em>, over which such civilizations release detectable signals.
</p><p><span><img src="data:image/gif;base64,R0lGODlhAQABAIAAAMLCwgAAACH5BAAAAAAALAAAAAABAAEAAAICRAEAOw==" data-src="https://static.cambridge.org/binary/version/id/urn:cambridge.org:id:binary:20231110114854105-0257:S1473550418000095:S1473550418000095_eqnU1.gif?pub-status=live" width="244" height="21" data-original-image="/binary/version/id/urn:cambridge.org:id:binary:20231110114854105-0257:S1473550418000095:S1473550418000095_eqnU1.gif" data-zoomable="false"><span data-mathjax-type="texmath"><span>$$N=R^{\ast }\cdot f_{\rm p}\cdot n_{e}\cdot f_{\rm l }\cdot f_{\rm i}\cdot f_{\rm c}\cdot L{.}$$</span></span></span></p><p> If over the course of a planet's existence, multiple industrial civilizations can arise over the span of time that life exists at all, the value of <em>f</em> <sub>c</sub> may in fact be &gt;1.</p><p> This is a particularly cogent issue in light of recent developments in astrobiology in which the first three terms, which all involve purely astronomical observations, have now been fully determined. It is now apparent that most stars harbour families of planets (Seager, <a href="#ref97"><span>Reference Seager</span>2013</a>). Indeed, many of those planets will be in the star's habitable zones (Dressing &amp; Charbonneau, <a href="#ref30"><span>Reference Dressing and Charbonneau</span>2013</a>; Howard, <a href="#ref47"><span>Reference Howard</span>2013</a>). These results allow the next three terms to be bracketed in a way that uses the exoplanet data to establish a constraint on exo-civilization pessimism. In Frank &amp; Sullivan (<a href="#ref34"><span>Reference Frank and Sullivan</span>2016</a>) such a ‘pessimism line’ was defined as the maximum ‘biotechnological’ probability (per habitable zone planets) <em>f</em> <sub>bt</sub> for humans to be the only time a technological civilization has evolved in cosmic history. Frank &amp; Sullivan (<a href="#ref34"><span>Reference Frank and Sullivan</span>2016</a>) found <em>f</em> <sub>bt</sub> in the range ~10<sup>−24</sup>–10<sup>−22</sup>.</p><p> Determination of the ‘pessimism line’ emphasizes the importance of three Drake equation terms <em>f</em> <sub>l</sub>, <em>f</em> <sub>i</sub> and <em>f</em> <sub>c</sub>. Earth's history often serves as a template for discussions of possible values for these probabilities. For example, there has been considerable discussion of how many times life began on Earth during the early Archean given the ease of abiogenisis (Patel et al., <a href="#ref83"><span>Reference Patel</span>2015</a>) including the possibility of a ‘shadow biosphere’ composed of descendants of a different origin event from the one which led to our Last Universal Common Ancestor (LUCA) (Cleland &amp; Copley, <a href="#ref19"><span>Reference Cleland and Copley</span>2006</a>). In addition, there is a long-standing debate concerning the number of times intelligence has evolved in terms of dolphins and other species (Marino, <a href="#ref69"><span>Reference Marino, Vakoch and Dowd</span>2015</a>). Thus, only the term <em>f</em> <sub>c</sub> has been commonly accepted to have a value on Earth of strictly 1.</p></div><div data-magellan-destination="sec1-2" id="sec1-2"><h3> Relevance to other solar system planets</h3><p> Consideration of previous civilizations on other solar system worlds has been taken on by Wright (<a href="#ref121"><span>Reference Wright</span>2017</a>) and Haqq-Misra &amp; Kopparapu (<a href="#ref42"><span>Reference Haqq-Misra and Kopparapu</span>2012</a>). We note here that abundant evidence exists of surface water in ancient Martian climates (3.8&nbsp;Ga) (e.g. Achille &amp; Hynek, <a href="#ref1"><span>Reference Achille and Hynek</span>2010</a>; Arvidson et al., <a href="#ref5"><span>Reference Arvidson</span>2014</a>), and speculation that early Venus (2&nbsp;Ga to 0.7&nbsp;Ga) was habitable (due to a dimmer sun and lower CO<sub>2</sub> atmosphere) has been supported by recent modelling studies (Way et al., <a href="#ref118"><span>Reference Way</span>2016</a>). Conceivably, deep drilling operations could be carried out on either planet in future to assess their geological history. This would constrain consideration of what the fingerprint might be of life, and even organized civilization (Haqq-Misra &amp; Kopparapu, <a href="#ref42"><span>Reference Haqq-Misra and Kopparapu</span>2012</a>). Assessments of prior Earth events and consideration of Anthropocene markers such as those we carry out below will likely provide a key context for those explorations.</p></div><div data-magellan-destination="sec1-3" id="sec1-3"><h3> Limitations of the geological record</h3><p> That this paper's title question is worth posing is a function of the incompleteness of the geological record. For the Quaternary (the last 2.5 million years), there is widespread extant physical evidence of, for instance, climate changes, soil horizons and archaeological evidence of non-<em>Homo Sapiens</em> cultures (Denisovians, Neanderthals, etc.) with occasional evidence of bipedal hominids dating back to at least 3.7&nbsp;Ma (e.g. the Laetoli footprints) (Leakey &amp; Hay, <a href="#ref63"><span>Reference Leakey and Hay</span>1979</a>). The oldest extant large-scale surface is in the Negev Desert and is&nbsp;~1.8&nbsp;Ma old (Matmon et al., <a href="#ref70"><span>Reference Matmon</span>2009</a>). However, pre-Quaternary land-evidence is far sparser, existing mainly in exposed sections, drilling and mining operations. In the ocean sediments, due to the recycling of ocean crust, there only exists sediment evidence for periods that post-date the Jurassic (~170&nbsp;Ma) (ODP Leg 801 Team, <a href="#ref79">2000</a>).</p><p> The fraction of life that gets fossilized is always extremely small and varies widely as a function of time, habitat and degree of soft tissue versus hard shells or bones (Behrensmeyer et al., <a href="#ref7"><span>Reference Behrensmeyer, Kidwell and Gastaldo</span>2000</a>). Fossilization rates are very low in tropical, forested environments but are higher in arid environments and fluvial systems. As an example, for all the dinosaurs that ever lived, there are only a few thousand near-complete specimens, or equivalently only a handful of individual animals across thousands of taxa per 100,000 years. Given the rate of new discovery of taxa of this age, it is clear that species as short-lived as <em>Homo sapiens</em> (so far) might not be represented in the existing fossil record at all.</p><p> The likelihood of objects surviving and being discovered is similarly unlikely. Zalasiewicz (<a href="#ref123"><span>Reference Zalasiewicz</span>2009</a>) speculates about preservation of objects or their forms, but the current area of urbanization is &lt;1% of the Earth's surface (Schneider et al., <a href="#ref96"><span>Reference Schneider, Friedl and Potere</span>2009</a>), and exposed sections and drilling sites for pre-Quaternary surfaces are orders of magnitude less as fractions of the original surface. Note that even for early human technology, complex objects are very rarely found. For instance, the Antikythera Mechanism (ca. 205 BCE) is a unique object until the Renaissance. Despite impressive recent gains in the ability to detect the wider impacts of civilization on landscapes and ecosystems (Kidwell, <a href="#ref53"><span>Reference Kidwell</span>2015</a>), we conclude that for potential civilizations older than about 4&nbsp;Ma, the chances of finding direct evidence of their existence via objects or fossilized examples of their population is small. We note, however, that one might ask the indirect question related to antecedents in the fossil record indicating species that might <em>lead</em> downstream to the evolution of later civilization-building species. Such arguments, for or against, the Silurian hypothesis would rest on evidence concerning highly social behaviour or high intelligence based on brain size. The claim would then be that there are other species in the fossil record which could, or could not, have evolved into civilization-builders. In this paper, however, we focus on physicochemical tracers for previous industrial civilizations. In this way, there is an opportunity to widen the search to tracers that are more widespread, even though they may be subject to more varied interpretations.</p></div><div data-magellan-destination="sec1-4" id="sec1-4"><h3> Scope of this paper</h3><p> We will restrict the scope of this paper to geochemical constraints on the existence of pre-Quaternary industrial civilizations, that may have existed since the rise of complex life on land. This rules out societies that might have been highly organized and potentially sophisticated but that did not develop industry and probably any purely ocean-based lifeforms. The focus is thus on the period between the emergence of complex life on land in the Devonian (~400&nbsp;Ma) in the Paleozoic era and the mid-Pliocene (~4&nbsp;Ma).</p></div></div><div data-magellan-destination="sec2" id="sec2"><h2> The geological footprint of the Anthropocene</h2><p> While an official declaration of the Anthropocene as a unique geological era is still pending (Crutzen, <a href="#ref24"><span>Reference Crutzen</span>2002</a>; Zalasiewicz et al., <a href="#ref125"><span>Reference Zalasiewicz</span>2017</a>), it is already clear that our human efforts will impact the geologic record being laid down today (Waters et al., <a href="#ref117"><span>Reference Waters</span>2014</a>). Some of the discussion of the specific boundary that will define this new period is not relevant for our purposes because the markers proposed (ice core gas concentrations, short-half-lived radioactivity, the Columbian exchange) (e.g. Lewis &amp; Maslin, <a href="#ref64"><span>Reference Lewis and Maslin</span>2015</a>; Hamilton, <a href="#ref40"><span>Reference Hamilton</span>2016</a>) are not going to be geologically stable or distinguishable on multi-million year timescales. However, there are multiple changes that have already occurred that will persist. We discuss a number of these below.</p><p> There is an interesting paradox in considering the Anthropogenic footprint on a geological timescale. The longer human civilization lasts, the larger the signal one would expect in the record. However, the longer a civilization lasts, the more sustainable its practices would need to have become in order to survive. The more sustainable a society (e.g. in energy generation, manufacturing or agriculture) the smaller the footprint on the rest of the planet. But the smaller the footprint, the less of a signal will be embedded in the geological record. Thus, the footprint of civilization might be self-limiting on a relatively short timescale. To avoid speculating about the ultimate fate of humanity, we will consider impacts that are already clear, or that are foreseeable under plausible trajectories for the next century (e.g. Nazarenko et al., <a href="#ref78"><span>Reference Nazarenko</span>2015</a>; Köhler, <a href="#ref56"><span>Reference Köhler</span>2016</a>).</p><p> We note that effective sedimentation rates in ocean sediment for cores with multi-million-year-old sediment are of the order of a few cm/1000 years at best, and while the degree of bioturbation may smear a short-period signal, the Anthropocene will likely only appear as a section a few cm thick, and appear almost instantaneously in the record.</p><div data-magellan-destination="sec2-1" id="sec2-1"><h3> Stable isotope anomalies of carbon, oxygen, hydrogen and nitrogen</h3><p> Since the mid-18th century, humans have released over 0.5 trillion tonnes of fossil carbon via the burning of coal, oil and natural gas (Le Quéré et al., <a href="#ref62"><span>Reference Le Quéré</span>2016</a>), at a rate orders of magnitude faster than natural long-term sources or sinks. In addition, there has been widespread deforestation and addition of carbon dioxide into the air via biomass burning. All of this carbon is biological in origin and is thus depleted in<sup>13</sup>C compared with the much larger pool of inorganic carbon (Revelle &amp; Suess, <a href="#ref90"><span>Reference Revelle and Suess</span>1957</a>). Thus, the ratio of <sup>13</sup>C to <sup>12</sup>C in the atmosphere, ocean and soils is decreasing (an impact known as the ‘Suess Effect’ Quay et al., <a href="#ref88"><span>Reference Quay, Tilbrook and Wong</span>1992</a>) with a current change of around −&nbsp;1‰ <em>δ</em> <sup>13</sup>C since the pre-industrial (Böhm et al., <a href="#ref11"><span>Reference Böhm</span>2002</a>; Eide et al., <a href="#ref31"><span>Reference Eide</span>2017</a>) in the surface ocean and atmosphere (<a href="#fig01">Fig. 1(a)</a>).
</p><div data-magellan-destination="fig01" id="fig01"><p><img data-src="https://static.cambridge.org/binary/version/id/urn:cambridge.org:id:binary:20231110114854105-0257:S1473550418000095:S1473550418000095_fig1g.jpeg?pub-status=live" width="1400" height="1368" data-original-image="/binary/version/id/urn:cambridge.org:id:binary:20231110114854105-0257:S1473550418000095:S1473550418000095_fig1g.jpeg" data-zoomable="true" src="https://www.cambridge.org/binary/version/id/urn:cambridge.org:id:binary:20231110114854105-0257:S1473550418000095:S1473550418000095_fig1g.jpeg"></p><div><p><span>Fig. 1.</span> Illustrative stable carbon isotopes and temperature (or proxy) profiles across three periods. (a) The modern era (from 1600 CE with projections to 2100). Carbon isotopes are from sea sponges (Böhm et al., <a href="#ref11"><span>Reference Böhm</span>2002</a>), and projections from Köhler (<a href="#ref56"><span>Reference Köhler</span>2016</a>). Temperatures are from Mann et al. (<a href="#ref68"><span>Reference Mann</span>2008</a>) (reconstructions), GISTEMP (Hansen et al., <a href="#ref41"><span>Reference Hansen</span>2010</a>) (instrumental) and projected to 2100 using results from Nazarenko et al. (<a href="#ref78"><span>Reference Nazarenko</span>2015</a>). Projections assume trajectories of emissions associated with RCP8.5 (van Vuuren et al., <a href="#ref113"><span>Reference van Vuuren</span>2011</a>). (b) The Paleocene–Eocene Thermal Maximum (55.5&nbsp;Ma). Data from two DSDP cores (589 and 1209B) (Tripati &amp; Elderfield, <a href="#ref109"><span>Reference Tripati and Elderfield</span>2004</a>) are used to estimate anomalous isotopic changes and a loess smooth with a span of 200&nbsp;kya is applied to make the trends clearer. Temperatures changes are estimated from observed <em>δ</em> <sup>18</sup>O<sub>carbonate</sub> using a standard calibration (Kim &amp; O'Neil, <a href="#ref54"><span>Reference Kim and O'Neil</span>1997</a>). (c) Oceanic Anoxic Event 1a (about 120&nbsp;Ma). Carbon isotopes are from the La Bédoule and Cau cores from the paleo-Tethys (Kuhnt et al., <a href="#ref59"><span>Reference Kuhnt, Holbourn and Moullade</span>2011</a>; Naafs et al., <a href="#ref77"><span>Reference Naafs</span>2016</a>) aligned as in Naafs et al. (<a href="#ref77"><span>Reference Naafs</span>2016</a>) and placed on an approximate age model. Data from Alstätte (Bottini &amp; Mutterlose, <a href="#ref14"><span>Reference Bottini and Mutterlose</span>2012</a>) and DSDP Site 398 (Li et al., <a href="#ref65"><span>Reference Li</span>2008</a>) are aligned based on coherence of the <em>δ</em> <sup>13</sup>C anomalies. Temperature change estimates are derived from TEX86 (Mutterlose et al., <a href="#ref76"><span>Reference Mutterlose</span>2014</a>; Naafs et al., <a href="#ref77"><span>Reference Naafs</span>2016</a>). Note that the y-axis spans the same range in all three cases, while the timescales vary significantly.</p></div></div><p> As a function of the increase of fossil carbon into the system, augmented by black carbon changes, other non-CO<sub>2</sub> trace greenhouse gases (e.g. N<sub>2</sub>O, CH<sub>4</sub> and chloro-fluoro-carbons (CFCs)), global industrialization has been accompanied by a warming of about 1°C so far since the mid-19th century (Bindoff et al., <a href="#ref10"><span>Reference Bindoff, Stocker, Qin, Plattner, Tignor, Allen, Boschung, Nauels, Xia, Bex and Midgley</span>2013</a>; GISTEMP Team, <a href="#ref38">2016</a>). Due to the temperature-related fractionation in the formation of carbonates (Kim &amp; O'Neil, <a href="#ref54"><span>Reference Kim and O'Neil</span>1997</a>) (<span data-mathjax-status="alt-graphic"><span><img src="data:image/gif;base64,R0lGODlhAQABAIAAAMLCwgAAACH5BAAAAAAALAAAAAABAAEAAAICRAEAOw==" data-src="https://static.cambridge.org/binary/version/id/urn:cambridge.org:id:binary:20231110114854105-0257:S1473550418000095:S1473550418000095_inline1.gif?pub-status=live" width="91" height="19" data-original-image="/binary/version/id/urn:cambridge.org:id:binary:20231110114854105-0257:S1473550418000095:S1473550418000095_inline1.gif" data-zoomable="false"><span data-mathjax-type="texmath"><span>$-0.2\permil \ \delta ^{18}$</span></span></span></span>O per °C) and strong correlation in the extra-tropics between temperature and <em>δ</em> <sup>18</sup>O (between 0.4 and 0.7‰ per °C) (and&nbsp;~8× as sensitive for deuterium isotopes relative to hydrogen (<em>δ</em>D)), we expect this temperature rise to be detectable in surface ocean carbonates (notably foraminifera), organic biomarkers, cave records (stalactites), lake ostracods and high-latitude ice cores, though only the first two of these will be retrievable in the timescales considered here.</p><p> The combustion of fossil fuel, the invention of the Haber–Bosch process, the large-scale application of nitrogenous fertilizers and the enhanced nitrogen fixation associated with cultivated plants, have caused a profound impact on nitrogen cycling (Canfield et al., <a href="#ref18"><span>Reference Canfield, Glazer and Falkowski</span>2010</a>), such that <em>δ</em> <sup>15</sup>N anomalies are already detectable in sediments remote from civilization (Holtgrieve et al., <a href="#ref46"><span>Reference Holtgrieve</span>2011</a>).</p></div><div data-magellan-destination="sec2-2" id="sec2-2"><h3> Sedimentological records</h3><p> There are multiple causes of a greatly increased sediment flow in rivers and therefore in deposition in coastal environments. The advent of agriculture and associated deforestation have lead to large increases in soil erosion (Goudie, <a href="#ref39"><span>Reference Goudie</span>2000</a>; National Research Council, <a href="#ref20">2010</a>). Furthermore, canalization of rivers (such as the Mississippi) have led to much greater oceanic deposition of sediment than would otherwise have occurred. This tendency is mitigated somewhat by concurrent increases in river dams which reduce sediment flow downstream. Additionally, increasing temperatures and atmospheric water vapour content have led to greater intensity of precipitation (Kunkel et al., <a href="#ref60"><span>Reference Kunkel</span>2013</a>) which, on its own, would also lead to greater erosion, at least regionally. Coastal erosion is also on the increase as a function of the rising sea level, and in polar regions is being enhanced by reductions in sea ice and thawing permafrost (Overeem et al., <a href="#ref81"><span>Reference Overeem</span>2011</a>).</p><p> In addition to changes in the flux of sediment from land to ocean, the composition of the sediment will also change. Due to the increased dissolution of CO<sub>2</sub> in the ocean as a function of anthropogenic CO<sub>2</sub> emissions, the upper ocean is acidifying (a 26% increase in H<sup>+</sup> or 0.1&nbsp;pH decrease since the 19th century) (Orr et al., <a href="#ref80"><span>Reference Orr</span>2005</a>). This will lead to an increase in CaCO<sub>3</sub> dissolution within the sediment that will last until the ocean can neutralize the increase. There will also be important changes in mineralogy (Zalasiewicz et al., <a href="#ref124"><span>Reference Zalasiewicz, Kryza and Williams</span>2013</a>; Hazen et al., <a href="#ref43"><span>Reference Hazen</span>2017</a>). Increases in continental weathering are also likely to change ratios of strontium and osmium (e.g.<sup>87</sup>Sr/<sup>86</sup>Sr and <sup>187</sup>Os/<sup>188</sup>Os ratios) (Jenkyns, <a href="#ref49"><span>Reference Jenkyns</span>2010</a>).</p><p> As discussed above, nitrogen load in rivers is increasing as a function of agricultural practices. This in turn is leading to more microbial activity in the coastal ocean which can deplete dissolved oxygen in the water column (Diaz &amp; Rosenberg, <a href="#ref26"><span>Reference Diaz and Rosenberg</span>2008</a>), and recent syntheses suggests a global decline already of about 2% (Ito et al., <a href="#ref48"><span>Reference Ito</span>2017</a>; Schmidtko et al., <a href="#ref94"><span>Reference Schmidtko, Stramma and Visbeck</span>2017</a>). This in turn is leading to an expansion of the oxygen minimum zones, greater ocean anoxia and the creation of so-called ‘dead-zones’ (Breitburg et al., <a href="#ref15"><span>Reference Breitburg</span>2018</a>). Sediment within these areas will thus have greater organic content and less bioturbation (Tyrrell, <a href="#ref112"><span>Reference Tyrrell</span>2011</a>). The ultimate extent of these dead zones is unknown.</p><p> Furthermore, anthropogenic fluxes of lead, chromium, antimony, rhenium, platinum group metals, rare earths and gold, are now much larger than their natural sources (Sen &amp; Peucker-Ehrenbrink, <a href="#ref98"><span>Reference Sen and Peucker-Ehrenbrink</span>2012</a>; Gałuszka et al., <a href="#ref36"><span>Reference Gałuszka, Migaszewski and Zalasiewicz</span>2013</a>), implying that there will be a spike in fluxes in these metals in river outflow and hence higher concentrations in coastal sediments.</p></div><div data-magellan-destination="sec2-3" id="sec2-3"><h3> Faunal radiation and extinctions</h3><p> The last few centuries have seen significant changes in the abundance and spread of small animals, particularly rats, mice and cats, etc. that are associated with human exploration and biotic exchanges. Isolated populations almost everywhere have now been superseded in many respects by these invasive species. The fossil record will likely indicate a large faunal radiation of these indicator species at this point. Concurrently, many other species have already, or are likely to become, extinct, and their disappearance from the fossil record will be noticeable. Given the perspective from many million years ahead, large mammal extinctions that occurred at the end of the last ice age will also associated with the onset of the Anthropocene.</p></div><div data-magellan-destination="sec2-4" id="sec2-4"><h3> Non-naturally occurring synthetics</h3><p> There are many chemicals that have been (or were) manufactured industrially that for various reasons can spread and persist in the environment for a long time (Bernhardt et al., <a href="#ref8"><span>Reference Bernhardt, Rosi and Gessner</span>2017</a>). Most notably, persistent organic pollutants (organic molecules that are resistant to degradation by chemical, photo-chemical or biological processes), are known to have spread across the world (even to otherwise pristine environments) (Beyer et al., <a href="#ref9"><span>Reference Beyer</span>2000</a>). Their persistence is often tied to being halogenated organics since the bond strength of C–Cl (for instance) is much stronger than C–C. For instance, polychlorinated biphenyls are known to have lifetimes of many hundreds of years in river sediment (Bopp, <a href="#ref12"><span>Reference Bopp</span>1979</a>). How long a detectable signal would persist in ocean sediment is, however, unclear.</p><p> Other chlorinated compounds may also have the potential for long-term preservation, specifically CFCs and related compounds. While there are natural sources for the most stable compound (CF<sub>4</sub>), there are only anthropogenic sources for C<sub>2</sub>F<sub>6</sub> and SF<sub>6</sub>, the next most stable compounds. In the atmosphere, their sink via photolytic destruction in the stratosphere limits their lifetimes to a few thousand years (Ravishankara et al., <a href="#ref89"><span>Reference Ravishankara</span>1993</a>). The compounds do dissolve in the the ocean and can be used as tracers of ocean circulation, but we are unaware of studies indicating how long these chemicals might survive and/or be detectable in ocean sediment given some limited evidence for microbial degradation in anaerobic environments (Denovan &amp; Strand, <a href="#ref25"><span>Reference Denovan and Strand</span>1992</a>).</p><p> Other classes of synthetic biomarkers may also persist in sediments. For instance, steroids, leaf waxes, alkenones and lipids can be preserved in sediment for many millions of years (i.e. Pagani et al., <a href="#ref82"><span>Reference Pagani</span>2006</a>). What might distinguish naturally occurring biomarkers from synthetics might be the chirality of the molecules. Most total synthesis pathways do not discriminate between D- and L-chirality, while biological processes are almost exclusively monochiral (Meierhenrich, <a href="#ref73"><span>Reference Meierhenrich</span>2008</a>) (for instance, naturally occurring amino acids are all L-forms, and almost all sugars are D-forms). Synthetic steroids that do not have natural counterparts are also now ubiquitous in water bodies.</p></div><div data-magellan-destination="sec2-5" id="sec2-5"><h3> Plastics</h3><p> Since 1950, there has been a huge increase in plastics being delivered into the ocean (Moore, <a href="#ref75"><span>Reference Moore</span>2008</a>; Eriksen et al., <a href="#ref33"><span>Reference Eriksen</span>2014</a>). Although many common forms of plastic (such as polyethylene and polypropylene) are buoyant in sea water, and even those that are nominally heavier than water may be incorporated into flotsam that remains at the surface, it is already clear that mechanical erosional processes will lead to the production of large amounts of plastic micro- and nano-particles (Cozar et al., <a href="#ref22"><span>Reference Cozar</span>2014</a>; Andrady, <a href="#ref4"><span>Reference Andrady</span>2015</a>). Surveys have shown increasing amounts of plastic ‘marine litter’ on the seafloor from coastal areas to deep basins and the Arctic (Pham et al., <a href="#ref87"><span>Reference Pham</span>2014</a>; Tekman et al., <a href="#ref108"><span>Reference Tekman, Krumpen and Bergmann</span>2017</a>). On beaches, novel aggregates ‘plastiglomerates’ have been found where plastic-containing debris comes into contact with high temperatures (Corcoran et al., <a href="#ref21"><span>Reference Corcoran, Moore and Jazvac</span>2014</a>).</p><p> The degradation of plastics is mostly by solar ultraviolet radiation and in the oceans occurs mostly in the photic zone (Andrady, <a href="#ref4"><span>Reference Andrady</span>2015</a>) and is notably temperature dependent (Andrady et al., <a href="#ref3"><span>Reference Andrady</span>1998</a>) (other mechanisms such as thermo-oxidation or hydrolysis do not readily occur in the ocean). The densification of small plastic particles by fouling organisms, ingestion and incorporation into organic ‘rains’ that sink to the sea floor is an effective delivery mechanism to the seafloor, leading to increasing accumulation in ocean sediment where degradation rates are much slower (Andrady, <a href="#ref4"><span>Reference Andrady</span>2015</a>). Once in the sediment, microbial activity is a possible degradation pathway (Shah et al., <a href="#ref99"><span>Reference Shah</span>2008</a>) but rates are sensitive to oxygen availability and suitable microbial communities.</p><p> As above, the ultimate long-term fate of these plastics in sediment is unclear, but the potential for very long-term persistence and detectability is high.</p></div><div data-magellan-destination="sec2-6" id="sec2-6"><h3> Transuranic elements</h3><p> Many radioactive isotopes that are related to anthropogenic fission or nuclear arms, have half-lives that are long, but not long enough to be relevant here. However, there are two isotopes that are potentially long-lived enough. Specifically, Plutonium-244 (half-life 80.8 million years) and Curium-247 (half-life 15 million years) would be detectable for a large fraction of the relevant time period if they were deposited in sufficient quantities, say, as a result of a nuclear weapon exchange. There are no known natural sources of <sup>244</sup>Pu outside of supernovae.</p><p> Attempts have been made to detect primordial <sup>244</sup>Pu on Earth with mixed success (Hoffman et al., <a href="#ref44"><span>Reference Hoffman</span>1971</a>; Lachner et al., <a href="#ref61"><span>Reference Lachner</span>2012</a>), indicating the rate of actinide meteorite accretion is small enough (Wallner et al., <a href="#ref116"><span>Reference Wallner</span>2015</a>) for this to be a valid marker in the event of a sufficiently large nuclear exchange. Similarly, <sup>247</sup>Cm is present in nuclear fuel waste and as a consequence of a nuclear explosion.</p><p> Anomalous isotopic ratios in elements with long-lived radioactive isotopes are also possible signatures, for instance, lower than usual <sup>235</sup>U ratios, and the presence of expected daughter products, in uranium ores in the Franceville Basin in the Gabon have been traced to naturally occurring nuclear fission in oxygenated, hydrated rocks&nbsp;~2&nbsp;Ga (Gauthier-Lafaye et al., <a href="#ref37"><span>Reference Gauthier-Lafaye, Holliger and Blanc</span>1996</a>).</p></div><div data-magellan-destination="sec2-7" id="sec2-7"><h3> Summary</h3><p> The Anthropocene layer in ocean sediment will be abrupt and multi-variate, consisting of seemingly concurrent-specific peaks in multiple geochemical proxies, biomarkers, elemental composition and mineralogy. It will likely demarcate a clear transition of faunal taxa prior to the event compared with afterwards. Most of the individual markers will not be unique in the context of Earth history as we demonstrate below, but the combination of tracers may be. However, we speculate that some specific tracers that would be unique, specifically persistent synthetic molecules, plastics and (potentially) very long-lived radioactive fallout in the event of nuclear catastrophe. Absent those markers, the uniqueness of the event may well be seen in the multitude of relatively independent fingerprints as opposed to a coherent set of changes associated with a single geophysical cause.</p></div></div><div data-magellan-destination="sec3" id="sec3"><h2> Abrupt paleozoic, mesozoic and cenozoic events</h2><p> The summary for the Anthropocene fingerprint above suggests that similarities might be found in (geologically) abrupt events with a multi-variate signature. In this section, we review a partial selection of known events in the paleo-record that have some similarities to the hypothesized eventual anthropogenic signature. The clearest class of event with such similarities are the hyperthermals, most notably the Paleocene–Eocene Thermal Maximum (56&nbsp;Ma) (McInerney &amp; Wing, <a href="#ref72"><span>Reference McInerney and Wing</span>2011</a>), but this also includes smaller hyperthermal events, ocean anoxic events in the Cretaceous and Jurassic and significant (if less well characterized) events of the Paleozoic. We do not consider of events (such as the K–T extinction event or the Eocene–Oligocene boundary) where there are very clear and distinct causes (asteroid impact combined with massive volcanism(Vellekoop et al., <a href="#ref114"><span>Reference Vellekoop</span>2014</a>), and the onset of Antarctic glaciation(Zachos et al., <a href="#ref122"><span>Reference Zachos</span>2001</a>) (likely linked to the opening of Drake Passage Cristini et al., <a href="#ref23"><span>Reference Cristini</span>2012</a>, respectively). There may be more such events in the record but that are not included here simply because they may not have been examined in detail, particularly in the pre-Cenozoic.</p><div data-magellan-destination="sec3-1" id="sec3-1"><h3> The Paleocene–Eocene thermal maximum (PETM)</h3><p> The existence of an abrupt spike in carbon and oxygen isotopes near the Paleocene/Eocene transition (56&nbsp;Ma) was first noted by Kennett &amp; Stott (<a href="#ref51"><span>Reference Kennett and Stott</span>1991</a>) and demonstrated to be global by Koch et al. (<a href="#ref55"><span>Reference Koch, Zachos and Gingerich</span>1992</a>). Since then, more detailed and high-resolution analyses on land and in the ocean have revealed a fascinating sequence of events lasting 100–200 kyr and involving a rapid input (in perhaps &lt;5 kyr Kirtland Turner et al., <a href="#ref111"><span>Reference Kirtland Turner</span>2017</a>) of exogenous carbon into the system (see review by McInerney &amp; Wing, <a href="#ref72"><span>Reference McInerney and Wing</span>2011</a>), possibly related to the intrusion of the North American Igneous Province into organic sediments (Storey et al., <a href="#ref105"><span>Reference Storey, Duncan and Swisher</span>2007</a>). Temperatures rose 5–7°C (derived from multiple proxies Tripati &amp; Elderfield, <a href="#ref109"><span>Reference Tripati and Elderfield</span>2004</a>), and there was a negative spike in carbon isotopes (&gt;3‰), and decreased ocean carbonate preservation in the upper ocean. There was an increase in kaolinite (clay) in many sediments (Schmitz et al., <a href="#ref95"><span>Reference Schmitz, Pujalte and Nú nez-Betelu</span>2001</a>), indicating greater erosion, though evidence for a global increase is mixed. During the PETM 30–50% of benthic foraminiferal taxa became extinct, and it marked the time of an important mammalian (Aubry et al., <a href="#ref6"><span>Reference Aubry, Lucas and Berggren</span>1998</a>) and lizard (Smith, <a href="#ref103"><span>Reference Smith</span>2009</a>) expansion across North America. Additionally, many metal abundances (including V, Zn, Mo, Cr) spiked during the event (Soliman et al., <a href="#ref104"><span>Reference Soliman</span>2011</a>).</p></div><div data-magellan-destination="sec3-2" id="sec3-2"><h3> Eocene events</h3><p> In the 6 million years following the PETM, there are a number of smaller, though qualitatively similar, hyperthermal events seen in the record (Slotnick et al., <a href="#ref101"><span>Reference Slotnick</span>2012</a>). Notably, the Eocene Thermal Maximum 2 event (ETM-2), and at least four other peaks are characterized by significant negative carbon isotope excursions, warming and relatively high sedimentation rates driven by increases in terrigenous input (D'Onofrio et al., <a href="#ref27"><span>Reference D'Onofrio</span>2016</a>). Arctic conditions during ETM-2 show evidence of warming, lower salinity and greater anoxia (Sluijs et al., <a href="#ref102"><span>Reference Sluijs</span>2009</a>). Collectively these events have been denoted Eocene Layers of Mysterious Origin (ELMOs)<a href="#fn2"><span>Footnote </span><sup>2</sup></a>.</p><p> Around 40&nbsp;Ma, another abrupt warming event occurs (the mid-Eocene Climate Optimum (MECO)), again with an accompanying carbon isotope anomaly (Galazzo et al., <a href="#ref35"><span>Reference Galazzo</span>2014</a>).</p></div><div data-magellan-destination="sec3-3" id="sec3-3"><h3> Cretaceous and Jurassic ocean anoxic events</h3><p> First identified by Schlanger &amp; Jenkyns (<a href="#ref93"><span>Reference Schlanger and Jenkyns</span>1976</a>), ocean anoxic events (OAEs), identified by periods of greatly increased organic carbon deposition and laminated black shale deposits, are times when significant portions of the ocean (either regionally or globally) became depleted in dissolved oxygen, greatly reducing aerobic bacterial activity. There is partial (though not ubiquitous) evidence during the larger OAEs for euxinia (when the ocean column becomes filled with hydrogen sulfide (H<sub>2</sub>S)) (Meyer &amp; Kump, <a href="#ref74"><span>Reference Meyer and Kump</span>2008</a>).</p><p> There were three major OAEs in the Cretaceous, the Weissert event (132&nbsp;Ma) (Erba et al., <a href="#ref32"><span>Reference Erba, Bartolini and Larson</span>2004</a>), OAE-1a around 120&nbsp;Ma lasting about 1&nbsp;Myr and another OAE-2 around 93&nbsp;Ma lasting around 0.8&nbsp;Myr (Kerr, <a href="#ref52"><span>Reference Kerr</span>1998</a>; Li et al., <a href="#ref65"><span>Reference Li</span>2008</a>; Malinverno et al., <a href="#ref67"><span>Reference Malinverno, Erba and Herbert</span>2010</a>; Li et al., <a href="#ref66"><span>Reference Li</span>2017</a>). At least four other minor episodes of organic black shale production are noted in the Cretaceous (the Faraoni event, OAE-1b, 1d and OAE-3) but seem to be restricted to the proto-Atlantic region (Takashima et al., <a href="#ref107"><span>Reference Takashima</span>2006</a>; Jenkyns, <a href="#ref49"><span>Reference Jenkyns</span>2010</a>). At least one similar event occurred in the Jurassic (183&nbsp;Ma) (Pearce et al., <a href="#ref84"><span>Reference Pearce</span>2008</a>).</p><p> The sequence of events during these events have two distinct fingerprints possibly associated with the two differing theoretical mechanisms for the events. For example, during OAE-1b, there is evidence of strong stratification and a stagnant deep ocean, while for OAE-2, the evidence suggests an decrease in stratification, increased upper ocean productivity and an expansion of the oxygen minimum zones (Takashima et al., <a href="#ref107"><span>Reference Takashima</span>2006</a>).</p><p> At the onset of the events (<a href="#fig01">Fig. 1(c)</a>), there is often a significant negative excursion in <em>δ</em> <sup>13</sup>C (as in the PETM), followed by a positive recovery during the events themselves as the burial of (light) organic carbon increased and compensated for the initial release (Jenkyns, <a href="#ref49"><span>Reference Jenkyns</span>2010</a>; Kuhnt et al., <a href="#ref59"><span>Reference Kuhnt, Holbourn and Moullade</span>2011</a>; Mutterlose et al., <a href="#ref76"><span>Reference Mutterlose</span>2014</a>; Naafs et al., <a href="#ref77"><span>Reference Naafs</span>2016</a>). Causes have been linked to the crustal formation/tectonic activity and enhanced CO<sub>2</sub> (or possibly CH<sub>4</sub>) release, causing global warmth (Jenkyns, <a href="#ref49"><span>Reference Jenkyns</span>2010</a>). Increased seawater values of <sup>87</sup>Sr/<sup>86</sup>Sr and <sup>187</sup>Os/<sup>188</sup>Os suggest increased runoff, greater nutrient supply and consequently higher upper ocean productivity (Jones, <a href="#ref50"><span>Reference Jones</span>2001</a>). Possible hiatuses in some OAE 1a sections are suggestive of an upper ocean dissolution event (Bottini et al., <a href="#ref13"><span>Reference Bottini</span>2015</a>).</p><p> Other important shifts in geochemical tracers during the OAEs include much lower nitrogen isotope ratios (<em>δ</em> <sup>15</sup>N), increases in metal concentrations (including As, Bi, Cd, Co, Cr, Ni, V) (Jenkyns, <a href="#ref49"><span>Reference Jenkyns</span>2010</a>). Positive shifts in sulphur isotopes are seen in most OAEs, with a curious exception in OAE-1a where the shift is negative (Turchyn et al., <a href="#ref110"><span>Reference Turchyn</span>2009</a>).</p></div><div data-magellan-destination="sec3-4" id="sec3-4"><h3> Early Mesozoic and Late Paleozoic events</h3><p> Starting from the Devonian period, there have been several major abrupt events registered in terrestrial sections. The sequences of changes and the comprehensiveness of geochemical analyses are less well known than for later events, partly due to the lack of existing ocean sediment, but these have been identified in multiple locations and are presumed to be global in extent.</p><p> The Late Devonian extinction around 380–360&nbsp;Ma, was one of the big five mass extinctions. It is associated with black shales and ocean anoxia (Algeo &amp; Scheckler, <a href="#ref2"><span>Reference Algeo and Scheckler</span>1998</a>), stretching from the Kellwasser events (~378&nbsp;Ma) to the Hangenberg event at the Devonian–Carboniferous boundary (359&nbsp;Ma) (Brezinski et al., <a href="#ref16"><span>Reference Brezinski</span>2009</a>; Vleeschouwer et al., <a href="#ref115"><span>Reference Vleeschouwer</span>2013</a>).</p><p> In the late Carboniferous, around 305&nbsp;Ma the Pangaean tropical rainforests collapsed (Sahney et al., <a href="#ref92"><span>Reference Sahney, Benton and Falcon-Lang</span>2010</a>). This was associated with a shift towards drier and cooler climate, and possibly a reduction in atmospheric oxygen, leading to extinctions of some mega-fauna.</p><p> Lastly, the end-Permian extinction event (252&nbsp;Ma) lasted about 60 kyr was accompanied by an initial decrease in carbon isotopes (−&nbsp;5–7‰), significant global warming and extensive deforestation and wildfires (Krull &amp; Retallack, <a href="#ref58"><span>Reference Krull and Retallack</span>2000</a>; Shen et al., <a href="#ref100"><span>Reference Shen</span>2011</a>; Burgess et al., <a href="#ref17"><span>Reference Burgess, Bowring and zhong Shen</span>2014</a>) associated with widespread ocean anoxia and euxinia (Wignall &amp; Twitchett, <a href="#ref119"><span>Reference Wignall and Twitchett</span>1996</a>). Pre-event spikes in nickel (Ni) have also been reported (Rothman et al., <a href="#ref91"><span>Reference Rothman</span>2014</a>).</p></div></div><div data-magellan-destination="sec4" id="sec4"><h2> Discussion and testable hypotheses</h2><p> There are undoubted similarities between previous abrupt events in the geological record and the likely Anthropocene signature in the geological record to come. Negative, abrupt <em>δ</em> <sup>13</sup>C excursions, warmings and disruptions of the nitrogen cycle are ubiquitous. More complex changes in biota, sedimentation and mineralogy are also common. Specifically, compared with the hypothesized Anthropocene signature, almost all changes found so far for the PETM are of the same sign and comparable magnitude. Some similarities would be expected if the main effect during any event was a significant global warming, however caused. Furthermore, there is evidence at many of these events that warming was driven by a massive input of exogeneous (biogenic) carbon, either as CO<sub>2</sub> or CH<sub>4</sub>. At least since the Carboniferous (300–350&nbsp;Ma), there has been sufficient fossil carbon to fuel an industrial civilization comparable with our own and any of these sources could provide the light carbon input. However, in many cases this input is contemporaneous to significant episodes of tectonic and/or volcanic activity, for instance, the coincidence of crustal formation events with the climate changes suggest that the intrusion of basaltic magmas into organic-rich shales and/or petroleum-bearing evaporites (Storey et al., <a href="#ref105"><span>Reference Storey, Duncan and Swisher</span>2007</a>; Svensen et al., <a href="#ref106"><span>Reference Svensen</span>2009</a>; Kravchinsky, <a href="#ref57"><span>Reference Kravchinsky</span>2012</a>) may have released large quantities of CO<sub>2</sub> or CH<sub>4</sub> to the atmosphere. Impacts to warming and/or carbon influx (such as increased runoff, erosion, etc.) appear to be qualitatively similar whenever in the geological period they occur. These changes are thus not sufficient evidence for prior industrial civilizations.</p><p> Current changes appear to be significantly faster than the paleoclimatic events (<a href="#fig01">Fig. 1</a>), but this may be partly due to limitations of chronology in the geological record. Attempts to time the length of prior events have used constant sedimentation estimates, or constant-flux markers (e.g.<sup>3</sup>He McGee &amp; Mukhopadhyay, <a href="#ref71"><span>Reference McGee and Mukhopadhyay</span>2012</a>), or orbital chronologies, or supposed annual or seasonal banding in the sediment (Wright &amp; Schaller, <a href="#ref120"><span>Reference Wright and Schaller</span>2013</a>). The accuracy of these methods suffer when there are large changes in sedimentation or hiatuses across these events (which is common), or rely on the imperfect identification of regularities with specific astronomical features (Pearson &amp; Nicholas, <a href="#ref85"><span>Reference Pearson and Nicholas</span>2014</a>; Pearson &amp; Thomas, <a href="#ref86"><span>Reference Pearson and Thomas</span>2015</a>). Additionally, bioturbation will often smooth an abrupt event even in a perfectly preserved sedimentary setting. Thus, the ability to detect an event onset of a few centuries (or less) in the record is questionable, and so direct isolation of an industrial cause based only on apparent timing is also not conclusive.</p><p> The specific markers of human industrial activity discussed above (plastics, synthetic pollutants, increased metal concentrations, etc.) are however a consequence of the specific path human society and technology has taken, and the generality of that pathway for other industrial species is totally unknown. Large-scale energy harnessing is potentially a more universal indicator, and given the large energy density in carbon-based fossil fuel, one might postulate that a light <em>δ</em> <sup>13</sup>C signal might be a common signal. Conceivably, solar, hydro or geothermal energy sources could have been tapped preferentially, and that would greatly reduce any geological footprint (as it would ours). However, any large release of biogenic carbon whether from methane hydrate pools or volcanic intrusions into organic-rich sediments, will have a similar signal. We therefore have a situation where the known unique markers might not be indicative, while the (perhaps) more expected markers are not sufficient.</p><p> We are aware that raising the possibility of a prior industrial civilization as a driver for events in the geological record might lead to rather unconstrained speculation. One would be able to fit any observations to an imagined civilization in ways that would be basically unfalsifiable. Thus, care must be taken not to postulate such a cause until actually positive evidence is available. The Silurian hypothesis cannot be regarded as likely merely because no other valid idea presents itself.</p><p> We nonetheless find the above analyses intriguing enough to motivate some additional research. Firstly, despite copious existing work on the likely Anthropocene signature, we recommend further synthesis and study on the persistence of uniquely industrial byproducts in ocean sediment environments. Are there other classes of compounds that will leave unique traces in the sediment geochemistry on multi-million year timescales? In particular, will the byproducts of common plastics, or organic long-chain synthetics, be detectable?</p><p> Secondly, and this is indeed more speculative, we propose that a deeper exploration of elemental and compositional anomalies in extant sediments spanning previous events be performed (although we expect that far more information has been obtained about these sections than has been referenced here). Oddities in these sections have been looked for previously as potential signals of impact events (successfully for the K–T boundary event, not so for any of the events mentioned above), ranging from iridium layers, shocked quartz, micro-tectites, magnetites, etc. But it may be that a new search and new analyses with the Silurian hypothesis in mind might reveal more. Anomalous behaviour in the past might be more clearly detectable in proxies normalized by weathering fluxes or other constant flux proxies in order to highlight times when productivity or metal production might have been artificially enhanced. Thirdly, should any unexplained anomalies be found, the question of whether there are candidate species in the fossil record may become more relevant, as might questions about their ultimate fate.</p><p> An intriguing hypothesis presents itself should any of the initial releases of light carbon described above indeed be related to a prior industrial civilization. As discussed in the section ‘Cretaceous and Jurassic ocean anoxic events’, these releases often triggered episodes of ocean anoxia (via increased nutrient supply) causing a massive burial of organic matter, which eventually became source strata for further fossil fuels. Thus, the prior industrial activity would have actually given rise to the potential for future industry via their own demise. Large-scale anoxia, in effect, might provide a self-limiting but self-perpetuating feedback of industry on the planet. Alternatively, it may be just be a part of a long-term episodic natural carbon cycle feedback on tectonically active planets.</p><p> Perhaps unusually, the authors of this paper are not convinced of the correctness of their proposed hypothesis. Were it to be true it would have profound implications and not just for astrobiology. However, most readers do not need to be told that it is always a bad idea to decide on the truth or falsity of an idea based on the consequences of it being true. While we strongly doubt that any previous industrial civilization existed before our own, asking the question in a formal way that articulates explicitly what evidence for such a civilization might look like raises its own useful questions related both to astrobiology and to Anthropocene studies. Thus, we hope that this paper will serve as motivation to improve the constraints on the hypothesis so that in future we may be better placed to answer our title question.</p></div></div><div data-magellan-destination="ack" id="ack"><h2> Acknowledgments</h2><p> No funding has been provided nor sought for this study. We thank Susan Kidwell for being generous with her time and helpful discussions, David Naafs and Stuart Robinson for help and pointers to data for OAE1a and Chris Reinhard for his thoughtful review. The GISTEMP data in <a href="#fig01">Fig. 1(a)</a> were from <a href="https://data.giss.nasa.gov/gistemp">https://data.giss.nasa.gov/gistemp</a> (accessed Jul 15 2017).</p></div></div>    <div id="references-list"><h2>References</h2> <div id="manual_ref-1" aria-flowto="reference-1-content reference-1-button"><p><span><span>Achille</span>, <span>GD</span></span> and <span><span>Hynek</span>, <span>BM</span></span> (<span>2010</span>) <span>Nature Geoscience</span> <span>3</span>, <span>459</span>.<a target="_blank" aria-label="Google Scholar link for " href="https://scholar.google.com/scholar?q=Achille,+GD+and+Hynek,+BM+(2010)+Nature+Geoscience+3,+459.">Google Scholar</a></p></div><div id="manual_ref-2" aria-flowto="reference-2-content reference-2-button"><p><span><span>Algeo</span>, <span>TJ</span></span> and <span><span>Scheckler</span>, <span>SE</span></span> (<span>1998</span>) <span>Philosophical Transactions of the Royal Society B: Biological Sciences</span> <span>353</span>, <span>113</span>.<a target="_blank" aria-label="Google Scholar link for " href="https://scholar.google.com/scholar?q=Algeo,+TJ+and+Scheckler,+SE+(1998)+Philosophical+Transactions+of+the+Royal+Society+B:+Biological+Sciences+353,+113.">Google Scholar</a></p></div><div id="manual_ref-3" aria-flowto="reference-3-content reference-3-button"><p><span><span>Andrady</span>, <span>A</span></span>, et al. (<span>1998</span>) <span>Journal of Photochemistry and Photobiology B: Biology</span> <span>46</span>, <span>96</span>.<a target="_blank" aria-label="Google Scholar link for " href="https://scholar.google.com/scholar?q=Andrady,+A,+et+al.+(1998)+Journal+of+Photochemistry+and+Photobiology+B:+Biology+46,+96.">Google Scholar</a></p></div><div id="manual_ref-4" aria-flowto="reference-4-content reference-4-button"><p><span><span>Andrady</span>, <span>AL</span></span> (<span>2015</span>) <span>Marine Anthropogenic Litter</span> (<span>Springer Nature</span>), <span>57</span>–<span>72</span>.<a target="_blank" aria-label="Google Scholar link for " href="https://scholar.google.com/scholar?q=Andrady,+AL+(2015)+Marine+Anthropogenic+Litter+(Springer+Nature),+57%E2%80%9372.">Google Scholar</a></p></div><div id="manual_ref-6" aria-flowto="reference-6-content reference-6-button"><p><span><span>Aubry</span>, <span>M-P</span></span>, <span><span>Lucas</span>, <span>S</span></span> and <span><span>Berggren</span>, <span>WA</span></span> eds (<span>1998</span>) <span>Late Paleocene-early Eocene Biotic and Climatic Events in the Marine and Terrestrial Records</span>. <span>Columbia University Press</span>, <span>New York, NY</span>.<a target="_blank" aria-label="Google Scholar link for " href="https://scholar.google.com/scholar?q=Aubry,+M-P,+Lucas,+S+and+Berggren,+WA+eds+(1998)+Late+Paleocene-early+Eocene+Biotic+and+Climatic+Events+in+the+Marine+and+Terrestrial+Records.+Columbia+University+Press,+New+York,+NY.">Google Scholar</a></p></div><div id="manual_ref-7" aria-flowto="reference-7-content reference-7-button"><p><span><span>Behrensmeyer</span>, <span>AK</span></span>, <span><span>Kidwell</span>, <span>SM</span></span> and <span><span>Gastaldo</span>, <span>RA</span></span> (<span>2000</span>) <span>Paleobiology</span> <span>26</span>, <span>103</span>.<a target="_blank" aria-label="Google Scholar link for " href="https://scholar.google.com/scholar?q=Behrensmeyer,+AK,+Kidwell,+SM+and+Gastaldo,+RA+(2000)+Paleobiology+26,+103.">Google Scholar</a></p></div><div id="manual_ref-8" aria-flowto="reference-8-content reference-8-button"><p><span><span>Bernhardt</span>, <span>ES</span></span>, <span><span>Rosi</span>, <span>EJ</span></span> and <span><span>Gessner</span>, <span>MO</span></span> (<span>2017</span>) <span>Frontiers in Ecology and the Environment</span> <span>15</span>, <span>84</span>.<a target="_blank" aria-label="Google Scholar link for " href="https://scholar.google.com/scholar?q=Bernhardt,+ES,+Rosi,+EJ+and+Gessner,+MO+(2017)+Frontiers+in+Ecology+and+the+Environment+15,+84.">Google Scholar</a></p></div><div id="manual_ref-9" aria-flowto="reference-9-content reference-9-button"><p><span><span>Beyer</span>, <span>A</span></span>, et al. (<span>2000</span>) <span>Environmental Science &amp; Technology</span> <span>34</span>, <span>699</span>.<a target="_blank" aria-label="Google Scholar link for " href="https://scholar.google.com/scholar?q=Beyer,+A,+et+al.+(2000)+Environmental+Science+&amp;+Technology+34,+699.">Google Scholar</a></p></div><div id="manual_ref-10" aria-flowto="reference-10-content reference-10-button"><p><span><span>Bindoff</span>, <span>NL</span></span>, et al. (<span>2013</span>) <span>Climate change 2013: the physical science basis</span>. In <span><span>Stocker</span>, <span>TF</span></span>, <span><span>Qin</span>, <span>D</span></span>, <span><span>Plattner</span>, <span>G-K</span></span>, <span><span>Tignor</span>, <span>M</span></span> , <span><span>Allen</span>, <span>SK</span></span> , <span><span>Boschung</span>, <span>J</span></span> , <span><span>Nauels</span>, <span>A</span></span> , <span><span>Xia</span>, <span>Y</span></span> , <span><span>Bex</span>, <span>V</span></span> and <span><span>Midgley</span>, <span>P</span></span> eds. <span>Contribution of Working Group I to the Fifth Assessment Report of the Intergovernmental Panel on Climate Change</span>. <span>Cambridge</span>: <span>Cambridge University Press</span>, pp. <span>867</span>–<span>952</span>.<a target="_blank" aria-label="Google Scholar link for " href="https://scholar.google.com/scholar?q=Bindoff,+NL,+et+al.+(2013)+Climate+change+2013:+the+physical+science+basis.+In+Stocker,+TF,+Qin,+D,+Plattner,+G-K,+Tignor,+M+,+Allen,+SK+,+Boschung,+J+,+Nauels,+A+,+Xia,+Y+,+Bex,+V+and+Midgley,+P+eds.+Contribution+of+Working+Group+I+to+the+Fifth+Assessment+Report+of+the+Intergovernmental+Panel+on+Climate+Change.+Cambridge:+Cambridge+University+Press,+pp.+867%E2%80%93952.">Google Scholar</a></p></div><div id="manual_ref-11" aria-flowto="reference-11-content reference-11-button"><p><span><span>Böhm</span>, <span>F</span></span>, et al. (<span>2002</span>) <span>Geochemistry, Geophysics, Geosystems</span> <span>3</span>, <span>1</span>.<a target="_blank" aria-label="Google Scholar link for " href="https://scholar.google.com/scholar?q=B%C3%B6hm,+F,+et+al.+(2002)+Geochemistry,+Geophysics,+Geosystems+3,+1.">Google Scholar</a></p></div><div id="manual_ref-14" aria-flowto="reference-14-content reference-14-button"><p><span><span>Bottini</span>, <span>C</span></span> and <span><span>Mutterlose</span>, <span>J</span></span> (<span>2012</span>) <span>Newsletters on Stratigraphy</span> <span>45</span>, <span>115</span>.<a target="_blank" aria-label="Google Scholar link for " href="https://scholar.google.com/scholar?q=Bottini,+C+and+Mutterlose,+J+(2012)+Newsletters+on+Stratigraphy+45,+115.">Google Scholar</a></p></div><div id="manual_ref-15" aria-flowto="reference-15-content reference-15-button"><p><span><span>Breitburg</span>, <span>D</span></span>, et al. (<span>2018</span>) <span>Science</span> <span>359</span>, <span>eaam7240</span>, doi: <span>10.1126/science.aam7240</span>.<a target="_blank" aria-label="Google Scholar link for " href="https://scholar.google.com/scholar?q=Breitburg,+D,+et+al.+(2018)+Science+359,+eaam7240,+doi:+10.1126/science.aam7240.">Google Scholar</a></p></div><div id="manual_ref-16" aria-flowto="reference-16-content reference-16-button"><p><span><span>Brezinski</span>, <span>DK</span></span>, et al. (<span>2009</span>) <span>Palaeogeography, Palaeoclimatology, Palaeoecology</span> <span>284</span>, <span>315</span>.<a target="_blank" aria-label="Google Scholar link for " href="https://scholar.google.com/scholar?q=Brezinski,+DK,+et+al.+(2009)+Palaeogeography,+Palaeoclimatology,+Palaeoecology+284,+315.">Google Scholar</a></p></div><div id="manual_ref-17" aria-flowto="reference-17-content reference-17-button"><p><span><span>Burgess</span>, <span>SD</span></span>, <span><span>Bowring</span>, <span>S</span></span> and <span><span>zhong Shen</span>, <span>S</span></span> (<span>2014</span>) <span>Proceedings of the National Academy of Sciences USA</span> <span>111</span>, <span>3316</span>.<a target="_blank" aria-label="Google Scholar link for " href="https://scholar.google.com/scholar?q=Burgess,+SD,+Bowring,+S+and+zhong+Shen,+S+(2014)+Proceedings+of+the+National+Academy+of+Sciences+USA+111,+3316.">Google Scholar</a></p></div><div id="manual_ref-18" aria-flowto="reference-18-content reference-18-button"><p><span><span>Canfield</span>, <span>DE</span></span>, <span><span>Glazer</span>, <span>AN</span></span> and <span><span>Falkowski</span>, <span>PG</span></span> (<span>2010</span>) <span>Science</span> <span>330</span>, <span>192</span>.<a target="_blank" aria-label="Google Scholar link for " href="https://scholar.google.com/scholar?q=Canfield,+DE,+Glazer,+AN+and+Falkowski,+PG+(2010)+Science+330,+192.">Google Scholar</a></p></div><div id="manual_ref-19" aria-flowto="reference-19-content reference-19-button"><p><span><span>Cleland</span>, <span>CE</span></span> and <span><span>Copley</span>, <span>SD</span></span> (<span>2006</span>) <span>International Journal of Astrobiology</span> <span>4</span>, <span>165</span>.<a target="_blank" aria-label="Google Scholar link for " href="https://scholar.google.com/scholar?q=Cleland,+CE+and+Copley,+SD+(2006)+International+Journal+of+Astrobiology+4,+165.">Google Scholar</a></p></div><div id="manual_ref-20" aria-flowto="reference-20-content reference-20-button"><p><span><span>Corcoran</span>, <span>PL</span></span>, <span><span>Moore</span>, <span>CJ</span></span> and <span><span>Jazvac</span>, <span>K</span></span> (<span>2014</span>) GSA Today, <span>24</span>, <span>4</span>–<span>8</span><a target="_blank" aria-label="Google Scholar link for " href="https://scholar.google.com/scholar?q=Corcoran,+PL,+Moore,+CJ+and+Jazvac,+K+(2014)+GSA+Today,+24,+4%E2%80%938">Google Scholar</a></p></div><div id="manual_ref-21" aria-flowto="reference-21-content reference-21-button"><p><span><span>Cozar</span>, <span>A</span></span>, et al. (<span>2014</span>) <span>Proceedings of the National Academy of Sciences USA</span> <span>111</span>, <span>10239</span>.<a target="_blank" aria-label="Google Scholar link for " href="https://scholar.google.com/scholar?q=Cozar,+A,+et+al.+(2014)+Proceedings+of+the+National+Academy+of+Sciences+USA+111,+10239.">Google Scholar</a></p></div><div id="manual_ref-22" aria-flowto="reference-22-content reference-22-button"><p><span><span>Cristini</span>, <span>L</span></span>, et al. (<span>2012</span>) <span>Palaeogeography, Palaeoclimatology, Palaeoecology</span> <span>339–341</span>, <span>66</span>.<a target="_blank" aria-label="Google Scholar link for " href="https://scholar.google.com/scholar?q=Cristini,+L,+et+al.+(2012)+Palaeogeography,+Palaeoclimatology,+Palaeoecology+339%E2%80%93341,+66.">Google Scholar</a></p></div><div id="manual_ref-27" aria-flowto="reference-27-content reference-27-button"><p><span><span>Drake</span>, <span>FD</span></span> (<span>1961</span>) Discussion at Space Science Board-National Academy of Sciences Conference on Extraterrestrial Intelligent Life, Green Bank, West Virginia, USA.<a target="_blank" aria-label="Google Scholar link for " href="https://scholar.google.com/scholar?q=Drake,+FD+(1961)+Discussion+at+Space+Science+Board-National+Academy+of+Sciences+Conference+on+Extraterrestrial+Intelligent+Life,+Green+Bank,+West+Virginia,+USA.">Google Scholar</a></p></div><div id="manual_ref-28" aria-flowto="reference-28-content reference-28-button"><p><span><span>Drake</span>, <span>FD</span></span> (<span>1965</span>) In <span><span>Mamikunian</span>, <span>G</span></span> and <span><span>Briggs</span>, <span>MH</span></span> eds. <span>Current Aspects of Exobiology</span>. <span>Pergamon</span>, <span>Oxford</span>, pp. <span>323</span>–<span>345</span>.<a target="_blank" aria-label="Google Scholar link for " href="https://scholar.google.com/scholar?q=Drake,+FD+(1965)+In+Mamikunian,+G+and+Briggs,+MH+eds.+Current+Aspects+of+Exobiology.+Pergamon,+Oxford,+pp.+323%E2%80%93345.">Google Scholar</a></p></div><div id="manual_ref-29" aria-flowto="reference-29-content reference-29-button"><p><span><span>Dressing</span>, <span>CD</span></span> and <span><span>Charbonneau</span>, <span>D</span></span> (<span>2013</span>) <span>The Astrophysical Journal</span> <span>767</span>, <span>95</span>.<a target="_blank" aria-label="Google Scholar link for " href="https://scholar.google.com/scholar?q=Dressing,+CD+and+Charbonneau,+D+(2013)+The+Astrophysical+Journal+767,+95.">Google Scholar</a></p></div><div id="manual_ref-30" aria-flowto="reference-30-content reference-30-button"><p><span><span>Eide</span>, <span>M</span></span>, et al. (<span>2017</span>) <span>Global Biogeochemical Cycles</span> <span>31</span>, <span>492</span>–<span>514</span>.<a target="_blank" aria-label="Google Scholar link for " href="https://scholar.google.com/scholar?q=Eide,+M,+et+al.+(2017)+Global+Biogeochemical+Cycles+31,+492%E2%80%93514.">Google Scholar</a></p></div><div id="manual_ref-31" aria-flowto="reference-31-content reference-31-button"><p><span><span>Erba</span>, <span>E</span></span>, <span><span>Bartolini</span>, <span>A</span></span> and <span><span>Larson</span>, <span>RL</span></span> (<span>2004</span>) <span>Geology</span> <span>32</span>, <span>149</span>.<a target="_blank" aria-label="Google Scholar link for " href="https://scholar.google.com/scholar?q=Erba,+E,+Bartolini,+A+and+Larson,+RL+(2004)+Geology+32,+149.">Google Scholar</a></p></div><div id="manual_ref-35" aria-flowto="reference-35-content reference-35-button"><p><span><span>Gałuszka</span>, <span>A</span></span>, <span><span>Migaszewski</span>, <span>ZM</span></span> and <span><span>Zalasiewicz</span>, <span>J</span></span> (<span>2013</span>) <span>Geological Society, London, Special Publications</span> <span>395</span>, <span>221</span>.<a target="_blank" aria-label="Google Scholar link for " href="https://scholar.google.com/scholar?q=Ga%C5%82uszka,+A,+Migaszewski,+ZM+and+Zalasiewicz,+J+(2013)+Geological+Society,+London,+Special+Publications+395,+221.">Google Scholar</a></p></div><div id="manual_ref-36" aria-flowto="reference-36-content reference-36-button"><p><span><span>Gauthier-Lafaye</span>, <span>F</span></span>, <span><span>Holliger</span>, <span>P</span></span> and <span><span>Blanc</span>, <span>P-L</span></span> (<span>1996</span>) <span>Geochimica et Cosmochimica Acta</span> <span>60</span>, <span>4831</span>.<a target="_blank" aria-label="Google Scholar link for " href="https://scholar.google.com/scholar?q=Gauthier-Lafaye,+F,+Holliger,+P+and+Blanc,+P-L+(1996)+Geochimica+et+Cosmochimica+Acta+60,+4831.">Google Scholar</a></p></div><div id="manual_ref-37" aria-flowto="reference-37-content reference-37-button"><p><span>GISTEMP Team</span> (<span>2016</span>) GISS Surface Temperature Analysis (GISTEMP).<a target="_blank" aria-label="Google Scholar link for " href="https://scholar.google.com/scholar?q=GISTEMP+Team+(2016)+GISS+Surface+Temperature+Analysis+(GISTEMP).">Google Scholar</a></p></div><div id="manual_ref-38" aria-flowto="reference-38-content reference-38-button"><p><span><span>Goudie</span>, <span>A</span></span> (<span>2000</span>) The human impact on the soil (The Human Impact on the Natural Environment. MIT Press), 88.<a target="_blank" aria-label="Google Scholar link for " href="https://scholar.google.com/scholar?q=Goudie,+A+(2000)+The+human+impact+on+the+soil+(The+Human+Impact+on+the+Natural+Environment.+MIT+Press),+88.">Google Scholar</a></p></div><div id="manual_ref-40" aria-flowto="reference-40-content reference-40-button"><p><span><span>Hansen</span>, <span>J</span></span>, et al. (<span>2010</span>) <span>Reviews in Geophysics</span> <span>48</span>, EID: RG4004.<a target="_blank" aria-label="Google Scholar link for " href="https://scholar.google.com/scholar?q=Hansen,+J,+et+al.+(2010)+Reviews+in+Geophysics+48,+EID:+RG4004.">Google Scholar</a></p></div><div id="manual_ref-41" aria-flowto="reference-41-content reference-41-button"><p><span><span>Haqq-Misra</span>, <span>J</span></span> and <span><span>Kopparapu</span>, <span>RK</span></span> (<span>2012</span>) <span>Acta Astronautica</span> <span>72</span>, <span>15</span>.<a target="_blank" aria-label="Google Scholar link for " href="https://scholar.google.com/scholar?q=Haqq-Misra,+J+and+Kopparapu,+RK+(2012)+Acta+Astronautica+72,+15.">Google Scholar</a></p></div><div id="manual_ref-42" aria-flowto="reference-42-content reference-42-button"><p><span><span>Hazen</span>, <span>RM</span></span>, et al. (<span>2017</span>) <span>American Mineralogist</span> <span>102</span>, <span>595</span>.<a target="_blank" aria-label="Google Scholar link for " href="https://scholar.google.com/scholar?q=Hazen,+RM,+et+al.+(2017)+American+Mineralogist+102,+595.">Google Scholar</a></p></div><div id="manual_ref-44" aria-flowto="reference-44-content reference-44-button"><p><span><span>Hogan</span>, <span>J</span></span> (<span>1977</span>) Inherit the Stars (Ballantine Books, New York).<a target="_blank" aria-label="Google Scholar link for " href="https://scholar.google.com/scholar?q=Hogan,+J+(1977)+Inherit+the+Stars+(Ballantine+Books,+New+York).">Google Scholar</a></p></div><div id="manual_ref-47" aria-flowto="reference-47-content reference-47-button"><p><span><span>Ito</span>, <span>T</span></span>, et al. (<span>2017</span>) <span>Geophysical Research Letters</span> <span>44</span>, <span>4214</span>–<span>4223</span>.<a target="_blank" aria-label="Google Scholar link for " href="https://scholar.google.com/scholar?q=Ito,+T,+et+al.+(2017)+Geophysical+Research+Letters+44,+4214%E2%80%934223.">Google Scholar</a></p></div><div id="manual_ref-48" aria-flowto="reference-48-content reference-48-button"><p><span><span>Jenkyns</span>, <span>HC</span></span> (<span>2010</span>) <span>Geochemistry, Geophysics, Geosystems</span> <span>11</span>, <span>Q03004</span>.<a target="_blank" aria-label="Google Scholar link for " href="https://scholar.google.com/scholar?q=Jenkyns,+HC+(2010)+Geochemistry,+Geophysics,+Geosystems+11,+Q03004.">Google Scholar</a></p></div><div id="manual_ref-51" aria-flowto="reference-51-content reference-51-button"><p><span><span>Kerr</span>, <span>AC</span></span> (<span>1998</span>) <span>Journal of the Geological Society</span> <span>155</span>, <span>619</span>.<a target="_blank" aria-label="Google Scholar link for " href="https://scholar.google.com/scholar?q=Kerr,+AC+(1998)+Journal+of+the+Geological+Society+155,+619.">Google Scholar</a></p></div><div id="manual_ref-52" aria-flowto="reference-52-content reference-52-button"><p><span><span>Kidwell</span>, <span>SM</span></span> (<span>2015</span>) <span>Proceedings of the National Academy of Sciences USA</span> <span>112</span>, <span>4922</span>.<a target="_blank" aria-label="Google Scholar link for " href="https://scholar.google.com/scholar?q=Kidwell,+SM+(2015)+Proceedings+of+the+National+Academy+of+Sciences+USA+112,+4922.">Google Scholar</a></p></div><div id="manual_ref-53" aria-flowto="reference-53-content reference-53-button"><p><span><span>Kim</span>, <span>S-T</span></span> and <span><span>O'Neil</span>, <span>JR</span></span> (<span>1997</span>) <span>Geochimica et Cosmochimica Acta</span> <span>61</span>, <span>3461</span>.<a target="_blank" aria-label="Google Scholar link for " href="https://scholar.google.com/scholar?q=Kim,+S-T+and+O%27Neil,+JR+(1997)+Geochimica+et+Cosmochimica+Acta+61,+3461.">Google Scholar</a></p></div><div id="manual_ref-54" aria-flowto="reference-54-content reference-54-button"><p><span><span>Kirtland Turner</span>, <span>S</span></span>, et al. (<span>2017</span>) <span>Nature Communications</span> <span>8</span>, Article ID: 353<a target="_blank" aria-label="Google Scholar link for " href="https://scholar.google.com/scholar?q=Kirtland+Turner,+S,+et+al.+(2017)+Nature+Communications+8,+Article+ID:+353">Google Scholar</a></p></div><div id="manual_ref-55" aria-flowto="reference-55-content reference-55-button"><p><span><span>Koch</span>, <span>PL</span></span>, <span><span>Zachos</span>, <span>JC</span></span> and <span><span>Gingerich</span>, <span>P</span></span> (<span>1992</span>) <span>Nature</span> <span>358</span>, <span>319</span>.<a target="_blank" aria-label="Google Scholar link for " href="https://scholar.google.com/scholar?q=Koch,+PL,+Zachos,+JC+and+Gingerich,+P+(1992)+Nature+358,+319.">Google Scholar</a></p></div><div id="manual_ref-56" aria-flowto="reference-56-content reference-56-button"><p><span><span>Köhler</span>, <span>P</span></span> (<span>2016</span>) <span>Environmental Research Letters</span> <span>11</span>, <span>124016</span>.<a target="_blank" aria-label="Google Scholar link for " href="https://scholar.google.com/scholar?q=K%C3%B6hler,+P+(2016)+Environmental+Research+Letters+11,+124016.">Google Scholar</a></p></div><div id="manual_ref-57" aria-flowto="reference-57-content reference-57-button"><p><span><span>Kravchinsky</span>, <span>VA</span></span> (<span>2012</span>) <span>Global and Planetary Change</span> <span>86–87</span>, <span>31</span>.<a target="_blank" aria-label="Google Scholar link for " href="https://scholar.google.com/scholar?q=Kravchinsky,+VA+(2012)+Global+and+Planetary+Change+86%E2%80%9387,+31.">Google Scholar</a></p></div><div id="manual_ref-58" aria-flowto="reference-58-content reference-58-button"><p><span><span>Krull</span>, <span>ES</span></span> and <span><span>Retallack</span>, <span>GJ</span></span> (<span>2000</span>) <span>Geological Society of America Bulletin</span> <span>112</span>, <span>1459</span>.<a target="_blank" aria-label="Google Scholar link for " href="https://scholar.google.com/scholar?q=Krull,+ES+and+Retallack,+GJ+(2000)+Geological+Society+of+America+Bulletin+112,+1459.">Google Scholar</a></p></div><div id="manual_ref-59" aria-flowto="reference-59-content reference-59-button"><p><span><span>Kuhnt</span>, <span>W</span></span>, <span><span>Holbourn</span>, <span>A</span></span> and <span><span>Moullade</span>, <span>M</span></span> (<span>2011</span>) <span>Geology</span> <span>39</span>, <span>323</span>.<a target="_blank" aria-label="Google Scholar link for " href="https://scholar.google.com/scholar?q=Kuhnt,+W,+Holbourn,+A+and+Moullade,+M+(2011)+Geology+39,+323.">Google Scholar</a></p></div><div id="manual_ref-60" aria-flowto="reference-60-content reference-60-button"><p><span><span>Kunkel</span>, <span>KE</span></span>, et al. (<span>2013</span>) <span>Bulletin of the American Meteorological Society</span> <span>94</span>, <span>499</span>.<a target="_blank" aria-label="Google Scholar link for " href="https://scholar.google.com/scholar?q=Kunkel,+KE,+et+al.+(2013)+Bulletin+of+the+American+Meteorological+Society+94,+499.">Google Scholar</a></p></div><div id="manual_ref-62" aria-flowto="reference-62-content reference-62-button"><p><span><span>Le Quéré</span>, <span>C</span></span>, et al. (<span>2016</span>) <span>Earth System Science Data</span> <span>8</span>, <span>605</span>.<a target="_blank" aria-label="Google Scholar link for " href="https://scholar.google.com/scholar?q=Le+Qu%C3%A9r%C3%A9,+C,+et+al.+(2016)+Earth+System+Science+Data+8,+605.">Google Scholar</a></p></div><div id="manual_ref-65" aria-flowto="reference-65-content reference-65-button"><p><span><span>Li</span>, <span>Y-X</span></span>, et al. (<span>2008</span>) <span>Earth and Planetary Science Letters</span> <span>271</span>, <span>88</span>.<a target="_blank" aria-label="Google Scholar link for " href="https://scholar.google.com/scholar?q=Li,+Y-X,+et+al.+(2008)+Earth+and+Planetary+Science+Letters+271,+88.">Google Scholar</a></p></div><div id="manual_ref-66" aria-flowto="reference-66-content reference-66-button"><p><span><span>Li</span>, <span>Y-X</span></span>, et al. (<span>2017</span>) <span>Earth and Planetary Science Letters</span> <span>462</span>, <span>35</span>.<a target="_blank" aria-label="Google Scholar link for " href="https://scholar.google.com/scholar?q=Li,+Y-X,+et+al.+(2017)+Earth+and+Planetary+Science+Letters+462,+35.">Google Scholar</a></p></div><div id="manual_ref-67" aria-flowto="reference-67-content reference-67-button"><p><span><span>Malinverno</span>, <span>A</span></span>, <span><span>Erba</span>, <span>E</span></span> and <span><span>Herbert</span>, <span>TD</span></span> (<span>2010</span>) <span>Paleoceanography</span> <span>25</span>, EID: PA2203.<a target="_blank" aria-label="Google Scholar link for " href="https://scholar.google.com/scholar?q=Malinverno,+A,+Erba,+E+and+Herbert,+TD+(2010)+Paleoceanography+25,+EID:+PA2203.">Google Scholar</a></p></div><div id="manual_ref-68" aria-flowto="reference-68-content reference-68-button"><p><span><span>Mann</span>, <span>ME</span></span>, et al. (<span>2008</span>) <span>Proceedings of the National Academy of Sciences USA</span> <span>105</span>, <span>13252</span>.<a target="_blank" aria-label="Google Scholar link for " href="https://scholar.google.com/scholar?q=Mann,+ME,+et+al.+(2008)+Proceedings+of+the+National+Academy+of+Sciences+USA+105,+13252.">Google Scholar</a></p></div><div id="manual_ref-69" aria-flowto="reference-69-content reference-69-button"><p><span><span>Marino</span>, <span>L</span></span> (<span>2015</span>) <span>Fraction of life-bearing planets on which intelligent life emerges, fi, 1961 to the present</span>. In <span><span>Vakoch</span>, <span>DA</span></span> and <span><span>Dowd</span>, <span>MF</span></span> (eds). <span>The Drake Equation: Estimating the Prevalence of Extraterrestrial Life through the Ages</span>. <span>Cambridge University Press</span>, <span>Cambridge, UK</span>, pp. <span>181</span>–<span>204</span>.<a target="_blank" aria-label="Google Scholar link for " href="https://scholar.google.com/scholar?q=Marino,+L+(2015)+Fraction+of+life-bearing+planets+on+which+intelligent+life+emerges,+fi,+1961+to+the+present.+In+Vakoch,+DA+and+Dowd,+MF+(eds).+The+Drake+Equation:+Estimating+the+Prevalence+of+Extraterrestrial+Life+through+the+Ages.+Cambridge+University+Press,+Cambridge,+UK,+pp.+181%E2%80%93204.">Google Scholar</a></p></div><div id="manual_ref-70" aria-flowto="reference-70-content reference-70-button"><p><span><span>Matmon</span>, <span>A</span></span>, et al. (<span>2009</span>) <span>Geological Society of America Bulletin</span> <span>121</span>, <span>688</span>.<a target="_blank" aria-label="Google Scholar link for " href="https://scholar.google.com/scholar?q=Matmon,+A,+et+al.+(2009)+Geological+Society+of+America+Bulletin+121,+688.">Google Scholar</a></p></div><div id="manual_ref-71" aria-flowto="reference-71-content reference-71-button"><p><span><span>McGee</span>, <span>D</span></span> and <span><span>Mukhopadhyay</span>, <span>S</span></span> (<span>2012</span>) <span>The Noble Gases as Geochemical Tracers</span>. <span>Berlin, Heidelberg</span>: <span>Springer</span>, pp. <span>155</span>–<span>176</span>.<a target="_blank" aria-label="Google Scholar link for " href="https://scholar.google.com/scholar?q=McGee,+D+and+Mukhopadhyay,+S+(2012)+The+Noble+Gases+as+Geochemical+Tracers.+Berlin,+Heidelberg:+Springer,+pp.+155%E2%80%93176.">Google Scholar</a></p></div><div id="manual_ref-72" aria-flowto="reference-72-content reference-72-button"><p><span><span>McInerney</span>, <span>FA</span></span> and <span><span>Wing</span>, <span>SL</span></span> (<span>2011</span>) <span>Annual Review of Earth and Planetary Sciences</span> <span>39</span>, <span>489</span>.<a target="_blank" aria-label="Google Scholar link for " href="https://scholar.google.com/scholar?q=McInerney,+FA+and+Wing,+SL+(2011)+Annual+Review+of+Earth+and+Planetary+Sciences+39,+489.">Google Scholar</a></p></div><div id="manual_ref-73" aria-flowto="reference-73-content reference-73-button"><p><span><span>Meierhenrich</span>, <span>U</span></span> (<span>2008</span>) <span>Amino Acids and the Asymmetry of Life</span>. <span>Berlin, Heidelberg</span>: <span>Springer</span>.<a target="_blank" aria-label="Google Scholar link for " href="https://scholar.google.com/scholar?q=Meierhenrich,+U+(2008)+Amino+Acids+and+the+Asymmetry+of+Life.+Berlin,+Heidelberg:+Springer.">Google Scholar</a></p></div><div id="manual_ref-74" aria-flowto="reference-74-content reference-74-button"><p><span><span>Meyer</span>, <span>KM</span></span> and <span><span>Kump</span>, <span>LR</span></span> (<span>2008</span>) <span>Annual Review of Earth and Planetary Sciences</span> <span>36</span>, <span>251</span>.<a target="_blank" aria-label="Google Scholar link for " href="https://scholar.google.com/scholar?q=Meyer,+KM+and+Kump,+LR+(2008)+Annual+Review+of+Earth+and+Planetary+Sciences+36,+251.">Google Scholar</a></p></div><div id="manual_ref-79" aria-flowto="reference-79-content reference-79-button"><p><span><span>Nazarenko</span>, <span>L</span></span>, et al. (<span>2015</span>) <span>Journal of Advances in Modeling Earth Systems</span> <span>7</span>, <span>244</span>.<a target="_blank" aria-label="Google Scholar link for " href="https://scholar.google.com/scholar?q=Nazarenko,+L,+et+al.+(2015)+Journal+of+Advances+in+Modeling+Earth+Systems+7,+244.">Google Scholar</a></p></div><div id="manual_ref-80" aria-flowto="reference-80-content reference-80-button"><p><span>ODP Leg 801 Team</span> (<span>2000</span>) Proceedings of the Ocean Drilling Program (International Ocean Discovery Program (IODP)).<a target="_blank" aria-label="Google Scholar link for " href="https://scholar.google.com/scholar?q=ODP+Leg+801+Team+(2000)+Proceedings+of+the+Ocean+Drilling+Program+(International+Ocean+Discovery+Program+(IODP)).">Google Scholar</a></p></div><div id="manual_ref-82" aria-flowto="reference-82-content reference-82-button"><p><span><span>Overeem</span>, <span>I</span></span>, et al. (<span>2011</span>) <span>Geophysical Research Letters</span> <span>38</span>, <span>L17503</span>.<a target="_blank" aria-label="Google Scholar link for " href="https://scholar.google.com/scholar?q=Overeem,+I,+et+al.+(2011)+Geophysical+Research+Letters+38,+L17503.">Google Scholar</a></p></div><div id="manual_ref-86" aria-flowto="reference-86-content reference-86-button"><p><span><span>Pearson</span>, <span>PN</span></span> and <span><span>Nicholas</span>, <span>CJ</span></span> (<span>2014</span>) <span>Proceedings of the National Academy of Sciences USA</span> <span>111</span>, <span>E1064</span>.<a target="_blank" aria-label="Google Scholar link for " href="https://scholar.google.com/scholar?q=Pearson,+PN+and+Nicholas,+CJ+(2014)+Proceedings+of+the+National+Academy+of+Sciences+USA+111,+E1064.">Google Scholar</a></p></div><div id="manual_ref-87" aria-flowto="reference-87-content reference-87-button"><p><span><span>Pearson</span>, <span>PN</span></span> and <span><span>Thomas</span>, <span>E</span></span> (<span>2015</span>) <span>Climate of the Past</span> <span>11</span>, <span>95</span>.<a target="_blank" aria-label="Google Scholar link for " href="https://scholar.google.com/scholar?q=Pearson,+PN+and+Thomas,+E+(2015)+Climate+of+the+Past+11,+95.">Google Scholar</a></p></div><div id="manual_ref-89" aria-flowto="reference-89-content reference-89-button"><p><span><span>Quay</span>, <span>PD</span></span>, <span><span>Tilbrook</span>, <span>B</span></span> and <span><span>Wong</span>, <span>CS</span></span> (<span>1992</span>) <span>Science</span> <span>256</span>, <span>74</span>.<a target="_blank" aria-label="Google Scholar link for " href="https://scholar.google.com/scholar?q=Quay,+PD,+Tilbrook,+B+and+Wong,+CS+(1992)+Science+256,+74.">Google Scholar</a></p></div><div id="manual_ref-92" aria-flowto="reference-92-content reference-92-button"><p><span><span>Rothman</span>, <span>DH</span></span>, et al. (<span>2014</span>) <span>Proceedings of the National Academy of Sciences USA</span> <span>111</span>, <span>5462</span>.<a target="_blank" aria-label="Google Scholar link for " href="https://scholar.google.com/scholar?q=Rothman,+DH,+et+al.+(2014)+Proceedings+of+the+National+Academy+of+Sciences+USA+111,+5462.">Google Scholar</a></p></div><div id="manual_ref-93" aria-flowto="reference-93-content reference-93-button"><p><span><span>Sahney</span>, <span>S</span></span>, <span><span>Benton</span>, <span>MJ</span></span> and <span><span>Falcon-Lang</span>, <span>HJ</span></span> (<span>2010</span>) <span>Geology</span> <span>38</span>, <span>1079</span>.<a target="_blank" aria-label="Google Scholar link for " href="https://scholar.google.com/scholar?q=Sahney,+S,+Benton,+MJ+and+Falcon-Lang,+HJ+(2010)+Geology+38,+1079.">Google Scholar</a></p></div><div id="manual_ref-94" aria-flowto="reference-94-content reference-94-button"><p><span><span>Schlanger</span>, <span>SO</span></span> and <span><span>Jenkyns</span>, <span>HC</span></span> (<span>1976</span>) <span>Geologie en Mijnbouw</span> <span>55</span>, <span>179</span>.<a target="_blank" aria-label="Google Scholar link for " href="https://scholar.google.com/scholar?q=Schlanger,+SO+and+Jenkyns,+HC+(1976)+Geologie+en+Mijnbouw+55,+179.">Google Scholar</a></p></div><div id="manual_ref-95" aria-flowto="reference-95-content reference-95-button"><p><span><span>Schmidtko</span>, <span>S</span></span>, <span><span>Stramma</span>, <span>L</span></span> and <span><span>Visbeck</span>, <span>M</span></span> (<span>2017</span>) <span>Nature</span> <span>542</span>, <span>335</span>.<a target="_blank" aria-label="Google Scholar link for " href="https://scholar.google.com/scholar?q=Schmidtko,+S,+Stramma,+L+and+Visbeck,+M+(2017)+Nature+542,+335.">Google Scholar</a></p></div><div id="manual_ref-96" aria-flowto="reference-96-content reference-96-button"><p><span><span>Schmitz</span>, <span>B</span></span>, <span><span>Pujalte</span>, <span>V</span></span> and <span><span>Nú nez-Betelu</span>, <span>K</span></span> (<span>2001</span>) <span>Palaeogeography, Palaeoclimatology, Palaeoecology</span> <span>165</span>, <span>299</span>.<a target="_blank" aria-label="Google Scholar link for " href="https://scholar.google.com/scholar?q=Schmitz,+B,+Pujalte,+V+and+N%C3%BA+nez-Betelu,+K+(2001)+Palaeogeography,+Palaeoclimatology,+Palaeoecology+165,+299.">Google Scholar</a></p></div><div id="manual_ref-97" aria-flowto="reference-97-content reference-97-button"><p><span><span>Schneider</span>, <span>A</span></span>, <span><span>Friedl</span>, <span>MA</span></span> and <span><span>Potere</span>, <span>D</span></span> (<span>2009</span>) <span>Environmental Research Letters</span> <span>4</span>, <span>044003</span>.<a target="_blank" aria-label="Google Scholar link for " href="https://scholar.google.com/scholar?q=Schneider,+A,+Friedl,+MA+and+Potere,+D+(2009)+Environmental+Research+Letters+4,+044003.">Google Scholar</a></p></div><div id="manual_ref-99" aria-flowto="reference-99-content reference-99-button"><p><span><span>Sen</span>, <span>IS</span></span> and <span><span>Peucker-Ehrenbrink</span>, <span>B</span></span> (<span>2012</span>) <span>Environmental Science &amp; Technology</span> <span>46</span>, <span>8601</span>.<a target="_blank" aria-label="Google Scholar link for " href="https://scholar.google.com/scholar?q=Sen,+IS+and+Peucker-Ehrenbrink,+B+(2012)+Environmental+Science+&amp;+Technology+46,+8601.">Google Scholar</a></p></div><div id="manual_ref-102" aria-flowto="reference-102-content reference-102-button"><p><span><span>Slotnick</span>, <span>BS</span></span>, et al. (<span>2012</span>) <span>The Journal of Geology</span> <span>120</span>, <span>487</span>.<a target="_blank" aria-label="Google Scholar link for " href="https://scholar.google.com/scholar?q=Slotnick,+BS,+et+al.+(2012)+The+Journal+of+Geology+120,+487.">Google Scholar</a></p></div><div id="manual_ref-104" aria-flowto="reference-104-content reference-104-button"><p><span><span>Smith</span>, <span>KT</span></span> (<span>2009</span>) <span>Journal of Systematic Palaeontology</span> <span>7</span>, <span>299</span>.<a target="_blank" aria-label="Google Scholar link for " href="https://scholar.google.com/scholar?q=Smith,+KT+(2009)+Journal+of+Systematic+Palaeontology+7,+299.">Google Scholar</a></p></div><div id="manual_ref-105" aria-flowto="reference-105-content reference-105-button"><p><span><span>Soliman</span>, <span>MF</span></span>, et al. (<span>2011</span>) <span>Palaeogeography, Palaeoclimatology, Palaeoecology</span> <span>310</span>, <span>365</span>.<a target="_blank" aria-label="Google Scholar link for " href="https://scholar.google.com/scholar?q=Soliman,+MF,+et+al.+(2011)+Palaeogeography,+Palaeoclimatology,+Palaeoecology+310,+365.">Google Scholar</a></p></div><div id="manual_ref-106" aria-flowto="reference-106-content reference-106-button"><p><span><span>Storey</span>, <span>M</span></span>, <span><span>Duncan</span>, <span>RA</span></span> and <span><span>Swisher</span>, <span>CC</span></span> (<span>2007</span>) <span>Science</span> <span>316</span>, <span>587</span>.<a target="_blank" aria-label="Google Scholar link for " href="https://scholar.google.com/scholar?q=Storey,+M,+Duncan,+RA+and+Swisher,+CC+(2007)+Science+316,+587.">Google Scholar</a></p></div><div id="manual_ref-107" aria-flowto="reference-107-content reference-107-button"><p><span><span>Svensen</span>, <span>H</span></span>, et al. (<span>2009</span>) <span>Earth and Planetary Science Letters</span> <span>277</span>, <span>490</span>.<a target="_blank" aria-label="Google Scholar link for " href="https://scholar.google.com/scholar?q=Svensen,+H,+et+al.+(2009)+Earth+and+Planetary+Science+Letters+277,+490.">Google Scholar</a></p></div><div id="manual_ref-109" aria-flowto="reference-109-content reference-109-button"><p><span><span>Tekman</span>, <span>MB</span></span>, <span><span>Krumpen</span>, <span>T</span></span> and <span><span>Bergmann</span>, <span>M</span></span> (<span>2017</span>) <span>Deep Sea Research Part I: Oceanographic Research Papers</span> <span>120</span>, <span>88</span>.<a target="_blank" aria-label="Google Scholar link for " href="https://scholar.google.com/scholar?q=Tekman,+MB,+Krumpen,+T+and+Bergmann,+M+(2017)+Deep+Sea+Research+Part+I:+Oceanographic+Research+Papers+120,+88.">Google Scholar</a></p></div><div id="manual_ref-110" aria-flowto="reference-110-content reference-110-button"><p><span><span>Tripati</span>, <span>AK</span></span> and <span><span>Elderfield</span>, <span>H</span></span> (<span>2004</span>) <span>Geochemistry, Geophysics, Geosystems</span> <span>5</span>, EID: Q02006.<a target="_blank" aria-label="Google Scholar link for " href="https://scholar.google.com/scholar?q=Tripati,+AK+and+Elderfield,+H+(2004)+Geochemistry,+Geophysics,+Geosystems+5,+EID:+Q02006.">Google Scholar</a></p></div><div id="manual_ref-111" aria-flowto="reference-111-content reference-111-button"><p><span><span>Turchyn</span>, <span>AV</span></span>, et al. (<span>2009</span>) <span>Earth and Planetary Science Letters</span> <span>285</span>, <span>115</span>.<a target="_blank" aria-label="Google Scholar link for " href="https://scholar.google.com/scholar?q=Turchyn,+AV,+et+al.+(2009)+Earth+and+Planetary+Science+Letters+285,+115.">Google Scholar</a></p></div><div id="manual_ref-112" aria-flowto="reference-112-content reference-112-button"><p><span><span>Tyrrell</span>, <span>T</span></span> (<span>2011</span>) <span>Philosophical Transactions of the Royal Society A: Mathematical, Physical and Engineering Sciences</span> <span>369</span>, <span>887</span>.<a target="_blank" aria-label="Google Scholar link for " href="https://scholar.google.com/scholar?q=Tyrrell,+T+(2011)+Philosophical+Transactions+of+the+Royal+Society+A:+Mathematical,+Physical+and+Engineering+Sciences+369,+887.">Google Scholar</a></p></div><div id="manual_ref-114" aria-flowto="reference-114-content reference-114-button"><p><span><span>Vellekoop</span>, <span>J</span></span>, et al. (<span>2014</span>) <span>Proceedings of the National Academy of Sciences USA</span> <span>111</span>, <span>7537</span>.<a target="_blank" aria-label="Google Scholar link for " href="https://scholar.google.com/scholar?q=Vellekoop,+J,+et+al.+(2014)+Proceedings+of+the+National+Academy+of+Sciences+USA+111,+7537.">Google Scholar</a></p></div><div id="manual_ref-115" aria-flowto="reference-115-content reference-115-button"><p><span><span>Vleeschouwer</span>, <span>DD</span></span>, et al. (<span>2013</span>) <span>Earth and Planetary Science Letters</span> <span>365</span>, <span>25</span>.<a target="_blank" aria-label="Google Scholar link for " href="https://scholar.google.com/scholar?q=Vleeschouwer,+DD,+et+al.+(2013)+Earth+and+Planetary+Science+Letters+365,+25.">Google Scholar</a></p></div><div id="manual_ref-116" aria-flowto="reference-116-content reference-116-button"><p><span><span>Wallner</span>, <span>A</span></span>, et al. (<span>2015</span>) <span>Nature Communications</span> <span>6</span>, <span>5956</span>.<a target="_blank" aria-label="Google Scholar link for " href="https://scholar.google.com/scholar?q=Wallner,+A,+et+al.+(2015)+Nature+Communications+6,+5956.">Google Scholar</a></p></div><div id="manual_ref-117" aria-flowto="reference-117-content reference-117-button"><p><span><span>Waters</span>, <span>C</span></span>, et al. (<span>2014</span>) <span>A Stratigraphical Basis for the Anthropocene</span>, GSL Special Publications No. 395. <span>London</span>: <span>Geological Society</span>.<a target="_blank" aria-label="Google Scholar link for " href="https://scholar.google.com/scholar?q=Waters,+C,+et+al.+(2014)+A+Stratigraphical+Basis+for+the+Anthropocene,+GSL+Special+Publications+No.+395.+London:+Geological+Society.">Google Scholar</a></p></div><div id="manual_ref-118" aria-flowto="reference-118-content reference-118-button"><p><span><span>Way</span>, <span>MJ</span></span>, et al. (<span>2016</span>) <span>Geophysical Research Letters</span> <span>43</span>, <span>8376</span>.<a target="_blank" aria-label="Google Scholar link for " href="https://scholar.google.com/scholar?q=Way,+MJ,+et+al.+(2016)+Geophysical+Research+Letters+43,+8376.">Google Scholar</a></p></div><div id="manual_ref-120" aria-flowto="reference-120-content reference-120-button"><p><span><span>Wright</span>, <span>JD</span></span> and <span><span>Schaller</span>, <span>MF</span></span> (<span>2013</span>) <span>Proceedings of the National Academy of Sciences USA</span> <span>110</span>, <span>15908</span>.<a target="_blank" aria-label="Google Scholar link for " href="https://scholar.google.com/scholar?q=Wright,+JD+and+Schaller,+MF+(2013)+Proceedings+of+the+National+Academy+of+Sciences+USA+110,+15908.">Google Scholar</a></p></div><div id="manual_ref-122" aria-flowto="reference-122-content reference-122-button"><p><span><span>Zachos</span>, <span>JC</span></span>, et al. (<span>2001</span>) <span>Eos Transactions AGU Fall Meeting Supplement</span> <span>82</span>, <span>767</span>.<a target="_blank" aria-label="Google Scholar link for " href="https://scholar.google.com/scholar?q=Zachos,+JC,+et+al.+(2001)+Eos+Transactions+AGU+Fall+Meeting+Supplement+82,+767.">Google Scholar</a></p></div><div id="manual_ref-123" aria-flowto="reference-123-content reference-123-button"><p><span><span>Zalasiewicz</span>, <span>J</span></span> (<span>2009</span>) <span>Earth Without Us</span>. <span>Oxford, UK</span>: <span>Oxford University Press</span>.<a target="_blank" aria-label="Google Scholar link for " href="https://scholar.google.com/scholar?q=Zalasiewicz,+J+(2009)+Earth+Without+Us.+Oxford,+UK:+Oxford+University+Press.">Google Scholar</a></p></div><div id="manual_ref-124" aria-flowto="reference-124-content reference-124-button"><p><span><span>Zalasiewicz</span>, <span>J</span></span>, <span><span>Kryza</span>, <span>R</span></span> and <span><span>Williams</span>, <span>M</span></span> (<span>2013</span>) <span>Geological Society, London, Special Publications</span> <span>395</span>, <span>109</span>.<a target="_blank" aria-label="Google Scholar link for " href="https://scholar.google.com/scholar?q=Zalasiewicz,+J,+Kryza,+R+and+Williams,+M+(2013)+Geological+Society,+London,+Special+Publications+395,+109.">Google Scholar</a></p></div><div id="manual_ref-125" aria-flowto="reference-125-content reference-125-button"><p><span><span>Zalasiewicz</span>, <span>JN</span></span>, et al. (<span>2017</span>) <span>Newsletters on Stratigraphy</span> <span>50</span>, <span>205</span>–<span>226</span>.<a target="_blank" aria-label="Google Scholar link for " href="https://scholar.google.com/scholar?q=Zalasiewicz,+JN,+et+al.+(2017)+Newsletters+on+Stratigraphy+50,+205%E2%80%93226.">Google Scholar</a></p></div></div></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Intel, Samsung, and TSMC Demo 3D-Stacked Transistors (289 pts)]]></title>
            <link>https://spectrum.ieee.org/cfet-intel-samsung-tsmc</link>
            <guid>38668713</guid>
            <pubDate>Sat, 16 Dec 2023 23:15:55 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://spectrum.ieee.org/cfet-intel-samsung-tsmc">https://spectrum.ieee.org/cfet-intel-samsung-tsmc</a>, See on <a href="https://news.ycombinator.com/item?id=38668713">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-headline="Intel, Samsung, and TSMC Demo 3D-Stacked Transistors" data-elid="2666591700" data-post-url="https://spectrum.ieee.org/cfet-intel-samsung-tsmc" data-authors="Samuel K. Moore" data-page-title="Intel, Samsung, and TSMC Demo 3D-Stacked Transistors - IEEE Spectrum"><p>A vision for future processors with nearly double the density of transistors is beginning to take shape, now that all three  advanced chipmakers have demonstrated <a href="https://spectrum.ieee.org/3d-cmos" target="_blank">CFETS, or complementary field-effect transistors</a>. CFETs are a single structure that stacks both the types of transistors needed for CMOS logic. At the <a href="https://www.ieee-iedm.org/" target="_blank">IEEE International Electron Devices Meeting</a> this week in San Francisco, <a href="https://www.intel.com/content/www/us/en/newsroom/news/where-tomorrow-begins-intels-components-research-labs.html" target="_blank">Intel</a>, <a href="https://research.samsung.com/" target="_blank">Samsung</a>, and <a href="https://research.tsmc.com/english/index.html" target="_blank">TSMC</a> showed what progress they’ve made toward the next evolution in transistors.<br></p><p>Chip companies are transitioning from the <a href="https://spectrum.ieee.org/how-the-father-of-finfets-helped-save-moores-law" target="_blank">FinFET</a> device structure in use since 2011 to <a href="https://spectrum.ieee.org/the-nanosheet-transistor-is-the-next-and-maybe-last-step-in-moores-law" target="_blank">nanosheet</a>, or gate-all-around, transistors. The names reflect the basic structure of the <a href="https://spectrum.ieee.org/tag/transistor">transistor</a>. In the FinFET, the gate controls the flow of current through a vertical silicon fin. In the nanosheet device, that fin is cut into a set of ribbons, each of which is surrounded by the gate. The CFET essentially takes a taller stack of ribbons and uses half for one device and half for the other. This device, as <a href="https://spectrum.ieee.org/tag/intel">Intel</a> engineers explained in the <a href="https://spectrum.ieee.org/3d-cmos" target="_self">December 2022 issue of <em>IEEE Spectrum</em></a>, builds the two types of transistor—nFETs and pFETs—on top of each other in a single, integrated process.</p><p>Experts estimate CFETs to roll out commercially seven to ten years from now, but there is still a lot of work before they are ready.</p><h2>Intel’s inverter</h2><p>Intel was earliest of the three to demonstrate the CFET, <a href="https://spectrum.ieee.org/intels-stacked-nanosheet-transistors-could-be-the-next-step-in-moores-law" target="_self">unveiling an early version at IEDM back in 2020</a>. This time around, Intel is reporting several improvements surrounding the simplest circuit that the CFET makes, an inverter. A CMOS inverter sends the same input voltage to the gates of both devices in the stack and produces an output that is the logical inverse of the input.</p><p>“The inverter is done on a single fin,” Marko Radosavljevic, principal engineer at Intel’s components research group, told reporters ahead of the conference. “At maximum scaling, it would be 50 percent” of the size of an ordinary CMOS inverter, he said.</p><p data-rm-resized-container="25%"><img alt="A graph with three colored lines beside a colorful 3D figure of rectangular blocks." data-rm-shortcode-id="9f68c791dbc4059c0b3bfdeb15f30e90" data-rm-shortcode-name="rebelmouse-image" data-runner-src="https://spectrum.ieee.org/media-library/a-graph-with-three-colored-lines-beside-a-colorful-3d-figure-of-rectangular-blocks.jpg?id=50817352&amp;width=980" height="451" id="d43dd" lazy-loadable="true" src="https://spectrum.ieee.org/media-library/a-graph-with-three-colored-lines-beside-a-colorful-3d-figure-of-rectangular-blocks.jpg?id=50817352&amp;width=980" width="452"><small placeholder="Add Photo Caption...">Intel’s inverter circuits depend on a new way of connecting the top and bottom transistors [yellow] and on contacting one of them from beneath the silicon [grey]</small><small placeholder="Add Photo Credit...">Intel</small></p><p>The hitch is that squeezing in all the interconnects needed to make that two-transistor stack into an inverter circuit eats away at the area advantage. To keep things tight, Intel tried to remove some of the congestion involved in connecting to the stacked device. In today’s transistors, all the connections come from above the device itself. But later this year, Intel is deploying a technology called <a href="https://spectrum.ieee.org/backside-power-delivery" target="_blank">backside power delivery</a> that allows interconnects to exist both above and below the surface of the silicon. Using that technology to contact the bottom transistor from below instead of from above significantly simplified the circuit. The resulting inverter had a density quality called contacted poly pitch (CPP, essentially the minimum distance from one transistor gate to the next) of 60 nanometers. Today’s 5 nm node chips have a CPP of about 50 nm.</p><p>Additionally, Intel improved the CFET stack’s electrical characteristics by increasing the number of nanosheets per device from two to three, decreasing the separation between the two devices from 50 nm to 30 nm, and using an improved geometry for connecting parts of the device.</p><h2>Samsung’s secret sauce</h2><p>Samsung went even smaller than Intel, showing results for 48-nm and 45-nm contacted poly pitch (CPP), compared to Intel’s 60 nm, though these were for individual devices, not complete inverters. Although there was some performance degradation in the smaller of Samsung’s two prototype CFETs, it wasn’t much, and the company’s researchers believe manufacturing process optimization will take care of it.</p><p>Crucial to Samsung’s success was the ability to electrically isolate the sources and drains of the stacked pFET and nFET devices. Without adequate isolation, the device, which Samsung calls a 3D stacked FET (3DSFET), will leak current. A key step to achieving that isolation was swapping an etching step involving wet chemicals with a new kind of dry etch. That led to an 80 percent boost in the yield of good devices.</p><p>Like Intel, Samsung contacted the bottom of the device from beneath the silicon to save space. However, the Korean chipmaker differed from the American one by using a single nanosheet in each of the paired devices, instead of Intel’s three. According to its researchers, increasing the number of nanosheets will enhance the CFET’s performance.</p><h2>TSMC takes its shot</h2><p>Like Samsung, TSMC too managed to get to an industrially-relevant pitch of 48 nm. Its device’s distinctions included a new way to form a dielectric layer between the top and bottom devices to keep them isolated. Nanosheets are generally formed from alternating layers of silicon and silicon germanium. At the appropriate step in the process, a silicon-germanium specific etching method removes that material, releasing the silicon nanowires. For the layer destined to isolate the two device from each other, TSMC used silicon germanium with an unusually high fraction of germanium, knowing that it would etch away faster than the other SiGe layers. That way the isolation layer could be built several steps before releasing the silicon nanowires.</p></div></div>]]></description>
        </item>
    </channel>
</rss>