<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0">
    <channel>
        <title>HN100 - Readable Contents</title>
        <link>https://hn.algolia.com/api/v1/search_by_date?tags=%28story,poll%29&amp;numericFilters=points%3E100</link>
        <description>Uses Readability to add bodies to the RSS feed</description>
        <lastBuildDate>Wed, 11 Jun 2025 19:30:02 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>en</language>
        <item>
            <title><![CDATA[The Beach Boys' Brian Wilson Dies at 82 (264 pts)]]></title>
            <link>https://pitchfork.com/news/the-beach-boys-brian-wilson-dies-at-82/</link>
            <guid>44249467</guid>
            <pubDate>Wed, 11 Jun 2025 16:52:49 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://pitchfork.com/news/the-beach-boys-brian-wilson-dies-at-82/">https://pitchfork.com/news/the-beach-boys-brian-wilson-dies-at-82/</a>, See on <a href="https://news.ycombinator.com/item?id=44249467">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-testid="ArticlePageChunks"><div data-journey-hook="client-content" data-testid="BodyWrapper"><p><a href="https://pitchfork.com/artists/4651-brian-wilson/">Brian Wilson</a>, the co-founder and primary songwriter of <a href="https://pitchfork.com/artists/310-the-beach-boys/">the Beach Boys</a>,&nbsp;has died, his family <a data-offer-url="https://www.instagram.com/p/DKxHMogupsf/" data-event-click="{&quot;element&quot;:&quot;ExternalLink&quot;,&quot;outgoingURL&quot;:&quot;https://www.instagram.com/p/DKxHMogupsf/&quot;}" href="https://www.instagram.com/p/DKxHMogupsf/" rel="nofollow noopener" target="_blank">announced</a>. While an official cause of death was not disclosed, the beloved musical auteur, who helped pioneer the studio-as-instrument, influencing generations of musicians in pop and beyond, was <a href="https://pitchfork.com/news/the-beach-boys-brian-wilson-living-with-neurocognitive-disorder-family-proposes-conservatorship/" target="_blank">revealed</a> in early 2024 to be living with a neurocognitive disorder akin to dementia. Wilson was 82 years old.</p><p>“We are heartbroken to announce that our beloved father Brian Wilson has passed away,” his family wrote in a statement shared on Wilson’s official <a data-offer-url="https://www.facebook.com/officialbrianwilson/posts/pfbid0Xp9R8CqoJwQwiKryetbhrLgu5KpXgsr2da5QEsFieP9zSdSy9nRE3MgyYoUCEbSYl" data-event-click="{&quot;element&quot;:&quot;ExternalLink&quot;,&quot;outgoingURL&quot;:&quot;https://www.facebook.com/officialbrianwilson/posts/pfbid0Xp9R8CqoJwQwiKryetbhrLgu5KpXgsr2da5QEsFieP9zSdSy9nRE3MgyYoUCEbSYl&quot;}" href="https://www.facebook.com/officialbrianwilson/posts/pfbid0Xp9R8CqoJwQwiKryetbhrLgu5KpXgsr2da5QEsFieP9zSdSy9nRE3MgyYoUCEbSYl" rel="nofollow noopener" target="_blank">Facebook</a>. “We are at a loss for words right now. Please respect our privacy at this time as our family is grieving. We realize that we are sharing our grief with the world. Love &amp; Mercy.”</p><p>Born in Inglewood, California, Wilson formed the band—then called the Pendletones—as a teenager with his brothers Dennis and Carl, their cousin Mike Love, and high school friend Al Jardine. Their first song, “Surfin’,” was released by Candix Records, who changed the band’s name to the Beach Boys without the members’ permission. A year later, the band&nbsp;signed with Capitol&nbsp;to&nbsp;release its debut,&nbsp;<em>Surfin’ Safari</em>; the following year, “Surfin’ U.S.A.” became the Beach Boys’ first U.S. Top 10 single. In&nbsp;1963, the band released three albums:&nbsp;<em>Surfin’ U.S.A.</em>, <em>Surfer Girl</em>, and <em>Little Deuce Coupe.</em> By then, Wilson had started his career as a producer for other musicians. He worked with Jan and Dean, the Castellas, Donna Loren, Sharon Marie, and others.</p><p>In 1964,&nbsp;Brian Wilson decided to stop&nbsp;touring with the Beach Boys after&nbsp;experiencing a panic attack due to the band’s heavy schedule. He focused on his production, and in 1965&nbsp;started work on the landmark experimental pop album&nbsp;<a href="https://pitchfork.com/features/article/9870-the-beach-boys-pet-sounds-celebrates-its-50th-anniversary-artists-pay-tribute-to-the-eternal-teenage-symphony/"><em>Pet Sounds</em></a>.&nbsp;Wilson was the mastermind behind the <em>Pet Sounds</em> sessions, working alongside famed studio musicians the&nbsp;Wrecking Crew. At the time of its release, <em>Pet Sounds</em> was considered a relative commercial flop and critical failure. In 2004, it was inducted&nbsp;into the Library of Congress’ National Recording Registry due to its cultural, historical, and aesthetic significance.</p><p>“I wanted to grow musically, so I experimented,” Wilson told the <a data-offer-url="https://hbr.org/2016/12/brian-wilson" data-event-click="{&quot;element&quot;:&quot;ExternalLink&quot;,&quot;outgoingURL&quot;:&quot;https://hbr.org/2016/12/brian-wilson&quot;}" href="https://hbr.org/2016/12/brian-wilson" rel="nofollow noopener" target="_blank"><em>Harvard Business Review</em></a> in 2016. “I wasn’t the type to sit around and be satisfied with an accomplishment, especially not in the studio. And I had ideas coming into my head all the time. Many had to do with using instruments as voices and voices as instruments. I would put sounds together to create something new. Some ideas didn’t work, because they were too difficult to achieve at the time. But most did. And then I immediately moved to the next thing.”</p></div><div data-journey-hook="client-content" data-testid="BodyWrapper"><p>Wilson planned a&nbsp;follow-up album called <em><a href="https://pitchfork.com/reviews/albums/16000-the-smile-sessions/">Smile</a>,</em> which he described as a&nbsp;“teenage symphony to God,” but it was scrapped after continual delays. After the album’s cancellation in 1967, Wilson’s role in the band receded. In 1968, he entered a psychiatric hospital for treatment. In the years after the release of&nbsp;<em>Pet Sounds</em>, Wilson briefly owned and operated a health food store called the Radiant Radish. He&nbsp;continued to work with the band while struggling with drug and alcohol addiction in the ’70s.</p><p>After a family&nbsp;intervention into Wilson’s deteriorating mental and physical&nbsp;health, he became embroiled with the controversial psychologist Eugene Landy. Their decades-long treatment later became the subject of a biographical film called&nbsp;<a href="https://pitchfork.com/news/58627-paul-dano-john-cusack-star-as-brian-wilson-in-love-mercy-trailer/"><em>Love &amp; Mercy</em></a>.&nbsp;The Beach Boys&nbsp;enjoyed a brief resurgence after&nbsp;their 1977 album&nbsp;<em>Love You</em>, but&nbsp;Wilson’s health issues persisted.&nbsp;In 1982, Landy&nbsp;removed him from the band&nbsp;for intensive treatment and exercised increasing&nbsp;control over Wilson’s financial and creative endeavors, prompting Wilson to release his debut solo album in 1988.&nbsp;In 1992, following legal action from Carl Wilson and other members of the Wilson family, Landy’s psychology license was revoked and he received a restraining order from Wilson.</p><p>Wilson eventually revisited&nbsp;<em>Smile,</em> reworking the archived studio sessions with Darian Sahanaja to bring the once-scrapped project to a live concert. He released&nbsp;<em>Brian Wilson: Presents Smile</em> in 2004 to critical acclaim. He continued to release solo work in the 2000s. His last solo album was 2015’s <a href="https://pitchfork.com/news/68850-read-the-introduction-and-first-chapter-of-brian-wilsons-memoir/"><em>No Pier Pressure</em></a>, which featured contributions from Kacey Musgraves and Zooey Deschanel. He released a <a href="https://pitchfork.com/news/68850-read-the-introduction-and-first-chapter-of-brian-wilsons-memoir/">memoir</a> in 2016, and the following year shared “<a href="https://pitchfork.com/news/listen-to-brian-wilsons-previously-unreleased-song-some-sweet-day/">Some Sweet Day</a>,” a previously unreleased song recorded in the 1990s, as well as a new single called “<a href="https://pitchfork.com/news/listen-to-brian-wilsons-new-song-run-james-run/">Run James Run</a>.”</p><p>Wilson and the Beach Boys were inducted into the Rock &amp; Roll Hall of Fame in 1988. Wilson <a data-offer-url="https://www.grammy.com/grammys/artists/brian-wilson" data-event-click="{&quot;element&quot;:&quot;ExternalLink&quot;,&quot;outgoingURL&quot;:&quot;https://www.grammy.com/grammys/artists/brian-wilson&quot;}" href="https://www.grammy.com/grammys/artists/brian-wilson" rel="nofollow noopener" target="_blank">won</a> two Grammy awards (out of nine nominations), and the Beach Boys were honored in 2001 with a Grammy Lifetime Achievement Award.</p><figure data-testid="IframeEmbed"><div data-testid="IframeEmbedContainer"><p><iframe height="315" width="560" sandbox="allow-scripts allow-popups allow-same-origin allow-presentation allow-popups-to-escape-sandbox" title="Embedded Frame" data-src="https://www.youtube-nocookie.com/embed/DVUBpzlELOg" allow="autoplay *; encrypted-media *; clipboard-write; autoplay; fullscreen; picture-in-picture"></iframe></p></div></figure></div></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[V-JEPA 2 world model and new benchmarks for physical reasoning (157 pts)]]></title>
            <link>https://ai.meta.com/blog/v-jepa-2-world-model-benchmarks/</link>
            <guid>44248165</guid>
            <pubDate>Wed, 11 Jun 2025 14:43:27 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://ai.meta.com/blog/v-jepa-2-world-model-benchmarks/">https://ai.meta.com/blog/v-jepa-2-world-model-benchmarks/</a>, See on <a href="https://news.ycombinator.com/item?id=44248165">Hacker News</a></p>
Couldn't get https://ai.meta.com/blog/v-jepa-2-world-model-benchmarks/: Error: Request failed with status code 500]]></description>
        </item>
        <item>
            <title><![CDATA[Show HN: RomM – An open-source, self-hosted ROM manager and player (123 pts)]]></title>
            <link>https://github.com/rommapp/romm</link>
            <guid>44247964</guid>
            <pubDate>Wed, 11 Jun 2025 14:25:05 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/rommapp/romm">https://github.com/rommapp/romm</a>, See on <a href="https://news.ycombinator.com/item?id=44247964">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text">

<div dir="auto">
  <p><a target="_blank" rel="noopener noreferrer" href="https://github.com/rommapp/romm/blob/release/.github/resources/isotipo.png"><img src="https://github.com/rommapp/romm/raw/release/.github/resources/isotipo.png" height="180px" width="auto" alt="romm logo"></a>
  <br>
  <a target="_blank" rel="noopener noreferrer" href="https://github.com/rommapp/romm/blob/release/.github/resources/logotipo.png"><img src="https://github.com/rommapp/romm/raw/release/.github/resources/logotipo.png" height="45px" width="auto" alt="romm logotype"></a></p><p dir="auto"><h3 tabindex="-1" dir="auto">
    A beautiful, powerful, self-hosted rom manager.
  </h3><a id="user-content-----a-beautiful-powerful-self-hosted-rom-manager--" aria-label="Permalink: 
    A beautiful, powerful, self-hosted rom manager.
  " href="#----a-beautiful-powerful-self-hosted-rom-manager--"></a></p>
  
<p dir="auto"><a href="https://github.com/rommapp/romm/blob/release/LICENSE"><img src="https://camo.githubusercontent.com/7ce8206775bef5a1f8540ffbe7e28eef7e4601bbf48c009d8a38650c333e5703/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6c6963656e73652f726f6d6d6170702f726f6d6d3f7374796c653d666f722d7468652d626164676526636f6c6f723d613332643261" alt="license-badge-img" data-canonical-src="https://img.shields.io/github/license/rommapp/romm?style=for-the-badge&amp;color=a32d2a"></a>
<a href="https://github.com/rommapp/romm/releases"><img src="https://camo.githubusercontent.com/238cf16a9a6e9d8f4220f66435e0431a6d04c14beb7a44c7609d87271a35538c/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f762f72656c656173652f726f6d6d6170702f726f6d6d3f7374796c653d666f722d7468652d6261646765" alt="release-badge-img" data-canonical-src="https://img.shields.io/github/v/release/rommapp/romm?style=for-the-badge"></a>
<a href="https://hub.docker.com/r/rommapp/romm" rel="nofollow"><img src="https://camo.githubusercontent.com/626dc557351dfc46009d10fc29b5c2ed9a946f2d4b746da44ce563ee7c478f46/68747470733a2f2f696d672e736869656c64732e696f2f646f636b65722f70756c6c732f726f6d6d6170702f726f6d6d3f7374796c653d666f722d7468652d6261646765266c6162656c3d70756c6c73" alt="docker-pulls-badge-img" data-canonical-src="https://img.shields.io/docker/pulls/rommapp/romm?style=for-the-badge&amp;label=pulls"></a></p>
<p dir="auto"><a href="https://discord.gg/P5HtHnhUDH" rel="nofollow"><img src="https://camo.githubusercontent.com/ab7559905653044cd7aab1e2c5082f3b0c390a0a17632752bcd41916fe56fe76/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f646973636f72642d3732383964613f7374796c653d666f722d7468652d6261646765" alt="discord-badge-img" data-canonical-src="https://img.shields.io/badge/discord-7289da?style=for-the-badge"></a>
<a href="https://docs.romm.app/latest/" rel="nofollow"><img src="https://camo.githubusercontent.com/370bc4357fb7df07e3ef934562c5434f60da5d2f16dae7d032d13620719cc49f/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f646f63732d3733366539623f7374796c653d666f722d7468652d6261646765" alt="docs-badge-img" data-canonical-src="https://img.shields.io/badge/docs-736e9b?style=for-the-badge"></a></p>
  </div>

<p dir="auto"><h2 tabindex="-1" dir="auto">Table of Contents</h2><a id="user-content-table-of-contents" aria-label="Permalink: Table of Contents" href="#table-of-contents"></a></p>
<ul dir="auto">
<li><a href="#table-of-contents">Table of Contents</a></li>
<li><a href="#overview">Overview</a>
<ul dir="auto">
<li><a href="#features">Features</a></li>
<li><a href="#preview">Preview</a></li>
</ul>
</li>
<li><a href="#installation">Installation</a></li>
<li><a href="#contributing">Contributing</a></li>
<li><a href="#community">Community</a></li>
<li><a href="#technical-support">Technical Support</a></li>
<li><a href="#project-support">Project Support</a></li>
<li><a href="#our-friends">Our Friends</a></li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Overview</h2><a id="user-content-overview" aria-label="Permalink: Overview" href="#overview"></a></p>
<p dir="auto">RomM (ROM Manager) allows you to scan, enrich, browse and play your game collection with a clean and responsive interface. With support for multiple platforms, various naming schemes, and custom tags, RomM is a must-have for anyone who plays on emulators.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Features</h2><a id="user-content-features" aria-label="Permalink: Features" href="#features"></a></p>
<ul dir="auto">
<li>Scans and enhance your game library with metadata from <a href="https://docs.romm.app/latest/Getting-Started/Generate-API-Keys/#igdb" rel="nofollow">IGDB</a>, <a href="https://docs.romm.app/latest/Getting-Started/Generate-API-Keys/#screenscraper" rel="nofollow">Screenscraper</a> and <a href="https://docs.romm.app/latest/Getting-Started/Generate-API-Keys/#mobygames" rel="nofollow">MobyGames</a></li>
<li>Fetch custom arwork from <a href="https://docs.romm.app/latest/Getting-Started/Generate-API-Keys/#steamgriddb" rel="nofollow">SteamGridDB</a></li>
<li>Display your achievements from <a href="https://docs.romm.app/latest/Getting-Started/Generate-API-Keys/#retroachievements" rel="nofollow">Retroachievements</a></li>
<li>Metadata available for <a href="https://docs.romm.app/latest/Platforms-and-Players/Supported-Platforms/" rel="nofollow">400+ platforms</a></li>
<li>Play games directly from the browser using <a href="https://docs.romm.app/latest/Platforms-and-Players/EmulatorJS-Player/" rel="nofollow">EmulatorJS</a> and <a href="https://docs.romm.app/latest/Platforms-and-Players/RuffleRS-Player/" rel="nofollow">RuffleRS</a></li>
<li>Share your library with friends with limited access and permissions</li>
<li>Official apps for <a href="https://github.com/rommapp/playnite-plugin">Playnite</a> and <a href="https://github.com/rommapp/muos-app">muOS</a></li>
<li>Supports multi-disk games, DLCs, mods, hacks, patches, and manuals</li>
<li>Parse and filter by <a href="https://docs.romm.app/latest/Getting-Started/Folder-Structure/#tag-support" rel="nofollow">tags</a> in filenames</li>
<li>View, upload, update, and delete games from any modern web browser</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Preview</h2><a id="user-content-preview" aria-label="Permalink: Preview" href="#preview"></a></p>
<markdown-accessiblity-table><table>
<thead>
<tr>
<th>🖥 Desktop</th>
<th>📱 Mobile</th>
</tr>
</thead>
<tbody>
<tr>
<td><a target="_blank" rel="noopener noreferrer" href="https://github.com/rommapp/romm/blob/release/.github/resources/screenshots/preview-desktop.webp"><img src="https://github.com/rommapp/romm/raw/release/.github/resources/screenshots/preview-desktop.webp" alt="desktop preview"></a></td>
<td><a target="_blank" rel="noopener noreferrer" href="https://github.com/rommapp/romm/blob/release/.github/resources/screenshots/preview-mobile.webp"><img src="https://github.com/rommapp/romm/raw/release/.github/resources/screenshots/preview-mobile.webp" alt="mobile preview"></a></td>
</tr>
</tbody>
</table></markdown-accessiblity-table>
<p dir="auto"><h2 tabindex="-1" dir="auto">Installation</h2><a id="user-content-installation" aria-label="Permalink: Installation" href="#installation"></a></p>
<p dir="auto">To start using RomM, check out the <a href="https://docs.romm.app/latest/Getting-Started/Quick-Start-Guide/" rel="nofollow">Quick Start Guide</a> in the docs. If you are having issues with RomM, please review the page for <a href="https://docs.romm.app/latest/Troubleshooting/Scanning-Issues/" rel="nofollow">troubleshooting steps</a>.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Contributing</h2><a id="user-content-contributing" aria-label="Permalink: Contributing" href="#contributing"></a></p>
<p dir="auto">To contribute to RomM, please check <a href="https://github.com/rommapp/romm/blob/release/CONTRIBUTING.md">Contribution Guide</a>.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Community</h2><a id="user-content-community" aria-label="Permalink: Community" href="#community"></a></p>
<p dir="auto">Here are a few projects maintained by members of our community. Please note that the RomM team does not regularly review their source code.</p>
<ul dir="auto">
<li><a href="https://github.com/idio-sync/romm-comm">romm-comm</a>: Discord Bot by @idio-sync</li>
<li><a href="https://github.com/PeriBluGaming/DeckRommSync-Standalone">DeckRommSync</a>: SteamOS downloader and sync by @PeriBluGaming</li>
<li>CasaOS app via the <a href="https://github.com/bigbeartechworld/big-bear-casaos">BigBear App Store</a></li>
</ul>
<p dir="auto">Join us on Discord, where you can ask questions, submit ideas, get help, showcase your collection, and discuss RomM with other users.</p>
<p dir="auto"><a href="https://discord.gg/P5HtHnhUDH" rel="nofollow"><img src="https://camo.githubusercontent.com/2d55f891c5f4e62ae883d68c0c1952600da5d11d5e7968b3aeb8e8169620e21b/68747470733a2f2f696e7669646765742e737769746368626c6164652e78797a2f50354874486e68554448" alt="discord-invite-img" data-canonical-src="https://invidget.switchblade.xyz/P5HtHnhUDH"></a></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Technical Support</h2><a id="user-content-technical-support" aria-label="Permalink: Technical Support" href="#technical-support"></a></p>
<p dir="auto">If you have any issues with RomM, please <a href="https://github.com/rommapp/romm/issues/new">open an issue</a> in this repository.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Project Support</h2><a id="user-content-project-support" aria-label="Permalink: Project Support" href="#project-support"></a></p>
<p dir="auto">Consider supporting the development of this project on Open Collective.</p>
<p dir="auto"><a href="https://opencollective.com/romm" rel="nofollow"><img src="https://camo.githubusercontent.com/06822411e9fb2f082e781925defacffc39ef14fa852b74e7c05b69bfb1f128e1/68747470733a2f2f6f70656e636f6c6c6563746976652e636f6d2f726f6d6d2f646f6e6174652f627574746f6e2e706e673f636f6c6f723d626c7565" alt="oc-donate-img" data-canonical-src="https://opencollective.com/romm/donate/button.png?color=blue"></a></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Our Friends</h2><a id="user-content-our-friends" aria-label="Permalink: Our Friends" href="#our-friends"></a></p>
<p dir="auto">Here are a few projects that we think you might like:</p>
<ul dir="auto">
<li><a href="https://emulatorjs.org/" rel="nofollow">EmulatorJS</a>: An embeddable, browser-based emulator</li>
<li><a href="https://retrodeck.net/" rel="nofollow">RetroDECK</a>: Retro gaming on SteamOS and Linux</li>
<li><a href="https://es-de.org/" rel="nofollow">ES-DE Frontend</a>: Emulator frontend for Linux, macOS and Windows</li>
<li><a href="https://github.com/gaseous-project/gaseous-server">Gaseous</a>: Another ROM manager with web-based emulator</li>
<li><a href="https://github.com/JMBeresford/retrom">Retrom</a>: A centralized game library/collection management service</li>
<li><a href="https://steamgriddb.github.io/steam-rom-manager/" rel="nofollow">Steam ROM Manager</a>: An app for managing ROMs in Steam</li>
</ul>




</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Bypassing GitHub Actions policies in the dumbest way possible (101 pts)]]></title>
            <link>https://blog.yossarian.net/2025/06/11/github-actions-policies-dumb-bypass</link>
            <guid>44247881</guid>
            <pubDate>Wed, 11 Jun 2025 14:15:54 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://blog.yossarian.net/2025/06/11/github-actions-policies-dumb-bypass">https://blog.yossarian.net/2025/06/11/github-actions-policies-dumb-bypass</a>, See on <a href="https://news.ycombinator.com/item?id=44247881">Hacker News</a></p>
<div id="readability-page-1" class="page">

<h2>ENOSUCHBLOG</h2>
<h2><em>Programming, philosophy, pedaling.</em></h2>

<ul>
    <li><a href="https://blog.yossarian.net/">Home</a></li>
    <li><a href="https://blog.yossarian.net/tags">Tags</a></li>
    <li><a href="https://blog.yossarian.net/series">Series</a></li>
    <li><a href="https://blog.yossarian.net/favorites">Favorites</a></li>
    <li><a href="https://blog.yossarian.net/archive">Archive</a></li>
    
    <li><a href="https://yossarian.net/">Main Site</a></li>
    <li><a href="https://yossarian.net/til">TILs</a></li>
    
</ul>

<hr>



<h2>
  <p>
    <span><em>Jun 11, 2025</em></span>

    &nbsp; &nbsp;

    
      <span>
        Tags:
        
        
          <a href="https://blog.yossarian.net/tags#security">security</a>
        
      </span>
    

    &nbsp; &nbsp;

    
  </p>
</h2>






<hr>


<p><strong>TL;DR</strong>: GitHub Actions provides a policy mechanism for limiting the kinds of
actions and reusable workflows that can be used within a repository,
organization, or entire enterprise. Unfortunately, this mechanism is <strong>trivial
to bypass</strong>. GitHub has told me that they don’t consider
this a security issue (I disagree), so I’m publishing this post as-is.</p>

<p><img src="https://blog.yossarian.net/assets/oops-all-uses.jpg" alt="" height="400px"></p>

<h2 id="background">Background</h2>

<p>GitHub Actions is GitHub’s CI/CD offering. I’m a big fan of it, despite its
<a href="https://www.wiz.io/blog/github-actions-security-guide">spotty</a>
<a href="https://grafana.com/blog/2025/04/27/grafana-security-update-no-customer-impact-from-github-workflow-vulnerability/">security</a>
<a href="https://blog.pypi.org/posts/2024-12-11-ultralytics-attack-analysis/">track</a>
<a href="https://adnanthekhan.com/2024/12/21/cacheract-the-monster-in-your-build-cache/">record</a>.</p>

<p>Because a CI/CD offering is essentially <strong>arbitrary code execution as
a service</strong>, users are <em>expected</em> to be careful about what they allow
to run in their workflows, <em>especially</em> privileged workflows that have access
to secrets and/or can modify the repository itself. That, in effect, means
that users need to be careful about what <a href="https://docs.github.com/en/actions/sharing-automations/avoiding-duplication">actions</a> and <a href="https://docs.github.com/en/actions/using-workflows/reusing-workflows">reusable workflows</a>
they trust.</p>

<p>Like with other open source ecosystems, downstream consumers (i.e., users
of GitHub Actions) retrieve their components (i.e., action definitions)
from an essentially open index (the <a href="https://github.com/marketplace?type=actions">“Actions Marketplace”</a>​<sup id="fnref:open"><a href="#fn:open" rel="footnote" role="doc-noteref">1</a></sup>).</p>

<p>To establish trust in those components, downstream users perform all of the
normal fuzzy heuristics: they look at the number of stars, the number
of other user, recency of activity, whether the user/organization
is a “good” one, and so forth.</p>

<p>Unfortunately, this isn’t good enough along two dimensions:</p>

<ol>
  <li>
    <p>Even actions that satisfy these heuristics can be compromised.
They’re heuristics after all, not verifiable assertions of quality
or trustworthiness.</p>

    <p>The recent <a href="https://unit42.paloaltonetworks.com/github-actions-supply-chain-attack/">tj-actions attack</a> typifies this: even popular, widely-used
actions are themselves software components, with their own supply chains
(and CI/CD setups).</p>
  </li>
  <li>
    <p>This kind of acceptance scheme <em>just doesn’t scale</em>, both in terms of
human effort and system complexity: complex CI/CD setups can have dozens
(or hundreds) of workflows, each of which can contain dozens (or hundreds)
of jobs that in turn employ actions and reusable workflows.</p>

    <p>These sorts of large setups don’t necessarily have a single owner
(or even a single team) responsible for gating admission and preventing a
the introduction of unvetted actions and reusable workflows.</p>
  </li>
</ol>

<h2 id="github-actions-policies">GitHub Actions policies</h2>

<p>The problem (as stated above) is best solved by <strong>eliminating the failure
mode</strong> itself: rather than giving the system’s committers the ability to
introduce new actions and reusable workflows without sufficient review,
the system should <strong>prevent them from doing so in the first place</strong>.</p>

<p>To their credit, GitHub understands this! They have a feature called
“Actions policies<sup id="fnref:name"><a href="#fn:name" rel="footnote" role="doc-noteref">2</a></sup>” that does exactly this. From the
<a href="https://docs.github.com/en/repositories/managing-your-repositorys-settings-and-features/enabling-features-for-your-repository/managing-github-actions-settings-for-a-repository#managing-github-actions-permissions-for-your-repository">Manage GitHub Actions settings</a> documentation:</p>

<blockquote>
  <p>You can restrict workflows to use actions and reusable workflows in specific
organizations and repositories. Specified actions cannot be set to more than
1000. (sic)</p>

  <p>To restrict access to specific tags or commit SHAs of an action or reusable
workflow, use the same syntax used in the workflow to select the action or
reusable workflow.</p>

  <ul>
    <li>
      <p>For an action, the syntax is <code>OWNER/REPOSITORY@TAG-OR-SHA</code>. For example, use
<code>actions/javascript-action@v1.0.1</code> to select a tag or
<code>actions/javascript-action@a824008085750b8e136effc585c3cd6082bd575f</code> to
select a SHA. For more information, see Using pre-written building blocks
in your workflow.</p>
    </li>
    <li>
      <p>For a reusable workflow, the syntax is
<code>OWNER/REPOSITORY/PATH/FILENAME@TAG-OR-SHA</code>. For example,
<code>octo-org/another-repo/.github/workflows/workflow.yml@v1</code>. For more
information, see Reusing workflows.</p>
    </li>
  </ul>

  <p>You can use the <code>*</code> wildcard character to match patterns. For example, to allow
all actions and reusable workflows in organizations that start with <code>space-org</code>,
you can specify <code>space-org*/*</code>. To allow all actions and reusable workflows in
repositories that start with <code>octocat</code>, you can use <code>*/octocat**@*</code>. For more
information about using the <code>*</code> wildcard, see Workflow syntax for GitHub
Actions.</p>

  <p>Use <code>,</code> to separate patterns. For example, to allow <code>octocat</code> and <code>octokit</code>, you can
specify <code>octocat/*, octokit/*</code>.</p>
</blockquote>

<p>GitHub also provides special “preset” cases for this functionality, such as
allowing only actions and reusable workflows that belong to the same
organization namespace as the repository itself. Here’s what that looks
like on a dummy organization and repository of mine:</p>

<p><img src="https://blog.yossarian.net/assets/actions-policy-1.png" alt="A screen capture of the &quot;Actions permissions&quot; section"></p>

<p>…and here’s what happens when I try to violate that policy, e.g. by using
<code>actions/checkout@v4</code> in a workflow:</p>

<p><img src="https://blog.yossarian.net/assets/actions-policy-2.png" alt="The error message from the resulting policy violation"></p>

<p>This is fantastic, except that it’s trivial to bypass. Let’s see how.</p>

<h2 id="bypassing-policies">Bypassing policies</h2>

<p>To understand how we’re going to bypass this, we need to understand a few
of the building blocks underneath actions and reusable workflows.</p>

<p>In particular:</p>

<ul>
  <li>Actions and reusable workflows share the same namespace as the rest of
GitHub, i.e. <code>owner/repo</code><sup id="fnref:slug"><a href="#fn:slug" rel="footnote" role="doc-noteref">3</a></sup>;</li>
  <li>When a user writes something like <code>uses: actions/checkout@v4</code> in a workflow,
GitHub <em>resolves</em> that reference to mean “the <code>action.yml</code> file defined at
tag <code>v4</code> in the <code>actions/checkout</code> repository”;</li>
  <li><code>uses:</code> keywords can <strong>also</strong> refer to relative paths on the runner itself.
For example, <code>uses: ./</code> runs the step with the <code>action.yml</code> in the current
directory.</li>
  <li>Relative paths from the runner are <strong>not</strong> inherently part of the repository
state itself: the runner is can contain any state introduced by previous
steps within the same job.</li>
</ul>

<p>These four aspects of GitHub Actions compose together into the <strong>world’s
dumbest policy bypass</strong>: instead of doing <code>uses: actions/checkout@v4</code>,
the user can <code>git clone</code> (or otherwise fetch) the <code>actions/checkout</code>
repository into the runner’s filesystem, and then use <code>uses: ./path/to/checkout</code>
to run the very same action.</p>

<p>Here’s what that looks like in practice:</p>

<div><pre><code><table><tbody><tr><td><pre>1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
</pre></td><td><pre><span>on</span><span>:</span> <span>[</span><span>push</span><span>,</span> <span>pull_request</span><span>]</span>

<span>jobs</span><span>:</span>
  <span>test</span><span>:</span>
    <span>runs-on</span><span>:</span> <span>ubuntu-latest</span>
    <span>steps</span><span>:</span>
      <span>-</span> <span>run</span><span>:</span> <span>|</span>
          <span>mkdir -p ./tmp</span>
          <span>git clone https://github.com/actions/checkout.git ./tmp/checkout</span>

      <span>-</span> <span>uses</span><span>:</span> <span>./tmp/checkout</span>
        <span>with</span><span>:</span>
          <span>repository</span><span>:</span> <span>woodruffw/gha-hazmat</span>
          <span>path</span><span>:</span> <span>gha-hazmat</span>

      <span>-</span> <span>run</span><span>:</span> <span>ls &amp;&amp; pwd</span>

      <span>-</span> <span>run</span><span>:</span> <span>ls tmp/checkout</span>
</pre></td></tr></tbody></table></code></pre></div>

<p>(The actual <code>with:</code> block of the <code>uses: ./tmp/checkout</code> step is inconsequential
— I just used that repository for the demo, but anything would work.)</p>

<p>And naturally, it works just fine:</p>

<p><img src="https://blog.yossarian.net/assets/actions-policy-3.png" alt="A screenshot of the bypass running successfully "></p>

<h2 id="fixing-it">Fixing it?</h2>

<p>The fix for this bypass is simple, if potentially somewhat painful:
GitHub Actions could consider “local” <code>uses:</code> references to be another
category for the purpose of policies, and reject them whenever the policy
doesn’t permit them.</p>

<p>This would seal off the entire problem, since <code>uses: ./foo</code> would just stop
working. The downside is that it would potentially break existing users
of policies who also use local actions and reusable workflows, assuming
there are significant numbers of them<sup id="fnref:honest"><a href="#fn:honest" rel="footnote" role="doc-noteref">4</a></sup>.</p>

<p>The other option would be to leave it the way it is, but <strong>explicitly document</strong>
local <code>uses:</code> references as a limitation of this policy mechanism. I honestly
think this would be perfectly fine; what matters is that users<sup id="fnref:users"><a href="#fn:users" rel="footnote" role="doc-noteref">5</a></sup>
are <em>informed</em> of a feature’s limitations, not necessarily that the feature
<em>lacks</em> limitations.</p>

<h2 id="does-this-matter">Does this matter?</h2>

<p>First, I’ll couch this again: this is not exactly fancy stuff. It’s a very
dumb bypass, and I don’t think it’s critical by any means.</p>

<p>At the same time, I think this <strong>matters a great deal</strong>: ineffective
policy mechanisms are <strong>worse</strong> than missing policy mechanisms, because they provide
all of the <em>feeling</em> of security through compliance while actually
<strong>incentivizing malicious forms of compliance</strong>.</p>

<p>In this case, the maliciously complying party is almost certainly a developer
just trying to get their job done: like most other developers who encounter
an inscrutable policy restriction, they will try to hack around it such that
the policy is satisfied in name only.</p>

<p>For that reason alone I think GitHub should fix this bypass, either by
<em>actually</em> fixing it or at least documenting its limitations. Without either
of those, projects and organizations are likely to <strong>mistakenly believe</strong> that
these sorts of policies provide a <em>security boundary</em> where <strong>none in fact
exists</strong>.</p>

<hr>




<hr>


<span>
  Discussions:
  
  <a href="https://infosec.exchange/@yossarian/114665097805372166">Mastodon</a>
  
  <a href="https://bsky.app/profile/yossarian.net/post/3lrdl3j3ez52s">Bluesky</a>
  
  <a href="https://www.reddit.com/r/enosuchblog/comments/1l8th6c/bypassing_github_actions_policies_in_the_dumbest/">Reddit</a>
  
</span>
<hr>



  






</div>]]></description>
        </item>
        <item>
            <title><![CDATA[Show HN: Ikuyo a Travel Planning Web Application (178 pts)]]></title>
            <link>https://ikuyo.kenrick95.org/</link>
            <guid>44247029</guid>
            <pubDate>Wed, 11 Jun 2025 12:44:15 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://ikuyo.kenrick95.org/">https://ikuyo.kenrick95.org/</a>, See on <a href="https://news.ycombinator.com/item?id=44247029">Hacker News</a></p>
&lt;Unparsable&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[Menstrual tracking app data is gold mine for advertisers that risks women safety (213 pts)]]></title>
            <link>https://www.cam.ac.uk/research/news/menstrual-tracking-app-data-is-a-gold-mine-for-advertisers-that-risks-womens-safety-report</link>
            <guid>44246920</guid>
            <pubDate>Wed, 11 Jun 2025 12:31:15 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.cam.ac.uk/research/news/menstrual-tracking-app-data-is-a-gold-mine-for-advertisers-that-risks-womens-safety-report">https://www.cam.ac.uk/research/news/menstrual-tracking-app-data-is-a-gold-mine-for-advertisers-that-risks-womens-safety-report</a>, See on <a href="https://news.ycombinator.com/item?id=44246920">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>Cambridge researchers urge public health bodies like the NHS to provide trustworthy, research-driven alternatives to platforms driven by profit.</p>
</div><div><p>Smartphone apps that track menstrual cycles are a “gold mine” for consumer profiling, collecting information on everything from exercise, diet and medication to sexual preferences, hormone levels and contraception use.</p>

<p><a href="https://www.mctd.ac.uk/femtech-high-stakes-tracking-menstruation/">This is according to a new report</a> from the University of Cambridge’s <a href="https://www.mctd.ac.uk/">Minderoo Centre for Technology and Democracy</a>, which argues that the financial worth of this data is “vastly underestimated” by users who supply profit-driven companies with highly intimate details in a market lacking in regulation.</p>

<p>The report’s authors caution that cycle tracking app (CTA) data in the wrong hands could result in risks to job prospects, workplace monitoring, health insurance discrimination and cyberstalking – and limit access to abortion.</p>

<p>They call for better governance of the booming ‘femtech’ industry to protect users when their data is sold at scale, arguing that apps must provide clear consent options rather than all-or-nothing data collection, and urge public health bodies to launch alternatives to commercial CTAs.</p>

<p>“Menstrual cycle tracking apps are presented as empowering women and addressing the gender health gap,” said Dr Stefanie Felsberger, lead author of the report from Cambridge’s Minderoo Centre. “Yet the business model behind their services rests on commercial use, selling user data and insights to third parties for profit.”</p>

<p>“There are real and frightening privacy and safety risks to women as a result of the commodification of the data collected by cycle tracking app companies.”</p>

<p>As most cycle tracking apps are targeted at women aiming to get pregnant, the download data alone is of huge commercial value, say researchers, as – other than home buying – no life event is linked to such dramatic shifts in consumer behaviour.</p>

<p>In fact, data on pregnancy is believed to be over two hundred times more valuable than data on age, gender or location for targeted advertising. The report points out that period tracking could also be used to target women at different points in their cycle. For example, the oestrogen or ‘mating’ phase could see an increase in cosmetics adverts.</p>

<p>Just the three most popular apps had estimated global download figures of a quarter of a billion in 2024. So-called femtech – digital products focused on women’s health and wellbeing – is estimated to reach over US$60 billion by 2027, with cycle tracking apps making up half of this market.</p>

<p>With such intense demand for period tracking, the report argues that the UK’s National Health Service (NHS) should develop its own transparent and trustworthy app to rival those from private companies, with apps allowing permission for data to be used in valid medical research.</p>

<p>“The UK is ideally positioned to solve the question of access to menstrual data for researchers, as well as privacy and data commodification concerns, by developing an NHS app to track menstrual cycles,” said Felsberger, who points out that Planned Parenthood in the US already has its own app, but the UK lacks an equivalent.</p>

<p>“Apps that are situated within public healthcare systems, and not driven primarily by profit, will mitigate privacy violations, provide much-needed data on reproductive health, and give people more agency over how their menstrual data is used.”</p>

<p>“The use of cycle tracking apps is at an all-time high,” said Prof Gina Neff, Executive Director of Cambridge’s Minderoo Centre. “Women deserve better than to have their menstrual tracking data treated as consumer data, but there is a different possible future.”</p>

<p>“Researchers could use this data to help answer questions about women’s health. Care providers could use this data for important information about their patients’ health. Women could get meaningful insights that they are searching for,” Neff said.</p>

<p>In the UK and EU, period tracking data is considered “special category”, as with that on genetics or ethnicity, and has more legal safeguarding. However, the report highlights how in the UK, apps designed for women's health have been used to charge women for illegally accessing abortion services</p>

<p>In the US, data about menstrual cycles has been collected by officials in an attempt to undermine abortion access. Despite this, data from CTAs are regulated simply as “general wellness” and granted no special protections.</p>

<p>“Menstrual tracking data is being used to control people’s reproductive lives,” said Felsberger. “It should not be left in the hands of private companies.”</p>

<p>Investigations by media, non-profit, and consumer groups have revealed CTAs sharing data with third parties ranging from advertisers and data brokers to tech giants such as Facebook and Google.</p>

<p>The report cites work <a href="http://chrome-extension//efaidnbmnnnibpcajpcglclefindmkaj/https:/www.privacyinternational.org/sites/default/files/2025-05/period%20tracking%20with%20recs.pdf">published last month</a> from Privacy International showing that, while the major CTA companies have updated their approach to data sharing, device information is still collected in the UK and US with “no meaningful consent”.</p>

<p>Despite data protection improvements, the report suggests that user information is still shared with third parties such as cloud-based delivery networks that move the data around, and outside developers contracted to handle app functionalities.</p>

<p>At the very least, commercial apps could include delete buttons, says Felsberger, allowing users to erase data in the app as well as the company servers, helping protect against situations – from legal to medical – where data could be used against them.&nbsp;</p>

<p>“Menstrual tracking in the US should be classed as medical data,” said Felsberger. “In the UK and EU, where this data is already afforded special category status, more focus needs to be placed on enforcing existing regulation.”</p>

<p>The report stresses the need to improve public awareness and digital literacy around period tracking. The researchers argue that schools should educate students on medical data apps and privacy, so young people are less vulnerable to health hoaxes.</p>

<p>The report ‘<a href="https://www.mctd.ac.uk/femtech-high-stakes-tracking-menstruation/">The High Stakes of Tracking Menstruation</a>’ is authored by Dr Stefanie Felsberger with a foreword by Professor Gina Neff and published by the Minderoo Centre for Technology and Democracy (MCTD).</p>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Firefox OS's story from a Mozilla insider not working on the project (2024) (114 pts)]]></title>
            <link>https://ludovic.hirlimann.net/2024/01/firefox-oss-story-from-mozila-insider.html</link>
            <guid>44246518</guid>
            <pubDate>Wed, 11 Jun 2025 11:36:47 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://ludovic.hirlimann.net/2024/01/firefox-oss-story-from-mozila-insider.html">https://ludovic.hirlimann.net/2024/01/firefox-oss-story-from-mozila-insider.html</a>, See on <a href="https://news.ycombinator.com/item?id=44246518">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
<h3>
Firefox OS's story from a mozilla insider not working on the project
</h3>
</div><div id="post-body-8603683558975605482">
<p><a href="https://pix.diaspodon.fr/storage/m/_v2/41/3b17271c9-2ea36d/f8WqF3f8DYaw/3tH3eMdS4oPNr889fLN3ymcy2eybKJK1XXoQiTWh.jpg"><img data-original-height="800" data-original-width="600" height="800" src="https://pix.diaspodon.fr/storage/m/_v2/41/3b17271c9-2ea36d/f8WqF3f8DYaw/3tH3eMdS4oPNr889fLN3ymcy2eybKJK1XXoQiTWh.jpg" width="600"></a></p><p>I clearly remember, but can't date it. I was working for Mozilla messaging at the time (<a href="https://en.wikipedia.org/wiki/Mozilla_Messaging">momo</a>), being the QA lead for Thunderbird. It was at the end of one of the Mozilla All-hands, maybe in 2011 or 2012. At one of the ending keynotes, we were introduced to Boot 2 Gecko. A hack that would let US - Mozilla own the platform to run a mobile browser on. At the time, the iPhone was going strong and Google was trying to catch up with Android. <a href="https://en.wikipedia.org/wiki/MeeGo">MeeGo</a> had been in development at Nokia for a while but was going nowhere even when Intel tried to help. <a href="https://en.wikipedia.org/wiki/BlackBerry">Blackberry</a> was slowly starting to die.<br></p><p>In the Silicon Valley everything was about mobile, mobile, mobile and the emerging South Easter Asian market, where people would skip computers and use smartphones to join the internet revolution. We were struggling with Chrome and the massive investment by Google to take market share. Our Firefox port on Android was having loads of issues. We were denied by Apple's policies to be present on iPhones. I was running <a href="https://play.google.com/store/search?q=firefox%20nightly&amp;c=apps">Nightly</a> on my then <a href="https://en.wikipedia.org/wiki/Samsung_Galaxy_S_series">Galaxy S Samsung</a> android powered phone. As android was open source, the idea to use it as a base for a complete phone OS that would make the web a platform emerged has an idea. At that time, Mozilla consisted of around 600 employees, all working on Firefox on the desktop. Most of our <a href="https://moztw.org/~petercpg/asknot/#!/progornoprog/softtest">huge community</a> was helping the project, making it available in many languages (Like <a href="https://en.wikipedia.org/wiki/West_Frisian_language">https://en.wikipedia.org/wiki/West_Frisian_language</a>), helping with some marketing efforts too.</p><p>B2G, or Android's version, were not Mozilla first effort to be present on Mobile. The first effort I'm aware of is <a href="https://www-archive.mozilla.org/projects/minimo/">Minimo</a>, who was targeting Palm like handheld devices.</p><p>As I said above, Mozilla's management was really afraid to miss the mobile revolution. They hired someone from the mobile industry to run the company, this led to some culture changes : no more a flat org, but a pyramidal one with middle managers. Culture became way less engineering centric, and started being a bit more top -&gt; down. Focus was now solely on B2G. This impacted my work, because it was decided that Thunderbird had no future (and no business model to support its development). That meant I changed roles in Mozilla and joined the IT organization, as I wanted to see the server side of Email (this was long before Mozilla switched to Google workplace for email). I always felt that B2G to be renamed Firefox OS, killed the team I was part of, that was working on TB. I have no insight on who made the decision and why, but that how I felt. This made me not liking B2G.</p><p>Besides becoming more like a normal company, the new CEO grew the size of our teams, added project managers, Sales people, to make sure B2G would reach a huge audience. We started making deals with phone carriers, and each of these had different requirements. We also made deals with phone makers, our Taiwan office was set up to be as close a possible as the Chinese phone makers - so we'd be at the edge of the mobile phone market. Having different deals owner made the life of the project complicated, as each of our partners had different sets of requirements for their go-to market plans. The teams were busy implementing X for partner X and Y for partner Y. Sometimes X and Y would conflict :-( With the rapid development pace, quality was omitted to reach launch deadlines. As B2G was the priority, this also meant that Firefox desktop was neglected and was slowly loosing ground to chrome. Not that we could compare the size of the devs teams, but as nobody in upper management cared about desktop, it was there, that's it. Remember that Firefox desktop was the cash machine that paid for all the rest. Without Desktop, no revenues. Then all of a sudden, by the end of 2015, Mozilla pulled the plug on B2G and got back and focused on its source of revenue, desktop. By then Mozilla had doubled in size, reaching almost 1200 employees.<br></p><p>I first got to play with Firefox OS back in 2012 when I switched to IT.</p><p><a href="https://pix.diaspodon.fr/storage/m/_v2/41/3b17271c9-2ea36d/v5lJOtoNXe9y/FzVj2Wx79IoiHYoyDrevGSbMf5OElb2geO1cnYd1.jpg"><img data-original-height="600" data-original-width="800" height="589" src="https://pix.diaspodon.fr/storage/m/_v2/41/3b17271c9-2ea36d/v5lJOtoNXe9y/FzVj2Wx79IoiHYoyDrevGSbMf5OElb2geO1cnYd1.jpg" width="611"></a></p><p>This was a <a href="https://www.devicesfaq.com/en/characteristics/turkcell-maxiplus-5">TURKCELL MaxiPLus5</a>. This was slow and unusable. I had work needs, so I never used it. The phone was available upon request for Mozilla employees willing to try B2G out. I'm not sure many were sent or used. After that, testing B2G as an employee was complicated if you were not working on the Firefox OS team. By early 2015, someone at the great idea to <a href="https://en.wikipedia.org/wiki/Eating_your_own_dog_food">dogfood</a> the product and four hundred phones were made available to employees. I requested one. This was a <a href="https://www.gsmarena.com/sony_xperia_z3-6539.php">Sony Xperia</a> phone, it came with a protective cover, a mandatory mailing list to share your experience with it and file bugs if you could. I was finally getting interested with the product. Took it with me as a secondary phone. That summer I went to <a href="https://www.flickr.com/photos/hirlimann/albums/72157669622246115/with/28756027225">Mongolia</a> and the carrier/OS didn't work together, so had to use it over Wi-Fi. I managed to find a few bugs with the email client (don't know why, but that's where I found bugs, as well as in picture/metadata handling). I was not alone reporting bugs, they were getting fixed too. But nonetheless, Management decided the experiment was over. Well that's not completely true, it lives at <a href="https://www.kaiostech.com/">https://www.kaiostech.com/</a></p><p>With retrospect, I think B2G was a good idea - challenging Apple would also have been a good idea, as we had an internal demo of <a href="https://en.wikipedia.org/wiki/Gecko_(software)">gecko</a> powered Firefox for iOS. Owning the complete stack was the right approach. It gives you the power to have something that work nicely on the devices you support. I think the development approach we took was the wrong one. We were too in a hurry and that ended up neglecting Desktop. I believe we should have engaged potential partners way later, with a better, more finished and more QAed product. We should have grown to work on B2G, but not at the expense of our source of revenue. We should have dogfooded the product a lot more and once ready reached out to partners. And then start using our community to market the product and gain market share and all. The death of B2G, also meant the death of most of our engagement with ordinary people, known as the Mozilla community.<br></p><p>&nbsp;I probably forgot some details - I'll gladly edit If I feel that the things I have forgotten are important.</p><p>Vision from a developer who worked on the graphics stack, in French <a href="https://linuxfr.org/nodes/107897/comments/1643786">https://linuxfr.org/nodes/107897/comments/1643786</a><br></p>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Show HN: S3mini – Tiny and fast S3-compatible client, no-deps, edge-ready (179 pts)]]></title>
            <link>https://github.com/good-lly/s3mini</link>
            <guid>44245577</guid>
            <pubDate>Wed, 11 Jun 2025 08:55:47 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/good-lly/s3mini">https://github.com/good-lly/s3mini</a>, See on <a href="https://news.ycombinator.com/item?id=44245577">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><p dir="auto"><h2 tabindex="-1" dir="auto">s3mini | Tiny &amp; fast S3 client for node and edge platforms.</h2><a id="user-content-s3mini--tiny--fast-s3-client-for-node-and-edge-platforms" aria-label="Permalink: s3mini | Tiny &amp; fast S3 client for node and edge platforms." href="#s3mini--tiny--fast-s3-client-for-node-and-edge-platforms"></a></p>
<p dir="auto"><code>s3mini</code> is an ultra-lightweight Typescript client (~14 KB minified, ≈15 % more ops/s) for S3-compatible object storage. It runs on Node, Bun, Cloudflare Workers, and other edge platforms. It has been tested on Cloudflare R2, Backblaze B2, DigitalOcean Spaces, and MinIO. (No Browser support!)</p>
<p dir="auto">[<a href="https://github.com/good-lly/s3mini">github</a>]
[<a href="https://github.com/good-lly/s3mini/issues">issues</a>]
[<a href="https://www.npmjs.com/package/s3mini" rel="nofollow">npm</a>]</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Features</h2><a id="user-content-features" aria-label="Permalink: Features" href="#features"></a></p>
<ul dir="auto">
<li>🚀 Light and fast: averages ≈15 % more ops/s and only ~14 KB (minified, not gzipped).</li>
<li>🔧 Zero dependencies; supports AWS SigV4 (no pre-signed requests).</li>
<li>🟠 Works on Cloudflare Workers; ideal for edge computing, Node, and Bun (no browser support).</li>
<li>🔑 Only the essential S3 APIs—improved list, put, get, delete, and a few more.</li>
<li>📦 <strong>BYOS3</strong> — <em>Bring your own S3-compatible bucket</em> (tested on Cloudflare R2, Backblaze B2, DigitalOcean Spaces, MinIO and Garage! Ceph and AWS are in the queue).</li>
</ul>
<p dir="auto"><h4 tabindex="-1" dir="auto">Tested On</h4><a id="user-content-tested-on" aria-label="Permalink: Tested On" href="#tested-on"></a></p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/good-lly/s3mini/blob/dev/testedon.png"><img src="https://github.com/good-lly/s3mini/raw/dev/testedon.png" alt="Tested On"></a></p>
<p dir="auto">Dev:</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/bf7b30c3cfdbd3ccc409731dfcf7309229b1fc1f2eaeece077ae0163720a2d7a/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f636f6d6d69742d61637469766974792f6d2f676f6f642d6c6c792f73336d696e692f6465763f636f6c6f723d67726565656e"><img src="https://camo.githubusercontent.com/bf7b30c3cfdbd3ccc409731dfcf7309229b1fc1f2eaeece077ae0163720a2d7a/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f636f6d6d69742d61637469766974792f6d2f676f6f642d6c6c792f73336d696e692f6465763f636f6c6f723d67726565656e" alt="GitHub commit activity (branch)" data-canonical-src="https://img.shields.io/github/commit-activity/m/good-lly/s3mini/dev?color=greeen"></a>
<a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/6a746b9ad4507bf242d021923444bddc75cc1a3d306fdc55134a85e6ccba8fa8/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6973737565732f676f6f642d6c6c792f73336d696e69"><img src="https://camo.githubusercontent.com/6a746b9ad4507bf242d021923444bddc75cc1a3d306fdc55134a85e6ccba8fa8/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6973737565732f676f6f642d6c6c792f73336d696e69" alt="GitHub Issues or Pull Requests" data-canonical-src="https://img.shields.io/github/issues/good-lly/s3mini"></a>
<a href="https://github.com/good-lly/s3mini/actions/workflows/codeql.yml"><img src="https://github.com/good-lly/s3mini/actions/workflows/codeql.yml/badge.svg?branch=dev" alt="CodeQL Advanced"></a>
<a href="https://github.com/good-lly/s3mini/actions/workflows/test-e2e.yml"><img src="https://github.com/good-lly/s3mini/actions/workflows/test-e2e.yml/badge.svg?branch=dev" alt="Test:e2e(all)"></a></p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/822c31326e4a40a59e2a813db9dcbdb1bce1c5b687237c4aae892d8850a43e9b/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f676f6f642d6c6c792f73336d696e693f7374796c653d736f6369616c"><img src="https://camo.githubusercontent.com/822c31326e4a40a59e2a813db9dcbdb1bce1c5b687237c4aae892d8850a43e9b/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f676f6f642d6c6c792f73336d696e693f7374796c653d736f6369616c" alt="GitHub Repo stars" data-canonical-src="https://img.shields.io/github/stars/good-lly/s3mini?style=social"></a>
<a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/4de7585b00e2b338207229723bab399bde2b3fefa531015a529b52d874aa75fd/68747470733a2f2f696d672e736869656c64732e696f2f6e706d2f646d2f73336d696e69"><img src="https://camo.githubusercontent.com/4de7585b00e2b338207229723bab399bde2b3fefa531015a529b52d874aa75fd/68747470733a2f2f696d672e736869656c64732e696f2f6e706d2f646d2f73336d696e69" alt="NPM Downloads" data-canonical-src="https://img.shields.io/npm/dm/s3mini"></a>
<a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/863f019439a06855c998b1a1a01302f70728781442f5236090683d1793445959/68747470733a2f2f696d672e736869656c64732e696f2f6e706d2f762f73336d696e693f636f6c6f723d677265656e"><img src="https://camo.githubusercontent.com/863f019439a06855c998b1a1a01302f70728781442f5236090683d1793445959/68747470733a2f2f696d672e736869656c64732e696f2f6e706d2f762f73336d696e693f636f6c6f723d677265656e" alt="NPM Version" data-canonical-src="https://img.shields.io/npm/v/s3mini?color=green"></a>
<a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/179914f6471af92eb18a918ccc151661f36231a77b5a8665f45e294eb52428af/68747470733a2f2f696d672e736869656c64732e696f2f62756e646c656a732f73697a652f73336d696e693f636f6c6f723d677265656e"><img src="https://camo.githubusercontent.com/179914f6471af92eb18a918ccc151661f36231a77b5a8665f45e294eb52428af/68747470733a2f2f696d672e736869656c64732e696f2f62756e646c656a732f73697a652f73336d696e693f636f6c6f723d677265656e" alt="npm package minimized gzipped size" data-canonical-src="https://img.shields.io/bundlejs/size/s3mini?color=green"></a>
<a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/130a065a22021d83141baa1284b5479f286d238097574885c18c4f1a78669ae4/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6c6963656e73652f676f6f642d6c6c792f73336d696e69"><img src="https://camo.githubusercontent.com/130a065a22021d83141baa1284b5479f286d238097574885c18c4f1a78669ae4/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6c6963656e73652f676f6f642d6c6c792f73336d696e69" alt="GitHub License" data-canonical-src="https://img.shields.io/github/license/good-lly/s3mini"></a></p>
<p dir="auto"><a href="https://github.com/good-lly/s3mini/issues/"> <img src="https://camo.githubusercontent.com/b20f68f697b27055937e1f8238eec47e2cb37f5fc74960da5dfbfe40e04af19a/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f636f6e747269627574696f6e732d77656c636f6d652d627269676874677265656e2e737667" alt="Contributions welcome" data-canonical-src="https://img.shields.io/badge/contributions-welcome-brightgreen.svg"></a></p>
<p dir="auto">Performance tests was done on local Minio instance. Your results may vary depending on environment and network conditions, so take it with a grain of salt.
<a target="_blank" rel="noopener noreferrer nofollow" href="https://raw.githubusercontent.com/good-lly/s3mini/dev/performance-screenshot.png"><img src="https://raw.githubusercontent.com/good-lly/s3mini/dev/performance-screenshot.png" alt="performance-image"></a></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Table of Contents</h2><a id="user-content-table-of-contents" aria-label="Permalink: Table of Contents" href="#table-of-contents"></a></p>
<ul dir="auto">
<li><a href="#supported-ops">Supported Ops</a></li>
<li><a href="#installation">Installation</a></li>
<li><a href="#usage">Usage</a></li>
<li><a href="#security-notes">Security Notes</a></li>
<li><a href="#contributions-welcomed">💙 Contributions welcomed!</a></li>
<li><a href="#license">License</a></li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Supported Ops</h2><a id="user-content-supported-ops" aria-label="Permalink: Supported Ops" href="#supported-ops"></a></p>
<p dir="auto">The library supports a subset of S3 operations, focusing on essential features, making it suitable for environments with limited resources.</p>
<p dir="auto"><h4 tabindex="-1" dir="auto">Bucket ops</h4><a id="user-content-bucket-ops" aria-label="Permalink: Bucket ops" href="#bucket-ops"></a></p>
<ul dir="auto">
<li>✅ HeadBucket (bucketExists)</li>
<li>✅ createBucket (createBucket)</li>
</ul>
<p dir="auto"><h4 tabindex="-1" dir="auto">Objects ops</h4><a id="user-content-objects-ops" aria-label="Permalink: Objects ops" href="#objects-ops"></a></p>
<ul dir="auto">
<li>✅ ListObjectsV2 (listObjects)</li>
<li>✅ GetObject (getObject, getObjectResponse, getObjectWithETag, getObjectRaw, getObjectArrayBuffer, getObjectJSON)</li>
<li>✅ PutObject (putObject)</li>
<li>✅ DeleteObject (deleteObject)</li>
<li>✅ HeadObject (objectExists, getEtag, getContentLength)</li>
<li>✅ listMultipartUploads</li>
<li>✅ CreateMultipartUpload (getMultipartUploadId)</li>
<li>✅ completeMultipartUpload</li>
<li>✅ abortMultipartUpload</li>
<li>✅ uploadPart</li>
<li>❌ CopyObject: Not implemented (tbd)</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Installation</h2><a id="user-content-installation" aria-label="Permalink: Installation" href="#installation"></a></p>



<blockquote>
<p dir="auto"><strong><g-emoji alias="warning">⚠️</g-emoji> Environment Support Notice</strong></p>
<p dir="auto">This library is designed to run in environments like <strong>Node.js</strong>, <strong>Bun</strong>, and <strong>Cloudflare Workers</strong>. It does <strong>not support browser environments</strong> due to the use of Node.js APIs and polyfills.</p>
<p dir="auto"><strong>Cloudflare Workers:</strong> To enable built-in Node.js Crypto API, add the <code>nodejs_compat</code> compatibility flag to your Wrangler configuration file. This also enables <code>nodejs_compat_v2</code> as long as your compatibility date is <code>2024-09-23</code> or later. <a href="https://developers.cloudflare.com/workers/configuration/compatibility-dates/#nodejs-compatibility-flag" rel="nofollow">Learn more about the Node.js compatibility flag and v2</a>.</p>
</blockquote>
<p dir="auto"><h2 tabindex="-1" dir="auto">Usage</h2><a id="user-content-usage" aria-label="Permalink: Usage" href="#usage"></a></p>
<div dir="auto" data-snippet-clipboard-copy-content="import { s3mini, sanitizeETag } from 's3mini';

const s3client = new s3mini({
  accessKeyId: config.accessKeyId,
  secretAccessKey: config.secretAccessKey,
  endpoint: config.endpoint,
  region: config.region,
});

// Basic bucket ops
let exists: boolean = false;
try {
  // Check if the bucket exists
  exists = await s3client.bucketExists();
} catch (err) {
  throw new Error(`Failed bucketExists() call, wrong credentials maybe: ${err.message}`);
}
if (!exists) {
  // Create the bucket based on the endpoint bucket name
  await s3client.createBucket();
}

// Basic object ops
// key is the name of the object in the bucket
const smallObjectKey: string = 'small-object.txt';
// content is the data you want to store in the object
// it can be a string or Buffer (recommended for large objects)
const smallObjectContent: string = 'Hello, world!';

// check if the object exists
const objectExists: boolean = await s3client.objectExists(smallObjectKey);
let etag: string | null = null;
if (!objectExists) {
  // put/upload the object, content can be a string or Buffer
  // to add object into &quot;folder&quot;, use &quot;folder/filename.txt&quot; as key
  const resp: Response = await s3client.putObject(smallObjectKey, smallObjectContent);
  // you can also get etag via getEtag method
  // const etag: string = await s3client.getEtag(smallObjectKey);
  etag = sanitizeETag(resp.headers.get('etag'));
}

// get the object, null if not found
const objectData: string | null = await s3client.getObject(smallObjectKey);
console.log('Object data:', objectData);

// get the object with ETag, null if not found
const response2: Response = await s3mini.getObject(smallObjectKey, { 'if-none-match': etag });
if (response2) {
  // ETag changed so we can get the object data and new ETag
  // Note: ETag is not guaranteed to be the same as the MD5 hash of the object
  // ETag is sanitized to remove quotes
  const etag2: string = sanitizeETag(response2.headers.get('etag'));
  console.log('Object data with ETag:', response2.body, 'ETag:', etag2);
} else {
  console.log('Object not found or ETag does match.');
}

// list objects in the bucket, null if bucket is empty
// Note: listObjects uses listObjectsV2 API and iterate over all pages
// so it will return all objects in the bucket which can take a while
// If you want to limit the number of objects returned, use the maxKeys option
// If you want to list objects in a specific &quot;folder&quot;, use &quot;folder/&quot; as prefix
// Example s3client.listObjects({&quot;/&quot; &quot;myfolder/&quot;})
const list: object[] | null = await s3client.listObjects();
if (list) {
  console.log('List of objects:', list);
} else {
  console.log('No objects found in the bucket.');
}

// delete the object
const wasDeleted: boolean = await s3client.deleteObject(smallObjectKey);

// Multipart upload
const multipartKey = 'multipart-object.txt';
const large_buffer = new Uint8Array(1024 * 1024 * 15); // 15 MB buffer
const partSize = 8 * 1024 * 1024; // 8 MB
const totalParts = Math.ceil(large_buffer.length / partSize);
// Beware! This will return always a new uploadId
// if you want to use the same uploadId, you need to store it somewhere
const uploadId = await s3client.getMultipartUploadId(multipartKey);
const uploadPromises = [];
for (let i = 0; i < totalParts; i++) {
  const partBuffer = large_buffer.subarray(i * partSize, (i + 1) * partSize);
  // upload each part
  // Note: uploadPart returns a promise, so you can use Promise.all to upload all parts in parallel
  // but be careful with the number of parallel uploads, it can cause throttling
  // or errors if you upload too many parts at once
  // You can also use generator functions to upload parts in batches
  uploadPromises.push(s3client.uploadPart(multipartKey, uploadId, partBuffer, i + 1));
}
const uploadResponses = await Promise.all(uploadPromises);
const parts = uploadResponses.map((response, index) => ({
  partNumber: index + 1,
  etag: response.etag,
}));
// Complete the multipart upload
const completeResponse = await s3client.completeMultipartUpload(multipartKey, uploadId, parts);
const completeEtag = completeResponse.etag;

// List multipart uploads
// returns object with uploadId and key
const multipartUploads: object = await s3client.listMultipartUploads();
// Abort the multipart upload
const abortResponse = await s3client.abortMultipartUpload(multipartUploads.key, multipartUploads.uploadId);

// Multipart download
// lets test getObjectRaw with range
const rangeStart = 2048 * 1024; // 2 MB
const rangeEnd = 8 * 1024 * 1024 * 2; // 16 MB
const rangeResponse = await s3client.getObjectRaw(multipartKey, false, rangeStart, rangeEnd);
const rangeData = await rangeResponse.arrayBuffer();"><pre><span>import</span> <span>{</span> <span>s3mini</span><span>,</span> <span>sanitizeETag</span> <span>}</span> <span>from</span> <span>'s3mini'</span><span>;</span>

<span>const</span> <span>s3client</span> <span>=</span> <span>new</span> <span>s3mini</span><span>(</span><span>{</span>
  <span>accessKeyId</span>: <span>config</span><span>.</span><span>accessKeyId</span><span>,</span>
  <span>secretAccessKey</span>: <span>config</span><span>.</span><span>secretAccessKey</span><span>,</span>
  <span>endpoint</span>: <span>config</span><span>.</span><span>endpoint</span><span>,</span>
  <span>region</span>: <span>config</span><span>.</span><span>region</span><span>,</span>
<span>}</span><span>)</span><span>;</span>

<span>// Basic bucket ops</span>
<span>let</span> <span>exists</span>: <span>boolean</span> <span>=</span> <span>false</span><span>;</span>
<span>try</span> <span>{</span>
  <span>// Check if the bucket exists</span>
  <span>exists</span> <span>=</span> <span>await</span> <span>s3client</span><span>.</span><span>bucketExists</span><span>(</span><span>)</span><span>;</span>
<span>}</span> <span>catch</span> <span>(</span><span>err</span><span>)</span> <span>{</span>
  <span>throw</span> <span>new</span> <span>Error</span><span>(</span><span>`Failed bucketExists() call, wrong credentials maybe: <span><span>${</span><span>err</span><span>.</span><span>message</span><span>}</span></span>`</span><span>)</span><span>;</span>
<span>}</span>
<span>if</span> <span>(</span><span>!</span><span>exists</span><span>)</span> <span>{</span>
  <span>// Create the bucket based on the endpoint bucket name</span>
  <span>await</span> <span>s3client</span><span>.</span><span>createBucket</span><span>(</span><span>)</span><span>;</span>
<span>}</span>

<span>// Basic object ops</span>
<span>// key is the name of the object in the bucket</span>
<span>const</span> <span>smallObjectKey</span>: <span>string</span> <span>=</span> <span>'small-object.txt'</span><span>;</span>
<span>// content is the data you want to store in the object</span>
<span>// it can be a string or Buffer (recommended for large objects)</span>
<span>const</span> <span>smallObjectContent</span>: <span>string</span> <span>=</span> <span>'Hello, world!'</span><span>;</span>

<span>// check if the object exists</span>
<span>const</span> <span>objectExists</span>: <span>boolean</span> <span>=</span> <span>await</span> <span>s3client</span><span>.</span><span>objectExists</span><span>(</span><span>smallObjectKey</span><span>)</span><span>;</span>
<span>let</span> <span>etag</span>: <span>string</span> <span>|</span> <span>null</span> <span>=</span> <span>null</span><span>;</span>
<span>if</span> <span>(</span><span>!</span><span>objectExists</span><span>)</span> <span>{</span>
  <span>// put/upload the object, content can be a string or Buffer</span>
  <span>// to add object into "folder", use "folder/filename.txt" as key</span>
  <span>const</span> <span>resp</span>: <span>Response</span> <span>=</span> <span>await</span> <span>s3client</span><span>.</span><span>putObject</span><span>(</span><span>smallObjectKey</span><span>,</span> <span>smallObjectContent</span><span>)</span><span>;</span>
  <span>// you can also get etag via getEtag method</span>
  <span>// const etag: string = await s3client.getEtag(smallObjectKey);</span>
  <span>etag</span> <span>=</span> <span>sanitizeETag</span><span>(</span><span>resp</span><span>.</span><span>headers</span><span>.</span><span>get</span><span>(</span><span>'etag'</span><span>)</span><span>)</span><span>;</span>
<span>}</span>

<span>// get the object, null if not found</span>
<span>const</span> <span>objectData</span>: <span>string</span> <span>|</span> <span>null</span> <span>=</span> <span>await</span> <span>s3client</span><span>.</span><span>getObject</span><span>(</span><span>smallObjectKey</span><span>)</span><span>;</span>
<span>console</span><span>.</span><span>log</span><span>(</span><span>'Object data:'</span><span>,</span> <span>objectData</span><span>)</span><span>;</span>

<span>// get the object with ETag, null if not found</span>
<span>const</span> <span>response2</span>: <span>Response</span> <span>=</span> <span>await</span> <span>s3mini</span><span>.</span><span>getObject</span><span>(</span><span>smallObjectKey</span><span>,</span> <span>{</span> <span>'if-none-match'</span>: <span>etag</span> <span>}</span><span>)</span><span>;</span>
<span>if</span> <span>(</span><span>response2</span><span>)</span> <span>{</span>
  <span>// ETag changed so we can get the object data and new ETag</span>
  <span>// Note: ETag is not guaranteed to be the same as the MD5 hash of the object</span>
  <span>// ETag is sanitized to remove quotes</span>
  <span>const</span> <span>etag2</span>: <span>string</span> <span>=</span> <span>sanitizeETag</span><span>(</span><span>response2</span><span>.</span><span>headers</span><span>.</span><span>get</span><span>(</span><span>'etag'</span><span>)</span><span>)</span><span>;</span>
  <span>console</span><span>.</span><span>log</span><span>(</span><span>'Object data with ETag:'</span><span>,</span> <span>response2</span><span>.</span><span>body</span><span>,</span> <span>'ETag:'</span><span>,</span> <span>etag2</span><span>)</span><span>;</span>
<span>}</span> <span>else</span> <span>{</span>
  <span>console</span><span>.</span><span>log</span><span>(</span><span>'Object not found or ETag does match.'</span><span>)</span><span>;</span>
<span>}</span>

<span>// list objects in the bucket, null if bucket is empty</span>
<span>// Note: listObjects uses listObjectsV2 API and iterate over all pages</span>
<span>// so it will return all objects in the bucket which can take a while</span>
<span>// If you want to limit the number of objects returned, use the maxKeys option</span>
<span>// If you want to list objects in a specific "folder", use "folder/" as prefix</span>
<span>// Example s3client.listObjects({"/" "myfolder/"})</span>
<span>const</span> <span>list</span>: <span>object</span><span>[</span><span>]</span> <span>|</span> <span>null</span> <span>=</span> <span>await</span> <span>s3client</span><span>.</span><span>listObjects</span><span>(</span><span>)</span><span>;</span>
<span>if</span> <span>(</span><span>list</span><span>)</span> <span>{</span>
  <span>console</span><span>.</span><span>log</span><span>(</span><span>'List of objects:'</span><span>,</span> <span>list</span><span>)</span><span>;</span>
<span>}</span> <span>else</span> <span>{</span>
  <span>console</span><span>.</span><span>log</span><span>(</span><span>'No objects found in the bucket.'</span><span>)</span><span>;</span>
<span>}</span>

<span>// delete the object</span>
<span>const</span> <span>wasDeleted</span>: <span>boolean</span> <span>=</span> <span>await</span> <span>s3client</span><span>.</span><span>deleteObject</span><span>(</span><span>smallObjectKey</span><span>)</span><span>;</span>

<span>// Multipart upload</span>
<span>const</span> <span>multipartKey</span> <span>=</span> <span>'multipart-object.txt'</span><span>;</span>
<span>const</span> <span>large_buffer</span> <span>=</span> <span>new</span> <span>Uint8Array</span><span>(</span><span>1024</span> <span>*</span> <span>1024</span> <span>*</span> <span>15</span><span>)</span><span>;</span> <span>// 15 MB buffer</span>
<span>const</span> <span>partSize</span> <span>=</span> <span>8</span> <span>*</span> <span>1024</span> <span>*</span> <span>1024</span><span>;</span> <span>// 8 MB</span>
<span>const</span> <span>totalParts</span> <span>=</span> <span>Math</span><span>.</span><span>ceil</span><span>(</span><span>large_buffer</span><span>.</span><span>length</span> <span>/</span> <span>partSize</span><span>)</span><span>;</span>
<span>// Beware! This will return always a new uploadId</span>
<span>// if you want to use the same uploadId, you need to store it somewhere</span>
<span>const</span> <span>uploadId</span> <span>=</span> <span>await</span> <span>s3client</span><span>.</span><span>getMultipartUploadId</span><span>(</span><span>multipartKey</span><span>)</span><span>;</span>
<span>const</span> <span>uploadPromises</span> <span>=</span> <span>[</span><span>]</span><span>;</span>
<span>for</span> <span>(</span><span>let</span> <span>i</span> <span>=</span> <span>0</span><span>;</span> <span>i</span> <span>&lt;</span> <span>totalParts</span><span>;</span> <span>i</span><span>++</span><span>)</span> <span>{</span>
  <span>const</span> <span>partBuffer</span> <span>=</span> <span>large_buffer</span><span>.</span><span>subarray</span><span>(</span><span>i</span> <span>*</span> <span>partSize</span><span>,</span> <span>(</span><span>i</span> <span>+</span> <span>1</span><span>)</span> <span>*</span> <span>partSize</span><span>)</span><span>;</span>
  <span>// upload each part</span>
  <span>// Note: uploadPart returns a promise, so you can use Promise.all to upload all parts in parallel</span>
  <span>// but be careful with the number of parallel uploads, it can cause throttling</span>
  <span>// or errors if you upload too many parts at once</span>
  <span>// You can also use generator functions to upload parts in batches</span>
  <span>uploadPromises</span><span>.</span><span>push</span><span>(</span><span>s3client</span><span>.</span><span>uploadPart</span><span>(</span><span>multipartKey</span><span>,</span> <span>uploadId</span><span>,</span> <span>partBuffer</span><span>,</span> <span>i</span> <span>+</span> <span>1</span><span>)</span><span>)</span><span>;</span>
<span>}</span>
<span>const</span> <span>uploadResponses</span> <span>=</span> <span>await</span> <span>Promise</span><span>.</span><span>all</span><span>(</span><span>uploadPromises</span><span>)</span><span>;</span>
<span>const</span> <span>parts</span> <span>=</span> <span>uploadResponses</span><span>.</span><span>map</span><span>(</span><span>(</span><span>response</span><span>,</span> <span>index</span><span>)</span> <span>=&gt;</span> <span>(</span><span>{</span>
  <span>partNumber</span>: <span>index</span> <span>+</span> <span>1</span><span>,</span>
  <span>etag</span>: <span>response</span><span>.</span><span>etag</span><span>,</span>
<span>}</span><span>)</span><span>)</span><span>;</span>
<span>// Complete the multipart upload</span>
<span>const</span> <span>completeResponse</span> <span>=</span> <span>await</span> <span>s3client</span><span>.</span><span>completeMultipartUpload</span><span>(</span><span>multipartKey</span><span>,</span> <span>uploadId</span><span>,</span> <span>parts</span><span>)</span><span>;</span>
<span>const</span> <span>completeEtag</span> <span>=</span> <span>completeResponse</span><span>.</span><span>etag</span><span>;</span>

<span>// List multipart uploads</span>
<span>// returns object with uploadId and key</span>
<span>const</span> <span>multipartUploads</span>: <span>object</span> <span>=</span> <span>await</span> <span>s3client</span><span>.</span><span>listMultipartUploads</span><span>(</span><span>)</span><span>;</span>
<span>// Abort the multipart upload</span>
<span>const</span> <span>abortResponse</span> <span>=</span> <span>await</span> <span>s3client</span><span>.</span><span>abortMultipartUpload</span><span>(</span><span>multipartUploads</span><span>.</span><span>key</span><span>,</span> <span>multipartUploads</span><span>.</span><span>uploadId</span><span>)</span><span>;</span>

<span>// Multipart download</span>
<span>// lets test getObjectRaw with range</span>
<span>const</span> <span>rangeStart</span> <span>=</span> <span>2048</span> <span>*</span> <span>1024</span><span>;</span> <span>// 2 MB</span>
<span>const</span> <span>rangeEnd</span> <span>=</span> <span>8</span> <span>*</span> <span>1024</span> <span>*</span> <span>1024</span> <span>*</span> <span>2</span><span>;</span> <span>// 16 MB</span>
<span>const</span> <span>rangeResponse</span> <span>=</span> <span>await</span> <span>s3client</span><span>.</span><span>getObjectRaw</span><span>(</span><span>multipartKey</span><span>,</span> <span>false</span><span>,</span> <span>rangeStart</span><span>,</span> <span>rangeEnd</span><span>)</span><span>;</span>
<span>const</span> <span>rangeData</span> <span>=</span> <span>await</span> <span>rangeResponse</span><span>.</span><span>arrayBuffer</span><span>(</span><span>)</span><span>;</span></pre></div>
<p dir="auto">For more check <a href="https://github.com/good-lly/s3mini/blob/dev/USAGE.md">USAGE.md</a> file, examples and tests.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Security Notes</h2><a id="user-content-security-notes" aria-label="Permalink: Security Notes" href="#security-notes"></a></p>
<ul dir="auto">
<li>The library masks sensitive information (access keys, session tokens, etc.) when logging.</li>
<li>Always protect your AWS credentials and avoid hard-coding them in your application (!!!). Use environment variables. Use environment variables or a secure vault for storing credentials.</li>
<li>Ensure you have the necessary permissions to access the S3 bucket and perform operations.</li>
<li>Be cautious when using multipart uploads, as they can incur additional costs if not managed properly.</li>
<li>Authors are not responsible for any data loss or security breaches resulting from improper usage of the library.</li>
<li>If you find a security vulnerability, please report it to us directly via email. For more details, please refer to the <a href="https://github.com/good-lly/s3mini/blob/dev/SECURITY.md">SECURITY.md</a> file.</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Contributions welcomed!</h2><a id="user-content-contributions-welcomed" aria-label="Permalink: Contributions welcomed!" href="#contributions-welcomed"></a></p>
<p dir="auto">Contributions are greatly appreciated! If you have an idea for a new feature or have found a bug, we encourage you to get involved:</p>
<ul dir="auto">
<li>
<p dir="auto"><em>Report Issues</em>: If you encounter a problem or have a feature request, please open an issue on GitHub. Include as much detail as possible (environment, error messages, logs, steps to reproduce, etc.) so we can understand and address the issue.</p>
</li>
<li>
<p dir="auto"><em>Pull Requests</em>: We welcome PRs! If you want to implement a new feature or fix a bug, feel free to submit a pull request to the latest <code>dev branch</code>. For major changes, it's a good idea to discuss your plans in an issue first.</p>
</li>
<li>
<p dir="auto"><em>Lightweight Philosophy</em>: When contributing, keep in mind that s3mini aims to remain lightweight and dependency-free. Please avoid adding heavy dependencies. New features should provide significant value to justify any increase in size.</p>
</li>
<li>
<p dir="auto"><em>Community Conduct</em>: Be respectful and constructive in communications. We want a welcoming environment for all contributors. For more details, please refer to our <a href="https://github.com/good-lly/s3mini/blob/dev/CODE_OF_CONDUCT.md">CODE_OF_CONDUCT.md</a>. No one reads it, but it's there for a reason.</p>
</li>
</ul>
<p dir="auto">If you figure out a solution to your question or problem on your own, please consider posting the answer or closing the issue with an explanation. It could help the next person who runs into the same thing!</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">License</h2><a id="user-content-license" aria-label="Permalink: License" href="#license"></a></p>
<p dir="auto">This project is licensed under the MIT License - see the <a href="https://github.com/good-lly/s3mini/blob/dev/LICENSE.md">LICENSE.md</a> file for details.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Sponsor This Project</h2><a id="user-content-sponsor-this-project" aria-label="Permalink: Sponsor This Project" href="#sponsor-this-project"></a></p>
<p dir="auto">Developing and maintaining s3mini (and other open-source projects) requires time and effort. If you find this library useful, please consider sponsoring its development. Your support helps ensure I can continue improving s3mini and other projects. Thank you!</p>
<p dir="auto"><a href="https://github.com/sponsors/good-lly"><img src="https://camo.githubusercontent.com/a12b8f68983edf77bb6d60e2b29a80bcfafd07082c88b0a5decd3c0f3b1dd672/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f2546302539462539322542385f4769744875622d53706f6e736f722d6666363962343f6c6f676f3d676974687562266c6f676f436f6c6f723d7768697465" alt="Become a Sponsor" data-canonical-src="https://img.shields.io/badge/%F0%9F%92%B8_GitHub-Sponsor-ff69b4?logo=github&amp;logoColor=white"></a></p>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Left-Pad (2024) (276 pts)]]></title>
            <link>https://azerkoculu.com/posts/left-pad</link>
            <guid>44245166</guid>
            <pubDate>Wed, 11 Jun 2025 07:44:01 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://azerkoculu.com/posts/left-pad">https://azerkoculu.com/posts/left-pad</a>, See on <a href="https://news.ycombinator.com/item?id=44245166">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>8 years have passed since the <a href="https://en.wikipedia.org/wiki/Npm_left-pad_incident">the left-pad incident</a> happened. It's been good for me to avoid this subject for focusing on actual work. Also, silence is gold. On the other hand, left-pad is seen as a notable event mentioned in books, so I'll share more info and thoughts about what exactly happened.</p>
<p>In most of 2016, I spent almost every weekend camping in remote areas without any signal. If you're curious how I felt when making the decision to unpublish; it was a choice made during self-reflection in nature.</p>
<p>Not driven by logic, anger, or greed.<br>
It was a decision guided by my heart.</p>
<p>And it came from a simple principle: if NPM breaks its own rules to remove one of my packages, they should remove all of them.</p>
<p>Not that I'm an inflexible "rules" person — quite the opposite. The spirit behind the rules matter more than the rules themselves to me. In a different context, we could be asking NPM to take down a package without owner's permission for a "good" reason. However, in this situation, a company like <a href="https://en.wikipedia.org/wiki/Kik_Messenger">Kik Messenger</a> was just posturing and exercising power over the open source community NPM was built on, sending threats such as "we'll bang on your door" and "take down your accounts". This is the context in which NPM went against the rules they wrote themselves, to serve something they deemed "higher" than the soul of their company and the community.</p>
<p>I was not afraid of Kik's threats, <br>
but NPM was afraid of losing Kik.</p>
<p>Many people oversimplify this event, framing it as "an angry man protested corporate interests". This narrative shows us three things; first, they didn't look at the dates of the emails. They don't understand the timeline. Second, they can't relate to standing your ground in a high pressure situation involving threats. And third, they haven't read Al-Ghazali yet, don't quite understand how (free) people make decisions.</p>
<p>There was nothing sudden or unexpected for NPM. I asked NPM to remove my modules, waited their response. I didn't set any deadline; NPM had the opportunity to adjust their APIs and tooling to make this transition smooth. Interestingly, they chose to provide me a script that removes all my packages at once. On the NPM side, I observed general condescending attitude towards developers, which led them make series of unreasonable decisions and ultimately blame me for all the cost.</p>
<p>Most of my open source work followed Unix philosophy, so the packages did one thing at a time. There was 350+ of them. In the surface, it looked like nobody used them. NPM didn't show usage stats, and there was almost no activity on Github. As a user, it was impossible to know the impact of unpublishing packages; but I still don't understand why NPM didn't take the time to find out if any of my modules were widely used and consider ways to handle the unpublishing without breaking anything.</p>
<h2>8 Years Later</h2>
<p>Several months after the left-pad incident, I quit my job and left US permanently, spent a year in Morocco, Jordan, Türkiye and Indonesia. I walked trails like Lycian Way, found new camping spots nobody knows about.</p>
<p>Left-pad was like a "death" and "re-birth" moment for me. The part of me passionate about open-source was dead, and something new took over. Now, I'm passionate about business, marketing, running companies / teams in different ways, as much as I'm about programming.</p>
<p>Life goes on.</p>
<p>Thanks for reading.</p>
<p><img src="https://live.staticflickr.com/65535/51992852851_0ba272a470_b.jpg" alt=""></p>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[It's the end of observability as we know it (and I feel fine) (266 pts)]]></title>
            <link>https://www.honeycomb.io/blog/its-the-end-of-observability-as-we-know-it-and-i-feel-fine</link>
            <guid>44243050</guid>
            <pubDate>Wed, 11 Jun 2025 00:34:55 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.honeycomb.io/blog/its-the-end-of-observability-as-we-know-it-and-i-feel-fine">https://www.honeycomb.io/blog/its-the-end-of-observability-as-we-know-it-and-i-feel-fine</a>, See on <a href="https://news.ycombinator.com/item?id=44243050">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
			<p>In a really broad sense, the history of observability tools over the past couple of decades have been about a pretty simple concept: how do we make terabytes of heterogeneous telemetry data comprehensible to human beings? New Relic did this for the Rails revolution, Datadog did it for the rise of AWS, and Honeycomb led the way for <a href="https://opentelemetry.io/" target="_blank" rel="noreferrer noopener">OpenTelemetry</a>.</p>
<p>The loop has been the same in each case. New abstractions and techniques for software development and deployment gain traction, those abstractions make software more accessible by hiding complexity, and that complexity requires new ways to monitor and measure what’s happening. We build tools like dashboards, adaptive alerting, and dynamic sampling. All of these help us compress the sheer amount of <em>stuff</em> happening into something that’s comprehensible to our human intelligence.</p>
<p>In AI, I see the death of this paradigm. It’s already real, it’s already here, and it’s going to fundamentally change the way we approach systems design and operation in the future.</p>
<div>
<hr>
<div>
<p>New to Honeycomb? Get your <strong>free</strong> account today.</p>

</div>

<hr>
</div>
<h2 id="useful">LLMs are just universal function approximators, but it turns out that those are really useful</h2>
<p>I’m going to tell you&nbsp;a story. It’s about this picture:</p>
<figure><img decoding="async" width="1024" height="808" src="https://www.honeycomb.io/wp-content/uploads/2025/06/image-1024x808.png" alt="LLMs are just universal function approximators, but it turns out that those are really useful" srcset="https://www.honeycomb.io/wp-content/uploads/2025/06/image-1024x808.png 1024w, https://www.honeycomb.io/wp-content/uploads/2025/06/image-768x606.png 768w, https://www.honeycomb.io/wp-content/uploads/2025/06/image-640x505.png 640w, https://www.honeycomb.io/wp-content/uploads/2025/06/image-348x275.png 348w, https://www.honeycomb.io/wp-content/uploads/2025/06/image.png 1460w" sizes="(max-width: 1024px) 100vw, 1024px"></figure>
<p>If you’ve ever seen a Honeycomb demo, you’ve probably seen this image. We love it, because it’s not only a great way to show a real-world problem—it’s something that plays well to our core strengths of enabling investigatory loops. Those little peaks you see in the heatmap represent slow requests in a <a href="https://www.honeycomb.io/frontend-observability">frontend</a> service that rise over time before suddenly resetting. They represent a small percentage of your users experiencing poor performance—and we all know what this means in the real world: lost sales, poor experience, and general malaise at the continued enshittification of software.</p>
<p>In a Honeycomb demo, we show you how easy it is to use our UI to understand what those spikes actually mean. You draw a box around them, and we run <a href="https://www.honeycomb.io/bubbleup">BubbleUp</a> to detect anomalies by analyzing the trace data that’s backing this visualization, showing you what’s similar and what’s different between the spikes and the baseline. Eventually, you can drill down to the specific service and even method call that’s causing the problem. It’s a great demo, and it really shows the power of our platform.</p>
<p>Last Friday, I showed a demo at our weekly internal Demo Day. It started with what I just showed you, and then I ran a single prompt through an AI agent that read as follows:</p>
<p><strong>Please investigate the odd latency spikes in the frontend service that happen every four hours or so, and tell me why they’re happening.</strong></p>
<figure><img decoding="async" width="1024" height="366" src="https://www.honeycomb.io/wp-content/uploads/2025/06/image-1-1024x366.png" alt="" srcset="https://www.honeycomb.io/wp-content/uploads/2025/06/image-1-1024x366.png 1024w, https://www.honeycomb.io/wp-content/uploads/2025/06/image-1-768x275.png 768w, https://www.honeycomb.io/wp-content/uploads/2025/06/image-1-1536x549.png 1536w, https://www.honeycomb.io/wp-content/uploads/2025/06/image-1-640x229.png 640w, https://www.honeycomb.io/wp-content/uploads/2025/06/image-1-348x124.png 348w, https://www.honeycomb.io/wp-content/uploads/2025/06/image-1.png 1600w" sizes="(max-width: 1024px) 100vw, 1024px"></figure>
<p>The screenshot here elides the remainder of the response from the LLM (please find the entire text at the end of this post), but there’s a few things I want to call out. First, this wasn’t anything too special. The agent was something I wrote myself in a couple of days; it’s just an LLM calling tools in a loop. The model itself is off-the-shelf <a href="https://www.anthropic.com/claude/sonnet" target="_blank" rel="noreferrer noopener">Claude Sonnet 4</a>. The integration with Honeycomb is our new <a href="https://www.honeycomb.io/blog/mcp-easy-as-1-2-3">Model Context Protocol (MCP)</a> server. It took 80 seconds, made eight tool calls, and not only did it tell me why those spikes happened, it figured it out in a pretty similar manner to how we’d tell you to do it with BubbleUp.</p>
<p>This isn’t a contrived example. I basically asked the agent the same question we’d ask you in a demo, and the agent figured it out with no additional prompts, training, or guidance. It effectively zero-shot a real-world scenario.</p>
<p>And it did it for <strong>sixty cents.</strong></p>
<h2 id="youcandoit">If I can do this, so can you. So can anyone.</h2>
<p>I want to be clear, this was perhaps the <em>least </em>optimized version of this workflow. Inference costs are only going down, and we can certainly make our MCP server more efficient. There are ways to reduce the amount of input tokens even more. We can play around with more tailored aggregations and function calls that return LLM-optimized query results. It’s an exciting new era!</p>
<p>It also should serve as a wakeup call to the entire industry. This is a seismic shift in how we should conceptualize observability tooling. <strong>If your product’s value proposition is nice graphs and easy instrumentation, you are </strong><strong><em>le cooked</em></strong><strong>. </strong>An LLM commoditizes the analysis piece, OpenTelemetry commoditizes the instrumentation piece. <strong>The moats are emptying.</strong></p>
<p>I’m not gonna sit here and say this destroys the idea of <a href="https://www.honeycomb.io/blog/ai-where-in-the-loop-should-humans-go">humans being involved in the process</a>, though. I don’t think that’s true. The rise of the cloud didn’t destroy the idea of IT. The existence of Rails doesn’t mean we don’t need server programmers. Productivity increases <em>expand the map</em>. There’ll be more software, of all shapes and sizes. We’re going to need <em>more</em> of everything.</p>
<p>The question, then, is: what does this require from us? Where does observability sit in a world where code is cheap, refactors are cheap, and analysis is a constant factor?</p>
<h2 id="feedback">Fast feedback is the only feedback</h2>
<p>I’m gonna put a marker out there: the only thing that really matters is fast, tight feedback loops at every stage of development and operations. AI thrives on speed—it’ll outrun you every time. To succeed, you need tools that move at the speed of AI as well. The slower your analysis engine, the worse your results. LLMs will generate, test, and discard hypotheses faster than you ever can. They might be wrong a dozen times before they get it right, but again, <em>we’re talking about fractional pennies here</em>.&nbsp;</p>
<p>I don’t see this as a reason to despair, but as something to celebrate. The things that AI-assisted development and operations can lead us to are the things <em>we’ve always been about</em>. Honeycomb’s entire modus operandi is predicated on fast feedback loops, collaborative knowledge sharing, and treating everything as an experiment. We’re built for the future that’s here today, on a platform that allows us to be the best tool for tomorrow.</p>
<p>The exact contours of what this industry looks like in two, five, or ten years are out of my view. I don’t think there’s anyone that can credibly tell you what things will look like. I’m also not really in the business of making predictions. What I can say, though, is that I think we’re going to see a spectrum of AI assistance in development and operational tasks across the entire <a href="https://www.honeycomb.io/blog/honeycomb-fit-software-development-lifecycle">software development lifecycle</a>:</p>
<ul>
<li>AI agents can run passively as you write and deploy code, suggesting fixes to improve code quality based on real-world observations from production.</li>
<li>Emergent system behavior can be detected and addressed by AI, with unattended agents identifying impacts to users, investigating them, compiling reports, and presenting those to human operators for approval and tweaks before making changes.</li>
<li>Bleeding-edge organizations can adopt entirely unattended SWE/SRE roles, powered by tools like Honeycomb and friends, optimizing and writing software to meet business needs and goals.</li>
</ul>
<p>In all of these cases, what matters most is <em>speed</em>. Traditional monitoring and observability tools don’t have what it takes to compete in this world. Pretty dashboards, pre-built alerts, and magic integrations aren’t sufficient for the future we’re building today. Sub-second query performance, unified data storage, and collaborative workflows between humans and AI are the keys to success in the future of observability.</p>
<p>Let’s go do something cool.</p>
<p><em>Addendum:</em> Here’s the full output from the investigation screenshot earlier.</p>
<p><strong>Perfect! Now I have a clear picture of what’s happening. Based on my investigation, I can explain the latency spikes you’re seeing in your frontend service.</strong></p>
<hr>
<p><strong>🔍 Investigation Results: Frontend Latency Spikes</strong></p>
<hr>
<p><strong>Root Cause Identified: Checkout Service Performance Issues</strong></p>
<p>The latency spikes you’re experiencing every ~4 hours are <strong>NOT actually periodic</strong>—this appears to be a <strong>load testing or synthetic traffic pattern</strong> rather than a scheduled system issue. Here’s what I found:</p>
<hr>
<p><strong>📊 Key Findings:</strong></p>
<ol>
<li><strong>Primary Culprit: Checkout Operations</strong><strong><br></strong>
<ul>
<li>/cart/checkout endpoint: <strong>P95 = 3.7 seconds</strong> (vs normal ~500ms)</li>
<li>CheckoutService/PlaceOrder: <strong>P95 = 4.2 seconds</strong><strong><br></strong></li>
<li><strong>Maximum latency observed: 7.6 seconds</strong><strong><br></strong></li>
</ul>
</li>
<li><strong>Spike Pattern Analysis:</strong><strong><br></strong>
<ul>
<li>Spikes are concentrated in <strong>checkout operations only</strong><strong><br></strong></li>
<li>Other endpoints remain relatively stable</li>
<li><strong>485 slow checkout operations</strong> in the last 24 hours</li>
<li>Pattern suggests <strong>load testing or batch checkout processing</strong><strong><br></strong></li>
</ul>
</li>
<li><strong>Downstream Service Impact:</strong><strong><br></strong>
<ul>
<li>CheckoutService/PlaceOrder: <strong>P95 = 4.6 seconds</strong><strong><br></strong></li>
<li>getDiscounts operation: <strong>P95 = 4.2 seconds</strong><strong><br></strong></li>
<li>ShippingService/ShipOrder: <strong>P95 = 4.0 seconds</strong><strong><br></strong></li>
</ul>
</li>
</ol>
<hr>
<p><strong>🕵️ Evidence from Trace Analysis:</strong></p>
<p>Looking at the sample traces, I can see:</p>
<ul>
<li><strong>User Agent Pattern:</strong> python-requests/2.28.2 and python-requests/2.31.0</li>
<li><strong>Consistent Client IPs:</strong> 52.45.250.5 and internal IPs</li>
<li><strong>High Cart Values:</strong> $6,861, $7,036, $259 (suggesting test scenarios)</li>
<li><strong>Same User IDs:</strong> Multiple requests from user 20109 with identical cart totals</li>
</ul>
<hr>
<p><strong>💡 Why This is Happening:</strong></p>
<p><strong>Most Likely Cause: Load Testing or Synthetic Monitoring</strong></p>
<ul>
<li>The pattern suggests <strong>automated testing</strong> rather than real user traffic</li>
<li>Python requests library indicates <strong>scripted traffic</strong><strong><br></strong></li>
<li>High cart values and repeated scenarios point to <strong>test data</strong><strong><br></strong></li>
<li>The “every 4 hours” pattern is likely your <strong>load testing schedule</strong><strong><br></strong></li>
</ul>
<p><strong>Technical Root Cause:</strong></p>
<ul>
<li><strong>Checkout service bottleneck</strong> during high-volume operations</li>
<li><strong>Discount calculation delays</strong> (4.2s P95)</li>
<li><strong>Shipping service delays</strong> during order processing</li>
<li>Possible <strong>database contention</strong> or <strong>external API timeouts</strong><strong><br></strong></li>
</ul>
<hr>
<p><strong>🎯 Recommendations:</strong></p>
<ol>
<li><strong>Immediate Actions:</strong><strong><br></strong>
<ul>
<li><strong>Verify if this is load testing</strong>—check with your testing team</li>
<li><strong>Monitor checkout service resources</strong> during spike periods</li>
<li><strong>Check discount service performance</strong> and external dependencies</li>
</ul>
</li>
<li><strong>Performance Optimization:</strong><strong><br></strong>
<ul>
<li><strong>Optimize discount calculation logic</strong><strong><br></strong></li>
<li><strong>Add caching</strong> for shipping calculations</li>
<li><strong>Implement circuit breakers</strong> for external payment/shipping APIs</li>
<li><strong>Scale checkout service</strong> if this represents real user load</li>
</ul>
</li>
<li><strong>Monitoring Improvements:</strong><strong><br></strong>
<ul>
<li><strong>Separate alerts</strong> for synthetic vs. real user traffic</li>
<li><strong>Add SLI tracking</strong> specifically for checkout operations</li>
<li><strong>Monitor downstream dependencies</strong> (payment, shipping, discount services)</li>
</ul>
</li>
</ol>
<hr>
<p><strong>🚨 Action Items:</strong></p>
<ol>
<li>Confirm if the 4-hour pattern matches your load testing schedule</li>
<li>If it’s real traffic, investigate checkout service capacity and dependencies</li>
<li>Consider implementing checkout-specific performance monitoring and alerting</li>
</ol>
<hr>
<p><strong>The good news is that your other services are performing well—this appears to be an isolated issue with the checkout flow, likely exacerbated by load testing scenarios.</strong></p>
<hr>
		</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[The Gentle Singularity (232 pts)]]></title>
            <link>https://blog.samaltman.com/the-gentle-singularity</link>
            <guid>44241549</guid>
            <pubDate>Tue, 10 Jun 2025 21:17:08 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://blog.samaltman.com/the-gentle-singularity">https://blog.samaltman.com/the-gentle-singularity</a>, See on <a href="https://news.ycombinator.com/item?id=44241549">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="post_body_2202875"><p>We are past the event horizon; the takeoff has started. Humanity is close to building digital superintelligence, and at least so far it’s much less weird than it seems like it should be.</p><p>Robots are not yet walking the streets, nor are most of us talking to AI all day. People still die of disease, we still can’t easily go to space, and there is a lot about the universe we don’t understand.</p><p>And yet, we have recently built systems that are smarter than people in many ways, and are able to significantly amplify the output of people using them. The least-likely part of the work is behind us; the scientific insights that got us to systems like GPT-4 and o3 were hard-won, but will take us very far.</p><p>AI will contribute to the world in many ways, but the gains to quality of life from AI driving faster scientific progress and increased productivity will be enormous; the future can be vastly better than the present. Scientific progress is the biggest driver of overall progress; it’s hugely exciting to think about how much more we could have.</p><p>In some big sense, ChatGPT is already more powerful than any human who has ever lived. Hundreds of millions of people rely on it every day and for increasingly important tasks; a small new capability can create a hugely positive impact; a small misalignment multiplied by hundreds of millions of people can cause a great deal of negative impact.</p><p>2025 has seen the arrival of agents that can do real cognitive work; writing computer code will never be the same. 2026 will likely see the arrival of systems that can figure out novel insights. 2027 may see the arrival of robots that can do tasks in the real world.</p><p>A lot more people will be able to create software, and art. But the world wants a lot more of both, and experts will probably still be much better than novices, as long as they embrace the new tools. Generally speaking, the ability for one person to get much more done in 2030 than they could in 2020 will be a striking change, and one many people will figure out how to benefit from.</p><p>In the most important ways, the 2030s may not be wildly different. People will still love their families, express their creativity, play games, and swim in lakes.<br></p><p>But in still-very-important-ways, the 2030s are likely going to be wildly different from any time that has come before. We do not know how far beyond human-level intelligence we can go, but we are about to find out.</p><p>In the 2030s, intelligence and energy—ideas, and the ability to make ideas happen—are going to become wildly abundant. These two have been the fundamental limiters on human progress for a long time; with abundant intelligence and energy (and good governance), we can theoretically have anything else.</p><p>Already we live with incredible digital intelligence, and after some initial shock, most of us are pretty used to it. Very quickly we go from being amazed that AI can generate a beautifully-written paragraph to wondering when it can generate a beautifully-written novel; or from being amazed that it can make live-saving medical diagnoses to wondering when it can develop the cures; or from being amazed it can create a small computer program to wondering when it can create an entire new company. This is how the singularity goes: wonders become routine, and then table stakes.</p><p>We already hear from scientists that they are two or three times more productive than they were before AI. Advanced AI is interesting for many reasons, but perhaps nothing is quite as significant as the fact that we can use it to do faster AI research. We may be able to discover new computing substrates, better algorithms, and who knows what else. If we can do a decade’s worth of research in a year, or a month, then the rate of progress will obviously be quite different.</p><p>From here on, the tools we have already built will help us find further scientific insights and aid us in creating better AI systems. Of course this isn’t the same thing as an AI system completely autonomously updating its own code, but nevertheless this is a larval version of recursive self-improvement.</p><p>There are other self-reinforcing loops at play. The economic value creation has started a flywheel of compounding infrastructure buildout to run these increasingly-powerful AI systems. And robots that can build other robots (and in some sense, datacenters that can build other datacenters) aren’t that far off.&nbsp;</p><p>If we have to make the first million humanoid robots the old-fashioned way, but then they can operate the entire supply chain—digging and refining minerals, driving trucks, running factories, etc.—to build more robots, which can build more chip fabrication facilities, data centers, etc, then the rate of progress will obviously be quite different.</p><p>As datacenter production gets automated, the cost of intelligence should eventually converge to near the cost of electricity. (People are often curious about how much energy a ChatGPT query uses; the average query uses about 0.34 watt-hours, about what an oven would use in a little over one second, or a high-efficiency lightbulb would use in a couple of minutes. It also uses about 0.000085 gallons of water; roughly one fifteenth of a teaspoon.)</p><p>The rate of technological progress will keep accelerating, and it will continue to be the case that people are capable of adapting to almost anything. There will be very hard parts like whole classes of jobs going away, but on the other hand the world will be getting so much richer so quickly that we’ll be able to seriously entertain new policy ideas we never could before. We probably won’t adopt a new social contract all at once, but when we look back in a few decades, the gradual changes will have amounted to something big.</p><p>If history is any guide, we will figure out new things to do and new things to want, and assimilate new tools quickly (job change after the industrial revolution is a good recent example). Expectations will go up, but capabilities will go up equally quickly, and we’ll all get better stuff. We will build ever-more-wonderful things for each other. People have a long-term important and curious advantage over AI: we are hard-wired to care about other people and what they think and do, and we don’t care very much about machines.</p><p>A subsistence farmer from a thousand years ago would look at what many of us do and say we have fake jobs, and think that we are just playing games to entertain ourselves since we have plenty of food and unimaginable luxuries. I hope we will look at the jobs a thousand years in the future and think they are very fake jobs, and I have no doubt they will feel incredibly important and satisfying to the people doing them.</p><p>The rate of new wonders being achieved will be immense. It’s hard to even imagine today what we will have discovered by 2035; maybe we will go from solving high-energy physics one year to beginning space colonization the next year; or from a major materials science breakthrough one year to true high-bandwidth brain-computer interfaces the next year. Many people will choose to live their lives in much the same way, but at least some people will probably decide to “plug in”.</p><p>Looking forward, this sounds hard to wrap our heads around. But probably living through it will feel impressive but manageable. From a relativistic perspective, the singularity happens bit by bit, and the merge happens slowly. We are climbing the long arc of exponential technological progress; it always looks vertical looking forward and flat going backwards, but it’s one smooth curve. (Think back to 2020, and what it would have sounded like to have something close to AGI by 2025, versus what the last 5 years have actually been like.)</p><p>There are serious challenges to confront along with the huge upsides. We do need to solve the safety issues, technically and societally, but then it’s critically important to widely distribute access to superintelligence given the economic implications. The best path forward might be something like:</p><ol>
<li><p>Solve the alignment problem, meaning that we can robustly guarantee that we get AI systems to learn and act towards what we collectively really want over the long-term (social media feeds are an example of misaligned AI; the algorithms that power those are incredible at getting you to keep scrolling and clearly understand your short-term preferences, but they do so by exploiting something in your brain that overrides your long-term preference).</p></li>
<li><p>Then focus on making superintelligence cheap, widely available, and not too concentrated with any person, company, or country. Society is resilient, creative, and adapts quickly. If we can harness the collective will and wisdom of people, then although we’ll make plenty of mistakes and some things will go really wrong, we will learn and adapt quickly and be able to use this technology to get maximum upside and minimal downside. Giving users a lot of freedom, within broad bounds society has to decide on, seems very important. The sooner the world can start a conversation about what these broad bounds are and how we define collective alignment, the better.</p></li>
</ol><p>We (the whole industry, not just OpenAI) are building a brain for the world. It will be extremely personalized and easy for everyone to use; we will be limited by good ideas. For a long time, technical people in the startup industry have made fun of “the idea guys”; people who had an idea and were looking for a team to build it. It now looks to me like they are about to have their day in the sun.</p><p>OpenAI is a lot of things now, but before anything else, we are a superintelligence research company. We have a lot of work in front of us, but most of the path in front of us is now lit, and the dark areas are receding fast. We feel extraordinarily grateful to get to do what we do.</p><p>Intelligence too cheap to meter is well within grasp. This may sound crazy to say, but if we told you back in 2020 we were going to be where we are today, it probably sounded more crazy than our current predictions about 2030.</p><p>May we scale smoothly, exponentially and uneventfully through superintelligence.</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Chatbots are replacing Google's search, devastating traffic for some publishers (193 pts)]]></title>
            <link>https://www.wsj.com/tech/ai/google-ai-news-publishers-7e687141</link>
            <guid>44241407</guid>
            <pubDate>Tue, 10 Jun 2025 21:03:27 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.wsj.com/tech/ai/google-ai-news-publishers-7e687141">https://www.wsj.com/tech/ai/google-ai-news-publishers-7e687141</a>, See on <a href="https://news.ycombinator.com/item?id=44241407">Hacker News</a></p>
Couldn't get https://www.wsj.com/tech/ai/google-ai-news-publishers-7e687141: Error: Request failed with status code 401]]></description>
        </item>
        <item>
            <title><![CDATA[Show HN: I made a 3D printed VTOL drone (375 pts)]]></title>
            <link>https://www.tsungxu.com/p/i-made-a-3d-printed-vtol-that-can</link>
            <guid>44241278</guid>
            <pubDate>Tue, 10 Jun 2025 20:47:11 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.tsungxu.com/p/i-made-a-3d-printed-vtol-that-can">https://www.tsungxu.com/p/i-made-a-3d-printed-vtol-that-can</a>, See on <a href="https://news.ycombinator.com/item?id=44241278">Hacker News</a></p>
<div id="readability-page-1" class="page"><div dir="auto"><div id="youtube2-e7AIKGDrlgs" data-attrs="{&quot;videoId&quot;:&quot;e7AIKGDrlgs&quot;,&quot;startTime&quot;:null,&quot;endTime&quot;:null}" data-component-name="Youtube2ToDOM"><p><iframe src="https://www.youtube-nocookie.com/embed/e7AIKGDrlgs?rel=0&amp;autoplay=0&amp;showinfo=0&amp;enablejsapi=0" frameborder="0" loading="lazy" gesture="media" allow="autoplay; fullscreen" allowautoplay="true" allowfullscreen="true" width="728" height="409"></iframe></p></div><p><br><span>I made this 130 mile capable VTOL drone in only 90 days. It can fly for 3 hours on a single charge. That would make it one of the longest range and endurance 3D printed VTOLs in the world.</span></p><p>This is the thing I'm most proud of building to date!</p><p><span>Before this project, I was a total CAD, 3D printing and aerodynamic modeling beginner. I had only </span><a href="https://www.tsungxu.com/p/first-vtol-design-build-and-flight" rel="">built and flown one VTOL before</a><span>. My CAD skills were restricted to extrusions of basic sketches. My Bambu A1 3D printer was only one month old, and I had never tried printing foaming PLA. I had never used any aerodynamic modeling tools to simulate lift-to-drag ratios or stability curves like Cm-alpha.</span></p><p>This video edit ended up shorter than I planned. Being my first Youtube video with significant post production effort, I underestimated the work required to make a longer in-depth video with voiceover, edited footage, etc.</p><p>Because of that, I did not discuss dozens of project challenges like how I chose the design parameters, learnt airframe CAD design, sourced components, improved foaming PLA print quality, in-depth power loss troubleshooting, and many more challenges this project faced.</p><p>If you want more details on my design and build process, let me know!</p><p><em><span>P.S. Reid Hoffman just </span><a href="https://x.com/reidhoffman/status/1932511491506262413" rel="">quote tweeted</a><span> my viral </span><a href="https://x.com/tsungxu/status/1932224925450043845" rel="">video post on X</a><span>:</span></em></p><blockquote><p><em>A century ago, you needed at least a brother and a bicycle shop to pioneer flight. Today, you just need the right toolchain…</em></p></blockquote><p>Incredibly humbling! </p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Show HN: A “Course” as an MCP Server (186 pts)]]></title>
            <link>https://mastra.ai/course</link>
            <guid>44241202</guid>
            <pubDate>Tue, 10 Jun 2025 20:36:55 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://mastra.ai/course">https://mastra.ai/course</a>, See on <a href="https://news.ycombinator.com/item?id=44241202">Hacker News</a></p>
<div id="readability-page-1" class="page"><div class="page"><nav><div><p><a href="https://mastra.ai/"><img alt="mastra" loading="lazy" width="24" height="23" decoding="async" data-nimg="1" src="https://mastra.ai/logo.svg"></a></p><ul data-open="false"><li><a href="https://mastra.ai/blog">Blog</a></li><li><a href="https://mastra.ai/university">University</a></li><li><a target="_blank" href="https://mastra.ai/docs">Docs</a></li></ul></div></nav><main><div><div><ul><li>Explained by Shane Thomas CPO of Mastra</li><li>Led by an MCP Agent</li><li>3 Lessons and more soon</li><li>12 Hours to compete</li><li>Interactive progress</li></ul><div><p>In this hands-on course, you'll learn to develop and deploy AI agents using Mastra, including how to equip them with tools, memory, and MCP. But here's the twist: you're not just learning about agents, you're learning from one.</p><p>This course is delivered entirely inside your agentic code editor, where your code agent guides you step-by-step, writes code with you, and helps you build real, working agents from the ground up.</p></div></div><div><div><p><span>Begin Mastra 101</span></p><div><p><span>1</span></p><p>Choose your editor and paste the command to install the mcp server automatically</p></div></div><div><p><span></span>Course Outline<span></span></p><div><div><p><span>Build your first agent</span><span><a href="https://youtu.be/buqexw-Xkvk" target="_blank">Watch lesson <!-- -->1<!-- --> intro</a></span></p><div><p>Build and deploy your first AI agent that can read data from external sources, use custom tools, and remember previous interactions. Set up your development environment, test your agent in a live playground, and ship your agent to production. This section covers the basics needed to create a working, memory and tool-enabled agent.</p><div><p>introduction to mastra</p><p>what is mastra</p><p>verifying installation</p></div></div></div><div><p><span>Adding Tools and MCP</span><span><a href="https://youtu.be/J5WVCPKTRxA" target="_blank">Watch lesson <!-- -->2<!-- --> intro</a></span></p><div><p>Add new abilities to your agent by connecting it to external services using MCP servers. Search across MCP registries to easily integrate tools for email, social media, GitHub, news, and local files, without writing any custom code.</p><div><p>introduction to mcp</p><p>installing mcp</p><p>setting up mcp configuration</p></div></div></div><div><p><span>Adding Memory</span><span><a href="https://youtu.be/Io2xtouOsMk" target="_blank">Watch lesson <!-- -->3<!-- --> intro</a></span></p><div><p>Add memory to your agent so it can remember previous conversations, user preferences, and important details. Learn how to configure conversation history, semantic recall, and working memory to create agents that provide more relevant and personalized responses.</p><div><p>understanding memory</p><p>why memory matters</p><p>installing memory</p></div></div></div></div><div><h3 id="faqs"><a href="#faqs">#</a>Faqs</h3></div></div></div></div></main></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[OpenAI o3-pro (173 pts)]]></title>
            <link>https://help.openai.com/en/articles/9624314-model-release-notes</link>
            <guid>44240999</guid>
            <pubDate>Tue, 10 Jun 2025 20:15:47 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://help.openai.com/en/articles/9624314-model-release-notes">https://help.openai.com/en/articles/9624314-model-release-notes</a>, See on <a href="https://news.ycombinator.com/item?id=44240999">Hacker News</a></p>
Couldn't get https://help.openai.com/en/articles/9624314-model-release-notes: Error: Request failed with status code 403]]></description>
        </item>
        <item>
            <title><![CDATA[Another Crack in the Chain of Trust: Uncovering (Yet Another) Secure Boot Bypass (118 pts)]]></title>
            <link>https://www.binarly.io/blog/another-crack-in-the-chain-of-trust</link>
            <guid>44240770</guid>
            <pubDate>Tue, 10 Jun 2025 19:55:18 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.binarly.io/blog/another-crack-in-the-chain-of-trust">https://www.binarly.io/blog/another-crack-in-the-chain-of-trust</a>, See on <a href="https://news.ycombinator.com/item?id=44240770">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><h6><em>By Binarly REsearch</em></h6><p>In this blog post, the Binarly Research team documents a Secure Boot bypass that likely impacts most devices supporting UEFI. At the center of this discovery is <a href="https://www.binarly.io/advisories/brly-dva-2025-001">CVE-2025-3052 (BRLY-2025-001)</a>, a memory corruption vulnerability in a module signed with Microsoft’s third-party UEFI certificate. Attackers can exploit this vulnerability to run unsigned code during the boot process, effectively bypassing Secure Boot and compromising the system’s chain of trust. Because the attacker’s code executes before the operating system even loads, it opens the door for attackers to install bootkits and undermine OS-level security defenses. </p><p>The story behind this discovery begins with a UEFI application we found on VirusTotal, signed with Microsoft’s third-party UEFI certificate. This application has been publicly available since November 2024 when it was first submitted. However, the signature data indicates it was actually signed in October 2022, suggesting it may have been circulating for much longer. </p><p>The application is a BIOS flashing tool for devices sold by DT Research, a company specializing in rugged mobile devices. Although this module was originally developed for DT Research devices only, it can run on any device that trusts the certificate with subject "<strong>Microsoft Corporation UEFI CA 2011</strong>" certificate. This includes the vast majority of systems, as the same certificate is widely used to sign the Linux bootloader shim, significantly increasing the potential impact of this vulnerability.</p><figure><p><img src="https://cdn.prod.website-files.com/65dc9e6c7079139a2996b5d7/6843694d5c893ab5cff2eefb_image1.png" loading="lazy" alt=""></p><figcaption><em>Figure 1. The vulnerable module signed with the Microsoft UEFI CA 2011 certificate</em></figcaption></figure><p>The root cause of this bug, once again, lies in the unsafe handling of NVRAM variables. Issues involving NVRAM variables have been a persistent problem in the UEFI ecosystem, <a href="https://www.binarly.io/advisories">with Binarly alone responsibly disclosing hundreds of related vulnerabilities</a> over the past few years. Because these bugs have been so widespread and impactful, our Binarly Transparency Platform performs multiple analyses to detect these bug classes. One of these checks looks specifically for any unsafe usage of data read from NVRAM variables, and this analysis is what automatically discovered CVE-2025-3052. </p><p>As shown in the image below, the signed application reads the content of the <em>IhisiParamBuffer</em> variable and directly uses it as a pointer for multiple memory write operations, without performing any validation or sanity checks on its value. This allows an attacker to set the <em>IhisiParamBuffer</em> variable to any arbitrary address in memory, effectively granting them an arbitrary memory write primitive.</p><figure><p><img src="https://cdn.prod.website-files.com/65dc9e6c7079139a2996b5d7/68436b2c089cba16f0d50471_image2.png" loading="lazy" alt=""></p><figcaption><em>Figure 2. The signed module blindly trusting an NVRAM variable for memory writes &nbsp;(CVE-2025-3052)</em></figcaption></figure><p>We reported this finding to CERT/CC on February 26, 2025. During our disclosure with Microsoft, it became clear that this issue wasn’t limited to a single module, but it actually affected 14 different modules. For this reason, to address CVE-2025-3052, Microsoft added 14 new hashes to the Secure Boot dbx as a mitigation measure, which was delivered as part of Patch Tuesday on June 10, 2025.</p><p><strong>Secure Boot and Microsoft Certificates</strong></p><p>Before delving into the details of CVE-2025-3052, let’s recap what Secure Boot is and what security problems it solves. First of all, Secure Boot plays a crucial role in the chain of trust built by UEFI. It can be thought of as a bridge connecting the system’s firmware to the operating system, carrying trust from one bank to the other. At the end of the DXE phase, with the platform initialized, the firmware is ready to boot the OS. With Secure Boot, the firmware cryptographically verifies the integrity and authenticity of the OS loader before handing over control. This check stops attackers from replacing the legitimate OS loader with a malicious component, such as a bootkit. Without Secure Boot, it is trivial for attackers to gain early execution, compromising OS defenses before the OS even has a chance to install them.</p><figure><p><img src="https://cdn.prod.website-files.com/65dc9e6c7079139a2996b5d7/68436b65a6fcb3dcd3042f62_image3.png" loading="lazy" alt=""></p><figcaption><em>Figure 3. Summary of the UEFI Secure Boot signing and verification process</em></figcaption></figure><p>UEFI applications are signed with the private part of a signing certificate (leaf certificate on the image) that chains through one or more intermediate certificates to a trusted, root CA certificate from <em>db</em>. This is for example what happens with applications signed by Microsoft, where the private key of the certificate with subject “Microsoft Corporation UEFI CA 2011” signs the signing/leaf certificate, whose private key is in turn used to sign the Authentocode hash of the UEFI Application.</p><p>The verification process is instead based on two signature databases: <em>db</em> and <em>dbx</em>. The <em>db</em> contains a list of trusted Authenticode hashes and root certificates:</p><ul role="list"><li>if any Authenticode hash in the <em>db</em> matches the application’s Authenticode hash, the application is allowed to run</li><li>if any root certificate in the <em>db</em> successfully verifies the application’s certificate chain, the application is allowed to run</li></ul><p>The dbx instead contains Authenticode hashes and certificates that are untrusted or revoked.</p><p>Depending on whether the application is signed, the firmware allows execution based on either its Authenticode hash or CA certificate being present in the <em>db</em> and not present in the <em>dbx</em>. By default, most devices are shipped with at least these trusted certificates present in <em>db</em>:</p><ol role="list"><li><em>Microsoft Windows Production PCA 2011</em> — used to sign the Windows bootloader.</li><li><em>Microsoft Corporation UEFI CA 2011</em> — used to validate third-party bootloaders and components.</li><li><em>OEM-owned certificate(s)</em> — specific to the hardware manufacturer, typically used for hardware-related or servicing modules.</li></ol><p>The second certificate is the one used to sign the vulnerable application. As previously mentioned, this certificate is widely trusted and present in <em>db</em>, since it’s used to sign components like the Linux’s shim, the EFI application responsible for booting Linux systems.</p><p><strong>The Beginning: Module Discovery and Reconnaissance</strong></p><p>This REsearch began after the discovery of the vulnerable <a href="https://www.virustotal.com/gui/file/a11996aaf7f32aa09f78db608106e25a47970bfeeec76d49af743eee71b55f06/">UEFI module on VirusTotal</a>. The first submission on VT was done in November 2024 but the signing time dates back to October 2022, meaning that this module may have been around for a longer time. The original submission filename was Dtbios-efi64-71.22.efi.The filename, combined with details from the Authenticode certificate and strings found within the binary itself, led us to conclude that the module has been developed by “DT Research, Inc”. </p><p>Additionally, the name of the vulnerable NVRAM variable immediately pointed to Insyde as the Independent BIOS Vendor (IBV) involved. This same variable had already been at the center of two vulnerabilities we previously reported: <a href="https://www.binarly.io/advisories/brly-2022-023">BRLY-2022-023</a> and <a href="https://www.binarly.io/advisories/brly-2023-005">BRLY-2023-005</a>.</p><p>After some reversing, we also conclude that the module is a BIOS update utility, as it reads a BIOS image from a file and attempts to flash it onto the device’s ROM.</p><figure><p><img src="https://cdn.prod.website-files.com/65dc9e6c7079139a2996b5d7/68436bb53f9e1ed4d4bdc755_image7.png" loading="lazy" alt=""></p><figcaption><em>Figure 4. A screenshot from VirusTotal of the vulnerable module</em></figcaption></figure><p><strong>The Intermezzo: Finding and Exploiting CVE-2025-3052</strong></p><p>Our Binarly Transparency Platform can automatically detect a wide range of UEFI-related vulnerabilities, including issues involving NVRAM variable handling. As a result, <strong>identifying the vulnerability behind CVE-2025-3052 was as simple as uploading the affected module to our platform</strong>. </p><p>The generated report (shown in the image below) immediately revealed the impact of this finding: the root cause behind the vulnerability is the use of untrusted pointers originating from an NVRAM variable. Additionally, our <a href="https://www.binarly.io/news/binarly-patents-new-method-for-reachability-analysis-for-binary-executables">patented reachability metric</a> confirmed that the vulnerable function is directly accessible from the module's entry point, thus likely exploitable.</p><figure><p><img src="https://cdn.prod.website-files.com/65dc9e6c7079139a2996b5d7/68436baad14dbc25aaa3c1f8_image9.png" loading="lazy" alt=""></p><figcaption><em>Figure 5. The report generated by the Binarly Transparency Platform.</em></figcaption></figure><p>The pseudocode shown in the next image, generated by our platform as evidence of the issue, enabled quick and effective triaging of the vulnerability. This pseudocode is generated using Binarly’s program analysis framework delivering comprehensive static analysis techniques. </p><p>The vulnerable function reads the value of the <em>IhisiParamBuffer</em> NVRAM variable and stores it in a global variable located at address <em>0xf7a0</em>. Later, the value of this global variable plus 0x18 is used as the target address for a memory write operation, setting it to zero. Subsequent writes are also attacker-controlled, since <em>var2</em> points to the same NVRAM variable (our platform generates independent findings for these additional write operations as well).</p><figure><p><img src="https://cdn.prod.website-files.com/65dc9e6c7079139a2996b5d7/68436cdccdac3447a48504c5_image5.webp" loading="lazy" alt=""></p><figcaption><em>Figure 6. The pseudo-code generated as evidence of the finding.</em></figcaption></figure><p>While this write capability is constrained, as it only allows writing zeros or a few constants to an arbitrary address, it provides a powerful primitive that can be exploited in a variety of creative ways. For our proof of concept (PoC), we chose to overwrite the global variable <a href="https://github.com/tianocore/edk2/blob/62cd7d338b389aa21c00f0fc35f14a6fa9ba23b0/MdeModulePkg/Core/Dxe/DxeMain.h#L250" target="_blank">gSecurity2</a>. This variable holds a pointer to the Security2 Architectural Protocol, which the LoadImage function uses to enforce Secure Boot. By setting it to zero, we <a href="https://github.com/tianocore/edk2/blob/f75e6952ca29c9d496c9d6fadaddb4c71cd0c4ed/MdeModulePkg/Core/Dxe/Image/Image.c#L1272" target="_blank">effectively disable Secure Boot</a>, allowing the execution of any unsigned UEFI modules.</p><p>One prerequisite for this vulnerability is that the IhisiParamBuffer variable must be writable. On Insyde-based devices, this variable is often locked and read-only. Therefore, unless there is another vulnerability (such as <a href="https://www.binarly.io/advisories/brly-2023-005">BRLY-2023-005</a>, which allows setting arbitrary variables even if locked) Insyde-based devices are not vulnerable. </p><p>All other devices, however, remain vulnerable. Thinking about it, this situation is <em>quite unique</em> and it highlights, once again, the complexities surrounding the UEFI supply chain security, where a mistake by one vendor can affect the entire ecosystem, <em>except for the vendor itself!</em></p><p>The following figure provides a high-level overview of an attack leveraging CVE-2025-3052. A privileged attacker can set the <em>IhisiParamBuffer</em> variable from the operating system (Step 1) and register the vulnerable module in the UEFI Boot Manager, or replace an existing OS loader with the malicious module. </p><p>Additionally, the attacker registers a second unsigned module containing the actual payload they intend to execute (Step 2). After the victim's device reboots (Step 3) and the firmware enters the Boot Device Selection (BDS) phase, the vulnerable module is executed. Its arbitrary write capability allows the attacker to overwrite <em>gSecurity2</em>, effectively disabling Secure Boot. With Secure Boot disabled, the second unsigned module is then loaded and executed by the firmware, granting the attacker arbitrary code execution at the end of the DXE phase (Step 4).</p><figure><p><img src="https://cdn.prod.website-files.com/65dc9e6c7079139a2996b5d7/6843692488f7dc517d59d828_image4.png" loading="lazy" alt=""></p><figcaption><em>Figure 7. The high-level overview of a PoC exploiting CVE-2025-3052</em></figcaption></figure><p>Based on the previous description, we recorded a demo video demonstrating the different stages of this PoC. One important aspect is that the PoC is transparent to the operating system: as shown at the end of the video (at minute 1:07), Secure Boot still appears to be enabled from the OS, even though it has been effectively bypassed.</p><figure><p><iframe allowfullscreen="true" frameborder="0" scrolling="no" src="https://www.youtube.com/embed/bX6TfPcccXk" title="Proof of Concept for CVE-2025-3052"></iframe></p></figure><p><strong>The Conclusion: Disclosure and Remediation for CVE-2025-3052</strong></p><p>After discovering this vulnerability, we opened a case with CERT/CC on February 26, 2025 to notify the affected parties and help provide a fix and detection for impacted Binarly customers. </p><p>During the triage process, Microsoft determined that the issue did not affect just a single module as initially believed, but actually 14 different modules. For this reason, the updated dbx released during the Patch Tuesday on June 10, 2025 contains 14 new hashes. </p><div><table>
  <thead>
    <tr>
      <th>Name</th>
      <th>Authenticode SHA256 hash</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>BiosFlashShell-efi64-80.02.efi</td>
      <td>C54A4060B3A76FA045B7B60EEAEBC8389780376BA3EF1F63D417BA1B5528E95</td>
    </tr>
    <tr>
      <td>BiosFlashShell-efi64-81.02.efi</td>
      <td>CBFAA286144EB2D165A6B17245BAD4F73058436C7292BE56DC6EBA29A369ADDF</td>
    </tr>
    <tr>
      <td>Dtbios-efi64-70.17.efi</td>
      <td>9D7E7174C281C6526B44C632BAA8C3320ADDD0C77DC90778CC14893882D74618</td>
    </tr>
    <tr>
      <td>Dtbios-efi64-70.18.efi</td>
      <td>9B1F35052CFC5FB06DABE5E8F7B747F081DA28D722DB59ADE253B9E38A7A3C76</td>
    </tr>
    <tr>
      <td>Dtbios-efi64-70.19.efi</td>
      <td>E3CE55E584371D3F2FBCA2241EF0711FF80876EBF71BAB07D8ECEE645A40DCFC</td>
    </tr>
    <tr>
      <td>Dtbios-efi64-70.20.efi</td>
      <td>EE093913ABBD34CB8B5EA31375179A8B55A298353C03AFE5055AA4E8E3F705EF</td>
    </tr>
    <tr>
      <td>Dtbios-efi64-70.21.efi</td>
      <td>B4E1880425F7857B741B921D04FD9276130927CF90A427C454B970E7A2F442F9</td>
    </tr>
    <tr>
      <td>Dtbios-efi64-70.22.efi</td>
      <td>CDA0B4A59390B36E1B654850428CBB5B4C7B5E4349E87ACDE97FB543736FF1D4</td>
    </tr>
    <tr>
      <td>Dtbios-efi64-71.17.efi</td>
      <td>C87EFD057497F90321D62A69B311912B8EF8A045FE9C5E6BD5C8C1A4E6F39629</td>
    </tr>
    <tr>
      <td>Dtbios-efi64-71.18.efi</td>
      <td>9E19DD645235341A555D6AC0665591453AE13918ECD37DF22DFBEE91EAA9A2DA</td>
    </tr>
    <tr>
      <td>Dtbios-efi64-71.19.efi</td>
      <td>63F67824FDA998798964FF33B87441857DA92F3A8EE3E04166EEC3156E4E6B82</td>
    </tr>
    <tr>
      <td>Dtbios-efi64-71.20.efi</td>
      <td>0BC4F078388D41AB039F87AE84CF8D39302CCBDD70C4ADEE02263EBF6A2DD328</td>
    </tr>
    <tr>
      <td>Dtbios-efi64-71.21.efi</td>
      <td>E2AEC271B9596A461EB6D54DB81785E4E4C615CFDA5F4504BCC0A329248A4D36</td>
    </tr>
    <tr>
      <td>Dtbios-efi64-71.22.efi</td>
      <td>6B4328EBCBE46ED9118FF2D4472DE329D70BA83016DF7A6F50F8AF92342160A1</td>
    </tr>
  </tbody>
</table></div><p>In terms of remediation, we recommend that impacted users update the <em>dbx</em> as soon as possible. In our previous research blog “<a href="https://www.binarly.io/blog/from-trust-to-trouble-the-supply-chain-implications-of-a-broken-dbx">From Trust to Trouble: The Supply Chain Implications of a Broken DBX</a>,” we described in detail the potential impact of outdated and inconsistent <em>dbx.</em></p><figure><p><iframe allowfullscreen="true" frameborder="0" scrolling="no" src="https://www.youtube.com/embed/TnECRMf2CoQ" title="Combining a Secure Boot Bypass with a Bootkit on Windows 11"></iframe></p></figure><p><strong>Detecting CVE-2025-3052</strong></p><p>Secure Boot bypasses continue to be a persistent issue within the UEFI ecosystem, with new vulnerabilities surfacing a few times each year. For example, in January 2025, <a href="https://www.welivesecurity.com/en/eset-research/under-cloak-uefi-secure-boot-introducing-cve-2024-7344/">ESET researchers disclosed</a> the details of another Secure Boot bypass &nbsp;(CVE-2024-7344). In addition, following the <a href="https://www.binarly.io/blog/from-trust-to-trouble-the-supply-chain-implications-of-a-broken-dbx">discrepancies we uncovered in the dbx</a> and their supply-chain implications that we discussed earlier this year, we decided to implement a new feature in our product specifically focused on Secure Boot security. This addition ensures that the Binarly Transparency Platform users are promptly informed with precise findings about any issues affecting Secure Boot, so that they can quickly remediate them.</p><p>Here is a demonstration of the Binarly Transparency Platform detecting and reporting on this Secure Boot bypass issue.</p><p>Binarly Deep Vulnerability Analysis (DVA) technology detects automatically CVE-2025-3052 as a previously unknown vulnerability (0-day):</p><p>Binarly Mitigate technology proactively detects signed vulnerable modules in DB/DBX databases to highlight potential high-impact risks:</p></div></div>]]></description>
        </item>
    </channel>
</rss>