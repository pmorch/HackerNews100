<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0">
    <channel>
        <title>HN100 - Readable Contents</title>
        <link>https://hn.algolia.com/api/v1/search_by_date?tags=%28story,poll%29&amp;numericFilters=points%3E100</link>
        <description>Uses Readability to add bodies to the RSS feed</description>
        <lastBuildDate>Thu, 26 Sep 2024 15:30:11 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>en</language>
        <item>
            <title><![CDATA[PostgreSQL 17 Released (165 pts)]]></title>
            <link>https://www.postgresql.org/about/news/postgresql-17-released-2936/</link>
            <guid>41657986</guid>
            <pubDate>Thu, 26 Sep 2024 13:10:24 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.postgresql.org/about/news/postgresql-17-released-2936/">https://www.postgresql.org/about/news/postgresql-17-released-2936/</a>, See on <a href="https://news.ycombinator.com/item?id=41657986">Hacker News</a></p>
Couldn't get https://www.postgresql.org/about/news/postgresql-17-released-2936/: Error: timeout of 10000ms exceeded]]></description>
        </item>
        <item>
            <title><![CDATA[Sam Altman: Long con was "child's play for me" (203 pts)]]></title>
            <link>https://old.reddit.com/r/AskReddit/comments/3cs78i/whats_the_best_long_con_you_ever_pulled/cszwpgq/</link>
            <guid>41657001</guid>
            <pubDate>Thu, 26 Sep 2024 11:13:35 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://old.reddit.com/r/AskReddit/comments/3cs78i/whats_the_best_long_con_you_ever_pulled/cszwpgq/">https://old.reddit.com/r/AskReddit/comments/3cs78i/whats_the_best_long_con_you_ever_pulled/cszwpgq/</a>, See on <a href="https://news.ycombinator.com/item?id=41657001">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><h6><a href="http://www.reddit.com/r/askreddit/submit?selftext=true&amp;title=%5BSerious%5D"> [ SERIOUS ] </a></h6>

<h5><a href="https://www.reddit.com/r/AskReddit/wiki/index#wiki_rules">Rules</a>:</h5>

<ol>
<li><p>You must post a clear and direct question in the title. The title may contain two, short, necessary context sentences.
No text is allowed in the textbox. Your thoughts/responses to the question can go in the comments section. <a href="https://www.reddit.com/r/AskReddit/wiki/index#wiki_-rule_1-">more &gt;&gt;</a></p></li>
<li><p>Any post asking for advice should be generic and not specific to your situation alone. <a href="https://www.reddit.com/r/AskReddit/wiki/index#wiki_-rule_2-">more &gt;&gt;</a></p></li>
<li><p>AskReddit is for open-ended discussion questions. <a href="https://www.reddit.com/r/AskReddit/wiki/index#wiki_-rule_3-">more &gt;&gt;</a></p></li>
<li><p>Posting, or seeking, any identifying personal information, real or fake, will result in a ban without a prior warning. <a href="https://www.reddit.com/r/AskReddit/wiki/index#wiki_-rule_4-">more &gt;&gt;</a></p></li>
<li><p>AskReddit is not your soapbox, personal army, or advertising platform. <a href="https://www.reddit.com/r/AskReddit/wiki/index#wiki_-rule_5-">more &gt;&gt;</a></p></li>
<li><p>[Serious] tagged posts are off-limits to jokes or irrelevant replies. <a href="https://www.reddit.com/r/AskReddit/wiki/index#wiki_-rule_6-">more &gt;&gt;</a></p></li>
<li><p>Soliciting money, goods, services, or favours is not allowed. <a href="https://www.reddit.com/r/AskReddit/wiki/index#wiki_-rule_7-">more &gt;&gt;</a></p></li>
<li><p>Mods reserve the right to remove content or restrict users' posting privileges as necessary if it is deemed detrimental to the subreddit or to the experience of others. <a href="https://www.reddit.com/r/AskReddit/wiki/index#wiki_-rule_8-">more &gt;&gt;</a></p></li>
<li><p>Comment replies consisting solely of images will be removed. <a href="https://www.reddit.com/r/AskReddit/wiki/index#wiki_-rule_9-">more &gt;&gt;</a></p></li>
<li><p>Do not post harmful misinformation. <a href="https://www.reddit.com/r/AskReddit/wiki/index/#wiki_-rule_10-">more &gt;&gt;</a></p></li>
<li><p>Spam, machine-generated content, and karma farming are not permitted. <a href="https://www.reddit.com/r/AskReddit/wiki/index/#wiki_-rule_11-">more &gt;&gt;</a></p></li>
</ol>

<h5>If you think your post has disappeared, see spam or an inappropriate post, please do not hesitate to <a href="https://www.reddit.com/message/compose?to=%2Fr%2FAskReddit">contact the mods</a>, we're happy to help.</h5>

<hr>

<h4>Tags to use:</h4>

<blockquote>
<h2><a href="https://www.reddit.com/r/AskReddit/wiki/index#wiki_-rule_6-">[Serious]</a></h2>
</blockquote>

<h3>Use a <strong>[Serious]</strong> post tag to designate your post as a serious, on-topic-only thread.</h3>



<h4>Filter posts by subject:</h4>

<p><a href="http://ud.reddit.com/r/AskReddit/#ud">Mod posts</a>
<a href="https://www.reddit.com/r/AskReddit/search/?q=flair%3Aserious&amp;sort=new&amp;restrict_sr=on&amp;t=all">Serious posts</a>
<a href="http://bu.reddit.com/r/AskReddit/#bu">Megathread</a>
<a href="http://nr.reddit.com/r/AskReddit/#nr">Breaking news</a>
<a href="https://old.reddit.com/r/AskReddit">Unfilter</a></p>



<h3>Please use spoiler tags to hide spoilers. <code>&gt;!insert spoiler here!&lt;</code></h3>



<h4>Other subreddits you might like:</h4>

<table><thead>
<tr>
<th>Related</th>
<th>Subreddits</th>
</tr>
</thead><tbody>
<tr>
<td><a href="https://reddit.com/r/AskReddit/wiki/sidebarsubs/#wiki_advice_and_relationships">Advice and Assistance</a></td>
<td><a href="https://reddit.com/r/AskReddit/wiki/sidebarsubs/#wiki_ask_a_______">Ask Others</a></td>
</tr>
<tr>
<td><a href="https://reddit.com/r/AskReddit/wiki/sidebarsubs/#wiki_askreddit_offshoots">AskReddit Offshoots</a></td>
<td><a href="https://reddit.com/r/AskReddit/wiki/sidebarsubs/#wiki_general_discussion">General Discussion</a></td>
</tr>
<tr>
<td><a href="https://reddit.com/r/AskReddit/wiki/sidebarsubs/#wiki_requests_.26amp.3B_assistance">Requests &amp; Assistance</a></td>
<td><a href="https://reddit.com/r/AskReddit/wiki/sidebarsubs/#wiki_what_is_this______">Help Me Identify This</a></td>
</tr>
<tr>
<td><a href="https://reddit.com/r/AskReddit/wiki/sidebarsubs/#wiki_reddit.2Fmeta">Reddit/Meta</a></td>
<td><a href="https://reddit.com/r/AskReddit/wiki/sidebarsubs/#wiki_find_subreddits">Find Subreddits</a></td>
</tr>
</tbody></table>



<h3>Ever read the reddiquette? <a href="https://old.reddit.com/wiki/reddiquette">Take a peek!</a></h3>


</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[OpenAI to Become For-Profit Company (519 pts)]]></title>
            <link>https://www.wsj.com/tech/ai/openai-chief-technology-officer-resigns-7a8b4639</link>
            <guid>41655954</guid>
            <pubDate>Thu, 26 Sep 2024 08:34:43 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.wsj.com/tech/ai/openai-chief-technology-officer-resigns-7a8b4639">https://www.wsj.com/tech/ai/openai-chief-technology-officer-resigns-7a8b4639</a>, See on <a href="https://news.ycombinator.com/item?id=41655954">Hacker News</a></p>
Couldn't get https://www.wsj.com/tech/ai/openai-chief-technology-officer-resigns-7a8b4639: Error: timeout of 10000ms exceeded]]></description>
        </item>
        <item>
            <title><![CDATA[Rewriting Rust (307 pts)]]></title>
            <link>https://josephg.com/blog/rewriting-rust/</link>
            <guid>41654871</guid>
            <pubDate>Thu, 26 Sep 2024 05:37:00 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://josephg.com/blog/rewriting-rust/">https://josephg.com/blog/rewriting-rust/</a>, See on <a href="https://news.ycombinator.com/item?id=41654871">Hacker News</a></p>
<div id="readability-page-1" class="page"><section>
                <p>The Rust programming language feels like a first generation product.</p>

<p>You know what I mean. Like the first iPhone - <a href="https://www.youtube.com/watch?v=MnrJzXM7a6o">which was amazing by the way</a>. They made an entire operating system around multitouch. A smart phone with no keyboard. And a working web browser. Within a few months, we all realised what the iPhone really wanted to be. Only, the first generation iphone wasn't quite there. It didn't have 3G internet. There was no GPS chip. And there was no app store. In the next few years, iPhones would get a lot better.</p>

<p>Rust feels a bit like that first iPhone.</p>

<p>I fell in love with Rust at the start. Algebraic types? Memory safety without compromising on performance? A modern package manager? Count me in. But now that I've been programming in rust for 4 years or so, it just feels like its never quite there.</p>

<p>And I don't know if it will ever be there. Progress on the language has slowed <em>so much</em>. When I first started using it, every release seemed to add new, great features in stable rust. Now? Crickets. The <a href="https://doc.rust-lang.org/unstable-book/the-unstable-book.html">rust "unstable book"</a> lists <em>700</em> different unstable features - which presumably are all implemented, but which have yet to be enabled in stable rust. Most of them are changes to the standard library - but seriously. Holy cow.</p>

<p>How much of this stuff will <em>ever</em> make it into the language proper? The rust RFC process is a graveyard of good ideas.</p>

<p>Features like <a href="https://doc.rust-lang.org/unstable-book/language-features/coroutines.html">Coroutines</a>. This RFC is 7 years old now. Make no mistake - coroutines are implemented in the compiler. They're just, not available for us "stable rust" peasants to use. If coroutines were a child, they would be in grade school by now. At this point, the coroutines RFC has lasted longer than World War 1 or 2.</p>

<p>I suspect rust is calcifying because its consensus process just doesn't scale. Early on, rust had a small group of contributors who just <em>decided</em> things. The monsters. Now, there are issue threads like <a href="https://github.com/rust-lang/rust/issues/93740#issuecomment-1041391284">this</a>, in which 25 smart, well meaning people spent 2 years and over 200 comments trying to figure out how to improve <code>Mutex</code>. And as far as I can tell, in the end they more or less gave up.</p>

<p>Maybe this is by design. Good languages are stable languages. It might be time to think of rust as a fully baked language - warts and all. Python 2.7 for life.</p>

<p>But that doesn't change anything for me. I want a better rust, and I feel powerless to make that happen. Where are my coroutines? Even javascript has <a href="https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Statements/function*">coroutines</a>.</p>

<h2 id="fantasylanguage">Fantasy language</h2>

<p>Sometimes I lie awake at night fantasising about forking the compiler. I know how I'd do it. In my fork, I'd leave all the rust stuff alone and but make my own "seph" <a href="https://doc.rust-lang.org/edition-guide/editions/">edition</a> of the rust language. Then I could add all sorts of breaking features to that edition. So long as my compiler still compiles mainline rust as well, I could keep using all the wonderful crates on Cargo.</p>

<p>I think about this a lot. If I did it, here's what I'd change:</p>

<h3 id="functiontraitseffects">Function traits (effects)</h3>

<p>Rust has traits on structs. These are used in all sorts of ways. Some are markers. Some are understood by the compiler (like <code>Copy</code>). Some are user defined.</p>

<p>Rust should also define a bunch of traits for functions. In other languages, function traits are called "effects".</p>

<p>This sounds weird at first glance - but hear me out. See, there's lots of different "traits" that functions have. Things like:</p>

<ul>
<li>Does the function ever panic?</li>
<li>Does the function have a fixed stack size?</li>
<li>Does the function run to the end, or does it yield / await?</li>
<li>If the function is a coroutine, what is the type of the continuation?</li>
<li>Is the function "pure" (ie, the same input produces the same output, and it has no side effects)</li>
<li>Does the function (directly or indirectly) run unsafe code in semi-trusted libraries?</li>
<li>Is the function guaranteed to terminate?</li>
</ul>

<p>And so on.</p>

<p>A function's parameters and return type are just associated types on the function:</p>

<pre><code>fn some_iter() -&gt; impl Iterator&lt;Item = usize&gt; {  
    vec![1,2,3].into_iter()
}

fn main() {  
    // Why doesn't this work already via FnOnce?
    let x: some_iter::Output = some_iter();
}
</code></pre>

<p><a href="https://rust-lang.github.io/rfcs/2515-type_alias_impl_trait.html">TAIT</a> eat your heart out.</p>

<p>Exposing these properties is super useful. For example, the linux kernel wants to guarantee (at compile time) that some block of code will never panic. This is impossible to do in rust today. But using function traits, we could explicitly mark a function as being able - or unable - to panic:</p>

<pre><code>#[disallow(Panic)] // Syntax TBD.
fn some_fn() { ... }  
</code></pre>

<p>And if the function does anything which could panic (even recursively), the compiler would emit an error.</p>

<p>The compiler already sort of implements traits on functions, like <code>Fn</code>, <code>FnOnce</code> and <code>FnMut</code>. But for some reason they're anemic. (Why??)</p>

<p>I want something like this:</p>

<pre><code>/// Automatically implemented on all functions.
trait Function {  
  type Args,
  type Output,
  type Continuation, // Unit type () for normal functions
  // ... and so on.

  fn call_once(self, args: Self::Args) -&gt; Self::Output;
}

trait NoPanic {} // Marker trait, implemented automatically by the compiler.

/// Automatically implemented on all functions which don't recurse.
trait KnownStackSize {  
  const STACK_SIZE: usize,
}
</code></pre>

<p>Then you could write code like this:</p>

<pre><code>fn some_iter() -&gt; impl Iterator&lt;Item = usize&gt; {  
  vec![1,2,3].into_iter();
}

struct SomeWrapperStruct {  
  iter: some_iter::Output, // In 2024 this is still impossible in stable rust.
}
</code></pre>

<p>Or with coroutines:</p>

<pre><code>coroutine fn numbers() -&gt; impl Iterator&lt;Item = usize&gt; {  
  yield 1;
  yield 2;
  yield 3;
}

coroutine fn double&lt;I: Iterator&lt;Item=usize&gt;&gt;(inner: I) -&gt; impl Iterator&lt;Item = usize&gt; {  
  for x in inner {
    yield x * 2;
  }
}

struct SomeStruct {  
  // Suppose we want to store the iterator. We can name it directly:
  iterator: double&lt;numbers&gt;::Continuation,
}
</code></pre>

<p>Or, say, take a function parameter but require that the parameter itself doesn't panic:</p>

<pre><code>fn foo&lt;F&gt;(f: F)  
    where F: NoPanic + FnOnce() -&gt; String
{ ... }
</code></pre>

<p>Yoshua Wuyts has an excellent <a href="https://blog.yoshuawuyts.com/extending-rusts-effect-system/">talk &amp; blog post</a> going into way more detail about effects - why they're useful and how this could work.</p>

<h3 id="compiletimecapabilities">Compile-time Capabilities</h3>

<p>Most rust projects pull in an insane number of 3rd party crates. Most of these crates are small utility libraries - like the <a href="https://crates.io/crates/human-size"><code>human-size</code></a> crate which formats file sizes for human consumption. Great stuff! But unfortunately, all of these little crates add supply chain risk. Any of those authors could push out an update which contains malicious code - cryptolockering our computers, our servers or sneaking bad code into our binaries.</p>

<p>I think this problem is similar to the problem of memory safety. Sure - its sometimes useful to write memory-unsafe code. The rust standard library is full of it. But rust's <code>unsafe</code> keyword lets authors opt in to potentially unsafe things. We only add <code>unsafe</code> blocks when its necessary.</p>

<p>Lets do the same thing for privileged function calls - like reading and writing to and from the filesystem or the network. This is useful stuff, but its potentially dangerous. Developers should actively whitelist code that is allowed to call these functions.</p>

<p>To implement this, first we want to add marker traits to all the security-sensitive functions in the standard library (opening a file from a string, <code>exec</code>, FFI, opening network connections, most unsafe functions that interact with raw pointers, and so on). So, for example, <a href="https://doc.rust-lang.org/std/fs/fn.write.html"><code>std::fs::write(path, contents)</code></a> writes to an arbitrary path on disk with the credentials of the user. We add some <code>#[cap(fs_write)]</code> marker tag to the function itself, marking that this can only be called from code which is in some way trusted. The compiler automatically "taints" any other functions which call <code>write</code> in the entire call tree.</p>

<p>Suppose I call a function in a 3rd party crate which needs the <code>fs_write</code> capability. In order to call that function, I need to explicitly whitelist that call. (Either by adding the permission explicitly in my <code>Cargo.toml</code> or maybe with an annotation at the call site).</p>

<p>So, lets say the <code>foo</code> crate contains a function like this. The function will be marked (tainted) with the "writes to filesystem" tag:</p>

<pre><code>// In crate `foo`.

// (this function is implicitly tagged with #[cap(fs_write)])
pub fn do_stuff() {  
  std::fs::write("blah.txt", "some text").unwrap();
}
</code></pre>

<p>When I try to run that function from my code:</p>

<pre><code>fn main() {  
  foo::do_stuff();
}
</code></pre>

<p>The compiler can give me a nice rusty error, like this:</p>

<pre><code>Error: foo::do_stuff() writes to the local filesystem, but the `foo` crate has not been trusted with this capability in Cargo.toml.

Tainted by this line in do_stuff:

  std::fs::write("blah.txt", "some text").unwrap();

Add this to your Cargo.toml to fix:

foo = { version = "1.0.0", allow_capabilities: ["fs_write"] }  
</code></pre>

<p>Obviously, most uses of <code>unsafe</code> would also require explicit whitelisting.</p>

<p>Most crates I use - like <code>human-size</code> or <code>serde</code> don't need any special capabilities to work. So we don't need to worry so much about their authors "turning evil" and adding malicious code to our software. Reducing the supply chain risk from the 100 or so crates I currently transitively depend on down to just a few would be massive.</p>

<p>This is a very simple, static way that capabilities could be introduced to Rust. But it might be possible &amp; better to change privileged code to require an extra <code>Capability</code> parameter (some unit struct type). And heavily restrict how <code>Capability</code> objects can be instantiated. Eg:</p>

<pre><code>struct FsWriteCapability;

impl FsWriteCapability {  
    fn new() { Self } // Only callable from the root crate
}

// Then change std::fs::write's signature to this:
pub fn write(path: Path, contents: &amp;[u8], cap: FsWriteCapability) { ... }  
</code></pre>

<p>This requires more boilerplate, but its much more flexible. (And obviously, we'd also need to, somehow, apply a similar treatment to <code>build.rs</code> scripts and <code>unsafe</code> blocks.)</p>

<p>The result of all of this is that utility crates become "uncorruptable". Imagine if crates.io is hacked and serde is maliciously updated to include with cryptolocker code. Today, that malicious code would be run automatically on millions of developer machines, and compiled into programs everywhere. With this change, you'd just get a compiler error.</p>

<p>This is huge, and singlehandedly this one feature is probably worth the cost of forking rust. At least, to someone. (Anyone want to sponsor this work?)</p>

<h3 id="pinmoveandstructborrows">Pin, Move and Struct Borrows</h3>

<blockquote>
  <p>Feel free to skip this section if Pin &amp; the borrow checker gives you a migraine.</p>
</blockquote>

<p><code>Pin</code> in rust is a weird, complicated hack to work around a hole in the borrow checker. Its a band-aid from the land of bizzaro choices that only make sense when you need to maintain backwards compatibility at all costs.</p>

<ul>
<li>Its the reverse of the trait you actually want. It would make way more sense to have a <code>Move</code> marker trait (like <code>Copy</code>) indicating objects which <em>can</em> move.</li>
<li>But <code>Pin</code> isn't an actual trait. There's only <code>Unpin</code> (double negative now) and <code>!Unpin</code> - which is not-not-not-<code>Move</code>. For example <a href="https://doc.rust-lang.org/1.81.0/src/core/marker.rs.html#923"><code>impl !Unpin for PhantomPinned</code></a>. Is <code>!Unpin</code> the same as <code>Pin</code>? Uhhhh, ... No? Because .. reasons? I get an instant headache when I think about this stuff. Here's the <a href="https://doc.rust-lang.org/std/marker/trait.Unpin.html">documentation for Unpin</a> if you want to try your luck.</li>
<li>Pin only applies to reference types. If you read through code which uses <code>Pin</code> a lot, you'll find unnecessary <code>Box</code>-ing of values <em>everywhere</em>. For example, <a href="https://docs.rs/tokio-stream/latest/src/tokio_stream/wrappers/broadcast.rs.html#11-18">in tokio</a>, or helper libraries like <a href="https://lib.rs/crates/ouroboros">ouroboros</a>, <a href="https://docs.rs/async-trait/latest/async_trait/">async<em>trait</em></a><em> and <a href="https://docs.rs/self_cell/latest/self_cell/">self</a></em><a href="https://docs.rs/self_cell/latest/self_cell/">cell</a>.</li>
<li>The pain spreads. Any function that takes a pinned value needs the value wrapped using some horrible abomonation <a href="https://doc.rust-lang.org/std/future/trait.Future.html">like <code>Future::poll(self: Pin&lt;&amp;mut Self&gt;, ..)</code></a>. And then you need to figure out how to read the actual values out using projections, which are so complicated there are <a href="https://docs.rs/pin-project/latest/pin_project/">multiple</a> <a href="https://crates.io/crates/pin-project-lite/">crates</a> for dealing with them. The pain cannot be confined. It spreads outwards, forever, corrupting everything.</li>
</ul>

<p>I swear, it took more effort to learn pinning in rust than it took me to learn the entire Go programming language. And I'm still not convinced I'm totally across it. And I'm not alone. I've heard the <a href="https://fuchsia.dev/">Fuchsia operating system project</a> abandoned Rust for C++ in some parts because of how impossibly complex Pin makes everything.</p>

<p>Why is <code>Pin</code> needed, anyway?</p>

<p>We can write rust functions like this:</p>

<pre><code>fn main() {  
    let x = vec![1,2,3];
    let y = &amp;x;

    //drop(x); // error[E0505]: cannot move out of `x` because it is borrowed
    dbg!(y);
}
</code></pre>

<p>All variables in a rust function are actually, secretly in one of 3 different states:</p>

<ul>
<li>Normal (owned)</li>
<li>Borrowed</li>
<li>Mutably borrowed</li>
</ul>

<p>While a variable is borrowed (<code>y = &amp;x</code>), you can't move, mutate or drop the variable. In this example, <code>x</code> is put into a special "borrowed" state throughout the lifetime of <code>y</code>. Variables in the "borrowed" state are pinned, immutable, and have a bunch of other constraints. This "borrowed state" is visible to the compiler, but its completely invisible to the programmer. You can't tell that something is borrowed until you try to compile your program. (Aside: I wish Rust IDEs made this state visible while programming!)</p>

<p>But at least this program <em>works</em>.</p>

<p>Unfortunately, there's no equivalent to this for structs. Lets turn the function <code>async</code>:</p>

<pre><code>async fn foo() {  
    let x = vec![1,2,3];
    let y = &amp;x;

    some_future().await;

    dbg!(y);
}
</code></pre>

<p>When you compile this, the compiler creates a hidden struct for you, which stores the suspended state of this function. It looks something like this:</p>

<pre><code>struct FooFuture {  
  x: Vec&lt;usize&gt;,
  y: &amp;'_ Vec&lt;usize&gt;,
}

impl Future for FooFuture { ... }  
</code></pre>

<p><code>x</code> is borrowed by <code>y</code>. So it needs to be placed under all the constraints of a borrowed variable:</p>

<ul>
<li>It must not move in memory. (It needs to be Pinned)</li>
<li>It must be immutable</li>
<li>We can't take mutable references to <code>x</code> (because of the &amp; xor &amp;mut rule).</li>
<li><code>x</code> must outlive <code>y</code>.</li>
</ul>

<p>But there's no syntax for this. Rust doesn't have syntax to mark a struct field as being in a borrowed state. And we can't express the lifetime of <code>y</code>.</p>

<p>Remember: the rust compiler already generates and uses structs like this whenever you use <code>async</code> functions. The compiler just doesn't provide any way to write code like this ourselves. Lets just extend the borrow checker and fix that!</p>

<p>I don't know what the ideal syntax would be, but I'm sure we can come up with something. For example, maybe <code>y</code> gets declared as a "local borrow", written as <code>y: &amp;'Self::x Vec&lt;usize&gt;</code>. The compiler uses that annotation to figure out that <code>x</code> is borrowed. And it puts it under the same set of constraints as a borrowed variable inside a function.</p>

<p>This would also let you work with self-referential structs, like an <a href="https://en.wikipedia.org/wiki/Abstract_syntax_tree">Abstract Syntax Tree (AST)</a> in a compiler:</p>

<pre><code>struct Ast {  
  source: String,
  ast_nodes: Vec&lt;&amp;'Self::source str&gt;,
}
</code></pre>

<p>This syntax could also be adapted to support partial borrows:</p>

<pre><code>impl Foo {  
  fn get_some_field&lt;'a&gt;(&amp;'a self) -&gt; &amp;'a::some_field usize {
    &amp;self.some_field
  }
}
</code></pre>

<p>This isn't a complete solution.</p>

<p>We'd also need a <code>Move</code> marker trait, to replace <code>Pin</code>. Any struct with borrowed fields can't be Moved - so it wouldn't have <code>impl Move</code>. I'd also consider a <code>Mover</code> trait, which would allow structs to intelligently move themselves in memory. Eg:</p>

<pre><code>trait Mover {  
  // Something like that.
  unsafe fn move(from: *Self, to: MaybeUninit&lt;&amp;mut Self&gt;);
}
</code></pre>

<p>We'd also need a sane, safe way to construct structs like this in the first place. I'm sure we can do better than <code>MaybeUninit</code>.</p>

<p>Miguel Young de la Sota <a href="https://www.youtube.com/watch?v=UrDhMWISR3w">gave a fantastic talk a few years ago</a> talking about <code>Move</code> in rust. But I think it would be much more "rusty" to lean on the borrow checker instead.</p>

<p>If you ask me, <code>Pin</code> is a dead end solution. Rust already has a borrow checker. Lets use it for structs.</p>

<h3 id="comptime">Comptime</h3>

<p>This is a hot opinion. I haven't spent a lot of time with zig, but at least from a distance I adore <a href="https://zig.guide/language-basics/comptime/">comptime</a>.</p>

<p>In the rust compiler we essentially implement two languages: Rust and the Rust Macro language. (Well, arguably there's 3 - because proc macros). The Rust programming language is lovely. But the rust macro languages are horrible.</p>

<p>But, if you already know rust, why not just use rust itself instead of sticking another language in there? This is the genius behind Zig's <code>comptime</code>. The compiler gets a little interpreter tacked on that can run parts of your code at compile time. Functions, parameters, if statements and loops can all be marked as compile-time code. Any non-comptime code in your block is emitted into the program itself.</p>

<p>I'm not going to explain the feature in full here. Instead, take in just how <em>gorgeous</em> this makes Zig's <a href="https://ziglang.org/documentation/master/#Case-Study-print-in-Zig">std <code>print</code> function</a>.</p>

<p>Its entirely implemented using comptime. So when you write this in zig:</p>

<pre><code>pub fn main() void {  
    print("here is a string: '{s}' here is a number: {}\n", .{ a_string, a_number });
}
</code></pre>

<p><code>print</code> takes the format string as a comptime parameter, and parses it within a <code>comptime</code> loop. Aside from a couple keywords, the function is just regular zig code - familiar to anyone who knows the language. It just gets executed within the compiler. And the result? It emits this beauty:</p>

<pre><code>pub fn print(self: *Writer, arg0: []const u8, arg1: i32) !void {  
    try self.write("here is a string: '");
    try self.printValue(arg0);
    try self.write("' here is a number: ");
    try self.printValue(arg1);
    try self.write("\n");
    try self.flush();
}
</code></pre>

<p>Read the <a href="https://ziglang.org/documentation/master/#Case-Study-print-in-Zig">full case study</a> for more details.</p>

<p>In comparison, I tried to look up how rust's <code>println!()</code> macro is implemented. But <a href="https://doc.rust-lang.org/src/std/macros.rs.html#138-145">println! calls some secret <code>format_args_nl</code> function</a>. I assume that function is hardcoded in the rust compiler itself.</p>

<p>Its not a great look when even the rust compiler authors don't want to use rust's macro language.</p>

<h3 id="weirdlittlefixes">Weird little fixes</h3>

<p>Bonus round time. Here's some other little "nits" I'd love to fix while we're at it:</p>

<ul>
<li><code>impl&lt;T: Copy&gt; for Range&lt;T&gt;</code>. If you know, you know.</li>
<li>Fix <a href="https://github.com/rust-lang/rust/issues/26925">derive with associated types</a>. <a href="https://play.rust-lang.org/?version=stable&amp;mode=debug&amp;edition=2021&amp;gist=6fd2c813f411f6eb1abb66a473425c89">Full example here</a>.</li>
<li>Make if-let expressions support logical AND. Its so simple, so obvious, and so useful. This should work:</li>
</ul>

<pre><code>// Compile error! We can't have nice things.
if let Some(x) = some_var &amp;&amp; some_expr { }  
</code></pre>

<p>You can sort of work around this problem today as below, but its awkward to write, hard to read and the semantics are different from how normal <code>if</code> statements work because it lacks short-circuit evaluation.</p>

<pre><code>// check_foo() will run even if some_var is None.
if let (Some(x), true) = (some_var, check_foo()) { ... }  
</code></pre>

<p><a href="https://play.rust-lang.org/?version=stable&amp;mode=debug&amp;edition=2021&amp;gist=e4c4521e6a0ab49462c0b9d55da97480">Full example here</a>.</p>

<p>Rust's ergonomics for raw pointers are also uniquely horrible. When I work with unsafe code, my code should be as easy to read &amp; write as humanly possible. But the rust compiler seems intent on punishing me for my sins. For example, if I have a reference to a struct in rust, I can write <code>myref.x</code>. But if I have a pointer, rust insists that I write <code>(*myptr).x</code> or, worse: <code>(*(*myptr).p).y</code>. Horrible. Horrible and entirely counterproductive. Unsafe code should be clear.</p>

<p>I'd also change all the built in collection types to take an <code>Allocator</code> as a constructor argument. I personally don't like Rust's decision to use a global allocator. Explicit is better than implicit.</p>

<h2 id="closingthoughts">Closing thoughts</h2>

<p>Thats all the ideas I have. I mean, async needs some love too. But there's so much to say on the topic that async deserves a post of its own.</p>

<p>Unfortunately, most of these changes would be incompatible with existing rust. Even adding security capabilities would require a new rust edition, since it introduces a new way that crates can break semver compatibility.</p>

<p>A few years ago I would have considered writing RFCs for all of these proposals. But I like programming more than I like dying slowly in the endless pit of github RFC comments. I don't want months of work to result in yet another idea in <a href="https://doc.rust-lang.org/reference/items/associated-items.html">rust's landfill of unrealised dreams</a>.</p>

<p>Maybe I should fork the compiler and do it myself. Urgh. So many projects. If I could live a million lifetimes, I'd devote one to working on compilers.</p>
            </section></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Show HN: Cronexpr, a Rust library to parse and iter crontab expression (112 pts)]]></title>
            <link>https://docs.rs/cronexpr/latest/cronexpr/</link>
            <guid>41654723</guid>
            <pubDate>Thu, 26 Sep 2024 05:10:46 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://docs.rs/cronexpr/latest/cronexpr/">https://docs.rs/cronexpr/latest/cronexpr/</a>, See on <a href="https://news.ycombinator.com/item?id=41654723">Hacker News</a></p>
Couldn't get https://docs.rs/cronexpr/latest/cronexpr/: Error: timeout of 10000ms exceeded]]></description>
        </item>
        <item>
            <title><![CDATA[Git-absorb: Git commit –fixup, but automatic (334 pts)]]></title>
            <link>https://github.com/tummychow/git-absorb</link>
            <guid>41653191</guid>
            <pubDate>Thu, 26 Sep 2024 00:12:53 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/tummychow/git-absorb">https://github.com/tummychow/git-absorb</a>, See on <a href="https://news.ycombinator.com/item?id=41653191">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><p dir="auto"><h2 tabindex="-1" dir="auto">git absorb</h2><a id="user-content-git-absorb" aria-label="Permalink: git absorb" href="#git-absorb"></a></p>
<p dir="auto">This is a port of Facebook's <a href="https://www.mercurial-scm.org/repo/hg/rev/5111d11b8719" rel="nofollow"><code>hg absorb</code></a>, which I first read about on <a href="https://groups.google.com/forum/#!msg/mozilla.dev.version-control/nh4fITFlEMk/ZNXgnAzxAQAJ" rel="nofollow">mozilla.dev.version-control</a>:</p>
<blockquote>
<ul dir="auto">
<li>Facebook demoed <code>hg absorb</code> which is probably the coolest workflow enhancement I've seen to version control in years. Essentially, when your working directory has uncommitted changes on top of draft changesets, you can run <code>hg absorb</code> and the uncommitted modifications are automagically folded ("absorbed") into the appropriate draft ancestor changesets. This is essentially doing <code>hg histedit</code> + "roll" actions without having to make a commit or manually make history modification rules. The command essentially looks at the lines that were modified, finds a changeset modifying those lines, and amends that changeset to include your uncommitted changes. If the changes can't be made without conflicts, they remain uncommitted. This workflow is insanely useful for things like applying review feedback. You just make file changes, run <code>hg absorb</code> and the mapping of changes to commits sorts itself out. It is magical.</li>
</ul>
</blockquote>
<p dir="auto"><h2 tabindex="-1" dir="auto">Elevator Pitch</h2><a id="user-content-elevator-pitch" aria-label="Permalink: Elevator Pitch" href="#elevator-pitch"></a></p>
<p dir="auto">You have a feature branch with a few commits. Your teammate reviewed the branch and pointed out a few bugs. You have fixes for the bugs, but you don't want to shove them all into an opaque commit that says <code>fixes</code>, because you believe in atomic commits. Instead of manually finding commit SHAs for <code>git commit --fixup</code>, or running a manual interactive rebase, do this:</p>
<div data-snippet-clipboard-copy-content="git add $FILES_YOU_FIXED
git absorb --and-rebase"><pre><code>git add $FILES_YOU_FIXED
git absorb --and-rebase
</code></pre></div>
<p dir="auto"><code>git absorb</code> will automatically identify which commits are safe to modify, and which staged changes belong to each of those commits. It will then write <code>fixup!</code> commits for each of those changes.</p>
<p dir="auto">With the <code>--and-rebase</code> flag, these fixup commits will be automatically integrated into the corresponding ones. Alternatively, you can check its output manually if you don't trust it, and then fold the fixups into your feature branch with git's built-in <a href="https://git-scm.com/docs/git-rebase#Documentation/git-rebase.txt---autosquash" rel="nofollow">autosquash</a> functionality:</p>
<div data-snippet-clipboard-copy-content="git add $FILES_YOU_FIXED
git absorb
git log # check the auto-generated fixup commits
git rebase -i --autosquash master"><pre><code>git add $FILES_YOU_FIXED
git absorb
git log # check the auto-generated fixup commits
git rebase -i --autosquash master
</code></pre></div>
<p dir="auto"><h2 tabindex="-1" dir="auto">Installing</h2><a id="user-content-installing" aria-label="Permalink: Installing" href="#installing"></a></p>
<p dir="auto">The easiest way to install <code>git absorb</code> is to download an artifact from the latest <a href="https://github.com/tummychow/git-absorb/releases">tagged release</a>. Artifacts are available for Windows, MacOS, and Linux (built on Ubuntu with statically linked libgit2). If you need a commit that hasn't been released yet, check the <a href="https://github.com/tummychow/git-absorb/actions/workflows/build.yml?query=event%3Apush+branch%3Amaster">latest CI artifact</a> or file an issue.</p>
<p dir="auto">Alternatively, <code>git absorb</code> is available in the following system package managers:</p>
<a href="https://repology.org/project/git-absorb/versions" rel="nofollow">
    <img src="https://camo.githubusercontent.com/782be13a15856ff171807dc4f47a704c26e7dd85ca9250a4f191dfdd1264fa03/68747470733a2f2f7265706f6c6f67792e6f72672f62616467652f766572746963616c2d616c6c7265706f732f6769742d6162736f72622e737667" alt="Packaging status" data-canonical-src="https://repology.org/badge/vertical-allrepos/git-absorb.svg">
</a>
<markdown-accessiblity-table><table>
<thead>
<tr>
<th>Repository</th>
<th>Command</th>
</tr>
</thead>
<tbody>
<tr>
<td>Arch Linux</td>
<td><code>pacman -S git-absorb</code></td>
</tr>
<tr>
<td>Debian</td>
<td><code>apt install git-absorb</code></td>
</tr>
<tr>
<td>DPorts</td>
<td><code>pkg install git-absorb</code></td>
</tr>
<tr>
<td>Fedora</td>
<td><code>dnf install git-absorb</code></td>
</tr>
<tr>
<td>FreeBSD Ports</td>
<td><code>pkg install git-absorb</code></td>
</tr>
<tr>
<td>Homebrew and Linuxbrew</td>
<td><code>brew install git-absorb</code></td>
</tr>
<tr>
<td>MacPorts</td>
<td><code>sudo port install git-absorb</code></td>
</tr>
<tr>
<td>nixpkgs stable and unstable</td>
<td><code>nix-env -iA nixpkgs.git-absorb</code></td>
</tr>
<tr>
<td>openSUSE</td>
<td><code>zypper install git-absorb</code></td>
</tr>
<tr>
<td>Ubuntu</td>
<td><code>apt install git-absorb</code></td>
</tr>
<tr>
<td>Void Linux</td>
<td><code>xbps-install -S git-absorb</code></td>
</tr>
<tr>
<td>GNU Guix</td>
<td><code>guix install git-absorb</code></td>
</tr>
</tbody>
</table></markdown-accessiblity-table>
<p dir="auto"><h2 tabindex="-1" dir="auto">Compiling from Source</h2><a id="user-content-compiling-from-source" aria-label="Permalink: Compiling from Source" href="#compiling-from-source"></a></p>
<p dir="auto"><a href="https://crates.io/crates/git-absorb" rel="nofollow"><img src="https://camo.githubusercontent.com/f476a92ce2878451af440486bc81ef2165b4c3717400beb4b8a9b189ddfcff10/68747470733a2f2f696d672e736869656c64732e696f2f6372617465732f762f6769742d6162736f72622e737667" alt="crates.io badge" data-canonical-src="https://img.shields.io/crates/v/git-absorb.svg"></a> <a href="https://github.com/tummychow/git-absorb/actions/workflows/build.yml"><img src="https://github.com/tummychow/git-absorb/actions/workflows/build.yml/badge.svg?branch=master&amp;event=push" alt="Build"></a></p>
<p dir="auto">You will need the following:</p>
<ul dir="auto">
<li><a href="https://github.com/rust-lang/cargo">cargo</a></li>
</ul>
<p dir="auto">Then <code>cargo install git-absorb</code>. Make sure that <code>$CARGO_HOME/bin</code> is on your <code>$PATH</code> so that git can find the command. (<code>$CARGO_HOME</code> defaults to <code>~/.cargo</code>.)</p>
<p dir="auto">Note that <code>git absorb</code> does <em>not</em> use the system libgit2. This means you do not need to have libgit2 installed to build or run it. However, this does mean you have to be able to build libgit2. (Due to <a href="https://github.com/alexcrichton/git2-rs/commit/76f4b74aef2bc2a54906ddcbf7fbe0018936a69d">recent changes</a> in the git2 crate, CMake is no longer needed to build it.)</p>
<p dir="auto">Note: <code>cargo install</code> does not currently know how to install manpages (<a href="https://github.com/rust-lang/cargo/issues/2729" data-hovercard-type="issue" data-hovercard-url="/rust-lang/cargo/issues/2729/hovercard">cargo#2729</a>), so if you use <code>cargo</code> for installation then <code>git absorb --help</code> will not work. Here is a manual workaround, assuming your system has a <code>~/.local/share/man/man1</code> directory that <code>man --path</code> knows about:</p>
<div data-snippet-clipboard-copy-content="wget https://raw.githubusercontent.com/tummychow/git-absorb/master/Documentation/git-absorb.1
mv git-absorb.1 ~/.local/share/man/man1"><pre><code>wget https://raw.githubusercontent.com/tummychow/git-absorb/master/Documentation/git-absorb.1
mv git-absorb.1 ~/.local/share/man/man1
</code></pre></div>
<p dir="auto"><h2 tabindex="-1" dir="auto">Usage</h2><a id="user-content-usage" aria-label="Permalink: Usage" href="#usage"></a></p>
<ol dir="auto">
<li><code>git add</code> any changes that you want to absorb. By design, <code>git absorb</code> will only consider content in the git index (staging area).</li>
<li><code>git absorb</code>. This will create a sequence of commits on <code>HEAD</code>. Each commit will have a <code>fixup!</code> message indicating the message (if unique) or SHA of the commit it should be squashed into.</li>
<li>If you are satisfied with the output, <code>git rebase -i --autosquash</code> to squash the <code>fixup!</code> commits into their predecessors. You can set the <a href="https://stackoverflow.com/a/29094904" rel="nofollow"><code>GIT_SEQUENCE_EDITOR</code></a> environment variable if you don't need to edit the rebase TODO file.</li>
<li>If you are not satisfied (or if something bad happened), <code>git reset --soft</code> to the pre-absorption commit to recover your old state. (You can find the commit in question with <code>git reflog</code>.) And if you think <code>git absorb</code> is at fault, please <a href="https://github.com/tummychow/git-absorb/issues/new">file an issue</a>.</li>
</ol>
<p dir="auto"><h2 tabindex="-1" dir="auto">How it works (roughly)</h2><a id="user-content-how-it-works-roughly" aria-label="Permalink: How it works (roughly)" href="#how-it-works-roughly"></a></p>
<p dir="auto"><code>git absorb</code> works by checking if two patches P1 and P2 <em>commute</em>, that is, if applying P1 before P2 gives the same result as applying P2 before P1.</p>
<p dir="auto"><code>git absorb</code> considers a range of commits ending at HEAD. The first commit can be specified explicitly with <code>--base &lt;ref&gt;</code>. By default the last 10 commits will be considered (see <a href="#configuration">Configuration</a> below for how to change this).</p>
<p dir="auto">For each hunk in the index, <code>git absorb</code> will check if that hunk commutes with the last commit, then the one before that, etc. When it finds a commit that does not commute with the hunk, it infers that this is the right parent commit for this change, and the hunk is turned into a fixup commit. If the hunk commutes with all commits in the range, it means we have not found a suitable parent commit for this change; a warning is displayed, and this hunk remains uncommitted in the index.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Configuration</h2><a id="user-content-configuration" aria-label="Permalink: Configuration" href="#configuration"></a></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Stack size</h3><a id="user-content-stack-size" aria-label="Permalink: Stack size" href="#stack-size"></a></p>
<p dir="auto">When run without <code>--base</code>, git-absorb will only search for candidate commits to fixup within a certain range (by default 10). If you get an error like this:</p>
<div data-snippet-clipboard-copy-content="WARN stack limit reached, limit: 10"><pre><code>WARN stack limit reached, limit: 10
</code></pre></div>
<p dir="auto">edit your local or global <code>.gitconfig</code> and add the following section</p>
<div dir="auto" data-snippet-clipboard-copy-content="[absorb]
    maxStack=50 # Or any other reasonable value for your project"><pre><span>[absorb]</span>
    <span>maxStack</span>=50 <span><span>#</span> Or any other reasonable value for your project</span></pre></div>
<p dir="auto"><h3 tabindex="-1" dir="auto">One fixup per fixable commit</h3><a id="user-content-one-fixup-per-fixable-commit" aria-label="Permalink: One fixup per fixable commit" href="#one-fixup-per-fixable-commit"></a></p>
<p dir="auto">By default, git-absorb will generate separate fixup commits for every absorbable hunk. Instead, can use the <code>-F</code> flag to create only 1 fixup commit for all hunks that absorb into the same commit.
To always have this behavior, set</p>
<div dir="auto" data-snippet-clipboard-copy-content="[absorb]
    oneFixupPerCommit = true"><pre><span>[absorb]</span>
    <span>oneFixupPerCommit</span> = true</pre></div>
<p dir="auto"><h3 tabindex="-1" dir="auto">Auto-stage all changes if nothing staged</h3><a id="user-content-auto-stage-all-changes-if-nothing-staged" aria-label="Permalink: Auto-stage all changes if nothing staged" href="#auto-stage-all-changes-if-nothing-staged"></a></p>
<p dir="auto">By default, git-absorb will only consider files that you've staged to the index via <code>git add</code>. However, sometimes one wants to try and absorb from all changes, which would require to stage them first via <code>git add .</code>. To avoid this extra step, set</p>
<div dir="auto" data-snippet-clipboard-copy-content="[absorb]
    autoStageIfNothingStaged = true"><pre><span>[absorb]</span>
    <span>autoStageIfNothingStaged</span> = true</pre></div>
<p dir="auto">which tells git-absorb, when no changes are staged, to auto-stage them all, create fixup commits where possible, and unstage remaining changes from the index.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Fixup target always SHA</h3><a id="user-content-fixup-target-always-sha" aria-label="Permalink: Fixup target always SHA" href="#fixup-target-always-sha"></a></p>
<p dir="auto">By default, git-absorb will create fixup commits with their messages pointing to the target commit's summary, and if there are duplicate summaries, will fallback to pointing to the target's SHA. Instead, can always point to the target's SHA via:</p>
<div dir="auto" data-snippet-clipboard-copy-content="[absorb]
    fixupTargetAlwaysSHA = true"><pre><span>[absorb]</span>
    <span>fixupTargetAlwaysSHA</span> = true</pre></div>
<p dir="auto"><h2 tabindex="-1" dir="auto">TODO</h2><a id="user-content-todo" aria-label="Permalink: TODO" href="#todo"></a></p>
<ul dir="auto">
<li>implement force flag</li>
<li>implement remote default branch check</li>
<li>add smaller force flags to disable individual safety checks</li>
<li>stop using <code>failure::err_msg</code> and ensure all error output is actionable by the user</li>
<li>slightly more log output in the success case</li>
<li>more tests (esp main module and integration tests)</li>
<li>document stack and commute details</li>
<li>more commutation cases (esp copy/rename detection)</li>
<li>don't load all hunks in memory simultaneously because they could be huge</li>
<li>implement some kind of index locking to protect against concurrent modifications</li>
</ul>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[WP Engine is banned from WordPress.org (144 pts)]]></title>
            <link>https://wordpress.org/news/2024/09/wp-engine-banned/</link>
            <guid>41652760</guid>
            <pubDate>Wed, 25 Sep 2024 22:59:26 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://wordpress.org/news/2024/09/wp-engine-banned/">https://wordpress.org/news/2024/09/wp-engine-banned/</a>, See on <a href="https://news.ycombinator.com/item?id=41652760">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
<p>Any WP Engine customers having trouble with their sites should <a href="https://wpengine.com/contact/">contact WP Engine support and ask them to fix it</a>.</p>



<p>I won’t bore you with the story of how WP Engine <a href="https://x.com/photomatt/status/1838502185879167069">broke thousands of customer sites yesterday in their haphazard attempt to block our attempts to inform the wider WordPress community</a> regarding their disabling and locking down a WordPress core feature in order to extract profit.</p>



<p><strong>What I will tell you is that, pending their legal claims and litigation against WordPress.org, WP Engine no longer has free access to WordPress.org’s resources.</strong></p>



<p>WP Engine wants to control your WordPress experience, they need to run their own user login system, update servers, plugin directory, theme directory, pattern directory, block directory, translations, photo directory, job board, meetups, conferences, bug tracker, forums, Slack, Ping-o-matic, and showcase. Their servers can no longer access our servers for free.</p>



<p>The reason WordPress sites don’t get hacked as much anymore is we work with hosts to block vulnerabilities at the network layer, WP Engine will need to replicate that security research on their own.</p>



<p>Why should WordPress.org provide these services to WP Engine for free, given their attacks on us?</p>



<p>WP Engine is free to offer their hacked up, bastardized simulacra of WordPress’s GPL code to their customers, and they can experience WordPress as WP Engine envisions it, with them getting all of the profits and providing all of the services.</p>



<p>If you want to experience WordPress, use any other host in the world besides WP Engine. <a href="https://wordpress.org/news/2024/09/wp-engine/">WP Engine is not WordPress</a>.</p>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[OpenAI to remove non-profit control and give Sam Altman equity (372 pts)]]></title>
            <link>https://www.reuters.com/technology/artificial-intelligence/openai-remove-non-profit-control-give-sam-altman-equity-sources-say-2024-09-25/</link>
            <guid>41651548</guid>
            <pubDate>Wed, 25 Sep 2024 20:31:48 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.reuters.com/technology/artificial-intelligence/openai-remove-non-profit-control-give-sam-altman-equity-sources-say-2024-09-25/">https://www.reuters.com/technology/artificial-intelligence/openai-remove-non-profit-control-give-sam-altman-equity-sources-say-2024-09-25/</a>, See on <a href="https://news.ycombinator.com/item?id=41651548">Hacker News</a></p>
Couldn't get https://www.reuters.com/technology/artificial-intelligence/openai-remove-non-profit-control-give-sam-altman-equity-sources-say-2024-09-25/: Error: Request failed with status code 401]]></description>
        </item>
        <item>
            <title><![CDATA[Mira Exits OpenAI (745 pts)]]></title>
            <link>https://twitter.com/miramurati/status/1839025700009030027</link>
            <guid>41651038</guid>
            <pubDate>Wed, 25 Sep 2024 19:35:31 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://twitter.com/miramurati/status/1839025700009030027">https://twitter.com/miramurati/status/1839025700009030027</a>, See on <a href="https://news.ycombinator.com/item?id=41651038">Hacker News</a></p>
&lt;Unparsable&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[Show HN: Httpdbg – A tool to trace the HTTP requests sent by your Python code (169 pts)]]></title>
            <link>https://github.com/cle-b/httpdbg</link>
            <guid>41650905</guid>
            <pubDate>Wed, 25 Sep 2024 19:18:12 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/cle-b/httpdbg">https://github.com/cle-b/httpdbg</a>, See on <a href="https://news.ycombinator.com/item?id=41650905">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><p dir="auto"><h2 tabindex="-1" dir="auto">httpdbg</h2><a id="user-content-httpdbg" aria-label="Permalink: httpdbg" href="#httpdbg"></a></p>
<p dir="auto"><code>httpdbg</code> is a tool for Python developers to easily debug the HTTP(S) client requests in a Python program.</p>
<p dir="auto">To use it, execute your program using the <code>pyhttpdbg</code> command instead of <code>python</code> and that's it. Open a browser to <code>http://localhost:4909</code> to view the requests:</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/cle-b/httpdbg/blob/main/ui.png?raw=true"><img src="https://github.com/cle-b/httpdbg/raw/main/ui.png?raw=true" alt=""></a></p>
<p dir="auto">Full documentation =&gt; <a href="https://httpdbg.readthedocs.io/en/latest/" rel="nofollow">https://httpdbg.readthedocs.io/</a></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">installation</h2><a id="user-content-installation" aria-label="Permalink: installation" href="#installation"></a></p>

<p dir="auto"><h2 tabindex="-1" dir="auto">usage</h2><a id="user-content-usage" aria-label="Permalink: usage" href="#usage"></a></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">interactive console</h3><a id="user-content-interactive-console" aria-label="Permalink: interactive console" href="#interactive-console"></a></p>
<p dir="auto">Open an interactive console using the command <code>pyhttpdbg</code>.</p>
<div dir="auto" data-snippet-clipboard-copy-content="(venv) dev@host:~/dir$ pyhttpdbg 
.... - - .--. -.. -... --. .... - - .--. -.. -... --. .... - - .--. -.. -... --.
  httpdbg - HTTP(S) requests available at http://localhost:4909/
.... - - .--. -.. -... --. .... - - .--. -.. -... --. .... - - .--. -.. -... --.
Python 3.10.6 (main, Aug 10 2022, 11:40:04) [GCC 11.3.0] on linux
Type &quot;help&quot;, &quot;copyright&quot;, &quot;credits&quot; or &quot;license&quot; for more information.
(InteractiveConsole)
>>> "><pre><span>(venv) dev@host:~/dir</span>$ <span>pyhttpdbg </span>
<span>.... - - .--. -.. -... --. .... - - .--. -.. -... --. .... - - .--. -.. -... --.</span>
<span>  httpdbg - HTTP(S) requests available at http://localhost:4909/</span>
<span>.... - - .--. -.. -... --. .... - - .--. -.. -... --. .... - - .--. -.. -... --.</span>
<span>Python 3.10.6 (main, Aug 10 2022, 11:40:04) [GCC 11.3.0] on linux</span>
<span>Type "help", "copyright", "credits" or "license" for more information.</span>
<span>(InteractiveConsole)</span>
<span>&gt;&gt;&gt; </span></pre></div>
<p dir="auto">Perform HTTP requests.</p>
<p dir="auto">You can inspect the HTTP requests directly in your web browser at <a href="http://localhost:4909/" rel="nofollow">http://localhost:4909</a>.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">script</h3><a id="user-content-script" aria-label="Permalink: script" href="#script"></a></p>
<p dir="auto">You can trace all the HTTP requests performed by a script</p>
<div dir="auto" data-snippet-clipboard-copy-content="pyhttpdbg --script filename.py [arg1 --arg2 ...]"><pre><span>pyhttpdbg --script filename.py [arg1 --arg2 ...]</span></pre></div>
<p dir="auto"><h3 tabindex="-1" dir="auto">pytest</h3><a id="user-content-pytest" aria-label="Permalink: pytest" href="#pytest"></a></p>
<p dir="auto">You can trace all the HTTP requests performed during your tests</p>
<div dir="auto" data-snippet-clipboard-copy-content="pyhttpdbg -m pytest [arg1 --arg2 ...]"><pre><span>pyhttpdbg -m pytest [arg1 --arg2 ...]</span></pre></div>
<p dir="auto">If you use the <code>pytest-xdist</code> plugin to execute your tests in parallel, then you must install the <code>pytest-httpdbg</code> plugin if you want to trace the requests done by the pytest workers.</p>
<div dir="auto" data-snippet-clipboard-copy-content="pip install httpdbg[pytest]"><pre><span>pip install httpdbg[pytest]</span></pre></div>
<p dir="auto"><h3 tabindex="-1" dir="auto">module</h3><a id="user-content-module" aria-label="Permalink: module" href="#module"></a></p>
<p dir="auto">You can trace all the HTTP requests performed by a library module run as a script using the <code>-m</code> command line argument.</p>
<p dir="auto">For example, you can view which HTTP requests are performed by <code>pip</code> when you install a package.</p>
<div dir="auto" data-snippet-clipboard-copy-content="pyhttpdbg -m pip install hookdns --upgrade"><pre><span>pyhttpdbg -m pip install hookdns --upgrade</span></pre></div>
<p dir="auto"><h2 tabindex="-1" dir="auto">Initiators</h2><a id="user-content-initiators" aria-label="Permalink: Initiators" href="#initiators"></a></p>
<p dir="auto">An initiator is the function/method that is at the origin of the HTTP requests. By default, we already support some packages but you can add your own initiators.</p>
<p dir="auto">To add a new package in the list of initiators, you can use the <code>-i</code> command line argument:</p>
<div dir="auto" data-snippet-clipboard-copy-content="pyhttpdbg -i api_client_pck --script my_script.py"><pre><span>pyhttpdbg -i api_client_pck --script my_script.py</span></pre></div>
<p dir="auto">You can use any package as an initiator, this is not limited to HTTP requests.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Already supported packages</h3><a id="user-content-already-supported-packages" aria-label="Permalink: Already supported packages" href="#already-supported-packages"></a></p>
<markdown-accessiblity-table><table>
<thead>
<tr>
<th>packages</th>
<th>status</th>
</tr>
</thead>
<tbody>
<tr>
<td>requests</td>
<td>supported</td>
</tr>
<tr>
<td>urllib3</td>
<td>supported</td>
</tr>
<tr>
<td>httpx</td>
<td>supported</td>
</tr>
<tr>
<td>aiohttp</td>
<td>supported</td>
</tr>
<tr>
<td>pytest</td>
<td>supported</td>
</tr>
<tr>
<td><em>your_package</em></td>
<td>yes, with the arg <em>-i your_package</em></td>
</tr>
</tbody>
</table></markdown-accessiblity-table>
<p dir="auto"><h2 tabindex="-1" dir="auto">configuration</h2><a id="user-content-configuration" aria-label="Permalink: configuration" href="#configuration"></a></p>
<p dir="auto">No configuration is necessary to start but some few settings are available for particular use.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">command line</h3><a id="user-content-command-line" aria-label="Permalink: command line" href="#command-line"></a></p>
<div dir="auto" data-snippet-clipboard-copy-content="usage: pyhttpdbg [-h] [--port PORT] [--version] [--initiator INITIATOR] [--keep-up | --force-quit]
                 [--console | --module MODULE | --script SCRIPT]

httdbg - a very simple tool to debug HTTP(S) client requests

options:
  -h, --help            show this help message and exit
  --port PORT, -p PORT  the web interface port
  --version, -v         print the httpdbg version
  --initiator INITIATOR, -i INITIATOR
                        add a new initiator (package)
  --keep-up, -k         keep the server up even if the requests have been read
  --force-quit, -q      stop the server even if the requests have not been read
  --console             run a python console (default)
  --module MODULE, -m MODULE
                        run library module as a script (the next args are passed to pytest as is)
  --script SCRIPT       run a script (the next args are passed to the script as is)"><pre><span>usage: pyhttpdbg [-h] [--port PORT] [--version] [--initiator INITIATOR] [--keep-up | --force-quit]</span>
<span>                 [--console | --module MODULE | --script SCRIPT]</span>

<span>httdbg - a very simple tool to debug HTTP(S) client requests</span>

<span>options:</span>
<span>  -h, --help            show this help message and exit</span>
<span>  --port PORT, -p PORT  the web interface port</span>
<span>  --version, -v         print the httpdbg version</span>
<span>  --initiator INITIATOR, -i INITIATOR</span>
<span>                        add a new initiator (package)</span>
<span>  --keep-up, -k         keep the server up even if the requests have been read</span>
<span>  --force-quit, -q      stop the server even if the requests have not been read</span>
<span>  --console             run a python console (default)</span>
<span>  --module MODULE, -m MODULE</span>
<span>                        run library module as a script (the next args are passed to pytest as is)</span>
<span>  --script SCRIPT       run a script (the next args are passed to the script as is)</span></pre></div>
<p dir="auto"><h3 tabindex="-1" dir="auto">web interace</h3><a id="user-content-web-interace" aria-label="Permalink: web interace" href="#web-interace"></a></p>
<p dir="auto">Clic on the <strong>⚙</strong> button on the top right of the page.</p>
<p dir="auto">Some options are available:</p>
<ul dir="auto">
<li>Hide the netloc in the url</li>
<li>Hide the initiator rows</li>
</ul>
<p dir="auto">To keep your configuration, bookmark the page with the full search query.</p>
<p dir="auto">Fox example, if you want to hide the initiator rows by default, the url will be:</p>
<div data-snippet-clipboard-copy-content="http://localhost:4909/?hi=on"><pre><code>http://localhost:4909/?hi=on
</code></pre></div>
<p dir="auto"><h2 tabindex="-1" dir="auto">web interface</h2><a id="user-content-web-interface" aria-label="Permalink: web interface" href="#web-interface"></a></p>
<p dir="auto">All the requests recorded are available on the web interface.</p>
<p dir="auto">The requests:</p>
<ul dir="auto">
<li>are still available in the web page even if the python process stopped (except if you force quit before the requests have been loaded by the web page).</li>
<li>are automatically cleaned if a new execution is detected.</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">documentation</h2><a id="user-content-documentation" aria-label="Permalink: documentation" href="#documentation"></a></p>
<p dir="auto"><a href="https://httpdbg.readthedocs.io/" rel="nofollow">https://httpdbg.readthedocs.io</a></p>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Eliminating Memory Safety Vulnerabilities at the Source (266 pts)]]></title>
            <link>https://security.googleblog.com/2024/09/eliminating-memory-safety-vulnerabilities-Android.html</link>
            <guid>41650647</guid>
            <pubDate>Wed, 25 Sep 2024 18:49:03 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://security.googleblog.com/2024/09/eliminating-memory-safety-vulnerabilities-Android.html">https://security.googleblog.com/2024/09/eliminating-memory-safety-vulnerabilities-Android.html</a>, See on <a href="https://news.ycombinator.com/item?id=41650647">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-version="1" id="header">
<div>
<p><a href="https://security.googleblog.com/">
<img height="50" src="https://www.gstatic.com/images/branding/googlelogo/2x/googlelogo_color_150x54dp.png">
</a></p><a href="https://security.googleblog.com/">
<h2>
            Security Blog
          </h2>
</a>
</div>
<p>
The latest news and insights from Google on security and safety on the Internet
</p>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[How to avoid a BSOD on your 2B dollar spacecraft (158 pts)]]></title>
            <link>https://clarkwakeland.com/blog/2024/avoiding-a-BSOD-on-your-satellite/</link>
            <guid>41650534</guid>
            <pubDate>Wed, 25 Sep 2024 18:40:26 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://clarkwakeland.com/blog/2024/avoiding-a-BSOD-on-your-satellite/">https://clarkwakeland.com/blog/2024/avoiding-a-BSOD-on-your-satellite/</a>, See on <a href="https://news.ycombinator.com/item?id=41650534">Hacker News</a></p>
<div id="readability-page-1" class="page"><article> <div id="markdown-content"> <div> <figure> <picture> <source srcset="https://clarkwakeland.com/assets/img/satellitesafemode-480.webp 480w,https://clarkwakeland.com/assets/img/satellitesafemode-800.webp 800w,https://clarkwakeland.com/assets/img/satellitesafemode-1400.webp 1400w," sizes="95vw" type="image/webp"> <img src="https://clarkwakeland.com/assets/img/satellitesafemode.png" width="100%" height="auto" title="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <p>Short answer: turn it off and turn it back on</p> <p>Long Answer…</p> <h3 id="background">Background</h3> <p>The lifecycle of most spacecraft consists of a final phase where all the systems are tested to various levels of synergy. One of the most important and complex set of tests are the <strong>C</strong>losed <strong>L</strong>oop <strong>T</strong>ests (<strong>CLT</strong>s), where the spacecraft is sent simulated orbital data, and then its attitude response is observed. It’s a closed loop because the attitude telemetry is fed back into the simulation while the test is occurring, effectively making the spacecraft and whatever hardware is currently being used part of the simulation. This particular test involved observing the response from control thrusters on the spacecraft when commanded to perform a slew and engine burn to a transfer orbit.</p> <p>To get the spacecraft response data back to in the loop, a set of memory addresses mapped to the sim needs to be uploaded onto the spacecraft RAM. These memory addresses were not determined while this test was being developed. Instead, a set of placeholder addresses meant for the previous spacecraft was used in the development environment.</p> <p>Ok, fair enough. When we’re developing our tests prior to them being run on the spacecraft, it’s useful to have <em>something</em> to upload, even if it’s not the final product.</p> <h2 id="the-vehicle">The Vehicle</h2> <p>The placeholder memory addresses were the <strong>actual memory addresses used for a previously developed spacecraft</strong>. As such, the name of the file containing the addresses was something like “XProp_transducer_addr_val.upx”. In all of the review meetings, this was most likely ignored because of how official it looked. All the engineers who worked on the last spacecraft assumed the naming convention was the same on this one, and those like me who were working on their first spacecraft had no other point of reference.</p> <p>The day comes and it’s finally time to run this test. Naturally, we get an unexplainable error, which is something I wish I could say we weren’t used to. The standard procedure is to stop the test and asses the vehicle state before deciding if we can redo the test or continue with other tests on different systems. We decide to run a seperate test but are still getting some strange errors, so we agree to turn the spacecraft off and back on to get it in a nominal configuration. Again, fairly standard procedure.</p> <p>Except turning the spacecraft off and on isn’t as simple as just flipping a switch or unplugging it. There are about a dozen steps that need to happen in a fairly specific order to ensure no hardware gets damaged in the process. We’re constantly checking telemetry during this teardown to see that nothing is on when a component further down the process is about to turn off.</p> <p>In our teardown script, we get an error we’ve never seen before. Some of the motors on the vehicle are not turning off. Ok, that’s weird. Let’s send the off command again, maybe there was a routing failure. Still nothing, but the commands are showing as “received”. Hmmmm. Well, unfortunately we can’t just continue, but we’ve got a few ideas as to what’s happening.</p> <p>Some of the telemetry that’s being downlinked from the satellite is displaying as active but showing no variation. I.e., a voltage or temperature reading is actively being downlinked as the same value, down to five sig figs, with no change. Further inspection of telemetry shows errors on the connection between the onboard computer and the ERIU.</p> <p>The <strong>E</strong>nhanced <strong>R</strong>emote <strong>I</strong>nterface <strong>U</strong>nits (<strong>ERIU</strong>s) are how the computer onboard the spacecraft communicates with all the different sensors and systems that read actual orbit and mission data. Think of the ERIUs as the pony express, relaying commands from the onboard computer to the wild west frontier of the satellite sensors. The sensors in turn send their telemetry back through the ERIU to the onboard computer for processing, or anywhere else that was specified in memory, like a CLT sim.</p> <div> <div> <figure> <picture> <source srcset="https://clarkwakeland.com/assets/img/ponyExpress-480.webp 480w,https://clarkwakeland.com/assets/img/ponyExpress-800.webp 800w,https://clarkwakeland.com/assets/img/ponyExpress-1400.webp 1400w," sizes="95vw" type="image/webp"> <img src="https://clarkwakeland.com/assets/img/ponyExpress.jpg" width="100%" height="auto" title="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <p> They don't make ERIUs like they used to </p> </div> <p>I think you can see where this is going. The addresses we loaded at the start of the test are meant for a completely different spacecraft and simulation setup. When the ERIU tried to send telemetry back to the sim, the addresses it was given weren’t actually pointing to anything, and it bascially throws a null pointer dereference and crashes. Some bus communication quirk is causing the last valid telemetry broadcast by the ERIU to be continously sent to the onboard computer.</p> <h2 id="the-problems">The Problems</h2> <p>There are two significant problems that are immediately apparent to us:</p> <ol> <li>Because the ERIU has crashed, we have no way of commanding different subsystems off, and cannot safely enter a powered off vehicle state.</li> <li>We are no longer getting active telemetry from the vehicle, which means that if something bad were happening to any system, we would not know about it.</li> </ol> <p>Problem #1 can luckily be put off for the time being while we focus on the more time sensitive problem #2. I should clarify that the vehicle is in a very safe and stable configuration at the moment. There is almost zero chance that something significant would break while we’re in this tricky spot of not recieving valid telemetry. That being said, as an engineer, having no way of describing your state is incredibly concering. An apt comparison would be if you were the flying an airplane at cruise altitude and your altimeter, airspeed indicator, and attitude indicator suddenly stopped updating. You’re <em>probably</em> not going to have anything bad happen to you, and there’s not much reason to believe your state is rapidly changing, but I doubt any pilot would want to experience that.</p> <p>Ok, we’ve got a clear goal: reboot the ERIU. Just like waking up for a morning swim, it is much easier said than done. In fact, the only way to reboot the ERIU is to reboot the whole damn onboard computer. This of course comes with its own set of issues and risks.</p> <p>The onboard computer, as part of its normal operation, continuously restarts a watchdog timer. If the watchdog timer has not been restarted and instead times out after ~30 seconds, the satellite enters something called <strong>safemode</strong>. Safemode is when all non critical functions are automatically shut down and the satellite becomes entirely focused on generating power by pointing its solar panels towards the Sun and trying to reestablish any communication that was lost. It’s a state the vehicle goes into when something bad happens, like total loss of attitude control or some other system failure.</p> <div> <div> <figure> <picture> <source srcset="https://clarkwakeland.com/assets/img/vaderfighter-480.webp 480w,https://clarkwakeland.com/assets/img/vaderfighter-800.webp 800w,https://clarkwakeland.com/assets/img/vaderfighter-1400.webp 1400w," sizes="95vw" type="image/webp"> <img src="https://clarkwakeland.com/assets/img/vaderfighter.gif" width="100%" height="auto" title="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <p> Vader's tie fighter entering safemode </p> </div> <p>Safemode is the satellite equivalent of a blue screen of death. An unexpected safemode occurring on the satellite during testing is something that must be communicated to the customer, even though it’s completely recoverable. Again, I wish I could say this hasn’t happened before. Long story short, the US government isn’t burning taxpayer dollars on a ten figure spaceship just to have us push a Crowdstrike update on it. They would be pretty upset, to put it lightly.</p> <p>Alright. Let’s not focus on the worst outcome. We can disable the watchdog timer, turn off some other telemetry, finally power cycle the onboard computer, and then turn everything else back on. Oh, and we need to do the exact same thing at the same time on the redundant computer as well. And there are a couple dozen commands in this manually created sequence, and getting one of them wrong could send the satellite to safemode. We got this. Did I mention that all of this is happening at midnight on a Saturday?</p> <p>After confirming our commands with a very tired flight software lead, we send the sequence, and it works! The onboard computer is back on, the ERIU is back on, and we’re getting what looks like reasonable telemetry from the other systems. We continue with the power off sequence and sure enough everything is powering down as expected. After ~12 hours of troubleshooting and communicating with other system engineers, we finally power down the vehicle safely at 1:45am.</p> <h2 id="retrospective">Retrospective</h2> <p>I hope I managed to explain the relevant systems in enough detail and not use too many acronyms, and at least explain the ones I did use. Working on space systems has exposed me to a seemingly infinite amount of different acronyms, some of which are the same but have different meanings. There was also a lot I glossed over, with the main time sink being the proper identification of the issue. There were many red herrings that were chased before we correctly determined that it was an ERIU crash. This would have ended much worse without the support of the amazing systems engineers who answered our calls in the middle of the night on a Saturday.</p> <p>I think what surprised me the most was how nonchalant the response was. We had documented all of our actions, so other people had read what happened and knew something had gone on. I wasn’t expecting any fanfare but we weren’t even debriefed on what happened. I guess this is the event that really got the point across to me about how if you do your job right, it’ll be like nothing ever happened. But I’ll take that over a BSOD on a multi billion dollar satellite any day.</p> </div> </article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Orion, Our First True Augmented Reality Glasses (1086 pts)]]></title>
            <link>https://about.fb.com/news/2024/09/introducing-orion-our-first-true-augmented-reality-glasses/</link>
            <guid>41650047</guid>
            <pubDate>Wed, 25 Sep 2024 17:56:53 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://about.fb.com/news/2024/09/introducing-orion-our-first-true-augmented-reality-glasses/">https://about.fb.com/news/2024/09/introducing-orion-our-first-true-augmented-reality-glasses/</a>, See on <a href="https://news.ycombinator.com/item?id=41650047">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
		<p><span>Five years ago, we announced to the world that we were building AR glasses. We don’t think people should have to make the choice between a world of information at your fingertips and being present in the physical world around you.</span></p>
<p><span>That’s why today, we’re unveiling </span><a href="https://about.meta.com/realitylabs/orion"><span>Orion</span></a><span>, which we believe is the most advanced pair of AR glasses ever made. Orion bridges the physical and virtual worlds, putting people at the center so they can be more present, connected and empowered in the world.&nbsp;&nbsp;&nbsp;</span></p>

<p><span>There are three primary reasons why AR glasses are key to unlocking the next great leap in human-oriented computing.</span></p>
<ul>
<li><span>They enable digital experiences that are unconstrained by the limits of a smartphone screen. With large holographic displays, you can use the physical world as your canvas, placing 2D and 3D content and experiences anywhere you want.</span></li>
<li><span>They seamlessly integrate contextual AI that can sense and understand the world around you in order to anticipate and proactively address your needs.</span></li>
<li><span>They’re lightweight and great for both indoor and outdoor use, and they let people see each other’s face, eyes and expressions.</span></li>
</ul>
<p>This slideshow requires JavaScript.</p>
<p><span>That’s the north star our industry has been building towards: a product combining the convenience and immediacy of wearables with a large display, high-bandwidth input and contextualized AI in a form that people feel comfortable wearing in their daily lives.&nbsp;</span></p>
<h2>The Evolution of Smart Glasses</h2>
<p><span>Ray-Ban Meta glasses have demonstrated the power of giving people hands-free access to key parts of their digital lives from their physical ones. We can talk to a smart AI assistant, connect with friends and capture the moments that matter – all without ever having to pull out a phone.</span></p>
<p><span>Yet while Ray-Ban Meta opened up an entirely new category of display-less glasses super-charged by AI, the XR industry has long dreamt of true AR glasses – a product that combines the benefits of a large holographic display and personalized AI assistance in a comfortable, all-day wearable form factor. Orion rises to the challenge.</span></p>
<h2>Groundbreaking AR Display in an Unparalleled Form</h2>
<p><span>We’ve been hard at work for years to take the incredible spatial experiences afforded by VR and MR headsets and miniaturize the technology necessary to deliver those experiences in a pair of lightweight, stylish glasses.&nbsp;</span></p>
<p><span>Nailing the form factor, delivering holographic displays, developing compelling AR experiences, creating new human-computer interaction (HCI) paradigms – and doing it all in one cohesive product – is one of the most difficult challenges our industry has ever faced. It was so challenging that we thought we had less than a 10% chance of pulling it off successfully. Until now.</span></p>
<p><span>Orion is a feat of miniaturization – t</span><span>he components are packed down to a fraction of a millimeter.</span><span> Dozens of innovations were required to get the design down to a contemporary form that you’d be comfortable wearing every day.&nbsp;</span></p>
<p><span>Orion has the largest field of view in the smallest AR glasses form to date. That field of view unlocks truly immersive use cases for Orion, from multitasking windows and big-screen entertainment to life-size holograms of people – all digital content that can seamlessly blend with your view of the physical world.</span></p>
<p><a href="https://about.fb.com/wp-content/uploads/2024/09/03_multiscreens.gif?resize=960%2C836"><img src="https://about.fb.com/wp-content/uploads/2024/09/03_multiscreens.gif?resize=960%2C836" alt="Video from the POV of someone looking at multiple holographic screens" width="960" height="836" data-recalc-dims="1"></a></p>
<p><span>But what makes Orion unique is that it is unmistakably a pair of glasses in both look and feel – complete with transparent lenses. Unlike MR headsets or other AR glasses today, you can still see other people’s eyes and expressions, so you can be present and share the experience with the people around you.&nbsp;</span></p>
<h2>Augmented Reality Experiences</h2>
<p><span>Of course, as with any piece of hardware, Orion is only as good as the things you can do with it. And while it’s still early days, the experiences afforded by Orion are an exciting glimpse of what’s to come.</span></p>
<p><span>We’ve got our smart assistant, </span><a href="https://www.meta.ai/"><span>Meta AI</span></a><span>, running on Orion. It understands what you’re looking at in the physical world and can help you with useful visualizations. So you can open up your refrigerator and ask for a recipe based on what’s inside. Or video call a friend while adjusting a digital family calendar as you wash the dishes.</span></p>
<p><a href="https://about.fb.com/wp-content/uploads/2024/09/04_recipes.gif?resize=960%2C836"><img loading="lazy" src="https://about.fb.com/wp-content/uploads/2024/09/04_recipes.gif?resize=960%2C836" alt="Video of ingredients with a recipe on a holographic screen next to it" width="960" height="836" data-recalc-dims="1"></a></p>
<p><span>You can take a hands-free video call to catch up with friends and family in real time, and you can stay connected on WhatsApp and Messenger to view and send messages. No need to pull out your phone, unlock it, find the right app and let your friend know you’re running late for dinner – you can do it all through your glasses.</span></p>
<p><span>Our teams continue to iterate on the experiences available through Orion today to build new immersive social experiences, and we can’t wait to share what’s next.</span></p>
<h2>A Purposeful Product Prototype</h2>
<p><span>While Orion won’t make its way into the hands of consumers, make no mistake: this is not a research prototype. It’s one of the most polished product prototypes we’ve ever developed, and is truly representative of something that could ship to consumers. Rather than rushing to put it on shelves, we decided to focus on internal development first, which means we can keep building quickly and continue to push the boundaries of the technology, helping us arrive at an even better consumer product faster.</span></p>
<p><a href="https://about.fb.com/wp-content/uploads/2024/09/05_whatcomesnext.png?resize=960%2C836"><img loading="lazy" src="https://about.fb.com/wp-content/uploads/2024/09/05_whatcomesnext.png?resize=960%2C836" alt="Picture of Orion glasses, wristband, and controller " width="960" height="836" srcset="https://about.fb.com/wp-content/uploads/2024/09/05_whatcomesnext.png?resize=960%2C836?w=1920 1920w, https://about.fb.com/wp-content/uploads/2024/09/05_whatcomesnext.png?resize=960%2C836?w=300 300w, https://about.fb.com/wp-content/uploads/2024/09/05_whatcomesnext.png?resize=960%2C836?w=768 768w, https://about.fb.com/wp-content/uploads/2024/09/05_whatcomesnext.png?resize=960%2C836?w=1024 1024w, https://about.fb.com/wp-content/uploads/2024/09/05_whatcomesnext.png?resize=960%2C836?w=1536 1536w, https://about.fb.com/wp-content/uploads/2024/09/05_whatcomesnext.png?resize=960%2C836?w=1240 1240w, https://about.fb.com/wp-content/uploads/2024/09/05_whatcomesnext.png?resize=960%2C836?w=689 689w" sizes="(max-width: 960px) 100vw, 960px" data-recalc-dims="1"></a></p>
<h2>What Comes Next</h2>
<p><span>Beginning today at Connect and continuing throughout the year, we’re opening up access to our Orion product prototype for Meta employees and select external audiences so our development team can learn, iterate and build towards our consumer AR glasses product line, which we plan to begin shipping in the near future.</span></p>
<p><span>And now that we’ve shared Orion with the world, we’re focused on a few things:</span></p>
<ul>
<li><span>Tuning the AR display quality to make the visuals even sharper</span></li>
<li><span>Optimizing wherever we can to make the form factor even smaller</span></li>
<li><span>Building at scale to make them more affordable</span></li>
</ul>
<p><span>In the next few years, you can expect to see new devices from us that build on our R&amp;D efforts. Orion isn’t just a window into the future – it’s a look at the very real possibilities within reach today. From Ray-Ban Meta glasses to Orion, we’ve seen the good that can come from letting people stay more present and empowered in the physical world, while tapping into all that the digital world has to offer.</span></p>
		</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Meta Quest 3S (154 pts)]]></title>
            <link>https://www.meta.com/tw/en/quest/quest-3s/</link>
            <guid>41649983</guid>
            <pubDate>Wed, 25 Sep 2024 17:50:42 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.meta.com/tw/en/quest/quest-3s/">https://www.meta.com/tw/en/quest/quest-3s/</a>, See on <a href="https://news.ycombinator.com/item?id=41649983">Hacker News</a></p>
&lt;Unparsable&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[Llama 3.2: Revolutionizing edge AI and vision with open, customizable models (767 pts)]]></title>
            <link>https://ai.meta.com/blog/llama-3-2-connect-2024-vision-edge-mobile-devices/?_fb_noscript=1</link>
            <guid>41649763</guid>
            <pubDate>Wed, 25 Sep 2024 17:29:27 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://ai.meta.com/blog/llama-3-2-connect-2024-vision-edge-mobile-devices/?_fb_noscript=1">https://ai.meta.com/blog/llama-3-2-connect-2024-vision-edge-mobile-devices/?_fb_noscript=1</a>, See on <a href="https://news.ycombinator.com/item?id=41649763">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><div><p>We’ve been excited by the <a href="https://ai.meta.com/blog/llama-usage-doubled-may-through-july-2024/" target="_blank" data-lnfb-mode="ie"><u>impact the Llama 3.1 herd of models have made</u></a> in the two months since we announced them, including the <a href="https://www.meta.ai/?utm_source=llama_meta_site&amp;utm_medium=web&amp;utm_content=Llama_nav&amp;utm_campaign=July_moment" target="_blank" data-lnfb-mode="ie"><u>405B</u></a>—the first open frontier-level AI model. While these models are incredibly powerful, we recognize that building with them requires significant compute resources and expertise. We’ve also heard from developers who don’t have access to these resources and still want the opportunity to build with Llama. As Meta Founder and CEO Mark Zuckerberg shared today at Connect, they won’t have to wait any longer. Today, we’re releasing Llama 3.2, which includes small and medium-sized vision LLMs (11B and 90B) and lightweight, text-only models (1B and 3B) that fit onto select edge and mobile devices.</p><p>It’s only been a year and a half since we first announced Llama, and we’ve made incredible progress in such a short amount of time. This year, <a href="https://ai.meta.com/blog/llama-usage-doubled-may-through-july-2024/" target="_blank" data-lnfb-mode="ie"><u>Llama has achieved 10x growth</u></a> and become the standard for responsible innovation. Llama also continues to lead on openness, modifiability, and cost efficiency, and it’s competitive with closed models—even leading in some areas. We believe that openness drives innovation and is the right path forward, which is why we continue to share our research and collaborate with our partners and the developer community.</p><p>We’re making Llama 3.2 models available for download on <a href="https://llama.meta.com/" target="_blank" data-lnfb-mode="ie"><u>llama.com</u></a> and <a href="https://huggingface.co/meta-llama" target="_blank" data-lnfb-mode="ie"><u>Hugging Face</u></a>, as well as available for immediate development on our broad ecosystem of partner platforms. Partners are an important part of this work, and we’ve worked with over 25 companies, including AMD, AWS, Databricks, Dell, Google Cloud, Groq, IBM, Intel, Microsoft Azure, NVIDIA, Oracle Cloud, and Snowflake, to enable services on day one. For the Llama 3.2 release, we’re also working with on-device partners Arm, MediaTek, and Qualcomm to offer a broad range of services at launch. Starting today, we’re also making <a href="https://github.com/meta-llama/llama-stack" target="_blank" data-lnfb-mode="ie"><u>Llama Stack</u></a> available to the community. More details on the latest release, including information on the <a href="https://euneedsai.com/" target="_blank" data-lnfb-mode="ie"><u>multimodal availability</u></a> in Europe, can be found in <a href="https://github.com/meta-llama/llama-models/blob/main/models/llama3_2/USE_POLICY.md" target="_blank" data-lnfb-mode="ie"><u>our acceptable use policy</u></a>.</p></div><p>Meet Llama 3.2</p></div><div><div><p>The two largest models of the Llama 3.2 collection, 11B and 90B, support image reasoning use cases, such as document-level understanding including charts and graphs, captioning of images, and visual grounding tasks such as directionally pinpointing objects in images based on natural language descriptions. For example, a person could ask a question about which month in the previous year their small business had the best sales, and Llama 3.2 can then reason based on an available graph and quickly provide the answer. In another example, the model could reason with a map and help answer questions such as when a hike might become steeper or the distance of a particular trail marked on the map. The 11B and 90B models can also bridge the gap between vision and language by extracting details from an image, understanding the scene, and then crafting a sentence or two that could be used as an image caption to help tell the story.</p><p>The lightweight 1B and 3B models are highly capable with multilingual text generation and tool calling abilities. These models empower developers to build personalized, on-device agentic applications with strong privacy where data never leaves the device. For example, such an application could help summarize the last 10 messages received, extract action items, and leverage tool calling to directly send calendar invites for follow-up meetings.</p><p>Running these models locally comes with two major advantages. First, prompts and responses can feel instantaneous, since processing is done locally. Second, running models locally maintains privacy by not sending data such as messages and calendar information to the cloud, making the overall application more private. Since processing is handled locally, the application can clearly control which queries stay on the device and which may need to be processed by a larger model in the cloud.</p></div><p>Model evaluations</p><div><p>Our evaluation suggests that the Llama 3.2 vision models are competitive with leading foundation models, Claude 3 Haiku and GPT4o-mini on image recognition and a range of visual understanding tasks. The 3B model outperforms the Gemma 2 2.6B and Phi 3.5-mini models on tasks such as following instructions, summarization, prompt rewriting, and tool-use, while the 1B is competitive with Gemma.</p><p>We evaluated performance on over 150 benchmark datasets that span a wide range of languages. For the vision LLMs, we evaluated performance on benchmarks for image understanding and visual reasoning.</p><br></div><p><img src="https://scontent.fzrh3-1.fna.fbcdn.net/v/t39.2365-6/461288018_1255239495501495_271827633811450582_n.png?_nc_cat=102&amp;ccb=1-7&amp;_nc_sid=e280be&amp;_nc_ohc=oeAoi7_K9ggQ7kNvgE8Zpdb&amp;_nc_ht=scontent.fzrh3-1.fna&amp;_nc_gid=ADyiZ_xNTdS_fAeqWnELSPZ&amp;oh=00_AYDMbCATap9ZGIWGhs6IVu17rVzyVC_Le8VgxoFJIFee1w&amp;oe=670EE46D" alt="" id="u_0_4_2L"></p><p><img src="https://scontent.fzrh3-1.fna.fbcdn.net/v/t39.2365-6/461157789_931406385491961_1692349435372036848_n.png?_nc_cat=100&amp;ccb=1-7&amp;_nc_sid=e280be&amp;_nc_ohc=cgqzw-5Xp68Q7kNvgH3D7-A&amp;_nc_ht=scontent.fzrh3-1.fna&amp;_nc_gid=ADyiZ_xNTdS_fAeqWnELSPZ&amp;oh=00_AYAtWvlCUwgtC2Tnoa7QT0N1wrgXCaXa_3Qylh6SV2gKzA&amp;oe=670EC398" alt="" id="u_0_5_Mg"></p><p>Vision models</p><div><p>As the first Llama models to support vision tasks, the 11B and 90B models required an entirely new model architecture that supports image reasoning.</p><p>To add image input support, we trained a set of adapter weights that integrate the pre-trained image encoder into the pre-trained language model. The adapter consists of a series of cross-attention layers that feed image encoder representations into the language model. We trained the adapter on text-image pairs to align the image representations with the language representations. During adapter training, we also updated the parameters of the image encoder, but intentionally did not update the language-model parameters. By doing that, we keep all the text-only capabilities intact, providing developers a drop-in replacement for Llama 3.1 models.</p><p>Our training pipeline consists of multiple stages, starting from pretrained Llama 3.1 text models. First, we add image adapters and encoders, then pretrain on large-scale noisy (image, text) pair data. Next, we train on medium-scale high quality in-domain and knowledge-enhanced (image, text) pair data.</p><p>In post-training, we use a similar recipe as the text models by doing several rounds of alignment on supervised fine-tuning, rejection sampling, and direct preference optimization. We leverage synthetic data generation by using the Llama 3.1 model to filter and augment question and answers on top of in-domain images, and use a reward model to rank all the candidate answers to provide high quality fine-tuning data. We also add safety mitigation data to produce a model with a high level of safety while retaining helpfulness of the mode</p><p>The end result is a set of models that can take in both image and text prompts, and deeply understand and reason on the combination. This is another step toward Llama models having even richer agentic capabilities.</p><br></div><p>Lightweight models</p><div><p>As we talked about with Llama 3.1, powerful teacher models can be leveraged to create smaller models that have improved performance. We used two methods—pruning and distillation—on the 1B and 3B models, making them the first highly capable lightweight Llama models that can fit on devices efficiently.</p><p>Pruning enabled us to reduce the size of extant models in the Llama herd while recovering as much knowledge and performance as possible. For the 1B and 3B models, we took the approach of using structured pruning in a single shot manner from the Llama 3.1 8B. This involved systematically removing parts of the network and adjusting the magnitude of the weights and gradients to create a smaller, more efficient model that retains the performance of the original network.</p><p>Knowledge distillation uses a larger network to impart knowledge on a smaller network, with the idea that a smaller model can achieve better performance using a teacher than it could from scratch. For the 1B and 3B in Llama 3.2, we incorporated logits from the Llama 3.1 8B and 70B models into the pre-training stage of the model development, where outputs (logits) from these larger models were used as token-level targets. Knowledge distillation was used after pruning to recover performance.</p><br></div><p><img src="https://scontent.fzrh3-1.fna.fbcdn.net/v/t39.2365-6/461209081_511117684875670_45564063096782202_n.png?_nc_cat=101&amp;ccb=1-7&amp;_nc_sid=e280be&amp;_nc_ohc=ieNr4u425SwQ7kNvgGrbiWa&amp;_nc_ht=scontent.fzrh3-1.fna&amp;_nc_gid=ADyiZ_xNTdS_fAeqWnELSPZ&amp;oh=00_AYDsxVu1KvPuqspn62ZVregQPBGZXtIPMO9ExVdzqB4gxw&amp;oe=670EE3DE" alt="" id="u_0_9_Gn"></p><div><p>In post-training, we use a similar recipe as Llama 3.1 and produce final chat models by doing several rounds of alignment on top of the pre-trained model. Each round involves supervised fine-tuning (SFT), rejection sampling (RS), and direct preference optimization (DPO).</p><p>In post-training, we scale context length support to 128K tokens, while maintaining the same quality as the pre-trained model. We also engage in synthetic data generation that goes through careful data processing and filtering to ensure high quality. We carefully blend the data to optimize for high quality across multiple capabilities like summarization, rewriting, instruction following, language reasoning, and tool use.</p><p>To enable the community to innovate on these models, we worked closely with Qualcomm and Mediatek, the top two mobile system on a chip (SoC) companies in the world, and Arm, who provides the foundational compute platform for <a href="https://www.arm.com/company" target="_blank" data-lnfb-mode="ie"><u>99</u></a><a href="https://www.arm.com/company" target="_blank" data-lnfb-mode="ie">%</a> of mobile devices. The weights being released today are based on BFloat16 numerics. Our teams are actively exploring quantized variants that will run even faster, and we hope to share more on that soon.</p><br></div><div><p>This demo is based on an unreleased quantized model.</p></div><div><p>This demo is based on an unreleased quantized model.</p></div><div><p>Llama Stack distributions</p><div><p>In July, we released a <a href="https://github.com/meta-llama/llama-stack/issues/6" target="_blank" data-lnfb-mode="ie"><u>request for comment</u></a> on the Llama Stack API, a standardized interface for canonical toolchain components (fine-tuning, synthetic data generation) to customize Llama models and build agentic applications. The engagement has been great.</p><p>Since then, we have been working hard to make the API real. We built a reference implementation of the APIs for inference, tool use, and RAG. In addition, we have been working with partners to adapt them to become providers for the APIs. Finally, we have introduced Llama Stack Distribution as a way to package multiple API Providers that work well together to provide a single endpoint for developers. We are now sharing with the community a simplified and consistent experience that will enable them to work with Llama models in multiple environments, including on-prem, cloud, single-node, and on-device.</p><br></div><p><img src="https://scontent.fzrh3-1.fna.fbcdn.net/v/t39.2365-6/460924239_3402957093334534_4357083070437107157_n.png?_nc_cat=109&amp;ccb=1-7&amp;_nc_sid=e280be&amp;_nc_ohc=zKTWaIWC3AIQ7kNvgH8H9c8&amp;_nc_ht=scontent.fzrh3-1.fna&amp;_nc_gid=ADyiZ_xNTdS_fAeqWnELSPZ&amp;oh=00_AYCvn28-wNqLSj9fc0cz1oxYweRRaHRS6smL3no1_ill3Q&amp;oe=670EC620" alt="" id="u_0_j_Ys"></p><div><div><p>The full set of releases includes:</p><ol><li>Llama CLI (command line interface) to build, configure, and run Llama Stack distributions</li><li>Client code in multiple languages, including python, node, kotlin, and swift</li><li>Docker containers for Llama Stack Distribution Server and Agents API Provider</li><li>Multiple distributions<ol><li>Single-node Llama Stack Distribution via Meta internal implementation and Ollama</li><li>Cloud Llama Stack distributions via AWS, Databricks, Fireworks, and Together</li><li>On-device Llama Stack Distribution on iOS implemented via PyTorch ExecuTorch</li><li>On-prem Llama Stack Distribution supported by Dell</li></ol></li></ol><p>We look forward to working with developers and partners to simplify all aspects of building with Llama models and welcome feedback.</p><br></div><p><img src="https://scontent.fzrh3-1.fna.fbcdn.net/v/t39.2365-6/460942153_931942502081982_4461283719059292584_n.png?_nc_cat=108&amp;ccb=1-7&amp;_nc_sid=e280be&amp;_nc_ohc=yPZiTRUpQyIQ7kNvgGmAxck&amp;_nc_ht=scontent.fzrh3-1.fna&amp;_nc_gid=ADyiZ_xNTdS_fAeqWnELSPZ&amp;oh=00_AYAln3XFhMf0mccyDv_zyuK2uKI2xqDCVc1PiKB__kZ7jQ&amp;oe=670ED455" alt="" id="u_0_k_yb"></p><div><p>System level safety</p><div><p>Taking an open approach has many benefits. It helps ensure that more people around the world can access the opportunities that AI provides, guards against concentrating power in the hands of a small few, and deploys technology more equitably and safely across society. As we continue to innovate, we also want to make sure we’re empowering developers to build safe and responsible systems.</p><p>Building on our previous release and continuous effort to support responsible innovation, today we’re adding new updates to our family of safeguards:</p><ul><li>First, we’re releasing Llama Guard 3 11B Vision, which is designed to support Llama 3.2’s new image understanding capability and filter text+image input prompts or text output responses to these prompts.</li><li>Second, as we released 1B and 3B Llama models to be used in more constrained environments like on-device, we also optimized Llama Guard to drastically reduce its deployment cost. Llama Guard 3 1B is based on the Llama 3.2 1B model and has been pruned and quantized bringing its size from 2,858 MB down to 438 MB, making it more efficient than ever to deploy.</li></ul><p>These new solutions are integrated into our reference implementations, demos, and applications and are ready for the open source community to use on day one.</p><br></div><div><p>Try Llama 3.2 today</p><div><p>Llama 3.2 is poised to reach more people than ever before and enable exciting new use cases. We believe sharing these models with the open source community isn’t enough. We want to make sure developers also have the tools they need to build with Llama responsibly. As part of our continued responsible release efforts, we’re offering developers new <a href="https://ai.meta.com/blog/responsible-ai-connect-2024/" target="_blank" data-lnfb-mode="ie"><u>tools and resources</u></a>, and as always, we’ll update best practices in our <a href="https://ai.meta.com/static-resource/responsible-use-guide/" target="_blank" data-lnfb-mode="ie"><u>Responsible Use Guide</u></a>.</p><p>We continue to share the latest advancements in the Llama ecosystem because we believe openness drives innovation and is good for developers, Meta, and the world. We’re excited to continue the conversations we’re having with our partners and the open source community, and as always, we can’t wait to see what the community builds using Llama 3.2 and Llama Stack.</p><p><i>This work was supported by our partners across the AI community. We’d like to thank and acknowledge (in alphabetical order): Accenture, AMD, Arm, AWS, Cloudflare, Databricks, Dell, Deloitte, Fireworks.ai, Google Cloud, Groq, Hugging Face, IBM watsonx, Infosys, Intel, Kaggle, Lenovo, LMSYS, MediaTek, Microsoft Azure, NVIDIA, OctoAI, Ollama, Oracle Cloud, PwC, Qualcomm, Sarvam AI, Scale AI, Snowflake, Together AI, and UC Berkeley - vLLM Project.</i></p><br></div><a href="https://www.llama.com/" data-ms-clickable="true" data-ms="{&quot;creative&quot;:&quot;click_external&quot;}" target="_blank" rel="noreferrer noopener" data-lnfb-mode="ie" id="u_0_o_MJ"><div><p>Learn more on the Llama website</p><svg viewBox="0 0 36 36" fill="none" xmlns="http://www.w3.org/2000/svg"><path fill-rule="evenodd" clip-rule="evenodd" d="M14.746 10H26V21.254L24.0301 21.2735L24.0297 13.377L11.4067 26L10 24.5933L22.6039 11.9894L14.746 11.9894V10Z" fill="CurrentColor"></path></svg></div></a></div></div></div></div></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Timeshare owner? The Mexican drug cartels want you (213 pts)]]></title>
            <link>https://krebsonsecurity.com/2024/09/timeshare-owner-the-mexican-drug-cartels-want-you/</link>
            <guid>41649134</guid>
            <pubDate>Wed, 25 Sep 2024 16:28:20 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://krebsonsecurity.com/2024/09/timeshare-owner-the-mexican-drug-cartels-want-you/">https://krebsonsecurity.com/2024/09/timeshare-owner-the-mexican-drug-cartels-want-you/</a>, See on <a href="https://news.ycombinator.com/item?id=41649134">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
												<p>The FBI is warning timeshare owners to be wary of a prevalent telemarketing scam involving a violent Mexican drug cartel that tries to trick people into believing someone wants to buy their property. This is the story of a couple who recently lost more than $50,000 to an ongoing timeshare scam that spans at least two dozen phony escrow, title and realty firms.</p>
<div id="attachment_68950"><p><img aria-describedby="caption-attachment-68950" decoding="async" src="https://krebsonsecurity.com/wp-content/uploads/2024/09/realestateassets-llc.png" alt="" width="751" height="405" srcset="https://krebsonsecurity.com/wp-content/uploads/2024/09/realestateassets-llc.png 2330w, https://krebsonsecurity.com/wp-content/uploads/2024/09/realestateassets-llc-768x414.png 768w, https://krebsonsecurity.com/wp-content/uploads/2024/09/realestateassets-llc-1536x828.png 1536w, https://krebsonsecurity.com/wp-content/uploads/2024/09/realestateassets-llc-2048x1104.png 2048w, https://krebsonsecurity.com/wp-content/uploads/2024/09/realestateassets-llc-782x422.png 782w, https://krebsonsecurity.com/wp-content/uploads/2024/09/realestateassets-llc-370x200.png 370w" sizes="(max-width: 751px) 100vw, 751px"></p><p id="caption-attachment-68950">One of the phony real estate companies trying to scam people out of money over fake offers to buy their timeshares.</p></div>
<p>One evening in late 2022, someone phoned <strong>Mr. &amp; Mrs. Dimitruk</strong>, a retired couple from Ontario, Canada and asked whether they’d ever considered selling their timeshare in Florida. The person on the phone referenced their timeshare address and said they had an interested buyer in Mexico. Would they possibly be interested in selling it?</p>
<p>The Dimitruks had purchased the timeshare years ago, but it wasn’t fully paid off — they still owed roughly $5,000 before they could legally sell it. That wouldn’t be an issue for this buyer, the man on the phone assured them.</p>
<p>With a few days, their contact at a escrow company in New York called <strong>ecurrencyescrow[.]llc</strong> faxed them forms to fill out and send back to start the process of selling their timeshare to the potential buyer, who had offered an amount that was above what the property was likely worth.</p>
<p>After certain forms were signed and faxed, the Dimitruks were asked to send a small wire transfer of more than $3,000 to handle “administrative” and “processing” fees, supposedly so that the sale would not be held up by any bureaucratic red tape down in Mexico.</p>
<p><img decoding="async" loading="lazy" src="https://krebsonsecurity.com/wp-content/uploads/2024/09/ecurrencyesc.png" alt="" width="749" height="751" srcset="https://krebsonsecurity.com/wp-content/uploads/2024/09/ecurrencyesc.png 1410w, https://krebsonsecurity.com/wp-content/uploads/2024/09/ecurrencyesc-768x770.png 768w, https://krebsonsecurity.com/wp-content/uploads/2024/09/ecurrencyesc-782x784.png 782w" sizes="(max-width: 749px) 100vw, 749px"></p>
<p>These document exchanges went on for almost a year, during which time the real estate brokers made additional financial demands, such as tax payments on the sale, and various administrative fees. Mrs. Dimitruk even sent them a $5,000 wire to pay off her remaining balance on the timeshare they thought they were selling.</p>
<p>In a phone interview with KrebsOnSecurity, Mr. Dimitruk said they lost over $50,000.</p>
<p>“They kept calling me after that saying, ‘Hey your money is waiting for you here’,” said <strong>William Dimitruk</strong>, a 73-year-old retired long-haul truck driver. “They said ‘We’re going to get in trouble if the money isn’t returned to you,’ and gave me a toll-free number to call them at.”</p>
<p>In the last call he had with the scammers, the man on the other end of the line confessed that some bad people had worked for them previously, but that those employees had been fired.</p>
<p>“Near the end of the call he said, ‘You’ve been dealing with some bad people and we fired all those bad guys,'” Dimitruk recalled. “So they were like, yeah it’s all good. You can go ahead and pay us more and we’ll send you your money.”</p>
<p>According to the FBI, there are indeed some very bad people behind these scams. The FBI warns the timeshare fraud schemes have been linked to the <strong>Jalisco New Generation drug cartel</strong> in Mexico.</p>
<p>In July 2024, the FBI and the Treasury Department’s <strong>Financial Crimes Enforcement Network</strong> (FinCEN) <a href="https://www.fincen.gov/news/news-releases/fincen-ofac-and-fbi-joint-notice-timeshare-fraud-associated-mexico-based" target="_blank" rel="noopener">warned</a> the Jalisco cartel is running boiler room-like call centers that target people who own timeshares:</p>
<blockquote><p>“Mexico-based [transnational criminal organizations] such as the Jalisco New Generation Cartel are increasingly targeting U.S. owners of timeshares in Mexico through complex and often yearslong telemarketing, impersonation, and advance fee schemes. They use the illicit proceeds to diversify their revenue streams and finance other criminal activities, including the manufacturing and trafficking of illicit fentanyl and other synthetic drugs into the United States.”</p></blockquote>
<p>A <a href="https://www.cbsnews.com/news/us-sanctions-cartel-accountants-timeshare-scams-target-americans/" target="_blank" rel="noopener">July 2024 CBS News story</a> about these scams notes that U.S. and Mexican officials last year confirmed that as many as eight young workers were confirmed dead after they apparently tried to quit jobs at a call center operated by the Jalisco cartel.</p>
<div id="attachment_68958"><p><img aria-describedby="caption-attachment-68958" decoding="async" loading="lazy" src="https://krebsonsecurity.com/wp-content/uploads/2024/09/jaliscocartel.png" alt="" width="750" height="580" srcset="https://krebsonsecurity.com/wp-content/uploads/2024/09/jaliscocartel.png 788w, https://krebsonsecurity.com/wp-content/uploads/2024/09/jaliscocartel-768x594.png 768w, https://krebsonsecurity.com/wp-content/uploads/2024/09/jaliscocartel-782x604.png 782w" sizes="(max-width: 750px) 100vw, 750px"></p><p id="caption-attachment-68958">Source: US Department of the Treasury’s Office of Foreign Assets Control.</p></div>
<p>The phony escrow company the Dimitruks dealt with — <strong>ecurrencyescrow[.]llc</strong> — is no longer online. But the documents sent by their contact there referenced a few other still-active domains, including <strong>realestateassetsllc[.]com</strong></p>
<p>The original registration records of both of these domains reference another domain — <strong>datasur[.]host</strong> — that is associated with dozens of other real estate and escrow-themed domains going back at least four years. Some of these domains are no longer active, while others have been previously suspended at different hosting providers. <span id="more-68943"></span></p>
<p>061nyr[.]net<br>
061-newyorkrealty[.]net<br>
1nydevelopersgroupllc[.]com<br>
1oceanrealtyllc[.]com<br>
advancedclosingservicesllc[.]com<br>
americancorporatetitle[.]com<br>
asesorialegalsiglo[.]com<br>
atencion-tributaria.[]com<br>
carolinasctinc[.]net<br>
closingandsettlementservices[.]com<br>
closingandsettlementsllc[.]com<br>
closingsettlementllc[.]com<br>
crefaescrowslimited[.]net<br>
ecurrencyescrow[.]llc<br>
empirerllc[.]com<br>
fiduciarocitibanamex[.]com<br>
fondosmx[.]org<br>
freightescrowcollc[.]com<br>
goldmansachs-investment[.]com<br>
hgvccorp[.]com<br>
infodivisionfinanciera[.]com<br>
internationaladvisorllc[.]com<br>
jadehillrealtyllc[.]com<br>
lewisandassociaterealty[.]com<br>
nyreputable[.]org<br>
privateinvestment.com[.]co<br>
realestateassetsllc[.]com<br>
realestateisinc[.]com<br>
settlementandmanagement[.]com<br>
stllcservices[.]com<br>
stllcservices[.]net<br>
thebluehorizonrealtyinc[.]com<br>
walshrealtyny[.]net<br>
windsorre[.]com</p>
<p>By loading ecurrencyescrowllc[.]com into <a href="https://web.archive.org/web/20230214194256/http://ecurrencyescrow.llc/" target="_blank" rel="noopener">the Wayback Machine at archive.org</a>, we can see text at the top of the page that reads, “Visit our resource library for videos and tools designed to make managing your escrow disbursements a breeze.”</p>
<p>Searching on that bit of text at <a href="https://publicwww.com/" target="_blank" rel="noopener">publicwww.com</a> shows the same text appears on the website of an escrow company called <strong>Escshieldsecurity Network</strong> (escshieldsecurity[.]com). This entity claims to have been around since 2009, but the domain itself is less than two years old, and there is no contact information associated with the site. The Pennsylvania Secretary of State also has no record of a business by this name at its stated address.</p>
<p>Incredibly, Escshieldsecurity pitches itself as a solution to timeshare closing scams.</p>
<p>“By 2015, cyber thieves had realized the amount of funds involved and had targeted the real estate, title and settlement industry,” the company’s website states. “As funding became more complex and risky, agents and underwriters had little time or resources to keep up. The industry needed a simple solution that allowed it to keep pace with new funding security needs.”</p>
<p>The domains associated with this scam will often reference legitimate companies and licensed professionals in the real estate and closing businesses, but those real professionals often have no idea they’re being impersonated until someone starts asking around. The truth is, the original reader tip that caused KrebsOnSecurity to investigate this scheme came from one such professional whose name and reputation was being used to scam others.</p>
<p>It is unclear whether the Dimitruks were robbed by people working for the Jalisco cartel, but it is clear that whoever is responsible for managing many of the above-mentioned domains — including the DNS provider datasur[.]host — recently compromised their computer with information-stealing malware.</p>
<p>That’s according to data collected by the breach tracking service <a href="https://www.constella.ai/" target="_blank" rel="noopener">Constella Intelligence</a> [Constella is currently an advertiser on KrebsOnSecurity]. Constella found that someone using the email address exposed in the DNS records for datasur[.]host — jyanes1920@gmail.com — also was relieved of credentials for managing most of the domains referenced above at a Mexican hosting provider.</p>
<p>It’s not unusual for victims of such scams to keep mum about their misfortune. Sometimes, it’s shame and embarrassment that prevents victims from filing a report with the local authorities. But in this case, victims who learn they’ve been robbed by a violent drug cartel have even more reason to remain silent.</p>
<p>William Dimitruk acknowledged that he and his wife haven’t yet filed a police report. But after acknowledging it could help prevent harm to other would-be victims, Mr. Dimitruk said he would consider it.</p>
<p>There is another reason victims of scams like this should notify authorities: Occasionally, the feds will bust up one of these scam operations and seize funds that were stolen from victims. But those investigations can take years, and it can be even more years before the government starts trying to figure out who got scammed and how to remunerate victims. All too often, the real impediment to returning some of those losses is that the feds have no idea who the victims are.</p>
<p>If you are the victim of a timeshare scam like this, please consider filing a report with the FBI’s Internet Crime Complaint Center (IC3), at <a href="https://www.ic3.gov/" target="_blank" rel="noopener">ic3.gov</a>. Other places where victims may wish to file a complaint:</p>
<p><strong>Federal Trade Commission – </strong><a href="https://www.ftccomplaintassistant.gov/" target="_blank" rel="noopener noreferrer">https://www.ftccomplaintassistant.gov</a><br>
<strong>International Consumer Protection and Enforcement Network – </strong><a href="https://www.econsumer.gov/en/Home/FileAComplaint/1#crnt" target="_blank" rel="noopener noreferrer">https://www.econsumer.gov/en</a><br>
<strong>Profeco – Mexican Attorney General</strong> –&nbsp;<a href="https://consulmex.sre.gob.mx/montreal/index.php/en/foreigners/services-foreigners/318-consumer-protection">https://consulmex.sre.gob.mx/montreal/index.php/en/foreigners/services-foreigners/318-consumer-protection</a></p>
											</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Show HN: Fast and Exact Algorithm for Image Merging (110 pts)]]></title>
            <link>https://github.com/C-Naoki/image-stitcher</link>
            <guid>41648965</guid>
            <pubDate>Wed, 25 Sep 2024 16:10:35 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/C-Naoki/image-stitcher">https://github.com/C-Naoki/image-stitcher</a>, See on <a href="https://news.ycombinator.com/item?id=41648965">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><p dir="auto"><h2 tabindex="-1" dir="auto">image-stitcher</h2><a id="user-content-image-stitcher" aria-label="Permalink: image-stitcher" href="#image-stitcher"></a></p>
<p dir="auto"><a href="https://www.python.org/downloads/release/python-390/" rel="nofollow"><img src="https://camo.githubusercontent.com/067bfbd6b45e87f5aceb63a0e83a1b3403ce6910b5a01c12c84eed6fa6e4274d/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f507974686f6e2d332e31302d677265656e2e737667" alt="Python 3.10" data-canonical-src="https://img.shields.io/badge/Python-3.10-green.svg"></a>
<a href="https://opensource.org/licenses/MIT" rel="nofollow"><img src="https://camo.githubusercontent.com/6cd0120cc4c5ac11d28b2c60f76033b52db98dac641de3b2644bb054b449d60c/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f4c6963656e73652d4d49542d79656c6c6f772e737667" alt="License: MIT" data-canonical-src="https://img.shields.io/badge/License-MIT-yellow.svg"></a>
<a href="https://zenn.dev/naoki0103/articles/image-stitcher-application" rel="nofollow"><img src="https://camo.githubusercontent.com/ef720357cffe2ced181e44b515aedc77083c4531e2eadd7d30a58101eecdcbf0/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f2546302539462539332539415a656e6e2d646f63756d656e746174696f6e2d696e666f726d6174696f6e616c2e737667" alt="RubyDoc" data-canonical-src="https://img.shields.io/badge/%F0%9F%93%9AZenn-documentation-informational.svg"></a></p>
<p dir="auto">This is a python implementation for stitching images by automatically searching for overlap region.</p>
<ul dir="auto">
<li><a href="#-usage">👨‍💻 Usage</a></li>
<li><a href="#-preview-of-results">🎯 Preview of results</a></li>
<li><a href="#-main-idea">🧠 Main Idea</a></li>
<li><a href="#%EF%B8%8F-support">🙋‍♂️ Support</a></li>
<li><a href="#%EF%B8%8F-contact">✉️ Contact</a></li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">👨‍💻 Usage</h2><a id="user-content--usage" aria-label="Permalink: 👨‍💻 Usage" href="#-usage"></a></p>
<div dir="auto" data-snippet-clipboard-copy-content="from src.main import main
from src.utils.visualizer import result_visualize

merged_image, cand = main(
    image1=image1,              # The first image to be combined
    image2=image2,              # The second image to be combined
    min_overlap=(5, 5),         # The minimum overlap region
    verbose=False,              # Whether to print the log
)
result_visualize(
    image1=image1,              # The first image to be combined
    image2=image2,              # The second image to be combined
    merged_image=merged_image,  # The output image
    cand=cand,                  # The parameters
)"><pre><span>from</span> <span>src</span>.<span>main</span> <span>import</span> <span>main</span>
<span>from</span> <span>src</span>.<span>utils</span>.<span>visualizer</span> <span>import</span> <span>result_visualize</span>

<span>merged_image</span>, <span>cand</span> <span>=</span> <span>main</span>(
    <span>image1</span><span>=</span><span>image1</span>,              <span># The first image to be combined</span>
    <span>image2</span><span>=</span><span>image2</span>,              <span># The second image to be combined</span>
    <span>min_overlap</span><span>=</span>(<span>5</span>, <span>5</span>),         <span># The minimum overlap region</span>
    <span>verbose</span><span>=</span><span>False</span>,              <span># Whether to print the log</span>
)
<span>result_visualize</span>(
    <span>image1</span><span>=</span><span>image1</span>,              <span># The first image to be combined</span>
    <span>image2</span><span>=</span><span>image2</span>,              <span># The second image to be combined</span>
    <span>merged_image</span><span>=</span><span>merged_image</span>,  <span># The output image</span>
    <span>cand</span><span>=</span><span>cand</span>,                  <span># The parameters</span>
)</pre></div>
<p dir="auto"><h2 tabindex="-1" dir="auto">🎯 Preview of results</h2><a id="user-content--preview-of-results" aria-label="Permalink: 🎯 Preview of results" href="#-preview-of-results"></a></p>
<p dir="auto">The results using <a href="https://www.cs.toronto.edu/~kriz/cifar.html" rel="nofollow"><code>CIFAR-10</code></a> are shown below. I would refer you to <a href="https://github.com/C-Naoki/image-stitcher/blob/main/notebooks/tutorial.ipynb"><code>tutorial.ipynb</code></a> for detailed results.</p>
<div dir="auto">
<p><a target="_blank" rel="noopener noreferrer" href="https://github.com/C-Naoki/image-stitcher/blob/main/docs/assets/input.png"><img src="https://github.com/C-Naoki/image-stitcher/raw/main/docs/assets/input.png" alt=""></a></p><p>
<b>Figure 1.</b> The example of input images. The red area represents an empty region. This application can combine these images while considering their rotation.
</p></div>
<div dir="auto">
<p><a target="_blank" rel="noopener noreferrer" href="https://github.com/C-Naoki/image-stitcher/blob/main/docs/assets/rotated.png"><img src="https://github.com/C-Naoki/image-stitcher/raw/main/docs/assets/rotated.png" alt=""></a></p><p>
<b>Figure 2.</b> The preprocessed input images. This rotation process is necessary to accurately combine the images. The green frame represents the overlap region between the input images.
</p></div>
<div dir="auto">
<p><a target="_blank" rel="noopener noreferrer" href="https://github.com/C-Naoki/image-stitcher/blob/main/docs/assets/result.png"><img src="https://github.com/C-Naoki/image-stitcher/raw/main/docs/assets/result.png" alt=""></a></p><p>
<b>Figure 3.</b> The output image.
</p></div>
<p dir="auto"><h2 tabindex="-1" dir="auto">🧠 Main Idea</h2><a id="user-content--main-idea" aria-label="Permalink: 🧠 Main Idea" href="#-main-idea"></a></p>
<div dir="auto">
<p><a target="_blank" rel="noopener noreferrer" href="https://github.com/C-Naoki/image-stitcher/blob/main/docs/assets/case1.png"><img src="https://github.com/C-Naoki/image-stitcher/raw/main/docs/assets/case1.png" width="500" alt=""></a></p><p>
<b>Figure 4.</b> The overview of this application in limited case.
</p></div>
<p dir="auto">This application is designed based on the overlap region's width <math-renderer data-static-url="https://github.githubassets.com/static" data-run-id="cfa9fe4674a4e7a09012a373241fef5f">$w_c$</math-renderer> and height <math-renderer data-static-url="https://github.githubassets.com/static" data-run-id="cfa9fe4674a4e7a09012a373241fef5f">$h_c$</math-renderer>. Thanks to this idea, we can simply limit the search space, thus preventing it from capturing overly small, suboptimal overlap region.</p>
<div dir="auto">
<p><a target="_blank" rel="noopener noreferrer" href="https://github.com/C-Naoki/image-stitcher/blob/main/docs/assets/case2.png"><img src="https://github.com/C-Naoki/image-stitcher/raw/main/docs/assets/case2.png" width="500" alt=""></a></p><p>
<b>Figure 5.</b> The overview of this application.
</p></div>
<p dir="auto">However, the above approach is not always applicable, specifically when <math-renderer data-static-url="https://github.githubassets.com/static" data-run-id="cfa9fe4674a4e7a09012a373241fef5f">$\min(h_1, h_2) &amp;lt; h_c$</math-renderer> or <math-renderer data-static-url="https://github.githubassets.com/static" data-run-id="cfa9fe4674a4e7a09012a373241fef5f">$\min(w_1, w_2) &amp;lt; w_c$</math-renderer>. To address this issue, I change the perspective of <math-renderer data-static-url="https://github.githubassets.com/static" data-run-id="cfa9fe4674a4e7a09012a373241fef5f">$w_c$</math-renderer> and <math-renderer data-static-url="https://github.githubassets.com/static" data-run-id="cfa9fe4674a4e7a09012a373241fef5f">$h_c$</math-renderer> like the above figure. Therefore, this application can handle images of arbitrary sizes.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">🙋‍♂️ Support</h2><a id="user-content-️-support" aria-label="Permalink: 🙋‍♂️ Support" href="#️-support"></a></p>
<p dir="auto">💙 If you like this app, give it a ⭐ and share it with friends!</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">✉️ Contact</h2><a id="user-content-️-contact" aria-label="Permalink: ✉️ Contact" href="#️-contact"></a></p>
<p dir="auto">💥 For questions or issues, feel free to open an <a href="https://github.com/C-Naoki/image-stitcher/issues">issue</a>. I appreciate your feedback and look forward to hearing from you!</p>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Launch HN: Haystack (YC S24) – Visualize and edit code on an infinite canvas (291 pts)]]></title>
            <link>https://github.com/haystackeditor/haystack-editor</link>
            <guid>41648564</guid>
            <pubDate>Wed, 25 Sep 2024 15:31:00 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/haystackeditor/haystack-editor">https://github.com/haystackeditor/haystack-editor</a>, See on <a href="https://news.ycombinator.com/item?id=41648564">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><p dir="auto"><h2 tabindex="-1" dir="auto">Haystack Editor Source Available</h2><a id="user-content-haystack-editor-source-available" aria-label="Permalink: Haystack Editor Source Available" href="#haystack-editor-source-available"></a></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">The Repository</h2><a id="user-content-the-repository" aria-label="Permalink: The Repository" href="#the-repository"></a></p>
<p dir="auto">This repository ("<code>Haystack Editor</code>") is where we (Haystack Software) develop the <a href="https://haystackeditor.com/" rel="nofollow">Haystack Editor</a> product together with the community. Not only do we work on code and issues here, we also publish our <a href="https://github.com/haystackeditor/haystack-editor/wiki/Roadmap">roadmap</a>. This source code is available under the [PolyForm Strict License 1.0.0] (<a href="https://polyformproject.org/licenses/strict/1.0.0" rel="nofollow">https://polyformproject.org/licenses/strict/1.0.0</a>).</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Haystack Editor</h2><a id="user-content-haystack-editor" aria-label="Permalink: Haystack Editor" href="#haystack-editor"></a></p>
<p dir="auto"><a href="https://haystackeditor.com/" rel="nofollow">Haystack Editor</a> is a distribution of the <code>Haystack Editor</code> repository with specific customizations released under a <a href="https://github.com/haystackeditor/haystack-tos/blob/main/license.txt/">terms of service</a>.</p>
<p dir="auto"><a href="https://haystackeditor.com/" rel="nofollow">Haystack Editor</a> combines the simplicity of a code editor with a canvas UI that makes it easier to understand code at a glance It provides comprehensive code editing, navigation, and understanding support along with lightweight debugging, a rich extensibility model, and lightweight integration with existing tools.</p>
<p dir="auto">Haystack is updated weekly with new features and bug fixes. You can download it for Windows, macOS, and Linux on <a href="https://haystackeditor.com/" rel="nofollow">Haystack’s website</a>.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Contributing</h2><a id="user-content-contributing" aria-label="Permalink: Contributing" href="#contributing"></a></p>
<p dir="auto">There are many ways in which you can participate in this project, for example:</p>
<ul dir="auto">
<li><a href="https://github.com/haystackeditor/haystack-editor/issues">Submit bugs and feature requests</a>, and help us verify as they are checked in</li>
<li>Review <a href="https://github.com/haystackeditor/haystack-editor/pulls">source code changes</a></li>
<li>Review the <a href="https://github.com/haystackeditor/haystack-editor/wiki">documentation</a> and make pull requests for anything from typos to additional and new content</li>
</ul>
<p dir="auto">If you are interested in fixing issues and contributing directly to the code base, please see the document <a href="https://github.com/haystackeditor/haystack-editor/wiki/How-to-Contribute">How to Contribute</a>, which covers the following:</p>
<ul dir="auto">
<li><a href="https://github.com/haystackeditor/haystack-editor/wiki/How-to-Contribute#build">How to build and run from source</a></li>
<li><a href="https://github.com/haystackeditor/haystack-editor/wiki/How-to-Contribute#build">The development workflow, including debugging and running tests</a></li>
<li><a href="https://github.com/haystackeditor/haystack-editor/wiki/How-to-Contribute#build">Submitting pull requests</a></li>
<li><a href="https://github.com/haystackeditor/haystack-editor/wiki/How-to-Contribute#build">Finding an issue to work on</a></li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Feedback</h2><a id="user-content-feedback" aria-label="Permalink: Feedback" href="#feedback"></a></p>
<ul dir="auto">
<li>Ask a question on <a href="https://discord.gg/apFrN6ABxc" rel="nofollow">Discord</a></li>
<li><a href="https://github.com/haystackeditor/haystack-editor/issues">File an issue</a></li>
<li>Follow <a href="https://x.com/AkshaySubr42403" rel="nofollow">@AkshaySubr42403</a> and let us know what you think!</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Related Projects</h2><a id="user-content-related-projects" aria-label="Permalink: Related Projects" href="#related-projects"></a></p>
<p dir="auto">Many of the core components and extensions to Haystack live in their own repositories on GitHub. For example, the <a href="https://github.com/microsoft/vscode-node-debug">node debug adapter</a> and the <a href="https://github.com/microsoft/vscode-mono-debug">mono debug adapter</a> repositories are separate from each other. Another example is the <a href="https://github.com/haystackeditor/pixijs">Pixi repository</a>.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Bundled Extensions</h2><a id="user-content-bundled-extensions" aria-label="Permalink: Bundled Extensions" href="#bundled-extensions"></a></p>
<p dir="auto">Haystack includes a set of built-in extensions located in the <a href="https://github.com/haystackeditor/haystack-editor/blob/main/extensions">extensions</a> folder, including grammars and snippets for many languages. Extensions that provide rich language support (code completion, Go to Definition) for a language have the suffix <code>language-features</code>. For example, the <code>json</code> extension provides coloring for <code>JSON</code> and the <code>json-language-features</code> extension provides rich language support for <code>JSON</code>.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">License</h2><a id="user-content-license" aria-label="Permalink: License" href="#license"></a></p>
<p dir="auto">Copyright (c) Haystack Software Inc. All rights reserved.</p>
<p dir="auto">Licensed under the [PolyForm Strict License 1.0.0] (<a href="https://polyformproject.org/licenses/strict/1.0.0" rel="nofollow">https://polyformproject.org/licenses/strict/1.0.0</a>).</p>
</article></div></div>]]></description>
        </item>
    </channel>
</rss>