<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0">
    <channel>
        <title>HN100 - Readable Contents</title>
        <link>https://hn.algolia.com/api/v1/search_by_date?tags=%28story,poll%29&amp;numericFilters=points%3E100</link>
        <description>Uses Readability to add bodies to the RSS feed</description>
        <lastBuildDate>Mon, 25 Nov 2024 21:30:03 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>en</language>
        <item>
            <title><![CDATA[Introducing The Model Context Protocol (308 pts)]]></title>
            <link>https://www.anthropic.com/news/model-context-protocol</link>
            <guid>42237424</guid>
            <pubDate>Mon, 25 Nov 2024 16:14:22 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.anthropic.com/news/model-context-protocol">https://www.anthropic.com/news/model-context-protocol</a>, See on <a href="https://news.ycombinator.com/item?id=42237424">Hacker News</a></p>
<div id="readability-page-1" class="page"><article><div><p>Today, we're open-sourcing the <a href="https://modelcontextprotocol.io/">Model Context Protocol</a> (MCP), a new standard for connecting AI assistants to the systems where data lives, including content repositories, business tools, and development environments. Its aim is to help frontier models produce better, more relevant responses.</p><p>As AI assistants gain mainstream adoption, the industry has invested heavily in model capabilities, achieving rapid advances in reasoning and quality. Yet even the most sophisticated models are constrained by their isolation from data—trapped behind information silos and legacy systems. Every new data source requires its own custom implementation, making truly connected systems difficult to scale.</p><p>MCP addresses this challenge. It provides a universal, open standard for connecting AI systems with data sources, replacing fragmented integrations with a single protocol. The result is a simpler, more reliable way to give AI systems access to the data they need.</p><h3>Model Context Protocol</h3><p>The Model Context Protocol is an open standard that enables developers to build secure, two-way connections between their data sources and AI-powered tools. The architecture is straightforward: developers can either expose their data through MCP servers or build AI applications (MCP clients) that connect to these servers.</p><p>Today, we're introducing three major components of the Model Context Protocol for developers:</p><ul><li>The Model Context Protocol <a href="https://github.com/modelcontextprotocol">specification and SDKs</a></li><li>Local MCP server support in the <a href="https://claude.ai/download">Claude Desktop apps</a></li><li>An <a href="https://github.com/modelcontextprotocol/servers">open-source repository</a> of MCP servers</li></ul><p>Claude 3.5 Sonnet is adept at quickly building MCP server implementations, making it easy for organizations and individuals to rapidly connect their most important datasets with a range of AI-powered tools. To help developers start exploring, we’re sharing pre-built MCP servers for popular enterprise systems like Google Drive, Slack, GitHub, Git, Postgres, and Puppeteer.</p><p>Early adopters like Block and Apollo have integrated MCP into their systems, while development tools companies including Zed, Replit, Codeium, and Sourcegraph are working with MCP to enhance their platforms—enabling AI agents to better retrieve relevant information to further understand the context around a coding task and produce more nuanced and functional code with fewer attempts.</p><p>"At Block, open source is more than a development model—it’s the foundation of our work and a commitment to creating technology that drives meaningful change and serves as a public good for all,” said Dhanji R. Prasanna, Chief Technology Officer at Block. “Open technologies like the Model Context Protocol are the bridges that connect AI to real-world applications, ensuring innovation is accessible, transparent, and rooted in collaboration. We are excited to partner on a protocol and use it to build agentic systems, which remove the burden of the mechanical so people can focus on the creative.”</p><p>Instead of maintaining separate connectors for each data source, developers can now build against a standard protocol. As the ecosystem matures, AI systems will maintain context as they move between different tools and datasets, replacing today's fragmented integrations with a more sustainable architecture.</p><h3>Getting started</h3><p>Developers can start building and testing MCP connectors today. Existing Claude for Work customers can begin testing MCP servers locally, connecting Claude to internal systems and datasets. We'll soon provide developer toolkits for deploying remote production MCP servers that can serve your entire Claude for Work organization.</p><p>To start building:</p><ul><li>Install pre-built MCP servers through the <a href="https://claude.ai/download">Claude Desktop app</a></li><li>Follow our <a href="https://modelcontextprotocol.io/quickstart">quickstart guide</a> to build your first MCP server</li><li>Contribute to our <a href="https://github.com/modelcontextprotocol">open-source repositories</a> of connectors and implementations</li></ul><h3>An open community</h3><p>We’re committed to building MCP as a collaborative, open-source project and ecosystem, and we’re eager to hear your feedback. Whether you’re an AI tool developer, an enterprise looking to leverage existing data, or an early adopter exploring the frontier, we invite you to build the future of context-aware AI together.</p></div></article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Show HN: Rill – Composable concurrency toolkit for Go (143 pts)]]></title>
            <link>https://github.com/destel/rill</link>
            <guid>42237166</guid>
            <pubDate>Mon, 25 Nov 2024 15:41:22 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/destel/rill">https://github.com/destel/rill</a>, See on <a href="https://news.ycombinator.com/item?id=42237166">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text">
<p dir="auto">Rill is a toolkit that brings composable concurrency to Go, making it easier to build concurrent programs from simple, reusable parts.
It reduces boilerplate while preserving Go's natural channel-based model.</p>
<div dir="auto" data-snippet-clipboard-copy-content="go get -u github.com/destel/rill"><pre>go get -u github.com/destel/rill</pre></div>
<p dir="auto"><h2 tabindex="-1" dir="auto">Goals</h2><a id="user-content-goals" aria-label="Permalink: Goals" href="#goals"></a></p>
<ul dir="auto">
<li>
<p dir="auto"><strong>Make common tasks easier.</strong><br>
Rill provides a cleaner and safer way of solving common concurrency problems, such as parallel job execution or
real-time event processing.
It removes boilerplate and abstracts away the complexities of goroutine, channel, and error management.
At the same time, developers retain full control over the concurrency level of all operations.</p>
</li>
<li>
<p dir="auto"><strong>Make concurrent code composable and clean.</strong><br>
Most functions in the library take Go channels as inputs and return new, transformed channels as outputs.
This allows them to be chained in various ways to build reusable pipelines from simpler parts,
similar to Unix pipes.
As a result, concurrent programs become clear sequences of reusable operations.</p>
</li>
<li>
<p dir="auto"><strong>Centralize error handling.</strong><br>
Errors are automatically propagated through a pipeline and can be handled in a single place at the end.
For more complex scenarios, Rill also provides tools to intercept and handle errors at any point in a pipeline.</p>
</li>
<li>
<p dir="auto"><strong>Simplify stream processing.</strong><br>
Thanks to Go channels, built-in functions can handle potentially infinite streams, processing items as they arrive.
This makes Rill a convenient tool for real-time processing or handling large datasets that don't fit in memory.</p>
</li>
<li>
<p dir="auto"><strong>Provide solutions for advanced tasks.</strong><br>
Beyond basic operations, the library includes ready-to-use functions for batching, ordered fan-in, map-reduce,
stream splitting, merging, and more. Pipelines, while usually linear, can have any cycle-free topology (DAG).</p>
</li>
<li>
<p dir="auto"><strong>Support custom extensions.</strong><br>
Since Rill operates on standard Go channels, it's easy to write custom functions compatible with the library.</p>
</li>
<li>
<p dir="auto"><strong>Keep it lightweight.</strong><br>
Rill has a small, type-safe, channel-based API, and zero dependencies, making it straightforward to integrate into existing projects.
It's also lightweight in terms of resource usage, ensuring that the number of memory allocations and goroutines
does not grow with the input size.</p>
</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Quick Start</h2><a id="user-content-quick-start" aria-label="Permalink: Quick Start" href="#quick-start"></a></p>
<p dir="auto">Let's look at a practical example: fetch users from an API, activate them, and save the changes back.
It shows how to control concurrency at each step while keeping the code clean and manageable.
<strong>ForEach</strong> returns on the first error, and context cancellation via defer stops all remaining fetches.</p>
<p dir="auto"><a href="https://pkg.go.dev/github.com/destel/rill#example-package" rel="nofollow">Try it</a></p>
<div dir="auto" data-snippet-clipboard-copy-content="func main() {
	ctx, cancel := context.WithCancel(context.Background())
	defer cancel()

	// Convert a slice of user IDs into a channel
	ids := rill.FromSlice([]int{1, 2, 3, 4, 5, 6, 7, 8, 9, 10}, nil)

	// Read users from the API.
	// Concurrency = 3
	users := rill.Map(ids, 3, func(id int) (*mockapi.User, error) {
		return mockapi.GetUser(ctx, id)
	})

	// Activate users.
	// Concurrency = 2
	err := rill.ForEach(users, 2, func(u *mockapi.User) error {
		if u.IsActive {
			fmt.Printf(&quot;User %d is already active\n&quot;, u.ID)
			return nil
		}

		u.IsActive = true
		err := mockapi.SaveUser(ctx, u)
		if err != nil {
			return err
		}

		fmt.Printf(&quot;User saved: %+v\n&quot;, u)
		return nil
	})

	// Handle errors
	fmt.Println(&quot;Error:&quot;, err)
}"><pre><span>func</span> <span>main</span>() {
	<span>ctx</span>, <span>cancel</span> <span>:=</span> <span>context</span>.<span>WithCancel</span>(<span>context</span>.<span>Background</span>())
	<span>defer</span> <span>cancel</span>()

	<span>// Convert a slice of user IDs into a channel</span>
	<span>ids</span> <span>:=</span> <span>rill</span>.<span>FromSlice</span>([]<span>int</span>{<span>1</span>, <span>2</span>, <span>3</span>, <span>4</span>, <span>5</span>, <span>6</span>, <span>7</span>, <span>8</span>, <span>9</span>, <span>10</span>}, <span>nil</span>)

	<span>// Read users from the API.</span>
	<span>// Concurrency = 3</span>
	<span>users</span> <span>:=</span> <span>rill</span>.<span>Map</span>(<span>ids</span>, <span>3</span>, <span>func</span>(<span>id</span> <span>int</span>) (<span>*</span>mockapi.<span>User</span>, <span>error</span>) {
		<span>return</span> <span>mockapi</span>.<span>GetUser</span>(<span>ctx</span>, <span>id</span>)
	})

	<span>// Activate users.</span>
	<span>// Concurrency = 2</span>
	<span>err</span> <span>:=</span> <span>rill</span>.<span>ForEach</span>(<span>users</span>, <span>2</span>, <span>func</span>(<span>u</span> <span>*</span>mockapi.<span>User</span>) <span>error</span> {
		<span>if</span> <span>u</span>.<span>IsActive</span> {
			<span>fmt</span>.<span>Printf</span>(<span>"User %d is already active<span>\n</span>"</span>, <span>u</span>.<span>ID</span>)
			<span>return</span> <span>nil</span>
		}

		<span>u</span>.<span>IsActive</span> <span>=</span> <span>true</span>
		<span>err</span> <span>:=</span> <span>mockapi</span>.<span>SaveUser</span>(<span>ctx</span>, <span>u</span>)
		<span>if</span> <span>err</span> <span>!=</span> <span>nil</span> {
			<span>return</span> <span>err</span>
		}

		<span>fmt</span>.<span>Printf</span>(<span>"User saved: %+v<span>\n</span>"</span>, <span>u</span>)
		<span>return</span> <span>nil</span>
	})

	<span>// Handle errors</span>
	<span>fmt</span>.<span>Println</span>(<span>"Error:"</span>, <span>err</span>)
}</pre></div>
<p dir="auto"><h2 tabindex="-1" dir="auto">Batching</h2><a id="user-content-batching" aria-label="Permalink: Batching" href="#batching"></a></p>
<p dir="auto">Processing items in batches rather than individually can significantly improve performance in many scenarios,
particularly when working with external services or databases. Batching reduces the number of queries and API calls,
increases throughput, and typically lowers costs.</p>
<p dir="auto">To demonstrate batching, let's improve the previous example by using the API's bulk fetching capability.
The <strong>Batch</strong> function transforms a stream of individual IDs into a stream of slices. This enables the use of <code>GetUsers</code> API
to fetch multiple users in a single call, instead of making individual <code>GetUser</code> calls.</p>
<p dir="auto"><a href="https://pkg.go.dev/github.com/destel/rill#example-package-Batching" rel="nofollow">Try it</a></p>
<div dir="auto" data-snippet-clipboard-copy-content="func main() {
	ctx, cancel := context.WithCancel(context.Background())
	defer cancel()

	// Convert a slice of user IDs into a channel
	ids := rill.FromSlice([]int{
		1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20,
		21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40,
	}, nil)

	// Group IDs into batches of 5
	idBatches := rill.Batch(ids, 5, -1)

	// Bulk fetch users from the API
	// Concurrency = 3
	userBatches := rill.Map(idBatches, 3, func(ids []int) ([]*mockapi.User, error) {
		return mockapi.GetUsers(ctx, ids)
	})

	// Transform the stream of batches back into a flat stream of users
	users := rill.Unbatch(userBatches)

	// Activate users.
	// Concurrency = 2
	err := rill.ForEach(users, 2, func(u *mockapi.User) error {
		if u.IsActive {
			fmt.Printf(&quot;User %d is already active\n&quot;, u.ID)
			return nil
		}

		u.IsActive = true
		err := mockapi.SaveUser(ctx, u)
		if err != nil {
			return err
		}

		fmt.Printf(&quot;User saved: %+v\n&quot;, u)
		return nil
	})

	// Handle errors
	fmt.Println(&quot;Error:&quot;, err)
}"><pre><span>func</span> <span>main</span>() {
	<span>ctx</span>, <span>cancel</span> <span>:=</span> <span>context</span>.<span>WithCancel</span>(<span>context</span>.<span>Background</span>())
	<span>defer</span> <span>cancel</span>()

	<span>// Convert a slice of user IDs into a channel</span>
	<span>ids</span> <span>:=</span> <span>rill</span>.<span>FromSlice</span>([]<span>int</span>{
		<span>1</span>, <span>2</span>, <span>3</span>, <span>4</span>, <span>5</span>, <span>6</span>, <span>7</span>, <span>8</span>, <span>9</span>, <span>10</span>, <span>11</span>, <span>12</span>, <span>13</span>, <span>14</span>, <span>15</span>, <span>16</span>, <span>17</span>, <span>18</span>, <span>19</span>, <span>20</span>,
		<span>21</span>, <span>22</span>, <span>23</span>, <span>24</span>, <span>25</span>, <span>26</span>, <span>27</span>, <span>28</span>, <span>29</span>, <span>30</span>, <span>31</span>, <span>32</span>, <span>33</span>, <span>34</span>, <span>35</span>, <span>36</span>, <span>37</span>, <span>38</span>, <span>39</span>, <span>40</span>,
	}, <span>nil</span>)

	<span>// Group IDs into batches of 5</span>
	<span>idBatches</span> <span>:=</span> <span>rill</span>.<span>Batch</span>(<span>ids</span>, <span>5</span>, <span>-</span><span>1</span>)

	<span>// Bulk fetch users from the API</span>
	<span>// Concurrency = 3</span>
	<span>userBatches</span> <span>:=</span> <span>rill</span>.<span>Map</span>(<span>idBatches</span>, <span>3</span>, <span>func</span>(<span>ids</span> []<span>int</span>) ([]<span>*</span>mockapi.<span>User</span>, <span>error</span>) {
		<span>return</span> <span>mockapi</span>.<span>GetUsers</span>(<span>ctx</span>, <span>ids</span>)
	})

	<span>// Transform the stream of batches back into a flat stream of users</span>
	<span>users</span> <span>:=</span> <span>rill</span>.<span>Unbatch</span>(<span>userBatches</span>)

	<span>// Activate users.</span>
	<span>// Concurrency = 2</span>
	<span>err</span> <span>:=</span> <span>rill</span>.<span>ForEach</span>(<span>users</span>, <span>2</span>, <span>func</span>(<span>u</span> <span>*</span>mockapi.<span>User</span>) <span>error</span> {
		<span>if</span> <span>u</span>.<span>IsActive</span> {
			<span>fmt</span>.<span>Printf</span>(<span>"User %d is already active<span>\n</span>"</span>, <span>u</span>.<span>ID</span>)
			<span>return</span> <span>nil</span>
		}

		<span>u</span>.<span>IsActive</span> <span>=</span> <span>true</span>
		<span>err</span> <span>:=</span> <span>mockapi</span>.<span>SaveUser</span>(<span>ctx</span>, <span>u</span>)
		<span>if</span> <span>err</span> <span>!=</span> <span>nil</span> {
			<span>return</span> <span>err</span>
		}

		<span>fmt</span>.<span>Printf</span>(<span>"User saved: %+v<span>\n</span>"</span>, <span>u</span>)
		<span>return</span> <span>nil</span>
	})

	<span>// Handle errors</span>
	<span>fmt</span>.<span>Println</span>(<span>"Error:"</span>, <span>err</span>)
}</pre></div>
<p dir="auto"><h2 tabindex="-1" dir="auto">Real-Time Batching</h2><a id="user-content-real-time-batching" aria-label="Permalink: Real-Time Batching" href="#real-time-batching"></a></p>
<p dir="auto">Real-world applications often need to handle events or data that arrives at unpredictable rates. While batching is still
desirable for efficiency, waiting to collect a full batch might introduce unacceptable delays when
the input stream becomes slow or sparse.</p>
<p dir="auto">Rill solves this with timeout-based batching: batches are emitted either when they're full or after a specified timeout,
whichever comes first. This approach ensures optimal batch sizes during high load while maintaining responsiveness during quiet periods.</p>
<p dir="auto">Consider an application that needs to update users' <em>last_active_at</em> timestamps in a database. The function responsible
for this - <code>UpdateUserTimestamp</code> can be called concurrently, at unpredictable rates, and from different parts of the application.
Performing all these updates individually may create too many concurrent queries, potentially overwhelming the database.</p>
<p dir="auto">In the example below, the updates are queued into <code>userIDsToUpdate</code> channel and then grouped into batches of up to 5 items,
with each batch sent to the database as a single query.
The <strong>Batch</strong> function is used with a timeout of 100ms, ensuring zero latency during high load,
and up to 100ms latency with smaller batches during quiet periods.</p>
<p dir="auto"><a href="https://pkg.go.dev/github.com/destel/rill#example-package-BatchingRealTime" rel="nofollow">Try it</a></p>
<div dir="auto" data-snippet-clipboard-copy-content="func main() {
	// Start the background worker that processes the updates
	go updateUserTimestampWorker()

	// Do some updates. They'll be automatically grouped into
	// batches: [1,2,3,4,5], [6,7], [8]
	UpdateUserTimestamp(1)
	UpdateUserTimestamp(2)
	UpdateUserTimestamp(3)
	UpdateUserTimestamp(4)
	UpdateUserTimestamp(5)
	UpdateUserTimestamp(6)
	UpdateUserTimestamp(7)
	time.Sleep(500 * time.Millisecond) // simulate sparse updates
	UpdateUserTimestamp(8)
}

// This is the queue of user IDs to update.
var userIDsToUpdate = make(chan int)

// UpdateUserTimestamp is the public API for updating the last_active_at column in the users table
func UpdateUserTimestamp(userID int) {
	userIDsToUpdate <- userID
}

// This is a background worker that sends queued updates to the database in batches.
// For simplicity, there are no retries, error handling and synchronization
func updateUserTimestampWorker() {

	ids := rill.FromChan(userIDsToUpdate, nil)

	idBatches := rill.Batch(ids, 5, 100*time.Millisecond)

	_ = rill.ForEach(idBatches, 1, func(batch []int) error {
		fmt.Printf(&quot;Executed: UPDATE users SET last_active_at = NOW() WHERE id IN (%v)\n&quot;, batch)
		return nil
	})
}"><pre><span>func</span> <span>main</span>() {
	<span>// Start the background worker that processes the updates</span>
	<span>go</span> <span>updateUserTimestampWorker</span>()

	<span>// Do some updates. They'll be automatically grouped into</span>
	<span>// batches: [1,2,3,4,5], [6,7], [8]</span>
	<span>UpdateUserTimestamp</span>(<span>1</span>)
	<span>UpdateUserTimestamp</span>(<span>2</span>)
	<span>UpdateUserTimestamp</span>(<span>3</span>)
	<span>UpdateUserTimestamp</span>(<span>4</span>)
	<span>UpdateUserTimestamp</span>(<span>5</span>)
	<span>UpdateUserTimestamp</span>(<span>6</span>)
	<span>UpdateUserTimestamp</span>(<span>7</span>)
	<span>time</span>.<span>Sleep</span>(<span>500</span> <span>*</span> <span>time</span>.<span>Millisecond</span>) <span>// simulate sparse updates</span>
	<span>UpdateUserTimestamp</span>(<span>8</span>)
}

<span>// This is the queue of user IDs to update.</span>
<span>var</span> <span>userIDsToUpdate</span> <span>=</span> <span>make</span>(<span>chan</span> <span>int</span>)

<span>// UpdateUserTimestamp is the public API for updating the last_active_at column in the users table</span>
<span>func</span> <span>UpdateUserTimestamp</span>(<span>userID</span> <span>int</span>) {
	<span>userIDsToUpdate</span> <span>&lt;-</span> <span>userID</span>
}

<span>// This is a background worker that sends queued updates to the database in batches.</span>
<span>// For simplicity, there are no retries, error handling and synchronization</span>
<span>func</span> <span>updateUserTimestampWorker</span>() {

	<span>ids</span> <span>:=</span> <span>rill</span>.<span>FromChan</span>(<span>userIDsToUpdate</span>, <span>nil</span>)

	<span>idBatches</span> <span>:=</span> <span>rill</span>.<span>Batch</span>(<span>ids</span>, <span>5</span>, <span>100</span><span>*</span><span>time</span>.<span>Millisecond</span>)

	<span>_</span> <span>=</span> <span>rill</span>.<span>ForEach</span>(<span>idBatches</span>, <span>1</span>, <span>func</span>(<span>batch</span> []<span>int</span>) <span>error</span> {
		<span>fmt</span>.<span>Printf</span>(<span>"Executed: UPDATE users SET last_active_at = NOW() WHERE id IN (%v)<span>\n</span>"</span>, <span>batch</span>)
		<span>return</span> <span>nil</span>
	})
}</pre></div>
<p dir="auto"><h2 tabindex="-1" dir="auto">Errors, Termination and Contexts</h2><a id="user-content-errors-termination-and-contexts" aria-label="Permalink: Errors, Termination and Contexts" href="#errors-termination-and-contexts"></a></p>
<p dir="auto">Error handling can be non-trivial in concurrent applications. Rill simplifies this by providing a structured approach to the problem.
Pipelines typically consist of a sequence of non-blocking channel transformations, followed by a blocking stage that returns a final result and an error.
The general rule is: any error occurring anywhere in a pipeline is propagated down to the final stage,
where it's caught by some blocking function and returned to the caller.</p>
<p dir="auto">Rill provides a wide selection of blocking functions. Here are some commonly used ones:</p>
<ul dir="auto">
<li><strong>ForEach:</strong> Concurrently applies a user function to each item in the stream.
<a href="https://pkg.go.dev/github.com/destel/rill#example-ForEach" rel="nofollow">Example</a></li>
<li><strong>ToSlice:</strong> Collects all stream items into a slice.
<a href="https://pkg.go.dev/github.com/destel/rill#example-ToSlice" rel="nofollow">Example</a></li>
<li><strong>First:</strong> Returns the first item or error encountered in the stream and discards the rest
<a href="https://pkg.go.dev/github.com/destel/rill#example-First" rel="nofollow">Example</a></li>
<li><strong>Reduce:</strong> Concurrently reduces the stream to a single value, using a user provided reducer function.
<a href="https://pkg.go.dev/github.com/destel/rill#example-Reduce" rel="nofollow">Example</a></li>
<li><strong>All:</strong> Concurrently checks if all items in the stream satisfy a user provided condition.
<a href="https://pkg.go.dev/github.com/destel/rill#example-All" rel="nofollow">Example</a></li>
<li><strong>Err:</strong> Returns the first error encountered in the stream or nil, and discards the rest of the stream.
<a href="https://pkg.go.dev/github.com/destel/rill#example-Err" rel="nofollow">Example</a></li>
</ul>
<p dir="auto">All blocking functions share a common behavior. In case of an early termination (before reaching the end of the input stream or in case of an error),
such functions initiate background draining of the remaining items. This is done to prevent goroutine leaks by ensuring that
all goroutines feeding the stream are allowed to complete.</p>
<p dir="auto">Rill is context-agnostic, meaning that it does not enforce any specific context usage.
However, it's recommended to make user-defined pipeline stages context-aware.
This is especially important for the initial stage, as it allows to stop feeding the pipeline with new items after the context cancellation.
In practice the first stage is often naturally context-aware through Go's standard APIs for databases, HTTP clients, and other external sources.</p>
<p dir="auto">In the example below the <code>CheckAllUsersExist</code> function uses several concurrent workers to check if all users<br>
from the given list exist. When an error occurs (like a non-existent user), the function returns that error<br>
and cancels the context, which in turn stops all remaining user fetches.</p>
<p dir="auto"><a href="https://pkg.go.dev/github.com/destel/rill#example-package-Context" rel="nofollow">Try it</a></p>
<div dir="auto" data-snippet-clipboard-copy-content="func main() {
	ctx := context.Background()

	// ID 999 doesn't exist, so fetching will stop after hitting it.
	err := CheckAllUsersExist(ctx, 3, []int{1, 2, 3, 4, 5, 999, 7, 8, 9, 10, 11, 12, 13, 14, 15})
	fmt.Printf(&quot;Check result: %v\n&quot;, err)
}

// CheckAllUsersExist uses several concurrent workers to check if all users with given IDs exist.
func CheckAllUsersExist(ctx context.Context, concurrency int, ids []int) error {
	// Create new context that will be canceled when this function returns
	ctx, cancel := context.WithCancel(ctx)
	defer cancel()

	// Convert the slice into a stream
	idsStream := rill.FromSlice(ids, nil)

	// Fetch users concurrently.
	users := rill.Map(idsStream, concurrency, func(id int) (*mockapi.User, error) {
		u, err := mockapi.GetUser(ctx, id)
		if err != nil {
			return nil, fmt.Errorf(&quot;failed to fetch user %d: %w&quot;, id, err)
		}

		fmt.Printf(&quot;Fetched user %d\n&quot;, id)
		return u, nil
	})

	// Return the first error (if any) and cancel remaining fetches via context
	return rill.Err(users)
}"><pre><span>func</span> <span>main</span>() {
	<span>ctx</span> <span>:=</span> <span>context</span>.<span>Background</span>()

	<span>// ID 999 doesn't exist, so fetching will stop after hitting it.</span>
	<span>err</span> <span>:=</span> <span>CheckAllUsersExist</span>(<span>ctx</span>, <span>3</span>, []<span>int</span>{<span>1</span>, <span>2</span>, <span>3</span>, <span>4</span>, <span>5</span>, <span>999</span>, <span>7</span>, <span>8</span>, <span>9</span>, <span>10</span>, <span>11</span>, <span>12</span>, <span>13</span>, <span>14</span>, <span>15</span>})
	<span>fmt</span>.<span>Printf</span>(<span>"Check result: %v<span>\n</span>"</span>, <span>err</span>)
}

<span>// CheckAllUsersExist uses several concurrent workers to check if all users with given IDs exist.</span>
<span>func</span> <span>CheckAllUsersExist</span>(<span>ctx</span> context.<span>Context</span>, <span>concurrency</span> <span>int</span>, <span>ids</span> []<span>int</span>) <span>error</span> {
	<span>// Create new context that will be canceled when this function returns</span>
	<span>ctx</span>, <span>cancel</span> <span>:=</span> <span>context</span>.<span>WithCancel</span>(<span>ctx</span>)
	<span>defer</span> <span>cancel</span>()

	<span>// Convert the slice into a stream</span>
	<span>idsStream</span> <span>:=</span> <span>rill</span>.<span>FromSlice</span>(<span>ids</span>, <span>nil</span>)

	<span>// Fetch users concurrently.</span>
	<span>users</span> <span>:=</span> <span>rill</span>.<span>Map</span>(<span>idsStream</span>, <span>concurrency</span>, <span>func</span>(<span>id</span> <span>int</span>) (<span>*</span>mockapi.<span>User</span>, <span>error</span>) {
		<span>u</span>, <span>err</span> <span>:=</span> <span>mockapi</span>.<span>GetUser</span>(<span>ctx</span>, <span>id</span>)
		<span>if</span> <span>err</span> <span>!=</span> <span>nil</span> {
			<span>return</span> <span>nil</span>, <span>fmt</span>.<span>Errorf</span>(<span>"failed to fetch user %d: %w"</span>, <span>id</span>, <span>err</span>)
		}

		<span>fmt</span>.<span>Printf</span>(<span>"Fetched user %d<span>\n</span>"</span>, <span>id</span>)
		<span>return</span> <span>u</span>, <span>nil</span>
	})

	<span>// Return the first error (if any) and cancel remaining fetches via context</span>
	<span>return</span> <span>rill</span>.<span>Err</span>(<span>users</span>)
}</pre></div>
<p dir="auto">In the example above only the second stage (<code>mockapi.GetUser</code>) of the pipeline is context-aware.
<strong>FromSlice</strong> works well here since the input is small, iteration is fast and context cancellation prevents expensive API calls regardless.
The following code demonstrates how to replace <strong>FromSlice</strong> with <strong>Generate</strong> when full context awareness becomes important.</p>
<div dir="auto" data-snippet-clipboard-copy-content="idsStream := rill.Generate(func(send func(int), sendErr func(error)) {
	for _, id := range ids {
		if ctx.Err() != nil {
			return
		}
		send(id)
	}
})"><pre><span>idsStream</span> <span>:=</span> <span>rill</span>.<span>Generate</span>(<span>func</span>(<span>send</span> <span>func</span>(<span>int</span>), <span>sendErr</span> <span>func</span>(<span>error</span>)) {
	<span>for</span> <span>_</span>, <span>id</span> <span>:=</span> <span>range</span> <span>ids</span> {
		<span>if</span> <span>ctx</span>.<span>Err</span>() <span>!=</span> <span>nil</span> {
			<span>return</span>
		}
		<span>send</span>(<span>id</span>)
	}
})</pre></div>
<p dir="auto"><h2 tabindex="-1" dir="auto">Order Preservation (Ordered Fan-In)</h2><a id="user-content-order-preservation-ordered-fan-in" aria-label="Permalink: Order Preservation (Ordered Fan-In)" href="#order-preservation-ordered-fan-in"></a></p>
<p dir="auto">Concurrent processing can boost performance, but since tasks take different amounts of time to complete,
the results' order usually differs from the input order. While out-of-order results are acceptable in many scenarios,
some cases require preserving the original order. This seemingly simple problem is deceptively challenging to solve correctly.</p>
<p dir="auto">To address this, Rill provides ordered versions of its core functions, such as <strong>OrderedMap</strong> or <strong>OrderedFilter</strong>.
These functions perform additional synchronization under the hood to ensure that if value <strong>x</strong> precedes value <strong>y</strong> in the input stream,
then <strong>f(x)</strong> will precede <strong>f(y)</strong> in the output.</p>
<p dir="auto">Here's a practical example: finding the first occurrence of a specific string among 1000 large files hosted online.
Downloading all files at once would consume too much memory, processing them sequentially would be too slow,
and traditional concurrency patterns do not preserve the order of files, making it challenging to find the first match.</p>
<p dir="auto">The combination of <strong>OrderedFilter</strong> and <strong>First</strong> functions solves this elegantly,
while downloading and keeping in memory at most 5 files at a time. <strong>First</strong> returns on the first match,
this triggers the context cancellation via defer, stopping URL generation and file downloads.</p>
<p dir="auto"><a href="https://pkg.go.dev/github.com/destel/rill#example-package-Ordering" rel="nofollow">Try it</a></p>
<div dir="auto" data-snippet-clipboard-copy-content="func main() {
	ctx, cancel := context.WithCancel(context.Background())
	defer cancel()

	// The string to search for in the downloaded files
	needle := []byte(&quot;26&quot;)

	// Generate a stream of URLs from https://example.com/file-0.txt 
	// to https://example.com/file-999.txt
	// Stop generating URLs if the context is canceled
	urls := rill.Generate(func(send func(string), sendErr func(error)) {
		for i := 0; i < 1000 &amp;&amp; ctx.Err() == nil; i++ {
			send(fmt.Sprintf(&quot;https://example.com/file-%d.txt&quot;, i))
		}
	})

	// Download and process the files
	// At most 5 files are downloaded and held in memory at the same time
	matchedUrls := rill.OrderedFilter(urls, 5, func(url string) (bool, error) {
		fmt.Println(&quot;Downloading:&quot;, url)

		content, err := mockapi.DownloadFile(ctx, url)
		if err != nil {
			return false, err
		}

		// keep only URLs of files that contain the needle
		return bytes.Contains(content, needle), nil
	})

	// Find the first matched URL
	firstMatchedUrl, found, err := rill.First(matchedUrls)
	if err != nil {
		fmt.Println(&quot;Error:&quot;, err)
		return
	}

	// Print the result
	if found {
		fmt.Println(&quot;Found in:&quot;, firstMatchedUrl)
	} else {
		fmt.Println(&quot;Not found&quot;)
	}
}"><pre><span>func</span> <span>main</span>() {
	<span>ctx</span>, <span>cancel</span> <span>:=</span> <span>context</span>.<span>WithCancel</span>(<span>context</span>.<span>Background</span>())
	<span>defer</span> <span>cancel</span>()

	<span>// The string to search for in the downloaded files</span>
	<span>needle</span> <span>:=</span> []<span>byte</span>(<span>"26"</span>)

	<span>// Generate a stream of URLs from https://example.com/file-0.txt </span>
	<span>// to https://example.com/file-999.txt</span>
	<span>// Stop generating URLs if the context is canceled</span>
	<span>urls</span> <span>:=</span> <span>rill</span>.<span>Generate</span>(<span>func</span>(<span>send</span> <span>func</span>(<span>string</span>), <span>sendErr</span> <span>func</span>(<span>error</span>)) {
		<span>for</span> <span>i</span> <span>:=</span> <span>0</span>; <span>i</span> <span>&lt;</span> <span>1000</span> <span>&amp;&amp;</span> <span>ctx</span>.<span>Err</span>() <span>==</span> <span>nil</span>; <span>i</span><span>++</span> {
			<span>send</span>(<span>fmt</span>.<span>Sprintf</span>(<span>"https://example.com/file-%d.txt"</span>, <span>i</span>))
		}
	})

	<span>// Download and process the files</span>
	<span>// At most 5 files are downloaded and held in memory at the same time</span>
	<span>matchedUrls</span> <span>:=</span> <span>rill</span>.<span>OrderedFilter</span>(<span>urls</span>, <span>5</span>, <span>func</span>(<span>url</span> <span>string</span>) (<span>bool</span>, <span>error</span>) {
		<span>fmt</span>.<span>Println</span>(<span>"Downloading:"</span>, <span>url</span>)

		<span>content</span>, <span>err</span> <span>:=</span> <span>mockapi</span>.<span>DownloadFile</span>(<span>ctx</span>, <span>url</span>)
		<span>if</span> <span>err</span> <span>!=</span> <span>nil</span> {
			<span>return</span> <span>false</span>, <span>err</span>
		}

		<span>// keep only URLs of files that contain the needle</span>
		<span>return</span> <span>bytes</span>.<span>Contains</span>(<span>content</span>, <span>needle</span>), <span>nil</span>
	})

	<span>// Find the first matched URL</span>
	<span>firstMatchedUrl</span>, <span>found</span>, <span>err</span> <span>:=</span> <span>rill</span>.<span>First</span>(<span>matchedUrls</span>)
	<span>if</span> <span>err</span> <span>!=</span> <span>nil</span> {
		<span>fmt</span>.<span>Println</span>(<span>"Error:"</span>, <span>err</span>)
		<span>return</span>
	}

	<span>// Print the result</span>
	<span>if</span> <span>found</span> {
		<span>fmt</span>.<span>Println</span>(<span>"Found in:"</span>, <span>firstMatchedUrl</span>)
	} <span>else</span> {
		<span>fmt</span>.<span>Println</span>(<span>"Not found"</span>)
	}
}</pre></div>
<p dir="auto"><h2 tabindex="-1" dir="auto">Stream Merging and FlatMap</h2><a id="user-content-stream-merging-and-flatmap" aria-label="Permalink: Stream Merging and FlatMap" href="#stream-merging-and-flatmap"></a></p>
<p dir="auto">Rill comes with the <strong>Merge</strong> function that combines multiple streams into a single one. Another, often overlooked,
function that can combine streams is <strong>FlatMap</strong>. It's a powerful tool that transforms each input item into its own stream,
and then merges all these streams together.</p>
<p dir="auto">In the example below, <strong>FlatMap</strong> transforms each department into a stream of users, then merges these streams into one.
Like other Rill functions, <strong>FlatMap</strong> gives full control over concurrency.
In this particular case the concurrency level is 3, meaning that users are fetched from at most 3 departments at the same time.</p>
<p dir="auto">Additionally, this example demonstrates how to write a reusable streaming wrapper over paginated API calls - the <code>StreamUsers</code> function.
This wrapper can be useful both on its own and as part of larger pipelines.</p>
<p dir="auto"><a href="https://pkg.go.dev/github.com/destel/rill#example-package-FlatMap" rel="nofollow">Try it</a></p>
<div dir="auto" data-snippet-clipboard-copy-content="func main() {
	ctx, cancel := context.WithCancel(context.Background())
	defer cancel()

	// Start with a stream of department names
	departments := rill.FromSlice([]string{&quot;IT&quot;, &quot;Finance&quot;, &quot;Marketing&quot;, &quot;Support&quot;, &quot;Engineering&quot;}, nil)

	// Stream users from all departments concurrently.
	// At most 3 departments at the same time.
	users := rill.FlatMap(departments, 3, func(department string) <-chan rill.Try[*mockapi.User] {
		return StreamUsers(ctx, &amp;mockapi.UserQuery{Department: department})
	})

	// Print the users from the combined stream
	err := rill.ForEach(users, 1, func(user *mockapi.User) error {
		fmt.Printf(&quot;%+v\n&quot;, user)
		return nil
	})
	fmt.Println(&quot;Error:&quot;, err)
}

// StreamUsers is a reusable streaming wrapper around the mockapi.ListUsers function.
// It iterates through all listing pages and uses [Generate] to simplify sending users and errors to the resulting stream.
// This function is useful both on its own and as part of larger pipelines.
func StreamUsers(ctx context.Context, query *mockapi.UserQuery) <-chan rill.Try[*mockapi.User] {
	return rill.Generate(func(send func(*mockapi.User), sendErr func(error)) {
		var currentQuery mockapi.UserQuery
		if query != nil {
			currentQuery = *query
		}

		for page := 0; ; page++ {
			currentQuery.Page = page

			users, err := mockapi.ListUsers(ctx, &amp;currentQuery)
			if err != nil {
				sendErr(err)
				return
			}

			if len(users) == 0 {
				break
			}

			for _, user := range users {
				send(user)
			}
		}
	})
}"><pre><span>func</span> <span>main</span>() {
	<span>ctx</span>, <span>cancel</span> <span>:=</span> <span>context</span>.<span>WithCancel</span>(<span>context</span>.<span>Background</span>())
	<span>defer</span> <span>cancel</span>()

	<span>// Start with a stream of department names</span>
	<span>departments</span> <span>:=</span> <span>rill</span>.<span>FromSlice</span>([]<span>string</span>{<span>"IT"</span>, <span>"Finance"</span>, <span>"Marketing"</span>, <span>"Support"</span>, <span>"Engineering"</span>}, <span>nil</span>)

	<span>// Stream users from all departments concurrently.</span>
	<span>// At most 3 departments at the same time.</span>
	<span>users</span> <span>:=</span> <span>rill</span>.<span>FlatMap</span>(<span>departments</span>, <span>3</span>, <span>func</span>(<span>department</span> <span>string</span>) <span>&lt;-</span><span>chan</span> rill.<span>Try</span>[<span>*</span>mockapi.<span>User</span>] {
		<span>return</span> <span>StreamUsers</span>(<span>ctx</span>, <span>&amp;</span>mockapi.<span>UserQuery</span>{<span>Department</span>: <span>department</span>})
	})

	<span>// Print the users from the combined stream</span>
	<span>err</span> <span>:=</span> <span>rill</span>.<span>ForEach</span>(<span>users</span>, <span>1</span>, <span>func</span>(<span>user</span> <span>*</span>mockapi.<span>User</span>) <span>error</span> {
		<span>fmt</span>.<span>Printf</span>(<span>"%+v<span>\n</span>"</span>, <span>user</span>)
		<span>return</span> <span>nil</span>
	})
	<span>fmt</span>.<span>Println</span>(<span>"Error:"</span>, <span>err</span>)
}

<span>// StreamUsers is a reusable streaming wrapper around the mockapi.ListUsers function.</span>
<span>// It iterates through all listing pages and uses [Generate] to simplify sending users and errors to the resulting stream.</span>
<span>// This function is useful both on its own and as part of larger pipelines.</span>
<span>func</span> <span>StreamUsers</span>(<span>ctx</span> context.<span>Context</span>, <span>query</span> <span>*</span>mockapi.<span>UserQuery</span>) <span>&lt;-</span><span>chan</span> rill.<span>Try</span>[<span>*</span>mockapi.<span>User</span>] {
	<span>return</span> <span>rill</span>.<span>Generate</span>(<span>func</span>(<span>send</span> <span>func</span>(<span>*</span>mockapi.<span>User</span>), <span>sendErr</span> <span>func</span>(<span>error</span>)) {
		<span>var</span> <span>currentQuery</span> mockapi.<span>UserQuery</span>
		<span>if</span> <span>query</span> <span>!=</span> <span>nil</span> {
			<span>currentQuery</span> <span>=</span> <span>*</span><span>query</span>
		}

		<span>for</span> <span>page</span> <span>:=</span> <span>0</span>; ; <span>page</span><span>++</span> {
			<span>currentQuery</span>.<span>Page</span> <span>=</span> <span>page</span>

			<span>users</span>, <span>err</span> <span>:=</span> <span>mockapi</span>.<span>ListUsers</span>(<span>ctx</span>, <span>&amp;</span><span>currentQuery</span>)
			<span>if</span> <span>err</span> <span>!=</span> <span>nil</span> {
				<span>sendErr</span>(<span>err</span>)
				<span>return</span>
			}

			<span>if</span> <span>len</span>(<span>users</span>) <span>==</span> <span>0</span> {
				<span>break</span>
			}

			<span>for</span> <span>_</span>, <span>user</span> <span>:=</span> <span>range</span> <span>users</span> {
				<span>send</span>(<span>user</span>)
			}
		}
	})
}</pre></div>
<p dir="auto"><h2 tabindex="-1" dir="auto">Go 1.23 Iterators</h2><a id="user-content-go-123-iterators" aria-label="Permalink: Go 1.23 Iterators" href="#go-123-iterators"></a></p>
<p dir="auto">Starting from Go 1.23, the language added <em>range-over-function</em> feature, allowing users to define custom iterators
for use in for-range loops. This feature enables Rill to integrate seamlessly with existing iterator-based functions
in the standard library and third-party packages.</p>
<p dir="auto">Rill provides <strong>FromSeq</strong> and <strong>FromSeq2</strong> functions to convert an iterator into a stream,
and <strong>ToSeq2</strong> function to convert a stream back into an iterator.</p>
<p dir="auto"><strong>ToSeq2</strong> can be a good alternative to <strong>ForEach</strong> when concurrency is not needed.
It gives more control and performs all necessary cleanup and draining, even if the loop is terminated early using <em>break</em> or <em>return</em>.</p>
<p dir="auto"><a href="https://pkg.go.dev/github.com/destel/rill#example-ToSeq2" rel="nofollow">Try it</a></p>
<div dir="auto" data-snippet-clipboard-copy-content="func main() {
	// Convert a slice of numbers into a stream
	numbers := rill.FromSlice([]int{1, 2, 3, 4, 5, 6, 7, 8, 9, 10}, nil)

	// Transform each number
	// Concurrency = 3
	squares := rill.Map(numbers, 3, func(x int) (int, error) {
		return square(x), nil
	})

	// Convert the stream into an iterator and use for-range to print the results
	for val, err := range rill.ToSeq2(squares) {
		if err != nil {
			fmt.Println(&quot;Error:&quot;, err)
			break // cleanup is done regardless of early exit
		}
		fmt.Printf(&quot;%+v\n&quot;, val)
	}
}"><pre><span>func</span> <span>main</span>() {
	<span>// Convert a slice of numbers into a stream</span>
	<span>numbers</span> <span>:=</span> <span>rill</span>.<span>FromSlice</span>([]<span>int</span>{<span>1</span>, <span>2</span>, <span>3</span>, <span>4</span>, <span>5</span>, <span>6</span>, <span>7</span>, <span>8</span>, <span>9</span>, <span>10</span>}, <span>nil</span>)

	<span>// Transform each number</span>
	<span>// Concurrency = 3</span>
	<span>squares</span> <span>:=</span> <span>rill</span>.<span>Map</span>(<span>numbers</span>, <span>3</span>, <span>func</span>(<span>x</span> <span>int</span>) (<span>int</span>, <span>error</span>) {
		<span>return</span> <span>square</span>(<span>x</span>), <span>nil</span>
	})

	<span>// Convert the stream into an iterator and use for-range to print the results</span>
	<span>for</span> <span>val</span>, <span>err</span> <span>:=</span> <span>range</span> <span>rill</span>.<span>ToSeq2</span>(<span>squares</span>) {
		<span>if</span> <span>err</span> <span>!=</span> <span>nil</span> {
			<span>fmt</span>.<span>Println</span>(<span>"Error:"</span>, <span>err</span>)
			<span>break</span> <span>// cleanup is done regardless of early exit</span>
		}
		<span>fmt</span>.<span>Printf</span>(<span>"%+v<span>\n</span>"</span>, <span>val</span>)
	}
}</pre></div>
<p dir="auto"><h2 tabindex="-1" dir="auto">Testing Strategy</h2><a id="user-content-testing-strategy" aria-label="Permalink: Testing Strategy" href="#testing-strategy"></a></p>
<p dir="auto">Rill has a test coverage of over 95%, with testing focused on:</p>
<ul dir="auto">
<li><strong>Correctness</strong>: ensuring that functions produce accurate results at different levels of concurrency</li>
<li><strong>Concurrency</strong>: confirming that correct number of goroutines is spawned and utilized</li>
<li><strong>Ordering</strong>: ensuring that ordered versions of functions preserve the order, while basic versions do not</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Contributing</h2><a id="user-content-contributing" aria-label="Permalink: Contributing" href="#contributing"></a></p>
<p dir="auto">Contributions are welcome! Whether it's reporting a bug, suggesting a feature, or submitting a pull request, your support helps improve Rill.
Please ensure that your code adheres to the existing style and includes relevant tests._</p>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Hey, wait – is employee performance Gaussian distributed? (235 pts)]]></title>
            <link>https://timdellinger.substack.com/p/hey-wait-is-employee-performance</link>
            <guid>42236841</guid>
            <pubDate>Mon, 25 Nov 2024 15:03:39 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://timdellinger.substack.com/p/hey-wait-is-employee-performance">https://timdellinger.substack.com/p/hey-wait-is-employee-performance</a>, See on <a href="https://news.ycombinator.com/item?id=42236841">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><article><div dir="auto"><p>tl;dr:</p><p><em>It’s probably Pareto-distributed, not Gaussian, which elucidates a few things about some of the problems that performance management processes have at large corporations, and also speaks to why it’s so hard to hire good people. Oh, and for the economists: the Marginal Productivity Theory of Wages is cleverly combined with the Gini Coefficient to arrive at the key insight.</em></p><p>Ahhh, it’s Q4 at Fortune 500 companies. That means it’s performance management season, when millions of employees are ranked and graded on their accomplishments over the past 12 months. Their bonuses and raises next year – or lack thereof, accompanied by a statement about their future at the company – depend on the outcome of this process.</p><p><span>I’ll graciously sidestep the discussion of how accurate these Fortune 500 managers could possibly be as they assess and rank their direct reports.&nbsp; (Accuracy here is certainly questionable, given issues such as apples-to-oranges comparisons, the principal-agent problem, the many flavors of bias, etc.). Similarly, I’ll also sidestep the negative effects that these performance management processes have on a company’s culture and work environment (don’t get me started!). Let’s instead focus on the statistical foundations of the process through the lens of a data scientist. </span><em>Spoiler alert: it’s built on shaky ground.</em></p><p>As a quick reminder of the process at a typical Fortune 500 company, managers will annually hold a series of meetings, with the express purpose of slotting all employees into place on a “vitality curve” that looks something like this:</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff58fbd1b-a66c-4884-bd46-516a0d84f666_576x432.png" data-component-name="Image2ToDOM" rel="nofollow ugc noopener"><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff58fbd1b-a66c-4884-bd46-516a0d84f666_576x432.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff58fbd1b-a66c-4884-bd46-516a0d84f666_576x432.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff58fbd1b-a66c-4884-bd46-516a0d84f666_576x432.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff58fbd1b-a66c-4884-bd46-516a0d84f666_576x432.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff58fbd1b-a66c-4884-bd46-516a0d84f666_576x432.png" width="576" height="432" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/f58fbd1b-a66c-4884-bd46-516a0d84f666_576x432.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:432,&quot;width&quot;:576,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:13446,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:false,&quot;topImage&quot;:true,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff58fbd1b-a66c-4884-bd46-516a0d84f666_576x432.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff58fbd1b-a66c-4884-bd46-516a0d84f666_576x432.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff58fbd1b-a66c-4884-bd46-516a0d84f666_576x432.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff58fbd1b-a66c-4884-bd46-516a0d84f666_576x432.png 1456w" sizes="100vw" fetchpriority="high"></picture></div></a></figure></div><p>Dividing lines are drawn that vary from company to company, and from year to year. The categories above reflect General Electric's original 10-70-20 split from the early 1980s, which is often cited as the canonical system: the top 20% of employees are given a handsome monetary bonus, and the bottom 10% are shown the door.</p><p>Sometimes I like to visualize the same data using a percentile chart instead – you can think of it as literally lining everyone up from shortest to tallest; each person’s “percentile” is the percent of the population that they’re taller than.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F81f16aa1-9c11-42bd-9c3a-6a628ec0f84d_571x432.png" data-component-name="Image2ToDOM" rel="nofollow ugc noopener"><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F81f16aa1-9c11-42bd-9c3a-6a628ec0f84d_571x432.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F81f16aa1-9c11-42bd-9c3a-6a628ec0f84d_571x432.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F81f16aa1-9c11-42bd-9c3a-6a628ec0f84d_571x432.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F81f16aa1-9c11-42bd-9c3a-6a628ec0f84d_571x432.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F81f16aa1-9c11-42bd-9c3a-6a628ec0f84d_571x432.png" width="571" height="432" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/81f16aa1-9c11-42bd-9c3a-6a628ec0f84d_571x432.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:432,&quot;width&quot;:571,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:20705,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F81f16aa1-9c11-42bd-9c3a-6a628ec0f84d_571x432.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F81f16aa1-9c11-42bd-9c3a-6a628ec0f84d_571x432.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F81f16aa1-9c11-42bd-9c3a-6a628ec0f84d_571x432.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F81f16aa1-9c11-42bd-9c3a-6a628ec0f84d_571x432.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p>The unspoken first step here was to assume a Gaussian bell curve when it comes to employee performance. And, heck, why not? The folks in HR who were paying attention during psychology class recall that a great many things in this world follow a Gaussian distribution, including traits that have predictive power when it comes to workplace performance. IQ is Gaussian. The Big Five Personality Trait known as Conscientiousness is likewise Gaussian. For what it’s worth, human height is also Gaussian, and that’s correlated with workplace success. And it turns out that many combinations of Gaussian variables (sums, convolutions…) are also Gaussian.</p><p><span>Giving it the data scientist “smell test” though, a few things jump out. First, are we really paying corporate salaries to people on the low end of the bell curve? The median individual income in the US is less than $50k, so my instinct is that those in corporate jobs are likely the top half of any bell curve – not the full bell curve. Also: the symmetry of the distribution doesn’t quite line up with my lived experience. I’ve seen many employees making delightful amounts (sometimes millions of dollars worth) of positive impact, but I don’t recall that their impact was offset by </span><em>the exact same number of employees</em><span> on the other end of the performance spectrum, making impact of the same millions-of-dollars magnitude, but directionally… un-stellar.</span></p><p>As we think about the impact of an employee, measured in dollars, let’s draw a connection that I haven’t seen made anywhere else. Economists will teach you something called the Marginal Productivity Theory of Wages, the idea being that the amount of money that a company is willing to spend on an employee is essentially the value that the company expects to get out of their work. This strikes me as mostly true, most of the time, and likely to be the case in the corporate world that we’re considering here.</p><p>And then on a different day, in a different part of the microeconomics textbook, there’s often a histogram of the distribution of wages: the Pareto distribution, a mathematically interesting power-law. Let’s put these two ideas from Economics together. In any given salary band, employers across the United States are paying folks wages that collectively form a Pareto distribution:</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F086a9b80-e288-4bee-a9b6-9faa20386937_584x432.png" data-component-name="Image2ToDOM" rel="nofollow ugc noopener"><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F086a9b80-e288-4bee-a9b6-9faa20386937_584x432.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F086a9b80-e288-4bee-a9b6-9faa20386937_584x432.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F086a9b80-e288-4bee-a9b6-9faa20386937_584x432.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F086a9b80-e288-4bee-a9b6-9faa20386937_584x432.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F086a9b80-e288-4bee-a9b6-9faa20386937_584x432.png" width="584" height="432" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/086a9b80-e288-4bee-a9b6-9faa20386937_584x432.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:432,&quot;width&quot;:584,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:18058,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F086a9b80-e288-4bee-a9b6-9faa20386937_584x432.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F086a9b80-e288-4bee-a9b6-9faa20386937_584x432.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F086a9b80-e288-4bee-a9b6-9faa20386937_584x432.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F086a9b80-e288-4bee-a9b6-9faa20386937_584x432.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p><em>And thus we arrive at my central idea: this is also the distribution of *performance* that we should expect from employees. Performance Management would do well to start with a Pareto assumption instead of a Gaussian assumption.</em></p><p>Like a good data scientist, I’m sweeping the mathematical details into the endnotes [0] so as to not befuddle the decision-makers, but fear not, the fun statistical stuff that I can’t shut up about is there in spades. Fun preview: the fact that corporate job postings these days include salary min/max lets us get an estimate of the size of a salary band, which was perhaps the piece of data that earlier thinkers who pondered such things were missing.</p><p>A Pareto distribution also passes the “sniff test” with respect to success and failure. I sometimes think that everything at a Fortune 500 company is Pareto distributed at every scale: my productivity this week, my productivity this year, the productivity of my team/project compared to others, and the success of my company compared to others. A few are quite successful, most are less successful. Supporting that notion with data is a task for another day, but let’s just say that I’ve already started the spreadsheet. In any case, let’s move forward to interpreting what we’ve discovered.</p><p>So what can be learned from realizing that performance follows this Pareto distribution?&nbsp;&nbsp;</p><p>Oh, here’s the percentile plot:</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa979784a-35aa-4852-b83c-c867d777ed96_571x432.png" data-component-name="Image2ToDOM" rel="nofollow ugc noopener"><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa979784a-35aa-4852-b83c-c867d777ed96_571x432.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa979784a-35aa-4852-b83c-c867d777ed96_571x432.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa979784a-35aa-4852-b83c-c867d777ed96_571x432.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa979784a-35aa-4852-b83c-c867d777ed96_571x432.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa979784a-35aa-4852-b83c-c867d777ed96_571x432.png" width="571" height="432" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/a979784a-35aa-4852-b83c-c867d777ed96_571x432.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:432,&quot;width&quot;:571,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:21721,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa979784a-35aa-4852-b83c-c867d777ed96_571x432.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa979784a-35aa-4852-b83c-c867d777ed96_571x432.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa979784a-35aa-4852-b83c-c867d777ed96_571x432.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa979784a-35aa-4852-b83c-c867d777ed96_571x432.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p>There are a few striking things here that have practical implications that inform how we might change the performance management process.</p><p><span>First, the Gaussian perspective holds that there is a bottom ~10% that performs </span><em>egregiously poorly,</em><span> and that this is an intrinsic part of the distribution. Corporations are in a constant battle to remove this portion from their payroll annually. On the other hand, looking at the Pareto percentile plot, the bottom 10% aren’t really all that different from the folks in the next 10%. As a matter of fact, there’s not an obvious place to draw a line to identify the “lowest end” employees to expunge. ~65% of employees are performing below the expectations that are associated with the salary midpoint (the green dashed line)!</span></p><p><span>But there </span><em>are</em><span> low-performing employees at large corporations; we’ve all seen them. My perspective is that they’re hiring errors. Yes, hiring errors should be addressed, but it’s not clear that there’s an obvious specific percentage of the workforce that is the result of hiring errors. We also know that some managers are great at hiring, and have a very low error rate – it’s a bit of a tragedy to force them to cut employees who are performing relatively reasonably.</span></p><p>What percentage of employees should be given extra financial rewards? Unfortunately, the Pareto plot doesn’t reveal an obvious cutpoint in this regard either. The top 35% are performing above what’s expected at the salary midpoint, so that’s a starting point. Many companies take a more granular approach than GE’s original formula, i.e. they have “good - better - best” categories for bonuses. That seems reasonable.</p><p>Regarding the very top performers, the performance management process often identifies employees who are candidates for promotion. Much like hiring errors that aren’t reflected by the Pareto distribution, I think that promotion candidates should similarly be seen as outliers. Reward them handsomely (but within bounds) since their performance is, in this case, literally off the charts, and be honest in recognizing that they’re currently being underpaid despite being at the top of the pay band.&nbsp; This is a small market inefficiency that the promotion will soon remedy.</p><p>Second, let’s consider what the Pareto percentile plot reveals about the process of firing and backfilling. Or more broadly, hiring in general. We can see from the performance vs frequency plot that in any given salary band, low performers are 3x as common as high performers! Wow. This aligns with my experience, and is an insight that speaks to the difficulty of hiring. And to the expected effectiveness of firing the bottom 10% and then expecting to hire people who are remarkably better.</p><p>Summarizing:</p><p><em><strong>There is no intrinsic bottom 10% that needs to be expunged annually.</strong></em><span> Let managers identify any hiring errors if they think their team can do better, but don’t set a target for this number based on faulty statistical notions.</span></p><p><em><strong>Hiring in general is difficult because low performers are 3x as common as high performers.</strong></em><strong> </strong><span>The Gaussian-inspired idea that you’ll probably get someone better if you fire the low end is inaccurate – the most likely replacement might not be remarkably better.</span></p><p>If someone were designing a performance management system from scratch, and they let data scientists into the room, the data scientists would likely note the following issues. These issues will all require thought, and a design that is specific to a given company (which is another way of saying that there are no easy answers that can be implemented tomorrow by everyone).</p><p>Any statistically driven system that results in decisions that cost lots of money should be monitored to make sure that it’s achieving its goals, and that it continues to work as time goes on. How do we know it’s working? What are our actual goals? There might need to be measures in place that let us know if the company is inaccurately categorizing people. Or if managers are hiring sacrificial lambs to protect their valued team. Or if there really is differentiation in performance between those who are being fired and those who are being kept.</p><p>The performance management process itself requires quite a bit of time and effort. Firing 10% of the company comes with costs (severance pay). Replacing those people is also costly (a helpful order of magnitude estimate is that the hiring process all told costs the company approximately a year’s salary). That new person will statistically be with the company for, say, three to five years. Is the increase in performance greater than the cost of the entire process? Are there opportunity costs, such that the company is better off with a less intense performance management system?&nbsp;</p><p>Performance management is a snapshot in time – one year. If an employee’s performance has been exemplary for three years in their current role, then has a down year, should the employee be shown the door? Would a longer term perspective on the employee’s suitability be a wiser approach?</p><p>Data scientists worry about this a lot. (Well, the good ones do!) Is one employee's success rate lower because they’re given all of the most difficult challenges? Now you’re measuring the way that projects are assigned. Are extroverts consistently given better ratings despite equal performance? Now you’re measuring personality traits and managerial bias.</p><p>Things like this can be somewhat accounted for and watched for when designing the assessment process, but there might be trickier things afoot. It’s a manager’s job to set their employees up for success, and managers are highly unlikely to admit that they weren’t doing so, especially during performance management season when they themselves are being graded.</p><p>It’s my opinion that the biggest factor in an employee's performance – perhaps bigger than the employee’s abilities and level of effort – is whether their manager set them up for success. It’s not easy for employees to change teams to get a better manager (perhaps it should be easier!). Maybe the rate at which employees jump ship these days and get jobs at other companies is an indicator that managers aren’t giving their employees the chance to succeed.</p><p>Similarly, it’s my observation that managers who are new to a company tend to do less-than-optimally their first time through the performance management process, especially when it comes to securing preferred performance slots for the people who work under them. The inevitable gamesmanship and political horse trading inherent in the process will go down differently at each company, and the first time through the process can be a harsh learning experience. What’s being measured might be your manager’s persuasiveness in the performance management sessions, and their political capital, as much as it is a measure of your performance.</p><p>Performance management, as practiced in many large corporations in 2024, is an outdated technology that is in need of an update. Abandoning the Gaussian assumption, and instead assuming a Pareto distribution reveals that there is no statistical basis for firing the bottom 10% of workers annually. Companies would be better off treating hiring mistakes as outliers, and should also think about whether it’s really worth it to fire 10% of their workforce annually. Monitoring to measure whether goals are being achieved, and cost analysis to determine if it’s worth it, are likely wise given the expenses of performance management.</p><p>I’ll also mention here that the conditions that birthed the modern performance management&nbsp; system (GE in the early 1980s) included an assumption of lifetime employment at a company, as long as major mistakes weren’t made. It was likely valuable, in those days, to implement some sort of carrots-and-sticks program. Forty years later, employees jump ship regularly. Perhaps withholding carrots (i.e. giving an insultingly small bonus for the lowest performers) might be enough of a stick, and healthier for all involved. Figuring all this out will take more work.</p><p><em>Executive summary: yep, there’s data from the early 20th Century that’s supportive, but seems to have been forgotten. The literature basically agrees that assessing an employee’s performance is quite subjective; identifying objective numbers isn’t an easy task, but what’s out there is consistent with the Pareto view and not the Gaussian view.&nbsp;</em></p><p>Attempting to be brief so as not to lose the reader entirely:</p><p><span>In 1957, William Shockley (Nobel Prize winner and noted jerkwad) looked at scientific publications and patents from the early 1950s from Los Alamos, Brookhaven, and Bell Labs. His conclusion – and the data are compelling in my view – was that performance is </span><em>definitely not Gaussian</em><span>. His take was that the distribution is Log-Normal (a distribution that can be very similar-looking to Pareto) based on a pet theory. In 1970, Clarence Zener (yes, he of the Zener diode) in a later publication looked at Shockley’s data along with some earlier data from a couple of other publication, including “a large sample of 6891 authors, everyone, in fact, listed in Chemical Abstracts during 1907-1916 whose names started with either A or B”, and put a lot of ink into the Log-Normal vs. Pareto question, since the answer is obviously </span><em>not Gaussian or anything even close to Gaussian.</em></p><p>More recently: Herman Anguinis has looked through data from sports, entertainment, politics, and amusingly enough, Materials Science journal publications, which makes me a part of his dataset (!). I don’t think he’s managed to put his finger on a slam-dunk dataset, but his data is directionally correct. My take on his work is that his data tends to center around total lifetime output, which speaks more to the length of one’s career than annual output, but his instincts are correct.</p><p>There are a few other references in the end notes for the curious, including a link to an AI-assisted lit search from undermind.ai.</p><p>References are all in endnote [1].</p><p><em>Executive summary: omg we’re unjustly penalizing ~5% of our employees and we didn’t know it. If we had a really big sample size, ~5% of employees would be placed into a higher performance tier based on their performance. We’re departing from objective standards by using a “forced distribution”.</em></p><p>Is there anything else that a Data Science perspective can elucidate regarding typical performance management practices? Ah yes! Let’s say that managers are sitting in a meeting evaluating a batch of 27 employees, aiming to identify the bottom 10%. That would be… 2.7 people. Someone says, “Hey, there’s no way that we can hit these percentages exactly. And what if this batch has no underperformers?”, and the response is often something like “the percentages will work out when all of the batches across the larger organization are rolled up together. So we rank-order the employees that are borderline cases, and the final cutoff lines will be drawn when everything is combined.” Everyone in the meeting nods at the idea that the Vitality Curve will emerge and all will be fair when there are enough employees batched up together. It occurs to me that this is testable with a little simulation, so I’ll believe it when I see some data!&nbsp; Perhaps a minimum cohort size might emerge where “rolling up” causes very little collateral damage?</p><p>Let’s simulate the performance of a cohort of Gaussian-distributed employees, and further let’s pretend that managers are miraculously perfect at evaluating employee performance, so a list of best-to-worst employees is generated in exactly correct order. (For the data scientists: AUROC = Mann-Whitney U-Test = 1). The question before us is thus: does the “rolling up” process introduce any inaccuracies?&nbsp; More specifically, since we secretly know every employee’s exact performance, are they correctly categorized by the process of rank ordering and drawing lines at specific percentiles?</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F306d61bb-0494-4da3-acab-b39901287a42_576x432.png" data-component-name="Image2ToDOM" rel="nofollow ugc noopener"><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F306d61bb-0494-4da3-acab-b39901287a42_576x432.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F306d61bb-0494-4da3-acab-b39901287a42_576x432.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F306d61bb-0494-4da3-acab-b39901287a42_576x432.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F306d61bb-0494-4da3-acab-b39901287a42_576x432.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F306d61bb-0494-4da3-acab-b39901287a42_576x432.png" width="576" height="432" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/306d61bb-0494-4da3-acab-b39901287a42_576x432.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:432,&quot;width&quot;:576,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:32057,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F306d61bb-0494-4da3-acab-b39901287a42_576x432.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F306d61bb-0494-4da3-acab-b39901287a42_576x432.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F306d61bb-0494-4da3-acab-b39901287a42_576x432.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F306d61bb-0494-4da3-acab-b39901287a42_576x432.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p><span>Here is a plot of the percent mis-categorized unjustly vs. cohort size, with 32 simulated annual performance reviews for each cohort size. The Welch GE cutoffs were used (bottom 10%, top 20%). I’ve only counted the number of employees who would have been in a </span><em>higher</em><span> category if objective standards were used instead of rank-and-chop; the numbers basically double if we decide to also count employees who are (beneficially) bumped up a category instead of down.&nbsp; My takeaway here is that there’s a very good chance of mis-categorizing 2.5% of a cohort no matter the cohort size, and 5% happens commonly even if the cohort size is a couple hundred.</span></p><p><strong>Size of a pay band</strong><span>: when posting corporate job openings, there’s a marvelous new practice of displaying a salary band (min,max). A casual browse has revealed the tendency that a minimum salary is generally around 0.65x of the maximum, for any given pay band. This allows us to visualize the Pareto distribution </span><em>within a pay band</em><span>, which I haven’t seen done anywhere in the literature yet. For those looking to play along at home: a pay band of 78.1 → 121.2 is centered on 100 and has min/max = 0.65.</span></p><p><strong>Pareto alpha</strong><span>=1.75 this was drawn from a fit to individual salary data in the US (thank you, IPUMS for the data, details below), which can also be cross-checked using the Gini Coefficient as follows: G = 1/(2a-1), where G = 0.4 is commonly cited for the US individual income (not household income!). Note that this one parameter, along with the size of the pay band, is all we need in order to specify the distribution! </span><a href="https://en.wikipedia.org/wiki/Pareto_distribution" rel="nofollow ugc noopener">https://en.wikipedia.org/wiki/Pareto_distribution</a><span> is a great starting place for those who wish to ponder the equations.</span></p><p><strong>Gaussian sigma</strong><span>: the outer edges of the pay band were assumed to be 3 sigma, but using e.g. 2.5 sigma gives similar results. Toss the outliers, which is 0.27% or 1.24% of the simulated data. (Easily justified: HR has rules about staying within the pay band, and is perfectly happy to underpay or overpay folks a little at the edges). Use 7.06 for the 3 sigma scenario, or 8.48 for the 2.5 sigma scenario, fits the 100 +/- 21.2 payband.</span></p><p><strong>Centering at 100 “performance units”</strong><span>: These plots are centered at 100 not so much to represent a salary of $100k, but rather because humans are fond of thinking of things on a scale of 0-100. A fun mathematical aspect of Pareto distributions is that they’re scale-invariant / self-similar, i.e., the dropoff from 100k to 90k is the same as the dropoff from 100 million to 90 million. So the choice of 100 is arbitrary, and the calculations work for any value that you might choose; the only quantity that matters here is the width of the pay band as a percent of, say, the minimum value. And similarly, with the Gaussian, the choice of median is arbitrary, the only thing that matters is sigma / xbar.</span></p><p>Plots are n=25_000</p><p>Perhaps I’ll clean up the Jupyter notebook and put it on GitHub at some point.</p><p>A few takeaways:&nbsp;</p><p>My reading of the more modern literature is that they’ve basically forgotten the Zener &amp; Shockley papers, which I find to have the most compelling data. Regarding Pareto vs. Log-Normal for employee performance (and for individual income): to me, there’s not much practical difference. (I do have my own pet theory about the origin of the Pareto (or LogNormal) in these circumstances, which involves neural networks and hopefully ends with a Renormalization Group universality class, but that’s a bigger project for a later time.) Back to the literature: there are a number of other papers and articles not listed here that champion the idea of “it’s not Gaussian”, but they’re a bit short on listing concrete suggestions on what one should do once that realization sinks in.</p><p>Clarence Zener “Statistical Theories of Success” Proceedings of the National Academy of Sciences Vol. 66, No. 1, pp. 18-24, May 1970</p><p>William Shockley “On the Statistics of Individual Variations of Productivity in Research Laboratories” Proceedings Of The IRE 1957 page 279</p><p>Richard J. Chambers II “Evaluating indicators of job performance: Distributions and types of analyses” Doctoral Dissertation College Of Education Louisiana Tech University November 2016</p><p>Ernest O’Boyle Jr. And Herman Aguinis “The Best And The Rest: Revisiting The Norm Of Normality Of Individual Performance” Personnel Psychology 2012, 65, 79–119</p><p>A. Drăgulescu and V.M. Yakovenkoa “Evidence for the exponential distribution of income in the USA” Eur. Phys. J. B 20, 585–589 (2001)</p><p>A. Christian Silva and Victor M. Yakovenko “Temporal evolution of the thermal and superthermal income classes in the USA during 1983–2001” Europhys. Lett., 69 (2), pp. 304–310 (2005)</p><p>Additionally, I did an AI-assisted literature search to make sure I wasn’t missing anything big and important, which you can access here:</p><p><a href="http://www.undermind.ai/query_app/display_one_search/41a512c42dbfb3065285856443c5399f87b275ff2fb77ec2b41fa5aec8972467/" rel="nofollow ugc noopener">http://www.undermind.ai/query_app/display_one_search/41a512c42dbfb3065285856443c5399f87b275ff2fb77ec2b41fa5aec8972467/</a></p><p>The data that I used to fit the Pareto to individual income (not household income!) came from the variables</p><ol><li><p>INCWAGE (Wage and salary income)</p></li><li><p>ASECWT (Annual Social and Economic Supplement Weight)</p></li></ol><p>from</p><p><span>Sarah Flood, Miriam King, Renae Rodgers, Steven Ruggles, J. Robert Warren, Daniel Backman, Annie Chen, Grace Cooper, Stephanie Richards, Megan Schouweiler, and Michael Westberry. IPUMS CPS: Version 11.0 [dataset]. Minneapolis, MN: IPUMS, 2023. </span><a href="https://doi.org/10.18128/D030.V11.0" rel="nofollow ugc noopener">https://doi.org/10.18128/D030.V11.0</a></p></div></article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Noise-Canceling Single-Layer Woven Silk and Cotton Fabric (134 pts)]]></title>
            <link>https://onlinelibrary.wiley.com/doi/10.1002/adma.202313328</link>
            <guid>42235909</guid>
            <pubDate>Mon, 25 Nov 2024 13:00:15 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://onlinelibrary.wiley.com/doi/10.1002/adma.202313328">https://onlinelibrary.wiley.com/doi/10.1002/adma.202313328</a>, See on <a href="https://news.ycombinator.com/item?id=42235909">Hacker News</a></p>
Couldn't get https://onlinelibrary.wiley.com/doi/10.1002/adma.202313328: Error: Request failed with status code 403]]></description>
        </item>
        <item>
            <title><![CDATA[Why I have resigned from the Royal Society (127 pts)]]></title>
            <link>http://deevybee.blogspot.com/2024/11/why-i-have-resigned-from-royal-society.html</link>
            <guid>42234497</guid>
            <pubDate>Mon, 25 Nov 2024 09:03:57 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="http://deevybee.blogspot.com/2024/11/why-i-have-resigned-from-royal-society.html">http://deevybee.blogspot.com/2024/11/why-i-have-resigned-from-royal-society.html</a>, See on <a href="https://news.ycombinator.com/item?id=42234497">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="post-body-1326298114103223151" itemprop="description articleBody">
<p><a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEg2Y-vThaI32BMolboVeqFScPWCpnjWhhmMRnj_tIxkrmET28q_V0IK0kYMFC_eH9ptMiYOyPhMK3Dexs3gLkB6ClY3Po2CaiyINEPcxv8G88dGEBA8J1xTS5753holfA3TOyt58ymqZ0XDC7TeplvQeAQNCePpiaTsmZA3IgbxwxLlLBJvLNdjch7DSBc/s800/TRS_location_image_1.jpg"><img data-original-height="300" data-original-width="800" height="120" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEg2Y-vThaI32BMolboVeqFScPWCpnjWhhmMRnj_tIxkrmET28q_V0IK0kYMFC_eH9ptMiYOyPhMK3Dexs3gLkB6ClY3Po2CaiyINEPcxv8G88dGEBA8J1xTS5753holfA3TOyt58ymqZ0XDC7TeplvQeAQNCePpiaTsmZA3IgbxwxLlLBJvLNdjch7DSBc/s320/TRS_location_image_1.jpg" width="320"></a></p><p>The Royal Society is a venerable institution founded in 1660, whose original members included such eminent men as Christopher Wren, Robert Hooke, Robert Boyle and Isaac Newton. It promotes science in many ways: administering grants, advising government, holding meetings and lectures, and publishing expert reports on scientific matters of public importance. &nbsp;</p><div><p>There are currently around 1,800 Fellows and Foreign Members of the Royal Society, all elected through a <a href="https://royalsociety.org/fellows-directory/election" target="_blank">stringent and highly competitive process</a> which includes nomination by two Fellows of the Royal Society (FRS), detailed scrutiny of the candidate's achievements and publications, reports by referees, and consideration by a committee of experts in their broad area of research. &nbsp;Although most Fellows are elected on the basis of their scientific contributions, others are nominated on the basis of "<i>wider contributions to science, engineering or medicine through leadership, organisation, scholarship or communication</i>".</p><div><p>For many scientists, election to the Royal Society is the pinnacle of their scientific career. It establishes that their achievements are recognised as exceptional, and the title FRS brings immediate respect from colleagues. Of course, things do not always work out as they should. Some Fellows may turn out to have published fraudulent work, or go insane and start promoting crackpot ideas. Although there are procedures that allow a fellow to be expelled from the Royal Society, I have been told this has not happened for over 150 years. It seems that election as a Fellow of the Royal Society, like loss of virginity, is something that can't readily be reversed.</p><div><p>This brings us, then, to the case of Elon Musk, who was <a href="https://royalsociety.org/people/elon-musk-13829/" target="_blank">elected as a Fellow of the Royal Society in 2018</a>&nbsp;on the basis of his technological achievements, notably in space travel and electrical vehicle development. Unfortunately,  since that time, his interests have extended to <a href="https://www.nbcnews.com/tech/social-media/elon-musk-turned-x-trump-echo-chamber-rcna174321" target="_blank">using social media for political propaganda</a>, while at the same time battling what he sees as "<a href="https://eu.usatoday.com/story/tech/2024/07/22/elon-musk-jordan-peterson-interview/74506785007/" target="_blank">woke mind virus</a>" and <a href="https://www.nbcnews.com/tech/tech-news/elon-musk-worries-free-speech-advocates-calls-prosecute-researchers-cr-rcna179194" target="_blank">attacks on free speech</a>.  Whereas previously he seemed to agree with mainstream scientific opinion on issues such as climate change and medicine, over the past year or two, he's started promoting alternative ideas. &nbsp;&nbsp;</p><div><p>In summer of 2024, a number of FRSs became concerned at how Musk was using his social media platform (previously Twitter, now termed X) to stir up racial unrest and anti-government sentiment in the UK.  Notable tweets by him from this period included incendiary comments and frank misinformation, as documented in this <a href="https://www.theguardian.com/commentisfree/article/2024/aug/09/uk-far-right-riots-elon-musk-x" target="_blank">Guardian article</a>.&nbsp;</p><div><p>This led to a number of Fellows expressing dismay that Musk had been elected. There was no formal consultation of the Fellowship but via informal email contacts, a group of 74 Fellows formulated a letter of concern that was sent in early August to the President of the Royal Society, raising doubts as to whether he was "<i>a fit and proper person to hold the considerable honour of being a Fellow of the Royal Society".</i>&nbsp;The letter specifically mentioned the way Musk had used his platform on X to make unjustified and divisive statements that served to inflame right-wing thuggery and racist violence in the UK.&nbsp;</p><div><p>I gather that at this point the Royal Society Council opted to consult a top lawyer to determine whether Musk's behaviour breached their Code of Conduct.  The problem with this course of action is that if you are uncertain about doing something that seems morally right but may have consequences, then it is easy to find a lawyer who will advise against doing it. That's just how lawyers work. They're paid to rescue people from ethical impulses that may get them into trouble.  And, sure enough, the lawyer determined that Musk hadn't breached the Code of Conduct.  If you want to see if you agree, you can find the Code of Conduct <a href="https://royalsociety.org/about-us/how-we-are-governed/governance" target="_blank">here</a>.</p><p>Many of the signatories of the letter, including me, were unhappy with this response.  We set about assembling further evidence of behaviours incompatible with the Code of Conduct.  There is a lot of material, which can be broadly divided into two categories, depending on whether it relates to "Scientific conduct" or "Principles". &nbsp;</p><div><p>
  

On <b>Scientific conduct</b>, the most relevant points from the Code of Conduct are:</p><blockquote><p><i>
  
2.6. Fellows and Foreign Members shall carry out their scientific research with regard to the Society's statement on research integrity and to the highest standards.&nbsp;</i></p><p><i>2.10. Fellows and Foreign Members shall treat all individuals in the scientific enterprise collegially and with courtesy, including supervisors, colleagues, other Society Fellows and Foreign Members, Society staff, students and other early‐career colleagues, technical and clerical staff, and interested members of the public.&nbsp;</i></p><p><i>2.11. Fellows and Foreign Members shall not engage in any form of discrimination, harassment, or bullying.</i></p></blockquote><div><p>Most of those I've spoken to agree that a serious breach of these principles was in 2022, when <a href="https://x.com/elonmusk/status/1601894132573605888?s=20&amp;t=aLV0JH6LGhKMYKYed2T9Fw" target="_blank">Musk tweeted</a>: "<i>My pronouns are Prosecute/Fauci</i>", thereby managing to simultaneously offend the LGBTQ community, express an antivaxx sentiment, and put Fauci, <a href="https://www.theguardian.com/us-news/article/2024/jun/03/anthony-fauci-covid-19-threats-harassment" target="_blank">already under attack from antivaxxers</a>, at further risk.  Fauci was not a Fellow at the time these comments were made, but that should not matter given the scope of the statement is "<i>individuals in the scientific community</i>".  This incident was <a href="https://www.cbsnews.com/news/elon-musk-anthony-fauci-viral-tweet-backlash-health-experts/" target="_blank">covered by CBS News.</a></p><p>Now that the US election is over, Musk seems emboldened to ramp up his attacks. On 19th November 2024, he&nbsp;<a href="https://x.com/elonmusk/status/1858873877184627039?s=58&amp;t=RqV9ku4ADjOWHIF2HcuosQ" target="_blank">retweeted this</a> to his millions of followers, followed by a <a href="https://x.com/elonmusk/status/1859596285549920658?s=58&amp;t=RqV9ku4ADjOWHIF2HcuosQ" target="_blank">compilation of attacks</a> on Fauci on 21st November,</p><p><a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEj-zZemJ-PPwsC6435Xfdf_jg-kbsqzKuam70Da9TDWjBk03RwGvKQMyV2s0dbcok7QmTUPxqIZz5B6BPh6J56pnk0wap3G3Fg9VSKaKVPgKu6bwKmUFLaReQFMcc-wk7T1O_LQOlayiVleniufKTA38fFtDlLWYKqlmqvzWoTW6HdNCQGgTMFyVdemK8s/s548/Screenshot%202024-11-24%20at%2010.43.21.png"><img data-original-height="548" data-original-width="467" height="320" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEj-zZemJ-PPwsC6435Xfdf_jg-kbsqzKuam70Da9TDWjBk03RwGvKQMyV2s0dbcok7QmTUPxqIZz5B6BPh6J56pnk0wap3G3Fg9VSKaKVPgKu6bwKmUFLaReQFMcc-wk7T1O_LQOlayiVleniufKTA38fFtDlLWYKqlmqvzWoTW6HdNCQGgTMFyVdemK8s/s320/Screenshot%202024-11-24%20at%2010.43.21.png" width="273"></a></p><p><b><i>Neuralink</i></b></p><p>There are also questions about the management of Musk's research project, Neuralink, which involves developing a brain-computer interface to help people who are paralysed.  While this is clearly a worthy goal, his approach to conducting research is characterised by refusal to let anyone interfere with how he does things. This has led to <a href="https://pcrm.widen.net/s/llzr7cg57q/request-for-glp-investigation-re-neuralink---with-enclosures---12.13.22" target="_blank">accusations of failure to adhere to regulatory procedures for Good Laboratory Practic</a>e. For instance, consider these quotes from <a href="https://spectrum.ieee.org/neuralink-human-trials" target="_blank">this article</a>:&nbsp;</p><div><div><blockquote><i>'I think what concerns people is that Neuralink could be cutting corners, and so far nobody has stopped them,' says Nick Ramsey, a clinical neuroscientist at University Medical Center Utrecht, in the Netherlands. &nbsp;</i><i>There’s an incredible push by Neuralink to bypass the conventional research world, and there’s little interaction with academics, as if they think that we’re on the wrong track—that we’re inching forward while they want to leap years forward.</i></blockquote></div><div><blockquote><i>In response to Musk's claim that no monkey had died because of Neuralink, the Physicians Committee for Responsible Medicine wrote to the SEC, claiming Musk’s comments were false. The group said it had obtained veterinary records from Neuralink’s experiments showing that at least 12 young, healthy monkeys were euthanized as a result of problems with Neuralink’s implant. The group alleged that Musk’s comments are misleading investors, and urged SEC regulators to investigate Musk and Neuralink for securities fraud.</i></blockquote></div><div><p>The problems with Neuralink do not stop with the ethics of the animals and the secrecy surrounding them.  In a <a href="https://doi.org/10.1038/d41586-024-00304-4" target="_blank">piece in Nature</a>, various scientists were interviewed about the first human trial that was conducted earlier this year. The main concern was lack of transparency. Human trials are usually recorded in <i>clinical.trials.gov</i>, which was set up precisely to make it easier to track if studies had followed a protocol.  Musk did not do this. His approach to the human trials again reflects his distaste for any regulations.  But the regulations are there for a purpose, and one would expect a Fellow of the Royal Society to abide by them; otherwise we end up with scandals such as <a href="https://www.panmacmillan.com/blogs/literary/theranos-elizabeth-holmes-john-carreyrou" target="_blank">Theranos</a>&nbsp;or the <a href="https://www.science.org/content/article/two-controversial-stem-cell-trials-could-harm-patients-critics-say" target="_blank">stem cell experiments</a> by Macchiarini and Birchall. The <a href="https://www.statnews.com/2024/07/08/neuralink-elon-musk-scientific-ethics-brain-computer-interface/" target="_blank">ethics of this kind of trial</a> also needs careful handling, especially in terms of the patient's understanding of possible adverse effects, their expectations of benefits, and the undertaking of researchers to provide long-term support for the prosthesis.</p><div><p>
  

If we turn to the more general issues that come under <b>Principles</b>, then the Code of Conduct states:&nbsp;</p><div><blockquote><i>Fellows and Foreign Members shall not act or fail to act in any way which would undermine the Society's mission or bring the Society into disrepute</i>.</blockquote></div><p>&nbsp;Here are some examples that I would regard as contrary to the Society's mission.</p><div><p><b><i>
  

 
Promoting vaccine hesitation</i></b></p><p>The Royal Society has done good work promoting public understanding of vaccines, as with<a href="https://royalsociety.org/blog/2021/01/why-we-know-vaccines-work/" target="_blank"> this blogpost</a> by Charles Bangham FRS.  In contrast, <a href="https://www.livemint.com/news/world/elon-musk-factchecked-after-misleading-post-on-covid-19-vaccine-efficacy-know-more-11697730755260.html" target="_blank">as described here</a>, Musk has promoted <a href="https://www.bbc.com/news/health-66313916" target="_blank">vaccine conspiracy theories</a> and anti-vaccine views on his platform. 
<a href="https://x.com/elonmusk/status/1817357502725407024" target="_blank">This Tweet</a> had 85 million views:</p><p><a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgg3N6oEahfkSpd0OHN-VK6TtLpOZMbP9r5VGszAJn4Vnha17tlUxfuZlhJf6SsHND_1mIO5OR__l5JYFqOVvLUdLj3yR_hCHs_G_yUeOiRcK_rsj89RE81ubkWmRQp3Y3cSWQhne03NMyDRHPY5jWZVhTbj_Hqtl2mEsL7RMz7U9H7K2My6TmzTODgjQY/s338/dead%20unvacc.png"><img data-original-height="338" data-original-width="295" height="320" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgg3N6oEahfkSpd0OHN-VK6TtLpOZMbP9r5VGszAJn4Vnha17tlUxfuZlhJf6SsHND_1mIO5OR__l5JYFqOVvLUdLj3yR_hCHs_G_yUeOiRcK_rsj89RE81ubkWmRQp3Y3cSWQhne03NMyDRHPY5jWZVhTbj_Hqtl2mEsL7RMz7U9H7K2My6TmzTODgjQY/s320/dead%20unvacc.png" width="279"></a></p><br><div><p><b><i>
  
 
Downplaying the climate emergency </i></b></p><p>In 2023 Musk <a href="https://www.theguardian.com/environment/2023/nov/20/elon-musk-green-credentials-clean-energy-climate-deniers" target="_blank">played down the seriousness of climate change</a>, and 2024 participated in a bizarre <a href="https://www.theguardian.com/environment/article/2024/aug/13/trump-musk-x-climate" target="_blank">interview with Donald Trump</a>, which dismayed climate experts.  Among the commenters was Michael Mann, who said “<i>It is sad that Elon Musk has become a climate change denier, but that’s what he is. He’s literally denying what the science has to say here</i>.”  Mann was elected as a Foreign Member of the Royal Society in 2024.</p><div><p><b><i>
  
 
Spreading deep fakes and misinformation on X</i></b></p><p>As recently as 2022, the Royal Society published<a href="https://royalsociety.org/-/media/policy/projects/online-information-environment/the-online-information-environment.pdf" target="_blank"> a report</a>&nbsp;in which Frank Kelly (FRS) <span>noted&nbsp;<span>the high priority that the Royal Society gives to accurate scientific communication:</span></span></p><div><div><blockquote><i>The Royal Society’s mission since it was established in 1660 has been to promote science for the benefit of humanity, and a major strand of that is to communicate accurately. But false information is interfering with that goal. It is accused of fuelling mistrust in vaccines, confusing discussions about tackling the climate crisis and influencing the
debate about genetically modified crops.&nbsp;</i></blockquote></div><div><blockquote><i>&nbsp;Musk’s reason for buying Twitter was to influence the social discourse. And influence he did—by using his enormous platform (203 million followers) to endorse Trump, spread disinformation about voter fraud and deep fakes of Kamala Harris, and amplify conspiracy theories about everything from vaccines to race replacement theory to misogyny.</i></blockquote><p>The most recent development is the announcement that Musk is to be co-director of the new Department of Government Efficiency (DOGE, an allusion to the cryptocurrency Dogecoin) in the Trump Administration, with a brief to cut waste and bureaucracy. The future for US science is starting to look bleak, with Musk being given unfettered powers to cut budgets to NIH and NASA, among others. &nbsp;<a href="https://x.com/america/status/1857228761915412814" target="_blank">This tweet</a>, which he endorsed, indicates that rather than using objective evidence, the cuts will fall on those who have criticized Trump, who will find bowdlerized summaries of their work used to generate public outrage. The tweet reads: &nbsp;"<i>Here’s what the U.S. Government wasted $900 Billion of your tax dollars on in 2023.  The Department of Government Efficiency (@DOGE) will fix this. America deserves leaders that prioritize sensible spending</i>" before presenting a chart listing items for cuts, with unsourced descriptions of expenditure, including:</p><div><div><ul><li><i>Dr Fauci's monkey business on NIH's "monkey island": &nbsp; $33,200,000&nbsp;</i></li><li><i>NIH's meth-head monkeys: &nbsp;portion of $12,000,000&nbsp;</i></li><li><i>Dr Fauci's transgender monkey study: $477,121</i></li></ul><p>I'm sad to say I agree with Alex Wild, Curator of Entomology at University of Texas Austin, who&nbsp;<a href="http://deevybee.blogspot.com/2024/11/%22https://bsky.app/profile/alexwild.bsky.social/post/3lbmvdzmxos2e" target="_blank">wrote</a>&nbsp;a few days ago: <i>"I hope federally funded scientists are preparing for large scale, bad faith attacks by Musk and his troll army. &nbsp;It’s pretty clear the DOGE operation is going to take snippets of grant proposals and papers, present them out of context, and direct weaponized harassment of individual people."</i></p></div><div><p><b>What next? &nbsp;</b></p><p>I've been told that in the light of the evolving situation, the Royal Society Council will look again at the case of Elon Musk.  In conversations I have had with them, they emphasise that they must adhere to their own procedures, which are specified in the Statutes, and which involve a whole series of stages of legal scrutiny, committee evaluation, discussion with the Fellow in question,  and ultimately a vote from the Fellowship, before a Fellow or Foreign Member could be expelled. While I agree that if you have a set of rules you should stick to them, I find the fact that nobody has been expelled for over 150 years telling. It does suggest that the Statutes are worded so that it is virtually impossible to do anything about Fellows who breach the Code of Conduct. In effect the Statutes serve a purpose of protecting the Royal Society from ever having to take action against one of its Fellows.</p><div><p>In the course of investigating this blogpost, I've become intimately familiar with the Code of Conduct, which requires me to "<i>treat all individuals in the scientific enterprise collegially and with courtesy, including ... foreign Members</i>".   I'm not willing to treat Elon Musk "<i>collegially and with courtesy</i>".  Any pleasure I may take in the distinction of the honour of an FRS is diminished by the fact it is shared with someone who appears to be modeling himself on a Bond villain, a man who has immeasurable wealth and power which he will use to threaten scientists who disagree with him. Accordingly, last week I resigned my FRS. I don't do this in the expectation of having any impact: in the context of over 350 years of Royal Society history, this is just a blip. I just feel far more comfortable to be dissociated from an institution that continues to honour this disreputable man.</p></div></div></div></div></div></div></div></div></div></div></div></div></div></div></div></div></div></div><p><span>Note: Comments will be accepted if they are by a named individual, civil, and on topic. They are moderated and there may be a delay before they appear online.&nbsp;</span></p></div>

</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Judge's Investigation into Patent Troll Results in Criminal Referrals (111 pts)]]></title>
            <link>https://www.eff.org/deeplinks/2024/11/judges-investigation-patent-troll-ip-edge-results-criminal-referrals</link>
            <guid>42234147</guid>
            <pubDate>Mon, 25 Nov 2024 07:55:19 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.eff.org/deeplinks/2024/11/judges-investigation-patent-troll-ip-edge-results-criminal-referrals">https://www.eff.org/deeplinks/2024/11/judges-investigation-patent-troll-ip-edge-results-criminal-referrals</a>, See on <a href="https://news.ycombinator.com/item?id=42234147">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
            <article role="article">
  
  
  <div><p><span></span><span>In 2022, three companies with strange names and no clear business purpose beyond&nbsp; patent litigation filed dozens of lawsuits in Delaware federal court, accusing businesses of all sizes of patent infringement. Some of these complaints claimed patent rights over basic aspects of modern life; one, for example, involved a&nbsp; </span><a href="https://www.eff.org/deeplinks/2023/02/stupid-patent-month-clocking-work-app"><span>patent that pertains to the process of clocking in to work through an app</span></a><span>.</span></p>
<p><span>These companies–named Mellaconic IP, Backertop Licensing, and Nimitz Technologies–seemed to be typical examples of “patent trolls,”</span> <span>companies whose primary business is suing others over patents or demanding licensing fees rather than providing actual products or services.&nbsp;</span></p>
<p><span>However, the cases soon took an unusual turn. The Delaware federal judge overseeing the cases, U.S. District Judge Colm Connolly, sought more information about the patents and their ownership. One of the alleged owners was a food-truck operator who had been promised “passive income,” but was entitled to only a small portion of any revenue generated from the lawsuits. Another owner was the spouse of an attorney at IP Edge, the patent-assertion company linked to all three LLCs.&nbsp;</span></p>
<p><span>Following an extensive investigation, the judge determined that attorneys associated with these shell companies had violated legal ethics rules. He pointed out that the attorneys may have misled Hau Bui, the food-truck owner, about his potential liability in the case. Judge Connolly wrote:&nbsp;</span></p>
<blockquote><p><span>[T]he disparity in legal sophistication between Mr. Bui and the IP Edge and Mavexar actors who dealt with him underscore that counsel's failures to comply with the Model Rules of Professional Conduct while representing Mr. Bui and his LLC in the Mellaconic cases are not merely technical or academic.</span></p>
</blockquote>
<p><span>Judge Connolly also concluded that IP Edge, the patent-assertion company behind hundreds of patent lawsuits and linked to the three LLCs, was the “de facto owner” of the patents asserted in his court, but that it attempted to hide its involvement. He wrote, “IP Edge, however, has gone to great lengths to hide the ‘we’ from the world,” with "we" referring to IP Edge. Connolly further noted, “IP Edge arranged for the patents to be assigned to LLCs it formed under the names of relatively unsophisticated individuals recruited by [IP Edge office manager] Linh Deitz.”&nbsp;</span></p>
<p><span>The judge </span><a href="https://news.bloomberglaw.com/ip-law/judges-litigation-funding-probe-reveals-ip-edges-human-toll"><span>referred</span></a><span> three IP Edge attorneys to the </span><a href="https://www.bloomberglaw.com/public/desktop/document/NimitzTechnologiesLLCvCNETMediaIncDocketNo121cv01247DDelAug302021/12?doc_id=X1MU2O97VP39OFRLOSOF11VA4JP"><span>Supreme Court of Texas’ Unauthorized Practice of Law Committee</span></a><span> for engaging in “unauthorized practices of law in Texas.” Judge Connolly also sent a </span><a href="https://www.bloomberglaw.com/public/desktop/document/NimitzTechnologiesLLCvCNETMediaIncDocketNo121cv01247DDelAug302021/11?doc_id=X2DE7OM6SOT9VIOU8VQAC5CBUSF"><span>letter to the Department of Justice</span></a><span>, suggesting an investigation into “individuals associated with IP Edge LLC and its affiliate Maxevar LLC.”&nbsp;</span></p>
<h3><span>Patent Trolls Tried To Shut Down This Investigation</span></h3>
<p><span>The attorneys involved in this wild patent trolling scheme challenged Judge Connolly’s authority to proceed with his investigation. However, because transparency in federal courts is essential and applicable to all parties, including patent assertion entities, </span><a href="https://www.eff.org/deeplinks/2022/12/judges-investigation-patent-trolls-must-be-allowed-move-forward"><span>EFF and two other patent reform groups filed a brief in support of the judge’s investigation</span></a><span>. The brief argued that “[t]he public has a right—and need—to know who is controlling and benefiting from litigation in publicly-funded courts.” Companies targeted by the patent trolls, as well as the Chamber of Commerce, filed their own briefs supporting the investigation.&nbsp;</span></p>
<p><span>The </span><a href="https://www.eff.org/files/2022/12/08/44_-_order_denying_nimitz_writ.pdf"><span>appeals court sided with us</span></a><span>, </span><a href="https://www.eff.org/deeplinks/2022/12/victory-judges-critical-investigation-patent-troll-companies-can-move-forward"><span>upholding</span></a><span> Judge Connolly’s authority to proceed, which led to the referral of the involved attorneys to the disciplinary counsel of their respective bar associations.&nbsp;</span></p>
<p><span>After this damning ruling, one of the patent troll companies and its alleged owner made a final effort at appealing this outcome. In July of this year, the U.S. Court of Appeals for the Federal Circuit </span><a href="https://cafc.uscourts.gov/opinions-orders/23-2367.OPINION.7-16-2024_2350699.pdf"><span>ruled </span></a><span>that investigating Backertop Licensing LLC and ordering its alleged owner to testify was “an appropriate means to investigate potential misconduct involving Backertop.”&nbsp;</span></p>
<p><span>In EFF’s view, these types of investigations into the murky world of patent trolling are not only appropriate but should happen more often. Now that the appeals court has ruled, let’s take a look at what we learned about the patent trolls in this case.&nbsp;</span></p>
<h3><span>Patent Troll Entities Linked To French Government</span></h3>
<p><span>One of the patent trolling entities, Nimitz Technologies LLC, asserted a single patent, U.S. Patent No. 7,848,328, against 11 companies. When the judge required Nimitz’s supposed owner, a man named Mark Hall, to testify in court, Hall could not describe anything about the patent or explain how Nimitz acquired it. He didn’t even know the name of the patent (“Broadcast Content Encapsulation”). When asked what technology was covered by the patent, he said, “I haven’t reviewed it enough to know,” and when asked how he paid for the patent, Hall replied, “no money exchanged hands.”&nbsp;</span></p>
<p><span>The exchange between Hall and Judge Connolly went as follows:&nbsp;</span></p>
<blockquote><p><span>Q. S</span>o how do you come to own something if you never paid for it with money?</p>
<p>A. I wouldn't be able to explain it very well. That would be a better question for Mavexar.</p>
<p>Q. Well, you're the owner?</p>
<p>A. Correct.</p>
<p>Q. How do you know you're the owner if you didn't pay anything for the patent?</p>
<p>A. Because I have the paperwork that says I'm the owner.</p>
</blockquote>
<p><span>(</span><a href="https://www.eff.org/document/nimitz-technologies-v-cnet-et-al-memorandum-order"><span>Nov. 27, 2023 Opinion</span></a><span>, pages 8-9.)&nbsp;</span></p>
<p><span>The Nimitz patent originated from the Finnish cell phone company Nokia, which later&nbsp;assigned it and several other patents to France Brevets, a French sovereign investment fund, in 2013. France Brevets, in turn, assigned&nbsp;the patent to a US company called Burley Licensing LLC, an entity linked to IP Edge, in 2021. Hau Bui (the food truck owner) signed on behalf of Burley, and Didier Patry, </span><a href="https://www.privateequitywire.co.uk/didier-patry-appointed-ceo-france-brevets-sovereign-investment-fund/"><span>then the CEO of France Brevets</span></a><span>, signed on behalf of the French fund.&nbsp;</span></p>
<p><span>France Brevets was </span><a href="https://www.iam-media.com/article/france-brevets-public-sector-links-hindered-its-monetisation-efforts-says-ceo"><span>an investment fund formed in 2009</span></a><span> with €100 million in seed money from the French government to manage intellectual property. France Brevets was set to receive 35% of any revenue related to “monetizing and enforcement” of the patent, with Burley agreeing to file at least one patent infringement lawsuit within a year, and collect a “total minimum Gross Revenue of US $100,000” within 24 months, or the patent rights would be given back to France Brevets.&nbsp;</span></p>
<p><span>Burley Licensing LLC, run by IP Edge personnel, then created Nimitz Technologies LLC— a company with no assets except for the single patent. They obtained a mailing address for it from a Staples in Frisco, Texas, and assigned the patent to the LLC in August 2021, while the obligations to France Brevets remained unchanged until the fund </span><a href="https://www.iam-media.com/article/france-brevets-public-sector-links-hindered-its-monetisation-efforts-says-ceo"><span>shut down in 2022</span></a><span>.</span></p>
<h3><span>The Bigger Picture</span></h3>
<p><span></span><span>It’s troubling that patent lawsuits are often funded by entities with no genuine interest in innovation, such as private equity firms. However, it’s even more concerning when foreign government-backed organizations like France Brevets manipulate the US patent system for profit. In this case, a Finnish company sold its patents to a French government fund, which used US-based IP lawyers to file baseless lawsuits against American companies, including well-known establishments like Reddit and Bloomberg, as well as smaller</span><a href="https://www.tastemade.com/about"><span> ones like Tastemade</span></a><span> and </span><a href="https://www.crunchbase.com/organization/skillshare"><span>Skillshare</span></a><span>.</span></p>
<p><span>Judges should enforce rules requiring transparency about third-party funding in patent lawsuits. When ownership is unclear, it’s appropriate to insist that the real owners show up and testify—before dragging dozens of companies into court over dubious software patents.&nbsp;</span></p>
<p><span>Related documents:&nbsp;</span></p>
<ul>
<li><a href="https://www.eff.org/files/2024/09/09/1_22-cv-00413-cfc_34_primary_document.pdf"><span>Memorandum and Order</span></a><span> referring counsel to disciplinary bodies (Nov. 23, 2023)&nbsp;</span></li>
<li><a href="https://cafc.uscourts.gov/opinions-orders/23-2367.OPINION.7-16-2024_2350699.pdf"><span>Federal Circuit Opinion</span></a><span> affirming the order requiring Lori LaPray to appear “for testimony regarding potential fraud on the court,” as well as the District Court’s order of monetary sanction against Ms. LaPray for subsequently failing to appear</span></li>
</ul>


</div>

          </article>
    </div><div>
          <h2>Join EFF Lists</h2>
        
    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[A Day in the Life: The Global BGP Table (169 pts)]]></title>
            <link>https://articles.foletta.org/post/2024-01-08-a-day-in-the-life-the-bgp-table/</link>
            <guid>42233565</guid>
            <pubDate>Mon, 25 Nov 2024 05:41:45 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://articles.foletta.org/post/2024-01-08-a-day-in-the-life-the-bgp-table/">https://articles.foletta.org/post/2024-01-08-a-day-in-the-life-the-bgp-table/</a>, See on <a href="https://news.ycombinator.com/item?id=42233565">Hacker News</a></p>
<div id="readability-page-1" class="page"><article><p>Much has been written and a lot of analysis performed on the global BGP table over the years, a significant portion by the inimitable <a href="https://bgp.potaroo.net/">Geoff Huston</a>. However this often focuses on is long term trends, like the growth of the routing table or the adoption of IPv6 , dealing with time frames of of months or years.</p><p>I was more interested in what was happening in the short term: what does it look like on the front line for those poor routers connected to the churning, foamy chaos of the interenet, trying their best to adhere to <a href="https://en.wikipedia.org/wiki/Robustness_principle">Postel’s Law</a>? What we’ll look at in this article is “a day in the life of the global BGP table”, exploring the intra-day shenanigans with an eye to finding some of the ridiculous things that go on out.</p><p>We’ll focus in on three key areas:</p><ul><li>General behaviour over the course of the day</li><li>Outlier path attributes</li><li>Flappy paths</li></ul><p>As you’ll see, we end up with more questions than answers, but I think that’s the hallmark of good exploratory work. Let’s dive in.</p><h2 id="let-the-yak-shaving-begin">Let the Yak Shaving Begin</h2><p>The first step, as always, is to get some data to work with. Parsing the debug outputs from various routers seemed like a recipe for disaster, so instead I did a little yak-shaving. I went back to a half-finished project BGP daemon I’d started writing years ago and got it into a working state. The result is <strong><a href="https://github.com/gregfoletta/bgpsee">bgpsee</a></strong>, a multi-threaded BGP peering tool for the CLI. Once peered with another router, all the BGP messages - OPENs, KEEPALIVES, and most importantly UPDATEs - are parsed and output as JSON.</p><p>For example, heres one of the BGP updates from the dataset we’re working with in this article:</p><div><pre><code data-lang="json">{
  <span>"recv_time"</span>: <span>1704483075</span>,
  <span>"id"</span>: <span>12349</span>,
  <span>"type"</span>: <span>"UPDATE"</span>,
  <span>"nlri"</span>: [ <span>"38.43.124.0/23"</span> ],
  <span>"withdrawn_routes"</span>: [],
  <span>"path_attributes"</span>: [
    {
      <span>"type"</span>: <span>"ORIGIN"</span>, <span>"type_code"</span>: <span>1</span>,
      <span>"origin"</span>: <span>"IGP"</span>
    },
    {
      <span>"type"</span>: <span>"AS_PATH"</span>, <span>"type_code"</span>: <span>2</span>,
      <span>"n_as_segments"</span>: <span>1</span>,
      <span>"path_segments"</span>: [
        {
          <span>"type"</span>: <span>"AS_SEQUENCE"</span>,
          <span>"n_as"</span>: <span>6</span>,
          <span>"asns"</span>: [ <span>45270</span>, <span>4764</span>, <span>2914</span>, <span>12956</span>, <span>27951</span>, <span>23456</span> ]
        }
      ]
    },
    {
      <span>"type"</span>: <span>"NEXT_HOP"</span>, <span>"type_code"</span>: <span>3</span>,
      <span>"next_hop"</span>: <span>"61.245.147.114"</span>
    },
    {
      <span>"type"</span>: <span>"AS4_PATH"</span>, <span>"type_code"</span>: <span>17</span>,
      <span>"n_as_segments"</span>: <span>1</span>,
      <span>"path_segments"</span>: [
        {
          <span>"type"</span>: <span>"AS_SEQUENCE"</span>,
          <span>"n_as"</span>: <span>6</span>,
          <span>"asns"</span>: [ <span>45270</span>,<span>4764</span>, <span>2914</span>, <span>12956</span>, <span>27951</span>, <span>273013</span> ]
        }
      ]
    }
  ]
}
</code></pre></div><p>Collected between 6/1/2024 and 7/1/2024, the full dataset consists of 464,673 BGP UPDATE messages received from a peer (many thanks to <a href="https://www.linkedin.com/in/andrew-vinton/">Andrew Vinton</a>) with a full BGP table. Let’s take a look at how this full table behaves over the course of the day.</p><h2 id="initial-send-number-of-v4-and-v6-paths">Initial Send, Number of v4 and v6 Paths</h2><p>When you first bring up a BGP peering with a router you get a big dump of of UPDATEs, what I’ll call the ‘first tranche’. It consists of all paths and associated network layer reachability information (NLRI, or more simply ‘routes’) in the router’s BGP table. After this first tranche the peering only receives UPDATEs for paths that have changed, or withdrawn routes which no longer have any paths. There’s no structural difference between the first tranche and the subsequent UPDATEs, except for the fact you received the first batch in the first 5 or so seconds of the peering coming up.</p><p>Here’s a breakdown of the number of distinct paths received in that first tranche, separated by IP version:</p><p><img src="https://articles.foletta.org/post/2024-01-08-a-day-in-the-life-the-bgp-table/index_files/figure-html/unnamed-chunk-7-1.png" width="672">
It’s important to highlight that this is a count of BGP paths, <strong>not</strong> routes. Each path is a unique combination of path attributes with associated NLRI information attached, sent in a distinct BGP UPDATE message. There could be one, or one-thousand routes associated with each path. In this first tranche the total number of routes across all of these paths is 949483.</p><h2 id="a-garden-hose-or-a-fire-hose">A Garden Hose or a Fire Hose?</h2><p>That’s all we’ll look at in the first tranche, we’ll focus our attention from this point on to the rest of the updates received across the day. The updates aren’t sent as a real-time stream, but in bunches based on the <a href="https://datatracker.ietf.org/doc/html/rfc4271#section-10">Route Advertisement Interval</a> timer, which for this peering was 30 seconds. Here’s a time-series view of the number of updates received during the course of the day:</p><p><img src="https://articles.foletta.org/post/2024-01-08-a-day-in-the-life-the-bgp-table/index_files/figure-html/unnamed-chunk-8-1.gif" alt="">
For IPv4 paths you’re looking on average at around 50 path updates every 30 seconds. For IPv6 it’s slightly lower, at around 47 path updates. While the averages are close, the variance is quite different, a standard deviation of 64.3 and 43 for v4 and v6 respectively.</p><p>Instead of looking at the total count of udpates, we can instead look at the total aggregate IP address change. We do this by adding up the total amount of IP addresses across all updates for every 30 second interval, then take the log2() of the sum. So for example: a /22, a /23 and a /24 would be \(log_2(2^{32-22} + 2^{32-23} + 2^{32-24})\)</p><p>Below is the log2() IPv4 address space, viewed as a time series and as a density plot. It shows that on average, every 30 seconds, around 2^16 IP addresses (i.e a /16) change paths in the global routing table, with 95% of time time the change in IP address space is between \(2^{20.75}\) (approx. a /11) and \(2^{13.85}\) (approx. a /18).</p><p><img src="https://articles.foletta.org/post/2024-01-08-a-day-in-the-life-the-bgp-table/index_files/figure-html/unnamed-chunk-9-1.png" width="672"></p><p>What is apparent in both the path and IP space changes over time is that there is some sort of cyclic behaviour in the IPv4 updates. To determine the period of this cycle we can use an <a href="https://otexts.com/fpp3/acf.html">ACF</a> or autocorrelation plot. We calculate the correlation between the number of paths received at time \(y_t\) versus the number received at \(y_{t-{1,t-2,…,t-n}}\) lags. I’ve grouped the updates together into 1 minute intervals, so 1 lag = 1 minute.</p><p><img src="https://articles.foletta.org/post/2024-01-08-a-day-in-the-life-the-bgp-table/index_files/figure-html/unnamed-chunk-10-1.png" width="672">
There is a strong correlation in the first 7 or so lags, which intuitively makes sense to me as path changes can create other path changes as they propagate around the world. But there also appears to be strong correlation at lags 40 and 41, indicating some cyclic behaviour every forty minutes. This gives us the first question which I’ll leave unanswered:</p><ul><li><em>What is causing the global IPv4 BGP table have a 40 minute cycle?</em>.</li></ul><h2 id="prepending-madness">Prepending Madness</h2><p>If you’re a network admin, there’s a couple of different ways you can influence how traffic enters your ASN. You can use longer network prefixes, but this doesn’t scale well and you’re not being a polite BGP citizen. You can use the MED attribute, but it’s non-transitive so it doesn’t work if you’re peered to multiple AS. The usual go-to is to modify the AS path length by prepending your own AS one or more times to certain peers, making that path less preferable.</p><p>In chaos of the global routing table, some people take this prepending too far. This has in the past caused <a href="https://blog.ipspace.net/2009/02/root-cause-analysis-oversized-as-paths/">large, global problems</a>. Let’s take a look at the top 50 AS path lengths for IPv4 and IPv6 updates respectively:</p><p><img src="https://articles.foletta.org/post/2024-01-08-a-day-in-the-life-the-bgp-table/index_files/figure-html/unnamed-chunk-11-1.png" width="672">
What stands out is the difference between IPv4 and IPv6. The largest IPv4 path length is 105, which is still pretty ridiculous given the fact that the largest non-prepended path in this dataset has a length of 14. But compared to the IPv6 paths it’s outright sensible: top of the table for IPv6 comes in at a whopping 599 ASes! An AS path is actually made up of one or more <a href="https://datatracker.ietf.org/doc/html/rfc4271#section-5.1.2">AS sets or AS sequences</a>, each of which have a maximum length of 255. So it’s taken three AS sequences to announce those routes.</p><p>Here’s the longest IPv4 path in all it’s glory with its 105 ASNs. It originated from AS149381 “Dinas Komunikasi dan Informatika Kabupaten Tulungagung” in Indonesia.</p><pre><code>[1] "45270 4764 9002 136106 45305 149381 149381 149381 149381 149381 149381 149381 149381 149381 149381 149381 149381 149381 149381 149381 149381 149381 149381 149381 149381 149381 149381 149381 149381 149381 149381 149381 149381 149381 149381 149381 149381 149381 149381 149381 149381 149381 149381 149381 149381 149381 149381 149381 149381 149381 149381 149381 149381 149381 149381 149381 149381 149381 149381 149381 149381 149381 149381 149381 149381 149381 149381 149381 149381 149381 149381 149381 149381 149381 149381 149381 149381 149381 149381 149381 149381 149381 149381 149381 149381 149381 149381 149381 149381 149381 149381 149381 149381 149381 149381 149381 149381 149381 149381 149381 149381 149381 149381 149381 149381"
</code></pre><p>We see that around 6 hours and 50 minutes later they realise the error in their ways and announce a path with only four ASes, rather than 105:</p><div id="xqhmswhyyp"><table data-quarto-disable-processing="false" data-quarto-bootstrap="false"><thead><tr><th rowspan="1" colspan="1" scope="col" id="recv_time">recv_time</th><th rowspan="1" colspan="1" scope="col" id="time_difference">time_difference</th><th rowspan="1" colspan="1" scope="col" id="id">id</th><th rowspan="1" colspan="1" scope="col" id="as_path_length">as_path_length</th><th rowspan="1" colspan="1" scope="col" id="type">type</th><th rowspan="1" colspan="1" scope="col" id="nlri">nlri</th></tr></thead><tbody><tr><td headers="recv_time">2024-01-06 06:31:18</td><td headers="time_difference">NA</td><td headers="id">66121</td><td headers="as_path_length">105</td><td headers="type">UPDATE</td><td headers="nlri">103.179.250.0/24</td></tr><tr><td headers="recv_time">2024-01-06 13:21:35</td><td headers="time_difference">6.84</td><td headers="id">280028</td><td headers="as_path_length">4</td><td headers="type">UPDATE</td><td headers="nlri">103.179.250.0/24</td></tr></tbody>
</table></div><p>Here’s the largest IPv6 path, with its mammoth 599 prefixes; I’ll let you enjoy scrolling to the right on this one:</p><pre><code>[1] "45270 4764 2914 29632 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 200579 200579 203868"
</code></pre><p>Interestingly it’s not the originator that’s prepending, but as8772 ‘NetAssist LLC’, an ISP out of Ukraine prepending to make paths to asn203868 (Rifqi Arief Pamungkas, again out of Indonesia) less preferable.</p><p>Why is there such a difference between the largest IPv4 and IPv6 path lengths? I had a couple of different theories, but then looked at the total number of ASNs in <em>all</em> positions for those top 50 longest paths, and it became apparent what was happening:</p><p><img src="https://articles.foletta.org/post/2024-01-08-a-day-in-the-life-the-bgp-table/index_files/figure-html/unnamed-chunk-15-1.png" width="672">
Looks like they let the junior network admin at NetAssist on to the tools too early!</p><h2 id="path-attributes">Path Attributes</h2><p>Each BGP update consist of network layer reachability information (routes) and path attributes. For example AS_PATH, NEXT_HOP, etc. There are four kinds of attributes:</p><ol><li>Well-known mandatory</li><li>Well-known discretionary</li><li>Optional transitive</li><li>Optional non-transitive</li></ol><p><a href="https://datatracker.ietf.org/doc/html/rfc4271#section-5">Section 5</a> of RFC4271 has a good description of all of these.</p><p>What we can do is take a look at the number of attributes we’ve seen across all of our IPv4 paths, placing this on on a log scale to make it easier to view:</p><p><img src="https://articles.foletta.org/post/2024-01-08-a-day-in-the-life-the-bgp-table/index_files/figure-html/unnamed-chunk-17-1.png" width="672"></p><p>The well-known mandatory attributes, ORIGIN, NEXT_HOP, and AS_PATH, are present in all updates, and have the same counts. There’s a few other common attributes (e.g.&nbsp;AGGREGATOR), and some less common ones (AS_PATHLIMIT and ATTR_SET). However some ASes have attached attribute 255 - the <a href="https://www.rfc-editor.org/rfc/rfc2042.html">reserved for development</a> attribute - to their updates.</p><p>At the time of receiving the updates my bgpsee daemon didn’t save value of these esoteric path attributes. But using <a href="https://routeviews.org/">routeviews.org</a> we can see that some ASes are still announcing paths with this attribute, and we can observe the raw bytes of its value:</p><pre><code>- AS265999 attrib. 255 value:       0000 07DB 0000 0001 0001 000A FF08 0000 0000 0C49 75B3
- AS10429 attrib. 255 value:        0000 07DB 0000 0001 0001 000A FF08 0000 0003 43DC 75C3
- AS52564 attrib. 255 valuue:       0000 07DB 0000 0001 0001 0012 FF10 0000 0000 0C49 75B3 0000 0000 4003 F1C9
</code></pre><p>Three different ISPs, all announcing paths with this strange path attribute, and raw bytes of the attribute having a similar structure.</p><p>This leads us to the second question which I’ll leave here unanswered:</p><ul><li><em>what vendor is deciding it’s a good idea to use this reserved for development attribute, and what are they using it for?</em>.</li></ul><h2 id="flippy-flappy-whos-having-a-bad-time">Flippy-Flappy: Who’s Having a Bad Time?</h2><p>Finally, let’s see who’s having a bad time: what are the top routes that are shifting paths or being withdrawn completely during the day. Here’s the top 10 active NLRIs with the number of times the route was included in an UPDATE:</p><div id="alweeynynt"><table data-quarto-disable-processing="false" data-quarto-bootstrap="false"><thead><tr><th rowspan="1" colspan="1" scope="col" id="nlri">nlri</th><th rowspan="1" colspan="1" scope="col" id="update_count">update_count</th></tr></thead><tbody><tr><td headers="nlri">140.99.244.0/23</td><td headers="update_count">2596</td></tr><tr><td headers="nlri">107.154.97.0/24</td><td headers="update_count">2583</td></tr><tr><td headers="nlri">45.172.92.0/22</td><td headers="update_count">2494</td></tr><tr><td headers="nlri">151.236.111.0/24</td><td headers="update_count">2312</td></tr><tr><td headers="nlri">205.164.85.0/24</td><td headers="update_count">2189</td></tr><tr><td headers="nlri">41.209.0.0/18</td><td headers="update_count">2069</td></tr><tr><td headers="nlri">143.255.204.0/22</td><td headers="update_count">2048</td></tr><tr><td headers="nlri">176.124.58.0/24</td><td headers="update_count">1584</td></tr><tr><td headers="nlri">187.1.11.0/24</td><td headers="update_count">1582</td></tr><tr><td headers="nlri">187.1.13.0/24</td><td headers="update_count">1580</td></tr></tbody>
</table></div><p>Looks like anyone on <strong>140.99.244.0/23</strong> was having a bad time during this day. This space is owned by a company called <a href="https://www.epicup.com/">EpicUp</a>… more like EpicDown! *groan*.</p><p>Graphing the updates and complete withdraws over the course of the day paints a bad picture</p><p><img src="https://articles.foletta.org/post/2024-01-08-a-day-in-the-life-the-bgp-table/index_files/figure-html/unnamed-chunk-19-1.png" width="672">
The top graph looks like a straight line, but that’s because this route is present in almost every single 30 second block of updates. There are 2,879 30-second blocks and it’s present as either a different path or a withdrawn route in 2,637 of them, or 92.8%!</p><p>We know the routes is flapping, but <em>how</em> is it flapping, and who is to blame? The best way to visualise this is a graph, with the ASNs in all paths to that network as nodes and edges showing the pairs of ASNs in the paths. I’ve colourised the edges by how many updates were seen with each pair of ASes, binned into groups of 300:</p><p><img src="https://articles.foletta.org/post/2024-01-08-a-day-in-the-life-the-bgp-table/index_files/figure-html/unnamed-chunk-21-1.png" width="672">
What a mess! You can make out the primary path down the centre through NTT (2914) and Lumen/Level3 (3356), but for whatever reason (bad link? power outages? router crashing?) the path is moving between these tier 1 ISPS and others, including Arelion (1299) and PCCW (3419). While it’s almost impossible to identify the exact reason for the route flapping using this data only, what it does show is the amazing peering diversity of modern global networks, and the the resiliency of a 33 year old routing protocol.</p><h2 id="just-the-beginning">Just The Beginning</h2><p>There’s a big problem with a data set like this: there’s just too much to look at. I needed to keep a lid on it so this article didn’t balloon out to 30,000 words, but there’s another five rabbit holes I could have gone down. That’s not including the the questions I’ve left unanswered.</p><p>With the global BGP table, you’ve got a summary of an entire world encapsulated in a few packets. Your BGP updates could could be political unrest, natural phenomena like earthquakes or fires, or simply a network admin’s fat finger. You’ve got the economics of internet peering, and you’ve got the human element of different administrators with different capabilities coming together to bring up connectivity. And somehow it manages to work, well, most of the time. There’s something both bizarre and beautiful about seeing all of that humanity encapsulated and streamed as small little updates into your laptop.</p></article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[How I configure my Git identities (397 pts)]]></title>
            <link>https://www.benji.dog/articles/git-config/</link>
            <guid>42233524</guid>
            <pubDate>Mon, 25 Nov 2024 05:32:33 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.benji.dog/articles/git-config/">https://www.benji.dog/articles/git-config/</a>, See on <a href="https://news.ycombinator.com/item?id=42233524">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><blockquote>
<p><strong>Note</strong>: I've had this post drafted for 3 YEARS!!! It's finally time to publish it.</p>
</blockquote>
<p>I like to mess with my <a href="https://github.com/benjifs/dotfiles">dotfiles</a> and every so often, I find out about a new way to do things and I spend more time than I should learning how to use it.</p>
<p>A few years ago I learned about <a href="https://git-scm.com/docs/git-config#_includes">includeIf</a> for including specific files if some condition was met for <code>git</code>. The example that I first saw was doing:</p>
<pre><code>[includeIf "gitdir:~/code/**"]
  path = ~/.config/git/personal
[includeIf "gitdir:~/work/**"]
  path = ~/.config/git/work
</code></pre>
<p>So that <code>~/.config/git/personal</code> is only included for <code>git</code> directories under <code>~/code</code> and <code>~/.config/git/work</code> is only included for directories under <code>~/work</code>. The contents of those included files varies but usually it contains your git identity, signing keys, etc. Here's an example of what that could look like:</p>
<pre><code>[user]
  name = benji
  email = benji@work.com
  signingkey = ~/.ssh/work.id_ed25519.pub
</code></pre>
<p>That works pretty well but I usually organize all my code in <code>~/workspace</code> regardless of whether its personal, <strong>work-1</strong>, <strong>work-2</strong>, etc. I wanted to be able to configure git depending on where that repo actually lives instead of where the directory is in my machine. Then I found out about <a href="https://git-scm.com/docs/git-config#Documentation/git-config.txt-codehasconfigremoteurlcode">hasconfig:remote.*.url:</a>!</p>
<p>This makes it so that I can configure git conditionally if the given remote URL exists for that directory I'm currently working in.</p>
<p>A few examples of what I do is:</p>
<pre><code>[includeIf "hasconfig:remote.*.url:git@github.com:orgname/**"]
  path = ~/.config/git/config-gh-org

[includeIf "hasconfig:remote.*.url:git@github.com:*/**"]
  path = ~/.config/git/config-gh

[includeIf "hasconfig:remote.*.url:git@gitlab.com:*/**"]
  path = ~/.config/git/config-gl

[includeIf "hasconfig:remote.*.url:git@git.sr.ht:*/**"]
  path = ~/.config/git/config-srht
</code></pre>
<p>Now if I'm in a directory where the remote matches <code>github.com:orgname/**</code> it would use <code>~/.config/git/config-gh-org</code>, otherwise it uses the general config file for any other GitHub repo.</p>
<hr>
<p>While that handles git identities, I still need to configure SSH keys separately to be able to <code>pull</code> and <code>push</code> to remotes. The simple version of my <code>~/.ssh/config</code> looks like this:</p>
<pre><code>Host gitlab.com
Hostname gitlab.com
User git
IdentityFile ~/.ssh/gitlab.id_ed25519

Host github.com
Hostname github.com
User git
IdentityFile ~/.ssh/github.id_ed25519
</code></pre>
<p>The only problem with this is that in order to use a different <code>IdentityFile</code> for the same <code>Hostname</code> so that I could use a different key for repos under <code>github.com/orgname</code>, I'd have to use a different value for <code>Host</code>. So in my case I would add the following to my <code>~/.ssh/config</code>:</p>
<pre><code>Host gh-work
Hostname github.com
User git
IdentityFile ~/.ssh/work.id_ed25519
</code></pre>
<p>Finally, to use that <code>Host</code> when I'm looking for a repo in <code>github.com/orgname</code>, I would add the following to my git config:</p>
<pre><code>[url "gh-work:orgname"]
  insteadOf = git@github.com:orgname
</code></pre>
<p>So when I <code>clone</code>, <code>pull</code>, or <code>push</code> a repo that's under my work's org account I can do:</p>
<pre><code>git clone git@github.com:orgname/project
</code></pre>
<p>and <code>insteadOf</code> would replace <code>github.com:orgname</code> with <code>gh-work:orgname</code> so that it uses the right info from my SSH config. It's a neat trick which I saw referenced in this <a href="https://www.kenmuse.com/blog/ssh-and-multiple-git-credentials/#git">article</a>.</p>
<hr>
<p>Are there any issues with this approach? Is there a better way to do this? I'm not sure so please let me know as I'd love to learn and I'll update this post accordingly.</p>
<h2>References</h2>
<ul>
<li><a href="https://fundor333.com/post/2021/advance-git-config-and-ssh-config/">https://fundor333.com/post/2021/advance-git-config-and-ssh-config/</a></li>
<li><a href="https://www.kenmuse.com/blog/ssh-and-multiple-git-credentials/#git">https://www.kenmuse.com/blog/ssh-and-multiple-git-credentials/#git</a></li>
<li><a href="https://garrit.xyz/posts/2023-10-13-organizing-multiple-git-identities">https://garrit.xyz/posts/2023-10-13-organizing-multiple-git-identities</a></li>
<li><a href="https://stevenharman.net/configure-ssh-keys-for-multiple-github-accounts">https://stevenharman.net/configure-ssh-keys-for-multiple-github-accounts</a></li>
</ul>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Making Your Connection Bad (104 pts)]]></title>
            <link>https://www.5snb.club/posts/2024/making-your-connection-bad/</link>
            <guid>42232620</guid>
            <pubDate>Mon, 25 Nov 2024 02:20:25 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.5snb.club/posts/2024/making-your-connection-bad/">https://www.5snb.club/posts/2024/making-your-connection-bad/</a>, See on <a href="https://news.ycombinator.com/item?id=42232620">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
      






<p>This is directly inspired by <a href="https://brr.fyi/posts/engineering-for-slow-internet">Engineering for Slow
Internet</a>. I figured I’d give running with
dogshit internet on my desktop and phone a go to see how poorly (or well!) specific applications behave.</p>
<p>This is not an in-depth review of the behavior, but just my general impressions running for a few
hours on 200kbit internet with significant packet loss.</p>
<p>F-Droid doesn’t have a download pause feature, but <em>does</em> resume interrupted downloads. Well done
:) Though it does seem to crash sometimes (I have reported this).</p>
<p>Steam has a download pause feature, and does seem to resume interrupted downloads across steam restarts.</p>
<p>Telegram is quite usable. Images are slow, but for chat, I’d be perfectly happy with it.</p>
<p>Discord will sometimes just kick you to a loading screen if it thinks your internet isn’t working.
If it <em>doesn’t</em> do that, it works pretty okay.</p>
<p><code>git</code> fetches/clones simply <em>do not</em> have any resumption, as far as I can see. But if you fetch
often enough, the transferred data might be small enough to be able to finish in one transfer. The
kernel <a href="https://kernel.org/cloning-linux-from-a-bundle.html" title="Cloning Linux from a bundle">has instructions to clone from a
bundle</a>, which
is one workaround for this issue. Doesn’t help you with pushes, though.</p>
<h2 id="how-to-do-this-on-linux">How to do this on linux</h2>
<p>Your name for <code>enp3s0</code> may be different. Change it to whatever your network adapter is.</p>
<pre data-lang="bash"><code data-lang="bash"><span>sudo</span><span> modprobe ifb
</span><span>
</span><span>sudo</span><span> ip link add name ifb0 type ifb
</span><span>sudo</span><span> ip link set dev ifb0 up
</span><span>
</span><span>sudo</span><span> tc qdisc add dev enp3s0 ingress
</span><span>sudo</span><span> tc filter add dev enp3s0 parent ffff: </span><span>\
</span><span>    u32 match u32 0 0 flowid 1:1 action mirred egress redirect dev ifb0
</span><span>
</span><span>sudo</span><span> tc qdisc add dev ifb0 root netem </span><span>\
</span><span>    delay 200ms 50ms 50 loss random 2% rate 200kbit
</span><span>sudo</span><span> tc qdisc add dev enp3s0 root netem </span><span>\
</span><span>    delay 200ms 50ms 50 loss random 2% rate 200kbit
</span></code></pre>
<p>If you want to test services running on <code>localhost</code>, replace <code>enp3s0</code> with <code>lo</code>. Though some other
programs on your system may Not Like It if <code>localhost</code> is slow, so do that at your own risk :3 (Be sure to replace
<code>enp3s0</code> with <code>lo</code> on the undo script!)</p>
<p>The post says</p>
<blockquote>
<p>If you’re an app developer reading this, can you tell me, off the top of your head, how your app
behaves on a link with 40 kbps available bandwidth, 1,000 ms latency, occasional jitter of up to
2,000 ms, packet loss of 10%, and a complete 15-second connectivity dropout every few minutes?</p>
</blockquote>
<p>If you want to change the settings to those, do:</p>
<p>(I can’t seem to emulate the connection dropout, but the 10% packet loss should be harsh enough.
Also, this is 40kbit up, and 40kbit down. Still, it’s Bad Enough.)</p>
<pre data-lang="bash"><code data-lang="bash"><span>sudo</span><span> tc qdisc change dev ifb0 root netem </span><span>\
</span><span>    delay 1000ms 2000ms 90 loss random 10% reorder 10% rate 40kbit
</span><span>sudo</span><span> tc qdisc change dev enp3s0 root netem </span><span>\
</span><span>    delay 1000ms 2000ms 90 loss random 10% reorder 10% rate 40kbit
</span></code></pre>
<p>To undo:</p>
<pre data-lang="bash"><code data-lang="bash"><span>sudo</span><span> tc qdisc delete dev ifb0 root
</span><span>sudo</span><span> tc qdisc delete dev enp3s0 root
</span><span>sudo</span><span> tc qdisc delete dev enp3s0 ingress
</span><span>sudo</span><span> modprobe</span><span> --remove</span><span> ifb
</span></code></pre>
<h2 id="how-to-do-this-on-android">How to do this on android</h2>
<p>And on an android device, it’s even easier to set up, but it only affects incoming download rate.
But for testing “Hey, does this application utterly shit itself if it doesn’t have a fast
network?”, that’s just fine. You need to enable developer options, though.</p>




<figure>
<a href="https://www.5snb.club/posts/2024/making-your-connection-bad/android-incoming-limit.png">
<picture>
<source srcset="https://www.5snb.club/processed_images/android-incoming-limit.ee93792abb6c4f30.webp" type="image/webp" height="1000" width="450">

<img src="https://www.5snb.club/processed_images/android-incoming-limit.13b19bb6aad57a07.jpg" alt="Inside Android Developer Options, Configure network rate limit set to 128kbps" height="1000" width="450">
</picture>
</a>
<figcaption>
<a href="https://www.5snb.club/posts/2024/making-your-connection-bad/android-incoming-limit.png">android-incoming-limit.png 144.86 kB
(864x1920)</a><br>

</figcaption>
</figure>
<h2 id="how-to-do-this-on-the-web">How to do this on the web</h2>




<figure>
<a href="https://www.5snb.club/posts/2024/making-your-connection-bad/firefox-limit.png">
<picture>
<source srcset="https://www.5snb.club/processed_images/firefox-limit.b47959fa6c856d7e.webp" type="image/webp" height="389" width="792">

<img src="https://www.5snb.club/processed_images/firefox-limit.621d59fb8cd57959.jpg" alt="Firefox developer tools, network tab, GPRS limiting selected" height="389" width="792">
</picture>
</a>
<figcaption>
<a href="https://www.5snb.club/posts/2024/making-your-connection-bad/firefox-limit.png">firefox-limit.png 57.96 kB
(792x389)</a>

<p>On firefox, you can enable network rate limiting by going to the Web Developer
Tools (Ctrl+Shift+I), going to the Network tab, then selecting the ratelimit using the
dropdown in the top right. If you hover over the dropdown, you can see the limiting
applied.</p>


</figcaption>
</figure>
<h2 id="further-reading">Further Reading</h2>
<ul>
<li><a href="https://bravenewgeek.com/sometimes-kill-9-isnt-enough/">Sometimes Kill -9 Isn’t Enough</a></li>
<li><a href="https://web.archive.org/web/20190410224615/https://wiki.linuxfoundation.org/networking/netem#how_can_i_use_netem_on_incoming_traffic">Linux wiki networking:netem “How can I use netem on incoming traffic?”</a></li>
<li><a href="https://firefox-source-docs.mozilla.org/devtools-user/network_monitor/throttling/index.html">Firefox Throttling docs</a></li>
<li><a href="https://man7.org/linux/man-pages/man8/tc-netem.8.html">tc-netem(8)</a></li>
</ul>


    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Wildlife monitoring technologies used to intimidate and spy on women (142 pts)]]></title>
            <link>https://www.cam.ac.uk/research/news/wildlife-monitoring-technologies-used-to-intimidate-and-spy-on-women-study-finds</link>
            <guid>42232289</guid>
            <pubDate>Mon, 25 Nov 2024 01:24:41 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.cam.ac.uk/research/news/wildlife-monitoring-technologies-used-to-intimidate-and-spy-on-women-study-finds">https://www.cam.ac.uk/research/news/wildlife-monitoring-technologies-used-to-intimidate-and-spy-on-women-study-finds</a>, See on <a href="https://news.ycombinator.com/item?id=42232289">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>Remotely operated camera traps, sound recorders and drones are increasingly being used in conservation science to monitor wildlife and natural habitats, and to keep watch on protected natural areas.</p>

<p>But Cambridge researchers studying a forest in northern India have found that the technologies are being deliberately misused by local government and male villagers to keep watch on women without their consent.</p>

<p>Cambridge researcher Dr Trishant Simlai spent 14 months interviewing 270 locals living around the Corbett Tiger Reserve, a national park in northern India, including many women from nearby villages.</p>

<p>His report, published today in the journal <a href="https://doi.org/10.17863/CAM.111664"><em>Environment and Planning F</em></a>, reveals how forest rangers in the national park deliberately fly drones over local women to frighten them out of the forest, and stop them collecting natural resources despite it being their legal right to do so.</p>

<p>The women, who previously found sanctuary in the forest away from their male-dominated villages, told Simlai they feel watched and inhibited by camera traps, so talk and sing much more quietly. This increases the chance of surprise encounters with potentially dangerous wildlife like elephants and tigers. One woman he interviewed has since been killed in a tiger attack.</p>

<p>The study reveals a worst-case scenario of deliberate human monitoring and intimidation. But the researchers say people are being unintentionally recorded by wildlife monitoring devices without their knowledge in many other places - even national parks in the UK.&nbsp;</p>

<p>“Nobody could have realised that camera traps put in the Indian forest to monitor mammals actually have a profoundly negative impact on the mental health of local women who use these spaces,” said Dr Trishant Simlai, a researcher in the University of Cambridge’s Department of Sociology and lead author of the report.</p>

<p>“These findings have caused quite a stir amongst the conservation community. It’s very common for projects to use these technologies to monitor wildlife, but this highlights that we really need to be sure they’re not causing unintended harm,” said Professor Chris Sandbrook, Director of the University of Cambridge’s Masters in Conservation Leadership programme, who was also involved in the report.</p>

<p>He added: “Surveillance technologies that are supposed to be tracking animals can easily be used to watch people instead – invading their privacy and altering the way they behave.”</p>

<p>Many areas of conservation importance overlap with areas of human use. The researchers call for conservationists to think carefully about the social implications of using remote monitoring technologies – and whether less invasive methods like surveys could provide the information they need instead.</p>

<p><strong><em>Intimidation and deliberate humiliation</em></strong></p>

<p>The women living near India’s Corbett Tiger Reserve use the forest daily in ways that are central to their lives: from gathering firewood and herbs to sharing life’s difficulties through traditional songs.</p>

<p>Domestic violence and alcoholism are widespread problems in this rural region and many women spend long hours in forest spaces to escape difficult home situations.</p>

<p>The women told Simlai that new technologies, deployed under the guise of wildlife monitoring projects, are being used to intimidate and exert power over them - by monitoring them too.&nbsp;</p>

<p>“A photograph of a woman going to the toilet in the forest – captured on a camera trap supposedly for wildlife monitoring - was circulated on local Facebook and WhatsApp groups as a means of deliberate harassment,” said Simlai.&nbsp;</p>

<p>He added: “I discovered that local women form strong bonds while working together in the forest, and they sing while collecting firewood to deter attacks by elephants and tigers. When they see camera traps they feel inhibited because they don’t know who’s watching or listening to them – and as a result they behave differently - often being much quieter, which puts them in danger.”</p>

<p>In places like northern India, the identity of local women is closely linked to their daily activities and social roles within the forest. The researchers say that understanding the various ways local women use forests is vital for effective forest management strategies.</p>

<p><em><strong>Reference: </strong>Simlai, T. et al: ‘<a href="https://doi.org/10.17863/CAM.111664">The Gendered Forest: Digital Surveillance Technologies for Conservation and Gender-Environment relationships</a>.’ November 2024. DOI:10.17863/CAM.111664</em><br>
&nbsp;</p>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[RFC 35140: HTTP Do-Not-Stab (2023) (753 pts)]]></title>
            <link>https://www.5snb.club/posts/2023/do-not-stab/</link>
            <guid>42232040</guid>
            <pubDate>Mon, 25 Nov 2024 00:43:03 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.5snb.club/posts/2023/do-not-stab/">https://www.5snb.club/posts/2023/do-not-stab/</a>, See on <a href="https://news.ycombinator.com/item?id=42232040">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
      






<p>Date: March 7, 2111</p>
<h2 id="abstract">Abstract</h2>
<p>This document defines the syntax and semantics of the <code>Do-Not-Stab</code> header, a proposed HTTP header
that allows users to indicate to a website their preferences about being stabbed. It also provides
a standard for how services should comply with such user preferences, if they wish to.</p>

<ul>
<li><code>[REDACTED]</code> (Google)</li>
<li><code>[REDACTED]</code> (Google)</li>
<li><code>[REDACTED]</code> (Google)</li>
<li><code>[REDACTED]</code> (Google)</li>
</ul>
<h2 id="introduction">Introduction</h2>
<p>Over the last 50 years, advancements in peripherals have allowed websites to stab users. A number
of industries have popped up to provide SaaS (Stabbings as a Service). Some users have expressed
discomfort when a knife is plunged into their chest, and this header allows those users to express
their personal preferences.</p>
<p>A user preference can, of course, be ignored by bad actors. However, most stabbings are not done by
malicious actors, they are simply law-abiding companies which will gladly stop stabbing you if you
ask. This standard provides a method for a user to easily opt-out of all stabbings, except those
mandated by law, and ones that the company wants to do anyways.</p>
<h2 id="syntax">Syntax</h2>
<p>The header has only one form, <code>Do-Not-Stab: 1</code>. This is because the lack of a header indicates a
clear preference that the user wants to be stabbed.</p>
<h2 id="defaults">Defaults</h2>
<p>A user-agent MUST NOT adopt <code>Do-Not-Stab: 1</code> as the default preference. If a user-agent were to do
this, web services SHOULD ignore the preference and stab the user anyways.</p>
<p>This is because user-agents are in no position to determine if a user wants to be stabbed or not,
this must be an explicit choice that the user makes.</p>
<h2 id="enforcement">Enforcement</h2>
<p>Microsoft has committed to supporting the <code>Do-Not-Stab</code> header inside the EEA (European Economic
Area). Outside of the EEA, support for the header is still in-progress, and you may get stabbed,
even with the header set. If you are in a country that leaves the EEA, you may get stabbed.</p>
<h2 id="exceptions">Exceptions</h2>
<p>Exceptions to the <code>Do-Not-Stab</code> header are accepted when commercial interests outweigh safety
concerns. These include, but are not limited to</p>
<ul>
<li>Stabbing users who have consented to being stabbed (even if they don’t know they consented)</li>
<li>Stabbings requested by a government. Websites SHOULD NOT try to challenge the legality of any
stabbings requested, the user probably deserved it.</li>
<li>Stabbings that are probably not going to kill the user.</li>
<li>Shareholders wanted it</li>
</ul>
<hr>

<p>seriously, what the fuck is with companies nowadays demanding that they be told to not do the
things they know they shouldn’t be doing anyways? why is microsoft respecting the user’s choice
only in the EEA? because they only <em>have</em> to there. extremely funny how they were also the ones to
set Do-Not-Track by default in IE, thereby getting everyone to ignore it for IE. because companies
are god damn children and must be told no explicitly by every person individually. it’s a fucking
wonder that DNT even got in as a general option and wasn’t mandated to be set per-origin, making it
even more fucking useless than it is.</p>
<p><a href="https://blogs.windows.com/windows-insider/2023/11/16/previewing-changes-in-windows-to-comply-with-the-digital-markets-act-in-the-european-economic-area/">https://blogs.windows.com/windows-insider/2023/11/16/previewing-changes-in-windows-to-comply-with-the-digital-markets-act-in-the-european-economic-area/</a></p>
<p>it’s fucking depressing when even the fucking bare minimum form of regulation is followed to the
letter and no more, because every company out there fucking hates you and would sell you out to
make a bit more money if they legally could. and even if they couldn’t, who’s going to stop them?</p>
<p>“We and our 756 partners process personal data[…]” wow big polycule this website is in, there’s no
fucking way they actually need to work with that many fucking companies, what the shit? adtech is a
scourge on humanity and serves zero fucking purpose.</p>


    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[SQLiteStudio: Create, edit, browse SQLite databases (354 pts)]]></title>
            <link>https://sqlitestudio.pl/</link>
            <guid>42232000</guid>
            <pubDate>Mon, 25 Nov 2024 00:36:46 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://sqlitestudio.pl/">https://sqlitestudio.pl/</a>, See on <a href="https://news.ycombinator.com/item?id=42232000">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
      <header>
  <a href="https://sqlitestudio.pl/gallery/"><img src="https://sqlitestudio.pl/img/front_shot.jpg"></a>
  
  <p>Create, edit, browse SQLite databases.</p>
  <a href="https://api.github.com/repos/pawelsalawa/sqlitestudio/zipball/3.4.6">Download <span></span></a>
  <a href="https://sqlitestudio.pl/donate/">Donate <span></span></a>
</header>

<div>
    <div>
        <div>
          <h2>3.4.6 released!</h2>
          <p>It's a hotfix release to address urgent problem of "black SQL code line" that appeared in 3.4.5. It also gets two more issues resolved.</p>
          <p><a href="https://sqlitestudio.pl/news/#159">Read More →</a>
        </p></div>
        <div><p>
          Posted on 23 November 2024
          <a href="https://sqlitestudio.pl/news/">More news →</a>
        </p></div>
      </div>
	<div>
		<a href="https://solidnaksiegowa.com/">
			<p><img src="https://sqlitestudio.pl/img/solidnaksiegowa.png">
			</p>
		</a>
	</div>
</div>



<div>
  <div>
        <h4>Feature rich</h4>
        <p>Powerful, yet light and fast.</p>
        <p><a href="https://sqlitestudio.pl/features/">Learn more</a>
      </p></div>
<div>
        <h4>Open Source</h4>
        <p>It's released under GPL license and is free to use for any purpose.</p>
        
      </div>
<div>
        <h4>Cross-platform</h4>
        <p>Runs on Windows, Linux and MacOS X.</p>
        
      </div>
<div>
        <h4>Portable</h4>
        <p>No need to install or uninstall. Just download, decompress and run.</p>
        
      </div>

</div>
    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[The Two Factions of C++ (353 pts)]]></title>
            <link>https://herecomesthemoon.net/2024/11/two-factions-of-cpp/</link>
            <guid>42231489</guid>
            <pubDate>Sun, 24 Nov 2024 23:21:36 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://herecomesthemoon.net/2024/11/two-factions-of-cpp/">https://herecomesthemoon.net/2024/11/two-factions-of-cpp/</a>, See on <a href="https://news.ycombinator.com/item?id=42231489">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
<article>
<div id="content"><div>
<figure data-imgstate="dither">
<img alt="Zero Ranger." data-dither="/2024/11/two-factions-of-cpp/images/dithers/not-bad-at-all_dithered.png" data-original="https://herecomesthemoon.net/2024/11/two-factions-of-cpp/images/not-bad-at-all_hu130da105ae916814d5129db49a9c3717_341311_800x800_fit_q90_h2_box_3.webp" loading="lazy" src="https://herecomesthemoon.net/2024/11/two-factions-of-cpp/images/dithers/not-bad-at-all_dithered.png">
<div>
<figcaption>
<span> Zero Ranger. </span>
<svg viewBox="0 0 100 100" xmlns="http://www.w3.org/2000/svg">
<rect height="24.28" width="24.28" x="13.51" y="13.58"></rect>
<rect height="24.28" width="24.28" x="37.93" y="37.86"></rect>
<rect height="24.28" width="24.28" x="62.21" y="13.58"></rect>
<rect height="24.28" width="24.28" x="13.51" y="62.14"></rect>
<rect height="24.28" width="24.28" x="62.21" y="62.14"></rect>
</svg>
<p><span>
             Toggle original/dithered image
          </span>
</p>
</figcaption>
</div>
</figure>
</div>
<p>There seems to be a lot of fighting and arguing over the future of C++.</p>
<p>On Reddit and a certain orange website, definitely, but also surely at the official C++ standard committee meetings. You don’t need to look very far.</p>
<h2 id="the-absolute-state-of-c">The Absolute State (of C++)</h2>
<p>It looks like we’re in the following situation:</p>
<ul>
<li>C++’s Evolution Working Group (EWG) just <a href="https://github.com/cplusplus/papers/issues/2121#issuecomment-2494153010" target="_blank">achieved consensus</a> on adopting <a href="https://www.open-std.org/jtc1/sc22/wg21/docs/papers/2024/p3466r0.pdf" target="_blank">P3466 R0 - (Re)affirm design principles for future C++ evolution</a>:
<ul>
<li>This means no ABI breaks, retain link compatibility with C and previous C++.</li>
<li>It also means no ‘viral annotations’ (no lifetime annotations, for example).<sup id="fnref:1"><a href="#fn:1" role="doc-noteref">1</a></sup></li>
<li>It heavily doubles down on a set of incompatible goals, ie. no ABI break and the zero-overhead-principle.<sup id="fnref:2"><a href="#fn:2" role="doc-noteref">2</a></sup></li>
<li>Whether this is good or bad, it is a (literal) doubling down on the current trajectory of the C++ language.</li>
</ul>
</li>
</ul>
<p>In the meantime:</p>
<ul>
<li>The US government wants people to stop using C++:
<ul>
<li><a href="https://www.cisa.gov/resources-tools/resources/product-security-bad-practices" target="_blank">The CISA</a></li>
<li><a href="https://media.defense.gov/2022/Nov/10/2003112742/-1/-1/0/CSI_SOFTWARE_MEMORY_SAFETY.PDF" target="_blank">The NSA</a></li>
<li><a href="https://www.whitehouse.gov/wp-content/uploads/2024/02/Final-ONCD-Technical-Report.pdf" target="_blank">The White House, apparently.</a></li>
<li>No, really. Various branches of the US government have released papers, reports, recommendation to warn the industry against usage of memory-unsafe languages.</li>
</ul>
</li>
<li>All sorts of big tech players are adopting Rust:
<ul>
<li>Microsoft is apparently <a href="https://www.theregister.com/2023/04/27/microsoft_windows_rust/" target="_blank">rewriting core-libraries in Rust</a>.</li>
<li>Google seems to <a href="https://security.googleblog.com/2021/04/rust-in-android-platform.html" target="_blank">be committing to Rust</a>, and in fact started working on a <a href="https://github.com/google/crubit" target="_blank">bidirectional C++/Rust interop tool</a>.</li>
<li>AWS is <a href="https://aws.amazon.com/blogs/devops/why-aws-is-the-best-place-to-run-rust/" target="_blank">using Rust</a>.</li>
<li>etc.</li>
</ul>
</li>
<li>Speaking of big tech, did you notice that <a href="https://herbsutter.com/2024/11/11/a-new-chapter-and-a-pivotal-year-for-cpp/" target="_blank">Herb Sutter is leaving Microsoft</a>, and that it seems like <a href="https://www.reddit.com/r/cpp/comments/1gkdr6e/msvc_c23_support/" target="_blank">MSVC is slow to implement C++23 features, and asking the community for prioritization</a>.</li>
<li>The infamous <a href="https://cor3ntin.github.io/posts/abi/" target="_blank">Prague ABI-vote</a> happened (tl;dr: “C++23 will not break ABI, it’s unclear if it ever will.”), Google supposedly significantly lowered its participation in the C++ development process, and instead started to work on <a href="https://www.youtube.com/watch?v=omrY53kbVoA" target="_blank">their own C++ successor language</a>. They even have a <a href="https://github.com/carbon-language/carbon-lang/blob/trunk/docs/project/difficulties_improving_cpp.md" target="_blank">summary</a> outlining all of the issues they had trying to improve C++.<sup id="fnref:3"><a href="#fn:3" role="doc-noteref">3</a></sup></li>
<li><a href="https://thephd.dev/finally-embed-in-c23" target="_blank">Stories</a> of people trying their best to participate in the C++-standard committee process across multiple years, only to be chewed up and spit out are widely known and shared throughout the community. (A feature landing in <em>C</em> first doesn’t help either.)</li>
<li>Modules are still not implemented. <a href="https://arewemodulesyet.org/" target="_blank">Are we modules yet?</a> <img alt="image" src="https://herecomesthemoon.net/2024/11/two-factions-of-cpp/images/modules.png"></li>
<li><a href="https://isocpp.org/files/papers/P3081R0.pdf" target="_blank">‘Safety Profiles’</a> are still in a weird state with no existing implementation, trying to retrofit <em>some</em> amount of safety onto existing C++ code while minimizing changes to existing code. Sean Baxter himself <a href="https://www.circle-lang.org/draft-profiles.html" target="_blank">took a stance</a> against profiles, and described C++ as “underspecified”.</li>
</ul>
<p>I don’t know about you, but if I were to look at all of this <em>as an outsider</em>, it sure would look as if C++ is <em>basically falling apart</em>, and as if a vast amount of people lost faith in the ability of C++’s committee to <em>somehow</em> stay on top of this.<sup id="fnref:4"><a href="#fn:4" role="doc-noteref">4</a></sup></p>
<h2 id="two-cultures">Two Cultures</h2>
<p>People seem to be looking for other solutions.</p>
<p>Say, Google. Google evidently lost faith in ’the process’ ever since the ABI-vote. This isn’t a loss of faith in the language itself, Google has one of the largest C++ codebases in the world, and it has served them incredibly well. It’s a loss of faith in the language’s ability to evolve as pressure mounts from different angles (potential government regulations, competing languages, a desire for better performance and safety guarantees from key players, etc.).</p>
<p>So what’s the problem? Why doesn’t C++ just…change?</p>
<p>Well, figuring that out is easy. Just <a href="https://isocpp.org/files/papers/P3081R0.pdf" target="_blank">look at what Herb Sutter said in his paper on profiles</a>:</p>
<blockquote>
<p>“<strong>We must minimize the need to change existing code.</strong> For adoption in existing code, decades of experience has consistently shown that most customers with large code bases cannot and will not change even 1% of their lines of code in order to satisfy strictness rules, not even for safety reasons unless regulatory requirements compel them to do so.” – Herb Sutter</p>
</blockquote>
<p>Cool. Is anyone surprised by this? I don’t think so.</p>
<p>Now, let’s contrast this with <a href="https://isocpp.org/wiki/faq/wg21#chandler-carruth" target="_blank">Chandler Carruth’s biography on the WG21 member page</a>:</p>
<blockquote>
<p>I led the design of <strong>C++ tooling and automated refactoring systems built on top of Clang</strong> and now part of the Clang project. […]<br>
Within Google, I led the effort to scale the <strong>automated Clang-based refactoring</strong> tools up to our entire codebase, over 100 million lines of C++ code. We can analyze and apply refactorings across the entire codebase in 20 minutes.</p>
</blockquote>
<p>Oh. Do you see it? (Yes you do, I highlighted it.)</p>
<p>It’s “automated migration tooling”. Except it’s not <em>just</em> that, automated migration tooling is just the peak, the single brightly glowing example.</p>
<p>We’re basically seeing a conflict between two starkly different camps of C++-users:</p>
<ul>
<li>Nimble, modern, highly capable tech corporations that understand that their code is an asset. (This isn’t strictly <em>big</em> tech. Any sane greenfield C++ startup will also fall into this category.)</li>
<li>Everyone else. Every ancient corporation where people are still fighting over how to indent their code, and some young engineer is begging management to allow him to set up a linter.</li>
</ul>
<p>One of these groups will be capable of handling a migration <em>somewhat</em> gracefully, and it’s the group that is capable of <strong>building their C++ stack from versioned source</strong>, not the group that still uses ancient pre-built libraries from 1998.</p>
<p>In practice, of course, this is a gradient. I can only imagine how much sweat, tears, bills and blood must’ve flown to turn big tech codebases from terrifying balls of mud into semi-manageable, buildable, linted, properly versioned, slightly-less-terrifying balls of mud.</p>
<p>With the bias of hindsight, it’s easy to think of all of this as inevitable: There was a clear disconnect between the needs of corporations such as Google (who use highly modern C++, have automated tooling and testing, and modern infrastructure), and the (very strong) desire for backwards compatibility.</p>
<p>To go out on a limb, the notion of a single, dialect-free and unified C++ seems like it’s been <strong>dead for years</strong>.<sup id="fnref:5"><a href="#fn:5" role="doc-noteref">5</a></sup> We have, at the very least, two major flavors of C++:</p>
<ul>
<li>Any <em>remotely</em> modern C++. Probably at least C++17. <code>uniqe_ptr</code>, <code>constexpr</code>, lambdas, <code>optional</code>. Everything can be built from versioned source using some sort of dedicated, clean and unified build process that’s at least slightly more sophisticated than raw CMake, and sort of looks like it just works if you squint a little. Some sort of static analyzers, formatter, linter. <em>Any</em> sort of agreement that keeping a codebase clean and modern is worthwhile.</li>
<li>Anything that’s not that. Any C++ that’s been sitting in ancient, dusted-up servers of a medium-sized bank. Any C++ that relies on some utterly ancient chunk of compiled code, whose source has been lost, and whose original authors are unreachable. Any C++ that sits deployed on pet-type servers, to the point that spinning it up anywhere else would take an engineer a full month just to figure out all of the implicit dependencies, configs, and environment variables. Any codebase which is primarily classified as a cost-center.</li>
</ul>
<p>You’ll notice that the main difference isn’t about C++ itself at all. The difference is <em>tooling</em> and the ability to build from versioned source in any clean, well-defined manner. Ideally, even the ability to <em>deploy</em> without needing to remember that one flag or environment variable the previous guy usually set to keep everything from imploding.</p>
<p>A lot of people will tell you that tooling isn’t the responsibility of the C++ standard committee, and <em>they are right</em>. Tooling isn’t the responsibility of the C++ standard committee, <em>because the C++ standard committee abdicates any responsibility for it</em> (it focuses on specifications for the C++ language, <em>not</em> on concrete implementations). This is by design, and it’s hard to blame them considering the legacy baggage. C++ is a standard unifying different implementations.</p>
<p>That said, if there’s <em>one</em> thing which Go got right, it’s that tooling matters. C++, in comparison, is from a prehistoric age before linters were invented. C++ has no unified build system, it has nothing even close to a unified package management system, it is incredibly hard to parse and analyze (this is <em>terrible</em>  for tooling), and is fighting a horrifying uphill battle against Hyrum’s Law for every change that needs to be made.</p>
<p>There’s a massive, growing rift between those two factions, and I honestly don’t see it closing anytime soon. The C++ committee seems pretty committed (committeed, if you will) to maintaining backwards compatibility, no matter the cost.</p>
<h2 id="consequences">Consequences</h2>
<p>This is why profiles are the way they are: Safety Profiles are <em>not</em> intended to solve the problems of modern, tech-savvy C++ corporations. They’re intended to bring improvements <em>without</em> requiring any changes to old code.</p>
<p>Likewise, modules. You’re intended to be able to “just” import a header file as a module, and there should not be any sort of backwards compatibility issues.</p>
<p>Of course, <em>everyone</em> loves features which can just be dropped-in and bring improvements without requiring any changes to old code. But it’s pretty clear that these features are designed (first and foremost) with the goal of ’legacy C++’ in mind. Any feature that would require a migration from legacy C++ is a non-starter for the C++ committee since, as Herb Sutter said, you essentially cannot expect people to migrate.</p>
<p>This is something which I try to keep in mind when I look at C++ papers: There’s two large audiences here. One is that of modern C++, the other is that of legacy C++. These two camps disagree fiercely, and many papers are written with the needs of one specific group in mind.</p>
<p>The C++-committee is trying to keep this rift from widening. That’s, presumably, why anything in the direction of <a href="https://safecpp.org/draft.html" target="_blank">Safe C++ by Sean Baxter</a> is a non-starter for them. This is a radical, sweeping change that could create a fundamentally new way of writing C++.</p>
<p>Of course, there’s also the question of whether specific C++ standard committee members are just being very, very stubborn, and grasping at straws to prevent an evolution which they personally aesthetically disagree with.</p>
<p>Far be it from to accuse anyone, but it wouldn’t be the first time I heard that the C++ committee applied double standards such as: “We expect a full, working implementation across several working compilers from you if you’d like to see this proposal approved, but we’re still happy to commit to certain vast projects (eg. modules, profiles) that have no functioning proof of concept implementation.”</p>
<p>If <em>this</em> were the case (I genuinely cannot say) then I really wouldn’t know for how much longer C++ could continue going down that road without a much more dramatic split.</p>
<p>And all of that that is not even getting into the massive can of worms and problems that’d be caused by breaking ABI compatibility.</p>
</div>

</article>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Bluesky is on the verge of overtaking Threads in all the ways that matter (286 pts)]]></title>
            <link>https://mashable.com/article/bluesky-gaining-ground-on-competitor-meta-threads</link>
            <guid>42231148</guid>
            <pubDate>Sun, 24 Nov 2024 22:23:14 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://mashable.com/article/bluesky-gaining-ground-on-competitor-meta-threads">https://mashable.com/article/bluesky-gaining-ground-on-competitor-meta-threads</a>, See on <a href="https://news.ycombinator.com/item?id=42231148">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
        

        
    <p>Meta-owned Threads started November with 5 times the daily app users of Bluesky. That number is now down to just 1.5.</p>
    

    
</div><section data-ga-module="content_body">
                        <div>
                <p><img src="https://helios-i.mashable.com/imagery/articles/05ga38nPz2tMnT4fT0Plssj/hero-image.fill.size_1248x702.v1732394007.jpg" alt="The image shows the logo of Bluesky" width="1248" height="702" srcset="https://helios-i.mashable.com/imagery/articles/05ga38nPz2tMnT4fT0Plssj/hero-image.fill.size_400x225.v1732394007.jpg 400w, https://helios-i.mashable.com/imagery/articles/05ga38nPz2tMnT4fT0Plssj/hero-image.fill.size_800x450.v1732394007.jpg 800w, https://helios-i.mashable.com/imagery/articles/05ga38nPz2tMnT4fT0Plssj/hero-image.fill.size_1248x702.v1732394007.jpg 1600w" sizes="(max-width: 1280px) 100vw, 1280px"></p><p><span>Credit: Matteo Della Torre/NurPhoto via Getty Images</span>
                            </p>
                        </div>

    
    
    
            <article id="article" data-autopogo="">
                                    <p><a href="https://mashable.com/article/bluesky-fake-accounts-problem" target="_self" data-ga-click="1" data-ga-label="$text" data-ga-item="text-link" data-ga-module="content_body">Bluesky</a>, the formerly Jack Dorsey-affiliated, decentralized answer to Elon Musk's X is closing the gap with Threads at breakneck speed. The browser version of Bluesky surpassed Threads in total usage weeks ago, but now the Bluesky app has exploded to 3.5 million daily active users, putting it just 1.5 times behind Meta’s Threads — an impressive feat considering the Threads app had 5x Bluesky's active users at the start of the month.</p>
<p>The momentum shift has been nothing short of seismic, especially in the wake of the November 5 election. According to Similarweb data <a href="https://www.ft.com/content/e1b52147-c171-4902-8bce-204ba0905912" target="_blank" data-ga-click="1" data-ga-label="$text" data-ga-item="text-link" data-ga-module="content_body" title="(opens in a new window)">reported by the<em> Financial Times</em></a>, Bluesky’s user base has ballooned by 300 percent since Election Day. Journalists, academics, and companies are fleeing Elon Musk’s chaotic X (formerly Twitter) in droves, and Bluesky is quickly becoming their platform of choice.</p><p>Why Bluesky over Threads? Meta CEO Mark Zuckerberg’s decision to <a href="https://mashable.com/article/instagram-threads-political-content" target="_self" data-ga-click="1" data-ga-label="$text" data-ga-item="text-link" data-ga-module="content_body">downplay political content</a> on Threads appears to have turned off many users seeking vibrant public discourse. Critics see it as an <a href="https://thehill.com/policy/technology/4934534-trump-zuckerberg-politics-facebook/" target="_blank" data-ga-click="1" data-ga-label="$text" data-ga-item="text-link" data-ga-module="content_body" title="(opens in a new window)">attempt to stay in President-elect Donald Trump’s good graces</a>, effectively neutering the platform’s potential as a forum for political and cultural debate. </p><section x-data="window.newsletter()" x-init="init()" data-ga-impression="" data-ga-category="newsletters" data-ga-module="incontent_nl_signup" data-ga-label="mashablelightspeed">
        <p>
            Mashable Light Speed
        </p>
        
        
    </section>
<p>Bluesky has quickly become the go-to platform for what commentator Max Read has <a href="https://maxread.substack.com/p/does-bluesky-have-the-juice" target="_blank" data-ga-click="1" data-ga-label="$text" data-ga-item="text-link" data-ga-module="content_body" title="(opens in a new window)">called</a> the "Politically Engaged Email Job Blob" — the same cohort that helped transform early Twitter into the cultural juggernaut it once was.</p><p>That said, Bluesky is still very much a work in progress. Its rapid growth has brought its share of headaches, including outages, glitches, and <a href="https://mashable.com/article/bluesky-fake-accounts-problem" target="_self" data-ga-click="1" data-ga-label="$text" data-ga-item="text-link" data-ga-module="content_body">scammers</a>. As more users flock to this latest "Twitter replacement," expect some inevitable growing pains along the way.</p>

                                        
                    </article>
    
    
    
            <div>
            <div>
                    <p><img src="https://helios-i.mashable.com/imagery/authors/03tgQJc7HbwhuDaGMuSAh8J/image.fill.size_100x100.v1673717191.jpg" alt="Headshot of a Black man" width="100" height="100" loading="lazy"></p><div>
                        
                        <p>Assistant Editor, General Assignments</p>
                    </div>
                </div>
            <div>
                <p>Currently residing in Chicago, Illinois, Chance Townsend is the General Assignments Editor at Mashable covering tech, video games, dating apps, digital culture, and whatever else comes his way. He has a Master's in Journalism from the University of North Texas and is a proud orange cat father. His writing has also appeared in PC Mag and <em>Mother Jones</em>.</p><p>In his free time, he cooks, loves to sleep, and finds great enjoyment in Detroit sports. If you have any stories, tips, recipes, or wanna talk shop about the Lions/Tigers/Pistons/Red Wings you can reach him at <a href="https://mashable.com/cdn-cgi/l/email-protection" data-cfemail="36555e57585553184259415845535852764c5f50505b53525f571855595b">[email&nbsp;protected]</a></p>
            </div>
        </div>
        
        
                    </section><section data-ga-module="content_body">
        <section>
                                    <hr>
            
                            
                            <div data-module="content-list" data-ga-module="recommendation-recirc" data-ga-element="content-stripe" data-ga-action="content-stripe">
                            
                                                    <hr>
                                            
                                                    <hr>
                                            
                                                    <hr>
                                            
                                                    <hr>
                                            <div data-ga-position="5">
    
    <a data-ga-click="" data-ga-item="image" data-ga-label="X users are fleeing to Bluesky: Here’s a quick-start guide on how to sign up" href="https://mashable.com/article/getting-started-on-bluesky-guide">
        <p><img src="https://helios-i.mashable.com/imagery/articles/00hLKqqdMxBC9PKtD18sBCb/hero-image.fill.size_220x133.v1731628618.jpg" alt="Bluesky logo" width="220" height="133" loading="lazy">


        </p>
        <p><img src="https://helios-i.mashable.com/imagery/articles/00hLKqqdMxBC9PKtD18sBCb/hero-image.fill.size_220x220.v1731628618.jpg" alt="Bluesky logo" width="220" height="220" loading="lazy">


        </p>
    </a>
</div>
                                                    </div>
                                </section>

    
    
    </section><div x-data="window.newsletter()" x-init="init()" data-ga-impression="" data-ga-category="newsletters" data-ga-module="footer_nl_signup" data-ga-label="Top Stories">
    

    <p>
        This newsletter may contain advertising, deals, or affiliate links. Subscribing to a newsletter indicates your consent to our <a href="https://www.ziffdavis.com/terms-of-use" target="_blank" rel="noopener" title="(opens in a new window)">Terms of Use</a> and <a href="https://www.ziffdavis.com/ztg-privacy-policy" target="_blank" rel="noopener" title="(opens in a new window)">Privacy Policy</a>. You may unsubscribe from the newsletters at any time.
    </p>
    
</div></div>]]></description>
        </item>
    </channel>
</rss>