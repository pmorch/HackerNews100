<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0">
    <channel>
        <title>HN100 - Readable Contents</title>
        <link>https://hn.algolia.com/api/v1/search_by_date?tags=%28story,poll%29&amp;numericFilters=points%3E100</link>
        <description>Uses Readability to add bodies to the RSS feed</description>
        <lastBuildDate>Sat, 22 Feb 2025 14:30:04 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>en</language>
        <item>
            <title><![CDATA[DOGE's Only Public Ledger Is Riddled with Mistakes (119 pts)]]></title>
            <link>https://www.nytimes.com/2025/02/21/upshot/doge-musk-trump-errors.html</link>
            <guid>43138238</guid>
            <pubDate>Sat, 22 Feb 2025 12:03:22 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.nytimes.com/2025/02/21/upshot/doge-musk-trump-errors.html">https://www.nytimes.com/2025/02/21/upshot/doge-musk-trump-errors.html</a>, See on <a href="https://news.ycombinator.com/item?id=43138238">Hacker News</a></p>
Couldn't get https://www.nytimes.com/2025/02/21/upshot/doge-musk-trump-errors.html: Error: Request failed with status code 403]]></description>
        </item>
        <item>
            <title><![CDATA[Who needs a sneaker bot when AI can hallucinate a win for you? (155 pts)]]></title>
            <link>https://www.eql.com/media/sneaker-bot-ai-error</link>
            <guid>43135382</guid>
            <pubDate>Sat, 22 Feb 2025 02:08:11 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.eql.com/media/sneaker-bot-ai-error">https://www.eql.com/media/sneaker-bot-ai-error</a>, See on <a href="https://news.ycombinator.com/item?id=43135382">Hacker News</a></p>
<div id="readability-page-1" class="page"><div fs-toc-element="contents" fs-richtext-element="rich-text"><p>Every February we see a big spike in retailers running sneaker launches on EQL to coincide with the 2025 NBA All-Star Weekend. This year, the festivities kicked off a little earlier than normal, courtesy of Jordan Brand, who have been getting serious about getting back their mojo and dropping some serious heat in the process. This year marks 40 years since a talented rookie by the name of Michael Jordan wore his signature shoes at the 1985 All-Star Dunk Contest in a colorway that is now firmly ingrained in sneaker culture lore. To mark the occasion, Jordan Brand recreated the shoe in what they say is the closest to OG spec ever. Sneakerheads have been anticipating the drop for months and the demand was predictably crazy.</p><p>At first, everything appeared to be going smoothly. Thousands of entries were rolling in, bots were being neutralized, winners were being picked and notified, but as we started notifying non-winners that they‚Äôd missed out, things got weird.</p><p>We started seeing reports of people being told that they had <em>won and lost in the same email‚Ä¶</em> the Schr√∂dinger‚Äôs cat of bug reports. What on earth was going on??</p><figure><p><img src="https://cdn.prod.website-files.com/63f4b950d4ba499d292a5807/67b6501b3cd8c968f609f0d6_Screenshot1.webp" loading="lazy" alt=""></p></figure><figure><p><img src="https://cdn.prod.website-files.com/63f4b950d4ba499d292a5807/67b650268e295d42fbd80cf9_Screenshot2.webp" loading="lazy" alt=""></p></figure><p>Now, we‚Äôre accustomed to a baseline level of crazy ‚Äì it comes with the territory of running some of the hottest product launches on the internet. Having a 24/7 support team who can bring calm to the chaos is big reason why brands work with EQL. But even for us, this one felt odd.</p><p>Fans were reporting that their email app was listing ‚ÄùYou‚Äôve been selected‚Äù but when they clicked on the email it displayed the heartbreaking ‚ÄúSORRY‚Äù non-winner message. Sneaker fans are a passionate bunch, and online launches often trigger the full range of human emotions - to put it mildly! People were understandably mad about the confusing messaging.</p><p>When things heat up, it‚Äôs helpful to have a community you can turn to. We operate in a space where people sometimes troll for fun ‚Äì or profit, like the <a href="https://x.com/EQLofficial/status/1632861398425559041">time</a> someone tried to pretend they‚Äôd won 750 pairs of the <a href="https://tiffany.runfair.com/en-GB/us/niketiffany-air-force-1-1837">Nike/Tiffany Air Force 1 1837</a> to pump a cookgroup.</p><p>We reached out to our <a href="https://discord.com/servers/eql-1136853080487514182">Discord community</a>, who confirmed that the issue was real and shared some more screenshots:</p><figure><p><img src="https://cdn.prod.website-files.com/63f4b950d4ba499d292a5807/67b6505e233e0e61481beccc_Screenshots3.webp" loading="lazy" alt=""></p></figure><p>The bizarre thing is that our winner emails don‚Äôt actually say ‚ÄúYou‚Äôve been selected‚Äù. As an on-call engineer, this is the point when you start questioning your life choices. You know that the issue is affecting thousands of users, but the offending phrase doesn‚Äôt appear anywhere in EQL‚Äôs codebase, aside from some very old launches several years ago.</p><p>As more screenshots poured in, another odd thing jumped out ‚Äì the winning phrase seemed to change from email to email. Some said ‚Äúselected <em>to purchase </em>JORDAN AJ1‚Äù, others said ‚Äúselected <em>for the </em>JORDAN AJ1‚Äù:</p><figure><p><img src="https://cdn.prod.website-files.com/63f4b950d4ba499d292a5807/67b6507dc983919b3c85a7df_Screenshot4.webp" loading="lazy" alt=""></p></figure><p>By this point, we‚Äôd narrowed down the affected users to a single email client - <strong>Yahoo Mail</strong>, which is where we got suspicious. Had Yahoo Mail introduced any features lately that might be causing this‚Ä¶?</p><p>As it turns out, yes, yes they had. A quick Google search revealed that a few months ago Yahoo jumped on the AI craze with the launch of ‚Äù<a href="https://www.yahooinc.com/press/yahoo-mail-launches-a-new-app-experience-with-mobile-first-ai-features"><strong>AI-generated, one-line email summaries‚Äù</strong></a><strong>.</strong></p><p>At this point, the penny dropped. Just like Apple AI <a href="https://edition.cnn.com/2025/01/16/media/apple-ai-news-fake-headlines/index.html">generating fake news summaries</a>, Yahoo AI was hallucinating the fake winner messages, presumably as a result of training their model on our old emails. Worse, they were putting an untrustworthy AI summary in the exact place that users expect to see an email subject, with no mention of it being AI-generated ü§Ø</p><p>Some people use Yahoo Mail to read emails from other providers like Gmail, so this issue hit a broad range of sneaker fans. And it‚Äôs worth mentioning that as of the time of writing, <strong>this AI feature is still live in Yahoo Mail</strong>, so more confusion is expected in future ‚Äì not just for sneaker fans receiving launch results, but for anyone reading important emails in Yahoo Mail.</p><p>For EQL users, if you‚Äôre ever in doubt, you can always double-check your launch results in the <a href="https://app.eql.com/">fans app</a>, contact our support team (support@eql.com) or join our helpful <a href="https://discord.com/servers/eql-1136853080487514182">Discord community</a>. Until then, we‚Äôll continue fighting the good fight to put products into the hands of real fans and trying to prevent bad AI from ruining your day! ü§ñ</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Start a computer club in the place that you live (2023) (149 pts)]]></title>
            <link>https://startacomputer.club/</link>
            <guid>43135176</guid>
            <pubDate>Sat, 22 Feb 2025 01:30:18 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://startacomputer.club/">https://startacomputer.club/</a>, See on <a href="https://news.ycombinator.com/item?id=43135176">Hacker News</a></p>
<div id="readability-page-1" class="page">

<!-- make translations available -->
<hr>







<hr>
<!-- translation section is done -->

<h2>YOU SHOULD START A COMPUTER CLUB IN THE PLACE THAT YOU LIVE</h2>
<pre dir="ltr" lang="eo">              ,---------------------------,
              |  /---------------------\  |
              | | LASTA NOVAƒ¥O:         | |
              | | VI DEVUS ESTABLI      | |
              | | KOMPUTILAN KLUBON     | |
              | | EN LA LOKO,           | |
              | | KIE VI LOƒúAS          | |
              |  \_____________________/  |
              |___________________________|
            ,---\_____     []     _______/------,
          /         /______________\           /|
        /___________________________________ /  | ___
        |                                   |   |    )
        |  _ _ _                 [-------]  |   |   (
        |  o o o                 [-------]  |  /    _)_
        |__________________________________ |/     /  /
    /-------------------------------------/|      ( )/
  /-/-/-/-/-/-/-/-/-/-/-/-/-/-/-/-/-/-/-/ /
/-/-/-/-/-/-/-/-/-/-/-/-/-/-/-/-/-/-/-/ /
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
</pre>
<p><em>i'm glad someone finally said it</em></p>

<h2>WHAT IS A COMPUTER CLUB?</h2>
<p>a computer club is where a group of people hang out and do computer together</p>

<h2>WHAT IS "DOING COMPUTER"?</h2>
<p>doing computer is whatever you want...on computers, together. our bias is towards programming and <span dir="ltr" lang="en"> diy </span> shared computing infrastructure. but there's also art and music and open data science and circuit-bending and a million other things we don't know about</p>

<h2>WHY SHOULD I START A COMPUTER CLUB?</h2>
<p>the political economy of computing is awful. have you read <a dir="ltr" lang="en" href="https://bookwyrm.social/book/525851/s/palo-alto"> palo alto</a>? me neither. we should read it. we deserve better than the <span dir="ltr" lang="en"> darpa-funded </span> visions of <span dir="ltr" lang="en"> xerox parc </span> technologists</p>
<p>make the political economy of computing less awful and bring it home to you and yours by starting a computer club in the place that you live</p>

<h2>RULES</h2>
<p>we can't make you do anything, so do whatever you want. but these are the things we think are really important</p>
<ul>
    <li><strong>hang out in real life:</strong> online has rude vibes, real life has kinder vibes. hang out in real life for trust and strength and to ground your computer club in your actual local context</li>
    <li><strong>reject corporate sponsorship:</strong> corporate sponsorship constrains behavior and undermines collective ownership. computer club isn't yours if a corporate sponsor might get upset by something you do</li>
    <li><strong>computer club is a collective project:</strong> you don't need a mission statement, but you might have an ethos that defines your computer club. computer club is beholden only to that ethos and the people who show up</li>
</ul>

<h2>GUIDELINES</h2>
<p>we still can't make you do anything, so continue doing whatever you want. but these are things we think are important to think about</p>
<ul>
    <li>computing is political, so let computer club be political too</li>
    <li>the <a href="https://www.recurse.com/social-rules"><span dir="ltr" lang="en"> recurse center </span> social rules</a> foster collaboration and psychological safety, consider using them</li>
    <li>be inspired by permaculture</li>
    <li>be inspired by small web</li>
    <li>be inspired by <span dir="ltr" lang="en"> diy </span> culture</li>
    <li>be inspired by computing as a medium through which better things are possible</li>
    <li>try to host computer club's stuff on your own computers in the place that you live</li>
    <li>be open to interdisciplinary computing</li>
    <li>be open to different histories and "skill levels" with computing</li>
</ul>

<h2>HOW SHOULD I START THE COMPUTER CLUB</h2>
<p>there are lots of ways to start a computer club; how you start will be unique to yours. but you could try</p>
<ul>
    <li>talk about what used to excite you about doing computer. talk about how the political economy of computing could be better. talk about these things publicly. the computer club might already be breathing in the communities you're in</li>
    <li>attend preexisting computing meetups and find like-minded people. meetups about "how to <span dir="ltr" lang="en"> node.js </span> apolitically" are sidelining people who want "how to <span dir="ltr" lang="en"> node.js </span> pro-socially." don't try to be a recruiter -- just be excited at events and find people who are also excited</li>
    <li>does your city have a food coop? food coops and computer clubs have a similar ethos; and food coops are a great place to get your food for the same reasons that a computer club is a great place to do computer. make friends at your food coop and find the computer-doers</li>
    <li>join a project or start a project and talk to people there about starting a computer club. are you or someone you know doing<ul>
        <li>web design for a <span dir="ltr" lang="en"> diy </span> venue? work together!</li>
        <li>technical support for a cool local project? work together!</li>
        <li>communications for your neighborhood's nascent mesh network? work together!</li>
        <li>analysis for local open data projects? work together!</li>
    </ul></li>
    <li>talk to existing computer clubs!</li>
</ul>

<h2>COMPUTER CLUBS AND SIMILAR THINGS THAT WE KNOW OF</h2>
<ul>
    <li dir="ltr" lang="en"><a href="https://cyberia.club/">cyberia computer club</a></li>
    <li dir="ltr" lang="en"><a href="https://www.ccc.de/">chaos computer club</a></li>
    <li dir="ltr" lang="en"><a href="https://bunk.computer/">bunk computer club</a></li>
    <li dir="ltr" lang="fr"><a href="https://deuxfleurs.fr/">deuxfleurs</a></li>
    <li dir="ltr" lang="en"><a href="https://lurk.org/">LURK</a></li>
    <li dir="ltr" lang="en"><a href="https://plover.digital/">plover digital</a></li>
    <li>computer clubs are inspired by <a dir="ltr" lang="en" href="https://en.m.wikipedia.org/wiki/Category:Hackerspaces"> hackerspaces</a></li>
    <li>[coming soon] your computer club!</li>
</ul>

<h2>TOOLS THAT ARE USEFUL FOR A COMPUTER CLUB</h2>
<ul>
    <li>computers (personal computers)</li>
    <li>computers (servers)</li>
    <li>computers (people doing computer)</li>
    <li><strong>chat:</strong><span dir="ltr" lang="en"> delta chat, matrix, zulip, mattermost, signal, discord</span></li>
    <li>love and trust!</li>
    <li>the <a href="https://www.recurse.com/social-rules"><span dir="ltr" lang="en"> recurse center </span> social rules</a></li>
    <li>a git forge (we like<span dir="ltr" lang="eo"> forgejo</span>)</li>
    <li>a physical <span> tilde </span> server (it can motivate everyone to create something together)</li>
    <li><strong>locations:</strong> living rooms, libraries, a cool local bookstore</li>
    <li><strong>mob programming:</strong> a big monitor, tv, or a projector. computing together is fun and contributes to your computer club's culture!</li>
    <li><a href="https://awesome-selfhosted.net/">a very thorough resource for collective computing infrastructure</a></li>
    <li>connections to other local projects</li>
    <li><em>connections to other local projects</em></li>
    <li><em><strong>connections to other local projects</strong></em></li>
</ul>

<h2>WHAT NOW?</h2>
<p>go! find the computer club! make the computer club! tell us about it!</p>

<hr>

<hr>


</div>]]></description>
        </item>
        <item>
            <title><![CDATA[We the Builders (474 pts)]]></title>
            <link>https://www.wethebuilders.org/</link>
            <guid>43133648</guid>
            <pubDate>Fri, 21 Feb 2025 22:07:20 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.wethebuilders.org/">https://www.wethebuilders.org/</a>, See on <a href="https://news.ycombinator.com/item?id=43133648">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-testid="gridContainer"><h2>Blog</h2><p>Real stories from federal employees.</p><ul><li><a href="https://www.wethebuilders.org/posts/a-tale-of-two-effiencies">A Tale of Two Efficiencies: U.S. Digital Service vs. DOGE</a></li><li><a href="https://www.wethebuilders.org/posts/what-is-us-digital-service">What is the US Digital Service and Why Does it Matter?</a></li></ul><h2>Who We Are</h2><p>For decades, we've done our jobs in the background. We made it easier to file taxes, get veterans' benefits, and apply for financial aid. During times of crisis, we helped refugees navigate immigration processes, helped everyone find vaccines, and helped parents find baby formula.</p><p>Along the way, we made government websites easier to use while protecting the integrity of your personal information.</p><p>If they really wanted to know how to use technology to build a more efficient country, they would ask us.</p><p>But they haven't. They are destroyers.</p><p>We are the builders.</p><h2>Our mission</h2><p>We don't work for DOGE. We have always worked for you.</p><p>Here, you'll find stories from real government employees: How we save you time and money, how we protect your personal information, and how DOGE's dangerous dismantling of government technology puts you at risk.</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[20 years working on the same software product (418 pts)]]></title>
            <link>https://successfulsoftware.net/2025/02/21/20-years-working-on-the-same-software-product/</link>
            <guid>43133174</guid>
            <pubDate>Fri, 21 Feb 2025 21:22:55 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://successfulsoftware.net/2025/02/21/20-years-working-on-the-same-software-product/">https://successfulsoftware.net/2025/02/21/20-years-working-on-the-same-software-product/</a>, See on <a href="https://news.ycombinator.com/item?id=43133174">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
			
<p>I released version 1 of my <a href="https://www.perfecttableplan.com/">table seating planning software</a>, PerfectTablePlan, in February 2005. 20 years ago this month. It was a different world. A world of Windows, shareware and CDs. A lot has changed since then, but PerfectTablePlan is now at version 7 and still going strong.</p>



<figure><a href="https://successfulsoftware.net/wp-content/uploads/2025/02/v1-screenshot-20th.png"><img data-attachment-id="12446" data-permalink="https://successfulsoftware.net/2025/02/21/20-years-working-on-the-same-software-product/v1-screenshot-20th/" data-orig-file="https://successfulsoftware.net/wp-content/uploads/2025/02/v1-screenshot-20th.png" data-orig-size="700,438" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="v1-screenshot-20th" data-image-description="" data-image-caption="" data-medium-file="https://successfulsoftware.net/wp-content/uploads/2025/02/v1-screenshot-20th.png?w=300" data-large-file="https://successfulsoftware.net/wp-content/uploads/2025/02/v1-screenshot-20th.png?w=625" width="700" height="438" src="https://successfulsoftware.net/wp-content/uploads/2025/02/v1-screenshot-20th.png?w=700" alt=""></a></figure>



<p>PerfectTablePlan v1</p>



<figure><a href="https://successfulsoftware.net/wp-content/uploads/2025/02/v7-screenshot-20th.png"><img data-attachment-id="12447" data-permalink="https://successfulsoftware.net/2025/02/21/20-years-working-on-the-same-software-product/v7-screenshot-20th/" data-orig-file="https://successfulsoftware.net/wp-content/uploads/2025/02/v7-screenshot-20th.png" data-orig-size="700,444" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="v7-screenshot-20th" data-image-description="" data-image-caption="" data-medium-file="https://successfulsoftware.net/wp-content/uploads/2025/02/v7-screenshot-20th.png?w=300" data-large-file="https://successfulsoftware.net/wp-content/uploads/2025/02/v7-screenshot-20th.png?w=625" width="700" height="444" src="https://successfulsoftware.net/wp-content/uploads/2025/02/v7-screenshot-20th.png?w=700" alt=""></a></figure>



<p>PerfectTablePlan v7</p>



<p>I have released several other products since then, and done some training and consulting, but PerfectTablePlan remains my most successful product. It‚Äôs success is due to a lot of hard work, and a certain amount of dumb luck.</p>



<p>I was getting married and I volunteered to do the seating plan for our wedding reception. It sounded like a relatively straightforward optimization problem, as we only had 60 guests and no family feuds to worry about. But it was surprisingly difficult to get right. I looked around for some software to help me. There were a couple of software packages, but I wasn‚Äôt impressed. I could do better myself! So I wrote a (very rough) first version, which I used for our wedding.</p>



<p>Things weren‚Äôt going great at my day job, at a small software startup. Maybe I could commercialize my table planner? I was a bit wary, as my potential competitors all seemed rather moribund and I didn‚Äôt think I would be able to make a living off it. But I thought I could do everything worth doing in 6-12 months and then start on the next product. Wrong on both counts!</p>



<p>Web-based software was still in its infancy in 2005. So I decided to write it as desktop software using C++ and cross-platform framework Qt, which I had plenty of experience in. Initially, I just released a Windows version. But I later added a Mac version as well. Qt has had its commercial ups and downs in the last 20 years, but it has grown with me and is now very robust, comprehensive and well documented. I think I made a good choice.</p>



<p>I financed PerfectTablePlan out of my own savings and it has been profitable every year since version 1 was launched. I could have taken on employees and grown the business, but I preferred to keep it as a <a href="https://successfulsoftware.net/2013/11/06/lifestyle-programming/">lifestyle business</a>. My wife does the accounts and proof reading and I do nearly everything else, with a bit of help from my accountant, web designers and a few other contractors. I don‚Äôt regret that decision. 20 years without meetings, ties or alarm clocks. My son was born 18 months after PerfectTablePlan was launched and it has been great to have the flexibility to be fully present as a Dad.</p>



<p>CDs, remember them? I sent out around 5,000 CDs (with some help from my father), before I stopped shipping CDs in 2016.</p>


<div>
<figure><a href="https://successfulsoftware.net/wp-content/uploads/2025/02/l-shadow-only-hq.png"><img data-attachment-id="12477" data-permalink="https://successfulsoftware.net/2025/02/21/20-years-working-on-the-same-software-product/l-shadow-only-hq-2/" data-orig-file="https://successfulsoftware.net/wp-content/uploads/2025/02/l-shadow-only-hq.png" data-orig-size="500,500" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="L-Shadow-Only-HQ" data-image-description="" data-image-caption="" data-medium-file="https://successfulsoftware.net/wp-content/uploads/2025/02/l-shadow-only-hq.png?w=300" data-large-file="https://successfulsoftware.net/wp-content/uploads/2025/02/l-shadow-only-hq.png?w=500" width="500" height="500" src="https://successfulsoftware.net/wp-content/uploads/2025/02/l-shadow-only-hq.png?w=500" alt=""></a></figure></div>


<p>During the lifetime of PerfectTablePlan it became clear that things were increasingly moving to the web. But I couldn‚Äôt face rewriting PerfectTablePlan from scratch for the web. Javascript. Ugh. Also PerfectTablePlan is quite compute intensive, using a genetic algorithm to generate an automated seating plan and I felt it was better running this on the customer‚Äôs local computers than my server. And some of my customers consider their seating plans to be confidential and don‚Äôt want to store them on third party servers. So I decided to stick with desktop. But, if I was starting PerfectTablePlan from scratch now, I might make a different decision.</p>



<p>Plenty of strange and wonderful things have happened over the last 20 years, including:</p>



<ul>
<li>PerfectTablePlan has been used by some very famous organizations for some very famous events (which we mostly don‚Äôt have permission to mention). It has seated royalty, celebrities and heads of state.</li>



<li>PerfectTablePlan was used as part of a <a href="https://www.perfecttableplan.com/newsletters/newsletter10_web.html">demonstration of the (controversial) first commercial quantum computer by D-Wave</a>.</li>



<li>A mock-up of PerfectTablePlan, including icons I did myself, was <a href="https://www.perfecttableplan.com/newsletters/newsletter_8.html">used without our permission</a> by Sony in their ‚ÄòBig day‚Äô TV comedy series. I threated them with legal action. Years later, I am still awaiting a reply.</li>



<li>I got to grapple with some interesting problems, including the mathematics of <a href="https://www.perfecttableplan.com/html/genetic_algorithm.html">large combinatorial problems</a> and <a href="https://successfulsoftware.net/2008/07/18/a-mathematical-digression/">elliptical tables</a>. Some customers have seated 4,000 guests and 4000! (4000x3999x3998 .. x 1) is a mind-bogglingly huge number.</li>



<li>A well known wedding magazine ran a promotion with a valid licence key clearly visible in a photograph of a PerfectTablePlan CD. I worked through the night to release a new version of PerfectTablePlan that didn‚Äôt work with this key.</li>



<li>I found out that <a href="https://www.perfecttableplan.com/html/the_dog_ate_my_cd.html">CDs are edible</a>.</li>



<li>I sponsored the <a href="https://thejunipertrust.org/jt_projects/bampti-kindergaten-school/">building of a kindergarten in Nepal</a>.</li>



<li>I once had to stay up late, in a state of some inebriation, to fix an issue so that a world famous event wasn‚Äôt a disaster (no I can‚Äôt tell you the event).</li>
</ul>



<p>The lowest point was the pandemic, when sales pretty much dropped to zero.</p>



<p>Competitors and operating systems have come and gone and the ecosystem for software has changed a lot, but PerfectTablePlan is still here and still paying the bills. It is about 145,000 lines of C++. Some of the code is a bit ugly and not how I would write it now. But the product is very solid, with very few bugs. The website and user documentation are also substantial pieces of work. The PDF version of the documentation is nearly 500 pages.</p>



<p>I now divide my time between PerfectTablePlan and my 2 other products: <a href="https://www.easydatatransform.com/">data wrangling software</a> Easy Data Transform and <a href="https://www.hyperplan.com/">visual planner</a> Hyper Plan. Having multiple products keeps things varied and avoids having all my eggs in one basket. In May 2024 I released PerfectTablePlan v7 with a load of improvements and new features. And I have plenty of ideas for future improvements. I fully expect to keep working on PerfectTablePlan until I retire (I‚Äôm 59 now).</p>




					</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Show HN: Slime OS ‚Äì An open-source app launcher for RP2040 based devices (119 pts)]]></title>
            <link>https://github.com/abeisgoat/slime_os</link>
            <guid>43132482</guid>
            <pubDate>Fri, 21 Feb 2025 20:22:57 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/abeisgoat/slime_os">https://github.com/abeisgoat/slime_os</a>, See on <a href="https://news.ycombinator.com/item?id=43132482">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><p dir="auto"><h2 tabindex="-1" dir="auto">slime_os</h2><a id="user-content-slime_os" aria-label="Permalink: slime_os" href="#slime_os"></a></p>
<p dir="auto">Slime OS is an app launcher for the <a href="https://collabs.shop/fca3j3" rel="nofollow">PicoVision</a> (and soon other RP2040 and RP2350 devices). It was originally designed for the <a href="https://youtu.be/rnwPmoWMGqk" rel="nofollow">Slimedeck Zero</a>, a mini cyberdeck project. However I hope to expand it to other devices and form factors.</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/abeisgoat/slime_os/blob/main/screenshot.gif"><img src="https://github.com/abeisgoat/slime_os/raw/main/screenshot.gif" alt="Slime OS launcher and i2c Scan app" data-animated-image=""></a></p>
<p dir="auto"><em>This README contains affiliate links which help support this project!</em></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Software</h2><a id="user-content-software" aria-label="Permalink: Software" href="#software"></a></p>
<p dir="auto">Slime OS runs in a limited 32-color mode with a 400x240 internal resolution which is interlaced up to 800x480. This resolution should scale well on most HDMI displays.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Setup</h3><a id="user-content-setup" aria-label="Permalink: Setup" href="#setup"></a></p>
<ol dir="auto">
<li>Flash the <a href="https://github.com/pimoroni/picovision/releases">widescreen build</a> of the PicoVision firmware to your PicoVision CPU.</li>
<li>Use <a href="https://thonny.org/" rel="nofollow">Thonny</a> to replace the contents of your PicoVision Micropython filesystem with the files in <code>src</code>.</li>
</ol>
<p dir="auto"><h3 tabindex="-1" dir="auto">Making apps</h3><a id="user-content-making-apps" aria-label="Permalink: Making apps" href="#making-apps"></a></p>
<p dir="auto">Please refer to an <a href="https://github.com/abeisgoat/slime_os/blob/main/src/flashlight_app.py">example app</a> for boiler plate.</p>
<p dir="auto">Slime OS includes various libraries which are used internally but may also be helpful when making apps.</p>
<p dir="auto">Begin by importing slime_os...</p>

<markdown-accessiblity-table><table>
<thead>
<tr>
<th>Slime OS Library</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td><a href="https://github.com/abeisgoat/slime_os/blob/main/src/slime_os/graphics.py">sos.gfx</a></td>
<td>Drawing methods including shapes, text, and other utilities.</td>
</tr>
<tr>
<td><a href="https://github.com/abeisgoat/slime_os/blob/main/src/slime_os/intents.py">sos.intents</a></td>
<td>Intents are used to send signals from an app to the OS, including quitting the app, swapping apps, or flipping the frame buffer.</td>
</tr>
<tr>
<td><a href="https://github.com/abeisgoat/slime_os/blob/main/src/slime_os/expansion.py">sos.ctrl</a></td>
<td>Controller for identifying expansions.</td>
</tr>
<tr>
<td><a href="https://github.com/abeisgoat/slime_os/blob/main/src/slime_os/keyboard_i2c.py">sos.kbd</a></td>
<td>Keyboard instance for reading buttons.</td>
</tr>
</tbody>
</table></markdown-accessiblity-table>
<p dir="auto"><h3 tabindex="-1" dir="auto">Issues</h3><a id="user-content-issues" aria-label="Permalink: Issues" href="#issues"></a></p>
<p dir="auto">This software is experimental and does not work completely, specifically issues include...</p>
<ul dir="auto">
<li>Input is only supported via an i2c keyboard, which is not documented (hoping to add USB keyboard soon)</li>
<li>Some apps are upside down due to the Slimedeck having the screen rotated 180 degrees. Newer apps use the <code>sos.graphics.*</code> methods which support the <code>display/flipped</code> value in <code>config.py</code>. Older apps use <code>self.display.*</code> and have wild math to rotate everything manually.</li>
<li>Everything is generally incomplete</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Hardware</h2><a id="user-content-hardware" aria-label="Permalink: Hardware" href="#hardware"></a></p>
<p dir="auto">Currently this project uses a very specific set of hardware, however I'd like to expand it to support other RP2040 and RP2350 boards in the future. Feel free to do PRs to add more general hardware support.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Mainboard</h3><a id="user-content-mainboard" aria-label="Permalink: Mainboard" href="#mainboard"></a></p>
<p dir="auto">This project is currently only tested on the <a href="https://collabs.shop/fca3j3" rel="nofollow">PicoVision</a>.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Keyboard</h3><a id="user-content-keyboard" aria-label="Permalink: Keyboard" href="#keyboard"></a></p>
<p dir="auto">The (currently) only support keyboard is based off this <a href="https://www.amazon.com/dp/B01IOZBNBC/?tag=boosteroven-20" rel="nofollow">XRT500 remote</a>. There are a few variants of this same "version" of remote, but I've only used this exact one.</p>
<p dir="auto">Along with the remote, you will need an <a href="https://www.adafruit.com/product/732" rel="nofollow">MCP23017</a> to convert the key matrix to I2C. I have a few extra of the <a href="https://abe.today/products/mcp23017-port-expander-for-xrt500-tv-remote" rel="nofollow">keyboard PCBs available for sale</a>. You could also pick up an <a href="https://www.adafruit.com/product/5346" rel="nofollow">Adafruit MCP23017 board</a> and wire it to the keyboard PCB by hand.</p>
<p dir="auto">I would like to add more input types in the future.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Expansion Port</h3><a id="user-content-expansion-port" aria-label="Permalink: Expansion Port" href="#expansion-port"></a></p>
<p dir="auto">The expansion port used on the Slimedeck is the <a href="https://cdn.shopify.com/s/files/1/0174/1800/files/DK925A-10M.pdf?v=1643016288" rel="nofollow">5-pin Dk925A-10M</a> which, as far as I can tell, are only <a href="https://collabs.shop/knlijz" rel="nofollow">sold via Pimoroni</a>.</p>
<p dir="auto">The pinout used for an expansion read left to right, with the edge connector facing you is...</p>
<blockquote>
<p dir="auto">5V - SDA/TX/CPU:GP0 - SCL/RX/CPU:GP1 - CTRL/GPU:GP29 - GND</p>
</blockquote>
<p dir="auto">A 4.7k resistor should be placed on the PicoVision connecting the GPU's GP29 to GND while the expansion side should place a 4.7k resistor between CTRL (GP29) and 5v.</p>
<p dir="auto">Although the plan is to eventually support various resistor values for different protocols / speeds, due to a design mistake this is not currently reliable. The PicoVision only has one ADC pin exposed (GPU:GP29) which is used as CTRL, however to reliably determine the value of the expansion resistor we also need a second ADC to act as a VREF (voltage reference) so we can determine the true voltage of our 5v line. As it stands if the 5v line is 5.2v or 4.9v due to load it may shift the value of our expansion voltage divider out of the expected range and the expansion may be incorrectly recognized. This is easily remidied with an external i2c ADC, but it is not currently implemented.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">License</h2><a id="user-content-license" aria-label="Permalink: License" href="#license"></a></p>
<p dir="auto">This software is licensed as MIT.</p>
<p dir="auto">App icons are from <a href="https://piiixl.itch.io/mega-1-bit-icons-bundle" rel="nofollow">PiiiXL on Itch.io</a> and are licensed <a href="https://creativecommons.org/licenses/by/4.0/deed.en" rel="nofollow">CC BY 4.0</a></p>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[The Ren'Py Visual Novel Engine (186 pts)]]></title>
            <link>https://www.renpy.org/</link>
            <guid>43132336</guid>
            <pubDate>Fri, 21 Feb 2025 20:09:32 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.renpy.org/">https://www.renpy.org/</a>, See on <a href="https://news.ycombinator.com/item?id=43132336">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>

    <p><img src="https://www.renpy.org/static/index-logo.png" alt=""></p><h2>What is Ren'Py?</h2>
    <p>Ren'Py is a visual novel engine ‚Äì used by thousands of creators from around the world ‚Äì
      that helps you use words, images, and sounds to tell interactive stories that run on computers and mobile devices.
      These can be both visual novels and life simulation games. The easy to learn script language allows
      anyone to efficiently write large visual novels, while its Python scripting is enough for complex
      simulation games.</p>
    <p>Ren'Py is open source and free for commercial use.</p>
    

    <h2>Where does it run?</h2>

    <div>

      <div>
        <p><img src="https://www.renpy.org/static/android-small.png" alt="">
        </p><p>
        Android 5.0+
      </p></div>

      <div>
        <p><img src="https://www.renpy.org/static/html5-small.png" alt="">
        </p><p>
        HTML5/Web Assembly (Beta)
      </p></div>

      <div>
        <p><img src="https://www.renpy.org/static/linux-small.png" alt="">
        </p><p>
        Linux x86_64/Arm
      </p></div>

      <div>
        <p><img src="https://www.renpy.org/static/windows-small.png" alt="">
        </p><p>
        Windows 7+
      </p></div>

      <div>
        <p><img src="https://www.renpy.org/static/osx-small.png" alt="">
        </p><p>
        Mac OS X 10.10+
      </p></div>

      <div>
        <p><img src="https://www.renpy.org/static/ios-small.png" alt="">
        </p><p>
        iOS 11+<br>
      </p></div>

    </div>

    <h2>Where do I get it?</h2>

    <div>

      <p>
          The latest official release of Ren'Py 8 is 8.3.4 "Second Star to the Right", released on
          December 8, 2024. Ren'Py 8 is recommended for all projects.
        </p>

      <p>
          The nightly fix version of Ren'Py is built every night, and contains fixes to the latest stable version. It isn't
          tested as well as the official release, but often has fixes that haven't made it through the release process.
        </p>




    </div>

    





    <div>
      <p>
        Ren'Py 7 is the legacy version of Ren'Py, to support ongoing projects that will be released in 2024.
        The latest version of Ren'Py 7 is 7.8.4 "Straight on Till Morning", released on
        December 8, 2024.
      </p>
      
    </div>

    <div>

        <h2>How do I keep in touch?</h2>

        <p>
          The best places to ask questions about Ren'Py are the <a href="http://lemmasoft.renai.us/forums/">Lemma Soft
            Forums</a>,
          the <a href="https://discord.gg/6ckxWYm">Ren'Py Discord</a>,
          and the <a href="http://webchat.freenode.net/?channels=renpy">#renpy IRC channel</a>.
        </p>

        <p>We make news about Ren'Py available on a number of social platforms:</p>

        <p>
          <b>Twitter:</b> You can follow Ren'Py's lead developer <a href="http://twitter.com/renpytom">@renpytom</a>
          for release announcements, development news, and general commentary on life.
        </p>

        <p>
          <b>Facebook:</b>
          We announce new releases on <a href="https://www.facebook.com/renpy">our Facebook page</a>.
        </p>


        <!--
    <p>
    <b>Deviantart:</b>
    Artists interested in Ren'Py and visual novels might want to visit our <a href="http://renpy.deviantart.com/">DeviantArt</a> group.
    </p>
     -->

      </div>

    <h2>Who is it sponsored by?</h2>

    



    <div>
      
<p>

Ariane Barnes


</p>

<p>

EriksBlue


</p>

<p>

Steven Shearer


</p>

<p>

Eris Discordia


</p>

<p>

Adam Wright


</p>

<p>

KEXBOY


</p>

<p>

Rachel


</p>

<p>

Felix Schmid


</p>

    </div>

    


    <p>
      To ask questions that aren't appropriate for a public forum, or to find a
      speaker for your visual novel-related conference or con, please <a href="https://www.renpy.org/email">contact us via email</a>.
    </p>

  </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Yocto, RockPi and SBOMs: Building modern embedded Linux images (132 pts)]]></title>
            <link>https://vpetersson.com/2025/02/21/yocto-rockpi-and-sboms.html</link>
            <guid>43131902</guid>
            <pubDate>Fri, 21 Feb 2025 19:34:37 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://vpetersson.com/2025/02/21/yocto-rockpi-and-sboms.html">https://vpetersson.com/2025/02/21/yocto-rockpi-and-sboms.html</a>, See on <a href="https://news.ycombinator.com/item?id=43131902">Hacker News</a></p>
<div id="readability-page-1" class="page"><article>
      <p><strong>TLDR</strong>: <em>I wanted to generate an up-to-date disk image for a Rock Pi 4 using Yocto that included CUPS and Docker to both get a better understanding of Yocto and test the new SBOM generation feature.</em></p>

<p>As with many single-board computers (SBCs) from China, the issue often isn‚Äôt the board itself but rather the software. RockPi from Radxa is no exception. If you go and download the <a href="https://wiki.radxa.com/Rock4/downloads">latest disk images</a> for this board, you will notice that they are all end-of-life (EoL). However, these boards are still great and work very well for many applications. This should be top of mind if you are building a product that uses any of these devices.</p>

<p>I wanted to use one of the RockPi 4 boards I had for a simple print server. It‚Äôs not a customer product, of course, but let‚Äôs assume it was. Since it has the option to add eMMC storage, I find it more reliable than Raspberry Pi (I know the Raspberry Pi 5 allows for proper storage). However, given that I neither trust the Radxa disk images nor did I want to set things up on an already EoL Linux distribution, I started doing some digging. As it turns out, the RockPi is supported in Yocto.</p>

<p>Say what you want about Raspberry Pi, but you can still download an up-to-date OS that runs on the Pi 1.
In this article, I will show you not only how to build a disk image with Yocto (in this case for the Rock Pi 4, but it can easily be adjusted for other boards), but we will also talk a bit about how Yocto generates SBOMs (hint: it‚Äôs really clever) and where to find your SBOMs.</p>
<h2 id="what-is-yocto-anyways">What is Yocto anyways?</h2>

<p>The Yocto Project is an open-source framework for building custom Linux distributions tailored to embedded systems. It provides a flexible, modular build system based on BitBake and OpenEmbedded, enabling developers to create highly optimized and reproducible Linux images for specific hardware. Yocto is widely used in industries like automotive, IoT, and networking due to its ability to support diverse architectures and long-term maintenance needs. With its layered architecture, extensive BSP support, and strong focus on customization, Yocto is a powerful tool for developers looking to build and maintain embedded Linux systems efficiently.</p>

<p>I‚Äôve toyed with it a few times over the years to build images for Raspberry Pis, but never really used it seriously. However, I recently crossed paths with some of the Yocto people in a CISA working group I‚Äôm co-chairing on <a href="https://github.com/CISA-SBOM-Community/SBOM-Generation">SBOM generation</a>. As it turns out, Yocto is very sophisticated when it comes to generating SBOMs, so I wanted to get some more up-to-date exposure to Yocto. Color me impressed. Not only did Yocto produce a Software Bill of Materials (SBOM) for me ‚Äì it did so without even asking me.</p>

<p>Since Yocto builds everything from source and is essentially a package manager, it is able to capture all the dependencies into an SBOM. Moreover, since Yocto maintains detailed information about every dependency, it is able to generate very high-quality SBOMs.</p>

<h2 id="key-yocto-terminology">Key Yocto Terminology</h2>

<p>Before we dive in, here are some key terms in Yocto that you probably want to understand:</p>

<ul>
  <li><strong>Poky</strong> ‚Äì The reference distribution of the Yocto Project, containing the OpenEmbedded build system, BitBake, and a set of metadata</li>
  <li><strong>Scarthgap</strong> ‚Äì The codename for the Yocto Project 5.0 release</li>
  <li><strong>Mickledore</strong> ‚Äì The codename for Yocto 4.2</li>
  <li><strong>Kirkstone</strong> ‚Äì The codename for Yocto 4.0, a long-term support (LTS) release</li>
  <li><strong>Dunfell</strong> ‚Äì The codename for Yocto 3.1, another LTS release</li>
  <li><strong>Layers</strong> ‚Äì Modular additions to the base Yocto version that provide extra functionality</li>
  <li><strong>BitBake</strong> ‚Äì The build tool used by Yocto to process recipes and generate images</li>
  <li><strong>OpenEmbedded (OE)</strong> ‚Äì The build framework Yocto is based on</li>
  <li><strong>Recipes</strong> (.bb files) ‚Äì Build instructions for individual packages or applications</li>
  <li><strong>BSP</strong> (Board Support Package) ‚Äì A set of metadata and configurations for specific hardware platforms</li>
</ul>

<h2 id="building-a-disk-image-with-yocto">Building a disk image with Yocto</h2>

<p>Before we build, you will need a pretty beefy server to build this image (or a lot of time). I‚Äôm using my <a href="https://vpetersson.com/2024/05/04/home-server-journey.html">home server</a>, and I think it took about an hour or two to build the initial version. Subsequent builds will be a lot faster due to cache.</p>

<p>I‚Äôve used an Ubuntu 24.04 VM to build my disk images, and you can find the base dependencies you need to install <a href="https://docs.yoctoproject.org/ref-manual/system-requirements.html">here</a>.</p>

<h3 id="lets-get-our-hands-dirty">Let‚Äôs get our hands dirty</h3>

<p>First, clone the repositories and set up the layers:</p>

<div><pre><code><span>$ </span>git clone <span>-b</span> scarthgap https://git.yoctoproject.org/poky
<span>$ </span><span>cd </span>poky
</code></pre></div>

<div><pre><code><span># Add layers</span>
<span>$ </span>git clone <span>-b</span> scarthgap git://git.yoctoproject.org/meta-arm
<span>$ </span>git clone <span>-b</span> scarthgap git://git.yoctoproject.org/meta-rockchip
<span>$ </span>git clone <span>-b</span> scarthgap git://git.openembedded.org/meta-openembedded
<span>$ </span>git clone <span>-b</span> scarthgap git://git.yoctoproject.org/meta-virtualization
</code></pre></div>

<div><pre><code><span>$ </span><span>source </span>oe-init-build-env
</code></pre></div>

<div><pre><code><span>$ </span>bitbake-layers add-layer ../meta-arm/meta-arm-toolchain
<span>$ </span>bitbake-layers add-layer ../meta-arm/meta-arm
<span>$ </span>bitbake-layers add-layer ../meta-rockchip
<span>$ </span>bitbake-layers add-layer ../meta-openembedded/meta-oe

<span># Add docker support</span>
<span>$ </span>bitbake-layers add-layer ../meta-openembedded/meta-python
<span>$ </span>bitbake-layers add-layer ../meta-openembedded/meta-networking
<span>$ </span>bitbake-layers add-layer ../meta-openembedded/meta-filesystems
<span>$ </span>bitbake-layers add-layer ../meta-virtualization
</code></pre></div>

<p>Next, adjust your <code>conf/local.conf</code> by appending these configurations:</p>

<div><pre><code>MACHINE <span>=</span> <span>"rock-pi-4b"</span>

INIT_MANAGER <span>=</span> <span>"systemd"</span>
DISTRO_FEATURES:append <span>=</span> <span>" virtualization wifi"</span>
DISTRO_FEATURES:remove <span>=</span> <span>" x11 wayland"</span>
CORE_IMAGE_EXTRA_INSTALL +<span>=</span> <span>"openssh cups cups-filters ghostscript qpdf vim docker e2fsprogs-resize2fs"</span>
</code></pre></div>

<p>Finally, build the image:</p>

<div><pre><code><span>$ </span>bitbake core-image-base
</code></pre></div>

<p>Note, if you‚Äôre building on Ubuntu 24.04, you might need to run:</p>

<div><pre><code><span>$ </span><span>sudo </span>apparmor_parser <span>-R</span> /etc/apparmor.d/unprivileged_userns
</code></pre></div>

<p>After the build completes, you can find your image here:</p>

<div><pre><code><span>$ </span><span>ls</span> <span>-lah</span> tmp/deploy/images/rock-pi-4b/core-image-base-rock-pi-4b.rootfs-<span>*</span>.wic
</code></pre></div>

<p>Flash this disk image and you should be good to go. Once it‚Äôs up and running, you should be able to SSH into the device using <code>root</code> and a blank password.</p>

<h2 id="on-updating">On updating</h2>

<p>It‚Äôs important to note that Yocto generates a disk image. By default, you cannot update this disk image by any other means than reflashing it (e.g., you can‚Äôt run ‚Äúapt update‚Äù). There are over-the-air (OTA) platforms that can be integrated into Yocto, such as <a href="https://mender.io/">Mender</a> and <a href="https://rauc.io/">RAUC</a>, but by default, you need to rebuild the image from scratch to update dependencies and patch vulnerabilities.</p>

<h2 id="finding-your-sboms">Finding Your SBOMs</h2>

<p>One of the cool features of Yocto is that it automatically generates SBOMs. You can find them in the deploy directory:</p>

<div><pre><code><span>$ </span><span>ls</span> <span>-lah</span> tmp/deploy/images/rock-pi-4b/<span>*</span>spdx<span>*</span>
<span>[</span>..]
</code></pre></div>

<p>You can extract the SPDX file with:</p>

<div><pre><code><span>$ </span><span>tar</span> <span>--zstd</span> <span>-xvf</span> <span>\</span>
    path/to/tmp/deploy/images/rock-pi-4b/core-image-base-rock-pi-4b.rootfs-<span>*</span>.spdx.tar.zst
</code></pre></div>

<p>Do note that this will generate a lot of files. You will find a file called <code>index.json</code> in there, which links to all other SBOMs using document linking.</p>

<p>(Check out my article <a href="https://sbomify.com/2025/02/21/mastering-sbom-generation-with-yocto/">Mastering SBOM Generation with Yocto</a> for more details on the SBOMs.)</p>

<h2 id="on-running-in-production">On running in production</h2>

<p>If you are intending to run this in production, please do not just copy the above. These images are configured for lab or test mode. Yocto is very well suited for production images, but you need to harden them and also have an OTA strategy in place. Alternatively, I can recommend <a href="https://www.balena.io/">Balena</a>, which uses Yocto under the hood and also supports the Rock Pi.</p>

<h2 id="future-improvements">Future improvements</h2>

<p>One limitation of the current disk image for Rock Pi is that you don‚Äôt have a functional TTY. You can SSH in, or you could use a serial console, but the regular TTY doesn‚Äôt work and I haven‚Äôt spent much time trying to figure out why. Also, the disk system doesn‚Äôt automatically expand to use all available space on the eMMC/SD.</p>

<p>Some things I‚Äôm planning to add in the future:</p>
<ul>
  <li>Add support for Tailscale (there‚Äôs a <a href="https://github.com/ChristophHandschuh/meta-tailscale">meta-tailscale</a> layer)</li>
  <li>Add support for auto disk expansion</li>
  <li>Add WiFi support</li>
</ul>

<h2 id="resources">Resources</h2>

<ul>
  <li><a href="https://docs.yoctoproject.org/">Yocto Project Documentation</a></li>
  <li><a href="https://kacperstapor.com/blog/24-11-2024/adding-docker-to-yocto-project">Adding Docker to Yocto Project</a></li>
  <li><a href="https://www.konsulko.com/rauc-on-rockchip">RAUC on Rockchip</a></li>
</ul>


      <div>
        <h4>Enjoyed this post? Check out my podcast!</h4>
        <p>If you found this interesting, you might enjoy <a href="https://vpetersson.com/podcast" onclick="if (!window.__cfRLUnblockHandlers) return false; trackEvent('Blog CTA', 'Click', 'Podcast Link', 'Yocto, RockPi and SBOMs: Building Modern Embedded Linux Images')" data-cf-modified-7e8e2357781c951b71c2fa6a-="">"Nerding Out with Viktor"</a> - my podcast where I dive deep into tech, entrepreneurship, and security with industry experts.</p>
        <div>
          <h5>Listen on:</h5>
          





        </div>
      </div>

      <i>Found an error or typo? File PR against <a href="https://github.com/vpetersson/vpetersson.com/tree/master/_posts/2025-02-21-yocto-rockpi-and-sboms.md" rel="nofollow">this file</a>.</i>
    </article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Suckless.org: software that sucks less (302 pts)]]></title>
            <link>https://suckless.org/</link>
            <guid>43131059</guid>
            <pubDate>Fri, 21 Feb 2025 18:27:00 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://suckless.org/">https://suckless.org/</a>, See on <a href="https://news.ycombinator.com/item?id=43131059">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="main">

<p>Home of <a href="https://dwm.suckless.org/">dwm</a>, <a href="https://tools.suckless.org/dmenu">dmenu</a> and
other quality software with a focus on simplicity, clarity, and frugality.</p>
<p>Read more about our <a href="https://suckless.org/philosophy">philosophy</a> and join us on the <a href="https://suckless.org/community">mailing
list</a>.</p>
<h2>News</h2>
<p><a href="https://suckless.org/atom.xml">Atom feed</a></p>
<h2>2024-11-26</h2>
<ul>
<li><a href="https://tools.suckless.org/x/svkbd/">svkbd 0.4.2</a> released: <a href="https://dl.suckless.org/tools/svkbd-0.4.2.tar.gz">download</a></li>
</ul>
<h2>2024-04-05</h2>
<ul>
<li><a href="https://st.suckless.org/">st 0.9.2</a> released: <a href="https://dl.suckless.org/st/st-0.9.2.tar.gz">download</a></li>
</ul>
<p>This reverts a commit and a regression with cursor move with wide glyphs, for
example with GNU readline.</p>
<h2>2024-03-20</h2>
<p>Below are some highlights of the changes for the recent releases of dmenu, dwm,
st and tabbed, see the git logs for all details:</p>
<p>General small Makefile improvements, rationale being: just be verbose and show
what is done: do not abstract/hide details from the user/developer.
Respect (more) the package manager and build system flags (CFLAGS, LDFLAGS, etc).</p>
<p><a href="https://git.suckless.org/dwm/log.html">dwm</a>:
</p><ul>
<li>Improvements to signal handling.</li>
<li>Fix: Avoid missing events when a keysym maps to multiple keycodes.</li>
</ul>

<p><a href="https://git.suckless.org/dmenu/log.html">dmenu</a>:
</p><ul>
<li>Reduce memory usage for reading the lines.</li>
<li>Fix: X11 BadMatch error when embedding on some windows.</li>
</ul>

<p><a href="https://git.suckless.org/st/log.html">st</a>:
</p><ul>
<li>Fix: bounds checks of dc.col.</li>
<li>Fix: buffer overflow when handling long composed input.</li>
<li>Ignore C1 control characters in UTF-8 mode.</li>
<li>Improvements to cell handling and wide characters.</li>
<li>Default config: decrease the default minlatency.</li>
<li><a href="https://git.suckless.org/st/log.html">Various other terminal fixes and compatibility improvements.</a></li>
</ul>

<p><a href="https://git.suckless.org/tabbed/log.html">tabbed</a>:
</p><ul>
<li>Fix: faulty zombie process reaping.</li>
<li>Improvements to signal handling.</li>
<li>Improve compatibility with compiling on older systems such as Slackware 11.</li>
</ul>

<p>Thanks to all contributors who submitted patches.</p>
<h2>2024-03-19</h2>
<ul>
<li><a href="https://tools.suckless.org/dmenu/">dmenu 5.3</a> released: <a href="https://dl.suckless.org/tools/dmenu-5.3.tar.gz">download</a></li>
<li><a href="https://dwm.suckless.org/">dwm 6.5</a> released: <a href="https://dl.suckless.org/dwm/dwm-6.5.tar.gz">download</a></li>
<li><a href="https://st.suckless.org/">st 0.9.1</a> released: <a href="https://dl.suckless.org/st/st-0.9.1.tar.gz">download</a></li>
<li><a href="https://tools.suckless.org/tabbed">tabbed 0.8</a> released: <a href="https://dl.suckless.org/tools/tabbed-0.8.tar.gz">download</a></li>
</ul>
<h2>2023-07-04</h2>
<p><a href="https://tools.suckless.org/slstatus">slstatus 1.0</a> released: <a href="https://dl.suckless.org/tools/slstatus-1.0.tar.gz">download</a></p>
<h2>2022-12-28</h2>
<p><a href="https://tools.suckless.org/lchat">lchat 1.0</a> released: <a href="https://dl.suckless.org/tools/lchat-1.0.tar.gz">download</a></p>
<h2>2022-11-02</h2>
<p><a href="https://libs.suckless.org/libgrapheme">libgrapheme 2.0.2</a> released: <a href="https://dl.suckless.org/libgrapheme/libgrapheme-2.0.2.tar.gz">download</a></p>
<h2>2022-10-08</h2>
<p><a href="https://libs.suckless.org/libgrapheme">libgrapheme 2.0.1</a> released: <a href="https://dl.suckless.org/libgrapheme/libgrapheme-2.0.1.tar.gz">download</a></p>
<h2>2022-10-06</h2>
<p><a href="https://libs.suckless.org/libgrapheme">libgrapheme 2.0.0</a> released: <a href="https://dl.suckless.org/libgrapheme/libgrapheme-2.0.0.tar.gz">download</a></p>
<h2>2022-10-04</h2>
<ul>
<li><a href="https://tools.suckless.org/dmenu/">dmenu 5.2</a> released: <a href="https://dl.suckless.org/tools/dmenu-5.2.tar.gz">download</a></li>
<li><a href="https://dwm.suckless.org/">dwm 6.4</a> released: <a href="https://dl.suckless.org/dwm/dwm-6.4.tar.gz">download</a></li>
<li><a href="https://tools.suckless.org/ii">ii 2.0</a> released: <a href="https://dl.suckless.org/tools/ii-2.0.tar.gz">download</a></li>
<li><a href="https://tools.suckless.org/sic">sic 1.3</a> released: <a href="https://dl.suckless.org/tools/sic-1.3.tar.gz">download</a></li>
<li><a href="https://tools.suckless.org/slock">slock 1.5</a> released: <a href="https://dl.suckless.org/tools/slock-1.5.tar.gz">download</a></li>
<li><a href="https://st.suckless.org/">st 0.9</a> released: <a href="https://dl.suckless.org/st/st-0.9.tar.gz">download</a></li>
<li><a href="https://tools.suckless.org/tabbed">tabbed 0.7</a> released: <a href="https://dl.suckless.org/tools/tabbed-0.7.tar.gz">download</a></li>
</ul>
<h2>2022-04-19</h2>
<p>Suckless now has a dark mode CSS style for its pages.
Surf also now has support for <a href="https://git.suckless.org/surf/commit/1f5b8f3bd1f37d4d3dc45d21285f34ef4752dbaa.html">dark mode</a>.</p>
<h2>2022-02-11</h2>
<p><a href="https://tools.suckless.org/dmenu/">dmenu 5.1</a> released: <a href="https://dl.suckless.org/tools/dmenu-5.1.tar.gz">download</a></p>
<h2>2022-01-07</h2>
<ul>
<li><a href="https://dwm.suckless.org/">dwm 6.3</a> released: <a href="https://dl.suckless.org/dwm/dwm-6.3.tar.gz">download</a></li>
<li><a href="https://tools.suckless.org/ii">ii 1.9</a> released: <a href="https://dl.suckless.org/tools/ii-1.9.tar.gz">download</a></li>
<li><a href="https://st.suckless.org/">st 0.8.5</a> released: <a href="https://dl.suckless.org/st/st-0.8.5.tar.gz">download</a></li>
<li><a href="https://tools.suckless.org/x/svkbd/">svkbd 0.4.1</a> released: <a href="https://dl.suckless.org/tools/svkbd-0.4.1.tar.gz">download</a></li>
</ul>
<h2>2021-12-22</h2>
<p><a href="https://libs.suckless.org/libgrapheme">libgrapheme 1.0.0</a> released: <a href="https://dl.suckless.org/libgrapheme/libgrapheme-1.0.0.tar.gz">download</a></p>
<h2>2021-07-30</h2>
<p><a href="https://tools.suckless.org/x/svkbd/">svkbd 0.4</a> released: <a href="https://dl.suckless.org/tools/svkbd-0.4.tar.gz">download</a></p>
<h2>2021-05-09</h2>
<p>On Tuesday, 2021-05-11 there will be scheduled maintenance of the suckless
servers. It's estimated this will take about 1 hour from about 21:00 to
22:00 UTC+02:00.</p>
<p>The mailinglist, website and source-code repositories will have some downtime.</p>
<p><strong>Update:</strong> the maintenance was finished at 2021-05-12 23:33 UTC+02:00.
P.S.: It didn't actually take 26h30, I just had forgotten to do it.</p>
<h2>2021-05-08</h2>
<p><a href="https://surf.suckless.org/">surf 2.1</a> released: <a href="https://dl.suckless.org/surf/surf-2.1.tar.gz">download</a></p>
<h2>2021-03-28</h2>
<p><a href="https://tools.suckless.org/x/svkbd/">svkbd 0.3</a> released: <a href="https://dl.suckless.org/tools/svkbd-0.3.tar.gz">download</a></p>
<h2>2021-03-28</h2>
<p>On Wednesday, 2021-03-31 there will be scheduled maintenance of the suckless
servers. It's estimated this will take about 2-3 hours from about 19:00 to
21:00 - 22:00 UTC+02:00.</p>
<p>The mailinglist, website and source-code repositories will have some downtime.</p>
<p><strong>Update:</strong> the maintenance was finished at 2021-03-31 19:10 UTC+02:00.</p>
<h2>2021-01-19</h2>
<p><a href="https://tools.suckless.org/scroll/">scroll 0.1</a> released: <a href="https://dl.suckless.org/tools/scroll-0.1.tar.gz">download</a></p>
<h2>2020-12-11</h2>
<p><a href="https://tools.suckless.org/x/svkbd/">svkbd 0.2.2</a> released: <a href="https://dl.suckless.org/tools/svkbd-0.2.2.tar.gz">download</a></p>
<h2>2020-09-18</h2>
<p><a href="https://tools.suckless.org/x/svkbd/">svkbd 0.2.1</a> released: <a href="https://dl.suckless.org/tools/svkbd-0.2.1.tar.gz">download</a></p>
<h2>2020-09-13</h2>
<p><a href="https://tools.suckless.org/x/svkbd/">svkbd 0.2</a> released: <a href="https://dl.suckless.org/tools/svkbd-0.2.tar.gz">download</a></p>
<h2>2020-09-02</h2>
<p><a href="https://tools.suckless.org/dmenu/">dmenu 5.0</a> released: <a href="https://dl.suckless.org/tools/dmenu-5.0.tar.gz">download</a></p>
<h2>2020-06-19</h2>
<p><a href="https://st.suckless.org/">st 0.8.4</a> released: <a href="https://dl.suckless.org/st/st-0.8.4.tar.gz">download</a></p>
<h2>2020-05-27</h2>
<p>The <a href="https://suckless.org/conferences/2020">slcon7</a> has been cancelled due to the 2019-nCoV
pandemic.</p>
<h2>2020-04-27</h2>
<p><a href="https://st.suckless.org/">st 0.8.3</a> released: <a href="https://dl.suckless.org/st/st-0.8.3.tar.gz">download</a></p>
<h2>2019-12-01</h2>
<p>On Wednesday, 2019-12-04 there will be scheduled maintenance of the suckless
servers. It's estimated this will take about 2-3 hours from about 19:00 to
21:00 - 22:00 UTC+01:00.</p>
<p>The mailinglist, website and source-code repositories will have some downtime.</p>
<p><strong>Update:</strong> the maintenance was finished at 2019-12-04 20:00 UTC+01:00.</p>
<h2>2019-04-04</h2>
<p>Registrations are now open for <a href="https://suckless.org/conferences/2019">slcon6</a> that will be held in
Bad Liebenzell, Germany on 2019-10-(04-06).</p>
<p>The CfP for interested participants will end on 2019-06-30.</p>
<h2>2019-03-30</h2>
<p>There is now a <a href="https://gunther.suckless.org/patches/">patch overview</a> tool to have a
quick overview of the patch status list. This list is generated each day from
the <a href="https://git.suckless.org/sites/">sites</a> repository. It checks if patches apply
cleanly in a normal patching manner. Of course it does not check patch
combinations.</p>
<ul>
<li><a href="https://suckless.org/hacking/">Hacking patches guidelines</a></li>
<li><a href="https://git.suckless.org/sites/file/testpatches.sh.html">Tool source-code</a></li>
</ul>
<p>Please keep the patches tidy and maintain or remove them.</p>
<h2>2019-02-09</h2>
<p><a href="https://st.suckless.org/">st 0.8.2</a> released: <a href="https://dl.suckless.org/st/st-0.8.2.tar.gz">download</a></p>
<p>This release has mostly bugfixes.</p>
<h2>2019-02-03</h2>
<ul>
<li><a href="https://dwm.suckless.org/">dwm 6.2</a> released: <a href="https://dl.suckless.org/dwm/dwm-6.2.tar.gz">download</a></li>
<li><a href="https://tools.suckless.org/dmenu/">dmenu 4.9</a> released: <a href="https://dl.suckless.org/tools/dmenu-4.9.tar.gz">download</a></li>
</ul>
<h2>2018-06-01</h2>
<p>The maintainance is completed. Let me know of any important things that are broken.
Internally we will keep tweaking the server configuration over the course of
time.</p>
<h2>2018-05-27</h2>
<p>There will be a scheduled server maintenance next Friday and Saturday, 2018-06-(01-02).
The migration to the new server will happen on these days and the git
repositories and mailing list will be frozen on the old (now current)
server.</p>
<h2>2018-04-11</h2>
<p><a href="https://tools.suckless.org/farbfeld/">farbfeld 4</a> released: <a href="https://dl.suckless.org/farbfeld/farbfeld-4.tar.gz">download</a></p>
<h2>2018-03-20</h2>
<p><a href="https://st.suckless.org/">st 0.8.1</a> released: <a href="https://dl.suckless.org/st/st-0.8.1.tar.gz">download</a></p>
<p>This release fixes some regressions introduced in the 0.8 release.</p>
<h2>2018-03-19</h2>
<p>Registrations for <a href="https://suckless.org/conferences/2018/">slcon5</a> are now open.</p>
<h2>2018-03-14</h2>
<ul>
<li><a href="https://tools.suckless.org/dmenu/">dmenu 4.8</a> released: <a href="https://dl.suckless.org/tools/dmenu-4.8.tar.gz">download</a></li>
<li><a href="https://st.suckless.org/">st 0.8</a> released: <a href="https://dl.suckless.org/st/st-0.8.tar.gz">download</a></li>
</ul>
<h2>2018-02-04</h2>
<p><a href="https://tools.suckless.org/ii">ii 1.8</a> released: <a href="https://dl.suckless.org/tools/ii-1.8.tar.gz">download</a></p>
<h2>2017-09-04</h2>
<p><a href="https://suckless.org/conferences/2017">suckless hackathon</a>: we met on Sep 1-3 2017 in W√ºrzburg, Germany.</p>
<h2>2017-09-04</h2>
<p><a href="https://tools.suckless.org/sent">sent 1</a> released: <a href="https://dl.suckless.org/tools/sent-1.tar.gz">download</a></p>
<h2>2017-08-30</h2>
<p>suckless.org now supports TLS using <a href="https://letsencrypt.org/">Let's Encrypt</a>.
Cloning git repos over HTTPS now works. Some links on the page have been
changed to allow both HTTP and HTTPS.</p>
<p>HSTS is not fully working yet. This will be fixed.</p>
<p>The IPv6 AAAA record was added and IPv6 is fully working now.</p>
<p>suckless has many subdomains, these should hopefully all work via TLS. If you
see a subdomain without a signed certificate please report it. If you find any
broken links on the wiki pages, these can be fixed by anyone.</p>
<h2>2017-07-03</h2>
<p>The suckless.org project is now hosted on a new server. All inactive accounts
have been removed during the relocation.</p>
<p>Please note that the new ECDSA key fingerprint is
SHA256:7DBXcYScmsxbv7rMJUJoJsY5peOrngD4QagiXX6MiQU.</p>
<h2>2017-05-06</h2>
<p><a href="https://tools.suckless.org/blind">blind 1.1</a> released:
<a href="https://dl.suckless.org/tools/blind-1.1.tar.gz">download</a></p>
<h2>2017-05-02</h2>
<p><a href="https://tools.suckless.org/dmenu">dmenu 4.7</a> released:
<a href="https://dl.suckless.org/tools/dmenu-4.7.tar.gz">download</a></p>
<h2>2017-04-14</h2>
<p><a href="https://tools.suckless.org/farbfeld/">farbfeld 3</a> released:
<a href="https://dl.suckless.org/farbfeld/farbfeld-3.tar.gz">download</a></p>
<h2>2017-03-28</h2>
<p><a href="https://surf.suckless.org/">surf</a> now uses webkit2 by default. The webkit1 version
is kept in the <a href="https://git.suckless.org/surf/log/?h=surf-webkit1">surf-webkit1</a>
branch. The ‚Äúmaster‚Äù branch doesn't exist anymore, HEAD is now
<a href="https://git.suckless.org/surf/log/">surf-webkit2</a>, so be sure to rebase your local
master commits onto surf-webkit1.</p>
<h2>2016-11-20</h2>
<p><a href="https://tools.suckless.org/slock">slock 1.4</a> released:
<a href="https://dl.suckless.org/tools/slock-1.4.tar.gz">download</a></p>
<h2>2016-09-26</h2>
<p>Videos of the <a href="https://suckless.org/conferences/2016">slcon 2016 talks</a> are now available.</p>
<h2>2016-08-24</h2>
<p><a href="https://suckless.org/conferences/2016">slcon3</a> preliminary schedule now published. If you want to
attend please register before: <strong>2016-09-01</strong>.</p>
<h2>2015-12-19</h2>
<p><a href="https://surf.suckless.org/">surf 0.7</a> released:
<a href="https://dl.suckless.org/surf/surf-0.7.tar.gz">download</a></p>
<h2>2015-11-25</h2>
<p><a href="https://tools.suckless.org/sent">sent 0.2</a> released:
<a href="https://dl.suckless.org/tools/sent-0.2.tar.gz">download</a></p>
<h2>2015-11-13</h2>
<p>Videos of the <a href="https://suckless.org/conferences/2015">slcon2 talks</a> are now available.</p>
<h2>2015-11-09</h2>
<p><a href="https://dwm.suckless.org/">dwm 6.1</a> released:
<a href="https://dl.suckless.org/dwm/dwm-6.1.tar.gz">download</a></p>
<h2>2015-09-23</h2>
<p>Kai and Anselm gave an interview about suckless.org on Randal Schwartz's <a href="https://twit.tv/shows/floss-weekly/episodes/355?autostart=false">FLOSS
Weekly show</a></p>
<h2>2015-07-07</h2>
<p><a href="https://st.suckless.org/">st 0.6</a> released:
<a href="https://dl.suckless.org/st/st-0.6.tar.gz">download</a></p>
<h2>2015-02-14</h2>
<p><a href="https://suckless.org/conferences/2015">slcon2</a> will be held in Budapest on 2015-10-(30-31).</p>
<p>The CfP for interested participants is now open and will end on 2015-04-30.</p>
<h2>2014-11-29</h2>
<p><a href="https://tools.suckless.org/x/lsw">lsw 0.3</a> released:
<a href="https://dl.suckless.org/tools/lsw-0.3.tar.gz">download</a></p>
<h2>2014-11-24</h2>
<p>There will be a
<a href="https://events.ccc.de/congress/2014/wiki/Assembly%3ASuckless">suckless assembly</a>
at the <a href="https://events.ccc.de/congress/2014">31C3</a>. The whole suckless
community is invited to come, meet and hack!</p>
<h2>2014-08-05</h2>
<p><a href="https://core.suckless.org/sinit">sinit 0.9.1</a> released:
<a href="https://dl.suckless.org/sinit/sinit-0.9.1.tar.gz">download</a></p>
<h2>2014-05-01</h2>
<p><a href="https://core.suckless.org/ubase">ubase 0.1</a> released:
<a href="https://dl.suckless.org/ubase/ubase-0.1.tar.gz">download</a></p>
<h2>2014-01-21</h2>
<p><a href="https://tools.suckless.org/tabbed">tabbed 0.6</a> released:
<a href="https://dl.suckless.org/tools/tabbed-0.6.tar.gz">download</a></p>
<h2>2013-06-16</h2>
<p><a href="https://tools.suckless.org/sic">sic 1.2</a> released:
<a href="https://dl.suckless.org/tools/sic-1.2.tar.gz">download</a></p>
<h2>2013-05-07</h2>
<p><a href="https://tools.suckless.org/x/xssstate">xssstate 1.1</a> released:
<a href="https://dl.suckless.org/tools/xssstate-1.1.tar.gz">download</a></p>
<h2>2013-05-06</h2>
<p><a href="https://tools.suckless.org/tabbed">tabbed 0.5</a> released:
<a href="https://dl.suckless.org/tools/tabbed-0.5.tar.gz">download</a></p>
<h2>2013-04-21</h2>
<p>We are glad to announce the <a href="https://suckless.org/conferences/2013">slcon 2013</a> programme.</p>
<h2>2012-11-29</h2>
<p>We are glad to announce the switch to git from mercurial in all of our
repositories. You can find them at <a href="https://git.suckless.org/">git.suckless.org</a> Many
thanks to 20h for his contribution!</p>
<h2>2012-10-28</h2>
<p><a href="https://tools.suckless.org/x/sprop">sprop 0.1</a> released:
<a href="https://dl.suckless.org/tools/sprop-0.1.tar.gz">download</a></p>
<h2>2012-10-14</h2>
<p>Today we heard a very sad news that our friend, contributor and philosophical
advisor Uriel has passed away peacefully. We will miss him a lot.</p>
<p><img src="https://suckless.org/uriel.png" alt="uriel"></p>
<p>RIP</p>
<h2>2011-05-14</h2>
<p>Anselm gave a talk about <strong>The 'suckless.org' universe</strong> at the <a href="http://www.linuxtag.org/">LinuxTag
2011</a> conference in Berlin.</p>
<h2>2011-01-31</h2>
<p><a href="https://tools.suckless.org/ii">ii 1.6</a> released (regression fix):
<a href="https://dl.suckless.org/tools/ii-1.6.tar.gz">download</a></p>
<h2>2010-06-04</h2>
<p><a href="https://tools.suckless.org/9base">9base-6</a> released:
<a href="https://dl.suckless.org/tools/9base-6.tar.gz">download</a></p>
<h2>2010-03-28</h2>
<p>We learned today that the previous wmii maintainer, who wasn't actively
involved since 2007, Denis Grelich,
<a href="https://web.archive.org/web/20140208043925/http://www.lmt.uni-saarland.de/de/aktuelles/grelich.html">died on 2010-03-12</a>.
We thank him for his work. Rest in peace.</p>
<h2>2010-03-07</h2>
<p>We applied as a mentoring organisation for GSoC 2010. See our <a href="https://suckless.org/project_ideas">project ideas
for GSoC 2010</a> page for further details.</p>
<h2>2010-02-13</h2>
<p>Some of us will visit <a href="http://chemnitzer.linux-tage.de/2010/">CLT2010</a>. Anselm
will give a
<a href="http://chemnitzer.linux-tage.de/2010/vortraege/detail.html?idx=308">talk</a>
about stali on the second day of CLT2010 at 17:00.</p>
<h2>2009-12-28</h2>
<p>There was a small community meeting in Berlin! Thanks to all attendees.</p>
<h2>2008-08-02</h2>
<p><a href="https://tools.suckless.org/x/wmname">wmname 0.1</a> released:
<a href="https://dl.suckless.org/tools/wmname-0.1.tar.gz">download</a></p>
<h2>2008-07-29</h2>
<p><a href="https://tools.suckless.org/x/sselp">sselp 0.2</a> released:
<a href="https://dl.suckless.org/tools/sselp-0.2.tar.gz">download</a></p>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[NASA's James Webb Space Telescope faces potential 20% budget cut (149 pts)]]></title>
            <link>https://www.space.com/space-exploration/james-webb-space-telescope/nasa-james-webb-space-telescope-faces-20-percent-budget-cuts</link>
            <guid>43131045</guid>
            <pubDate>Fri, 21 Feb 2025 18:25:16 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.space.com/space-exploration/james-webb-space-telescope/nasa-james-webb-space-telescope-faces-20-percent-budget-cuts">https://www.space.com/space-exploration/james-webb-space-telescope/nasa-james-webb-space-telescope-faces-20-percent-budget-cuts</a>, See on <a href="https://news.ycombinator.com/item?id=43131045">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-widget-type="contentparsed" id="content">

<section>
<div>
<div>
<picture data-new-v2-image="true">
<source type="image/webp" srcset="https://cdn.mos.cms.futurecdn.net/qpAkShLp2kqGCkfGqGBMuf-1920-80.jpg.webp 1920w, https://cdn.mos.cms.futurecdn.net/qpAkShLp2kqGCkfGqGBMuf-1200-80.jpg.webp 1200w, https://cdn.mos.cms.futurecdn.net/qpAkShLp2kqGCkfGqGBMuf-1024-80.jpg.webp 1024w, https://cdn.mos.cms.futurecdn.net/qpAkShLp2kqGCkfGqGBMuf-970-80.jpg.webp 970w, https://cdn.mos.cms.futurecdn.net/qpAkShLp2kqGCkfGqGBMuf-650-80.jpg.webp 650w, https://cdn.mos.cms.futurecdn.net/qpAkShLp2kqGCkfGqGBMuf-480-80.jpg.webp 480w, https://cdn.mos.cms.futurecdn.net/qpAkShLp2kqGCkfGqGBMuf-320-80.jpg.webp 320w" sizes="(min-width: 1000px) 600px, calc(100vw - 40px)">
<img src="https://cdn.mos.cms.futurecdn.net/qpAkShLp2kqGCkfGqGBMuf-1200-80.jpg" alt="a spacecraft with a large gold hexagon on top in deep space" srcset="https://cdn.mos.cms.futurecdn.net/qpAkShLp2kqGCkfGqGBMuf-1920-80.jpg 1920w, https://cdn.mos.cms.futurecdn.net/qpAkShLp2kqGCkfGqGBMuf-1200-80.jpg 1200w, https://cdn.mos.cms.futurecdn.net/qpAkShLp2kqGCkfGqGBMuf-1024-80.jpg 1024w, https://cdn.mos.cms.futurecdn.net/qpAkShLp2kqGCkfGqGBMuf-970-80.jpg 970w, https://cdn.mos.cms.futurecdn.net/qpAkShLp2kqGCkfGqGBMuf-650-80.jpg 650w, https://cdn.mos.cms.futurecdn.net/qpAkShLp2kqGCkfGqGBMuf-480-80.jpg 480w, https://cdn.mos.cms.futurecdn.net/qpAkShLp2kqGCkfGqGBMuf-320-80.jpg 320w" sizes="(min-width: 1000px) 600px, calc(100vw - 40px)" data-original-mos="https://cdn.mos.cms.futurecdn.net/qpAkShLp2kqGCkfGqGBMuf.jpg" data-pin-media="https://cdn.mos.cms.futurecdn.net/qpAkShLp2kqGCkfGqGBMuf.jpg" data-pin-nopin="true" fetchpriority="high">
</picture>
</div>
<figcaption>
<span>(Image credit: dima_zel/iStock/Getty Images)</span>
</figcaption>
</div>

<div id="article-body">
<p>The scientists behind NASA's largest and most powerful space telescope ever built are bracing for potentially crippling budget cuts, and the observatory is only halfway through its primary mission.</p><p>The team overseeing NASA's <a data-analytics-id="inline-link" href="https://www.space.com/21925-james-webb-space-telescope-jwst.html" data-before-rewrite-localise="https://www.space.com/21925-james-webb-space-telescope-jwst.html"><u>James Webb Space Telescope</u></a> (JWST) has been directed to prepare for up to 20% in budget cuts that would touch on every aspect of the flagship observatory's operations, which are managed by the Space Telescope Science Institute (STScI) in Maryland. The potential cut comes even as the space observatory is more in demand than ever before, with astronomers requesting the equivalent of nine years' worth of Webb observing time in one operational year.</p><p>"NASA is having budget constraints across the entire board, so the institute is being asked to consider a significant ‚Äî about 20% ‚Äî cut to our operational budget for the mission starting later this year," Tom Brown, who leads the Webb mission office at STScI, told a crowd of scientists last month at the 245th American Astronomical Society (AAS) meeting in National Harbor, Maryland. "So the impacts of that, if it comes to pass, pretty much cut across the entire mission."</p><p>NASA's <a data-analytics-id="inline-link" href="https://www.space.com/nasa-white-house-2025-budget-request" data-before-rewrite-localise="https://www.space.com/nasa-white-house-2025-budget-request"><u>$25.4 billion budget request for 2025</u></a> set aside $317 million to fund the Webb space telescope, as well as the <a data-analytics-id="inline-link" href="https://www.space.com/15892-hubble-space-telescope.html" data-before-rewrite-localise="https://www.space.com/15892-hubble-space-telescope.html"><u>Hubble Space Telescope</u></a> and <a data-analytics-id="inline-link" href="https://www.space.com/18669-chandra-x-ray-observatory.html" data-before-rewrite-localise="https://www.space.com/18669-chandra-x-ray-observatory.html"><u>Chandra X-ray Observatory</u></a> that together comprise NASA's currently operational "Great Observatories." The Hubble Telescope program is facing a potential 20% budget cut of its own, <a data-analytics-id="inline-link" href="https://spacenews.com/hubble-budget-cuts-could-impact-science-and-mission-operations/"><u>according to SpaceNews</u></a>. And Chandra <a data-analytics-id="inline-link" href="https://www.space.com/chandra-x-ray-observatory-nasa-fy2025-budget" data-before-rewrite-localise="https://www.space.com/chandra-x-ray-observatory-nasa-fy2025-budget"><u>is facing the end of its mission</u></a>, with NASA's 2025 budget request including plans to wind down operations, with its budget dropping from $41.1 million this year to just $5.2 million in 2029.</p><p>But unlike Hubble, which turns 35 this spring, and Chandra, which launched in 1999, Webb is in its prime, approaching the midpoint of a primary 10-year mission. It could last at least 20 years or more, NASA officials have said. The mission is an international partnership between NASA, the European Space Agency and the Canadian Space Agency.</p><p>"Frankly, this mission works far better than, really, most folks expected it to, you know," Brown said during the Webb town hall event on Jan. 15 at the AAS conference. "It's extremely worrisome that, while we're in the middle of the prime mission, we're also maybe looking at significant budget cuts."</p><figure data-bordeaux-image-check=""><div><p><picture><source type="image/webp" srcset="https://cdn.mos.cms.futurecdn.net/uQQra7dvnHyLzW64uprJSg-320-80.png.webp 320w, https://cdn.mos.cms.futurecdn.net/uQQra7dvnHyLzW64uprJSg-480-80.png.webp 480w, https://cdn.mos.cms.futurecdn.net/uQQra7dvnHyLzW64uprJSg-650-80.png.webp 650w, https://cdn.mos.cms.futurecdn.net/uQQra7dvnHyLzW64uprJSg-970-80.png.webp 970w, https://cdn.mos.cms.futurecdn.net/uQQra7dvnHyLzW64uprJSg-1024-80.png.webp 1024w, https://cdn.mos.cms.futurecdn.net/uQQra7dvnHyLzW64uprJSg-1200-80.png.webp 1200w" sizes="(min-width: 1000px) 970px, calc(100vw - 40px)"><img src="https://cdn.mos.cms.futurecdn.net/uQQra7dvnHyLzW64uprJSg-320-80.png" alt="The galaxy GN-z11 as seen by Hubble (inset) an illustration of a feeding black hole" srcset="https://cdn.mos.cms.futurecdn.net/uQQra7dvnHyLzW64uprJSg-320-80.png 320w, https://cdn.mos.cms.futurecdn.net/uQQra7dvnHyLzW64uprJSg-480-80.png 480w, https://cdn.mos.cms.futurecdn.net/uQQra7dvnHyLzW64uprJSg-650-80.png 650w, https://cdn.mos.cms.futurecdn.net/uQQra7dvnHyLzW64uprJSg-970-80.png 970w, https://cdn.mos.cms.futurecdn.net/uQQra7dvnHyLzW64uprJSg-1024-80.png 1024w, https://cdn.mos.cms.futurecdn.net/uQQra7dvnHyLzW64uprJSg-1200-80.png 1200w" sizes="(min-width: 1000px) 970px, calc(100vw - 40px)" loading="lazy" data-original-mos="https://cdn.mos.cms.futurecdn.net/uQQra7dvnHyLzW64uprJSg.png" data-pin-media="https://cdn.mos.cms.futurecdn.net/uQQra7dvnHyLzW64uprJSg.png"></picture></p></div><figcaption itemprop="caption description"><span>The James Webb Space Telescope has made mind-boggling discoveriesin its first four years, like this one of the galaxy GN-z11 with the oldest and farthest black hole ever seen. </span><span itemprop="copyrightHolder">(Image credit: NMASA, ESA, P. Oesch (Yale University), G. Brammer (STScI), P. van Dokkum (Yale University), and G. Illingworth (University of California, Santa Cruz) (Inset) Robert Lea)</span></figcaption></figure><p>The $10 billion Webb space telescope survived a tumultuous development process, one that included cost overruns and technical delays that nearly killed the observatory before it ever flew. Lawmakers with the House Appropriations Committee <a data-analytics-id="inline-link" href="https://www.space.com/12187-nasa-budget-bill-cancels-space-telescope-house.html" data-before-rewrite-localise="https://www.space.com/12187-nasa-budget-bill-cancels-space-telescope-house.html"><u>proposed cancelling the mission</u></a> in 2011, a decade before <a data-analytics-id="inline-link" href="https://www.space.com/nasa-james-webb-space-telescope-launch-success" data-before-rewrite-localise="https://www.space.com/nasa-james-webb-space-telescope-launch-success"><u>Webb's Christmas Day launch in 2021</u></a>, only to back down after backlash from scientists and influential politicians defending the observatory.</p><div data-hydrate="true" id="slice-container-newsletterForm-articleInbodyContent-rTLnXKeQtgwCKwvYQVHpC6"><section><p>Breaking space news, the latest updates on rocket launches, skywatching events and more!</p></section></div><p>Since its 2021 launch, the Webb space telescope <a data-analytics-id="inline-link" href="https://www.space.com/james-webb-space-telescope-first-year-astronomers-in-tears" data-before-rewrite-localise="https://www.space.com/james-webb-space-telescope-first-year-astronomers-in-tears"><u>has outmatched even the most optimistic predictions</u></a> for its performance. Its infrared optics have looked deep into the universe's past, observed distant galaxies and exoplanets, and even peered at our own local solar system planets closer to home.</p><p>"In a nutshell, it is truly fulfilling its promise," Macarena Garcia Marin, STScI's Webb project scientist, said during the same town hall event. "Across every field, JWST is truly delivering cutting-edge science."</p><p>Some of Webb's budget challenges stem from its operational costs, which were set "idealistically low" in 2011 when the observatory was saved from cancellation. Those costs, coupled with inflation rates that were much higher than expected and less flexibility in NASA's budget, have also contributed, Brown said.</p><p>According to a <a data-analytics-id="inline-link" href="https://www.stsci.edu/files/live/sites/www/files/home/jwst/news-events/events/2025/_documents/0125-jwst-townhall-mission-status-brown.pdf"><u>presentation by Brown</u></a>, a 20% cut to Webb's operational budget would definitely affect how much science the telescope could perform. The impacts would be felt across teams that review proposals for observing targets, data analysis, observatory efficiencies, and anomaly resolution when something goes wrong, not to mention the need to engage with the scientific community and public on Webb's science results.</p><p>"It's a huge cut. That's not like kind of trying to nibble away at the edges," Brown told Space.com. "That impacts everything across the board, all the way up to how many modes we're offering to the observers."</p><p>Those impacts, Brown said, would likely be felt for the first time in October, when the next fiscal year begins.</p><p>Brown's comments at the Webb observatory town hall at AAS came just before the <a data-analytics-id="inline-link" href="https://www.space.com/space-exploration/we-will-pursue-our-manifest-destiny-into-the-stars-president-trump-wants-astronauts-to-raise-the-american-flag-on-mars" data-before-rewrite-localise="https://www.space.com/space-exploration/we-will-pursue-our-manifest-destiny-into-the-stars-president-trump-wants-astronauts-to-raise-the-american-flag-on-mars"><u>inauguration of President Donald Trump</u></a>, who in subsequent weeks created the Department of Government Efficiency headed by SpaceX CEO <a data-analytics-id="inline-link" href="https://www.space.com/18849-elon-musk.html" data-before-rewrite-localise="https://www.space.com/18849-elon-musk.html"><u>Elon Musk</u></a> to reduce government spending. DOGE, as it's known, has worked to dismantle some entire agencies, like the U.S. Agency for International Development, which provides aid to other countries during disasters and other emergencies, while also overseeing massive cuts to the federal workforce. Nearly <a data-analytics-id="inline-link" href="https://www.space.com/the-universe/earth/over-1-000-nasa-employees-saved-from-dismissal-as-trump-downsizes-federal-workforce" data-before-rewrite-localise="https://www.space.com/the-universe/earth/over-1-000-nasa-employees-saved-from-dismissal-as-trump-downsizes-federal-workforce"><u>1,000 NASA jobs could be eliminated</u></a>, though they appear to have been saved from layoffs earlier this week.</p><p>Trump has nominated American billionaire entrepreneur <a data-analytics-id="inline-link" href="https://www.space.com/space-exploration/private-spaceflight/who-is-jared-isaacman-trumps-pick-for-nasa-chief" data-before-rewrite-localise="https://www.space.com/space-exploration/private-spaceflight/who-is-jared-isaacman-trumps-pick-for-nasa-chief"><u>Jared Isaacman</u></a>, who has flown in orbit twice on private SpaceX missions he financed himself, to serve as the next NASA administrator, though Isaacman has yet to be confirmed. The agency is currently being led by Acting Administrator <a data-analytics-id="inline-link" href="https://www.space.com/space-exploration/missions/who-is-janet-petro-trumps-pick-for-acting-nasa-administrator" data-before-rewrite-localise="https://www.space.com/space-exploration/missions/who-is-janet-petro-trumps-pick-for-acting-nasa-administrator"><u>Janet Petro</u></a>, former director of the agency's Kennedy Space Center in Florida.</p>
</div>
<p><em><a href="https://forums.space.com/">Join our Space Forums</a> to keep talking space on the latest missions, night sky and more! And if you have a news tip, correction or comment, let us know at: <a href="mailto:community@space.com">community@space.com.</a></em></p>

<div id="slice-container-authorBio-rTLnXKeQtgwCKwvYQVHpC6"><p>Tariq is the Editor-in-Chief of <a href="https://www.space.com/" data-before-rewrite-localise="https://www.space.com/">Space.com</a> and joined the team in 2001, first as an intern and staff writer, and later as an editor. He covers human spaceflight, exploration and space science, as well as skywatching and entertainment. He became Space.com's Managing Editor in 2009 and Editor-in-Chief in 2019. Before joining Space.com, Tariq was a staff reporter for The Los Angeles Times covering education and city beats in La Habra, Fullerton and Huntington Beach. In October 2022, <a href="https://www.nscfl.org/kolcum-award/" target="_blank">Tariq received the Harry Kolcum Award</a> for excellence in space reporting from the National Space Club Florida Committee. He is also an Eagle Scout (yes, he has the Space Exploration merit badge) and went to Space Camp four times as a kid and a fifth time as an adult. He has journalism degrees from the University of Southern California and New York University. You can find Tariq at Space.com and as the co-host to the <a href="https://twit.tv/shows/this-week-in-space" target="_blank">This Week In Space podcast</a> with space historian Rod Pyle on the <a href="https://twit.tv/" target="_blank">TWiT network</a>. To see his latest project, you can follow Tariq on&nbsp;Twitter <a href="https://twitter.com/tariqjmalik" target="_blank">@tariqjmalik</a>.</p></div>

</section>



<div data-test-id="more-about">
<p>More about science astronomy</p>


</div>

<div id="slice-container-relatedArticles"><p><h3>Most Popular</h3></p></div>








</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[I think Yann Lecun was right about LLMs (but perhaps only by accident) (116 pts)]]></title>
            <link>https://substack.com/home/post/p-157633768</link>
            <guid>43131022</guid>
            <pubDate>Fri, 21 Feb 2025 18:23:14 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://substack.com/home/post/p-157633768">https://substack.com/home/post/p-157633768</a>, See on <a href="https://news.ycombinator.com/item?id=43131022">Hacker News</a></p>
<div id="readability-page-1" class="page"><div tab="[object Object]"><p><h3 translated="">The app for independent voices</h3></p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Richard Feynman's blackboard at the time of his death (1988) (396 pts)]]></title>
            <link>https://digital.archives.caltech.edu/collections/Images/1.10-29/</link>
            <guid>43131017</guid>
            <pubDate>Fri, 21 Feb 2025 18:22:59 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://digital.archives.caltech.edu/collections/Images/1.10-29/">https://digital.archives.caltech.edu/collections/Images/1.10-29/</a>, See on <a href="https://news.ycombinator.com/item?id=43131017">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
      <article><p>These digitized collections are accessible for purposes of education and research. Due to the nature of archival collections, archivists at the Caltech Archives and Special Collections are not always able to identify copyright and rights of privacy, publicity, or trademark. We are eager to <a href="mailto:archives@caltech.edu">hear from any rights holders</a>, so that we may obtain accurate information. Upon request, we‚Äôll remove material from public view while we address a rights issue.</p></article>
    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Lithium-sulfur battery retains 80% charge capacity after 25,000 cycles (111 pts)]]></title>
            <link>https://techxplore.com/news/2025-01-lithium-sulfur-battery-retains-capacity.html</link>
            <guid>43130923</guid>
            <pubDate>Fri, 21 Feb 2025 18:15:38 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://techxplore.com/news/2025-01-lithium-sulfur-battery-retains-capacity.html">https://techxplore.com/news/2025-01-lithium-sulfur-battery-retains-capacity.html</a>, See on <a href="https://news.ycombinator.com/item?id=43130923">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
									    
<div data-thumb="https://scx1.b-cdn.net/csz/news/tmb/2025/lithium-sulfur-battery.jpg" data-src="https://scx2.b-cdn.net/gfx/news/hires/2025/lithium-sulfur-battery.jpg" data-sub-html="The experimental evidence of diffusion of I<sub>2</sub> along the SE particle surface and the occurrence of the reaction between I<sub>2</sub> as redox mediator and Li<sub>2</sub>S. Credit: <i>Nature</i> (2025). DOI: 10.1038/s41586-024-08298-9">
        <figure>
            <img src="https://scx1.b-cdn.net/csz/news/800a/2025/lithium-sulfur-battery.jpg" alt="Lithium-sulfur battery retains 80% charge capacity after 25,000 cycles" title="The experimental evidence of diffusion of I2 along the SE particle surface and the occurrence of the reaction between I2 as redox mediator and Li2S. Credit: Nature (2025). DOI: 10.1038/s41586-024-08298-9" width="800" height="530">
             <figcaption>
                The experimental evidence of diffusion of I<sub>2</sub> along the SE particle surface and the occurrence of the reaction between I<sub>2</sub> as redox mediator and Li<sub>2</sub>S. Credit: <i>Nature</i> (2025). DOI: 10.1038/s41586-024-08298-9
            </figcaption>        </figure>
    </div><p>An international team of engineers and materials scientists has developed a lithium-sulfur battery capable of retaining 80% of its charge capacity after 25,000 cycles. Their paper is <a href="https://www.nature.com/articles/s41586-024-08298-9" target="_blank">published</a> in the journal <i>Nature</i>.</p>

                                        
                                              
                                        
                                                                                    <p>To make batteries smaller and lighter, engineers continually look for new materials. Such efforts tend to focus on the electrodes where lithium is held by other materials. Finding a better material to hold the lithium could result in an overall lighter and more compact battery.</p>
<p>One of the more promising materials is sulfur, due to its quality, abundance and low cost. Unfortunately, some of sulfur's reactions with lithium lead to ion loss, and worse, it tends to expand, leading to degradation and a short battery life.</p>
<p>In this new study, the research team working in China found a way around such problems and built a battery that can hold up longer than other batteries over thousands of recharge cycles.</p>
<p>The approach uses sulfur to create a solid electrode‚Äîits porous atomic structure allows for ion diffusion without movement of intermediaries. To create the electrode, the team created a glass-like mixture made from <a href="https://techxplore.com/tags/sulfur/" rel="tag">sulfur</a>, boron, lithium, phosphorus and iodine. The latter proved to be the key; it helped speed the movement of electrons through the <a href="https://techxplore.com/tags/redox+reactions/" rel="tag">redox reactions</a>, which led to faster reaction speeds.</p>

<div data-thumb="https://scx1.b-cdn.net/csz/news/tmb/2025/lithium-sulfur-battery-1.jpg" data-src="https://scx2.b-cdn.net/gfx/news/hires/2025/lithium-sulfur-battery-1.jpg" data-sub-html="Fundamental concept and characterization of the lithium thioborophosphate iodide glass-phase solid electrolytes. Credit: <i>Nature</i> (2025). DOI: 10.1038/s41586-024-08298-9">
        <figure>
            <img src="https://scx1.b-cdn.net/csz/news/800a/2025/lithium-sulfur-battery-1.jpg" alt="Lithium-sulfur battery retains 80% charge capacity after 25,000 cycles" title="Fundamental concept and characterization of the lithium thioborophosphate iodide glass-phase solid electrolytes. Credit: Nature (2025). DOI: 10.1038/s41586-024-08298-9">
             <figcaption>
                Fundamental concept and characterization of the lithium thioborophosphate iodide glass-phase solid electrolytes. Credit: <i>Nature</i> (2025). DOI: 10.1038/s41586-024-08298-9
            </figcaption>        </figure>
    </div>
<p>The result was a battery that could be charged quickly, even when exposed to <a href="https://techxplore.com/tags/high+temperatures/" rel="tag">high temperatures</a>. But more importantly, the battery was capable of retaining an 80% charge capacity after undergoing 25,000 charge/recharge cycles‚Äîa noticeable improvement over typical lithium-ion batteries, which tend to degrade after just 1,000 cycles.</p>
<p>The researchers suggest more work is required to improve the <a href="https://techxplore.com/tags/energy+density/" rel="tag">energy density</a> and perhaps to find other materials to use for the mix to ensure a low-weight battery.</p>

                                                                                
                                        											<div>
																								<p><strong>More information:</strong>
												Huimin Song et al, All-solid-state Li‚ÄìS batteries with fast solid‚Äìsolid sulfur reaction, <i>Nature</i> (2025). <a data-doi="1" href="https://dx.doi.org/10.1038/s41586-024-08298-9" target="_blank">DOI: 10.1038/s41586-024-08298-9</a>
																								
																								</p>
																							</div>
                                        											
										                                                                                    <p>
                                                ¬© 2025 Science X Network
                                            </p>
                                                                                
                                        <!-- print only -->
                                        <div>
                                            <p><strong>Citation</strong>:
                                                Lithium-sulfur battery retains 80% charge capacity after 25,000 cycles (2025, January 20)
                                                retrieved 22 February 2025
                                                from https://techxplore.com/news/2025-01-lithium-sulfur-battery-retains-capacity.html
                                            </p>
                                            <p>
                                            This document is subject to copyright. Apart from any fair dealing for the purpose of private study or research, no
                                            part may be reproduced without the written permission. The content is provided for information purposes only.
                                            </p>
                                        </div>
                                        
									</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[SWE-Bench tainted by answer leakage; real pass rates significantly lower (335 pts)]]></title>
            <link>https://arxiv.org/abs/2410.06992</link>
            <guid>43130732</guid>
            <pubDate>Fri, 21 Feb 2025 17:59:48 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://arxiv.org/abs/2410.06992">https://arxiv.org/abs/2410.06992</a>, See on <a href="https://news.ycombinator.com/item?id=43130732">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="content-inner">
    
    
                
    <p><a href="https://arxiv.org/pdf/2410.06992">View PDF</a>
    <a href="https://arxiv.org/html/2410.06992v2">HTML (experimental)</a></p><blockquote>
            <span>Abstract:</span>Large Language Models (LLMs) in Software Engineering (SE) can offer assistance for coding. To facilitate a rigorous evaluation of LLMs in practical coding contexts, Carlos et al. introduced the SWE-bench dataset, which comprises 2,294 real-world GitHub issues and their corresponding pull requests, collected from 12 widely used Python repositories. Several impressive LLM-based toolkits recently are developed and evaluated on this dataset. However, a systematic evaluation of the quality of SWE-bench remains missing. In this paper, we addressed this gap by presenting an empirical analysis of the SWE-bench dataset. We conducted a manual screening of instances where SWEAgent + GPT-4 successfully resolved issues by comparing the model-generated patches with the actual pull requests. SWE-Agent+GPT-4 was at the top of SWE-bench leaderboard during the time of our study. Our analysis reveals some critical issues with the SWE-bench dataset: 1) 32.67% of the successful patches involve cheating as the solutions were directly provided in the issue report or the comments. We refer to as solution leakage problem. 2) 31.08% of the passed patches are suspicious patches due to weak test cases, i.e., the tests were not adequate to verify the correctness of a patch. When we filtered out these problematic issues, the resolution rate of SWE-Agent+GPT-4 dropped from 12.47% to 3.97%. We also observed that the same data quality issues also exist in the two variants of SWE-bench, i.e., SWE-bench Lite and SWE-Bench Verified. In addition, over 94% of the issues were created before LLM's knowledge cutoff dates, posing potential data leakage issues.
    </blockquote>

    <!--CONTEXT-->
    
  </div><div>
      <h2>Submission history</h2><p> From: Song Wang [<a href="https://arxiv.org/show-email/8151c019/2410.06992" rel="nofollow">view email</a>]      <br>            <strong><a href="https://arxiv.org/abs/2410.06992v1" rel="nofollow">[v1]</a></strong>
        Wed, 9 Oct 2024 15:38:53 UTC (3,863 KB)<br>
    <strong>[v2]</strong>
        Thu, 10 Oct 2024 13:13:09 UTC (1,714 KB)<br>
</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Why Ruby on Rails still matters (388 pts)]]></title>
            <link>https://www.contraption.co/rails-versus-nextjs/</link>
            <guid>43130546</guid>
            <pubDate>Fri, 21 Feb 2025 17:46:15 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.contraption.co/rails-versus-nextjs/">https://www.contraption.co/rails-versus-nextjs/</a>, See on <a href="https://news.ycombinator.com/item?id=43130546">Hacker News</a></p>
Couldn't get https://www.contraption.co/rails-versus-nextjs/: Error: timeout of 10000ms exceeded]]></description>
        </item>
        <item>
            <title><![CDATA[The Profitable Startup (137 pts)]]></title>
            <link>https://linear.app/blog/the-profitable-startup</link>
            <guid>43130480</guid>
            <pubDate>Fri, 21 Feb 2025 17:41:04 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://linear.app/blog/the-profitable-startup">https://linear.app/blog/the-profitable-startup</a>, See on <a href="https://news.ycombinator.com/item?id=43130480">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>For years, startups have been taught to prioritize growth over everything else. Profitability was seen as unambitious or even wrong ‚Äì something to worry about when you hit scale. Why focus on profits when money and valuations were easy to come by?</p><p>But that thinking was always flawed.</p><p>Profitability isn't unambitious; it's controlling your own destiny. It means you don't have to rely on investors for survival. It means you can focus on your unaltered vision and mission. And it means you as a founder decide the pace of growth. And once you experience it, it's hard to imagine doing things any other way.</p><p><a href="https://paulgraham.com/ramenprofitable.html">Paul Graham famously wrote about "ramen profitability"</a> ‚Äì the point where a founding team could survive without external funding. He argued this made startups more attractive to investors, showing they could get customers to pay, were serious about building valuable products, and were disciplined with expenses.</p><p>Graham wrote his essay in 2009. I‚Äôd argue that we now live in a world where it‚Äôs not just easier to get ramen profitable, but traditionally profitable ‚Äì while also growing fast.</p><p>At Linear we didn't set out to be profitable but kind of stumbled into it. We believed that to win this market we really needed to build a superior tool. The best way we knew how to do that was to keep the team small and focused. And when we launched after a year in private beta, almost all of our 100 beta users converted to paid customers. To our surprise, we realized it wouldn't take that long to become profitable if we kept the costs in check. Twelve months after launch, we hit profitability, and we've stayed profitable ever since.</p><p>I don't know why hiring massive teams ever became the norm. In my own experience, small teams always delivered better quality, and faster. Maybe it's fear of missing out if you don't grow the team fast. Maybe it's investors whispering that your team is "understaffed compared to benchmarks." Being understaffed compared to benchmarks almost always should be a source of pride, not a problem. People should be surprised how small your team is, not how big it is.</p><p>What holds you back is rarely team size ‚Äì it's the clarity of your focus, skill and ability to execute. Larger teams mean slower progress, more management overhead, more meetings, more opinions, and usually dilution of vision and standards. Yet growing the team has somehow become a symbol of success.</p><p>At Linear, we hired our first employee after six months and roughly doubled the team each year. With each hire, we make sure they truly elevate the team. We don't set out to hire ten engineers ‚Äì we hire the next <em>great</em> engineer. This intentional approach has allowed us to maintain both quality and culture.</p><p>The most underrated thing about profitability is how much peace of mind it gives you. Once you're profitable, you stop worrying about survival and focus on what really matters: building something great. Building the way you want. Instead of optimizing for the next fundraising round, you optimize for value creation.</p><p>While profitability might not come quickly for every startup, I believe it's achievable sooner than most think. If you're creating a new market, or truly require massive scale like a social network, or significant upfront investment like a hardware company, it might take longer. But if you're in a category where there isn't hard upfront investment, and you get some level of product-market fit with customers willing to pay, you can probably be profitable. You can decide to become profitable. And usually, it's a decision about how much and how fast you hire.</p><p><strong>Measure What Matters</strong></p><p>Revenue per employee is one of the clearest ways to see you‚Äôre hiring appropriately. While some of the best public companies benchmark at $1-2M per employee, for startups it's not unreasonable to target the range of $500k-$1M per employee.</p><p><strong>Understand Your Risk Profile</strong></p><p>Are you building something highly speculative where you're not sure if there's a market for it, or are you building something that already has a market but with a different take on it? In the former case profitability takes longer, but in the latter it could happen right away. Most software today, especially in the B2B space, is about building a modern version of something existing.</p><p><strong>Hire Intentionally and Slower</strong></p><p>For most software startups, ten people before product-market fit should be your ceiling, not your target. After PMF, every hire should address a specific, pressing need ‚Äì not just fill out an org chart. At Linear, our deliberately slow headcount growth forced us to be selective, which meant making better hires. It also protected our culture, since rapid hiring often dilutes the very things that made your startup special in the first place. When you hire less, you naturally hire better.</p><p><strong>Raise on Your Own Terms </strong></p><p>Being profitable doesn't mean you have to be anti-investors. It means you have that choice, and investors are quite interested in profitable companies that also grow fast. You can raise more, less, or nothing. You can wait for the right timing, the right partner, or fund. For most ambitious startups, it can still be a good idea to raise something even if you could get by bootstrapping. Investors can still be helpful, and the additional cash balance can help you to make larger investments, or acquisitions.</p><p>The point is that you can be and are allowed to be profitable as a startup. It's not a bad thing, it's not an oxymoron or as hard as people make it out to be. The secret is that a lot of successful companies actually were quite profitable early on, they just didn't talk about it. When you're profitable, you make decisions based on what's best for your customers and your product, not what's best for impressing investors.</p><p>I didn't set out to build a profitable startup. But once I got there, I realized I wouldn't want to build a company any other way.</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Bybit CEO Confirms Exchange Was Hacked for $1.46B (234 pts)]]></title>
            <link>https://www.tradingview.com/news/coindesk:cda1c390e094b:0-bybit-ceo-confirms-exchange-was-hacked-for-1-46b-says-his-firm-can-cover-the-loss/</link>
            <guid>43130143</guid>
            <pubDate>Fri, 21 Feb 2025 17:15:03 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.tradingview.com/news/coindesk:cda1c390e094b:0-bybit-ceo-confirms-exchange-was-hacked-for-1-46b-says-his-firm-can-cover-the-loss/">https://www.tradingview.com/news/coindesk:cda1c390e094b:0-bybit-ceo-confirms-exchange-was-hacked-for-1-46b-says-his-firm-can-cover-the-loss/</a>, See on <a href="https://news.ycombinator.com/item?id=43130143">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-name="news-story-content"><article><p><span><time datetime="Fri, 21 Feb 2025 15:34:03 GMT"><span>Feb 21, 2025</span><span>15:34<!-- --> <!-- -->UTC</span></time></span></p><p><span><p>Cryptocurrency exchange Bybit has experienced $1.46 billion worth of "suspicious outflows," according to blockchain sleuth ZachXBT.</p><p>The wallet in question appears to have sent 401,346 ETH ($1.1 billion) as well as several other iterations of staked ether (stETH) to a fresh wallet, which is now liquidating mETH and stETH on decentralized exchanges, etherscan shows. The wallet has sold around $200 million worth of stETH so far.</p><p>Bybit CEO Ben Zhou wrote on X that a hacker "took control of the specific ETH cold wallet and transferred all the ETH in the cold wallet to this unidentified address."</p><p>"Please rest assured that all other cold wallets are secure. All withdrawals are normal," he added.</p><p>"My sources confirm it's a security incident," ZachXBT added on Telegram.</p><p>$1.46 billion would equate to the largest cryptocurrency hack of all time in dollar terms, with $470 million being lost in the Mt Gox Hack, $530 million in the 2018 hack of CoinCheck, and $650 million in the Ronin Bridge exploit.</p><p>BTC and ETH dropped more than 1.5% and 2%, respectively, following the transfers.</p><p>UPDATE (15:44 UTC, Feb. 21): Adds quote from Bybit CEO and details of historical crypto hacks.</p></span></p></article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[DeepDive in everything of Llama3: revealing detailed insights and implementation (180 pts)]]></title>
            <link>https://github.com/therealoliver/Deepdive-llama3-from-scratch</link>
            <guid>43129887</guid>
            <pubDate>Fri, 21 Feb 2025 16:57:13 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/therealoliver/Deepdive-llama3-from-scratch">https://github.com/therealoliver/Deepdive-llama3-from-scratch</a>, See on <a href="https://news.ycombinator.com/item?id=43129887">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><p dir="auto">
    <a target="_blank" rel="noopener noreferrer" href="https://github.com/therealoliver/Deepdive-llama3-from-scratch/blob/main/images/logo.png"><img src="https://github.com/therealoliver/Deepdive-llama3-from-scratch/raw/main/images/logo.png" width="600px"></a>
</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Deepdive-llama3-from-scratch</h2><a id="user-content-deepdive-llama3-from-scratch" aria-label="Permalink: Deepdive-llama3-from-scratch" href="#deepdive-llama3-from-scratch"></a></p>
<p dir="auto">
    <a href="https://github.com/therealoliver/Deepdive-llama3-from-scratch/blob/main/LICENSE"><img src="https://camo.githubusercontent.com/8714a45b348376ef75d52e9adcc597d84fc855efd11b2e5eca5b2b5cb52a99fe/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6c6963656e73652f7468657265616c6f6c697665722f44656570646976652d6c6c616d61332d66726f6d2d73637261746368" alt="License" data-canonical-src="https://img.shields.io/github/license/therealoliver/Deepdive-llama3-from-scratch"></a>
    <a href="https://github.com/therealoliver/Deepdive-llama3-from-scratch/stargazers"><img src="https://camo.githubusercontent.com/12e1dc75bd78c858f8d5f6986ed939b9c9a8403e4f4d528da7d0759d3a8ae25d/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f7468657265616c6f6c697665722f44656570646976652d6c6c616d61332d66726f6d2d73637261746368" alt="GitHub stars" data-canonical-src="https://img.shields.io/github/stars/therealoliver/Deepdive-llama3-from-scratch"></a>
    <a href="#from_me"><img src="https://camo.githubusercontent.com/32783fcfbbf44f49658a770f3403e9a13329d7c170898e02ba0e5d4baf85aaac/68747470733a2f2f696d672e736869656c64732e696f2f62616467652fe298952532304275792532306d6525323061253230636f666665652d666636396234" alt="Buy me a coffee" data-canonical-src="https://img.shields.io/badge/‚òï%20Buy%20me%20a%20coffee-ff69b4"></a>
</p>

<hr>
<div dir="auto"><p>This project is an enhanced version based on <a href="https://github.com/naklecha/llama3-from-scratch">naklecha/llama3-from-scratch</a>. It has been comprehensively improved and optimized on the basis of the original project, aiming to help everyone more easily understand and master the implementation principle and the detailed reasoning process of the Llama3 model. Thanks to the contributions of the original author :)
</p></div>
<p dir="auto"><h3 tabindex="-1" dir="auto">
The following are the core improvements of this project:
</h3><a id="user-content-the-following-are-the-core-improvements-of-this-project" aria-label="Permalink: 
The following are the core improvements of this project:
" href="#the-following-are-the-core-improvements-of-this-project"></a></p>
<ol dir="auto">
<li>
<p dir="auto"><strong>Structural Optimization</strong><br>
The presentation sequence of the content has been rearranged, and the directory structure has been adjusted to make the learning process clearer and more reasonable, facilitating everyone to understand the code step by step.</p>
</li>
<li>
<p dir="auto"><strong>Code Annotations</strong><br>
A large number of detailed code annotations have been added to teach you how to understand the function of each piece of code. Even beginners can get started easily.</p>
</li>
<li>
<p dir="auto"><strong>Dimension Tracking</strong><br>
The changes in the matrix dimensions in each step of the calculation are fully annotated, making it easier for you to understand the entire process.</p>
</li>
<li>
<p dir="auto"><strong>Principle Explanation</strong><br>
Abundant principle-related explanations and a large number of detailed derivations have been added. It not only tells you "what to do" but also deeply explains "why to do it", helping you fundamentally master the design concept of the model.</p>
</li>
<li>
<p dir="auto"><strong>KV-Cache Insights</strong><br>
An additional derivation chapter on KV-Cache has been added, covering detailed core concepts, principle derivations, and the application process in the attention mechanism, allowing you to understand every detail and philosophy of KV-Cache from its roots.</p>
</li>
<li>
<div dir="auto"><p><strong>Bilingual Documents</strong><br>
Code files in both Chinese and English are provided. The native Chinese translation avoids the problem of inaccurate expressions caused by machine translation.
</p></div>
</li>
</ol>
<hr>
<p dir="auto"><h2 tabindex="-1" dir="auto">Table of Contents</h2><a id="user-content-table-of-contents" aria-label="Permalink: Table of Contents" href="#table-of-contents"></a></p>
<ul dir="auto">
<li><a href="#loading-the-model">Loading the model</a>
<ul dir="auto">
<li><a href="#loading-the-tokenizer">Loading the tokenizer</a></li>
<li><a href="#reading-model-files-and-configuration-files">Reading model files and configuration files</a>
<ul dir="auto">
<li><a href="#inferring-model-details-using-the-configuration-file">Inferring model details using the configuration file</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#convert-the-input-text-into-embeddings">Convert the input text into embeddings</a>
<ul dir="auto">
<li><a href="#convert-the-text-into-a-sequence-of-token-ids">Convert the text into a sequence of token ids</a></li>
<li><a href="#convert-the-sequence-of-token-ids-into-embeddings">Convert the sequence of token ids into embeddings</a></li>
</ul>
</li>
<li><a href="#build-the-first-transformer-block">Build the first Transformer block</a>
<ul dir="auto">
<li><a href="#normalization">Normalization</a>
<ul dir="auto">
<li><a href="#using-rms-normalization-for-embeddings">Using RMS normalization for embeddings</a></li>
</ul>
</li>
<li><a href="#implementing-the-single-head-attention-mechanism-from-scratch">Implementing the single-head attention mechanism from scratch</a>
<ul dir="auto">
<li><a href="#obtain-the-qkv-vectors-corresponding-to-the-input-tokens">Obtain the QKV vectors corresponding to the input tokens</a>
<ul dir="auto">
<li><a href="#obtain-the-query-vector">Obtain the query vector</a>
<ul dir="auto">
<li><a href="#unfold-the-query-weight-matrix">Unfold the query weight matrix</a></li>
<li><a href="#obtain-the-first-head">Obtain the first head</a></li>
<li><a href="#multiply-the-token-embeddings-by-the-query-weights-to-obtain-the-query-vectors-corresponding-to-the-tokens">Multiply the token embeddings by the query weights to obtain the query vectors corresponding to the tokens</a></li>
</ul>
</li>
<li><a href="#obtain-the-key-vector-almost-the-same-as-the-query-vector">Obtain the key vector (almost the same as the query vector)</a></li>
<li><a href="#obtain-the-value-vector-almost-the-same-as-the-key-vector">Obtain the value vector (almost the same as the key vector)</a></li>
</ul>
</li>
<li><a href="#add-positional-information-to-the-query-and-key-vectors">Add positional information to the query and key vectors</a>
<ul dir="auto">
<li><a href="#rotary-position-encoding-rope">Rotary Position Encoding (RoPE)</a></li>
<li><a href="#add-positional-information-to-the-query-vectors">Add positional information to the query vectors</a></li>
<li><a href="#add-positional-information-to-the-key-vectors-same-as-the-query">Add positional information to the key vectors (same as the query)</a></li>
</ul>
</li>
<li><a href="#everythings-ready-lets-start-calculating-the-attention-weights-between-tokens">Everything's ready. Let's start calculating the attention weights between tokens.</a>
<ul dir="auto">
<li><a href="#multiply-the-query-and-key-vectors-to-obtain-the-attention-scores">Multiply the query and key vectors to obtain the attention scores.</a></li>
<li><a href="#now-we-must-mask-the-future-query-key-scores">Now we must mask the future query-key scores.</a></li>
<li><a href="#calculate-the-final-attention-weights-that-is-softmaxscore">Calculate the final attention weights, that is, softmax(score).</a></li>
</ul>
</li>
<li><a href="#finally-calculate-the-final-result-of-the-single-head-attention-mechanism">Finally! Calculate the final result of the single-head attention mechanism!</a></li>
</ul>
</li>
<li><a href="#calculate-the-multi-head-attention-mechanism-a-simple-loop-to-repeat-the-above-process">Calculate the multi-head attention mechanism (a simple loop to repeat the above process)</a>
<ul dir="auto">
<li><a href="#calculate-the-result-for-each-head">Calculate the result for each head</a></li>
<li><a href="#merge-the-results-of-each-head-into-a-large-matrix">Merge the results of each head into a large matrix</a></li>
<li><a href="#head-to-head-information-interaction-linear-mapping-the-final-step-of-the-self-attention-layer">Head-to-head information interaction (linear mapping), the final step of the self-attention layer!</a></li>
</ul>
</li>
<li><a href="#perform-the-residual-operation-add">Perform the residual operation (add)</a></li>
<li><a href="#perform-the-second-normalization-operation">Perform the second normalization operation</a></li>
<li><a href="#perform-the-calculation-of-the-ffn-feed-forward-neural-network-layer">Perform the calculation of the FFN (Feed-Forward Neural Network) layer</a></li>
<li><a href="#perform-the-residual-operation-again-finally-we-get-the-final-output-of-the-transformer-block">Perform the residual operation again (Finally, we get the final output of the Transformer block!)</a></li>
</ul>
</li>
<li><a href="#everything-is-here-lets-complete-the-calculation-of-all-32-transformer-blocks-happy-reading-">Everything is here. Let's complete the calculation of all 32 Transformer blocks. Happy reading :)</a></li>
<li><a href="#lets-complete-the-last-step-and-predict-the-next-token">Let's complete the last step and predict the next token</a>
<ul dir="auto">
<li><a href="#first-perform-one-last-normalization-on-the-output-of-the-last-transformer-layer">First, perform one last normalization on the output of the last Transformer layer</a></li>
<li><a href="#then-make-the-prediction-based-on-the-embedding-corresponding-to-the-last-token-perform-a-linear-mapping-to-the-vocabulary-dimension">Then, make the prediction based on the embedding corresponding to the last token (perform a linear mapping to the vocabulary dimension)</a></li>
<li><a href="#heres-the-prediction-result">Here's the prediction result!</a></li>
</ul>
</li>
<li><a href="#lets-dive-deeper-and-see-how-different-embeddings-or-token-masking-strategies-might-affect-the-prediction-results-">Let's dive deeper and see how different embeddings or token masking strategies might affect the prediction results :)</a></li>
<li><a href="#need-to-predict-multiple-tokens-just-using-kv-cache-it-really-took-me-a-lot-of-effort-to-sort-this-out-orz">Need to predict multiple tokens? Just using KV-Cache! (It really took me a lot of effort to sort this out. Orz)</a></li>
<li><a href="#thank-you-all-thanks-for-your-continuous-learning-love-you-all-">Thank you all. Thanks for your continuous learning. Love you all :)</a>
<ul dir="auto">
<li><a href="#from-me">From Me</a></li>
<li><a href="#from-the-author-of-predecessor-project">From the author of predecessor project</a></li>
</ul>
</li>
<li><a href="#license">LICENSE</a></li>
</ul>
<hr>
<p dir="auto"><h3 tabindex="-1" dir="auto">
Now, let's start the formal learning process!
</h3><a id="user-content-now-lets-start-the-formal-learning-process" aria-label="Permalink: 
Now, let's start the formal learning process!
" href="#now-lets-start-the-formal-learning-process"></a></p>
<br>
<div dir="auto"><p>In this file, I implemented Llama3 from scratch, one tensor and matrix multiplication at a time.
<br>
Also, I'm going to load tensors directly from the model file that meta provided for Llama3 (Meta-Llama-3-8B), you need to download the weights before running this file. Here is the offical link to download the weights: <a href="https://llama.meta.com/llama-downloads/" rel="nofollow">https://llama.meta.com/llama-downloads/</a>
<br>
Note: This project uses the original model files, that is, the models in the "original" folder of the downloaded model files.
</p></div>
<p dir="auto"><h3 tabindex="-1" dir="auto">
    Please Note! There is a small mistake in the figure:<br>
    </h3><a id="user-content-----please-note-there-is-a-small-mistake-in-the-figure----" aria-label="Permalink: 
    Please Note! There is a small mistake in the figure:" href="#----please-note-there-is-a-small-mistake-in-the-figure----"></a></p><p dir="auto"><h4 tabindex="-1" dir="auto">
        In each Transformer block, the input of the second "add" operation should be the output of the feed-forward layer and the output of the first "add" operation, instead of the result after normalization.
        <br>
        If we consider multi-head self-attention and feed-forward as the same type of operations (both for feature transformation), then the forms and processes of the two "normalization - feature transformation - residual connection (add)" are exactly the same.
    </h4><a id="user-content---------in-each-transformer-block-the-input-of-the-second-add-operation-should-be-the-output-of-the-feed-forward-layer-and-the-output-of-the-first-add-operation-instead-of-the-result-after-normalization----------------if-we-consider-multi-head-self-attention-and-feed-forward-as-the-same-type-of-operations-both-for-feature-transformation-then-the-forms-and-processes-of-the-two-normalization---feature-transformation---residual-connection-add-are-exactly-the-same----" aria-label="Permalink: 
        In each Transformer block, the input of the second &quot;add&quot; operation should be the output of the feed-forward layer and the output of the first &quot;add&quot; operation, instead of the result after normalization.
        
        If we consider multi-head self-attention and feed-forward as the same type of operations (both for feature transformation), then the forms and processes of the two &quot;normalization - feature transformation - residual connection (add)&quot; are exactly the same.
    " href="#--------in-each-transformer-block-the-input-of-the-second-add-operation-should-be-the-output-of-the-feed-forward-layer-and-the-output-of-the-first-add-operation-instead-of-the-result-after-normalization----------------if-we-consider-multi-head-self-attention-and-feed-forward-as-the-same-type-of-operations-both-for-feature-transformation-then-the-forms-and-processes-of-the-two-normalization---feature-transformation---residual-connection-add-are-exactly-the-same----"></a></p>

<p><a target="_blank" rel="noopener noreferrer" href="https://github.com/therealoliver/Deepdive-llama3-from-scratch/blob/main/images/archi.png"><img src="https://github.com/therealoliver/Deepdive-llama3-from-scratch/raw/main/images/archi.png"></a>
</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Loading the model</h2><a id="user-content-loading-the-model" aria-label="Permalink: Loading the model" href="#loading-the-model"></a></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Loading the tokenizer</h2><a id="user-content-loading-the-tokenizer" aria-label="Permalink: Loading the tokenizer" href="#loading-the-tokenizer"></a></p>
<p dir="auto">The tokenizer is used to split the input text string into a sequence of sub-words, making it easier to input to the model.
<br>
I'm not going to implement a bpe tokenizer (but andrej karpathy has a really clean implementation),
link to his implementation: <a href="https://github.com/karpathy/minbpe">https://github.com/karpathy/minbpe</a></p>
<p><a target="_blank" rel="noopener noreferrer" href="https://github.com/therealoliver/Deepdive-llama3-from-scratch/blob/main/images/karpathyminbpe.png"><img src="https://github.com/therealoliver/Deepdive-llama3-from-scratch/raw/main/images/karpathyminbpe.png" width="600"></a>
</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Summary of the steps to load the BPE-based tokenizer:</h3><a id="user-content-summary-of-the-steps-to-load-the-bpe-based-tokenizer" aria-label="Permalink: Summary of the steps to load the BPE-based tokenizer:" href="#summary-of-the-steps-to-load-the-bpe-based-tokenizer"></a></p>
<ol dir="auto">
<li>Loading regular words: Load the local tokenizer model dictionary (which only contains regular subwords and no special tokens).</li>
<li>Definition of the special words: Manually define special tokens (using ready-made ones or modifying based on the ready-made ones).</li>
<li>Definition of the text rough-splitting rule: Define the regular expression for text rough-splitting (just using a ready-made one). The input will go through two steps of rough-splitting (based on the regular expression) and fine-splitting (based on BPE) to obtain the final tokenization result.</li>
<li>Create tokenizer: Create a text encoder-decoder object based on the open-sourced tiktoken library by OpenAI (which can further split the rough-splitting result based on the BPE algorithm).</li>
</ol>
<div dir="auto" data-snippet-clipboard-copy-content="# Loading the BPE-based Tokenizer

# Import related libraries
from pathlib import Path  # Used to obtain the file name/model name from the file path
import tiktoken  # An open-source library developed by OpenAI for text encoding and decoding (mutual conversion between text and token IDs)
from tiktoken.load import load_tiktoken_bpe  # Load the BPE model
import torch  # Used for building models and matrix calculations
import json  # Used for loading configuration files
import matplotlib.pyplot as plt  # Used for plotting graphs


tokenizer_path = &quot;Meta-Llama-3-8B/original/tokenizer.model&quot;  # Path to the tokenizer model

# Special tokens outside the regular dictionary.
# These special tokens are present in the 'added_tokens' field of both 'tokenizer.json' and 'tokenizer_config.json' in the &quot;Meta-Llama-3-8B/&quot; path
special_tokens = [
            &quot;<|begin_of_text|>&quot;,
            &quot;<|end_of_text|>&quot;,
            &quot;<|reserved_special_token_0|>&quot;,  # Reserved special tokens from 0 to 250
            &quot;<|reserved_special_token_1|>&quot;,
            &quot;<|reserved_special_token_2|>&quot;,
            &quot;<|reserved_special_token_3|>&quot;,
            &quot;<|start_header_id|>&quot;,  # Start of header information, used to mark the header information that wraps structured data, such as metadata
            &quot;<|end_header_id|>&quot;,  # End of header information
            &quot;<|reserved_special_token_4|>&quot;,
            &quot;<|eot_id|>&quot;,  # end of turn, used to mark the end of the current turn in multi-turn conversations
        ] + [f&quot;<|reserved_special_token_{i}|>&quot; for i in range(5, 256 - 5)]


# Load the BPE model (actually a dictionary)
# A dictionary of subword(bytes type, decoded with utf-8)-rank(id) pairs, with 128000 words, not including the 256 special tokens above,
# so the total size of the model's dictionary will be 128256 in after operation (but not here)
# The rank values are an increasing sequence starting from 0, used to determine the priority order of subword unit merging,
# the higher the priority, the earlier the merging. Therefore, the variable name here is &quot;mergeable_ranks&quot; instead of something like BPE or word dictionary
# The special tokens are not added to the dictionary probably for flexibility,
# making it easy to add specific tokens when facing different model architectures or tasks with different special tokens, and keeping the dictionary size unchanged
mergeable_ranks = load_tiktoken_bpe(tokenizer_path)


# Create a text encoder-decoder object
# The pat_str is roughly divided into three types: words with abbreviations &amp; words, Chinese segments, 1-3-digit numbers &amp; other special characters
tokenizer = tiktoken.Encoding(
    name=Path(tokenizer_path).name,  # Name of the encoder, which is convenient when debugging and logging to use different encoders
    pat_str=r&quot;(?i:'s|'t|'re|'ve|'m|'ll|'d)|[^\r\n\p{L}\p{N}]?\p{L}+|\p{N}{1,3}| ?[^\s\p{L}\p{N}]+[\r\n]*|\s*[\r\n]+|\s+(?!\S)|\s+&quot;,  # Regular expression for initially roughly splitting the text into a token sequence
    mergeable_ranks=mergeable_ranks,  # Pass in the loaded BPE model
    special_tokens={token: len(mergeable_ranks) + i for i, token in enumerate(special_tokens)},  # Dictionary for adding special token-id pairs
)


# Test whether the creation is successful, that is, whether the encoder-decoder can run correctly
print(tokenizer.decode(tokenizer.encode(&quot;create tokenizer successed!&quot;)))


# The following is a case test to test the effects and differences between the rough splitting of pat_str and the fine splitting of the tokenizer
# The regular expression of pat_str only provides a preliminary splitting,
# some long sentences or Chinese text will not be split and will be further refined based on the BPE algorithm in the tokenizer
import regex  # Since some Unicode syntax such as \p{L} is used in pat_str, the re library cannot be used

## Create a regular expression
pat_str=r&quot;(?i:'s|'t|'re|'ve|'m|'ll|'d)|[^\r\n\p{L}\p{N}]?\p{L}+|\p{N}{1,3}| ?[^\s\p{L}\p{N}]+[\r\n]*|\s*[\r\n]+|\s+(?!\S)|\s+&quot;
pattern = regex.compile(pat_str)

## Text segmentation
text = &quot;Hello world! It's a test. ËøôÊòØ‰∏Ä‰∏™ÊµãËØï. alongwords. a long words. 123 456 789.&quot;  # testing string
re_tokens = pattern.findall(text)  # Split the string using the regular expression
merge_tokens_id = tokenizer.encode(text)  # Split the string using the tokenizer
merge_tokens = [tokenizer.decode([i]) for i in merge_tokens_id]  # Convert the id sequence of the tokenizer's splitting result into an actual subword sequence

## Output result
print(&quot;Original string: &quot;, text)
print(&quot;Regular expression splitting result: &quot;, re_tokens)
print(&quot;Tokenizer splitting result: &quot;, merge_tokens)
print(&quot;Tokenizer splitting result ids: &quot;, list(zip(merge_tokens, merge_tokens_id)))

## From the results, it can be seen that the leading spaces of all words are retained, rather than being a single space token or being deleted.
## This is beneficial for the model to correctly understand the boundary information between words, such as 'alongwords' in the example."><pre><span># Loading the BPE-based Tokenizer</span>

<span># Import related libraries</span>
<span>from</span> <span>pathlib</span> <span>import</span> <span>Path</span>  <span># Used to obtain the file name/model name from the file path</span>
<span>import</span> <span>tiktoken</span>  <span># An open-source library developed by OpenAI for text encoding and decoding (mutual conversion between text and token IDs)</span>
<span>from</span> <span>tiktoken</span>.<span>load</span> <span>import</span> <span>load_tiktoken_bpe</span>  <span># Load the BPE model</span>
<span>import</span> <span>torch</span>  <span># Used for building models and matrix calculations</span>
<span>import</span> <span>json</span>  <span># Used for loading configuration files</span>
<span>import</span> <span>matplotlib</span>.<span>pyplot</span> <span>as</span> <span>plt</span>  <span># Used for plotting graphs</span>


<span>tokenizer_path</span> <span>=</span> <span>"Meta-Llama-3-8B/original/tokenizer.model"</span>  <span># Path to the tokenizer model</span>

<span># Special tokens outside the regular dictionary.</span>
<span># These special tokens are present in the 'added_tokens' field of both 'tokenizer.json' and 'tokenizer_config.json' in the "Meta-Llama-3-8B/" path</span>
<span>special_tokens</span> <span>=</span> [
            <span>"&lt;|begin_of_text|&gt;"</span>,
            <span>"&lt;|end_of_text|&gt;"</span>,
            <span>"&lt;|reserved_special_token_0|&gt;"</span>,  <span># Reserved special tokens from 0 to 250</span>
            <span>"&lt;|reserved_special_token_1|&gt;"</span>,
            <span>"&lt;|reserved_special_token_2|&gt;"</span>,
            <span>"&lt;|reserved_special_token_3|&gt;"</span>,
            <span>"&lt;|start_header_id|&gt;"</span>,  <span># Start of header information, used to mark the header information that wraps structured data, such as metadata</span>
            <span>"&lt;|end_header_id|&gt;"</span>,  <span># End of header information</span>
            <span>"&lt;|reserved_special_token_4|&gt;"</span>,
            <span>"&lt;|eot_id|&gt;"</span>,  <span># end of turn, used to mark the end of the current turn in multi-turn conversations</span>
        ] <span>+</span> [<span>f"&lt;|reserved_special_token_<span><span>{</span><span>i</span><span>}</span></span>|&gt;"</span> <span>for</span> <span>i</span> <span>in</span> <span>range</span>(<span>5</span>, <span>256</span> <span>-</span> <span>5</span>)]


<span># Load the BPE model (actually a dictionary)</span>
<span># A dictionary of subword(bytes type, decoded with utf-8)-rank(id) pairs, with 128000 words, not including the 256 special tokens above,</span>
<span># so the total size of the model's dictionary will be 128256 in after operation (but not here)</span>
<span># The rank values are an increasing sequence starting from 0, used to determine the priority order of subword unit merging,</span>
<span># the higher the priority, the earlier the merging. Therefore, the variable name here is "mergeable_ranks" instead of something like BPE or word dictionary</span>
<span># The special tokens are not added to the dictionary probably for flexibility,</span>
<span># making it easy to add specific tokens when facing different model architectures or tasks with different special tokens, and keeping the dictionary size unchanged</span>
<span>mergeable_ranks</span> <span>=</span> <span>load_tiktoken_bpe</span>(<span>tokenizer_path</span>)


<span># Create a text encoder-decoder object</span>
<span># The pat_str is roughly divided into three types: words with abbreviations &amp; words, Chinese segments, 1-3-digit numbers &amp; other special characters</span>
<span>tokenizer</span> <span>=</span> <span>tiktoken</span>.<span>Encoding</span>(
    <span>name</span><span>=</span><span>Path</span>(<span>tokenizer_path</span>).<span>name</span>,  <span># Name of the encoder, which is convenient when debugging and logging to use different encoders</span>
    <span>pat_str</span><span>=</span><span>r"(?i:'s|'t|'re|'ve|'m|'ll|'d)|[^\r\n\p{L}\p{N}]?\p{L}+|\p{N}{1,3}| ?[^\s\p{L}\p{N}]+[\r\n]*|\s*[\r\n]+|\s+(?!\S)|\s+"</span>,  <span># Regular expression for initially roughly splitting the text into a token sequence</span>
    <span>mergeable_ranks</span><span>=</span><span>mergeable_ranks</span>,  <span># Pass in the loaded BPE model</span>
    <span>special_tokens</span><span>=</span>{<span>token</span>: <span>len</span>(<span>mergeable_ranks</span>) <span>+</span> <span>i</span> <span>for</span> <span>i</span>, <span>token</span> <span>in</span> <span>enumerate</span>(<span>special_tokens</span>)},  <span># Dictionary for adding special token-id pairs</span>
)


<span># Test whether the creation is successful, that is, whether the encoder-decoder can run correctly</span>
<span>print</span>(<span>tokenizer</span>.<span>decode</span>(<span>tokenizer</span>.<span>encode</span>(<span>"create tokenizer successed!"</span>)))


<span># The following is a case test to test the effects and differences between the rough splitting of pat_str and the fine splitting of the tokenizer</span>
<span># The regular expression of pat_str only provides a preliminary splitting,</span>
<span># some long sentences or Chinese text will not be split and will be further refined based on the BPE algorithm in the tokenizer</span>
<span>import</span> <span>regex</span>  <span># Since some Unicode syntax such as \p{L} is used in pat_str, the re library cannot be used</span>

<span>## Create a regular expression</span>
<span>pat_str</span><span>=</span><span>r"(?i:'s|'t|'re|'ve|'m|'ll|'d)|[^\r\n\p{L}\p{N}]?\p{L}+|\p{N}{1,3}| ?[^\s\p{L}\p{N}]+[\r\n]*|\s*[\r\n]+|\s+(?!\S)|\s+"</span>
<span>pattern</span> <span>=</span> <span>regex</span>.<span>compile</span>(<span>pat_str</span>)

<span>## Text segmentation</span>
<span>text</span> <span>=</span> <span>"Hello world! It's a test. ËøôÊòØ‰∏Ä‰∏™ÊµãËØï. alongwords. a long words. 123 456 789."</span>  <span># testing string</span>
<span>re_tokens</span> <span>=</span> <span>pattern</span>.<span>findall</span>(<span>text</span>)  <span># Split the string using the regular expression</span>
<span>merge_tokens_id</span> <span>=</span> <span>tokenizer</span>.<span>encode</span>(<span>text</span>)  <span># Split the string using the tokenizer</span>
<span>merge_tokens</span> <span>=</span> [<span>tokenizer</span>.<span>decode</span>([<span>i</span>]) <span>for</span> <span>i</span> <span>in</span> <span>merge_tokens_id</span>]  <span># Convert the id sequence of the tokenizer's splitting result into an actual subword sequence</span>

<span>## Output result</span>
<span>print</span>(<span>"Original string: "</span>, <span>text</span>)
<span>print</span>(<span>"Regular expression splitting result: "</span>, <span>re_tokens</span>)
<span>print</span>(<span>"Tokenizer splitting result: "</span>, <span>merge_tokens</span>)
<span>print</span>(<span>"Tokenizer splitting result ids: "</span>, <span>list</span>(<span>zip</span>(<span>merge_tokens</span>, <span>merge_tokens_id</span>)))

<span>## From the results, it can be seen that the leading spaces of all words are retained, rather than being a single space token or being deleted.</span>
<span>## This is beneficial for the model to correctly understand the boundary information between words, such as 'alongwords' in the example.</span></pre></div>
<div data-snippet-clipboard-copy-content="create tokenizer successed!
Original string:  Hello world! It's a test. ËøôÊòØ‰∏Ä‰∏™ÊµãËØï. alongwords. a long words. 123 456 789.
Regular expression splitting result:  ['Hello', ' world', '!', ' It', &quot;'s&quot;, ' a', ' test', '.', ' ËøôÊòØ‰∏Ä‰∏™ÊµãËØï', '.', ' alongwords', '.', ' a', ' long', ' words', '.', ' ', '123', ' ', '456', ' ', '789', '.']
Tokenizer splitting result:  ['Hello', ' world', '!', ' It', &quot;'s&quot;, ' a', ' test', '.', ' Ëøô', 'ÊòØ‰∏Ä‰∏™', 'ÊµãËØï', '.', ' along', 'words', '.', ' a', ' long', ' words', '.', ' ', '123', ' ', '456', ' ', '789', '.']
Tokenizer splitting result ids:  [('Hello', 9906), (' world', 1917), ('!', 0), (' It', 1102), (&quot;'s&quot;, 596), (' a', 264), (' test', 1296), ('.', 13), (' Ëøô', 122255), ('ÊòØ‰∏Ä‰∏™', 122503), ('ÊµãËØï', 82805), ('.', 13), (' along', 3235), ('words', 5880), ('.', 13), (' a', 264), (' long', 1317), (' words', 4339), ('.', 13), (' ', 220), ('123', 4513), (' ', 220), ('456', 10961), (' ', 220), ('789', 16474), ('.', 13)]"><pre><code>create tokenizer successed!
Original string:  Hello world! It's a test. ËøôÊòØ‰∏Ä‰∏™ÊµãËØï. alongwords. a long words. 123 456 789.
Regular expression splitting result:  ['Hello', ' world', '!', ' It', "'s", ' a', ' test', '.', ' ËøôÊòØ‰∏Ä‰∏™ÊµãËØï', '.', ' alongwords', '.', ' a', ' long', ' words', '.', ' ', '123', ' ', '456', ' ', '789', '.']
Tokenizer splitting result:  ['Hello', ' world', '!', ' It', "'s", ' a', ' test', '.', ' Ëøô', 'ÊòØ‰∏Ä‰∏™', 'ÊµãËØï', '.', ' along', 'words', '.', ' a', ' long', ' words', '.', ' ', '123', ' ', '456', ' ', '789', '.']
Tokenizer splitting result ids:  [('Hello', 9906), (' world', 1917), ('!', 0), (' It', 1102), ("'s", 596), (' a', 264), (' test', 1296), ('.', 13), (' Ëøô', 122255), ('ÊòØ‰∏Ä‰∏™', 122503), ('ÊµãËØï', 82805), ('.', 13), (' along', 3235), ('words', 5880), ('.', 13), (' a', 264), (' long', 1317), (' words', 4339), ('.', 13), (' ', 220), ('123', 4513), (' ', 220), ('456', 10961), (' ', 220), ('789', 16474), ('.', 13)]
</code></pre></div>
<p dir="auto"><h2 tabindex="-1" dir="auto">Reading model files and configuration files</h2><a id="user-content-reading-model-files-and-configuration-files" aria-label="Permalink: Reading model files and configuration files" href="#reading-model-files-and-configuration-files"></a></p>
<p dir="auto">Generally, reading a model file depends on how its model class is written and the variable names within it.
<br>
However, since we are implementing Llama3 from scratch, we will read one tensor file at a time.</p>
<p><a target="_blank" rel="noopener noreferrer" href="https://github.com/therealoliver/Deepdive-llama3-from-scratch/blob/main/images/model.png"><img src="https://github.com/therealoliver/Deepdive-llama3-from-scratch/raw/main/images/model.png" width="600"></a>
</p>
<div dir="auto" data-snippet-clipboard-copy-content="# Load the model, a dictionary such as {&quot;network-layer-name&quot;: tensor-type parameters}
model = torch.load(&quot;Meta-Llama-3-8B/original/consolidated.00.pth&quot;)

# Print the names of the first 20 network layers to verify if the model is loaded correctly.
print(json.dumps(list(model.keys())[:20], indent=4))"><pre><span># Load the model, a dictionary such as {"network-layer-name": tensor-type parameters}</span>
<span>model</span> <span>=</span> <span>torch</span>.<span>load</span>(<span>"Meta-Llama-3-8B/original/consolidated.00.pth"</span>)

<span># Print the names of the first 20 network layers to verify if the model is loaded correctly.</span>
<span>print</span>(<span>json</span>.<span>dumps</span>(<span>list</span>(<span>model</span>.<span>keys</span>())[:<span>20</span>], <span>indent</span><span>=</span><span>4</span>))</pre></div>
<div data-snippet-clipboard-copy-content="[
    &quot;tok_embeddings.weight&quot;,
    &quot;layers.0.attention.wq.weight&quot;,
    &quot;layers.0.attention.wk.weight&quot;,
    &quot;layers.0.attention.wv.weight&quot;,
    &quot;layers.0.attention.wo.weight&quot;,
    &quot;layers.0.feed_forward.w1.weight&quot;,
    &quot;layers.0.feed_forward.w3.weight&quot;,
    &quot;layers.0.feed_forward.w2.weight&quot;,
    &quot;layers.0.attention_norm.weight&quot;,
    &quot;layers.0.ffn_norm.weight&quot;,
    &quot;layers.1.attention.wq.weight&quot;,
    &quot;layers.1.attention.wk.weight&quot;,
    &quot;layers.1.attention.wv.weight&quot;,
    &quot;layers.1.attention.wo.weight&quot;,
    &quot;layers.1.feed_forward.w1.weight&quot;,
    &quot;layers.1.feed_forward.w3.weight&quot;,
    &quot;layers.1.feed_forward.w2.weight&quot;,
    &quot;layers.1.attention_norm.weight&quot;,
    &quot;layers.1.ffn_norm.weight&quot;,
    &quot;layers.2.attention.wq.weight&quot;
]"><pre><code>[
    "tok_embeddings.weight",
    "layers.0.attention.wq.weight",
    "layers.0.attention.wk.weight",
    "layers.0.attention.wv.weight",
    "layers.0.attention.wo.weight",
    "layers.0.feed_forward.w1.weight",
    "layers.0.feed_forward.w3.weight",
    "layers.0.feed_forward.w2.weight",
    "layers.0.attention_norm.weight",
    "layers.0.ffn_norm.weight",
    "layers.1.attention.wq.weight",
    "layers.1.attention.wk.weight",
    "layers.1.attention.wv.weight",
    "layers.1.attention.wo.weight",
    "layers.1.feed_forward.w1.weight",
    "layers.1.feed_forward.w3.weight",
    "layers.1.feed_forward.w2.weight",
    "layers.1.attention_norm.weight",
    "layers.1.ffn_norm.weight",
    "layers.2.attention.wq.weight"
]
</code></pre></div>
<div dir="auto" data-snippet-clipboard-copy-content="# Load the configuration file.
# The specific meaning of each configuration is described in the next section.
with open(&quot;Meta-Llama-3-8B/original/params.json&quot;, &quot;r&quot;) as f:
    config = json.load(f)
config"><pre><span># Load the configuration file.</span>
<span># The specific meaning of each configuration is described in the next section.</span>
<span>with</span> <span>open</span>(<span>"Meta-Llama-3-8B/original/params.json"</span>, <span>"r"</span>) <span>as</span> <span>f</span>:
    <span>config</span> <span>=</span> <span>json</span>.<span>load</span>(<span>f</span>)
<span>config</span></pre></div>
<div data-snippet-clipboard-copy-content="{'dim': 4096,
 'n_layers': 32,
 'n_heads': 32,
 'n_kv_heads': 8,
 'vocab_size': 128256,
 'multiple_of': 1024,
 'ffn_dim_multiplier': 1.3,
 'norm_eps': 1e-05,
 'rope_theta': 500000.0}"><pre><code>{'dim': 4096,
 'n_layers': 32,
 'n_heads': 32,
 'n_kv_heads': 8,
 'vocab_size': 128256,
 'multiple_of': 1024,
 'ffn_dim_multiplier': 1.3,
 'norm_eps': 1e-05,
 'rope_theta': 500000.0}
</code></pre></div>
<p dir="auto"><h3 tabindex="-1" dir="auto">Inferring model details using the configuration file</h3><a id="user-content-inferring-model-details-using-the-configuration-file" aria-label="Permalink: Inferring model details using the configuration file" href="#inferring-model-details-using-the-configuration-file"></a></p>
<markdown-accessiblity-table><table>
<thead>
<tr>
<th>Configuration Item</th>
<th>Configuration Value</th>
<th>Meaning</th>
</tr>
</thead>
<tbody>
<tr>
<td>dim</td>
<td>4096</td>
<td>Dimension of the hidden layer, i.e., the vector representation of each token has a dimension of 4096.</td>
</tr>
<tr>
<td>n_layers</td>
<td>32</td>
<td>Number of model layers, i.e., the model has 32 Transformer layers or say Transformer blocks.</td>
</tr>
<tr>
<td>n_heads</td>
<td>32</td>
<td>Number of heads in multi-head attention, i.e., each multi-head attention block has 32 heads. The so-called multi-head means that multiple independent attention mechanisms are used simultaneously to capture different features or information of the input data.</td>
</tr>
<tr>
<td>n_kv_heads</td>
<td>8</td>
<td>Number of heads in key-value attention, used for Grouped Query Attention (GQA). That is, the key-value attention has 8 heads, while the query has n_heads=32 heads. Every 4 query heads will share a set of key-value pairs.</td>
</tr>
<tr>
<td>vocab_size</td>
<td>128256</td>
<td>Size of the vocabulary, including 128000 ordinary tokens and 256 special tokens.</td>
</tr>
<tr>
<td>multiple_of</td>
<td>1024</td>
<td>Multiple constraint on the dimension of the hidden layer. That is, the dimension of the model's hidden layer should be a multiple of 1024 to optimize computational efficiency.</td>
</tr>
<tr>
<td>ffn_dim_multiplier</td>
<td>1.3</td>
<td>Multiplier for the hidden layer dimension of the feed-forward network layer, used to calculate the hidden layer dimension of the FFN. The calculation process can be seen in the corresponding section.</td>
</tr>
<tr>
<td>norm_eps</td>
<td>1e-05</td>
<td>Constant added to the denominator in layer normalization calculation to prevent division by zero and ensure numerical stability.</td>
</tr>
<tr>
<td>rope_theta</td>
<td>500000.0</td>
<td>Basic frequency scaling factor in Rotary Position Encoding (RoPE), which controls the periodicity and resolution of position encoding, thus affecting the model's ability to capture sequences of different lengths and positional relationships.</td>
</tr>
</tbody>
</table></markdown-accessiblity-table>

<p dir="auto"><h3 tabindex="-1" dir="auto">
Based on the configuration details, the internal calculation process of attention given an input can be inferred as follows:
</h3><a id="user-content-based-on-the-configuration-details-the-internal-calculation-process-of-attention-given-an-input-can-be-inferred-as-follows" aria-label="Permalink: 
Based on the configuration details, the internal calculation process of attention given an input can be inferred as follows:
" href="#based-on-the-configuration-details-the-internal-calculation-process-of-attention-given-an-input-can-be-inferred-as-follows"></a></p>
<pre>input(L, 4096) -&gt; query_proj(L, 128, 32)
               -&gt; key_proj(L, 128, 8)
               -&gt; value_proj(L, 128, 8)
                                           -&gt; group_query_attention(L, 128, 32)
                                           -&gt; output_proj(L, 4096)
                                                                                   -&gt; output(L, 4096)
</pre>
<div dir="auto" data-snippet-clipboard-copy-content="# Record these configurations, which will be gradually used later.
dim = config[&quot;dim&quot;]
n_layers = config[&quot;n_layers&quot;]
n_heads = config[&quot;n_heads&quot;]
n_kv_heads = config[&quot;n_kv_heads&quot;]
vocab_size = config[&quot;vocab_size&quot;]
multiple_of = config[&quot;multiple_of&quot;]
ffn_dim_multiplier = config[&quot;ffn_dim_multiplier&quot;]
norm_eps = config[&quot;norm_eps&quot;]
rope_theta = torch.tensor(config[&quot;rope_theta&quot;])"><pre><span># Record these configurations, which will be gradually used later.</span>
<span>dim</span> <span>=</span> <span>config</span>[<span>"dim"</span>]
<span>n_layers</span> <span>=</span> <span>config</span>[<span>"n_layers"</span>]
<span>n_heads</span> <span>=</span> <span>config</span>[<span>"n_heads"</span>]
<span>n_kv_heads</span> <span>=</span> <span>config</span>[<span>"n_kv_heads"</span>]
<span>vocab_size</span> <span>=</span> <span>config</span>[<span>"vocab_size"</span>]
<span>multiple_of</span> <span>=</span> <span>config</span>[<span>"multiple_of"</span>]
<span>ffn_dim_multiplier</span> <span>=</span> <span>config</span>[<span>"ffn_dim_multiplier"</span>]
<span>norm_eps</span> <span>=</span> <span>config</span>[<span>"norm_eps"</span>]
<span>rope_theta</span> <span>=</span> <span>torch</span>.<span>tensor</span>(<span>config</span>[<span>"rope_theta"</span>])</pre></div>
<p dir="auto"><h2 tabindex="-1" dir="auto">Convert the input text into embeddings</h2><a id="user-content-convert-the-input-text-into-embeddings" aria-label="Permalink: Convert the input text into embeddings" href="#convert-the-input-text-into-embeddings"></a></p>
<p dir="auto">Before inputting the text in string form to the network layer, it needs to be converted into vector form for mathematical calculations.
<br>
The required process is: use the tokenizer to split the input text into a subword sequence -&gt; convert the subwords into vector representations.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Convert the text into a sequence of token ids</h2><a id="user-content-convert-the-text-into-a-sequence-of-token-ids" aria-label="Permalink: Convert the text into a sequence of token ids" href="#convert-the-text-into-a-sequence-of-token-ids"></a></p>
<p dir="auto">Here, we use tiktoken (a library from OpenAI) as the tokenizer.</p>
<p><a target="_blank" rel="noopener noreferrer" href="https://github.com/therealoliver/Deepdive-llama3-from-scratch/blob/main/images/tokens.png"><img src="https://github.com/therealoliver/Deepdive-llama3-from-scratch/raw/main/images/tokens.png" width="600"></a>
</p>
<div dir="auto" data-snippet-clipboard-copy-content="# Convert the input prompt into a sequence of token ids
prompt = &quot;the answer to the ultimate question of life, the universe, and everything is &quot;  # Input text
tokens = [128000] + tokenizer.encode(prompt)  # Perform subword segmentation and add a special token <|begin_of_text|> indicating the start of the text at the beginning of the text. Dimension: [17]
print(tokens)  # Check the segmentation result
tokens = torch.tensor(tokens)  # Convert to tensor type for subsequent matrix calculations. [17]

# Convert the token ids into a specific sequence of token subwords, which is only for display purposes and not actually needed
prompt_split_as_tokens = [tokenizer.decode([token]) for token in tokens]
print(prompt_split_as_tokens)"><pre><span># Convert the input prompt into a sequence of token ids</span>
<span>prompt</span> <span>=</span> <span>"the answer to the ultimate question of life, the universe, and everything is "</span>  <span># Input text</span>
<span>tokens</span> <span>=</span> [<span>128000</span>] <span>+</span> <span>tokenizer</span>.<span>encode</span>(<span>prompt</span>)  <span># Perform subword segmentation and add a special token &lt;|begin_of_text|&gt; indicating the start of the text at the beginning of the text. Dimension: [17]</span>
<span>print</span>(<span>tokens</span>)  <span># Check the segmentation result</span>
<span>tokens</span> <span>=</span> <span>torch</span>.<span>tensor</span>(<span>tokens</span>)  <span># Convert to tensor type for subsequent matrix calculations. [17]</span>

<span># Convert the token ids into a specific sequence of token subwords, which is only for display purposes and not actually needed</span>
<span>prompt_split_as_tokens</span> <span>=</span> [<span>tokenizer</span>.<span>decode</span>([<span>token</span>]) <span>for</span> <span>token</span> <span>in</span> <span>tokens</span>]
<span>print</span>(<span>prompt_split_as_tokens</span>)</pre></div>
<div data-snippet-clipboard-copy-content="[128000, 1820, 4320, 311, 279, 17139, 3488, 315, 2324, 11, 279, 15861, 11, 323, 4395, 374, 220]
['<|begin_of_text|>', 'the', ' answer', ' to', ' the', ' ultimate', ' question', ' of', ' life', ',', ' the', ' universe', ',', ' and', ' everything', ' is', ' ']"><pre><code>[128000, 1820, 4320, 311, 279, 17139, 3488, 315, 2324, 11, 279, 15861, 11, 323, 4395, 374, 220]
['&lt;|begin_of_text|&gt;', 'the', ' answer', ' to', ' the', ' ultimate', ' question', ' of', ' life', ',', ' the', ' universe', ',', ' and', ' everything', ' is', ' ']
</code></pre></div>
<p dir="auto"><h2 tabindex="-1" dir="auto">Convert the sequence of token ids into embeddings</h2><a id="user-content-convert-the-sequence-of-token-ids-into-embeddings" aria-label="Permalink: Convert the sequence of token ids into embeddings" href="#convert-the-sequence-of-token-ids-into-embeddings"></a></p>
<div dir="auto"><p>Sorry, this is the only part in this codebase where I use built-in neural network modules.
<br>
In short, our original [17x1] token sequence is now [17x4096], that is, 17 embeddings of length 4096 (one for each token).
</p><p>
Note: Pay attention to the change in the shape of this tensor, which will make it easier for you to understand the entire process (And i will annotate the shape changes in all steps).</p></div>
<p><a target="_blank" rel="noopener noreferrer" href="https://github.com/therealoliver/Deepdive-llama3-from-scratch/blob/main/images/embeddings.png"><img src="https://github.com/therealoliver/Deepdive-llama3-from-scratch/raw/main/images/embeddings.png" width="600"></a>
</p>
<div dir="auto" data-snippet-clipboard-copy-content="# Create an embedding network layer to map discrete token ids to a continuous vector space
embedding_layer = torch.nn.Embedding(vocab_size, dim)

# Update the parameters of the embedding network with the pre-trained parameter values in Llama3
embedding_layer.weight.data.copy_(model[&quot;tok_embeddings.weight&quot;])

# Use the embedding network to convert the input sequence of token ids into vector representations
# The embedding network only looks up the corresponding vectors based on the ids in a dictionary and does not involve interactions between tokens.
# [17] -> [17x4096]
token_embeddings_unnormalized = embedding_layer(tokens).to(torch.bfloat16)  # By default, it is in full-precision float32. Here, we change it to the half-precision format to reduce memory usage.

token_embeddings_unnormalized.shape"><pre><span># Create an embedding network layer to map discrete token ids to a continuous vector space</span>
<span>embedding_layer</span> <span>=</span> <span>torch</span>.<span>nn</span>.<span>Embedding</span>(<span>vocab_size</span>, <span>dim</span>)

<span># Update the parameters of the embedding network with the pre-trained parameter values in Llama3</span>
<span>embedding_layer</span>.<span>weight</span>.<span>data</span>.<span>copy_</span>(<span>model</span>[<span>"tok_embeddings.weight"</span>])

<span># Use the embedding network to convert the input sequence of token ids into vector representations</span>
<span># The embedding network only looks up the corresponding vectors based on the ids in a dictionary and does not involve interactions between tokens.</span>
<span># [17] -&gt; [17x4096]</span>
<span>token_embeddings_unnormalized</span> <span>=</span> <span>embedding_layer</span>(<span>tokens</span>).<span>to</span>(<span>torch</span>.<span>bfloat16</span>)  <span># By default, it is in full-precision float32. Here, we change it to the half-precision format to reduce memory usage.</span>

<span>token_embeddings_unnormalized</span>.<span>shape</span></pre></div>

<p dir="auto"><h2 tabindex="-1" dir="auto">Build the first Transformer block</h2><a id="user-content-build-the-first-transformer-block" aria-label="Permalink: Build the first Transformer block" href="#build-the-first-transformer-block"></a></p>
<p dir="auto">From the pre-trained parameters involved in the first Transformer block shown below, it includes:</p>
<ol dir="auto">
<li>Two normalizations (attention_norm and ffn_norm)</li>
<li>Implementation of the attention mechanism (4 attention.w)</li>
<li>Implementation of the feed-forward network layer (3 feed_forward.w)</li>
<li>(Of course, it also includes two residual connection operations that do not require pre-trained parameters)</li>
</ol>
<p dir="auto">In general, the operation process in a Transformer block is as follows:
<br>
Normalization -&gt; Multi-head self-attention -&gt; Residual connection -&gt; Normalization -&gt; Feed-forward neural network -&gt; Residual connection</p>
<div dir="auto" data-snippet-clipboard-copy-content="# Display all the weight parameters and their shapes of the first Transformer block
for k, v in model.items():
    if not k.startswith('layers'):
        continue
    if k.startswith('layers.1'):
        break
    print(k, v.shape)"><pre><span># Display all the weight parameters and their shapes of the first Transformer block</span>
<span>for</span> <span>k</span>, <span>v</span> <span>in</span> <span>model</span>.<span>items</span>():
    <span>if</span> <span>not</span> <span>k</span>.<span>startswith</span>(<span>'layers'</span>):
        <span>continue</span>
    <span>if</span> <span>k</span>.<span>startswith</span>(<span>'layers.1'</span>):
        <span>break</span>
    <span>print</span>(<span>k</span>, <span>v</span>.<span>shape</span>)</pre></div>
<div data-snippet-clipboard-copy-content="layers.0.attention.wq.weight torch.Size([4096, 4096])
layers.0.attention.wk.weight torch.Size([1024, 4096])
layers.0.attention.wv.weight torch.Size([1024, 4096])
layers.0.attention.wo.weight torch.Size([4096, 4096])
layers.0.feed_forward.w1.weight torch.Size([14336, 4096])
layers.0.feed_forward.w3.weight torch.Size([14336, 4096])
layers.0.feed_forward.w2.weight torch.Size([4096, 14336])
layers.0.attention_norm.weight torch.Size([4096])
layers.0.ffn_norm.weight torch.Size([4096])"><pre><code>layers.0.attention.wq.weight torch.Size([4096, 4096])
layers.0.attention.wk.weight torch.Size([1024, 4096])
layers.0.attention.wv.weight torch.Size([1024, 4096])
layers.0.attention.wo.weight torch.Size([4096, 4096])
layers.0.feed_forward.w1.weight torch.Size([14336, 4096])
layers.0.feed_forward.w3.weight torch.Size([14336, 4096])
layers.0.feed_forward.w2.weight torch.Size([4096, 14336])
layers.0.attention_norm.weight torch.Size([4096])
layers.0.ffn_norm.weight torch.Size([4096])
</code></pre></div>
<p dir="auto">There are two points to note here:</p>
<ol dir="auto">
<li>The shape of the weight matrix of a neural network is (output dimension, input dimension). During the calculation, the parameter matrix W will be transposed to (input dimension, output dimension) and then multiplied by the input X, i.e., the output Y = XW.T. You will see this in the subsequent calculations.</li>
<li>Since Llama3 uses the grouped attention mechanism, every 4 query heads will share a set of kv vectors (for details, see the section on the details of the configuration file above). Therefore, the dimension of the weight matrix of kv is [1024, 4096], which is 1/4 of that of q ([4096, 4096]).</li>
</ol>
<p dir="auto"><h2 tabindex="-1" dir="auto">Normalization</h2><a id="user-content-normalization" aria-label="Permalink: Normalization" href="#normalization"></a></p>
<div dir="auto"><p>The normalization operation aims to constrain the scale differences in the data, avoiding issues such as unstable training process caused by excessive differences in vector values.
</p><p>
After normalization, the shape of the tensor remains [17x4096], the same as that of the embedding.</p></div>
<p><a target="_blank" rel="noopener noreferrer" href="https://github.com/therealoliver/Deepdive-llama3-from-scratch/blob/main/images/norm.png"><img src="https://github.com/therealoliver/Deepdive-llama3-from-scratch/raw/main/images/norm.png" width="600"></a>
</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Using RMS normalization for embeddings</h3><a id="user-content-using-rms-normalization-for-embeddings" aria-label="Permalink: Using RMS normalization for embeddings" href="#using-rms-normalization-for-embeddings"></a></p>
<p dir="auto">Llama3 uses the Root Mean Square (RMS) normalization method, and its calculation formula is shown in the figure below.
<br>
It should be noted that we need a norm_eps parameter (from the configurations) because we don't want to accidentally set the RMS to 0 and perform a division by zero.
<br>
The formula is as follows:</p>
<p><a target="_blank" rel="noopener noreferrer" href="https://github.com/therealoliver/Deepdive-llama3-from-scratch/blob/main/images/rms.png"><img src="https://github.com/therealoliver/Deepdive-llama3-from-scratch/raw/main/images/rms.png" width="600"></a>
</p>

<p dir="auto">In addition, you may have noticed the gi parameter in the formula. This is a scaling factor learned during the model training process, used to scale the normalization result of each dimension again to enhance the model's expressive ability. Its dimension is the same as the feature dimension of the embedding, i.e., [4096].</p>
<div dir="auto" data-snippet-clipboard-copy-content="# Define the calculation function for RMS normalization
# Each token will be normalized independently
# norm_weights is the pre-trained scaling factor (i.e., gi in the formula) to enhance the model's representational ability. It can be loaded from the model file and has 4096 dimensions
# torch.rsqrt used to calculates the reciprocal of the square root of a tensor, i.e., 1/RMS(a)
def rms_norm(tensor, norm_weights):
    return (tensor * torch.rsqrt(tensor.pow(2).mean(-1, keepdim=True) + norm_eps)) * norm_weights"><pre><span># Define the calculation function for RMS normalization</span>
<span># Each token will be normalized independently</span>
<span># norm_weights is the pre-trained scaling factor (i.e., gi in the formula) to enhance the model's representational ability. It can be loaded from the model file and has 4096 dimensions</span>
<span># torch.rsqrt used to calculates the reciprocal of the square root of a tensor, i.e., 1/RMS(a)</span>
<span>def</span> <span>rms_norm</span>(<span>tensor</span>, <span>norm_weights</span>):
    <span>return</span> (<span>tensor</span> <span>*</span> <span>torch</span>.<span>rsqrt</span>(<span>tensor</span>.<span>pow</span>(<span>2</span>).<span>mean</span>(<span>-</span><span>1</span>, <span>keepdim</span><span>=</span><span>True</span>) <span>+</span> <span>norm_eps</span>)) <span>*</span> <span>norm_weights</span></pre></div>
<div dir="auto" data-snippet-clipboard-copy-content="# Normalize the input
token_embeddings = rms_norm(token_embeddings_unnormalized, model[&quot;layers.0.attention_norm.weight&quot;])  # [17x4096] &amp; [4096] -> [17x4096]
model[&quot;layers.0.attention_norm.weight&quot;].shape, token_embeddings.shape"><pre><span># Normalize the input</span>
<span>token_embeddings</span> <span>=</span> <span>rms_norm</span>(<span>token_embeddings_unnormalized</span>, <span>model</span>[<span>"layers.0.attention_norm.weight"</span>])  <span># [17x4096] &amp; [4096] -&gt; [17x4096]</span>
<span>model</span>[<span>"layers.0.attention_norm.weight"</span>].<span>shape</span>, <span>token_embeddings</span>.<span>shape</span></pre></div>
<div data-snippet-clipboard-copy-content="(torch.Size([4096]), torch.Size([17, 4096]))"><pre><code>(torch.Size([4096]), torch.Size([17, 4096]))
</code></pre></div>
<p dir="auto"><h2 tabindex="-1" dir="auto">Implementing the single-head attention mechanism from scratch</h2><a id="user-content-implementing-the-single-head-attention-mechanism-from-scratch" aria-label="Permalink: Implementing the single-head attention mechanism from scratch" href="#implementing-the-single-head-attention-mechanism-from-scratch"></a></p>
<p dir="auto">In the calculation of multi-head attention on each layer, 32 heads are involved. However, the calculation processes of these heads are completely identical and independent. Therefore, in this section, we will first implement the single-head attention calculation process, and expand it to multi-head calculation in the next section.</p>
<p><a target="_blank" rel="noopener noreferrer" href="https://github.com/therealoliver/Deepdive-llama3-from-scratch/blob/main/images/qkv.png"><img src="https://github.com/therealoliver/Deepdive-llama3-from-scratch/raw/main/images/qkv.png" width="600"></a>
</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">
The core calculation of the attention mechanism is the calculation formula shown in the following figure.
</h3><a id="user-content-the-core-calculation-of-the-attention-mechanism-is-the-calculation-formula-shown-in-the-following-figure" aria-label="Permalink: 
The core calculation of the attention mechanism is the calculation formula shown in the following figure.
" href="#the-core-calculation-of-the-attention-mechanism-is-the-calculation-formula-shown-in-the-following-figure"></a></p>
<ol dir="auto">
<li>We need to obtain the query, key, and value vectors by performing a linear mapping on the input embeddings.</li>
<li>Subsequently, based on the QK vectors, we obtain the attention weights between tokens, that is, for each token, the scores of the importance or relevance of other tokens to it.</li>
<li>Finally, based on the attention weights, we weight the value vectors to obtain the attention results corresponding to each token.</li>
</ol>
<p><a target="_blank" rel="noopener noreferrer" href="https://github.com/therealoliver/Deepdive-llama3-from-scratch/blob/main/images/softmax.png"><img src="https://github.com/therealoliver/Deepdive-llama3-from-scratch/raw/main/images/softmax.png" width="600px"></a>
</p>
<p dir="auto">Back to the point. Let's first load the attention heads of the first-layer Transformer.
<br>
&gt; When we load the query, key, value, and output weight matrices from the model (the output weight is used for information fusion among multiple heads to generate the final attention output), we will notice that their shapes are: [4096x4096], [1024x4096], [1024x4096], [4096x4096].
<br>
&gt; At first glance, this seems strange because ideally, we would like the q, k, v of each head to be independent of each other (in which case their shapes would be: 32x[128x4096], 8x[128x4096], 8x[128x4096]).
<br>
&gt; The author of the code binds them together because this helps parallelize the multiplication calculation of the attention heads.
<br>
&gt; But we will unfold everything...</p>
<div dir="auto" data-snippet-clipboard-copy-content="# Show the shapes of the attention weight matrices of the current q, k, v and o.
print(
    model[&quot;layers.0.attention.wq.weight&quot;].shape,  # [4096x4096]
    model[&quot;layers.0.attention.wk.weight&quot;].shape,  # [1024x4096]
    model[&quot;layers.0.attention.wv.weight&quot;].shape,  # [1024x4096]
    model[&quot;layers.0.attention.wo.weight&quot;].shape   # [4096x4096]
)"><pre><span># Show the shapes of the attention weight matrices of the current q, k, v and o.</span>
<span>print</span>(
    <span>model</span>[<span>"layers.0.attention.wq.weight"</span>].<span>shape</span>,  <span># [4096x4096]</span>
    <span>model</span>[<span>"layers.0.attention.wk.weight"</span>].<span>shape</span>,  <span># [1024x4096]</span>
    <span>model</span>[<span>"layers.0.attention.wv.weight"</span>].<span>shape</span>,  <span># [1024x4096]</span>
    <span>model</span>[<span>"layers.0.attention.wo.weight"</span>].<span>shape</span>   <span># [4096x4096]</span>
)</pre></div>
<div data-snippet-clipboard-copy-content="torch.Size([4096, 4096]) torch.Size([1024, 4096]) torch.Size([1024, 4096]) torch.Size([4096, 4096])"><pre><code>torch.Size([4096, 4096]) torch.Size([1024, 4096]) torch.Size([1024, 4096]) torch.Size([4096, 4096])
</code></pre></div>
<p dir="auto"><h3 tabindex="-1" dir="auto">Obtain the QKV vectors corresponding to the input tokens</h3><a id="user-content-obtain-the-qkv-vectors-corresponding-to-the-input-tokens" aria-label="Permalink: Obtain the QKV vectors corresponding to the input tokens" href="#obtain-the-qkv-vectors-corresponding-to-the-input-tokens"></a></p>
<p dir="auto">In this section, we will convert the input token embeddings into query, key, and value vectors for the purpose of attention mechanism computation.</p>
<p dir="auto"><h4 tabindex="-1" dir="auto">Obtain the query vector</h4><a id="user-content-obtain-the-query-vector" aria-label="Permalink: Obtain the query vector" href="#obtain-the-query-vector"></a></p>
<p dir="auto"><h5 tabindex="-1" dir="auto">Unfold the query weight matrix</h5><a id="user-content-unfold-the-query-weight-matrix" aria-label="Permalink: Unfold the query weight matrix" href="#unfold-the-query-weight-matrix"></a></p>
<p dir="auto">We will first unfold the queries from multiple attention heads, and the final shape will be [32x128x4096].
<br>
Here, 32 is the number of attention heads in Llama3, 128 is the vector dimension of the query head, and 4096 is the dimension of the token embedding (the reason why the dimension of the embedding is in the last dimension is that when multiplying the input and the weight, it is usually = X*W.T, that is, multiplying by the transpose of the weight).</p>
<div dir="auto" data-snippet-clipboard-copy-content="# Load and modify the shape of the query weight matrix of layers.0 to unfold it in the form of multiple heads
q_layer0 = model[&quot;layers.0.attention.wq.weight&quot;]  # Default shape is [4096x4096]
head_dim = q_layer0.shape[0] // n_heads  # Dimension of the attention head, 4096/32 = 128
q_layer0 = q_layer0.view(n_heads, head_dim, dim)  # Unfolded dimension, [32x128x4096]
q_layer0.shape"><pre><span># Load and modify the shape of the query weight matrix of layers.0 to unfold it in the form of multiple heads</span>
<span>q_layer0</span> <span>=</span> <span>model</span>[<span>"layers.0.attention.wq.weight"</span>]  <span># Default shape is [4096x4096]</span>
<span>head_dim</span> <span>=</span> <span>q_layer0</span>.<span>shape</span>[<span>0</span>] <span>//</span> <span>n_heads</span>  <span># Dimension of the attention head, 4096/32 = 128</span>
<span>q_layer0</span> <span>=</span> <span>q_layer0</span>.<span>view</span>(<span>n_heads</span>, <span>head_dim</span>, <span>dim</span>)  <span># Unfolded dimension, [32x128x4096]</span>
<span>q_layer0</span>.<span>shape</span></pre></div>
<div data-snippet-clipboard-copy-content="torch.Size([32, 128, 4096])"><pre><code>torch.Size([32, 128, 4096])
</code></pre></div>
<p dir="auto"><h5 tabindex="-1" dir="auto">Obtain the first head</h5><a id="user-content-obtain-the-first-head" aria-label="Permalink: Obtain the first head" href="#obtain-the-first-head"></a></p>
<p dir="auto">Here, I access the first head of the query weight matrix in the first layer. The shape of this query weight matrix is [128x4096].</p>
<div dir="auto" data-snippet-clipboard-copy-content="# Extract the weights of the first head
q_layer0_head0 = q_layer0[0]  # [32x128x4096] -> [128x4096]
q_layer0_head0.shape"><pre><span># Extract the weights of the first head</span>
<span>q_layer0_head0</span> <span>=</span> <span>q_layer0</span>[<span>0</span>]  <span># [32x128x4096] -&gt; [128x4096]</span>
<span>q_layer0_head0</span>.<span>shape</span></pre></div>

<p dir="auto"><h5 tabindex="-1" dir="auto">Multiply the token embeddings by the query weights to obtain the query vectors corresponding to the tokens</h5><a id="user-content-multiply-the-token-embeddings-by-the-query-weights-to-obtain-the-query-vectors-corresponding-to-the-tokens" aria-label="Permalink: Multiply the token embeddings by the query weights to obtain the query vectors corresponding to the tokens" href="#multiply-the-token-embeddings-by-the-query-weights-to-obtain-the-query-vectors-corresponding-to-the-tokens"></a></p>
<p dir="auto">Here, you can see that the shape of the result is [17x128]. This is because we have 17 tokens, and for each token, there is a query vector of length 128.</p>
<p><a target="_blank" rel="noopener noreferrer" href="https://github.com/therealoliver/Deepdive-llama3-from-scratch/blob/main/images/q_per_token.png"><img src="https://github.com/therealoliver/Deepdive-llama3-from-scratch/raw/main/images/q_per_token.png" width="600"></a>
</p>
<div dir="auto" data-snippet-clipboard-copy-content="# Calculate the query values of inputs on the first query head
# Q0_head0 = XW0_Q_head0.T
q_per_token = torch.matmul(token_embeddings, q_layer0_head0.T)  # [17x4096] x [4096x128] = [17x128]
q_per_token.shape"><pre><span># Calculate the query values of inputs on the first query head</span>
<span># Q0_head0 = XW0_Q_head0.T</span>
<span>q_per_token</span> <span>=</span> <span>torch</span>.<span>matmul</span>(<span>token_embeddings</span>, <span>q_layer0_head0</span>.<span>T</span>)  <span># [17x4096] x [4096x128] = [17x128]</span>
<span>q_per_token</span>.<span>shape</span></pre></div>

<p dir="auto"><h4 tabindex="-1" dir="auto">Obtain the key vector (almost the same as the query vector)</h4><a id="user-content-obtain-the-key-vector-almost-the-same-as-the-query-vector" aria-label="Permalink: Obtain the key vector (almost the same as the query vector)" href="#obtain-the-key-vector-almost-the-same-as-the-query-vector"></a></p>
<p><a target="_blank" rel="noopener noreferrer" href="https://github.com/therealoliver/Deepdive-llama3-from-scratch/blob/main/images/keys.png"><img src="https://github.com/therealoliver/Deepdive-llama3-from-scratch/raw/main/images/keys.png" width="600px"></a>
</p>
<p dir="auto">I want to take a lazy, so I won't elaborate on the calculation process of the key vector again. Orz. The only thing you need to remember is:
<br>
&gt; The key also generates a 128-dimensional vector.
<br>
&gt; The number of parameters of the weight matrix of the key is only 1/4 of that of the query, because the weight of each key is shared by 4 heads simultaneously to reduce the required amount of calculation.</p>
<div dir="auto" data-snippet-clipboard-copy-content="# Load and modify the shape of the key weight matrix of layers.0 to expand it in a multi-head form
# Different from the query weight matrix, the key has 8 attention heads, so the number of parameters is 1/4 of that of the query matrix
k_layer0 = model[&quot;layers.0.attention.wk.weight&quot;]  # [1024x4096]
k_layer0 = k_layer0.view(n_kv_heads, k_layer0.shape[0] // n_kv_heads, dim) # [8x128x4096]
k_layer0.shape"><pre><span># Load and modify the shape of the key weight matrix of layers.0 to expand it in a multi-head form</span>
<span># Different from the query weight matrix, the key has 8 attention heads, so the number of parameters is 1/4 of that of the query matrix</span>
<span>k_layer0</span> <span>=</span> <span>model</span>[<span>"layers.0.attention.wk.weight"</span>]  <span># [1024x4096]</span>
<span>k_layer0</span> <span>=</span> <span>k_layer0</span>.<span>view</span>(<span>n_kv_heads</span>, <span>k_layer0</span>.<span>shape</span>[<span>0</span>] <span>//</span> <span>n_kv_heads</span>, <span>dim</span>) <span># [8x128x4096]</span>
<span>k_layer0</span>.<span>shape</span></pre></div>
<div data-snippet-clipboard-copy-content="torch.Size([8, 128, 4096])"><pre><code>torch.Size([8, 128, 4096])
</code></pre></div>
<div dir="auto" data-snippet-clipboard-copy-content="# Extract the weights of the first head
k_layer0_head0 = k_layer0[0]  # [8x128x4096] -> [128x4096]
k_layer0_head0.shape"><pre><span># Extract the weights of the first head</span>
<span>k_layer0_head0</span> <span>=</span> <span>k_layer0</span>[<span>0</span>]  <span># [8x128x4096] -&gt; [128x4096]</span>
<span>k_layer0_head0</span>.<span>shape</span></pre></div>

<div dir="auto" data-snippet-clipboard-copy-content="# Calculate the key vectors corresponding to the inputs of the first head
# K0_head0 = XW0_K_head0.T
k_per_token = torch.matmul(token_embeddings, k_layer0_head0.T)  # [17x4096] x [4096x128] = [17x128]
k_per_token.shape"><pre><span># Calculate the key vectors corresponding to the inputs of the first head</span>
<span># K0_head0 = XW0_K_head0.T</span>
<span>k_per_token</span> <span>=</span> <span>torch</span>.<span>matmul</span>(<span>token_embeddings</span>, <span>k_layer0_head0</span>.<span>T</span>)  <span># [17x4096] x [4096x128] = [17x128]</span>
<span>k_per_token</span>.<span>shape</span></pre></div>

<p dir="auto"><h4 tabindex="-1" dir="auto">Obtain the value vector (almost the same as the key vector)</h4><a id="user-content-obtain-the-value-vector-almost-the-same-as-the-key-vector" aria-label="Permalink: Obtain the value vector (almost the same as the key vector)" href="#obtain-the-value-vector-almost-the-same-as-the-key-vector"></a></p>
<p><a target="_blank" rel="noopener noreferrer" href="https://github.com/therealoliver/Deepdive-llama3-from-scratch/blob/main/images/value.png"><img src="https://github.com/therealoliver/Deepdive-llama3-from-scratch/raw/main/images/value.png" width="600px"></a>
</p>
<p dir="auto">&gt; Similar to the key weights, the value weights are also shared by every 4 attention heads (to save computation).
<br>
&gt; Therefore, the shape of the value weight matrix is [8x128x4096].</p>
<div dir="auto" data-snippet-clipboard-copy-content="# Load and modify the shape of the value weight matrix of layers.0 to expand it in a multi-head form
# Similar to the key weight matrix, the value also has 8 attention heads, so the number of parameters is also 1/4 of that of the query matrix
v_layer0 = model[&quot;layers.0.attention.wv.weight&quot;]  # [1024x4096]
v_layer0 = v_layer0.view(n_kv_heads, v_layer0.shape[0] // n_kv_heads, dim)  # [1024x4096] -> [8x128x4096]
v_layer0.shape"><pre><span># Load and modify the shape of the value weight matrix of layers.0 to expand it in a multi-head form</span>
<span># Similar to the key weight matrix, the value also has 8 attention heads, so the number of parameters is also 1/4 of that of the query matrix</span>
<span>v_layer0</span> <span>=</span> <span>model</span>[<span>"layers.0.attention.wv.weight"</span>]  <span># [1024x4096]</span>
<span>v_layer0</span> <span>=</span> <span>v_layer0</span>.<span>view</span>(<span>n_kv_heads</span>, <span>v_layer0</span>.<span>shape</span>[<span>0</span>] <span>//</span> <span>n_kv_heads</span>, <span>dim</span>)  <span># [1024x4096] -&gt; [8x128x4096]</span>
<span>v_layer0</span>.<span>shape</span></pre></div>
<div data-snippet-clipboard-copy-content="torch.Size([8, 128, 4096])"><pre><code>torch.Size([8, 128, 4096])
</code></pre></div>
<div dir="auto" data-snippet-clipboard-copy-content="# Extract the weights of the first head
v_layer0_head0 = v_layer0[0]  # [8x128x4096] -> [128x4096]
v_layer0_head0.shape"><pre><span># Extract the weights of the first head</span>
<span>v_layer0_head0</span> <span>=</span> <span>v_layer0</span>[<span>0</span>]  <span># [8x128x4096] -&gt; [128x4096]</span>
<span>v_layer0_head0</span>.<span>shape</span></pre></div>

<div dir="auto" data-snippet-clipboard-copy-content="# Calculate the value vectors corresponding to the inputs of the first head
# V0_head0 = XW0_V_head0.T
v_per_token = torch.matmul(token_embeddings, v_layer0_head0.T)  # [17x4096] x [4096x128] = [17x128]
v_per_token.shape"><pre><span># Calculate the value vectors corresponding to the inputs of the first head</span>
<span># V0_head0 = XW0_V_head0.T</span>
<span>v_per_token</span> <span>=</span> <span>torch</span>.<span>matmul</span>(<span>token_embeddings</span>, <span>v_layer0_head0</span>.<span>T</span>)  <span># [17x4096] x [4096x128] = [17x128]</span>
<span>v_per_token</span>.<span>shape</span></pre></div>

<p dir="auto"><h3 tabindex="-1" dir="auto">Add positional information to the query and key vectors</h3><a id="user-content-add-positional-information-to-the-query-and-key-vectors" aria-label="Permalink: Add positional information to the query and key vectors" href="#add-positional-information-to-the-query-and-key-vectors"></a></p>
<ul dir="auto">
<li>For natural language, the sequential relationship and relative positions between words are extremely important. For example, "The dog bites the man" and "The man bites the dog" have completely different semantic information. Moreover, our intuition also tells us that the correlation between relatively close words is usually greater than that between distant words.</li>
<li>Therefore, we need to provide positional information between tokens during the attention calculation process, so that the model can better capture the dependencies in the sequence.</li>
<li>Why add it to the query and key vectors? Because the query and key vectors are used to calculate the attention weights, i.e., the importance of each token to other tokens. This requires both of them to know the positions and relative positional relationships of any two tokens when calculating the similarity between them.</li>
<li>Why not add it to the value vectors? Because the value vectors are only used for weighted summation. The positional information has already been considered in the interaction between the query and the key. Therefore, the value vectors only need to provide content information.</li>
</ul>
<p dir="auto">We will use RoPE (Rotary Position Encoding) to add positional information to these vectors.</p>
<p dir="auto"><h4 tabindex="-1" dir="auto">Rotary Position Encoding (RoPE)</h4><a id="user-content-rotary-position-encoding-rope" aria-label="Permalink: Rotary Position Encoding (RoPE)" href="#rotary-position-encoding-rope"></a></p>
<div dir="auto"><p>You can watch this video to understand its mathematical principles in detail (this is also the one I watched):
<a href="https://www.youtube.com/watch?v=o29P0Kpobz0&amp;t=530s" rel="nofollow">https://www.youtube.com/watch?v=o29P0Kpobz0&amp;t=530s</a></p><p>
The general idea of RoPE is to regard the vector as being in the complex space, and then generate specific rotation matrix based on positions. By multiplying the vector with the rotation matrix, rotation in the complex space can be achieved, thereby adding the relative position information to the vector. (That is, taking the positional relationship of the input vectors as a rotation at different angles in a complex space.)
<br>
(Similar to the rotation of planar position coordinates around an axis through the multiplication of trigonometric-function-based matrices in robot kinematics.)
</p><p>
RoPE is usually applied to the query and key vectors in the self-attention mechanism. When calculating the attention scores, the query and key vectors are first rotated based on the corresponding rotation matrix of RoPE. Then, operations such as dot-product calculation and softmax normalization are performed. In this way, the Transformer can take positional information into account when calculating attentions and better capture the dependencies in the text.</p></div>
<p><a target="_blank" rel="noopener noreferrer" href="https://github.com/therealoliver/Deepdive-llama3-from-scratch/blob/main/images/rope.png"><img src="https://github.com/therealoliver/Deepdive-llama3-from-scratch/raw/main/images/rope.png" width="600"></a>
</p>

<p dir="auto"><h3 tabindex="-1" dir="auto">
The specific calculation process of RoPE is as follows:
</h3><a id="user-content-the-specific-calculation-process-of-rope-is-as-follows" aria-label="Permalink: 
The specific calculation process of RoPE is as follows:
" href="#the-specific-calculation-process-of-rope-is-as-follows"></a></p>
<ol dir="auto">
<li>Divide the dimensions of each vector into pairs (because the derivation of high-dimensional rotation matrices is complex, and excessively high dimensions will significantly increase the computational complexity, while the formulas for two-dimensional rotation are relatively mature and simple, making them easy to calculate).</li>
<li>For each pair, obtain <math-renderer data-static-url="https://github.githubassets.com/static" data-run-id="19d90f3d5e551008e8fb4bc953d823b8">$\Large \theta=\frac{1}{rope\_theta^{i/D}}$</math-renderer>, where <math-renderer data-static-url="https://github.githubassets.com/static" data-run-id="19d90f3d5e551008e8fb4bc953d823b8">$i$</math-renderer> is the <math-renderer data-static-url="https://github.githubassets.com/static" data-run-id="19d90f3d5e551008e8fb4bc953d823b8">$i$</math-renderer>-th pair and <math-renderer data-static-url="https://github.githubassets.com/static" data-run-id="19d90f3d5e551008e8fb4bc953d823b8">$D$</math-renderer> is the total number of pairs. That is, the positional information of the current dimension pair within the vector.</li>
<li>For each vector, obtain <math-renderer data-static-url="https://github.githubassets.com/static" data-run-id="19d90f3d5e551008e8fb4bc953d823b8">$\Large m$</math-renderer>, which represents that the vector corresponds to the <math-renderer data-static-url="https://github.githubassets.com/static" data-run-id="19d90f3d5e551008e8fb4bc953d823b8">$m$</math-renderer>-th token. That is, the positional information of the current vector within the entire input vectors.</li>
<li>For each pair, <a target="_blank" rel="noopener noreferrer" href="https://github.com/therealoliver/Deepdive-llama3-from-scratch/blob/main/images/pmatrix.png"><img src="https://github.com/therealoliver/Deepdive-llama3-from-scratch/raw/main/images/pmatrix.png" alt="png"></a> , where <math-renderer data-static-url="https://github.githubassets.com/static" data-run-id="19d90f3d5e551008e8fb4bc953d823b8">$res$</math-renderer> is the result of rotating the vector pair by <math-renderer data-static-url="https://github.githubassets.com/static" data-run-id="19d90f3d5e551008e8fb4bc953d823b8">$m\theta$</math-renderer> degrees in the complex space.</li>
<li>Perform the above calculations on all dimension pairs of all vectors to obtain the final RoPE result.</li>
</ol>
<p><a target="_blank" rel="noopener noreferrer" href="https://github.com/therealoliver/Deepdive-llama3-from-scratch/blob/main/images/qsplit.png"><img src="https://github.com/therealoliver/Deepdive-llama3-from-scratch/raw/main/images/qsplit.png" width="600"></a>
</p>

<p dir="auto"><h3 tabindex="-1" dir="auto">
In the actual code implementation, to simplify the calculation process, the above-mentioned calculation based on the rotation matrix (Step 4) will be converted into a calculation in the complex number domain. The principle is as follows:
</h3><a id="user-content-in-the-actual-code-implementation-to-simplify-the-calculation-process-the-above-mentioned-calculation-based-on-the-rotation-matrix-step-4-will-be-converted-into-a-calculation-in-the-complex-number-domain-the-principle-is-as-follows" aria-label="Permalink: 
In the actual code implementation, to simplify the calculation process, the above-mentioned calculation based on the rotation matrix (Step 4) will be converted into a calculation in the complex number domain. The principle is as follows:
" href="#in-the-actual-code-implementation-to-simplify-the-calculation-process-the-above-mentioned-calculation-based-on-the-rotation-matrix-step-4-will-be-converted-into-a-calculation-in-the-complex-number-domain-the-principle-is-as-follows"></a></p>
<ol dir="auto">
<li>The rectangular coordinates <math-renderer data-static-url="https://github.githubassets.com/static" data-run-id="19d90f3d5e551008e8fb4bc953d823b8">$(x, y)$</math-renderer> can be regarded as the coordinate representation of the complex number <math-renderer data-static-url="https://github.githubassets.com/static" data-run-id="19d90f3d5e551008e8fb4bc953d823b8">$\large x + yi$</math-renderer> on the complex plane.</li>
<li>The polar form of a complex number can be expressed as <math-renderer data-static-url="https://github.githubassets.com/static" data-run-id="19d90f3d5e551008e8fb4bc953d823b8">$\large re^{i\theta}$</math-renderer>, where <math-renderer data-static-url="https://github.githubassets.com/static" data-run-id="19d90f3d5e551008e8fb4bc953d823b8">$r$</math-renderer> is the modulus and <math-renderer data-static-url="https://github.githubassets.com/static" data-run-id="19d90f3d5e551008e8fb4bc953d823b8">$\theta$</math-renderer> is the angle.</li>
<li>The multiplication calculation in polar coordinates <math-renderer data-static-url="https://github.githubassets.com/static" data-run-id="19d90f3d5e551008e8fb4bc953d823b8">$\large r_1e^{i\theta_1} \times r_2e^{i\theta_2} = r_1r_2e^{i(\theta_1 + \theta_2)}$</math-renderer> can be regarded as increasing the length of coordinate_1 by <math-renderer data-static-url="https://github.githubassets.com/static" data-run-id="19d90f3d5e551008e8fb4bc953d823b8">$r_2$</math-renderer> times and rotating the angle by <math-renderer data-static-url="https://github.githubassets.com/static" data-run-id="19d90f3d5e551008e8fb4bc953d823b8">$\theta_2$</math-renderer> degrees.</li>
<li>Therefore, if you want to rotate the coordinates by <math-renderer data-static-url="https://github.githubassets.com/static" data-run-id="19d90f3d5e551008e8fb4bc953d823b8">$m\theta$</math-renderer> degrees, you can define a rotation factor <math-renderer data-static-url="https://github.githubassets.com/static" data-run-id="19d90f3d5e551008e8fb4bc953d823b8">$\large e^{im\theta}$</math-renderer> with a modulus of 1 and an angle of <math-renderer data-static-url="https://github.githubassets.com/static" data-run-id="19d90f3d5e551008e8fb4bc953d823b8">$m\theta$</math-renderer>. Multiplying it by the coordinates will be equivalent to the rotation method based on the rotation matrix.</li>
<li>In addition, according to Euler's formula, we have <math-renderer data-static-url="https://github.githubassets.com/static" data-run-id="19d90f3d5e551008e8fb4bc953d823b8">$\large re^{i\theta} = r\cos\theta + r\sin{\theta i} = x + yi$</math-renderer> and <math-renderer data-static-url="https://github.githubassets.com/static" data-run-id="19d90f3d5e551008e8fb4bc953d823b8">$\large e^{im\theta} = \cos{m\theta} + \sin{m\theta i}$</math-renderer>.</li>
<li>Therefore, rotating a two-dimensional coordinate <math-renderer data-static-url="https://github.githubassets.com/static" data-run-id="19d90f3d5e551008e8fb4bc953d823b8">$(x, y)$</math-renderer> by <math-renderer data-static-url="https://github.githubassets.com/static" data-run-id="19d90f3d5e551008e8fb4bc953d823b8">$m\theta$</math-renderer> degrees can be obtained through <math-renderer data-static-url="https://github.githubassets.com/static" data-run-id="19d90f3d5e551008e8fb4bc953d823b8">$\large re^{i\theta^\prime} \times e^{im\theta} = (x + yi) \times (\cos{m\theta} + \sin{m\theta i})$</math-renderer> (the product of two complex numbers).</li>
</ol>
<p dir="auto"><h4 tabindex="-1" dir="auto">Add positional information to the query vectors</h4><a id="user-content-add-positional-information-to-the-query-vectors" aria-label="Permalink: Add positional information to the query vectors" href="#add-positional-information-to-the-query-vectors"></a></p>
<div dir="auto"><p>In the following steps, we will first split the query vectors into pairs, and then perform angle rotation on each pair, as shown in the above steps.
</p><p>
Now we have a vector with a shape of [17x64x2]. This is obtained by splitting the 128-dimensional query vectors corresponding to each token in the prompt into 64 pairs, and each pair will be rotated by </p><math-renderer data-static-url="https://github.githubassets.com/static" data-run-id="19d90f3d5e551008e8fb4bc953d823b8">$m\theta$</math-renderer><p> degrees.</p></div>
<div dir="auto" data-snippet-clipboard-copy-content="# Split the query vectors in pairs along the dimension direction.
# .float() is for switch back to full precision to ensure the precision and numerical stability in the subsequent trigonometric function calculations.
# [17x128] -> [17x64x2]
q_per_token_split_into_pairs = q_per_token.float().view(q_per_token.shape[0], -1, 2)
q_per_token_split_into_pairs.shape"><pre><span># Split the query vectors in pairs along the dimension direction.</span>
<span># .float() is for switch back to full precision to ensure the precision and numerical stability in the subsequent trigonometric function calculations.</span>
<span># [17x128] -&gt; [17x64x2]</span>
<span>q_per_token_split_into_pairs</span> <span>=</span> <span>q_per_token</span>.<span>float</span>().<span>view</span>(<span>q_per_token</span>.<span>shape</span>[<span>0</span>], <span>-</span><span>1</span>, <span>2</span>)
<span>q_per_token_split_into_pairs</span>.<span>shape</span></pre></div>

<p dir="auto"><h3 tabindex="-1" dir="auto">
Start to obtain the complex-domain representation of the rotation matrix.
</h3><a id="user-content-start-to-obtain-the-complex-domain-representation-of-the-rotation-matrix" aria-label="Permalink: 
Start to obtain the complex-domain representation of the rotation matrix.
" href="#start-to-obtain-the-complex-domain-representation-of-the-rotation-matrix"></a></p>
<p><a target="_blank" rel="noopener noreferrer" href="https://github.com/therealoliver/Deepdive-llama3-from-scratch/blob/main/images/freq_cis.png"><img src="https://github.com/therealoliver/Deepdive-llama3-from-scratch/raw/main/images/freq_cis.png" width="600"></a>
</p>
<div dir="auto" data-snippet-clipboard-copy-content="# Calculate Œ∏. Step 1: Obtain i/D.
# [64]
zero_to_one_split_into_64_parts = torch.tensor(range(64))/64  # Each feature has 64 dimension pairs after segmentation, so 64 Œ∏ values are required
zero_to_one_split_into_64_parts"><pre><span># Calculate Œ∏. Step 1: Obtain i/D.</span>
<span># [64]</span>
<span>zero_to_one_split_into_64_parts</span> <span>=</span> <span>torch</span>.<span>tensor</span>(<span>range</span>(<span>64</span>))<span>/</span><span>64</span>  <span># Each feature has 64 dimension pairs after segmentation, so 64 Œ∏ values are required</span>
<span>zero_to_one_split_into_64_parts</span></pre></div>
<div data-snippet-clipboard-copy-content="tensor([0.0000, 0.0156, 0.0312, 0.0469, 0.0625, 0.0781, 0.0938, 0.1094, 0.1250,
        0.1406, 0.1562, 0.1719, 0.1875, 0.2031, 0.2188, 0.2344, 0.2500, 0.2656,
        0.2812, 0.2969, 0.3125, 0.3281, 0.3438, 0.3594, 0.3750, 0.3906, 0.4062,
        0.4219, 0.4375, 0.4531, 0.4688, 0.4844, 0.5000, 0.5156, 0.5312, 0.5469,
        0.5625, 0.5781, 0.5938, 0.6094, 0.6250, 0.6406, 0.6562, 0.6719, 0.6875,
        0.7031, 0.7188, 0.7344, 0.7500, 0.7656, 0.7812, 0.7969, 0.8125, 0.8281,
        0.8438, 0.8594, 0.8750, 0.8906, 0.9062, 0.9219, 0.9375, 0.9531, 0.9688,
        0.9844])"><pre><code>tensor([0.0000, 0.0156, 0.0312, 0.0469, 0.0625, 0.0781, 0.0938, 0.1094, 0.1250,
        0.1406, 0.1562, 0.1719, 0.1875, 0.2031, 0.2188, 0.2344, 0.2500, 0.2656,
        0.2812, 0.2969, 0.3125, 0.3281, 0.3438, 0.3594, 0.3750, 0.3906, 0.4062,
        0.4219, 0.4375, 0.4531, 0.4688, 0.4844, 0.5000, 0.5156, 0.5312, 0.5469,
        0.5625, 0.5781, 0.5938, 0.6094, 0.6250, 0.6406, 0.6562, 0.6719, 0.6875,
        0.7031, 0.7188, 0.7344, 0.7500, 0.7656, 0.7812, 0.7969, 0.8125, 0.8281,
        0.8438, 0.8594, 0.8750, 0.8906, 0.9062, 0.9219, 0.9375, 0.9531, 0.9688,
        0.9844])
</code></pre></div>
<div dir="auto" data-snippet-clipboard-copy-content="# Calculate Œ∏. Step 2: Obtain Œ∏.
# rope_theta is used to control information such as the periodicity of the position encoding.
# For details, please refer to the configuration information section.
freqs = 1.0 / (rope_theta ** zero_to_one_split_into_64_parts)  # [64]
freqs"><pre><span># Calculate Œ∏. Step 2: Obtain Œ∏.</span>
<span># rope_theta is used to control information such as the periodicity of the position encoding.</span>
<span># For details, please refer to the configuration information section.</span>
<span>freqs</span> <span>=</span> <span>1.0</span> <span>/</span> (<span>rope_theta</span> <span>**</span> <span>zero_to_one_split_into_64_parts</span>)  <span># [64]</span>
<span>freqs</span></pre></div>
<div data-snippet-clipboard-copy-content="tensor([1.0000e+00, 8.1462e-01, 6.6360e-01, 5.4058e-01, 4.4037e-01, 3.5873e-01,
        2.9223e-01, 2.3805e-01, 1.9392e-01, 1.5797e-01, 1.2869e-01, 1.0483e-01,
        8.5397e-02, 6.9566e-02, 5.6670e-02, 4.6164e-02, 3.7606e-02, 3.0635e-02,
        2.4955e-02, 2.0329e-02, 1.6560e-02, 1.3490e-02, 1.0990e-02, 8.9523e-03,
        7.2927e-03, 5.9407e-03, 4.8394e-03, 3.9423e-03, 3.2114e-03, 2.6161e-03,
        2.1311e-03, 1.7360e-03, 1.4142e-03, 1.1520e-03, 9.3847e-04, 7.6450e-04,
        6.2277e-04, 5.0732e-04, 4.1327e-04, 3.3666e-04, 2.7425e-04, 2.2341e-04,
        1.8199e-04, 1.4825e-04, 1.2077e-04, 9.8381e-05, 8.0143e-05, 6.5286e-05,
        5.3183e-05, 4.3324e-05, 3.5292e-05, 2.8750e-05, 2.3420e-05, 1.9078e-05,
        1.5542e-05, 1.2660e-05, 1.0313e-05, 8.4015e-06, 6.8440e-06, 5.5752e-06,
        4.5417e-06, 3.6997e-06, 3.0139e-06, 2.4551e-06])"><pre><code>tensor([1.0000e+00, 8.1462e-01, 6.6360e-01, 5.4058e-01, 4.4037e-01, 3.5873e-01,
        2.9223e-01, 2.3805e-01, 1.9392e-01, 1.5797e-01, 1.2869e-01, 1.0483e-01,
        8.5397e-02, 6.9566e-02, 5.6670e-02, 4.6164e-02, 3.7606e-02, 3.0635e-02,
        2.4955e-02, 2.0329e-02, 1.6560e-02, 1.3490e-02, 1.0990e-02, 8.9523e-03,
        7.2927e-03, 5.9407e-03, 4.8394e-03, 3.9423e-03, 3.2114e-03, 2.6161e-03,
        2.1311e-03, 1.7360e-03, 1.4142e-03, 1.1520e-03, 9.3847e-04, 7.6450e-04,
        6.2277e-04, 5.0732e-04, 4.1327e-04, 3.3666e-04, 2.7425e-04, 2.2341e-04,
        1.8199e-04, 1.4825e-04, 1.2077e-04, 9.8381e-05, 8.0143e-05, 6.5286e-05,
        5.3183e-05, 4.3324e-05, 3.5292e-05, 2.8750e-05, 2.3420e-05, 1.9078e-05,
        1.5542e-05, 1.2660e-05, 1.0313e-05, 8.4015e-06, 6.8440e-06, 5.5752e-06,
        4.5417e-06, 3.6997e-06, 3.0139e-06, 2.4551e-06])
</code></pre></div>
<div dir="auto" data-snippet-clipboard-copy-content="# Calculate mŒ∏
# 'outer' is for outer product calculation, and 'arange(17)' represents the m corresponding to each vector (since the input has 17 tokens, 17 m values are needed).
# The result has a shape of [17x64], meaning that each vector corresponding to a token has 64 mŒ∏ values, which are used to calculate the rotation of each of the 64 dimension pairs.
freqs_for_each_token = torch.outer(torch.arange(17), freqs)  # [17] &amp; [64] -> [17x64]"><pre><span># Calculate mŒ∏</span>
<span># 'outer' is for outer product calculation, and 'arange(17)' represents the m corresponding to each vector (since the input has 17 tokens, 17 m values are needed).</span>
<span># The result has a shape of [17x64], meaning that each vector corresponding to a token has 64 mŒ∏ values, which are used to calculate the rotation of each of the 64 dimension pairs.</span>
<span>freqs_for_each_token</span> <span>=</span> <span>torch</span>.<span>outer</span>(<span>torch</span>.<span>arange</span>(<span>17</span>), <span>freqs</span>)  <span># [17] &amp; [64] -&gt; [17x64]</span></pre></div>
<div dir="auto" data-snippet-clipboard-copy-content="# Obtain (cosmŒ∏ + sinmŒ∏i), that is, convert mŒ∏ to the complex-number form
# Regard the rotation angle mŒ∏ as a polar-coordinate form with a modulus of 1, and then convert it to a complex-number representation
# The two inputs of 'polar' represent the modulus (set to 1, meaning only the angle is changed without affecting the length) and the angle (i.e., mŒ∏) respectively
freqs_cis = torch.polar(torch.ones_like(freqs_for_each_token), freqs_for_each_token)  # [17x64] -> [17x64]
print(freqs_cis.shape)

# View freqs_cis at some positions, just for display
token_to_show = [1, 3, 5]  # View the 2nd, 4th, and 6th rows
fig, axs = plt.subplots(1, len(token_to_show), figsize=(5 * len(token_to_show), 4))  # Generate a figure window with 3 sub-plots in 1 row and 3 columns
for i, index in enumerate(token_to_show):
    value = freqs_cis[index]
    for j, element in enumerate(value):
        # Plot a blue line from the origin to the coordinate point, with the real part as the x-coordinate and the imaginary part as the y-coordinate.
        axs[i].plot([0, element.real], [0, element.imag], color='blue', linewidth=1, label=f&quot;Index: {j}&quot;)
        # Draw red numerical annotations to represent the i-th pair of dimensions.
        axs[i].annotate(f&quot;{j}&quot;, xy=(element.real, element.imag), color='red')
    axs[i].set_xlabel('Real')
    axs[i].set_ylabel('Imaginary')
    axs[i].set_title(f'Plot of {index + 1}th of freqs_cis')
plt.show()

&quot;&quot;&quot;
Note: As shown in the figures, tokens in later positions have larger rotation angles, but within a single token, earlier vector dimensions have larger rotation angles.
      You can explore further to see if there are any mathematical reasons behind this if you are interested. X_X
&quot;&quot;&quot;"><pre><span># Obtain (cosmŒ∏ + sinmŒ∏i), that is, convert mŒ∏ to the complex-number form</span>
<span># Regard the rotation angle mŒ∏ as a polar-coordinate form with a modulus of 1, and then convert it to a complex-number representation</span>
<span># The two inputs of 'polar' represent the modulus (set to 1, meaning only the angle is changed without affecting the length) and the angle (i.e., mŒ∏) respectively</span>
<span>freqs_cis</span> <span>=</span> <span>torch</span>.<span>polar</span>(<span>torch</span>.<span>ones_like</span>(<span>freqs_for_each_token</span>), <span>freqs_for_each_token</span>)  <span># [17x64] -&gt; [17x64]</span>
<span>print</span>(<span>freqs_cis</span>.<span>shape</span>)

<span># View freqs_cis at some positions, just for display</span>
<span>token_to_show</span> <span>=</span> [<span>1</span>, <span>3</span>, <span>5</span>]  <span># View the 2nd, 4th, and 6th rows</span>
<span>fig</span>, <span>axs</span> <span>=</span> <span>plt</span>.<span>subplots</span>(<span>1</span>, <span>len</span>(<span>token_to_show</span>), <span>figsize</span><span>=</span>(<span>5</span> <span>*</span> <span>len</span>(<span>token_to_show</span>), <span>4</span>))  <span># Generate a figure window with 3 sub-plots in 1 row and 3 columns</span>
<span>for</span> <span>i</span>, <span>index</span> <span>in</span> <span>enumerate</span>(<span>token_to_show</span>):
    <span>value</span> <span>=</span> <span>freqs_cis</span>[<span>index</span>]
    <span>for</span> <span>j</span>, <span>element</span> <span>in</span> <span>enumerate</span>(<span>value</span>):
        <span># Plot a blue line from the origin to the coordinate point, with the real part as the x-coordinate and the imaginary part as the y-coordinate.</span>
        <span>axs</span>[<span>i</span>].<span>plot</span>([<span>0</span>, <span>element</span>.<span>real</span>], [<span>0</span>, <span>element</span>.<span>imag</span>], <span>color</span><span>=</span><span>'blue'</span>, <span>linewidth</span><span>=</span><span>1</span>, <span>label</span><span>=</span><span>f"Index: <span><span>{</span><span>j</span><span>}</span></span>"</span>)
        <span># Draw red numerical annotations to represent the i-th pair of dimensions.</span>
        <span>axs</span>[<span>i</span>].<span>annotate</span>(<span>f"<span><span>{</span><span>j</span><span>}</span></span>"</span>, <span>xy</span><span>=</span>(<span>element</span>.<span>real</span>, <span>element</span>.<span>imag</span>), <span>color</span><span>=</span><span>'red'</span>)
    <span>axs</span>[<span>i</span>].<span>set_xlabel</span>(<span>'Real'</span>)
    <span>axs</span>[<span>i</span>].<span>set_ylabel</span>(<span>'Imaginary'</span>)
    <span>axs</span>[<span>i</span>].<span>set_title</span>(<span>f'Plot of <span><span>{</span><span>index</span> <span>+</span> <span>1</span><span>}</span></span>th of freqs_cis'</span>)
<span>plt</span>.<span>show</span>()

<span>"""</span>
<span>Note: As shown in the figures, tokens in later positions have larger rotation angles, but within a single token, earlier vector dimensions have larger rotation angles.</span>
<span>      You can explore further to see if there are any mathematical reasons behind this if you are interested. X_X</span>
<span>"""</span></pre></div>

<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/therealoliver/Deepdive-llama3-from-scratch/blob/main/images/output_47_1.png"><img src="https://github.com/therealoliver/Deepdive-llama3-from-scratch/raw/main/images/output_47_1.png" alt="png"></a></p>
<div data-snippet-clipboard-copy-content="'\nNote: As shown in the figures, tokens in later positions have larger rotation angles, but within a single token, earlier vector dimensions have larger rotation angles.\n      You can explore further to see if there are any mathematical reasons behind this if you are interested. X_X\n'"><pre><code>'\nNote: As shown in the figures, tokens in later positions have larger rotation angles, but within a single token, earlier vector dimensions have larger rotation angles.\n      You can explore further to see if there are any mathematical reasons behind this if you are interested. X_X\n'
</code></pre></div>
<p dir="auto"><h3 tabindex="-1" dir="auto">
Now we have provided a complex number (an angle-changing vector) for each dimension pair of the query vector corresponding to each token.
</h3><a id="user-content-now-we-have-provided-a-complex-number-an-angle-changing-vector-for-each-dimension-pair-of-the-query-vector-corresponding-to-each-token" aria-label="Permalink: 
Now we have provided a complex number (an angle-changing vector) for each dimension pair of the query vector corresponding to each token.
" href="#now-we-have-provided-a-complex-number-an-angle-changing-vector-for-each-dimension-pair-of-the-query-vector-corresponding-to-each-token"></a></p>
<br>
Now we can convert our query (the one divided into pairs) into complex numbers and then rotate these queries through dot-product calculation. :)
<div dir="auto" data-snippet-clipboard-copy-content="# Obtain (x + yi)
# That is, convert the dimension pairs into complex numbers. After the conversion, the shape of the dimensions will change from [17x64x2] to [17x64].
q_per_token_as_complex_numbers = torch.view_as_complex(q_per_token_split_into_pairs)  # [17x64x2] -> [17x64]
q_per_token_as_complex_numbers.shape"><pre><span># Obtain (x + yi)</span>
<span># That is, convert the dimension pairs into complex numbers. After the conversion, the shape of the dimensions will change from [17x64x2] to [17x64].</span>
<span>q_per_token_as_complex_numbers</span> <span>=</span> <span>torch</span>.<span>view_as_complex</span>(<span>q_per_token_split_into_pairs</span>)  <span># [17x64x2] -&gt; [17x64]</span>
<span>q_per_token_as_complex_numbers</span>.<span>shape</span></pre></div>

<div dir="auto" data-snippet-clipboard-copy-content="# Calculate (x + yi) * (cosmŒ∏ + sinmŒ∏i)
# That is, perform the rotation operation to obtain the final result.
q_per_token_as_complex_numbers_rotated = q_per_token_as_complex_numbers * freqs_cis  # [17x64] * [17x64] = [17x64]
q_per_token_as_complex_numbers_rotated.shape"><pre><span># Calculate (x + yi) * (cosmŒ∏ + sinmŒ∏i)</span>
<span># That is, perform the rotation operation to obtain the final result.</span>
<span>q_per_token_as_complex_numbers_rotated</span> <span>=</span> <span>q_per_token_as_complex_numbers</span> <span>*</span> <span>freqs_cis</span>  <span># [17x64] * [17x64] = [17x64]</span>
<span>q_per_token_as_complex_numbers_rotated</span>.<span>shape</span></pre></div>

<p dir="auto"><h3 tabindex="-1" dir="auto">
Obtain the rotated vectors (restore the shape).
</h3><a id="user-content-obtain-the-rotated-vectors-restore-the-shape" aria-label="Permalink: 
Obtain the rotated vectors (restore the shape).
" href="#obtain-the-rotated-vectors-restore-the-shape"></a></p>
<br>
We can represent the complex numbers as real numbers again to obtain the query results in the form of dimension pairs.
<div dir="auto" data-snippet-clipboard-copy-content="# Convert the complex-number results back to the real-number dimension-pair form.
q_per_token_split_into_pairs_rotated = torch.view_as_real(q_per_token_as_complex_numbers_rotated)  # [17x64] -> [17x64x2]
q_per_token_split_into_pairs_rotated.shape"><pre><span># Convert the complex-number results back to the real-number dimension-pair form.</span>
<span>q_per_token_split_into_pairs_rotated</span> <span>=</span> <span>torch</span>.<span>view_as_real</span>(<span>q_per_token_as_complex_numbers_rotated</span>)  <span># [17x64] -&gt; [17x64x2]</span>
<span>q_per_token_split_into_pairs_rotated</span>.<span>shape</span></pre></div>

<p dir="auto">Merge the rotated dimensions. In this way, we obtain a new query vector (the rotated query vector) with a shape of [17x128], where 17 represents the number of tokens and 128 represents the dimension of the query vector.</p>
<div dir="auto" data-snippet-clipboard-copy-content="# Restore the dimension-pair results to the original form of the query vectors, and obtain the final query vector.
q_per_token_rotated = q_per_token_split_into_pairs_rotated.view(q_per_token.shape)  # [17x64x2] -> [17x128]
q_per_token_rotated.shape"><pre><span># Restore the dimension-pair results to the original form of the query vectors, and obtain the final query vector.</span>
<span>q_per_token_rotated</span> <span>=</span> <span>q_per_token_split_into_pairs_rotated</span>.<span>view</span>(<span>q_per_token</span>.<span>shape</span>)  <span># [17x64x2] -&gt; [17x128]</span>
<span>q_per_token_rotated</span>.<span>shape</span></pre></div>

<p dir="auto"><h4 tabindex="-1" dir="auto">Add positional information to the key vectors (same as the query)</h4><a id="user-content-add-positional-information-to-the-key-vectors-same-as-the-query" aria-label="Permalink: Add positional information to the key vectors (same as the query)" href="#add-positional-information-to-the-key-vectors-same-as-the-query"></a></p>
<div dir="auto" data-snippet-clipboard-copy-content="# Split the key vectors into pairs along the dimension direction to form dimension pairs (modify the shape).
k_per_token_split_into_pairs = k_per_token.float().view(k_per_token.shape[0], -1, 2)  # [17x128] -> [17x64x2]
k_per_token_split_into_pairs.shape"><pre><span># Split the key vectors into pairs along the dimension direction to form dimension pairs (modify the shape).</span>
<span>k_per_token_split_into_pairs</span> <span>=</span> <span>k_per_token</span>.<span>float</span>().<span>view</span>(<span>k_per_token</span>.<span>shape</span>[<span>0</span>], <span>-</span><span>1</span>, <span>2</span>)  <span># [17x128] -&gt; [17x64x2]</span>
<span>k_per_token_split_into_pairs</span>.<span>shape</span></pre></div>

<div dir="auto" data-snippet-clipboard-copy-content="# Obtain (x + yi)
# That is, convert the dimension pairs into complex numbers. After the conversion, the shape of the dimensions will change from [17x64x2] to [17x64].
k_per_token_as_complex_numbers = torch.view_as_complex(k_per_token_split_into_pairs)  # [17x64x2] -> [17x64]
k_per_token_as_complex_numbers.shape"><pre><span># Obtain (x + yi)</span>
<span># That is, convert the dimension pairs into complex numbers. After the conversion, the shape of the dimensions will change from [17x64x2] to [17x64].</span>
<span>k_per_token_as_complex_numbers</span> <span>=</span> <span>torch</span>.<span>view_as_complex</span>(<span>k_per_token_split_into_pairs</span>)  <span># [17x64x2] -&gt; [17x64]</span>
<span>k_per_token_as_complex_numbers</span>.<span>shape</span></pre></div>

<div dir="auto" data-snippet-clipboard-copy-content="# Calculate (x + yi) * (cosmŒ∏ + sinmŒ∏i)
# That is, perform the rotation operation to obtain the final result.
# And convert the result back to the real-number form.
k_per_token_split_into_pairs_rotated = torch.view_as_real(k_per_token_as_complex_numbers * freqs_cis)  # [17x64] * [17x64] = [17x64] -> [17x64x2]
k_per_token_split_into_pairs_rotated.shape"><pre><span># Calculate (x + yi) * (cosmŒ∏ + sinmŒ∏i)</span>
<span># That is, perform the rotation operation to obtain the final result.</span>
<span># And convert the result back to the real-number form.</span>
<span>k_per_token_split_into_pairs_rotated</span> <span>=</span> <span>torch</span>.<span>view_as_real</span>(<span>k_per_token_as_complex_numbers</span> <span>*</span> <span>freqs_cis</span>)  <span># [17x64] * [17x64] = [17x64] -&gt; [17x64x2]</span>
<span>k_per_token_split_into_pairs_rotated</span>.<span>shape</span></pre></div>

<div dir="auto" data-snippet-clipboard-copy-content="# Restore the dimension-pair results to the original form of the key vectors, and obtain the final key vector.
k_per_token_rotated = k_per_token_split_into_pairs_rotated.view(k_per_token.shape)  # [17x64x2] -> [17x128]
k_per_token_rotated.shape"><pre><span># Restore the dimension-pair results to the original form of the key vectors, and obtain the final key vector.</span>
<span>k_per_token_rotated</span> <span>=</span> <span>k_per_token_split_into_pairs_rotated</span>.<span>view</span>(<span>k_per_token</span>.<span>shape</span>)  <span># [17x64x2] -&gt; [17x128]</span>
<span>k_per_token_rotated</span>.<span>shape</span></pre></div>

<p dir="auto"><h3 tabindex="-1" dir="auto">
At this stage, we have the rotated query vectors and key vectors corresponding to each token.
</h3><a id="user-content-at-this-stage-we-have-the-rotated-query-vectors-and-key-vectors-corresponding-to-each-token" aria-label="Permalink: 
At this stage, we have the rotated query vectors and key vectors corresponding to each token.
" href="#at-this-stage-we-have-the-rotated-query-vectors-and-key-vectors-corresponding-to-each-token"></a></p>
<p><a target="_blank" rel="noopener noreferrer" href="https://github.com/therealoliver/Deepdive-llama3-from-scratch/blob/main/images/keys0.png"><img src="https://github.com/therealoliver/Deepdive-llama3-from-scratch/raw/main/images/keys0.png" width="600px"></a>
</p>
<p dir="auto">The shape of each query and key vector remains [17x128].</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Everything's ready. Let's start calculating the attention weights between tokens.</h3><a id="user-content-everythings-ready-lets-start-calculating-the-attention-weights-between-tokens" aria-label="Permalink: Everything's ready. Let's start calculating the attention weights between tokens." href="#everythings-ready-lets-start-calculating-the-attention-weights-between-tokens"></a></p>
<p dir="auto">This will involve a three-step process:</p>
<ol dir="auto">
<li>Calculate the attention scores: score = Q x K</li>
<li>Mask the future tokens: score = mask(score)</li>
<li>Calculate the attention weights: res = softmax(score)</li>
</ol>
<p dir="auto">Let's get started! :)</p>
<p dir="auto"><h4 tabindex="-1" dir="auto">Multiply the query and key vectors to obtain the attention scores.</h4><a id="user-content-multiply-the-query-and-key-vectors-to-obtain-the-attention-scores" aria-label="Permalink: Multiply the query and key vectors to obtain the attention scores." href="#multiply-the-query-and-key-vectors-to-obtain-the-attention-scores"></a></p>
<p dir="auto">In this way, we will get the score values between each token and all other tokens.
<br>
These scores represent how strongly each token's query relates to every other token's key.
<br>
This is the self-attention!
<br>
The shape of this attention score matrix (qk_per_token) is [17x17], where 17 is the number of tokens in the input prompt.</p>
<p><a target="_blank" rel="noopener noreferrer" href="https://github.com/therealoliver/Deepdive-llama3-from-scratch/blob/main/images/qkmatmul.png"><img src="https://github.com/therealoliver/Deepdive-llama3-from-scratch/raw/main/images/qkmatmul.png" width="600px"></a>
</p>
<div dir="auto" data-snippet-clipboard-copy-content="# Calculate the attention score
# At the same time, perform normalization to prevent the subsequent softmax calculation results from being overly skewed towards 0 or 1,
# (the dot-product values may be too large when the dimensions are large),
# which could lead to vanishing gradients or exploding gradients, so as to maintain numerical stability.
qk_per_token = torch.matmul(q_per_token_rotated, k_per_token_rotated.T)/(head_dim)**0.5  # [17x128] x [128x17] = [17x17]
qk_per_token.shape"><pre><span># Calculate the attention score</span>
<span># At the same time, perform normalization to prevent the subsequent softmax calculation results from being overly skewed towards 0 or 1,</span>
<span># (the dot-product values may be too large when the dimensions are large),</span>
<span># which could lead to vanishing gradients or exploding gradients, so as to maintain numerical stability.</span>
<span>qk_per_token</span> <span>=</span> <span>torch</span>.<span>matmul</span>(<span>q_per_token_rotated</span>, <span>k_per_token_rotated</span>.<span>T</span>)<span>/</span>(<span>head_dim</span>)<span>**</span><span>0.5</span>  <span># [17x128] x [128x17] = [17x17]</span>
<span>qk_per_token</span>.<span>shape</span></pre></div>

<p dir="auto"><h4 tabindex="-1" dir="auto">Now we must mask the future query-key scores.</h4><a id="user-content-now-we-must-mask-the-future-query-key-scores" aria-label="Permalink: Now we must mask the future query-key scores." href="#now-we-must-mask-the-future-query-key-scores"></a></p>
<p dir="auto">During the training process of Llama 3, the QK scores of future tokens will be masked.
<br>
Why? Because during training, we only learn how to use past tokens to predict the current token. If we don't mask future tokens, it will lead to the leakage of prediction information.
<br>
Therefore, during the inference process, we also need to set the future tokens to 0 (to ensure the logical consistency between the training and inference processes).
<br></p>
<p dir="auto">Of course, if you're as curious as I am about what would happen without masking, you can check the results of the additional experiment I conducted in the last section after you've finished learning. (^_&lt;)</p>
<p><a target="_blank" rel="noopener noreferrer" href="https://github.com/therealoliver/Deepdive-llama3-from-scratch/blob/main/images/mask.png"><img src="https://github.com/therealoliver/Deepdive-llama3-from-scratch/raw/main/images/mask.png" width="600px"></a>
</p>
<div dir="auto" data-snippet-clipboard-copy-content="# First, take a look at the score matrix before masking
def display_qk_heatmap(qk_per_token):
    _, ax = plt.subplots()  # Create a figure window

    # `imshow` is commonly used to display data in the form of a two-dimensional array or matrix,
    # it maps the matrix elements to grayscale or color values, so it can be used to draw heatmaps.
    # Convert the tensor back to full precision, then detach it from the computational graph to avoid potential gradient calculation and storage issues.
    # Specify to use the 'viridis' color mapping scheme to display the image (blue -> green -> yellow).
    im = ax.imshow(qk_per_token.to(float).detach(), cmap='viridis')

    # Set the number and labels of the x and y axis ticks to ensure correct one-to-one correspondence.
    ax.set_xticks(range(len(prompt_split_as_tokens)))
    ax.set_yticks(range(len(prompt_split_as_tokens)))
    ax.set_xticklabels(prompt_split_as_tokens)
    ax.set_yticklabels(prompt_split_as_tokens)

    # Add a color bar on the side.
    # Specify `im` to identify the correct color mapping and value range.
    # Specify the sub-plot it belongs to as `ax` (if there are multiple sub-plots, it would be `ax = ax[i]`).
    ax.figure.colorbar(im, ax=ax)
    
display_qk_heatmap(qk_per_token)"><pre><span># First, take a look at the score matrix before masking</span>
<span>def</span> <span>display_qk_heatmap</span>(<span>qk_per_token</span>):
    <span>_</span>, <span>ax</span> <span>=</span> <span>plt</span>.<span>subplots</span>()  <span># Create a figure window</span>

    <span># `imshow` is commonly used to display data in the form of a two-dimensional array or matrix,</span>
    <span># it maps the matrix elements to grayscale or color values, so it can be used to draw heatmaps.</span>
    <span># Convert the tensor back to full precision, then detach it from the computational graph to avoid potential gradient calculation and storage issues.</span>
    <span># Specify to use the 'viridis' color mapping scheme to display the image (blue -&gt; green -&gt; yellow).</span>
    <span>im</span> <span>=</span> <span>ax</span>.<span>imshow</span>(<span>qk_per_token</span>.<span>to</span>(<span>float</span>).<span>detach</span>(), <span>cmap</span><span>=</span><span>'viridis'</span>)

    <span># Set the number and labels of the x and y axis ticks to ensure correct one-to-one correspondence.</span>
    <span>ax</span>.<span>set_xticks</span>(<span>range</span>(<span>len</span>(<span>prompt_split_as_tokens</span>)))
    <span>ax</span>.<span>set_yticks</span>(<span>range</span>(<span>len</span>(<span>prompt_split_as_tokens</span>)))
    <span>ax</span>.<span>set_xticklabels</span>(<span>prompt_split_as_tokens</span>)
    <span>ax</span>.<span>set_yticklabels</span>(<span>prompt_split_as_tokens</span>)

    <span># Add a color bar on the side.</span>
    <span># Specify `im` to identify the correct color mapping and value range.</span>
    <span># Specify the sub-plot it belongs to as `ax` (if there are multiple sub-plots, it would be `ax = ax[i]`).</span>
    <span>ax</span>.<span>figure</span>.<span>colorbar</span>(<span>im</span>, <span>ax</span><span>=</span><span>ax</span>)
    
<span>display_qk_heatmap</span>(<span>qk_per_token</span>)</pre></div>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/therealoliver/Deepdive-llama3-from-scratch/blob/main/images/output_65_0.png"><img src="https://github.com/therealoliver/Deepdive-llama3-from-scratch/raw/main/images/output_65_0.png" alt="png"></a></p>
<div dir="auto" data-snippet-clipboard-copy-content="# Generate the masking matrix
# Set the positions of the elements to be masked to negative infinity, and set the positions that do not need to be masked to 0.
# Then, add it to the score matrix to achieve the masking effect (negative infinity will tend to 0 when calculating the softmax).

# `torch.full` is used to generate a tensor with a specified shape and filling value.
# Here, a [17x17] matrix filled with negative infinity is first generated.
# Specify that the device of this matrix is the same as that of the previous tokens to ensure that there are no errors in subsequent calculations,
# for example, if the previous tokens are on the GPU and the device is not specified here, the `mask` will be newly created on the CPU, and an error will occur when adding the two.
mask = torch.full((len(tokens), len(tokens)), float(&quot;-inf&quot;), device=tokens.device)  # [17x17]

# `torch.triu` is used to return the upper-triangular part of the matrix, and set the rest to 0 (use `torch.tril` to get the lower-triangular part).
# `diagonal` is the offset of the diagonal. When it's 1, it means taking the upper-triangular part starting from 1 position above the main diagonal to avoid masking the token itself.
mask = torch.triu(mask, diagonal=1)  # [17x17]

mask, mask.shape"><pre><span># Generate the masking matrix</span>
<span># Set the positions of the elements to be masked to negative infinity, and set the positions that do not need to be masked to 0.</span>
<span># Then, add it to the score matrix to achieve the masking effect (negative infinity will tend to 0 when calculating the softmax).</span>

<span># `torch.full` is used to generate a tensor with a specified shape and filling value.</span>
<span># Here, a [17x17] matrix filled with negative infinity is first generated.</span>
<span># Specify that the device of this matrix is the same as that of the previous tokens to ensure that there are no errors in subsequent calculations,</span>
<span># for example, if the previous tokens are on the GPU and the device is not specified here, the `mask` will be newly created on the CPU, and an error will occur when adding the two.</span>
<span>mask</span> <span>=</span> <span>torch</span>.<span>full</span>((<span>len</span>(<span>tokens</span>), <span>len</span>(<span>tokens</span>)), <span>float</span>(<span>"-inf"</span>), <span>device</span><span>=</span><span>tokens</span>.<span>device</span>)  <span># [17x17]</span>

<span># `torch.triu` is used to return the upper-triangular part of the matrix, and set the rest to 0 (use `torch.tril` to get the lower-triangular part).</span>
<span># `diagonal` is the offset of the diagonal. When it's 1, it means taking the upper-triangular part starting from 1 position above the main diagonal to avoid masking the token itself.</span>
<span>mask</span> <span>=</span> <span>torch</span>.<span>triu</span>(<span>mask</span>, <span>diagonal</span><span>=</span><span>1</span>)  <span># [17x17]</span>

<span>mask</span>, <span>mask</span>.<span>shape</span></pre></div>
<div data-snippet-clipboard-copy-content="(tensor([[0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],
         [0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],
         [0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],
         [0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],
         [0., 0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],
         [0., 0., 0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],
         [0., 0., 0., 0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],
         [0., 0., 0., 0., 0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],
         [0., 0., 0., 0., 0., 0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],
         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf],
         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf],
         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf],
         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., -inf, -inf, -inf, -inf],
         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., -inf, -inf, -inf],
         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., -inf, -inf],
         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., -inf],
         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]),
 torch.Size([17, 17]))"><pre><code>(tensor([[0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],
         [0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],
         [0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],
         [0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],
         [0., 0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],
         [0., 0., 0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],
         [0., 0., 0., 0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],
         [0., 0., 0., 0., 0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],
         [0., 0., 0., 0., 0., 0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],
         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf],
         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf],
         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf],
         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., -inf, -inf, -inf, -inf],
         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., -inf, -inf, -inf],
         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., -inf, -inf],
         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., -inf],
         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]),
 torch.Size([17, 17]))
</code></pre></div>
<div dir="auto" data-snippet-clipboard-copy-content="# Mask the scores of future tokens
qk_per_token_after_masking = qk_per_token + mask  # [17x17] + [17x17] = [17x17]
display_qk_heatmap(qk_per_token_after_masking)  # Display the attention scores after masking"><pre><span># Mask the scores of future tokens</span>
<span>qk_per_token_after_masking</span> <span>=</span> <span>qk_per_token</span> <span>+</span> <span>mask</span>  <span># [17x17] + [17x17] = [17x17]</span>
<span>display_qk_heatmap</span>(<span>qk_per_token_after_masking</span>)  <span># Display the attention scores after masking</span></pre></div>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/therealoliver/Deepdive-llama3-from-scratch/blob/main/images/output_67_0.png"><img src="https://github.com/therealoliver/Deepdive-llama3-from-scratch/raw/main/images/output_67_0.png" alt="png"></a></p>
<p dir="auto"><h4 tabindex="-1" dir="auto">Calculate the final attention weights, that is, softmax(score).</h4><a id="user-content-calculate-the-final-attention-weights-that-is-softmaxscore" aria-label="Permalink: Calculate the final attention weights, that is, softmax(score)." href="#calculate-the-final-attention-weights-that-is-softmaxscore"></a></p>
<p><a target="_blank" rel="noopener noreferrer" href="https://github.com/therealoliver/Deepdive-llama3-from-scratch/blob/main/images/softmax.png"><img src="https://github.com/therealoliver/Deepdive-llama3-from-scratch/raw/main/images/softmax.png" width="600px"></a>
</p>
<div dir="auto" data-snippet-clipboard-copy-content="# Calculate the attention weights
# That is, calculate the softmax values of the scores.
# `dim = 1` indicates that the softmax calculation is performed row-by-row, and the result is converted to half-precision to be consistent with the subsequent value vectors.
qk_per_token_after_masking_after_softmax = torch.nn.functional.softmax(qk_per_token_after_masking, dim=1).to(torch.bfloat16)  # [17x17] -> [17x17]
display_qk_heatmap(qk_per_token_after_masking_after_softmax)"><pre><span># Calculate the attention weights</span>
<span># That is, calculate the softmax values of the scores.</span>
<span># `dim = 1` indicates that the softmax calculation is performed row-by-row, and the result is converted to half-precision to be consistent with the subsequent value vectors.</span>
<span>qk_per_token_after_masking_after_softmax</span> <span>=</span> <span>torch</span>.<span>nn</span>.<span>functional</span>.<span>softmax</span>(<span>qk_per_token_after_masking</span>, <span>dim</span><span>=</span><span>1</span>).<span>to</span>(<span>torch</span>.<span>bfloat16</span>)  <span># [17x17] -&gt; [17x17]</span>
<span>display_qk_heatmap</span>(<span>qk_per_token_after_masking_after_softmax</span>)</pre></div>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/therealoliver/Deepdive-llama3-from-scratch/blob/main/images/output_69_0.png"><img src="https://github.com/therealoliver/Deepdive-llama3-from-scratch/raw/main/images/output_69_0.png" alt="png"></a></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Finally! Calculate the final result of the single-head attention mechanism!</h3><a id="user-content-finally-calculate-the-final-result-of-the-single-head-attention-mechanism" aria-label="Permalink: Finally! Calculate the final result of the single-head attention mechanism!" href="#finally-calculate-the-final-result-of-the-single-head-attention-mechanism"></a></p>
<p><a target="_blank" rel="noopener noreferrer" href="https://github.com/therealoliver/Deepdive-llama3-from-scratch/blob/main/images/attention.png"><img src="https://github.com/therealoliver/Deepdive-llama3-from-scratch/raw/main/images/attention.png" width="600px"></a>
</p>
<p dir="auto">Principle: The previous attention weights (ranging from 0 to 1) are used to determine what proportion of each value vector should be used for each token (i.e., to weight the value vectors).</p>
<p dir="auto">Example: If the input consists of 3 tokens, the attention result of the first token might be: res = 0.6 * value_1 + 0.3 * value_2 + 0.1 * value_3</p>
<p dir="auto">The shape of the attention result after the multiplication of the weight matrix and the value matrix is [17x128].</p>
<div dir="auto" data-snippet-clipboard-copy-content="# Calculate the final result of the single-head attention
qkv_attention = torch.matmul(qk_per_token_after_masking_after_softmax, v_per_token)  # [17x17] x [17x128] = [17x128]
qkv_attention.shape"><pre><span># Calculate the final result of the single-head attention</span>
<span>qkv_attention</span> <span>=</span> <span>torch</span>.<span>matmul</span>(<span>qk_per_token_after_masking_after_softmax</span>, <span>v_per_token</span>)  <span># [17x17] x [17x128] = [17x128]</span>
<span>qkv_attention</span>.<span>shape</span></pre></div>

<p dir="auto"><h2 tabindex="-1" dir="auto">Calculate the multi-head attention mechanism (a simple loop to repeat the above process)</h2><a id="user-content-calculate-the-multi-head-attention-mechanism-a-simple-loop-to-repeat-the-above-process" aria-label="Permalink: Calculate the multi-head attention mechanism (a simple loop to repeat the above process)" href="#calculate-the-multi-head-attention-mechanism-a-simple-loop-to-repeat-the-above-process"></a></p>
<p><a target="_blank" rel="noopener noreferrer" href="https://github.com/therealoliver/Deepdive-llama3-from-scratch/blob/main/images/heads.png"><img src="https://github.com/therealoliver/Deepdive-llama3-from-scratch/raw/main/images/heads.png" width="600px"></a>
</p>
<p dir="auto">We now have the attention values for the first head of the first layer.
<br></p>
<div dir="auto"><p>Now we need to run a loop to perform exactly the same mathematical process as in the previous cell, but for each head in the first layer.
</p></div>
<p dir="auto"><h3 tabindex="-1" dir="auto">
It's worth noting that in the <a href="https://github.com/meta-llama/llama3/blob/main/llama/model.py#L90">official Llama3 code implementation</a>, the multi-head attention calculation adopts the method of one-time matrix multiplication instead of time consuming for-loop calculations. The general process is as follows:
</h3><a id="user-content-its-worth-noting-that-in-the-official-llama3-code-implementation-the-multi-head-attention-calculation-adopts-the-method-of-one-time-matrix-multiplication-instead-of-time-consuming-for-loop-calculations-the-general-process-is-as-follows" aria-label="Permalink: 
It's worth noting that in the official Llama3 code implementation, the multi-head attention calculation adopts the method of one-time matrix multiplication instead of time consuming for-loop calculations. The general process is as follows:
" href="#its-worth-noting-that-in-the-official-llama3-code-implementation-the-multi-head-attention-calculation-adopts-the-method-of-one-time-matrix-multiplication-instead-of-time-consuming-for-loop-calculations-the-general-process-is-as-follows"></a></p>
<ol dir="auto">
<li>Based on matrix parallelism, calculate the QKV vectors: [17x4096] x [4096x4096] or [4096x1024] = [17x4096] or [17x1024], and then reshape them to [32x17x128] or [8x17x128].</li>
<li>After obtaining the QKV vectors, duplicate the internal parts of the K and V vectors to make their shapes consistent with the Q vector. At this time, the shapes of all of them are [32x17x128].</li>
<li>When calculating the scores, use the transpose method to exchange the positions of the last two dimensions of the tensors to complete the matrix multiplication. For example, <code>torch.matmul(q, k.transpose(1,2)) / head_dim ** 0.5</code>. At this time, it is [32x17x128] x [32x128x17] = [32x17x17].</li>
<li>The same principle applies to other matrix calculations.</li>
</ol>
<p dir="auto">Note: The matrix shape changes in each step of the above process are simplified versions, only for illustration to facilitate understanding, which are different from the change process in the official Llama3 implementation (the official implementation involves a large number of shape change processes).</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Calculate the result for each head</h3><a id="user-content-calculate-the-result-for-each-head" aria-label="Permalink: Calculate the result for each head" href="#calculate-the-result-for-each-head"></a></p>
<div dir="auto" data-snippet-clipboard-copy-content="# Calculate the multi-head attention results
# That is, a loop of the previous single-head attention calculation process
qkv_attention_store = []

for head in range(n_heads):
    # Extract the QKV weight matrices corresponding to the current head
    q_layer0_head = q_layer0[head]  # [32x128x4096] -> [128x4096]
    k_layer0_head = k_layer0[head//4]  # Every 4 heads share one key weight, [8x128x4096] -> [128x4096]
    v_layer0_head = v_layer0[head//4]  # Every 4 heads share one value weight, [8x128x4096] -> [128x4096]
    
    # Calculate XW to obtain the QKV vectors
    # [17x4096] x [4096x128] = [17x128]
    q_per_token = torch.matmul(token_embeddings, q_layer0_head.T)
    k_per_token = torch.matmul(token_embeddings, k_layer0_head.T)
    v_per_token = torch.matmul(token_embeddings, v_layer0_head.T)
    
    # Add position information to the query vector (RoPE)
    q_per_token_split_into_pairs = q_per_token.float().view(q_per_token.shape[0], -1, 2)  # Divide vector into pairs along the dimensions direction to form dimension pairs. [17x128] -> [17x64x2]
    q_per_token_as_complex_numbers = torch.view_as_complex(q_per_token_split_into_pairs)  # Convert to complex number representation, (x,y) -> (x+yi). [17x64x2] -> [17x64]
    q_per_token_as_complex_numbers_rotated = q_per_token_as_complex_numbers * freqs_cis[:len(tokens)]  # Calculate (x+yi)*(cosmŒ∏+sinmŒ∏i) to complete the rotation operation. [17x64] * [17x64] = [17x64]
    q_per_token_split_into_pairs_rotated = torch.view_as_real(q_per_token_as_complex_numbers_rotated)  # Convert the result back to real number representation, (x+yi) -> (x,y). [17x64] -> [17x64x2]
    q_per_token_rotated = q_per_token_split_into_pairs_rotated.view(q_per_token.shape)  # Convert the result back to the original vector shape to obtain the final query vector. [17x64x2] -> [17x128]

    # Add position information to the key vector (RoPE)
    k_per_token_split_into_pairs = k_per_token.float().view(k_per_token.shape[0], -1, 2)  # Divide vector into pairs along the dimensions direction to form dimension pairs. [17x128] -> [17x64x2]
    k_per_token_as_complex_numbers = torch.view_as_complex(k_per_token_split_into_pairs)  # Convert to complex number representation, (x,y) -> (x+yi). [17x64x2] -> [17x64]
    k_per_token_as_complex_numbers_rotated = k_per_token_as_complex_numbers * freqs_cis[:len(tokens)]  # Calculate (x+yi)*(cosmŒ∏+sinmŒ∏i) to complete the rotation operation. [17x64] * [17x64] = [17x64]
    k_per_token_split_into_pairs_rotated = torch.view_as_real(k_per_token_as_complex_numbers_rotated)  # Convert the result back to real number representation, (x+yi) -> (x,y). [17x64] -> [17x64x2]
    k_per_token_rotated = k_per_token_split_into_pairs_rotated.view(k_per_token.shape)  # Convert the result back to the original vector shape to obtain the final key vector. [17x64x2] -> [17x128]

    # Calculate the attention scores and normalize the scores simultaneously (i.e., Q√óK/sqrt(dim))
    qk_per_token = torch.matmul(q_per_token_rotated, k_per_token_rotated.T)/(head_dim)**0.5  # [17x128] x [128x17] = [17x17]
    
    # Mask the scores of future tokens
    mask = torch.full(qk_per_token.shape, float(&quot;-inf&quot;), device=tokens.device)  # Create a matrix with the same shape as the attention scores, filled with negative infinity, and stored in the same device as other vectors to prevent errors in subsequent calculations. [17x17]
    mask = torch.triu(mask, diagonal=1)  # Keep the negative infinity in the upper-triangular part and set others to 0 (i.e., the upper-triangular area represents future tokens that need to be masked). The diagonal offset is 1 to avoid masking the token itself. [17x17]
    qk_per_token_after_masking = qk_per_token + mask  # Add the attention scores with the masking matrix, making the upper-triangular part of the score matrix become negative infinity, which will tend to 0 after the subsequent softmax operation. [17x17]
    
    # Calculate the attention weights (i.e., softmax(score))
    # Meanwhile, convert it back to half-precision (because it will be multiplied with the value vector v_per_token later, so the data types need to be the same).
    qk_per_token_after_masking_after_softmax = torch.nn.functional.softmax(qk_per_token_after_masking, dim=1).to(torch.bfloat16)  # Calculate the softmax row-by-row. [17x17]
    
    # Calculate the final result of the attention mechanism (i.e., softmax(score) √ó V)
    qkv_attention = torch.matmul(qk_per_token_after_masking_after_softmax, v_per_token)  # [17x17] √ó [17x128] = [17x128]
    
    # Record the result of this head
    qkv_attention_store.append(qkv_attention)

len(qkv_attention_store)"><pre><span># Calculate the multi-head attention results</span>
<span># That is, a loop of the previous single-head attention calculation process</span>
<span>qkv_attention_store</span> <span>=</span> []

<span>for</span> <span>head</span> <span>in</span> <span>range</span>(<span>n_heads</span>):
    <span># Extract the QKV weight matrices corresponding to the current head</span>
    <span>q_layer0_head</span> <span>=</span> <span>q_layer0</span>[<span>head</span>]  <span># [32x128x4096] -&gt; [128x4096]</span>
    <span>k_layer0_head</span> <span>=</span> <span>k_layer0</span>[<span>head</span><span>//</span><span>4</span>]  <span># Every 4 heads share one key weight, [8x128x4096] -&gt; [128x4096]</span>
    <span>v_layer0_head</span> <span>=</span> <span>v_layer0</span>[<span>head</span><span>//</span><span>4</span>]  <span># Every 4 heads share one value weight, [8x128x4096] -&gt; [128x4096]</span>
    
    <span># Calculate XW to obtain the QKV vectors</span>
    <span># [17x4096] x [4096x128] = [17x128]</span>
    <span>q_per_token</span> <span>=</span> <span>torch</span>.<span>matmul</span>(<span>token_embeddings</span>, <span>q_layer0_head</span>.<span>T</span>)
    <span>k_per_token</span> <span>=</span> <span>torch</span>.<span>matmul</span>(<span>token_embeddings</span>, <span>k_layer0_head</span>.<span>T</span>)
    <span>v_per_token</span> <span>=</span> <span>torch</span>.<span>matmul</span>(<span>token_embeddings</span>, <span>v_layer0_head</span>.<span>T</span>)
    
    <span># Add position information to the query vector (RoPE)</span>
    <span>q_per_token_split_into_pairs</span> <span>=</span> <span>q_per_token</span>.<span>float</span>().<span>view</span>(<span>q_per_token</span>.<span>shape</span>[<span>0</span>], <span>-</span><span>1</span>, <span>2</span>)  <span># Divide vector into pairs along the dimensions direction to form dimension pairs. [17x128] -&gt; [17x64x2]</span>
    <span>q_per_token_as_complex_numbers</span> <span>=</span> <span>torch</span>.<span>view_as_complex</span>(<span>q_per_token_split_into_pairs</span>)  <span># Convert to complex number representation, (x,y) -&gt; (x+yi). [17x64x2] -&gt; [17x64]</span>
    <span>q_per_token_as_complex_numbers_rotated</span> <span>=</span> <span>q_per_token_as_complex_numbers</span> <span>*</span> <span>freqs_cis</span>[:<span>len</span>(<span>tokens</span>)]  <span># Calculate (x+yi)*(cosmŒ∏+sinmŒ∏i) to complete the rotation operation. [17x64] * [17x64] = [17x64]</span>
    <span>q_per_token_split_into_pairs_rotated</span> <span>=</span> <span>torch</span>.<span>view_as_real</span>(<span>q_per_token_as_complex_numbers_rotated</span>)  <span># Convert the result back to real number representation, (x+yi) -&gt; (x,y). [17x64] -&gt; [17x64x2]</span>
    <span>q_per_token_rotated</span> <span>=</span> <span>q_per_token_split_into_pairs_rotated</span>.<span>view</span>(<span>q_per_token</span>.<span>shape</span>)  <span># Convert the result back to the original vector shape to obtain the final query vector. [17x64x2] -&gt; [17x128]</span>

    <span># Add position information to the key vector (RoPE)</span>
    <span>k_per_token_split_into_pairs</span> <span>=</span> <span>k_per_token</span>.<span>float</span>().<span>view</span>(<span>k_per_token</span>.<span>shape</span>[<span>0</span>], <span>-</span><span>1</span>, <span>2</span>)  <span># Divide vector into pairs along the dimensions direction to form dimension pairs. [17x128] -&gt; [17x64x2]</span>
    <span>k_per_token_as_complex_numbers</span> <span>=</span> <span>torch</span>.<span>view_as_complex</span>(<span>k_per_token_split_into_pairs</span>)  <span># Convert to complex number representation, (x,y) -&gt; (x+yi). [17x64x2] -&gt; [17x64]</span>
    <span>k_per_token_as_complex_numbers_rotated</span> <span>=</span> <span>k_per_token_as_complex_numbers</span> <span>*</span> <span>freqs_cis</span>[:<span>len</span>(<span>tokens</span>)]  <span># Calculate (x+yi)*(cosmŒ∏+sinmŒ∏i) to complete the rotation operation. [17x64] * [17x64] = [17x64]</span>
    <span>k_per_token_split_into_pairs_rotated</span> <span>=</span> <span>torch</span>.<span>view_as_real</span>(<span>k_per_token_as_complex_numbers_rotated</span>)  <span># Convert the result back to real number representation, (x+yi) -&gt; (x,y). [17x64] -&gt; [17x64x2]</span>
    <span>k_per_token_rotated</span> <span>=</span> <span>k_per_token_split_into_pairs_rotated</span>.<span>view</span>(<span>k_per_token</span>.<span>shape</span>)  <span># Convert the result back to the original vector shape to obtain the final key vector. [17x64x2] -&gt; [17x128]</span>

    <span># Calculate the attention scores and normalize the scores simultaneously (i.e., Q√óK/sqrt(dim))</span>
    <span>qk_per_token</span> <span>=</span> <span>torch</span>.<span>matmul</span>(<span>q_per_token_rotated</span>, <span>k_per_token_rotated</span>.<span>T</span>)<span>/</span>(<span>head_dim</span>)<span>**</span><span>0.5</span>  <span># [17x128] x [128x17] = [17x17]</span>
    
    <span># Mask the scores of future tokens</span>
    <span>mask</span> <span>=</span> <span>torch</span>.<span>full</span>(<span>qk_per_token</span>.<span>shape</span>, <span>float</span>(<span>"-inf"</span>), <span>device</span><span>=</span><span>tokens</span>.<span>device</span>)  <span># Create a matrix with the same shape as the attention scores, filled with negative infinity, and stored in the same device as other vectors to prevent errors in subsequent calculations. [17x17]</span>
    <span>mask</span> <span>=</span> <span>torch</span>.<span>triu</span>(<span>mask</span>, <span>diagonal</span><span>=</span><span>1</span>)  <span># Keep the negative infinity in the upper-triangular part and set others to 0 (i.e., the upper-triangular area represents future tokens that need to be masked). The diagonal offset is 1 to avoid masking the token itself. [17x17]</span>
    <span>qk_per_token_after_masking</span> <span>=</span> <span>qk_per_token</span> <span>+</span> <span>mask</span>  <span># Add the attention scores with the masking matrix, making the upper-triangular part of the score matrix become negative infinity, which will tend to 0 after the subsequent softmax operation. [17x17]</span>
    
    <span># Calculate the attention weights (i.e., softmax(score))</span>
    <span># Meanwhile, convert it back to half-precision (because it will be multiplied with the value vector v_per_token later, so the data types need to be the same).</span>
    <span>qk_per_token_after_masking_after_softmax</span> <span>=</span> <span>torch</span>.<span>nn</span>.<span>functional</span>.<span>softmax</span>(<span>qk_per_token_after_masking</span>, <span>dim</span><span>=</span><span>1</span>).<span>to</span>(<span>torch</span>.<span>bfloat16</span>)  <span># Calculate the softmax row-by-row. [17x17]</span>
    
    <span># Calculate the final result of the attention mechanism (i.e., softmax(score) √ó V)</span>
    <span>qkv_attention</span> <span>=</span> <span>torch</span>.<span>matmul</span>(<span>qk_per_token_after_masking_after_softmax</span>, <span>v_per_token</span>)  <span># [17x17] √ó [17x128] = [17x128]</span>
    
    <span># Record the result of this head</span>
    <span>qkv_attention_store</span>.<span>append</span>(<span>qkv_attention</span>)

<span>len</span>(<span>qkv_attention_store</span>)</pre></div>

<p dir="auto"><h3 tabindex="-1" dir="auto">Merge the results of each head into a large matrix</h3><a id="user-content-merge-the-results-of-each-head-into-a-large-matrix" aria-label="Permalink: Merge the results of each head into a large matrix" href="#merge-the-results-of-each-head-into-a-large-matrix"></a></p>
<p><a target="_blank" rel="noopener noreferrer" href="https://github.com/therealoliver/Deepdive-llama3-from-scratch/blob/main/images/stacked.png"><img src="https://github.com/therealoliver/Deepdive-llama3-from-scratch/raw/main/images/stacked.png" width="600px"></a>
</p>
Now we have the results of the attention mechanism for all 32 heads in the first layer. Next, we'll merge all the attention values into a large matrix with a shape of [17x4096].
<br>
We're almost done with the calculation of the attention layer :)
<div dir="auto" data-snippet-clipboard-copy-content="# Merge the multi-head attention matrices
stacked_qkv_attention = torch.cat(qkv_attention_store, dim=-1)  # Concatenate along the second dimension, 32x[17x128] -> [17x4096]
stacked_qkv_attention.shape"><pre><span># Merge the multi-head attention matrices</span>
<span>stacked_qkv_attention</span> <span>=</span> <span>torch</span>.<span>cat</span>(<span>qkv_attention_store</span>, <span>dim</span><span>=</span><span>-</span><span>1</span>)  <span># Concatenate along the second dimension, 32x[17x128] -&gt; [17x4096]</span>
<span>stacked_qkv_attention</span>.<span>shape</span></pre></div>

<p dir="auto"><h3 tabindex="-1" dir="auto">Head-to-head information interaction (linear mapping), the final step of the self-attention layer!</h3><a id="user-content-head-to-head-information-interaction-linear-mapping-the-final-step-of-the-self-attention-layer" aria-label="Permalink: Head-to-head information interaction (linear mapping), the final step of the self-attention layer!" href="#head-to-head-information-interaction-linear-mapping-the-final-step-of-the-self-attention-layer"></a></p>
<p><a target="_blank" rel="noopener noreferrer" href="https://github.com/therealoliver/Deepdive-llama3-from-scratch/blob/main/images/weightmatrix.png"><img src="https://github.com/therealoliver/Deepdive-llama3-from-scratch/raw/main/images/weightmatrix.png" width="600px"></a>
</p>
The last step of the attention calculation for layer0 is to perform the final linear mapping, that is, multiply the combined attention matrix by the output weight matrix.
<div dir="auto" data-snippet-clipboard-copy-content="# Load the output weight matrix of layers.0
w_layer0 = model[&quot;layers.0.attention.wo.weight&quot;]  # [4096x4096]
w_layer0.shape"><pre><span># Load the output weight matrix of layers.0</span>
<span>w_layer0</span> <span>=</span> <span>model</span>[<span>"layers.0.attention.wo.weight"</span>]  <span># [4096x4096]</span>
<span>w_layer0</span>.<span>shape</span></pre></div>

<p dir="auto">This is just a simple linear layer, so we only need matrix multiplication.</p>
<div dir="auto" data-snippet-clipboard-copy-content="# Perform the linear mapping of the attention matrix
# This is the final output of the attention layer
embedding_delta = torch.matmul(stacked_qkv_attention, w_layer0.T)  # [17x4096] x [4096x4096] = [17x4096]
embedding_delta.shape"><pre><span># Perform the linear mapping of the attention matrix</span>
<span># This is the final output of the attention layer</span>
<span>embedding_delta</span> <span>=</span> <span>torch</span>.<span>matmul</span>(<span>stacked_qkv_attention</span>, <span>w_layer0</span>.<span>T</span>)  <span># [17x4096] x [4096x4096] = [17x4096]</span>
<span>embedding_delta</span>.<span>shape</span></pre></div>

<p dir="auto"><h2 tabindex="-1" dir="auto">Perform the residual operation (add)</h2><a id="user-content-perform-the-residual-operation-add" aria-label="Permalink: Perform the residual operation (add)" href="#perform-the-residual-operation-add"></a></p>
<p><a target="_blank" rel="noopener noreferrer" href="https://github.com/therealoliver/Deepdive-llama3-from-scratch/blob/main/images/afterattention.png"><img src="https://github.com/therealoliver/Deepdive-llama3-from-scratch/raw/main/images/afterattention.png" width="600px"></a>
</p>
Now we have the value of the input vector after the attention mechanism is applied. At this time, we need to add the original input vector to it (i.e., the residual operation, to ensure that information is not easily lost and alleviate the problem of gradient vanishing).
<div dir="auto" data-snippet-clipboard-copy-content="# Add the output of the attention layer to the original input to complete the residual operation
embedding_after_edit = token_embeddings_unnormalized + embedding_delta  # [17x4096] + [17x4096] = [17x4096]
embedding_after_edit.shape"><pre><span># Add the output of the attention layer to the original input to complete the residual operation</span>
<span>embedding_after_edit</span> <span>=</span> <span>token_embeddings_unnormalized</span> <span>+</span> <span>embedding_delta</span>  <span># [17x4096] + [17x4096] = [17x4096]</span>
<span>embedding_after_edit</span>.<span>shape</span></pre></div>

<p dir="auto"><h2 tabindex="-1" dir="auto">Perform the second normalization operation</h2><a id="user-content-perform-the-second-normalization-operation" aria-label="Permalink: Perform the second normalization operation" href="#perform-the-second-normalization-operation"></a></p>
<p><a target="_blank" rel="noopener noreferrer" href="https://github.com/therealoliver/Deepdive-llama3-from-scratch/blob/main/images/norm_after.png"><img src="https://github.com/therealoliver/Deepdive-llama3-from-scratch/raw/main/images/norm_after.png" width="600px"></a>
</p>
<div dir="auto" data-snippet-clipboard-copy-content="# Normalize the result of the residual operation
embedding_after_edit_normalized = rms_norm(embedding_after_edit, model[&quot;layers.0.ffn_norm.weight&quot;])  # [17x4096] &amp; [4096] -> [17x4096]
embedding_after_edit_normalized.shape"><pre><span># Normalize the result of the residual operation</span>
<span>embedding_after_edit_normalized</span> <span>=</span> <span>rms_norm</span>(<span>embedding_after_edit</span>, <span>model</span>[<span>"layers.0.ffn_norm.weight"</span>])  <span># [17x4096] &amp; [4096] -&gt; [17x4096]</span>
<span>embedding_after_edit_normalized</span>.<span>shape</span></pre></div>

<p dir="auto"><h2 tabindex="-1" dir="auto">Perform the calculation of the FFN (Feed-Forward Neural Network) layer</h2><a id="user-content-perform-the-calculation-of-the-ffn-feed-forward-neural-network-layer" aria-label="Permalink: Perform the calculation of the FFN (Feed-Forward Neural Network) layer" href="#perform-the-calculation-of-the-ffn-feed-forward-neural-network-layer"></a></p>
<p><a target="_blank" rel="noopener noreferrer" href="https://github.com/therealoliver/Deepdive-llama3-from-scratch/blob/main/images/swiglu.png"><img src="https://github.com/therealoliver/Deepdive-llama3-from-scratch/raw/main/images/swiglu.png" width="600px"></a>
</p>
<br>
In Llama3, they used the SwiGLU feed-forward network. This network architecture can effectively increase nonlinear characteristics when the model needs.
<br>
Nowadays, this kind of feed-forward network architecture is very common in large language models.
<p dir="auto"><h3 tabindex="-1" dir="auto">Why Introduce Nonlinear Layers:</h3><a id="user-content-why-introduce-nonlinear-layers" aria-label="Permalink: Why Introduce Nonlinear Layers:" href="#why-introduce-nonlinear-layers"></a></p>
<ul dir="auto">
<li>The Nonlinearity is at the core of why neural network models can be considered "universal function approximators". In traditional neural network models, we use nonlinear activation functions (such as sigmoid, ReLU, etc.) to increase the model's expressive power, enabling it to fit the complex patterns hidden in the training data.</li>
<li>However, in the Transformer, the attention mechanism is essentially a linear weighted sum of the value vectors (even though the weights are obtained through nonlinear calculation of the softmax function, it's still just a linear weighting for the values). Therefore, although it can capture global dependencies, its output is still only a linear combination of the input. At this time, the Transformer model is actually lacks nonlinear capabilities.</li>
<li>So, it is necessary to add an FFN network after the self-attention layer to introduce nonlinear transformation capabilities to the model, thus improving the model's ability to model complex semantic relationships.</li>
</ul>
<p dir="auto"><h3 tabindex="-1" dir="auto">Generally, introducing nonlinear layers can play the following roles:</h3><a id="user-content-generally-introducing-nonlinear-layers-can-play-the-following-roles" aria-label="Permalink: Generally, introducing nonlinear layers can play the following roles:" href="#generally-introducing-nonlinear-layers-can-play-the-following-roles"></a></p>
<ol dir="auto">
<li>Add nonlinear capabilities to the model to facilitate the model's learning and training.</li>
<li>Enhance the model's information abstraction ability, enabling the model to represent data features and patterns at different levels during the layer-by-layer learning process. For example, the lower-layer networks can identify basic language structures (such as part-of-speech), while the higher-layer networks can understand more complex semantic information (such as sentiment, intention).</li>
<li>In addition, a current view holds that the attention layer is mainly used for input context interaction, while the FFN layer is where the LLMs mainly stores and remembers general knowledge during training (given to its nonlinear representation ability), so that it can find answers to input questions from general knowledge.</li>
</ol>
<p dir="auto"><h3 tabindex="-1" dir="auto">SwiGLU Network Structure:</h3><a id="user-content-swiglu-network-structure" aria-label="Permalink: SwiGLU Network Structure:" href="#swiglu-network-structure"></a></p>
<ol dir="auto">
<li>Perform a linear transformation on the input: <math-renderer data-static-url="https://github.githubassets.com/static" data-run-id="19d90f3d5e551008e8fb4bc953d823b8">$X^\prime = XW_3$</math-renderer>
</li>
<li>Gating unit: <math-renderer data-static-url="https://github.githubassets.com/static" data-run-id="19d90f3d5e551008e8fb4bc953d823b8">$GATE = Activation\_Function(XW_1)$</math-renderer>, which is used to selectively pass information. That is, assuming that the information in <math-renderer data-static-url="https://github.githubassets.com/static" data-run-id="19d90f3d5e551008e8fb4bc953d823b8">$X^\prime$</math-renderer> has different importance, so the information should be weighted and passed based on the score of the gating unit, thus improving the expressive ability of the model.</li>
<li>The activation function used is a Swish activation function (hence the network is called SwiGLU, which is a combination of the Swish activation function and the Gated Linear Unit (GLU)). The formula is: <math-renderer data-static-url="https://github.githubassets.com/static" data-run-id="19d90f3d5e551008e8fb4bc953d823b8">$Swish = X \cdot \sigma(\beta X)$</math-renderer>, where <math-renderer data-static-url="https://github.githubassets.com/static" data-run-id="19d90f3d5e551008e8fb4bc953d823b8">$\sigma$</math-renderer> is the sigmoid activation function. In SwiGLU, <math-renderer data-static-url="https://github.githubassets.com/static" data-run-id="19d90f3d5e551008e8fb4bc953d823b8">$\beta$</math-renderer> is set to 1 (in the original formula, it is a learnable parameter).</li>
<li>Therefore, the specific calculation of the gating unit is: <math-renderer data-static-url="https://github.githubassets.com/static" data-run-id="19d90f3d5e551008e8fb4bc953d823b8">$GATE = XW_1 \cdot \sigma(XW_1)$</math-renderer>. In PyTorch, this activation function is called silu, that is <math-renderer data-static-url="https://github.githubassets.com/static" data-run-id="19d90f3d5e551008e8fb4bc953d823b8">$GATE = silu(XW_1)$</math-renderer>.</li>
<li>Application of the gating mechanism: <math-renderer data-static-url="https://github.githubassets.com/static" data-run-id="19d90f3d5e551008e8fb4bc953d823b8">$X^\prime = X^\prime \cdot GATE$</math-renderer>
</li>
<li>Perform a linear transformation again: <math-renderer data-static-url="https://github.githubassets.com/static" data-run-id="19d90f3d5e551008e8fb4bc953d823b8">$Y = X^\prime W_2$</math-renderer>
</li>
</ol>
<p dir="auto"><h3 tabindex="-1" dir="auto">Calculation of the Dimension Size of the Hidden Layer in the Feed-Forward Layer (Based on the Official Implementation Process of Llama3):</h3><a id="user-content-calculation-of-the-dimension-size-of-the-hidden-layer-in-the-feed-forward-layer-based-on-the-official-implementation-process-of-llama3" aria-label="Permalink: Calculation of the Dimension Size of the Hidden Layer in the Feed-Forward Layer (Based on the Official Implementation Process of Llama3):" href="#calculation-of-the-dimension-size-of-the-hidden-layer-in-the-feed-forward-layer-based-on-the-official-implementation-process-of-llama3"></a></p>
<ol dir="auto">
<li>Input dimension is dim = 4096</li>
<li>hidden_dim = 4 * dim = 16384  # First, magnify it by four times. When initializing the feed-forward layer in the Transformer block, the input hidden_dim is multiplied by four.</li>
<li>hidden_dim = int(2 * hidden_dim / 3) = 10922 # Then, magnify it by 2/3 times. Such scaling is first performed within the feed-forward layer.</li>
<li>hidden_dim = int(ffn_dim_multiplier * hidden_dim) = int(1.3 * 10922) = 14198  # Then, magnify it by ffn_dim_multiplier times. The ffn_dim_multiplier is defined as 1.3 in the model configuration file.</li>
<li>hidden_dim = multiple_of * ((hidden_dim + multiple_of - 1) // multiple_of) = 1024 * ((14198 + 1024 - 1) // 1024) = 14336  # Adjust it to an integer multiple of multiple_of. The multiple_of is defined as 1024 in the model configuration file to ensure that the dimensions of all hidden layers in the model are multiples of 1024, so as to improve the computational efficiency.</li>
<li>Finally, we get the dimension size of the hidden layer is 14336.</li>
</ol>
<div dir="auto" data-snippet-clipboard-copy-content="# Calculate the feed-forward network layer
# The dimension size of the hidden layer is 14336
w1 = model[&quot;layers.0.feed_forward.w1.weight&quot;]  # [14336x4096]
w3 = model[&quot;layers.0.feed_forward.w3.weight&quot;]  # [14336x4096]
w2 = model[&quot;layers.0.feed_forward.w2.weight&quot;]  # [4096x14336]
print(w1.shape, w3.shape, w2.shape)

# output = (silu(XW1) * XW3)W2
# [17x4096] x [4096x14336] x [14336x4096] = [17x4096]
output_after_feedforward = torch.matmul(torch.functional.F.silu(torch.matmul(embedding_after_edit_normalized, w1.T)) * torch.matmul(embedding_after_edit_normalized, w3.T), w2.T)
output_after_feedforward.shape"><pre><span># Calculate the feed-forward network layer</span>
<span># The dimension size of the hidden layer is 14336</span>
<span>w1</span> <span>=</span> <span>model</span>[<span>"layers.0.feed_forward.w1.weight"</span>]  <span># [14336x4096]</span>
<span>w3</span> <span>=</span> <span>model</span>[<span>"layers.0.feed_forward.w3.weight"</span>]  <span># [14336x4096]</span>
<span>w2</span> <span>=</span> <span>model</span>[<span>"layers.0.feed_forward.w2.weight"</span>]  <span># [4096x14336]</span>
<span>print</span>(<span>w1</span>.<span>shape</span>, <span>w3</span>.<span>shape</span>, <span>w2</span>.<span>shape</span>)

<span># output = (silu(XW1) * XW3)W2</span>
<span># [17x4096] x [4096x14336] x [14336x4096] = [17x4096]</span>
<span>output_after_feedforward</span> <span>=</span> <span>torch</span>.<span>matmul</span>(<span>torch</span>.<span>functional</span>.<span>F</span>.<span>silu</span>(<span>torch</span>.<span>matmul</span>(<span>embedding_after_edit_normalized</span>, <span>w1</span>.<span>T</span>)) <span>*</span> <span>torch</span>.<span>matmul</span>(<span>embedding_after_edit_normalized</span>, <span>w3</span>.<span>T</span>), <span>w2</span>.<span>T</span>)
<span>output_after_feedforward</span>.<span>shape</span></pre></div>
<div data-snippet-clipboard-copy-content="torch.Size([14336, 4096]) torch.Size([14336, 4096]) torch.Size([4096, 14336]) torch.Size([17, 4096])"><pre><code>torch.Size([14336, 4096]) torch.Size([14336, 4096]) torch.Size([4096, 14336]) torch.Size([17, 4096])
</code></pre></div>
<p dir="auto"><h2 tabindex="-1" dir="auto">Perform the residual operation again (Finally, we get the final output of the Transformer block!)</h2><a id="user-content-perform-the-residual-operation-again-finally-we-get-the-final-output-of-the-transformer-block" aria-label="Permalink: Perform the residual operation again (Finally, we get the final output of the Transformer block!)" href="#perform-the-residual-operation-again-finally-we-get-the-final-output-of-the-transformer-block"></a></p>
<div dir="auto" data-snippet-clipboard-copy-content="# Add the output of the feed-forward layer to the original input to complete the residual operation
# This is the final result of a Transformer block
layer_0_embedding = embedding_after_edit+output_after_feedforward  # [17x4096] + [17x4096] = [17x4096]
layer_0_embedding.shape"><pre><span># Add the output of the feed-forward layer to the original input to complete the residual operation</span>
<span># This is the final result of a Transformer block</span>
<span>layer_0_embedding</span> <span>=</span> <span>embedding_after_edit</span><span>+</span><span>output_after_feedforward</span>  <span># [17x4096] + [17x4096] = [17x4096]</span>
<span>layer_0_embedding</span>.<span>shape</span></pre></div>

<p dir="auto"><h3 tabindex="-1" dir="auto">
Finally, we have the new embeddings of each token after passing through the first layer.
</h3><a id="user-content-finally-we-have-the-new-embeddings-of-each-token-after-passing-through-the-first-layer" aria-label="Permalink: 
Finally, we have the new embeddings of each token after passing through the first layer.
" href="#finally-we-have-the-new-embeddings-of-each-token-after-passing-through-the-first-layer"></a></p>
<br>
There are only 31 layers left to complete (just one for loop away).
<br>
You can imagine that this processed embedding contains all the information of the tokens proposed in the first layer.
<br>
Now, each layer will encode more complex queries in the asked question. Until the end, we will get an embedding that knows all the information about the next token we need.
<p dir="auto"><h2 tabindex="-1" dir="auto">Everything is here. Let's complete the calculation of all 32 Transformer blocks. Happy reading :)</h2><a id="user-content-everything-is-here-lets-complete-the-calculation-of-all-32-transformer-blocks-happy-reading-" aria-label="Permalink: Everything is here. Let's complete the calculation of all 32 Transformer blocks. Happy reading :)" href="#everything-is-here-lets-complete-the-calculation-of-all-32-transformer-blocks-happy-reading-"></a></p>
<p><a target="_blank" rel="noopener noreferrer" href="https://github.com/therealoliver/Deepdive-llama3-from-scratch/blob/main/images/god.png"><img src="https://github.com/therealoliver/Deepdive-llama3-from-scratch/raw/main/images/god.png" width="600px"></a>
</p>
<p dir="auto">Yes, that's it. All the work we've done before will be presented here at once to complete the calculation of each layer.
<br></p>
<div dir="auto" data-snippet-clipboard-copy-content="# Now, let's start to complete the calculation of all 32 Transformer blocks!

# Use the embeddings of the input tokens as the initial input.
final_embedding = token_embeddings_unnormalized  # [17x4096]

# Perform layer-by-layer calculation for the 32-layer Transformer blocks
for layer in range(n_layers):
    #########################################################################################################################
    ################### Round 1: Normalization - Feature Transformation - Residual Operation ###############################
    
    ########################### The first normalization ###################################################
    
    # The first normalization
    layer_embedding_norm = rms_norm(final_embedding, model[f&quot;layers.{layer}.attention_norm.weight&quot;])  # [17x4096] &amp; [4096] -> [17x4096]
    
    ################ The first feature transformation - Multi-Head Self-Attention ########################
    
    # Obtain the qkv weight matrix of the attention mechanism for the current layer
    q_layer = model[f&quot;layers.{layer}.attention.wq.weight&quot;]  # [4096x4096]
    q_layer = q_layer.view(n_heads, q_layer.shape[0] // n_heads, dim)  # [32x128x4096]
    k_layer = model[f&quot;layers.{layer}.attention.wk.weight&quot;]  # [1024x4096]
    k_layer = k_layer.view(n_kv_heads, k_layer.shape[0] // n_kv_heads, dim)  # [8x128x4096]
    v_layer = model[f&quot;layers.{layer}.attention.wv.weight&quot;]  # [1024x4096]
    v_layer = v_layer.view(n_kv_heads, v_layer.shape[0] // n_kv_heads, dim)  # [8x128x4096]
    
    # Used to store the calculation results of the attention mechanism for each head
    qkv_attention_store = []
    
    # Calculate the attention mechanism results for each head
    for head in range(n_heads):
        # Extract the QKV weight matrices corresponding to the current head
        q_layer_head = q_layer[head]  # [32x128x4096] -> [128x4096]
        k_layer_head = k_layer[head//4]  # Every 4 heads share one key weight, [8x128x4096] -> [128x4096]
        v_layer_head = v_layer[head//4]  # Every 4 heads share one value weight, [8x128x4096] -> [128x4096]
        
        # Calculate XW to obtain the QKV vectors
        # [17x4096] x [4096x128] = [17x128]
        q_per_token = torch.matmul(layer_embedding_norm, q_layer_head.T)
        k_per_token = torch.matmul(layer_embedding_norm, k_layer_head.T)
        v_per_token = torch.matmul(layer_embedding_norm, v_layer_head.T)
        
        # Add position information to the query vector (RoPE)
        q_per_token_split_into_pairs = q_per_token.float().view(q_per_token.shape[0], -1, 2)  # Divide vector into pairs along the dimensions direction to form dimension pairs. [17x128] -> [17x64x2]
        q_per_token_as_complex_numbers = torch.view_as_complex(q_per_token_split_into_pairs)  # Convert to complex number representation, (x,y) -> (x+yi). [17x64x2] -> [17x64]
        q_per_token_as_complex_numbers_rotated = q_per_token_as_complex_numbers * freqs_cis  # Calculate (x+yi)*(cosmŒ∏+sinmŒ∏i) to complete the rotation operation. [17x64] * [17x64] = [17x64]
        q_per_token_split_into_pairs_rotated = torch.view_as_real(q_per_token_as_complex_numbers_rotated)  # Convert the result back to real number representation, (x+yi) -> (x,y). [17x64] -> [17x64x2]
        q_per_token_rotated = q_per_token_split_into_pairs_rotated.view(q_per_token.shape)  # Convert the result back to the original vector shape to obtain the final query vector. [17x64x2] -> [17x128]
        
        # Add position information to the key vector (RoPE)
        k_per_token_split_into_pairs = k_per_token.float().view(k_per_token.shape[0], -1, 2)  # Divide vector into pairs along the dimensions direction to form dimension pairs. [17x128] -> [17x64x2]
        k_per_token_as_complex_numbers = torch.view_as_complex(k_per_token_split_into_pairs)  # Convert to complex number representation, (x,y) -> (x+yi). [17x64x2] -> [17x64]
        k_per_token_as_complex_numbers_rotated = k_per_token_as_complex_numbers * freqs_cis  # Calculate (x+yi)*(cosmŒ∏+sinmŒ∏i) to complete the rotation operation. [17x64] * [17x64] = [17x64]
        k_per_token_split_into_pairs_rotated = torch.view_as_real(k_per_token_as_complex_numbers_rotated)  # Convert the result back to real number representation, (x+yi) -> (x,y). [17x64] -> [17x64x2]
        k_per_token_rotated = k_per_token_split_into_pairs_rotated.view(k_per_token.shape)  # Convert the result back to the original vector shape to obtain the final key vector. [17x64x2] -> [17x128]
        
        # Calculate the attention scores and normalize the scores simultaneously (i.e., Q√óK/sqrt(dim))
        qk_per_token = torch.matmul(q_per_token_rotated, k_per_token_rotated.T)/(128)**0.5  # [17x128] x [128x17] = [17x17]
        
        # Mask the scores of future tokens
        mask = torch.full(qk_per_token.shape, float(&quot;-inf&quot;), device=qk_per_token.device)  # Create a matrix with the same shape as the attention scores, filled with negative infinity, and stored in the same device as other vectors to prevent errors in subsequent calculations. [17x17]
        mask = torch.triu(mask, diagonal=1)  # Keep the negative infinity in the upper-triangular part and set others to 0 (i.e., the upper-triangular area represents future tokens that need to be masked). The diagonal offset is 1 to avoid masking the token itself. [17x17]
        qk_per_token_after_masking = qk_per_token + mask  # Add the attention scores with the masking matrix, making the upper-triangular part of the score matrix become negative infinity, which will tend to 0 after the subsequent softmax operation. [17x17]
        
        # Calculate the attention weights (i.e., softmax(score))
        # Meanwhile, convert it back to half-precision (because it will be multiplied with the value vector v_per_token later, so the data types need to be the same).
        qk_per_token_after_masking_after_softmax = torch.nn.functional.softmax(qk_per_token_after_masking, dim=1).to(torch.bfloat16)  # Calculate the softmax row-by-row. [17x17]
        
        # Calculate the final result of the attention mechanism (i.e., softmax(score) √ó V)
        qkv_attention = torch.matmul(qk_per_token_after_masking_after_softmax, v_per_token)  # [17x17] x [17x128] = [17x128]
        
        # Record the result of this head
        qkv_attention_store.append(qkv_attention)
    
    # Merge the multi-head attention results
    stacked_qkv_attention = torch.cat(qkv_attention_store, dim=-1)  # Merge the second dimension, that is, 32x[17x128] -> [17x4096]
    
    # Perform a linear mapping on the results to generate the final multi-head self-attention mechanism results
    o_layer = model[f&quot;layers.{layer}.attention.wo.weight&quot;]
    embedding_delta = torch.matmul(stacked_qkv_attention, o_layer.T)  # [17x4096] x [4096x4096] = [17x4096]

    ########################### The first residual operation ##############################################
    
    # The first Residual Operation
    # Add the output of the attention layer to the original input to complete the residual operation
    embedding_after_edit = final_embedding + embedding_delta  # [17x4096] + [17x4096] = [17x4096]
    
    
    #########################################################################################################################
    #################### Round 2: Normalization - Feature Transformation - Residual Operation ##############################
    
    ########################### The second normalization ##################################################
    
    # The second normalization
    embedding_after_edit_normalized = rms_norm(embedding_after_edit, model[f&quot;layers.{layer}.ffn_norm.weight&quot;])  # [17x4096] &amp; [4096] -> [17x4096]
    
    ################## The second feature transformation - Feed-Forward Network ##########################
    
    # Load the parameter matrix of the feed-forward network (SwiGLU)
    w1 = model[f&quot;layers.{layer}.feed_forward.w1.weight&quot;]  # [14336x4096]
    w3 = model[f&quot;layers.{layer}.feed_forward.w3.weight&quot;]  # [14336x4096]
    w2 = model[f&quot;layers.{layer}.feed_forward.w2.weight&quot;]  # [4096x14336]
    
    # Calculate the results of the feed-forward network (output = (silu(XW1) * XW3)W2)
    # [17x4096] x [4096x14336] x [14336x4096] = [17x4096]
    output_after_feedforward = torch.matmul(torch.functional.F.silu(torch.matmul(embedding_after_edit_normalized, w1.T)) * torch.matmul(embedding_after_edit_normalized, w3.T), w2.T)
    
    ########################### The second residual operation ##############################################
    
    # The second residual operation, obtain the final output result of the current Transformer block
    # Add the output of the feed-forward layer to the original input to complete the residual operation
    final_embedding = embedding_after_edit+output_after_feedforward  # [17x4096] + [17x4096] = [17x4096]"><pre><span># Now, let's start to complete the calculation of all 32 Transformer blocks!</span>

<span># Use the embeddings of the input tokens as the initial input.</span>
<span>final_embedding</span> <span>=</span> <span>token_embeddings_unnormalized</span>  <span># [17x4096]</span>

<span># Perform layer-by-layer calculation for the 32-layer Transformer blocks</span>
<span>for</span> <span>layer</span> <span>in</span> <span>range</span>(<span>n_layers</span>):
    <span>#########################################################################################################################</span>
    <span>################### Round 1: Normalization - Feature Transformation - Residual Operation ###############################</span>
    
    <span>########################### The first normalization ###################################################</span>
    
    <span># The first normalization</span>
    <span>layer_embedding_norm</span> <span>=</span> <span>rms_norm</span>(<span>final_embedding</span>, <span>model</span>[<span>f"layers.<span><span>{</span><span>layer</span><span>}</span></span>.attention_norm.weight"</span>])  <span># [17x4096] &amp; [4096] -&gt; [17x4096]</span>
    
    <span>################ The first feature transformation - Multi-Head Self-Attention ########################</span>
    
    <span># Obtain the qkv weight matrix of the attention mechanism for the current layer</span>
    <span>q_layer</span> <span>=</span> <span>model</span>[<span>f"layers.<span><span>{</span><span>layer</span><span>}</span></span>.attention.wq.weight"</span>]  <span># [4096x4096]</span>
    <span>q_layer</span> <span>=</span> <span>q_layer</span>.<span>view</span>(<span>n_heads</span>, <span>q_layer</span>.<span>shape</span>[<span>0</span>] <span>//</span> <span>n_heads</span>, <span>dim</span>)  <span># [32x128x4096]</span>
    <span>k_layer</span> <span>=</span> <span>model</span>[<span>f"layers.<span><span>{</span><span>layer</span><span>}</span></span>.attention.wk.weight"</span>]  <span># [1024x4096]</span>
    <span>k_layer</span> <span>=</span> <span>k_layer</span>.<span>view</span>(<span>n_kv_heads</span>, <span>k_layer</span>.<span>shape</span>[<span>0</span>] <span>//</span> <span>n_kv_heads</span>, <span>dim</span>)  <span># [8x128x4096]</span>
    <span>v_layer</span> <span>=</span> <span>model</span>[<span>f"layers.<span><span>{</span><span>layer</span><span>}</span></span>.attention.wv.weight"</span>]  <span># [1024x4096]</span>
    <span>v_layer</span> <span>=</span> <span>v_layer</span>.<span>view</span>(<span>n_kv_heads</span>, <span>v_layer</span>.<span>shape</span>[<span>0</span>] <span>//</span> <span>n_kv_heads</span>, <span>dim</span>)  <span># [8x128x4096]</span>
    
    <span># Used to store the calculation results of the attention mechanism for each head</span>
    <span>qkv_attention_store</span> <span>=</span> []
    
    <span># Calculate the attention mechanism results for each head</span>
    <span>for</span> <span>head</span> <span>in</span> <span>range</span>(<span>n_heads</span>):
        <span># Extract the QKV weight matrices corresponding to the current head</span>
        <span>q_layer_head</span> <span>=</span> <span>q_layer</span>[<span>head</span>]  <span># [32x128x4096] -&gt; [128x4096]</span>
        <span>k_layer_head</span> <span>=</span> <span>k_layer</span>[<span>head</span><span>//</span><span>4</span>]  <span># Every 4 heads share one key weight, [8x128x4096] -&gt; [128x4096]</span>
        <span>v_layer_head</span> <span>=</span> <span>v_layer</span>[<span>head</span><span>//</span><span>4</span>]  <span># Every 4 heads share one value weight, [8x128x4096] -&gt; [128x4096]</span>
        
        <span># Calculate XW to obtain the QKV vectors</span>
        <span># [17x4096] x [4096x128] = [17x128]</span>
        <span>q_per_token</span> <span>=</span> <span>torch</span>.<span>matmul</span>(<span>layer_embedding_norm</span>, <span>q_layer_head</span>.<span>T</span>)
        <span>k_per_token</span> <span>=</span> <span>torch</span>.<span>matmul</span>(<span>layer_embedding_norm</span>, <span>k_layer_head</span>.<span>T</span>)
        <span>v_per_token</span> <span>=</span> <span>torch</span>.<span>matmul</span>(<span>layer_embedding_norm</span>, <span>v_layer_head</span>.<span>T</span>)
        
        <span># Add position information to the query vector (RoPE)</span>
        <span>q_per_token_split_into_pairs</span> <span>=</span> <span>q_per_token</span>.<span>float</span>().<span>view</span>(<span>q_per_token</span>.<span>shape</span>[<span>0</span>], <span>-</span><span>1</span>, <span>2</span>)  <span># Divide vector into pairs along the dimensions direction to form dimension pairs. [17x128] -&gt; [17x64x2]</span>
        <span>q_per_token_as_complex_numbers</span> <span>=</span> <span>torch</span>.<span>view_as_complex</span>(<span>q_per_token_split_into_pairs</span>)  <span># Convert to complex number representation, (x,y) -&gt; (x+yi). [17x64x2] -&gt; [17x64]</span>
        <span>q_per_token_as_complex_numbers_rotated</span> <span>=</span> <span>q_per_token_as_complex_numbers</span> <span>*</span> <span>freqs_cis</span>  <span># Calculate (x+yi)*(cosmŒ∏+sinmŒ∏i) to complete the rotation operation. [17x64] * [17x64] = [17x64]</span>
        <span>q_per_token_split_into_pairs_rotated</span> <span>=</span> <span>torch</span>.<span>view_as_real</span>(<span>q_per_token_as_complex_numbers_rotated</span>)  <span># Convert the result back to real number representation, (x+yi) -&gt; (x,y). [17x64] -&gt; [17x64x2]</span>
        <span>q_per_token_rotated</span> <span>=</span> <span>q_per_token_split_into_pairs_rotated</span>.<span>view</span>(<span>q_per_token</span>.<span>shape</span>)  <span># Convert the result back to the original vector shape to obtain the final query vector. [17x64x2] -&gt; [17x128]</span>
        
        <span># Add position information to the key vector (RoPE)</span>
        <span>k_per_token_split_into_pairs</span> <span>=</span> <span>k_per_token</span>.<span>float</span>().<span>view</span>(<span>k_per_token</span>.<span>shape</span>[<span>0</span>], <span>-</span><span>1</span>, <span>2</span>)  <span># Divide vector into pairs along the dimensions direction to form dimension pairs. [17x128] -&gt; [17x64x2]</span>
        <span>k_per_token_as_complex_numbers</span> <span>=</span> <span>torch</span>.<span>view_as_complex</span>(<span>k_per_token_split_into_pairs</span>)  <span># Convert to complex number representation, (x,y) -&gt; (x+yi). [17x64x2] -&gt; [17x64]</span>
        <span>k_per_token_as_complex_numbers_rotated</span> <span>=</span> <span>k_per_token_as_complex_numbers</span> <span>*</span> <span>freqs_cis</span>  <span># Calculate (x+yi)*(cosmŒ∏+sinmŒ∏i) to complete the rotation operation. [17x64] * [17x64] = [17x64]</span>
        <span>k_per_token_split_into_pairs_rotated</span> <span>=</span> <span>torch</span>.<span>view_as_real</span>(<span>k_per_token_as_complex_numbers_rotated</span>)  <span># Convert the result back to real number representation, (x+yi) -&gt; (x,y). [17x64] -&gt; [17x64x2]</span>
        <span>k_per_token_rotated</span> <span>=</span> <span>k_per_token_split_into_pairs_rotated</span>.<span>view</span>(<span>k_per_token</span>.<span>shape</span>)  <span># Convert the result back to the original vector shape to obtain the final key vector. [17x64x2] -&gt; [17x128]</span>
        
        <span># Calculate the attention scores and normalize the scores simultaneously (i.e., Q√óK/sqrt(dim))</span>
        <span>qk_per_token</span> <span>=</span> <span>torch</span>.<span>matmul</span>(<span>q_per_token_rotated</span>, <span>k_per_token_rotated</span>.<span>T</span>)<span>/</span>(<span>128</span>)<span>**</span><span>0.5</span>  <span># [17x128] x [128x17] = [17x17]</span>
        
        <span># Mask the scores of future tokens</span>
        <span>mask</span> <span>=</span> <span>torch</span>.<span>full</span>(<span>qk_per_token</span>.<span>shape</span>, <span>float</span>(<span>"-inf"</span>), <span>device</span><span>=</span><span>qk_per_token</span>.<span>device</span>)  <span># Create a matrix with the same shape as the attention scores, filled with negative infinity, and stored in the same device as other vectors to prevent errors in subsequent calculations. [17x17]</span>
        <span>mask</span> <span>=</span> <span>torch</span>.<span>triu</span>(<span>mask</span>, <span>diagonal</span><span>=</span><span>1</span>)  <span># Keep the negative infinity in the upper-triangular part and set others to 0 (i.e., the upper-triangular area represents future tokens that need to be masked). The diagonal offset is 1 to avoid masking the token itself. [17x17]</span>
        <span>qk_per_token_after_masking</span> <span>=</span> <span>qk_per_token</span> <span>+</span> <span>mask</span>  <span># Add the attention scores with the masking matrix, making the upper-triangular part of the score matrix become negative infinity, which will tend to 0 after the subsequent softmax operation. [17x17]</span>
        
        <span># Calculate the attention weights (i.e., softmax(score))</span>
        <span># Meanwhile, convert it back to half-precision (because it will be multiplied with the value vector v_per_token later, so the data types need to be the same).</span>
        <span>qk_per_token_after_masking_after_softmax</span> <span>=</span> <span>torch</span>.<span>nn</span>.<span>functional</span>.<span>softmax</span>(<span>qk_per_token_after_masking</span>, <span>dim</span><span>=</span><span>1</span>).<span>to</span>(<span>torch</span>.<span>bfloat16</span>)  <span># Calculate the softmax row-by-row. [17x17]</span>
        
        <span># Calculate the final result of the attention mechanism (i.e., softmax(score) √ó V)</span>
        <span>qkv_attention</span> <span>=</span> <span>torch</span>.<span>matmul</span>(<span>qk_per_token_after_masking_after_softmax</span>, <span>v_per_token</span>)  <span># [17x17] x [17x128] = [17x128]</span>
        
        <span># Record the result of this head</span>
        <span>qkv_attention_store</span>.<span>append</span>(<span>qkv_attention</span>)
    
    <span># Merge the multi-head attention results</span>
    <span>stacked_qkv_attention</span> <span>=</span> <span>torch</span>.<span>cat</span>(<span>qkv_attention_store</span>, <span>dim</span><span>=</span><span>-</span><span>1</span>)  <span># Merge the second dimension, that is, 32x[17x128] -&gt; [17x4096]</span>
    
    <span># Perform a linear mapping on the results to generate the final multi-head self-attention mechanism results</span>
    <span>o_layer</span> <span>=</span> <span>model</span>[<span>f"layers.<span><span>{</span><span>layer</span><span>}</span></span>.attention.wo.weight"</span>]
    <span>embedding_delta</span> <span>=</span> <span>torch</span>.<span>matmul</span>(<span>stacked_qkv_attention</span>, <span>o_layer</span>.<span>T</span>)  <span># [17x4096] x [4096x4096] = [17x4096]</span>

    <span>########################### The first residual operation ##############################################</span>
    
    <span># The first Residual Operation</span>
    <span># Add the output of the attention layer to the original input to complete the residual operation</span>
    <span>embedding_after_edit</span> <span>=</span> <span>final_embedding</span> <span>+</span> <span>embedding_delta</span>  <span># [17x4096] + [17x4096] = [17x4096]</span>
    
    
    <span>#########################################################################################################################</span>
    <span>#################### Round 2: Normalization - Feature Transformation - Residual Operation ##############################</span>
    
    <span>########################### The second normalization ##################################################</span>
    
    <span># The second normalization</span>
    <span>embedding_after_edit_normalized</span> <span>=</span> <span>rms_norm</span>(<span>embedding_after_edit</span>, <span>model</span>[<span>f"layers.<span><span>{</span><span>layer</span><span>}</span></span>.ffn_norm.weight"</span>])  <span># [17x4096] &amp; [4096] -&gt; [17x4096]</span>
    
    <span>################## The second feature transformation - Feed-Forward Network ##########################</span>
    
    <span># Load the parameter matrix of the feed-forward network (SwiGLU)</span>
    <span>w1</span> <span>=</span> <span>model</span>[<span>f"layers.<span><span>{</span><span>layer</span><span>}</span></span>.feed_forward.w1.weight"</span>]  <span># [14336x4096]</span>
    <span>w3</span> <span>=</span> <span>model</span>[<span>f"layers.<span><span>{</span><span>layer</span><span>}</span></span>.feed_forward.w3.weight"</span>]  <span># [14336x4096]</span>
    <span>w2</span> <span>=</span> <span>model</span>[<span>f"layers.<span><span>{</span><span>layer</span><span>}</span></span>.feed_forward.w2.weight"</span>]  <span># [4096x14336]</span>
    
    <span># Calculate the results of the feed-forward network (output = (silu(XW1) * XW3)W2)</span>
    <span># [17x4096] x [4096x14336] x [14336x4096] = [17x4096]</span>
    <span>output_after_feedforward</span> <span>=</span> <span>torch</span>.<span>matmul</span>(<span>torch</span>.<span>functional</span>.<span>F</span>.<span>silu</span>(<span>torch</span>.<span>matmul</span>(<span>embedding_after_edit_normalized</span>, <span>w1</span>.<span>T</span>)) <span>*</span> <span>torch</span>.<span>matmul</span>(<span>embedding_after_edit_normalized</span>, <span>w3</span>.<span>T</span>), <span>w2</span>.<span>T</span>)
    
    <span>########################### The second residual operation ##############################################</span>
    
    <span># The second residual operation, obtain the final output result of the current Transformer block</span>
    <span># Add the output of the feed-forward layer to the original input to complete the residual operation</span>
    <span>final_embedding</span> <span>=</span> <span>embedding_after_edit</span><span>+</span><span>output_after_feedforward</span>  <span># [17x4096] + [17x4096] = [17x4096]</span></pre></div>
<p dir="auto"><h2 tabindex="-1" dir="auto">Let's complete the last step and predict the next token</h2><a id="user-content-lets-complete-the-last-step-and-predict-the-next-token" aria-label="Permalink: Let's complete the last step and predict the next token" href="#lets-complete-the-last-step-and-predict-the-next-token"></a></p>
<p dir="auto">Now we have obtained the final embeddings, which contains all the information we needed to predict the next token.
<br>
The shape of this embedding is the same as that of the input token embedding, both being [17x4096], where 17 is the number of tokens and 4096 is the dimension of the embedding.</p>
<p><a target="_blank" rel="noopener noreferrer" href="https://github.com/therealoliver/Deepdive-llama3-from-scratch/blob/main/images/last_norm.png"><img src="https://github.com/therealoliver/Deepdive-llama3-from-scratch/raw/main/images/last_norm.png" width="600px"></a>
</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">First, perform one last normalization on the output of the last Transformer layer</h2><a id="user-content-first-perform-one-last-normalization-on-the-output-of-the-last-transformer-layer" aria-label="Permalink: First, perform one last normalization on the output of the last Transformer layer" href="#first-perform-one-last-normalization-on-the-output-of-the-last-transformer-layer"></a></p>
<div dir="auto" data-snippet-clipboard-copy-content="# Perform the last normalization in the entire model
final_embedding = rms_norm(final_embedding, model[&quot;norm.weight&quot;])  # [17x4096] &amp; [4096] -> [17x4096]
final_embedding.shape"><pre><span># Perform the last normalization in the entire model</span>
<span>final_embedding</span> <span>=</span> <span>rms_norm</span>(<span>final_embedding</span>, <span>model</span>[<span>"norm.weight"</span>])  <span># [17x4096] &amp; [4096] -&gt; [17x4096]</span>
<span>final_embedding</span>.<span>shape</span></pre></div>

<p dir="auto"><h2 tabindex="-1" dir="auto">Then, make the prediction based on the embedding corresponding to the last token (perform a linear mapping to the vocabulary dimension)</h2><a id="user-content-then-make-the-prediction-based-on-the-embedding-corresponding-to-the-last-token-perform-a-linear-mapping-to-the-vocabulary-dimension" aria-label="Permalink: Then, make the prediction based on the embedding corresponding to the last token (perform a linear mapping to the vocabulary dimension)" href="#then-make-the-prediction-based-on-the-embedding-corresponding-to-the-last-token-perform-a-linear-mapping-to-the-vocabulary-dimension"></a></p>
<p><a target="_blank" rel="noopener noreferrer" href="https://github.com/therealoliver/Deepdive-llama3-from-scratch/blob/main/images/finallayer.png"><img src="https://github.com/therealoliver/Deepdive-llama3-from-scratch/raw/main/images/finallayer.png" width="600px"></a>
</p>
<br>
We will use the output decoder (a linear mapping layer) to convert the embedding vector of the last token into a prediction result for the next token (the dimension is the size of the vocabulary. If we apply a softmax function to the result, the value of each dimension represents the probability that the next token belongs to that word).
<div dir="auto"><p>Why do we only use the output vector of the last token to predict the next token?
<br>
Because during training, the model's objective is to predict the next token based on the current token and all previous tokens. Therefore, the output vector corresponding to each token is used to predict the next token relative to itself, rather than the next token for the entire input.
</p></div>
<p dir="auto">We hope the answer is 42 in our example :)
<br>
Note: 42 is the answer to "the answer to the ultimate question of life, the universe, and everything is " according to the book <em>The Hitchhiker's Guide to the Galaxy</em>. Most modern large language models will answer 42, which will verify the correctness of our entire code! Good luck to us :)</p>
<div dir="auto" data-snippet-clipboard-copy-content="# Perform the last linear mapping to map the embeddings to the size of the vocabulary dimension as a prediction for the next token
logits = torch.matmul(final_embedding[-1], model[&quot;output.weight&quot;].T)  # [17x4096] -> [4096] -> [4096] x [4096x128256] = [128256]
logits.shape"><pre><span># Perform the last linear mapping to map the embeddings to the size of the vocabulary dimension as a prediction for the next token</span>
<span>logits</span> <span>=</span> <span>torch</span>.<span>matmul</span>(<span>final_embedding</span>[<span>-</span><span>1</span>], <span>model</span>[<span>"output.weight"</span>].<span>T</span>)  <span># [17x4096] -&gt; [4096] -&gt; [4096] x [4096x128256] = [128256]</span>
<span>logits</span>.<span>shape</span></pre></div>

<p dir="auto"><h2 tabindex="-1" dir="auto">Here's the prediction result!</h2><a id="user-content-heres-the-prediction-result" aria-label="Permalink: Here's the prediction result!" href="#heres-the-prediction-result"></a></p>
<div dir="auto" data-snippet-clipboard-copy-content="# Extract the id corresponding to the dimension with the highest probability,
# is gonna be the predicted next token's id
next_token = torch.argmax(logits, dim=-1)  # Get the index corresponding to the maximum value, which is the predicted next token id. [128256] -> [1]
next_token"><pre><span># Extract the id corresponding to the dimension with the highest probability,</span>
<span># is gonna be the predicted next token's id</span>
<span>next_token</span> <span>=</span> <span>torch</span>.<span>argmax</span>(<span>logits</span>, <span>dim</span><span>=</span><span>-</span><span>1</span>)  <span># Get the index corresponding to the maximum value, which is the predicted next token id. [128256] -&gt; [1]</span>
<span>next_token</span></pre></div>

<div dir="auto" data-snippet-clipboard-copy-content="# Based on the predicted id, restore it to the specific predicted value
tokenizer.decode([next_token.item()])"><pre><span># Based on the predicted id, restore it to the specific predicted value</span>
<span>tokenizer</span>.<span>decode</span>([<span>next_token</span>.<span>item</span>()])</pre></div>

<p><a target="_blank" rel="noopener noreferrer" href="https://github.com/therealoliver/Deepdive-llama3-from-scratch/blob/main/images/42.png"><img src="https://github.com/therealoliver/Deepdive-llama3-from-scratch/raw/main/images/42.png" width="600px"></a>
</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Let's dive deeper and see how different embeddings or token masking strategies might affect the prediction results :)</h2><a id="user-content-lets-dive-deeper-and-see-how-different-embeddings-or-token-masking-strategies-might-affect-the-prediction-results-" aria-label="Permalink: Let's dive deeper and see how different embeddings or token masking strategies might affect the prediction results :)" href="#lets-dive-deeper-and-see-how-different-embeddings-or-token-masking-strategies-might-affect-the-prediction-results-"></a></p>
<p dir="auto">Now we've got the final prediction results. If you're still interested, let's explore some of the issues that might have been mentioned before~
<br></p>
<p dir="auto">We'll briefly explore three scenarios:</p>
<ol dir="auto">
<li>Apart from the top-1 result, what else is predicted in the current prediction, that is, the top-k results?</li>
<li>What can be predicted if we use the output embedding of other tokens for prediction?</li>
<li>If the future tokens were not masked during the attention calculation before, how would the prediction results differ?</li>
</ol>
<div dir="auto" data-snippet-clipboard-copy-content="# Let's first take a look at the top-k prediction results
logits_sort, logits_idx = torch.sort(logits, dim=-1, descending=True)  # Put the token with the highest probability prediction at the front, [128256]
[tokenizer.decode([i]) for i in logits_idx[:10]]  # View the top 10 high-probability results"><pre><span># Let's first take a look at the top-k prediction results</span>
<span>logits_sort</span>, <span>logits_idx</span> <span>=</span> <span>torch</span>.<span>sort</span>(<span>logits</span>, <span>dim</span><span>=</span><span>-</span><span>1</span>, <span>descending</span><span>=</span><span>True</span>)  <span># Put the token with the highest probability prediction at the front, [128256]</span>
[<span>tokenizer</span>.<span>decode</span>([<span>i</span>]) <span>for</span> <span>i</span> <span>in</span> <span>logits_idx</span>[:<span>10</span>]]  <span># View the top 10 high-probability results</span></pre></div>
<div data-snippet-clipboard-copy-content="['42', '6', '43', '41', '4', '1', '45', '3', '2', '46']"><pre><code>['42', '6', '43', '41', '4', '1', '45', '3', '2', '46']
</code></pre></div>
<div dir="auto" data-snippet-clipboard-copy-content="# Next, let's to see what can we get by using the embeddings of other tokens for prediction
logits_all_token = torch.matmul(final_embedding, model[&quot;output.weight&quot;].T)  # Map the embeddings to the same size as the vocabulary, [17x4096] x [4096x128256] = [17x128256]
logits_all_token_sort, logits_all_token_idx = torch.sort(logits_all_token, dim=-1, descending=True)  # Put the token with the highest probability prediction at the front, [17x128256]

print('Input tokens:', prompt_split_as_tokens)  # Display the input tokens, [17]

# Display the results of the next-token prediction based on the output embedding of each token
for i in range(len(final_embedding)):
    print(f'Predict results based on {i+1}th token:', [tokenizer.decode([j]) for j in logits_all_token_idx[i][:10]])  # Output the top 10 high-probability results
    
_=&quot;&quot;&quot;
It can be seen that when making predictions based on each token, the prediction result is the possible result of the next token after the &quot;current token&quot;,
rather than the prediction result of the entire complete input.
Therefore, in actual prediction, only the embedding of the last token will be used for prediction.
&quot;&quot;&quot;"><pre><span># Next, let's to see what can we get by using the embeddings of other tokens for prediction</span>
<span>logits_all_token</span> <span>=</span> <span>torch</span>.<span>matmul</span>(<span>final_embedding</span>, <span>model</span>[<span>"output.weight"</span>].<span>T</span>)  <span># Map the embeddings to the same size as the vocabulary, [17x4096] x [4096x128256] = [17x128256]</span>
<span>logits_all_token_sort</span>, <span>logits_all_token_idx</span> <span>=</span> <span>torch</span>.<span>sort</span>(<span>logits_all_token</span>, <span>dim</span><span>=</span><span>-</span><span>1</span>, <span>descending</span><span>=</span><span>True</span>)  <span># Put the token with the highest probability prediction at the front, [17x128256]</span>

<span>print</span>(<span>'Input tokens:'</span>, <span>prompt_split_as_tokens</span>)  <span># Display the input tokens, [17]</span>

<span># Display the results of the next-token prediction based on the output embedding of each token</span>
<span>for</span> <span>i</span> <span>in</span> <span>range</span>(<span>len</span>(<span>final_embedding</span>)):
    <span>print</span>(<span>f'Predict results based on <span><span>{</span><span>i</span><span>+</span><span>1</span><span>}</span></span>th token:'</span>, [<span>tokenizer</span>.<span>decode</span>([<span>j</span>]) <span>for</span> <span>j</span> <span>in</span> <span>logits_all_token_idx</span>[<span>i</span>][:<span>10</span>]])  <span># Output the top 10 high-probability results</span>
    
<span>_</span><span>=</span><span>"""</span>
<span>It can be seen that when making predictions based on each token, the prediction result is the possible result of the next token after the "current token",</span>
<span>rather than the prediction result of the entire complete input.</span>
<span>Therefore, in actual prediction, only the embedding of the last token will be used for prediction.</span>
<span>"""</span></pre></div>
<div data-snippet-clipboard-copy-content="Input tokens: ['<|begin_of_text|>', 'the', ' answer', ' to', ' the', ' ultimate', ' question', ' of', ' life', ',', ' the', ' universe', ',', ' and', ' everything', ' is', ' ']
Predict results based on 1th token: ['Question', 'def', '#', 'The', 'import', 'Tags', 'A', 'package', 'Home', 'I']
Predict results based on 2th token: [' ', ' best', ' first', ' most', ' new', ' world', ' last', ' same', ' way', ' number']
Predict results based on 3th token: [' to', ' is', ' was', ' of', ' lies', ',', ' for', ' you', ' key', ' will']
Predict results based on 4th token: [' the', ' this', ' your', ' all', ' that', ' a', ' my', ' life', ' &quot;', ' everything']
Predict results based on 5th token: [' question', ' problem', ' above', ' ultimate', ' first', ' r', ' following', ' questions', ' most', ' previous']
Predict results based on 6th token: [' question', ' questions', ' mystery', '\xa0', ' quest', '\n', ' life', ' philosophical', ' qu', ' problem']
Predict results based on 7th token: [' of', '\n', ' to', ' is', '?\n', ',', '.\n', ':', '...\n', ' about']
Predict results based on 8th token: [' life', ' Life', ' the', '\xa0', ' everything', ' existence', '\n', ' LIFE', ' all', ' human']
Predict results based on 9th token: [',', ' the', '\n', ' and', ' is', ',\n', '.\n', '?\n', '...\n', '...']
Predict results based on 10th token: [' the', ' universe', ' and', ' etc', '\xa0', ' is', ' death', ' of', ' or', ' everything']
Predict results based on 11th token: [' universe', ' Universe', '\n', '\xa0', ' un', ' univers', ' uni', ' cosmos', ' universal', ' u']
Predict results based on 12th token: [',', ' and', ' &amp;', '\n', ',\n', ' ,', '...\n', ',and', '...', '\xa0']
Predict results based on 13th token: [' and', ' everything', ' &amp;', ' the', ' etc', '\xa0', ' is', ' or', ' ...\n', ' an']
Predict results based on 14th token: [' everything', '\xa0', ' the', ' every', '\n', ' ever', ' all', ' Everything', ' EVERY', '...']
Predict results based on 15th token: ['\n', ' is', '.\n', '.', '?\n', ',', ' (', '\n\n', '...\n', ' in']
Predict results based on 16th token: [' ', '\n', '...', '...\n', ':', ' forty', ' not', ' &quot;', '‚Ä¶', ' a']
Predict results based on 17th token: ['42', '6', '43', '41', '4', '1', '45', '3', '2', '46']"><pre><code>Input tokens: ['&lt;|begin_of_text|&gt;', 'the', ' answer', ' to', ' the', ' ultimate', ' question', ' of', ' life', ',', ' the', ' universe', ',', ' and', ' everything', ' is', ' ']
Predict results based on 1th token: ['Question', 'def', '#', 'The', 'import', 'Tags', 'A', 'package', 'Home', 'I']
Predict results based on 2th token: [' ', ' best', ' first', ' most', ' new', ' world', ' last', ' same', ' way', ' number']
Predict results based on 3th token: [' to', ' is', ' was', ' of', ' lies', ',', ' for', ' you', ' key', ' will']
Predict results based on 4th token: [' the', ' this', ' your', ' all', ' that', ' a', ' my', ' life', ' "', ' everything']
Predict results based on 5th token: [' question', ' problem', ' above', ' ultimate', ' first', ' r', ' following', ' questions', ' most', ' previous']
Predict results based on 6th token: [' question', ' questions', ' mystery', '\xa0', ' quest', '\n', ' life', ' philosophical', ' qu', ' problem']
Predict results based on 7th token: [' of', '\n', ' to', ' is', '?\n', ',', '.\n', ':', '...\n', ' about']
Predict results based on 8th token: [' life', ' Life', ' the', '\xa0', ' everything', ' existence', '\n', ' LIFE', ' all', ' human']
Predict results based on 9th token: [',', ' the', '\n', ' and', ' is', ',\n', '.\n', '?\n', '...\n', '...']
Predict results based on 10th token: [' the', ' universe', ' and', ' etc', '\xa0', ' is', ' death', ' of', ' or', ' everything']
Predict results based on 11th token: [' universe', ' Universe', '\n', '\xa0', ' un', ' univers', ' uni', ' cosmos', ' universal', ' u']
Predict results based on 12th token: [',', ' and', ' &amp;', '\n', ',\n', ' ,', '...\n', ',and', '...', '\xa0']
Predict results based on 13th token: [' and', ' everything', ' &amp;', ' the', ' etc', '\xa0', ' is', ' or', ' ...\n', ' an']
Predict results based on 14th token: [' everything', '\xa0', ' the', ' every', '\n', ' ever', ' all', ' Everything', ' EVERY', '...']
Predict results based on 15th token: ['\n', ' is', '.\n', '.', '?\n', ',', ' (', '\n\n', '...\n', ' in']
Predict results based on 16th token: [' ', '\n', '...', '...\n', ':', ' forty', ' not', ' "', '‚Ä¶', ' a']
Predict results based on 17th token: ['42', '6', '43', '41', '4', '1', '45', '3', '2', '46']
</code></pre></div>
<div dir="auto" data-snippet-clipboard-copy-content="# Finally, let's take a look at what the prediction results will be if we don't mask future tokens when calculating attention
# At this time, the prediction results based on each token will be as follows
# It can be seen that due to the visibility of future tokens, the embeddings of each token will more accurately predict &quot;the next token for it&quot; (it's a bit like &quot;cheating&quot;) 

_=&quot;&quot;&quot;
Input tokens: ['<|begin_of_text|>', 'the', ' answer', ' to', ' the', ' ultimate', ' question', ' of', ' life', ',', ' the', ' universe', ',', ' and', ' everything', ' is', ' ']
Predict results based on 1th token: ['://', '.Forms', '_REF', ' Angeles', '.swing', 'ÔøΩ', 'php', '–≤–æ', 'ysics', 'ÔøΩ']
Predict results based on 2th token: [' answer', ' Hitch', ' universe', ' question', ' ultimate', ' meaning', ' hitch', ' Universe', ' Answer', ' reason']
Predict results based on 3th token: [' to', ' is', ',', ':', ' was', '\n', ' ', ' (', '\n\n', ' of']
Predict results based on 4th token: [' the', ' life', ' this', ' which', ' everything', ' that', ' how', ' why', ' ', ' all']
Predict results based on 5th token: [' ultimate', ' question', ' great', ' meaning', ' universe', ' Ultimate', ' everything', ' life', ' holy', ' greatest']
Predict results based on 6th token: [' question', ' answer', ' is', ' was', '\n', ' questions', ' mystery', '\n\n', ' what', ' Question']
Predict results based on 7th token: [' of', ' is', '\n', ',', ' about', ':', ' to', ' in', ' (', '<|end_of_text|>']
Predict results based on 8th token: [' life', ' existence', ' everything', ' Life', ' the', ' death', ' time', ' all', ' why', ' which']
Predict results based on 9th token: [',', ' is', ' the', '\n', ':', ' (', '...', ' and', ' ,', ' -']
Predict results based on 10th token: [' the', ' and', ' is', ' death', ' The', ' which', ' or', '\xa0', ' existence', ' don']
Predict results based on 11th token: [' universe', ' answer', ' cosmos', ' world', ' existence', ' Universe', ' everything', ' un', ' meaning', ' question']
Predict results based on 12th token: [',', ' and', ' is', ' &amp;', '\n', ' ,', '.', '...', ' (', ' ']
Predict results based on 13th token: [' and', ' &amp;', ' don', ' the', ' is', ' a', ' or', ' Douglas', '\xa0', '<|end_of_text|>']
Predict results based on 14th token: [' everything', ' dough', ' don', ' ever', ' deep', ' Douglas', ' the', ' every', ' all', ' death']
Predict results based on 15th token: ['\n', ' is', ',', '.', ' ', ' (', ':', '<|end_of_text|>', '\n\n', '.\n']
Predict results based on 16th token: [' ', '\n', ' forty', '...', ' &quot;', '42', ' the', ':', '\xa0', ' to']
Predict results based on 17th token: ['42', '6', '4', '41', '1', '2', '3', '7', '5', '43']
&quot;&quot;&quot;"><pre><span># Finally, let's take a look at what the prediction results will be if we don't mask future tokens when calculating attention</span>
<span># At this time, the prediction results based on each token will be as follows</span>
<span># It can be seen that due to the visibility of future tokens, the embeddings of each token will more accurately predict "the next token for it" (it's a bit like "cheating") </span>

<span>_</span><span>=</span><span>"""</span>
<span>Input tokens: ['&lt;|begin_of_text|&gt;', 'the', ' answer', ' to', ' the', ' ultimate', ' question', ' of', ' life', ',', ' the', ' universe', ',', ' and', ' everything', ' is', ' ']</span>
<span>Predict results based on 1th token: ['://', '.Forms', '_REF', ' Angeles', '.swing', 'ÔøΩ', 'php', '–≤–æ', 'ysics', 'ÔøΩ']</span>
<span>Predict results based on 2th token: [' answer', ' Hitch', ' universe', ' question', ' ultimate', ' meaning', ' hitch', ' Universe', ' Answer', ' reason']</span>
<span>Predict results based on 3th token: [' to', ' is', ',', ':', ' was', '<span>\n</span>', ' ', ' (', '<span>\n</span><span>\n</span>', ' of']</span>
<span>Predict results based on 4th token: [' the', ' life', ' this', ' which', ' everything', ' that', ' how', ' why', ' ', ' all']</span>
<span>Predict results based on 5th token: [' ultimate', ' question', ' great', ' meaning', ' universe', ' Ultimate', ' everything', ' life', ' holy', ' greatest']</span>
<span>Predict results based on 6th token: [' question', ' answer', ' is', ' was', '<span>\n</span>', ' questions', ' mystery', '<span>\n</span><span>\n</span>', ' what', ' Question']</span>
<span>Predict results based on 7th token: [' of', ' is', '<span>\n</span>', ',', ' about', ':', ' to', ' in', ' (', '&lt;|end_of_text|&gt;']</span>
<span>Predict results based on 8th token: [' life', ' existence', ' everything', ' Life', ' the', ' death', ' time', ' all', ' why', ' which']</span>
<span>Predict results based on 9th token: [',', ' is', ' the', '<span>\n</span>', ':', ' (', '...', ' and', ' ,', ' -']</span>
<span>Predict results based on 10th token: [' the', ' and', ' is', ' death', ' The', ' which', ' or', '<span>\xa0</span>', ' existence', ' don']</span>
<span>Predict results based on 11th token: [' universe', ' answer', ' cosmos', ' world', ' existence', ' Universe', ' everything', ' un', ' meaning', ' question']</span>
<span>Predict results based on 12th token: [',', ' and', ' is', ' &amp;', '<span>\n</span>', ' ,', '.', '...', ' (', ' ']</span>
<span>Predict results based on 13th token: [' and', ' &amp;', ' don', ' the', ' is', ' a', ' or', ' Douglas', '<span>\xa0</span>', '&lt;|end_of_text|&gt;']</span>
<span>Predict results based on 14th token: [' everything', ' dough', ' don', ' ever', ' deep', ' Douglas', ' the', ' every', ' all', ' death']</span>
<span>Predict results based on 15th token: ['<span>\n</span>', ' is', ',', '.', ' ', ' (', ':', '&lt;|end_of_text|&gt;', '<span>\n</span><span>\n</span>', '.<span>\n</span>']</span>
<span>Predict results based on 16th token: [' ', '<span>\n</span>', ' forty', '...', ' "', '42', ' the', ':', '<span>\xa0</span>', ' to']</span>
<span>Predict results based on 17th token: ['42', '6', '4', '41', '1', '2', '3', '7', '5', '43']</span>
<span>"""</span></pre></div>
<p dir="auto"><h2 tabindex="-1" dir="auto">Need to predict multiple tokens? Just using KV-Cache! (It really took me a lot of effort to sort this out. Orz)</h2><a id="user-content-need-to-predict-multiple-tokens-just-using-kv-cache-it-really-took-me-a-lot-of-effort-to-sort-this-out-orz" aria-label="Permalink: Need to predict multiple tokens? Just using KV-Cache! (It really took me a lot of effort to sort this out. Orz)" href="#need-to-predict-multiple-tokens-just-using-kv-cache-it-really-took-me-a-lot-of-effort-to-sort-this-out-orz"></a></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">How to Continuously Predict Multiple Tokens</h3><a id="user-content-how-to-continuously-predict-multiple-tokens" aria-label="Permalink: How to Continuously Predict Multiple Tokens" href="#how-to-continuously-predict-multiple-tokens"></a></p>
<div dir="auto"><p>Now, we've completed the prediction of the next word for the input text. But what if our expected output requires multiple tokens?
<br>
For example, in practical llm applications, models usually don't output just one word. Instead, they often output a passage of text, or even a very long text. How is this ability achieved?
<br>
Actually, it's quite simple. We just need to repeatedly call the llm's prediction process to gradually generate a complete sentence or paragraph.
<br>
This process is like "snowballing". Each time we predict a word, we add this word to the current input sequence, and then call the model again for a new round of prediction. The prediction stops when we encounter a stop symbol (a special token "&lt;|end_of_text|&gt;" in llama3) or reach the maximum length limit (a hyperparameter max_seq_len).
</p><p>
Does this sound inefficient? Yes!
<br>
That's why there are well-known caching mechanisms like KV-Cache. By caching the KV vectors of historical tokens, we can reduce the input and computational load, thus improving the inference efficiency.
<br>
Thanks to the caching mechanism, when we use a large model for inference, you may notice that waiting for the first token to be output is often the most time-consuming. But once the first token is output, the output speed of subsequent tokens will increase significantly.
</p></div>
<p dir="auto"><h3 tabindex="-1" dir="auto">Advantages and Disadvantages of KV-Cache</h3><a id="user-content-advantages-and-disadvantages-of-kv-cache" aria-label="Permalink: Advantages and Disadvantages of KV-Cache" href="#advantages-and-disadvantages-of-kv-cache"></a></p>
<div dir="auto"><p><strong>Advantage</strong>: When continuously predicting, we only need to input the new token each time instead of the entire text sequence. This greatly improves the calculation speed during inference.
<br>
<strong>Disadvantage</strong>: Due to the caching mechanism, it will consume more memory resources during inference.
</p></div>
<p dir="auto"><h3 tabindex="-1" dir="auto">Principle Derivation of KV-Cache</h3><a id="user-content-principle-derivation-of-kv-cache" aria-label="Permalink: Principle Derivation of KV-Cache" href="#principle-derivation-of-kv-cache"></a></p>
<p dir="auto">KV-Cache comes from the observation and analysis of the above matrix calculation process. By analyzing the calculation process of each input token, we can find that in most calculation steps, the calculation of each token is actually relatively independent and rately involves interaction with other tokens. Only when calculating the attention mechanism will token-to-token interactions be involved, thus requiring the caching of historical KV vectors.
<br></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Here is the specific derivation logic of KV-Cache:</h3><a id="user-content-here-is-the-specific-derivation-logic-of-kv-cache" aria-label="Permalink: Here is the specific derivation logic of KV-Cache:" href="#here-is-the-specific-derivation-logic-of-kv-cache"></a></p>
<ol dir="auto">
<li><strong>Premise</strong>: To predict the next token, we only need to get the output result of the last token (just as we did in the prediction chapter).</li>
<li><strong>Non-attention parts only needs to calculate the new tokens</strong>: Except for the attention calculation, the calculations of all other parts are independent among tokens. So we only need to calculate the new tokens and don't need to input historical tokens (I'll expand the analysis below).</li>
<li><strong>Attention parts also only needs to calculate the new tokens</strong>: In the attention layer, due to the masking mechanism, the output results of historical tokens won't be affected by future new tokens. So their inputs and outputs at each layer are fixed, that is, the QKV vectors of historical tokens will not change because of the addition of new tokens. Thus, we only need to calculate the attention of the new tokens.</li>
<li><strong>Calculate the new token's attention mechanism</strong>: The attention layer is used to let the token obtain the context information of historical tokens. So, for each new token, we need to calculate the weighted sum using the value vectors of all tokens. Therefore, we need to store the values of historical tokens.</li>
<li><strong>Calculate the new token's attention weights</strong>: As known from point 4, we also need to obtain the importance information, i.e., weights, between the new tokens and historical tokens first. So we need to calculate the product of the key vectors of the new tokens with the key vectors of all tokens. Therefore, we need to store the keys of historical tokens.</li>
<li><strong>Acquisition of KV-Cache</strong>: As known from points 4 and 5, we need to store the KV vectors of historical tokens. Since the query vectors are not used, we don't need to store them. This is how the kv-cache came about.</li>
<li><strong>Efficiency of KV-Cache</strong>: As known from point 3, the historical KV vectors won't change. So they can be incrementally updated during the continuous prediction process without modifying the historical content. In this way, each time we predict, we only need to input and calculate the result of the newly added tokens instead of taking the complete sequence as input, thus greatly improving the inference efficiency.</li>
</ol>
<p dir="auto"><h3 tabindex="-1" dir="auto">Additional: Analysis of the Independence of Token Calculation in KV-Cache</h3><a id="user-content-additional-analysis-of-the-independence-of-token-calculation-in-kv-cache" aria-label="Permalink: Additional: Analysis of the Independence of Token Calculation in KV-Cache" href="#additional-analysis-of-the-independence-of-token-calculation-in-kv-cache"></a></p>
<p dir="auto"><strong>All components except the attention layer (no interaction among them)</strong>:</p>
<ol dir="auto">
<li><strong>Two times normalizations</strong>: Each token vector is normalized in its own feature dimension without using other tokens.</li>
<li><strong>Two times residual connections (add)</strong>: Each token vector adds its own output result to itself without using other tokens.</li>
<li><strong>Feed-forward network (FFN)</strong>: Each token vector is multiplied by the same weight matrices W1, W2, W3 to get the result, and other tokens are not used during this process. Imagine that if the number of input tokens is 17, the calculation of FFN can be simplified as: [17x4096] x [4096x14336] x [14336x4096] = [17x4096]. This is actually equivalent to inputting one token at a time and then concatenating the 17 results into a matrix, that is: 17 times ([1x4096] x [4096x14336] x [14336x4096] = [1x4096]) = 17x[1x4096] =&gt; [17x4096]. Therefore, when each token is calculated in the feed-forward layer, there is actually no interaction with other tokens.</li>
</ol>
<p dir="auto"><strong>Attention layer (only have one-way interaction between new tokens and historical tokens)</strong>:</p>
<ol dir="auto">
<li><strong>Calculate QKV vectors</strong>: Each token vector is multiplied by the same QKV weight matrices to get the result without using other tokens.</li>
<li><strong>Add positional information to QK vectors</strong>: Each token vector performs an independent rotation operation based on its own position without using the specific content of other tokens.</li>
<li><strong>Calculate attention weights</strong>: The attention weights represent the correlation between each token and every historical tokens preceding it, and are independent of future tokens. Therefore, the results of historical tokens are independent of new tokens. And new tokens need the key vector cache of historical tokens.</li>
<li><strong>Calculate the result of the attention mechanism</strong>: The attention mechanism calculates the weighted sum of value vectors based on attention weights. So, similar to the conclusion in the previous point, the results of historical tokens are also independent of new tokens. And new tokens need the value vector cache of historical tokens.
</li>
</ol>
<p dir="auto"><h3 tabindex="-1" dir="auto">Attention Calculation Process Based on KV-Cache</h3><a id="user-content-attention-calculation-process-based-on-kv-cache" aria-label="Permalink: Attention Calculation Process Based on KV-Cache" href="#attention-calculation-process-based-on-kv-cache"></a></p>
<p dir="auto">To clearly show the calculation process, we only derive the single-head scenario (the principle and process of extending it to the multi-head scenario are exactly the same as the previous multi-head attention implementation):</p>
<ol dir="auto">
<li>Assume that the historical input tokens are <math-renderer data-static-url="https://github.githubassets.com/static" data-run-id="19d90f3d5e551008e8fb4bc953d823b8">$S_1$</math-renderer> with a length of N. Based on KV-Cache, we will store the KV result matrix of each head. The shape of a single head is [Nxhead_dim] = [Nx128].</li>
<li>Assume that the newly added input tokens are <math-renderer data-static-url="https://github.githubassets.com/static" data-run-id="19d90f3d5e551008e8fb4bc953d823b8">$S_2$</math-renderer> with a length of M (it can be newly predicted tokens or the input of a new round of user dialogue or any other scenarios).</li>
<li>Calculate the QKV vectors of the new tokens: <math-renderer data-static-url="https://github.githubassets.com/static" data-run-id="19d90f3d5e551008e8fb4bc953d823b8">$Q,K,V = S_2W_{Q,K,V}$</math-renderer> =&gt; [Mx4096] x [4096x128] = [Mx128].</li>
<li>Add positional information to the QK vectors: The positions of new tokens should start from N + 1, not from 0. [Mx128] -&gt; [Mx128].</li>
<li>Add the new KV values to the KV cache to get the updated KV matrix, that is, [Nx128] -&gt; [(N + M)x128].</li>
<li>Calculate the attention weights of the new tokens: Attention_weight = softmax(QK/sqrt(d) + mask) =&gt; [Mx128] x [128x(N + M)] = [Mx(N + M)].</li>
<li>Calculate the final result of the attention mechanism for the new tokens: Attention_weight x V =&gt; [Mx(N + M)] x [(N + M)x128] = [Mx128].</li>
<li>Concatenate the results of each head and perform a linear mapping to get the final output of the attention layer, with a shape of 32x[Mx128] -&gt; [Mx4096].
</li>
</ol>
<p dir="auto">Since our previous learning process has been quite comprehensive, we won't implement the code for the optimization scheme here (if you're interested, you can refer to the official code of Llama 3, which is relatively easy to implement). Just like the parallel calculation of multi-head attention mentioned before, knowing that the calculation process can be optimized is enough~</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Thank you all. Thanks for your continuous learning. Love you all :)</h2><a id="user-content-thank-you-all-thanks-for-your-continuous-learning-love-you-all-" aria-label="Permalink: Thank you all. Thanks for your continuous learning. Love you all :)" href="#thank-you-all-thanks-for-your-continuous-learning-love-you-all-"></a></p>
<p dir="auto">Our learning has come to an end. I hope you have also enjoyed this reading process!</p>

<p dir="auto"><h2 tabindex="-1" dir="auto">From Me</h2><a id="user-content-from-me" aria-label="Permalink: From Me" href="#from-me"></a></p>
<p dir="auto">If you've come across this work, thank you for your trust and for learning all the way to this point. I'm glad to be of help to you~
<br></p>
<p dir="auto">If you'd like to support my work</p>
<ol dir="auto">
<li>give it a star‚≠ê~ :)</li>
<li>buy me a coffee~ <a href="https://ko-fi.com/therealoliver" rel="nofollow">https://ko-fi.com/therealoliver</a></li>
</ol>

<p dir="auto"><h2 tabindex="-1" dir="auto">From the author of predecessor project</h2><a id="user-content-from-the-author-of-predecessor-project" aria-label="Permalink: From the author of predecessor project" href="#from-the-author-of-predecessor-project"></a></p>
<p dir="auto">If you want to support my work</p>
<ol dir="auto">
<li>follow me on twitter <a href="https://twitter.com/naklecha" rel="nofollow">https://twitter.com/naklecha</a></li>
<li>or, buy me a coffee <a href="https://www.buymeacoffee.com/naklecha" rel="nofollow">https://www.buymeacoffee.com/naklecha</a></li>
</ol>
<p dir="auto">Honestly, if you made it this far you already made my day :)</p>
<p dir="auto">what motivates me?</p>
<p dir="auto">My friends and I are on a mission - to make research more accessible!
We created a research lab called A10 - <a href="http://aaaaaaaaaa.org/" rel="nofollow">AAAAAAAAAA.org</a></p>
<p dir="auto">A10 twitter - <a href="https://twitter.com/aaaaaaaaaaorg" rel="nofollow">https://twitter.com/aaaaaaaaaaorg</a></p>
<p dir="auto">our thesis:</p>
<p><a target="_blank" rel="noopener noreferrer" href="https://github.com/therealoliver/Deepdive-llama3-from-scratch/blob/main/images/a10.png"><img src="https://github.com/therealoliver/Deepdive-llama3-from-scratch/raw/main/images/a10.png" width="600px"></a>
</p>
<p>
Thanks again to the original author for the base code and illustrations, which also taught me a lot
</p><p dir="auto"><h2 tabindex="-1" dir="auto">LICENSE</h2><a id="user-content-license" aria-label="Permalink: LICENSE" href="#license"></a></p>
<p dir="auto">Copyright (c) 2025 Jinlong Zhang (<a href="https://github.com/therealoliver">https://github.com/therealoliver</a>)</p>
<p dir="auto">Copyright (c) 2024 Nishant Aklecha</p>
<p dir="auto">MIT</p>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Removing Jeff Bezos from my bed (822 pts)]]></title>
            <link>https://trufflesecurity.com/blog/removing-jeff-bezos-from-my-bed</link>
            <guid>43129439</guid>
            <pubDate>Fri, 21 Feb 2025 16:27:54 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://trufflesecurity.com/blog/removing-jeff-bezos-from-my-bed">https://trufflesecurity.com/blog/removing-jeff-bezos-from-my-bed</a>, See on <a href="https://news.ycombinator.com/item?id=43129439">Hacker News</a></p>
Couldn't get https://trufflesecurity.com/blog/removing-jeff-bezos-from-my-bed: Error: timeout of 10000ms exceeded]]></description>
        </item>
        <item>
            <title><![CDATA[Tesla recalls 380k vehicles in US over power steering assist issue (182 pts)]]></title>
            <link>https://www.reuters.com/business/autos-transportation/tesla-recalls-380000-vehicles-us-over-power-steering-issue-2025-02-21/</link>
            <guid>43128987</guid>
            <pubDate>Fri, 21 Feb 2025 15:57:53 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.reuters.com/business/autos-transportation/tesla-recalls-380000-vehicles-us-over-power-steering-issue-2025-02-21/">https://www.reuters.com/business/autos-transportation/tesla-recalls-380000-vehicles-us-over-power-steering-issue-2025-02-21/</a>, See on <a href="https://news.ycombinator.com/item?id=43128987">Hacker News</a></p>
Couldn't get https://www.reuters.com/business/autos-transportation/tesla-recalls-380000-vehicles-us-over-power-steering-issue-2025-02-21/: Error: Request failed with status code 401]]></description>
        </item>
        <item>
            <title><![CDATA[Apple pulls encrypted iCloud security feature in UK amid backdoor demands (208 pts)]]></title>
            <link>https://www.macrumors.com/2025/02/21/apple-pulls-encrypted-icloud-security-feature-uk/</link>
            <guid>43128870</guid>
            <pubDate>Fri, 21 Feb 2025 15:49:18 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.macrumors.com/2025/02/21/apple-pulls-encrypted-icloud-security-feature-uk/">https://www.macrumors.com/2025/02/21/apple-pulls-encrypted-icloud-security-feature-uk/</a>, See on <a href="https://news.ycombinator.com/item?id=43128870">Hacker News</a></p>
<div id="readability-page-1" class="page"><div role="main" id="maincontent"><article expanded="true"><div data-io-article-url="/2025/02/21/apple-pulls-encrypted-icloud-security-feature-uk/"><p>Apple has withdrawn its Advanced Data Protection iCloud feature from the United Kingdom following government demands for backdoor access to encrypted user data, according to <a href="https://www.bloomberg.com/news/articles/2025-02-21/apple-removes-end-to-end-encryption-feature-from-uk-after-backdoor-order"><em>Bloomberg</em></a>. The move comes after UK officials secretly ordered Apple to provide unrestricted access to encrypted iCloud content worldwide.</p>
<p><img src="https://images.macrumors.com/t/DJt4yhlUGQL70lVg5j2lXcCiu10=/400x0/article-new/2025/02/iCloud-Versus-UK-Key-Feature.jpg?lossy" srcset="https://images.macrumors.com/t/DJt4yhlUGQL70lVg5j2lXcCiu10=/400x0/article-new/2025/02/iCloud-Versus-UK-Key-Feature.jpg?lossy 400w,https://images.macrumors.com/t/wvBp_HGYzUZeneatDI1358r-HVg=/800x0/article-new/2025/02/iCloud-Versus-UK-Key-Feature.jpg?lossy 800w,https://images.macrumors.com/t/a9cMrXLU2fJN_7zb0__5cvQFIeo=/1600x0/article-new/2025/02/iCloud-Versus-UK-Key-Feature.jpg 1600w,https://images.macrumors.com/t/0TD9k13MPLmH0gwTFeVngM_zIs4=/2500x0/filters:no_upscale()/article-new/2025/02/iCloud-Versus-UK-Key-Feature.jpg 2500w" sizes="(max-width: 900px) 100vw, 697px" alt="iCloud Versus UK Key Feature" width="2500" height="1406"><br>Customers who are already using Advanced Data Protection, or ADP, will need to manually disable it during an unspecified grace period to keep their iCloud accounts, according to the report. Apple said it will issue additional guidance in the future to affected users and that it "does not have the ability to automatically disable it on their behalf."</p>
<p>The <a href="https://www.macrumors.com/2025/02/07/uk-government-orders-access-icloud/">UK government's demand</a> came through a "technical capability notice" under the Investigatory Powers Act (IPA), requiring Apple to create a backdoor that would allow British security officials to access encrypted user data globally. The order would have compromised Apple's Advanced Data Protection feature, which provides end-to-end encryption for iCloud data including Photos, Notes, Messages backups, and device backups.<br>
</p>
<blockquote><p>"We are gravely disappointed that the protections provided by ADP will not be available to our customers in the UK given the continuing rise of data breaches and other threats to customer privacy," Apple said in a statement. "ADP protects iCloud data with end-to-end encryption, which means the data can only be decrypted by the user who owns it, and only on their trusted devices."</p></blockquote>
<p>Apple's decision to pull the feature rather than comply with the UK's demands is consistent with the company's previous statements that it would consider withdrawing encrypted services from the UK rather than compromise security. Apple has long opposed creating backdoors in its products, maintaining that such access points would inevitably be discovered by malicious actors.</p>
<p><img src="https://images.macrumors.com/t/JWvmpOLixjCnUvbh58cIk1qqKzQ=/400x0/article-new/2025/02/advanced-data-protection.jpg?lossy" srcset="https://images.macrumors.com/t/JWvmpOLixjCnUvbh58cIk1qqKzQ=/400x0/article-new/2025/02/advanced-data-protection.jpg?lossy 400w,https://images.macrumors.com/t/PZY36wCtYE298Dn3jVuf7VynYV8=/800x0/article-new/2025/02/advanced-data-protection.jpg?lossy 800w,https://images.macrumors.com/t/xBplnSN_h1PNJPMPpyYO0osUC00=/1600x0/article-new/2025/02/advanced-data-protection.jpg 1600w,https://images.macrumors.com/t/iDglo1AZeUl_ER84F4U8L7vK5yg=/2500x0/filters:no_upscale()/article-new/2025/02/advanced-data-protection.jpg 2500w" sizes="(max-width: 900px) 100vw, 697px" alt="advanced data protection" width="1072" height="388" data-old-src="https://images.macrumors.com/images-new/1x1.trans.gif" data-src="https://images.macrumors.com/t/JWvmpOLixjCnUvbh58cIk1qqKzQ=/400x0/article-new/2025/02/advanced-data-protection.jpg?lossy" data-srcset="https://images.macrumors.com/t/JWvmpOLixjCnUvbh58cIk1qqKzQ=/400x0/article-new/2025/02/advanced-data-protection.jpg?lossy 400w,https://images.macrumors.com/t/PZY36wCtYE298Dn3jVuf7VynYV8=/800x0/article-new/2025/02/advanced-data-protection.jpg?lossy 800w,https://images.macrumors.com/t/xBplnSN_h1PNJPMPpyYO0osUC00=/1600x0/article-new/2025/02/advanced-data-protection.jpg 1600w,https://images.macrumors.com/t/iDglo1AZeUl_ER84F4U8L7vK5yg=/2500x0/filters:no_upscale()/article-new/2025/02/advanced-data-protection.jpg 2500w"></p><p><em>Notice UK iCloud users now see after the feature was pulled</em></p><p>The UK order was particularly controversial as it would have required Apple to provide access to data from users outside the UK without their governments' knowledge. Additionally, the IPA makes it illegal for companies to disclose the existence of such government demands.</p>
<p>US security agencies, including the FBI and NSA, have been advocating for increased use of encryption to protect against Chinese cyber threats, creating potential conflicts between UK and US security interests.</p>
<p>"Enhancing the security of cloud storage with end-to-end encryption is more urgent than ever before,‚Äù said Apple on Friday, per <em>Bloomberg</em>. The company added that it "remains committed to offering our users the highest level of security for their personal data and are hopeful that we will be able to do so in the future in the United Kingdom."</p>
<p>Note that the loss of Advanced Data Protection in the UK does not affect the existing end-to-end encryption of several other Apple features available in the country, including iMessage, FaceTime, password management and health data.</p>
<p><small>Note: Due to the political or social nature of the discussion regarding this topic, the discussion thread is located in our <a href="https://forums.macrumors.com/forums/political-news.218/">Political News</a> forum.  All forum members and site visitors are welcome to read and follow the thread, but posting is limited to forum members with at least 100 posts.</small></p>
</div></article><p><h2>Popular Stories</h2></p><div><h3><a href="https://www.macrumors.com/2025/02/19/apple-announces-iphone-16e/">Apple Announces iPhone 16e With A18 Chip and Apple Intelligence, Pricing Starts at $599</a></h3><p>Wednesday February 19, 2025 8:02 am PST by <a href="https://www.macrumors.com/author/joe-rossignol/" rel="author">Joe Rossignol</a></p><p>Apple today introduced the iPhone 16e, its newest entry-level smartphone. The device succeeds the third-generation iPhone SE, which has now been discontinued.
The iPhone 16e features a larger 6.1-inch OLED display, up from a 4.7-inch LCD on the iPhone SE. The display has a notch for Face ID, and this means that Apple no longer sells any iPhones with a Touch ID fingerprint button, marking the ...</p></div><div><h3><a href="https://www.macrumors.com/2025/02/18/iphone-17-pro-models-aluminum-frame-rumor/">iPhone 17 Pro Models Rumored to Feature Aluminum Frame Instead of Titanium Frame</a></h3><p>Tuesday February 18, 2025 12:02 pm PST by <a href="https://www.macrumors.com/author/joe-rossignol/" rel="author">Joe Rossignol</a></p><p>Over the years, Apple has switched from an aluminum frame to a stainless steel frame to a titanium frame for its highest-end iPhones. And now, it has been rumored that Apple will go back to using aluminum for three out of four iPhone 17 models.
In an investor note with research firm GF Securities, obtained by MacRumors this week, Apple supply chain analyst Jeff Pu said the iPhone 17, iPhone...</p></div><div><h3><a href="https://www.macrumors.com/2025/02/20/new-apple-products-still-expecting-this-spring/">Here Are the New Apple Products We're Still Expecting This Spring</a></h3><p>Thursday February 20, 2025 5:06 am PST by <a href="https://www.macrumors.com/author/tim-hardwick/" rel="author">Tim Hardwick</a></p><p>Now that Apple has announced its new more affordable iPhone 16e, our thoughts turn to what else we are expecting from the company this spring. 
There are three product categories that we are definitely expecting to get upgraded before spring has ended. Keep reading to learn what they are. If we're lucky, Apple might make a surprise announcement about a completely new product category.
M4...</p></div><div><h3><a href="https://www.macrumors.com/2025/02/17/iphone-design-to-change-significantly/">iPhone Design to Change 'Significantly' This Year</a></h3><p>Apple is set to "significantly change" the iPhone's design language later this year, according to a Weibo leaker.
In a new post, the user known "Digital Chat Station" said that the iPhone's design is "starting to change significantly" this year. The "iPhone 17 Air" reportedly features a "horizontal, bar-shaped" design on the rear, likely referring to an elongated camera bump. On the other...</p></div><div><h3><a href="https://www.macrumors.com/2025/02/13/apple-launch-february-19/">Tim Cook Teases an 'Apple Launch' Next Wednesday</a></h3><p>Thursday February 13, 2025 8:07 am PST by <a href="https://www.macrumors.com/author/joe-rossignol/" rel="author">Joe Rossignol</a></p><p>In a social media post today, Apple CEO Tim Cook teased an upcoming "launch" of some kind scheduled for Wednesday, February 19.
"Get ready to meet the newest member of the family," he said, with an #AppleLaunch hashtag.
The post includes a short video with an animated Apple logo inside a circle.
Cook did not provide an exact time for the launch, or share any other specific details, so...</p></div><div><h3><a href="https://www.macrumors.com/2025/02/19/ios-18-4-release-date/">Here's When Apple Will Release iOS 18.4</a></h3><p>Wednesday February 19, 2025 11:38 am PST by <a href="https://www.macrumors.com/author/juli-clover/" rel="author">Juli Clover</a></p><p>Following the launch of the iPhone 16e, Apple updated its iOS 18, iPadOS 18, and macOS Sequoia pages to give a narrower timeline on when the next updates are set to launch.
All three pages now state that new Apple Intelligence features and languages will launch in early April, an update from the more broader April timeframe that Apple provided before. The next major point updates will be iOS ...</p></div><div><h3><a href="https://www.macrumors.com/2025/02/14/ios-18-4-beta-next-week/">iOS 18.4 Coming Next Week With These New Features for Your iPhone</a></h3><p>Friday February 14, 2025 6:18 am PST by <a href="https://www.macrumors.com/author/joe-rossignol/" rel="author">Joe Rossignol</a></p><p>The first iOS 18.4 beta for iPhones should be just around the corner, and the update is expected to include many new features and changes.
Bloomberg's Mark Gurman expects the iOS 18.4 beta to be released by next week.
Below, we outline what to expect from iOS 18.4 so far.
Apple Intelligence for Siri
Siri is expected to get several enhancements powered by Apple Intelligence on iOS...</p></div><div><h3><a href="https://www.macrumors.com/2025/02/14/two-older-apple-products-getting-updated/">Two of Apple's Oldest Products Are Finally Getting Updated This Year</a></h3><p>Friday February 14, 2025 6:03 am PST by <a href="https://www.macrumors.com/author/joe-rossignol/" rel="author">Joe Rossignol</a></p><p>Apple released the HomePod mini in November 2020, followed by the AirTag in May 2021, and both still remain first-generation products.
Fortunately, rumors suggest that both the HomePod mini and the AirTag will finally be updated at some point this year.
Below, we recap rumors about the HomePod mini 2 and AirTag 2.
HomePod mini 2
In January 2025, Bloomberg's Mark Gurman said Apple is ...</p></div></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Apple pulls data protection tool after UK government security row (1128 pts)]]></title>
            <link>https://www.bbc.com/news/articles/cgj54eq4vejo</link>
            <guid>43128253</guid>
            <pubDate>Fri, 21 Feb 2025 15:05:24 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.bbc.com/news/articles/cgj54eq4vejo">https://www.bbc.com/news/articles/cgj54eq4vejo</a>, See on <a href="https://news.ycombinator.com/item?id=43128253">Hacker News</a></p>
<div id="readability-page-1" class="page"><article><div data-testid="byline-new" data-component="byline-block"><p><span>Zoe Kleinman</span></p><p><span>Technology editor<!-- --><span>‚Ä¢</span><a href="https://twitter.com/zsk" target="_blank">@zsk</a></span></p></div><figure><div data-component="image-block"><p><img src="https://www.bbc.com/bbcx/grey-placeholder.png"><img sizes="(min-width: 1280px) 50vw, (min-width: 1008px) 66vw, 96vw" srcset="https://ichef.bbci.co.uk/news/240/cpsprodpb/5df2/live/6e826890-f043-11ef-afe3-3909ee34e697.jpg.webp 240w,https://ichef.bbci.co.uk/news/320/cpsprodpb/5df2/live/6e826890-f043-11ef-afe3-3909ee34e697.jpg.webp 320w,https://ichef.bbci.co.uk/news/480/cpsprodpb/5df2/live/6e826890-f043-11ef-afe3-3909ee34e697.jpg.webp 480w,https://ichef.bbci.co.uk/news/640/cpsprodpb/5df2/live/6e826890-f043-11ef-afe3-3909ee34e697.jpg.webp 640w,https://ichef.bbci.co.uk/news/800/cpsprodpb/5df2/live/6e826890-f043-11ef-afe3-3909ee34e697.jpg.webp 800w,https://ichef.bbci.co.uk/news/1024/cpsprodpb/5df2/live/6e826890-f043-11ef-afe3-3909ee34e697.jpg.webp 1024w,https://ichef.bbci.co.uk/news/1536/cpsprodpb/5df2/live/6e826890-f043-11ef-afe3-3909ee34e697.jpg.webp 1536w" src="https://ichef.bbci.co.uk/news/480/cpsprodpb/5df2/live/6e826890-f043-11ef-afe3-3909ee34e697.jpg.webp" alt="Getty Images The Apple logo in front of a high rise building"><span>Getty Images</span></p></div></figure><div data-component="text-block"><p>Apple is taking the unprecedented step of removing its highest level data security tool from customers in the UK, after the government demanded access to user data.<!-- --></p><p><a target="_blank" href="https://support.apple.com/en-gb/108756#:~:text=Advanced%20Data%20Protection%20is%20designed,secured%20using%20standard%20data%20protection.">Advanced Data Protection<!-- --></a> (ADP) means only account holders can view items such as photos or documents they have stored online through a process known as end-to-end encryption.<!-- --></p><p>But earlier this month <!-- --><a target="_self" href="https://www.bbc.co.uk/news/articles/c20g288yldko">the UK government asked<!-- --></a> for the right to see the data, which currently not even Apple can access.<!-- --></p><p>Apple did not comment at the time but has consistently opposed creating a "backdoor" in its encryption service, arguing that if it did so, it would only be a matter of time before bad actors also found a way in.<!-- --></p><p>Now the tech giant has decided it will no longer be possible to activate ADP in the UK.<!-- --></p><p>It means eventually not all UK customer data stored on iCloud - Apple's cloud storage service - will be <!-- --><a target="_blank" href="https://support.apple.com/en-us/102651">fully encrypted<!-- --></a>.<!-- --></p><p>Data with standard encryption is accessible by Apple and shareable with law enforcement, if they have a warrant.<!-- --></p><p>In a statement the Home Office said: "We do not comment on operational matters, including for example confirming or denying the existence of any such notices."<!-- --></p><p>In a statement Apple said it was "gravely disappointed" that the security feature would no longer be available to British customers.<!-- --></p><p>"As we have said many times before, we have never built a backdoor or master key to any of our products, and we never will," it continued.<!-- --></p><ul><li><a target="_self" href="https://www.bbc.co.uk/news/technology-64863448">How does encryption work?<!-- --></a></li></ul><p>The ADP service is opt-in, meaning people have to sign up to get the protection it provides.<!-- --></p><p>From 1500GMT on Friday, any Apple user in the UK attempting to turn it on has been met with an error message.<!-- --></p></div><figure><div data-component="image-block"><p><img src="https://www.bbc.com/bbcx/grey-placeholder.png"><img sizes="(min-width: 1280px) 50vw, (min-width: 1008px) 66vw, 96vw" srcset="https://ichef.bbci.co.uk/news/240/cpsprodpb/a729/live/a86b5cb0-f06b-11ef-8c03-7dfdbeeb2526.jpg.webp 240w,https://ichef.bbci.co.uk/news/320/cpsprodpb/a729/live/a86b5cb0-f06b-11ef-8c03-7dfdbeeb2526.jpg.webp 320w,https://ichef.bbci.co.uk/news/480/cpsprodpb/a729/live/a86b5cb0-f06b-11ef-8c03-7dfdbeeb2526.jpg.webp 480w,https://ichef.bbci.co.uk/news/640/cpsprodpb/a729/live/a86b5cb0-f06b-11ef-8c03-7dfdbeeb2526.jpg.webp 640w,https://ichef.bbci.co.uk/news/800/cpsprodpb/a729/live/a86b5cb0-f06b-11ef-8c03-7dfdbeeb2526.jpg.webp 800w,https://ichef.bbci.co.uk/news/1024/cpsprodpb/a729/live/a86b5cb0-f06b-11ef-8c03-7dfdbeeb2526.jpg.webp 1024w,https://ichef.bbci.co.uk/news/1536/cpsprodpb/a729/live/a86b5cb0-f06b-11ef-8c03-7dfdbeeb2526.jpg.webp 1536w" src="https://ichef.bbci.co.uk/news/480/cpsprodpb/a729/live/a86b5cb0-f06b-11ef-8c03-7dfdbeeb2526.jpg.webp" alt="Apple The Apple error message"><span>Apple</span></p></div></figure><div data-component="text-block"><p>Existing users' access will be disabled at a later date. <!-- --></p><p>It is not known how many people have signed up for ADP since it became available to British Apple customers in December 2022.<!-- --></p><p>Prof Alan Woodward - a cyber-security expert at Surrey University - said it was a "very disappointing development" which amounted to "an act of self harm" by the government.<!-- --></p><p>"All the UK government has achieved is to weaken online security and privacy for UK based users," he told the BBC.<!-- --></p><p>"It was na√Øve of the UK government to think they could tell a US technology company what to do globally," he added.<!-- --></p></div><p data-component="subheadline-block"><h2>What did the UK ask for?<!-- --></h2></p><div data-component="text-block"><p>The request was served by the Home Office under the Investigatory Powers Act (IPA), which compels firms to provide information to law enforcement agencies.<!-- --></p><p>Apple would not comment on the notice and the Home Office refused to either confirm or deny its existence, but the BBC and the Washington Post spoke to a number of sources familiar with the matter.<!-- --></p><p>It provoked a fierce backlash from privacy campaigners, who called it an "unprecedented attack" on the private data of individuals.<!-- --></p><p>Two <!-- --><a target="_self" href="https://www.bbc.co.uk/news/articles/c5yvn90pl5no">senior US politicians said<!-- --></a> it was so serious a threat to American national security that the US government should re-evaluate its intelligence-sharing agreements with the UK unless it was withdrawn.<!-- --></p><p>It is not clear that Apple's actions will fully address those concerns, as the IPA order applies worldwide and ADP will continue to operate in other countries.<!-- --></p><p>In its statement, Apple said it regretted the action it had taken.<!-- --></p><p>"Enhancing the security of cloud storage with end-to-end-encryption is more urgent than ever before," it said.<!-- --></p><p>"Apple remains committed to offering our users the highest level of security for their personal data and are hopeful that we will be able to do so in future in the UK."<!-- --></p><p>The row comes amid growing push-back in the US against regulation being imposed on its tech sector from elsewhere.<!-- --></p><p>In a speech at the AI Action Summit in Paris at the beginning of February, US Vice President JD Vance made it clear that the US was increasingly concerned about it. <!-- --></p><p>"The Trump administration is troubled by reports that some foreign governments are considering tightening the screws on US tech companies with international footprints," he said.<!-- --></p></div></article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Johnny.Decimal ‚Äì A system to organise your life (368 pts)]]></title>
            <link>https://johnnydecimal.com</link>
            <guid>43128093</guid>
            <pubDate>Fri, 21 Feb 2025 14:52:14 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://johnnydecimal.com">https://johnnydecimal.com</a>, See on <a href="https://news.ycombinator.com/item?id=43128093">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="_11-01">  
<p><strong>Johnny.Decimal is designed to help you find things quickly, with more confidence, and less stress.</strong></p>
<p>You assign a unique ID to everything in your life.</p>
<picture>  <source media="(prefers-color-scheme: light) and (min-width: 600px)" srcset="https://johnnydecimal.com/img/v6/11.01A-Diagram_1552_NYC--dtop-1_resize-light-cx-1000x609.png">  <source media="(prefers-color-scheme: dark) and (min-width: 600px)" srcset="https://johnnydecimal.com/img/v6/11.01A-Diagram_1552_NYC--dtop-1_resize-dark-cx-1000x609.png">  <source media="(prefers-color-scheme: light) and (max-width: 599px)" srcset="https://johnnydecimal.com/img/v6/11.01A-Diagram_1552_NYC--mob-1_resize-light-cx-500x470.png">  <source media="(prefers-color-scheme: dark) and (max-width: 599px)" srcset="https://johnnydecimal.com/img/v6/11.01A-Diagram_1552_NYC--mob-1_resize-dark-cx-500x470.png"> <img alt="A diagram showing the structure of a Johnny.Decimal number. The number is 15.52 and it explains how the '1' is an area, which groups related categories in sets of 10. The '15' is the category, in this case 'travel'. And '52' is just an ID; they start at 01. The title of this, our 52nd travel thing, is 'Trip to NYC'." src="https://johnnydecimal.com/img/v6/11.01A-Diagram_1552_NYC--dtop-1_resize-light-cx-1000x609.png" width="500" height="304">  </picture> 
<p>These IDs help you stay organised. They impose constraints that make it harder to get lost. And you create your own index to link everything in your life together.</p>
<p>The system is free to use and the concepts are the same at home, work, or that club you manage.</p>
<h2 id="the-problem">The problem</h2>
<p>In real life, if you stored your stuff in piles of badly-labelled boxes you'd never find anything again.</p>
<p>If you put <em>those</em> boxes in boxes, in boxes, you'd never know which box to open to find the next box. It would be chaos. But this is how you save your computer files.</p>
<picture>  <source media="(prefers-color-scheme: light) and (min-width: 600px)" srcset="https://johnnydecimal.com/img/v6/11.01B-Finder_chaos_v2--1_vfx-light-cx-628x526.png">  <source media="(prefers-color-scheme: dark) and (min-width: 600px)" srcset="https://johnnydecimal.com/img/v6/11.01B-Finder_chaos_v2--1_vfx-dark-cx-628x526.png">  <source media="(prefers-color-scheme: light) and (max-width: 599px)" srcset="https://johnnydecimal.com/img/v6/11.01B-Finder_chaos_v2--1_vfx-light-cx-628x526.png">  <source media="(prefers-color-scheme: dark) and (max-width: 599px)" srcset="https://johnnydecimal.com/img/v6/11.01B-Finder_chaos_v2--1_vfx-dark-cx-628x526.png"> <img alt="A screenshot of a MacOS Finder window showing a bunch of folders, nested terribly, all named similarly. It's a confusing mess." src="https://johnnydecimal.com/img/v6/11.01B-Finder_chaos_v2--1_vfx-light-cx-628x526.png" width="314" height="263"> <figcaption>FIGURE 11.01B. A CHAOTIC FILE SYSTEM WITH MANY LEVELS OF FOLDERS.</figcaption> </picture> 
<h2 id="the-solution">The solution</h2>
<p>Here's one way to think about how a Johnny.Decimal system works. In this simple analogy, an area is a shelf, a category is a box, and an ID is a manila folder.</p>
<h3 id="step-1-buy-ten-shelves">Step 1: Buy ten shelves</h3>
<p>Imagine a computer is a garage. We can't put everything on the floor, so we buy ten shelves. Then we dedicate each one to an area of our life -- <code>life admin</code>, <code>home business</code>, and <code>tennis club</code>.<sup><a href="#user-content-fn-room-to-grow" id="user-content-fnref-room-to-grow" data-footnote-ref="" aria-describedby="footnote-label">1</a></sup></p>
<picture>  <source media="(prefers-color-scheme: light) and (min-width: 600px)" srcset="https://johnnydecimal.com/img/v6/11.01C-Berkeley_shelves_x3--0_original-light-cx-1312x918.png">  <source media="(prefers-color-scheme: dark) and (min-width: 600px)" srcset="https://johnnydecimal.com/img/v6/11.01C-Berkeley_shelves_x3--0_original-dark-cx-1312x918.png">  <source media="(prefers-color-scheme: light) and (max-width: 599px)" srcset="https://johnnydecimal.com/img/v6/11.01C-Berkeley_shelves_x3--0_original-light-cx-1312x918.png">  <source media="(prefers-color-scheme: dark) and (max-width: 599px)" srcset="https://johnnydecimal.com/img/v6/11.01C-Berkeley_shelves_x3--0_original-dark-cx-1312x918.png"> <img alt="A line drawing of three storage shelves. Think your classic Ikea 'Billy' bookshelf. At the top they're labelled 'life admin', 'home business', and 'tennis club'. They're empty." loading="lazy" src="https://johnnydecimal.com/img/v6/11.01C-Berkeley_shelves_x3--0_original-light-cx-1312x918.png" width="1312" height="918"> <figcaption>FIGURE 11.01C. A SHELF FOR EACH MAJOR AREA OF OUR LIFE.</figcaption> </picture> 
<h3 id="step-2-add-some-boxes">Step 2: Add some boxes</h3>
<p>Each shelf has space for ten boxes, so we categorise what we want to store. In life admin we decide on five and label them: <code>me</code>, <code>house</code>, <code>money</code>, <code>online</code>, and <code>travel</code>. Our boxes have space for a number, so we add that too.<sup><a href="#user-content-fn-startat11" id="user-content-fnref-startat11" data-footnote-ref="" aria-describedby="footnote-label">2</a></sup></p>
<picture>  <source media="(prefers-color-scheme: light) and (min-width: 600px)" srcset="https://johnnydecimal.com/img/v6/11.01D-Berkeley_shelf_w_boxes--0_original-light-cx-448x914.png">  <source media="(prefers-color-scheme: dark) and (min-width: 600px)" srcset="https://johnnydecimal.com/img/v6/11.01D-Berkeley_shelf_w_boxes--0_original-dark-cx-448x914.png">  <source media="(prefers-color-scheme: light) and (max-width: 599px)" srcset="https://johnnydecimal.com/img/v6/11.01D-Berkeley_shelf_w_boxes--0_original-light-cx-448x914.png">  <source media="(prefers-color-scheme: dark) and (max-width: 599px)" srcset="https://johnnydecimal.com/img/v6/11.01D-Berkeley_shelf_w_boxes--0_original-dark-cx-448x914.png"> <img alt="The same shelf and boxes, but now the labels have numbers at the front. The shelf is labelled '10-19 Life admin', and the boxes are labelled '11 Me', '12 House', '13 Money', '14 Online', and '15 Travel'." loading="lazy" src="https://johnnydecimal.com/img/v6/11.01D-Berkeley_shelf_w_boxes--0_original-light-cx-448x914.png" width="224" height="407"> <figcaption>FIGURE 11.01D: OUR LIFE ADMIN SHELF ENDS UP WITH FIVE BOXES.</figcaption> </picture> 
<h3 id="step-3-file-your-stuff-in-folders">Step 3: File your stuff in folders</h3>
<p>We put our documents in manila folders. Each folder gets a number starting at <code>.11</code> so we can track them. In this case, we've put some insurance policies in <code>15.23 Travel insurance</code>. Then put the folder in a box.</p>
<picture>  <source media="(prefers-color-scheme: light) and (min-width: 600px)" srcset="https://johnnydecimal.com/img/v6/11.01E-Berkeley_folder_box--0_original-light-cx-1252x434.png">  <source media="(prefers-color-scheme: dark) and (min-width: 600px)" srcset="https://johnnydecimal.com/img/v6/11.01E-Berkeley_folder_box--0_original-dark-cx-1252x434.png">  <source media="(prefers-color-scheme: light) and (max-width: 599px)" srcset="https://johnnydecimal.com/img/v6/11.01E-Berkeley_folder_box--0_original-light-cx-1252x434.png">  <source media="(prefers-color-scheme: dark) and (max-width: 599px)" srcset="https://johnnydecimal.com/img/v6/11.01E-Berkeley_folder_box--0_original-dark-cx-1252x434.png"> <img alt="Line drawing representing a manila folder. It's labelled '15.23 Travel insurance' and contains 3 documents, labelled 'Claim form', 'Payment receipt', and 'Policy document'." loading="lazy" src="https://johnnydecimal.com/img/v6/11.01E-Berkeley_folder_box--0_original-light-cx-1252x434.png" width="626" height="217"> <figcaption>FIGURE 11.01E. WE PUT OUR DOCUMENTS IN NUMBERED FOLDERS AND STORE THEM IN THE RELEVANT BOX.</figcaption> </picture> 
<h3 id="this-is-how-we-structure-our-file-system">This is how we structure our file system</h3>
<p>Let's return to our computer. The shelves have become our area folders. The boxes are category folders. And the manila folders are the IDs where we save our files.</p>
<picture>  <source media="(prefers-color-scheme: light) and (min-width: 600px)" srcset="https://johnnydecimal.com/img/v6/11.01F-Finder_box_shelf_folder--1_labels-light-cx-648x522.png">  <source media="(prefers-color-scheme: dark) and (min-width: 600px)" srcset="https://johnnydecimal.com/img/v6/11.01F-Finder_box_shelf_folder--1_labels-dark-cx-648x522.png">  <source media="(prefers-color-scheme: light) and (max-width: 599px)" srcset="https://johnnydecimal.com/img/v6/11.01F-Finder_box_shelf_folder--1_labels-light-cx-648x522.png">  <source media="(prefers-color-scheme: dark) and (max-width: 599px)" srcset="https://johnnydecimal.com/img/v6/11.01F-Finder_box_shelf_folder--1_labels-dark-cx-648x522.png"> <img alt="Screenshot of macOS Finder. It shows a parent folder '10-19 Life admin', labelled 'SHELF'. It contains folder '15 Travel', labelled 'BOX'. And it contains '11.53 Travel insurance', which is labelled 'FOLDER'." loading="lazy" src="https://johnnydecimal.com/img/v6/11.01F-Finder_box_shelf_folder--1_labels-light-cx-648x522.png" width="324" height="261"> <figcaption>FIGURE 11.01F. A NEAT FILE STRUCTURE WITH AREAS, CATEGORIES, AND IDS.</figcaption> </picture> 
<h2 id="benefits-of-the-johnnydecimal-id">Benefits of the Johnny.Decimal ID</h2>
<p>Each of our storage folders now has a number, the ID. It always has two digits, a decimal, and two digits. For example, <code>15.23</code> <code>22.11</code> <code>31.17</code>. This number is really useful.</p>
<h3 id="it-provides-structure">It provides structure</h3>
<p>The ID tells us exactly where a thing is. The numbers before the decimal are the item's category, and they define the structure of your system.</p>
<p>At a glance, you know what sort of thing the item contains. You'll be astonished at how many of your category numbers you remember.</p>
<h3 id="theyre-easy-to-communicate">They're easy to communicate</h3>
<p>They're short, memorable, and can be spoken out loud. Say it like "sixteen oh-two" or "thirty-one dot seventeen".</p>
<p>This is really handy when you want to tell someone (including your future self) where a thing is.</p>
<h3 id="things-stay-where-they-are">Things stay where they are</h3>
<p>If you use the alphabet to name folders, they move when a new one is created. So you never get a chance to develop muscle memory.</p>
<p>Numbers solve this problem. In the example above, <code>11 Me</code> comes before <code>12 House</code> because the folders sort by number. If we made a new folder, <code>16 Aardvark collection</code>, nothing would move.</p>
<h3 id="it-imposes-limits">It imposes limits</h3>
<p>The 'no more than ten' concept is at the heart of Johnny.Decimal.</p>
<p>When you start looking for something, you have no more than ten area folders to choose from. Select one and ignore the rest. Now you have no more than ten category folders to choose from. Repeat the process.</p>
<p>You then arrive in a folder with no more than one hundred IDs. If the ID was created recently it will have a higher number. If not, lower. And things created together, stick together. The alphabet isn't around to ruin the party.</p>
<h2 id="i-like-it-what-next">I like it! What next?</h2>
<p>Welcome to the Johnny.Decimal family, there's plenty to go on with:</p>
<ul>
<li>
<p>Explore the site to <a href="https://johnnydecimal.com/11.02/">learn more</a> about the system.</p>
</li>
<li>
<p>If you're a small business owner, head over to <a href="https://smallbusiness.johnnydecimal.com/">the just-announced small business system</a>.</p>
</li>
<li>
<p>Get organised fast with the '<a href="https://johnnydecimal.com/14.11/">life admin</a>' pack. Check out the <a href="https://johnnydecimal.com/14.21/">workbook</a> or <a href="https://johnnydecimal.com/14.22/">workshop</a> for more comprehensive guidance.</p>
</li>
<li>
<p>Follow the <a href="https://johnnydecimal.com/22.02/">blog</a> or sign up to the <a href="https://johnnydecimal.com/21.02/">mailing list</a>.</p>
</li>
<li>
<p>Ask for help in the friendly <a href="https://forum.johnnydecimal.com/">forum</a> or <a href="https://johnnydecimal.com/23.02/">Discord</a>, or <a href="mailto:hello@johnnydecimal.com">email me</a> (I answer every message).</p>
</li>
</ul>
<section data-footnotes="">
<ol>
<li id="user-content-fn-room-to-grow">
<p>We don't try to use all ten shelves -- there's room to grow. <a href="#user-content-fnref-room-to-grow" data-footnote-backref="" aria-label="Back to reference 1">‚Ü©</a></p>
</li>
<li id="user-content-fn-startat11">
<p>I'll explain later why the first box isn't number <code>01</code> or <code>10</code>. <a href="#user-content-fnref-startat11" data-footnote-backref="" aria-label="Back to reference 2">‚Ü©</a></p>
</li>
</ol>
</section>  </div></div>]]></description>
        </item>
    </channel>
</rss>