<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0">
    <channel>
        <title>HN100 - Readable Contents</title>
        <link>https://hn.algolia.com/api/v1/search_by_date?tags=%28story,poll%29&amp;numericFilters=points%3E100</link>
        <description>Uses Readability to add bodies to the RSS feed</description>
        <lastBuildDate>Fri, 11 Oct 2024 13:30:03 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>en</language>
        <item>
            <title><![CDATA[Nobel Peace Prize for 2024 awarded to Nihon Hidankyo (131 pts)]]></title>
            <link>https://www.nobelprize.org/press-release-peace-2024/</link>
            <guid>41807681</guid>
            <pubDate>Fri, 11 Oct 2024 09:01:54 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.nobelprize.org/press-release-peace-2024/">https://www.nobelprize.org/press-release-peace-2024/</a>, See on <a href="https://news.ycombinator.com/item?id=41807681">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="content" role="main">
<section>

<article>
<header>
<h2>
Press release </h2>
</header>

<h5><b>English</b><br><a href="https://www.nobelprize.org/prizes/peace/2024/222035-press-release-norwegian/">Norwegian</a></h5>
<figure><img decoding="async" width="226" height="121" src="https://www.nobelprize.org/uploads/2022/10/nobel-peace-prize-logo.jpg" alt="The Nobel Peace Prize logo"></figure>
<h2>Announcement</h2>
<p><a href="https://www.nobelpeaceprize.org/" target="_blank" rel="noreferrer noopener">The Norwegian Nobel Committee</a> has decided to award the Nobel Peace Prize for 2024 to the Japanese organisation Nihon Hidankyo. This grassroots movement of atomic bomb survivors from Hiroshima and Nagasaki, also known as Hibakusha, is receiving the Peace Prize for its efforts to achieve a world free of nuclear weapons and for demonstrating through witness testimony that nuclear weapons must never be used again.</p>
<p>In response to the atomic bomb attacks of August 1945, a global movement arose whose members have worked tirelessly to raise awareness about the catastrophic humanitarian consequences of using nuclear weapons. Gradually, a powerful international norm developed, stigmatising the use of nuclear weapons as morally unacceptable. This norm has become known as “the nuclear taboo”.</p>
<p>The testimony of the Hibakusha – the survivors of Hiroshima and Nagasaki – is unique in this larger context.</p>
<p>These historical witnesses have helped to generate and consolidate widespread opposition to nuclear weapons around the world by drawing on personal stories, creating educational campaigns based on their own experience, and issuing urgent warnings against the spread and use of nuclear weapons. The Hibakusha help us to describe the indescribable, to think the unthinkable, and to somehow grasp the incomprehensible pain and suffering caused by nuclear weapons.</p>
<p>The Norwegian Nobel Committee wishes nevertheless to acknowledge one encouraging fact: No nuclear weapon has been used in war in nearly 80 years. The extraordinary efforts of Nihon Hidankyo and other representatives of the Hibakusha have contributed greatly to the establishment of the nuclear taboo. It is therefore alarming that today this taboo against the use of nuclear weapons is under pressure.</p>
<p>The nuclear powers are modernising and upgrading their arsenals; new countries appear to be preparing to acquire nuclear weapons; and threats are being made to use nuclear weapons in ongoing warfare. At this moment in human history, it is worth reminding ourselves what nuclear weapons are: the most destructive weapons the world has ever seen.</p>
<p>Next year will mark 80 years since two American atomic bombs killed an estimated 120&nbsp;000 inhabitants of Hiroshima and Nagasaki. A comparable number died of burn and radiation injuries in the months and years that followed. Today’s nuclear weapons have far greater destructive power. They can kill millions and would impact the climate catastrophically. A nuclear war could destroy our civilisation.</p>
<p>The fates of those who survived the infernos of Hiroshima and Nagasaki were long concealed and neglected. In 1956, local Hibakusha associations along with victims of nuclear weapons tests in the Pacific formed the Japan Confederation of A- and H-Bomb Sufferers Organisations. This name was shortened in Japanese to Nihon Hidankyo. It would become the largest and most influential Hibakusha organisation in Japan.</p>
<p>The core of Alfred Nobel’s vision was the belief that committed individuals can make a difference. In awarding this year’s Nobel Peace Prize to Nihon Hidankyo, the Norwegian Nobel Committee wishes to honour all survivors who, despite physical suffering and painful memories, have chosen to use their costly experience to cultivate hope and engagement for peace.</p>
<p>Nihon Hidankyo has provided thousands of witness accounts, issued resolutions and public appeals, and sent annual delegations to the United Nations and a variety of peace conferences to remind the world of the pressing need for nuclear disarmament.</p>
<p>One day, the Hibakusha will no longer be among us as witnesses to history. But with a strong culture of remembrance and continued commitment, new generations in Japan are carrying forward the experience and the message of the witnesses. They are inspiring and educating people around the world. In this way they are helping to maintain the nuclear taboo – a precondition of a peaceful future for humanity.</p>
<p>The decision to award the Nobel Peace Prize for 2024 to Nihon Hidankyo is securely anchored in Alfred Nobel’s will. This year’s prize joins a distinguished list of Peace Prizes that the Committee has previously awarded to champions of nuclear disarmament and arms control.</p>
<p>The Nobel Peace Prize for 2024 fulfils Alfred Nobel’s desire to recognise efforts of the greatest benefit to humankind.</p>
<p>Oslo, 11 October 2024</p>

<div>
<p><a href="#content">
Back to top </a></p><svg width="18px" height="15px" viewBox="0 0 20 17" version="1.1" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" role="image" aria-labelledby="back-to-top-title  back-to-top-desc">
<title id="back-to-top-title">Back To Top</title>
<desc id="back-to-top-desc">Takes users back to the top of the page</desc>
<g stroke="none" stroke-width="1" fill="none" fill-rule="evenodd">
<g transform="translate(-474.000000, -9998.000000)" fill="#2E2A25">
<g transform="translate(474.000000, 9998.000000)">
<g transform="translate(10.000000, 10.000000) rotate(45.000000) translate(-10.000000, -10.000000) translate(3.000000, 3.000000)">
<rect x="0" y="0" width="2" height="14"></rect>
<rect x="0" y="0" width="14" height="2"></rect>
</g>
<rect x="9" y="3" width="2" height="14"></rect>
</g>
</g>
</g>
</svg>
</div>
</article>

</section>
<section>

<article>
<div>
<header>
<p>
<h2>
<a href="https://www.nobelprize.org/">
Coming up </a>
</h2>
</p>
</header>
<div><p>
Don't miss the Nobel Prize announcements 7-14 October!</p><p>Watch the live stream of the announcements. </p></div>
</div>
<figure>
<a href="https://www.nobelprize.org/"><picture><source data-srcset="https://www.nobelprize.org/uploads/2023/09/2024_Announcement_Recommended_Live-992x656.jpg" media="(min-width: 220px)"><source data-srcset="https://www.nobelprize.org/uploads/2023/09/2024_Announcement_Recommended_Live-1520x1008.jpg" media="(min-width: 900px)"><source data-srcset="https://www.nobelprize.org/uploads/2023/09/2024_Announcement_Recommended_Live.jpg" media="(min-width: 1400px)"><img src="https://www.nobelprize.org/uploads/2023/09/2024_Announcement_Recommended_Live-1024x676.jpg" alt="Illustration" data-old-src="data:image/gif;base64,R0lGODlhAQABAAAAACH5BAEKAAEALAAAAAABAAEAAAICTAEAOw=="></picture></a> </figure>
</article>
</section>
<section>

<form id="67090ae8419c5" method="GET" action="">
<p><label for="mobile-dropdown">
Select the category or categories you would like to filter by </label>

</p>
<div>
<p><label>Select the category or categories you would like to filter by</label></p><p><label for="physics">

<span>
Physics </span>
</label>
</p>
<p><label for="chemistry">

<span>
Chemistry </span>
</label>
</p>
<p><label for="medicine">

<span>
Medicine </span>
</label>
</p>
<p><label for="literature">

<span>
Literature </span>
</label>
</p>
<p><label for="peace">

<span>
Peace </span>
</label>
</p>
<p><label for="economic-sciences">

<span>
Economic Sciences </span>
</label>
</p>
</div>
<p><label for="increment-down">
Decrease the year by one </label>

<label for="increment-input">
Choose a year you would like to search in </label>

<label for="increment-up">
Increase the year by one </label>

</p>

</form>
</section>


</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Tesla Robotaxi (222 pts)]]></title>
            <link>https://www.tesla.com/we-robot</link>
            <guid>41805706</guid>
            <pubDate>Fri, 11 Oct 2024 03:24:25 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.tesla.com/we-robot">https://www.tesla.com/we-robot</a>, See on <a href="https://news.ycombinator.com/item?id=41805706">Hacker News</a></p>
Couldn't get https://www.tesla.com/we-robot: Error: timeout of 10000ms exceeded]]></description>
        </item>
        <item>
            <title><![CDATA[$2 H100s: How the GPU Rental Bubble Burst (316 pts)]]></title>
            <link>https://www.latent.space/p/gpu-bubble</link>
            <guid>41805446</guid>
            <pubDate>Fri, 11 Oct 2024 02:19:42 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.latent.space/p/gpu-bubble">https://www.latent.space/p/gpu-bubble</a>, See on <a href="https://news.ycombinator.com/item?id=41805446">Hacker News</a></p>
<div id="readability-page-1" class="page"><div dir="auto"><p><em><strong>Swyx’s note:</strong><span> we’re on a roll catching up with former guests! Apart from our recent guest spot on </span><a href="https://www.listennotes.com/podcasts/high-agency-the/why-your-ai-product-needs-ALy02ewNtDC/" rel="">Raza Habib’s chat with Hamel Husain</a><span> (see </span><a href="https://www.latent.space/p/humanloop" rel="">Raza’s first pod here</a><span>). </span></em></p><p><em><span>We’re delighted to welcome Eugene Cheah (see </span><a href="https://www.latent.space/p/rwkv" rel="">his first pod on RWKV last year</a><span>) as a rare guest </span><strong>writer </strong><span>for our newsletter</span><strong>.</strong><span> Eugene has now cofounded </span><a href="https://featherless.ai/" rel="">Featherless.AI</a><span>, an inference platform with the world’s largest collection of open source models (~2,000) instantly accessible via a single API for a </span><strong>flat rate</strong><span> ($10-$75+ a month).</span></em></p><p><em><span>Recently there has been a lot of excitement with NVIDIA’s new Blackwell series rolling out to OpenAI, with the company saying it is </span><a href="https://x.com/firstadopter/status/1844417947277852925" rel="">sold out for the next year</a><span> and Jensen noting that it could be the “</span><a href="https://x.com/The_AI_Investor/status/1844080690046058843" rel="">most successful product in the history of the industry</a><span>”. With cousin Lisa hot on his heels </span><a href="https://www.cnbc.com/2024/10/10/amd-launches-mi325x-ai-chip-to-rival-nvidias-blackwell-.html" rel="">announcing the MI3 25 X</a><span> and </span><a href="https://news.ycombinator.com/item?id=41702789" rel="">Cerebras filing for IPO</a><span>, it is time to dive deep on the GPU market again (see also </span><a href="https://www.latent.space/p/semianalysis" rel="">former guest</a><span> </span><a href="https://www.dwarkeshpatel.com/p/dylan-jon" rel="">Dylan Patel’s pod</a><span> for his trademark candid take on the industry of course): </span></em></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8f05a21e-05b3-4691-a2bb-44547e0acd7e_1074x1268.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8f05a21e-05b3-4691-a2bb-44547e0acd7e_1074x1268.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8f05a21e-05b3-4691-a2bb-44547e0acd7e_1074x1268.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8f05a21e-05b3-4691-a2bb-44547e0acd7e_1074x1268.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8f05a21e-05b3-4691-a2bb-44547e0acd7e_1074x1268.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8f05a21e-05b3-4691-a2bb-44547e0acd7e_1074x1268.png" width="421" height="497.04655493482306" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/8f05a21e-05b3-4691-a2bb-44547e0acd7e_1074x1268.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:1268,&quot;width&quot;:1074,&quot;resizeWidth&quot;:421,&quot;bytes&quot;:1493562,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:false,&quot;topImage&quot;:true,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8f05a21e-05b3-4691-a2bb-44547e0acd7e_1074x1268.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8f05a21e-05b3-4691-a2bb-44547e0acd7e_1074x1268.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8f05a21e-05b3-4691-a2bb-44547e0acd7e_1074x1268.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8f05a21e-05b3-4691-a2bb-44547e0acd7e_1074x1268.png 1456w" sizes="100vw" fetchpriority="high"></picture></div></a></figure></div><p><em><span>Do we yet have an answer to </span><a href="https://www.latent.space/p/mar-jun-2024" rel="">the $600bn question</a><span>? It is now consensus that the capex on foundation model training is the “</span><a href="https://x.com/GavinSBaker/status/1720819375517716610" rel="">fastest depreciating asset in history</a><span>”, but the jury on GPU infra spend is still out and </span><a href="https://www.latent.space/i/140396949/mixtral-sparks-a-gpuinference-race-to-the-bottom" rel="">the GPU Rich Wars are raging</a><span>.</span></em></p><p><em><span>What follows is Eugene’s take on GPU economics as he is now an inference provider, diving deep on the H100 market, as a possible read for what is to come for the Blackwell generation. Not financial advice! We also recommend </span><a href="https://blog.lepton.ai/the-missing-guide-to-the-h100-gpu-market-91ebfed34516" rel="">Yangqing Jia’s guide</a><span>.</span></em></p><p><em><strong>TLDR: Don’t buy H100s. The market has flipped from shortage ($8/hr) to oversupplied ($2/hr), because of reserved compute resales, open model finetuning, and decline in new foundation model co’s. Rent instead.</strong><p><span>(Unless you have some combination of discounted H100s, discounted electricity, or a Sovereign AI angle where the location of your GPU is critical to your customers, or you have billions and need a super large cluster for frontier model training)</span></p><p><span>For the general market, it makes little sense to be investing in new H100s today, when </span><strong>you can rent it at near cost, when you need it</strong><span>, with the current oversupply.</span></p></em></p><p><span>ChatGPT was launched in November 2022, built on the A100 series. The H100s arrived in March 2023. </span><strong>The pitch to investors and founders was simple: </strong><span>Compared to A100s, </span><strong>the new H100s were 3x more powerful, but only 2x the sticker price</strong><span>.</span></p><p>If you were faster to ramp up on H100s, you too, can build a bigger, better model, and maybe even leapfrog OpenAI to Artificial General Intelligence - If you have the capital to match their wallet! </p><p>With this desire, $10-100’s billions of dollars were invested into GPU-rich AI startups to build this next revolution. Which lead to ….</p><p><strong>The sudden surge in H100 demand</strong></p><p><span>Market prices shot through the roof, the original rental rates of H100 started at approximately </span><em><strong>$4.70 an hour</strong></em><span> but were going for </span><em><strong>over $8</strong></em><span>. For all the desperate founders rushing to train their models to convince their investors for their next $100 million round.</span></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F10d1b8c6-8c88-4579-a65d-3ce677c98d88_2798x1576.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F10d1b8c6-8c88-4579-a65d-3ce677c98d88_2798x1576.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F10d1b8c6-8c88-4579-a65d-3ce677c98d88_2798x1576.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F10d1b8c6-8c88-4579-a65d-3ce677c98d88_2798x1576.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F10d1b8c6-8c88-4579-a65d-3ce677c98d88_2798x1576.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_2400,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F10d1b8c6-8c88-4579-a65d-3ce677c98d88_2798x1576.png" width="404" height="227.52747252747253" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/10d1b8c6-8c88-4579-a65d-3ce677c98d88_2798x1576.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:false,&quot;imageSize&quot;:&quot;large&quot;,&quot;height&quot;:820,&quot;width&quot;:1456,&quot;resizeWidth&quot;:404,&quot;bytes&quot;:265694,&quot;alt&quot;:&quot;&quot;,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false}" alt="" title="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F10d1b8c6-8c88-4579-a65d-3ce677c98d88_2798x1576.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F10d1b8c6-8c88-4579-a65d-3ce677c98d88_2798x1576.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F10d1b8c6-8c88-4579-a65d-3ce677c98d88_2798x1576.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F10d1b8c6-8c88-4579-a65d-3ce677c98d88_2798x1576.png 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption>Nvidia, literally pitched to their investors &amp; datacenter customers, in their 2023 investor presentation - the “market opportunity” on renting H100s at $4/hr</figcaption></figure></div><p><span>For GPU farms, it felt like free money - if you can get these founders to rent your H100 SXMGPUs at $4.70 an hour or more, or even get them to pay it upfront, </span><strong>the payback period was &lt;1.5 years</strong><span>. From then on, it was free-flowing cash of over $100k per GPU, per year.</span></p><p>With no end to the GPU demand in sight, their investors agreed, with even larger investments…</p><p><span>Physical goods, unlike digital goods, suffer from lag time. Especially when there are </span><a href="https://www.ft.com/content/c7e9cfa9-3f68-47d3-92fc-7cf85bcb73b3" rel="">multiple shipment delays</a><span>.</span></p><p>For most of 2023, the H100 prices felt like they would forever be above $4.70 (unless you were willing to do a huge upfront downpayment)</p><p>At the start of 2024, the H100 prices reached approximately $2.85 across multiple providers.</p><p>As more providers come online, however… I started to get emails like this:</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F14ef9576-25d7-4b63-b70b-57c442789fe3_2202x1248.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F14ef9576-25d7-4b63-b70b-57c442789fe3_2202x1248.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F14ef9576-25d7-4b63-b70b-57c442789fe3_2202x1248.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F14ef9576-25d7-4b63-b70b-57c442789fe3_2202x1248.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F14ef9576-25d7-4b63-b70b-57c442789fe3_2202x1248.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F14ef9576-25d7-4b63-b70b-57c442789fe3_2202x1248.png" width="1456" height="825" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/14ef9576-25d7-4b63-b70b-57c442789fe3_2202x1248.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:825,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:413942,&quot;alt&quot;:&quot;&quot;,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false}" alt="" title="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F14ef9576-25d7-4b63-b70b-57c442789fe3_2202x1248.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F14ef9576-25d7-4b63-b70b-57c442789fe3_2202x1248.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F14ef9576-25d7-4b63-b70b-57c442789fe3_2202x1248.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F14ef9576-25d7-4b63-b70b-57c442789fe3_2202x1248.png 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption>While I have not been successful with acquiring H100 nodes (8xH100) at $4/hour, I have confirmed multiple times, that you can do so at $8 - $16/hour</figcaption></figure></div><p>In Aug 2024, if you're willing to auction for a small slice of H100 time (days to weeks), you can start finding H100 GPUs for $1 to $2 an hour.</p><p><strong>We are looking at a &gt;= 40% price drop per year</strong><span>, especially for small clusters. NVIDIA’s marketing projection of $4 per GPU hour across 4 years, has evaporated away in under 1.5 years.</span></p><p><span>And that is horrifying because it means someone out there is potentially </span><a href="https://en.wikipedia.org/wiki/Bagholder" rel="">left holding the bag</a><span> - especially so if they just bought it as a new GPUs. So what is going on?</span></p><blockquote><p><em>This will be focusing on the economical cost, and the ROI on leasing, against various market rates. Not the opportunity cost, or buisness value.</em></p></blockquote><p>The average H100 SXM GPU in a data center costs $50k or more to set up, maintain, and operate (aka most of the CAPEX). Excluding electricity and cooling OPEX cost. More details on the calculation are provided later in this article.</p><p><span>But what does that mean for unit economics today, as an investment?</span><br><span>Especially if we assume a 5-year lifespan on the GPUs itself today.</span></p><p>Generally, there are two business models for leasing H100, which we would cover.</p><ul><li><p>Short on-demand leases (by the hour - by the week - or the month)</p></li><li><p>Longterm reservation (3-5 years)</p></li></ul><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7815afb5-d4f8-4444-804d-5fc96fa094c6_1884x990.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7815afb5-d4f8-4444-804d-5fc96fa094c6_1884x990.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7815afb5-d4f8-4444-804d-5fc96fa094c6_1884x990.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7815afb5-d4f8-4444-804d-5fc96fa094c6_1884x990.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7815afb5-d4f8-4444-804d-5fc96fa094c6_1884x990.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7815afb5-d4f8-4444-804d-5fc96fa094c6_1884x990.png" width="1456" height="765" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/7815afb5-d4f8-4444-804d-5fc96fa094c6_1884x990.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:765,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:618581,&quot;alt&quot;:&quot;&quot;,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false}" alt="" title="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7815afb5-d4f8-4444-804d-5fc96fa094c6_1884x990.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7815afb5-d4f8-4444-804d-5fc96fa094c6_1884x990.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7815afb5-d4f8-4444-804d-5fc96fa094c6_1884x990.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7815afb5-d4f8-4444-804d-5fc96fa094c6_1884x990.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p><strong>In summary, for an on-demand workload</strong></p><ul><li><p><strong>&gt;$2.85</strong><span> : Beat stock market IRR</span></p></li><li><p><strong>&lt;$2.85</strong><span> : Loses to stock market IRR</span></p></li><li><p><strong>&lt;$1.65</strong><span> : Expect loss in investment</span></p></li></ul><p>For the above ROI and revenue forecast projection, we introduced “blended price”, where we assume a gradual drop to 50% in the rental price across 5 years.</p><p>This is arguably a conservative/optimistic estimate given the &gt;= 40% price drop per year we see now. But it’s a means of projecting an ROI while taking into account a certain % of price drop.</p><p>At $4.50/hour, even when blended, we get to see the original pitch for data center providers from NVIDIA, where they practically print money after 2 years. Giving an IRR (Internal rate of return) of 20+%.</p><p>However, at $2.85/hour, this is where it starts to be barely above 10% IRR.</p><p>Meaning, if you are buying a new H100 server today, and if the market price is less than $2.85/hour, you can barely beat the market, assuming 100% allocation (which is an unreasonable assumption). Anything, below that price, and you're better off with the stock market, instead of a H100 infrastructure company, as an investment.</p><p><strong>And if the price falls below $1.65/hour, you are doomed to make losses on the H100 over the 5 years, as an infra provider</strong><span>. Especially, if you just bought the nodes and cluster this year.</span></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb3453721-463c-4de2-9bda-cd2fa3133ada_1884x984.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb3453721-463c-4de2-9bda-cd2fa3133ada_1884x984.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb3453721-463c-4de2-9bda-cd2fa3133ada_1884x984.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb3453721-463c-4de2-9bda-cd2fa3133ada_1884x984.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb3453721-463c-4de2-9bda-cd2fa3133ada_1884x984.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb3453721-463c-4de2-9bda-cd2fa3133ada_1884x984.png" width="1456" height="760" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/b3453721-463c-4de2-9bda-cd2fa3133ada_1884x984.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:760,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:642212,&quot;alt&quot;:&quot;&quot;,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false}" alt="" title="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb3453721-463c-4de2-9bda-cd2fa3133ada_1884x984.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb3453721-463c-4de2-9bda-cd2fa3133ada_1884x984.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb3453721-463c-4de2-9bda-cd2fa3133ada_1884x984.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb3453721-463c-4de2-9bda-cd2fa3133ada_1884x984.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p>Many infrastructure providers, especially the older ones - were not naive about this - Because they had been burnt firsthand by GPU massive rental price drops, after a major price pump, from the crypto days - they had seen this cycle before.</p><p><strong>So for this cycle, last year, they pushed heavily for a 3-5 year upfront commitment and/or payment at the $4+ price range. </strong><span>(typically with 50% to 100% upfront)</span><strong>. </strong><span>Today, they push the $2.85+ price range - locking in their profits.</span></p><p>This happened aggressively during the 2023 AI peak with various foundation model companies, especially in the image generation space, indirectly forced into high-priced 3-5 year contracts, just so to get to the front-of-the-line of a new cluster, and be first to make their target model, to help close the next round.</p><p>It may not be the most economical move, but it lets them move faster than the competition.</p><p>This, however, has led to some interesting market dynamics - if you are paying $3 or $4 per hour for your H100, for the next 3 years, locked into a contract.</p><p><span>When a model creator is done training a model, you have no more use for the cluster. </span><strong>What would they do? - they resell and start recouping some of the costs.</strong></p><p>From hardware to AI inference / finetune, it can be broadly viewed as the following</p><ul><li><p>Hardware vendors partnered with Nvidia (one-time purchase cost)</p></li><li><p>Datacenter Infrastructure providers &amp; partners (selling long-term reservations, on facility space and/or H100 nodes)</p></li><li><p><span>VC Funds, Large Companies, and Startups: that plann</span><em>ed</em><span> to build foundation models (or have already finished building their models)</span></p></li><li><p><strong>Resellers of capacity: Runpod, SFCompute, Together.ai, Vast.ai, GPUlist.ai</strong></p></li><li><p>Managed AI Inference / Finetune providers: who use a combination of the above</p></li></ul><p><span>While any layer down the stack may be vertically integrated (skipping the infra players for example), the key drivers here are the </span><strong>“Resellers of unused capacity” </strong><span>and the rise of “good enough” open weights models like </span><a href="https://www.latent.space/p/llama-3" rel="">Llama 3</a><span>, as they are all major influencing factors in the current H100 economical pressures.</span></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F333cc1d8-8e8b-47dc-b9f5-1a02e0eefd7b_2140x1280.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F333cc1d8-8e8b-47dc-b9f5-1a02e0eefd7b_2140x1280.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F333cc1d8-8e8b-47dc-b9f5-1a02e0eefd7b_2140x1280.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F333cc1d8-8e8b-47dc-b9f5-1a02e0eefd7b_2140x1280.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F333cc1d8-8e8b-47dc-b9f5-1a02e0eefd7b_2140x1280.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F333cc1d8-8e8b-47dc-b9f5-1a02e0eefd7b_2140x1280.png" width="1456" height="871" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/333cc1d8-8e8b-47dc-b9f5-1a02e0eefd7b_2140x1280.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:871,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:932212,&quot;alt&quot;:&quot;&quot;,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false}" alt="" title="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F333cc1d8-8e8b-47dc-b9f5-1a02e0eefd7b_2140x1280.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F333cc1d8-8e8b-47dc-b9f5-1a02e0eefd7b_2140x1280.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F333cc1d8-8e8b-47dc-b9f5-1a02e0eefd7b_2140x1280.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F333cc1d8-8e8b-47dc-b9f5-1a02e0eefd7b_2140x1280.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p><em><strong><span>The rise of open weights models, on-par with closed-source models.</span><br><span>Is resulting in a fundamental shift in the market</span></strong></em></p><blockquote><p><em><strong>↑↑ Increased demand for AI inference &amp; fine-tuning</strong><p><span>Because many “open” models, lack proper “open source” licenses, but are being distributed freely, and used widely, even commercially. We will refer to them collectively as “open-weights” or “open” models instead here.</span></p></em></p></blockquote><p>In general, with multiple open-weights models of various sizes being built, so has the growth in demand for inference and fine-tuning them. This is largely driven by two major events</p><ul><li><p>The arrival of GPT4 class open models (eg. 405B LLaMA3, DeepSeek-v2)</p></li><li><p>The maturity and adoption of small (~8B) and medium (~70B) fine-tuned models</p></li></ul><p>Today, for the vast majority of use cases, enterprises may need, there are already off-the-shelf open-weights models. Which might be a small step behind proprietary models in certain benchmarks.</p><p>Provides an advantage with the following</p><ul><li><p><strong>Flexibility</strong><span>: Domain / Task specific finetunes</span></p></li><li><p><strong>Reliability</strong><span>: No more minor model updates, breaking use case (there is currently low community trust that model weights are not quietly changed without notification in public API endpoints, causing inexplicable regressions)</span></p></li><li><p><strong>Security &amp; Privacy</strong><span>: Assurance that their prompts and customer data are safe.</span></p></li></ul><p>All of this leads to the current continuous growth and adoption of open models, with the growth in demand for inference and finetunes.</p><p>But it does cause another problem…</p><blockquote><p><em><strong>↓↓ Shrinking foundation model creator market (Small &amp; Medium)</strong><p><span>We used “model creators” to collectively refer to organization that create models from scratch. For fine-tuners, we refer to them as “model finetuners”</span></p></em></p></blockquote><p>Many enterprises, and multiple small &amp; medium foundation model creator startups - especially those who raised on the pitch of “smaller, specialized domain-specific models”, are groups who had no long-term plans / goals for training large foundation models from scratch ( &gt;= 70B ).</p><p>For both groups, they both came to the realization that it is more economical and effective to fine-tune existing Open Weights models, instead of “training on their own”.</p><p><strong>This ended up creating a triple whammy in reducing the demand for H100s!</strong></p><ol><li><p><strong>Finetuning is significantly cheaper than training from scratch.</strong></p><ol><li><p>Because the demands for fine-tuning are significantly less in compute requirements (typically 4 nodes or less, usually a single node), compared to training from scratch (from 16 nodes, usually more, for 7B and up models).</p></li><li><p>This industry-wide switch essentially killed a large part of smaller cluster demands.</p></li></ol></li><li><p><strong>Scaling back on foundation model investment (at small/mid-tier)</strong></p><ol><li><p>In 2023, there was a huge wave of small and medium foundation models, within the text and image space.</p></li><li><p>Today, however, unless you are absolutely confident you can surpass llama3, or you are bringing something new to the table (eg. new architecture, 100x lower inference, 100+ languages, etc), there are ~no more foundation model cos being founded from scratch.</p></li><li><p>In general, the small &amp; medium, open models created by the bigger players (Facebook, etc), make it hard for smaller players to justify training foundation models - unless they have a strong differentiator to do so (tech or data) - or have plans to scale to larger models.</p></li><li><p>And this has been reflected lately with investors as well, as there has been a sharp decline in new foundation model creators’ funding. With the vast majority of smaller groups having switched over to finetuning. (this sentiment is combined with the recent less than desired exits for multiple companies).</p></li><li><p>Presently today, there is approximately worldwide by my estimate:</p><ul><li><p>&lt;20 Large model creator teams (aka 70B++, may create small models as well)</p></li><li><p>&lt;30 Small / Medium model creator teams (7B - 70B)</p></li></ul></li><li><p>Collectively there are less than &lt;50 teams worldwide who would be in the market for 16 nodes of H100s (or much more), at any point in time, to do foundation model training.</p></li><li><p>There are more than 50 clusters of H100 worldwide with more than 16 nodes.</p></li></ol></li><li><p><strong>Excess capacity from reserved nodes is coming online</strong></p><ol><li><p>For the cluster owners, especially the various foundation model startups and VCs, who made long reservations, in the initial “land grab” of the year 2023.</p></li><li><p><span>With the switch to finetuning, and the very long wait times of the H100’s</span><br><span>(it peaked at &gt;= 6 months), it is very well possible that many of these groups had already made the upfront payment before they made the change, essentially making their prepaid hardware “obsolete on arrival”.</span></p></li><li><p>Alternatively, those who had the hardware arrive on time, to train their first few models, had come to the same realization it would be better to fine-tune their next iteration of models. Instead of building on their own.</p></li><li><p><span>In both cases, they would have unused capacity, which comes online via </span><strong>“Compute Resellers”</strong><span> joining the market supply….</span></p></li></ol></li></ol><p>Another major factor, is how all the major Model Creators, such as Facebook, X.AI, and arguably OpenAI (if you count them as part of Microsoft) are moving away from an existing public provider, and building their own billion-dollar clusters, removing the demand that the existing clusters depend on.</p><p>The move is happening mostly for the following reasons:</p><ul><li><p>Existing ~1k node clusters (which costs &gt;$50M to build), is no longer big enough for them, to train bigger models</p></li><li><p>At a billion-dollar scale, it is better for accounting to purchase assets (of servers, land, etc), which has booked value (part of company valuation and assets), instead of pure expenses leasing.</p></li><li><p>If you do not have the people (they do), you could straight up buy small datacenters companies, who have the expertise to build this for you.</p></li></ul><p>With the demand gradually weaning away in stages. These clusters are coming online to the public cloud market instead.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F23dbc794-e567-4b34-a6e7-761fafef2ccc_2710x1812.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F23dbc794-e567-4b34-a6e7-761fafef2ccc_2710x1812.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F23dbc794-e567-4b34-a6e7-761fafef2ccc_2710x1812.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F23dbc794-e567-4b34-a6e7-761fafef2ccc_2710x1812.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F23dbc794-e567-4b34-a6e7-761fafef2ccc_2710x1812.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F23dbc794-e567-4b34-a6e7-761fafef2ccc_2710x1812.png" width="1456" height="974" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/23dbc794-e567-4b34-a6e7-761fafef2ccc_2710x1812.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:974,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:1268863,&quot;alt&quot;:&quot;&quot;,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false}" alt="" title="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F23dbc794-e567-4b34-a6e7-761fafef2ccc_2710x1812.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F23dbc794-e567-4b34-a6e7-761fafef2ccc_2710x1812.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F23dbc794-e567-4b34-a6e7-761fafef2ccc_2710x1812.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F23dbc794-e567-4b34-a6e7-761fafef2ccc_2710x1812.png 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption>Vast.ai essentially does a free market system, where providers from all over the world, are forced to compete with each other</figcaption></figure></div><p>Recall all the H100 large shipment delays in 2023, or 6 months or more? They are coming online, now - along with the H200, B200, etc.</p><p>This is alongside, the various unused compute, coming online (from existing startups, enterprises or VCs as covered earlier).</p><p><span>The bulk of this is done via </span><strong>Compute Resellers</strong><span>, such as : together.ai, sfcompute, runpod, vast.ai, etc</span></p><p>In most cases, cluster owners have a small or medium cluster, (typically 8-64 nodes), that is underutilized. With the money already “spent” for the cluster.</p><p>With the primary goal is to recoup as much of the cost as possible, they rather undercut the market and guarantee an allocation, instead of competing with the main providers, and possibly have no allocation.</p><p>This is typically done either via a fixed rate, an auction system, or just a free market listing, etc. With the later 2 driving the market price downwards.</p><p>Another major factor, is once your outside of the training / fine-tune space. The inference space is filled with alternatives, especially if your running smaller models.</p><p>One do not need to pay for the premium invoked by H100’s Infiniband and/or nvidia.</p><p>H100 premium for training is priced into the hardware. For example nvidia themselves recommend the L40S, which is the more price competitive alternative for inference.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fdf705375-9942-4ea5-86b9-b41d3661096b_1842x556.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fdf705375-9942-4ea5-86b9-b41d3661096b_1842x556.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fdf705375-9942-4ea5-86b9-b41d3661096b_1842x556.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fdf705375-9942-4ea5-86b9-b41d3661096b_1842x556.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fdf705375-9942-4ea5-86b9-b41d3661096b_1842x556.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fdf705375-9942-4ea5-86b9-b41d3661096b_1842x556.png" width="1456" height="439" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/df705375-9942-4ea5-86b9-b41d3661096b_1842x556.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:439,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:414838,&quot;alt&quot;:&quot;&quot;,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false}" alt="" title="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fdf705375-9942-4ea5-86b9-b41d3661096b_1842x556.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fdf705375-9942-4ea5-86b9-b41d3661096b_1842x556.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fdf705375-9942-4ea5-86b9-b41d3661096b_1842x556.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fdf705375-9942-4ea5-86b9-b41d3661096b_1842x556.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p>Which Is 1/3rd the performance, at 1/5th the price. But does not work well with multi-node training. Undercutting their very own H100 for this segment.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F55aee33d-e501-4923-9fd2-39740f781ed5_2610x826.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F55aee33d-e501-4923-9fd2-39740f781ed5_2610x826.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F55aee33d-e501-4923-9fd2-39740f781ed5_2610x826.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F55aee33d-e501-4923-9fd2-39740f781ed5_2610x826.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F55aee33d-e501-4923-9fd2-39740f781ed5_2610x826.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F55aee33d-e501-4923-9fd2-39740f781ed5_2610x826.png" width="1456" height="461" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/55aee33d-e501-4923-9fd2-39740f781ed5_2610x826.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:461,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:2982780,&quot;alt&quot;:&quot;&quot;,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false}" alt="" title="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F55aee33d-e501-4923-9fd2-39740f781ed5_2610x826.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F55aee33d-e501-4923-9fd2-39740f781ed5_2610x826.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F55aee33d-e501-4923-9fd2-39740f781ed5_2610x826.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F55aee33d-e501-4923-9fd2-39740f781ed5_2610x826.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p>Both AMD and Intel may be late into the game with their MX300, and Gaudi 3 respectively.</p><p>This has been tested and verified by us, having used these systems. They are generally:</p><ul><li><p>Cheaper than a H100 in purchase cost</p></li><li><p>Have more memory and compute than a H100, and outperforms on a single node.</p></li><li><p>Overall, they are great hardware!</p></li></ul><p>The catch? They have minor driver issues in training and are entirely unproven in large multi-node cluster training.</p><p>Which as we covered is largely irrelevant to the current landscape. To anyone but &lt;50 teams. The market for H100 has been moving towards inference and single or small cluster fine-tuning.</p><p>All of which these GPUs have been proven to work at. For the use cases, the vast majority of the market is asking for.</p><p>These 2 competitors are full drop-in replacements. With working off-the-shelf inference code (eg. VLLM) or finetuning code for most common model architectures (primarily LLaMA3, followed by others).</p><p>So, if you have compatibility sorted out. Its highly recommended to have a look.</p><p>With Ethereum moving towards proof of stake, ASIC dominating the bitcoin mining race, and the general crypto market condition.</p><p>GPU usage in mining for crypto has been a downward trend, and in several cases unprofitable. And has since been flooding the GPU public cloud market.</p><p>And while the vast majority of these GPUs are unusable for training, or even for inference, due to hardware constraints (low PCIe bandwidth, network, etc). The hardware has been flooding the market and has been repurposed for AI inference workloads.</p><p>In most cases if you are under &lt;10B, you can get decent performance with these GPUs, out of the box, for really low prices.</p><p>If you optimize it further (though various tricks), you can even get large 405B models to run on a small cluster of this hardware, cheaper then an H100 node (which is what is typically used)</p><p><em><span>H100 Prices are becoming commodity-prices cheap.</span><br><span>Or even being rented at a loss - if so, what now?</span></em></p><p>On a high level, it is expected that big clusters still get to charge a premium (&gt;=$2.90 / hour) because there is no other option. For those who truly need it.</p><p>We are starting to see this trend for example with Voltage Park:</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc1dd60f1-1e51-4ec7-ac64-d5a2b506f2f0_2118x1078.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc1dd60f1-1e51-4ec7-ac64-d5a2b506f2f0_2118x1078.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc1dd60f1-1e51-4ec7-ac64-d5a2b506f2f0_2118x1078.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc1dd60f1-1e51-4ec7-ac64-d5a2b506f2f0_2118x1078.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc1dd60f1-1e51-4ec7-ac64-d5a2b506f2f0_2118x1078.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc1dd60f1-1e51-4ec7-ac64-d5a2b506f2f0_2118x1078.png" width="1456" height="741" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/c1dd60f1-1e51-4ec7-ac64-d5a2b506f2f0_2118x1078.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:741,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:647709,&quot;alt&quot;:&quot;&quot;,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false}" alt="" title="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc1dd60f1-1e51-4ec7-ac64-d5a2b506f2f0_2118x1078.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc1dd60f1-1e51-4ec7-ac64-d5a2b506f2f0_2118x1078.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc1dd60f1-1e51-4ec7-ac64-d5a2b506f2f0_2118x1078.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc1dd60f1-1e51-4ec7-ac64-d5a2b506f2f0_2118x1078.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p>Where clusters with Infiniband are charged at a premium.</p><p>While the Ethernet-based instances, which are perfectly fine for inference are priced at a lower rate. Adjusting the prices for the respective use case/availability.</p><p>While there’s been a general decline in foundation model creator teams, it is hard to predict if there will be a resurgence, with the growth in open weights, and/or alternative architectures.</p><p>It is also, expected that in the future, we will see further segmentation by cluster sizes. Where a large 512-node cluster with Infiniband may be billed higher per GPU than a 16-node cluster.</p><p>There is a lot against you, if you price it below $2.25, depending on your OPEX, you risk potentially being unprofitable.</p><p>If you price it too high &gt;= $3, you might not be able to get sufficient buyers to fill capacity.</p><p>If you're late, you could not recoup the cost in the early $4/hour days.</p><p>Overall, these cluster investments will be rough for the key stakeholders and investors.</p><p>While I doubt it’s the case, if new clusters, make a large segment of the AI portfolio investments. We may see additional rippling effects in the funding ecosystem from burnt investors.</p><p>Instead of a negative outlook, a neutral outlook would be some of the unused compute foundation model creators, coming online, are already paid for.</p><p>The funding market has already priced in and paid for this cluster and its model training. And “extracted its value” which they used for their current and next funding round.</p><p><span>Most of these purchases were made before the popularity of </span><strong>Compute Resellers</strong><span>, the cost was already priced in.</span></p><p>If anything, the current revenue they get from their excess H100 compute, and the lowered prices we get, are beneficial to both parties</p><p>If so the negative market impact is minimal, while overall it’s a net positive win for the ecosystem.</p><p>Given that the open-weights model has entered the GPT-4 class arena. Falling H100 prices will be the multiplier unlock for open-weights AI adoption.</p><p>It will be more affordable, for hobbyists, AI developers, and engineers, to run, fine-tune, and tinker with these open models.</p><p><span>Especially if there is no major leap for GPT5++,</span><strong> </strong><span>because it will mean that the gap between open-weights and closed-source models will blur.</span></p><p>This is strongly needed, as the market is currently not sustainable. As there lacks the value capture on the application layer for paying users (which trickles down the platform, models, and infra layers)</p><p>In a way, if everyone is building shovels (including us), and applications with paying users are not being built (and collecting revenue and value).</p><p>But when AI inference and fine-tuning becomes cheaper than ever.</p><p>It can potentially kick off the AI application wave. If it has not already slowly started so.</p><p><em><strong>Spending on new H100’s hardware is likely a loss-maker</strong><p><span>Unless you have some combination of discounted H100s, discounted electricity, or a Sovereign AI angle where the location of your GPU is critical to your customers. Or you have billions and need a super large cluster.</span></p><p><span>If you're investing, consider investing elsewhere.</span><br><span>Or the stock market index itself for a better rate of returns. IMO</span></p></em></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3e858927-de4c-499b-9ba0-261dd470a88b_1948x1088.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3e858927-de4c-499b-9ba0-261dd470a88b_1948x1088.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3e858927-de4c-499b-9ba0-261dd470a88b_1948x1088.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3e858927-de4c-499b-9ba0-261dd470a88b_1948x1088.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3e858927-de4c-499b-9ba0-261dd470a88b_1948x1088.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3e858927-de4c-499b-9ba0-261dd470a88b_1948x1088.png" width="1456" height="813" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/3e858927-de4c-499b-9ba0-261dd470a88b_1948x1088.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:813,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:664297,&quot;alt&quot;:&quot;&quot;,&quot;title&quot;:&quot;&quot;,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false}" alt="" title="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3e858927-de4c-499b-9ba0-261dd470a88b_1948x1088.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3e858927-de4c-499b-9ba0-261dd470a88b_1948x1088.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3e858927-de4c-499b-9ba0-261dd470a88b_1948x1088.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3e858927-de4c-499b-9ba0-261dd470a88b_1948x1088.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p>At Featherless.AI - We currently host the world’s largest collection of OpenSource AI models, instantly accessible, serverlessly, with unlimited requests from $10 a month, at a fixed price.</p><p><span>We have indexed and made over 2,000 models ready for inference today. This is 10x the catalog of openrouter.ai, the largest model provider </span><em>aggregator</em><span>, </span><em>and </em><span>is the world’s largest collection of Open Weights models available serverlessly for instant inference. Without the need for any expensive dedicated GPUs</span></p><p>And our platform makes this possible, as it’s able to dynamically hot-swap between models in seconds.</p><p>It’s designed to be easy to use, with full OpenAI API compatibility, so you can just plug our platform in as a replacement to your existing AI API for your AI agents. Running in the background</p><p>And we do all of this; As we believe that AI should be easily accessible to everyone, regardless of language or social status.</p><p>On the technical side of things, related to this article.</p><p>It is a challenge having PetaBytes’s worth of AI models, and growing, running 24/7 - while being hardware profitable (we are), because we needed to optimize every layer of our platform, down to how we choose the GPU hardware.</p><p>In an industry, where the typical inference provider pitch is typically along the lines of winning with their, special data center advantages, and CUDA optimization that they perform on their own hardware. Hardware is CAPEX intensive. (Which is being pitched and funded even today)</p><p>We were saying the opposite, which defied most investors’ sensibilities - we were saying we would be avoiding buying new hardware like the plague.</p><p>We came to a realization, that most investors, their analysts, and founders failed to realize, thanks to the billions in hardware investments to date. GPUs are commodity hardware. Faster than all of us expected.</p><p>Few investors have even realized we have reached commodity-level prices at $2.85 in certain places, let alone loss-making prices of a dollar. Because most providers (ignoring certain exceptions), only show their full prices after quotation or after login.</p><p>And that was the trigger, which got me to write this article.</p><p>While we do optimize our inference CUDA and kernels as well. On the hardware side; We’ve bet on hardware commoditizing and have focussed instead on the orchestration layer above.</p><p>So for us, this is a mix of sources from, AWS spot (preferred), to various data center grade providers (eg. Tensordock, Runpod) with security and networking compliances that meet our standards.</p><p>Leveraging them with our own proprietary model hot swapping, which boots new models up in under a second. Keeping our fleet of GPUs right-sized to our workload, while using a custom version of our RWKV foundation model as a low-cost speculative decoder. All of which allows us to take full advantage of this market trend, and future GPU price drops, as newer (and older) GPUs come online to replace the H100s. And scale aggressively.</p><p><em>PS: If you are looking at building the world's largest inference platform, and are aligned with our goals - to make AI accessible to everyone, regardless of language or status. Reach out to us at: hello@featherless.ai</em></p><p><em><span>Head over to Eugene’s Blog </span></em></p><p><em><span> for </span><a href="https://substack.tech-talk-cto.com/p/d4ffab7a-3f0d-4e6e-ade0-e74409770196?postPreview=paid&amp;updated=2024-08-25T03%3A36%3A59.886Z&amp;audience=everyone&amp;free_preview=false&amp;freemail=true" rel="">more footnotes on xAI’s H100 cluster</a><span> we cut from this piece.</span></em><span> </span></p><p><strong>Additional Sources:</strong></p><ul><li><p><span>GPU data: </span><a href="https://www.techpowerup.com/gpu-specs/h100-sxm5-80-gb.c3900" rel="">Tech Power Up Database</a><span>. The A100 SXM had 624 bf16 TFlops, the H100 SXM was 1,979 bf16 TFlops</span></p></li><li><p><span>Microsoft &amp; AWS allocated over $40 billion in AI infra alone: </span><a href="https://www.wsj.com/tech/ai/big-tech-moves-more-ai-spending-abroad-088988de" rel="">Wall Street Journal</a></p></li><li><p><span>“600 Billion Dollars “ is about: </span><a href="https://www.sequoiacap.com/article/ais-600b-question/" rel="">Sequoia’s AI article</a></p></li><li><p><span>Nvidia investor slides for Oct 2014: </span><a href="https://s201.q4cdn.com/141608511/files/doc_presentations/2023/Oct/01/ndr_presentation_oct_2023_final.pdf" rel="">page 14 has the pitch for “data centers”</a></p></li><li><p><span>Semi Analysis: </span><a href="https://www.semianalysis.com/p/100000-h100-clusters-power-network" rel="">deepdive for H100 clusters, w/ 5 year lifespan approx for components</a></p></li><li><p><span>Spreadsheet for : </span><a href="https://docs.google.com/spreadsheets/d/1kZosZmvaecG6P4-yCPzMN7Ha3ubMcTmF9AeJNDKeo98/edit?usp=sharing" rel="">new H100 ROI (Aug 2024)</a></p></li><li><p><span>Spreadsheet for: </span><a href="https://docs.google.com/spreadsheets/d/1Ft3RbeZ-w43kYSiLfYc1vxO41mK5lmJpcPC9GOYHAWc/edit?usp=sharing" rel="">H100 Infiniband Cluster math (Aug 2024)</a></p></li></ul></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[WordPress Alternatives (136 pts)]]></title>
            <link>https://darn.es/wordpress-alternatives/</link>
            <guid>41805391</guid>
            <pubDate>Fri, 11 Oct 2024 02:03:30 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://darn.es/wordpress-alternatives/">https://darn.es/wordpress-alternatives/</a>, See on <a href="https://news.ycombinator.com/item?id=41805391">Hacker News</a></p>
<div id="readability-page-1" class="page"><article>
      <heading-anchors>
        <div><p>📝</p><p>Editor note: Due to this article's unexpected attention, I've included a few more alternatives that people suggested. I've also added some contextual notes you should know before diving into these options.</p></div><p>Due to <em>gestures vaguely, </em>everything going on <a href="https://css-tricks.com/catching-up-on-the-wordpress-wp-engine-sitch/" rel="noreferrer">right now with WordPress</a>, I thought I'd put together a list of alternative CMSs that better fit the criteria someone might have for their website. The modern CMS landscape is super broad, with the very definition of "Content Management System" being stretched. Some see it as a full-package website platform, and some see it as just UI for their content stored elsewhere.</p><p>The criteria for this list are "Can it be downloaded, dropped onto a server, and you'll have a website?" This eliminates API and git-based CMSs, which I enjoy using; however, wiring a daisy chain of tools is just not viable for many.</p><figure><a href="https://ghost.org/"><div><p>Ghost: The best open source blog &amp; newsletter platform</p><p>Beautiful, modern publishing with email newsletters and paid subscriptions built-in. Used by Platformer, 404Media, Lever News, Tangle, The Browser, and thousands more.</p><p><img src="https://darn.es/img/Z24D4b6-favicon-1.ico" alt=""></p></div><p><img src="https://darn.es/img/Z1hjw9Q-ghost.png" alt="" onerror="this.style.display = 'none'"></p></a></figure><p>People will already know I have a soft spot for Ghost. But what you might not know is what I'd recommend for hosting.</p><figure><a href="https://www.magicpages.co/"><div><p>Magic Pages</p><p>Get your Ghost CMS publication up and running in no time with Magic Pages’ Ghost CMS web hosting – starting at $4/month!</p><p><img src="https://darn.es/img/iczFH-favicon-196x196-1.jpg" alt=""><span>Magic Pages</span></p></div><p><img src="https://darn.es/img/Z1EYPIA-MagicPages.co-3.png" alt="" onerror="this.style.display = 'none'"></p></a></figure><p>Magic Pages is what I'm using for <a href="https://designsystems.wtf/" rel="noreferrer">Design Systems WTF</a>, and it's been great! The uptime is good, the price is very reasonable, and <a href="https://www.jannis.io/" rel="noreferrer">Jannis</a> provides a personal touch with support. In addition, this sidesteps Ghost's own hosting option (Ghost Pro), which I would be wary of due to <a href="https://x.com/amyhoy/status/1449482190224384000">past experiences with other customers</a>.</p><figure><a href="https://getkirby.com/"><div><p>Kirby is the CMS that adapts to you</p><p>Kirby is the content management system that adapts to any project. Made for developers, designers, creators and clients.</p><p><img src="https://darn.es/img/1CTUoB-favicon.1704303350.svg" alt=""><span>Kirby CMS</span></p></div><p><img src="https://darn.es/img/1RRwiG-opengraph.png" alt="" onerror="this.style.display = 'none'"></p></a></figure><p>I have not used Kirby in client work, but I hear only good things. It's file-based, which seems super appealing to someone like myself who gets cold sweats when opening a database.</p><figure><a href="https://getindiekit.com/"><div><p>Indiekit</p><p>The little server that connects your website to the independent web.</p><p><img src="https://darn.es/img/1Py0ge-icon.svg" alt=""><span>Get Started</span></p></div><p><img src="https://darn.es/img/Z1QWsfN-opengraph-image.png" alt="" onerror="this.style.display = 'none'"></p></a></figure><p>Indiekit seems like an interesting option; it's also file-based but needs a database to manage existing content.</p><figure><a href="https://craftcms.com/"><div><p>Craft CMS</p><p>Craft is a flexible, user-friendly CMS for creating custom digital experiences on the web and beyond.</p><p><img src="https://darn.es/img/1o1WvD-apple-touch-icon.png" alt=""><span>Craft CMS</span></p></div><p><img src="https://darn.es/img/ZXh3Gs-social-craft-cms.png" alt="" onerror="this.style.display = 'none'"></p></a></figure><p>It's a bit more of a commercial option with Craft CMS, but it does offer a free option for solo creators. Warning, though, as you'll need to spend time architecting your content structure by the looks of it.</p><figure><a href="https://www.classicpress.net/"><div><p>ClassicPress | Stable. Lightweight. Instantly Familiar.</p><p>ClassicPress is a community-led open source content management system. A fork of WordPress 4.9, it retains the WordPress classic editor as the default option.</p><p><img src="https://darn.es/img/1MLpdH-cropped-icon-gradient-500x500.png" alt=""><span>ClassicPress</span></p></div><p><img src="https://darn.es/img/1lFgeF-classicpress-cms-for-creators.jpg" alt="" onerror="this.style.display = 'none'"></p></a></figure><p>ClassicPress appears to be a direct fork of WordPress but at version 6.2.3. It seems perfect for anyone looking for "the good old days." However, it uses the official WordPress plugin API, so it's not a 100% clean break if that's what you're going for.  Thanks to <a href="https://voxpelli.com/" rel="noreferrer">Pelle Wessman</a> for this suggestion.</p><figure><a href="https://statamic.com/"><div><p>Statamic is a powerful, highly scalable CMS built on Laravel.</p><p>The open source, flat-first, Laravel + Git powered CMS designed for building easy to manage websites.</p><p><img src="https://darn.es/img/Z2ddyQl-favicon-196x196.png" alt=""><span>Statamic</span></p></div><p><img src="https://darn.es/img/Z1MKo0C-card-2023.jpg" alt="" onerror="this.style.display = 'none'"></p></a></figure><p>I've had several people suggest Statamic. It does look pretty good, plus they have a free solo plan (similar to Craft CMS). I think if the cofounder hadn't brazenly <a href="https://jaygeorge.co.uk/blog/intolerance">endorsed a horrendously damaging politician</a>, I'd have tried it.</p><p>I was going to suggest <a href="https://grabaperch.com/" rel="noreferrer">Perch</a> and <a href="http://buckets.io/" rel="noreferrer">Buckets</a> on this list, but public activity seems low for both. The Perch website even has SSL certificate issues, which isn't a good sign. Check them out if you're interested, but you have been warned.</p><h3 id="honourable-mention">Honourable mention</h3><figure><a href="http://anchorcms.com/"><div><p>Lifting Anchor</p><p>Help on how to use Anchor</p><p><img src="https://darn.es/img/p9SGH-1433533.png" alt=""><span>Anchor CMS</span></p></div><p><img src="https://darn.es/img/Z2kqVUC-screenshot.png" alt="" onerror="this.style.display = 'none'"></p></a></figure><p>Many years ago, I contributed to Anchor, a humble PHP-based CMS that grew a little community around itself. Sadly, the creator, Charlotte, passed away in 2020, and the remaining core team couldn't keep the project going while juggling other responsibilities. I think of it fondly and wish we could give it the time it deserves. </p><p>The theming and custom types aspects were wonderfully simple; heck, I even made a whole site dedicated to themes and sites built with it:</p><figure><a href="https://anchorthemes.com/"><div><p>Welcome - Anchor Themes</p><p>Themes and sites built for &lt;a href=“https://anchorcms.com”&gt;Anchor&lt;/a&gt;, obviously</p><p><img src="https://darn.es/img/xtffw-link-icon.svg" alt=""><span>Anchor Themes</span><span>David Darnes</span></p></div><p><img src="https://darn.es/img/Z1nRKSm-facebook.png" alt="" onerror="this.style.display = 'none'"></p></a></figure><p>I'll try to keep this list up to date if I recall any others I've used in the past. Hopefully, you find this useful if you're seeking alternative CMSs.</p>
      </heading-anchors>
    </article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[The FBI created a coin to investigate crypto pump-and-dump schemes (106 pts)]]></title>
            <link>https://www.theverge.com/2024/10/10/24267098/fbi-coin-crypto-token-nexgenai-sec-doj-fraud-investigation</link>
            <guid>41802823</guid>
            <pubDate>Thu, 10 Oct 2024 19:52:47 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.theverge.com/2024/10/10/24267098/fbi-coin-crypto-token-nexgenai-sec-doj-fraud-investigation">https://www.theverge.com/2024/10/10/24267098/fbi-coin-crypto-token-nexgenai-sec-doj-fraud-investigation</a>, See on <a href="https://news.ycombinator.com/item?id=41802823">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>The FBI created a cryptocurrency as part of an investigation into price manipulation in crypto markets, the <a href="https://www.justice.gov/usao-ma/pr/eighteen-individuals-and-entities-charged-international-operation-targeting-widespread">government revealed</a> on Wednesday. The FBI’s Ethereum-based token, NexFundAI, was created with the help of “cooperating witnesses.”</p><p>As a result of the investigation, the Securities and Exchange Commission <a href="https://www.sec.gov/newsroom/press-releases/2024-166">charged</a> three “market makers” and nine people for allegedly engaging in schemes to boost the prices of certain crypto assets. The Department of Justice charged 18 people and entities for “widespread fraud and manipulation” in crypto markets.</p><p>The defendants allegedly made false claims about their tokens and executed so-called “wash trades” to create the impression of an active trading market, prosecutors claim. The three market makers — ZMQuant, CLS Global, and MyTrade — allegedly wash traded or conspired to wash trade on behalf of NexFundAI, an Ethereum-based token they didn’t realize was created by the FBI.&nbsp;</p><p>“What the FBI uncovered in this case is essentially a new twist to old-school financial crime,” Jodi Cohen, the special agent in charge of the FBI’s Boston division, said in a statement. “What we uncovered has resulted in charges against the leadership of four cryptocurrency companies, and four crypto ‘market makers’ and their employees who are accused of spearheading a sophisticated trading scheme that allegedly bilked honest investors out of millions of dollars.”</p><p>Liu Zhou, a “market maker” working with MyTrade MM, allegedly told promoters of NexFundAI that MyTrade MM was better than its competitors because they “control the pump and dump” allowing them to “do inside trading easily.”</p><p>An FBI spokesperson <a href="https://www.coindesk.com/policy/2024/10/09/prosecutors-charge-two-crypto-market-makers-employees-with-market-manipulation-fraud/">told <em>CoinDesk</em></a> that there was limited trading activity on the coin but didn’t share additional information. On a Wednesday press call, Joshua Levy, the acting US attorney for the District of Massachusetts, said trading on the token was disabled, according to <em>CoinDesk</em>.</p><p>The DOJ has reportedly secured $25 million from “fraudulent proceeds” that will be returned to investors.</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[The Copenhagen Book: general guideline on implementing auth in web applications (582 pts)]]></title>
            <link>https://thecopenhagenbook.com/</link>
            <guid>41801883</guid>
            <pubDate>Thu, 10 Oct 2024 18:37:01 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://thecopenhagenbook.com/">https://thecopenhagenbook.com/</a>, See on <a href="https://news.ycombinator.com/item?id=41801883">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="content">
      
      <main>
<p>The Copenhagen Book provides a general guideline on implementing auth in web applications. It is free, open-source, and community-maintained. It may be opinionated or incomplete at times but we hope this fills a certain void in online resources. We recommend using this alongside the <a href="https://cheatsheetseries.owasp.org/index.html">OWASP Cheat Sheet Series</a>.</p>
<p>If you have any suggestions or concerns, consider opening a new issue.</p>
<p><em>Created by <a href="https://github.com/pilcrowOnPaper">Pilcrow</a></em></p>
</main>
    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[TypedDicts are better than you think (124 pts)]]></title>
            <link>https://blog.changs.co.uk/typeddicts-are-better-than-you-think.html</link>
            <guid>41801415</guid>
            <pubDate>Thu, 10 Oct 2024 17:56:11 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://blog.changs.co.uk/typeddicts-are-better-than-you-think.html">https://blog.changs.co.uk/typeddicts-are-better-than-you-think.html</a>, See on <a href="https://news.ycombinator.com/item?id=41801415">Hacker News</a></p>
<div id="readability-page-1" class="page"><section id="content">
    <article>
      <header>
        
      </header>

      <div>
<!-- /.post-info -->        <p><code>TypedDict</code> was introduced in <a href="https://peps.python.org/pep-0589/">PEP-589</a> which landed in Python 3.8.</p>
<p>The primary use case was to create type annotations for dictionaries. For example,</p>
<div><pre><span></span><code><span>class</span> <span>Movie</span><span>(</span><span>TypedDict</span><span>):</span>
    <span>title</span><span>:</span> <span>str</span>


<span>movie</span><span>:</span> <span>Movie</span> <span>=</span> <span>{</span><span>"title"</span><span>:</span> <span>"Avatar"</span><span>}</span>
</code></pre></div>

<p>I remember thinking at the time that this was pretty neat, but I tend to use <code>dataclass</code> or <code>pydantic</code> to represent 'record' type data. Instead I use dictionaries more as a collection, so the standard <code>dict[KT, VT]</code> annotation is enough.</p>
<h3>Non-totality</h3>
<p>I revisited typeddicts when I looked at implementing a HTTP patch endpoint.</p>
<p>Let's suppose I have a data structure represented by the following dataclass:</p>
<div><pre><span></span><code><span>@dataclass</span>
<span>class</span> <span>User</span><span>:</span>
    <span>id</span><span>:</span> <span>UUID</span>
    <span>name</span><span>:</span> <span>str</span>
    <span>subscription</span><span>:</span> <span>str</span> <span>|</span> <span>None</span> <span>=</span> <span>None</span>
</code></pre></div>

<p>Where <code>subscription = None</code> means no subscription.</p>
<p>Let's say we want to option to patch name, subscription. You might define the patch body using dataclass:</p>
<div><pre><span></span><code><span>@dataclass</span>
<span>class</span> <span>PatchUser</span><span>:</span>
    <span>name</span><span>:</span> <span>str</span> <span>|</span> <span>None</span> <span>=</span> <span>None</span>
    <span>subscription</span><span>:</span> <span>str</span> <span>|</span> <span>None</span> <span>=</span> <span>None</span>
</code></pre></div>

<p>Here we have a problem, for subscription does <code>None</code> mean don't change or remove subscription. </p>
<p>We can fix this a number of ways, for example, we can take the string <code>'none'</code> to mean no subscription instead, or make a new sentinel value called <code>NoChange</code> to indicate no changes.</p>
<p>These solutions all feel a little awkward, this is because dataclasses don't have a concept of a field being missing. But this is where dictionaries shine. Dictionaries are not general expected to have all the fields available. We get a <code>KeyError</code> if a field is missing and there are convenience methods such as <code>.get(key, [default])</code> to fetch a key that is not guaranteed to be present.</p>
<p>This makes <code>TypedDict</code> the ideal data structure in this scenario:</p>
<div><pre><span></span><code><span>class</span> <span>PatchUser</span><span>(</span><span>TypedDict</span><span>,</span> <span>total</span><span>=</span><span>False</span><span>):</span>
    <span>name</span><span>:</span> <span>str</span> <span>|</span> <span>None</span>
    <span>subscription</span><span>:</span> <span>str</span> <span>|</span> <span>None</span> <span>=</span> <span>None</span>
</code></pre></div>

<p>Since <code>total</code> is False here (by default it is set to True), <code>name</code> or <code>subscription</code> can be absent from the dictionary. Which represents the PATCH operation much better than a <code>dataclass</code> or Pydantic model.</p>
<p>Further additions in <a href="https://peps.python.org/pep-0655/">PEP-655</a> allows us to mark individual fields as <code>Required</code> or <code>NotRequired</code> which further increases its flexibility.</p>
<blockquote>
<p>If you're wondering about FastAPI support for TypedDict, <a href="https://docs.pydantic.dev/2.3/usage/types/dicts_mapping/#typeddict">Pydantic supports it out of the box</a>. So your TypedDict can be used in a FastAPI endpoint.</p>
</blockquote>
<h3>Using <code>TypedDict</code> as <code>**kwargs</code></h3>
<p><a href="https://peps.python.org/pep-0692/">PEP-692</a> introduced the ability to type variadic keyword arguments using <code>TypedDict</code>.</p>
<p>So the following two snippets are equivalent.
Without <code>TypedDict</code>:</p>
<div><pre><span></span><code><span>def</span> <span>my_function</span><span>(</span><span>*</span><span>,</span> <span>option1</span><span>:</span> <span>int</span><span>,</span> <span>option2</span><span>:</span> <span>str</span><span>)</span> <span>-&gt;</span> <span>None</span><span>:</span>
    <span>...</span>
</code></pre></div>

<p>Using <code>TypedDict</code>:</p>
<div><pre><span></span><code><span>from</span> <span>typing</span> <span>import</span> <span>TypedDict</span><span>,</span> <span>Unpack</span>


<span>class</span> <span>Options</span><span>(</span><span>TypedDict</span><span>):</span>
    <span>option1</span><span>:</span> <span>int</span>
    <span>option2</span><span>:</span> <span>str</span>


<span>def</span> <span>my_function</span><span>(</span><span>**</span><span>options</span><span>:</span> <span>Unpack</span><span>[</span><span>Options</span><span>])</span> <span>-&gt;</span> <span>None</span><span>:</span>
    <span>...</span>
</code></pre></div>

<p>At a glance I can say that the TypedDict option is rather verbose. Though it does become more useful if Options were used in multiple function definitions.</p>
<div><pre><span></span><code><span>def</span> <span>my_function2</span><span>(</span><span>**</span><span>options</span><span>)</span> <span>-&gt;</span> <span>None</span><span>:</span>
    <span>...</span>


<span>def</span> <span>my_function3</span><span>(</span><span>*</span><span>,</span> <span>other_option</span><span>:</span> <span>str</span><span>,</span> <span>**</span><span>options</span><span>)</span> <span>-&gt;</span> <span>None</span><span>:</span>
    <span>...</span>
</code></pre></div>

<p>Where it truely shines is once again with non-totality.</p>
<p>Suppose we have the following scenario, where we want to create a custom version of pytest.fixture, but still pass through some arguments.</p>
<div><pre><span></span><code><span>def</span> <span>fixture</span><span>(</span><span>scope</span><span>:</span> <span>str</span> <span>=</span> <span>"module"</span><span>,</span> <span>autouse</span><span>:</span> <span>bool</span> <span>=</span> <span>False</span><span>):</span>
    <span>return</span> <span>pytest</span><span>.</span><span>fixture</span><span>(</span><span>scope</span><span>,</span> <span>autouse</span><span>)</span>
</code></pre></div>

<p>Here to get the typing right I not only have to find the type of each argument but also the default value. It would be better if we use <code>**kwargs</code> so we can just avoid passing the arguments through. And to keep type information we just need to use our trusty <code>TypedDict</code> once more:</p>
<div><pre><span></span><code><span>class</span> <span>FixtureOptions</span><span>(</span><span>TypedDict</span><span>,</span> <span>total</span><span>=</span><span>False</span><span>):</span>
    <span>scope</span><span>:</span> <span>str</span>
    <span>autouse</span><span>:</span> <span>bool</span>


<span>def</span> <span>fixture</span><span>(</span><span>**</span><span>options</span><span>:</span> <span>Unpack</span><span>[</span><span>FixtureOptions</span><span>]):</span>
    <span># Some custom implementations</span>
    <span>...</span>
    <span>return</span> <span>pytest</span><span>.</span><span>fixture</span><span>(</span><span>**</span><span>options</span><span>)</span>
</code></pre></div>

<p>Non-totallity means that we don't have to pass in scope and autouse. We can just have the default.</p>
<h4>Sentinels</h4>
<p>We can achieve similar behaviour with sentinels:</p>
<div><pre><span></span><code><span>UNSPECIFIED</span><span>:</span> <span>Any</span> <span>=</span> <span>object</span><span>()</span>  <span># Has to be Any type so it could be set as default for other types.</span>

<span>def</span> <span>my_func</span><span>(</span><span>option1</span><span>:</span> <span>bool</span> <span>=</span> <span>UNSPECIFIED</span><span>,</span> <span>...</span><span>)</span> <span>-&gt;</span> <span>...</span><span>:</span>
    <span>if</span> <span>option1</span> <span>is</span> <span>UNSPECIFIED</span><span>:</span>
        <span>...</span>
    <span>...</span>
</code></pre></div>

<p>Sentinels work well enough here, but we have to remember to handle them. Additionally type annotations for sentinels can be a bit awkward, here we made <code>UNSPECIFIED</code> an <code>Any</code> type, but it means that inside the function <code>option1</code> is only typed as <code>bool</code>. There are options to expose the sentinel type but they may add even more confusion.</p>
<h3>Using <code>TypedDict</code> to pass in dependencies</h3>
<p>We can do even more with <a href="https://peps.python.org/pep-0692/">PEP-692</a>! When I first learned about the PEP, I thought it was only about function signature. But reading through it more thoroughly, I discovered that another consequence of the PEP is that type checkers can now check for function invocation when using TypedDicts:</p>
<div><pre><span></span><code><span>def</span> <span>purge</span><span>(</span><span>queue</span><span>:</span> <span>str</span><span>,</span> <span>timeout</span><span>:</span> <span>float</span><span>)</span> <span>-&gt;</span> <span>...</span><span>:</span>
    <span>...</span>


<span>class</span> <span>Options</span><span>(</span><span>TypedDict</span><span>):</span>
    <span>queue</span><span>:</span> <span>str</span>
    <span>timeout</span><span>:</span> <span>float</span>


<span>class</span> <span>WrongOptions</span><span>(</span><span>TypedDict</span><span>):</span>
    <span>queue</span><span>:</span> <span>str</span>
    <span>timeout</span><span>:</span> <span>timedelta</span>


<span>options</span><span>:</span> <span>Options</span> <span>=</span> <span>...</span>
<span>purge</span><span>(</span><span>**</span><span>options</span><span>)</span>  <span># ✅</span>

<span>wrong_options</span><span>:</span> <span>WrongOptions</span> <span>=</span> <span>...</span>
<span>purge</span><span>(</span><span>**</span><span>wrong_options</span><span>)</span>  <span># ❌</span>
</code></pre></div>

<p>This feature is necessary in many situations such as cases where we pass through the kwargs. For example, in the <code>fixture</code> example, when we invoke <code>pytest.fixture(**options)</code> the type checker will perform proper type checking.</p>
<p>But we can use it in more creative ways.</p>
<h4>Dependency Injection</h4>
<p>Let's consider a situation where we have many resources that share some dependencies. </p>
<div><pre><span></span><code><span>class</span> <span>UserClient</span><span>:</span>
    <span>def</span> <span>__init__</span><span>(</span><span>self</span><span>,</span> <span>db</span><span>:</span> <span>Engine</span><span>,</span> <span>user_service</span><span>:</span> <span>APIClient</span><span>)</span> <span>-&gt;</span> <span>None</span><span>:</span>
        <span>...</span>


<span>class</span> <span>ProjectClient</span><span>:</span>
    <span>def</span> <span>__init__</span><span>(</span><span>self</span><span>,</span> <span>db</span><span>:</span> <span>Engine</span><span>,</span> <span>user_service</span><span>:</span> <span>APIClient</span><span>,</span> <span>project_service</span><span>:</span> <span>APIClient</span><span>)</span> <span>-&gt;</span> <span>None</span><span>:</span>
        <span>...</span>
</code></pre></div>

<p>We want a way to create all the dependencies in one place and pass in the dependencies.</p>
<p>Essentially we need something that is the union of all kwargs of the resources. That suddernly sounds a lot like a TypedDict:</p>
<div><pre><span></span><code><span>class</span> <span>Dependencies</span><span>(</span><span>TypedDict</span><span>):</span>
    <span>db</span><span>:</span> <span>Engine</span>
    <span>user_service</span><span>:</span> <span>APIClient</span>
    <span>project_service</span><span>:</span> <span>APIClient</span>


<span>def</span> <span>create_deps</span><span>(</span><span>...</span><span>)</span> <span>-&gt;</span> <span>Dependencies</span><span>:</span>
    <span>...</span>
</code></pre></div>

<p>Unfortunately this won't work since <code>UserClient</code> can't take <code>project_service</code> as a kwarg.</p>
<p>To fix this, we need to rewrite the resources such that we accept arbitrary arguments.</p>
<div><pre><span></span><code><span>class</span> <span>UserClient</span><span>:</span>
    <span>def</span> <span>__init__</span><span>(</span><span>self</span><span>,</span> <span>...</span><span>,</span> <span>**</span><span>_</span><span>)</span> <span>-&gt;</span> <span>None</span><span>:</span>
        <span>...</span>
<span>...</span>
</code></pre></div>

<p>And then we can do the injection like this:</p>
<div><pre><span></span><code><span>class</span> <span>ResourceWithMissing</span><span>:</span>
    <span>def</span> <span>__init__</span><span>(</span><span>self</span><span>,</span> <span>other</span><span>:</span> <span>Any</span><span>,</span> <span>**</span><span>_</span><span>)</span> <span>-&gt;</span> <span>None</span><span>:</span>
        <span>...</span>


<span>def</span> <span>inject</span><span>(</span><span>deps</span><span>:</span> <span>Dependencies</span><span>):</span>
    <span>UserClient</span><span>(</span><span>**</span><span>deps</span><span>)</span>  <span># ✅</span>
    <span>ProjectClient</span><span>(</span><span>**</span><span>deps</span><span>)</span>  <span># ✅</span>
    <span>ResourceWithMissing</span><span>(</span><span>**</span><span>deps</span><span>)</span>  <span># ❌</span>
    <span>...</span>


<span>inject</span><span>(</span><span>create_deps</span><span>(</span><span>...</span><span>))</span>
</code></pre></div>

<p>With the solution complete, we can now rely on the type system to check the dependency injection to see if any arguments are incorrect or missing. </p>
<p>I will admit that changing resource signature with <code>**_</code> is not ideal, but this is a smaller change than most dependency injection frameworks. And we get static type checking which a lot of the frameworks won't support.</p>
<h3>Upcoming Features</h3>
<p><a href="https://peps.python.org/pep-0728/">PEP-728</a> will allow types of extra items to be defined, and a typed dict to be closed meaning no extra items can be defined.</p>
<p>This new change looks like it'll help us define record types more precisely.</p>
<p>I personally haven't thought of many other use cases for it, but as I've demonstrated above it's always worth reading through the PEP and experimenting with the new change. </p>
<p><a href="https://peps.python.org/pep-0705/">PEP-705</a> might already be out by the time you read this. This will allow for read only items to be specified.</p>
<p>This is primarily intended for situations where different typed dicts intuitively should be compatible but potential mutations (deletions) can create problems.</p>
      </div><!-- /.entry-content -->

    </article>
  </section><div id="contentinfo">
                        <address id="about">
                                Proudly powered by <a rel="nofollow" href="https://getpelican.com/">Pelican</a>, which takes great advantage of <a rel="nofollow" href="https://www.python.org/">Python</a>.
                        </address><!-- /#about -->

                        <p>The theme is by <a rel="nofollow" href="https://www.smashingmagazine.com/2009/08/designing-a-html-5-layout-from-scratch/">Smashing Magazine</a>, thanks!</p>
                </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Show HN: HTML for People (471 pts)]]></title>
            <link>https://htmlforpeople.com</link>
            <guid>41801334</guid>
            <pubDate>Thu, 10 Oct 2024 17:47:42 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://htmlforpeople.com">https://htmlforpeople.com</a>, See on <a href="https://news.ycombinator.com/item?id=41801334">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="main-content">
      
  
<p>HTML isn’t only for people working in the tech field. It’s for anybody, the way documents are for anybody. HTML is just another type of document. A very special one—the one the web is built on.</p>
<p>I’m <a href="https://blakewatson.com/">Blake Watson</a>. I’ve been building websites since the early 2000s. Though I work professionally in the field, I feel strongly that <em>anyone</em> should be able to make a website with HTML if they want. This book will teach you how to do just that. It doesn’t require any previous experience making websites or coding. I will cover everything you need to know to get started in an approachable and friendly way.</p>
<p>Ready? Let’s do it!</p>


  <p>
    <a href="https://htmlforpeople.com/intro">Read the introduction</a>
    <a href="https://htmlforpeople.com/zero-to-internet-your-first-website">Start coding already!</a>
  </p>

    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Microsoft Recall is now an explorer.exe dependency (202 pts)]]></title>
            <link>https://github.com/ChrisTitusTech/winutil/issues/2697</link>
            <guid>41801331</guid>
            <pubDate>Thu, 10 Oct 2024 17:47:37 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/ChrisTitusTech/winutil/issues/2697">https://github.com/ChrisTitusTech/winutil/issues/2697</a>, See on <a href="https://news.ycombinator.com/item?id=41801331">Hacker News</a></p>
<div id="readability-page-1" class="page"><div disabled="" sortable="">
          <p dir="auto">There are 3 settings you can use to practically disable copilot/recall completely. These will do the following things<br>
1: Remove the package so it doesn't even appear in search or app lists<br>
2: Remove the package so it doesn't activate if you hit the new copilot key on keyboards<br>
3: Remove the package so it doesn't even show up as an app that was installed in apps and features</p>
<p dir="auto">In group policy there are 2 settings</p>
<p dir="auto">"turn off windows copilot"<br>
(UserConfig\AdministrativeTemplates\WindowsComponents\Windows Copilot)</p>
<p dir="auto">"turn off saving snapshots to windows"<br>
(UserConfig\AdministrativeTemplates\WindowsComponents\Windows Ai)</p>
<p dir="auto">These 2 group policy options enter 2 registry keys located here</p>
<p dir="auto">HKEY_LOCAL_MACHINE\SOFTWARE\Policies\Microsoft\Windows\WindowsCopilot<br>
HKEY_CURRENT_USER\Software\Policies\Microsoft\Windows\WindowsCopilot<br>
"TurnOffWindowsCopilot"<br>
Dword (1)</p>
<p dir="auto">HKEY_CURRENT_USER\Software\Policies\Microsoft\Windows\WindowsAI<br>
HKEY_LOCAL_MACHINE\SOFTWARE\Policies\Microsoft\Windows\WindowsCopilot<br>
"DisableAIDataAnalysis"<br>
Dword (1)</p>
<p dir="auto">If you also use an app called "AppXPackagesManager"<br>
<a href="https://github.com/Savitarax/File-Resources">https://github.com/Savitarax/File-Resources</a><br>
(I am NOT the original creator of said app, I am merely providing the software to use)</p>
<p dir="auto">Copilot is an option that you can remove listed in user packages</p>
<p dir="auto">After doing these steps.<br>
I can't get copilot to activate in any way, nor can I find recall.</p>
<p dir="auto">Figured i'd list this to anyone that was looking for a pretty solid solution if they were looking for the template to do so.</p>
      </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Studios: Please Don't Spoil the Movie We Are Seated to See (228 pts)]]></title>
            <link>http://fxrant.blogspot.com/2024/06/studios-dont-spoil-movie-we-are-seated.html</link>
            <guid>41801300</guid>
            <pubDate>Thu, 10 Oct 2024 17:44:55 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="http://fxrant.blogspot.com/2024/06/studios-dont-spoil-movie-we-are-seated.html">http://fxrant.blogspot.com/2024/06/studios-dont-spoil-movie-we-are-seated.html</a>, See on <a href="https://news.ycombinator.com/item?id=41801300">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="post-body-8595952586617710821" itemprop="description articleBody">
<p><a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhXlFhEI0V8S6K4imrSag-M5PNYBCJIV-ze_r-r6R06QUqdyY49YhxiO79peJKdwjV8mH06lM5R-JFgSU57mfctbt1eyM52NbW9L0F95gZqMaq8RIsQInxb7Jrkf8wi5V3Zw7TRod8OsP3I72jsP5ZgwGzaNAWfdjERvIgFLBtSSPvfvP9Aq5U/s720/alien-1979.jpg.webp" imageanchor="1"><img data-original-height="480" data-original-width="720" height="265" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhXlFhEI0V8S6K4imrSag-M5PNYBCJIV-ze_r-r6R06QUqdyY49YhxiO79peJKdwjV8mH06lM5R-JFgSU57mfctbt1eyM52NbW9L0F95gZqMaq8RIsQInxb7Jrkf8wi5V3Zw7TRod8OsP3I72jsP5ZgwGzaNAWfdjERvIgFLBtSSPvfvP9Aq5U/w400-h265/alien-1979.jpg.webp" width="400"></a></p><p>I tweeted this incredibly non-controversial take and it got a huge reaction, so I thought I'd recycle the content for a blog post. Enjoy.</p><p>We took our kid to see "Alien" (1979) on the big screen during its one-week-only theatrical run. We told him there was a good chance of a pre-show featurette that would spoil the movie, so he needed to be ready to cover his eyes.</p><p>Well, that's exactly what happened.</p><p>My kid threw his hoodie over his eyes while a pre-show interview between Fede Alvarez and Ridley Scott appeared, featuring tons of behind-the-scenes photos of the alien, the chestburster scene, and discussion of the legacy of the classic film.</p><p>Why do this before the movie!??!</p><p>If even one person in that theater hadn't seen the film yet, it puts a huge damper on the surprise and delight that the movie would bring them, which is sad. We WANT to bring first-timers to theaters to see classic movies. Don't ruin it for them.</p><p><b>Play that shit AFTER the movie.</b></p><p>This has happened with several re-releases for me. Fathom did this to "Star Trek II: The Wrath of Khan" (pre-show highlighted a main character's death!) and "Close Encounters" (pre-show showed the effing aliens!). And now "Alien" (Disney/Fox).</p><p>The solution is simple: preserve the wonder for first-timers by putting these featurettes AFTER the movie. Tease it before the feature.</p><p>Anyway, <a href="https://www.fathomevents.com/events/close-encounters-of-the-third-kind-2024-re-release/">"Close Encounters of the Third Kind" is coming to theaters again this summer </a>(Fathom), so get ready to cover the eyes of first-timers before the show.</p><p><a href="https://x.com/tvaziri/status/1796955123337372032">Original tweet thread.</a></p>

</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Game Programming in Prolog (226 pts)]]></title>
            <link>https://thingspool.net/morsels/page-10.html</link>
            <guid>41800764</guid>
            <pubDate>Thu, 10 Oct 2024 16:50:40 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://thingspool.net/morsels/page-10.html">https://thingspool.net/morsels/page-10.html</a>, See on <a href="https://news.ycombinator.com/item?id=41800764">Hacker News</a></p>
<div id="readability-page-1" class="page">

<a href="https://thingspool.net/morsels/list.html">Back to List</a>

<h3>Author: Youngjin Kang</h3>
<h3>Date: August 25, 2024</h3>

<hr>

<h3><b>Introduction</b></h3>
<p>As a fan of unconventional programming paradigms, I enjoy learning new programming languages which are drastically different from the typical object-oriented ones such as C#, Java, and the like. The most iconic of them are LISP (which is a powerful language for both functional programming as well as metalinguistic patterns in software development) and Prolog (which is one of the most popular languages in logic programming). Learning these languages is quite hard, compared to being acquainted with usual C-style imperative languages such as Ruby and Python, yet it has turned out to be one of the most effective ways of exercising one's brain.</p>
<p>By the time I started learning LISP via MIT's 1986 lecture series called "SICP (Structure and Interpretation of Computer Programs)" back in 2018, I was already quite familiar with some of its core concepts (such as lambda expressions, higher-order functions, etc) because they were already integrated as some of the main features of C#, which was the language I was using all the time as a Unity game developer. Also, my academic background in electrical engineering (signal processing in particular) helped me easily grasp the idea of "stream processing" which appeared in the latter half of the lecture series. Thus, learning LISP and its functional design patterns was not as difficult as I imagined it to be.</p>
<p>A major intellectual challenge, however, struck me when I began to study Prolog - the famous logic programming language which is notorious for its esoteric syntax. The grammar itself did not appear to be complicated at all; it was just as minimal as that of LISP. The way in which programming had to be done in Prolog, though, was stressful enough to fry the engine of my brain. The way it approached data structures (such as lists) and algorithms based upon mathematical relations was something so revolutionarily novel to me, that it seriously opened up a new horizon in my faculty of computational reasoning.</p>
<p>While Prolog's approach in software development was quite alien to me, I managed to notice a number of familiar associations between Prolog and many useful topics in engineering. I discovered, for example, that the so-called "relational databases" (e.g. MySQL) are named so not because they comprise data tables which are related to each other via references, but because each row of a data table can be considered an n-ary predicate (where 'n' is the number of columns in the table) in Prolog's syntax. Besides, I found out that the input/output behavior of each digital circuit component (e.g. logic gate) could be implemented as an n-ary relation (where 'n' is the total number of the input/output ports combined), implying that an "object", whether it be a piece of hardware or a piece of pure data in memory, may as well be defined as a relation in logic programming (just like an object may as well be defined as a function in functional programming). Furthermore, the declarative nature of Prolog strongly convinced me that it must be optimal for data-driven design.</p>
<p>These realizations soon led me to contemplate upon the notion that, maybe, logic programming has a great deal of potential in the design and implementation of highly complex systems, such as a video game's core gameplay mechanics. I began to ask myself, "Will it be possible to develop an entire game using the grammar of logic programming?"</p>
<p>Indeed, there are reasons why most game developers just stick to general-purpose programming languages (such as C#) for making games, aside from purely experimental purposes. Implementing an entire game based on Prolog, for instance, is perhaps too much of a challenge for those who are not hardcore mathematicians. Also, Prolog may not be the best language to use for parts of the project which are not necessarily made of a complex web of relations, such as simple I/O modules, graphics modules, audio modules, physics modules, and the like.</p>
<p>However, I believe that at least the core mechanics of a game can definitely be implemented using the language of Prolog, and that we will be able to solve a plethora of complex design problems by doing so. It is because a gameplay system which is structured in terms of a set of declarative statements will be far more robust, modular, and free of confusing edge cases (e.g. race conditions) than an imperative system.</p>
<p>For this alternative methodology to be successful, one must start by designing the system in terms of logical relations/predicates only, and nothing else (That is, no functions, no structs, no classes, no interfaces, no state variables, etc). This will allow us to construct a gameplay system which is purely driven by the soul of Prolog.</p>
<hr>

<h3><b>World and Actors</b></h3>
<p>The core idea in Prolog-based game programming is to utilize relations as the most primitive building blocks of the system, just like basic circuit components (e.g. resistors, transistors, capacitors, inductors, etc) are the most primitive building blocks of an electric circuit. It is sensible, therefore, to start this journey by considering the most rudimentary relations (e.g. unary and binary) first, and see if these elements can serve as the most essential nuts and bolts of the game.</p>
<img src="https://thingspool.net/morsels/e01.jpg" alt="Game Programming in Prolog - Part 1 (Figure 1)">
<p>Suppose that we are designing a game, and that the game consists of two major parts - world and actors (see the image above). The world is a scene in which everything is supposed to happen, and actors are objects which belong to the world. Examples of actors include "players", "enemies", "obstacles", "items", and pretty much any discrete entities which have their own names and attributes. Actors are able to interact with each other (as well as with themselves), from which various events occur. What we refer to as "gameplay" is a chain of such events.</p>
<p>We will begin formulating a gameplay system based off of this conceptual backbone. All you need to remember is that there is a world, and that the world contains a number of actors, each of which possesses its own state and behavior.</p>
<hr>

<h3><b>Tags</b></h3>
<p>First of all, let us identify each individual actor with a unique name. If there are two actors in the world, for instance, we will simply assume that the name "actor1" and "actor2" will be used to indicate the first and second actors, respectively.</p>
<img src="https://thingspool.net/morsels/e02.jpg" alt="Game Programming in Prolog - Part 1 (Figure 2)">
<p>The first piece of logic I will illustrate is the idea of tags. A tag is a keyword which, when attached to an actor, describes what the actor stands for. When an actor has the tag "bread" attached to it, for example, we should be able to tell that the actor is a piece of bread.</p>
<p>The Prolog code below assigns the tag "bread" to both actor1 and actor2, in the form of unary predicates (The tag "bread" itself is an unary relation, and "bread(actor1)" &amp; "bread(actor2)" are two separate instances of it). This implies that both actor1 and actor2 are pieces of bread.</p>
<div><pre><code>bread(actor1).
bread(actor2).</code></pre></div>
<img src="https://thingspool.net/morsels/e03.jpg" alt="Game Programming in Prolog - Part 1 (Figure 3)">
<p>An actor can have multiple tags as well. However, one may feel that it is a bit too tedious to manually assign a bunch of tags to each individual actor. For example, let us say that every piece of bread must also be labeled as flammable and decomposable. This means that, whenever an actor is associated with the tag "bread", we are obliged to always ensure that it is also associated with the tag "flammable" and "decomposable". Manually attaching these two additional tags to every "bread" actor is way too cumbersome and error-prone. Fortunately, the following pair of horn clauses neatly solve this problem. They enforce the following two rules:</p>
<p>(1) Whenever tag "bread" is assigned to actor X, tag "flammable" will automatically be assigned to actor X.<br>(2) Whenever tag "bread" is assigned to actor X, tag "decomposable" will automatically be assigned to actor X.</p>
<div><pre><code>flammable(X) :- bread(X).
decomposable(X) :- bread(X).</code></pre></div>
<img src="https://thingspool.net/morsels/e04.jpg" alt="Game Programming in Prolog - Part 1 (Figure 4)">
<p>These horn clauses, therefore, serve as part of the game's "config data" - a list of data entries in the game's technical design document (like the ones you would see on a spreadsheet) telling us the characteristics of each individual character type, skill type, mission type, and so forth. The tags called "flammable" and "decomposable" in our case, for instance, are characteristics which belong to the type-specifier called "bread", meaning that any actor which can be identified as "bread" is a composition of two properties called "flammable" and "decomposable".</p>
<p>A decent analogy can be found in Unity game engine, where we may create a prefab called "Bread" with two components in it - "Flammable" and "Decomposable". Or, in a general object-oriented programming environment, "Bread" may stand for the name of a class which implements two interfaces called "IFlammable" and "IDecomposable".</p>
<p>In a way, therefore, horn clauses in Prolog play the role of data type definitions.</p>
<img src="https://thingspool.net/morsels/e05.jpg" alt="Game Programming in Prolog - Part 1 (Figure 5)">
<p>Aside from these pre-configured tags (which all rely on the presence of the tag "bread"), one may as well attach a custom tag to an actor as needed. For example, imagine that a wizard happened to enchant actor2 (i.e. the second piece of bread). This means that, unlike actor1 which is an ordinary piece of bread, actor2 must be an "enchanted" piece of bread which is required to have the tag "enchanted" attached to it for the purpose of showing us that it has been enchanted. The code below ensures that this is the case.</p>

<img src="https://thingspool.net/morsels/e06.jpg" alt="Game Programming in Prolog - Part 1 (Figure 6)">
<p>The tags "flammable" and "decomposable" are characteristics of all pieces of bread, whereas the tag "enchanted" is a characteristic of only special pieces of bread which have been enchanted by a wizard.</p>
<hr>

<h3><b>Relationships</b></h3>
<p>So far, we have been using tags for specifying the characteristics of each individual actor. In a gameplay system, however, we also need to specify relationships between actors, such as ways in which they interact, etc.</p>
<p>In an ecosystem, predators chase preys and preys run from predators. In a dating simulator, a guy tries to flirt with girls and girls reject him. In a social simulator (such as The Sims), people are either friends or enemies of each other, or somewhere in between. In the game of chess, a bishop devours a rook diagonally and a rook devours a bishop orthogonally. These are all relationships out of which the game's dynamics emerge.</p>
<p>Defining actor-to-actor relationships in Prolog is pretty straightforward. Just like an unary predicate can be used to characterize a single actor, a binary predicate can be used to characterize a relationship between a pair of actors. And by means of a horn clause, such a relationship can be dynamically deduced from a set of requisite conditions.</p>
<p>The following code is an example of a relationship. Suppose that there is a third actor called "actor3", and that we have declared it as a human (by attaching the tag "human" to it). Since a human is able to eat a piece of bread, we can confidently assert that "X can eat Y if X is a human and Y is a piece of bread". Here, "X can eat Y" is a relationship which holds whenever X is associated with tag "human" and Y is associated with tag "bread".</p>
<div><pre><code>human(actor3).
canEat(X, Y) :- human(X), bread(Y).</code></pre></div>
<img src="https://thingspool.net/morsels/e07.jpg" alt="Game Programming in Prolog - Part 1 (Figure 7)">
<p>Here is another example. Since a piece of bread is decomposable (because anything which is identified as "bread" must also be identified as "decomposable"), we know that microbes such as fungi are capable of spoiling it. If there is an actor with the tag "fungus" attached to it, therefore, we will be able to tell that it must be able to spoil any other actor which is "decomposable". This is yet another case of a relationship between two types of actors; it is a relationship which says, "X can spoil Y if X is a fungus and Y is decomposable". The following code shows its definition.</p>
<div><pre><code>fungus(actor4).
canSpoil(X, Y) :- fungus(X), decomposable(Y).</code></pre></div>
<img src="https://thingspool.net/morsels/e08.jpg" alt="Game Programming in Prolog - Part 1 (Figure 8)">
<p>There is something still missing here, though. While I have demonstrated that it is possible to assign characteristics to individual actors as well as their mutual connections (i.e. relationships), I have not shown yet how to make these characteristics change over time. They all have been static so far, and the declarative nature of Prolog does not seem to offer an easy solution to make things dynamic.</p>
<p>If we want to create a game rather than a fixed landscape of how things are shaped permanently, we better let them move and interact as time goes by. In the next part of the series, I will explain how the game loop shall be conceptualized in Prolog.</p>
<p>(Will be continued in <a href="https://thingspool.net/morsels/page-11.html">Part 2</a>)</p>


</div>]]></description>
        </item>
        <item>
            <title><![CDATA[DeskPad – A virtual monitor for screen sharing (927 pts)]]></title>
            <link>https://github.com/Stengo/DeskPad</link>
            <guid>41800602</guid>
            <pubDate>Thu, 10 Oct 2024 16:36:54 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/Stengo/DeskPad">https://github.com/Stengo/DeskPad</a>, See on <a href="https://news.ycombinator.com/item?id=41800602">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><div dir="auto"><h3 tabindex="-1" dir="auto">
  <a href="https://github.com/Stengo/DeskPad/blob/main/DeskPad/Assets.xcassets/AppIcon.appiconset/Icon-256.png">
  <img src="https://github.com/Stengo/DeskPad/raw/main/DeskPad/Assets.xcassets/AppIcon.appiconset/Icon-256.png?raw=true" alt="DeskPad Icon" width="128">
  </a>
</h3><a id="user-content-------" aria-label="Permalink: " href="#------"></a></div>
<p dir="auto"><h2 tabindex="-1" dir="auto">DeskPad</h2><a id="user-content-deskpad" aria-label="Permalink: DeskPad" href="#deskpad"></a></p>
<p dir="auto">A virtual monitor for screen sharing</p>
<div dir="auto"><h3 tabindex="-1" dir="auto">
  <a href="https://github.com/Stengo/DeskPad/blob/main/screenshot.jpg">
  <img src="https://github.com/Stengo/DeskPad/raw/main/screenshot.jpg?raw=true" alt="DeskPad Screenshot">
  </a>
</h3><a id="user-content--------1" aria-label="Permalink: " href="#-------1"></a></div>
<p dir="auto">Certain workflows require sharing the entire screen (usually due to switching through multiple applications), but if the presenter has a much larger display than the audience it can be hard to see what is happening.</p>
<p dir="auto">DeskPad creates a virtual display that is mirrored within its application window so that you can create a dedicated, easily shareable workspace.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Installation</h2><a id="user-content-installation" aria-label="Permalink: Installation" href="#installation"></a></p>
<p dir="auto">You can either download the <a href="https://github.com/Stengo/DeskPad/releases">latest release binary</a> or install via <a href="https://brew.sh/" rel="nofollow">Homebrew</a> by calling <code>brew install deskpad</code>.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Usage</h2><a id="user-content-usage" aria-label="Permalink: Usage" href="#usage"></a></p>
<p dir="auto">DeskPad behaves like any other display. Launching the app is equivalent to plugging in a monitor, so macOS will take care of properly arranging your windows to their previous configuration.</p>
<p dir="auto">You can change the display resolution through the system preferences and the application window will adjust accordingly.</p>
<p dir="auto">Whenever you move your mouse cursor to the virtual display, DeskPad will highlight its title bar in blue and move the application window to the front to let you know where you are.</p>
<div dir="auto"><h3 tabindex="-1" dir="auto">
  <a href="https://github.com/Stengo/DeskPad/blob/main/demonstration.gif">
  <img src="https://github.com/Stengo/DeskPad/raw/main/demonstration.gif?raw=true" alt="DeskPad Demonstration" data-animated-image="">
  </a>
</h3><a id="user-content--------2" aria-label="Permalink: " href="#-------2"></a></div>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[The science behind on-the-wrist blood pressure tracking (129 pts)]]></title>
            <link>https://www.empirical.health/blog/apple-watch-blood-pressure/</link>
            <guid>41799324</guid>
            <pubDate>Thu, 10 Oct 2024 14:40:43 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.empirical.health/blog/apple-watch-blood-pressure/">https://www.empirical.health/blog/apple-watch-blood-pressure/</a>, See on <a href="https://news.ycombinator.com/item?id=41799324">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>   <p>There are many <a href="https://www.bloomberg.com/news/articles/2023-11-01/apple-plans-hypertension-sleep-apnea-detection-for-next-watch">rumors</a> that an upcoming Apple Watch will measure blood pressure; similar features exist on Samsung watches internationally and are likely to come to the US once cleared by the FDA.</p>
<p>In 2018, I helped run one of the <a href="https://ojs.aaai.org/index.php/AAAI/article/view/11891">first studies</a> to show that health sensor data from wearables, when combined with a deep neural network, can pick up on signs of high blood pressure, sleep apnea, atrial fibrillation, and more. While the sensors have advanced in the last 7 years, the underlying science remains the same—and offers clues to the future.</p>
<p>In this post, I’ll try to explain the science behind blood pressure on the wrist (e.g., pulse wave velocity), past medical literature on using deep neural networks to glean signal from consumer wearables, likely limitations of wrist-based blood pressure, and how doctors and patients can incorporate it into medical practice.</p>
<h2 id="pulse-wave-analysis-for-blood-pressure">Pulse Wave Analysis for blood pressure</h2>
<p>When your heart beats, it sends a pressure wave—your pulse—throughout your body. As far back as antiquity, doctors could roughly sense blood pressure by pressing their finger against an artery. The first machine for measuring blood pressure, the sphygmograph, was invented in 1854. The modern modern blood pressure cuff, or sphygmomanometer, is its descendant.</p>
<p><img src="https://www.empirical.health/_astro/sphygmograph.CeSoVw_i_lHfGr.webp" alt="" width="1456" height="682" loading="lazy" decoding="async"></p>
<p>Sphygmomanometers require actively pressing the artery to measure pressure.</p>
<p>But what if you can’t apply force to the artery? On a watch, you only have an optical sensors—which are nearly touchless. But there are still clues that let you infer blood pressure from <em>speed</em> and <em>shape</em> of each pulse wave.</p>
<p><em>Pulse Wave Velocity (PWV)</em> is the speed at which the pulse propagates through the circulatory system. Similar to a string being pulled taught, when your blood pressure is higher, the wave travels faster.</p>
<p>If the Watch measures the precise times when (a) the heart contracts and (b) the pulse wave arrives at the wrist, then the difference tells you how your blood pressure is changing throughout the day. You can measure the exact time of the heart beat through an ECG (which Apple Watches, Samsungs, Fitbits, etc already have) or through the body’s mechanical response to the blood ejected from the heart during each heartbeat (this is called a ballistocardiogram).</p>
<p><img src="https://www.empirical.health/_astro/pulse_wave.C5LxX3ml_ZtGI4N.webp" alt="" width="850" height="510" loading="lazy" decoding="async"></p>
<p>Second, the shape of the wave offers indirect clues to blood pressure. The <em>pulse rise time</em> and the various wave amplitudes (pulse wave amplitude, pulse wave systolic peak, etc) correlate with blood pressure. All else being equal, a higher amplitude and a faster rise time correlate with more arterial stiffness and higher blood pressure.</p>
<h2 id="apple-watch-blood-pressure-would-need-to-be-calibrated-using-a-cuff">Apple Watch blood pressure would need to be calibrated using a cuff</h2>
<p>Over time, your blood pressure can drift. And every person’s parameters are a bit different. So wrist-based blood pressure needs to be calibrated initially by comparing them to a cuff, and then periodically re-calibrated (say, every 30 days).</p>
<h2 id="why-apple-watchs-future-blood-pressure-sensor-matters-for-health">Why Apple Watch’s future blood pressure sensor matters for health</h2>
<p>So why does this matter? Isn’t taking blood pressure part of the typical doctor visit?</p>
<h2 id="120-million-people-in-the-us-are-diagnosed-with-high-blood-pressure-but-only-23-have-it-under-control">120 million people in the US are diagnosed with high blood pressure, but only 23% have it under control.</h2>
<p>That means 77% of patients don’t have their blood pressure under control. These figures are from HHS, the American College of Cardiology, and the American Heart association. Blood pressure control often requires both stacking multiple medications and dietary changes that are challenging to achieve.</p>
<h2 id="high-blood-pressure-causes-heart-attacks">High blood pressure causes heart attacks</h2>
<p><img src="https://www.empirical.health/_astro/blood_pressure_heart_attacks.D8pRrgAw_ZiT2AK.webp" alt="" width="679" height="360" loading="lazy" decoding="async"></p>
<p>Alongside cholesterol or apoB, high blood pressure is one of main risk factors for a heart attack.</p>
<p>Every 20 mmHg increase in your systolic blood pressure, or 10 mmgHg increase in diastolic blood pressure, doubles your mortality.</p>
<p>To make it concrete — let’s say you’re a 40 year old man with good cholesterol numbers (160 total cholesterol, 60 HDL). If your blood pressure is 115/75 (good), your lifetime risk of a heart attack is only 5%. If your blood pressure is 150/110 (high), your lifetime risk is 10x higher — 50% (<a href="https://tools.acc.org/ldl/ascvd_risk_estimator/index.html#!/calulate/estimator/">source</a>).</p>
<h2 id="blood-pressure-is-sensitive-to-daily-counterintuitive-nutritional-choices">Blood pressure is sensitive to daily, counterintuitive nutritional choices</h2>
<p>People with high blood pressure are often recommended the DASH diet, where you try to eat <a href="https://www.heart.org/en/health-topics/high-blood-pressure/changes-you-can-make-to-manage-high-blood-pressure/how-potassium-can-help-control-high-blood-pressure">3,500 to 5,000 mg of potassium</a> per day and <a href="https://www.heart.org/en/health-topics/high-blood-pressure/changes-you-can-make-to-manage-high-blood-pressure/shaking-the-salt-habit-to-lower-high-blood-pressure">under 1,500 mg of sodium</a>. It’s surprisingly difficult to control your dietary potassium and sodium, however, since the set of foods are counterintuitive.</p>
<p>Hint: bananas don’t even make the top 10 of high-potassium foods. The set of high-potassium foods is very heterogenous—spinach, avocado, sweet potato, and white beans are all among the top 10. Sodium is even more insidious — much of our consumption comes from things like bread, which don’t necessarily taste salty.</p>
<p>Continuous glucose monitors have revolutionized treatment for those with diabetes. Could a continuous blood pressure monitor give similar, real-time feedback on dietary choices? With the benefit of AI, you can imagine that everybody would have a virtual nutritionalist.</p>
<h2 id="whats-next">What’s next?</h2>
<p>As of this writing (September 2024), blood pressure on the wrist is all just rumors. We’ll update this post as the situation changes.</p>
<p><img src="https://www.empirical.health/_astro/empirical_hypertension.Z_5qMrR7_Z2tHXLJ.webp" alt="" width="1920" height="1080" loading="lazy" decoding="async"></p>
<p>And if you own an external blood pressure cuff and want to manage your hypertension, try out
<a href="https://apps.apple.com/us/app/empirical-health-for-watch/id6449271489">Empirical Health on the App Store</a> today.</p>  </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[AAA Gaming on Asahi Linux (753 pts)]]></title>
            <link>https://rosenzweig.io/blog/aaa-gaming-on-m1.html</link>
            <guid>41799068</guid>
            <pubDate>Thu, 10 Oct 2024 14:16:28 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://rosenzweig.io/blog/aaa-gaming-on-m1.html">https://rosenzweig.io/blog/aaa-gaming-on-m1.html</a>, See on <a href="https://news.ycombinator.com/item?id=41799068">Hacker News</a></p>
<div id="readability-page-1" class="page"> <header><p>10 Oct 2024</p></header><p>Gaming on Linux on M1 is here! We’re thrilled to release our Asahi game playing toolkit, which integrates our Vulkan 1.3 drivers with x86 emulation and Windows compatibility. Plus a bonus: conformant OpenCL 3.0.</p> <p>Asahi Linux now ships the only conformant <a href="https://www.khronos.org/conformance/adopters/conformant-products/opengl#submission_3470">OpenGL®</a>,<!--
[OpenGL® ES](https://www.khronos.org/conformance/adopters/conformant-products/opengles#submission_1045),--> <a href="https://www.khronos.org/conformance/adopters/conformant-products/opencl#submission_433">OpenCL™</a>, and <a href="https://www.khronos.org/conformance/adopters/conformant-products#submission_7910">Vulkan®</a> drivers for this hardware. As for gaming… while today’s release is an alpha, <a href="https://store.steampowered.com/app/870780/Control_Ultimate_Edition/"><strong>Control</strong></a> runs well!</p> <figure> <a href="https://rosenzweig.io/blog/Games-Asahi/Control-small.png"><img src="https://rosenzweig.io/blog/Games-Asahi/Control-small.avif" alt="Control"></a> </figure> <h2 id="installation">Installation</h2> <p>First, install <a href="https://asahilinux.org/fedora/">Fedora Asahi Remix</a>. Once installed, get the latest drivers with <code>dnf upgrade --refresh &amp;&amp; reboot</code>. Then just <code>dnf install steam</code> and play. While all M1/M2-series systems work, most games require 16GB of memory due to emulation overhead.</p> <h2 id="the-stack">The stack</h2> <p>Games are typically x86 Windows binaries rendering with DirectX, while our target is Arm Linux with Vulkan. We need to handle each difference:</p> <ul> <li><a href="https://fex-emu.com/">FEX</a> emulates x86 on Arm.</li> <li><a href="https://www.winehq.org/">Wine</a> translates Windows to Linux.</li> <li><a href="https://github.com/doitsujin/dxvk">DXVK</a> and <a href="https://github.com/HansKristian-Work/vkd3d-proton">vkd3d-proton</a> translate DirectX to Vulkan.</li> </ul> <p>There’s one curveball: page size. Operating systems allocate memory in fixed size “pages”. If an application expects smaller pages than the system uses, they will break due to insufficient alignment of allocations. That’s a problem: x86 expects 4K pages but Apple systems use 16K pages.</p> <p>While Linux can’t mix page sizes between processes, it <em>can</em> virtualize another Arm Linux kernel with a different page size. So we run games inside a tiny virtual machine using <a href="https://github.com/AsahiLinux/muvm">muvm</a>, passing through devices like the GPU and game controllers. The hardware is happy because the system is 16K, the game is happy because the virtual machine is 4K, and you’re happy because you can play <a href="https://store.steampowered.com/app/377160/Fallout_4/"><strong>Fallout 4</strong></a>.</p> <figure> <a href="https://rosenzweig.io/blog/Games-Asahi/Fallout4-small.png"><img src="https://rosenzweig.io/blog/Games-Asahi/Fallout4-small.avif" alt="Fallout 4"></a> </figure> <h2 id="vulkan">Vulkan</h2> <p>The final piece is an adult-level Vulkan driver, since translating DirectX requires Vulkan 1.3 with many extensions. Back in April, I wrote <a href="https://rosenzweig.io/blog/vk13-on-the-m1-in-1-month.html">Honeykrisp</a>, the only Vulkan 1.3 driver for Apple hardware. I’ve since added DXVK support. Let’s look at some new features.</p> <h3 id="tessellation">Tessellation</h3> <p>Tessellation enables games like <a href="https://store.steampowered.com/app/292030/The_Witcher_3_Wild_Hunt/"><strong>The Witcher 3</strong></a> to generate geometry. The M1 has hardware tessellation, but it is too limited for DirectX, Vulkan, or OpenGL. We must instead tessellate with arcane compute shaders, as detailed in <a href="https://www.youtube.com/live/pDsksRBLXPk">today’s talk at XDC2024</a>.</p> <figure> <a href="https://rosenzweig.io/blog/Games-Asahi/Witcher3-small.png"><img src="https://rosenzweig.io/blog/Games-Asahi/Witcher3-small.avif" alt="The Witcher 3"></a> </figure> <h3 id="geometry-shaders">Geometry shaders</h3> <p>Geometry shaders are an older, cruder method to generate geometry. Like tessellation, the M1 lacks geometry shader hardware so we emulate with compute. Is that fast? No, but geometry shaders are slow <a href="http://www.joshbarczak.com/blog/?p=667">even on desktop GPUs</a>. They don’t need to be fast – just fast enough for games like <a href="https://store.steampowered.com/app/1139900/Ghostrunner/"><strong>Ghostrunner</strong></a>.</p> <figure> <a href="https://rosenzweig.io/blog/Games-Asahi/Ghostrunner-small.png"><img src="https://rosenzweig.io/blog/Games-Asahi/Ghostrunner-small.avif" alt="Ghostrunner"></a> </figure> <h3 id="enhanced-robustness">Enhanced robustness</h3> <p>“Robustness” permits an application’s shaders to access buffers out-of-bounds without crashing the hardware. In OpenGL and Vulkan, out-of-bounds loads may return arbitrary elements, and out-of-bounds stores may corrupt the buffer. Our OpenGL driver <a href="https://rosenzweig.io/blog/conformant-gl46-on-the-m1.html">exploits this definition</a> for efficient robustness on the M1.</p> <p>Some games require stronger guarantees. In DirectX, out-of-bounds loads return zero, and out-of-bounds stores are ignored. DXVK therefore requires <a href="https://docs.vulkan.org/guide/latest/robustness.html#_vk_ext_robustness2"><code>VK_EXT_robustness2</code></a>, a Vulkan extension strengthening robustness.</p> <p>Like before, we implement robustness with compare-and-select instructions. A naïve implementation would <em>compare</em> a loaded index with the buffer size and <em>select</em> a zero result if out-of-bounds. However, our GPU loads are vector while arithmetic is scalar. Even if we disabled page faults, we would need up to four compare-and-selects per load.</p> <div><pre><code><span><span>load</span> R, buffer, index * <span>16</span></span>
<span><span>ulesel</span> R[<span>0</span>], index, size, R[<span>0</span>], <span>0</span></span>
<span><span>ulesel</span> R[<span>1</span>], index, size, R[<span>1</span>], <span>0</span></span>
<span><span>ulesel</span> R[<span>2</span>], index, size, R[<span>2</span>], <span>0</span></span>
<span><span>ulesel</span> R[<span>3</span>], index, size, R[<span>3</span>], <span>0</span></span></code></pre></div> <p>There’s a trick: reserve <em>64 gigabytes</em> of zeroes using virtual memory voodoo. Since every 32-bit index multiplied by 16 fits in 64 gigabytes, any index into this region loads zeroes. For out-of-bounds loads, we simply replace the buffer address with the reserved address while preserving the index. Replacing a 64-bit address costs just two 32-bit compare-and-selects.</p> <div><pre><code><span><span>ulesel</span> buffer.lo, index, size, buffer.lo, RESERVED.lo</span>
<span><span>ulesel</span> buffer.hi, index, size, buffer.hi, RESERVED.hi</span>
<span><span>load</span> R, buffer, index * <span>16</span></span></code></pre></div> <p>Two instructions, not four.</p> <h2 id="next-steps">Next steps</h2> <p>Sparse texturing is next for Honeykrisp, which will unlock more DX12 games. The alpha already runs DX12 games that don’t require sparse, like <a href="https://store.steampowered.com/app/1091500/Cyberpunk_2077/"><strong>Cyberpunk 2077</strong></a>.</p> <figure> <a href="https://rosenzweig.io/blog/Games-Asahi/Cyberpunk2077-small.png"><img src="https://rosenzweig.io/blog/Games-Asahi/Cyberpunk2077-small.avif" alt="Cyberpunk 2077"></a> </figure> <p>While many games are playable, newer AAA titles don’t hit 60fps <em>yet</em>. Correctness comes first. Performance improves next. Indie games like <a href="https://store.steampowered.com/app/367520/Hollow_Knight/"><strong>Hollow Knight</strong></a> do run full speed.</p> <figure> <a href="https://rosenzweig.io/blog/Games-Asahi/HollowKnight-small.png"><img src="https://rosenzweig.io/blog/Games-Asahi/HollowKnight-small.avif" alt="Hollow Knight"></a> </figure> <p>Beyond gaming, we’re adding general purpose x86 emulation based on this stack. For more information, <a href="https://docs.fedoraproject.org/en-US/fedora-asahi-remix/x86-support/">see the FAQ</a>.</p> <p>Today’s alpha is a taste of what’s to come. Not the final form, but enough to enjoy <a href="https://store.steampowered.com/app/620/Portal_2/"><strong>Portal 2</strong></a> while we work towards “1.0”.</p> <figure> <a href="https://rosenzweig.io/blog/Games-Asahi/Portal2-small.png"><img src="https://rosenzweig.io/blog/Games-Asahi/Portal2-small.avif" alt="Portal 2"></a> </figure> <h2 id="acknowledgements">Acknowledgements</h2> <p>This work has been years in the making with major contributions from…</p> <ul> <li><a href="https://rosenzweig.io/">Alyssa Rosenzweig</a></li> <li><a href="https://lina.yt/me">Asahi Lina</a></li> <li><a href="https://social.treehouse.systems/@chaos_princess">chaos_princess</a></li> <li><a href="https://github.com/davide125">Davide Cavalca</a></li> <li><a href="https://mastodon.social/@dougall">Dougall Johnson</a></li> <li><a href="https://ella.gay/">Ella Stanforth</a></li> <li><a href="https://www.gfxstrand.net/faith/welcome/">Faith Ekstrand</a></li> <li><a href="https://social.treehouse.systems/@janne">Janne Grunau</a></li> <li><a href="https://chaos.social/@karolherbst">Karol Herbst</a></li> <li><a href="https://social.treehouse.systems/@marcan">marcan</a></li> <li><a href="https://mary.zone/">Mary Guillemard</a></li> <li><a href="https://neal.gompa.dev/">Neal Gompa</a></li> <li><a href="https://sinrega.org/">Sergio López</a></li> <li><a href="https://github.com/TellowKrinkle">TellowKrinkle</a></li> <li><a href="https://github.com/teohhanhui">Teoh Han Hui</a></li> <li><a href="https://mastodon.gamedev.place/@robclark">Rob Clark</a></li> <li><a href="https://github.com/sonicadvance1">Ryan Houdek</a></li> </ul> <p>… Plus hundreds of developers whose work we build upon, spanning the Linux, Mesa, Wine, and FEX projects. Today’s release is thanks to the magic of open source.</p> <p>We hope you enjoy the magic.</p> <p>Happy gaming.</p> <p><a href="https://rosenzweig.io/">Back to home</a></p> </div>]]></description>
        </item>
    </channel>
</rss>