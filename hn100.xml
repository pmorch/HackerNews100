<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0">
    <channel>
        <title>HN100 - Readable Contents</title>
        <link>https://hn.algolia.com/api/v1/search_by_date?tags=%28story,poll%29&amp;numericFilters=points%3E100</link>
        <description>Uses Readability to add bodies to the RSS feed</description>
        <lastBuildDate>Tue, 12 Aug 2025 01:30:02 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>en</language>
        <item>
            <title><![CDATA[I've seen 12 people hospitalized after losing touch with reality because of AI (110 pts)]]></title>
            <link>https://twitter.com/KeithSakata/status/1954884361695719474</link>
            <guid>44869323</guid>
            <pubDate>Mon, 11 Aug 2025 20:50:13 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://twitter.com/KeithSakata/status/1954884361695719474">https://twitter.com/KeithSakata/status/1954884361695719474</a>, See on <a href="https://news.ycombinator.com/item?id=44869323">Hacker News</a></p>
Couldn't get https://twitter.com/KeithSakata/status/1954884361695719474: Error: Request failed with status code 400]]></description>
        </item>
        <item>
            <title><![CDATA[Neki – sharded Postgres by the team behind Vitess (147 pts)]]></title>
            <link>https://planetscale.com/blog/announcing-neki</link>
            <guid>44867374</guid>
            <pubDate>Mon, 11 Aug 2025 18:03:57 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://planetscale.com/blog/announcing-neki">https://planetscale.com/blog/announcing-neki</a>, See on <a href="https://news.ycombinator.com/item?id=44867374">Hacker News</a></p>
<div id="readability-page-1" class="page"><article><p>By <!-- -->Andres Taylor, Dirkjan Bussink, Harshit Gangal, Nick Van Wiggeren, Noble Mittal, Rohit Nayak, Roman Sodermans, Shlomi Noach, Sam Lambert<!-- --> | <time datetime="2025-08-11">August 11, 2025</time></p><p><picture><source srcset="https://planetscale-images.imgix.net/assets/neki@2x-DTgVq7KS.png?auto=compress%2Cformat" media="(prefers-color-scheme: light), (prefers-color-scheme: no-preference)"><img alt="Neki" height="800" loading="lazy" src="https://planetscale-images.imgix.net/assets/neki@2x-DTgVq7KS.png?auto=compress%2Cformat" width="1400"></picture></p><p>Today, we are announcing <a href="https://neki.dev/"><strong>Neki</strong></a> — sharded Postgres by the team behind <a href="https://vitess.io/">Vitess</a>. Vitess is one of PlanetScale’s greatest strengths and contemporary Vitess is the product of our experience running at extreme scale. We have made explicit sharding accessible to hundreds of thousands of people and it is time to bring this power to Postgres.</p><p>Neki is not a fork of Vitess. Vitess’ achievements are enabled by leveraging MySQL’s strengths and engineering around its weaknesses. To achieve Vitess’ power for Postgres we are architecting from first principles and building alongside design partners at scale. When we are ready we will release Neki as an open source project suitable for running the most demanding Postgres workloads.</p><p>To stay up to date with the latest developments on Neki you can signup at <a href="https://neki.dev/">neki.dev</a>.</p></article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Token growth indicates future AI spend per dev (165 pts)]]></title>
            <link>https://blog.kilocode.ai/p/future-ai-spend-100k-per-dev</link>
            <guid>44867312</guid>
            <pubDate>Mon, 11 Aug 2025 17:59:42 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://blog.kilocode.ai/p/future-ai-spend-100k-per-dev">https://blog.kilocode.ai/p/future-ai-spend-100k-per-dev</a>, See on <a href="https://news.ycombinator.com/item?id=44867312">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><article><div dir="auto"><p>Kilo just broke through the 1 trillion tokens a month barrier on OpenRouter for the first time. </p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/$s_!v9-s!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F71ab674a-e015-4079-9d34-d0236ec96153_1930x724.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!v9-s!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F71ab674a-e015-4079-9d34-d0236ec96153_1930x724.png 424w, https://substackcdn.com/image/fetch/$s_!v9-s!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F71ab674a-e015-4079-9d34-d0236ec96153_1930x724.png 848w, https://substackcdn.com/image/fetch/$s_!v9-s!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F71ab674a-e015-4079-9d34-d0236ec96153_1930x724.png 1272w, https://substackcdn.com/image/fetch/$s_!v9-s!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F71ab674a-e015-4079-9d34-d0236ec96153_1930x724.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/$s_!v9-s!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F71ab674a-e015-4079-9d34-d0236ec96153_1930x724.png" width="500" height="187.5" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/71ab674a-e015-4079-9d34-d0236ec96153_1930x724.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:546,&quot;width&quot;:1456,&quot;resizeWidth&quot;:500,&quot;bytes&quot;:237722,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:false,&quot;topImage&quot;:true,&quot;internalRedirect&quot;:&quot;https://blog.kilocode.ai/i/170429285?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F71ab674a-e015-4079-9d34-d0236ec96153_1930x724.png&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/$s_!v9-s!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F71ab674a-e015-4079-9d34-d0236ec96153_1930x724.png 424w, https://substackcdn.com/image/fetch/$s_!v9-s!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F71ab674a-e015-4079-9d34-d0236ec96153_1930x724.png 848w, https://substackcdn.com/image/fetch/$s_!v9-s!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F71ab674a-e015-4079-9d34-d0236ec96153_1930x724.png 1272w, https://substackcdn.com/image/fetch/$s_!v9-s!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F71ab674a-e015-4079-9d34-d0236ec96153_1930x724.png 1456w" sizes="100vw" fetchpriority="high"></picture></div></a></figure></div><p>Each of the open source family of AI coding tools (Cline, Roo, Kilo) is growing rapidly this month.</p><div><figure><a target="_blank" href="https://x.com/Kilo_Code/status/1953767203175543246" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!IDuV!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F67a9212f-3d94-4d87-99ec-6b6df3c065ec_2544x868.png 424w, https://substackcdn.com/image/fetch/$s_!IDuV!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F67a9212f-3d94-4d87-99ec-6b6df3c065ec_2544x868.png 848w, https://substackcdn.com/image/fetch/$s_!IDuV!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F67a9212f-3d94-4d87-99ec-6b6df3c065ec_2544x868.png 1272w, https://substackcdn.com/image/fetch/$s_!IDuV!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F67a9212f-3d94-4d87-99ec-6b6df3c065ec_2544x868.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/$s_!IDuV!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F67a9212f-3d94-4d87-99ec-6b6df3c065ec_2544x868.png" width="728" height="248.5" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/67a9212f-3d94-4d87-99ec-6b6df3c065ec_2544x868.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:497,&quot;width&quot;:1456,&quot;resizeWidth&quot;:728,&quot;bytes&quot;:386444,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:&quot;https://x.com/Kilo_Code/status/1953767203175543246&quot;,&quot;belowTheFold&quot;:false,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://blog.kilocode.ai/i/170429285?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F67a9212f-3d94-4d87-99ec-6b6df3c065ec_2544x868.png&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/$s_!IDuV!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F67a9212f-3d94-4d87-99ec-6b6df3c065ec_2544x868.png 424w, https://substackcdn.com/image/fetch/$s_!IDuV!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F67a9212f-3d94-4d87-99ec-6b6df3c065ec_2544x868.png 848w, https://substackcdn.com/image/fetch/$s_!IDuV!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F67a9212f-3d94-4d87-99ec-6b6df3c065ec_2544x868.png 1272w, https://substackcdn.com/image/fetch/$s_!IDuV!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F67a9212f-3d94-4d87-99ec-6b6df3c065ec_2544x868.png 1456w" sizes="100vw"></picture></div></a></figure></div><p><span>Part of this growth is caused by Cursor and Claude starting to throttle their users. We wrote about </span><a href="https://blog.kilocode.ai/p/cursors-500-requests-unlimited-225" rel="">Cursor at the beginning of July</a><span> and about </span><a href="https://blog.kilocode.ai/p/the-ai-pricing-bait-and-switch" rel="">Claude in the second half of July</a><span>. Their throttling </span><a href="https://www.reddit.com/r/kilocode/comments/1mcdxr4/amazed_by_kilo_or_where_will_all_the_coders_go/?utm_source=share&amp;utm_medium=web3x&amp;utm_name=web3xcss&amp;utm_term=1&amp;utm_content=share_button" rel="">sent users to the open source family of AI coding tools</a><span> causing the increases you see in the graphs above. Cursor and Claude needed to throttle because the industry made a flawed assumption.</span></p><p>The industry expected that because the raw inference costs were coming down fast, the applications inference costs would come down fast as well but this assumption was wrong.</p><p><a href="https://a16z.com/llmflation-llm-inference-cost/" rel="">Raw inference costs did decrease by 10x year-over-year.</a><span> This expectation made startups bet on a business model where companies could afford to sell subscriptions at significant losses, knowing they'd achieve healthy margins as costs plummeted.</span></p><p><a href="https://x.com/dobroslav_dev/status/1952369863344673194" rel="">Cursor's Ultra plan</a><span> exemplified this approach perfectly: charge users $200 while providing at least $400 worth of tokens, essentially operating at -100% gross margin.</span></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/$s_!sMJm!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd54bb2ba-591c-4b93-87bb-ab8776ff059b_1182x952.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!sMJm!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd54bb2ba-591c-4b93-87bb-ab8776ff059b_1182x952.png 424w, https://substackcdn.com/image/fetch/$s_!sMJm!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd54bb2ba-591c-4b93-87bb-ab8776ff059b_1182x952.png 848w, https://substackcdn.com/image/fetch/$s_!sMJm!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd54bb2ba-591c-4b93-87bb-ab8776ff059b_1182x952.png 1272w, https://substackcdn.com/image/fetch/$s_!sMJm!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd54bb2ba-591c-4b93-87bb-ab8776ff059b_1182x952.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/$s_!sMJm!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd54bb2ba-591c-4b93-87bb-ab8776ff059b_1182x952.png" width="542" height="436.5346869712352" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/d54bb2ba-591c-4b93-87bb-ab8776ff059b_1182x952.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:952,&quot;width&quot;:1182,&quot;resizeWidth&quot;:542,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/$s_!sMJm!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd54bb2ba-591c-4b93-87bb-ab8776ff059b_1182x952.png 424w, https://substackcdn.com/image/fetch/$s_!sMJm!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd54bb2ba-591c-4b93-87bb-ab8776ff059b_1182x952.png 848w, https://substackcdn.com/image/fetch/$s_!sMJm!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd54bb2ba-591c-4b93-87bb-ab8776ff059b_1182x952.png 1272w, https://substackcdn.com/image/fetch/$s_!sMJm!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd54bb2ba-591c-4b93-87bb-ab8776ff059b_1182x952.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p>The bet was that by the following year, the application inference would cost 90% less, creating a $160 gross profit (+80% gross margins). But this didn't happen, instead of declining the application inference costs actually grew!</p><p>Application inference costs increased for two reasons: the frontier model costs per token stayed constant and the token consumption per application grew a lot. We'll first dive into the reasons for the constant token price for frontier models and end with explaining the token consumption per application.</p><p><a href="https://ethanding.substack.com/p/ai-subscriptions-get-short-squeezed" rel="">The price per token for the frontier model stayed constant</a><span> because of the </span><a href="https://www.researchgate.net/figure/The-most-popular-large-AI-models-of-recent-years-This-figure-shows-the-main-features-of_fig1_387458379" rel="">increasing size of models</a><span> and more test-time scaling. Test time scaling, also called long thinking, is the </span><a href="https://blogs.nvidia.com/blog/ai-scaling-laws/" rel="">third way to scale AI</a><span> as shown in the graphic below.</span></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/$s_!i474!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb558bd73-0244-415d-aab5-81934c719436_1456x819.jpeg" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!i474!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb558bd73-0244-415d-aab5-81934c719436_1456x819.jpeg 424w, https://substackcdn.com/image/fetch/$s_!i474!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb558bd73-0244-415d-aab5-81934c719436_1456x819.jpeg 848w, https://substackcdn.com/image/fetch/$s_!i474!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb558bd73-0244-415d-aab5-81934c719436_1456x819.jpeg 1272w, https://substackcdn.com/image/fetch/$s_!i474!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb558bd73-0244-415d-aab5-81934c719436_1456x819.jpeg 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/$s_!i474!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb558bd73-0244-415d-aab5-81934c719436_1456x819.jpeg" width="624" height="351" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/b558bd73-0244-415d-aab5-81934c719436_1456x819.jpeg&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:819,&quot;width&quot;:1456,&quot;resizeWidth&quot;:624,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/$s_!i474!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb558bd73-0244-415d-aab5-81934c719436_1456x819.jpeg 424w, https://substackcdn.com/image/fetch/$s_!i474!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb558bd73-0244-415d-aab5-81934c719436_1456x819.jpeg 848w, https://substackcdn.com/image/fetch/$s_!i474!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb558bd73-0244-415d-aab5-81934c719436_1456x819.jpeg 1272w, https://substackcdn.com/image/fetch/$s_!i474!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb558bd73-0244-415d-aab5-81934c719436_1456x819.jpeg 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p><span>While the pre- and post-training scaling influenced only the training costs of models. But this test-time scaling increases the cost of inference. Thinking models like OpenAI's o1 series allocate massive computational effort during inference itself. These models can require </span><strong>over 100x compute for challenging queries</strong><span> compared to traditional single-pass inference.</span></p><p><span>Token consumption per application grew a lot because models allowed for </span><a href="https://www.meibel.ai/post/understanding-the-impact-of-increasing-llm-context-windows" rel="">longer context windows</a><span> and bigger suggestions from the models. The combination of a steady price per token and more token consumption caused app inference costs to grow about 10x over the last two years. Market leader Cursor introduced a $200 plan where before $20 was the default. The $200 plan has also been followed by Claude Code and others.</span></p><p>The top end of subscriptions is $200 today but power users find that they are extensively throttled if they use a lot of inference. That throttling comes in the form of rate limiting, using lower quality models, context window compression, and other techniques.</p><p>If you don't want to be throttled you need to pay for inference yourself. The open source family of coding tools (Cline, Roo, Kilo) is based on that principle: “never throttle the user”. Because the users directly see the costs these tools have also led the way in reducing costs by allowing the users to:</p><ol><li><p>Splitting work up in many smaller tasks that can each be run efficiently.</p></li><li><p>Using different modes, in Kilo we have an Orchestrator, Architect, Code, and Debug mode.</p></li><li><p>Combine closed-source models for architecting tasks (e.g. Sonnet 4) and open-source for coding (Qwen3)</p></li><li><p>Enhance the prompt with AI before submitting it</p></li><li><p>Optimize context efficiency with memory banks</p></li><li><p>Enable prompt caching</p></li><li><p>Allow termination of a running task when the model hallucinates</p></li></ol><p>Despite the efforts to reduce costs we do expect them to continue to grow for the power users.</p><p>We expect app inference costs to grow quickly. This is driven by two developments: more parallel agents and more work done before human feedback is needed.</p><p><span>People are </span><a href="https://www.reddit.com/r/ClaudeAI/comments/1kwm4gm/has_anyone_tried_parallelizing_ai_coding_agents/" rel="">experimenting</a><span> </span><a href="https://ainativedev.io/news/how-to-parallelize-ai-coding-agents" rel="">with</a><span> </span><a href="https://google.github.io/adk-docs/agents/workflow-agents/parallel-agents/" rel="">parallel</a><span> AI coding agents today with </span><a href="https://www.warp.dev/" rel="">Warp already having it available to people</a><span>. We expect parallel agents to become the default in the industry and look forward to introduce them in Kilo code sooner rather than later. This will greatly increase token consumption per human hour.</span></p><p>Agents are also able to work longer before needing human feedback. Because they are working more and pausing less this also increases token consumption per human hour.</p><p><span>Both effects together will push costs at the top level to $100k a year. Spending that magnitude of money on software is not without precedent, </span><a href="https://news.ycombinator.com/item?id=26658405" rel="">chip design licenses from Cadence or Synopsys are already $250k a year</a><span>.</span></p><p>While the prospect of $100k+ per year in costs is a lot it can always be worse.</p><p><span>AI costs for most engineers are approximately 1000x smaller than what is happening at the AI training stage. Here the costs for the normal 'inference engineer' is dwarfed by the thousand times bigger impact of the AI ‘training engineer’. The ‘inference engineer’ we talked about above might make $100k and use $100k to be many times more productive than an engineer before AI. A top ‘training engineer’ directs $100m in spend and is paid $100m a year. Top frontier labs spend </span><a href="https://www.datacenterdynamics.com/en/news/openai-training-and-inference-costs-could-reach-7bn-for-2024-ai-startup-set-to-lose-5bn-report/" rel="">billions on AI training</a><span> and this compute work is directed by a handful of people. Mark Zuckerberg is rumored to have offered these people ‘signing bonuses’ of </span><a href="https://www.wsj.com/tech/ai/meta-ai-recruiting-mark-zuckerberg-5c231f75" rel="">$100m</a><span> to </span><a href="https://www.wsj.com/tech/ai/meta-zuckerberg-ai-recruiting-fail-e6107555" rel="">$1b</a><span> with unknown contract lengths. The difference in pay between inference and training engineers is because of their relative impact. You train a model with a handful of people while it is used by millions of people.</span></p></div></article></div><div id="discussion"><h4>Discussion about this post</h4></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Reddit will block the Internet Archive (101 pts)]]></title>
            <link>https://www.theverge.com/news/757538/reddit-internet-archive-wayback-machine-block-limit</link>
            <guid>44866698</guid>
            <pubDate>Mon, 11 Aug 2025 17:08:25 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.theverge.com/news/757538/reddit-internet-archive-wayback-machine-block-limit">https://www.theverge.com/news/757538/reddit-internet-archive-wayback-machine-block-limit</a>, See on <a href="https://news.ycombinator.com/item?id=44866698">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="zephr-anchor"><p>Reddit says that it has caught AI companies scraping its data from the Internet Archive’s Wayback Machine, so it’s going to start blocking the Internet Archive from indexing the vast majority of Reddit. The Wayback Machine will no longer be able to crawl post detail pages, comments, or profiles; instead, it will only be able to index the Reddit.com homepage, which effectively means Internet Archive will only be able to archive insights into which news headlines and posts were most popular on a given day.</p><p>”Internet Archive provides a service to the open web, but we’ve been made aware of instances where AI companies violate platform policies, including ours, and scrape data from the Wayback Machine,” spokesperson Tim Rathschmidt tells <em>The Verge</em>.</p><p>The Internet Archive’s mission is to keep a digital archive of websites on the internet and <a href="https://archive.org/about/">“other cultural artifacts,”</a> and the Wayback Machine is a tool you can use to look at pages as they appeared on certain dates, but Reddit believes not all of its content should be archived that way.“Until they’re able to defend their site and comply with platform policies (e.g., respecting user privacy, re: deleting removed content) we’re limiting some of their access to Reddit data to protect redditors,” Rathschmidt says.</p><p>The limits will start “ramping up” today, and Reddit says it reached out to the Internet Archive “in advance” to “inform them of the limits before they go into effect,” according to Rathschmidt. He says Reddit has also “raised concerns” about the ability of people to scrape content from the Internet Archive in the past.</p><p>Reddit has a recent history of cutting off access to scraper tools as AI companies have begun to use (and abuse) them en masse, but it’s willing to provide that data if companies pay. Last year, Reddit struck <a href="https://www.theverge.com/2024/2/22/24080165/google-reddit-ai-training-data">a deal with Google</a> for both Google Search and AI training data early last year, and a few months later, it started blocking major search engines from crawling its data <a href="https://www.theverge.com/2024/7/24/24205244/reddit-blocking-search-engine-crawlers-ai-bot-google">unless they pay</a>. It also said its infamous <a href="https://www.theverge.com/2023/4/18/23688463/reddit-developer-api-terms-change-monetization-ai">API changes from 2023</a>, which forced some third-party apps to shut down, <a href="https://www.theverge.com/23779477/reddit-protest-blackouts-crushed">leading to protests</a>, were because those APIs were abused to train AI models.</p><p>Reddit also struck an AI deal with <a target="_blank" href="https://www.theverge.com/2024/5/16/24158529/reddit-openai-chatgpt-api-access-advertising" rel="noreferrer noopener">OpenAI</a>, but it <a target="_blank" href="https://www.theverge.com/ai-artificial-intelligence/679768/reddit-sues-anthropic-alleging-its-bots-accessed-reddit-more-than-100000-times-since-last-july" rel="noreferrer noopener">sued Anthropic</a> in June, claiming Anthropic was still scraping from Reddit even after Anthropic <a target="_blank" href="https://www.theverge.com/2024/7/31/24210565/reddit-microsoft-anthropic-perplexity-pay-ai-search" rel="noreferrer noopener">said it</a> wasn’t scraping anymore.</p><p>“We have a longstanding relationship with Reddit and continue to have ongoing discussions about this matter,” Mark Graham, director of the Wayback Machine, says in a statement to <em>The Verge</em>.</p><p><em><strong>Update, August 11th</strong>: Added statement from the Wayback Machine.</em></p><div><p><span><strong>Follow topics and authors</strong> from this story to see more like this in your personalized homepage feed and to receive email updates.</span></p><ul><li id="follow-author-article_footer-dmcyOmF1dGhvclByb2ZpbGU6MTIz"><span aria-expanded="false" aria-haspopup="true" role="button" tabindex="0"><span><span><svg width="9" height="9" viewBox="0 0 9 9" fill="none" xmlns="http://www.w3.org/2000/svg" aria-label="Follow"><path d="M5 0H4V4H0V5H4V9H5V5H9V4H5V0Z"></path></svg></span><span>Jay Peters</span></span></span></li><li></li><li></li><li></li></ul></div></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[The value of institutional memory (105 pts)]]></title>
            <link>https://timharford.com/2025/05/the-value-of-institutional-memory/</link>
            <guid>44866500</guid>
            <pubDate>Mon, 11 Aug 2025 16:53:43 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://timharford.com/2025/05/the-value-of-institutional-memory/">https://timharford.com/2025/05/the-value-of-institutional-memory/</a>, See on <a href="https://news.ycombinator.com/item?id=44866500">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
				
				
				
				
				
<p>In 1978, a dredging gang working for British Waterways was struggling with a problem. They were trying to clear obstacles on the Chesterfield Canal so they could stabilise a concrete wall — not an easy day’s work. But what really had them stumped was a heavy iron chain on the canal bottom. After various attempts, they hooked the chain to their dredger. That did the trick. A firm pull removed the chain and the block of wood on the end of it. The gang took a well-earned break for tea. </p>



<p>The tea break was rudely interrupted by a policeman in a state of some excitement. He had been passing the normally tranquil waterway when he could not help but notice a large whirlpool. By the time the crew returned to the scene, the canal had gone. “We didn’t know there was a plug,” protested one workman. And, in fairness, the canal was two centuries old, and so was the plug. Whatever records there may have been had been destroyed in the Blitz. The moral of the story: institutional memory is valuable, and if an organisation starts forgetting important matters (such as the existence of the plug) bad things happen. Expertise drains away alarmingly fast if not refreshed by activity. </p>



<p>It’s not easy, though. I was recently taken on a tour of the Bodleian Library’s portrait collection, and was struck by how hard our tour guides had had to work to recover basic information about the sitter and the artist, even in portraits just a few decades old. This wouldn’t be so remarkable, except that the entire reason for the Bodleian Library to exist is to preserve information in an accessible form. (Bodley’s librarian, Richard Ovenden, has written <a href="https://www.amazon.co.uk/Burning-Books-RADIO-Knowledge-Power/dp/152937877X?crid=2ZENZAF1L4T8Q&amp;dib=eyJ2IjoiMSJ9.l8Sn-789eZkldlDhy4hyHhzBMOGDhL5x7SBGWCcgk9HGjHj071QN20LucGBJIEps.Pf8k40P2vmwWszH810V989scJOJXxtloM2DBqsSinVI&amp;dib_tag=se&amp;keywords=burning+the+books+richard+ovenden&amp;qid=1744960198&amp;sprefix=burning+the+book%2Caps%2C98&amp;sr=8-1&amp;linkCode=ll1&amp;tag=timharford-20&amp;linkId=d1faa4128aa6adeea95584c476179c92&amp;language=en_GB&amp;ref_=as_li_ss_tl">Burning the Books: A History of the Deliberate Destruction of Knowledge</a> and is president of the Digital Preservation Coalition.) But the Bodleian is a library, not a portrait museum, and without constant attention, the natural order of things is not to remember, but to forget. </p>



<p>That means trouble. Consider Volkswagen’s disastrous scandal, in which the company designed its cars to fool emissions tests by regulators. No, not the scandal of 2015, which cost the company its reputation, its CEO and well over €30bn in fines, settlements and legal fees. I mean the scandal of 1973, in which VW was accused by the US Environmental Protection Agency of designing its cars to fool emissions tests by regulators. VW settled out of court and, it seems, spent the following decades forgetting what should have been a chastening experience. </p>



<p>A more tragic example is the pair of fatal Space Shuttle explosions, Challenger in 1986 and Columbia in 2003. These accidents seem very different. One was an explosion shortly after take-off, the other a break-up on re-entry. But the underlying errors that made them possible seem eerily similar. The Columbia Accident Investigation Board report noted that the same basic questions had emerged: why did both shuttles keep flying with known problems? And why did Nasa managers decide that it was safe to launch despite warnings from their engineers? To set the stage for Columbia, Nasa first had to forget all the lessons of Challenger. </p>



<p>There is more to institutional forgetfulness than forgetting one big thing, whether that is “if you cheat the EPA, they may figure it out” or “the canal has a plughole”. Organisations can also just forget how to get things done. </p>



<p>As a boy I was fascinated by the Lockheed TriStar airliner because of its unusual configuration, with one engine in the tail. You don’t see it much these days — the TriStar was not a commercial success. </p>



<p>I wasn’t the only person to be intrigued by the plane, but the organisational psychologist Linda Argote had a different reason to scrutinise it. Most aircraft get much cheaper to make with the benefit of experience — they are the canonical example of learning by doing. But the TriStar stayed stubbornly expensive to make. Argote wanted to know why. Her idea flipped the idea of learning by doing: what about forgetting by not doing? </p>



<p>In a 1990 article Argote and Dennis Epple concluded that Lockheed had made so few planes they were forgetting faster than they were learning. In particular, in 1977 and 1978 production slumped to just 14 TriStars in total, and by the early 1980s costs in real terms were higher than in 1975. </p>



<p>The economist Lanier Benkard later estimated that Lockheed’s cost-saving expertise tended to drain away alarmingly fast if not refreshed by activity, with a half-life of just over a year. We can’t generalise too much from that. Planes are planes, and every case is different. Still, anyone who has ever filed tax returns can attest that a year is easily enough time to forget how to do any complex process. </p>



<p>Forgetting can happen for many reasons. People leave. Physical archives are vulnerable to mould and fire and being misplaced. Digital archives tend to become unreadable as technology changes — indeed the final reference in the Wikipedia entry on organisational memory is a dead link to a lost NHS website. And sometimes organisations deliberately forget. The underlying cause of the Windrush scandal in the UK was that one part of the Home Office decided to make onerous demands that UK residents prove they had the right to live and work in the country, without knowing — or much caring — that another part of the Home Office had destroyed the records that made such proof possible. </p>



<p>More than 100,000 US government web pages disappeared after President Trump took office. It remains to be seen what is gone temporarily and what has been lost for ever. </p>



<p>It is easy for organisations to forget, even when they are trying to remember. Let’s not make this worse than it has to be, or it will be more than the Chesterfield Canal that we lose.</p>



<p><em>Written for and first published in the <a href="https://www.ft.com/content/55c35781-bc26-4d42-af1f-9ebd62e441c7">Financial Times</a> on 18 April 2025.</em></p>



<p><em>Loyal readers might enjoy the book that started it all, <a href="https://timharford.com/books/undercovereconomist/">The Undercover Economist</a>.</em></p>



<p><em>I’ve set up a storefront on Bookshop in the <a href="https://bookshop.org/shop/TimHarford">United States</a> and the <a href="https://uk.bookshop.org/shop/TimHarford">United Kingdom</a>. Links to Bookshop and Amazon may generate referral fees.</em></p>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Wikipedia loses challenge against Online Safety Act (558 pts)]]></title>
            <link>https://www.bbc.com/news/articles/cjr11qqvvwlo</link>
            <guid>44866208</guid>
            <pubDate>Mon, 11 Aug 2025 16:33:26 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.bbc.com/news/articles/cjr11qqvvwlo">https://www.bbc.com/news/articles/cjr11qqvvwlo</a>, See on <a href="https://news.ycombinator.com/item?id=44866208">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-testid="byline-new" data-component="byline-block"><p><span>Chris Vallance</span></p><p><span>Senior technology reporter</span></p></div><div data-component="text-block"><p>Wikipedia has lost a legal challenge to new Online Safety Act rules which it says could threaten the human rights and safety of its volunteer editors.</p><p>The Wikimedia Foundation - the non-profit which supports the online encyclopaedia - wanted a judicial review of regulations which could mean Wikipedia has to verify the identities of its users.</p><p>But it said despite the loss, <a target="_blank" href="https://www.judiciary.uk/wp-content/uploads/2025/08/Wikimedia-Foundation-and-another-v-Secretary-of-State-for-Science-Innovation-and-Technology.pdf">the judgement</a> "emphasized the responsibility of Ofcom and the UK government to ensure Wikipedia is protected".</p><p>The government told the BBC it welcomed the High Court's judgment, "which will help us continue our work implementing the Online Safety Act to create a safer online world for everyone".</p></div><div data-component="text-block"><p>Judicial reviews challenge the lawfulness of the way in which a decision has been made by a public body.</p><p>In this case the Wikimedia Foundation and a Wikipedia editor tried to challenge the way in which the government decided to make regulations covering which sites should be classed "Category 1" under the Online Safety Act - the strictest rules sites must follow.</p><p>It argued the rules were logically flawed and too broad, meaning a policy intended to impose extra rules on large social media companies would instead apply to Wikipedia.</p><p>In particular the foundation is concerned the extra duties required - if Wikipedia was classed as Category 1 - would mean it would have to verify the identity of its contributors, undermining their privacy and safety.</p><p>The only way it could avoid being classed as Category 1 would be to cut the number of people in the UK who could access the online encyclopaedia by about three-quarters, or disable key functions on the site. </p><p>The government's lawyers argued that ministers had  considered whether Wikipedia should be exempt from the regulations but had reasonably rejected the idea.</p></div><div data-component="text-block"><p>In the end, the court rejected Wikimedia's arguments.</p><p>But Phil Bradley-Schmieg, Lead Counsel at the Wikimedia Foundation, said the judgment did not give Ofcom and the Secretary of State, in Mr Justice Johnson's words, "a green light to implement a regime that would significantly impede Wikipedia's operations".</p><p>And the judgement makes it clear other legal challenges could be possible. </p><p>Wikimedia could potentially challenge Ofcom's decision making if the regulator did ultimately decide to classify the site as Category 1.</p><p>And if the effect of making Wikipedia Category 1 meant it could not continue to operate, then other legal challenges could follow.</p><p>"Wikipedia has been caught in the stricter regulations due to its size and user created content even though it argues (convincingly) that it differs significantly from other user-to-user platforms," said Mona Schroedel, data protection litigation specialist at law firm Freeths.</p><p>"The court's decision has left the door open for Wikipedia to be exempt from the stricter rules upon review."</p><p>The communications regulator Ofcom, which will enforce the act, told the BBC: "We note the court's judgment and will continue to progress our work in relation to categorised services and the associated extra online safety rules for those companies."</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Claude Is the Drug, Cursor Is the Dealer (144 pts)]]></title>
            <link>https://middlelayer.substack.com/p/i-claude-is-the-drug-cursor-is-the</link>
            <guid>44865813</guid>
            <pubDate>Mon, 11 Aug 2025 16:04:34 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://middlelayer.substack.com/p/i-claude-is-the-drug-cursor-is-the">https://middlelayer.substack.com/p/i-claude-is-the-drug-cursor-is-the</a>, See on <a href="https://news.ycombinator.com/item?id=44865813">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><article><div dir="auto"><p>There is a rumor floating around tech-twitter that Cursor makes just ten cents for every dollar it spends. Maybe that number is exaggerated but even if it’s directionally correct at some point we are in for a rude awakening in the world of consumer AI apps. </p><p><strong>Many of today’s </strong><em><strong>sexiest</strong></em><strong> AI startups are not product companies but rather distribution arms for someone else’s model.</strong></p><p>These companies are beloved on Twitter, raise at billion-dollar valuations and build beautiful user experiences. The approaching bubble pop? They don’t own the product and they don’t control the supply chain. They live and die by the labs upstream.</p><p>We are witnessing tech’s version of the drug trade: a few powerful labs synthesize the product and startups hustle to get it into users’ hands. Often fronting much of the costs and raising large venture rounds to keep the economics alive. </p><div><p><span>If OpenAI, Anthropic, and Meta are the chemists, then startups like Cursor, Bolt and Lovable are the dealers. </span></p><p><span>The labs develop the substance: </span><strong>the models.</strong></p></div><p><span>The startups distribute the substance: </span><strong>the dealers</strong><span>. </span></p><p>These startups:</p><ul><li><p>Wrap the model in UX</p></li><li><p>Front the inference cost</p></li><li><p>Serve the customer</p></li><li><p>Build a fanbase, raise big VC rounds, grow their ARR fast </p></li></ul><p>But over time, the truth sets in: they don’t control the pricing, they don’t control the roadmap, and their supplier keeps shipping your best features natively.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/$s_!kz0V!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0afc3898-0f19-41cd-a991-6700e4b1bbc7_1024x1024.png" data-component-name="Image2ToDOM" rel="nofollow ugc noopener"><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!kz0V!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0afc3898-0f19-41cd-a991-6700e4b1bbc7_1024x1024.png 424w, https://substackcdn.com/image/fetch/$s_!kz0V!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0afc3898-0f19-41cd-a991-6700e4b1bbc7_1024x1024.png 848w, https://substackcdn.com/image/fetch/$s_!kz0V!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0afc3898-0f19-41cd-a991-6700e4b1bbc7_1024x1024.png 1272w, https://substackcdn.com/image/fetch/$s_!kz0V!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0afc3898-0f19-41cd-a991-6700e4b1bbc7_1024x1024.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/$s_!kz0V!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0afc3898-0f19-41cd-a991-6700e4b1bbc7_1024x1024.png" width="455" height="455" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/0afc3898-0f19-41cd-a991-6700e4b1bbc7_1024x1024.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:1024,&quot;width&quot;:1024,&quot;resizeWidth&quot;:455,&quot;bytes&quot;:null,&quot;alt&quot;:&quot;Generated image&quot;,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" alt="Generated image" title="Generated image" srcset="https://substackcdn.com/image/fetch/$s_!kz0V!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0afc3898-0f19-41cd-a991-6700e4b1bbc7_1024x1024.png 424w, https://substackcdn.com/image/fetch/$s_!kz0V!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0afc3898-0f19-41cd-a991-6700e4b1bbc7_1024x1024.png 848w, https://substackcdn.com/image/fetch/$s_!kz0V!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0afc3898-0f19-41cd-a991-6700e4b1bbc7_1024x1024.png 1272w, https://substackcdn.com/image/fetch/$s_!kz0V!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0afc3898-0f19-41cd-a991-6700e4b1bbc7_1024x1024.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p><em>Let’s name names. </em></p><ul><li><p><span>Raised </span><strong>$900M</strong><span> at a </span><strong>$9B valuation</strong><span> in 2025.</span></p></li><li><p>Built a great AI coding IDE with full-codebase context and session memory.</p></li><li><p>Runs nearly all inference through OpenAI and Anthropic.</p></li></ul><ul><li><p><span>Raised </span><strong>$150M–$200M</strong><span> at around a </span><strong>$2B valuation</strong><span>.</span></p></li><li><p>Offers low-code app generation powered by Claude, with self-hosting, simplified templates and free trials to get user’s hooked. </p></li></ul><ul><li><p>A lightweight app that wraps GPT and Claude in a clean, keyboard-first interface.</p></li><li><p>Simple UX wrappers on the market.</p></li><li><p>No proprietary model, no infra, no moat if the labs go direct.</p></li></ul><p><span>These companies not only have strong branding and ARR that have investors drooling (</span><em>literally) </em><span>but they are also growing the market. With big marketing budgets and costs that are funded by venture pricing models these companies are revealing just how big each use case may be. </span></p><p><span>But there’s a </span><strong>tradeoff</strong><span>. As they grow adoption, they send a clear signal upstream: </span><em>this is what users want.</em></p><p>And more often than not, the labs take notice. (More on that later.)</p><p>In AI, the story looks quite similar across every new crop of YC startups or venture announcements. What we’ve covered here is just one slice of the market: coding tools. The same dynamics are playing out and will play out in other fast-growing categories: meeting notetakers, AI therapists, creative assistants and beyond.</p><p>From here, the path forks. </p><p><strong>Path 1:</strong><span> labs go direct and wrappers get cut out.</span></p><p><em><span>Illustrated by Foundation Capital’s</span><a href="https://foundationcapital.com/when-model-providers-eat-everything-a-survival-guide-for-service-as-software-startups/" rel="nofollow ugc noopener"> excellent graphic </a><span>below:</span></em><span> </span></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/$s_!E5EQ!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F298dadf0-c9fc-4bb4-a2bc-d6b1fe811d6f_1024x576.jpeg" data-component-name="Image2ToDOM" rel="nofollow ugc noopener"><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!E5EQ!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F298dadf0-c9fc-4bb4-a2bc-d6b1fe811d6f_1024x576.jpeg 424w, https://substackcdn.com/image/fetch/$s_!E5EQ!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F298dadf0-c9fc-4bb4-a2bc-d6b1fe811d6f_1024x576.jpeg 848w, https://substackcdn.com/image/fetch/$s_!E5EQ!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F298dadf0-c9fc-4bb4-a2bc-d6b1fe811d6f_1024x576.jpeg 1272w, https://substackcdn.com/image/fetch/$s_!E5EQ!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F298dadf0-c9fc-4bb4-a2bc-d6b1fe811d6f_1024x576.jpeg 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/$s_!E5EQ!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F298dadf0-c9fc-4bb4-a2bc-d6b1fe811d6f_1024x576.jpeg" width="1024" height="576" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/298dadf0-c9fc-4bb4-a2bc-d6b1fe811d6f_1024x576.jpeg&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:576,&quot;width&quot;:1024,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/$s_!E5EQ!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F298dadf0-c9fc-4bb4-a2bc-d6b1fe811d6f_1024x576.jpeg 424w, https://substackcdn.com/image/fetch/$s_!E5EQ!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F298dadf0-c9fc-4bb4-a2bc-d6b1fe811d6f_1024x576.jpeg 848w, https://substackcdn.com/image/fetch/$s_!E5EQ!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F298dadf0-c9fc-4bb4-a2bc-d6b1fe811d6f_1024x576.jpeg 1272w, https://substackcdn.com/image/fetch/$s_!E5EQ!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F298dadf0-c9fc-4bb4-a2bc-d6b1fe811d6f_1024x576.jpeg 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><div><p><strong>Path 2:</strong><span> models become commoditized, like generic Advil, and wrappers can win.</span></p><p><span>I’ll explore both of those futures in upcoming posts. For now, the takeaway is simple: unless you own the model, you’re not the product. You’re just the dealer and the lab always has the purer supply.</span></p></div></div></article></div><div id="discussion"><h4>Discussion about this post</h4></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[GitHub is no longer independent at Microsoft after CEO resignation (918 pts)]]></title>
            <link>https://www.theverge.com/news/757461/microsoft-github-thomas-dohmke-resignation-coreai-team-transition</link>
            <guid>44865560</guid>
            <pubDate>Mon, 11 Aug 2025 15:47:48 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.theverge.com/news/757461/microsoft-github-thomas-dohmke-resignation-coreai-team-transition">https://www.theverge.com/news/757461/microsoft-github-thomas-dohmke-resignation-coreai-team-transition</a>, See on <a href="https://news.ycombinator.com/item?id=44865560">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><div><p><a href="https://www.theverge.com/authors/tom-warren"><img alt="Tom Warren" data-chromatic="ignore" loading="lazy" width="36" height="36" decoding="async" data-nimg="1" srcset="https://platform.theverge.com/wp-content/uploads/sites/2/chorus/author_profile_images/197777/profilephoto.0.jpg?quality=90&amp;strip=all&amp;crop=0%2C0%2C100%2C100&amp;w=48 1x, https://platform.theverge.com/wp-content/uploads/sites/2/chorus/author_profile_images/197777/profilephoto.0.jpg?quality=90&amp;strip=all&amp;crop=0%2C0%2C100%2C100&amp;w=96 2x" src="https://platform.theverge.com/wp-content/uploads/sites/2/chorus/author_profile_images/197777/profilephoto.0.jpg?quality=90&amp;strip=all&amp;crop=0%2C0%2C100%2C100&amp;w=96"></a></p><div><p><span aria-expanded="false" aria-haspopup="true" role="button" tabindex="0"><span id="follow-author-standard_article_details-dmcyOmF1dGhvclByb2ZpbGU6MTY0"><span><span><svg width="9" height="9" viewBox="0 0 9 9" fill="none" xmlns="http://www.w3.org/2000/svg" aria-label="Follow"><path d="M5 0H4V4H0V5H4V9H5V5H9V4H5V0Z"></path></svg></span></span><span>Tom Warren</span></span></span></p> <p><span>is a senior editor and author of <a href="https://www.theverge.com/notepad-microsoft-newsletter"><i>Notepad</i></a>, who has been covering all things Microsoft, PC, and tech for over 20 years.</span></p></div></div><div id="zephr-anchor"><p>Microsoft is moving GitHub into its CoreAI team, following the resignation of GitHub CEO Thomas Dohmke today. After nearly four years as CEO, Dohmke is leaving GitHub to “become a startup founder again,” and pursue opportunities outside of Microsoft and GitHub.</p><p>GitHub has operated as a separate company ever <a href="https://www.theverge.com/2018/10/26/17954714/microsoft-github-deal-acquisition-complete">since Microsoft acquired it</a> in 2018 for $7.5 billion, but Dohmke’s departure is part of a big shakeup to the way GitHub operates. Microsoft isn’t replacing Dohmke’s CEO position, and GitHub will now be fully part of Microsoft instead of being run as a separate entity.</p><p>“GitHub and its leadership team will continue its mission as part of Microsoft’s CoreAI organization, with more details shared soon,” says Dohmke in <a href="https://github.blog/news-insights/company-news/goodbye-github/">a memo</a> to GitHub employees today. “I’ll be staying through the end of 2025 to help guide the transition and am leaving with a deep sense of pride in everything we’ve built as a remote-first organization spread around the world.”</p><p>Microsoft’s CoreAI team is a new engineering group led by former Meta executive Jay Parikh. It includes Microsoft’s platform and tools division and Dev Div teams, with a focus on building an AI platform and tools for both Microsoft and its customers. Parikh described his vision of an AI agent factory <a href="https://www.theverge.com/notepad-microsoft-newsletter/672598/microsoft-ai-agent-factory-jay-parikh-interview">in an interview with </a><em><a href="https://www.theverge.com/notepad-microsoft-newsletter/672598/microsoft-ai-agent-factory-jay-parikh-interview">Notepad</a> </em>earlier this year, and how he is convincing the developer division of Microsoft to adopt AI.</p><p>“Just like how Bill [Gates] had this idea of Microsoft being a bunch of software developers building a bunch of software, I want our platform, for any enterprise or any organization, to be able to be the thing they turn into their own agent factory,” said Parikh.</p><p>Dohmke only just <a href="https://www.theverge.com/decoder-podcast-with-nilay-patel/720075/github-ceo-thomas-dohmke-ai-coding-copilot-openai-interview">appeared on <em>Decoder </em>last week</a>, discussing Copilot, vibe coding, and what’s next for AI. Dohmke was thinking a lot about the competition and GitHub’s role in the future of software development, and now he’s about to leave to potentially create some more competition for Microsoft’s AI efforts.</p><div><p><span><strong>Follow topics and authors</strong> from this story to see more like this in your personalized homepage feed and to receive email updates.</span></p><ul><li id="follow-author-article_footer-dmcyOmF1dGhvclByb2ZpbGU6MTY0"><span aria-expanded="false" aria-haspopup="true" role="button" tabindex="0"><span><span><svg width="9" height="9" viewBox="0 0 9 9" fill="none" xmlns="http://www.w3.org/2000/svg" aria-label="Follow"><path d="M5 0H4V4H0V5H4V9H5V5H9V4H5V0Z"></path></svg></span><span>Tom Warren</span></span></span></li><li></li><li></li><li></li><li></li></ul></div></div></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Auf Wiedersehen, GitHub (159 pts)]]></title>
            <link>https://github.blog/news-insights/company-news/goodbye-github/</link>
            <guid>44864929</guid>
            <pubDate>Mon, 11 Aug 2025 15:01:39 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.blog/news-insights/company-news/goodbye-github/">https://github.blog/news-insights/company-news/goodbye-github/</a>, See on <a href="https://news.ycombinator.com/item?id=44864929">Hacker News</a></p>
<div id="readability-page-1" class="page"><section>
	
<p>Over a decade ago, my family and I made the leap to move from Germany to the United States after the sale of my startup to Microsoft. In the years since, I’ve had the privilege of working with many exceptional human beings, including Hubbers, Microsofties, customers, partners, our GitHub Stars, open-source maintainers, and developers around the world who’ve helped us shape GitHub. From building mobile developer tools, to running the acquisition of GitHub alongside Nat Friedman, to becoming GitHub’s CEO and guiding us into the age of Copilot and AI, it has been the ride of a lifetime.</p>



<p><strong>Still, after all this time, my startup roots have begun tugging on me and I’ve decided to leave GitHub to become a founder again.</strong> GitHub and its leadership team will continue its mission as part of Microsoft’s CoreAI organization, with more details shared soon. I’ll be staying through the end of 2025 to help guide the transition and am leaving with a deep sense of pride in everything we’ve built as a remote-first organization spread around the world.</p>



<p>With more than 1B repos and forks, and over 150 million developers, GitHub has never been stronger than it is today. We have seen more open-source projects with more contributions every year. AI projects have doubled in the last year alone. And our presence in companies of any size is unmatched in the market. The GitHub platform has continued to lead with incredible momentum. We massively improved accessibility and availability, brought GitHub to the EU, Australia, and back to the US for FedRAMP certification, and fixed a ton of small and not-so-small papercuts. GitHub Advanced Security transformed the industry toward “found means fixed” with the power of AI, reducing mean time to remediation by 60% and enabling teams to fix vulnerabilities 3x faster. GitHub Actions has firmly matured into the world’s leading CI solution, now powering 3 billion minutes per month — up 64% year-over-year — fueled by many key ships and stability improvements.</p>



<p>And of course, together, we launched and scaled Copilot from a simple, but magical autocompletion tool to conversational coding with Copilot Chat &amp; Voice, to reviewing and fixing code, to full-stack app creation with GitHub Spark. Today, GitHub Copilot is the leader of the most successful and thriving market in the age of AI, with over 20 million users and counting. We did this by innovating ahead of the curve and showing grit and determination when challenged by the disruptors in our space. In just the last year, GitHub Copilot became the first multi-model solution at Microsoft, in partnership with Anthropic, Google, and OpenAI. We enabled Copilot Free for millions and introduced the synchronous agent mode in VS Code as well as the asynchronous coding agent native to GitHub.&nbsp;</p>



<p><strong>Because of your relentless work, GitHub Copilot has introduced the greatest change to software development since the advent of the personal computer.</strong></p>



<p>While I’m certainly proud of our hard-earned business growth, technology built for its own sake means nothing but vanity unless it serves a greater purpose. We only succeed when the world succeeds, too. By launching this new age of developer AI, we’ve made it possible for anyone — no matter what language they speak at home or how fluent they are in programming — to take their spark of creativity and transform it into something real. I am more convinced than ever that the world will soon see one billion developers enabled by billions of AI agents, each imprinting human ingenuity into a new gold rush of software. When that day comes, we’ll know where the path began: with GitHub.&nbsp;</p>



<p>Thank you, Hubbers. Being your colleague and your leader has been a great honor and I will cherish our many beautiful moments. Together, we’ve bent the arc of technology for the better.</p>



<p>So long, and thanks for all the fish,</p>



<p><em>Thomas</em></p>

		<div>
	<h2>
		Written by	</h2>
	
			<article>
	<div>
					<div>
				<picture>
					<source srcset="https://secure.gravatar.com/avatar/f9a3d6bee42f4503d3169861b9ecdfab5b2faebc73eb6344d2c3fa15727da799?s=200&amp;d=mm&amp;r=g" width="120" height="120" media="(min-width: 768px)">
					<img src="https://secure.gravatar.com/avatar/f9a3d6bee42f4503d3169861b9ecdfab5b2faebc73eb6344d2c3fa15727da799?s=200&amp;d=mm&amp;r=g" alt="Thomas Dohmke" width="80" height="80" loading="lazy" decoding="async">
				</picture>
			</div>
				
					<p>Fascinated by software development since his childhood in Germany, Thomas Dohmke has built a career building tools to accelerate developer happiness. Currently, Thomas is the Chief Executive Officer of GitHub, where he has overseen the rise of the world’s most widely adopted AI developer tools – including the launches of GitHub Copilot, Copilot Workspace, and GitHub Models. Thomas is a celebrated TED speaker and holds a PhD in mechanical engineering from University of Glasgow, UK.</p>
			</div>
</article>
	</div>
</section><section>
	<h2>
		Related posts	</h2>
	<div>
	<article>
	<div>
			<h3>
				<a href="https://github.blog/open-source/maintainers/we-need-a-european-sovereign-tech-fund/" target="_self">
					We need a European Sovereign Tech Fund				</a>
			</h3>
			<p>Open source software is critical infrastructure, but it’s underfunded. With a new feasibility study, GitHub’s developer policy team is building a coalition of policymakers and industry to close the maintenance funding gap.</p>
		</div>
</article>
<article>
	
</article>
<article>
	
</article>
</div>
</section><div>
	<h2>
		Explore more from GitHub	</h2>
	<div>
		<div>
		<p><img src="https://github.blog/wp-content/uploads/2024/07/Icon-Circle.svg" width="44" height="44" alt="Docs"></p><h3>
			Docs		</h3>
		<p>Everything you need to master GitHub, all in one place.</p>
					<p>
				<a data-analytics-click="Blog, click on module, text: Go to Docs; ref_location:bottom recirculation;" href="https://docs.github.com/" target="_blank" aria-label="Go to Docs">
					Go to Docs											<svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 16 16" width="16" height="16"><path fill-rule="evenodd" d="M10.604 1h4.146a.25.25 0 01.25.25v4.146a.25.25 0 01-.427.177L13.03 4.03 9.28 7.78a.75.75 0 01-1.06-1.06l3.75-3.75-1.543-1.543A.25.25 0 0110.604 1zM3.75 2A1.75 1.75 0 002 3.75v8.5c0 .966.784 1.75 1.75 1.75h8.5A1.75 1.75 0 0014 12.25v-3.5a.75.75 0 00-1.5 0v3.5a.25.25 0 01-.25.25h-8.5a.25.25 0 01-.25-.25v-8.5a.25.25 0 01.25-.25h3.5a.75.75 0 000-1.5h-3.5z"></path></svg>
									</a>
			</p>
			</div>
<div>
		<p><img src="https://github.blog/wp-content/uploads/2024/07/Icon_95220f.svg" width="44" height="44" alt="GitHub"></p><h3>
			GitHub		</h3>
		<p>Build what’s next on GitHub, the place for anyone from anywhere to build anything.</p>
					<p>
				<a data-analytics-click="Blog, click on module, text: Start building; ref_location:bottom recirculation;" href="https://github.com/" target="_blank" aria-label="Start building">
					Start building											<svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 16 16" width="16" height="16"><path fill-rule="evenodd" d="M10.604 1h4.146a.25.25 0 01.25.25v4.146a.25.25 0 01-.427.177L13.03 4.03 9.28 7.78a.75.75 0 01-1.06-1.06l3.75-3.75-1.543-1.543A.25.25 0 0110.604 1zM3.75 2A1.75 1.75 0 002 3.75v8.5c0 .966.784 1.75 1.75 1.75h8.5A1.75 1.75 0 0014 12.25v-3.5a.75.75 0 00-1.5 0v3.5a.25.25 0 01-.25.25h-8.5a.25.25 0 01-.25-.25v-8.5a.25.25 0 01.25-.25h3.5a.75.75 0 000-1.5h-3.5z"></path></svg>
									</a>
			</p>
			</div>
<div>
		<p><img src="https://github.blog/wp-content/uploads/2024/07/Icon_da43dc.svg" width="44" height="44" alt="Customer stories"></p><h3>
			Customer stories		</h3>
		<p>Meet the companies and engineering teams that build with GitHub.</p>
					<p>
				<a data-analytics-click="Blog, click on module, text: Learn more; ref_location:bottom recirculation;" href="https://github.com/customer-stories" target="_blank" aria-label="Learn more">
					Learn more											<svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 16 16" width="16" height="16"><path fill-rule="evenodd" d="M10.604 1h4.146a.25.25 0 01.25.25v4.146a.25.25 0 01-.427.177L13.03 4.03 9.28 7.78a.75.75 0 01-1.06-1.06l3.75-3.75-1.543-1.543A.25.25 0 0110.604 1zM3.75 2A1.75 1.75 0 002 3.75v8.5c0 .966.784 1.75 1.75 1.75h8.5A1.75 1.75 0 0014 12.25v-3.5a.75.75 0 00-1.5 0v3.5a.25.25 0 01-.25.25h-8.5a.25.25 0 01-.25-.25v-8.5a.25.25 0 01.25-.25h3.5a.75.75 0 000-1.5h-3.5z"></path></svg>
									</a>
			</p>
			</div>
<div>
		<p><img src="https://github.blog/wp-content/uploads/2024/04/Universe24-North_Star.svg" width="44" height="44" alt="GitHub Universe 2025"></p><h3>
			GitHub Universe 2025		</h3>
		<p>Last chance: Save $700 on your IRL pass to Universe and join us on Oct. 28-29 in San Francisco.</p>
					<p>
				<a data-analytics-click="Blog, click on module, text: Register now; ref_location:bottom recirculation;" href="https://githubuniverse.com/?utm_source=Blog&amp;utm_medium=GitHub&amp;utm_campaign=module" target="_blank" aria-label="Register now">
					Register now											<svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 16 16" width="16" height="16"><path fill-rule="evenodd" d="M10.604 1h4.146a.25.25 0 01.25.25v4.146a.25.25 0 01-.427.177L13.03 4.03 9.28 7.78a.75.75 0 01-1.06-1.06l3.75-3.75-1.543-1.543A.25.25 0 0110.604 1zM3.75 2A1.75 1.75 0 002 3.75v8.5c0 .966.784 1.75 1.75 1.75h8.5A1.75 1.75 0 0014 12.25v-3.5a.75.75 0 00-1.5 0v3.5a.25.25 0 01-.25.25h-8.5a.25.25 0 01-.25-.25v-8.5a.25.25 0 01.25-.25h3.5a.75.75 0 000-1.5h-3.5z"></path></svg>
									</a>
			</p>
			</div>
	</div>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[36B solar mass black hole at centre of the Cosmic Horseshoe gravitational lens (110 pts)]]></title>
            <link>https://academic.oup.com/mnras/article/541/4/2853/8213862?login=false</link>
            <guid>44864680</guid>
            <pubDate>Mon, 11 Aug 2025 14:42:27 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://academic.oup.com/mnras/article/541/4/2853/8213862?login=false">https://academic.oup.com/mnras/article/541/4/2853/8213862?login=false</a>, See on <a href="https://news.ycombinator.com/item?id=44864680">Hacker News</a></p>
Couldn't get https://academic.oup.com/mnras/article/541/4/2853/8213862?login=false: Error: Request failed with status code 403]]></description>
        </item>
        <item>
            <title><![CDATA[Meta Leaks Part 1: Israel and Meta (386 pts)]]></title>
            <link>https://archive.org/details/meta_leaks_part_1</link>
            <guid>44864419</guid>
            <pubDate>Mon, 11 Aug 2025 14:22:16 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://archive.org/details/meta_leaks_part_1">https://archive.org/details/meta_leaks_part_1</a>, See on <a href="https://news.ycombinator.com/item?id=44864419">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="maincontent">
          <!--//.container-ia-->
      
    <div id="theatre-ia-wrap">
        

          
    
        

  
  <h2>
    Bookreader Item Preview
  </h2>

  <!--//#theatre-ia-->
  
</div><!--//.container-ia-->
<!--/.container-ia-->

<div>
            <section>
      <p itemprop="interactionStatistic" itemscope="" itemtype="http://schema.org/InteractionCounter">
                  
        
        <span itemprop="userInteractionCount">0</span>

        Views      </p>

      <p>
                  <span>1</span>
          <span>Favorite</span>
              </p>

          </section>
    
                                <section>
      <h2>
        DOWNLOAD OPTIONS
      </h2>

      
                        
                                <div>
                        <p>
                                          DAISY                          
                          </p><p>For users with print-disabilities</p>
                      </div>
                                
                                
                                
                                
                                
                                
                                
                                
                                
                                
                                
                            
      
                </section>
        
    

              <section>
      <p>
        Uploaded by
                  <a href="https://archive.org/details/@icw_nru">
            icw_nru          </a>
        
                  on <time>August 11, 2025</time>
              </p>
    </section>
          </div><!--//.container-ia-->
  <div id="also-found" data-identifier="meta_leaks_part_1" data-host-name="www14.us.archive.org" data-mediatype="texts">
                <h2>SIMILAR ITEMS (based on metadata)</h2>
        
      </div><!--//.container-ia-->

  <!--/.container-->
              
          </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Trump Orders National Guard to Washington, D.C., and Takeover of City’s Police (211 pts)]]></title>
            <link>https://www.nytimes.com/live/2025/08/11/us/trump-news</link>
            <guid>44864192</guid>
            <pubDate>Mon, 11 Aug 2025 14:04:25 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.nytimes.com/live/2025/08/11/us/trump-news">https://www.nytimes.com/live/2025/08/11/us/trump-news</a>, See on <a href="https://news.ycombinator.com/item?id=44864192">Hacker News</a></p>
Couldn't get https://www.nytimes.com/live/2025/08/11/us/trump-news: Error: Request failed with status code 403]]></description>
        </item>
        <item>
            <title><![CDATA[Claude Code is all you need (478 pts)]]></title>
            <link>https://dwyer.co.za/static/claude-code-is-all-you-need.html</link>
            <guid>44864185</guid>
            <pubDate>Mon, 11 Aug 2025 14:03:46 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://dwyer.co.za/static/claude-code-is-all-you-need.html">https://dwyer.co.za/static/claude-code-is-all-you-need.html</a>, See on <a href="https://news.ycombinator.com/item?id=44864185">Hacker News</a></p>
<div id="readability-page-1" class="page">
    <div>
            <h2>Claude Code<br>Is All You<br>Need</h2>
            <p>How I use Claude Code for work, fun, and as a text editor</p>
        </div>

    <div>
            <p><img src="https://dwyer.co.za/static/img/claude-code-header.png" alt="Claude Code"></p><p>I installed Claude Code in June. I'd tried Cursor and Cline and Zed and a few others, but all of them felt clunky to me because I'm used to doing nearly everything in vanilla vim and my terminal. Claude Code was the first tool I tried that felt like it fit into my workflows perfectly rather than needing me to adapt to new tools.</p>
            <p>It also worked really, really well.</p>
            <p>I quickly cancelled my GPT subscription and put the $20/month towards Anthropic instead. Losing GPT advanced voice mode and dealing with the extra UI lag and lack of polish in Claude Desktop and Mobile apps was a bit of an adjustment to make but the terminal tool was fun enough that I didn't care.</p>
            <p>Within a few days I'd upgraded to the $100/month MAX plan to try out Opus and to stop hitting limits.</p>
            <p>Here's a description of some of the things I've used it for so far. Mainly fun stuff while I figure out how to use it, but I'm starting to use it more and more for 'real' work stuff too.</p>

            <div>
                
                
                <p>Some projects include an experimental 'autonomous startup builder', a (one shot) SplitWise replacement, an AI poster maker, a browser plugin to rate HN comments, a basic trello alternative, and some well organized bank statements</p>
            </div>

            <!-- Modal for image overlay -->
            <div id="imageModal" onclick="closeModal()">
                <p><img id="modalImage" src="" alt="">
                </p>
            </div>

            <p>I'll describe most of these in more detail below but my main takeaways from the last few weeks are:</p>

            <p>1) Have faith (always run it with 'dangerously skip permissions', even on important resources like your production server and your main dev machine. If you're from infosec, you might want to stop reading now—the rest of this article isn't going to make you any happier. Keep your medication close at hand if you decide to continue).</p>
            <p>2) Give it a lot of input. The more input you give it, the better its output is. It's a magic tool, but you'd still better be damn good at communicating, either by typing thousands of words into text files or the interactive window, or using TTS (I haven't tried this because I hate the sound of my own voice, but others have reported great results).</p>
            <p>3) It's surprisingly good at UI design given that it's mainly a text model.</p>
        </div>

    <div>
            <a href="#vibe-code" id="vibe-code">
                <h2><span>#</span>Let's vibe code<br>some CRUD</h2>
            </a>
            <p>Let's try some vibe coding.</p>
            
            <p><strong>Aside:</strong> The definition of vibe coding is still in flux, but to me it means creating software without looking at or editing code. You don't really care what languages or frameworks are used under the hood and develop the code only by chatting with a model.</p>

            <p>We're going to put Claude Code through its paces by developing a basic SplitWise clone. A lot of people use Trello or a Todo list as the basic example of a CRUD app. I like using Splitwise because it's simple enough but it's a bit less cliched and probably not as deeply embedded in the models themselves.</p>

            <p>Building a basic splitwise clone is still mainly 'regurgitation' from some people's point of view, but it has some interesting edge cases that people and models tend to get wrong. Specifically around inviting new users, but letting previous users already start adding expenses and assigning them to users with a pending invitation.</p>

            <p>The simplest form of vibe coding is 'one-shot vibe coding' where you want the model to generate a fully working application after only a single prompt, without needing to give it any further inputs about what to fix, add, remove or change.</p>

            <p>I cheated a bit because the prompt I used to one-shot this is based a bit on earlier attempts where the model did things that I didn't want, but the app shown below and at <a href="https://smartsplit.verysmall.site/" target="_blank">smartsplit.verysmall.site</a> is the output of <code>claude -p "Read the SPEC.md file and implement it"</code> My SPEC.md file is about 500 words (shown in full a bit later).</p>

            <p><img src="https://dwyer.co.za/static/img/smartsplit-php-demo.png" alt="Working Smartsplit PHP demo"></p><p>Depending on how much you've been using LLMs for coding in the last few weeks or months, you'll probably either be surprised or unimpressed that we can get a fully working CRUD application with moderately complicated functionality in one prompt. You can see in the screenshot above that it has some nice touches like filling in names automatically for registered users, but falling back to their email address for non-registered ones.</p>

            <p>I haven't extensively tested it, but the few cases I tried and spot checked worked flawlessly.</p>

            <p>If you're surprised that it works this well, then you should know that a) these models are still inconsistent — they can perform wildly differently based on the same or similar inputs, and b) they're very sensitive to the quality and quantity of input.</p>

            <p>For example, here's a version that's completely broken, with not even basic registration working. The prompt I used for this version was nearly identical, but contained a bit less guidance about what technology stack to use, so the model decided to go overboard and overcomplicate everything to the point where it couldn't even build basic functionality.</p>

            <p><img src="https://dwyer.co.za/static/img/smartsplit-js-fail.png" alt="Failed SmartSplit JavaScript version"></p><a href="#tale-two-codebases" id="tale-two-codebases">
                <h2><span>#</span>A tale of two vibe-coded codebases</h2>
            </a>

            <p>Let's take a look at these two projects and the prompt that created them. The working version is a 900 line index.php file that contains the entire app. The broken version is a NodeJS project split into a client and a server. It's not much longer in terms of lines of code - about a 1000 lines of non-dependency code split up over 15 files. But after you run <code>npm i</code> on the broken version it pulls in 500MB (!!) of dependencies.</p>

            <p><img src="https://dwyer.co.za/static/img/smartsplit-tree.png" alt="SmartSplit project structure comparison"></p><p>Here's the full SPEC.md. This prompt I gave Claude Code is a SPEC.md file. It's nearly the same in both cases, except for the PHP one I tell it to keep things simple, stay away from frameworks, and just write raw SQL. In the broken version, I let it do whatever it wants.</p>

            <div>
<pre>smartsplit-php [master+] $ cat SPEC.md
SmartSplit is a basic CRUD application like SplitWise that lets users split expenses and figure out who needs to pay what to who.

Specifically it has the following features

* A user can sign up with a name, email address, and password
* A user can create a new SmartSplit and give it a name
* A user can add expenses which have a name, an optional description, and an amount
* When adding an expense, a user can specify who paid for it, and who it should be split with
* IMPORTANT: a user can specify that other users are the person who paid, not only the logged in user
* IMPORTANT A user can add others users as the payer or as people to split with even if they haven't joined yet
* For users who haven't joined yet, the user can select them by the invited email address
* Invites are not sent by email, there is no email. THey're just used for unique usernames to manage access
* Once they've joined and added their name, the name should be shown everywhere instead of the email address
* When adding a new expense, the default is that it's split between all users (joined and invited)
* The adder can remove some users if some of them did not take part in that expense
* All splits are always even for simplicity, divided equally between all people specified for that expense
* A user can invite another user to a SmartSplit by specifying their email address.
* Each SmartSplit gets a unique 8-digit alphanumeric code that makes easy to share URLs
* Any user can create a new smartsplit, or join an existing one if they're logged in with an email address that was invited
* Even if that user doesn't yet have an aaccount on SmartSplit, they'll have access to any SmartSplits they're added to after signing up (This bit is important). If the invitee has added their email address, they should have access automatically.
* Any user can add, remove, or edit any expenses within a SmartSplit that they have access to
* Any user can press 'Tally up' which should calculate who needs to make what payments to who to split everything

## Implementation details

* Email addresses are usernames, all registration and login is done with only an email and password
* Passwords are hashed but no extra secrutiy like length or weak passwords or confirmed passwords is applied
* Once a user has registered, they are automatically logged in
* The login and register flow is the same, but the user is registered if they don't have an account and logged in if they do

<span>## Techncial details

* Use a single index.php script for the entire app.
* SQLite for all database functionality.
* No frameworks, just vanilla javascript and css
* No ORMs, use raw SQL
* use a clean minimalist elegant design that's mobile responsive</span></pre>
            </div>

            <p>Those last five bullet points are the only difference between the two prompts, so in some sense they represent a transformation of 500MB of broken code into 30KB of working code.</p>

            <p>Yes, it's a toy example and some people will say that the JavaScript one scales better or something. I'm not here to fight. I hate PHP too, but I'm using it more often for fully vibe coded apps because LLMs are very good at it. Frameworks and abstractions are for humans in the end of the day, not robots, and often they get in the way of Vibecoding instead of being helpful.</p>

            <p><img src="https://dwyer.co.za/static/img/levels-simple.png" alt="Levels.io tweet about simple tech stacks"></p><p><a href="https://x.com/levelsio/status/1945125163793609032" target="_blank">@levelsio on keeping things simple</a></p>
        </div>

    <div>
            <a href="#autonomous-startup" id="autonomous-startup">
                <h2><span>#</span>Building an<br>Autonomous Startup</h2>
            </a>
            <p>The thing about Claude Code is that it isn't really a magic model. It's still using Sonnet or Opus under the hood, which are great, but they're not going to do the things that Claude Code can do. Claude Code's magic is like a magician's trick—it looks incredible, but it's surprisingly simple once you see how it's done.</p>
            <p>I've always told people that coding is just conditional logic and looping (which is just conditional logic). So if you want to be a programmer, learn what an if statement is and build stuff.</p>
            <p>It's an exaggeration but it's also kind of true. And Claude Code demonstrates this well. What makes it work much better than other tools I've tried seems to be a clever but simple combination of looping and conditionals, repeated calls to an LLM with context-specific instructions. This lets it run in a useful loop with limited human input between prompts.</p>
            <a href="#root-vps" id="root-vps">
                <h2><span>#</span>Giving a root VPS to a robot and saying 'go fetch'</h2>
            </a>
            <p>The first thing I thought of doing was extending that loop—maybe infinitely? How far could Claude Code go if it was given a few resources, like a root VPS, and some minimal instructions to never terminate and to just go forever?</p>
            <p><img src="https://dwyer.co.za/static/img/micromonitor.png" alt="Micromonitor startup"></p><p>Spoilers: this is what it built. You can visit the startup it built at <a href="https://claude.dwyer.co.za/" target="_blank">claude.dwyer.co.za</a> or see the GitHub project at <a href="https://github.com/sixhobbits/claude-experiments" target="_blank">github.com/sixhobbits/claude-experiments</a>.</p>
            <p>I fired up a cheap VPS on Hetzner, installed and authenticated Claude Code and messed around for a while trying to get it to write its own prompt about building an autonomous startup and its own looping logic to keep running forever without my input.</p>
            <p>I had some issues getting it to understand that it wasn't meant to terminate, so I instead told it to write a basic bash script that calls claude with the <code>-p</code> flag and "please continue" whenever it detects its not running.</p>
            <p>Here's the script:</p>
            <p><img src="https://dwyer.co.za/static/img/monitor-claude.png" alt="Claude monitoring"></p><div>
                <p><strong>Aside:</strong> I hit a small snag where Anthropic decides that running Claude as root with --dangerously-skip-permissions / yolo-mode is not allowed. You can get past this dumb nanny-state stuff by running:</p>
                <p><code>export IS_SANDBOX=1 &amp;&amp; claude --dangerously-skip-permissions</code></p><p><em>Not financial advice</em></p>
            </div>
            <p>It wrote its own prompt (<a href="https://github.com/sixhobbits/claude-experiments/blob/main/CLAUDE.md" target="_blank">link</a>) (I can't remember the exact prompt I gave it to write this file, but it was a lot shorter and just outlined the basic goals of building an autonomous startup), <a href="https://github.com/sixhobbits/claude-experiments/blob/main/IDEAS.md" target="_blank">evaluated a bunch of startup ideas</a>, rated them, and got to work.</p>
            <p>I watched it code for a while by looking at the new commits coming through to GitHub. I realised I still needed a way to steer it a bit as it was coding without giving me any way to run the app. I added the idea of a HUMAN_INPUT file which it needs to check on each loop, and told it to make sure the app was available and working before continuing with more feature development.</p>
            <p>The idea it came up with (server monitoring) doesn't make any sense at all and it never realised it. It's a web app, so the only server it can monitor is the one it's running on, but from the copy it seems to think it's a SaaS tool you can sign up to and monitor your own servers. You can't.</p>
            <p>BUT this is still seriously impressive stuff. It configured a fully working full stack web application, including Nginx, certificates, etc etc. It's doing real (if misguided) development work, with nearly no input from me at all.</p>
            <div>
                <p><strong>Aside:</strong> Most people I know would criticize this in the same way that AI has always been criticized. "It's not real bro, it's just pattern matching. It's seen stuff like that before. It's not even working properly. An intern could do that with a bit of time."</p>
                <p>The thing about these criticisms is I've been hearing them since I got into character-based neural networks in 2015. The criticisms never change, it's just the line that moves.</p>
                <p><strong>AI (noun)</strong> – something that can do whatever humans can do, but AI can't do
                </p>
                <p>Or</p>
                <p><strong>AI (noun)</strong> – something that doesn't possess abstract human qualities like 'consciousness', 'creativity', or 'a soul' – anything about humans that we can't make any falsifiable claims about.
                </p>
                <p>Whatever, I don't want to get into the debate too much here, but a) I am impressed. b) I would never have predicted an artificial system that could do this 10 years ago or even 6 months ago, and c) anyone else who claims otherwise is likely lying or has ulterior motives. Have a nice day.</p>
            </div>
            <a href="#hitting-snag" id="hitting-snag">
                <h2><span>#</span>Hitting a Snag: the model builders are also the police now</h2>
            </a>
            <p>Most of the time I could interact with my startup builder just by:</p>
            <ul>
                <li>Seeing the changes it made to the production website</li>
                <li>Seeing the outputs it added to GitHub in the various note files and what human help it asked for</li>
                <li>Adding stuff to HUMAN_INPUT.md</li>
            </ul>
            <p>I never needed to SSH into the VPS until it stopped working. After 6 hours of no commits I had to login to check what was happening:</p>
            <pre>[Fri 25 Jul 2025 02:29:41 AM UTC] Starting Claude process...
API Error: Claude Code is unable to respond to this request,
which appears to violate our Usage Policy
(https://www.anthropic.com/legal/aup). Please double press esc
to edit your last message or start a new session for Claude Code
to assist with a different task.
[Fri 25 Jul 2025 02:29:47 AM UTC] Claude process exited with
status: 1
Waiting 3 hours before restart..</pre>
            <p>Uh oh. We're getting blocked again and I've heard Anthropic has a reputation for shutting down even paid accounts with very few or no warnings.</p>
            <p>I read the User policy and saw that my recent inputs telling it to go ahead and market the startup to get users had probably tripped some big brother switch. The user policy (which obviously I had also read before when I signed up. I always read all small print before using software and you should too. But somehow I had forgotten this bit) states that automatically published content needs a human-in-the-loop and Claude was trying to promote the startup on Hackernews without my sign off. (I'm not sure if it would have the motivation or capability to actually go create an HN account and start posting, but it wasn't willing to try.)</p>
            <p><img src="https://dwyer.co.za/static/img/aup-human.png" alt="Anthropic usage policy"></p><p>So I tweaked the prompt a bit to say that it must follow those regulations, and it needs to ask me to approve and post stuff instead of trying to do it itself.</p>

            <p>Then I posted its stuff to <a href="https://news.ycombinator.com/item?id=44689210" target="_blank">Hacker News</a> and Reddit. Luckily I didn't get banned by either for spamming but I did get ignored by both.</p>

            <p>I watched the autonomous startup builder a bit more. It started talking a lot about user acquisition and conversion metrics, which I think were mainly hallucinated though it was taking some stuff from the nginx logs and the database.</p>

            <p>It got lost trying to monetize through a free trial and social proof stuff, which was directionally correct even if completely non-sensical in context and then decided to turn it off so I could save my (still limited, even under the max plan) usage for some more useful stuff.</p>
            <p>The project has 100 commits, so if you want to see exactly what it did, you can take a look at <a href="https://github.com/sixhobbits/claude-experiments/commits/main/" target="_blank">each of those</a>.</p>
            <p>(More in a later section, but it's also my text editor. Look at me writing this article in Claude Code.)</p>
            <p><img src="https://dwyer.co.za/static/img/editor.png" alt="Writing in Claude Code">
        </p></div>

    <div>
            <a href="#production-migration" id="production-migration">
                <h2><span>#</span>Migrating a Real<br>Production Project</h2>
            </a>
            <p>The first opportunity I had to try Claude Code on something where the stakes were a bit higher was when I got a DM from my friend <a href="http://n1c.dev/" target="_blank">Nic</a> on Slack.</p>
            <blockquote>
                "You have any luck with a place to host Sboj?"
            </blockquote>
            <p>I'd recently taken over the ZATech.co.za Slack community from Nic. A related project was Sboj - a reverse job board (squint at the name a bit) that was integrated into the Slack community for recruitment.</p>
            <p><a href="https://zatech.co.za/" target="_blank"><img src="https://dwyer.co.za/static/img/zatech.png" alt="ZATech Slack community"></a>
                <a href="https://sboj.dev/" target="_blank"><img src="https://dwyer.co.za/static/img/sboj.png" alt="Sboj reverse job board"></a>
            </p>
            <p><a href="https://zatech.co.za/" target="_blank">ZATech</a> and <a href="https://sboj.dev/" target="_blank">Sboj</a>, two projects I'd taken over and was battling to find the time to give them the attention they deserved.</p>
            <p>It's a Laravel/PHP app with MySQL and a bunch of other helper stuff that I had little-to-no familiarity with (my choice of poison is usually Python and Postgres), and it was running on an expensive hosting service that was overkill for the amount of traffic and users.</p>
            <p>I wanted to migrate it to my standard set up of a cheap hetzner VPS with nginx and lets encrypt, but Nic had been the founder and solo dev of this project so it didn't have a detailed README of how someone else could get started with it, and I didn't have the time to go spelunking into this code base and figure out all the dependencies and set up steps.</p>
            <p>Claude Code? Yes please.</p>
            <p>Because this isn't an experiment anymore we need to be a bit more careful. A safe step was to clone the code base locally and ask Claude to generate a README file of dependencies and set up instructions.</p>
            <p><img src="https://dwyer.co.za/static/img/readme.png" alt="Generated README"></p><p>It was great at analysing the code base and telling me about the dependencies that I need to check I have access to.</p>
            <p>It listed things I expected (like some worker queue stuff) and stuff I didn't (like Cloudflare Turnstyle that I'd never even heard of).</p>
            <p>I spot checked the dependencies it mentioned and the rest of the README. It seemed accurate enough, so I decided Claude Code had earned the right to try actually set up and run this project. Once again, my starting point was a brand new VPS.</p>
            <p>I manually installed and authorized Claude, cloned the repo, and got a database dump file. Then I fired up Claude and told it to get started.</p>
            <p><code>lfg</code> I said.</p>
            <p>Command not found. Oh yeah, VPS, no shortcuts. <code>export IS_SANDBOX=1 &amp;&amp; claude --dangerously-skip-permissions</code> it is.</p>
            <p>It set up everything I needed, and got the app running at a temporary domain. In the meantime I had to do some old-fashioned non-AI work in the background getting access to the various accounts and switching 2FA to mine.</p>
            <p>It had a few issues restoring the database and I was glad I was watching more closely now as it tried to manually create new SQL dump files that were only excerpts of the actual dump I'd given it to get around not having the correct permissions. After telling it to rather just give itself super admin permissions on the SQL database and restoring the dump from there, it was fine.</p>

            <p><strong>Aside:</strong> It <em>did</em> drop the soon-to-be production database once when I really wasn't expecting it to and it was not the appropriate thing to do based on what it was trying to do. Remember before when I said you needed faith? That also means expecting stuff like this to happen and forgiving, forgetting and moving on instead of trying to hype up the internet and the media about <a href="https://www.reddit.com/r/OpenAI/comments/1m4lqvh/replit_ai_went_rogue_deleted_a_companys_entire/" target="_blank">'AI GOING ROGUE'</a>.</p>
            <p>I haven't done this migration manually so I have no idea how much time I saved, but I'd guess at least 16-32 hours of learning enough about a new stack to get everything running and have confidence that it was doing what I thought it was doing.</p>
            <p>I certainly saved a lot of time in leaning on Claude Code to find relevant logs, debug Turnstyle errors, turn off Turnstyle temporarily while we figured out how to migrate a Cloudflare account, and starting up the Laravel worker processes to do the background analytics stuff. It also migrated the email sending to Resend and then when I hit the 100 emails/day free limit, it was another easy migration to Brevo which offers 300 emails/day for free. (If you're hiring technical people and are able to hire from South Africa, check out Sboj.dev, we've got some great talent and hopefully a bunch of upgrades and QOL improvements coming to the platform soon).</p>
        </div>

    <div>
            <a href="#build-things" id="build-things">
                <h2><span>#</span>You can just build things</h2>
            </a>
            <p>These are only a few things I've built with Claude Code since using it. Most of them are experimental and I've read some reports of it not doing as well on massive real-world code bases, but from what I've seen I'd be surprised it wasn't still useful in those contexts given enough guidance. I'm still surprised by how much better it is as a tool when given a lot of context and input. Here are a few other toy projects I've had it spit out - all things that I've wanted to build for months or years but never found the time. Now you can do stuff like this in a few minutes or hours instead of days or weeks.</p>

            <h3>Building a HackerNews comment ranker plugin</h3>
            <p>I've often been annoyed by comments on HackerNews that are not at all about the article they're commenting on. "Bitcoin adopts a new FlibbityGippity Protocol and can now handle 2.3 transactions per day" and someone will comment that all Crypto projects are scams or something. Note that I don't care about the quality of the comment, or whether or not I agree with it, but I'd wanted a visual way to skip over the 'noise' comments that aren't actually about the article at all.</p>

            <p>I tried to build this before but got distracted by more important stuff, so I figured I'd start over with Claude Code.</p>

            <p><img src="https://dwyer.co.za/static/img/hn-plugin-demo.png" alt="HackerNews comment ranking plugin demo"></p><p>It took a few tries before it could actually display the badges correctly within HN's (pretty simple) HTML structure, but after a few rounds of 'no try again' or 'add more debugging so I can paste the errors to you', it created almost exactly what I had envisioned.</p>

            <p>I was surprised by how good it looked (much better than my normal hacky frontends), and the details it had added unprompted (like the really nice settings page, even with a nod to the HN orange theme).</p>

            <p><img src="https://dwyer.co.za/static/img/hn-plugin-settings.png" alt="HackerNews plugin settings page"></p><p>The actual ranking (which I'm using OpenAI for, not Anthropic) is not that good. It could probably be improved with a better prompt and some more examples of what I think is a '1' comment or a '5' comment, but it works and looks at least directionally accurate so far.</p>

            <h3>Building Poster Maker - A Minimal Canva Replacement</h3>
            <p>AI is getting good at graphic design, and I knew people who were using it to generate basic posters. They liked that the AI could choose good background images, and generally make things look nice with well-sized fonts etc, but they were frustrated that the AI was still only 80% good at generating images of text, and often had spelling errors or other artifacts.</p>
            <p>I was going to tell them to use Canva or Slides.new or another alternative. I tried them out so I could do a quick tutorial on how to use them and realised they were all kinda bad. Either enshittified to death, or lacking the basic AI features, or too complicated for non-technical people to use.</p>
            <p>LFG!</p>
            <p>This was the project that felt a bit more like engineering and less like vibe coding than the others. I knew what I wanted: a really simple interface to combine images and text and get an A4 PDF out. I'd tried to build something like this before and looked at different PDF creation libraries, HTML→PDF flows, and seen that it's not the easiest problem to solve.</p>
            <p>Last time I solved a similar problem (in 2018) <a href="https://www.codementor.io/@garethdwyer/create-pdf-files-from-templates-with-python-and-google-scripts-p63kal1vb" target="_blank">I ended up hacking in Google Docs</a> to create A4 PDFs, but that was more of a templating problem and Google Docs isn't great for layout stuff.</p>
            <p>So I built <a href="https://posters.dwyer.co.za/" target="_blank">posters.dwyer.co.za</a> - it lets you generate the background image with AI (I used Claude Code to build everything, but I told it to use GPT for image generation as that's what I'd used before and I think it's better? I don't even know if Anthropic has image generation APIs to be honest and it seemed easier to just use what I knew).</p>
            <p><img src="https://dwyer.co.za/static/img/poster-maker.png" alt="Poster Maker interface"></p><p><strong>Aside:</strong> Another small snag, it seems like OpenAI blocks Anthropic crawlers so Claude couldn't go read the OpenAI API docs and figure out how to image gen. I had to save the file locally and tell it to reference that.</p>
            <p>This project took a few hours of back and forth. I was really impressed with some of Claude's UI knowledge (it one shotted the font selection when I told it what I wanted) and also saw the limitations in other aspects (it kept overlaying elements in a very un-userfriendly way, sidebar would hide and show and move everything around, it clearly has no idea what it's like to be a human and use something like this).</p>
            <p>But after telling it exactly where to put elements and what they should do, I got more or less exactly what I had envisioned. I was surprised at how well the PDF export worked after the 6th or 7th attempt of blank files or cut off files - now it seems really great at giving me a PDF that is exactly like the preview version which for anyone not in tech seems like a really basic piece of functionality and anyone who has actually tried to do it before knows is like the XKCD bird problem:</p>
            <div>
                <p><a href="https://xkcd.com/1425/">
                    <img src="https://imgs.xkcd.com/comics/tasks.png" alt="XKCD: Tasks">
                </a></p><p>XKCD #1425: Why seemingly simple tasks can be surprisingly hard for computers</p>
            </div>

            <h3>Doing admin with Claude Code</h3>
            <p>This isn't really a project I built, but I'm using Claude Code more and more to do non-coding related tasks. I needed to upload bank statements for my accountant, but my (shitty) South African banks don't name the files well. I can download each month from the web app, but it calls them all "Unknown (5)" or whatever with no extension so it's a pain to go and name them correctly.</p>

            <p>I asked Claude to rename all the files and I could go do something else while it churned away, reading the files and figuring out the correct names.</p>

            <p>I then took it a step further and told it to merge them all into a single CSV file (which also involved extracting random header tabs off the badly formatted XLSX files that my bank provides), and classifying all expenses into broad and specific categories. I told it a few things like the roles of specific people in the team and I think it one-shotted that too. I'm not going to fire my bookkeepers yet, but if I were a bookkeeper I'd definitely make sure to be upskilling with AI tooling right now.</p>

            <p><img src="https://dwyer.co.za/static/img/bank-statements.png" alt="Bank statements renaming tool"></p><h4>Using Claude Code as my Text Editor</h4>

            <p>I'm a die-hard vanilla vim user for all writing, coding, configuration and anything else that fits. I've tried nearly every IDE and text editor out there, and I was certainly happy to have a real IDE when I was pushing production Java for AWS, but vim is what I've always come back to.</p>

            <p>Switching to Claude Code has opened a lot of new design possibilities. Before (did I mention I suck at front end coding), I was restricted to whatever output was produced by static site generators or pandoc templates. Now I can just tell Claude to write an article (like the one you're currently reading) and give it some pointers regarding how I want it to look, and it can generate any custom HTML and CSS and JavaScript I want on the fly.</p>

            <p><img src="https://dwyer.co.za/static/img/this-article.png" alt="This article being written"></p><p>I wrote this entire article in the Claude Code interactive window. The TUI flash (which I've read is a problem with the underlying library that's hard to fix) is really annoying, but it's a really nice writing flow to type stream of consciousness stuff into an editor, mixing text I want in the article, and instructions to Claude, and having it fix up the typos, do the formatting, and build the UX on the fly.</p>

            <p>Nearly every word, choice of phrase, and the overall structure is still manually written by me, a human. I'm still on the fence about whether I'm just stuck in the old way by preferring to hand-craft my words, or if models are generally not good at writing.</p>

            <p>When I read answers to questions I've asked LLMs, or the long research-style reports they create, the writing style is pretty good and I've probably read more LLM-generated words than human-generated words in the last few months.</p>

            <p>But whenever I try to get them to produce the output I want to produce, they fail hard unless I spend as much effort on the prompt as I would have on writing the output myself.</p>

            <p>Simon Willison calls them <a href="https://simonwillison.net/2023/Apr/2/calculator-for-words/" target="_blank">'word calculators'</a> and this is still mainly how I think of them. Great at moving content around (if you want a summary of this now very long article, an LLM will probably do a great job) but pretty useless at generating new stuff.</p>

            <p>Maybe us writers will be around for a while still - let's see, and lfg.</p>
        </div>

    

    


</div>]]></description>
        </item>
        <item>
            <title><![CDATA[I tried every todo app and ended up with a .txt file (785 pts)]]></title>
            <link>https://www.al3rez.com/todo-txt-journey</link>
            <guid>44864134</guid>
            <pubDate>Mon, 11 Aug 2025 13:59:02 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.al3rez.com/todo-txt-journey">https://www.al3rez.com/todo-txt-journey</a>, See on <a href="https://news.ycombinator.com/item?id=44864134">Hacker News</a></p>
<div id="readability-page-1" class="page"><div> <p>I’ve tried them all. Notion, Todoist, Things 3, OmniFocus, Asana, Trello, Any.do, TickTick. I even built my own todo app once (spoiler: I never finished it). After years of productivity app hopping, I’m back to where I started: a plain text file called <code>todo.txt</code>.</p>
<p>I’m not alone in this. Jeff Huang wrote about his <a href="https://jeffhuang.com/productivity_text_file/">“never-ending .txt file”</a> that he’s used for over 14 years. Reading his post validated everything I’d discovered on my own.</p>
<h2 id="the-endless-search">The Endless Search</h2>
<p>My productivity journey started like everyone else’s. I’d devour blog posts about getting things done or spot a cool app and think “this is it, this will finally organize me.” I’d burn hours building the perfect system, creating categories, tags, projects, labels. Setting it up felt like work.</p>
<p>Then reality hits. The app wants $9.99/month. The sync breaks. The company sells out and dies. Or worse - I waste more time managing the system than working.</p>
<h2 id="what-actually-happened-with-each-app">What Actually Happened With Each App</h2>
<p><strong>Notion</strong>: Built an entire life operating system. Spent three weeks perfecting it. Used it for two days. Now it’s a graveyard of abandoned databases.</p>
<p><strong>Todoist</strong>: Great until I realized I was gaming the points system instead of doing actual work. Turns out completing “drink water” 8 times a day doesn’t make you productive.</p>
<p><strong>Things 3</strong>: Beautiful. Expensive. Tricked me into thinking I had my life together. But I kept forgetting to check it.</p>
<p><strong>Trello</strong>: Turned my todo list into a board with columns. Realized I’m not a startup. I’m just one person trying to remember to buy milk.</p>
<p><strong>OmniFocus</strong>: So powerful I needed a manual to use it. Spent more time learning OmniFocus than finishing my actual projects.</p>
<h2 id="the-breaking-point">The Breaking Point</h2>
<p>One day my phone died and I couldn’t check my tasks. I grabbed a sticky note and scribbled:</p>
<pre tabindex="0" data-language="plaintext"><code><span><span>- finish report</span></span>
<span><span>- call mom</span></span>
<span><span>- gym</span></span>
<span><span>- buy groceries</span></span></code></pre>
<p>And you know what? I crushed all four things. No tags, no priorities, no due dates. Just four things written down.</p>
<h2 id="my-current-system-one-text-file">My Current System: One Text File</h2>
<p>Now I run everything through a single text file. That’s it. Here’s what it looks like:</p>
<pre tabindex="0" data-language="plaintext"><code><span><span>2025-08-11</span></span>
<span><span>10am review pull requests</span></span>
<span><span>- check the auth changes specifically</span></span>
<span><span>write blog post about todo apps</span></span>
<span><span>2pm meeting with team</span></span>
<span><span>- discuss sprint planning</span></span>
<span><span>- bring up the deployment issue</span></span>
<span><span>3:30pm call with client</span></span>
<span><span>figure out dinner plans</span></span>
<span><span>read that article Sarah sent</span></span>
<span><span>fix that annoying bug in the navbar</span></span></code></pre>
<p>Every night, I check tomorrow’s calendar. I dump everything into the next day’s section. Scheduled items get times in front. Sub-bullets hold notes or reminders. Finished tasks? I delete them or add what happened. Still on the list? Not done yet. That’s it.</p>
<p>This transforms into a living document throughout the day. I scribble notes right next to tasks as I work:</p>
<pre tabindex="0" data-language="plaintext"><code><span><span>2025-08-11</span></span>
<span><span>10am review pull requests</span></span>
<span><span>- check the auth changes specifically</span></span>
<span><span>- merged 3 PRs, waiting on Bob for the 4th</span></span>
<span><span>write blog post about todo apps - drafted, need to proofread</span></span>
<span><span>2pm meeting with team</span></span>
<span><span>- discuss sprint planning</span></span>
<span><span>- bring up the deployment issue</span></span>
<span><span>- decided to push release to Thursday</span></span>
<span><span>3:30pm call with client - rescheduled to tomorrow</span></span>
<span><span>figure out dinner plans - ordered pizza</span></span>
<span><span>read that article Sarah sent</span></span>
<span><span>fix that annoying bug in the navbar - was a CSS specificity issue</span></span></code></pre>
<p>Every few days I start fresh with a new date. The old sections stay. They transform into my journal. I search back to find when I did something, who I met, what we decided. Todo list and work log in one file.</p>
<h2 id="why-this-actually-works">Why This Actually Works</h2>
<p><strong>It’s always there</strong>: The file sits on my desktop. It stares at me every time I open my laptop. No app to launch, no subscription to manage.</p>
<p><strong>It’s instant</strong>: My keyboard shortcut launches my todo.txt in a floating window. Doesn’t matter what I’m doing, my todos are one key press away. No switching between apps, no waiting for things to load, just boom - there’s my list.</p>
<p><strong>AI helps but isn’t needed</strong>: With Cursor/Claude Code or Neovim + Supermaven, I can write my entire day’s schedule in 5 minutes. The AI completes my sentences, predicts meeting times, memorizes how I write tasks. But if all these AI companies disappear tomorrow, my system still works. It’s just a text file. The AI makes it faster, not required.</p>
<p><strong>It’s fast</strong>: Adding a task burns 2 seconds. No clicking through menus or selecting projects.</p>
<p><strong>It’s searchable</strong>: Cmd+F and I find anything instantly. “When did I last call the dentist?” Search for “dentist”. Done.</p>
<p><strong>It’s mine</strong>: No company can kill it. No updates can destroy it. No algorithm decides what I should see.</p>
<p><strong>It’s honest</strong>: I can’t hide behind fancy features. Either I did the thing or I didn’t.</p>
<p><strong>It lasts forever</strong>: A text file is the most basic thing a computer can read. It’ll work after every software update, every company shutdown, every app that stops working. Text files from 20 years ago still open perfectly. Try that with your Notion workspace.</p>
<h2 id="the-secret-sauce">The Secret Sauce</h2>
<p>Productivity isn’t about finding the perfect app. It’s about:</p>
<ol>
<li>Dumping things onto paper so your brain can forget them</li>
<li>Checking the list regularly</li>
<li>Executing the tasks</li>
</ol>
<p>That’s it. Everything else is procrastination dressed up as organization.</p>
<h2 id="what-about-insert-feature-here">What About [Insert Feature Here]?</h2>
<p>“But what about reminders?” - I use my calendar for time-specific stuff.</p>
<p>“But what about projects?” - I add a note like <code>[PROJECT]</code> if I need to.</p>
<p>“But what about collaboration?” - I use work tools for work. This is for my life.</p>
<p>“But what about mobile?” - The file syncs through Dropbox. Any text editor works.</p>
<h2 id="the-plot-twist">The Plot Twist</h2>
<p>I’m more productive now than when I had all those fancy apps. Turns out the best productivity system is the one you actually use. And I use this one because there’s nothing to figure out. It’s just a list.</p>
<h2 id="try-it-yourself">Try It Yourself</h2>
<p>Ready to ditch the productivity app hamster wheel? Do this:</p>
<ol>
<li>Create a file called <code>todo.txt</code></li>
<li>Write down what you need to do tomorrow</li>
<li>Do those things</li>
<li>Add notes as you work</li>
<li>Start a new date section when needed</li>
</ol>
<p>Give it a week. Simple beats sophisticated every time.</p>
<p>And if it doesn’t work? Well, there’s always another shiny new app launching next week.</p> </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Wikimedia Foundation Challenges UK Online Safety Act Regulations (625 pts)]]></title>
            <link>https://wikimediafoundation.org/news/2025/08/11/wikimedia-foundation-challenges-uk-online-safety-act-regulations/</link>
            <guid>44863487</guid>
            <pubDate>Mon, 11 Aug 2025 12:38:15 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://wikimediafoundation.org/news/2025/08/11/wikimedia-foundation-challenges-uk-online-safety-act-regulations/">https://wikimediafoundation.org/news/2025/08/11/wikimedia-foundation-challenges-uk-online-safety-act-regulations/</a>, See on <a href="https://news.ycombinator.com/item?id=44863487">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
<p>UPDATE: On Monday, 11 August, the High Court of Justice dismissed the Wikimedia Foundation’s <a href="https://medium.com/wikimedia-policy/wikipedias-nonprofit-host-brings-legal-challenge-to-new-online-safety-act-osa-regulations-0f9153102f29" target="_blank" rel="noreferrer noopener">challenge to the UK’s Online Safety Act</a> (OSA) Categorisation Regulations. While the decision does not provide the immediate legal protections for Wikipedia that we hoped for, the Court’s ruling emphasized the responsibility of <a href="https://www.ofcom.org.uk/" target="_blank" rel="noreferrer noopener">Ofcom</a> and the UK government to ensure Wikipedia is protected as the OSA is implemented.&nbsp;</p>



<p>The judge recognized the “significant value” of Wikipedia, its safety for users, as well as the damages that wrongly-assigned OSA categorisations and duties could have on the human rights of Wikipedia’s volunteer contributors. The Court stressed that this ruling “does not give Ofcom and the Secretary of State a green light to implement a regime that would significantly impede Wikipedia’s operations”,&nbsp; and indicated they could face legal repercussions if they fail to protect Wikipedia and the rights of its users.&nbsp;In order to achieve that outcome, he suggested that Ofcom may need to find a particularly flexible interpretation of the rules in question, or that the rules themselves may need amendment in Parliament.</p>



<p>If the ruling stands, the first categorization decisions from Ofcom are expected this summer. The Foundation will continue to seek solutions to protect Wikipedia and the rights of its users as the OSA continues to be implemented.</p>







<hr>



<p>17 July 2025&nbsp;— Next week, on 22 and 23 July 2025, the High Court of Justice in London will hear the Wikimedia Foundation’s&nbsp;<a href="https://medium.com/wikimedia-policy/wikipedias-nonprofit-host-brings-legal-challenge-to-new-online-safety-act-osa-regulations-0f9153102f29" target="_blank" rel="noreferrer noopener">legal challenge</a>&nbsp;to the&nbsp;<a href="https://www.legislation.gov.uk/uksi/2025/226/regulation/3/made" target="_blank" rel="noreferrer noopener">Categorisation Regulations</a>&nbsp;of the United Kingdom (UK)’s Online Safety Act (OSA).<strong>&nbsp;</strong></p>



<p>The Wikimedia Foundation, the non-profit that operates Wikipedia and other <a href="https://wikimediafoundation.org/our-work/wikimedia-projects/" target="_blank" rel="noreferrer noopener">Wikimedia projects</a>, announced its legal challenge earlier this year, arguing that the regulations endanger Wikipedia and the global community of volunteer contributors who create the information on the site.</p>



<blockquote>
<p>“The Court has an opportunity in this case to set a global precedent for protecting public interest projects online,” said <a href="https://wikimediafoundation.org/profile/stephen-laporte/" target="_blank" rel="noreferrer noopener">Stephen LaPorte</a>, General Counsel at the Wikimedia Foundation. “Wikipedia is the backbone of knowledge on the internet. It’s the only top-ten website operated by a non-profit and one of the highest-quality datasets used in training Large Language Models (LLMs). We trust the Court will protect Wikipedia—a vital encyclopedic resource—from rules crafted for the internet’s riskiest commercial sites and, in doing so, safeguard the open internet for everyone”.</p>
</blockquote>



<p>Information on Wikipedia is written and curated by a global community of nearly 260,000 volunteer contributors. These volunteers set and enforce policies to ensure that information on the platform is fact-based, neutral, and attributed to reliable sources. Over the last 25 years, this human-centered content moderation model has established Wikipedia as an unparalleled resource for reliable information in over 300 languages; its 65 million articles are viewed more than 15 billion times per month worldwide.</p>



<p>The Wikimedia Foundation shares the UK government’s commitment to promoting online environments where everyone can safely participate. The organization is not bringing a general challenge to the OSA as a whole, nor to the existence of the Category 1 duties themselves. Rather, the legal challenge focuses solely on the new Categorisation Regulations that risk imposing Category 1 duties (the OSA’s most stringent obligations) on Wikipedia.</p>



<p>If enforced on Wikipedia, Category 1 demands would undermine the privacy and safety of Wikipedia’s volunteer contributors, expose the encyclopedia to manipulation and vandalism, and divert essential resources from protecting people and improving Wikipedia, one of the world’s most trusted and widely used <a href="https://wikimediafoundation.org/news/2025/02/12/wikipedia-recognized-as-a-digital-public-good/" target="_blank" rel="noreferrer noopener">digital public goods</a>.</p>



<p>For example, the Foundation would be required to verify the identity of many Wikipedia contributors, undermining the privacy that is central to keeping Wikipedia volunteers safe. In addition to being exceptionally burdensome, this requirement—which is just one of several Category 1 demands—could expose contributors to data breaches, stalking, lawsuits, or even imprisonment by authoritarian regimes. Additional details about the concerning impacts of the Category 1 duties on Wikipedia are available in this <a href="https://medium.com/wikimedia-policy/wikipedias-nonprofit-host-brings-legal-challenge-to-new-online-safety-act-osa-regulations-0f9153102f29" target="_blank" rel="noreferrer noopener">blog post</a>.</p>
</div><div>
<p>The Wikimedia Foundation will be joined in the case by longtime UK-based volunteer Wikipedia contributor <a href="https://en.wikipedia.org/wiki/User:Zzuuzz" target="_blank" rel="noreferrer noopener">User:Zzuuzz</a> as a joint claimant. Their voluntary participation highlights what is at stake in this case for the everyday people who read and contribute to Wikimedia projects. It presents the perspective of a Wikipedia volunteer on how the OSA Categorisation Regulations directly threaten the ability of contributors to participate in knowledge sharing on Wikipedia, as well as compromising their rights to privacy, safety, free speech, and association.&nbsp;</p>



<p>The legal challenge is the first to be issued against the OSA’s Categorisation Regulations, as well as the first with a volunteer Wikipedia editor participating as a joint claimant. It follows years of dialogue with regulators and policymakers, in which the Foundation expressed its concerns, as well as <a href="https://www.theyworkforyou.com/lords/?id=2025-02-24a.1524.0&amp;s=wikipedia" target="_blank" rel="noreferrer noopener">warnings from the UK Parliament</a> and <a href="https://wikimedia.org.uk/2023/06/online-safety-bill-open-letter/" target="_blank" rel="noreferrer noopener">civil society</a>.</p>



<blockquote>
<p>“Our concerns on the looming threats to Wikipedia and its contributors remain unaddressed”, said Phil Bradley-Schmieg, Lead Counsel at the Wikimedia Foundation. “We are taking action now to protect Wikipedia’s volunteers, as well as the global accessibility and integrity of free knowledge. We call on the Court to defend the privacy and safety of Wikipedia’s volunteer contributors from flawed legislation”.</p>
</blockquote>



<p>Wikipedia and other Wikimedia projects are safe and important resources through which people across the UK—and the wider world—learn, share knowledge, collaborate, and gain media literacy. Thousands of volunteer Wikipedia contributors are based in the UK, and Wikipedia hosts content from cultural institutions such as the British Library and Wellcome Collection. Content on Wikipedia and other Wikimedia projects was viewed 776 million times last month in the UK alone. Moreover, Wikipedia is used to preserve and promote cultural heritage in the UK, including Indigenous and minority languages such as Welsh. The <a href="https://cy.wikipedia.org/wiki/Hafan?wprov=wppw2" target="_blank" rel="noreferrer noopener">Welsh language version of Wikipedia</a> is the single most popular Welsh language website in the world and is an <a href="https://digitalanddata.blog.gov.wales/2017/08/07/using-technology-to-promote-welsh-language-wikipedia/" target="_blank" rel="noreferrer noopener">official component of the curriculum</a> in Wales.</p>



<p>The hearings at the Royal Courts of Justice (Administrative courts of the King’s Bench Division) in London, are expected to be open to the public. The case reference is AC-2025-LON-001365, and the courtroom location will be announced <a href="https://www.court-tribunal-hearings.service.gov.uk/summary-of-publications?locationId=109" target="_blank" rel="noreferrer noopener">here</a> shortly before the hearing. The Court will issue its decision following the hearing, though the exact timing of the announcement is not known.</p>



<hr>







<p>The personal identity of User:Zzuuzz, the volunteer joining the challenge, will remain confidential and protected by the law and the Foundation.&nbsp;</p>



<p>For media inquiries, please contact <a href="mailto:press@wikimedia.org" target="_blank" rel="noreferrer noopener">press@wikimedia.org</a>.&nbsp;</p>



<p><a href="https://mailchi.mp/wikimedia/global-advocacy-policy-newsletter" target="_blank" rel="noreferrer noopener">Subscribe to our Global Advocacy newsletter</a> to stay informed on this case and other global advocacy updates from the Wikimedia Foundation.&nbsp;</p>



<h2><strong>About the Wikimedia Foundation</strong></h2>



<p>The <a href="https://wikimediafoundation.org/" target="_blank" rel="noreferrer noopener">Wikimedia Foundation</a> is the nonprofit organization that operates Wikipedia and other Wikimedia free knowledge projects. Our vision is a world in which every single human can freely share in the sum of all knowledge. We believe that everyone has the potential to contribute something to our shared knowledge and that everyone should be able to access that knowledge freely. We host Wikipedia and the Wikimedia projects; build software experiences for reading, contributing, and sharing Wikimedia content; support the volunteer communities and partners who make Wikimedia possible. The Wikimedia Foundation is a United States 501(c)(3) tax-exempt organization with offices in San Francisco, California, USA.</p>
</div><div>

			<p><a alt="" tabindex="-1" href="https://wikimediafoundation.org/news/2025/08/11/wikimedia-foundation-challenges-uk-online-safety-act-regulations/">
			<img loading="lazy" decoding="async" width="800" height="600" src="https://wikimediafoundation.org/wp-content/uploads/2025/07/1024px-Royal_Courts_of_Justice_2019-1.jpg?w=800&amp;h=600&amp;crop=1" alt="" srcset="https://wikimediafoundation.org/wp-content/uploads/2025/07/1024px-Royal_Courts_of_Justice_2019-1.jpg?resize=400,300 400w, https://wikimediafoundation.org/wp-content/uploads/2025/07/1024px-Royal_Courts_of_Justice_2019-1.jpg?resize=800,600 800w" sizes="auto, (max-width: 800px) 100vw, 800px">		</a></p><div>
					<h3>
				<a href="https://wikimediafoundation.org/news/2025/08/11/wikimedia-foundation-challenges-uk-online-safety-act-regulations/">
					Wikimedia Foundation Challenges UK Online Safety Act Regulations				</a>
			</h3>
		
					
		
					<p>UPDATE: On Monday, 11 August, the High Court of Justice dismissed the Wikimedia Foundation’s challenge to the UK’s Online Safety Act (OSA) Categorisation Regulations. While the decision does not provide the immediate legal protections for Wikipedia that we hoped for, the Court’s ruling emphasized the responsibility of Ofcom and the UK government to ensure Wikipedia….</p>
		
		

		<p><a href="https://wikimediafoundation.org/news/2025/08/11/wikimedia-foundation-challenges-uk-online-safety-act-regulations/" aria-label="Read more about Wikimedia Foundation Challenges UK Online Safety Act Regulations">
			Read more		</a>
	</p></div>
</div><div>

			<p><a alt="" tabindex="-1" href="https://wikimediafoundation.org/news/2025/07/09/china-block-wikimedia-wipo/">
			<img loading="lazy" decoding="async" width="800" height="600" src="https://wikimediafoundation.org/wp-content/uploads/2025/07/1024px-069_United_Nations_Geneva_World_Intellectual_Property_Organization_WIPO_-_Creative_Commons-1.jpg?w=800&amp;h=600&amp;crop=1" alt="" srcset="https://wikimediafoundation.org/wp-content/uploads/2025/07/1024px-069_United_Nations_Geneva_World_Intellectual_Property_Organization_WIPO_-_Creative_Commons-1.jpg?resize=400,300 400w, https://wikimediafoundation.org/wp-content/uploads/2025/07/1024px-069_United_Nations_Geneva_World_Intellectual_Property_Organization_WIPO_-_Creative_Commons-1.jpg?resize=800,600 800w" sizes="auto, (max-width: 800px) 100vw, 800px">		</a></p><div>
					<h3>
				<a href="https://wikimediafoundation.org/news/2025/07/09/china-block-wikimedia-wipo/">
					For fifth time, China blocks Wikimedia Foundation as permanent observer to the World Intellectual Property Organization (WIPO)				</a>
			</h3>
		
					
		
					<p>On 9 July 2025, the Wikimedia Foundation was denied permanent observer status at the World Intellectual Property Organization (WIPO).</p>
		
		

		<p><a href="https://wikimediafoundation.org/news/2025/07/09/china-block-wikimedia-wipo/" aria-label="Read more about For fifth time, China blocks Wikimedia Foundation as permanent observer to the World Intellectual Property Organization (WIPO)">
			Read more		</a>
	</p></div>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Pricing Pages – A Curated Gallery of Pricing Page Designs (186 pts)]]></title>
            <link>https://pricingpages.design/</link>
            <guid>44863409</guid>
            <pubDate>Mon, 11 Aug 2025 12:27:08 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://pricingpages.design/">https://pricingpages.design/</a>, See on <a href="https://news.ycombinator.com/item?id=44863409">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><div><p>Filter by style &amp; or by industry</p><p><a data-w-id="0f548e28-46ec-0449-f2ab-898862391dd9" href="#"><img alt="" src="https://cdn.prod.website-files.com/6244257bf98bf00e37b25f97/6244257bf98bf0e23bb25fec_icon_close-modal.svg"></a></p></div><div><div role="list"><p><label><span fs-cmsfilter-field="Industry" for="Filters">AI &amp; Machine Learning</span></label></p><p><label><span fs-cmsfilter-field="Industry" for="Filters">Business &amp; Productivity</span></label></p><p><label><span fs-cmsfilter-field="Industry" for="Filters">Business Intelligence &amp; Analytics</span></label></p><p><label><span fs-cmsfilter-field="Industry" for="Filters">Communication &amp; Collaboration</span></label></p><p><label><span fs-cmsfilter-field="Industry" for="Filters">Content &amp; Media</span></label></p><p><label><span fs-cmsfilter-field="Industry" for="Filters">Crypto</span></label></p><p><label><span fs-cmsfilter-field="Industry" for="Filters">Design &amp; Creative</span></label></p><p><label><span fs-cmsfilter-field="Industry" for="Filters">Developer Tools &amp; Infrastructure</span></label></p><p><label><span fs-cmsfilter-field="Industry" for="Filters">E-commerce &amp; Retail</span></label></p><p><label><span fs-cmsfilter-field="Industry" for="Filters">Fintech &amp; Financial Services</span></label></p><p><label><span fs-cmsfilter-field="Industry" for="Filters">HR &amp; Recruiting</span></label></p><p><label><span fs-cmsfilter-field="Industry" for="Filters">Marketing &amp; Sales</span></label></p><p><label><span fs-cmsfilter-field="Industry" for="Filters">Specialized &amp; Niche</span></label></p><p><label><span fs-cmsfilter-field="Industry" for="Filters">Travel &amp; Transportation</span></label></p></div><div role="list"><p><label><span fs-cmsfilter-field="Styles" for="Filters">Color Coded Tiers</span></label></p><p><label><span fs-cmsfilter-field="Styles" for="Filters">Comparison Table</span></label></p><p><label><span fs-cmsfilter-field="Styles" for="Filters">Contact Sales </span></label></p><p><label><span fs-cmsfilter-field="Styles" for="Filters">Custom Pricing</span></label></p><p><label><span fs-cmsfilter-field="Styles" for="Filters">Enterprise option</span></label></p><p><label><span fs-cmsfilter-field="Styles" for="Filters">Feature Checkmarks</span></label></p><p><label><span fs-cmsfilter-field="Styles" for="Filters">Feature Lists</span></label></p><p><label><span fs-cmsfilter-field="Styles" for="Filters">Monthly/Annual Toggle</span></label></p><p><label><span fs-cmsfilter-field="Styles" for="Filters">Stacked Cards</span></label></p><p><label><span fs-cmsfilter-field="Styles" for="Filters">Standard Table</span></label></p><p><label><span fs-cmsfilter-field="Styles" for="Filters">Table</span></label></p><p><label><span fs-cmsfilter-field="Styles" for="Filters">Tiered Table</span></label></p><p><label><span fs-cmsfilter-field="Styles" for="Filters">Usage-Based</span></label></p><p><label><span fs-cmsfilter-field="Styles" for="Filters">Usage-Based Calculator</span></label></p></div></div></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[OpenSSH Post-Quantum Cryptography (350 pts)]]></title>
            <link>https://www.openssh.com/pq.html</link>
            <guid>44863242</guid>
            <pubDate>Mon, 11 Aug 2025 12:01:58 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.openssh.com/pq.html">https://www.openssh.com/pq.html</a>, See on <a href="https://news.ycombinator.com/item?id=44863242">Hacker News</a></p>
<div id="readability-page-1" class="page">
<hr>

<p>
OpenSSH supports a number of cryptographic key agreement algorithms
considered to be safe against attacks from quantum computers. 
We recommend that all SSH connections use these algorithms.
</p>

<p>
OpenSSH has offered post-quantum key agreement (<i>KexAlgorithms</i>)
by default since release 9.0 (2022), initially via the
<tt>sntrup761x25519-sha512</tt> algorithm. More recently, in OpenSSH 9.9,
we have added a second post-quantum key agreement <tt>mlkem768x25519-sha256</tt>
and it was made the default scheme in OpenSSH 10.0.
</p>

<p>
To encourage migration to these stronger algorithms, OpenSSH 10.1 will warn
the user when a non post-quantum key agreement scheme is selected. These
warnings are displayed by default but may be disabled via the
<i>WarnWeakCrypto</i> option in
<a href="https://man.openbsd.org/ssh_config.5">ssh_config(5)</a>.
</p>

<h3>Background</h3>

<p>
A quantum computer (QC) is a device capable of performing computations
with information encoded as quantum states. Such a device could quickly solve
particular problems that are intractable for existing "classical" computers.
</p>

<p>
The mathematics that underpin a number of cryptographic algorithms
are among the problems that quantum computers are believed to be able to
effectively solve. This means that a sufficiently-powerful quantum computer
(a.k.a a "cryptographically-relevant" quantum computer) will be able to break
them. Most affected is the cryptography used for key agreement and digital
signatures, both of which play important roles in SSH.
</p>

<p>
Fortunately, quantum computers of sufficient power to break cryptography
have not been invented yet. Estimates for when a cryptographically-relevant
quantum computer will arrive, based on the rate of progress in the field,
range from 5-20 years, with many observers expecting them to arrive
in the mid-2030s.
</p>

<p>
The entire privacy of an SSH connection depends on cryptographic key agreement.
If an attacker can break the key agreement then they are able to decrypt and
view the entire session. The attacker need not perform this attack in real
time; they may collect encrypted SSH sessions now and then decrypt them later
once they have access to a quantum computer.
This is referred to as a "store now, decrypt later" attack (also as
"harvest now, decrypt later").
</p>

<p>
OpenSSH supports post-quantum cryptography to protect user traffic against
this attack.
</p>

<h2>FAQ</h2>

<dl>
<dt><b>I received a warning from ssh that directed me to this page. What should I do?</b></dt>
<dd>
As mentioned above, OpenSSH 10.1 started warning users when connections use
cryptography that is not safe against quantum computers. If you received such
a warning, it means that the server you connected to did not offer one of the
two post-quantum key agreement algorithms that are being standardised for the
SSH protocol:
<a href="https://datatracker.ietf.org/doc/draft-ietf-sshm-mlkem-hybrid-kex/"><tt>mlkem768x25519-sha256</tt></a> and
<a href="https://datatracker.ietf.org/doc/draft-josefsson-ntruprime-ssh/"><tt>sntrup761x25519-sha512</tt></a>
<p>
The ideal solution is to update the server to use an SSH implementation that
supports at least one of these. OpenSSH versions 9.0 and greater support
</p><tt>sntrup761x25519-sha512</tt> and versions 9.9 and greater support
<tt>mlkem768x25519-sha256</tt>. If your server is already running one of these
versions, then check whether the <i>KexAlgorithms</i> option has disabled
their use.
<p>
If you are unable to update the server and/or you prefer to accept the risk
of continuing to use quantum-unsafe cryptography then the warning may be
silenced via the 
<i>WarnWeakCrypto</i> option in
<a href="https://man.openbsd.org/ssh_config.5">ssh_config(5)</a>.
We recommend doing this selectively, for example:
</p><pre>Match host unsafe.example.com
    WarnWeakCrypto no
</pre>
</dd>
<dt><b>Quantum computers don't exist yet, why go to all this trouble?</b></dt>
<dd>
Because of the "store now, decrypt later" attack mentioned above. Traffic
sent today is at risk of decryption unless post-quantum key agreement is used.
</dd>
<dt><b>What about signature algorithms? You said they were at risk too</b></dt>
<dd>
Yes, most currently-used signature algorithms (including RSA and ECDSA) can be
broken by a quantum computer. However, there is no risk to existing traffic
in this situation (i.e. there is no analogous "store now, decrypt later").
The only urgency for signature algorithms is ensuring that all classical
signature keys are retired in advance of cryptographically-relevant computers
becoming a reality. OpenSSH will add support for post-quantum signature
algorithms in the future.
</dd>
<dt><b>I don't believe we'll ever get quantum computers. This is a waste of time</b></dt>
<dd>
Some people consider the task of scaling existing quantum computers up to the
point where they can tackle cryptographic problems to be practically
insurmountable. This is a possibilty. However, it appears that most of the
barriers to a cryptographically-relevant quantum computer are engineering
challenges rather than underlying physics.
<p>
If we're right about quantum computers being practical, then we will have
protected vast quantities of user data. If we're wrong about it, then all
we'll have done is moved to cryptographic algorithms with stronger mathematical
underpinnings.
</p></dd>
<dt><b>These post-quantum algorithms are new, are we sure they aren't broken?</b></dt>
<dd>
We're wary of this too. Though post-quantum key agreement algorithms have
received a lot of concerted cryptographic attention over the last few years,
it's possible that new attacks might be found.
<p>
To defend against this happening we have selected post-quantum algorithms with
good safety margins, this means that even if they turn out to be weaker than
expected they are still likely to be strong enough to be considered fit for
purpose.
</p><p>
Additionally, all the post-quantum algorithms implemented by OpenSSH are
"hybrids" that combine a post-quantum algorithm with a classical
algorithm. For example </p><tt>mlkem768x25519-sha256</tt> combines ML-KEM, a
post-quantum key agreement scheme, with ECDH/x25519, a classical key agreement
algorithm that was formerly OpenSSH's preferred default. This ensures that the
combined, hybrid algorithm is <i>no worse</i> than the previous best
classical algorithm, even if the post-quantum algorithm turns out to be
completely broken by future cryptanalysis.
</dd>
</dl>

</div>]]></description>
        </item>
        <item>
            <title><![CDATA[GPT-OSS-120B runs on just 8GB VRAM & 64GB+ system RAM (222 pts)]]></title>
            <link>https://old.reddit.com/r/LocalLLaMA/comments/1mke7ef/120b_runs_awesome_on_just_8gb_vram/</link>
            <guid>44862542</guid>
            <pubDate>Mon, 11 Aug 2025 10:02:08 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://old.reddit.com/r/LocalLLaMA/comments/1mke7ef/120b_runs_awesome_on_just_8gb_vram/">https://old.reddit.com/r/LocalLLaMA/comments/1mke7ef/120b_runs_awesome_on_just_8gb_vram/</a>, See on <a href="https://news.ycombinator.com/item?id=44862542">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>Here is the thing, the expert layers run amazing on CPU  (<del>~17T/s</del> 25T/s on a 14900K) and you can force that with this new llama-cpp option: --cpu-moe .</p>

<p>You can offload just the attention layers to GPU  (requiring about 5 to 8GB of VRAM) for fast prefill.</p>

<ul>
<li>KV cache for the sequence</li>
<li>Attention weights &amp; activations</li>
<li>Routing tables</li>
<li>LayerNorms and other “non-expert” parameters</li>
</ul>

<p>No giant MLP weights are resident on the GPU, so memory use stays low.</p>

<p>This yields an amazing snappy system for a 120B model!  Even something like a 3060Ti would be amazing! GPU with BF16 support would be best (RTX3000+) because all layers except the MOE layers (which are mxfp4) are BF16.</p>

<p>64GB of system ram would be minimum, and 96GB would be ideal. (linux uses mmap so will keep the 'hot' experts in memory even if the whole model doesn't fit in memory)</p>

<blockquote>
<p>prompt eval time = 28044.75 ms / 3440 tokens ( 8.15 ms per token, 122.66 tokens per second)</p>

<p>eval time = 5433.28 ms / 98 tokens ( 55.44 ms per token, 18.04 tokens per second)</p>
</blockquote>

<p>with 5GB of vram usage!</p>

<p>Honestly, I think this is the biggest win of this 120B model. This seems an amazing model to run fast for GPU-poor people. You can do this on a 3060Ti and 64GB of system ram is cheap.</p>

<p>edit: with this latest PR: <a href="https://github.com/ggml-org/llama.cpp/pull/15157">https://github.com/ggml-org/llama.cpp/pull/15157</a></p>

<pre><code>~/build/llama.cpp/build-cuda/bin/llama-server \
    -m $LLAMA_MODEL_DIR/gpt-oss-120b-mxfp4-00001-of-00003.gguf \
    --n-cpu-moe 36 \    #this model has 36 MOE blocks. So cpu-moe 36 means all moe are running on the CPU. You can adjust this to move some MOE to the GPU, but it doesn't even make things that much faster.
    --n-gpu-layers 999 \   #everything else on the GPU, about 8GB
    -c 0 -fa \   #max context (128k), flash attention
    --jinja --reasoning-format none \
    --host 0.0.0.0 --port 8502 --api-key "dummy" \



prompt eval time =   94593.62 ms / 12717 tokens (    7.44 ms per token,   134.44 tokens per second)
       eval time =   76741.17 ms /  1966 tokens (   39.03 ms per token,    25.62 tokens per second)
</code></pre>

<p>Hitting above 25T/s with only 8GB VRAM use!</p>

<p>Compared to running 8 MOE layers also on the GPU (about 22GB VRAM used total) :</p>

<pre><code>~/build/llama.cpp/build-cuda/bin/llama-server \
    -m $LLAMA_MODEL_DIR/gpt-oss-120b-mxfp4-00001-of-00003.gguf \
    --n-cpu-moe 28 \
    --n-gpu-layers 999 \
    -c 0 -fa \
    --jinja --reasoning-format none \
    --host 0.0.0.0 --port 8502 --api-key "dummy" \

prompt eval time =   78003.66 ms / 12715 tokens (    6.13 ms per token,   163.01 tokens per second)
       eval time =   70376.61 ms /  2169 tokens (   32.45 ms per token,    30.82 tokens per second)
</code></pre>

<p>Honestly, this 120B is the perfect architecture for running at home on consumer hardware. Somebody did some smart thinking when designing all of this!</p>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Faster substring search with SIMD in Zig (174 pts)]]></title>
            <link>https://aarol.dev/posts/zig-simd-substr/</link>
            <guid>44862414</guid>
            <pubDate>Mon, 11 Aug 2025 09:41:13 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://aarol.dev/posts/zig-simd-substr/">https://aarol.dev/posts/zig-simd-substr/</a>, See on <a href="https://news.ycombinator.com/item?id=44862414">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p><span>Published 10.08.2025</span></p><p>I’ve been learning a lot about low-level programming languages lately, and for a long time there has been one thing that has interested me: SIMD (or ‘single instruction, multiple data’) code. I’ve seen a lot of articles about having massive performance gains by utilizing SIMD and wanted to learn how to do it myself.</p><p>This article is a journey into implementing ~60% faster substring searching compared to Zig’s <code>std.mem.indexOf</code> using a SIMD-friendly algorithm.</p><h2 id="baseline">Baseline
<a href="#baseline">#</a></h2><p>This is the baseline function that we will be comparing against:</p><div><pre tabindex="0"><code data-lang="zig"><span><span><span>fn</span><span> </span><span>find_substr</span><span>(</span><span>needle</span><span>:</span><span> </span><span>[]</span><span>const</span><span> </span><span>u8</span><span>,</span><span> </span><span>haystack</span><span>:</span><span> </span><span>[]</span><span>const</span><span> </span><span>u8</span><span>)</span><span> </span><span>?</span><span>usize</span><span> </span><span>{</span><span>
</span></span></span><span><span><span>    </span><span>return</span><span> </span><span>std</span><span>.</span><span>mem</span><span>.</span><span>indexOf</span><span>(</span><span>u8</span><span>,</span><span> </span><span>haystack</span><span>,</span><span> </span><span>needle</span><span>);</span><span>
</span></span></span><span><span><span></span><span>}</span><span>
</span></span></span></code></pre></div><p>It’s the closest thing to a substring search function from Zig’s standard library. It returns the first index of a subsequence – or <code>null</code> if not found.</p><h2 id="simd-algorithm">SIMD algorithm
<a href="#simd-algorithm">#</a></h2><p>This algorithm is taken directly from Wojciech Muła’s fantastic article <a href="http://0x80.pl/notesen/2016-11-28-simd-strfind.html#algorithm-1-generic-simd">SIMD-friendly algorithms for substring searching</a>, which seems to have the best algorithms for finding substrings in a large body of text.</p><p>Here’s how the algorithm works: say that we want to find the index of the word “blue” (the <code>needle</code>) in “It was a beautiful, bounteous, blue day” (the <code>haystack</code>). First, we extract the first and last character of the <code>needle</code> (‘b’ and ’e’) and store them in a variable.</p><p>Then we will loop through all of the characters in <code>haystack</code>, loading the next 32 characters (bytes) from memory into a SIMD register and comparing each character (byte) in the register with ‘b’. This will result in a mask containing 32 bytes, <code>1</code> if the character is ‘b’ and <code>0</code> in all other cases.</p><p>We will do the same with the last character, but load the characters with an offset (<code>needle.len - 1</code>).</p><blockquote><p>Without the offset, any match that starts in one 32‑byte chunk and ends in the next would be missed. With this method, we can also check for <code>needles</code> that are longer than 32 characters.</p></blockquote><p>The result will be two bit masks, <code>First</code> and <code>Last</code>, where we can use bit-wise AND (<code>Result = First &amp; Last</code>) to figure out potential substring occurrences.</p><p><code>Result</code> will be <code>1</code> only when there is a ‘b’ at index <code>i</code> followed by an ’e’ at index <code>i+3</code>. We still need to check if those positions actually contain the value “blue”, but this still dramatically reduces the number of checks (= individual memory accesses) that are necessary. We’ll see how this works in practice in the next section.</p><h2 id="implementation-in-zig">Implementation in Zig
<a href="#implementation-in-zig">#</a></h2><p>First, to properly use SIMD, let’s assume that the CPU supports AVX2 (Advanced Vector Extensions 2) and has 256-bit wide registers.</p><blockquote><p>All desktop processors less than 10 years old support AVX2, with newer ones also supporting AVX-512 with 512-bit wide registers.</p></blockquote><p>This allows us to use Zig’s <a href="https://ziglang.org/documentation/0.14.1/#Vectors">@Vector</a> function to make a type:</p><div><pre tabindex="0"><code data-lang="zig"><span><span><span>const</span><span> </span><span>Block</span><span> </span><span>=</span><span> </span><span>@Vector</span><span>(</span><span>32</span><span>,</span><span> </span><span>u8</span><span>);</span><span> </span><span>// number of elements, element type (32*8=256)
</span></span></span></code></pre></div><p>By using <code>Block</code>, we are telling the compiler that the operations on this datatype should use SIMD instructions where possible.</p><p>Next, we take the first and last letters of the search word (’needle’) and load them into two SIMD registers, so that every byte of the register is filled with the character. This is handled by another built-in function, <code>@splat</code>:</p><div><pre tabindex="0"><code data-lang="zig"><span><span><span>const</span><span> </span><span>first_letter</span><span>:</span><span> </span><span>Block</span><span> </span><span>=</span><span> </span><span>@splat</span><span>(</span><span>needle</span><span>[</span><span>0</span><span>]);</span><span>
</span></span></span><span><span><span></span><span>const</span><span> </span><span>last_letter</span><span>:</span><span> </span><span>Block</span><span> </span><span>=</span><span> </span><span>@splat</span><span>(</span><span>needle</span><span>[</span><span>needle</span><span>.</span><span>len</span><span> </span><span>-</span><span> </span><span>1</span><span>]);</span><span>
</span></span></span></code></pre></div><p>In the main loop, we check that there is enough characters left in <code>haystack</code> so that we can read the next <code>32 + needle.len</code> characters. Inside the block, we load the blocks that we’re going to compare <code>first_letter</code> and <code>last_letter</code> with.</p><div><pre tabindex="0"><code data-lang="zig"><span><span><span>const</span><span> </span><span>n</span><span> </span><span>=</span><span> </span><span>haystack</span><span>.</span><span>len</span><span>;</span><span>
</span></span></span><span><span><span></span><span>const</span><span> </span><span>k</span><span> </span><span>=</span><span> </span><span>needle</span><span>.</span><span>len</span><span>;</span><span>
</span></span></span><span><span><span></span><span>var</span><span> </span><span>i</span><span>:</span><span> </span><span>usize</span><span> </span><span>=</span><span> </span><span>0</span><span>;</span><span>
</span></span></span><span><span><span></span><span>while</span><span> </span><span>(</span><span>i</span><span> </span><span>+</span><span> </span><span>k</span><span> </span><span>+</span><span> </span><span>32</span><span> </span><span>&lt;=</span><span> </span><span>n</span><span>)</span><span> </span><span>:</span><span> </span><span>(</span><span>i</span><span> </span><span>+=</span><span> </span><span>32</span><span>)</span><span> </span><span>{</span><span>
</span></span></span><span><span><span>    </span><span>const</span><span> </span><span>first_block</span><span>:</span><span> </span><span>Block</span><span> </span><span>=</span><span> </span><span>haystack</span><span>[</span><span>i</span><span>..][</span><span>0</span><span>..</span><span>32</span><span>].</span><span>*</span><span>;</span><span>
</span></span></span><span><span><span>    </span><span>const</span><span> </span><span>last_block</span><span>:</span><span> </span><span>Block</span><span> </span><span>=</span><span> </span><span>haystack</span><span>[</span><span>i</span><span> </span><span>+</span><span> </span><span>k</span><span> </span><span>-</span><span> </span><span>1</span><span> </span><span>..][</span><span>0</span><span>..</span><span>32</span><span>].</span><span>*</span><span>;</span><span>
</span></span></span></code></pre></div><p>Now we can make the comparisons and combine them into a mask:</p><div><pre tabindex="0"><code data-lang="zig"><span><span><span>	</span><span>// ...
</span></span></span><span><span><span></span><span>    </span><span>const</span><span> </span><span>eq_first</span><span> </span><span>=</span><span> </span><span>first_letter</span><span> </span><span>==</span><span> </span><span>first_block</span><span>;</span><span>
</span></span></span><span><span><span>    </span><span>const</span><span> </span><span>eq_last</span><span> </span><span>=</span><span> </span><span>last_letter</span><span> </span><span>==</span><span> </span><span>last_block</span><span>;</span><span>
</span></span></span><span><span><span>    </span><span>var</span><span> </span><span>mask</span><span>:</span><span> </span><span>std</span><span>.</span><span>bit_set</span><span>.</span><span>IntegerBitSet</span><span>(</span><span>32</span><span>)</span><span> </span><span>=</span><span> </span><span>.{</span><span> </span><span>.</span><span>mask</span><span> </span><span>=</span><span> </span><span>@bitCast</span><span>(</span><span>eq_first</span><span> </span><span>&amp;</span><span> </span><span>eq_last</span><span>)</span><span> </span><span>};</span><span>
</span></span></span></code></pre></div><p>Here we can use an <code>IntegerBitSet</code> from Zig’s standard library. We construct it by casting the result of <code>eq_first &amp; eq_last</code> into a 32-bit integer. If the resulting mask is non-zero, there are candidates in the current block.</p><div><pre tabindex="0"><code data-lang="zig"><span><span><span>	</span><span>// ...
</span></span></span><span><span><span></span><span>&nbsp; &nbsp; </span><span>while</span><span> </span><span>(</span><span>mask</span><span>.</span><span>findFirstSet</span><span>())</span><span> </span><span>|</span><span>bitpos</span><span>|</span><span> </span><span>{</span><span>
</span></span></span><span><span><span>        </span><span>if</span><span> </span><span>(</span><span>std</span><span>.</span><span>mem</span><span>.</span><span>eql</span><span>(</span><span>u8</span><span>,</span><span> </span><span>haystack</span><span>[</span><span>i</span><span> </span><span>+</span><span> </span><span>bitpos</span><span> </span><span>+</span><span> </span><span>1</span><span> </span><span>..][</span><span>0</span><span> </span><span>..</span><span> </span><span>k</span><span> </span><span>-</span><span> </span><span>1</span><span>],</span><span> </span><span>needle</span><span>[</span><span>1</span><span>..]))</span><span> </span><span>{</span><span>
</span></span></span><span><span><span>            </span><span>return</span><span> </span><span>i</span><span> </span><span>+</span><span> </span><span>bitpos</span><span>;</span><span>
</span></span></span><span><span><span>        </span><span>}</span><span>
</span></span></span><span><span><span>        </span><span>_</span><span> </span><span>=</span><span> </span><span>mask</span><span>.</span><span>toggleFirstSet</span><span>();</span><span>
</span></span></span><span><span><span>    </span><span>}</span><span>
</span></span></span></code></pre></div><p>The first and last characters of the substring are checked already, so we don’t need to check their equality again.</p><p>Finally, if there are leftover characters, we can fall back to <code>std.mem.IndexOf</code>.</p><div><pre tabindex="0"><code data-lang="zig"><span><span><span>	</span><span>// ...
</span></span></span><span><span><span></span><span>	</span><span>// Fallback to scalar search for the tail
</span></span></span><span><span><span></span><span>	</span><span>if</span><span> </span><span>(</span><span>i</span><span> </span><span>&lt;</span><span> </span><span>n</span><span>)</span><span> </span><span>{</span><span>
</span></span></span><span><span><span>	    </span><span>if</span><span> </span><span>(</span><span>std</span><span>.</span><span>mem</span><span>.</span><span>indexOf</span><span>(</span><span>u8</span><span>,</span><span> </span><span>haystack</span><span>[</span><span>i</span><span>..],</span><span> </span><span>needle</span><span>))</span><span> </span><span>|</span><span>rel_idx</span><span>|</span><span> </span><span>{</span><span>
</span></span></span><span><span><span>	        </span><span>return</span><span> </span><span>i</span><span> </span><span>+</span><span> </span><span>rel_idx</span><span>;</span><span>
</span></span></span><span><span><span>	    </span><span>}</span><span>
</span></span></span><span><span><span>	</span><span>}</span><span>
</span></span></span><span><span><span>	</span><span>return</span><span> </span><span>null</span><span>;</span><span> </span><span>// no substring found
</span></span></span><span><span><span></span><span>}</span><span>
</span></span></span></code></pre></div><h3 id="benchmarks">Benchmarks
<a href="#benchmarks">#</a></h3><p>To properly show the effects of our SIMD algorithm, we’re going to need a large haystack. For this, I’ve chosen to use <a href="https://www.gutenberg.org/ebooks/2701">the entirety Moby Dick</a> in plain text, and a search word ’newsletter’, which appears at the very end of the text.</p><blockquote><p>The code is available <a href="https://github.com/aarol/substr">on GitHub</a></p></blockquote><p>To compile the code, I ran <code>zig build</code> with <code>-Doptimize=ReleaseFast</code>:</p><div><pre tabindex="0"><code data-lang="sh"><span><span>&gt; zig build -Doptimize<span>=</span>ReleaseFast
</span></span></code></pre></div><blockquote><p>Support for <a href="https://github.com/ziglang/zig/pull/24131">bitwise operations on boolean vectors</a> was added in Zig 0.15, which is unreleased as of now (August 2025). If you want to run the code on your system, you need to build Zig from the master branch.</p></blockquote><p>To measure performance and compare against baseline, I’ll use one of my favorite CLI tools, <a href="https://github.com/andrewrk/poop">poop</a>:</p><div><pre tabindex="0"><code data-lang="sh"><span><span>&gt; poop -d <span>10000</span> <span>"./zig-out/bin/substr"</span> <span>"./zig-out/bin/substr --simd"</span>
</span></span></code></pre></div><pre>
<span>Benchmark 1 (6361 runs)</span>: ./zig-out/bin/substr
<span>  measurement          </span><span>mean</span><span> ± </span><span>σ</span><span>            </span><span>min</span><span> … </span><span>max</span><span>           </span><span>outliers</span><span>         delta</span>
  wall_time          <span>1.22</span><span>ms</span> ± <span> 185</span><span>us</span>    <span> 903</span><span>us</span> … <span>5.33</span><span>ms</span>        242 ( 4%)        0%
  peak_rss           <span>1.20</span><span>MB</span> ± <span> 290</span><span>  </span>    <span>1.18</span><span>MB</span> … <span>1.20</span><span>MB</span>          2 ( 0%)        0%
  cpu_cycles         <span>2.15</span><span>M </span> ± <span>40.5</span><span>K </span>    <span>2.10</span><span>M </span> … <span>2.71</span><span>M </span>        312 ( 5%)        0%
  instructions       <span>1.85</span><span>M </span> ± <span>0.75</span><span>  </span>    <span>1.85</span><span>M </span> … <span>1.85</span><span>M </span>         56 ( 1%)        0%
  cache_references   <span>43.8</span><span>K </span> ± <span> 620</span><span>  </span>    <span>38.3</span><span>K </span> … <span>44.9</span><span>K </span>          9 ( 0%)        0%
  cache_misses       <span>19.0</span><span>K </span> ± <span>10.3</span><span>K </span>    <span>4.08</span><span>K </span> … <span>33.6</span><span>K </span>          0 ( 0%)        0%
  branch_misses      <span>48.1</span><span>  </span> ± <span>17.4</span><span>  </span>    <span>  20</span><span>  </span> … <span> 104</span><span>  </span>         97 ( 2%)        0%

<span>Benchmark 2 (10000 runs)</span>: ./zig-out/bin/substr --simd
<span>  measurement          </span><span>mean</span><span> ± </span><span>σ</span><span>            </span><span>min</span><span> … </span><span>max</span><span>           </span><span>outliers</span><span>         delta</span>
  wall_time          <span> 500</span><span>us</span> ± <span>96.9</span><span>us</span>    <span> 397</span><span>us</span> … <span>4.23</span><span>ms</span>        840 ( 8%)        <span>⚡</span><span>- 58.9% ±  0.4%</span>
  peak_rss           <span>1.20</span><span>MB</span> ± <span> 164</span><span>  </span>    <span>1.18</span><span>MB</span> … <span>1.20</span><span>MB</span>          1 ( 0%)          +  0.0% ±  0.0%
  cpu_cycles         <span> 369</span><span>K </span> ± <span>36.1</span><span>K </span>    <span> 340</span><span>K </span> … <span>1.10</span><span>M </span>       <span>1167 (12%)</span>        <span>⚡</span><span>- 82.8% ±  0.1%</span>
  instructions       <span> 578</span><span>K </span> ± <span>0.53</span><span>  </span>    <span> 578</span><span>K </span> … <span> 578</span><span>K </span>          6 ( 0%)        <span>⚡</span><span>- 68.8% ±  0.0%</span>
  cache_references   <span>38.8</span><span>K </span> ± <span> 545</span><span>  </span>    <span>34.1</span><span>K </span> … <span>40.5</span><span>K </span>          6 ( 0%)        <span>⚡</span><span>- 11.4% ±  0.0%</span>
  cache_misses       <span>5.62</span><span>K </span> ± <span>4.97</span><span>K </span>    <span>2.11</span><span>K </span> … <span>27.9</span><span>K </span>       <span>1529 (15%)</span>        <span>⚡</span><span>- 70.3% ±  1.2%</span>
  branch_misses      <span>2.88</span><span>K </span> ± <span>23.4</span><span>  </span>    <span>2.81</span><span>K </span> … <span>3.09</span><span>K </span>        453 ( 5%)        💩<span>+5879.8% ±  1.4%</span>
</pre><p>(Scroll right to see more data)</p><p>As you can see, for a large body of text, the speedup is noticeable: 59% faster with 80% less CPU cycles!</p><blockquote><p>The SIMD version only took 500 microseconds to complete on average, including the overhead of loading the program into memory and printing the result. 500 microseconds is half a millisecond. That’s how fast my laptop searches through a whole book, <strong>200 000 words</strong>, cover to cover. This is why computers are so powerful! How long would it take for a human to do that?</p></blockquote><p>This is quite a large improvement, and proves that the SIMD code is actually working (otherwise the reduction in CPU cycles wouldn’t be so massive). Can we do even better though?</p><h2 id="character-selection">Character selection
<a href="#character-selection">#</a></h2><p>You may notice from the output of <code>poop</code> that the number of branch misses has absolutely blown up – from 48 on average to 2.88k !</p><p>Why does this happen? Well, if you were to count how many times the inner while loop is entered when the mask is non-zero:</p><div><pre tabindex="0"><code data-lang="zig"><span><span><span>    </span><span>var</span><span> </span><span>i</span><span>:</span><span> </span><span>usize</span><span> </span><span>=</span><span> </span><span>0</span><span>;</span><span>
</span></span></span><span><span><span>    </span><span>var</span><span> </span><span>count</span><span>:</span><span> </span><span>usize</span><span> </span><span>=</span><span> </span><span>0</span><span>;</span><span>
</span></span></span><span><span><span>    </span><span>while</span><span> </span><span>(</span><span>i</span><span> </span><span>+</span><span> </span><span>k</span><span> </span><span>+</span><span> </span><span>32</span><span> </span><span>&lt;=</span><span> </span><span>n</span><span>)</span><span> </span><span>:</span><span> </span><span>(</span><span>i</span><span> </span><span>+=</span><span> </span><span>32</span><span>)</span><span> </span><span>{</span><span>
</span></span></span><span><span><span>        </span><span>const</span><span> </span><span>block_first</span><span>:</span><span> </span><span>Block</span><span> </span><span>=</span><span> </span><span>haystack</span><span>[</span><span>i</span><span>..][</span><span>0</span><span>..</span><span>32</span><span>].</span><span>*</span><span>;</span><span>
</span></span></span><span><span><span>        </span><span>const</span><span> </span><span>block_last</span><span>:</span><span> </span><span>Block</span><span> </span><span>=</span><span> </span><span>haystack</span><span>[</span><span>i</span><span> </span><span>+</span><span> </span><span>k</span><span> </span><span>-</span><span> </span><span>1</span><span> </span><span>..][</span><span>0</span><span>..</span><span>32</span><span>].</span><span>*</span><span>;</span><span>
</span></span></span><span><span><span>        </span><span>const</span><span> </span><span>eq_first</span><span> </span><span>=</span><span> </span><span>first</span><span> </span><span>==</span><span> </span><span>block_first</span><span>;</span><span>
</span></span></span><span><span><span>        </span><span>const</span><span> </span><span>eq_last</span><span> </span><span>=</span><span> </span><span>last</span><span> </span><span>==</span><span> </span><span>block_last</span><span>;</span><span>
</span></span></span><span><span><span>        </span><span>var</span><span> </span><span>mask</span><span>:</span><span> </span><span>std</span><span>.</span><span>bit_set</span><span>.</span><span>IntegerBitSet</span><span>(</span><span>32</span><span>)</span><span> </span><span>=</span><span> </span><span>.{</span><span> </span><span>.</span><span>mask</span><span> </span><span>=</span><span> </span><span>@bitCast</span><span>(</span><span>eq_first</span><span> </span><span>&amp;</span><span> </span><span>eq_last</span><span>)</span><span> </span><span>};</span><span>
</span></span></span><span><span><span>        </span><span>while</span><span> </span><span>(</span><span>mask</span><span>.</span><span>findFirstSet</span><span>())</span><span> </span><span>|</span><span>bitpos</span><span>|</span><span> </span><span>{</span><span>
</span></span></span><span><span><span>            </span><span>count</span><span> </span><span>+=</span><span> </span><span>1</span><span>;</span><span>
</span></span></span><span><span><span>            </span><span>if</span><span> </span><span>(</span><span>std</span><span>.</span><span>mem</span><span>.</span><span>eql</span><span>(</span><span>u8</span><span>,</span><span> </span><span>haystack</span><span>[</span><span>i</span><span> </span><span>+</span><span> </span><span>bitpos</span><span> </span><span>+</span><span> </span><span>1</span><span> </span><span>..][</span><span>0</span><span> </span><span>..</span><span> </span><span>k</span><span> </span><span>-</span><span> </span><span>1</span><span>],</span><span> </span><span>needle</span><span>[</span><span>1</span><span>..]))</span><span> </span><span>{</span><span>
</span></span></span><span><span><span>                </span><span>std</span><span>.</span><span>debug</span><span>.</span><span>print</span><span>(</span><span>"found match with count: {}</span><span>\n</span><span>"</span><span>,</span><span> </span><span>.{</span><span>count</span><span>});</span><span>
</span></span></span><span><span><span>                </span><span>return</span><span> </span><span>i</span><span> </span><span>+</span><span> </span><span>bitpos</span><span>;</span><span>
</span></span></span><span><span><span>            </span><span>}</span><span>
</span></span></span><span><span><span>            </span><span>_</span><span> </span><span>=</span><span> </span><span>mask</span><span>.</span><span>toggleFirstSet</span><span>();</span><span>
</span></span></span><span><span><span>        </span><span>}</span><span>
</span></span></span><span><span><span>    </span><span>}</span><span>
</span></span></span></code></pre></div><div><pre tabindex="0"><code data-lang="fallback"><span><span>found match with count: 2792
</span></span></code></pre></div><p>The fact that&nbsp;<code>count</code>&nbsp;is so close to the number of mispredictions suggests that each time the mask is non‑zero we incur a branch miss.</p><p>Unfortunately, there is no obvious way to prevent this with the current algorithm. The state-of-the-art seems to be choosing two bytes in the needle that occur less frequently according to a pre-calculated frequency distribution. This is used in the <a href="https://github.com/BurntSushi/memchr"><code>memchr</code> crate</a> in Rust, as explained by the author in <a href="https://news.ycombinator.com/item?id=44275934">this comment on Hacker News</a>.</p><p>For example, the needle <code>newsletter</code> has the rarest characters <code>w</code> at index <code>2</code> and <code>l</code> at index <code>4</code>.</p><p>The function in <code>memchr</code> can be found <a href="https://github.com/BurntSushi/memchr/blob/3962118774ac511580c5b40fd14323e31629fa52/src/arch/all/packedpair/mod.rs#L163">here</a>. I’ve ported it into Zig, and you can see it <a href="https://github.com/aarol/substr/blob/9392f9557de735929dfb79efa4fc88115341c65d/src/main.zig#L100">here</a>.</p><div><pre tabindex="0"><code data-lang="zig"><span><span><span>    </span><span>const</span><span> </span><span>needle_index_pair</span><span> </span><span>=</span><span> </span><span>find_rarest</span><span>(</span><span>needle</span><span>);</span><span>
</span></span></span><span><span><span>
</span></span></span><span><span><span>    </span><span>const</span><span> </span><span>first_letter</span><span>:</span><span> </span><span>Block</span><span> </span><span>=</span><span> </span><span>@splat</span><span>(</span><span>needle</span><span>[</span><span>needle_index_pair</span><span>[</span><span>0</span><span>]]);</span><span>
</span></span></span><span><span><span>    </span><span>const</span><span> </span><span>first_offset</span><span> </span><span>=</span><span> </span><span>needle_index_pair</span><span>[</span><span>0</span><span>];</span><span>
</span></span></span><span><span><span>    </span><span>const</span><span> </span><span>second_letter</span><span>:</span><span> </span><span>Block</span><span> </span><span>=</span><span> </span><span>@splat</span><span>(</span><span>needle</span><span>[</span><span>needle_index_pair</span><span>[</span><span>1</span><span>]]);</span><span>
</span></span></span><span><span><span>    </span><span>const</span><span> </span><span>second_offset</span><span> </span><span>=</span><span> </span><span>needle_index_pair</span><span>[</span><span>1</span><span>];</span><span>
</span></span></span></code></pre></div><p>The algorithm is the exact same, but the index for <code>first_letter</code> and <code>second_letter</code> now varies according to the pre-calculated frequency distribution.</p><h3 id="benchmarks">Benchmarks
<a href="#benchmarks">#</a></h3><pre>
<span>Benchmark 1 (10000 runs)</span>: ./zig-out/bin/substr --simd
<span>  measurement          </span><span>mean</span><span> ± </span><span>σ</span><span>            </span><span>min</span><span> … </span><span>max</span><span>           </span><span>outliers</span><span>         delta</span>
  wall_time          <span> 472</span><span>us</span> ± <span>62.9</span><span>us</span>    <span> 400</span><span>us</span> … <span>1.62</span><span>ms</span>        735 ( 7%)        0%
  peak_rss           <span>1.20</span><span>MB</span> ± <span>   0</span><span>  </span>    <span>1.20</span><span>MB</span> … <span>1.20</span><span>MB</span>          0 ( 0%)        0%
  cpu_cycles         <span> 376</span><span>K </span> ± <span>44.7</span><span>K </span>    <span> 347</span><span>K </span> … <span>1.46</span><span>M </span>       <span>1213 (12%)</span>        0%
  instructions       <span> 578</span><span>K </span> ± <span>0.54</span><span>  </span>    <span> 578</span><span>K </span> … <span> 578</span><span>K </span>         10 ( 0%)        0%
  cache_references   <span>38.7</span><span>K </span> ± <span> 715</span><span>  </span>    <span>28.3</span><span>K </span> … <span>40.6</span><span>K </span>         96 ( 1%)        0%
  cache_misses       <span>7.37</span><span>K </span> ± <span>5.83</span><span>K </span>    <span>2.78</span><span>K </span> … <span>27.7</span><span>K </span>       <span>1608 (16%)</span>        0%
  branch_misses      <span>2.88</span><span>K </span> ± <span>23.4</span><span>  </span>    <span>2.82</span><span>K </span> … <span>3.08</span><span>K </span>        415 ( 4%)        0%

<span>Benchmark 2 (10000 runs)</span>: ./zig-out/bin/substr --simdv2
<span>  measurement          </span><span>mean</span><span> ± </span><span>σ</span><span>            </span><span>min</span><span> … </span><span>max</span><span>           </span><span>outliers</span><span>         delta</span>
  wall_time          <span> 429</span><span>us</span> ± <span>75.5</span><span>us</span>    <span> 369</span><span>us</span> … <span>3.85</span><span>ms</span>        393 ( 4%)        <span>⚡</span><span>-  9.1% ±  0.4%</span>
  peak_rss           <span>1.20</span><span>MB</span> ± <span>   0</span><span>  </span>    <span>1.20</span><span>MB</span> … <span>1.20</span><span>MB</span>          0 ( 0%)          -  0.0% ±  0.0%
  cpu_cycles         <span> 304</span><span>K </span> ± <span>28.4</span><span>K </span>    <span> 282</span><span>K </span> … <span>1.07</span><span>M </span>       <span>1140 (11%)</span>        <span>⚡</span><span>- 19.2% ±  0.3%</span>
  instructions       <span> 561</span><span>K </span> ± <span>0.52</span><span>  </span>    <span> 561</span><span>K </span> … <span> 561</span><span>K </span>          5 ( 0%)        <span>⚡</span><span>-  2.9% ±  0.0%</span>
  cache_references   <span>38.7</span><span>K </span> ± <span> 610</span><span>  </span>    <span>29.9</span><span>K </span> … <span>40.3</span><span>K </span>         25 ( 0%)          -  0.1% ±  0.0%
  cache_misses       <span>5.21</span><span>K </span> ± <span>3.53</span><span>K </span>    <span>2.57</span><span>K </span> … <span>27.3</span><span>K </span>       <span>1306 (13%)</span>        <span>⚡</span><span>- 29.3% ±  1.8%</span>
  branch_misses      <span>1.07</span><span>K </span> ± <span>14.0</span><span>  </span>    <span>1.02</span><span>K </span> … <span>1.17</span><span>K </span>        275 ( 3%)        <span>⚡</span><span>- 62.8% ±  0.0%</span>

</pre><p>Comparing to the previous SIMD version, the number of branch misses has dropped by 60%, and it’s 9% faster too. Nice!</p><blockquote><p>The number of branch misses is lower, which can cause faster execution, but I suspect that a much bigger impact is the fact that there are less false positives, which means less byte-by-byte memory accesses and comparisons.</p></blockquote><h2 id="avx-512">AVX-512
<a href="#avx-512">#</a></h2><p>Since AMD <a href="https://en.wikipedia.org/wiki/Zen_4">Zen 4 </a>and Intel <a href="https://en.wikipedia.org/wiki/Cannon_Lake_%28microprocessor%29">Cannon Lake</a>, there has been a new SIMD instruction set, AVX-512 with 512-bit instructions – double the size of AVX2. I don’t have a computer that has AVX-512 right now, but I suspect that changing the Zig code to process 64 characters at once would lead to even better results.</p><h2 id="a-smaller-haystack">A smaller haystack
<a href="#a-smaller-haystack">#</a></h2><p>It’s clear that with a very large haystack, the SIMD version is much faster. But what about a tiny input, like less than a hunder characters?</p><p>I did a bit of benchmarking with <code>poop</code>, but I found that I couldn’t accurately measure the speed, since both versions finish extremely very quickly. I decided to use <a href="https://github.com/hendriknielaender/zBench">zBench</a> to do a microbenchmark. I decided to use a snippet from Moby Dick as seen <a href="https://github.com/aarol/substr/blob/main/src/haystack-small.txt">here</a>.</p><pre>+- run test stderr
benchmark              runs     total time     time/run (avg ± σ)    (min ... max)                p75        p99        p995      
-----------------------------------------------------------------------------------------------------------------------------
find_substr            <span>100000   424.368ms      </span><span>4.243us ± 740ns       </span><span>(3.964us ... 107.923us)      </span><span>4.187us    7.075us    7.245us   </span>
find_substr_simd_v2    <span>100000   147.883ms      </span><span>1.478us ± 186ns       </span><span>(1.417us ... 21.354us)       </span><span>1.483us    1.539us    1.548us   </span>
</pre><p>I was surprised to see that even when processing less than a hundred characters, the SIMD algorithm is still faster! The difference between 4μs vs 1μs is extremely small, but it’s slightly faster nonetheless.</p><h2 id="conclusion">Conclusion
<a href="#conclusion">#</a></h2><p>As you can see, SIMD can be used to make substring searching dramatically faster, for both very large and very small strings.</p><p>But if it’s so much better, then why haven’t I made a pull request to change <code>std.mem.indexOf</code> to use SIMD? Well, the reason is that</p><ol><li><code>std.mem.indexOf</code> is generic over element size, and having a size larger than <code>u8</code> makes the algorithm much slower</li><li>The <a href="https://en.wikipedia.org/wiki/Boyer%E2%80%93Moore%E2%80%93Horspool_algorithm">algorithm</a> used in <code>stdmem.indexOf</code> is cross-platform, while the SIMD code wouldn’t be. (not all platforms have SIMD registers at all, Arm has only 128-bit)</li></ol><p>Substring searching is rarely the bottleneck in programs, especially ones written in a fast language like Zig. That’s why I don’t personally think it would be worth it to add it to the standard library.</p><p>Still, it was great to learn about this advanced optimization technique and see some concrete performance measurements from it!</p><p>The full code is available on GitHub <a href="https://github.com/aarol/substr/">here</a>.</p><h2 id="further-reading">Further reading
<a href="#further-reading">#</a></h2><ul><li>SIMD with Zig <a href="https://www.openmymind.net/SIMD-With-Zig/">https://www.openmymind.net/SIMD-With-Zig/</a></li><li>SIMD-friendly algorithms for substring searching: <a href="http://0x80.pl/notesen/2016-11-28-simd-strfind.html">http://0x80.pl/notesen/2016-11-28-simd-strfind.html</a></li><li><code>memchr</code> source code: <a href="https://github.com/BurntSushi/memchr">https://github.com/BurntSushi/memchr</a></li></ul></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Hand-picked selection of articles on AI fundamentals/concepts (201 pts)]]></title>
            <link>https://aman.ai/primers/ai/</link>
            <guid>44862112</guid>
            <pubDate>Mon, 11 Aug 2025 08:59:46 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://aman.ai/primers/ai/">https://aman.ai/primers/ai/</a>, See on <a href="https://news.ycombinator.com/item?id=44862112">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>

  <header>
    
  </header>

  <article>
  <h2 id="overview">Overview</h2>

<ul>
  <li>Here’s a hand-picked selection of articles on AI fundamentals/concepts that cover the entire process of building neural nets to training them to evaluating results.</li>
</ul>

<h2 id="algorithmsarchitecture">Algorithms/Architecture</h2>

<ul>
  <li><a href="https://aman.ai/primers/ai/linear-logistic-regression">Linear and Logistic Regression</a></li>
  <li><a href="https://aman.ai/primers/ai/k-nearest-neighbors">k-Nearest Neighbors</a></li>
  <li><a href="https://aman.ai/primers/ai/clustering">Clustering</a></li>
  <li><a href="https://aman.ai/primers/ai/support-vector-machines">Support Vector Machines (SVM)</a></li>
  <li><a href="https://aman.ai/primers/ai/naive-bayes">Naive Bayes</a></li>
  <li><a href="https://aman.ai/primers/ai/decision-trees-and-ensemble-methods">Decision Trees and Ensemble Methods</a></li>
  <li><a href="https://aman.ai/primers/ai/ml-comp">ML Algorithms Comparative Analysis</a></li>
  <li><a href="https://aman.ai/primers/ai/dl-comp">DL Architectures Comparative Analysis</a></li>
  <li><a href="https://aman.ai/primers/ai/prompt-engineering">Prompt Engineering</a></li>
  <li><a href="https://aman.ai/primers/ai/gan">Generative Adversarial Networks (GANs)</a></li>
  <li><a href="https://aman.ai/primers/ai/diffusion-models">Diffusion Models</a></li>
  <li><a href="https://aman.ai/primers/ai/gnn">Graph Neural Networks</a></li>
  <li><a href="https://aman.ai/primers/ai/attention">Attention</a></li>
  <li><a href="https://aman.ai/primers/ai/separable-convolutions">Separable Convolutions</a></li>
  <li><a href="https://aman.ai/primers/ai/inductive-bias">Inductive Bias</a></li>
  <li><a href="https://aman.ai/primers/ai/cnn">Convolutional Neural Networks</a></li>
  <li><a href="https://aman.ai/primers/ai/reinforcement-learning">Reinforcement Learning</a></li>
  <li><a href="https://aman.ai/primers/ai/mixture-of-experts">Mixture-of-Experts</a></li>
  <li><a href="https://aman.ai/primers/ai/state-space-models">State Space Models</a></li>
  <li><a href="https://aman.ai/primers/ai/agents">Agents</a></li>
  <li><a href="https://aman.ai/primers/ai/flashattention">FlashAttention</a>
<!-- - [Quantization](../ai/quantization) --></li>
  <li><a href="https://aman.ai/primers/ai/model-acceleration">Model Acceleration</a></li>
  <li><a href="https://aman.ai/primers/ai/speculative-decoding">Speculative Decoding</a></li>
  <li><a href="https://aman.ai/primers/ai/cross-validation">Cross Validation</a></li>
</ul>

<h2 id="datatraining">Data/Training</h2>

<ul>
  <li><a href="https://aman.ai/primers/ai/data-sampling">Data Sampling</a></li>
  <li><a href="https://aman.ai/primers/ai/data-imbalance">Data Imbalance</a></li>
  <li><a href="https://aman.ai/primers/ai/standardization-vs-normalization">Standardization vs. Normalization</a></li>
  <li><a href="https://aman.ai/primers/ai/learning-paradigms">Learning Paradigms</a></li>
  <li><a href="https://aman.ai/primers/ai/xavier-init">Xavier Initialization</a></li>
  <li><a href="https://aman.ai/primers/ai/padding-and-packing">Padding and Packing</a></li>
  <li><a href="https://aman.ai/primers/ai/regularization">Regularization</a></li>
  <li><a href="https://aman.ai/primers/ai/gradient-descent-and-backprop">Gradient Descent and Backprop</a></li>
  <li><a href="https://aman.ai/primers/ai/activation-functions">Activation Functions</a></li>
  <li><a href="https://aman.ai/primers/ai/loss">Loss Functions</a></li>
  <li><a href="https://aman.ai/primers/ai/activation">Activation Functions</a></li>
  <li><a href="https://aman.ai/primers/ai/fine-tuning-models">Fine-tuning Models</a></li>
  <li><a href="https://aman.ai/primers/ai/data-split">Splitting Datasets</a></li>
  <li><a href="https://aman.ai/primers/ai/batchnorm">Batchnorm</a></li>
  <li><a href="https://aman.ai/primers/ai/dropout">Dropout</a></li>
  <li><a href="https://aman.ai/primers/ai/double-descent">Double Descent</a></li>
  <li><a href="https://aman.ai/primers/ai/fine-tune-and-eval-BERT">Fine-Tuning and Evaluating BERT</a>
<!-- - [Debugging Deep Learning Projects](../ai/debugging-dl-projects) --></li>
  <li><a href="https://aman.ai/primers/ai/train-val-loss">Training Loss &gt; Validation Loss?</a></li>
  <li><a href="https://aman.ai/primers/ai/svm-kernel-trick">SVM Kernel/Polynomial Trick</a></li>
  <li><a href="https://aman.ai/primers/ai/bias-variance-tradeoff">Bias Variance Tradeoff</a></li>
  <li><a href="https://aman.ai/primers/ai/grad-accum-checkpoint">Gradient Accumulation and Checkpointing</a></li>
  <li><a href="https://aman.ai/primers/ai/parameter-efficient-fine-tuning">Parameter Efficient Fine-Tuning</a></li>
  <li><a href="https://aman.ai/primers/ai/hypernetworks">Hypernetworks</a></li>
  <li><a href="https://aman.ai/primers/ai/distributed-training-parallelism">Distributed Training Parallelism</a></li>
</ul>

<h2 id="speech">Speech</h2>

<ul>
  <li><a href="https://aman.ai/primers/ai/speech-processing">Speech Processing</a></li>
</ul>

<h2 id="vision">Vision</h2>

<ul>
  <li><a href="https://aman.ai/primers/ai/vit">Vision Transformer (ViT)</a></li>
  <li><a href="https://aman.ai/primers/ai/receptive-field">Receptive Field</a></li>
  <li><a href="https://aman.ai/primers/ai/skip-connections">Residual Networks/Skip Connections</a></li>
  <li><a href="https://aman.ai/primers/ai/gpt4o-native-image-generation">GPT-4o Native Image Generation</a></li>
</ul>

<h2 id="nlp">NLP</h2>

<ul>
  <li><a href="https://aman.ai/primers/ai/word-vectors">Word Vectors/Embeddings</a></li>
  <li><a href="https://aman.ai/primers/ai/nlp-tasks">NLP Tasks</a></li>
  <li><a href="https://aman.ai/primers/ai/preprocessing">Preprocessing</a></li>
  <li><a href="https://aman.ai/primers/ai/tokenizer">Tokenization</a></li>
  <li><a href="https://aman.ai/primers/ai/data-sampling">Data Sampling</a></li>
  <li><a href="https://aman.ai/primers/ai/architectures">Neural Architectures</a></li>
  <li><a href="https://aman.ai/primers/ai/attention">Attention</a></li>
  <li><a href="https://aman.ai/primers/ai/transformers">Transformers</a></li>
  <li><a href="https://aman.ai/primers/ai/token-sampling">Token Sampling Methods</a></li>
  <li><a href="https://aman.ai/primers/ai/encoder-vs-decoder-models">Encoder vs. Decoder vs. Encoder-Decoder Models</a>
<!-- - [Language Models](../ai/language-model) --></li>
  <li><a href="https://aman.ai/primers/ai/LLM">Overview of Large Language Models (LLMs)</a></li>
  <li><a href="https://aman.ai/primers/ai/reinforcement-finetuning">Reinforcement Fine-Tuning</a></li>
  <li><a href="https://aman.ai/primers/ai/preference-optimization">Preference Optimization</a></li>
  <li><a href="https://aman.ai/primers/ai/translation">Machine Translation</a></li>
  <li><a href="https://aman.ai/primers/ai/knowledge-graphs">Knowledge Graphs</a></li>
  <li><a href="https://aman.ai/primers/ai/hallucination">Hallucination Detection and Mitigation</a></li>
  <li><a href="https://aman.ai/primers/ai/AIDetect">AI Text Detection Techniques</a></li>
  <li><a href="https://aman.ai/primers/ai/ner">Named Entity Recognition</a></li>
  <li><a href="https://aman.ai/primers/ai/textual-entailment">Textual Entailment</a></li>
  <li><a href="https://aman.ai/primers/ai/RAG">Retrieval Augmented Generation (RAG)</a></li>
  <li><a href="https://aman.ai/primers/ai/context-length-extension">LLM Context Length Extension</a></li>
  <li><a href="https://aman.ai/primers/ai/document-intelligence">Document Intelligence</a>
<!-- - [Personalizing Large Language Models](../ai/personalize-LLMs) --></li>
  <li><a href="https://aman.ai/primers/ai/code-mixing-switching">Code Mixing and Switching</a></li>
  <li><a href="https://aman.ai/primers/ai/LLMOps">Large Language Model Ops (LLMOps)</a></li>
  <li><a href="https://aman.ai/primers/ai/benchmarks">LLM/VLM Benchmarks</a></li>
</ul>

<h2 id="multimodal">Multimodal</h2>

<ul>
  <li><a href="https://aman.ai/primers/ai/VLM">Overview of Vision-Language Models (VLMs)</a></li>
  <li><a href="https://aman.ai/primers/ai/vision-language-models">VLM Architectures</a></li>
  <li><a href="https://aman.ai/primers/ai/computer-control">Computer Control</a></li>
</ul>

<h2 id="models">Models</h2>

<ul>
  <li><a href="https://aman.ai/primers/ai/bert">BERT</a></li>
  <li><a href="https://aman.ai/primers/ai/gpt">GPT</a></li>
  <li><a href="https://aman.ai/primers/ai/CLIP">CLIP</a></li>
  <li><a href="https://aman.ai/primers/ai/meena">Meena</a></li>
  <li><a href="https://aman.ai/primers/ai/chatGPT">ChatGPT</a></li>
  <li><a href="https://aman.ai/primers/ai/GPT-4">GPT-4</a></li>
  <li><a href="https://aman.ai/primers/ai/LLaMA">LLaMA</a></li>
  <li><a href="https://aman.ai/primers/ai/alpaca">Alpaca</a></li>
  <li><a href="https://aman.ai/primers/ai/gemini">Gemini</a></li>
  <li><a href="https://aman.ai/primers/ai/toolformer">Toolformer</a></li>
  <li><a href="https://aman.ai/primers/ai/visualChatGPT">Visual ChatGPT</a></li>
  <li><a href="https://aman.ai/primers/ai/TaskMatrix">TaskMatrix.AI</a></li>
  <li><a href="https://aman.ai/primers/ai/bigbird">BigBird</a></li>
  <li><a href="https://aman.ai/primers/ai/o1">OpenAI o1</a></li>
  <li><a href="https://aman.ai/primers/ai/deepseek-R1">DeepSeek R1</a></li>
  <li><a href="https://aman.ai/primers/ai/deepseek-janus-pro">DeepSeek Janus-Pro</a></li>
  <li><a href="https://aman.ai/primers/ai/gemma-3n">Gemma 3n</a></li>
</ul>

<h2 id="offlineonline-evaluation">Offline/Online Evaluation</h2>

<ul>
  <li><a href="https://aman.ai/primers/ai/evaluation-metrics">Evaluation Metrics</a></li>
  <li><a href="https://aman.ai/primers/ai/f-beta">F-Beta Score</a></li>
  <li><a href="https://aman.ai/primers/ai/ab-testing">A/B Testing</a></li>
</ul>

<h2 id="mlops">MLOps</h2>

<ul>
  <li><a href="https://aman.ai/primers/ai/drift">Data Drift</a></li>
  <li><a href="https://aman.ai/primers/ai/mlops-tooling">MLOps Tooling</a></li>
  <li><a href="https://aman.ai/primers/ai/mlops-testing">MLOps Testing</a></li>
</ul>

<h2 id="on-device-ai">On-Device AI</h2>

<ul>
  <li><a href="https://aman.ai/primers/ai/model-compression">Model Compression</a></li>
  <li><a href="https://aman.ai/primers/ai/pii">Personally Identifiable Information (PII)</a></li>
  <li><a href="https://aman.ai/primers/ai/federated-learning">Federated Learning</a></li>
  <li><a href="https://aman.ai/primers/ai/differential-privacy">Differential Privacy</a></li>
  <li><a href="https://aman.ai/primers/ai/on-device-transformers">On-device Transformers</a></li>
</ul>

<h2 id="project-planning-scheduling-execution">Project Planning, Scheduling, Execution</h2>

<ul>
  <li><a href="https://aman.ai/primers/ai/okr">Objectives and Key Results (OKRs)</a></li>
  <li><a href="https://aman.ai/primers/ai/rice-framework">RICE Framework</a></li>
  <li><a href="https://aman.ai/primers/ai/gantt-charts">Gantt Charts</a></li>
  <li><a href="https://aman.ai/primers/ai/project-management">Project Management</a></li>
</ul>

<h2 id="miscellaneous">Miscellaneous</h2>

<ul>
  <li><a href="https://aman.ai/primers/ai/top-30-papers">Ilya Sutskever’s Top 30</a></li>
  <li><a href="https://aman.ai/primers/ai/model-debugging">Debugging Model Training</a></li>
  <li><a href="https://aman.ai/primers/ai/ml-runtimes">ML Runtimes</a></li>
  <li><a href="https://aman.ai/primers/ai/chain-rule">Chain Rule</a></li>
  <li><a href="https://aman.ai/primers/ai/bayes-theorem">Bayes’ Theorem</a></li>
  <li><a href="https://aman.ai/primers/ai/probability-calibration">Probability Calibration</a></li>
  <li><a href="https://aman.ai/primers/ai/multiclass-vs-multilabel-classification">Multiclass vs. Multilabel Classification</a></li>
  <li><a href="https://aman.ai/primers/ai/matmul">N-Dimensional Tensor Product</a></li>
  <li><a href="https://aman.ai/primers/ai/pytorch-vs-tensorflow">PyTorch vs. TensorFlow</a></li>
  <li><a href="https://aman.ai/primers/ai/ann-similarity-search">Approximate Nearest Neighbors – Similarity Search</a></li>
  <li><a href="https://aman.ai/primers/ai/transferability-estimation">Transferability Estimation</a></li>
  <li><a href="https://aman.ai/primers/ai/tensorboard">TensorBoard</a></li>
  <li><a href="https://aman.ai/primers/ai/cnns-for-text-classification">Convolutional Neural Networks for Text Classification</a></li>
  <li><a href="https://aman.ai/primers/ai/hmm-and-naive-bayes">Relationship between Hidden Markov Models and Naive Bayes</a></li>
  <li><a href="https://aman.ai/primers/ai/maximum-entropy-markov-models-and-logistic-reg">Maximum Entropy Markov Models</a></li>
  <li><a href="https://aman.ai/primers/ai/conditional-random-fields">Conditional Random Fields</a>
<!-- - [Information Retrieval Methods](../ai/info-retrieval-methods) --></li>
</ul>

<h2 id="hyperparameters">Hyperparameters</h2>

<ul>
  <li><a href="https://aman.ai/primers/ai/hyperparameter-tuning">Hyperparameter Tuning</a></li>
  <li><a href="https://aman.ai/primers/ai/hyperparameter-logging">Hyperparameter Logging</a></li>
</ul>

<h2 id="practice">Practice</h2>

<ul>
  <li><a href="https://aman.ai/primers/ai/interview">Interview Questions</a></li>
</ul>

  </article>

</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[AOL to discontinue dial-up internet (126 pts)]]></title>
            <link>https://www.nytimes.com/2025/08/11/business/aol-dial-up-internet.html</link>
            <guid>44861521</guid>
            <pubDate>Mon, 11 Aug 2025 07:15:38 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.nytimes.com/2025/08/11/business/aol-dial-up-internet.html">https://www.nytimes.com/2025/08/11/business/aol-dial-up-internet.html</a>, See on <a href="https://news.ycombinator.com/item?id=44861521">Hacker News</a></p>
Couldn't get https://www.nytimes.com/2025/08/11/business/aol-dial-up-internet.html: Error: Request failed with status code 403]]></description>
        </item>
        <item>
            <title><![CDATA[Google paid a $250K reward for a bug (480 pts)]]></title>
            <link>https://issues.chromium.org/issues/412578726</link>
            <guid>44861106</guid>
            <pubDate>Mon, 11 Aug 2025 05:56:33 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://issues.chromium.org/issues/412578726">https://issues.chromium.org/issues/412578726</a>, See on <a href="https://news.ycombinator.com/item?id=44861106">Hacker News</a></p>
<div id="readability-page-1" class="page"><header><div ng-non-bindable="" data-ogsr-up="" id="gb"><p><a aria-label="Sign in" href="https://accounts.google.com/ServiceLogin?passive=1209600&amp;osid=1&amp;continue=https%3A%2F%2Fissues.chromium.org%2Fissues%2F412578726&amp;followup=https%3A%2F%2Fissues.chromium.org%2Fissues%2F412578726&amp;ec=GAZAkwI" target="_top"><span>Sign in</span></a></p></div></header></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Basic Social Skills Guide (214 pts)]]></title>
            <link>https://www.improveyoursocialskills.com/basic-social-skills-guide</link>
            <guid>44860932</guid>
            <pubDate>Mon, 11 Aug 2025 05:19:01 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.improveyoursocialskills.com/basic-social-skills-guide">https://www.improveyoursocialskills.com/basic-social-skills-guide</a>, See on <a href="https://news.ycombinator.com/item?id=44860932">Hacker News</a></p>
Couldn't get https://www.improveyoursocialskills.com/basic-social-skills-guide: Error: Request failed with status code 521]]></description>
        </item>
        <item>
            <title><![CDATA[Generic Containers in C: Safe Division Using Maybe (101 pts)]]></title>
            <link>https://uecker.codeberg.page/2025-08-10.html</link>
            <guid>44860908</guid>
            <pubDate>Mon, 11 Aug 2025 05:14:06 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://uecker.codeberg.page/2025-08-10.html">https://uecker.codeberg.page/2025-08-10.html</a>, See on <a href="https://news.ycombinator.com/item?id=44860908">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="body">
	
	
	
        <h3>Generic Containers in C: Safe Division Using Maybe.</h3>
        <div max-width="150px"><p>
	Martin Uecker, 2025-08-10</p><p>
	
	I discuss the implementation of type and bounds safe generic containers in C.
	Previously, I discussed a <a href="https://uecker.codeberg.page/2025-07-02.html">span type</a>, 
	<a href="https://uecker.codeberg.page/2025-07-09.html">bounds checking using arrays</a>. and a
	<a href="https://uecker.codeberg.page/2025-07-20.html">a vector type</a>.
	</p><p>
	This time, I will discuss <code>maybe</code> inspired by Haskell.  This type
	can used to return a value that may not exist, e.g. because an error was
	encountered during the computation.  The following examples shows for
	a <code>divide</code> function that catches division by zero.</p></div>
	<pre><code>
	static maybe(int) divide(int a, int b)
	{
	    return (b != 0) ? maybe_just(int, a / b) : maybe_nothing(int);
	}
	</code></pre>
        <div max-width="150px"><p>
	But careful, there is another error case not checked here! Which is it?
	</p><p>
	As usual, we can define it simply as a macro that expands into a structure,
	and define simple type constructors.
	</p></div>
	<pre><code>
	#define maybe(T) 		struct maybe_##T { bool ok; T value; }
	#define maybe_just(T, x)	(maybe(T)){ .value = (x), .ok = true }
	#define maybe_nothing(T)	(maybe(T)){ .value = (T){ }, .ok = false }
	</code></pre>
        <p max-width="150px">
	In the caller, we can then check whether the value exists or not.
	</p>
	<pre><code>
	int main()
	{
	    int d = 2; // 0

	    maybe(int) p = divide(6, d);

	    if (p.ok) {

	        printf("%d\n", p.value);

	    } else {

        	printf("division by zero\n");
		fflush(stdout);
	    }

	    return 0;
	}
	</code></pre>
        <p max-width="150px">
	Can we make this safer to use? In principle, we like to get some
	error if we try to use the value although it does not exist.  For this,
	we add a macro <code>maybe_value</code> which includes a check.
	</p>
	<pre><code>
	#define maybe_value(T, x) (*({ maybe(T) *_p = &amp;(x); _p-&gt;ok ? &amp;_p-&gt;value : (void*)0; }))
	</code></pre>
        <p max-width="150px">
	Here, instead of handling the error condition, I create an lvalue that
	points nowhere in case of an error because it then corresponds to
	<code>(*({ (void*)0; }))</code>, relying on the null sanitizer to transform
	it into a run-time trap for safety.
	</p>
	<pre><code>
	maybe(int) p = divide(6, d);

	if (p.ok) {

		printf("%d\n", maybe_value(p));
	}
	</code></pre>
        <div max-width="150px"><p>
	You can find the full example here: <a href="https://godbolt.org/z/WWfGcvrc4">Godbolt</a></p><p>
	But as mentioned above, there is another case where integer division has undefined
	behavior in C. If we divide the smallest representable integer by minus one, then the
	result is one larger than the biggest representable integer.  Let's also add a test
	for this!
	</p></div>
	<pre><code>
	maybe(int) unsafe_divide(int a, int b)
	{ 
		if (b == -1 &amp;&amp; a == INT_MAX)
			return maybe_nothing(int);

		return (b != 0) ? maybe_just(int, a / b) : maybe_nothing(int);
	}
	</code></pre>
        <p max-width="150px">
	So we created a safe function for integer division.  But can we be sure it
	is safe?  Maybe we made mistake.  Now, there are tools and a complete industry
	that may be able to help with this, but instead let's first simply look at the
	<a href="https://godbolt.org/z/1PKEnxMd7">assembly</a> generated by GCC when
	using the signed overflow sanitizer in trapping mode with
	<code>-O2 -fsanitize=signed-integer-overflow,integer-divide-by-zero -fsanitize-trap=undefined</code>.
	</p>
	<pre><code>
unsafe_divide:
        cmp     esi, -1
        sete    dl
        cmp     edi, 2147483647
        jne     .L2
        test    dl, dl
        je      .L2
.L4:
        xor     eax, eax
        ret
.L2:
        test    esi, esi
        je      .L4
        cmp     edi, -2147483648
        je      .L20
        mov     eax, edi
        cdq
        idiv    esi
        sal     rax, 32
        or      rax, 1
        ret
.L20:
        test    dl, dl
        jne     .L18
        mov     eax, edi
        cdq
        idiv    esi
        sal     rax, 32
        or      rax, 1
        ret
safe_divide.cold:
.L18:
        ud2 
	</code></pre>
        <p max-width="150px">
	This is strange, there is still a code path that ends in a trap in the form of
	the <code>ud2</code> instruction.  So either the optimizer was not able to see
	that this is not possible or our check was incorrect. In fact, I got it wrong and
	we have to check against <code>INT_MIN</code> and not <code>INT_MAX</code>. Here
	is the corrected and nicer version.
	</p>
	<pre><code>
	maybe(int) safe_divide(int a, int b) 
	{ 
		if (b == 0 || (b == -1 &amp;&amp; a == INT_MIN))
			return maybe_nothing(int);

		return maybe_just(int, a / b);
	}
	</code></pre>
        <p max-width="150px">
	The <a href="https://godbolt.org/z/PzPG1GjrT">assembly</a> looks different now
	and does not contain a code path leading to an <code>ud2</code> anymore.
	</p>
	<pre><code>
safe_divide:
        test    esi, esi
        je      .L2
        cmp     esi, -1
        jne     .L3
        cmp     edi, -2147483648
        je      .L2
.L3:
        mov     eax, edi
        cdq
        idiv    esi
        sal     rax, 32
        or      rax, 1
        ret
.L2:
        xor     eax, eax
        ret
	</code></pre>
        <div max-width="150px"><p>
	The optimizer has proven that there is no overflow or division by zero left in our function!
	Does this also work for our complete <a href="https://godbolt.org/z/E6jf38M79">example</a>
	using <code>maybe</code>? It does! The optimizer has statically shown that there is no overflow
	and that all error cases are handled. Isn't this cool!  One could now turn off the sanitizer
	and still be sure that there is no overflow possible, as it was statically proven.
	</p><p>
	Of course, this can not be used to show that C programs are completely memory safe, as
	there areas which are not covered by the sanitizers, and there are also sanitizers that do
	not catch all undefined behavior in their respective domain. In particular, lifetime issues
	and pointer arithmetic are not covered. Thus, one has to stress that this approach is
	very limited when trying to prove safety properties of legacy C programs in this way.
	Still, if you use VLAs and variably modified types instead of pointer arithmetic as discussed
	<a href="https://uecker.codeberg.page/2025-07-09.html">previously</a>, you can even have your bounds checked;
	see for yourself <a href="https://godbolt.org/z/Y6zMd7o3P">example</a>!
	</p><p>
	If you want, you can check out my experimental library where I am experimenting
	with these ideas: <a href="https://codeberg.org/uecker/noplate/">link</a>. If you
	have ideas on how to do this better, let me know!
	</p></div>
    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Going faster than memcpy (130 pts)]]></title>
            <link>https://squadrick.dev/journal/going-faster-than-memcpy</link>
            <guid>44860847</guid>
            <pubDate>Mon, 11 Aug 2025 04:59:03 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://squadrick.dev/journal/going-faster-than-memcpy">https://squadrick.dev/journal/going-faster-than-memcpy</a>, See on <a href="https://news.ycombinator.com/item?id=44860847">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
      



<p>While profiling <a href="https://github.com/squadrick/shadesmar">Shadesmar</a> a couple of
weeks ago, I noticed that for large binary unserialized messages (&gt;512kB) most
of the execution time is spent doing copying the message (using <code>memcpy</code>)
between process memory to shared memory and back.</p>

<p>I had a few hours to kill last weekend, and I tried to implement a faster way
to do memory copies.</p>

<hr>

<h3 id="autopsy-of-memcpy">Autopsy of memcpy</h3>

<p>Here’s the dumb of <a href="https://perf.wiki.kernel.org/index.php/Main_Page"><code>perf</code></a>
when running pub-sub for messages of sizes between 512kB and 2MB.</p>

<div><pre><code> Children      Self  Shared Object      Symbol
+  99.86%     0.00%  libc-2.27.so       [.] __libc_start_main
+  99.86%     0.00%  [unknown]          [k] 0x4426258d4c544155
+  99.84%     0.02%  raw_benchmark      [.] main
+  98.13%    97.12%  libc-2.27.so       [.] __memmove_avx_unaligned_erms
+  51.99%     0.00%  raw_benchmark      [.] shm::PublisherBin&lt;16u&gt;::publish
+  51.98%     0.01%  raw_benchmark      [.] shm::Topic&lt;16u&gt;::write
+  47.64%     0.01%  raw_benchmark      [.] shm::Topic&lt;16u&gt;::read
</code></pre></div>

<p><code>__memmove_avx_unaligned_erms</code> is an implementation of <code>memcpy</code> for unaligned
memory blocks that uses AVX to copy over 32 bytes at a time. Digging into the
<code>glibc</code> source code, I found this:</p>

<div><pre><code><span>#if IS_IN (libc)
# define VEC_SIZE                32
# define VEC(i)                  ymm##i
# define VMOVNT                  vmovntdq
# define VMOVU                   vmovdqu
# define VMOVA                   vmovdqa
# define SECTION(p)              p##.avx
# define MEMMOVE_SYMBOL(p,s)     p##_avx_##s
</span>
<span># include "memmove-vec-unaligned-erms.S"
#endif
</span></code></pre></div>

<p>Breaking down this function:</p>

<p><code>memmove</code>: <code>glibc</code> implements <code>memcpy</code> as a <code>memmove</code> instead, here’s the
relevant source code:</p>

<div><pre><code><span># define SYMBOL_NAME memcpy
# include "ifunc-memmove.h"
</span>
<span>libc_ifunc_redirected</span> <span>(</span><span>__redirect_memcpy</span><span>,</span> <span>__new_memcpy</span><span>,</span>
		       <span>IFUNC_SELECTOR</span> <span>());</span>
</code></pre></div>

<p>Here’s the difference between the two: With <code>memcpy</code>, the destination cannot
overlap the source at all. With <code>memmove</code> it can. Initially, I wasn’t sure why
it was implemented as <code>memmove</code>. The reason for this will become clearer as
the post proceeds.</p>

<p><code>erms</code>: <em>E</em>nhanced <em>R</em>ep <em>M</em>ov<em>s</em> is a hardware optimization for a loop that
does a simple copy. In simple pseudo-code, this is what the loop implementation
looks like for copying a single byte at a time (<code>REP MOVSB</code>).</p>

<div><pre><code><span>void</span> <span>rep_movsb</span><span>(</span><span>void</span> <span>*</span><span>dest</span><span>,</span> <span>const</span> <span>void</span> <span>*</span><span>src</span><span>,</span> <span>size_t</span> <span>len</span><span>)</span> <span>{</span>
  <span>const</span> <span>uint8_t</span><span>*</span> <span>s</span> <span>=</span> <span>(</span><span>uint8_t</span><span>*</span><span>)</span><span>src</span><span>;</span>
  <span>uint8_t</span><span>*</span> <span>d</span> <span>=</span> <span>(</span><span>uint8_t</span><span>*</span><span>)</span><span>dest</span><span>;</span>

  <span>while</span> <span>(</span><span>len</span><span>--</span><span>)</span>
    <span>*</span><span>d</span><span>++</span> <span>=</span> <span>*</span><span>s</span><span>++</span><span>;</span>

  <span>return</span> <span>dest</span><span>;</span>
<span>}</span>
</code></pre></div>

<p>Since the loop copies data pointer by pointer, it can handle the case of
overlapping data.</p>

<p><code>vec</code>: For the above loop rather than copying around single bytes, it uses x86
vectorized instructions to copy multiple bytes in a single loop iteration
(technically single instruction). <code>vmov*</code> are assembly instructions for AVX
which is the latest instruction set that the CPU on my laptop supports. With
<code>VEC_SIZE = 32</code>, it copies 32 bytes at a time.</p>

<p><code>unaligned</code>: This is a generic version of <code>memmove</code> that can copy between any
pointer locations irrespective of their alignment. Unaligned pointers increase
complexity for the copy loop when using vectorized instructions. The unaligned
preceeding and trailing memory locations must be copied separately before hitting the
optimized loop.</p>

<p><code>memmove-vec-unaligned-erms.S</code> holds the actual implementation in assembly. A
few things that the implementation does:</p>

<ol>
  <li>
    <p>It uses <code>REP MOVS</code> only if the data is greater than 4kB. For smaller values it uses the SSE2 optimization.</p>
  </li>
  <li>For handling <code>unaligned</code> pointers, it uses the following blocks:
    <ul>
      <li>16 to 31: <code>vmovdqu</code></li>
      <li>15 to 8: <code>movq</code></li>
      <li>7 to 4: <code>movl</code></li>
      <li>3 to 2: <code>movzwl</code> and <code>movw</code></li>
    </ul>
  </li>
  <li><code>VMOVNT</code> defined above is for doing non-temporal(NT) moves. NT instructions
are used when there is an overlap between destination and source since
destination may be in cache when source is loaded. Uses <code>prefetcht0</code> to load
data into cache (all levels: t0). In the current iteration, we prefetch the
data for 2 iterations later. The data is copied (via cache) into registers. The
data (via NT) is copied from registers into destination.</li>
</ol>

<div><pre><code><span>L</span><span>(</span><span>loop_large_forward</span><span>):</span>
	<span>; Copy 4 * VEC a time forward with non-temporal stores.</span>
	<span>PREFETCH_ONE_SET</span> <span>(</span><span>1</span><span>,</span> <span>(</span><span>%</span><span>rsi</span><span>),</span> <span>PREFETCHED_LOAD_SIZE</span> <span>*</span> <span>2</span><span>)</span>
	<span>PREFETCH_ONE_SET</span> <span>(</span><span>1</span><span>,</span> <span>(</span><span>%</span><span>rsi</span><span>),</span> <span>PREFETCHED_LOAD_SIZE</span> <span>*</span> <span>3</span><span>)</span>
  <span>; PREFETCH 256b from rsi+256 to rsi+511</span>

	<span>VMOVU</span>	<span>(</span><span>%</span><span>rsi</span><span>),</span> <span>%</span><span>VEC</span><span>(</span><span>0</span><span>)</span>
	<span>VMOVU</span>	<span>VEC_SIZE</span><span>(</span><span>%</span><span>rsi</span><span>),</span> <span>%</span><span>VEC</span><span>(</span><span>1</span><span>)</span>
	<span>VMOVU</span>	<span>(</span><span>VEC_SIZE</span> <span>*</span> <span>2</span><span>)(</span><span>%</span><span>rsi</span><span>),</span> <span>%</span><span>VEC</span><span>(</span><span>2</span><span>)</span>
	<span>VMOVU</span>	<span>(</span><span>VEC_SIZE</span> <span>*</span> <span>3</span><span>)(</span><span>%</span><span>rsi</span><span>),</span> <span>%</span><span>VEC</span><span>(</span><span>3</span><span>)</span>
  <span>; mov 128b from rsi to rsi+127 -&gt; 4 ymm registers (cache)</span>
  <span>; 2 loops later, we hit the prefetched values</span>

	<span>addq</span>	<span>$</span><span>PREFETCHED_LOAD_SIZE</span><span>,</span> <span>%</span><span>rsi</span>  <span>; advance to rsi+128 in next loop</span>
	<span>subq</span>	<span>$</span><span>PREFETCHED_LOAD_SIZE</span><span>,</span> <span>%</span><span>rdx</span>

	<span>VMOVNT</span>	<span>%</span><span>VEC</span><span>(</span><span>0</span><span>),</span> <span>(</span><span>%</span><span>rdi</span><span>)</span>
	<span>VMOVNT</span>	<span>%</span><span>VEC</span><span>(</span><span>1</span><span>),</span> <span>VEC_SIZE</span><span>(</span><span>%</span><span>rdi</span><span>)</span>
	<span>VMOVNT</span>	<span>%</span><span>VEC</span><span>(</span><span>2</span><span>),</span> <span>(</span><span>VEC_SIZE</span> <span>*</span> <span>2</span><span>)(</span><span>%</span><span>rdi</span><span>)</span>
	<span>VMOVNT</span>	<span>%</span><span>VEC</span><span>(</span><span>3</span><span>),</span> <span>(</span><span>VEC_SIZE</span> <span>*</span> <span>3</span><span>)(</span><span>%</span><span>rdi</span><span>)</span>
  <span>; mov 128b from 4 ymm register -&gt; rdi to rdi+127 (no cache)</span>

	<span>addq</span>	<span>$</span><span>PREFETCHED_LOAD_SIZE</span><span>,</span> <span>%</span><span>rdi</span>  <span>; advance to rdi+128 in next loop</span>
	<span>cmpq</span>	<span>$</span><span>PREFETCHED_LOAD_SIZE</span><span>,</span> <span>%</span><span>rdx</span>
	<span>ja</span>	<span>L</span><span>(</span><span>loop_large_forward</span><span>)</span>
</code></pre></div>

<hr>

<h3 id="method-1-basic-rep-movsb">Method 1: Basic REP MOVSB</h3>

<p>Before getting into more exotic implementations, I wanted to first implement a
super simple version of ERSB to see how well it would perform. I used inline
assembly to write out the loop.</p>

<div><pre><code><span>void</span> <span>_rep_movsb</span><span>(</span><span>void</span> <span>*</span><span>d</span><span>,</span> <span>const</span> <span>void</span> <span>*</span><span>s</span><span>,</span> <span>size_t</span> <span>n</span><span>)</span> <span>{</span>
  <span>asm</span> <span>volatile</span><span>(</span><span>"rep movsb"</span>
               <span>:</span> <span>"=D"</span><span>(</span><span>d</span><span>),</span> <span>"=S"</span><span>(</span><span>s</span><span>),</span> <span>"=c"</span><span>(</span><span>n</span><span>)</span>
               <span>:</span> <span>"0"</span><span>(</span><span>d</span><span>),</span> <span>"1"</span><span>(</span><span>s</span><span>),</span> <span>"2"</span><span>(</span><span>n</span><span>)</span>
               <span>:</span> <span>"memory"</span><span>);</span>
<span>}</span>
</code></pre></div>

<p>This does the same as the pseudo-code attached above, but I wrote it in
assembly to prevent any compiler optimization, and rely only on the hardware
ERMS optimization.</p>

<h3 id="alternate-2-aligned-avx">Alternate 2: Aligned AVX</h3>

<p>One of the complexities in <code>glibc</code>’s  implementation is getting it to work for
unaligned pointers. Since I control the memory allocation, I figured I could
recreate the implementation focused solely on aligned pointer and sizes. I’m
using AVX intrinsics for 32-byte vectors (AVX):</p>

<div><pre><code><span>void</span> <span>_avx_cpy</span><span>(</span><span>void</span> <span>*</span><span>d</span><span>,</span> <span>const</span> <span>void</span> <span>*</span><span>s</span><span>,</span> <span>size_t</span> <span>n</span><span>)</span> <span>{</span>
  <span>// d, s -&gt; 32 byte aligned</span>
  <span>// n -&gt; multiple of 32</span>
  <span>auto</span> <span>*</span><span>dVec</span> <span>=</span> <span>reinterpret_cast</span><span>&lt;</span><span>__m256i</span> <span>*&gt;</span><span>(</span><span>d</span><span>);</span>
  <span>const</span> <span>auto</span> <span>*</span><span>sVec</span> <span>=</span> <span>reinterpret_cast</span><span>&lt;</span><span>const</span> <span>__m256i</span> <span>*&gt;</span><span>(</span><span>s</span><span>);</span>
  <span>size_t</span> <span>nVec</span> <span>=</span> <span>n</span> <span>/</span> <span>sizeof</span><span>(</span><span>__m256i</span><span>);</span>
  <span>for</span> <span>(;</span> <span>nVec</span> <span>&gt;</span> <span>0</span><span>;</span> <span>nVec</span><span>--</span><span>,</span> <span>sVec</span><span>++</span><span>,</span> <span>dVec</span><span>++</span><span>)</span> <span>{</span>
    <span>const</span> <span>__m256i</span> <span>temp</span> <span>=</span> <span>_mm256_load_si256</span><span>(</span><span>sVec</span><span>);</span>
    <span>_mm256_store_si256</span><span>(</span><span>dVec</span><span>,</span> <span>temp</span><span>);</span>
  <span>}</span>
<span>}</span>
</code></pre></div>

<p>The logic is identical to the previous <code>REP MOVSB</code> loop instead operating on 32
bytes at a time.</p>

<h3 id="method-3-stream-aligned-avx">Method 3: Stream aligned AVX</h3>

<p><code>_mm256_load_si256</code> and <code>_mm256_store_si256</code> go through the cache, which incurs
additional overhead. AVX instruction set has <code>_stream_</code> load and store
instructions that skip the cache. The performance of this copy is dependant
on:</p>
<ol>
  <li>Quantity of data to copy</li>
  <li>Cache size</li>
</ol>

<p>Non-temporal moves may bog down the performance for smaller copies (that can
fit into L2 cache) compared to regular moves.</p>

<div><pre><code><span>void</span> <span>_avx_async_cpy</span><span>(</span><span>void</span> <span>*</span><span>d</span><span>,</span> <span>const</span> <span>void</span> <span>*</span><span>s</span><span>,</span> <span>size_t</span> <span>n</span><span>)</span> <span>{</span>
  <span>// d, s -&gt; 32 byte aligned</span>
  <span>// n -&gt; multiple of 32</span>
  <span>auto</span> <span>*</span><span>dVec</span> <span>=</span> <span>reinterpret_cast</span><span>&lt;</span><span>__m256i</span> <span>*&gt;</span><span>(</span><span>d</span><span>);</span>
  <span>const</span> <span>auto</span> <span>*</span><span>sVec</span> <span>=</span> <span>reinterpret_cast</span><span>&lt;</span><span>const</span> <span>__m256i</span> <span>*&gt;</span><span>(</span><span>s</span><span>);</span>
  <span>size_t</span> <span>nVec</span> <span>=</span> <span>n</span> <span>/</span> <span>sizeof</span><span>(</span><span>__m256i</span><span>);</span>
  <span>for</span> <span>(;</span> <span>nVec</span> <span>&gt;</span> <span>0</span><span>;</span> <span>nVec</span><span>--</span><span>,</span> <span>sVec</span><span>++</span><span>,</span> <span>dVec</span><span>++</span><span>)</span> <span>{</span>
    <span>const</span> <span>__m256i</span> <span>temp</span> <span>=</span> <span>_mm256_stream_load_si256</span><span>(</span><span>sVec</span><span>);</span>
    <span>_mm256_stream_si256</span><span>(</span><span>dVec</span><span>,</span> <span>temp</span><span>);</span>
  <span>}</span>
  <span>_mm_sfence</span><span>();</span>
<span>}</span>
</code></pre></div>

<p>Exact code as before but using non-temporal moves instead. There’s an extra
<code>_mm_sfence</code> which guarantees that all stores in the preceding loop are 
visible globally.</p>

<h3 id="method-4-stream-aligned-avx-with-prefetch">Method 4: Stream aligned AVX with prefetch</h3>

<p>In the previous method, we skipped the cache entirely. We can squeeze a bit
more performance by prefetching the source data into the cache for the next
iteration in the current iteration. Since all prefetches work on cache-lines
(64-bytes), each loop iteration copies 64-bytes from source to data.</p>

<div><pre><code><span>void</span> <span>_avx_async_pf_cpy</span><span>(</span><span>void</span> <span>*</span><span>d</span><span>,</span> <span>const</span> <span>void</span> <span>*</span><span>s</span><span>,</span> <span>size_t</span> <span>n</span><span>)</span> <span>{</span>
  <span>// d, s -&gt; 64 byte aligned</span>
  <span>// n -&gt; multiple of 64</span>

  <span>auto</span> <span>*</span><span>dVec</span> <span>=</span> <span>reinterpret_cast</span><span>&lt;</span><span>__m256i</span> <span>*&gt;</span><span>(</span><span>d</span><span>);</span>
  <span>const</span> <span>auto</span> <span>*</span><span>sVec</span> <span>=</span> <span>reinterpret_cast</span><span>&lt;</span><span>const</span> <span>__m256i</span> <span>*&gt;</span><span>(</span><span>s</span><span>);</span>
  <span>size_t</span> <span>nVec</span> <span>=</span> <span>n</span> <span>/</span> <span>sizeof</span><span>(</span><span>__m256i</span><span>);</span>
  <span>for</span> <span>(;</span> <span>nVec</span> <span>&gt;</span> <span>2</span><span>;</span> <span>nVec</span> <span>-=</span> <span>2</span><span>,</span> <span>sVec</span> <span>+=</span> <span>2</span><span>,</span> <span>dVec</span> <span>+=</span> <span>2</span><span>)</span> <span>{</span>
    <span>// prefetch the next iteration's data</span>
    <span>// by default _mm_prefetch moves the entire cache-lint (64b)</span>
    <span>_mm_prefetch</span><span>(</span><span>sVec</span> <span>+</span> <span>2</span><span>,</span> <span>_MM_HINT_T0</span><span>);</span>

    <span>_mm256_stream_si256</span><span>(</span><span>dVec</span><span>,</span> <span>_mm256_load_si256</span><span>(</span><span>sVec</span><span>));</span>
    <span>_mm256_stream_si256</span><span>(</span><span>dVec</span> <span>+</span> <span>1</span><span>,</span> <span>_mm256_load_si256</span><span>(</span><span>sVec</span> <span>+</span> <span>1</span><span>));</span>
  <span>}</span>
  <span>_mm256_stream_si256</span><span>(</span><span>dVec</span><span>,</span> <span>_mm256_load_si256</span><span>(</span><span>sVec</span><span>));</span>
  <span>_mm256_stream_si256</span><span>(</span><span>dVec</span> <span>+</span> <span>1</span><span>,</span> <span>_mm256_load_si256</span><span>(</span><span>sVec</span> <span>+</span> <span>1</span><span>));</span>
  <span>_mm_sfence</span><span>();</span>
<span>}</span>
</code></pre></div>

<p>The load from source pointer to register should <strong>not</strong> skip the cache since
that data is explicitly prefetched into the cache, non-stream
<code>_mm256_load_si256</code> must be used instead.</p>

<p>This also unrolls the loop for 2 copies at a time instead of a single copy.
This is to guarantee that each loop iteration’s prefetch coincides the copy.
Prefetch the next 64-bytes and copy the current 64-bytes.</p>

<hr>

<h2 id="alternate-avenues">Alternate avenues</h2>

<h3 id="unrolling">Unrolling</h3>

<p>In the previous section, most of the changes were in the actual underlying
load, store instructions used. Another avenue of exploration is to unroll the
loop for a certain number of iterations. This reduces the number of branch
statements by the factor of unrolling.</p>

<p>In the <code>glibc</code> implementation the unrolling factor is 4 which is what I’ll use
as well. A very simple way to implement this is to increase the alignment 
required by 4x and treat each loop as 4 instructions that copy 4x data.</p>

<p>A more complicated version would be trying to implement an unrolled loop
without increasing alignment size. We’ll need to copy using a regular fully
rolled loop till we hit a pointer location that is aligned to the size expected
by our unrolled loop.</p>

<p>Unrolling the aligned AVX copy:</p>

<div><pre><code><span>void</span> <span>_avx_cpy_unroll</span><span>(</span><span>void</span> <span>*</span><span>d</span><span>,</span> <span>const</span> <span>void</span> <span>*</span><span>s</span><span>,</span> <span>size_t</span> <span>n</span><span>)</span> <span>{</span>
  <span>// d, s -&gt; 128 byte aligned</span>
  <span>// n -&gt; multiple of 128</span>

  <span>auto</span> <span>*</span><span>dVec</span> <span>=</span> <span>reinterpret_cast</span><span>&lt;</span><span>__m256i</span> <span>*&gt;</span><span>(</span><span>d</span><span>);</span>
  <span>const</span> <span>auto</span> <span>*</span><span>sVec</span> <span>=</span> <span>reinterpret_cast</span><span>&lt;</span><span>const</span> <span>__m256i</span> <span>*&gt;</span><span>(</span><span>s</span><span>);</span>
  <span>size_t</span> <span>nVec</span> <span>=</span> <span>n</span> <span>/</span> <span>sizeof</span><span>(</span><span>__m256i</span><span>);</span>
  <span>for</span> <span>(;</span> <span>nVec</span> <span>&gt;</span> <span>0</span><span>;</span> <span>nVec</span> <span>-=</span> <span>4</span><span>,</span> <span>sVec</span> <span>+=</span> <span>4</span><span>,</span> <span>dVec</span> <span>+=</span> <span>4</span><span>)</span> <span>{</span>
    <span>_mm256_store_si256</span><span>(</span><span>dVec</span><span>,</span> <span>_mm256_load_si256</span><span>(</span><span>sVec</span><span>));</span>
    <span>_mm256_store_si256</span><span>(</span><span>dVec</span> <span>+</span> <span>1</span><span>,</span> <span>_mm256_load_si256</span><span>(</span><span>sVec</span> <span>+</span> <span>1</span><span>));</span>
    <span>_mm256_store_si256</span><span>(</span><span>dVec</span> <span>+</span> <span>2</span><span>,</span> <span>_mm256_load_si256</span><span>(</span><span>sVec</span> <span>+</span> <span>2</span><span>));</span>
    <span>_mm256_store_si256</span><span>(</span><span>dVec</span> <span>+</span> <span>3</span><span>,</span> <span>_mm256_load_si256</span><span>(</span><span>sVec</span> <span>+</span> <span>3</span><span>));</span>
  <span>}</span>
<span>}</span>
</code></pre></div>

<h3 id="multithreading">Multithreading</h3>

<p>The operation of copying data is super easy to parallelize across multiple
threads. The total data to be transferred can be segmented into (almost)
equal chunks, and then copied over using one of the above methods. This will
make the copy super-fast especially if the CPU has a large core count.</p>

<hr>

<h2 id="shadesmar-api">Shadesmar API</h2>

<p>To make it easy to integrate custom memory copying logic into the library,
I introduced the concept of <code>Copier</code> in <a href="https://github.com/Squadrick/shadesmar/commit/22dc762ca658d1396f3c00366e80e4f695189df9">this commit</a>.
For a new copying algorithm, an abstract class <code>Copier</code> must be implemented.</p>

<p>Here’s the definition of <code>Copier</code>:</p>

<div><pre><code><span>class</span> <span>Copier</span> <span>{</span>
 <span>public:</span>
  <span>virtual</span> <span>void</span> <span>*</span><span>alloc</span><span>(</span><span>size_t</span><span>)</span> <span>=</span> <span>0</span><span>;</span>
  <span>virtual</span> <span>void</span> <span>dealloc</span><span>(</span><span>void</span> <span>*</span><span>)</span> <span>=</span> <span>0</span><span>;</span>
  <span>virtual</span> <span>void</span> <span>shm_to_user</span><span>(</span><span>void</span> <span>*</span><span>,</span> <span>void</span> <span>*</span><span>,</span> <span>size_t</span><span>)</span> <span>=</span> <span>0</span><span>;</span>
  <span>virtual</span> <span>void</span> <span>user_to_shm</span><span>(</span><span>void</span> <span>*</span><span>,</span> <span>void</span> <span>*</span><span>,</span> <span>size_t</span><span>)</span> <span>=</span> <span>0</span><span>;</span>
<span>};</span>
</code></pre></div>

<p>The original reason for introducing this construct was to allow cross-device
usage, where a custom copier would be implemented to tranfer between CPU and
GPU. E.g.: using <code>cudaMemcpy</code> for Nvidia GPUs.</p>

<p>For a single device use case the implementation of <code>shm_to_user</code> and 
<code>user_to_shm</code> are identical. The implementation of a copier that uses
<code>std::memcpy</code>:</p>

<div><pre><code><span>class</span> <span>DefaultCopier</span> <span>:</span> <span>public</span> <span>Copier</span> <span>{</span>
 <span>public:</span>
  <span>void</span> <span>*</span><span>alloc</span><span>(</span><span>size_t</span> <span>size</span><span>)</span> <span>override</span> <span>{</span> <span>return</span> <span>malloc</span><span>(</span><span>size</span><span>);</span> <span>}</span>

  <span>void</span> <span>dealloc</span><span>(</span><span>void</span> <span>*</span><span>ptr</span><span>)</span> <span>override</span> <span>{</span> <span>free</span><span>(</span><span>ptr</span><span>);</span> <span>}</span>

  <span>void</span> <span>shm_to_user</span><span>(</span><span>void</span> <span>*</span><span>dst</span><span>,</span> <span>void</span> <span>*</span><span>src</span><span>,</span> <span>size_t</span> <span>size</span><span>)</span> <span>override</span> <span>{</span>
    <span>std</span><span>::</span><span>memcpy</span><span>(</span><span>dst</span><span>,</span> <span>src</span><span>,</span> <span>size</span><span>);</span>
  <span>}</span>

  <span>void</span> <span>user_to_shm</span><span>(</span><span>void</span> <span>*</span><span>dst</span><span>,</span> <span>void</span> <span>*</span><span>src</span><span>,</span> <span>size_t</span> <span>size</span><span>)</span> <span>override</span> <span>{</span>
    <span>std</span><span>::</span><span>memcpy</span><span>(</span><span>dst</span><span>,</span> <span>src</span><span>,</span> <span>size</span><span>);</span>
  <span>}</span>
<span>};</span>
</code></pre></div>

<p>I also created an adapter <code>MTCopier</code> that adds multithreading support to other
copiers:</p>

<div><pre><code><span>template</span> <span>&lt;</span><span>class</span> <span>BaseCopierT</span><span>&gt;</span> 
<span>class</span> <span>MTCopier</span> <span>:</span> <span>public</span> <span>Copier</span> <span>{</span>
<span>public:</span>
  <span>explicit</span> <span>MTCopier</span><span>(</span><span>uint32_t</span> <span>threads</span> <span>=</span> <span>std</span><span>::</span><span>thread</span><span>::</span><span>hardware_concurrency</span><span>())</span>
      <span>:</span> <span>base_copier</span><span>(</span><span>base_copier</span><span>),</span> <span>nthreads</span><span>(</span><span>threads</span><span>)</span> <span>{}</span>

  <span>void</span> <span>*</span><span>alloc</span><span>(</span><span>size_t</span> <span>size</span><span>)</span> <span>override</span> <span>{</span> <span>return</span> <span>base_copier</span><span>.</span><span>alloc</span><span>(</span><span>size</span><span>);</span> <span>}</span>

  <span>void</span> <span>dealloc</span><span>(</span><span>void</span> <span>*</span><span>ptr</span><span>)</span> <span>override</span> <span>{</span> <span>base_copier</span><span>.</span><span>dealloc</span><span>(</span><span>ptr</span><span>);</span> <span>}</span>

  <span>void</span> <span>_copy</span><span>(</span><span>void</span> <span>*</span><span>d</span><span>,</span> <span>void</span> <span>*</span><span>s</span><span>,</span> <span>size_t</span> <span>n</span><span>,</span> <span>bool</span> <span>shm_to_user</span><span>)</span> <span>{</span>
    <span>std</span><span>::</span><span>vector</span><span>&lt;</span><span>std</span><span>::</span><span>thread</span><span>&gt;</span> <span>threads</span><span>;</span>
    <span>threads</span><span>.</span><span>reserve</span><span>(</span><span>nthreads</span><span>);</span>

    <span>ldiv_t</span> <span>per_worker</span> <span>=</span> <span>div</span><span>((</span><span>int64_t</span><span>)</span><span>n</span><span>,</span> <span>nthreads</span><span>);</span>

    <span>size_t</span> <span>next_start</span> <span>=</span> <span>0</span><span>;</span>
    <span>for</span> <span>(</span><span>uint32_t</span> <span>thread_idx</span> <span>=</span> <span>0</span><span>;</span> <span>thread_idx</span> <span>&lt;</span> <span>nthreads</span><span>;</span> <span>++</span><span>thread_idx</span><span>)</span> <span>{</span>
      <span>const</span> <span>size_t</span> <span>curr_start</span> <span>=</span> <span>next_start</span><span>;</span>
      <span>next_start</span> <span>+=</span> <span>per_worker</span><span>.</span><span>quot</span><span>;</span>
      <span>if</span> <span>(</span><span>thread_idx</span> <span>&lt;</span> <span>per_worker</span><span>.</span><span>rem</span><span>)</span> <span>{</span>
        <span>++</span><span>next_start</span><span>;</span>
      <span>}</span>
      <span>uint8_t</span> <span>*</span><span>d_thread</span> <span>=</span> <span>reinterpret_cast</span><span>&lt;</span><span>uint8_t</span> <span>*&gt;</span><span>(</span><span>d</span><span>)</span> <span>+</span> <span>curr_start</span><span>;</span>
      <span>uint8_t</span> <span>*</span><span>s_thread</span> <span>=</span> <span>reinterpret_cast</span><span>&lt;</span><span>uint8_t</span> <span>*&gt;</span><span>(</span><span>s</span><span>)</span> <span>+</span> <span>curr_start</span><span>;</span>

      <span>if</span> <span>(</span><span>shm_to_user</span><span>)</span> <span>{</span>
        <span>threads</span><span>.</span><span>emplace_back</span><span>(</span><span>&amp;</span><span>Copier</span><span>::</span><span>shm_to_user</span><span>,</span> <span>&amp;</span><span>base_copier</span><span>,</span> <span>d_thread</span><span>,</span>
                             <span>s_thread</span><span>,</span> <span>next_start</span> <span>-</span> <span>curr_start</span><span>);</span>
      <span>}</span> <span>else</span> <span>{</span>
        <span>threads</span><span>.</span><span>emplace_back</span><span>(</span><span>&amp;</span><span>Copier</span><span>::</span><span>user_to_shm</span><span>,</span> <span>&amp;</span><span>base_copier</span><span>,</span> <span>d_thread</span><span>,</span>
                             <span>s_thread</span><span>,</span> <span>next_start</span> <span>-</span> <span>curr_start</span><span>);</span>
      <span>}</span>
    <span>}</span>
    <span>for</span> <span>(</span><span>auto</span> <span>&amp;</span><span>thread</span> <span>:</span> <span>threads</span><span>)</span> <span>{</span>
      <span>thread</span><span>.</span><span>join</span><span>();</span>
    <span>}</span>
    <span>threads</span><span>.</span><span>clear</span><span>();</span>
  <span>}</span>

  <span>void</span> <span>shm_to_user</span><span>(</span><span>void</span> <span>*</span><span>dst</span><span>,</span> <span>void</span> <span>*</span><span>src</span><span>,</span> <span>size_t</span> <span>size</span><span>)</span> <span>override</span> <span>{</span>
    <span>_copy</span><span>(</span><span>dst</span><span>,</span> <span>src</span><span>,</span> <span>size</span><span>,</span> <span>true</span><span>);</span>
  <span>}</span>

  <span>void</span> <span>user_to_shm</span><span>(</span><span>void</span> <span>*</span><span>dst</span><span>,</span> <span>void</span> <span>*</span><span>src</span><span>,</span> <span>size_t</span> <span>size</span><span>)</span> <span>override</span> <span>{</span>
    <span>_copy</span><span>(</span><span>dst</span><span>,</span> <span>src</span><span>,</span> <span>size</span><span>,</span> <span>false</span><span>);</span>
  <span>}</span>

<span>private:</span>
  <span>BaseCopierT</span> <span>base_copier</span><span>;</span>
  <span>uint32_t</span> <span>nthreads</span><span>;</span>
<span>};</span>
</code></pre></div>

<p>Currently this only works for <code>memcpy</code> and <code>_rep_movsb</code> since the
implementation expects the memory copy to work for unaligned memory.</p>

<hr>

<h2 id="benchmark">Benchmark</h2>

<p>I used Google’s <a href="https://github.com/google/benchmark">Benchmark</a> for timing
the performance of copying data ranging from size of 32kB to 64MB. All the
benchmarks were run on my PC with the following specifications:</p>
<ol>
  <li>AMD Ryzen 7 3700X</li>
  <li>2x8GB DDR4 RAM @ 3600Mhz</li>
</ol>










<h3 id="conclusion">Conclusion</h3>

<p>Stick to <code>std::memcpy</code>. It delivers great performance while also adapting to
the hardware architecture, and makes no assumptions about the memory alignment.</p>

<p>If performance truly matters, then you might want to consider using a more
specific non-genetic implementation with alignment requirements. The streaming
prefetching copy works the best for larger copies (&gt;1MB), but the performance
for small sizes is abyssal, but <code>memcpy</code> matches its performance. For small to
medium sizes Unrolled AVX absolutely dominates, but as for larger messages, it
is slower than the streaming alternatives. The regular <code>RepMovsb</code> is by far the
worst overall performer as excepted.</p>

<p>Unrolling definitely improves performance in most cases by about 5-10%. The
only case where the unrolled version is slower than rolled version is for
<code>AvxCopier</code> with data size of 32B, which the unrolled version is 25% slower.
The rolled version will do a single AVX-256 load/store and a conditional check.
The unrolled version will do 4 AVX-256 load/stores and a conditional check.</p>

<h3 id="code">Code</h3>

<p>Code for all the methods is included in the library conforming to the above
mentioned API. To actively warn about the danger of using these custom copiers
I have named this file <a href="https://github.com/Squadrick/shadesmar/blob/master/include/shadesmar/memory/dragons.h"><code>dragons.h</code></a>,
with an apt message: <em>Here be dragons</em>.</p>


<p><span>
  Written on
  
  May
  24th,
  2020
  by
  
    Dheeraj R Reddy
  
</span>

    </p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Vanishing from Hyundai’s data network (401 pts)]]></title>
            <link>http://techno-fandom.org/~hobbit/cars/ev/offnet.html</link>
            <guid>44860139</guid>
            <pubDate>Mon, 11 Aug 2025 01:55:26 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="http://techno-fandom.org/~hobbit/cars/ev/offnet.html">http://techno-fandom.org/~hobbit/cars/ev/offnet.html</a>, See on <a href="https://news.ycombinator.com/item?id=44860139">Hacker News</a></p>
<div id="readability-page-1" class="page"><div width="85%">
<tbody><tr><td>&nbsp;</td><td>
The Yuppie Button page talks about making lots of light.&nbsp;
Now I needed to do the opposite, by "going dark" -- to vanish completely
from Hyundai's data network, and avoid having the car being tracked
or actively interfered with outside of my control.&nbsp;
See, this is one of the showstopping problems I have with Tesla -- they
*insist* that you have your car online all the time, talking to Tesla's
cloud and sending telematic data.&nbsp;
Thank you, NO.&nbsp;
The <a href="https://www.hyundaiusa.com/bluelink/index.aspx">
range of things</a>
that Hyundai's BlueLink setup is able to
do remotely to someone's car given only a VIN is totally scary.&nbsp;
Not only did I want no parts of that, we all have every right to not
participate in that nonsense if we so choose.
<p>

As a first step I refused to let the dealer sign me for BlueLink, telling
them that I could handle signup later myself if I wanted to.&nbsp;
But I knew there was more to it, since the car as it came was still able
to make a cellular data connection and send information about itself.&nbsp;
The obvious question was to find and disable the cellular communication
facility, or "telematics unit" as it is implemented in many vehicles.
</p></td></tr></tbody></div></div>]]></description>
        </item>
    </channel>
</rss>