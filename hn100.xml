<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0">
    <channel>
        <title>HN100 - Readable Contents</title>
        <link>https://hn.algolia.com/api/v1/search_by_date?tags=%28story,poll%29&amp;numericFilters=points%3E100</link>
        <description>Uses Readability to add bodies to the RSS feed</description>
        <lastBuildDate>Sun, 25 Feb 2024 01:00:06 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>en</language>
        <item>
            <title><![CDATA[Google helped destroy adoption of RSS feeds (720 pts)]]></title>
            <link>https://openrss.org/blog/how-google-helped-destroy-adoption-of-rss-feeds</link>
            <guid>39493770</guid>
            <pubDate>Sat, 24 Feb 2024 18:16:19 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://openrss.org/blog/how-google-helped-destroy-adoption-of-rss-feeds">https://openrss.org/blog/how-google-helped-destroy-adoption-of-rss-feeds</a>, See on <a href="https://news.ycombinator.com/item?id=39493770">Hacker News</a></p>
<div id="readability-page-1" class="page"><rss version="2.0" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>&lt;![CDATA[openrss.org]]&gt;</title><!--[CDATA[https://openrss.org/blog/how-google-helped-destroy-adoption-of-rss-feeds]]--><description><!--[CDATA[ ]]--></description></channel></rss></div>]]></description>
        </item>
        <item>
            <title><![CDATA[U.S. rice exports to Haiti have unhealthy levels of arsenic, study finds (114 pts)]]></title>
            <link>https://www.reuters.com/world/americas/us-rice-exports-haiti-have-unhealthy-levels-arsenic-study-finds-2024-02-24/</link>
            <guid>39493713</guid>
            <pubDate>Sat, 24 Feb 2024 18:09:22 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.reuters.com/world/americas/us-rice-exports-haiti-have-unhealthy-levels-arsenic-study-finds-2024-02-24/">https://www.reuters.com/world/americas/us-rice-exports-haiti-have-unhealthy-levels-arsenic-study-finds-2024-02-24/</a>, See on <a href="https://news.ycombinator.com/item?id=39493713">Hacker News</a></p>
Couldn't get https://www.reuters.com/world/americas/us-rice-exports-haiti-have-unhealthy-levels-arsenic-study-finds-2024-02-24/: Error: Request failed with status code 401]]></description>
        </item>
        <item>
            <title><![CDATA[Some OpenBSD features that aren't widely known (151 pts)]]></title>
            <link>https://dataswamp.org/~solene/2024-02-20-rarely-known-openbsd-features.html</link>
            <guid>39493046</guid>
            <pubDate>Sat, 24 Feb 2024 16:57:53 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://dataswamp.org/~solene/2024-02-20-rarely-known-openbsd-features.html">https://dataswamp.org/~solene/2024-02-20-rarely-known-openbsd-features.html</a>, See on <a href="https://news.ycombinator.com/item?id=39493046">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
<article id="20240220">
  <header>
  
    
    <p>Written by <em>Solène</em>, on 20 February 2024.<br>Tags: 
<span><a href="https://dataswamp.org/~solene/tag-openbsd.html">#openbsd</a></span>


<span><a href="https://dataswamp.org/~solene/tag-unix.html">#unix</a></span>

</p>
    
    
  </header>
  <h2 id="_Introduction">1. Introduction <a href="#_Introduction">§</a></h2>
<p>In this blog post, you will learn about some OpenBSD features that can be useful, but not widespread.
</p>
<p>They often have a niche usage, but it's important to know they exist to prevent you from reinventing the wheel :)
</p>
<p><a href="https://www.openbsd.org/">OpenBSD official project website</a></p>
<h2 id="_Features">2. Features <a href="#_Features">§</a></h2>
<p>The following list of features are not all OpenBSD specific as some can be found on other BSD systems.  Most of the knowledge will not be useful to Linux users.
</p>
<h2 id="_Secure_level">2.1. Secure level <a href="#_Secure_level">§</a></h2>
<p>The secure level is a sysctl named <code>kern.securelevel</code>, it has 4 different values from level -1 to level 2, and it's only possible to increase the level.  By default, the system enters the secure level 1 when in multi-user (the default when booting a regular installation).
</p>
<p>It's then possible to escalate to the last secure level (2), which will enable the following extra security:
</p>
<ul>

  <li>all raw disks are read-only, so it's not possible to try to make a change to the storage devices</li>
  <li>the time is almost lock, it's only possible to modify the clock slowly by small steps (maybe 1 second max every so often)</li>
  <li>the PF firewall rules can't be modified, flushed or altered</li>
</ul>

<p>This feature is mostly useful for dedicated firewall with rules that rarely change.  Preventing the time to change is really useful for remote logging as it allows being sure of "when" things happened, and you can be assured the past logs weren't modified.
</p>
<p>The default security level 1 already enable some extra security like "immutable" and "append-only" file flags can't be removed, these overlooked flags (that can be applied with chflags) can lock down files to prevent anyone from modifying them.  The append-only flag is really useful for logs because you can't modify the content, but this doesn't prevent adding new content, history can't be modified this way.
</p>
<p><a href="https://man.openbsd.org/securelevel">OpenBSD manual pages: securelevel</a></p>
<p><a href="https://man.openbsd.org/chflags">OpenBSD manual pages: chflags</a></p>
<p>This feature exists in other BSD systems.
</p>

<p>OpenBSD's memory allocator can be tweaked, system-wide or per command, to add extra checks.  This could be either used for security reasons or to look for memory allocation related bugs in a program (this is VERY common...).
</p>
<p>There are two methods to apply the changes:
</p>
<ul>

  <li>system-wide by using the sysctl <code>vm.malloc_conf</code>, either immediately with the sysctl command, or at boot in <code>/etc/sysctl.conf</code> (make sure you quote its value there, some characters such as <code>&gt;</code> will create troubles otherwise, been there...)</li>
  <li>on the command line by prepending <code>env MALLOC_OPTIONS="flags" program_to_run</code></li>
</ul>

<p>The man page gives a list of flags to use as option, the easiest to use is <code>S</code> (for security checks).  It is stated in the man page that a program misbehaving with any flag other than X is buggy, so it's not YOUR fault if you use malloc options and the program is crashing.
</p>
<p><a href="https://man.openbsd.org/malloc">OpenBSD manual pages: malloc (search for MALLOC OPTIONS)</a></p>
<h2 id="_File_flags">2.3. File flags <a href="#_File_flags">§</a></h2>
<p>You are certainly used to files attributes like permissions or ownership, but on many file systems (including OpenBSD ffs), there are flags as well!
</p>
<p>The file flags can be altered with the command <code>chflags</code>, there are a couple of flags available:
</p>
<ul>

  <li>nodump: prevent the files from being saved by the command <code>dump</code> (except if you use a flag in dump to bypass this)</li>
  <li>sappnd: the file can only be used in writing append mode, only root can set / remove this flag</li>
  <li>schg: the file can not be change, it becomes immutable, only root can alter this flag</li>
  <li>uappnd: same as sappnd mode but the user can alter the flag</li>
  <li>uchg: same as schg mode but the user can alter the flag</li>
</ul>

<p>As explained in the secure level section above, in the secure level 1 (default !), the flags sappnd and schg can't be removed, you would need to boot in single user mode to remove these flags.
</p>
<p>Tip: remove the flags on a file with <code>chflags 0 file [...]</code>
</p>
<p>You can check the flags on files using <code>ls -ol</code>, this would look like this:
</p>
<pre><code>terra$ chflags uchg get_extra_users.sh
terra$ ls -lo get_extra_users.sh        
-rwxr-xr-x  1 solene  solene  uchg 749 Apr  3  2023 get_extra_users.sh

terra$ chflags 0 get_extra_users.sh     
terra$ ls -lo get_extra_users.sh     
-rwxr-xr-x  1 solene  solene  - 749 Apr  3  2023 get_extra_users.sh
</code></pre>
<p><a href="https://man.openbsd.org/chflags">OpenBSD manual pages: chflags</a></p>

<p>OpenBSD crontab format received a few neat additions over the last years.
</p>
<ul>

  <li>random number for time field: you can use <code>~</code> in a field instead of a number or <code>*</code> to generate a random value that will remain stable until the crontab is reloaded.  Things like <code>~/5</code> work.  You can force the random value within a range with <code>20~40</code> to get values between 20 and 40.</li>
  <li>only send an email if the return code isn't 0 for the cron job: add <code>-n</code> between the time and the command, like in <code>0 * * * * -n /bin/something</code>.</li>
  <li>only run one instance of a job at a time: add <code>-s</code> between the time and the command, like in <code>* * * * * -s /bin/something</code>.  This is incredibly useful for cron job that shouldn't be running twice in parallel, if the job duration is longer than usual, you are ensured it will never start a new instance until the previous one is done.</li>
  <li>no logging: add <code>-q</code> between the time and the command, like in <code>* * * * -q /bin/something</code>, the effect will be that this cron job will not be logged in <code>/var/cron/log</code>.</li>
</ul>

<p>It's possible to use a combination of flags like <code>-ns</code>.  The random time is useful when you have multiple systems, and you don't want them to all run a command at the same time, like in a case they would trigger a huge I/O on a remote server.  This was created to prevent the usual <code>0 * * * * sleep $(( $RANDOM % 3600 )) &amp;&amp; something</code> that would run a sleep command for a random time up to an hour before running a command.
</p>
<p><a href="https://man.openbsd.org/crontab.5">OpenBSD manual pages: crontab</a></p>

<p>One cool feature on OpenBSD is the ability to easily create an installation media with pre-configured answers.  This is done by injecting a specific file in the <code>bsd.rd</code> install kernel.
</p>
<p>There is a simple tool named upobsd that was created by semarie@ to easily modify such bsd.rd file to include the autoinstall file, I forked the project to continue its maintenance.
</p>
<p>In addition to automatically installing OpenBSD with users, ssh configuration, sets to install etc...  it's also possible to add a site.tgz archive along with the usual sets archives that includes files you want to add to the system, this can include a script to run at first boot to trigger some automation!
</p>
<p>These features are a must-have if you run OpenBSD in production, and you have many of them to manage, enrolling a new device to the fleet should be automated as possible.
</p>
<p><a href="https://github.com/rapenne-s/upobsd">GitHub project page: upobsd</a></p>
<p><a href="https://man.openbsd.org/autoinstall">OpenBSD manual pages: autoinstall</a></p>
<h2 id="_apmd_daemon_hooks">2.6. apmd daemon hooks <a href="#_apmd_daemon_hooks">§</a></h2>
<p>Apmd is certainly running on most OpenBSD laptop and desktop around, but it has features that aren't related to its command line flags, so you may have missed them.
</p>
<p>There are different file names that can contain a script to be run upon some event such as suspend, resume, hibernate etc...
</p>
<p>A classic usage is to run <code>xlock</code> in one's X session on suspend, so the system will require a password on resume.
</p>
<p><a href="https://dataswamp.org/~solene/2021-07-30-openbsd-xidle-xlock.html#_Resume_/_Suspend_case">Older blog post: xlock from apmd suspend script</a></p>
<p>The man page explains all, but basically this works like this for running a backup program when you connect your laptop to the power plug:
</p>
<pre><code># mkdir -p /etc/apm
# vi /etc/apm/powerup
</code></pre>
<p>You need to write a regular script:
</p>
<pre><code>#!/bin/sh

/usr/local/bin/my_backup_script
</code></pre>
<p>Then, make it executable
</p>
<pre><code># chmod +x /etc/apm/powerup
</code></pre>
<p>The daemon apmd will automatically run this script when you connect a system back to AC power.
</p>
<p>The method is the same for:
</p>
<ul>

  <li>hibernate</li>
  <li>resume</li>
  <li>suspend</li>
  <li>standby</li>
  <li>hibernate</li>
  <li>powerup</li>
  <li>powerdown</li>
</ul>

<p>This makes it very easy to schedule tasks on such events.
</p>
<p><a href="https://man.openbsd.org/apmd#FILES">OpenBSD manual page: apmd (section FILES)</a></p>
<h2 id="_Using_hotplugd_for_hooks_on_devices_events">2.7. Using hotplugd for hooks on devices events <a href="#_Using_hotplugd_for_hooks_on_devices_events">§</a></h2>
<p>A bit similar to what apmd by running a script upon events, hotplugd is a service that allow running a script when a device is added / removed.
</p>
<p>A typical use is to automatically mount an USB memory stick when plugged in the system, or start cups daemon when powering on your USB printer.
</p>
<p>The script receives two parameters that represents the device class and device name, so you can use them in your script to know what was connected.  The example provided in the man page is a good starting point.
</p>
<p>The scripts aren't really straightforward to write, you need to make a precise list of hardware you expect and what to run for each, and don't forget to skip unknown hardware.  Don't forget to make the scripts executable, otherwise it won't work.
</p>
<p><a href="https://man.openbsd.org/hotplugd">OpenBSD manual page: hotplugd</a></p>
<h2 id="_Altroot">2.8. Altroot <a href="#_Altroot">§</a></h2>
<p>Finally, there is a feature that looks pretty cool. In the daily script, if an OpenBSD partition <code>/altroot/</code> exists in <code>/etc/fstab</code> and the daily script environment has a variable <code>ROOTBACKUP=1</code>, the root partition will be duplicated to it.  This permit keeping an extra root partition in sync with the main root partition.  Obviously, it's more useful if the altroot partition is on another drive.  The duplication is done with <code>dd</code>.  You can look at the exact code by checking the script <code>/etc/daily</code>.
</p>
<p>However, it's not clear how to boot from this partition if you didn't install a bootloader or created an EFI partition on the disk...
</p>
<p><a href="https://man.openbsd.org/hier">OpenBSD manual pages: hier (hier stands for file system hierarchy)</a></p>
<p><a href="https://man.openbsd.org/daily">OpenBSD manual pages: daily</a></p>
<p><a href="https://www.openbsd.org/faq/faq14.html#altroot">OpenBSD FAQ: Root partition backup</a></p>
<h2 id="_talk:_local_chat_in_the_terminal">2.9. talk: local chat in the terminal <a href="#_talk:_local_chat_in_the_terminal">§</a></h2>
<p>OpenBSD comes with a program named "talk", this creates a 1 to 1 chat with another user, either on the local system or a remote one (setup is more complicated).  This is not asynchronous, the two users must be logged in the system to use <code>talk</code>.
</p>
<p>This program isn't OpenBSD specific and can be used on Linux as well, but it's so fun, effective and easy to setup I wanted to write about it.
</p>
<p>The setup is easy:
</p>
<pre><code># echo "ntalk		dgram	udp	wait	root	/usr/libexec/ntalkd	ntalkd" &gt;&gt; /etc/inetd.conf
# rcctl enable inetd
# rcctl start inetd
</code></pre>
<p>The communication happens on localhost on UDP ports 517 and 518, don't open them to the Internet!  If you want to allow a remote system, use a VPN to encrypt the traffic and allow ports 517/518 only for the VPN.
</p>
<p>The usage is simple, if you want alice and bob to talk to each other:
</p>
<ul>

  <li>alice type <code>talk bob</code>, and bob must be logged in as well</li>
  <li>bob receives a message in their terminal that alice wants to talk</li>
  <li>bob type <code>talk alice</code></li>
  <li>a terminal UI appears for both users, what they write will appear on the top half of the UI, and the messages from recipient will appear on the half bottom</li>
</ul>

<p>This is a bit archaic, but it works fine and comes with the base system.  It does the job when you just want to speak to someone.
</p>
<h2 id="_Conclusion">3. Conclusion <a href="#_Conclusion">§</a></h2>
<p>There are interesting features on OpenBSD that I wanted to highlight a bit, maybe you will find them useful.  If you know cool features that could be added to this list, please reach me!
</p>

</article>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Ledger (112 pts)]]></title>
            <link>https://lock.cmpxchg8b.com/ledger.html</link>
            <guid>39492924</guid>
            <pubDate>Sat, 24 Feb 2024 16:46:35 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://lock.cmpxchg8b.com/ledger.html">https://lock.cmpxchg8b.com/ledger.html</a>, See on <a href="https://news.ycombinator.com/item?id=39492924">Hacker News</a></p>
<div id="readability-page-1" class="page">

<nav id="TOC" role="doc-toc">
<ul>
<li><a href="#intro">Intro</a></li>
<li><a href="#getting-started">Getting Started</a></li>
<li><a href="#examples">Examples</a></li>
<li><a href="#advice">Advice</a></li>
</ul>
</nav>
<section id="intro">
<h2>Intro</h2>
<p>One of my new years resolutions was to commit to using <a href="https://ledger-cli.org/">ledger</a> – a commandline accounting tool. Ledger is a bit like Quicken, GnuCash or Mint, but for UNIX nerds.</p>
<blockquote>
<p>Note: No relation to the cr*pto product with the same name.</p>
</blockquote>
<p>It’s coming up to the end of the year, and this was one of the few resolutions I actually managed to keep!</p>
<p>I think you can <em>probably</em> accomplish everything ledger can do with other personal finance software. The reason you would choose ledger is that you’re sold on the efficiency and scriptability of the commandline, along with the grep, editor and revision control friendly file format.</p>
<p>If that sounds interesting, go take a look at the <a href="https://ledger-cli.org/doc/ledger3.html#Introduction-to-Ledger">docs</a> to learn more!</p>
<p>The TL;DR is that I’m hooked, and will keep using it.</p>
</section>
<section id="getting-started">
<h2>Getting Started</h2>
<p>It’s not easy to get started with ledger, and you’re probably going to have some uncommon financial arrangements that the documentation didn’t cover exactly. It could be anything, maybe something mundane like you split the utilities with a partner, but you’re the one who pays the bill?<a href="#fn1" id="fnref1" role="doc-noteref"><sup>1</sup></a></p>
<p>These things are easy to handle once you’re familiar with ledger concepts, but as nobody has the exact same arrangements as you, it’s not always a copy-and-paste situation.</p>
<p>This is where ledger’s flexibility can be a curse – the answer to questions like “How do I handle …?” tends to be “However you like!”. Cool, but you’re not going to have any opinions formed yet, so how do you know what you like?</p>
<p>I don’t really have a good solution – I think you just have to wing it until you’re proficient, then go back and fix any naive mistakes you made 🙈</p>
<p>Another problem is that you need a few months worth of data before you can fairly evaluate if you find it useful or not. That’s a big commitment to something that you’re not sure you’re actually going to enjoy!</p>
<p>After you’ve finally built up some data… your reward is having to learn the query syntax so you can actually do something useful with it… and that’s when you’ll realize you didn’t organize things optimally, and have to go back and edit the last few months of data! 😂</p>
<p>Now that I’ve scared everyone off, I can tell you I don’t regret that effort – ledger is great! You know that feeling when you find a solution that just feels “correct”? Well, ledger gives me that feeling.</p>
</section>
<section id="examples">
<h2>Examples</h2>
<p>So, what exactly does ledger look like? Well, there is no user interface, it’s a commandline tool to query your finances.</p>
<blockquote>
<p>Note: If you want graphs, you pipe the output into <code>gnuplot</code>, <a href="https://www.sundialdreams.com/report-scripts-for-ledger-cli-with-gnuplot/">like this</a>. If you’re a stats nerd, you might prefer <a href="https://github.com/esovetkin/ledger-plots">ledger-plots</a>.</p>
</blockquote>
<p>You can automatically import transactions from whatever format your bank provides, but I prefer to enter some transactions manually. I use vim’s <code>colorcolumn</code> feature to make margins and keep everything neatly aligned.</p>
<p>Once you’ve got importing and editing working smoothly, you can start answering questions about your finances.</p>
<ul>
<li>How much have I spent this month?</li>
</ul>
<pre><code>$ ledger bal --period "this month" ^Expenses:</code></pre>
<ul>
<li>How much did I spend on gas each month this year?</li>
</ul>
<pre><code>$ ledger reg --period "this year" --monthly ^Expenses:Auto:Gas</code></pre>
<ul>
<li>What is my current networth?</li>
</ul>
<pre><code>$ ledger bal --depth 1 --market ^Assets: ^Liabilities:</code></pre>
<ul>
<li>How much have my investments grown?</li>
</ul>
<pre><code>$ ledger bal --gain ^Assets:Brokerage</code></pre>
<ul>
<li>Okay, but what about my investment in stock FOOBAR specifically?</li>
</ul>
<pre><code>$ ledger bal --gain --limit 'commodity == "FOOBAR"' ^Assets:Brokerage</code></pre>
<ul>
<li>Okay, but how much of that is long term capital gains?</li>
</ul>
<pre><code>$ ledger bal --gain --limit 'commodity == "FOOBAR"' --limit 'lot_date(amount) &lt; [365 days ago]' ^Assets:Brokerage</code></pre>
<ul>
<li>How much did I spend at McDonalds in June?</li>
</ul>
<pre><code>$ ledger bal --period "this june" ^Expenses: and @McDonalds
</code></pre>
<ul>
<li>How much do I spend each month on average in total?</li>
</ul>
<pre><code>$ ledger reg --period "this year" --average --monthly --depth 1 ^Expenses:</code></pre>
<ul>
<li>Where is that money going, but only show me the categories I spend more than $20 on?</li>
</ul>
<pre><code>$ ledger bal --period "last 12 months" --amount "amount / 12" --display 'top_amount(amount) &gt; 20' ^Expenses:</code></pre>
<ul>
<li>Did I earn enough credit card rewards to justify the fees, or should I cancel them?</li>
</ul>
<pre><code>$ ledger reg --period "this year" --subtotal --related ^Income:Reward</code></pre>
<ul>
<li>Show me all the donations I made that I can deduct on my taxes?</li>
</ul>
<pre><code>$ ledger reg --period "this year" %deductible</code></pre>
<p>These are just some random examples, there are also budgeting and forecasting tools. It’s flexible enough that people use it for tracking billable hours, inventory and so on.</p>
<p>Naturally, it handles multiple currencies, arbitrary commodities, and you can track them as precisely (or coarsely) as you like.</p>
</section>
<section id="advice">
<h2>Advice</h2>
<p>This post is really just encouraging anyone considering getting started to take the leap if you were thinking about getting started in the new year!</p>
</section>
<section role="doc-endnotes">
<hr>
<ol>
<li id="fn1" role="doc-endnote"><p>fyi, you could probably record that like this:</p>
<pre><code>; Pay the power bill, John owes me half
2023/10/1 * Power Company
   Expenses:Utilities:Power    $50
   Assets:Receivables:John     $50
   Assets:Checking            -$100
; John reimbursed me.
2023/10/2 * John Doe  ; Flatmate
   Assets:Checking             $50
   Assets:Receivables:John    -$50</code></pre>
<p>…but maybe you want to record it like this:</p>
<pre><code>; Pay the power bill, John owes me half
2023/10/1 * Power Company
   Expenses:Utilities:Power    $100
   Assets:Checking             $100
; John reimbursed me his half.
2023/10/2 * John Doe  ; Flatmate
   Assets:Checking             $50
   Expenses:Utilities:Power   -$50 ; [=2023/10/1]</code></pre>
<p>There is not always one right answer!<a href="#fnref1" role="doc-backlink">↩︎</a></p></li>
</ol>
</section>



</div>]]></description>
        </item>
        <item>
            <title><![CDATA[V Language Review (2023) (152 pts)]]></title>
            <link>https://n-skvortsov-1997.github.io/reviews/</link>
            <guid>39492680</guid>
            <pubDate>Sat, 24 Feb 2024 16:18:20 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://n-skvortsov-1997.github.io/reviews/">https://n-skvortsov-1997.github.io/reviews/</a>, See on <a href="https://news.ycombinator.com/item?id=39492680">Hacker News</a></p>
<div id="readability-page-1" class="page"><div aria-label="Content"><p>So you’ve found a new programming language called V. It looks nice, has a lot of promises on the website, nice syntax, but how does it really work?</p>

<p>Everything described here is correct for the <a href="https://github.com/vlang/v/commit/b66447cf11318d5499bd2d797b97b0b3d98c3063"><code>b66447cf11318d5499bd2d797b97b0b3d98c3063</code></a> commit. This is a summary of my experience with the language over 6 months + information that I found on Discord while I was writing this article.</p>

<p>The article is quite long, because I tried to describe everything in as much detail as possible and anyone could reproduce the same behavior.</p>

<p>Where do you start learning a new programming language? That’s right, from the documentation. V documentation is one huge  <a href="https://github.com/vlang/v/blob/master/doc/docs.md"><code>docs.md</code></a> file.</p>

<p>Not far from the beginning, you can notice the built-in types that V has. The small <code>soon</code> prefix for types <code>i128</code> and <code>u128</code> describes the state of the entire language. This note has been in the documentation for at least 4 years (<a href="https://github.com/vlang/v/commit/65a8db85254b8d8d02098843202142e61aa02570"><code>commit</code></a>), apparently we need to wait a little longer.</p>

<p><img src="https://n-skvortsov-1997.github.io/reviews/img/1-resized.png" alt=""></p>

<p>Next, you may notice that, unlike C and Go, <code>int</code> is always 32 bit. But, in release <a href="https://github.com/vlang/v/releases/tag/0.4.3">0.4.3</a> it is now 64 bit on 64-bit systems and 32 bit on 32-bit systems.</p>

<p>A couple of errors, you say, but no, this is the whole of V documentation. There are so few developers to keep the documentation in the correct state. Documentation often does not describe the most important parts of the language — for example, the <a href="https://github.com/vlang/v/blob/master/doc/docs.md#generics">section</a> about generics consists of a couple of code examples without proper description.</p>

<p>Promises as the following are also common in documentation:</p>

<blockquote>
  <p>Currently, generic function definitions must declare their type parameters, but in the future V will infer generic type parameters from single-letter type names in runtime parameter types.</p>
</blockquote>

<p>And now you scroll down to the most interesting thing, memory management in V. In modern programming languages, this is almost the most important part of the language. What does V offer? First of all, this is “Garbage Collection”, a good option that greatly simplifies life, the second option “arena”, also a great option, “manual memory management” is also available for experienced programmers. The last and most interesting option is “autofree”.</p>

<p>The first two options work relatively well, so let’s look at the last two.</p>

<h3 id="manual-memory-management">Manual memory management</h3>

<p>In this mode, libc’s malloc function is used for all allocations and the developer must clean up the memory himself. But what about the memory allocated inside standard library functions? Let’s take a look at the <code>is_ascii</code> <a href="https://github.com/vlang/v/blob/cc220e60a5a0cc787b68ae357c8ecfd2dc561b6f/vlib/builtin/string.v#L2379C2-L2379C2">method</a>:</p>

<pre><code>@[inline]
pub fn (s string) is_ascii() bool {
    return !s.bytes().any(it &lt; u8(` `) || it &gt; u8(`~`))
}
</code></pre>

<p>It might seem like a small safe function, but if you call it with manual memory management, you will have leaks, since no one clears the memory allocated by the <code>bytes()</code> <a href="https://github.com/vlang/v/blob/cc220e60a5a0cc787b68ae357c8ecfd2dc561b6f/vlib/builtin/string.v#L2040">method</a>. Here are <a href="https://github.com/vlang/v/blob/master/vlib/builtin/string.v#L822">more</a> <a href="https://github.com/vlang/v/blob/cc220e60a5a0cc787b68ae357c8ecfd2dc561b6f/vlib/builtin/string.v#L901">such</a> <a href="https://github.com/vlang/v/blob/master/vlib/builtin/string.v#L338">examples</a>. And this is only in string methods; in the standard library it is everywhere.</p>

<p>Well, you can just not use these functions in your code. Let’s see what if you want a web server on V in “manual” mode. V has a built-in framework called <code>vweb</code>. The official examples include the following code: https://github.com/vlang/v/blob/master/examples/vweb/vweb_example.v.</p>

<p>I’ve simplified it as much as possible:</p>

<pre><code>module main

import vweb

struct App {
    vweb.Context
}

pub fn (mut app App) index() vweb.Result {
    return app.text("Hello World")
}

fn main() {
    vweb.run(&amp;App{}, 8082)
}
</code></pre>

<p><a href="https://github.com/vlang/v/blob/master/vlib/vweb/vweb.v#L571">Next</a> <a href="https://github.com/vlang/v/blob/cc220e60a5a0cc787b68ae357c8ecfd2dc561b6f/vlib/vweb/vweb.v#L542">arrays</a> <a href="https://github.com/vlang/v/blob/master/vlib/vweb/vweb.v#L572">never</a> <a href="https://github.com/vlang/v/blob/cc220e60a5a0cc787b68ae357c8ecfd2dc561b6f/vlib/vweb/vweb.v#L958">deallocated</a> in <a href="https://github.com/vlang/v/blob/cc220e60a5a0cc787b68ae357c8ecfd2dc561b6f/vlib/vweb/vweb.v#L729">vweb</a> code. This means that your application on vweb will have leaks.</p>

<p>Well, not everyone writes web, maybe you need a simple CLI utility? Unfortunately, all string interpolations in the standard library for the CLI allocate memory then not cleaned up.</p>

<p>From all of the above, I can draw the following conclusion: manual memory management in V is a feature that cannot be used in real applications. At most in simple programs where you write everything from scratch or memory leaks are not critical for you.</p>

<h3 id="autofree">autofree</h3>

<p>Now we come to the most interesting thing in this section. Let’s see how this mode is described in the documentation:</p>

<blockquote>
  <p>The second way is autofree, it can be enabled with <code>-autofree</code>. It takes care of most objects (~90-100%): the compiler inserts necessary free calls automatically during compilation. Remaining small percentage of objects is freed via GC. The developer doesn’t need to change anything in their code. “It just works”, like in Python, Go, or Java, except there’s no heavy GC tracing everything or expensive RC for each object.</p>
</blockquote>

<p>Surprisingly, they were able to lie in every sentence. Let’s start from the very beginning, the documentation assures us that 90–100% will be cleaned up automatically using <code>free</code> calls inserted by the compiler. This sounds pretty optimistic, considering that to achieve the same thing in Rust, you need a lot of help to the compiler. The V compiler turns out to be “much smarter” than the Rust compiler.</p>

<p>While I was looking at <a href="https://github.com/vlang/v/discussions?discussions_q=is%3Aopen+autofree&amp;page=1">discussions</a> in the language repository, I came across a interesting comment (<a href="https://github.com/vlang/v/discussions/12343#discussioncomment-5828322">link</a>):</p>

<blockquote>
  <p>In my v program, only 0.1% was autofreed and 99.9% was freed by the garbage collector. It all depends on the program you are making. The GC is still really fast though.</p>
</blockquote>

<p>But, let’s not take this as a fact and try it ourselves. Here’s the simplest code:</p>

<pre><code>module main

struct Data {
    data []bool
}

fn main() {
    p := Data{
        data: [true, false]
    }
    println(p.data)
}
</code></pre>

<p>Compile it using <code>v -autofree main.v</code>. And run <code>valgrind</code>:</p>

<div><pre><code>=653065= Memcheck, a memory error detector
=653065= Copyright (C) 2002-2017, and GNU GPL'd, by Julian Seward et al.
=653065= Using Valgrind-3.18.1 and LibVEX; rerun with -h for copyright info 
=653065= Command: /test
=653065=
[true, false]
=653065=
=653065= HEAP SUMMARY:
=653065=   in use at exit: 2 bytes in 1 blocks
=653065= total heap usage: 6 allocs, 5 frees, 1,085 bytes allocated
=653065=
=653065= LEAK SUMMARY:
=653065=    definitely lost: 2 bytes in 1 blocks
=653065=    indirectly lost: 0 bytes in 0 blocks
=653065=      possibly lost: 0 brtes in 0 blocks
=653065=    still reachable: 0 bytes in 0 blocks
=653065=         suppressed: 0 bytes in 0 blocks
=653065= Rerun with --leak-check=full to see details of leaked memory
=653065=
=653065= ERROR SUMMARY: 0 errors from 0 contexts (suppressed: 0 from 0)
</code></pre></div>

<p>Something went wrong. What’s also interesting is that we have a single array in the program, but valgrind shows that a 1kb of memory was allocated. Keep this point in mind as we move on to the website’s statement that V avoids unnecessary allocations.</p>

<p>Let’s compile a simple example with a vweb server:</p>

<pre><code>module main

import vweb

struct App {
    vweb.Context
}

pub fn (mut app App) index() vweb.Result {
    return app.text("Hello World")
}

fn main() {
    vweb.run(&amp;App{}, 8082)
}
</code></pre>



<p>And let’s run it with valgrind. I didn’t make requests to server, but just waited 10 seconds:</p>

<div><pre><code>=653318= Command: /server
=653318=
[Vweb] Running app on http://localhost:8082/
[Vweb] We have 1 workers
=653318=
=653318= Process terminating with default action of signal 2 (SIGINT)
=653318=   at 0x498882D: select (select.c:69)
=653318=   by 0×58AAF2: net__select (in /home/skvortsov/server)
=653318=   by 0x642D81: net__select_deadline (in /home/skvortsov/server)
=653318=   by 0×58B122: net__wait_for_common (in /home/skvortsov/server)
=653318=   by 0x58B40B: net__wait_for_read (in /home/skvortsov/server)
=653318=   by 0x591D43: net__TepListener_wait_for_accept (in /home/skvortsov/server)
=653318=   by 0x5915F9: net__TepListener_accept_only (in /home/skvortsov/server)
=653318=   by 0x5E57E1: vweb__run_at_T_main_App (in /home/skvortsov/server)
=653318=   by 0x5E4262: vweb__run_T_main__App (in /home/skvortsov/server)
=653318=   by 0x5F0102: main__main (in /home/skvortsov/server)
=653318=   by 0x63F220: main (in /home/skvortsov/server)
=653318=
=653318= HEAP SUMMARY:
=653318=    in use at exit: 122,833 bytes in 2,395 blocks
=653318=   total heap usage: 2,659 allocs, 264 frees, 541,413 bytes allocated
=653318=
=653318= LEAK SUMMARY:
=653318=    definitely lost: 1,004 bytes in 32 blocks
=653318=    indirectly lost: 106 bytes in 15 blocks
=653318=      possibly lost: 272 bytes in 1 blocks
=653318=    still reachable: 121,451 bytes in 2,347 blocks
=653318=         suppressed: 0 bytes in 0 blocks
=653318- Rerun with --leak-check=full to see details of leaked memory
=653318=
=653318= For lists of detected and suppressed errors, rerun with: -s 
=653318= ERROR SUMMARY: 0 errors from 0 contexts (suppressed: 0 from 0)
</code></pre></div>

<p>Without requests, in 10 seconds we definitely lost 1kb of memory.</p>

<p>Let’s return to the description from the documentation:</p>

<blockquote>
  <p>Remaining small percentage of objects is freed via GC.</p>
</blockquote>

<p>And again it’s not true. I found a recent <a href="https://github.com/vlang/v/commit/207203f5998e6b1844a32fe628e0eb64325db64d">commit</a> in which passing the <code>-autofree</code> flag immediately sets <code>gc</code> to <code>none</code>:</p>

<div><pre><code><span>'-autofree'</span> {
  res.autofree = true
<span>+ res.gc_mode = .no_gc
</span>  res.build_options &lt;&lt; arg
<span>}</span>
</code></pre></div>

<p>So, the statement is completely false.</p>

<p>Do you know why this change was made? Let’s look at an example:</p>

<pre><code>fn main() {
    ptr := malloc(1)
    free(ptr)
}
</code></pre>

<p>And let’s look at the <a href="https://github.com/vlang/v/blob/cc220e60a5a0cc787b68ae357c8ecfd2dc561b6f/vlib/builtin/builtin.c.v#L586">definition</a> of the <code>free</code> function:</p>

<pre><code>@[unsafe]
pub fn free(ptr voidptr) {
    $if prealloc {
        return
    } $else $if gcboehm ? {
        // It is generally better to leave it to Boehm's gc to free things.
        // Calling C.GC_FREE(ptr) was tried initially, but does not work
        // well with programs that do manual management themselves.
        //
        // The exception is doing leak detection for manual memory management:
        $if gcboehm_leak ? {
            unsafe { C.GC_FREE(ptr) }
        }
    } $else {
        C.free(ptr)
    }
}
</code></pre>

<p><a href="https://github.com/vlang/v/blob/master/doc/docs.md#if-condition"><code>$if</code></a> specifies a condition evaluated during compilation. Previously, when we passed only <code>-autofree</code> and did not explicitly pass <code>-gc none</code>, then the condition <code>$if gcboehm ?</code> was true and since <code>gcboehm_leak</code> is also not set by default, then <code>free</code> ended up becoming a noop function that did nothing.</p>

<p>Here is the C code that was generated:</p>

<div><pre><code><span>void</span> <span>_v_free</span><span>(</span><span>voidptr</span> <span>ptr</span><span>)</span> <span>{</span>
    <span>#if defined(_VPREALLOC)
</span>    <span>{</span>
    <span>}</span>
    <span>#elif defined(_VGCBOEHM)
</span>    <span>{</span>
    <span>}</span>
    <span>#else
</span>    <span>{</span>
    <span>}</span>
    <span>#endif
</span><span>}</span>
</code></pre></div>

<p>All of this code is C preprocessor, so the compiler sees this code as follows:</p>

<div><pre><code><span>void</span> <span>_v_free</span><span>(</span><span>voidptr</span> <span>ptr</span><span>)</span> <span>{</span>
<span>}</span>
</code></pre></div>

<p>And that doesn’t free anything.</p>

<p>Let’s go back to the documentation:</p>

<blockquote>
  <p>Remaining small percentage of objects is freed via GC.</p>
</blockquote>

<p>And this was not true, even if V inserted <code>free</code> somewhere, they had no effect and everything was cleared by the GC. Even after this fix, this is not true because the GC is disabled when the <code>-autofree</code> flag is passed.</p>

<p>You may have seen this video: https://www.youtube.com/watch?v=gmB8ea8uLsM</p>

<p>In it, the author of the language shows his editor <a href="https://github.com/vlang/ved">Ved</a> and shows how he compiles it using <code>v . -autofree</code> and states that its technology is sufficiently developed that such a complex application as a text editor does not leak.</p>

<p>I tried to build the editor with the latest version of V with the <code>autofree</code> flag and got the following error when I launched the binary:</p>

<div><pre><code>V panic: as cast: cannot cast `map[string]toml.ast.Value` to `[]toml.ast.Value`
v hash: 0966fd3
/tmp/v_1000/ved.5480247081914024169.tmp.c:13797: at _v_panic: Backtrace
/tmp/v_1000/ved.5480247081914024169.tmp.c:14296: by __as_cast
/tmp/v_1000/ved.5480247081914024169.tmp.c:43867: by toml__Doc_value_
/tmp/v_1000/ved.5480247081914024169.tmp.c:43836: by toml__Doc_value
/tmp/v_1000/ved.5480247081914024169.tmp.c:44579: by main__Config_init_colors
/tmp/v_1000/ved.5480247081914024169.tmp.c:44551: by main__Config_reload_config
/tmp/v_1000/ved.5480247081914024169.tmp.c:46595: by main__main
/tmp/v_1000/ved.5480247081914024169.tmp.c:50668: by main
</code></pre></div>

<p>Without <code>autofree</code> everything worked without problems. Well, apparently autofree has only gotten worse in 3 years.</p>

<p>Interestingly enough, the project of the language author does not work with the main feature of his language.</p>

<p>Let’s try to build the compiler itself with <code>autofree</code>:</p>



<p>And let’s try to compile itself again with the resulting binary:</p>



<p>And we get an error at runtime:</p>

<div><pre><code>./v2 self
/tmp/v_1000/v2.10486918756004741764.tmp.c:25123: at string_starts_with: RUNTIME ERROR: invalid memory access
/tmp/v_1000/v2.10486918756004741764.tmp.c:35912: by os__impl_walk_ext
/tmp/v_1000/v2.10486918756004741764.tmp.c:35888: by os__walk_ext
/tmp/v_1000/v2.10486918756004741764.tmp.c:42645: by v__pref__detect_musl
/tmp/v_1000/v2.10486918756004741764.tmp.c:42743: by v__pref__parse_args_and_show_errors
/tmp/v_1000/v2.10486918756004741764.tmp.c:4811: by main__main
/tmp/v_1000/v2.10486918756004741764.tmp.c:5835: by main
</code></pre></div>

<p>Let’s go back to the last part of the documentation:</p>

<blockquote>
  <p>The developer doesn’t need to change anything in their code. “It just works”, like in Python, Go, or Java, except there’s no heavy GC tracing everything or expensive RC for each object.</p>
</blockquote>

<p>What we found out above, before commit <a href="https://github.com/vlang/v/commit/207203f5998e6b1844a32fe628e0eb64325db64d">207203f</a> when passing the <code>-autofree</code> flag we <strong>got</strong> “heavy GC tracing everything”, and after we <strong>get</strong> memory leaks even in the simplest examples.</p>

<p>I would also like to note that the author of the language promised to make this technology “production ready” back to version <code>0.3</code> (<a href="https://github.com/vlang/v/blob/0f9537ece544b7fda31cadf4dc95fd4b552f94be/ROADMAP.md">commit</a>), then to <a href="https://discord.com/channels/592103645835821068/592106336838352923/1136589637658345563">0.5</a>. and maybe in <a href="https://discord.com/channels/273534239310479360/818964227783262209/1146427952083513467">0.6</a>, and in <a href="https://github.com/vlang/v/blob/master/ROADMAP.md">ROADMAP</a> in 1.0.</p>

<blockquote>
  <p>“Fake it <del>till you make it</del>”.</p>
</blockquote>

<p>It’s interesting that the author of the language does not <a href="https://discord.com/channels/592103645835821068/592106336838352923/1126201270902997114">see the point</a> in using <code>autofree</code> with GC, although the documentation says that it is GC that clears the remaining “10%” of objects. Marvelous.</p>

<p><img src="https://n-skvortsov-1997.github.io/reviews/img/2-resized.png" alt=""></p>

<p>Thus, from all of the above, we can conclude that <code>autofree</code> is a very crude technology. The author of the language tried to promote it through that video, and judging by the comments he succeeded, I don’t understand why people believe him, because a simple test shows that even simple programs leak as hell.</p>

<p><strong>After almost 5 years, the most interesting feature of V is still in a very early state, and the author does nothing but promise that everything will happen soon.</strong></p>

<p>It is already clear that the author of the language and his loyal followers will begin to say that <code>autofree</code> is not yet production ready, but the problems that I described above even for the very first alpha version are unacceptable.</p>

<h3 id="gc-default">GC (default)</h3>

<p>In this section, I want to discuss the remaining shortcomings of V in the memory management system.</p>

<p>Let’s go back to the documentation:</p>

<blockquote>
  <p>V avoids doing unnecessary allocations in the first place by using value types, string buffers, promoting a simple abstraction-free code style.</p>
</blockquote>

<p>It is stated that V does not make unnecessary allocations. Let’s check it out. In V, if you convert a structure into an interface, you get memory allocation with no options to avoid it, so you will get a bunch of extra allocations for nothing:</p>

<pre><code>interface IFoo {
    name string
}

struct Foo {
    name string
}

fn get_ifoo() IFoo {
    return Foo{name: 'foo'}
}

fn main() {
    foo := get_ifoo()
    println(foo.name)
}
</code></pre>

<p>C code:</p>

<div><pre><code><span>VV_LOCAL_SYMBOL</span> <span>main__IFoo</span> <span>main__get_ifoo</span><span>(</span><span>void</span><span>)</span> <span>{</span>
    <span>main__IFoo</span> <span>_t1</span> <span>=</span> <span>I_main__Foo_to_Interface_main__IFoo</span><span>(((</span><span>main__Foo</span><span>*</span><span>)</span><span>memdup</span><span>(</span><span>&amp;</span><span>(</span><span>main__Foo</span><span>){.</span><span>name</span> <span>=</span> <span>_SLIT</span><span>(</span><span>"foo"</span><span>),},</span> <span>sizeof</span><span>(</span><span>main__Foo</span><span>))));</span>
    <span>return</span> <span>_t1</span><span>;</span>
<span>}</span>
</code></pre></div>

<p><code>memdup</code> sends memory to heap via <code>_v_malloc</code>. In this small piece of code, you can also notice another feature of V, “readable” generated C code.</p>

<p>There is no escape analysis in V, and any pointers you create in a function make unnecessary allocations to the heap:</p>

<pre><code>fn main() {
    a := 100
    b := &amp;a
    println(b)
}
</code></pre>

<p>In C code:</p>

<div><pre><code><span>void</span> <span>main__main</span><span>(</span><span>void</span><span>)</span> <span>{</span>
    <span>int</span> <span>*</span><span>a</span> <span>=</span> <span>HEAP</span><span>(</span><span>int</span><span>,</span> <span>(</span><span>100</span><span>));</span> <span>// heap allocation</span>
    <span>int</span><span>*</span> <span>b</span> <span>=</span> <span>&amp;</span><span>(</span><span>*</span><span>(</span><span>a</span><span>));</span>
    <span>string</span> <span>_t1</span> <span>=</span> <span>str_intp</span><span>(</span><span>1</span><span>,</span> <span>_MOV</span><span>((</span><span>StrIntpData</span><span>[])}));</span> <span>println</span><span>(</span><span>_t1</span><span>);</span> <span>string_free</span><span>(</span><span>&amp;</span><span>_t1</span><span>);</span>
    <span>;</span>
<span>}</span>
</code></pre></div>

<p>The documentation says:</p>

<blockquote>
  <p>Due to performance considerations V tries to put objects on the stack if possible but allocates them on the heap when obviously necessary.</p>
</blockquote>

<p>V does not allocate to the heap <strong>only those objects whose address is not taken</strong> in the entire function, V doesn’t do escape analysis and considers any taking of an address as a leak (in terms of “Escape Analysis”) from the function. And this doesn’t match the statement “Due to performance considerations V tries to put objects on the stack if possible” because any address taking results in an allocation on the heap even if it could have been avoided.</p>

<p>In the example above you can say that everything is correct, <code>b</code> leaks into the <code>println</code> function, so let’s look at an example without the call:</p>

<pre><code>fn main() {
    a := 100
    b := &amp;a
}
</code></pre>

<p>C code:</p>

<div><pre><code><span>void</span> <span>main__main</span><span>(</span><span>void</span><span>)</span> <span>{</span>
    <span>int</span> <span>*</span><span>a</span> <span>=</span> <span>HEAP</span><span>(</span><span>int</span><span>,</span> <span>(</span><span>100</span><span>));</span>
    <span>int</span><span>*</span> <span>b</span> <span>=</span> <span>&amp;</span><span>(</span><span>*</span><span>(</span><span>a</span><span>));</span>
<span>}</span>
</code></pre></div>

<p>And still allocated on heap.</p>

<p>Instead of doing a smart escape analysis in V has a hack through the special <code>heap</code> attribute:</p>

<blockquote>
  <p>A solution to this dilemma is the <code>[heap]</code> <a href="https://github.com/vlang/v/blob/master/doc/docs.md#attributes">attribute</a> at the declaration of <code>struct MyStruct</code>. It instructs the compiler to <em>always</em> allocate <code>MyStruct</code>-objects on the heap.</p>
</blockquote>

<p>This is a bad solution because the developer cannot control where the object is allocated in each instantiation; by marking the structure with this attribute, you automatically get unnecessary allocations that could have been avoided.</p>

<hr>

<p>The quote from the beginning of the section also mentioned string buffers, so let’s take a look:</p>

<pre><code>import strings

fn main() {
    before := gc_heap_usage()
    mut sb := strings.new_builder(1)
    sb.write_string("hello")
    res := sb.str()
    after := gc_heap_usage()
    println(res)
    println(after.bytes_since_gc - before.bytes_since_gc)
}
</code></pre>



<p>Allocates 48 bytes, although there are only 5 bytes in the string.</p>

<p>Perhaps things are better with string interpolation?</p>

<pre><code>fn main() {
    before := gc_heap_usage()
    world := "world"
    res := "hello ${world}"
    after := gc_heap_usage()
    println(res)
    println(after.bytes_since_gc - before.bytes_since_gc)
}
</code></pre>



<p>Oho-ho, allocates 304 bytes for string of 11 characters. Impressive.</p>

<p>Let’s talk a little more about <code>arena</code> in this section.</p>

<h3 id="arena--prealloc">Arena (<code>-prealloc</code>)</h3>

<p>What does the documentation tell us about this mode? The only mention in the documentation I found was this line:</p>

<blockquote>
  <p>Arena allocation is available via v <code>-prealloc</code>.</p>
</blockquote>

<p>Oops. As I said, the documentation in V is bad.</p>

<p>Let me tell you myself, an arena is a way of working with memory, when at the start of the program a large chunk of memory is allocated at once, for example, 16 megabytes. Then, all allocations occur in this chunk; all explicit memory free does nothing. When a chunk is full, a new one is allocated, and so on. Before the program ends, all memory is freed.</p>

<p>This method is usually best suited for short-lived programs, such as compilers, where memory consumption may be less preferable to faster runtime.</p>

<p>What is the advantage of this mode? If small objects are often allocated in a program, then their allocation will take literally several arithmetic operations, instead of access to the operating system for memory each time.</p>

<p>Let’s dive into the world of V. All the implementation code can be found in the <a href="https://github.com/vlang/v/blob/master/vlib/builtin/prealloc.c.v"><code>prealloc.c.v</code></a> file.</p>

<p>The first thing we see is the <code>@[has_globals]</code> attribute of the module. But wait a minute:</p>

<blockquote>
  <p>By default V does not allow global variables. However, in low level applications they have their place so their usage can be enabled with the compiler flag <code>-enable-globals</code>.</p>
</blockquote>

<p>But ok.</p>

<p>Below we see exactly the reason for the presence of this flag:</p>

<div><pre><code>__global g_memory_block &amp;VMemoryBlock
</code></pre></div>

<p>Global variable. <code>__global</code>.</p>

<p>But since it’s global, what about multithreading? I didn’t see any mutexes, which means that <code>-prealloc</code> cannot be used in multithreaded programs safely. Is this written somewhere in the documentation? Nope. There is a comment in the file itself where this is written, apparently the author of the language believes that all users should first read the source code of the compiler.</p>

<h3 id="conclusions-about-memory-management">Conclusions about memory management</h3>

<p>Some parts are raw, some are unsafe, some don’t work, some don’t work as described. This is just what I could find. If such sloppiness is everywhere, this may mean that it is quite possible that there are even more critical bugs that we simply are not aware of.</p>

<p>This is where we’ll finish talking about working with memory in V.</p>

<p>Next, let’s quickly go through the site before a new interesting topic, coroutines in V.</p>

<h2 id="site-claims">Site claims</h2>

<p>Site is stated that there are no <code>null</code> in the language, without taking into account <code>unsafe</code> code. So:</p>

<pre><code>struct Foo {
    data &amp;string
}

fn main() {
    println(Foo{
        data: 0
    })
}
</code></pre>

<p>there is no <code>null</code>, but you can assign 0 to a pointer. ¯_(ツ)_/¯</p>

<p><img src="https://n-skvortsov-1997.github.io/reviews/img/3-resized.png" alt=""></p>

<p>Then the site tells us that there is no UB in the language. Let’s open the <a href="https://en.wikipedia.org/wiki/Undefined_behavior">article</a> about UB on the wiki.</p>

<p>Overflows in V really haven’t been UB since <a href="https://github.com/vlang/v/commit/c6412597abe24cdf099c9031ebdc47a3a263d141">recently</a>. It took 4 years from the release of the language to fix this UB. Although there is not a word about this in the documentation, the language also has no specification, so for the user this fact is hidden behind the compiler code. Here is a description of the С flag that was added as a fix:</p>

<blockquote>
  <p>This option instructs the compiler to assume that signed arithmetic overflow of addition, subtraction and multiplication wraps around using twos-complement representation. This flag enables some optimizations and disables others.</p>
</blockquote>

<p>Honestly, in a safe language, as the site says, I would expect the ability to perform these operations safely with the ability to specify the behavior on overflow (<code>a.safe_add(b) or { panic("aaaa") }</code>), and by default – panic.</p>

<p>Let’s try another example from the wiki article:</p>

<pre><code>fn main() {
    a := 100
    b := 200
    println(&amp;a &lt; &amp;b)
}
</code></pre>

<p>C code:</p>

<div><pre><code><span>void</span> <span>main__main</span><span>(</span><span>void</span><span>)</span> <span>{</span>
    <span>int</span> <span>a</span> <span>=</span> <span>100</span><span>;</span>
    <span>int</span> <span>b</span> <span>=</span> <span>200</span><span>;</span>
    <span>println</span><span>(</span><span>&amp;</span><span>a</span> <span>&lt;</span> <span>&amp;</span><span>b</span> <span>?</span> <span>_SLIT</span><span>(</span><span>"true"</span><span>)</span> <span>:</span> <span>_SLIT</span><span>(</span><span>"false"</span><span>));</span>
<span>}</span>
</code></pre></div>

<p>Code from V article:</p>

<pre><code>int main(void)
{
  int a = 0;
  int b = 0;
  return &amp;a &lt; &amp;b; /* undefined behavior */
}
</code></pre>

<p>One-to-one, it’s UB.</p>

<p>Let’s try to dereference a null pointer:</p>

<pre><code>struct Data {
    name string
}

fn (d &amp;Data) some() {
    println(d.name)
}

struct Foo {
mut:
    data &amp;Data
}

fn main() {
    mut foo := Foo{
        data: 0
    }
    foo.data.some()
}
</code></pre>

<div><pre><code>code.v:6: at main__Data_some: RUNTIME ERROR: invalid memory access
code.v:18: by main__main
code.13715926371810092027.tmp.c:16997: by main
</code></pre></div>

<p>No safety at all.</p>

<p>Let’s move forward.</p>

<blockquote>
  <p>No undefined values</p>
</blockquote>

<p>Okay, let’s create a structure with an interface field:</p>

<pre><code>interface IFoo {
    name() string
}

struct Foo {
mut:
    foo IFoo
}

fn main() {
    mut foo := Foo{}
    println(foo.foo.name())
}
</code></pre>

<p>And run it:</p>

<pre><code>RUNTIME ERROR: invalid memory access
</code></pre>

<p>Oops, the problem is that an uninitialized field with an interface type actually has an undefined value. But you won’t be able to find information about this in the documentation.</p>

<blockquote>
  <p>No global variables <em>(can be enabled for low level apps like kernels via a flag)</em></p>
</blockquote>

<p>We’ve already seen a hack through <code>[has_globals]</code>. Although it seems it’s only allowed for the compiler. So that’s true.</p>

<p>Let’s move to the performance section:</p>

<blockquote>
  <p>C interop without any costs</p>
</blockquote>

<p>Indeed, that’s true.</p>

<blockquote>
  <p>Minimal amount of allocations</p>
</blockquote>

<p>It has already been proven above that this is not true.</p>

<blockquote>
  <p>Built-in serialization without runtime reflection</p>
</blockquote>

<p>That’s indeed true.</p>

<blockquote>
  <p>Compiles to native binaries without any dependencies: a simple web server is only about 250 KB</p>
</blockquote>

<p>Let’s try to compile the official <a href="https://github.com/vlang/v/blob/master/examples/vweb/vweb_example.v">example</a> with <code>V 0.4.3 c3cf9ee.cc220e6</code> on Ubuntu 22.04.</p>

<p>It took indefinitely long to compile this example with the <code>-prod</code> flag, so I manually inserted the required optimization flags.</p>

<p>Let’s try to compile:</p>

<div><pre><code>v ./v/examples/vweb/vweb_example.v -cflags "-Os" -o vweb_server
</code></pre></div>

<p>And check the size:</p>

<div><pre><code>-rwxr-xr-x 1 root root 4511876 Nov 19 14:03 vweb_server
</code></pre></div>

<p>Oops, 4mb, a bit far from 250 KB. Let’s try a couple of tricks:</p>

<div><pre><code>v ./v/examples/vweb/vweb_example.v -cflags "-Os -flto" -o vweb_server -skip-unused
</code></pre></div>

<div><pre><code>-rwxr-xr-x 1 root root 2784628 Nov 19 14:07 vweb_server
</code></pre></div>

<p>Better, only 2.7 megabytes, but still not 250 KB.</p>

<p>Another trick I found in a <a href="https://github.com/vlang/v/discussions/19792">GitHub discussion</a>:</p>

<div><pre><code>v ./v/examples/vweb/vweb_example.v -cflags "-Os -flto" -o vweb_server -skip-unused -d use_openssl
</code></pre></div>

<div><pre><code>-rwxr-xr-x 1 root root 1565316 Nov 19 14:09 vweb_server
</code></pre></div>

<p>We’re getting closer, but I don’t have other tricks.</p>

<p>Well, maybe everything got statically linked, so the size is big:</p>

<div><pre><code>ldd ./vweb_server
    linux-vdso.so.1 (0x00007ffde677d000)
    libatomic.so.1 =&gt; /lib/x86_64-linux-gnu/libatomic.so.1 (0x00007fc25455a000)
    libc.so.6 =&gt; /lib/x86_64-linux-gnu/libc.so.6 (0x00007fc254332000)
    /lib64/ld-linux-x86-64.so.2 (0x00007fc25456e000)
</code></pre></div>

<p>What about with <code>-d use_openssl</code>?</p>

<div><pre><code>ldd ./vweb_server
    linux-vdso.so.1 (0x00007ffc2b242000)
    libatomic.so.1 =&gt; /lib/x86_64-linux-gnu/libatomic.so.1 (0x00007fa673d80000)
    libc.so.6 =&gt; /lib/x86_64-linux-gnu/libc.so.6 (0x00007fa673b58000)
    libssl.so.3 =&gt; /lib/x86_64-linux-gnu/libssl.so.3 (0x00007fa673ab4000)
    libcrypto.so.3 =&gt; /lib/x86_64-linux-gnu/libcrypto.so.3 (0x00007fa673671000)
    /lib64/ld-linux-x86-64.so.2 (0x00007fa673d94000)
</code></pre></div>

<p>Hmm, as a result, the size is 5-17 times larger, and there are a lot of dependencies.</p>

<p>Let’s go further.</p>

<blockquote>
  <p>As fast as C (V’s main backend compiles to human readable C), with equivalent code.
<em>V does introduce some overhead for safety (such as array bounds checking, GC free), but these features can be disabled/bypassed when performance is more important.</em></p>
</blockquote>

<p>Just because you compile in C doesn’t mean you instantly get the same performance as handwritten C code. I have already shown above how V carelessly works with memory; not a single experienced C developer will make such mistakes.</p>

<p>V can be as fast as C, but then a lot of things in the language cannot be used: string interpolation, interfaces, sum types, arrays and much more.</p>



<p>The official site says:</p>

<blockquote>
  <p>V can translate your entire C project and offer you the safety, simplicity, and compilation speed-up (via modules).</p>
</blockquote>

<p>Sounds great, let’s try it. Before that, let’s pay attention on another statement:</p>

<blockquote>
  <p>A blog post about translating DOOM will be published.</p>
</blockquote>

<p>You can find the same statement on the <a href="https://web.archive.org/web/20200709094936/https://vlang.io/">official website in 2020</a>. Maybe we need to wait a little longer.
During the article, I have already pointed out such moments several times; this is the distinctive feature of V, <strong>to promise and not to deliver</strong>.</p>

<p>Well, let’s move on to c2v. Its repository can be found here: https://github.com/vlang/c2v</p>

<p>It doesn’t have to be downloaded, it can be used via <code>v translate</code>. By the way, you will not find this command in <code>v help</code>:</p>

<div><pre><code>V supports the following commands:

* Project Scaffolding Utilities:
  new                          Setup the file structure for a V project
                               (in a sub folder).
  init                         Setup the file structure for an already existing
                               V project.

* Commonly Used Utilities:
  run                          Compile and run a V program. Delete the
                               executable after the run.
  crun                         Compile and run a V program without deleting the
                               executable.
                               If you run the same program a second time,
                               without changing the source files,
                               V will just run the executable, without
                               recompilation. Suitable for scripting.
  test                         Run all test files in the provided directory.
  fmt                          Format the V code provided.
  vet                          Report suspicious code constructs.
  doc                          Generate the documentation for a V module.
  vlib-docs                    Generate and open the documentation of all the
                               vlib modules.
  repl                         Run the REPL.
  watch                        Re-compile/re-run a source file, each time it is
                               changed.
  where                        Find and print the location of current project
                               declarations.
</code></pre></div>

<p>Did I mention that the documentation is bad?</p>

<p>Let’s take a simple example:</p>



<p>Let’s call the command <code>v translate wrapper main.h</code> and open the resulting file:</p>

<pre><code>[translated]
module.

fn C.foo(a int, b int) int

pub fn foo(a int, b int) int {
  return C.foo(a, b)
}
</code></pre>

<p>Everything seems fine, but the module name is incorrect.</p>

<p>In C libraries, constants are often defined using <code>#define</code>:</p>

<div><pre><code><span>#define VERSION 1.0
</span>
<span>int</span> <span>foo</span><span>(</span><span>int</span> <span>a</span><span>,</span> <span>int</span> <span>b</span><span>);</span>
</code></pre></div>

<p>But as a result, c2v simply skips this constant, and it is not in the V code. The generated V code with <code>#define</code> fully equals to the code without it.</p>

<p>Okay, let’s take a slightly more complicated example:</p>

<div><pre><code><span>#include &lt;stdlib.h&gt;
</span>
<span>typedef</span> <span>struct</span> <span>{</span>
    <span>union</span> <span>{</span>
        <span>char</span> <span>*</span><span>ptr</span><span>;</span>
        <span>char</span> <span>small</span><span>[</span><span>16</span><span>];</span>
    <span>};</span>
    <span>size_t</span> <span>size</span><span>;</span>
<span>}</span> <span>string</span><span>;</span>

<span>int</span> <span>foo</span><span>(</span><span>string</span> <span>str</span><span>);</span>
</code></pre></div>

<p>This is a simple string implementation.</p>

<pre><code>[translated]
module .

struct Lldiv_t { 
    quot i64
    rem i64
}
struct String { 
    size usize
}
fn C.foo(str String) int

pub fn foo(str String) int {
    return C.foo(str)
}
</code></pre>

<p>Wait a minute, what is this <code>Lldiv_t</code> and why is there only one field in the structure…</p>

<p>I really like constancy:</p>

<div><pre><code><span>int</span> <span>foo</span><span>(</span><span>const</span> <span>int</span> <span>*</span><span>const</span> <span>val</span><span>);</span>
</code></pre></div>

<p>But c2v doesn’t:</p>

<pre><code>[translated]
module .

fn C.foo(val Int *const) int

pub fn foo(val Int *const) int {
    return C.foo(val)
}
</code></pre>

<p>Absolutely incorrect code.</p>

<p>You will say that I am making everything up and no one writes such code, okay, let’s take an example from real life. Let’s take the library that V uses for JSON: https://github.com/DaveGamble/cJSON.</p>

<div><pre><code>v translate wrapper cJSON.h
</code></pre></div>

<p>Aaaaaand…</p>

<p>Almost all is incorrect:</p>

<pre><code>fn C.cJSON_GetObjectItem(object CJSON *const, string_ Char *const) &amp;CJSON

pub fn cjson_getobjectitem(object CJSON *const, string_ Char *const) &amp;CJSON {
    return C.cJSON_GetObjectItem(object, string_)
}

...

fn C.cJSON_IsTrue(item CJSON *const) CJSON_bool

pub fn cjson_istrue(item CJSON *const) CJSON_bool {
    return C.cJSON_IsTrue(item)
}

...

fn C.cJSON_ReplaceItemViaPointer(parent CJSON *const, item CJSON *const, replacement &amp;CJSON) CJSON_bool

pub fn cjson_replaceitemviapointer(parent CJSON *const, item CJSON *const, replacement &amp;CJSON) CJSON_bool {
    return C.cJSON_ReplaceItemViaPointer(parent, item, replacement)
}
</code></pre>

<p>Okay, let’s take another one, for example, <a href="https://github.com/vlang/v/blob/master/thirdparty/libbacktrace/backtrace.h"><code>backtrace.h</code></a>.</p>

<div><pre><code>v translate wrapper backtrace.h
</code></pre></div>

<p>Better, although we lost <code>Backtrace_state</code> and <code>Uintptr_t</code> and have an error:</p>

<div><pre><code>backtrace.v:29:59: error: unexpected token `&amp;`, expecting name
   27 | fn C.backtrace_print(state &amp;Backtrace_state, skip int,  &amp;C.FILE)
   28 | 
   29 | pub fn backtrace_print(state &amp;Backtrace_state, skip int,  &amp;C.FILE)  {
      |                                                           ^
   30 |     C.backtrace_print(state, skip, &amp;C.FILE)
   31 | }
</code></pre></div>

<p>Well, okay, c2v is not great with wrappers, but the tool can also translate entire C code into V.</p>

<p>Let’s start with something simple:</p>

<div><pre><code><span>#include &lt;stdio.h&gt;
</span>
<span>int</span> <span>main</span><span>(</span><span>int</span> <span>argc</span><span>,</span> <span>char</span> <span>**</span><span>argv</span><span>)</span> <span>{</span>
    <span>printf</span><span>(</span><span>"%d"</span><span>,</span> <span>argc</span><span>);</span>
    <span>return</span> <span>1</span><span>;</span>
<span>}</span>
</code></pre></div>

<p>Call c2v:</p>



<p>Oops, where did <code>argc</code> go:</p>

<pre><code>[translated]
module main

fn main() {
    C.printf(c'%d', argc)
    return
}
</code></pre>

<p>I specifically returned 1 from <code>main</code> in the C code, but c2v ignored this and simply inserted <code>return</code>, thereby changing the behavior of the program.</p>

<p>Let’s take something more complicated:</p>

<div><pre><code><span>#include &lt;stdio.h&gt;
#include &lt;stdlib.h&gt;
</span>
<span>void</span><span>*</span> <span>my_malloc</span><span>(</span><span>size_t</span> <span>size</span><span>)</span> <span>{</span>
    <span>void</span><span>*</span> <span>ptr</span> <span>=</span> <span>malloc</span><span>(</span><span>size</span><span>);</span>
    <span>if</span> <span>(</span><span>ptr</span> <span>==</span> <span>NULL</span><span>)</span> <span>{</span>
        <span>printf</span><span>(</span><span>"malloc failed"</span><span>);</span>
        <span>exit</span><span>(</span><span>1</span><span>);</span>
    <span>}</span>
    <span>return</span> <span>ptr</span><span>;</span>
<span>}</span>

<span>int</span> <span>main</span><span>(</span><span>int</span> <span>argc</span><span>,</span> <span>char</span> <span>**</span><span>argv</span><span>)</span> <span>{</span>
    <span>int</span> <span>*</span><span>a</span> <span>=</span> <span>my_malloc</span><span>(</span><span>sizeof</span><span>(</span><span>int</span><span>));</span>
    <span>*</span><span>a</span> <span>=</span> <span>1</span><span>;</span>
    <span>printf</span><span>(</span><span>"%d"</span><span>,</span> <span>*</span><span>a</span><span>);</span>
    <span>return</span> <span>1</span><span>;</span>
<span>}</span>
</code></pre></div>

<p>V:</p>

<pre><code>[translated]
module main

struct Lldiv_t {
    quot i64
    rem  i64
}

fn my_malloc(size usize) voidptr {
    ptr := C.malloc(size)
    if ptr == (unsafe { nil }) {
        C.printf(c'malloc failed')
        C.exit(1)
    }
    return ptr
}

fn main() {
    a := my_malloc(sizeof(int))
    *a = 1
    C.printf(c'%d', *a)
    return
}
</code></pre>

<p>It looks correct at first glance, but c2v has lost the fact that the result of <code>my_malloc(...)</code> is cast implicitly in <code>int*</code>. So if you have implicit casts in your code, then apparently everything will not work out of the box. But in C, implicit casts are rare, so it’s not a problem, right?</p>

<p><img src="https://n-skvortsov-1997.github.io/reviews/img/4-resized.png" alt=""></p>

<p>We can talk about this for a long time; such simple examples already show how crude and unfinished this tool is. Considering the fact that it was announced and began development somewhere in 2020, the tool achieved such excellent success in just 3 years.</p>

<hr>

<p>Returning to the site:</p>

<blockquote>
  <p>Powerful graphics libraries</p>
</blockquote>

<blockquote>
  <p>The following features are planned:</p>

  <ul>
    <li>Loading complex 3D objects with textures</li>
    <li>Camera (moving, looking around)</li>
    <li>Skeletal animation</li>
  </ul>

  <p>DirectX, Vulkan, and Metal support is planned.</p>
</blockquote>

<p>At least <a href="https://web.archive.org/web/20200426171536/https://vlang.io/">three years</a> they promise what will happen. But the main thing is to promise, right?</p>

<h2 id="v-ui">V UI</h2>

<p>Another project that showed promise, but something went wrong. Project repository: https://github.com/vlang/ui</p>

<p>Over the last year, the project has had about 80 commits, of which a maximum of 10–20 are <strong>not fixes for the new version V</strong>. The project has been abandoned and is not being developed.</p>

<p>But let’s see, in the readme we are greeted with an example:</p>

<p><img src="https://n-skvortsov-1997.github.io/reviews/img/23-resized.png" alt=""></p>

<p>Do you know what’s interesting? If you go to <a href="https://github.com/vlang/ui/tree/3dd0e7a3f6cb5f316bacf1d556cb8abde25d2c84">repository from 2020</a> then the same picture will be there.</p>

<p>The project has <a href="https://github.com/vlang/ui/issues/31">ROADMAP</a> which was created in 2020, after 3 years only several points there were closed.</p>

<p>You can say that perhaps developers spend all their time on the compiler, but at the same time, there are new projects like <a href="https://github.com/vlang/education-platform">Education Platform</a>, <a href="https://veery.cc/">veery</a>, <a href="https://discord.com/channels/592103645835821068/592106336838352923/1176183805174894653">vbrowser</a>, <a href="https://github.com/vlang/heroesV">heroesV</a>. You may notice that all these projects start and are quickly abandoned. The same thing with V UI, but it lived a little longer, like the <a href="https://github.com/vlang/vinix">operating system</a> on V, which was developed while there was a person with experience, as soon as he left, the project died.</p>

<p>The V UI project description on the website says:</p>

<blockquote>
  <p>V has a UI module that uses custom drawing, similar to Qt and Flutter, but with as much similarity to the native GUI toolkit as possible.</p>
</blockquote>

<p>Okay, let’s check the examples:</p>

<p><img src="https://n-skvortsov-1997.github.io/reviews/img/5-resized.png" alt=""></p>

<p>I was able to copy the value in the password field without any problems, security is not a strong point of V UI:</p>

<p><img src="https://n-skvortsov-1997.github.io/reviews/img/6-resized.png" alt=""></p>

<p>And these are official examples:</p>

<p><img src="https://n-skvortsov-1997.github.io/reviews/img/7-resized.png" alt=""></p>

<p><img src="https://n-skvortsov-1997.github.io/reviews/img/8-resized.png" alt=""></p>

<p>It’s hard for me to say where the authors saw the maximum similarity with the native UI.</p>

<p>The last part of this section sounds somewhat familiar:</p>

<blockquote>
  <p>Coming soon:</p>

  <ul>
    <li>a Delphi-like visual editor for building native GUI apps</li>
    <li>iOS support</li>
  </ul>
</blockquote>

<p>Again, promises that are <a href="https://web.archive.org/web/20200513204922/https://vlang.io/">three years old</a>.</p>

<h2 id="coroutines">Coroutines</h2>

<p>Сoroutines in V. From the very beginning, V copied Go, and if it’s easy to copy the syntax, then to copy goroutines you need to have very mature and senior developers. Therefore, from the very beginning, V builds its multithreading on threads with all the disadvantages.</p>

<p>But at some point the creator of the language thought, what’s stopping us from making coroutines. The coroutines were “done” in three commits:</p>

<ul>
  <li><a href="https://github.com/vlang/v/commit/45f16a2640d94202f98e32c5be67ba950662217f">all: coroutines (part 1)</a></li>
  <li><a href="https://github.com/vlang/v/commit/9db10c8f61c88625f33171cc9b4f2821af0a6678">all: coroutines (part 2)</a></li>
  <li><a href="https://github.com/vlang/v/commit/786865d34972b2bb53c66e0aaaad9af5cf8d76d1">coroutines: init() that runs automatically</a></li>
</ul>

<p>With a difference of 4 hours, an impressive speed of implementation.</p>

<p>Let’s see what kind of implementation coroutines have in V, stackless or stackful. The main implementation file is <a href="https://github.com/vlang/v/blob/45f16a2640d94202f98e32c5be67ba950662217f/vlib/coroutines/coroutines.v">coroutines/coroutines.v</a>.</p>

<p>Wait a minute, this is not the implementation I expected:</p>

<pre><code>#flag -I @VEXEROOT/thirdparty/photon
#flag @VEXEROOT/thirdparty/photon/photonwrapper.so

#include "photonwrapper.h"

fn C.photon_init_default() int
fn C.photon_thread_create11(f voidptr)
fn C.photon_sleep_s(n int)
fn C.photon_sleep_ms(n int)

// sleep is coroutine-safe version of time.sleep()
pub fn sleep(duration time.Duration) {
    C.photon_sleep_ms(duration.milliseconds())
}
</code></pre>

<p>What we see here are bindings for some third-party library. Here is the link to it: https://github.com/alibaba/PhotonLibOS. So, what we get is that coroutines in V are a 10-line wrapper over a third-party C++ library.</p>

<p>Well, okay, in the examples there is code that will help us understand the strengths of coroutines <a href="https://github.com/vlang/v/blob/master/examples/coroutines/simple_coroutines.v">simple_coroutines.v</a> (or not). The whole example is a couple of loops with a sleep calls. Well, let’s try to build it:</p>

<div><pre><code>v -use-coroutines ./examples/coroutines/simple_coroutines.v 
coroutines .so not found, downloading...
done!
</code></pre></div>

<p>Um, it downloads a dynamic library from somewhere unknown, but okay.</p>

<p>We get some output, but how do we understand that it is correct? The biggest difficulty of coroutines is context switching, that is, when one coroutine, for example, waits for a file to be read, and gives way to another coroutine. And here’s the problem: in V, the entire standard library is written in a synchronous manner.</p>

<p>Let’s, for example, look at <a href="https://github.com/vlang/v/blob/6cc51f254f6a6ea921726f6014107a7100ad97d1/vlib/os/os.c.v#L111">file read</a> in V:</p>

<pre><code>pub fn read_file(path string) !string {
  ...
  nelements := int(C.fread(str, 1, allocate, fp))
  ...
}
</code></pre>

<p>Here we see that V calls a C <code>read</code> function that reads a given number of bytes into the buffer. The problem is that <code>read</code> is a blocking function and the context will never be switched. One of the PhotonLibOS project maintainers <a href="https://github.com/alibaba/PhotonLibOS/issues/148#issuecomment-1761298839">says the same</a> about this. The same goes for the network, V also uses the blocking API from C.</p>

<p>And from this, it becomes clear that coroutines in V are not only a useless binding for a third-party lib, but also non-working as expected. Let’s see what their “author”, the creator of the language, says:</p>

<p>https://discord.com/channels/592103645835821068/592106336838352923/1165748025377960037</p>

<p><img src="https://n-skvortsov-1997.github.io/reviews/img/9-resized.png" alt=""></p>

<p>https://discord.com/channels/592103645835821068/592106336838352923/1160886627208544308</p>

<p><img src="https://n-skvortsov-1997.github.io/reviews/img/10-resized.png" alt=""></p>

<p>https://discord.com/channels/592103645835821068/697813437237166131/1138567669323415562</p>

<p><img src="https://n-skvortsov-1997.github.io/reviews/img/11-resized.png" alt=""></p>

<p>https://discord.com/channels/592103645835821068/592320321995014154/1116015948038672414</p>

<p><img src="https://n-skvortsov-1997.github.io/reviews/img/12-resized.png" alt=""></p>

<p>https://discord.com/channels/592103645835821068/592106336838352923/1160744638529933463</p>

<p><img src="https://n-skvortsov-1997.github.io/reviews/img/13-resized.png" alt=""></p>

<p>He lies about the last missing coroutine feature, when the coroutines simply don’t work as expected. He lies that coroutines work with IO. I’ll clarify that by working, I personally mean context switching when necessary, and not the fact that the program does not crash.</p>

<p>At the same time, nothing bothers him, and he is already planning to create a new framework for the web and publish links with headings about coroutines:</p>

<p>https://news.ycombinator.com/item?id=37174056</p>

<p><img src="https://n-skvortsov-1997.github.io/reviews/img/14-resized.png" alt=""></p>

<p>And most importantly, I only saw one person from the community who expressed the opinion that the current implementation of coroutines does not work. The rest of the core developers are apparently too busy to check the feature that comes first in <a href="https://github.com/vlang/v/releases/tag/0.4">CHANGELOG</a> version 0.4:</p>

<p><img src="https://n-skvortsov-1997.github.io/reviews/img/15-resized.png" alt=""></p>

<p>And all this without touching on the fact that the creator of the language forces the language to depend on a corporation that at any moment can simply stop supporting the library. If V integrates IO and network from this library to get context switches, then it will be even worse. V will depend on this corporation not only at the coroutine level, but even at the level of simple operations like reading a file or requests over the network. Moreover, this library only supports two main operating systems (Linux and macOS), which means that code with coroutines loses all the flexibility that V has thanks to the C compiler.</p>

<p><img src="https://n-skvortsov-1997.github.io/reviews/img/16-resized.png" alt=""></p>

<p>Also, the author of the language <a href="https://discord.com/channels/592103645835821068/592106336838352923/1162727123472101416">doesn’t understand</a> that you can’t just do two versions of functions, because when you call a function in a coroutine, the function for the coroutine must be used, and if outside the coroutine, the usual one.</p>

<p>This library also does not fully support Windows, which means that V will only get coroutines on Windows if the authors of PhotonLibOS are so kind as to implement it.</p>



<p>Community V is an interesting phenomenon. If you go to the V Discord server, you are unlikely to find criticism of V or the author of the language there. And do you know why? Because the author of the language bans people for their opinions.</p>

<p>For example, I managed to “unsuccessfully” answer a person’s question in the V Telegram channel:</p>

<p><img src="https://n-skvortsov-1997.github.io/reviews/img/17-resized.png" alt=""></p>

<p>For which I was immediately banned without explanation or attempt to show where I was wrong.</p>

<p>Next, to my post on the Discord server where I described the situation, I received responses from several people, one said that everything is not so clear, and we don’t know the whole truth, and the second called me a troll. About an hour later, the author of the language deleted all messages after my post and banned me without explanation also in Discord.</p>

<p>I want to clarify, this post has problems because I wrote it right after I was banned, and perhaps I could have been less arrogant:</p>

<p><img src="https://n-skvortsov-1997.github.io/reviews/img/18-resized.png" alt=""></p>

<p>The creator of the language also <a href="https://discord.com/channels/592103645835821068/853624878556512266/1176019939551879268">called</a> me a troll:</p>

<p><img src="https://n-skvortsov-1997.github.io/reviews/img/19-resized.png" alt=""></p>

<p>Do you know why? Because he needed to justify himself to a person who directly stated that he did not approve of such behavior. Apologize? Admit mistake? No, this is not Alex’s way; his way is to dehumanize the victim by calling him a troll and banning anywhere.</p>

<p><img src="https://n-skvortsov-1997.github.io/reviews/img/20-resized.png" alt=""></p>

<p>And you know what’s the funniest thing? Today this person was banned. Why? Because he disagrees with the policy for which I was banned the first time and asked to remove his article from <a href="https://github.com/vlang/education-platform">vlang/education-platform</a>. This was the only article with content, <a href="https://github.com/vlang/education-platform/tree/master/lessons">the other two</a> consist of two phrases: “V is great.” and “In this lesson, we’ll examine one of the simplest codes in V.”, which were apparently written by Alex himself.</p>

<p><img src="https://n-skvortsov-1997.github.io/reviews/img/21-resized.png" alt=""></p>

<p>Another lie, Alex himself wrote to me in Telegram, not the moderator. He told me that I could come to him in a private message in Telegram, and he would have unbanned me. Think about it, first he bans you on two platforms even though you didn’t break the rules, and then he says that you could come in private messages, and he would unban you. Unfortunately, I can’t provide a screenshot, since Alex deleted all the messages (familiar behavior, isn’t it?) that he wrote to me when he realized that I was not ready to humiliate myself.</p>

<p>Today I was also banned for the third time, I created a specially new account to report that I was writing this article.</p>

<p><img src="https://n-skvortsov-1997.github.io/reviews/img/22-resized.png" alt=""></p>

<p>And also to see how far Alex would go to try to shut me up.</p>

<p>Well, the result of this is obvious, although today I was able to communicate with more people from the community than last time, and we even came to some kind of understanding. However, a few hours later Alex came and unceremoniously deleted all my messages and banned me. asvln’s messages were also deleted and he was banned.</p>

<p>It is also interesting that after the messages were deleted, none of those with whom I spoke expressed disturbance about the deletion and bans, and here either they agree with Alex’s actions, or they just don’t care what is happening in their community, or they are simply afraid to speak out something against Alex, as this will lead to their ban. <strong>And each option is worse than the other.</strong></p>

<p>Apparently the author of the language does not understand that trying to shut people up will only cause more damage. This time I’m documenting everything carefully.</p>

<p>Considering the fact that I have not seen criticism like this, all the brave souls here are banned. 3
years have passed, and nothing has changed, the author of other articles in which V is not praised as a divine creation was also <a href="https://christine.website/blog/vlang-update-2020-06-17/">banned</a>. And this is not the last example, here is a person <a href="https://twitter.com/MaxGraey/status/1430073855062814720">banned on Twitter</a> for arguing with the author of the language. I’m almost sure that there were dozens, maybe hundreds of such cases.</p>

<p>Do you want to be part of the community where banning for facts is ok, calling those who try to find out, compare or point out flaws as trolls is ok, and where the only correct opinion belongs to one person?</p>

<p>Me not.</p>

<p>I really want to see Alex’s Volt, which he promises people from 2019; apparently the main feature there will be the ability to ban people with the power of thought with automatic clearing of the chat. Beta was <a href="https://discord.com/channels/592103645835821068/708726848523075644/1109496090161594458">promised</a> in May 2023, but something went wrong and the author of the language simply <a href="https://discord.com/channels/592103645835821068/708726848523075644/1123052269227737241">ignores</a> people since then (another distinctive “feature” of Alex).</p>

<hr>

<p>As a result, the author of the language tried to shut me up, but got this article. I can already see how he is trying to justify himself by calling me a troll, a hater or something else, but whatever the reason for this article, all of the above are facts.</p>

<p>Of course, the author of the language will say that these problems are easy to fix (good luck). After a “week” Alex will say that they are fixed, but this does not solve the global problems in the language, when problems are fixed only when they are pointed out. Developers are not interested in looking for bugs on their own. And this is obvious because, apart from the compiler, they do not develop large projects on V on which they could quickly find all these problems.</p>

<p>Despite the fact that the language is almost 5 years old, you can still find very primitive problems that the language developers for some reason did not find before the users. The author of the language promises, as was the case with autofree, then postpones and promises again, and so on ad infinitum, how can one even trust such a person.</p>

<p>This is where the article ends; only you can decide whether V is worth spending time on. In this article, I have only scratched the surface of V; to describe everything that V is bad at, I would need to write a book. I hope that this article will help you make the right decision.</p>

<p>Bye.</p>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Institutions try to preserve the problem to which they are the solution (864 pts)]]></title>
            <link>https://effectiviology.com/shirky-principle/</link>
            <guid>39491863</guid>
            <pubDate>Sat, 24 Feb 2024 14:53:08 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://effectiviology.com/shirky-principle/">https://effectiviology.com/shirky-principle/</a>, See on <a href="https://news.ycombinator.com/item?id=39491863">Hacker News</a></p>
<div id="readability-page-1" class="page"><div itemprop="text"><p><img decoding="async" src="https://effectiviology.com/wp-content/uploads/Shirky-Principle.jpg" alt="" width="1000" height="667" srcset="https://effectiviology.com/wp-content/uploads/Shirky-Principle.jpg 1000w, https://effectiviology.com/wp-content/uploads/Shirky-Principle-300x200.jpg 300w, https://effectiviology.com/wp-content/uploads/Shirky-Principle-768x512.jpg 768w" sizes="(max-width: 1000px) 100vw, 1000px"></p><p>The <em>Shirky principle</em> is the adage that “institutions will try to preserve the problem to which they are the solution”. More broadly, it can also be characterized as the adage that “every entity tends to prolong the problem it is solving”.</p><p>For example, the Shirky principle means that a government agency that’s meant to address a certain societal issue may hinder attempts by others to address the issue, in order to ensure that the agency remains relevant. Alternatively, the agency may become so focused on the current way in which it addresses the issue that it will fail to adopt better new solutions as they become available, thus prolonging the issue.</p><p>The Shirky principle has important implications in various domains, so it’s important to understand it. As such, in the following article you will learn more about this principle, and see what you can do about it in practice.</p><div id="ez-toc-container"><nav><ul><li><a href="#Examples_of_the_Shirky_principle" title="Examples of the Shirky principle">Examples of the Shirky principle</a></li><li><a href="#Origin_and_formulations_of_the_Shirky_principle" title="Origin and formulations of the Shirky principle">Origin and formulations of the Shirky principle</a></li><li><a href="#Caveats_about_the_Shirky_principle" title="Caveats about the Shirky principle">Caveats about the Shirky principle</a></li><li><a href="#Accounting_for_the_Shirky_principle" title="Accounting for the Shirky principle">Accounting for the Shirky principle</a></li><li><a href="#Related_concepts" title="Related concepts">Related concepts</a></li><li><a href="#Summary_and_conclusions" title="Summary and conclusions">Summary and conclusions</a></li></ul></nav></div><h2><span id="Examples_of_the_Shirky_principle"></span>Examples of the Shirky principle<span></span></h2><p>An example of the Shirky principle are tax-filing companies who <a href="http://web.archive.org/web/20210509181435/https:/www.propublica.org/article/inside-turbotax-20-year-fight-to-stop-americans-from-filing-their-taxes-for-free">lobby</a> the government to <a href="http://web.archive.org/web/20210509175424/https:/www.propublica.org/article/filing-taxes-could-be-free-simple-hr-block-intuit-lobbying-against-it">prevent</a> it from <a href="http://web.archive.org/web/20210509181647/https:/www.politico.com/agenda/story/2018/07/18/tax-filing-congress-irs-000683/">offering</a> a free and easy way to <a href="http://web.archive.org/web/20210416223809/https:/www.nytimes.com/2015/04/16/technology/personaltech/turbotax-or-irs-as-tax-preparer-intuit-has-a-favorite.html">file taxes</a>, to ensure that the companies can continue to make a profit. A similar example of this are private prison companies who <a href="http://web.archive.org/web/20210509182458/https:/www.washingtonpost.com/posteverything/wp/2015/04/28/how-for-profit-prisons-have-become-the-biggest-lobby-no-one-is-talking-about/">lobby</a> the government to <a href="http://web.archive.org/web/20210509184717/https:/www.theguardian.com/commentisfree/2012/sep/27/lawmakers-lobbyists-keep-lock-private-prison-business">support</a> policies that <a href="http://web.archive.org/web/20210509185251/https:/escholarship.org/uc/item/3qj7q63d">increase</a> the number of <a href="http://web.archive.org/web/20210509185809/https:/papers.ssrn.com/sol3/papers.cfm?abstract_id=2794145">incarcerated</a> people and&nbsp;the <a href="https://doi.org/10.1002/9781118519639.wbecpx175">duration</a> of their incarceration.</p><p>Another <a href="https://sagepub.com/en-us/nam/encyclopedia-of-social-media-and-politics/book239101">well-known</a> example of the Shirky principle is described in “<a href="https://amzn.to/3tqsrZz">Cognitive Surplus</a>”, a book by Clay Shirky that contained one of the first discussions of this principle:</p><blockquote><p>“PickupPal.com is… a carpooling site designed to coordinate drivers and riders planning to travel along the same route.</p><p>In May 2008 the Ontario-based bus company Trentway-Wagar… petitioned the Ontario Highway Transport Board (OHTB) to shut PickupPal down on the grounds that, by helping coordinate drivers and riders, it worked too well to be a carpool. Trentway-Wagar invoked Section 11 of the Ontario Public Vehicles Act, which stipulated that carpooling could happen only between home and work (rather than, say, school or hospital.) It had to happen within municipal lines. It had to involve the same driver each day. And gas or travel expense could be reimbursed no more frequently than weekly.</p><p>Trentway-Wagar was arguing that because carpooling used to be inconvenient, it should always be inconvenient, and if that inconvenience disappeared, then it should be reinserted by legal fiat. Curiously, an organization that commits to helping society manage a problem also commits itself to the preservation of that same problem, as its institutional existence hinges on society’s continued need for its management. Bus companies provide a critical service—public transportation—but they also commit themselves, as Trentway-Wagar did, to fending off competition from alternative ways of moving people from one place to another.</p><p>The OHTB upheld Trentway-Wagar’s complaint and ordered PickupPal to stop operating in Ontario. PickupPal decided to fight the case—and lost in the hearing. But public attention became focused on the issue, and in a year of high gas prices, burgeoning environmental concern, and a financial downturn, almost no one took Trentway-Wagar’s side. The public reaction, channeled through everything from an online petition to T-shirt sales, had one message: Save PickupPal. The idea that people couldn’t use such a service was too hot for the politicians in Ontario to ignore. Within weeks of Trentway-Wagar’s victory, the Ontario legislature amended the Public Vehicles Act to make PickupPal legal again.”</p></blockquote><p>In addition, the Shirky principle can also apply to entities other than institutions. For example, an individual employee who’s in charge of a certain process in their workplace might resist attempts to automate that process, in order to ensure that the employee remains necessary to their employer.</p><p>A well-known example of the Shirky effect in this context is the <em>cobra effect</em>. It <a href="https://doi.org/10.1057/s41302-021-00187-7">describes</a> a <a href="http://web.archive.org/web/20210510110107/https:/freakonomics.com/podcast/the-cobra-effect-a-new-freakonomics-radio-podcast/">case</a> where British colonial officials in Delhi (India), set a bounty on dead cobras, in order to reduce the cobra population. However, this led citizens to breed the cobras for profit, and eventually to release them when the bounty was canceled.</p><p>A similar incident occurred circa 1902 in Hanoi (Vietnam), which was under French colonial rule at the time, when French officials sought to reduce the rat population in the city:</p><blockquote><p>“To fight the infestation citywide, the colonial administration added vigilantes to its team of professional killers. Appealing to both civic duty and to the pocketbook, a one-cent bounty was paid for each rat tail brought to the authorities (it was decided that the handing in of an entire rat corpse would create too much of a burden for the already taxed municipal health authorities).</p><p>Unfortunately, this scheme backfired. Despite initial apparent success, the authorities soon discovered that the best laid plans of mice and men often go awry. As soon the municipal administrators publicized the reward program, Vietnamese residents began to bring in thousands of tails. While many desk-bound administrators delighted in the numbers of apparently eliminated rats, more alert officials in the field began to notice a disturbing development. There were frequent sightings of rats without tails going about their business in the city streets. After some perplexity, the authorities realized that less-than-honest but quite resourceful characters were catching rats, but merely cutting off the tails and letting the still-living pests go free (perhaps to breed and produce more valuable tails).</p><p>Later, things became even more serious as health inspectors discovered a disturbing development in the suburbs of Hanoi. These officials found that more enterprising but equally deceptive individuals were actually raising rats to collect the bounty. One can only imagine the frustration of the municipal authorities, who realized that their best efforts at <em>dératisation</em> [extermination of rats] had actually increased the rodent population by indirectly encouraging rat-farming.”</p><p>— From “Of rats, rice, and race: The great Hanoi rat massacre, an episode in French colonial history” (Vann, <a href="https://doi.org/10.1353/fch.2003.0027">2003</a>)</p></blockquote><p>Finally, note that the phenomenon described by the Shirky principle—entities prolonging a problem to which they are the solution—isn’t necessarily the result of intentional actions. For example, a company may inadvertently perpetuate the problem that it solves, because its processes are so focused on the mediocre solution that they’re currently selling, that they don’t realize a better solution exists. Similarly, a company may discourage the use of a certain approach to solving a problem because it previously failed for them, even after technological advancements make this approach viable.</p><h2><span id="Origin_and_formulations_of_the_Shirky_principle"></span>Origin and formulations of the Shirky principle<span></span></h2><p>The Shirky principle was proposed in a <a href="http://web.archive.org/web/20210508141233/https:/kk.org/thetechnium/the-shirky-prin/">2010 blog post</a> by Kevin Kelly, editor of <em>Wired</em> magazine, who based it on the speaking and writing of scholar Clay Shirky.</p><p>Specifically, Kelly attributed the adage that “Institutions will try to preserve the problem to which they are the solution” to a statement that Shirky made in a recent talk, and noted that similar statements were made by Shirky in an associated blog post (“<a href="http://web.archive.org/web/20100404013927/http:/www.shirky.com/weblog/2010/04/the-collapse-of-complex-business-models/">The Collapse of Complex Business Models</a>”) and book (“<a href="https://amzn.to/3tqsrZz">Cognitive Surplus</a>”). There, Shirky states that “an organization that commits to helping society manage a problem also commits itself to the preservation of that same problem, as its institutional existence hinges on society’s continued need for its management”.</p><p>In addition to mentioning the key quote that is now known as the Shirky principle, Kelly also says the following in his <a href="http://web.archive.org/web/20210508141233/https:/kk.org/thetechnium/the-shirky-prin/">blog post</a>:</p><blockquote><p>“The Shirky Principle declares that complex solutions (like a company, or an industry) can become so dedicated to the problem they are the solution to, that often they inadvertently perpetuate the problem.”</p></blockquote><p>Later, he also says the following with regard to this principle (bold added here for emphasis):</p><blockquote><p>“In a strong sense we are defined by the problems we are solving. Yin/Yang, problem/solution, both sides form one unit. <strong>Because of the Shirky Principle, which says that every entity tends to prolong the problem it is solving</strong>, progress sometimes demands that we let go of problems.”</p></blockquote><p>Essentially, in his writing on the topic, Kelly offers three formulations of the Shirky principle, which differ in subtle but important ways:</p><ul><li>The first formulation—“Institutions will try to preserve the problem to which they are the solution”—refers to <em>institutions</em>, and states that they will <em>try</em> to preserve problems, which implies that they do so intentionally.</li><li>The second formulation—“Complex solutions (like a company, or an industry) can become so dedicated to the problem they are the solution to, that often they inadvertently perpetuate the problem”—refers to <em>complex solutions</em>, and states that they often <em>inadvertently</em> perpetuate the problem, which implies that they do so unintentionally.</li><li>The third formulation—”Every entity tends to prolong the problem it is solving”—refers to <em>entities</em>, and states that they <em>tend to</em> prolong problems, without making any claim about their intentions.</li></ul><p>The first formulation is the one that’s most commonly used when people discuss the Shirky principle, though Kelly does not actually refer to it as the Shirky principle in his original blog post. The third formulation, on the other hand, is the most general, though one issue with it is that it states that “every” entity engages in this kind of behavior, which is too absolute of a claim. However, this issue can be addressed by slightly changing this formulation, into “entities tend to prolong the problems they are solving”.</p><p><em>Note</em>: In <a href="http://web.archive.org/web/20210508141233/https:/kk.org/thetechnium/the-shirky-prin/">his post</a>, Kelly states that Shirky’s observation reminds him “of the clarity of the Peter Principle, which says that a person in an organization will be promoted to the level of their incompetence. At which point their past achievements will prevent them from being fired, but their incompetence at this new level will prevent them from being promoted again, so they stagnate in their incompetence.”.</p><h2><span id="Caveats_about_the_Shirky_principle"></span>Caveats about the Shirky principle<span></span></h2><p>There are some caveats about the Shirly principle that are important to keep in mind:</p><ul><li><strong>The Shirky principle is just a general observation.</strong> As such, there are many situations where it’s incorrect. For example, an institution may successfully solve the problem to which they are the solution because there’s greater profit to be made that way than by prolonging the problem.</li><li><strong>The Shirky principle can involve various types of entities.</strong> Though the best-known formulation of the Shirky principle refers to “institutions”, this principle can apply to various types of entities, including individuals and small social groups. This is noted in the general formulation of the principle (“every entity tends to prolong the problem it is solving”).</li><li><strong>The Shirky principle can involve various causes.</strong> For example, one company may prolong a problem unintentionally, due to passivity or inertia, whereas another company may prolong a problem intentionally, due to greed or self-preservation. This is reflected in the general formulation of this principle, which doesn’t make any claims regarding the causes or intentionality of this phenomenon.</li><li><strong>The Shirky principle can involve various patterns of behavior.</strong> For example, one company may prolong an existing problem by not dedicating resources to developing new solutions, whereas another company may actively prevent others from developing such solutions.</li></ul><p>In addition, the behaviors associated with the Shirly principle can vary in other ways. For example:</p><ul><li>An entity may not just preserve an existing problem, but also exacerbate it.</li><li>An entity may create a problem that did not previously exist, if they can be the solution to it.</li><li>An entity may perpetuate a problem that it benefits from, even if the entity is not actually a solution to the problem, though the entity may pretend that it is.</li></ul><p>Based on this, a broader version of Shirky’s principle can be expressed as:</p><blockquote><p>“Entities often promote problems that they benefit from”.</p></blockquote><h2><span id="Accounting_for_the_Shirky_principle"></span>Accounting for the Shirky principle<span></span></h2><p>Accounting for the Shirky principle can be beneficial when it comes to several things:</p><ul><li><strong>Understanding past and current behavior.</strong> For example, it can help you understand why certain institutions are seemingly so bad at solving certain problems, despite all the resources—like time, effort, and money—that they dedicate to those problems.</li><li><strong>Predicting future behavior.</strong> For example, it can help you predict that an executive will keep perpetuating a certain problem, in order to improve their own status within a company, even though this leads to worse outcomes for the company itself.</li><li><strong>Modifying behavior.</strong> For example, if this makes you aware of someone’s incentive to prolong a problem, that could lead you to either eliminate the perverse incentive or create a stronger disincentive. Similarly, this could lead you to point out the issue to the entity in question, in order to encourage them to try and change their behavior themselves if doing so can benefit them in the long term.</li></ul><p>When deciding how and whether to use your understanding of the Shirky principle in practice, it can help to assess relevant factors pertaining to your situation, such as what’s causing someone to act in accordance with this principle, and what outcomes their behavior leads to. For example, you will likely respond differently to a government agency that’s perpetuating a problem due to inefficient bureaucracy, than to a private company that’s perpetuating a problem out of greed, or to an individual who’s acting out of desperate self-preservation.</p><p>Finally, there are also two useful concepts worth keeping in mind when accounting for Shirky’s principle:</p><ul><li><a href="https://effectiviology.com/cui-bono/"><em><strong>Cui bono</strong></em></a>, which is a Latin phrase that means “who benefits?”, and which is used to suggest that there’s a high probability that those responsible for a certain event are the ones who stand to gain from it.</li><li><a href="https://effectiviology.com/hanlons-razor/"><em><strong>Hanlon’s razor</strong></em></a>, which is the adage that you should “never attribute to malice that which is adequately explained by stupidity”, and which, when applied broadly, suggests that when assessing people’s actions, you should not assume that they acted out of a desire to cause harm, as long as there is a reasonable alternative explanation.</li></ul><h2>Related concepts<span></span></h2><p><a href="https://effectiviology.com/parkinsons-law/"><em>Parkinson’s law</em></a> is the adage that “work expands so as to fill the time which is available for its completion” (or more generally, that “work expands to consume the resources available for its completion”). It relates to Shirky’s principle, since both concepts present a common way in which entities are inefficient or ineffective in dealing with problems that they’re supposed to solve.</p><p>Shirky’s principle also relates to another phenomenon that was <a href="https://doi.org/10.1088/1742-5468/2009/03/p03008">identified</a> by Parkinson, whereby the growth of a bureaucratic or administrative body is often associated with a substantial decrease in its overall efficiency. This is <a href="http://web.archive.org/web/20130331045219/http:/www.economist.com/node/14116121">attributed</a> to the desire of officials to increase the number of their subordinates, and to officials’ tendency to create work for each other.</p><p>In addition, a similar famous concept that’s related to Shirky’s principle has been expressed by novelist and social reformer Upton Sinclair, who <a href="http://web.archive.org/web/20210507155626/https:/www.oxfordreference.com/view/10.1093/acref/9780191826719.001.0001/q-oro-ed4-00010168">said</a> that “It is difficult to get a man to understand something when his salary depends on his not understanding it.”</p><h2><span id="Summary_and_conclusions"></span>Summary and conclusions<span></span></h2><ul><li>The <em>Shirky principle</em> is the adage that “institutions will try to preserve the problem to which they are the solution”.</li><li>For example, the Shirky principle means that a government agency that’s meant to address a certain societal issue may hinder attempts by others to address the issue, in order to ensure that the agency remains relevant.</li><li>This principle can be expressed more broadly as “every entity tends to prolong the problem it is solving”, since it can involve entities other than institutions (e.g., individuals), and various patterns of behavior (e.g., unintentionally focusing on an outdated solution vs. intentionally interfering with competition).</li><li>This principle can also be extended to say that “entities often promote problems that they benefit from”, since entities can also create new problems, exacerbate existing ones, and perpetuate problems that they don’t actually solve.</li><li>Accounting for this principle can help understand past and current behavior, predict future behavior, and modify problematic behaviors (e.g., by removing perverse incentives).</li></ul><hr> </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Every single new Google product (115 pts)]]></title>
            <link>https://twitter.com/MarcosBL/status/1761094858205229430</link>
            <guid>39491795</guid>
            <pubDate>Sat, 24 Feb 2024 14:45:11 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://twitter.com/MarcosBL/status/1761094858205229430">https://twitter.com/MarcosBL/status/1761094858205229430</a>, See on <a href="https://news.ycombinator.com/item?id=39491795">Hacker News</a></p>
Couldn't get https://twitter.com/MarcosBL/status/1761094858205229430: Error: Request failed with status code 400]]></description>
        </item>
        <item>
            <title><![CDATA[Understanding, using, and finetuning Gemma (110 pts)]]></title>
            <link>https://lightning.ai/lightning-ai/studios/understanding-using-and-finetuning-gemma</link>
            <guid>39491646</guid>
            <pubDate>Sat, 24 Feb 2024 14:18:20 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://lightning.ai/lightning-ai/studios/understanding-using-and-finetuning-gemma">https://lightning.ai/lightning-ai/studios/understanding-using-and-finetuning-gemma</a>, See on <a href="https://news.ycombinator.com/item?id=39491646">Hacker News</a></p>
&lt;Unparsable&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[Microsoft Joins the MapLibre Sponsorship Program (122 pts)]]></title>
            <link>https://maplibre.org/news/2024-02-20-msft-announcement/</link>
            <guid>39491349</guid>
            <pubDate>Sat, 24 Feb 2024 13:31:37 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://maplibre.org/news/2024-02-20-msft-announcement/">https://maplibre.org/news/2024-02-20-msft-announcement/</a>, See on <a href="https://news.ycombinator.com/item?id=39491349">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><div><p><time datetime=" 2024-02-20">Feb 20, 2024</time>
<span>·</span>
<span>1 minutes read</span></p></div><div><p>Microsoft joins the MapLibre Sponsorship Program as a Silver Sponsor.</p><p><img src="https://maplibre.org/news/2024-02-20-msft-announcement/maplibre-msft-logo.png" width="100%"></p><p>The Silver tier <a href="https://opencollective.com/maplibre/contributions/733415">donation</a> of USD 80,000 will be used to 20 percent for development of new features and to 80 percent for general maintenance of the MapLibre codebases and project coordination.</p><p>From the very beginning Microsoft was instrumental in building the MapLibre Organization and with the membership in the MapLibre Sponsorship Program, Microsoft lays out the foundation for a sustainable common future.</p><p>Microsoft uses MapLibre GL JS in both consumer and enterprise map products.</p><p>Bing Maps deploys MapLibre GL JS in production, a feat which was only possible thanks to performance improvements that were contributed to MapLibre by Bing Maps Engineers.</p><p>Azure Maps serves business customers all around the world with high-quality mapping products which are built on top of MapLibre GL JS.</p><p>We look forward to all the great contributions from Microsoft and are honored to have Microsoft as a Sponsor!</p></div></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Everyone seems to forget why GNOME and GNOME 3 and Unity happened (2022) (140 pts)]]></title>
            <link>https://liam-on-linux.dreamwidth.org/85359.html</link>
            <guid>39490879</guid>
            <pubDate>Sat, 24 Feb 2024 11:51:22 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://liam-on-linux.dreamwidth.org/85359.html">https://liam-on-linux.dreamwidth.org/85359.html</a>, See on <a href="https://news.ycombinator.com/item?id=39490879">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>&nbsp;That is *what* it came from, yes, but not *why*.</p><p>The "why" part seems to be forgotten now: because Microsoft was threatening to sue all the Linux vendors shipping Windows 95-like desktops.</p><p>https://www.theregister.com/2006/11/20/microsoft_claims_linux_code</p><p>Microsoft invented the Win95 desktop from scratch. Its own previous Ones (e.g. Windows for Workgroups 3.11, Windows NT 3.51 and OS/2 1.x) looked nothing like it.</p><p>The task bar, the Start menu, the system tray, "My Computer", "Network Neighbourhood", all that: all original, *patented* Microsoft designs. There was nothing like it before.&nbsp;</p><p>(The closest was Acorn's RISC OS, with an "icon bar" that works very differently, on the Archimedes computer. A handful of those were imported to North America, and right after, NeXT "invented" the Dock, and then Microsoft invented the task bar which is quite a bit more sophisticated.</p><p>One source: the team that programmed it. Here's me moderating a panel discussion by most of the surviving members of Acorn's programming team, on video from a month ago:</p><p>https://www.youtube.com/watch?v=P_SDL0IwbCc</p><p>SUSE signed a patent-sharing deal:</p><p>https://www.theregister.com/2006/11/03/microsoft_novell_suse_linux/</p><p>Note: SUSE is the biggest German Linux company. (Source: I worked for them until last year.) KDE is a German project. SUSE developers did a lot of the work on KDE.&nbsp;</p><p>So, when SUSE signed up, KDE was safe.</p><p>Red Hat and Ubuntu refused to sign.</p><p>So, both needed *non* Windows like desktops, ASAP, without a Start menu, without a taskbar, without a window menu at top left and minimize/maximize/close at top right, and so on.</p><p>Red Hat is the main sponsor of GNOME development. (When KDE was first launched, Qt was not GPL, so Red Hat refused to bundle it or support it, and wrote its own environment instead.)</p><p>Ubuntu tried to get involved with the development of GNOME 3, and was rebuffed. So it went its own way with Unity instead: basically, a Mac OS X rip-off, only IMHO done better. Myself, I still use both Unity and macOS every day. They are like twins, and switching between them is very easy.</p><p>So both RH and Ubuntu switched to non-Windows-like desktops by default.</p><p>In the end MS did not sue anyone... but it got what it wanted: total chaos in the Linux desktop world.</p><p>Before the threats, almost everyone used GNOME 2. Even SUSE bundled GNOME because its corporate owner bought the main GNOME 3rd party developers, Ximian, and forcibly merged the company into SUSE:</p><p>https://www.theregister.com/2004/01/07/novell_marries_suse_to_ximian/</p><p>SUSE, Red Hat, Debian, Ubuntu, even Sun Solaris used GNOME 2. Everyone liked GNOME 2.</p><p>Then Microsoft rattled its sabre, and the FOSS UNIX world splintered in all directions.</p><p>RH uses GNOME 3. Ubuntu used Unity, alienated a lot of people who only knew how to use Windows-like desktops, and that made Mint a huge success. GNOME 2 got forked as MATE, and Mint adopted it, helping a lot. Mint also built its own fork of GNOME 3, Cinnamon. Formerly tiny niche desktops like Xfce and LXDE got a *huge* boost. Debian adopted GNOME 3 and systemd, annoying lots of its developers and causing the Devuan fork to happen.</p><p>Here's an analysis I wrote at the time:</p><p>https://www.theregister.com/2013/06/03/thank_microsoft_for_linux_desktop_fail/</p><p>Yes, Unity evolved out of the Ubuntu netbook desktop, but the reason _why_ it did is that Ubuntu was getting threatened.</p><p>(Xubuntu and Lubuntu and Kubuntu are not official and not the defaults, so they don't endanger it.)</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[How to think about software quality (2022) (128 pts)]]></title>
            <link>https://www.evalapply.org/posts/how-to-not-die-by-a-thousand-cuts/index.html</link>
            <guid>39490543</guid>
            <pubDate>Sat, 24 Feb 2024 10:25:13 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.evalapply.org/posts/how-to-not-die-by-a-thousand-cuts/index.html">https://www.evalapply.org/posts/how-to-not-die-by-a-thousand-cuts/index.html</a>, See on <a href="https://news.ycombinator.com/item?id=39490543">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="the-very-top">
        
  <main id="main">
    <article id="blog-post">
  <header>
    <div>
      <p>How To Not Die By A Thousand Cuts. Or, How To Think About Software Quality.</p>
      <p><span>↑ <a href="#site-header" rel="bookmark">menu</a></span>
           <span>↓ <a href="#blog-post-footer" rel="bookmark">discuss</a></span>
           <span>↓ <a href="#blog-post-toc" rel="bookmark">toc</a></span>
      </p>
      <p>Not a weighty meandering 300 page Zen dialogue on Motorcycle Maintenance. Merely a meandering blog post in which one contemplates /Quality/ of software products.</p>
      <p>
        <span>Published: 2022-01-20</span>
        <span>Updated: 2023-03-10</span>
        <span>Tags:  / <a href="https://www.evalapply.org/tags/quality/index.html#main">#quality</a> / <a href="https://www.evalapply.org/tags/risk/index.html#main">#risk</a> / <a href="https://www.evalapply.org/tags/systems/index.html#main">#systems</a>
        </span>
      </p>
      <hr>
    </div>
  </header>
  <section>
      <p id="blog-post-toc">
  <details open="">
    <summary>
      <strong>Contents</strong>
    </summary>
    <nav>
      <a href="#what-is-the-nature-of-software-products">What is the nature of software products?</a>
<a href="#whom-to-hold-responsible-for-software-quality-assurance">Whom to hold responsible for Software Quality Assurance?</a>
<a href="#why">Why?</a>
<a href="#is-it-different-for-different-kinds-of-products">Is it different for different kinds of products?</a>
<a href="#how-to-destroy-quality">How to destroy Quality?</a>
<a href="#how-to-create-quality">How to create Quality?</a>
<a href="#the-first-skill-is-to-learn-to-suffer-constructively.">The first skill is to learn to suffer constructively.</a>
<a href="#caveats-mea-culpa-etc.">Caveats, mea culpa, etc.</a>
    </nav>
  </details>
</p>
<hr>
  <p>First off, what even is Quality?</p>
<p>All things emerge, change, and die. I think <em>Quality</em> is the experience of the process. The idea of <em>Good Quality</em> essentially boils down to performing the process with grace, and leaving the place better than we found it.</p>
<p>Further, the process of emergence and change—i.e. living—is also the process of dying. It follows that to think clearly about the Quality of the former one must think clearly about the Quality of the latter. The saddest way it can unfold is a slow painful degradation without healing succour meaning or hope. The proverbial death by a thousand cuts. I hope you never witness such a passing, even from afar.</p>
<p>Ok, that got dark fast, and if we're not careful, we will produce a 300 page Zen dialogue on Motorcycle Maintenance. So we will distract ourselves with the much smaller, lighter—and I'd argue, even pleasant—task of contemplating Quality of software products.</p>
<p>None of what follows is novel, but I feel the message <em>and</em> its surrounding context bears repeating, because if it is not obvious already, software fails us all the time. Far too often with terrible consequences.</p>
<h2 id="what-is-the-nature-of-software-products">What is the nature of software products?</h2>
<p>See? This is already easier than asking "What is the nature of life?".</p>
<p>Like any other machine, a software product is wrought of the labour of many minds and hands, and it requires maintenance and upkeep throughout its life.</p>
<p>Unlike <em>all</em> other machines, it is pure concept, and as such it is infinitely malleable and mutable. And mutate it does, all the time.</p>
<p>Sometimes, "finished" software emerges, only needing minor fixes and patches, but remaining the same in purpose, interface, and behaviour. Many Unix tools fall in this category. Some projects like ZeroMQ make it their explicit goal. Many Clojure programmers value such "finished-ness". Such scattered examples exist.</p>
<p>Most software does not have this luxury. Most software must change indefinitely because the world it must serve changes indefinitely. The Emacs editor is a software product that has evolved non-stop for <em>nearly half a century</em> since it emerged in 1976, and it continues to thrive. This post was written in Emacs.</p>
<p>There is a strong reinforcing feedback loop too. Software changes the world fast, forcing software to change faster. The current reincarnation of Machine Learning and AI can be viewed as an expression of this process. We're basically saying it's all accelerating so much that it is getting humanly <em>impossible</em> to write and revise software fast enough, to out-OODA the pace of change. So we must instead find algorithms that sense the world and then dynamically generate or revise other algorithms to achieve system objectives (viz. alter the world further in our favour).</p>
<p>We have to wonder, how do we make sure our product continues to thrive and succeed under such unrelenting pressure of constant and sometimes violent change? And who's neck should be on the line for it?</p>
<h2 id="whom-to-hold-responsible-for-software-quality-assurance">Whom to hold responsible for Software Quality Assurance?</h2>
<p>The Usual Suspects?</p>
<ul>
<li>Those "Quality Assurance" boffins? Developers? UX people? DevOps?</li>
</ul>
<p>The Less Usual Suspects?</p>
<ul>
<li><p>Product managers? Analysts? Customer success? Sales? Marketing?</p></li>
<li><p>The CEO?</p></li>
<li><p>The AI?</p></li>
</ul>
<p>Consider the scenarios below. All of them directly impact customers, making them think "bad quality". Consider who is responsible for the underlying problem (or more likely, problems)?</p>
<ul>
<li>Your app framework is extremely performant and glitch-free. Your app bombs.</li>
<li>A feature does exactly what it promises, but people fail to use it right.</li>
<li>Your company committed half of itself to ship a second product in record time, but customers never really wanted it.</li>
<li>A huge update was pushed out on a do-or-die basis. Naturally it misbehaves, can't be rolled back, costs 5x as much to get right as it took to ship, and the rework effectively adds months to your plan of world domination.</li>
<li>Your service fails to scale. You discover there were no benchmarks.</li>
<li>A deployment breaks production. You discover a bad configuration.</li>
<li>A feature leaks data to unintended users and breaks SLAs / regulations. Your CEO releases a statement blaming a DevOps engineer.</li>
<li>A several-hour glitch goes un-monitored, causing serious widespread data corruption.</li>
<li>Your production noticeably degrades often. A large sea mammal is your mascot.</li>
<li>Your production seldom degrades, but when it does, it takes down half the known Internet along with it.</li>
<li>and on and on…</li>
</ul>
<p>In a quiet moment of honest self-reflection, you may confess to the mirror that the thousand cuts metaphor applies. That any of the above scenarios were likely the product of corner-cuts, often near-invisible to the naked eye in the moment. Corner-cuts that added up—nay, <em>compounded</em>—over time; slowly as band-aids, then as stitches and casts, and then suddenly as gangrene. And maybe the whole thing died of those cuts, or continued as a barely alive entity until someone had the heart to pull the plug (or offer a bail out).</p>
<p>You may even confess that maybe, just <em>maybe</em>, the job of assuring the goodness of a product belongs to <em>every function involved in the product's life</em>.</p>
<h2 id="why">Why?</h2>
<p>Suppose we model a traditional software production workflow, i.e. Analysis -&gt; Product requirements -&gt; UX/Design -&gt; Development -&gt; "QA" -&gt; Production.</p>
<p>Such a strictly linear model is common in the software industry at large. This is what it translates to in terms of time, complexity, costs, and risks.</p>
<pre><code>                                                              ^   Feedback
 Analysis -&gt; Product -&gt; UX/Design -&gt; Dev -&gt; "QA" -&gt;  Prod --./--&gt; arrives
                                                           /      too late
                                                         /-
                                                       /-
                                                     /- ^
                                                  /--   | Price of fixing
                                               /--      | errors and
                                           /---   ^     | corner cuts.
                                       /---       |     |
                                  /----   ^       |     | ~ AND/OR ~
                             /----        |       |     | Compounding of
                     /------   ^          |       |     | software debt.
            /--------          |          |       |     |
  ----------      ^            |          |       |     | ~ AND/OR ~
   ^              |            |          |       |     | Increasing odds
   |              |            |          |       |     | of being wrong.
---+--------------+------------+----------+-------+-----+----------------&gt;
                            Time, Complexity, Sunk costs
</code></pre>
<p>Visualising a linear workflow this way suggests some things:</p>
<ul>
<li>All the risk is actually front-loaded at the Analysis stage. If that is wrong, then everything is wrong.</li>
<li>The workflow looks linear, but has a compounding growth debt/risk profile.</li>
<li>By tasking a single group with "assuring" product quality, we maximize our odds of being too wrong too late, as well as of entirely failing to spot bad news.</li>
</ul>
<p>What's not obvious from the picture is that the risk is rooted in <em>feedback delays</em>. Weak signals die when the deliver pressure is high.</p>
<p>Our death-by-cuts risk profile will look the same, if the workflow is strictly linear as depicted above. It doesn't matter if we do it slowly in big batches over months, or faster as smaller batches over days. Small linearised batches may even worsen the aggregate risk profile, such as when market feedback loops are delayed or discontinuous. The smaller the batch, the more likely it is that feedback about several batches ago gets to us now. Such delayed feedback tends to severely disrupt strictly linear flows.</p>
<p>The above picture is also incomplete. For the full story, we need to talk deeply about systems (a longer conversation, for another day). We can make a small start by doing scenarios. Consider points on a product spectrum, ways to destroy/create quality, and what might help us go from worse to better?</p>
<h2 id="is-it-different-for-different-kinds-of-products">Is it different for different kinds of products?</h2>
<p>Suppose we contrast two typical ends of the product spectrum defined by primary customer. Which one risks death by a thousand cuts?</p>
<table>
<thead>
<tr>
<th>Trait</th>
<th>Enterprise Product</th>
<th>Consumer Product</th>
</tr>
</thead>
<tbody>
<tr>
<td>Key growth metric</td>
<td>Revenue Growth</td>
<td>User Growth</td>
</tr>
<tr>
<td>Key sales driver</td>
<td>Referrals + executive credibility</td>
<td>Referrals + Friends-and-family experiences</td>
</tr>
<tr>
<td>Customer risk</td>
<td>High risk/reward per account</td>
<td>Tiny unit economics per account</td>
</tr>
<tr>
<td>Contract risk</td>
<td>SLAs with crippling penalties</td>
<td>1 EULA / ToS that users don't read</td>
</tr>
<tr>
<td>etc …</td>
<td>…</td>
<td>…</td>
</tr>
</tbody>
</table>
<p>Well, here's the thing. Not only does all software mutate, we <em>also</em> end up performing all kinds of deep surgery on the <em>organisation</em> that produces it. The whole thing—product and org—is <em>simultaneously</em> flexed, reconfigured, and even totally redesigned in-place with rapidity that is very uncommon in other industries. Why? Because software fundamentally is peoples' thoughts being played on repeat.</p>
<p>So however we break it down, the common theme is this. Every hotfix is a cut. Every complaint is a cut. Every app crash is a cut. Every service outage is a cut. And so on. Each cut heals slowly and destroys Quality and value(ation).</p>
<h2 id="how-to-destroy-quality">How to destroy Quality?</h2>
<p>It's useful to come up with ways to destroy quality, so that we may contrast those with ways to generate quality. I've seen and heard all of the following in work life so far (hopefully without actively perpetrating them, but memory is a fickle beast).</p>
<ul>
<li>Misconstrue and mislabel Software Testing as Quality Assurance. Testing is <em>not</em> "Quality Assurance".</li>
<li>Ostensibly make all teams responsible for their "QA", which really means make the least experienced people do it day-to-day.</li>
<li>Create a culture where it's normal to say things like this):
<ul>
<li>"Hey I'm adding this to the sprint. It's a small thing, so let's not slip our deadline."</li>
<li>"Testing is boring."</li>
<li>"We'll fix it if customers complain."</li>
<li>"Who the f*#$ wrote this code?"</li>
<li>"Ah yes, those are known flaky tests. Just re-trigger the build."</li>
<li>"You don't know your job. Ship this." (This one stung. I'll tell you over beer/coffee :).</li>
</ul></li>
<li>Ensure designers, developers, and testers work on tasks and priorities set by others.</li>
<li>Ensure someone catches the blame for mistakes.</li>
<li>Set up incentives to make departments compete with each other.</li>
<li>Hire a Vogon or a Darth Vader CEO.</li>
<li>Further <a href="https://danluu.com/wat/">normalise all kinds of deviance</a>.</li>
</ul>
<p>This was just a shortlist of things I recalled while writing this post. Think up as many ways as you can. #protip for inspiration: read CIA's now-declassified <a href="https://www.gutenberg.org/files/26184/page-images/26184-images.pdf">Simple Sabotage Field Manual</a>. Pay special attention to part 11: <em>General Interference with Organisations and Production</em>.</p>
<h2 id="how-to-create-quality">How to create Quality?</h2>
<p>One clue is to <em>not</em> do quality-destroying things. Another is to do the <em>inverse</em> of quality-destroying things (e.g. share know-how instead of hoarding it.) A third is to notice whether high-quality product producing organisations have any common traits (they do). Most important, perhaps, is to understand that there is no formula for how to acquire those traits.</p>
<p>To design and build high quality software products, it is imperative to design and build high quality organisation-wide systems and culture. We have many tools, frameworks, fundamental ideas at our disposal. But no "best practices" process or methodology or "one weird trick" style intervention can fix broken systems and broken people.</p>
<p>The "way" has to be co-evolved:</p>
<ul>
<li>by collaborative stakeholders,</li>
<li>spread across the org,</li>
<li>appropriate to the org's unique context,</li>
<li>along with customers, partners, and the immediate ecosystem.</li>
</ul>
<p>This is universally a very difficult process, with challenges surprisingly similar to what it takes to recover fitness after a year of slacking off. It requires mindset, leadership, and persistent holistic intelligent <em>eval/apply</em> behaviour. And all of that derives from <em>perspective</em>.</p>
<blockquote>
<p>"<em>Perspective is worth 80 IQ points.</em>"</p>
<p>— Alan Kay</p>
</blockquote>
<p>So, if we are to chart a course from Worse Quality to Better Quality, then it must be our first duty to purposely get really uncomfortable by seeking out new-to-us, diverse, status-quo-challenging perspective. And …</p>
<h2 id="the-first-skill-is-to-learn-to-suffer-constructively.">The first skill is to learn to suffer constructively.</h2>
<p>We suffer, you and I.</p>
<p>It is inevitable. Yet, it is also why life flourishes. <em>"Why are we suffering?"</em> is a great discussion to have, because constructive suffering yields quality outcomes.</p>
<p>OK, back to the real world…</p>
<p>The path to recovering a <em>previous</em> fitness peak after a year of slacking off is filled with sore muscles, cursing at the alarm clock, far too many days of being a generally irritable snappy person, and a constant mental battle against mainlining deliciously easy instant gratification. It gets harder before it gets easier. Then we reach the top of the previous S-curve. And we must begin the cycle again, to climb the next one.</p>
<p>We are very fortunate.</p>
<p>Fellow sufferers have been fostering quality-generative conversation and change all around us. We have access to a growing body of top-notch industry research <em>and</em> experience reports. Without exaggerating, very many of these lessons have been paid for in tears, blood, lives. Let's augment our intuitions with these power tools. Those hard-won <em>80 extra IQ points</em> are ours for the taking.</p>
<p>Some selected resources.</p>
<p>Many inputs have shaped my thinking about Quality (well, all the things, because everything is connected); people, events, books, lectures etc. If you're wondering where to go. These are not prescriptions, but a sort of sampling platter. Triggers for your own searches. Please send me more!</p>
<p>Systems:</p>
<ul>
<li><a href="https://www.chelseagreen.com/product/thinking-in-systems/">Thinking in Systems</a> is a great primer.</li>
</ul>
<p>Software complexity:</p>
<ul>
<li><a href="http://shaffner.us/cs/papers/tarpit.pdf">Out of The Tar Pit</a></li>
<li><a href="https://www.cgl.ucsf.edu/Outreach/pc204/NoSilverBullet.html">No Silver Bullet</a></li>
<li><a href="http://ecoplexity.org/files/uploads/Simon.pdf">The Architecture of Complexity</a></li>
<li><a href="https://www.youtube.com/watch?v=LKtk3HCgTa8&amp;list=PLZdCLR02grLrEwKaZv-5QbUzK0zGKOOcr&amp;index=18">Simple made Easy</a></li>
</ul>
<p>Failure:</p>
<ul>
<li><a href="https://www.researchgate.net/publication/228797158_How_complex_systems_fail">How Complex Systems Fail</a></li>
<li><a href="https://www.amazon.in/Human-Error-James-Reason/dp/0521314194/">Human Error</a></li>
<li><a href="https://safetydifferently.com/">Safety Differently</a></li>
</ul>
<p>Doing Together:</p>
<ul>
<li><a href="https://www.goodreads.com/book/show/13629.The_Mythical_Man_Month">The Mythical Man Month</a></li>
<li><a href="https://www.goodreads.com/book/show/6667514-the-checklist-manifesto">The Checklist Manifesto</a></li>
<li>Critical Chain Project Management (<a href="https://www.goodreads.com/book/show/113934.The_Goal">The Goal</a> is a fine place to start, if you have no idea what CCPM is.)</li>
<li><a href="https://www.goodreads.com/book/show/1501427.Managing_The_Design_Factory">Managing the Design Factory</a></li>
<li><a href="https://www.goodreads.com/book/show/6278270-the-principles-of-product-development-flow">The Principles of Product Development Flow</a></li>
<li><a href="http://www.kitchensoap.com/2012/10/25/on-being-a-senior-engineer/">Mature Optimization</a></li>
<li><a href="https://www.kaner.com/pdfs/GoodTest.pdf">What is a Good Test Case?</a></li>
</ul>
<p>Oneself (heavily biased, because I identify as a software programmer):</p>
<ul>
<li><a href="http://www.kitchensoap.com/2012/10/25/on-being-a-senior-engineer/">On Being a Senior Engineer</a></li>
<li><a href="http://www.arl.wustl.edu/projects/fpx/research/HowToBeAProgrammer.pdf">How to Be A Programmer</a></li>
<li><a href="https://www.goodreads.com/book/show/213233.Better">Better: A Surgeon's Notes on Performance</a></li>
<li><a href="https://www.youtube.com/watch?v=f84n5oFoZBc&amp;list=PLZdCLR02grLrEwKaZv-5QbUzK0zGKOOcr&amp;index=9">Hammock Driven Development</a></li>
</ul>
<p>"Practical philosophy", for lack of better words:</p>
<ul>
<li><a href="https://www.youtube.com/watch?v=QCwqnjxqfmY&amp;list=PLZdCLR02grLrEwKaZv-5QbUzK0zGKOOcr&amp;index=19">Design, Composition, Performance</a></li>
<li><a href="https://www.goodreads.com/book/show/324750.High_Output_Management">High Output Management</a></li>
<li><a href="https://www.semanticscholar.org/paper/Destruction-and-Creation-Boyd/483359fa9420efcddde5a17da597f462c2a788c2">Destruction and Creation</a></li>
<li>Stuff from <em><a href="https://www.theschooloflife.com/">The School of Life</a></em> (corny name, yes, but give it a chance :)</li>
</ul>
<p>I recently discovered Gene Kim's podcast, <a href="https://itrevolution.com/the-idealcast-podcast/">The Idealcast</a>. Gene is gathering fantastic people and resources in one place. Definitely have a look-see.</p>
<h2 id="caveats-mea-culpa-etc.">Caveats, mea culpa, etc.</h2>
<p>I am very much a work-in-progress, and this post is my current intuition.</p>
<p>The post is heavily coloured by many witting and uwitting eval/apply loops comprised of personal failures, ignorant mistakes, and occasional wins, over the last about 20 years of professional life. And well, life life. It is also informed by the good fortune of having learned by working with people who understand the world far better than I do. And obviously a lot of reading, thinking, talking, frequently "in anger" after having hit walls and obstacles.</p>
<p>So please take what is useful, and discard the rest.</p>
<p>May the source be with you _\\//</p>
  </section>
  
</article>
  </main>
      
      <!-- Cloudflare Web Analytics -->
      
      <!-- End Cloudflare Web Analytics -->
    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[A modest update to Qubes OS (131 pts)]]></title>
            <link>https://lwn.net/SubscriberLink/962787/35f1ff3af9031437/</link>
            <guid>39490264</guid>
            <pubDate>Sat, 24 Feb 2024 09:13:42 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://lwn.net/SubscriberLink/962787/35f1ff3af9031437/">https://lwn.net/SubscriberLink/962787/35f1ff3af9031437/</a>, See on <a href="https://news.ycombinator.com/item?id=39490264">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
<!-- $Id: slink-trial,v 1.1 2005-11-04 21:27:01 corbet Exp $ -->
<center>
<table>
<tbody><tr><td>
<h3>Welcome to LWN.net</h3>
<p>
The following subscription-only content has been made available to you 
by an LWN subscriber.  Thousands of subscribers depend on LWN for the 
best news from the Linux and free software communities.  If you enjoy this 
article, please consider accepting the trial offer on the right.  Thank you
for visiting LWN.net!
</p></td><td>
<div>
<h3>Free trial subscription</h3>
           <p>
           Try LWN for free for 1 month: no payment
           or credit card required.  <a href="https://lwn.net/Promo/slink-trial2-3/claim">Activate
           your trial subscription now</a> and see why thousands of
           readers subscribe to LWN.net.
           
</p></div>
</td>
</tr>

</tbody></table>
</center>

<p><a href="https://www.qubes-os.org/">Qubes OS</a> is a security-focused desktop Linux distribution built on Fedora Linux and the <a href="https://www.qubes-os.org/faq/#why-does-qubes-use-xen-instead-of-kvm-or-some-other-hypervisor">Xen hypervisor</a>. Qubes uses virtualization to run applications, system services, and devices access via virtual machines called "<a href="https://www.qubes-os.org/doc/glossary/#qube">qubes</a>" that have varying levels of trust and <a href="https://www.qubes-os.org/doc/how-to-use-disposables/">persistence</a> to provide an open-source "<q>reasonably secure</q>" operating system with "<q>serious privacy</q>". The Qubes 4.2.0 release, from <a href="https://www.qubes-os.org/doc/releases/4.2/release-notes/">December 2023</a>, brings a number of refinements to make Qubes OS easier to manage and use.</p>

<h4>A quick overview</h4>

<p>Qubes OS is designed to be a single-user desktop operating system that provides strong security out of the box through isolation between applications and services, rather than trying to ensure that the applications or services are secure in and of themselves. The vision for Qubes is laid out in the Qubes OS <a href="https://www.qubes-os.org/attachment/doc/arch-spec-0.3.pdf">architecture document</a> written in 2010. While that specification isn't fully implemented yet, each release brings Qubes a bit closer to the ideal.</p>

<p>As currently implemented, Qubes uses the Xen hypervisor to run a Fedora-based <a href="https://www.qubes-os.org/doc/glossary/#admin-qube">admin qube</a> (<a href="https://www.qubes-os.org/doc/glossary/#dom0">dom0</a>) with direct hardware access that provides administration and orchestration of unprivileged guest domains (<a href="https://www.qubes-os.org/doc/glossary/#domu">domU</a>) based on <a href="https://www.qubes-os.org/doc/templates/">templates</a> (VM data stored as LVM volumes) that are used to run applications (<a href="https://www.qubes-os.org/doc/glossary/#app-qube">app qubes</a>) or provide services (<a href="https://www.qubes-os.org/doc/glossary/#service-qube">service qubes</a>) like networking, USB access, and more to the app qubes. For example, networking and firewall services are each provided by separate system qubes ("sys-net" and "sys-firewall", respectively), and access to USB devices is through "sys-usb". Note that the Qubes website and documentation tend to use the term "VM" and "qube" interchangeably.</p>

<p>Templates are the starting point for app and system qubes—app qubes take their root file system (that is, programs and system files) from templates. Any software that users want to persist in an app qube should be installed in a template, rather than an app qube, otherwise it will be discarded when the app qube restarts. If a user wants Emacs or LibreOffice, the Qubes way is to install it into one of the templates and then spin up an app qube based on that template to use the application.</p>

<p>Each qube has a level of trust somewhere between "unsafe and untrusted" to "safe and ultimately trusted". The admin qube, for example, is considered safe and ultimately trusted. The sys-net and sys-usb qubes are considered untrusted, and the firewall qube is considered moderately trusted. Qubes OS ties all of that together and presents the user with a coherent desktop experience. To the user, it is meant to feel like using a regular desktop environment and applications, rather than using half-dozen or more VMs that are unaware of one another. Qube windows are displayed with <a href="https://www.qubes-os.org/doc/getting-started/#color--security">colored borders</a>, to give users visual cues about which qube is running the application and its safety level.</p>

<blockquote>
<a href="https://lwn.net/Articles/962902/"><img src="https://static.lwn.net/images/2024/qubes-desktop-sm.png" alt="[Qubes OS desktop]" title="Qubes OS desktop"></a>
</blockquote>

<p>LWN last looked at Qubes ahead of the <a href="https://lwn.net/Articles/873255/">4.1.0 release</a> in October 2021. That release made major overhauls to the Qubes architecture, splitting out display handling to its <a href="https://www.qubes-os.org/doc/gui-domain/">own domain</a> and making changes to the <a href="https://www.qubes-os.org/news/2020/06/22/new-qrexec-policy-system/">Qrexec</a> policy system. This release follows up those changes with a number of more user-visible changes such as rewrites of several Qubes GUI management tools, simpler <a href="https://www.qubes-os.org/doc/split-gpg/">split GPG</a> management (which lets users store private GPG keys in a trusted qube and make use of them in less trusted qubes), changes to default Fedora and Debian templates, and more.</p>

<p>Qubes's approach to security means a more complex, and <a href="https://lwn.net/Articles/764048/">sometimes cumbersome</a>, user experience. Moving from a Linux distribution like Fedora or Debian to Qubes OS will take more adjustment than one might expect. For example, installing software on a Fedora desktop is usually as simple as "<tt>dnf&nbsp;install&nbsp;package</tt>". But installing software to use within a Fedora-based qube requires <a href="https://www.qubes-os.org/doc/how-to-install-software/">several additional steps</a> on Qubes OS, plus restarting VMs. Other activities, such as configuring a Bluetooth input or audio device is much more complicated and not well-documented. Then again, it's also not encouraged—Bluetooth isn't <a href="https://forum.qubes-os.org/t/wireless-bluetooth-support/5943">considered secure</a>, so why focus on making it easier to configure? But when it comes to using Qubes OS as intended, this release includes some major work to add polish and improve the user experience.</p>

<h4>GUI application improvements</h4>

<p>One of the first improvements users will notice is the redesigned <a href="https://www.qubes-os.org/news/2021/11/12/new-qubes-application-menu/">application menu</a>, first made available as a preview in Qubes 4.1, and now the default. On a "normal" Linux distribution, the menu of applications generally only has to display one version of Firefox, one terminal, one file manager, and so forth. Qubes, however, helps users work more securely by compartmentalizing applications to qubes by task or profile. How users <a href="https://www.qubes-os.org/doc/how-to-organize-your-qubes/">organize</a> their work is up to them, but Qubes offers "work", "personal", and "untrusted" qubes by default—each qube with its own installation of Firefox, terminal, and file manager. (These are color coded when running, so users might see a yellow border for personal applications, a blue border for work, and red for untrusted.)</p>

<p>The Qubes model of separating activities into isolated compartments is good for security—users can visit untrusted sites in the untrusted qube, restrict banking to another qube, and separate work in yet another qube—but more challenging to present in a user-friendly fashion. Prior versions of Qubes had a <a href="https://forum.qubes-os.org/t/guide-xfce-global-dark-mode-in-qubes-4-0-4-1/10757">single-menu layout</a> that was unwieldy as the number of applications, templates, and services grew. The current application menu organizes application qubes, template qubes, and service qubes separately, and breaks out Qubes tools like the global configuration and policy editor into their own menu. The effect is still busy compared to a "regular" desktop distribution, but it does seem a marked improvement over the old menu. The ability to add applications from various qubes to a Favorites menu is a great improvement, though there is no obvious way to configure the application menu to display favorites immediately when first opened. Perhaps this will show up in the next Qubes release—if it does, it will probably appear in the Qubes global configuration application.</p>

<p>The global configuration application in 4.2.0 represents work that the project started discussing in September 2021. In <a href="https://github.com/QubesOS/qubes-issues/issues/6898">the ticket</a> discussing the design, Nina Eleanor Alter described target demographics for the global UI as non-technical, high-risk users, and technical users "<q>excited about Qubes but lacking the attention span or time to copiously read whitepapers or the docs</q>". Alter said that Linux users may be comfortable with multiple applets to configure system behaviors but, "<q>it delivers a poor execution and discovery experience to all users</q>"; and users coming from Windows or macOS expect a single settings UI.</p>

<p>The idea is to make Qubes more discoverable, and the new UI does this by bringing together settings for file access, clipboard handling, updates, USB devices, URL handling, miscellaneous general settings, and device information. Users have a single GUI for working with system-wide settings that were not particularly discoverable in prior versions, such as setting up split GPG.</p>

<p>The Create New Qube application has been updated too, though Qubes 4.2.0 seems to have shipped with the old and new applications with different labels in the Applications Menu. The new application is titled "Create New Qube" and the old application is listed as "Create Qubes VM", though both show "Create New Qube" in the title bar when running.</p>

<blockquote>
<a href="https://lwn.net/Articles/962902#create"><img src="https://static.lwn.net/images/2024/qubes-create-new-side-sm.png" alt="[Create New Qubes applications]" title="Create New Qubes applications"></a>
</blockquote>

<p>As shown in the screenshot, the new and improved version provides access to more options and settings, as well as some guidance provided via tooltips. (One note on tooltips in Qubes—while working in Qubes, tooltips displayed in various applications lingered long after moving the mouse, switching windows, or even navigating to another workspace.) The current iteration of the Create New Qube application does seem more intuitive than the old, and provides the ability choose the default applications available, set initial RAM for the qube, and more.</p>

<p>The Qubes Update application (appropriately) received an update in this release as well. Qubes includes Fedora, Debian, and Whonix templates as part of the default installation and provides access to many others. Over time it would be trivial to have half-a-dozen template OSes that need regular updates. The Update application streamlines this by checking in the background for updates and then notifying of updates for running qubes at regular intervals. It will also attempt to perform updates every seven days for templates that are not used in that timeframe, though this interval is configurable, or users can update them manually. After updates have been staged, the updater will offer to restart qubes based on the updated templates. Qubes that have running applications will not be targeted for restart by default, so users can run updates without fear that Qubes will unceremoniously shut down their work.</p>

<h4>Template updates</h4>

<p>Another interesting change with this release the <a href="https://github.com/QubesOS/qubes-issues/issues/7784">use of Xfce</a> editions for Fedora and Debian instead of GNOME to <a href="https://github.com/QubesOS/qubes-issues/issues/7028">reduce memory usage</a> and provide a better selection of default applications. Marek Marczykowski-Górecki said that Fedora's GNOME template has too many "<q>problematic</q>" packages that "<q>either conflict with something or simply don't work with our GUI agent</q>". The project had been looking for ways to slim memory usage in Fedora qubes for some time, with a number of GNOME packages targeted for exclusion, including <a href="https://github.com/QubesOS/qubes-issues/issues/8403">GNOME Tracker</a>. Note that the Qubes OS default desktop has been Xfce <a href="https://www.qubes-os.org/doc/releases/3.2/release-notes/">since</a> the <a href="https://lwn.net/Articles/705827/">3.2 release</a> in September 2016.</p>

<p>Support for SELinux in Fedora templates has been a long time in coming. The <a href="https://github.com/QubesOS/qubes-issues/issues/4239">issue</a> tracking the work was opened in 2018, while the <a href="https://github.com/QubesOS/qubes-issues/issues/4239#issuecomment-1419947028">work finally landed</a> in February 2023 and then made its way into the 4.2.0 release. One might wonder why exactly users might need or want SELinux in Fedora qubes, given that Qubes OS is meant to be a single-user system. Each qube is already isolated from others and and the user has full run of each qube. Templates, for example, allow <a href="https://www.qubes-os.org/doc/vm-sudo/">sudo with no password</a> because all of the user data in a running qube is available to the same person anyway, so there's little sense in forcing them to type a password every time they use <tt>sudo</tt>. Even though Qubes does little to restrict user privileges within each qube, Marczykowski-Górecki <a href="https://github.com/QubesOS/qubes-issues/issues/4239#issuecomment-1712660098">noted</a> that the addition of SELinux is useful for applications that provide sandboxing inside a Fedora template, like Podman or bubblewrap, and also help provide extra hardening when using <tt>qvm-copy</tt> to send files between qubes.</p>

<h4>A modest update</h4>

<p>Overall, 4.2.0 is a somewhat modest update in terms of new features—though it does contain plenty of the usual version updates and <a href="https://github.com/QubesOS/qubes-issues/issues?q=is%3Aissue+is%3Aclosed+reason%3Acompleted+milestone%3A%22Release+4.2%22+-label%3A%22R%3A+cannot+reproduce%22+-label%3A%22R%3A+declined%22+-label%3A%22R%3A+duplicate%22+-label%3A%22R%3A+not+applicable%22+-label%3A%22R%3A+self-closed%22+-label%3A%22R%3A+upstream+issue%22+">bug fixes</a>. But the focus on improving Qubes OS usability is important. While popular Linux distributions like Fedora or Ubuntu count users in the millions, the Qubes project <a href="https://www.qubes-os.org/statistics/">counts</a> its users in the tens of thousands. Surely more users need what Qubes has to offer, but security tools that are too hard to use tend not to be used. Bolstering Qubes usability is just as important as striving toward implementing the Qubes architecture specification.</p><br clear="all">
               <br clear="all">
               <blockquote>
<p>
<b>Did you like this article?</b>  Please accept our 
<a href="https://lwn.net/Promo/slink-trial2-3/claim">trial subscription offer</a> to be
able to see more content like it and to participate in the discussion.
</p>
</blockquote>
<hr><p>
           (<a href="https://lwn.net/Login/?target=/Articles/962787/">Log in</a> to post comments)
           </p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Power Metal: is it really about dragons? (2018) (272 pts)]]></title>
            <link>https://notes.atomutek.org/power-metal-and-dragons.html</link>
            <guid>39489920</guid>
            <pubDate>Sat, 24 Feb 2024 07:47:15 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://notes.atomutek.org/power-metal-and-dragons.html">https://notes.atomutek.org/power-metal-and-dragons.html</a>, See on <a href="https://news.ycombinator.com/item?id=39489920">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
    <!-- summary: One day a friend described Power Metal as the love child of opera and metal singing about dragons. I decided to check if it was really about dragons. -->

<p>Some years ago, Matt Daniels <a href="https://pudding.cool/2017/02/vocabulary/index.html">[1]</a> wrote a great article on who has the largest vocabulary in hip-hop. I thought it was brilliant: comparing Shakespeare to famous hip-hop artists. It got me thinking and gave me the inspiration of doing something similar with a genre I love: Power Metal. People know metal or heavy metal but not necessarily power metal. For me, power metal has speed (remember that last song on Guitar Hero), clean vocals (you can understand the words, looking at you grind metal) and lyrics about dragons. Here I will just confirm or infirm my assumption about dragons.</p>
<p>I used the lyrics of 58 bands, which should fall under the label of Power Metal, at least according to <a href="https://reddit.com/r/powermetal/">/r/powermetal</a> <a href="https://www.reddit.com/r/PowerMetal/wiki/essential">[2]</a> and myself. This article will dive into the vocabulary of the power metal bands, what makes a song power metal or not and who wrote the most positive and negative power metal songs ever (at least from my dataset).</p>
<h2>TL;DR</h2>
<ul>
<li>Running Wild (2949 unique words) has the biggest vocabulary, followed by Helloween (2641) and  Elvenking (2505),</li>
<li>Bands from Spain, Germany and Finland have an average of more than 1600 words vocabulary; in comparison native countries like UK, US and Scotland have an average of 925, 1383 and 1501 words respectively,</li>
<li>The most metal words are <strong>deliverance, defender, honour, forevermore, realm</strong> and the least are <strong>shit, baby, fuck, girl, verse</strong>,</li>
<li>The most negative song is Condemned To Hell by Gamma Ray and the most positive There's Something In The Skies by Dark Moor,</li>
<li>Source code: to be released soon,</li>
<li>Dataset: not available but you can find lyrics all over the internet ;)</li>
</ul>
<h2>The dataset</h2>
<p>The dataset includes 58 bands: Alestorm, Angra, At Vance, Avantasia, Blind Guardian, Borealis, Cain’s Offering, Concerto Moon, Dark Moor, Demons &amp; Wizards, Dragonforce, Dragonland, Dream Evil, Edguy, Elegy, Elvenking, Fairyland, Falconer, Firewind, Freedom Call, Gamma Ray, Gloryhammer, Grave Digger, Hammerfall, Heavenly, Helloween, Hibria, Highland Glory, Iron Savior, Judicator, Kamelot, Keldian, Labyrinth, Lost Horizon, Manowar, Masterplan, Nightwish, Nocturnal Rites, Orden Ogan, Pagan’s Mind, Pathfinder, Persuader, Power Quest, Powerwolf, Primal Fear, Rhapsody, Rhapsody Of Fire, Running Wild, Sabaton, Secret Sphere, Seventh Wonder, Sonata Arctica, Stratovarius, Theocracy, Twilight Force, Twilightning, Unisonic, Wisdom; and a total of 4808 songs in English.</p>
<h3>Distribution of bands by country</h3>
<table>
<thead>
<tr>
<th>Country</th>
<th>Number of bands</th>
</tr>
</thead>
<tbody>
<tr>
<td>Brazil</td>
<td>2</td>
</tr>
<tr>
<td>Canada</td>
<td>1</td>
</tr>
<tr>
<td>Finland</td>
<td>5</td>
</tr>
<tr>
<td>France</td>
<td>1</td>
</tr>
<tr>
<td>Germany</td>
<td>16</td>
</tr>
<tr>
<td>Greece</td>
<td>1</td>
</tr>
<tr>
<td>Hungary</td>
<td>1</td>
</tr>
<tr>
<td>Italy</td>
<td>5</td>
</tr>
<tr>
<td>Japan</td>
<td>1</td>
</tr>
<tr>
<td>Netherlands</td>
<td>1</td>
</tr>
<tr>
<td>Norway</td>
<td>3</td>
</tr>
<tr>
<td>Poland</td>
<td>1</td>
</tr>
<tr>
<td>Scotland</td>
<td>1</td>
</tr>
<tr>
<td>Spain</td>
<td>1</td>
</tr>
<tr>
<td>Sweden</td>
<td>10</td>
</tr>
<tr>
<td>UK</td>
<td>3</td>
</tr>
<tr>
<td>US</td>
<td>5</td>
</tr>
</tbody>
</table>
<p>Bands from Germany and Sweden are over-represented compared to other countries. This is due to the dataset creation, which was mostly based on the albums recommendation of <a href="https://reddit.com/r/powermetal/">/r/powermetal</a>. Therefore I won’t go deeper in regions difference.</p>
<h2>Text analysis of the lyrics</h2>
<h2>Vocabulary</h2>
<p>Calculating the vocabulary length could be done in several ways: counting the unique words (with inflections, meaning singular, plural nouns, conjugated verbs, etc.), counting the lemma form of the words (more or less the dictionary form) and finally the stemmed version which is when you get the stem of a word (we reduce a word to the its minimal non-changing parts). There is no right way to do the counting. All the methods have their pros and cons:</p>
<ul>
<li>stemming should be fast but the cutting might reduce to a stem common to two different words (ex: markets/marketing would give the stem market);</li>
<li>lemmatization requires knowing the part of speech (POS) which might slow your processing but would be more accurate.</li>
</ul>
<p>Since I preferred fast results over a possible better accuracy, I chose the stemmed version (Porter) of the words for the vocabulary processing. Based on this stemming transformation, I could plot the following chart describing the growth of vocabulary range along the years (from 1982 to 2018).</p>
<p>Play with the slider it's interactive!</p>


<p><strong>Largest vocabulary in PowerMetal in unique words</strong>* (in 2018)</p>
<ol>
<li>Running Wild (2949)</li>
<li>Helloween (2641)</li>
<li>Elvenking (2502)</li>
<li>Sonata Arctica (2467)</li>
<li>Edguy (2385)</li>
</ol>
<p>* <sub>at least in my dataset</sub></p>
<h2>What makes a power metal song power metal?</h2>
<p>We need to look at the importance of the words used in the song. Common metrics are <strong>tf</strong> (term frequency of a word), <strong>idf</strong> (inverse document frequency, how rare a word is in a document), <strong>tf-idf</strong> (term frequency-inverse document frequency, measuring how important a word is in a corpus)<a href="https://en.wikipedia.org/wiki/Tf%E2%80%93idf">[3]</a>.</p>
<p>Iain Barr chose the <strong>tf</strong> approach and measured the <em>Metalness</em> of a word / lyrics as follow:</p>
<p>$$ M_w = \log \frac{freq^{metal-corpus}_w}{freq^{corpus}_w} $$</p>
<p>where $M_w$ is the Metalness of a word, $freq^{metal-corpus}_w$ the frequency of the word $w$ in the metal corpus, $freq^{corpus}_w$ the frequency of the word $w$ in another corpus.</p>
<p>He looked at the distribution of a specific word in two corpus - his metal lyrics dataset and the Brown corpus, an ensemble of literary texts from the 60s <a href="https://www.degeneratestate.org/posts/2016/Apr/20/heavy-metal-and-natural-language-processing-part-1/">[4]</a>.</p>
<p>Nonetheless using a literary corpus might not be ideal since the type of text and choice of words between a novel and a lyric are quite different. I chose to use a dataset already prepared called Metrolyrics dataset, available on Kaggle <a href="https://www.kaggle.com/gyani95/380000-lyrics-from-metrolyrics">[5]</a>.</p>
<p>After a bit of cleaning and processing, I got the following results (interactive chart):</p>


<p>A power metal is not necessarily about <strong>dragons</strong> (in the 47th position in the Metalness ranking): it’s all about deliverance, honour, defender, realm with a forevermore touch. Looking at the top 50 words, all these words wouldn’t appear out of place in a heroic fantasy novel or a dungeons &amp; dragons handbook.</p>
<p>On the opposite, words like <em>shit, baby, fuck, girl, verse</em> are not that metal.</p>
<h3>PowerMetalness ranking</h3>
<p><strong>The most power metal songs</strong> 🤘🤘🤘🤘🤘</p>
<ol>
<li>Freedom Call    -    66 Warriors</li>
<li>Grave Digger   -      The Emperor’s Death</li>
<li>Manowar        -     Hail And Kill</li>
<li>Dream Evil     -        Made Of Metal</li>
<li>Primal Fear    -     Evil Spell</li>
</ol>
<p><strong>The least power metal songs</strong> 🤘</p>
<ol>
<li>Gamma Ray    -     Money</li>
<li>Helloween    -     Anything My Mama Don't Like</li>
<li>Iron Savior   -      Dance With Somebody (Mando Diao cover)</li>
<li>Highland Glory  -   Love Gun (KISS cover)</li>
<li>Masterplan     -    Black Dog (Led Zeppelin cover)</li>
</ol>
<p>As you notice, the least power metal songs are mostly covers of non power metal bands, which makes sense.</p>
<p>If we look also at what words are the most important for each band, you will be able to distinguish what makes these bands unique. Let’s have a look at some iconic bands such as Alestorm, Manowar, Rhapsody and Sabaton.</p>
<h3>Alestorm</h3>
<p><strong>Words</strong>: drink, sail, sea, pirate, quest<br>
For a band which is part of the pirate metal genre, these words are the perfect representation of pirate-related vocabulary.</p>
<h3>Manowar</h3>
<p><strong>Words</strong>: die, ride, metal, live, fight<br>
My knowledge of Manowar is a bit lacking but this iconic band seems to give importance to equally iconic words.</p>
<h3>Rhapsody</h3>
<p><strong>Words</strong>: holy, ancient, king, wind, land<br>
One of my favorite bands. Their lyrics are epic and the most important words for them represent that.</p>
<h3>Sabaton</h3>
<p><strong>Words</strong>: war, death, army, strike, way<br>
Sabaton writes a lot about real wars and this is reflected in their top words.</p>
<h2>Cluster bands by their lyrics</h2>
<p>Lyrics can be used to find similarities between bands. To do so, I’m using hierarchical clustering, a clustering technique where you don’t need to already know the number of clusters in your data in advance.</p>
<p><a href="https://notes.atomutek.org/img/band-clustering.png"><img alt="Band clustering" src="https://notes.atomutek.org/img/band-clustering.png"></a></p>
<p>Note that Rhapsody and Rhapsody of Fire are clustered together. For people who might not know, Rhapsody of Fire broke into in two bands, one keeping the original name and the other becoming Rhapsody. From a lyric point of view, as Luca Turilli wrote most if not all the songs for the bands, these two bands are basically the same which is confirmed in the clustering graph.</p>
<p>Out of this clustering emerge four clusters (interpretations my own, happy to hear yours):</p>
<ul>
<li><span><strong>Rhapsody-ish</strong></span>: uplifting epic and fantasy lyrics</li>
<li><span><strong>Edguy-ish</strong></span>: wide range of lyrics (?)</li>
<li><span><strong>Blind Guardian-ish</strong></span>: dark epic lyrics</li>
<li><span><strong>Manowar-ish</strong></span>: brutal, powerful lyrics</li>
</ul>
<p>Some bands are remotely affiliated with others:</p>
<ul>
<li><strong>Affiliated but independent</strong>: Gloryhammer with Rhapsody-ish, Alestorm with Manowar-ish</li>
<li><strong>The outsider</strong>: Concerto Moon (Japanese band)</li>
</ul>
<p>The cluster might be improved by looking at the topics of the lyrics and work from that.</p>
<h2>Sentiment analysis</h2>
<p>After having looked into the words, let’s feel them. I use VADER (Valence Aware Dictionary and sEntiment Reasoner) to analyze the sentiments in the lyrics. The datasets VADER was built upon include tweets, movie/amazon reviews and New York Times editorials. The authors claim that it’s “specifically attuned to sentiments expressed in social media” <a href="https://github.com/cjhutto/vaderSentiment">[6]</a> but it shouldn’t impact much my analysis.</p>
<h3>Positive sentiments</h3>
<p>Out of 58 bands, only 20 bands were considered overall as positive. I’m not surprised by this as the lyrics in power metal are about epic themes which are not per se the most joyous theme.</p>
<h4>Songs</h4>
<p><strong>The most positive songs in the dataset</strong></p>
<ol>
<li>Dark Moor   -      There's Something In The Skies</li>
<li>Sonata Arctica  -   I Have A Right</li>
<li>Sonata Arctica  -   Half A Marathon Man</li>
<li>Power Quest     -    Sacred Land</li>
<li>Freedom Call    -     A Perfect Day</li>
</ol>
<h4>Bands</h4>
<ol>
<li>Twilight Force</li>
<li>Freedom Call</li>
<li>Nightwish</li>
<li>Fairyland</li>
<li>Power Quest</li>
</ol>
<h3>Negative sentiments</h3>
<h4>Songs</h4>
<p><strong>The most negative songs in the dataset</strong></p>
<ul>
<li>Gamma Ray    -     Condemned To Hell</li>
<li>Dragonforce   -      War!</li>
<li>Sabaton     -    Burn In Hell</li>
<li>Edguy       -      Sacred Hell</li>
<li>Running Wild     -    Genocide</li>
</ul>
<p>Just looking at the titles of the songs are enough to say we are not talking about bunnies and sweet cakes.</p>
<h4>Bands</h4>
<ul>
<li>Sabaton</li>
<li>Judicator</li>
<li>Hibria</li>
<li>Dragonforce</li>
<li>Powerwolf</li>
</ul>
<h2>Conclusion</h2>
<p>Power Metal is not that into dragons (47th rank). Indeed Power Metal has a fantasy and epic style where of course dragons don't clash with the theme.
The project was fun to do and I discovered new bands along the way. There will be some updates on the analysis in the future, as I might add new bands and build on people’s feedbacks. I plan to release the source code I used for this analysis after some cleaning.</p>
<p>If you want me to add your favorite band(s), <a href="mailto:notes@atomutek.org?subject=[power%20metal]What%20about%20my%20favorite%20band?">send me an email</a> and I will try my best to add it/them!</p>
<p>Meanwhile keep 🤘!</p>
<h3>Acknowledgement</h3>
<ul>
<li><a href="https://jabalazs.github.io/">Jorge</a>, for the great comments and advice</li>
</ul>
<h2>Further reading</h2>
<ul>
<li><a href="https://helda.helsinki.fi/bitstream/handle/10138/136524/keywords.pdf">Jesse Taina - Keywords in heavy metal lyrics (2014)</a></li>
<li><a href="https://paulelvers.com/post/emotionsineuropeanmusic/">Sentiment analysis of musical taste: a cross-European comparison (2018)</a></li>
<li><a href="https://www.reddit.com/r/PowerMetal/comments/4dlw6v/">Dragonforce lyrical analysis (2016)</a></li>
<li><a href="https://www.reddit.com/r/PowerMetal/comments/4bxppd/">Most commonly used words in power metal lyrics (2016)</a></li>
</ul>
<h2>References</h2>
<ol>
<li><a href="https://pudding.cool/2017/02/vocabulary/index.html">https://pudding.cool/2017/02/vocabulary/index.html</a></li>
<li><a href="https://www.reddit.com/r/PowerMetal/wiki/essential">https://www.reddit.com/r/PowerMetal/wiki/essential</a></li>
<li><a href="https://en.wikipedia.org/wiki/Tf%E2%80%93idf">https://en.wikipedia.org/wiki/Tf-idf</a></li>
<li><a href="https://www.degeneratestate.org/posts/2016/Apr/20/heavy-metal-and-natural-language-processing-part-1/">https://www.degeneratestate.org/posts/2016/Apr/20/heavy-metal-and-natural-language-processing-part-1/</a></li>
<li><a href="https://www.kaggle.com/gyani95/380000-lyrics-from-metrolyrics">https://www.kaggle.com/gyani95/380000-lyrics-from-metrolyrics</a></li>
<li><a href="https://github.com/cjhutto/vaderSentiment">https://github.com/cjhutto/vaderSentiment</a></li>
</ol>
  </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Quality is a hard sell in big tech (161 pts)]]></title>
            <link>https://www.pcloadletter.dev/blog/big-tech-quality/</link>
            <guid>39489519</guid>
            <pubDate>Sat, 24 Feb 2024 06:09:46 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.pcloadletter.dev/blog/big-tech-quality/">https://www.pcloadletter.dev/blog/big-tech-quality/</a>, See on <a href="https://news.ycombinator.com/item?id=39489519">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="skip">
			


<ul>
	<li><time datetime="2024-02-23">23 February 2024</time></li>
</ul>

<p>I have noticed a trend in a handful of products I've worked on at big tech companies. I have friends at other big tech companies that have noticed a similar trend: The products are kind of crummy.</p>
<p>Here are some experiences that I have often encountered:</p>
<ul>
<li>the UI is flakey and/or unintuitive</li>
<li>there is a lot of cruft in the codebase that has never been cleaned up</li>
<li>bugs that have "acceptable" workarounds that never get fixed</li>
<li>packages/dependencies are badly out of date</li>
<li>the developer experience is crummy (bad build times, easily breakable processes)</li>
</ul>
<p>One of the reasons I have found for these issues is that we simply aren't investing enough time to increase product quality: we have poorly or nonexistent quality metrics, invest minimally in testing infrastructure (and actually writing tests), and don't invest in improving the inner loop. But why is this?</p>
<p>My experience has been that quality is simply a hard sell in bigh tech.</p>
<p>Let's first talk about something that's an <em>easy</em> sell right now: AI everything. Why is this an easy sell? Well, Microsoft could announce they put ChatGPT in a toaster and their stock price would jump $5/share. The sad truth is that big tech is hyper-focused on doing the things that make their stock prices go up in the short-term.</p>
<p>It's hard to make this connection with quality initiatives. If your software is slightly less shitty, the stock price won't jump next week. So instead of being able to sell the obvious benefit of shiny new features, you need to have an Engineering Manager willing to risk having lower <a href="https://www.pcloadletter.dev/blog/impact-based-performance-evaluation">impact</a> for the sake of having a better product. Even if there is broad consensus in your team, group, org that these quality improvements are necessary, there's a point up the corporate hierarchy where it simply doesn't matter to them. Certainly not as much as shipping some feature to great fanfare.</p>
<h2 id="part-of-a-bigger-strategy" tabindex="-1">Part of a bigger strategy? <a href="#part-of-a-bigger-strategy">#</a></h2>
<p>Cory Doctorow has <a href="https://pluralistic.net/2023/11/22/who-wins-the-argument/#corporations-are-people-my-friend">said some interesting things</a> about <em>enshittification</em> in big tech:</p>
<p><em>"enshittification is a three-stage process: first, surpluses are allocated to users until they are locked in. Then they are withdrawn and given to business-customers until they are locked in. Then all the value is harvested for the company's shareholders, leaving just enough residual value in the service to keep both end-users and business-customers glued to the platform."</em></p>
<p>At a macro level, it's possible this is the strategy: hook users initially, make them dependent on your product, and then cram in superficial features that make the stock go up but don't offer real value, and keep the customers simply because they really have no choice but to use your product (an enterprise Office 365 customer probably isn't switching anytime soon).</p>
<p>This does seem to have been a good strategy in the <em>short-term</em>: look at Microsoft's stock ever since they started cranking out AI everything. But how can the quality corner-cutting work long-term?</p>
<h2 id="i-hope-the-hubris-will-backfire" tabindex="-1">I hope the hubris will backfire <a href="#i-hope-the-hubris-will-backfire">#</a></h2>
<p>Something will have to give. Big tech products can't just keep getting shittier—can they? I'd like to think some smaller competitors will come eat their lunch, but I'm not sure. Hopefully we're not all too entrenched in the big tech ecosystem for this to happen.</p>

<ul><li>Previous: <a href="https://www.pcloadletter.dev/blog/coding-interviews/">Coding interviews are effective</a></li>
</ul>



		</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[The Earth is getting greener (146 pts)]]></title>
            <link>https://www.vox.com/down-to-earth/2024/2/7/24057308/earth-global-greening-climate-change-carbon</link>
            <guid>39489431</guid>
            <pubDate>Sat, 24 Feb 2024 05:48:55 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.vox.com/down-to-earth/2024/2/7/24057308/earth-global-greening-climate-change-carbon">https://www.vox.com/down-to-earth/2024/2/7/24057308/earth-global-greening-climate-change-carbon</a>, See on <a href="https://news.ycombinator.com/item?id=39489431">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
  <p id="vgbLBf">Maybe you’ve heard: Earth, our planet, is not doing great. Tropical forests are <a href="https://www.vox.com/down-to-earth/2022/10/12/23399105/biodiversity-loss-wwf-living-planet-index">getting cut down</a>. Parking lots are replacing <a href="https://www.vox.com/down-to-earth/22662490/grasslands-better-than-lawns-yard#:~:text=Covering%20about%2040%20percent%20of,in%20smoke%20during%20a%20wildfire.">bird-filled grasslands</a>. Climate change is fueling <a href="https://www.vox.com/climate/2023/6/8/23753980/canada-fires-smoke-climate-change-air-quality">forest-razing wildfires</a>. On the whole, natural, plant-filled habitats, seem to be <a href="https://www.vox.com/climate/23868423/florida-coral-reef-bleaching-heat-wave-climate-change">disappearing</a>.</p>
<p id="jKiTmF">Despite this destruction, scientists keep coming to an odd conclusion: The Earth is growing greener. Not green in the metaphorical “sustainable” sense, but in the literal color green. </p>
<p id="3zqXES">In the last four decades, the extent of green vegetation — i.e., the amount of leaves in a given area — has substantially increased across the planet, according to a <a href="https://www.sciencedirect.com/science/article/pii/S2351989423004262?dgcid=raven_sd_aip_email">number of recent scientific studies</a> based on satellite data. There’s actually more green space today, not less. And this “global greening” phenomenon is not just occurring on land. Large parts of the oceans are getting greener, too, <a href="https://www.nature.com/articles/s41586-023-06321-z">research shows</a>. Our blue planet, it seems, is increasingly a green planet. </p>
<p id="mwKoo9">Understanding Earth’s color is key to understanding Earth and our future on it. “Greenness” often corresponds to the planet’s ability to absorb carbon dioxide, the primary greenhouse gas that drives <a href="https://www.vox.com/climate" data-source="encore">climate change</a>. The more leaves, the more photosynthesis, a chemical reaction that gobbles up CO2. That’s the good news in global greening: It’s helping offset some of the impacts of climate change.  </p>
<p id="lIAa3x">But there’s more to greening than meets the eye. The changing color isn’t so much a sign that forests and other ecosystems are regrowing but that humans are altering the environment on a truly planetary scale — often, with dire consequences. </p>
  <figure>
  <span>
    
    <span data-original="https://cdn.vox-cdn.com/uploads/chorus_asset/file/25269947/modis_wonderglobe_lrg.jpeg">
      
        <picture data-cid="site/picture_element-1708800114_6348_64557" data-cdata="{&quot;asset_id&quot;:25269947,&quot;ratio&quot;:&quot;*&quot;}">
  


  <source srcset="https://cdn.vox-cdn.com/thumbor/l9ZoYdCLkoVxGHGJXz7j8fY0m5k=/0x0:4096x4096/320x0/filters:focal(0x0:4096x4096):format(webp):no_upscale()/cdn.vox-cdn.com/uploads/chorus_asset/file/25269947/modis_wonderglobe_lrg.jpeg 320w, https://cdn.vox-cdn.com/thumbor/ZvUIEX8EHEul46TXCQqaleHccMI=/0x0:4096x4096/520x0/filters:focal(0x0:4096x4096):format(webp):no_upscale()/cdn.vox-cdn.com/uploads/chorus_asset/file/25269947/modis_wonderglobe_lrg.jpeg 520w, https://cdn.vox-cdn.com/thumbor/P6_-EOjVLrewuv7qeSIazynxJ_U=/0x0:4096x4096/720x0/filters:focal(0x0:4096x4096):format(webp):no_upscale()/cdn.vox-cdn.com/uploads/chorus_asset/file/25269947/modis_wonderglobe_lrg.jpeg 720w, https://cdn.vox-cdn.com/thumbor/3kPY86TcRAXto57bm_fdahM5irE=/0x0:4096x4096/920x0/filters:focal(0x0:4096x4096):format(webp):no_upscale()/cdn.vox-cdn.com/uploads/chorus_asset/file/25269947/modis_wonderglobe_lrg.jpeg 920w, https://cdn.vox-cdn.com/thumbor/buwZgXMsHntTABu4hj4ncRiKKUw=/0x0:4096x4096/1120x0/filters:focal(0x0:4096x4096):format(webp):no_upscale()/cdn.vox-cdn.com/uploads/chorus_asset/file/25269947/modis_wonderglobe_lrg.jpeg 1120w, https://cdn.vox-cdn.com/thumbor/sVxZV5wcGMxEGILXtpWmNiGAUXk=/0x0:4096x4096/1320x0/filters:focal(0x0:4096x4096):format(webp):no_upscale()/cdn.vox-cdn.com/uploads/chorus_asset/file/25269947/modis_wonderglobe_lrg.jpeg 1320w, https://cdn.vox-cdn.com/thumbor/pmhlQjoz0FNO9eq0rGBjh2aYNeo=/0x0:4096x4096/1520x0/filters:focal(0x0:4096x4096):format(webp):no_upscale()/cdn.vox-cdn.com/uploads/chorus_asset/file/25269947/modis_wonderglobe_lrg.jpeg 1520w, https://cdn.vox-cdn.com/thumbor/A1_a7MplDo999clA65x_2moXq3U=/0x0:4096x4096/1720x0/filters:focal(0x0:4096x4096):format(webp):no_upscale()/cdn.vox-cdn.com/uploads/chorus_asset/file/25269947/modis_wonderglobe_lrg.jpeg 1720w, https://cdn.vox-cdn.com/thumbor/hI0jrDvjL0W_6y5rVPlHVG8uFYs=/0x0:4096x4096/1920x0/filters:focal(0x0:4096x4096):format(webp):no_upscale()/cdn.vox-cdn.com/uploads/chorus_asset/file/25269947/modis_wonderglobe_lrg.jpeg 1920w" sizes="(min-width: 1221px) 846px, (min-width: 880px) calc(100vw - 334px), 100vw" type="image/webp">


<img srcset="https://cdn.vox-cdn.com/thumbor/vs5YVt6zrWoDsZO0IS6DmU5apx0=/0x0:4096x4096/320x0/filters:focal(0x0:4096x4096):no_upscale()/cdn.vox-cdn.com/uploads/chorus_asset/file/25269947/modis_wonderglobe_lrg.jpeg 320w, https://cdn.vox-cdn.com/thumbor/CaAV1_CXSofTO60APXClCBxValc=/0x0:4096x4096/520x0/filters:focal(0x0:4096x4096):no_upscale()/cdn.vox-cdn.com/uploads/chorus_asset/file/25269947/modis_wonderglobe_lrg.jpeg 520w, https://cdn.vox-cdn.com/thumbor/S9dEE1dbwcF0cfU_Xn4d-J82Mng=/0x0:4096x4096/720x0/filters:focal(0x0:4096x4096):no_upscale()/cdn.vox-cdn.com/uploads/chorus_asset/file/25269947/modis_wonderglobe_lrg.jpeg 720w, https://cdn.vox-cdn.com/thumbor/6fe60btKVRo3xAfzIQKd0WJoGkM=/0x0:4096x4096/920x0/filters:focal(0x0:4096x4096):no_upscale()/cdn.vox-cdn.com/uploads/chorus_asset/file/25269947/modis_wonderglobe_lrg.jpeg 920w, https://cdn.vox-cdn.com/thumbor/6_sCiuwTtpiZjUWFaPAVIMYpxyk=/0x0:4096x4096/1120x0/filters:focal(0x0:4096x4096):no_upscale()/cdn.vox-cdn.com/uploads/chorus_asset/file/25269947/modis_wonderglobe_lrg.jpeg 1120w, https://cdn.vox-cdn.com/thumbor/r6sBWwT0xli9TV6IemARQ9o0utI=/0x0:4096x4096/1320x0/filters:focal(0x0:4096x4096):no_upscale()/cdn.vox-cdn.com/uploads/chorus_asset/file/25269947/modis_wonderglobe_lrg.jpeg 1320w, https://cdn.vox-cdn.com/thumbor/cCnGsdArV1iJVc_-KM1fUdb2o1k=/0x0:4096x4096/1520x0/filters:focal(0x0:4096x4096):no_upscale()/cdn.vox-cdn.com/uploads/chorus_asset/file/25269947/modis_wonderglobe_lrg.jpeg 1520w, https://cdn.vox-cdn.com/thumbor/5l3xjVUCaWsNkA7B91-9Zrf_coU=/0x0:4096x4096/1720x0/filters:focal(0x0:4096x4096):no_upscale()/cdn.vox-cdn.com/uploads/chorus_asset/file/25269947/modis_wonderglobe_lrg.jpeg 1720w, https://cdn.vox-cdn.com/thumbor/MTfrlTrPejbKT_46WYU6WLxpmUA=/0x0:4096x4096/1920x0/filters:focal(0x0:4096x4096):no_upscale()/cdn.vox-cdn.com/uploads/chorus_asset/file/25269947/modis_wonderglobe_lrg.jpeg 1920w" sizes="(min-width: 1221px) 846px, (min-width: 880px) calc(100vw - 334px), 100vw" alt="" loading="lazy" data-upload-width="4096" width="4096" height="4096" src="https://cdn.vox-cdn.com/thumbor/F8XOAAablvn1-eatTEBLs__gn4o=/0x0:4096x4096/1200x0/filters:focal(0x0:4096x4096):no_upscale()/cdn.vox-cdn.com/uploads/chorus_asset/file/25269947/modis_wonderglobe_lrg.jpeg">

</picture>

      
    </span>
    
  </span>
  
    <span>
      
        <figcaption>A true-color image of Earth taken by NASA satellites more than two decades ago.</figcaption>
      
      
        <cite>NASA</cite>
      
    </span>
  
</figure>

<h3 id="CY2YWK">Why Earth is getting greener</h3>
<p id="yMlyEd">Much of what we know about our planet on a global scale comes from satellites. Some of them are equipped with high-tech sensors that measure different wavelengths of light. With help from computer models, these sensors can roughly approximate the amount of leaves in a given area on the ground. More “greening” means the ground has more leaves, typically because it has more plants, or those plants have more (or larger) leaves on them. </p>
<p id="INrYRK">The global greening effect, which <a href="https://www.nature.com/articles/nclimate3004">dates back to the 1980s or earlier</a>, isn’t tiny. In one <a href="https://www.nature.com/articles/s41893-019-0220-7">2019 study</a> published in the journal <em>Nature Sustainability</em>, scientists found that the Earth had increased its green leaf area (i.e., the amount of leaves) by 5 percent in the last two decades. That’s equivalent to an area the size of the Amazon rainforest covered in a thin layer of leaves. A <a href="https://www.sciencedirect.com/science/article/pii/S2351989423004262?dgcid=raven_sd_aip_email">more recent paper</a>, meanwhile, found that the world is not only leafier, but the rate of greening is actually accelerating across more than half of its land. </p>
  <figure>
  <span>
    
    <span data-original="https://cdn.vox-cdn.com/uploads/chorus_asset/file/25269958/41893_2019_220_Fig1_HTML__2_.jpg">
      
        <picture data-cid="site/picture_element-1708800114_9675_64558" data-cdata="{&quot;asset_id&quot;:25269958,&quot;ratio&quot;:&quot;*&quot;}">
  


  <source srcset="https://cdn.vox-cdn.com/thumbor/bTF1IV8R5c1WuYauTtUfYB_YM8I=/0x0:2114x1058/320x0/filters:focal(0x0:2114x1058):format(webp):no_upscale()/cdn.vox-cdn.com/uploads/chorus_asset/file/25269958/41893_2019_220_Fig1_HTML__2_.jpg 320w, https://cdn.vox-cdn.com/thumbor/z_QA25PBuS67qJ1sN_71O0NqJB4=/0x0:2114x1058/520x0/filters:focal(0x0:2114x1058):format(webp):no_upscale()/cdn.vox-cdn.com/uploads/chorus_asset/file/25269958/41893_2019_220_Fig1_HTML__2_.jpg 520w, https://cdn.vox-cdn.com/thumbor/sOsPNPDmTa1msGr4ltFX4wyW4HI=/0x0:2114x1058/720x0/filters:focal(0x0:2114x1058):format(webp):no_upscale()/cdn.vox-cdn.com/uploads/chorus_asset/file/25269958/41893_2019_220_Fig1_HTML__2_.jpg 720w, https://cdn.vox-cdn.com/thumbor/SUDCmrdjqWjvw0YAW0I7LI9Z5Kw=/0x0:2114x1058/920x0/filters:focal(0x0:2114x1058):format(webp):no_upscale()/cdn.vox-cdn.com/uploads/chorus_asset/file/25269958/41893_2019_220_Fig1_HTML__2_.jpg 920w, https://cdn.vox-cdn.com/thumbor/kYDjRmYTwXEJny_qvsY4f1Khq_g=/0x0:2114x1058/1120x0/filters:focal(0x0:2114x1058):format(webp):no_upscale()/cdn.vox-cdn.com/uploads/chorus_asset/file/25269958/41893_2019_220_Fig1_HTML__2_.jpg 1120w, https://cdn.vox-cdn.com/thumbor/EbvJND0CKl3-7xG4JKayOKjjNUo=/0x0:2114x1058/1320x0/filters:focal(0x0:2114x1058):format(webp):no_upscale()/cdn.vox-cdn.com/uploads/chorus_asset/file/25269958/41893_2019_220_Fig1_HTML__2_.jpg 1320w, https://cdn.vox-cdn.com/thumbor/getDT_DKBmNdY__IGz5b4y_UJ3M=/0x0:2114x1058/1520x0/filters:focal(0x0:2114x1058):format(webp):no_upscale()/cdn.vox-cdn.com/uploads/chorus_asset/file/25269958/41893_2019_220_Fig1_HTML__2_.jpg 1520w, https://cdn.vox-cdn.com/thumbor/B3zE2TUa9pBUbWJ2mBpFt3zJNt0=/0x0:2114x1058/1720x0/filters:focal(0x0:2114x1058):format(webp):no_upscale()/cdn.vox-cdn.com/uploads/chorus_asset/file/25269958/41893_2019_220_Fig1_HTML__2_.jpg 1720w, https://cdn.vox-cdn.com/thumbor/6DgNRZQwSMTqMdNtmn9iRZBpB5s=/0x0:2114x1058/1920x0/filters:focal(0x0:2114x1058):format(webp):no_upscale()/cdn.vox-cdn.com/uploads/chorus_asset/file/25269958/41893_2019_220_Fig1_HTML__2_.jpg 1920w" sizes="(min-width: 1221px) 846px, (min-width: 880px) calc(100vw - 334px), 100vw" type="image/webp">


<img srcset="https://cdn.vox-cdn.com/thumbor/3D3iZxKXWi1d2lTnDbL6cSun9t8=/0x0:2114x1058/320x0/filters:focal(0x0:2114x1058):no_upscale()/cdn.vox-cdn.com/uploads/chorus_asset/file/25269958/41893_2019_220_Fig1_HTML__2_.jpg 320w, https://cdn.vox-cdn.com/thumbor/xpjOIDIOuabdhayZ809mm6khsK0=/0x0:2114x1058/520x0/filters:focal(0x0:2114x1058):no_upscale()/cdn.vox-cdn.com/uploads/chorus_asset/file/25269958/41893_2019_220_Fig1_HTML__2_.jpg 520w, https://cdn.vox-cdn.com/thumbor/bjkM1JvQ7kCGdZfmAjavSl32JZk=/0x0:2114x1058/720x0/filters:focal(0x0:2114x1058):no_upscale()/cdn.vox-cdn.com/uploads/chorus_asset/file/25269958/41893_2019_220_Fig1_HTML__2_.jpg 720w, https://cdn.vox-cdn.com/thumbor/YxQG6XAABDJZLfLYL4CCFxALqeo=/0x0:2114x1058/920x0/filters:focal(0x0:2114x1058):no_upscale()/cdn.vox-cdn.com/uploads/chorus_asset/file/25269958/41893_2019_220_Fig1_HTML__2_.jpg 920w, https://cdn.vox-cdn.com/thumbor/nS80fKRGYw_kkZig033iEIapdnI=/0x0:2114x1058/1120x0/filters:focal(0x0:2114x1058):no_upscale()/cdn.vox-cdn.com/uploads/chorus_asset/file/25269958/41893_2019_220_Fig1_HTML__2_.jpg 1120w, https://cdn.vox-cdn.com/thumbor/7U3vPszioqfyA3-0W2U90g2TLJM=/0x0:2114x1058/1320x0/filters:focal(0x0:2114x1058):no_upscale()/cdn.vox-cdn.com/uploads/chorus_asset/file/25269958/41893_2019_220_Fig1_HTML__2_.jpg 1320w, https://cdn.vox-cdn.com/thumbor/3HCcQ0kbZ1ci_uhf32wW-XLvEU4=/0x0:2114x1058/1520x0/filters:focal(0x0:2114x1058):no_upscale()/cdn.vox-cdn.com/uploads/chorus_asset/file/25269958/41893_2019_220_Fig1_HTML__2_.jpg 1520w, https://cdn.vox-cdn.com/thumbor/mvLNZOxcB0tOQcL-arQVO_sCmSU=/0x0:2114x1058/1720x0/filters:focal(0x0:2114x1058):no_upscale()/cdn.vox-cdn.com/uploads/chorus_asset/file/25269958/41893_2019_220_Fig1_HTML__2_.jpg 1720w, https://cdn.vox-cdn.com/thumbor/_-G2L3N-gRbXepXHuGGBtwV4TOo=/0x0:2114x1058/1920x0/filters:focal(0x0:2114x1058):no_upscale()/cdn.vox-cdn.com/uploads/chorus_asset/file/25269958/41893_2019_220_Fig1_HTML__2_.jpg 1920w" sizes="(min-width: 1221px) 846px, (min-width: 880px) calc(100vw - 334px), 100vw" alt="" loading="lazy" data-upload-width="2114" width="2114" height="1058" src="https://cdn.vox-cdn.com/thumbor/Ki7QV4-xUkST04RgnBWeQ-jFKkg=/0x0:2114x1058/1200x0/filters:focal(0x0:2114x1058):no_upscale()/cdn.vox-cdn.com/uploads/chorus_asset/file/25269958/41893_2019_220_Fig1_HTML__2_.jpg">

</picture>

      
    </span>
    
  </span>
  
    <span>
      
        <figcaption>Recent trends in global greening. Darker green indicates where the planet is becoming leafier.</figcaption>
      
      
        <cite><a href="https://www.nature.com/articles/s41893-019-0220-7" target="_blank">Chen et al./Nature Sustainability</a></cite>
      
    </span>
  
</figure>

<p id="N4bnLA">These results are somewhat counterintuitive. In an age of deforestation, you might expect Earth to get browner or more gray, as satellites see stumps in place of trees and runways in place of wetlands. Where is all this green color coming from? </p>
<p id="AKTG7D">One explanation is <a href="https://www.vox.com/air-quality" data-source="encore">air pollution</a>. Carbon dioxide is not only a pollutant but a fertilizer — a key ingredient in photosynthesis that helps plants grow. Some farmers inject CO2 into their greenhouses to accelerate plant growth. But now we’re fertilizing plants on a global scale: In the last two centuries, <a href="https://climate.nasa.gov/vital-signs/carbon-dioxide/">NASA reports</a>, humans have increased the CO2 content in the air by roughly 50 percent. All that extra CO2 is accelerating leaf growth, and satellites can see it. </p>
<p id="rdWN28">Humans are also just growing more plants. The 2019 <em>Nature Sustainability</em> study found that the dominant driver of recent global greening is a combination of more farming and, to a lesser extent, more tree planting. People are growing more crops on the same amount of land and turning barren patches of soil into verdant farms. </p>
<p id="VSH3om">These trends are especially prominent in <a href="https://www.vox.com/china" data-source="encore">China</a> and <a href="https://www.vox.com/india" data-source="encore">India</a>. Together, these two countries account for roughly one-third of all greening, the study found. </p>
<p id="LBtzES">“The intensification of agriculture that’s been happening in India over the past four decades is stunning,” said Joshua Gray, a geospatial scientist at North Carolina State University, who was not affiliated with the 2019 paper. </p>
<p id="fhq8Pb">China, meanwhile, has planted tens of billions of trees, often in plots with just one species, over the last four decades, <a href="http://english.scio.gov.cn/chinavoices/2023-04/06/content_85213960.htm#:~:text=From%201982%20to%202021%2C%20Chinese,trees%20across%20the%20vast%20country.">according to</a> the country’s government. The idea behind this massive tree-planting campaign is, among other goals, to stop land from drying out, reduce erosion, and provide people with a source of income from timber.</p>
<h3 id="x7Vcd8">Green can be good</h3>
<p id="TE88By">Broadly speaking, a leafier planet can help the climate. Our oceans and lands, including forests, absorb <a href="https://climate.nasa.gov/faq/55/how-might-earths-atmosphere-land-and-ocean-systems-respond-to-changes-in-carbon-dioxide-over-time/">more than half</a> of the CO2 that countries spew into the air. These “carbon sinks” keep global warming from getting worse than it already is, and at least on land, they have been growing for <a href="https://climate.mit.edu/ask-mit/how-much-human-produced-carbon-dioxide-taken-faster-plant-growth-around-world">several decades.</a> </p>
<p id="edNJoV">Global greening, Gray said, is one reason why the land sink has ballooned. </p>
  <figure>
  <span>
    
    <span data-original="https://cdn.vox-cdn.com/uploads/chorus_asset/file/25269973/ImageWall6_1600x1200_359.jpg">
      
        <picture data-cid="site/picture_element-1708800114_6540_64559" data-cdata="{&quot;asset_id&quot;:25269973,&quot;ratio&quot;:&quot;*&quot;}">
  


  <source srcset="https://cdn.vox-cdn.com/thumbor/2Mj41B0ye-gOcCE2T_Ncbped5Qc=/0x0:1600x1200/320x0/filters:focal(0x0:1600x1200):format(webp):no_upscale()/cdn.vox-cdn.com/uploads/chorus_asset/file/25269973/ImageWall6_1600x1200_359.jpg 320w, https://cdn.vox-cdn.com/thumbor/aduSxeNokfSsgBU3YRWYYWr7zTI=/0x0:1600x1200/520x0/filters:focal(0x0:1600x1200):format(webp):no_upscale()/cdn.vox-cdn.com/uploads/chorus_asset/file/25269973/ImageWall6_1600x1200_359.jpg 520w, https://cdn.vox-cdn.com/thumbor/8FjH65hNBnbwSyGiSXX4-7nj6ho=/0x0:1600x1200/720x0/filters:focal(0x0:1600x1200):format(webp):no_upscale()/cdn.vox-cdn.com/uploads/chorus_asset/file/25269973/ImageWall6_1600x1200_359.jpg 720w, https://cdn.vox-cdn.com/thumbor/gtoitDxbGfER4SgvuRqdyYZyzbs=/0x0:1600x1200/920x0/filters:focal(0x0:1600x1200):format(webp):no_upscale()/cdn.vox-cdn.com/uploads/chorus_asset/file/25269973/ImageWall6_1600x1200_359.jpg 920w, https://cdn.vox-cdn.com/thumbor/Bx7IYafWAaRSUsI_jsZS2NBn0Vo=/0x0:1600x1200/1120x0/filters:focal(0x0:1600x1200):format(webp):no_upscale()/cdn.vox-cdn.com/uploads/chorus_asset/file/25269973/ImageWall6_1600x1200_359.jpg 1120w, https://cdn.vox-cdn.com/thumbor/SrqHWU6i7uc1IQsbgVyPjUrZ_Xo=/0x0:1600x1200/1320x0/filters:focal(0x0:1600x1200):format(webp):no_upscale()/cdn.vox-cdn.com/uploads/chorus_asset/file/25269973/ImageWall6_1600x1200_359.jpg 1320w, https://cdn.vox-cdn.com/thumbor/bBGFl90rHgHUgToU7TTyojQqp3A=/0x0:1600x1200/1520x0/filters:focal(0x0:1600x1200):format(webp):no_upscale()/cdn.vox-cdn.com/uploads/chorus_asset/file/25269973/ImageWall6_1600x1200_359.jpg 1520w, https://cdn.vox-cdn.com/thumbor/fGZwtFeSBXYYNkwXwV-wKm3mMeM=/0x0:1600x1200/1720x0/filters:focal(0x0:1600x1200):format(webp):no_upscale()/cdn.vox-cdn.com/uploads/chorus_asset/file/25269973/ImageWall6_1600x1200_359.jpg 1720w, https://cdn.vox-cdn.com/thumbor/zE9-uDvNKd2aVV2T1OLMOQb09_Y=/0x0:1600x1200/1920x0/filters:focal(0x0:1600x1200):format(webp):no_upscale()/cdn.vox-cdn.com/uploads/chorus_asset/file/25269973/ImageWall6_1600x1200_359.jpg 1920w" sizes="(min-width: 1221px) 846px, (min-width: 880px) calc(100vw - 334px), 100vw" type="image/webp">


<img srcset="https://cdn.vox-cdn.com/thumbor/fFD0k_MiUYd9JCj1X3Pt7f1OFTc=/0x0:1600x1200/320x0/filters:focal(0x0:1600x1200):no_upscale()/cdn.vox-cdn.com/uploads/chorus_asset/file/25269973/ImageWall6_1600x1200_359.jpg 320w, https://cdn.vox-cdn.com/thumbor/8EvNPe4ktBOkeA0fzC434DI52Fk=/0x0:1600x1200/520x0/filters:focal(0x0:1600x1200):no_upscale()/cdn.vox-cdn.com/uploads/chorus_asset/file/25269973/ImageWall6_1600x1200_359.jpg 520w, https://cdn.vox-cdn.com/thumbor/ruXfftxnTg5as-ce7_pjW4tkB4c=/0x0:1600x1200/720x0/filters:focal(0x0:1600x1200):no_upscale()/cdn.vox-cdn.com/uploads/chorus_asset/file/25269973/ImageWall6_1600x1200_359.jpg 720w, https://cdn.vox-cdn.com/thumbor/EZyO0nZd1TtN2GpaweRtYhQBs1A=/0x0:1600x1200/920x0/filters:focal(0x0:1600x1200):no_upscale()/cdn.vox-cdn.com/uploads/chorus_asset/file/25269973/ImageWall6_1600x1200_359.jpg 920w, https://cdn.vox-cdn.com/thumbor/sfA1hcJkJtbUGQWeU5CPmbfZksc=/0x0:1600x1200/1120x0/filters:focal(0x0:1600x1200):no_upscale()/cdn.vox-cdn.com/uploads/chorus_asset/file/25269973/ImageWall6_1600x1200_359.jpg 1120w, https://cdn.vox-cdn.com/thumbor/3U8ICBDs3-GCSbbXUXQboOxWSg8=/0x0:1600x1200/1320x0/filters:focal(0x0:1600x1200):no_upscale()/cdn.vox-cdn.com/uploads/chorus_asset/file/25269973/ImageWall6_1600x1200_359.jpg 1320w, https://cdn.vox-cdn.com/thumbor/VqUMvnC7fDBoJv_FRFbnM8gZ9cY=/0x0:1600x1200/1520x0/filters:focal(0x0:1600x1200):no_upscale()/cdn.vox-cdn.com/uploads/chorus_asset/file/25269973/ImageWall6_1600x1200_359.jpg 1520w, https://cdn.vox-cdn.com/thumbor/wnKp3CqfHjnaO0fubAeeGIDjTC4=/0x0:1600x1200/1720x0/filters:focal(0x0:1600x1200):no_upscale()/cdn.vox-cdn.com/uploads/chorus_asset/file/25269973/ImageWall6_1600x1200_359.jpg 1720w, https://cdn.vox-cdn.com/thumbor/WKZI8SbO4keql86HOw_l0NFdux0=/0x0:1600x1200/1920x0/filters:focal(0x0:1600x1200):no_upscale()/cdn.vox-cdn.com/uploads/chorus_asset/file/25269973/ImageWall6_1600x1200_359.jpg 1920w" sizes="(min-width: 1221px) 846px, (min-width: 880px) calc(100vw - 334px), 100vw" alt="" loading="lazy" data-upload-width="1600" width="1600" height="1200" src="https://cdn.vox-cdn.com/thumbor/5s-Xfm5x7zqXbAE1lLqnFuPk-QM=/0x0:1600x1200/1200x0/filters:focal(0x0:1600x1200):no_upscale()/cdn.vox-cdn.com/uploads/chorus_asset/file/25269973/ImageWall6_1600x1200_359.jpg">

</picture>

      
    </span>
    
  </span>
  
    <span>
      
        <figcaption>A phytoplankton bloom near Iceland in the summer of 2010.</figcaption>
      
      
        <cite>NASA</cite>
      
    </span>
  
</figure>

<p id="c1I84l">In the ocean, however, greening is far more mysterious; it’s not clear why the sea is getting greener or what that means for the climate. The observed shift in color is likely caused by phytoplankton, a tiny plant-like organism that, like plants, absorbs CO2. Greener seas might mean there’s simply more phytoplankton in some areas. Alternatively, there could be a shift in the phytoplankton community toward species that produce more green pigment, according to B.B. Cael, a scientist at the National Oceanography Centre who has <a href="https://www.nature.com/articles/s41586-023-06321-z">studied ocean greening</a>. The difference matters because it determines how much carbon the oceans can absorb. </p>
<p id="UpjOnM">Better satellite data will help figure some of this out. That’s one reason why <a href="https://www.vox.com/space" data-source="encore">NASA</a> <a href="https://pace.gsfc.nasa.gov/">is expected to launch</a> a satellite called PACE, which will measure ocean color to better understand how plankton communities influence Earth’s climate. </p>
  <figure>
  <span>
    
    <span data-original="https://cdn.vox-cdn.com/uploads/chorus_asset/file/25269942/GSFC_20230617_PACE_019382_2000w.jpeg">
      
        <picture data-cid="site/picture_element-1708800114_2656_64560" data-cdata="{&quot;asset_id&quot;:25269942,&quot;ratio&quot;:&quot;*&quot;}">
  


  <source srcset="https://cdn.vox-cdn.com/thumbor/A13JtR8xJaKAHAXzHrFNJOtYOBA=/0x0:1999x1333/320x0/filters:focal(0x0:1999x1333):format(webp):no_upscale()/cdn.vox-cdn.com/uploads/chorus_asset/file/25269942/GSFC_20230617_PACE_019382_2000w.jpeg 320w, https://cdn.vox-cdn.com/thumbor/OzcLyirjUhTEDktfGEx6Xa-T9cA=/0x0:1999x1333/520x0/filters:focal(0x0:1999x1333):format(webp):no_upscale()/cdn.vox-cdn.com/uploads/chorus_asset/file/25269942/GSFC_20230617_PACE_019382_2000w.jpeg 520w, https://cdn.vox-cdn.com/thumbor/MU3f1xz1X21jIvLHLVIUKWbp-u8=/0x0:1999x1333/720x0/filters:focal(0x0:1999x1333):format(webp):no_upscale()/cdn.vox-cdn.com/uploads/chorus_asset/file/25269942/GSFC_20230617_PACE_019382_2000w.jpeg 720w, https://cdn.vox-cdn.com/thumbor/Au_nbxA71viGROSGkyoQQT-jDHg=/0x0:1999x1333/920x0/filters:focal(0x0:1999x1333):format(webp):no_upscale()/cdn.vox-cdn.com/uploads/chorus_asset/file/25269942/GSFC_20230617_PACE_019382_2000w.jpeg 920w, https://cdn.vox-cdn.com/thumbor/Ibng6HGf6TLH3p57qPHE8LzgZdc=/0x0:1999x1333/1120x0/filters:focal(0x0:1999x1333):format(webp):no_upscale()/cdn.vox-cdn.com/uploads/chorus_asset/file/25269942/GSFC_20230617_PACE_019382_2000w.jpeg 1120w, https://cdn.vox-cdn.com/thumbor/fj1YexF0oe2LQw_S14kh3G7W_IA=/0x0:1999x1333/1320x0/filters:focal(0x0:1999x1333):format(webp):no_upscale()/cdn.vox-cdn.com/uploads/chorus_asset/file/25269942/GSFC_20230617_PACE_019382_2000w.jpeg 1320w, https://cdn.vox-cdn.com/thumbor/j_VdTIDP7eZ_qqJZd6HLmhQ9XtI=/0x0:1999x1333/1520x0/filters:focal(0x0:1999x1333):format(webp):no_upscale()/cdn.vox-cdn.com/uploads/chorus_asset/file/25269942/GSFC_20230617_PACE_019382_2000w.jpeg 1520w, https://cdn.vox-cdn.com/thumbor/KpWJJjNMBTnuFP8A4nsx6segP7s=/0x0:1999x1333/1720x0/filters:focal(0x0:1999x1333):format(webp):no_upscale()/cdn.vox-cdn.com/uploads/chorus_asset/file/25269942/GSFC_20230617_PACE_019382_2000w.jpeg 1720w, https://cdn.vox-cdn.com/thumbor/SnTC0QNjFI9f5gShwF6YYYEwTWc=/0x0:1999x1333/1920x0/filters:focal(0x0:1999x1333):format(webp):no_upscale()/cdn.vox-cdn.com/uploads/chorus_asset/file/25269942/GSFC_20230617_PACE_019382_2000w.jpeg 1920w" sizes="(min-width: 1221px) 846px, (min-width: 880px) calc(100vw - 334px), 100vw" type="image/webp">


<img srcset="https://cdn.vox-cdn.com/thumbor/FmZsEf85Zmfo65IwNuyXw_1rA7s=/0x0:1999x1333/320x0/filters:focal(0x0:1999x1333):no_upscale()/cdn.vox-cdn.com/uploads/chorus_asset/file/25269942/GSFC_20230617_PACE_019382_2000w.jpeg 320w, https://cdn.vox-cdn.com/thumbor/zqwZ52e1rbN9xVsN1ka8hJMsHQ0=/0x0:1999x1333/520x0/filters:focal(0x0:1999x1333):no_upscale()/cdn.vox-cdn.com/uploads/chorus_asset/file/25269942/GSFC_20230617_PACE_019382_2000w.jpeg 520w, https://cdn.vox-cdn.com/thumbor/YM1zyHSvxod7QBVMN9rwdE0jTb8=/0x0:1999x1333/720x0/filters:focal(0x0:1999x1333):no_upscale()/cdn.vox-cdn.com/uploads/chorus_asset/file/25269942/GSFC_20230617_PACE_019382_2000w.jpeg 720w, https://cdn.vox-cdn.com/thumbor/DBqXH5t3EOM1LOK9IAO1CMdgmSU=/0x0:1999x1333/920x0/filters:focal(0x0:1999x1333):no_upscale()/cdn.vox-cdn.com/uploads/chorus_asset/file/25269942/GSFC_20230617_PACE_019382_2000w.jpeg 920w, https://cdn.vox-cdn.com/thumbor/yUGB--L_vuJEBLKlS5HHV53xC1I=/0x0:1999x1333/1120x0/filters:focal(0x0:1999x1333):no_upscale()/cdn.vox-cdn.com/uploads/chorus_asset/file/25269942/GSFC_20230617_PACE_019382_2000w.jpeg 1120w, https://cdn.vox-cdn.com/thumbor/1wqNDB7R3_F1jOfFYR9HN6cILbU=/0x0:1999x1333/1320x0/filters:focal(0x0:1999x1333):no_upscale()/cdn.vox-cdn.com/uploads/chorus_asset/file/25269942/GSFC_20230617_PACE_019382_2000w.jpeg 1320w, https://cdn.vox-cdn.com/thumbor/mYuvZ9lZqnaglzsFbhFaIZe_ZRA=/0x0:1999x1333/1520x0/filters:focal(0x0:1999x1333):no_upscale()/cdn.vox-cdn.com/uploads/chorus_asset/file/25269942/GSFC_20230617_PACE_019382_2000w.jpeg 1520w, https://cdn.vox-cdn.com/thumbor/XVW_ZSrkuqwjNpFbSocqYMyg2K0=/0x0:1999x1333/1720x0/filters:focal(0x0:1999x1333):no_upscale()/cdn.vox-cdn.com/uploads/chorus_asset/file/25269942/GSFC_20230617_PACE_019382_2000w.jpeg 1720w, https://cdn.vox-cdn.com/thumbor/5hJq7Wwe8v1sb2zXQEk1pa2WlpA=/0x0:1999x1333/1920x0/filters:focal(0x0:1999x1333):no_upscale()/cdn.vox-cdn.com/uploads/chorus_asset/file/25269942/GSFC_20230617_PACE_019382_2000w.jpeg 1920w" sizes="(min-width: 1221px) 846px, (min-width: 880px) calc(100vw - 334px), 100vw" alt="" loading="lazy" data-upload-width="1999" width="1999" height="1333" src="https://cdn.vox-cdn.com/thumbor/CK6gcLVWs3Y7GjGnW-Of04rsC4Q=/0x0:1999x1333/1200x0/filters:focal(0x0:1999x1333):no_upscale()/cdn.vox-cdn.com/uploads/chorus_asset/file/25269942/GSFC_20230617_PACE_019382_2000w.jpeg">

</picture>

      
    </span>
    
  </span>
  
    <span>
      
        <figcaption>NASA engineers test the PACE observatory satellite in a space environment simulator.</figcaption>
      
      
        <cite>NASA</cite>
      
    </span>
  
</figure>

<h3 id="RMHflq">The big problems behind the green sheen</h3>
<p id="DVQggM">There’s a lot that color alone leaves out, such as what that “green” is made of. </p>
<p id="CmCDvF">To sensors on a satellite, a rainforest in Indonesia and a nearby monoculture of coffee or rubber trees look similar. They both appear green. Yet these two landscapes are dramatically different: The rainforest is home to orangutans and rare plants and helps regulate the local climate, whereas the plantation is relatively devoid of life. Measurements of color alone fail to capture these important differences. </p>
<p id="jor17n">More than that, they can mask ecosystem destruction, said Robin Chazdon, a tropical ecologist and part-time scientist at the World Resources Institute, an environmental group. Companies commonly tear up native forests to plant commercial crops. Satellite data alone struggles to capture these changes in land use. </p>
<p id="CI0gCD">“It’s glossing over the reality of what’s actually happened,” Chazdon said of global greening measurements.</p>
<p id="uhWlYi">Greening caused by tree planting — common in China and India — can also be problematic, she said. Planted forests often comprise just one or two tree species and don’t offer much in the way of biodiversity or other benefits, like erosion control, she said. In some cases, the trees eventually die. </p>
<p id="pmwXKu">The growth in green farmland, similarly, has some pretty serious consequences. Industrial farms not only replace native ecosystems but require huge amounts of water and chemicals, such as fertilizers and pesticides (which are <a href="https://www.annualreviews.org/doi/full/10.1146/annurev-environ-120920-111015">known to harm humans and ecosystems</a>). Consider the Imperial Valley of Southern California. Once a desert, it’s now <a href="https://www.vox.com/the-highlight/23648116/colorado-river-lake-mead-agriculture-leafy-greens">covered in vast stretches of farmland</a>. Those farms have turned the region green — and it’s visible from space — yet they’ve done so, in part, by draining the Colorado River and fueling a water war in the West. </p>
<p id="qyWcB0">What’s more is that while plants absorb carbon, industrial cropland typically <a href="https://iopscience.iop.org/article/10.1088/1748-9326/acd5e8">produces more carbon emissions than it absorbs</a> over the long term. Making nitrogen fertilizer and other agrochemicals requires a huge amount of energy, which typically comes from <a href="https://www.vox.com/fossil-fuels" data-source="encore">fossil fuels</a>. Plus, much of the carbon absorbed by plants on a farm gets reemitted into the environment after they’re harvested.</p>
  <figure>
  <span>
    
    <span data-original="https://cdn.vox-cdn.com/uploads/chorus_asset/file/25269988/NASA_Crop_Circles_Kansas.jpeg">
      
        <picture data-cid="site/picture_element-1708800114_8514_64561" data-cdata="{&quot;asset_id&quot;:25269988,&quot;ratio&quot;:&quot;*&quot;}">
  


  <source srcset="https://cdn.vox-cdn.com/thumbor/-r3k_H0zKgquKoTRR-hG4Sa60-I=/0x0:2589x2481/320x0/filters:focal(0x0:2589x2481):format(webp):no_upscale()/cdn.vox-cdn.com/uploads/chorus_asset/file/25269988/NASA_Crop_Circles_Kansas.jpeg 320w, https://cdn.vox-cdn.com/thumbor/AxaCqlR6xcT1kX_z0uyh3blbo88=/0x0:2589x2481/520x0/filters:focal(0x0:2589x2481):format(webp):no_upscale()/cdn.vox-cdn.com/uploads/chorus_asset/file/25269988/NASA_Crop_Circles_Kansas.jpeg 520w, https://cdn.vox-cdn.com/thumbor/V1yqUXKiOB67fH0Shpxy0gpUYZg=/0x0:2589x2481/720x0/filters:focal(0x0:2589x2481):format(webp):no_upscale()/cdn.vox-cdn.com/uploads/chorus_asset/file/25269988/NASA_Crop_Circles_Kansas.jpeg 720w, https://cdn.vox-cdn.com/thumbor/4MTc18VZATHtkrjx5k0GjwVKSf8=/0x0:2589x2481/920x0/filters:focal(0x0:2589x2481):format(webp):no_upscale()/cdn.vox-cdn.com/uploads/chorus_asset/file/25269988/NASA_Crop_Circles_Kansas.jpeg 920w, https://cdn.vox-cdn.com/thumbor/3qDDHIQNi00ExGI9YQAWWtdoF84=/0x0:2589x2481/1120x0/filters:focal(0x0:2589x2481):format(webp):no_upscale()/cdn.vox-cdn.com/uploads/chorus_asset/file/25269988/NASA_Crop_Circles_Kansas.jpeg 1120w, https://cdn.vox-cdn.com/thumbor/0NB9S8nXFXuXLCS2XH9Gx7xPdlc=/0x0:2589x2481/1320x0/filters:focal(0x0:2589x2481):format(webp):no_upscale()/cdn.vox-cdn.com/uploads/chorus_asset/file/25269988/NASA_Crop_Circles_Kansas.jpeg 1320w, https://cdn.vox-cdn.com/thumbor/WTn3SG2onpLaeaNH6BuFLqibdcw=/0x0:2589x2481/1520x0/filters:focal(0x0:2589x2481):format(webp):no_upscale()/cdn.vox-cdn.com/uploads/chorus_asset/file/25269988/NASA_Crop_Circles_Kansas.jpeg 1520w, https://cdn.vox-cdn.com/thumbor/X9nvteP93t0ZP17-lKt7M-pOhIU=/0x0:2589x2481/1720x0/filters:focal(0x0:2589x2481):format(webp):no_upscale()/cdn.vox-cdn.com/uploads/chorus_asset/file/25269988/NASA_Crop_Circles_Kansas.jpeg 1720w, https://cdn.vox-cdn.com/thumbor/vyUFmM4GV4iskYwnrCUeriDMzw8=/0x0:2589x2481/1920x0/filters:focal(0x0:2589x2481):format(webp):no_upscale()/cdn.vox-cdn.com/uploads/chorus_asset/file/25269988/NASA_Crop_Circles_Kansas.jpeg 1920w" sizes="(min-width: 1221px) 846px, (min-width: 880px) calc(100vw - 334px), 100vw" type="image/webp">


<img srcset="https://cdn.vox-cdn.com/thumbor/sH9b1WlJytgYHMXVKgyBYw8A6vo=/0x0:2589x2481/320x0/filters:focal(0x0:2589x2481):no_upscale()/cdn.vox-cdn.com/uploads/chorus_asset/file/25269988/NASA_Crop_Circles_Kansas.jpeg 320w, https://cdn.vox-cdn.com/thumbor/yQMYOWjd0DRx7NjXZh4wGNZ04Hk=/0x0:2589x2481/520x0/filters:focal(0x0:2589x2481):no_upscale()/cdn.vox-cdn.com/uploads/chorus_asset/file/25269988/NASA_Crop_Circles_Kansas.jpeg 520w, https://cdn.vox-cdn.com/thumbor/ODhwhTo_qwTdoFbTX2mGQK90_hQ=/0x0:2589x2481/720x0/filters:focal(0x0:2589x2481):no_upscale()/cdn.vox-cdn.com/uploads/chorus_asset/file/25269988/NASA_Crop_Circles_Kansas.jpeg 720w, https://cdn.vox-cdn.com/thumbor/IiNM31faOsLu7YMQssOolCZHi3k=/0x0:2589x2481/920x0/filters:focal(0x0:2589x2481):no_upscale()/cdn.vox-cdn.com/uploads/chorus_asset/file/25269988/NASA_Crop_Circles_Kansas.jpeg 920w, https://cdn.vox-cdn.com/thumbor/KKS4omNwAyNtS3HlR-GtyRYB-lY=/0x0:2589x2481/1120x0/filters:focal(0x0:2589x2481):no_upscale()/cdn.vox-cdn.com/uploads/chorus_asset/file/25269988/NASA_Crop_Circles_Kansas.jpeg 1120w, https://cdn.vox-cdn.com/thumbor/dFtR8PXdWY_o05EtKKk4YXgRnhY=/0x0:2589x2481/1320x0/filters:focal(0x0:2589x2481):no_upscale()/cdn.vox-cdn.com/uploads/chorus_asset/file/25269988/NASA_Crop_Circles_Kansas.jpeg 1320w, https://cdn.vox-cdn.com/thumbor/QhNYFkFFk_9BoK5VWYle69RRPCM=/0x0:2589x2481/1520x0/filters:focal(0x0:2589x2481):no_upscale()/cdn.vox-cdn.com/uploads/chorus_asset/file/25269988/NASA_Crop_Circles_Kansas.jpeg 1520w, https://cdn.vox-cdn.com/thumbor/_Zb32OVLe_8t2w6nKhMdBtoyqmg=/0x0:2589x2481/1720x0/filters:focal(0x0:2589x2481):no_upscale()/cdn.vox-cdn.com/uploads/chorus_asset/file/25269988/NASA_Crop_Circles_Kansas.jpeg 1720w, https://cdn.vox-cdn.com/thumbor/qm4FOu5yt_1t9f_6SAjbTA4obAQ=/0x0:2589x2481/1920x0/filters:focal(0x0:2589x2481):no_upscale()/cdn.vox-cdn.com/uploads/chorus_asset/file/25269988/NASA_Crop_Circles_Kansas.jpeg 1920w" sizes="(min-width: 1221px) 846px, (min-width: 880px) calc(100vw - 334px), 100vw" alt="" loading="lazy" data-upload-width="2589" width="2589" height="2481" src="https://cdn.vox-cdn.com/thumbor/sU7IggIU20KfflIDzuhRzPBrU8I=/0x0:2589x2481/1200x0/filters:focal(0x0:2589x2481):no_upscale()/cdn.vox-cdn.com/uploads/chorus_asset/file/25269988/NASA_Crop_Circles_Kansas.jpeg">

</picture>

      
    </span>
    
  </span>
  
    <span>
      
        <figcaption>Farms of corn, wheat, and sorghum in southwestern Kansas, seen by a satellite in 2001.</figcaption>
      
      
        <cite>NASA</cite>
      
    </span>
  
</figure>

<p id="POAXOi">The other problem: While CO2 fertilization can make some crops grow faster, research has also found that it can decrease their nutritional value — such as the concentration of <a href="https://lamont.columbia.edu/news/how-climate-change-will-affect-plants">protein, and minerals like calcium and magnesium</a> — for a <a href="https://www.the-scientist.com/news-opinion/as-carbon-dioxide-goes-up-plants-nutrient-content-declines-70720">number of complicated reasons</a>. So pumping CO2 into the air means more but often less-nutritious vegetation (and globally, more than <a href="https://www.who.int/publications/m/item/WHO-WFP-UNICEF-statement-micronutrients-deficiencies-emergency">2 billion people</a> are nutrient-deficient). </p>
<p id="cgFZsn">So, yes, greening is complicated. It’s not inherently good. Sometimes it’s very bad. Context, it turns out, matters a lot. </p>
<p id="drHrcv">If there’s anything we can glean from color alone it’s the scale of human impact. It’s not that nature is healing — that forests are growing back because we left them alone — but that we have drastically changed the atmosphere, the ground, and the ocean. We have changed the very look of our planet, and it’s visible from space.</p>
  <div data-cid="site/article_footer-1708800114_9778_64562" data-cdata="{&quot;base_type&quot;:&quot;Entry&quot;,&quot;id&quot;:23821349,&quot;timestamp&quot;:1707303600,&quot;published_timestamp&quot;:1707303600,&quot;show_published_and_updated_timestamps&quot;:false,&quot;title&quot;:&quot;The Earth is getting greener. Hurray?&quot;,&quot;type&quot;:&quot;Feature&quot;,&quot;url&quot;:&quot;https://www.vox.com/down-to-earth/2024/2/7/24057308/earth-global-greening-climate-change-carbon&quot;,&quot;entry_layout&quot;:{&quot;key&quot;:&quot;unison_default&quot;,&quot;layout&quot;:&quot;unison_main&quot;,&quot;template&quot;:&quot;minimal&quot;},&quot;additional_byline&quot;:null,&quot;authors&quot;:[{&quot;id&quot;:7608276,&quot;name&quot;:&quot;Benji Jones&quot;,&quot;url&quot;:&quot;https://www.vox.com/authors/benji-jones&quot;,&quot;twitter_handle&quot;:&quot;BenjiSJones&quot;,&quot;profile_image_url&quot;:&quot;https://cdn.vox-cdn.com/thumbor/LyLMy4jhwu87YudQF1Is6XEMp0o=/512x512/cdn.vox-cdn.com/author_profile_images/194907/Insider_Headshot-358-slack.0.jpg&quot;,&quot;title&quot;:&quot;&quot;,&quot;email&quot;:&quot;&quot;,&quot;short_author_bio&quot;:&quot;is a senior environmental reporter at Vox, covering biodiversity loss and climate change. Before joining Vox, he was a senior energy reporter at Insider. Benji previously worked as a wildlife researcher.&quot;}],&quot;byline_enabled&quot;:true,&quot;byline_credit_text&quot;:&quot;By&quot;,&quot;byline_serial_comma_enabled&quot;:true,&quot;comment_count&quot;:0,&quot;comments_enabled&quot;:false,&quot;legacy_comments_enabled&quot;:false,&quot;coral_comments_enabled&quot;:false,&quot;coral_comment_counts_enabled&quot;:false,&quot;commerce_disclosure&quot;:null,&quot;community_name&quot;:&quot;Vox&quot;,&quot;community_url&quot;:&quot;https://www.vox.com/&quot;,&quot;community_logo&quot;:&quot;\r\n<svg width=\&quot;386px\&quot; height=\&quot;385px\&quot; viewBox=\&quot;0 0 386 385\&quot; version=\&quot;1.1\&quot; xmlns=\&quot;http://www.w3.org/2000/svg\&quot; xmlns:xlink=\&quot;http://www.w3.org/1999/xlink\&quot; >\r\n    \r\n    <title>vox-mark</title>\r\n    \r\n    <defs></defs>\r\n    <g id=\&quot;Page-1\&quot; stroke=\&quot;none\&quot; stroke-width=\&quot;1\&quot; fill=\&quot;none\&quot; fill-rule=\&quot;evenodd\&quot; >\r\n        <path d=\&quot;M239.811,0 L238.424,6 L259.374,6 C278.011,6 292.908,17.38 292.908,43.002 C292.908,56.967 287.784,75.469 276.598,96.888 L182.689,305.687 L159.283,35.693 C159.283,13.809 168.134,6 191.88,6 L205.854,6 L207.247,0 L1.409,0 L0,6 L13.049,6 C28.88,6 35.863,15.885 37.264,34.514 L73.611,385 L160.221,385 L304.525,79.217 C328.749,31.719 349.237,6 372.525,6 L384.162,6 L385.557,0 L239.811,0\&quot; id=\&quot;vox-mark\&quot; fill=\&quot;#444745\&quot; ></path>\r\n    </g>\r\n</svg>&quot;,&quot;cross_community&quot;:false,&quot;internal_groups&quot;:[{&quot;base_type&quot;:&quot;EntryGroup&quot;,&quot;id&quot;:112403,&quot;timestamp&quot;:1708779605,&quot;title&quot;:&quot;Approach — Dissects something complicated&quot;,&quot;type&quot;:&quot;SiteGroup&quot;,&quot;url&quot;:&quot;&quot;,&quot;slug&quot;:&quot;approach-dissects-something-complicated&quot;,&quot;community_logo&quot;:&quot;\r\n<svg width=\&quot;386px\&quot; height=\&quot;385px\&quot; viewBox=\&quot;0 0 386 385\&quot; version=\&quot;1.1\&quot; xmlns=\&quot;http://www.w3.org/2000/svg\&quot; xmlns:xlink=\&quot;http://www.w3.org/1999/xlink\&quot; >\r\n    \r\n    <title>vox-mark</title>\r\n    \r\n    <defs></defs>\r\n    <g id=\&quot;Page-1\&quot; stroke=\&quot;none\&quot; stroke-width=\&quot;1\&quot; fill=\&quot;none\&quot; fill-rule=\&quot;evenodd\&quot; >\r\n        <path d=\&quot;M239.811,0 L238.424,6 L259.374,6 C278.011,6 292.908,17.38 292.908,43.002 C292.908,56.967 287.784,75.469 276.598,96.888 L182.689,305.687 L159.283,35.693 C159.283,13.809 168.134,6 191.88,6 L205.854,6 L207.247,0 L1.409,0 L0,6 L13.049,6 C28.88,6 35.863,15.885 37.264,34.514 L73.611,385 L160.221,385 L304.525,79.217 C328.749,31.719 349.237,6 372.525,6 L384.162,6 L385.557,0 L239.811,0\&quot; id=\&quot;vox-mark\&quot; fill=\&quot;#444745\&quot; ></path>\r\n    </g>\r\n</svg>&quot;,&quot;community_name&quot;:&quot;Vox&quot;,&quot;community_url&quot;:&quot;https://www.vox.com/&quot;,&quot;cross_community&quot;:false,&quot;entry_count&quot;:564,&quot;always_show&quot;:false,&quot;description&quot;:&quot;&quot;,&quot;disclosure&quot;:&quot;&quot;,&quot;cover_image_url&quot;:&quot;&quot;,&quot;cover_image&quot;:null,&quot;title_image_url&quot;:&quot;&quot;,&quot;intro_image&quot;:null,&quot;four_up_see_more_text&quot;:&quot;View All&quot;},{&quot;base_type&quot;:&quot;EntryGroup&quot;,&quot;id&quot;:112404,&quot;timestamp&quot;:1708776008,&quot;title&quot;:&quot;Approach — Connects something to larger stakes&quot;,&quot;type&quot;:&quot;SiteGroup&quot;,&quot;url&quot;:&quot;&quot;,&quot;slug&quot;:&quot;approach-connects-something-to-larger-stakes&quot;,&quot;community_logo&quot;:&quot;\r\n<svg width=\&quot;386px\&quot; height=\&quot;385px\&quot; viewBox=\&quot;0 0 386 385\&quot; version=\&quot;1.1\&quot; xmlns=\&quot;http://www.w3.org/2000/svg\&quot; xmlns:xlink=\&quot;http://www.w3.org/1999/xlink\&quot; >\r\n    \r\n    <title>vox-mark</title>\r\n    \r\n    <defs></defs>\r\n    <g id=\&quot;Page-1\&quot; stroke=\&quot;none\&quot; stroke-width=\&quot;1\&quot; fill=\&quot;none\&quot; fill-rule=\&quot;evenodd\&quot; >\r\n        <path d=\&quot;M239.811,0 L238.424,6 L259.374,6 C278.011,6 292.908,17.38 292.908,43.002 C292.908,56.967 287.784,75.469 276.598,96.888 L182.689,305.687 L159.283,35.693 C159.283,13.809 168.134,6 191.88,6 L205.854,6 L207.247,0 L1.409,0 L0,6 L13.049,6 C28.88,6 35.863,15.885 37.264,34.514 L73.611,385 L160.221,385 L304.525,79.217 C328.749,31.719 349.237,6 372.525,6 L384.162,6 L385.557,0 L239.811,0\&quot; id=\&quot;vox-mark\&quot; fill=\&quot;#444745\&quot; ></path>\r\n    </g>\r\n</svg>&quot;,&quot;community_name&quot;:&quot;Vox&quot;,&quot;community_url&quot;:&quot;https://www.vox.com/&quot;,&quot;cross_community&quot;:false,&quot;entry_count&quot;:678,&quot;always_show&quot;:false,&quot;description&quot;:&quot;&quot;,&quot;disclosure&quot;:&quot;&quot;,&quot;cover_image_url&quot;:&quot;&quot;,&quot;cover_image&quot;:null,&quot;title_image_url&quot;:&quot;&quot;,&quot;intro_image&quot;:null,&quot;four_up_see_more_text&quot;:&quot;View All&quot;}],&quot;image&quot;:{&quot;ratio&quot;:&quot;*&quot;,&quot;original_url&quot;:&quot;https://cdn.vox-cdn.com/uploads/chorus_image/image/73118253/Vox_PaigeVickers_Benji.0.jpeg&quot;,&quot;network&quot;:&quot;unison&quot;,&quot;bgcolor&quot;:&quot;white&quot;,&quot;pinterest_enabled&quot;:false,&quot;caption&quot;:null,&quot;credit&quot;:&quot;Paige Vickers/Vox&quot;,&quot;focal_area&quot;:{&quot;top_left_x&quot;:1140,&quot;top_left_y&quot;:535,&quot;bottom_right_x&quot;:1446,&quot;bottom_right_y&quot;:841},&quot;bounds&quot;:[0,0,1920,1080],&quot;uploaded_size&quot;:{&quot;width&quot;:1920,&quot;height&quot;:1080},&quot;focal_point&quot;:null,&quot;image_id&quot;:73118253,&quot;alt_text&quot;:&quot;An illustration of a giant paintbrush painting Earth green.&quot;},&quot;hub_image&quot;:{&quot;ratio&quot;:&quot;*&quot;,&quot;original_url&quot;:&quot;https://cdn.vox-cdn.com/uploads/chorus_image/image/73118253/Vox_PaigeVickers_Benji.0.jpeg&quot;,&quot;network&quot;:&quot;unison&quot;,&quot;bgcolor&quot;:&quot;white&quot;,&quot;pinterest_enabled&quot;:false,&quot;caption&quot;:null,&quot;credit&quot;:&quot;Paige Vickers/Vox&quot;,&quot;focal_area&quot;:{&quot;top_left_x&quot;:1140,&quot;top_left_y&quot;:535,&quot;bottom_right_x&quot;:1446,&quot;bottom_right_y&quot;:841},&quot;bounds&quot;:[0,0,1920,1080],&quot;uploaded_size&quot;:{&quot;width&quot;:1920,&quot;height&quot;:1080},&quot;focal_point&quot;:null,&quot;image_id&quot;:73118253,&quot;alt_text&quot;:&quot;An illustration of a giant paintbrush painting Earth green.&quot;},&quot;lede_image&quot;:{&quot;ratio&quot;:&quot;*&quot;,&quot;original_url&quot;:&quot;https://cdn.vox-cdn.com/uploads/chorus_image/image/73118268/Vox_PaigeVickers_Benji.0.jpeg&quot;,&quot;network&quot;:&quot;unison&quot;,&quot;bgcolor&quot;:&quot;white&quot;,&quot;pinterest_enabled&quot;:false,&quot;caption&quot;:null,&quot;credit&quot;:&quot;Paige Vickers/Vox&quot;,&quot;focal_area&quot;:{&quot;top_left_x&quot;:1140,&quot;top_left_y&quot;:535,&quot;bottom_right_x&quot;:1446,&quot;bottom_right_y&quot;:841},&quot;bounds&quot;:[0,0,1920,1080],&quot;uploaded_size&quot;:{&quot;width&quot;:1920,&quot;height&quot;:1080},&quot;focal_point&quot;:null,&quot;image_id&quot;:73118268,&quot;alt_text&quot;:&quot;An illustration of a giant paintbrush painting Earth green.&quot;},&quot;group_cover_image&quot;:null,&quot;picture_standard_lead_image&quot;:{&quot;ratio&quot;:&quot;*&quot;,&quot;original_url&quot;:&quot;https://cdn.vox-cdn.com/uploads/chorus_image/image/73118268/Vox_PaigeVickers_Benji.0.jpeg&quot;,&quot;network&quot;:&quot;unison&quot;,&quot;bgcolor&quot;:&quot;white&quot;,&quot;pinterest_enabled&quot;:false,&quot;caption&quot;:null,&quot;credit&quot;:&quot;Paige Vickers/Vox&quot;,&quot;focal_area&quot;:{&quot;top_left_x&quot;:1140,&quot;top_left_y&quot;:535,&quot;bottom_right_x&quot;:1446,&quot;bottom_right_y&quot;:841},&quot;bounds&quot;:[0,0,1920,1080],&quot;uploaded_size&quot;:{&quot;width&quot;:1920,&quot;height&quot;:1080},&quot;focal_point&quot;:null,&quot;image_id&quot;:73118268,&quot;alt_text&quot;:&quot;An illustration of a giant paintbrush painting Earth green.&quot;,&quot;picture_element&quot;:{&quot;loading&quot;:&quot;eager&quot;,&quot;html&quot;:{},&quot;alt&quot;:&quot;An illustration of a giant paintbrush painting Earth green.&quot;,&quot;default&quot;:{&quot;srcset&quot;:&quot;https://cdn.vox-cdn.com/thumbor/70Mi1AT5TzccMvAAjtG3D3upnC4=/0x0:1920x1080/320x240/filters:focal(1140x535:1446x841)/cdn.vox-cdn.com/uploads/chorus_image/image/73118268/Vox_PaigeVickers_Benji.0.jpeg 320w, https://cdn.vox-cdn.com/thumbor/bofH5DG5Vt5y91vLIKBt2ZtA1cg=/0x0:1920x1080/620x465/filters:focal(1140x535:1446x841)/cdn.vox-cdn.com/uploads/chorus_image/image/73118268/Vox_PaigeVickers_Benji.0.jpeg 620w, https://cdn.vox-cdn.com/thumbor/UzQRyTEhH64xng2emkD0FiR_x9E=/0x0:1920x1080/920x690/filters:focal(1140x535:1446x841)/cdn.vox-cdn.com/uploads/chorus_image/image/73118268/Vox_PaigeVickers_Benji.0.jpeg 920w, https://cdn.vox-cdn.com/thumbor/ypxZABnODgiSqG6nJlkszwjc-BI=/0x0:1920x1080/1220x915/filters:focal(1140x535:1446x841)/cdn.vox-cdn.com/uploads/chorus_image/image/73118268/Vox_PaigeVickers_Benji.0.jpeg 1220w, https://cdn.vox-cdn.com/thumbor/h4PvDW_sSIqaJYezEyCo1YHoyhI=/0x0:1920x1080/1520x1140/filters:focal(1140x535:1446x841)/cdn.vox-cdn.com/uploads/chorus_image/image/73118268/Vox_PaigeVickers_Benji.0.jpeg 1520w&quot;,&quot;webp_srcset&quot;:&quot;https://cdn.vox-cdn.com/thumbor/wq5T1OJEFVFlU1gmT6kOtLq6014=/0x0:1920x1080/320x240/filters:focal(1140x535:1446x841):format(webp)/cdn.vox-cdn.com/uploads/chorus_image/image/73118268/Vox_PaigeVickers_Benji.0.jpeg 320w, https://cdn.vox-cdn.com/thumbor/p9rdOsUWMqqDHKOD-6iQSbRTpB4=/0x0:1920x1080/620x465/filters:focal(1140x535:1446x841):format(webp)/cdn.vox-cdn.com/uploads/chorus_image/image/73118268/Vox_PaigeVickers_Benji.0.jpeg 620w, https://cdn.vox-cdn.com/thumbor/njnNDR8DkXV00c4rzGz61n0rsd0=/0x0:1920x1080/920x690/filters:focal(1140x535:1446x841):format(webp)/cdn.vox-cdn.com/uploads/chorus_image/image/73118268/Vox_PaigeVickers_Benji.0.jpeg 920w, https://cdn.vox-cdn.com/thumbor/J830QofNB74bzhWasrfrOkPVOyQ=/0x0:1920x1080/1220x915/filters:focal(1140x535:1446x841):format(webp)/cdn.vox-cdn.com/uploads/chorus_image/image/73118268/Vox_PaigeVickers_Benji.0.jpeg 1220w, https://cdn.vox-cdn.com/thumbor/8R9iKNA-kw1fX4FeHpnv79nHG5A=/0x0:1920x1080/1520x1140/filters:focal(1140x535:1446x841):format(webp)/cdn.vox-cdn.com/uploads/chorus_image/image/73118268/Vox_PaigeVickers_Benji.0.jpeg 1520w&quot;,&quot;media&quot;:null,&quot;sizes&quot;:&quot;(min-width: 809px) 485px, (min-width: 600px) 60vw, 100vw&quot;,&quot;fallback&quot;:&quot;https://cdn.vox-cdn.com/thumbor/hFuXvl8jF9v5UNGY-J13XPOHQmw=/0x0:1920x1080/1200x900/filters:focal(1140x535:1446x841)/cdn.vox-cdn.com/uploads/chorus_image/image/73118268/Vox_PaigeVickers_Benji.0.jpeg&quot;},&quot;art_directed&quot;:[]}},&quot;image_is_placeholder&quot;:false,&quot;image_is_hidden&quot;:false,&quot;network&quot;:&quot;vox&quot;,&quot;omits_labels&quot;:false,&quot;optimizable&quot;:false,&quot;promo_headline&quot;:&quot;The Earth is getting greener. Hurray?&quot;,&quot;recommended_count&quot;:0,&quot;recs_enabled&quot;:false,&quot;slug&quot;:&quot;down-to-earth/2024/2/7/24057308/earth-global-greening-climate-change-carbon&quot;,&quot;dek&quot;:&quot;Humans are literally changing the color of the planet. Scientists are worried.&quot;,&quot;homepage_title&quot;:&quot;The Earth is getting greener. Hurray?&quot;,&quot;homepage_description&quot;:&quot;Humans are literally changing the color of the planet. Scientists are worried.&quot;,&quot;show_homepage_description&quot;:false,&quot;title_display&quot;:&quot;The Earth is getting greener. Hurray?&quot;,&quot;pull_quote&quot;:null,&quot;voxcreative&quot;:false,&quot;show_entry_time&quot;:true,&quot;show_dates&quot;:true,&quot;paywalled_content&quot;:false,&quot;paywalled_content_box_logo_url&quot;:&quot;&quot;,&quot;paywalled_content_page_logo_url&quot;:&quot;&quot;,&quot;paywalled_content_main_url&quot;:&quot;&quot;,&quot;article_footer_body&quot;:&quot;At Vox, we believe that clarity is power, and that power shouldn’t only be available to those who can afford to pay. That’s why we keep our work free. Millions rely on Vox’s clear, high-quality journalism to understand the forces shaping today’s world. <a href=\&quot;http://vox.com/pages/support-now?itm_campaign=evergreen&amp;itm_medium=site&amp;itm_source=footer\&quot;>Support our mission and help keep Vox free for all by making a financial contribution to Vox today.</a> \r\n&quot;,&quot;article_footer_header&quot;:&quot;<a href=\&quot;http://vox.com/pages/support-now?itm_campaign=evergreen&amp;itm_medium=site&amp;itm_source=footer\&quot;>Will you help keep Vox free for all?</a>&quot;,&quot;use_article_footer&quot;:true,&quot;article_footer_cta_annual_plans&quot;:&quot;{\r\n  \&quot;default_plan\&quot;: 1,\r\n  \&quot;plans\&quot;: [\r\n    {\r\n      \&quot;amount\&quot;: 50,\r\n      \&quot;plan_id\&quot;: 99546\r\n    },\r\n    {\r\n      \&quot;amount\&quot;: 100,\r\n      \&quot;plan_id\&quot;: 99547\r\n    },\r\n    {\r\n      \&quot;amount\&quot;: 150,\r\n      \&quot;plan_id\&quot;: 99548\r\n    },\r\n    {\r\n      \&quot;amount\&quot;: 200,\r\n      \&quot;plan_id\&quot;: 99549\r\n    }\r\n  ]\r\n}&quot;,&quot;article_footer_cta_button_annual_copy&quot;:&quot;year&quot;,&quot;article_footer_cta_button_copy&quot;:&quot;Yes, I'll give&quot;,&quot;article_footer_cta_button_monthly_copy&quot;:&quot;month&quot;,&quot;article_footer_cta_default_frequency&quot;:&quot;monthly&quot;,&quot;article_footer_cta_monthly_plans&quot;:&quot;{\r\n  \&quot;default_plan\&quot;: 0,\r\n  \&quot;plans\&quot;: [\r\n    {\r\n      \&quot;amount\&quot;: 5,\r\n      \&quot;plan_id\&quot;: 99543\r\n    },\r\n    {\r\n      \&quot;amount\&quot;: 10,\r\n      \&quot;plan_id\&quot;: 99544\r\n    },\r\n    {\r\n      \&quot;amount\&quot;: 25,\r\n      \&quot;plan_id\&quot;: 99545\r\n    },\r\n    {\r\n      \&quot;amount\&quot;: 50,\r\n      \&quot;plan_id\&quot;: 46947\r\n    }\r\n  ]\r\n}&quot;,&quot;article_footer_cta_once_plans&quot;:&quot;{\r\n  \&quot;default_plan\&quot;: 0,\r\n  \&quot;plans\&quot;: [\r\n    {\r\n      \&quot;amount\&quot;: 20,\r\n      \&quot;plan_id\&quot;: 69278\r\n    },\r\n    {\r\n      \&quot;amount\&quot;: 50,\r\n      \&quot;plan_id\&quot;: 48880\r\n    },\r\n    {\r\n      \&quot;amount\&quot;: 100,\r\n      \&quot;plan_id\&quot;: 46607\r\n    },\r\n    {\r\n      \&quot;amount\&quot;: 250,\r\n      \&quot;plan_id\&quot;: 46946\r\n    }\r\n  ]\r\n}&quot;,&quot;use_article_footer_cta_read_counter&quot;:true,&quot;use_article_footer_cta&quot;:true,&quot;groups&quot;:[{&quot;base_type&quot;:&quot;EntryGroup&quot;,&quot;id&quot;:103390,&quot;timestamp&quot;:1708608604,&quot;title&quot;:&quot;Down to Earth&quot;,&quot;type&quot;:&quot;SiteGroup&quot;,&quot;url&quot;:&quot;https://www.vox.com/down-to-earth&quot;,&quot;slug&quot;:&quot;down-to-earth&quot;,&quot;community_logo&quot;:&quot;\r\n<svg width=\&quot;386px\&quot; height=\&quot;385px\&quot; viewBox=\&quot;0 0 386 385\&quot; version=\&quot;1.1\&quot; xmlns=\&quot;http://www.w3.org/2000/svg\&quot; xmlns:xlink=\&quot;http://www.w3.org/1999/xlink\&quot; >\r\n    \r\n    <title>vox-mark</title>\r\n    \r\n    <defs></defs>\r\n    <g id=\&quot;Page-1\&quot; stroke=\&quot;none\&quot; stroke-width=\&quot;1\&quot; fill=\&quot;none\&quot; fill-rule=\&quot;evenodd\&quot; >\r\n        <path d=\&quot;M239.811,0 L238.424,6 L259.374,6 C278.011,6 292.908,17.38 292.908,43.002 C292.908,56.967 287.784,75.469 276.598,96.888 L182.689,305.687 L159.283,35.693 C159.283,13.809 168.134,6 191.88,6 L205.854,6 L207.247,0 L1.409,0 L0,6 L13.049,6 C28.88,6 35.863,15.885 37.264,34.514 L73.611,385 L160.221,385 L304.525,79.217 C328.749,31.719 349.237,6 372.525,6 L384.162,6 L385.557,0 L239.811,0\&quot; id=\&quot;vox-mark\&quot; fill=\&quot;#444745\&quot; ></path>\r\n    </g>\r\n</svg>&quot;,&quot;community_name&quot;:&quot;Vox&quot;,&quot;community_url&quot;:&quot;https://www.vox.com/&quot;,&quot;cross_community&quot;:false,&quot;entry_count&quot;:176,&quot;always_show&quot;:false,&quot;description&quot;:&quot;The biodiversity crisis, explained&quot;,&quot;disclosure&quot;:&quot;&quot;,&quot;cover_image_url&quot;:&quot;&quot;,&quot;cover_image&quot;:null,&quot;title_image_url&quot;:&quot;https://cdn.vox-cdn.com/uploads/chorus_asset/file/22428046/hublogo.png&quot;,&quot;intro_image&quot;:null,&quot;four_up_see_more_text&quot;:&quot;View All&quot;,&quot;primary&quot;:true},{&quot;base_type&quot;:&quot;EntryGroup&quot;,&quot;id&quot;:27530,&quot;timestamp&quot;:1708608604,&quot;title&quot;:&quot;Climate&quot;,&quot;type&quot;:&quot;SiteGroup&quot;,&quot;url&quot;:&quot;https://www.vox.com/climate&quot;,&quot;slug&quot;:&quot;climate&quot;,&quot;community_logo&quot;:&quot;\r\n<svg width=\&quot;386px\&quot; height=\&quot;385px\&quot; viewBox=\&quot;0 0 386 385\&quot; version=\&quot;1.1\&quot; xmlns=\&quot;http://www.w3.org/2000/svg\&quot; xmlns:xlink=\&quot;http://www.w3.org/1999/xlink\&quot; >\r\n    \r\n    <title>vox-mark</title>\r\n    \r\n    <defs></defs>\r\n    <g id=\&quot;Page-1\&quot; stroke=\&quot;none\&quot; stroke-width=\&quot;1\&quot; fill=\&quot;none\&quot; fill-rule=\&quot;evenodd\&quot; >\r\n        <path d=\&quot;M239.811,0 L238.424,6 L259.374,6 C278.011,6 292.908,17.38 292.908,43.002 C292.908,56.967 287.784,75.469 276.598,96.888 L182.689,305.687 L159.283,35.693 C159.283,13.809 168.134,6 191.88,6 L205.854,6 L207.247,0 L1.409,0 L0,6 L13.049,6 C28.88,6 35.863,15.885 37.264,34.514 L73.611,385 L160.221,385 L304.525,79.217 C328.749,31.719 349.237,6 372.525,6 L384.162,6 L385.557,0 L239.811,0\&quot; id=\&quot;vox-mark\&quot; fill=\&quot;#444745\&quot; ></path>\r\n    </g>\r\n</svg>&quot;,&quot;community_name&quot;:&quot;Vox&quot;,&quot;community_url&quot;:&quot;https://www.vox.com/&quot;,&quot;cross_community&quot;:false,&quot;entry_count&quot;:2744,&quot;always_show&quot;:false,&quot;description&quot;:&quot;Vox's coverage of climate change, renewable energy, conservation, and other environmental issues&quot;,&quot;disclosure&quot;:&quot;&quot;,&quot;cover_image_url&quot;:&quot;&quot;,&quot;cover_image&quot;:null,&quot;title_image_url&quot;:&quot;&quot;,&quot;intro_image&quot;:null,&quot;four_up_see_more_text&quot;:&quot;View All&quot;,&quot;primary&quot;:false}],&quot;layout&quot;:&quot;&quot;,&quot;featured_placeable&quot;:false,&quot;video_placeable&quot;:false,&quot;disclaimer&quot;:null,&quot;volume_placement&quot;:&quot;lede&quot;,&quot;video_autoplay&quot;:false,&quot;youtube_url&quot;:&quot;http://bit.ly/voxyoutube&quot;,&quot;facebook_video_url&quot;:&quot;&quot;,&quot;play_in_modal&quot;:true,&quot;user_preferences_for_privacy_enabled&quot;:false,&quot;show_branded_logos&quot;:true,&quot;uses_video_lede&quot;:false,&quot;image_brightness&quot;:&quot;image-dark&quot;,&quot;display_logo_lockup&quot;:false,&quot;svg_logo_data&quot;:&quot;<svg id=\&quot;Layer_1\&quot; xmlns=\&quot;http://www.w3.org/2000/svg\&quot; viewBox=\&quot;0 0 242 121\&quot;><path fill=\&quot;#ffffff\&quot; d=\&quot;M110.674 3.528h3.474L114.564 2H71.63l-.418 1.528h6.253c5.418 0 9.726 3.75 9.726 11.255 0 4.168-1.8 9.587-4.72 16.118L54.82 92.32l-6.81-79.756c-.556-6.252 2.5-9.03 9.59-9.03h4.027L62.042 2H1.6l-.557 1.528h3.89c4.725 0 6.532 2.918 7.087 8.615l10.7 103.1h25.427l42.518-90.038c6.392-13.48 13.2-21.677 20.01-21.677zm-5.002 112.27c-3.89 0-6.253-1.25-6.253-7.642 0-8.06 2.91-23.76 6.11-38.072.41 6.67 5 13.2 11.81 13.2 1.67 0 3.06-.138 4.44-.417-6.26 27.236-8.76 32.932-16.12 32.932zm121.024-54.19c8.06 0 13.2-6.67 13.2-14.173 0-6.392-4.585-11.116-11.115-11.116-11.81 0-17.36 9.31-27.09 26.53-2.08-10.7-6.94-24.73-19.45-24.73-14.03 0-30.15 20.01-45.02 32.37-6.67 5.56-14.17 9.17-20.15 9.17-6.11 0-9.72-6.26-9.72-17.23 4.31-17.93 6.67-22.65 13.34-22.65 4.59 0 6.53 2.64 6.53 8.06 0 5.69-1.25 15.42-3.75 27.51 6.67-2.09 16.68-10.42 25.01-19.45-4.44-10.56-13.89-17.79-27.65-17.79-25.42 0-47.66 22.78-47.66 48.35 0 17.65 12.51 30.984 32.1 30.984 32.38 0 45.86-28.066 45.86-47.52 0-2.78-.14-4.86-.42-7.364C155.717 57.14 162.108 52 167.388 52c5.975 0 10.7 15.007 15.423 37.657-4.17 4.58-8.34 13.474-10.42 15.002-.836-8.06-6.115-13.06-13.2-13.06-7.92 0-13.48 7.5-13.48 13.893 0 7.226 5 11.95 11.53 11.95 13.76 0 17.65-13.062 26.265-24.595 2.64 12.363 8.754 24.59 19.313 24.59 12.506 0 24.178-10.7 30.15-18.34l-1.11-1.81c-3.89 3.753-7.642 6.254-11.95 6.254-7.78 0-13.34-16.81-17.645-37.1 2.5-3.47 6.53-12.225 9.31-15.28 1.95 3.612 5.978 10.42 15.15 10.42z\&quot;/></svg>&quot;}">
       <!-- end of .contribute--frequency-container -->

      <div>
        <p><label tabindex="0" role="radio" aria-checked="true">
                
                <p>
                  <span>$5</span><span>/month</span>
                </p>
              </label>
            
              <label tabindex="0" role="radio" aria-checked="true">
                
                <p>
                  <span>$10</span><span>/month</span>
                </p>
              </label>
            
              <label tabindex="0" role="radio" aria-checked="true">
                
                <p>
                  <span>$25</span><span>/month</span>
                </p>
              </label>
            
              <label tabindex="0" role="radio" aria-checked="true">
                
                <p>
                  <span>$50</span><span>/month</span>
                </p>
              </label>
            

            <label tabindex="0">
              
              <span>Other</span>
            </label>
          </p>
        </div>

        

        <a href="https://vox.memberful.com/checkout?plan=" id="contribute--submit">
          <p>
            Yes, I'll give $5<span>/month</span>
          </p>
        </a>

        <p>
          Yes, I'll give $5<span>/month</span>
        </p>

        <div>
            <p>
              <span>
                We accept credit card, Apple Pay, and
              </span>
              <span>
                Google Pay. You can also contribute via
              </span>
            </p>
            <p><a href="https://www.paypal.com/donate/?hosted_button_id=VSP4PYJX98SHL" target="_blank">
              <img src="https://cdn.vox-cdn.com/uploads/chorus_asset/file/22734206/paypal_logo.png" alt="" width="136" height="42">
            </a>
          </p></div>

      </div> <!-- end of .right-column -->

 <!-- end of .c-article-footer-cta -->

</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Please Make Your Table Headings Sticky (310 pts)]]></title>
            <link>https://btxx.org/posts/Please_Make_Your_Table_Headings_Sticky/</link>
            <guid>39488836</guid>
            <pubDate>Sat, 24 Feb 2024 03:16:51 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://btxx.org/posts/Please_Make_Your_Table_Headings_Sticky/">https://btxx.org/posts/Please_Make_Your_Table_Headings_Sticky/</a>, See on <a href="https://news.ycombinator.com/item?id=39488836">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="pagebody" role="main" class="page">
<p>I often stumble upon large data sets or table layouts across the web. When these tables contain hundreds of rows of content, things become problematic once you start to scroll...</p>

<p><video width="100%" controls="">
  <source src="https://btxx.org/posts/Please_Make_Your_Table_Headings_Sticky/not-fixed-header-tables.mp4" type="video/mp4">
Your browser does not support the video tag.
</video> </p>

<p>Look at that table header disappear! Now, if I scroll all the way down to item #300 (for example) will I remember what each column's data is associated with? If this is my first time looking at this table - probably not. Luckily we can fix this (no pun intended!) with a tiny amount of CSS.</p>



<p>Check it out:</p>

<p><video width="100%" controls="">
  <source src="https://btxx.org/ikiwiki/git/fixed-header-tables.mp4" type="video/mp4">
Your browser does not support the video tag.
</video> </p>

<p>Pretty awesome, right? It might look like magic but it's actually very easy to implement. You only need to add 2 CSS properties on your <code>thead</code>:</p>

<pre><code>position: sticky;
top: 0;
</code></pre>

<p>That's it! Best of all, <code>sticky</code> has <a href="https://caniuse.com/?search=sticky">~96% global support</a> which means this isn't some "bleeding-edge" property and can safely support a ton of browsers. Not to mention the improved experience for your end-users!</p>

<p>You can view a live demo of this table on the <a href="https://codepen.io/bradleytaunt/pen/bGZyJBj">CodePen example pen</a>.</p>

<p>If you found this interesting, feel free to check out my other table-focused post: <a href="https://btxx.org/posts/tables/">Making Tables Responsive With Minimal CSS</a></p>

</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[How to lose two jobs in one year (225 pts)]]></title>
            <link>https://jbennetcodes.medium.com/how-to-lose-two-jobs-in-one-year-e8e428702b91</link>
            <guid>39488833</guid>
            <pubDate>Sat, 24 Feb 2024 03:16:05 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://jbennetcodes.medium.com/how-to-lose-two-jobs-in-one-year-e8e428702b91">https://jbennetcodes.medium.com/how-to-lose-two-jobs-in-one-year-e8e428702b91</a>, See on <a href="https://news.ycombinator.com/item?id=39488833">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><div><a rel="noopener follow" href="https://jbennetcodes.medium.com/?source=post_page-----e8e428702b91--------------------------------"><div aria-hidden="false"><p><img alt="Irina Truong" src="https://miro.medium.com/v2/resize:fill:88:88/1*WG91mjIEby9z6Yu0ehXv9Q.jpeg" width="44" height="44" loading="lazy" data-testid="authorPhoto"></p></div></a></div><p id="3b79">and learn to accept imperfection</p><figure><figcaption>generated with <a href="https://replicate.com/stability-ai/stable-diffusion" rel="noopener ugc nofollow" target="_blank">https://replicate.com/stability-ai/stable-diffusion</a></figcaption></figure><h2 id="4b17">Elastic</h2><p id="c325">The year 2022 was stressful. My family was trying and failing to buy a house, because the housing market was insane. Of course, this was a first-world problem. On February 24, 2022, the Russian army invaded Ukraine, my home country, which I left in 2010, but still consider home. Friends and relatives lost homes, jobs, and any security in their future.</p><p id="22cc">But to me personally, what happened on November 30 of that year was the biggest shock of all.</p><p id="5d30">I was in the middle of my work day. I noticed an email dropped in my inbox, and I opened Gmail to read it. The email cut right to the chase:</p><blockquote><p id="d887">Hello,</p><p id="7eac">Earlier today, Elastic announced that we are reducing our team by 13%, and unfortunately, you’ve been included in this action.</p></blockquote><p id="c730">The rest of it was details. I love details. The details are great. They give you the sense of being grounded in reality, even though the brain is trying to reject it. However, I was not able to process the details just yet. I had too many feelings to get over. So I screamed and cried.</p><p id="d2a6">My husband ran over and started asking questions; he probably thought someone died. To me, it was almost like someone did: my identity as a software engineer. I had been a software engineer for twenty years, most of them in senior and lead positions, and I believed that good engineers don’t get laid off.</p><p id="5b2e">I was naive.</p><p id="d8eb">As layoffs go, Elastic was great. I was out of work immediately, but I was kept on payroll and benefits for the month of December. I also received 14 weeks of severance pay and 6 months of healthcare coverage.</p><p id="a9c9">I cried for a few days, but I had to pull myself together and start looking for a new job ASAP. Of course, we postponed our house search. We had savings, but I didn’t want to dip into them, and I also was the one to provide medical insurance coverage for my whole family, because my husband worked as a contractor with no benefits.</p><p id="1759">I had a lot of connections with my ex-coworkers. One of my coworkers from <a href="https://www.parse.ly/" rel="noopener ugc nofollow" target="_blank">Parse.ly</a> was just hired at <a href="https://www.coiled.io/" rel="noopener ugc nofollow" target="_blank">Coiled</a>, a data engineering startup. Coiled is the company behind Dask, a distributed data processing framework written in Python. I have been interested in Dask for a long time, so I applied for one of their open positions. I was able to pass their interviews, and I started a new job as a backend engineer in Coiled in January 2023.</p><h2 id="eec2">Coiled</h2><p id="e42f">I learned a lot while working at Coiled, but it was very different from Elastic. Elastic was a large company, where every process was structured and formalized. Coiled was a small startup where directions changed all the time, and nothing was ever clear-cut. In addition, I still felt shell-shocked after the Elastic layoff. I no longer had confidence that I was a good engineer. So I thought, perhaps I should keep a low profile, listen more, speak less, work hard, and concentrate on being as useful as possible.</p><p id="d99e">To this day, I don’t know what went wrong with Coiled. Perhaps the decision to keep a low profile was wrong. Perhaps I wasn’t useful enough. I never felt that the more senior engineers on the team fully accepted me; I didn’t have a feeling of belonging. Still, some of the teammates I worked with were great, we collaborated very well together, and I believed that the rest of the team would accept me too, once I became more experienced with the project and could deliver more value.</p><p id="96a0">In July 2023, my manager started a regularly scheduled 1:1 (on Zoom, because everyone was remote) with the following phrase:</p><blockquote><p id="b1d8">I have bad news. We have to let you go.</p></blockquote><p id="eb31">I wish I had taken the news well (I didn’t). Still, the second time around the blow was less sharp. I guess humans can get used to anything.</p><h2 id="9b9b">A mass</h2><p id="b458">This time, I didn’t have as much protection in terms of severance pay. Coiled gave me 6 weeks —not too bad for a startup. I started searching for a new job immediately, but I was not as lucky. It took me a lot longer to find a new job. It was August 2023, more and more companies embraced the frugal mindset, and the market was flooded with engineers laid off from Twitter, Google, Meta, etc.</p><p id="3f14">While I was looking, something else happened.</p><p id="3d51">I needed to have surgery (not life-threatening). With Coiled, I was covered by medical insurance until the end of August, and I asked the surgeon to please try and schedule it while still within coverage. As part of a pre-surgery checkup, the doctor sent me to have a lung X-ray. An hour after the X-ray appointment, when I had barely got back, I received a call from the doctor.</p><blockquote><p id="0684">“The X-ray shows a 3 cm mass in your lung. You need to schedule a lung CT as soon as you can. You might have to postpone this surgery.”</p></blockquote><p id="1a12">I asked what kind of mass it was; she said it was not clear without further testing. I called the X-ray place. The earliest CT date they could give me was ten days away.</p><p id="30ba">They were the longest ten days of my life. I kept thinking how, if the worst came to pass, my kids (daughter and son, then 9 and 3 years old) would have to grow up without their mom, and how hard it would be for them to see me dying. I was still studying, doing job interviews, and handling recruiter phone calls.</p><p id="2c59">Fun stuff, eh?</p><p id="0f51">After the CT, I received a phone call from my doctor within hours:</p><blockquote><p id="9a00">“Ms Truong, can you talk? It’s good news.”</p><p id="5dc2">“Yes, of course”.</p><p id="bc00">“The mass in your lung, it’s scar tissue. Have you ever had pneumonia?”</p><p id="8b2a">“Yes, I had it as a student, around twenty years old.”</p><p id="431c">“Well, that’s probably what did it. Everything is well. You are clear to have your surgery.”</p></blockquote><p id="0c4e">It’s hard to describe what I felt. I was very happy, but it was more than just being happy for myself. I was (irrationally) happy for my kids. They won’t have to grow up orphans. Not this time, Universe. Not this time.</p><p id="2c5c">The surgery happened on schedule, within the insurance coverage, and was a success. The recovery was painful and took weeks, but that was expected.</p><p id="a2ce">Something else changed. Suddenly, the two layoffs stopped being of any consequence. They quite frankly didn’t matter anymore.</p><blockquote><p id="d018">“Frankly, my dear, I don’t give a damn.”</p><p id="12cd">Rhett Butler, Gone with the Wind (1939 movie)</p></blockquote><h2 id="7986">Silence</h2><p id="29eb">The ups and downs of 2022 and 2023 shook my confidence badly. I didn’t feel like I could or should write articles anymore. I was still wondering if I could be of any value to any company again, or anyone else, for that matter. The happiness of not dying gave me some peace of mind, but it wasn’t enough to overcome all the other things that happened. So I stopped writing about software development, data analysis, or anything else; I didn’t sign up for any conferences or meetups; I didn’t regain the ability or desire to speak in public. I hid.</p><p id="6ab2">This is not a story of a hero. This is not a story about conquering all obstacles. This is a story about imperfection. About struggling, stumbling, and blundering through life.</p><p id="d505">I accepted a few axioms:</p><ul><li id="28f0">Anything can happen to anyone. War? It can happen to me. Cancer? It can happen to me. I can’t control everything.</li><li id="f2b4">I will never be able to protect my kids from everything either.</li><li id="6cde">My work is only part of my identity.</li><li id="30bc">I am not, and will never be perfect.</li><li id="95b5">Change is inevitable.</li><li id="fee9">I’m not alone.</li></ul><p id="cd58">A lot of friends stepped up and helped with advice, referrals, or simply words of support and encouragement during this time. I’m immensely grateful to all of them.</p><p id="ce35">The war in Ukraine is still ongoing. My family still doesn’t own a home. However, I did eventually find a new job. I have a great team, a challenging product to work on, and a source of income again. I’m absorbed in my work and starting to breathe again. And the feeling that I have something to say that’s worth listening to is starting to return.</p><p id="c7ca">Thank you for listening.</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[The Ultimate Guide to PostgreSQL Data Change Tracking (123 pts)]]></title>
            <link>https://exaspark.medium.com/the-ultimate-guide-to-postgresql-data-change-tracking-c3fa88779572</link>
            <guid>39488719</guid>
            <pubDate>Sat, 24 Feb 2024 02:54:22 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://exaspark.medium.com/the-ultimate-guide-to-postgresql-data-change-tracking-c3fa88779572">https://exaspark.medium.com/the-ultimate-guide-to-postgresql-data-change-tracking-c3fa88779572</a>, See on <a href="https://news.ycombinator.com/item?id=39488719">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><div><a rel="noopener follow" href="https://exaspark.medium.com/?source=post_page-----c3fa88779572--------------------------------"><div aria-hidden="false"><p><img alt="exAspArk" src="https://miro.medium.com/v2/resize:fill:88:88/1*xykLd-j1bsw7K_ZFcSxe1Q.png" width="44" height="44" loading="lazy" data-testid="authorPhoto"></p></div></a></div><p id="0b01">PostgreSQL, one of the most popular databases, was named DBMS of the Year 2023 by <a href="https://db-engines.com/en/blog_post/106" rel="noopener ugc nofollow" target="_blank">DB-Engines Ranking</a> and is used more than any other database among startups according to <a href="https://www.hntrends.com/2024/january.html?compare=SQL+Server" rel="noopener ugc nofollow" target="_blank">HN Hiring Trends</a>.</p><figure><figcaption>PostgreSQL is the most popular database among startups</figcaption></figure><p id="c97f">The SQL standard has included features related to <a href="https://en.wikipedia.org/wiki/Temporal_database" rel="noopener ugc nofollow" target="_blank">temporal databases</a> since 2011, which allow storing data changes over time rather than just the current data state. However, relational databases don’t completely follow the standards. In the case of PostgreSQL, it doesn’t support these features, even though there has been a submitted <a href="https://www.postgresql.org/message-id/flat/CALAY4q-cXCD0r4OybD%3Dw7Hr7F026ZUY6%3DLMsVPUe6yw_PJpTKQ%40mail.gmail.com" rel="noopener ugc nofollow" target="_blank">patch</a> with some discussions.</p><p id="59ed">Let’s explore five alternative methods of data change tracking in PostgreSQL available to us in 2024.</p><h2 id="82a0">Triggers and Audit Table</h2><figure><figcaption>A PostgreSQL trigger with an audit table</figcaption></figure><p id="c0ea">PostgreSQL allows adding triggers with custom procedural SQL code performed on row changes with <code>INSERT</code>, <code>UPDATE</code>, and <code>DELETE</code> queries. The official PostgreSQL wiki describes a generic <a href="https://wiki.postgresql.org/wiki/Audit_trigger" rel="noopener ugc nofollow" target="_blank">audit trigger function</a>. Let’s have a quick look at a simplified example.</p><p id="b5ab">First, create a table called <code>logged_actions</code> in a separate schema called <code>audit</code>:</p><pre><span id="ca96">CREATE schema audit;<p>CREATE TABLE audit.logged_actions (<br>  schema_name TEXT NOT NULL,<br>  table_name TEXT NOT NULL,<br>  user_name TEXT,<br>  action_tstamp TIMESTAMP WITH TIME ZONE NOT NULL DEFAULT current_timestamp,<br>  action TEXT NOT NULL CHECK (action IN ('I','D','U')),<br>  original_data TEXT,<br>  new_data TEXT,<br>  query TEXT<br>);</p></span></pre><p id="1c7d">Next, create a function to insert audit records and establish a trigger on a table you wish to track, such as <code>my_table</code>:</p><pre><span id="8630">CREATE OR REPLACE FUNCTION audit.if_modified_func() RETURNS TRIGGER AS $body$<br>BEGIN<br>  IF (TG_OP = 'UPDATE') THEN<br>    INSERT INTO audit.logged_actions (schema_name,table_name,user_name,action,original_data,new_data,query)<br>    VALUES (TG_TABLE_SCHEMA::TEXT,TG_TABLE_NAME::TEXT,session_user::TEXT,substring(TG_OP,1,1),ROW(OLD.*),ROW(NEW.*),current_query());<br>    RETURN NEW;<br>  elsif (TG_OP = 'DELETE') THEN<br>    INSERT INTO audit.logged_actions (schema_name,table_name,user_name,action,original_data,query)<br>    VALUES (TG_TABLE_SCHEMA::TEXT,TG_TABLE_NAME::TEXT,session_user::TEXT,substring(TG_OP,1,1),ROW(OLD.*),current_query());<br>    RETURN OLD;<br>  elsif (TG_OP = 'INSERT') THEN<br>    INSERT INTO audit.logged_actions (schema_name,table_name,user_name,action,new_data,query)<br>    VALUES (TG_TABLE_SCHEMA::TEXT,TG_TABLE_NAME::TEXT,session_user::TEXT,substring(TG_OP,1,1),ROW(NEW.*),current_query());<br>    RETURN NEW;<br>  END IF;<br>END;<br>$body$<br>LANGUAGE plpgsql;<p>CREATE TRIGGER my_table_if_modified_trigger<br>AFTER INSERT OR UPDATE OR DELETE ON my_table<br>FOR EACH ROW EXECUTE PROCEDURE if_modified_func();</p></span></pre><p id="dd2d">Once it’s done, row changes made in <code>my_table</code> will create records in <code>audit.logged_actions</code>:</p><pre><span id="ca4a">INSERT INTO my_table(x,y) VALUES (1, 2);<br>SELECT * FROM audit.logged_actions;</span></pre><p id="9d98">If you want to further improve this solution by using JSONB columns instead of TEXT, ignoring changes in certain columns, pausing auditing a table, and so on, check out the SQL example in this <a href="https://github.com/2ndQuadrant/audit-trigger" rel="noopener ugc nofollow" target="_blank">audit-trigger</a> repo and its forks.</p><h2 id="ef6d">Downsides</h2><ul><li id="3a13">Performance. Triggers add performance overhead by inserting additional records synchronously on every <code>INSERT</code>, <code>UPDATE</code>, and <code>DELETE</code> operation.</li><li id="54dc">Security. Anyone with superuser access can modify the triggers and make unnoticed data changes. It is also recommended to make sure that records in the audit table cannot be modified or removed.</li><li id="f890">Maintenance. Managing complex triggers across many constantly changing tables can become cumbersome. Making a small mistake in an SQL script can break queries or data change tracking functionality.</li></ul><h2 id="cc80">Triggers and Notify/Listen</h2><figure><figcaption>A PostgreSQL trigger with Notify</figcaption></figure><p id="570f">This approach is similar to the previous one but instead of writing data changes in the audit table directly, we pass them through a pub/sub mechanism through a trigger to another system dedicated to reading and storing these data changes:</p><pre><span id="b228">CREATE OR REPLACE FUNCTION if_modified_func() RETURNS TRIGGER AS $body$<br>BEGIN<br>  IF (TG_OP = 'UPDATE') THEN<br>    PEFORM pg_notify('data_changes', json_build_object(<br>      'schema_name', TG_TABLE_SCHEMA::TEXT,<br>      'table_name', TG_TABLE_NAME::TEXT,<br>      'user_name', session_user::TEXT,<br>      'action', substring(TG_OP,1,1),<br>      'original_data', jsonb_build(OLD),<br>      'new_data', jsonb_build(NEW)<br>    )::TEXT);<br>    RETURN NEW;<br>  elsif (TG_OP = 'DELETE') THEN<br>    PEFORM pg_notify('data_changes', json_build_object(<br>      'schema_name', TG_TABLE_SCHEMA::TEXT,<br>      'table_name', TG_TABLE_NAME::TEXT,<br>      'user_name', session_user::TEXT,<br>      'action', substring(TG_OP,1,1),<br>      'original_data', jsonb_build(OLD)<br>    )::TEXT);<br>    RETURN OLD;<br>  elsif (TG_OP = 'INSERT') THEN<br>    PEFORM pg_notify('data_changes', json_build_object(<br>      'schema_name', TG_TABLE_SCHEMA::TEXT,<br>      'table_name', TG_TABLE_NAME::TEXT,<br>      'user_name', session_user::TEXT,<br>      'action', substring(TG_OP,1,1),<br>      'new_data', jsonb_build(NEW)<br>    )::TEXT);<br>    RETURN NEW;<br>  END IF;<br>END;<br>$body$<br>LANGUAGE plpgsql;<p>CREATE TRIGGER my_table_if_modified_trigger<br>AFTER INSERT OR UPDATE OR DELETE ON my_table<br>FOR EACH ROW EXECUTE PROCEDURE if_modified_func();</p></span></pre><p id="7b86">Now it’s possible to run a separate process running as a worker that listens to messages containing data changes and stores them separately:</p><pre><span id="eeb5">LISTEN data_changes;</span></pre><h2 id="cffa">Downsides</h2><ul><li id="9bfb">“At most once” delivery<strong>.</strong> Listen/notify notifications are not persisted meaning if a listener disconnects, it may miss updates that happened before it reconnected again.</li><li id="5371">Payload size limit. Listen/notify messages have a maximum payload size of 8000 bytes by default. For larger payloads, it is recommended to store them in the DB audit table and send only references of the records.</li><li id="242a">Debugging. Troubleshooting issues related to triggers and listen/notify in a production environment can be challenging due to their asynchronous and distributed nature.</li></ul><h2 id="7a5b">Application-Level Tracking</h2><figure><figcaption>Application-level tracking with a PostgreSQL audit table</figcaption></figure><p id="17ed">If you have control over the codebase that connects and makes data changes in a PostgreSQL database, then one of the following options is also available to you:</p><ul><li id="5a3c">Manually record all data changes when issuing <code>INSERT</code>, <code>UPDATE</code>, and <code>DELETE</code> queries</li><li id="d590">Use existing open-source libraries that integrate with popular ORMs</li></ul><p id="5af4">For example, there is <a href="https://github.com/paper-trail-gem/paper_trail" rel="noopener ugc nofollow" target="_blank">paper_trail</a> for Ruby on Rails with ActiveRecord and <a href="https://github.com/jazzband/django-simple-history" rel="noopener ugc nofollow" target="_blank">django-simple-history</a> for Django. At a high level, they use callbacks or middlewares to insert additional records into an audit table. Here is a simplified example written in Ruby:</p><pre><span id="2aa4">class User &lt; ApplicationRecord<br>  after_commit :track_data_changes<p>  private</p><p>  def track_data_changes<br>    AuditRecord.create!(auditable: self, changes: changes)<br>  end<br>end</p></span></pre><p id="9927">On the application level, <a href="https://martinfowler.com/eaaDev/EventSourcing.html" rel="noopener ugc nofollow" target="_blank">Event Sourcing</a> can also be implemented with an append-only log as the source of truth. But it’s a separate, big, and exciting topic that deserves a separate blog post.</p><h2 id="13aa">Downsides</h2><ul><li id="7281">Reliability. Application-level data change tracking is not as accurate as database-level change tracking. For example, data changes made outside an app will not be tracked, developers may accidentally skip callbacks, or there could be data inconsistencies if a query changing the data has succeeded but a query inserting an audit record failed.</li><li id="d197">Performance. Manually capturing changes and inserting them in the database via callbacks leads to both runtime application and database overhead.</li><li id="f949">Scalability. These audit tables are usually stored in the same database and can quickly become unmanageable, which can require separating the storage, implementing declarative partitioning, and continuous archiving.</li></ul><h2 id="df40">Change Data Capture</h2><p id="7fef"><a href="https://en.wikipedia.org/wiki/Change_data_capture" rel="noopener ugc nofollow" target="_blank">Change Data Capture</a> (CDC) is a pattern of identifying and capturing changes made to data in a database and sending those changes to a downstream system. Most often it is used for <a href="https://en.wikipedia.org/wiki/Extract,_transform,_load" rel="noopener ugc nofollow" target="_blank">ETL</a> to send data to a data warehouse for analytical purposes.</p><p id="3fae">There are multiple approaches to implementing CDC. One of them, which doesn’t intersect with what we have already discussed, is a log-based CDC. With PostgreSQL, it is possible to connect to the <a href="https://www.postgresql.org/docs/current/wal-intro.html" rel="noopener ugc nofollow" target="_blank">Write-Ahead Log</a> (WAL) that is used for data durability, recovery, and replication to other instances.</p><figure><figcaption>CDC with PostgreSQL logical replication</figcaption></figure><p id="2f27">PostgreSQL supports two types of replications: physical replication and logical replication. The latter allows decoding WAL changes on a row level and filtering them out, for example, by table name. This is exactly what we need to implement data change tracking with CDC.</p><p id="5cad">Here are the basic steps necessary for retrieving data changes by using logical replication:</p><p id="af59">1. Set <code>wal_level</code> to <code>logical</code> in <code>postgresql.conf</code> and restart the database.</p><p id="258c">2. Create a publication like a “pub/sub channel” for receiving data changes:</p><pre><span id="0d95">CREATE PUBLICATION my_publication FOR ALL TABLES;</span></pre><p id="8f8c">3. Create a logical replication slot like a “cursor position” in the WAL:</p><pre><span id="dfec">SELECT * FROM pg_create_logical_replication_slot('my_replication_slot', 'wal2json')</span></pre><p id="11b9">4. Fetch the latest unread changes:</p><pre><span id="ea6c">SELECT * FROM pg_logical_slot_get_changes('my_replication_slot', NULL, NULL)</span></pre><p id="a137">To implement log-based CDC with PostgreSQL, I would recommend using the existing open-source solutions. The most popular one is <a href="https://github.com/debezium/debezium" rel="noopener ugc nofollow" target="_blank">Debezium</a>.</p><h2 id="0a5e">Downsides</h2><ul><li id="6b4d">Limited context. PostgreSQL WAL contains only low-level information about row changes and doesn’t include information about an SQL query that triggered the change, information about a user, or any application-specific context.</li><li id="aefd">Complexity. Implementing CDC adds a lot of system complexity. This involves running a server that connects to PostgreSQL as a replica, consumes data changes, and stores them somewhere.</li><li id="e7e7">Tuning. Running it in a production environment may require a deeper understanding of PostgreSQL internals and properly configuring the system. For example, periodically flushing the position for a replication slot to reclaim WAL disk space.</li></ul><h2 id="6aa3">Integrated Change Data Capture</h2><figure><figcaption>Integrated CDC with application context</figcaption></figure><p id="5539">To overcome the challenge of limited information about data changes stored in the WAL, we can use a clever approach of passing additional context to the WAL directly.</p><p id="6fbd">Here is a simple example of passing additional context on row changes:</p><pre><span id="8e85">CREATE OR REPLACE FUNCTION if_modified_func() RETURNS TRIGGER AS $body$<br>BEGIN<br>  PERFORM pg_logical_emit_message(true, 'my_message', 'ADDITIONAL_CONTEXT');<p>  IF (TG_OP = 'DELETE') THEN<br>    RETURN OLD;<br>  ELSE<br>    RETURN NEW;<br>  END IF;<br>END;<br>$body$<br>LANGUAGE plpgsql;</p><p>CREATE TRIGGER my_table_if_modified_trigger<br>AFTER INSERT OR UPDATE OR DELETE ON my_table<br>FOR EACH ROW EXECUTE PROCEDURE if_modified_func();</p></span></pre><p id="525c">Notice the <code>pg_logical_emit_message</code> function that was added to PostgreSQL as an internal function for plugins. It allows namespacing and emitting messages that will be stored in the WAL. Reading these messages became possible with the standard logical decoding plugin <code>pgoutput</code> since PostgreSQL v14.</p><p id="b783">There is an open-source project called <a href="https://github.com/BemiHQ/bemi" rel="noopener ugc nofollow" target="_blank">Bemi</a> which allows tracking not only low-level data changes but also reading any custom context with CDC and stitching everything together. Full disclaimer, I’m one of the core contributors.</p><p id="ee57">For example, it can integrate with popular ORMs and adapters to pass application-specific context with all data changes:</p><pre><span id="dca3">import { setContext } from "@bemi-db/prisma";<br>import express, { Request } from "express";<p>const app = express();</p><p>app.use(<br>  // Customizable context<br>  setContext((req: Request) =&gt; ({<br>    userId: req.user?.id,<br>    endpoint: req.url,<br>    params: req.body,<br>  }))<br>);</p></span></pre><h2 id="b6d1">Downsides</h2><ul><li id="2b7b">Complexity and tuning related to implementing CDC.</li></ul><p id="59c2">If you need a ready-to-use cloud solution that can be integrated and connected to PostgreSQL in a few minutes, check out <a href="https://bemi.io/" rel="noopener ugc nofollow" target="_blank">bemi.io</a>.</p><h2 id="14d1">Conclusion</h2><figure><figcaption>PostgreSQL data change tracking approach comparison</figcaption></figure><ol><li id="6feb">If you need basic data change tracking, <strong>triggers with an audit table</strong> are a great initial solution.</li><li id="776f"><strong>Triggers with listen/notify</strong> are a good option for simple testing in a development environment.</li><li id="6b4e">If you value application-specific context (information about a user, API endpoint, etc.) over reliability, you can use <strong>application-level tracking</strong>.</li><li id="f97e"><strong>Change Data Capture</strong> is a good option if you prioritize reliability and scalability as a unified solution that can be reused, for example, across many databases.</li><li id="329d">Finally, <strong>integrated Change Data Capture </strong>is your best bet if you need a robust data change tracking system that can also be integrated into your application. Go with <a href="https://bemi.io/" rel="noopener ugc nofollow" target="_blank">bemi.io</a> if you need a cloud-managed solution.</li></ol></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[GPT in 500 Lines of SQL (919 pts)]]></title>
            <link>https://explainextended.com/2023/12/31/happy-new-year-15/</link>
            <guid>39488668</guid>
            <pubDate>Sat, 24 Feb 2024 02:45:31 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://explainextended.com/2023/12/31/happy-new-year-15/">https://explainextended.com/2023/12/31/happy-new-year-15/</a>, See on <a href="https://news.ycombinator.com/item?id=39488668">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
	<p><small><strong>Translations</strong>: <a href="https://habr.com/ru/articles/786276/" rel="noopener" target="_blank">Russian</a></small></p>
<p>This year, the talk of the town was AI and how it can do everything for you.</p>
<p>I like it when someone or something does everything for me. To this end, I decided to ask ChatGPT to write my New Year's post:</p>
<blockquote><p>
"Hey ChatGPT. Can you implement a large language model in SQL?"<br>
"No, SQL is not suitable for implementing large language models. SQL is a language for managing and querying data in relational databases. Implementing a language model requires complex algorithms, large datasets, and often deep learning frameworks, which are beyond the capabilities of SQL."
</p></blockquote>
<p>It just proves that if you want something done right, you have to do it yourself.</p>
<p>Encouraged by this optimistic forecast, today we will implement a large language model in SQL.</p>
<p><img fetchpriority="high" decoding="async" width="700" height="539" data-attachment-id="7315" data-permalink="https://explainextended.com/2023/12/31/happy-new-year-15/kandinsky-download-1703639350496/" data-orig-file="https://explainextended.com/wp-content/uploads/2023/12/kandinsky-download-1703639350496.jpg" data-orig-size="700,539" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="Scribe" data-image-description="" data-image-caption="" data-medium-file="https://explainextended.com/wp-content/uploads/2023/12/kandinsky-download-1703639350496-300x231.jpg" data-large-file="https://explainextended.com/wp-content/uploads/2023/12/kandinsky-download-1703639350496.jpg" src="https://explainextended.com/wp-content/uploads/2023/12/kandinsky-download-1703639350496.jpg" alt="" srcset="https://explainextended.com/wp-content/uploads/2023/12/kandinsky-download-1703639350496.jpg 700w, https://explainextended.com/wp-content/uploads/2023/12/kandinsky-download-1703639350496-300x231.jpg 300w" sizes="(max-width: 700px) 100vw, 700px"></p>
<h3>Theory</h3>
<p>While writing this post, I used the wonderful article <a href="https://jaykmody.com/blog/gpt-from-scratch/" rel="noopener" target="_blank">GPT in 60 Lines of NumPy</a> by Jay Mody. This article explains the inner workings of a GPT model much better than I can hope to do. Still, a little recap is in order.</p>
<h4>What is a generative large language model from a technical perspective?</h4>
<p>A generative LLM is a function. It takes a text string as input (called "prompt" in AI parlance), and returns an array of strings and numbers. Here's what the signature of this function looks like:</p>
<p><code>llm(prompt: str) -&gt; list[tuple[str, float]]</code></p>
<p>This function is deterministic. It does a lot of math under the hood, but all this math is hardwired. If you call it repeatedly with the same input, it will always return the same output.</p>
<p>It may come as a surprise to anyone who's been using ChatGPT and similar products because they can give different answers to the same question. Yet, it's true. We will shortly see how it works.</p>
<h4>What are the values this function returns?</h4>
<p>Something like this:</p>
<pre title="">llm("I wish you a happy New")

0       (' Year', 0.967553)
1       (' Years', 0.018199688)
2       (' year', 0.003573329)
3       (' York', 0.003114716)
4       (' New', 0.0009022804)
…
50252   (' carbohyd', 2.3950911e-15)
50253   (' volunte', 2.2590102e-15)
50254   ('pmwiki', 1.369229e-15)
50255   (' proport', 1.1198108e-15)
50256   (' cumbers', 7.568147e-17)
</pre>
<p>It returns an array of tuples. Each tuple consists of a word (or, rather, a string) and a number. The number is the probability that this word will continue the prompt. The model "thinks" that the phrase "I wish you a happy New" will be followed by the character sequence " Year" with a probability of 96.7%, " Years" of 1.8% and so on.</p>
<p>The word "think" above is quoted because, of course, the model doesn't really think. It mechanically returns arrays of words and numbers according to some hardwired internal logic.</p>
<h4>If it's that dumb and deterministic, how can it generate different texts?</h4>
<p>Large language models are used in text applications (chatbots, content generators, code assistants etc). These applications repeatedly call the model and select the word suggested by it (with some degree of randomness). The next suggested word is added to the prompt and the model is called again. This continues in a loop until enough words are generated.</p>
<p>The accrued sequence of words will look like a text in a human language, complete with grammar, syntax and even what appears to be intelligence and reasoning. In this aspect, it is not unlike a <a href="https://en.wikipedia.org/wiki/Discrete-time_Markov_chain" rel="noopener" target="_blank">Markov chain</a> which works on the same principle.</p>
<p>The internals of a large language model are wired up so that the next suggested word will be a natural continuation of the prompt, complete with its grammar, semantics and sentiment. Equipping a function with such a logic became possible through a series of scientific breakthroughs (and programming drudgery) that have resulted in the development of the family of algorithms known as GPT, or Generative Pre-trained Transformer.</p>
<h4>What does "Generative Pre-trained Transformer" mean?</h4>
<p>"Generative" means that it generates text (by adding continuations to the prompt recursively, as we saw earlier).</p>
<p>"Transformer" means that it uses a particular type of neural network, first developed by Google and described in <a href="https://arxiv.org/pdf/1706.03762.pdf" rel="noopener" target="_blank">this paper</a>.</p>
<p>"Pre-trained" is a little bit historical. Initially, the ability for the model to continue text was thought of as just a prerequisite for a more specialized task: inference (finding logical connections between phrases), classification (for instance, guessing the number of stars in a hotel rating from the text of the review), machine translation and so on. It was thought that these two parts should have been trained separately, the language part being just a <em>pre-</em>training for a "real" task that would follow.</p>
<p>As the original GPT paper puts it:</p>
<blockquote><p>
We demonstrate that large gains on these tasks can be realized by generative pre-training of a language model on a diverse corpus of unlabeled text, followed by discriminative fine-tuning on each specific task.
</p></blockquote>
<p>It was not until later that people realized that, with a model large enough, the second step was often not necessary. A Transformer model, trained to do nothing else than generate texts, turned out to be able to follow human language instructions that were contained in these texts, with no additional training ("fine-tuning" in AI parlance) required.</p>
<p>With that out of the way, let's focus on the implementation.<br>
<span id="more-7143"></span></p>
<h3>Generation</h3>
<p>Here is what happens when we try to generate text from the prompt using GPT2:</p>
<pre title="">def generate(prompt: str) -&gt; str:
  # Transforms a string into a list of tokens.
  tokens = tokenize(prompt) # tokenize(prompt: str) -&gt; list[int]

  while True:

    # Runs the algorithm.
    # Returns tokens' probabilities: a list of 50257 floats, adding up to 1.
    candidates = gpt2(tokens) # gpt2(tokens: list[int]) -&gt; list[float]

    # Selects the next token from the list of candidates
    next_token = select_next_token(candidates)
    # select_next_token(candidates: list[float]) -&gt; int

    # Append it to the list of tokens
    tokens.append(next_token)

    # Decide if we want to stop generating.
    # It can be token counter, timeout, stopword or something else.
    if should_stop_generating():
      break

  # Transform the list of tokens into a string
  completion = detokenize(tokens) # detokenize(tokens: list[int]) -&gt; str
  return completion
</pre>
<p>Let's implement all these pieces one by one in SQL.</p>
<h3>Tokenizer</h3>
<p>Before a text can be fed to a neural network, it needs to be converted into a list of numbers. Of course, that's barely news: that's what text encodings like Unicode do. Plain Unicode, however, doesn't really work well with neural networks.</p>
<p>Neural networks, at their core, do a lot of matrix multiplications and capture whatever predictive powers they have in the coefficients of these matrixes. Some of these matrixes have one row per every possible value in the "alphabet"; others have one row per "character".</p>
<p>Here, the words "alphabet" and "character" don't have the usual meaning. In Unicode, the "alphabet" is 149186 characters long (this is how many different Unicode points there are at the time of this writing), and a "character" can be something like this: ﷽ (yes, that's a single Unicode point number 65021, encoding <a href="https://en.wikipedia.org/wiki/Basmala">a whole phrase in Arabic</a> that is particularly important for the Muslims). Note that the very same phrase could have been written in usual Arabic letters. It means that the same text can have many encodings.</p>
<p>As an illustration, let's take the word "PostgreSQL". If we were to encode it (convert to an array of numbers) using Unicode, we would get 10 numbers that could potentially be from 1 to 149186. It means that our neural network would need to store a matrix with 149186 rows in it and perform a number of calculations on 10 rows from this matrix. Some of these rows (corresponding to the letters of the English alphabet) would be used a lot and pack a lot of information; others, like poop emoji and obscure symbols from dead languages, would hardly be used at all, but still take up space.</p>
<p>Naturally, we want to keep both these numbers, the "alphabet" length and the "character" count, as low as possible. Ideally, all the "characters" in our alphabet should be distributed uniformly, and we still want our encoding to be as powerful as Unicode.</p>
<p>The way we can do that, intuitively, is to assign unique numbers to sequences of words that occur often in the texts we work with. In Unicode, the same religious phrase in Arabic can be encoded using either a single code point, or letter by letter. Since we are rolling our own encoding, we can do the same for the words and phrases that are important for the model (i.e. show up often in texts).</p>
<p>For instance, we could have separate numbers for "Post", "greSQL" and "ing". This way, the words "PostgreSQL" and "Posting" would both have a length of 2 in our representation. And of course, we would still maintain separate code points for shorter sequences and individual bytes. Even if we come across gibberish or a text in a foreign language, it would still be encodable, albeit longer.</p>
<p>GPT2 uses a variation of the algorithm called <a href="https://en.wikipedia.org/wiki/Byte_pair_encoding" rel="noopener" target="_blank">Byte pair encoding</a> to do precisely that. Its tokenizer uses a dictionary of 50257 code points (in AI parlance, "tokens") that correspond to different byte sequences in UTF-8 (plus the "end of text" as a separate token).</p>
<p>This dictionary was built by statistical analysis performed like this:</p>
<ol>
<li>Start with a simple encoding of 256 tokens: one token per byte.</li>
<li>Take a large corpus of texts (preferably the one the model will be trained on).</li>
<li>Encode it.</li>
<li>Calculate which pair of tokens is the most frequent. Let's assume it's 0x20 0x74 (space followed by the lowercase "t").</li>
<li>Assign the next available value (257) to this pair of bytes.</li>
<li>Repeat the steps 3-5, now paying attention to the byte sequences. If a sequence of bytes can be encoded with a complex token, use the complex token. If there are ambiguities (say, "abc" can, at some point, be encoded as "a" + "bc" or "ab" + "c"), use the one with the lowest number (because it was added earlier and hence is more frequent). Do this recursively until all sequences that can collapse into a single token will collapse into a single token.</li>
<li>Perform the collapse 50000 times over.</li>
</ol>
<p>The number 50000 was chosen more or less arbitrarily by the developers. Other models keep the number of tokens in a similar range (from 30k to 100k).</p>
<p>At every iteration of this algorithm, a new token that is a concatenation of two previous ones will be added to the dictionary. Ultimately, we will end up with 50256 tokens. Add a fixed-number token for "end-of-text", and we're done.</p>
<p>The GPT2 version of BTE has another layer of encoding: the token dictionary maps tokens to strings and not arrays of bytes. Mapping from bytes to string characters is defined in <a href="https://github.com/openai/gpt-2/blob/a74da5d99abaaba920de8131d64da2862a8f213b/src/encoder.py#L9-L28" rel="noopener" target="_blank">this function</a>. We will save the dictionary it produces in the table <code>encoder</code>.</p>
<p>Let's see how we can implement the tokenizer in SQL.</p>
<p>The tokenizer is an integral part of GPT2, and the token dictionary can be downloaded from OpenAI's website along with the rest of the model. We will need to import it into the table <code>tokenizer</code>. At the bottom of this post, you will find a link to the code repository. Its code will automate populating database tables needed for the model.</p>
<p>In a recursive CTE, we will split this word into tokens (starting with single bytes) and merge the best adjacent pairs, until there is nothing left to merge. The merging itself happens in a nested recursive CTE.</p>
<p>For the demo, I will use the word "Mississippilessly". Each record in the resultset shows the best pair to collapse found so far, and also the progress through the query.</p>
<pre title="">WITH    RECURSIVE
        bpe AS
        (
        SELECT  (n + 1)::BIGINT AS position, character, TRUE AS continue, 1 AS step,
                NULL::INT AS token, NULL::TEXT AS combined
        FROM    CONVERT_TO('Mississippilessly', 'UTF-8') AS bytes
        CROSS JOIN LATERAL
                GENERATE_SERIES(0, LENGTH(bytes) - 1) AS n
        JOIN    encoder
        ON      byte = GET_BYTE(bytes, n)
        UNION ALL
        (
        WITH    RECURSIVE
                base AS
                (
                SELECT  *
                FROM    bpe
                WHERE   continue
                ),
                bn AS
                (
                SELECT  ROW_NUMBER() OVER (ORDER BY position) AS position,
                        continue,
                        character,
                        character || LEAD(character) OVER (ORDER BY position) AS cluster
                FROM    base
                ),
                top_rank AS
                (
                SELECT  tokenizer.*
                FROM    bn
                CROSS JOIN LATERAL
                        (
                        SELECT  *
                        FROM    tokenizer
                        WHERE   tokenizer.cluster = bn.cluster
                        LIMIT   1
                        ) tokenizer
                ORDER BY
                        token
                LIMIT   1
                ),
                breaks AS
                (
                SELECT  0::BIGINT AS position, 1 AS length
                UNION ALL
                SELECT  bn.position,
                        CASE WHEN token IS NULL THEN 1 ELSE 2 END
                FROM    breaks
                JOIN    bn
                ON      bn.position = breaks.position + length
                LEFT JOIN
                        top_rank
                USING   (cluster)
                )
        SELECT  position, character, token IS NOT NULL,
                (SELECT step + 1 FROM base LIMIT 1), token, top_rank.cluster
        FROM    breaks
        LEFT JOIN
                top_rank
        ON      1 = 1
        CROSS JOIN LATERAL
                (
                SELECT  STRING_AGG(character, '' ORDER BY position) AS character
                FROM    bn
                WHERE   bn.position &gt;= breaks.position
                        AND bn.position &lt; breaks.position + length
                ) bn
        WHERE   position &gt; 0
        )
        )
SELECT  step, MAX(token) AS token, MAX(combined) AS combined, ARRAY_AGG(character ORDER BY position)
FROM    bpe
WHERE   continue
GROUP BY
        step
ORDER BY
        step
</pre>
<div>
<table>
<tbody><tr>
<th>step</th>
<th>token</th>
<th>combined</th>
<th>array_agg</th>
</tr>
<tr>
<td>1</td>
<td>None</td>
<td>None</td>
<td>['M', 'i', 's', 's', 'i', 's', 's', 'i', 'p', 'p', 'i', 'l', 'e', 's', 's', 'l', 'y']</td>
</tr>
<tr>
<td>2</td>
<td>271</td>
<td>is</td>
<td>['M', 'is', 's', 'is', 's', 'i', 'p', 'p', 'i', 'l', 'e', 's', 's', 'l', 'y']</td>
</tr>
<tr>
<td>3</td>
<td>274</td>
<td>es</td>
<td>['M', 'is', 's', 'is', 's', 'i', 'p', 'p', 'i', 'l', 'es', 's', 'l', 'y']</td>
</tr>
<tr>
<td>4</td>
<td>306</td>
<td>ly</td>
<td>['M', 'is', 's', 'is', 's', 'i', 'p', 'p', 'i', 'l', 'es', 's', 'ly']</td>
</tr>
<tr>
<td>5</td>
<td>346</td>
<td>il</td>
<td>['M', 'is', 's', 'is', 's', 'i', 'p', 'p', 'il', 'es', 's', 'ly']</td>
</tr>
<tr>
<td>6</td>
<td>381</td>
<td>pp</td>
<td>['M', 'is', 's', 'is', 's', 'i', 'pp', 'il', 'es', 's', 'ly']</td>
</tr>
<tr>
<td>7</td>
<td>408</td>
<td>ess</td>
<td>['M', 'is', 's', 'is', 's', 'i', 'pp', 'il', 'ess', 'ly']</td>
</tr>
<tr>
<td>8</td>
<td>747</td>
<td>iss</td>
<td>['M', 'iss', 'iss', 'i', 'pp', 'il', 'ess', 'ly']</td>
</tr>
<tr>
<td>9</td>
<td>3974</td>
<td>ipp</td>
<td>['M', 'iss', 'iss', 'ipp', 'il', 'ess', 'ly']</td>
</tr>
<tr>
<td>10</td>
<td>17140</td>
<td>Miss</td>
<td>['Miss', 'iss', 'ipp', 'il', 'ess', 'ly']</td>
</tr>
<tr>
<td>11</td>
<td>30608</td>
<td>iless</td>
<td>['Miss', 'iss', 'ipp', 'iless', 'ly']</td>
</tr>
</tbody></table>
</div>
<p>On each step, the BPE algorithm finds the best pair of tokens to merge and merges them (you can see the merged pair and its rank in the output). This procedure brings down the token space size from Unicode's 150k to 50k, and the number of tokens (in this particular word) from 17 to 5. Both are great improvements.</p>
<p>When working with multiple words, the tokenizer first splits the text into separate words using <a href="https://github.com/openai/gpt-2/blob/master/src/encoder.py#L53" rel="noopener" target="_blank">this regexp</a> and merges the tokens inside each word separately. Unfortunately, PostgreSQL doesn't support Unicode character properties in regexps, so I had to tweak it a little bit (probably killing proper Unicode support in the process). Here's how it looks in SQL:</p>
<pre title="">WITH    input AS
        (
        SELECT  'PostgreSQL is great' AS prompt
        ),
        clusters AS
        (
        SELECT  part_position, bpe.*
        FROM    input
        CROSS JOIN LATERAL
                REGEXP_MATCHES(prompt, '''s|''t|''re|''ve|''m|''ll|''d| ?\w+| ?\d+| ?[^\s\w\d]+|\s+(?!\S)|\s+', 'g') WITH ORDINALITY AS rm (part, part_position)
        CROSS JOIN LATERAL
                (
                WITH    RECURSIVE
                        bpe AS
                        (
                        SELECT  (n + 1)::BIGINT AS position, character, TRUE AS continue
                        FROM    CONVERT_TO(part[1], 'UTF-8') AS bytes
                        CROSS JOIN LATERAL
                                GENERATE_SERIES(0, LENGTH(bytes) - 1) AS n
                        JOIN    encoder
                        ON      byte = GET_BYTE(bytes, n)
                        UNION ALL
                        (
                        WITH    RECURSIVE
                                base AS
                                (
                                SELECT  *
                                FROM    bpe
                                WHERE   continue
                                ),
                                bn AS
                                (
                                SELECT  ROW_NUMBER() OVER (ORDER BY position) AS position,
                                        continue,
                                        character,
                                        character || LEAD(character) OVER (ORDER BY position) AS cluster
                                FROM    base
                                ),
                                top_rank AS
                                (
                                SELECT  tokenizer.*
                                FROM    bn
                                CROSS JOIN LATERAL
                                        (
                                        SELECT  *
                                        FROM    tokenizer
                                        WHERE   tokenizer.cluster = bn.cluster
                                        LIMIT   1
                                        ) tokenizer
                                ORDER BY
                                        token
                                LIMIT   1
                                ),
                                breaks AS
                                (
                                SELECT  0::BIGINT AS position, 1 AS length
                                UNION ALL
                                SELECT  bn.position,
                                        CASE WHEN token IS NULL THEN 1 ELSE 2 END
                                FROM    breaks
                                JOIN    bn
                                ON      bn.position = breaks.position + length
                                LEFT JOIN
                                        top_rank
                                USING   (cluster)
                                )
                        SELECT  position, character, token IS NOT NULL
                        FROM    breaks
                        LEFT JOIN
                                top_rank
                        ON      1 = 1
                        CROSS JOIN LATERAL
                                (
                                SELECT  STRING_AGG(character, '' ORDER BY position) AS character
                                FROM    bn
                                WHERE   bn.position &gt;= breaks.position
                                        AND bn.position &lt; breaks.position + length
                                ) bn
                        WHERE   position &gt; 0
                        )
                        )
                SELECT  position, character AS cluster
                FROM    bpe
                WHERE   NOT continue
                ) bpe
        ),
        tokens AS
        (
        SELECT  token, cluster
        FROM    clusters
        JOIN    tokenizer
        USING   (cluster)
        )
SELECT  *
FROM    tokens
</pre>
<div>
<table>
<tbody><tr>
<th>token</th>
<th>cluster</th>
</tr>
<tr>
<td>6307</td>
<td>Post</td>
</tr>
<tr>
<td>47701</td>
<td>greSQL</td>
</tr>
<tr>
<td>318</td>
<td>Ġis</td>
</tr>
<tr>
<td>1049</td>
<td>Ġgreat</td>
</tr>
</tbody></table>
</div>
<p>The weird character Ġ is the whitespace.</p>
<p>This query tokenizes the prompt and converts it into an array of numbers. This way, the prompt is ready for its journey through the layers of the model.</p>
<h3>Embeddings</h3>
<p>The tokens represent parts of the human languages (about 0.75 words per token, in general), so any model that is trying to succeed at text completion should somehow encode the relationships between these parts. Even in isolation, the parts of the speech have sets of orthogonal properties.</p>
<p>Let's take the word "subpoena" (which happens to have a whole token in itself in the GPT2 tokenizer). Is it a noun? Yes, very much so. Is it a verb? Well, sort of. Is it an adjective? Not that much, but it can be if you squint hard enough. Is it legalese? Hell yes. And so on.</p>
<p>All these properties are orthogonal, i.e. independent of each other. A word can be a legalese noun but not an adjective or a verb. In English, any combination thereof can happen.</p>
<p>Things with orthogonal properties are best encoded using vectors. Instead of having a single property (like a token number), we can have many. And it helps if we can wiggle them as we want. For instance, for a word to continue the phrase "A court decision cited by the lawyer mentions the …" we would probably want something that's heavy on the legalese dimension and at the same time heavy on being a noun. We don't really care if it has a side hustle being an adjective, a verb, or a flower.</p>
<p>In math, mapping narrower values into wider spaces (such as token IDs to vectors) is called an <a href="https://en.wikipedia.org/wiki/Embedding" rel="noopener" target="_blank">embedding</a>. This is exactly what we are doing here.</p>
<p>How do we decide which properties these vectors represent? We don't. We just provide enough vector space for every token and hope that the model during its training phase will populate these dimensions with something meaningful. GPT2 uses 768 dimensions for its vectors. There is no telling in advance (and, actually, even in the retrospective) what property of the word will, say, the dimension 247 encode. Surely it would encode something, but it's not easy to tell what it is.</p>
<p>What properties of each token do we want to embed in the vector space? Anything that has any bearing on what the next token would be.</p>
<p>Token id? Of course. Different tokens mean different things.</p>
<p>Position of the token in the text? Yes, please. "Blue violet" and "violet blue" are not the same thing.</p>
<p>Relationships of tokens to each other? Sure! That's, probably, the most important part of the job, and the Attention block of the Transformer architecture was the first one to get it right.</p>
<p>Tokens and positions are easy to embed. Let's say we have the phrase "PostgreSQL is great", which, as we already know, maps to four tokens: <code>[6307, 47701, 318, 1049]</code>.</p>
<p>Among other parameters of GPT2, there are two matrixes called WTE (word token embedding) and WPE (word position embedding). As the names suggest, the former stores embeddings of the tokens, and the latter stores embeddings of the positions. The actual values of these embeddings have been populated ("learned") during the training of GPT2. As far as we are concerned, they are constants that live in the database tables <code>wte</code> and <code>wpe</code>.</p>
<p>WTE is 50257×768 and WPE is 1024×768. The latter means that the maximum number of tokens that we can use in a prompt to GPT2 is 1024. If we provide more tokens in the prompt, we just won't be able to pull positional embeddings for them. It's an architectural aspect ("hyperparameter" in AI parlance) of the model that is set at design time and cannot be changed by training. When people talk about the "context window" of an LLM, they mean this number.</p>
<p>We have the token 6307 at place 0, 47701 at 1, 318 at 2, and 1049 at 3. For each of these tokens and positions, we have two vectors: one from WTE and another one from WPE. We need to <a href="https://github.com/jaymody/picoGPT/blob/29e78cc52b58ed2c1c483ffea2eb46ff6bdec785/gpt2.py#L75" rel="noopener" target="_blank">add them together</a>. The four resulting vectors will be the inputs for the next part of the algorithm: the feed-forward neural network with the attention mechanism.</p>
<p>For the SQL part, we will use <a href="https://github.com/pgvector/pgvector" rel="noopener" target="_blank">pgvector</a>, a PostgreSQL extension.</p>
<p><em>A little disclaimer: normally, I write code for my New Year posts in vanilla SQL, sometimes with pure SQL functions as helpers. It would be perfectly possible to do it for this post as well by defining vector operations on arrays, at the cost of some performance decrease (it was done in version 1 and worked, albeit slowly). With the advent of the AI and growing importance of vector databases, pgvector or its equivalent will definitely make it into the core of PostgreSQL within two or three releases. I just decided to ride the wave of the future.</em></p>
<p>Here's how we do that in SQL:</p>
<pre title="">WITH    embeddings AS
        (
        SELECT  place, values
        FROM    UNNEST(ARRAY[6307, 47701, 318, 1049]) WITH ORDINALITY AS tokens (token, ordinality)
        CROSS JOIN LATERAL
                (
                SELECT  ordinality - 1 AS place
                ) o
        CROSS JOIN LATERAL
                (
                SELECT  wte.values + wpe.values AS values
                FROM    wte
                CROSS JOIN
                        wpe
                WHERE   wte.token = tokens.token
                        AND wpe.place = o.place
                ) embedding
        )
SELECT  place, (values::REAL[])[0:5]
FROM    embeddings
</pre>
<div>
<table>
<tbody><tr>
<th>place</th>
<th>values</th>
</tr>
<tr>
<td>0</td>
<td>[0.1035146, -0.22879261, 0.18413992, -0.29924694, 0.18642524]</td>
</tr>
<tr>
<td>1</td>
<td>[0.10757777, -0.0011023134, -0.0077463835, 0.03656415, -0.14654925]</td>
</tr>
<tr>
<td>2</td>
<td>[-0.005507436, -0.07471258, 0.11009377, -0.11708109, -0.14026159]</td>
</tr>
<tr>
<td>3</td>
<td>[-0.04785268, -0.0792546, 0.1628486, -0.3598496, 0.11462127]</td>
</tr>
</tbody></table>
</div>
<p>(To keep the output short, this query only shows the first 5 dimensions for each vector)</p>
<h3>Attention</h3>
<p>The part that really makes the Transformer architecture tick is the self-attention mechanism. It was first described in the 2017 paper <a href="https://arxiv.org/abs/1706.03762" rel="noopener" target="_blank">"Attention is all you need"</a> by Vasmani et al., probably <em>the</em> most famous AI paper, whose name has since become a <a href="https://en.wikipedia.org/wiki/Snowclone" rel="noopener" target="_blank">snowclone</a> (a cliché for naming other papers).</p>
<p>So far, we have several vectors that, hopefully, encode some syntactic and semantic properties of the words in our prompt. We need these properties to somehow transfer to the last vector. A little spoiler alert: at the end of the day, it will be the last vector that will store the embedding for the continuation word.</p>
<p>In a phrase like "I looked at the violet and saw that it was not the usual …", the ellipsis has to be something you see (and this notion has to jump from "saw"), something that's a property of a violet (jumping from "violet" to "it" and then to the ellipsis), and something that is "unusual" (jumping from "not" and "usual" and flipping the sign in the dimensions responsible for the usualness). The analogy in the real world would be a person reading a book in a foreign language that they kind of have a basic command of, but don't quite know very well. They would need to consciously trace their way from one word to another, and if they don't <em>pay attention</em> to the crucial part of the phrase, their understanding would be wrong.</p>
<p>To enable this transfer of meaning from one token to another, we need to allow the vectors of all the tokens to influence each other. If we want to populate the word "it" with some concrete semantics, how much of the semantics should come from the previous vectors in the prompt, and how much should remain from the word "it" itself?</p>
<p>To solve this problem, the model uses 12 sets of matrixes called Q (query), K (key) and V (value). Each of them has 64 columns. They are obtained from the vector embeddings through a 768×2304 linear transformation <code>c_attn</code>, whose weights and biases are stored in the tables <code>c_attn_w</code> and <code>c_attn_b</code>.</p>
<p>The result of <code>c_attn</code> is a matrix with <code>n_token</code> rows and 2304 columns (3×12×64). It consists of 12 Q matrixes, 12 K matrixes and 12 V matrixes stacked horizontally, in this order.</p>
<p>Each set of Q, K and V is called a "head". They are used to perform the step known as "multi-headed causal self-attention", by calculating the attention function.</p>
<p>Here's the formula for the attention function:</p>
<p><img decoding="async" src="https://s0.wp.com/latex.php?latex=A+%3D+%5Cmathrm%7Bsoftmax%7D%28%5Cdfrac%7BQK%5ET%7D%7B%5Csqrt%7Bd_k%7D%7D+%2B+M%29V&amp;bg=fff&amp;fg=1c1c1c&amp;s=0&amp;c=20201002" alt="A = \mathrm{softmax}(\dfrac{QK^T}{\sqrt{d_k}} + M)V">,</p>
<p>where softmax is the weight normalization function. It's defined like this:</p>
<p><img decoding="async" src="https://s0.wp.com/latex.php?latex=%5Cmathrm%7Bsoftmax_n%7D%28%5Ctextbf%7BR%7D%29+%3D+%5Cdfrac%7Be%5E%7BR_n%7D%7D%7B%5Csum%5Climits_n+e%5E%7BR_n%7D+%7D&amp;bg=fff&amp;fg=1c1c1c&amp;s=0&amp;c=20201002" alt="\mathrm{softmax_n}(\textbf{R}) = \dfrac{e^{R_n}}{\sum\limits_n e^{R_n} }"></p>
<p><img decoding="async" src="https://s0.wp.com/latex.php?latex=M+&amp;bg=fff&amp;fg=1c1c1c&amp;s=0&amp;c=20201002" alt="M "> is a constant matrix called a "causal mask". It is defined like this: <img decoding="async" src="https://s0.wp.com/latex.php?latex=M+%3D+%5Cbegin%7Bbmatrix%7D++++++0+%26+-inf+%26+-inf+%26+%5Cdots++%26+-inf+%5C%5C++++++0+%26+0+%26+-inf+%26+%5Cdots++%26+-inf+%5C%5C++++++%5Cvdots+%26+%5Cvdots+%26+%5Cvdots+%26+%5Cddots+%26+%5Cvdots+%5C%5C++++++0+%26+0+%26+0+%26+%5Cdots+%26+-inf+%5C%5C++++++0+%26+0+%26+0+%26+%5Cdots+%26+0++%5Cend%7Bbmatrix%7D+&amp;bg=fff&amp;fg=1c1c1c&amp;s=0&amp;c=20201002" alt="M = \begin{bmatrix}      0 &amp; -inf &amp; -inf &amp; \dots  &amp; -inf \\      0 &amp; 0 &amp; -inf &amp; \dots  &amp; -inf \\      \vdots &amp; \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\      0 &amp; 0 &amp; 0 &amp; \dots &amp; -inf \\      0 &amp; 0 &amp; 0 &amp; \dots &amp; 0  \end{bmatrix} "></p>
<p>Softmax turns negative infinities into zeros.</p>
<h4>Why do we need masking?</h4>
<p>The prompt in our previous examples had 4 tokens, and the first thing the model did was calculate the 4 embeddings for these 4 tokens. As the model progresses, these vectors will undergo a lot of calculations, but for the most part, they will be independent and parallel. Changes in one vector will not affect the other vectors, as if they had not existed. The self-attention block is the only place in the whole model where the vectors affect each other.</p>
<p>Once the model is done with the math, the candidates for the next token will be decided solely from the last embedding. All the information flow should be directed towards this last vector and not from it. The transient values of the last embedding should not affect the transient values of the previous embeddings during the forward pass of the model.</p>
<p>That's why we "mask" the latter embeddings so that they don't influence the earlier embeddings through this particular channel. Hence the word "causal" in "multi-headed causal self-attention".</p>
<h4>Why are the matrixes called "query", "key" and "value"?</h4>
<p>To be honest, I'm not sure it's even a good analogy. But I'll still do my take on the intuition behind it.</p>
<p>In machine learning, generally, calculations should not involve variable-length loops or statement branching. Everything should be done through the composition of simple analytic functions (additions, multiplications, powers, logarithms and trig). It allows backpropagation, which relies on technologies like <a href="https://en.wikipedia.org/wiki/Automatic_differentiation" rel="noopener" target="_blank">automatic differentiation</a>, to work efficiently. </p>
<p>The mathematical model of the key-value store is the expression</p>
<p><img decoding="async" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle+%5Cbegin%7Bcases%7Dv%2C+%26+k+%3D+q+%5C%5C+0%2C+%26+%5Ctext%7Botherwise%7D+%5Cend%7Bcases%7D&amp;bg=fff&amp;fg=1c1c1c&amp;s=0&amp;c=20201002" alt="\displaystyle \begin{cases}v, &amp; k = q \\ 0, &amp; \text{otherwise} \end{cases}"></p>
<p>, but it's not a smooth, differentiable function and it will not work well with backpropagation. To make it work, we would need to turn it into a smooth function that would be <em>close</em> to <img decoding="async" src="https://s0.wp.com/latex.php?latex=v&amp;bg=fff&amp;fg=1c1c1c&amp;s=0&amp;c=20201002" alt="v"> when <img decoding="async" src="https://s0.wp.com/latex.php?latex=k&amp;bg=fff&amp;fg=1c1c1c&amp;s=0&amp;c=20201002" alt="k"> is close to <img decoding="async" src="https://s0.wp.com/latex.php?latex=q&amp;bg=fff&amp;fg=1c1c1c&amp;s=0&amp;c=20201002" alt="q">, and <em>close</em> to <img decoding="async" src="https://s0.wp.com/latex.php?latex=0&amp;bg=fff&amp;fg=1c1c1c&amp;s=0&amp;c=20201002" alt="0"> otherwise.</p>
<p>The Gaussian distribution ("bell curve"), scaled to <img decoding="async" src="https://s0.wp.com/latex.php?latex=v&amp;bg=fff&amp;fg=1c1c1c&amp;s=0&amp;c=20201002" alt="v">, with the expectation of <img decoding="async" src="https://s0.wp.com/latex.php?latex=k&amp;bg=fff&amp;fg=1c1c1c&amp;s=0&amp;c=20201002" alt="k"> and a small enough standard deviation would do perfectly for this purpose:</p>
<p><img decoding="async" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle+%5Cfrac%7Bv%7D%7B%5Csigma%5Csqrt%7B2%5Cpi%7D%7D+%5C%2C+%5Cmathrm%7Bexp%7D%5Cleft%28-%5Cfrac%7B%5Cleft%28q+-+k%5Cright%29%5E2%7D%7B2%5Csigma%5E2%7D%5Cright%29&amp;bg=fff&amp;fg=1c1c1c&amp;s=0&amp;c=20201002" alt="\displaystyle \frac{v}{\sigma\sqrt{2\pi}} \, \mathrm{exp}\left(-\frac{\left(q - k\right)^2}{2\sigma^2}\right)"></p>
<p>, where <img decoding="async" src="https://s0.wp.com/latex.php?latex=%5Csigma&amp;bg=fff&amp;fg=1c1c1c&amp;s=0&amp;c=20201002" alt="\sigma"> is an arbitrary parameter, defining how sharp the bell curve is.</p>
<p>In a vector space with many enough dimensions, if we take a fixed vector <img decoding="async" src="https://s0.wp.com/latex.php?latex=%5Cmathbf+K&amp;bg=fff&amp;fg=1c1c1c&amp;s=0&amp;c=20201002" alt="\mathbf K"> and several vectors <img decoding="async" src="https://s0.wp.com/latex.php?latex=%5Cmathbf+Q&amp;bg=fff&amp;fg=1c1c1c&amp;s=0&amp;c=20201002" alt="\mathbf Q"> that randomly and uniformly deviate from <img decoding="async" src="https://s0.wp.com/latex.php?latex=%5Cmathbf+K&amp;bg=fff&amp;fg=1c1c1c&amp;s=0&amp;c=20201002" alt="\mathbf K"> on every dimension, their dot products will naturally form the bell curve. So, in the vector space, the concept of a "differentiable key-value store" can be modeled by the expression <img decoding="async" src="https://s0.wp.com/latex.php?latex=%28%5Ctextbf+Q+%5Ccdot+%5Ctextbf+K%29+%5Ctextbf+V&amp;bg=fff&amp;fg=1c1c1c&amp;s=0&amp;c=20201002" alt="(\textbf Q \cdot \textbf K) \textbf V">, which is what we are using in our attention function.</p>
<p>Again, this analogy is far-fetched. It's best not to pay too much attention (no pun intended) to these concepts of attention, meaning flow, hash tables and so on. Just think of them as an inspiration for a math trick that has been put to the test and proved to work really well.</p>
<p>Let's illustrate this step:</p>
<pre title="">WITH    embeddings AS
        (
        SELECT  place, values
        FROM    UNNEST(ARRAY[6307, 47701, 318, 1049]) WITH ORDINALITY AS tokens (token, ordinality)
        CROSS JOIN LATERAL
                (
                SELECT  ordinality - 1 AS place
                ) o
        CROSS JOIN LATERAL
                (
                SELECT  wte.values + wpe.values AS values
                FROM    wte
                CROSS JOIN
                        wpe
                WHERE   wte.token = tokens.token
                        AND wpe.place = o.place
                ) embedding
        ),
        c_attn_w AS
        (
        SELECT  *
        FROM    c_attn_w
        WHERE   block = 0
        ),
        c_attn_b AS
        (
        SELECT  *
        FROM    c_attn_b
        WHERE   block = 0
        ),
        ln_1_g AS
        (
        SELECT  *
        FROM    ln_1_g
        WHERE   block = 0
        ),
        ln_1_b AS
        (
        SELECT  *
        FROM    ln_1_b
        WHERE   block = 0
        ),
        mha_norm AS
        (
        SELECT  place, mm.values + c_attn_b.values AS values
        FROM    (
                SELECT  place, ARRAY_AGG(INNER_PRODUCT(c_attn_w.values, layer_norm.values) ORDER BY y)::VECTOR(2304) AS values
                FROM    (
                        SELECT  place, agg.values * ln_1_g.values + ln_1_b.values AS values
                        FROM    (
                                SELECT  place, norm.values
                                FROM    embeddings
                                CROSS JOIN LATERAL
                                        (
                                        SELECT  AVG(value) AS mean,
                                                VAR_POP(value) AS variance
                                        FROM    UNNEST(values::REAL[]) value
                                        ) agg
                                CROSS JOIN LATERAL
                                        (
                                        SELECT  ARRAY_AGG((value - mean) / SQRT(variance + 1E-5) ORDER BY ordinality)::VECTOR(768) AS values
                                        FROM    UNNEST(values::REAL[]) WITH ORDINALITY AS n(value, ordinality)
                                        ) norm
                                ) agg
                        CROSS JOIN
                                ln_1_b
                        CROSS JOIN
                                ln_1_g
                        ) layer_norm
                CROSS JOIN
                        c_attn_w
                GROUP BY
                        place
                ) mm
        CROSS JOIN
                c_attn_b
        ),
        head AS
        (
        SELECT  place,
                (values::REAL[])[1:64]::VECTOR(64) AS q,
                (values::REAL[])[1 + 768:64 + 768]::VECTOR(64) AS k,
                (values::REAL[])[1 + 1536:64 + 1536]::VECTOR(64) AS v
        FROM    mha_norm
        ),
        sm_input AS
        (
        SELECT  h1.place AS x, h2.place AS y, INNER_PRODUCT(h1.q, h2.k) / 8 + CASE WHEN h2.place &gt; h1.place THEN -1E10 ELSE 0 END AS value
        FROM    head h1
        CROSS JOIN
                head h2
        ),
        sm_diff AS
        (
        SELECT  x, y, value - MAX(value) OVER (PARTITION BY x) AS diff
        FROM    sm_input
        ),
        sm_exp AS
        (
        SELECT  x, y, CASE WHEN diff &lt; -745.13 THEN 0 ELSE EXP(diff) END AS e
        FROM    sm_diff
        ),
        softmax AS
        (
        SELECT  x, y AS place, e / SUM(e) OVER (PARTITION BY x) AS value
        FROM    sm_exp
        ),
        attention AS
        (
        SELECT  place, (ARRAY_AGG(value ORDER BY ordinality))[:3] AS values
        FROM    (
                SELECT  x AS place, SUM(ARRAY_FILL(softmax.value, ARRAY[64])::VECTOR(64) * head.v) AS values
                FROM    softmax
                JOIN    head
                USING   (place)
                GROUP BY
                        x
                ) q
        CROSS JOIN LATERAL
                UNNEST(values::REAL[]) WITH ORDINALITY v (value, ordinality)
        GROUP BY
                place
        )
SELECT  place,
        (SELECT STRING_AGG(TO_CHAR(n, 'S0.000'), ' ') || ' …' FROM UNNEST((q::REAL[])[:3]) AS n) AS q,
        (SELECT STRING_AGG(TO_CHAR(n, 'S0.000'), ' ') || ' …' FROM UNNEST((k::REAL[])[:3]) AS n) AS k,
        (SELECT STRING_AGG(TO_CHAR(n, 'S0.000'), ' ') || ' …' FROM UNNEST((v::REAL[])[:3]) AS n) AS v,
        matrix,
        (SELECT STRING_AGG(TO_CHAR(n, 'S0.000'), ' ') || ' …' FROM UNNEST((values::REAL[])[:3]) AS n) AS attention
FROM    head
JOIN    attention
USING   (place)
JOIN    (
        SELECT  x AS place, STRING_AGG(CASE WHEN value &gt; 0 THEN TO_CHAR(value, '0.00') ELSE '    0' END, ' ' ORDER BY place) AS matrix
        FROM    softmax
        GROUP BY
                x
        ) softmax_grouped
USING   (place)
</pre>
<div>
<table>
<tbody><tr>
<th>place</th>
<th>q</th>
<th>k</th>
<th>v</th>
<th>matrix</th>
<th>attention</th>
</tr>
<tr>
<td>0</td>
<td>+0.381 -0.579 +0.073 …</td>
<td>-1.395 +2.367 +0.332 …</td>
<td>-0.006 +0.192 +0.047 …</td>
<td> 1.00     0     0     0</td>
<td>-0.006 +0.192 +0.047 …</td>
</tr>
<tr>
<td>1</td>
<td>+1.518 +0.827 -0.388 …</td>
<td>-2.380 +3.714 +0.659 …</td>
<td>-0.315 -0.062 +0.018 …</td>
<td> 0.73  0.27     0     0</td>
<td>-0.089 +0.124 +0.039 …</td>
</tr>
<tr>
<td>2</td>
<td>+0.238 -0.226 +0.344 …</td>
<td>-1.952 +2.404 +1.953 …</td>
<td>+0.256 -0.268 +0.301 …</td>
<td> 0.67  0.26  0.07     0</td>
<td>-0.069 +0.095 +0.057 …</td>
</tr>
<tr>
<td>3</td>
<td>+1.130 -0.011 -0.103 …</td>
<td>-2.855 +2.053 +2.813 …</td>
<td>+0.176 +0.019 -0.099 …</td>
<td> 0.59  0.19  0.12  0.10</td>
<td>-0.016 +0.071 +0.058 …</td>
</tr>
</tbody></table>
</div>
<p>Here is what we did:</p>
<ol>
<li>Before calculating the attention function, we normalized the vectors by applying the linear transformation <img decoding="async" src="https://s0.wp.com/latex.php?latex=%5Cmathbf+R%5E%5Cprime+%3D+%5Cmathbf%7BR%5CGamma_1%7D+%2B+%5Cmathbf%7BB_1%7D&amp;bg=fff&amp;fg=1c1c1c&amp;s=0&amp;c=20201002" alt="\mathbf R^\prime = \mathbf{R\Gamma_1} + \mathbf{B_1}">. The matrix <img decoding="async" src="https://s0.wp.com/latex.php?latex=%5Cmathbf%7B%5CGamma_1%7D+&amp;bg=fff&amp;fg=1c1c1c&amp;s=0&amp;c=20201002" alt="\mathbf{\Gamma_1} "> and the vector <img decoding="async" src="https://s0.wp.com/latex.php?latex=%5Cmathbf%7BB_1%7D+&amp;bg=fff&amp;fg=1c1c1c&amp;s=0&amp;c=20201002" alt="\mathbf{B_1} "> are called "scale" and "shift", accordingly. They are learned parameters of the model, which are stored in the tables <code>ln_1_g</code> and <code>ln_1_b</code></li>
<li>We are only showing the first head of the first layer of the algorithm. After we multiplied the vectors by the learned coefficients from <code>c_attn_w</code> and <code>c_attn_b</code> ("weight" and "bias"), we sliced the resulting 2304-vectors, taking 64-vectors starting at the positions 0, 768 and 1536. They correspond to the vectors Q, K and V for the first head.</li>
<li><code>EXP</code> in PostgreSQL fails on really small numbers, that's why we shortcut to zero if the argument to <code>EXP</code> is less than -745.13.</li>
<li>We are only showing the first three elements for each vector. The attention matrix we show in full.</li>
</ol>
<p>As we can see, the first value vector got copied to the output as is (as it will do in every other layer of the algorithm). It means that once the model has been trained, the output embedding for the first token will be only defined by the value of the first token. In general, during the recursive inference phase, where tokens only get added to the prompt, only the last embedding in the output will ever change compared to the previous iteration. This is what the causal mask does.</p>
<p>Looking a bit forward: the attention block is the <em>only</em> place in the entire algorithm where tokens can influence each other during the forward pass. Since we have disabled the ability of later tokens to influence the previous ones in this step, all the calculations done on the previous tokens can be reused between the forward passes of the model.</p>
<p>Remember, the model operates by appending tokens to the prompt. If our original (tokenized) prompt is "Post greSQL Ġis Ġgreat" and the next one will be (for instance) "Post greSQL Ġis Ġgreat Ġfor", all the results of the calculations made on the first four tokens can be reused for the new prompt; they will never change, regardless of what is appended to them.</p>
<p>Jay Mody's illustrative article doesn't make use of this fact (and neither do we, for the sake of simplicity), but the original GPT2 implementation <a href="https://github.com/openai/gpt-2/blob/a74da5d99abaaba920de8131d64da2862a8f213b/src/model.py#L161" rel="noopener" target="_blank">does</a>.</p>
<p>Once all the heads are done, we will end up with 12 matrixes, each 64 columns wide and <code>n_tokens</code> rows tall. To map it back to the dimension of embedding vectors (768), we just need to stack these matrixes horizontally.</p>
<p>The final step of multi-headed attention involves projecting the values through a learned linear transformation of the same dimension. Its weights and biases are stored in the tables <code>c_proj_w</code> and <code>c_proj_b</code>.</p>
<p>Here's what the code for a complete multi-headed attention step in the first layer looks like:</p>
<pre title="">WITH    embeddings AS
        (
        SELECT  place, values
        FROM    UNNEST(ARRAY[6307, 47701, 318, 1049]) WITH ORDINALITY AS tokens (token, ordinality)
        CROSS JOIN LATERAL
                (
                SELECT  ordinality - 1 AS place
                ) o
        CROSS JOIN LATERAL
                (
                SELECT  wte.values + wpe.values AS values
                FROM    wte
                CROSS JOIN
                        wpe
                WHERE   wte.token = tokens.token
                        AND wpe.place = o.place
                ) embedding
        ),
        c_proj_w AS
        (
        SELECT  *
        FROM    c_proj_w
        WHERE   block = 0
        ),
        c_proj_b AS
        (
        SELECT  *
        FROM    c_proj_b
        WHERE   block = 0
        ),
        mlp_c_fc_w AS
        (
        SELECT  *
        FROM    mlp_c_fc_w
        WHERE   block = 0
        ),
        mlp_c_fc_b AS
        (
        SELECT  *
        FROM    mlp_c_fc_b
        WHERE   block = 0
        ),
        mlp_c_proj_w AS
        (
        SELECT  *
        FROM    mlp_c_proj_w
        WHERE   block = 0
        ),
        mlp_c_proj_b AS
        (
        SELECT  *
        FROM    mlp_c_proj_b
        WHERE   block = 0
        ),
        c_attn_w AS
        (
        SELECT  *
        FROM    c_attn_w
        WHERE   block = 0
        ),
        c_attn_b AS
        (
        SELECT  *
        FROM    c_attn_b
        WHERE   block = 0
        ),
        ln_1_g AS
        (
        SELECT  *
        FROM    ln_1_g
        WHERE   block = 0
        ),
        ln_1_b AS
        (
        SELECT  *
        FROM    ln_1_b
        WHERE   block = 0
        ),
        mha_norm AS
        (
        SELECT  place, mm.values + c_attn_b.values AS values
        FROM    (
                SELECT  place, ARRAY_AGG(INNER_PRODUCT(c_attn_w.values, layer_norm.values) ORDER BY y)::VECTOR(2304) AS values
                FROM    (
                        SELECT  place, agg.values * ln_1_g.values + ln_1_b.values AS values
                        FROM    (
                                SELECT  place, norm.values
                                FROM    embeddings
                                CROSS JOIN LATERAL
                                        (
                                        SELECT  AVG(value) AS mean,
                                                VAR_POP(value) AS variance
                                        FROM    UNNEST(values::REAL[]) value
                                        ) agg
                                CROSS JOIN LATERAL
                                        (
                                        SELECT  ARRAY_AGG((value - mean) / SQRT(variance + 1E-5) ORDER BY ordinality)::VECTOR(768) AS values
                                        FROM    UNNEST(values::REAL[]) WITH ORDINALITY AS n(value, ordinality)
                                        ) norm
                                ) agg
                        CROSS JOIN
                                ln_1_b
                        CROSS JOIN
                                ln_1_g
                        ) layer_norm
                CROSS JOIN
                        c_attn_w
                GROUP BY
                        place
                ) mm
        CROSS JOIN
                c_attn_b
        ),
        heads AS
        (
        SELECT  place, head,
                (values::REAL[])[(head * 64 + 1):(head * 64 + 64)]::VECTOR(64) AS q,
                (values::REAL[])[(head * 64 + 1 + 768):(head * 64 + 64 + 768)]::VECTOR(64) AS k,
                (values::REAL[])[(head * 64 + 1 + 1536):(head * 64 + 64 + 1536)]::VECTOR(64) AS v
        FROM    mha_norm
        CROSS JOIN
                GENERATE_SERIES(0, 11) head
        ),
        sm_input AS
        (
        SELECT  head, h1.place AS x, h2.place AS y, INNER_PRODUCT(h1.q, h2.k) / 8 + CASE WHEN h2.place &gt; h1.place THEN -1E10 ELSE 0 END AS value
        FROM    heads h1
        JOIN    heads h2
        USING   (head)
        ),
        sm_diff AS
        (
        SELECT  head, x, y, value - MAX(value) OVER (PARTITION BY head, x) AS diff
        FROM    sm_input
        ),
        sm_exp AS
        (
        SELECT  head, x, y, CASE WHEN diff &lt; -745.13 THEN 0 ELSE EXP(diff) END AS e
        FROM    sm_diff
        ),
        softmax AS
        (
        SELECT  head, x, y AS place, e / SUM(e) OVER (PARTITION BY head, x) AS value
        FROM    sm_exp
        ),
        attention AS
        (
        SELECT  place, ARRAY_AGG(value ORDER BY head * 64 + ordinality)::VECTOR(768) AS values
        FROM    (
                SELECT  head, x AS place, SUM(ARRAY_FILL(softmax.value, ARRAY[64])::VECTOR(64) * heads.v) AS values
                FROM    softmax
                JOIN    heads
                USING   (head, place)
                GROUP BY
                        head, x
                ) q
        CROSS JOIN LATERAL
                UNNEST(values::REAL[]) WITH ORDINALITY v (value, ordinality)
        GROUP BY
                place
        ),
        mha AS
        (
        SELECT  place, w.values + c_proj_b.values AS values
        FROM    (
                SELECT  attention.place, ARRAY_AGG(INNER_PRODUCT(attention.values, c_proj_w.values) ORDER BY c_proj_w.place)::VECTOR(768) AS values
                FROM    attention
                CROSS JOIN
                        c_proj_w
                GROUP BY
                        attention.place
                ) w
        CROSS JOIN
                c_proj_b
        )
SELECT  place,
        (SELECT STRING_AGG(TO_CHAR(n, 'S0.000'), ' ') || ' …' FROM UNNEST((values::REAL[])[:10]) AS n) AS q
FROM    mha
</pre>
<div>
<table>
<tbody><tr>
<th>place</th>
<th>q</th>
</tr>
<tr>
<td>0</td>
<td>+0.814 -1.407 +0.171 +0.008 +0.065 -0.049 -0.407 +1.178 -0.234 -0.061 …</td>
</tr>
<tr>
<td>1</td>
<td>+1.150 -0.430 +0.083 +0.030 +0.010 +0.015 -0.245 +3.778 -0.445 -0.004 …</td>
</tr>
<tr>
<td>2</td>
<td>-0.219 -0.745 -0.116 +0.032 +0.064 -0.044 +0.290 +3.187 -0.074 -0.003 …</td>
</tr>
<tr>
<td>3</td>
<td>-0.526 -0.757 -0.510 -0.008 +0.027 -0.017 +0.302 +2.842 +0.188 -0.028 …</td>
</tr>
</tbody></table>
</div>
<p>Before the results of multi-headed attention are passed to the next step, the original inputs are added to them. This trick was described in the original transformer paper. It's supposed to help with vanishing and exploding gradients.</p>
<p>It's a common problem during training: sometimes the gradients of the parameters turn out too big or too small. Changing them on the training iteration either has very little effect on the loss function (and so the model converges very slowly), or, on the opposite, has such a big effect that even a small change throws the loss function too far away from its local minimum, negating the training efforts.</p>
<h3>Feedforward</h3>
<p>This is what the deep neural networks do. The larger part of the model parameters is actually used at this step.</p>
<p>This step is a <a href="https://en.wikipedia.org/wiki/Feedforward_neural_network#Multilayer_perceptron" rel="noopener" target="_blank">multi-layer perceptron</a> with three layers (768, 3072, 768), using the Gaussian Error Linear Unit (GELU) as an activation function:</p>
<p><img decoding="async" src="https://s0.wp.com/latex.php?latex=%5Cmathrm%7BGELU%7D%28x%29+%3D+%5Cdisplaystyle+%5Cfrac+x+2+%5Cleft%281+%2B+%5Cmathrm%7Berf%7D%5C%2C%5Cfrac+x+%7B%5Csqrt+2%7D%5Cright%29&amp;bg=fff&amp;fg=1c1c1c&amp;s=0&amp;c=20201002" alt="\mathrm{GELU}(x) = \displaystyle \frac x 2 \left(1 + \mathrm{erf}\,\frac x {\sqrt 2}\right)"><br>
<img decoding="async" src="https://s0.wp.com/latex.php?latex=%5Cmathrm%7Berf%7D%5C%2Cx+%3D+%5Cdisplaystyle+%5Cfrac%7B2%7D%7B%5Csqrt+%5Cpi%7D%5Cint_0%5Ex%7Be%5E%7B-t%5E2%7D%7D%5C%2Cdt&amp;bg=fff&amp;fg=1c1c1c&amp;s=0&amp;c=20201002" alt="\mathrm{erf}\,x = \displaystyle \frac{2}{\sqrt \pi}\int_0^x{e^{-t^2}}\,dt"></p>
<p>This function <a href="https://arxiv.org/abs/1606.08415" rel="noopener" target="_blank">has been observed</a> to yield good results in deep neural networks. It can be analytically approximated like this:</p>
<p><img decoding="async" src="https://s0.wp.com/latex.php?latex=%5Cmathrm%7BGELU%7D%28x%29+%5Cdisplaystyle+%5Capprox+0.5x+%5Cleft%281+%2B+%5Cmathrm%7Btanh%7D%5Cleft%5B0.797884%5Cleft%28x+%2B+0.044715x%5E3%5Cright%29+%5Cright%5D%5Cright%29+&amp;bg=fff&amp;fg=1c1c1c&amp;s=0&amp;c=20201002" alt="\mathrm{GELU}(x) \displaystyle \approx 0.5x \left(1 + \mathrm{tanh}\left[0.797884\left(x + 0.044715x^3\right) \right]\right) "></p>
<p>The learned linear transformation parameters for layer connections are called <code>c_fc</code> (768 → 3072) and <code>c_proj</code> (3072 → 768). The values for the first layer are first normalized using the coefficients in the learned parameter <code>ln_2</code>. After the feedforward step is completed, its input is again added to the output. This, too, is a part of the original transformer design.</p>
<p>The whole feedforward step looks like this:</p>
<p><img decoding="async" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle+%5Cmathrm%7BFFN%7D%28%5Cmathbf+R%29+%3D+%5Cmathbf+R+%2B+%5Cmathrm%7Bc%5C_proj%7D%5Cleft%28%5Cmathrm%7BGELU%7D%5Cleft%28%5Cmathrm%7Bc%5C_fc%7D%5Cleft%28%5Cmathrm%7Bln%5C_2%7D%5Cleft%28%5Cmathbf+R%5Cright%29%5Cright%29%5Cright%29%5Cright%29+&amp;bg=fff&amp;fg=1c1c1c&amp;s=0&amp;c=20201002" alt="\displaystyle \mathrm{FFN}(\mathbf R) = \mathbf R + \mathrm{c\_proj}\left(\mathrm{GELU}\left(\mathrm{c\_fc}\left(\mathrm{ln\_2}\left(\mathbf R\right)\right)\right)\right) "></p>
<p>And here's how we do this in SQL:</p>
<pre title="">WITH    embeddings AS
        (
        SELECT  place, values
        FROM    UNNEST(ARRAY[6307, 47701, 318, 1049]) WITH ORDINALITY AS tokens (token, ordinality)
        CROSS JOIN LATERAL
                (
                SELECT  ordinality - 1 AS place
                ) o
        CROSS JOIN LATERAL
                (
                SELECT  wte.values + wpe.values AS values
                FROM    wte
                CROSS JOIN
                        wpe
                WHERE   wte.token = tokens.token
                        AND wpe.place = o.place
                ) embedding
        ),
        c_proj_w AS
        (
        SELECT  *
        FROM    c_proj_w
        WHERE   block = 0
        ),
        c_proj_b AS
        (
        SELECT  *
        FROM    c_proj_b
        WHERE   block = 0
        ),
        mlp_c_fc_w AS
        (
        SELECT  *
        FROM    mlp_c_fc_w
        WHERE   block = 0
        ),
        mlp_c_fc_b AS
        (
        SELECT  *
        FROM    mlp_c_fc_b
        WHERE   block = 0
        ),
        mlp_c_proj_w AS
        (
        SELECT  *
        FROM    mlp_c_proj_w
        WHERE   block = 0
        ),
        mlp_c_proj_b AS
        (
        SELECT  *
        FROM    mlp_c_proj_b
        WHERE   block = 0
        ),
        c_attn_w AS
        (
        SELECT  *
        FROM    c_attn_w
        WHERE   block = 0
        ),
        c_attn_b AS
        (
        SELECT  *
        FROM    c_attn_b
        WHERE   block = 0
        ),
        ln_1_g AS
        (
        SELECT  *
        FROM    ln_1_g
        WHERE   block = 0
        ),
        ln_1_b AS
        (
        SELECT  *
        FROM    ln_1_b
        WHERE   block = 0
        ),
        ln_2_b AS
        (
        SELECT  *
        FROM    ln_2_b
        WHERE   block = 0
        ),
        ln_2_g AS
        (
        SELECT  *
        FROM    ln_2_g
        WHERE   block = 0
        ),
        mha_norm AS
        (
        SELECT  place, mm.values + c_attn_b.values AS values
        FROM    (
                SELECT  place, ARRAY_AGG(INNER_PRODUCT(c_attn_w.values, layer_norm.values) ORDER BY y)::VECTOR(2304) AS values
                FROM    (
                        SELECT  place, agg.values * ln_1_g.values + ln_1_b.values AS values
                        FROM    (
                                SELECT  place, norm.values
                                FROM    embeddings
                                CROSS JOIN LATERAL
                                        (
                                        SELECT  AVG(value) AS mean,
                                                VAR_POP(value) AS variance
                                        FROM    UNNEST(values::REAL[]) value
                                        ) agg
                                CROSS JOIN LATERAL
                                        (
                                        SELECT  ARRAY_AGG((value - mean) / SQRT(variance + 1E-5) ORDER BY ordinality)::VECTOR(768) AS values
                                        FROM    UNNEST(values::REAL[]) WITH ORDINALITY AS n(value, ordinality)
                                        ) norm
                                ) agg
                        CROSS JOIN
                                ln_1_b
                        CROSS JOIN
                                ln_1_g
                        ) layer_norm
                CROSS JOIN
                        c_attn_w
                GROUP BY
                        place
                ) mm
        CROSS JOIN
                c_attn_b
        ),
        heads AS
        (
        SELECT  place, head,
                (values::REAL[])[(head * 64 + 1):(head * 64 + 64)]::VECTOR(64) AS q,
                (values::REAL[])[(head * 64 + 1 + 768):(head * 64 + 64 + 768)]::VECTOR(64) AS k,
                (values::REAL[])[(head * 64 + 1 + 1536):(head * 64 + 64 + 1536)]::VECTOR(64) AS v
        FROM    mha_norm
        CROSS JOIN
                GENERATE_SERIES(0, 11) head
        ),
        sm_input AS
        (
        SELECT  head, h1.place AS x, h2.place AS y, INNER_PRODUCT(h1.q, h2.k) / 8 + CASE WHEN h2.place &gt; h1.place THEN -1E10 ELSE 0 END AS value
        FROM    heads h1
        JOIN    heads h2
        USING   (head)
        ),
        sm_diff AS
        (
        SELECT  head, x, y, value - MAX(value) OVER (PARTITION BY head, x) AS diff
        FROM    sm_input
        ),
        sm_exp AS
        (
        SELECT  head, x, y, CASE WHEN diff &lt; -745.13 THEN 0 ELSE EXP(diff) END AS e
        FROM    sm_diff
        ),
        softmax AS
        (
        SELECT  head, x, y AS place, e / SUM(e) OVER (PARTITION BY head, x) AS value
        FROM    sm_exp
        ),
        attention AS
        (
        SELECT  place, ARRAY_AGG(value ORDER BY head * 64 + ordinality)::VECTOR(768) AS values
        FROM    (
                SELECT  head, x AS place, SUM(ARRAY_FILL(softmax.value, ARRAY[64])::VECTOR(64) * heads.v) AS values
                FROM    softmax
                JOIN    heads
                USING   (head, place)
                GROUP BY
                        head, x
                ) q
        CROSS JOIN LATERAL
                UNNEST(values::REAL[]) WITH ORDINALITY v (value, ordinality)
        GROUP BY
                place
        ),
        mha AS
        (
        SELECT  place, w.values + c_proj_b.values + embeddings.values AS values
        FROM    (
                SELECT  attention.place, ARRAY_AGG(INNER_PRODUCT(attention.values, c_proj_w.values) ORDER BY c_proj_w.place)::VECTOR(768) AS values
                FROM    attention
                CROSS JOIN
                        c_proj_w
                GROUP BY
                        attention.place
                ) w
        CROSS JOIN
                c_proj_b
        JOIN    embeddings
        USING   (place)
        ),
        ffn_norm AS
        (
        SELECT  place, agg.values * ln_2_g.values + ln_2_b.values AS values
        FROM    (
                SELECT  place, norm.values
                FROM    mha
                CROSS JOIN LATERAL
                        (
                        SELECT  AVG(value) AS mean,
                                VAR_POP(value) AS variance
                        FROM    UNNEST(values::REAL[]) value
                        ) agg
                CROSS JOIN LATERAL
                        (
                        SELECT  ARRAY_AGG((value - mean) / SQRT(variance + 1E-5) ORDER BY ordinality)::VECTOR(768) AS values
                        FROM    UNNEST(values::REAL[]) WITH ORDINALITY AS n(value, ordinality)
                        ) norm
                ) agg
        CROSS JOIN
                ln_2_b
        CROSS JOIN
                ln_2_g
        ),
        ffn_a AS
        (
        SELECT  gelu.place, gelu.values
        FROM    (
                SELECT  place, w.values + mlp_c_fc_b.values AS values
                FROM    (
                        SELECT  ffn_norm.place, ARRAY_AGG(INNER_PRODUCT(ffn_norm.values, mlp_c_fc_w.values) ORDER BY mlp_c_fc_w.place)::VECTOR(3072) AS values
                        FROM    ffn_norm
                        CROSS JOIN
                                mlp_c_fc_w
                        GROUP BY
                                ffn_norm.place
                        ) w
                CROSS JOIN
                        mlp_c_fc_b
                ) v
        CROSS JOIN LATERAL
                (
                SELECT  place, ARRAY_AGG(0.5 * value * (1 + TANH(0.797884560802 * (value + 0.044715 * value*value*value))) ORDER BY ordinality)::VECTOR(3072) AS values
                FROM    UNNEST(values::REAL[]) WITH ORDINALITY n (value, ordinality)
                GROUP BY
                        place
                ) gelu
        ),
        ffn AS
        (
        SELECT  place, w.values + mlp_c_proj_b.values + mha.values AS values
        FROM    (
                SELECT  ffn_a.place, ARRAY_AGG(INNER_PRODUCT(ffn_a.values, mlp_c_proj_w.values) ORDER BY mlp_c_proj_w.place)::VECTOR(768) AS values
                FROM    ffn_a
                CROSS JOIN
                        mlp_c_proj_w
                GROUP BY
                        ffn_a.place
                ) w
        CROSS JOIN
                mlp_c_proj_b
        JOIN    mha
        USING   (place)
        )
SELECT  place,
        (SELECT STRING_AGG(TO_CHAR(n, 'S0.000'), ' ') || ' …' FROM UNNEST((values::REAL[])[:10]) AS n) AS q
FROM    ffn
</pre>
<div>
<table>
<tbody><tr>
<th>place</th>
<th>q</th>
</tr>
<tr>
<td>0</td>
<td>+0.309 -1.267 -0.250 -1.111 -0.226 +0.549 -0.346 +0.645 -1.603 -0.501 …</td>
</tr>
<tr>
<td>1</td>
<td>+0.841 -1.081 +0.227 -1.029 -1.554 +1.061 -0.070 +5.258 -1.892 -0.973 …</td>
</tr>
<tr>
<td>2</td>
<td>-1.256 -0.528 -0.846 -0.288 +0.166 +0.409 +0.019 +3.393 +0.085 -0.212 …</td>
</tr>
<tr>
<td>3</td>
<td>-1.007 -1.719 -0.725 -1.417 -0.086 -0.144 +0.605 +3.272 +1.051 -0.666 …</td>
</tr>
</tbody></table>
</div>
<p>This output is what comes out of the first block of GPT2.</p>
<h3>Blocks</h3>
<p>What we saw in the previous steps is repeated in layers (called "blocks"). The blocks are set up in a pipeline so that the output of a previous block goes straight to the next one. Each block has its own set of learned parameters.</p>
<p>In SQL, we would need to connect the blocks using a recursive CTE.</p>
<p>Once the final block produces the values, we need to normalize it using the learned parameter <code>ln_f</code>.</p>
<p>Here's what the model ultimately looks like:</p>
<p><img decoding="async" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle+%5Cmathrm%7BGPT%7D%28tokens%29+%3D+%5Cmathrm%7Bln%5C_f%7D%28%5Cmathbf+R_%7B12%7D%29&amp;bg=fff&amp;fg=1c1c1c&amp;s=0&amp;c=20201002" alt="\displaystyle \mathrm{GPT}(tokens) = \mathrm{ln\_f}(\mathbf R_{12})"></p>
<p><img decoding="async" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle+%5Cmathbf+R_%7Bn%7D+%3D+%5Cmathrm%7Bblock_n%7D%28%5Cmathbf+R_%7Bn-1%7D%29%2C+n+%3E+0&amp;bg=fff&amp;fg=1c1c1c&amp;s=0&amp;c=20201002" alt="\displaystyle \mathbf R_{n} = \mathrm{block_n}(\mathbf R_{n-1}), n > 0"></p>
<p><img decoding="async" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle+%5Cmathrm%7Bblock_n%7D%28%5Cmathbf+R%29+%3D+%5Cmathrm%7Bffn_n%7D%28%5Cmathrm%7Bmha_n%7D%28%5Cmathbf+R%29%29&amp;bg=fff&amp;fg=1c1c1c&amp;s=0&amp;c=20201002" alt="\displaystyle \mathrm{block_n}(\mathbf R) = \mathrm{ffn_n}(\mathrm{mha_n}(\mathbf R))"></p>
<p><img decoding="async" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle+%5Cmathbf+R_0+%3D+%5Cmathrm%7Bwte%7D%28tokens%29+%2B+%5Cmathrm%7Bwpe%7D%28%5B1+%5Cldots+%5Cmathrm%7Bdim%7D%28tokens%29%5D%29&amp;bg=fff&amp;fg=1c1c1c&amp;s=0&amp;c=20201002" alt="\displaystyle \mathbf R_0 = \mathrm{wte}(tokens) + \mathrm{wpe}([1 \ldots \mathrm{dim}(tokens)])"></p>
<p>And here's how it looks in SQL:</p>
<pre title="">WITH    RECURSIVE
        initial AS
        (
        SELECT  ARRAY[6307, 47701, 318, 1049] AS input
        ),
        hparams AS
        (
        SELECT  12 AS n_block
        ),
        embeddings AS
        (
        SELECT  place, values
        FROM    initial
        CROSS JOIN
                hparams
        CROSS JOIN LATERAL
                UNNEST(input) WITH ORDINALITY AS tokens (token, ordinality)
        CROSS JOIN LATERAL
                (
                SELECT  ordinality - 1 AS place
                ) o
        CROSS JOIN LATERAL
                (
                SELECT  wte.values + wpe.values AS values
                FROM    wte
                CROSS JOIN
                        wpe
                WHERE   wte.token = tokens.token
                        AND wpe.place = o.place
                ) embedding
        ),
        transform AS
        (
        SELECT  0 AS block, place, values
        FROM    embeddings
        UNION ALL
        (
        WITH    previous AS
                (
                SELECT  *
                FROM    transform
                )
        SELECT  block + 1 AS block, transformed_layer.*
        FROM    hparams
        CROSS JOIN LATERAL
                (
                SELECT  block
                FROM    previous
                WHERE   block &lt; 12
                LIMIT   1
                ) q
        CROSS JOIN LATERAL
                (
                WITH    ln_2_b AS
                        (
                        SELECT  *
                        FROM    ln_2_b
                        WHERE   block = q.block
                        ),
                        ln_2_g AS
                        (
                        SELECT  *
                        FROM    ln_2_g
                        WHERE   block = q.block
                        ),
                        c_proj_w AS
                        (
                        SELECT  *
                        FROM    c_proj_w
                        WHERE   block = q.block
                        ),
                        c_proj_b AS
                        (
                        SELECT  *
                        FROM    c_proj_b
                        WHERE   block = q.block
                        ),
                        mlp_c_fc_w AS
                        (
                        SELECT  *
                        FROM    mlp_c_fc_w
                        WHERE   block = q.block
                        ),
                        mlp_c_fc_b AS
                        (
                        SELECT  *
                        FROM    mlp_c_fc_b
                        WHERE   block = q.block
                        ),
                        mlp_c_proj_w AS
                        (
                        SELECT  *
                        FROM    mlp_c_proj_w
                        WHERE   block = q.block
                        ),
                        mlp_c_proj_b AS
                        (
                        SELECT  *
                        FROM    mlp_c_proj_b
                        WHERE   block = q.block
                        ),
                        c_attn_w AS
                        (
                        SELECT  *
                        FROM    c_attn_w
                        WHERE   block = q.block
                        ),
                        c_attn_b AS
                        (
                        SELECT  *
                        FROM    c_attn_b
                        WHERE   block = q.block
                        ),
                        ln_1_g AS
                        (
                        SELECT  *
                        FROM    ln_1_g
                        WHERE   block = q.block
                        ),
                        ln_1_b AS
                        (
                        SELECT  *
                        FROM    ln_1_b
                        WHERE   block = q.block
                        ),
                        mha_norm AS
                        (
                        SELECT  place, mm.values + c_attn_b.values AS values
                        FROM    (
                                SELECT  place, ARRAY_AGG(INNER_PRODUCT(c_attn_w.values, layer_norm.values) ORDER BY y)::VECTOR(2304) AS values
                                FROM    (
                                        SELECT  place, agg.values * ln_1_g.values + ln_1_b.values AS values
                                        FROM    (
                                                SELECT  place, norm.values
                                                FROM    previous
                                                CROSS JOIN LATERAL
                                                        (
                                                        SELECT  AVG(value) AS mean,
                                                                VAR_POP(value) AS variance
                                                        FROM    UNNEST(values::REAL[]) value
                                                        ) agg
                                                CROSS JOIN LATERAL
                                                        (
                                                        SELECT  ARRAY_AGG((value - mean) / SQRT(variance + 1E-5) ORDER BY ordinality)::VECTOR(768) AS values
                                                        FROM    UNNEST(values::REAL[]) WITH ORDINALITY AS n(value, ordinality)
                                                        ) norm
                                                ) agg
                                        CROSS JOIN
                                                ln_1_b
                                        CROSS JOIN
                                                ln_1_g
                                        ) layer_norm
                                CROSS JOIN
                                        c_attn_w
                                GROUP BY
                                        place
                                ) mm
                        CROSS JOIN
                                c_attn_b
                        ),
                        heads AS
                        (
                        SELECT  place, head,
                                (values::REAL[])[(head * 64 + 1):(head * 64 + 64)]::VECTOR(64) AS q,
                                (values::REAL[])[(head * 64 + 1 + 768):(head * 64 + 64 + 768)]::VECTOR(64) AS k,
                                (values::REAL[])[(head * 64 + 1 + 1536):(head * 64 + 64 + 1536)]::VECTOR(64) AS v
                        FROM    mha_norm
                        CROSS JOIN
                                GENERATE_SERIES(0, 11) head
                        ),
                        sm_input AS
                        (
                        SELECT  head, h1.place AS x, h2.place AS y, INNER_PRODUCT(h1.q, h2.k) / 8 + CASE WHEN h2.place &gt; h1.place THEN -1E10 ELSE 0 END AS value
                        FROM    heads h1
                        JOIN    heads h2
                        USING   (head)
                        ),
                        sm_diff AS
                        (
                        SELECT  head, x, y, value - MAX(value) OVER (PARTITION BY head, x) AS diff
                        FROM    sm_input
                        ),
                        sm_exp AS
                        (
                        SELECT  head, x, y, CASE WHEN diff &lt; -745.13 THEN 0 ELSE EXP(diff) END AS e
                        FROM    sm_diff
                        ),
                        softmax AS
                        (
                        SELECT  head, x, y AS place, e / SUM(e) OVER (PARTITION BY head, x) AS value
                        FROM    sm_exp
                        ),
                        attention AS
                        (
                        SELECT  place, ARRAY_AGG(value ORDER BY head * 64 + ordinality)::VECTOR(768) AS values
                        FROM    (
                                SELECT  head, x AS place, SUM(ARRAY_FILL(softmax.value, ARRAY[64])::VECTOR(64) * heads.v) AS values
                                FROM    softmax
                                JOIN    heads
                                USING   (head, place)
                                GROUP BY
                                        head, x
                                ) q
                        CROSS JOIN LATERAL
                                UNNEST(values::REAL[]) WITH ORDINALITY v (value, ordinality)
                        GROUP BY
                                place
                        ),
                        mha AS
                        (
                        SELECT  place, w.values + c_proj_b.values + previous.values AS values
                        FROM    (
                                SELECT  attention.place, ARRAY_AGG(INNER_PRODUCT(attention.values, c_proj_w.values) ORDER BY c_proj_w.place)::VECTOR(768) AS values
                                FROM    attention
                                CROSS JOIN
                                        c_proj_w
                                GROUP BY
                                        attention.place
                                ) w
                        CROSS JOIN
                                c_proj_b
                        JOIN    previous
                        USING   (place)
                        ),
                        ffn_norm AS
                        (
                        SELECT  place, agg.values * ln_2_g.values + ln_2_b.values AS values
                        FROM    (
                                SELECT  place, norm.values
                                FROM    mha
                                CROSS JOIN LATERAL
                                        (
                                        SELECT  AVG(value) AS mean,
                                                VAR_POP(value) AS variance
                                        FROM    UNNEST(values::REAL[]) value
                                        ) agg
                                CROSS JOIN LATERAL
                                        (
                                        SELECT  ARRAY_AGG((value - mean) / SQRT(variance + 1E-5) ORDER BY ordinality)::VECTOR(768) AS values
                                        FROM    UNNEST(values::REAL[]) WITH ORDINALITY AS n(value, ordinality)
                                        ) norm
                                ) agg
                        CROSS JOIN
                                ln_2_b
                        CROSS JOIN
                                ln_2_g
                        ),
                        ffn_a AS
                        (
                        SELECT  gelu.place, gelu.values
                        FROM    (
                                SELECT  place, w.values + mlp_c_fc_b.values AS values
                                FROM    (
                                        SELECT  ffn_norm.place, ARRAY_AGG(INNER_PRODUCT(ffn_norm.values, mlp_c_fc_w.values) ORDER BY mlp_c_fc_w.place)::VECTOR(3072) AS values
                                        FROM    ffn_norm
                                        CROSS JOIN
                                                mlp_c_fc_w
                                        GROUP BY
                                                ffn_norm.place
                                        ) w
                                CROSS JOIN
                                        mlp_c_fc_b
                                ) v
                        CROSS JOIN LATERAL
                                (
                                SELECT  place, ARRAY_AGG(0.5 * value * (1 + TANH(0.797884560802 * (value + 0.044715 * value*value*value))) ORDER BY ordinality)::VECTOR(3072) AS values
                                FROM    UNNEST(values::REAL[]) WITH ORDINALITY n (value, ordinality)
                                GROUP BY
                                        place
                                ) gelu
                        ),
                        ffn AS
                        (
                        SELECT  place, w.values + mlp_c_proj_b.values + mha.values AS values
                        FROM    (
                                SELECT  ffn_a.place, ARRAY_AGG(INNER_PRODUCT(ffn_a.values, mlp_c_proj_w.values) ORDER BY mlp_c_proj_w.place)::VECTOR(768) AS values
                                FROM    ffn_a
                                CROSS JOIN
                                        mlp_c_proj_w
                                GROUP BY
                                        ffn_a.place
                                ) w
                        CROSS JOIN
                                mlp_c_proj_b
                        JOIN    mha
                        USING   (place)
                        )
                SELECT  *
                FROM    ffn
                ) transformed_layer
        )
        ),
        block_output AS
        (
        SELECT  *
        FROM    hparams
        JOIN    transform
        ON      transform.block = n_block
        ),
        ln_f AS
        (
        SELECT  place, norm.values * ln_f_g.values + ln_f_b.values AS values
        FROM    block_output
        CROSS JOIN LATERAL
                (
                SELECT  AVG(value) AS mean,
                        VAR_POP(value) AS variance
                FROM    UNNEST(values::REAL[]) AS n(value)
                ) agg
        CROSS JOIN LATERAL
                (
                SELECT  ARRAY_AGG((value - mean) / SQRT(variance + 1E-5) ORDER BY ordinality)::VECTOR(768) AS values
                FROM    UNNEST(values::REAL[]) WITH ORDINALITY AS n (value, ordinality)
                ) norm
        CROSS JOIN
                ln_f_b
        CROSS JOIN
                ln_f_g
        )
SELECT  place,
        (SELECT STRING_AGG(TO_CHAR(n, 'S0.000'), ' ') || ' …' FROM UNNEST((values::REAL[])[:10]) AS n) AS q
FROM    ln_f
</pre>
<div>
<table>
<tbody><tr>
<th>place</th>
<th>q</th>
</tr>
<tr>
<td>0</td>
<td>-0.153 -0.126 -0.368 +0.028 -0.013 -0.198 +0.661 +0.056 -0.228 -0.001 …</td>
</tr>
<tr>
<td>1</td>
<td>-0.157 -0.314 +0.291 -0.386 -0.273 -0.054 +3.397 +0.440 -0.137 -0.243 …</td>
</tr>
<tr>
<td>2</td>
<td>-0.912 -0.220 -0.886 -0.661 +0.491 -0.050 +0.693 +1.128 +0.031 -0.577 …</td>
</tr>
<tr>
<td>3</td>
<td>-0.098 -0.323 -1.479 -0.736 +0.235 -0.608 +1.774 +0.566 -0.057 -0.211 …</td>
</tr>
</tbody></table>
</div>
<p>This is the output of the model. </p>
<p>The fourth vector is the actual embedding of the next token predicted by the model. We just need to map it back to the tokens.</p>
<h3>Tokens</h3>
<p>We have an embedding (a 768-vector) which, according to the model, captures the semantics and the grammar of the most likely continuation of the prompt. Now we need to map it back to the token.</p>
<p>One of the first steps the model makes is mapping the tokens to their embeddings. It is done through the 50257×768 matrix <code>wpe</code>. We will need to use the same matrix to map the embedding back to the token.</p>
<p>The problem is that the exact reverse mapping is not possible: the embedding will not (likely) be equal to any of the rows in the matrix. So we will need to find the "closest" token to the embedding.</p>
<p>Since the dimensions of embeddings capture (as we hope) some semantic and grammatical aspects of the token, we need them to match as closely as possible. One way to consolidate the closeness of each dimension would be to just calculate the dot product of the two embeddings. The higher the dot product, the closer the token is to the prediction.</p>
<p>To do this, we will multiply the embedding by the matrix <code>wte</code>. The result will be a single-column matrix, 50257 rows tall. Each value in this result will be the dot product of the predicted embedding and the token embedding. The higher this number, the more likely it is for the token to continue the prompt.</p>
<p>To pick the next token, we will need to convert the similarities to probabilities. To do this, we will use our good friend softmax (the same function that we used to normalize attention weights).</p>
<h4>Why use softmax for probabilities?</h4>
<p>Softmax has the nice property of satisfying <a href="https://en.wikipedia.org/wiki/Luce%27s_choice_axiom" rel="noopener" target="_blank">Luce's choice axiom</a>. It means that the relative probabilities of two options don't depend on the presence or probability of other options. If A is twice as probable as B, then the presence or absence of other options will not change this ratio (although it of course can change the absolute values).</p>
<p>The vector of dot products ("logit" in AI parlance) contains arbitrary scores that don't have an intrinsic scale. If A has a larger score than B, we know that it's more likely, but that's about it. We can tweak the inputs to softmax as we please, as long as they keep their order (i.e. larger scores stay larger).</p>
<p>One common way to do that is to normalize the scores by subtracting the greatest value from the set from them (so that the biggest score becomes 0 and the rest become negative numbers). Then we take some fixed number (let's say five or ten) top scores. Finally, we multiply each score by a constant before feeding it to softmax.</p>
<p>The number of top scores that we take is usually called <img decoding="async" src="https://s0.wp.com/latex.php?latex=top%5C_n&amp;bg=fff&amp;fg=1c1c1c&amp;s=0&amp;c=20201002" alt="top\_n"> and the multiplication constant (or, rather, its reverse) is called "temperature" (<img decoding="async" src="https://s0.wp.com/latex.php?latex=T&amp;bg=fff&amp;fg=1c1c1c&amp;s=0&amp;c=20201002" alt="T">). The higher the temperature, the more smoothed out the probabilities, and the bigger the chance that the next picked token will not be just the first one.</p>
<p>The formula for tokens' probabilities is <img decoding="async" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle+p_n+%3D+%5Cmathrm%7Bsoftmax_n%5Cleft%28%5Cfrac%7B%5Cmathbf%7Bscores%7D%7D%7BT%7D%5Cright%29%7D&amp;bg=fff&amp;fg=1c1c1c&amp;s=0&amp;c=20201002" alt="\displaystyle p_n = \mathrm{softmax_n\left(\frac{\mathbf{scores}}{T}\right)}">, where <img decoding="async" src="https://s0.wp.com/latex.php?latex=%5Cmathbf%7Bscores%7D&amp;bg=fff&amp;fg=1c1c1c&amp;s=0&amp;c=20201002" alt="\mathbf{scores}"> is the set of <img decoding="async" src="https://s0.wp.com/latex.php?latex=top%5C_n&amp;bg=fff&amp;fg=1c1c1c&amp;s=0&amp;c=20201002" alt="top\_n"> scores.</p>
<h4>Why is it called "temperature"?</h4>
<p>The softmax function has another name: <a href="https://en.wikipedia.org/wiki/Boltzmann_distribution" rel="noopener" target="_blank">Boltzmann distribution</a>. It's extensively used in physics. Among other things, it serves as a base for the <a href="https://en.wikipedia.org/wiki/Barometric_formula" rel="noopener" target="_blank">barometric formula</a>, which tells how density or air varies with altitude.</p>
<p>Intuitively, hot air rises. It spreads further away from the Earth. When air is hot, it's more likely for an air molecule to bounce off its neighbors and jump at an otherwise impossible height. Compared to colder temperatures, air density increases at higher altitudes and drops at sea level.</p>
<p>See how air behaves at different temperatures:</p>


<p><em>Courtesy of Dominic Ford, <a href="https://sciencedemos.org.uk/balls.php" rel="noopener" target="_blank">Bouncing Balls and the Boltzmann Distribution</a></em></p>
<p>By analogy, a large "temperature" increases the probability of second-choice tokens being selected (at the expense of the first-choice tokens, of course). The inference becomes less predictable and more "creative".</p>
<p>Let's put this all into SQL. The prompt was "PostgreSQL is great". Here are the top 5 tokens that, according to the model, are most likely to continue this phrase, and their probabilities at different temperatures:</p>
<pre title="">WITH    RECURSIVE
        initial AS
        (
        SELECT  ARRAY[6307, 47701, 318, 1049] AS input
        ),
        hparams AS
        (
        SELECT  12 AS n_block,
                5 AS top_n,
                ARRAY_LENGTH(input, 1) AS n_seq
        FROM    initial
        ),
        embeddings AS
        (
        SELECT  place, values
        FROM    initial
        CROSS JOIN
                hparams
        CROSS JOIN LATERAL
                UNNEST(input) WITH ORDINALITY AS tokens (token, ordinality)
        CROSS JOIN LATERAL
                (
                SELECT  ordinality - 1 AS place
                ) o
        CROSS JOIN LATERAL
                (
                SELECT  wte.values + wpe.values AS values
                FROM    wte
                CROSS JOIN
                        wpe
                WHERE   wte.token = tokens.token
                        AND wpe.place = o.place
                ) embedding
        ),
        transform AS
        (
        SELECT  0 AS block, place, values
        FROM    embeddings
        UNION ALL
        (
        WITH    previous AS
                (
                SELECT  *
                FROM    transform
                )
        SELECT  block + 1 AS block, transformed_layer.*
        FROM    hparams
        CROSS JOIN LATERAL
                (
                SELECT  block
                FROM    previous
                WHERE   block &lt; 12
                LIMIT   1
                ) q
        CROSS JOIN LATERAL
                (
                WITH    ln_2_b AS
                        (
                        SELECT  *
                        FROM    ln_2_b
                        WHERE   block = q.block
                        ),
                        ln_2_g AS
                        (
                        SELECT  *
                        FROM    ln_2_g
                        WHERE   block = q.block
                        ),
                        c_proj_w AS
                        (
                        SELECT  *
                        FROM    c_proj_w
                        WHERE   block = q.block
                        ),
                        c_proj_b AS
                        (
                        SELECT  *
                        FROM    c_proj_b
                        WHERE   block = q.block
                        ),
                        mlp_c_fc_w AS
                        (
                        SELECT  *
                        FROM    mlp_c_fc_w
                        WHERE   block = q.block
                        ),
                        mlp_c_fc_b AS
                        (
                        SELECT  *
                        FROM    mlp_c_fc_b
                        WHERE   block = q.block
                        ),
                        mlp_c_proj_w AS
                        (
                        SELECT  *
                        FROM    mlp_c_proj_w
                        WHERE   block = q.block
                        ),
                        mlp_c_proj_b AS
                        (
                        SELECT  *
                        FROM    mlp_c_proj_b
                        WHERE   block = q.block
                        ),
                        c_attn_w AS
                        (
                        SELECT  *
                        FROM    c_attn_w
                        WHERE   block = q.block
                        ),
                        c_attn_b AS
                        (
                        SELECT  *
                        FROM    c_attn_b
                        WHERE   block = q.block
                        ),
                        ln_1_g AS
                        (
                        SELECT  *
                        FROM    ln_1_g
                        WHERE   block = q.block
                        ),
                        ln_1_b AS
                        (
                        SELECT  *
                        FROM    ln_1_b
                        WHERE   block = q.block
                        ),
                        mha_norm AS
                        (
                        SELECT  place, mm.values + c_attn_b.values AS values
                        FROM    (
                                SELECT  place, ARRAY_AGG(INNER_PRODUCT(c_attn_w.values, layer_norm.values) ORDER BY y)::VECTOR(2304) AS values
                                FROM    (
                                        SELECT  place, agg.values * ln_1_g.values + ln_1_b.values AS values
                                        FROM    (
                                                SELECT  place, norm.values
                                                FROM    previous
                                                CROSS JOIN LATERAL
                                                        (
                                                        SELECT  AVG(value) AS mean,
                                                                VAR_POP(value) AS variance
                                                        FROM    UNNEST(values::REAL[]) value
                                                        ) agg
                                                CROSS JOIN LATERAL
                                                        (
                                                        SELECT  ARRAY_AGG((value - mean) / SQRT(variance + 1E-5) ORDER BY ordinality)::VECTOR(768) AS values
                                                        FROM    UNNEST(values::REAL[]) WITH ORDINALITY AS n(value, ordinality)
                                                        ) norm
                                                ) agg
                                        CROSS JOIN
                                                ln_1_b
                                        CROSS JOIN
                                                ln_1_g
                                        ) layer_norm
                                CROSS JOIN
                                        c_attn_w
                                GROUP BY
                                        place
                                ) mm
                        CROSS JOIN
                                c_attn_b
                        ),
                        heads AS
                        (
                        SELECT  place, head,
                                (values::REAL[])[(head * 64 + 1):(head * 64 + 64)]::VECTOR(64) AS q,
                                (values::REAL[])[(head * 64 + 1 + 768):(head * 64 + 64 + 768)]::VECTOR(64) AS k,
                                (values::REAL[])[(head * 64 + 1 + 1536):(head * 64 + 64 + 1536)]::VECTOR(64) AS v
                        FROM    mha_norm
                        CROSS JOIN
                                GENERATE_SERIES(0, 11) head
                        ),
                        sm_input AS
                        (
                        SELECT  head, h1.place AS x, h2.place AS y, INNER_PRODUCT(h1.q, h2.k) / 8 + CASE WHEN h2.place &gt; h1.place THEN -1E10 ELSE 0 END AS value
                        FROM    heads h1
                        JOIN    heads h2
                        USING   (head)
                        ),
                        sm_diff AS
                        (
                        SELECT  head, x, y, value - MAX(value) OVER (PARTITION BY head, x) AS diff
                        FROM    sm_input
                        ),
                        sm_exp AS
                        (
                        SELECT  head, x, y, CASE WHEN diff &lt; -745.13 THEN 0 ELSE EXP(diff) END AS e
                        FROM    sm_diff
                        ),
                        softmax AS
                        (
                        SELECT  head, x, y AS place, e / SUM(e) OVER (PARTITION BY head, x) AS value
                        FROM    sm_exp
                        ),
                        attention AS
                        (
                        SELECT  place, ARRAY_AGG(value ORDER BY head * 64 + ordinality)::VECTOR(768) AS values
                        FROM    (
                                SELECT  head, x AS place, SUM(ARRAY_FILL(softmax.value, ARRAY[64])::VECTOR(64) * heads.v) AS values
                                FROM    softmax
                                JOIN    heads
                                USING   (head, place)
                                GROUP BY
                                        head, x
                                ) q
                        CROSS JOIN LATERAL
                                UNNEST(values::REAL[]) WITH ORDINALITY v (value, ordinality)
                        GROUP BY
                                place
                        ),
                        mha AS
                        (
                        SELECT  place, w.values + c_proj_b.values + previous.values AS values
                        FROM    (
                                SELECT  attention.place, ARRAY_AGG(INNER_PRODUCT(attention.values, c_proj_w.values) ORDER BY c_proj_w.place)::VECTOR(768) AS values
                                FROM    attention
                                CROSS JOIN
                                        c_proj_w
                                GROUP BY
                                        attention.place
                                ) w
                        CROSS JOIN
                                c_proj_b
                        JOIN    previous
                        USING   (place)
                        ),
                        ffn_norm AS
                        (
                        SELECT  place, agg.values * ln_2_g.values + ln_2_b.values AS values
                        FROM    (
                                SELECT  place, norm.values
                                FROM    mha
                                CROSS JOIN LATERAL
                                        (
                                        SELECT  AVG(value) AS mean,
                                                VAR_POP(value) AS variance
                                        FROM    UNNEST(values::REAL[]) value
                                        ) agg
                                CROSS JOIN LATERAL
                                        (
                                        SELECT  ARRAY_AGG((value - mean) / SQRT(variance + 1E-5) ORDER BY ordinality)::VECTOR(768) AS values
                                        FROM    UNNEST(values::REAL[]) WITH ORDINALITY AS n(value, ordinality)
                                        ) norm
                                ) agg
                        CROSS JOIN
                                ln_2_b
                        CROSS JOIN
                                ln_2_g
                        ),
                        ffn_a AS
                        (
                        SELECT  gelu.place, gelu.values
                        FROM    (
                                SELECT  place, w.values + mlp_c_fc_b.values AS values
                                FROM    (
                                        SELECT  ffn_norm.place, ARRAY_AGG(INNER_PRODUCT(ffn_norm.values, mlp_c_fc_w.values) ORDER BY mlp_c_fc_w.place)::VECTOR(3072) AS values
                                        FROM    ffn_norm
                                        CROSS JOIN
                                                mlp_c_fc_w
                                        GROUP BY
                                                ffn_norm.place
                                        ) w
                                CROSS JOIN
                                        mlp_c_fc_b
                                ) v
                        CROSS JOIN LATERAL
                                (
                                SELECT  place, ARRAY_AGG(0.5 * value * (1 + TANH(0.797884560802 * (value + 0.044715 * value*value*value))) ORDER BY ordinality)::VECTOR(3072) AS values
                                FROM    UNNEST(values::REAL[]) WITH ORDINALITY n (value, ordinality)
                                GROUP BY
                                        place
                                ) gelu
                        ),
                        ffn AS
                        (
                        SELECT  place, w.values + mlp_c_proj_b.values + mha.values AS values
                        FROM    (
                                SELECT  ffn_a.place, ARRAY_AGG(INNER_PRODUCT(ffn_a.values, mlp_c_proj_w.values) ORDER BY mlp_c_proj_w.place)::VECTOR(768) AS values
                                FROM    ffn_a
                                CROSS JOIN
                                        mlp_c_proj_w
                                GROUP BY
                                        ffn_a.place
                                ) w
                        CROSS JOIN
                                mlp_c_proj_b
                        JOIN    mha
                        USING   (place)
                        )
                SELECT  *
                FROM    ffn
                ) transformed_layer
        )
        ),
        block_output AS
        (
        SELECT  *
        FROM    hparams
        JOIN    transform
        ON      transform.block = n_block
        ),
        ln_f AS
        (
        SELECT  place, norm.values * ln_f_g.values + ln_f_b.values AS values
        FROM    block_output
        CROSS JOIN LATERAL
                (
                SELECT  AVG(value) AS mean,
                        VAR_POP(value) AS variance
                FROM    UNNEST(values::REAL[]) AS n(value)
                ) agg
        CROSS JOIN LATERAL
                (
                SELECT  ARRAY_AGG((value - mean) / SQRT(variance + 1E-5) ORDER BY ordinality)::VECTOR(768) AS values
                FROM    UNNEST(values::REAL[]) WITH ORDINALITY AS n (value, ordinality)
                ) norm
        CROSS JOIN
                ln_f_b
        CROSS JOIN
                ln_f_g
        ),
        logits AS
        (
        SELECT  logits.*
        FROM    hparams
        CROSS JOIN LATERAL
                (
                SELECT  token, INNER_PRODUCT(ln_f.values, wte.values) AS value
                FROM    ln_f
                CROSS JOIN
                        wte
                WHERE   ln_f.place = n_seq - 1
                ORDER BY
                        value DESC
                LIMIT   (top_n)
                ) logits
        ),
        temperatures (temperature) AS
        (
        VALUES
        (0.5),
        (1),
        (2)
        ),
        tokens AS
        (
        SELECT  token, value, softmax, temperature
        FROM    temperatures
        CROSS JOIN LATERAL
                (
                SELECT  *, (e / SUM(e) OVER ()) AS softmax
                FROM    (
                        SELECT  *,
                                (value - MAX(value) OVER ()) / temperature AS diff
                        FROM    logits
                        ) exp_x
                CROSS JOIN LATERAL
                        (
                        SELECT  CASE WHEN diff &lt; -745.13 THEN 0 ELSE EXP(diff) END AS e
                        ) exp
                ) q
        )
SELECT  token,
        cluster,
        TO_CHAR(t1.value, 'S00.000') AS score,
        TO_CHAR(t1.softmax, '0.00') AS "temperature = 0.5",
        TO_CHAR(t2.softmax, '0.00') AS "temperature = 1",
        TO_CHAR(t3.softmax, '0.00') AS "temperature = 2"
FROM    (
        SELECT  *
        FROM    tokens
        WHERE   temperature = 0.5
        ) t1
JOIN    (
        SELECT  *
        FROM    tokens
        WHERE   temperature = 1
        ) t2
USING   (token)
JOIN    (
        SELECT  *
        FROM    tokens
        WHERE   temperature = 2
        ) t3
USING   (token)
JOIN    tokenizer
USING   (token)
</pre>
<div>
<table>
<tbody><tr>
<th>token</th>
<th>cluster</th>
<th>score</th>
<th>temperature = 0.5</th>
<th>temperature = 1</th>
<th>temperature = 2</th>
</tr>
<tr>
<td>329</td>
<td>Ġfor</td>
<td>-85.435</td>
<td> 0.74</td>
<td> 0.48</td>
<td> 0.33</td>
</tr>
<tr>
<td>11</td>
<td>,</td>
<td>-86.232</td>
<td> 0.15</td>
<td> 0.22</td>
<td> 0.22</td>
</tr>
<tr>
<td>13</td>
<td>.</td>
<td>-86.734</td>
<td> 0.05</td>
<td> 0.13</td>
<td> 0.17</td>
</tr>
<tr>
<td>379</td>
<td>Ġat</td>
<td>-86.785</td>
<td> 0.05</td>
<td> 0.12</td>
<td> 0.17</td>
</tr>
<tr>
<td>284</td>
<td>Ġto</td>
<td>-87.628</td>
<td> 0.01</td>
<td> 0.05</td>
<td> 0.11</td>
</tr>
</tbody></table>
</div>
<h3>Inference</h3>
<p>Finally, we are ready to do some real inference: run the model, select a token according to its probability, add it to the prompt and repeat until enough tokens are generated.</p>
<p>The LLM itself, as we saw before, is deterministic: it's just a series of matrix multiplications and other math operations on predefined constants. As long as the prompt and the hyperparameters like temperature and top_n are the same, the output will also be the same.</p>
<p>The only non-deterministic process is token selection. There is randomness involved in it (to a variable degree). That's why GPT-based chatbots can give different answers to the same prompt.</p>
<p>We will use the phrase "Happy New Year! I wish" as the prompt and make the model generate 10 new tokens for this prompt. The temperature will be set to 2, and top_n will be set to 5.</p>
<p>The query runs for 2:44 minutes on my machine. Here's its output:</p>
<pre title="">SELECT SETSEED(0.20231231);

WITH    RECURSIVE
        input AS
        (
        SELECT  'Happy New Year! I wish you' AS prompt,
                10 AS threshold,
                2 AS temperature,
                1 AS top_n
        ),
        clusters AS
        (
        SELECT  part_position, bpe.*
        FROM    input
        CROSS JOIN LATERAL
                REGEXP_MATCHES(prompt, '''s|''t|''re|''ve|''m|''ll|''d| ?\w+| ?\d+| ?[^\s\w\d]+|\s+(?!\S)|\s+', 'g') WITH ORDINALITY AS rm (part, part_position)
        CROSS JOIN LATERAL
                (
                WITH    RECURSIVE
                        bpe AS
                        (
                        SELECT  (n + 1)::BIGINT AS position, character, TRUE AS continue
                        FROM    CONVERT_TO(part[1], 'UTF-8') AS bytes
                        CROSS JOIN LATERAL
                                GENERATE_SERIES(0, LENGTH(bytes) - 1) AS n
                        JOIN    encoder
                        ON      byte = GET_BYTE(bytes, n)
                        UNION ALL
                        (
                        WITH    RECURSIVE
                                base AS
                                (
                                SELECT  *
                                FROM    bpe
                                WHERE   continue
                                ),
                                bn AS
                                (
                                SELECT  ROW_NUMBER() OVER (ORDER BY position) AS position,
                                        continue,
                                        character,
                                        character || LEAD(character) OVER (ORDER BY position) AS cluster
                                FROM    base
                                ),
                                top_rank AS
                                (
                                SELECT  tokenizer.*
                                FROM    bn
                                CROSS JOIN LATERAL
                                        (
                                        SELECT  *
                                        FROM    tokenizer
                                        WHERE   tokenizer.cluster = bn.cluster
                                        LIMIT   1
                                        ) tokenizer
                                ORDER BY
                                        token
                                LIMIT   1
                                ),
                                breaks AS
                                (
                                SELECT  0::BIGINT AS position, 1 AS length
                                UNION ALL
                                SELECT  bn.position,
                                        CASE WHEN token IS NULL THEN 1 ELSE 2 END
                                FROM    breaks
                                JOIN    bn
                                ON      bn.position = breaks.position + length
                                LEFT JOIN
                                        top_rank
                                USING   (cluster)
                                )
                        SELECT  position, character, token IS NOT NULL
                        FROM    breaks
                        LEFT JOIN
                                top_rank
                        ON      1 = 1
                        CROSS JOIN LATERAL
                                (
                                SELECT  STRING_AGG(character, '' ORDER BY position) AS character
                                FROM    bn
                                WHERE   bn.position &gt;= breaks.position
                                        AND bn.position &lt; breaks.position + length
                                ) bn
                        WHERE   position &gt; 0
                        )
                        )
                SELECT  position, character AS cluster
                FROM    bpe
                WHERE   NOT continue
                ) bpe
        ),
        tokens AS
        (
        SELECT  ARRAY_AGG(token ORDER BY part_position, position) AS input
        FROM    clusters
        JOIN    tokenizer
        USING   (cluster)
        ),
        gpt AS
        (
        SELECT  input, ARRAY_LENGTH(input, 1) AS original_length
        FROM    tokens
        UNION ALL
        SELECT  input || next_token.token, original_length
        FROM    gpt
        CROSS JOIN
                input
        CROSS JOIN LATERAL
                (
                WITH    RECURSIVE
                        hparams AS
                        (
                        SELECT  ARRAY_LENGTH(input, 1) AS n_seq,
                                12 AS n_block
                        ),
                        embeddings AS
                        (
                        SELECT  place, values
                        FROM    hparams
                        CROSS JOIN LATERAL
                                UNNEST(input) WITH ORDINALITY AS tokens (token, ordinality)
                        CROSS JOIN LATERAL
                                (
                                SELECT  ordinality - 1 AS place
                                ) o
                        CROSS JOIN LATERAL
                                (
                                SELECT  wte.values + wpe.values AS values
                                FROM    wte
                                CROSS JOIN
                                        wpe
                                WHERE   wte.token = tokens.token
                                        AND wpe.place = o.place
                                ) embedding
                        ),
                        transform AS
                        (
                        SELECT  0 AS block, place, values
                        FROM    embeddings
                        UNION ALL
                        (
                        WITH    previous AS
                                (
                                SELECT  *
                                FROM    transform
                                )
                        SELECT  block + 1 AS block, transformed_layer.*
                        FROM    hparams
                        CROSS JOIN LATERAL
                                (
                                SELECT  block
                                FROM    previous
                                WHERE   block &lt; 12
                                LIMIT   1
                                ) q
                        CROSS JOIN LATERAL
                                (
                                WITH    ln_2_b AS
                                        (
                                        SELECT  *
                                        FROM    ln_2_b
                                        WHERE   block = q.block
                                        ),
                                        ln_2_g AS
                                        (
                                        SELECT  *
                                        FROM    ln_2_g
                                        WHERE   block = q.block
                                        ),
                                        c_proj_w AS
                                        (
                                        SELECT  *
                                        FROM    c_proj_w
                                        WHERE   block = q.block
                                        ),
                                        c_proj_b AS
                                        (
                                        SELECT  *
                                        FROM    c_proj_b
                                        WHERE   block = q.block
                                        ),
                                        mlp_c_fc_w AS
                                        (
                                        SELECT  *
                                        FROM    mlp_c_fc_w
                                        WHERE   block = q.block
                                        ),
                                        mlp_c_fc_b AS
                                        (
                                        SELECT  *
                                        FROM    mlp_c_fc_b
                                        WHERE   block = q.block
                                        ),
                                        mlp_c_proj_w AS
                                        (
                                        SELECT  *
                                        FROM    mlp_c_proj_w
                                        WHERE   block = q.block
                                        ),
                                        mlp_c_proj_b AS
                                        (
                                        SELECT  *
                                        FROM    mlp_c_proj_b
                                        WHERE   block = q.block
                                        ),
                                        c_attn_w AS
                                        (
                                        SELECT  *
                                        FROM    c_attn_w
                                        WHERE   block = q.block
                                        ),
                                        c_attn_b AS
                                        (
                                        SELECT  *
                                        FROM    c_attn_b
                                        WHERE   block = q.block
                                        ),
                                        ln_1_g AS
                                        (
                                        SELECT  *
                                        FROM    ln_1_g
                                        WHERE   block = q.block
                                        ),
                                        ln_1_b AS
                                        (
                                        SELECT  *
                                        FROM    ln_1_b
                                        WHERE   block = q.block
                                        ),
                                        mha_norm AS
                                        (
                                        SELECT  place, mm.values + c_attn_b.values AS values
                                        FROM    (
                                                SELECT  place, ARRAY_AGG(INNER_PRODUCT(c_attn_w.values, layer_norm.values) ORDER BY y)::VECTOR(2304) AS values
                                                FROM    (
                                                        SELECT  place, agg.values * ln_1_g.values + ln_1_b.values AS values
                                                        FROM    (
                                                                SELECT  place, norm.values
                                                                FROM    previous
                                                                CROSS JOIN LATERAL
                                                                        (
                                                                        SELECT  AVG(value) AS mean,
                                                                                VAR_POP(value) AS variance
                                                                        FROM    UNNEST(values::REAL[]) value
                                                                        ) agg
                                                                CROSS JOIN LATERAL
                                                                        (
                                                                        SELECT  ARRAY_AGG((value - mean) / SQRT(variance + 1E-5) ORDER BY ordinality)::VECTOR(768) AS values
                                                                        FROM    UNNEST(values::REAL[]) WITH ORDINALITY AS n(value, ordinality)
                                                                        ) norm
                                                                ) agg
                                                        CROSS JOIN
                                                                ln_1_b
                                                        CROSS JOIN
                                                                ln_1_g
                                                        ) layer_norm
                                                CROSS JOIN
                                                        c_attn_w
                                                GROUP BY
                                                        place
                                                ) mm
                                        CROSS JOIN
                                                c_attn_b
                                        ),
                                        heads AS
                                        (
                                        SELECT  place, head,
                                                (values::REAL[])[(head * 64 + 1):(head * 64 + 64)]::VECTOR(64) AS q,
                                                (values::REAL[])[(head * 64 + 1 + 768):(head * 64 + 64 + 768)]::VECTOR(64) AS k,
                                                (values::REAL[])[(head * 64 + 1 + 1536):(head * 64 + 64 + 1536)]::VECTOR(64) AS v
                                        FROM    mha_norm
                                        CROSS JOIN
                                                GENERATE_SERIES(0, 11) head
                                        ),
                                        sm_input AS
                                        (
                                        SELECT  head, h1.place AS x, h2.place AS y, INNER_PRODUCT(h1.q, h2.k) / 8 + CASE WHEN h2.place &gt; h1.place THEN -1E10 ELSE 0 END AS value
                                        FROM    heads h1
                                        JOIN    heads h2
                                        USING   (head)
                                        ),
                                        sm_diff AS
                                        (
                                        SELECT  head, x, y, value - MAX(value) OVER (PARTITION BY head, x) AS diff
                                        FROM    sm_input
                                        ),
                                        sm_exp AS
                                        (
                                        SELECT  head, x, y, CASE WHEN diff &lt; -745.13 THEN 0 ELSE EXP(diff) END AS e
                                        FROM    sm_diff
                                        ),
                                        softmax AS
                                        (
                                        SELECT  head, x, y AS place, e / SUM(e) OVER (PARTITION BY head, x) AS value
                                        FROM    sm_exp
                                        ),
                                        attention AS
                                        (
                                        SELECT  place, ARRAY_AGG(value ORDER BY head * 64 + ordinality)::VECTOR(768) AS values
                                        FROM    (
                                                SELECT  head, x AS place, SUM(ARRAY_FILL(softmax.value, ARRAY[64])::VECTOR(64) * heads.v) AS values
                                                FROM    softmax
                                                JOIN    heads
                                                USING   (head, place)
                                                GROUP BY
                                                        head, x
                                                ) q
                                        CROSS JOIN LATERAL
                                                UNNEST(values::REAL[]) WITH ORDINALITY v (value, ordinality)
                                        GROUP BY
                                                place
                                        ),
                                        mha AS
                                        (
                                        SELECT  place, w.values + c_proj_b.values + previous.values AS values
                                        FROM    (
                                                SELECT  attention.place, ARRAY_AGG(INNER_PRODUCT(attention.values, c_proj_w.values) ORDER BY c_proj_w.place)::VECTOR(768) AS values
                                                FROM    attention
                                                CROSS JOIN
                                                        c_proj_w
                                                GROUP BY
                                                        attention.place
                                                ) w
                                        CROSS JOIN
                                                c_proj_b
                                        JOIN    previous
                                        USING   (place)
                                        ),
                                        ffn_norm AS
                                        (
                                        SELECT  place, agg.values * ln_2_g.values + ln_2_b.values AS values
                                        FROM    (
                                                SELECT  place, norm.values
                                                FROM    mha
                                                CROSS JOIN LATERAL
                                                        (
                                                        SELECT  AVG(value) AS mean,
                                                                VAR_POP(value) AS variance
                                                        FROM    UNNEST(values::REAL[]) value
                                                        ) agg
                                                CROSS JOIN LATERAL
                                                        (
                                                        SELECT  ARRAY_AGG((value - mean) / SQRT(variance + 1E-5) ORDER BY ordinality)::VECTOR(768) AS values
                                                        FROM    UNNEST(values::REAL[]) WITH ORDINALITY AS n(value, ordinality)
                                                        ) norm
                                                ) agg
                                        CROSS JOIN
                                                ln_2_b
                                        CROSS JOIN
                                                ln_2_g
                                        ),
                                        ffn_a AS
                                        (
                                        SELECT  gelu.place, gelu.values
                                        FROM    (
                                                SELECT  place, w.values + mlp_c_fc_b.values AS values
                                                FROM    (
                                                        SELECT  ffn_norm.place, ARRAY_AGG(INNER_PRODUCT(ffn_norm.values, mlp_c_fc_w.values) ORDER BY mlp_c_fc_w.place)::VECTOR(3072) AS values
                                                        FROM    ffn_norm
                                                        CROSS JOIN
                                                                mlp_c_fc_w
                                                        GROUP BY
                                                                ffn_norm.place
                                                        ) w
                                                CROSS JOIN
                                                        mlp_c_fc_b
                                                ) v
                                        CROSS JOIN LATERAL
                                                (
                                                SELECT  place, ARRAY_AGG(0.5 * value * (1 + TANH(0.797884560802 * (value + 0.044715 * value*value*value))) ORDER BY ordinality)::VECTOR(3072) AS values
                                                FROM    UNNEST(values::REAL[]) WITH ORDINALITY n (value, ordinality)
                                                GROUP BY
                                                        place
                                                ) gelu
                                        ),
                                        ffn AS
                                        (
                                        SELECT  place, w.values + mlp_c_proj_b.values + mha.values AS values
                                        FROM    (
                                                SELECT  ffn_a.place, ARRAY_AGG(INNER_PRODUCT(ffn_a.values, mlp_c_proj_w.values) ORDER BY mlp_c_proj_w.place)::VECTOR(768) AS values
                                                FROM    ffn_a
                                                CROSS JOIN
                                                        mlp_c_proj_w
                                                GROUP BY
                                                        ffn_a.place
                                                ) w
                                        CROSS JOIN
                                                mlp_c_proj_b
                                        JOIN    mha
                                        USING   (place)
                                        )
                                SELECT  *
                                FROM    ffn
                                ) transformed_layer
                        )
                        ),
                        block_output AS
                        (
                        SELECT  *
                        FROM    hparams
                        JOIN    transform
                        ON      transform.block = n_block
                        ),
                        ln_f AS
                        (
                        SELECT  place, norm.values * ln_f_g.values + ln_f_b.values AS values
                        FROM    block_output
                        CROSS JOIN LATERAL
                                (
                                SELECT  AVG(value) AS mean,
                                        VAR_POP(value) AS variance
                                FROM    UNNEST(values::REAL[]) AS n(value)
                                ) agg
                        CROSS JOIN LATERAL
                                (
                                SELECT  ARRAY_AGG((value - mean) / SQRT(variance + 1E-5) ORDER BY ordinality)::VECTOR(768) AS values
                                FROM    UNNEST(values::REAL[]) WITH ORDINALITY AS n (value, ordinality)
                                ) norm
                        CROSS JOIN
                                ln_f_b
                        CROSS JOIN
                                ln_f_g
                        ),
                        logits AS
                        (
                        SELECT  token, INNER_PRODUCT(ln_f.values, wte.values) AS value
                        FROM    hparams
                        JOIN    ln_f
                        ON      ln_f.place = n_seq - 1
                        CROSS JOIN
                                wte
                        ORDER BY
                                value DESC
                        LIMIT   (top_n)
                        ),
                        tokens AS
                        (
                        SELECT  token,
                                high - softmax AS low,
                                high
                        FROM    (
                                SELECT  *,
                                        SUM(softmax) OVER (ORDER BY softmax) AS high
                                FROM    (
                                        SELECT  *, (e / SUM(e) OVER ()) AS softmax
                                        FROM    (
                                                SELECT  *,
                                                        (value - MAX(value) OVER ()) / temperature AS diff
                                                FROM    logits
                                                ) exp_x
                                        CROSS JOIN LATERAL
                                                (
                                                SELECT  CASE WHEN diff &lt; -745.13 THEN 0 ELSE EXP(diff) END AS e
                                                ) exp
                                        ) q
                                ) q
                        ),
                        next_token AS
                        (
                        SELECT  *
                        FROM    (
                                SELECT  RANDOM() AS rnd
                                ) r
                        CROSS JOIN LATERAL
                                (
                                SELECT  *
                                FROM    tokens
                                WHERE   rnd &gt;= low
                                        AND rnd &lt; high
                                ) nt
                        )
                SELECT  *
                FROM    next_token
                ) next_token
        WHERE   ARRAY_LENGTH(input, 1) &lt; original_length + threshold
                AND next_token.token &lt;&gt; 50256
        ),
        output AS
        (
        SELECT  CONVERT_FROM(STRING_AGG(SET_BYTE('\x00', 0, byte), '' ORDER BY position), 'UTF8') AS response
        FROM    (
                SELECT  STRING_AGG(cluster, '' ORDER BY ordinality) AS response
                FROM    input
                JOIN    gpt
                ON      ARRAY_LENGTH(input, 1) = original_length + threshold
                CROSS JOIN LATERAL
                        UNNEST(input) WITH ORDINALITY n (token, ordinality)
                JOIN    tokenizer
                USING   (token)
                ) q
        CROSS JOIN LATERAL
                STRING_TO_TABLE(response, NULL) WITH ORDINALITY n (character, position)
        JOIN    encoder
        USING   (character)
        )
SELECT  *
FROM    output
</pre>
<div>
<table>
<tbody><tr>
<th>response</th>
</tr>
<tr>
<td>Happy New Year! I wish you all the best in your new year!
</td>
</tr>
</tbody></table>
</div>
<p>This part the AI got right. I do wish you all the best in your new year!</p>
<p>You can find the queries and the installation code in the GitHub repository: <a href="https://github.com/quassnoi/explain-extended-2024" rel="noopener" target="_blank">quassnoi/explain-extended-2024</a></p>
<p>
<big><strong>Happy New Year!</strong></big>
</p>
<p>Previous New Year posts:</p>
<ul>
<li><a href="https://explainextended.com/2009/12/31/happy-new-year/">2010: SQL graphics in Oracle, MySQL, SQL Server and PostgreSQL</a></li>
<li><a href="https://explainextended.com/2010/12/31/happy-new-year-2/">2011: Drawing a clock in SQL</a></li>
<li><a href="https://explainextended.com/2011/12/31/happy-new-year-3/">2012: Drawing snowflakes in SQL</a></li>
<li><a href="https://explainextended.com/2012/12/31/happy-new-year-4/">2013: View of Earth from space in SQL</a></li>
<li><a href="https://explainextended.com/2013/12/31/happy-new-year-5/">2014: Drawing fractals in SQL</a></li>
<li><a href="https://explainextended.com/2014/12/31/happy-new-year-6/">2015: Composing music in SQL</a></li>
<li><a href="https://explainextended.com/2015/12/31/happy-new-year-7/">2016: Conway’s Game of Life in SQL</a></li>
<li><a href="https://explainextended.com/2016/12/31/happy-new-year-8/">2017: The Sultan’s Riddle in SQL</a></li>
<li><a href="https://explainextended.com/2017/12/31/happy-new-year-9/">2018: Settlers of Catan in SQL</a></li>
<li><a href="https://explainextended.com/2018/12/31/happy-new-year-10/">2019: GIF decoder in SQL</a></li>
<li><a href="https://explainextended.com/2019/12/31/happy-new-year-11/">2020: A stereogram in SQL</a></li>
<li><a href="https://explainextended.com/2020/12/31/happy-new-year-12/">2021: 3D picture of the coronavirus in SQL</a></li>
<li><a href="https://explainextended.com/2021/12/31/happy-new-year-13/">2022: Quantum computer emulator in SQL</a></li>
<li><a href="https://explainextended.com/2022/12/31/happy-new-year-14/">2023: Solving the Rubik’s Cube in SQL</a></li>
</ul>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Chronic stress spreads cancer (139 pts)]]></title>
            <link>https://www.cshl.edu/chronic-stress-spreads-cancer-heres-how/</link>
            <guid>39488653</guid>
            <pubDate>Sat, 24 Feb 2024 02:43:45 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.cshl.edu/chronic-stress-spreads-cancer-heres-how/">https://www.cshl.edu/chronic-stress-spreads-cancer-heres-how/</a>, See on <a href="https://news.ycombinator.com/item?id=39488653">Hacker News</a></p>
<div id="readability-page-1" class="page"><article id="post-68688">
    <!-- HEADLINE -->
    <header>
		
    </header><!-- .entry-header -->

    <!-- // FEATURE IMAGE -->
<div>
                 <p><img src="https://www.cshl.edu/wp-content/uploads/2024/02/Lung_Cancer_Metastasis.jpg" alt="Image of lung cancer metastasis in a mouse"></p><figcaption>For a recent CSHL Cancer Center study, Adjunct Professor Mikala Egeblad (now a Bloomberg Distinguished Professor with Johns Hopkins University) and postdoc Xue-Yan He (now Assistant Professor of Cell Biology &amp; Physiology at Washington University School of Medicine in St. Louis) teamed with CSHL Professor Linda Van Aelst. Above: lung cancer metastasis in a mouse that underwent experiments designed to simulate the stress that cancer patients experience.</figcaption></div>

	<div>
		<div><p><em>
Read time 3 minutes | <span><time datetime="Thursday, 22 February  2024">Thursday, 22 February 2024</time></span>				</em></p>           
           
				</div><!--/.row-->	

<!-- MAIN POST CONTENT AREA -->
		<div>
			<div>


				

<p><a href="#" rel="nofollow" onclick="window.print(); return false;" title="Printer Friendly, PDF &amp; Email"><img decoding="async" src="https://www.cshl.edu/wp-content/uploads/2023/03/print_pdf_icon.png" alt="Print Friendly, PDF &amp; Email"></a></p><p>Stress is inevitable. But too much of it can be terrible for our health. Chronic stress can increase our risk for heart disease and strokes. It may also help cancer spread. How this works has remained a mystery—a challenge for cancer care. Xue-Yan He, a former postdoc in Cold Spring Harbor Laboratory (CSHL) Adjunct Professor <a href="https://www.cshl.edu/research/faculty-staff/mikala-egeblad/">Mikala Egeblad</a>’s lab, explains:</p>
<p><span data-url="https://www.cshl.edu/wp-content/uploads/2024/02/He_Stress_Cancer_Patients.mp3" data-plays="1">“Stress is something we cannot really avoid in cancer patients. You can imagine if you are diagnosed, you cannot stop thinking about the disease or insurance or family. So it is very important to understand how stress works on us.”</span></p>
<p>Now, He and Egeblad may have reached a breakthrough in understanding exactly that. Working with CSHL Professor <a href="https://www.cshl.edu/research/faculty-staff/linda-van-aelst/">Linda Van Aelst</a>, they discovered that stress causes certain white blood cells called neutrophils to form sticky web-like structures that make body tissues more susceptible to metastasis. The finding could point to new treatment strategies that stop cancer’s spread before it starts.</p>
<p>The team arrived at their discovery by mimicking chronic stress in mice with cancer. They first removed tumors that had been growing in mice’s breasts and spreading cancer cells to their lungs. Next, they exposed the mice to stress. What He observed was shocking. Egeblad recalls:</p>
<p><span data-url="https://www.cshl.edu/wp-content/uploads/2024/02/Egeblad_Increase_Metastatic_Lesions.mp3" data-plays="1">“She saw this scary increase in metastatic lesions in these animals. It was up to a fourfold increase in metastasis.”</span></p>
<p>The team found that stress hormones called glucocorticoids acted on the neutrophils. These “stressed” neutrophils formed spider-web-like structures called <a href="https://www.cshl.edu/how-an-antiviral-immune-reaction-can-go-too-far/">NETs (neutrophil extracellular traps)</a>. NETs form when neutrophils expel DNA. Normally, they can defend us against invading microorganisms. However, in cancer, NETs create a metastasis-friendly environment. </p>
<figure id="attachment_68683"><a href="https://www.cshl.edu/wp-content/uploads/2024/02/Cancer_stress_Dnase_I_Comparison.jpg"><img decoding="async" fetchpriority="high" data-headline="Cancer spread faster and more furiously in stressed mice (middle column) than in a control group (left column). By comparison, cancer cells in stressed mice treated with an enzyme called DNase I (right column) were largely non-proliferating, and the treatment caused a significant reduction in stress-induced metastasis." src="https://www.cshl.edu/wp-content/uploads/2024/02/Cancer_stress_Dnase_I_Comparison.jpg" alt="Image of comparisons in cancer growth with low stress, high stress, and DNase I" width="1920" height="1080" srcset="https://www.cshl.edu/wp-content/uploads/2024/02/Cancer_stress_Dnase_I_Comparison.jpg 1920w, https://www.cshl.edu/wp-content/uploads/2024/02/Cancer_stress_Dnase_I_Comparison-814x458.jpg 814w, https://www.cshl.edu/wp-content/uploads/2024/02/Cancer_stress_Dnase_I_Comparison-1249x703.jpg 1249w, https://www.cshl.edu/wp-content/uploads/2024/02/Cancer_stress_Dnase_I_Comparison-768x432.jpg 768w, https://www.cshl.edu/wp-content/uploads/2024/02/Cancer_stress_Dnase_I_Comparison-1536x864.jpg 1536w" sizes="(max-width: 1920px) 100vw, 1920px"></a><figcaption>Cancer spread faster and more furiously in stressed mice (middle column) than in a control group (left column). By comparison, cancer cells in stressed mice treated with an enzyme called DNase I (right column) were largely non-proliferating, and the treatment caused a significant reduction in stress-induced metastasis.</figcaption></figure>
<p>To confirm that stress triggers NET formation, leading to increased metastasis, He performed three tests. First, she removed neutrophils from the mice using antibodies. Next, she injected a NET-destroying drug into the animals. Lastly, she used mice whose neutrophils couldn’t respond to glucocorticoids. Each test achieved similar results. “The stressed mice no longer developed more metastasis,” He says.</p>
<p>Notably, the team found that chronic stress caused <a href="https://www.cshl.edu/drug-halts-immune-reactions-to-save-damaged-lungs/">NET formation</a> to modify lung tissue even in mice without cancer. “It’s almost preparing your tissue for getting cancer,” Egeblad explains. </p>
<p>To Van Aelst, the implication, though startling, is clear. “Reducing stress should be a component of cancer treatment <em>and</em> prevention,” she says.</p>
<p>The team also speculates that future drugs preventing NET formation could benefit patients whose cancer hasn’t yet metastasized. Such new treatments could slow or stop cancer’s spread, offering much-needed relief.</p>
<p><strong>Written by</strong>: <a href="https://www.cshl.edu/author/osborne/">Margaret Osborne</a>, <em>Science Writer</em> | <a href="mailto:publicaffairs@cshl.edu">publicaffairs@cshl.edu</a> | 516-367-8455</p><hr><section id="funding"><p><strong>Funding</strong></p><p><a href="#" rel="nofollow" onclick="window.print(); return false;" title="Printer Friendly, PDF &amp; Email"><img decoding="async" src="https://www.cshl.edu/wp-content/uploads/2023/03/print_pdf_icon.png" alt="Print Friendly, PDF &amp; Email"></a></p>
<p>National Institutes of Health, Department of Defense Breast Cancer Research Program, American Association for Cancer Research, Cancer Research Institute, German Research Foundation</p>
</section><section id="citation"><p><strong>Citation</strong></p><p><a href="#" rel="nofollow" onclick="window.print(); return false;" title="Printer Friendly, PDF &amp; Email"><img decoding="async" src="https://www.cshl.edu/wp-content/uploads/2023/03/print_pdf_icon.png" alt="Print Friendly, PDF &amp; Email"></a></p>
<p>He, X. Y., <em>et al</em>., “Chronic stress increases metastasis via neutrophil-mediated changes to the microenvironment”, <em>Cancer Cell</em>, February 22, 2024. DOI: <a href="https://doi.org/10.1016/j.ccell.2024.01.013" rel="noopener" target="_blank">10.1016/j.ccell.2024.01.013</a></p>
</section><!-- Core facilities-->
<div><h4>Core Facilites</h4></div>
<div id="newsletter">
	<p>
		<h4>
			Stay informed
		</h4>
	</p>

<div>
<p>Sign up for our newsletter to get the latest discoveries, upcoming events, videos, podcasts, and a news roundup delivered straight to your inbox every month.
</p>
<p><a href="https://www.cshl.edu/news-stand/newsletter/"><span></span> &nbsp; Newsletter Signup</a></p>
</div></div>
			</div><!-- /.col-->
			<!-- SIDEBAR SECTION -->
			<div>


				

<h3>Tags</h3>			 <!-- /.row -->     
	</div><!-- /.entry-content -->
</div></div></article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Show HN: Consol3 – A 3D engine for the terminal that executes on the CPU (154 pts)]]></title>
            <link>https://github.com/Victormeriqui/Consol3</link>
            <guid>39488529</guid>
            <pubDate>Sat, 24 Feb 2024 02:17:42 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/Victormeriqui/Consol3">https://github.com/Victormeriqui/Consol3</a>, See on <a href="https://news.ycombinator.com/item?id=39488529">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><h2 tabindex="-1" dir="auto">Consol3</h2>
<p dir="auto">A graphics engine that executes entirely on the CPU and uses the console as the display</p>
<h2 tabindex="-1" dir="auto">Intro</h2>
<h2 tabindex="-1" dir="auto">Videos</h2>
<p dir="auto"><a href="https://www.youtube.com/watch?v=khu1oPdL6ww" rel="nofollow"><img src="https://camo.githubusercontent.com/582aa9b313454e4313bd5ce2433523fd57976e3a6d7b219636b691716e2019cd/68747470733a2f2f696d672e796f75747562652e636f6d2f76692f6b6875316f50644c3677772f302e6a7067" alt="Rasterization" data-canonical-src="https://img.youtube.com/vi/khu1oPdL6ww/0.jpg"></a><br>
<a href="https://www.youtube.com/watch?v=IXVWcb05Z5U" rel="nofollow"><img src="https://camo.githubusercontent.com/09fa87b569c5f967553f5d88c5457a0d968aa60b8975d98ece1b2cfd5be0a5dd/68747470733a2f2f696d672e796f75747562652e636f6d2f76692f49585657636230355a35552f302e6a7067" alt="Ray Marching" data-canonical-src="https://img.youtube.com/vi/IXVWcb05Z5U/0.jpg"></a></p>
<h3 tabindex="-1" dir="auto">Software Rendering</h3>
<p dir="auto">Consol3 is a 3D graphics engine that doesn't use the graphics card to render any frame, instead the same calculations that would typically be made by the GPU hardware is done in software, every single vertex transformation, matrix calculation, etc is calculated on the CPU</p>
<p dir="auto">To make the engine more flexible, some concepts typically used for programming GPUs are implemented, for example Shaders - However these are still entirely handled on the CPU</p>
<h3 tabindex="-1" dir="auto">Dependencies</h3>
<p dir="auto">No external dependencies will ever be used in this engine, the goal is to do everything using only what the OS already provides, that means no external math libraries, window managers, resource loaders, etc</p>
<h4 tabindex="-1" dir="auto">Older Versions</h4>
<p dir="auto">Building this engine is a hobby of mine, and I've been working on it infrequently for some years now, as such it has gone through many refactors, partial and complete rewrites, this is the latest version of the engine</p>
<h2 tabindex="-1" dir="auto">Building</h2>
<p dir="auto">This project uses CMake, to build it simply create a build folder on the cloned repository, cd into it and run <code>cmake ..</code>, followed by <code>make</code> - after compiling 2
executables should be generated inside the build folder:<br>
- Consol3_raster<br>
- Consol3_voxel</p>
<p dir="auto">Consol3_raster will have a scene with only rasterized meshes, lights &amp; other experiments<br>
Consol3_voxel will have a scene with ray marched voxels, along with a particle-like simulation for sand, water, lava, steam and ice using the voxels</p>
<p dir="auto">The project can be built for either Windows or Linux, on Linux no mouse input is supported yet (use the arrow keys to control the look direction), and only a few frame drawers are supported</p>
<h2 tabindex="-1" dir="auto">Controls</h2>
<h3 tabindex="-1" dir="auto">Raster</h3>
<p dir="auto">Mouse 2,3,4,5 - Control lights<br>
Numbers 1, 2, 3, 4, 5, 6, 7, 8, 9, 0 - Enable different floor showcases<br>
P - play animations</p>
<h3 tabindex="-1" dir="auto">Voxel</h3>
<p dir="auto">Mouse2 - Spawn currently selected element<br>
Numbers 1, 2, 3, 4, 5, 6, 7, 8, 9, 0 - Select different elements<br>
Q and E - Control cursor distance<br>
R and T - Control cursor size</p>
<h3 tabindex="-1" dir="auto">Common for both</h3>
<p dir="auto">WASD - Move<br>
Capslock - Toggle mouse camera<br>
Page Up and Down - Change frame drawer<br>
Arrow keys - Change camera direction<br>
Shift - Slow down movement</p>
<h2 tabindex="-1" dir="auto">Rendering</h2>
<h3 tabindex="-1" dir="auto">Rasterization</h3>
<p dir="auto">The engine has a flexible rasterization pipeline that can be controlled by using different "Shaders", these shaders are similar in concept to GPU shaders, in the sense that they can modify the data that is passed on to the next stage</p>
<p dir="auto">The pipeline to render a mesh is as follows:<br>
1. The first step of the pipeline is calling the vertex shader for each triangle, giving it the triangle vertices and the mesh transformations, the shader then applies the transformations and projection, and decides whether the triangle should be culled (backface culling)<br>
2. Then the resulting triangle is clipped of any offscreen vertices, for this the triangles are clipped against different planes, 2 per each axis<br>
3. The resulting vertices from clipping are transformed to screen space and then sent to the rasterizer<br>
4. The rasterizer then calculates the coordinates that are inside the triangle and calls the fragment shader for each coordinate<br>
5. The fragment shader then decides which color to output on each coordinate, using solid colors, textures, and shading techniques</p>
<p dir="auto">The engine uses barycentric rasterization to determine which pixels are inside a triangle</p>
<h4 tabindex="-1" dir="auto">Rasterization Features</h4>
<h6 tabindex="-1" dir="auto">OBJ file loading</h6>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/Victormeriqui/Consol3/blob/master/images/obj.png"><img src="https://github.com/Victormeriqui/Consol3/raw/master/images/obj.png" width="200" height="200"></a></p>
<h6 tabindex="-1" dir="auto">MD2 file loading</h6>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/Victormeriqui/Consol3/blob/master/images/md2.png"><img src="https://github.com/Victormeriqui/Consol3/raw/master/images/md2.png" width="200" height="200"></a></p>
<h6 tabindex="-1" dir="auto">Directional lights</h6>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/Victormeriqui/Consol3/blob/master/images/directionallight.png"><img src="https://github.com/Victormeriqui/Consol3/raw/master/images/directionallight.png" width="200" height="200"></a></p>
<h6 tabindex="-1" dir="auto">Point lights</h6>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/Victormeriqui/Consol3/blob/master/images/pointlight.gif"><img src="https://github.com/Victormeriqui/Consol3/raw/master/images/pointlight.gif" width="200" height="200" data-animated-image=""></a></p>
<h6 tabindex="-1" dir="auto">Spot lights</h6>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/Victormeriqui/Consol3/blob/master/images/spotlight.png"><img src="https://github.com/Victormeriqui/Consol3/raw/master/images/spotlight.png" width="200" height="200"></a></p>
<h6 tabindex="-1" dir="auto">Textures</h6>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/Victormeriqui/Consol3/blob/master/images/textures.gif"><img src="https://github.com/Victormeriqui/Consol3/raw/master/images/textures.gif" width="200" height="200" data-animated-image=""></a></p>
<h6 tabindex="-1" dir="auto">Shadow maps</h6>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/Victormeriqui/Consol3/blob/master/images/shadows.png"><img src="https://github.com/Victormeriqui/Consol3/raw/master/images/shadows.png" width="200" height="200"></a></p>
<h6 tabindex="-1" dir="auto">Shadows from multiple sources</h6>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/Victormeriqui/Consol3/blob/master/images/multishadows.png"><img src="https://github.com/Victormeriqui/Consol3/raw/master/images/multishadows.png" width="200" height="200"></a></p>
<h6 tabindex="-1" dir="auto">MD2 Animations</h6>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/Victormeriqui/Consol3/blob/master/images/animation1.gif"><img src="https://github.com/Victormeriqui/Consol3/raw/master/images/animation1.gif" width="200" height="200" data-animated-image=""></a>  <a target="_blank" rel="noopener noreferrer" href="https://github.com/Victormeriqui/Consol3/blob/master/images/animation2.gif"><img src="https://github.com/Victormeriqui/Consol3/raw/master/images/animation2.gif" width="200" height="200" data-animated-image=""></a></p>
<h6 tabindex="-1" dir="auto">Ico-sphere generation</h6>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/Victormeriqui/Consol3/blob/master/images/sphere.gif"><img src="https://github.com/Victormeriqui/Consol3/raw/master/images/sphere.gif" width="200" height="200" data-animated-image=""></a></p>
<h6 tabindex="-1" dir="auto">Normal maps</h6>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/Victormeriqui/Consol3/blob/master/images/normal_map.png"><img src="https://github.com/Victormeriqui/Consol3/raw/master/images/normal_map.png" width="200" height="200"></a>  <a target="_blank" rel="noopener noreferrer" href="https://github.com/Victormeriqui/Consol3/blob/master/images/normal_map.gif"><img src="https://github.com/Victormeriqui/Consol3/raw/master/images/normal_map.gif" width="200" height="200" data-animated-image=""></a></p>
<h6 tabindex="-1" dir="auto">Specular highlights</h6>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/Victormeriqui/Consol3/blob/master/images/specular.gif"><img src="https://github.com/Victormeriqui/Consol3/raw/master/images/specular.gif" width="200" height="200" data-animated-image=""></a></p>
<h6 tabindex="-1" dir="auto">Colored lighting</h6>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/Victormeriqui/Consol3/blob/master/images/colored_lights.png"><img src="https://github.com/Victormeriqui/Consol3/raw/master/images/colored_lights.png" width="200" height="200"></a></p>
<h3 tabindex="-1" dir="auto">Ray Marching</h3>
<p dir="auto">The engine also has a different rendering technique based on ray marching instead of rasterization. For this a regular 3D grid is defined, where each cell represents a voxel, and can either be filled with a specific particle type, or empty (Air)</p>
<p dir="auto">When rendering a frame, rays are marched from the camera origin towards the looking direction, and are stopped in case they hit one of the non-air voxels</p>
<p dir="auto">For this ray marching experiment, a simple physics simulation was also implemented, where different elements can be spawned and played around with, the currently supported elements are:<br>
- Sand<br>
- Ice<br>
- Water<br>
- Steam<br>
- Stone<br>
- Lava<br>
- Steel</p>
<h4 tabindex="-1" dir="auto">Ray Marching Features</h4>
<h5 tabindex="-1" dir="auto">Voxel Shading</h5>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/Victormeriqui/Consol3/blob/master/images/voxel_shading.png"><img src="https://github.com/Victormeriqui/Consol3/raw/master/images/voxel_shading.png" width="200" height="200"></a></p>
<h5 tabindex="-1" dir="auto">Voxel Shadows</h5>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/Victormeriqui/Consol3/blob/master/images/voxel_shadows.png"><img src="https://github.com/Victormeriqui/Consol3/raw/master/images/voxel_shadows.png" width="200" height="200"></a></p>
<h5 tabindex="-1" dir="auto">Rasterization &amp; Ray Marching on the same scene</h5>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/Victormeriqui/Consol3/blob/master/images/voxel_raster.gif"><img src="https://github.com/Victormeriqui/Consol3/raw/master/images/voxel_raster.gif" width="200" height="200" data-animated-image=""></a></p>
<h5 tabindex="-1" dir="auto">Physics Simulation</h5>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/Victormeriqui/Consol3/blob/master/images/sand.gif"><img src="https://github.com/Victormeriqui/Consol3/raw/master/images/sand.gif" width="200" height="200" data-animated-image=""></a></p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/Victormeriqui/Consol3/blob/master/images/water.gif"><img src="https://github.com/Victormeriqui/Consol3/raw/master/images/water.gif" width="200" height="200" data-animated-image=""></a></p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/Victormeriqui/Consol3/blob/master/images/lava.gif"><img src="https://github.com/Victormeriqui/Consol3/raw/master/images/lava.gif" width="200" height="200" data-animated-image=""></a></p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/Victormeriqui/Consol3/blob/master/images/lava%20+%20water%20floor.gif"><img src="https://github.com/Victormeriqui/Consol3/raw/master/images/lava%20+%20water%20floor.gif" width="200" height="200" data-animated-image=""></a></p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/Victormeriqui/Consol3/blob/master/images/tankers%20draining.gif"><img src="https://github.com/Victormeriqui/Consol3/raw/master/images/tankers%20draining.gif" width="200" height="200" data-animated-image=""></a></p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/Victormeriqui/Consol3/blob/master/images/frozen%20tanker.gif"><img src="https://github.com/Victormeriqui/Consol3/raw/master/images/frozen%20tanker.gif" width="200" height="200" data-animated-image=""></a></p>
<h2 tabindex="-1" dir="auto">Frame Drawers</h2>
<p dir="auto">After rendering a full frame, in order to actually draw to the console output the engine has a flexible system where different "frame drawers" can be used<br>
These are the components in charge of making a specific RGB color pixel be represented in the console</p>
<p dir="auto">For this, different techniques are employed for different effects/quality, some techniques allow more colors, some allow more performance</p>
<h5 tabindex="-1" dir="auto">Greyscale Frame Drawer</h5>
<p dir="auto">Overrides the palette with 16 shades from black to white<br>
<a target="_blank" rel="noopener noreferrer" href="https://github.com/Victormeriqui/Consol3/blob/master/images/greyscale.png"><img src="https://github.com/Victormeriqui/Consol3/raw/master/images/greyscale.png" width="200" height="200"></a></p>
<h6 tabindex="-1" dir="auto">Dithered Greyscale Frame Drawer</h6>
<p dir="auto">Similar to the previous one, but also takes advantage of the dithering block characters (░▒▓) to dither different combinations of the 16 shades, expands the original 16 to 80 shades<br>
<a target="_blank" rel="noopener noreferrer" href="https://github.com/Victormeriqui/Consol3/blob/master/images/ditheredgreyscale.png"><img src="https://github.com/Victormeriqui/Consol3/raw/master/images/ditheredgreyscale.png" width="200" height="200"></a></p>
<h6 tabindex="-1" dir="auto">Dithered Frame Drawer</h6>
<p dir="auto">Uses the same mechanism from the previous Frame Drawer but with the default palette, giving more depth to the default colors (10 shades per color)<br>
<a target="_blank" rel="noopener noreferrer" href="https://github.com/Victormeriqui/Consol3/blob/master/images/dithered.png"><img src="https://github.com/Victormeriqui/Consol3/raw/master/images/dithered.png" width="200" height="200"></a></p>
<h6 tabindex="-1" dir="auto">VT Escape Sequence Frame Drawer</h6>
<p dir="auto">Uses escape sequences to set the colors of each pixel, allowing for full 32 bit real RGB colors, or indexed colors (256 color palette)<br>
<a target="_blank" rel="noopener noreferrer" href="https://github.com/Victormeriqui/Consol3/blob/master/images/vtescapesequence.png"><img src="https://github.com/Victormeriqui/Consol3/raw/master/images/vtescapesequence.png" width="200" height="200"></a></p>
<h6 tabindex="-1" dir="auto">Text Only Frame Drawer</h6>
<p dir="auto">Does not use any attribute change, thus the only color is white, the lightness of each pixel is controlled through the character in the cell<br>
The current characters used are: " ·;%░≡¥▒▓█"<br>
<a target="_blank" rel="noopener noreferrer" href="https://github.com/Victormeriqui/Consol3/blob/master/images/ascii.png"><img src="https://github.com/Victormeriqui/Consol3/raw/master/images/ascii.png" width="200" height="200"></a></p>
<h3 tabindex="-1" dir="auto">Shaders</h3>
<p dir="auto">Vertex and Fragment shaders can be created, they are basically classes that implement a vertex and fragment stage in the rasterization pipeline, and can pass data around via the class members</p>
<p dir="auto">A simple shader:</p>
<div dir="auto" data-snippet-clipboard-copy-content="bool PlainShader::VertexShader(Vertex&amp; v0, Vertex&amp; v1, Vertex&amp; v2, const MVPTransform&amp; mvp_mats)
{
	TransformVertexMVP(v0, mvp_mats);
	TransformVertexMVP(v1, mvp_mats);
	TransformVertexMVP(v2, mvp_mats);

	vert_v0_texture_coord = v0.GetTextureCoords();
	vert_v1_texture_coord = v1.GetTextureCoords();
	vert_v2_texture_coord = v2.GetTextureCoords();

	return !IsBackface(v0.GetPosition(), v1.GetPosition(), v2.GetPosition());
}

RGBColor PlainShader::FragmentShader(RGBColor color, const Triangle&amp; triangle, float barcoord0, float barcoord1, float barcoord2)
{
	Vector2 frag_texture_coord = PerspectiveCorrectInterpolate<Vector2>(vert_v0_texture_coord,
									    vert_v1_texture_coord,
									    vert_v2_texture_coord,
									    triangle,
									    barcoord0,
									    barcoord1,
									    barcoord2);

	RGBColor final_color = texture->GetColorFromTextureCoords(frag_texture_coord.x, frag_texture_coord.y);
	final_color.BlendMultiply(color);

	return final_color;
}"><pre><span>bool</span> <span>PlainShader::VertexShader</span>(Vertex&amp; v0, Vertex&amp; v1, Vertex&amp; v2, <span>const</span> MVPTransform&amp; mvp_mats)
{
	<span>TransformVertexMVP</span>(v0, mvp_mats);
	<span>TransformVertexMVP</span>(v1, mvp_mats);
	<span>TransformVertexMVP</span>(v2, mvp_mats);

	vert_v0_texture_coord = v0.<span>GetTextureCoords</span>();
	vert_v1_texture_coord = v1.<span>GetTextureCoords</span>();
	vert_v2_texture_coord = v2.<span>GetTextureCoords</span>();

	<span>return</span> !<span>IsBackface</span>(v0.<span>GetPosition</span>(), v1.<span>GetPosition</span>(), v2.<span>GetPosition</span>());
}

RGBColor <span>PlainShader::FragmentShader</span>(RGBColor color, <span>const</span> Triangle&amp; triangle, <span>float</span> barcoord0, <span>float</span> barcoord1, <span>float</span> barcoord2)
{
	Vector2 frag_texture_coord = PerspectiveCorrectInterpolate&lt;Vector2&gt;(vert_v0_texture_coord,
									    vert_v1_texture_coord,
									    vert_v2_texture_coord,
									    triangle,
									    barcoord0,
									    barcoord1,
									    barcoord2);

	RGBColor final_color = texture-&gt;<span>GetColorFromTextureCoords</span>(frag_texture_coord.<span>x</span>, frag_texture_coord.<span>y</span>);
	final_color.<span>BlendMultiply</span>(color);

	<span>return</span> final_color;
}</pre></div>
<h3 tabindex="-1" dir="auto">Planned Features</h3>
<ul dir="auto">
<li>Faster vertex transformations with SIMD</li>
<li>Faster rasterizer with multipixel filling</li>
<li>Faster rasterizer with binning</li>
</ul>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[DVD's New Cousin Can Store More Than a Petabit (131 pts)]]></title>
            <link>https://spectrum.ieee.org/data-storage-petabit-optical-disc</link>
            <guid>39488375</guid>
            <pubDate>Sat, 24 Feb 2024 01:44:00 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://spectrum.ieee.org/data-storage-petabit-optical-disc">https://spectrum.ieee.org/data-storage-petabit-optical-disc</a>, See on <a href="https://news.ycombinator.com/item?id=39488375">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-headline="DVD’s New Cousin Can Store More Than a Petabit" data-elid="2667335462" data-post-url="https://spectrum.ieee.org/data-storage-petabit-optical-disc" data-authors="Charles Q. Choi" data-page-title="DVD’s New Cousin Can Store More Than a Petabit - IEEE Spectrum"><p>A novel disc the size of a DVD can hold more than 1 million gigabits—roughly as much as is <a href="https://www.washingtonpost.com/technology/2022/10/27/laser-powered-chip-internet-data-transfer/" target="_blank">transmitted per second over the entire world’s Internet</a>—by storing data in three dimensions as opposed to two, a new study finds.<br></p><p>Optical discs such as CDs and <a href="https://spectrum.ieee.org/fivedimensional-dvd-could-store-16-terabytes" target="_self">DVDs</a> encode data using a series of microscopic pits. These pits, and the islands between them, together represent the 0s and 1s of binary code that computers use to symbolize information. CD, DVD, and Blu-ray players use lasers to read the data encoded in these discs.</p><p>“The use of ultrahigh-density optical data storage technology in big data centers is now possible.” <strong>—Min Gu, University of Shanghai for Science and Technology</strong></p><p>Although optical discs are low in cost and highly durable, they are limited by the amount of data they can hold, which is usually stored in a single layer. Previously, scientists investigated encoding data on optical discs in many layers in <a href="https://spectrum.ieee.org/racetrack-memory" target="_self">three dimensions</a> to boost their capacity. However, a key barrier that prior research faced was how the optics used to read and write this data were limited to roughly the size of the wavelengths of light they used.</p><p>Now scientists in China have developed a way to encode data on 100 layers in optical discs. In addition, the data is recorded using spots as small as 54 nanometers wide, roughly a tenth of the size of the wavelengths of visible light used to read and write the data.</p><p>All in all, a DVD-size version of the new disc has a capacity of up to 1.6 <a href="https://spectrum.ieee.org/frequency-comb" target="_self">petabits</a>—that is, 1.6 million gigabits. This is some 4,000 times as much data density as a <a href="https://spectrum.ieee.org/the-consumer-electronics-hall-of-fame-samsung-bdp1000" target="_self">Blu-ray disc</a> and 24 times as much as the currently most advanced hard disks. The researchers suggest their new optical disc can enable a data center capable of exabit storage—a billion gigabits—to fit inside a room instead of a stadium-size space.</p><p>“The use of ultrahigh-density optical data storage technology in big data centers is now possible,” says <a href="https://en.wikipedia.org/wiki/Min_Gu" target="_blank">Min Gu</a>, professor of <a href="https://www.atse.org.au/news-and-events/article/min-gu-appointed-executive-chancellor-in-shanghai/" target="_blank">optical-electrical and computer engineering</a> at the <a href="https://en.wikipedia.org/wiki/University_of_Shanghai_for_Science_and_Technology" target="_blank">University of Shanghai for Science and Technology</a>.</p><h3>How to store a petabit on one disc</h3><p>The strategy the researchers used to write the data relies on a pair of lasers. The first, a green 515-nanometer laser, triggers spot formation, whereas the second, a red 639-nm laser, switches off the writing process. By controlling the time between firing of the lasers, the scientists could produce spots smaller than the wavelengths of light used to create them.</p><p>The procedure used to create blank discs is compatible with conventional DVD mass production and can be completed within 6 minutes.</p><p>To read the data, the researchers again depended on a pair of lasers. The first, a blue 480-nm beam, can make spots fluoresce, while the second, an orange 592-nm light, switches off the fluorescence process. Precise control over the firing of these lasers can single out which specific nanometer-scale spot ends up fluorescing.</p><p>This new strategy depends on a novel light-sensitive material called AIE-DDPR that is capable of all these varied responses to different wavelengths of light. “It has been a 10-year effort searching for this kind of material,” Gu says. “The difficulty has been how the writing and reading processes affect each other in a given material—in particular, in a three-dimensional geometry.”</p><p>The scientists encoded data on layers each separated by 1 micrometer. They found that the writing quality stayed comparable across all the layers. “Personally, I was surprised that nanoscale writing-recoding and reading processes both work well in our newly invented material,” Gu says.</p><p>The researchers note that the entire procedure used to create blank discs made using AIE-DDPR films is compatible with conventional DVD mass production and can be completed within 6 minutes. Gu says these new discs may therefore prove to be manufacturable at commercial scales.</p><p>Currently, he says, the new discs have a writing speed of about 100 milliseconds and an energy consumption of microjoules to millijoules. </p><p>Still, Gu says, the researchers would like to see their new discs used in big data centers. As a result, they’re working to improve their new method’s writing speed and energy consumption. He suggests this may be possible using new, more energy-efficient recording materials. He says more layers in each disc may be possible in the future, using better lenses and fewer aberrations in their optics.</p><p>The scientists detailed <u><a href="https://www.nature.com/articles/s41586-023-06980-y" target="_blank">their findings</a></u> online 21 February in the journal <em>Nature</em>.</p></div></div>]]></description>
        </item>
    </channel>
</rss>