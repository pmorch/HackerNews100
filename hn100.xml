<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0">
    <channel>
        <title>HN100 - Readable Contents</title>
        <link>https://hn.algolia.com/api/v1/search_by_date?tags=%28story,poll%29&amp;numericFilters=points%3E100</link>
        <description>Uses Readability to add bodies to the RSS feed</description>
        <lastBuildDate>Tue, 17 Jun 2025 05:30:01 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>en</language>
        <item>
            <title><![CDATA[Generative AI coding tools and agents do not work for me (154 pts)]]></title>
            <link>https://blog.miguelgrinberg.com/post/why-generative-ai-coding-tools-and-agents-do-not-work-for-me</link>
            <guid>44294633</guid>
            <pubDate>Tue, 17 Jun 2025 00:33:45 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://blog.miguelgrinberg.com/post/why-generative-ai-coding-tools-and-agents-do-not-work-for-me">https://blog.miguelgrinberg.com/post/why-generative-ai-coding-tools-and-agents-do-not-work-for-me</a>, See on <a href="https://news.ycombinator.com/item?id=44294633">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>People keep asking me If I use Generative AI tools for coding and what I think of them, so this is my effort to put my thoughts in writing, so that I can send people here instead of having to repeat myself every time I get the question.</p>
<p>From the title you already know that this isn't a pro-AI blog post. But it isn't an anti-AI post either, at least I don't think it is. There are already plenty of articles by AI promoters and AI critics, so I don't feel there is a need for me to write one more of those. While I'm definitely not neutral on the subject, in this article I'm just going to share my personal experience with these tools, from a strictly technical point of view.</p>
<h2>AI is not faster</h2>
<p>Really the main and most important reason why GenAI tools do not work for me is that <strong>they do not make me any faster</strong>. It's really that simple.</p>
<p>It would be easy to use GenAI coding tools to have code written for me. A coding agent would be the most convenient, as it would edit my files while I do something else. This all sounds great, in principle.</p>
<p>The problem is that I'm going to be responsible for that code, so I cannot blindly add it to my project and hope for the best. I could only incorporate AI generated code into a project of mine after I thoroughly review it and make sure I understand it well. I have to feel confident that I can modify or extend this piece of code in the future, or else I cannot use it.</p>
<p>Unfortunately reviewing code is actually harder than most people think. It takes me at least the same amount of time to review code not written by me than it would take me to write the code myself, if not more. There is actually a well known saying in our industry that goes something like "it’s harder to read code than to write it." I believe it was Joel Spolsky (creator of Stack Overflow and Trello) who formalized it first in his <a href="https://www.joelonsoftware.com/2000/04/06/things-you-should-never-do-part-i/">Things You Should Never Do, Part I</a> article.</p>
<p>You could argue that code that was written by AI can be considered a black box. I guess you can convince yourself that as long as the code works as intended it is safe to use without the need to review it, which would translate into some productivity increase. I think this is highly irresponsible, because the AI is not going to assume any liability if this code ever malfunctions. I'm always the responsible party for the code I produce, with or without AI. Taking on such a large risk is nuts, in my opinion.</p>
<p>This is even more important for some of the work that I do where there are contracts signed, with associated legal obligations and money payments. If I'm hired as a professional, I really have no other choice than to be one. AI tools cannot help me make more money or do my work in less time. The only way I could achieve those things is by degrading the quality of the work and introducing risk, and I'm not willing to do that.</p>
<h2>AI is not a multiplier</h2>
<p>I've heard people say that GenAI coding tools are a multiplier or enabler for them. Basically those who make this claim say that they are able to work faster and tackle more difficult problems when using GenAI. Unfortunately these claims are just based on the perception of the subjects themselves, so there is no hard data to back them up. I guess it is possible that some people can be more efficient reviewing code than I am, but I honestly doubt it. What I think happens is that these people save time because they only spot review the AI generated code, or skip the review phase altogether, which as I said above would be a deal breaker for me.</p>
<p>Another common argument I've heard is that Generative AI is helpful when you need to write code in a language or technology you are not familiar with. To me this also makes little sense. The part that I enjoy the most about working as a software engineer is learning new things, so not knowing something has never been a barrier for me. The more you practice learning the easier and faster it gets! In recent times I had to learn Rust, Go, TypeScript, WASM, Java and C# for various projects, and I wouldn't delegate this learning effort to an AI, even if it saved me time. Which it wouldn't, because of all the reasons above about being responsible for the code that I produce. Sorry if I'm a bit repetitive on this.</p>
<h2>AI code is different than human code</h2>
<p>I made all these points to a friend the other day and he asked me why then I gladly accept open source contributions to my projects when they are made by people. Aren't those also code that is not written by myself? Why are those okay but AI generated code is not?</p>
<p>The truth that may be shocking to some is that open source contributions submitted by users do not really save me time either, because I also feel I have to do a rigorous review of them. But I enjoy working with users who have an interest in my projects and take time to report bugs, request new features or submit code changes. These interactions are a source of new ideas more than anything, so they directly help me do better work. This is what I love the most of working in open source!</p>
<p>My friend, who is still unconvinced, suggests I could launch a bunch of AI agents in parallel to create PRs for all my open bugs. It's a game changer, he says. Unfortunately that would cost me money and likely make me slower, for the reasons explained above. Even if we assume that AI coding tools are sophisticated enough (they are not) to fix issues in my projects with little or no supervision, I'm still the bottleneck because all that code has to be reviewed before it can be merged.</p>
<p>The unfortunate side of AI coding tools being widely available is that some users now also generate low effort pull requests with them. I have received some of these, and it's interesting that there is a sort of <a href="https://en.wikipedia.org/wiki/Uncanny_valley">uncanny valley</a> effect that triggers in me when I start reading AI generated code that hasn't been edited and refined by a real person. When I come across pull requests of this type I start asking questions to the submitters about the weird parts of their submissions, because I consider them responsible for the code they want me to merge. They rarely respond.</p>
<h2>AI is not the same as an intern</h2>
<p>Many AI advocates say that you should treat your AI coding tool as an intern that is eager to please. I think the people who say this never worked with interns!</p>
<p>In the beginning, delegating work to an intern causes a productivity decrease for you, for the same reasons I enumerated above. Interns need a lot of hand-holding, and all the code they produce needs to be carefully reviewed before it is accepted.</p>
<p>But interns <em>learn</em> and get better over time. The time that you spend reviewing code or providing feedback to an intern is not wasted, it is an investment in the future. The intern absorbs the knowledge you share and uses it for new tasks you assign to them later on. The need for close supervision goes down throughout the duration of the internship. In the end, interns are often hired by their companies as full time employees because they become successful independent contributors.</p>
<p>An AI tool can only resemble an intern with <a href="https://en.wikipedia.org/wiki/Anterograde_amnesia">anterograde amnesia</a>, which would be a bad kind of intern to have. For every new task this "AI intern" resets back to square one without having learned a thing!</p>
<h2>Conclusion</h2>
<p>I hope with this article I've made the technical issues I have with applying GenAI coding tools to my work clear.</p>
<p>In my experience, there is no such thing as a free lunch with AI coding. I believe people who claim that it makes them faster or more productive are making a conscious decision to relax their quality standards to achieve those gains. Either that or they just say this because they personally benefit from selling AI to you.</p></div><p>Thank you for visiting my blog! If you enjoyed this article, please consider supporting my work and keeping me caffeinated with a small one-time donation through <a href="https://www.buymeacoffee.com/miguelgrinberg">Buy me a coffee</a>. Thanks!</p></div>]]></description>
        </item>
        <item>
            <title><![CDATA[OpenAI wins $200M U.S. defense contract (132 pts)]]></title>
            <link>https://www.cnbc.com/2025/06/16/openai-wins-200-million-us-defense-contract.html</link>
            <guid>44293988</guid>
            <pubDate>Mon, 16 Jun 2025 22:31:58 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.cnbc.com/2025/06/16/openai-wins-200-million-us-defense-contract.html">https://www.cnbc.com/2025/06/16/openai-wins-200-million-us-defense-contract.html</a>, See on <a href="https://news.ycombinator.com/item?id=44293988">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="SpecialReportArticle-ArticleBody-6" data-module="ArticleBody" data-test="articleBody-2" data-analytics="SpecialReportArticle-articleBody-6-2"><div id="ArticleBody-InlineImage-108160114" data-test="InlineImage"><p>OpenAI CEO Sam Altman speaks during the Snowflake Summit in San Francisco on June 2, 2025.</p><p>Justin Sullivan | Getty Images News | Getty Images</p></div><div><p>OpenAI has been awarded a $200 million contract to provide the U.S. Defense Department with artificial intelligence tools.</p><p>The department announced the one-year contract on Monday, months after OpenAI said it <a href="https://www.cnbc.com/2024/12/04/openai-partners-with-defense-company-anduril.html">would collaborate</a> with defense technology startup Anduril to deploy advanced AI systems for "national security missions."</p><p>"Under this award, the performer will develop prototype frontier AI capabilities to address critical national security challenges in both warfighting and enterprise domains," the Defense Department said. It's the first contract with OpenAI listed on the Department of Defense's website.</p><p>Anduril received a $100 million defense contract in December. Weeks earlier, OpenAI rival Anthropic said it would work with <span data-test="QuoteInBody" id="SpecialReportArticle-QuoteInBody-2"><a href="https://www.cnbc.com/quotes/PLTR/">Palantir</a><span><span id="-WatchlistDropdown" data-analytics-id="-WatchlistDropdown"></span></span></span> and <span data-test="QuoteInBody" id="SpecialReportArticle-QuoteInBody-3"><a href="https://www.cnbc.com/quotes/AMZN/">Amazon</a><span><span id="-WatchlistDropdown" data-analytics-id="-WatchlistDropdown"></span></span></span> to supply its AI models to U.S. defense and intelligence agencies.</p><p>Sam Altman, OpenAI's co-founder and CEO, said in a discussion with OpenAI board member and former National Security Agency leader Paul Nakasone at a <a href="https://www.youtube.com/watch?v=Po8tMs2OLdk" target="_blank">Vanderbilt University event</a> in April that "we have to and are proud to and really want to engage in national security areas."</p><p>In a <a href="https://openai.com/global-affairs/introducing-openai-for-government/" target="_blank">blog post</a>, OpenAI said the contract represents the first arrangement in a new initiative named OpenAI for Government, which includes the <a href="https://www.cnbc.com/2025/01/28/openai-launches-chatgpt-gov-for-us-government-agencies.html">existing ChatGPT Gov</a> product. OpenAI for Government will give U.S. government bodies access custom AI models for national security, support and product roadmap information.</p><p>"This contract, with a $200 million ceiling, will bring OpenAI's industry-leading expertise to help the Defense Department identify and prototype how frontier AI can transform its administrative operations, from improving how service members and their families get health care, to streamlining how they look at program and acquisition data, to supporting proactive cyber defense," the company said. "All use cases must be consistent with OpenAI's usage policies and guidelines."</p><p>The Defense Department specified that the contract is with OpenAI Public Sector LLC, and that the work will mostly occur in the National Capital Region, which encompasses Washington, D.C., and several nearby counties in Maryland and Virginia.</p><p>Meanwhile, OpenAI is working to build additional computing power in the U.S. In January, <a href="https://www.cnbc.com/2025/01/21/trump-ai-openai-oracle-softbank.html">Altman appeared</a> alongside <a href="https://www.cnbc.com/donald-trump/">President Donald Trump</a> at the White House to announce the $500 billion Stargate project to build AI infrastructure in the U.S.</p><p>The new contract will represent a small portion of revenue at OpenAI, which is generating over $10 billion in annualized sales. In March, the company <a href="https://www.cnbc.com/2025/03/31/openai-closes-40-billion-in-funding-the-largest-private-fundraise-in-history-softbank-chatgpt.html">announced</a> a $40 billion financing round at a $300 billion valuation.</p><p>In April,&nbsp;<span data-test="QuoteInBody" id="SpecialReportArticle-QuoteInBody-10"><a href="https://www.cnbc.com/quotes/MSFT/">Microsoft</a><span><span id="-WatchlistDropdown" data-analytics-id="-WatchlistDropdown"></span></span></span>, which supplies cloud infrastructure to OpenAI,&nbsp;said the U.S. Defense Information Systems Agency has <a href="https://devblogs.microsoft.com/azuregov/azure-openai-authorization/" target="_blank">authorized the use</a> of the Azure OpenAI service with secret classified information.&nbsp;</p><p><strong>WATCH:</strong> <a href="https://www.cnbc.com/video/2025/06/09/openai-hits-10-billion-in-annual-recurring-revenue.html">OpenAI hits $10 billion in annual recurring revenue</a></p></div><div id="Placeholder-ArticleBody-Video-108156648" data-test="VideoPlaceHolder" role="region" tabindex="0" data-vilynx-id="7000378838" aria-labelledby="Placeholder-ArticleBody-Video-108156648"><p><img src="https://image.cnbcfm.com/api/v1/image/108156649-17494892081749489205-40189485738-1080pnbcnews.jpg?v=1749489207&amp;w=750&amp;h=422&amp;vtcrop=y" alt="OpenAI hits $10 billion in annual recurring revenue"><span></span><span></span></p></div></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[What happens when clergy take psilocybin (107 pts)]]></title>
            <link>https://nautil.us/clergy-blown-away-by-psilocybin-1217112/</link>
            <guid>44293610</guid>
            <pubDate>Mon, 16 Jun 2025 21:34:09 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://nautil.us/clergy-blown-away-by-psilocybin-1217112/">https://nautil.us/clergy-blown-away-by-psilocybin-1217112/</a>, See on <a href="https://news.ycombinator.com/item?id=44293610">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
			<div>
					<!-- Speedbump Header (Desktop) -->
										<!-- Speedbump Copy  -->
				
		<p>
			<span>The full <i>Nautilus</i> archive</span>
			<span>•</span>
			<span>eBooks &amp; Special Editions</span>
			<span>•</span>
			<span>Ad-free reading</span>
		</p>
		<ul>
			<li>The full <i>Nautilus</i> archive </li>
			<li>eBooks &amp; Special Editions</li>
			<li>Ad-free reading</li>
		</ul>
					
			</div>

			<!-- Speedbump Image  -->
			<p><img src="https://nautil.us/wp-content/plugins/fragment-blocks/src/blocks/speedbump-join/assets/join.png" alt="Join" loading="lazy">
			</p>
		</div><div>
                        
            <p><span>A</span>lmost a decade ago, a Baptist Biblical scholar, a Catholic priest, several rabbis, an Islamic leader, a Zen Buddhist roshi, and more than a dozen other religious leaders walked into a lab—and took high doses of magic mushrooms.</p>
      
    <p>All of them said it was their first time taking the drug. The <a href="https://nautil.us/what-hallucinogens-will-make-you-see-308247/" target="_blank" rel="noreferrer noopener">mind-altering details</a> of these guided trips were recorded at the time and over the following 16 months, but it wasn’t until recently that the results of the controversial experiment came to light.</p><p>One might wonder how a single psilocybin trip could compare to the catalog of rich transcendent experiences that might accumulate over a lifetime of religious devotion. But according to the <a href="https://www.liebertpub.com/doi/10.1089/psymed.2023.0044" target="_blank" rel="noreferrer noopener">findings</a>, which were published in the peer-reviewed journal <em>Psychedelic Medicine</em>, the vast majority of the 33 clergy who participated in the study—more than 90 percent—said taking psilocybin was one of the most spiritually meaningful and deeply sacred experiences of their lives. Almost half said it was the most profound thing they had ever experienced, period. Many of them also said it made them better religious leaders.&nbsp;</p><div><p>Now, years later, <a href="https://www.newyorker.com/magazine/2025/05/26/this-is-your-priest-on-drugs" target="_blank" rel="noreferrer noopener">some of these clergy</a> have become evangelists for psychedelics, incorporating them into their own religious teachings. For some of them, the experience led to a release from attachment to dogmas and greater openness to other forms of religious experience. For at least one participant, it was a dark, empty, terrifying trip. Still, none of them ruled out using psilocybin again in the future.</p><p>Publication of the study took so long in part due to charges of ethical lapses, including potential conflicts of interest related to funding sources, as well as the direct involvement of a funder in the research itself. But these conflicts were eventually resolved through disclosure, which the authors say they always intended. Questions also swirled around certain flaws in the study’s execution, which even the authors, scientists at Johns Hopkins University and New York University, admit. </p><p>One issue was bias: Participants may have been primed to see their experiences as sacred by language used in recruitment ads and by the expectations of those running the experiment. (Many of those who chose to participate were also considering leaving the profession at the outset and so could have been seeking a way to reconnect with the divine.) The sample was also small, heavily white, male, and Christian; and representation of a number of major world religions, including Indigenous religious traditions, Hinduism, Taoism, and Confucianism, was absent.</p></div>
          <div>
            <p>ADVERTISEMENT</p>
            
            
      <p>
        Nautilus Members enjoy an ad-free experience.
        <a href="https://nautil.us/concierge-login" data-ev-act="login" data-ev-cat="article-ad" data-ev-label="in body ad">
          Log in
        </a>
        or
        <a href="https://nautil.us/join" data-ev-act="subscribe" data-ev-cat="article-ad" data-ev-label="in body ad">
          Join now
        </a>.
      </p>
          </div><p>Still the results raise questions about the relationship between hallucinogens and religious experience. Most of the major world religions today (Hinduism, Judaism, Buddhism, Christianity, Islam) do not advocate the use of mind-altering substances. But psychedelic plants and mushrooms have been employed in sacred ceremonies by <a href="https://nautil.us/the-long-history-of-psychedelic-theft-430330/" target="_blank" rel="noreferrer noopener">Indigenous cultures</a> in the Americas for millennia, and many psychedelic researchers suspect they drove pagan mystical experiences in ancient Greece that may have served as the foundations for some religions, including <a href="https://nautil.us/is-christianity-based-on-psychedelic-trips-623594/" target="_blank" rel="noreferrer noopener">Christianity</a>.</p><p>William James, considered the father of American psychology and author of <em>The Varieties of Religious Experience</em>, is said to have to come to many of his own <a href="https://www.sciencedirect.com/science/article/abs/pii/S2352452917301445#:~:text=For%20James%2C%20the%20use%20of,%2C%20constant%2C%20eternal)%2C%20driven" target="_blank" rel="noreferrer noopener">most central ideas</a> at least in part through hallucinatory experiences with nitrous oxide: the value of religion, the importance of mystical experience, the universe as pluralistic. But transcendence is not an unequivocal good: As one religious scholar found, you can have <a href="https://nautil.us/you-can-have-too-much-transcendence-448748/" target="_blank" rel="noreferrer noopener">too much</a> of it. <img decoding="async" src="https://assets.nautil.us/sites/3/nautilus/nautilus-favicon-14.png?fm=png" alt=""></p><p><em>Lead image: New Africa / Shutterstock</em></p>              
                            <ul>
                                      <li>
                      <div>
                        <h6>
                          Kristen French                        </h6>
                        <p>
                          Posted on <time datetime="2025-06-12T04:50:00-05:00">June 12, 2025</time>
                        </p>
                      </div>
                                                <p>
                            Kristen French is an associate editor at <i>Nautilus</i>.                          </p>
                                            </li>
                                  </ul>
                    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Show HN: Chawan TUI web browser (212 pts)]]></title>
            <link>https://chawan.net/news/chawan-0-2-0.html</link>
            <guid>44293260</guid>
            <pubDate>Mon, 16 Jun 2025 20:48:43 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://chawan.net/news/chawan-0-2-0.html">https://chawan.net/news/chawan-0-2-0.html</a>, See on <a href="https://news.ycombinator.com/item?id=44293260">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>

<p>
Version 0.2.0 of the Chawan TUI browser has been released.
</p><p>
A tarball of the source tree is available
<a href="https://chawan.net/dl/chawan-0-2-0-src.tar.xz">here</a>.  Please refer to the README
file for compilation instructions.
</p><p>
A static binary distribution for
<a href="https://chawan.net/dl/chawan-0-2-0-linux-amd64.tar.xz">amd64 Linux</a> also exists.
To install it, extract the archive somewhere and run <code>make install</code>
as root.  (To uninstall, run <code>make uninstall</code>.)
</p><p>
The same distribution is also available as a <a href="https://chawan.net/dl/chawan-0-2-0-amd64.deb">.deb
package</a>.
</p><h2 id="information-for-package-maintainers"><a href="#information-for-package-maintainers">##</a> Information for package maintainers</h2>
<p>
The current list of mandatory runtime dependencies is:
</p><ul>
<li>libssh2.
</li><li>libbrotli (more precisely, libbrotlicommon and libbrotlidec).
</li><li>OpenSSL or LibreSSL.  For OpenSSL, you will want 3.0 or later.
For LibreSSL, the version in OpenBSD 7.7 has been tested.
</li></ul>
<p>
Previous development versions had other dependencies which no
longer apply, and can be dropped.  In particular, zlib, libseccomp,
termcap/ncurses and libcurl are no longer used.
</p><p>
If you run into an issue while packaging Chawan, please contact me
before trying to patch over it.  Chances are we can solve it upstream.
</p><h2 id="whats-next"><a href="#whats-next">##</a> What's next</h2>
<p>
It took a bit longer than expected, but I finally feel OK putting a
version on this.  It has all features I wanted from an MVP, and no known
fatal bugs.
</p><p>
The v0.2 branch in git will only receive bugfixes.  Further work on new
features will continue on the master branch.
</p><p>
For the next release, I hope to improve upon the layout module's
performance &amp; correctness, and to make the UI somewhat more user
friendly.  Stay tuned :)
</p>

</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Retrobootstrapping Rust for some reason (111 pts)]]></title>
            <link>https://graydon2.dreamwidth.org/317484.html</link>
            <guid>44293044</guid>
            <pubDate>Mon, 16 Jun 2025 20:19:44 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://graydon2.dreamwidth.org/317484.html">https://graydon2.dreamwidth.org/317484.html</a>, See on <a href="https://news.ycombinator.com/item?id=44293044">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>Elsewhere I've been asked about the task of replaying the bootstrap process for rust. I figured it would be fairly straightforward, if slow. But as we got into it, there were <em>just</em> enough tricky / non-obvious bits in the process that it's worth making some notes here for posterity.</p><p>UPDATE: someone has <a href="https://github.com/LegionMammal978/rust-from-ocaml">also scripted many of the subsequent snapshot builds</a> covering many years of rust's post-bootstrap development. Consider the rest of this post just a verbose primer for interpreting their work.</p><h3>context</h3><p>Rust started its life as a compiler written in ocaml, called <b>rustboot</b>. This compiler did <em>not</em> use LLVM, it just emitted 32-bit i386 machine code in 3 object file formats (Linux PE, macOS Mach-O, and Windows PE).</p><p>We then wrote a <em>second</em> compiler <em>in Rust</em> called <b>rustc</b> that <em>did</em> use LLVM as its backend (and which, yes, is the genesis of today's rustc) and ran rustboot on rustc to produce a so-called "stage0 rustc". Then stage0 rustc was fed the sources of rustc again, producing a stage1 rustc. Successfully executing this stage0 -&gt; stage1 step (rather than just crashing mid-compilation) is what we're going to call "bootstrapping". There's also a third step: running stage1 rustc on rustc's sources again to get a stage2 rustc and checking that it is bit-identical to the stage1 rustc. Successfully doing <em>that</em> we're going to call "fixpoint".</p><p>Shortly after we reached the fixpoint we <a href="https://github.com/rust-lang/rust/commit/6997adf76342b7a6fe03c4bc370ce5fc5082a869">discarded rustboot</a>. We stored stage1 rustc binaries as snapshots on a shared download server and all subsequent rust builds were based on downloading and running that. Any time there was an incompatible language change made, we'd add support and re-snapshot the resulting stage1, gradually growing a long list of snapshots marking the progress of rust over time.</p><h3>time travel and bit rot</h3><p>Each snapshot can typically only compile rust code in the rust repository written between its birth and the next snapshot. This makes replaying the entire history awkward (<a href="https://github.com/LegionMammal978/rust-from-ocaml">see above</a>). We're not going to do that here. This post is just about replaying the initial bootstrap and fixpoint, which happened back in April 2011, 14 years ago.</p><p>Unfortunately all the tools involved -- from the host OS and system libraries involved to compilers and compiler-components -- were and are moving targets. Everything bitrots. Some examples discovered along the way:</p><ul><br><li>Modern clang and gcc won't compile the LLVM used back then (C++ has changed too much -- and I tried several CXXFLAGS=-std=c++NN variants!)<br></li><li>Modern gcc won't even compile the gcc used back then (apparently C as well!)<br></li><li>Modern ocaml won't compile rustboot (ditto)<br></li><li>14-year-old git won't even connect to modern github (ssh and ssl have changed too much)<br></li></ul><h3>debian</h3><p>We're in a certain amount of luck though:</p><ul><br><li>Debian has maintained both EOL'ed docker images and still-functioning fetchable package archives at the same URLs as 14 years ago. So we can time-travel using that. A VM image would also do, and if you have old install media you could presumably build one up again if you are patient.<br></li><li>It is easier to use i386 since that's all rustboot emitted. There's some indication in the Makefile of support for multilib-based builds from x86-64 (I honestly don't remember if my desktop was 64 bit at the time) but 32bit is much more straightforward.<br></li><li>So: <tt>docker pull --platform linux/386 debian/eol:squeeze</tt> gets you an environment that works.<br></li><li>You'll need to install rust's prerequisites also: g++, make, ocaml, ocaml-native-compilers, python.<br></li></ul><h3>rust</h3><p>The next problem is figuring out the code to build. Not totally trivial but not <em>too</em> hard. The best resource for tracking this period of time in rust's history is actually the rust-dev mailing list archive. There's <a href="https://www.mail-archive.com/rust-dev@mozilla.org/info.html">a copy online at mail-archive.com</a> (and Brian keeps <a href="https://github.com/brson/rust-dev-archives">a public backup of the mbox file</a> in case that goes away). Here's <a href="https://www.mail-archive.com/rust-dev@mozilla.org/msg00329.html">the announcement that we hit a fixpoint</a> in April 2011. You kinda have to just know that's what to look for. So that's the rust commit to use: 6daf440037cb10baab332fde2b471712a3a42c76. This commit still exists in the rust-lang/rust repo, no problem getting it (besides having to copy it into the container since the container can't contact github, haha).</p><h3>LLVM</h3><p>Unfortunately we only started pinning LLVM to specific versions, using submodules, <em>after</em> bootstrap, closer to the initial "0.1 release". So we have to guess at the LLVM version to use. To add some difficulty: LLVM at the time was developed on subversion, and we were developing rust against <a href="https://github.com/brson/llvm">a fork of a git mirror of their SVN</a>. Fishing around in that repo at least finds a version that builds -- <a href="https://github.com/brson/llvm/commit/45e1a53efd40a594fa8bb59aee75bb0984770d29">45e1a53efd40a594fa8bb59aee75bb0984770d29</a>, which is "the commit that exposed </p><tt>LLVMAddEarlyCSEPass</tt><p>", a symbol used in the rustc LLVM interface. I bootstrapped with that (brson/llvm) commit but subversion also numbers all commits, and they were preserved in the conversion to the modern LLVM repo, so you can see the same svn id 129087 as <a href="https://github.com/llvm/llvm-project/commit/e4e4e3758097d7967fa6edf4ff878ba430f84f6e">e4e4e3758097d7967fa6edf4ff878ba430f84f6e</a> over in the official LLVM git repo, in case brson/llvm goes away in the future.</p><p>Configuring LLVM for this build is also a little bit subtle. The best bet is to actually read the rust 0.1 configure script -- when it was managing the LLVM build itself -- and work out what it would have done. But I have done that and can now save you the effort: </p><tt>./configure --enable-targets=x86 --build=i686-unknown-linux-gnu --host=i686-unknown-linux-gnu --target=i686-unknown-linux-gnu --disable-docs --disable-jit --enable-bindings=none --disable-threads --disable-pthreads --enable-optimized</tt><p>So: configure and build that, stick the resulting bin dir in your path, and configure and make rust, and you're good to go!</p><pre>root@65b73ba6edcc:/src/rust# sha1sum stage*/rustc
639f3ab8351d839ede644b090dae90ec2245dfff  stage0/rustc
81e8f14fcf155e1946f4b7bf88cefc20dba32bb9  stage1/rustc
81e8f14fcf155e1946f4b7bf88cefc20dba32bb9  stage2/rustc
</pre><h3>Observations</h3><p>On my machine I get: 1m50s to build stage0, 3m40s to build stage1, 2m2s to build stage2. Also stage0/rustc is a 4.4mb binary whereas stage1/rustc and stage2/rustc are (identical) 13mb binaries.</p><p>While this is somewhat congruent with my recollections -- rustboot produced code faster, but its code ran slower -- the effect size is actually much less than I remember. I'd convinced myself retroactively that rustboot was produced <em>abysmally</em> worse code than rustc-with-LLVM. But out-of-the-gate LLVM only boosted performance by 2x (and cost of 3x the code size)! Of course I also have a faster machine now. At the time bootstrap cycles took about a half hour each (<a href="https://www.mail-archive.com/rust-dev@mozilla.org/msg00331.html">according to this: 15 minutes for the 2nd stage</a>).</p><p>Of course you can still see this as a condemnation of the entire "super slow dynamic polymorphism" model of rust-at-the-time, either way. It may seem funny that this version of rustc bootstraps faster than today's rustc, but this "can barely bootstrap" version was a mere 25kloc. Today's rustc is 600kloc. It's really comparing apples to oranges.</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Transparent peer review to be extended to all of Nature's research papers (111 pts)]]></title>
            <link>https://www.nature.com/articles/d41586-025-01880-9</link>
            <guid>44292342</guid>
            <pubDate>Mon, 16 Jun 2025 18:51:33 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.nature.com/articles/d41586-025-01880-9">https://www.nature.com/articles/d41586-025-01880-9</a>, See on <a href="https://news.ycombinator.com/item?id=44292342">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
    <header>
        <div>
            <ul data-test="article-identifier">
                <li data-test="article-category"><span>EDITORIAL</span></li>
                <li><time datetime="2025-06-16">16 June 2025</time></li>
                
            </ul>

            

            <div>
                
                <p>
                    From today, all new submissions to <i>Nature</i> that are published will be accompanied by referees’ reports and author responses — to illuminate the process of producing rigorous science.
                </p>
            </div>
        </div>
        
    </header>
    
</div><div data-track-context="article body">
                            
        
        <p data-test="access-message">
                You have full access to this article via your institution.</p>
        
    

                    </div><div data-test="main-content">
                    
                        <figure><picture><source type="image/webp" srcset="https://media.nature.com/lw767/magazine-assets/d41586-025-01880-9/d41586-025-01880-9_51107390.jpg?as=webp 767w, https://media.nature.com/lw319/magazine-assets/d41586-025-01880-9/d41586-025-01880-9_51107390.jpg?as=webp 319w" sizes="(max-width: 319px) 319px, (min-width: 1023px) 100vw,  767px"><img alt="Two white speech bubbles on a yellow background with two clear speech bubbles overlaid on top" loading="lazy" src="https://media.nature.com/lw767/magazine-assets/d41586-025-01880-9/d41586-025-01880-9_51107390.jpg"><figcaption><p><span>A published research paper is the result of an extensive conversation between authors and reviewers, guided by editors.</span><span>Credit: Getty</span></p></figcaption></picture></figure><p>Since 2020, <i>Nature</i> has offered authors the opportunity to have their <a href="https://www.nature.com/articles/d41586-020-00309-9" data-track="click" data-label="https://www.nature.com/articles/d41586-020-00309-9" data-track-category="body text link">peer-review file published alongside their paper</a>. Our colleagues at <i>Nature Communications</i> <a href="https://www.nature.com/articles/ncomms10277" data-track="click" data-label="https://www.nature.com/articles/ncomms10277" data-track-category="body text link">have been doing so since 2016</a>. Until now, <i>Nature</i> authors could opt in to this process of transparent peer review. From 16 June, however, new submissions of manuscripts that are published as research articles in <i>Nature</i> will automatically include a link to the reviewers’ reports and author responses.</p><p>It means that, over time, more <i>Nature</i> papers will include a peer-review file. The identity of the reviewers will remain anonymous, unless they choose otherwise — as happens now. But the exchanges between the referees and the authors will be accessible to all. Our aim in doing so is to open up what many see as the ‘black box’ of science, shedding light on how a research paper is made. This serves to increase transparency and (we hope) to build trust in the scientific process.</p><article data-label="Related"><a href="https://www.nature.com/articles/d41586-022-00056-z" data-track="click" data-track-label="recommended article"><img alt="" src="https://media.nature.com/w400/magazine-assets/d41586-025-01880-9/d41586-025-01880-9_20176346.jpg"><p>Research evaluation needs to change with the times</p></a></article><p>As <a href="https://www.nature.com/articles/d41586-022-00493-w" data-track="click" data-label="https://www.nature.com/articles/d41586-022-00493-w" data-track-category="body text link">we have written previously</a>, a published research paper is the result of an extensive conversation between authors and reviewers, guided by editors. These discussions, which can last for months, aim to improve a study’s clarity and the robustness of its conclusions. It is a hugely important process that should receive increased recognition, including acknowledgement of the reviewers involved, if they choose to be named. For early-career researchers, there is great value in seeing inside a process that is key to their career development. Making peer-reviewer reports public also enriches science communication: it’s a chance to add to the ‘story’ of how a result is arrived at, or a conclusion supported, even if it includes only the perspectives of authors and reviewers. The full story of a paper is, of course, more complex, involving many other contributors.</p><p>Many people think of science as something fixed and unchanging. But scientific knowledge evolves as new or more-nuanced evidence comes to light. Scientists constantly discuss their results, yet these debates are not contained in research papers and often remain unreported in wider science-communication efforts.</p><article data-label="Related"><a href="https://www.nature.com/articles/d41586-019-01162-1" data-track="click" data-track-label="recommended article"><img alt="" src="https://media.nature.com/w400/magazine-assets/d41586-025-01880-9/d41586-025-01880-9_17620936.jpg"><p>Three-year trial shows support for recognizing peer reviewers</p></a></article><p>The COVID-19 pandemic provided a brief interlude during which much of the world got to see how research works, <a href="https://www.nature.com/articles/d41586-020-01444-z" data-track="click" data-label="https://www.nature.com/articles/d41586-020-01444-z" data-track-category="body text link">almost in real time</a>. It’s easy to forget that, right from the start, we were continuously learning something new about the nature and behaviour of the SARS-CoV-2 virus. On television screens, in newspapers and on social media worldwide, scientists were discussing among themselves and with public audiences the nature of the virus, how it infects people and how it spreads. They were debating treatments and prevention methods, constantly adjusting everyone’s knowledge <a href="https://www.nature.com/articles/d41586-024-01173-7" data-track="click" data-label="https://www.nature.com/articles/d41586-024-01173-7" data-track-category="body text link">as fresh evidence came to light</a>. And then, it went mostly back to business as usual.</p><p>We hope that publishing the peer-reviewer reports of all newly submitted <i>Nature</i> papers shows, in a small way, that this doesn’t need to remain the case. <i>Nature</i> started mandating peer review for all published research articles only in 1973 (<a href="https://doi.org/10.1098/rsnr.2015.0029" data-track="click" data-label="https://doi.org/10.1098/rsnr.2015.0029" data-track-category="body text link">M. Baldwin <i>Notes Rec.</i> <b>69</b>, 337–352; 2015</a>). But the convention in most fields is still to keep the content of these peer-review exchanges confidential. That has meant that the wider research community, and the world, has had few opportunities to learn what is discussed.</p><p>Peer review improves papers. The exchanges between authors and referees should be seen as a crucial part of the scientific record, just as they are a key part of doing and disseminating research.</p>
                    
                </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Show HN: Canine – A Heroku alternative built on Kubernetes (189 pts)]]></title>
            <link>https://github.com/czhu12/canine</link>
            <guid>44292103</guid>
            <pubDate>Mon, 16 Jun 2025 18:27:59 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/czhu12/canine">https://github.com/czhu12/canine</a>, See on <a href="https://news.ycombinator.com/item?id=44292103">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/czhu12/canine/blob/main/public/images/logo-full.png?raw=true"><img src="https://github.com/czhu12/canine/raw/main/public/images/logo-full.png?raw=true" alt="alt text"></a></p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer nofollow" href="https://raw.githubusercontent.com/czhu12/canine/refs/heads/main/public/images/deployment_styled.png"><img src="https://raw.githubusercontent.com/czhu12/canine/refs/heads/main/public/images/deployment_styled.png" alt="Deployment Screenshot"></a></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">About the project</h2><a id="user-content-about-the-project" aria-label="Permalink: About the project" href="#about-the-project"></a></p>
<p dir="auto">Canine is an easy to use intuitive deployment platform for Kubernetes clusters.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Requirements</h2><a id="user-content-requirements" aria-label="Permalink: Requirements" href="#requirements"></a></p>
<ul dir="auto">
<li>Docker v24.0.0 or higher</li>
<li>Docker Compose v2.0.0 or higher</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Installation</h2><a id="user-content-installation" aria-label="Permalink: Installation" href="#installation"></a></p>
<div dir="auto" data-snippet-clipboard-copy-content="curl -sSL https://raw.githubusercontent.com/czhu12/canine/refs/heads/main/install/install.sh | bash"><pre>curl -sSL https://raw.githubusercontent.com/czhu12/canine/refs/heads/main/install/install.sh <span>|</span> bash</pre></div>
<hr>
<p dir="auto">Or run manually if you prefer:</p>
<div dir="auto" data-snippet-clipboard-copy-content="git clone https://github.com/czhu12/canine.git
cd canine/install
docker compose up -d"><pre>git clone https://github.com/czhu12/canine.git
<span>cd</span> canine/install
docker compose up -d</pre></div>
<p dir="auto">and open <a href="http://localhost:3000/" rel="nofollow">http://localhost:3000</a> in a browser.</p>
<p dir="auto">To customize the web ui port, supply the PORT env var when running docker compose:</p>
<div dir="auto" data-snippet-clipboard-copy-content="PORT=3456 docker compose up -d"><pre>PORT=3456 docker compose up -d</pre></div>
<p dir="auto"><h2 tabindex="-1" dir="auto">Cloud</h2><a id="user-content-cloud" aria-label="Permalink: Cloud" href="#cloud"></a></p>
<p dir="auto">Canine Cloud offers additional features for small teams:</p>
<ul dir="auto">
<li>GitHub integration for seamless deployment workflows</li>
<li>Team collaboration with role-based access control</li>
<li>Real-time metric tracking and monitoring</li>
<li>Way less maintenance for you</li>
</ul>
<p dir="auto">For more information &amp; pricing, take a look at our landing page <a href="https://canine.sh/" rel="nofollow">https://canine.sh</a>.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Repo Activity</h2><a id="user-content-repo-activity" aria-label="Permalink: Repo Activity" href="#repo-activity"></a></p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/3f53843fd98b39d804baecfefd9765ab4742b2e148654dbaabe5d7b28c986826/68747470733a2f2f7265706f62656174732e6178696f6d2e636f2f6170692f656d6265642f306166346365386137356634613132656337383937336464663730323163373639623961303035312e737667"><img src="https://camo.githubusercontent.com/3f53843fd98b39d804baecfefd9765ab4742b2e148654dbaabe5d7b28c986826/68747470733a2f2f7265706f62656174732e6178696f6d2e636f2f6170692f656d6265642f306166346365386137356634613132656337383937336464663730323163373639623961303035312e737667" alt="Alt" title="Repobeats analytics image" data-canonical-src="https://repobeats.axiom.co/api/embed/0af4ce8a75f4a12ec78973ddf7021c769b9a0051.svg"></a></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">License</h2><a id="user-content-license" aria-label="Permalink: License" href="#license"></a></p>
<p dir="auto"><a href="https://github.com/czhu12/canine/blob/main/LICENSE">Apache 2.0 License</a></p>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Getting free internet on a cruise, saving $170 (140 pts)]]></title>
            <link>https://angad.me/blog/2025/getting-free-cruise-internet/</link>
            <guid>44291630</guid>
            <pubDate>Mon, 16 Jun 2025 17:38:45 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://angad.me/blog/2025/getting-free-cruise-internet/">https://angad.me/blog/2025/getting-free-cruise-internet/</a>, See on <a href="https://news.ycombinator.com/item?id=44291630">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>

<article>
   

  <div><p>Picture this, you’re a teenager in the middle of the ocean on a cruise ship. All is good, except you’re lacking your lifeblood: internet. You could pay $170 for seven days of throttled internet on a <em>single</em> device, and perhaps split it via a travel router or hotspot, but that still seems less than ideal.</p>
<p>I’ve been travelling Europe with family and am currently on REDACTEDCRUISELINE Cruises’ REDACTEDCRUISELINE cruise ship. For the past two days, the ship has mostly been in port, so I was able to use a cellular connection. This quickly became not feasible as we got further from land, where there was no coverage. At around the same time, I wanted to download the REDACTEDCRUISELINE Cruises app on my phone, and realized that it would give me a <em>one-time 15-minute</em> internet connection. I activated it, and quickly realized that it didn’t limit you to just the Play Store or App Store: all websites could be accessed. I soon realized that this “one-time” download was MAC-address dependent and thus, could be bypassed by switching MAC addresses. However, doing so logs you out of the REDACTEDSERVICE portal, which is required to get the 15-minutes of free internet.</p>
<p>This means that in order to get just 15 minutes of internet, the following is required:</p>
<ol>
<li>Change MAC address</li>
<li>Login to REDACTEDSERVICE with date-of-birth and room number, binding the MAC address to your identity</li>
<li>Send a request with your booking ID activating 15 minutes of free internet, intended to be used for downloading the REDACTEDCRUISELINE app
<ul>
<li>Not completely sure, but it did initially seem that to activate unrestricted access to the internet, you would need to send a simple HTTP to play.google.com, although I don’t think this is needed.</li>
</ul>
</li>
</ol>
<p>This process, on the surface, seems extremely arduous. But if this could be automated, it would suddenly become a much more viable proposition. After looking at the fetch requests the REDACTEDSERVICE portal was sending to log in and activate the free sessions, I realized that it shouldn’t be <em>too</em> hard to automate.</p>
<p>Conveniently, my family also brought a travel router running OpenWRT as we were planning on purchasing the throttled single-device plan (which is ~$170 for the entire cruise) and using the router as a hotspot so we could connect multiple devices to the internet. This router (a GL.iNet) allows you to change the MAC address via the admin portal, needing only a single POST request. This meant that if I could string together the API requests to change the MAC address (after getting a token from the router login endpoint), login to REDACTEDSERVICE, and request the free internet session, I would have free internet.</p>
<p>I first tried copying the requests as cURL commands via DevTools into Notepad and using a local LLM to vibe-code a simple bash file. Realizing this wasn’t going to work, I began work on a Python script instead. I converted the cURL commands to <code>requests</code> via Copilot (yes, I used LLMs to an extent when building this) and started chaining together the requests.</p>
<p>The only issues I faced that took some time to overcome were figuring out how to repeat the requests when needed and being resistant to unexpected HTTP errors. For the former, I initially tried repeating it on an interval (first through a <code>while True</code> loop and later via shell scripting in the container) but later realized it was much easier by checking if internet access expired by sending a request to example.com and checking if it fails. For the latter, I used <code>while True</code> loops to allow the requests to be retried and executed <code>break</code> when the requests succeeded. The only other issue, that still exists, is that occasionally, the connection will drop out while the session is refreshed, although this seems to happen less than every 15 minutes and only lasts for a minute or two.</p>
<p>After comparing speeds with another guy I met on the cruise (who also happens to be a high-school programmer), who had the highest-level REDACTEDSERVICE plan, there seems to be no additional throttling (7+ Mbps). Even better, after connecting my power bank’s integrated USB-C cable to the router, I’m able to move around the ship for hours. So far, I’ve been using the router for nearly ~7 hours and my 10,000 mAh power bank is still at 42% battery. In fact, I’m writing this very post while connected to the router.</p>


  </div>

  
</article>
    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Darklang Goes Open Source (152 pts)]]></title>
            <link>https://blog.darklang.com/darklang-goes-open-source/</link>
            <guid>44290653</guid>
            <pubDate>Mon, 16 Jun 2025 15:41:44 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://blog.darklang.com/darklang-goes-open-source/">https://blog.darklang.com/darklang-goes-open-source/</a>, See on <a href="https://news.ycombinator.com/item?id=44290653">Hacker News</a></p>
<div id="readability-page-1" class="page"><section>
        <p>As part of <a href="https://blog.darklang.com/goodbye-dark-inc-welcome-darklang-inc/" rel="noreferrer"><u>shutting down Dark Inc.</u></a> and <a href="https://blog.darklang.com/first-steps-of-darklang-inc/" rel="noreferrer"><u>forming Darklang Inc.</u>,</a> we've finally open-sourced all of our repositories. <a href="https://github.com/darklang?ref=blog.darklang.com"><u>Our source code</u></a> is now under the Apache License 2.0.</p><p>For years, we wrestled with questions of sustainability and how to build something that truly empowers developers. We've long believed in open source philosophically, but felt that Darklang's unique architecture and business model required a different approach.</p><h2 id="why-we-initially-chose-source-available"><strong>Why We Initially Chose Source-Available</strong></h2><p>We originally designed Darklang as a hosted-only platform where you'd code at darklang.com and programs would instantly be live in production. We believed this centralized approach was necessary for features like safe code migration and unified deployment, and that offering self-hosting would undermine our sustainability model.</p><p>The core challenge was building something valuable while ensuring we could continue working on it long-term. Traditional open source funding models all had limitations, so Darklang was designed as "a language with a business model" - users with serious workloads would fund ecosystem development through our hosting platform.</p><h2 id="what-changed-our-thinking"><strong>What Changed Our Thinking</strong></h2><p>Three key shifts changed our perspective:</p><p><strong>Product maturity and user feedback</strong>: The real barrier to Darklang's adoption was never licensing - it was product maturity. As we've gotten closer to building something people love, staying source-available started feeling like an unnecessary risk. We consistently heard that people wanted us to be more open.</p><p><strong>Building for local-first development</strong>: Our technical direction evolved significantly. We're now building Darklang to run locally as a CLI, with the ability to deploy to our cloud or elsewhere. Nobody wants to run a proprietary language binary on their own machine.</p><p><strong>New business opportunities</strong>: The developer tools market has matured since 2017. We now see successful companies charging for team collaboration features and AI-powered tools while keeping the core platform accessible. These create value that teams are willing to pay for, while always having the option to self-host.</p><h2 id="why-open-source"><strong>Why Open Source</strong></h2><p>Open source enables Darklang to be accessible, inspectable, and community-owned. It aligns with our philosophy of democratizing programming and ensures the platform can persist and evolve regardless of any single company's fate.</p><p>We've learned how to deliver Darklang's key benefits - invisible infrastructure, deployless deployment, trace-driven development - without requiring our specific editor or hosting environment. This makes open source viable while preserving what makes Darklang special.</p><h2 id="open-questions"><strong>Open Questions</strong></h2><p>We're still exploring some interesting technical challenges around licensing in the Darklang ecosystem. GitHub handles this by attaching LICENSE.md files, but in a world where a package manager syncs types and functions directly, there are some neat challenges to think through. The core platform being open source gives us a solid foundation to build on.</p><hr><p>Comment on <a href="https://news.ycombinator.com/item?id=44290653&amp;ref=blog.darklang.com" rel="noreferrer">Hacker News</a>, <a href="https://x.com/darklang/status/1934633874610643126?ref=blog.darklang.com" rel="noreferrer">Twitter</a>, <a href="https://mas.to/@darklang/114693755778778354?ref=blog.darklang.com" rel="noreferrer">Mastodon</a>, <a href="https://lobste.rs/s/q0zon6/goodbye_dark_inc_hello_darklang_inc?ref=blog.darklang.com" rel="noreferrer">Lobste.rs</a>, <a href="https://www.linkedin.com/feed/update/urn:li:activity:7340415295649423360/?ref=blog.darklang.com" rel="noreferrer">LinkedIn</a></p>
    </section></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Object personification in autism: This paper will be sad if you don't read (2018) (102 pts)]]></title>
            <link>https://pubmed.ncbi.nlm.nih.gov/30101594/</link>
            <guid>44290582</guid>
            <pubDate>Mon, 16 Jun 2025 15:34:48 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://pubmed.ncbi.nlm.nih.gov/30101594/">https://pubmed.ncbi.nlm.nih.gov/30101594/</a>, See on <a href="https://news.ycombinator.com/item?id=44290582">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="article-page" data-article-pmid="30101594">
    

    <main id="article-details">
  
  
<!-- "Filters applied" shows only when page is redirected from search -->
<!-- because search found one result -->

  

  

  

  



  


<header id="heading">
  
    
      <div id="full-view-heading">
        
          <div>
            

            
  
    <div>
      
<p><span>. </span><span>2019 May;23(4):1042-1045.</span>

    </p></div>
  
  
    
      <p><span>
        doi: 10.1177/1362361318793408.
      </span>
    
    
    
      <span>
        Epub 2018 Aug 11.
      </span>
    
  


          </p></div>
          



          

          

        
        
          <p>
  

  
    Affiliations
  

  
    
  
</p>
        
        
          
        
        
  
    <ul id="full-view-identifiers">
      
        <li>
          <span>
  <span>
    
      PMID:
    
  </span>

  
    <strong title="PubMed ID">30101594</strong>
  

  
</span>

        </li>
      
        <li>
          <span>
  <span>
    
      DOI:
    
  </span>

  
    <a target="_blank" rel="noopener" ref="linksrc=article_id_link&amp;article_id=10.1177/1362361318793408&amp;id_type=DOI" href="https://doi.org/10.1177/1362361318793408" data-ga-category="full_text" data-ga-action="DOI">
      10.1177/1362361318793408
    </a>
  

  
</span>

        </li>
      
    </ul>
  


        
        
      </div>
      <div id="short-view-heading">
        

        

<h2>
  
    
    
    
    
      
  Object personification in autism: This paper will be very sad if you don't read it


    
  
</h2>

        

        <p><span>
    
      
        <span><span>Rebekah C White</span><span>&nbsp;et al.</span></span>
      
    
  </span>
  
    
      <span>
        Autism<span>.</span>
      </span>
      
        <span>
          <span>2019 May</span><span>.</span>
        </span>
      
    
  
</p>

        
        
        
      </div>
    
  
</header>

  



  

  



  <div id="abstract">
    
      <h2>
        Abstract
        
      </h2>
      
        
          
            <p>
      
      Object personification is the attribution of human characteristics to non-human agents. In online forums, autistic individuals commonly report experiencing this phenomenon. Given that approximately half of all autistic individuals experience difficulties identifying their own emotions, the suggestion that object personification may be a feature of autism seems almost paradoxical. Why would a person experience sympathy for objects, when they struggle to understand and verbalise the emotions of other people as well as their own? An online survey was used to assess tendency for personification in 87 autistic and 263 non-autistic adults. Together, our results indicate that object personification occurs commonly among autistic individuals, and perhaps more often (and later in life) than in the general population. Given that in many cases, autistic people report their personification experiences as distressing, it is important to consider the reasons for the increased personification and identify structures for support.
    </p>
          
        
      

      
    

    

    
      


  

  
    <p>
      
        <strong>
          Keywords:
        </strong>
      
      anthropomorphism; autism spectrum disorders; cognition (attention, learning, memory); perception; personification.
    </p>
  


    

  </div>


  
  



  <p id="disclaimer">
  <a href="https://pubmed.ncbi.nlm.nih.gov/disclaimer/" target="_blank" data-ga-category="literature_resources" data-ga-action="disclaimer_link" data-ga-label="under_abstract">PubMed Disclaimer</a>
</p>
  

  
  

  

  
  
    <div id="similar">
      <h2>
        Similar articles
      </h2>
      
        <ul id="similar-articles-list">
          
  
    
      
  
    <li>
  
  <div>
    <p><a href="https://pubmed.ncbi.nlm.nih.gov/36601321/" ref="article_id=36601321&amp;linksrc=similar_articles_link&amp;ordinalpos=1" data-ga-category="similar_article" data-ga-action="36601321" data-ga-label="">
      
        A Friendly Article: The Qualitative Investigation of Anthropomorphism in Autistic and Nonautistic Adults.
      
    </a></p><p><span>Negri O, White RC, Remington A.</span>
        
      
    
    <span>Negri O, et al.</span>
    <span>Autism Adulthood. 2019 Dec 1;1(4):286-296. doi: 10.1089/aut.2019.0027. Epub 2019 Dec 13.</span>
    <span>Autism Adulthood. 2019.</span>
  
  
  <span>PMID: <span>36601321</span></span>
  <span>Free PMC article.</span>
  
    
  
  
  
  
</p>

  </div>
  
    </li>
  


    
  

  
    
      
  
    <li>
  
  <div>
    <p><a href="https://pubmed.ncbi.nlm.nih.gov/33719765/" ref="article_id=33719765&amp;linksrc=similar_articles_link&amp;ordinalpos=2" data-ga-category="similar_article" data-ga-action="33719765" data-ga-label="">
      
        Autistic traits and loneliness in autism are associated with increased tendencies to anthropomorphise.
      
    </a></p><p><span>Caruana N, White RC, Remington A.</span>
        
      
    
    <span>Caruana N, et al.</span>
    <span>Q J Exp Psychol (Hove). 2021 Jul;74(7):1295-1304. doi: 10.1177/17470218211005694. Epub 2021 Mar 27.</span>
    <span>Q J Exp Psychol (Hove). 2021.</span>
  
  
  <span>PMID: <span>33719765</span></span>
  
  
    
  
  
  
  
</p>

  </div>
  
    </li>
  


    
  

  
    
      
  
    <li>
  
  <div>
    <p><a href="https://pubmed.ncbi.nlm.nih.gov/38923977/" ref="article_id=38923977&amp;linksrc=similar_articles_link&amp;ordinalpos=3" data-ga-category="similar_article" data-ga-action="38923977" data-ga-label="">
      
        Affective Contact in Autism: A Phenomenological Study of the Emotional Experiences of Autistic Adults.
      
    </a></p><p><span>Dallman A.</span>
        
      
    
    <span>Dallman A.</span>
    <span>Am J Occup Ther. 2024 Jul 1;78(4):7804205070. doi: 10.5014/ajot.2024.050502.</span>
    <span>Am J Occup Ther. 2024.</span>
  
  
  <span>PMID: <span>38923977</span></span>
  
  
    
  
  
  
  
</p>

  </div>
  
    </li>
  


    
  

  
    
      
  
    <li>
  
  <div>
    <p><a href="https://pubmed.ncbi.nlm.nih.gov/39080988/" ref="article_id=39080988&amp;linksrc=similar_articles_link&amp;ordinalpos=4" data-ga-category="similar_article" data-ga-action="39080988" data-ga-label="">
      
        Emotion dysregulation in autism: A meta-analysis.
      
    </a></p><p><span>McDonald RG, Cargill MI, Khawar S, Kang E.</span>
        
      
    
    <span>McDonald RG, et al.</span>
    <span>Autism. 2024 Dec;28(12):2986-3001. doi: 10.1177/13623613241257605. Epub 2024 Jul 30.</span>
    <span>Autism. 2024.</span>
  
  
  <span>PMID: <span>39080988</span></span>
  
  
    
  
  <span>Review.</span>
  
  
</p>

  </div>
  
    </li>
  


    
  

  
    
      
  
    <li>
  
  <div>
    <p><a href="https://pubmed.ncbi.nlm.nih.gov/10587883/" ref="article_id=10587883&amp;linksrc=similar_articles_link&amp;ordinalpos=5" data-ga-category="similar_article" data-ga-action="10587883" data-ga-label="">
      
        Children with autism experience problems with both objects and people.
      
    </a></p><p><span>Williams E, Costall A, Reddy V.</span>
        
      
    
    <span>Williams E, et al.</span>
    <span>J Autism Dev Disord. 1999 Oct;29(5):367-78. doi: 10.1023/a:1023026810619.</span>
    <span>J Autism Dev Disord. 1999.</span>
  
  
  <span>PMID: <span>10587883</span></span>
  
  
    
  
  <span>Review.</span>
  
  
</p>

  </div>
  
    </li>
  


    
  


        </ul>

        
          

        
      
    </div>
  


  


  
    <div id="citedby">
      <h2>
        Cited by
      </h2>
      
        <ul id="citedby-articles-list">
          
  
    
      
  
    <li>
  
  <div>
    <p><a href="https://pubmed.ncbi.nlm.nih.gov/34538099/" ref="article_id=34538099&amp;linksrc=citedby_articles_link&amp;ordinalpos=1" data-ga-category="cited_by" data-ga-action="34538099" data-ga-label="">
      
        Anthropomorphic tendencies in autism: A conceptual replication and extension of White and Remington (2019) and preliminary development of a novel anthropomorphism measure.
      
    </a></p><p><span>Clutterbuck RA, Shah P, Leung HS, Callan MJ, Gjersoe N, Livingston LA.</span>
        
      
    
    <span>Clutterbuck RA, et al.</span>
    <span>Autism. 2022 May;26(4):940-950. doi: 10.1177/13623613211039387. Epub 2021 Sep 18.</span>
    <span>Autism. 2022.</span>
  
  
  <span>PMID: <span>34538099</span></span>
  <span>Free PMC article.</span>
  
    
  
  
  
  
</p>

  </div>
  
    </li>
  


    
  

  
    
      
  
    <li>
  
  <div>
    <p><a href="https://pubmed.ncbi.nlm.nih.gov/38899122/" ref="article_id=38899122&amp;linksrc=citedby_articles_link&amp;ordinalpos=2" data-ga-category="cited_by" data-ga-action="38899122" data-ga-label="">
      
        Differential relationships between autistic traits and anthropomorphic tendencies in adults and early adolescents.
      
    </a></p><p><span>Gao RR, Si SW, Lin XX, Wang YZ, Wang N, Wang JY, Luo F.</span>
        
      
    
    <span>Gao RR, et al.</span>
    <span>Front Psychol. 2024 Jun 5;15:1281207. doi: 10.3389/fpsyg.2024.1281207. eCollection 2024.</span>
    <span>Front Psychol. 2024.</span>
  
  
  <span>PMID: <span>38899122</span></span>
  <span>Free PMC article.</span>
  
    
  
  
  
  
</p>

  </div>
  
    </li>
  


    
  

  
    
      
  
    <li>
  
  <div>
    <p><a href="https://pubmed.ncbi.nlm.nih.gov/30483176/" ref="article_id=30483176&amp;linksrc=citedby_articles_link&amp;ordinalpos=3" data-ga-category="cited_by" data-ga-action="30483176" data-ga-label="">
      
        Imagining Others' Minds: The Positive Relation Between Children's Role Play and Anthropomorphism.
      
    </a></p><p><span>Severson RL, Woodard SR.</span>
        
      
    
    <span>Severson RL, et al.</span>
    <span>Front Psychol. 2018 Nov 13;9:2140. doi: 10.3389/fpsyg.2018.02140. eCollection 2018.</span>
    <span>Front Psychol. 2018.</span>
  
  
  <span>PMID: <span>30483176</span></span>
  <span>Free PMC article.</span>
  
    
  
  
  
  
</p>

  </div>
  
    </li>
  


    
  

  
    
      
  
    <li>
  
  <div>
    <p><a href="https://pubmed.ncbi.nlm.nih.gov/38625652/" ref="article_id=38625652&amp;linksrc=citedby_articles_link&amp;ordinalpos=4" data-ga-category="cited_by" data-ga-action="38625652" data-ga-label="">
      
        Autistic traits and anthropomorphism: the case of vehicle fascia perception.
      
    </a></p><p><span>Forby L, Pazhoohi F, Kingstone A.</span>
        
      
    
    <span>Forby L, et al.</span>
    <span>Cogn Process. 2024 Aug;25(3):513-519. doi: 10.1007/s10339-024-01187-z. Epub 2024 Apr 16.</span>
    <span>Cogn Process. 2024.</span>
  
  
  <span>PMID: <span>38625652</span></span>
  
  
    
  
  
  
  
</p>

  </div>
  
    </li>
  


    
  

  
    
      
  
    <li>
  
  <div>
    <p><a href="https://pubmed.ncbi.nlm.nih.gov/39410951/" ref="article_id=39410951&amp;linksrc=citedby_articles_link&amp;ordinalpos=5" data-ga-category="cited_by" data-ga-action="39410951" data-ga-label="">
      
        Bringing the autistic lifeworld to supportive technology design: an enactive approach.
      
    </a></p><p><span>van Huizen JC, van Dijk J, Staal WG, van der Voort MC.</span>
        
      
    
    <span>van Huizen JC, et al.</span>
    <span>CoDesign. 2023 Dec 28;20(2):243-265. doi: 10.1080/15710882.2023.2295952. eCollection 2024.</span>
    <span>CoDesign. 2023.</span>
  
  
  <span>PMID: <span>39410951</span></span>
  <span>Free PMC article.</span>
  
    
  
  
  
  
</p>

  </div>
  
    </li>
  


    
  


        </ul>

        
          

        
      
    </div>
  


  
  

  
  
    <div id="mesh-terms">
      <h2>
        MeSH terms
      </h2>

      <ul><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li></ul>
    </div>
  


  

  

  

  



  
  

  



  
  <div id="linkout">
    <h2>
      LinkOut - more resources
    </h2>

    <ul><li><h3>Full Text Sources</h3><ul><li><a href="https://journals.sagepub.com/doi/10.1177/1362361318793408?url_ver=Z39.88-2003&amp;rfr_id=ori:rid:crossref.org&amp;rfr_dat=cr_pub%20%200pubmed" target="_blank" rel="noopener" ref="linksrc=linkout_link&amp;PrId=4487&amp;uid=30101594&amp;db=pubmed&amp;itool=Abstract&amp;log$=linkoutlink&amp;nlmid=9713494" data-ga-category="link_out" data-ga-action="Full Text Sources" data-ga-label="Atypon">
                    Atypon
                  </a></li></ul></li><li><h3>Other Literature Sources</h3><ul><li><a href="https://scite.ai/reports/30101594" target="_blank" rel="noopener" ref="linksrc=linkout_link&amp;PrId=10307&amp;uid=30101594&amp;db=pubmed&amp;itool=Abstract&amp;log$=linkoutlink&amp;nlmid=9713494" data-ga-category="link_out" data-ga-action="Other Literature Sources" data-ga-label="scite Smart Citations">
                    scite Smart Citations
                  </a></li></ul></li><li><h3>Medical</h3><ul><li><a href="https://medlineplus.gov/autismspectrumdisorder.html" target="_blank" rel="noopener" ref="linksrc=linkout_link&amp;PrId=10405&amp;uid=30101594&amp;db=pubmed&amp;itool=Abstract&amp;log$=linkoutlink&amp;nlmid=9713494" data-ga-category="link_out" data-ga-action="Medical" data-ga-label="MedlinePlus Consumer Health Information">
                    MedlinePlus Consumer Health Information
                  </a></li><li><a href="https://medlineplus.gov/autismspectrumdisorder.html" target="_blank" rel="noopener" ref="linksrc=linkout_link&amp;PrId=3162&amp;uid=30101594&amp;db=pubmed&amp;itool=Abstract&amp;log$=linkoutlink&amp;nlmid=9713494" data-ga-category="link_out" data-ga-action="Medical" data-ga-label="MedlinePlus Health Information">
                    MedlinePlus Health Information
                  </a></li></ul></li></ul>
  </div>


</main>

    
  


    

    

  </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Benzene at 200 (193 pts)]]></title>
            <link>https://www.chemistryworld.com/opinion/benzene-at-200/4021504.article</link>
            <guid>44290413</guid>
            <pubDate>Mon, 16 Jun 2025 15:16:47 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.chemistryworld.com/opinion/benzene-at-200/4021504.article">https://www.chemistryworld.com/opinion/benzene-at-200/4021504.article</a>, See on <a href="https://news.ycombinator.com/item?id=44290413">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><div data-attachment="541423" data-sequence="1">
<p><img alt="Benzene and bunting in chalk on a blackboard" src="https://d2cbg94ubxgsnp.cloudfront.net/Pictures/480xAny/4/2/3/541423_benzenecelebrationshutterstock_127297022copy_916610.jpg" srcset="https://d2cbg94ubxgsnp.cloudfront.net/Pictures/480xAny/4/2/3/541423_benzenecelebrationshutterstock_127297022copy_916610.jpg 480w" loading="eager" width="6449" height="4299"></p>


</div>
<p>In 1825, Michael Faraday discovered one of the most fascinating compounds in chemistry: <a title="Benzene" href="https://www.chemistryworld.com/3005709.article">benzene</a>.&nbsp;While isolating the components of oily residues of illuminating gas, Faraday identified a mysterious liquid, with a peculiar aromatic smell, which would go on to transform the landscape of chemistry.</p>
<p>Within the pages of the <em>Philosophical Transactions of the Royal Society of London</em>, Faraday <a title="On new compounds of carbon and hydrogen, and on certain other products obtained during the decomposition of oil by heat" href="https://doi.org/10.1098/rstl.1825.0022">described</a> this seemingly simple yet profoundly unique molecule. What set benzene apart, even in its earliest discovery, was its resistance to easy chemical classification. Its peculiar behaviour, such as its surprising stability despite being highly unsaturated, hinted at a deeper mystery that would not be fully resolved until the mid-19th century with the proposal of its cyclic structure.</p>
<p>Benzene’s physical properties only added to its mystique. This colourless liquid emitted a faintly sweet, intoxicating aroma – a hallmark of aromatic compounds. With a boiling point of 80.1°C, it was volatile and highly flammable, making it both a chemical curiosity and a potential industrial tool. Early chemists were captivated by its ability to dissolve fats, oils and other nonpolar substances, which made it a valuable solvent for experimentation and industrial processes. Yet, it was benzene’s chemical properties – its reactivity and stability – that would become the cornerstone of an entire branch of organic chemistry: aromatic compounds.</p>
<p>Today, benzene is everywhere, interwoven into the structures of more complex molecules that enhance our daily lives in fields as diverse as health, energy, advanced materials, electronics, food, dyes and biotechnology. This humble molecule opened the doors to a vast universe of aromatic compounds and an endless array of applications that have redefined our world.</p>
<h3 id="Stability_and_tunability">Stability and tunability</h3>
<p>Following benzene’s legacy came polycyclic aromatic hydrocarbons (PAHs), a fascinating class of organic molecules composed of fused benzene rings. These structures not only preserve benzene’s aromatic stability, thanks to their electron delocalisation, but also exhibit unique electronic and optical properties determined by their size and arrangement. While smaller PAHs, like <a title="Naphthalene" href="https://www.chemistryworld.com/7362.article">naphthalene</a> and anthracene, had been characterised in the 19th century, the discovery of larger, more complex systems unveiled entirely new and surprising properties – from discrete energy levels in simpler molecules to semiconducting behaviours in larger systems like pentacene.</p>
<p>The synthesis and study of these compounds paved the way for nanographenes, opening new dimensions in chemistry and materials science. Through meticulous control over their molecular structures, researchers have learned to design advanced materials with tunable properties, such as electron conductivity, fluorescence, chirality and chemical reactivity. This painstaking precision highlights the intrinsic beauty of chemistry at its most fundamental level, an art of exactitude that continues to push the boundaries of possibility.</p>
<p>A landmark achievement in this journey was the discovery of hexabenzocoronene (HBC), in 1958. This molecule, composed of 42 carbon atoms forming 13 hexagonal rings in a perfectly flat structure, remained the largest fully characterised polycyclic aromatic hydrocarbon for decades. Yet the creativity of organic chemistry knows no bounds. Klaus Müllen, a pioneer in the exploration of nanographenes, succeeded in 2002 in synthesising a remarkable structure formed by 222 carbon atoms with a diameter of 3nm, demonstrating the immense potential of organic synthesis to construct tailor-made graphene molecules.</p>
<blockquote>
<p>Graphene stands out as the ultimate expression of benzene’s versatility</p>
</blockquote>
<p>The fusion of benzene rings has given rise to some of the most remarkable materials in modern science, including fullerenes and carbon nanotubes. Fullerenes, often referred to as <a title="Buckminsterfullerene" href="https://www.chemistryworld.com/3005718.article">buckyballs</a>, are spherical molecules composed entirely of carbon atoms arranged in a pattern of hexagons and pentagons, resembling a molecular soccer ball. These structures, discovered in 1985, owe their stability and symmetry to the aromaticity derived from benzene-like rings. Similarly, carbon nanotubes – long, cylindrical structures composed of fused aromatic rings – have captivated scientists with their extraordinary strength, flexibility and electrical conductivity. Both fullerenes and nanotubes exemplify the limitless potential of carbon chemistry, with benzene as the foundational building block.</p>
<p>Among these innovations, graphene stands out as the ultimate expression of benzene’s versatility. This two-dimensional material, consisting of a single layer of carbon atoms arranged in a honeycomb lattice, is essentially a sheet of fused benzene rings. Graphene’s remarkable properties – its transparency, strength, flexibility and electrical conductivity – have earned it the title of a ‘gift of gods’. Graphene, like benzene before it, has the power to revolutionise multiple fields, from electronics and energy storage to medicine and materials science.</p>
<p>Beyond its scientific impact, benzene holds a special place in education. Generations of high school and university students have been introduced to the elegance of its structure and the profound mystery surrounding its stability. The study of benzene serves as an accessible entry point for understanding broader concepts like aromaticity, resonance and molecular orbitals. By celebrating the bicentennial of its discovery, we honour not only the legacy of Faraday but also the enduring role of benzene in inspiring curiosity, innovation and the next generation of chemists.</p>
<div>
<p>To celebrate the 200th anniversary of benzene’s discovery and its extraordinary legacy, the Royal Society of Chemistry will release a thematic special issue, uniting several RSC journals in a collaborative tribute to benzene’s unparalleled influence.</p>
<p>This special issue, edited by Ben Feringa and Nazario Martín, will illuminate the enduring relevance of benzene by exploring its far-reaching legacy in carbon-based systems, from the fundamental concepts of aromaticity and antiaromaticity to groundbreaking research on polycyclic aromatic hydrocarbons (PAHs), molecular nanographenes (via both top-down and bottom-up approaches), graphene and its derivatives, carbon nanotubes and fullerenes. It will also delve into cutting-edge developments from (anti)aromatic compounds synthesised through on-surface methodologies to benzene-based molecular machines.</p>
</div></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[OpenTelemetry for Go: Measuring overhead costs (107 pts)]]></title>
            <link>https://coroot.com/blog/opentelemetry-for-go-measuring-the-overhead/</link>
            <guid>44290331</guid>
            <pubDate>Mon, 16 Jun 2025 15:09:17 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://coroot.com/blog/opentelemetry-for-go-measuring-the-overhead/">https://coroot.com/blog/opentelemetry-for-go-measuring-the-overhead/</a>, See on <a href="https://news.ycombinator.com/item?id=44290331">Hacker News</a></p>
<div id="readability-page-1" class="page"><div itemprop="text"><p>Everything comes at a cost — and observability is no exception. When we add metrics, logging, or distributed tracing to our applications, it helps us understand what’s going on with performance and key UX metrics like success rate and latency. But what’s the cost?</p>
<p>I’m not talking about the price of observability tools here, I mean the instrumentation overhead. If an application logs or traces everything it does, that’s bound to slow it down or at least increase resource consumption. Of course, that doesn’t mean we should give up on observability. But it does mean we should measure the overhead so we can make informed tradeoffs.</p>
<p>These days, when people talk about instrumenting applications, in 99% of cases they mean OpenTelemetry. OpenTelemetry is a vendor-neutral open source framework for collecting telemetry data from your app such as metrics, logs, and traces. It’s quickly become the industry standard.</p>
<p>In this post, I want to measure the overhead of using OpenTelemetry in a Go application. To do that, I’ll use a super simple Go HTTP server that increments a counter in an in-memory database <a href="https://valkey.io/">Valkey</a> (a Redis fork) on every request. The idea behind the benchmark is straightforward:</p>
<ul>
<li>First, we’ll run the app under load without any instrumentation and measure its performance and resource usage.</li>
<li>Then, using the exact same workload, we’ll repeat the test with OpenTelemetry SDK for Go enabled and compare the results.</li>
</ul>
<h2>Test setup</h2>
<p>For this benchmark, I’ll use four Linux nodes, each with 4 vCPUs and 8GB of RAM. One will run the application, another will host Valkey, a third will be used for the load generator, and the fourth for observability (using Coroot Community Edition).</p>
<p><img fetchpriority="high" decoding="async" src="https://coroot.com/wp-content/uploads/2025/06/Screenshot-2025-06-12-at-18.20.16.png" alt="" width="3164" height="220" srcset="https://coroot.com/wp-content/uploads/2025/06/Screenshot-2025-06-12-at-18.20.16.png 3164w, https://coroot.com/wp-content/uploads/2025/06/Screenshot-2025-06-12-at-18.20.16-300x21.png 300w, https://coroot.com/wp-content/uploads/2025/06/Screenshot-2025-06-12-at-18.20.16-1024x71.png 1024w" sizes="(max-width: 3164px) 100vw, 3164px"></p>
<p>I want to make sure the components involved in the test don’t interfere with each other, so I’m running them on separate nodes. This time, I’m not using Kubernetes, instead, I’ll run everything in plain Docker containers. I’m also using the host network mode for all containers, to avoid <em>docker-proxy</em> introducing any additional latency into the network path.</p>
<p>Now, let’s take a look at the application code:</p>
<pre><code>package main

import (
	"context"
	"log"
	"net/http"
	"os"
	"strconv"

	"github.com/go-redis/redis/extra/redisotel"
	"github.com/go-redis/redis/v8"

	"go.opentelemetry.io/contrib/instrumentation/net/http/otelhttp"
	"go.opentelemetry.io/otel"
	"go.opentelemetry.io/otel/exporters/otlp/otlptrace"
	"go.opentelemetry.io/otel/exporters/otlp/otlptrace/otlptracehttp"
	"go.opentelemetry.io/otel/propagation"
	"go.opentelemetry.io/otel/sdk/trace"
)

var (
	rdb *redis.Client
)

func initTracing() {
	rdb.AddHook(redisotel.TracingHook{})
	client := otlptracehttp.NewClient()
	exporter, err := otlptrace.New(context.Background(), client)
	if err != nil {
		log.Fatal(err)
	}
	tracerProvider := trace.NewTracerProvider(trace.WithBatcher(exporter))
	otel.SetTracerProvider(tracerProvider)
	otel.SetTextMapPropagator(propagation.NewCompositeTextMapPropagator(
		propagation.TraceContext{},
		propagation.Baggage{},
	))
}

func handler(w http.ResponseWriter, r *http.Request) {
	cmd := rdb.Incr(r.Context(), "counter")
	if err := cmd.Err(); err != nil {
		http.Error(w, err.Error(), http.StatusInternalServerError)
		return
	}
	_, _ = w.Write([]byte(strconv.FormatInt(cmd.Val(), 10)))
}

func main() {
	rdb = redis.NewClient(&amp;redis.Options{Addr: os.Getenv("REDIS_SERVER")})
	h := http.Handler(http.HandlerFunc(handler))
	if os.Getenv("ENABLE_OTEL") != "" {
		log.Println("enabling opentelemetry")
		initTracing()
		h = otelhttp.NewHandler(http.HandlerFunc(handler), "GET /")
	}
	log.Fatal(http.ListenAndServe(":8080", h))
}
</code></pre>
<p>By default, the application runs without instrumentation. Only if the environment variable ENABLE_OTEL is set, the OpenTelemetry SDK will be initialized. So runs without this variable will serve as the baseline for comparison.</p>
<h2>Running the Benchmark</h2>
<p>Now let’s start all the components and begin testing.</p>
<p>First, we launch Valkey using the following command:</p>
<pre><code>docker run --name valkey -d --net=host valkey/valkey</code></pre>
<p>Next, we start the Go app and point it to the Valkey instance by IP:</p>
<pre><code>docker run -d --name app -e REDIS_SERVER="192.168.1.2:6379" --net=host failurepedia/redis-app:0.5</code></pre>
<p>To generate load, I’ll use <a href="https://github.com/giltene/wrk2">wrk2</a>, which allows precise control over request rate. In this test, I’m setting it to 10,000 requests per second using 100 connections and 8 threads. Each run will last 20 minutes:</p>
<pre><code>docker run --rm --name load-generator -ti cylab/wrk2 \
   -t8 -c100 -d1200s -R10000 --u_latency http://192.168.1.3:8080/
</code></pre>
<h2>Results</h2>
<p>Let’s take a look at the results.</p>
<p>We started by running the app without any instrumentation. This serves as our baseline for performance and resource usage. Based on metrics gathered by Coroot using eBPF, the app successfully handled 10,000 requests per second. The majority of requests were served in under 5 milliseconds. The 95th percentile (p95) latency was around 5ms, the 99th percentile (p99) was about 10ms, with occasional spikes reaching up to 20ms.</p>
<p><img decoding="async" src="https://coroot.com/wp-content/uploads/2025/06/Screenshot-2025-06-13-at-12.24.29.png" alt="" width="1586" height="922" srcset="https://coroot.com/wp-content/uploads/2025/06/Screenshot-2025-06-13-at-12.24.29.png 1586w, https://coroot.com/wp-content/uploads/2025/06/Screenshot-2025-06-13-at-12.24.29-300x174.png 300w, https://coroot.com/wp-content/uploads/2025/06/Screenshot-2025-06-13-at-12.24.29-1024x595.png 1024w" sizes="(max-width: 1586px) 100vw, 1586px"></p>
<p>CPU usage was steady at around 2 CPU cores (or 2 CPU seconds per second), and memory consumption stayed low at roughly 10 MB.</p>
<p><img loading="lazy" decoding="async" src="https://coroot.com/wp-content/uploads/2025/06/Screenshot-2025-06-13-at-12.37.09.png" alt="" width="1544" height="416" srcset="https://coroot.com/wp-content/uploads/2025/06/Screenshot-2025-06-13-at-12.37.09.png 1544w, https://coroot.com/wp-content/uploads/2025/06/Screenshot-2025-06-13-at-12.37.09-300x81.png 300w, https://coroot.com/wp-content/uploads/2025/06/Screenshot-2025-06-13-at-12.37.09-1024x276.png 1024w" sizes="auto, (max-width: 1544px) 100vw, 1544px"></p>
<p><img loading="lazy" decoding="async" src="https://coroot.com/wp-content/uploads/2025/06/Screenshot-2025-06-13-at-12.37.22.png" alt="" width="1544" height="404" srcset="https://coroot.com/wp-content/uploads/2025/06/Screenshot-2025-06-13-at-12.37.22.png 1544w, https://coroot.com/wp-content/uploads/2025/06/Screenshot-2025-06-13-at-12.37.22-300x78.png 300w, https://coroot.com/wp-content/uploads/2025/06/Screenshot-2025-06-13-at-12.37.22-1024x268.png 1024w" sizes="auto, (max-width: 1544px) 100vw, 1544px"></p>
<p>So that’s our baseline. Now, let’s restart the app container with the OpenTelemetry SDK enabled and see how things change:</p>
<pre><code>docker run -d --name app \
  -e REDIS_SERVER="192.168.1.2:6379" \
  -e ENABLE_OTEL=1 \
  -e OTEL_SERVICE_NAME="app" \
  -e OTEL_EXPORTER_OTLP_TRACES_ENDPOINT="http://192.168.1.4:8080/v1/traces" \
  --net=host failurepedia/redis-app:0.5</code></pre>
<p data-start="1069" data-end="1166">Everything else stayed the same – the infrastructure, the workload, and the duration of the test.</p>
<p data-start="1168" data-end="1202">Now let’s break down what changed.</p>
<p data-start="1168" data-end="1202"><img loading="lazy" decoding="async" src="https://coroot.com/wp-content/uploads/2025/06/Screenshot-2025-06-13-at-14.29.16.png" alt="" width="1584" height="428" srcset="https://coroot.com/wp-content/uploads/2025/06/Screenshot-2025-06-13-at-14.29.16.png 1584w, https://coroot.com/wp-content/uploads/2025/06/Screenshot-2025-06-13-at-14.29.16-300x81.png 300w, https://coroot.com/wp-content/uploads/2025/06/Screenshot-2025-06-13-at-14.29.16-1024x277.png 1024w" sizes="auto, (max-width: 1584px) 100vw, 1584px"></p>
<p>Memory usage increased from around 10 megabytes to somewhere between 15 and 18 megabytes. This additional overhead comes from the SDK and its background processes for handling telemetry data. While there is a clear difference, it doesn’t look like a significant increase in absolute terms, especially for modern applications where memory budgets are typically much larger.</p>
<p><img loading="lazy" decoding="async" src="https://coroot.com/wp-content/uploads/2025/06/Screenshot-2025-06-13-at-14.28.59.png" alt="" width="1560" height="438" srcset="https://coroot.com/wp-content/uploads/2025/06/Screenshot-2025-06-13-at-14.28.59.png 1560w, https://coroot.com/wp-content/uploads/2025/06/Screenshot-2025-06-13-at-14.28.59-300x84.png 300w, https://coroot.com/wp-content/uploads/2025/06/Screenshot-2025-06-13-at-14.28.59-1024x288.png 1024w" sizes="auto, (max-width: 1560px) 100vw, 1560px"></p>
<p>CPU usage jumped from 2 cores to roughly 2.7 cores. That’s about a 35 percent increase. This is expected since the app is now tracing every request, preparing and exporting spans, and doing more work in the background.</p>
<p>To understand exactly where this additional CPU usage was coming from, I used Coroot’s built-in eBPF-based CPU profiler to capture and compare profiles before and after enabling OpenTelemetry.</p>
<p><img loading="lazy" decoding="async" src="https://coroot.com/wp-content/uploads/2025/06/Screenshot-2025-06-13-at-14.53.14.png" alt="" width="3156" height="1680" srcset="https://coroot.com/wp-content/uploads/2025/06/Screenshot-2025-06-13-at-14.53.14.png 3156w, https://coroot.com/wp-content/uploads/2025/06/Screenshot-2025-06-13-at-14.53.14-300x160.png 300w, https://coroot.com/wp-content/uploads/2025/06/Screenshot-2025-06-13-at-14.53.14-1024x545.png 1024w" sizes="auto, (max-width: 3156px) 100vw, 3156px"></p>
<p>The profiler showed that about 10 percent of total CPU time was spent in <em>go.opentelemetry.io/otel/sdk/trace.NewBatchSpanProcessor</em>, which handles span batching and export. Redis calls also got slightly more expensive — tracing added around 7 percent CPU overhead to go-redis operations. The rest of the increase came from instrumented HTTP handlers and middleware.</p>
<p>In short, the overhead comes from OpenTelemetry’s span processing pipeline, not from the app’s core logic.</p>
<p><img loading="lazy" decoding="async" src="https://coroot.com/wp-content/uploads/2025/06/Screenshot-2025-06-13-at-12.41.57.png" alt="" width="1552" height="914" srcset="https://coroot.com/wp-content/uploads/2025/06/Screenshot-2025-06-13-at-12.41.57.png 1552w, https://coroot.com/wp-content/uploads/2025/06/Screenshot-2025-06-13-at-12.41.57-300x177.png 300w, https://coroot.com/wp-content/uploads/2025/06/Screenshot-2025-06-13-at-12.41.57-1024x603.png 1024w" sizes="auto, (max-width: 1552px) 100vw, 1552px"></p>
<p data-start="1376" data-end="1510">Latency also changed, though not dramatically. With OpenTelemetry enabled, more requests fell into the 5 to 10 millisecond range. The 99th percentile latency went from 10 to about 15 milliseconds. Throughput remained stable at around 10,000 requests per second. We didn’t see any errors or timeouts.</p>
<p data-start="1376" data-end="1510"><img loading="lazy" decoding="async" src="https://coroot.com/wp-content/uploads/2025/06/Screenshot-2025-06-13-at-14.45.49.png" alt="" width="1546" height="410" srcset="https://coroot.com/wp-content/uploads/2025/06/Screenshot-2025-06-13-at-14.45.49.png 1546w, https://coroot.com/wp-content/uploads/2025/06/Screenshot-2025-06-13-at-14.45.49-300x80.png 300w, https://coroot.com/wp-content/uploads/2025/06/Screenshot-2025-06-13-at-14.45.49-1024x272.png 1024w" sizes="auto, (max-width: 1546px) 100vw, 1546px"></p>
<p data-start="1376" data-end="1510">Network traffic also increased. With tracing enabled, the app started exporting telemetry data to Coroot, which resulted in an outbound traffic volume of about 4 megabytes per second, or roughly 32 megabits per second. For high-throughput services or environments with strict network constraints, this is something to keep in mind when enabling full request-level tracing.</p>
<p>Overall, enabling OpenTelemetry introduced a noticeable but controlled overhead.&nbsp;These numbers aren’t negligible, especially at scale — but they’re also not a dealbreaker. For most teams, the visibility gained through distributed tracing and the ability to troubleshoot issues faster will justify the tradeoff.</p>
<h2>eBPF-based instrumentation</h2>
<p>I often hear from engineers, especially in ad tech and other high-throughput environments, that they simply can’t afford the overhead of distributed tracing. At the same time, observability is absolutely critical for them. This is exactly the kind of scenario where eBPF-based instrumentation fits well.</p>
<p>Instead of modifying application code or adding SDKs, an agent can observe application behavior at the kernel level using eBPF. Coroot’s agent supports this approach and is capable of collecting both metrics and traces using eBPF, without requiring any changes to the application itself.</p>
<p>However, in high-load environments like the one used in this benchmark, we generally recommend disabling eBPF-based tracing and working with metrics only. Metrics still allow us to clearly see how services interact with each other, without storing data about every single request. They’re also much more efficient in terms of storage and runtime overhead.</p>
<p>Throughout both runs of our test, Coroot’s agent was running on each node. Here’s what its CPU usage looked like:</p>
<p><img loading="lazy" decoding="async" src="https://coroot.com/wp-content/uploads/2025/06/Screenshot-2025-06-13-at-15.12.03.png" alt="" width="1562" height="448" srcset="https://coroot.com/wp-content/uploads/2025/06/Screenshot-2025-06-13-at-15.12.03.png 1562w, https://coroot.com/wp-content/uploads/2025/06/Screenshot-2025-06-13-at-15.12.03-300x86.png 300w, https://coroot.com/wp-content/uploads/2025/06/Screenshot-2025-06-13-at-15.12.03-1024x294.png 1024w" sizes="auto, (max-width: 1562px) 100vw, 1562px"></p>
<p data-start="1287" data-end="1572"><em>Node201</em> was running Valkey, <em>node203</em> was running the app, and <em>node204</em> was the load generator. As the chart shows, even under consistent load, the agent’s CPU usage stayed under 0.3 cores. That makes it lightweight enough for production use, especially when working in metrics-only mode.</p>
<p data-start="1574" data-end="1650">This approach offers a practical balance: good visibility with minimal cost.</p>
<h2>Final Thoughts</h2>
<p>Observability comes at a cost, but as this experiment shows, that cost depends heavily on how you choose to implement it.</p>
<p>OpenTelemetry SDKs provide detailed traces and deep visibility, but they also introduce measurable overhead in terms of CPU, memory, and network traffic. For many teams, especially when fast incident resolution is a priority, that tradeoff is entirely justified.</p>
<p>At the same time, eBPF-based instrumentation offers a more lightweight option. It allows you to collect meaningful metrics without modifying application code and keeps resource usage minimal, especially when tracing is disabled and only metrics are collected.</p>
<p>The right choice depends on your goals. If you need full traceability and detailed diagnostics, SDK-based tracing is a strong option. If your priority is low overhead and broad system visibility, eBPF-based metrics might be the better fit.</p>
<p>Observability isn’t free, but with the right approach, it can be both effective and efficient.</p>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Salesforce study finds LLM agents flunk CRM and confidentiality tests (136 pts)]]></title>
            <link>https://www.theregister.com/2025/06/16/salesforce_llm_agents_benchmark/</link>
            <guid>44289554</guid>
            <pubDate>Mon, 16 Jun 2025 13:59:18 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.theregister.com/2025/06/16/salesforce_llm_agents_benchmark/">https://www.theregister.com/2025/06/16/salesforce_llm_agents_benchmark/</a>, See on <a href="https://news.ycombinator.com/item?id=44289554">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="body">
<p>A new benchmark developed by academics shows that LLM-based AI agents perform below par on standard CRM tests and fail to understand the need for customer confidentiality.</p>
<p>A team led by Kung-Hsiang Huang, a Salesforce AI researcher, showed that using a new benchmark relying on synthetic data, LLM agents achieve around a 58 percent success rate on tasks that can be completed in a single step without needing follow-up actions or more information.</p>
<p>Using the benchmark tool CRMArena-Pro, the team also showed performance of LLM agents drops to 35 percent when a task requires multiple steps.</p>

    

<p>Another cause for concern is highlighted in the LLM agents' handling of confidential information. "Agents demonstrate low confidentiality awareness, which, while improvable through targeted prompting, often negatively impacts task performance," a <a target="_blank" href="https://arxiv.org/pdf/2505.18878">paper published at the end of last month said</a>.</p>

        


        

<p>The Salesforce AI Research team argued that existing benchmarks failed to rigorously measure the capabilities or limitations of AI agents, and largely ignored an assessment of their ability to recognize sensitive information and adhere to appropriate data handling protocols.</p>
<ul>

<li><a href="https://www.theregister.com/2025/06/16/bt_chief_says_ai_could_cut_more_staff/">BT chief says AI could deliver more job cuts, hints at Openreach sell-off</a></li>

<li><a href="https://www.theregister.com/2025/06/16/opinion_column_lrm/">Put Large Reasoning Models under pressure and they stop making sense, say boffins</a></li>

<li><a href="https://www.theregister.com/2025/06/15/ai_model_collapse_pollution/">The launch of ChatGPT polluted the world forever, like the first atomic weapons tests</a></li>

<li><a href="https://www.theregister.com/2025/06/13/cloud_costs_ai_inferencing/">Enterprise AI adoption stalls as inferencing costs confound cloud customers</a></li>
</ul>
<p>The research unit's CRMArena-Pro tool is fed a data pipeline of realistic synthetic data to populate a Salesforce organization, which serves as the sandbox environment. The agent takes user queries and decides between an API call or a response to the users to get more clarification or provide answers.</p>
<p>"These findings suggest a significant gap between current LLM capabilities and the multifaceted demands of real-world enterprise scenarios," the paper said.</p>
<p>The findings might worry both developers and users of LLM-powered AI agents. Salesforce co-founder and CEO Marc Benioff told investors last year that AI agents represented "<a target="_blank" href="https://www.theregister.com/2024/08/29/salesforce_pricing_per_ai_conversation/">a very high margin opportunity</a>" for the SaaS CRM vendor as it takes a share in efficiency savings accrued by customers using AI agents to help get more work out of each employee.</p>

        

<p>Elsewhere, the UK government has said it would <a target="_blank" href="https://www.theregister.com/2025/06/12/nhs_tech_spending_review/">target savings of £13.8 billion ($18.7 billion) by 2029</a> with a digitization and efficiency drive that relies, in part, on the adoption of AI agents.</p>
<p>AI agents might well be useful, however, organizations should be wary of banking on any benefits before they are proven. ®</p>                                


                    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[WhatsApp introduces ads in its app (306 pts)]]></title>
            <link>https://www.nytimes.com/2025/06/16/technology/whatsapp-ads.html</link>
            <guid>44289412</guid>
            <pubDate>Mon, 16 Jun 2025 13:38:59 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.nytimes.com/2025/06/16/technology/whatsapp-ads.html">https://www.nytimes.com/2025/06/16/technology/whatsapp-ads.html</a>, See on <a href="https://news.ycombinator.com/item?id=44289412">Hacker News</a></p>
Couldn't get https://www.nytimes.com/2025/06/16/technology/whatsapp-ads.html: Error: Request failed with status code 403]]></description>
        </item>
        <item>
            <title><![CDATA[Working on databases from prison (734 pts)]]></title>
            <link>https://turso.tech/blog/working-on-databases-from-prison</link>
            <guid>44288937</guid>
            <pubDate>Mon, 16 Jun 2025 12:32:02 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://turso.tech/blog/working-on-databases-from-prison">https://turso.tech/blog/working-on-databases-from-prison</a>, See on <a href="https://news.ycombinator.com/item?id=44288937">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>I'm very excited to announce that I have recently joined Turso as a software engineer. For many in the field, including myself, getting to work on databases and solve unique challenges with such a talented team would be a dream job, but it is that much more special to me because of my unusual and unlikely circumstances. As difficult as it might be to believe, I am currently incarcerated and I landed this job from my cell in state prison. If you don’t know me, let me tell you more about how I got here.</p>
<h2 id="how-i-got-here"><a href="#how-i-got-here">#</a>How I got here</h2>
<p>Nearly two years have passed since I published <a href="https://pthorpe92.dev/intro/my-story">How I got here</a> to my blog. That post was my first real contact with the outside world in years, as I'd been off all social media and the internet since 2017. The response and support I would receive from the tech community caught me completely off guard.</p>
<p>A brief summary is that I'm currently serving prison time for poor decisions and lifestyle choices I made in my twenties, all related to drugs. Three years ago, I enrolled in a prison college program that came with the unique opportunity to access a computer with limited internet access. This immediately reignited a teenage love for programming and a lightbulb immediately lit up: that this would be my way out of the mess I had gotten myself into over the past 15 years. I quickly outgrew the curriculum, preferring instead to spend ~15+ hours a day on projects and open source contributions.</p>
<p>Through fortunate timing and lots of hard work, I was selected to be one of the first participants in the Maine Dept of Correction’s remote work program, where residents who meet certain requirements are allowed to seek out remote employment opportunities. I landed a software engineering job at a startup called <a href="https://unlockedlabs.org/">Unlocked Labs</a> building education solutions for incarcerated learners, while contributing to open source on the side. After just a year, I was leading their development team.</p>
<h3 id="finding-turso--hacking-on-project-limbo"><a href="#finding-turso--hacking-on-project-limbo">#</a>Finding Turso: hacking on project Limbo</h3>
<p>Last December I was between side-projects and browsing Hacker News when I discovered Project Limbo, an effort by Turso to rewrite SQLite from scratch. I'd never worked on relational databases, but some experience with a cache had recently sparked an interest in storage engines. Luckily for me I saw that the project was fairly young with plenty of low hanging fruit to cut my teeth on.</p>
<p>To put this entirely into perspective for some of you may be difficult, but in prison there isn’t exactly a whole lot to do and programming <em>absolutely</em> consumes my life. I either write code or manage Kubernetes clusters or other infrastructure for about 90 hours a week, and my only entertainment is a daily hour of tech/programming YouTube; mostly consisting of The Primeagen, whose story was a huge inspiration to me early on.</p>
<p>Through Prime, I had known about Turso since the beginning and had watched several interviews with Glauber and Pekka discussing their Linux kernel backgrounds and talking about the concept of distributed, multi-tenant SQLite. These were folks I'd looked up to for years and definitely could not have imagined that I would eventually be in any position to be contributing meaningfully to such an ambitious project of theirs. So needless to say, for those first PR's, just the thought of a kernel maintainer reviewing my code had made me quite nervous.</p>
<p>Helping build Limbo quickly became my new obsession. I split my time between my job and diving deep into SQLite source code, academic papers on database internals, and Andy Pavlo's CMU lectures. I was active on the <a href="https://discord.gg/turso">Turso Discord</a> but I don't think I considered whether anyone was aware that one of the top contributors was doing so from a prison cell. My story and information are linked on my GitHub, but it's subtle enough where you could miss it if you didn't read the whole profile. A couple months later, I got a Discord message from Glauber introducing himself and asking if we could meet.</p>
<p>In January, Glauber's <a href="https://x.com/glcst/status/1879553564177055855">tweet</a> about our interaction caught the attention of The Primeagen, and he ended up <a href="https://www.youtube.com/watch?v=XPyKbPLGzFI">reading my blog post</a> on his stream, bringing a whole lot of new attention to it.</p>
<p>To this day I receive semi-regular emails either from developers, college kids or others who maybe have either gone through addiction or similar circumstances, or just want to reach out for advice on how to best start contributing to open source or optimize their learning path.</p>
<h3 id="what-s-next"><a href="#what-s-next">#</a>What's Next</h3>
<p>I'm incredibly proud to be an example to others of how far hard work, determination and discipline will get you, and will be forever grateful for the opportunities given to me by the Maine Dept of Corrections to even be able to work hard in the first place, and to Unlocked Labs for giving me a chance and hiring me at a time when most assuredly no-one else would.</p>
<p>I'm also incredibly proud to announce that I am now working for Turso full time, something I would never have dreamed would be possible just a few years ago, I'm very excited to be a part of the team and to get to help build the modern evolution of SQLite.</p>
<p>Although some recent bad news from the court means that I won't be coming home as early as my family and I had hoped, my only choice is to view this as a blessing and for the next 10 months, will instead just be able to continue to dedicate time and focus to advancing my career at such a level that just wouldn't be possible otherwise.</p>
<p>Thank you to everyone who has taken the time to reach out over the past couple years, to my team at Unlocked Labs, and especially my parents. Thanks to Turso for the opportunity and to all the other companies with fair chance hiring policies who believe that people deserve a second chance. This journey has been totally surreal and every day I am still in awe of how far my life has come from the life I lived even just a few years ago.</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Snorting the AGI with Claude Code (251 pts)]]></title>
            <link>https://kadekillary.work/blog/#2025-06-16-snorting-the-agi-with-claude-code</link>
            <guid>44288377</guid>
            <pubDate>Mon, 16 Jun 2025 11:01:32 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://kadekillary.work/blog/#2025-06-16-snorting-the-agi-with-claude-code">https://kadekillary.work/blog/#2025-06-16-snorting-the-agi-with-claude-code</a>, See on <a href="https://news.ycombinator.com/item?id=44288377">Hacker News</a></p>
<div id="readability-page-1" class="page">
    <!-- terminal header -->
    <div>
        <p>kade@localhost:~$</p>
        <p><span>█</span>
      </p></div>

    <div>
      <header>
        <div>
          <p><a id="nav-link" href="https://kadekillary.work/">← home</a>
        </p></div>
      </header>

      <main>
        

        
      </main>
    </div>

    
    
    
  

</div>]]></description>
        </item>
        <item>
            <title><![CDATA[Tesla blows past stopped school bus and hits kid-sized dummies in FSD tests (139 pts)]]></title>
            <link>https://www.engadget.com/transportation/tesla-blows-past-stopped-school-bus-and-hits-kid-sized-dummies-in-full-self-driving-tests-183756251.html</link>
            <guid>44288000</guid>
            <pubDate>Mon, 16 Jun 2025 09:58:01 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.engadget.com/transportation/tesla-blows-past-stopped-school-bus-and-hits-kid-sized-dummies-in-full-self-driving-tests-183756251.html">https://www.engadget.com/transportation/tesla-blows-past-stopped-school-bus-and-hits-kid-sized-dummies-in-full-self-driving-tests-183756251.html</a>, See on <a href="https://news.ycombinator.com/item?id=44288000">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>A revealing demonstration with Tesla's Full Self-Driving mode is raising concerns about whether fully autonomous cars are ready to hit the streets. Tesla has <a data-i13n="cpos:1;pos:1" href="https://www.bloomberg.com/news/articles/2025-05-28/tesla-targets-june-12-launch-of-robotaxi-service-in-austin?sref=10lNAhZ9&amp;embedded-checkout=true" rel="nofollow noopener" target="_blank" data-ylk="slk:reportedly pushed back;cpos:1;pos:1;elm:context_link;itc:0;sec:content-canvas">reportedly pushed back</a> the rollout of its upcoming all-electric, fully autonomous car called the Cybercab, while a recent demonstration in Austin, Texas showed a Tesla Model Y running through a school bus' flashing lights and stop signs, and hitting child-size mannequins. The tests were conducted by The Dawn Project, along with Tesla Takedown and ResistAustin, and showed Tesla's Full Self-Driving software repeating the same mistake eight times.</p><p>It's worth noting that Tesla's autonomous driving feature is formally known as Full Self-Driving (Supervised) and "requires a fully attentive driver and will display a series of escalating warnings requiring driver response." Tesla even has a warning that says, "failure to follow these instructions could cause damage, serious injury or death." However, it's not the first time that Tesla's FSD software has found itself in hot water. The Dawn Project, whose founder Dan O'Dowd is the CEO of a company that offers competing automated driving system software, previously took out ads warning about the dangers of Tesla's Full Self-Driving and how it would fail to yield around school buses. In April 2024, a Model S using Full Self-Driving was involved in a <a data-i13n="cpos:2;pos:1" href="https://www.engadget.com/tesla-involved-in-fatal-washington-crash-was-using-self-driving-mode-170706606.html" data-ylk="slk:crash in Washington;cpos:2;pos:1;elm:context_link;itc:0;sec:content-canvas">crash in Washington</a>, where a motorcyclist died.</p><p>With anticipation building up for an <a data-i13n="cpos:3;pos:1" href="https://www.engadget.com/transportation/evs/tesla-will-start-offering-public-robotaxi-rides-in-austin-on-june-22-says-elon-musk-161801916.html" data-ylk="slk:eventual Cybercab rollout;cpos:3;pos:1;elm:context_link;itc:0;sec:content-canvas">eventual Cybercab rollout</a> on June 22, the company's CEO posted some <a data-i13n="elm:context_link;elmt:doNotAffiliate;cpos:4;pos:1" href="https://x.com/elonmusk/status/1932591896939147494" rel="nofollow noopener" target="_blank" data-ylk="slk:additional details on X;elm:context_link;elmt:doNotAffiliate;cpos:4;pos:1;itc:0;sec:content-canvas">additional details on X</a>. According to Elon Musk, Tesla is "being super paranoid about safety, so the date could shift." Beyond that, Musk also posted that the "first Tesla that drives itself from factory end of line all the way to a customer house is June 28."</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Start your own Internet Resiliency Club (544 pts)]]></title>
            <link>https://bowshock.nl/irc/</link>
            <guid>44287395</guid>
            <pubDate>Mon, 16 Jun 2025 07:38:17 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://bowshock.nl/irc/">https://bowshock.nl/irc/</a>, See on <a href="https://news.ycombinator.com/item?id=44287395">Hacker News</a></p>
<div id="readability-page-1" class="page"><div class="page">

            

            <section>
    <header>
        
    </header>
    <div>
        <p>Thanks to war, geopolitics, and climate change, Europe will have more
frequent and more severe <a href="https://nltimes.nl/2025/05/09/russia-preparing-war-europe-dutch-pm-activities-finnish-border">internet
disruptions</a>
in the very near future. Governments and businesses need to prepare
for catastrophic loss of communications. Unfortunately, the necessary
changes are risky and expensive, which means they won’t do it until a
<a href="https://layeraleph.com/">crisis</a> is already here. However, small
groups of volunteers with a little bit of time and money can provide
crucial initial leadership to bootstrap recovery.</p>
<p>An Internet Resiliency Club is a group of internet experts who can
communicate with each other across a few kilometers without any
centralized infrastructure using cheap, low-power, unlicensed
<a href="https://en.wikipedia.org/wiki/LoRa">LoRa</a> radios and open source
<a href="https://meshtastic.org/">Meshtastic</a> text messaging software. These
volunteer groups can use their radios, technical skills, and personal
connections with other experts to restore internet connectivity.</p>
<p>This page is a quick-start guide to forming your own Internet
Resiliency Club. You can also join a <a href="https://lists.bowshock.nl/mailman/listinfo/irc">mailing
list</a> for general
questions and discussion about internet resiliency clubs:</p>
<p><a href="https://lists.bowshock.nl/mailman/listinfo/irc">https://lists.bowshock.nl/mailman/listinfo/irc</a></p>
<ul>
<li><a href="#short-version">Skip directly to the section with hardware
recommendations and setup instructions</a></li>
<li><a href="https://ripe90.ripe.net/archives/video/1565/">Watch the 10 minute video</a> of the RIPE 90 talk</li>
<li><a href="https://ripe90.ripe.net/wp-content/uploads/presentations/36-Internet-Resiliency-Club-RIPE-90.pdf">Read the slides</a> for the 10 minute RIPE 90 talk</li>
<li><a href="https://pretalx.t-dose.org/media/2025/submissions/LYGXHY/resources/Internet_Resiliency_Club_T-DOSE_YqGkZ7U.pdf">Read the longer slides</a> for the 45 minute T-DOSE 2025 talk</li>
</ul>

<p>I am <a href="https://www.linkedin.com/in/valerieaurora/">Valerie Aurora</a>, a
systems software engineer with 25 years of experience in open source
software, operating systems, networking, file systems, and volunteer
organizing. When I moved from San Francisco to Amsterdam in 2023, I
started looking for ways to give back to my new home. In addition to
systems consulting, I am a special rapporteur for the EU’s Cyber
Resilience Act, serve as a RIPE Meeting program committee member, and
speak at European technical conferences.</p>
<h2 id="why-internet-resiliency-club">Why Internet Resiliency Club?</h2>
<p>One of my nightmares is waking up one morning and discovering that the
power is out, the internet is down, my cell phone doesn’t work, and
when I turn on the emergency radio (if you have one), all you hear is
“Swan Lake” on repeat.</p>
<p>As a recent immigrant to Amsterdam, I began to realize that this
nightmare was increasingly likely. Russia regularly knocks out
communications and power in Ukraine, using both bombs and hackers. In
2022, German windmills were disabled by malware aimed at Ukraine.
Dubious tankers continue to “accidentally” drag their anchors and cut
undersea cables in the Baltic. The head of NATO advised everyone to
keep three days of supplies at home.</p>
<h2 id="ukraines-advice-on-network-resilience">Ukraine’s advice on network resilience</h2>
<p>What made me finally take action is watching a video created by
<a href="https://1-ix.net/en/poslugy-en-translation/ua-exchange/">Ukrainian IXP
1-IX</a> to
teach other European countries what Ukrainian internet operators have
learned about hardening and repairing internet infrastructure leading
up to and following the 2022 Russian invasion. The practical realities
of keeping networks operating during war were sobering: building
camoflouged router rooms with 3 days of generator power, replacing
active fiber optic cable with passive, getting military service
exemptions for their personnel, etc.. You can watch the most recent
version, <a href="https://ripe90.ripe.net/archives/video/1582/">“Network Resilience: Experiences of survival and development
during the war in
Ukraine”</a>, a 30 minute
presentation at RIPE 90.</p>
<h2 id="what-is-the-dutch-government-doing-to-prepare">What is the Dutch government doing to prepare?</h2>
<p>Unfortunately, the government of the Netherlands is not following
Ukraine’s lead. <a href="https://berthub.eu/articles/posts/cyber-security-pre-war-reality-check/">Bert Hubert’s blog post</a>
describes the Netherlands’ cloud-based “emergency communications”
system, which will definitely not work in any emergency that affects
power or internet connectivity.</p>
<p>I have asked many Dutch network operators if there is any official
plan for the communications equivalent of a “black start” of the
electrical grid. If there is one, it isn’t being shared with the
people who will have to implement it.</p>
<h2 id="crisis-engineering-to-the-rescue">Crisis engineering to the rescue</h2>
<p>The final piece of the idea came from a class I took on <a href="https://layeraleph.com/event/">Crisis
Engineering</a> from Layer Aleph, on how
organizations facing an existential crisis either swiftly transform
themselves into a more functional form, or they fail and become even
more dysfunctional. Our class’s first question was, “How do you
convince an organization that a crisis is coming and they need to
prepare for it?”</p>
<p>Their answer was both depressing and freeing: “You can’t. All you can
do is be prepared with tools and a plan for when the crisis arrives.
That’s when the organization will listen.”</p>
<h2 id="what-can-i-do-personally">What can I do personally?</h2>
<p>I started thinking about what I could personally do without any help
from government or businesses. What if I could organize a group of
volunteer networking experts who could communicate without any
centralized infrastructure? We could effectively bootstrap
communications recovery with just a few volunteers and some cheap
hardware.</p>
<h2 id="ham-radio-is-too-expensive-difficult-and-power-hungry">Ham radio is too expensive, difficult, and power-hungry</h2>
<p>Initially I looked into ham radio, but it is just too expensive,
difficult, and power-hungry to be practical. Then Alexander Yurtchenko
told me about LoRa (Long Range) radio and Meshtastic, a cheap,
low-power method of sending text messages across a few kilometers.</p>
<p>After a few months of part-time research and organizing, the Amsterdam
Internet Resiliency Club was born. This page exists to make it easier
for other people to start Internet Resiliency Clubs in their area.</p>
<h2 id="we-need-volunteer-internet-resiliency-organizations">We need volunteer internet resiliency organizations</h2>
<p>The evidence that Internet Resiliency Clubs are necessary keeps
growing. Since I started this project, the city of Amsterdam announced
that it is planning for three weeks without electricity. Spain and
Portugal lost power for most of a day. The U.S. re-elected Donald
Trump, who may at some point realize that he can hold Europe hostage
by threatening to cut off access to U.S.-owned internet services like
AWS and Microsoft Exchange. Simultaneously, large parts of Dutch
government are migrating to email hosted by Microsoft, and major Dutch
technology firms continue to migrate to AWS and Microsoft Azure.</p>
<p>If you and I don’t do this, dear reader, no one will.</p>
<h2 id="short-version">Short version</h2>
<p>How to form an Internet Resiliency Club:</p>
<ul>
<li>Collect a group of internet-y people within ~10 km of each other</li>
<li>Decide how to communicate normally (Signal, Matrix, email, etc.)</li>
<li>Buy everyone LoRa (Long Range) radios and a powerbank with trickle charge</li>
<li>Install Meshtastic on the LoRa radios</li>
<li>Choose a LoRa channel to communicate on</li>
<li>Organize meetups, send messages over Meshtastic, have fun</li>
</ul>
<p>If you work for a internet infrastructure company, you can suggest
giving interested employees a LoRa radio, a mobile phone powerbank,
and maybe even a small solar panel for their personal use (perhaps as
part of an annual gift or bonus).</p>
<h2 id="lora">LoRa</h2>
<p>LoRa radios have several advantages for use in emergency
communications:</p>
<ul>
<li>no centralized infrastructure needed</li>
<li>no license needed</li>
<li>cheap (starting at ~€20)</li>
<li>low-power (&lt; 1W, can power with an ordinary mobile phone powerbank)</li>
<li>runs open source Meshtastic firmware</li>
<li>can send text messages across several line-of-sight hops (several kms)</li>
<li>can connect via Bluetooth or WiFi to phones/computers</li>
<li>many urban areas have a good Meshtastic network already</li>
</ul>
<p>Amateur ham radio can transmit at higher bandwidth for longer
distances, but requires extensive training, licensing, larger
antennas, and more power. Ideally, both would be available in an
emergency.</p>
<h2 id="lorameshtastic-basics">LoRa/Meshtastic basics</h2>
<p>With a LoRa radio running the Meshtastic firmware, anyone can send
text messages to anyone else with a Meshtastic node as long as it
takes three or fewer forwards from other Meshtastic nodes to get from
source to destination (usually around ~10 km but highly dependent on
local terrain and weather).</p>
<p>Specifically, <a href="https://en.wikipedia.org/wiki/LoRa">LoRa</a> is a
proprietary technique for sending low bit-rate radio messages (~1 - 25
kbps) using very low power (&lt; 1W), derived from chirp spread spectrum
techniques. <a href="https://meshtastic.org/">Meshtastic</a> is open source
firmware for LoRa radios that uses a flood-forward mesh protocol to
send message across up to three line-of-sight hops between LoRa nodes
running Meshtastic.</p>
<p>LoRa radios are for sale online. The cheapest versions are development
boards, intended for companies to use while building a product, often
without batteries, cases, or good antennas. To use them, you must
connect to them from a phone or computer, either over Bluetooth via
the Meshtastic app or over WiFi using a web browser. The more
expensive systems may include an enclosure, battery, solar panel,
larger screen, keyboard, etc. Some can be used without an additional
phone or computer.</p>
<h2 id="battery-power">Battery power</h2>
<p>LoRa radios use relatively little power, often in the range of 100 -
200 mA. A normal mobile phone power bank with a capacity of 10000 -
20000 mAh can power a LoRa radio approximately 2 - 8 days, depending
on chipset, time spent transmitting, whether WiFi or Bluetooth are in
use, etc. The powerbank should support “trickle charging”; without
this, many powerbanks will stop supplying power because the power draw
of many LoRa radios is so low that the powerbank thinks nothing is
connected and stops supplying power.</p>
<h2 id="solar-power">Solar power</h2>
<p>LoRa radios can be powered by directly plugging them into a small
solar panel with USB output, or by charging a battery used by the LoRa
radio. A small folding 800 cm^2 solar panel generating 15w with a
5W/500 mA max output is sufficient to power many LoRa radios. With
this small of a setup, you don’t need fuses, charge controllers,
buck/boost converters, or anything other than the solar panel and an
optional mobile phone power bank.</p>
<h2 id="which-lora-radio-to-buy">Which LoRa radio to buy</h2>
<p>LoRa radios are available in a huge range of capabilities and
features. For an Internet Resiliency Club, we recommend one of:</p>
<ul>
<li>Heltec V3: no case, no battery, WiFi/Bluetooth, OLED display, USB-C</li>
<li>LILYGO T-Echo: case, built-in battery, Bluetooth, e-ink display, USB-C</li>
</ul>
<p><em>IMPORTANT: Never turn on a LoRa device without an antenna attached!
The power sent to the antenna can destroy the device if there is no
antenna attached to radiate it.</em></p>
<p>Note: While many LoRa devices have USB-C ports, they often don’t
implement USB-C PD (Power Delivery) and won’t charge their battery
correctly on USB-C to USB-C cables. Use a USB-A to USB-C cable (often
supplied with the device).</p>
<h2 id="heltec-v3-series">Heltec V3 series</h2>
<p>If you have more time than money, try the latest Heltec V3, currently
one of the cheapest boards available at around €20. It has a
postage stamp-sized OLED screen, a couple of tiny buttons,
WiFi/Bluetooth, and USB-C input/power (but use a USB-A to USB-C
cable). Received messages are displayed on the OLED and can be cycled
through with tiny buttons. Sending messages requires connecting to it
via WiFi or Bluetooth.</p>
<p>It has no case, but the little plastic box it comes in can easily be
turned into one with a sharp pen knife. It also has no battery, but it
is a good idea to have a separate power bank anyway since you need a
working phone or computer to send messages. It has no GPS.</p>
<p>The <a href="https://meshtastic.org/docs/hardware/devices/heltec-automation/lora32/?heltec=v3">Meshtastic
page on this
board</a>
includes links to purchase from in Europe. I bought mine from
<a href="https://www.tinytronics.nl/en/development-boards/microcontroller-boards/with-lora/heltec-wifi-lora-32-esp32-s3-sx1262-with-0.96-inch-oled-display">TinyTronics</a>.</p>
<h2 id="lilygo-t-echo">LILYGO T-Echo</h2>
<p>If you have more money than time, I recommend the LILYGO T-Echo, a
simple small low-power ready-to-use handheld device for about
€80. It has ~3cm square e-ink display, a case with a few buttons,
Bluetooth, GPS, and about a day’s worth of battery.
Input/output/charging is via USB-C (but use a USB-A to USB-C cable).
Received messages are displayed on the e-ink screen and can be cycled
through with the buttons. Sending messages requires connecting with
another device via Bluetooth.</p>
<p>The <a href="https://meshtastic.org/docs/hardware/devices/lilygo/techo/">Meshtastic
page on this
board</a>
includes links to purchase from in Europe. I bought mine from
<a href="https://www.tinytronics.nl/en/development-boards/microcontroller-boards/with-gps/lilygo-ttgo-t-echo-nrf52840-lora-868mhz-bme280-gnss-black">TinyTronics</a>.</p>
<h2 id="lilygo-t-deck">LILYGO T-Deck</h2>
<p>If you want a standalone device that doesn’t require a separate phone
or computer to send messages, the <a href="https://www.tinytronics.nl/nl/development-boards/microcontroller-boards/met-lora/lilygo-t-deck-esp32-s3-toetsenbord-met-2.8-inch-ips-display-en-touchscreen-lora-868mhz-zwart">LILYGO
T-Deck</a>
includes a keyboard, trackball, and touch screen for about €70 -
80, depending on whether it includes a case and whether the antenna is
internal or external. It has about 8 hours of battery. I’m not a fan
because the screen and keyboard aren’t as good as the one on your
phone and take extra battery to run. It is often out of stock,
especially if you’re looking for a case and external antenna.</p>
<p>The <a href="https://meshtastic.org/docs/hardware/devices/lilygo/tdeck/">Meshtastic page on this
board</a>
includes links to purchase from in Europe.</p>
<h2 id="upgrading-the-antenna">Upgrading the antenna</h2>
<p>Most of the antennas that ship with evaluation boards are not very
good. One option for an upgrade if you’re using the recommended 868
MHz network is the <a href="https://eleshop.eu/868-mhz-antenne-met-sma-connector.html">Taoglas
TI.08.A</a>.</p>
<p><em>IMPORTANT: Never turn on a LoRa device without an antenna attached!
The power sent to the antenna can destroy the device if there is no
antenna attached to radiate it.</em></p>
<h2 id="flashing-installing-the-meshtastic-firmware">Flashing (installing) the Meshtastic firmware</h2>
<p>Some boards ship with Meshtastic already installed, but it’s
undoubtedly several months out of date. Flashing LoRa boards is
relatively easy; it can be as simple as using the <a href="https://meshtastic.org/docs/getting-started/flashing-firmware/esp32/">Meshtastic web
browser
flasher</a>
(requires Chrome or Edge) or dragging and dropping a file into a
mounted USB drive presented by the device. A command line tool using a
serial interface is also an option, but may require some fiddling with
a Python env.</p>

<p>In Europe, two frequencies are available for use by LoRa: <a href="https://meshtastic.org/docs/configuration/region-by-country/">868 MHz and
433
MHz</a>.
868 MHz is the <a href="https://meshtastic.org/docs/overview/radio-settings/">most popular for Meshtastic users in
Europe</a>. Several
modem presets are available; use the default mode <code>LONG_FAST</code> unless you
have a specific reason not to.</p>
<p>LoRa has
<a href="https://meshtastic.org/docs/configuration/radio/channels/">channels</a>,
a stream of messages using the same encryption key and channel name.
Each device is configured with a default primary channel shared by all
Meshtastic nodes. You can also configure secondary channels that can
only be accessed by nodes with the same key and channel name. Choose
an encryption key and channel name for a shared secondary channel. You
can share a QR code to configure a device with the appropriate
channels and settings.</p>
<h2 id="meetups-and-practice">Meetups and practice</h2>
<p>The best time to learn how to work together with a group of people is
before a crisis, not during it. Crisis engineering tells us that a
team is more likely to be successful if everyone has already worked
together.</p>
<p>Since this is a volunteer group, “working” together has to be fun.
Invite your group to do fun things together, changing up what activity
you are doing, where it is located, and what time it is held so that
a wide variety of people can participate.</p>
<h2 id="mailing-list">Mailing list</h2>
<p>If you have more questions or suggestions, please join our <a href="https://lists.bowshock.nl/mailman/listinfo/irc">mailing
list</a>:</p>
<p><a href="https://lists.bowshock.nl/mailman/listinfo/irc">https://lists.bowshock.nl/mailman/listinfo/irc</a></p>
<h2 id="credits">Credits</h2>
<p>Many people helped me with Internet Resiliency Club:</p>
<ul>
<li><a href="https://1-ix.net/en/poslugy-en-translation/ua-exchange/">Ukrainian IXP
1-IX</a> and other Ukrainian operators for their video <a href="https://ripe90.ripe.net/archives/video/1582/">“Network Resilience: Experiences of survival and development during the war in Ukraine”</a></li>
<li><a href="https://www.linkedin.com/in/andrew-yourtchenko-9304551/">Andrew Yourtchenko</a> for early brainstorming</li>
<li><a href="https://layeraleph.com/">Layer Aleph</a> and their <a href="https://layeraleph.com/event/">Crisis Engineering</a> class</li>
<li>Bert Hubert’s entire blog, but especially <a href="https://berthub.eu/articles/posts/cyber-security-pre-war-reality-check/">Cyber Security: A Pre-War Reality Check</a></li>
<li><a href="https://trmm.net/">Trammell Hudson</a> for helping me with battery and solar questions</li>
<li>Timo Hilbrink for researching antennas</li>
<li>The entire Amsterdam Internet Resiliency Club</li>
</ul>

    </div>
    

            </section>

            

            

        </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Nanonets-OCR-s – OCR model that transforms documents into structured markdown (295 pts)]]></title>
            <link>https://huggingface.co/nanonets/Nanonets-OCR-s</link>
            <guid>44287043</guid>
            <pubDate>Mon, 16 Jun 2025 06:14:56 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://huggingface.co/nanonets/Nanonets-OCR-s">https://huggingface.co/nanonets/Nanonets-OCR-s</a>, See on <a href="https://news.ycombinator.com/item?id=44287043">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
	<!-- HTML_TAG_START --><p>Nanonets-OCR-s is a powerful, state-of-the-art image-to-markdown OCR model that goes far beyond traditional text extraction. It transforms documents into structured markdown with intelligent content recognition and semantic tagging, making it ideal for downstream processing by Large Language Models (LLMs).</p>
<p>Nanonets-OCR-s is packed with features designed to handle complex documents with ease:</p>
<ul>
<li><strong>LaTeX Equation Recognition:</strong> Automatically converts mathematical equations and formulas into properly formatted LaTeX syntax. It distinguishes between inline (<code>$...$</code>) and display (<code>$$...$$</code>) equations.</li>
<li><strong>Intelligent Image Description:</strong> Describes images within documents using structured <code>&lt;img&gt;</code> tags, making them digestible for LLM processing. It can describe various image types, including logos, charts, graphs and so on, detailing their content, style, and context.</li>
<li><strong>Signature Detection &amp; Isolation:</strong> Identifies and isolates signatures from other text, outputting them within a <code>&lt;signature&gt;</code> tag. This is crucial for processing legal and business documents.</li>
<li><strong>Watermark Extraction:</strong> Detects and extracts watermark text from documents, placing it within a <code>&lt;watermark&gt;</code> tag.</li>
<li><strong>Smart Checkbox Handling:</strong> Converts form checkboxes and radio buttons into standardized Unicode symbols (<code>☐</code>, <code>☑</code>, <code>☒</code>) for consistent and reliable processing.</li>
<li><strong>Complex Table Extraction:</strong> Accurately extracts complex tables from documents and converts them into both markdown and HTML table formats.</li>
</ul>
<p>📢 <a rel="nofollow" href="https://nanonets.com/research/nanonets-ocr-s">Read the full announcement</a> | 🤗 <a rel="nofollow" href="https://huggingface.co/spaces/Souvik3333/Nanonets-ocr-s">Hugging Face Space Demo</a></p>
<h2>
	<a rel="nofollow" href="#usage" id="usage">
		
	</a>
	<span>
		Usage
	</span>
</h2>
<h3>
	<a rel="nofollow" href="#using-transformers" id="using-transformers">
		
	</a>
	<span>
		Using transformers
	</span>
</h3>
<pre><code><span>from</span> PIL <span>import</span> Image
<span>from</span> transformers <span>import</span> AutoTokenizer, AutoProcessor, AutoModelForImageTextToText

model_path = <span>"nanonets/Nanonets-OCR-s"</span>

model = AutoModelForImageTextToText.from_pretrained(
    model_path, 
    torch_dtype=<span>"auto"</span>, 
    device_map=<span>"auto"</span>, 
    attn_implementation=<span>"flash_attention_2"</span>
)
model.<span>eval</span>()

tokenizer = AutoTokenizer.from_pretrained(model_path)
processor = AutoProcessor.from_pretrained(model_path)


<span>def</span> <span>ocr_page_with_nanonets_s</span>(<span>image_path, model, processor, max_new_tokens=<span>4096</span></span>):
    prompt = <span>"""Extract the text from the above document as if you were reading it naturally. Return the tables in html format. Return the equations in LaTeX representation. If there is an image in the document and image caption is not present, add a small description of the image inside the &lt;img&gt;&lt;/img&gt; tag; otherwise, add the image caption inside &lt;img&gt;&lt;/img&gt;. Watermarks should be wrapped in brackets. Ex: &lt;watermark&gt;OFFICIAL COPY&lt;/watermark&gt;. Page numbers should be wrapped in brackets. Ex: &lt;page_number&gt;14&lt;/page_number&gt; or &lt;page_number&gt;9/22&lt;/page_number&gt;. Prefer using ☐ and ☑ for check boxes."""</span>
    image = Image.<span>open</span>(image_path)
    messages = [
        {<span>"role"</span>: <span>"system"</span>, <span>"content"</span>: <span>"You are a helpful assistant."</span>},
        {<span>"role"</span>: <span>"user"</span>, <span>"content"</span>: [
            {<span>"type"</span>: <span>"image"</span>, <span>"image"</span>: <span>f"file://<span>{image_path}</span>"</span>},
            {<span>"type"</span>: <span>"text"</span>, <span>"text"</span>: prompt},
        ]},
    ]
    text = processor.apply_chat_template(messages, tokenize=<span>False</span>, add_generation_prompt=<span>True</span>)
    inputs = processor(text=[text], images=[image], padding=<span>True</span>, return_tensors=<span>"pt"</span>)
    inputs = inputs.to(model.device)
    
    output_ids = model.generate(**inputs, max_new_tokens=max_new_tokens, do_sample=<span>False</span>)
    generated_ids = [output_ids[<span>len</span>(input_ids):] <span>for</span> input_ids, output_ids <span>in</span> <span>zip</span>(inputs.input_ids, output_ids)]
    
    output_text = processor.batch_decode(generated_ids, skip_special_tokens=<span>True</span>, clean_up_tokenization_spaces=<span>True</span>)
    <span>return</span> output_text[<span>0</span>]

image_path = <span>"/path/to/your/document.jpg"</span>
result = ocr_page_with_nanonets_s(image_path, model, processor, max_new_tokens=<span>15000</span>)
<span>print</span>(result)
</code></pre>
<h3>
	<a rel="nofollow" href="#using-vllm" id="using-vllm">
		
	</a>
	<span>
		Using vLLM
	</span>
</h3>
<ol>
<li>Start the vLLM server.</li>
</ol>
<pre><code>vllm serve nanonets/Nanonets-OCR-s
</code></pre>
<ol start="2">
<li>Predict with the model</li>
</ol>
<pre><code><span>from</span> openai <span>import</span> OpenAI
<span>import</span> base64

client = OpenAI(api_key=<span>"123"</span>, base_url=<span>"http://localhost:8000/v1"</span>)

model = <span>"nanonets/Nanonets-OCR-s"</span>

<span>def</span> <span>encode_image</span>(<span>image_path</span>):
    <span>with</span> <span>open</span>(image_path, <span>"rb"</span>) <span>as</span> image_file:
        <span>return</span> base64.b64encode(image_file.read()).decode(<span>"utf-8"</span>)

<span>def</span> <span>ocr_page_with_nanonets_s</span>(<span>img_base64</span>):
    response = client.chat.completions.create(
        model=model,
        messages=[
            {
                <span>"role"</span>: <span>"user"</span>,
                <span>"content"</span>: [
                    {
                        <span>"type"</span>: <span>"image_url"</span>,
                        <span>"image_url"</span>: {<span>"url"</span>: <span>f"data:image/png;base64,<span>{img_base64}</span>"</span>},
                    },
                    {
                        <span>"type"</span>: <span>"text"</span>,
                        <span>"text"</span>: <span>"Extract the text from the above document as if you were reading it naturally. Return the tables in html format. Return the equations in LaTeX representation. If there is an image in the document and image caption is not present, add a small description of the image inside the &lt;img&gt;&lt;/img&gt; tag; otherwise, add the image caption inside &lt;img&gt;&lt;/img&gt;. Watermarks should be wrapped in brackets. Ex: &lt;watermark&gt;OFFICIAL COPY&lt;/watermark&gt;. Page numbers should be wrapped in brackets. Ex: &lt;page_number&gt;14&lt;/page_number&gt; or &lt;page_number&gt;9/22&lt;/page_number&gt;. Prefer using ☐ and ☑ for check boxes."</span>,
                    },
                ],
            }
        ],
        temperature=<span>0.0</span>,
        max_tokens=<span>15000</span>
    )
    <span>return</span> response.choices[<span>0</span>].message.content

test_img_path = <span>"/path/to/your/document.jpg"</span>
img_base64 = encode_image(test_img_path)
<span>print</span>(ocr_page_with_nanonets_s(img_base64))
</code></pre>
<h3>
	<a rel="nofollow" href="#using-docext" id="using-docext">
		
	</a>
	<span>
		Using docext
	</span>
</h3>
<pre><code>pip install docext
python -m docext.app.app --model_name hosted_vllm/nanonets/Nanonets-OCR-s
</code></pre>
<p>Checkout <a rel="nofollow" href="https://github.com/NanoNets/docext/tree/dev/markdown">GitHub</a> for more details.</p>
<h2>
	<a rel="nofollow" href="#bibtex" id="bibtex">
		
	</a>
	<span>
		BibTex
	</span>
</h2>
<pre><code>@misc{Nanonets-OCR-S,
  title={Nanonets-OCR-S: A model for transforming documents into structured markdown with intelligent content recognition and semantic tagging},
  author={Souvik Mandal and Ashish Talewar and Paras Ahuja and Prathamesh Juvatkar},
  year={2025},
}
</code></pre>
<!-- HTML_TAG_END --></div></div>]]></description>
        </item>
    </channel>
</rss>