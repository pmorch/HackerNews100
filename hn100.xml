<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0">
    <channel>
        <title>HN100 - Readable Contents</title>
        <link>https://hn.algolia.com/api/v1/search_by_date?tags=%28story,poll%29&amp;numericFilters=points%3E100</link>
        <description>Uses Readability to add bodies to the RSS feed</description>
        <lastBuildDate>Tue, 19 Dec 2023 19:00:06 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>en</language>
        <item>
            <title><![CDATA[The changing face of post-pandemic New York City (131 pts)]]></title>
            <link>https://www.osc.ny.gov/press/releases/2023/12/changing-face-post-pandemic-new-york-city</link>
            <guid>38698353</guid>
            <pubDate>Tue, 19 Dec 2023 17:15:05 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.osc.ny.gov/press/releases/2023/12/changing-face-post-pandemic-new-york-city">https://www.osc.ny.gov/press/releases/2023/12/changing-face-post-pandemic-new-york-city</a>, See on <a href="https://news.ycombinator.com/item?id=38698353">Hacker News</a></p>
Couldn't get https://www.osc.ny.gov/press/releases/2023/12/changing-face-post-pandemic-new-york-city: Error: Request failed with status code 403]]></description>
        </item>
        <item>
            <title><![CDATA[Fedora Asahi Remix (130 pts)]]></title>
            <link>https://asahilinux.org/fedora/</link>
            <guid>38696612</guid>
            <pubDate>Tue, 19 Dec 2023 15:17:40 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://asahilinux.org/fedora/">https://asahilinux.org/fedora/</a>, See on <a href="https://news.ycombinator.com/item?id=38696612">Hacker News</a></p>
<div id="readability-page-1" class="page">
<div id="eye-catch">
<div>
<p>Introducing</p><p>The most polished Linux® for Apple Silicon Macs.</p><h2>Install from macOS</h2><pre><code id="curl">curl https://alx.sh | sh</code> <a id="copy-button" href="#"><i></i></a></pre></div><p><img src="https://asahilinux.org/img/far_landing/far_laptop.svg" alt="Fedora Asahi Remix on a laptop">
</p></div><section>
<h2 id="divfedora-linux-39--apple-silicon--fedora-asahi-remixdiv"><p>Fedora Linux 39 + Apple Silicon = Fedora Asahi Remix</p></h2><p><img width="200" src="https://asahilinux.org/img/far_landing/fedora_remix.png">Fedora Asahi Remix is the result of a close multi-year collaboration between the Asahi Linux project and the <a href="https://fedoraproject.org/">Fedora Project</a>. We’ve worked hard in order to bring you a fully integrated distro, cooperating closely to get improvements and bug fixes to users as quickly as possible. All of our Asahi platform-specific packages are in upstream Fedora and fully supported in Fedora Linux 39.</p><p>With Fedora’s excellent 64-bit ARM support and mature development process, you can expect a solid and high-quality experience without any unwanted surprises. Fedora Asahi Remix is based on Fedora Linux 39, the latest Fedora Linux release with the newest software versions across the board. All M1 and M2 series MacBook, Mac Mini, Mac Studio, and iMac devices are supported.</p></section><section>
<h2 id="divfedora-asahi-remix--kde-plasmadiv"><p>Fedora Asahi Remix ❤️ KDE Plasma</p></h2><p><img width="150" src="https://asahilinux.org/img/far_landing/kde-logo-white-blue-rounded-source.svg">We are proud to offer <a href="https://kde.org/plasma-desktop/">KDE Plasma</a> as our flagship desktop environment. With leading edge Wayland support and a highly customizable experience plus wide support for Apple hardware features, KDE Plasma is a joy to use on Apple Silicon.</p><p>Want to use Night Color to keep your screen from disrupting your sleep cycle? No worries, it just works. Tweak your trackpad settings for a more comfortable experience? Everything’s right there in System Settings. Are things on screen too big or too small? Just adjust the display scale to your heart’s content, even in 5% increments. We’ve worked with the KDE project to bring you bug fixes and improvements to improve platform support, and we’ve also built a custom Calamares-based initial setup wizard so you can be up and running in no time with minimal fuss.</p><p>Fedora Linux 39 comes with KDE Plasma 5.27, with the latest patches and improvements. But that’s not all: Stay tuned for the upcoming Fedora Linux 40, which will bring us KDE Plasma 6 with even more improvements.</p><p>Rather use <a href="https://www.gnome.org/">GNOME</a>? No worries, we’ve got you covered with GNOME 45. Or, if you prefer to roll your own desktop configuration or want to set up a headless server, our Server and Minimal images will let you set things up exactly the way you want to.</p></section><section>
<h2 id="div100-wayland-experiencediv"><p>100% Wayland Experience</p></h2><p><img width="150" src="https://asahilinux.org/img/far_landing/Wayland_Logo.svg">Whether you’re a KDE enthusiast or a GNOME lover, Fedora Asahi Remix comes right out of the box with a 100% <a href="https://wayland.freedesktop.org/">Wayland</a> environment, bringing you the newest desktop and display server technologies, which are a perfect match for Apple hardware. You’ll get a buttery smooth desktop, with absolutely no tearing or glitching, just like on macOS. Experience seamless HiDPI support in our KDE Plasma builds, even across multiple displays with different display scales.</p><p>And with upcoming improvements in the Wayland ecosystem, we’ll be able to support new technologies such as HDR and display notches, as well as proper display calibration. Have some X11 apps to run? No worries, XWayland is available and fully supported as a bridge for legacy applications.</p></section><section>
<h2 id="divopengl-deprecated-not-herediv"><p>OpenGL, deprecated? Not here</p></h2><p><img width="200" src="https://asahilinux.org/img/far_landing/OpenGL_ES_logo.svg">Fedora Asahi Remix ships with non-conformant OpenGL 3.3 support including GPU-accelerated geometry shaders and transform feedback, as well as the world’s first and only <a href="https://www.khronos.org/conformance/adopters/conformant-products/opengles#submission_1007">certified conformant OpenGL ES 3.1</a> implementation for Apple Silicon.</p><p>We support open graphics standards and test against official and industry-standard test suites, which means you can be confident that your apps and games will run correctly and render as they’re intended to. And we’re not stopping there, with OpenGL 4.x and Vulkan support in the works. We aim to unlock the full potential of Apple Silicon graphics, well beyond what is possible by layering on top of vendor-proprietary APIs like Metal.</p></section><section>
<h2 id="divthe-best-linux-laptop-audio-youve-ever-hearddiv"><p>The best Linux laptop audio you’ve ever heard</p></h2><p><img width="200" src="https://asahilinux.org/img/far_landing/curves.svg">Over the past two years, we’ve worked hard to pioneer the world’s first fully integrated DSP solution for the desktop Linux ecosystem. Just install Fedora Asahi Remix and enjoy high-quality audio right out of the box, no setup needed. We’ve worked together with the <a href="https://pipewire.org/">PipeWire</a> and <a href="https://gitlab.freedesktop.org/pipewire/wireplumber">WirePlumber</a> projects to add support for fully automatic and transparent DSP configuration, and then individually measured and calibrated 8+ different machine models, designing a customized DSP filter configuration for each one.</p><p>With our in-house <a href="https://github.com/chadmed/bankstown">Bankstown</a> bass boost technology and our own pioneering open source <a href="https://github.com/AsahiLinux/speakersafetyd">Smart Amp</a> implementation to safely provide full loudness and dynamic range, the result is the best audio you’ve ever heard on a Linux laptop. And we’ve even optimized the scheduling and power consumption of the DSP processing, so you’ll get excellent battery life while playing back audio.</p></section><section id="device-support">
<p>
<h2 id="device-support">Device support</h2></p><div>
<div id="info-dev1">
<h3>Chips</h3><h3>Features</h3><div>
<p>Display</p><p>Keyboard (+ Backlight)</p><p>Trackpad</p><p>Headset Jack</p><p>Speakers</p><p>Camera</p><p>MagSafe*</p><p>USB Type C (USB 3.0)</p><p>Wi-Fi</p><p>Bluetooth</p><p>USB-C Displays</p><p>Thunderbolt / USB4</p><p>Microphone</p><p>Touch ID</p></div><p>* Available on M2 model only.</p></div><div id="info-dev2">
<h3>Chips</h3><div>
<p>M1</p><p>M1 Pro</p><p>M1 Max</p><p>M2</p><p>M2 Pro</p><p>M2 Max</p></div><h3>Features</h3><div>
<p>Display*</p><p>Keyboard (+ Backlight)</p><p>Trackpad</p><p>Touch Bar†</p><p>Headset Jack</p><p>Speakers</p><p>Camera</p><p>MagSafe‡</p><p>USB Type C (USB 3.0)</p><p>HDMI‡</p><p>SD Card‡</p><p>Wi-Fi</p><p>Bluetooth</p><p>USB-C Displays</p><p>Thunderbolt / USB4</p><p>Microphone</p><p>Touch ID</p></div><p>* Local dimming available on 14" and 16" models. Maximum 60Hz refresh rate on all models. HDR/120Hz not yet supported.</p><p>† Available on 13" models only.</p><p>‡ Available on 14" and 16" models only.</p></div><div id="info-dev3">
<h3>Chips</h3><h3>Features</h3><div>
<p>Headset Jack</p><p>Speaker</p><p>USB Type A (3.0)</p><p>USB Type C (3.0)</p><p>HDMI</p><p>Ethernet (1/10 Gbps)</p><p>Wi-Fi</p><p>Bluetooth</p><p>USB-C Displays</p><p>Thunderbolt / USB4</p></div></div><div id="info-dev4">
<h3>Chips</h3><div>
<p>M1 Max</p><p>M1 Ultra</p><p>M2 Max</p><p>M2 Ultra</p></div><h3>Features</h3><div>
<p>Headset Jack</p><p>Speaker</p><p>USB Type A (3.0)</p><p>USB Type C (3.0)</p><p>HDMI</p><p>SD Card</p><p>Ethernet (10 Gbps)</p><p>Wi-Fi</p><p>Bluetooth</p><p>USB-C Displays</p><p>Thunderbolt / USB4</p></div></div><div id="info-dev5">
<h3>Chips</h3><h3>Features</h3><div>
<p>Display</p><p>Headset Jack</p><p>Speakers</p><p>Camera</p><p>USB Type C (3.0)</p><p>Ethernet (1 Gbps)</p><p>Wi-Fi</p><p>Bluetooth</p><p>USB-C Displays</p><p>Thunderbolt / USB4</p><p>Microphone</p></div></div></div></section>
</div>]]></description>
        </item>
        <item>
            <title><![CDATA[JetBrains forces AI freemium plugin that cannot be completely removed into IDEs (119 pts)]]></title>
            <link>https://youtrack.jetbrains.com/issue/LLM-1760/Can-not-remove-Jetbrains-AI-Assistant-plugin-completely</link>
            <guid>38696325</guid>
            <pubDate>Tue, 19 Dec 2023 15:00:12 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://youtrack.jetbrains.com/issue/LLM-1760/Can-not-remove-Jetbrains-AI-Assistant-plugin-completely">https://youtrack.jetbrains.com/issue/LLM-1760/Can-not-remove-Jetbrains-AI-Assistant-plugin-completely</a>, See on <a href="https://news.ycombinator.com/item?id=38696325">Hacker News</a></p>
&lt;Unparsable&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[Jepsen: MySQL 8.0.34 (152 pts)]]></title>
            <link>https://jepsen.io/analyses/mysql-8.0.34</link>
            <guid>38695750</guid>
            <pubDate>Tue, 19 Dec 2023 14:17:25 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://jepsen.io/analyses/mysql-8.0.34">https://jepsen.io/analyses/mysql-8.0.34</a>, See on <a href="https://news.ycombinator.com/item?id=38695750">Hacker News</a></p>
<div id="readability-page-1" class="page"><p><a href="https://www.mysql.com/">MySQL</a> is a popular relational database. We revisit Kleppmann’s 2014 <a href="https://github.com/ept/hermitage/blob/master/mysql.md">Hermitage</a> and confirm that MySQL’s Repeatable Read still allows G2-item, G-single, and lost update. Using our transaction consistency checker <a href="https://github.com/jepsen-io/elle">Elle</a>, we show that MySQL Repeatable Read also violates internal consistency. Furthermore, it violates Monotonic Atomic View: transactions can observe some of another transaction’s effects, then later fail to observe other effects of that same transaction. We demonstrate violations of ANSI SQL’s requirements for Repeatable Read. We believe MySQL Repeatable Read is somewhat stronger than Read Committed. As a lagniappe, we show that AWS RDS MySQL clusters routinely violate Serializability. This work was performed independently without compensation, and conducted in accordance with the <a href="https://jepsen.io/ethics">Jepsen ethics policy</a>.</p><article>
  <div>
<h2 data-number="1" id="background"> Background</h2>
<p><a href="https://www.mysql.com/">MySQL</a> needs little introduction. Over the last 28 years it has become one of the most widely deployed SQL databases. MySQL is primarily used for online transaction processing (OLTP) workloads, but is also deployed as a part of OLAP and queuing systems.</p>
<p>MySQL was designed as a single-server database, but has been extended with various multi-node replication schemes, including <a href="https://www.percona.com/blog/overview-of-different-mysql-replication-solutions/">several flavors</a> of <a href="https://dev.mysql.com/doc/refman/8.0/en/binlog-replication-configuration-overview.html">binlog replication</a>, <a href="https://dev.mysql.com/blog-archive/mysql-group-replication-a-quick-start-guide/">group replication</a>, <a href="https://dev.mysql.com/doc/refman/8.0/en/mysql-cluster.html">NDB cluster</a>, and third-party plugins like <a href="https://galeracluster.com/">Galera Cluster</a> &amp; <a href="https://www.percona.com/software/mysql-database/percona-xtradb-cluster">Percona XtraDB Cluster</a>. Previous Jepsen work discussed <a href="https://aphyr.com/posts/328-call-me-maybe-percona-xtradb-cluster">Percona XtraDB Cluster</a> and <a href="https://aphyr.com/posts/327-jepsen-mariadb-galera-cluster">Galera Cluster</a>. In this analysis we focus on single-server MySQL, but we also evaluated clusters with a single writeable primary and read-only secondaries using <a href="https://dev.mysql.com/doc/refman/8.0/en/replication-howto.html">binlog replication</a>.</p>
<p>MySQL also supports multiple storage engines which have different safety properties. We focus on the default: <a href="https://dev.mysql.com/doc/refman/8.0/en/innodb-introduction.html">InnoDB</a>. Throughout this text, we use “MySQL” to mean “MySQL using the InnoDB storage engine.”</p>
<h2 data-number="1.1" id="ansi-sql-isolation-is-bad-actually"> ANSI SQL Isolation is Bad, Actually</h2>
<p>In order to discuss the nuances of SQL isolation levels, we must first explain some history. In 1977 Gray, Lorie, Putzolu, and Traiger published <a href="https://www.cs.cmu.edu/~natassa/courses/15-721/papers/GrayLocks.pdf">Granularity of Locks and Degrees of Consistency in a Shared Data Base</a>, which introduced four increasingly safe degrees of transaction consistency. In 1973 IBM developed System R, one of the first relational databases, and shortly thereafter <a href="https://learnsql.com/blog/history-of-sql/">introduced SQL</a> as a query language for it. System R’s success spawned a slew of relational databases using SQL, many with distinct flavors of concurrency control. Starting in <a href="https://archive.org/details/federalinformati127nati">1986</a> ANSI <a href="https://blog.ansi.org/sql-standard-iso-iec-9075-2023-ansi-x3-135/">released</a><a href="#fn1" id="fnref1" role="doc-noteref"><sup>1</sup></a> a <a href="https://learnsql.com/blog/history-of-sql-standards/">series of standards</a> codifying SQL behavior. The third revision of the standard, SQL-92, defined the semantics of concurrent transactions through four transaction isolation levels, again with increasing degrees of safety. As with Gray et al., these isolation levels were related to the behavior of increasingly conservative locking regimes. However, to allow databases which used non-locking concurrency control, ANSI phrased their levels in terms of three possible phenomena which should not occur. As the standard puts it, “the following phenomena are possible:”<a href="#fn2" id="fnref2" role="doc-noteref"><sup>2</sup></a></p>
<dl>
<dt>P1 (“Dirty Read”)</dt>
<dd>SQL-transaction T1 modifies a row. SQL-transaction T2 then reads that row before T1 performs a COMMIT. If T1 then performs a ROLLBACK, T2 will have read a row that was never committed and that may thus be considered to have never existed.
</dd>
<dt>P2 (“Non-Repeatable Read”)</dt>
<dd>SQL-transaction T1 reads a row. SQL-transaction T2 then modifies or deletes that row and performs a COMMIT. If T1 then attempts to reread the row, it may receive the modified value or discover that the row has been deleted.
</dd>
<dt>P3 (“Phantom”)</dt>
<dd>SQL-transaction T1 reads the set of rows N that satisfy some &lt;search condition&gt;. SQL-transaction T2 then executes SQL-statements that generate one or more rows that satisfy the &lt;search condition&gt; used by SQL-transaction T1. If SQL-transaction T1 then repeats the initial read with the same &lt;search condition&gt;, it obtains a different collection of rows.
</dd>
</dl>
<p>ANSI SQL defines four isolation levels in terms of these anomalies. It begins by stating that transactions which execute at the Serializable isolation level must be equivalent to some serial execution, i.e., one in which that set of transactions executed one after the other. Then it says “the isolation levels are different with respect to phenomena P1, P2, and P3.” The standard provides the following table which “specifies the phenomena that are possible and not possible for a given isolation level”:</p>
<table>
<thead>
<tr>
<th>Level</th>
<th>P1</th>
<th>P2</th>
<th>P3</th>
</tr>
</thead>
<tbody>
<tr>
<td>Read Uncommitted</td>
<td>Possible</td>
<td>Possible</td>
<td>Possible</td>
</tr>
<tr>
<td>Read Committed</td>
<td>Not Possible</td>
<td>Possible</td>
<td>Possible</td>
</tr>
<tr>
<td>Repeatable Read</td>
<td>Not Possible</td>
<td>Not Possible</td>
<td>Possible</td>
</tr>
<tr>
<td>Serializable</td>
<td>Not Possible</td>
<td>Not Possible</td>
<td>Not Possible</td>
</tr>
</tbody>
</table>
<p>In 1995 Berenson, Bernstein, Gray,<a href="#fn3" id="fnref3" role="doc-noteref"><sup>3</sup></a> Melton, and the O’Neils published <a href="https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/tr-95-51.pdf">A Critique of ANSI SQL Isolation Levels</a>, which laid out critical flaws in these definitions. “The three ANSI phenomena are ambiguous. Even their broadest interpretations do not exclude anomalous behavior.”</p>
<p>For example, P1 says something bad might happen if <span><em>T</em><sub>1</sub></span> were to abort, but doesn’t actually say whether it aborts or not. Some people interpreted the standard to require <span><em>T</em><sub>1</sub></span> aborts. This would make it legal under read committed for transactions to read as-yet-uncommitted state from other transactions (so long as they went on to commit). <span><em>T</em><sub>1</sub></span> could write <span><em>x</em> = 1</span>, <span><em>T</em><sub>2</sub></span> could write <span><em>y</em> = 2</span>, and <span><em>T</em><sub>1</sub></span> and <span><em>T</em><sub>2</sub></span> could both see each other’s effects. This kind of circular information flow seems bad, but whether the standard allows it is a matter of interpretation. Similar ambiguities exist for P2 and P3.</p>
<p>Even interpreted broadly, preventing P1, P2, and P3 does not ensure Serializability. The standard omits a critical phenomenon P0 (“dirty write”), in which transaction <span><em>T</em><sub>1</sub></span> writes some row, transaction <span><em>T</em><sub>2</sub></span> overwrites <span><em>T</em><sub>1</sub></span>’s write, and <span><em>T</em><sub>1</sub></span> commits. This is clearly undesirable, but legal under ANSI Read Uncommitted, Read Committed, and Repeatable Read. Furthermore, ANSI SQL P3 only prohibits inserts affecting a predicate, but not updates or deletes.</p>
<p>In 1999, <a href="https://pmg.csail.mit.edu/papers/adya-phd.pdf">Atul Adya built on Berenson et al.’s critique</a> and developed formal and implementation-independent definitions of various transaction isolation levels, including those in ANSI SQL.<a href="#fn4" id="fnref4" role="doc-noteref"><sup>4</sup></a> As he notes:</p>
<blockquote>
<p>The ANSI definitions are imprecise because they allow at least two interpretations; furthermore, the anomaly interpretation is definitely incorrect. The preventative interpretation [meaning Berenson et al.’s interpretation which added P0, expanded P3, and so on] is correct in the sense that it rules out undesirable (i.e., non-serializable) histories. However, this interpretation is overly restrictive since it also rules out correct behavior that does not lead to inconsistencies and can occur in a real system. Thus, any system that allows such histories is disallowed by this interpretation, e.g., databases based on optimistic mechanisms.</p>
</blockquote>
<p>Adya first defines a dependency graph between transactions. There are three main types of dependencies, which we summarize informally:</p>
<dl>
<dt>Write-Write</dt>
<dd>Transaction <span><em>T</em><sub>1</sub></span> writes some version <span><em>x</em><sub>1</sub></span> of object <span><em>x</em></span>, which transaction <span><em>T</em><sub>2</sub></span> overwrites by installing the next version of <span><em>x</em></span>: <span><em>x</em><sub>2</sub></span>.
</dd>
<dt>Write-Read</dt>
<dd>Transaction <span><em>T</em><sub>1</sub></span> writes version <span><em>x</em><sub>1</sub></span>, which transaction <span><em>T</em><sub>2</sub></span> reads.
</dd>
<dt>Read-Write</dt>
<dd>Transaction <span><em>T</em><sub>1</sub></span> reads version <span><em>x</em><sub>1</sub></span>, which transaction <span><em>T</em><sub>2</sub></span> overwrites by installing the next version of <span><em>x</em></span>: <span><em>x</em><sub>2</sub></span>.
</dd>
</dl>
<p>Adya then defines portable isolation levels PL-1, PL-2, PL-2.99, and PL-3, which capture what the ANSI SQL standard (arguably) intended. Each level rules out progressively broader kinds of cycles in the transaction dependency graph:</p>
<dl>
<dt>PL-1 (“Read Uncommitted”)</dt>
<dd>Prohibits G0 (“write cycle”): a cycle of write-write dependencies. This is analogous to Berenson’s P0 (“dirty write”).
</dd>
<dt>PL-2 (“Read Committed”)</dt>
<dd>Prohibits G0 and G1. G1 consists of three anomalies: G1a (“aborted read”), G1b (“intermediate read”)<a href="#fn5" id="fnref5" role="doc-noteref"><sup>5</sup></a>, and G1c (“cyclic information flow”): a cycle of write-write or write-read dependencies. This captures the essence of the preventative interpretation of P1.
</dd>
<dt>PL-2.99 (“Repeatable Read”)</dt>
<dd>Prohibits G0, G1, and G2-item: a cycle involving write-write, write-read, or read-write edges <em>without predicates</em>. This captures the essence of ANSI SQL Repeatable Read, which is distinguished from Serializable only by predicate safety.
</dd>
<dt>PL-3 (“Serializable”)</dt>
<dd>Prohibits G0, G1, and G2: a cycle involving write-write, write-read, or read-write edges (with or without predicates). This guarantees equivalence to a serial execution.
</dd>
</dl>
<p>Adya’s dependency graph-based isolation levels resolved the ambiguities of the ANSI definitions, and remains the most widely-used formalism for characterizing transaction histories and anomalies. Jepsen generally uses Adya’s formalism.</p>
<p>Although the database community has known for decades that ANSI SQL’s isolation level definitions are broken, the standard’s language remained unchanged. The same ambiguous, incomplete definitions are still present in the <a href="https://webstore.ansi.org/standards/iso/isoiec90752023-2502169">2023 revision of the standard</a>.</p>
<h2 data-number="1.2" id="repeatable-read"> Repeatable Read</h2>
<p>ANSI SQL’s isolation levels are bad, but some levels have caused more problems than others.&nbsp; The fact that different database vendors provide isolation levels with the same names is useful only if the semantics of a particular level are consistent across vendors. And for three of the isolation levels, this is usually true. Most databases we’ve evaluated do ensure at least PL-1 for read uncommitted, PL-2 for Read Committed, and PL-3 for Serializable.<a href="#fn6" id="fnref6" role="doc-noteref"><sup>6</sup></a> However, there is less agreement on the semantics of Repeatable Read.</p>
<p>Adya’s PL-2.99 definition of Repeatable Read is quite strict, ruling out all dependency cycles except those involving predicate edges. The ANSI definition, while ambiguous, appears similarly strict: it prohibits all listed anomalies except “phantoms,” which depend on predicate reads. This is not surprising when we consider the roots of the isolation levels in locking regimes: the original Repeatable Read was the isolation level you got when you followed strict two-phase locking (holding read and write locks until the end of the transaction) but did not enforce predicate locking.</p>
<p>For some reason DB vendors have chosen different definitions of Repeatable Read than Adya and the ANSI standard, and almost no vendors provide the same guarantees at Repeatable Read. In fact, Microsoft SQL Server is the only database that we have tested for which Repeatable Read appears to correspond to PL-2.99 and the ANSI definition. In Postgres, <a href="https://jepsen.io/analyses/postgresql-12.3">Repeatable Read means Snapshot Isolation</a>, a level that is neither stronger nor weaker than PL-2.99.<a href="#fn7" id="fnref7" role="doc-noteref"><sup>7</sup></a></p>
<p>With this diversity of implementations in mind, we turn to the question at hand: what does MySQL do?</p>
<h2 data-number="1.3" id="mysql-isolation"> MySQL Isolation</h2>
<p>The <a href="https://dev.mysql.com/doc/refman/8.0/en/innodb-transaction-isolation-levels.html">transaction isolation levels</a> documentation for MySQL indicates that MySQL with InnoDB “offers all four transaction isolation levels described by the SQL:1992 standard”: <a href="https://dev.mysql.com/doc/refman/8.0/en/innodb-transaction-isolation-levels.html#isolevel_read-uncommitted">Read Uncommitted</a>, <a href="https://dev.mysql.com/doc/refman/8.0/en/innodb-transaction-isolation-levels.html#isolevel_read-committed">Read Committed</a>, <a href="https://dev.mysql.com/doc/refman/8.0/en/innodb-transaction-isolation-levels.html#isolevel_repeatable-read">Repeatable Read</a>, and <a href="https://dev.mysql.com/doc/refman/8.0/en/innodb-transaction-isolation-levels.html#isolevel_serializable">Serializable</a>. The documentation goes on to explain how MySQL achieves these isolation levels.</p>
<p>At MySQL Read Uncommitted, transactions should behave “like <a href="https://dev.mysql.com/doc/refman/8.0/en/innodb-transaction-isolation-levels.html#isolevel_read-uncommitted">Read Committed</a>,” except for allowing <a href="https://dev.mysql.com/doc/refman/8.0/en/glossary.html#glos_dirty_read">dirty read</a>: an anomaly where a read observes “data that was updated by another transaction but not yet committed.”</p>
<p>At MySQL <a href="https://dev.mysql.com/doc/refman/8.0/en/innodb-transaction-isolation-levels.html#isolevel_read-committed">Read Committed</a>, every individual <a href="https://dev.mysql.com/doc/refman/8.0/en/innodb-consistent-read.html">consistent read</a> reads from a fresh snapshot of committed state. A “consistent read” is the default behavior for reads (e.g.&nbsp;<code>SELECT * FROM problems</code>) and is the focus of this report. <a href="https://dev.mysql.com/doc/refman/8.0/en/innodb-locking-reads.html">There are also</a> stronger reads (e.g.&nbsp;<code>SELECT ... FOR UPDATE</code>) which explicitly request locks, and weaker reads (e.g.&nbsp;<code>SELECT ... SKIP LOCKED</code>) which skip some of the default locks.</p>
<p>MySQL <a href="https://dev.mysql.com/doc/refman/8.0/en/innodb-transaction-isolation-levels.html#isolevel_repeatable-read">Repeatable Read</a>, the default isolation level, ensures safety through a snapshot mechanism:</p>
<blockquote>
<p>Consistent reads within the same transaction read the snapshot established by the first read. This means that if you issue several plain (nonlocking) <code>SELECT</code> statements within the same transaction, these <code>SELECT</code> statements are consistent also with respect to each other.</p>
</blockquote>
<p>MySQL’s <a href="https://dev.mysql.com/doc/refman/8.0/en/innodb-consistent-read.html">consistent read documentation</a> further emphasizes that reads should operate on a snapshot of the database taken by the first read in a transaction.</p>
<blockquote>
<p>If the transaction isolation level is <code>REPEATABLE READ</code> (the default level), all consistent reads within the same transaction read the snapshot established by the first such read in that transaction….</p>
<p>Suppose that you are running in the default <code>REPEATABLE READ</code> isolation level. When you issue a consistent read (that is, an ordinary <code>SELECT</code> statement), InnoDB gives your transaction a timepoint according to which your query sees the database. If another transaction deletes a row and commits after your timepoint was assigned, you do not see the row as having been deleted. Inserts and updates are treated similarly.</p>
</blockquote>
<p>The documentation for <a href="https://dev.mysql.com/doc/refman/8.0/en/innodb-transaction-isolation-levels.html#isolevel_serializable">Serializable</a> isolation says Serializable is “like <code>REPEATABLE READ</code>, but <code>InnoDB</code> implicitly converts all plain <code>SELECT</code> statements to <code>SELECT ... FOR SHARE</code> if <code>autocommit</code> is disabled.”</p>
<p>There ends the isolation level documentation. However, if one digs deeper into the <a href="https://dev.mysql.com/doc/refman/8.0/en/innodb-consistent-read.html">consistent read documentation</a>, there is a curious note on the semantics of Repeatable Read:</p>
<blockquote>
<p>The snapshot of the database state applies to <code>SELECT</code> statements within a transaction, not necessarily to DML statements. If you insert or modify some rows and then commit that transaction, a <code>DELETE</code> or <code>UPDATE</code> statement issued from another concurrent <code>REPEATABLE READ</code> transaction could affect those just-committed rows, even though the session could not query them. If a transaction does update or delete rows committed by a different transaction, those changes do become visible to the current transaction.</p>
</blockquote>
<p>This is confusing: the ANSI SQL standard and MySQL’s <a href="https://dev.mysql.com/doc/refman/8.0/en/sql-data-manipulation-statements.html">own reference manual</a> both consider <code>SELECT</code> to be a DML statement, but this note seems to think they’re different. It appears that writes made by a Repeatable Read transaction can affect rows that the transaction could not read. But what does it mean for a different transaction’s updates to become visible to the current transaction? How does that align with MySQL’s claim that multiple reads in a Repeatable Read transaction “read the snapshot established by the first read”? What happened to the timepoint assigned by the first read?</p>
<p>This calls for a test.</p>
<h2 data-number="2" id="test-design"> Test Design</h2>
<p>We designed a <a href="https://github.com/jepsen-io/mysql">small test suite for MySQL</a> using the <a href="https://github.com/jepsen-io/jepsen">Jepsen testing library</a> at version 0.3.4. We used the <code>mysql-connector-j</code> JDBC adapter as our client. We tested MySQL 8.0.34, and <a href="https://mariadb.org/">MariaDB</a> 10.11.3 on Debian Bookworm. Our tests ran against a single MySQL node as well as binlog-replicated clusters with one or two read-only followers, without failover. We also ran our test suite against a hosted MySQL service: AWS’s <a href="https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/multi-az-db-clusters-concepts.html">RDS Cluster</a>, using the “Multi-AZ DB Cluster” profile. This is the recommended default for production workloads, and offers a binlog-replicated deployment of MySQL 8.0.34 where secondary nodes support read queries.</p>
<p>Our tests included basic fault injection for process pauses, crashes, and network partitions, as well as the loss of un-fsynced writes to disk. However, almost every finding we discuss in this work occurred in healthy, single-node MySQL instances.</p>
<h2 data-number="2.1" id="list-append"> List Append</h2>
<p>Our main workload used <a href="https://github.com/jepsen-io/elle">Elle</a>’s list-append checker for transactional isolation. In a nutshell, Elle infers Adya’s write-write, write-read, and read-write dependencies between transactions, then looks for cycles in the resulting dependency graph. Each cycle it finds demonstrates that a particular set of isolation levels do not hold.</p>
<p>At a high level the <a href="https://github.com/jepsen-io/mysql/blob/4c239cb5c66a7f1a55fa02ce4c9f43b7a70e9d0b/src/jepsen/mysql/append.clj">append workload</a> performs randomly generated transactions comprising reads and appends of unique integer elements to a collection of lists identified by primary key. As in <a href="https://jepsen.io/analyses/postgresql-12.3">our previous tests of SQL databases</a>, we encoded these lists as a <code>text</code> field of comma-separated values, one per row, and used SQL <code>CONCAT</code> to append elements.<a href="#fn8" id="fnref8" role="doc-noteref"><sup>8</sup></a> We split these rows across multiple tables with a structure like</p>
<div id="cb1"><pre><code><span id="cb1-1"><span>create</span> <span>table</span> <span>"txn0"</span> (</span>
<span id="cb1-2">  <span>id</span> <span>int</span> <span>not</span> <span>null</span> <span>primary</span> <span>key</span>,</span>
<span id="cb1-3">  val text</span>
<span id="cb1-4">);</span></code></pre></div>
<p>Over the last few years we’ve made several improvements to Elle which allow it to detect more anomalies.<a href="#fn9" id="fnref9" role="doc-noteref"><sup>9</sup></a> When some appended elements are never read in a list-append test, Elle now <a href="https://github.com/jepsen-io/elle/commit/bccbc1ed6bc175453b40d514bdf873e554494633">infers ww and rw dependencies</a>, placing them after the last value seen in the longest successful read. <a href="https://github.com/jepsen-io/elle/commit/9da9f48d4bf1ab998533d9d6f5c0de4b732365ce">We now detect P4 (lost update)</a> anomalies explicitly, even when version orders are uninferable. Elle also searches for <a href="https://github.com/jepsen-io/elle/commit/3abdb6a56b6c816199796d8c635125f7ecd197cd">cycles involving multiple nonadjacent read-write anti-dependencies which also include real-time and process edges</a>. This lets us detect more subtle violations of both strong and strong session Snapshot Isolation.</p>
<h2 data-number="2.2" id="non-repeatable-read"> Non-Repeatable Read</h2>
<p>When Elle identified internal consistency violations at Repeatable Read, we designed a <a href="https://github.com/jepsen-io/mysql/blob/4c239cb5c66a7f1a55fa02ce4c9f43b7a70e9d0b/src/jepsen/mysql/nonrepeatable_read.clj">workload specifically to stress</a> MySQL’s Repeatable Read semantics, which works as follows. We create a simple table of people identified by primary key, and populate it with a single row:</p>
<div id="cb2"><pre><code><span id="cb2-1"><span>create</span> <span>table</span> people (</span>
<span id="cb2-2">  <span>id</span>     <span>int</span> <span>not</span> <span>null</span>,</span>
<span id="cb2-3">  name   text <span>not</span> <span>null</span>,</span>
<span id="cb2-4">  gender text <span>not</span> <span>null</span>,</span>
<span id="cb2-5">  <span>primary</span> <span>key</span> (<span>id</span>))</span>
<span id="cb2-6">);</span>
<span id="cb2-7"><span>insert</span> <span>into</span> people (<span>id</span>, name, gender)</span>
<span id="cb2-8">  <span>values</span> (<span>0</span>, <span>"moss"</span>, <span>"enby"</span>);</span></code></pre></div>
<p>We then perform a series of write transactions which update only the row’s name. Concurrently, a second series of transactions each read the row’s name, update its gender field, and read the name again. Violations of Repeatable Read manifest as the row’s name changing between the two reads. We also perform deletions and re-insertions of row 0, in case they behave differently than plain updates.</p>
<h2 data-number="2.3" id="monotonic-atomic-view"> Monotonic Atomic View</h2>
<p>We also designed a <a href="https://github.com/jepsen-io/mysql/blob/4c239cb5c66a7f1a55fa02ce4c9f43b7a70e9d0b/src/jepsen/mysql/mav.clj">second targeted workload</a> to illustrate violations of <a href="https://jepsen.io/consistency/models/monotonic-atomic-view">Monotonic Atomic View</a>. This workload creates a single table with two rows:</p>
<div id="cb3"><pre><code><span id="cb3-1"><span>create</span> <span>table</span> mav (</span>
<span id="cb3-2">  <span>id</span>      <span>int</span> <span>not</span> <span>null</span>,</span>
<span id="cb3-3">  `value` <span>int</span> <span>not</span> <span>null</span>,</span>
<span id="cb3-4">  noop    <span>int</span> <span>not</span> <span>null</span>,</span>
<span id="cb3-5">  <span>primary</span> <span>key</span> (<span>id</span>)</span>
<span id="cb3-6">);</span>
<span id="cb3-7"><span>insert</span> <span>into</span> mav (<span>id</span>, `value`, noop)</span>
<span id="cb3-8">  <span>values</span> (<span>0</span>, <span>0</span>, <span>0</span>);</span>
<span id="cb3-9"><span>insert</span> <span>into</span> mav (<span>id</span>, `value`, noop)</span>
<span id="cb3-10">  <span>values</span> (<span>1</span>, <span>0</span>, <span>0</span>);</span></code></pre></div>
<p>We perform a mix of write and read transactions. Each write increments the <code>value</code> of row 0, then increments row 1. Reads select the value of row 0, set the <code>noop</code> field of row 1 to a random value, then read the values of 1 and 0. Under Monotonic Atomic View, these reads should be monotonically increasing. For example, once a reader observes value <code>2</code>, it should thereafter see every row’s value as <code>2</code> or higher.</p>
<h2 data-number="2.4" id="lazyfs"> LazyFS</h2>
<p>In 2022 Jepsen commissioned the University of Porto’s <a href="https://www.inesctec.pt/en">INESC TEC</a> to develop <a href="https://github.com/dsrhaslab/lazyfs">LazyFS</a>: a FUSE filesystem for simulating the loss of un-fsynced writes. LazyFS maintains an in-memory page cache of data which has been written but not fsynced, flushing it to underlying storage only as the cache fills or <code>fsync</code> calls are made. A test harness can ask LazyFS to discard its cache at any time, simulating what might happen during a power failure. João Pedro Rodrigues Azevedo’s <a href="https://repositorium.sdum.uminho.pt/bitstream/1822/84475/1/Joao%20Pedro%20Rodrigues%20Azevedo.pdf">dissertation</a> discusses this work in detail, including several database bugs.</p>
<p><a href="https://github.com/dsrhaslab/lazyfs">LazyFS</a> has been <a href="https://github.com/jepsen-io/jepsen/blob/f69fe9929af8528d289b9ba4f72bdc18bad35157/jepsen/src/jepsen/lazyfs.clj">integrated with Jepsen</a> for a little over a year, but this is the first public Jepsen report including it. We tested MySQL by killing the MySQL process, <a href="https://github.com/jepsen-io/mysql/blob/4c239cb5c66a7f1a55fa02ce4c9f43b7a70e9d0b/src/jepsen/mysql/db/mysql.clj#L200-L203">asking LazyFS to drop uncommitted writes</a>, then restarting the process.</p>
<h2 data-number="3" id="results"> Results</h2>
<h2 data-number="3.1" id="g2-item-at-repeatable-read"> G2-item at Repeatable Read</h2>
<p>Adya’s Repeatable Read (PL-2.99) prohibits G2-item: a cycle of write-write, write-read, and read-write dependency edges, where those edges do not involve predicates. However, MySQL’s Repeatable Read routinely allows G2-item, even on a single healthy node. Kleppmann <a href="https://github.com/ept/hermitage/blob/master/mysql.md#write-skew-g2-item">reported this behavior</a> in 2014 and it still occurs today. Take for example <a href="https://s3.amazonaws.com/jepsen.io/analyses/mysql-8.0.34/rr-g2-item-20230929T000636.621.zip">this list-append test</a>, which exhibited 214 cycles in just 40 seconds. Here is one of those cycles comprising two transactions, neither of which saw each other’s effects.</p>

<p>In this diagram the top transaction read key 141 and saw the value <code>[1 2]</code>, then appended <code>1</code> to key 140. The bottom transaction appended <code>3</code> to key 141, read key 141 and observed the value <code>[1 2 3]</code>, then read key 140 and found it did not exist. The top transaction must have executed before the bottom transaction, since it failed to observe the bottom transaction’s append of <code>3</code>. But the bottom transaction must have executed before the top transaction, since it read key 140 before any append! This cycle involves purely reads and updates by primary key, and is therefore G2-item.<a href="#fn10" id="fnref10" role="doc-noteref"><sup>10</sup></a></p>
<p>Transactions which fail to see each other’s effects could violate important invariants. Consider two independent electricians, each adding a new 20 amp circuit to a breaker panel. Each might visit the site to check<a href="#fn11" id="fnref11" role="doc-noteref"><sup>11</sup></a> that the total load on each circuit (including the one they intend to add) would not exceed the 100 amp capacity of the panel, then return a few days later to add the circuit. Under MySQL’s Repeatable Read, both could see a load of 70 amps, add a 20 amp circuit, and create a total load of 110 amps—exceeding the safe load of the panel.<a href="#fn12" id="fnref12" role="doc-noteref"><sup>12</sup></a></p>
<p>While this behavior is prohibited by PL-2.99 Repeatable Read, it could be interpreted as legal under ANSI SQL Repeatable Read. The standard’s definition of P2 (non-Repeatable Read) only discusses a transaction which reads the same row twice and observes some other transaction’s effects. Since these transactions never read a row twice, they do not exhibit P2! This is one of many ways in which the standard fails to capture anomalous behavior.</p>
<h2 data-number="3.2" id="g-single-at-repeatable-read"> G-single at Repeatable Read</h2>
<p>The example of G2-item we presented above involved a pair of transactions linked by adjacent read-write edges: in short, neither observed the other’s effects. However, MySQL Repeatable Read also exhibits G-single (a.k.a. read skew): cycles composed of write-write, write-read, and read-write edges, but where read-write edges are never adjacent to one another. <a href="https://github.com/ept/hermitage/blob/master/mysql.md#read-skew-g-single">Kleppmann reported this behavior in 2014</a>, and we can confirm it still occurs in MySQL 8.0.34. Like G2-item, G-single cycles involving only item dependencies are prohibited under PL-2.99 Repeatable Read.</p>
<p>Take, for example, this <a href="https://s3.amazonaws.com/jepsen.io/analyses/mysql-8.0.34/rr-everything-20231003T100453.595-0500.zip">sixty-second append test</a> of a single MySQL node without any faults. At roughly 140 transactions per second it exhibited 244 instances of G-single (plus 305 more instances of G2-item). Since the append test uses no predicate operations, all of these are violations of Repeatable Read. Here is one of those cycles:</p>

<p>The top transaction here appended <code>9</code> to key 363, then <code>5</code> to key 377. The bottom transaction failed to observe the append to 377, but also managed to append <code>10</code> to key 363 after the top transaction. We know this because a later read observed key 363’s value as <code>[5 6 4 7 8 9 10]</code>. This violates both Repeatable Read (which rules out any cycle of item edges) and Snapshot Isolation (which rules out G-single in general).</p>
<p>In short: one transaction can both fail to observe but also overwrite another. More complex cycles involving write-read edges also occur. In this case the dependency edges involved different keys, which suggests an interesting question: what would happen if two transactions conflicted on a <em>single</em> key?</p>
<h2 data-number="3.3" id="lost-update-at-repeatable-read"> Lost Update at Repeatable Read</h2>
<p>Phenomenon P4 (lost update) is a special case of G-single in which exactly two transactions are linked by a write-write and read-write cycle on a single key. In other words: two transactions read the same version of some key, and both go on to update it. This is expressly prohibited by <a href="https://jepsen.io/consistency/models/snapshot-isolation">Snapshot Isolation</a> and <a href="https://jepsen.io/consistency/models/repeatable-read">PL-2.99 Repeatable Read</a>. However, <a href="https://github.com/ept/hermitage/blob/master/mysql.md#lost-update-p4">Kleppmann showed</a> in 2014 that MySQL Repeatable Read allowed lost update, and we can confirm that it still occurs routinely, even on a single node without faults. Here is a second cycle from the <a href="https://s3.amazonaws.com/jepsen.io/analyses/mysql-8.0.34/rr-everything-20231003T100453.595-0500.zip">same test run</a>:</p>

<p>Both of these transactions read key 636, found it missing, and went on to write what they thought would be the first element. This is an obvious instance of lost update: at most one of these transactions should have been able to commit.<a href="#fn13" id="fnref13" role="doc-noteref"><sup>13</sup></a> We also have less obvious examples:</p>

<p>This cluster involves two G-single cycles. The smaller, comprising just the bottom two transactions, has no read of key 1167 before the middle transaction’s write: it is not a classic instance of lost update. However, its read of key 1167 = [1] implies that the state of that key <em>prior</em> to its append of 1 must have been empty, which looks “lost-update-esque.” Moreover, the top transaction <em>also</em> read the unborn version of key 1167 before appending 4 to it. That, together with the bottom transaction, must be lost update.</p>
<p>A later read <code>[:r 1167 [1 3 4]]</code> suggests the following sequence of events. All three transactions must have started before 1167 existed. The middle transaction appended <code>1</code> and read <code>[1]</code> back. Then the bottom transaction appended <code>3</code>, and finally the top transaction appended <code>4</code>. All three transactions eventually committed.</p>
<p>These instances of lost update were caught by Elle’s cycle detection system, since they were involved in G-single. However, Elle’s cycle detection relies on inferring the order of writes to a given key, which we can (mostly) do only if some read observes them. We have recently extended Elle to detect instances of lost update which are invisible to the cycle detector. In these tests, we search for two or more committed transactions which read the same version of some key <span><em>k</em></span>, then all write <span><em>k</em></span>. Regardless of whether we see their effects or not, the mere fact that both committed implies lost update. For example:</p>
<div id="cb4"><pre><code><span id="cb4-1">{<span>:key</span> <span>892</span>,</span>
<span id="cb4-2"> <span>:value</span> <span>nil</span>,</span>
<span id="cb4-3"> <span>:txns</span></span>
<span id="cb4-4"> [{<span>:process</span> <span>6</span>,</span>
<span id="cb4-5">   <span>:type</span> <span>:ok</span>,</span>
<span id="cb4-6">   <span>:f</span> <span>:txn</span>,</span>
<span id="cb4-7">   <span>:value</span> [[<span>:r</span> <span>892</span> <span>nil</span>]</span>
<span id="cb4-8">           [<span>:r</span> <span>891</span> <span>nil</span>]</span>
<span id="cb4-9">           [<span>:append</span> <span>892</span> <span>1</span>]</span>
<span id="cb4-10">           [<span>:r</span> <span>892</span> [<span>2</span> <span>5</span> <span>4</span> <span>1</span>]]],</span>
<span id="cb4-11">   <span>:index</span> <span>14806</span>,</span>
<span id="cb4-12">   <span>:time</span> <span>49518094450</span>}</span>
<span id="cb4-13">  {<span>:process</span> <span>18</span>,</span>
<span id="cb4-14">   <span>:type</span> <span>:ok</span>,</span>
<span id="cb4-15">   <span>:f</span> <span>:txn</span>,</span>
<span id="cb4-16">   <span>:value</span> [[<span>:r</span> <span>892</span> <span>nil</span>]</span>
<span id="cb4-17">           [<span>:append</span> <span>892</span> <span>8</span>]</span>
<span id="cb4-18">           [<span>:r</span> <span>891</span> [<span>2</span> <span>3</span>]]</span>
<span id="cb4-19">           [<span>:append</span> <span>891</span> <span>9</span>]],</span>
<span id="cb4-20">   <span>:index</span> <span>14842</span>,</span>
<span id="cb4-21">   <span>:time</span> <span>49636093552</span>}]}</span></code></pre></div>
<p>Both of these committed transactions read the unborn (<code>nil</code>) version of key 892 and wrote to it. Out of 9,048 successful transactions in this test, our new checker found 446 distinct transactions involved in 198 instances of lost update. Only 47 of those instances appeared in some cycle.</p>
<p>In short: MySQL Repeatable Read transactions cannot safely read a value and then write it. The standard ORM pattern where a program starts a transaction, loads an object into memory, manipulates it, saves it back to the database, then commits, may find that MySQL silently discards those committed changes. Although PL-2.99 Repeatable Read is supposed to make this pattern safe, MySQL Repeatable Read does not. MySQL users must instead perform their own explicit locking.</p>
<p>An attentive reader may have noticed the above example is more alarming than first meets the eye. The first transaction read the empty state of key 892, appended a single value, then read a version of key 892 including <em>three additional values</em>. Where did those come from?</p>
<h2 data-number="3.4" id="non-repeatable-read-at-repeatable-read"> Non-Repeatable Read at Repeatable Read</h2>
<p>MySQL Repeatable Read exhibits <em>internal consistency anomalies</em>: consistency violations whose effects are visible within a single transaction. These occur even on a single healthy MySQL node. In that <a href="https://s3.amazonaws.com/jepsen.io/analyses/mysql-8.0.34/rr-everything-20231003T100453.595-0500.zip">same test run</a>, 126 of 9,048 committed transactions exhibited internal consistency errors. For example:</p>
<div id="cb5"><pre><code><span id="cb5-1">{<span>:op</span></span>
<span id="cb5-2"> {<span>:process</span> <span>12</span>,</span>
<span id="cb5-3">  <span>:type</span> <span>:ok</span>,</span>
<span id="cb5-4">  <span>:f</span> <span>:txn</span>,</span>
<span id="cb5-5">  <span>:value</span> [[<span>:r</span> <span>1185</span> <span>nil</span>]</span>
<span id="cb5-6">          [<span>:append</span> <span>1185</span> <span>6</span>]</span>
<span id="cb5-7">          [<span>:append</span> <span>1182</span> <span>8</span>]</span>
<span id="cb5-8">          [<span>:r</span> <span>1185</span> [<span>3</span> <span>4</span> <span>2</span> <span>6</span>]]],</span>
<span id="cb5-9">  <span>:index</span> <span>19874</span>,</span>
<span id="cb5-10">  <span>:time</span> <span>65980191472</span>},</span>
<span id="cb5-11"> <span>:mop</span> [<span>:r</span> <span>1185</span> [<span>3</span> <span>4</span> <span>2</span> <span>6</span>]],</span>
<span id="cb5-12"> <span>:expected</span> [<span>6</span>]}</span></code></pre></div>
<p>This transaction read the unborn (<code>nil</code>) state of key 1185, and decided to append <code>6</code> to it. It then read key 1185 and observed <code>[3 4 2 6]</code>. Three elements appeared out of thin air. Or consider:</p>
<div id="cb6"><pre><code><span id="cb6-1">{<span>:op</span></span>
<span id="cb6-2"> {<span>:process</span> <span>19</span>,</span>
<span id="cb6-3">  <span>:type</span> <span>:ok</span>,</span>
<span id="cb6-4">  <span>:f</span> <span>:txn</span>,</span>
<span id="cb6-5">  <span>:value</span> [[<span>:append</span> <span>1099</span> <span>10</span>]</span>
<span id="cb6-6">          [<span>:r</span> <span>1096</span> [<span>1</span> <span>2</span> <span>3</span>]]</span>
<span id="cb6-7">          [<span>:append</span> <span>1096</span> <span>7</span>]</span>
<span id="cb6-8">          [<span>:r</span> <span>1096</span> [<span>1</span> <span>2</span> <span>3</span> <span>4</span> <span>5</span> <span>6</span> <span>7</span>]]],</span>
<span id="cb6-9">  <span>:index</span> <span>18404</span>,</span>
<span id="cb6-10">  <span>:time</span> <span>61061580955</span>},</span>
<span id="cb6-11"> <span>:mop</span> [<span>:r</span> <span>1096</span> [<span>1</span> <span>2</span> <span>3</span> <span>4</span> <span>5</span> <span>6</span> <span>7</span>]],</span>
<span id="cb6-12"> <span>:expected</span> [<span>1</span> <span>2</span> <span>3</span> <span>7</span>]}</span></code></pre></div>
<p>This transaction read key 1096 and obtained the list <code>[1 2 3]</code>. It appended <code>7</code>, then read the key again, and found three additional values (<code>4</code>, <code>5</code>, and <code>6</code>) inserted in its place. This is forbidden under PL-2.99 Repeatable Read: there must be a read-write dependency from this transaction to some other, and a write-read (or similar) dependency chain leading back. It is forbidden under ANSI Repeatable Read: the transaction performed two reads of the same object and saw different states resulting from a different transaction! The point of Repeatable Read—both for ANSI and Adya—is that once a transaction observes some value, it can count on that value being stable for the remainder of the transaction. MySQL does the opposite: a write is an invitation for another transaction to sneak in and clobber the state you just read.</p>
<p>This behavior allows <a href="https://s3.amazonaws.com/jepsen.io/analyses/mysql-8.0.34/gender-20231003T193907.588.zip">incredible transactions</a> like the following, recorded during a repeatable-read workload:</p>
<div id="cb7"><pre><code><span id="cb7-1"><span>set</span> <span>transaction</span> <span>isolation</span> <span>level</span> Repeatable <span>Read</span>;</span>
<span id="cb7-2"></span>
<span id="cb7-3"><span>start</span> <span>transaction</span>;</span>
<span id="cb7-4"><span>select</span> name <span>from</span> people <span>where</span> <span>id</span> <span>=</span> <span>0</span>;</span>
<span id="cb7-5">  <span>--&gt; "pebble"</span></span>
<span id="cb7-6"><span>update</span> people <span>set</span> gender <span>=</span> <span>"femme"</span> <span>where</span> <span>id</span> <span>=</span> <span>0</span>;</span>
<span id="cb7-7"><span>select</span> name <span>from</span> people <span>where</span> <span>id</span> <span>=</span> <span>0</span>;</span>
<span id="cb7-8">  <span>--&gt; "moss"</span></span>
<span id="cb7-9"><span>commit</span>;</span></code></pre></div>
<p>This transaction read a person’s name, set their gender, and read their name again. Despite executing at Repeatable Read, their name spontaneously changed from “pebble” to “moss”.</p>
<p>Violations of internal consistency <a href="https://software.imdea.org/~andrea.cerone/works/Framework.pdf">are forbidden under</a> Read Atomic, Causal Consistency, Parallel Snapshot Isolation, Prefix Consistency, Snapshot Isolation, and Serializability. It also seems clear that this transaction satisfies ANSI SQL’s informal definition of a “non-repeatable read.” It violates MySQL’s <a href="https://dev.mysql.com/doc/refman/8.0/en/innodb-transaction-isolation-levels.html#isolevel_repeatable-read">isolation levels documentation</a>, which claims that “consistent reads within the same transaction read the snapshot established by the first read.” It contradicts MySQL’s <a href="https://dev.mysql.com/doc/refman/8.0/en/innodb-consistent-read.html">consistent read documentation</a>, which specifically states that InnoDB assigns a timepoint on a transaction’s first read, and the effects of concurrent transactions should not appear in subsequent reads.</p>
<p>If we add other transactions which insert or delete the row, we can observe rows <a href="https://s3.amazonaws.com/jepsen.io/analyses/mysql-8.0.34/rr-nonrepeatable-insert-delete-20231003T231826.zip">popping into existence</a> in the middle of a Repeatable Read transaction:</p>
<div id="cb8"><pre><code><span id="cb8-1"><span>start</span> <span>transaction</span>;</span>
<span id="cb8-2"><span>select</span> name <span>from</span> people <span>where</span> <span>id</span> <span>=</span> <span>0</span> <span>--&gt; nil</span></span>
<span id="cb8-3"><span>update</span> people <span>set</span> gender <span>=</span> <span>"butch"</span> <span>where</span> <span>id</span> <span>=</span> <span>0</span>;</span>
<span id="cb8-4"><span>select</span> name <span>from</span> people <span>where</span> <span>id</span> <span>=</span> <span>0</span>; <span>--&gt; "moss"</span></span>
<span id="cb8-5"><span>commit</span>;</span></code></pre></div>
<p>However, we have not yet observed a row vanishing due to a concurrent delete. Perhaps this is because the update statement updates <em>no</em> rows, leaving the snapshot intact. Whatever the reason, the consistent read documentation’s claim that deletes, inserts, and updates “are treated similarly” appears incorrect: deletes seem to work differently from inserts and updates.</p>
<h2 data-number="3.5" id="non-monotonic-view"> Non-Monotonic View</h2>
<p><a href="https://github.com/ept/hermitage">Kleppmann’s Hermitage</a> lists MySQL Repeatable Read as <a href="https://jepsen.io/consistency/models/monotonic-atomic-view">monotonic atomic view</a>. Per <a href="https://amplab.cs.berkeley.edu/wp-content/uploads/2013/10/hat-vldb2014.pdf">Bailis et al</a>, Monotonic Atomic View ensures that once a transaction <span><em>T</em><sub>2</sub></span> observes an effect of transaction <span><em>T</em><sub>1</sub></span>, <span><em>T</em><sub>2</sub></span> observes <em>all</em> effects of <span><em>T</em><sub>1</sub></span>. Even if MySQL Repeatable Read fetches a fresh snapshot on each write, it might still provide Monotonic Atomic View if the snapshots are monotone. This is how <a href="https://github.com/ept/hermitage/blob/master/postgres.md">Postgres read committed</a> works.</p>
<p>This is not the case in MySQL. In healthy single-node deployments, MySQL routinely violates Monotonic Atomic View at Repeatable Read. Recall that our Monotonic Atomic View workload has two rows whose <code>value</code>s are initially <code>0</code>. Writer transactions increment the value of row 0, then row 1: both rows’ values should appear to advance in lockstep. However, the first read transaction from <a href="https://s3.amazonaws.com/jepsen.io/analyses/mysql-8.0.34/rr-mav-20231005T140119.171-0500.zip">this monotonic-atomic-view test</a> observed:</p>
<div id="cb9"><pre><code><span id="cb9-1"><span>start</span> <span>transaction</span>;</span>
<span id="cb9-2"><span>select</span> <span>value</span> <span>from</span> mav <span>where</span> <span>id</span> <span>=</span> <span>0</span>;    <span>--&gt; 0</span></span>
<span id="cb9-3"><span>update</span> mav <span>set</span> noop <span>=</span> <span>73</span> <span>where</span> <span>id</span> <span>=</span> <span>1</span>;</span>
<span id="cb9-4"><span>select</span> <span>value</span> <span>from</span> mav <span>where</span> <span>id</span> <span>=</span> <span>1</span>;    <span>--&gt; 1</span></span>
<span id="cb9-5"><span>select</span> <span>value</span> <span>from</span> mav <span>where</span> <span>id</span> <span>=</span> <span>0</span>;    <span>--&gt; 0</span></span>
<span id="cb9-6"><span>commit</span>;</span></code></pre></div>
<p>This read transaction saw the state of row 0 prior to the first write transaction. Then it saw the writer’s increment of row 1. Under monotonic atomic view, it should have gone on to observe all of the writer’s effects—including the increment of row 0. However, when it selected row 0 it saw the old value, not the new one. This is a non-monotonic read!</p>
<p>MySQL’s <a href="https://dev.mysql.com/doc/refman/8.0/en/innodb-consistent-read.html">consistent read documentation</a> talks about snapshots extensively, but this behavior doesn’t look like a snapshot at all. Snapshot systems usually provide a consistent, point-in-time view of the database state. They are usually atomic: either all of a transaction’s effects are included, or none are. Even if MySQL had somehow obtained a non-atomic snapshot from the <em>middle</em> of the write transaction, it must have seen the increment of row 0 before the increment of row 1. This is not the case: this read transaction observed the increment of row 1 but <em>not</em> row 0. In what sense can this possibly be considered a snapshot?</p>
<h2 data-number="3.6" id="fractured-read-like-anomalies-with-rds-serializable"> Fractured Read-Like Anomalies with RDS Serializable</h2>
<p>A common strategy for improving both the availability and throughput of a production MySQL database is to deploy one or more <em>read replicas</em>. These replicas continually apply binlogs that are shipped to them by the read/write primary instance, accept connections, and permit read-only transactions to run. Some cloud vendors (e.g.&nbsp;Amazon RDS) configure one or two read replicas as a part of their default production deployment profile.</p>
<p>We found that AWS RDS MySQL routinely violated Serializability at Serializable isolation, even in healthy clusters. Consider <a href="http://jepsen.io.s3.amazonaws.com/analyses/mysql-8.0.34/20230829T165306.964Z.zip">this append test</a> which ran on an RDS MySQL cluster with the default recommended production profile. It exhibited several G2-item and G-single anomalies, like the following:</p>

<p>The top transaction appended <code>3</code> to key 2215, and that write was visible to the middle transaction. The middle transaction appended <code>8</code> to key 2219, which was visible to the bottom transaction. However, the bottom transaction missed the top transaction’s write! All G-single anomalies we found involved at least three transactions linked by at least two write-read edges.</p>
<p>Exactly what kind of anomaly is this, and how severe is it? It is clearly an instance of G-single, since it has exactly one read-write edge. It is also G2-item, since it does not involve predicates. This implies RDS MySQL’s “Serializable” isolation violates Snapshot Isolation, Repeatable Read, and Serializability.</p>
<p>However, G-single is a broad class of anomalies, and this appears unlike the other instances of G-single we’ve discussed so far. It is not lost update: no transaction reads then writes the same key. Unlike our previous example of G-single which involved a write-write edge, this anomaly has only write-read and read-write edges. It somewhat resembles <a href="https://people.eecs.berkeley.edu/~alig/papers/ramp.pdf">fractured read</a>, in which a transaction reads only a subset of another transaction’s writes. However, this anomaly involves a reader <span><em>T</em><sub>3</sub></span> which observes a writer <span><em>T</em><sub>2</sub></span>’s effects, but does not observe an earlier <span><em>T</em><sub>1</sub></span> which was visible to <span><em>T</em><sub>2</sub></span>. It is in some sense a “transitive” fractured read.</p>
<p>Regarding severity, we observe first that <em>any</em> instance of G-single, when running all transactions at the Serializable isolation level, is significant. The received wisdom in the MySQL community is to avoid using Serializable unless absolutely necessary. The MySQL manual discourages users from using Serializable at all, stating:</p>
<blockquote>
<p><code>SERIALIZABLE</code> enforces even stricter rules than <code>REPEATABLE READ</code>, and is used mainly in specialized situations, such as with XA transactions and for troubleshooting issues with concurrency and deadlocks.</p>
</blockquote>
<p>Given this guidance, we would expect users to run transactions at this strongest isolation level only when they know they need a high degree of safety, and are willing to pay the performance cost of extra synchronization in order to rule out anomalies. Graver still, fractured read-like anomalies (as instances of G2-item) are forbidden by Repeatable Read. They should occur only at Read Committed and below. That they arise at “Serializable” is troubling.</p>
<p>We suspect this behavior is due in part to RDS’s choice of default parameters for production clusters. Among the large variety of configuration parameters that govern the behavior of read replicas is one whose very name should make us uneasy: <a href="https://dev.mysql.com/doc/refman/8.0/en/replication-options-replica.html#sysvar_replica_preserve_commit_order"><code>replica_preserve_commit_order</code></a>.<a href="#fn14" id="fnref14" role="doc-noteref"><sup>14</sup></a></p>
<blockquote>
<p>For multithreaded replicas (replicas on which <code>replica_parallel_workers</code> is set to a value greater than 0), setting <code>replica_preserve_commit_order=ON</code> ensures that transactions are executed and committed on the replica in the same order as they appear in the replica’s relay log. This prevents gaps in the sequence of transactions that have been executed from the replica’s relay log, and preserves the same transaction history on the replica as on the source (with the limitations listed below).</p>
</blockquote>
<p>Serializable systems are supposed to guarantee transactions execute in (what appears to be) a total order. Failing to preserve that order on a replica seems like it would be a bad thing, even if it permitted more parallelism in applying the log entries. The documentation goes on to say that while this parameter used to be disabled by default, MySQL version 8.0.27 and higher default to <code>replica_preserve_commit_order=ON</code>. However, RDS’s default parameters still choose <code>replica_preserve_commit_order=OFF</code>. If we apply this setting to our local test clusters, we observe similar instances of G-single and G2-item.</p>
<table>
<thead>
<tr>
<th>№</th>
<th>Summary</th>
<th>Event Required</th>
<th>Fixed in</th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td>G2-item at Repeatable Read</td>
<td>None</td>
<td>Unresolved</td>
</tr>
<tr>
<td>2</td>
<td>G-single at Repeatable Read</td>
<td>None</td>
<td>Unresolved</td>
</tr>
<tr>
<td>3</td>
<td>Lost update at Repeatable Read</td>
<td>None</td>
<td>Unresolved</td>
</tr>
<tr>
<td>4</td>
<td>Non-Repeatable read at Repeatable Read</td>
<td>None</td>
<td>Unresolved</td>
</tr>
<tr>
<td>5</td>
<td>Non-monotonic view at Repeatable Read</td>
<td>None</td>
<td>Unresolved</td>
</tr>
<tr>
<td>6</td>
<td>Fractured read-like anomalies at Serializable (in RDS)</td>
<td>None</td>
<td>Unresolved</td>
</tr>
</tbody>
</table>
<h2 data-number="4" id="discussion"> Discussion</h2>
<p>First, the good news. In our testing, MySQL 8.0.34’s Read Uncommitted, read committed, and Serializable isolation levels appeared to satisfy PL-1 read uncommitted, PL-2 Read Committed, and PL-3 Serializable, respectively. This held both for single nodes and small clusters with read-only replicas using binlog replication, and through process pauses, crashes, and network partitions.</p>
<p>Our LazyFS fault injection scheme did not discover problems with MySQL’s default settings. With <a href="https://dev.mysql.com/doc/refman/8.0/en/innodb-parameters.html#sysvar_innodb_flush_log_at_trx_commit"><code>innodb_flush_log_at_trx_commit</code></a> at the default setting of <code>1</code>, process crashes followed by the loss of un-fsynced data did not result in the loss of committed transactions. When we adjusted that setting to <code>0</code>, MySQL fsynced only once every <span><em>n</em></span> seconds and we observe data loss.</p>
<p>The bad news: MySQL’s “Repeatable Read” does not satisfy PL-2.99 Repeatable Read: it exhibits G2-item anomalies including write skew. It does not satisfy Snapshot Isolation: it exhibits G-single, including read skew and lost update. Lost update rules out cursor stability. Reads in MySQL “Repeatable Read” are not repeatable, even under the ambiguous definitions of the ANSI SQL standard. Its transactions violate internal consistency, which rules out Read Atomic, Causal, Consistent View, Prefix, and Parallel snapshot isolation. <a href="https://github.com/ept/hermitage#summary-of-test-results">Kleppmann’s 2014 Hermitage</a> suggested MySQL Repeatable Read might be Monotonic Atomic View, but this cannot be true: we found monotonicity violations.</p>
<p>Some authors <a href="https://fileadmin.cs.lth.se/cs/Education/EDAF20/lectures/transactions.pdf">characterize</a> MySQL Repeatable Read as <a href="https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/tr-95-51.pdf">Snapshot Isolation</a>. For example, Kleppmann’s <a href="https://www.oreilly.com/library/view/designing-data-intensive-applications/9781491903063/"><em>Designing Data-Intensive Applications</em></a> says “PostgreSQL and MySQL call their Snapshot Isolation level Repeatable Read”.<a href="#fn15" id="fnref15" role="doc-noteref"><sup>15</sup></a> Formalizations of Snapshot Isolation <a href="https://jepsen.io/consistency/models/snapshot-isolation">vary</a>, but most make it appear as if all of a transaction’s reads occurred at the transaction start time (plus local changes). This is not true in MySQL: when a write occurs, multiple reads of a key may reveal <em>newer</em> versions of that key resulting from other transactions’ writes. Moreover, Snapshot Isolated systems generally appear as if all of a transaction’s writes occur atomically. MySQL allows a transaction to read some, but not all, of another transaction’s writes. This is inconsistent with every version of Snapshot Isolation we are familiar with.</p>
<p>It isn’t clear what MySQL Repeatable Read actually <em>is</em>. It allows histories which violate Monotonic Atomic View and cursor stability; we know it cannot be equal to or stronger than those models. We have not observed G0 (dirty writes), G1a (aborted reads), G1b (intermediate reads), or G1c (cyclic infomation flow); it appears at least as strong as Read Committed. The repeatability of <em>some</em> reads means it is actually stronger than Read Committed.</p>

<p>In this <a href="https://jepsen.io/consistency">graph of consistency models</a> an arrow from A to B means that B is strictly stronger than A. By this we mean the histories permitted by B are a strict subset of those permitted by A: a system which provides B provides A as well. It seems likely that MySQL Repeatable Read is incomparable to Monotonic Atomic View: it allows violations of Monotonic Atomic View, but also rules out some non-repeatable reads that Monotonic Atomic View allows. Likewise, it is incomparable to Repeatable Read: MySQL Repeatable Read <a href="https://www.pythian.com/blog/understanding-mysql-isolation-levels-repeatable-read">appears to prohibit certain phantoms</a> which are legal under both ANSI and PL-2.99 Repeatable Read. However, we are unsure exactly which other models are strictly stronger than MySQL Repeatable Read. Is every prefix-consistent history legal under MySQL Repeatable Read? Or are they too incomparable? Because MySQL Repeatable Read’s behavior is so unusual, and because we lack a formal definition of its properties, we are unsure where to draw additional arrows in the diagram above.</p>
<p>As always, we caution that Jepsen takes an experimental approach to safety verification: we can prove the presence of bugs, but not their absence. While we make extensive efforts to find problems, we cannot prove correctness.</p>

<p>The behavior of MySQL Repeatable Read appears poorly understood in the MySQL community. <a href="https://priyankvex.com/2018/10/20/tackling-lost-updates-problem-in-database-using-better-isolation-level/">Several</a> <a href="https://levelup.gitconnected.com/preventing-data-inconsistencies-in-mysql-strategies-for-avoiding-lost-updates-cfdb04107f7c">authors</a> <a href="https://www.zghurskyi.com/lost-update/">believe</a> <a href="https://forums.mysql.com/read.php?22,56420,56420#msg-56420">Repeatable</a> <a href="https://vladmihalcea.com/a-beginners-guide-to-database-locking-and-the-lost-update-phenomena/">Read</a> <a href="https://amirsoleimani.medium.com/understanding-database-isolation-level-via-examples-mysql-and-postgres-a86b5502d404">should</a> <a href="https://flylib.com/books/en/1.63.1.107/1/">prevent</a> lost update. However, <a href="https://stackoverflow.com/questions/46315232/how-to-use-transactions-in-mysql-to-avoid-lost-updates">several</a> <a href="https://forums.mysql.com/read.php?22,56420,57733">others</a> <a href="https://stackoverflow.com/questions/9060400/repeatable-read-and-second-lost-updates-issue">acknowledge</a> <a href="https://www.up-2date.com/post/lost-update">it</a> <a href="https://blog.jcoglan.com/2020/10/12/reading-and-writing-part-3/">actually</a> <a href="https://forum.hibernate.org/viewtopic.php?p=2489608">does</a> <a href="https://stackoverflow.com/questions/10040785/mysql-repeatable-read-and-lost-update-phantom-reads?rq=3">not</a>, and advise (for example) explicit locking tactics. Similarly, <a href="https://www.prisma.io/dataguide/mysql/inserting-and-modifying-data/using-transactions">many</a> <a href="https://mydbops.wordpress.com/2018/06/22/back-to-basics-isolation-levels-in-mysql/">internet</a> <a href="https://buildatscale.tech/transaction-isolation-level-in-innodb/">sources</a> <a href="https://stackoverflow.com/questions/42668158/mysql-repeatable-read-transaction-unexpected-behavior">state</a> (<a href="https://decentro.tech/blog/decoding-isolation-levels-in-mysql/">incorrectly</a>) that MySQL repeatable reads <a href="https://www.prisma.io/dataguide/mysql/inserting-and-modifying-data/using-transactions">are</a> <a href="https://www.tutorialspoint.com/mysql/mysql_set_transaction.htm">repeatable</a>. This is understandable: <a href="https://dev.mysql.com/doc/refman/8.0/en/innodb-transaction-isolation-levels.html#isolevel_repeatable-read">MySQL</a> and <a href="https://mariadb.com/kb/en/set-transaction/#repeatable-read">MariaDB</a>’s own documentation makes this claim. Those claims are contradicted by a single sentence in a note buried in the <a href="https://dev.mysql.com/doc/refman/8.0/en/innodb-consistent-read.html">MySQL consistent reads documentation</a>. Other blog posts and articles <a href="https://yizhang82.dev/innodb-repeatable-read">acknowledge</a> (<a href="https://dev.to/techschoolguru/understand-isolation-levels-read-phenomena-in-mysql-postgres-c2e">some indirectly</a>) that MySQL Repeatable Read <a href="https://www.pythian.com/blog/understanding-mysql-isolation-levels-repeatable-read">actually</a> <a href="https://www.percona.com/blog/what-if-mysqls-repeatable-reads-cause-you-to-lose-money/">allows</a> non-repeatable reads.</p>
<p>Cabral and Murphy’s 2009 <a href="https://www.oreilly.com/library/view/mysql-administrators-bible/9780470416914/"><em>MySQL Administrator’s Bible</em></a> states that MySQL “supports the four standard isolation levels,” and emphasizes at length that Repeatable Read prevents a transaction from observing another transaction’s concurrent writes:</p>
<blockquote>
<p>Using the <code>REPEATABLE READ</code> isolation level, all reads within a transaction show the same data values, even if a second transaction has committed a data change while the first transaction was still running. If a transaction starts, reads a row, waits 60 seconds, and reads the same row again, both data reads will be the same—even if in those 60 seconds another transaction has changed and committed data. The first transaction has the same data when it repeats the read….</p>
<p><code>REPEATABLE READ</code> may not seem like a good idea—after all, if the data changes, shouldn’t a transaction be aware of that? The problem is that a transaction may take different actions based on the values of the data. Data values changing within a transaction may lead to unexpected consequences.</p>
</blockquote>
<p>Cabral and Murphy repeat that Repeatable Read “allows a transaction to see the same data for values it has already read regardless of whether or not the data has been changed.” In their section on multi-version concurrency control, they emphasize the independence of transaction snapshots:</p>
<blockquote>
<p>If a second transaction starts, it “checks out” its own copy of the data. If the first transaction makes changes and commits, the second transaction will not see the data. The second transaction can only work with the data it has. There is no way to update the data that the second transaction sees, though the second transaction could issue a ROLLBACK and start the transaction again to see the new data.</p>
</blockquote>
<p>This is also wrong: writing a row modifies the transaction’s local copy of the data.</p>
<p>Grippa &amp; Kuzmichev’s 2021 <a href="https://www.oreilly.com/library/view/learning-mysql-2nd/9781492085911/"><em>Learning MySQL</em></a> states that MySQL supports all of the SQL:1992 standard isolation levels. They too claim:</p>
<blockquote>
<p>With the <code>REPEATABLE READ</code> isolation level, there are thus no dirty reads and or non-repeatable reads. Each transaction reads the snapshot established by the first read.</p>
</blockquote>
<p>However, the section on Serializable isolation actually demonstrates (perhaps inadvertently) that MySQL’s Repeatable Read allows both lost update, a change in read snapshot, and a resulting internal consistency violation! It then shows that <code>Serializable</code> prevents those anomalies. It doesn’t name the anomalies, instead opting to say that “this doesn’t make sense”, but the behavior is visible to a careful reader. It’s not clear if the authors realize the example contradicts their earlier claims about non-repeatable reads and snapshot integrity.</p>
<h2 data-number="4.2" id="recommendations"> Recommendations</h2>
<p>The core problem is that MySQL claims to implement Repeatable Read but actually provides something much weaker. We see two avenues to resolve this problem.</p>
<p>The first is to keep MySQL’s behavior as it is, and to clearly document the consistency model “Repeatable Read” actually provides. There is precedent in other databases: PostgreSQL’s Repeatable Read <a href="https://jepsen.io/analyses/postgresql-12.3">is actually Snapshot Isolation</a>, and exhibits behaviors which violate PL-2.99 Repeatable Read. However, PostgreSQL’s documentation eventually <a href="https://www.postgresql.org/docs/current/transaction-iso.html#XACT-REPEATABLE-READ">mentions</a> that their Repeatable Read implementation is actually Snapshot Isolation. MySQL could similarly document that their “Repeatable Read” means “Read Committed, plus some sort of guarantees that hold until the transaction writes something, at which point mysteries occur.” A precise characterization of those mysteries would be most welcome.</p>
<p>The second option is to treat these behaviors as bugs and fix them. Jepsen would be delighted if MySQL and other vendors were to commit to providing PL-2.99 Repeatable Read. However, even satisfying the incomplete, ambiguous ANSI definition of Repeatable Read would be an improvement over current affairs.</p>
<p>In the meantime, MySQL users who require PL-2.99 or ANSI Repeatable Read should be cautious of MySQL Repeatable Read. Reads may not be repeatable, or even reflect a snapshot of committed state. The common ORM pattern in which a transaction reads an object into memory, modifies it, then writes it back within a transaction, may cause committed updates to be silently lost. Users requiring Repeatable Read semantics should use MySQL’s Serializable isolation instead. Alternatively, they can selectively strengthen reads performed at <code>READ COMMITTED</code> using locking techniques like <code>SELECT ... FOR UPDATE</code>.</p>
<h2 data-number="4.3" id="rds"> RDS</h2>
<p>AWS RDS MySQL cluster exhibits read skew and G2-item at its “Serializable” isolation level. Users who rely on Serializability should set <code>slave_preserve_commit_order</code> to <code>ON</code> in their RDS parameter groups. Jepsen suggests that AWS either change the default, or clearly explain the allowed violations of Serializability in the <a href="https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/MySQL.KnownIssuesAndLimitations.html">known limitations</a> documentation for RDS MySQL.</p>
<h2 data-number="4.4" id="future-work"> Future Work</h2>
<p>MySQL’s binlog replication appears fragile. We observed a number of mysterious scenarios in which replication halted in our local Jepsen tests. We also found that a few minutes of testing could <a href="https://mastodon.jepsen.io/@jepsen/111231274947177218">completely break</a> AWS RDS’s MySQL replication: even a simple <code>CREATE DATABASE</code> would succeed on the primary and fail to appear on the secondaries. We waited an hour without observing recovery. MySQL’s <a href="https://dev.mysql.com/doc/refman/8.0/en/replication-solutions-unexpected-replica-halt.html">default settings are known to be unsafe</a> in replicated systems. We made no attempt to promote nodes from secondaries to primaries, or to explore <a href="https://mariadb.com/kb/en/replication-overview/#common-replication-setups">exciting topologies</a> like ring or star replication. Future work might explore these behaviors.</p>
<p>We have begun research into more general-purpose predicate tests, but this work is still early. Once ready, we’d like to evaluate MySQL predicate safety and see if it differs from primary-key operations.</p>
<h2 data-number="4.5" id="a-plea-to-standards-bodies"> A Plea to Standards Bodies</h2>
<p>Twenty-eight years after Berenson et al.&nbsp;demonstrated that ANSI SQL’s isolation levels are ambiguous and incomplete, seven revisions of the ANSI &amp; ISO standards have left its definitions unchanged.<a href="#fn16" id="fnref16" role="doc-noteref"><sup>16</sup></a> P0 is still legal at every level up to Repeatable Read. We still don’t know whether circular information flow is legal at Read Committed. P3 still doesn’t mention deletes. Internal behavior remains unspecified. <a href="https://www.vldb.org/pvldb/vol7/p181-bailis.pdf">The</a> <a href="https://www.cs.cornell.edu/lorenzo/papers/Crooks17Seeing.pdf">research</a> <a href="https://arxiv.org/abs/1903.00731">community</a> <a href="https://software.imdea.org/~andrea.cerone/works/Framework.pdf">has</a> <a href="https://software.imdea.org/~gotsman/papers/si-podc16.pdf">moved</a> <a href="http://www.cs.ox.ac.uk/people/hongseok.yang/paper/popl14-final.pdf">on</a> <a href="https://asc.di.fct.unl.pt/~nmp/pubs/europar-2-2013.pdf">to</a> <a href="https://dsf.berkeley.edu/cs286/papers/ssi-tods2005.pdf">new</a> <a href="https://www.inf.usi.ch/faculty/pedone/Paper/2004/IC_TECH_REPORT_200421.pdf">formalisms</a>. Many are based on <a href="https://pmg.csail.mit.edu/papers/adya-phd.pdf">Adya’s 1999 thesis</a>, which struggled to capture “what the SQL standard actually meant.”</p>
<p>If you happen to sit on the ISO/IEC JTC1/SC 32 Data Management and Interchange committee, please imagine the soft chords of a heart-tugging piano lament have begun to play. A montage of transactional anomalies appears on your screen. Internal anomalies. Lost updates. Dirty writes. Jepsen is looking into the camera, holding a database.</p>
<blockquote>
<p>Hi. This is Jepsen. Will you be an angel for a helpless database? Every day major databases exhibit anomalous behavior which ISO/IEC 9075-2 fails to characterize. For just <a href="https://pmg.csail.mit.edu/papers/icde00.pdf">a few pages of formalism</a>, you can give vendors and users a clear, meaningful, and portable definition of isolation levels.</p>
<p>It’s been almost three decades. Act now.</p>
</blockquote>
<p><em>Jepsen wishes to thank <a href="https://www.inesctec.pt/en">INESC TEC</a> and in particular João Azevedo, Ricardo Macedo, João Tiago Paulo, José Pereira, and Maria Ramos, for building LazyFS. Our thanks also to <a href="https://github.com/jgpc42">Justin Conklin</a>, who contributed ASM advice and code for a significant performance improvement in Jepsen’s underlying analysis library. <a href="https://www.irenekannyo.com/">Irene Kannyo</a> provided invaluable editorial support. This work was performed independently by Jepsen without compensation, in accordance with the <a href="https://jepsen.io/ethics">Jepsen ethics policy</a>.</em></p>
<section role="doc-endnotes">
<hr>
<ol>
<li id="fn1" role="doc-endnote"><p>Ever sticklers for precision, ANSI <a href="https://blog.ansi.org/sql-standard-iso-iec-9075-2023-ansi-x3-135/">reminds readers</a> that while ANSI approves, copyrights, and publishes standards, “there are no ANSI standards, only standards developed by ANSI-approved committees, many operating in accordance with the ANSI Essential Requirements (American National Standards).” In this work, “the ANSI SQL standard” refers to ANSI X3.135, also known as ISO/IEC 9075. ANSI X3.135 (<a href="https://www.youtube.com/watch?v=3xe7uS62fwY">not an ANSI standard</a>) was originally produced by the Accredited Standards Committee (ASC)’s ANSI Database Technical Committee (ANSI X3H2). ISO 9075, a technically identical standard, was published a few months later. Today ISO/IEC 9075 is developed by the ISO/IEC Joint Technical Committee (JTC) 1 for Information Technology, and INCITS (an ANSI-accredited standards developing organization and the successor to ANSI X3), adopts it for use as an American National Standard.<a href="#fnref1" role="doc-backlink">↩︎</a></p></li>
<li id="fn2" role="doc-endnote"><p>The relevant portion of the SQL standard <a href="https://webstore.ansi.org/standards/iso/isoiec90752023-2502169?source=blog">costs over two hundred dollars</a>, making it inaccessible to casual readers. Precise phrasing is critical for this work, so we reproduce its definitions verbatim.<a href="#fnref2" role="doc-backlink">↩︎</a></p></li>
<li id="fn3" role="doc-endnote"><p>Yes, that Jim Gray, of System R fame!<a href="#fnref3" role="doc-backlink">↩︎</a></p></li>
<li id="fn4" role="doc-endnote"><p>Readers looking for a shorter version should try <a href="https://pmg.csail.mit.edu/papers/icde00.pdf">Adya’s ICDE paper</a> with Barbara Liskov and Patrick O’Neil.<a href="#fnref4" role="doc-backlink">↩︎</a></p></li>
<li id="fn5" role="doc-endnote"><p>We omit a detailed explanation of the non-cyclic anomalies, as well as a discussion of aborted and committed transactions, internal and external reads/writes, version orders, etc., for brevity.<a href="#fnref5" role="doc-backlink">↩︎</a></p></li>
<li id="fn6" role="doc-endnote"><p>Vendors rarely come out and <em>say</em> they intend to provide the Adya levels, but in practice their implementations either prevent (e.g.) G0 at Read Committed, or, when informed of the behavior, correct it. There are exceptions. For instance, RedPanda <a href="https://jepsen.io/analyses/redpanda-21.10.1">considers G0 legal</a> at “Read Committed,” and Oracle’s “Serializable” is <a href="https://github.com/ept/hermitage/blob/master/oracle.md">actually Snapshot Isolation</a>. Many databases provide higher isolation than is required under Adya’s formalism. For instance, PostgreSQL appears to provide Monotonic Atomic View at read committed.<a href="#fnref6" role="doc-backlink">↩︎</a></p></li>
<li id="fn7" role="doc-endnote"><p>Because write skew can occur in Snapshot Isolation but not PL-2.99, and phantoms can occur in PL-2.99 but some are prevented by Snapshot Isolation, neither is strictly stronger than the other. For more details, see <a href="https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/tr-95-51.pdf">Berenson et al.</a>, which explains that while A3 (the strict interpretation of phantoms) is prohibited by Snapshot Isolation, it sometimes permits P3 (the broad interpretation).<a href="#fnref7" role="doc-backlink">↩︎</a></p></li>
<li id="fn8" role="doc-endnote"><p>There has been <a href="https://arxiv.org/abs/2301.07313">some confusion</a> about whether Elle’s list-append workload requires a custom datatype not supported by SQL databases. We are happy to share that the SQL standard has included both string and array concatenation since 1999, and that these datatypes and operators are broadly supported by vendors. Elle also includes a workload for plain read-write registers.<a href="#fnref8" role="doc-backlink">↩︎</a></p></li>
<li id="fn9" role="doc-endnote"><p><a href="http://mpaxos.com/pub/viper-eurosys23.pdf">Some</a> <a href="https://arxiv.org/abs/2301.07313">authors</a> have claimed some or all of Elle’s checkers are unsound. To the best of our knowledge Elle is sound: every anomaly it finds is “real.” As our paper discussed, Elle is not complete: it may fail to detect some anomalies. This work makes Elle more complete.<a href="#fnref9" role="doc-backlink">↩︎</a></p></li>
<li id="fn10" role="doc-endnote"><p>Write skew (A5B) as presented by <a href="https://arxiv.org/pdf/cs/0701157.pdf">Berenson et al</a> requires both transactions read before writing. This particular anomaly is write-skew-esque: the two transactions have overlapping read sets and disjoint write sets causing each to fail to observe the other’s effects. However, one transaction happened to read after writing, rather than before.<a href="#fnref10" role="doc-backlink">↩︎</a></p></li>
<li id="fn11" role="doc-endnote"><p>Breaker panels have a fixed number of slots, some of which may be left free. Adding a circuit involves installing a circuit breaker in a free slot. In this model of G2-item, the electricians check each slot on the breaker panel by performing a series of primary-key reads. If they used a predicate read, this would be G2.<a href="#fnref11" role="doc-backlink">↩︎</a></p></li>
<li id="fn12" role="doc-endnote"><p>This scenario happened to one of your authors. Thankfully the panel limit was not exceeded.<a href="#fnref12" role="doc-backlink">↩︎</a></p></li>
<li id="fn13" role="doc-endnote"><p>Some readers might contend that while this meets the formal definition of “lost update,” the updates themselves aren’t so much “lost” as simply “stacked on top of each other in potential violation of constraints.” Consider, however, that if these update operations were blind writes instead of list appends, the resulting state would be indistinguishable from a history in which one of the writes simply never happened. This is the reason P4 is called “lost update.”<a href="#fnref13" role="doc-backlink">↩︎</a></p></li>
<li id="fn14" role="doc-endnote"><p>The RDS parameter group for configuring MySQL uses an older name for this setting: <code>slave_preserve_commit_order</code>. MySQL is in the process of adopting more inclusive language, and Jepsen uses the newer “replica” term wherever possible.<a href="#fnref14" role="doc-backlink">↩︎</a></p></li>
<li id="fn15" role="doc-endnote"><p>Kleppman goes on to note in a later section that MySQL/InnoDB fails to prevent lost update, and that this fails to meet “some authors” definitions of Snapshot Isolation.<a href="#fnref15" role="doc-backlink">↩︎</a></p></li>
<li id="fn16" role="doc-endnote"><p>From the late 1980s through 1995 <a href="https://tdan.com/is-sql-a-real-standard-anymore/4923">NIST performed conformance testing</a> to evaluate whether databases correctly implemented the SQL standard. One wonders how they would evaluate transaction safety today.<a href="#fnref16" role="doc-backlink">↩︎</a></p></li>
</ol>
</section>
  </div>
</article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[An In-depth Look at Gemini's Language Abilities (108 pts)]]></title>
            <link>https://arxiv.org/abs/2312.11444</link>
            <guid>38695583</guid>
            <pubDate>Tue, 19 Dec 2023 14:01:53 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://arxiv.org/abs/2312.11444">https://arxiv.org/abs/2312.11444</a>, See on <a href="https://news.ycombinator.com/item?id=38695583">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="content-inner">
    
    
                
    <p><a href="https://arxiv.org/pdf/2312.11444.pdf">Download PDF</a></p><blockquote>
            <span>Abstract:</span>The recently released Google Gemini class of models are the first to comprehensively report results that rival the OpenAI GPT series across a wide variety of tasks. In this paper, we do an in-depth exploration of Gemini's language abilities, making two contributions. First, we provide a third-party, objective comparison of the abilities of the OpenAI GPT and Google Gemini models with reproducible code and fully transparent results. Second, we take a closer look at the results, identifying areas where one of the two model classes excels. We perform this analysis over 10 datasets testing a variety of language abilities, including reasoning, answering knowledge-based questions, solving math problems, translating between languages, generating code, and acting as instruction-following agents. From this analysis, we find that Gemini Pro achieves accuracy that is close but slightly inferior to the corresponding GPT 3.5 Turbo on all tasks that we benchmarked. We further provide explanations for some of this under-performance, including failures in mathematical reasoning with many digits, sensitivity to multiple-choice answer ordering, aggressive content filtering, and others. We also identify areas where Gemini demonstrates comparably high performance, including generation into non-English languages, and handling longer and more complex reasoning chains. Code and data for reproduction can be found at <a href="https://github.com/neulab/gemini-benchmark" rel="external noopener nofollow">this https URL</a>
    </blockquote>

    <!--CONTEXT-->
    
  </div><div>
      <h2>Submission history</h2><p> From: Graham Neubig [<a href="https://arxiv.org/show-email/6908451c/2312.11444">view email</a>]      <br>    <strong>[v1]</strong>
        Mon, 18 Dec 2023 18:47:42 UTC (3,372 KB)<br>
</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Firefox 121 defaults to Wayland on Linux (170 pts)]]></title>
            <link>https://www.omgubuntu.co.uk/2023/12/firefox-121-released-now-defaults-to-wayland-on-linux</link>
            <guid>38695533</guid>
            <pubDate>Tue, 19 Dec 2023 13:56:19 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.omgubuntu.co.uk/2023/12/firefox-121-released-now-defaults-to-wayland-on-linux">https://www.omgubuntu.co.uk/2023/12/firefox-121-released-now-defaults-to-wayland-on-linux</a>, See on <a href="https://news.ycombinator.com/item?id=38695533">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
                                <article id="post-275524">
                                                
                            <div>
                        
<p><strong>Mozilla Firefox 121 has been released, and it’s a notable one for Linux Wayland users.</strong></p>



<p>For the Ubuntu 23.10 release, the <a href="https://www.omgubuntu.co.uk/2023/09/wayland-firefox-snap-default">Firefox Snap runs in Wayland mode</a> by default (and like many of you I’ve noticed nothing but bountiful benefits resulting from the switch). </p>



<p>Mozilla’s workshop elves were clearly happy with the success of that trial as they’ve now chosen to make Firefox 121 run in Wayland mode by default for all Linux users (who use Wayland; the browser runs under Xorg/X11 as well as it ever did).</p>



<p>Why is Firefox enabling native Wayland mode by default a big deal? And how does that mode differ to the xWayland mode the browser has been running in on Wayland sessions prior to this release?</p>



<ul>
<li><strong>Better graphics performance</strong></li>



<li><strong>Non-blurry rendering on HiDPI displays/fractional scaling</strong></li>



<li><strong>Per-monitor DPI settings respected</strong></li>



<li><strong>Full support for touchpad <em>and</em> touchscreen gestures</strong></li>
</ul>



<p>Plus:</p>



<ul>
<li><strong>It’s embracing the future ✨</strong></li>
</ul>



<p>Okay, so that bullet point isn’t something Mozilla is talking up but, as a Linux user, it’s clear that Wayland <strong>is</strong> the future for modern, secure, performant display server needs on Linux. Like 64-bit computing, it just is.</p>



<p>The days of Wayland being a “someday” feature, off in the distance, obscured by wisps of hope and concern, are gone. It’s here, it’s in use, it’s where the momentum and demand are at, and it’s bringing cool new things with it, like HDR support to Linux.</p>



<p>And for Mozilla Firefox to continue providing Linux users with a solid, dependable, and integrated experience on Linux, it too has to keep pace with technological changes — and defaulting to Wayland by default (where supported) is a key play in that.</p>



<p>Got a bit sidetracked there, sorry 😅.</p>



<p>Other notable <a href="https://www.mozilla.org/en-US/firefox/121.0/releasenotes/" target="_blank" rel="noreferrer noopener">changes in Firefox 121</a>:</p>



<ul>
<li><strong>New option to force-underline links in websites</strong></li>



<li><strong>Easier to delete drawings/text/images when editing PDFs</strong></li>



<li><strong>Support for Voice Control commands on macOS</strong></li>



<li><strong>Prompts users to install Microsoft AV1 Extension on Windows</strong></li>
</ul>



<p>Linux-specific bug fixes include:</p>



<ul>
<li><strong>PiP window showing black borders on top and bottom</strong></li>



<li><strong>AppStream metainfo in Firefox Flatpak now follows latest spec</strong></li>



<li><strong>Dialogs/windows becoming ‘unnecessarily’ large in KDE Wayland</strong></li>



<li><strong>Mouse cursor inappropriately changes icon</strong></li>



<li><strong><a href="https://hg.mozilla.org/mozilla-central/rev/86de5a0de5cd" target="_blank" rel="noreferrer noopener">Fix for issue causing slow startup</a> on certain distros</strong></li>



<li><strong><a href="https://bugzilla.mozilla.org/show_bug.cgi?id=1849186" target="_blank" rel="noreferrer noopener">Browsing history being leaked to syslogs</a> in GNOME</strong></li>
</ul>



<p>Firefox 121 boasts further expansion to its web platform capabilities, including support for&nbsp;<code>hanging</code>&nbsp;&amp;&nbsp;<code>each-line</code>&nbsp;keywords in the CSS text-indent property, and support for <code>text-wrap: balance</code> to ‘harmonize’ line lengths in short, multi-line text blocks.</p>



<h2>Download Firefox 121</h2>



<p>I think that covers the bulk of the changes here, but if you happen to notice something I didn’t, do let me know by leaving a comment down in, well the comments section.</p>



<p>Otherwise, look out for an update to Mozilla Firefox 121 on your system in the next day or so. It’ll come as an in-app update (existing Firefox users on Windows and macOS, Linux binary users); a background update (users of the Firefox Snap); or as a repo update (users of PPAs, AUR, etc).</p>



<p>Alternatively, you can download Mozilla Firefox from the official website.</p>
                                                                                                
                                                                    </div>
                </article>
                                
                
                
            </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[James Webb Space Telescope captures high-resolution image of Uranus (479 pts)]]></title>
            <link>https://webbtelescope.org/contents/news-releases/2023/news-2023-150</link>
            <guid>38695337</guid>
            <pubDate>Tue, 19 Dec 2023 13:37:28 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://webbtelescope.org/contents/news-releases/2023/news-2023-150">https://webbtelescope.org/contents/news-releases/2023/news-2023-150</a>, See on <a href="https://news.ycombinator.com/item?id=38695337">Hacker News</a></p>
Couldn't get https://webbtelescope.org/contents/news-releases/2023/news-2023-150: Error: Request failed with status code 403]]></description>
        </item>
        <item>
            <title><![CDATA[Volkswagen Will Bring Back Physical Buttons in New Cars (371 pts)]]></title>
            <link>https://insideevs.com/news/701296/vw-physical-controls-to-return/</link>
            <guid>38694886</guid>
            <pubDate>Tue, 19 Dec 2023 12:51:12 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://insideevs.com/news/701296/vw-physical-controls-to-return/">https://insideevs.com/news/701296/vw-physical-controls-to-return/</a>, See on <a href="https://news.ycombinator.com/item?id=38694886">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="post_box"> <div> <p><span data-time="1702909828"></span><span>Dec 18, 2023</span><span> at</span> 2:30pm ET</p>  </div> <div> <p>Volkswagen is still intent on marching its entire lineup of vehicles toward electrification in many of its markets. With that modernization of vehicle powertrains also comes more contemporary interiors; that means meshing design languages with current industry trends like minimalism, which involves ditching physical interior buttons with touch screen controls.</p> <section contenteditable="false" draggable="true" data-widget="widget_faq" data-noinit="1"> <h3><span><svg><use xlink:href="https://insideevs.com/design/dist/critical/icons/sprite-common-0-dc6b7de11d922cb73553899f27128303.svg#sqr-arrow"></use></svg></span>Get Fully Charged</h3>  </section> <p>Unfortunately for VW, that hasn't exactly been well received by consumers. Owners have pushed back against the German automaker moving controls to the large tablet-like infotainment touch screen on the dashboard and haptic-based steering wheel buttons, finally forcing the automaker to reverse its decision on going buttonless—and that all starts with the new <a href="https://insideevs.com/news/682384/vw-id2all-concept-walkaround-brand-ceo/" data-inline-widget="internal-links" data-type-id="0" data-params="%7B%22article_edition_id%22%3A%22682384%22%2C%22section%22%3A%221%22%2C%22alias%22%3A%22vw-id2all-concept-walkaround-brand-ceo%22%7D">ID.2all concept</a>.</p> <section contenteditable="false" draggable="true" data-widget="image" data-border="" data-id="7184833"> <span> <svg> <use xlink:href="https://insideevs.com/design/dist/critical/icons/sprite-common-4-aab71d103bb6c273cab9291678c627d9.svg#search"></use> </svg> </span> <picture> <source type="image/webp" data-srcset=" https://cdn.motor1.com/images/mgl/P3nPeA/s5/volkswagen-id.2all.webp 213w, https://cdn.motor1.com/images/mgl/P3nPeA/s6/volkswagen-id.2all.webp 445w, https://cdn.motor1.com/images/mgl/P3nPeA/s4/volkswagen-id.2all.webp 889w, https://cdn.motor1.com/images/mgl/P3nPeA/s3/volkswagen-id.2all.webp 1280w, https://cdn.motor1.com/images/mgl/P3nPeA/s2/volkswagen-id.2all.webp 1440w, https://cdn.motor1.com/images/mgl/P3nPeA/s1/volkswagen-id.2all.webp 1920w " sizes="(max-width: 767px) calc(100vw - 30px), (max-width: 1023px) calc(100vw - 50px), 649px"> <source type="image/jpeg" data-srcset=" https://cdn.motor1.com/images/mgl/P3nPeA/s5/volkswagen-id.2all.jpg 213w, https://cdn.motor1.com/images/mgl/P3nPeA/s6/volkswagen-id.2all.jpg 445w, https://cdn.motor1.com/images/mgl/P3nPeA/s4/volkswagen-id.2all.jpg 889w, https://cdn.motor1.com/images/mgl/P3nPeA/s3/volkswagen-id.2all.jpg 1280w, https://cdn.motor1.com/images/mgl/P3nPeA/s2/volkswagen-id.2all.jpg 1440w, https://cdn.motor1.com/images/mgl/P3nPeA/s1/volkswagen-id.2all.jpg 1920w " sizes="(max-width: 767px) calc(100vw - 30px), (max-width: 1023px) calc(100vw - 50px), 649px"> <img src="https://cdn.motor1.com/images/static/16x9-tr.png" alt="Volkswagen ID.2all" width="16" height="9" loading="lazy"> </picture> </section> <p>Volkswagen has been trying to fix its interiors for a few years now. Under former CEO Herbert Diess, the German automaker decided to follow in<a href="https://insideevs.com/news/696419/tesla-cybertruck-interior-video/" data-inline-widget="internal-links" data-type-id="0" data-params="%7B%22article_edition_id%22%3A%22696419%22%2C%22section%22%3A%221%22%2C%22alias%22%3A%22tesla-cybertruck-interior-video%22%7D"> Tesla's footsteps</a> and centralize a vast majority of its controls to the infotainment screen. It also removed the physical buttons from its steering wheels and replaced them with touch-sensitive capacitive buttons instead. This move, according to VW, "frustrated customers who shouldn't be frustrated."</p> <p>The automaker has since reverted its since on the steering wheel buttons and is looking to now claw back its reputation for something that its current CEO, Thomas Schäfer, says "did a lot of damage" to the brand.</p> <p>That change all starts with the Volkswagen ID 2. Recently, VW's interior designer, Darius Watola, spoke to <a href="https://www.autocar.co.uk/car-news/new-cars/volkswagen-id-2" target="_blank" rel="noopener">Autocar</a> on the ID.2 concept's take on the company's design language for future vehicle interiors. Watola confirmed that the concept showed a new approach for all models across the VW brand which was revamped due to customer feedback.</p> <p>A row of physical, backlit buttons now sits directly below the <a href="https://insideevs.com/news/700047/bmw-3d-touchscreen-prototype/" data-inline-widget="internal-links" data-type-id="0" data-params="%7B%22article_edition_id%22%3A%22700047%22%2C%22section%22%3A%221%22%2C%22alias%22%3A%22bmw-3d-touchscreen-prototype%22%7D">touch screen</a> on the ID.2 concept. The buttons provide customers with easy access to commonly used HVAC controls, which—while it doesn't address <em>every</em> control in the car—is a step in the right direction. The car will also get a manual volume button and a large center knob (a la BMW iDrive) which provides complementary controls for other aspects of the vehicle.</p> <p>The controls also clearly lean on the importance of feel, even featuring metal knurling so occupants can easily feel them without taking their eyes off of the road.</p> <section contenteditable="false" draggable="true" data-widget="video"><img draggable="false" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABAAAAAJCAYAAAA7KqwyAAAAGXRFWHRTb2Z0d2FyZQBBZG9iZSBJbWFnZVJlYWR5ccllPAAAABpJREFUeNpi/P//PwMlgImBQjBqwLAwACDAAOVfAw9/ZDvcAAAAAElFTkSuQmCC" alt=""> <p><iframe allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen="" frameborder="0" height="315" title="YouTube video player" width="560" data-src="https://www.youtube.com/embed/I1MgIf7UP-0?si=kfNSJ8ZneVK3jwD_" data-src-param=""></iframe></p> </section> <p>"Once you have it, don't touch it again," said Volkswagen CEO Thomas Schäfer in an interview with <a href="https://www.autocar.co.uk/car-news/new-cars/volkswagen-ceo-fixing-frustrating-interiors-did-lot-damage" target="_blank" rel="noopener"><em>Autocar</em></a> earlier this year. "Bloody leave it. Don't confuse our customers every time a new model comes out and something is completely different. Optimize it. Bring into the future. But don't change buttons from here to there, to there and here."</p> <p>It's no secret that consumers have pushed back on automakers who simply slapped an iPad on the dashboard in place of physical controls. Heck, <a href="https://slate.com/business/2023/04/cars-buttons-touch-screens-vw-porsche-nissan-hyundai.html" target="_blank" rel="noopener">Volkswagen isn't the first automaker to change its stance back to the old-school physical button approach</a>, either. Let's be real—if Volkswagen is really trying to regain its relevancy in markets like the United States (even if this change was geared at the European market), it has to take customer feedback seriously. And it looks like the Germans are starting to do exactly that.</p> <section contenteditable="false" draggable="true" data-widget="related-content" data-widget-size="content" data-params="%7B%22type_id%22%3A0%2C%22title_id%22%3A%22%22%2C%22items%22%3A%5B%7B%22article_edition_id%22%3A%22700676%22%2C%22title%22%3A%22You%20Can%20Get%20An%20Incredible%20Deal%20On%20A%20Volkswagen%20ID.4%20Right%20Now%22%2C%22alias%22%3A%22vw-id4-holiday-deals%22%2C%22section%22%3A%221%22%2C%22is_video%22%3A%220%22%2C%22images%22%3A%7B%22s5%22%3A%22https%3A%2F%2Fcdn.motor1.com%2Fimages%2Fmgl%2Fx7xyk%2Fs5%2Fid.4-charging.jpg%22%7D%7D%2C%7B%22article_edition_id%22%3A%22700247%22%2C%22title%22%3A%22This%20Volkswagen%20ID.3%20Lost%2010%25%20Of%20Its%20Battery%20Capacity%20After%2030%2C000%20Miles%22%2C%22alias%22%3A%22vw-id3-battery-degradation-30k-miles-video%22%2C%22section%22%3A%221%22%2C%22is_video%22%3A%221%22%2C%22images%22%3A%7B%22s5%22%3A%22https%3A%2F%2Fcdn.motor1.com%2Fimages%2Fmgl%2FNGj48j%2Fs5%2Fvolkswagen-id.3-battery-degradation-after-30-000-miles-source-bjorn-nyland-youtube.jpg%22%7D%7D%5D%7D"> <p>More VW News</p>  </section> </div> </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[My cat water fountain comes with a spicy USB power adapter (273 pts)]]></title>
            <link>https://ounapuu.ee/posts/2023/12/19/spicy-usb-adapter/</link>
            <guid>38694549</guid>
            <pubDate>Tue, 19 Dec 2023 12:03:36 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://ounapuu.ee/posts/2023/12/19/spicy-usb-adapter/">https://ounapuu.ee/posts/2023/12/19/spicy-usb-adapter/</a>, See on <a href="https://news.ycombinator.com/item?id=38694549">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
      
      <p>It turns out that you can’t trust any USB type A power adapter to be within spec.</p>







  




<figure>
    
    <a href="https://ounapuu.ee/posts/2023/12/19/spicy-usb-adapter/media/cover.jpg">
        <img src="https://ounapuu.ee/posts/2023/12/19/spicy-usb-adapter/media/cover_hu274ae1922594f3b973c5b875085287a1_706710_800x0_resize_q80_box.jpg" width="800" height="375" alt="image">
    </a>
	
</figure>

<p>I have a <a href="https://www.catit.com/products/drinking-fountains/flower-fountain/">Catit Flower Fountain</a>
for my two adorable cats. The idea of a water fountain for cats may sound odd,
but having one really helps with cats staying hydrated and that alone avoids all
sorts of health issues.</p>
<p>At one point I wanted to see if I could create a sort of a DIY UPS for the
water fountain. It would be quite bad if I was at work and a power outage results
in cats not being able to drink water (they don’t really care for normal water
bowls after getting the fountain). I had some battery banks available for testing,
and I noticed that the pump for the water fountain is powered over a USB type A cable.</p>
<p>Should be easy, right?</p>
<p>Apparently not.</p>
<p>I tried multiple different power banks between the water fountain and the
USB power adapter that came with it, and all of them would work for a bit
and turn off after some time. I didn’t think much of it back then, but I did
notice that two of the power banks I used started glitching out during
normal use elsewhere.</p>
<p>Months later, I attached an IKEA power strip to the side of my work desk
to make charging various things easier. It also has two USB type A ports and the
water fountain was near the desk temporarily, so I plugged it in there. It worked,
but I noticed that the water fountain was quieter now, the “hum” that it makes
was almost gone. That made me curious, so I used the original adapter again
and the “hum” was there again.</p>
<p>I took a look at the original power adapter specs to see if there’s a difference
in the amount of current that these two different USB power sources provide.
What I discovered instead was that the power adapter that comes with the fountain
outputs a solid <em><strong>7.5V</strong></em>. USB type A ports typically provide about 5V, with a maximum
of 5.25V from my observations in the real world.</p>







  




<figure>
    
    <a href="https://ounapuu.ee/posts/2023/12/19/spicy-usb-adapter/media/image0.jpg">
        <img src="https://ounapuu.ee/posts/2023/12/19/spicy-usb-adapter/media/image0_hu274ae1922594f3b973c5b875085287a1_176314_800x0_resize_q80_box.jpg" width="800" height="499" alt="Yikes.">
    </a>
    <figcaption>
      Yikes.
    </figcaption>
    
</figure>

<p>7.5V over USB type A is <em>probably</em> not safe with other devices, especially since a normal person
only sees a USB port on the adapter and thinks that it is perfectly safe to use
it to charge their phone or other devices. Yes, properly implemented USB type <em><strong>C</strong></em>
ports can negotiate all sorts of voltages, but this is not one of them.</p>
<p>Probably explains why my power banks are acting odd now and glitching out.</p>
<p>This is why I have trust issues.</p>
<h3 id="2023-12-19-update">2023-12-19 update</h3>
<p>By popular demand, here are the two adorable cats.</p>







  




<figure>
    
    <a href="https://ounapuu.ee/posts/2023/12/19/spicy-usb-adapter/media/cats.jpg">
        <img src="https://ounapuu.ee/posts/2023/12/19/spicy-usb-adapter/media/cats_hu380fbf338ff34647e66732a5f1fb677a_1888448_800x0_resize_q80_box.jpg" width="800" height="600" alt="Tux and Põssa. Põssa can be roughly translated to &quot;Piggy&quot; in English. I'll let you guess how he got that name.">
    </a>
    <figcaption>
      Tux and Põssa. Põssa can be roughly translated to "Piggy" in English. I'll let you guess how he got that name.
    </figcaption>
    
</figure>


<p>If you’re not a spammer,
<a href="mailto:ihavesomethoughtsonyourblog@ounapuu.ee">just send me an e-mail!</a></p>
<p>Places where you can discuss this post:</p>
<ul>
<li>
<p><a href="https://news.ycombinator.com/item?id=38694549">Hacker News</a></p>
</li>
<li>
<p><a href="https://www.linkedin.com/posts/%F0%9F%94%A5-herman-%C3%B5unapuu-600516152_my-cat-water-fountain-comes-with-a-spicy-activity-7142846707364528128-Jt7U">LinkedIn</a></p>
</li>
</ul>

    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[FOSDEM 2024 Schedule (152 pts)]]></title>
            <link>https://fosdem.org/2024/schedule/</link>
            <guid>38694390</guid>
            <pubDate>Tue, 19 Dec 2023 11:36:37 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://fosdem.org/2024/schedule/">https://fosdem.org/2024/schedule/</a>, See on <a href="https://news.ycombinator.com/item?id=38694390">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="main">
      
      
      

      
        
      

      

<div>
  <div>
    <h2>Quick links:</h2>
<ul>
  <li>Timetables: 
    <a href="https://fosdem.org/2024/schedule/day/saturday/">Saturday</a>, <a href="https://fosdem.org/2024/schedule/day/sunday/">Sunday</a>
  </li>
  <li>Overview of <a href="https://fosdem.org/2024/schedule/roomtracks/">rooms per days</a></li>
  <li>Mobile apps: <a href="https://fosdem.org/2024/schedule/mobile/">many platforms</a></li>
  <li>What's on now: <a href="https://fosdem.org/2024/schedule/streaming/">Live streaming</a></li>
  <li>Booklet: <a href="https://fosdem.org/2024/assets/booklet/booklet_2023-9c6039f1cdbcccb3cf6157da8dec2f99a3f54f7d4b0eaf6fbbfcbed55e0af05d.pdf">Info</a> </li>
  <li>Programme <a href="https://fosdem.org/2024/schedule/amendments/">Schedule amendments</a></li>

  <li>Printable PDF:
  
    <a href="https://fosdem.org/2024/schedule/pdf/a4.pdf">A4</a>
  
    <a href="https://fosdem.org/2024/schedule/pdf/a3.pdf">A3</a>
  

</li></ul>
    </div>
    <div>


<p>
  Every year, FOSDEM hosts a wide variety of activities.
  This page gives an overview with links to further information about
  scheduled events. <b>All times CET (UTC+1).</b>
</p>

<p>
  FOSDEM is a rather busy conference.

  This edition features 514 speakers, 482
  events, and 65 tracks.

  We do our best to provide you with as much information and navigation options
  about the schedule as we can.
</p>

<p>
  Activities take place in <a href="https://fosdem.org/2024/schedule/rooms/">

  32

  rooms</a>.
  An <a href="https://fosdem.org/2024/schedule/roomtracks/">overview of the room
  occupation by track</a> is available, too.
</p>

<p>
  There are essentially the following categories of sessions and activities:

  <a href="#keynotes">keynotes</a>,

  <a href="#maintracks">main tracks</a>,
  <a href="#devrooms">developer rooms</a>,
  <a href="#lightningtalks">lightning talks</a>,
  <a href="#stands">stands</a> and
  <a href="#bofs">BOFs</a>.
  <!--a href="#certification">certification exams</a-->
</p>

<p>
  For a complete (but crowded) overview, there is also a
  <a href="https://fosdem.org/2024/schedule/events/">page that lists all events</a>.<br>

  A <a href="https://fosdem.org/2024/schedule/speakers/">list of all speakers</a> is available, too.
</p>

  <p>During the event, the <a href="https://fosdem.org/2024/schedule/streaming/">live streaming page</a> is also 
  updated every few minutes to show you what is currently scheduled in each room.</p>
    </div>
</div>


<h3>Keynotes</h3>
<p>
  Like almost every other conference, FOSDEM invites speakers to gently start
  the day with talks concerning slightly less technical, but nevertheless
  interesting topics from the Open Source realm.
  The closing keynote at the end of the conference helps everyone digest the
  vast amount of insightful information collected over the weekend.
</p>




<h3>Main tracks</h3>
<p>
  The <q>main tracks</q> consist of series of talks that are organised by topic,
  where the FOSDEM program committee selects suggestions and actively invites
  speakers on those topics.
</p>


<p>
  For this edition the main tracks are:
</p>

<table>
  <thead>
    <tr>
      <th>Track</th>
      
        <th>Saturday</th>
      
        <th>Sunday</th>
      
    </tr>
  </thead>
  <tbody>
    
    <tr>
      <td><a href="https://fosdem.org/2024/schedule/track/main/">Main Track</a></td>
      
      
      <td>-</td>
      
      
      
      <td>-</td>
      
      
      </tr>
    
  </tbody>
</table>


<h3><a name="devrooms" id="devrooms"></a>Developer rooms</h3>
<p>
  The vast majority of events (talks, hacking sessions, open discussions) are
  held in so-called <q>developer rooms</q> (<q>devrooms</q>), which are
  organized and managed by open source projects themselves, or even
  associations between several such projects on a common topic in order to
  foster collaboration.
</p>

<p>
  Here is the list of the devrooms that are present at this edition of FOSDEM,
  in alphabetical order:
</p>
<div>
        
        
        <div>
            <ul>
                
                <li><a href="https://fosdem.org/2024/schedule/track/ai_ml/">AI and Machine Learning</a></li>
                
                <li><a href="https://fosdem.org/2024/schedule/track/api/">APIs &amp; friends</a></li>
                
                <li><a href="https://fosdem.org/2024/schedule/track/collaboration-and_content-management/">Collaboration and Content Management</a></li>
                
                <li><a href="https://fosdem.org/2024/schedule/track/community/">Community</a></li>
                
                <li><a href="https://fosdem.org/2024/schedule/track/confidential-computing/">Confidential Computing</a></li>
                
                <li><a href="https://fosdem.org/2024/schedule/track/containers/">Containers</a></li>
                
                <li><a href="https://fosdem.org/2024/schedule/track/dns/">DNS</a></li>
                
                <li><a href="https://fosdem.org/2024/schedule/track/debuggers-and-analysis/">Debuggers and analysis tools</a></li>
                
                <li><a href="https://fosdem.org/2024/schedule/track/declarative-and-minimalistic-computing/">Declarative and Minimalistic Computing</a></li>
                
                <li><a href="https://fosdem.org/2024/schedule/track/open-website-alliance/">Designing Futures of FOSS Content Management with the Open Website Alliance</a></li>
                
                <li><a href="https://fosdem.org/2024/schedule/track/distributions/">Distributions</a></li>
                
                <li><a href="https://fosdem.org/2024/schedule/track/embedded-mobile-and-automotive/">Embedded, Mobile and Automotive</a></li>
                
                <li><a href="https://fosdem.org/2024/schedule/track/emulator/">Emulator</a></li>
                
                <li><a href="https://fosdem.org/2024/schedule/track/energy/">Energy: Reimagining this Ecosystem through Open Source</a></li>
                
                <li><a href="https://fosdem.org/2024/schedule/track/erlang/">Erlang, Elixir, Gleam and Friends</a></li>
                
                <li><a href="https://fosdem.org/2024/schedule/track/junior/">FOSDEM junior</a></li>
                
                <li><a href="https://fosdem.org/2024/schedule/track/educational/">FOSS Educational Programming Languages</a></li>
                
                <li><a href="https://fosdem.org/2024/schedule/track/foss-on-mobile-devices/">FOSS on Mobile Devices</a></li>
                
                <li><a href="https://fosdem.org/2024/schedule/track/free-java/">Free Java</a></li>
                
                <li><a href="https://fosdem.org/2024/schedule/track/gcc/">GCC</a></li>
                
            </ul>
        </div>
        
        <div>
            <ul>
                
                <li><a href="https://fosdem.org/2024/schedule/track/go/">Go</a></li>
                
                <li><a href="https://fosdem.org/2024/schedule/track/graphics/">Graphics</a></li>
                
                <li><a href="https://fosdem.org/2024/schedule/track/hpc-big-data-data-science/">HPC, Big Data &amp; Data Science</a></li>
                
                <li><a href="https://fosdem.org/2024/schedule/track/identity-and-access-management/">Identity and Access Management</a></li>
                
                <li><a href="https://fosdem.org/2024/schedule/track/javascript/">JavaScript</a></li>
                
                <li><a href="https://fosdem.org/2024/schedule/track/kernel/">Kernel</a></li>
                
                <li><a href="https://fosdem.org/2024/schedule/track/llvm/">LLVM</a></li>
                
                <li><a href="https://fosdem.org/2024/schedule/track/legal/">Legal and Policy Issues</a></li>
                
                <li><a href="https://fosdem.org/2024/schedule/track/libre-soc-fpga-and-vlsi/">Libre-SOC, FPGA and VLSI</a></li>
                
                <li><a href="https://fosdem.org/2024/schedule/track/matrix/">Matrix</a></li>
                
                <li><a href="https://fosdem.org/2024/schedule/track/microkernel/">Microkernel and Component-based OS</a></li>
                
                <li><a href="https://fosdem.org/2024/schedule/track/modern-email/">Modern Email</a></li>
                
                <li><a href="https://fosdem.org/2024/schedule/track/monitoring-observability/">Monitoring &amp; Observability</a></li>
                
                <li><a href="https://fosdem.org/2024/schedule/track/mozilla/">Mozilla</a></li>
                
                <li><a href="https://fosdem.org/2024/schedule/track/network/">Network</a></li>
                
                <li><a href="https://fosdem.org/2024/schedule/track/nix-devroom/">Nix and NixOS</a></li>
                
                <li><a href="https://fosdem.org/2024/schedule/track/open-hardware-and-cadcam/">Open Hardware and CAD/CAM</a></li>
                
                <li><a href="https://fosdem.org/2024/schedule/track/open-media/">Open Media</a></li>
                
                <li><a href="https://fosdem.org/2024/schedule/track/open-research/">Open Research</a></li>
                
                <li><a href="https://fosdem.org/2024/schedule/track/open-source-design/">Open Source Design</a></li>
                
            </ul>
        </div>
        
        <div>
            <ul>
                
                <li><a href="https://fosdem.org/2024/schedule/track/open-source-firmware-bmc-and-bootloader/">Open Source Firmware, BMC and Bootloader</a></li>
                
                <li><a href="https://fosdem.org/2024/schedule/track/eu-policy/">Open Source In The European Legislative Landscape</a></li>
                
                <li><a href="https://fosdem.org/2024/schedule/track/perl-raku/">Perl and Raku</a></li>
                
                <li><a href="https://fosdem.org/2024/schedule/track/postgresql/">PostgreSQL</a></li>
                
                <li><a href="https://fosdem.org/2024/schedule/track/public-code-and-digital-public-goods/">Public Code and Digital Public Goods</a></li>
                
                <li><a href="https://fosdem.org/2024/schedule/track/python-devroom/">Python Devroom</a></li>
                
                <li><a href="https://fosdem.org/2024/schedule/track/risc-v-devroom/">RISC-V</a></li>
                
                <li><a href="https://fosdem.org/2024/schedule/track/railways-and-open-transport/">Railways and Open Transport</a></li>
                
                <li><a href="https://fosdem.org/2024/schedule/track/real-time-communications/">Real Time Communications (RTC)</a></li>
                
                <li><a href="https://fosdem.org/2024/schedule/track/retrocomputing/">Retrocomputing</a></li>
                
                <li><a href="https://fosdem.org/2024/schedule/track/ruby/">Ruby</a></li>
                
                <li><a href="https://fosdem.org/2024/schedule/track/rust/">Rust</a></li>
                
                <li><a href="https://fosdem.org/2024/schedule/track/software-bill-of-materials/">Software Bill of Materials</a></li>
                
                <li><a href="https://fosdem.org/2024/schedule/track/software-defined-storage/">Software Defined Storage</a></li>
                
                <li><a href="https://fosdem.org/2024/schedule/track/radio/">Software-Defined Radio and Amateur Radio</a></li>
                
                <li><a href="https://fosdem.org/2024/schedule/track/testing-continuous-delivery/">Testing and Continuous delivery</a></li>
                
                <li><a href="https://fosdem.org/2024/schedule/track/tool-the-docs/">Tool the docs</a></li>
                
                <li><a href="https://fosdem.org/2024/schedule/track/translations/">Translations</a></li>
                
                <li><a href="https://fosdem.org/2024/schedule/track/virtualization-and-cloud-infrastructure/">Virtualization and Cloud Infrastructure</a></li>
                
                <li><a href="https://fosdem.org/2024/schedule/track/web-performance/">Web Performance</a></li>
                
            </ul>
        </div>
        
    </div>


<h3>Lightning talks</h3>
<p>
  The lightning talk is a very popular format, used at many conferences, where
  speakers have a mere 15 minutes at their disposal to showcase an open
  source project, an idea, or a concept thereof.
</p>

<p>
  While that brief lapse of time may seem awkward, it almost always leads the
  presenters to concentrate on the absolute essence and what is really
  important, which is why it is often a much appreciated approach, as is the
  wide variety of the topics.
</p>


<p>
  There are currently <a href="https://fosdem.org/2024/schedule/track/lightning_talks/">1
  lightning talks in the schedule</a>.
</p>


<h3>Stands</h3><p>
  Stands offer a unique chance to get in touch with developers or project
  members, and discover at a glance what they do.
</p>


<p>
  A list of projects that will be present with a stand can be found on the
  <a href="https://fosdem.org/2024/stands/">FOSDEM stands page</a>.
</p>


<h3>BOFs</h3><p>
  BOF stands for Birds Of a Feather who, as the saying goes, flock together.
  FOSDEM has three meeting rooms that may be booked in 30 or 60 minute blocks
  for discussions.  All the meetings are public so anyone who is interested can
  attend if there is enough space. 
</p>


<div>
        
        <div>
            <ul>
                
                <li><a href="https://fosdem.org/2024/schedule/track/bofa/">BOF - Track A</a></li>
                
            </ul>
        </div>
        
        <div>
            <ul>
                
                <li><a href="https://fosdem.org/2024/schedule/track/bofb/">BOF - Track B</a></li>
                
            </ul>
        </div>
        
        <div>
            <ul>
                
                <li><a href="https://fosdem.org/2024/schedule/track/bofc/">BOF - Track C</a></li>
                
            </ul>
        </div>
        
    </div>



<!--a name="certification"></a>
<h3> Certification Exams</h3>
<p>
  Several certification organizations provide FOSDEM visitors with the
  opportunity of taking certification exams during the conference.
</p>
<p>
  Further details are available on the <a href="page:/certification/">certification page</a>.
</p>

  <p><span class="label label-info">Certification exams have not been scheduled yet.</span></p>

-->

<h3> The FOSDEM Fringe</h3>
<p>
  The FOSDEM Fringe consists of independent events involving free and open source software taking place in the days around the FOSDEM weekend.  Why not extend your trip?
</p>
<p>
  Further details are available on the <a href="https://fosdem.org/2024/fringe/">FOSDEM Fringe page</a>.
</p>

<h3>Mobile Apps</h3>
<p>People have submitted a variety of <a href="https://fosdem.org/2024/schedule/mobile/">apps for mobile devices</a> that display the FOSDEM schedule.
</p>

<h3>Raw schedule data</h3>
<p>
The schedule data is available in:
</p><ul>
<li>Pentabarf <a href="https://fosdem.org/2024/schedule/xml">XML</a>
</li><li><a href="https://fosdem.org/2024/schedule/ical">iCal</a>
</li><li><a href="https://fosdem.org/2024/schedule/xcal">xCal</a>
</li></ul>


    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Hardware Intrinsics in .NET 8 (101 pts)]]></title>
            <link>https://devblogs.microsoft.com/dotnet/dotnet-8-hardware-intrinsics/</link>
            <guid>38693948</guid>
            <pubDate>Tue, 19 Dec 2023 10:27:22 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://devblogs.microsoft.com/dotnet/dotnet-8-hardware-intrinsics/">https://devblogs.microsoft.com/dotnet/dotnet-8-hardware-intrinsics/</a>, See on <a href="https://news.ycombinator.com/item?id=38693948">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="featured">
                         <p>
            December 11th, 2023</p><!-- .entry-meta -->
        <p>.NET has a long history of providing access to additional hardware functionality via APIs that are intrinsically understood by the JIT compiler. This started on .NET Framework back in 2014 and expanded with the introduction of .NET Core 3.0 in 2019. Since then, the runtime has iteratively provided more APIs and taken better advantage of this in each release.</p>
<p>As a brief overview:</p>
<ul>
<li>2014 – .NET 4.5.2 – First APIs exposed in the <code>System.Numerics</code> namespace
<ul>
<li>Introduces <code>Vector&lt;T&gt;</code></li>
<li>Introduces <code>Vector2</code>, <code>Vector3</code>, <code>Vector4</code>, <code>Matrix4x4</code>, <code>Quaternion</code>, and <code>Plane</code></li>
<li>64-bit only</li>
<li>See also: https://devblogs.microsoft.com/dotnet/the-jit-finally-proposed-jit-and-simd-are-getting-married/</li>
</ul>
</li>
<li>2019 – .NET Core 3.0 – First APIs exposed in the <code>System.Runtime.Intrinsics</code> namespace
<ul>
<li>Introduces <code>Vector128&lt;T&gt;</code> and <code>Vector256&lt;T&gt;</code></li>
<li>Introduces <code>Sse</code>, <code>Sse2</code>, <code>Sse3</code>, <code>Ssse3</code>, <code>Sse41</code>, <code>Sse42</code>, <code>Avx</code>, <code>Avx2</code>, <code>Fma</code>, <code>Bmi1</code>, <code>Bmi2</code>, <code>Lzcnt</code>, <code>Popcnt</code>, <code>Aes</code>, and <code>Pclmul</code> for <code>x86</code>/<code>x64</code></li>
<li>32-bit and 64-bit support</li>
<li>See also: https://devblogs.microsoft.com/dotnet/hardware-intrinsics-in-net-core/</li>
</ul>
</li>
<li>2020 – .NET 5 – Arm support added to the <code>System.Runtime.Intrinsics</code> namespace
<ul>
<li>Introduces <code>Vector64&lt;T&gt;</code></li>
<li>Introduces <code>AdvSimd</code>, <code>ArmBase</code>, <code>Dp</code>, <code>Rdm</code>, <code>Aes</code>, <code>Crc32</code>, <code>Sha1</code>, and <code>Sha256</code> for <code>Arm</code>/<code>Arm64</code></li>
<li>Introduces <code>X86Base</code> for <code>x86</code>/<code>x64</code></li>
<li>See also: https://devblogs.microsoft.com/dotnet/announcing-net-5-0-preview-7/</li>
</ul>
</li>
<li>2021 – .NET 6 – Codegen and infrastructure improvements
<ul>
<li>Introduces <code>AvxVnni</code> for <code>x86</code>/<code>x64</code></li>
<li>Rewrites the <code>System.Numerics</code> implementation to use <code>System.Runtime.Intrinsics</code></li>
<li>See also: https://devblogs.microsoft.com/dotnet/performance-improvements-in-net-6/</li>
</ul>
</li>
<li>2022 – .NET 7 – Support for writing cross platform algorithms
<ul>
<li>Introduces significant new functionality on the <code>Vector64&lt;T&gt;</code>, <code>Vector128&lt;T&gt;</code>, and <code>Vector256&lt;T&gt;</code> types that works across platforms</li>
<li>Introduces <code>X86Serialize</code> for <code>x86</code>/<code>x64</code></li>
<li>Brings the API surface exposed by the above vector types and <code>Vector&lt;T&gt;</code> to a parity</li>
<li>See also: https://devblogs.microsoft.com/dotnet/performance_improvements_in_net_7/</li>
</ul>
</li>
<li>2023 – .NET 8 – <a href="https://webassembly.org/">Wasm</a> support and AVX-512
<ul>
<li>Introduces <code>PackedSimd</code> and <code>WasmBase</code> for <code>Wasm</code></li>
<li>Introduces <code>Vector512&lt;T&gt;</code></li>
<li>Introduces <code>Avx512F</code>, <code>Avx512BW</code>, <code>Avx512CD</code>, <code>Avx512DQ</code>, and <code>Avx512Vbmi</code> for <code>x86</code>/<code>x64</code></li>
<li>See also: The rest of this blog post</li>
</ul>
</li>
</ul>
<p>Because of this work, with every release .NET libraries and applications gain more power to take advantage of the underlying hardware. In this post I’ll cover in depth what we introduced in .NET 8 and the type of functionality it enables.</p>
<h2 id="webassembly-support">WebAssembly Support</h2>
<p><a href="https://devblogs.microsoft.com/dotnet/extending-web-assembly-to-the-cloud/">WebAssembly</a>, or Wasm for short, is essentially code that runs in your browser and which allows a much higher performance profile than typical interpreted scripting support. As a platform, Wasm has started providing underlying SIMD (Single Instruction, Multiple Data) support so that core algorithms can be accelerated and .NET has correspondingly opted to expose support for this functionality via hardware intrinsics.</p>
<p>This support is very similar to the foundations that other platforms provide and so we won’t go into it in significant detail. Rather, you can simply expect that your existing cross platform algorithms using <code>Vector128&lt;T&gt;</code> will implicitly light up where supported. If you want to take more direct advantage of functionality that is unique to Wasm, then you can explicitly use the APIs exposed by the <code>PackedSimd</code> and <code>WasmBase</code> classes in the <code>System.Runtime.Intrinsics.Wasm</code> namespace.</p>
<h2 id="avx-512-support">AVX-512 Support</h2>
<p>AVX-512 is a new feature set provided for x86 and x64 computers. It brings along with it a large set of new instructions and hardware functionality that wasn’t previously available including support for 16 additional SIMD registers, dedicated masking, and operating on 512-bits of data at a time. Access to this functionality requires a relatively new processor, namely it requires Skylake-X or newer from Intel and Zen4 or newer from AMD. Because of this, the number of users that can take advantage of this new functionality is smaller, but the improvements it can bring to that hardware are still significant and make it worthwhile to support for data heavy workloads. Additionally, the JIT will opportunistically utilize these instructions for existing SIMD code where it determines benefit to exist. Some examples include:</p>
<ul>
<li>using <code>vpternlog</code> instead of <code>and, andn, or</code> when a bitwise conditional select is done (<code>Vector128.ConditionalSelect</code>)</li>
<li>using the EVEX encoding to fit more operations into less bytes of code, such as for embedded broadcasts (<code>x + Vector128.Create(5)</code>)</li>
<li>using newer instructions where support now exists with AVX-512, such as for full-width shuffling and many <code>long</code>/<code>ulong</code> (<code>Int64</code>/<code>UInt64</code>) operations</li>
<li>there were other improvements, that are not listed here, as well and you can expect even more to be added over time
<ul>
<li>some cases such as <code>Vector&lt;T&gt;</code> allowing scaling to 512-bits were not completed in .NET 8</li>
</ul>
</li>
</ul>
<p>In order to support the new vector size of 512-bits, .NET introduced the <code>Vector512&lt;T&gt;</code> type. This exposes the same general API surface as the other fixed-sized vector types such as <code>Vector256&lt;T&gt;</code>. It likewise continues exposing the <code>Vector512.IsHardwareAccelerated</code> property that allows you to determine whether the general logic should be accelerated in the hardware or if it will end up emulating the behavior via a software fallback.</p>
<p>Vector512 is accelerated with AVX-512 by default on Ice Lake and newer hardware (and thus <code>Vector512.IsHardwareAccelerated</code> reports <code>true</code>), where AVX-512 instructions do not cause the CPU to significantly downclock; where-as utilizing AVX-512 instructions can cause more significant downclocking on Skylake-X, Cascade Lake, and Cooper Lake based hardware (see also <code>2.5.3 Skylake Server Power Management</code> in the <code>Intel® 64 and IA-32 Architectures Optimization Reference Manual: Volume 1</code>). While this is ultimately beneficial for large workloads, it can negatively impact other smaller workloads and as such we default to reporting <code>false</code> for <code>Vector512.IsHardwareAccelerated</code> on these platforms. <code>Avx512F.IsSupported</code> will still report true and the underlying implementation of <code>Vector512</code> will still utilize <code>AVX-512</code> instructions if called directly. This allows workloads to take advantage of the functionality where they know it to be explicitly beneficial without accidentally causing a negative impact to others.</p>
<h3 id="special-thanks">Special Thanks</h3>
<p>This functionality was made possible via significant contributions by our friends at Intel. The .NET team and Intel have collaborated many times over the years and this continued by us working together on the overall design and implementation, allowing the AVX-512 support to land in .NET 8.</p>
<p>There was also a great deal of input and validation from the .NET community that helped achieve success and make the release all the better.</p>
<p>If you would like to contribute or provide input, please join us in the <a href="https://github.com/dotnet/runtime">dotnet/runtime</a> repos on GitHub, and tune into API Review on the <a href="https://www.youtube.com/@NETFoundation/streams">.NET Foundation YouTube</a> channel by following our <a href="https://apireview.net/schedule">schedule</a> where you can see us discuss new additions to the .NET Libraries and even provide your own input via the chat channel.</p>
<h3 id="its-not-just-512-bits">It’s not just 512-bits?</h3>
<p>Contrary to the name, AVX-512 is not just about 512-bit support. The additional registers, masking support, embedded rounding or broadcast support, and new instructions also all exist for 128-bit and 256-bit vectors. This means that your existing workloads can implicitly get better and you can take explicit advantage of newer functionality where such implicit light up is not possible.</p>
<p>When SSE was first introduced in 1999 on the Intel Pentium III, it provided 8 registers each 128-bits in length. These registers were known as <code>xmm0</code> through <code>xmm7</code>. When the x64 platform was later introduced in 2003 on the AMD Athlon 64, it then provided 8 additional registers that were accessible to 64-bit code. These registers were named <code>xmm8</code> through <code>xmm15</code>. This initial support used a simple encoding scheme that worked in a very similar manner to the general purpose instructions and only allowed for 2 registers to be specified. For something like addition which requires 2 inputs, this meant that one of the registers acted as both an input and an output. This meant that if your input and output needed to be different, you needed 2 instructions to complete the operation. Effectively, your <code>z = x + y</code> would become <code>z = x; z += y</code>. At the high level these behave the same, but at the low level there is 2 rather than 1 step to make it happen.</p>
<p>This was then further expanded in 2011 when Intel introduced AVX on the Sandy Bridge based processors by expanding the support to 256-bits. These newer registers were named <code>ymm0</code> through <code>ymm15</code>, with only registers up to <code>ymm7</code> being accessible to 32-bit code. This also introduced a new encoding known as <code>VEX</code> (Vector Extensions) which allowed for 3 registers to be encoded. This meant that you could encode <code>z = x + y</code> directly and didn’t have to break it into 2 separate steps.</p>
<p>AVX-512 was then introduced by Intel in 2017 with the Skylake-X based processors. This expanded that support to 512-bits and named the registers <code>zmm0</code> through <code>zmm15</code>. It also introduced 16 new registers, aptly named <code>zmm16</code> through <code>zmm31</code> and which also have <code>xmm16-xmm31</code> and <code>ymm16-ymm31</code> variants. As with the previous cases, only registers up to <code>zmm7</code> are accessible to 32-bit code. It introduced 8 new registers, named <code>k0</code> through <code>k7</code>, designed to support “masking” and another new encoding named <code>EVEX</code> (Enhanced Vector Extensions) which allows all this new information to be expressed. The EVEX encoding also has other features that allow more common information and operations to be expressed in a more compact fashion. This can help decrease code size while improving performance.</p>
<h3 id="what-new-instructions-exist">What new instructions exist?</h3>
<p>There is a lot of new functionality, far too much to cover everything in this blog post. But some of the most notable new instructions provide things like:</p>
<ul>
<li>Support for doing operations like <code>Abs</code>, <code>Max</code>, <code>Min</code>, and shifting on 64-bit integers – previously this functionality had to be emulated using multiple instructions</li>
<li>Support for doing conversions between unsigned integers and floating-point types</li>
<li>Support for working with floating-point edge cases</li>
<li>Support for fully rearranging the elements in a vector or multiple vectors</li>
<li>Support for doing 2 bitwise operations in a single instruction</li>
</ul>
<p>The 64-bit integer support is notable because it means working with 64-bit data doesn’t need to use a slower or alternative code sequence to support the same functionality. It makes it much easier to just write your code and expect it to behave the same regardless of the underlying data type you’re working with.</p>
<p>The floating-point to unsigned integer conversion support is notable for similar reasons. Converting from <code>double</code> to <code>long</code> required a single instruction, but converting from <code>double</code> to <code>ulong</code> required many instructions. With AVX-512 this becomes a single instruction and allows users to get the expected performance when working with unsigned data. This can be common in various image processing or Machine Learning scenarios.</p>
<p>The expanded support for floating-point data is one of my favorite features of AVX-512. Some examples include the ability to extract the unbiased exponent (<code>Avx512F.GetExponent</code>) or the normalized mantissa (<code>Avx512F.GetMantissa</code>), to round a floating-point value to a specific number of fraction bits (<code>Avx512F.RoundScale</code>), to multiply a value by 2^x (<code>Avx512F.Scale</code>, known in C as <code>scalebn</code>), to perform <code>Min</code>, <code>Max</code>, <code>MinMagnitude</code>, and <code>MaxMagnitude</code> with correct handling of <code>+0</code> and <code>-0</code> (<code>Avx512DQ.Range</code>), and even to perform reductions which are useful when handling large values for trigonometric functions like <code>Sin</code> or <code>Cos</code> (<code>Avx512DQ.Reduce</code>).</p>
<p>However, one of my personal favorites is an instruction named <code>vfixupimm</code> (<code>Avx512F.Fixup</code>). At a high level, this instruction lets you detect a number of input edge cases and “fixup” the output to be one of the common outputs and to do this per element. This can massively improve the performance of some algorithms and greatly reduces the amount of handling required. The way it works is it takes 4 inputs known as <code>left</code>, <code>right</code>, <code>table</code>, and <code>control</code>. It first does a classification of the floating-point value in <code>right</code> and determines if it is <code>QNaN</code> (0), <code>SNaN</code> (1), <code>+/-0</code> (2), <code>+1</code> (3), <code>-Infinity</code> (4), <code>+Infinity</code> (5), <code>Negative</code> (6), or <code>Positive</code> (7). It then uses that to read <code>4</code> bits from <code>table</code> (<code>QNaN</code> being <code>0</code>, reads bits <code>0..3</code>; <code>Negative</code> being <code>6</code> reads bits <code>24..27</code>). The value of those 4 bits in <code>table</code> then determines what the result will be. The possible results (per element) are:</p>
<table>
<thead>
<tr>
<th>Bit Pattern</th>
<th>Definition</th>
</tr>
</thead>
<tbody>
<tr>
<td>0b0000</td>
<td>left[i]</td>
</tr>
<tr>
<td>0b0001</td>
<td>right[i]</td>
</tr>
<tr>
<td>0b0010</td>
<td>QNaN(right[i])</td>
</tr>
<tr>
<td>0b0011</td>
<td>QNaN</td>
</tr>
<tr>
<td>0b0100</td>
<td>-Infinity</td>
</tr>
<tr>
<td>0b0101</td>
<td>+Infinity</td>
</tr>
<tr>
<td>0b0110</td>
<td>IsNegative(right[i]) ? -Infinity : +Infinity</td>
</tr>
<tr>
<td>0b0111</td>
<td>-0.0</td>
</tr>
<tr>
<td>0b1000</td>
<td>+0.0</td>
</tr>
<tr>
<td>0b1001</td>
<td>-1.0</td>
</tr>
<tr>
<td>0b1010</td>
<td>+1.0</td>
</tr>
<tr>
<td>0b1011</td>
<td>+0.5</td>
</tr>
<tr>
<td>0b1100</td>
<td>+90.0</td>
</tr>
<tr>
<td>0b1101</td>
<td>PI / 2</td>
</tr>
<tr>
<td>0b1110</td>
<td>MaxValue</td>
</tr>
<tr>
<td>0b1111</td>
<td>MinValue</td>
</tr>
</tbody>
</table>
<p>With SSE there was some support for rearranging the data in a vector. Say, for example, you had <code>0, 1, 2, 3</code> and you wanted it ordered <code>3, 1, 2, 0</code>. With the introduction of AVX and the expansion to 256-bits, this support was likewise expanded. However, due to how the instructions operated you’d actually do the same 128-bit operation twice. This made it simple to expand existing algorithms to 256-bits since you effectively are just doing the same thing twice. However, it made working with other algorithms more difficult when you actually needed to consider the entire vector cohesively. There were some instructions that let you rearrange the data across the entire 256-bit vector, but they were often limited either in how the data could be rearranged or in the types they supported (full shuffle of byte elements is a notable example of missing support). AVX-512 has many of the same considerations for its expanded 512-bit support. However, it also introduces new instructions that fill the gap and now let you fully rearrange the elements for any size of element.</p>
<p>Finally, one of my other personal favorites is an instruction named <code>vpternlog</code> (<code>Avx512F.TernaryLogic</code>). This instruction lets you take any 2 bitwise operations and combine them, so they can be executed in a single instruction. For example, you can do <code>(a &amp; b) | c</code>. The way it works is that it takes 4 inputs, <code>a</code>, <code>b</code>, <code>c</code>, and <code>control</code>. You then have 3 keys to remember: <code>A: 0xF0</code>, <code>B: 0xCC</code>, <code>C: 0xAA</code>. In order to represent the operation desired, you simply build the <code>control</code> by performing that operation on those keys. So, if you wanted to simply return <code>a</code>, you’d use <code>0xF0</code>. If you wanted to do <code>a &amp; b</code>, you’d use <code>(byte)(0xF0 &amp; 0xCC)</code>. If you wanted to do <code>(a &amp; b) | c</code>, then it is <code>(byte)((0xF0 &amp; 0xCC) | 0xAA</code>. There are 256 different operations possible in total, with the basic building blocks being those keys and the following bitwise operations:</p>
<table>
<thead>
<tr>
<th>Operation</th>
<th>Definition</th>
</tr>
</thead>
<tbody>
<tr>
<td>not</td>
<td>~x</td>
</tr>
<tr>
<td>and</td>
<td>x &amp; y</td>
</tr>
<tr>
<td>nand</td>
<td>~x &amp; y</td>
</tr>
<tr>
<td>or</td>
<td>x</td>
<td>y</td>
</tr>
<tr>
<td>nor</td>
<td>~x</td>
<td>y</td>
</tr>
<tr>
<td>xor</td>
<td>x ^ y</td>
</tr>
<tr>
<td>xnor</td>
<td>~x ^ y</td>
</tr>
</tbody>
</table>
<p>There are then some special operations that are also supported given the above basic operations and which can expand even further.</p>
<table>
<thead>
<tr>
<th>Operation</th>
<th>Definition</th>
</tr>
</thead>
<tbody>
<tr>
<td>false</td>
<td>Bit pattern of 0x00</td>
</tr>
<tr>
<td>true</td>
<td>Bit pattern of 0xFF</td>
</tr>
<tr>
<td>major</td>
<td>Returns 0 if two or more input bits are 0, returns 1 if two or more input bits are 1</td>
</tr>
<tr>
<td>minor</td>
<td>Returns 0 if two or more input bits are 1, returns 1 if two or more input bits are 0</td>
</tr>
<tr>
<td>conditional select</td>
<td>Logically <code>(x &amp; y) | (~x &amp; z)</code>, which works since it is <code>(x and y) or (x nand y)</code></td>
</tr>
</tbody>
</table>
<p>In .NET 8 we didn’t complete the support to implicitly recognize and fold these patterns to emit <code>vpternlog</code>. We expect that to debut in .NET 9.</p>
<h3 id="what-is-masking-support">What is masking support?</h3>
<p>At the simplest level, writing vectorized code involves using SIMD to do the same basic operation on <code>Count</code> different elements of type <code>T</code> in a single instruction. This works very nicely when the same operation needs to be done to all data. However, not all data is necessarily uniform and sometimes you need to handle particular inputs differently. For example, you may want to do a different operation for positive vs negative numbers. You may need to return a different result if the user has passed in <code>NaN</code>, and so on. When writing regular code, you would normally handle this with a branch and this works very nicely. When writing vectorized code, however, such branches break the ability to use SIMD instructions since you have to handle each element independently. .NET takes advantage of this in various locations, including the new <code>TensorPrimitives</code> APIs where it allows us to handle trailing data that would otherwise not fit into a full vector.</p>
<p>The typical solution for this is to write “branch-free” code. One of the simplest ways to do this is to compute both answers and then use bitwise operations to pick the correct answer. You can think of this a lot like a ternary condition <code>cond ? result1 : result2</code>. In order to support this in SIMD, there exists an API named <code>ConditionalSelect</code> which takes a mask and both results. The mask is also a vector, but its values are typically either <code>AllBitsSet</code> or <code>Zero</code>. When you have this pattern, then the implementation of <code>ConditionalSelect</code> is effectively <code>(cond &amp; result1) | (~cond &amp; result2)</code>. What this breaks down to doing is taking bits from <code>result1</code> where the corresponding bit in <code>cond</code> is <code>1</code> and otherwise taking the corresponding bit from <code>result2</code> (when the bit in <code>cond</code> is <code>0</code>). So if you wanted to convert all negative values to <code>0</code>, you would have something like <code>(x &lt; 0) ? 0 : x</code> for regular code and <code>Vector128.ConditionalSelect(Vector128.LessThan(x, Vector128.Zero), Vector128.Zero, x)</code> for vectorized code. It’s a bit more verbose, but can also provide significant performance improvement.</p>
<p>When hardware first started having SIMD support, you would have to support this masking very literally by doing 3 instructions: <code>and, nand, or</code>. As newer hardware came out, more optimized versions were added that allowed you to do this in a single instruction, such as <code>blendv</code> on x86/x64 and <code>bsl</code> on Arm64. AVX-512 then took this a step further by introducing dedicated hardware support for expressing masks and tracking them in registers (the previously mentioned <code>k0-k7</code>). It then provided additional support for allowing this masking to be done as part of almost any other operation. So rather than having to specify <code>vcmpltps; vblendvps; vaddps</code> (compare, mask, then add), you could instead encode that mask directly as part of the addition (and thus emit <code>vcmpltps; vaddps</code> instead). This allows the hardware to represent more operations in less space, improving code density, and to better take advantage of the intended behavior.</p>
<p>Notably we do not directly expose a 1-to-1 concept with the underlying hardware for masking here. Rather, the JIT continues taking and returning a regular vector for comparison results and does the relevant pattern recognition and subequent opportunistic lightup of masking features based on this. This allows the exposed API surface to be significantly smaller (over 3000 fewer APIs), for existing code to largely “just work” and take advantage of the newer hardware support without explicit action, and for users wanting to support AVX-512 to not have to learn new concepts or write code in a new way.</p>
<h3 id="what-about-examples-of-using-avx-512-in-practice">What about examples of using AVX-512 in practice?</h3>
<p>AVX-512 can be used to accelerate all of the same scenarios as SSE or AVX based scenarios. An easy way to identify where the .NET Libraries are already using this acceleration is to search for the places we’re calling <code>Vector512.IsHardwareAccelerated</code>, this can be done using <a href="https://source.dot.net/#System.Private.CoreLib/src/libraries/System.Private.CoreLib/src/System/Runtime/Intrinsics/Vector512.cs,576d90eecb8bc9d4,references">source.dot.net</a>.</p>
<p>We’ve accelerated cases such as:</p>
<ul>
<li>System.Collections.BitArray – <a href="https://source.dot.net/#System.Collections/System/Collections/BitArray.cs,137">creation</a>, <a href="https://source.dot.net/#System.Collections/System/Collections/BitArray.cs,137">bitwise and</a>, <a href="https://source.dot.net/#System.Collections/System/Collections/BitArray.cs,137">bitwise or</a>, <a href="https://source.dot.net/#System.Collections/System/Collections/BitArray.cs,137">bitwise xor</a>, <a href="https://source.dot.net/#System.Collections/System/Collections/BitArray.cs,137">bitwise not</a></li>
<li>System.Linq.Enumerable – <a href="https://source.dot.net/#System.Linq/System/Linq/MaxMin.cs,70">Max and Min</a></li>
<li>System.Buffers.Text.Base64 – <a href="https://source.dot.net/#System.Private.CoreLib/src/libraries/System.Private.CoreLib/src/System/Buffers/Text/Base64Decoder.cs,72">Decoding</a>, <a href="https://source.dot.net/#System.Private.CoreLib/src/libraries/System.Private.CoreLib/src/System/Buffers/Text/Base64Encoder.cs,71">Encoding</a></li>
<li>System.String – <a href="https://source.dot.net/#System.Private.CoreLib/src/libraries/System.Private.CoreLib/src/System/Globalization/Ordinal.cs,166">Equals, IgnoreCase</a></li>
<li>System.Span – <a href="https://source.dot.net/#System.Private.CoreLib/src/libraries/System.Private.CoreLib/src/System/SpanHelpers.Packed.cs,117">IndexOf</a>, <a href="https://source.dot.net/#System.Private.CoreLib/src/libraries/System.Private.CoreLib/src/System/SpanHelpers.Packed.cs,117">IndexOfAny</a>, <a href="https://source.dot.net/#System.Private.CoreLib/src/libraries/System.Private.CoreLib/src/System/SpanHelpers.Packed.cs,117">IndexOfAnyInRange</a>, <a href="https://source.dot.net/#System.Private.CoreLib/src/libraries/System.Private.CoreLib/src/System/SpanHelpers.Byte.cs,66">SequenceEqual</a>, <a href="https://source.dot.net/#System.Private.CoreLib/src/libraries/System.Private.CoreLib/src/System/SpanHelpers.Byte.cs,66">Reverse</a>, <a href="https://source.dot.net/#System.Private.CoreLib/src/libraries/System.Private.CoreLib/src/System/SpanHelpers.Packed.cs,117">Contains</a>, etc</li>
</ul>
<p>There are other examples throughout the .NET libraries and general .NET ecosystem, far too many to list and cover. These include, but are not limited to, scenarios such as color conversions, image processing, machine learning, text transcoding, JSON parsing, software rendering, ray tracing, game acceleration, and much more.</p>
<h3 id="whats-next">What’s next?</h3>
<p>We plan to continue to improve the hardware intrinsics support in .NET when and where it makes sense. Please note the following items are forward thinking and speculative. The list is not complete and we provide no guarantees any of these features will land or when they will ship if they do.</p>
<p>Some of the items on our longer term roadmap include the following:</p>
<ul>
<li><code>SVE</code> and SVE2 for Arm64</li>
<li><code>AVX10</code> for x86/x64</li>
<li>Allowing <code>Vector&lt;T&gt;</code> to implicitly expand to 512-bits</li>
<li>An <code>ISimdVector&lt;TSelf, T&gt;</code> interface to allow better reuse of SIMD logic</li>
<li>An analyzer to help encourage users to use the cross-platform APIs where the semantics are identical (use <code>x + y</code> instead of <code>Sse.Add(x, y)</code>)</li>
<li>An analyzer to recognize patterns that may have more optimal alternatives (do <code>value + value</code> instead of <code>value * 2</code> or <code>Sse.UnpackHigh(value, value)</code> instead of <code>Sse.Shuffle(value, value, 0b11_11_10_10)</code></li>
<li>Additional explicit usage of hardware intrinsics in various .NET APIs</li>
<li>Additional cross-platform APIs to help abstract common operation
<ul>
<li>getting the index of the first/last match in a mask</li>
<li>getting the number of matches in a mask</li>
<li>determining if any matches exist</li>
<li>allowing non-deterministic behavior for cases like <code>Shuffle</code> or <code>ConditionalSelect</code></li>
<li>these APIs have a well-defined behavior on all platforms today, such as <code>Shuffle</code> treating any out of range index as zeroing the destination element</li>
<li>the new APIs, such as <code>ShuffleUnsafe</code>, would instead allow different behavior for out of range indices</li>
<li>for such a scenario, Arm64 would have the same behavior, while x64 only has the same behavior if the most-significant bit is set</li>
</ul>
</li>
<li>Additional pattern recognition for cases like
<ul>
<li>embedded masking (AVX512, AVX10, SVE/SVE2)</li>
<li>combined bitwise-operations (<code>vpternlog</code> on AVX512)</li>
<li>limited JIT time constant folding opportunities</li>
</ul>
</li>
</ul>
<p>If you’re looking to use hardware intrinsics in .NET, we encourage you to try out the APIs available in the <a href="https://learn.microsoft.com/dotnet/api/system.runtime.intrinsics?view=net-8.0">System.Runtime.Intrinsics namespace</a>, log <a href="https://github.com/dotnet/runtime/issues/new?assignees=&amp;labels=api-suggestion&amp;projects=&amp;template=02_api_proposal.yml&amp;title=%5BAPI+Proposal%5D%3A+">API suggestions</a> for functionality you feel is missing or could be improved, and to engage in our preview releases to try out the functionality before it ships so you can help make each release better than the last!</p>

        

        
		
        
	</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Qt 6.6 and 6.7 Make QML Faster Than Ever: A New Benchmark and Analysis (120 pts)]]></title>
            <link>https://www.qt.io/blog/qt-6.6-and-6.7-make-qml-faster-than-ever-a-new-benchmark-and-analysis</link>
            <guid>38692318</guid>
            <pubDate>Tue, 19 Dec 2023 05:51:11 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.qt.io/blog/qt-6.6-and-6.7-make-qml-faster-than-ever-a-new-benchmark-and-analysis">https://www.qt.io/blog/qt-6.6-and-6.7-make-qml-faster-than-ever-a-new-benchmark-and-analysis</a>, See on <a href="https://news.ycombinator.com/item?id=38692318">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
          

          

          <p>
            December 18, 2023 by <a href="https://www.qt.io/blog/author/ulf-hermann">Ulf Hermann</a>
            | <a href="#commento">Comments</a>
          </p>

          

          <p><span id="hs_cos_wrapper_post_body" data-hs-cos-general-type="meta_field" data-hs-cos-type="rich_text"><p>It has been a while since the <a href="https://www.qt.io/blog/compiling-qml-to-c-a-4x-speedup?hsLang=en">last time</a> I've posted here and a lot has happened to the Qt Quick Compiler infrastructure. It's time to show some updated numbers. The benchmark discussed in my previous post was heavily focused on value types and lists of value types. It applied some rather complex tricks to eke out the maximum speedup between the interpreted and compiled versions of the same program.</p>
<!--more-->
<p>Today I'm going to work with something more familiar to most people. I've written a new benchmark that's mostly based on object types (and lists of those), and refrains from underhandedly instructing the compiler's type propagation using secret knowledge about the quirks of JavaScript operators. It also does something useful this time around. I've re-implemented the DeltaBlue constraint solver found in the V8 benchmarks in typed QML.</p>
<p>In and of itself this is a somewhat foolish endeavour. Since I want to use object types, I use a separate QObject for each variable and each constraint. QObjects, as we all know, have a rather significant static overhead. Allocating a QObject just to store a few integers is quite a waste. The original implementation uses JavaScript objects. While still not ideal, those are somewhat more lightweight. Furthermore, in order to run the actual algorithm, we have to call a lot of functions, and calling functions in QML contexts is generally more expensive than doing the same in JavaScript contexts. This is because the context and scope hierarchy in QML is much more complex, and we often have to perform extra type conversions.</p>
<p>So, why did I do this? Most of you will want to deal with QObjects in a lot of places since all of Qt Quick is built on QObjects. You cannot avoid allocating a QObject if you need an Item. So, in a way the implementation using QObject as storage for everything, while slower, is also more realistic. You may argue that I should have written the benchmark with Qt Quick itself to get even more realistic. I decided not to do so because as soon as you add actual graphics to the mix, you have to deal with a lot more noisy data. Qt Quick itself often adds unpredictable overhead you don't want to deal with in a benchmark. For example, if you happen to have any text in your application, it has to create the font database at some point. Or, the scenegraph performs complex operations in the background to put the pixels on the screen. Those operations may or may not happen in a separate thread, and if so, there is still a synchronization phase for each frame. Finally, the graphics driver itself kicks in and performs its own calculations. This is all very interesing if you're benchmarking Qt Quick. However, I want to benchmark the QML language here. For me this is all just noise. Therefore, I've written a non-graphical application built with QObjects. You can find the code in <a href="https://git.qt.io/ulherman/deltablue">this repository</a>.</p>
<p>And here is the good news: The performance numbers for dealing with QObjects and calling typed functions on them have improved massively in Qt 6.6 and Qt 6.7.</p>
<h3>Time taken to run the DeltaBlue benchmark with different versions of Qt</h3>
<a href="https://www.qt.io/hubfs/result.svg?hsLang=en" rel="noopener"><img src="https://www.qt.io/hubfs/result.svg" alt="result" width="800" height="400"></a>
<p>On the Y axis you see the milliseconds it took to run one iteration of the benchmark. Lower is better. The benchmark was run with:</p>
<ul>
<li><strong>Qt 5.15</strong>, the last version of the Qt5 series. This is our baseline. In Qt 5.15 the Qt Quick Compiler didn't generate any C++ code for functions and bindings. It only produced byte code to be interpreted or JIT-compiled.</li>
<li><strong>Qt 6.2</strong>, since that is when the new Qt Quick Compiler was introduced as tech preview.</li>
<li><strong>Qt 6.5</strong>, the last LTS version.</li>
<li><strong>Qt 6.6</strong>, the most recent release, <strong>highlighted</strong> where appropriate.</li>
<li>A recent snapshot of <strong>Qt 6.7</strong>, <strong>highlighted</strong> where appropriate</li>
</ul>
<h2>The setup</h2>
<p>The benchmarked program takes as input a number of variables and constraints between them. The variables are effectively numbers and the constraints hold:</p>
<ol>
<li>An input variable</li>
<li>An output variable</li>
<li>A scale variable</li>
<li>An offset variable</li>
</ol>
<p>Either of these can be null. The constraint solver then manipulates the variable values, trying to achieve a state where for each constraint we get:</p>
<pre><code>output == input * scale + offset
</code></pre>
<p>There are more details to it, but this is the gist. Suffice to say, it's a somewhat demanding computational problem and as such well suited for our purposes.</p>
<p>We run this on two sets of inputs: 1. A chain of alternating variables and constraints, 100 variables long. 2. A projection where 100 inputs are scaled and offset into 100 outputs.</p>
<p>The split between those two inputs is not very interesting. I'm giving them both together as a single data point in all the discussions below.</p>
<h3>Collected data points</h3>
<p>As mentioned before, there are two implementations of the actual algorithm:</p>
<ol>
<li>The JavaScript version, almost as found in the V8 benchmark suite.</li>
<li>The QML version I've written.</li>
</ol>
<p>Finally, I've split the execution into two phases for both implementations:</p>
<ol>
<li>A setup phase where all the objects are created that shall hold the variables and constraints.</li>
<li>The actual execution of the DeltaBlue algorithm.</li>
</ol>
<p>Combining all this, a single run of the benchmark produces 4 data points:</p>
<ol>
<li>The total time for the QML version</li>
<li>The object creation time for the QML version</li>
<li>The total time for the JavaScript version</li>
<li>The object creation time for the JavaScript version</li>
</ol>
<p>It has to be said that we cannot run the exact same code for all versions of Qt to be tested:</p>
<ul>
<li>Qt 6.2 and Qt 5.15 cannot declare and initialize list properties in one QML binding/declaration. So, those had to be split in two lines.</li>
<li>Qt 6.2 and Qt 5.15 do not know <a href="https://doc.qt.io/qt-6/qtqml-documents-definetypes.html#componentbehavior">pragma ComponentBehavior</a> so this had to be dropped, causing some IDs to become invisible to the compiler.</li>
<li>Qt 6.2 and Qt 5.15 do not know that ':/qt/qml' is a default import path. It's added manually.</li>
<li>Qt 6.2 and Qt 5.15 cannot construct a QQmlComponent <a href="https://doc.qt.io/qt-6/qqmlcomponent.html#QQmlComponent-6">from URI and name</a>. They have to load by URL instead.</li>
<li>Qt 5.15 has no proper build system API for QML modules. We build using qmake and <code>CONFIG+=qtquickcompiler</code> instead.</li>
<li>Qt 5.15 does not understand imports without versions. We add some versions to make it happy.</li>
</ul>
<p>I could have avoided some of those differences, but I intentionally used the new features. They lead to improved performance where they are available, and the improved performance is what we are after.</p>
<h3>No disk cache mode</h3>
<p>Now, since we don't have enough dimensions in our data, yet, we're adding another one. The benchmark by default uses Qt Quick Compiler to compile bindings and functions to C++. Comparing the numbers produced by the compiled code should give us the speedup caused by the compilation for each version of Qt, right? Well, unless the performance of the interpreter has also changed. In order to control for this, we also run the benchmark with <code>QML_DISABLE_DISK_CACHE=1</code> for each version of Qt. This makes it ignore the compiled artifacts and instead work with the QML source code.</p>
<p>Finally, the Qt Quick Compiler Extensions have an extra feature that comes in very handy here:</p>
<h3>Static mode</h3>
<p>Consider three files A.qml, B.qml, and C.qml:</p>
<pre><code>// A.qml
import QtQml
QtObject { property int v: 11 }

// B.qml
import QtQml
A { property string v: "foos" }

// C.qml
import QtQml
QtObject { 
    property A a: A {}
    function evil(b: B) { a = b }
    function bark() { console.log(a.v) }
}
</code></pre>
<p>If you instantiate <code>C</code> and play with the <code>evil</code> and <code>bark</code> functions a bit, you will discover a feature of the QML language you didn't want to know about. It's called property shadowing. For great many properties and methods we cannot know in advance what types they will have at run time. This is a nasty problem for the Qt Quick Compiler. In Qt 6.6 it has learned to deal with it by wrapping the affected values in QVariant and checking their types where necessary. This comes at a performance cost, though. <a href="https://doc.qt.io/qt-6/qtqml-qml-script-compiler.html">qmlsc</a> has an extra option <code>--static</code> that tells it to ignore any shadowing. You can use it at your own risk. There are some properties that are intentionally shadowed. For example we're moving the <a href="https://codereview.qt-project.org/c/qt/qtdeclarative/+/383968">focusReason</a> property to QQuickItem, leaving a property of the same name in QQuickControl for backwards compatibility. Most shadowing, however, is a mistake.</p>
<p>The <code>--static</code> option was not available in Qt 6.2 and only takes effect with Qt 6.5, 6.6, and 6.7.</p>
<p>In our benchmark, we know we haven't shadowed anything, and we don't want to pay the performance price of checking. Therefore, we add a third, <code>static</code>, mode to each benchmark run to see how much we can gain in comparison to the normal, <code>shadowable</code> mode.</p>
<h2>The results</h2>
<p>I've tried very hard to produce stable, comparable, data. The benchmarks are run on a linux machine booted directly into a shell, without init system. For program run I first try to warm the caches by performing a dry run, and then run 1000 iterations of the benchmark. For each benchmark function the program is re-started from scratch so that they cannot interfer with each other. So let's go back to the graph above.</p>
<p>The first thing to note here is that I was not fully successful in my attempts to produce clean data. The JavaScript numbers should all be the same, especially within a single version of Qt. The way the QML code is compiled should not have any effect on the JavaScript execution. All the JavaScript at play here lives in a separate <code>deltablue.js</code> file that cannot be compiled to C++. Realizing this, I advise you to take all of the data with a roughly 5%-sized grain of salt.</p>
<p>Another thing you can immediately see is that the QML version of the algorithm is generally much slower than the JavaScript version. As noted above, this is due to it being built on QObjects rather than JavaScript objects.</p>
<p>On top of this, there is a noticable drop in performance between Qt 5.15 and Qt 6.2, for the QML version. If you look at the code you notice that there are a lot of <code>as</code> casts in there that tell the compiler what type to expect for some potentially shadowed value. In 5.15 <code>as</code> is a no-op. It was originally meant as a compile time only construct. Later, however, we noticed that this will lead to behavior differences between compiled and interpreted/JIT'ed code. To avoid those, we introduced type checks for both the compiled code and the interpreter and JIT. So, the later versions of Qt do more work here, but for Qt 6.2 and 6.5 it does not pay off, yet. Qt 6.2 and 6.5 still have to interpret or JIT most of the code here since their compilers' language coverage is rather limited.</p>
<p>With that out of the way, let's look at the happy side of things. I've highlighted it in orange and red. Qt 6.6 takes about <strong>half</strong> the time Qt 6.5 takes to run the QML version of the benchmark, and Qt 6.7 improves on this some more. In static mode we get down to about <strong>a third</strong> of the 6.5 numbers. Here we get into a territory where the object creation overhead starts to dominate the benchmark. With Qt 6.7 in static mode, it took less time to run the whole benchmark than it took for the object creation alone with Qt 6.2.</p>
<p>Object creation also includes initial binding evaluation, which is why the object creation also benefits from compilation of bindings and expressions to C++. A complementary solution to object creation overhead will be <a href="https://doc.qt.io/qt-6/qtqml-qml-type-compiler.html">qmltc</a>, once it's ready.</p></span></p>

          

          
          <hr>

          <h6>Blog Topics:</h6>
          
          


        </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Gokrazy – Go Appliances (227 pts)]]></title>
            <link>https://gokrazy.org/</link>
            <guid>38692167</guid>
            <pubDate>Tue, 19 Dec 2023 05:23:20 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://gokrazy.org/">https://gokrazy.org/</a>, See on <a href="https://news.ycombinator.com/item?id=38692167">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="body">
          

        

<h2 id="gokrazy-go-appliances">gokrazy Go appliances</h2>
<p>With gokrazy, you can deploy your Go programs as
<a href="https://en.wikipedia.org/wiki/Computer_appliance">appliances</a> to a Raspberry Pi
or PC (<a href="https://gokrazy.org/platforms/">→ supported platforms</a>).</p>
<p>For a long time, we were unhappy about having to spend so much time on each of
our various Raspberry Pis, taking care of security updates and other general
Linux distribution maintenance.</p>
<p>Then, we had a crazy idea: what if we massively reduced the overall system
complexity by getting rid of all software we don’t strictly need, and instead
built up a minimal system from scratch entirely in Go, a memory safe programming
language?</p>
<p>Turns out this is feasible. gokrazy is the result. See it in action in <a href="https://www.youtube.com/watch?v=6FBGjdT8ZW4&amp;list=PLxMVx8p-A48A6H-EWqzN-HStDH1dOW5PU">this
first installation demo
video</a>:</p>

<p>
  <iframe src="https://www.youtube-nocookie.com/embed/6FBGjdT8ZW4" allowfullscreen="" title="gokrazy demo: first installation and adding programs"></iframe>
</p>

<!--<img src="logo.svg" width="50" height="50" alt="gokrazy logo" title="gokrazy logo">-->
<h2 id="your-apps--only-4-moving-parts">Your app(s) + only 4 moving parts</h2>
<ol>
<li>the <a href="https://github.com/gokrazy/kernel">Linux kernel</a>
<ul>
<li>new versions are typically available &lt; 24h after upstream release!</li>
</ul>
</li>
<li>the <a href="https://github.com/gokrazy/firmware">Raspberry Pi firmware files</a></li>
<li>the <a href="https://go.dev/">Go</a> compiler and standard library</li>
<li>the gokrazy system (minimal init system, updater, DHCP, NTP, …)</li>
</ol>
<h2 id="uniformity">Uniformity</h2>
<p>What’s appealing about building an appliance entirely in Go? You get the same
advantages you get when building Go software elsewhere:</p>
<ul>
<li>All components mentioned above (except for the Go compiler) are managed as Go
modules, using the same tooling you’re already familiar with.</li>
<li>Go has very quick compilation times; the <code>gok run</code> command allows for a fast
edit-run loop.</li>
<li>Go’s
<a href="https://about.sourcegraph.com/blog/go/an-introduction-to-go-tool-trace-rhys-hiltner">tracing</a>
and <a href="https://go.dev/blog/pprof">profiling</a> support can be used on the entire
system</li>
<li>With Go’s <a href="https://github.com/golang/go/wiki/Modules#when-should-i-use-the-replace-directive">replace
directive</a>,
you can quickly modify any part of the system with the same workflow.</li>
</ul>
<h2 id="web-status-interface">Web status interface</h2>
<p>On a regular Linux distribution, we’d largely use systemctl’s start,
stop, restart and status verbs to manage our applications. gokrazy
comes with a <a href="https://gokrazy.org/overview.png">convenient web interface</a> for
seeing process status and stopping/restarting processes.</p>
<h2 id="debugging">Debugging</h2>
<p>Sometimes, an interactive <code>busybox</code> session or a quick
<code>tcpdump</code> run are invaluable. <a href="https://github.com/gokrazy/breakglass">breakglass</a> allows
you to temporarily enable SSH/SCP-based authenticated remote code
execution: scp your statically compiled binary, then run it
interactively via ssh.</p>
<p>Due to no C runtime environment being present, your code must compile
with the environment variable <code>CGO_ENABLED=0</code>. To
cross-compile for the Raspberry Pi 3 or 4,
use <code>GOARCH=arm64</code>. If your program still builds, you’re
good to go!</p>
<h2 id="network-updates">Network updates</h2>
<p>After building a new gokrazy image on your computer, you can easily
update an existing gokrazy installation in-place thanks to the A/B
partitioning scheme we use. Just use the <code>gok update</code>
command.</p>
<h2 id="minimal-state-and-configuration">Minimal state and configuration</h2>
<p>A tiny amount of configuration is built into the images (e.g.
hostname, password, serial console behavior). In general, we prefer
auto-configuration (e.g. DHCP) over config files. If you need more
configurability, you may need to replace some of our programs.</p>
	
  
        
        </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[No star, No fix (145 pts)]]></title>
            <link>https://github.com/daeuniverse/dae/issues/368</link>
            <guid>38691652</guid>
            <pubDate>Tue, 19 Dec 2023 03:59:53 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/daeuniverse/dae/issues/368">https://github.com/daeuniverse/dae/issues/368</a>, See on <a href="https://news.ycombinator.com/item?id=38691652">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-view-component="true" id="discussion_bucket">                  <h2>Comments</h2>
<div data-hpc="" data-quote-markdown=".js-comment-body" data-discussion-hovercards-enabled="" data-issue-and-pr-hovercards-enabled="" data-team-hovercards-enabled="">
      <div data-gid="I_kwDOI02JMM559wPK" data-url="/daeuniverse/dae/issues/368/partials/body?issue=368" data-channel="eyJjIjoiaXNzdWU6MjA0NjIzMTQ5OCIsInQiOjE3MDI5ODM2MDR9--7b477d495a6a708f651cf41763d4f43b9446c10ab91149faaf3247ef1204798b">

  <p><a data-hovercard-type="user" data-hovercard-url="/users/jschwinger233/hovercard" data-octo-click="hovercard-link-click" data-octo-dimensions="link_type:self" href="https://github.com/jschwinger233"><img src="https://avatars.githubusercontent.com/u/6435624?s=80&amp;u=f43e6bd6a046c8fe88cc6cb920eb6dd0a5622d0a&amp;v=4" width="40" height="40" alt="@jschwinger233"></a>

</p>

  <div data-body-version="204bd3a66f04e654db6d934e116b0111c938022f4c9d29d34929d55d1b225578" id="issue-2046231498">

        <task-lists disabled="" sortable="">
<div>
          <h3 dir="auto">Improvement Suggestion</h3>
<p dir="auto">My understanding is dae is supposed to run on Linux &gt;= 5.8, let's (let me) add some test via github action against different kernels.</p>
<p dir="auto">I am proposing to use cilium/little-vm-helper action for running VM inside a github action container.</p>
<p dir="auto">If maintainers are in favor of this plan, I can draft a PR for review in days.</p>
<h3 dir="auto">Potential Benefits</h3>
<p dir="auto"><em>No response</em></p>
      </div>
</task-lists>


        
      </div>

</div>


      <div>
    


      <div data-gid="IC_kwDOI02JMM5u3RsF" data-url="/daeuniverse/dae/comments/IC_kwDOI02JMM5u3RsF/partials/timeline_issue_comment">

  <p><a href="https://github.com/apps/dae-bot"><img src="https://avatars.githubusercontent.com/in/345419?s=80&amp;v=4" width="40" height="40" alt="@dae-bot"></a>

</p>


  <div data-body-version="27c88546ab622c6ab68af3587822693e8ab1d621319d307a8bcee7b626f86cac" id="issuecomment-1859984133">

        <task-lists disabled="" sortable="">
<div>
          <p dir="auto">Thanks for opening this issue!</p>
      </div>
</task-lists>


        
      </div>

</div>


      

      <div data-gid="IC_kwDOI02JMM5u3RuA" data-url="/daeuniverse/dae/comments/IC_kwDOI02JMM5u3RuA/partials/timeline_issue_comment">

  <p><a data-hovercard-type="user" data-hovercard-url="/users/daebot/hovercard" data-octo-click="hovercard-link-click" data-octo-dimensions="link_type:self" href="https://github.com/daebot"><img src="https://avatars.githubusercontent.com/u/127670536?s=80&amp;u=4ecc4ef98f4485e1a295d58653719d1d0b4e1efc&amp;v=4" width="40" height="40" alt="@daebot"></a>

</p>


  <div data-body-version="6dce9d94822567386d1dc1c1ebfcd9f4a4ff1b2a3545d47e91139dc9d63d81db" id="issuecomment-1859984256">

        <task-lists disabled="" sortable="">
<div>
          <p dir="auto">❣️ This issue is marked as <code>wontfix</code> as you have not yet starred this repo. Please kindly consider giving a star to this repo. Your support means a lot to us. Thanks for your understanding. After you become a stargazer, please also reply to this message with the keyword <code>understood</code>. Afterward, I will reopen this issue for you. Once again, your support is much appreciated. Cheers.</p>
      </div>
</task-lists>


        <div data-view-component="true">
  <!-- '"` --><!-- </textarea></xmp> --><form data-turbo="false" action="/daeuniverse/dae/reactions" accept-charset="UTF-8" method="post">
    
      
    <div>
          <tool-tip id="tooltip-e05aaf8f-1793-4964-81ae-7f7b898c7a59" for="reactions--reaction_button_component-9141da" popover="manual" data-direction="n" data-type="description" data-view-component="true">oguzhaninan, SommerEngineering, rgwood, libreom, FridayCandour, Valium007, barats, Saransh-Sharma, and dfxe reacted with thumbs up emoji</tool-tip>
          <tool-tip id="tooltip-677b9ad7-f6e0-49a2-baf3-83ffd1a05049" for="reactions--reaction_button_component-df386c" popover="manual" data-direction="n" data-type="description" data-view-component="true">la3rence, cryptarch, qrohlf, svidovich, mwsundberg, bbickerton, SolarMerps, fran-cap, adamtajti, Nicksil, and 99 more reacted with thumbs down emoji</tool-tip>
          <tool-tip id="tooltip-a2b716bd-ca31-498a-a912-25ac7590f0fa" for="reactions--reaction_button_component-cd6117" popover="manual" data-direction="n" data-type="description" data-view-component="true">adamtajti, twitchard, SommerEngineering, nilslindemann, squigglezworth, Burrish, condy0919, remcohaszing, dthierbach, tobz1000, and 8 more reacted with laugh emoji</tool-tip>
          <tool-tip id="tooltip-ea0d1eeb-096c-40be-a380-71c6ab357a52" for="reactions--reaction_button_component-b6e4f9" popover="manual" data-direction="n" data-type="description" data-view-component="true">emcmanus, aaronmurniadi, MaxKellermann, and aarroyoc reacted with confused emoji</tool-tip>
          <tool-tip id="tooltip-1d649832-7a43-4085-9d18-9187b8cdab5a" for="reactions--reaction_button_component-52f468" popover="manual" data-direction="n" data-type="description" data-view-component="true">antekone reacted with heart emoji</tool-tip>
      
    </div>
</form></div>
      </div>

</div>


      

      <div data-gid="IC_kwDOI02JMM5u3SQA" data-url="/daeuniverse/dae/comments/IC_kwDOI02JMM5u3SQA/partials/timeline_issue_comment">

  <p><a data-hovercard-type="user" data-hovercard-url="/users/jschwinger233/hovercard" data-octo-click="hovercard-link-click" data-octo-dimensions="link_type:self" href="https://github.com/jschwinger233"><img src="https://avatars.githubusercontent.com/u/6435624?s=80&amp;u=f43e6bd6a046c8fe88cc6cb920eb6dd0a5622d0a&amp;v=4" width="40" height="40" alt="@jschwinger233"></a>

</p>


  <div data-body-version="79ba705f698368235b08252f7783d865c1e8d1c3ea429bee4422994d6bd06881" id="issuecomment-1859986432">

        <task-lists disabled="" sortable="">
<div>
          <blockquote>
<p dir="auto">❣️ This issue is marked as <code>wontfix</code> as you have not yet starred this repo. Please kindly consider giving a star to this repo. Your support means a lot to us. Thanks for your understanding. After you become a stargazer, please also reply to this message with the keyword <code>understood</code>. Afterward, I will reopen this issue for you. Once again, your support is much appreciated. Cheers.</p>
</blockquote>
<p dir="auto">understood</p>
      </div>
</task-lists>


        
      </div>

</div>


      

      <div data-gid="IC_kwDOI02JMM5u3SVu" data-url="/daeuniverse/dae/comments/IC_kwDOI02JMM5u3SVu/partials/timeline_issue_comment">

  <p><a data-hovercard-type="user" data-hovercard-url="/users/daebot/hovercard" data-octo-click="hovercard-link-click" data-octo-dimensions="link_type:self" href="https://github.com/daebot"><img src="https://avatars.githubusercontent.com/u/127670536?s=80&amp;u=4ecc4ef98f4485e1a295d58653719d1d0b4e1efc&amp;v=4" width="40" height="40" alt="@daebot"></a>

</p>


  <div data-body-version="3ac33f509b80c3d9f5e215886701c2cf56e191a7a067aa2606246109d200b304" id="issuecomment-1859986798">

        <task-lists disabled="" sortable="">
<div>
          <p dir="auto">✅ Thanks for giving a star to the repository. Your support means a lot to us. I removed the <code>wonfix</code> mark for you. Hopefully, someone can dig in and resolve the issue for you.</p>
      </div>
</task-lists>


        <div data-view-component="true">
  <!-- '"` --><!-- </textarea></xmp> --><form data-turbo="false" action="/daeuniverse/dae/reactions" accept-charset="UTF-8" method="post">
    
      
    <div>
          <tool-tip id="tooltip-420e9fb7-99a1-4d99-aa23-5af6f22db4da" for="reactions--reaction_button_component-07fb6b" popover="manual" data-direction="n" data-type="description" data-view-component="true">rgwood, libreom, and Saransh-Sharma reacted with thumbs up emoji</tool-tip>
          <tool-tip id="tooltip-75f98762-64b1-481d-97a1-4f9ff9ba0a94" for="reactions--reaction_button_component-1cc5a9" popover="manual" data-direction="n" data-type="description" data-view-component="true">aaronmurniadi, maciejkos, MaxKellermann, FridayCandour, djmattyg007, ivan-achlaqullah, tomalbrc, hamishc, abhijitnandy, foxpy, and 26 more reacted with thumbs down emoji</tool-tip>
          <tool-tip id="tooltip-38b10874-abb3-486b-90bd-7360efb08f00" for="reactions--reaction_button_component-935ef4" popover="manual" data-direction="n" data-type="description" data-view-component="true">squigglezworth reacted with laugh emoji</tool-tip>
      
    </div>
</form></div>
      </div>

</div>


      

      <div data-gid="IC_kwDOI02JMM5u4fAE" data-url="/daeuniverse/dae/comments/IC_kwDOI02JMM5u4fAE/partials/timeline_issue_comment">

  <p><a data-hovercard-type="user" data-hovercard-url="/users/yqlbu/hovercard" data-octo-click="hovercard-link-click" data-octo-dimensions="link_type:self" href="https://github.com/yqlbu"><img src="https://avatars.githubusercontent.com/u/31861128?s=80&amp;u=261db873d99315596f57491fd11c3e80903df410&amp;v=4" width="40" height="40" alt="@yqlbu"></a>

</p>


  

</div>


      <div data-gid="IC_kwDOI02JMM5u4may" data-url="/daeuniverse/dae/comments/IC_kwDOI02JMM5u4may/partials/timeline_issue_comment">

  <p><a data-hovercard-type="user" data-hovercard-url="/users/mzz2017/hovercard" data-octo-click="hovercard-link-click" data-octo-dimensions="link_type:self" href="https://github.com/mzz2017"><img src="https://avatars.githubusercontent.com/u/30586220?s=80&amp;u=b3e5fd7d30cc2cf7ad9d4b7886035ee03dc52255&amp;v=4" width="40" height="40" alt="@mzz2017"></a>

</p>


  <div data-body-version="c9f3c2b1b884e157311ee833fb8a1f381a0005ce0e7af14308068227e5cb9369" id="issuecomment-1860331186">

        <task-lists disabled="" sortable="">
<div>
          <p dir="auto">LGTM. I'm glad to hear this exciting work.</p>
      </div>
</task-lists>


        
      </div>

</div>


      <div data-gid="IC_kwDOI02JMM5vA_Y8" data-url="/daeuniverse/dae/comments/IC_kwDOI02JMM5vA_Y8/partials/timeline_issue_comment">

  <p><a data-hovercard-type="user" data-hovercard-url="/users/Draky50110/hovercard" data-octo-click="hovercard-link-click" data-octo-dimensions="link_type:self" href="https://github.com/Draky50110"><img src="https://avatars.githubusercontent.com/u/1558487?s=80&amp;u=ce1f61c72da27c67a4ee1fc36ee41c8280f76823&amp;v=4" width="40" height="40" alt="@Draky50110"></a>

</p>


  <div data-body-version="736abc32ef9086e1aab6d50b9295c29f415c040880f992329363ede09d99bd74" id="issuecomment-1862530620">

        <task-lists disabled="" sortable="">
<div>
          <p dir="auto"><a data-hovercard-type="user" data-hovercard-url="/users/daebot/hovercard" data-octo-click="hovercard-link-click" data-octo-dimensions="link_type:self" href="https://github.com/daebot">@daebot</a> if you want, I can freely star your repo, as long as you either pay me or suck my dick.</p>
<p dir="auto">Hope your project will die.</p>
<p dir="auto">Don't thanks me, as begging a star is a pleasure to see in this shittification of the world.</p>
      </div>
</task-lists>


        
      </div>

</div>






  <!-- Rendered timeline since 2023-12-19 02:47:29 -->
  
</div>


  </div>

</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[The Illustrated GPT-2: Visualizing Transformer Language Models (2019) (195 pts)]]></title>
            <link>https://jalammar.github.io/illustrated-gpt2/</link>
            <guid>38691583</guid>
            <pubDate>Tue, 19 Dec 2023 03:48:23 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://jalammar.github.io/illustrated-gpt2/">https://jalammar.github.io/illustrated-gpt2/</a>, See on <a href="https://news.ycombinator.com/item?id=38691583">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
    <p><span>Discussions:
<a href="https://news.ycombinator.com/item?id=20677411">Hacker News (64 points, 3 comments)</a>, <a href="https://www.reddit.com/r/MachineLearning/comments/cp8prq/p_the_illustrated_gpt2_visualizing_transformer/">Reddit r/MachineLearning (219 points, 18 comments)</a>
</span></p>

<p><span>Translations: <a href="https://lolitasian.blog.csdn.net/article/details/125529598">Simplified Chinese</a>, <a href="https://lbourdois.github.io/blog/nlp/GPT2/">French</a>, <a href="https://chloamme.github.io/2021/12/08/illustrated-gpt2-korean.html">Korean</a>, <a href="https://habr.com/ru/post/490842/">Russian</a>, <a href="https://devrimdanyal.medium.com/gpt-2-transformat%C3%B6r-dil-modellerinin-g%C3%B6rselle%C5%9Ftirilmesi-fc4bfd510223">Turkish</a></span></p>

<p><img src="https://jalammar.github.io/images/gpt2/openAI-GPT-2-3.png">
  <br>
</p>

<p>This year, we saw a dazzling application of machine learning. <a href="https://openai.com/blog/better-language-models/">The OpenAI GPT-2</a> exhibited impressive ability of writing coherent and passionate essays that exceed what we anticipated current language models are able to produce. The GPT-2 wasn’t a particularly novel architecture – it’s architecture is very similar to the decoder-only transformer. The GPT2 was, however, a very large, transformer-based language model trained on a massive dataset. In this post, we’ll look at the architecture that enabled the model to produce its results. We will go into the depths of its self-attention layer. And then we’ll look at applications for the decoder-only transformer beyond language modeling.</p>

<p>My goal here is to also supplement my earlier post, <a href="https://jalammar.github.io/illustrated-transformer/">The Illustrated Transformer</a>, with more visuals explaining the inner-workings of transformers, and how they’ve evolved since the original paper. My hope is that this visual language will hopefully make it easier to explain later Transformer-based models as their inner-workings continue to evolve.</p>

<!--more-->

<div>

  <p><strong>Contents</strong></p>

  <ul>
    <li><strong><a href="#part-1-got-and-language-modeling">Part 1: GPT2 And Language Modeling</a></strong>
      <ul>
        <li>What is a Language Model</li>
        <li>Transformers for Language Modeling</li>
        <li>One Difference From BERT</li>
        <li>The Evolution of The Transformer Block</li>
        <li>Crash Course in Brain Surgery: Looking Inside GPT-2</li>
        <li>A Deeper Look Inside</li>
        <li>End of part #1: The GPT-2, Ladies and Gentlemen</li>
      </ul>
    </li>
    <li><strong><a href="#part-2-illustrated-self-attention">Part 2: The Illustrated Self-Attention</a></strong>
      <ul>
        <li>Self-Attention (without masking)</li>
        <li>1- Create Query, Key, and Value Vectors</li>
        <li>2- Score</li>
        <li>3- Sum</li>
        <li>The Illustrated Masked Self-Attention</li>
        <li>GPT-2 Masked Self-Attention</li>
        <li>Beyond Language modeling</li>
        <li>You’ve Made it!</li>
      </ul>
    </li>
    <li><strong><a href="#part-3-beyond-language-modeling">Part 3: Beyond Language Modeling</a></strong>
      <ul>
        <li>Machine Translation</li>
        <li>Summarization</li>
        <li>Transfer Learning</li>
        <li>Music Generation</li>
      </ul>
    </li>
  </ul>

</div>

<h2 id="part-1-gpt2-and-language-modeling-">Part #1: GPT2 And Language Modeling <a href="#part-1-got-and-language-modeling" name="part-1-got-and-language-modeling">#</a></h2>

<p>So what exactly is a language model?</p>

<h3 id="what-is-a-language-model">What is a Language Model</h3>
<p>In <a href="https://jalammar.github.io/illustrated-word2vec/">The Illustrated Word2vec</a>, we’ve looked at what a language model is – basically a machine learning model that is able to look at part of a sentence and predict the next word. The most famous language models are smartphone keyboards that suggest the next word based on what you’ve currently typed.</p>

<p><img src="https://jalammar.github.io/images/word2vec/swiftkey-keyboard.png">
  <br>
</p>

<p>In this sense, we can say that the GPT-2 is basically the next word prediction feature of a keyboard app, but one that is much larger and more sophisticated than what your phone has. The GPT-2 was trained on a massive 40GB dataset called WebText that the OpenAI researchers crawled from the internet as part of the research effort. To compare in terms of storage size, the keyboard app I use, SwiftKey, takes up 78MBs of space. The smallest variant of the trained GPT-2, takes up 500MBs of storage to store all of its parameters. The largest GPT-2 variant is 13 times the size so it could take up more than 6.5 GBs of storage space.</p>

<p><img src="https://jalammar.github.io/images/gpt2/gpt2-sizes.png">
  <br>
</p>

<p>One great way to experiment with GPT-2 is using the <a href="https://gpt2.apps.allenai.org/?text=Joel%20is">AllenAI GPT-2 Explorer</a>. It uses GPT-2 to display ten possible predictions for the next word (alongside their probability score). You can select a word then see the next list of predictions to continue writing the passage.</p>

<h3 id="transformers-for-language-modeling">Transformers for Language Modeling</h3>

<p>As we’ve seen in The <a href="https://jalammar.github.io/illustrated-transformer/">Illustrated Transformer</a>, the original transformer model is made up of an encoder and decoder – each is a stack of what we can call transformer blocks. That architecture was appropriate because the model tackled machine translation  – a problem where encoder-decoder architectures have been successful in the past.</p>

<p><img src="https://jalammar.github.io/images/xlnet/transformer-encoder-decoder.png">
  <br>
</p>

<p>A lot of the subsequent research work saw the architecture shed either the encoder or decoder, and use just one stack of transformer blocks – stacking them up as high as practically possible, feeding them massive amounts of training text, and throwing vast amounts of compute at them (hundreds of thousands of dollars to train some of these language models, likely millions in the case of <a href="https://deepmind.com/blog/alphastar-mastering-real-time-strategy-game-starcraft-ii/">AlphaStar</a>).</p>

<p><img src="https://jalammar.github.io/images/gpt2/gpt-2-transformer-xl-bert-3.png">
  <br>
</p>

<p>How high can we stack up these blocks? It turns out that’s one of the main distinguishing factors between the different GPT2 model sizes:</p>

<p><img src="https://jalammar.github.io/images/gpt2/gpt2-sizes-hyperparameters-3.png">
  <br>
</p>

<h3 id="one-difference-from-bert">One Difference From BERT</h3>
<blockquote>
<strong>First Law of Robotics</strong><br>
A robot may not injure a human being or, through inaction, allow a human being to come to harm.
</blockquote>

<p>The GPT-2 is built using transformer decoder blocks. BERT, on the other hand, uses transformer encoder blocks. We will examine the difference in a following section. But one key difference between the two is that GPT2, like traditional language models, outputs one token at a time. Let’s for example prompt a well-trained GPT-2 to recite the first law of robotics:</p>

<p><img src="https://jalammar.github.io/images/xlnet/gpt-2-output.gif">
  <br>
</p>

<p>The way these models actually work is that after each token is produced, that token is added to the sequence of inputs. And that new sequence becomes the input to the model in its next step. This is an idea called “auto-regression”. This is one of the ideas that <a href="https://karpathy.github.io/2015/05/21/rnn-effectiveness/">made RNNs unreasonably effective</a>.</p>

<p><img src="https://jalammar.github.io/images/xlnet/gpt-2-autoregression-2.gif">
  <br>
</p>

<p>The GPT2, and some later models like TransformerXL and XLNet are auto-regressive in nature. BERT is not. That is a trade off. In losing auto-regression, BERT gained the ability to incorporate the context on both sides of a word to gain better results. XLNet brings back autoregression while finding an alternative way to incorporate the context on both sides.</p>

<h3 id="the-evolution-of-the-transformer-block">The Evolution of the Transformer Block</h3>

<p>The <a href="https://arxiv.org/abs/1706.03762">initial transformer paper</a> introduced two types of transformer blocks:</p>

<h4 id="the-encoder-block">The Encoder Block</h4>

<p>First is the encoder block:</p>

<p><img src="https://jalammar.github.io/images/xlnet/transformer-encoder-block-2.png">
  <br>
  An encoder block from the original transformer paper can take inputs up until a certain max sequence length (e.g. 512 tokens). It's okay if an input sequence is shorter than this limit, we can just pad the rest of the sequence.
</p>

<h4 id="the-decoder-block">The Decoder Block</h4>
<p>Second, there’s the decoder block which has a small architectural variation from the encoder block – a layer to allow it to pay attention to specific segments from the encoder:</p>

<p><img src="https://jalammar.github.io/images/xlnet/transformer-decoder-block-2.png">
  <br>
</p>

<p>One key difference in the self-attention layer here, is that it masks future tokens – not by changing the word to [mask] like BERT, but by interfering in the self-attention calculation blocking information from tokens that are to the right of the position being calculated.</p>

<p>If, for example, we’re to highlight the path of position #4, we can see that it is only allowed to attend to the present and previous tokens:</p>

<p><img src="https://jalammar.github.io/images/xlnet/transformer-decoder-block-self-attention-2.png">
  <br>
</p>

<p>It’s important that the distinction between self-attention (what BERT uses) and masked self-attention (what GPT-2 uses) is clear. A normal self-attention block allows a position to peak at tokens to its right. Masked self-attention prevents that from happening:</p>

<p><img src="https://jalammar.github.io/images/gpt2/self-attention-and-masked-self-attention.png">
  <br>
</p>

<h4 id="the-decoder-only-block">The Decoder-Only Block</h4>
<p>Subsequent to the original paper, <a href="https://arxiv.org/pdf/1801.10198.pdf">Generating Wikipedia by Summarizing Long Sequences</a> proposed another arrangement of the transformer block that is capable of doing language modeling. This model threw away the Transformer encoder. For that reason, let’s call the model the “Transformer-Decoder”. This early transformer-based language model was made up of a stack of six transformer decoder blocks:</p>

<p><img src="https://jalammar.github.io/images/xlnet/transformer-decoder-intro.png">
  <br>
  The decoder blocks are identical. I have expanded the first one so you can see its self-attention layer is the masked variant. Notice that the model now can address up to 4,000 tokens in a certain segment -- a massive upgrade from the 512 in the original transformer.
</p>

<p>These blocks were very similar to the original decoder blocks, except they did away with that second self-attention layer. A similar architecture was examined in <a href="https://arxiv.org/pdf/1808.04444.pdf">Character-Level Language Modeling with Deeper Self-Attention</a> to create a language model that predicts one letter/character at a time.</p>

<p>The OpenAI GPT-2 model uses these decoder-only blocks.</p>

<h3 id="crash-course-in-brain-surgery-looking-inside-gpt-2">Crash Course in Brain Surgery: Looking Inside GPT-2</h3>

<blockquote>
  <p>Look inside and you will see,
The words are cutting deep inside my brain.
Thunder burning, quickly burning,
Knife of words is driving me insane, insane yeah.
~<strong><a href="https://en.wikipedia.org/wiki/Budgie_(band)">Budgie</a></strong></p>

</blockquote>

<p>Let’s lay a trained GPT-2 on our surgery table and look at how it works.</p>

<p><img src="https://jalammar.github.io/images/gpt2/gpt-2-layers-2.png">
  <br>
  The GPT-2 can process 1024 tokens. Each token flows through all the decoder blocks along its own path.
</p>

<p>The simplest way to run a trained GPT-2 is to allow it to ramble on its own (which is technically called <em>generating unconditional samples</em>) – alternatively, we can give it a prompt to have it speak about a certain topic (a.k.a generating <em>interactive conditional samples</em>). In the rambling case, we can simply hand it the start token and have it start generating words (the trained model uses <code>&lt;|endoftext|&gt;</code> as its start token. Let’s call it <code>&lt;s&gt;</code> instead).</p>

<div>
  <p><img src="https://jalammar.github.io/images/gpt2/gpt2-simple-output-2.gif"></p>
</div>

<p>The model only has one input token, so that path would be the only active one. The token is processed successively through all the layers, then a vector is produced along that path. That vector can be scored against the model’s vocabulary (all the words the model knows, 50,000 words in the case of GPT-2). In this case we selected the token with the highest probability, ‘the’. But we can certainly mix things up – you know how if you keep clicking the suggested word in your keyboard app, it sometimes can stuck in repetitive loops where the only way out is if you click the second or third suggested word. The same can happen here. GPT-2 has a parameter called top-k that we can use to have the model consider sampling words other than the top word (which is the case when top-k = 1).</p>

<p>In the next step, we add the output from the first step to our input sequence, and have the model make its next prediction:</p>

<p><img src="https://jalammar.github.io/images/gpt2/gpt-2-simple-output-3.gif">
  <br>
</p>

<p>Notice that the second path is the only one that’s active in this calculation. Each layer of GPT-2 has retained its own interpretation of the first token and will use it in processing the second token (we’ll get into more detail about this in the following section about self-attention). GPT-2 does not re-interpret the first token in light of the second token.</p>

<h3 id="a-deeper-look-inside">A Deeper Look Inside</h3>

<h4 id="input-encoding">Input Encoding</h4>
<p>Let’s look at more details to get to know the model more intimately. Let’s start from the input. As in other NLP models we’ve discussed before, the model looks up the embedding of the input word in its embedding matrix – one of the components we get as part of a trained model.</p>

<p><img src="https://jalammar.github.io/images/gpt2/gpt2-token-embeddings-wte-2.png">
  <br>
  Each row is a word embedding: a list of numbers representing a word and capturing some of its meaning. The size of that list is different in different GPT2 model sizes. The smallest model uses an embedding size of 768 per word/token.
</p>

<p>So in the beginning, we look up the embedding of the start token <code>&lt;s&gt;</code> in the embedding matrix. Before handing that to the first block in the model, we need to incorporate positional encoding – a signal that indicates the order of the words in the sequence to the transformer blocks. Part of the trained model is a matrix that contains a positional encoding vector for each of the 1024 positions in the input.</p>

<p><img src="https://jalammar.github.io/images/gpt2/gpt2-positional-encoding.png">
  <br>
</p>

<p>With this, we’ve covered how input words are processed before being handed to the first transformer block. We also know two of the weight matrices that constitute the trained GPT-2.</p>

<p><img src="https://jalammar.github.io/images/gpt2/gpt2-input-embedding-positional-encoding-3.png">
  <br>
  Sending a word to the first transformer block means looking up its embedding and adding up the positional encoding vector for position #1.
</p>

<h4 id="a-journey-up-the-stack">A journey up the Stack</h4>

<p>The first block can now process the token by first passing it through the self-attention process, then passing it through its neural network layer. Once the first transformer block processes the token, it sends its resulting vector up the stack to be processed by the next block. The process is identical in each block, but each block has its own weights in both self-attention and the neural network sublayers.</p>

<p><img src="https://jalammar.github.io/images/gpt2/gpt2-transformer-block-vectors-2.png">
  <br>
</p>

<h4 id="self-attention-recap">Self-Attention Recap</h4>

<p>Language heavily relies on context. For example, look at the second law:</p>

<blockquote>

<strong>Second Law of Robotics</strong><br>
A robot must obey the orders given <strong>it</strong> by human beings except where <strong>such orders</strong> would conflict with the <strong>First Law</strong>.

</blockquote>

<p>I have highlighted three places in the sentence where the words are referring to other words. There is no way to understand or process these words without incorporating the context they are referring to. When a model processes this sentence, it has to be able to know that:</p>
<ul>
  <li><strong>it</strong> refers to the robot</li>
  <li><strong>such orders</strong> refers to the earlier part of the law, namely “the orders given it by human beings”</li>
  <li><strong>The First Law</strong> refers to the entire First Law</li>
</ul>

<p>This is what self-attention does. It bakes in the model’s understanding of relevant and associated words that explain the context of a certain word before processing that word (passing it through a neural network). It does that by assigning scores to how relevant each word in the segment is, and adding up their vector representation.</p>

<p>As an example, this self-attention layer in the top block is paying attention to “a robot” when it processes the word “it”. The vector it will pass to its neural network is a sum of the vectors for each of the three words multiplied by their scores.</p>

<p><img src="https://jalammar.github.io/images/gpt2/gpt2-self-attention-example-2.png">
  <br>
</p>

<h4 id="self-attention-process">Self-Attention Process</h4>
<p>Self-attention is processed along the path of each token in the segment. The significant components are three vectors:</p>

<ul>
  <li><span>Query</span>: The query is a representation of the current word used to score against all the other words (using their keys). We only care about the query of the token we’re currently processing.</li>
  <li><span>Key</span>: Key vectors are like labels for all the words in the segment. They’re what we match against in our search for relevant words.</li>
  <li><span>Value</span>: Value vectors are actual word representations, once we’ve scored how relevant each word is, these are the values we add up to represent the current word.</li>
</ul>

<p><img src="https://jalammar.github.io/images/gpt2/self-attention-example-folders-3.png">
  <br>
</p>

<p>A crude analogy is to think of it like searching through a filing cabinet. The query is like a sticky note with the topic you’re researching. The keys are like the labels of the folders inside the cabinet. When you match the tag with a sticky note, we take out the contents of that folder, these contents are the value vector. Except you’re not only looking for one value, but a blend of values from a blend of folders.</p>

<p>Multiplying the query vector by each key vector produces a score for each folder (technically: dot product followed by softmax).</p>

<p><img src="https://jalammar.github.io/images/gpt2/self-attention-example-folders-scores-3.png">
  <br>
</p>

<p>We multiply each value by its score and sum up – resulting in our self-attention outcome.</p>

<p><img src="https://jalammar.github.io/images/gpt2/gpt2-value-vector-sum.png">
  <br>
</p>

<p>This weighted blend of value vectors results in a vector that paid 50% of its “attention” to the word <code>robot</code>, 30% to the word <code>a</code>, and 19% to the word <code>it</code>. Later in the post, we’ll got deeper into self-attention. But first, let’s continue our journey up the stack towards the output of the model.</p>

<h4 id="model-output">Model Output</h4>

<p>When the top block in the model produces its output vector (the result of its own self-attention followed by its own neural network), the model multiplies that vector by the embedding matrix.</p>

<p><img src="https://jalammar.github.io/images/gpt2/gpt2-output-projection-2.png">
  <br>
</p>

<p>Recall that each row in the embedding matrix corresponds to the embedding of a word in the model’s vocabulary. The result of this multiplication is interpreted as a score for each word in the model’s vocabulary.</p>

<p><img src="https://jalammar.github.io/images/gpt2/gpt2-output-scores-2.png">
  <br>
</p>

<p>We can simply select the token with the highest score (top_k = 1). But better results are achieved if the model considers other words as well. So a better strategy is to sample a word from the entire list using the score as the probability of selecting that word (so words with a higher score have a higher chance of being selected). A middle ground is setting top_k to 40, and having the model consider the 40 words with the highest scores.</p>

<p><img src="https://jalammar.github.io/images/gpt2/gpt2-output.png">
  <br>
</p>

<p>With that, the model has completed an iteration resulting in outputting a single word. The model continues iterating until the entire context is generated (1024 tokens) or until an end-of-sequence token is produced.</p>

<h3 id="end-of-part-1-the-gpt-2-ladies-and-gentlemen">End of part #1: The GPT-2, Ladies and Gentlemen</h3>

<p>And there we have it. A run down of how the GPT2 works. If you’re curious to know exactly what happens inside the self-attention layer, then the following bonus section is for you. I created it to introduce more visual language to describe self-attention in order to make describing later transformer models easier to examine and describe (looking at you, TransformerXL and XLNet).</p>

<p>I’d like to note a few oversimplifications in this post:</p>
<ul>
  <li>I used “words” and “tokens” interchangeably. But in reality, GPT2 uses Byte Pair Encoding to create the tokens in its vocabulary. This means the tokens are usually parts of words.</li>
  <li>The example we showed runs GPT2 in its inference/evaluation mode. That’s why it’s only processing one word at a time. At training time, the model would be trained against longer sequences of text and processing multiple tokens at once. Also at training time, the model would process larger batch sizes (512) vs. the batch size of one that evaluation uses.</li>
  <li>I took liberties in rotating/transposing vectors to better manage the spaces in the images. At implementation time, one has to be more precise.</li>
  <li>Transformers use a lot of layer normalization, which is pretty important. We’ve noted a few of these in the Illustrated Transformer, but focused more on self-attention in this post.</li>
  <li>There are times when I needed to show more boxes to represent a vector. I indicate those as “zooming in”. For example:</li>
</ul>

<p><img src="https://jalammar.github.io/images/gpt2/zoom-in.png">
  <br>
</p>

<h2 id="part-2-the-illustrated-self-attention-">Part #2: The Illustrated Self-Attention <a name="part-2-illustrated-self-attention" href="#part-2-illustrated-self-attention">#</a></h2>

<p>Earlier in the post we showed this image to showcase self-attention being applied in a layer that is processing the word <code>it</code>:</p>

<p><img src="https://jalammar.github.io/images/gpt2/gpt2-self-attention-1-2.png">
  <br>
</p>

<p>In this section, we’ll look at the details of how that is done. Note that we’ll look at it in a way to try to make sense of what happens to individual words. That’s why we’ll be showing many single vectors. The actual implementations are done by multiplying giant matrices together. But I want to focus on the intuition of what happens on a word-level here.</p>

<h3 id="self-attention-without-masking">Self-Attention (without masking)</h3>
<p>Let’s start by looking at the original self-attention as it’s calculated in an encoder block. Let’s look at a toy transformer block that can only process four tokens at a time.</p>

<p>Self-attention is applied through three main steps:</p>

<ol>
  <li>Create the Query, Key, and Value vectors for each path.</li>
  <li>For each input token, use its query vector to score against all the other key vectors</li>
  <li>Sum up the value vectors after multiplying them by their associated scores.</li>
</ol>

<p><img src="https://jalammar.github.io/images/xlnet/self-attention-summary.png">
  <br>
</p>

<h3 id="1--create-query-key-and-value-vectors">1- Create Query, Key, and Value Vectors</h3>
<p>Let’s focus on the first path. We’ll take its query, and compare against all the keys. That produces a score for each key. The first step in self-attention is to calculate the three vectors for each token path (let’s ignore attention heads for now):</p>

<p><img src="https://jalammar.github.io/images/xlnet/self-attention-1.png">
  <br>
</p>

<h3 id="2--score">2- Score</h3>
<p>Now that we have the vectors, we use the query and key vectors only for step #2. Since we’re focused on the first token, we multiply its query by all the other key vectors resulting in a score for each of the four tokens.</p>
<p><img src="https://jalammar.github.io/images/xlnet/self-attention-2.png">
  <br>
</p>

<h3 id="3--sum">3- Sum</h3>

<p>We can now multiply the scores by the value vectors. A value with a high score will constitute a large portion of the resulting vector after we sum them up.</p>

<p><img src="https://jalammar.github.io/images/xlnet/self-attention-3-2.png">
  <br>
  The lower the score, the more transparent we're showing the value vector. That's to indicate how multiplying by a small number dilutes the values of the vector.
</p>

<p>If we do the same operation for each path, we end up with a vector representing each token containing the appropriate context of that token. Those are then presented to the next sublayer in the transformer block (the feed-forward neural network):</p>

<p><img src="https://jalammar.github.io/images/xlnet/self-attention-summary.png">
  <br>
</p>

<h3 id="the-illustrated-masked-self-attention">The Illustrated Masked Self-Attention</h3>

<p>Now that we’ve looked inside a transformer’s self-attention step, let’s proceed to look at masked self-attention. Masked self-attention is identical to self-attention except when it comes to step #2. Assuming the model only has two tokens as input and we’re observing the second token. In this case, the last two tokens are masked. So the model interferes in the scoring step. It basically always scores the future tokens as 0 so the model can’t peak to future words:</p>

<p><img src="https://jalammar.github.io/images/xlnet/masked-self-attention-2.png">
  <br>
</p>

<p>This masking is often implemented as a matrix called an attention mask. Think of a sequence of four words (“robot must obey orders”, for example). In a language modeling scenario, this sequence is absorbed in four steps – one per word (assuming for now that every word is a token). As these models work in batches, we can assume a batch size of 4 for this toy model that will process the entire sequence (with its four steps) as one batch.</p>

<p><img src="https://jalammar.github.io/images/gpt2/transformer-decoder-attention-mask-dataset.png">
  <br>
</p>

<p>In matrix form, we calculate the scores by multiplying a queries matrix by a keys matrix. Let’s visualize it as follows, except instead of the word, there would be the query (or key) vector associated with that word in that cell:</p>

<p><img src="https://jalammar.github.io/images/gpt2/queries-keys-attention-mask.png">
  <br>
</p>

<p>After the multiplication, we slap on our attention mask triangle. It set the cells we want to mask to -infinity or a very large negative number (e.g. -1 billion in GPT2):</p>

<p><img src="https://jalammar.github.io/images/gpt2/transformer-attention-mask.png">
  <br>
</p>

<p>Then, applying softmax on each row produces the actual scores we use for self-attention:</p>

<p><img src="https://jalammar.github.io/images/gpt2/transformer-attention-masked-scores-softmax.png">
  <br>
</p>

<p>What this scores table means is the following:</p>
<ul>
  <li>When the model processes the first example in the dataset (row #1), which contains only one word (“robot”), 100% of its attention will be on that word.</li>
  <li>When the model processes the second example in the dataset (row #2), which contains the words (“robot must”), when it processes the word “must”, 48% of its attention will be on “robot”, and 52% of its attention will be on “must”.</li>
  <li>And so on</li>
</ul>

<h3 id="gpt-2-masked-self-attention">GPT-2 Masked Self-Attention</h3>
<p>Let’s get into more detail on GPT-2’s masked attention.</p>

<h4 id="evaluation-time-processing-one-token-at-a-time">Evaluation Time: Processing One Token at a Time</h4>
<p>We can make the GPT-2 operate exactly as masked self-attention works. But during evaluation, when our model is only adding one new word after each iteration, it would be inefficient to recalculate self-attention along earlier paths for tokens that have already been processed.</p>

<p>In this case, we process the first token (ignoring <code>&lt;s&gt;</code> for now).</p>

<p><img src="https://jalammar.github.io/images/gpt2/gpt2-self-attention-qkv-1-2.png">
  <br>
</p>

<p>GPT-2 holds on to the key and value vectors of the the <code>a</code> token. Every self-attention layer holds on to its respective key and value vectors for that token:</p>

<p><img src="https://jalammar.github.io/images/gpt2/gpt2-self-attention-qkv-2-2.png">
  <br>
</p>

<p>Now in the next iteration, when the model processes the word <code>robot</code>, it does not need to generate query, key, and value queries for the <code>a</code> token. It just reuses the ones it saved from the first iteration:</p>

<p><img src="https://jalammar.github.io/images/gpt2/gpt2-self-attention-qkv-3-2.png">
  <br>
</p>

<h4 id="gpt-2-self-attention-1--creating-queries-keys-and-values">GPT-2 Self-attention: 1- Creating queries, keys, and values</h4>

<p>Let’s assume the model is processing the word <code>it</code>. If we’re talking about the bottom block, then its input for that token would be the embedding of <code>it</code> + the positional encoding for slot #9:</p>

<p><img src="https://jalammar.github.io/images/gpt2/gpt2-self-attention-1.png">
  <br>
</p>

<p>Every block in a transformer has its own weights (broken down later in the post). The first we encounter is the weight matrix that we use to create the queries, keys, and values.</p>

<p><img src="https://jalammar.github.io/images/gpt2/gpt2-self-attention-2.png">
  <br>
  Self-attention multiplies its input by its weight matrix (and adds a bias vector, not illustrated here).
</p>

<p>The multiplication results in a vector that’s basically a concatenation of the query, key, and value vectors for the word <code>it</code>.</p>

<p><img src="https://jalammar.github.io/images/gpt2/gpt2-self-attention-3.png">
  <br>
  Multiplying the input vector by the attention weights vector (and adding a bias vector aftwards) results in the key, value, and query vectors for this token.
</p>

<h4 id="gpt-2-self-attention-15--splitting-into-attention-heads">GPT-2 Self-attention: 1.5- Splitting into attention heads</h4>

<p>In the previous examples, we dove straight into self-attention ignoring the “multi-head” part. It would be useful to shed some light on that concept now. Self attention is conducted multiple times on different parts of the Q,K,V vectors. “Splitting” attention heads is simply reshaping the long vector into a matrix. The small GPT2 has 12 attention heads, so that would be the first dimension of the reshaped matrix:</p>

<p><img src="https://jalammar.github.io/images/gpt2/gpt2-self-attention-split-attention-heads-1.png">
  <br>
</p>

<p>In the previous examples, we’ve looked at what happens inside one attention head. One way to think of multiple attention-heads is like this (if we’re to only visualize three of the twelve attention heads):</p>

<p><img src="https://jalammar.github.io/images/gpt2/gpt2-self-attention-split-attention-heads-2.png">
  <br>
</p>

<h4 id="gpt-2-self-attention-2--scoring">GPT-2 Self-attention: 2- Scoring</h4>
<p>We can now proceed to scoring – knowing that we’re only looking at one attention head (and that all the others are conducting a similar operation):</p>

<p><img src="https://jalammar.github.io/images/gpt2/gpt2-self-attention-scoring.png">
  <br>
</p>

<p>Now the token can get scored against all of keys of the other tokens (that were calculated in attention head #1 in previous iterations):</p>

<p><img src="https://jalammar.github.io/images/gpt2/gpt2-self-attention-scoring-2.png">
  <br>
</p>

<h4 id="gpt-2-self-attention-3--sum">GPT-2 Self-attention: 3- Sum</h4>
<p>As we’ve seen before, we now multiply each value with its score, then sum them up, producing the result of self-attention for attention-head #1:</p>

<p><img src="https://jalammar.github.io/images/gpt2/gpt2-self-attention-multihead-sum-1.png">
  <br>
</p>

<h4 id="gpt-2-self-attention-35--merge-attention-heads">GPT-2 Self-attention: 3.5- Merge attention heads</h4>

<p>The way we deal with the various attention heads is that we first concatenate them into one vector:</p>
<p><img src="https://jalammar.github.io/images/gpt2/gpt2-self-attention-merge-heads-1.png">
  <br>
</p>

<p>But the vector isn’t ready to be sent to the next sublayer just yet. We need to first turn this Frankenstein’s-monster of hidden states into a homogenous representation.</p>

<h4 id="gpt-2-self-attention-4--projecting">GPT-2 Self-attention: 4- Projecting</h4>

<p>We’ll let the model learn how to best map concatenated self-attention results into a vector that the feed-forward neural network can deal with. Here comes our second large weight matrix that projects the results of the attention heads into the output vector of the self-attention sublayer:</p>

<p><img src="https://jalammar.github.io/images/gpt2/gpt2-self-attention-project-1.png">
  <br>
</p>

<p>And with this, we have produced the vector we can send along to the next layer:</p>

<p><img src="https://jalammar.github.io/images/gpt2/gpt2-self-attention-project-2.png">
  <br>
</p>

<h4 id="gpt-2-fully-connected-neural-network-layer-1">GPT-2 Fully-Connected Neural Network: Layer #1</h4>

<p>The fully-connected neural network is where the block processes its input token after self-attention has included the appropriate context in its representation. It is made up of two layers. The first layer is four times the size of the model (Since GPT2 small is 768, this network would have 768*4 = 3072 units). Why four times? That’s just the size the original transformer rolled with (model dimension was 512 and layer #1 in that model was 2048). This seems to give transformer models enough representational capacity to handle the tasks that have been thrown at them so far.</p>

<p><img src="https://jalammar.github.io/images/gpt2/gpt2-mlp1.gif">
  <br>
  (Not shown: A bias vector)
</p>

<h4 id="gpt-2-fully-connected-neural-network-layer-2---projecting-to-model-dimension">GPT-2 Fully-Connected Neural Network: Layer #2 - Projecting to model dimension</h4>

<p>The second layer projects the result from the first layer back into model dimension (768 for the small GPT2). The result of this multiplication is the result of the transformer block for this token.</p>

<p><img src="https://jalammar.github.io/images/gpt2/gpt2-mlp-2.gif">
  <br>
  (Not shown: A bias vector)
</p>

<h3 id="youve-made-it">You’ve Made <span>It</span>!</h3>
<p>That’s the most detailed version of the transformer block we’ll get into! You now pretty much have the vast majority of the picture of what happens inside of a transformer language model. To recap, our brave input vector encounters these weight matrices:</p>

<p><img src="https://jalammar.github.io/images/gpt2/gpt2-transformer-block-weights-2.png">
  <br>
</p>

<p>And each block has its own set of these weights. On the other hand, the model has only one token embedding matrix and one positional encoding matrix:</p>

<p><img src="https://jalammar.github.io/images/gpt2/gpt2-weights-2.png">
  <br>
</p>

<p>If you want to see all the parameters of the model, then I have tallied them here:</p>

<p><img src="https://jalammar.github.io/images/gpt2/gpt2-117-parameters.png">
  <br>
</p>

<p>They add up to 124M parameters instead of 117M for some reason. I’m not sure why, but that’s how many of them seems to be in the published code (please correct me if I’m wrong).</p>

<h2 id="part-3-beyond-language-modeling-">Part 3: Beyond Language Modeling <a href="#part-3-beyond-language-modeling" name="part-3-beyond-language-modeling">#</a></h2>
<p>The decoder-only transformer keeps showing promise beyond language modeling. There are plenty of applications where it has shown success which can be described by similar visuals as the above. Let’s close this post by looking at some of these applications</p>

<h3 id="machine-translation">Machine Translation</h3>
<p>An encoder is not required to conduct translation. The same task can be addressed by a decoder-only transformer:</p>

<p><img src="https://jalammar.github.io/images/gpt2/decoder-only-transformer-translation.png">
  <br>
</p>

<h3 id="summarization">Summarization</h3>

<p>This is the task that the first decoder-only transformer was trained on. Namely, it was trained to read a wikipedia article (without the opening section before the table of contents), and to summarize it. The actual opening sections of the articles were used as the labels in the training datasest:</p>

<p><img src="https://jalammar.github.io/images/gpt2/wikipedia-summarization.png">
  <br>
</p>

<p>The paper trained the model against wikipedia articles, and thus the trained model was able to summarize articles:</p>

<p><img src="https://jalammar.github.io/images/gpt2/decoder-only-summarization.png">
  <br>
</p>

<h3 id="transfer-learning">Transfer Learning</h3>
<p>In <a href="https://arxiv.org/abs/1905.08836">Sample Efficient Text Summarization Using a Single Pre-Trained Transformer</a>, a decoder-only transformer is first pre-trained on language modeling, then finetuned to do summarization. It turns out to achieve better results than a pre-trained encoder-decoder transformer in limited data settings.</p>

<p>The GPT2 paper also shows results of summarization after pre-training the model on language modeling.</p>

<h3 id="music-generation">Music Generation</h3>
<p>The <a href="https://magenta.tensorflow.org/music-transformer">Music Transformer</a> uses a decoder-only transformer to generate music with expressive timing and dynamics. “Music Modeling” is just like language modeling – just let the model learn music in an unsupervised way, then have it sample outputs (what we called “rambling”, earlier).</p>

<p>You might be curious as to how music is represented in this scenario. Remember that language modeling can be done through vector representations of either characters, words, or tokens that are parts of words. With a musical performance (let’s think about the piano for now), we have to represent the notes, but also velocity – a measure of how hard the piano key is pressed.</p>

<p><img src="https://jalammar.github.io/images/gpt2/music-transformer-performance-encoding-3.png">
  <br>
</p>

<p>A performance is just a series of these one-hot vectors. A midi file can be converted into such a format. The paper has the following example input sequence:</p>

<p><img src="https://jalammar.github.io/images/gpt2/music-representation-example.png">
  <br>
</p>

<p>The one-hot vector representation for this input sequence would look like this:</p>

<p><img src="https://jalammar.github.io/images/gpt2/music-transformer-input-representation-2.png">
  <br>
</p>

<p>I love a visual in the paper that showcases self-attention in the Music Transformer. I’ve added some annotations to it here:</p>

<p><img src="https://jalammar.github.io/images/gpt2/music-transformer-self-attention-2.png">
  <br>
  "Figure 8: This piece has a recurring triangular contour. The query is at one of the latter peaks and it attends to all of the previous high notes on the peak, all the way to beginning of the piece." ... "[The] figure shows a query (the source of all the attention lines) and previous memories being attended to (the notes that are receiving more softmax probabiliy is highlighted in). The coloring of the attention lines correspond to different heads and the width to the weight of the softmax probability."
</p>

<p>If you’re unclear on this representation of musical notes, <a href="https://www.youtube.com/watch?v=ipzR9bhei_o">check out this video</a>.</p>

<h2 id="conclusion">Conclusion</h2>
<p>This concludes our journey into the GPT2, and our exploration of its parent model, the decoder-only transformer. I hope that you come out of this post with a better understanding of self-attention and more comfort that you understand more of what goes on inside a transformer.</p>

<h2 id="resources">Resources</h2>
<ul>
  <li>The <a href="https://github.com/openai/gpt-2">GPT2 Implementation</a> from OpenAI</li>
  <li>Check out the <a href="https://github.com/huggingface/pytorch-transformers">pytorch-transformers</a> library from <a href="https://huggingface.co/">Hugging Face</a> in addition to GPT2, it implements BERT, Transformer-XL, XLNet and other cutting-edge transformer models.</li>
</ul>

<h2 id="acknowledgements">Acknowledgements</h2>
<p>Thanks to <a href="https://twitter.com/lukaszkaiser">Lukasz Kaiser</a>, <a href="https://www.cl.uzh.ch/de/people/team/compling/mmueller.html">Mathias Müller</a>, <a href="https://twitter.com/peterjliu">Peter J. Liu</a>, <a href="https://twitter.com/rsepassi">Ryan Sepassi</a> and <a href="https://www.linkedin.com/in/mohammad-saleh-39614224/">Mohammad Saleh</a> for feedback on earlier versions of this post.</p>

<p>Comments or corrections? Please tweet me at <a href="https://twitter.com/JayAlammar">@JayAlammar</a></p>

<!--
### Just Add Memory

So far, our models have only considered the keys and values from the current segment. What's to stop us from adding a bunch more keys and values representing words from previous tokens? Nothing stops us! That's exactly what memory is in this context


<div class="img-div-any-width" markdown="0">
  <image src="/images/xlnet/memory-self-attention.png"/>
  <br />
</div>

And there we have it! The model can now incorporate all previous tokens in previous segments into the self-attention calculation.


Let's go over an example to make sure we're on the same page. Say we want to process the first eight words of The Second Law using a toy memory-transformer with one block and four token segment length. From now on, we'll show vectors vertically rather than horizontally so we can squeeze them into matrices:


### Memory-Compression

In practice, we can very quickly run out memory if we memorize the keys and values of all previous tokens in a long text sequence. Here, we can turn to the idea of compressing this memory to save space. If we're to rotate our key and value vectors like the following:

<div class="img-div-any-width" markdown="0">
  <image src="/images/xlnet/keys-and-values.png"/>
  <br />
</div>

We can compress it by compressing every three vectors into one:

<div class="img-div-any-width" markdown="0">
  <image src="/images/xlnet/transformer-memory-compression.png"/>
  <br />
</div>

The compression is done using a convolutional neural network which learns (during training time) how to effectively turn every three key vectors into a a single vector. Likewise with the values vectors. Again, in technical jargon, the compression is done using a CNN with a kernel size of 3 and a stride of 3.
-->

  </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Why do programmers need private offices with doors? (266 pts)]]></title>
            <link>https://blobstreaming.org/why-do-programmers-need-private-offices-with-doors-do-not-disturb/</link>
            <guid>38691468</guid>
            <pubDate>Tue, 19 Dec 2023 03:30:02 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://blobstreaming.org/why-do-programmers-need-private-offices-with-doors-do-not-disturb/">https://blobstreaming.org/why-do-programmers-need-private-offices-with-doors-do-not-disturb/</a>, See on <a href="https://news.ycombinator.com/item?id=38691468">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>

    <article>

        <header>

            

            <div>
                    
                    <p><time datetime="2023-12-18">Dec 18, 2023</time>
                            <span><span>—</span> 10 min read</span>
                    </p>
                </div>

                <figure>
        <img srcset="https://images.unsplash.com/photo-1532550256335-c281a64ac9f6?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=M3wxMTc3M3wwfDF8c2VhcmNofDExfHxkb29yfGVufDB8fHx8MTcwMjg4Njg3MHww&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=320 320w,
                    https://images.unsplash.com/photo-1532550256335-c281a64ac9f6?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=M3wxMTc3M3wwfDF8c2VhcmNofDExfHxkb29yfGVufDB8fHx8MTcwMjg4Njg3MHww&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=600 600w,
                    https://images.unsplash.com/photo-1532550256335-c281a64ac9f6?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=M3wxMTc3M3wwfDF8c2VhcmNofDExfHxkb29yfGVufDB8fHx8MTcwMjg4Njg3MHww&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=960 960w,
                    https://images.unsplash.com/photo-1532550256335-c281a64ac9f6?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=M3wxMTc3M3wwfDF8c2VhcmNofDExfHxkb29yfGVufDB8fHx8MTcwMjg4Njg3MHww&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=1200 1200w,
                    https://images.unsplash.com/photo-1532550256335-c281a64ac9f6?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=M3wxMTc3M3wwfDF8c2VhcmNofDExfHxkb29yfGVufDB8fHx8MTcwMjg4Njg3MHww&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=2000 2000w" src="https://images.unsplash.com/photo-1532550256335-c281a64ac9f6?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=M3wxMTc3M3wwfDF8c2VhcmNofDExfHxkb29yfGVufDB8fHx8MTcwMjg4Njg3MHww&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=1200" alt="Why do programmers need private offices with doors? (Do Not Disturb)">
            <figcaption><span>Photo by </span><a href="https://unsplash.com/@pawel_czerwinski?utm_source=ghost&amp;utm_medium=referral&amp;utm_campaign=api-credit"><span>Pawel Czerwinski</span></a><span> / </span><a href="https://unsplash.com/?utm_source=ghost&amp;utm_medium=referral&amp;utm_campaign=api-credit"><span>Unsplash</span></a></figcaption>
    </figure>

        </header>

        <section>
            <p>It’s a common occurrence: You’re sitting at your desk, lost in thought, trying to solve a problem that’s been blocking your work all week.</p>
<!--kg-card-begin: html-->

<!-- ad1 -->
<ins data-ad-client="ca-pub-7131771765855059" data-ad-slot="1765624809" data-ad-format="auto" data-full-width-responsive="true"></ins>

<!--kg-card-end: html-->
<p>Deep in your brain you’re building a structure of thoughts and possibilities undreamed of in anyone’s philosophy: You identify concepts and stack them against each other, turning them and flipping them and slotting them together. Like a keystone arch, as soon as it’s finished it’ll be strong enough to stand for generations—but it would topple to the ground in an instant if you stopped holding the pieces together before it was done.</p><figure><img src="https://blobstreaming.org/content/images/2023/12/AR37TxJoQWia0YSaWqN4.jpg" alt="" loading="lazy" width="640" height="429" srcset="https://blobstreaming.org/content/images/size/w600/2023/12/AR37TxJoQWia0YSaWqN4.jpg 600w, https://blobstreaming.org/content/images/2023/12/AR37TxJoQWia0YSaWqN4.jpg 640w"><figcaption><span>Keystone of the Monumental Arch, Palmyra · ©&nbsp;Verity Cridland/flickr</span></figcaption></figure><p>A colleague comes along and notices you sitting there, maybe with your mouth hanging ever-so-slightly open, maybe with your head cocked sideways and one eye twitching and the general look of a person who has just inhaled a bug (just me?), and it’s obvious you’re not doing anything, so he taps you on the shoulder and says, "Can I just steal your brain for a moment? I wanted to ask you…"</p><p>And there, in that instant, the glorious half-arch you've been struggling to hold together comes crashing to the ground around you.</p>
<!--kg-card-begin: html-->

<!-- ad2 -->
<ins data-ad-client="ca-pub-7131771765855059" data-ad-slot="2751688820" data-ad-format="auto" data-full-width-responsive="true"></ins>

<!--kg-card-end: html-->
<p>Like many of life’s most infuriating events, it’s easy to pin this one on people just generally being terrible. And hey, maybe people are just terrible. But there’s another theory which says it all comes down to miscommunication and different people accustomed to two very different kinds of work, so (in the name of charity and goodwill) let’s examine the generous possibility instead.</p><h2 id="brick-by-brick">Brick by Brick</h2><p>These two types of work were first identified, to my knowledge, by the programmer and essayist Paul Graham. The first type of work he describes can be interrupted harmlessly—nothing is lost if you stop for a second (or an hour, or a day) and come back to it later. An example might be building a simple pyramid out of wooden blocks (don’t pretend you don’t miss it). If someone wants to talk to you while you’re building your wooden pyramid you can easily stop, put down the block you were holding, and go off to chat for a minute or five; the tower will still be there when you come back, and you’ll have no problem picking up where you left off.</p><figure><img src="https://web.archive.org/web/20141031102702im_/http://d2b96ra3bt6d2m.cloudfront.net/unsafe/https://www.filepicker.io/api/file/tEbzlcoQROP5noB9d9LC" alt="Top Ten Blocks" loading="lazy"><figcaption><span>Top Ten Blocks · ©&nbsp;rweait-osm/flickr</span></figcaption></figure><p>For a person accustomed to this kind of work, it’s obvious that if a colleague comes along and needs your brain for a moment, you should stop what you’re doing and help. Those 30-second interruptions cost them exactly 30 seconds, and what kind of person can’t spare 30 seconds for a friend in need?</p><figure><img src="https://web.archive.org/web/20141031102702im_/http://d2b96ra3bt6d2m.cloudfront.net/unsafe/https://www.filepicker.io/api/file/ubXHr2UrSwKiQxXrNemQ" alt="" loading="lazy"></figure><p>The second type of work, however, is a completely different matter. The second type of work is like building our arch: If you stop for even an instant, you might drop half the pieces, drop the knowledge you were slowly accumulating of the strong points and weak points and which blocks go where. In these situations, a 30-second interruption can cost you a great deal more than 30 seconds—I’d argue, in fact, that for a Do Not Disturb task, a single brief interruption can easily halve your productivity.&nbsp;</p>
<!--kg-card-begin: html-->

<!-- ad3 -->
<ins data-ad-client="ca-pub-7131771765855059" data-ad-slot="3897249537" data-ad-format="auto" data-full-width-responsive="true"></ins>

<!--kg-card-end: html-->
<h2 id="square-by-square">Square by Square</h2><p>Does that sound extreme, maybe even implausible? Here’s a (slightly) mathematical spin that I think shows why it’s credible. (Bear with me on the math, I promise it will be worth it.)</p><p>One way to think about harmlessly-interruptible work is that it’s any kind of task where productivity is linear in the amount of time you spend on it. That simply means that if you spend 2 units of time on a task, you get 2 units of output back. For linear-type work, it really doesn’t matter whether those 2 units of time are spent in an uninterrupted two-hour block or in two separate one-hour blocks, because 1 + 1 = 2. (You can check my sums, I’ll wait around.)</p><figure><iframe width="200" height="113" src="https://www.youtube.com/embed/pnSJLnODRsE?feature=oembed" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen="" title="The Social Network - Checking Your Math On That Scene"></iframe></figure><p>Once we see it this way, it’s easier to figure out what’s going on with the Do Not Disturb work. Do Not Disturb work is any kind of task where productivity rises in a variety of non-linear ways: For example (though there are many others), if productivity rises in line with the square of time spent. This could mean that a 2-hour block of work gives us 2-squared units of productivity, which is 4 units of productivity, but two one-hour blocks of work give us only 1² + 1² units of productivity, which is 2 units of productivity. Suddenly that 30-second interruption makes a huge difference:</p><figure><img src="https://web.archive.org/web/20141031102702im_/http://d2b96ra3bt6d2m.cloudfront.net/unsafe/https://www.filepicker.io/api/file/0QXc5MDgTP2lBdaj8EZU" alt="" loading="lazy"></figure><p>As Graham puts it, for Do Not Disturbers, "The cost of having someone from personnel call you about a form you forgot to fill out can be huge. This is why hackers give you such a baleful stare as they turn from their screen to answer your question. Inside their heads a giant house of cards is tottering."</p><h2 id="door-to-door">Door to Door</h2><p>As I said previously, I think that the simple misunderstanding between people who are accustomed to harmlessly-interruptible work and people who are accustomed to Do Not Disturb work causes many of the infuriations of the modern workplace. The harmlessly-interruptibles get exasperated at that one colleague who is so obstinate and anti-social she refuses to stop for the briefest conversation, or even take out her earbuds for half a minute, when someone is at her desk with an important message to give her.</p>
<!--kg-card-begin: html-->

<!-- ad4 -->
<ins data-ad-client="ca-pub-7131771765855059" data-ad-slot="6060378447" data-ad-format="auto" data-full-width-responsive="true"></ins>

<!--kg-card-end: html-->
<p>Meanwhile, the Do Not Disturbers can’t fathom how their colleagues could be so un-empathetic as to wreck two hours of work in a heartbeat… just to tell them that a meeting they never wanted to attend has been moved from three to three-thirty. If we could all just hang a sign on our door saying what kind of work we’re in the middle of, so that colleagues could know in advance whether to cut in for a second or save it for later, everyone would be better off.</p><p>Of course, this assumes that you have a door to hang that sign on. Here we see another insight from the Two Types of Work theory. I believe that the eternal tension between open-plan and private offices can be explained (among other factors) by the two types of work. If you’re doing harmlessly-interrupted work, then it might make sense to be surrounded by your colleagues, bouncing ideas, shooting quick questions—the productivity gained by immediate answers could outweigh the small losses resulting from those brief interruptions.&nbsp;</p><blockquote>Graham writes that he "once saw a recruiting ad for Microsoft with a big picture of a door. Work for us, the premise was, and we'll give you a place to work where you can actually get work done."</blockquote><p>In terms of Do Not Disturb work, on the other hand, an open plan office is a living nightmare—so much so that companies in the know make private offices for Do Not Disturbers a major part of their recruitment package. Graham writes that he "once saw a recruiting ad for Microsoft with a big picture of a door. Work for us, the premise was, and we'll give you a place to work where you can actually get work done." Similarly, when programming guru Joel Spolsky was having new offices designed for his company, Fog Creek, he told the architect that "private offices with doors that close were absolutely required and not open to negotiation."</p>
<!--kg-card-begin: html-->

<!-- ad5 -->
<ins data-ad-client="ca-pub-7131771765855059" data-ad-slot="8574861140" data-ad-format="auto" data-full-width-responsive="true"></ins>

<!--kg-card-end: html-->
<p>In fact, Spolsky went so far as to completely eschew outside funding for his company partly because "I don't think it's possible to have private offices for developers when you're VC-funded...I think that it's worth paying for in terms of the productivity you get…[but] it would be considered completely unacceptable by the average VC…because it's considered an extravagance of a successful company or something like that. And, you know, 'Why aren't you all in the same room talking?'" Why aren’t you all in the same room talking?—I think we all know the answer to that one.</p><h2 id="youve-got-mail">You've Got Mail</h2><p>I similarly believe that Do Not Disturb work is at the heart of the reason why so ⋅ many ⋅ productivity ⋅ writers argue for blocking out the first few hours of the day for completely ignoring email. Email is the ultimate 30-second distraction: It hardly takes a moment to read and reply to somebody’s message, and while you’re doing it you feel productive and useful and meaningful in the world. But every time you break from your Do Not Disturb work to answer that quick question (or just impulsively flip to your email tab), you’re dropping half the bricks in your arch, or maybe stopping yourself from starting an arch in the first place. Nothing breaks the imagination like a letter from a friend.</p><p>Of course, the notion that there are two totally different types of people, doing totally different types of work, is more than a little oversimplified (a stark dichotomy that turns out to be oversimplified? Who would have thought!). While some of us will immediately recognize Do Not Disturb work as the main type of task we do each day, I think most of us have some amount of Do Not Disturb work in our lives which we ought to prioritize in the early morning, or late at night, or whenever it is that we have even the slightest possibility of finding (and guarding) uninterrupted work time.</p><p>Which is to acknowledge an inconvenient truth: An uninterrupted block of time, unless guarded jealously, will never stay that way—a slow devolution towards disorder seems to be the natural way of things unless we intervene decisively against it. We need to fence off any possible Do Not Disturb time using whatever measures necessary: switching off WiFi, working inside the closet, pretending we have to stay home today because the kids have chicken pox (again). If you’re going to build an arch, it’s often necessary to spend resources first on a fence around that arch — a fence to keep the baying hounds of the outside world at bay.</p><h2 id="a-space-in-time">A Space in Time</h2><p>A few more tips about Do Not Disturb work. First, if you can possibly leave your Do Not Disturb time open-ended—say, I’ll start this work at 8 a.m. and whenever I happen to finish it I’ll go back to checking emails—that’s very much for the best.</p><p>Graham writes that, in some ways, "scheduled distractions may be worse than unscheduled ones. If you know you have a meeting in an hour, you don't even start working on something hard." It isn’t possible to engage with Do Not Disturb work indefinitely; at some point your brain fuzzes up like an old peach, and you really must go do something else. But there’s a very big difference between driving on an open road until you run out of gas and driving with a cop car bearing down on your rearview mirror: In the latter case you’re likely to say, "Well, I wouldn’t have got much more done anyway," and pull over to the shoulder much sooner than necessary.</p><blockquote>Graham writes that, in some ways, "scheduled distractions may be worse than unscheduled ones. If you know you have a meeting in an hour, you don't even start working on something hard."&nbsp;</blockquote><p>Second, it’s important to understand that not all distractions are created equal. As Graham notes, "The danger of a distraction depends not on how long it is, but on how much it scrambles your brain. A programmer can leave the office and go get a sandwich without losing the code in his head. But the wrong kind of interruption can wipe your brain in 30 seconds."</p><p>Graham clearly means a sandwich from that one local sandwich place that the person always goes to, because the exact same sandwich run would be absolutely arch-toppling if it involved going to a different café and having to think about anything. The secret to acceptable interruptions, while hard to describe precisely, could be something about your ability to deal with them on autopilot: They must be completely unsurprising and not require any decisions.</p><h2 id="work-with-me">Work With Me</h2><p>Finally, a word to the managers of the Do Not Disturbers. I realize that, as an executive, this can all start to sound like special pleading. Not only am I saying that I want to be left alone and not be disturbed under any circumstances—when it looks to a normal observer like I’m sitting here and daydreaming—but I’m also saying that my colleagues aren’t allowed to disturb me for even 30 seconds with important questions, yet I can wander off for 30 minutes at lunch and that’s a-okay. How does that even make sense? It doesn’t, I know. It sounds insane. So I realize there’s a lot to take on trust here.&nbsp;</p><p>I think it’s important to appreciate, though, just how enjoyable Do Not Disturb work is. You remember, as a kid, how you invented fantasy worlds in your head, filled with brilliant inventions and fantastical quests, and when your folks called you down for dinner you simply ignored them because you didn’t want to lose your place in the story? That’s Do Not Disturb work. How about the feeling when you meet a special someone, and the only thing you want in the world is to stay up all night walking and talking with them, trying to understand them at the deepest level? That’s Do Not Disturb work. Do Not Disturb work is Newton gazing at the apple tree; Do Not Disturb work is Paradise found. Do Not Disturb work is, far and away, the greatest part of a working day, so I firmly believe that any person tasked with doing it and given the conditions to make it possible would be absolutely crazy to spend those sacred hours doing anything else.&nbsp;</p><p>Managers, I swear, this is a chance to be a hero: Stand boldly between your Do Not Disturbers and the outside world with a sword in your hand shouting "none shall pass." Because deep in their brains, in measureless caverns, your Do Not Disturbers are building arches and citadels and domes in the air—the structures that the greatest companies are made from.</p><p><strong>This essay is lovingly dedicated to the hours 8 till 12, without which none of my real work would ever get done.</strong></p>
        </section>

    </article>


</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[The first experimental support for LK99-type superconductivity (121 pts)]]></title>
            <link>https://arxiv.org/abs/2312.10391</link>
            <guid>38691268</guid>
            <pubDate>Tue, 19 Dec 2023 03:01:36 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://arxiv.org/abs/2312.10391">https://arxiv.org/abs/2312.10391</a>, See on <a href="https://news.ycombinator.com/item?id=38691268">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="content-inner">
    
    
                
    <p><a href="https://arxiv.org/pdf/2312.10391.pdf">Download PDF</a>
    <a href="https://browse.arxiv.org/html/2312.10391v1">HTML (experimental)</a></p><blockquote>
            <span>Abstract:</span>We observe a considerable hysteresis effect of low-field microwave absorption (LFMA) in copper-substituted lead apatite. By continuously rotating samples under external magnetic field, this effect is diminished which can not be renewed by a strong magnetic field but will be spontaneously recovered after two days, indicating its glassy features and excluding possibility of any ferromagnetism. The intensity of LFMA is found to sharply decrease at around 250K, suggesting a phase transition takes place. A lattice gauge model is then employed to assign these effects to the transition between superconducting Meissner phase and vortex glass, and the slow dynamics wherein is calculated as well.
    </blockquote>

    <!--CONTEXT-->
    
  </div><div>
      <h2>Submission history</h2><p> From: Yao Yao [<a href="https://arxiv.org/show-email/4f44e5db/2312.10391">view email</a>]      <br>    <strong>[v1]</strong>
        Sat, 16 Dec 2023 09:08:54 UTC (1,515 KB)<br>
</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Unbricking my MacBook took an email to Tim Cook (and a #1 post on Hacker News) (456 pts)]]></title>
            <link>https://www.tokyodev.com/articles/unbricking-my-macbook-took-an-email-to-tim-cook</link>
            <guid>38691025</guid>
            <pubDate>Tue, 19 Dec 2023 02:19:21 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.tokyodev.com/articles/unbricking-my-macbook-took-an-email-to-tim-cook">https://www.tokyodev.com/articles/unbricking-my-macbook-took-an-email-to-tim-cook</a>, See on <a href="https://news.ycombinator.com/item?id=38691025">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
<p>I previously wrote about <a href="https://www.tokyodev.com/articles/not-setting-up-find-my-bricked-my-macbook">how my lost MacBook was returned to me with an Activation Lock on it</a>, and how Apple refused to unlock it for me. Well, I did manage to finally get it unlocked. All it took was an email to Tim Cook. I don’t know how repeatable this process is, but here’s what happened.</p>
<p>After publishing the article, it somehow <a href="https://news.ycombinator.com/item?id=37865941">reached the top of Hacker News</a>. I’d hoped that would trigger someone from Apple to reach out to me, but no one did.</p>
<p>I also shared it with a much smaller audience: a Discord server that I run for English speaking software developers in Japan. <a href="https://www.tokyodev.com/authors/scott-rothrock">Our moderator</a> pointed out that he’d had success in the past by emailing <a href="https://www.tokyodev.com/cdn-cgi/l/email-protection" data-cfemail="a3d7c0ccccc8e3c2d3d3cfc68dc0ccce">[email&nbsp;protected]</a>, as he’d done so in the past and had his assistants escalate things to people who could resolve them for him.</p>
<p>Having exhausted other avenues, I gave it a shot:</p>
<blockquote>
<p>Subject: Activation Lock support requests denied</p>
<p>My lost MacBook was returned to me with Activation Lock on. Despite providing my receipt from the Apple Store, my request to remove the lock has been denied. I wrote an article about my experience that got to the top of the popular social media site “Hacker News”. I’d love to have a happy ending to this frustrating story.</p>
</blockquote>
<p>Four business days later I got a response from one of Tim’s executive assistants in Japan. From there, we exchanged emails and phone calls over the next two weeks, where the assistant was able to get to the bottom of things.</p>
<p>They explained to me that the MacBook was wiped in the middle of August (after I had lost it) and then reported lost by a newly created iCloud account with an email address starting with “p”. My requests to have it unlocked were being rejected as per their policy of not unlocking devices reported as lost, even if an original proof of purchase is provided. However, because of the documentation I was able to provide, they were convinced that it was my MacBook, and thus unlocked it.</p>
<p>Would this have worked had I not been able to reference the article’s performance on Hacker News? I’m not sure, but it seems likely that was at least a contributing factor. After all, I didn’t actually provide the assistant with any more proof than I had submitted with the activation unlock request. That being said, if you have an issue that can’t be resolved via normal channels, emailing Tim Cook is worth a shot.</p>
<h2 id="what-really-happened-to-my-macbook">What really happened to my MacBook</h2>
<p>By this time, I’d already purchased a replacement machine, and so getting the MacBook unlocked was as much about the principle of things as anything else. However, I also wanted my curiosity sated as to what had actually happened.</p>
<p>Apple had provided me with “how” it got locked: the computer was wiped, locked to a new iCloud account, then reported as lost. They didn’t give me the “why” though. Though my original article mentioned it as a theoretical possibility, I saw no incentive for the person who returned my MacBook to do this. So I emailed the person about it reseting the laptop, and after a couple of emails back and forth, I found out more or less what happened.</p>
<p>While the person didn’t reset it themselves, they did take it to a shop, and asked them to unlock it. The shop didn’t unlock it, however, they did reset it. This wasn’t obvious to the person, but they reported that while my login profile information was visible before they gave it to the shop, it wasn’t afterwards, presumably as the Activation Lock was on.</p>
<p>Despite the shop not having unlocked it for the person, they reported that the shop asked to be paid. When pressed on why the shop asked to be paid despite the shop not unlocking it, the person stopped responding.</p>
<p>My theory is that the shop reset the MacBook and reported it as lost with a new Apple ID in order to extort the person.</p>
<p>Perhaps the person didn’t want to admit it, but they actually paid money to the shop, who initially gave it back to them “unlocked”. Later the shop could lock it again by reporting it as lost, as a way to ask for more money from the person again.</p>
<p>Or maybe the shop wanted to ensure they’d be the only one able to unlock it, and so returned the MacBook, telling the person that they found a way to unlock it, but only if the person paid a higher fee than they were willing to pay.</p>
<p>This would explain why the iCloud address started with “p”. The shop saw my name was “Paul McMahon” from the login screen, and so they created an email address that sounded like it could be mine to give them plausible deniability when the person saw the lock screen.</p>
<h2 id="apple-doesnt-consider-this-to-be-a-security-vulnerability-but-maybe-it-should">Apple doesn’t consider this to be a security vulnerability, but maybe it should</h2>
<p>When first activating a MacBook, Apple makes it easy to skip setting up FindMy. But given the severe consequences for not doing so, I think they either need to revise the setup workflow to make this downside abundantly clear, or revisit their unlock policy altogether.</p>
<p>While I was an edge case, having my MacBook taken to an unscrupulous shop by someone who didn’t own it, similarly unscrupulous shops could do the same thing to others. After all, I even wondered about the possibility that I had set up an iCloud email in the past, and had somehow activated the computer with it.</p>
<p>I reported this to <a href="https://security.apple.com/">Apple Security Research</a>, but it was dismissed with “We’re unable to identify a security issue in your report.” I suppose I could have asked my contact at Apple to flag it for further review, but at this point, I’m ready to let it go.</p>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Google's True Moonshot (160 pts)]]></title>
            <link>https://stratechery.com/2023/googles-true-moonshot/</link>
            <guid>38690060</guid>
            <pubDate>Mon, 18 Dec 2023 23:58:38 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://stratechery.com/2023/googles-true-moonshot/">https://stratechery.com/2023/googles-true-moonshot/</a>, See on <a href="https://news.ycombinator.com/item?id=38690060">Hacker News</a></p>
<div id="readability-page-1" class="page"><article id="post-12087">
	<!-- .entry-header -->

	<div>
		<p>When I first went independent with Stratechery, I had a plan to make money on the side with speaking, consulting, etc.; what made me pull the plug on the latter was my last company speaking gig, with Google in November 2015 (I have always disclosed this <a href="https://stratechery.com/about/#ethics">on my About page</a>). It didn’t seem tenable for me to have any sort of conflict of interest with companies I was covering, and the benefit of learning more about the companies I covered — the justification I told myself for taking the engagement — was outweighed by the inherent limitations that came from non-public data. And so, since late 2015, my business model has been fully aligned to my nature: fully independent, with access to the same information as everyone else.<sup id="rf1-12087"><a href="#fn1-12087" title="I do still speak at conferences, but last spoke for pay in January 2017" rel="footnote">1</a></sup></p>
<p>I bring this up for three reasons, that I shall get to through the course of this Article. The first one has to do with titles: it was at that talk that a Google employee asked me what I thought of invoking the then-unannounced Google Assistant by saying “OK Google”. “OK Google” was definitely a different approach from Apple and Amazon’s “Siri” and “Alexa”, respectively, and I liked it: instead of pretending that the assistant was the dumbest human you have ever talked to, why not portray it as the smartest robot, leaning on the brand name that Google had built over time?</p>
<p>“OK Google” was, in practice, not as compelling as I hoped. It was better than Siri or Alexa, but it had all of the same limitations that were inherent to the natural language processing approach: you had to get the incantations right to get the best results, and the capabilities and responses were ultimately more deterministic than you might have hoped. That, though, wasn’t necessarily a problem for the brand: Google search is, at its core, still about providing the right incantations to get the set of results you are hoping for; Google Assistant, like Search, excelled in more mundane but critical attributes like speed and accuracy, if not personality and creativity.</p>
<p>What was different from search is that an Assistant needed to provide one answer, not a list of possible answers. This, though, was very much in keeping with Google’s fundamental nature; I once wrote in a Stratechery Article:</p>
<blockquote><p>
  An assistant has to be far more proactive than, for example, a search results page; it’s not enough to present possible answers: rather, an assistant needs to give the right answer.</p>
<p>  This is a welcome shift for Google the technology; from the beginning the search engine has included an “I’m Feeling Lucky” button, so confident was Google founder Larry Page that the search engine could deliver you the exact result you wanted, and while yesterday’s Google Assistant demos were canned, the results, particularly when it came to contextual awareness, were far more impressive than the other assistants on the market. More broadly, few dispute that Google is a clear leader when it comes to the artificial intelligence and machine learning that underlie their assistant.
</p></blockquote>
<p>That paragraph was from <a href="https://stratechery.com/2016/google-and-the-limits-of-strategy/">Google and the Limits of Strategy</a>, where I first laid out some of the fundamental issues that have, over the last year, come into much sharper focus. On one hand, Google had the data, infrastructure, and customer touch points to win the “Assistant” competition; that remains the case today when it comes to generative AI, which promises the sort of experience I always hoped for from “OK Google.” On the other hand, “I’m feeling lucky” may have been core to Google’s nature, but it was counter to their business model; I continued in that Article:</p>
<blockquote><p>
  A business, though, is about more than technology, and Google has two significant shortcomings when it comes to assistants in particular. First, as I explained after this year’s Google I/O, the company has a <a href="https://stratechery.com/2016/googles-go-to-market-gap/">go-to-market gap</a>: assistants are only useful if they are available, which in the case of hundreds of millions of iOS users means downloading and using a separate app (or building the sort of experience that, like Facebook, users will willingly spend extensive amounts of time in).</p>
<p>  Secondly, though, Google has a business-model problem: the “I’m Feeling Lucky Button” guaranteed that the search in question would not make Google any money. After all, if a user doesn’t have to choose from search results, said user also doesn’t have the opportunity to click an ad, thus choosing the winner of the competition Google created between its advertisers for user attention. Google Assistant has the exact same problem: where do the ads go?
</p></blockquote>
<p>It is now eight years on from that talk, and seven years on from the launch of Google Assistant, but all of the old questions are as pertinent as ever.</p>
<h3>Google’s Horizontal Webs</h3>
<p>My first point brings me to the second reason I’m reminded of that Google talk: my presentation was entitled “The Opportunity — and the Enemy.” The opportunity was mobile, the best market the tech industry had ever seen; the enemy was Google itself, which even then was still under-investing in its iOS apps.</p>
<p>In the presentation I highlighted the fact that Google’s apps still didn’t support <a href="https://en.wikipedia.org/wiki/Force_Touch">Force Touch</a>, which Apple had introduced to iOS over a year earlier; to me this reflected the strategic mistake the company made in prioritizing Google Maps on Android, which culminated in Apple making its own mapping service. My point was one <a href="https://stratechery.com/2013/the-android-detour/">I had been making on Stratechery from the beginning</a>: Google was a services company, which meant their optimal strategy was to serve all devices; by favoring Android they were letting the tail wag the dog.</p>
<p>Eight years on, and it’s clear I wasn’t the only one who saw the Maps fiasco as a disaster to be learned from: one of the most interesting revelations from the <a href="https://stratechery.com/2023/googles-apple-payments-apple-services-narrative-vs-reality-googles-motivation/">ongoing DOJ antitrust case against Google</a> was reported by <a href="https://stratechery.com/2023/googles-apple-payments-apple-services-narrative-vs-reality-googles-motivation/">Bloomberg</a>:</p>
<blockquote><p>
  Two years after Apple Inc. dropped Google Maps as its default service on iPhones in favor of its own app, Google had regained only 40% of the mobile traffic it used to have on its mapping service, a Google executive testified in the antitrust trial against the Alphabet Inc. company. Michael Roszak, Google’s vice president for finance, said Tuesday that the company used the Apple Maps switch as “a data point” when modeling what might happen if the iPhone maker replaced Google’s search engine as the default on Apple’s Safari browser.
</p></blockquote>
<p>It’s a powerful data point, and I think the key to understanding what you might call the Google Aggregator Paradox: if Google wins by being better, then why does it fight so hard for defaults, both for search and, in the case of Android, the Play Store? The answer, I think, is that it is best to not even take the chance of alternative defaults being good enough. This is made easier given the structure of these deals, which are revenue shares, not payments; this does show up on Google’s income statement as Traffic Acquisition Costs (TAC), but from a cash flow perspective it is foregone zero marginal cost revenue. There is no pain of payment, just somewhat lower profitability on zero marginal cost searches.</p>
<p>The bigger cost is increasingly legal: the decision in <a href="https://stratechery.com/2020/united-states-v-google/">the DOJ case</a> won’t come down until next year, and Google may very well win; it’s hard to argue that the company ought not be able to bid on Apple’s default search placement if its competitors can (if anything the case <a href="https://stratechery.com/2023/googles-apple-payments-apple-services-narrative-vs-reality-googles-motivation/">demonstrates Apple’s power</a>).</p>
<p>That’s not Google’s only legal challenge, though: last week the company lost another antitrust case, this time to Epic. I explained why the company lost — while Apple won — in <a href="https://stratechery.com/2023/google-loses-antitrust-case-to-epic-the-differences-between-apple-and-google-revisited-the-tying-question/">last Tuesday’s Update</a>:</p>
<blockquote><p>
  That last point may seem odd in light of Apple’s victory, but again, Apple was offering an integrated product that it fully controlled and customers were fully aware of, and is thus, under U.S. antitrust law, free to set the price of entry however it chooses. Google, on the other hand, “entered into one or more agreements that unreasonably restrained trade” — that quote is from the jury instructions, and is taken directly from the Sherman Act — by which the jurors mean basically all of them: the Google Play Developer Distribution Agreement, investment agreements under the Games Velocity Program (i.e. Project Hug), and Android’s mobile application distribution agreement and revenue share agreements with OEMs, were all ruled illegal.</p>
<p>  This goes back to the point I made above: Google’s fundamental legal challenge with Android is that it sought to have its cake and eat it too: it wanted all of the shine of open source and all of the reach and network effects of being a horizontal operating system provider and all of the control and profits of Apple, but the only way to do that was to pretty clearly (in my opinion) violate antitrust law.
</p></blockquote>
<p>Google’s Android strategy was, without question, brilliant, particularly when you realize that <a href="https://abovethecrowd.com/2011/03/24/freight-train-that-is-android/">the ultimate goal was to protect search</a>. By making it “open source”, Google got all of the phone carriers desperate for an iOS alternative on board, ensuring that hated rival Microsoft was not the alternative to Apple as it had been on PCs; a modular approach, though, is inherently more fragmented — and Google didn’t just want an alternative to Apple, they wanted to beat them, particularly in the early days of the smartphone wars — so the company spun a web of contracts and incentives to ensure that Android was only really usable with Google’s services. For this <a href="https://stratechery.com/2018/the-european-commission-versus-android/">the company was rightly found guilty of antitrust violations in the EU</a>, and now, for similar reasons, in the U.S.</p>
<p>The challenge for Google is that the smartphone market has a lot more friction than search: the company needs to coordinate both OEMs and developers; when it came to search the company could simply take advantage of the openness of the web. This resulted in tension between Google’s nature — being the one-stop shop for information — and the business model of being a horizontal app platform and operating system provider. It’s not dissimilar to the tension the company faces with its Assistant, and in the future with Generative AI: the company wants to simply give you the answer, but how to do that while still making money?</p>
<h3>Infrastructure, Data, and Ecosystems</h3>
<p>The third reason I remember that weekend in 2015 is it was the same month that Google open-sourced TensorFlow, its machine-learning framework. I thought it was a great move, and wrote in <a href="https://stratechery.com/2015/tensorflow-and-monetizing-intellectual-property/">TensorFlow and Monetizing Intellectual Property</a>:</p>
<blockquote><p>
  I’m hardly qualified to judge the technical worth of TensorFlow, but I feel pretty safe in assuming that <a href="https://deliprao.com/archives/98">it is excellent</a> and likely far beyond what any other company could produce. Machine learning, though, is about a whole lot more than a software system: specifically, it’s about a whole lot of data, and an infrastructure that can process that data. And, unsurprisingly, those are two areas where Google has a dominant position.</p>
<p>  Indeed, as good as TensorFlow might be, I bet it’s the weakest of these three pieces Google needs to truly apply machine learning to all its various business, both those of today and those of the future. Why not, then, leverage the collective knowledge of machine learning experts all over the world to make TensorFlow better? Why not make a move to ensure the machine learning experts of the future grow up with TensorFlow as the default? And why not ensure that the industry’s default machine learning system utilizes standards set in place by Google itself, with a design already suited for Google’s infrastructure?</p>
<p>  After all, contra Gates’ 2005 claim, it turns out the value of pure intellectual property is not derived from government-enforced exclusivity, but rather from the complementary pieces that surround that intellectual property which are far more difficult to replicate. Google is betting that its lead in both data and infrastructure are significant and growing, and that’s a far better bet in my mind than an all-too-often futile attempt to derive value from an asset that by its very nature can be replicated endlessly.
</p></blockquote>
<p>In fact, it turned out that <a href="https://medium.com/@markurtz/2022-state-of-competitive-ml-the-downfall-of-tensorflow-e2577c499a4d">TensorFlow was not so excellent</a> — that link I used to support my position in the above excerpt now 404s — and it has been surpassed by Meta’s PyTorch in particular; at Google Cloud Next the company <a href="https://stratechery.com/2023/google-cloud-next-the-gcp-nvidia-partnership-kurian-explains-the-deal/">announced a partnership with Nvidia</a> to build out OpenXLA as a compiler of sorts to ensure that output from TensorFlow, Jax, and PyTorch can run on any hardware. This matters for Google because <a href="https://www.semianalysis.com/p/google-gemini-eats-the-world-gemini">those infrastructure advantages very much exist</a>; the more important “Tensor” product for Google is its Tensor Processing Unit series of chips, the existence of which make Google uniquely able to scale beyond whatever allocation it can get of Nvidia GPUs.</p>
<p>The importance of TPUs was demonstrated with the announcement of Gemini, Google’s latest AI model; the company claims the “Ultra” variant, which it hasn’t yet released, is better than GPT-4. What is notable is that Gemini was trained and will run inference on TPUs. While <a href="https://stratechery.com/2023/an-interview-with-nat-friedman-and-daniel-gross-about-the-human-component-of-ai/#tpus">there are some questions about the ultimate scalability of TPUs</a>, for now Google is the best positioned to both train and, more importantly, serve generative AI in a cost efficient way.</p>
<p>Then there is data: <a href="https://www.theinformation.com/articles/how-google-got-back-on-its-feet-in-ai-race">a recent report in The Information</a> claims that Gemini relies heavily on data from YouTube, and that is not the only proprietary data Google has access to: free Gmail and Google Docs are another massive resource, although <a href="https://www.vox.com/technology/2023/7/27/23808499/ai-openai-google-meta-data-privacy-nope">it is unclear to what extent Google is using that data, or if it is, for what</a>. At a minimum there is little question that Google has the most accessible repository of Internet data going back a quarter of a century to when Larry Page and Sergey Brin first started crawling the open web from their dorm room.</p>
<p>And so we are back where we started: Google has incredible amounts of data and the best infrastructure, but once again, an unsteady relationship with the broader development community.</p>
<h3>Gemini and Seamless AI</h3>
<p>The part of the Gemini announcement that drew the most attention did not have anything to do with infrastructure or data: what everyone ended up talking about was the company’s Gemini demo, and the fact it wasn’t representative of Gemini’s actual capabilities. Here’s the demo:</p>

<p><a href="https://stratechery.com/2023/google-gemini-performance-infrastructure-and-integration-apple-mlx/">Pammy Olson for Bloomberg Opinion</a> was the first to highlight the problem:</p>
<blockquote><p>
  In reality, the demo also wasn’t carried out in real time or in voice. When asked about the video by Bloomberg Opinion, a Google spokesperson said it was made by “using still image frames from the footage, and prompting via text,” and they pointed to a site showing how others could interact with Gemini with photos of their hands, or of drawings or other objects. In other words, the voice in the demo was reading out human-made prompts they’d made to Gemini, and showing them still images. That’s quite different from what Google seemed to be suggesting: that a person could have a smooth voice conversation with Gemini as it watched and responded in real time to the world around it.
</p></blockquote>
<p>This was obviously a misstep, and a bizarre one at that: as <a href="https://stratechery.com/2023/google-gemini-performance-infrastructure-and-integration-apple-mlx/">I noted in an Update</a> Google, given its long-term advantages in this space, would have been much better served in being transparent, particularly since it suddenly finds itself with a trustworthiness advantage <a href="https://stratechery.com/2023/openais-misalignment-and-microsofts-gain/">relative to Microsoft and OpenAI</a>. The goal for the company should be demonstrating competitiveness and competence; a fake demo did the opposite.</p>
<p>And yet, I can understand how the demo came to be; it is getting close to the holy grail of Assistants: an entity with which you can conduct a free-flowing conversation, without the friction of needing to invoke the right incantations or type and read big blocks of text. If Gemini Ultra really is better than GPT-4, or even roughly competitive, than I believe this capability is close. After all, I got a taste of it with GPT-4 and its voice capabilities; from <a href="https://stratechery.com/2023/ai-hardware-and-virtual-reality/">AI, Hardware, and Virtual Reality</a>:</p>
<blockquote><p>
  The first AI announcement of the week was literally AI that can talk: OpenAI announced that you can now converse with ChatGPT, and I found the experience profound.</p>
<p>  You have obviously been able to chat with ChatGPT via text for many months now; what I only truly appreciated after talking with ChatGPT, though, was just how much work it was to type out questions and read answers. There was, in other words, a human constraint in our conversations that made it feel like I was using a tool; small wonder that the vast majority of my interaction with ChatGPT has been to do some sort of research, or try to remember something on the edge of my memory, too fuzzy to type a clear search term into Google.</p>
<p>  Simply talking, though, removed that barrier: I quickly found myself having philosophical discussions including, for example, the nature of virtual reality. It was the discussion itself that provided a clue: virtual reality feels real, but something can only feel real if human constraints are no longer apparent. In the case of conversation, there is no effort required to talk to another human in person, or on the phone; to talk to them via chat is certainly convenient, but there is a much more tangible separation. So it is with ChatGPT.
</p></blockquote>
<p>The problem is that this experience requires a pretty significant suspension of disbelief, because there is too much friction. You have to open the OpenAI app, then you have to set it to voice mode, then you have to wait for it to connect, then every question and answer contains a bit too much lag, and the answers start sounding like blocks of text instead of a conversation. Notice, though, that Google is much better placed than OpenAI to solve all of these challenges:</p>
<ul>
<li>Google sells its own phones which could be configured to have a conversation UI by default (or with Google’s Pixel Buds). This removes the friction of opening an app and setting a mode. Google also has a fleet of home devices already designed for voice interaction. </li>
<li>Google has massive amounts of infrastructure all over the globe, with the lowest latency and fastest response. This undergirds search today, but it could undergird a new generative AI assistant tomorrow.</li>
<li>Google has access to gobs of data specifically tied to human vocal communication, thanks to YouTube in particular.</li>
</ul>
<p>In short, the Gemini demo may have been faked, but Google is by far the company best positioned to make it real.</p>
<h3>Pixie</h3>
<p>There was one other interesting tidbit in <a href="https://www.theinformation.com/articles/how-google-got-back-on-its-feet-in-ai-race">The Information article</a> (<em>emphasis mine</em>):</p>
<blockquote><p>
  Over the next few months, Google will have to show it can integrate the AI models it groups under the Gemini banner into its products, without cannibalizing existing businesses such as search. It has already put a less advanced version of Gemini into Bard, the chatbot it created to compete with ChatGPT, which has so far seen limited uptake. In the future, it plans to use Gemini across nearly its entire line of products, from its search engine to its productivity applications and <strong>an AI assistant called Pixie that will be exclusive to its Pixel devices</strong>, two people familiar with the matter said. Products could also include wearable devices, such as glasses that could make use of the AI’s ability to recognize the objects a wearer is seeing, according to a person with knowledge of internal discussions. The device could then advise them, say, on how to use a tool, solve a math problem or play a musical instrument.
</p></blockquote>
<p>The details of Pixie, such as they were, came at the very end:</p>
<blockquote><p>
  The rollout of Pixie, an AI assistant exclusively for Pixel devices, could boost Google’s hardware business at a time when tech companies are racing to integrate their hardware with new AI capabilities. Pixie will use the information on a customer’s phone — including data from Google products like Maps and Gmail — to evolve into a far more personalized version of the Google Assistant, according to one of the people with knowledge of the project. The feature could launch as soon as next year with the Pixel 9 and the 9 Pro, this person said.
</p></blockquote>
<p>That Google is readying a super-charged version of the Google Assistant is hardly a surprise; what is notable is the reporting that it will be exclusive to Pixel devices. This is counter to Gemini itself: the Gemini Nano model, which is designed to run on smartphones, will be available to all Android devices with neural processing units like Google’s Tensor G3. That is very much in-line with the post-Maps Google: services are the most valuable when they are available everywhere, and Pixel has a tiny amount of marketshare.</p>
<p>That, by extension, makes me think that the “Pixie exclusive to Pixel” report is mistaken, particularly since I’ve been taken in by this sort of thing before. That Google Assistant piece I quote above — <a href="https://stratechery.com/2016/google-and-the-limits-of-strategy/">Google and the Limits of Strategy</a> — interpreted the launch of Google Assistant on Pixel devices as evidence that Google was trying to differentiate its own hardware:</p>
<blockquote><p>
  Today’s world, though, is not one of (somewhat) standards-based browsers that treat every web page the same, creating the conditions for Google’s superior technology to become the door to the Internet; it is one of closed ecosystems centered around hardware or social networks, and having failed at the latter, Google is having a go at the former. To put it more generously, Google has adopted Alan Kay’s maxim that “People who are really serious about software should make their own hardware.” To that end the company introduced multiple hardware devices, including a new phone, the previously-announced Google Home device, new Chromecasts, and a new VR headset. Needless to say, all make it far easier to use Google services than any 3rd-party OEM does, much less Apple’s iPhone.</p>
<p>  What is even more interesting is that Google has also introduced a new business model: the Pixel phone starts at $649, the same as an iPhone, and while it will take time for Google to achieve the level of scale and expertise to match Apple’s profit margins, the fact there is unquestionably a big margin built-in is a profound new direction for the company.</p>
<p>  The most fascinating point of all, though, is how Google intends to sell the Pixel: the Google Assistant is, at least for now, exclusive to the first true Google phone, delivering a differentiated experience that, at least theoretically, justifies that margin. It is a strategy that certainly sounds familiar, raising the question of whether this is a replay of the turn-by-turn navigation disaster. Is Google forgetting that they are a horizontal company, one whose business model is designed to maximize reach, not limit it?
</p></blockquote>
<p>My argument was that Google was in fact being logical, for the business model reasons I articulated both in that Article and at the beginning of this year in <a href="https://stratechery.com/2023/ai-and-the-big-five/">AI and the Big Five</a>: simply giving the user the right answer threatened the company’s core business model, which meant it made sense to start diversifying into new ones. And then, just a few months later, Google Assistant <a href="https://stratechery.com/2017/google-assistant-added-to-android-amit-singhal-out-at-uber/">was available to other Android device makers</a>. It was probably the right decision, for the same reason that the company should have never diminished its iOS maps product in favor of Android.</p>
<p>And yet, all of the reasoning I laid out for making the Google Assistant a differentiator still hold: AI is a threat to Search for all of the same reasons I laid out in 2016, and Google is uniquely positioned to create the best Assistant. The big potential difference with Pixie is that it might actually be good, and a far better differentiator than the Google Assistant. The reason, remember, is not just about Gemini versus GPT-4: it’s because Google actually sells hardware, and has the infrastructure and data to back it up.</p>
<h3>Google’s True Moonshot</h3>
<p>Google’s collection of moonshots — from Waymo to Google Fiber to Nest to Project Wing to Verily to Project Loon (and the list goes on) — have mostly been science projects that have, for the most part, served to divert profits from Google Search away from shareholders. Waymo is probably the most interesting, but even if it succeeds, it is ultimately a car service rather far afield from Google’s mission statement “to organize the world’s information and make it universally accessible and useful.”</p>
<p>What, though, if the mission statement were the moonshot all along? What if “I’m Feeling Lucky” were not a whimsical button on a spartan home page, but the default way of interacting with all of the world’s information? What if an AI Assistant were so good, and so natural, that anyone with seamless access to it simply used it all the time, without thought?</p>
<p>That, needless to say, is probably the only thing that truly scares Apple. Yes, Android has its advantages to iOS, but they aren’t particularly meaningful to most people, and even for those that care — like me — they are not large enough to give up on iOS’s overall superior user experience. The only thing that drives meaningful shifts in platform marketshare are paradigm shifts, and while I doubt the v1 version of Pixie would be good enough to drive switching from iPhone users, there is at least a path to where it does exactly that.</p>
<p>Of course Pixel would need to win in the Android space first, and that would mean massively more investment by Google in go-to-market activities in particular, from opening stores to subsidizing carriers to ramping up production capacity. It would not be cheap, which is why it’s no surprise that Google hasn’t truly invested to make Pixel a meaningful player in the smartphone space.</p>
<p>The potential payoff, though, is astronomical: a world with Pixie everywhere means a world where Google makes real money from selling hardware, in addition to services for enterprises and schools, and cloud services that leverage Google’s infrastructure to provide the same capabilities to businesses. Moreover, it’s a world where Google is truly integrated: the company already makes the chips, in both its phones and its data centers, it makes the models, and it does it all with the largest collection of data in the world.</p>
<p>This path does away with the messiness of complicated relationships with OEMs and developers and the like, which I think suits the company: Google, at its core, has always been much more like Apple than Microsoft. It wants to control everything, it just needs to do it legally; that the best manifestation of AI is almost certainly dependent on a fully integrated (and thus fully seamless) experience means that the company can both control everything and, if it pulls this gambit off, serve everyone.</p>
<p>The problem is that the risks are massive: Google would not only be risking search revenue, it would also estrange its OEM partners, all while spending astronomical amounts of money. The attempt to be the one AI Assistant that everyone uses — and pays for — is the polar opposite of the conservative approach the company has taken to the Google Aggregator Paradox. Paying for defaults and buying off competitors is the strategy of a company seeking to protect what it has; spending on a bold assault on the most dominant company in tech is to risk it all.</p>
<p>And yet, to simply continue on the current path, folding AI into its current products and selling it via Google Cloud, is a risk of another sort. Google is not going anywhere anytime soon, and Search has a powerful moat in terms of usefulness, defaults, and most critically, user habits; Google Cloud, no matter the scenario, remains an attractive way to monetize Google AI and leverage its infrastructure, and perhaps that will be seen as enough. Where will such a path lead in ten or twenty years, though?</p>
<p>Ultimately, this is a question for leadership, and I though Daniel Gross’s observation on this point in <a href="https://stratechery.com/2023/an-interview-with-nat-friedman-and-daniel-gross-about-the-human-component-of-ai/">the recent Stratechery Interview with him and Nat Friedman</a> was insightful:</p>
<blockquote><p>
  So to me, yeah, does Google figure out how to master AI in the infrastructure side? Feels pretty obvious, they’ll figure it out, it’s not that hard. The deeper question is, on the much higher margin presumably, consumer angle, do they just cede too much ground to startups, Perplexity or ChatGPT or others? I don’t know what the answer is there and forecasting that answer is a little bit hard because it probably literally depends on three or four people at Google and whether they want to take the risk and do it.</p>
<p>  We definitively know that if the founders weren’t in the story — we could not definitively, but forecast with pretty good odds — that it would just run its course and it would gradually lose market share over time and we’d all sail into a world of agents. However, we saw Sergey Brin as an individual contributor on the Gemini paper and we have friends that work on Gemini and they say that’s not a joke, he is involved day-to-day. He has a tremendous amount of influence, power, and control over Google so if he’s staring at that, together with his co-founder, I do think they could overnight kill a lot of startups, really damage ChatGPT, and just build a great product, but that requires a moment of [founder initiative].</p>
<p>  It’s possible, it’s just hard to forecast if they will do it or not. In my head, that is the main question that matters in terms of whether Google adds or loses a zero. I think they’ll build the capability, there’s no doubt about it.
</p></blockquote>
<p>I agree. Google could build the AI to win it all. It’s not guaranteed they would succeed, but the opportunity is there if they want to go for it. That is the path that would be in the nature of the Google that conquered the web twenty years ago, the Google that saw advertising as the easiest way to monetize what was an unbridled pursuit of self-contained technological capability.</p>
<p>The question is if that nature been superceded by one focused on limiting losses and extracting profits; yes, there is still tremendous technological invention, but <a href="https://www.asymco.com/2014/04/16/innoveracy-misunderstanding-innovation/">as Horace Dediu explained on Asymco</a>, that is different than innovation, which means actually making products that move markets. Can Google still do that? Do they want to? Whither Google?</p>
<hr><ol><li id="fn1-12087"><p>I do still speak at conferences, but last spoke for pay in January 2017&nbsp;<a href="#rf1-12087" title="Return to footnote 1.">↩</a></p></li></ol>
	</div><!-- .entry-content -->
</article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Grindavik [video] (233 pts)]]></title>
            <link>https://www.youtube.com/watch?v=hvcP4kVVOnk</link>
            <guid>38689924</guid>
            <pubDate>Mon, 18 Dec 2023 23:40:44 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.youtube.com/watch?v=hvcP4kVVOnk">https://www.youtube.com/watch?v=hvcP4kVVOnk</a>, See on <a href="https://news.ycombinator.com/item?id=38689924">Hacker News</a></p>
&lt;Unparsable&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[Lessons from a never-ending personal project (236 pts)]]></title>
            <link>https://siddhesh.substack.com/p/projects</link>
            <guid>38689869</guid>
            <pubDate>Mon, 18 Dec 2023 23:33:22 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://siddhesh.substack.com/p/projects">https://siddhesh.substack.com/p/projects</a>, See on <a href="https://news.ycombinator.com/item?id=38689869">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><article><div dir="auto"><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F673ab69d-1a20-4aab-8b11-d7387d06e7c8_1682x604.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F673ab69d-1a20-4aab-8b11-d7387d06e7c8_1682x604.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F673ab69d-1a20-4aab-8b11-d7387d06e7c8_1682x604.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F673ab69d-1a20-4aab-8b11-d7387d06e7c8_1682x604.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F673ab69d-1a20-4aab-8b11-d7387d06e7c8_1682x604.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F673ab69d-1a20-4aab-8b11-d7387d06e7c8_1682x604.png" width="1456" height="523" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/673ab69d-1a20-4aab-8b11-d7387d06e7c8_1682x604.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:523,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:2173672,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:false,&quot;topImage&quot;:true,&quot;internalRedirect&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F673ab69d-1a20-4aab-8b11-d7387d06e7c8_1682x604.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F673ab69d-1a20-4aab-8b11-d7387d06e7c8_1682x604.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F673ab69d-1a20-4aab-8b11-d7387d06e7c8_1682x604.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F673ab69d-1a20-4aab-8b11-d7387d06e7c8_1682x604.png 1456w" sizes="100vw" fetchpriority="high"></picture></div></a><figcaption></figcaption></figure></div><p><span>This year I learned that finishing a personal project you’re doing just for yourself is impossible. Irrespective of what it is you’re making and in what medium, it’s impossible to really know what you want and separate it from what you want to want. It’s impossible to set “deadlines” if they’re not real, that is, imposed by an external authority. It’s impossible to start something and, given enough time and input, not have it grow and evolve into something else entirely. It’s impossible to avoid </span><a href="https://en.wikipedia.org/wiki/Scope_creep" rel="">scope creep</a><span>.</span></p><p><span>Some context: I didn’t publish anything on </span><em>Obvious Bicycle</em><span> for the last six months because I’ve been busy writing an essay since March. It’s a piece about music (similar to </span><a href="https://siddhesh.substack.com/p/fav-album-covers" rel="">this one</a><span>), but it’s not meant for this blog, and so I told myself I’d start writing here again </span><em>just as soon </em><span>as I got done with it. Easy peasy.</span></p><p>I thought that piece would take a month or so to write, but I’ve now been at it for nine months, it’s running at 17,000 words (no joke), and is completely out of my control. I used to lie and tell myself that I’d be done with it first by the end of September, then October, and then November, but I eventually learned my lesson. Now I know that it’ll be done when it’s done, whether that’s a month or a decade from now. I’ll keep chipping away at it bits at a time.</p><p>I finally understand what the phrase “it’ll take as long as it takes” means.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F853b475d-2a19-4de4-b9ce-e27e0cf303c3_640x640.jpeg" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F853b475d-2a19-4de4-b9ce-e27e0cf303c3_640x640.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F853b475d-2a19-4de4-b9ce-e27e0cf303c3_640x640.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F853b475d-2a19-4de4-b9ce-e27e0cf303c3_640x640.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F853b475d-2a19-4de4-b9ce-e27e0cf303c3_640x640.jpeg 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F853b475d-2a19-4de4-b9ce-e27e0cf303c3_640x640.jpeg" width="330" height="330" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/853b475d-2a19-4de4-b9ce-e27e0cf303c3_640x640.jpeg&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:640,&quot;width&quot;:640,&quot;resizeWidth&quot;:330,&quot;bytes&quot;:null,&quot;alt&quot;:&quot;Nurture - Album by Porter Robinson | Spotify&quot;,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:false,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null}" alt="Nurture - Album by Porter Robinson | Spotify" title="Nurture - Album by Porter Robinson | Spotify" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F853b475d-2a19-4de4-b9ce-e27e0cf303c3_640x640.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F853b475d-2a19-4de4-b9ce-e27e0cf303c3_640x640.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F853b475d-2a19-4de4-b9ce-e27e0cf303c3_640x640.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F853b475d-2a19-4de4-b9ce-e27e0cf303c3_640x640.jpeg 1456w" sizes="100vw"></picture></div></a><figcaption>Live footage of me once I’m done writing that piece</figcaption></figure></div><p>I generally don’t enjoy writing meta-posts about the process of writing itself or about learning “lessons” from things on this blog (urgh), but exceptions must be made. I learned some weird and valuable stuff from my never-ending-essay.</p><p>The poet Paul Valéry once said that poems are only abandoned, never finished. I’m realizing that every project has a point where the creator, no matter how unsatisfied, has to stop iterating and decide to put what they’ve made out into the world. And it’s always a compromise, because they know every little flaw about the thing they’re making, what about it can and should be improved. They know all the ideas they haven’t had a chance to try out yet, all the features that are yet to be added. There’s always more to do.</p><p><span>Writing my music piece, every time I thought I was happy with a paragraph, I’d read it again in a few weeks and think of something to add to it. Every section in it that I thought was good enough turned out to be not good enough - given enough time, there were </span><em>always, </em><span>always some changes I could think of to make it better. Not a single sentence survived its first-draft stage. Will the piece ever reach a point where I’m perfectly happy with every single thing in it, or do I just have to abandon it and call it quits at some point? Yes.</span></p><p>And if a simple writeup can be iterated and reworked so frequently, I can’t imagine how complex, big budget projects even work. I can’t imagine how film directors must feel watching their own movies, once the dust of production settles. They must hate it, noticing every flaw and every other choice they wanted to make differently but couldn’t due to budget or time constraints. </p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F86a7e758-cbc4-4806-bff5-13204a84c3bc_519x662.jpeg" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F86a7e758-cbc4-4806-bff5-13204a84c3bc_519x662.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F86a7e758-cbc4-4806-bff5-13204a84c3bc_519x662.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F86a7e758-cbc4-4806-bff5-13204a84c3bc_519x662.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F86a7e758-cbc4-4806-bff5-13204a84c3bc_519x662.jpeg 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F86a7e758-cbc4-4806-bff5-13204a84c3bc_519x662.jpeg" width="413" height="526.7938342967245" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/86a7e758-cbc4-4806-bff5-13204a84c3bc_519x662.jpeg&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:662,&quot;width&quot;:519,&quot;resizeWidth&quot;:413,&quot;bytes&quot;:null,&quot;alt&quot;:&quot;No photo description available.&quot;,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null}" alt="No photo description available." title="No photo description available." srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F86a7e758-cbc4-4806-bff5-13204a84c3bc_519x662.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F86a7e758-cbc4-4806-bff5-13204a84c3bc_519x662.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F86a7e758-cbc4-4806-bff5-13204a84c3bc_519x662.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F86a7e758-cbc4-4806-bff5-13204a84c3bc_519x662.jpeg 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption>LMAO</figcaption></figure></div><p>Of course, even this very blog post has better versions that I could continue working towards - I just had to say screw it and hit Publish at one point. It’d be ironic if I never finished the essay about not being able to finish projects.</p><p>Deadlines are great. Anyone who has ever gotten anything done did so because the work was expected from them by a certain date, and not finishing it by then would have consequences.</p><p><span>But there’s another side to it: for any </span><a href="http://www.paulgraham.com/own.html" rel="">project of your own</a><span>, where no one’s really expecting anything, if you simply continue working on it as long as it feels right, it will evolve into something you wouldn’t - and couldn’t - have imagined when you started, in the best possible way. It’ll be unrecognizable from version zero. That’s one reason it’s worth working on something for a long time.</span></p><p>But at some point, if you want said project to be useful and/or seen by the world in any way, you have to stop improoving it and put it out there. Otherwise you’ll end up working on it till eternity with no rewards for your labor. This is what I’m doing now, for better or (mostly) for worse. But where is that “some point”? Nobody knows. If I had the answers I wouldn’t be writing this blog post.</p><p>So the whole thing is basically a push and pull between saying “let’s keep iterating” and saying “this is good enough, let’s ship it”. This is exactly why any product in tech comes out in versions - it’s never perfect, you’re just never done working on it, so you release it in stable chunks and continue iterating. But the same is not possible with art, with paintings and albums and plays - once it’s out, it’s out. So how do directors and writers and musicians decide when something is release-ready? Nobody knows. The answer to that question might be the very definition of having good taste.</p><p><span>One of my favourite sayings is that “if you’re reading it, it’s for you.” Meaning that the things you willingly consume aren’t a coincidence. But I’d go one step further and say that if you’re doing something, then it’s definitely for you, no matter how uncool it is. It’s </span><a href="https://perell.com/note/surrendering-to-your-nature/" rel="">who you are</a><span>.</span></p><p>If I’d been someone cooler than I am, I would’ve spent all this time writing a guitar riff or dirt biking in the desert or something, but instead I wrote an essay on my laptop that I don’t really wanna show anyone. This was an extremely dumb use of my time, and it won’t help me in my career or make me money or get me clout. But I did it anyway, and the best reason for continuing to do it was that I was already doing it in the first place. So it goes.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F64ef41f5-4255-42a2-8835-b6e6672b7e7c_589x139.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F64ef41f5-4255-42a2-8835-b6e6672b7e7c_589x139.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F64ef41f5-4255-42a2-8835-b6e6672b7e7c_589x139.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F64ef41f5-4255-42a2-8835-b6e6672b7e7c_589x139.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F64ef41f5-4255-42a2-8835-b6e6672b7e7c_589x139.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F64ef41f5-4255-42a2-8835-b6e6672b7e7c_589x139.png" width="589" height="139" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/64ef41f5-4255-42a2-8835-b6e6672b7e7c_589x139.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:139,&quot;width&quot;:589,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:15276,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F64ef41f5-4255-42a2-8835-b6e6672b7e7c_589x139.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F64ef41f5-4255-42a2-8835-b6e6672b7e7c_589x139.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F64ef41f5-4255-42a2-8835-b6e6672b7e7c_589x139.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F64ef41f5-4255-42a2-8835-b6e6672b7e7c_589x139.png 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption>PG agrees</figcaption></figure></div><p>So this piece I’m writing is about a topic that requires zero research or further reading. It’s literally me writing about my favourite albums just for the sake of it. So of all things in the world, I should’ve known what I was gonna say, right? Right?! </p><p>This was a topic where I thought I knew what I thought, and all I had to do was write said thoughts down. But that’s not how writing works. It’s not a tool to express what you already perfectly know, but rather a tool to find out what you know by writing down what you think you know. It’s not just a medium of communication, it’s a medium of self-discovery. It’s magic.</p><p>Said Ray Bradbury:</p><blockquote><p>"If you can’t read and write, you can’t think. Your thoughts are dispersed if you don’t know how to read and write. You’ve got to be able to look at your thoughts on paper and discover what a fool you were."</p></blockquote><p><span>A fool indeed. Every time I re-read something I’d written in that piece, I got a more accurate assessment of what I wanted to say and was amazed at how I had </span><em>not conveyed that</em><span> </span><em>at all</em><span>. It was miraculous, honestly, how big the gap was between a) what I thought I thought about a topic, b) what I actually thought about the topic, and c) what I wrote down about the topic. All the rewrites were simply attempts at bridging that gap.</span></p><p>And if this was true for a topic I’m into and spend a lot of time thinking about with no effort, I can’t imagine how shitty my thought process is for all the other issues of the world, like politics and AI and why Manchester United is doing so badly right now. Sure, I have (often strong) opinions on these issues, but how well thought out are these opinions if I haven’t written about them, really? We’re all such fools, we don’t even know it - if we did, we wouldn’t be. Mark Twain knew:</p><blockquote><p>“It ain’t what you don’t know that gets you into trouble. It’s what you know for sure that just ain’t so.”</p></blockquote><p>My boy Bertrand Russel once wrote what is literally my favourite sentence in the English language, which pretty much nails what I’ve been trying to say here but way more succinctly and poetically:</p><blockquote><p>“Everything is vague to a degree you do not realize till you have tried to make it precise, and everything precise is so remote from everything that we normally think, that you cannot for a moment suppose that is what we really mean when we say what we think.”</p></blockquote><p>The more you write, the more you realize how universally true this sentence is. It’s depressing how lossy all our attempts at communication are, written or otherwise. </p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fed00365b-a80e-44f5-bc5c-7bc14f0ca67e_743x192.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fed00365b-a80e-44f5-bc5c-7bc14f0ca67e_743x192.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fed00365b-a80e-44f5-bc5c-7bc14f0ca67e_743x192.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fed00365b-a80e-44f5-bc5c-7bc14f0ca67e_743x192.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fed00365b-a80e-44f5-bc5c-7bc14f0ca67e_743x192.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fed00365b-a80e-44f5-bc5c-7bc14f0ca67e_743x192.png" width="743" height="192" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/ed00365b-a80e-44f5-bc5c-7bc14f0ca67e_743x192.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:192,&quot;width&quot;:743,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:32016,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fed00365b-a80e-44f5-bc5c-7bc14f0ca67e_743x192.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fed00365b-a80e-44f5-bc5c-7bc14f0ca67e_743x192.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fed00365b-a80e-44f5-bc5c-7bc14f0ca67e_743x192.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fed00365b-a80e-44f5-bc5c-7bc14f0ca67e_743x192.png 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption>Amen</figcaption></figure></div><p>I now understand why construction projects can go billions of dollars over budget, why movies can take decades to produce, why novels take years to write. I don’t judge these as failures anymore. I understand why GTA 6 has taken so long. I forgive you, Rockstar Games.</p><p>Because if writing a stupid essay that requires zero research, zero external coordination, and zero dollars, only my time and focus, is this difficult to finish, I’m astounded that anything ever gets done in the world in the first place, and that too well.</p><p>Finishing anything is a miracle in and of itself. A huge shoutout to anyone who’s ever done anything to completion at all.</p></div></article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[LLMLingua: Compressing Prompts for Faster Inferencing (136 pts)]]></title>
            <link>https://github.com/microsoft/LLMLingua</link>
            <guid>38689653</guid>
            <pubDate>Mon, 18 Dec 2023 23:05:29 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/microsoft/LLMLingua">https://github.com/microsoft/LLMLingua</a>, See on <a href="https://news.ycombinator.com/item?id=38689653">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-target="readme-toc.content">
            <article itemprop="text"><div dir="auto">  
    <p><a target="_blank" rel="noopener noreferrer" href="https://github.com/microsoft/LLMLingua/blob/main/images/LLMLingua_logo.png"><img src="https://github.com/microsoft/LLMLingua/raw/main/images/LLMLingua_logo.png" alt="LLMLingua" width="100"></a>  
    </p>  
    <p dir="auto">  
        <h2 tabindex="-1" dir="auto">LLMLingua: Compressing Prompts for Accelerated Inference of Large Language Models &amp; LongLLMLingua</h2>  
    </p>  
</div>
<p dir="auto">
| <a href="https://llmlingua.com/" rel="nofollow"><b>Project Page</b></a>| <a href="https://arxiv.org/abs/2310.05736" rel="nofollow"><b>LLMLingua Paper</b></a> | <a href="https://arxiv.org/abs/2310.06839" rel="nofollow"><b>LongLLMLingua Paper</b></a> | <a href="https://huggingface.co/spaces/microsoft/LLMLingua" rel="nofollow"><b>HF Space Demo</b></a> |
</p>
<details open="">
  <summary>
    
    <span aria-label="Video description LLMLingua_demo.mp4">LLMLingua_demo.mp4</span>
    <span></span>
  </summary>

  <video src="https://private-user-images.githubusercontent.com/30883354/273619158-eb0ea70d-6d4c-4aa7-8977-61f94bb87438.mp4?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTEiLCJleHAiOjE3MDI5ODM5MDcsIm5iZiI6MTcwMjk4MzYwNywicGF0aCI6Ii8zMDg4MzM1NC8yNzM2MTkxNTgtZWIwZWE3MGQtNmQ0Yy00YWE3LTg5NzctNjFmOTRiYjg3NDM4Lm1wND9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFJV05KWUFYNENTVkVINTNBJTJGMjAyMzEyMTklMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjMxMjE5VDExMDAwN1omWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPTUzNjM1MjM3NDI0MmNkNjNiMjRmYTUxMjY5MzcwNWNlZjkxMGFiYTc0ZTUyYjNlMzU1M2VhNTM5ODRhOThmNDgmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0JmFjdG9yX2lkPTAma2V5X2lkPTAmcmVwb19pZD0wIn0.VLZhJeq1I-nRNkeyJcxlxd0zjwGn4KseHSml7kN3noA" data-canonical-src="https://private-user-images.githubusercontent.com/30883354/273619158-eb0ea70d-6d4c-4aa7-8977-61f94bb87438.mp4?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTEiLCJleHAiOjE3MDI5ODM5MDcsIm5iZiI6MTcwMjk4MzYwNywicGF0aCI6Ii8zMDg4MzM1NC8yNzM2MTkxNTgtZWIwZWE3MGQtNmQ0Yy00YWE3LTg5NzctNjFmOTRiYjg3NDM4Lm1wND9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFJV05KWUFYNENTVkVINTNBJTJGMjAyMzEyMTklMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjMxMjE5VDExMDAwN1omWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPTUzNjM1MjM3NDI0MmNkNjNiMjRmYTUxMjY5MzcwNWNlZjkxMGFiYTc0ZTUyYjNlMzU1M2VhNTM5ODRhOThmNDgmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0JmFjdG9yX2lkPTAma2V5X2lkPTAmcmVwb19pZD0wIn0.VLZhJeq1I-nRNkeyJcxlxd0zjwGn4KseHSml7kN3noA" controls="controls" muted="muted">

  </video>
</details>

<h2 tabindex="-1" dir="auto">News</h2>
<ul dir="auto">
<li>🖥 You can find the slides of EMNLP‘23 in <b><a href="https://drive.google.com/file/d/1GxQLAEN8bBB2yiEdQdW4UKoJzZc0es9t/view" rel="nofollow">Session 5</a></b> and <b><a href="https://drive.google.com/file/d/1LJBUfJrKxbpdkwo13SgPOqugk-UjLVIF/view" rel="nofollow">BoF-6</a></b>;</li>
<li>📚 We launched a <a href="https://medium.com/@iofu728/longllmlingua-bye-bye-to-middle-loss-and-save-on-your-rag-costs-via-prompt-compression-54b559b9ddf7" rel="nofollow">blog</a> to showcase the benefits in RAG and long context scenarios. Please see the script of the example at <a href="https://github.com/microsoft/LLMLingua/blob/main/examples/Retrieval.ipynb">this link</a>;</li>
<li>🎈 We launched a <a href="https://llmlingua.com/" rel="nofollow">project page</a> showcasing real-world case studies, including RAG, Online Meetings, CoT, and Code;</li>
<li>👨‍🦯 We have launched a series of examples in the <a href="https://github.com/microsoft/LLMLingua/blob/main/examples">'./examples'</a> folder, which include <a href="https://github.com/microsoft/LLMLingua/blob/main/examples/RAG.ipynb">RAG</a>, <a href="https://github.com/microsoft/LLMLingua/blob/main/examples/OnlineMeeting.ipynb">Online Meeting</a>, <a href="https://github.com/microsoft/LLMLingua/blob/main/examples/CoT.ipynb">CoT</a>, <a href="https://github.com/microsoft/LLMLingua/blob/main/examples/Code.ipynb">Code</a>, and <a href="https://github.com/microsoft/LLMLingua/blob/main/examples/RAGLlamaIndex.ipynb">RAG using LlamaIndex</a>;</li>
<li>👾 LongLLMLingua has been incorporated into the <a href="https://github.com/run-llama/llama_index/blob/main/llama_index/indices/postprocessor/longllmlingua.py">LlamaIndex pipeline</a>, which is a widely used RAG framework.</li>
</ul>
<h2 tabindex="-1" dir="auto">Tl;DR</h2>
<p dir="auto">LLMLingua, that uses a well-trained small language model after alignment, such as GPT2-small or LLaMA-7B, to detect the unimportant tokens in the prompt and enable inference with the compressed prompt in black-box LLMs, achieving up to 20x compression with minimal performance loss.</p>
<p dir="auto"><a href="https://arxiv.org/abs/2310.05736" rel="nofollow">LLMLingua: Compressing Prompts for Accelerated Inference of Large Language Models</a> (EMNLP 2023)<br>
<em>Huiqiang Jiang, Qianhui Wu, Chin-Yew Lin, Yuqing Yang and Lili Qiu</em></p>
<p dir="auto">LongLLMLingua is a method that enhances LLMs' ability to perceive key information in long-context scenarios using prompt compression, achieving up to $28.5 in cost savings per 1,000 samples while also improving performance.</p>
<p dir="auto"><a href="https://arxiv.org/abs/2310.06839" rel="nofollow">LongLLMLingua: Accelerating and Enhancing LLMs in Long Context Scenarios via Prompt Compression</a> (Under Review)<br>
<em>Huiqiang Jiang, Qianhui Wu, Xufang Luo, Dongsheng Li, Chin-Yew Lin, Yuqing Yang and Lili Qiu</em></p>
<h2 tabindex="-1" dir="auto">🎥 Overview</h2>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/microsoft/LLMLingua/blob/main/images/LLMLingua_motivation.png"><img src="https://github.com/microsoft/LLMLingua/raw/main/images/LLMLingua_motivation.png" alt="image"></a></p>
<ul dir="auto">
<li>Have you ever tried to input a long text and ask ChatGPT to summarize it, only to be told that it exceeds the token limit? ​</li>
<li>Have you ever spent a lot of time fine-tuning the personality of ChatGPT, only to find that it forgets the previous instructions after a few rounds of dialogue? ​</li>
<li>Have you ever used the GPT3.5/4 API for experiments, and got good results, but also received a huge bill after a few days? ​</li>
</ul>
<p dir="auto">Large language models, such as ChatGPT and GPT-4, impress us with their amazing generalization and reasoning abilities, but they also come with some drawbacks, such as the prompt length limit and the prompt-based pricing scheme.​</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/microsoft/LLMLingua/blob/main/images/motivation.png"><img src="https://github.com/microsoft/LLMLingua/raw/main/images/motivation.png" alt="image"></a></p>
<p dir="auto">Now you can use <strong>LLMLingua</strong> &amp; <strong>LongLLMLingua</strong>!​</p>
<p dir="auto">A simple and efficient method to compress prompt up to <strong>20x</strong>.​</p>
<ul dir="auto">
<li>💰 <strong>Saving cost</strong>, not only prompt, but also the generation length;​</li>
<li>📝 <strong>Support longer contexts</strong>, enhance the density of key information in the prompt and mitigate loss in the middle, thereby improving overall performance.</li>
<li>⚖️ <strong>Robustness</strong>, no need any training for the LLMs;​</li>
<li>🕵️ <strong>Keeping</strong> the original prompt knowledge like ICL, reasoning, etc.​</li>
<li>📜 <strong>KV-Cache compression</strong>, speedup inference;​</li>
<li>🪃 <strong>GPT-4 can recovery all key information from the compressed prompt</strong>.​</li>
</ul>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/microsoft/LLMLingua/blob/main/images/LLMLingua.png"><img src="https://github.com/microsoft/LLMLingua/raw/main/images/LLMLingua.png" alt="image"></a></p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/microsoft/LLMLingua/blob/main/images/LongLLMLingua.png"><img src="https://github.com/microsoft/LLMLingua/raw/main/images/LongLLMLingua.png" alt="image"></a></p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/microsoft/LLMLingua/blob/main/images/LLMLingua_demo.png"><img src="https://github.com/microsoft/LLMLingua/raw/main/images/LLMLingua_demo.png" alt="image"></a></p>
<p dir="auto">If you find this repo helpful, please cite the following papers:</p>
<div dir="auto" data-snippet-clipboard-copy-content="@inproceedings{jiang-etal-2023-llmlingua,
    title = &quot;LLMLingua: Compressing Prompts for Accelerated Inference of Large Language Models&quot;,
    author = &quot;Huiqiang Jiang and Qianhui Wu and Chin-Yew Lin and Yuqing Yang and Lili Qiu&quot;,
    booktitle = &quot;Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing&quot;,
    month = dec,
    year = &quot;2023&quot;,
    publisher = &quot;Association for Computational Linguistics&quot;,
    url = &quot;https://arxiv.org/abs/2310.05736&quot;,
}"><pre><span>@inproceedings</span>{<span>jiang-etal-2023-llmlingua</span>,
    <span>title</span> = <span><span>"</span>LLMLingua: Compressing Prompts for Accelerated Inference of Large Language Models<span>"</span></span>,
    <span>author</span> = <span><span>"</span>Huiqiang Jiang and Qianhui Wu and Chin-Yew Lin and Yuqing Yang and Lili Qiu<span>"</span></span>,
    <span>booktitle</span> = <span><span>"</span>Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing<span>"</span></span>,
    <span>month</span> = dec,
    <span>year</span> = <span><span>"</span>2023<span>"</span></span>,
    <span>publisher</span> = <span><span>"</span>Association for Computational Linguistics<span>"</span></span>,
    <span>url</span> = <span><span>"</span>https://arxiv.org/abs/2310.05736<span>"</span></span>,
}</pre></div>
<div dir="auto" data-snippet-clipboard-copy-content="@article{jiang-etal-2023-longllmlingua,
    title = &quot;LongLLMLingua: Accelerating and Enhancing LLMs in Long Context Scenarios via Prompt Compression&quot;,
    author = &quot;Huiqiang Jiang and Qianhui Wu and and Xufang Luo and Dongsheng Li and Chin-Yew Lin and Yuqing Yang and Lili Qiu&quot;,
    url = &quot;https://arxiv.org/abs/2310.06839&quot;,
    journal = &quot;ArXiv preprint&quot;,
    volume = &quot;abs/2310.06839&quot;,
    year = &quot;2023&quot;,
}"><pre><span>@article</span>{<span>jiang-etal-2023-longllmlingua</span>,
    <span>title</span> = <span><span>"</span>LongLLMLingua: Accelerating and Enhancing LLMs in Long Context Scenarios via Prompt Compression<span>"</span></span>,
    <span>author</span> = <span><span>"</span>Huiqiang Jiang and Qianhui Wu and and Xufang Luo and Dongsheng Li and Chin-Yew Lin and Yuqing Yang and Lili Qiu<span>"</span></span>,
    <span>url</span> = <span><span>"</span>https://arxiv.org/abs/2310.06839<span>"</span></span>,
    <span>journal</span> = <span><span>"</span>ArXiv preprint<span>"</span></span>,
    <span>volume</span> = <span><span>"</span>abs/2310.06839<span>"</span></span>,
    <span>year</span> = <span><span>"</span>2023<span>"</span></span>,
}</pre></div>
<h2 tabindex="-1" dir="auto">🎯 Quick Start</h2>
<p dir="auto">Install LLMLingua,</p>

<p dir="auto">Then, you can use LLMLingua to compress your prompt,</p>
<div dir="auto" data-snippet-clipboard-copy-content="from llmlingua import PromptCompressor

llm_lingua = PromptCompressor()
compressed_prompt = llm_lingua.compress_prompt(prompt, instruction=&quot;&quot;, question=&quot;&quot;, target_token=200)

# > {'compressed_prompt': 'Question: Sam bought a dozen boxes, each with 30 highlighter pens inside, for $10 each box. He reanged five of boxes into packages of sixlters each and sold them $3 per. He sold the rest theters separately at the of three pens $2. How much did make in total, dollars?\nLets think step step\nSam bought 1 boxes x00 oflters.\nHe bought 12 * 300ters in total\nSam then took 5 boxes 6ters0ters.\nHe sold these boxes for 5 *5\nAfterelling these  boxes there were 3030 highlighters remaining.\nThese form 330 / 3 = 110 groups of three pens.\nHe sold each of these groups for $2 each, so made 110 * 2 = $220 from them.\nIn total, then, he earned $220 + $15 = $235.\nSince his original cost was $120, he earned $235 - $120 = $115 in profit.\nThe answer is 115',
#  'origin_tokens': 2365,
#  'compressed_tokens': 211,
#  'ratio': '11.2x',
#  'saving': ', Saving $0.1 in GPT-4.'}

## Or use the quantation model, like TheBloke/Llama-2-7b-Chat-GPTQ, only need <8GB GPU memory.
## Before that, you need to pip install optimum auto-gptq
llm_lingua = PromptCompressor(&quot;TheBloke/Llama-2-7b-Chat-GPTQ&quot;, model_config={&quot;revision&quot;: &quot;main&quot;})"><pre><span>from</span> <span>llmlingua</span> <span>import</span> <span>PromptCompressor</span>

<span>llm_lingua</span> <span>=</span> <span>PromptCompressor</span>()
<span>compressed_prompt</span> <span>=</span> <span>llm_lingua</span>.<span>compress_prompt</span>(<span>prompt</span>, <span>instruction</span><span>=</span><span>""</span>, <span>question</span><span>=</span><span>""</span>, <span>target_token</span><span>=</span><span>200</span>)

<span># &gt; {'compressed_prompt': 'Question: Sam bought a dozen boxes, each with 30 highlighter pens inside, for $10 each box. He reanged five of boxes into packages of sixlters each and sold them $3 per. He sold the rest theters separately at the of three pens $2. How much did make in total, dollars?\nLets think step step\nSam bought 1 boxes x00 oflters.\nHe bought 12 * 300ters in total\nSam then took 5 boxes 6ters0ters.\nHe sold these boxes for 5 *5\nAfterelling these  boxes there were 3030 highlighters remaining.\nThese form 330 / 3 = 110 groups of three pens.\nHe sold each of these groups for $2 each, so made 110 * 2 = $220 from them.\nIn total, then, he earned $220 + $15 = $235.\nSince his original cost was $120, he earned $235 - $120 = $115 in profit.\nThe answer is 115',</span>
<span>#  'origin_tokens': 2365,</span>
<span>#  'compressed_tokens': 211,</span>
<span>#  'ratio': '11.2x',</span>
<span>#  'saving': ', Saving $0.1 in GPT-4.'}</span>

<span>## Or use the quantation model, like TheBloke/Llama-2-7b-Chat-GPTQ, only need &lt;8GB GPU memory.</span>
<span>## Before that, you need to pip install optimum auto-gptq</span>
<span>llm_lingua</span> <span>=</span> <span>PromptCompressor</span>(<span>"TheBloke/Llama-2-7b-Chat-GPTQ"</span>, <span>model_config</span><span>=</span>{<span>"revision"</span>: <span>"main"</span>})</pre></div>
<p dir="auto">You can refer to the <a href="https://github.com/microsoft/LLMLingua/blob/main/examples"><strong>examples</strong></a> to understand how to use <strong>LLMLingua</strong> and <strong>LongLLMLingua</strong> in practical scenarios, such as RAG, Online Meeting, CoT, Code, and RAG using LlamaIndex. Additionally, you can refer to the <a href="https://github.com/microsoft/LLMLingua/blob/main/DOCUMENT.md"><strong>document</strong></a> for more recommendations on how to use LLMLingua effectively.</p>
<h2 tabindex="-1" dir="auto">Frequently Asked Questions</h2>
<p dir="auto">show in <a href="https://github.com/microsoft/LLMLingua/blob/main/Transparency_FAQ.md">Transparency_FAQ.md</a></p>
<h2 tabindex="-1" dir="auto">Contributing</h2>
<p dir="auto">This project welcomes contributions and suggestions.  Most contributions require you to agree to a
Contributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us
the rights to use your contribution. For details, visit <a href="https://cla.opensource.microsoft.com/" rel="nofollow">https://cla.opensource.microsoft.com</a>.</p>
<p dir="auto">When you submit a pull request, a CLA bot will automatically determine whether you need to provide
a CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions
provided by the bot. You will only need to do this once across all repos using our CLA.</p>
<p dir="auto">This project has adopted the <a href="https://opensource.microsoft.com/codeofconduct/" rel="nofollow">Microsoft Open Source Code of Conduct</a>.
For more information see the <a href="https://opensource.microsoft.com/codeofconduct/faq/" rel="nofollow">Code of Conduct FAQ</a> or
contact <a href="mailto:opencode@microsoft.com">opencode@microsoft.com</a> with any additional questions or comments.</p>
<h2 tabindex="-1" dir="auto">Trademarks</h2>
<p dir="auto">This project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft
trademarks or logos is subject to and must follow
<a href="https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general" rel="nofollow">Microsoft's Trademark &amp; Brand Guidelines</a>.
Use of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.
Any use of third-party trademarks or logos are subject to those third-party's policies.</p>
</article>
          </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Volcanic eruption started just north of the town of Grindavik, Iceland (106 pts)]]></title>
            <link>https://www.visir.is/g/20232505181d/eld-gos-hafid-a-reykja-nes-skaga</link>
            <guid>38689617</guid>
            <pubDate>Mon, 18 Dec 2023 23:01:12 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.visir.is/g/20232505181d/eld-gos-hafid-a-reykja-nes-skaga">https://www.visir.is/g/20232505181d/eld-gos-hafid-a-reykja-nes-skaga</a>, See on <a href="https://news.ycombinator.com/item?id=38689617">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
                                <!-- ************** ARTICLE SUMMARY ************** -->
 
<p>Gos hófst um fjóra kílómetra norðaustan Grindavíkur, norðan Sundhnúks í Sundhnúkaröðinni, klukkan 22:17 í kvöld.</p>

<!--AD-MISSING-->

<article>
  
  <div disabled="" itemprop="articleBody" data-element-type="body" data-element-label="Meginmál" data-element-data-type="xml">
    <ul>
      <li>Undanfari eldgoss var skjálftahrina við Sundhnjúkagíga sem hófst skyndilega í kvöld klukkan 21:00.</li>
      <li>Almannavarnir hafa lýst yfir neyðarástandi. Almenningur hefur verið beðinn um að stöðva ekki bíla sína á Reykjanesbraut.</li>
      <li>Grindavíkursvæðið hefur verið rýmt.</li>
      <li>Á öðrum tímanum var var áætluð lengd sprungunnar meira en fjórir kílómetrar. Til samanburðar var lengd sprungunnar í eldgosinu við Litla-Hrút í júlí um 800 til 900 metrar.</li>
      <li>Áætlað hraunflæði í eldgosinu er um 100 til 200 rúmmetrar á sekúndu sem er margfalt meira en í fyrri gosum á Reykjanesskaga síðustu ár.</li>
    </ul>
    <p>
      <i>Nánar í vaktinni hér að neðan. Ráð er að endurhlaða síðuna ef vaktin birtist ekki. Þá má sjá beina útsendingu í spilaranum hér að neðan.</i>
    </p>
    <figure></figure>
  </div>
  
  
</article>                        </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[I Love Ruby (392 pts)]]></title>
            <link>https://eliseshaffer.com/2023/12/18/i-love-ruby/</link>
            <guid>38688453</guid>
            <pubDate>Mon, 18 Dec 2023 21:18:52 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://eliseshaffer.com/2023/12/18/i-love-ruby/">https://eliseshaffer.com/2023/12/18/i-love-ruby/</a>, See on <a href="https://news.ycombinator.com/item?id=38688453">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
      <a href="https://eliseshaffer.com/writing"><i></i>Back</a>

<span>Dec 18, 2023</span>


<figure>
    <img src="https://eliseshaffer.com/images/2023-12-18-ruby-logo.png" alt="Ruby logo">
    <figcaption>
        The Ruby logo is Copyright © 2006, Yukihiro Matsumoto and is licensed under Creative Commons Attribution-ShareAlike 2.5
    </figcaption>
</figure>

<p>I’ve done a lot in my career. From working on business support software to big backend systems and even robotics. If there’s been a consistent through-line to my career, it’s the Ruby programming language.</p>

<p>In recent years, Ruby has fallen out of favor with the tech industry, but I still love it. It’s a great language with so many strengths that it hurts when I see people trash Ruby or say its time is over. Thankfully, I’m not alone. There are so many Ruby devs still out here. The community is niche, but thriving. People are puzzled by this sometimes. Once, a colleague once said “rubyists really love ruby,” with an air of surprise. I’ve heard others trash ruby for not having types. One person laughed at me when I said Ruby was an “expressive language” and said, “it doesn’t express anything about the implementation.”</p>

<p>So why do myself and others love Ruby so much? I thought I’d explore this a bit.</p>

<h2 id="fostering-programmer-happiness">Fostering Programmer Happiness</h2>
<p>First and foremost, Ruby is a language that strives to make programmers happy. This is an underrated value and one that often gets our community mocked by our peers within other communities. The language is meant be joyful to use. This is a value and an ethos that permeates the language, the ecosystem of gems, and the community. Everything else that Ruby is stems from this value.</p>

<p>When I write Ruby, I take so much joy out of the process. Even the bad parts. In the middle of a high stress incident, Ruby still sparks joy. In fact, the only time I don’t enjoy Ruby is when I’m working on a piece of code that doesn’t follow Ruby practices, patterns, and idioms. Which brings me to the next topic</p>

<h2 id="expressiveness-encouraged">Expressiveness Encouraged</h2>
<p>Ruby is probably the most expressive programming language on Earth. Between its metaprogramming features and cultural idioms, Ruby allows programmers to write code that clearly expresses its intent. Well written ruby code can often read like natural language. Features like predicate methods even give up punctuation. So you can easily write something like:</p>

<div><pre><code><span>if</span> <span>@subscription</span><span>.</span><span>supports_feature?</span><span>(</span><span>:feature_a</span><span>)</span>
  <span>...</span>
<span>else</span>
  <span>...</span>
<span>end</span>
</code></pre></div>

<p>This is often why ruby programmers don’t like comments. In most cases, the language makes comments unnecessary. And when you do need them, it’s often when you’re doing a very specific  or obscure thing that requires context to understand. It’s clear from the code why you need a comment in that case.</p>

<p>Tied into this is the ecosystems embrace of domain specific languages(DSLs), which give us the power to write incredibly expressive code that reads like what it’s doing. For example, RSpec’s DSL reads exactly like how a person might talk about what they want to test:</p>

<div><pre><code><span>RSpec</span><span>.</span><span>describe</span> <span>Ticket</span> <span>do</span>
  <span>context</span> <span>'when the ticket is closed'</span> <span>do</span>
    <span>it</span> <span>'emails the requestor with a confirmation'</span> <span>do</span>
      <span>...</span>
	<span>end</span>
  <span>end</span>
<span>end</span>
</code></pre></div>

<p><del>Methods</del>, sorry “approaches,” like this have made Ruby so joyful to use.</p>

<h2 id="a-language-made-just-for-me">A Language Made Just For Me</h2>
<p>Many rubyists, myself included, recount stories of how Ruby and Rails just fit their brain. I recently told this story on <a href="https://www.remoteruby.com/2260490/14003706-unlocking-the-power-of-state-machines-in-code-development-with-elise-schaefer">Remote Ruby</a> about learning ruby. I said that when I was learning Ruby it felt like it was made just for me. I could guess method names and signatures and I would be right most of the time. When I was wrong, I could switch the order of arguments. This was so true that quickly I learned to just try something based on intuition before reading the docs. I know so many rubyists who have similar stories.</p>

<p>Feeling recognition in the language you’re programming is so powerful.</p>

<h2 id="community-and-values">Community And Values</h2>
<p>It’s so hard to talk about Ruby without talking about how the community shapes what Ruby is and how it feels to use the language. Ruby is a great language with so many benefits. There’s lots of features and values built into its core that make it a joy to use.</p>

<p>But as Kent Beck said at RailsConf in 2020, “Software design is an exercise in human relationships.” He was talking about how we think about designing software. But I think this quote applies to our community and our values as well. If you’ve followed this blog or listened to The Ruby on Rails Podcast since I took over, you know that community is a big part of my value system. That value is shared by other rubyists.</p>

<p>I attended RubyConf in San Diego this year. These conferences are always a blast. It feels like I’m coming home to see old friends. The community building that happens at these conferences makes lifelong colleagues and friends. There a spirit of learning, and cooperation. We want to help each other, teach each other, learn from each other. The Ruby community is an incredible group of people of welcoming, kind, and supportive peers. Without the community, Ruby couldn’t be what it is.</p>

<p>There’s a lot of reasons to like ruby. And there’s so many things that rubyists love. I haven’t covered everything. But Ruby’s primary features are joy and community. That’s hard to beat.</p>

<p>Do you love Ruby? <a href="mailto:elise.shaffer+blog@hey.com">Tell me what you love about it</a></p>


    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Tiny Code Christmas (122 pts)]]></title>
            <link>https://tcc.lovebyte.party/</link>
            <guid>38688355</guid>
            <pubDate>Mon, 18 Dec 2023 21:11:51 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://tcc.lovebyte.party/">https://tcc.lovebyte.party/</a>, See on <a href="https://news.ycombinator.com/item?id=38688355">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
      <h2 id="welcome-to-tiny-code-christmas">Welcome to Tiny Code Christmas!</h2>
<p><strong>TCC started on the 11th of December 2023, but you can join at any time!</strong></p>
<p><a href="https://lovebyte.party/">LoveByte’s</a> Tiny Code Christmas is an event to help you take your first steps in demoscene sizecoding with Lua based fantasy consoles like <a href="https://tic80.com/">TIC-80</a> and <a href="https://www.lexaloffle.com/pico-8.php">PICO-8</a>! Over 12 days, from the 11th to the 22nd of December, learn the effects and techniques used in Byte Jams, Byte Battles, and Tiny Intros!</p>
<p>Watch the TCC invite by jtruk and dave84 that was released at Inercia for both <a href="https://www.youtube.com/watch?v=Rseb2JgDMuc">TIC-80</a> and <a href="https://www.youtube.com/watch?v=f694SdjhUBA">PICO-8</a>!</p>

<p>
  <iframe src="https://www.youtube.com/embed/Rseb2JgDMuc" allowfullscreen="" title="YouTube Video"></iframe>
</p>

<p>You can check out our participant’s amazing work from last year in our <a href="https://youtu.be/Mz3hK4SQ6U0">Tiny Code Christmas New Years Eve Showcase Here!</a></p>
<h2 id="the-12-daily-challenges">The 12 Daily Challenges:</h2>
<h2 id="day-0---getting-startedday0"><a href="https://tcc.lovebyte.party/day0">Day 0 - Getting Started</a></h2>
<p>All you need to get ready for tomorrow!</p>
<h2 id="day-1---making-shapesday1"><a href="https://tcc.lovebyte.party/day1">Day 1 - Making Shapes</a></h2>
<p>Get to grips with the basics of TIC-80 and PICO-8!</p>
<h2 id="day-2---snow-time-like-the-presentday2"><a href="https://tcc.lovebyte.party/day2">Day 2 - Snow time() Like the Present</a></h2>
<p>Add some basic animation to your scene!</p>
<h2 id="day-3---little-by-littleday3"><a href="https://tcc.lovebyte.party/day3">Day 3 - Little By Little</a></h2>
<p>Create a full-screen effect by manipulating each pixel individually!</p>
<h2 id="day-4---sines-of-the-timesday4"><a href="https://tcc.lovebyte.party/day4">Day 4 - Sines of the Times</a></h2>
<p>Create an animated plasma effect!</p>
<h2 id="day-5---polar-expressday5"><a href="https://tcc.lovebyte.party/day5">Day 5 - Polar Express</a></h2>
<p>Create an animated tunnel effect using polar coordinates!</p>
<h2 id="day-6---hello-demosceneday6"><a href="https://tcc.lovebyte.party/day6">Day 6 - Hello, Demoscene!</a></h2>
<p>Create an animated sine-scroller effect!</p>
<h2 id="day-7---mix-and-matchday7"><a href="https://tcc.lovebyte.party/day7">Day 7 - Mix and Match</a></h2>
<p>Show us what you got by combining effects!</p>
<h2 id="day-8---round-and-roundday8"><a href="https://tcc.lovebyte.party/day8">Day 8 - Round and Round</a></h2>
<p>Create an animated tentacle effect by rotating circles!</p>
<h2 id="day-9---shade-the-bobday9"><a href="https://tcc.lovebyte.party/day9">Day 9 - Shade the Bob</a></h2>
<p>Create a classic shadebob effect!</p>
<p><strong>Check back tomorrow for Day 10!</strong></p>


<p>Explore the classic demoscene fire effect!</p>

<p>Actually, lets code some metaballs instead!</p>

<p>Create a proximitor/connected dots effect!</p>

<p>Create a full screen rotation effect!</p>

<p>Combine rotation with a perspective effect!</p>

<p>Combine rotation with a perspective effect!</p>

<p>Get creative with a snowflake!</p>

<p>To code a demo from scratch, you must first invent the universe…</p>
<h2 id="day-9---3d-dot-landscapeday9extra"><a href="https://tcc.lovebyte.party/day9extra">Day 9 - 3D Dot Landscape</a></h2>
<p>Create a 3D plasma landscape!</p>
<p><strong>Check back tomorrow for Day 10!</strong></p>

<p>Join us for 12 days of tiny challenges to gradually introduce you to size coding and effects! A new video will be released each day to introduce a new concept and outline the challenge.</p>
<p>A little bit of programming knowledge will help but you don’t need a lot. The challenges will introduce demoscene concepts without jargon so it is friendly for newcomers to the scene! Tell your friends!</p>
<h2 id="keep-in-touch">Keep In Touch!</h2>
<p>Join the <a href="https://discord.gg/pUS5kCJTzp">LoveByte Discord</a>. Find us at #lovebyte on IRCnet! Follow LoveByte on <a href="https://mobile.twitter.com/lovebyteparty">Twitter</a> and <a href="https://graphics.social/@lovebyteparty">Mastodon</a>!</p>

    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Librum: Open-Source e-book platform (238 pts)]]></title>
            <link>https://github.com/Librum-Reader/Librum</link>
            <guid>38688127</guid>
            <pubDate>Mon, 18 Dec 2023 20:55:29 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/Librum-Reader/Librum">https://github.com/Librum-Reader/Librum</a>, See on <a href="https://news.ycombinator.com/item?id=38688127">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-target="readme-toc.content">
            <article itemprop="text"><h2 tabindex="-1" dir="auto">Librum</h2>
<p dir="auto">Librum is an application designed to make reading <b>enjoyable</b> and <b>straightforward</b> for everyone.</p>
<p dir="auto">It's not <strong>just</strong> an e-book reader. With Librum, you can manage your own online library and access it from any device anytime, anywhere. It has features like note-taking, bookmarking, and highlighting, while offering customization to make it as personal as you want!</p>
<p dir="auto">Librum also provides free access to over 70,000 books and personal reading statistics while being free and completely open source.</p>
<h2 tabindex="-1" dir="auto">Preview</h2>
<p dir="auto">A simple and modern interface</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://private-user-images.githubusercontent.com/69865187/261866073-bf1d0401-62bd-4f4e-b008-523fb2efd275.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTEiLCJleHAiOjE3MDI5NTUxMDUsIm5iZiI6MTcwMjk1NDgwNSwicGF0aCI6Ii82OTg2NTE4Ny8yNjE4NjYwNzMtYmYxZDA0MDEtNjJiZC00ZjRlLWIwMDgtNTIzZmIyZWZkMjc1LnBuZz9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFJV05KWUFYNENTVkVINTNBJTJGMjAyMzEyMTklMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjMxMjE5VDAzMDAwNVomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPTM0ZGI5MTM3MTQyOTQ4NjkwZDdkYzZlMWRiMGRkNGRkZTBmMmJiY2QwNmVkMjM3MjdhY2UzNDhmOGJhZmUzMzMmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0JmFjdG9yX2lkPTAma2V5X2lkPTAmcmVwb19pZD0wIn0.Yq5oBLzIONA9WhYlWn0nmDo7ABZi5yIUdLhtCIL4RRs"><img src="https://private-user-images.githubusercontent.com/69865187/261866073-bf1d0401-62bd-4f4e-b008-523fb2efd275.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTEiLCJleHAiOjE3MDI5NTUxMDUsIm5iZiI6MTcwMjk1NDgwNSwicGF0aCI6Ii82OTg2NTE4Ny8yNjE4NjYwNzMtYmYxZDA0MDEtNjJiZC00ZjRlLWIwMDgtNTIzZmIyZWZkMjc1LnBuZz9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFJV05KWUFYNENTVkVINTNBJTJGMjAyMzEyMTklMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjMxMjE5VDAzMDAwNVomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPTM0ZGI5MTM3MTQyOTQ4NjkwZDdkYzZlMWRiMGRkNGRkZTBmMmJiY2QwNmVkMjM3MjdhY2UzNDhmOGJhZmUzMzMmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0JmFjdG9yX2lkPTAma2V5X2lkPTAmcmVwb19pZD0wIn0.Yq5oBLzIONA9WhYlWn0nmDo7ABZi5yIUdLhtCIL4RRs" alt="image"></a></p>

<p dir="auto">Setup and manage your own library</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://private-user-images.githubusercontent.com/69865187/244929153-ea94fc68-1bf0-4933-8d80-43a57c6590c5.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTEiLCJleHAiOjE3MDI5NTUxMDUsIm5iZiI6MTcwMjk1NDgwNSwicGF0aCI6Ii82OTg2NTE4Ny8yNDQ5MjkxNTMtZWE5NGZjNjgtMWJmMC00OTMzLThkODAtNDNhNTdjNjU5MGM1LnBuZz9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFJV05KWUFYNENTVkVINTNBJTJGMjAyMzEyMTklMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjMxMjE5VDAzMDAwNVomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPWY3MDRmMzg0NjllMmE4OTA4MWFhY2YzN2IyY2ViODFmODZmMDJkMzdhODFkYTRiZTNiOTNiODcxYTYxNWVlMDQmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0JmFjdG9yX2lkPTAma2V5X2lkPTAmcmVwb19pZD0wIn0.aD5NN-xtFZ6hz8A0q0QfKFOkFXj2D4skeUkoMTfUyq8"><img src="https://private-user-images.githubusercontent.com/69865187/244929153-ea94fc68-1bf0-4933-8d80-43a57c6590c5.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTEiLCJleHAiOjE3MDI5NTUxMDUsIm5iZiI6MTcwMjk1NDgwNSwicGF0aCI6Ii82OTg2NTE4Ny8yNDQ5MjkxNTMtZWE5NGZjNjgtMWJmMC00OTMzLThkODAtNDNhNTdjNjU5MGM1LnBuZz9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFJV05KWUFYNENTVkVINTNBJTJGMjAyMzEyMTklMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjMxMjE5VDAzMDAwNVomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPWY3MDRmMzg0NjllMmE4OTA4MWFhY2YzN2IyY2ViODFmODZmMDJkMzdhODFkYTRiZTNiOTNiODcxYTYxNWVlMDQmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0JmFjdG9yX2lkPTAma2V5X2lkPTAmcmVwb19pZD0wIn0.aD5NN-xtFZ6hz8A0q0QfKFOkFXj2D4skeUkoMTfUyq8" alt="HomeScreenDark"></a></p>

<p dir="auto">Customize Librum to make it personal to you</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://private-user-images.githubusercontent.com/69865187/262316821-b8995cf1-a0e6-4993-8c8b-92f7f8e79ebd.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTEiLCJleHAiOjE3MDI5NTUxMDUsIm5iZiI6MTcwMjk1NDgwNSwicGF0aCI6Ii82OTg2NTE4Ny8yNjIzMTY4MjEtYjg5OTVjZjEtYTBlNi00OTkzLThjOGItOTJmN2Y4ZTc5ZWJkLnBuZz9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFJV05KWUFYNENTVkVINTNBJTJGMjAyMzEyMTklMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjMxMjE5VDAzMDAwNVomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPTQzOWZjZWFkZGRmZDhhMzZjMjM2MzUxNTBjMDhjYTM0Y2RiMzU2ZGMxNDA1YmZhMDE2MTE2NzQxYWQzOThiOWEmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0JmFjdG9yX2lkPTAma2V5X2lkPTAmcmVwb19pZD0wIn0.QYBJLcTLRX9CubnhuSAvrlV2ivZ_jt18WaW84psln1E"><img src="https://private-user-images.githubusercontent.com/69865187/262316821-b8995cf1-a0e6-4993-8c8b-92f7f8e79ebd.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTEiLCJleHAiOjE3MDI5NTUxMDUsIm5iZiI6MTcwMjk1NDgwNSwicGF0aCI6Ii82OTg2NTE4Ny8yNjIzMTY4MjEtYjg5OTVjZjEtYTBlNi00OTkzLThjOGItOTJmN2Y4ZTc5ZWJkLnBuZz9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFJV05KWUFYNENTVkVINTNBJTJGMjAyMzEyMTklMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjMxMjE5VDAzMDAwNVomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPTQzOWZjZWFkZGRmZDhhMzZjMjM2MzUxNTBjMDhjYTM0Y2RiMzU2ZGMxNDA1YmZhMDE2MTE2NzQxYWQzOThiOWEmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0JmFjdG9yX2lkPTAma2V5X2lkPTAmcmVwb19pZD0wIn0.QYBJLcTLRX9CubnhuSAvrlV2ivZ_jt18WaW84psln1E" alt="image"></a></p>
<br>
<h2 tabindex="-1" dir="auto">How can I get Librum?</h2>
<p dir="auto">Simply go to <a href="https://librumreader.com/" rel="nofollow">https://librumreader.com</a> to download Librum.</p>
<p dir="auto">If you want to build Librum from source, follow the instructions <a href="#build-guide">here</a>.</p>
<br>
<h2 tabindex="-1" dir="auto">Translations</h2>
<p dir="auto">Librum is currently available in:</p>
<ul dir="auto">
<li>English</li>
<li>German</li>
<li>Russian</li>
<li>Mandarin</li>
</ul>
<p dir="auto">If you want to translate Librum to another language, follow the steps below:</p>
<ul dir="auto">
<li>Download the file at: <a href="https://github.com/Librum-Reader/Librum/blob/main/src/presentation/translations/librum_en.ts">https://github.com/Librum-Reader/Librum/blob/main/src/presentation/translations/librum_en.ts</a></li>
<li>Rename the file to contain your language's suffix, e.g. "librum_ru.ts" for Russian or "librum_de.ts" for German</li>
<li>Download the translation software (Qt Linguist) either for Windows from <a href="https://github.com/thurask/Qt-Linguist">https://github.com/thurask/Qt-Linguist</a> or using the Qt Installer at <a href="https://www.qt.io/download-open-source" rel="nofollow">https://www.qt.io/download-open-source</a></li>
<li>Now start Qt Linguist, open the downloaded file, set the target language to the language you want to translate to and start translating.
(For a quick guide on Qt Linguist, check out: <a href="https://youtu.be/xNIz78IPBu0?t=347" rel="nofollow">https://youtu.be/xNIz78IPBu0?t=347</a>)</li>
</ul>
<p dir="auto">Once you are done, create a pull request or an issue with your new translation file!<br>
If you run into any problems, need guidance or have questions, feel free to reach out to us at: <a href="mailto:contact@librumreader.com">contact@librumreader.com</a></p>

<p dir="auto">Notes:</p>
<ul dir="auto">
<li>Make sure that your translations are approximately the same length as the original text</li>
<li>Make sure that you keep to the punctuation and capitalisation</li>
<li>Make sure that your translations make sense in the context they are in</li>
</ul>
<br>
<h2 tabindex="-1" dir="auto">Documentation</h2>
<p dir="auto">For documentation go to <a href="https://github.com/Librum-Reader/Librum/wiki">Librum's GitHub-wiki</a></p>
<br>
<h2 tabindex="-1" dir="auto">Donations</h2>
<div dir="auto"><p>Donations make it possible for us to cover our server costs and allow us to make investments into new areas of development.
<br>
If you would like to support us, check out: <a href="https://librumreader.com/contribute/donate" rel="nofollow">https://librumreader.com/contribute/donate</a> or become a github sponsor!
</p><p>

As a team of opensource developers we rely on donations to continue working on projects like Librum. Your help is greatly appreciated.</p></div>
<br>
<h2 tabindex="-1" dir="auto">Contributing</h2>

<ol dir="auto">
<li>Discord (m_david#0631)</li>
<li>Email (<a href="mailto:contact@librumreader.com">contact@librumreader.com</a>)</li>
</ol>
<br>
We are following a pull request workflow where every contribution is sent as a pull request and merged into the dev/develop branch for testing. <br>
Please make sure to run clang format, keep to the conventions used throughout the application and ensure that all tests pass, before submitting any pull request.
<h2 tabindex="-1" dir="auto">Contact</h2>
<p dir="auto">For questions, you can reach us under: <a href="mailto:help@librumreader.com">help@librumreader.com</a>
<br>
For business related contact, reach out to us here: <a href="mailto:contact@librumreader.com">contact@librumreader.com</a></p>
<br>
<h2 tabindex="-1" dir="auto">Details</h2>
<h3 tabindex="-1" dir="auto">Supported platforms</h3>
<p dir="auto">Part of Librum's aim is to work on <strong>any</strong> platform. No matter where you are or which device you use, you can always continue your book with Librum, as it is <b>cross platform</b>.<br>
We support:</p>
<ul dir="auto">
<li>Windows</li>
<li>GNU/Linux</li>
<li>MacOS</li>
<li>IOS (Coming Soon)</li>
<li>Android (Coming Soon)</li>
</ul>
<br>
<h3 tabindex="-1" dir="auto">Supported formats</h3>
<p dir="auto">Librum is the best choice for all kinds of books, since Librum supports <b>all</b> major book formats<br>
including:</p>
<ul dir="auto">
<li>PDF</li>
<li>EPUB</li>
<li>CBZ (Comic books)</li>
<li>XPS</li>
<li>PS</li>
<li>All plain text formats</li>
<li>Images</li>
</ul>
<br>
<h3 tabindex="-1" dir="auto">Features</h3>
<p dir="auto">Librum's objective is to make your reading more <b>productive</b>; to that end, we provide you with a variety of features that you can access via a <b>simple</b> and <b>straightforward</b> interface.<br>
These features include:</p>
<ul dir="auto">
<li>A modern e-reader</li>
<li>A personalized and customizable library</li>
<li>Book meta-data editing</li>
<li>A free in-app bookstore with more than 70.000 books</li>
<li>Book syncing across all of your devices</li>
<li>Highlighting</li>
<li>Bookmarking</li>
<li>Text search</li>
<li>Unlimited customization</li>
<li>Note-taking (Coming Soon)</li>
<li>TTS (Coming Soon)</li>
<li>Personalized reading statistics (Coming Soon)</li>
<li>No-login book reading (Coming Soon)</li>
</ul>
<p dir="auto">Want a new feature? Feel free to leave a feature request ticket!</p>

<h2 tabindex="-1" dir="auto">Build Guide</h2>
<p dir="auto">Follow this guide to build Librum from source.
<br></p>
<h2 tabindex="-1" dir="auto">For GNU/Linux</h2>
<h3 tabindex="-1" dir="auto">Prerequisites</h3>
<ul dir="auto">
<li>cmake                             (<a href="https://cmake.org/download" rel="nofollow">https://cmake.org/download</a>)</li>
<li>make                              (<a href="http://ftp.gnu.org/gnu/make" rel="nofollow">http://ftp.gnu.org/gnu/make</a>)</li>
<li>g++                               (<a href="https://gcc.gnu.org/" rel="nofollow">https://gcc.gnu.org</a>)</li>
<li>python3-venv                      (on ubuntu use <code>sudo apt install python3-venv</code>)</li>
<li>Qt 6.5                            (<a href="https://www.qt.io/download-open-source" rel="nofollow">https://www.qt.io/download-open-source</a>)</li>
</ul>
<h3 tabindex="-1" dir="auto">Installation</h3>
<p dir="auto">The installation is straight forward, just follow the steps below:</p>
<br>
<ol dir="auto">
<li>Clone the repository.
<div dir="auto" data-snippet-clipboard-copy-content="git clone https://github.com/Librum-Reader/Librum.git --recursive"><pre>git clone https://github.com/Librum-Reader/Librum.git --recursive</pre></div>
</li>
<li>Step into the cloned project folder.

</li>
<li>Create the build folder and step into it.
<div dir="auto" data-snippet-clipboard-copy-content="mkdir build-Release
cd build-Release"><pre>mkdir build-Release
<span>cd</span> build-Release</pre></div>
</li>
<li>Run cmake.
<div dir="auto" data-snippet-clipboard-copy-content="cmake -DCMAKE_INSTALL_PREFIX=/usr -DCMAKE_BUILD_TYPE=Release -DBUILD_TESTS=Off -DCMAKE_PREFIX_PATH=<path/to/Qt> .."><pre>cmake -DCMAKE_INSTALL_PREFIX=/usr -DCMAKE_BUILD_TYPE=Release -DBUILD_TESTS=Off -DCMAKE_PREFIX_PATH=<span>&lt;</span>path/to/Qt<span>&gt;</span> ..</pre></div>
Set <code>CMAKE_PREFIX_PATH</code> to your Qt installation path. Installing Qt via the online installer usually installs it to <code>/home/&lt;name&gt;/Qt/&lt;version&gt;/gcc_64</code></li>
<li>Build the project
<div dir="auto" data-snippet-clipboard-copy-content="cmake --build . -j $(nproc)"><pre>cmake --build <span>.</span> -j <span><span>$(</span>nproc<span>)</span></span></pre></div>
</li>
<li>Install Librum

</li>
</ol>
<br>
<h3 tabindex="-1" dir="auto">Troubleshooting</h3>
<p dir="auto">Here are solutions to some common errors. If your error is not listed here, please open an issue.
<br></p>
<ul dir="auto">
<li>
<p dir="auto">Error: <code>Failed to find required Qt component "Quick".</code><br></p>
</li>
<li>
<p dir="auto">Solution: Install the libGL mesa dev package, on ubuntu its <code>sudo apt install libgl1-mesa-dev</code> and on fedora its <code>sudo dnf install mesa-libGL-devel</code>.</p>
</li>
<li>
<p dir="auto">Error: <code>Could not load the qt platform plugin "xcb" even though it was found</code></p>
</li>
<li>
<p dir="auto">Solution: Install the libxcb-cursor-dev, on ubuntu its <code>sudo apt install libxcb-cursor-dev</code></p>
</li>
</ul>
<br>
<h2 tabindex="-1" dir="auto">For Windows</h2>
<h3 tabindex="-1" dir="auto">Prerequisites</h3>
<ul dir="auto">
<li>cmake                             (<a href="https://cmake.org/download" rel="nofollow">https://cmake.org/download</a>)</li>
<li>Visual Studio <b>19</b>           (<a href="https://visualstudio.microsoft.com/de/vs/older-downloads" rel="nofollow">https://visualstudio.microsoft.com/de/vs/older-downloads</a>)</li>
<li>Python                            (<a href="https://www.python.org/downloads" rel="nofollow">https://www.python.org/downloads</a>)</li>
<li>Qt 6.5                            (<a href="https://www.qt.io/download-open-source" rel="nofollow">https://www.qt.io/download-open-source</a>)</li>
</ul>
<h3 tabindex="-1" dir="auto">Installation</h3>
<p dir="auto">To build Librum on windows, run the following commands in the Powershell:</p>
<br>
<ol dir="auto">
<li>Clone the repository.
<div dir="auto" data-snippet-clipboard-copy-content="git clone https://github.com/Librum-Reader/Librum.git --recursive"><pre>git clone https://github.com/Librum-Reader/Librum.git --recursive</pre></div>
</li>
<li>Step into the cloned project folder.

</li>
<li>Create the build folder and step into it.

</li>
<li>Run cmake.
<div dir="auto" data-snippet-clipboard-copy-content="cmake -DBUILD_TESTS=Off -DCMAKE_PREFIX_PATH=<path/to/qt> .."><pre>cmake -DBUILD_TESTS=Off -DCMAKE_PREFIX_PATH=<span>&lt;</span>path/to/qt<span>&gt;</span> ..</pre></div>
Set <code>CMAKE_PREFIX_PATH</code> to your Qt installation path. Installing Qt via the online installer usually installs it to <code>&lt;Drive&gt;\\Qt\\&lt;version&gt;\\msvc2019_64</code></li>
<li>Build the project
<div dir="auto" data-snippet-clipboard-copy-content="cmake --build . --config Release"><pre>cmake --build <span>.</span> --config Release</pre></div>
</li>
<li>Run the app

</li>
</ol>
<h3 tabindex="-1" dir="auto">Additional Info</h3>
<p dir="auto">Here are some things to keep in mind during the build process.
<br></p>
<ul dir="auto">
<li>Make sure to add cmake and the Qt binaries to the <code>PATH</code> environment variable</li>
<li>You need Visual Studio 2019, newer versions will <strong>not</strong> work</li>
<li>For the Qt installation, you <strong>only</strong> need to choose "MSVC 2019 64-bit", you can untick everything else to reduce the download size</li>
</ul>
<br>
<h2 tabindex="-1" dir="auto">For MacOS</h2>
<h3 tabindex="-1" dir="auto">Prerequisites</h3>
<ul dir="auto">
<li>cmake                             (<a href="https://cmake.org/download" rel="nofollow">https://cmake.org/download</a>)</li>
<li>make                              (<a href="http://ftp.gnu.org/gnu/make" rel="nofollow">http://ftp.gnu.org/gnu/make</a>)</li>
<li>g++                               (<a href="https://gcc.gnu.org/" rel="nofollow">https://gcc.gnu.org</a>)</li>
<li>python3                           (<a href="https://www.python.org/downloads" rel="nofollow">https://www.python.org/downloads</a>)</li>
<li>Qt 6.5                            (<a href="https://www.qt.io/download-open-source" rel="nofollow">https://www.qt.io/download-open-source</a>)</li>
</ul>
<h3 tabindex="-1" dir="auto">Installation</h3>
<p dir="auto">The installation is straight forward, just follow the steps below:</p>
<br>
<ol dir="auto">
<li>Clone the repository.
<div dir="auto" data-snippet-clipboard-copy-content="git clone https://github.com/Librum-Reader/Librum.git --recursive"><pre>git clone https://github.com/Librum-Reader/Librum.git --recursive</pre></div>
</li>
<li>Step into the cloned project folder.

</li>
<li>Create the build folder and step into it.
<div dir="auto" data-snippet-clipboard-copy-content="mkdir build-Release
cd build-Release"><pre>mkdir build-Release
<span>cd</span> build-Release</pre></div>
</li>
<li>Run cmake.
<div dir="auto" data-snippet-clipboard-copy-content="cmake -DCMAKE_INSTALL_PREFIX=/usr/local -DCMAKE_BUILD_TYPE=Release -DBUILD_TESTS=Off -DCMAKE_PREFIX_PATH=<path/to/Qt> .."><pre>cmake -DCMAKE_INSTALL_PREFIX=/usr/local -DCMAKE_BUILD_TYPE=Release -DBUILD_TESTS=Off -DCMAKE_PREFIX_PATH=<span>&lt;</span>path/to/Qt<span>&gt;</span> ..</pre></div>
Set <code>CMAKE_PREFIX_PATH</code> to your Qt installation path. Installing Qt via the online installer usually installs it to <code>/Users/&lt;name&gt;/Qt/&lt;version&gt;/macos</code></li>
<li>Build the project
<div dir="auto" data-snippet-clipboard-copy-content="cmake --build . -j $(nproc)"><pre>cmake --build <span>.</span> -j <span><span>$(</span>nproc<span>)</span></span></pre></div>
</li>
<li>Install Librum

</li>
</ol>
<br>
</article>
          </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[PostgREST: Providing HTML Content Using Htmx (316 pts)]]></title>
            <link>https://postgrest.org/en/stable/how-tos/providing-html-content-using-htmx.html</link>
            <guid>38687997</guid>
            <pubDate>Mon, 18 Dec 2023 20:47:36 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://postgrest.org/en/stable/how-tos/providing-html-content-using-htmx.html">https://postgrest.org/en/stable/how-tos/providing-html-content-using-htmx.html</a>, See on <a href="https://news.ycombinator.com/item?id=38687997">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="providing-html-content-using-htmx" itemprop="articleBody" role="main" itemscope="itemscope" itemtype="http://schema.org/Article">

<dl>
<dt>author</dt>
<dd><p><a href="https://github.com/laurenceisla">Laurence Isla</a></p>
</dd>
</dl>
<p>This how-to shows a way to return HTML content and use the <a href="https://htmx.org/">htmx library</a> to handle the AJAX requests.
Htmx expects an HTML response and uses it to replace an element inside the DOM (see the <a href="https://htmx.org/docs/#introduction">htmx introduction</a> in the docs).</p>
<p><img alt="../_images/htmx-demo.gif" src="https://postgrest.org/en/stable/_images/htmx-demo.gif"></p><div id="preparatory-configuration">
<h2>Preparatory Configuration<a href="#preparatory-configuration" title="Permalink to this headline"></a></h2>
<p>We will make a to-do app based on the <a href="https://postgrest.org/en/stable/tutorials/tut0.html#tut0"><span>Tutorial 0 - Get it Running</span></a>, so make sure to complete it before continuing.</p>
<p>To simplify things, we won’t be using authentication, so grant all permissions on the <code><span>todos</span></code> table to the <code><span>web_anon</span></code> user.</p>
<div><pre><span></span><span>grant</span><span> </span><span>all</span><span> </span><span>on</span><span> </span><span>api</span><span>.</span><span>todos</span><span> </span><span>to</span><span> </span><span>web_anon</span><span>;</span>
<span>grant</span><span> </span><span>usage</span><span>,</span><span> </span><span>select</span><span> </span><span>on</span><span> </span><span>sequence</span><span> </span><span>api</span><span>.</span><span>todos_id_seq</span><span> </span><span>to</span><span> </span><span>web_anon</span><span>;</span>
</pre></div>
<p>Next, add the <code><span>text/html</span></code> as a <a href="https://postgrest.org/en/stable/references/api/media_type_handlers.html#custom-media"><span>Media Type Handlers</span></a>. With this, PostgREST can identify the request made by your web browser (with the <code><span>Accept:</span> <span>text/html</span></code> header)
and return a raw HTML document file.</p>
<div><pre><span></span><span>create</span><span> </span><span>domain</span><span> </span><span>"text/html"</span><span> </span><span>as</span><span> </span><span>text</span><span>;</span>
</pre></div>
</div>
<div id="creating-an-html-response">
<h2>Creating an HTML Response<a href="#creating-an-html-response" title="Permalink to this headline"></a></h2>
<p>Let’s create a function that returns a basic HTML file, using <a href="https://v2.tailwindcss.com/">Tailwind CSS</a> for styling.</p>
<div><pre><span></span><span>create</span><span> </span><span>or</span><span> </span><span>replace</span><span> </span><span>function</span><span> </span><span>api</span><span>.</span><span>index</span><span>()</span><span> </span><span>returns</span><span> </span><span>"text/html"</span><span> </span><span>as</span><span> </span><span>$$</span>
<span>  </span><span>select</span><span> </span><span>$</span><span>html</span><span>$</span>
<span>    &lt;!DOCTYPE html&gt;</span>
<span>    &lt;html&gt;</span>
<span>    &lt;head&gt;</span>
<span>      &lt;meta charset="utf-8"&gt;</span>
<span>      &lt;meta name="viewport" content="width=device-width, initial-scale=1"&gt;</span>
<span>      &lt;title&gt;PostgREST + HTMX To-Do List&lt;/title&gt;</span>
<span>      &lt;!-- Tailwind for CSS styling --&gt;</span>
<span>      &lt;link href="https://unpkg.com/tailwindcss@2.2.19/dist/tailwind.min.css" rel="stylesheet"&gt;</span>
<span>    &lt;/head&gt;</span>
<span>    &lt;body class="bg-gray-900"&gt;</span>
<span>      &lt;div class="flex justify-center"&gt;</span>
<span>        &lt;div class="max-w-lg mt-5 p-6 bg-gray-800 border border-gray-800 rounded-lg shadow-xl"&gt;</span>
<span>          &lt;h5 class="mb-3 text-2xl font-bold tracking-tight text-white"&gt;PostgREST + HTMX To-Do List&lt;/h5&gt;</span>
<span>        &lt;/div&gt;</span>
<span>      &lt;/div&gt;</span>
<span>    &lt;/body&gt;</span>
<span>    &lt;/html&gt;</span>
<span>  $</span><span>html</span><span>$</span><span>;</span>
<span>$$</span><span> </span><span>language</span><span> </span><span>sql</span><span>;</span>
</pre></div>
<p>The web browser will open the web page at <code><span>http://localhost:3000/rpc/index</span></code>.</p>
<p><img alt="../_images/htmx-simple.jpg" src="https://postgrest.org/en/stable/_images/htmx-simple.jpg">
</p></div>
<div id="listing-and-creating-to-dos">
<h2>Listing and Creating To-Dos<a href="#listing-and-creating-to-dos" title="Permalink to this headline"></a></h2>
<p>Now, let’s show a list of the to-dos already inserted in the database.</p>
<div><pre><span></span><span>create</span><span> </span><span>or</span><span> </span><span>replace</span><span> </span><span>function</span><span> </span><span>api</span><span>.</span><span>html_todo</span><span>(</span><span>api</span><span>.</span><span>todos</span><span>)</span><span> </span><span>returns</span><span> </span><span>text</span><span> </span><span>as</span><span> </span><span>$$</span>
<span>  </span><span>select</span><span> </span><span>format</span><span>(</span><span>$</span><span>html</span><span>$</span>
<span>    &lt;li class="py-3"&gt;</span>
<span>      &lt;span class="ml-2 %2$s"&gt;</span>
<span>        %3$s</span>
<span>      &lt;/span&gt;</span>
<span>    &lt;/li&gt;</span>
<span>    $</span><span>html</span><span>$</span><span>,</span>
<span>    </span><span>$1</span><span>.</span><span>id</span><span>,</span>
<span>    </span><span>case</span><span> </span><span>when</span><span> </span><span>$1</span><span>.</span><span>done</span><span> </span><span>then</span><span> </span><span>'line-through text-gray-400'</span><span> </span><span>else</span><span> </span><span>''</span><span> </span><span>end</span><span>,</span>
<span>    </span><span>$1</span><span>.</span><span>task</span>
<span>  </span><span>);</span>
<span>$$</span><span> </span><span>language</span><span> </span><span>sql</span><span> </span><span>stable</span><span>;</span>

<span>create</span><span> </span><span>or</span><span> </span><span>replace</span><span> </span><span>function</span><span> </span><span>api</span><span>.</span><span>html_all_todos</span><span>()</span><span> </span><span>returns</span><span> </span><span>text</span><span> </span><span>as</span><span> </span><span>$$</span>
<span>  </span><span>select</span><span> </span><span>coalesce</span><span>(</span>
<span>    </span><span>'&lt;ul id="todo-list" role="list" class="divide-y divide-gray-700 text-gray-100"&gt;'</span>
<span>      </span><span>||</span><span> </span><span>string_agg</span><span>(</span><span>api</span><span>.</span><span>html_todo</span><span>(</span><span>t</span><span>),</span><span> </span><span>''</span><span> </span><span>order</span><span> </span><span>by</span><span> </span><span>t</span><span>.</span><span>id</span><span>)</span><span> </span><span>||</span>
<span>    </span><span>'&lt;/ul&gt;'</span><span>,</span>
<span>    </span><span>'&lt;p class="text-gray-100"&gt;There is nothing else to do.&lt;/p&gt;'</span>
<span>  </span><span>)</span>
<span>  </span><span>from</span><span> </span><span>api</span><span>.</span><span>todos</span><span> </span><span>t</span><span>;</span>
<span>$$</span><span> </span><span>language</span><span> </span><span>sql</span><span>;</span>
</pre></div>
<p>These two functions are used to build the to-do list template. We won’t use them as PostgREST endpoints.</p>
<ul>
<li><p>The <code><span>api.html_todo</span></code> function uses the table <code><span>api.todos</span></code> as a parameter and formats each item into a list element <code><span>&lt;li&gt;</span></code>.
The PostgreSQL <a href="https://www.postgresql.org/docs/current/functions-string.html#FUNCTIONS-STRING-FORMAT">format</a> is useful to that end.
It replaces the values according to the position in the template, e.g. <code><span>%1$s</span></code> will be replaced with the value of <code><span>$1.id</span></code> (the first parameter).</p></li>
<li><p>The <code><span>api.html_all_todos</span></code> function returns the <code><span>&lt;ul&gt;</span></code> wrapper for all the list elements.
It uses <a href="https://www.postgresql.org/docs/current/functions-aggregate.html">string_arg</a> to concatenate all the to-dos in a single text value.
It also returns an alternative message, instead of a list, when the <code><span>api.todos</span></code> table is empty.</p></li>
</ul>
<p>Next, let’s add an endpoint to register a to-do in the database and modify the <code><span>/rpc/index</span></code> page accordingly.</p>
<div><pre><span></span><span>create</span><span> </span><span>or</span><span> </span><span>replace</span><span> </span><span>function</span><span> </span><span>api</span><span>.</span><span>add_todo</span><span>(</span><span>_task</span><span> </span><span>text</span><span>)</span><span> </span><span>returns</span><span> </span><span>"text/html"</span><span> </span><span>as</span><span> </span><span>$$</span>
<span>  </span><span>insert</span><span> </span><span>into</span><span> </span><span>api</span><span>.</span><span>todos</span><span>(</span><span>task</span><span>)</span><span> </span><span>values</span><span> </span><span>(</span><span>_task</span><span>);</span>
<span>  </span><span>select</span><span> </span><span>api</span><span>.</span><span>html_all_todos</span><span>();</span>
<span>$$</span><span> </span><span>language</span><span> </span><span>sql</span><span>;</span>

<span>create</span><span> </span><span>or</span><span> </span><span>replace</span><span> </span><span>function</span><span> </span><span>api</span><span>.</span><span>index</span><span>()</span><span> </span><span>returns</span><span> </span><span>"text/html"</span><span> </span><span>as</span><span> </span><span>$$</span>
<span>select</span><span> </span><span>$</span><span>html</span><span>$</span>
<span>  &lt;!DOCTYPE html&gt;</span>
<span>  &lt;html&gt;</span>
<span>  &lt;head&gt;</span>
<span>    &lt;meta charset="utf-8"&gt;</span>
<span>    &lt;meta name="viewport" content="width=device-width, initial-scale=1"&gt;</span>
<span>    &lt;title&gt;PostgREST + HTMX To-Do List&lt;/title&gt;</span>
<span>    &lt;!-- Tailwind for CSS styling --&gt;</span>
<span>    &lt;link href="https://unpkg.com/tailwindcss@2.2.19/dist/tailwind.min.css" rel="stylesheet"&gt;</span>
<span>    &lt;!-- htmx for AJAX requests --&gt;</span>
<span>    &lt;script src="https://unpkg.com/htmx.org"&gt;&lt;/script&gt;</span>
<span>  &lt;/head&gt;</span>
<span>  &lt;body class="bg-gray-900"</span>
<span>        hx-headers='{"Accept": "text/html"}'&gt;</span>
<span>    &lt;div class="flex justify-center"&gt;</span>
<span>      &lt;div class="max-w-lg mt-5 p-6 bg-gray-800 border border-gray-800 rounded-lg shadow-xl"&gt;</span>
<span>        &lt;h5 class="mb-3 text-2xl font-bold tracking-tight text-white"&gt;PostgREST + HTMX To-Do List&lt;/h5&gt;</span>
<span>        &lt;form hx-post="/rpc/add_todo"</span>
<span>              hx-target="#todo-list-area"</span>
<span>              hx-trigger="submit"</span>
<span>              hx-on="htmx:afterRequest: this.reset()"&gt;</span>
<span>          &lt;input class="bg-gray-50 border text-sm rounded-lg block w-full p-2.5 mb-3 bg-gray-700 border-gray-600 placeholder-gray-400 text-white focus:ring-blue-500 focus:border-blue-500"</span>
<span>                 type="text" name="_task" placeholder="Add a todo..."&gt;</span>
<span>        &lt;/form&gt;</span>
<span>        &lt;div id="todo-list-area"&gt;</span>
<span>          $</span><span>html</span><span>$</span>
<span>            </span><span>||</span><span> </span><span>api</span><span>.</span><span>html_all_todos</span><span>()</span><span> </span><span>||</span>
<span>          </span><span>$</span><span>html</span><span>$</span>
<span>        &lt;div&gt;</span>
<span>      &lt;/div&gt;</span>
<span>    &lt;/div&gt;</span>
<span>  &lt;/body&gt;</span>
<span>  &lt;/html&gt;</span>
<span>  $</span><span>html</span><span>$</span><span>;</span>
<span>$$</span><span> </span><span>language</span><span> </span><span>sql</span><span>;</span>
</pre></div>
<ul>
<li><p>The <code><span>/rpc/add_todo</span></code> endpoint allows us to add a new to-do using the <code><span>_task</span></code> parameter and returns an <code><span>html</span></code> with all the to-dos in the database.</p></li>
<li><p>The <code><span>/rpc/index</span></code> now adds the <code><span>hx-headers='{"Accept":</span> <span>"text/html"}'</span></code> tag to the <code><span>&lt;body&gt;</span></code>.
This will make sure that all htmx elements inside the body send this header, otherwise PostgREST won’t recognize it as HTML.</p>
<p>There is also a <code><span>&lt;form&gt;</span></code> element that uses the htmx library. Let’s break it down:</p>
<ul>
<li><p><code><span>hx-post="/rpc/add_todo"</span></code>: sends an AJAX POST request to the <code><span>/rpc/add_todo</span></code> endpoint, with the value of the <code><span>_task</span></code> from the <code><span>&lt;input&gt;</span></code> element.</p></li>
<li><p><code><span>hx-target="#todo-list-area"</span></code>: the HTML content returned from the request will go inside <code><span>&lt;div</span> <span>id="todo-list-area"&gt;&lt;/div&gt;</span></code> (which is the list of to-dos).</p></li>
<li><p><code><span>hx-trigger="submit"</span></code>: htmx will do this request when submitting the form (by pressing enter while inside the <code><span>&lt;input&gt;</span></code>).</p></li>
<li><p><code><span>hx-on="htmx:afterRequest:</span> <span>this.reset()"&gt;</span></code>: this is a Javascript command that clears the form <a href="https://htmx.org/events/#htmx:afterRequest">after the request is done</a>.</p></li>
</ul>
</li>
</ul>
<p>With this, the <code><span>http://localhost:3000/rpc/index</span></code> page lists all the todos and adds new ones by submitting tasks in the input element.
Don’t forget to refresh the <a href="https://postgrest.org/en/stable/references/schema_cache.html#schema-reloading"><span>schema cache</span></a>.</p>
<p><img alt="../_images/htmx-insert.gif" src="https://postgrest.org/en/stable/_images/htmx-insert.gif">
</p></div>
<div id="editing-and-deleting-to-dos">
<h2>Editing and Deleting To-Dos<a href="#editing-and-deleting-to-dos" title="Permalink to this headline"></a></h2>
<p>Now, let’s modify <code><span>api.html_todo</span></code> and make it more functional.</p>
<div><pre><span></span><span>create</span><span> </span><span>or</span><span> </span><span>replace</span><span> </span><span>function</span><span> </span><span>api</span><span>.</span><span>html_todo</span><span>(</span><span>api</span><span>.</span><span>todos</span><span>)</span><span> </span><span>returns</span><span> </span><span>text</span><span> </span><span>as</span><span> </span><span>$$</span>
<span>select</span><span> </span><span>format</span><span>(</span><span>$</span><span>html</span><span>$</span>
<span>&lt;li class="py-3"&gt;</span>
<span>  &lt;div class="flex justify-between items-center"&gt;</span>
<span>    &lt;div id="todo-edit-area-%1$s" class="pr-5"&gt;</span>
<span>      &lt;form id="edit-task-state-%1$s"</span>
<span>            hx-post="/rpc/change_todo_state"</span>
<span>            hx-vals='{"_id": %1$s, "_done": %4$s}'</span>
<span>            hx-target="#todo-list-area"</span>
<span>            hx-trigger="click"&gt;</span>
<span>        &lt;span class="ml-2 %2$s cursor-pointer"&gt;</span>
<span>          %3$s</span>
<span>        &lt;/span&gt;</span>
<span>      &lt;/form&gt;</span>
<span>    &lt;/div&gt;</span>
<span>    &lt;div&gt;</span>
<span>      &lt;button class="p-1.5 rounded-full hover:bg-gray-700 focus:ring-gray-800"</span>
<span>              hx-get="/rpc/html_editable_task"</span>
<span>              hx-vals='{"_id": "%1$s"}'</span>
<span>              hx-target="#todo-edit-area-%1$s"</span>
<span>              hx-trigger="click"&gt;</span>
<span>        &lt;svg class="w-4 h-4 text-blue-300" aria-hidden="true" xmlns="http://www.w3.org/2000/svg" fill="currentColor" viewBox="0 0 20 18"&gt;</span>
<span>          &lt;path d="M12.687 14.408a3.01 3.01 0 0 1-1.533.821l-3.566.713a3 3 0 0 1-3.53-3.53l.713-3.566a3.01 3.01 0 0 1 .821-1.533L10.905 2H2.167A2.169 2.169 0 0 0 0 4.167v11.666A2.169 2.169 0 0 0 2.167 18h11.666A2.169 2.169 0 0 0 16 15.833V11.1l-3.313 3.308Zm5.53-9.065.546-.546a2.518 2.518 0 0 0 0-3.56 2.576 2.576 0 0 0-3.559 0l-.547.547 3.56 3.56Z"/&gt;</span>
<span>          &lt;path d="M13.243 3.2 7.359 9.081a.5.5 0 0 0-.136.256L6.51 12.9a.5.5 0 0 0 .59.59l3.566-.713a.5.5 0 0 0 .255-.136L16.8 6.757 13.243 3.2Z"/&gt;</span>
<span>        &lt;/svg&gt;</span>
<span>      &lt;/button&gt;</span>
<span>      &lt;button class="p-1.5 rounded-full hover:bg-gray-700 focus:ring-gray-800"</span>
<span>              hx-post="/rpc/delete_todo"</span>
<span>              hx-vals='{"_id": %1$s}'</span>
<span>              hx-target="#todo-list-area"</span>
<span>              hx-trigger="click"&gt;</span>
<span>        &lt;svg class="w-4 h-4 text-red-400" aria-hidden="true" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 18 20"&gt;</span>
<span>          &lt;path stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M1 5h16M7 8v8m4-8v8M7 1h4a1 1 0 0 1 1 1v3H6V2a1 1 0 0 1 1-1ZM3 5h12v13a1 1 0 0 1-1 1H4a1 1 0 0 1-1-1V5Z"/&gt;</span>
<span>        &lt;/svg&gt;</span>
<span>      &lt;/button&gt;</span>
<span>    &lt;/div&gt;</span>
<span>  &lt;/div&gt;</span>
<span>&lt;/li&gt;</span>
<span>$</span><span>html</span><span>$</span><span>,</span>
<span>  </span><span>$1</span><span>.</span><span>id</span><span>,</span>
<span>  </span><span>case</span><span> </span><span>when</span><span> </span><span>$1</span><span>.</span><span>done</span><span> </span><span>then</span><span> </span><span>'line-through text-gray-400'</span><span> </span><span>else</span><span> </span><span>''</span><span> </span><span>end</span><span>,</span>
<span>  </span><span>$1</span><span>.</span><span>task</span><span>,</span>
<span>  </span><span>(</span><span>not</span><span> </span><span>$1</span><span>.</span><span>done</span><span>)</span><span>::</span><span>text</span>
<span>);</span>
<span>$$</span><span> </span><span>language</span><span> </span><span>sql</span><span> </span><span>stable</span><span>;</span>
</pre></div>
<p>Let’s deconstruct the new htmx features added:</p>
<ul>
<li><p>The <code><span>&lt;form&gt;</span></code> element is configured as follows:</p>
<ul>
<li><p><code><span>hx-post="/rpc/change_todo_state"</span></code>: does an AJAX POST request to that endpoint. It will toggle the <code><span>done</span></code> state of the to-do.</p></li>
<li><p><code><span>hx-vals='{"_id":</span> <span>%1$s,</span> <span>"_done":</span> <span>%4$s}'</span></code>: adds the parameters to the request.
This is an alternative to using hidden inputs inside the <code><span>&lt;form&gt;</span></code>.</p></li>
<li><p><code><span>hx-trigger="click"</span></code>: htmx does the request after clicking on the element.</p></li>
</ul>
</li>
<li><p>For the first <code><span>&lt;button&gt;</span></code>:</p>
<ul>
<li><p><code><span>hx-get="/rpc/html_editable_task"</span></code>: it does an AJAX GET request to that endpoint.
It returns an HTML with an input that will allow us to edit the task.</p></li>
<li><p><code><span>hx-target="#todo-edit-area"</span></code>: the returned HTML will replace the element with this id.
In this case, this replaces an individual task, not the whole list.</p></li>
<li><p><code><span>hx-vals='{"id":</span> <span>"eq.%1$s"}'</span></code>: adds the query parameters to the GET request.
Note that this needs the <code><span>eq.</span></code> operator because it represents a table column not a function parameter.</p></li>
</ul>
</li>
<li><p>For the second <code><span>&lt;button&gt;</span></code>:</p>
<ul>
<li><p><code><span>hx-post="/rpc/delete_todo"</span></code>: this post request will delete the corresponding to-do.</p></li>
</ul>
</li>
</ul>
<p>Clicking on the first button will enable the task editing.
That’s why we create the <code><span>api.html_editable_task</span></code> function as an endpoint:</p>
<div><pre><span></span><span>create</span><span> </span><span>or</span><span> </span><span>replace</span><span> </span><span>function</span><span> </span><span>api</span><span>.</span><span>html_editable_task</span><span>(</span><span>_id</span><span> </span><span>int</span><span>)</span><span> </span><span>returns</span><span> </span><span>"text/html"</span><span> </span><span>as</span><span> </span><span>$$</span>
<span>select</span><span> </span><span>format</span><span> </span><span>(</span><span>$</span><span>html</span><span>$</span>
<span>&lt;form id="edit-task-%1$s"</span>
<span>      hx-post="/rpc/change_todo_task"</span>
<span>      hx-headers='{"Accept": "text/html"}'</span>
<span>      hx-vals='{"_id": %1$s}'</span>
<span>      hx-target="#todo-list-area"</span>
<span>      hx-trigger="submit,focusout"&gt;</span>
<span>  &lt;input class="bg-gray-50 border text-sm rounded-lg block w-full p-2.5 bg-gray-700 border-gray-600 text-white focus:ring-blue-500 focus:border-blue-500"</span>
<span>         id="task-%1$s" type="text" name="_task" value="%2$s" autofocus&gt;</span>
<span>&lt;/form&gt;</span>
<span>$</span><span>html</span><span>$</span><span>,</span>
<span>  </span><span>id</span><span>,</span>
<span>  </span><span>task</span>
<span>)</span>
<span>from</span><span> </span><span>api</span><span>.</span><span>todos</span>
<span>where</span><span> </span><span>id</span><span> </span><span>=</span><span> </span><span>_id</span><span>;</span>
<span>$$</span><span> </span><span>language</span><span> </span><span>sql</span><span>;</span>
</pre></div>
<p>In this example, this will return an input field that allows us to edit the corresponding to-do task.</p>
<p>Finally, let’s add the endpoints that will modify and delete the to-dos in the database.</p>
<div><pre><span></span><span>create</span><span> </span><span>or</span><span> </span><span>replace</span><span> </span><span>function</span><span> </span><span>api</span><span>.</span><span>change_todo_state</span><span>(</span><span>_id</span><span> </span><span>int</span><span>,</span><span> </span><span>_done</span><span> </span><span>boolean</span><span>)</span><span> </span><span>returns</span><span> </span><span>"text/html"</span><span> </span><span>as</span><span> </span><span>$$</span>
<span>  </span><span>update</span><span> </span><span>api</span><span>.</span><span>todos</span><span> </span><span>set</span><span> </span><span>done</span><span> </span><span>=</span><span> </span><span>_done</span><span> </span><span>where</span><span> </span><span>id</span><span> </span><span>=</span><span> </span><span>_id</span><span>;</span>
<span>  </span><span>select</span><span> </span><span>api</span><span>.</span><span>html_all_todos</span><span>();</span>
<span>$$</span><span> </span><span>language</span><span> </span><span>sql</span><span>;</span>

<span>create</span><span> </span><span>or</span><span> </span><span>replace</span><span> </span><span>function</span><span> </span><span>api</span><span>.</span><span>change_todo_task</span><span>(</span><span>_id</span><span> </span><span>int</span><span>,</span><span> </span><span>_task</span><span> </span><span>text</span><span>)</span><span> </span><span>returns</span><span> </span><span>"text/html"</span><span> </span><span>as</span><span> </span><span>$$</span>
<span>  </span><span>update</span><span> </span><span>api</span><span>.</span><span>todos</span><span> </span><span>set</span><span> </span><span>task</span><span> </span><span>=</span><span> </span><span>_task</span><span> </span><span>where</span><span> </span><span>id</span><span> </span><span>=</span><span> </span><span>_id</span><span>;</span>
<span>  </span><span>select</span><span> </span><span>api</span><span>.</span><span>html_all_todos</span><span>();</span>
<span>$$</span><span> </span><span>language</span><span> </span><span>sql</span><span>;</span>

<span>create</span><span> </span><span>or</span><span> </span><span>replace</span><span> </span><span>function</span><span> </span><span>api</span><span>.</span><span>delete_todo</span><span>(</span><span>_id</span><span> </span><span>int</span><span>)</span><span> </span><span>returns</span><span> </span><span>"text/html"</span><span> </span><span>as</span><span> </span><span>$$</span>
<span>  </span><span>delete</span><span> </span><span>from</span><span> </span><span>api</span><span>.</span><span>todos</span><span> </span><span>where</span><span> </span><span>id</span><span> </span><span>=</span><span> </span><span>_id</span><span>;</span>
<span>  </span><span>select</span><span> </span><span>api</span><span>.</span><span>html_all_todos</span><span>();</span>
<span>$$</span><span> </span><span>language</span><span> </span><span>sql</span><span>;</span>
</pre></div>
<p>All of those functions return an HTML list of to-dos that will replace the outdated one:</p>
<ul>
<li><p>The <code><span>api.change_todo_state</span></code> function updates the <code><span>done</span></code> column using the <code><span>_id</span></code> and the <code><span>_done</span></code> values from the request.</p></li>
<li><p>The <code><span>api.delete_todo</span></code> function deletes a to-do using the <code><span>_id</span></code> value from the request.</p></li>
<li><p>The <code><span>api.change_todo_task</span></code> function modifies the <code><span>task</span></code> column  using the <code><span>_id</span></code> and the <code><span>_task</span></code> value from the request.</p></li>
</ul>
<p>After refreshing the <a href="https://postgrest.org/en/stable/references/schema_cache.html#schema-reloading"><span>schema cache</span></a>, the page at <code><span>http://localhost:3000/rpc/index</span></code> will allow us to edit, delete and complete any to-do.</p>
<p><img alt="../_images/htmx-edit-delete.gif" src="https://postgrest.org/en/stable/_images/htmx-edit-delete.gif"></p><p>With that, we completed the to-do list functionality.</p>
</div>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[3Blue1Brown Calculus Blog Series (283 pts)]]></title>
            <link>https://www.3blue1brown.com/topics/calculus</link>
            <guid>38687809</guid>
            <pubDate>Mon, 18 Dec 2023 20:34:18 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.3blue1brown.com/topics/calculus">https://www.3blue1brown.com/topics/calculus</a>, See on <a href="https://news.ycombinator.com/item?id=38687809">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><a data-active="false" data-mini="false" data-reverse="false" href="https://www.3blue1brown.com/lessons/essence-of-calculus"><div><p><img src="https://img.youtube.com/vi/WUvTyaaNkzM/maxresdefault.jpg" alt=""></p></div><p><span><span>The Essence of Calculus</span></span><span>An overview of what calculus is all about, with an emphasis on making it seem like something students could discover for themselves.  The central example is that of rediscovering the formula for a circle's area, and how this is an isolated instance of the fundamental theorem of calculus</span><span><span data-mini="false">Chapter 1</span><span data-mini="false"><i></i></span><span>Apr 28, 2017</span> </span></p></a><a data-active="false" data-mini="false" data-reverse="false" href="https://www.3blue1brown.com/lessons/derivatives"><div><p><img src="https://img.youtube.com/vi/9vKqVkMQHKk/maxresdefault.jpg" alt=""></p></div><p><span><span>The paradox of the derivative</span></span><span>An introduction to what a derivative is, and how it formalizes an otherwise paradoxical idea.</span><span><span data-mini="false">Chapter 2</span><span data-mini="false"><i></i></span><span>Apr 29, 2017</span> </span></p></a><a data-active="false" data-mini="false" data-reverse="false" href="https://www.3blue1brown.com/lessons/derivatives-power-rule"><div><p><img src="https://img.youtube.com/vi/S0_qX4VJhMQ/maxresdefault.jpg" alt=""></p></div><p><span><span>Power Rule through geometry</span></span><span>Introduction to the derivatives of polynomial terms thought about geometrically and intuitively. The goal is for these formulas to feel like something the student could have discovered, rather than something to be memorized.</span><span><span data-mini="false">Chapter 3</span><span data-mini="false"><i></i></span><span>Apr 30, 2017</span> </span></p></a><a data-active="false" data-mini="false" data-reverse="false" href="https://www.3blue1brown.com/lessons/derivatives-trig-functions"><div><p><img src="https://www.3blue1brown.com/content/lessons/2017/derivatives-trig-functions/thumbnail.png" alt=""></p></div><p><span><span>Trig Derivatives through geometry</span></span><span>Introduction to the derivatives trigonometric functions thought about geometrically and intuitively.</span><span><span data-mini="false">Chapter 4</span><span data-mini="false"><i></i></span><span>Apr 30, 2017</span> </span></p></a><a data-active="false" data-mini="false" data-reverse="false" href="https://www.3blue1brown.com/lessons/chain-rule-and-product-rule"><div><p><img src="https://img.youtube.com/vi/YG15m2VwSjA/maxresdefault.jpg" alt=""></p></div><p><span><span>Visualizing the chain rule and product rule</span></span><span>The product rule and chain rule in calculus can feel like they were pulled out of thin air, but is there an intuitive way to think about them?</span><span><span data-mini="false">Chapter 5</span><span data-mini="false"><i></i></span><span>May 1, 2017</span> </span></p></a><a data-active="false" data-mini="false" data-reverse="false" href="https://www.3blue1brown.com/lessons/eulers-number"><div><p><img src="https://img.youtube.com/vi/m2MIpDrF7Es/maxresdefault.jpg" alt=""></p></div><p><span><span>What's so special about Euler's number e?</span></span><span>What is the derivative of a^x?  Why is e^x its own derivative?  This video shows how to think about the rule for differentiating exponential functions.</span><span><span data-mini="false">Chapter 6</span><span data-mini="false"><i></i></span><span>May 2, 2017</span> </span></p></a><a data-active="false" data-mini="false" data-reverse="false" href="https://www.3blue1brown.com/lessons/implicit-differentiation"><div><p><img src="https://img.youtube.com/vi/qb40J4N1fa4/maxresdefault.jpg" alt=""></p></div><p><span><span>Implicit differentiation, what's going on here?</span></span><span>How to think about implicit differentiation in terms of functions with multiple inputs, and tiny nudges to those inputs.</span><span><span data-mini="false">Chapter 7</span><span data-mini="false"><i></i></span><span>May 3, 2017</span> </span></p></a><a data-active="false" data-mini="false" data-reverse="false" href="https://www.3blue1brown.com/lessons/limits"><div><p><img src="https://img.youtube.com/vi/kfF40MiS7zA/maxresdefault.jpg" alt=""></p></div><p><span><span>Limits and the definition of derivatives</span></span><span>What are limits? How are they defined? How are they used to define the derivative?</span><span><span data-mini="false">Chapter 8</span><span data-mini="false"><i></i></span><span>May 4, 2017</span> </span></p></a><a data-active="false" data-mini="false" data-reverse="false" href="https://www.3blue1brown.com/lessons/epsilon-delta"><div><p><img src="https://www.3blue1brown.com/content/lessons/2017/epsilon-delta/thumbnail.png" alt=""></p></div><p><span><span>(ε, δ) "epsilon delta" definitions of limits</span></span><span>How does (ε, δ) "epsilon delta" help us formalize what exactly it means for one value to approach another?</span><span><span data-mini="false">Chapter 9</span><span data-mini="false"><i></i></span><span>May 4, 2017</span> </span></p></a><a data-active="false" data-mini="false" data-reverse="false" href="https://www.3blue1brown.com/lessons/l-hopitals-rule"><div><p><img src="https://www.3blue1brown.com/content/lessons/2017/l-hopitals-rule/thumbnail.png" alt=""></p></div><p><span><span>L'Hôpital's rule</span></span><span>What is L'Hopital's rule and how does it help us evaluate limits?</span><span><span data-mini="false">Chapter 10</span><span data-mini="false"><i></i></span><span>May 4, 2017</span> </span></p></a><a data-active="false" data-mini="false" data-reverse="false" href="https://www.3blue1brown.com/lessons/integration"><div><p><img src="https://img.youtube.com/vi/rfG8ce4nNh0/maxresdefault.jpg" alt=""></p></div><p><span><span>Integration and the fundamental theorem of calculus</span></span><span>What is integration?  Why is it computed as the opposite of differentiation?  What is the fundamental theorem of calculus?</span><span><span data-mini="false">Chapter 11</span><span data-mini="false"><i></i></span><span>May 5, 2017</span> </span></p></a><a data-active="false" data-mini="false" data-reverse="false" href="https://www.3blue1brown.com/lessons/area-and-slope"><div><p><img src="https://www.3blue1brown.com/content/lessons/2017/area-and-slope/thumbnail-2880-1620.png" alt=""></p></div><p><span><span>What does area have to do with slope?</span></span><span>Derivatives are about slope, and integration is about area. These ideas seem completely different, so why are they inverses?</span><span><span data-mini="false">Chapter 12</span><span data-mini="false"><i></i></span><span>May 6, 2017</span> </span></p></a><a data-active="false" data-mini="false" data-reverse="false" href="https://www.3blue1brown.com/lessons/higher-order-derivatives"><div><p><img src="https://img.youtube.com/vi/BLkz5LGWihw/maxresdefault.jpg" alt=""></p></div><p><span><span>Higher order derivatives</span></span><span>What is the second derivative?  Third derivative?  How do you think about these?</span><span><span data-mini="false">Chapter 13</span><span data-mini="false"><i></i></span><span>May 7, 2017</span> </span></p></a><a data-active="false" data-mini="false" data-reverse="false" href="https://www.3blue1brown.com/lessons/taylor-series"><div><p><img src="https://www.3blue1brown.com/content/lessons/2017/taylor-series/thumbnail.png" alt=""></p></div><p><span><span>Taylor series</span></span><span>Taylor series are extremely useful in engineering and math, but what are they?  This video shows why they're useful, and how to make sense of the formula.</span><span><span data-mini="false">Chapter 14</span><span data-mini="false"><i></i></span><span>May 7, 2017</span> </span></p></a><a data-active="false" data-mini="false" data-reverse="false" href="https://www.3blue1brown.com/lessons/taylor-series-geometric-view"><div><p><img src="https://img.youtube.com/vi/3d6DsjIBzJ4/maxresdefault.jpg" alt=""></p></div><p><span><span>Taylor series (geometric view)</span></span><span>A different perspective of Taylor Series that's related to the fundamental theorem of calculus.</span><span><span data-mini="false">Chapter 15</span><span data-mini="false"><i></i></span><span>May 7, 2017</span> </span></p></a><a data-active="false" data-mini="false" data-reverse="false" href="https://www.3blue1brown.com/lessons/derivatives-and-transforms"><div><p><img src="https://img.youtube.com/vi/CfW845LNObM/maxresdefault.jpg" alt=""></p></div><p><span><span>The other way to visualize derivatives</span></span><span>A visual for derivatives which generalizes more nicely to topics beyond calculus. Thinking of a function as a transformation, the derivative measure how much that function locally stretches or squishes a given region.</span><span><span data-mini="false">Chapter 16</span><span data-mini="false"><i></i></span><span>May 19, 2018</span> </span></p></a></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[PostgreSQL 16 Bi-Directional Logical Replication (123 pts)]]></title>
            <link>https://www.highgo.ca/2023/12/18/new-in-postgresql-16-bi-directional-logical-replication/</link>
            <guid>38687357</guid>
            <pubDate>Mon, 18 Dec 2023 19:58:02 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.highgo.ca/2023/12/18/new-in-postgresql-16-bi-directional-logical-replication/">https://www.highgo.ca/2023/12/18/new-in-postgresql-16-bi-directional-logical-replication/</a>, See on <a href="https://news.ycombinator.com/item?id=38687357">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
                        
<h2>Introduction</h2>



<p>In this blog, we’ll be going over some more advanced topics new in Postgres 16. Having some experience with Linux, Postgres, and SQL is necessary as we’ll not only be going over these new features but also how to implement them. This blog was written using PostgreSQL 16 (Development Version) running on Ubuntu 23.04. First I’ll go over some background and a brief introduction to what Bi-Directional Replication is, and why it’s important, then finish off with how we implement Bi-Directional Logical Replication.</p>



<h2>Background</h2>



<p>Before we can start learning about Bi-Directional Logical Replication we first have to understand Logical Replication.</p>



<h3>Basics of Logical Replication</h3>



<p>Logical replication has been supported since PostgreSQL 10 and has received numerous updates and improvements in the following years. Logical Replication is the process of copying (ie. replicating) data objects represented as their changes. This way we can copy only specific changes of objects like tables rather than whole databases, and stream these changes across different platforms and versions. This is all in contrast to Physical replication which uses exact block addresses and as a result, is limited to only copying entire databases and cannot stream across platforms or versions since the data must match in both.</p>



<figure><img decoding="async" loading="lazy" width="701" height="231" src="https://www.highgo.ca/wp-content/uploads/2023/12/logical-replication-1.png" alt="" srcset="https://www.highgo.ca/wp-content/uploads/2023/12/logical-replication-1.png 701w, https://www.highgo.ca/wp-content/uploads/2023/12/logical-replication-1-300x99.png 300w" sizes="(max-width: 701px) 100vw, 701px"><figcaption>Fig 1: Logical Replication Architecture Overview</figcaption></figure>



<p>Logical replication also introduces two very important elements necessary for understanding its Bi-Directional counterpart. These are Publishers and Subscribers, you can think of them as a leader node (Publisher) and a follower node (Subscriber). The Publisher will gather up its recent changes and send them as an ordered list of commands to the Subscriber. Once received the Subscriber takes this series of commands and applies it to its data. If both databases started with the same data, then the Subscriber will be up-to-date with the Publisher.</p>



<h3>Bi-Directional Replication</h3>



<p>Now that we understand what Logical Replication is, what is Bi-Directional Replication doing differently? In short, Bi-Directional Logical Replication is when all nodes in the replication are both Publisher and Subscriber. Each database can now handle read and write requests, and all the changes will be streamed to one another. This is the Bi-Directional aspect, as rather than changes flowing in one direction as before, they flow in both directions.</p>



<figure><img decoding="async" loading="lazy" src="https://www.highgo.ca/wp-content/uploads/2023/12/unnamed.png" alt="" width="434" height="218" srcset="https://www.highgo.ca/wp-content/uploads/2023/12/unnamed.png 401w, https://www.highgo.ca/wp-content/uploads/2023/12/unnamed-300x150.png 300w" sizes="(max-width: 434px) 100vw, 434px"><figcaption>Fig. 2: Bi-Directional Replication Architecture Overview</figcaption></figure>



<p>What Postgres 16 adds is a new parameter to the WITH statement that filters out replication from certain nodes. Bi-Directional Logical Replication uses this parameter WITH(ORIGIN = NONE), this filters out all replication from connections with origins that are not NONE. Essentially, this only allows newly added data to be replicated, you can probably see why this is the case. If one database inserts new data and replicates it to a second, this second database will replicate the data also inserting it thus triggering another replication to the original database. We quickly get an infinite loop of replication, which is why this option is necessary to keep everything finite.</p>



<h4>Benefits</h4>



<p>The main benefit of Bi-Direction Logical Replication is that it allows more availability for both read and write requests since we have two Primary nodes. This can be extremely beneficial for a wide range of applications where writing is especially needed.</p>



<h4>Drawbacks</h4>



<p>Bi-Directional Logical Replication requires a few preconditions to operate correctly, as such many of its drawbacks are from these specific conditions. For example, when setting up replication the tables in each database must follow the same schema. Same name and columns, otherwise the Subscriber will not be able to find the table. Until Logical Replication can support replication of the Data Definition Language (DDL) used to create the tables, the user must do this manually to ensure consistency.</p>



<h2>Setting Up</h2>



<p>Now that we understand the basics of Bi-Directional Logical Replication, we can look into how we implement it between two databases. The beginning will be quite similar to setting up regular Logical Replication, but with a very important difference when we are creating the Publishers and Subscribers.</p>



<p>First, we will create the two primary databases which will follow each other:</p>



<pre><code>$ initdb -D database1
$ initdb -D database2</code></pre>



<p>In each database’s postgres.conf file set each’s way_level to logical and give each one a unique port number:</p>



<h4>postgres.conf database1</h4>



<pre><code>port = 5432
wal_level = logical</code></pre>



<h4>postgres.conf database2</h4>



<pre><code>port = 5433
wal_level = logical</code></pre>



<p>Start both databases:</p>



<pre><code>pg_ctl -D database1 -l database1.log start
pg_ctl -D database2 -l database2.log start</code></pre>



<p>Create the Publishers for each Database:</p>



<pre><code># CREATE PUBLICATION mypub1 FOR TABLE mytable;
# CREATE PUBLICATION mypub2 FOR TABLE mytable;</code></pre>



<p>Create the Subscribers for each Database:</p>



<pre><code># CREATE SUBSCRIPTION mysub1 CONNECTION 'host=127.0.0.1 port=5433 user=postgres dbname=postgres' PUBLICATION mypub2 WITH(ORIGIN = NONE);

# CREATE SUBSCRIPTION mysub2 CONNECTION 'host=127.0.0.1 port=5432 user=postgres dbname=postgres' PUBLICATION mypub1 WITH(ORIGIN = NONE); </code></pre>



<p>Note the order we created the publishers and subscribers, it is very important to first create the Publishers and then the Subscribers. You can refer to Figure 2 if you want a more visual representation, the number in the corner of each component denotes the order of their creation.</p>



<p>Now, when any data is inserted into either database, it should be replicated across both nodes.</p>



<h2>Conclusion</h2>



<p>In this blog, we went over the new Bi-Directional Logical Replication feature in PostgreSQL 16. To start, we went over a brief background on Logical Replication and the Publisher/Subscriber model used for synchronizing data. We then went over how Bi-Directional Logical Replication works and the new parameter that allows it to function without triggering infinite replication loops. Finally, we looked at how to set up Bi-Directional Replication with two primary PostgreSQL databases. With support for synchronization between primary nodes, increasing availability and data persistence should be a breeze for any of your database applications.</p>



<h2>References</h2>



<ul>
<li>C, Vigneshwaran. <em>Bi-Directional Replication Using Origin Filtering in PostgreSQL</em>, Fujitsu, 31 Aug. 2023, www.postgresql.fastware.com/blog/bi-directional-replication-using-origin-filtering-in-postgresql.</li>
</ul>
<div itemtype="http://schema.org/Person" itemscope="" itemprop="author"><p><img alt="Tristen Raab" src="https://secure.gravatar.com/avatar/dc00783f1bcdc3fc1c905031e7fd9373?s=100&amp;d=mm&amp;r=g" srcset="https://secure.gravatar.com/avatar/dc00783f1bcdc3fc1c905031e7fd9373?s=200&amp;d=mm&amp;r=g 2x" height="100" width="100" itemprop="image" loading="lazy" decoding="async"></p><div><p>Tristen received his Bachelor of Applied Science in Computer Engineering from the University of British Columbia in May 2023. He joined HighGo Software Inc. as a Software Engineer fresh out of university and is very excited for his engineering journey to begin. His main interests include Machine Learning, Embedded Systems, and Database Management Systems. With experience in C/C++ and advanced relational databases, Tristen hopes to contribute significantly to the PostgreSQL community as he continues to learn and grow his expertise.</p></div></div>                                                                    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[The teenager who lives like it's the 1940s (223 pts)]]></title>
            <link>https://www.bbc.com/news/articles/crgpjpr35nko</link>
            <guid>38687171</guid>
            <pubDate>Mon, 18 Dec 2023 19:41:34 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.bbc.com/news/articles/crgpjpr35nko">https://www.bbc.com/news/articles/crgpjpr35nko</a>, See on <a href="https://news.ycombinator.com/item?id=38687171">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="main-content" data-testid="main-content"><article><header data-component="headline-block"></header><div data-component="video-block"><figure><figcaption><span>Media caption, </span><p>Callum drives his 1938 Austin Cambridge around Kirkcaldy (Video by Georgina Davies)</p></figcaption></figure></div><div data-component="text-block"><p>Callum Grubb is only 19 but he lives his life as if it is the 1940s.</p><p>Nearly everything he owns is from the time period, including his clothes.</p><p>The teenager drives a black 1938 Austin Cambridge, only uses a telephone from the 1940s, and rides a 1952 Raleigh bicycle.</p><p>"I joke to my friend, that it's gone beyond an obsession," he says.</p><p>"I'm as old-fashioned as they come."</p></div><div data-component="image-block"><figure><figcaption><span>Image caption, </span><p>Callum is an avid collector of everything and anything 1940s</p></figcaption></figure></div><div data-component="image-block"><figure><p><span><span></span></span><span role="text"><span>Image source, </span>Ron Walker</span></p><figcaption><span>Image caption, </span><p>Callum's car is 10 years older than his grandmother, Anne</p></figcaption></figure></div><div data-component="text-block"><p>Callum's car is 10 years older than his grandmother, Anne, who he has lived with at her home in Kirkcaldy, Fife, since he was 12.</p><p>And while 75-year-old Anne regularly uses a mobile phone, her grandson does not even own one.</p><p>"I'm rubbish with technology," Callum says.</p><p>"I was forced to have a laptop for college, and I hated it."</p></div><div data-component="image-block"><figure><figcaption><span>Image caption, </span><p>Callum and his grandmother have lived together for seven years</p></figcaption></figure></div><div data-component="text-block"><p>Callum says he fell in love with the 40s during his first year of high school.</p><p>"I've always loved history," he says.</p><p>"When I was younger, I looked at my great grandad's prisoner-of-war diaries and I just love everything about the period."</p><p>He now collects everything related to the period, such as oil lamps and a vintage record player that he uses to listen to his favourites like Vera Lynn, Anne Shelton and Frank Sinatra.</p><p>"I couldn't tell you a modern singer if you asked me," he says.</p><p>However, Callum told the BBC's Good Morning Scotland programme - via his 1940s Bakelite rotary-dial telephone - that there is one thing from the post-war period he definitely does not do.</p><p>"We don’t ration,” he says. “I like my food too much for that."</p></div><div data-component="image-block"><figure><figcaption><span>Image caption, </span><p>Callum with another of his vintage telephone, which will soon be connected to his landline</p></figcaption></figure></div><div data-component="text-block"><p>Callum's latest addition to his vintage collection is his car, a 1938 Austin Cambridge named Poppy.  </p><p>He had been saving up for the purchase since he was 13.</p><p>"Callum just came and told me, 'I want an old Austin'," his mum Claire says.</p><p>"I didn't think it would ever happen."</p><p>He bought the car for £7,000 in early November from a man he met while visiting a museum. </p><p>"It has no seatbelts, but I absolutely love it," Callum says.</p><p>He has the original invoice showing the car cost £215 in 1938, the equivalent of about £18,000 today.</p></div><div data-component="image-block"><figure><figcaption><span>Image caption, </span><p>Callum's car, Poppy, has a bench seat in the back, no seatbelts, and an all-leather interior</p></figcaption></figure></div><div data-component="text-block"><p>It is not illegal to drive a classic car without seatbelts - and Callum has driven his nearly every day since he got it.</p><p>"It's like you are back in time, especially when you get into the old country roads," he says.</p><p>"The car survived the blitz in London but it can only reach 50mph and even that is pushing it," Callum laughs.</p><p>"It's great to drive, there's always a clear road ahead but behind a sea of traffic.</p><p>"But folk don't tend to mind," he adds.</p></div><div data-component="image-block"><figure><p><span><span></span></span><span role="text"><span>Image source, </span>Ron Walker</span></p><figcaption><span>Image caption, </span><p>Callum's car is the most valued piece in his collection</p></figcaption></figure></div><div data-component="text-block"><p>Callum is regularly seen around Kirkcaldy in his car, and has become a local celebrity.</p><p>He was even invited by a local bakery to park his car outside the front of their store for a grand-opening. </p><p> "People of all ages wave at the car, old and young alike," Callum says.</p><p>But he regularly takes the car further afield than Kirkcaldy. </p><p>"We love to run along the coast in the old car, my friend Lynsey and I," he says.</p><p>"I usually stop at the antique shops along the way because it can be hard to get some of my clothes.</p><p>"My gran dreads me coming home because she knows I'll end up bringing something back with me."</p></div><div data-component="image-block"><figure><p><span><span></span></span><span role="text"><span>Image source, </span>Ron Walker</span></p><figcaption><span>Image caption, </span><p>Callum likes to take trips along the coast in his car, with his friend Lynsey</p></figcaption></figure></div><div data-component="text-block"><p>Callum says people used to be shocked when he said he did not have a mobile phone.</p><p>"I keep very busy, I assure you," he says.</p><p>He spends a lot of time with his car, maintaining and servicing it, even though it does not require an MOT or road tax, like all cars manufactured more than 40 years ago.</p><p>When Callum isn't with his car, he's out with friends or working at the local dog kennels.</p><p>He says he has friends of all ages, although he admits he shares more in common with people much older than him.</p><p>"I always get to chat to older people, they tell me they remember their dad or granddad with the car when they were my age," he says.</p></div><div data-component="image-block"><figure><p><span><span></span></span><span role="text"><span>Image source, </span>Ron Walker</span></p><figcaption><span>Image caption, </span><p>Callum and his much-loved rescue dog, Cassie</p></figcaption></figure></div><div data-component="text-block"><p>Callum's gran, Anne Walker, says her house has become the epicentre for Callum's collections.</p><p>"I wake up to a picture of Winston Churchill and an old vintage car," she says.</p><p>Callum has lived at his gran's house since he was 12, after his grandad, John, suddenly passed away. He's kept her company ever since.</p><p>"I've learned more about history from Callum, than I've ever known," the 75-year-old says.</p><p>"We always watch old films together, he's in love with Ginger Rogers," Anne says.</p></div><div data-component="image-block"><figure><figcaption><span>Image caption, </span><p>One of the rarest items in Callum's collection is a coronation mug for King Edward VIII </p></figcaption></figure></div><div data-component="text-block"><p>Callum's mum, Claire, is a nursery officer at Fife Council, and says she has always supported his decision.</p><p>"Callum went away on a school trip when he was about 12 and came back with an old-fashioned hat on," she says.</p><p>"I thought it was funny, and I just asked him, 'Where did you find this?'.</p><p>"He said 'that's the way I want to dress, that's going to be me'.</p><p>"Ever since then, that's just been Callum," his mum says.</p><p>Claire says that Callum is doing great, and she's watched him blossom into a 1940s gentlemen.</p></div><div data-component="image-block"><figure><p><span><span></span></span><span role="text"><span>Image source, </span>Claire Grubb</span></p><figcaption><span>Image caption, </span><p>Callum wasn't always in love with 40s</p></figcaption></figure></div><div data-component="text-block"><p>Callum says he can't understand other people's fascination with his collection.</p><p>"You never think you're that interesting" he says.</p><p>"This is just my life."</p><p>Callum says: "A lot of these cars sit in a museum and are never used, but that's not what they're for, they're meant to be used.</p><p>"So that's what I'm doing.</p><p>"It's good to see the reaction, I always go by and beep the horn." </p></div></article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[To be great, be good, repeatably (2019) (139 pts)]]></title>
            <link>https://blog.stephsmith.io/how-to-be-great/</link>
            <guid>38686997</guid>
            <pubDate>Mon, 18 Dec 2023 19:26:30 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://blog.stephsmith.io/how-to-be-great/">https://blog.stephsmith.io/how-to-be-great/</a>, See on <a href="https://news.ycombinator.com/item?id=38686997">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="site-main">
<article>

<figure>
<img srcset="https://blog.stephsmith.io/content/images/size/w300/2019/06/Screen-Shot-2019-06-01-at-6.19.20-PM-3.png 300w,
                            https://blog.stephsmith.io/content/images/size/w600/2019/06/Screen-Shot-2019-06-01-at-6.19.20-PM-3.png 600w,
                            https://blog.stephsmith.io/content/images/size/w1000/2019/06/Screen-Shot-2019-06-01-at-6.19.20-PM-3.png 1000w,
                            https://blog.stephsmith.io/content/images/size/w2000/2019/06/Screen-Shot-2019-06-01-at-6.19.20-PM-3.png 2000w" sizes="(max-width: 800px) 400px,
                            (max-width: 1170px) 700px,
                            1400px" src="https://blog.stephsmith.io/content/images/size/w2000/2019/06/Screen-Shot-2019-06-01-at-6.19.20-PM-3.png" alt="How to Be Great? Just Be Good, Repeatably">
</figure>
<div>
<p><em>Edit: Thank you to the 100k people who have read this piece. It looks like many people are struggling with the concept of "greatness". Let's continue to support one another on the journey there. </em></p><p>Over the years, we’ve all encountered our fair share of successes and failures. As I’ve acquired more of both under my name, I’ve started to contemplate which experiences were truly “great” and why.</p><p>Interestingly enough, I realized that it was not the sporadic highs that were exceptional, but instead the long hauls; the sequences of events that seemed minimal at each juncture, but compounded into major gains. This led me to think further about what <em>greatness</em> truly means. I’ve come to learn that it’s not about overnight successes or flashes of excellence, but periods of repeatable habits.</p><blockquote><em><strong>Perhaps “great’, is just “good”, but repeatable. </strong></em></blockquote><h2 id="consider-this">Consider This</h2><p>Before stepping into the bulk of this article, I want to clarify two things:</p><ul><li>Greatness is not instantaneous</li><li>Greatness is earned</li></ul><p>The first step in becoming great is recognizing that you’re likely not already great. In fact, it comes from recognizing that there is no such thing as greatness at a specific instance in time. Greatness is instead a reflection of a period of effort, since greatness in a single instance can be reduced to luck. </p><p>Moreover, being “great” is not about being better than someone else. It is about being dependable and disciplined, and ultimately it is earned. </p><p>Many people, in theory, want to be “great”. In fact, each month 1000 people search “how to be great”, 260 people search “how to become perfect”, and 2400 people search “how to be the best”, looking for discrete answers on how to get from 0 to 1. Yet, many people in life realistically do not want to put in the effort over a sustained period of time to actually get to 1. They are looking for the “secrets to success” that in many ways, do not exist. You know what brings success? Hard work brings success. </p><p>So before proceeding forward in this article, I implore each of you to consider that if greatness truly is a reflection of non-instantaneous, earned effort, ask yourself if that’s the life you’d like to live. Ask yourself whether you’d like to spend your days, weeks, months, and years in a constant uphill battle. </p><p>If you ultimately find that you don’t want to do that, that’s fine! It doesn’t make you less of a person. At least you’ve broken from the holding pattern of thinking you want to do X but not understanding why you haven’t gotten there yet. And if that’s the case, go enjoy your Netflix and chill completely guilt-free.</p><p>With that in mind, let’s dive into what truly makes someone “great’. </p><h2 id="it-s-hard-to-be-consistent">It’s Hard to be Consistent</h2><p>There’s a false impression that success or notoriety comes with being flashy. This notion comes from the media focusing on outliers, whether it be events or personalities which diverge from the norm. Not only can this encourage people to aim for notoriety just for the sake of it (think Elizabeth Holmes), but it makes the rest of us believe that correlation (of those outliers) is causation; in other words, success of those individuals is due to their offbeat ways. But here’s another storyline: the most sure and therefore the <em>best</em> way to “success” is through consistency.</p><blockquote><em>“Until you work as hard as those you admire, don’t explain away their success as luck.”</em> - James Clear, <a href="https://amzn.to/31t6vQZ">Atomic Habits</a></blockquote><p>To be clear, consistency isn’t necessarily the easiest way to success, but one that can be achieved with a higher level of certainty, rather than hoping for a lottery win or someone to “discover” you. Continuous effort is a more thoughtful approach that leads to greatness when the following statements are true:</p><ol><li>Inputs are consistent over time</li><li>Intentional inputs lead to expected outputs</li></ol><h3 id="consistency">Consistency<br></h3><blockquote><em>“No one who can rise before dawn three hundred sixty days a year fails to make his family rich”</em> - <a href="https://amzn.to/2Fcnn4K">Outliers</a></blockquote><p>There is a famous saying from Napolean Hill which says, <em>“If you cannot do great things, do small things in a great way”</em>. I would actually argue the quote should be, <em>“If you cannot do great things, do small things a great number of times”.</em></p><p>If you don’t have the opportunity to “do great things”, focus on consistently achieving small wins. These small things in fact do not need to be done in a great way, but a good way, repeatably. In fact, I would advise not to focus on perfection, as it is often the enemy of the successful. </p><p>There’s glimmer and hoopla around unpredictability, but in reality, it’s much more difficult and therefore impressive, to be predictably good. For example:</p><ul><li>It’s easy to wake up whenever you “feel like it”. </li><li>It’s hard to stick to a routine of getting up at 6AM.</li><li>It’s easy to pivot from side project to side project, focusing on the new shiny object of the month.</li><li>It’s hard to stick with a side project for years, many of which may not be profitable for a long while. </li><li>It’s easy to give up on someone when you hit a roadblock or the next potential partner becomes available. </li><li>It’s hard to be faithful and invest in a relationship for decades.</li></ul><p>We normally set out in life with good intentions. We <em>intend</em> to set a morning routine or work on a business until it's profitable or to “love someone forever”. We imagine that as we invest in something, we will naturally continue to move in the right direction. If anything, things will get easier, right?</p><figure><img src="https://lh4.googleusercontent.com/NQV_7RXfM2ZYDJsT4E4JAaAW2i9KphrB7-fE8XveJlrW-Msf1aIot9ayls-wSCWIroq155Fzdz6vE3d8br71E6Tc5sTQRyhnT0Zb6EJCUoV41B5EoZSczjtWcHo9KV-T9c6s16Cb"></figure><p>The described trajectory is what we perceive on the left. Predictable, linear, and a direct reflection of effort put in. </p><p><strong>Rarely does success in anything look like that.</strong> Life is a series of tiny nodes that tend to look more like the right hand side. There two key elements worth calling out in the more realistic graph on the right:</p><ul><li>Compounding is always present. The earlier steps in any process will be more strenuous, yet it’s difficult to imagine the potential compounding that comes later on. </li><li>With the ups, there are always downs. This seems obvious, but we often forget this when we are in periods of down. We quit at these local minimums (the highlighted sections in red above), because we cannot see the next peak right around the corner. </li></ul><figure><blockquote><div lang="en" dir="ltr"><p>a friend just emailed me this note in response to my 'burn out' video. wanted to share;</p><p>'the addiction to having success is what makes you feel unsuccessful at the times when you're not feeling the immediate dopamine hit of your work 'succeeding' at that precise moment.'</p></div>— Casey Neistat (@Casey) <a href="https://twitter.com/Casey/status/1093520669286064133?ref_src=twsrc%5Etfw">February 7, 2019</a></blockquote>

</figure><figure><blockquote><p lang="en" dir="ltr">We have lost our ability to appreciate delayed gratification and, some of us who struggle with perfectionism, can either create repeatedly until we ‘get it right’ never truly feeling fulfilled with our work, or, we avoid creating all together in fear of failing.</p>— Marcio Novelli (@MarcioNovelli) <a href="https://twitter.com/MarcioNovelli/status/1093522120536150017?ref_src=twsrc%5Etfw">February 7, 2019</a></blockquote>

</figure><p>The local minima are especially psychologically taxing due to something called the <a href="https://en.m.wikipedia.org/wiki/Hedonic_treadmill">Hedonic treadmill</a> or hedonic adaptation. Essentially, as someone achieves new successes in various aspects of their lives, their baseline shifts to reflect that new level and therefore, their expectations and desires are re-established as well. There is no net gain in happiness and thus, it becomes even more difficult to stay “level-headed” during these down moments. </p><p>That is exactly why a specific search for success can be problematic and instead of looking for unsustainable shortcuts in life, it’s much more effective (and healthy) to aim for continuous habits that bring you success as a byproduct, not as the end goal.</p><blockquote><em>“The only way to become excellent is to be endlessly fascinated by doing the same thing over and over. You have to fall in love with boredom.” - <a href="https://amzn.to/31t6vQZ">Atomic Habits</a>, James Clear</em></blockquote><p>On your journey to greatness, you need to fall in love with the process which includes many local minima and maxima. Staying consistent and pushing through both of these continuously is what will truly differentiate you from those that are simply “good” and isolate you as one of the few that are “great’.</p><h3 id="inputs-outputs">Inputs → Outputs</h3><p>The second important aspect of achieving greatness is acting with intention. Your actions and results will not always reflect your intentions, but as you move towards “greatness”, you should have a better idea of what inputs actually deliver output. You’ll still make mistakes﹣as we all do﹣but you’ll have a better grasp on what is more likely to work out. For example, your success rate may be 30%, versus someone flying blind with a 5% success rate. </p><p>Let’s look at a simple example:</p><p>Imagine company X has two sales people. Salesperson A happens to land a $1M deal in his first week. However, he struggles to land anything substantial for the next 6 months. Meanwhile, salesperson B manages to develop a process over the first month, bringing in only $100k which he’s able to scale up and double month over month.</p><p>After six months, this will be the revenue generated by each party.</p><figure><img src="https://blog.stephsmith.io/content/images/2019/05/image-5.png"></figure><p>You’re probably thinking… "So what? That’s just a classic example of compounding." </p><p>Yes! That’s exactly the point. The best things in life often aren’t miracles, but well-thought out approaches that are sustainable. The same thing is true with businesses, marriages, and just about anything with repeatable elements. If you invest time into solving for what leads to success continuously, you will reap those benefits for years to come. <em>So even in the least quantifiable situations, reflect back on what could’ve made a previous loss a future win. </em></p><p>Consider the best companies over time. None of them emerged overnight, nor was there a single inflection point that determined the success or notoriety of these companies. The line of separation between the “great” companies of all time and the “not so great”, is their ability to stand the test of time. </p><p>Would you rather be Juicero that <a href="https://www.crunchbase.com/organization/juicero">raised $100M</a> and <a href="https://techcrunch.com/2017/09/01/rip-juicero-the-400-venture-backed-juice-machine/">went bankrupt</a> within a year of its Series C, or Zoom, which <a href="https://www.crunchbase.com/organization/zoom-video-communications/funding_rounds/funding_rounds_list#section-funding-rounds">took almost 8 years to take on more funding than $30M</a> and is now one of the most profitable and highly sought after “unicorns” in the valley?</p><p>On top of consistency, greatness comes from asking the right questions and iterating to learn what inputs drive favourable outputs, and ideally <strong>why</strong>. “Greatness” comes from an identified or researched process that when followed, has some degree of certainty in the outcome. </p><p>“Moving fast and breaking things” is not a strategy, unless you are clearly defining a process of learning so that in the future, you can “move fast and break less of the same things”.</p>


<h2 id="a-habit-of-progression">A Habit of Progression</h2><blockquote><em>“The definition of insanity is doing the same thing over and over again, but expecting different results.” </em>- Albert Einstein</blockquote><p>Understand that in order to achieve the things you want in life, you’ll need to establish a <strong>habit of progression</strong>. You literally need to become good at being decent. </p><p>There is one thing to clarify: this habit of progression must come with the right inputs. Being consistent with something leading you in the wrong direction, will <em>unsurprisingly</em> lead you in the wrong direction. So if this is the way you are constantly moving (excluding short periods of local minima), pivot until you determine what the right inputs are. I recently stumbled upon a concept of <a href="https://web.archive.org/web/20200908160755/jdreyespaez.site/zero-acceleration/">zero acceleration, but non-zero velocity</a>, which encapsulates this idea well. </p><p>Before you find the path that you want to double down on, this habit of progression takes the form of iteration. I see many people who are stuck in this stage and feel like they’re moving nowhere. Perhaps they go take a degree for a year and find that wasn’t right. Maybe they go and work for a company for two more and realize that wasn’t right either. </p><p>If you’re struggling to identify the right path, create more nodes of optimization. For example: if you’re making changes every year, you only have maybe 80 in your entire life to make. Instead, try testing things intentionally every month or even every week. Pilot a lot and then double down when you have found your path towards “good”. </p><p>You may ask, “what makes good, good?”. Ask yourself the question: “If I were to continue this every day for the next year, would I be in a better place?” If the answer is yes, you have a path towards “good”. </p><p>Once you have found your inputs, then you’re in a good place to turn those inputs into the right habits through deliberate practice. Ie: you’ll be in a place to shift from good to great. </p><p>This process of shifting between iteration and consistency is all part of developing a habit of progression. Once you make this habit your north star, you are no longer dependent on that &nbsp;“one big break” or that one company to “finally give you a chance”. </p><p>And finally, if you’re reading this advice and think, “I’ve heard this before”, then ask yourself whether you’ve truly acted on this advice. When is the last time you truly iterated and tried new things? When is the last time you found something <em>good</em> and then truly stuck to it for years?</p><h2 id="two-steps-out">Two Steps Out </h2><p>While you’re moving towards “greatness”, keep in mind that it will likely happen slowly and that’s okay.</p><p>When I think about the growth trajectory of my life and similarly, anyone that I’ve been close with, changes have always happened slowly. Whether it was a close family member falling into deep mental illness or friends building businesses to near-unicorn valuations, nothing ever happened overnight. Even more notably, no one would have ever expected those outcomes years prior. </p><p>In the grand scheme of things, I think it’s because you can only ever see two steps out. What do I mean by that?</p><p>Say that in life there are 100 tiers of happiness. Of course, life is more complicated and dynamic than this view, but bear with me for a minute. Let’s say that you’re on “Tier 57”. &nbsp;You may be able to see Tier 58 and 59, but I think it’s nearly impossible to fully empathize or even comprehend level 21 and similarly 89, unless of course you’ve been there before. Even if you have, it becomes a distant memory that’s difficult to fully internalize. Remember, the Hedonic treadmill is almost always at play. </p><figure><img src="https://blog.stephsmith.io/content/images/2019/06/image-3.png"></figure><p>Why is this important? Everyone wishes to elevate their life and in association, their happiness. For us to reach these top tiers, we cannot hope for this to <em>just happen</em>. We must expose ourselves to various inputs that may lead to better outputs, and train ourselves to recognize what’s working. </p><p>And that’s exactly the point of continuous improvement. Since I believe that we can only ever see “two levels out”, we can’t discover these new inputs without slow, but repeatable change. We must explore 58 and then 59 and then all of a sudden, 61 will appear as this new array of opportunity we had never considered before. </p><p>As a more tangible example, when I started working in an office, I simply couldn’t fully visualize remote work. I knew it existed, but I couldn’t truly imagine this new way of living. And even once I started working remotely, it took years of iteration and pivoting to expand into the lifestyle that I now call my own. And of course, there’s probably many more tiers to explore that I simply haven’t visualized yet. </p><blockquote><em>“To exist is to change, to change is to mature, to mature is to go on creating oneself endlessly.” - Henri Bergson</em></blockquote><p>That’s precisely why it’s good to continuously surround yourself with new environments and people, and to focus on slow, but steady compounding. I think this tweet from Michael Nielson captures what I’m trying to communicate. </p><figure><blockquote><p lang="en" dir="ltr">Since reading this 6 months ago, I've come to think it's half a dozen of the best paragraphs I've ever read on how to get much, much better at anything: (by @autotrnslucence ) <a href="https://t.co/dMoEwHtjfU">pic.twitter.com/dMoEwHtjfU</a></p>— michael_nielsen (@michael_nielsen) <a href="https://twitter.com/michael_nielsen/status/1074150124169773056?ref_src=twsrc%5Etfw">December 16, 2018</a></blockquote>

</figure><p>Try to remind yourself as you’re iterating, that there are new levels that you can’t even conceptualize right now. Regardless of how far along you are, know that these new levels of success will appear as you work towards the next one or two. And soon enough, you’ll be 10 levels ahead of what you could have ever imagined. </p>


<h2 id="stop-speculating">Stop Speculating</h2><blockquote><em>“I have seen impractical and improbable things accomplished. All it took to achieve improbable things was an optimistic attitude and a refusal to give up.” - <a href="https://amzn.to/2XwrlzI">The Woman Who Smashed Codes</a></em></blockquote><p>James Clear, from <a href="https://amzn.to/31t6vQZ">Atomic Habits</a>, mentions a study in which students in Jerrey Uelsmann’s University of Florida photography class were divided into two groups. The first group would be the “quantity” group, while the second would be the “quality” group. The former would be judged solely on the number of photographs they submitted, while the latter would be graded on the excellence of a single image.</p><p>The interesting outcome of the experiment was that the best photos were produced not by the quality group, but by the quantity group. Why? While the quality group spent their time speculating what perfection may have been, the quantity group took action in testing what was truly great. </p><blockquote><em>“It is easy to get bogged down trying to find the optimal plan for change: the fastest way to lose weight, the best program to build muscle, the perfect idea for a side hustle. We are so focused on figuring out the best approach that we never get around to taking action” - James Clear, <a href="https://amzn.to/31t6vQZ">Atomic Habits</a></em></blockquote><p>In other words, the “search” for greatness is often misguided, perhaps because what we imagine to be great, is in fact not that great at all. Instead of speculating what may make you great, get out there and start <strong>doing</strong>. Do not look for perfection or even greatness, but instead signs of “good” and start making tangible progress. </p><figure><blockquote><div lang="en" dir="ltr"><p>Building products, writing, and painting are not mental excercises, they are physical ones.</p><p>Reading to improve is like watching someone else workout – it does almost nothing for you. </p><p>To run better, run. <br>To paint better, paint. <br>To write better, write.<br>To build better, build.</p></div>— Sahil Lavingia (@shl) <a href="https://twitter.com/shl/status/1118224158020685824?ref_src=twsrc%5Etfw">April 16, 2019</a></blockquote>

</figure><h2 id="how-do-you-become-great">How Do You Become Great?</h2><p>So if you’re still asking the question, “How do I become great in life?”, I would ask you to reframe the question as “How do I become good in life” or even “How do I become decent” and focus on developing those habits to repeat over time. Transform these habits to be your baseline. </p><p>Remember, there is no “magic moment” when you become great, so if you are looking for your path towards greatness, stop looking for “greatness” and consider that your most probable path there is just to focus on what’s good. </p><p>If you have an understanding of what inputs equal favourable outputs then continue moving in that direction. As you move past the local minima and maxima, you’ll soon be beating out the 50% that quit at X time, the 75% that quit at Y time, and the 90% that quit at Z time. Soon enough, you’ll be the great one that was once just “good” among the rest, but stuck with it and learned something along the way.</p><blockquote>In being consistent over time, you <strong>become</strong> the outlier. </blockquote><p>Remember: <strong>great is just good, but repeatable. </strong></p><hr><div><p>This article was originally inspired by me trying to more deeply understand what “made people great’, but ended up being a dive into some of the psychology I’ve been experiencing more recently. Over the last few months, I think I’ve been on one of my local minimums in terms of direct “success”, but in writing this I feel motivated to keep pushing to my next local maximum, with the understanding that there will be many more of both moving forward. </p><p>If you’re interested in learning more about habit building and long-term progression, I would recommend the following <a href="https://stephsmith.io/books">books</a>:</p></div><ol><li><a href="https://amzn.to/2Xd1MUa">Power of Habit</a></li><li><a href="https://amzn.to/31t6vQZ">Atomic Habits</a></li><li><a href="https://amzn.to/2RCsAYV">Outliers</a></li><li><a href="https://amzn.to/2KiSWOv">Algorithms to Live By</a></li></ol><hr><p>PS: If you liked this article, I have a feeling you'll enjoy this <a href="http://listenandlearn.co/">podcast</a> episode about other narratives that we believe, that may no longer be serving us.</p><hr><p>Related posts about <strong><a href="https://blog.stephsmith.io/tag/personal-growth/">Personal Development</a></strong>:</p><ul><li><a href="https://blog.stephsmith.io/you-dont-need-to-quit-your-job-to-make/">You Don't Need to Quit Your Job to Make</a></li></ul><hr>

</div>
<section>
<h3>Subscribe to Steph | Smith</h3>
<p>Get the latest posts delivered right to your inbox</p>

</section>

</article>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[VW is putting buttons back in cars (343 pts)]]></title>
            <link>https://www.thedrive.com/news/vw-is-putting-buttons-back-in-cars-because-people-complained-enough</link>
            <guid>38686967</guid>
            <pubDate>Mon, 18 Dec 2023 19:24:11 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.thedrive.com/news/vw-is-putting-buttons-back-in-cars-because-people-complained-enough">https://www.thedrive.com/news/vw-is-putting-buttons-back-in-cars-because-people-complained-enough</a>, See on <a href="https://news.ycombinator.com/item?id=38686967">Hacker News</a></p>
<div id="readability-page-1" class="page"><section data-id="page-content" data-og-area="article-blocks" id="incArticle"><p>Whether you've driven a new <a href="https://www.thedrive.com/category/volkswagen-news" target="_blank" rel="noreferrer noopener">Volkswagen</a> or not, there's a good chance you've heard about its interiors. People who care about the German everyman brand have been extremely opinionated about the cars' lack of physical buttons. Customers' cries have even reached VW CEO Thomas Schäfer's ears, as <a href="https://www.thedrive.com/news/vw-ceo-admits-frustrating-golf-id-4-interiors-did-a-lot-of-damage" target="_blank" rel="noreferrer noopener">he said</a> the "frustrating" <a href="https://www.thedrive.com/news/vw-already-talking-about-dumping-touchscreen-controls-in-future-evs" target="_blank" rel="noreferrer noopener">touchscreen controls</a> "definitely did a lot of damage." Now, VW interior designer Darius Watola&nbsp;is reported as saying that the brand is going back to buttons on all new cars.</p><p>The news comes from <em>Autocar</em>, which quotes Watola as saying the ID.2All concept and its buttons "showed a new approach for all models." Like Schäfer, he referenced the public's feedback. There are still touchscreens, to be sure—the infotainment display is large and in charge, and there's also a digital gauge cluster. But instead of all the controls being hidden behind menus in these displays, they're toggled via switches on the center stack.</p><figure data-og-block-area="article-blocks" data-og-block-nth="1" data-og-block-type="core/image"><span data-rawhtml="1">The ID.2All Concept still looks pretty sparse inside, but at least it has physical controls for the HVAC and all four windows. The production ID.4 can't even say that. <em>VW</em></span></figure><p>This is reassuring for the simple fact that actual buttons just work. The <a href="https://www.thedrive.com/news/vw-brings-back-steering-wheel-buttons-after-many-customers-complain" target="_blank" rel="noreferrer noopener">touch sliders and whatnot</a> in the Mk8 Golf and ID.4 are finicky, only operating like they should some of the time. They also aren't backlit, making them difficult to find and use in the dark. Owners and professional critics alike have been tough on modern VWs for these exact reasons; <a href="https://www.thedrive.com/new-cars/43809/2021-volkswagen-id-4-review-a-practical-ev-ruined-by-frustrating-tech" target="_blank" rel="noreferrer noopener">my co-worker Jerry Perez went as far as saying</a> the ID.4's tech "ruined" what was an otherwise practical and livable EV.</p><p>Speaking on the car's infotainment, <a href="https://www.thedrive.com/new-cars/43809/2021-volkswagen-id-4-review-a-practical-ev-ruined-by-frustrating-tech" target="_blank" rel="noreferrer noopener">Jerry said</a>:</p><p>"It's awful, likely the worst I've ever come across. I was never able to find a channel list for the XM satellite radio because there isn't an option to do such a thing—at least not one you can find in under 30 minutes of poking around. Then there's a climate system on/off icon on the screen that you must click to turn the AC or heater on. You can't just push the fan speed icon and expect it to turn on. No, you must tap the "on" button and then adjust the temperature or fan speed separately. This extra step makes no sense—and to add insult to injury, I couldn't turn the heater on countless times because the climate portion of the OS was unresponsive. Other times, it would simply say that the function couldn't be performed at the time. Why? No idea."</p><p>As you can tell based on that blurb, actual buttons will be welcomed back with open arms.</p><p><em>Got a tip or question for the author? Contact them directly: caleb@thedrive.com</em></p></section></div>]]></description>
        </item>
    </channel>
</rss>