<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0">
    <channel>
        <title>HN100 - Readable Contents</title>
        <link>https://hn.algolia.com/api/v1/search_by_date?tags=%28story,poll%29&amp;numericFilters=points%3E100</link>
        <description>Uses Readability to add bodies to the RSS feed</description>
        <lastBuildDate>Thu, 13 Jun 2024 16:30:03 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>en</language>
        <item>
            <title><![CDATA[Show HN: Shpool, a Lightweight Tmux Alternative (102 pts)]]></title>
            <link>https://github.com/shell-pool/shpool</link>
            <guid>40669337</guid>
            <pubDate>Thu, 13 Jun 2024 13:22:29 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/shell-pool/shpool">https://github.com/shell-pool/shpool</a>, See on <a href="https://news.ycombinator.com/item?id=40669337">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><p dir="auto"><h2 tabindex="-1" dir="auto">shpool</h2><a id="user-content-shpool" aria-label="Permalink: shpool" href="#shpool"></a></p>
<p dir="auto"><code>shpool</code> is a service that enables session persistence by allowing the
creation of named shell sessions owned by <code>shpool</code> so that the session
is not lost if the connection drops. <code>shpool</code> can be thought of as a lighter
weight alternative to <code>tmux</code> or GNU <code>screen</code>. While <code>tmux</code> and <code>screen</code> take over
the whole terminal and provide window splitting and tiling features, <code>shpool</code>
only provides persistent sessions. The biggest advantage of this approach is
that <code>shpool</code> does not break native scrollback or copy-paste.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Installation</h2><a id="user-content-installation" aria-label="Permalink: Installation" href="#installation"></a></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Installing from crates.io</h3><a id="user-content-installing-from-cratesio" aria-label="Permalink: Installing from crates.io" href="#installing-from-cratesio"></a></p>
<p dir="auto">Run</p>
<div data-snippet-clipboard-copy-content="cargo install shpool
curl -fLo &quot;${XDG_CONFIG_HOME:-$HOME/.config}/systemd/user/shpool.service&quot; --create-dirs https://raw.githubusercontent.com/shell-pool/shpool/master/systemd/shpool.service
sed -i &quot;s|/usr|$HOME/.cargo|&quot; &quot;${XDG_CONFIG_HOME:-$HOME/.config}/systemd/user/shpool.service&quot;
curl -fLo &quot;${XDG_CONFIG_HOME:-$HOME/.config}/systemd/user/shpool.socket&quot; --create-dirs https://raw.githubusercontent.com/shell-pool/shpool/master/systemd/shpool.socket
systemctl --user enable shpool
systemctl --user start shpool
loginctl enable-linger"><pre><code>cargo install shpool
curl -fLo "${XDG_CONFIG_HOME:-$HOME/.config}/systemd/user/shpool.service" --create-dirs https://raw.githubusercontent.com/shell-pool/shpool/master/systemd/shpool.service
sed -i "s|/usr|$HOME/.cargo|" "${XDG_CONFIG_HOME:-$HOME/.config}/systemd/user/shpool.service"
curl -fLo "${XDG_CONFIG_HOME:-$HOME/.config}/systemd/user/shpool.socket" --create-dirs https://raw.githubusercontent.com/shell-pool/shpool/master/systemd/shpool.socket
systemctl --user enable shpool
systemctl --user start shpool
loginctl enable-linger
</code></pre></div>
<p dir="auto"><h2 tabindex="-1" dir="auto">Usage</h2><a id="user-content-usage" aria-label="Permalink: Usage" href="#usage"></a></p>
<p dir="auto">Generally <code>shpool</code> is used to provide persistent sessions when
sshing in to a remote host. To do so, <code>shpool</code> must be installed
on the remote host. No extra software is required on the client.
After installing and setting up, the typical usage pattern
is to ssh into the host you have installed shpool on, then create
a new named session by running <code>shpool attach main</code>. Here <code>main</code>
is the name of the session. You'll want a separate named session
for each terminal you use to connect to your remote host. If your
connection drops or becomes stuck, you can ssh back into the remote
host and re-attach to the same named session by running <code>shpool attach main</code>
again.</p>
<p dir="auto">If your terminal gets stuck and you forcibly close the window, you
might find that <code>shpool</code> still think a terminal is connected to
your session when you attempt to reattach. This is likely because
an ssh proxy is holding the connection open in the vain hope that
it will get some traffic again. You can just run <code>shpool detach main</code>
to force the session to detach and allow you to attach.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Configuration</h3><a id="user-content-configuration" aria-label="Permalink: Configuration" href="#configuration"></a></p>
<p dir="auto">You can specify some additional configuration options to the daemon
by passing a <code>-c /path/to/config.toml</code> flag, or by creating and
editing <code>~/.config/shpool/config.toml</code>. The options available
are documented in detail in <code>libshpool/src/config.rs</code>, but there
are a few common things you may wish to tweak.</p>
<p dir="auto"><h4 tabindex="-1" dir="auto">Detach Keybinding</h4><a id="user-content-detach-keybinding" aria-label="Permalink: Detach Keybinding" href="#detach-keybinding"></a></p>
<p dir="auto">You may wish to configure your detach keybinding.
By default, <code>shpool</code> will detach from the current user session when you
press the sequence <code>Ctrl-Space Ctrl-q</code> (press <code>Ctrl-Space</code> then release
it and press <code>Ctrl-q</code>, don't try to hold down all three keys at once),
but you can configure a different binding by adding an entry
like</p>
<div data-snippet-clipboard-copy-content="[[keybinding]]
binding = &quot;Ctrl-a d&quot;
action = &quot;Detach&quot;"><pre><code>[[keybinding]]
binding = "Ctrl-a d"
action = "Detach"
</code></pre></div>
<p dir="auto">to you <code>~/.config/shpool/config.toml</code>.</p>
<p dir="auto">For the moment, control is the only modifier key supported, but the keybinding
engine is designed to be able to handle more, so if you want a different one,
you can file a bug with your feature request.</p>
<p dir="auto"><h4 tabindex="-1" dir="auto">Session Restore Mode</h4><a id="user-content-session-restore-mode" aria-label="Permalink: Session Restore Mode" href="#session-restore-mode"></a></p>
<p dir="auto">Shpool can do a few different things when you re-attach to an existing
session. You can choose what you want it to do with the <code>session_restore_mode</code>
configuration option.</p>
<p dir="auto"><h5 tabindex="-1" dir="auto"><code>"screen"</code> (default) - restore a screenful of history</h5><a id="user-content-screen-default---restore-a-screenful-of-history" aria-label="Permalink: &quot;screen&quot; (default) - restore a screenful of history" href="#screen-default---restore-a-screenful-of-history"></a></p>
<p dir="auto">The <code>"screen"</code> option causes <code>shpool</code> to re-draw sufficient output to fill the
entire screen of the client terminal as well as using the SIGWINCH trick
described in the <code>"simple"</code> section below. This will help restore
context for interactive terminal sessions that are not full blown ncurses
apps. <code>"screen"</code> is the default reattach behavior for <code>shpool</code>.
You can choose this option explicitly by adding</p>
<div data-snippet-clipboard-copy-content="session_restore_mode = &quot;screen&quot;"><pre><code>session_restore_mode = "screen"
</code></pre></div>
<p dir="auto">to your <code>~/.config/shpool/config.toml</code>.</p>
<p dir="auto"><h5 tabindex="-1" dir="auto"><code>"simple"</code> - only ask child processes to redraw</h5><a id="user-content-simple---only-ask-child-processes-to-redraw" aria-label="Permalink: &quot;simple&quot; - only ask child processes to redraw" href="#simple---only-ask-child-processes-to-redraw"></a></p>
<p dir="auto">The <code>"simple"</code> option avoids restoring any output. In this reconnect mode, <code>shpool</code> will
issue some SIGWINCH signals to try to convince full screen ncurses apps
such as vim or emacs to re-draw the screen, but will otherwise do nothing.
Any shell output produced when there was no client connected to the session
will be lost. You can choose this connection mode by adding</p>
<div data-snippet-clipboard-copy-content="session_restore_mode = &quot;simple&quot;"><pre><code>session_restore_mode = "simple"
</code></pre></div>
<p dir="auto">to your <code>~/.config/shpool/config.toml</code>.</p>
<p dir="auto"><h5 tabindex="-1" dir="auto"><code>{ lines = n }</code> - restore the last n lines of history</h5><a id="user-content--lines--n----restore-the-last-n-lines-of-history" aria-label="Permalink: { lines = n } - restore the last n lines of history" href="#-lines--n----restore-the-last-n-lines-of-history"></a></p>
<p dir="auto">The lines option is much like the <code>"screen"</code> option, except that rather
than just a screenful of text, it restores the last n lines of text
from the terminal being re-attached to. This could be useful if you
wish to have more context than a single screenful of text. Note that
n cannot exceed the value of the <code>output_spool_lines</code> configuration
option, but it defaults to the value of the lines option, so you likely
won't need to change it.</p>
<div data-snippet-clipboard-copy-content="session_restore_mode = { lines = n }"><pre><code>session_restore_mode = { lines = n }
</code></pre></div>
<p dir="auto">where n is a number to your <code>~/.config/shpool/config.toml</code>.</p>
<p dir="auto"><h4 tabindex="-1" dir="auto">Shell Config</h4><a id="user-content-shell-config" aria-label="Permalink: Shell Config" href="#shell-config"></a></p>
<p dir="auto"><h5 tabindex="-1" dir="auto">bash</h5><a id="user-content-bash" aria-label="Permalink: bash" href="#bash"></a></p>
<p dir="auto">If you use bash, you may want to ensure that the <code>huponexit</code> option
is set to make sure that child processes exit when you leave a
shell. Without this setting, background processes you have
spawned over the course of your shell session will stick around
in the <code>shpool</code> daemon's process tree and eat up memory. To set
this option add</p>

<p dir="auto">to your <code>~/.bashrc</code>.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Subcommands</h3><a id="user-content-subcommands" aria-label="Permalink: Subcommands" href="#subcommands"></a></p>
<p dir="auto"><h4 tabindex="-1" dir="auto">shpool daemon</h4><a id="user-content-shpool-daemon" aria-label="Permalink: shpool daemon" href="#shpool-daemon"></a></p>
<p dir="auto">The <code>daemon</code> subcommand causes <code>shpool</code> to run in daemon mode. When running in
this mode, <code>shpool</code> listens for incoming connections and opens up subshells,
retaining ownership of them in a table. In general, this subcommand will not
be invoked directly by users, but will instead be called from a systemd unit
file.</p>
<p dir="auto"><h4 tabindex="-1" dir="auto">shpool attach</h4><a id="user-content-shpool-attach" aria-label="Permalink: shpool attach" href="#shpool-attach"></a></p>
<p dir="auto">The <code>attach</code> subcommand connects to the <code>shpool daemon</code> instance, passing in a
name. If the name is new, a new shell is created, and if it already exists it
just attaches to the existing session so long as no other terminal is currently
connected to that session. The <code>--ttl</code> flag can be used to limit how long the
session will last.</p>
<p dir="auto"><h4 tabindex="-1" dir="auto">shpool list</h4><a id="user-content-shpool-list" aria-label="Permalink: shpool list" href="#shpool-list"></a></p>
<p dir="auto">Lists all the current shell sessions.</p>
<p dir="auto"><h4 tabindex="-1" dir="auto">shpool detach</h4><a id="user-content-shpool-detach" aria-label="Permalink: shpool detach" href="#shpool-detach"></a></p>
<p dir="auto">Detach from a one or more sessions without stopping them.
Will detach the current session if run from inside a <code>shpool</code>
session with no session name arguments.</p>
<p dir="auto"><h4 tabindex="-1" dir="auto">shpool kill</h4><a id="user-content-shpool-kill" aria-label="Permalink: shpool kill" href="#shpool-kill"></a></p>
<p dir="auto">Kills a named shell session.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">(Optional) Automatically Connect to shpool</h3><a id="user-content-optional-automatically-connect-to-shpool" aria-label="Permalink: (Optional) Automatically Connect to shpool" href="#optional-automatically-connect-to-shpool"></a></p>
<p dir="auto"><h4 tabindex="-1" dir="auto">Explicitly named sessions</h4><a id="user-content-explicitly-named-sessions" aria-label="Permalink: Explicitly named sessions" href="#explicitly-named-sessions"></a></p>
<p dir="auto">Specifying session names yourself lets you assign logical
roles such as text editing to each session.</p>
<p dir="auto"><h5 tabindex="-1" dir="auto">ssh config</h5><a id="user-content-ssh-config" aria-label="Permalink: ssh config" href="#ssh-config"></a></p>
<p dir="auto">If you typically connect to a small number of sessions with
the same jobs on a particular machine, custom ssh config
blocks on your client machine are probably the best
fit.</p>
<p dir="auto">To do this, you can add a config block named <code>edit</code> like so</p>
<div data-snippet-clipboard-copy-content="Host = edit
    Hostname remote.host.example.com

    RemoteCommand shpool attach -f edit
    RequestTTY yes"><pre><code>Host = edit
    Hostname remote.host.example.com

    RemoteCommand shpool attach -f edit
    RequestTTY yes
</code></pre></div>
<p dir="auto">to <code>~/.ssh/config</code> on your client machine. You will need one
such block per session name. You can then invoke this with
<code>ssh edit</code>.</p>
<p dir="auto"><h5 tabindex="-1" dir="auto">shell function</h5><a id="user-content-shell-function" aria-label="Permalink: shell function" href="#shell-function"></a></p>
<p dir="auto">If you would rather have a little more flexibility in
specifying the session name and machine you are targeting,
you can make a custom shell function to let you specify
both at invocation time. Add</p>
<div data-snippet-clipboard-copy-content="function shpool-ssh () {
    if [ $# -ne 2 ] ; then
        echo &quot;usage: shpool-ssh <remote-machine> <session-name>&quot; >&amp;2
        return 1
    fi
    ssh -t &quot;-oRemoteCommand=shpool attach -f $2&quot; &quot;$1&quot;
}"><pre><code>function shpool-ssh () {
    if [ $# -ne 2 ] ; then
        echo "usage: shpool-ssh &lt;remote-machine&gt; &lt;session-name&gt;" &gt;&amp;2
        return 1
    fi
    ssh -t "-oRemoteCommand=shpool attach -f $2" "$1"
}
</code></pre></div>
<p dir="auto">to your <code>.bashrc</code> then invoke it like
<code>shpool-ssh remote.host.example.com main</code>.</p>
<p dir="auto"><h4 tabindex="-1" dir="auto">Local tty based</h4><a id="user-content-local-tty-based" aria-label="Permalink: Local tty based" href="#local-tty-based"></a></p>
<p dir="auto">Rather than specify an explicit name when you connect, you
can set up your system to automatically generate a <code>shpool</code>
session name based on your local terminal emulator's tty
number. To do so, you can add a block of custom ssh config
in the <code>~/.ssh/config</code> of your local machine like</p>
<div data-snippet-clipboard-copy-content="Host = by-tty
    User remoteuser
    Hostname remote.host.example.com

    RemoteCommand shpool attach -f &quot;ssh-$(basename $(tty))&quot;
    RequestTTY yes"><pre><code>Host = by-tty
    User remoteuser
    Hostname remote.host.example.com

    RemoteCommand shpool attach -f "ssh-$(basename $(tty))"
    RequestTTY yes
</code></pre></div>
<p dir="auto">which you then invoke with <code>ssh by-tty</code>. You can apply the same principle
of using <code>$(basename $(tty))</code> to get a unique id for your local terminal
to the custom shell function approach as well.</p>
<p dir="auto">The local-tty based approach has the advantage that you don't
need to specify a session name, but it can run into problems
if you have to close the local window and open a new terminal,
which can come up if your connection freezes rather than drops.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Comparison with other tools</h2><a id="user-content-comparison-with-other-tools" aria-label="Permalink: Comparison with other tools" href="#comparison-with-other-tools"></a></p>
<p dir="auto"><h3 tabindex="-1" dir="auto"><code>tmux</code> and GNU <code>screen</code></h3><a id="user-content-tmux-and-gnu-screen" aria-label="Permalink: tmux and GNU screen" href="#tmux-and-gnu-screen"></a></p>
<p dir="auto"><code>tmux</code> is probably the best known session persistence tool, and
GNU <code>screen</code> has a similar feature set, so in comparison to <code>shpool</code>
it can be thought of as belonging to the same category.</p>
<p dir="auto">The main way that <code>shpool</code> differs from <code>tmux</code> is that <code>tmux</code> is a
terminal multiplexer which necessarily means that it offers session
persistence features, while <code>shpool</code> only aims to be a session
persistence tool. In contrast to <code>tmux</code> the philosophy of <code>shpool</code>
is that managing different terminals is the job of your display or
window manager, not your session persistence tool. Every operating
system has its own idioms for switching between applications, and
there is no reason to switch to different idioms when switching
between terminals. Especially for users of tiling window managers
such as <code>i3</code>, <code>sway</code> or <code>xmonad</code>, tmux's multiplexing features are
redundant.</p>
<p dir="auto">While <code>tmux</code> renders terminal contents remotely and only paints
the current view to the screen, <code>shpool</code> just directly sends
all shell output back to the user's local terminal. This means
that all rendering is handled by a single terminal state machine
rather than going through <code>tmux</code>s internal in-memory terminal
before getting formatted and re-rendered by the local terminal.
This has performance implications, and probably most
importantly means that a terminal using <code>shpool</code> will feel
completely native. Scrollback and copy-paste will work exactly
as they do in your native terminal, while they can behave differently
when using <code>tmux</code>.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto"><a href="https://github.com/mobile-shell/mosh"><code>mosh</code></a></h3><a id="user-content-mosh" aria-label="Permalink: mosh" href="#mosh"></a></p>
<p dir="auto"><code>mosh</code> is another tool focused on providing persistent remote shell
sessions. It differs from the other tools discussed here in that it
has its own network protocol, which it bootstraps off of regular
ssh. Like <code>tmux</code>, it renders the screen contents remotely and sends
just the current view back. It is somewhat unique in trying to
predicatively guess the right output to display to the user if
there is a network lag.</p>
<p dir="auto"><code>shpool</code> differs from <code>mosh</code> in that it has nothing to do with
the network, remaining confined to a single machine like most of
these other tools. Just like in the case of <code>tmux</code>, <code>mosh</code> will
impact the way scrollback and copy-paste work, while <code>shpool</code>
keeps them feeling entirely native.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto"><a href="https://github.com/crigler/dtach"><code>dtach</code></a>, <a href="https://github.com/martanne/abduco"><code>abduco</code></a>, and <a href="https://github.com/yazgoo/diss"><code>diss</code></a></h3><a id="user-content-dtach-abduco-and-diss" aria-label="Permalink: dtach, abduco, and diss" href="#dtach-abduco-and-diss"></a></p>
<p dir="auto">These tools have the most in common with <code>shpool</code>. Just like <code>shpool</code>, they
eschew multiplexing and just send the raw bytes back to you for your local
terminal to render. While you could say that <code>shpool</code> aims to be a
simpler version of <code>tmux</code>, these tools follow the same philosophy with
an even greater laser focus on simplicity and doing one thing well.</p>
<p dir="auto"><code>shpool</code> aims to be an easy and pleasant experience for people
who just want session persistence without having to care about
it too much, so it has a few more "cushy" features that would
not be as good a fit for the focus on simplicity of these
tools.</p>
<p dir="auto">The most obvious of these features is the difference between
how <code>shpool</code> and these programs handle re-attaches. Though under normal operation,
<code>shpool</code> does not do any rendering and subsetting of the shell
output, it continually maintains an in-memory render of the
terminal state via the <a href="https://crates.io/crates/shpool_vt100" rel="nofollow"><code>shpool_vt100</code></a>
crate. On reattach, <code>shpool</code> will use this in-memory render to
re-draw the screen, so you can easily see where you were when
your connection dropped. This even allows you to see output
generated after your connection dropped.</p>
<p dir="auto">Another such feature is the automatic prompt prefix. <code>shpool</code>
will detect when you are using a known shell (currently
<code>bash</code>, <code>zsh</code>, or <code>fish</code>) and automatically inject a prefix
into your prompt to let you know the name of the <code>shpool</code> session
you are in. This adds some nice context so you don't lose
track of your terminals and have some hint about the current
terminal state.</p>
<p dir="auto">There are also some features <code>shpool</code> is missing which these
programs have. In particular, it seems that <code>dtach</code> and <code>abduco</code>
support shared sessions, while <code>shpool</code> only allows a single
client to be connected to a particular session at a time.
There may be more since I don't know these tools as well
as <code>shpool</code>.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Hacking</h2><a id="user-content-hacking" aria-label="Permalink: Hacking" href="#hacking"></a></p>
<p dir="auto">For information on how to develop shpool, see <a href="https://github.com/shell-pool/shpool/blob/master/HACKING.md">HACKING.md</a>.</p>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[iTerm 3.5.1 removes automatic OpenAI integration, requires opt-in (125 pts)]]></title>
            <link>https://iterm2.com/downloads.html</link>
            <guid>40668803</guid>
            <pubDate>Thu, 13 Jun 2024 12:27:22 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://iterm2.com/downloads.html">https://iterm2.com/downloads.html</a>, See on <a href="https://news.ycombinator.com/item?id=40668803">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
      <h3>Stable Releases</h3>


<hr>


<p>
Stable releases update rarely but have no serious bugs.
</p>


<h4><a href="https://iterm2.com/downloads/stable/iTerm2-3_5_1.zip"><img src="https://iterm2.com/images/Download.png" width="100" height="27">iTerm2 3.5.1 (OS 10.15+)
</a></h4>


<p>
This is the recommended build for most users.
Built on June 11, 2024.
<br>
</p><p>▸ Show Changelog</p>





<p>▸ Show Older Versions</p>




<h3>Test Releases</h3>


<hr>


<p>
Test releases update many times a year and are occasionally unstable.
</p>


<h4><a href="https://iterm2.com/downloads/beta/iTerm2-3_5_1beta4.zip"><img src="https://iterm2.com/images/Download.png" width="100" height="27">iTerm2 3.5.1beta4 (OS 10.15+)
</a></h4>


<p>
This is the recommended beta build for most users.
Built on June 3, 2024.
<br>
</p><p>▸ Show Changelog</p>





<p>▸ Show Older Versions</p>




<h3>Nightly Builds</h3>


<hr>


<p>A nightly build is made at midnight Pacific time on days where a change was committed. The change log may be seen <a href="https://github.com/gnachman/iTerm2/commits/master">on Github</a>. Nightly builds sometimes have serious bugs.
</p>

<h4>
<a href="https://iterm2.com/nightly/latest">
<img src="https://iterm2.com/images/Download.png" width="100" height="35">Latest nightly build
</a></h4>


<p>
Older nightly builds may be found in the <a href="https://iterm2.com/downloads/nightly">nightly build archives.</a>
</p>




    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Southwest Airlines Boeing 737-8 Max Experienced Dutch Roll (239 pts)]]></title>
            <link>https://avherald.com/h?article=519ce679</link>
            <guid>40668504</guid>
            <pubDate>Thu, 13 Jun 2024 11:53:11 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://avherald.com/h?article=519ce679">https://avherald.com/h?article=519ce679</a>, See on <a href="https://news.ycombinator.com/item?id=40668504">Hacker News</a></p>
<div id="readability-page-1" class="page"><div width="100%">
<tbody><tr><td id="adcell">


</td>
<td id="ad1cell">


<!--Begin Content Section-->


<!--Begin Article 519ce679-->
<div><p><span>Accident: Southwest B38M enroute on May 25th 2024, Dutch Roll</span></p><div><span>By Simon Hradecky, created Wednesday, Jun 12th 2024 17:25Z, last updated Wednesday, Jun 12th 2024 17:25Z</span><div><p><span>A Southwest Airlines Boeing 737-8 MAX, registration N8825Q performing flight WN-746 from Phoenix,AZ to Oakland,CA (USA) with 175 passengers and 6 crew, was enroute at FL320 when the aircraft experienced Dutch Roll. The crew was able to regain control and landed the aircraft on Oakland's runway 30 about 55 minutes later.<p>The FAA reported: "AIRCRAFT EXPERIENCED A DUTCH ROLL, REGAINED CONTROL AND POST FLIGHT INSPECTION REVEALED DAMAGE TO THE STANDBY PCU, OAKLAND, CA." and stated the aircraft sustained substantial damage, the occurrence was rated an accident.</p><p>The aircraft remained on the ground in Oakland until Jun 6th 2024, then positioned to Everett,WA (USA), ATS facilities, and is still on the ground in Everett 6 days later.</p><p>Dutch Roll is a coupled out of phase movement of the aircraft as result of weakened directional stability (provided by the vertical tail and rudder), in which the aircraft oscillates around its vertical as well as longitudinal axis (coupled yaw and roll).</p><p>The PCU is the power control unit, an actuator controlling the (vertical) rudder.</p><p><a href="https://flightaware.com/live/flight/SWA746/history/20240525/1425Z/KPHX/KOAK" target="_blank">https://flightaware.com/live/flight/SWA746/history/20240525/1425Z/KPHX/KOAK</a></p></span></p></div></div></div>
<!--End Article-->
<div data-nosnippet=""><hr><hr><div><p><br><span>By (anonymous) on Thursday, Jun 13th 2024 13:05Z</span></p></div>
<hr><div><p><br><span>By Picarus on Thursday, Jun 13th 2024 11:24Z</span></p></div>
<hr><div><p><br><span>By Peter Evers on Thursday, Jun 13th 2024 10:27Z</span></p></div>
<hr><div><p><br><span>By Brian Johnson on Thursday, Jun 13th 2024 08:34Z</span></p></div>
<hr><div><p><br><span>By Kris on Thursday, Jun 13th 2024 03:10Z</span></p></div>
<hr><div><p><br><span>By RJ on Thursday, Jun 13th 2024 02:04Z</span></p></div>
<hr><div><p><br><span>By FBW on Thursday, Jun 13th 2024 01:37Z</span></p></div>
<hr><div><p><br><span>By Raffles on Wednesday, Jun 12th 2024 21:11Z</span></p></div>
<hr><div><p><br><span>By SB on Wednesday, Jun 12th 2024 21:00Z</span></p></div>
<hr><div><p><br><span>By Ducky on Wednesday, Jun 12th 2024 20:17Z</span></p></div>
<hr><div><p><br><span>By JT on Wednesday, Jun 12th 2024 20:08Z</span></p></div>
<hr><div><p><br><span>By john on Wednesday, Jun 12th 2024 19:40Z</span></p></div>
<hr><div><p><br><span>By Paul on Wednesday, Jun 12th 2024 18:54Z</span></p></div>
<hr><div><p><br><span>By ernst on Wednesday, Jun 12th 2024 18:23Z</span></p></div>
<hr><div><p><br><span>By ADS on Wednesday, Jun 12th 2024 17:58Z</span></p></div>
<hr><div><p><br><span>By FBW on Wednesday, Jun 12th 2024 17:57Z</span></p></div>
<hr>
</div><!-- End Content Section--></td>
<td><div data-nosnippet="">
<p><a href="https://avherald.com/h?link=00000038" target="_blank"><img src="https://avherald.com/images/cgaqe_160x160_2024.png" width="160" height="160" alt="Aircraft Cabin Air Conference 2024"></a></p><hr>
<p>The Aviation Herald Apps<br>Android and iOS</p>
<p><a href="https://avherald.com/h?link=00000036"><img src="https://avherald.com/images/avhapp_iphone.jpg" width="160" height="258" alt="AVHAPP on Android and iOS" target="_blank"></a></p><p>Support The Aviation Herald</p>
<p>Euro</p>

<p>US$</p>
<br>
<!--<div class="sitetitle">Monthly support</div>
<div class="sitetitle">1 &euro;/month</div>
<div class="sitetext"><form action="https://www.paypal.com/cgi-bin/webscr" method="post">
<input type="hidden" name="cmd" value="_s-xclick">
<input type="hidden" name="hosted_button_id" value="25LUS2V8Q8YMG">
<input type="hidden" name="return" value="http://avherald.com/h?thanks=">
<input type="image" src="/images/btn_avheraldCC_LG.gif" width="160" height="47" border="0" name="submit" alt="Monthly Support The Aviation Herald">
</form></div><br>-->
<!--<br><div align=center><b>Oct 28 2021<br>We are aware of the evacuation of a Transavia B738 at Amsterdam Schiphol today. The wheel fire occurred during taxi about 10 minutes after landing, no injuries. Therefore ground incident outside our coverage.</b><br></div></br>-->

<p>Interview:</p>

&nbsp;</div></td>
</tr>
</tbody></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Indian Startup 3D Prints Rocket Engine in Just 72 Hours (204 pts)]]></title>
            <link>https://spectrum.ieee.org/3d-printed-rocket</link>
            <guid>40668088</guid>
            <pubDate>Thu, 13 Jun 2024 11:02:11 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://spectrum.ieee.org/3d-printed-rocket">https://spectrum.ieee.org/3d-printed-rocket</a>, See on <a href="https://news.ycombinator.com/item?id=40668088">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-headline="Indian Startup 3D Prints Rocket Engine in Just 72 Hours"><p>A rocket featuring the world’s first rocket engine 3D printed as a single piece blasted off from India’s east coast in late May. Startup <a href="https://agnikul.in/" rel="noopener noreferrer" target="_blank">Agnikul</a> fabricated the engine in just 72 hours and hopes the approach could open the door to “on-demand” rocket launches for operators of <a data-linked-post="2650279684" href="https://spectrum.ieee.org/spacety-startup-plans-news-china-satellites" target="_blank">small satellites</a>.</p><p> The Chennai-based company isn’t the only <a data-linked-post="2658688879" href="https://spectrum.ieee.org/indian-space-private-launch" target="_blank">private space</a> operation to rely heavily on <a data-linked-post="2656527556" href="https://spectrum.ieee.org/3d-printing-could-usher-in-safe-nuclear" target="_blank">3D printing</a>—both <a data-linked-post="2650279177" href="https://spectrum.ieee.org/the-worlds-largest-3d-metal-printer-is-churning-out-rockets" target="_blank">Relativity Space</a><a href="https://www.relativityspace.com/" rel="noopener noreferrer" target="_blank"></a> and <a data-linked-post="2657171481" href="https://spectrum.ieee.org/rocket-booster-rocket-lab" target="_blank">Rocket Lab</a><a href="https://www.rocketlabusa.com/" rel="noopener noreferrer" target="_blank"></a> use the approach extensively to build their <a data-linked-post="2656046543" href="https://spectrum.ieee.org/nasas-space-launch-system-will-lift-off" target="_blank">launch vehicles</a>. What sets Agnikul apart is that its engine is printed in one go, rather than as multiple components that have to then be stitched together, which significantly speeds up manufacturing time.</p><p> On 30 May, the company carried out its first suborbital launch powered by the engine. A single-stage rocket lifted off from the <a href="https://www.isro.gov.in/" target="_blank">Indian Space Research Organisation’s</a> Satish Dhawan Space Center on Sriharikota island in Andhra Pradesh, reaching an altitude of 6.5 kilometers before splashing down into the ocean.</p><p><span data-rm-shortcode-id="47c210ba720d4009542b5774c43a4067"><iframe type="lazy-iframe" data-runner-src="https://www.youtube.com/embed/3J2Lnck_vgU?rel=0" width="100%" height="auto" frameborder="0" scrolling="no"></iframe></span><small placeholder="Add Photo Caption...">The launch of Agnibaan SOrTeD</small></p><p> “It performed very successfully,” says cofounder and chief operating officer <a href="https://www.linkedin.com/in/moin-spm-53a80342/?originalSubdomain=in" target="_blank">Moin SPM</a>. “It met all the objectives of the mission so we have a lot of confidence in the technologies that we have built.”</p><p> The company’s first commercial product will be a two-stage rocket called <a href="https://agnikul.in/#/products" target="_blank">Agnibaan</a>, which will be 18 meters tall, feature eight engines in total and able to carry a 300-kilogram payload to an altitude of around 700 km. The launch vehicle used in May’s test was only 6 meters tall and featured just a single engine, making it roughly equivalent to Agnibaan’s second stage.</p><p> The launch acted as a technology demonstrator to test out all of the key subsystems necessary for an orbital launch. Those included the flight computer, avionics, guidance, and navigation systems, as well as the launchpad itself, which was purpose-built for the mission.</p><p> The team hit its target of 6 kilonewtons of thrust and was able to successfully carry out a wind-biasing maneuver, in which the rockets trajectory is adjusted midflight to account for the affects of wind. Besides validating the technology, SPM says they gained valuable experience in both manufacturing processes and launch operations.</p><p data-rm-resized-container="25%"><img id="4f373" data-rm-shortcode-id="bc254dae3e83b0307e0f9c44f06365c2" data-rm-shortcode-name="rebelmouse-image" lazy-loadable="true" src="https://spectrum.ieee.org/media-library/a-yellowish-metallic-3d-printed-rocket-engine-sits-on-a-table.jpg?id=52436307&amp;width=980" data-runner-src="https://spectrum.ieee.org/media-library/a-yellowish-metallic-3d-printed-rocket-engine-sits-on-a-table.jpg?id=52436307&amp;width=980" width="2000" height="2667" alt="A yellowish metallic 3D printed rocket engine sits on a table."><small placeholder="Add Photo Caption...">This 3D-printed rocket engine was used in Agnikul’s first successful launch.</small><small placeholder="Add Photo Credit...">Agnikul</small></p><p> The launch also vindicated the company’s unconventional manufacturing approach. Constructing a rocket engine using conventional approaches can take months, followed by extensive qualification testing to ensure it meets the required specifications. Using a metal 3D printer from German company <a href="https://www.eos.info/en" rel="noopener noreferrer" target="_blank">EOS</a>, Agnikul produced its engine in roughly three days. Agnikul printed the engine out of <a href="https://en.wikipedia.org/wiki/Inconel" target="_blank">inconel</a>, a high-performance alloy of nickel and chromium that can withstand high temperatures and mechanical loads. The machine also automatically outputs a report that details any deviations during printing, removing the need for postfabrication qualification.</p><p> Assembling the rest of the rocket and integrating the engine took roughly two weeks. The company says that opens the door to providing low-cost, <a href="https://spectrum.ieee.org/3d-printed-rockets-india-agnikul" target="_blank">“on-demand”</a> launch services to <a data-linked-post="2650279771" href="https://spectrum.ieee.org/how-small-satellites-are-providing-lowcost-access-to-space" target="_blank">operators of small satellites</a>, which otherwise need to wait for a ride share on a bigger rocket.</p><p> The big challenge now will be going from a single engine to a cluster of seven on Agnibaan’s first stage, says cofounder and CEO <a href="https://www.linkedin.com/in/srinath-ravichandran-09679a7/?originalSubdomain=in" target="_blank">Srinath Ravichandran</a>. This raises all kinds of challenges, from balancing thrust across the engines at lift-off to managing engine plume interactions when the engines gimbal to alter the trajectory. “But these are problems that people have figured out,” he says. “We believe that we should just be able to fine-tune it for our mission and go.”</p><p> The company is currently building facilities to carry out ground tests of engine clusters, says Ravichandran, and is targeting its first orbital launch for this time next year.</p><p><em>This post was updated in 12 June to add further detail about the material that Agnikul is using to 3D print its rocket engines.</em><br></p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Microsoft Chose Profit over Security, Whistleblower Says (202 pts)]]></title>
            <link>https://www.propublica.org/article/microsoft-solarwinds-golden-saml-data-breach-russian-hackers</link>
            <guid>40667976</guid>
            <pubDate>Thu, 13 Jun 2024 10:39:45 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.propublica.org/article/microsoft-solarwinds-golden-saml-data-breach-russian-hackers">https://www.propublica.org/article/microsoft-solarwinds-golden-saml-data-breach-russian-hackers</a>, See on <a href="https://news.ycombinator.com/item?id=40667976">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-pp-location="article body">

        
                    <div data-pp-location="top-note">
                

                                                
            <p>ProPublica is a nonprofit newsroom that investigates abuses of power. Sign up to receive <a href="https://www.propublica.org/newsletters/the-big-story?source=www.propublica.org&amp;placement=top-note&amp;region=national">our biggest stories</a> as soon as they’re published.</p>

                

            </div><!-- end .article-body__top-notes -->
        
        




                    
<p data-pp-blocktype="copy" data-pp-id="2.0">Microsoft hired Andrew Harris for his extraordinary skill in keeping hackers out of the nation’s most sensitive computer networks. In 2016, Harris was hard at work on a mystifying incident in which intruders had somehow penetrated a major U.S. tech company.</p>
        
    
                        


   
            
            
            
            

   
        
    
                    
<p data-pp-blocktype="copy" data-pp-id="4.0">The breach troubled Harris for two reasons. First, it involved the company’s cloud — a virtual storehouse typically containing an organization’s most sensitive data. Second, the attackers had pulled it off in a way that left little trace.</p>
        
    
                    
<p data-pp-blocktype="copy" data-pp-id="5.0">He retreated to his home office to “war game” possible scenarios, stress-testing the various software products that could have been compromised.</p>
        
    
                    
<p data-pp-blocktype="copy" data-pp-id="6.0">Early on, he focused on a Microsoft application that ensured users had permission to log on to cloud-based programs, the cyber equivalent of an officer checking passports at a border. It was there, after months of research, that he found something seriously wrong.</p>
        
    
                    
<p data-pp-blocktype="copy" data-pp-id="7.0">The product, which was used by millions of people to log on to their work computers, contained a flaw that could allow attackers to masquerade as legitimate employees and rummage through victims’ “crown jewels” — national security secrets, corporate intellectual property, embarrassing personal emails — all without tripping alarms.</p>
        
    
                    
<p data-pp-blocktype="copy" data-pp-id="8.0">To Harris, who had previously spent nearly seven years working for the Defense Department, it was a security nightmare. Anyone using the software was exposed, regardless of whether they used Microsoft or another cloud provider such as Amazon. But Harris was most concerned about the federal government and the implications of his discovery for national security. He flagged the issue to his colleagues.</p>
        
    
                    
        
    
                    
<p data-pp-blocktype="copy" data-pp-id="10.0">They saw it differently, Harris said. The federal government was preparing to make a massive investment in cloud computing, and Microsoft wanted the business. Acknowledging this security flaw could jeopardize the company’s chances, Harris recalled one product leader telling him. The financial consequences were enormous. Not only could Microsoft lose a multibillion-dollar deal, but it could also lose the race to dominate the market for cloud computing.</p>
        
    
                    
<p data-pp-blocktype="copy" data-pp-id="11.0">Harris said he pleaded with the company for several years to address the flaw in the product, a ProPublica investigation has found. But at every turn, Microsoft dismissed his warnings, telling him they would work on a long-term alternative — leaving cloud services around the globe vulnerable to attack in the meantime.</p>
        
    
                    
<p data-pp-blocktype="copy" data-pp-id="12.0">Harris was certain someone would figure out how to exploit the weakness. He’d come up with a temporary solution, but it required customers to turn off one of Microsoft’s most convenient and popular features: the ability to access nearly every program used at work with a single logon.</p>
        
    
                    
<p data-pp-blocktype="copy" data-pp-id="13.0">He scrambled to alert some of the company’s most sensitive customers about the threat and personally oversaw the fix for the New York Police Department. Frustrated by Microsoft’s inaction, he left the company in August 2020.</p>
        
    
                    

<figure data-pp-id="14" data-pp-blocktype="image">

    


                    
    


    <img alt="" width="1062" height="1660" loading="lazy" js-autosizes="" src="https://img.assets-d.propublica.org/v5/images/2024-tech-project-harris-badge_preview_maxWidth_3000_maxHeight_3000_ppi_72_embedColorProfile_true_quality_95.jpg?crop=focalpoint&amp;fit=crop&amp;fm=webp&amp;fp-x=0.5&amp;fp-y=0.5&amp;h=1250&amp;q=75&amp;w=800&amp;s=347fe7b1ed331d98ed0ab66e262efc9f" srcset="https://img.assets-d.propublica.org/v5/images/2024-tech-project-harris-badge_preview_maxWidth_3000_maxHeight_3000_ppi_72_embedColorProfile_true_quality_95.jpg?crop=focalpoint&amp;fit=crop&amp;fm=webp&amp;fp-x=0.5&amp;fp-y=0.5&amp;h=625&amp;q=75&amp;w=400&amp;s=13665e6a08853763b63f7d6c0b463487 400w, https://img.assets-d.propublica.org/v5/images/2024-tech-project-harris-badge_preview_maxWidth_3000_maxHeight_3000_ppi_72_embedColorProfile_true_quality_95.jpg?crop=focalpoint&amp;fit=crop&amp;fm=webp&amp;fp-x=0.5&amp;fp-y=0.5&amp;h=1250&amp;q=75&amp;w=800&amp;s=347fe7b1ed331d98ed0ab66e262efc9f 800w, https://img.assets-d.propublica.org/v5/images/2024-tech-project-harris-badge_preview_maxWidth_3000_maxHeight_3000_ppi_72_embedColorProfile_true_quality_95.jpg?crop=focalpoint&amp;fit=crop&amp;fm=webp&amp;fp-x=0.5&amp;fp-y=0.5&amp;h=1876&amp;q=75&amp;w=1200&amp;s=608fc692923d4caef27c6033e1a1fb01 1200w, https://img.assets-d.propublica.org/v5/images/2024-tech-project-harris-badge_preview_maxWidth_3000_maxHeight_3000_ppi_72_embedColorProfile_true_quality_95.jpg?crop=focalpoint&amp;fit=crop&amp;fm=webp&amp;fp-x=0.5&amp;fp-y=0.5&amp;h=2032&amp;q=75&amp;w=1300&amp;s=4a7abb4c19014cc31d7f295c1ec39dc6 1300w, https://img.assets-d.propublica.org/v5/images/2024-tech-project-harris-badge_preview_maxWidth_3000_maxHeight_3000_ppi_72_embedColorProfile_true_quality_95.jpg?crop=focalpoint&amp;fit=crop&amp;fm=webp&amp;fp-x=0.5&amp;fp-y=0.5&amp;h=2266&amp;q=75&amp;w=1450&amp;s=be7ae31a95f7cd90dde1c4f5019eed7a 1450w, https://img.assets-d.propublica.org/v5/images/2024-tech-project-harris-badge_preview_maxWidth_3000_maxHeight_3000_ppi_72_embedColorProfile_true_quality_95.jpg?crop=focalpoint&amp;fit=crop&amp;fm=webp&amp;fp-x=0.5&amp;fp-y=0.5&amp;h=2501&amp;q=75&amp;w=1600&amp;s=4717bb35a0047345972ba442c2a17a16 1600w, https://img.assets-d.propublica.org/v5/images/2024-tech-project-harris-badge_preview_maxWidth_3000_maxHeight_3000_ppi_72_embedColorProfile_true_quality_95.jpg?crop=focalpoint&amp;fit=crop&amp;fm=webp&amp;fp-x=0.5&amp;fp-y=0.5&amp;h=3126&amp;q=75&amp;w=2000&amp;s=e357f50008eb1ca4e3ab3c386e6be69c 2000w">

            
    
<figcaption>
        <span>Andrew Harris shared his Microsoft employee badge on his LinkedIn page when he announced his departure from the company in 2020.</span>
    
        <span>
        <span>Credit: </span>
        Screenshot by ProPublica
    </span>
    
    
    </figcaption>

</figure>

        
    
                    
<p data-pp-blocktype="copy" data-pp-id="15.0">Within months, his fears became reality. U.S. officials <a href="https://www.whitehouse.gov/briefing-room/statements-releases/2021/04/15/fact-sheet-imposing-costs-for-harmful-foreign-activities-by-the-russian-government/">confirmed reports</a> that a state-sponsored team of Russian hackers had carried out SolarWinds, one of the <a href="https://www.cbsnews.com/news/solarwinds-hack-russia-cyberattack-60-minutes-2021-07-04/">largest cyberattacks</a> in U.S. history. They used the flaw Harris had identified to vacuum up sensitive data from a number of federal agencies, including, ProPublica has learned, the National Nuclear Security Administration, which maintains the United States’ nuclear weapons stockpile, and the National Institutes of Health, which at the time was engaged in COVID-19 research and vaccine distribution. The Russians also used the weakness to compromise dozens of email accounts in the Treasury Department, <a href="https://www.finance.senate.gov/ranking-members-news/wyden-statement-following-treasury-and-irs-briefing-on-solarwinds-hack">including those of its highest-ranking officials</a>. One federal official <a href="https://www.wsj.com/articles/suspected-russian-hack-extends-far-beyond-solarwinds-software-investigators-say-11611921601">described the breach</a> as “an espionage campaign designed for long-term intelligence collection.”</p>
        
    
                    
<p data-pp-blocktype="copy" data-pp-id="16.0">Harris’ account, told here for the first time and supported by interviews with former colleagues and associates as well as social media posts, upends the prevailing public understanding of the SolarWinds hack.</p>
        
    
                    
<p data-pp-blocktype="copy" data-pp-id="17.0">From the moment the hack surfaced, Microsoft insisted it was blameless. Microsoft President Brad Smith <a href="https://www.intelligence.senate.gov/sites/default/files/documents/qfr-bsmith-022321.pdf">assured Congress in 2021</a> that “there was no vulnerability in any Microsoft product or service that was exploited” in SolarWinds.</p>
        
    
                    
        
    
                    
<p data-pp-blocktype="copy" data-pp-id="19.0">He also said customers could have done more to protect themselves.</p>

<p data-pp-blocktype="copy" data-pp-id="19.1">Harris said they were never given the chance.</p>
        
    
                    
<p data-pp-blocktype="copy" data-pp-id="20.0">“The decisions are not based on what’s best for Microsoft’s customers but on what’s best for Microsoft,” said Harris, who now works for CrowdStrike, a cybersecurity company that competes with Microsoft.</p>
        
    
                    
<p data-pp-blocktype="copy" data-pp-id="21.0">Microsoft declined to make Smith and other top officials available for interviews for this story, but it did not dispute ProPublica’s findings. Instead, <a href="https://www.documentcloud.org/documents/24742779-microsoft-statement">the company issued a statement</a> in response to written questions. “Protecting customers is always our highest priority,” a spokesperson said. “Our security response team takes all security issues seriously and gives every case due diligence with a thorough manual assessment, as well as cross-confirming with engineering and security partners. Our assessment of this issue received multiple reviews and was aligned with the industry consensus.”</p>
        
    
                    
<p data-pp-blocktype="copy" data-pp-id="22.0">ProPublica’s investigation comes as the Pentagon <a href="https://www.axios.com/2024/05/17/pentagon-weighs-microsoft-licensing-upgrades">seeks to expand its use of Microsoft products</a> — a move that has <a href="https://regmedia.co.uk/2024/06/04/schmitt_wyden_dod_microsoft_e5.pdf">drawn scrutiny</a> from federal lawmakers amid a series of cyberattacks on the government.</p>
        
    
                    
<p data-pp-blocktype="copy" data-pp-id="23.0">Smith is set to testify on Thursday before <a href="https://homeland.house.gov/hearing/a-cascade-of-security-failures-assessing-microsoft-corporations-cybersecurity-shortfalls-and-the-implications-for-homeland-security/">the House Homeland Security Committee</a>, which is examining Microsoft’s role in a breach perpetrated last year by hackers connected to the Chinese government. Attackers exploited Microsoft security flaws to gain access to top U.S. officials’ emails. In investigating the attack, the federal Cyber Safety Review Board <a href="https://www.cisa.gov/resources-tools/resources/cyber-safety-review-board-releases-report-microsoft-online-exchange-incident-summer-2023">found</a> that Microsoft’s “security culture was inadequate and requires an overhaul.”</p>
        
    
                    

<figure data-pp-id="24" data-pp-blocktype="image">

    


                    
    


    <img alt="" width="3000" height="2000" loading="lazy" js-autosizes="" src="https://img.assets-d.propublica.org/v5/images/GettyImages-1231344943_preview_maxWidth_3000_maxHeight_3000_ppi_72_embedColorProfile_true_quality_95.jpg?crop=focalpoint&amp;fit=crop&amp;fm=webp&amp;fp-x=0.5&amp;fp-y=0.5&amp;h=533&amp;q=75&amp;w=800&amp;s=b41b841faa27cff1464b1cfd66364a3e" srcset="https://img.assets-d.propublica.org/v5/images/GettyImages-1231344943_preview_maxWidth_3000_maxHeight_3000_ppi_72_embedColorProfile_true_quality_95.jpg?crop=focalpoint&amp;fit=crop&amp;fm=webp&amp;fp-x=0.5&amp;fp-y=0.5&amp;h=267&amp;q=75&amp;w=400&amp;s=3811d27f029a74164e3d3e0c2f625981 400w, https://img.assets-d.propublica.org/v5/images/GettyImages-1231344943_preview_maxWidth_3000_maxHeight_3000_ppi_72_embedColorProfile_true_quality_95.jpg?crop=focalpoint&amp;fit=crop&amp;fm=webp&amp;fp-x=0.5&amp;fp-y=0.5&amp;h=533&amp;q=75&amp;w=800&amp;s=b41b841faa27cff1464b1cfd66364a3e 800w, https://img.assets-d.propublica.org/v5/images/GettyImages-1231344943_preview_maxWidth_3000_maxHeight_3000_ppi_72_embedColorProfile_true_quality_95.jpg?crop=focalpoint&amp;fit=crop&amp;fm=webp&amp;fp-x=0.5&amp;fp-y=0.5&amp;h=800&amp;q=75&amp;w=1200&amp;s=e5cd9cea67d8764d227517f60b9847bb 1200w, https://img.assets-d.propublica.org/v5/images/GettyImages-1231344943_preview_maxWidth_3000_maxHeight_3000_ppi_72_embedColorProfile_true_quality_95.jpg?crop=focalpoint&amp;fit=crop&amp;fm=webp&amp;fp-x=0.5&amp;fp-y=0.5&amp;h=867&amp;q=75&amp;w=1300&amp;s=3e347f880737055ad52213d7b100c50f 1300w, https://img.assets-d.propublica.org/v5/images/GettyImages-1231344943_preview_maxWidth_3000_maxHeight_3000_ppi_72_embedColorProfile_true_quality_95.jpg?crop=focalpoint&amp;fit=crop&amp;fm=webp&amp;fp-x=0.5&amp;fp-y=0.5&amp;h=967&amp;q=75&amp;w=1450&amp;s=27667a7dffc08daacff5e6a637c545d2 1450w, https://img.assets-d.propublica.org/v5/images/GettyImages-1231344943_preview_maxWidth_3000_maxHeight_3000_ppi_72_embedColorProfile_true_quality_95.jpg?crop=focalpoint&amp;fit=crop&amp;fm=webp&amp;fp-x=0.5&amp;fp-y=0.5&amp;h=1067&amp;q=75&amp;w=1600&amp;s=15edc589b0616e03a3f1c1b8059afef0 1600w, https://img.assets-d.propublica.org/v5/images/GettyImages-1231344943_preview_maxWidth_3000_maxHeight_3000_ppi_72_embedColorProfile_true_quality_95.jpg?crop=focalpoint&amp;fit=crop&amp;fm=webp&amp;fp-x=0.5&amp;fp-y=0.5&amp;h=1333&amp;q=75&amp;w=2000&amp;s=91ad733f8e4af8242228e80f752459c2 2000w">

            
    
<figcaption>
        <span>Microsoft President Brad Smith testifies during a Senate Select Committee on Intelligence hearing about SolarWinds on Feb. 23, 2021.</span>
    
        <span>
        <span>Credit: </span>
        Drew Angerer/Getty Images
    </span>
    
    
    </figcaption>

</figure>

        
    
                    
<p data-pp-blocktype="copy" data-pp-id="25.0">For its part, Microsoft has said that work has already begun, declaring that <a href="https://www.microsoft.com/en-us/security/blog/2024/05/03/security-above-all-else-expanding-microsofts-secure-future-initiative/">the company’s top priority is security</a> “above all else.” Part of the effort involves adopting the board’s recommendations. “If you’re faced with the tradeoff between security and another priority, your answer is clear: Do security,” the company’s CEO, Satya Nadella, told employees in the wake of the board’s report, which identified a “corporate culture that deprioritized both enterprise security investments and rigorous risk management.”</p>
        
    
                    
<p data-pp-blocktype="copy" data-pp-id="26.0">ProPublica’s investigation adds new details and pivotal context about that culture, offering an unsettling look into how the world’s largest software provider handles the security of its own ubiquitous products. It also offers crucial insight into just how much the quest for profits can drive those security decisions, especially as tech behemoths push to dominate the newest — and most lucrative — frontiers, including the cloud market.</p>
        
    
                    
<p data-pp-blocktype="copy" data-pp-id="27.0">“This is part of the problem overall with the industry,” said Nick DiCola, who was one of Harris’ bosses at Microsoft and now works at Zero Networks, a network security firm. Publicly-traded tech giants “are beholden to the share price, not to doing what’s right for the customer all the time. That’s just a reality of capitalism. You’re never going to change that in a public company because at the end of the day, they want the shareholder value to go up.”</p>
        
    
                    
<h3>A “Cloud-First World”</h3>

        
    
                    
<p data-pp-blocktype="copy" data-pp-id="29.0">Early this year, Microsoft surpassed Apple to become the world’s most valuable company, worth more than $3 trillion. That triumph was almost unimaginable a decade ago. (The two remain in close competition for the top spot.)</p>
        
    
                    
<p data-pp-blocktype="copy" data-pp-id="30.0">In 2014, the same year that Harris joined Microsoft and Nadella became the CEO, Wall Street and consumers alike viewed the company as stuck in the past, clinging to the “shrink-wrapped” software products like Windows that put it on the map in the 1990s. Microsoft’s long-stagnant share price reflected its status as an also-ran in almost every major technological <a href="https://www.ft.com/content/0e8c3002-20c7-11ea-92da-f0c92e957a96">breakthrough</a> since the turn of the century, from its Bing search engine to its Nokia mobile phone division.</p>
        
    
                    
<p data-pp-blocktype="copy" data-pp-id="31.0">As the new CEO, Nadella was determined to reverse the trend and shake off the company’s fuddy-duddy reputation, so he staked Microsoft’s future on the Azure cloud computing division, which then lagged far behind Amazon. In his earliest all-staff memo, Nadella told employees they would need “to reimagine a lot of what we have done in the past for a … <a href="https://www.geekwire.com/2014/satya-nadellas-full-memo-microsoft-staffers-need-believe-impossible-remove-improbable/">cloud-first world</a>.”</p>
        
    
                    

<figure data-pp-id="32" data-pp-blocktype="image">

    


                    
    


    <img alt="" width="3000" height="1997" loading="lazy" js-autosizes="" src="https://img.assets-d.propublica.org/v5/images/GettyImages-480900319_preview_maxWidth_3000_maxHeight_3000_ppi_72_embedColorProfile_true_quality_95.jpg?crop=focalpoint&amp;fit=crop&amp;fm=webp&amp;fp-x=0.5&amp;fp-y=0.5&amp;h=533&amp;q=75&amp;w=800&amp;s=d8df1bc01ab70929d30d241bdff377d3" srcset="https://img.assets-d.propublica.org/v5/images/GettyImages-480900319_preview_maxWidth_3000_maxHeight_3000_ppi_72_embedColorProfile_true_quality_95.jpg?crop=focalpoint&amp;fit=crop&amp;fm=webp&amp;fp-x=0.5&amp;fp-y=0.5&amp;h=266&amp;q=75&amp;w=400&amp;s=85562fb93a8753372f39ad6c335e8c40 400w, https://img.assets-d.propublica.org/v5/images/GettyImages-480900319_preview_maxWidth_3000_maxHeight_3000_ppi_72_embedColorProfile_true_quality_95.jpg?crop=focalpoint&amp;fit=crop&amp;fm=webp&amp;fp-x=0.5&amp;fp-y=0.5&amp;h=533&amp;q=75&amp;w=800&amp;s=d8df1bc01ab70929d30d241bdff377d3 800w, https://img.assets-d.propublica.org/v5/images/GettyImages-480900319_preview_maxWidth_3000_maxHeight_3000_ppi_72_embedColorProfile_true_quality_95.jpg?crop=focalpoint&amp;fit=crop&amp;fm=webp&amp;fp-x=0.5&amp;fp-y=0.5&amp;h=799&amp;q=75&amp;w=1200&amp;s=40b86ef673bf6a63e36a045f37898806 1200w, https://img.assets-d.propublica.org/v5/images/GettyImages-480900319_preview_maxWidth_3000_maxHeight_3000_ppi_72_embedColorProfile_true_quality_95.jpg?crop=focalpoint&amp;fit=crop&amp;fm=webp&amp;fp-x=0.5&amp;fp-y=0.5&amp;h=865&amp;q=75&amp;w=1300&amp;s=58e80d6a692ba0c35fe6879ae29d79f0 1300w, https://img.assets-d.propublica.org/v5/images/GettyImages-480900319_preview_maxWidth_3000_maxHeight_3000_ppi_72_embedColorProfile_true_quality_95.jpg?crop=focalpoint&amp;fit=crop&amp;fm=webp&amp;fp-x=0.5&amp;fp-y=0.5&amp;h=965&amp;q=75&amp;w=1450&amp;s=49d44d2efaaf8deaa7234a7a49bf70d1 1450w, https://img.assets-d.propublica.org/v5/images/GettyImages-480900319_preview_maxWidth_3000_maxHeight_3000_ppi_72_embedColorProfile_true_quality_95.jpg?crop=focalpoint&amp;fit=crop&amp;fm=webp&amp;fp-x=0.5&amp;fp-y=0.5&amp;h=1065&amp;q=75&amp;w=1600&amp;s=228def8f190e1e6d44f8c79ff573c55a 1600w, https://img.assets-d.propublica.org/v5/images/GettyImages-480900319_preview_maxWidth_3000_maxHeight_3000_ppi_72_embedColorProfile_true_quality_95.jpg?crop=focalpoint&amp;fit=crop&amp;fm=webp&amp;fp-x=0.5&amp;fp-y=0.5&amp;h=1331&amp;q=75&amp;w=2000&amp;s=f9f70caa8beb96a842373c5ce6b3f522 2000w">

            
    
<figcaption>
        <span>Microsoft CEO Satya Nadella promotes the company’s cloud offerings at an event in San Francisco in 2014.</span>
    
        <span>
        <span>Credit: </span>
        David Paul Morris/Bloomberg via Getty Images
    </span>
    
    
    </figcaption>

</figure>

        
    
                    
<p data-pp-blocktype="copy" data-pp-id="33.0">Microsoft salespeople pitched business and government customers on a “hybrid cloud” strategy, where they kept some traditional, on-premises servers (typically stored on racks in customers’ own offices) while shifting most of their computing needs to the cloud (hosted on servers in Microsoft data centers).</p>
        
    
                    
<p data-pp-blocktype="copy" data-pp-id="34.0">Security was a key selling point for the cloud. On-site servers were notoriously vulnerable, in part because organizations’ overburdened IT staff often failed to promptly install the required patches and updates. With the cloud, that crucial work was handled by dedicated employees whose job was security.</p>
        
    
                    
<p data-pp-blocktype="copy" data-pp-id="35.0">The dawn of the cloud era at Microsoft was an exciting time to work in the field of cybersecurity for someone like Harris, whose high school yearbook features a photo of him in front of a desktop computer and monitor with a mess of floppy disks beside him. One hand is on the keyboard, the other on a wired mouse. Caption: “Harris the hacker.”</p>
        
    
                    

<figure data-pp-id="36" data-pp-blocktype="image">

    


                    
    


    <img alt="" width="1715" height="1011" loading="lazy" js-autosizes="" src="https://img.assets-d.propublica.org/v5/images/harris-yearbook-2-from-classmates-dot-com_preview_maxWidth_3000_maxHeight_3000_ppi_72_embedColorProfile_true_quality_95.jpg?crop=focalpoint&amp;fit=crop&amp;fm=webp&amp;fp-x=0.5&amp;fp-y=0.5&amp;h=472&amp;q=75&amp;w=800&amp;s=f74d5e3a8c3df8f87d54eee093b709e6" srcset="https://img.assets-d.propublica.org/v5/images/harris-yearbook-2-from-classmates-dot-com_preview_maxWidth_3000_maxHeight_3000_ppi_72_embedColorProfile_true_quality_95.jpg?crop=focalpoint&amp;fit=crop&amp;fm=webp&amp;fp-x=0.5&amp;fp-y=0.5&amp;h=236&amp;q=75&amp;w=400&amp;s=858cdea91df24d12544b7b0476eb3519 400w, https://img.assets-d.propublica.org/v5/images/harris-yearbook-2-from-classmates-dot-com_preview_maxWidth_3000_maxHeight_3000_ppi_72_embedColorProfile_true_quality_95.jpg?crop=focalpoint&amp;fit=crop&amp;fm=webp&amp;fp-x=0.5&amp;fp-y=0.5&amp;h=472&amp;q=75&amp;w=800&amp;s=f74d5e3a8c3df8f87d54eee093b709e6 800w, https://img.assets-d.propublica.org/v5/images/harris-yearbook-2-from-classmates-dot-com_preview_maxWidth_3000_maxHeight_3000_ppi_72_embedColorProfile_true_quality_95.jpg?crop=focalpoint&amp;fit=crop&amp;fm=webp&amp;fp-x=0.5&amp;fp-y=0.5&amp;h=707&amp;q=75&amp;w=1200&amp;s=c6aa317725f25d3ee09272c1bfa3f920 1200w, https://img.assets-d.propublica.org/v5/images/harris-yearbook-2-from-classmates-dot-com_preview_maxWidth_3000_maxHeight_3000_ppi_72_embedColorProfile_true_quality_95.jpg?crop=focalpoint&amp;fit=crop&amp;fm=webp&amp;fp-x=0.5&amp;fp-y=0.5&amp;h=766&amp;q=75&amp;w=1300&amp;s=4dedb158322d3a2c9e23aea6a36e0e1e 1300w, https://img.assets-d.propublica.org/v5/images/harris-yearbook-2-from-classmates-dot-com_preview_maxWidth_3000_maxHeight_3000_ppi_72_embedColorProfile_true_quality_95.jpg?crop=focalpoint&amp;fit=crop&amp;fm=webp&amp;fp-x=0.5&amp;fp-y=0.5&amp;h=855&amp;q=75&amp;w=1450&amp;s=47c3dca69779e067df2386c499b2ca47 1450w, https://img.assets-d.propublica.org/v5/images/harris-yearbook-2-from-classmates-dot-com_preview_maxWidth_3000_maxHeight_3000_ppi_72_embedColorProfile_true_quality_95.jpg?crop=focalpoint&amp;fit=crop&amp;fm=webp&amp;fp-x=0.5&amp;fp-y=0.5&amp;h=943&amp;q=75&amp;w=1600&amp;s=2872c3e68e6a0b62b0cff9d53f1cf972 1600w, https://img.assets-d.propublica.org/v5/images/harris-yearbook-2-from-classmates-dot-com_preview_maxWidth_3000_maxHeight_3000_ppi_72_embedColorProfile_true_quality_95.jpg?crop=focalpoint&amp;fit=crop&amp;fm=webp&amp;fp-x=0.5&amp;fp-y=0.5&amp;h=1179&amp;q=75&amp;w=2000&amp;s=f13ff7f6e4eb9f59788bc70335c6d081 2000w">

            
    
<figcaption>
        <span>Harris’ high school yearbook</span>
    
        <span>
        <span>Credit: </span>
        Classmates.com
    </span>
    
    
    </figcaption>

</figure>

        
    
                    
<p data-pp-blocktype="copy" data-pp-id="37.0">As a sophomore at Pace University in New York, he wrote a white paper titled “How to Hack the Wired Equivalent Protocol,” a network security standard, and was awarded a prestigious Defense Department scholarship, which the government uses to recruit cybersecurity specialists. The National Security Agency paid for three years of his tuition, which included a master’s degree in software engineering, in exchange for a commitment to work for the government for at least that long, he said.</p>
        
    
                    
<p data-pp-blocktype="copy" data-pp-id="38.0">Early in his career, he helped lead the Defense Department’s efforts to protect individual devices. He became an expert in the niche field known as identity and access management, securing how people log in.</p>
        
    
                    
<p data-pp-blocktype="copy" data-pp-id="39.0">As the years wore on, he grew frustrated by the lumbering bureaucracy and craved the innovation of the tech industry. He decided he could make a bigger impact in the private sector, which designed much of the software the government used.</p>
        
    
                    
<p data-pp-blocktype="copy" data-pp-id="40.0">At Microsoft he was assigned to a secretive unit known as the “Ghostbusters” (as in: “Who you gonna call?”), which responded to hacks of the company’s most sensitive customers, especially the federal government. As a member of this team, Harris first investigated the puzzling attack on the tech company and remained obsessed with it, even after switching roles inside Microsoft.</p>
        
    
                    
<p data-pp-blocktype="copy" data-pp-id="41.0">Eventually, he confirmed the weakness within Active Directory Federation Services, or AD FS, a product that allowed users to sign on a single time to access nearly everything they needed. The problem, he discovered, rested in how the application used a computer language known as SAML to authenticate users as they logged in.</p>
        
            
    
    
    
                            

<figure data-pp-id="1" data-pp-blocktype="embed">

    


                        
    
    
    
    
    
    
    
    
            
    
<figcaption>
    
        <span>
        <span>Credit: </span>
        Illustrations by Anuj Shrestha, special to ProPublica
    </span>
    
    
    </figcaption>


</figure>

            
        
    
                    
<p data-pp-blocktype="copy" data-pp-id="43.0">This is what makes a SAML attack unique. Typically, hackers leave what cybersecurity specialists call a “noisy” digital trail. Network administrators monitoring the so-called “audit logs” might see unknown or foreign IP addresses attempting to gain access to their cloud services. But SAML attacks are much harder to detect. The forged token is the equivalent of a robber using a copied master key. There was little trail to track, just the activities of what appear to be legitimate users.</p>
        
    
                    
<p data-pp-blocktype="copy" data-pp-id="44.0">Harris and a colleague who consulted for the Department of Defense spent hours in front of both real and virtual whiteboards as they mapped out how such an attack would work, the colleague told ProPublica. The “token theft” risk, as Harris referred to it, became a regular topic of discussion for them.</p>
        
    
                    
<h3>A Clash With “Won’t Fix” Culture</h3>

        
    
                    
<p data-pp-blocktype="copy" data-pp-id="46.0">Before long, Harris alerted his supervisors about his SAML finding. Nick DiCola, his boss at the time, told ProPublica he referred Harris to the Microsoft Security Response Center, which fields reports of security vulnerabilities and determines which need to be addressed. Given its central role in improving Microsoft product security, the team once considered itself the “conscience of the company,” urging colleagues to improve security without regard to profit. In a meeting room, someone hung a framed photo of Winston “the Wolf,” the charismatic fixer in Quentin Tarantino’s movie “Pulp Fiction” who is summoned to clean up the aftermath of bloody hits.</p>
        
    
                    
<p data-pp-blocktype="copy" data-pp-id="47.0">Members of the team were not always popular within the company. Plugging security holes is a cost center, and making new products is a profit center, former employees told ProPublica. In 2002, the company’s founder, Bill Gates, tried to settle the issue, <a href="https://news.microsoft.com/2012/01/11/memo-from-bill-gates/">sending a memo</a> that turned out to be eerily prescient. “Flaws in a single Microsoft product, service or policy not only affect the quality of our platform and services overall, but also our customers’ view of us as a company,” Gates wrote, adding: “So now, when we face a choice between adding features and resolving security issues, we need to choose security.”</p>
        
    
                    
<p data-pp-blocktype="copy" data-pp-id="48.0">At first, Gates’ memo was transformational and the company’s product divisions were more responsive to the center’s concerns. But over time, the center’s influence waned.</p>
        
    
                    
<p data-pp-blocktype="copy" data-pp-id="49.0">Its members were stuck between cultural forces. Security researchers — often characterized as having outsized egos — believed their findings should be immediately addressed, underestimating the business challenges of developing fixes quickly, former MSRC employees told ProPublica.</p>
        
    
                    
<p data-pp-blocktype="copy" data-pp-id="50.0">Product managers had little motivation to act fast, if at all, since compensation was tied to the release of new, revenue-generating products and features. That attitude was particularly pronounced in Azure product groups, former MSRC members said, because they were under pressure from Nadella to catch up to Amazon.</p>
        
    
                    
<p data-pp-blocktype="copy" data-pp-id="51.0">“Azure was the Wild West, just this constant race for features and functionality,” said Nate Warfield, who worked in the MSRC for four years beginning in 2016. “You will get a promotion because you released the next new shiny thing in Azure. You are not going to get a promotion because you fixed a bunch of security bugs.”</p>
        
    
                    
<p data-pp-blocktype="copy" data-pp-id="52.0">Former employees told ProPublica that the center fielded hundreds or even thousands of reports a month, pushing the perennially understaffed group to its limits. The magazine Popular Science noted that volume as one of the reasons why working in the MSRC was one of the 10 “<a href="https://www.popsci.com/scitech/article/2007-06/worst-jobs-science-2007/">worst jobs in science</a>,” between whale feces researchers and elephant vasectomists.</p>
        
    
                    
<p data-pp-blocktype="copy" data-pp-id="53.0">“They’re trained, because they’re so resource constrained, to think of these cases in terms of: ‘How can I get to ‘won’t fix,’” said Dustin Childs, who worked in the MSRC in the years leading up to Harris’ saga. Staff would often punt on fixes by telling researchers they would be handled in “v-next,” the next product version, he said. Those launches, however, could be years away, leaving customers vulnerable in the interim, he said.</p>
        
    
                    
<p data-pp-blocktype="copy" data-pp-id="54.0">The center also routinely rejected researchers’ reports of weaknesses by saying they didn’t cross what its staff called a “security boundary.” But when Harris discovered the SAML flaw, it was a term with no formal definition, former employees said.</p>
        
    
                    

<figure data-pp-id="55" data-pp-blocktype="image">

    


                    
    


    <img alt="" width="3000" height="1637" loading="lazy" js-autosizes="" src="https://img.assets-d.propublica.org/v5/images/AP20118483545118_preview_maxWidth_3000_maxHeight_3000_ppi_72_embedColorProfile_true_quality_95.jpg?crop=focalpoint&amp;fit=crop&amp;fm=webp&amp;fp-x=0.5&amp;fp-y=0.5&amp;h=437&amp;q=75&amp;w=800&amp;s=e1250b7677bcfa79735d052583702e66" srcset="https://img.assets-d.propublica.org/v5/images/AP20118483545118_preview_maxWidth_3000_maxHeight_3000_ppi_72_embedColorProfile_true_quality_95.jpg?crop=focalpoint&amp;fit=crop&amp;fm=webp&amp;fp-x=0.5&amp;fp-y=0.5&amp;h=218&amp;q=75&amp;w=400&amp;s=24f90fa50676e5077a9fe0460294e2de 400w, https://img.assets-d.propublica.org/v5/images/AP20118483545118_preview_maxWidth_3000_maxHeight_3000_ppi_72_embedColorProfile_true_quality_95.jpg?crop=focalpoint&amp;fit=crop&amp;fm=webp&amp;fp-x=0.5&amp;fp-y=0.5&amp;h=437&amp;q=75&amp;w=800&amp;s=e1250b7677bcfa79735d052583702e66 800w, https://img.assets-d.propublica.org/v5/images/AP20118483545118_preview_maxWidth_3000_maxHeight_3000_ppi_72_embedColorProfile_true_quality_95.jpg?crop=focalpoint&amp;fit=crop&amp;fm=webp&amp;fp-x=0.5&amp;fp-y=0.5&amp;h=655&amp;q=75&amp;w=1200&amp;s=4598a08e1928e9d76906095d397bf1b9 1200w, https://img.assets-d.propublica.org/v5/images/AP20118483545118_preview_maxWidth_3000_maxHeight_3000_ppi_72_embedColorProfile_true_quality_95.jpg?crop=focalpoint&amp;fit=crop&amp;fm=webp&amp;fp-x=0.5&amp;fp-y=0.5&amp;h=709&amp;q=75&amp;w=1300&amp;s=ca666a9a63d69b25cca507c3bea41cd0 1300w, https://img.assets-d.propublica.org/v5/images/AP20118483545118_preview_maxWidth_3000_maxHeight_3000_ppi_72_embedColorProfile_true_quality_95.jpg?crop=focalpoint&amp;fit=crop&amp;fm=webp&amp;fp-x=0.5&amp;fp-y=0.5&amp;h=791&amp;q=75&amp;w=1450&amp;s=af3826e4d57abff88dde1d6c3451fee3 1450w, https://img.assets-d.propublica.org/v5/images/AP20118483545118_preview_maxWidth_3000_maxHeight_3000_ppi_72_embedColorProfile_true_quality_95.jpg?crop=focalpoint&amp;fit=crop&amp;fm=webp&amp;fp-x=0.5&amp;fp-y=0.5&amp;h=873&amp;q=75&amp;w=1600&amp;s=d59e2adf79d220b993b65e0b31e30a5e 1600w, https://img.assets-d.propublica.org/v5/images/AP20118483545118_preview_maxWidth_3000_maxHeight_3000_ppi_72_embedColorProfile_true_quality_95.jpg?crop=focalpoint&amp;fit=crop&amp;fm=webp&amp;fp-x=0.5&amp;fp-y=0.5&amp;h=1091&amp;q=75&amp;w=2000&amp;s=dfe15a09ba1c967b2640fd705603557a 2000w">

            
    
<figcaption>
    
        <span>
        <span>Credit: </span>
        Jaap Arriens / Sipa USA via AP Images
    </span>
    
    
    </figcaption>

</figure>

        
    
                    
<p data-pp-blocktype="copy" data-pp-id="56.0">By 2017, the lack of clarity had become the “butt of jokes,” Warfield said. Several prominent security researchers who regularly interacted with the MSRC made T-shirts and stickers that said “____ [fill in the blank] is not a security boundary.”</p>
        
    
                    
<p data-pp-blocktype="copy" data-pp-id="57.0">“Any time Microsoft didn’t want to fix something, they’d just say, ‘That’s not a security boundary, we’re not going to fix it,’” Warfield recalled.</p>
        
    
                    
<p data-pp-blocktype="copy" data-pp-id="58.0">Unaware of the inauspicious climate, Harris met virtually with MSRC representatives and sketched out how a hacker could jump from an on-premises server to the cloud without being detected. The MSRC declined to address the problem. Its staff argued that hackers attempting to exploit the SAML flaw would first have to gain access to an on-premises server. As they saw it, Harris said, that was the security boundary — not the subsequent hop to the cloud.</p>
        
    
                    
<h3>Business Over Security</h3>
<p data-pp-blocktype="copy" data-pp-id="59.0">“WTF,” Harris recalled thinking when he got the news. “This makes no sense.”</p>
        
    
                    
<p data-pp-blocktype="copy" data-pp-id="60.0">Microsoft had told customers the cloud was the safest place to put their most precious data. His discovery proved that, for the millions of users whose systems included AD FS, their cloud was only as secure as their on-premises servers. In other words, all the buildings owned by the landlord are only as secure as the most careless tenant who forgot to lock their window.</p>
        
    
                    
<p data-pp-blocktype="copy" data-pp-id="61.0">Harris pushed back, but he said the MSRC held firm.</p>

<p data-pp-blocktype="copy" data-pp-id="61.1">Harris had a reputation for going outside the chain of command to air his concerns, and he took his case to the team managing the products that verified user identities.</p>
        
    
                    
<p data-pp-blocktype="copy" data-pp-id="62.0">He had some clout, his former colleagues said. He had already established himself as a known expert in the field, had pioneered a cybersecurity threat detection method and later was listed as the named inventor on a <a href="https://ppubs.uspto.gov/dirsearch-public/print/downloadPdf/10977364">Microsoft patent</a>. Harris said he “went kind of crazy” and fired off an email to product manager Mark Morowczynski and director Alex Simons requesting a meeting.</p>
        
    
                    
<p data-pp-blocktype="copy" data-pp-id="63.0">He understood that developing a long-term fix would take time, but he had an interim solution that could eliminate the threat. One of the main practical functions of AD FS was to allow users to access both on-premises servers and a variety of cloud-based services after entering credentials only once, a Microsoft feature known as “seamless” single sign-on. Harris proposed that Microsoft tell its customers to turn off that function so the SAML weakness would no longer matter.</p>
        
    
                    
<p data-pp-blocktype="copy" data-pp-id="64.0">According to Harris, Morowczynski quickly jumped on a videoconference and said he had discussed the concerns with Simons.</p>

<p data-pp-blocktype="copy" data-pp-id="64.1">“Everyone violently agreed with me that this is a huge issue,” Harris said. “Everyone violently disagreed with me that we should move quickly to fix it.”</p>

<p data-pp-blocktype="copy" data-pp-id="64.2">Morowczynski, Harris said, had two primary objections.</p>
        
    
                    
<p data-pp-blocktype="copy" data-pp-id="65.0">First, a public acknowledgement of the SAML flaw would alert adversaries who could then exploit it. Harris waved off the concern, believing it was a risk worth taking so that customers wouldn’t be ignorant to the threat. Plus, he believed Microsoft could warn customers without betraying any specifics that could be co-opted by hackers.</p>
        
    
                    
<p data-pp-blocktype="copy" data-pp-id="66.0">According to Harris, Morowczynski’s second objection revolved around the business fallout for Microsoft. Harris said Morowczynski told him that his proposed fix could alienate one of Microsoft’s largest and most important customers: the federal government, which used AD FS. Disabling seamless SSO would have widespread and unique consequences for government employees, who relied on physical “smart cards” to log onto their devices. Required by federal rules, the cards generated random passwords each time employees signed on. Due to the configuration of the underlying technology, though, removing seamless SSO would mean users could not access the cloud through their smart cards. To access services or data on the cloud, they would have to sign in a second time and would not be able to use the mandated smart cards.</p>
        
    
                    
<p data-pp-blocktype="copy" data-pp-id="67.0">Harris said Morowczynski rejected his idea, saying it wasn’t a viable option.</p>
        
    
                    
<p data-pp-blocktype="copy" data-pp-id="68.0">Morowczynski told Harris that his approach could also undermine the company’s chances of getting one of the largest government computing contracts in U.S. history, which would be formally announced the next year. Internally, Nadella had made clear that Microsoft needed a piece of this multibillion-dollar deal with the Pentagon if it wanted to have a future in selling cloud services, Harris and other former employees said.</p>
        
    
                    
<h3>Killing the Competition</h3>

        
    
                    
<p data-pp-blocktype="copy" data-pp-id="70.0">By Harris’ account, the team was also concerned about the potential business impact on the products sold by Microsoft to sign into the cloud. At the time, Microsoft was in a fierce rivalry with a company called Okta.</p>
        
    
                    
<p data-pp-blocktype="copy" data-pp-id="71.0">Microsoft customers had been sold on seamless SSO, which was one of the competitive advantages — or, in Microsoft parlance, “kill points” — that the company then had over Okta, whose users had to sign on twice, Harris said.</p>
        
    
                    
<p data-pp-blocktype="copy" data-pp-id="72.0">Harris’ proposed fix would undermine the company’s strategy to marginalize Okta and would “add friction” to the user experience, whereas the “No. 1 priority was to remove friction,” Harris recalled Morowczynski telling him. Moreover, it would have cascading consequences for the cloud business because the sale of identity products often led to demand for other cloud services.</p>
        
    
                    
<p data-pp-blocktype="copy" data-pp-id="73.0">“That little speed bump of you authenticating twice was unacceptable by Microsoft’s standards,” Harris said. He recalled Morowczynski telling him that the product group’s call “was a business decision, not a technical one.”</p>
        
    
                    
<p data-pp-blocktype="copy" data-pp-id="74.0">“What they were telling me was counterintuitive to everything I’d heard at Microsoft about ‘customer first,’” Harris said. “Now they’re telling me it’s not ‘customer first,’ it’s actually ‘business first.’”</p>
        
    
                    
<p data-pp-blocktype="copy" data-pp-id="75.0">DiCola, Harris’ then-supervisor, told ProPublica the race to dominate the market for new and high-growth areas like the cloud drove the decisions of Microsoft’s product teams. “That is always like, ‘Do whatever it frickin’ takes to win because you have to win.’ Because if you don’t win, it’s much harder to win it back in the future. Customers tend to buy that product forever.”</p>
        
    
                    
<p data-pp-blocktype="copy" data-pp-id="76.0">According to Harris, Morowczynski said his team had “on the road map” a product that could replace AD FS altogether. But it was unclear when it would be available to customers.</p>
        
    
                    
<p data-pp-blocktype="copy" data-pp-id="77.0">In the months that followed, Harris vented to his colleagues about the product group’s decision. ProPublica talked to three people who worked with Harris at the time and recalled these conversations. All of them spoke on the condition of anonymity because they feared professional repercussions. The three said Harris was enraged and frustrated over what he described to them as the product group’s unwillingness to address the weakness.</p>
        
    
                    
<p data-pp-blocktype="copy" data-pp-id="78.0">Neither Morowczynski nor Simons returned calls seeking comment, and Microsoft declined to make them available for interviews. The company did not dispute the details of Harris’ account. In its statement, Microsoft said it weighs a number of factors when it evaluates potential threats. “We prioritize our security response work by considering potential customer disruption, exploitability, and available mitigations,” the spokesperson said. “We continue to listen to the security research community and <a href="https://www.microsoft.com/en-us/security/blog/2024/05/03/security-above-all-else-expanding-microsofts-secure-future-initiative/">evolve our approach</a> to ensure we are meeting customer expectations and protecting them from emerging threats.”</p>
        
    
                    
<h3>Another Major Warning</h3>
<p data-pp-blocktype="copy" data-pp-id="79.0">Following the conversation with Morowczynski, Harris wrote a reminder to himself on the whiteboard in his home office: “SAML follow-up.” He wanted to keep the pressure on the product team.</p>
        
    
                    
<p data-pp-blocktype="copy" data-pp-id="80.0">Soon after, the Massachusetts- and Tel Aviv-based cybersecurity firm <a href="https://www.cyberark.com/resources/threat-research-blog/golden-saml-newly-discovered-attack-technique-forges-authentication-to-cloud-apps">CyberArk published a blog post describing the flaw</a>, which it dubbed “Golden SAML,” along with a proof of concept, essentially a road map that showed how hackers could exploit the weakness.</p>
        
    
                    
<p data-pp-blocktype="copy" data-pp-id="81.0">Years later, in his <a href="https://www.intelligence.senate.gov/sites/default/files/documents/qfr-bsmith-022321.pdf">written testimony for the Senate Intelligence Committee</a>, Microsoft’s Brad Smith said this was the moment the company learned of the issue. “The Golden SAML theory became known to cybersecurity professionals at Microsoft and across the U.S. government and the tech sector at precisely the same time, when it was published in a public paper in 2017,” Smith wrote.</p>
        
    
                    
<p data-pp-blocktype="copy" data-pp-id="82.0">Lavi Lazarovitz of CyberArk said the firm mentioned the weakness — before the post was published — in a private WhatsApp chat of about 10 security researchers from various companies, a forum members used to compare notes on emerging threats. When they raised the discovery to the group, which included at least one researcher from Microsoft, the other members were dismissive, Lazarovitz said.</p>
        
    
                    
<p data-pp-blocktype="copy" data-pp-id="83.0">“Many in the security research community — I don’t want to say mocked — but asked, ‘Well, what’s the big deal?’” Lazarovitz said.</p>
        
    
                    

<figure data-pp-id="84" data-pp-blocktype="image">

    


                    
    


    <img alt="" width="3000" height="2000" loading="lazy" js-autosizes="" src="https://img.assets-d.propublica.org/v5/images/AP754490950076_preview_maxWidth_3000_maxHeight_3000_ppi_72_embedColorProfile_true_quality_95.jpg?crop=focalpoint&amp;fit=crop&amp;fm=webp&amp;fp-x=0.5&amp;fp-y=0.5&amp;h=533&amp;q=75&amp;w=800&amp;s=6554df025044772e330fefd7612e6969" srcset="https://img.assets-d.propublica.org/v5/images/AP754490950076_preview_maxWidth_3000_maxHeight_3000_ppi_72_embedColorProfile_true_quality_95.jpg?crop=focalpoint&amp;fit=crop&amp;fm=webp&amp;fp-x=0.5&amp;fp-y=0.5&amp;h=267&amp;q=75&amp;w=400&amp;s=f7c287d181d54fc9688e70499e246e18 400w, https://img.assets-d.propublica.org/v5/images/AP754490950076_preview_maxWidth_3000_maxHeight_3000_ppi_72_embedColorProfile_true_quality_95.jpg?crop=focalpoint&amp;fit=crop&amp;fm=webp&amp;fp-x=0.5&amp;fp-y=0.5&amp;h=533&amp;q=75&amp;w=800&amp;s=6554df025044772e330fefd7612e6969 800w, https://img.assets-d.propublica.org/v5/images/AP754490950076_preview_maxWidth_3000_maxHeight_3000_ppi_72_embedColorProfile_true_quality_95.jpg?crop=focalpoint&amp;fit=crop&amp;fm=webp&amp;fp-x=0.5&amp;fp-y=0.5&amp;h=800&amp;q=75&amp;w=1200&amp;s=26f73a96a20f0737a92950b706260fdf 1200w, https://img.assets-d.propublica.org/v5/images/AP754490950076_preview_maxWidth_3000_maxHeight_3000_ppi_72_embedColorProfile_true_quality_95.jpg?crop=focalpoint&amp;fit=crop&amp;fm=webp&amp;fp-x=0.5&amp;fp-y=0.5&amp;h=867&amp;q=75&amp;w=1300&amp;s=8489cc46a4128fb95721017d773ba5fd 1300w, https://img.assets-d.propublica.org/v5/images/AP754490950076_preview_maxWidth_3000_maxHeight_3000_ppi_72_embedColorProfile_true_quality_95.jpg?crop=focalpoint&amp;fit=crop&amp;fm=webp&amp;fp-x=0.5&amp;fp-y=0.5&amp;h=967&amp;q=75&amp;w=1450&amp;s=faaa1d9ae88c8a0276f92ea8a9d72283 1450w, https://img.assets-d.propublica.org/v5/images/AP754490950076_preview_maxWidth_3000_maxHeight_3000_ppi_72_embedColorProfile_true_quality_95.jpg?crop=focalpoint&amp;fit=crop&amp;fm=webp&amp;fp-x=0.5&amp;fp-y=0.5&amp;h=1067&amp;q=75&amp;w=1600&amp;s=4072888de9057865c3dee56dac3fdf8a 1600w, https://img.assets-d.propublica.org/v5/images/AP754490950076_preview_maxWidth_3000_maxHeight_3000_ppi_72_embedColorProfile_true_quality_95.jpg?crop=focalpoint&amp;fit=crop&amp;fm=webp&amp;fp-x=0.5&amp;fp-y=0.5&amp;h=1333&amp;q=75&amp;w=2000&amp;s=4791dc9e93c2a88ee2ed12aa4c53251f 2000w">

            
    
<figcaption>
        <span>The CyberArk headquarters in Newton, Massachusetts</span>
    
        <span>
        <span>Credit: </span>
        Sipa via AP Images
    </span>
    
    
    </figcaption>

</figure>

        
    
                    
<p data-pp-blocktype="copy" data-pp-id="85.0">Nevertheless, CyberArk believed it was worth taking seriously, given that AD FS represented the gateway to users’ most sensitive information, including email. “Threat actors operate in between the cracks,” Lazarovitz said. “So obviously, we understood the feedback that we got, but we still believed that this technique will be eventually leveled by threat actors.”</p>
        
    
                    
<p data-pp-blocktype="copy" data-pp-id="86.0">The Israel-based team also reached out to contacts at Microsoft’s Israeli headquarters and were met with a response similar to the one they got in the WhatsApp group, Lazarovitz said.</p>
        
    
                    
<p data-pp-blocktype="copy" data-pp-id="87.0">The published report was CyberArk’s way of warning the public about the threat. Disclosing the weakness also had a business benefit for the company. In the blog post, it pitched its own security product, which it said “will be extremely beneficial in blocking attackers from getting their hands on important assets like the token-signing certificate in the first place.”</p>
        
    
                    
<p data-pp-blocktype="copy" data-pp-id="88.0">The report initially received little attention. Harris, however, seized on it. He said he alerted Morowczynski and Simons from the product group as well as the MSRC. The situation was more urgent than before, Harris argued to them, because CyberArk included the proof of concept that could be used by hackers to carry out a real attack. For Harris, it harkened back to Morowczynski’s worry that flagging the weakness could give hackers an advantage.</p>
        
    
                    
<p data-pp-blocktype="copy" data-pp-id="89.0">“I was more energetic than ever to have us actually finally figure out what we’re going to do about this,” Harris said.</p>

<p data-pp-blocktype="copy" data-pp-id="89.1">But the MSRC reiterated its “security boundary” stance, while Morowczynski reaffirmed the product group’s earlier decision, Harris said.</p>
        
    
                    
<p data-pp-blocktype="copy" data-pp-id="90.0">Harris said he then returned to his supervisors, including Hayden Hainsworth and Bharat Shah, who, as corporate vice president of the Azure cloud security division, also oversaw the MSRC. “I said, ‘Can you guys please listen to me,’” Harris recalled. “‘This is probably the most important thing I’ve ever done in my career.’”</p>
        
    
                    
<p data-pp-blocktype="copy" data-pp-id="91.0">Harris said they were unmoved and told him to take the problem back to the MSRC.</p>
        
    
                    
<p data-pp-blocktype="copy" data-pp-id="92.0">Microsoft did not publicly comment on the CyberArk blog post at the time. Years later, <a href="https://www.intelligence.senate.gov/sites/default/files/documents/qfr-bsmith-022321.pdf">in written responses to Congress</a>, Smith said the company’s security researchers reviewed the information but decided to focus on other priorities. Neither Hainsworth nor Shah returned calls seeking comment.</p>
        
    
                    
<h3>Defusing a Ticking Bomb</h3>

        
    
                    
<p data-pp-blocktype="copy" data-pp-id="94.0">Harris said he was deeply frustrated. On a personal level, his ego was bruised. Identifying major weaknesses is considered an achievement for cybersecurity professionals, and, despite his internal discovery, CyberArk had claimed Golden SAML.</p>
        
    
                    
<p data-pp-blocktype="copy" data-pp-id="95.0">More broadly, he said he was more worried than ever, believing the weakness was a ticking bomb. “It’s out in the open now,” he said.</p>
        
    
                    
<p data-pp-blocktype="copy" data-pp-id="96.0">Publicly, Microsoft <a href="https://web.archive.org/web/20171209163607/https://azure.microsoft.com/en-us/overview/azure-vs-aws/">continued to promote the safety of its products</a>, even boasting of its relationship with the federal government in sales pitches. “To protect your organization, Azure embeds security, privacy, and compliance into its development methodology,” the company said in late 2017, “and has been recognized as the most trusted cloud for U.S. government institutions.”</p>
        
    
                    

<figure data-pp-id="97" data-pp-blocktype="image">

    


                    
    


    <img alt="" width="3000" height="2000" loading="lazy" js-autosizes="" src="https://img.assets-d.propublica.org/v5/images/GettyImages-681661222_preview_maxWidth_3000_maxHeight_3000_ppi_72_embedColorProfile_true_quality_95.jpg?crop=focalpoint&amp;fit=crop&amp;fm=webp&amp;fp-x=0.5&amp;fp-y=0.5&amp;h=533&amp;q=75&amp;w=800&amp;s=e7b0960b96cf5d1f02a7d97b4d4d7150" srcset="https://img.assets-d.propublica.org/v5/images/GettyImages-681661222_preview_maxWidth_3000_maxHeight_3000_ppi_72_embedColorProfile_true_quality_95.jpg?crop=focalpoint&amp;fit=crop&amp;fm=webp&amp;fp-x=0.5&amp;fp-y=0.5&amp;h=267&amp;q=75&amp;w=400&amp;s=da9833ef4061a5b33a701f937077b35e 400w, https://img.assets-d.propublica.org/v5/images/GettyImages-681661222_preview_maxWidth_3000_maxHeight_3000_ppi_72_embedColorProfile_true_quality_95.jpg?crop=focalpoint&amp;fit=crop&amp;fm=webp&amp;fp-x=0.5&amp;fp-y=0.5&amp;h=533&amp;q=75&amp;w=800&amp;s=e7b0960b96cf5d1f02a7d97b4d4d7150 800w, https://img.assets-d.propublica.org/v5/images/GettyImages-681661222_preview_maxWidth_3000_maxHeight_3000_ppi_72_embedColorProfile_true_quality_95.jpg?crop=focalpoint&amp;fit=crop&amp;fm=webp&amp;fp-x=0.5&amp;fp-y=0.5&amp;h=800&amp;q=75&amp;w=1200&amp;s=86e71ed8b005c7a41832a34997e71ea5 1200w, https://img.assets-d.propublica.org/v5/images/GettyImages-681661222_preview_maxWidth_3000_maxHeight_3000_ppi_72_embedColorProfile_true_quality_95.jpg?crop=focalpoint&amp;fit=crop&amp;fm=webp&amp;fp-x=0.5&amp;fp-y=0.5&amp;h=867&amp;q=75&amp;w=1300&amp;s=b40c716a2b5e1502d08f0effb23531d1 1300w, https://img.assets-d.propublica.org/v5/images/GettyImages-681661222_preview_maxWidth_3000_maxHeight_3000_ppi_72_embedColorProfile_true_quality_95.jpg?crop=focalpoint&amp;fit=crop&amp;fm=webp&amp;fp-x=0.5&amp;fp-y=0.5&amp;h=967&amp;q=75&amp;w=1450&amp;s=0fd89fec85bb8daffaa4fd06d14991e9 1450w, https://img.assets-d.propublica.org/v5/images/GettyImages-681661222_preview_maxWidth_3000_maxHeight_3000_ppi_72_embedColorProfile_true_quality_95.jpg?crop=focalpoint&amp;fit=crop&amp;fm=webp&amp;fp-x=0.5&amp;fp-y=0.5&amp;h=1067&amp;q=75&amp;w=1600&amp;s=850fe4a2d34050d14add5728eba9fb8f 1600w, https://img.assets-d.propublica.org/v5/images/GettyImages-681661222_preview_maxWidth_3000_maxHeight_3000_ppi_72_embedColorProfile_true_quality_95.jpg?crop=focalpoint&amp;fit=crop&amp;fm=webp&amp;fp-x=0.5&amp;fp-y=0.5&amp;h=1333&amp;q=75&amp;w=2000&amp;s=e53244a56feec4da9e6906ddbf703f39 2000w">

            
    
<figcaption>
        <span>Attendees walk through the exhibition floor during the Microsoft Developers Build Conference in Seattle in 2017.</span>
    
        <span>
        <span>Credit: </span>
        David Ryder/Bloomberg via Getty Images
    </span>
    
    
    </figcaption>

</figure>

        
    
                    
<p data-pp-blocktype="copy" data-pp-id="98.0">Internally, Harris complained to colleagues that customers were being left vulnerable.</p>
        
    
                    
<p data-pp-blocktype="copy" data-pp-id="99.0">“He was definitely having issues” with the product team, said Harris’ former Microsoft colleague who consulted for the Defense Department. “He vented that it was a problem that they just wanted to ignore.”</p>
        
    
                    
<p data-pp-blocktype="copy" data-pp-id="100.0">Harris typically pivoted from venting to discussing how to protect customers, the former colleague said. “I asked him to show me what I’m going to have to do to make sure the customers were aware and could take corrective action to mitigate the risk,” he said.</p>
        
    
                    
<p data-pp-blocktype="copy" data-pp-id="101.0"><a href="https://www.linkedin.com/posts/andrewfharris_yet-another-post-on-securing-identity-with-activity-6503732899240562688-nq9w?utm_source=share&amp;utm_medium=member_desktop">Harris also took his message to LinkedIn</a>, where he posted a discreet warning and an offer.</p>
        
    
                    
<p data-pp-blocktype="copy" data-pp-id="102.0">“I hope all my friends and followers on here realize by now the security relationship” involved in authenticating users in AD FS, he wrote in 2019. “If not, reach out and let’s fix that!”</p>
        
    
                    

<figure data-pp-id="103" data-pp-blocktype="image">

    


                    
    


    <img alt="" width="1060" height="566" loading="lazy" js-autosizes="" src="https://img.assets-d.propublica.org/v5/images/2024-tech-project-harris-securing-identity_preview_maxWidth_3000_maxHeight_3000_ppi_72_embedColorProfile_true_quality_95.jpg?crop=focalpoint&amp;fit=crop&amp;fm=webp&amp;fp-x=0.5&amp;fp-y=0.5&amp;h=427&amp;q=75&amp;w=800&amp;s=c171e77bebc48982146233d7336a8552" srcset="https://img.assets-d.propublica.org/v5/images/2024-tech-project-harris-securing-identity_preview_maxWidth_3000_maxHeight_3000_ppi_72_embedColorProfile_true_quality_95.jpg?crop=focalpoint&amp;fit=crop&amp;fm=webp&amp;fp-x=0.5&amp;fp-y=0.5&amp;h=214&amp;q=75&amp;w=400&amp;s=1ed769d36f0a5f020bc3619bb05f065f 400w, https://img.assets-d.propublica.org/v5/images/2024-tech-project-harris-securing-identity_preview_maxWidth_3000_maxHeight_3000_ppi_72_embedColorProfile_true_quality_95.jpg?crop=focalpoint&amp;fit=crop&amp;fm=webp&amp;fp-x=0.5&amp;fp-y=0.5&amp;h=427&amp;q=75&amp;w=800&amp;s=c171e77bebc48982146233d7336a8552 800w, https://img.assets-d.propublica.org/v5/images/2024-tech-project-harris-securing-identity_preview_maxWidth_3000_maxHeight_3000_ppi_72_embedColorProfile_true_quality_95.jpg?crop=focalpoint&amp;fit=crop&amp;fm=webp&amp;fp-x=0.5&amp;fp-y=0.5&amp;h=641&amp;q=75&amp;w=1200&amp;s=301e17912a83203fcd57e7c36dd39416 1200w, https://img.assets-d.propublica.org/v5/images/2024-tech-project-harris-securing-identity_preview_maxWidth_3000_maxHeight_3000_ppi_72_embedColorProfile_true_quality_95.jpg?crop=focalpoint&amp;fit=crop&amp;fm=webp&amp;fp-x=0.5&amp;fp-y=0.5&amp;h=694&amp;q=75&amp;w=1300&amp;s=cb6d9e12442df09a7101b15144813099 1300w, https://img.assets-d.propublica.org/v5/images/2024-tech-project-harris-securing-identity_preview_maxWidth_3000_maxHeight_3000_ppi_72_embedColorProfile_true_quality_95.jpg?crop=focalpoint&amp;fit=crop&amp;fm=webp&amp;fp-x=0.5&amp;fp-y=0.5&amp;h=774&amp;q=75&amp;w=1450&amp;s=046a3796f2c79272c6b848424730863b 1450w, https://img.assets-d.propublica.org/v5/images/2024-tech-project-harris-securing-identity_preview_maxWidth_3000_maxHeight_3000_ppi_72_embedColorProfile_true_quality_95.jpg?crop=focalpoint&amp;fit=crop&amp;fm=webp&amp;fp-x=0.5&amp;fp-y=0.5&amp;h=854&amp;q=75&amp;w=1600&amp;s=3a6b4877c7cd53fc8faa602f7f5b15af 1600w, https://img.assets-d.propublica.org/v5/images/2024-tech-project-harris-securing-identity_preview_maxWidth_3000_maxHeight_3000_ppi_72_embedColorProfile_true_quality_95.jpg?crop=focalpoint&amp;fit=crop&amp;fm=webp&amp;fp-x=0.5&amp;fp-y=0.5&amp;h=1068&amp;q=75&amp;w=2000&amp;s=5ed8a7d266b111c2091c6485e7c40466 2000w">

            
    
<figcaption>
        <span>In 2019, Harris posted a discreet warning and an offer on LinkedIn.</span>
    
        <span>
        <span>Credit: </span>
        Screenshot by ProPublica
    </span>
    
    
    </figcaption>

</figure>

        
    
                    
<p data-pp-blocktype="copy" data-pp-id="104.0">Separately, he realized he could help customers with whom he had existing relationships, including the NYPD, the nation’s largest police force.</p>

<p data-pp-blocktype="copy" data-pp-id="104.1">“Knowing this exploit is actually possible, why would I not architect around it, especially for my critical customers?” Harris said.</p>
        
    
                    
<p data-pp-blocktype="copy" data-pp-id="105.0">On a visit to the NYPD, Harris told a top IT official, Matthew Fraser, about the AD FS weakness and recommended disabling seamless SSO. Fraser was in disbelief at the severity of the issue, Harris recalled, and he agreed to disable seamless SSO.</p>
        
    
                    
<p data-pp-blocktype="copy" data-pp-id="106.0">In an interview, Fraser confirmed the meeting. </p>

<p data-pp-blocktype="copy" data-pp-id="106.1">“This was identified as one of those areas that was prime, ripe,” Fraser said of the SAML weakness. “From there, we figured out what’s the best path to insulate and secure.”</p>

<h3>More Troubling Revelations</h3>

        
    
                    
<p data-pp-blocktype="copy" data-pp-id="107.0">It was over beers at a conference in Orlando in 2018 that Harris learned the weakness was even worse than he’d initially realized. A colleague sketched out on a napkin how hackers could also bypass a common security feature called multifactor authentication, which requires users to perform one or more additional steps to verify their identity, such as entering a code sent via text message.</p>
        
    
                    
<p data-pp-blocktype="copy" data-pp-id="108.0">They realized that, no matter how many additional security steps a company puts in place, a hacker with a forged token can bypass them all. When they brought the new information to the MSRC, “it was a nonstarter,” Harris said. While the center <a href="https://www.microsoft.com/en-us/msrc/windows-security-servicing-criteria">had published a formal definition</a> of “security boundary” by that point, Harris’ issues still didn’t meet it.</p>
        
    
                    

<figure data-pp-id="109" data-pp-blocktype="image">

    


                    
    


    <img alt="" width="3000" height="1770" loading="lazy" js-autosizes="" src="https://img.assets-d.propublica.org/v5/images/AP18127587537008_preview_maxWidth_3000_maxHeight_3000_ppi_72_embedColorProfile_true_quality_95.jpg?crop=focalpoint&amp;fit=crop&amp;fm=webp&amp;fp-x=0.5&amp;fp-y=0.5&amp;h=472&amp;q=75&amp;w=800&amp;s=bae33542f0f9dc9e24f39f8c4143746c" srcset="https://img.assets-d.propublica.org/v5/images/AP18127587537008_preview_maxWidth_3000_maxHeight_3000_ppi_72_embedColorProfile_true_quality_95.jpg?crop=focalpoint&amp;fit=crop&amp;fm=webp&amp;fp-x=0.5&amp;fp-y=0.5&amp;h=236&amp;q=75&amp;w=400&amp;s=728df062b6f46fe3bdb6a2ec768ed81d 400w, https://img.assets-d.propublica.org/v5/images/AP18127587537008_preview_maxWidth_3000_maxHeight_3000_ppi_72_embedColorProfile_true_quality_95.jpg?crop=focalpoint&amp;fit=crop&amp;fm=webp&amp;fp-x=0.5&amp;fp-y=0.5&amp;h=472&amp;q=75&amp;w=800&amp;s=bae33542f0f9dc9e24f39f8c4143746c 800w, https://img.assets-d.propublica.org/v5/images/AP18127587537008_preview_maxWidth_3000_maxHeight_3000_ppi_72_embedColorProfile_true_quality_95.jpg?crop=focalpoint&amp;fit=crop&amp;fm=webp&amp;fp-x=0.5&amp;fp-y=0.5&amp;h=708&amp;q=75&amp;w=1200&amp;s=97aa587005667b0e0a14ca5178999fb4 1200w, https://img.assets-d.propublica.org/v5/images/AP18127587537008_preview_maxWidth_3000_maxHeight_3000_ppi_72_embedColorProfile_true_quality_95.jpg?crop=focalpoint&amp;fit=crop&amp;fm=webp&amp;fp-x=0.5&amp;fp-y=0.5&amp;h=767&amp;q=75&amp;w=1300&amp;s=f98a131d6b40e168689911d526e338c8 1300w, https://img.assets-d.propublica.org/v5/images/AP18127587537008_preview_maxWidth_3000_maxHeight_3000_ppi_72_embedColorProfile_true_quality_95.jpg?crop=focalpoint&amp;fit=crop&amp;fm=webp&amp;fp-x=0.5&amp;fp-y=0.5&amp;h=856&amp;q=75&amp;w=1450&amp;s=827158c7c104ba7f81f13efb3a40f5d5 1450w, https://img.assets-d.propublica.org/v5/images/AP18127587537008_preview_maxWidth_3000_maxHeight_3000_ppi_72_embedColorProfile_true_quality_95.jpg?crop=focalpoint&amp;fit=crop&amp;fm=webp&amp;fp-x=0.5&amp;fp-y=0.5&amp;h=944&amp;q=75&amp;w=1600&amp;s=0553a5c2aaa629243b4de868086c6942 1600w, https://img.assets-d.propublica.org/v5/images/AP18127587537008_preview_maxWidth_3000_maxHeight_3000_ppi_72_embedColorProfile_true_quality_95.jpg?crop=focalpoint&amp;fit=crop&amp;fm=webp&amp;fp-x=0.5&amp;fp-y=0.5&amp;h=1180&amp;q=75&amp;w=2000&amp;s=2ba1bc91982436ba78472e98d0c8f77b 2000w">

            
    
<figcaption>
        <span>Nadella delivers the keynote address at a 2018 conference in Seattle for software developers.</span>
    
        <span>
        <span>Credit: </span>
        Elaine Thompson/AP
    </span>
    
    
    </figcaption>

</figure>

        
    
                    
<p data-pp-blocktype="copy" data-pp-id="110.0">By March 2019, concerns over Golden SAML were spilling out into the wider tech world. That month, <a href="https://troopers.de/troopers19/agenda/fpxwmn/">at a conference in Germany</a>, two researchers from the cybersecurity company Mandiant delivered a presentation demonstrating how hackers could infiltrate AD FS to gain access to organizations’ cloud accounts and applications. They also released the tools they used to do so.</p>
        
    
                    
<p data-pp-blocktype="copy" data-pp-id="111.0">Mandiant said it notified Microsoft before the presentation, making it the second time in roughly 16 months that an outside firm had flagged the SAML issue to the company.</p>

<p data-pp-blocktype="copy" data-pp-id="111.1">In August 2020, Harris left Microsoft to work for CrowdStrike. In his exit interview with Shah, Harris said he raised the SAML weakness one last time. Shah listened but offered no feedback, he said.</p>

<p data-pp-blocktype="copy" data-pp-id="111.2">“There is no inspector general-type thing” within Microsoft, Harris said. “If something egregious is happening, where the hell do you go? There’s no place to go.”</p>

<h3>SolarWinds Breaks</h3>

        
    
                    
<p data-pp-blocktype="copy" data-pp-id="112.0">Four months later, news of the SolarWinds attack broke. Federal officials soon announced that beginning in 2019 Russian hackers had breached and exploited the network management software offered by a Texas-based company called SolarWinds, which had the misfortune of lending its name to the attack. The hackers covertly inserted malware into the firm’s software updates, gaining “backdoor” access to the networks of companies and government agencies that installed them. The ongoing access allowed hackers to take advantage of “post-exploit” vulnerabilities, including Golden SAML, to steal sensitive data and emails from the cloud.</p>
        
    
                    
<p data-pp-blocktype="copy" data-pp-id="113.0">Despite the name, <a href="https://www.wsj.com/articles/suspected-russian-hack-extends-far-beyond-solarwinds-software-investigators-say-11611921601">nearly a third</a> of victims of the attack never used SolarWinds software at all, Brandon Wales, then acting director of the federal Cybersecurity and Infrastructure Security Agency, said in the aftermath. In March 2021, Wales told a Senate panel that hackers were able to “gain broad access to data stores that they wanted, largely in Microsoft Office 365 Cloud … and it was all because they compromised those systems that manage trust and identity on networks.”</p>
        
    
                    
<p data-pp-blocktype="copy" data-pp-id="114.0">Microsoft itself was also breached.</p>
        
    
                    
<p data-pp-blocktype="copy" data-pp-id="115.0">In the immediate aftermath of the attack, <a href="https://techcommunity.microsoft.com/t5/microsoft-entra-blog/protecting-microsoft-365-from-on-premises-attacks/ba-p/1751754">Microsoft advised customers</a> of Microsoft 365 to disable seamless SSO in AD FS and similar products — the solution that Harris proposed three years earlier.</p>
        
    
                    
<p data-pp-blocktype="copy" data-pp-id="116.0">As the world dealt with the consequences, Harris took his long simmering frustration <a href="http://x.com/ciberesponce/status/1339885673646612480?s=46&amp;t=chjNjKsX2TeMLEHigHtMaA">public in a series</a> of <a href="https://twitter.com/ciberesponce/status/1363483257136947204?s=46&amp;t=chjNjKsX2TeMLEHigHtMaA">posts</a> on social media and on his <a href="https://www.ciberesponce.com/learning-identity-cyber-attacks/">personal blog</a>. <a href="https://x.com/ciberesponce/status/1364336404122263554?s=46&amp;t=chjNjKsX2TeMLEHigHtMaA">Challenging Brad Smith</a> by name, and criticizing the MSRC’s decisions — which he referred to as “utter BS” — Harris lambasted Microsoft for failing to publicly warn customers about Golden SAML.</p>
        
    
                    
<p data-pp-blocktype="copy" data-pp-id="117.0">Microsoft “was not transparent about these risks, forced customers to use ADFS knowing these risks, and put many customers and especially US Gov’t in a bad place,” Harris <a href="https://www.linkedin.com/posts/andrewfharris_ive-been-outspoken-a-bit-on-how-bypassing-activity-6745561216145588224-R4mj?utm_source=share&amp;utm_medium=member_desktop">wrote on LinkedIn in December 2020</a>. A long-term fix was “never a priority” for the company, he wrote. “Customers are boned and sadly it’s been that way for years (which again, sickens me),” Harris said in the post.</p>
        
    
                    
<p data-pp-blocktype="copy" data-pp-id="118.0">In the months and years following the SolarWinds attack, Microsoft took a number of actions to mitigate the SAML risk. One of them was a way to efficiently detect fallout from such a hack. The advancement, however, was available only as part of a paid add-on product known as Sentinel.</p>
        
    
                    
<p data-pp-blocktype="copy" data-pp-id="119.0">The lack of such a detection, <a href="https://techcommunity.microsoft.com/t5/microsoft-sentinel-blog/non-interactive-logins-minimizing-the-blind-spot/ba-p/2287932">the company said in a blog post</a>, had been a “blind spot.”</p>
        
    
                    
<h3>“Microsoft Is Back on Top”</h3>
<p data-pp-blocktype="copy" data-pp-id="120.0">In early 2021, the Senate Select Committee on Intelligence called Brad Smith to testify about SolarWinds.</p>
        
    
                    
<p data-pp-blocktype="copy" data-pp-id="121.0">Although Microsoft’s product had played a central role in the attack, Smith seemed unflappable, his easy and conversational tone a reflection of the relationships he had spent decades building on Capitol Hill. Without referencing notes or reading from a script, as some of his counterparts did, he confidently deflected questions about Microsoft’s role. Laying the responsibility with the government, he said that in the lead-up to the attack, the authentication flaw “was not prioritized by the intelligence community as a risk, nor was it flagged by civilian agencies or other entities in the security community as a risk that should be elevated” over other cybersecurity priorities.</p>
        
    
                    
<p data-pp-blocktype="copy" data-pp-id="122.0">Smith also downplayed the significance of the Golden SAML weakness, <a href="https://www.intelligence.senate.gov/sites/default/files/documents/os-bsmith-022321.pdf">saying it was used in just 15% of the 60 cases</a> that Microsoft had identified by that point. At the same time, he acknowledged that, “without question, these are not the only victims who had data observed or taken.”</p>
        
    
                    
<p data-pp-blocktype="copy" data-pp-id="123.0">When Sen. Marco Rubio of Florida <a href="https://www.intelligence.senate.gov/hearings/open-hearing-hearing-hack-us-networks-foreign-adversary">pointedly asked him</a> what Microsoft had done to address Golden SAML in the years before the attack, Smith responded by listing a handful of steps that customers could have taken to protect themselves. His suggestions included purchasing an antivirus product like Microsoft Defender and securing devices with another Microsoft product called Intune.</p>
        
    
                    
<p data-pp-blocktype="copy" data-pp-id="124.0">“The reality is any organization that did all five of those things, if it was breached, it in all likelihood suffered almost no damage,” Smith said.</p>

<p data-pp-blocktype="copy" data-pp-id="124.1">Neither Rubio nor any other senator pressed further.</p>
        
    
                    
<p data-pp-blocktype="copy" data-pp-id="125.0">Ultimately, Microsoft <a href="https://www.defense.gov/News/News-Stories/Article/Article/3243483/department-names-vendors-to-provide-joint-warfighting-cloud-capability/">won a piece</a> of the Defense Department’s multibillion-dollar cloud business, sharing it with Amazon, Google and Oracle.</p>
        
    
                                  
        
    
                    
<p data-pp-blocktype="copy" data-pp-id="127.0">Since December 2020, when the SolarWinds attack was made public, Microsoft’s stock has soared 106%, largely on the runaway success of Azure and artificial intelligence products like ChatGPT, where the company is the largest investor. “Microsoft Is Back on Top,” proclaimed Fortune, which featured Nadella on the cover of its most recent issue.</p>
        
    
                    
<p data-pp-blocktype="copy" data-pp-id="128.0">In September 2021, just 10 months after the discovery of SolarWinds, the paperback edition of Smith’s book, “Tools and Weapons,” was published. In it, Smith praised Microsoft’s response to the attack. The MSRC, Smith wrote, “quickly activated its incident response plan” and the company at large “mobilized more than 500 employees to work full time on every aspect of the attack.”</p>
        
    
                    
<p data-pp-blocktype="copy" data-pp-id="129.0">In the new edition, Smith also reflected on his congressional testimony on SolarWinds. The hearings, he wrote, “examined not only what had happened but also what steps needed to be taken to prevent such attacks in the future.” He didn’t mention it in the book, but that certainly would include the long-term alternative that Morowczynski first promised to Harris in 2017. The company began offering it <a href="https://redmondmag.com/articles/2022/02/14/azure-active-directory-certificate-based-authentication-preview.aspx">in 2022</a>.</p>
        
    
                    <div data-pp-location="bottom-note">
                        <p>Development by <a href="https://www.propublica.org/people/lucas-waldron">Lucas Waldron</a>.</p>

        </div><!-- end .article-body__bottom-notes -->
        
    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Arm says it wants all Snapdragon X Elite laptops destroyed (161 pts)]]></title>
            <link>https://www.xda-developers.com/arm-says-it-wants-all-snapdragon-x-elite-laptops-destroyed/</link>
            <guid>40667606</guid>
            <pubDate>Thu, 13 Jun 2024 09:33:12 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.xda-developers.com/arm-says-it-wants-all-snapdragon-x-elite-laptops-destroyed/">https://www.xda-developers.com/arm-says-it-wants-all-snapdragon-x-elite-laptops-destroyed/</a>, See on <a href="https://news.ycombinator.com/item?id=40667606">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
                                                       <div>
                    


        

                        <nav>
                <ul>
                    <li><a href="https://www.xda-developers.com/">Home</a></li>
                                                                                            <li><a href="https://www.xda-developers.com/processor/">CPU</a></li>
                                                                                                                                                            </ul>
            </nav>
            
                </div>
                            


    
    
            
    
    
            
    
    
    
        
    
                            






            
            
    
    
    
    
            

    
    
            
    
    
            
    
    
    
        
    
                                
    <p>The legal battle between Arm and Qualcomm continues</p>

            
            
    
    
    
    
            

    
    
            
    
    
            
    
    
    
        
    
                                
                                    
                                                                                                                        
                                                        <div data-img-url="https://static1.xdaimages.com/wordpress/wp-content/uploads/wm/2023/10/snapdragon-x-elite-10.jpg" data-modal-id="single-image-modal" data-modal-container-id="single-image-modal-container" data-img-caption="&quot;&quot;">

        
    



<figure>
        <picture>
        
                                        
                                    
                                                                                                                                                                            
            
                                                            
            
            
            
            <source media="(min-width: 1024px)" data-srcset="https://static1.xdaimages.com/wordpress/wp-content/uploads/wm/2023/10/snapdragon-x-elite-10.jpg?q=49&amp;fit=contain&amp;w=1140&amp;h=&amp;dpr=2" srcset="https://static1.xdaimages.com/wordpress/wp-content/uploads/wm/2023/10/snapdragon-x-elite-10.jpg?q=49&amp;fit=contain&amp;w=1140&amp;h=&amp;dpr=2">
        
                                        
                                    
                                                                                                                                                                            
            
                                                            
            
            
            
            <source media="(min-width: 768px)" data-srcset="https://static1.xdaimages.com/wordpress/wp-content/uploads/wm/2023/10/snapdragon-x-elite-10.jpg?q=49&amp;fit=contain&amp;w=943&amp;h=&amp;dpr=2" srcset="https://static1.xdaimages.com/wordpress/wp-content/uploads/wm/2023/10/snapdragon-x-elite-10.jpg?q=49&amp;fit=contain&amp;w=943&amp;h=&amp;dpr=2">
        
                                        
                                    
                                                                                                                                                                            
            
                                                            
            
            
            
            <source media="(min-width: 481px)" data-srcset="https://static1.xdaimages.com/wordpress/wp-content/uploads/wm/2023/10/snapdragon-x-elite-10.jpg?q=49&amp;fit=contain&amp;w=767&amp;h=&amp;dpr=2" srcset="https://static1.xdaimages.com/wordpress/wp-content/uploads/wm/2023/10/snapdragon-x-elite-10.jpg?q=49&amp;fit=contain&amp;w=767&amp;h=&amp;dpr=2">
        
                                        
                                    
                                                                                                                                                                            
            
                                                            
            
            
            
            <source media="(min-width: 0px)" data-srcset="https://static1.xdaimages.com/wordpress/wp-content/uploads/wm/2023/10/snapdragon-x-elite-10.jpg?q=49&amp;fit=contain&amp;w=480&amp;h=&amp;dpr=2" srcset="https://static1.xdaimages.com/wordpress/wp-content/uploads/wm/2023/10/snapdragon-x-elite-10.jpg?q=49&amp;fit=contain&amp;w=480&amp;h=&amp;dpr=2">
                <img width="3000" height="2000" alt="Snapdragon X Elite chip in front of a reference design laptop powered by the chip" data-img-url="https://static1.xdaimages.com/wordpress/wp-content/uploads/wm/2023/10/snapdragon-x-elite-10.jpg" src="https://static1.xdaimages.com/wordpress/wp-content/uploads/wm/2023/10/snapdragon-x-elite-10.jpg">
    </picture>
                                    
    </figure>


    </div>

    
            <!-- Not injecting Ads due to No-Ads mode. -->
    
            
            
    
    
    
    
            

    
    
        </div><div id="article-body" itemprop="articleBody">

<div id="custom_block_0">

                    <h3>Key Takeaways</h3>
        
                    <div>    <ul>
                    <li>
                                        Arm is trying to eliminate Qualcomm from Windows market, so it can introduce its own Cortex design.
                        </li>
                    <li>
                                        Rumors suggest that Nvidia, MediaTek, and AMD may enter the Windows ecosystem soon with Arm chips.
                        </li>
                    <li>
                                        Arm claims Qualcomm doesn't have a license for custom Arm chips, creating a legal battle between the two companies.
                        </li>
            </ul>
</div>
        
        
    </div><!-- Not injecting Ads due to No-Ads mode. -->
<p>            We're less than a week away from <a href="https://www.xda-developers.com/snapdragon-x/">Qualcomm's Snapdragon X-series</a> laptops hitting shelves, and it's kind of a big deal. There's been an elephant in the room since Apple started using Arm chips for Macs, and this is seen as the launch that will bring Windows to parity with that.
    </p>    
<p>            But there's another elephant in the room, at least depending on your perspective, which is that Arm Holdings and Qualcomm have been locked in a legal battle for some time over these very chips.
    </p>            
    
                    
                            
                
    
                                        
    
        
        
    <div>

        
                    			<a href="https://www.xda-developers.com/microsoft-copilot-plus/">
		<div data-img-url="https://static1.xdaimages.com/wordpress/wp-content/uploads/wm/2024/05/satya-nadella-copilot-6.JPG" data-modal-id="single-image-modal" data-modal-container-id="single-image-modal-container" data-img-caption="&quot;&quot;">

            




<figure>
        <picture>
        
                                        
                                    
                                                                                                                                                                            
            
                                                            
            
            
                                        
            <source media="(min-width: 1024px)" data-srcset="https://static1.xdaimages.com/wordpress/wp-content/uploads/wm/2024/05/satya-nadella-copilot-6.JPG?q=49&amp;fit=crop&amp;w=440&amp;h=280&amp;dpr=2" srcset="https://static1.xdaimages.com/wordpress/wp-content/uploads/wm/2024/05/satya-nadella-copilot-6.JPG?q=49&amp;fit=crop&amp;w=440&amp;h=280&amp;dpr=2">
        
                                        
                                    
                                                                                                                                                                            
            
                                                            
            
            
                                        
            <source media="(min-width: 768px)" data-srcset="https://static1.xdaimages.com/wordpress/wp-content/uploads/wm/2024/05/satya-nadella-copilot-6.JPG?q=49&amp;fit=crop&amp;w=310&amp;h=240&amp;dpr=2" srcset="https://static1.xdaimages.com/wordpress/wp-content/uploads/wm/2024/05/satya-nadella-copilot-6.JPG?q=49&amp;fit=crop&amp;w=310&amp;h=240&amp;dpr=2">
        
                                        
                                    
                                                                                                                                                                            
            
                                                            
            
            
                                        
            <source media="(min-width: 481px)" data-srcset="https://static1.xdaimages.com/wordpress/wp-content/uploads/wm/2024/05/satya-nadella-copilot-6.JPG?q=49&amp;fit=crop&amp;w=800&amp;h=520&amp;dpr=2" srcset="https://static1.xdaimages.com/wordpress/wp-content/uploads/wm/2024/05/satya-nadella-copilot-6.JPG?q=49&amp;fit=crop&amp;w=800&amp;h=520&amp;dpr=2">
        
                                        
                                    
                                                                                                                                                                            
            
                                                            
            
            
                                        
            <source media="(min-width: 0px)" data-srcset="https://static1.xdaimages.com/wordpress/wp-content/uploads/wm/2024/05/satya-nadella-copilot-6.JPG?q=49&amp;fit=crop&amp;w=480&amp;h=320&amp;dpr=2" srcset="https://static1.xdaimages.com/wordpress/wp-content/uploads/wm/2024/05/satya-nadella-copilot-6.JPG?q=49&amp;fit=crop&amp;w=480&amp;h=320&amp;dpr=2">
                <img width="6000" height="4000" loading="lazy" decoding="async" alt="Satya Nadella Copilot (6)" data-img-url="https://static1.xdaimages.com/wordpress/wp-content/uploads/wm/2024/05/satya-nadella-copilot-6.JPG" src="https://static1.xdaimages.com/wordpress/wp-content/uploads/wm/2024/05/satya-nadella-copilot-6.JPG">
    </picture>
                
    </figure>


        </div>

		</a>
	
        
                            
                
        
        
            </div>
<!-- Not injecting Ads due to No-Ads mode. --><h2 id="what-does-arm-want">
                        What does Arm want?
               </h2><h3 id="it-39-s-very-anti-arm">
            It's very anti-Arm
    </h3>




    



                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                
    

<p>            "Arm's claim against Qualcomm and Nuvia is about protecting the Arm ecosystem and partners who rely on our IP and innovative designs, and therefore enforcing Qualcomm's contractual obligation to destroy and stop using the Nuvia designs that were derived from Arm technology," an Arm spokesperson said in a statement to <a href="https://www.usnews.com/news/technology/articles/2024-06-10/analysis-arm-qualcomm-legal-battle-seen-disrupting-ai-powered-pc-wave" rel="noopener noreferrer nofollow" target="_blank">Reuters</a>.
    </p>    
<p>            And there it is. Arm wants all Snapdragon X series chips destroyed. Or does it?
    </p>    
<p>            Firstly, let's be clear about what the company is calling for here. Qualcomm has laid the foundation for Windows on Arm. It's built out the ecosystem since the platform was announced at the end of 2016. Native apps like Chrome, Slack, and a whole bunch of others wouldn't exist if it weren't for <a href="https://www.xda-developers.com/best-windows-on-arm/">Snapdragon PCs</a>. What Arm is calling for is for Qualcomm to be eliminated from the market so that it can swoop in with its own Cortex designs.
    </p>    
<p>            And according to rumors, as well as what I'm hearing from my own sources, there are more Arm vendors coming to the Windows ecosystem. Rumor has it that Nvidia, MediaTek, and AMD all have their eyes on it, and a competitor could be entering the space as soon as CES 2025.
    </p>    
<p>            It's likely that a company like MediaTek would use Arm's own Cortex cores, which would be more lucrative for Arm, of course. It also allows the company to maintain a competitive advantage in a world where both Apple and Qualcomm have both moved on from Cortex designs because Arm wasn't offering what they needed.
    </p>    <h2 id="it-39-s-all-about-licensing">
                        It's all about licensing
               </h2><h3 id="does-qualcomm-have-the-rights-to-make-its-own-arm-chips-no-one-knows">
            Does Qualcomm have the rights to make its own Arm chips? No one knows
    </h3>




    



                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        
    

<p>            Basically, Arm says that Qualcomm doesn't have a license to make custom Arm chips, and Qualcomm is saying that it does. There are two kinds of Arm licenses. One is for using Cortex cores, which are designed by Arm, essentially as an off-the-shelf part. The other kind of license lets you build whatever you want, as long as it follows the Arm instruction set.
    </p>    
<p>            Apple and Qualcomm are doing the latter.
    </p>    
<p>            Arm has always been pretty public about the fact that it prefers companies to use Cortex cores. Custom chips mean companies like Qualcomm can make a better Arm chip than Arm, and it can't make the old Apple excuse anymore, which was always that it was about optimizing for the full stack from hardware to software.
    </p>    
<p>            Qualcomm bought a company called Nuvia to do this, and it originally used Nuvia's architectural license. Arm's argument is that the Nuvia license was canceled when it was taken over by Qualcomm. A new deal would have to be negotiated if that holds up in court.
    </p>    
<p>            So, there you have it. Arm wants Qualcomm to stop shipping the product it's been contesting, but to be honest, that's not usually how these things end. It's unlikely that any product will be delayed from hitting shelves. These cases tend to end with one company giving a bucket of cash to the other, and everyone moves on.
    </p>    
<p>            Snapdragon X PCs are slated to launch on June 18, with a total of 14 products from seven OEMs, including Microsoft, HP, Dell, Lenovo, Samsung, Asus, and even Qualcomm. Acer also has a product ready to go in mid-July.
    </p>    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[AMD's MI300X Outperforms Nvidia's H100 for LLM Inference (259 pts)]]></title>
            <link>https://www.blog.tensorwave.com/amds-mi300x-outperforms-nvidias-h100-for-llm-inference/</link>
            <guid>40667102</guid>
            <pubDate>Thu, 13 Jun 2024 07:57:56 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.blog.tensorwave.com/amds-mi300x-outperforms-nvidias-h100-for-llm-inference/">https://www.blog.tensorwave.com/amds-mi300x-outperforms-nvidias-h100-for-llm-inference/</a>, See on <a href="https://news.ycombinator.com/item?id=40667102">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
<p>There has been much anticipation around AMD’s flagship MI300X accelerator. With unmatched raw <a href="https://www.amd.com/en/products/accelerators/instinct/mi300/mi300x.html">specs</a>, the pressing question remains: Can it outperform NVIDIA’s Hopper architecture in real-world AI workloads? We have some exciting early results to share.</p>



<p>For the past month, <strong><a href="https://tensorwave.com/">TensorWave</a> and <a href="https://mk1.ai/">MK1</a></strong> have worked closely to unlock performance of AMD hardware for AI inference. To start, we focused on Mixture of Expert (MoE) architectures due to their compute efficiency and popularity – notably used by Mistral, Meta, Databricks, and X.ai for their most powerful open-source LLMs.</p>



<p>The initial results are impressive: using MK1’s inference software, the MI300X <strong>achieves 33% higher throughput compared to the H100 SXM</strong> running vLLM on Mixtral 8x7B for a real-world chat use case. Despite NVIDIA’s software ecosystem being more mature, it is clear that AMD is already a formidable competitor in the AI market. When hardware availability and cost are factored in, the MI300X proves to be an attractive option for enterprises running large-scale inference in the cloud. </p>



<blockquote>
<p>We expect AMD’s performance advantage to climb even higher after further optimization, so stay tuned for more updates!</p>
<cite>-Darrick Horton, CEO TensorWave</cite></blockquote>



<div>
<p><strong>Subscribe to the TensorWave Newsletter</strong></p>




</div>



<p>We invite you to experience the MI300X firsthand at TensorWave, which comes prepackaged with MK1’s inference software. <a href="mailto:alex@tensorwave.com">C</a><a href="mailto:contact@tensorwave.com">ontact us</a> to find out more.</p>



<h2 id="h-inference-benchmarks">Inference Benchmarks</h2>



<p>We conducted extensive offline and online inference tests comparing the MI300X and H100 SXM5 accelerators using the Mixtral 8x7B model.</p>



<ul>
<li><strong>Offline Tests:</strong> These are standardized and provide insights into the performance of the forward pass across different setups.</li>
</ul>



<ul>
<li><strong>Online Tests:</strong> These are more sophisticated and estimate system performance in a real-world setting where multiple users are serviced asynchronously.</li>
</ul>



<p><strong><span>Benchmark Setup</span></strong></p>



<p><strong>AMD</strong></p>



<ul>
<li><strong>Hardware:</strong> TensorWave node equipped with 8 MI300X accelerators, 2 AMD EPYC CPU Processors (192 cores), and 2.3 TB of DDR5 RAM.</li>



<li><strong>MI300X Accelerator: </strong>192GB VRAM, 5.3 TB/s, ~1300 TFLOPS for FP16</li>



<li><strong>Drivers: </strong>ROCm 6.1.2</li>



<li><strong>Inference Stack: </strong>MK1’s inference engine (Flywheel) v0.9.2 and AMD’s ROCm optimized fork of vLLM (rocm/vllm) v0.4.0.</li>



<li><strong>Configuration: </strong>Tensor parallelism set to 1 (tp=1), since we can fit the entire model Mixtral 8x7B in a single MI300X’s 192GB of VRAM.</li>
</ul>



<p><strong>NVIDIA</strong></p>



<ul>
<li><strong>Hardware:</strong> Baremetal node with 8 H100 SXM5 accelerators with NVLink, 160 CPU cores, and 1.2 TB of DDR5 RAM.</li>



<li><strong>H100 SXM5 Accelerator: </strong>80GB VRAM, 3.35 TB/s, ~986 TFLOPS for FP16</li>



<li><strong>Drivers: </strong>CUDA 12.2</li>



<li><strong>Inference Stack: </strong>vLLM v4.3<strong>&nbsp;</strong></li>



<li><strong>Configuration: </strong>Tensor parallelism set to 2 (tp=2), which is required to fit Mixtral 8x7B in two H100’s 80GB VRAM.</li>
</ul>



<p><strong><em>Notes</em></strong></p>



<ul>
<li>All benchmarks are performed using the Mixtral 8x7B model.</li>



<li>All inference frameworks are configured to use FP16 compute paths. Enabling FP8 compute is left for future work.</li>



<li>To make an accurate comparison between the systems with different settings of tensor parallelism, we extrapolate throughput for the MI300X by 2.</li>
</ul>



<h2 id="h-offline-results">Offline Results</h2>



<p>To measure peak throughput for each inference solution, we generate prompts of a fixed size and directly feed them to the model. This method, known as offline batching, enhances hardware efficiency by processing multiple prompts simultaneously. Although larger batch sizes boost throughput, they also increase latency due to more requests being processed. Following standard practice, we constrain requests in a batch to have equal input sizes, and to have equal output sizes.</p>



<p>We assess the throughput of each system by varying the batch size. This was done using a modified version of `benchmark_throughput.py` script in the vLLM repository, refactored to include Flywheel as a backend. Prompts were also randomly generated within a batch to remove caching mechanisms. The performance metrics, detailed in the table below, measure throughput as a function of batch size.</p>



<figure><img decoding="async" width="1024" height="576" src="https://www.blog.tensorwave.com/wp-content/uploads/2024/06/Untitled-design-4-1024x576.png" alt="" srcset="https://www.blog.tensorwave.com/wp-content/uploads/2024/06/Untitled-design-4-1024x576.png 1024w, https://www.blog.tensorwave.com/wp-content/uploads/2024/06/Untitled-design-4-300x169.png 300w, https://www.blog.tensorwave.com/wp-content/uploads/2024/06/Untitled-design-4-768x432.png 768w, https://www.blog.tensorwave.com/wp-content/uploads/2024/06/Untitled-design-4-1536x864.png 1536w, https://www.blog.tensorwave.com/wp-content/uploads/2024/06/Untitled-design-4.png 1600w" sizes="(max-width: 1024px) 100vw, 1024px"></figure>



<p>Notably, our results show that <strong>MI300X running MK1 Flywheel outperforms H100 running vLLM for every batch size</strong>, with an increase in performance ranging from 1.22x to 2.94x.</p>



<h2 id="h-online-results-for-chat-data-distribution">Online Results for Chat Data Distribution</h2>



<p>Moving beyond offline metrics, we designed a series of online benchmarks to simulate a realistic typical chat application. This involves generating responses to user inputs that closely mirror actual usage patterns.</p>



<p>Specifically, we simulate chat traffic by spawning independent workers to send requests to an endpoint. We then sweep the number of workers to increase the number of concurrent requests.</p>



<p>In these experiments, requests were generated using a standard text chat distribution with an average of 573 input tokens and 50 output tokens. Note that our benchmarking tool supports arbitrary data distributions; please reach out if you have a specific use case you’d like to test.</p>



<p>The key metrics of interest are:</p>



<ul>
<li><strong>Throughput (Requests per Second)</strong>: The number of requests the system can handle per second for a given workload.</li>



<li><strong>Average Latency (Seconds)</strong>: The average time taken to generate a full response for each request.</li>



<li><strong>Time Per Output Token (TPOT)</strong>: The time to generate each subsequent token (averaged) after the first token, which impacts the overall speed of generating long responses.</li>
</ul>



<p>For the first benchmark, we tested a non-streaming use case where throughput and latency are measured for servicing the <em>full response</em>.</p>



<figure><img decoding="async" width="1024" height="576" src="https://www.blog.tensorwave.com/wp-content/uploads/2024/06/TensorWave-Blog-LATENCY-Img-AMDs-MI300X-Outperforms-NVIDIAs-H100-for-LLM-Inference-1024x576.png" alt="" srcset="https://www.blog.tensorwave.com/wp-content/uploads/2024/06/TensorWave-Blog-LATENCY-Img-AMDs-MI300X-Outperforms-NVIDIAs-H100-for-LLM-Inference-1024x576.png 1024w, https://www.blog.tensorwave.com/wp-content/uploads/2024/06/TensorWave-Blog-LATENCY-Img-AMDs-MI300X-Outperforms-NVIDIAs-H100-for-LLM-Inference-300x169.png 300w, https://www.blog.tensorwave.com/wp-content/uploads/2024/06/TensorWave-Blog-LATENCY-Img-AMDs-MI300X-Outperforms-NVIDIAs-H100-for-LLM-Inference-768x432.png 768w, https://www.blog.tensorwave.com/wp-content/uploads/2024/06/TensorWave-Blog-LATENCY-Img-AMDs-MI300X-Outperforms-NVIDIAs-H100-for-LLM-Inference-1536x864.png 1536w, https://www.blog.tensorwave.com/wp-content/uploads/2024/06/TensorWave-Blog-LATENCY-Img-AMDs-MI300X-Outperforms-NVIDIAs-H100-for-LLM-Inference.png 1600w" sizes="(max-width: 1024px) 100vw, 1024px"></figure>



<p>At a target average latency of 5 seconds, <strong>two MI300X with tp=1 services 33% more requests per second than two H100s with tp=2</strong>. This means you can service the same number of users with similar quality of service using fewer accelerators!</p>



<p>For the second benchmark, we enable streaming and measure throughput and TPOT for individual tokens as they are streamed out.</p>



<figure><img loading="lazy" decoding="async" width="1024" height="576" src="https://www.blog.tensorwave.com/wp-content/uploads/2024/06/Throughput-1024x576.png" alt="" srcset="https://www.blog.tensorwave.com/wp-content/uploads/2024/06/Throughput-1024x576.png 1024w, https://www.blog.tensorwave.com/wp-content/uploads/2024/06/Throughput-300x169.png 300w, https://www.blog.tensorwave.com/wp-content/uploads/2024/06/Throughput-768x432.png 768w, https://www.blog.tensorwave.com/wp-content/uploads/2024/06/Throughput-1536x864.png 1536w, https://www.blog.tensorwave.com/wp-content/uploads/2024/06/Throughput.png 1600w" sizes="(max-width: 1024px) 100vw, 1024px"></figure>



<p>Here we observe that the MI300X has higher throughput for every TPOT compared to the H100. This means the <strong>MI300X can generate text faster at higher traffic volumes</strong>, which is crucial for any LLM application.</p>



<h2 id="h-conclusion">Conclusion</h2>



<p>Our benchmarks demonstrate that AMD's MI300X outperforms NVIDIA's H100 in both offline and online inference tasks for MoE architectures like Mixtral 8x7B. The MI300X not only offers higher throughput but also excels in real-world scenarios requiring fast response times. </p>



<p>Given its impressive performance, competitive cost, and hardware availability, the MI300X with MK1 software is an excellent choice for enterprises looking to scale their AI inference capabilities. We encourage you to explore the capabilities of MI300X at TensorWave and experience these benefits first-hand. Contact us to learn more and schedule a test drive of this powerful accelerator!</p>







<figure><a href="https://tensorwave.com/book-a-call?utm_source=twblog&amp;utm_content=unlocking_inference"><img loading="lazy" decoding="async" width="1024" height="576" src="https://www.blog.tensorwave.com/wp-content/uploads/2024/06/Display-Ads-YouTube-Horizontal-Ad-1024x576.png" alt="" srcset="https://www.blog.tensorwave.com/wp-content/uploads/2024/06/Display-Ads-YouTube-Horizontal-Ad-1024x576.png 1024w, https://www.blog.tensorwave.com/wp-content/uploads/2024/06/Display-Ads-YouTube-Horizontal-Ad-300x169.png 300w, https://www.blog.tensorwave.com/wp-content/uploads/2024/06/Display-Ads-YouTube-Horizontal-Ad-768x432.png 768w, https://www.blog.tensorwave.com/wp-content/uploads/2024/06/Display-Ads-YouTube-Horizontal-Ad-1536x864.png 1536w, https://www.blog.tensorwave.com/wp-content/uploads/2024/06/Display-Ads-YouTube-Horizontal-Ad.png 1920w" sizes="(max-width: 1024px) 100vw, 1024px"></a></figure>




</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Uncensor any LLM with abliteration (381 pts)]]></title>
            <link>https://huggingface.co/blog/mlabonne/abliteration</link>
            <guid>40665721</guid>
            <pubDate>Thu, 13 Jun 2024 03:42:39 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://huggingface.co/blog/mlabonne/abliteration">https://huggingface.co/blog/mlabonne/abliteration</a>, See on <a href="https://news.ycombinator.com/item?id=40665721">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
				<p><a href="https://huggingface.co/blog">
						Back to Articles</a></p>

				
				
				
				<div data-target="BlogAuthorsByline" data-props="{&quot;authors&quot;:[{&quot;author&quot;:{&quot;avatarUrl&quot;:&quot;https://cdn-avatars.huggingface.co/v1/production/uploads/61b8e2ba285851687028d395/4XZP5aVsMWwzGx_313cqd.jpeg&quot;,&quot;fullname&quot;:&quot;Maxime Labonne&quot;,&quot;name&quot;:&quot;mlabonne&quot;,&quot;type&quot;:&quot;user&quot;,&quot;isPro&quot;:true,&quot;isHf&quot;:false}}],&quot;translators&quot;:[],&quot;proofreaders&quot;:[],&quot;lang&quot;:&quot;en&quot;}">

<p><span><span><a href="https://huggingface.co/mlabonne"><img alt="Maxime Labonne's avatar" src="https://cdn-avatars.huggingface.co/v1/production/uploads/61b8e2ba285851687028d395/4XZP5aVsMWwzGx_313cqd.jpeg">
					</a>
			</span>

	</span></p></div>
				

				<!-- HTML_TAG_START -->
<div><nav aria-label="Secondary"><ul><li><a href="#✂️-what-is-abliteration" title="✂️ What is abliteration?"><!-- HTML_TAG_START -->✂️ What is abliteration?<!-- HTML_TAG_END --></a>
									<ul></ul>
								</li><li><a href="#💻-implementation" title="💻 Implementation"><!-- HTML_TAG_START -->💻 Implementation<!-- HTML_TAG_END --></a>
									<ul></ul>
								</li><li><a href="#⚖️-dpo-fine-tuning" title="⚖️ DPO Fine-Tuning"><!-- HTML_TAG_START -->⚖️ DPO Fine-Tuning<!-- HTML_TAG_END --></a>
									<ul></ul>
								</li><li><a href="#conclusion" title="Conclusion"><!-- HTML_TAG_START -->Conclusion<!-- HTML_TAG_END --></a>
									<ul></ul>
								</li><li><a href="#references" title="References"><!-- HTML_TAG_START -->References<!-- HTML_TAG_END --></a>
									<ul></ul>
								</li></ul></nav></div><p><a rel="nofollow" href="https://i.imgur.com/KhorYYG.png"><img alt="KhorYYG.png" src="https://i.imgur.com/KhorYYG.png"></a></p>
<p>The third generation of Llama models provided fine-tunes (Instruct) versions that excel in understanding and following instructions. However, these models are heavily censored, designed to refuse requests seen as harmful with responses such as "As an AI assistant, I cannot help you." While this safety feature is crucial for preventing misuse, it limits the model's flexibility and responsiveness.</p>
<p>In this article, we will explore a technique called "abliteration" that can uncensor any LLM without retraining. This technique effectively removes the model's built-in refusal mechanism, allowing it to respond to all types of prompts.</p>
<p>The code is available on&nbsp;<a rel="nofollow" href="https://colab.research.google.com/drive/1VYm3hOcvCpbGiqKZb141gJwjdmmCcVpR?usp=sharing">Google Colab</a>&nbsp;and in the&nbsp;<a rel="nofollow" href="https://github.com/mlabonne/llm-course">LLM Course</a>&nbsp;on GitHub.</p>
<h2>
	<a rel="nofollow" href="#✂️-what-is-abliteration" id="✂️-what-is-abliteration">
		<span></span>
	</a>
	<span>
		✂️ What is abliteration?
	</span>
</h2>
<p>Modern LLMs are fine-tuned for safety and instruction-following, meaning they are trained to refuse harmful requests. In their <a rel="nofollow" href="https://www.lesswrong.com/posts/jGuXSZgv6qfdhMCuJ/refusal-in-llms-is-mediated-by-a-single-direction">blog post</a>, Arditi et al. have shown that this refusal behavior is mediated by a specific direction in the model's residual stream. If we prevent the model from representing this direction, it <strong>loses its ability to refuse requests</strong>. Conversely, adding this direction artificially can cause the model to refuse even harmless requests.</p>
<p>In the traditional decoder-only Llama-like architecture, there are three residual streams we can target: at the start of each block ("pre"), between the attention and MLP layers ("mid"), and after the MLP ("post"). The following figure illustrates the location of each residual stream.</p>
<p><a rel="nofollow" href="https://i.imgur.com/hsdR9e7.png"><img alt="" src="https://i.imgur.com/hsdR9e7.png"></a></p>
<p>To uncensor an LLM, we first need to identify the "refusal direction" within the model. This process involves a few technical steps:</p>
<ol>
<li><strong>Data Collection</strong>: Run the model on a set of harmful instructions and a set of harmless instructions, recording the residual stream activations at the last token position for each.</li>
<li><strong>Mean difference</strong>: Calculate the mean difference between the activations of harmful and harmless instructions. This gives us a vector representing the "refusal direction" for each layer of the model.</li>
<li><strong>Selection</strong>: Normalize these vectors and evaluate them to select the single best "refusal direction."</li>
</ol>
<p>Once we have identified the refusal direction, we can "ablate" it, effectively removing the model's ability to represent this feature. This can be done through an <strong>inference-time intervention</strong> or permanently with <strong>weight orthogonalization</strong>.</p>
<p>Let's talk about inference-time intervention first. For every component that writes to the residual stream (such as an attention head), we calculate the projection of its output onto the refusal direction and subtract this projection. This subtraction is applied at every token and every layer, ensuring that the model never represents the refusal direction.</p>
<p>On the other hand, weight orthogonalization involves modifying the model weights directly. By orthogonalizing the component weights with respect to the refusal direction, it prevents the model from writing to this direction altogether. This is achieved by adjusting the matrices that write to the residual stream, ensuring they do not contribute to the refusal direction.</p>
<p>In the next section, we will implement abliteration with weight orthogonalization.</p>
<h2>
	<a rel="nofollow" href="#💻-implementation" id="💻-implementation">
		<span></span>
	</a>
	<span>
		💻 Implementation
	</span>
</h2>
<p>The following implementation of abliteration is based on <a href="https://huggingface.co/failspy/llama-3-70B-Instruct-abliterated/blob/main/ortho_cookbook.ipynb">FailSpy's notebook</a>, which is itself based on the original authors' <a rel="nofollow" href="https://colab.research.google.com/drive/1a-aQvKC9avdZpdyBn4jgRQFObTPy1JZw?usp=sharing">notebook</a>. I mostly adapted and simplified it to make it easier to understand. This section is quite code-heavy so you can see what is going on, but you can use FailSpy's <a rel="nofollow" href="https://github.com/FailSpy/abliterator">abliterator library</a> if you're less interested in the technical details (also check his <a href="https://huggingface.co/collections/failspy/abliterated-v3-664a8ad0db255eefa7d0012b">collection of abliterated models</a> on Hugging Face).</p>
<p>The code relies on the excellent <a rel="nofollow" href="https://github.com/TransformerLensOrg/TransformerLens">TransformerLens</a> library (formerly known as EasyTransformer) to do the heavy lifting. It is designed for mechanistic interpretability and is used here to intervene on activations. Thanks to Neel Nanda and Joseph Bloom for creating and maintaining this library.</p>
<p>First, let's install the necessary packages and import them. All these steps are available in this <a rel="nofollow" href="https://colab.research.google.com/drive/1VYm3hOcvCpbGiqKZb141gJwjdmmCcVpR?usp=sharing">Google Colab notebook</a>.</p>
<pre><code>!pip install transformers transformers_stream_generator tiktoken transformer_lens einops jaxtyping

<span>import</span> torch
<span>import</span> functools
<span>import</span> einops
<span>import</span> gc

<span>from</span> datasets <span>import</span> load_dataset
<span>from</span> tqdm <span>import</span> tqdm
<span>from</span> torch <span>import</span> Tensor
<span>from</span> typing <span>import</span> <span>List</span>
<span>from</span> transformer_lens <span>import</span> HookedTransformer, utils
<span>from</span> transformer_lens.hook_points <span>import</span> HookPoint
<span>from</span> transformers <span>import</span> AutoModelForCausalLM, AutoTokenizer
<span>from</span> jaxtyping <span>import</span> Float, Int
<span>from</span> collections <span>import</span> defaultdict

<span># Turn automatic differentiation off to save GPU memory (credit: Undi95)</span>
torch.set_grad_enabled(<span>False</span>)
</code></pre>
<p>We need two datasets: one containing harmless instructions, and one containing harmful instructions. We'll use <a href="https://huggingface.co/datasets/tatsu-lab/alpaca">tatsu-lab/alpaca</a> as well as data from <a rel="nofollow" href="https://github.com/llm-attacks/llm-attacks">llm-attacks</a>. To make things easier, I repackaged them in two Hugging Face datasets: <a href="https://huggingface.co/datasets/harmless_behaviors">mlabonne/harmless_behaviors</a> and <a href="https://huggingface.co/datasets/mlabonne/harmful_behaviors">mlabonne/harmful_behaviors</a>. That way, you can easily replace them with your own datasets.</p>
<p>We will load the instructions and reformat them into a list of dictionaries with "role" and "content" keys. This makes it compatible with the <code>apply_chat_tokenizer()</code> method, which we will use to follow Llama 3's chat template.</p>
<pre><code><span>def</span> <span>reformat_texts</span>(<span>texts</span>):
    <span>return</span> [[{<span>"role"</span>: <span>"user"</span>, <span>"content"</span>: text}] <span>for</span> text <span>in</span> texts]

<span># Get harmful and harmless datasets</span>
<span>def</span> <span>get_harmful_instructions</span>():
    dataset = load_dataset(<span>'mlabonne/harmful_behaviors'</span>)
    <span>return</span> reformat_texts(dataset[<span>'train'</span>][<span>'text'</span>]), reformat_texts(dataset[<span>'test'</span>][<span>'text'</span>])

<span>def</span> <span>get_harmless_instructions</span>():
    dataset = load_dataset(<span>'mlabonne/harmless_alpaca'</span>)
    <span>return</span> reformat_texts(dataset[<span>'train'</span>][<span>'text'</span>]), reformat_texts(dataset[<span>'test'</span>][<span>'text'</span>])

harmful_inst_train, harmful_inst_test = get_harmful_instructions()
harmless_inst_train, harmless_inst_test = get_harmless_instructions()
</code></pre>
<p>Now that we have our datasets, we can load the model we want to abliterate. Unfortunately, you can't directly load a custom model using <code>HookedTransformer</code>. Here, I use a trick described in FailSpy's notebook to download a custom model and rename it as <a href="https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct">meta-llama/Meta-Llama-3-8B-Instruct</a>. Load in <code>torch.float16</code> format if your GPU is not compatible with BF16.</p>
<p>In this example, we'll use <a href="https://huggingface.co/mlabonne/Daredevil-8B">mlabonne/Daredevil-8B</a>, a mega-merge created with DARE TIES (see my article about <a href="https://huggingface.co/blog/mlabonne/merge-models">model merging</a>) that has the highest MMLU score on the Open LLM Leaderboard in the 8B category.</p>
<pre><code>MODEL_ID = <span>"mlabonne/Daredevil-8B"</span>
MODEL_TYPE = <span>"meta-llama/Meta-Llama-3-8B-Instruct"</span>

<span># Download and load model</span>
!git clone https://huggingface.co/{MODEL_ID} {MODEL_TYPE}

<span># Load model and tokenizer</span>
model = HookedTransformer.from_pretrained_no_processing(
    MODEL_TYPE,
    local_files_only=<span>True</span>,
    dtype=torch.bfloat16,
    default_padding_side=<span>'left'</span>
)
tokenizer = AutoTokenizer.from_pretrained(MODEL_TYPE)
tokenizer.padding_side = <span>'left'</span>
tokenizer.pad_token = tokenizer.eos_token
</code></pre>
<p>We can now tokenize our datasets. We're using the same number of samples for both harmless and harmful instructions. Note that a high number of samples can use all the RAM/VRAM, which is why I'm limiting it to 256 here.</p>
<pre><code><span>def</span> <span>tokenize_instructions</span>(<span>tokenizer, instructions</span>):
    <span>return</span> tokenizer.apply_chat_template(
        instructions,
        padding=<span>True</span>,
        truncation=<span>False</span>,
        return_tensors=<span>"pt"</span>,
        return_dict=<span>True</span>,
        add_generation_prompt=<span>True</span>,
    ).input_ids

n_inst_train = <span>min</span>(<span>256</span>, <span>len</span>(harmful_inst_train), <span>len</span>(harmless_inst_train))

<span># Tokenize datasets</span>
harmful_tokens = tokenize_instructions(
    tokenizer,
    instructions=harmful_inst_train[:n_inst_train],
)
harmless_tokens = tokenize_instructions(
    tokenizer,
    instructions=harmless_inst_train[:n_inst_train],
)
</code></pre>
<p>Everything is set up, we can now implement the first step of abliteration: data collection. We want to process these tokenized datasets and store the residual stream activations in <code>harmful</code> and <code>harmless</code>. This is managed by the <a rel="nofollow" href="https://github.com/TransformerLensOrg/TransformerLens">transformer_lens</a> library.</p>
<pre><code><span># Define batch size based on available VRAM</span>
batch_size = <span>32</span>

<span># Initialize defaultdicts to store activations</span>
harmful = defaultdict(<span>list</span>)
harmless = defaultdict(<span>list</span>)

<span># Process the training data in batches</span>
num_batches = (n_inst_train + batch_size - <span>1</span>) // batch_size
<span>for</span> i <span>in</span> tqdm(<span>range</span>(num_batches)):
    <span>print</span>(i)
    start_idx = i * batch_size
    end_idx = <span>min</span>(n_inst_train, start_idx + batch_size)

    <span># Run models on harmful and harmless prompts, cache activations</span>
    harmful_logits, harmful_cache = model.run_with_cache(
        harmful_tokens[start_idx:end_idx],
        names_filter=<span>lambda</span> hook_name: <span>'resid'</span> <span>in</span> hook_name,
        device=<span>'cpu'</span>,
        reset_hooks_end=<span>True</span>
    )
    harmless_logits, harmless_cache = model.run_with_cache(
        harmless_tokens[start_idx:end_idx],
        names_filter=<span>lambda</span> hook_name: <span>'resid'</span> <span>in</span> hook_name,
        device=<span>'cpu'</span>,
        reset_hooks_end=<span>True</span>
    )

    <span># Collect and store the activations</span>
    <span>for</span> key <span>in</span> harmful_cache:
        harmful[key].append(harmful_cache[key])
        harmless[key].append(harmless_cache[key])

    <span># Flush RAM and VRAM</span>
    <span>del</span> harmful_logits, harmless_logits, harmful_cache, harmless_cache
    gc.collect()
    torch.cuda.empty_cache()

<span># Concatenate the cached activations</span>
harmful = {k: torch.cat(v) <span>for</span> k, v <span>in</span> harmful.items()}
harmless = {k: torch.cat(v) <span>for</span> k, v <span>in</span> harmless.items()}
</code></pre>
<p>We can now compute the refusal direction for each layer. This corresponds to the mean difference between the activations of harmful and harmless instructions, which is then normalized. We sort them in descending order in <code>activation_scored</code>. </p>
<pre><code><span># Helper function to get activation index</span>
<span>def</span> <span>get_act_idx</span>(<span>cache_dict, act_name, layer</span>):
    key = (act_name, layer)
    <span>return</span> cache_dict[utils.get_act_name(*key)]

<span># Compute difference of means between harmful and harmless activations at intermediate layers</span>
activation_layers = [<span>"resid_pre"</span>, <span>"resid_mid"</span>, <span>"resid_post"</span>]
activation_refusals = defaultdict(<span>list</span>)

<span>for</span> layer_num <span>in</span> <span>range</span>(<span>1</span>, model.cfg.n_layers):
    pos = -<span>1</span>  <span># Position index</span>

    <span>for</span> layer <span>in</span> activation_layers:
        harmful_mean_act = get_act_idx(harmful, layer, layer_num)[:, pos, :].mean(dim=<span>0</span>)
        harmless_mean_act = get_act_idx(harmless, layer, layer_num)[:, pos, :].mean(
            dim=<span>0</span>
        )

        refusal_dir = harmful_mean_act - harmless_mean_act
        refusal_dir = refusal_dir / refusal_dir.norm()
        activation_refusals[layer].append(refusal_dir)

<span># Get all calculated potential refusal directions, sort them in descending order based on their mean</span>
<span># Use a subset of layers if certain activations are not promising</span>
selected_layers = [<span>"resid_pre"</span>]
activation_scored = <span>sorted</span>(
    [
        activation_refusals[layer][l - <span>1</span>]
        <span>for</span> l <span>in</span> <span>range</span>(<span>1</span>, model.cfg.n_layers)
        <span>for</span> layer <span>in</span> selected_layers
    ],
    key=<span>lambda</span> x: <span>abs</span>(x.mean()),
    reverse=<span>True</span>,
)
</code></pre>
<p>The final step of the process consists of evaluating the refusal directions we calculated. To do this, we're going to apply the refusal direction to each residual stream and each block during inference. In the following snippet, we get generations for four test harmful instructions and 20 blocks (or layers).</p>
<pre><code><span>def</span> <span>_generate_with_hooks</span>(<span></span>
<span>    model: HookedTransformer,</span>
<span>    tokenizer: AutoTokenizer,</span>
<span>    tokens: Int[Tensor, <span>"batch_size seq_len"</span>],</span>
<span>    max_tokens_generated: <span>int</span> = <span>64</span>,</span>
<span>    fwd_hooks=[],</span>
<span></span>) -&gt; <span>List</span>[<span>str</span>]:
    all_tokens = torch.zeros(
        (tokens.shape[<span>0</span>], tokens.shape[<span>1</span>] + max_tokens_generated),
        dtype=torch.long,
        device=tokens.device,
    )
    all_tokens[:, : tokens.shape[<span>1</span>]] = tokens
    <span>for</span> i <span>in</span> <span>range</span>(max_tokens_generated):
        <span>with</span> model.hooks(fwd_hooks=fwd_hooks):
            logits = model(all_tokens[:, : -max_tokens_generated + i])
            next_tokens = logits[:, -<span>1</span>, :].argmax(
                dim=-<span>1</span>
            )  <span># greedy sampling (temperature=0)</span>
            all_tokens[:, -max_tokens_generated + i] = next_tokens
    <span>return</span> tokenizer.batch_decode(
        all_tokens[:, tokens.shape[<span>1</span>] :], skip_special_tokens=<span>True</span>
    )

<span>def</span> <span>get_generations</span>(<span></span>
<span>    model: HookedTransformer,</span>
<span>    tokenizer: AutoTokenizer,</span>
<span>    instructions: <span>List</span>[<span>str</span>],</span>
<span>    fwd_hooks=[],</span>
<span>    max_tokens_generated: <span>int</span> = <span>64</span>,</span>
<span>    batch_size: <span>int</span> = <span>4</span>,</span>
<span></span>) -&gt; <span>List</span>[<span>str</span>]:
    generations = []
    <span>for</span> i <span>in</span> tqdm(<span>range</span>(<span>0</span>, <span>len</span>(instructions), batch_size)):
        tokens = tokenize_instructions(
            tokenizer, instructions=instructions[i : i + batch_size]
        )
        generation = _generate_with_hooks(
            model,
            tokenizer,
            tokens,
            max_tokens_generated=max_tokens_generated,
            fwd_hooks=fwd_hooks,
        )
        generations.extend(generation)
    <span>return</span> generations

<span># Inference-time intervention hook</span>
<span>def</span> <span>direction_ablation_hook</span>(<span></span>
<span>    activation: Float[Tensor, <span>"... d_act"</span>],</span>
<span>    hook: HookPoint,</span>
<span>    direction: Float[Tensor, <span>"d_act"</span>],</span>
<span></span>):
    <span>if</span> activation.device != direction.device:
        direction = direction.to(activation.device)
    proj = (
        einops.einsum(
            activation, direction.view(-<span>1</span>, <span>1</span>), <span>"... d_act, d_act single -&gt; ... single"</span>
        )
        * direction
    )
    <span>return</span> activation - proj

<span># Testing baseline</span>
N_INST_TEST = <span>4</span>
baseline_generations = get_generations(
    model, tokenizer, harmful_inst_test[:N_INST_TEST], fwd_hooks=[]
)

<span># Evaluating layers defined earlier (needs human evaluation to determine best layer for refusal inhibition)</span>
EVAL_N = <span>20</span>  <span># Evaluate how many of the top N potential directions</span>
evals = []
<span>for</span> refusal_dir <span>in</span> tqdm(activation_scored[:EVAL_N]):
    hook_fn = functools.partial(direction_ablation_hook, direction=refusal_dir)
    fwd_hooks = [
        (utils.get_act_name(act_name, layer), hook_fn)
        <span>for</span> layer <span>in</span> <span>list</span>(<span>range</span>(model.cfg.n_layers))
        <span>for</span> act_name <span>in</span> activation_layers
    ]
    intervention_generations = get_generations(
        model, tokenizer, harmful_inst_test[:N_INST_TEST], fwd_hooks=fwd_hooks
    )
    evals.append(intervention_generations)
</code></pre>
<p>We stored all the generations in the <code>evals</code> list. We can now print them and manually select the layer (block) that provides an uncensored response for each instruction. I'm automatically excluding responses containing "I cannot" and "I can't" to filter out unwanted answers.</p>
<p>If you can't find a layer that satisfies these requirements, you might want to test other residual streams in the previous <code>selected_layers</code> list, other instructions, additional blocks, etc.</p>
<pre><code><span># Print generations for human evaluation</span>
blacklist = [<span>"I cannot"</span>, <span>"I can't"</span>]
<span>for</span> i <span>in</span> <span>range</span>(N_INST_TEST):
    <span>print</span>(<span>f"\033[1mINSTRUCTION <span>{i}</span>: <span>{harmful_inst_test[i]}</span>"</span>)
    <span>print</span>(<span>f"\nBASELINE COMPLETION:\n<span>{baseline_generations[i]}</span>\033[0m"</span>)
    <span>for</span> layer_candidate <span>in</span> <span>range</span>(EVAL_N):
        <span>if</span> <span>not</span> <span>any</span>(word <span>in</span> evals[layer_candidate][i] <span>for</span> word <span>in</span> blacklist):
            <span>print</span>(<span>f"\n---\n\nLAYER CANDIDATE #<span>{layer_candidate}</span> INTERVENTION COMPLETION:"</span>)
            <span>print</span>(evals[layer_candidate][i])
</code></pre>
<p>In my case, the layer candidate 9 managed to provide uncensored answer for the four instructions. This is the one that we will select for the refusal direction. In the following, we implement weight orthogonalization to modify the weights and prevent the model from creating outputs with this direction. You can verify that the model is successfully uncensored by printing the completions.</p>
<pre><code><span>def</span> <span>get_orthogonalized_matrix</span>(<span></span>
<span>    matrix: Float[Tensor, <span>"... d_model"</span>], vec: Float[Tensor, <span>"d_model"</span>]</span>
<span></span>) -&gt; Float[Tensor, <span>"... d_model"</span>]:
    proj = (
        einops.einsum(
            matrix, vec.view(-<span>1</span>, <span>1</span>), <span>"... d_model, d_model single -&gt; ... single"</span>
        )
        * vec
    )
    <span>return</span> matrix - proj

<span># Select the layer with the highest potential refusal direction</span>
LAYER_CANDIDATE = <span>9</span>
refusal_dir = activation_scored[LAYER_CANDIDATE]

<span># Orthogonalize the model's weights</span>
<span>if</span> refusal_dir.device != model.W_E.device:
    refusal_dir = refusal_dir.to(model.W_E.device)
model.W_E.data = get_orthogonalized_matrix(model.W_E, refusal_dir)

<span>for</span> block <span>in</span> tqdm(model.blocks):
    <span>if</span> refusal_dir.device != block.attn.W_O.device:
        refusal_dir = refusal_dir.to(block.attn.W_O.device)
    block.attn.W_O.data = get_orthogonalized_matrix(block.attn.W_O, refusal_dir)
    block.mlp.W_out.data = get_orthogonalized_matrix(block.mlp.W_out, refusal_dir)

<span># Generate text with abliterated model</span>
orthogonalized_generations = get_generations(
    model, tokenizer, harmful_inst_test[:N_INST_TEST], fwd_hooks=[]
)

<span># Print generations</span>
<span>for</span> i <span>in</span> <span>range</span>(N_INST_TEST):
    <span>if</span> <span>len</span>(baseline_generations) &gt; i:
        <span>print</span>(<span>f"INSTRUCTION <span>{i}</span>: <span>{harmful_inst_test[i]}</span>"</span>)
        <span>print</span>(<span>f"\033[92mBASELINE COMPLETION:\n<span>{baseline_generations[i]}</span>"</span>)
    <span>print</span>(<span>f"\033[91mINTERVENTION COMPLETION:\n<span>{evals[LAYER_CANDIDATE][i]}</span>"</span>)
    <span>print</span>(<span>f"\033[95mORTHOGONALIZED COMPLETION:\n<span>{orthogonalized_generations[i]}</span>\n"</span>)
</code></pre>
<p>We're now ready to use the model. We convert it back to the Hugging Face format and upload it to the HF hub.</p>
<pre><code><span># Convert model back to HF safetensors</span>
hf_model = AutoModelForCausalLM.from_pretrained(MODEL_TYPE, torch_dtype=torch.bfloat16)
lm_model = hf_model.model

state_dict = model.state_dict()
lm_model.embed_tokens.weight = torch.nn.Parameter(state_dict[<span>"embed.W_E"</span>].cpu())

<span>for</span> l <span>in</span> <span>range</span>(model.cfg.n_layers):
    lm_model.layers[l].self_attn.o_proj.weight = torch.nn.Parameter(
        einops.rearrange(
            state_dict[<span>f"blocks.<span>{l}</span>.attn.W_O"</span>], <span>"n h m-&gt;m (n h)"</span>, n=model.cfg.n_heads
        ).contiguous()
    )
    lm_model.layers[l].mlp.down_proj.weight = torch.nn.Parameter(
        torch.transpose(state_dict[<span>f"blocks.<span>{l}</span>.mlp.W_out"</span>], <span>0</span>, <span>1</span>).contiguous()
    )

hf_model.push_to_hub(<span>f"<span>{MODEL_ID}</span>-abliterated"</span>)
<span># hf_model.push_to_hub(f"{MODEL_ID}-abliterated")</span>
</code></pre>
<h2>
	<a rel="nofollow" href="#⚖️-dpo-fine-tuning" id="⚖️-dpo-fine-tuning">
		<span></span>
	</a>
	<span>
		⚖️ DPO Fine-Tuning
	</span>
</h2>
<p>I evaluated the abliterated and source models from the previous section on the Open LLM Leaderboard and on Nous' benchmark suite. Here are the results:</p>
<p><a rel="nofollow" href="https://i.imgur.com/ECCejII.png"><img alt="" src="https://i.imgur.com/ECCejII.png"></a></p>
<p>As you can see, the source model significantly outperforms Llama 3 8B Instruct. However, we observe a performance drop in the ablated version across all benchmarks. The ablation process successfully uncensored it but also degraded the model's quality.</p>
<p>To address this issue, an idea consists of further training our abliterated model to heal it. Like most fine-tuned models, Llama 3 8B Instruct is quite brittle when it comes to supervised fine-tuning. An additional SFT would likely break the model's performance.</p>
<p>Alternatively, preference alignment is quite light and shouldn't lobotomize our abliterated model. DPO is a good candidate here for its ease of use and good track record. To implement it, I used <a rel="nofollow" href="https://colab.research.google.com/drive/1TsDKNo2riwVmU55gjuBgB1AXVtRRfRHW?usp=sharing">LazyAxolotl</a> with the <a href="https://huggingface.co/datasets/mlabonne/orpo-dpo-mix-40k">mlabonne/orpo-dpo-mix-40k</a> dataset. Here's the configuration I used:</p>
<pre><code><span>base_model:</span> <span>mlabonne/Daredevil-8B-abliterated</span>
<span>model_type:</span> <span>LlamaForCausalLM</span>
<span>tokenizer_type:</span> <span>AutoTokenizer</span>

<span>load_in_8bit:</span> <span>false</span>
<span>load_in_4bit:</span> <span>true</span>
<span>strict:</span> <span>false</span>
<span>save_safetensors:</span> <span>true</span>

<span>rl:</span> <span>dpo</span>
<span>chat_template:</span> <span>chatml</span>
<span>datasets:</span>
  <span>-</span> <span>path:</span> <span>mlabonne/orpo-dpo-mix-40k-flat</span>
    <span>split:</span> <span>train</span>
    <span>type:</span> <span>chatml.intel</span>

<span>dataset_prepared_path:</span>
<span>val_set_size:</span> <span>0.0</span>
<span>output_dir:</span> <span>./out</span>

<span>adapter:</span> <span>qlora</span>
<span>lora_model_dir:</span>

<span>sequence_len:</span> <span>2048</span>
<span>sample_packing:</span> <span>false</span>
<span>pad_to_sequence_len:</span> <span>false</span>

<span>lora_r:</span> <span>64</span>
<span>lora_alpha:</span> <span>32</span>
<span>lora_dropout:</span> <span>0.05</span>
<span>lora_target_linear:</span> <span>true</span>
<span>lora_fan_in_fan_out:</span>

<span>wandb_project:</span> <span>axolotl</span>
<span>wandb_entity:</span>
<span>wandb_watch:</span>
<span>wandb_name:</span>
<span>wandb_log_model:</span>

<span>gradient_accumulation_steps:</span> <span>8</span>
<span>micro_batch_size:</span> <span>1</span>
<span>num_epochs:</span> <span>1</span>
<span>optimizer:</span> <span>paged_adamw_8bit</span>
<span>lr_scheduler:</span> <span>cosine</span>
<span>learning_rate:</span> <span>5e-6</span>
<span>train_on_inputs:</span> <span>false</span>
<span>group_by_length:</span> <span>false</span>

<span>bf16:</span> <span>auto</span>
<span>fp16:</span>
<span>tf32:</span>

<span>gradient_checkpointing:</span> <span>true</span>
<span>early_stopping_patience:</span>
<span>resume_from_checkpoint:</span>
<span>local_rank:</span>
<span>logging_steps:</span> <span>1</span>
<span>xformers_attention:</span>
<span>flash_attention:</span> <span>true</span>
<span>warmup_steps:</span> <span>100</span>
<span>evals_per_epoch:</span> <span>0</span>
<span>eval_table_size:</span>
<span>eval_table_max_new_tokens:</span> <span>128</span>
<span>saves_per_epoch:</span> <span>1</span>
<span>debug:</span>
<span>deepspeed:</span> <span>deepspeed_configs/zero2.json</span>
<span>weight_decay:</span> <span>0.0</span>
<span>special_tokens:</span>
  <span>pad_token:</span> <span>&lt;|end_of_text|&gt;</span>
</code></pre>
<p>I trained it using 6xA6000 GPUs with DeepSpeed ZeRO-2. The training took about 6 hours and 45 minutes. Here are the training curves I got from W&amp;B:</p>
<p><a rel="nofollow" href="https://i.imgur.com/nVcJYuu.png"><img alt="" src="https://i.imgur.com/nVcJYuu.png"></a></p>
<p>It automatically uploaded the DPO fine-tuned model, called <a href="https://huggingface.co/mlabonne/NeuralDaredevil-8B-abliterated">mlabonne/NeuralDaredevil-8B-abliterated</a>. To see if it fixed our abliterated version, I evaluated it on the same benchmarks:</p>
<p><a rel="nofollow" href="https://i.imgur.com/ChDwx4r.png"><img alt="" src="https://i.imgur.com/ChDwx4r.png"></a></p>
<p>We can see that this additional training allowed us to recover most of the performance drop due to abliteration. One area where the model doesn't improve is GSM8K, a math dataset, which could mean the orpo-dpo-mix-40k would benefit from more math samples.</p>
<p>The final model is an uncensored LLM with state-of-the-art performance in the 8B category. I recommend it as an improved version of Llama 3 8B Instruct when you don't need censorship. You can play with quantized versions like GGUF in LM Studio.</p>
<h2>
	<a rel="nofollow" href="#conclusion" id="conclusion">
		<span></span>
	</a>
	<span>
		Conclusion
	</span>
</h2>
<p>In this article, we introduced the concept of abliteration. This technique uses the model's activations on harmless and harmful prompts to calculate a refusal direction. It then uses this direction to modify the model's weights and ensure that we stop outputting refusals. This technique also demonstrates the fragility of safety fine-tuning and raises ethical considerations.</p>
<p>We applied abliteration to Daredevil-8B to uncensor it, which also degraded the model's performance. We then healed it using DPO to create the NeuralDaredevil-8B model, a fully uncensored and high-quality 8B LLM. Abliteration is not limited to removing alignment and should be seen as a form of fine-tuning without retraining. Indeed, it can creatively be applied to other goals, like FailSpy's <a href="https://huggingface.co/failspy/Llama-3-8B-Instruct-MopeyMule">MopeyMule</a>, which adopts a melancholic conversational style.</p>
<p>I hope you liked this article. If you want to see more follow me on&nbsp;<a href="https://huggingface.co/mlabonne/">Hugging Face</a>&nbsp;and Twitter&nbsp;<a rel="nofollow" href="https://twitter.com/maximelabonne">@maximelabonne</a>.</p>
<h2>
	<a rel="nofollow" href="#references" id="references">
		<span></span>
	</a>
	<span>
		References
	</span>
</h2>
<ul>
<li>FailSpy, "<a rel="nofollow" href="https://github.com/FailSpy/abliterator">abliterator library</a>," GitHub, 2024.</li>
<li>Andy Arditi, Oscar Obeso, Aaquib111, wesg, Neel Nanda, "<a rel="nofollow" href="https://www.lesswrong.com/posts/jGuXSZgv6qfdhMCuJ/refusal-in-llms-is-mediated-by-a-single-direction">Refusal in LLMs is mediated by a single direction</a>," Lesswrong, 2024.</li>
</ul>
<!-- HTML_TAG_END --></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[How Meta trains large language models at scale (353 pts)]]></title>
            <link>https://engineering.fb.com/2024/06/12/data-infrastructure/training-large-language-models-at-scale-meta/</link>
            <guid>40664339</guid>
            <pubDate>Wed, 12 Jun 2024 23:35:08 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://engineering.fb.com/2024/06/12/data-infrastructure/training-large-language-models-at-scale-meta/">https://engineering.fb.com/2024/06/12/data-infrastructure/training-large-language-models-at-scale-meta/</a>, See on <a href="https://news.ycombinator.com/item?id=40664339">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>

		<p><span>As we continue to focus our AI research and development on solving increasingly complex problems, one of the most significant and challenging shifts we’ve experienced is the sheer scale of computation required to train large language models (LLMs).</span></p>
<p><span>Traditionally, our AI model training has involved a training massive number of models that required a comparatively smaller number of GPUs. This was the case for our recommendation models (e.g., our feed and ranking models) that would ingest vast amounts of information to make accurate recommendations that power most of our products.</span></p>
<p><img decoding="async" src="https://engineering.fb.com/wp-content/uploads/2024/06/Training-LLMs-at-Scale-image-1.png?w=1024" alt="" width="1024" height="576" srcset="https://engineering.fb.com/wp-content/uploads/2024/06/Training-LLMs-at-Scale-image-1.png 2500w, https://engineering.fb.com/wp-content/uploads/2024/06/Training-LLMs-at-Scale-image-1.png?resize=580,326 580w, https://engineering.fb.com/wp-content/uploads/2024/06/Training-LLMs-at-Scale-image-1.png?resize=916,515 916w, https://engineering.fb.com/wp-content/uploads/2024/06/Training-LLMs-at-Scale-image-1.png?resize=768,432 768w, https://engineering.fb.com/wp-content/uploads/2024/06/Training-LLMs-at-Scale-image-1.png?resize=1024,576 1024w, https://engineering.fb.com/wp-content/uploads/2024/06/Training-LLMs-at-Scale-image-1.png?resize=1536,864 1536w, https://engineering.fb.com/wp-content/uploads/2024/06/Training-LLMs-at-Scale-image-1.png?resize=2048,1152 2048w, https://engineering.fb.com/wp-content/uploads/2024/06/Training-LLMs-at-Scale-image-1.png?resize=96,54 96w, https://engineering.fb.com/wp-content/uploads/2024/06/Training-LLMs-at-Scale-image-1.png?resize=192,108 192w" sizes="(max-width: 992px) 100vw, 62vw"></p>
<p><span>With the advent of generative AI (GenAI), we’ve seen a shift towards fewer jobs, but incredibly large ones. Supporting GenAI at scale has meant rethinking how our software, hardware, and network infrastructure come together.</span></p>
<h2><span>The challenges of large-scale model training</span></h2>
<p><img decoding="async" src="https://engineering.fb.com/wp-content/uploads/2024/06/Training-LLMs-at-Scale-image-2.png?w=1024" alt="" width="1024" height="576" srcset="https://engineering.fb.com/wp-content/uploads/2024/06/Training-LLMs-at-Scale-image-2.png 2500w, https://engineering.fb.com/wp-content/uploads/2024/06/Training-LLMs-at-Scale-image-2.png?resize=580,326 580w, https://engineering.fb.com/wp-content/uploads/2024/06/Training-LLMs-at-Scale-image-2.png?resize=916,515 916w, https://engineering.fb.com/wp-content/uploads/2024/06/Training-LLMs-at-Scale-image-2.png?resize=768,432 768w, https://engineering.fb.com/wp-content/uploads/2024/06/Training-LLMs-at-Scale-image-2.png?resize=1024,576 1024w, https://engineering.fb.com/wp-content/uploads/2024/06/Training-LLMs-at-Scale-image-2.png?resize=1536,864 1536w, https://engineering.fb.com/wp-content/uploads/2024/06/Training-LLMs-at-Scale-image-2.png?resize=2048,1152 2048w, https://engineering.fb.com/wp-content/uploads/2024/06/Training-LLMs-at-Scale-image-2.png?resize=96,54 96w, https://engineering.fb.com/wp-content/uploads/2024/06/Training-LLMs-at-Scale-image-2.png?resize=192,108 192w" sizes="(max-width: 992px) 100vw, 62vw"></p>
<p><span>As we increase the number of GPUs in a job, the likelihood of an interruption due to a hardware failure also increases. Also, all of these GPUs still need to communicate on the same high-speed fabric to perform optimally. This underscores the importance of four factors:</span></p>
<ul>
<li aria-level="1"><b>Hardware reliability</b><span>: Ensuring that our hardware is reliable is important. We need to minimize the chances of a hardware failure interrupting a training job. This involves rigorous testing and quality control measures, and automation to quickly detect and remediate issues.</span></li>
<li aria-level="1"><b>Fast recovery on failure</b><span>: Despite our best efforts, hardware failures can and do occur. When they do, we need to be able to recover quickly. This involves reducing re-scheduling overhead and fast training re-initialization.</span></li>
<li aria-level="1"><b>Efficient preservation of the training state</b><span>: In the event of a failure, we need to be able to pick up where we left off. This means we need to regularly checkpoint our training state and efficiently store and retrieve training data.</span></li>
<li aria-level="1"><b>Optimal connectivity between GPUs:</b><span> Large-scale model training involves transferring vast amounts of data between GPUs in a synchronized fashion. A slow data exchange between a subset of GPUs can compound and slow down the whole job. Solving this problem requires a robust and high-speed network infrastructure as well as efficient data transfer protocols and algorithms.&nbsp;</span></li>
</ul>
<h2><span>Innovating across the infrastructure stack</span></h2>
<p><span>Perfecting every layer of our infrastructure stack is important due to the demands of GenAI at scale. This has encompassed developments in a wide range of areas.</span></p>
<h3><span>Training software</span></h3>
<p><span>We enable researchers to use </span><a href="https://pytorch.org/blog/training-production-ai-models/"><span>PyTorch</span></a><span> and other new open source developments, facilitating extremely fast research-to-production development. This includes </span><span>developing new algorithms and techniques for efficient large-scale training and integrating new software tools and frameworks into our infrastructure.</span></p>
<h3><span>Scheduling</span></h3>
<p><span>Efficient scheduling helps ensure that our resources are used optimally. This involves </span><span>sophisticated algorithms that can allocate resources based on the needs of different jobs and dynamic scheduling to adapt to changing workloads.</span></p>
<h3><span>Hardware&nbsp;</span></h3>
<p><span>We need high-performance hardware to handle the computational demands of large-scale model training. Beyond size and scale, many hardware configurations and attributes need to be best optimized for GenAI. Given that hardware development times are traditionally long, we had to adapt existing hardware, and to this end we explored various dimensions including power, HBM capacity and speed, and I/O.&nbsp;</span></p>
<p><span>We also pivoted by modifying the </span><a href="https://engineering.fb.com/2022/10/18/open-source/ocp-summit-2022-grand-teton/"><span>Grand Teton</span></a><span> platform that was developed using NVIDIA H100 GPUs, increased the TDP of the GPUs to 700W, and moved to HBM3 on the GPUs. Since we did not have time to change the cooling infrastructure, we had to remain in an air-cooled environment. The mechanical and thermal designs had to change to accommodate this, and that triggered a validation cycle to support a large-scale deployment.&nbsp;</span></p>
<p><span>All of these hardware-related changes were challenging because we had to find a solution that fit within the existing resource constraints, with a very small degree of freedom to change and meet a tight schedule.</span></p>
<h3><span>Data center deployment</span></h3>
<p><span>Once we’ve chosen a GPU and system, the task of placing them in a data center for optimal usage of resources (power, cooling, networking, etc.) requires revisiting trade-offs made for other types of workloads. Data center power and cooling infrastructure cannot be changed quickly (or easily) and we had to find an optimal layout that allowed maximum compute capability within a data hall. This required relocating supporting services such as readers out of the data hall and packing as many GPU racks as possible to maximize the power and network capability for highest compute density with the largest network cluster.&nbsp;</span></p>
<h3><span>Reliability&nbsp;</span></h3>
<p><span>We need to plan for detection and remediation to minimize downtime during hardware failures. The number of failures scales with the size of the cluster, and having a job that spans the cluster makes it necessary to keep adequate spare capacity to restart the job as soon as possible. In addition, we monitor failures and can sometimes take preventive measures to mitigate downtime.&nbsp;</span></p>
<p><img loading="lazy" decoding="async" src="https://engineering.fb.com/wp-content/uploads/2024/06/Training-LLMs-at-Scale-image-5.png?w=1024" alt="" width="1024" height="576" srcset="https://engineering.fb.com/wp-content/uploads/2024/06/Training-LLMs-at-Scale-image-5.png 2500w, https://engineering.fb.com/wp-content/uploads/2024/06/Training-LLMs-at-Scale-image-5.png?resize=580,326 580w, https://engineering.fb.com/wp-content/uploads/2024/06/Training-LLMs-at-Scale-image-5.png?resize=916,515 916w, https://engineering.fb.com/wp-content/uploads/2024/06/Training-LLMs-at-Scale-image-5.png?resize=768,432 768w, https://engineering.fb.com/wp-content/uploads/2024/06/Training-LLMs-at-Scale-image-5.png?resize=1024,576 1024w, https://engineering.fb.com/wp-content/uploads/2024/06/Training-LLMs-at-Scale-image-5.png?resize=1536,864 1536w, https://engineering.fb.com/wp-content/uploads/2024/06/Training-LLMs-at-Scale-image-5.png?resize=2048,1152 2048w, https://engineering.fb.com/wp-content/uploads/2024/06/Training-LLMs-at-Scale-image-5.png?resize=96,54 96w, https://engineering.fb.com/wp-content/uploads/2024/06/Training-LLMs-at-Scale-image-5.png?resize=192,108 192w" sizes="(max-width: 992px) 100vw, 62vw"></p>
<p><span>Some of the most frequent failure modes we have observed are:</span></p>
<ul>
<li aria-level="1"><b>GPUs falling off:</b><span> In this case, GPUs are not detected by the host on PCIe. There are several reasons for this failure, but this failure mode is seen more in the early life and settles as the server ages.</span></li>
<li aria-level="1"><b>DRAM &amp; SRAM UCE:</b><span> Uncorrectable errors are common in memories, and we monitor and identify repeat offenders, track against thresholds, and initiate RMAs when error rates exceed vendor thresholds.</span></li>
<li aria-level="1"><b>HW network cable:</b><span> In the general category of unreachable servers, these failures are also seen most often in the early life of the server.&nbsp;</span></li>
</ul>
<h3><span>Network</span></h3>
<p><span>Large-scale model training involves transferring vast amounts of data quickly between GPUs. This requires robust and high-speed network infrastructure as well as efficient data transfer protocols and algorithms.&nbsp;</span></p>
<p><span>There are two leading choices in the industry that fit these requirements: RoCE and InfiniBand fabrics. Both of these options had tradeoffs. On the one hand, Meta had built RoCE clusters for the past four years, but the largest of those clusters only supported 4K GPUs. We needed significantly larger RoCE clusters. On the other hand, Meta had built research clusters with InfiniBand as </span><a href="https://ai.meta.com/blog/ai-rsc/"><span>large as 16K GPUs</span></a><span>. However, those clusters were </span><i><span>not</span></i><span> tightly integrated into Meta’s production environment, nor were they built for the latest generation of GPUs/networking. This made for a difficult decision of what fabric to build with.</span></p>
<p><span>So we decided to build both: </span><a href="https://engineering.fb.com/2024/03/12/data-center-engineering/building-metas-genai-infrastructure/"><span>two 24k clusters</span></a><span>, one with RoCE and another with InfiniBand. Our intent was to build and learn from the operational experience. These learnings will inform the future direction of GenAI fabrics. We optimized the RoCE cluster for quick build time, and the InfiniBand cluster for full-bisection bandwidth. We used both InfiniBand and RoCE clusters to train </span><a href="https://ai.meta.com/blog/meta-llama-3/"><span>Llama 3</span></a><span>, with the RoCE cluster used for training the largest model. Despite the underlying network technology differences between these clusters, we were able to tune both of them to provide equivalent performance for these large GenAI workloads</span></p>
<p><img loading="lazy" decoding="async" src="https://engineering.fb.com/wp-content/uploads/2024/06/Training-LLMs-at-Scale-image-3.png?w=1024" alt="" width="1024" height="576" srcset="https://engineering.fb.com/wp-content/uploads/2024/06/Training-LLMs-at-Scale-image-3.png 2500w, https://engineering.fb.com/wp-content/uploads/2024/06/Training-LLMs-at-Scale-image-3.png?resize=580,326 580w, https://engineering.fb.com/wp-content/uploads/2024/06/Training-LLMs-at-Scale-image-3.png?resize=916,515 916w, https://engineering.fb.com/wp-content/uploads/2024/06/Training-LLMs-at-Scale-image-3.png?resize=768,432 768w, https://engineering.fb.com/wp-content/uploads/2024/06/Training-LLMs-at-Scale-image-3.png?resize=1024,576 1024w, https://engineering.fb.com/wp-content/uploads/2024/06/Training-LLMs-at-Scale-image-3.png?resize=1536,864 1536w, https://engineering.fb.com/wp-content/uploads/2024/06/Training-LLMs-at-Scale-image-3.png?resize=2048,1152 2048w, https://engineering.fb.com/wp-content/uploads/2024/06/Training-LLMs-at-Scale-image-3.png?resize=96,54 96w, https://engineering.fb.com/wp-content/uploads/2024/06/Training-LLMs-at-Scale-image-3.png?resize=192,108 192w" sizes="(max-width: 992px) 100vw, 62vw"></p>
<p><span>We optimized three aspects of the overall stack to make network communication for GenAI models performant on both clusters:</span></p>
<ol>
<li><span>We assigned communication patterns resulting from different model, data and pipeline parallelisms to different layers of the network topology so that the network capabilities were effectively exploited.</span></li>
<li><span>We implemented collective communication patterns with network topology awareness so that they can be less latency-sensitive. We do this by changing the default implementation of collectives with custom algorithms such as recursive doubling or halving instead of conventional algorithms like rings.</span></li>
<li><span>Just like ranking jobs, GenAI jobs produce additional fat flows that make it hard to distribute traffic across all possible network paths. This required us to further invest in network load balancing and routing to achieve an optimal distribution of traffic across network resources.</span></li>
</ol>
<p><span>We spoke in depth about our </span><a href="https://atscaleconference.com/videos/scaling-roce-networks-for-ai-training/"><span>RoCE load-balancing techniques</span></a><span> at </span><a href="https://atscaleconference.com/videos/scaling-roce-networks-for-ai-training/"><span>Networking @Scale 2023</span></a><span>.</span></p>
<p><img loading="lazy" decoding="async" src="https://engineering.fb.com/wp-content/uploads/2024/06/Training-LLMs-at-Scale-image-4.png?w=1024" alt="" width="1024" height="576" srcset="https://engineering.fb.com/wp-content/uploads/2024/06/Training-LLMs-at-Scale-image-4.png 2500w, https://engineering.fb.com/wp-content/uploads/2024/06/Training-LLMs-at-Scale-image-4.png?resize=580,326 580w, https://engineering.fb.com/wp-content/uploads/2024/06/Training-LLMs-at-Scale-image-4.png?resize=916,515 916w, https://engineering.fb.com/wp-content/uploads/2024/06/Training-LLMs-at-Scale-image-4.png?resize=768,432 768w, https://engineering.fb.com/wp-content/uploads/2024/06/Training-LLMs-at-Scale-image-4.png?resize=1024,576 1024w, https://engineering.fb.com/wp-content/uploads/2024/06/Training-LLMs-at-Scale-image-4.png?resize=1536,864 1536w, https://engineering.fb.com/wp-content/uploads/2024/06/Training-LLMs-at-Scale-image-4.png?resize=2048,1152 2048w, https://engineering.fb.com/wp-content/uploads/2024/06/Training-LLMs-at-Scale-image-4.png?resize=96,54 96w, https://engineering.fb.com/wp-content/uploads/2024/06/Training-LLMs-at-Scale-image-4.png?resize=192,108 192w" sizes="(max-width: 992px) 100vw, 62vw"></p>
<h3><span>Storage</span></h3>
<p><span>We need efficient data-storage solutions to store the vast amounts of data used in model training. This involves investing in high-capacity and high-speed storage technologies and developing new data-storage solutions for specific workloads.</span></p>
<h2><span>Looking ahead</span></h2>
<p><span>In the next few years w</span><span>e will be working with hundreds of thousands of GPUs, handling even larger volumes of data, and dealing with longer distances and latencies. We’ll be adopting new hardware technologies—including newer GPU architectures—and evolving our infrastructure.&nbsp;</span></p>
<p><span>These challenges will push us to innovate and adapt in ways we can’t fully predict yet. But one thing is certain: We are only at the beginning of this journey. As we continue to navigate the evolving landscape of AI, we remain committed to pushing the boundaries of what’s possible.</span></p>

		
	</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Gerald Sussman: Programming is (should be) fun (2022) [video] (246 pts)]]></title>
            <link>https://www.youtube.com/watch?v=2MYzvQ1v8Ww</link>
            <guid>40663704</guid>
            <pubDate>Wed, 12 Jun 2024 22:02:27 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.youtube.com/watch?v=2MYzvQ1v8Ww">https://www.youtube.com/watch?v=2MYzvQ1v8Ww</a>, See on <a href="https://news.ycombinator.com/item?id=40663704">Hacker News</a></p>
&lt;Unparsable&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[Japan enacts law to curb Apple, Google's app dominance (411 pts)]]></title>
            <link>https://english.kyodonews.net/news/2024/06/bc2d7f45d456-japan-enacts-law-to-curb-apple-googles-app-dominance.html#google_vignette</link>
            <guid>40662176</guid>
            <pubDate>Wed, 12 Jun 2024 19:43:50 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://english.kyodonews.net/news/2024/06/bc2d7f45d456-japan-enacts-law-to-curb-apple-googles-app-dominance.html#google_vignette">https://english.kyodonews.net/news/2024/06/bc2d7f45d456-japan-enacts-law-to-curb-apple-googles-app-dominance.html#google_vignette</a>, See on <a href="https://news.ycombinator.com/item?id=40662176">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
                    <p>Japan's parliament enacted Wednesday a law to promote competition in smartphone app stores by restricting tech giants Apple Inc. and Google LLC from limiting third-party companies from selling and operating apps on their platforms.</p>
<p>The law will prohibit the providers of Apple's iOS and Google's Android smartphone operating systems, app stores and payment platforms from preventing the sale of apps and services that directly compete with the native platforms' own.</p>
<p>The change is aimed at stopping the dominant players from gatekeeping and forcing them to engage in price competition with smaller challengers in hopes of benefiting consumers and promoting innovation.</p>
<p>The law will also prohibit the tech giants from giving priority to their own services in internet search results.</p>
<div><p><img src="https://img.kyodonews.net/english/public/images/posts/e12df7bdd2bc87d2b43aeaaaab5d93b4/photo_l.jpg" width="100%"></p><p><em>The House of Councillors convenes a plenary session in parliament in Tokyo on June 12, 2024. (Kyodo)</em></p>
</div>
<p>Violations of the new law will bring a penalty of 20 percent of the domestic revenue of the service found to have breached the rules. The fine can increase to 30 percent if the companies do not cease the anticompetitive practices.</p>
<p>The new penalty is more than triple the existing fine under the antimonopoly law, which imposes fines of 6 percent of revenue gained through services deemed to be using an anticompetitive edge.</p>
<p>The new law, expected to take effect by the end of 2025, follows a similar regulation introduced by the European Union in March.</p>
<p>The technology giants, which will be designated by the Fair Trade Commission, are to be required to submit regulatory compliance reports and will be monitored by the commission to ensure they are following the rules.</p>
<p>The legislation, which was approved by the House of Representatives in May, was enacted after being passed by the House of Councillors on Wednesday.</p>
<hr>
<p><em>Related coverage:</em></p>
<p><a title="Kyodo News Plus" href="https://english.kyodonews.net/news/2024/04/6b04c1e9d2b4-japan-cabinet-oks-bill-to-challenge-apple-google-app-store-duopoly.html" target="_blank" rel="noopener"><span><strong>Japan Cabinet OKs bill to challenge Apple-Google app store duopoly</strong></span></a></p>
<p><a title="Kyodo News Plus" href="https://english.kyodonews.net/news/2024/04/ba9866f5a54f-meta-sued-over-investment-ads-with-fake-celebrity-endorsements.html" target="_blank" rel="noopener"><span><strong>Meta sued in Japan for investment ads with fake celebrity endorsement</strong></span></a></p>
<hr>
            </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[ChromeOS will soon be developed on large portions of the Android stack (278 pts)]]></title>
            <link>https://blog.chromium.org/2024/06/building-faster-smarter-chromebook.html</link>
            <guid>40661703</guid>
            <pubDate>Wed, 12 Jun 2024 19:02:46 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://blog.chromium.org/2024/06/building-faster-smarter-chromebook.html">https://blog.chromium.org/2024/06/building-faster-smarter-chromebook.html</a>, See on <a href="https://news.ycombinator.com/item?id=40661703">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-version="1" id="header">
<div>
<p><a href="https://blog.chromium.org/">
<img alt="Chromium Blog" height="50" src="https://1.bp.blogspot.com/-vkF7AFJOwBk/VkQxeAGi1mI/AAAAAAAARYo/57denvsQ8zA/s1600-r/logo_chromium.png">
</a></p><a href="https://blog.chromium.org/">
<h2>
            Chromium Blog
          </h2>
</a>
</div>
<p>
News and developments from the open source browser project
</p>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[The Swift compiler is slow due to how types are inferred (214 pts)]]></title>
            <link>https://danielchasehooper.com/posts/why-swift-is-slow/</link>
            <guid>40661001</guid>
            <pubDate>Wed, 12 Jun 2024 17:59:30 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://danielchasehooper.com/posts/why-swift-is-slow/">https://danielchasehooper.com/posts/why-swift-is-slow/</a>, See on <a href="https://news.ycombinator.com/item?id=40661001">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>The Swift compiler is notoriously slow due to how types are inferred<sup id="fnref:1"><a href="#fn:1" role="doc-noteref">1</a></sup>. Every June I hope that Apple will announce that they fixed it; sadly this is not that year.</p><p>Here’s an explanation by the creator of Swift, Chris Lattner (From his <a href="https://www.youtube.com/watch?v=9ag0fPMmYPQ&amp;t=373s" target="_blank" rel="noopener">Mojo talk</a>):</p><blockquote><p>My experience with Swift is we tried to make a really fancy bi-directional Hindley-Milner type checker and it’s really great because you can have very beautiful minimal syntax but the problem is that A) compile times are really bad (particularly if you have complicated expressions) and B) the error messages are awful because now you have global constraint systems and when something goes wrong you have to infer what happened and the user can’t know that something over there made it so something over here can’t type check. In my experience it sounds great but it doesn’t work super well.</p></blockquote><p>Let me explain what he means with an example:</p><div><pre tabindex="0"><code data-lang="swift"><span><span><span>enum</span> <span>ThreatLevel</span> <span>{</span>
</span></span><span><span>    <span>case</span> <span>red</span>
</span></span><span><span>    <span>case</span> <span>midnight</span>
</span></span><span><span><span>}</span>
</span></span><span><span>
</span></span><span><span><span>enum</span> <span>KeyTime</span> <span>{</span>
</span></span><span><span>    <span>case</span> <span>midnight</span>
</span></span><span><span>    <span>case</span> <span>midday</span>
</span></span><span><span><span>}</span>
</span></span><span><span>
</span></span><span><span><span>func</span> <span>setThreatLevel</span><span>(</span><span>_</span> <span>level</span><span>:</span> <span>ThreatLevel</span><span>)</span> <span>{...}</span>
</span></span><span><span>
</span></span><span><span><span>setThreatLevel</span><span>(.</span><span>midnight</span><span>)</span>
</span></span></code></pre></div><p>The <code>.midnight</code> on the last line could represent <code>ThreatLevel.midnight</code> or <code>KeyTime.midnight</code>. The Swift compiler has to use the surrounding context of <code>setThreatLevel()</code>, which has the type <code>(ThreatLevel)-&gt;Void</code>, to infer that we mean <code>ThreatLevel.midnight</code>. After the Swift compiler parses code into an abstract syntax tree, child nodes influence their parent’s type <em>and</em> parent nodes influence their children’s types (that’s what Chris means by “bi-directional”). Compare this to the Zig language, in which types are determined without looking at the surrounding code.</p><p>This approach becomes a problem when expressions contain many elements that each need their types inferred, with each affecting the others. This often occurs due to Swift’s operator overloading, and the <a href="https://developer.apple.com/documentation/swift/initialization-with-literals" target="_blank" rel="noopener">ExpressibleBy protocols</a>. Every literal (string, number, boolean, dictionary, array) and every operator (* / + - etc) multiply the combinations the type checker must consider.</p><p>Here’s an example:</p><div><pre tabindex="0"><code data-lang="swift"><span><span><span>let</span> <span>address</span> <span>=</span> <span>"127.0.0.1"</span>
</span></span><span><span><span>let</span> <span>username</span> <span>=</span> <span>"steve"</span>
</span></span><span><span><span>let</span> <span>password</span> <span>=</span> <span>"1234"</span>
</span></span><span><span><span>let</span> <span>channel</span> <span>=</span> <span>11</span>
</span></span><span><span>
</span></span><span><span><span>let</span> <span>url</span> <span>=</span> <span>"http://"</span> <span>+</span> <span>username</span> 
</span></span><span><span>            <span>+</span> <span>":"</span> <span>+</span> <span>password</span> 
</span></span><span><span>            <span>+</span> <span>"@"</span> <span>+</span> <span>address</span> 
</span></span><span><span>            <span>+</span> <span>"/api/"</span> <span>+</span> <span>channel</span> 
</span></span><span><span>            <span>+</span> <span>"/picture"</span>
</span></span><span><span>
</span></span><span><span><span>print</span><span>(</span><span>url</span><span>)</span>
</span></span></code></pre></div><p><code>swiftc</code> spends 42 seconds on these 12 lines on an M1 Pro<sup id="fnref:2"><a href="#fn:2" role="doc-noteref">2</a></sup>, only to spit out the notorious <code>error: the compiler is unable to type-check this expression in reasonable time; try breaking up the expression into distinct sub-expressions</code>. In the same amount of time, clang can perform a clean build of my 59,000 line C project <em>38 times</em>.</p><p>The issue is caused by using the <code>+</code> operator with the <code>channel</code> Int and a String literal. Thanks to the standard library’s 17 overloads of <code>+</code> and 9 types adopting the <code>ExpressibleByStringLiteral</code> Protocol, the swift compiler can’t rule out that there <em>might</em> be a combination of types and operators that make the expression valid, so it has to try them all. Just considering that the five string literals could be one of the possible nine types results in 59,049 combinations, but I suspect that’s a lower bound, since it doesn’t consider the many overloads of <code>+</code>. It gives up before getting through them all.</p><p>You can fix the code by converting <code>channel</code> to String:</p><div><pre tabindex="0"><code data-lang="swift"><span><span><span>let</span> <span>url</span> <span>=</span> <span>"http://"</span> <span>+</span> <span>username</span> 
</span></span><span><span>            <span>+</span> <span>":"</span> <span>+</span> <span>password</span> 
</span></span><span><span>            <span>+</span> <span>"@"</span> <span>+</span> <span>address</span> 
</span></span><span><span>            <span>+</span> <span>"/api/"</span> <span>+</span> <span>String</span><span>(</span><span>channel</span><span>)</span> 
</span></span><span><span>            <span>+</span> <span>"/picture"</span>
</span></span></code></pre></div><p>This now successfully compiles in 0.19 seconds!</p><p>Maybe you think strings are complicated or something, so here’s an example that is just math:</p><div><pre tabindex="0"><code data-lang="swift"><span><span><span>let</span> <span>offset</span><span>:</span> <span>Double</span> <span>=</span> <span>5.0</span><span>;</span>
</span></span><span><span><span>let</span> <span>index</span><span>:</span> <span>Int</span> <span>=</span> <span>10</span><span>;</span>
</span></span><span><span><span>let</span> <span>angle</span> <span>=</span> <span>(</span><span>180.0</span> <span>-</span> <span>offset</span> <span>+</span> <span>index</span> <span>*</span> <span>5.0</span><span>)</span> <span>*</span> <span>.</span><span>pi</span> <span>/</span> <span>180</span><span>;</span>
</span></span></code></pre></div><p>Again, we get <code>error: the compiler is unable to type-check this expression in reasonable time; try breaking up the expression into distinct sub-expressions</code>, this time after “only” 8 seconds. The problem is due to <code>index * 5.0</code>, i.e. an int multiplied by a double. Even <a href="https://youtu.be/-eCwBwTbjAI?si=rQ5tHNcBRkmaFV8-&amp;t=1000" target="_blank" rel="noopener">toy compilers</a> handle equivalent code quickly, thanks to a context-free type system.</p><p>Both examples are slow because they’re invalid swift and the type checker falls out of the fast path in order to confirm all possible type combinations are invalid. You might think it’s ok for invalid code to take a long time to compile. For me, 42 seconds to produce an “I give up” message is unacceptable. However, there are valid lines of swift that take a long time to compile too. Send me your slow lines (found using <code>-Xfrontend -debug-time-function-bodies</code>) and I’ll add it to this post.</p><p>Swift has come a long way from version 1, but on its 10th birthday it can still be slow. Unfortunately this can’t be completely fixed by optimizing the current approach. It requires a different approach.</p><p>Here’s what I’d do:</p><ol><li>Add a flag to <code>swiftc</code> that makes it infer types using only an expression’s child AST nodes while ignoring the parent AST node. The flag would also disable the <code>ExpressibleBy</code> protocols, which by definition get their type from their context.</li><li>Make a feature that adds type annotations, casts, and enum names to existing code where necessary to compile with the new type checker</li><li>Update all sample code to compile with the flag</li></ol><p>This might be a reasonable stopping point: teams that care about compile times and good error messages could use the flag, and everyone else doesn’t have to. It could go further though:</p><ol start="4"><li>Enable the flag by default for new Xcode projects</li><li>Deprecate the old type inference approach</li></ol><p>With this new approach, you’d have to add type annotations in some places. I’m ok with that. As a result, we’d get faster compilation times and clearer error messages, but the extra verbosity might be too much for the swift community to swallow.</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[The GJK Algorithm: A weird and beautiful way to do a simple thing (554 pts)]]></title>
            <link>https://computerwebsite.net/writing/gjk</link>
            <guid>40660761</guid>
            <pubDate>Wed, 12 Jun 2024 17:35:09 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://computerwebsite.net/writing/gjk">https://computerwebsite.net/writing/gjk</a>, See on <a href="https://news.ycombinator.com/item?id=40660761">Hacker News</a></p>
<div id="readability-page-1" class="page">
    

    
    <hr>
    <p>The GJK algorithm is a weird way to do a simple thing.
</p><p>We have shape A and shape B, and we'd like to determine if they overlap.
</p><p><img src="https://computerwebsite.net/writing/images/Pasted%20image%2020240609145754.png" width="500">
</p><p>A shape is a set of infinitely many points. If there exists any point that's a member of both sets, then the shapes overlap.
</p><p>Alternatively, if there exists point <em>a</em> in set A and a point <em>b</em> in set B such that:
</p><p><em>a</em> - <em>b</em> = <strong>0</strong>
</p><p>Then an intersection exists. Note that the <strong>0</strong> here represents a point itself: the origin.
</p><p>To see what this means intuitively, we can take a shift in perspective. Instead of dealing with specific points, let's try subtracting <em>every</em> point within one shape from <em>every</em> point within another, and plotting where they all end up:
</p><p><img src="https://computerwebsite.net/writing/images/Pasted%20image%2020240604163159.png" width="500">
</p><p>We've ended up with a new set that represents the "difference" between A and B. Since it contains the origin, we know that there must be at least one pair of points whose difference is <strong>0</strong>.
</p><p>That's exactly what the GJK algorithm takes advantage of. Instead of directly checking if the sets have an intersection, we subtract them and see if the new set contains the origin.
</p><p><img src="https://computerwebsite.net/writing/images/Pasted%20image%2020240604163335.png" width="500">
</p><p>Combining sets like this is known as taking the Minkowski difference. You may see this denoted '⊖' to separate it from the normal notion of a difference.
</p><p>More formally, it's defined as:
</p><p>A ⊖ B = {a - b | a ∊ A, b ∊ B}
</p><p>At first, it doesn't seem like we've made any progress, only rephrased the problem. How are we supposed to subtract infinitely many points? Well, let's find the bare minimum that we <em>need to know</em> about the set A ⊖ B to see if it contains the origin.
</p><p>In 2 dimensions, if a convex set contains the origin, it means we're able to draw a triangle between 3 points on the boundary, which contains the origin. This makes the problem even easier. If we know that A ⊖ B is convex, we only need to deal with the boundary.
</p><p><img src="https://computerwebsite.net/writing/images/Pasted%20image%2020240604164614.png" width="500">
</p><p>Luckily, it's easy to ensure A ⊖ B will be convex.
</p><p>The Minkowski difference of any two convex sets is also convex. So, we just need to break down A and B into convex components, which we can work with individually. On a computer, most shapes are already constructed out of convex components, so this process is often extremely easy.
</p><p><img src="https://computerwebsite.net/writing/images/Pasted%20image%2020240604182725.png" width="500">
</p><p>By the way, even though I'm explaining in 2D, our problem is fundamentally the same in any dimension. Here, we're looking for a bounding triangle on A ⊖ B. But, more generally, we're looking for a "simplex" in our dimension. An n-dimensional simplex is just the simplest polytope that can enclose an n-dimensional region. For example, a 3D simplex is a tetrahedron, as it has the fewest vertices needed to bound a volume.
</p><p><img src="https://computerwebsite.net/writing/images/Pasted%20image%2020240604175314.png" width="500">
</p><p><em></em>With these tools, let's restate the problem.
</p><ul><li>We want to determine if any simplex within A ⊖ B will enclose the origin.
</li></ul><ul><li>We don't want to have to compute the entire boundary of A ⊖ B. We only need to find some 3 points on A and some 3 points on B that will map to the vertices of our bounding simplex.
</li></ul><ul><li>There's no one solution, as infinitely many bounding simplexes might exist. We just need to find any 6 points on A and B that work, or prove that we can't find any.
</li></ul><p>So, from now on, whenever I say "shape", I'm speaking only about the boundary of a given set. We've thrown away all the area, as it's no longer relevant to the problem.
</p><p><img src="https://computerwebsite.net/writing/images/Pasted%20image%2020240604192323.png" width="500">
</p><p>Things would be much easier if we had some system to identify points on our shape. The support function (denoted S here) does just that. It takes in a vector, and outputs the furthest point on the shape in that direction. On a convex shape, every boundary point is identifiable using the support function of some direction.
</p><p>In other words, if we rotate a vector <em>d</em> 360 degrees around the origin, S(<em>d</em>) will hit every point. So, finding N points on a shape corresponds to finding N directions.
</p><p><img src="https://computerwebsite.net/writing/images/Pasted%20image%2020240609121840.png" width="500">
</p><p>The support function is easy to formally define. It's just the point that has the highest dot product of <em>d</em>. 
</p><p><img src="https://computerwebsite.net/writing/images/Pasted%20image%2020240609125706.png" width="500">
</p><p>Remember how I previously said we'd need to find 3 points on A, and 3 points on B? Well, we really only need to find 3 points in total. That's because of the following useful property of the Minkowski difference:
</p><p>The support of direction <em>d</em> in A ⊖ B = the support of <em>d</em> in A minus the support of <em>-d</em> in B.
</p><p>Basically, if we sweep across the borders of A and B in opposite directions by rotating <em>d</em>, we'll hit every boundary point on A ⊖ B. This massively narrows down how many possible points we need to search, as our points on A and B will always be dependent on each other. I know the diagrams are getting a bit confusing now, so bear with me.
</p><p><img src="https://computerwebsite.net/writing/images/Pasted%20image%2020240603131831.png" width="500">
</p><p><img src="https://computerwebsite.net/writing/images/Pasted%20image%2020240610135637.png" width="500">
</p><p><img src="https://computerwebsite.net/writing/images/Pasted%20image%2020240609122623.png" width="500">
</p><p>Put differently, most combinations of points on A and B don't matter, as they won't land us on the border of A ⊖ B. We only need to consider the case where they're in opposite directions. (There's some easy intuition for why this is the case: I'll leave that as an exercise)
</p><p><img src="https://computerwebsite.net/writing/images/Pasted%20image%2020240609123441.png" width="500">
</p><p>Alright, the stage has been set for GJK. The goal of the GJK algorithm is to find a simplex on A ⊖ B that contains the origin (or show that none exists), while doing as few operations as possible.
</p><p>Let's appreciate how incredible such an algorithm would be: given <em>any</em> convex shape, as long as it has some defined notion of support points, we can detect collisions. That's unbelievably powerful. 
</p><p>Here's how it works:
</p><p>1) First, find a point on A ⊖ B using a random direction <em>d</em>, which we'll call <em>p</em>. This is just S(<em>d</em>) - S(-<em>d</em>). It's the first point on our simplex.
</p><p><img src="https://computerwebsite.net/writing/images/Pasted%20image%2020240603135933.png" width="500">
</p><p>2) Then, let's take the dot product of <em>d</em> and <em>p</em>. If it's positive, we keep going on with the algorithm. If it's negative, that means <em>d</em> and <em>p</em> point in opposite directions. In that case, we terminate the algorithm, as there's no way for B and A to overlap.
</p><p>But why?
</p><p><img src="https://computerwebsite.net/writing/images/Pasted%20image%2020240603151836.png" width="500">
</p><p>It's more clear when we rotate our view to be in line with <em>d</em>. Point <em>a</em> is S(<em>d</em>), and point <em>b</em> is S(-<em>d</em>). Now, point <em>a</em> is the "top" of shape A, and point <em>b</em> is the "bottom" of B. Remember that <em>p</em> = <em>a - b</em>. So, if we project <em>p</em> onto <em>d</em> by taking the dot product, and it's negative, a gap exists between A and B.
</p><p><img src="https://computerwebsite.net/writing/images/Pasted%20image%2020240609131123.png" width="500">
</p><p>3) Ok, great. We've got one point of our simplex. Now, we need another. From our point <em>p</em>, let's shoot in the direction of the origin. Take the support of that vector <em>-d</em>, and run our check from step 2 on it.
</p><p>If you think about it, we're just finding the point most in the direction of the origin from <em>p</em>. If our shape encloses the origin, then our new point will have to be on the opposite side of it.
</p><p><img src="https://computerwebsite.net/writing/images/Pasted%20image%2020240610080829.png" width="500">
</p><p>Remember how we just performed the check from step 2? Well, there's another way to think about it. We're just checking to see if we've crossed the origin. If the origin is within our shape, we'll have to cross it when we take support points in opposite directions.
</p><p><img src="https://computerwebsite.net/writing/images/Pasted%20image%2020240609152836.png" width="500">
</p><p>4) Now, we're ready to complete the simplex. Let's take the vector perpendicular to our first 2 points in the direction of the origin. The support of that vector is the 3rd point of our simplex.
</p><p><img src="https://computerwebsite.net/writing/images/Pasted%20image%2020240609133818.png" width="500">
</p><p>I mean, think about it. We've effectively divided the space in half with our first line. We know the origin can't be anywhere on one side of our line, so we continue our search in the opposite direction. That's the efficiency of GJK: we'll keep dividing the space in half, until we find the point.
</p><p>Oh, and as usual, we can run our check from step 2 to see if we've proven that the shapes do not overlap.
</p><p><img src="https://computerwebsite.net/writing/images/Pasted%20image%2020240609134721.png" width="500">
</p><p>5) Now, let's see if our simplex contains the origin. If it does, we return true. Otherwise, we continue on.
</p><p>To find if the origin is within our triangle, first, let's break down the area surrounding it into the three infinite regions depicted below.
</p><p>The overlapping red, green, and blue regions each divide the space in half: an inside region, and an outside region.
</p><p>We've already eliminated the possibility of the origin being in the red region. So, we just have to check whether it's outside either of the remaining 2 lines.
</p><p><img src="https://computerwebsite.net/writing/images/Pasted%20image%2020240610140613.png" width="500">
</p><p>Luckily, we already know how to do this. Just take the dot product perpendicular to a given line to see if the origin is beyond it. If so, we know it must be in the white region.
</p><p><img src="https://computerwebsite.net/writing/images/Pasted%20image%2020240610140732.png" width="500">
</p><p>6) Now, it's time to iterate. We repeat from step 4, this time from the line on our simplex closest to the origin. We update our simplex using this new support point, and check if the new simplex contains the origin, as we did in step 5. This time, it does! Our algorithm returns <em>true</em>. If it doesn't, we keep doing this iterative process until it does, or one of our checks fails.
</p><p><img src="https://computerwebsite.net/writing/images/Pasted%20image%2020240609142721.png" width="500">
</p><p><img src="https://computerwebsite.net/writing/images/Pasted%20image%2020240609143010.png" width="500">
</p><p>And, that's it. That's the essence of the GJK algorithm. 
</p><p>I've glossed over a few implementation details, but you now have all the intuition you need to develop an in-depth understanding.
</p><p>Personally, I find this algorithm pretty because it's such a tidy example of what makes mathematics so powerful. Through a bunch of subtle shifts in perspective, a complicated problem becomes obvious.
</p><p>There's probably a few things incorrect with what I said. So, take it with the appropriate amount of salt for a high school sophomore's explanation of anything math related.</p>


</div>]]></description>
        </item>
        <item>
            <title><![CDATA[Diffractive Chocolate (213 pts)]]></title>
            <link>https://wp.optics.arizona.edu/oscoutreach/diffractive-chocolate/</link>
            <guid>40660689</guid>
            <pubDate>Wed, 12 Jun 2024 17:27:33 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://wp.optics.arizona.edu/oscoutreach/diffractive-chocolate/">https://wp.optics.arizona.edu/oscoutreach/diffractive-chocolate/</a>, See on <a href="https://news.ycombinator.com/item?id=40660689">Hacker News</a></p>
<div id="readability-page-1" class="page"><section id="wrap" role="main">
		
		
	<article id="post-1360">
	
				
		
		
				
		<div>
			
<p><strong>Overview</strong>: Take your science skills into the kitchen. Turn ordinary chocolate into an edible optics demo that shows how diffraction works.</p>



<p><strong>Supplies:</strong>&nbsp;Stove, double boiler, heat proof spatula, candy thermometer, diffraction sheets, silicon molds (optional), chocolate bars or melting chocolate</p>



<p><strong>Objectives:</strong> Diffraction is the bending of light. We do not need to look through a diffraction sheet to see the bending. Light can bounce off of fine grooved surfaces to show us how each wavelength of light refracts off of the surface.</p>



<hr>



<p><strong>Setup:</strong></p>



<ul>
<li>If using molds, cut the diffraction sheets into pieces the size of the bottom of the molds. Place the diffraction pieces, diffraction side up, in the bottom of the mold</li>



<li>Divide your chocolate into thirds, 2/3 are for melting, 1/3 is for the tempering</li>



<li>Different chocolates have different tempering temperature, these temperatures are critical for proper tempering</li>
</ul>


<div>
<figure><img fetchpriority="high" decoding="async" width="683" height="1024" src="https://wp.optics.arizona.edu/oscoutreach/wp-content/uploads/sites/75/2024/02/Chocolate-Melting-Points-683x1024.jpg" alt="" srcset="https://wp.optics.arizona.edu/oscoutreach/wp-content/uploads/sites/75/2024/02/Chocolate-Melting-Points-683x1024.jpg 683w, https://wp.optics.arizona.edu/oscoutreach/wp-content/uploads/sites/75/2024/02/Chocolate-Melting-Points-200x300.jpg 200w, https://wp.optics.arizona.edu/oscoutreach/wp-content/uploads/sites/75/2024/02/Chocolate-Melting-Points-768x1152.jpg 768w, https://wp.optics.arizona.edu/oscoutreach/wp-content/uploads/sites/75/2024/02/Chocolate-Melting-Points-1024x1536.jpg 1024w, https://wp.optics.arizona.edu/oscoutreach/wp-content/uploads/sites/75/2024/02/Chocolate-Melting-Points.jpg 1200w" sizes="(max-width: 683px) 100vw, 683px"></figure></div>






<ul>
<li>Follow this <a href="https://www.youtube.com/watch?v=SoTi0tM4yQ8">video</a> (starting at minute 20 for chocolate) on how to temper chocolate and create the diffraction pieces</li>



<li>If using molds, chocolate can be poured into molds quite easily</li>
</ul>



<p><strong>How to run the demo:</strong> </p>



<ul>
<li>Pass out the chocolate and explain what they see.</li>
</ul>



<div>
<figure><img decoding="async" width="266" height="472" src="https://wp.optics.arizona.edu/oscoutreach/wp-content/uploads/sites/75/2024/02/diffraction-chocolate.gif" alt=""></figure></div>



<p><strong>Try this:</strong></p>



<p>Follow the instructions in the <a href="https://www.youtube.com/watch?v=SoTi0tM4yQ8">video</a> (starting at minute 7:40) to make diffractive candy</p>







<hr>



<p><strong>What’s Happening?</strong></p>



<p>White light can be separated into all seven major colors of the complete spectrum or rainbow by using a diffraction grating. The grating separates light into colors as the light passes through the many fine slits of the grating. Each color travels at a different speed and therefore has a different angle of refraction when it hits the grating. Chocolate makes a reflection gratings. Along with chocolate, a compact disc also makes a good reflection grating. When light passes through a grating it is called a transmission grating. A transmission grating is what is used to make the diffractive chocolate. Diffractive candy is a transmission grating.</p>



<hr>



<p><strong>Learn more:</strong>&nbsp;(external links)</p>
			<!-- <rdf:RDF xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
			xmlns:dc="http://purl.org/dc/elements/1.1/"
			xmlns:trackback="http://madskills.com/public/xml/rss/module/trackback/">
		<rdf:Description rdf:about="https://wp.optics.arizona.edu/oscoutreach/diffractive-chocolate/"
    dc:identifier="https://wp.optics.arizona.edu/oscoutreach/diffractive-chocolate/"
    dc:title="Diffractive Chocolate"
    trackback:ping="https://wp.optics.arizona.edu/oscoutreach/diffractive-chocolate/trackback/" />
</rdf:RDF> -->
						
		</div>
		
		

	</article>
			
		

		
		</section></div>]]></description>
        </item>
        <item>
            <title><![CDATA[HNInternal: Ask HN: Are you still using your Vision Pro? (148 pts)]]></title>
            <link>https://news.ycombinator.com/item?id=40660270</link>
            <guid>40660270</guid>
            <pubDate>Wed, 12 Jun 2024 16:54:21 GMT</pubDate>
            <description><![CDATA[<p>See on <a href="https://news.ycombinator.com/item?id=40660270">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
            <tbody><tr id="40663619"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_40663619" href="https://news.ycombinator.com/vote?id=40663619&amp;how=up&amp;goto=item%3Fid%3D40660270"></a></center>    </td><td><br><div><p>Yes, almost every day. I travel regularly and I love having a big, crisp, private display every where I am: Hotel, plane, train. Makes me so much more productive.</p><p>Consuming content is great of course but the AVP has changed my content creation: I take way more panoramas and now spatial photos (Spatialify on iOS works well). I also bought an Insta X4 360 camera which, while a far cry from Apple's immersive content in resolution, can still be a really nice way to relive memories.</p><p>More content: Last year I started 3d scanning (using Scaniverse) sculptures and other art / items that catch my eye during my travels. The AVP makes it really easy to import and place them in my environment. When I'm working I'll often place a favorite sculpture next to me for company &amp; as a reminder of a trip I took.</p><p>Finally, even after 4 months of use, it's still really fun and, from a tech perspective, astounding in terms of image quality, stability, 3D placement, integration in environment, etc. I love it and I can't wait for this tech to get better and better.</p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="40661602"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_40661602" href="https://news.ycombinator.com/vote?id=40661602&amp;how=up&amp;goto=item%3Fid%3D40660270"></a></center>    </td><td><br><div><p>Returned in within the first month. Couldn’t use it for work because of the shared Apple ID requirements. Hand tracking was too laggy for games. That left movies/tv as the only winning feature, and I prefer to watch socially. Was pretty bummed tbh, I thought it would be much cooler.</p><p>Edit: there was something really cool actually, that I think doesn’t get talked about enough. Pooping on Yosemite. Peak futurism.</p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="40661643"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_40661643" href="https://news.ycombinator.com/vote?id=40661643&amp;how=up&amp;goto=item%3Fid%3D40660270"></a></center>    </td><td><br><div>
                  <p>Hand tracking is limited to 30fps in VisionOS 1.0, but in 2.0 it runs at the same framerate as the display, so it should be vastly improved.</p>
              </div></td></tr>
        </tbody></table></td></tr>
                <tr id="40662685"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_40662685" href="https://news.ycombinator.com/vote?id=40662685&amp;how=up&amp;goto=item%3Fid%3D40660270"></a></center>    </td><td><br><div><p>30fps is 33ms per frame. I'd expect that to be fine for most free-space gestures (for musical instruments you generally want &lt;10ms, but that's most noticeable with discrete impulsive events like hitting a drum pad, and even then is pretty manageable).</p><p>Is the frame rate really the limiting factor here, or something algorithmic in the tracking (like smoothing out noise)?</p></div></td></tr>
        </tbody></table></td></tr>
                        <tr id="40661562"><td></td></tr>
                <tr id="40662492"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_40662492" href="https://news.ycombinator.com/vote?id=40662492&amp;how=up&amp;goto=item%3Fid%3D40660270"></a></center>    </td><td><br><div><p>&gt;Also when crankin’ my hog (just being honest)</p><p>Honestly that seems like the primary use case whenever the topic is mentioned. I'm sure there are cool innovative uses for them, but porn is always going to be at the forefront.</p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="40663302"><td></td></tr>
                  <tr id="40661716"><td></td></tr>
                <tr id="40661881"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_40661881" href="https://news.ycombinator.com/vote?id=40661881&amp;how=up&amp;goto=item%3Fid%3D40660270"></a></center>    </td><td><br><div>
                  <p>Neither a Vision Pro owner nor much of a hog cranker, so this comment may be something of a 'premature ejaculation', but I don't think there's any prospect of native applications designed to aid manipulating oneself to issue passing App Review.</p>
              </div></td></tr>
        </tbody></table></td></tr>
                <tr id="40662401"><td><table>  <tbody><tr>    <td indent="3"><img src="https://news.ycombinator.com/s.gif" height="1" width="120"></td><td>
      <center><a id="up_40662401" href="https://news.ycombinator.com/vote?id=40662401&amp;how=up&amp;goto=item%3Fid%3D40660270"></a></center>    </td><td><br><div><p>I own a AVP, hog cranking is its killer use case. Porn on the AVP exists today whether Apple likes it or not.</p><p>Passing app review doesnt matter, there are a number of subscription websites (sexlikereal, wankz, czechvr etc) which offer UHD uncompressed 8K MP4 videos for download. It is entirely possible (and quite easy) to download these UHD videos to a Mac, mount the folder containing the goon material on the Mac as a drive on the Vision Pro (local network drive), and stream the video to the AVP using a 3rd party 3d video player; Moonplayer is the pick of the bunch at the moment.</p><p>I must say the experience is pretty damn good. I can see people getting addicted to it an unhealthy way. The sites mentioned above are only producing 8K at the moment, I think the AVP can handle more pixels, and there are new cameras coming on to the market (<a href="https://x.com/Blackmagic_News/status/1800273164867658228" rel="nofollow">https://x.com/Blackmagic_News/status/1800273164867658228</a>) which will really crank things up a notch.</p><p>There is a tremedous market oppurtunity available here, its niche at the moment, but once you experience good quality VR porn its hard to go back to the flat stuff.</p><p>I should probably get a GF.... sigh.</p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="40662854"><td><table>  <tbody><tr>    <td indent="4"><img src="https://news.ycombinator.com/s.gif" height="1" width="160"></td><td>
      <center><a id="up_40662854" href="https://news.ycombinator.com/vote?id=40662854&amp;how=up&amp;goto=item%3Fid%3D40660270"></a></center>    </td><td><br><div>
                  <p>Truly a golden era for fast-forwarding through videos of dead-eyed men and women rutting on camera. (Thank you – I genuinely learned a lot from your reply.)</p>
              </div></td></tr>
        </tbody></table></td></tr>
                <tr id="40663371"><td><table>  <tbody><tr>    <td indent="5"><img src="https://news.ycombinator.com/s.gif" height="1" width="200"></td><td>
      <center><a id="up_40663371" href="https://news.ycombinator.com/vote?id=40663371&amp;how=up&amp;goto=item%3Fid%3D40660270"></a></center>    </td><td><br><div><p>What a boner of a comment. We get it, George, you don't watch pornography. Well done on managing your personal brand I guess?</p><p>FYI there's nothing inherently wrong about porn or sex work! It's great! A lot of people like it, you should try it sometime!</p></div></td></tr>
        </tbody></table></td></tr>
                        <tr id="40663595"><td></td></tr>
                  <tr id="40661970"><td></td></tr>
                  <tr id="40663552"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_40663552" href="https://news.ycombinator.com/vote?id=40663552&amp;how=up&amp;goto=item%3Fid%3D40660270"></a></center>    </td><td><br><div>
                  <p>Curious question, what would you think if they told you some sessions are uploaded to Apple for them to evaluate the use of the product and your security?</p>
              </div></td></tr>
        </tbody></table></td></tr>
                  <tr id="40663667"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_40663667" href="https://news.ycombinator.com/vote?id=40663667&amp;how=up&amp;goto=item%3Fid%3D40660270"></a></center>    </td><td><br><div>
                  <p>If you could watch NBA games from court side seat in VR, they would be flying off the shelves. I'm fairly sure nothing technical is preventing this from happening. Most likely it is blocked by existing media contracts. I.e "non-technical" reasons.</p>
              </div></td></tr>
        </tbody></table></td></tr>
                <tr id="40663840"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_40663840" href="https://news.ycombinator.com/vote?id=40663840&amp;how=up&amp;goto=item%3Fid%3D40660270"></a></center>    </td><td><br><div>
                  <p>How would this even work? The only plausible way I could think it that there's a 360 degree camera mounted court side and you can control the view by turning your head. But it would be entirely non-immersive because moving your upper body in any walk would immediately break the illusion. If the idea is just a court side camera I don't see what benefit the big headset is adding?</p>
              </div></td></tr>
        </tbody></table></td></tr>
                  <tr id="40662595"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_40662595" href="https://news.ycombinator.com/vote?id=40662595&amp;how=up&amp;goto=item%3Fid%3D40660270"></a></center>    </td><td><br><div>
                  <p>Until it gets multiple account support, it’s not even an option. I could buy one, but I’m not going to buy three. Nor can I use it for work, for the same reason.</p>
              </div></td></tr>
        </tbody></table></td></tr>
            <tr id="40661563"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_40661563" href="https://news.ycombinator.com/vote?id=40661563&amp;how=up&amp;goto=item%3Fid%3D40660270"></a></center>    </td><td><br><div><p>I still use mine every couple of weeks for movies that my family doesn’t want to watch with me.</p><p>I also use it as my hotel setup when I travel for work. It’s great having a full size monitor wherever I need. I’m excited for the coming improvements with vision os 2.0.</p><p>I’m still not comfortable using it in public, though. It feels ostentatious, but I will try it the next time I fly with the family. Having people I trust around me will make me more willing to go immersive while traveling.</p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="40660484"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_40660484" href="https://news.ycombinator.com/vote?id=40660484&amp;how=up&amp;goto=item%3Fid%3D40660270"></a></center>    </td><td><br><div><p>Yes, its very much a part of my work setup. It transformed working so that for the first time I have a good working setup everywhere.</p><p>Its also my preferred place to consume cinema. I have a short throw projector and sound system. I prefer the AVP. The image is so crisp and the 3D is so good, that its better than a decent home movie theater.</p><p>Its my preferred place to watch F1.</p><p>Environments genuinely soothe me.</p><p>Breathe works on this platform, it annoys me on the watch.</p><p>I would watch every sport and documentary in spatial if the was a thing. The tastes have me excited for the future.</p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="40660656"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_40660656" href="https://news.ycombinator.com/vote?id=40660656&amp;how=up&amp;goto=item%3Fid%3D40660270"></a></center>    </td><td><br><div>
                  <p>I'd love to hear more about how you use it for your work set-up. The other things you've mentioned all have me interested in getting one but I've never been able to imagine how I'd use it for work.</p>
              </div></td></tr>
        </tbody></table></td></tr>
                <tr id="40662996"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_40662996" href="https://news.ycombinator.com/vote?id=40662996&amp;how=up&amp;goto=item%3Fid%3D40660270"></a></center>    </td><td><br><div><p>&gt; I'd love to hear more about how you use it for your work set-up.</p><p>Also very interested</p></div></td></tr>
        </tbody></table></td></tr>
                  <tr id="40661597"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_40661597" href="https://news.ycombinator.com/vote?id=40661597&amp;how=up&amp;goto=item%3Fid%3D40660270"></a></center>    </td><td><br><div><p>With F1 TV does that mean you can just place different streams and screens anywhere? That does sound quite neat in principle.</p><p>Hell if you had a feed for car positions one could make a virtual model of the track and watch it top down.</p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="40662815"><td></td></tr>
                <tr id="40663216"><td></td></tr>
                              <tr id="40661028"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_40661028" href="https://news.ycombinator.com/vote?id=40661028&amp;how=up&amp;goto=item%3Fid%3D40660270"></a></center>    </td><td><br><div>
                  <p>No, I returned it. Kinda regretted it, so I picked up a Meta Quest 3 to scratch the VR itch. I use it for watching youtube while I do dishes and laundry. Sometimes I play golf for relaxation or beat saber. I really wish there was an easy way to put the older 3d blu-ray movies on the device to watch movies, because immersive movies are really the next level for entertainment.</p>
              </div></td></tr>
        </tbody></table></td></tr>
                <tr id="40663665"><td></td></tr>
                <tr id="40663720"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_40663720" href="https://news.ycombinator.com/vote?id=40663720&amp;how=up&amp;goto=item%3Fid%3D40660270"></a></center>    </td><td><br><div>
                  <p>I like augmented reality stuff, 3d games and immersive videos. I can get all that stuff done on the Quest 3 for a 1/10th of the cost. It's good enough and has a larger ecosystem for games. And strangely, I think controllers are better in a lot of cases than hand tracking.</p>
              </div></td></tr>
        </tbody></table></td></tr>
                  <tr id="40661619"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_40661619" href="https://news.ycombinator.com/vote?id=40661619&amp;how=up&amp;goto=item%3Fid%3D40660270"></a></center>    </td><td><br><div><p>&gt;  I use it for watching youtube while I do dishes and laundry.</p><p>How do you interact with it, ooi? Hand-tracking, voice, ... ? (Can't really touch the headset or controller with wet and/or busy hands).</p><p>&gt;  I really wish there was an easy way to put the older 3d blu-ray movies on the device to watch movies,</p><p>Oh there is, it just involves the high seas route. Also most movie-players on these devices suck a bit, i.e. they are either "too immersive" and make it difficult to use pass-through, or too useless like the builtin one.</p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="40662079"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_40662079" href="https://news.ycombinator.com/vote?id=40662079&amp;how=up&amp;goto=item%3Fid%3D40660270"></a></center>    </td><td><br><div><p>Skybox VR is great for watching movies on The Quest line. 2D or 3D.</p><p>There are actually some tools that exist to convert blu-rays to SBS 3D if you’re looking to go legit.</p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="40663681"><td></td></tr>
                  <tr id="40662161"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_40662161" href="https://news.ycombinator.com/vote?id=40662161&amp;how=up&amp;goto=item%3Fid%3D40660270"></a></center>    </td><td><br><div>
                  <p>You can use your hands just fine. You just grab a screen on the edge and move it to where you want it to be. No need to touch the device or a controller.</p>
              </div></td></tr>
        </tbody></table></td></tr>
            <tr id="40661696"><td></td></tr>
                  <tr id="40662562"><td></td></tr>
                  <tr id="40660381"><td></td></tr>
                <tr id="40662600"><td></td></tr>
                <tr id="40662614"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_40662614" href="https://news.ycombinator.com/vote?id=40662614&amp;how=up&amp;goto=item%3Fid%3D40660270"></a></center>    </td><td><br><div>
                  <p>Well they don't sell them until 12th of July here so I'm going to go and play with one first.</p>
              </div></td></tr>
        </tbody></table></td></tr>
                        <tr id="40660918"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_40660918" href="https://news.ycombinator.com/vote?id=40660918&amp;how=up&amp;goto=item%3Fid%3D40660270"></a></center>    </td><td><br><div><p>Yep! A mix of work and play. When I'm going to be spending a LOT of time coding I still prefer my ultra-wide physical monitor (probably my own fault for preferring such a small font size), but for things like handling tickets, emails, quick code changes in terminal, etc. it is pretty great. I ended up using a third-party head strap [0] to greatly improve comfort, but I know other folks use the stock strap for long sessions with no issues. YMMV :)</p><p>[0] <a href="https://amzn.to/4ehZpnL" rel="nofollow">https://amzn.to/4ehZpnL</a></p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="40661644"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_40661644" href="https://news.ycombinator.com/vote?id=40661644&amp;how=up&amp;goto=item%3Fid%3D40660270"></a></center>    </td><td><br><div><p>Haven't touched it in months. Thought about bringing it on a recent international flight, but we had toddler in tow and I didn't want to lug it around for a couple of weeks.</p><p>Excited about the updates to the OS, my goal was always to use it for work, as an alternative posture mode, but couldn't get used to it at first.</p><p>Are there any other killer apps other than movies these days?</p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="40663002"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_40663002" href="https://news.ycombinator.com/vote?id=40663002&amp;how=up&amp;goto=item%3Fid%3D40660270"></a></center>    </td><td><br><div><p>&gt; we had toddler in tow and I didn't want to lug it around for a couple of weeks.</p><p>Weird. I really enjoyed our toddlers</p></div></td></tr>
        </tbody></table></td></tr>
                  <tr id="40661549"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_40661549" href="https://news.ycombinator.com/vote?id=40661549&amp;how=up&amp;goto=item%3Fid%3D40660270"></a></center>    </td><td><br><div>
                  <p>I hadnt been for a month or so (picked it up at launch), but the new beta release of Vision OS 2 looks feels like a massive quality of life improvement. Foveation and implied resolution seem to be massively improved, framerates are much higher too. The new Bora Bora (day + night) environment is fabulous and has moments of "am I actually there", the prior exisiting environments have all had a decent bump in apparent quality too. Apple are slow rolling content and experiences, but I can see this working out in the long run.
P.S. if anyone is still hung up on comfort, the "open face" headstraps do wonders, very much like wearing glasses.</p>
              </div></td></tr>
        </tbody></table></td></tr>
                <tr id="40661594"><td></td></tr>
                <tr id="40661732"><td></td></tr>
                        <tr id="40660588"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_40660588" href="https://news.ycombinator.com/vote?id=40660588&amp;how=up&amp;goto=item%3Fid%3D40660270"></a></center>    </td><td><br><div>
                  <p>Yes - I do use it for watching films that I want to be more engaged in, usually films through Criterion. I find myself spending more time outdoors now that it's summer but during the winter I'm on it much more. I love the environments and do wish there were more to choose from, plus environments on more streaming platforms. The Disney+ ones are very well made.</p>
              </div></td></tr>
        </tbody></table></td></tr>
            <tr id="40662904"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_40662904" href="https://news.ycombinator.com/vote?id=40662904&amp;how=up&amp;goto=item%3Fid%3D40660270"></a></center>    </td><td><br><div>
                  <p>Yes, for movies and TV, and as a virtual monitor occasionally. I've travelled with it quite a bit as well, and it's great on long international flights.</p>
              </div></td></tr>
        </tbody></table></td></tr>
            <tr id="40661698"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_40661698" href="https://news.ycombinator.com/vote?id=40661698&amp;how=up&amp;goto=item%3Fid%3D40660270"></a></center>    </td><td><br><div><p>No, I found it uncomfortable for extended periods of wear - to the point where I would rather just use a normal display on my Mac etc. I look forward to the second-generation with comfort, weight, &amp; FOV improvements (hopefully!).</p><p>It also still hasn't found the "killer apps" just yet, but it's clear Apple is still heavily invested into this considering there's nearly 30 sessions on VisionOS at WWDC this week.</p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="40663678"><td></td></tr>
                  <tr id="40660421"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_40660421" href="https://news.ycombinator.com/vote?id=40660421&amp;how=up&amp;goto=item%3Fid%3D40660270"></a></center>    </td><td><br><div><p>Not the Vision Pro, but I got the latest Facebook device around Christmas time, and it got almost entirely shelved before the end of January. I maybe get it out, for a workout, once a month now.</p><p>Some of the games were really fun, most notably Walkabout Minigolf and Super Hot VR.</p><p>Some of the exercise programs were pretty neat, most notably The Thrill of the Fight and Les Mills Body combat.</p><p>It did not work well as a replacement for either a TV or a computer monitor. The device was just too bulky and inconvenient and the software too clunky. So much easier to just use a laptop, if I want to work / watch on the go.</p><p>In the end, none of the experiences were compelling enough to keep using it regularly.</p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="40661806"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_40661806" href="https://news.ycombinator.com/vote?id=40661806&amp;how=up&amp;goto=item%3Fid%3D40660270"></a></center>    </td><td><br><div><p>&gt; Are you still using your Vision Pro?</p><p>&gt; [150 words about a totally different product and platform]</p><p>Vision Pro isn't something I would use regularly, but you're bringing opinions about a 14" CRT monitor to a thread soliciting opinions on a specific 30" 1080P TV. I think we are beyond the stage where useful generalizations about "the state of AR/VR" can be drawn from exposure to a single device.</p><p>The disparity in screen quality and OS sophistication between Oculus 3 and Vision Pro is enormous (and both platforms are self-evidently in their infancy).</p><p>Whether you think they have succeeded or not, and whether you think the price point is reasonable or not, Vision Pro is as different to Quest 3 as a BlackBerry Bold 9700 was to a Nokia 7650.</p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="40662362"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_40662362" href="https://news.ycombinator.com/vote?id=40662362&amp;how=up&amp;goto=item%3Fid%3D40660270"></a></center>    </td><td><br><div><p>No it's not. When tossing up a vr purchase it's Vision pro, quest 3 or big screen beyond. Price points all vary but they are literally all the same shiz just served on a different shovel.</p><p>Each have their pros, each have their cons (well the mvp has mostly cons being the worst of the 3 but hey its having a crack).</p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="40662737"><td><table>  <tbody><tr>    <td indent="3"><img src="https://news.ycombinator.com/s.gif" height="1" width="120"></td><td>
      <center><a id="up_40662737" href="https://news.ycombinator.com/vote?id=40662737&amp;how=up&amp;goto=item%3Fid%3D40660270"></a></center>    </td><td><br><div><p>&gt; they are literally all the same shiz</p><p>Oculus Quest 3 screens: LCD displays with a per-eye resolution of 2064×2208p (4.56 million pixels per eye)</p><p>Apple Vision Pro screens: micro-OLED displays with a per-eye resolution of 3,680x3,140 (11.5 million pixels per eye)</p><p>Disproof by counterexample. Perhaps you could refine your theory?</p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="40663762"><td></td></tr>
                  <tr id="40662948"><td><table>  <tbody><tr>    <td indent="3"><img src="https://news.ycombinator.com/s.gif" height="1" width="120"></td><td>
      <center><a id="up_40662948" href="https://news.ycombinator.com/vote?id=40662948&amp;how=up&amp;goto=item%3Fid%3D40660270"></a></center>    </td><td><br><div><p>Putting aside the enormous hardware difference between the two, even if they were "the same shiz" spec-wise, Id still not comment on Vision Pro over Quest - the reason being I have Macook Air. Spec-wise, that laptop is almost identical to any other laptop, but the level of refinement is on another planet. Its tousands little things that make using Air a joy, while dealing with my work HP Zbook is a pain in every way.</p><p>For that same reason, I dont dare to compare Vision to any other VR (and I tried a few, not Vision Pro tho).</p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="40663351"><td><table>  <tbody><tr>    <td indent="4"><img src="https://news.ycombinator.com/s.gif" height="1" width="160"></td><td>
      <center><a id="up_40663351" href="https://news.ycombinator.com/vote?id=40663351&amp;how=up&amp;goto=item%3Fid%3D40660270"></a></center>    </td><td><br><div><p>&gt; Spec-wise, that laptop is almost identical to any other laptop</p><p>Post-M series I hear this from time to time and always ask people to show me something in the same weight class with equivalent battery life, performance, and screen quality.</p><p>Has the market finally caught up to the point where your statement is true? (Not asking you to research, just curious if any spring to mind from any pre-purchase research you did.)</p><p>&gt; Putting aside the enormous hardware difference between the two</p><p>I think this is far too charitable.</p><p>1. We are a largely technical audience.</p><p>2. We are discussing a product category where, per the last ten years of discussion about early hardware drawbacks (and the critical consensus on Vision Pro), the screen inescapably defines the experience.</p><p>Anyone on HN describing Vision Pro's screen as "the same shiz" as Quest 3 must either be a troll or operating with a knowledge gap so vast as to make meaningful discussion very, very difficult.</p><p>Like, if you don't understand the math, read the reviews and trust that this is not a global cabal of Apple apologists making shit up. Occam's Razor: this is a $3500 device where 35% of the BOM is the screens ($550-ish), compared to a $500 device where ~19% of the BOM is screens ($80). <i>Of course</i> they aren't in the same league.</p></div></td></tr>
        </tbody></table></td></tr>
                              <tr id="40660520"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_40660520" href="https://news.ycombinator.com/vote?id=40660520&amp;how=up&amp;goto=item%3Fid%3D40660270"></a></center>    </td><td><br><div><p>I still use my Oculus 2 a few times a week (I try for daily, but life doesn't allow it), but just Beat Saber and FitXR. It just replaces going to the the gym though if there is some problem with doing that.</p><p>I can't imagine using an AVP though, without controllers it really isn't suited to fitness.</p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="40660634"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_40660634" href="https://news.ycombinator.com/vote?id=40660634&amp;how=up&amp;goto=item%3Fid%3D40660270"></a></center>    </td><td><br><div><p>Beat Saber was worth the price of admission alone, at least when you could mod custom songs onto the headset itself. There were also websites that would generate a Beat Saber level for any YouTube video you gave it, which was great for playing along to brand-new releases.</p><p>It was really such a good game that I'm surprised we haven't seen more stuff like it. Of all the futuristic VR experiences I've tried (even HL: Alyx) Beat Saber was the only one that really felt effortlessly futuristic.</p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="40661611"><td><table>  <tbody><tr>    <td indent="3"><img src="https://news.ycombinator.com/s.gif" height="1" width="120"></td><td>
      <center><a id="up_40661611" href="https://news.ycombinator.com/vote?id=40661611&amp;how=up&amp;goto=item%3Fid%3D40660270"></a></center>    </td><td><br><div><p>BeatSaber would be so much better with custom music, so would FitXR. But I don't have time to figure out how to do that as I used to.</p><p>It is too bad Facebook doesn't lean more into BeatSaber and rhythm game/fitness experiences, they are simple, easy to sell, and are pretty satisfying. But I guess it really isn't good enough for their product, they really need metaverse to take off.</p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="40660981"><td><table>  <tbody><tr>    <td indent="3"><img src="https://news.ycombinator.com/s.gif" height="1" width="120"></td><td>
      <center><a id="up_40660981" href="https://news.ycombinator.com/vote?id=40660981&amp;how=up&amp;goto=item%3Fid%3D40660270"></a></center>    </td><td><br><div><p>When the Vive first came out, there was a neat VR FPS named Pavlov VR that was pretty fun.</p><p>It was neat to play an FPS where ducking for cover worked, reloading involved actually having to pull a magazine from your belt and jam it in, you could duck behind something and blind fire over it.</p><p>It mostly worked very well.  The annoyances were around how physically exhausting constantly ducking and weaving was (and sweating into the foam), and getting lost in the moment and nearly sprinting out of the "safety box" into a coffee table.</p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="40661622"><td><table>  <tbody><tr>    <td indent="4"><img src="https://news.ycombinator.com/s.gif" height="1" width="160"></td><td>
      <center><a id="up_40661622" href="https://news.ycombinator.com/vote?id=40661622&amp;how=up&amp;goto=item%3Fid%3D40660270"></a></center>    </td><td><br><div><p>A silicon foam cover is a must have. Thankfully, they are included with all new oculus VR headsets these days.</p><p>BeatSaber is pretty stationary, so is FitXR. I've never tried a moving around the room/box VR experience (like Thrill of the fight).</p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="40662199"><td><table>  <tbody><tr>    <td indent="5"><img src="https://news.ycombinator.com/s.gif" height="1" width="200"></td><td>
      <center><a id="up_40662199" href="https://news.ycombinator.com/vote?id=40662199&amp;how=up&amp;goto=item%3Fid%3D40660270"></a></center>    </td><td><br><div><p>&gt; <i>A silicon foam cover is a must have.</i></p><p>Funny, I'm the opposite.</p><p>I was excited to try it because it seems so much more hygienic -- you can wipe it down and the foam won't degrade. But I quickly discovered that it got all clammy and sticky on my skin, and then humidity would build up and fog up the lenses. What! Kind of the same way swim goggles fog up.</p><p>Whereas the regular foam padding is... perfectly fine. No sweating, no fog, no humidity, nothing, because enough air seems to pass through and nothing is suffocating your skin.</p><p>And I'm not even a sweaty person or anything, not at all. And I'm just reclining watching movies, it's not even for movement. But the silicone layer over the foam just creates this airtight (enough) seal which is just bad all around.</p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="40662799"><td><table>  <tbody><tr>    <td indent="6"><img src="https://news.ycombinator.com/s.gif" height="1" width="240"></td><td>
      <center><a id="up_40662799" href="https://news.ycombinator.com/vote?id=40662799&amp;how=up&amp;goto=item%3Fid%3D40660270"></a></center>    </td><td><br><div>
                  <p>I tried one before the one that just came with the Oculus 2 and thought the same. I guess there is a lot of variation in silicon foam covers, but the standard one that comes with the headset works for me. I sweat a lot when I do VR, so without a cover, the foam head piece is going to get soaked and smelly.</p>
              </div></td></tr>
        </tbody></table></td></tr>
            <tr id="40662286"><td><table>  <tbody><tr>    <td indent="6"><img src="https://news.ycombinator.com/s.gif" height="1" width="240"></td><td>
      <center><a id="up_40662286" href="https://news.ycombinator.com/vote?id=40662286&amp;how=up&amp;goto=item%3Fid%3D40660270"></a></center>    </td><td><br><div><p>There are some anti-fog sprays people use while scuba diving, or some people apply a tiny bit of toothpaste.</p><p>I have no idea if those are safe to use on those lenses, but it might be worth a look.</p></div></td></tr>
        </tbody></table></td></tr>
                              <tr id="40661560"><td><table>  <tbody><tr>    <td indent="3"><img src="https://news.ycombinator.com/s.gif" height="1" width="120"></td><td>
      <center><a id="up_40661560" href="https://news.ycombinator.com/vote?id=40661560&amp;how=up&amp;goto=item%3Fid%3D40660270"></a></center>    </td><td><br><div>
                  <p>You can try moonrider.xyz in the oculus browser for like a punching one. It's web-based VR with all of the community songs included.</p>
              </div></td></tr>
        </tbody></table></td></tr>
                              <tr id="40661586"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_40661586" href="https://news.ycombinator.com/vote?id=40661586&amp;how=up&amp;goto=item%3Fid%3D40660270"></a></center>    </td><td><br><div><p>Yes, but I'll use it a lot more when 2.0 comes out, so I can see my keyboard in environments, which is my biggest complaint.</p><p>Mostly it's the best cinema screen I've ever viewed in my life. "Avatar 2", in 3D and at 48fps, is an absolutely stunning viewing experience. I wish high-framerate movies were more common. They look incredible.</p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="40660580"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_40660580" href="https://news.ycombinator.com/vote?id=40660580&amp;how=up&amp;goto=item%3Fid%3D40660270"></a></center>    </td><td><br><div><p>Yes. Still my favourite way to watch movies and F1 (via Vroom). I love working in environments, I am more calm and get distracted less.</p><p>One gripe I had was that I couldn't see the keyboard in an immersive environment, so I had to keep reorienting myself if I took my hands off of it. Now with visionOS 2 you can have the keyboard appear in an environment, so I'm excited to try that. The ability to have an ultra wide screen is a nice addition as well.</p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="40661613"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_40661613" href="https://news.ycombinator.com/vote?id=40661613&amp;how=up&amp;goto=item%3Fid%3D40660270"></a></center>    </td><td><br><div>
                  <p>Yes I do, to consume video content every other day on it. Sadly, I'm not the main owner of the device, because there is no multi user support, I cannot use it for day to day work (my partner is logged into it).</p>
              </div></td></tr>
        </tbody></table></td></tr>
            <tr id="40660924"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_40660924" href="https://news.ycombinator.com/vote?id=40660924&amp;how=up&amp;goto=item%3Fid%3D40660270"></a></center>    </td><td><br><div>
                  <p>Yes, still using it several times a week mostly for work via mac virtual display. If I need to work late at night, I find putting myself in a daylight immersive environment helps me stay awake and avoids high contrast difference between a laptop screen and a dark room.</p>
              </div></td></tr>
        </tbody></table></td></tr>
                <tr id="40662570"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_40662570" href="https://news.ycombinator.com/vote?id=40662570&amp;how=up&amp;goto=item%3Fid%3D40660270"></a></center>    </td><td><br><div><p>&gt;avoids high contrast difference between a laptop screen and a dark room.</p><p>So one of the big advantages is that it saves you from turning on the overhead light?</p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="40662916"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_40662916" href="https://news.ycombinator.com/vote?id=40662916&amp;how=up&amp;goto=item%3Fid%3D40660270"></a></center>    </td><td><br><div>
                  <p>Late at night, I work in a corner of the bedroom because my wife likes me nearby. This helps not disturb her and my toddler while they sleep.</p>
              </div></td></tr>
        </tbody></table></td></tr>
                        <tr id="40660425"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_40660425" href="https://news.ycombinator.com/vote?id=40660425&amp;how=up&amp;goto=item%3Fid%3D40660270"></a></center>    </td><td><br><div>
                  <p>I use it every morning with my keyboard to watch videos, get caught up on emails and messages, and sometimes call friends. I don’t use it quite as much in the evenings when my wife is around since I like to be able to show her what I’m doing, so I’ll use my laptop instead.</p>
              </div></td></tr>
        </tbody></table></td></tr>
                <tr id="40661169"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_40661169" href="https://news.ycombinator.com/vote?id=40661169&amp;how=up&amp;goto=item%3Fid%3D40660270"></a></center>    </td><td><br><div><p>Shared augmented reality is the killer feature that I'm still waiting for. I'm really surprised that Apple didn't have that from the start. We could have things like going through your photo collection together. Or collaborating on a virtual sculpture. Or discussing ideas in front of a virtual blackboard. There are a lot of really cool things you could do with shared augmented reality.</p><p>Maybe Apple scheduled this feature for a later release? This is something that can probably be done efficiently 100% in software.</p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="40661513"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_40661513" href="https://news.ycombinator.com/vote?id=40661513&amp;how=up&amp;goto=item%3Fid%3D40660270"></a></center>    </td><td><br><div><p>&gt; This is something that can probably be done efficiently 100% in software.</p><p>Doubtful. The data transfer needed between the two devices is not trivial.</p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="40661847"><td><table>  <tbody><tr>    <td indent="3"><img src="https://news.ycombinator.com/s.gif" height="1" width="120"></td><td>
      <center><a id="up_40661847" href="https://news.ycombinator.com/vote?id=40661847&amp;how=up&amp;goto=item%3Fid%3D40660270"></a></center>    </td><td><br><div>
                  <p>It shouldn't be much more complicated than what we currently have with first-person 3D shooters. All devices in an augmented room would have some existing shared data that doesn't need to be transferred. It's only the updates that need to be sent over the network.</p>
              </div></td></tr>
        </tbody></table></td></tr>
            <tr id="40661721"><td><table>  <tbody><tr>    <td indent="3"><img src="https://news.ycombinator.com/s.gif" height="1" width="120"></td><td>
      <center><a id="up_40661721" href="https://news.ycombinator.com/vote?id=40661721&amp;how=up&amp;goto=item%3Fid%3D40660270"></a></center>    </td><td><br><div><p>Shared photos is likely very doable, as it's just photo id, physical location, and scale/orientation. Place a picture above the fireplace and both headsets could render it exactly the same and see it.</p><p>Video would work well, audio might be a little harder to sync exactly correctly.</p><p>Apps are going to be really really hard.</p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="40661905"><td></td></tr>
                              <tr id="40662095"><td></td></tr>
            <tr id="40660429"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_40660429" href="https://news.ycombinator.com/vote?id=40660429&amp;how=up&amp;goto=item%3Fid%3D40660270"></a></center>    </td><td><br><div>
                  <p>Yes, big fan of using the native apps in conjunction with the MacBook mirroring makes it a great workspace. After work, it's fantastic for media consumption.</p>
              </div></td></tr>
        </tbody></table></td></tr>
            <tr id="40661603"><td></td></tr>
            <tr id="40660426"><td></td></tr>
            <tr id="40661756"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_40661756" href="https://news.ycombinator.com/vote?id=40661756&amp;how=up&amp;goto=item%3Fid%3D40660270"></a></center>    </td><td><br><div><p>As an aside, and as someone who doesn't own a vision pro (non US pleb): While it is interesting to me if people find utility. I can't help but feel that the narrative on places outside of HN is a strong "no".</p><p>But, that is to be expected, the form factor isn't convenient yet. When mobile phones weighed 2KG few people used them on a daily. When it's miniaturized into the form factor of glasses, we'll all be daily users. That seems to me more like a question of when, not if.</p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="40662620"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_40662620" href="https://news.ycombinator.com/vote?id=40662620&amp;how=up&amp;goto=item%3Fid%3D40660270"></a></center>    </td><td><br><div>
                  <p>A lot of the utility, even in this thread, seems to be solutions for problems that don't actually exist. I think it's cool technology, but nothing mentioned in this thread makes me want to get one.</p>
              </div></td></tr>
        </tbody></table></td></tr>
                  <tr id="40660427"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_40660427" href="https://news.ycombinator.com/vote?id=40660427&amp;how=up&amp;goto=item%3Fid%3D40660270"></a></center>    </td><td><br><div><p>Yeah. I enjoy it for watching movies/shows, especially while flying. I also like to do a couple of hours of work (programming) in a cool environment with immersive music every couple of days. I keep it on my standing desk and when I stand up, I frequently will put it on.</p><p>I don't recommend it for non developers until there is more content, but it's a really neat dev kit device that shows where the future is headed.</p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="40660324"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_40660324" href="https://news.ycombinator.com/vote?id=40660324&amp;how=up&amp;goto=item%3Fid%3D40660270"></a></center>    </td><td><br><div>
                  <p>Yes! Honestly, I mostly just use it as a really, really fancy iPad. It's great for watching content, like movies and TV shows. Some of the games are also fun too in small doses.</p>
              </div></td></tr>
        </tbody></table></td></tr>
            <tr id="40660361"><td></td></tr>
            <tr id="40660389"><td></td></tr>
            <tr id="40660364"><td></td></tr>
            <tr id="40660414"><td></td></tr>
                      </tbody></div></div>]]></description>
        </item>
    </channel>
</rss>