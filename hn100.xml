<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0">
    <channel>
        <title>HN100 - Readable Contents</title>
        <link>https://hn.algolia.com/api/v1/search_by_date?tags=%28story,poll%29&amp;numericFilters=points%3E100</link>
        <description>Uses Readability to add bodies to the RSS feed</description>
        <lastBuildDate>Thu, 09 May 2024 23:00:04 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>en</language>
        <item>
            <title><![CDATA[It's always TCP_NODELAY (333 pts)]]></title>
            <link>https://brooker.co.za/blog/2024/05/09/nagle.html</link>
            <guid>40310896</guid>
            <pubDate>Thu, 09 May 2024 17:54:49 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://brooker.co.za/blog/2024/05/09/nagle.html">https://brooker.co.za/blog/2024/05/09/nagle.html</a>, See on <a href="https://news.ycombinator.com/item?id=40310896">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="post">


<p>It's not the 1980s anymore, thankfully.</p>

<p>The first thing I check when debugging latency issues in distributed systems is whether <a href="https://linux.die.net/man/7/tcp">TCP_NODELAY</a> is enabled. And it’s not just me. Every distributed system builder I know has lost hours to latency issues quickly fixed by enabling this simple socket option, suggesting that the default behavior is wrong, and perhaps that the whole concept is outmoded.</p>

<p>First, let’s be clear about what we’re talking about. There’s no better source than John Nagle’s <a href="https://datatracker.ietf.org/doc/html/rfc896">RFC896</a> from 1984<sup><a href="#foot1">1</a></sup>. First, the problem statement:</p>

<blockquote>
  <p>There is a special problem associated with small  packets.   When TCP  is  used  for  the transmission of single-character messages originating at a keyboard, the typical result  is  that  41  byte packets (one  byte  of data, 40 bytes of header) are transmitted for each byte of useful data.  This 4000%  overhead  is  annoying but tolerable on lightly loaded networks.</p>
</blockquote>

<p>In short, Nagle was interested in better amortizing the cost of TCP headers, to get better throughput out of the network. Up to 40x better throughput! These tiny packets had two main causes: human-interactive applications like shells, where folks were typing a byte at a time, and poorly implemented programs that dribbled messages out to the kernel through many <code>write</code> calls. Nagle’s proposal for fixing this was simple and smart:</p>

<blockquote>
  <p>A  simple and elegant solution has been discovered.</p>
</blockquote>

<blockquote>
  <p>The solution is to inhibit the sending of new TCP  segments  when new  outgoing  data  arrives  from  the  user  if  any previously transmitted data on the connection remains unacknowledged.</p>
</blockquote>

<p>When many people talk about Nagle’s algorithm, they talk about timers, but RFC896 doesn’t use any kind of timer other than the round-trip time on the network.</p>

<p><em>Nagle’s Algorithm and Delayed Acks</em></p>

<p>Nagle’s nice, clean, proposal interacted poorly with another TCP feature: delayed <code>ACK</code>. The idea behind delayed <code>ACK</code> is to delay sending the acknowledgement of a packet at least until there’s some data to send back (e.g. a <code>telnet</code> session echoing back the user’s typing), or until a timer expires. <a href="https://datatracker.ietf.org/doc/html/rfc813">RFC813</a> from 1982 is that first that seems to propose delaying <code>ACKs</code>:</p>

<blockquote>
  <p>The receiver of data will   refrain   from   sending   an   acknowledgement   under   certain circumstances, in which case it must set a timer which  will  cause  the acknowledgement  to be sent later.  However, the receiver should do this only where it is a reasonable guess that some other event will intervene and prevent the necessity of the timer  interrupt.</p>
</blockquote>

<p>which is then formalized further in <a href="https://datatracker.ietf.org/doc/html/rfc1122">RFC1122</a> from 1989. The interaction between these two features causes a problem: Nagle’s algorithm is blocking sending more data until an <code>ACK</code> is received, but delayed ack is delaying that <code>ack</code> until a response is ready. Great for keeping packets full, not so great for latency-sensitive pipelined applications.</p>

<p>This is a point Nagle has made himself several times. For example in this <a href="https://news.ycombinator.com/item?id=10608356">Hacker News comment</a>:</p>

<blockquote>
  <p>That still irks me. The real problem is not tinygram prevention. It’s ACK delays, and that stupid fixed timer. They both went into TCP around the same time, but independently. I did tinygram prevention (the Nagle algorithm) and Berkeley did delayed ACKs, both in the early 1980s. The combination of the two is awful.</p>
</blockquote>

<p>As systems builders this is should be a familiar situation: two reasonable features of the system that interact to create an undesirable behavior. This kind of interaction is one of the things that makes protocol design so hard.</p>

<p><em>Is Nagle blameless?</em></p>

<p>Unfortunately, it’s not just delayed ACK. Even without delayed ack and that <em>stupid fixed timer</em>, the behavior of Nagle’s algorithm probably isn’t what we want in distributed systems. A single in-datacenter RTT is typically around 500μs, then a couple of milliseconds between datacenters in the same region, and up to hundreds of milliseconds going around the globe. Given the vast amount of work a modern server can do in even a few hundred microseconds, delaying sending data for even one RTT isn’t clearly a win.</p>

<p>To make a clearer case, let’s turn back to the justification behind Nagle’s algorithm: amortizing the cost of headers and avoiding that 40x overhead on single-byte packets. But does anybody send single byte packets anymore? Most distributed databases and systems don’t. Partially that’s because they simply have more to say, partially its because of additional overhead of protocols like TLS, and partially its because of encoding and serialization overhead. But mostly, they have more to say.</p>

<p>The core concern of not sending tiny messages is still a very real one, but we’ve very effectively pushed that into the application layer. Sending a byte at a time wrapped in JSON isn’t going to be very efficient, no matter what Nagle’s algorithm does.</p>

<p><em>Is Nagle needed?</em></p>

<p>First, the uncontroversial take: if you’re building a latency-sensitive distributed system running on modern datacenter-class hardware, enable <code>TCP_NODELAY</code> (disable Nagle’s algorithm) without worries. You don’t need to feel bad. It’s not a sin. It’s OK. Just go ahead.</p>

<p>More controversially, I suspect that Nagle’s algorithm just isn’t needed on modern systems, given the traffic and application mix, and the capabilities of the hardware we have today. In other words, <code>TCP_NODELAY</code> should be the default. That’s going to make some “<code>write</code> every byte” code slower than it would otherwise be, but those applications should be fixed anyway if we care about efficiency.</p>

<p><em>Footnotes</em></p>

<ol>
  <li><a name="foot1"></a> I won’t got into it here, but RFC896 is also one of the earliest statements I can find of metastable behavior in computer networks. In it, Nagle says: “This condition is stable. Once the  saturation point has been reached, if the algorithm for selecting packets to be dropped is fair, the network will continue to operate in a degraded condition.”</li>
</ol>


</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Leaked deck reveals how OpenAI is pitching publisher partnerships (259 pts)]]></title>
            <link>https://www.adweek.com/media/openai-preferred-publisher-program-deck/</link>
            <guid>40310228</guid>
            <pubDate>Thu, 09 May 2024 16:56:55 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.adweek.com/media/openai-preferred-publisher-program-deck/">https://www.adweek.com/media/openai-preferred-publisher-program-deck/</a>, See on <a href="https://news.ycombinator.com/item?id=40310228">Hacker News</a></p>
<div id="readability-page-1" class="page"><section data-continue-reading-wrapper="">
                                    



<div>
		
		<p>Mark your calendar for Mediaweek, October 29-30 in New York City. We’ll unpack the biggest shifts shaping the future of media—from tv to retail media to tech—and how marketers can prep to stay ahead. <a href="https://event.adweek.com/mediaweek-2024/4442894?ref=nativep1&amp;itm_source=ROS&amp;itm_medium=display&amp;itm_campaign=nativeMediaweek24&amp;itm_content=p1"><strong>Register</strong></a> with early-bird rates before sale ends!</p>
	</div>


<div><p>The generative artificial intelligence firm OpenAI has been pitching partnership opportunities to news publishers through an initiative called the Preferred Publishers Program, according to a deck obtained by ADWEEK and interviews with four industry executives.</p><p>OpenAI has been courting premium publishers dating back to July 2023, when it struck a licensing agreement with the Associated Press. It has since inked public partnerships with Axel Springer, The Financial Times, <a href="https://www.adweek.com/media/le-monde-english-subscribers-olympics/" target="_blank" rel="noreferrer noopener">Le Monde</a>, Prisa and Dotdash Meredith, although it has declined to share the specifics of any of its deals.</p><p>A representative for OpenAI disputed the accuracy of the information in the deck, which is more than three months old. The <a href="https://www.adweek.com/category/artificial-intelligence/" target="_blank">gen AI</a> firm also negotiates deals on a per-publisher basis, rather than structuring all of its deals uniformly, the representative said.</p><p>“We are engaging in productive conversations and partnerships with many news publishers around the world,” said a representative for OpenAI. “Our confidential documents are for discussion purposes only and ADWEEK’s reporting contains a number of mischaracterizations and outdated information.”</p><p>Nonetheless, the leaked deck reveals the basic structure of the partnerships OpenAI is proposing to media companies, as well as the incentives it is offering for their collaboration.</p><section> <p><a href="https://www.adweek.com/media/publishers-ai-licensing-negotiations-mark-an-inflection-point/" target="_blank"><img decoding="async" src="https://static-prod.adweek.com/wp-content/uploads/2023/11/publisher-ai-licensing-2023-640x360.jpg" alt="Copyright law favors creators, but commercial compromise offers a hedge against uncertainty."></a></p>  </section><h4><strong>Details from the pitch deck</strong></h4><p>The Preferred Publisher Program has five primary components, according to the deck.</p><p>First, it is available only to “select, high-quality editorial partners,” and its purpose is to help ChatGPT users more easily discover and engage with publishers’ brands and content.</p><p>Additionally, members of the program receive priority placement and “richer brand expression” in chat conversations, and their content benefits from more prominent link treatments. Finally, through PPP, OpenAI also offers licensed financial terms to publishers.</p><p>The financial incentives participating publishers can expect to receive are grouped into two buckets: guaranteed value and variable value.</p><p>Guaranteed value is a licensing payment that compensates the publisher for allowing OpenAI to access its backlog of data, while variable value is contingent on display success, a metric based on the number of users engaging with linked or displayed content.</p><p>The resulting financial offer would combine the guaranteed and variable values into one payment, which would be structured on an annual basis.&nbsp;</p><p>“The PPP program is more about scraping than training,” said one executive. “OpenAI has presumably already ingested and trained on these publishers’ archival data, but it needs access to contemporary content to answer contemporary queries.”</p><!--nextpage--><p>In return for these payments, OpenAI would gain two benefits. </p><p>It would have the ability to train on a publisher’s content and the license to display that information in ChatGPT products, complete with attribution and links. It would also get to announce the publisher as a preferred partner and work with them to build out these experiences.</p><h4><strong>Participation boosts publisher payouts</strong></h4><p>According to the deck, publisher participation in PPP creates a better experience for OpenAI users, which will help shift engagement toward browsing, i.e. queries that result in responses with links.</p><p>Roughly 25% of ChatGPT users already use the browse function, but the company expects that a majority of users will do so once the feature is broadly rolled out. If more users engage with publishers’ links, the <a href="https://www.adweek.com/category/media-news/" target="_blank" rel="noreferrer noopener">media companies</a> could earn larger payments for their variable value.&nbsp;</p><p>PPP members will see their content receive its “richer brand expression” through a series of content display products: the branded hover link, the anchored link and the in-line treatment.</p><p>In the hover treatment, which is available today, OpenAI will hyperlink keywords in its responses to search queries. The links appear as blue text and reveal a clickable tab when moused over. </p><p>In the anchor treatment, branded, clickable buttons appear below ChatGPT’s response to a user query. And the in-line product inserts a pullquote into the text of ChatGPT’s response, whose font is larger and includes a clickable, branded link.&nbsp;</p><p>All three content display products seek to cite the publishers whose writing is being used to answer the search query, although the setup will likely lead fewer users to visit publishers’ websites.&nbsp;</p><p>A recent model from The Atlantic found that if a search engine like Google were to integrate AI into search, it would answer a user’s query 75% of the time without requiring a clickthrough to its website.</p><h4><strong>Where publishers go from here</strong></h4><p>The details of the program add further color to the complicated relationship between digital publishers and OpenAI. The uncertain legal standing of the data-scraping methodology that OpenAI uses to power its large-language models has made licensing negotiations between the two parties complex.</p><p>While some publishers have opted to partner with OpenAI, others, <a href="https://www.adweek.com/media/open-ai-response-new-york-times-lawsuit/" target="_blank">including recent NewFronts participant The New York Times</a> and eight Alden Global Capital titles, have sued the tech firm on the grounds that it has used copyrighted articles without permission.</p><p>The vast majority of news publishers, as well as independent websites, have neither partnered with OpenAI nor taken legal action. According to one media executive, through programs such as Preferred Publisher, OpenAI is looking to change that.</p><p>“At the recent Aspen Conference in New York on AI and the news,” the person said, “OpenAI was very open about their need to attract publishers into their partnership program.”&nbsp;</p><!--nextpage--><p><em>This story has updated to include a response from OpenAI.</em></p></div>
<div id="meter-count">
  
  
    
    
  
  <a href="#" onclick="ShowAndHide()">
    
  </a>
  
  
  <div>
        <p>
          <h3>Enjoying Adweek's Content? Register for More Access!</h3>
        </p>
      </div>
</div>                                                                    </section></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Leaked FBI Email Reportedly Shows Desperation to Justify Warrantless Wiretaps (148 pts)]]></title>
            <link>https://gizmodo.com/leaked-fbi-email-warrantless-wiretaps-section-702-1851464520</link>
            <guid>40309957</guid>
            <pubDate>Thu, 09 May 2024 16:34:17 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://gizmodo.com/leaked-fbi-email-warrantless-wiretaps-section-702-1851464520">https://gizmodo.com/leaked-fbi-email-warrantless-wiretaps-section-702-1851464520</a>, See on <a href="https://news.ycombinator.com/item?id=40309957">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>Congress reauthorized America’s warrantless wiretapping program last month after some <span><a data-ga="[[&quot;Embedded Url&quot;,&quot;Internal link&quot;,&quot;https://gizmodo.com/national-security-threat-likely-nukes-in-space-1851257693&quot;,{&quot;metric25&quot;:1}]]" href="https://gizmodo.com/national-security-threat-likely-nukes-in-space-1851257693">successful fearmongering</a></span> by national security hawks on Capitol Hill. But an internal FBI email, leaked to <span><a data-ga="[[&quot;Embedded Url&quot;,&quot;External link&quot;,&quot;https://www.wired.com/story/fbi-section-702-us-person-queries-email/&quot;,{&quot;metric25&quot;:1}]]" href="https://www.wired.com/story/fbi-section-702-us-person-queries-email/" target="_blank" rel="noopener noreferrer">Wired</a></span> on Wednesday, may accidentally reveal how the federal law enforcement agency plans to overstep the spirit of the law, while technically maintaining the letter of the law.<br></p><div data-video-id="196937" data-monetizable="false" data-position="sidebar" data-video-title="Approaching Queerness in Doctor Who" data-video-blog-id="4" data-video-network="gizmodo" data-video-duration="72" data-playlist="196937,196931,196911" data-current="196937"><div><p>Approaching Queerness in Doctor Who</p></div><video disablepictureinpicture="" muted="" playsinline="" width="100%" height="100%" crossorigin="anonymous" preload="none"><source data-src="https://vid.kinja.com/prod/196937/196937_240p.mp4" label="240p" type="video/mp4"><source data-src="https://vid.kinja.com/prod/196937/196937_480p.mp4" label="480p" type="video/mp4"><source data-src="https://vid.kinja.com/prod/196937/196937_720p.mp4" label="720p" type="video/mp4"><source data-src="https://vid.kinja.com/prod/196937/196937_1080p.mp4" label="1080p" type="video/mp4"><track kind="captions" label="English" src="https://kinja.com/api/videoupload/caption/22499.vtt" srclang="en"></video><div><ul><li data-label="">Off</li><li data-label="English">English</li></ul></div></div><p>The controversial spying program is Section 702 in the Foreign Intelligence Surveillance Act (FISA) and allows the interception of foreign communications that sometimes include American citizens. The program ostensibly includes safeguards to ensure the law isn’t being used to unnecessarily spy on Americans, but it’s pretty clear from this new email that the FBI likes being able to get communications from Americans.<br></p><p>The email obtained by <span><a data-ga="[[&quot;Embedded Url&quot;,&quot;External link&quot;,&quot;https://www.wired.com/story/fbi-section-702-us-person-queries-email/&quot;,{&quot;metric25&quot;:1}]]" href="https://www.wired.com/story/fbi-section-702-us-person-queries-email/" target="_blank" rel="noopener noreferrer">Wired</a></span> dated April 20 was written by FBI Deputy Director Paul Abbate and sent out to employees internally.<br></p><p>“To continue to demonstrate why tools like this are essential to our mission, we need to <em>use</em> them, while also holding ourselves accountable for doing so properly and in compliance with legal requirements,” the email reads, according to Wired, which notes that the italicization on the word “use” was in the original email.</p><p>The FBI email made things even more explicit by encouraging searches for Americans when looking through intercepted communications.<br></p><p>“I urge everyone to continue to look for ways to appropriately use US  person queries to advance the mission, with the added confidence that  this new pre-approval requirement will help ensure that those queries  are fully compliant with the law,” the email reads.</p><p>The FBI’s response to Wired is particularly interesting, making it worth quoting at length. From Wired:<br></p><blockquote data-type="BlockQuote"><p>Following publication, FBI spokesperson Susan McKee provided a statement  from the bureau that mischaracterized WIRED’s reporting, inaccurately  claiming it “alleged that that the FBI instructed its employees to  violate the law or FBI policies.” The statement added that Abbate’s  email “emphasized Congress’ recognition of the vital importance of FISA  Section 702 to protect the American people and was sent to ensure that  FBI personnel were immediately aware of, and in compliance with, the  privacy enhancing changes the law has put in place.”</p></blockquote><p>Obviously, the FBI is going to say everyone at the agency follows the law since they quite literally are the law. But Wired spoke with Rep. Zoe Lofgren, a Democrat from California who notes this newly leaked email “directly contradicts earlier assertions” by the FBI when the agency was trying to get the law reauthorized.</p><p>It’s all a mess. The FBI got exactly what it wanted with the reauthorization of Section 702, something that was never really in doubt, even with pressure from a handful of politicians who opposed it. To paraphrase former president Richard Nixon, it’s <span><a data-ga="[[&quot;Embedded Url&quot;,&quot;External link&quot;,&quot;https://www.nixonlibrary.gov/media/video/excerpt-frost-interview-final&quot;,{&quot;metric25&quot;:1}]]" href="https://www.nixonlibrary.gov/media/video/excerpt-frost-interview-final" target="_blank" rel="noopener noreferrer">not illegal</a></span> when the FBI does it. But what are you going to do in such a <span><a data-ga="[[&quot;Embedded Url&quot;,&quot;External link&quot;,&quot;https://www.vox.com/2014/4/18/5624310/martin-gilens-testing-theories-of-american-politics-explained&quot;,{&quot;metric25&quot;:1}]]" href="https://www.vox.com/2014/4/18/5624310/martin-gilens-testing-theories-of-american-politics-explained" target="_blank" rel="noopener noreferrer">ridiculously rigged system</a></span>? </p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Familial Transmission of Personality Is Higher Than Shown in Typical Studies (120 pts)]]></title>
            <link>https://osf.io/preprints/psyarxiv/7ygp6</link>
            <guid>40309840</guid>
            <pubDate>Thu, 09 May 2024 16:24:38 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://osf.io/preprints/psyarxiv/7ygp6">https://osf.io/preprints/psyarxiv/7ygp6</a>, See on <a href="https://news.ycombinator.com/item?id=40309840">Hacker News</a></p>
&lt;Unparsable&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[ESP32 Drum Synth Machine (142 pts)]]></title>
            <link>https://github.com/zircothc/DRUM_2004_V1</link>
            <guid>40309759</guid>
            <pubDate>Thu, 09 May 2024 16:17:14 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/zircothc/DRUM_2004_V1">https://github.com/zircothc/DRUM_2004_V1</a>, See on <a href="https://news.ycombinator.com/item?id=40309759">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-turbo-body="">
      


    <div>
      <p><a href="#start-of-content" data-skip-target-assigned="false">Skip to content</a>
      
      <span data-view-component="true">
    <span data-view-component="true"></span>
</span></p><react-partial partial-name="keyboard-shortcuts-dialog" data-ssr="false">
  
  
  
</react-partial>




      

        

            


<header role="banner" data-color-mode="light" data-light-theme="light" data-dark-theme="dark">
  <h2>Navigation Menu</h2>

  

  <div>
          <nav aria-label="Global">
            <ul>
                <li>
      
      <div>
            <ul>
                <li>
  <a data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Product&quot;,&quot;action&quot;:&quot;click to go to Actions&quot;,&quot;label&quot;:&quot;ref_cta:Actions;&quot;}" href="https://github.com/features/actions">
      
      <div>
        <p>Actions</p><p>
        Automate any workflow
      </p></div>

    
</a></li>

                <li>
  <a data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Product&quot;,&quot;action&quot;:&quot;click to go to Packages&quot;,&quot;label&quot;:&quot;ref_cta:Packages;&quot;}" href="https://github.com/features/packages">
      
      <div>
        <p>Packages</p><p>
        Host and manage packages
      </p></div>

    
</a></li>

                <li>
  <a data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Product&quot;,&quot;action&quot;:&quot;click to go to Security&quot;,&quot;label&quot;:&quot;ref_cta:Security;&quot;}" href="https://github.com/features/security">
      
      <div>
        <p>Security</p><p>
        Find and fix vulnerabilities
      </p></div>

    
</a></li>

                <li>
  <a data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Product&quot;,&quot;action&quot;:&quot;click to go to Codespaces&quot;,&quot;label&quot;:&quot;ref_cta:Codespaces;&quot;}" href="https://github.com/features/codespaces">
      
      <div>
        <p>Codespaces</p><p>
        Instant dev environments
      </p></div>

    
</a></li>

                <li>
  <a data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Product&quot;,&quot;action&quot;:&quot;click to go to Copilot&quot;,&quot;label&quot;:&quot;ref_cta:Copilot;&quot;}" href="https://github.com/features/copilot">
      
      <div>
        <p>Copilot</p><p>
        Write better code with AI
      </p></div>

    
</a></li>

                <li>
  <a data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Product&quot;,&quot;action&quot;:&quot;click to go to Code review&quot;,&quot;label&quot;:&quot;ref_cta:Code review;&quot;}" href="https://github.com/features/code-review">
      
      <div>
        <p>Code review</p><p>
        Manage code changes
      </p></div>

    
</a></li>

                <li>
  <a data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Product&quot;,&quot;action&quot;:&quot;click to go to Issues&quot;,&quot;label&quot;:&quot;ref_cta:Issues;&quot;}" href="https://github.com/features/issues">
      
      <div>
        <p>Issues</p><p>
        Plan and track work
      </p></div>

    
</a></li>

                <li>
  <a data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Product&quot;,&quot;action&quot;:&quot;click to go to Discussions&quot;,&quot;label&quot;:&quot;ref_cta:Discussions;&quot;}" href="https://github.com/features/discussions">
      
      <div>
        <p>Discussions</p><p>
        Collaborate outside of code
      </p></div>

    
</a></li>

            </ul>
          </div>
</li>


                <li>
      
      
</li>


                <li>
      
      <div>
          <div>
            <ul>
                <li>
  <a data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Open Source&quot;,&quot;action&quot;:&quot;click to go to GitHub Sponsors&quot;,&quot;label&quot;:&quot;ref_cta:GitHub Sponsors;&quot;}" href="https://github.com/sponsors">
      
      <div>
        <p>GitHub Sponsors</p><p>
        Fund open source developers
      </p></div>

    
</a></li>

            </ul>
          </div>
          <div>
            <ul>
                <li>
  <a data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Open Source&quot;,&quot;action&quot;:&quot;click to go to The ReadME Project&quot;,&quot;label&quot;:&quot;ref_cta:The ReadME Project;&quot;}" href="https://github.com/readme">
      
      <div>
        <p>The ReadME Project</p><p>
        GitHub community articles
      </p></div>

    
</a></li>

            </ul>
          </div>
          
      </div>
</li>


                <li>
    <a data-analytics-event="{&quot;category&quot;:&quot;Header menu top item (logged out)&quot;,&quot;action&quot;:&quot;click to go to Pricing&quot;,&quot;label&quot;:&quot;ref_cta:Pricing;&quot;}" href="https://github.com/pricing">Pricing</a>
</li>

            </ul>
          </nav>

        <div>
                


<qbsearch-input data-scope="repo:zircothc/DRUM_2004_V1" data-custom-scopes-path="/search/custom_scopes" data-delete-custom-scopes-csrf="Xls1zmfjVRFg9r4Wl7imLC4d7t70EC8VFGEzJ3l8PkSTdyhMRwvLvr6FuGcBXE97nR8Ke0WUSEFhP2Op2rYh0Q" data-max-custom-scopes="10" data-header-redesign-enabled="false" data-initial-value="" data-blackbird-suggestions-path="/search/suggestions" data-jump-to-suggestions-path="/_graphql/GetSuggestedNavigationDestinations" data-current-repository="zircothc/DRUM_2004_V1" data-current-org="" data-current-owner="zircothc" data-logged-in="false" data-copilot-chat-enabled="false" data-blackbird-indexed-repo-csrf="<esi:include src=&quot;/_esi/rails_csrf_token_form_hidden?r=%2BiiCrl8XCZuAbyfAa9SDOHr3qvpevm5qEbQhyJ5E7dgHAE61KDk5rdJcw3r8O2y5EM1VVRGGeCAcnvPZAMPdBvItIhbHV0XYmg%2Fv8CP8RyrNEfe%2FO5azXeP83%2F0HfBzzThpa0pxtmgMeyOyb4XMSLxiL230rkV5xdveUyYtTqNEBGgJlJY5TshB80I2tgtZ6Os3NZL2gmjgXM9vDNSDwHD25huy6KhKPg0NpkNddr0SY6FIkrm2fjLlC3oqfuwpNkEgayvjoI7eDRnt67C1s%2Fz2aEhb9BHhYTqxbEYCpXRNdYPNAFBbjCQPJN9yoDnoEubkElq7AvFFM3NylFzAzSvtZjzOE1yO0JplkVZvZaeOlAUOj2K0dk%2F%2FSOwgbMQHPFoQesHKli8w8R2C3332APP4N2Vnm2xWkH6ctA5FLcp3EBJKNBNVzl4MatYE58pi4SwNAbI21ugU4PfXxFLyWGciBcMvFNnQhpAcYNENsQzIi5ugAkpKIzJQnqtEHdidObAsVK3H6mhqBCyrZrDPYIjfu--YhueA04nZryfZRfM--cAd3gG%2Fp%2FjkDIvZzaBkuUA%3D%3D&quot; />">
  <div data-modal-dialog-overlay="" data-action="click:qbsearch-input#searchInputContainerClicked">
  <modal-dialog data-action="close:qbsearch-input#handleClose cancel:qbsearch-input#handleClose" data-target="qbsearch-input.searchSuggestionsDialog" role="dialog" id="search-suggestions-dialog" aria-modal="true" aria-labelledby="search-suggestions-dialog-header" data-view-component="true">
      <h2 id="search-suggestions-dialog-header">Search code, repositories, users, issues, pull requests...</h2>
    
</modal-dialog></div>
  
  <div>
    
<dialog-helper>
  <dialog data-target="qbsearch-input.feedbackDialog" data-action="close:qbsearch-input#handleDialogClose cancel:qbsearch-input#handleDialogClose" id="feedback-dialog" aria-modal="true" aria-labelledby="feedback-dialog-title" aria-describedby="feedback-dialog-description" data-view-component="true">
    <div data-view-component="true">
    <p>
      <h2 id="feedback-dialog-title">
        Provide feedback
      </h2>
    </p>
    
  </div>
      <scrollable-region data-labelled-by="feedback-dialog-title">
        
      </scrollable-region>
      
</dialog></dialog-helper>

    <custom-scopes data-target="qbsearch-input.customScopesManager">
    
<dialog-helper>
  <dialog data-target="custom-scopes.customScopesModalDialog" data-action="close:qbsearch-input#handleDialogClose cancel:qbsearch-input#handleDialogClose" id="custom-scopes-dialog" aria-modal="true" aria-labelledby="custom-scopes-dialog-title" aria-describedby="custom-scopes-dialog-description" data-view-component="true">
    <div data-view-component="true">
    <p>
      <h2 id="custom-scopes-dialog-title">
        Saved searches
      </h2>
        <h2 id="custom-scopes-dialog-description">Use saved searches to filter your results more quickly</h2>
    </p>
    
  </div>
      <scrollable-region data-labelled-by="custom-scopes-dialog-title">
        
      </scrollable-region>
      
</dialog></dialog-helper>
    </custom-scopes>
  </div>
</qbsearch-input>

            <p><a href="https://github.com/signup?ref_cta=Sign+up&amp;ref_loc=header+logged+out&amp;ref_page=%2F%3Cuser-name%3E%2F%3Crepo-name%3E&amp;source=header-repo&amp;source_repo=zircothc%2FDRUM_2004_V1" data-hydro-click="{&quot;event_type&quot;:&quot;authentication.click&quot;,&quot;payload&quot;:{&quot;location_in_page&quot;:&quot;site header menu&quot;,&quot;repository_id&quot;:null,&quot;auth_type&quot;:&quot;SIGN_UP&quot;,&quot;originating_url&quot;:&quot;https://github.com/zircothc/DRUM_2004_V1&quot;,&quot;user_id&quot;:null}}" data-hydro-click-hmac="81dd76527504e666ffcf483d0fe30a1f780d08432723a3a9b88f6d261439a7e9" data-analytics-event="{&quot;category&quot;:&quot;Sign up&quot;,&quot;action&quot;:&quot;click to sign up for account&quot;,&quot;label&quot;:&quot;ref_page:/<user-name>/<repo-name>;ref_cta:Sign up;ref_loc:header logged out&quot;}">
              Sign up
            </a>
        </p></div>
      </div>
</header>

      
    </div>

  








    


    
    <include-fragment data-base-src="https://github.com/notifications/beta/shelf"></include-fragment>





  <div itemscope="" itemtype="http://schema.org/SoftwareSourceCode" data-commit-hovercards-enabled="" data-discussion-hovercards-enabled="" data-issue-and-pr-hovercards-enabled="">
    <main id="js-repo-pjax-container">
      
  





    






  
  

  



<turbo-frame id="repo-content-turbo-frame" target="_top" data-turbo-action="advance">
    <div id="repo-content-pjax-container">

  

  <include-fragment src="/zircothc/DRUM_2004_V1/spoofed_commit_check/a9e446620f1cd0590298ad68cbce2fedecb34a5c" data-test-selector="spoofed-commit-check"></include-fragment>

  <div data-view-component="true">        


















<react-partial partial-name="repos-overview" data-ssr="true">
  
  
  <div data-target="react-partial.reactRoot"><div><h2>Repository files navigation</h2><nav aria-label="Repository files"><ul role="list"><li><a href="#" aria-current="page"><span data-component="icon"></span><span data-component="text" data-content="README">README</span></a></li></ul></nav></div><div data-hpc="true"><article itemprop="text"><p dir="auto"><h2 tabindex="-1" dir="auto">DRUM_2004_V1</h2><a id="user-content-drum_2004_v1" aria-label="Permalink: DRUM_2004_V1" href="#drum_2004_v1"></a></p>
<p dir="auto">ESP32 DRUM SYNTH MACHINE</p>
<p dir="auto">This is my DRUM SYNTH LOFI MACHINE.</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://private-user-images.githubusercontent.com/17828930/326164677-c8327dc2-a3f7-4d81-8d82-ebfe2a7c45c3.jpg?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MTUyODg3MDQsIm5iZiI6MTcxNTI4ODQwNCwicGF0aCI6Ii8xNzgyODkzMC8zMjYxNjQ2NzctYzgzMjdkYzItYTNmNy00ZDgxLThkODItZWJmZTJhN2M0NWMzLmpwZz9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNDA1MDklMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjQwNTA5VDIxMDAwNFomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPTU1MjAxMzAzNmNiNGJlZWY0Y2RkNDgzZTMxZGU2NGMxYTJiOTYzYjRmMDJiYzgyN2JmZjE3NjExNjQ3ZGFhNDcmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0JmFjdG9yX2lkPTAma2V5X2lkPTAmcmVwb19pZD0wIn0.Uyua3Yza2N0QuZGlt3lN_wHPan8EM8u4O6Llrz74cEc"><img src="https://private-user-images.githubusercontent.com/17828930/326164677-c8327dc2-a3f7-4d81-8d82-ebfe2a7c45c3.jpg?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MTUyODg3MDQsIm5iZiI6MTcxNTI4ODQwNCwicGF0aCI6Ii8xNzgyODkzMC8zMjYxNjQ2NzctYzgzMjdkYzItYTNmNy00ZDgxLThkODItZWJmZTJhN2M0NWMzLmpwZz9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNDA1MDklMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjQwNTA5VDIxMDAwNFomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPTU1MjAxMzAzNmNiNGJlZWY0Y2RkNDgzZTMxZGU2NGMxYTJiOTYzYjRmMDJiYzgyN2JmZjE3NjExNjQ3ZGFhNDcmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0JmFjdG9yX2lkPTAma2V5X2lkPTAmcmVwb19pZD0wIn0.Uyua3Yza2N0QuZGlt3lN_wHPan8EM8u4O6Llrz74cEc" alt="IMG_20240406_150440"></a></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Synth engine:</h2><a id="user-content-synth-engine" aria-label="Permalink: Synth engine:" href="#synth-engine"></a></p>
<ul dir="auto">
<li>Wavetable synthesizer based on DZL Arduino library "The Synth" (<a href="https://github.com/dzlonline/the_synth">https://github.com/dzlonline/the_synth</a>)</li>
<li>16 sound polyphony</li>
<li>Sound parameters: Table, Length, Envelope, Pitch, Modulation, + Volume, Pan and Filter.</li>
<li>Filter (LowPassFilter) comes from Mozzi library (<a href="https://github.com/sensorium/Mozzi">https://github.com/sensorium/Mozzi</a>)</li>
</ul>
<p dir="auto">SEQUENCER:</p>
<ul dir="auto">
<li>16 step/pattern editor and random generators (pattern, sound parameters and notes)</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Hardware:</h2><a id="user-content-hardware" aria-label="Permalink: Hardware:" href="#hardware"></a></p>
<ul dir="auto">
<li>Lolin S2 Mini (ESP32 S2)</li>
<li>PCM5102A I2s dac</li>
<li>24 push buttons (8x3)</li>
<li>Rotary encoder</li>
<li>OLED display I2c</li>
<li>32 LED WS2812B</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Software:</h2><a id="user-content-software" aria-label="Permalink: Software:" href="#software"></a></p>
<p dir="auto">IDE:
Arduino 1.8.19</p>
<p dir="auto">Boards:
Expressif Systems 2.0.14</p>
<p dir="auto">Board: Lolin S2 Mini</p>
<p dir="auto">Libraries:</p>
<ul dir="auto">
<li>Sequencer Timer - uClock: <a href="https://github.com/midilab/uClock">https://github.com/midilab/uClock</a></li>
<li>RGB Leds - Adafruit Neopixel: <a href="https://github.com/adafruit/Adafruit_NeoPixel">https://github.com/adafruit/Adafruit_NeoPixel</a></li>
<li>OLED - u8g2: <a href="https://github.com/olikraus/u8g2">https://github.com/olikraus/u8g2</a></li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Notes:</h2><a id="user-content-notes" aria-label="Permalink: Notes:" href="#notes"></a></p>
<p dir="auto">Schematics uploaded.</p>
<p dir="auto">Join solder pads near SCK pin in PCM5102A module.</p>
<p dir="auto">Video demo of the prototype:</p>
<p dir="auto"><a href="https://www.youtube.com/watch?v=rXl1gpWJp-g" rel="nofollow"><img src="https://camo.githubusercontent.com/6859cb54d06b51e0e0690b1ff5f288e2ec91981a74f66c0be1331ed00486f80d/68747470733a2f2f696d672e796f75747562652e636f6d2f76692f72586c316770574a702d672f302e6a7067" alt="IMG_20240406_150231" data-canonical-src="https://img.youtube.com/vi/rXl1gpWJp-g/0.jpg"></a></p>
<p dir="auto">Waiting PCBs to build the first one :)
<a target="_blank" rel="noopener noreferrer" href="https://private-user-images.githubusercontent.com/17828930/326164735-feb9b928-f76a-4b51-93ea-a7afbd6a5c28.jpg?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MTUyODg3MDQsIm5iZiI6MTcxNTI4ODQwNCwicGF0aCI6Ii8xNzgyODkzMC8zMjYxNjQ3MzUtZmViOWI5MjgtZjc2YS00YjUxLTkzZWEtYTdhZmJkNmE1YzI4LmpwZz9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNDA1MDklMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjQwNTA5VDIxMDAwNFomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPWNlZTFjNjg4YzljOWQyMTUwMjE1NjQ1YmI3MDExYjk2ZDgxZmY5MmJlZDYyNDVmMzEyMDc0YzkyMjI2NDAxZjUmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0JmFjdG9yX2lkPTAma2V5X2lkPTAmcmVwb19pZD0wIn0.Zq6yD07v-buQ7WaQxTmfWfWiBKceY6Unmnnss2J2fDs"><img src="https://private-user-images.githubusercontent.com/17828930/326164735-feb9b928-f76a-4b51-93ea-a7afbd6a5c28.jpg?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MTUyODg3MDQsIm5iZiI6MTcxNTI4ODQwNCwicGF0aCI6Ii8xNzgyODkzMC8zMjYxNjQ3MzUtZmViOWI5MjgtZjc2YS00YjUxLTkzZWEtYTdhZmJkNmE1YzI4LmpwZz9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNDA1MDklMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjQwNTA5VDIxMDAwNFomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPWNlZTFjNjg4YzljOWQyMTUwMjE1NjQ1YmI3MDExYjk2ZDgxZmY5MmJlZDYyNDVmMzEyMDc0YzkyMjI2NDAxZjUmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0JmFjdG9yX2lkPTAma2V5X2lkPTAmcmVwb19pZD0wIn0.Zq6yD07v-buQ7WaQxTmfWfWiBKceY6Unmnnss2J2fDs" alt="IMG_20240406_150231"></a>
<a target="_blank" rel="noopener noreferrer" href="https://private-user-images.githubusercontent.com/17828930/326164951-e1001f26-0993-4221-90d1-e9a2f710af0f.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MTUyODg3MDQsIm5iZiI6MTcxNTI4ODQwNCwicGF0aCI6Ii8xNzgyODkzMC8zMjYxNjQ5NTEtZTEwMDFmMjYtMDk5My00MjIxLTkwZDEtZTlhMmY3MTBhZjBmLnBuZz9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNDA1MDklMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjQwNTA5VDIxMDAwNFomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPTA2ZmM0NTAzMDkyM2VmZjBiYzFmODI5MzcyMzBhMTZiNGU4YjFkODk0NzNhM2UwZGY5NjQ2ZGVmNmZmYzc3OTQmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0JmFjdG9yX2lkPTAma2V5X2lkPTAmcmVwb19pZD0wIn0.DAz14RYXzspi3g35ofVXaOrf49ivpYcGhtBmBx8F5ZA"><img src="https://private-user-images.githubusercontent.com/17828930/326164951-e1001f26-0993-4221-90d1-e9a2f710af0f.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MTUyODg3MDQsIm5iZiI6MTcxNTI4ODQwNCwicGF0aCI6Ii8xNzgyODkzMC8zMjYxNjQ5NTEtZTEwMDFmMjYtMDk5My00MjIxLTkwZDEtZTlhMmY3MTBhZjBmLnBuZz9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNDA1MDklMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjQwNTA5VDIxMDAwNFomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPTA2ZmM0NTAzMDkyM2VmZjBiYzFmODI5MzcyMzBhMTZiNGU4YjFkODk0NzNhM2UwZGY5NjQ2ZGVmNmZmYzc3OTQmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0JmFjdG9yX2lkPTAma2V5X2lkPTAmcmVwb19pZD0wIn0.DAz14RYXzspi3g35ofVXaOrf49ivpYcGhtBmBx8F5ZA" alt="board"></a></p>
</article></div></div>
</react-partial>

        </div></div>

</turbo-frame>


    </main>
  </div>

          




    <ghcc-consent id="ghcc" data-initial-cookie-consent-allowed="" data-cookie-consent-required="true"></ghcc-consent>


  

    <template id="site-details-dialog">
  <details class="details-reset details-overlay details-overlay-dark lh-default color-fg-default hx_rsm" open="">
    <summary role="button" aria-label="Close dialog"></summary>
    <details-dialog class="Box Box--overlay d-flex flex-column anim-fade-in fast hx_rsm-dialog hx_rsm-modal">
      <button class="Box-btn-octicon m-0 btn-octicon position-absolute right-0 top-0" type="button" aria-label="Close dialog" data-close-dialog="">
        <svg aria-hidden="true" height="16" viewBox="0 0 16 16" version="1.1" width="16" data-view-component="true" class="octicon octicon-x">
    <path d="M3.72 3.72a.75.75 0 0 1 1.06 0L8 6.94l3.22-3.22a.749.749 0 0 1 1.275.326.749.749 0 0 1-.215.734L9.06 8l3.22 3.22a.749.749 0 0 1-.326 1.275.749.749 0 0 1-.734-.215L8 9.06l-3.22 3.22a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042L6.94 8 3.72 4.78a.75.75 0 0 1 0-1.06Z"></path>
</svg>
      </button>
      <div class="octocat-spinner my-6 js-details-dialog-spinner"></div>
    </details-dialog>
  </details>
</template>

    

    <template id="snippet-clipboard-copy-button">
  <div class="zeroclipboard-container position-absolute right-0 top-0">
    <clipboard-copy aria-label="Copy" class="ClipboardButton btn js-clipboard-copy m-2 p-0 tooltipped-no-delay" data-copy-feedback="Copied!" data-tooltip-direction="w">
      <svg aria-hidden="true" height="16" viewBox="0 0 16 16" version="1.1" width="16" data-view-component="true" class="octicon octicon-copy js-clipboard-copy-icon m-2">
    <path d="M0 6.75C0 5.784.784 5 1.75 5h1.5a.75.75 0 0 1 0 1.5h-1.5a.25.25 0 0 0-.25.25v7.5c0 .138.112.25.25.25h7.5a.25.25 0 0 0 .25-.25v-1.5a.75.75 0 0 1 1.5 0v1.5A1.75 1.75 0 0 1 9.25 16h-7.5A1.75 1.75 0 0 1 0 14.25Z"></path><path d="M5 1.75C5 .784 5.784 0 6.75 0h7.5C15.216 0 16 .784 16 1.75v7.5A1.75 1.75 0 0 1 14.25 11h-7.5A1.75 1.75 0 0 1 5 9.25Zm1.75-.25a.25.25 0 0 0-.25.25v7.5c0 .138.112.25.25.25h7.5a.25.25 0 0 0 .25-.25v-7.5a.25.25 0 0 0-.25-.25Z"></path>
</svg>
      <svg aria-hidden="true" height="16" viewBox="0 0 16 16" version="1.1" width="16" data-view-component="true" class="octicon octicon-check js-clipboard-check-icon color-fg-success d-none m-2">
    <path d="M13.78 4.22a.75.75 0 0 1 0 1.06l-7.25 7.25a.75.75 0 0 1-1.06 0L2.22 9.28a.751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018L6 10.94l6.72-6.72a.75.75 0 0 1 1.06 0Z"></path>
</svg>
    </clipboard-copy>
  </div>
</template>
<template id="snippet-clipboard-copy-button-unpositioned">
  <div class="zeroclipboard-container">
    <clipboard-copy aria-label="Copy" class="ClipboardButton btn btn-invisible js-clipboard-copy m-2 p-0 tooltipped-no-delay d-flex flex-justify-center flex-items-center" data-copy-feedback="Copied!" data-tooltip-direction="w">
      <svg aria-hidden="true" height="16" viewBox="0 0 16 16" version="1.1" width="16" data-view-component="true" class="octicon octicon-copy js-clipboard-copy-icon">
    <path d="M0 6.75C0 5.784.784 5 1.75 5h1.5a.75.75 0 0 1 0 1.5h-1.5a.25.25 0 0 0-.25.25v7.5c0 .138.112.25.25.25h7.5a.25.25 0 0 0 .25-.25v-1.5a.75.75 0 0 1 1.5 0v1.5A1.75 1.75 0 0 1 9.25 16h-7.5A1.75 1.75 0 0 1 0 14.25Z"></path><path d="M5 1.75C5 .784 5.784 0 6.75 0h7.5C15.216 0 16 .784 16 1.75v7.5A1.75 1.75 0 0 1 14.25 11h-7.5A1.75 1.75 0 0 1 5 9.25Zm1.75-.25a.25.25 0 0 0-.25.25v7.5c0 .138.112.25.25.25h7.5a.25.25 0 0 0 .25-.25v-7.5a.25.25 0 0 0-.25-.25Z"></path>
</svg>
      <svg aria-hidden="true" height="16" viewBox="0 0 16 16" version="1.1" width="16" data-view-component="true" class="octicon octicon-check js-clipboard-check-icon color-fg-success d-none">
    <path d="M13.78 4.22a.75.75 0 0 1 0 1.06l-7.25 7.25a.75.75 0 0 1-1.06 0L2.22 9.28a.751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018L6 10.94l6.72-6.72a.75.75 0 0 1 1.06 0Z"></path>
</svg>
    </clipboard-copy>
  </div>
</template>




    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[HNInternal: Launch HN: Muddy (YC S19) – Multiplayer browser for getting work done (143 pts)]]></title>
            <link>https://news.ycombinator.com/item?id=40309342</link>
            <guid>40309342</guid>
            <pubDate>Thu, 09 May 2024 15:38:48 GMT</pubDate>
            <description><![CDATA[<p>See on <a href="https://news.ycombinator.com/item?id=40309342">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><td colspan="2"></td><td><div><p>Hey HN! This is Jimmy, Ron and Austa from Muddy (<a href="https://feelmuddy.com/">https://feelmuddy.com/</a>). Muddy is a browser for work that automatically keeps project files organized in the same place where you use and share them. Here’s a demo: <a href="https://www.youtube.com/watch?v=tZr49aN3sjQ" rel="nofollow">https://www.youtube.com/watch?v=tZr49aN3sjQ</a>. Download and try it out here: <a href="https://feelmuddy.com/">https://feelmuddy.com/</a>.</p><p>Building together in the past, we were incredibly frustrated with how much friction there is to get anything done on our computers. I was losing time everyday digging through chat logs looking for that one important link or breaking others out of flow by asking where something is.</p><p>Web apps promised to help us get more done—and they do, but each in its own silo, so there’s still a ton of redundancy to deal with. Every app has its own way of organizing files, its own notification inbox, its own search system. Conversations live everywhere and there isn’t a single view to see everything about a project. Remember when files simply lived in folders rather than the “cloud”?</p><p>We started dedicating time to organizing our files in shared docs and limiting new apps we used. This helped – but the second we didn’t stay on top of organization, links became stale and things got messy again.</p><p>Muddy started as a hack week project we built for ourselves—a single place to use web apps with others, but personalized for each user automatically. Everyone gets their own view for every project, designed around how they work.</p><p>Muddy users work on projects in spaces, which are like automatic tab groups. Users share apps (any site works—a Github PR, Figma file, Trello board—whatever you want) into the project’s shared timeline and Muddy automatically opens relevant tabs for you. It’s a single click to open up all the apps you need for the project.</p><p>Under the hood, Muddy works in the background to keep track of the timeline and uses a LLM to continuously organize apps and keep everything on to date. It considers signals like the popularity of a file, naming conventions, and conversations to figure out what’s relevant. So everyone is presented with an updated list of important tabs, without anyone lifting a finger. Our actual browser is based on Chromium.</p><p>When you need to revisit something from weeks ago, you can rewind the project timeline to that point in a single click. Apps open up in the timeline so you’ll see your files right away. For sites that don’t have built in collaboration features (like documentation), Muddy lets you do annotations directly on the website.</p><p>Projects sometimes get big and need to be broken up. Across all your spaces, Muddy can answer questions like ChatGPT, cite your files as sources, and return apps directly. This is possible since Muddy’s AI shares your browser and can use your authenticated apps locally (with privacy in mind).</p><p>Other browsers like Chrome and Arc focus on solo productivity with sharing as a bolt-on. We think productivity depends on how well you can work with others, and should be the first class consideration. And doing organizational work manually is unsustainable.</p><p>Muddy will have paid subscriptions for teams with additional features like shared passwords, team organization, custom shortcuts, and SSO management. Those aren’t built out yet and the base product will be free. No part of our revenue will come from data monetization.</p><p>We’d love for you to give Muddy a spin! You can download Muddy for Mac or Windows on our website and add others once inside: <a href="https://feelmuddy.com/">https://feelmuddy.com/</a>. We’ll be around to answer questions and look forward to any and all feedback!</p></div></td></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Microsoft PlayReady – Complete Client Identity Compromise (173 pts)]]></title>
            <link>https://seclists.org/fulldisclosure/2024/May/5</link>
            <guid>40308261</guid>
            <pubDate>Thu, 09 May 2024 13:53:06 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://seclists.org/fulldisclosure/2024/May/5">https://seclists.org/fulldisclosure/2024/May/5</a>, See on <a href="https://news.ycombinator.com/item?id=40308261">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="nst-content">

<!--X-Body-Begin-->
<!--X-User-Header-->
<a href="https://seclists.org/fulldisclosure/"><img src="https://seclists.org/images/fulldisclosure-logo.png" width="80" alt="fulldisclosure logo"></a>
<h2><a href="https://seclists.org/fulldisclosure/">Full Disclosure</a>
mailing list archives</h2>
<!--X-User-Header-End-->
<!--X-TopPNI-->


<!--X-TopPNI-End-->
<!--X-MsgBody-->
<!--X-Subject-Header-Begin-->

<hr>
<!--X-Subject-Header-End-->
<!--X-Head-of-Message-->


<em>From</em>: Security Explorations &lt;contact () security-explorations com&gt;<br>

<em>Date</em>: Thu, 9 May 2024 10:02:26 +0200<br>

<!--X-Head-of-Message-End-->
<!--X-Head-Body-Sep-Begin-->
<hr>
<!--X-Head-Body-Sep-End-->
<!--X-Body-of-Message-->
<pre>Hello All,

We have come up with two attack scenarios that make it possible to
extract private ECC keys used by a PlayReady client (Windows SW DRM
scenario) for the communication with a license server and identity
purposes.

More specifically, we successfully demonstrated the extraction of the
following keys:
- private signing key used to digitally sign license requests issued
by PlayReady client,
- private encryption key used to decrypt license responses received by
the client (decrypt license blobs carrying encrypted content keys).

A proof for the above (which Microsoft should be able to confirm) is
available at this link:
<a rel="nofollow" href="https://security-explorations.com/samples/wbpmp_id_compromise_proof.txt">https://security-explorations.com/samples/wbpmp_id_compromise_proof.txt</a>

While PlayReady security is primary about security of content keys,
ECC keys that make up client identity are even more important. Upon
compromise, these keys can be used to mimic a PlayReady client outside
of a Protected Media Path environment and regardless of the imposed
security restrictions.

In that context, extraction of ECC keys used as part of a PlayReady
client identity constitute an ultimate compromise of a PlayReady
client on Windows ("escape" of the PMP environment, ability to request
licenses and decrypt content keys).

Content key extraction from Protected Media Path process (through XOR
key or white-box crypto data structures) in a combination with this
latest identity compromise attack means that there is nothing left to
break when it comes to Windows SW DRM implementation.

Let this serve as a reminder that PlayReady content protection
implemented in software and on a client side has little chances of a
“survival” (understood as a state of not being successfully reverse
engineered and compromised). In that context, this is vendor’s
responsibility to constantly increase the bar and with the use of all
available technological means.

Thank you.

Best Regards,
Adam Gowdiak

----------------------------------
Security Explorations -
AG Security Research Lab
<a rel="nofollow" href="https://security-explorations.com/">https://security-explorations.com</a>
----------------------------------
_______________________________________________
Sent through the Full Disclosure mailing list
<a rel="nofollow" href="https://nmap.org/mailman/listinfo/fulldisclosure">https://nmap.org/mailman/listinfo/fulldisclosure</a>
Web Archives &amp; RSS: <a rel="nofollow" href="https://seclists.org/fulldisclosure/">https://seclists.org/fulldisclosure/</a></pre>
<!--X-Body-of-Message-End-->
<!--X-MsgBody-End-->
<!--X-Follow-Ups-->
<hr>
<!--X-Follow-Ups-End-->
<!--X-References-->
<!--X-References-End-->
<!--X-BotPNI-->

<h3>Current thread:</h3>
<ul>
<li><strong>Microsoft PlayReady - complete client identity compromise</strong> <em>Security Explorations (May 09)</em>
</li></ul>


<!--X-BotPNI-End-->
<!--X-User-Footer-->
<!--X-User-Footer-End-->
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[No "Zero-Shot" Without Exponential Data (148 pts)]]></title>
            <link>https://arxiv.org/abs/2404.04125</link>
            <guid>40307832</guid>
            <pubDate>Thu, 09 May 2024 13:08:05 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://arxiv.org/abs/2404.04125">https://arxiv.org/abs/2404.04125</a>, See on <a href="https://news.ycombinator.com/item?id=40307832">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="content-inner">
    
    
                
    <p><a href="https://arxiv.org/pdf/2404.04125">View PDF</a>
    <a href="https://arxiv.org/html/2404.04125v2">HTML (experimental)</a></p><blockquote>
            <span>Abstract:</span>Web-crawled pretraining datasets underlie the impressive "zero-shot" evaluation performance of multimodal models, such as CLIP for classification/retrieval and Stable-Diffusion for image generation. However, it is unclear how meaningful the notion of "zero-shot" generalization is for such multimodal models, as it is not known to what extent their pretraining datasets encompass the downstream concepts targeted for during "zero-shot" evaluation. In this work, we ask: How is the performance of multimodal models on downstream concepts influenced by the frequency of these concepts in their pretraining datasets? We comprehensively investigate this question across 34 models and five standard pretraining datasets (CC-3M, CC-12M, YFCC-15M, LAION-400M, LAION-Aesthetics), generating over 300GB of data artifacts. We consistently find that, far from exhibiting "zero-shot" generalization, multimodal models require exponentially more data to achieve linear improvements in downstream "zero-shot" performance, following a sample inefficient log-linear scaling trend. This trend persists even when controlling for sample-level similarity between pretraining and downstream datasets, and testing on purely synthetic data distributions. Furthermore, upon benchmarking models on long-tailed data sampled based on our analysis, we demonstrate that multimodal models across the board perform poorly. We contribute this long-tail test set as the "Let it Wag!" benchmark to further research in this direction. Taken together, our study reveals an exponential need for training data which implies that the key to "zero-shot" generalization capabilities under large-scale training paradigms remains to be found.
    </blockquote>

    <!--CONTEXT-->
    
  </div><div>
      <h2>Submission history</h2><p> From: Vishaal Udandarao [<a href="https://arxiv.org/show-email/3ec97a25/2404.04125">view email</a>]      <br>            <strong><a href="https://arxiv.org/abs/2404.04125v1">[v1]</a></strong>
        Thu, 4 Apr 2024 17:58:02 UTC (37,862 KB)<br>
    <strong>[v2]</strong>
        Mon, 8 Apr 2024 21:14:43 UTC (37,863 KB)<br>
</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Exploring HN by mapping and analyzing 40M posts and comments for fun (286 pts)]]></title>
            <link>https://blog.wilsonl.in/hackerverse/</link>
            <guid>40307519</guid>
            <pubDate>Thu, 09 May 2024 12:31:04 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://blog.wilsonl.in/hackerverse/">https://blog.wilsonl.in/hackerverse/</a>, See on <a href="https://news.ycombinator.com/item?id=40307519">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
    <article>
      
<p><img src="https://blog.wilsonl.in/hackerverse/map.png" alt="Semantic map of Hacker News posts."></p>
<p>The above is a map of all Hacker News posts since its <a href="https://news.ycombinator.com/item?id=1">founding</a>, laid semantically i.e. where there <em>should</em> be some relationship between positions and distances. I've been building it and some other interesting stuff over the past few weeks, to play around with <a href="https://platform.openai.com/docs/guides/embeddings">text embeddings</a>. Given that HN has a lot of interesting, curated content and <a href="https://github.com/HackerNews/API">exposes all its content programatically</a>, I thought it'd be a fun place to start.</p>
<p>A quick primer of embeddings: they are a powerful and cool way to represent <em>something</em> (in this case, text) as a point in a high-dimensional <a href="https://en.wikipedia.org/wiki/Latent_space">space</a>, which in practical terms just means an array of floats, one for its coordinate in that dimension. The absolute position doesn't really mean much, but their relativity to each other is where much of their usefulness comes in, because "similar" things should be nearby, while dissimilar things are far apart. Text embeddings these days often <a href="https://en.wikipedia.org/wiki/BERT_(language_model)">come from language models</a>, given their SOTA understanding of the meaning of text, and it's pretty trivial to generate them given the high-quality open source <a href="https://huggingface.co/spaces/mteb/leaderboard">models</a> and <a href="https://huggingface.co/sentence-transformers">libraries</a>, that are freely accessible to anyone with a CPU or GPU.</p>
<p>Going in, my theories for what I could do <em>if</em> I had the embeddings were:</p>
<ul>
<li>Powerful search: given HN's curated high bar of content, I knew there were lots of interesting insights and things I've missed over the years. It'd be cool if I could query something like "how to communicate well" and instantly surface the best advice over the years for communicating.</li>
<li>Recommendations: it might be possible to build a personalized discovery engine by navigating the latent space of HN content biased towards/away from dis/interest areas.</li>
<li>Analysis: there are a lot of opinions on HN. It should be possible to calculate the sentiment and popularity of various topics within the community, find opposing viewpoints, etc.</li>
</ul>
<p>These sounded pretty interesting, so I decided to dive right in. In this blog post, I'll lay out my journey starting from no data and no code, to interactive search, analysis, and spatial visualization tools leveraging millions of HN content, dive into all the interesting diverse problems and solutions that came up along the way, and hopefully give you some indication (and hopefully motivation) of the power and applicability of embeddings in many areas.</p>
<p>You may also have better ideas of using the data or the demo that I came up with. I'm also opening up all the data and source code that I built as part of this journey, and invite you to use them to play around, suggest and refine ideas, or kick off your own creative projects and learning journeys. Over 30 million comments and 4 million posts are available to download <a href="https://github.com/wilsonzlin/hackerverse/releases/tag/dataset-39996091">here</a>, which include metadata (IDs, scores, authors, etc.), embeddings, and texts (including crawled web pages). The code is also <a href="https://github.com/wilsonzlin/hackerverse">completely open source</a>; feel free to fork, open PRs, or raise issues. If you do end up using the code or data, I'd appreciate a reference back to this project and blog post.</p>
<p>If you want to jump right to the demo, <a href="#demo">click here</a>. Otherwise, let's dive in!</p>
<h2 id="fetching-items-from-hn">Fetching items from HN</h2>
<p>HN has a very simple <a href="https://github.com/HackerNews/API">public API</a>:</p>
<pre><code>GET /v0/item/$ITEM_ID.json
Host: hacker-news.firebaseio.com
</code></pre><p>Everything is an <em>item</em>, and the response always has the same JSON object structure:</p>
<pre><code><span>{</span>
  <span>"by"</span><span>:</span> <span>"dhouston"</span><span>,</span>
  <span>"descendants"</span><span>:</span> <span>71</span><span>,</span>
  <span>"id"</span><span>:</span> <span>8863</span><span>,</span>
  <span>"score"</span><span>:</span> <span>111</span><span>,</span>
  <span>"time"</span><span>:</span> <span>1175714200</span><span>,</span>
  <span>"title"</span><span>:</span> <span>"My YC app: Dropbox - Throw away your USB drive"</span><span>,</span>
  <span>"type"</span><span>:</span> <span>"story"</span><span>,</span>
  <span>"url"</span><span>:</span> <span>"http://www.getdropbox.com/u/2/screencast.html"</span>
<span>}</span>
</code></pre><p>There's also a <a href="https://hacker-news.firebaseio.com/v0/maxitem.json">maxitem.json</a> API, which gives the largest ID. As of this writing, the max item ID is over 40 million. Even with a very nice and low 10 ms mean response time, this would take over 4 days to crawl, so we need some parallelism.</p>
<p>I decided to write a <a href="https://github.com/wilsonzlin/hackerverse/tree/master/enqueuer">quick service</a> in Node.js to do this. An initial approach with a <a href="https://github.com/wilsonzlin/xtjs-lib/blob/master/Semaphore.ts">semaphore</a> and then queueing up the fetch Promises, despite being simple and async, ended up being too slow, with most CPU time being spent in userspace JS code.</p>
<p>It's a good reminder that Node.js can handle async I/O pretty well, but it's still fundamentally a single-threaded dynamic language, and those few parts running JS code can still drag down performance. I moved to using the <a href="https://nodejs.org/api/worker_threads.html">worker threads</a> API and distributed the fetches across all CPUs, which ended up saturating all cores on my machine, mostly spent in kernel space (a good sign). The final code ended up looking something like:</p>
<pre><code><span>new</span> <span>WorkerPool</span>(__filename, <span>cpus</span>().<span>length</span>)
  .<span>workerTask</span>(<span>"process"</span>, <span>async</span> (<span>id</span>: <span>number</span>) =&gt; {
    <span>const</span> item = <span>await</span> <span>fetchItem</span>(id);
    <span>await</span> <span>processItem</span>(item);
  })
  .<span>leader</span>(<span>async</span> (pool) =&gt; {
    <span>let</span> nextId = <span>await</span> <span>getNextIdToResumefrom</span>();
    <span>const</span> maxId = <span>await</span> <span>fetchHnMaxId</span>();

    <span>let</span> nextIdToCommit = nextId;
    <span>const</span> idsPendingCommit = <span>new</span> <span>Set</span>&lt;<span>number</span>&gt;();
    <span>let</span> flushing = <span>false</span>;
    <span>const</span> <span>maybeFlushId</span> = <span>async</span> (<span></span>) =&gt; {
      <span>if</span> (flushing) {
        <span>return</span>;
      }
      flushing = <span>true</span>;
      <span>let</span> didChange = <span>false</span>;
      <span>while</span> (idsPendingCommit.<span>has</span>(nextIdToCommit)) {
        idsPendingCommit.<span>delete</span>(nextIdToCommit);
        nextIdToCommit++;
        didChange = <span>true</span>;
      }
      <span>if</span> (didChange) {
        <span>await</span> <span>recordNextIdToResumeFrom</span>(nextIdToCommit);
      }
      flushing = <span>false</span>;
    };

    <span>const</span> <span>CONCURRENCY</span> = <span>cpus</span>().<span>length</span> * <span>16</span>;
    <span>await</span> <span>Promise</span>.<span>all</span>(
      <span>Array</span>.<span>from</span>({ <span>length</span>: <span>CONCURRENCY</span> }, <span>async</span> () =&gt; {
        <span>while</span> (nextId &lt;= maxId) {
          <span>const</span> id = nextId++;
          <span>await</span> pool.<span>execute</span>(<span>"process"</span>, id);
          idsPendingCommit.<span>add</span>(id);
          <span>maybeFlushId</span>();
        }
      }),
    );
  })
  .<span>go</span>();
</code></pre><p><a href="https://github.com/wilsonzlin/xtjs-lib/blob/master/WorkerPool.ts">WorkerPool</a> is a helper class I made to simplify using worker threads, by making it easy to make type-checked requests between the main thread and a pool of worker threads. The parallelism fetched things out-of-order, so for idempotency, I recorded the marker in order so I don't skip anything if it is interrupted.</p>
<p>Some interesting things I noticed about the HN items returned by the API:</p>
<ul>
<li>Scores never seem to be below -1.</li>
<li>You can't get the downvotes for posts, and the votes at all for comments.</li>
<li>Some posts and comments have blank titles and texts/URLs, despite not being flagged or deleted; I presume they were moderated.</li>
<li>It's possible for comment IDs to have be smaller than an ancestor, probably due to a moderator moving the comment tree.</li>
</ul>
<p>I've exported the HN crawler (in TypeScript) to its own <a href="https://github.com/wilsonzlin/crawler-toolkit-hn">project</a>, if you're ever in need to fetch HN items.</p>
<h2 id="generating-embeddings">Generating embeddings</h2>
<p>My initial theory was that the title alone would be enough to semantically represent a post, so I dove right in with just the data collected so far to generate some embeddings. The <a href="https://huggingface.co/spaces/mteb/leaderboard">Massive Text Embedding Benchmark (MTEB)</a> is a good place to compare the latest SOTA models, where I found <a href="https://huggingface.co/BAAI/bge-m3">BGE-M3</a>, the latest iteration of the popular <a href="https://huggingface.co/BAAI/bge-base-en-v1.5">FlagEmbedding</a> models that came out last year. Their v3 version supports generating "lexical weights" for basically free, which are essentially sparse bags-of-words, a map from token ID to weight, which you can use with an algorithm like BM25 in addition to the normal "dense" embeddings for hybrid retrieval. According to their <a href="https://arxiv.org/pdf/2402.03216">paper</a>, this increases retrieval performance significantly.</p>
<p>The infrastructure required for generating embeddings is not so trivial:</p>
<ul>
<li>Models are computationally expensive, with good ones having anything from millions to billions of parameters.</li>
<li>Like most AI models, they are much more efficient to compute on GPUs, but GPU clusters are expensive.</li>
<li>Inference can take hundreds of milliseconds, meaning a processing rate in the ballpark of <em>tens</em> per second. That's almost a <em>year</em> to process 40 million inputs on one GPU.</li>
<li>The GPUs are likely separate from our data and server machines. A way to ensure a flowing full pipe from our data to our GPUs will ensure our GPUs are not expensively idling.</li>
</ul>
<p>Fortunately, I discovered <a href="https://runpod.io/">RunPod</a>, a provider of machines with GPUs that you can deploy your containers onto, at a cost far cheaper than major cloud providers. They also have more cost-effective GPUs like RTX 4090, while still running in datacenters with fast Internet connections. This made scaling up a price-accessible option to mitigate the inference time required.</p>
<p>The GPUs were scattered around the world, so DB connection latency and connection overhead became a problem, and the decentralized client-side pooling caused a lot of server overhead. I created <a href="https://github.com/wilsonzlin/db-rpc">db-rpc</a> to mitigate these aspects. It's a simple service that proxies SQL queries over HTTP/2 to a local DB with a large shared connection pool. HTTP/2 supports multiplexing (= many queries, one connection, no blocking), lighter connections, and quicker handshakes, so for the simple queries I'm making (CRUD statements), it worked great.</p>
<p>A simple message queue was needed to distribute the item IDs to embed to the various GPU workers, which is exactly what AWS SQS offers. Unfortunately, it has quite low rate limits and is expensive per message, which is annoying given the millions of tiny job messages. Some batching can mitigate this partially, but I often need something like this, so I created <a href="https://github.com/wilsonzlin/queued">queued</a>, a RocksDB-based queue service written in Rust. It handles 100K+ op/s with one node, so there's no worrying about batching, message sizes, rate limits, and costs. RocksDB's write-optimized design makes for a great queue storage backing.</p>
<p>The optimizations paid off: after scaling to ~150 GPUs, all 40 million posts and comments were embedded in only a few hours. A snapshot from my Grafana dashboard indicates some of the impact:</p>
<p><img src="https://blog.wilsonl.in/hackerverse/embedder-grafana.png" alt="Grafana dashboard of a period during the embeddings process."></p>
<p>The GPU utilization stayed peak for the entire time, and the processing rate was stable. Connection latencies remained steady and low on average despite many distributed workers and concurrent queries, although there was not as much batching because it was quite expensive to embed each input (at around 600 ms <em>per input</em>), as previously mentioned.</p>
<h2 id="adding-additional-context-by-crawing-the-web">Adding additional context by crawing the web</h2>
<p>Unfortunately, my theory about the titles being enough did not pay off. While it worked well for most posts because they have descriptive titles, a lot have <a href="https://news.ycombinator.com/item?id=21686264">"strange"</a>, <a href="https://news.ycombinator.com/item?id=25771953">creative</a>, <a href="https://news.ycombinator.com/item?id=21379174">ambiguous</a> titles that don't play well with the embedding model. Also, the model tended to cluster <em>Ask HN</em> and <em>Show HN</em> posts together, regardless of topic, probably because, given that the entire input was just the title, those two phrases were significant. I needed more context to give to the model.</p>
<p>For text posts and comments, the answer is simple. However, for the vast majority of link posts, this would mean crawling those pages being linked to. So I wrote up a quick <a href="https://github.com/wilsonzlin/hackerverse/tree/master/crawler">Rust service</a> to fetch the URLs linked to and parse the HTML for metadata (title, picture, author, etc.) and text. This was CPU-intensive so an initial Node.js-based version was 10x slower and a Rust rewrite was worthwhile. Fortunately, other than that, it was mostly smooth and painless, likely because HN links are pretty good (responsive servers, non-pathological HTML, etc.).</p>
<p>Extracting text involved parsing the HTML using the excellent <a href="https://docs.rs/scraper/latest/scraper/">scraper</a>, removing <a href="https://github.com/wilsonzlin/hackerverse/blob/14fde395984a693e0d05c3bfc6f37bb2a7f7f549/crawler/parse.rs#L45">semantically non-primary HTML5 elements</a>, and <a href="https://github.com/wilsonzlin/hackerverse/blob/14fde395984a693e0d05c3bfc6f37bb2a7f7f549/crawler/parse.rs#L140">traversing the remaining tree</a>.</p>
<p>A <em>lot</em> of content even on Hacker News suffers from the well-known <a href="https://en.wikipedia.org/wiki/Link_rot">link rot</a>: around 200K resulted in a 404, DNS lookup failure, or connection timeout, which is a sizable "hole" in the dataset that would be nice to mend. Fortunately, the <a href="https://en.wikipedia.org/wiki/Internet_Archive">Internet Archive</a> has an <a href="https://archive.org/help/wayback_api.php">API</a> that we can use to use to programmatically fetch archived copies of these pages. So, as a final push for a more "complete" dataset, I used the Wayback API to fetch the last few thousands of articles, some dating back years, which was very annoying because IA has very, very low rate limits (around 5 per minute).</p>
<p>Our end tally of pages that could not be fetched:</p>
<p><img src="https://blog.wilsonl.in/hackerverse/fetch_err.webp" alt="Pie chart of fetch error counts."></p>
<p>Not bad, out of 4 million. That's less than 5% of all fetched pages.</p>
<h2 id="embeddings,-attempt-two">Embeddings, attempt two</h2>
<p>Web pages are long, but luckily the BGE-M3 model supports a context window of 8192 tokens. However, the model is really slow, as we saw previously, so I decided to switch to <a href="https://huggingface.co/jinaai/jina-embeddings-v2-small-en">jina-embeddings-v2-small-en</a> which has a far smaller parameter count, while still having good performance (according to MTEB). This saved a lot of time and money, as the inference time dropped to 6 ms (100x faster):</p>
<p><img src="https://blog.wilsonl.in/hackerverse/embedder-grafana-jinav2small.png" alt="Grafana dashboard of embedder using jina-embeddings-v2-small-en."></p>
<p>The GPUs could not actually be saturated, because any increase in batch size would cause OOMs due to the extended length of the inputs.</p>
<p>Some pages did not have a lot of text (e.g. more creative visual pages), or could not be fetched at all. To still ensure these posts still had decent context, I packed the top HN comments for those posts after the page text as extra "insurance":</p>
<pre><code><span>const</span> topComments = item.<span>kids</span>?.<span>slice</span>() ?? [];
<span>const</span> <span>MAX_LEN</span> = <span>1024</span> * <span>64</span>; <span>// 64 KiB.</span>
<span>while</span> (topComments.<span>length</span> &amp;&amp; embInput.<span>length</span> &lt; <span>MAX_LEN</span>) {
  <span>// Embellish with top-level top comments (`item.kids` are ranked already). This is useful if the page isn't primarily text, could not be fetched, etc.</span>
  <span>const</span> i = <span>await</span> <span>fetchItem</span>(topComments.<span>shift</span>()!);
  <span>// We don't want to include negative comments as part of the post's text representation.</span>
  <span>if</span> (!i || i.<span>type</span> !== <span>"comment"</span> || i.<span>dead</span> || i.<span>deleted</span> || i.<span>score</span>! &lt; <span>0</span>) {
    <span>continue</span>;
  }
  <span>const</span> text = <span>extractText</span>(i.<span>text</span> ?? <span>""</span>);
  <span>if</span> (!addedSep) {
    <span>// Use Markdown syntax, colon, and ASCII border to really emphasise separation.</span>
    embInput += <span>"\n\n# Comments:\n=========="</span>;
    addedSep = <span>true</span>;
  } <span>else</span> {
    embInput += <span>"\n\n----------"</span>;
  }
  embInput += <span>"\n\n"</span> + text;
}
embInput = embInput.<span>slice</span>(<span>0</span>, <span>MAX_LEN</span>);
</code></pre><p>For comments, many refer to parents or ancestors, so wouldn't make sense alone. Using a similar approach, I traversed comments' ancestors and built up a longer context with everything up to the post title. Now, the inputs for posts and comments were nice and full of context, hopefully corresponding to more accurate, useful embeddings.</p>
<p>One thing I did this time round was to create a <code>kv</code> table that could hold arbitrary keys and values, and store these large values (embeddings, texts, etc.) there. When stored alongside the row, they would make the row "fat" and updates to any column (even for a tiny value) were expensive. Schema changes were also expensive. These weren't worth the benefits of having these mostly-opaque values within a row, which were basically none.</p>
<h2 id="umap">UMAP</h2>
<p><a href="https://umap-learn.readthedocs.io/en/latest/index.html">Uniform Manifold Approximation and Projection (UMAP)</a> is a dimensionality reduction technique, which means to take our large 1024-dimension embedding vectors and turn them into points in fewer dimensional space <em>while</em> still (trying) to preserve most of the semantic relationships between points. <a href="https://scikit-learn.org/stable/modules/decomposition.html#pca">PCA</a> and <a href="https://scikit-learn.org/stable/modules/manifold.html#t-sne">t-SNE</a> are similar algorithms in this space you may have heard of, but UMAP is newer and <a href="https://umap-learn.readthedocs.io/en/latest/performance.html">makes the case</a> that it offers a better performance-accuracy trade-off than the others. Regardless of which algorithm, dim. reduction is often used to visualize embeddings, because it's hard to "see" points in 1024-dimensional space. We'll use UMAP to reduce our embeddings to 2D space, so we can scatter plot it and do some basic eyeballing for basic checks and anything interesting with our dataset.</p>
<p>Generating the 2D embeddings is not hard. UMAP takes in two main things: a PyNNDescent graph, and the original embeddings. There are <a href="https://umap-learn.readthedocs.io/en/latest/parameters.html">a few hyperparameters</a> that affect the distribution of points in the lower dim. space, as well as the primary parameter: the target dimensionality.</p>
<pre><code><span>import</span> umap

<span>with</span> <span>open</span>(<span>"ann.joblib"</span>, <span>"rb"</span>) <span>as</span> f:
    ann = joblib.load(f)
knn_indices, knn_dists = ann.neighbor_graph
mat_emb = np.memmap(<span>"emb.mat"</span>, dtype=np.float32, mode=<span>"r"</span>, shape=(N_ITEMS, <span>512</span>))

mapper = umap.UMAP(
    precomputed_knn=(knn_indices, knn_dists, ann),
    n_components=<span>2</span>,
    metric=<span>"cosine"</span>,
    n_neighbors=N_NEIGHBORS,
    min_dist=MIN_DIST,
)
mapper.fit(mat_emb)
<span># Save the lower dim. embeddings.</span>
mat_umap = mapper.embedding_
<span>with</span> <span>open</span>(<span>"emb_umap.mat"</span>, <span>"wb"</span>) <span>as</span> f:
    f.write(mat_umap.tobytes())
<span># Save the UMAP model for later use.</span>
<span>with</span> <span>open</span>(<span>"umap-model.joblib"</span>, <span>"wb"</span>) <span>as</span> f:
    joblib.dump(mapper, f)
</code></pre><p>Training is highly parallel and, with millions of high dim. inputs, can take a while, so I spun up a c7i.metal-48xl VM on EC2. After about an hour and a half, maxing out the 96-core processor, it was done, and an equivalent <code>(N_ITEMS, 2)</code> matrix was available. I saved both the 2D embeddings, as well as the trained model which can be later used to transform other embeddings without running the fitting process again.</p>
<p>Let's now plot these 2D embeddings and see what we find.</p>
<pre><code><span>import</span> matplotlib.pyplot <span>as</span> plt

mat_umap = load_umap()

plt.figure(figsize=(<span>10</span>, <span>10</span>))
plt.gca().invert_yaxis()
plt.scatter(mat_umap[:, <span>0</span>], mat_umap[:, <span>1</span>], s=<span>1</span>)

plt.savefig(<span>"umap.webp"</span>, dpi=<span>300</span>, bbox_inches=<span>"tight"</span>)
</code></pre><p><img src="https://blog.wilsonl.in/hackerverse/umap.webp" alt="Plot of all post UMAP embeddings."></p>
<p>Oops, too many points. Let's do a quick and easy way to reduce the points by tiling into a fine (but finite) grid and selecting only the highest-scoring post from each cell. Let's also add some titles so we see how those points relate:</p>
<pre><code><span>import</span> pyarrow

df = pyarrow.feather.read_feather(<span>"posts.arrow"</span>, memory_map=<span>True</span>)
df_titles = pyarrow.feather.read_feather(<span>"post_titles.arrow"</span>, memory_map=<span>True</span>)
df = df.merge(df_titles, on=<span>"id"</span>, how=<span>"inner"</span>)
df[<span>"x"</span>] = mat_umap[:, <span>0</span>]
df[<span>"y"</span>] = mat_umap[:, <span>1</span>]

x_range = df[<span>"x"</span>].<span>max</span>() - df[<span>"x"</span>].<span>min</span>()
y_range = df[<span>"y"</span>].<span>max</span>() - df[<span>"y"</span>].<span>min</span>()
grid_size = <span>100</span>
df[<span>"cell_x"</span>] = df[<span>"x"</span>] // (x_range / grid_size)
df[<span>"cell_y"</span>] = df[<span>"y"</span>] // (y_range / grid_size)
df = df.sort_values(<span>"score"</span>, ascending=<span>False</span>).drop_duplicates([<span>"cell_x"</span>, <span>"cell_y"</span>])

plt.figure(figsize=(<span>50</span>, <span>50</span>))
plt.gca().invert_yaxis()
plt.scatter(df[<span>"x"</span>], df[<span>"y"</span>], s=<span>1</span>)
<span>for</span> i, row <span>in</span> df.sample(n=<span>1000</span>).iterrows():
    plt.annotate(row[<span>"text"</span>], (row[<span>"x"</span>], row[<span>"y"</span>]), fontsize=<span>3</span>)
</code></pre><p>It's a bit hard to see (there are still lots of points and text labels, so the image is very high resolution), but if you open the image and zoom in, you can see the 2D space and the semantic relationships between points a bit more clearly:</p>
<p><img src="https://blog.wilsonl.in/hackerverse/umap-labelled.webp" alt="Plot of some post UMAP embeddings, with some labelled."></p>
<p>It also looks like adding more context paid off, as those previously-mentioned posts with "exotic" titles are now placed in more accurate points near related content.</p>
<h2 id="cosine-similiarity">Cosine similiarity</h2>
<p>All the data is now ready. A lot of using embeddings involves finding the similarity between them. This is basically the inverse of the distance: if something is far away (high distance), it's not similar (low similarity), and vice versa. From school, we may think of one way to measure the distance between two points:</p>
<pre><code>x1, y1 = (<span>2</span>, <span>3</span>)
x2, y2 = (<span>4</span>, <span>5</span>)
dist = math.sqrt((x2 - x1)**<span>2</span> + (y2 - y1)**<span>2</span>)
</code></pre><p>Most of us knows the formula as the <a href="https://en.wikipedia.org/wiki/Pythagorean_theorem">Pythagorean theorem</a>, and this way of calculating distance is known as the <a href="https://en.wikipedia.org/wiki/Euclidean_distance">Euclidean distance</a>, which feels intuitive to us. However, it's not the only way of calculating distance, and it's not the one commonly used for text embeddings.</p>
<p>You may have seen the cosine metric used a lot w.r.t. embeddings and semantic similarity. You might be wondering why the euclidean distance isn't used instead, given how more "normal" it seems in our world. Here's an example to demonstrate why:</p>
<p><img src="https://blog.wilsonl.in/hackerverse/cosine-vs-euclid.webp" alt="Cosine vs. euclid distance metrics."></p>
<p>We can see that in this case, where perhaps the X axis represents "more cat" and Y axis "more dog", using the euclidean distance (i.e. physical distance length), a pitbull is somehow more similar to a Siamese cat than a "dog", whereas intuitively we'd expect the opposite. The fact that a pitbull is "very dog" somehow makes it closer to a "very cat". Instead, if we take the angle distance between lines (i.e. cosine distance, or 1 minus angle), the world makes sense again.</p>
<p>In summary, cosine distances are useful when magitude does not matter (as much), and when using the magnitude would be misleading. This is often the case with text content, where an intense long discussion about X should be similar to X and not an intense long discussion about Y. A good blog post on this is <a href="https://cmry.github.io/notes/euclidean-v-cosine">Euclidean vs. Cosine Distance</a> by Chris Emmery.</p>
<p>Going forward, we'll use the cosine distance/metric/similarity a lot. The core of it is:</p>
<pre><code>dist = corpus_embeddings @ query_embeddings.T
sim = <span>1</span> - dist
</code></pre><p>The <code>@</code> operator performs a dot product. Normally, you then divide by the product of their magnitudes, but in this case, they are unit vectors, so that isn't needed.</p>
<h2 id="building-a-map">Building a map</h2>
<p>Wouldn't it be cool to have an interactive visualization of the latent space of all these embeddings, a "map of the Hacker News universe" so to speak? Something like Google Maps, where you can pan and zoom and move around, with terrain and landmarks and coordinates? It sounds like a fun and challenging thing to try and do!</p>
<p>I first scoped out how this map should work:</p>
<ul>
<li>Zooming in (via pinching or the mousewheel) should increase the amount of points shown, and points should get farther apart.</li>
<li>Some points should be labelled, but not all because it would get cluttered and text would overlap.</li>
<li>Clicking a point should show more details about that post.</li>
<li>It should be in the browser, as a web app, that works well on mobile and desktop and touch and mouse.</li>
</ul>
<p>Basically, it should work like you'd expect, like Google Maps.</p>
<h2 id="preparing-the-map-data">Preparing the map data</h2>
<p>There are millions of points, not ideal to send all at once to the client. Working backwards, there are two "axes" that dictate the points shown: position and zoom, so structuring and segmenting the data alongst those lines is a good start.</p>
<p>The first thing that came to mind was tiling: divide the map space into a grid, then pack all points into each cell. The client can load tiles on-demand as they need to, instead of the whole map. There are probably more sophisticated ways of tiling, esp. given that points aren't distributed evenly and clients have different viewports, but the effectiveness is great given its simplicity. A tile can simply be referred to by its (x, y) coordinate, and stored in any KV datastore (e.g. S3), making it easy to distribute and requiring no server-side logic.</p>
<p>For zooming, my approach was a loop for each Level of Detail (LOD), subdividing into 2x more grid cells per axis (and therefore 4x more points), but making sure to first copy over all points picked by the previous level, because the user doesn't expect a point to disappear as they zoom in.</p>
<p>I aimed for each tile to be less than 20 KiB when packed, a balance between fast to load on mediocre Internet and sizable to avoid excessive fetches. This limited tiles to around 1,500 points: 4+4 bytes for the (x, y), 4 bytes for the ID, and 2 bytes for the score.</p>
<p>To ensure a diverse set of points per tile, I further subdivided each tile to pick at least something from each area; this reduced the chance that a less-populated area in the fringes seems completely non-existent on the map from afar. Any remaining capacity was taken by sampling uniformly, which should visually represent the background distribution on the map.</p>
<p>The code is pretty straightforward and self-explanatory in the <a href="https://github.com/wilsonzlin/hackerverse/blob/master/build-map/main.py">build-map</a> service, so I won't repeat the code here.</p>
<h2 id="building-the-web-app">Building the web app</h2>
<p>It took a while to figure out the best approach. Using even thousands of DOM elements (one for each point) completely trashed performance, so that was out of the picture. Having a giant Canvas, dynamically rendering points only as they come into view, and changing its DOM position and scale when panning/zooming, didn't work either; the points would grow in size when zooming in, and at sufficiently large zoom levels, the image became too big (clearing the Canvas took too long, and memory usage was extreme). Eventually I settled on a Canvas and drawing on every update of the "viewport", which represents the position and zoom level of the current user. Despite needing to redraw potentially thousands of points on every frame (to feel smooth and responsive), this worked really well, and was the simplest approach.</p>
<p>I kept the labelling algorithm simple: repeatedly pick the highest scoring post (a reasonable metric), unless the label would intersect with another existing label. <a href="https://en.wikipedia.org/wiki/R-tree">R-trees</a> can do the job of finding box collisions well, and thankfully <a href="https://github.com/mourner/rbush">RBush</a> exists, an excellent and fast R-tree implementation in JS. There exists a <a href="https://developer.mozilla.org/en-US/docs/Web/API/CanvasRenderingContext2D/measureText">measureText()</a> API in the browser, but fetching thousands (or more) titles just to measure them and pick a handful was neither fast nor efficient, so I packed post title lengths into a byte array (so a thousand would only be ~1 KB) and simply used a tuned formula to approximate the length, which worked reasonably well:</p>
<p><img src="https://blog.wilsonl.in/hackerverse/map-bboxes.png" alt="Map of posts with bounding boxes around labels."></p>
<p>The one-off initial calculations of these boxes and collisions was CPU intensive and caused stuttering, so I moved it to its own thread using <a href="https://developer.mozilla.org/en-US/docs/Web/API/Web_Workers_API/Using_web_workers">Web Workers</a>. I also experimented with <a href="https://developer.mozilla.org/en-US/docs/Web/API/OffscreenCanvas">OffscreenCanvas</a>, but it didn't do much; the render logic was very efficient already, and given that 99% of the app was the map (represented by the Canvas), having "the rest of the app" still respond while the map was rendering was not very useful.</p>
<h2 id="adding-some-visual-appeal-and-guidance">Adding some visual appeal and guidance</h2>
<p>There was something still disorienting and "dull" about the map. Real maps have landmarks, cities, borders, terrains, and colors; there is a sense of direction, orientation, and navigation as you browse and navigate the map. Let's try to add some of those, analogizing where necessary.</p>
<p>Terrain and borders will require some analogizing, since there are no real geographical or geopolitical features of this map. If you look on Google Maps, you'll notice that there are shades of terrain, but no smooth gradients. They are contours, quickly informing the viewer of the intensity of something compared to everywhere else, in their case vegetation. For our map, intensity could represent density of points, quickly signifying where there's a lot of interest, activity, content, engagement, popularity, discussion, etc. It would also provide some of the orientation, because a user can sense their movement based on the shifting terrain.</p>
<p><a href="https://scikit-learn.org/stable/modules/density.html">Kernel Density Estimation</a> would seem like the perfect tool for this job. Basically, you take a bunch of discrete values and generate a smooth curve around it, inferring the underlying distribution. Matthew Conlen <a href="https://mathisonian.github.io/kde/">created a cool visual walkthrough</a> explaining intuitively in more detail. Unfortunately, attempts using standard libraries like <a href="https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.gaussian_kde.html">SciPy</a> took too long. However, as I was playing around with them, something occurred to me when I saw "Gaussian kernel" (one option for KDE): why not try Gaussian blurring? They seem to be related: when you blur an image, you have a <a href="https://en.wikipedia.org/wiki/Kernel_(image_processing)">kernel</a> and apply a Gaussian function, which when you look at <a href="https://en.wikipedia.org/wiki/File:Normal_Distribution_PDF.svg">its plot</a>, it essentially "pushes out" values from the kernel's center (in layman's terms) — this is how you get the <em>smoothing</em> effect from blurs. Would applying such a blur also "push out" and smoothing the many discrete sharp points into a meaningful representation of the approximate density?</p>
<p>My approach will be to map each point to a cell in a large grid with the same aspect ratio, where each cell's value will be the count of points mapped to it. If the grid is too small, everything's clumped together, but if it's too big, everything's too far apart and it's mostly sparse everywhere, so finding a balance is necessary.</p>
<pre><code>ppc = <span>32</span> <span># Points per cell (approximate).</span>
x_min, x_max = (xs.<span>min</span>(), xs.<span>max</span>())
y_min, y_max = (ys.<span>min</span>(), ys.<span>max</span>())
grid_width = <span>int</span>((x_max - x_min) * ppc)
grid_height = <span>int</span>((y_max - y_min) * ppc)

gv = pd.DataFrame({
    <span>"x"</span>: (xs - x_min).clip(<span>0</span>, grid_width - <span>1</span>).astype(<span>"int32"</span>),
    <span>"y"</span>: (ys - y_min).clip(<span>0</span>, grid_height - <span>1</span>).astype(<span>"int32"</span>),
})
gv = gv.groupby([<span>"x"</span>, <span>"y"</span>]).size().reset_index(name=<span>"density"</span>)

grid = np.zeros((grid_height, grid_width), dtype=np.float32)
grid[gv[<span>"y"</span>], gv[<span>"x"</span>]] = gv[<span>"density"</span>]
grid = gaussian_filter(grid, sigma=<span>1</span>)

g_min, g_max = grid.<span>min</span>(), grid.<span>max</span>()
level_size = (g_max - g_min) / levels
grid = (grid - g_min) // level_size
<span># Some values may lie exactly on the max.</span>
grid = np.clip(grid, <span>0</span>, levels - <span>1</span>)
</code></pre><p>Let's quickly render an image, using the values as the alpha component, to see how it looks so far.</p>
<pre><code>alpha = grid.astype(np.float32) / (levels - <span>1</span>) * <span>255</span>

img = np.full(
    (grid_height, grid_width, <span>4</span>),
    (<span>144</span>, <span>224</span>, <span>190</span>, <span>0</span>), <span># Green-ish.</span>
    dtype=np.uint8,
)
img[:, :, <span>3</span>] = alpha.astype(np.uint8)

Image.fromarray(img, <span>"RGBA"</span>).save(<span>"terrain.webp"</span>)
</code></pre><p><img src="https://blog.wilsonl.in/hackerverse/terrain-linear.webp" alt="Initial terrain map."></p>
<p>It does not look so good. It looks like most cells were zero, and there are seemingly very few areas with any actual posts.</p>
<p>I have a theory: just as the distribution of votes/likes on social media across posts exhibit power-law distributions, perhaps the same is true of the popularity of topics? If there were a few topics that are posted about 10x more than most others, then it could explain the above map, as few areas would fall in the middle tiers. Let's apply a <code>log</code> to the values:</p>
<pre><code>gv[<span>"density"</span>] = np.log(gv[<span>"density"</span>] + <span>1</span>)
</code></pre><p><img src="https://blog.wilsonl.in/hackerverse/terrain-logarithmic.webp" alt="Logarithmic terrain map."></p>
<p>That looks much nicer. It also has implicit "borders" at the places where different <em>log(density)</em> levels meet, which is cool.</p>
<p>Instead of rendering it as a giant image, which is inefficient to transport and blurry when zoomed in, I'll create SVG paths instead, given that there are literally only 4 colors in this entire figure. On the client, I'll draw and fill in these paths that form a polygon. This will ensure that the "terrain" looks sharp (including at borders) even when zoomed in. I'll use <a href="https://opencv.org/">OpenCV</a>'s built-in <a href="https://en.wikipedia.org/wiki/Contour_line">contour</a> functions to calculate the path around these levels and export them as a closed polygon.</p>
<pre><code>shapes: <span>Dict</span>[<span>int</span>, <span>List</span>[npt.NDArray[np.float32]]] = {}
<span>for</span> level <span>in</span> <span>range</span>(levels):
    shapes[level] = []
    num_shapes, labelled_image = cv2.connectedComponents((grid == level).astype(np.uint8))
    <span># Ignore label 0 as it's the background.</span>
    <span>for</span> shape_no <span>in</span> <span>range</span>(<span>1</span>, num_shapes):
        shape_mask = labelled_image == shape_no
        <span># Use RETR_EXTERNAL as we only want the outer edges, and don't care about inner holes since they'll be represented by other larger-level shapes.</span>
        shape_contours, _ = cv2.findContours(shape_mask.astype(np.uint8), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
        <span>for</span> shape_border_points <span>in</span> shape_contours:
            <span># The resulting shape is (N, 1, 2), where N is the number of points. Remove unnecessary second dimension.</span>
            shape_border_points = shape_border_points.squeeze(<span>1</span>)
            <span>if</span> shape_border_points.shape[<span>0</span>] &lt; <span>4</span>:
                <span># Not a polygon.</span>
                <span>continue</span>
            <span># We want level 0 only when it cuts out an inner hole in a larger level.</span>
            <span>if</span> level == <span>0</span> <span>and</span> (<span>0</span>, <span>0</span>) <span>in</span> shape_border_points:
                <span>continue</span>

            <span># Convert back to original scale.</span>
            shape_border_points[:, <span>0</span>] = shape_border_points[:, <span>0</span>] / ppc + x_min
            shape_border_points[:, <span>1</span>] = shape_border_points[:, <span>1</span>] / ppc + y_min
            shapes[level].append(shape_border_points.astype(np.float32))
</code></pre><h2 id="cities">Cities</h2>
<p>I also wanted to add some "cities" (representing the common topic within some radius), so you can find your way around and not get lost in the many points and titles shown at once, and have some sense of direction and where to start. The UMAP model has been saved, so all that's necessary is to embed the city names and get their (x, y) position using the UMAP model:</p>
<pre><code>CITIES = [<span>"Programming"</span>, <span>"Startups"</span>, <span>"Marketing"</span>, ...]
embs = model.encode(CITIES)
points = umapper.transform(embs)
</code></pre><p>There were some automated ways of doing this that I explored a bit. Using LLMs to generate these automatically. K-means clustering to figure out optimal points and radii. Unfortunately, it was not so trivial: it was hard to prompt the LLM to output what I expected, likely because describing the task was not trivial, and K-means did not find a lot of meaningful clusters that I would (as a human labeller) group together. You'd expect that there would be some hierarchy, but some topics are really popular on their own (sometimes even more than their logical "parent"), like "Programming" vs. "Rust", so they needed to be shown at the same detail level. Ultimately, it only required a few cities before it looked good, so manually walking the map and jotting down a few cities only took an hour or so.</p>
<h2 id="pushing-things-to-the-edge">Pushing things to the edge</h2>
<p>As you're browsing the map, you want it to feel snappy and responsive, because you're trying to find something, get immersed, get orientated, etc., and having parts be blank or partial interrupts this flow. Therefore, I knew I needed to reduce the time it took to get the map onto the screen. The rendering part was fast, it was fetching the data that took a while; I had started by putting all the map data on Cloudflare R2 in the ENAM region, but the latency was too high (600 ms to a few seconds) despite the physical latency being ~200 ms. However, even 200 ms was not really great, given that <a href="https://www.pubnub.com/blog/how-fast-is-realtime-human-perception-and-technology/">100 ms is the treshold where things feel "instant"</a>. Given that the limitation was a law of physics, there was only one real way to reduce that latency and that was to move the data closer to the user. So I spun up a few tiny servers in major regions: Virginia, San Jose, London, and Sydney (near me). I wrote a basic Rust server to ship out the data and get the most bang-for-buck from these tiny servers (plus, why spend all this effort only to have a slow server?), and had some tiny JS to pick the nearest server from the client:</p>
<pre><code><span>const</span> <span>EDGES</span> = [
  <span>"ap-sydney-1"</span>,
  <span>"uk-london-1"</span>,
  <span>"us-ashburn-1"</span>,
  <span>"us-sanjose-1"</span>,
] <span>as</span> <span>const</span>;

<span>const</span> edge = <span>await</span> <span>Promise</span>.<span>race</span>(
  <span>EDGES</span>.<span>map</span>(<span>async</span> (edge) =&gt; {
    <span>// Run a few times to avoid potential cold start biases.</span>
    <span>for</span> (<span>let</span> i = <span>0</span>; i &lt; <span>3</span>; i++) {
      <span>await</span> <span>fetch</span>(<span>`https://<span>${edge}</span>.edge-hndr.wilsonl.in/healthz`</span>);
    }
    <span>return</span> edge;
  }),
);
</code></pre><p>Some anycast, CDN, etc. solution may have been even cooler, but likely costly and overkill.</p>
<p>One thing that puzzled me was how much more memory was being used by the process compared to the actual data itself. The data is built once then pushed to all the edge servers, and it's in MessagePack format, so it has the bloat from type markers and property names. Once deserialized into Rust structures, it should all be memory offsets. So I was surprised when memory usage was 2-4x the source data size. I could only really think of four reasons:</p>
<ul>
<li>I used the wrong type (e.g. <code>f64</code> instead of <code>f32</code>).</li>
<li>Struct padding.</li>
<li>Vec, HashMap overallocation.</li>
<li>Memory allocator fragmentation or other inefficiency.</li>
</ul>
<p>I didn't look too much into this, but it was the one thing left that otherwise made the edge setup pretty neat and efficient. If anyone has any ideas, let me know.</p>
<h2 id="testing-out-the-search">Testing out the search</h2>
<p>Now that we got our app and data all up and running, let's see if our theory about better query comprehension and search results pans out. Let's try a simple query: "entering the tech industry".</p>
<p><img src="https://blog.wilsonl.in/hackerverse/search-entering-the-tech-industry.png" alt="Search results for &quot;entering the tech industry&quot;"></p>
<p>It gives some nice results, both upvoted and less noticed ones, and seem to be pretty relevant and helpful. Compare this to HN's current search service:</p>
<p><img src="https://blog.wilsonl.in/hackerverse/search-algolia-entering-the-tech-industry.png" alt="Search results for &quot;entering the tech industry&quot; by Algolia"></p>
<p>We can see the power of semantic embeddings over something like literal text matching. Of course, we could try more queries and get better results, but the embeddings-powered search engine got it immediately, did not return only part of the results, and there was no need to optimize the query or "reverse engineer" the algorithm.</p>
<p>How about a question, instead of just matching?</p>
<p><img src="https://blog.wilsonl.in/hackerverse/search-what-happened-to-wework.png" alt="Search results for &quot;what happened to wework&quot;"></p>
<p>It gave us results about WeWork over the years, from layoffs to stock tank to bankruptcy, a nice holistic view. Notice that the results don't literally contain the words "what happened", and most aren't even questions. The model seems to have "understood" our query, which is pretty nice considering this isn't some generative model, just cosine distances.</p>
<p>There is one quirk: the bottom result is completely irrelevant. This is largely my fault; I did not filter out results that are too dissimilar, so it ended up including generic results. This is something trivially fixable though.</p>
<p>Given the curated quality of HN posts (and scores), can we find some sage advice over the years about something important, like say career growth, just by typing something simple?</p>
<p><img src="https://blog.wilsonl.in/hackerverse/search-career-growth.png" alt="Search results for &quot;career growth&quot;"></p>
<p>Seems pretty good to me: not literally matching the words so creative, interesting, diverse essays showed up. The curation of HN really helps here; I did a quick comparison to the results from a well-known search engine and these results seemed far better. Obviously they have a much more difficult job and far more sophisticated pipelines, but it goes to show that good data + powerful semantic embeddings can go pretty far.</p>
<p>I've hard-coded some interesting queries I found into the app, which show up as suggestions; everything from "linus rants" to "self bootstrapping" to "cool things with css". Try them out and any other queries. What works well? What doesn't? Did you find anything interesting, rare, and/or useful? Let me know!</p>
<p>One thing I haven't mentioned yet is that the results are not <em>directly</em> the similarity matches. There are weights involved in calculating the final score (i.e. rank), and cosine similarity is a big one but not the only one.</p>
<p>Another important one is score. This may not seem as necessary given how powerful these embedding models are, but it is, even with 100% accurate models. Consider two posts talking about some topic, say Rust. One is written by <a href="https://news.ycombinator.com/user?id=steveklabnik">steveklabnik</a>. Another one says that Rust is terrible because it's a garbage-collected dynamic scripting language. To the model, these are both (mostly) talking about Rust, so they are very close together. Yet, it's obvious to most people that the latter should not be ranked as high as the former. (It should probably be filtered out entirely.) This highlights the importance of <em>social proof</em> in search and recommendation systems, because there are things that the model doesn't understand (and maybe never can) because of context, trends, events, etc. Incorporating the score ensures some social proof is taken into consideration.</p>
<p>Another weight is time. Some queries usually prefer newer content to older ones. The common example is news about some recent event; usually it's more important to show the latest updates than yesterday's or some distant but similar event. We can incorporate this by adding a negative weight component proportional to <em>log(age)</em>, so that non-fresh content quickly drops off.</p>
<h2 id="automatic-virtual-subcommunities">Automatic virtual subcommunities</h2>
<p>Another neat feature enabled by these embeddings is "virtual" subcommunities. Just type a community name (or description) and all the posts that meet some similarity threshold show up, like your very own subreddit on the fly. Hacker News doesn't have the ability to further subdivide posts, so this was a cool way to have a curated set of posts focused on a specific interest.</p>
<p><img src="https://blog.wilsonl.in/hackerverse/community.png" alt="Apple virtual subcommunity."></p>
<p>If you're wondering where those snippets and images came from, that's what the crawler was extracting and storing as metadata for each page. It usually comes in handy down the line when you want to show/list the pages as results somewhere, as otherwise you'd only have a URL. While I could've also saved the <a href="https://developer.mozilla.org/en-US/docs/Web/HTML/Element/link">site icon metadata</a>, it's tricky to parse, so I decided to keep it simple by just fetching <code>/favicon.ico</code> of the domain on the client side.</p>
<p>It's just as possible to show interesting comment threads and discussions. Unfortunately, scores aren't available to us, so we can only sort by timestamp, but luckily most comments on HN are pretty useful and insightful:</p>
<p><img src="https://blog.wilsonl.in/hackerverse/community-comments.png" alt="Entrepreneurship virtual community comments."></p>
<p>It would be an interesting challenge to try and rank the comments without the score, perhaps involving user comment histories, engagement around that comment, and the post, topic, and contents. This may be something as simple as a linear equation, to a deep learning model.</p>
<p>We can also see who are the most influential, active, passionate about something by calculating how many comments a user makes proportional to the similarity.</p>
<p><img src="https://blog.wilsonl.in/hackerverse/community-cloudflare-users.png" alt="Cloudflare virtual community top users."></p>
<p>We can see that this works well: <a href="https://news.ycombinator.com/user?id=jgrahamc">jgrahamc</a> and <a href="https://news.ycombinator.com/user?id=eastdakota">eastdakota</a> are the CTO and CEO of Cloudflare respectively. We managed to do this without needing to classify each comment or use a more fragile and inaccurate keyword-based search. All it takes is some matrix operations:</p>
<pre><code>q = model.embed(<span>"cloudflare"</span>)
df_comments[<span>"sim"</span>] = mat_comments @ q
min_threshold = <span>0.8</span>
df_comments_relevant = df_comments[df[<span>"sim"</span>] &gt;= min_threshold]
df_scores = df_comments_relevant.groupby(<span>"author"</span>).agg({<span>"sim"</span>: <span>"sum"</span>}).reset_index().sort_by(<span>"sim"</span>, ascending=<span>False</span>)[:<span>20</span>]
</code></pre><p>One important realisation was that pre-filtering is really slow and usually unnecessary compared to post-filtering. By pre-filtering, I mean removing rows before doing similarity matching. This is because you end up needing to also remove those corresponding rows from the embedding matrix, and that can mean having to reconstruct (read: gigabytes of memory copying) the entire matrix or use much slower partially-vectorized computations. It's usually better to just filter the rows after finding the most similar rows i.e. post-filter.</p>
<p>Note that a minimum threshold is important, because "dissimilarity" can be as high as 0.6, which makes the set of non-relevant items (in this case, users) have high scores due to the size of such sets. In this example, <em>most</em> comments aren't talking about Cloudflare, so any user that has a lot of comments would otherwise dominate this leaderboard just by sheer volume of comments as 100,000 * 0.6 is still higher than 500 * 0.999.</p>
<h2 id="analyzing-the-entire-dataset">Analyzing the entire dataset</h2>
<p>What can we do with the 30 million comments? Two things I wanted to try to analyze at scale were popularity and sentiment. Could I see how HN feels about something over time, and the impact that major events has on the sentiment? Can I track the growth and fall of various interests and topics, and how they compare against their competition?</p>
<p>I don't have sentiment data, but there are lots of high-quality open source sentiment classification models, on HuggingFace, using <a href="https://huggingface.co/docs/transformers/en/index">Transformers</a>. I decided to use the <a href="https://huggingface.co/cardiffnlp/twitter-roberta-base-sentiment-latest">TweetEval</a> model as it was similarly trained on social media content. Tweets are short, and I didn't know how well it'd work on contextualized comments (which I added when generating embeddings), so to keep it aligned with the model I only used the comments themselves without adding any context. A queue was created, the comments were pushed, a GPU cluster spun up to process the jobs, and the results were stored.</p>
<p>The model was much smaller, so increasing the batch size was a good idea to get more bang-for-buck from the GPUs. Increasing the batch size uses more VRAM, but decreases the amount of host-GPU memory transfer (which can be the bottleneck given how fast GPUs are) and possibly increases parallelism. It's finicky though, because, at least for transformer-based models, it can sometimes cause memory spikes and OOM errors. This is because the input matrix has to be a "rectangle", so all inputs (which are converted into tokens) must be padded to the longest input length to maintain this constraint. If you have a batch where 4 texts are length 1, then the input size and internal state is relatively small. But if you instead have a batch size of 5 and the next text has length 1024, then all those sizes suddenly jump by thousands. I added some basic code to conservatively guess the optimal batch size given the VRAM, but I'm curious if this problem has already been tackled more dynamically, given its implications for efficiency.</p>
<p>Once the data was ready, it was time for some number crunching. Let's check out the sentiment of Rust over time:</p>
<p><img src="https://blog.wilsonl.in/hackerverse/sentiment-rust.png" alt="Sentiment of Rust."></p>
<p>Values below 0 represent the count of negative comments (where confidence of negative sentiment &gt; 0.5) and above 0 represent positive (where confidence of positive sentiment &gt; 0.5); I probably need to polish and clear it up a bit more. Nonetheless, we can see that there's generally a lot of positive sentiment about Rust (which isn't surprising if you've been around on HN). There was a spike in positivity around the 1.0 announcement, which makes sense, and the more negative posts correlated with a lot of negative comments (according to the model). This is similar to how bots measure sentiment on social media and predict the price of stocks; using powerful semantic embeddings would probably beat any keyword- or bag-of-words-based algorithm. I will say, assuming the model is accurate and I did a reasonable job, there seems to be a lot of negative sentiment on HN <em>in general</em>.</p>
<p>We can also estimate the popularity of Rust compared to other languages by weighing the score and similarity. Unfortunately, HN does not expose comment scores, so we can't use them.</p>
<p><img src="https://blog.wilsonl.in/hackerverse/popularity-languages.png" alt="Popularity of Go, JavaScript, Python, and Rust."></p>
<p>It seems like Rust is doing great, but not as popular as the other languages. Some of the similarity thresholds may need tuning, so I may be wrong here; have a play with it yourself and try various queries and thresholds. Share anything interesting you find!</p>
<p>These were very basic demos and analyses of the data available, and I'm sure there are infinitely more ways to slice and dice the data in interesting, insightful, useful, sophisticated ways. I have many more ideas myself, but wanted to open up the code and data sooner so you can build on top of this, either with more ideas and suggestions, or to play with your own research and visualization projects.</p>
<h2 id="big-data-number-crunching-with-a-gpu">Big data number crunching with a GPU</h2>
<p>One last thing before I wrap this long post up. The analysis queries were taking a while (10-30 seconds) to number-crunch for each one, which was annoying when playing around with it. This was on a 32-core machine, so it was not for a lack of horsepower. I was thinking of various ways to index, preprocess, or prepare the data, when it occurred to me that there already exists a powerful device for lots of vectorized number-crunching, and it's why we run all our AI models on it, but it doesn't have to be restricted to those. Fortunately, libraries like <a href="https://cupy.dev/">CuPy</a> and <a href="https://docs.rapids.ai/api/cudf/stable/">cuDF</a> exist, which basically have the same API as NumPy and pandas (respectively) but run everything on the GPU, so it was pretty trivial to port over. Now, queries run in hundreds of milliseconds, and life is great. It's so fast I didn't even bother using a built ANN graph.</p>
<p>The only tricky thing was loading the data on the GPU. Given how large the matrix of embeddings was (30M x 512), it was critical to manage memory effectively, because it wasn't actually possible to fit anything more than 1x the matrix in either system or video memory. Some key points:</p>
<ul>
<li>Loading in batches can cause a lot of allocations, which can fragment memory, so in reality you may not be able to load in chunks and then concatenate at the end. (Concatenation also requires contiguous memory, which usually means copying into a separate memory location.)</li>
<li>If you read the bytes from disk, load into a NumPy array, convert into a CuPy array, and then copy over to the GPU, that's 4 copies, 3 of which are in memory.</li>
<li>CuPy seems to need to have the entire matrix in system memory first before it can copy over to the GPU. For example, <code>cupy.asarray(np_matrix)</code> actually creates a copy of <code>np_matrix</code> in system memory first.</li>
</ul>
<p>Ultimately I ended up memory-mapping the matrix on disk, preallocating an uninitialized matrix on the GPU of the same size, then copying over in chunks. This had the benefit of avoiding reading from disk into Python memory first, and using exactly 1x the system RAM and VRAM.</p>
<h2 id="demo">Demo</h2>
<p>You can find the app at <a href="https://hn.wilsonl.in/">hn.wilsonl.in</a>. The main page is the map and search, but you can find the other tools (communities and analysis) by clicking the button in the top right. If you find an interesting community, analysis, etc., feel free to share the URL with others; the query is always stored in the URL.</p>
<p>Note that the demo dataset is cut off at around 10 April 2024, so it contains recent but not live posts and comments.</p>
<h2 id="what's-next">What's next</h2>
<p>There is much more I wanted to explore, learn, build, but I did not get the time. Some ideas I'm thinking of going into:</p>
<ul>
<li>Live data that is continuously kept up to date.</li>
<li>Deep learning powered recommendations system, a <a href="https://en.wikipedia.org/wiki/StumbleUpon">StumbleUpon</a> over the curated HN web.</li>
<li>Improving search results by training a reranker.</li>
<li>Interesting "paths" and "journeys" along the map.</li>
<li>Analyzing users more: who are the most similar/opposite to each other? Who is the most expert in various niches?</li>
<li>…</li>
</ul>
<p>However, I'm more interested in hearing from the community. What do you want to see? How useful were these tools? What were shortcomings or things I overlooked? What other cool ideas do you have? Share any thoughts, feedback, interesting findings, complaints—there's likely a lot more potential with these data and tools, and I'm hoping that, by opening it up, there's some interested people who will push this further than I can alone.</p>
<p>If there's any interest in diving deeper or clarifying any aspect of this project, let me know, I'd be happy to. Once again, you can find all the <a href="https://github.com/wilsonzlin/hackerverse/releases/tag/dataset-39996091">data</a> and <a href="https://github.com/wilsonzlin/hackerverse">code</a> on GitHub.</p>
<p>If you managed to make it all the way here, thanks for reading!</p>

    </article>
  </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Show HN: An SQS Alternative on Postgres (165 pts)]]></title>
            <link>https://github.com/tembo-io/pgmq</link>
            <guid>40307454</guid>
            <pubDate>Thu, 09 May 2024 12:21:53 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/tembo-io/pgmq">https://github.com/tembo-io/pgmq</a>, See on <a href="https://news.ycombinator.com/item?id=40307454">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><p dir="auto"><h2 tabindex="-1" dir="auto">Postgres Message Queue (PGMQ)</h2><a id="user-content-postgres-message-queue-pgmq" aria-label="Permalink: Postgres Message Queue (PGMQ)" href="#postgres-message-queue-pgmq"></a></p>
<p dir="auto">A lightweight message queue. Like <a href="https://aws.amazon.com/sqs/" rel="nofollow">AWS SQS</a> and <a href="https://github.com/smrchy/rsmq">RSMQ</a> but on Postgres.</p>
<p dir="auto">Try it for free at <a href="https://tembo.io/" rel="nofollow">tembo.io</a></p>
<p dir="auto"><a href="https://join.slack.com/t/tembocommunity/shared_invite/zt-293gc1k0k-3K8z~eKW1SEIfrqEI~5_yw" rel="nofollow"><img src="https://camo.githubusercontent.com/503899b9c81b00cb3eb72ac40b042daad8fcd5aec105d593c763646fdb04bc5b/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f25343074656d626f2d636f6d6d756e6974793f6c6f676f3d736c61636b266c6162656c3d736c61636b" alt="Static Badge" data-canonical-src="https://img.shields.io/badge/%40tembo-community?logo=slack&amp;label=slack"></a>
<a href="https://ossrank.com/p/3809" rel="nofollow"><img src="https://camo.githubusercontent.com/ada21af437440bd8ccb611d66b4671e5f16e289b37ce4de49128bf9abf64ddf3/68747470733a2f2f736869656c64732e696f2f656e64706f696e743f75726c3d68747470733a2f2f6f737372616e6b2e636f6d2f736869656c642f33383039" alt="OSSRank" data-canonical-src="https://shields.io/endpoint?url=https://ossrank.com/shield/3809"></a>
<a href="https://pgxn.org/dist/pgmq/" rel="nofollow"><img src="https://camo.githubusercontent.com/10f6f5d6df8fa4673d210d4336688f8247718d022fcc6f9bb846639e36c26119/68747470733a2f2f62616467652e667572792e696f2f70672f70676d712e737667" alt="PGXN version" data-canonical-src="https://badge.fury.io/pg/pgmq.svg"></a></p>
<p dir="auto"><strong>Documentation</strong>: <a href="https://tembo-io.github.io/pgmq/" rel="nofollow">https://tembo-io.github.io/pgmq/</a></p>
<p dir="auto"><strong>Source</strong>: <a href="https://github.com/tembo-io/pgmq">https://github.com/tembo-io/pgmq</a></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Features</h2><a id="user-content-features" aria-label="Permalink: Features" href="#features"></a></p>
<ul dir="auto">
<li>Lightweight - No background worker or external dependencies, just Postgres functions packaged in an extension</li>
<li>Guaranteed "exactly once" delivery of messages to a consumer within a visibility timeout</li>
<li>API parity with <a href="https://aws.amazon.com/sqs/" rel="nofollow">AWS SQS</a> and <a href="https://github.com/smrchy/rsmq">RSMQ</a></li>
<li>Messages stay in the queue until explicitly removed</li>
<li>Messages can be archived, instead of deleted, for long-term retention and replayability</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Support</h2><a id="user-content-support" aria-label="Permalink: Support" href="#support"></a></p>
<p dir="auto">Postgres 12-16.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Table of Contents</h2><a id="user-content-table-of-contents" aria-label="Permalink: Table of Contents" href="#table-of-contents"></a></p>
<ul dir="auto">
<li><a href="#postgres-message-queue-pgmq">Postgres Message Queue (PGMQ)</a>
<ul dir="auto">
<li><a href="#features">Features</a></li>
<li><a href="#support">Support</a></li>
<li><a href="#table-of-contents">Table of Contents</a></li>
<li><a href="#installation">Installation</a></li>
<li><a href="#client-libraries">Client Libraries</a></li>
<li><a href="#sql-examples">SQL Examples</a>
<ul dir="auto">
<li><a href="#creating-a-queue">Creating a queue</a></li>
<li><a href="#send-two-messages">Send two messages</a></li>
<li><a href="#read-messages">Read messages</a></li>
<li><a href="#pop-a-message">Pop a message</a></li>
<li><a href="#archive-a-message">Archive a message</a></li>
<li><a href="#delete-a-message">Delete a message</a></li>
<li><a href="#drop-a-queue">Drop a queue</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#configuration">Configuration</a>
<ul dir="auto">
<li><a href="#partitioned-queues">Partitioned Queues</a></li>
<li><a href="#visibility-timeout-vt">Visibility Timeout (vt)</a></li>
<li><a href="#-contributors">✨ Contributors</a></li>
</ul>
</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Installation</h2><a id="user-content-installation" aria-label="Permalink: Installation" href="#installation"></a></p>
<p dir="auto">The fastest way to get started is by running the Tembo docker image, where PGMQ comes pre-installed.</p>
<div dir="auto" data-snippet-clipboard-copy-content="docker run -d --name postgres -e POSTGRES_PASSWORD=postgres -p 5432:5432 quay.io/tembo/pgmq-pg:latest"><pre>docker run -d --name postgres -e POSTGRES_PASSWORD=postgres -p 5432:5432 quay.io/tembo/pgmq-pg:latest</pre></div>
<p dir="auto">If you'd like to build from source, you can follow the instructions in <a href="https://github.com/tembo-io/pgmq/blob/main/CONTRIBUTING.md">CONTRIBUTING.md</a>.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Updating</h3><a id="user-content-updating" aria-label="Permalink: Updating" href="#updating"></a></p>
<p dir="auto">To update PGMQ versions, follow the instructions in <a href="https://github.com/tembo-io/pgmq/blob/main/UPDATING.md">UPDATING.md</a>.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Client Libraries</h2><a id="user-content-client-libraries" aria-label="Permalink: Client Libraries" href="#client-libraries"></a></p>
<ul dir="auto">
<li><a href="https://github.com/tembo-io/pgmq/tree/main/pgmq-rs">Rust</a></li>
<li><a href="https://github.com/tembo-io/pgmq/tree/main/tembo-pgmq-python">Python</a></li>
</ul>
<p dir="auto">Community</p>
<ul dir="auto">
<li><a href="https://github.com/craigpastro/pgmq-go">Go</a></li>
<li><a href="https://github.com/v0idpwn/pgmq-elixir">Elixir</a></li>
<li><a href="https://github.com/v0idpwn/off_broadway_pgmq">Elixir + Broadway</a></li>
<li><a href="https://github.com/adamalexandru4/pgmq-spring">Java (Spring Boot)</a></li>
<li><a href="https://github.com/Muhammad-Magdi/pgmq-js">Javascript (NodeJs)</a></li>
<li><a href="https://github.com/brianpursley/Npgmq">.NET</a></li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">SQL Examples</h2><a id="user-content-sql-examples" aria-label="Permalink: SQL Examples" href="#sql-examples"></a></p>
<div dir="auto" data-snippet-clipboard-copy-content="# Connect to Postgres
psql postgres://postgres:postgres@0.0.0.0:5432/postgres"><pre><span><span>#</span> Connect to Postgres</span>
psql postgres://postgres:postgres@0.0.0.0:5432/postgres</pre></div>
<div dir="auto" data-snippet-clipboard-copy-content="-- create the extension in the &quot;pgmq&quot; schema
CREATE EXTENSION pgmq;"><pre><span><span>--</span> create the extension in the "pgmq" schema</span>
CREATE EXTENSION pgmq;</pre></div>
<p dir="auto"><h3 tabindex="-1" dir="auto">Creating a queue</h3><a id="user-content-creating-a-queue" aria-label="Permalink: Creating a queue" href="#creating-a-queue"></a></p>
<p dir="auto">Every queue is its own table in the <code>pgmq</code> schema. The table name is the queue name prefixed with <code>q_</code>.
For example, <code>pgmq.q_my_queue</code> is the table for the queue <code>my_queue</code>.</p>
<div dir="auto" data-snippet-clipboard-copy-content="-- creates the queue
SELECT pgmq.create('my_queue');"><pre><span><span>--</span> creates the queue</span>
<span>SELECT</span> <span>pgmq</span>.<span>create</span>(<span><span>'</span>my_queue<span>'</span></span>);</pre></div>
<div data-snippet-clipboard-copy-content=" create
-------------

(1 row)"><pre lang="text"><code> create
-------------

(1 row)
</code></pre></div>
<p dir="auto"><h3 tabindex="-1" dir="auto">Send two messages</h3><a id="user-content-send-two-messages" aria-label="Permalink: Send two messages" href="#send-two-messages"></a></p>
<div dir="auto" data-snippet-clipboard-copy-content="-- messages are sent as JSON
SELECT * from pgmq.send('my_queue', '{&quot;foo&quot;: &quot;bar1&quot;}');
SELECT * from pgmq.send('my_queue', '{&quot;foo&quot;: &quot;bar2&quot;}');"><pre><span><span>--</span> messages are sent as JSON</span>
<span>SELECT</span> <span>*</span> <span>from</span> <span>pgmq</span>.<span>send</span>(<span><span>'</span>my_queue<span>'</span></span>, <span><span>'</span>{"foo": "bar1"}<span>'</span></span>);
<span>SELECT</span> <span>*</span> <span>from</span> <span>pgmq</span>.<span>send</span>(<span><span>'</span>my_queue<span>'</span></span>, <span><span>'</span>{"foo": "bar2"}<span>'</span></span>);</pre></div>
<p dir="auto">The message id is returned from the send function.</p>
<div data-snippet-clipboard-copy-content=" send
-----------
         1
(1 row)

 send
-----------
         2
(1 row)"><pre lang="text"><code> send
-----------
         1
(1 row)

 send
-----------
         2
(1 row)
</code></pre></div>
<p dir="auto"><h3 tabindex="-1" dir="auto">Read messages</h3><a id="user-content-read-messages" aria-label="Permalink: Read messages" href="#read-messages"></a></p>
<p dir="auto">Read <code>2</code> message from the queue. Make them invisible for <code>30</code> seconds.
If the messages are not deleted or archived within 30 seconds, they will become visible again
and can be read by another consumer.</p>
<div dir="auto" data-snippet-clipboard-copy-content="SELECT * FROM pgmq.read('my_queue', 30, 2);"><pre><span>SELECT</span> <span>*</span> <span>FROM</span> <span>pgmq</span>.<span>read</span>(<span><span>'</span>my_queue<span>'</span></span>, <span>30</span>, <span>2</span>);</pre></div>
<div data-snippet-clipboard-copy-content=" msg_id | read_ct |          enqueued_at          |              vt               |     message
--------+---------+-------------------------------+-------------------------------+-----------------
      1 |       1 | 2023-08-16 08:37:54.567283-05 | 2023-08-16 08:38:29.989841-05 | {&quot;foo&quot;: &quot;bar1&quot;}
      2 |       1 | 2023-08-16 08:37:54.572933-05 | 2023-08-16 08:38:29.989841-05 | {&quot;foo&quot;: &quot;bar2&quot;}"><pre lang="text"><code> msg_id | read_ct |          enqueued_at          |              vt               |     message
--------+---------+-------------------------------+-------------------------------+-----------------
      1 |       1 | 2023-08-16 08:37:54.567283-05 | 2023-08-16 08:38:29.989841-05 | {"foo": "bar1"}
      2 |       1 | 2023-08-16 08:37:54.572933-05 | 2023-08-16 08:38:29.989841-05 | {"foo": "bar2"}
</code></pre></div>
<p dir="auto">If the queue is empty, or if all messages are currently invisible, no rows will be returned.</p>
<div dir="auto" data-snippet-clipboard-copy-content="SELECT pgmq.read('my_queue', 30, 1);"><pre><span>SELECT</span> <span>pgmq</span>.<span>read</span>(<span><span>'</span>my_queue<span>'</span></span>, <span>30</span>, <span>1</span>);</pre></div>
<div data-snippet-clipboard-copy-content=" msg_id | read_ct | enqueued_at | vt | message
--------+---------+-------------+----+---------"><pre lang="text"><code> msg_id | read_ct | enqueued_at | vt | message
--------+---------+-------------+----+---------
</code></pre></div>
<p dir="auto"><h3 tabindex="-1" dir="auto">Pop a message</h3><a id="user-content-pop-a-message" aria-label="Permalink: Pop a message" href="#pop-a-message"></a></p>
<div dir="auto" data-snippet-clipboard-copy-content="-- Read a message and immediately delete it from the queue. Returns `None` if the queue is empty.
SELECT pgmq.pop('my_queue');"><pre><span><span>--</span> Read a message and immediately delete it from the queue. Returns `None` if the queue is empty.</span>
<span>SELECT</span> <span>pgmq</span>.<span>pop</span>(<span><span>'</span>my_queue<span>'</span></span>);</pre></div>
<div data-snippet-clipboard-copy-content=" msg_id | read_ct |          enqueued_at          |              vt               |     message
--------+---------+-------------------------------+-------------------------------+-----------------
      1 |       1 | 2023-08-16 08:37:54.567283-05 | 2023-08-16 08:38:29.989841-05 | {&quot;foo&quot;: &quot;bar1&quot;}"><pre lang="text"><code> msg_id | read_ct |          enqueued_at          |              vt               |     message
--------+---------+-------------------------------+-------------------------------+-----------------
      1 |       1 | 2023-08-16 08:37:54.567283-05 | 2023-08-16 08:38:29.989841-05 | {"foo": "bar1"}
</code></pre></div>
<p dir="auto"><h3 tabindex="-1" dir="auto">Archive a message</h3><a id="user-content-archive-a-message" aria-label="Permalink: Archive a message" href="#archive-a-message"></a></p>
<p dir="auto">Archiving a message removes it from the queue and inserts it to the archive table.</p>
<div dir="auto" data-snippet-clipboard-copy-content="-- Archive message with msg_id=2.
SELECT pgmq.archive('my_queue', 2);"><pre><span><span>--</span> Archive message with msg_id=2.</span>
<span>SELECT</span> <span>pgmq</span>.<span>archive</span>(<span><span>'</span>my_queue<span>'</span></span>, <span>2</span>);</pre></div>
<div data-snippet-clipboard-copy-content=" archive
--------------
 t
(1 row)"><pre lang="text"><code> archive
--------------
 t
(1 row)
</code></pre></div>
<div dir="auto" data-snippet-clipboard-copy-content="-- Archive tables have the prefix `a_`:
SELECT * FROM pgmq.a_my_queue;"><pre><span><span>--</span> Archive tables have the prefix `a_`:</span>
<span>SELECT</span> <span>*</span> <span>FROM</span> <span>pgmq</span>.<span>a_my_queue</span>;</pre></div>
<div data-snippet-clipboard-copy-content=" msg_id | read_ct |         enqueued_at          |          archived_at          |              vt               |     message
--------+---------+------------------------------+-------------------------------+-------------------------------+-----------------
      2 |       1 | 2023-04-25 00:55:40.68417-05 | 2023-04-25 00:56:35.937594-05 | 2023-04-25 00:56:20.532012-05 | {&quot;foo&quot;: &quot;bar2&quot;}"><pre lang="text"><code> msg_id | read_ct |         enqueued_at          |          archived_at          |              vt               |     message
--------+---------+------------------------------+-------------------------------+-------------------------------+-----------------
      2 |       1 | 2023-04-25 00:55:40.68417-05 | 2023-04-25 00:56:35.937594-05 | 2023-04-25 00:56:20.532012-05 | {"foo": "bar2"}
</code></pre></div>
<p dir="auto"><h3 tabindex="-1" dir="auto">Delete a message</h3><a id="user-content-delete-a-message" aria-label="Permalink: Delete a message" href="#delete-a-message"></a></p>
<p dir="auto">Send another message, so that we can delete it.</p>
<div dir="auto" data-snippet-clipboard-copy-content="SELECT pgmq.send('my_queue', '{&quot;foo&quot;: &quot;bar3&quot;}');"><pre><span>SELECT</span> <span>pgmq</span>.<span>send</span>(<span><span>'</span>my_queue<span>'</span></span>, <span><span>'</span>{"foo": "bar3"}<span>'</span></span>);</pre></div>
<div data-snippet-clipboard-copy-content=" send
-----------
        3
(1 row)"><pre lang="text"><code> send
-----------
        3
(1 row)
</code></pre></div>
<p dir="auto">Delete the message with id <code>3</code> from the queue named <code>my_queue</code>.</p>
<div dir="auto" data-snippet-clipboard-copy-content="SELECT pgmq.delete('my_queue', 3);"><pre><span>SELECT</span> <span>pgmq</span>.<span>delete</span>(<span><span>'</span>my_queue<span>'</span></span>, <span>3</span>);</pre></div>
<div data-snippet-clipboard-copy-content=" delete
-------------
 t
(1 row)"><pre lang="text"><code> delete
-------------
 t
(1 row)
</code></pre></div>
<p dir="auto"><h3 tabindex="-1" dir="auto">Drop a queue</h3><a id="user-content-drop-a-queue" aria-label="Permalink: Drop a queue" href="#drop-a-queue"></a></p>
<p dir="auto">Delete the queue <code>my_queue</code>.</p>
<div dir="auto" data-snippet-clipboard-copy-content="SELECT pgmq.drop_queue('my_queue');"><pre><span>SELECT</span> <span>pgmq</span>.<span>drop_queue</span>(<span><span>'</span>my_queue<span>'</span></span>);</pre></div>
<div data-snippet-clipboard-copy-content=" drop_queue
-----------------
 t
(1 row)"><pre lang="text"><code> drop_queue
-----------------
 t
(1 row)
</code></pre></div>
<p dir="auto"><h2 tabindex="-1" dir="auto">Configuration</h2><a id="user-content-configuration" aria-label="Permalink: Configuration" href="#configuration"></a></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Partitioned Queues</h2><a id="user-content-partitioned-queues" aria-label="Permalink: Partitioned Queues" href="#partitioned-queues"></a></p>
<p dir="auto">You will need to install <a href="https://github.com/pgpartman/pg_partman/">pg_partman</a> if you want to use <code>pgmq</code> partitioned queues.</p>
<p dir="auto"><code>pgmq</code> queue tables can be created as a partitioned table by using <code>pgmq.create_partitioned()</code>. <a href="https://github.com/pgpartman/pg_partman/">pg_partman</a>
handles all maintenance of queue tables. This includes creating new partitions and dropping old partitions.</p>
<p dir="auto">Partitions behavior is configured at the time queues are created, via <code>pgmq.create_partitioned()</code>. This function has three parameters:</p>
<p dir="auto"><code>queue_name: text</code>: The name of the queue. Queues are Postgres tables prepended with <code>q_</code>. For example, <code>q_my_queue</code>. The archive is instead prefixed by <code>a_</code>, for example <code>a_my_queue</code>.</p>
<p dir="auto"><code>partition_interval: text</code> - The interval at which partitions are created. This can be either any valid Postgres <code>Duration</code> supported by pg_partman, or an integer value. When it is a duration, queues are partitioned by the time at which messages are sent to the table (<code>enqueued_at</code>). A value of <code>'daily'</code> would create a new partition each day. When it is an integer value, queues are partitioned by the <code>msg_id</code>. A value of <code>'100'</code> will create a new partition every 100 messages. The value must agree with <code>retention_interval</code> (time based or numeric). The default value is <code>daily</code>.</p>
<p dir="auto"><code>retention_interval: text</code> - The interval for retaining partitions. This can be either any valid Postgres <code>Duration</code> supported by pg_partman, or an integer value. When it is a duration, partitions containing data greater than the duration will be dropped. When it is an integer value, any messages that have a <code>msg_id</code> less than <code>max(msg_id) - retention_interval</code> will be dropped. For example, if the max <code>msg_id</code> is 100 and the <code>retention_interval</code> is 60, any partitions with <code>msg_id</code> values less than 40 will be dropped. The value must agree with <code>partition_interval</code> (time based or numeric). The default is <code>'5 days'</code>. Note: <code>retention_interval</code> does not apply to messages that have been deleted via <code>pgmq.delete()</code> or archived with <code>pgmq.archive()</code>. <code>pgmq.delete()</code> removes messages forever and <code>pgmq.archive()</code> moves messages to the corresponding archive table forever (for example, <code>a_my_queue</code>).</p>
<p dir="auto">In order for automatic partition maintenance to take place, several settings must be added to the <code>postgresql.conf</code> file, which is typically located in the postgres <code>DATADIR</code>.
<code>pg_partman_bgw.interval</code>
in <code>postgresql.conf</code>. Below are the default configuration values set in Tembo docker images.</p>
<p dir="auto">Add the following to <code>postgresql.conf</code>. Note, changing <code>shared_preload_libraries</code> requires a restart of Postgres.</p>
<p dir="auto"><code>pg_partman_bgw.interval</code> sets the interval at which <code>pg_partman</code> conducts maintenance. This creates new partitions and dropping of partitions falling out of the <code>retention_interval</code>. By default, <code>pg_partman</code> will keep 4 partitions "ahead" of the currently active partition.</p>
<div data-snippet-clipboard-copy-content="shared_preload_libraries = 'pg_partman_bgw' # requires restart of Postgres
pg_partman_bgw.interval = 60
pg_partman_bgw.role = 'postgres'
pg_partman_bgw.dbname = 'postgres'"><pre><code>shared_preload_libraries = 'pg_partman_bgw' # requires restart of Postgres
pg_partman_bgw.interval = 60
pg_partman_bgw.role = 'postgres'
pg_partman_bgw.dbname = 'postgres'
</code></pre></div>
<p dir="auto"><h2 tabindex="-1" dir="auto">Visibility Timeout (vt)</h2><a id="user-content-visibility-timeout-vt" aria-label="Permalink: Visibility Timeout (vt)" href="#visibility-timeout-vt"></a></p>
<p dir="auto">pgmq guarantees exactly once delivery of a message within a visibility timeout. The visibility timeout is the amount of time a message is invisible to other consumers after it has been read by a consumer. If the message is NOT deleted or archived within the visibility timeout, it will become visible again and can be read by another consumer. The visibility timeout is set when a message is read from the queue, via <code>pgmq.read()</code>. It is recommended to set a <code>vt</code> value that is greater than the expected time it takes to process a message. After the application successfully processes the message, it should call <code>pgmq.delete()</code> to completely remove the message from the queue or <code>pgmq.archive()</code> to move it to the archive table for the queue.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">✨ Contributors</h2><a id="user-content--contributors" aria-label="Permalink: ✨ Contributors" href="#-contributors"></a></p>
<p dir="auto">Thanks goes to these incredible people:</p>
<a href="https://github.com/tembo-io/pgmq/graphs/contributors">
  <img src="https://camo.githubusercontent.com/edb80ae9a4639740585e6fd4f6856ed67a19b5708989b6eff212cb9b32511e5b/68747470733a2f2f636f6e747269622e726f636b732f696d6167653f7265706f3d74656d626f2d696f2f70676d71" data-canonical-src="https://contrib.rocks/image?repo=tembo-io/pgmq">
</a>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Deaf girl is cured in world first gene therapy trial (405 pts)]]></title>
            <link>https://www.independent.co.uk/news/health/deaf-cure-girl-gene-therapy-b2541735.html</link>
            <guid>40307138</guid>
            <pubDate>Thu, 09 May 2024 11:38:14 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.independent.co.uk/news/health/deaf-cure-girl-gene-therapy-b2541735.html">https://www.independent.co.uk/news/health/deaf-cure-girl-gene-therapy-b2541735.html</a>, See on <a href="https://news.ycombinator.com/item?id=40307138">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="main"><div data-newsletter-key="receiveIndyHealthNews"><p><img src="https://static.independent.co.uk/static-assets/images/newsletter/health/health.jpg" loading="lazy" alt="Health Check"></p><div><p><h3 data-nosnippet="">Sign up for our free Health Check email to receive exclusive analysis on the week in health</h3><h3 data-nosnippet="">Get our free Health Check email</h3></p></div></div><p>A <a href="https://www.independent.co.uk/topic/british">British</a> girl has had her hearing restored after becoming the first in the world to take part in a groundbreaking new <a href="https://www.independent.co.uk/topic/gene-therapy">gene therapy</a> trial.</p><p>Opal Sandy, aged 18 months, was born completely deaf due to the condition auditory neuropathy, which is caused by the disruption of nerve impulses travelling from the inner <a href="https://www.independent.co.uk/topic/ear">ear</a> to the <a href="https://www.independent.co.uk/topic/brain">brain</a>.</p><p>Now, thanks to a “one and done” gene therapy being trialled in the UK and worldwide, Opal’s hearing is almost normal – and could even improve further.</p><p>The little girl from Oxfordshire, who has a genetic form of auditory neuropathy, was treated at Addenbrooke’s Hospital, which is part of Cambridge University Hospitals NHS Foundation Trust.</p><p>Professor Manohar Bance, an ear surgeon at the trust and chief investigator for the trial, told the PA news agency the results were “better than I hoped or expected” and may point to a cure for patients with this type of deafness.</p><p>He said: “We have results from [Opal] which are very spectacular – so close to normal hearing restoration. So we do hope it could be a potential cure.”</p><div><figure><figcaption>Opal (centre) with her mother Jo, father James and sister Nora (left) at their home in Eynsham, Oxfordshire<span> <!-- -->(<!-- -->Andrew Matthews/PA Wire<!-- -->)</span></figcaption></figure></div><p>Auditory neuropathy can be caused by a fault in the OTOF gene, which is responsible for making a protein called otoferlin. This enables cells in the ear to communicate with the hearing nerve.</p><p>To overcome the fault, the “new era” gene therapy – from biotech firm Regeneron – delivers a working copy of the gene to the ear.</p><p>In Opal’s case, she received an infusion containing the working gene in her right ear during surgery last September.</p><p>Her parents Jo and James, both 33, noticed improvements to her hearing within four weeks when Opal turned her head to loud clapping.</p><p>“When she first turned, I couldn’t believe it,” Mrs Sandy told PA.</p><p>“I thought it was a fluke or like a change in light or something that had caught her eye, but I repeated it a few times.</p><p>“I picked my phone up and texted James, and said, ‘I think it’s working’. I was absolutely gobsmacked. I thought it was a fluke.”</p><p>But even more impressive results were on the horizon.</p><p>Some 24 weeks after surgery, in February this year, tests in Cambridge showed Opal could also hear soft sounds such as a whisper.</p><p>“The audiologist played back some of the sounds that she was responding to and they were ridiculously quiet sort of sounds that in the real world wouldn’t catch your attention during a conversation,” Mrs Sandy said.</p><p>“Certainly since February, we’ve noticed her sister [Nora] waking her up in the morning because she’s running around on the landing, or someone rings on the door so her nap’s cut short.</p><p>“She’s definitely responding more to sort of what we would call functional sounds rather than just sounds that we use to test her.</p><p>“We were told she had near-normal hearing last time – I think they got responses at sort of 25 to 30 decibels.</p><p>“I think normal hearing is classed at 20 decibels, so she’s not far off. Before, she had no hearing whatsoever.”</p><p>Prof Bance said Opal’s hearing is now “close to normal”, adding: “We hope she’ll get back to normal by the next testing.”</p><p>He added that the treatment is “a one-and-done therapy, so hopefully you have your treatment and then you go back to your life”.</p><p>A second child has also received the gene therapy treatment at Cambridge University Hospitals, with positive results seen recently, six weeks after surgery.</p><p>The overall phase 1/2 Chord trial consists of three parts, with three deaf children, including Opal, receiving a low dose of gene therapy in one ear only.</p><p>A different set of three children will get a high dose on one side. Then, if that is shown to be safe, more children will receive a dose in both ears at the same time.</p><p>Up to 18 youngsters from the UK, Spain and the US are being recruited to the trial and will be followed up for five years.</p><p>Prof Bance said: “My entire life, gene therapy has been ‘five years away’, and I’ve been in practice about 30 years.</p><p>“So, for me, it was almost unreal that this moment had arrived.</p><p>“It was just the fact that we’ve been hearing about this for so long, and there’s been so much work, decades of work … to finally see something that actually worked in humans … It was quite spectacular and a bit awe-inspiring really.</p><p>“It felt very special.”</p><p>At the moment, the gold standard treatment for auditory neuropathy is cochlear implants.</p><p>Opal had one fitted to her left ear at the same time as she underwent gene therapy in her right ear, to ensure she got hearing as soon as possible.</p><p>The youngster is the first patient globally to receive the Regeneron therapy and “she’s the youngest globally that’s been done to date as far as we know,” Prof Bance said.</p><p>China has also been working on targeting the same gene, with positive results, though Prof Bance said theirs uses a different technology and slightly different mode of delivery.</p><p>Medics in Philadelphia have also reported a good outcome with a type of gene therapy on an 11-year-old boy, who was operated on after Opal.</p><p>Prof Bance said he believes the trial is “just the beginning of gene therapies”, adding: “It marks a new era in the treatment for deafness.</p><p>“It also supports the development of other gene therapies that may prove to make a difference in other genetic-related hearing conditions, many of which are more common than auditory neuropathy.”</p><p>He said it could take a while for more children to benefit from gene therapy. The treatment was currently not available on the NHS.</p><p>“What’s really helped though is that the NHS does pay for genetic testing now for hearing loss,” he said.</p><div><figure><figcaption>Opal Sandy can now hear unaided for the first time<span> <!-- -->(<!-- -->Andrew Matthews/PA Wire<!-- -->)</span></figcaption></figure></div><p>Opal’s surgery, which was carried out under general anaesthetic, was very similar to fitting a cochlear implant, Prof Bance continued.</p><p>“So basically, we find the inner ear and we open the inner ear and infuse the treatment, in this particular case using a catheter, over 16 minutes,” he said.</p><p>“We have to make a release hole in another part of the ear to let the treatment out because it has to go all the way through the ear.</p><p>“And then we just repair and close up, so it’s actually a very similar approach to a cochlear implant, except we don’t put the implant in.”</p><p>Martin McLean, senior policy adviser at the National Deaf Children’s Society, welcomed the study, saying it would lead to learning regarding gene therapies for deafness with a specific genetic cause.</p><p>“We would like to emphasise that, with the right support from the start, deafness should never be a barrier to happiness or fulfilment,” he said.</p><p>“As a charity, we support families to make informed choices about medical technologies, so that they can give their deaf child the best possible start in life.”</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[What I mean when I say that machine learning in Elixir is production-ready (125 pts)]]></title>
            <link>https://cigrainger.com/elixirconf-eu-2024-keynote/</link>
            <guid>40307108</guid>
            <pubDate>Thu, 09 May 2024 11:33:49 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://cigrainger.com/elixirconf-eu-2024-keynote/">https://cigrainger.com/elixirconf-eu-2024-keynote/</a>, See on <a href="https://news.ycombinator.com/item?id=40307108">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
    







<p>
    <i>
        <time datetime="2024-05-09">
            09 May, 2024
        </time>
    </i>
</p>


    <p>My ElixirConf EU 2024 keynote <a href="https://youtu.be/5FlZHkc4Mq4?si=agofn18zjoKHZw6S">"Ship it! A roadmap for putting Nx into production"</a> is up on youtube. I'm hoping it will help folks gain the confidence to go out and... ship it!</p>
<p>I often say 'machine learning in Elixir is ready for production'. On further reflection, I realised that this statement dramatically undersells the true potential and capabilities of machine learning within the Elixir ecosystem. (I already posted these reflections <a href="https://twitter.com/cigrainger/status/1788147429021237592">on Twitter</a>).</p>
<p>When I say "production-ready," it's not just about the ability to deploy machine learning models in a live environment. In the context of Elixir, being production-ready means deep integration with the weird and wonderful thing called the BEAM (Bogdan/Björn's Erlang Abstract Machine). It means seamless compatibility with OTP (Open Telecom Platform) primitives, which are the building blocks of Elixir applications.</p>
<p>This deep integration is what makes machine learning in Elixir so powerful. Just as Elixir has proven to be a cheat code for building web applications and distributed systems, it now offers the same advantage for machine learning tasks. The Elixir ecosystem provides a secret weapon that empowers developers to build and deploy machine learning solutions with unparalleled ease and efficiency.</p>
<p>For a growing set of areas where Elixir's machine learning ecosystem is applicable (which is already pretty extensive), I firmly believe that you're better off shipping inference with Elixir than basically anything else. In many cases, it's even worthwhile to conduct experiments and model training in Elixir, despite the relative youth of that part of the ecosystem.</p>
<p>So, what makes machine learning in Elixir so powerful? Mainly: OTP. Also: good choices. <a href="https://github.com/elixir-nx/nx">Nx</a> was built from the ground up with inspiration from JAX, a popular machine learning framework. By leveraging metaprogramming and embracing pluggable backends and compilers from day one, Nx benefits from a significant second-mover advantage. This architectural decision, which I covered in more detail during my ElixirConf EU talk, unlocks huge potential that we see play out in the libraries built on top of it like <a href="https://github.com/elixir-nx/axon">Axon</a>, <a href="https://github.com/elixir-nx/bumblebee">Bumblebee</a>, and <a href="https://github.com/elixir-nx/scholar">Scholar</a>.</p>
<p>Thanks to these foundational choices, the entire Elixir machine learning ecosystem can reap the benefits. With <code>defn</code>, a macro provided by Nx, machine learning capabilities are practically built into the language itself. Take a moment to reflect on the significance of this: by simply writing Elixir code, you gain access to multi-stage compilation that targets various hardware platforms, including GPUs and TPUs.</p>
<p>Moreover, <a href="https://hexdocs.pm/nx/Nx.Serving.html"><code>Nx.Serving</code></a>, a module in <code>Nx</code> for serving machine learning models, is just wild. You get out-of-the-box distributed, clustered, hardware-agnostic automatic batching without the caller needing to concern itself with any of that? Just wild. It's the right separation of concerns.</p>
<p>The actor model of concurrency, which is at the heart of Elixir, proves to be the perfect abstraction for serving machine learning workloads and, just as importantly, integrating them into broader systems. By encapsulating models within processes, you can leverage all the built-in features that make the BEAM so powerful. <code>Nx.Serving</code> is just another application in your supervision tree, making it just another component of your robust and fault-tolerant system.</p>
<p>So integrating machine learning into a Phoenix application becomes a breeze. You can take advantage of libraries like Oban for durable and robust job processing, Broadway for concurrent data processing pipelines with backpressure, and FLAME for lambda-like execution on the BEAM. You can send pubsub messages about progress, results, etc to LiveViews (which are, of course, just processes). And of course those updates can trigger changes in views like I describe in <a href="https://x.com/cigrainger/status/1788490429341532351">this thread</a>.</p>
<p>The beauty of the Elixir ecosystem lies in its ability to create brilliant features for end users with minimal effort. We routinely handle thousands and millions of messages per day over pubsub and between processes, showcasing the scalability and efficiency of the system.</p>
<p><img alt="Christopher Grainger" src="https://bear-images.sfo2.cdn.digitaloceanspaces.com/chris07-1715253805-0.png"></p>
<p>You can add machine learning to Saša Jurić's famous chart (above). So if Rails is considered the "one person framework," then Phoenix can be seen as the "half person framework," from basic CRUD operations to the cutting edge of LLMs.</p>
<p>Machine learning in Elixir is production-ready. In this ecosystem, that statement means a lot.</p>
<iframe allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen="" frameborder="0" height="315" referrerpolicy="strict-origin-when-cross-origin" src="https://www.youtube-nocookie.com/embed/5FlZHkc4Mq4?si=a8Vq6usf0BEaDHqF" title="YouTube video player" width="560"></iframe>





    
    <p>
        
        <a href="https://cigrainger.com/blog/?q=elixir">#elixir</a>
        
        <a href="https://cigrainger.com/blog/?q=machine%20learning">#machine learning</a>
        
        <a href="https://cigrainger.com/blog/?q=nx">#nx</a>
        
        <a href="https://cigrainger.com/blog/?q=talks">#talks</a>
        
    </p>
    

    
    


    



  </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Algebraic Data Types for C99 (286 pts)]]></title>
            <link>https://github.com/Hirrolot/datatype99</link>
            <guid>40307098</guid>
            <pubDate>Thu, 09 May 2024 11:31:27 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/Hirrolot/datatype99">https://github.com/Hirrolot/datatype99</a>, See on <a href="https://news.ycombinator.com/item?id=40307098">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><div dir="auto">
  <p><a href="https://github.com/Hirrolot/datatype99/blob/master/examples/binary_tree.c"><img src="https://github.com/Hirrolot/datatype99/raw/master/images/preview.png" width="600"></a></p><p dir="auto"><h2 tabindex="-1" dir="auto">Datatype99</h2><a id="user-content-datatype99" aria-label="Permalink: Datatype99" href="#datatype99"></a></p>
  <p><a href="https://github.com/Hirrolot/datatype99/actions">
    <img src="https://github.com/Hirrolot/datatype99/workflows/C/C++%20CI/badge.svg">
  </a>
  <a href="https://lists.sr.ht/~hirrolot/metalang99" rel="nofollow">
    <img src="https://camo.githubusercontent.com/c386c579e5acb1bee827aaea93d933bcd3e41e9d19098d9d9f9bd7f02e982dda/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f6d61696c696e672532306c6973742d6c697374732e73722e68742d6f72616e6765" data-canonical-src="https://img.shields.io/badge/mailing%20list-lists.sr.ht-orange">
  </a></p><p dir="auto">Safe, intuitive <a href="https://en.wikipedia.org/wiki/Algebraic_data_type" rel="nofollow">algebraic data types</a> with exhaustive pattern matching &amp; compile-time introspection facilities. No external tools required, pure C99.</p>
</div>
<p dir="auto"><h2 tabindex="-1" dir="auto">Highlights</h2><a id="user-content-highlights" aria-label="Permalink: Highlights" href="#highlights"></a></p>
<ul dir="auto">
<li>
<p dir="auto"><strong>Type-safe.</strong> Such things as improperly typed variants, non-exhaustive pattern matching, and invalid field access are caught at compile-time.</p>
</li>
<li>
<p dir="auto"><strong>Portable.</strong> Everything you need is a standard-conforming C99 compiler; neither the standard library, nor compiler/platform-specific functionality or VLA are required.</p>
</li>
<li>
<p dir="auto"><strong>Predictable.</strong> Datatype99 comes with formal <a href="#semantics">code generation semantics</a>, meaning that the generated data layout is guaranteed to always be the same.</p>
</li>
<li>
<p dir="auto"><strong>Comprehensible errors.</strong> Datatype99 is <a href="#q-what-about-compile-time-errors">resilient to bad code</a>.</p>
</li>
<li>
<p dir="auto"><strong>Battle-tested.</strong> Datatype99 is used at <a href="https://openipc.org/" rel="nofollow">OpenIPC</a> to develop real-time streaming software for IP cameras; this includes an <a href="https://github.com/OpenIPC/smolrtsp/">RTSP 1.0 implementation</a> along with ~50k lines of private code.</p>
</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Installation</h2><a id="user-content-installation" aria-label="Permalink: Installation" href="#installation"></a></p>
<p dir="auto">Datatype99 consists of one header file <code>datatype99.h</code> and one dependency <a href="https://github.com/Hirrolot/metalang99">Metalang99</a>. To use it in your project, you need to:</p>
<ol dir="auto">
<li>Add <code>datatype99</code> and <code>metalang99/include</code> to your include directories.</li>
<li>Specify <a href="https://gcc.gnu.org/onlinedocs/gcc/Preprocessor-Options.html" rel="nofollow"><code>-ftrack-macro-expansion=0</code></a> (GCC) or <a href="https://clang.llvm.org/docs/ClangCommandLineReference.html#cmdoption-clang-fmacro-backtrace-limit" rel="nofollow"><code>-fmacro-backtrace-limit=1</code></a> (Clang) to avoid useless macro expansion errors.</li>
</ol>
<p dir="auto">If you use CMake, the recommended way is <a href="https://cmake.org/cmake/help/latest/module/FetchContent.html" rel="nofollow"><code>FetchContent</code></a>:</p>
<div dir="auto" data-snippet-clipboard-copy-content="include(FetchContent)

FetchContent_Declare(
    datatype99
    URL https://github.com/Hirrolot/datatype99/archive/refs/tags/v1.2.3.tar.gz # v1.2.3
)

FetchContent_MakeAvailable(datatype99)

target_link_libraries(MyProject datatype99)

# Disable full macro expansion backtraces for Metalang99.
if(CMAKE_C_COMPILER_ID STREQUAL &quot;Clang&quot;)
  target_compile_options(MyProject PRIVATE -fmacro-backtrace-limit=1)
elseif(CMAKE_C_COMPILER_ID STREQUAL &quot;GNU&quot;)
  target_compile_options(MyProject PRIVATE -ftrack-macro-expansion=0)
endif()"><pre><span>include</span>(FetchContent)

FetchContent_Declare(
    datatype99
    URL https://github.com/Hirrolot/datatype99/archive/refs/tags/v1.2.3.tar.gz <span><span>#</span> v1.2.3</span>
)

FetchContent_MakeAvailable(datatype99)

<span>target_link_libraries</span>(MyProject datatype99)

<span><span>#</span> Disable full macro expansion backtraces for Metalang99.</span>
<span>if</span>(CMAKE_C_COMPILER_ID <span>STREQUAL</span> <span>"Clang"</span>)
  <span>target_compile_options</span>(MyProject <span>PRIVATE</span> -fmacro-backtrace-limit=1)
<span>elseif</span>(CMAKE_C_COMPILER_ID <span>STREQUAL</span> <span>"GNU"</span>)
  <span>target_compile_options</span>(MyProject <span>PRIVATE</span> -ftrack-macro-expansion=0)
<span>endif</span>()</pre></div>
<p dir="auto">(By default, <code>datatype99/CMakeLists.txt</code> downloads Metalang99 <a href="https://github.com/Hirrolot/metalang99/releases/tag/v1.13.2">v1.13.2</a> from the GitHub releases; if you want to override this behaviour, you can do so by invoking <a href="https://cmake.org/cmake/help/latest/module/FetchContent.html#command:fetchcontent_declare" rel="nofollow"><code>FetchContent_Declare</code></a> earlier.)</p>
<p dir="auto">Optionally, you can <a href="https://en.wikipedia.org/wiki/Precompiled_header" rel="nofollow">precompile headers</a> in your project that rely on Datatype99. This will decrease compilation time, because the headers will not be compiled each time they are included.</p>
<p dir="auto">Happy hacking!</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Usage</h2><a id="user-content-usage" aria-label="Permalink: Usage" href="#usage"></a></p>
<p dir="auto">Put simply, Datatype99 is just a syntax sugar over <a href="https://en.wikipedia.org/wiki/Tagged_union" rel="nofollow">tagged unions</a>; the only difference is that it is more safe and concise. For example, to represent a binary tree, you would normally write something like this:</p>
<div dir="auto" data-snippet-clipboard-copy-content="typedef struct {
    struct BinaryTree *lhs;
    int x;
    struct BinaryTree *rhs;
} BinaryTreeNode;

typedef struct {
    enum { Leaf, Node } tag;
    union {
        int leaf;
        BinaryTreeNode node;
    } data;
} BinaryTree;"><pre><span>typedef</span> <span>struct</span> {
    <span>struct</span> <span>BinaryTree</span> <span>*</span><span>lhs</span>;
    <span>int</span> <span>x</span>;
    <span>struct</span> <span>BinaryTree</span> <span>*</span><span>rhs</span>;
} <span>BinaryTreeNode</span>;

<span>typedef</span> <span>struct</span> {
    <span>enum</span> { <span>Leaf</span>, <span>Node</span> } <span>tag</span>;
    <span>union</span> {
        <span>int</span> <span>leaf</span>;
        <span>BinaryTreeNode</span> <span>node</span>;
    } <span>data</span>;
} <span>BinaryTree</span>;</pre></div>
<p dir="auto">To avoid this boilerplate, you can use Datatype99:</p>
<div dir="auto" data-snippet-clipboard-copy-content="datatype(
    BinaryTree,
    (Leaf, int),
    (Node, BinaryTree *, int, BinaryTree *)
);"><pre><span>datatype</span>(
    <span>BinaryTree</span>,
    (<span>Leaf</span>, <span>int</span>),
    (<span>Node</span>, <span>BinaryTree</span> <span>*</span><span></span>, <span>int</span>, <span>BinaryTree</span> <span>*</span>)
);</pre></div>
<p dir="auto">Say you want to sum all nodes and leafs in your binary tree. Then you may write something like this:</p>
<div dir="auto" data-snippet-clipboard-copy-content="int sum(const BinaryTree *tree) {
    switch (tree->tag) {
    case Leaf:
        return tree->data.leaf;
    case Node:
        return sum(tree->data.node.lhs) + tree->data.node.x + sum(tree->data.node.rhs);
    }

    // Invalid input (no such variant).
    return -1;
}"><pre><span>int</span> <span>sum</span>(<span>const</span> <span>BinaryTree</span> <span>*</span><span>tree</span>) {
    <span>switch</span> (<span>tree</span><span>-&gt;</span><span>tag</span>) {
    <span>case</span> <span>Leaf</span>:
        <span>return</span> <span>tree</span><span>-&gt;</span><span>data</span>.<span>leaf</span>;
    <span>case</span> <span>Node</span>:
        <span>return</span> <span>sum</span>(<span>tree</span><span>-&gt;</span><span>data</span>.<span>node</span>.<span>lhs</span>) <span>+</span> <span>tree</span><span>-&gt;</span><span>data</span>.<span>node</span>.<span>x</span> <span>+</span> <span>sum</span>(<span>tree</span><span>-&gt;</span><span>data</span>.<span>node</span>.<span>rhs</span>);
    }

    <span>// Invalid input (no such variant).</span>
    <span>return</span> <span>-1</span>;
}</pre></div>
<p dir="auto">... but what if you accidentally access <code>tree-&gt;data.node</code> after <code>case Leaf:</code>? Your compiler would not warn you, thus resulting in a business logic bug.</p>
<p dir="auto">With Datatype99, you can rewrite <code>sum</code> as follows, using a technique called <em>pattern matching</em>:</p>
<div dir="auto" data-snippet-clipboard-copy-content="int sum(const BinaryTree *tree) {
    match(*tree) {
        of(Leaf, x) return *x;
        of(Node, lhs, x, rhs) return sum(*lhs) + *x + sum(*rhs);
    }

    // Invalid input (no such variant).
    return -1;
}"><pre><span>int</span> <span>sum</span>(<span>const</span> <span>BinaryTree</span> <span>*</span><span>tree</span>) {
    <span>match</span>(<span>*</span><span>tree</span>) {
        <span>of</span>(<span>Leaf</span>, <span>x</span>) <span>return</span> <span>*</span><span>x</span>;
        <span>of</span>(<span>Node</span>, <span>lhs</span>, <span>x</span>, <span>rhs</span>) <span>return</span> <span>sum</span>(<span>*</span><span>lhs</span>) <span>+</span> <span>*</span><span>x</span> <span>+</span> <span>sum</span>(<span>*</span><span>rhs</span>);
    }

    <span>// Invalid input (no such variant).</span>
    <span>return</span> <span>-1</span>;
}</pre></div>
<p dir="auto"><code>of</code> gives you variables called <em>bindings</em>: <code>x</code>, <code>lhs</code>, or <code>rhs</code>. This design has a few neat aspects:</p>
<ul dir="auto">
<li><strong>Compile-time safety.</strong> The bindings of <code>Node</code> are invisible after <code>of(Leaf, x)</code> and vice versa, so compilation will fail to proceed if you access them inappropriately.</li>
<li><strong>Flexibility.</strong> Bindings have pointer types so that you can mutate them, thereby mutating the whole <code>tree</code>; in order to obtain a value, you can dereference them, as shown in the example: <code>return *x;</code>.</li>
</ul>
<p dir="auto">The last thing unmentioned is how you construct variants. Internally, Datatype99 generates <code>inline static</code> functions called <em>value constructors</em>; you can use them as follows:</p>
<div dir="auto" data-snippet-clipboard-copy-content="BinaryTree leaf5 = Leaf(5);
BinaryTree leaf7 = Leaf(7);
BinaryTree node = Node(&amp;leaf5, 123, &amp;leaf7);"><pre><span>BinaryTree</span> <span>leaf5</span> <span>=</span> <span>Leaf</span>(<span>5</span>);
<span>BinaryTree</span> <span>leaf7</span> <span>=</span> <span>Leaf</span>(<span>7</span>);
<span>BinaryTree</span> <span>node</span> <span>=</span> <span>Node</span>(<span>&amp;</span><span>leaf5</span>, <span>123</span>, <span>&amp;</span><span>leaf7</span>);</pre></div>
<p dir="auto">Finally, just a few brief notes about pattern matching:</p>
<ul dir="auto">
<li>To match the default case, write <code>otherwise { ... }</code> at the end of <code>match</code>.</li>
<li>To ignore a binding, write <code>_</code>: <code>of(Foo, a, b, _, d)</code>.</li>
<li><strong>PLEASE</strong>, <a href="#top-level-breakcontinue">do <strong>not</strong> use top-level <code>break</code>/<code>continue</code></a> inside statements provided to <code>of</code> and <code>ifLet</code>; use <code>goto</code> labels instead.</li>
</ul>
<p dir="auto">Congratulations, this is all you need to know to write most of the stuff! If you feel fancy, you can also introspect your types at compile-time; see <a href="https://github.com/Hirrolot/datatype99/blob/master/examples/derive"><code>examples/derive/</code></a> for the examples.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Syntax and semantics</h2><a id="user-content-syntax-and-semantics" aria-label="Permalink: Syntax and semantics" href="#syntax-and-semantics"></a></p>
<p dir="auto">Having a well-defined semantics of the macros, you can write an FFI which is quite common in C.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">EBNF syntax</h3><a id="user-content-ebnf-syntax" aria-label="Permalink: EBNF syntax" href="#ebnf-syntax"></a></p>
<div dir="auto" data-snippet-clipboard-copy-content="<datatype>      ::= &quot;datatype(&quot; [ <derive-clause> &quot;,&quot; ] <datatype-name> { &quot;,&quot; <variant> }+ &quot;)&quot; ;
<record>        ::= &quot;record(&quot;   [ <derive-clause> &quot;,&quot; ] <record-name>   { &quot;,&quot; <field>   }* &quot;)&quot; ;
<datatype-name> ::= <ident> ;
<record-name>   ::= <ident> ;

<variant>       ::= &quot;(&quot; <variant-name> { &quot;,&quot; <type> }* &quot;)&quot; ;
<field>         ::= &quot;(&quot; <type> &quot;,&quot; <field-name> &quot;)&quot; ;
<variant-name>  ::= <ident> ;
<field-name>    ::= <ident> ;

<derive-clause> ::= &quot;derive(&quot; <deriver-name> { &quot;,&quot; <deriver-name> }* &quot;)&quot; ;
<deriver-name>  ::= <ident> ;

<match>         ::= &quot;match(&quot; <lvalue> &quot;) {&quot; { <of> }* [ <otherwise> ] &quot;}&quot; ;
<matches>       ::= &quot;MATCHES(&quot; <expr> &quot;,&quot; <ident> &quot;)&quot; ;
<if-let>        ::= &quot;ifLet(&quot; <lvalue> &quot;,&quot; <variant-name> &quot;,&quot; <ident> { &quot;,&quot; <ident> }* &quot;)&quot; <stmt> ;
<of>            ::= &quot;of(&quot; <variant-name> { &quot;,&quot; <ident> }* &quot;)&quot; <stmt> ;
<otherwise>     ::= &quot;otherwise&quot; <stmt> ;"><pre><span>&lt;datatype&gt;</span>      <span>::=</span> <span><span>"</span>datatype(<span>"</span></span> [ &lt;<span>derive-clause</span>&gt; <span><span>"</span>,<span>"</span></span> ] &lt;<span>datatype-name</span>&gt; { <span><span>"</span>,<span>"</span></span> &lt;<span>variant</span>&gt; }<span>+</span> <span><span>"</span>)<span>"</span></span> ;
<span>&lt;record&gt;</span>        <span>::=</span> <span><span>"</span>record(<span>"</span></span>   [ &lt;<span>derive-clause</span>&gt; <span><span>"</span>,<span>"</span></span> ] &lt;<span>record-name</span>&gt;   { <span><span>"</span>,<span>"</span></span> &lt;<span>field</span>&gt;   }<span>*</span> <span><span>"</span>)<span>"</span></span> ;
<span>&lt;datatype-name&gt;</span> <span>::=</span> &lt;<span>ident</span>&gt; ;
<span>&lt;record-name&gt;</span>   <span>::=</span> &lt;<span>ident</span>&gt; ;
<span></span>
<span>&lt;variant&gt;</span>       <span>::=</span> <span><span>"</span>(<span>"</span></span> &lt;<span>variant-name</span>&gt; { <span><span>"</span>,<span>"</span></span> &lt;<span>type</span>&gt; }<span>*</span> <span><span>"</span>)<span>"</span></span> ;
<span>&lt;field&gt;</span>         <span>::=</span> <span><span>"</span>(<span>"</span></span> &lt;<span>type</span>&gt; <span><span>"</span>,<span>"</span></span> &lt;<span>field-name</span>&gt; <span><span>"</span>)<span>"</span></span> ;
<span>&lt;variant-name&gt;</span>  <span>::=</span> &lt;<span>ident</span>&gt; ;
<span>&lt;field-name&gt;</span>    <span>::=</span> &lt;<span>ident</span>&gt; ;
<span></span>
<span>&lt;derive-clause&gt;</span> <span>::=</span> <span><span>"</span>derive(<span>"</span></span> &lt;<span>deriver-name</span>&gt; { <span><span>"</span>,<span>"</span></span> &lt;<span>deriver-name</span>&gt; }<span>*</span> <span><span>"</span>)<span>"</span></span> ;
<span>&lt;deriver-name&gt;</span>  <span>::=</span> &lt;<span>ident</span>&gt; ;
<span></span>
<span>&lt;match&gt;</span>         <span>::=</span> <span><span>"</span>match(<span>"</span></span> &lt;<span>lvalue</span>&gt; <span><span>"</span>) {<span>"</span></span> { &lt;<span>of</span>&gt; }<span>*</span> [ &lt;<span>otherwise</span>&gt; ] <span><span>"</span>}<span>"</span></span> ;
<span>&lt;matches&gt;</span>       <span>::=</span> <span><span>"</span>MATCHES(<span>"</span></span> &lt;<span>expr</span>&gt; <span><span>"</span>,<span>"</span></span> &lt;<span>ident</span>&gt; <span><span>"</span>)<span>"</span></span> ;
<span>&lt;if-let&gt;</span>        <span>::=</span> <span><span>"</span>ifLet(<span>"</span></span> &lt;<span>lvalue</span>&gt; <span><span>"</span>,<span>"</span></span> &lt;<span>variant-name</span>&gt; <span><span>"</span>,<span>"</span></span> &lt;<span>ident</span>&gt; { <span><span>"</span>,<span>"</span></span> &lt;<span>ident</span>&gt; }<span>*</span> <span><span>"</span>)<span>"</span></span> &lt;<span>stmt</span>&gt; ;
<span>&lt;of&gt;</span>            <span>::=</span> <span><span>"</span>of(<span>"</span></span> &lt;<span>variant-name</span>&gt; { <span><span>"</span>,<span>"</span></span> &lt;<span>ident</span>&gt; }<span>*</span> <span><span>"</span>)<span>"</span></span> &lt;<span>stmt</span>&gt; ;
<span>&lt;otherwise&gt;</span>     <span>::=</span> <span><span>"</span>otherwise<span>"</span></span> &lt;<span>stmt</span>&gt; ;</pre></div>
<details>
  <summary>Note: shortened vs. postfixed versions</summary>
<p dir="auto">Each listed identifier in the above grammar corresponds to a macro name defined by default -- these are called <em>shortened versions</em>. On the other hand, there are also <em>postfixed versions</em> (<code>match99</code>, <code>of99</code>, <code>derive99</code>, etc.), which are defined unconditionally. If you want to avoid name clashes caused by shortened versions, define <code>DATATYPE99_NO_ALIASES</code> before including <code>datatype99.h</code>. Library headers are strongly advised to use the postfixed macros, but without resorting to <code>DATATYPE99_NO_ALIASES</code>.</p>
</details>
<p dir="auto"><h3 tabindex="-1" dir="auto">Semantics</h3><a id="user-content-semantics" aria-label="Permalink: Semantics" href="#semantics"></a></p>
<p dir="auto">(It might be helpful to look at the <a href="https://godbolt.org/z/rebxMxW43" rel="nofollow">generated data layout</a> of <a href="https://github.com/Hirrolot/datatype99/blob/master/examples/binary_tree.c"><code>examples/binary_tree.c</code></a>.)</p>
<p dir="auto"><h4 tabindex="-1" dir="auto"><code>datatype</code></h4><a id="user-content-datatype" aria-label="Permalink: datatype" href="#datatype"></a></p>
<ol dir="auto">
<li>Before everything, the following type definition is generated:</li>
</ol>
<div data-snippet-clipboard-copy-content="typedef struct <datatype-name> <datatype-name>;"><pre><code>typedef struct &lt;datatype-name&gt; &lt;datatype-name&gt;;
</code></pre></div>
<ol start="2" dir="auto">
<li>For each non-empty variant, the following type definition is generated (the metavariable <code>&lt;type&gt;</code> ranges over a corresponding variant's types):</li>
</ol>
<div data-snippet-clipboard-copy-content="typedef struct <datatype-name><variant-name> {
    <type>0 _0;
    ...
    <type>N _N;
} <datatype-name><variant-name>;"><pre><code>typedef struct &lt;datatype-name&gt;&lt;variant-name&gt; {
    &lt;type&gt;0 _0;
    ...
    &lt;type&gt;N _N;
} &lt;datatype-name&gt;&lt;variant-name&gt;;
</code></pre></div>
<ol start="3" dir="auto">
<li>For each non-empty variant, the following type definitions to types of each field of <code>&lt;datatype-name&gt;&lt;variant-name&gt;</code> are generated:</li>
</ol>
<div data-snippet-clipboard-copy-content="typedef <type>0 <variant-name>_0;
...
typedef <type>N <variant-name>_N;"><pre><code>typedef &lt;type&gt;0 &lt;variant-name&gt;_0;
...
typedef &lt;type&gt;N &lt;variant-name&gt;_N;
</code></pre></div>
<ol start="4" dir="auto">
<li>For each variant, the following type definition to a corresponding sum type is generated:</li>
</ol>
<div data-snippet-clipboard-copy-content="typedef struct <datatype-name> <variant-name>SumT;"><pre><code>typedef struct &lt;datatype-name&gt; &lt;variant-name&gt;SumT;
</code></pre></div>
<ol start="5" dir="auto">
<li>For each sum type, the following tagged union is generated (inside the union, only fields to structures of non-empty variants are generated):</li>
</ol>
<div data-snippet-clipboard-copy-content="typedef enum <datatype-name>Tag {
    <variant-name>0Tag, ..., <variant-name>NTag
} <datatype-name>Tag;

typedef union <datatype-name>Variants {
    char dummy;

    <datatype-name><variant-name>0 <variant-name>0;
    ...
    <datatype-name><variant-name>N <variant-name>N;
} <datatype-name>Variants;

struct <datatype-name> {
    <datatype-name>Tag tag;
    <datatype-name>Variants data;
};"><pre><code>typedef enum &lt;datatype-name&gt;Tag {
    &lt;variant-name&gt;0Tag, ..., &lt;variant-name&gt;NTag
} &lt;datatype-name&gt;Tag;

typedef union &lt;datatype-name&gt;Variants {
    char dummy;

    &lt;datatype-name&gt;&lt;variant-name&gt;0 &lt;variant-name&gt;0;
    ...
    &lt;datatype-name&gt;&lt;variant-name&gt;N &lt;variant-name&gt;N;
} &lt;datatype-name&gt;Variants;

struct &lt;datatype-name&gt; {
    &lt;datatype-name&gt;Tag tag;
    &lt;datatype-name&gt;Variants data;
};
</code></pre></div>
<details>
  <summary>Note on char dummy;</summary>
<p dir="auto"><code>char dummy;</code> is needed to make the union contain at least one item, according to the standard, even if all variants are empty. Such a <code>datatype</code> would enforce strict type checking unlike plain C <code>enum</code>s.</p>
</details>
<ol start="6" dir="auto">
<li>For each variant, the following function called a <em>value constructor</em> is generated:</li>
</ol>
<div data-snippet-clipboard-copy-content="inline static <datatype-name> <variant-name>(/* ... */) { /* ... */ }"><pre><code>inline static &lt;datatype-name&gt; &lt;variant-name&gt;(/* ... */) { /* ... */ }
</code></pre></div>
<p dir="auto">If the variant has no parameters, this function will take <code>void</code> and initialise <code>.data.dummy</code> to <code>'\0'</code>; otherwise, it will take the corresponding variant parameters and initialise the result value as expected.</p>
<ol start="7" dir="auto">
<li>Now, when a sum type is fully generated, the derivation process takes place. Each deriver taken from <code>derive(...)</code> is invoked sequentially, from left to right, as</li>
</ol>
<div data-snippet-clipboard-copy-content="ML99_call(DATATYPE99_DERIVE_##<deriver-name>I, v(<datatype-name>), variants...)"><pre><code>ML99_call(DATATYPE99_DERIVE_##&lt;deriver-name&gt;I, v(&lt;datatype-name&gt;), variants...)
</code></pre></div>
<p dir="auto">where</p>
<ul dir="auto">
<li><code>&lt;deriver-name&gt;I</code> corresponds to a <a href="https://metalang99.readthedocs.io/en/latest/#definitions" rel="nofollow">Metalang99-compliant</a> macro of the form <code>#define DATATYPE99_DERIVE_##&lt;deriver-name&gt;I_IMPL(name, variants) /* ... */</code>.</li>
<li><code>variants...</code> is a <a href="https://metalang99.readthedocs.io/en/latest/list.html" rel="nofollow">list</a> of variants represented as two-place <a href="https://metalang99.readthedocs.io/en/latest/tuple.html" rel="nofollow">tuples</a>: <code>(&lt;variant-name&gt;, types...)</code>, where
<ul dir="auto">
<li><code>types...</code> is a <a href="https://metalang99.readthedocs.io/en/latest/list.html" rel="nofollow">list</a> of types of the corresponding variant.</li>
</ul>
</li>
</ul>
<p dir="auto">Put simply, a deriver is meant to generate something global for a sum type, like interface implementations or almost any other stuff. In terms of Rust, you can think of it as of the <a href="https://doc.rust-lang.org/reference/attributes/derive.html" rel="nofollow"><code>derive</code> attribute</a>.</p>
<p dir="auto"><h4 tabindex="-1" dir="auto"><code>record</code></h4><a id="user-content-record" aria-label="Permalink: record" href="#record"></a></p>
<p dir="auto"><code>record</code> represents a <em>record type</em>: it is simply a <code>struct</code> for which the derivation process is defined.</p>
<ol dir="auto">
<li>The following structure is generated:</li>
</ol>
<div data-snippet-clipboard-copy-content="typedef struct <record-name> {
    // Only if <record-name> has no fields:
    char dummy;

    <type>0 <field-name>0;
    ...
    <type>N <field-name>N;
} <record-name>;"><pre><code>typedef struct &lt;record-name&gt; {
    // Only if &lt;record-name&gt; has no fields:
    char dummy;

    &lt;type&gt;0 &lt;field-name&gt;0;
    ...
    &lt;type&gt;N &lt;field-name&gt;N;
} &lt;record-name&gt;;
</code></pre></div>
<details>
  <summary>Note on char dummy;</summary>
<p dir="auto"><code>char dummy;</code> is needed to make the structure contain at least one item, according to the standard. Such <code>record(Foo)</code> can be used to implement interfaces for it (see <a href="https://github.com/Hirrolot/interface99">Interface99</a>).</p>
</details>
<ol start="2" dir="auto">
<li>Each deriver taken from <code>derive(...)</code> is invoked sequentially, from left to right, as</li>
</ol>
<div data-snippet-clipboard-copy-content="ML99_call(DATATYPE99_RECORD_DERIVE_##<deriver-name>I, v(<record-name>), fields...)"><pre><code>ML99_call(DATATYPE99_RECORD_DERIVE_##&lt;deriver-name&gt;I, v(&lt;record-name&gt;), fields...)
</code></pre></div>
<p dir="auto">where</p>
<ul dir="auto">
<li><code>&lt;deriver-name&gt;I</code> corresponds to a <a href="https://metalang99.readthedocs.io/en/latest/#definitions" rel="nofollow">Metalang99-compliant</a> macro of the form <code>#define DATATYPE99_RECORD_DERIVE_##&lt;deriver-name&gt;I_IMPL(name, fields) /* ... */</code>.</li>
<li><code>fields...</code> is a <a href="https://metalang99.readthedocs.io/en/latest/list.html" rel="nofollow">list</a> of fields represented as two-place <a href="https://metalang99.readthedocs.io/en/latest/tuple.html" rel="nofollow">tuples</a>: <code>(&lt;type&gt;, &lt;field-name&gt;)</code>. If a record contains no fields, the list would consist only of <code>(char, dummy)</code>.</li>
</ul>
<p dir="auto"><h4 tabindex="-1" dir="auto"><code>match</code></h4><a id="user-content-match" aria-label="Permalink: match" href="#match"></a></p>
<p dir="auto"><code>match</code> has the expected semantics: it sequentially tries to match the given instance of a sum type against the given variants, and, if a match has succeeded, it executes the corresponding statement and moves down to the next instruction (<code>match(val) { ... } next-instruction;</code>). If all the matches have failed, it executes the statement after <code>otherwise</code> and moves down to the next instruction.</p>
<p dir="auto">A complete <code>match</code> construct results in a single C statement.</p>
<p dir="auto"><h4 tabindex="-1" dir="auto"><code>of</code></h4><a id="user-content-of" aria-label="Permalink: of" href="#of"></a></p>
<p dir="auto"><code>of</code> accepts a matched variant name as a first argument and the rest of arguments comprise a comma-separated list of bindings.</p>
<ul dir="auto">
<li>A binding equal to <code>_</code> is ignored.</li>
<li>A binding <strong>not</strong> equal to <code>_</code> stands for a pointer to a corresponding data of the variant (e.g., let there be <code>(Foo, T1, T2)</code> and <code>of(Foo, x, y)</code>, then <code>x</code> has the type <code>T1 *</code> and <code>y</code> is <code>T2 *</code>).</li>
</ul>
<p dir="auto">There can be more than one <code>_</code> binding, however, non-<code>_</code> bindings must be distinct.</p>
<p dir="auto">To match an empty variant, write <code>of(Bar)</code>.</p>
<p dir="auto"><h4 tabindex="-1" dir="auto"><code>MATCHES</code></h4><a id="user-content-matches" aria-label="Permalink: MATCHES" href="#matches"></a></p>
<p dir="auto"><code>MATCHES</code> just tests an instance of a sum type for a given variant. If the given instance corresponds to the given variant, it expands to truthfulness, otherwise it expands to falsehood.</p>
<p dir="auto"><h4 tabindex="-1" dir="auto"><code>matches</code></h4><a id="user-content-matches-1" aria-label="Permalink: matches" href="#matches-1"></a></p>
<p dir="auto"><strong>DEPRECATED</strong>: use <a href="#MATCHES"><code>MATCHES</code></a> instead.</p>
<p dir="auto"><h4 tabindex="-1" dir="auto"><code>ifLet</code></h4><a id="user-content-iflet" aria-label="Permalink: ifLet" href="#iflet"></a></p>
<p dir="auto"><code>ifLet</code> tries to match the given instance of a sum type against the given variant, and, if a match has succeeded, it executes the corresponding statement.</p>
<p dir="auto">Think of <code>ifLet(&lt;expr&gt;, &lt;variant-name&gt;, vars...) { /* ... */ }</code> as of an abbreviation of</p>
<div data-snippet-clipboard-copy-content="match(<expr>) {
    of(<variant-name>, vars...) { /* ... */ }
    otherwise {}
}"><pre><code>match(&lt;expr&gt;) {
    of(&lt;variant-name&gt;, vars...) { /* ... */ }
    otherwise {}
}
</code></pre></div>
<p dir="auto">A complete <code>ifLet</code> construct results in a single C statement.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Unit type</h2><a id="user-content-unit-type" aria-label="Permalink: Unit type" href="#unit-type"></a></p>
<p dir="auto">The unit type <code>UnitT99</code> represents the type of a single value, <code>unit_v99</code> (it should not be assigned to anything else). These are defined as follows:</p>
<div dir="auto" data-snippet-clipboard-copy-content="typedef char UnitT99;
static const UnitT99 unit_v99 = '\0';"><pre><span>typedef</span> <span>char</span> <span>UnitT99</span>;
<span>static</span> <span>const</span> <span>UnitT99</span> <span>unit_v99</span> <span>=</span> <span>'\0'</span>;</pre></div>
<p dir="auto">If <code>DATATYPE99_NO_ALIASES</code> remains undefined prior to <code>#include &lt;datatype99.h&gt;</code>, <code>UnitT99</code> and <code>unit_v99</code> are also accessible through object-like macros <code>UnitT</code> &amp; <code>unit_v</code>.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Derive helper attributes</h2><a id="user-content-derive-helper-attributes" aria-label="Permalink: Derive helper attributes" href="#derive-helper-attributes"></a></p>
<p dir="auto">You can pass named arguments to a deriver; these are called <em>derive helper attributes</em>. They must be specified as object-like macros of the form:</p>
<div data-snippet-clipboard-copy-content="#define <variant-name>_<namespace>_<attribute-name> attr(/* attribute value */)"><pre><code>#define &lt;variant-name&gt;_&lt;namespace&gt;_&lt;attribute-name&gt; attr(/* attribute value */)
</code></pre></div>
<p dir="auto">where <code>&lt;namespace&gt;</code> is either <code>&lt;datatype-name&gt;</code>/<code>&lt;record-name&gt;</code> or <code>&lt;variant-name&gt;</code>/<code>&lt;field-name&gt;</code> for <code>datatype</code>/<code>record</code>-specific and variant/field-specific attributes, respectively.</p>
<p dir="auto">To manipulate derive helper attributes, there are a few predefined macros:</p>
<ul dir="auto">
<li>
<p dir="auto"><code>DATATYPE99_attrIsPresent</code>/<code>DATATYPE99_ATTR_IS_PRESENT</code></p>
<p dir="auto">Accepts an attribute name and checks if it is present or not. It can be used to check the presence of an optional attribute.</p>
</li>
<li>
<p dir="auto"><code>DATATYPE99_attrValue</code>/<code>DATATYPE99_ATTR_VALUE</code></p>
<p dir="auto">Accepts an attribute name extracts its value. A provided attribute <strong>must</strong> be present.</p>
</li>
<li>
<p dir="auto"><code>DATATYPE99_assertAttrIsPresent</code></p>
<p dir="auto">Accepts an attribute name and emits a fatal error if the attribute is not present, otherwise results in emptiness. It can be used for mandatory attributes.</p>
</li>
</ul>
<p dir="auto">(The naming convention here is the same <a href="https://metalang99.readthedocs.io/en/latest/#naming-conventions" rel="nofollow">as of Metalang99</a>.)</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Miscellaneous</h2><a id="user-content-miscellaneous" aria-label="Permalink: Miscellaneous" href="#miscellaneous"></a></p>
<ul dir="auto">
<li>
<p dir="auto">The macros <code>DATATYPE99_MAJOR</code>, <code>DATATYPE99_MINOR</code>, <code>DATATYPE99_PATCH</code>, <code>DATATYPE99_VERSION_COMPATIBLE(x, y, z)</code>, and <code>DATATYPE99_VERSION_EQ(x, y, z)</code> have the <a href="https://metalang99.readthedocs.io/en/latest/#version-manipulation-macros" rel="nofollow">same semantics as of Metalang99</a>.</p>
</li>
<li>
<p dir="auto">For each macro using <code>ML99_EVAL</code>, Datatype99 provides its <a href="https://metalang99.readthedocs.io/en/latest/#definitions" rel="nofollow">Metalang99-compliant</a> counterpart which can be used inside derivers and other Metalang99-compliant macros:</p>
</li>
</ul>
<table>
<thead>
<tr>
<th>Macro</th>
<th>Metalang99-compliant counterpart</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>datatype</code></td>
<td><code>DATATYPE99_datatype</code></td>
</tr>
<tr>
<td><code>record</code></td>
<td><code>DATATYPE99_record</code></td>
</tr>
<tr>
<td><code>of</code></td>
<td><code>DATATYPE99_of</code></td>
</tr>
<tr>
<td><code>ifLet</code></td>
<td><code>DATATYPE99_ifLet</code></td>
</tr>
</tbody>
</table>
<p dir="auto">(An <a href="https://hirrolot.gitbook.io/metalang99/partial-application" rel="nofollow">arity specifier</a> and <a href="https://metalang99.readthedocs.io/en/latest/#definitions" rel="nofollow">desugaring macro</a> are provided for each of the above macros.)</p>
<ul dir="auto">
<li>There is a built-in deriver <code>dummy</code> which generates nothing. It is defined both for record and sum types.</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Guidelines</h2><a id="user-content-guidelines" aria-label="Permalink: Guidelines" href="#guidelines"></a></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Clang-Format issues</h3><a id="user-content-clang-format-issues" aria-label="Permalink: Clang-Format issues" href="#clang-format-issues"></a></p>
<p dir="auto">If you use <a href="https://clang.llvm.org/docs/ClangFormatStyleOptions.html" rel="nofollow">Clang-Format</a>, cancel formatting for a <code>datatype</code> definition using <code>// clang-format off</code> &amp; <code>// clang-format on</code> to make it look prettier, as in the examples.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto"><code>#undef</code> derive helper attributes</h3><a id="user-content-undef-derive-helper-attributes" aria-label="Permalink: #undef derive helper attributes" href="#undef-derive-helper-attributes"></a></p>
<p dir="auto">Always <code>#undef</code> derive helper attributes after a corresponding <code>datatype</code> definition not to pollute your namespace.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Descriptive names</h3><a id="user-content-descriptive-names" aria-label="Permalink: Descriptive names" href="#descriptive-names"></a></p>
<p dir="auto">If the meaning of variant parameters is not clear from the context, give them descriptive names. This can be achieved in several ways:</p>
<div dir="auto" data-snippet-clipboard-copy-content="// 1. Define type aliases to variant parameters.
typedef double XCoordinate;
typedef double YCoordinate;

typedef double Width;
typedef double Height;

datatype(
    Shape,
    (Point, XCoordinate, YCoordinate),
    (Rectangle, Width, Height)
);

// 2. Define separate structures.
typedef struct {
    double x, y;
} Point;

typedef struct {
    double width, height;
} Rectangle;

datatype(
    Shape,
    (MkPoint, Point),
    (MkRectangle, Rectangle)
);"><pre><span>// 1. Define type aliases to variant parameters.</span>
<span>typedef</span> <span>double</span> <span>XCoordinate</span>;
<span>typedef</span> <span>double</span> <span>YCoordinate</span>;

<span>typedef</span> <span>double</span> <span>Width</span>;
<span>typedef</span> <span>double</span> <span>Height</span>;

<span>datatype</span>(
    <span>Shape</span>,
    (<span>Point</span>, <span>XCoordinate</span>, <span>YCoordinate</span>),
    (<span>Rectangle</span>, <span>Width</span>, <span>Height</span>)
);

<span>// 2. Define separate structures.</span>
<span>typedef</span> <span>struct</span> {
    <span>double</span> <span>x</span>, <span>y</span>;
} <span>Point</span>;

<span>typedef</span> <span>struct</span> {
    <span>double</span> <span>width</span>, <span>height</span>;
} <span>Rectangle</span>;

<span>datatype</span>(
    <span>Shape</span>,
    (<span>MkPoint</span>, <span>Point</span>),
    (<span>MkRectangle</span>, <span>Rectangle</span>)
);</pre></div>
<p dir="auto">Comparison:</p>
<ul dir="auto">
<li>The former option has more concise syntax: <code>MkPoint(x, y)</code> instead of <code>MkPoint((Point){x, y})</code>.</li>
<li>The latter option is more appropriate when the structures are to be used separately from the containing sum type.</li>
<li>The latter option allows for more graduate control over the data layout: you can accompain the structures with compiler-specific attributes, alignment properties like <code>__attribute__ ((__packed__))</code>, etc.</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Pitfalls</h2><a id="user-content-pitfalls" aria-label="Permalink: Pitfalls" href="#pitfalls"></a></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Top-level <code>break</code>/<code>continue</code></h3><a id="user-content-top-level-breakcontinue" aria-label="Permalink: Top-level break/continue" href="#top-level-breakcontinue"></a></p>
<p dir="auto">Do <strong>not</strong> use <code>break</code>/<code>continue</code> inside a statement provided to <code>of</code>/<code>ifLet</code> but outside of any <code>for</code>/<code>while</code> loops in that statement. For example, this code is fine:</p>
<div dir="auto" data-snippet-clipboard-copy-content="match(x) {
    of(Foo, a, b, c) {
        for (int i = 0; i < 10; i++) {
            continue;
        }
    }
}"><pre><span>match</span>(<span>x</span>) {
    <span>of</span>(<span>Foo</span>, <span>a</span>, <span>b</span>, <span>c</span>) {
        <span>for</span> (<span>int</span> <span>i</span> <span>=</span> <span>0</span>; <span>i</span> <span>&lt;</span> <span>10</span>; <span>i</span><span>++</span>) {
            <span>continue</span>;
        }
    }
}</pre></div>
<p dir="auto">But this code is <strong>not</strong> fine:</p>
<div dir="auto" data-snippet-clipboard-copy-content="for (int i = 0; i < 10; i++) {
    match(x) {
        of(Foo, a, b, c) {
            if (a == 7) { break; }
            continue;
        }
    }
}"><pre><span>for</span> (<span>int</span> <span>i</span> <span>=</span> <span>0</span>; <span>i</span> <span>&lt;</span> <span>10</span>; <span>i</span><span>++</span>) {
    <span>match</span>(<span>x</span>) {
        <span>of</span>(<span>Foo</span>, <span>a</span>, <span>b</span>, <span>c</span>) {
            <span>if</span> (<span>a</span> <span>==</span> <span>7</span>) { <span>break</span>; }
            <span>continue</span>;
        }
    }
}</pre></div>
<p dir="auto">To make it valid, you can rewrite it as follows:</p>
<div dir="auto" data-snippet-clipboard-copy-content="for (int i = 0; i < 10; i++) {
    match(x) {
        of(Foo, a, b, c) {
            if (a == 7) { goto my_break; }
            goto my_continue;
        }
    }

    // Datatype99 prohibits top-level `break`/`continue`.
    my_continue:;
}
my_break:;"><pre><span>for</span> (<span>int</span> <span>i</span> <span>=</span> <span>0</span>; <span>i</span> <span>&lt;</span> <span>10</span>; <span>i</span><span>++</span>) {
    <span>match</span>(<span>x</span>) {
        <span>of</span>(<span>Foo</span>, <span>a</span>, <span>b</span>, <span>c</span>) {
            <span>if</span> (<span>a</span> <span>==</span> <span>7</span>) { goto my_break; }
            goto my_continue;
        }
    }

    <span>// Datatype99 prohibits top-level `break`/`continue`.</span>
    my_continue:;
}
my_break:;</pre></div>
<p dir="auto"><h3 tabindex="-1" dir="auto">Array as a variant parameter</h3><a id="user-content-array-as-a-variant-parameter" aria-label="Permalink: Array as a variant parameter" href="#array-as-a-variant-parameter"></a></p>
<p dir="auto">To specify an array as a variant parameter, you must put it into a separate <code>struct</code>; see <a href="https://github.com/Hirrolot/datatype99/blob/master/examples/array_in_variant.c"><code>examples/array_in_variant.c</code></a>.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Mutable bindings</h3><a id="user-content-mutable-bindings" aria-label="Permalink: Mutable bindings" href="#mutable-bindings"></a></p>
<p dir="auto">Bindings introduced by <code>of</code> are <strong>always</strong> mutable, so make sure you do <strong>not</strong> mutate them if the value passed to <code>match</code> is qualified as <code>const</code>.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Credits</h2><a id="user-content-credits" aria-label="Permalink: Credits" href="#credits"></a></p>
<p dir="auto">Thanks to Rust and ML for their implementations of sum types.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Publications</h2><a id="user-content-publications" aria-label="Permalink: Publications" href="#publications"></a></p>
<ul dir="auto">
<li><a href="https://hirrolot.github.io/posts/pretty-printable-enumerations-in-pure-c.html" rel="nofollow"><em>Pretty-Printable Enumerations in Pure C</em></a> by Hirrolot.</li>
<li><a href="https://hirrolot.github.io/posts/whats-the-point-of-the-c-preprocessor-actually.html" rel="nofollow"><em>What’s the Point of the C Preprocessor, Actually?</em></a> by Hirrolot.</li>
<li><a href="https://hirrolot.github.io/posts/macros-on-steroids-or-how-can-pure-c-benefit-from-metaprogramming.html" rel="nofollow"><em>Macros on Steroids, Or: How Can Pure C Benefit From Metaprogramming</em></a> by Hirrolot.</li>
<li><a href="https://hirrolot.github.io/posts/extend-your-language-dont-alter-it.html" rel="nofollow"><em>Extend Your Language, Don’t Alter It</em></a> by Hirrolot.</li>
<li><a href="https://hirrolot.github.io/posts/compiling-algebraic-data-types-in-pure-c99.html" rel="nofollow"><em>Compiling Algebraic Data Types in Pure C99</em></a> by Hirrolot.</li>
<li><a href="https://www.reddit.com/r/ProgrammingLanguages/comments/nc1o18/comparing_algebraic_data_types_rust_and_datatype99/" rel="nofollow"><em>Comparing Rust and Datatype99</em></a> by Hirrolot.</li>
<li><a href="https://hirrolot.github.io/posts/compile-time-introspection-of-sum-types-in-pure-c99.html" rel="nofollow"><em>Compile-Time Introspection of Sum Types in Pure C99</em></a> by Hirrolot.</li>
<li><a href="https://hirrolot.github.io/posts/unleashing-sum-types-in-pure-c99.html" rel="nofollow"><em>Unleashing Sum Types in Pure C99</em></a> by Hirrolot.</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Release procedure</h2><a id="user-content-release-procedure" aria-label="Permalink: Release procedure" href="#release-procedure"></a></p>
<ol dir="auto">
<li>Update <code>DATATYPE99_MAJOR</code>, <code>DATATYPE99_MINOR</code>, and <code>DATATYPE99_PATCH</code> in <code>datatype99.h</code>.</li>
<li>Update <code>CHANGELOG.md</code>.</li>
<li>Release the project in <a href="https://github.com/Hirrolot/datatype99/releases">GitHub Releases</a>.</li>
</ol>
<p dir="auto"><h2 tabindex="-1" dir="auto">FAQ</h2><a id="user-content-faq" aria-label="Permalink: FAQ" href="#faq"></a></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Q: Why use C instead of Rust/Zig/whatever else?</h3><a id="user-content-q-why-use-c-instead-of-rustzigwhatever-else" aria-label="Permalink: Q: Why use C instead of Rust/Zig/whatever else?" href="#q-why-use-c-instead-of-rustzigwhatever-else"></a></p>
<p dir="auto">A: There is a lot of software written in plain C that can benefit from Datatype99; C is #1 programming language as of 2020, <a href="https://jaxenter.com/c-programming-may-2020-171598.html" rel="nofollow">according to TIOBE</a>. People use C due to technical and social reasons:</p>
<ul dir="auto">
<li>
<p dir="auto">Datatype99 can be seamlessly integrated into existing codebases written in pure C -- just <code>#include &lt;datatype99.h&gt;</code> and you are ready to go. On the other hand, other languages force you to separate native C files from their sources, which is clearly less convenient.</p>
</li>
<li>
<p dir="auto">In some environments, developers strick to pure C for historical reasons (e.g., embedded devices, Linux and other operating systems).</p>
</li>
<li>
<p dir="auto">C has a stable ABI which is vital for some projects (e.g., plugin systems such as <a href="https://github.com/metacall/core">MetaCall</a>).</p>
</li>
<li>
<p dir="auto">C is a mature language with a complete specification and a plenitude of libraries. Rust has no complete specification, and <a href="https://ziglang.org/" rel="nofollow">Zig</a> is not yet production-ready. I know a few stories when these two languages were rejected for new projects, and I can understand this decision.</p>
</li>
<li>
<p dir="auto">Historically, C has been targeting nearly all platforms. This is not the case with Rust, which depends on LLVM as for now.</p>
</li>
<li>
<p dir="auto">Your company obligates you to use C.</p>
</li>
<li>
<p dir="auto">Etc.</p>
</li>
</ul>
<p dir="auto">See also:</p>
<ul dir="auto">
<li><a href="https://drewdevault.com/2019/03/25/Rust-is-not-a-good-C-replacement.html" rel="nofollow"><em>"Rust is not a good C replacement"</em></a> by Drew DeVault.</li>
</ul>
<p dir="auto">Overall, if you can afford a more modern/high-level language, I encourage you to do so instead of using old C. However, many people do not have this possibility (or it would be too costly).</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Q: Why not third-party code generators?</h3><a id="user-content-q-why-not-third-party-code-generators" aria-label="Permalink: Q: Why not third-party code generators?" href="#q-why-not-third-party-code-generators"></a></p>
<p dir="auto">A: See <a href="https://github.com/Hirrolot/metalang99#q-why-not-third-party-code-generators">Metalang99's README &gt;&gt;</a>.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Q: How does it work?</h3><a id="user-content-q-how-does-it-work" aria-label="Permalink: Q: How does it work?" href="#q-how-does-it-work"></a></p>
<p dir="auto">A: In short, <code>datatype</code> expands to a tagged union with value constructors; <code>match</code> expands to a switch statement. To generate all this stuff, <a href="https://github.com/Hirrolot/metalang99">Metalang99</a> is used, a preprocessor metaprogramming library.</p>
<p dir="auto">More on it in <a href="https://hirrolot.github.io/posts/compiling-algebraic-data-types-in-pure-c99.html" rel="nofollow"><em>"Compiling Algebraic Data Types in Pure C99"</em></a>.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Q: Does it work on C++?</h3><a id="user-content-q-does-it-work-on-c" aria-label="Permalink: Q: Does it work on C++?" href="#q-does-it-work-on-c"></a></p>
<p dir="auto">A: Yes, C++11 and onwards is supported.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Q: What is the difference between Datatype99 and Metalang99?</h3><a id="user-content-q-what-is-the-difference-between-datatype99-and-metalang99" aria-label="Permalink: Q: What is the difference between Datatype99 and Metalang99?" href="#q-what-is-the-difference-between-datatype99-and-metalang99"></a></p>
<p dir="auto">A: <a href="https://github.com/Hirrolot/metalang99">Metalang99</a> is a functional language for metaprogramming, whereas Datatype99 is an implementation of algebraic data types written in this language.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Q: What about compile-time errors?</h3><a id="user-content-q-what-about-compile-time-errors" aria-label="Permalink: Q: What about compile-time errors?" href="#q-what-about-compile-time-errors"></a></p>
<p dir="auto">A: Some kinds of syntactic errors are detected by the library itself:</p>
<p dir="auto"><h4 tabindex="-1" dir="auto">Error: <code>Bar(int)</code> instead of <code>(Bar, int)</code></h4><a id="user-content-error-barint-instead-of-bar-int" aria-label="Permalink: Error: Bar(int) instead of (Bar, int)" href="#error-barint-instead-of-bar-int"></a></p>
<p dir="auto">[<code>playground.c</code>]</p>
<div dir="auto" data-snippet-clipboard-copy-content="datatype(A, (Foo, int), Bar(int));"><pre><span>datatype</span>(<span>A</span>, (<span>Foo</span>, <span>int</span>), <span>Bar</span>(<span>int</span>));</pre></div>
<p dir="auto">[<code>/bin/sh</code>]</p>
<div data-snippet-clipboard-copy-content="$ gcc playground.c -Imetalang99/include -Idatatype99 -ftrack-macro-expansion=0
playground.c:3:1: error: static assertion failed: &quot;ML99_assertIsTuple: Bar(int) must be (x1, ..., xN)&quot;
    3 | datatype(A, (Foo, int), Bar(int));
      | ^~~~~~~~"><pre><code>$ gcc playground.c -Imetalang99/include -Idatatype99 -ftrack-macro-expansion=0
playground.c:3:1: error: static assertion failed: "ML99_assertIsTuple: Bar(int) must be (x1, ..., xN)"
    3 | datatype(A, (Foo, int), Bar(int));
      | ^~~~~~~~
</code></pre></div>
<hr>
<p dir="auto"><h4 tabindex="-1" dir="auto">Error: Missing comma</h4><a id="user-content-error-missing-comma" aria-label="Permalink: Error: Missing comma" href="#error-missing-comma"></a></p>
<p dir="auto">[<code>playground.c</code>]</p>
<div dir="auto" data-snippet-clipboard-copy-content="datatype(A, (Foo, int) (Bar, int));"><pre><span>datatype</span>(<span>A</span>, (<span>Foo</span>, <span>int</span>) (<span>Bar</span>, <span>int</span>));</pre></div>
<p dir="auto">[<code>/bin/sh</code>]</p>
<div data-snippet-clipboard-copy-content="$ gcc playground.c -Imetalang99/include -Idatatype99 -ftrack-macro-expansion=0
playground.c:3:1: error: static assertion failed: &quot;ML99_assertIsTuple: (Foo, int) (Bar, int) must be (x1, ..., xN), did you miss a comma?&quot;
    3 | datatype(A, (Foo, int) (Bar, int));
      | ^~~~~~~~"><pre><code>$ gcc playground.c -Imetalang99/include -Idatatype99 -ftrack-macro-expansion=0
playground.c:3:1: error: static assertion failed: "ML99_assertIsTuple: (Foo, int) (Bar, int) must be (x1, ..., xN), did you miss a comma?"
    3 | datatype(A, (Foo, int) (Bar, int));
      | ^~~~~~~~
</code></pre></div>
<hr>
<p dir="auto"><h4 tabindex="-1" dir="auto">Error: Trailing comma is prohibited</h4><a id="user-content-error-trailing-comma-is-prohibited" aria-label="Permalink: Error: Trailing comma is prohibited" href="#error-trailing-comma-is-prohibited"></a></p>
<p dir="auto">[<code>playground.c</code>]</p>
<div dir="auto" data-snippet-clipboard-copy-content="datatype(A, (Foo, int), (Bar, int), /* trailing comma is prohibited */);"><pre><span>datatype</span>(<span>A</span>, (<span>Foo</span>, <span>int</span>), (<span>Bar</span>, <span>int</span>), <span>/* trailing comma is prohibited */</span><span></span>);</pre></div>
<p dir="auto">[<code>/bin/sh</code>]</p>
<div data-snippet-clipboard-copy-content="$ gcc playground.c -Imetalang99/include -Idatatype99 -ftrack-macro-expansion=0
playground.c:3:1: error: static assertion failed: &quot;ML99_assertIsTuple: must be (x1, ..., xN)&quot;
    3 | datatype(A, (Foo, int), (Bar, int), /* trailing comma is prohibited */);
      | ^~~~~~~~"><pre><code>$ gcc playground.c -Imetalang99/include -Idatatype99 -ftrack-macro-expansion=0
playground.c:3:1: error: static assertion failed: "ML99_assertIsTuple: must be (x1, ..., xN)"
    3 | datatype(A, (Foo, int), (Bar, int), /* trailing comma is prohibited */);
      | ^~~~~~~~
</code></pre></div>
<p dir="auto">(For better diagnostics, use the latest Metalang99.)</p>
<p dir="auto">The others are understandable as well:</p>
<p dir="auto"><h4 tabindex="-1" dir="auto">Error: unknown type name specified in <code>datatype</code></h4><a id="user-content-error-unknown-type-name-specified-in-datatype" aria-label="Permalink: Error: unknown type name specified in datatype" href="#error-unknown-type-name-specified-in-datatype"></a></p>
<p dir="auto">[<code>playground.c</code>]</p>
<div dir="auto" data-snippet-clipboard-copy-content="datatype(Foo, (FooA, NonExistingType));"><pre><span>datatype</span>(<span>Foo</span>, (<span>FooA</span>, <span>NonExistingType</span>));</pre></div>
<p dir="auto">[<code>/bin/sh</code>]</p>
<div data-snippet-clipboard-copy-content="playground.c:3:1: error: unknown type name ‘NonExistingType’
    3 | datatype(
      | ^~~~~~~~
playground.c:3:1: error: unknown type name ‘NonExistingType’
playground.c:3:1: error: unknown type name ‘NonExistingType’"><pre><code>playground.c:3:1: error: unknown type name ‘NonExistingType’
    3 | datatype(
      | ^~~~~~~~
playground.c:3:1: error: unknown type name ‘NonExistingType’
playground.c:3:1: error: unknown type name ‘NonExistingType’
</code></pre></div>
<hr>
<p dir="auto"><h4 tabindex="-1" dir="auto">Error: non-exhaustive <code>match</code></h4><a id="user-content-error-non-exhaustive-match" aria-label="Permalink: Error: non-exhaustive match" href="#error-non-exhaustive-match"></a></p>
<p dir="auto">[<code>playground.c</code>]</p>
<div dir="auto" data-snippet-clipboard-copy-content="match(*tree) {
    of(Leaf, x) return *x;
    // of(Node, lhs, x, rhs) return sum(*lhs) + *x + sum(*rhs);
}"><pre><span>match</span>(<span>*</span><span>tree</span>) {
    <span>of</span>(<span>Leaf</span>, <span>x</span>) <span>return</span> <span>*</span><span>x</span>;
    <span>// of(Node, lhs, x, rhs) return sum(*lhs) + *x + sum(*rhs);</span>
}</pre></div>
<p dir="auto">[<code>/bin/sh</code>]</p>
<div data-snippet-clipboard-copy-content="playground.c: In function ‘sum’:
playground.c:6:5: warning: enumeration value ‘NodeTag’ not handled in switch [-Wswitch]
    6 |     match(*tree) {
      |     ^~~~~"><pre><code>playground.c: In function ‘sum’:
playground.c:6:5: warning: enumeration value ‘NodeTag’ not handled in switch [-Wswitch]
    6 |     match(*tree) {
      |     ^~~~~
</code></pre></div>
<hr>
<p dir="auto"><h4 tabindex="-1" dir="auto">Error: excess binders in <code>of</code></h4><a id="user-content-error-excess-binders-in-of" aria-label="Permalink: Error: excess binders in of" href="#error-excess-binders-in-of"></a></p>
<p dir="auto">[<code>playground.c</code>]</p>
<div dir="auto" data-snippet-clipboard-copy-content="match(*tree) {
    of(Leaf, x, excess) return *x;
    of(Node, lhs, x, rhs) return sum(*lhs) + *x + sum(*rhs);
}"><pre><span>match</span>(<span>*</span><span>tree</span>) {
    <span>of</span>(<span>Leaf</span>, <span>x</span>, <span>excess</span>) <span>return</span> <span>*</span><span>x</span>;
    <span>of</span>(<span>Node</span>, <span>lhs</span>, <span>x</span>, <span>rhs</span>) <span>return</span> <span>sum</span>(<span>*</span><span>lhs</span>) <span>+</span> <span>*</span><span>x</span> <span>+</span> <span>sum</span>(<span>*</span><span>rhs</span>);
}</pre></div>
<p dir="auto">[<code>/bin/sh</code>]</p>
<div data-snippet-clipboard-copy-content="playground.c: In function ‘sum’:
playground.c:15:9: error: unknown type name ‘Leaf_1’; did you mean ‘Leaf_0’?
   15 |         of(Leaf, x, excess) return *x;
      |         ^~
      |         Leaf_0
playground.c:15:9: error: ‘BinaryTreeLeaf’ has no member named ‘_1’; did you mean ‘_0’?
   15 |         of(Leaf, x, excess) return *x;
      |         ^~
      |         _0"><pre><code>playground.c: In function ‘sum’:
playground.c:15:9: error: unknown type name ‘Leaf_1’; did you mean ‘Leaf_0’?
   15 |         of(Leaf, x, excess) return *x;
      |         ^~
      |         Leaf_0
playground.c:15:9: error: ‘BinaryTreeLeaf’ has no member named ‘_1’; did you mean ‘_0’?
   15 |         of(Leaf, x, excess) return *x;
      |         ^~
      |         _0
</code></pre></div>
<hr>
<p dir="auto"><h4 tabindex="-1" dir="auto">Error: improperly typed variant arguments</h4><a id="user-content-error-improperly-typed-variant-arguments" aria-label="Permalink: Error: improperly typed variant arguments" href="#error-improperly-typed-variant-arguments"></a></p>
<p dir="auto">[<code>playground.c</code>]</p>
<div dir="auto" data-snippet-clipboard-copy-content="BinaryTree tree = Leaf(&quot;hello world&quot;);"><pre><span>BinaryTree</span> <span>tree</span> <span>=</span> <span>Leaf</span>(<span>"hello world"</span>);</pre></div>
<p dir="auto">[<code>/bin/sh</code>]</p>
<div data-snippet-clipboard-copy-content="playground.c: In function ‘main’:
playground.c:18:28: warning: passing argument 1 of ‘Leaf’ makes integer from pointer without a cast [-Wint-conversion]
   18 |     BinaryTree tree = Leaf(&quot;hello world&quot;);
      |                            ^~~~~~~~~~~~~
      |                            |
      |                            char *
playground.c:6:1: note: expected ‘int’ but argument is of type ‘char *’
    6 | datatype(
      | ^~~~~~~~"><pre><code>playground.c: In function ‘main’:
playground.c:18:28: warning: passing argument 1 of ‘Leaf’ makes integer from pointer without a cast [-Wint-conversion]
   18 |     BinaryTree tree = Leaf("hello world");
      |                            ^~~~~~~~~~~~~
      |                            |
      |                            char *
playground.c:6:1: note: expected ‘int’ but argument is of type ‘char *’
    6 | datatype(
      | ^~~~~~~~
</code></pre></div>
<hr>
<p dir="auto"><h4 tabindex="-1" dir="auto">Error: an undereferenced binder</h4><a id="user-content-error-an-undereferenced-binder" aria-label="Permalink: Error: an undereferenced binder" href="#error-an-undereferenced-binder"></a></p>
<p dir="auto">[<code>playground.c</code>]</p>
<div dir="auto" data-snippet-clipboard-copy-content="int sum(const BinaryTree *tree) {
    match(*tree) {
        of(Leaf, x) return x; // x is int *
        of(Node, lhs, x, rhs) return sum(*lhs) + *x + sum(*rhs);
    }
}"><pre><span>int</span> <span>sum</span>(<span>const</span> <span>BinaryTree</span> <span>*</span><span>tree</span>) {
    <span>match</span>(<span>*</span><span>tree</span>) {
        <span>of</span>(<span>Leaf</span>, <span>x</span>) <span>return</span> <span>x</span>; <span>// x is int *</span>
        <span>of</span>(<span>Node</span>, <span>lhs</span>, <span>x</span>, <span>rhs</span>) <span>return</span> <span>sum</span>(<span>*</span><span>lhs</span>) <span>+</span> <span>*</span><span>x</span> <span>+</span> <span>sum</span>(<span>*</span><span>rhs</span>);
    }
}</pre></div>
<p dir="auto">[<code>/bin/sh</code>]</p>
<div data-snippet-clipboard-copy-content="playground.c: In function ‘sum’:
playground.c:17:28: warning: returning ‘Leaf_0 *’ {aka ‘int *’} from a function with return type ‘int’ makes integer from pointer without a cast [-Wint-conversion]
   17 |         of(Leaf, x) return x; // x is int *
      |                            ^"><pre><code>playground.c: In function ‘sum’:
playground.c:17:28: warning: returning ‘Leaf_0 *’ {aka ‘int *’} from a function with return type ‘int’ makes integer from pointer without a cast [-Wint-conversion]
   17 |         of(Leaf, x) return x; // x is int *
      |                            ^
</code></pre></div>
<hr>
<p dir="auto">From my experience, nearly 95% of errors make sense.</p>
<p dir="auto">If an error is not comprehensible at all, try to look at generated code (<code>-E</code>). Hopefully, the <a href="#semantics">code generation semantics</a> is formally defined so normally you will not see something unexpected.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Q: What about IDE support?</h3><a id="user-content-q-what-about-ide-support" aria-label="Permalink: Q: What about IDE support?" href="#q-what-about-ide-support"></a></p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/Hirrolot/datatype99/blob/master/images/suggestion.png"><img src="https://github.com/Hirrolot/datatype99/raw/master/images/suggestion.png" width="600px"></a></p>
<p dir="auto">A: VS Code automatically enables suggestions of generated types but, of course, it does not support macro syntax highlighting.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Q: Which compilers are tested?</h3><a id="user-content-q-which-compilers-are-tested" aria-label="Permalink: Q: Which compilers are tested?" href="#q-which-compilers-are-tested"></a></p>
<p dir="auto">A: Datatype99 is known to work on these compilers:</p>
<ul dir="auto">
<li>GCC</li>
<li>Clang</li>
<li>MSVC</li>
<li>TCC</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Troubleshooting</h2><a id="user-content-troubleshooting" aria-label="Permalink: Troubleshooting" href="#troubleshooting"></a></p>
<p dir="auto"><h3 tabindex="-1" dir="auto"><code>warning: control reaches end of non-void function [-Wreturn-type]</code></h3><a id="user-content-warning-control-reaches-end-of-non-void-function--wreturn-type" aria-label="Permalink: warning: control reaches end of non-void function [-Wreturn-type]" href="#warning-control-reaches-end-of-non-void-function--wreturn-type"></a></p>
<p dir="auto">This warning happens when you try to return control from within a <code>match</code> statement, and your compiler thinks that not all hypothetical variants are handled. For example:</p>
<div dir="auto" data-snippet-clipboard-copy-content="datatype(MyType, (Foo), (Bar));

int handle(MyType val) {
    match(val) {
        of(Foo) return 5;
        of(Bar) return 7;
    }
}"><pre><span>datatype</span>(<span>MyType</span>, (<span>Foo</span>), (<span>Bar</span>));

<span>int</span> <span>handle</span>(<span>MyType</span> <span>val</span>) {
    <span>match</span>(<span>val</span>) {
        <span>of</span>(<span>Foo</span>) <span>return</span> <span>5</span>;
        <span>of</span>(<span>Bar</span>) <span>return</span> <span>7</span>;
    }
}</pre></div>
<p dir="auto">The above code may seem perfect at first glance, but in fact, it is not. The reason is this: <code>match(val)</code> boils down to <code>switch(val.tag)</code> under the hood, with <code>val.tag</code> being an ordinary C enumeration consisting of the variants <code>Foo</code> and <code>Bar</code>. But what if a caller provides us with neither <code>Foo</code> nor <code>Bar</code>, but with something like <code>42</code> (not a valid variant)? Since <code>enum</code> is merely another way to give integers names, a compiler would not complain on the <em>caller</em> site. However, on the <em>callee</em> site, we would have the warning:</p>
<div data-snippet-clipboard-copy-content="test.c: In function ‘handle’:
test.c:10:1: warning: control reaches end of non-void function [-Wreturn-type]
   10 | }
      | ^"><pre><code>test.c: In function ‘handle’:
test.c:10:1: warning: control reaches end of non-void function [-Wreturn-type]
   10 | }
      | ^
</code></pre></div>
<p dir="auto">The solution is to either panic or return some error-signaling code, like this:</p>
<div dir="auto" data-snippet-clipboard-copy-content="int handle(MyType val) {
    match(val) {
        of(Foo) return 5;
        of(Bar) return 7;
    }

    // Invalid input (no such variant).
    return -1;
}"><pre><span>int</span> <span>handle</span>(<span>MyType</span> <span>val</span>) {
    <span>match</span>(<span>val</span>) {
        <span>of</span>(<span>Foo</span>) <span>return</span> <span>5</span>;
        <span>of</span>(<span>Bar</span>) <span>return</span> <span>7</span>;
    }

    <span>// Invalid input (no such variant).</span>
    <span>return</span> <span>-1</span>;
}</pre></div>
<p dir="auto">See <a href="https://github.com/Hirrolot/datatype99/issues/9" data-hovercard-type="issue" data-hovercard-url="/Hirrolot/datatype99/issues/9/hovercard">issue #9</a>.</p>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Show HN: Browser-based knitting (pattern) software (110 pts)]]></title>
            <link>https://github.com/alefore/knit</link>
            <guid>40307089</guid>
            <pubDate>Thu, 09 May 2024 11:29:32 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/alefore/knit">https://github.com/alefore/knit</a>, See on <a href="https://news.ycombinator.com/item?id=40307089">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><p dir="auto"><h2 tabindex="-1" dir="auto">Knit</h2><a id="user-content-knit" aria-label="Permalink: Knit" href="#knit"></a></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Introduction</h2><a id="user-content-introduction" aria-label="Permalink: Introduction" href="#introduction"></a></p>
<p dir="auto">Knit is a web application to help you knit simple wavy scarfs.
It allows you to specify a few input parameters
concerning the scarf's length and shape
in order to generate the "pattern":
the sequence of rows that you can knit.
Knit displays a render of the scarf
(given your input parameters)
and helps you keep track of which row you're on.</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/alefore/knit/blob/main/images/000.jpg"><img src="https://github.com/alefore/knit/raw/main/images/000.jpg" alt="Sample scarf"></a></p>
<p dir="auto">The scarf shapes (i.e., the position of increases and decreases)
are generated using Bézier curves given the input parameters.</p>
<p dir="auto">This application state is contained entirely in the browser.
There is no server-side component or communication
(beyond just loading the static files).</p>
<p dir="auto">You can see it in action in Github pages:
<a href="https://alefore.github.io/knit/" rel="nofollow">https://alefore.github.io/knit/</a></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Example</h2><a id="user-content-example" aria-label="Permalink: Example" href="#example"></a></p>
<p dir="auto">The following is a scarf I knit (<a href="https://alefore.github.io/knit/#TotalLength=654&amp;CenterWidth=26&amp;Shape=Thin" rel="nofollow">parameters</a>):</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/alefore/knit/blob/main/images/001.jpg"><img src="https://github.com/alefore/knit/raw/main/images/001.jpg" alt="Sample scarf"></a></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Using Knit</h2><a id="user-content-using-knit" aria-label="Permalink: Using Knit" href="#using-knit"></a></p>
<p dir="auto">In order to knit a new scarf using this software:</p>
<ol dir="auto">
<li>
<p dir="auto">Load it from Github pages:
<a href="https://alefore.github.io/knit/" rel="nofollow">https://alefore.github.io/knit/</a></p>
</li>
<li>
<p dir="auto">Optionally, save a local copy.
I recommend using a local copy
as a way to freeze the version you're working on.
I may change the implementation.
Make sure all files (including the JavaScript logic) are saved
and loaded from your copy
(rather than just the <code>index.html</code> entry point).</p>
</li>
<li>
<p dir="auto">Optionally, knit a small swatch to inform
the length and width of your scarf in rows and stitches
(e.g., if each of your rows measures 4mm,
you'll need 250 rows for a 1m long scarf).
This enables the software to be completely agnostic to your
yarn width, needle size, and knitting tension.</p>
</li>
<li>
<p dir="auto">Configure your scarf:
adjust the parameters until you're satisfied.
You can mouse over each input for more information.
When you're satisfied, click <em>Knit</em>.</p>
</li>
<li>
<p dir="auto">Knitting your scarf typically takes a few hours,
depending on your parameters.
Unless you plan to knit your entire scarf in a single sitting,
consider doing a simple test before you start:
advance a few rows in the software (without knitting),
put your phone (or computer) away (e.g., lock it or such),
and come back to your browser.
Confirm that your browser correctly remembers which row you were on.</p>
</li>
<li>
<p dir="auto">Start knitting.
Cast-on 6 stitches.
This is the left tip of your scarf.</p>
</li>
<li>
<p dir="auto">Work through the rows, starting at row 0.
Whenever you finish a row, click "Next" to advance;
you can also press space, up or down or,
on mobile devices, swipe left or right.</p>
</li>
<li>
<p dir="auto">Bind-off the last 6 stitches.</p>
</li>
</ol>
<p dir="auto"><h3 tabindex="-1" dir="auto">Display</h3><a id="user-content-display" aria-label="Permalink: Display" href="#display"></a></p>
<p dir="auto">Rows look something like this: "92↓ (13 Δ1) 2K KFB 6K WYIF 3SLP"</p>
<p dir="auto">In this case:</p>
<ul dir="auto">
<li>
<p dir="auto">92 is the number of row (starting at 0, so this would be the 93rd row).</p>
</li>
<li>
<p dir="auto">↓ tells you the direction you're knitting in
(relative to the render).
You can also think of down as "right side"
and up as "wrong side".</p>
</li>
<li>
<p dir="auto">(13 Δ1) tells you that you should have 13 stitches at the end of this row.
The delta, if present, tells you that this row increases
the number of stitches by the amount given
(or shrinks, if negative).</p>
</li>
<li>
<p dir="auto">The rest are the steps for this row.</p>
</li>
<li>
<p dir="auto">The percentage number (shown only for the current row)
is an estimate of how much of the scarf you've already knitted
(at the start of this row).
This is based on a simple calculation
based on the number of stitches in all rows.</p>
</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Future</h2><a id="user-content-future" aria-label="Permalink: Future" href="#future"></a></p>
<p dir="auto">The following is a list of things I'd like to add:</p>
<ul dir="auto">
<li>
<p dir="auto">Clock that counts how much time you've spent in each row.
This would help me know if I've forgotten to register completion of a row.</p>
</li>
<li>
<p dir="auto">More patterns.
There's nothing in the underlying software that is specific to these scarfs.
This should support socks and hats and …</p>
</li>
</ul>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Boeing 737 crashes during take-off in Senegal (105 pts)]]></title>
            <link>https://www.bbc.co.uk/news/articles/ce5ljpnggp4o</link>
            <guid>40307084</guid>
            <pubDate>Thu, 09 May 2024 11:28:53 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.bbc.co.uk/news/articles/ce5ljpnggp4o">https://www.bbc.co.uk/news/articles/ce5ljpnggp4o</a>, See on <a href="https://news.ycombinator.com/item?id=40307084">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-component="text-block"><p><b>A Boeing 737-300 aircraft has skidded off a runway in Senegal, injuring at least 10 people, four of them seriously.</b></p><p>The incident occurred as Air Senegal flight HC 301 was taking off for the Malian capital Bamako in the early hours of Thursday, Dakar's Blaise Diagne airport said in a statement.</p><p>The pilot was slightly injured, but most of the 78 passengers on board were not hurt in the incident.</p><p>Operations were halted at the airport for a few hours but have now resumed.</p></div><div data-component="text-block"><p>Emergency services at the airport were mobilised to evacuate passengers, the airport's statement said.</p><p>An inquiry is under way to determine the causes of the incident, which took place at around 0100 GMT.</p><p>Boeing has not commented on the incident, nor has Transair, the private company from which Air Senegal chartered the plane.</p><p>Though it is not yet known what caused the crash, it comes as the manufacturer faces a deepening crisis over its safety record.</p><div><ul role="list"><li><p><a href="https://www.bbc.co.uk/news/business-68573686">How much trouble is Boeing in?</a></p></li></ul></div><p>An unused door blew out of an Alaska Airlines Boeing 737 Max in January shortly after take-off in the US.</p><p>The company is facing a criminal investigation into that incident.</p><p>The Senegal crash comes as a former quality inspector at Boeing's largest supplier told the BBC <a href="https://www.bbc.co.uk/news/business-68979354">that plane bodies regularly left the factory with serious defects</a>.</p><p>The company, Spirit AeroSystems, said it "strongly disagreed" with the allegations.</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Libyear (222 pts)]]></title>
            <link>https://libyear.com/</link>
            <guid>40306449</guid>
            <pubDate>Thu, 09 May 2024 09:05:31 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://libyear.com/">https://libyear.com/</a>, See on <a href="https://news.ycombinator.com/item?id=40306449">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
          <h2>Example 2</h2>
          <p>
            If your system has a one year old dependency and a three year old
            dependency, then your whole system is four libyears old.
          </p>
          <h2>A Healthy App</h2>
          <p>
            At <a href="https://www.singlebrook.com/">Singlebrook</a> we try to
            keep our client’s apps below 10 libyears. We regularly rescue
            projects that are over 100 libyears behind.
          </p>
          <h2>Etymology</h2>
          <p>
            "lib" is short for "library", the most common form of dependency.
          </p>
        </div><div>
          <h2>Other Measurements</h2>
          <p>
            <code>libyear-bundler</code> implements some of the other metrics
            described by (Bouwers, Eekelen, Visser, 2015).
          </p>
          <p>
            The <code>--versions</code> flag provides a metric for an installed
            dependency’s freshness relative to the newest release’s major, minor, and
            patch versions. Of course, this is most useful for dependencies that
            follow a consistent versioning scheme such as semver.
          </p>
          <p>
            The <code>--releases</code> flag provides a metric for the number of
            releases between an installed version of dependency and the newest
            released version of the dependency.
          </p>
          <p>
            Each metric has it’s own advantages and disadvantages, and all quantify
            the maintenance burden for an app. Taken together, they can help
            prioritize maintenance for an inherited app, or help maintain a baseline
            level of dependency freshness for an ongoing project.
          </p>
        </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[UTC, Tai, and Unix Time (2001) (118 pts)]]></title>
            <link>https://cr.yp.to/proto/utctai.html</link>
            <guid>40306352</guid>
            <pubDate>Thu, 09 May 2024 08:40:27 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://cr.yp.to/proto/utctai.html">https://cr.yp.to/proto/utctai.html</a>, See on <a href="https://news.ycombinator.com/item?id=40306352">Hacker News</a></p>
<div id="readability-page-1" class="page">
<a href="https://cr.yp.to/djb.html">D. J. Bernstein</a>
<br><a href="https://cr.yp.to/time.html">Time</a>

<h2>What is TAI?</h2>
TAI, Temps Atomique International (French for International Atomic Time),
measures real time.
One second of TAI time is a constant duration defined by cesium radiation.
TAI has been measured continuously since 1955
and is the foundation of all civil time standards.
<p>
TAI times are identified by year, month, day, hour, minute, and second.
There are exactly 86400 TAI seconds in every TAI day.
TAI days are labelled by the Gregorian calendar.
</p><h2>What is UTC?</h2>
One day of Earth's rotation isn't exactly 86400 seconds.
It's closer to 86400.002 seconds,
wobbling slightly from day to day.
<p>
UTC, Coordinated Universal Time, is based on TAI, and very similar to it,
except that UTC has <b>leap seconds</b> every year or two.
For example, here's how UTC and TAI handled the end of June 1997:
</p><p>
1997-06-30 23:59:59 UTC = 1997-07-01 00:00:29 TAI<br>
1997-06-30 23:59:60 UTC = 1997-07-01 00:00:30 TAI<br>
1997-07-01 00:00:00 UTC = 1997-07-01 00:00:31 TAI<br>
</p><p>
Notice the 23:59:60 in UTC.
That's a leap second.
It extended 1997-06-30 UTC to 86401 seconds.
Before the leap second, the TAI-UTC difference was 30 seconds;
after the leap second, the TAI-UTC difference was 31 seconds.
</p><p>
By inserting occasional leap seconds into UTC,
astronomers slow down UTC's progression to match Earth's rotation.
That way the Sun will always be overhead at 12:00:00 UTC in England.
(It's conceivable, but unlikely,
that someday Earth's rotation will speed up past 1/86400 Hz.
In that case astronomers will create negative leap seconds:
UTC will skip from 23:59:58 to 00:00:00.)
</p><p>
Other time zones are based on UTC---e.g.,
UTC minus 5 hours---so noon has a predictable relationship to the Sun
in every time zone.
</p><p>
The leap-second system was introduced at the beginning of 1972.
At that point UTC was TAI minus 10 seconds.
</p><h2>What is UNIX time?</h2>
UNIX time counts the number of seconds since an ``epoch.''
This is very convenient for programs that work with time intervals:
the difference between two UNIX time values is a
real-time difference measured in seconds,
within the accuracy of the local clock.
Thousands of programmers rely on this fact.
<p>
What is the epoch?
Originally it was defined as
the beginning of 1970 GMT.
GMT, Greenwich Mean Time,
is a traditional term for the time zone in England.
Unfortunately, it is ambiguous;
it can refer to a variety of astronomical time scales.
</p><p>
Arthur David Olson's popular time library
uses an epoch of 1970-01-01 00:00:10 TAI.
</p><h2>What's the problem?</h2>
For many years,
the UNIX localtime() time-display routine didn't support leap seconds.
In effect it treated TAI as UTC.
Its displays slipped 1 second away from the correct local time
as each leap second passed.
Nobody cared;
clocks weren't set that accurately anyway.
<p>
Unfortunately,
xntpd, a program that synchronizes clocks using the Network Time Protocol,
pandered to those broken localtime() libraries,
at the expense of reliability.
Watch how the xntpd time scale increases as a leap second occurs:
</p><p>
1997-06-30 23:59:59.7 UTC -&gt; 867715199.7 xntpd<br>
1997-06-30 23:59:59.8 UTC -&gt; 867715199.8 xntpd<br>
1997-06-30 23:59:59.9 UTC -&gt; 867715199.9 xntpd<br>
1997-06-30 23:59:60.0 UTC -&gt; 867715200.0 xntpd<br>
1997-06-30 23:59:60.1 UTC -&gt; 867715200.1 xntpd<br>
1997-06-30 23:59:60.2 UTC -&gt; 867715200.2 xntpd<br>
1997-06-30 23:59:60.3 UTC -&gt; 867715200.3 xntpd<br>
1997-06-30 23:59:60.4 UTC -&gt; 867715200.4 xntpd<br>
1997-06-30 23:59:60.5 UTC -&gt; 867715200.5 xntpd<br>
1997-06-30 23:59:60.6 UTC -&gt; 867715200.6 xntpd<br>
1997-06-30 23:59:60.7 UTC -&gt; 867715200.7 xntpd<br>
1997-06-30 23:59:60.8 UTC -&gt; 867715200.8 xntpd<br>
1997-06-30 23:59:60.9 UTC -&gt; 867715200.9 xntpd<br>
1997-07-01 00:00:00.0 UTC -&gt; 867715200.0 xntpd<br>
1997-07-01 00:00:00.1 UTC -&gt; 867715200.1 xntpd<br>
1997-07-01 00:00:00.2 UTC -&gt; 867715200.2 xntpd<br>
</p><p>
The xntpd time scale repeats itself!
It cannot be reliably converted to UTC.
</p><p>
By resetting the clock at each leap second,
xntpd extracts a correct UTC display
(except, of course, during leap seconds)
from the broken localtime() libraries.
Meanwhile,
it produces incorrect results
for applications that add and subtract real times.
</p><h2>Why not fix it?</h2>
It's easy enough to fix xntpd.
It's also easy to fix localtime() to handle leap seconds.
In fact, some vendors have already adopted Olson's time library.
<p>
The main obstacle is POSIX.
POSIX is a ``standard'' designed
by a vendor consortium several years ago
to eliminate progress and protect the installed base.
The behavior of the broken localtime() libraries
was documented and turned into a POSIX requirement.
</p><p>
Fortunately, the POSIX rules are so outrageously dumb---for
example, they require that 2100 be a leap year,
contradicting the Gregorian calendar---that no
self-respecting engineer would obey them.
</p><h2>References</h2>
The 
<a href="http://www.boulder.nist.gov/timefreq/">NIST
Time and Frequency Division Home Page</a>
is a good starting point for programmers who want to learn about
time measurement.
<p>
The Olson library is available from
<a href="ftp://elsie.nci.nih.gov/pub/">ftp://elsie.nci.nih.gov/pub/</a>.
The above argument against the xntpd time scale
is shamelessly stolen from one of Olson's manual pages.
</p><p>
In preparation for the Y2036 and Y2038 disasters,
I've put together some
<a href="https://cr.yp.to/libtai.html">64-bit time manipulation code</a>,
including very fast UTC-to-TAI conversion.
My library supports the same TAI epoch as the Olson library.
</p><p>
I've also put together a
<a href="https://cr.yp.to/clockspeed.html">very simple clock-synchronization package</a>,
including a Network Time Protocol client
that handles leap seconds correctly.


</p></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Tracking Illicit Brazilian Beef from the Amazon to Your Burger (229 pts)]]></title>
            <link>https://e360.yale.edu/features/marcel-gomes-interview</link>
            <guid>40306249</guid>
            <pubDate>Thu, 09 May 2024 08:09:54 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://e360.yale.edu/features/marcel-gomes-interview">https://e360.yale.edu/features/marcel-gomes-interview</a>, See on <a href="https://news.ycombinator.com/item?id=40306249">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>

  
    




<header>
  <div>
      <h3>INTERVIEW</h3>  
  

    <p>Journalist Marcel Gomes has traced beef in supermarkets and fast food restaurants in the U.S. and Europe to Brazilian ranches on illegally cleared land. In an <em>e360</em> interview, he talks about the challenges of documenting the supply chains and getting companies to clean them up. </p>
  

 
  
    

    







  </div> <!-- article titles content -->
</header> <!-- article titles -->
  
  


    
    <section>

                      
                        

<div>
<p>Investigative journalism can be a very deep dive. By the end of his probe into the supply chain of JBS, the world’s largest meat processing and packing company, Marcel Gomes reckons he and his team at the São Paulo-based nonprofit <em>Repórter Brasil</em> knew more about the origins of the beef it supplies from the Amazon to the world’s hamburger chains and supermarkets than the company itself. </p>
<p>With grassroots support from labor unions and Indigenous communities, he had mapped the complex networks of cattle farms responsible for illegal deforestation. He then tracked the often-illicit beef through JBS’s slaughterhouses and packing plants to the freezers, shelves, and customer trays of retail outlets and fast-food restaurants around the world. When his sleuths were done, the fingerprints of forest destruction were plain to see. Six of Europe’s biggest retail chains reacted by halting purchases of JBS beef. 
</p>
<p>That investigation just won Gomes, 45, a Goldman Environment Prize. But sadly, he says in an interview with <em>Yale Environment 360</em>, when he went to San Francisco last month to pick up the prize, stores there still had tainted beef on their shelves. 
</p>
</div>
                        
<div>
  <blockquote>“We hired researchers in Europe and the U.S. to visit stores to find Brazilian beef and take pictures of the seals.”</blockquote>
</div>
                        

<div>
<p><strong>Yale Environment 360</strong>: Why did you choose to investigate JBS?
</p>
<p><strong>Marcel Gomes:</strong> Well, Brazil is the world’s biggest beef exporter. My country has more than 200 million cattle, and ranching is the single biggest driver of deforestation in the Amazon, where more than 40 percent of the cattle are raised. JBS is our biggest beef company. It slaughters more than 12 million animals a year, exporting their meat to the United States, Europe, and across the world. And it’s not just a beef company. It exports leather — for instance to Germany, where it makes car seats — and biodiesel made from beef tallow. </p>
</div>
                        
 <!-- footnoteBlock -->
                        

<div>
<p><strong>e360</strong>: Can you describe how you tracked its supply chain?
</p>
<p><strong>Gomes:</strong> Since 2011, Brazil has had legislation intended to improve the transparency of sources of supply of beef and other agricultural commodities. So, at <em>Repórter Brasil</em>, which was founded in 2008, we began to collect this public data on everything related to environmental, social, and labor issues. We started to cross-check the data so we could trace supply chains right from the ranch to consumers.</p>
<p>Then we put this information together with data on areas where ranchers had been fined for environmental violations such as deforestation, and where there were reports of modern slavery and forced or child labor. We also used satellite images to identify which farms had seen deforestation each year, and we tapped into data on the transport of cattle from those farms to the slaughterhouses receiving the cattle.</p>
</div>
                        
<div>

  <figure>

    <div>
      
                  
      <p><a href="https://e360.yale.edu/assets/site/Cattle-Farm_Goldman-Environmental-Prize-2.jpg" data-caption="A cattle farm in the Amazon." data-credit="Fernando Martinho">
  
  
  
    
  
  
  
      
    
                
    
                
    
                
    
                
    
                        
  <img sizes="(min-width: 1450px) 1260px, (min-width: 980px) 940px, calc(100vw - 40px)" srcset="https://e360.yale.edu/assets/site/Cattle-Farm_Goldman-Environmental-Prize-2.jpg 1200w, https://e360.yale.edu/assets/site/_200xAUTO_stretch_center-center/Cattle-Farm_Goldman-Environmental-Prize-2.jpg 200w, https://e360.yale.edu/assets/site/_400xAUTO_stretch_center-center/Cattle-Farm_Goldman-Environmental-Prize-2.jpg 400w, https://e360.yale.edu/assets/site/_600xAUTO_stretch_center-center/Cattle-Farm_Goldman-Environmental-Prize-2.jpg 600w, https://e360.yale.edu/assets/site/_800xAUTO_stretch_center-center/Cattle-Farm_Goldman-Environmental-Prize-2.jpg 800w, https://e360.yale.edu/assets/site/_1000xAUTO_stretch_center-center/Cattle-Farm_Goldman-Environmental-Prize-2.jpg 1000w" src="https://e360.yale.edu/assets/site/_400xAUTO_stretch_center-center/Cattle-Farm_Goldman-Environmental-Prize-2.jpg" alt="A cattle farm in the Amazon.">
</a>
      </p>
          </div>

        <figcaption>
              <p><span>A cattle farm in the Amazon.</span>
          <span>Fernando Martinho</span></p>
    </figcaption>
    
  </figure>

</div> <!-- imageBlock -->
                        

<div>
<p>The second part of our investigations has been the consumer market. JBS sells around the world. So, in 2021, we hired researchers in Europe and the U.S., as well as Brazil, to visit stores to find Brazilian beef and take pictures of the seals and export tracking numbers. Through those numbers, we could track back to the original packing plant where the beef came from. For us it was very important to go to the supermarkets and take pictures, and to have that clear evidence.</p>
<p>Then we connected both ends of the supply chain, and called everybody — the supermarkets, the slaughterhouses, the traders, JBS, the public health authorities, everyone — to tell them what we had found and ask for them to respond. Several supermarket chains in Europe believed our findings straight away and announced boycotts just as our report was published. They included Sainsbury’s in the U.K., Carrefour in Belgium, and Auchan in France. Later, others joined them.
</p>
<p><strong>e360</strong>: How important is your grassroots work with labor unions and Indigenous people?
</p>
<p><strong>Gomes:</strong> Very important. They guide us in the field to identify the ranches, and sometimes protect us from violence. They know the routes the trucks take, the names of people we can interview, and the workers who can tell inspectors about slavery. Indigenous communities especially know about the environmental impact they are suffering from the deforestation.</p>
</div>
                        
<div>
  <blockquote>“We need to change the system in Brazil, to find a new way to trace the cattle moving between farms and to stop cattle laundering.”</blockquote>
</div>
                        

<p><strong>e360</strong>: You talk about cattle laundering — raising cattle on recently deforested land, and then moving the animals to graze somewhere else for a short while to create the impression of a supply chain that doesn’t involve deforestation. Is that a big issue?</p>
                        
 <!-- footnoteBlock -->
                        

<div>
<p><strong>Gomes:</strong> Yes. It is common to move cattle from farm to farm in Brazil, often three or four times. There can be legitimate reasons for doing this, because cattle raising is quite a specialized business. But it can also cover up a “dirty” trail.</p>
<p>In theory there are rules to prevent the trading of cattle from illegal to legal areas. But in practice it is very easy to do, especially when you have several members of a family doing the same business. One brother can move cattle to the land of another brother, with no paperwork. Also, you can change the boundaries of your own land, for instance, by turning one ranch into two, one clean and the other dirty. Then you can just trade with companies like JBS from the clean part. 
</p>
<p>That “laundering” works because JBS and most of the other meat processors only keep a record of their direct supplier, the last step in the chain. So a lot of what happens before that is hidden.</p>
</div>
                        
<div>

  <figure>

    <div>
      
                  
      <p><a href="https://e360.yale.edu/assets/site/Marcel-Gomes_Credit-Fernando-Martinho-2.jpg" data-caption="A cattle farm in the Amazon." data-credit="Fernando Martinho">
  
  
  
    
  
  
  
      
    
                
    
                
    
                
    
                
    
                
    
                
    
            
  <img sizes="(min-width: 1450px) 1260px, (min-width: 980px) 940px, calc(100vw - 40px)" srcset="https://e360.yale.edu/assets/site/Marcel-Gomes_Credit-Fernando-Martinho-2.jpg 1500w, https://e360.yale.edu/assets/site/_200xAUTO_stretch_center-center/Marcel-Gomes_Credit-Fernando-Martinho-2.jpg 200w, https://e360.yale.edu/assets/site/_400xAUTO_stretch_center-center/Marcel-Gomes_Credit-Fernando-Martinho-2.jpg 400w, https://e360.yale.edu/assets/site/_600xAUTO_stretch_center-center/Marcel-Gomes_Credit-Fernando-Martinho-2.jpg 600w, https://e360.yale.edu/assets/site/_800xAUTO_stretch_center-center/Marcel-Gomes_Credit-Fernando-Martinho-2.jpg 800w, https://e360.yale.edu/assets/site/_1000xAUTO_stretch_center-center/Marcel-Gomes_Credit-Fernando-Martinho-2.jpg 1000w, https://e360.yale.edu/assets/site/_1200xAUTO_stretch_center-center/Marcel-Gomes_Credit-Fernando-Martinho-2.jpg 1200w, https://e360.yale.edu/assets/site/_1260xAUTO_stretch_center-center/Marcel-Gomes_Credit-Fernando-Martinho-2.jpg 1260w" src="https://e360.yale.edu/assets/site/_400xAUTO_stretch_center-center/Marcel-Gomes_Credit-Fernando-Martinho-2.jpg" alt="A cattle farm in the Amazon.">
</a>
      </p>
          </div>

        <figcaption>
              <p><span>A cattle farm in the Amazon.</span>
          <span>Fernando Martinho</span></p>
    </figcaption>
    
  </figure>

</div> <!-- imageBlock -->
                        

<div>
<p><strong>e360</strong>: Until your investigations. I am guessing that by the end of your research, you knew a lot more about the JBS supply chain than JBS did. Am I right?</p>
<p><strong>Gomes:</strong> Yes. I can say that is possible. We were monitoring the supply chain from the first ranch, which we know they didn’t do.</p>
<p><strong>e360</strong>: I wonder why not. Is cattle laundering something that the companies have encouraged, or do they turn a blind eye, or maybe it is ignorance that happened almost by accident?</p>
<p><strong>Gomes:</strong> Well, I don’t know. But if, in the future, companies [such as JBS] decide to eliminate the dirty part of their supply chain, they will lose a big market share. So there are business reasons not to monitor the whole supply chain. We need to change the system in Brazil, to find a new way to trace the cattle moving between farms and to stop cattle laundering. Without that, we won’t have JBS and other companies operating in a sustainable way.
</p>
<p><strong>e360</strong>: Did JBS ever try to prevent your findings from being published?
</p>
<p><strong>Gomes:</strong> They have in the past. But in the last few years they have been more responsive, as the media and NGOs have used the information we have gathered to pressure them. We have a dialogue now. We send them the result of our investigations, and they provide us with information. They have also started to remove ranches from their supply chain.</p>
</div>
                        
<div>
  <blockquote>“Companies like McDonald’s make statements saying they have dialogues with their suppliers. But we don’t see any big change.”</blockquote>
</div>
                        

<div>
<p><strong>e360</strong>: What about the federal government now that Lula [Luiz Inácio Lula da Silva] is back as president?</p>
<p><strong>Gomes:</strong> The government agencies listen to us. There are a lot of meetings. Things are happening, but it is hard. And there is also a lot of politics. We do find that, under Lula, farmers are abiding by the laws more, however. Deforestation rates in the Amazon have decreased in the last year and a half, since Lula returned to power. But we still have a big problem in the cerrado, the big savanna region to the south and east [of the Amazon]. You can still legally clear 80 percent of the trees on your land there. That needs to change.</p>
<p><strong>e360</strong>: JBS has promised to clean up its cattle supply chain by, for instance, eliminating deforestation by direct suppliers by 2030 and from its indirect suppliers by 2035. Can it achieve those targets?</p>
<p><strong>Gomes:</strong> If they properly monitor their indirect suppliers then, yes, it is possible. But right now they are still really involved in illegal deforestation in different parts of Brazil.
</p>
<p>Of course, if they exclude all suppliers with environmental nonconformities they are going to lose a lot of them. So it will take time to reshape their business, and I cannot see a big change yet.</p>
</div>
                        
<div>

  <figure>

    <div>
      
                  
      <p><a href="https://e360.yale.edu/assets/site/_1500x1500_fit_center-center_80/JBS-Facility_Getty.jpg" data-caption="A JBS facility in Tucuma, Brazil." data-credit="Jonne Roriz / Bloomberg via Getty Images">
  
  
  
    
  
  
  
      
    
                
    
                
    
                
    
                
    
                
    
                
    
            
  <img sizes="(min-width: 1450px) 1260px, (min-width: 980px) 940px, calc(100vw - 40px)" srcset="https://e360.yale.edu/assets/site/JBS-Facility_Getty.jpg 5464w, https://e360.yale.edu/assets/site/_200xAUTO_stretch_center-center/JBS-Facility_Getty.jpg 200w, https://e360.yale.edu/assets/site/_400xAUTO_stretch_center-center/JBS-Facility_Getty.jpg 400w, https://e360.yale.edu/assets/site/_600xAUTO_stretch_center-center/JBS-Facility_Getty.jpg 600w, https://e360.yale.edu/assets/site/_800xAUTO_stretch_center-center/JBS-Facility_Getty.jpg 800w, https://e360.yale.edu/assets/site/_1000xAUTO_stretch_center-center/JBS-Facility_Getty.jpg 1000w, https://e360.yale.edu/assets/site/_1200xAUTO_stretch_center-center/JBS-Facility_Getty.jpg 1200w, https://e360.yale.edu/assets/site/_1260xAUTO_stretch_center-center/JBS-Facility_Getty.jpg 1260w" src="https://e360.yale.edu/assets/site/_400xAUTO_stretch_center-center/JBS-Facility_Getty.jpg" alt="A JBS facility in Tucuma, Brazil.">
</a>
      </p>
          </div>

        <figcaption>
              <p><span>A JBS facility in Tucuma, Brazil.</span>
          <span>Jonne Roriz / Bloomberg via Getty Images</span></p>
    </figcaption>
    
  </figure>

</div> <!-- imageBlock -->
                        

<div>
<p><strong>e360</strong>: How about JBS’s customers? Some big European retailers have reacted to your revelations by banning or distancing themselves from JBS. Have you had the same response in the U.S.?</p>
<p><strong>Gomes:</strong> No, it didn’t happen. Nor in Brazil either. But they say they want more information from JBS about the issues. So at least we know the company’s supply chain is being monitored more by retailers. Fast food companies like McDonald’s and Burger King make statements saying they have dialogues with their suppliers. But we don’t see any big change in selecting or eliminating suppliers.
</p>
<p><strong>e360</strong>: What about JBS’s banks and investors?
</p>
<p><strong>Gomes:</strong> A few days ago, a group of Indigenous peoples held a meeting with one of JBS’s banks in Brazil to talk about how the company is buying cattle raised illegally inside their lands.
</p>
<p>We [at <em>Repórter Brasil</em>] also have partnerships with NGOs abroad that target banks. In France, for instance, we provided information for a <a href="https://shuftipro.com/news/ngo-coalition-files-a-complaint-against-french-banks-for-money-laundering-linked-to-amazon-deforestation/#:~:text=The%20NGO%20coalition%20filed%20a,directly%20linked%20to%20Amazon%20deforestation">lawsuit</a> brought last year against, among others, BNP Paribas, the largest banking group in the world, over potentially funding illegal deforestation by JBS in Brazil.</p>
</div>
                        
<div>
  <blockquote>“We did the first investigations in Brazil of slave labor in the cattle, soy, coffee, and orange-juice industries… Cattle came out badly.”</blockquote>
</div>
                        

<div>
<p><strong>e360</strong>: What about modern slavery, such as forced and child labor? That is another issue you have investigated, I think.
</p>
<p><strong>Gomes:</strong> Yes. We did the first investigations in Brazil of slave labor in the cattle, soy, coffee, and orange-juice industries. We mapped the supply chains of supermarkets and fast food chains to see if they are connected to farms or other places with slave labor.
</p>
<p>Cattle came out badly. We found that of the 55,000 workers who had been released by government inspectors from slave conditions since 1995, about a third were in the cattle industry. Most of the people involved in deforestation for cattle are enslaved people.
</p>
<p><strong>e360</strong>: What else are you working on?
</p>
<p><strong>Gomes:</strong> We concentrate on investigating the supply chains of Brazilian commodities that are of interest to our campaigning partners in other parts of the world. For instance, we have looked at the labor practices of orange growers supplying juice to soft-drinks companies such as Coca-Cola, who sell on to McDonald’s among many others.
</p>
<p>Late last year we published a <a href="https://reporterbrasil.org.br/2023/11/starbucks-slave-and-child-labour-found-at-certified-coffee-farms-in-minas-gerais/">report</a> about Starbucks, showing that some of its coffee suppliers were responsible for serious human rights abuses. That contradicted the company’s claim to have 100 percent ethical sourcing. The report is now being used in a <a href="https://www.washingtonpost.com/business/2024/01/11/starbucks-sued-lawsuit-ethical-coffee/">lawsuit</a> against Starbucks in the U.S. Starbucks is now engaging over that, which is nice.</p>
</div>
                        
 <!-- footnoteBlock -->
                        

<div>
<p><strong>e360</strong>: In 10 years, what do you think you will be working on? Where will things stand?</p>
<p><strong>Gomes:</strong> I don’t think Brazil will be a lot better. We will still see a lot of problems. We are making progress, but I don’t think I will be able to retire. There will be plenty more to investigate. 
</p>
</div>
                  </section>


    

  </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Shell sold carbon credits for carbon that was never captured (102 pts)]]></title>
            <link>https://www.cbc.ca/news/climate/shell-greenpeace-quest-1.7196792</link>
            <guid>40304901</guid>
            <pubDate>Thu, 09 May 2024 03:02:02 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.cbc.ca/news/climate/shell-greenpeace-quest-1.7196792">https://www.cbc.ca/news/climate/shell-greenpeace-quest-1.7196792</a>, See on <a href="https://news.ycombinator.com/item?id=40304901">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="detailContent"><p><span><a href="https://www.cbc.ca/news/climate"><span>News</span></a></span></p><p>Shell sold millions of carbon credits for reductions in greenhouse gas emissions that never happened, allowing the company to turn a profit on its fledgling carbon capture and storage project, according to a new report by Greenpeace Canada.</p><h2 lang="en">Greenpeace describes deal with Alberta as 'hidden subsidy' that awarded money for 'phantom' credits</h2><div data-cy="storyWrapper"><figure><p><img alt="An employee looks at the facility" src="https://i.cbc.ca/1.7041427.1715214044!/fileImage/httpImage/image.jpg_gen/derivatives/16x9_780/carbon-capture.jpg" data-cy="leadmedia-story-img" fetchpriority="high"></p><figcaption>Shell's Quest facility near Edmonton has stored nine million tonnes of carbon dioxide since 2015.<!-- --> <!-- -->(Jason Franson/Bloomberg)</figcaption></figure><div><p dir="ltr">Shell sold millions of carbon credits for reductions in greenhouse gas emissions that never happened, allowing the company to turn a profit on its fledgling carbon capture and storage project, according to a <a href="https://www.greenpeace.org/static/planet4-canada-stateless/2024/02/4b010c8b-en-selling-hot-air-report.pdf"><u>new report</u></a> by Greenpeace Canada.</p><p dir="ltr">Under an agreement with the Alberta government, Shell was awarded two tonnes' worth of emissions reduction credits for each tonne of carbon it actually captured and stored underground at its Quest plant, near Edmonton.</p><p dir="ltr">This took place between 2015 and 2021 through a subsidy program for <a href="https://www.cbc.ca/news/climate/carbon-capture-canada-iea-1.7041175"><u>carbon, capture, utilisation and storage projects</u></a> (CCUS), which are championed by the oil and gas sector as a way to cut its greenhouse gas emissions.</p><p dir="ltr">At the time, Quest was the only operational CCUS facility in Alberta. The subsidy program ended in 2022.&nbsp;</p><p dir="ltr">During this period, Shell was able to sell 5.7 million tonnes of what Greenpeace describes as "phantom" credits, making more than $200 million for the company. These credits were sold to other oilsands companies on the Alberta carbon market, Greenpeace alleged.</p><p dir="ltr">Such sales would not have been illegal, but amounted to a "hidden subsidy" within the program&nbsp;which&nbsp;undercut the effectiveness of industrial carbon pricing, says Keith Stewart, senior energy strategist at Greenpeace and the author of the report.</p><p dir="ltr">"Carbon capture projects that have been advertised as a solution to pollution in the oilsands have been almost entirely paid for by the public," he said.</p><p dir="ltr">Shell has received $777 million from the federal and provincial governments&nbsp;and $406 million in revenue from carbon offsets, according to company records cited by Greenpeace.</p><p dir="ltr">In all, taxpayer funding has covered 93 per cent of the costs of Shell's Quest project to date, Greenpeace said.</p><p dir="ltr">Since 2015, the Quest project has stored nine million tonnes of CO2. (By comparison, emissions from the oil and gas sector totalled just over <a href="http://www.cbc.ca/news/canada/calgary/alberta-2024-national-inventory-report-2022-greenhouse-gas-emissions-1.7192042#:~:text=Alberta's%20emissions%20totalled%20270%20megatonnes,province's%20271%20megatonnes%20in%202021.">158 million tonnes</a> in 2022, the most recent federal data available.)</p><h2>'Smear job'&nbsp;</h2><p dir="ltr">Carbon offsets are bought and sold under a trading system, with governments putting a price on carbon dioxide emissions to compel companies to fight climate change.</p><p dir="ltr">Since 2007, Alberta has run a mandatory carbon offset system for large emitters, such as oil and gas companies. If they produce more than their allotted levels of carbon dioxide, they must purchase credits to offset those emissions.</p><div dir="ltr"><figure><p><img loading="lazy" alt="Equipment and buildings at an oilsands mine" srcset="https://i.cbc.ca/1.5474405.1700862621!/fileImage/httpImage/image.jpg_gen/derivatives/original_300/fort-hills-suncor.jpg 300w,https://i.cbc.ca/1.5474405.1700862621!/fileImage/httpImage/image.jpg_gen/derivatives/original_460/fort-hills-suncor.jpg 460w,https://i.cbc.ca/1.5474405.1700862621!/fileImage/httpImage/image.jpg_gen/derivatives/original_620/fort-hills-suncor.jpg 620w,https://i.cbc.ca/1.5474405.1700862621!/fileImage/httpImage/image.jpg_gen/derivatives/original_780/fort-hills-suncor.jpg 780w,https://i.cbc.ca/1.5474405.1700862621!/fileImage/httpImage/image.jpg_gen/derivatives/original_1180/fort-hills-suncor.jpg 1180w" sizes="(max-width: 300px) 300px,(max-width: 460px) 460px,(max-width: 620px) 620px,(max-width: 780px) 780px,(max-width: 1180px) 1180px" src="https://i.cbc.ca/1.5474405.1700862621!/fileImage/httpImage/image.jpg_gen/derivatives/original_780/fort-hills-suncor.jpg" data-cy="image-img"></p><figcaption>An oilsands mine in northern Alberta. A proposed carbon capture and storage facility to be built in northeastern Alberta is designed to cut emissions from the oilsands industry.<!-- --> <!-- -->(Kyle Bakx/CBC)</figcaption></figure></div><p dir="ltr">Ryan Fournier, a spokesperson for Alberta's Environment Minister Rebecca Schulz, said the report is a "smear job by Greenpeace."</p><p dir="ltr">Fournier acknowledged in an email that the Alberta government previously offered "important credits to help accelerate CCUS development."</p><p dir="ltr">But he described the program as a "targeted incentive to help drive CCUS investments at a time when this was still an unproven technology."&nbsp;</p><p dir="ltr">The <a href="https://www.shell.ca/en_ca/about-us/projects-and-sites/quest-carbon-capture-and-storage-project.html">Quest facility</a> is operated by Shell Canada and owned by Canadian Natural Resources, Chevron and Shell Canada.</p><p dir="ltr">In response to the report, Shell Canada spokesperson Stephen Doolan said carbon capture technology is critical to achieving international climate targets.</p><p dir="ltr">He said that as "a result of innovative fiscal and regulatory frameworks, nine million tonnes of CO2 have been captured at Shell's Quest facility that would have otherwise been released into the atmosphere."</p><p dir="ltr">Neither the province nor Shell denied the sale of the extra credits.</p><p dir="ltr">Doolan later followed up to add that the incentive had previously been publicly announced by the Alberta&nbsp;government and&nbsp;that it was in place "only until project costs broke even."</p><p><em><strong>WATCH&nbsp;|&nbsp;CO2 site worries Albertans: </strong></em><span><span><div title="Massive carbon capture facility worries Alberta residents" role="button" tabindex="0"><div><p><img src="https://thumbnails.cbc.ca/maven_legacy/thumbnails/671/607/carbon_mpx.jpg" srcset="" alt="" loading="lazy"></p></div><div><h3>Massive carbon capture facility worries Alberta residents</h3></div></div><span>Canadian oilsands companies want to build a $16.5-billion carbon capture project near Cold Lake, Alta. Residents fear that pumping millions of tonnes of CO2 underground will endanger their communities.</span></span></span></p><h2 dir="ltr">Heavily reliant on subsidies</h2><p dir="ltr">Pierre-Olivier Pineau, a professor and researcher in energy policy at HEC Montreal, said the Greenpeace report illustrates "a key underlying problem" for carbon capture and storage, that "the economic environment isn't yet there to make them sound business."&nbsp;</p><p dir="ltr">"It has to rely on subsidies, which become problematic because the government ends up subsidizing polluters," he said, adding that it also shows the need for a higher price on carbon.</p><p>"CCUS can only be correctly incentivized through a [higher] penalty on carbon emission," he said.&nbsp;</p><p>Without a sufficiently high price,&nbsp;Pineau&nbsp;says CCUS&nbsp;projects will be cancelled because "they are not as profitable as dumping CO2 straight in the atmosphere" — unless, as in the case of Shell, they are heavily subsidized, he said.</p><p dir="ltr">Last week, Edmonton-based Capital Power Corp. announced it was abandoning plans to build a $2.4-billion carbon capture and storage project at its Genesee natural gas-fired power plant southwest of Edmonton.</p><p dir="ltr">Up to three million tonnes of carbon dioxide per year would have been captured at the facility.</p><p dir="ltr">The Pathways Alliance, a consortium of Canada's largest oilsands companies, is still trying to move ahead with a $16.5-billion carbon capture pipeline project, but is seeking about two-thirds of that amount to be covered by subsidies.</p><h2 dir="ltr">Watching for loopholes</h2><p dir="ltr">Federal data released last week found <a href="https://www.cbc.ca/news/canada/calgary/alberta-2024-national-inventory-report-2022-greenhouse-gas-emissions-1.7192042#:~:text=Alberta's%20emissions%20totalled%20270%20megatonnes,province's%20271%20megatonnes%20in%202021."><u>Alberta was lagging</u></a> behind other provinces in terms of emissions reductions, with the oil and gas sector still the largest contributor to greenhouse gas emissions.</p><p dir="ltr">A spokesperson for Natural Resources Minister Jonathan Wilkinson said "the oil and gas sector needs to move forward on achieving reductions in absolute emissions."</p><p dir="ltr">"It is time for the sector to be spending money and putting into place solutions that will reduce carbon pollution and will ultimately strengthen the sector's long-term competitiveness," said Carolyn Svonkin.</p><div dir="ltr"><ul><li><a href="https://www.cbc.ca/news/canada/calgary/alberta-2024-national-inventory-report-2022-greenhouse-gas-emissions-1.7192042" text="Alberta's emissions down slightly but still make up lion's share of Canada's greenhouse gas" flag="" data-contentid=""><span>Alberta's emissions down slightly but still make up lion's share of Canada's greenhouse gas</span></a></li></ul><ul><li><a href="https://www.cbc.ca/news/canada/edmonton/capital-power-ccs-1.7192920" text="Financial risks of novel technology likely derailed Alberta carbon capture project, analysts say" flag="" data-contentid=""><span>Financial risks of novel technology likely derailed Alberta carbon capture project, analysts say</span></a></li></ul></div><p dir="ltr">In reference to the Greenpeace report, Svonkin noted that the federal government updated its national carbon pricing benchmark in 2021 "to ensure all provincial and territorial pricing systems are comparable in terms of stringency and effectiveness."&nbsp;</p><p dir="ltr">"This put an end to processes that could have rewarded industry for emissions reductions that are not real," the statement said.</p><p dir="ltr">The federal government is expected to announce details for its emissions cap on the oil and gas sector in the coming months. Stewart said he wants to make sure there aren't similar "loopholes built into it in obscure ways that undercut the effectiveness of the policy."</p><div dir="ltr"><ul><li><a href="https://www.cbc.ca/news/canada/saskatoon/boundary-dam-carbon-capture-missing-emmision-goals-1.7191867" text="Missed emissions goals at Sask. carbon capture project raising questions" flag="" data-contentid=""><span>Missed emissions goals at Sask. carbon capture project raising questions</span></a></li></ul></div></div></div><div><h2>ABOUT THE AUTHOR</h2><div><figure><p><img loading="lazy" alt="" srcset="https://i.cbc.ca/1.3502943.1669653259!/fileImage/httpImage/image.jpg_gen/derivatives/square_300/benjamin-shingler.jpg 300w,https://i.cbc.ca/1.3502943.1669653259!/fileImage/httpImage/image.jpg_gen/derivatives/square_460/benjamin-shingler.jpg 460w,https://i.cbc.ca/1.3502943.1669653259!/fileImage/httpImage/image.jpg_gen/derivatives/square_620/benjamin-shingler.jpg 620w" sizes="(max-width: 258pxpx) 258pxpx" src="https://i.cbc.ca/1.3502943.1669653259!/fileImage/httpImage/image.jpg_gen/derivatives/square_620/benjamin-shingler.jpg" data-cy="author-image-img"></p></figure></div><p>Benjamin Shingler is a senior writer based in Montreal, covering climate policy, health and social issues. He previously worked at The Canadian Press and the New Brunswick Telegraph-Journal. </p><ul></ul></div></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[The Time I Lied to the CTO and Saved the Day (460 pts)]]></title>
            <link>https://GrumpyOldDev.com/post/the-one-where-i-lie-to-the-cto/</link>
            <guid>40304453</guid>
            <pubDate>Thu, 09 May 2024 01:39:27 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://GrumpyOldDev.com/post/the-one-where-i-lie-to-the-cto/">https://GrumpyOldDev.com/post/the-one-where-i-lie-to-the-cto/</a>, See on <a href="https://news.ycombinator.com/item?id=40304453">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>This was several years back.  Keep in mind that early in my
career, my father had told me that doing a good job
often meant doing what needed to be done in spite of your boss.
And by that he meant that you can either make your boss successful
and happy or you can run every decision by your boss. In which
case no one is successful or happy.</p>
<p>I was working at the time for a Fortune 500 company and our CTO had
signed up to deliver a big project for an important client with
whom he had personal connections.  He also decided to outsource a
key part of it to a large tech services firm who claimed they had a product that would do most of the heavy lifting for us.</p>
<p>As has been typical in my career, when the vendor said they had a
product, what they really meant was they had something vaguely
resembling a product that vaguely matched what we needed, and with
heavy customization they could torture it into doing what we
needed.  Of course by customizing their “product” we cleverly
combined all the downsides of vendor software with all the
downsides of custom software.  We simultaneously achieved the holy
grail of bad ideas: an inflexible vendor package that would
have to be forced into doing something it wasn’t designed to do
but would also be forked from their main product codebase  - guaranteeing sooner or later it would be end-of-lifed once the
vendor realized how expensive it was to keep maintaining.
We grumbled to each other about what a horrifically
bad idea this was, especially considering the vendor’s proven
track record in not delivering anything on time.</p>
<p>Because the CTO had a yearly turnover of his direct reports,
every status call about the
project took some variation of “great idea, boss” even though
literally no one involved thought it was even a good idea.<br>
Or even a mediocre idea.  It was a bad idea.</p>
<p>The rest of the project required heavy development in-house
for all the other pieces, so we had our own challenges keeping us
busy.  But even so, as project dates through the summer slipped
and the vendor promised that any day now their product would be
ready to integrate for an October launch date,
it became more and more
obvious to everyone but the CTO that the project was in trouble.
Finally in August the vendor delivered their “product” and
we began the death march to integrate with it.</p>
<p>In September we encountered a show stopper bug.  The vendor
product stored every customer transaction as a json record in a
giant json document.  So as test data accumulated, performance of
the product got slower and slower.  Adding a new transaction
involved reading the entire json document out of the database,
then appending the new record to the end.  The vendor claimed they
could fix this by indexing the transaction fields, and that seemed
to help for a bit until we ran into problem number two.</p>
<p>The database they chose was MongoDB and at the time Mongo had a
record limit of 16MB per document.  So in October when the conversion team started putting real customer data in, we
started hitting the 16MB limit and things took a really
interesting turn.  A decision was made to hide this limitation from
the client and go live a month late, but in the meantime start a
skunkworks project to replace the vendor integration.
Without telling the vendor, either.  So we were simultaneously lying
to our clients <em>and</em> our valued technology partners.</p>
<p>Grumpy Old Dev at the time was
more Enthusiastic Young Dev and so he set his team to building the
replacement.  The vendor had roughtly 70 people engaged on the
project.  Grumpy Young Dev assigned 3 people to replace it. One
to design the database, one to build the back end to interface
with the database, and one to build the business logic/web services.</p>
<p>The client was told we would have a new version in January for them
to test.  It would fix the most critical defects they had
accepted in the original go-live.  But they weren’t told we were
rewriting the entire core system. In just under 2 months. When
the original project had taken over a year to launch.  With only 3
people. Over the holidays.  (You see where this is going.)</p>
<p>And so around about middle of December everyone working on the
project was told (not asked) to work through the holidays.<br>
Mind you, most of us had already been working 60-80 hour weeks
for the past 6 months just to make the original(ish) launch date.<br>
Everyone was burnt out.</p>
<p>At this point, if you’re reading and you’re not a delivery driven
technical person, you’re probably thinking this is crazy and
it’s time to quit. And you’d be right.  But.  Many of us who really
enjoy software development feel a bit like rock stars.  You spend
months or even years putting the show together, and then your
launch date is like a performance. And you want to hit your launch
date. Part of it is like theater people: the show must go on.  But
you also want to feel like a rock star when all your hard work hits
real users for the first time and you feel that thrill of <em>I did
that</em>.  <em>People like what I did.</em>  I overcame the impossible.  A software launch is like performing live theater for introverts.</p>
<p><img src="https://media1.tenor.com/m/FhNCzZkupJwAAAAC/goldengod-golden.gif" alt="i am a golden god"></p>
<p>So, by this point it’s almost Christmas. The team has basically
built the replacement software in one month of work.  There are
still some features to hash out.  But these are clever developers
and they’ve been hitting their marks and I know we are going to
make the testing date if they don’t burn out.</p>
<p>So when the CTO comes to me and says holidays are cancelled, I say
“OK.”…</p>
<p>And then for one of the proudest moments of my life, thinking back to my dad’s advice about getting the job done in spite of the boss…<br>
I tell my three guys “take the week off. I got this.”<br>
And I dial in every morning for the mandatory death march status
call with the CTO and I <em>lie</em>.</p>
<ul>
<li>“The team is working hard. Today
we hit milestone integration point #73.”</li>
<li>“The team made good progress yesterday, we finished another web
service.”</li>
</ul>
<p>Every day I showed up and told the big boss that we were
hard at work on stuff that we had already completed over the
previous month.</p>
<p>The guys came back a week later, refreshed.</p>
<p>And we hit our dates in January, went live with a great launch, and
were rock stars for a bit.  Maybe more like <a href="https://en.wikipedia.org/wiki/Herman's_Hermits">Herman’s Hermits</a> than <a href="https://en.wikipedia.org/wiki/The_Beatles">The Beatles</a>. But it still felt good.</p>
<p><img src="https://filmforum.org/do-not-enter-or-modify-or-erase/client-uploads/films/large_a_hard_days_night_blu-ray2a740.jpg" alt="a hard day’s night"></p>
<p>And that’s the time I lied to the CTO.</p>
<ul>
  
</ul>

    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Opening Windows in Linux with sockets, bare hands and 200 lines of C (216 pts)]]></title>
            <link>https://hereket.com/posts/from-scratch-x11-windowing/</link>
            <guid>40303661</guid>
            <pubDate>Wed, 08 May 2024 23:14:28 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://hereket.com/posts/from-scratch-x11-windowing/">https://hereket.com/posts/from-scratch-x11-windowing/</a>, See on <a href="https://news.ycombinator.com/item?id=40303661">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="content">
        
        
        <p><time datetime="2024-05-08">2024-05-08</time></p>
        
        <nav id="TableOfContents">
  <ul>
    <li><a href="#intro">Intro</a></li>
    <li><a href="#open-connection">Open connection</a></li>
    <li><a href="#creating-window">Creating window</a>
      <ul>
        <li><a href="#mapping-showing-window">Mapping (showing) window</a></li>
        <li><a href="#event-loop">Event loop</a></li>
      </ul>
    </li>
    <li><a href="#additional-functionality">Additional Functionality</a>
      <ul>
        <li><a href="#open-font">Open font</a></li>
        <li><a href="#create-graphic-context-gc">Create Graphic Context (GC)</a></li>
        <li><a href="#writing-text">Writing text</a></li>
        <li><a href="#final-code">Final code</a></li>
      </ul>
    </li>
    <li><a href="#conclusion">Conclusion</a></li>
  </ul>
</nav>
        <p><img src="https://hereket.com/posts/from-scratch-x11-windowing/images/base.png" alt="Simple x11 window opened with
sockets"></p>
<h2 id="intro">Intro</h2>
<p>In this post I want to create a single file C file program to open a windows
inside Linux without using xlib or any similar libraries. The idea is to explore
X11 protocol and see how it is used to interact with X server to create windows.</p>
<p>Before I had strong association that X11 was some magic thing to manipulate windows
and it’s resources. I was very surprised to learn that it is actually just a
“regular” <strong>network</strong> protocol for two parties to communicate like HTTP, FTP,
IMAP, SMPT and etc. But if in IMAP your have a server that contains all your
emails and you send it commands to get information about emails and get their
content. In X11 you have a server that contains all your windows and its
resources and you just communicate with it via a connection.</p>
<p>To do this the only resource we need is <a href="https://www.x.org/releases/X11R7.7/doc/xproto/x11protocol.html">X11
documentation</a>.
It is a very small document and can be easily consumed to better understand
window communication in linux systems that still use Xorg for window management.
The strange thing is that this document is way more approachable that Xlib’s
documentation which totally broke my illusion that Xlib was supposed to
simplify X11.</p>
<p>In this post we will try to implement this model:
<img src="https://hereket.com/posts/from-scratch-x11-windowing/images/client-server-communication.png" alt="X11 Initialization
request"></p>
<h2 id="open-connection">Open connection</h2>
<p>Xorg uses unix sockets instead of regular sockets for the communication with the
apps. (You can switch it to using regular sockets if you want to monitor
connections in wireshark. Look in  <a href="https://hereket.com/posts/monitoring-raw-x11-communication/">one of my old articles</a>
on how to do it) For us, regular users, it does not really matter as everything
will be pretty much the same with only tiny difference in socket setup.</p>
<pre tabindex="0"><code>int Socket = socket(AF_UNIX, SOCK_STREAM, 0);
VerifyOrDie(Socket &gt; 0, "Couldn't open a socket(...)");

struct sockaddr_un Address;
memset(&amp;Address, 0, sizeof(struct sockaddr_un));
Address.sun_family = AF_UNIX;
strncpy(Address.sun_path, "/tmp/.X11-unix/X0", sizeof(Address.sun_path)-1);

int Status = connect(Socket, (struct sockaddr *)&amp;Address, sizeof(Address));
VerifyOrDieWidthErrno(Status == 0, "Couldn't connect to a unix socket with connect(...)");
</code></pre><p>We just get socket from the kernel with <strong>socket(…)</strong> but use <strong>AF_UNIX</strong> instead
of <strong>AF_INET</strong> to tell it that we are plan to setup unix socket communication.
Then we use regular <strong>connect(…)</strong> to connect the socket. Another difference
is that we are using <strong>struct sockaddr_un</strong> for the connection description and
set it’s path to: <code>/tmp/.X11-unix/X0</code></p>
<p>To simplify error checking and to reduce code size I am using this two utility
function for easier error checking and reporting. They check if condition is
correct. If not they just print error and fully exit out of program execution.</p>
<pre tabindex="0"><code>void VerifyOrDie(int IsSuccess, const char *Message) {
    if(!IsSuccess)  {
        fprintf(stderr, "%s", Message);
        exit(13);
    }
}

void VerifyOrDieWidthErrno(int IsSuccess, const char *Message) {
    if(!IsSuccess)  {
        perror(Message);
        exit(13);
    }
}
</code></pre><p>This is pretty much. With just this we are ready to comuunicate with the X
server. Initially server expects a connection iniation from user and the very
first byte of the request must be a identification of communication byte order:
little endian or big endian. Documentation calls it MSB (Most significant byte)
and LSB(Least signification byte). After this byte is sent all data will be
processed as either little endian or big endian untill connection termination.
Two possible values for this first byte is ’l’ (0x6c) or ‘B’ (0x42).</p>
<!-- raw HTML omitted -->
<!-- raw HTML omitted -->
<!-- raw HTML omitted -->
<!-- raw HTML omitted -->
<!-- raw HTML omitted -->
<!-- raw HTML omitted -->
<!-- raw HTML omitted -->
<!-- raw HTML omitted -->
<!-- raw HTML omitted -->
<!-- raw HTML omitted -->
<!-- raw HTML omitted -->
<!-- raw HTML omitted -->
<!-- raw HTML omitted -->
<!-- raw HTML omitted -->
<p><img src="https://hereket.com/posts/from-scratch-x11-windowing/images/initialization-request.png" alt="X11 Initialization request"></p>
<p>In earlier implementation of this program I used struct to hold this
initialization data and later just serialize it to send over the socket. But
then I came to a conclusion that for demonstration purposes using just an array
and filling it ‘by hand’ is a better and simpler approach to show the concept.</p>
<p>Initial request seems like a lot of information to fill but if we skip
authorization process we can drastically reduce amount of code we need to write
and understand. So for this I chose to not use authorization with a little
trick. This give us just two parameters that we need to fill: stream endiannes
and major version (minor version is set automatically).</p>
<pre tabindex="0"><code>uint8_t InitializationRequest[12] = {};
InitializationRequest[0] = 'l';
InitializationRequest[2] = 11;
</code></pre><p>We are creating an array of 12 bytes. First byte we set to ’l’ or 0x6c and third
byte to 11 to indicate 11th version of the protocol. Everything else is set to 0
by initialization and indicates to X server that we will not be using and
sending any authorization data.</p>
<p>For this simplification to work we need to disable regular cookie based
authentication and allow all app on local machine to connect to X server
directly. To to that you need to open a terminal and type <code>xhost +local:</code></p>
<p>Later if you want to revert back you could use <strong>xhost -local:</strong> to force cookie
based authentication back. We could have implemented basic authentication but it
is not described in the general X11 documentation page linked above and we
decided to stick only to that one document. Plus it is not really that
interesting and we can save some screen space. If you forget to disable
authentication you will get this error when you run final program:</p>
<pre tabindex="0"><code>State: 0
MajorVersion: 11
MinorVersion: 0
AdditionalDataLength: 16
Reason: Authorization required, but no authorization protocol specified
</code></pre><p>With this details done we can just simply send our initialization request and
read back data.</p>
<pre tabindex="0"><code>char SendBuffer[16*1024] = {};
char ReadBuffer[16*1024] = {};

int BytesWritten = write(Socket, (char*)&amp;InitializationRequest, sizeof(InitializationRequest));
VerifyOrDie(BytesWritten == sizeof(InitializationRequest), "Wrong amount of bytes written during initialization");

int BytesRead = read(Socket, ReadBuffer, 8);
</code></pre><p>If we look into documentation, we will see that there are three possible
responses to initialization request: error, authentication and sucess. We can
safely ignore authentication as we are using it in this example. Error usually
means that the connection is refused (with some explanation why). No matter what
response type we get back first byte will always indicate response type: 0 -
Failed, 2 - Authenticate, 1 - Success. Success response will pretty big and
error/authenticate responses will be at least 8 bytes. For this reason we can
safely request to read 8 bytes to get basic info about what happend with our
request and info about server.</p>
<p>One importannt sidenote is that we create two buffers of 16 killobytes on the
stack and use it for reading and writing data. This is safe amount for basic
communication with server and helps use to not think about about rosource
management in this context. Also we don’t need to clear whole buffer after we
done processing it since specification allows having ‘dirty bytes’ in areas that
are not in the perimeter of current request/response.</p>
<p>After that we can choose how to continue our processing based on the first byte.</p>
<pre tabindex="0"><code>if(ReadBuffer[0] == RESPONSE_STATE_FAILED) {
    DumpResponseError(Socket, ReadBuffer);
}
else if(ReadBuffer[0] == RESPONSE_STATE_AUTHENTICATE) {
    AuthenticateX11();
}
else if(ReadBuffer[0] == RESPONSE_STATE_SUCCESS) {
...
}
</code></pre><p>Here are utility functions to dump information on error.</p>
<pre tabindex="0"><code>void DumpResponseError(int Socket, char* ReadBuffer) {
        uint8_t ReasonLength = ReadBuffer[1];
        uint16_t MajorVersion = *((uint16_t*)&amp;ReadBuffer[2]);
        uint16_t MinorVersion = *((uint16_t*)&amp;ReadBuffer[4]);
        uint16_t AdditionalDataLength = *((uint16_t*)&amp;ReadBuffer[6]); // Length in 4-byte units of "additional data"
        uint8_t *Message = (uint8_t*)&amp;ReadBuffer[8];

        int BytesRead = read(Socket, ReadBuffer + 8, READ_BUFFER_SIZE-8);

        printf("State: %d\n", ReadBuffer[0]);
        printf("MajorVersion: %d\n", MajorVersion);
        printf("MinorVersion: %d\n", MinorVersion);
        printf("AdditionalDataLength: %d\n", AdditionalDataLength);
        printf("Reason: %s\n", Message);
}


void AuthenticateX11() {
    fprintf(stderr, "Current version of the app does not support authentication.\n");
    exit(13);
}
</code></pre><p><img src="https://hereket.com/posts/from-scratch-x11-windowing/images/initialization-response-error.png" alt="X11 Initialization request"></p>
<p>So if we get anything but success we just print debug information and exit the
program. When we get normal success response we have to do some work to process
returned response.</p>
<pre tabindex="0"><code>BytesRead = read(Socket, ReadBuffer + 8, READ_BUFFER_SIZE-8);

uint16_t MajorVersion = *((uint16_t*)&amp;ReadBuffer[2]);
uint16_t MinorVersion = *((uint16_t*)&amp;ReadBuffer[4]);
uint16_t AdditionalDataLength = *((uint16_t*)&amp;ReadBuffer[6]); // Length in 4-byte units of "additional data"

uint32_t ResourceIdBase = *((uint32_t*)&amp;ReadBuffer[12]);
uint32_t ResourceIdMask = *((uint32_t*)&amp;ReadBuffer[16]);
uint16_t LengthOfVendor = *((uint16_t*)&amp;ReadBuffer[24]);
uint8_t NumberOfFormants = *((uint16_t*)&amp;ReadBuffer[29]);
uint8_t *Vendor = (uint8_t *)&amp;ReadBuffer[40];

int32_t VendorPad = PAD(LengthOfVendor);
int32_t FormatByteLength = 8 * NumberOfFormants;
int32_t ScreensStartOffset = 40 + LengthOfVendor + VendorPad + FormatByteLength;

uint32_t RootWindow = *((uint32_t*)&amp;ReadBuffer[ScreensStartOffset]);
uint32_t RootVisualId = *((uint32_t*)&amp;ReadBuffer[ScreensStartOffset + 32]);

GlobalIdBase = ResourceIdBase;
GlobalIdMask = ResourceIdMask;
GlobalRootWindow = RootWindow;
GlobalRootVisualId = RootVisualId;
</code></pre><p>First of all we read all output from server. We ask to read buffer size (16k)
minus 8 bytes that we already read an put it inside our buffer by offsetting 8
bytes of already read data. In my system this second read(…) returned 9804
bytes or 9804+8 = 9812 total response bytes for our initialization request.
Documentation show that this binary format contains quite a few information:
basic server info, root window, data formats, types, screen info, depth info,
visual types and etc. For a full blown production system it better to parse it
it all but in our exploratory phase we can get just the basics and get away with
it.</p>
<p>Major version, minor version, additional length are not required but I got them
to verify that everything is working as expected.</p>
<p>Then we get <strong>Resource Id Base</strong> and <strong>Resource Id Mask</strong>. What are these? Well
unfortunately even though X server is managing window resources it delegates
“naming” resource to us (the client). It might me for optimisation purposes. So
what this means is that when we need to create a window, crete graphic context
or font we must provide the “name” for it. This “name” is just an increasing
integer value with some processing. Id does not have to be contigous and can be
reused once resource is freed but we won’t be doing ID management here just get
increasing numbers without reusing. Also Resource ID’s have never top three bits
set. To get id we take our local id and OR it with resource base id.</p>
<p>In my system when I run this code I get base id of ‘0x3200000’ and mask of
‘0x1fffff’. So let’s if I have a local resouce if of 1 then I OR it with
0x3200000 and just mask it with 0x1fffff just to make sure that id has to three
bits ’turned off’.</p>
<p>Next we use  NumberOfFormants, VendorPad, FormatByteLength to get
ScreensStartOffset which in turn is used to get offset to first screen data
bytes. From this we need <strong>RootWindow</strong> and <strong>RootVisualId</strong>. And this is pretty
much all that wee needed from all that 9kb response. Then I just put them into
global variables for later use. (A better approach is to contain it all in a
struct but we are not architecturing software here but just exploring a
protocol). PAD macro just calculates padding to make sure that data length is
multiple of 4;</p>
<p>And with this we conclude intiation. There are a lot of words but basically we
just send one small request and get back large response.</p>
<h2 id="creating-window">Creating window</h2>
<p>Here we will be create a request to create a window.</p>
<p><img src="https://hereket.com/posts/from-scratch-x11-windowing/images/create-window-request.png" alt="X11 Initialization
request"></p>
<pre tabindex="0"><code>int32_t WindowId = GetNextId();
int32_t Depth = 0;
int32_t X = 100;
int32_t Y = 100;
uint32_t Width = 600;
uint32_t Height = 300;
uint32_t BorderWidth = 1;
int32_t CreateWindowFlagCount = 2;
int RequestLength = 8+CreateWindowFlagCount;

SendBuffer[0] = X11_REQUEST_CREATE_WINDOW;
SendBuffer[1] = Depth;
*((int16_t *)&amp;SendBuffer[2]) = RequestLength;
*((int32_t *)&amp;SendBuffer[4]) = WindowId;
*((int32_t *)&amp;SendBuffer[8]) = GlobalRootWindow;
*((int16_t *)&amp;SendBuffer[12]) = X;
*((int16_t *)&amp;SendBuffer[14]) = Y;
*((int16_t *)&amp;SendBuffer[16]) = Width;
*((int16_t *)&amp;SendBuffer[18]) = Height;
*((int16_t *)&amp;SendBuffer[20]) = BorderWidth;
*((int16_t *)&amp;SendBuffer[22]) = WINDOWCLASS_INPUTOUTPUT;
*((int32_t *)&amp;SendBuffer[24]) = GlobalRootVisualId;
*((int32_t *)&amp;SendBuffer[28]) = X11_FLAG_WIN_EVENT | X11_FLAG_BACKGROUND_PIXEL;
*((int32_t *)&amp;SendBuffer[32]) = 0xff000000;
*((int32_t *)&amp;SendBuffer[36]) = X11_EVENT_FLAG_EXPOSURE | X11_EVENT_FLAG_KEY_PRESS;

BytesWritten = write(Socket, (char *)&amp;SendBuffer, RequestLength*4);
</code></pre><p>Here we just setup basic variables for readability puproses and the fill an
array SenbBuffer array with relevant data. And then just send it. First byte is
always describes request. In this case we are creating a window and set it to 1.
Depth parameter is not that important and we can set it to 0. Next we calclate
RequestLength. This always indicates total request size including header and
extra parameters. The only caveat is that it is measured in 4 byte chunks. So we
have 32 required bytes and some extra. Thus we have 32/4=8 bytes and extra 2
four byte blocks for extra data.</p>
<p>Documentation explains this dynamic LISTofVALUE as “The value-list contains one
value for each bit set to 1 in the mask, from least significant to most
significant bit in the mask.” Since the mask is X11_FLAG_WIN_EVENT |
X11_FLAG_BACKGROUND_PIXEL or 0x00000002 | 0x00000800. This in turn give us
‘0b100000000010’. So background pixel will be first value to be provided and
second is a list of event masks. For the background we provided just a black
color with 0xff000000. You can easily change it to green by replacing it with
0xff00ff00.</p>
<p>Next value (starting byte 36) we provide xored list of events that we want to
recieve back from X server. In this case we want to get back Exposure (0x8000) and
KeyPress (0x0001) events. These will be important later once we start processing
event from server.</p>
<p>One thing I didn’t show is <strong>GetNextId()</strong>. It is just a utility function of the
functionality discussed earlier about how to “generate” new if for a resource.
For simplicity it uses global index of last id and increases it by on each
iteration.</p>
<pre tabindex="0"><code>int32_t GetNextId() {
    int32_t Result = (GlobalIdMask &amp; GlobalId) | GlobalIdBase;
    GlobalId += 1;
    return Result;
}
</code></pre><h3 id="mapping-showing-window">Mapping (showing) window</h3>
<p>By this time we have already create a window resource on the server side. It is
not shown to the screen yet because we need to map it first. Compared to
creating window this request is pretty small.</p>
<pre tabindex="0"><code>SendBuffer[0] = X11_REQUEST_MAP_WINDOW;
SendBuffer[1] = 0; // NOTE: Unused
*((int16_t *)&amp;SendBuffer[2]) = 2;
*((int32_t *)&amp;SendBuffer[4]) = WindowId;

BytesWritten = write(Socket, (char *)&amp;SendBuffer, 2*4);
</code></pre><p><img src="https://hereket.com/posts/from-scratch-x11-windowing/images/map-window-request.png" alt="X11 Initialization
request"></p>
<p>As usual we user first byte to set request type. This time it is “Map Window” or
0x8. Second byte is unused and can be set to anything. Third byte is request
size which we set to 2 (result of 8/4). And the last byte parameter we set the
window ID created in the last step.</p>
<p>With this we are pretty much done with basic window opening. Now if we just put
something like <strong>sleep(5)</strong> at the end of the code we will get a window which
will be shown for 5 seconds. Then after the 5 seconds are passed the program
will close and X server will recycle all resources.</p>
<h3 id="event-loop">Event loop</h3>
<p>Instead of sleeping let’s try to actually create a loop that will listen events
sent back to us from server and just block when there is nothing to do or to
show. Since when creating we indicated that we are interested in Exposure and
KeyPress events there will be event to notify about regions that need to up
repainted and pressed keys.</p>
<p>For this I chose to use linux regular polling mechanism which will block us
untill we have something to do. Nothing special. Just setup one socket
descriptor into list.</p>
<pre tabindex="0"><code>struct pollfd PollDescriptors[1] = {};
PollDescriptors[0].fd = Socket;
PollDescriptors[0].events = POLLIN;
int32_t DescriptorCount = 1;
</code></pre><p>Then we can create an endless loop which will be check if a IsProgramRunning
variable set to true or false (1 or 0).</p>
<pre tabindex="0"><code>int32_t IsProgramRunning = 1;
while(IsProgramRunning){
    int32_t EventCount = poll(PollDescriptors, DescriptorCount, -1);

    if(PollDescriptors[0].revents &amp; POLLERR) {
        fprintf(stderr, "------- Error\n");
    }

    if(PollDescriptors[0].revents &amp; POLLHUP) {
        printf("---- Connection close\n");
        IsProgramRunning = 0;
    }

    GetAndProcessReply(PollDescriptors[0].fd);
}
</code></pre><p>When we call poll(…) we set timeout to -1 to make sure it never times out.
Once there is an event the program will unblock and continue execution. Then we
check if the event was error or we the connection hung up. On error we just log
it and continue like nothing happened. On POLLHUP (program close) just terminate
program. Else we just process the reply.</p>
<pre tabindex="0"><code>void PrintResponseError(char *Data, int32_t Size) {
    char ErrorCode = Data[1];
    printf("\033[0;31m");
    printf("Response Error: [%d]", ErrorCode);
    printf("\033[0m\n");
}

void PrintAndProcessEvent(char *Data, int32_t Size) {
    char EventCode = Data[0];
    printf("Some event occured: %d\n", EventCode);
}

void GetAndProcessReply(int Socket) {
    char Buffer[1024] = {};
    int32_t BytesRead = read(Socket, Buffer, 1024);
    uint8_t Code = Buffer[0];

    if(Code == 0) {
        PrintResponseError(Buffer, BytesRead);
    } else if (Code == 1) {
        printf("---------------- Reply to request\n");
    } else {
        PrintAndProcessEvent(Buffer, BytesRead);
    }
}
</code></pre><p>And with this we have a “fully functional” window with event loop. Even though
the processing is pretty simple and we just log errors and events but still from
here it will be that hard to extend and try more advanced events processing.</p>
<p>Whole code for simple version for this program (about 200 lines of code)</p>
<pre tabindex="0"><code>#include &lt;sys/socket.h&gt;
#include &lt;sys/un.h&gt;
#include &lt;stdio.h&gt;
#include &lt;stdint.h&gt;
#include &lt;stdlib.h&gt;
#include &lt;unistd.h&gt;
#include &lt;fcntl.h&gt;
#include &lt;poll.h&gt;


int32_t GlobalId = 0;
int32_t GlobalIdBase = 0;
int32_t GlobalIdMask = 0;
int32_t GlobalRootWindow = 0;
int32_t GlobalRootVisualId = 0;

#define READ_BUFFER_SIZE 16*1024

#define RESPONSE_STATE_FAILED 0
#define RESPONSE_STATE_SUCCESS 1
#define RESPONSE_STATE_AUTHENTICATE 2

#define X11_REQUEST_CREATE_WINDOW 1
#define X11_REQUEST_MAP_WINDOW 8
#define X11_REQUEST_IMAGE_TEXT_8 76
#define X11_REQUEST_OPEN_FONT 45
#define X11_REQUEST_CREATE_GC 55


#define X11_EVENT_FLAG_KEY_PRESS 0x00000001
#define X11_EVENT_FLAG_KEY_RELEASE 0x00000002
#define X11_EVENT_FLAG_EXPOSURE 0x8000

#define PAD(N) ((4 - (N % 4)) % 4)

void VerifyOrDie(int IsSuccess, const char *Message) {
    if(!IsSuccess)  {
        fprintf(stderr, "%s", Message);
        exit(13);
    }
}

void VerifyOrDieWidthErrno(int IsSuccess, const char *Message) {
    if(!IsSuccess)  {
        perror(Message);
        exit(13);
    }
}

void DumpResponseError(int Socket, char* ReadBuffer) {
        uint8_t ReasonLength = ReadBuffer[1];
        uint16_t MajorVersion = *((uint16_t*)&amp;ReadBuffer[2]);
        uint16_t MinorVersion = *((uint16_t*)&amp;ReadBuffer[4]);
        uint16_t AdditionalDataLength = *((uint16_t*)&amp;ReadBuffer[6]); // Length in 4-byte units of "additional data"
        uint8_t *Message = (uint8_t*)&amp;ReadBuffer[8];

        int BytesRead = read(Socket, ReadBuffer + 8, READ_BUFFER_SIZE-8);

        printf("State: %d\n", ReadBuffer[0]);
        printf("MajorVersion: %d\n", MajorVersion);
        printf("MinorVersion: %d\n", MinorVersion);
        printf("AdditionalDataLength: %d\n", AdditionalDataLength);
        printf("Reason: %s\n", Message);
}

void AuthenticateX11() {
    fprintf(stderr, "Current version of the app does not support authentication.\n");
    fprintf(stderr, "Please run 'xhost +local:' in your terminal to disable cookie based authentication\n");
    fprintf(stderr, "and allow local apps to communication with Xorg without it.");
}

int32_t GetNextId() {
    int32_t Result = (GlobalIdMask &amp; GlobalId) | GlobalIdBase;
    GlobalId += 1;
    return Result;
}

void PrintResponseError(char *Data, int32_t Size) {
    char ErrorCode = Data[1];
    printf("\033[0;31m");
    printf("Response Error: [%d]", ErrorCode);
    printf("\033[0m\n");
}

void PrintAndProcessEvent(char *Data, int32_t Size) {
    char EventCode = Data[0];
    printf("Some event occured: %d\n", EventCode);

}

void GetAndProcessReply(int Socket) {
    char Buffer[1024] = {};
    int32_t BytesRead = read(Socket, Buffer, 1024);

    uint8_t Code = Buffer[0];

    if(Code == 0) {
        PrintResponseError(Buffer, BytesRead);
    } else if (Code == 1) {
        printf("---------------- Unexpected reply\n");
    } else {
        // NOTE: Event?
        PrintAndProcessEvent(Buffer, BytesRead);
    }
}

int main(){
    int Socket = socket(AF_UNIX, SOCK_STREAM, 0);
    VerifyOrDie(Socket &gt; 0, "Couldn't open a socket(...)");

    struct sockaddr_un Address;
    memset(&amp;Address, 0, sizeof(struct sockaddr_un));
    Address.sun_family = AF_UNIX;
    strncpy(Address.sun_path, "/tmp/.X11-unix/X0", sizeof(Address.sun_path)-1);

    int Status = connect(Socket, (struct sockaddr *)&amp;Address, sizeof(Address));
    VerifyOrDieWidthErrno(Status == 0, "Couldn't connect to a unix socket with connect(...)");

    char SendBuffer[16*1024] = {};
    char ReadBuffer[16*1024] = {};

    uint8_t InitializationRequest[12] = {};
    InitializationRequest[0] = 'l';
    InitializationRequest[1] = 0;
    InitializationRequest[2] = 11;

    int BytesWritten = write(Socket, (char*)&amp;InitializationRequest, sizeof(InitializationRequest));
    VerifyOrDie(BytesWritten == sizeof(InitializationRequest), "Wrong amount of bytes written during initialization");

    int BytesRead = read(Socket, ReadBuffer, 8);

    if(ReadBuffer[0] == RESPONSE_STATE_FAILED) {
        DumpResponseError(Socket, ReadBuffer);
    }
    else if(ReadBuffer[0] == RESPONSE_STATE_AUTHENTICATE) {
        AuthenticateX11();
    }
    else if(ReadBuffer[0] == RESPONSE_STATE_SUCCESS) {
        printf("INIT Response SUCCESS. BytesRead: %d\n", BytesRead);

        BytesRead = read(Socket, ReadBuffer + 8, READ_BUFFER_SIZE-8);
        printf("---------------------------%d\n", BytesRead);

        /* -------------------------------------------------------------------------------- */
        uint8_t _Unused = ReadBuffer[1];
        uint16_t MajorVersion = *((uint16_t*)&amp;ReadBuffer[2]);
        uint16_t MinorVersion = *((uint16_t*)&amp;ReadBuffer[4]);
        uint16_t AdditionalDataLength = *((uint16_t*)&amp;ReadBuffer[6]); // Length in 4-byte units of "additional data"

        uint32_t ResourceIdBase = *((uint32_t*)&amp;ReadBuffer[12]);
        uint32_t ResourceIdMask = *((uint32_t*)&amp;ReadBuffer[16]);
        uint16_t LengthOfVendor = *((uint16_t*)&amp;ReadBuffer[24]);
        uint8_t NumberOfFormants = *((uint16_t*)&amp;ReadBuffer[29]);
        uint8_t *Vendor = (uint8_t *)&amp;ReadBuffer[40];

        int32_t VendorPad = PAD(LengthOfVendor);
        int32_t FormatByteLength = 8 * NumberOfFormants;
        int32_t ScreensStartOffset = 40 + LengthOfVendor + VendorPad + FormatByteLength;

        uint32_t RootWindow = *((uint32_t*)&amp;ReadBuffer[ScreensStartOffset]);
        uint32_t RootVisualId = *((uint32_t*)&amp;ReadBuffer[ScreensStartOffset + 32]);

        GlobalIdBase = ResourceIdBase;
        GlobalIdMask = ResourceIdMask;
        GlobalRootWindow = RootWindow;
        GlobalRootVisualId = RootVisualId;

        printf("Base: %d\n", ResourceIdBase);
        printf("IdMask: %d\n", ResourceIdMask);
        printf("LengthOfVendor: %d\n", LengthOfVendor);

        /* -------------------------------------------------------------------------------- */
        // ------------------------------ Create Window
        int32_t WindowId = GetNextId();
        int32_t Depth = 0;
        int32_t X = 100;
        int32_t Y = 100;
        uint32_t Width = 600;
        uint32_t Height = 300;
        uint32_t BorderWidth = 1;
        int32_t CreateWindowFlagCount = 2;
        int RequestLength = 8+CreateWindowFlagCount;


#define WINDOWCLASS_COPYFROMPARENT 0
#define WINDOWCLASS_INPUTOUTPUT 1
#define WINDOWCLASS_INPUTONLY 2

#define X11_FLAG_BACKGROUND_PIXEL 0x00000002 
#define X11_FLAG_WIN_EVENT 0x00000800 

        SendBuffer[0] = X11_REQUEST_CREATE_WINDOW;
        SendBuffer[1] = Depth;
        *((int16_t *)&amp;SendBuffer[2]) = RequestLength;
        *((int32_t *)&amp;SendBuffer[4]) = WindowId;
        *((int32_t *)&amp;SendBuffer[8]) = GlobalRootWindow;
        *((int16_t *)&amp;SendBuffer[12]) = X;
        *((int16_t *)&amp;SendBuffer[14]) = Y;
        *((int16_t *)&amp;SendBuffer[16]) = Width;
        *((int16_t *)&amp;SendBuffer[18]) = Height;
        *((int16_t *)&amp;SendBuffer[20]) = BorderWidth;
        *((int16_t *)&amp;SendBuffer[22]) = WINDOWCLASS_INPUTOUTPUT;
        *((int32_t *)&amp;SendBuffer[24]) = GlobalRootVisualId;
        *((int32_t *)&amp;SendBuffer[28]) = X11_FLAG_WIN_EVENT | X11_FLAG_BACKGROUND_PIXEL;
        *((int32_t *)&amp;SendBuffer[32]) = 0xff000000;
        *((int32_t *)&amp;SendBuffer[36]) = X11_EVENT_FLAG_EXPOSURE | X11_EVENT_FLAG_KEY_PRESS;


        BytesWritten = write(Socket, (char *)&amp;SendBuffer, RequestLength*4);
        printf("Create Window: BytesWritten: %d\n", BytesWritten);


        // ------------------------------ Map Window

        SendBuffer[0] = X11_REQUEST_MAP_WINDOW;
        SendBuffer[1] = 0;
        *((int16_t *)&amp;SendBuffer[2]) = 2;
        *((int32_t *)&amp;SendBuffer[4]) = WindowId;

        BytesWritten = write(Socket, (char *)&amp;SendBuffer, 2*4);
        printf("Map Window: BytesWritten: %d\n", BytesWritten);


        /* ------------------------------------------------------------------------------- */

        struct pollfd PollDescriptors[1] = {};
        PollDescriptors[0].fd = Socket;
        PollDescriptors[0].events = POLLIN;
        int32_t DescriptorCount = 1;

        int32_t IsProgramRunning = 1;
        while(IsProgramRunning){
            int32_t EventCount = poll(PollDescriptors, DescriptorCount, -1);

            if(PollDescriptors[0].revents &amp; POLLERR) {
                printf("------- Error\n");
            }

            if(PollDescriptors[0].revents &amp; POLLHUP) {
                printf("---- Connection close\n");
                IsProgramRunning = 0;
            }

            GetAndProcessReply(PollDescriptors[0].fd);
        }
    }

}
</code></pre><p>To compile it just run this from terminal:
<code>gcc basic.c -o basic</code></p>
<p>Here is video showing how it looks:

    
<iframe height="400" src="https://www.youtube.com/embed/OUMEAjHHcOI" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen=""></iframe>


</p>
<h2 id="additional-functionality">Additional Functionality</h2>
<p>Now let’s try to add some extra functionality and see if will be to complex to
extend it and see if we can actually see something somewhat useful on the
screen.</p>
<p>First of all I will make tiny refactoring and move some code into functions to
make code a bit more readable. Specifically we will move intiation, window
creation and mapping into their specific functions. The event loop will remain
the same for now.</p>
<pre tabindex="0"><code>int SetupStatus = X_InitiateConnection(Socket);

if(SetupStatus == 0) {
    // X, Y, Width, Height setup
    int WindowId = X_CreatWindow(Socket, X, Y, Width, Height);
    X_MapWindow(Socket, WindowId);

    // Event loop
}
</code></pre><p><strong>X_InitiateConnection</strong>, <strong>X_CreatWindow</strong>, <strong>X_MapWindow</strong> are just condensed
code from previous section.</p>
<h3 id="open-font">Open font</h3>
<p>In the end I am planning to use some text for that we will need to ask for a
front from the X server. The drill is pretty much the same thing. Prepare data,
stuck into buffer and send it to the server.</p>
<pre tabindex="0"><code>void X_OpenFont(i32 Socket, i8 *FontName, i32 FontId) {
    char SendBuffer[16*1024] = {};
    int BytesWritten = 0;
    int BytesRead = 0;

    i32 FontNameLength = strlen((char *)FontName);
    i32 Pad = PAD(FontNameLength);
    int RequestLength = (3 + (FontNameLength + Pad)/4);

    SendBuffer[0] = X11_REQUEST_OPEN_FONT;
    SendBuffer[1] = 0;
    *((u16 *)&amp;SendBuffer[2]) = RequestLength;
    *((u32 *)&amp;SendBuffer[4]) = FontId;
    *((u16 *)&amp;SendBuffer[8]) = FontNameLength;
    strncpy(SendBuffer + 12, (char *)FontName, FontNameLength);

    i32 WriteSize = 12 + FontNameLength + Pad;
    BytesWritten = write(Socket, (char *)&amp;SendBuffer, WriteSize);
}
</code></pre><p><img src="https://hereket.com/posts/from-scratch-x11-windowing/images/open-font-request.png" alt="X11 Initialization
request"></p>
<p>FontId is something that we as client generate ourself. For that we have
GetNextId(). We pass it and a font name to the function requesting server to
create font resource with specified ID and try to find closest match to the name
we provided.</p>
<h3 id="create-graphic-context-gc">Create Graphic Context (GC)</h3>
<p>A lot of window operations require graphic context to do it’s operations. So we
need to get it before we can do any graphic changing actions.</p>
<pre tabindex="0"><code>void X_CreateGC(int32_t Socket, int32_t GcId, int32_t FontId) {
    char SendBuffer[16*1024] = {};

    int32_t CreateGcFlagCount = 3;
    int RequestLength = 4 + CreateGcFlagCount;

    SendBuffer[0] = X11_REQUEST_CREATE_GC;
    SendBuffer[1] = 0;
    *((int16_t *)&amp;SendBuffer[2]) = RequestLength;
    *((int32_t *)&amp;SendBuffer[4]) = GcId;
    *((int32_t *)&amp;SendBuffer[8]) = GlobalRootWindow;
    *((int32_t *)&amp;SendBuffer[12]) = X11_FLAG_FG | X11_FLAG_BG | X11_FLAG_FONT;
    *((int32_t *)&amp;SendBuffer[16]) = 0xFF00FF00; // Foreground
    *((int32_t *)&amp;SendBuffer[20]) = 0xFF000000; // Background
    *((int32_t *)&amp;SendBuffer[24]) = FontId; // Font

    write(Socket, (char *)&amp;SendBuffer, RequestLength*4);
}
</code></pre><p><img src="https://hereket.com/posts/from-scratch-x11-windowing/images/create-gc-request.png" alt="X11 Initialization
request"></p>
<p>Here we see similar patter of passing parameters based on mask and it’s bit
offsets. We have required 16 bytes of setup data and extra variable bytes based
on how much data we want to set. In this example we are setting just font,
background and forground colors. Thus we set X11_FLAG_FG | X11_FLAG_BG |
X11_FLAG_FONT mask bits and pass appropriate parameters at the end of the
request. There are quite a few parameters that can be passed like different
stroke types, drawing functions and etc. But we stick to simplicity for now.</p>
<h3 id="writing-text">Writing text</h3>
<p>The last function for today will be text writing. It is pretty simple.</p>
<pre tabindex="0"><code>void WriteText(int Socket, int WindowId, int GCid, i16 X, i16 Y, const char *Text, i32 TextLength) {
    char SendBuffer[16*1024] = {};

    u32 ContentLength = 4 + (TextLength + PAD(TextLength))/4;

    SendBuffer[0] = (u8)X11_REQUEST_IMAGE_TEXT_8;
    SendBuffer[1] = TextLength;
    *((i16 *)&amp;SendBuffer[2]) = ContentLength; 
    *((i32 *)&amp;SendBuffer[4]) = WindowId;
    *((i32 *)&amp;SendBuffer[8]) = GCid;
    *((i16 *)&amp;SendBuffer[12]) = X; 
    *((i16 *)&amp;SendBuffer[14]) = Y; 

    strncpy(&amp;SendBuffer[16], (char *)Text, TextLength);
    write(Socket, (char *)&amp;SendBuffer, ContentLength*4);
}
</code></pre><p><img src="https://hereket.com/posts/from-scratch-x11-windowing/images/image-text-8.png" alt="X11 Initialization
request"></p>
<p>We repeated this drill so many times that it should be trivial by now.
Calculating length in 4 byte blocks. Set required where we want to draw
(window), which brush to draw with (graphic context) and X,Y offsets in the
window where we want to draw. At the of request copy the text that we want to
draw. And that is it.</p>
<p>The only thing left if draw this text on ech refresh. Here for simplicity
purposes we will be drawing text on each event but correct approach would have
been to draw it on Expose events. But the text already became too be big so
simplicity wins today.</p>
<p>So in the main loop we add this code:</p>
<pre tabindex="0"><code>const char* t1 = "Hello, World!";
const char* t2 = "This is a test text directly written to X";
const char* t3 = "Whooha. Is this even legal? Let's keep a secret!";

WriteText(Socket, WindowId, GcId, 10, 20, t1, strlen(t1));
WriteText(Socket, WindowId, GcId, 10, 35, t2, strlen(t2));
WriteText(Socket, WindowId, GcId, 10, 50, t3, strlen(t3));
</code></pre><p>And that is it! Hey we build a simple window with text. Congratulations.</p>
<p>Here is full source code which contains a bit more functionality than discussed
in post (text movement and more detailed debug info):</p>
<h3 id="final-code">Final code</h3>
<pre tabindex="0"><code>#include &lt;sys/socket.h&gt;
#include &lt;sys/un.h&gt;
#include &lt;stdio.h&gt;
#include &lt;stdint.h&gt;
#include &lt;stdlib.h&gt;
#include &lt;unistd.h&gt;
#include &lt;fcntl.h&gt;
#include &lt;poll.h&gt;


int32_t GlobalId = 0;
int32_t GlobalIdBase = 0;
int32_t GlobalIdMask = 0;
int32_t GlobalRootWindow = 0;
int32_t GlobalRootVisualId = 0;

int32_t GlobalTextOffsetX = 10;
int32_t GlobalTextOffsetY = 20;

#define READ_BUFFER_SIZE 16*1024

#define RESPONSE_STATE_FAILED 0
#define RESPONSE_STATE_SUCCESS 1
#define RESPONSE_STATE_AUTHENTICATE 2

#define X11_REQUEST_CREATE_WINDOW 1
#define X11_REQUEST_MAP_WINDOW 8
#define X11_REQUEST_IMAGE_TEXT_8 76
#define X11_REQUEST_OPEN_FONT 45
#define X11_REQUEST_CREATE_GC 55


#define X11_EVENT_FLAG_KEY_PRESS 0x00000001
#define X11_EVENT_FLAG_KEY_RELEASE 0x00000002
#define X11_EVENT_FLAG_EXPOSURE 0x8000


#define WINDOWCLASS_COPYFROMPARENT 0
#define WINDOWCLASS_INPUTOUTPUT 1
#define WINDOWCLASS_INPUTONLY 2

#define X11_FLAG_BACKGROUND_PIXEL 0x00000002 
#define X11_FLAG_WIN_EVENT 0x00000800 

#define X11_FLAG_FG 0x00000004
#define X11_FLAG_BG 0x00000008
#define X11_FLAG_FONT 0x00004000
#define X11_FLAG_GC_EXPOSURE 0x00010000

#define PAD(N) ((4 - (N % 4)) % 4)

void VerifyOrDie(int IsSuccess, const char *Message) {
    if(!IsSuccess)  {
        fprintf(stderr, "%s", Message);
        exit(13);
    }
}

void VerifyOrDieWidthErrno(int IsSuccess, const char *Message) {
    if(!IsSuccess)  {
        perror(Message);
        exit(13);
    }
}

void DumpResponseError(int Socket, char* ReadBuffer) {
        uint8_t ReasonLength = ReadBuffer[1];
        uint16_t MajorVersion = *((uint16_t*)&amp;ReadBuffer[2]);
        uint16_t MinorVersion = *((uint16_t*)&amp;ReadBuffer[4]);
        uint16_t AdditionalDataLength = *((uint16_t*)&amp;ReadBuffer[6]); // Length in 4-byte units of "additional data"
        uint8_t *Message = (uint8_t*)&amp;ReadBuffer[8];

        int BytesRead = read(Socket, ReadBuffer + 8, READ_BUFFER_SIZE-8);

        printf("State: %d\n", ReadBuffer[0]);
        printf("MajorVersion: %d\n", MajorVersion);
        printf("MinorVersion: %d\n", MinorVersion);
        printf("AdditionalDataLength: %d\n", AdditionalDataLength);
        printf("Reason: %s\n", Message);
}

void AuthenticateX11() {
    fprintf(stderr, "Current version of the app does not support authentication.\n");
    fprintf(stderr, "Please run 'xhost +local:' in your terminal to disable cookie based authentication\n");
    fprintf(stderr, "and allow local apps to communication with Xorg without it.");
}

int32_t GetNextId() {
    int32_t Result = (GlobalIdMask &amp; GlobalId) | GlobalIdBase;
    GlobalId += 1;
    return Result;
}

void PrintResponseError(char *Data, int32_t Size) {
    char ErrorCode = Data[1];
    const char *ErrorNames[] = {
        "Unknown Error",
        "Request",
        "Value",
        "Window",
        "Pixmap",
        "Atom",
        "Cursor",
        "Font",
        "Match",
        "Drawable",
        "Access",
        "Alloc",
        "Colormap",
        "GContext",
        "IDChoice",
        "Name",
        "Length",
        "Implementation",
    };

    const char* ErrorName = "Unknown error";
    if(ErrorCode &lt; sizeof(ErrorNames) / sizeof(ErrorNames[0])) {
        ErrorName = ErrorNames[ErrorCode];
    }

    
    uint16_t Minor = *((uint16_t*)&amp;Data[8]);
    uint8_t Major = *((uint8_t*)&amp;Data[10]);

    printf("\033[0;31m");
    printf("Response Error: [%d] %s", ErrorCode, ErrorName);
    printf("	Minor: %d, Major: %d", Minor, Major);
    printf("\033[0m\n");


}

void PrintAndProcessEvent(char *Data, int32_t Size) {
    char EventCode = Data[0];
    const char* EventNames[] = {
        "-- Wrong Event Code --",
        "-- Wrong Event Code --",
        "KeyPress",
        "KeyRelease",
        "ButtonPress",
        "ButtonRelease",
        "MotionNotify",
        "EnterNotify",
        "LeaveNotify",
        "FocusIn",
        "FocusOut",
        "KeymapNotify",
        "Expose",
        "GraphicsExposure",
        "NoExposure",
        "VisibilityNotify",
        "CreateNotify",
        "DestroyNotify",
        "UnmapNotify",
        "MapNotify",
        "MapRequest",
        "ReparentNotify",
        "ConfigureNotify",
        "ConfigureRequest",
        "GravityNotify",
        "ResizeRequest",
        "CirculateNotify",
        "CirculateRequest",
        "PropertyNotify",
        "SelectionClear",
        "SelectionRequest",
        "SelectionNotify",
        "ColormapNotify",
        "ClientMessage",
        "MappingNotify",
    };

#define REPLY_EVENT_CODE_KEY_PRESS 2
#define REPLY_EVENT_CODE_EXPOSE 12

const char* TERMINAL_TEXT_COLOR_RED = "\033[0;32m";
const char* TERMINAL_TEXT_COLOR_CLEAR = "\033[0m";

    if(EventCode == REPLY_EVENT_CODE_EXPOSE) {
        // NOTE: Exposure event
        const char *EventName = "Expose";
        uint16_t SequenceNumber = *((uint16_t*)&amp;Data[2]);
        uint32_t Window = *((uint32_t*)&amp;Data[4]);
        uint16_t X = *((uint16_t*)&amp;Data[8]);
        uint16_t Y = *((uint16_t*)&amp;Data[10]);
        uint16_t Width = *((uint16_t*)&amp;Data[12]);
        uint16_t Height = *((uint16_t*)&amp;Data[14]);
        uint16_t Count = *((uint16_t*)&amp;Data[16]);

        printf(TERMINAL_TEXT_COLOR_RED);
            printf("%s: ", EventName);
        printf(TERMINAL_TEXT_COLOR_CLEAR);

        printf("Seq %d, ", SequenceNumber);
        printf("Win %d: ", Window);
        printf("X %d: ", X);
        printf("Y %d: ", Y);
        printf("Width %d: ", Width);
        printf("Height %d: ", Height);
        printf("Count %d: ", Count);
        printf("\n");
        /* printf("%s: Seq %d\n", EventName, SequenceNumber); */
    } else if(EventCode == REPLY_EVENT_CODE_KEY_PRESS) {
        const char *EventName = "KeyPress";
        char KeyCode = Data[1];
        uint16_t SequenceNumber = *((uint16_t*)&amp;Data[2]);
        uint32_t TimeStamp = *((uint32_t*)&amp;Data[4]);
        uint32_t RootWindow = *((uint32_t*)&amp;Data[8]);
        uint32_t EventWindow = *((uint32_t*)&amp;Data[12]);
        uint32_t ChildWindow = *((uint32_t*)&amp;Data[16]); // NOTE: Always 0
        int16_t RootX = *((int16_t*)&amp;Data[20]);
        int16_t RootY = *((int16_t*)&amp;Data[22]);
        int16_t EventX = *((int16_t*)&amp;Data[24]);
        int16_t EventY = *((int16_t*)&amp;Data[26]);
        int16_t SetOfKeyButMask = *((int16_t*)&amp;Data[28]);
        int8_t IsSameScreen = *((int8_t*)&amp;Data[30]);

        printf(TERMINAL_TEXT_COLOR_RED);
            printf("%s: ", EventName);
        printf(TERMINAL_TEXT_COLOR_CLEAR);

        // NOTE: Temporary hack that will not work everywhere
        int StepSize = 10;
        if(KeyCode == 25) { GlobalTextOffsetY += StepSize; }
        if(KeyCode == 39) { GlobalTextOffsetY -= StepSize; }
        if(KeyCode == 38) { GlobalTextOffsetX -= StepSize; }
        if(KeyCode == 40) { GlobalTextOffsetX += StepSize; }

        printf("Code %u, ", (uint8_t)KeyCode);
        printf("Seq %d, ", SequenceNumber);
        printf("Time %d, ", TimeStamp);
        printf("Root %d, ", RootWindow);
        printf("EventW %d, ", EventWindow);
        printf("Child %d, ", ChildWindow);
        printf("RX %d, ", RootX);
        printf("RY %d, ", RootY);
        printf("EX %d, ", EventX);
        printf("EY %d, ", EventY);
        printf("\n");
    } else {
        const char* EventName = " - Unknown Event Code -";
        if(EventCode &lt; sizeof(EventNames) / sizeof(EventNames[0])) {
            EventName = EventNames[EventCode];
        }
        // printf("-------------Event: %s\n", EventName);
        // for(int i = 0; i &lt; Size; i++) {
            // printf("%c", Data[i]);
        // }
        // printf("\n");
    }

}

void GetAndProcessReply(int Socket) {
    char Buffer[1024] = {};
    int32_t BytesRead = read(Socket, Buffer, 1024);

    uint8_t Code = Buffer[0];

    if(Code == 0) {
        PrintResponseError(Buffer, BytesRead);
    } else if (Code == 1) {
        printf("---------------- Unexpected reply\n");
    } else {
        // NOTE: Event?
        PrintAndProcessEvent(Buffer, BytesRead);
    }
}

int X_InitiateConnection(int Socket) { 
    // TODO: Remove global variables and put them into 'connection' struct.
    int SetupStatus = 1;
    char SendBuffer[16*1024] = {};
    char ReadBuffer[16*1024] = {};

    uint8_t InitializationRequest[12] = {};
    InitializationRequest[0] = 'l';
    InitializationRequest[1] = 0;
    InitializationRequest[2] = 11;

    int BytesWritten = write(Socket, (char*)&amp;InitializationRequest, sizeof(InitializationRequest));
    VerifyOrDie(BytesWritten == sizeof(InitializationRequest), "Wrong amount of bytes written during initialization");

    int BytesRead = read(Socket, ReadBuffer, 8);

    if(ReadBuffer[0] == RESPONSE_STATE_FAILED) {
        DumpResponseError(Socket, ReadBuffer);
    }
    else if(ReadBuffer[0] == RESPONSE_STATE_AUTHENTICATE) {
        AuthenticateX11();
    }
    else if(ReadBuffer[0] == RESPONSE_STATE_SUCCESS) {
        printf("INIT Response SUCCESS. BytesRead: %d\n", BytesRead);

        BytesRead = read(Socket, ReadBuffer + 8, READ_BUFFER_SIZE-8);
        printf("---------------------------%d\n", BytesRead);

        /* -------------------------------------------------------------------------------- */
        uint8_t _Unused = ReadBuffer[1];
        uint16_t MajorVersion = *((uint16_t*)&amp;ReadBuffer[2]);
        uint16_t MinorVersion = *((uint16_t*)&amp;ReadBuffer[4]);
        uint16_t AdditionalDataLength = *((uint16_t*)&amp;ReadBuffer[6]); // Length in 4-byte units of "additional data"

        uint32_t ResourceIdBase = *((uint32_t*)&amp;ReadBuffer[12]);
        uint32_t ResourceIdMask = *((uint32_t*)&amp;ReadBuffer[16]);
        uint16_t LengthOfVendor = *((uint16_t*)&amp;ReadBuffer[24]);
        uint8_t NumberOfFormants = *((uint16_t*)&amp;ReadBuffer[29]);
        uint8_t *Vendor = (uint8_t *)&amp;ReadBuffer[40];

        int32_t VendorPad = PAD(LengthOfVendor);
        int32_t FormatByteLength = 8 * NumberOfFormants;
        int32_t ScreensStartOffset = 40 + LengthOfVendor + VendorPad + FormatByteLength;

        uint32_t RootWindow = *((uint32_t*)&amp;ReadBuffer[ScreensStartOffset]);
        uint32_t RootVisualId = *((uint32_t*)&amp;ReadBuffer[ScreensStartOffset + 32]);

        GlobalIdBase = ResourceIdBase;
        GlobalIdMask = ResourceIdMask;
        GlobalRootWindow = RootWindow;
        GlobalRootVisualId = RootVisualId;

        SetupStatus = 0;
    }

    return SetupStatus;
}

int X_CreatWindow(int Socket, int X, int Y, int Width, int Height) {
    // TODO: Put this into 'connection' struct
    char SendBuffer[16*1024] = {};
    char ReadBuffer[16*1024] = {};

    int32_t WindowId = GetNextId();
    int32_t Depth = 0;
    uint32_t BorderWidth = 1;
    int32_t CreateWindowFlagCount = 2;
    int RequestLength = 8+CreateWindowFlagCount;

    SendBuffer[0] = X11_REQUEST_CREATE_WINDOW;
    SendBuffer[1] = Depth;
    *((int16_t *)&amp;SendBuffer[2]) = RequestLength;
    *((int32_t *)&amp;SendBuffer[4]) = WindowId;
    *((int32_t *)&amp;SendBuffer[8]) = GlobalRootWindow;
    *((int16_t *)&amp;SendBuffer[12]) = X;
    *((int16_t *)&amp;SendBuffer[14]) = Y;
    *((int16_t *)&amp;SendBuffer[16]) = Width;
    *((int16_t *)&amp;SendBuffer[18]) = Height;
    *((int16_t *)&amp;SendBuffer[20]) = BorderWidth;
    *((int16_t *)&amp;SendBuffer[22]) = WINDOWCLASS_INPUTOUTPUT;
    *((int32_t *)&amp;SendBuffer[24]) = GlobalRootVisualId;
    *((int32_t *)&amp;SendBuffer[28]) = X11_FLAG_WIN_EVENT | X11_FLAG_BACKGROUND_PIXEL;
    *((int32_t *)&amp;SendBuffer[32]) = 0xff000000;
    *((int32_t *)&amp;SendBuffer[36]) = X11_EVENT_FLAG_EXPOSURE | X11_EVENT_FLAG_KEY_PRESS;

    int BytesWritten = write(Socket, (char *)&amp;SendBuffer, RequestLength*4);

    return WindowId;
}

int X_MapWindow(int Socket, int WindowId) {
    // TODO: Put this into 'connection' struct
    char SendBuffer[16*1024] = {};
    char ReadBuffer[16*1024] = {};

    SendBuffer[0] = X11_REQUEST_MAP_WINDOW;
    SendBuffer[1] = 0;
    *((int16_t *)&amp;SendBuffer[2]) = 2;
    *((int32_t *)&amp;SendBuffer[4]) = WindowId;

    int BytesWritten = write(Socket, (char *)&amp;SendBuffer, 2*4);
    return 0;
}

void X_OpenFont(int32_t Socket, char *FontName, int32_t FontId) {
    char SendBuffer[16*1024] = {};
    char ReadBuffer[16*1024] = {};
    int BytesWritten = 0;
    int BytesRead = 0;

    int32_t FontNameLength = strlen((char *)FontName);
    int32_t Pad = PAD(FontNameLength);
    int RequestLength = (3 + (FontNameLength + Pad)/4);

    SendBuffer[0] = X11_REQUEST_OPEN_FONT;
    SendBuffer[1] = 0;
    *((uint16_t *)&amp;SendBuffer[2]) = RequestLength;
    *((uint32_t *)&amp;SendBuffer[4]) = FontId;
    *((uint16_t *)&amp;SendBuffer[8]) = FontNameLength;
    strncpy(SendBuffer + 12, (char *)FontName, FontNameLength);

    int32_t WriteSize = 12 + FontNameLength + Pad;
    BytesWritten = write(Socket, (char *)&amp;SendBuffer, WriteSize);
}

void X_CreateGC(int32_t Socket, int32_t GcId, int32_t FontId) {
    char SendBuffer[16*1024] = {};

    int32_t CreateGcFlagCount = 3;
    int RequestLength = 4 + CreateGcFlagCount;

    SendBuffer[0] = X11_REQUEST_CREATE_GC;
    SendBuffer[1] = 0;
    *((int16_t *)&amp;SendBuffer[2]) = RequestLength;
    *((int32_t *)&amp;SendBuffer[4]) = GcId;
    *((int32_t *)&amp;SendBuffer[8]) = GlobalRootWindow;
    *((int32_t *)&amp;SendBuffer[12]) = X11_FLAG_FG | X11_FLAG_BG | X11_FLAG_FONT;
    *((int32_t *)&amp;SendBuffer[16]) = 0xFF00FF00; // Foreground
    *((int32_t *)&amp;SendBuffer[20]) = 0xFF000000; // Background
    *((int32_t *)&amp;SendBuffer[24]) = FontId; // Font

    write(Socket, (char *)&amp;SendBuffer, RequestLength*4);
}

void WriteText(int Socket, int WindowId, int GCid, int16_t X, int16_t Y, const char *Text, int32_t TextLength) {
    char Buffer[16*1024] = {};

    uint32_t ContentLength = 4 + (TextLength + PAD(TextLength))/4;

    Buffer[0] = (uint8_t)X11_REQUEST_IMAGE_TEXT_8;
    Buffer[1] = TextLength;
    *((int16_t *)&amp;Buffer[2]) = ContentLength; 
    *((int32_t *)&amp;Buffer[4]) = WindowId;
    *((int32_t *)&amp;Buffer[8]) = GCid;
    *((int16_t *)&amp;Buffer[12]) = X; 
    *((int16_t *)&amp;Buffer[14]) = Y; 

    strncpy(&amp;Buffer[16], (char *)Text, TextLength);
    int BytesWritten = write(Socket, (char *)&amp;Buffer, ContentLength*4);
}

int main(){
    int Socket = socket(AF_UNIX, SOCK_STREAM, 0);
    VerifyOrDie(Socket &gt; 0, "Couldn't open a socket(...)");

    struct sockaddr_un Address;
    memset(&amp;Address, 0, sizeof(struct sockaddr_un));
    Address.sun_family = AF_UNIX;
    strncpy(Address.sun_path, "/tmp/.X11-unix/X0", sizeof(Address.sun_path)-1);

    int Status = connect(Socket, (struct sockaddr *)&amp;Address, sizeof(Address));
    VerifyOrDieWidthErrno(Status == 0, "Couldn't connect to a unix socket with connect(...)");

    int SetupStatus = X_InitiateConnection(Socket);

    if(SetupStatus == 0) {
        int32_t X = 100;
        int32_t Y = 100;
        uint32_t Width = 600;
        uint32_t Height = 300;
        int WindowId = X_CreatWindow(Socket, X, Y, Width, Height);

        X_MapWindow(Socket, WindowId);

        int32_t FontId = GetNextId();
        X_OpenFont(Socket, (int8_t *)"fixed", FontId);

        int32_t GcId = GetNextId();
        X_CreateGC(Socket, GcId, FontId);

        struct pollfd PollDescriptors[1] = {};
        PollDescriptors[0].fd = Socket;
        PollDescriptors[0].events = POLLIN;
        int32_t DescriptorCount = 1;
        int32_t IsProgramRunning = 1;
        while(IsProgramRunning){
            int32_t EventCount = poll(PollDescriptors, DescriptorCount, -1);

            if(PollDescriptors[0].revents &amp; POLLERR) {
                printf("------- Error\n");
            }

            if(PollDescriptors[0].revents &amp; POLLHUP) {
                printf("---- Connection close\n");
                IsProgramRunning = 0;
            }

            char* t1 = "Hello, World!";
            char* t2 = "This is a test text directly written to X";
            char* t3 = "Whooha. Is this even legal? Let's keep a secret!";
            WriteText(Socket, WindowId, GcId, GlobalTextOffsetX, GlobalTextOffsetY, t1, strlen(t1));
            WriteText(Socket, WindowId, GcId, GlobalTextOffsetX, GlobalTextOffsetY + 15, t2, strlen(t2));
            WriteText(Socket, WindowId, GcId, GlobalTextOffsetX, GlobalTextOffsetY + 30, t3, strlen(t3));

            GetAndProcessReply(PollDescriptors[0].fd);
        }
    }

}
</code></pre><p>Compile it with:
<code>gcc main.c -o main</code></p>
<p>Here is how it looks.</p>

    
<iframe height="400" src="https://www.youtube.com/embed/Bd8kat3Bisc" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen=""></iframe>



<h2 id="conclusion">Conclusion</h2>
<p>X Server is slowly being depricated in the linux world and being replaced
Wayland. Still X11 is an interesting protocol to look at from the perspective of
binary communication and management of resource which require fast speeds.</p>
<p>In this post I tried to cover basic information and create a simple but working
app that is simple, defined in single file and easily compiles. No external code
except libc was used. I find it fascinating when you can open black boxes and
see how gears move each other. Hope you liked it as well.</p>

    </div></div>]]></description>
        </item>
    </channel>
</rss>