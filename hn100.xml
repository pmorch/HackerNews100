<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0">
    <channel>
        <title>HN100 - Readable Contents</title>
        <link>https://hn.algolia.com/api/v1/search_by_date?tags=%28story,poll%29&amp;numericFilters=points%3E100</link>
        <description>Uses Readability to add bodies to the RSS feed</description>
        <lastBuildDate>Wed, 01 Jan 2025 23:30:05 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>en</language>
        <item>
            <title><![CDATA[Terence Tao: One of my papers got declined today (307 pts)]]></title>
            <link>https://mathstodon.xyz/@tao/113721192051328193</link>
            <guid>42568399</guid>
            <pubDate>Wed, 01 Jan 2025 19:12:57 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://mathstodon.xyz/@tao/113721192051328193">https://mathstodon.xyz/@tao/113721192051328193</a>, See on <a href="https://news.ycombinator.com/item?id=42568399">Hacker News</a></p>
&lt;Unparsable&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[Welcome to the Public Domain in 2025 (115 pts)]]></title>
            <link>https://blog.archive.org/2025/01/01/welcome-to-the-public-domain-in-2025/</link>
            <guid>42567280</guid>
            <pubDate>Wed, 01 Jan 2025 16:59:51 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://blog.archive.org/2025/01/01/welcome-to-the-public-domain-in-2025/">https://blog.archive.org/2025/01/01/welcome-to-the-public-domain-in-2025/</a>, See on <a href="https://news.ycombinator.com/item?id=42567280">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
			
<figure><a href="http://blog.archive.org/wp-content/uploads/2025/01/PDDmontageRS_2025.png"><img decoding="async" width="1024" height="613" src="http://blog.archive.org/wp-content/uploads/2025/01/PDDmontageRS_2025-1024x613.png" alt="" srcset="https://blog.archive.org/wp-content/uploads/2025/01/PDDmontageRS_2025-1024x613.png 1024w, https://blog.archive.org/wp-content/uploads/2025/01/PDDmontageRS_2025-300x180.png 300w, https://blog.archive.org/wp-content/uploads/2025/01/PDDmontageRS_2025-768x460.png 768w, https://blog.archive.org/wp-content/uploads/2025/01/PDDmontageRS_2025-1536x919.png 1536w, https://blog.archive.org/wp-content/uploads/2025/01/PDDmontageRS_2025-2048x1226.png 2048w, https://blog.archive.org/wp-content/uploads/2025/01/PDDmontageRS_2025-624x373.png 624w" sizes="(max-width: 1024px) 100vw, 1024px"></a><figcaption>Image credit: Montage of materials moving into the public domain in 2025. Duke Law Center for the Study of the Public Domain.</figcaption></figure>



<h2>Celebrate the public domain with the Internet Archive in the following ways:</h2>



<ul>
<li><strong>Register</strong> for our Public Domain Day celebrations on January 22 – both <a href="https://www.eventbrite.com/e/singin-in-the-public-domain-public-domain-day-2025-tickets-1104135491979">virtual</a> and <a href="https://www.eventbrite.com/e/celebrate-public-domain-day-2025-at-the-internet-archive-tickets-1117297439719">in-person</a>.</li>



<li><strong>Submit</strong> a short film to our Public Domain Film Remix <a href="https://blog.archive.org/2024/12/16/2025-public-domain-day-remix-contest-the-internet-archive-is-looking-for-creative-short-films-made-by-you/">contest</a>.</li>



<li><strong>Explore</strong> the works that have entered the public domain in 2025, below.</li>
</ul>



<p>On January 1, 2025, we celebrate published works from 1929 and published sound recordings from 1924 entering the public domain! The passage of these works into the public domain celebrates our shared cultural heritage. The ability to breathe new life into long forgotten works, remix the most popular and enduring works of the time, and to better circulate the oddities we find in thrift stores, attics, and on random pockets of the internet are now freely available for us all.</p>



<p>While not at the same blockbuster level as 2024 with <a href="https://archive.org/details/steamboat-willie_1928"><em>Steamboat Willie</em>’s</a> passage into the public domain, works from 1929 still inhabit strong cultural significance today. The works of 1929 continue to capture the Lost Generation’s voice, the rise of sound film, and the emerging modern moment of the 1920s.&nbsp;</p>



<p><strong>Musical Compositions</strong></p>



<p>Show tunes and Jazz dominated the year with many standards that we remember today first being published. While best known for the 1952 film of the same name, <a href="https://archive.org/details/singin-in-the-rain-01">Singin’ in the Rain</a> was first published in 1929 and serves as the inspiration for our <a href="https://blog.archive.org/2024/12/16/2025-public-domain-day-remix-contest-the-internet-archive-is-looking-for-creative-short-films-made-by-you/">remix contest</a> this year. George Gershwin also officially published (and copyrighted) his suite <em>An American in Paris</em> following a premiere in late 1928.</p>



<p>Below is sheet music for some popular compositions of the time.</p>



<ul>
<li><a href="https://archive.org/details/singin-in-the-rain-01">Singin’ in the Rain</a></li>



<li><a href="https://archive.org/details/wedding-of-the-painted-doll-1929-sheet-music">Wedding of the Painted Doll</a></li>



<li><a href="https://archive.org/details/you-were-meant-for-me-1929-sheet-music/mode/2up">You Were Meant For Me</a></li>



<li><a href="https://archive.org/details/am-i-blue-1929-sheet-music">Am I Blue?</a></li>



<li><a href="https://archive.org/details/aint-misbehavin-1929-sheet-music">Ain’t Misbehavin’</a></li>



<li><a href="https://archive.org/details/what-is-this-thing-called-love-sheet-music-1929">What Is This Thing Called Love?</a></li>



<li><a href="https://archive.org/details/imslp-104723-pmlp-03667-ravel-bolero-cropped">Bolero</a></li>
</ul>



<p><img decoding="async" loading="lazy" src="https://lh7-rt.googleusercontent.com/docsz/AD_4nXe64S2RBCuwV3QgBs1QyzRvGGlsSxrPGp_hQmGdCWd7JWU2pLFezU3MX1GQD7gEf_xV7IJ7ngpOW1P9WfC7_FkMH2TTYWcClV2ecNkcfpNHk7tubARMMF4uH2JC6NWbTViccXoa5g?key=wO0fMbPUiRPFE3WA74sNCVBS" width="237" height="325"></p>



<p><strong>Literature</strong></p>



<p>Reflections on World War I continued with<a href="https://archive.org/details/dli.ernet.531939/mode/2up"> <em>A Farewell to Arms</em></a> by Ernest Hemingway, the first English translation of <a href="https://archive.org/details/allquietonwester0000unse_f7g8/mode/2up"><em>All Quiet on the Western Front</em></a>, and Richard Aldington’s <a href="https://archive.org/details/deathofheronovel0000rich/mode/2up"><em>Death of a Hero</em></a>. William Faulkner published his modernist novel <a href="https://archive.org/details/soundfury0000will_o8d3/mode/2up"><em>The Sound and the Fury</em></a>. A. A. Milne followed up 1928’s <a href="https://archive.org/details/houseatpoohcorne0000unse_e5y6"><em>The House at Pooh Corner</em></a> by adapting <a href="https://archive.org/details/windinwillows00grah"><em>The Wind in the Willows</em></a><em> </em>into the play <a href="https://archive.org/details/toad-of-toad-hall-1929"><em>Toad of Toad Hall</em></a>. Detective fiction thrived in 1929, with The Maltese Falcon serialized in Black Mask, Agatha Christie captivating readers with <a href="https://archive.org/details/sevendialsmyster0000agat_n1n0/mode/2up"><em>The Seven Dials Mystery</em></a>, and the first Ellery Queen novel, The Roman Hat Mystery, making its debut. Explore our <a href="https://archive.org/details/periodicals?tab=collection&amp;query=date%3A1929&amp;sort=-date">1929 periodicals</a> to find more hidden detective gems.</p>



<p><img decoding="async" loading="lazy" src="https://lh7-rt.googleusercontent.com/docsz/AD_4nXe89LykI0ONLADaL63mMqm9ZA5YpYxMrrvyGRp7a2v3Q25-wKjAZ9QTCwRHYe8KHUTjZTz6oEY5k4_zZGCQOv-1_Pqo8S3q46HWYelXrrpWggSlLLEMWwKZ3Fh4VO_EjGk0jgiJyg?key=wO0fMbPUiRPFE3WA74sNCVBS" width="205" height="301"></p>



<p>While not a towering work of literature, the first set of comic strips featuring Popeye also are joining the public domain. Popeye first made an appearance in <em>Thimble Theatre</em> on <a href="https://archive.org/details/popeyes-first-appearance-in-thimble-theatre-january-17-1929">January 17, 1929</a>. Initially just a side character for an adventure arc featuring gambling and sailing, Popeye rose quickly to fame. By February 4, 1931 the <em>Thimble Theatre</em> would feature a subtitle, <em>Starring Popeye</em>, before being renamed just Popeye later on.</p>



<p>Below is a further selection of works from the year:</p>



<ul>
<li><a href="https://archive.org/details/ropeplayinthreea00hami/mode/2up?view=theater"><em>Rope</em></a><em> </em>by Patrick Hamilton</li>



<li><a href="https://archive.org/details/a-room-of-ones-own_202208/mode/2up?q=A+Room+of+One%5C%27s+Own"><em>A Room of One’s Own</em></a> <em>by Virginia Woolfe</em></li>



<li><a href="https://archive.org/details/highwindinjamaic0000unse_c7o5/page/n3/mode/2up"><em>A High Wind in Jamaica</em></a><em> </em>by Richard Hughes</li>



<li><a href="https://archive.org/details/issexnecessary0000unse_u2j1/page/n5/mode/2up?q=Is+Sex+Necessary%3F"><em>Is Sex Necessary? Or, Why You Feel the Way You Do</em></a><em> </em>by E. B. White and James Thurber</li>



<li><a href="https://archive.org/details/lookhomewardange0000thom_f9h1/page/n5/mode/2up"><em>Look Homeward, Angel</em></a><em> </em>by Thomas Wolfe</li>
</ul>



<p>Dive into Archive’s literary collection to unearth more classics from <a href="https://archive.org/details/internetarchivebooks?tab=collection&amp;query=date%3A1929">1929</a>.</p>



<p><strong>Films</strong></p>



<p><img decoding="async" loading="lazy" src="https://lh7-rt.googleusercontent.com/docsz/AD_4nXfYdvV6W3ELpXhXvm5vfoHb1V5KgBXHBReKMD_7vCtS_agw744iZqjs2DAFwTVSEuL0CsGwCSmN7R315BpjC2Fwv7nCfdsARD3pR2K7xBNsw5U2-dieXdd-InzMQ31ZDKC9LKi4?key=wO0fMbPUiRPFE3WA74sNCVBS" width="298.3833017077799" height="222.2158273381295"></p>



<p>Last year Mickey Mouse made a splash with <a href="https://archive.org/details/steamboat-willie_1928"><em>Steamboat Willie</em></a> cruising into the public domain. This year <a href="https://archive.org/details/@smd5dq/lists/2/mickey-mouse-1929?sort=-date"><strong>TWELVE</strong></a><strong> </strong>more Mickey shorts join to flesh out the notable events of Mickey’s young career. He speaks his first words in <a href="https://archive.org/details/the-karnival-kid-1929"><em>The Karnival Kid</em></a>, he wears gloves for the first time in <a href="https://archive.org/details/the-opry-house-1929_202412"><em>The Opry House</em></a>, and Ub Iwerks leaves the studio at year’s end with <a href="https://archive.org/details/wild-waves-1929"><em>Wild Waves</em></a>. Disney animation also kickstarted their <em>Silly Symphonies</em> series with the haunting tales <a href="https://archive.org/details/the-skeleton-dance-1929"><em>The Skeleton Dance</em></a> and <a href="https://archive.org/details/hells-bells-1929"><em>Hell’s Bells</em></a>.</p>



<p>In <a href="https://archive.org/details/feature_films?tab=collection&amp;query=date%3A1929">1929</a>, if your film wanted to have any attention it needed sound. Musical films were everywhere with <a href="https://archive.org/details/musicals_202210/1929-02-01+-+The+Broadway+Melody+(1929).mp4"><em>The Broadway Melody</em></a><em> </em>winning the second ever Best Picture award at the Oscars, <a href="https://archive.org/details/the-hollywood-revue-1929"><em>The Hollywood Revue</em></a> introducing the world to “Singin’ in the Rain”, and the Marx Brothers making their big screen debut with <a href="https://archive.org/details/the-cocoanuts-1929_202412"><em>The Cocoanuts</em></a>.</p>



<p><img decoding="async" loading="lazy" src="https://lh7-rt.googleusercontent.com/docsz/AD_4nXcP9ZxMrKL61-d8QmGPk5poaAAyLJqPcQQUogbNltvPJTXeChaJ1i3lXYJPegyWniIcFhbhIYGOdGwcSQ_FcWNt5ck5TakVhNlOoMNFKytYS7qFZHeMwjNfz3UKf09MEePlAc5sBg?key=wO0fMbPUiRPFE3WA74sNCVBS" width="170" height="257"></p>



<p>Below is a list of more significant films from the year:</p>



<ul>
<li><a href="https://archive.org/details/all-americans-1929"><em>All Americans</em></a><em> </em>(dir. Joseph Santley)</li>



<li><a href="https://archive.org/details/blackmail_1929"><em>Blackmail</em></a><em> </em>(dir. Alfred Hitchcock)</li>



<li><a href="https://archive.org/details/lambchops-1929"><em>Lambchops</em></a><em> </em>(dir. Murray Roth)</li>



<li><a href="https://archive.org/details/StellasMerits1929ClaraBowFredricMarchMarcelineDayJackOakie"><em>The Wild Party</em></a><em> </em>(dir. Dorothy Arzner)</li>



<li><a href="https://archive.org/details/spite-marriage-1929"><em>Spite Marriage</em></a><em> </em>(dir. Buster Keaton and Edward Sedgwick)</li>



<li><a href="https://archive.org/details/1929-08-06-say-it-with-songs-1929"><em>Say It with Songs</em></a><em> </em>(dir. Lloyd Bacon)</li>



<li><a href="https://ia804708.us.archive.org/16/items/musicals_202210/1929-08-20%20-%20Hallelujah%20%281929%29.mp4"><em>Hallelujah</em></a><em> </em>(dir. King Vidor)</li>



<li><a href="https://archive.org/details/welcome-danger-1929"><em>Welcome Danger</em></a><em> </em>(dir. Clyde Bruckman and Malcolm St. Clair)</li>



<li><a href="https://archive.org/details/rec-20231215161834/The+Black+Watch+720p.mp4"><em>The Black Watch</em></a><em> </em>(dir. John Ford)</li>



<li>The first five <a href="https://archive.org/details/@smd5dq/lists/3/1929-silly-symphonies"><em>Silly Symphonies</em></a> (dir. Walt Disney or Ub Iwerks)</li>
</ul>



<p>Our film remix contest is ongoing until January 17, 2025, so please upload your submissions! Read more <a href="https://blog.archive.org/2024/12/16/2025-public-domain-day-remix-contest-the-internet-archive-is-looking-for-creative-short-films-made-by-you/">here</a>.</p>



<h2>Additional resources</h2>



<ul>
<li><a href="https://archive.us20.list-manage.com/track/click?u=38bd6154386f64fcd92204a25&amp;id=9986cec031&amp;e=caa90e5a55">Learn more</a> about what’s moving into the public domain in 2025 from Jennifer Jenkins and James Boyle of Duke Law’s Center for the Study of the Public Domain.</li>



<li>Check out the <a href="https://www.techdirt.com/2024/12/28/the-public-domain-game-jam-starts-next-week/">Public Domain Game Jam</a> from Techdirt.</li>



<li>Public Domain Review <a href="https://archive.us20.list-manage.com/track/click?u=38bd6154386f64fcd92204a25&amp;id=844175f270&amp;e=caa90e5a55">highlights the materials moving into the public domain</a> in 2025.</li>



<li>Interested in what’s happening with the public domain in Europe? Communia is hosting a <a href="https://archive.us20.list-manage.com/track/click?u=38bd6154386f64fcd92204a25&amp;id=2116ff8c9b&amp;e=caa90e5a55">one-day event</a> on January 9 in Brussels.</li>
</ul>



<p><strong>In honor of Public Domain Day, this post is published with a CC0 Waiver dedicating it to the public domain.</strong></p>
					</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Tesla replaces laid-off U.S. workers with foreigners using visas pushed by Musk (108 pts)]]></title>
            <link>https://www.msn.com/en-ca/autos/electric-cars/tesla-replaces-laid-off-u-s-workers-with-foreigners-using-visas-pushed-by-musk-report/ar-AA1wILAQ</link>
            <guid>42566784</guid>
            <pubDate>Wed, 01 Jan 2025 15:58:26 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.msn.com/en-ca/autos/electric-cars/tesla-replaces-laid-off-u-s-workers-with-foreigners-using-visas-pushed-by-musk-report/ar-AA1wILAQ">https://www.msn.com/en-ca/autos/electric-cars/tesla-replaces-laid-off-u-s-workers-with-foreigners-using-visas-pushed-by-musk-report/ar-AA1wILAQ</a>, See on <a href="https://news.ycombinator.com/item?id=42566784">Hacker News</a></p>
&lt;Unparsable&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[Databases in 2024: A Year in Review (331 pts)]]></title>
            <link>https://www.cs.cmu.edu/~pavlo/blog/2025/01/2024-databases-retrospective.html</link>
            <guid>42566192</guid>
            <pubDate>Wed, 01 Jan 2025 14:27:39 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.cs.cmu.edu/~pavlo/blog/2025/01/2024-databases-retrospective.html">https://www.cs.cmu.edu/~pavlo/blog/2025/01/2024-databases-retrospective.html</a>, See on <a href="https://news.ycombinator.com/item?id=42566192">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
        <p>Like a shot to your <a target="_blank" href="https://youtu.be/pMoBAk-HFIg">dome piece</a>, I'm back to hit you with my annual roundup of what happened in the <a target="_blank" href="https://youtu.be/WkVohPC_YpU?t=37">rumble-tumble</a> game of databases. Yes, I used to write this article on the <a target="_blank" href="https://ottertune.com/">OtterTune</a> blog, but the company is dead (RIP). I'm doing this joint on my professor blog.</p>
<p>There is much to cover from the past year, from 10-figure acquisitions, vendors running wild in the streets with license changes, and the most famous database octogenarian splashing cash to recruit a college quarterback to impress his new dimepiece.</p>
<p>I promised my first wife that I would write more professionally this year. I have also been informed that some universities assign my annual blog articles as required reading in their database courses. Let's see how it goes.</p>
<p>Previous entries:</p>
<ul>
  <li> <a onclick="javascript:pageTracker._trackPageview('/outgoing/www.cs.cmu.edu');" href="https://www.cs.cmu.edu/~pavlo/blog/2024/01/2023-databases-retrospective.html">Databases in 2023: A Year in Review</a></li>
  <li> <a onclick="javascript:pageTracker._trackPageview('/outgoing/www.cs.cmu.edu');" href="https://www.cs.cmu.edu/~pavlo/blog/2022/12/2022-databases-retrospective.html">Databases in 2022: A Year in Review</a></li>
  <li> <a onclick="javascript:pageTracker._trackPageview('/outgoing/www.cs.cmu.edu');" href="https://www.cs.cmu.edu/~pavlo/blog/2021/12/2021-databases-retrospective.html">Databases in 2021: A Year in Review</a></li>
</ul>

<!-- *************** #1: LICENSES *************** -->
<h2 id="licenses">It’s My Database and I’m Going to License it the Way I Want! </h2>
<p>We live in the golden era of databases. There are many excellent (<a target="_blank" href="https://youtu.be/8Woy5I511L8">relational</a>) choices for all types of application domains. Many are open-source despite being built by for-profit companies backed by VC money.</p>
<p>But VCs want that money back and their <a target="_blank" href="https://youtu.be/1Za8BtLgKv8?t=290">trap</a> full, so these companies turn out a hosted service for their DBMSs on the cloud. But the cloud makes open-source DBMSs a tricky business. If a system becomes too popular, then a cloud vendor (like Amazon) slaps it up as a service and makes more money than the company paying for the development of the software. This threat is why many database companies change to more restrictive source code licenses that protect against cloud vendors from reselling their products. MongoDB was among the first to do this <a target="_blank" href="https://techcrunch.com/2018/10/16/mongodb-switches-up-its-open-source-license/">in 2018</a> when they switched to their <a target="_blank" href="https://en.wikipedia.org/wiki/Server_Side_Public_License">Server Side Public License</a> (SSPL).</p>
<p>This past year was a turbulent one for license changes, and the most prominent two were Redis™ and Elasticsearch.</p>
<h4 id="licenses-redis">Redis:</h4>
<p>Redis Ltd. (the company) is on an aggressive path towards its IPO. Originally starting as Redis Labs in 2011, they switched their name to <a target="_blank" href="https://redis.io/blog/becoming-one-redis/">Redis Ltd. in 2021</a> when they acquired the Redis trademark from its creator (<a target="_blank" href="https://github.com/antirez">Salvatore Sanfilippo</a>), who Redis Labs had bankrolled. Over the last few years, Redis Ltd. has attempted to consolidate control over the Redis landscape. The company has also tried to cast off the perception that the system is primarily used as an in-memory cache by adding support for vectors and other data models.</p>
<p>In March 2024, Redis Ltd. announced they were <a target="_blank" href="https://redis.io/blog/redis-adopts-dual-source-available-licensing/">switching</a> from the system's original (very permissive) <a target="_blank" href="https://en.wikipedia.org/wiki/BSD_licenses">BSD-3</a> license to a dual license comprising the proprietary <a target="_blank" href="https://redis.com/legal/rsalv2-agreement/">Redis Source Available License</a> and MongoDB's SSPL. The company announced this change the same day they announced the acquisition of <a target="_blank" href="https://www.speedb.io/">Speedb</a>, an <a target="_blank" href="https://github.com/speedb-io/speedb">open-source fork</a> of RocksDB.</p>
<p>The backlash to the Redis license move was <a target="_blank" href="https://lwn.net/Articles/966631/">quick</a>. The same week the license changed, <a target="_blank" href="https://www.thestack.technology/battle-of-the-redis-forks-begins/">two forks</a> were announced based on the original BSD-3 code line: <a target="_blank" href="https://valkey.io/">Valkey</a> and <a target="_blank" href="https://redict.io/">Redict</a>. Valkey started at Amazon, but engineers at Google and Oracle quickly joined. In only one week, the Valkey project clapped back at Redis Ltd. when it <a target="_blank" href="https://www.linuxfoundation.org/press/linux-foundation-launches-open-source-valkey-community">became part of the Linux Foundation</a>, and several major companies shifted their development efforts to it. Redis Ltd. did not help their perception of being up to no good when they got frisky with their beloved trademark and <a target="_blank" href="https://twitter.com/TomHacohen/status/1861137484249252093">started taking over open-source Redis extensions</a>.</p>
<p>In an obvious homage to when <a target="_blank" href="https://youtu.be/9xqvqybGMHk">Bushwick Bill (RIP), Scarface, and Willie D got back together in 2015</a>, the Redis' creator <a target="_blank" href="https://antirez.com/news/144">announced in December 2024</a> that he is in touch with the Redis Ltd. management and is looking to make a comeback to reunite the Redis community.</p>
<h4 id="licenses-elasticsearch">Elasticsearch:</h4>
<p>Elastic N.V. is the for-profit company backing the development of the leading text-search Elasticsearch DBMS. In 2021, they announced their <a target="_blank" href="https://www.elastic.co/blog/elastic-license-update">switch to a dual-license model</a> of the <a target="_blank" href="https://www.elastic.co/blog/elastic-license-v2">Elastic License</a> and MongoDB's SSPL. Again, this was in response to the rising prominence of Amazon's Elasticsearch offering, even though the service had been out <a target="_blank" href="https://aws.amazon.com/blogs/aws/new-amazon-elasticsearch-service/">since 2015</a>. Amazon didn't take this switch kindly and announced their <a target="_blank" href="https://opensearch.org/">OpenSearch</a> fork.</p>
<p>Three years later, Elastic N.V. announced in August 2024 that they <a target="_blank" href="https://www.elastic.co/blog/elasticsearch-is-open-source-again">reverted their license change</a> and switched to the <a target="_blank" href="https://en.wikipedia.org/wiki/GNU_Affero_General_Public_License">AGPL</a>. Their blog article announcing this change references Kendrick Lamar's songs (e.g., <a target="_blank" href="https://www.youtube.com/watch?v=H58vbez_m4E">Not Like Us</a>). Amazon did not like being called the <a target="_blank" href="https://www.bbc.com/news/articles/c0rgl497k59o">Drake of Databases</a> and announced the following month that they were transferring ownership of the <a target="_blank" href="https://www.linuxfoundation.org/press/linux-foundation-announces-opensearch-software-foundation-to-foster-open-collaboration-in-search-and-analytics">OpenSearch project to the Linux Foundation</a>.</p>
<!-- BEGIN: ANDY'S TAKE -->
<div>
<h3><img decoding="async" loading="lazy" src="https://www.cs.cmu.edu/~pavlo/images/blog/017/pavlo-head-sideview-lomo.png" alt="Andy's Head" width="32" height="41">Andy’s Take:</h3>
<p>This turmoil seems like a lot just over licenses, but remember, there is big money in databases. And this is just two systems! I didn't even discuss Greenplum <a target="_blank" href="https://news.ycombinator.com/item?id=40507691">quietly killing</a> off their open-source repository after nine years and going proprietary. But people did not notice because nobody willingly runs Greenplum anymore. The only DBMS I know of that made the same open-source reversal is <a target="_blank" href="https://github.com/ALTIBASE/altibase/blob/main/README.md">Altibase in 2023</a>.</p>
<p>I'll be blunt: I don't care for Redis. It is slow, it has fake <a target="_blank" href="https://redis.io/docs/latest/develop/interact/transactions/">transactions</a>, and its query syntax is a freakshow. Our experiments at CMU found <a target="_blank" href="https://www.dragonflydb.io/">Dragonfly</a> to have much more impressive performance numbers (even with a single CPU core). In my database course, I use the Redis query language as an example of <a target="_blank" href="https://youtu.be/fZbwD1gzjLk?t=2018">what not to do</a>. Nevertheless, I am sympathetic to Redis Ltd.'s plight of being overrun by Amazon. However, the company is overestimating the barrier of entry to build a simplistic system like Redis; it is much lower than building a full-featured DBMS (e.g., Postgres), so there are several alternatives to the OG Redis. They are not in position of strength where such posturing will be tolerated by the community.</p>
<p>The Elasticsearch saga is the same story as Redis, except they are further along in the plot: the company announces license change, competitors create an open-source fork, and the company reverts to an open-source license to muted fanfare.</p>
<p>Notice that Redis and Elasticsearch are receiving more backlash compared to other systems that made similar moves. There was no major effort to fork off MongoDB, <a target="_blank" href="https://neo4j.com/open-core-and-neo4j/">Neo4j</a>, <a target="_blank" href="https://www.infoq.com/news/2018/12/confluent-license-changes/">Kafka</a>, or <a target="_blank" href="https://web.archive.org/web/20240703021228/https://www.cockroachlabs.com/blog/oss-relicensing-cockroachdb/">CockroachDB</a> when they announced their license changes. CockroachDB even <a target="_blank" href="https://techcrunch.com/2024/08/15/cockroach-labs-shakes-up-its-licensing-to-force-bigger-companies-to-pay/">changed its license again in 2024</a> to make larger enterprises start paying up. It cannot be because the Redis and Elasticsearch install base is so much larger than these other systems, and therefore, there were more people upset by the change since the number of MongoDB and Kafka installations was equally as large when they switched their licenses. In the case of Redis, I can only think that people perceive Redis Ltd. as unfairly profiting off others' work since the company's founders were not the system's original creators. An analysis of Redis' source code repository also shows that a <a target="_blank" href="https://lwn.net/Articles/966631/">sizable percentage of contributions</a> to the DBMS comes from outside the company (e.g., Tencent, Alibaba). This "stolen valor" was the reason for the ire HashiCorp received when they <a target="_blank" href="https://techcrunch.com/2023/09/20/terraform-fork-gets-a-new-name-opentofu-and-joins-linux-foundation/">changed Terraform's license</a> in 2023.</p>
<p>The overarching issue in these license shuffles is the <a target="_blank" href="https://techcrunch.com/2023/12/26/the-eternal-struggle-between-open-source-and-proprietary-software/">long-term viability of open-source independent software vendors</a> (ISVs) in the database market. The cloud vendors are behemoths with infinite money. If an open-source DBMS takes off, they will start hosting it and make more money than the ISV. Or they will add your DBMS's wire protocol as a front-end to an existing DBMS, like when AWS added <a target="_blank" href="https://aws.amazon.com/about-aws/whats-new/2024/03/amazon-timestream-influxdb-available/">InfluxDB v2 protocol support</a> for their Timestream DBMS in March 2024. They can then be the girlfriend of the aforementioned Bushwick Bill and <a target="_blank" href="https://www.youtube.com/watch?v=i3M41aqHyfQ">shoot you in the eye</a>, like when AWS announced that their new Valkey-compatible services are <a target="_blank" href="https://www.lastweekinaws.com/blog/aws-valkey-play-when-a-fork-becomes-a-price-cut/">30% cheaper</a> than their Redis-compatible services.</p>
</div>
<!-- END: ANDY'S TAKE -->

<!-- *************** #2: GANGWAR *************** -->
<h2 id="gangwar">The Databricks vs. Snowflake Gangwar Continues </h2>
<p>There continues to be no love lost between Databricks and Snowflake. This fight is a classic database war that has spilled out into the streets. The two companies’ previous <a target="_blank" href="https://www.cs.cmu.edu/~pavlo/blog/2021/12/2021-databases-retrospective.html#benchmark-violence">feud over query performance</a> has expanded to other areas of data management and become much more expensive.</p>
<p>Databricks took the first shot in March 2024 when they announced they spent $10 million to build their <a target="_blank" href="https://www.databricks.com/blog/introducing-dbrx-new-state-art-open-llm">DBRX open-source LLM</a> with 132 billion parameters. The <a target="_blank" href="https://www.databricks.com/research/mosaic">Mosaic</a> team led the development of the DBRX model, which Databricks <a target="_blank" href="https://techcrunch.com/2023/06/26/databricks-picks-up-mosaicml-an-openai-competitor-for-1-3b/">acquired in 2023 for $1.3 billion</a>. One month later, Snowflake rolled up to the same corner and lit it up using their <a target="_blank" href="https://www.snowflake.com/en/blog/arctic-open-efficient-foundation-language-models-snowflake/">Arctic open-source LLM</a> with 480 billion parameters. Snowflake boasted they only spent <a target="_blank" href="https://www.chaosgenius.io/blog/snowflake-arctic-vs-dbrx/#top-10-key-differences%E2%80%94snowflake-arctic-vs-dbrx">$2 million to train their model</a> while outperforming DBRX for “enterprise” tasks like SQL generation. You can tell that Snowflake cares most about taking a shot at Databricks because their announcement shows other LLMs doing better than them (e.g., Llama3), but they highlight how they are better than DBRX. One AI researcher was confused about why <a target="_blank" href="https://medium.com/@mario.defelipe/my-deception-with-databricks-dbrx-and-snowflake-arctic-enterprise-llms-b4fd4faf752a#c0e4">Snowflake focused so much on DBRX</a> in their analysis and not the other models; this person does not know how much blood these two database rivals have spilled.</p>
<p>While the public LLM battle raged, another front in the war between Databricks and Snowflake opened up behind the scenes over catalogs. For most of the 2010s, Hive’s <a target="_blank" href="https://cwiki.apache.org/confluence/display/hive/hcatalog+usinghcat">HCatalog</a> has been the de facto catalog system on data lakes. <a target="_blank" href="https://iceberg.apache.org/">Iceberg</a> and <a target="_blank" href="https://hudi.apache.org/">Hudi</a> emerged as replacements in the late 2010s from Netflix and Uber, respectively, and both became top-level Apache projects backed by VC-funded startups. These systems provide a meta-data service to track files and support transactional ingestion of new data on an object store (e.g., S3). Databricks has a proprietary catalog service called <a target="_blank" href="https://www.databricks.com/product/unity-catalog">Unity</a>, which works with its <a target="_blank" href="https://delta.io/">DeltaLake</a> platform. Snowflake announced their <a target="_blank" href="https://www.snowflake.com/blog/expanding-the-data-cloud-with-apache-iceberg/">initial integration with Iceberg-backed tables</a> in 2022. They then <a target="_blank" href="https://medium.com/snowflake/an-overview-of-snowflake-apache-iceberg-tables-d5e85864ac99">expanded their support</a> for Iceberg over the next few years. Then, they looked into acquiring <a target="_blank" href="https://www.tabular.io/">Tabular</a>, the main company behind Iceberg, to compete against Databricks with Unity and DeltaLake. The story goes that Snowflake was about to <a target="_blank" href="https://financialpost.com/pmn/business-pmn/inside-the-snowflake-databricks-rivalry-and-why-both-fear-microsoft">close the deal for $600 million</a>. But then Databricks crashed the party and splashed <a target="_blank" href="https://techcrunch.com/2024/08/14/databricks-reportedly-paid-2-billion-in-tabular-acquisition/">$2 billion to acquire Tabular</a>. Databricks announced the acquisition the same day as the Snowflake CEO’s conference keynote address, where he was announcing their new <a target="_blank" href="https://venturebeat.com/data-infrastructure/snowflake-unveils-polaris-a-vendor-neutral-open-catalog-implementation-of-apache-iceberg/">open-source Polaris catalog service</a> in June 2024. Databricks continued to kick Snowflake in the teeth when they announced they were <a target="_blank" href="https://twitter.com/databricks/status/1801293028612837877">open-sourcing their Unity catalog</a> the following week. Straight  <a target="_blank" href="https://www.youtube.com/watch?v=50Tl8E0Vvms">murdergram</a>.</p>
<!-- BEGIN: ANDY'S TAKE -->
<div>
<h3><img decoding="async" loading="lazy" src="https://www.cs.cmu.edu/~pavlo/images/blog/017/pavlo-head-sideview-lomo.png" alt="Andy's Head" width="32" height="41">Andy’s Take:</h3>
<p>What is interesting about this database battle is that it is not just about raw performance numbers. It isn’t like the old Oracle versus Informix shootout of the 1990s, where they were mostly boasting about faster query latencies. It is true that the battle also went beyond just benchmark shots when <a target="_blank" href="https://archive.is/JvvhM">Informix sued Oracle</a> (and later had to <a target="_blank" href="https://www.cnet.com/tech/services-and-software/informix-withdraws-oracle-suit/">withdraw their suit</a>) because Oracle poached some top Informix executives. Later, the world found out that the Informix CEO had cooked the company’s books to <a target="_blank" href="https://www.sfgate.com/business/article/ex-ceo-at-informix-indicted-in-fraud-case-2751792.php">inflate revenue numbers</a> to look better against Oracle and had to do a <a target="_blank" href="https://www.eweek.com/database/ex-informix-ceo-gets-jail/">two-month bid</a> in the federal clink.</p>
<p>Instead, the Snowflake versus Databricks battle has expanded to be about the ecosystem around the database. That is, it is about the infrastructure people use to get their data into a database and then the tools they use on that data. Vectorized execution engines for analytical queries are a  <a target="_blank" href="https://db.cs.cmu.edu/seminar2024/">commodity</a> now. Databricks and every other OLAP vendor follow Snowflake’s architecture design from 2013, originally based on one of the Snowflake co-founders’ <a target="_blank" href="https://www.youtube.com/watch?v=moQY_eiHCTs">Ph.D. thesis</a>. What matters now are quality-of-life facets (which are hard to monetize and compare with competitors), compatibility with other tools, and AI/LLM magic.</p>
<p>At least the competition between Snowflake and Databricks has an upside for consumers. Such ferocity means better products and technology for data (e.g., Snowflake’s Polaris is now an  <a target="_blank" href="https://polaris.apache.org/">Apache project</a>) and eventually (hopefully) lower prices. It’s not like the previous pissing match between Oracle and SalesForce CEOs, where it was two rich guys taking pop shots at each other during their <a target="_blank" href="https://phys.org/news/2011-10-ellison-benioff-billionaire.html">expensive conferences</a>.</p>
</div>
<!-- END: ANDY'S TAKE -->

<!-- *************** #3: DUCKDB *************** -->
<h2 id="duckdb">Shoving Ducks Into Everything </h2>
<p>In the same way that Postgres is the default choice for anyone starting a new operational database, DuckDB has entered the zeitgeist as the default choice for someone wanting to run analytical queries on their data. Pandas previously held DuckDB's crowned position. Given DuckDB's insane portability, there are several efforts to stick it inside existing DBMSs that do not have great support for OLAP workloads. This year, we saw the release of four different extensions to stick DuckDB up inside Postgres.</p>
<p>The first announcement came in May 2024 when <a target="_blank" href="https://www.crunchydata.com/">Crunchy Data</a> revealed their <a target="_blank" href="https://www.crunchydata.com/blog/how-we-fused-duckdb-into-postgres-with-crunchy-bridge-for-analytics">proprietary bridge</a> for rewiring Postgres to route OLAP queries to DuckDB. They later announced an expanded version of their extension to leverage DuckDB's <a target="_blank" href="https://duckdb.org/docs/extensions/spatial/overview.html">geospatial capabilities</a> to <a target="_blank" href="https://www.crunchydata.com/blog/postgis-meets-duckdb-crunchy-bridge-for-analytics-goes-spatial">accelerate PostGIS queries</a>.</p>
<p>In June 2024, ParadeDB <a target="_blank" href="https://www.linkedin.com/posts/philippemnoel_im-incredibly-excited-to-announce-duckdb-activity-7212107481123020800-UUg6/">announced</a> their open-source extension (<a target="_blank" href="https://github.com/paradedb/pg_analytics">pg_analytics</a>) that uses Postgres' foreign data wrapper API to call into DuckDB; they previously were using DataFusion in an earlier version (<a target="_blank" href="https://github.com/paradedb/paradedb/tree/dev/pg_lakehouse">pg_lakehouse</a>) but switched to the Duck.</p>
<p>Then, in August 2024, the next DuckDB-for-Postgres extension (<a target="_blank" href="https://github.com/duckdb/pg_duckdb">pg_duck</a>) came out. The source code for this extension is hosted under the <a target="_blank" href="https://duckdblabs.com/">DuckDB Labs</a> GitHub organization. As such, this is the officially sanctioned DuckDB extension for Postgres. The original <a target="_blank" href="https://www.theregister.com/2024/08/20/postgresql_duckdb_extension/">announcement</a> touted this project as being a collaboration between <a target="_blank" href="https://motherduck.com/">MotherDuck</a>, <a target="_blank" href="https://www.hydra.so/">Hydra</a>, Microsoft, and <a target="_blank" href="https://neon.tech/">Neon</a>. The latter two were (allegedly) kicked out of the mix over a dispute on development controls, similar to <a target="_blank" href="https://youtu.be/ECAfnZIN1-A">Arabian Prince</a> leaving NWA. The repository now only lists it as a joint effort between MotherDuck and Hydra.</p>
<p>The latest DuckDB extension <a target="_blank" href="https://mooncake.dev/blog/how-we-built-pgmooncake">dropped</a> in November 2024 with <a target="_blank" href="https://github.com/Mooncake-Labs/pg_mooncake">pg_mooncake</a>. Mooncake differs from the other three because it supports writing data through Postgres into Iceberg tables with full transaction support.</p>
<!-- BEGIN: ANDY'S TAKE -->
<div>
<h3><img decoding="async" loading="lazy" src="https://www.cs.cmu.edu/~pavlo/images/blog/017/pavlo-head-sideview-lomo.png" alt="Andy's Head" width="32" height="41">Andy’s Take:</h3>
<p>Most OLAP queries do not access that much data. Fivetran analyzed traces from Snowflake and Redshift and showed that the <a target="_blank" href="https://www.fivetran.com/blog/how-do-people-use-snowflake-and-redshift">median amount of data scanned by queries is only 100 MB</a>. Such a small amount of data means a single DuckDB instance is enough for most to handle most queries.</p>
<p>DuckDB's convenience and portability are the reasons for its proliferation in the Postgres community. Although ClickHouse has existed since 2016, it was not as easy to run as DuckDB until recently (see this <a target="_blank" href="https://clickhouse.com/blog/clickhouse-over-the-years-with-benchmarks">blog article</a> that discusses the steps to deploy ClickHouse in 2018). These DuckDB extensions are a single entry point to the broader data ecosystem. Users no longer need to install separate extensions to access data in Iceberg and separately for S3. DuckDB can handle all of that for you. It allows organizations to gain high-performance analytics without needing an expensive data warehouse.</p>
<p>Postgres' support for extensions and plugins is impressive. One of the <a target="_blank" href="https://dsf.berkeley.edu/papers/ERL-M85-95.pdf">original design goals</a> of Postgres from the 1980s was to be extensible. The intention was to easily support new access methods and new data types and operations on those data types (i.e., object-relational). Since 2006, Postgres' "hook" API. Our <a target="_blank" href="http://reports-archive.adm.cs.cmu.edu/anon/2023/abstracts/23-144.html">research</a> shows that Postgres has the most expansive and diverse extension ecosystem compared to every other DBMS. We also found that the DBMS's lack of guard rails means that extensions can <a target="_blank" href="https://www.youtube.com/watch?v=U7v0fubktoY">interfere with each other and cause incorrect behavior</a>.</p>
<p>Earlier projects that added columnar storage to Postgres (e.g., Citus, Timescale) only solved part of the problem. Columnar data formats improve retrieving data from storage. However, a DBMS cannot fully exploit those formats if it still uses a <a target="_blank" href="https://www.youtube.com/watch?v=tsbbwiWw9VE&amp;list=PLSE8ODhjZXjYa_zX-KeMJui7pcN1rIaIJ&amp;index=5">row-oriented query processing model</a> (e.g., Postgres). Using DuckDB provides both columnar storage and vectorized query processing.</p>
<p>There is likely a <a target="_blank" href="https://en.wikipedia.org/wiki/Turducken">turducken</a> joke here involving an elephant, but I will not make it because I do not want to get <a target="_blank" href="https://www.cmu.edu/policies/faculty/appointment-and-tenure-policy.html#dismissal">fired</a> or put on probation by the university (again).</p>
</div>
<!-- END: ANDY'S TAKE -->

<!-- *************** #4: DUCKDB *************** -->
<h2 id="random">Random Happenings </h2>
<p>Many one-off events happened with databases last year that you might have overlooked. Here is a quick summary of them:</p>
<h4>Releases:</h4>
<ul>
    <li> <a target="_blank" href="https://aws.amazon.com/blogs/database/introducing-amazon-aurora-dsql/">Amazon Aurora DSQL</a>
    <p>There isn't much public information yet about how AWS implemented their new "Spanner-like" DBMS (see <a target="_blank" href="https://brooker.co.za/blog/2024/12/03/aurora-dsql">Mark Brooker's discussion</a> about the DBMS architecture). The key ideas are a distributed log service (rumors were it was going to be based on now-defunct QLDB) and timestamp ordering via <a target="_blank" href="https://aws.amazon.com/blogs/compute/its-about-time-microsecond-accurate-clocks-on-amazon-ec2-instances/">Time Sync</a>. But this announcement shows you how much brand recognition the name "Aurora" carries in the database world because AWS used it for this new DBMS that seemingly shares no code with their flagship Aurora Postgres RDS offering.
    </p></li>

    <li> <a target="_blank" href="https://cedardb.com/blog/ode_to_postgres/">CedarDB</a>
    <p><a target="_blank" href="https://umbra-db.com/">Umbra</a> is one of the most state-of-the-art DBMSs written by the world's <a onclick="javascript:pageTracker._trackPageview('/outgoing/twitter.com');" href="https://twitter.com/andy_pavlo/status/1221464821717258242">greatest database systems researcher</a> (<a onclick="javascript:pageTracker._trackPageview('/outgoing/en.wikipedia.org');" href="https://en.wikipedia.org/wiki/Thomas_Neumann">Thomas Neumann</a>). But Thomas is content with staying at his university to work on Umbra, remaining comfortably on top of the <a onclick="javascript:pageTracker._trackPageview('/outgoing/benchmark.clickhouse.com');" href="https://benchmark.clickhouse.com/">Clickbench</a> leaderboard, and not worrying about pesky customers. That's why his top Ph.D. students forked his code and are commercializing it as CedarDB.
    </p></li>

    <li> <a target="_blank" href="https://cloud.google.com/blog/products/databases/announcing-sql-support-for-bigtable">Google Bigtable</a>
    <p>The only interesting part of this announcement is that the former vanguard of the NoSQL movement in the late 2000s now supports SQL in 2024.
    </p></li>

    <li> <a target="_blank" href="https://turso.tech/blog/introducing-limbo-a-complete-rewrite-of-sqlite-in-rust">Limbo</a>
    <p>Turso has been working on the  <a target="_blank" href="https://libsql.org/">libSQL</a> fork of SQLite for a while, but they went all out in 2024 by announcing a complete rewrite of SQLite in Rust. In their announcement, they correctly point out that the value of SQLite is not just from its code, but also from the <a target="_blank" href="https://sqlite.org/th3.html">insane test engineering</a> that ensures it runs correctly everywhere. That is why the Limbo developers are working with a <a target="_blank" href="https://antithesis.com/">deterministic testing startup</a> by ex-FoundationDB people. See FoundationDB's <a target="_blank" href="https://www.youtube.com/watch?v=OJb8A6h9jQQ&amp;list=PLSE8ODhjZXjagqlf1NxuBQwaMkrHXi-iz&amp;index=22">2020 CMU-DB talk</a> for more information on what this testing means.
    </p></li>

    <li> <a target="_blank" href="https://www.microsoft.com/en-us/research/blog/introducing-garnet-an-open-source-next-generation-faster-cache-store-for-accelerating-applications-and-services/">Microsoft Garnet</a>
    <p>This key-value store is the successor to the impressive <a target="_blank" href="https://microsoft.github.io/FASTER/">FASTER</a> system from Microsoft Research. It is compatible with Redis and supports inter-query parallelism, larger-than-memory databases, and real transactions. Redis should <u>not</u> be anybody's first choice these days.
    </p></li>

    <li> <a target="_blank" href="https://dev.mysql.com/doc/relnotes/mysql/9.0/en/news-9-0-0.html">MySQL v9</a>
    <p>Six years after MySQL v8 went GA, the team turned v9 out on the streets. But people quickly found that it crashed if your database had <a target="_blank" href="https://perconadev.atlassian.net/browse/PS-9306">more than 8000 tables</a>. I am underwhelmed with the <a target="_blank" href="https://dev.mysql.com/doc/refman/9.0/en/mysql-nutshell.html">feature list</a> in this new major version. Oracle is putting all its time and energy into its proprietary <a target="_blank" href="https://www.oracle.com/mysql/">MySQL Heatwave</a> service. MySQL is still widely used, but the excitement is not there anymore. Everyone has moved on to Postgres.
    </p></li>

    <li> <a target="_blank" href="https://prometheus.io/blog/2024/11/14/prometheus-3-0/">Prometheus v3</a>
    <p>It has been seven years since the latest major version of Prometheus. There are so many <a target="_blank" href="https://dbdb.io/browse?compatible=prometheus">compatible alternatives</a> now that the OG Prometheus may not be the best option for some organizations.
    </p></li>
</ul>

<h4>Acquisitions:</h4>
<ul>
    <li> <a target="_blank" href="https://www.marketscreener.com/quote/stock/ALTERYX-INC-34336524/news/Clearlake-Capital-Group-and-Insight-Partners-Complete-Acquisition-of-Alteryx-46231296/">Alteryx → Private Equity</a>
    <p>I've never met anybody who uses Alteryx, and I don't have an opinion about them.
    </p></li>

    <li> <a target="_blank" href="https://www.prnewswire.com/news-releases/k1-acquires-mariadb-a-leading-database-software-company-and-appoints-new-ceo-302243508.html">MariaDB → Private Equity</a>
    <p>Hopefully the PE people buying the MariaDB Corporation can clean up the mess. See my analysis from last year about the <a target="_blank" href="https://www.cs.cmu.edu/~pavlo/blog/2024/01/2023-databases-retrospective.html#mariadb">MariaDB dumpster fire</a>.
    </p></li>

    <li> <a target="_blank" href="https://supabase.com/blog/supabase-aquires-oriole">OrioleDB → Supabase</a>
    <p>This purchase makes sense if you are one of the leading Postgres ISVs. Postgres has a great front-end but an <a target="_blank" href="https://www.cs.cmu.edu/~pavlo/blog/2023/04/the-part-of-postgresql-we-hate-the-most.html">outdated storage architecture</a>. OrioleDB fixes that problem.
    </p></li>

    <li> <a target="_blank" href="https://www.techtimes.com/articles/306944/20240731/database-startup-clickhouse-announces-peerdb-acquistion.htm">PeerDB → ClickHouse</a>
    <p>Better ETL tooling to get data out of Postgres and into ClickHouse. This is a smart move by ClickHouse, Inc.</p></li>

    <li> <a target="_blank" href="https://www.timescale.com/blog/best-postgresql-gui-popsql-joins-timescale/">PopSQL → Timescale</a>
    <p>They bought themselves a fancy SQL editor UI. It is a quality-of-life improvement.</p></li>

    <li> <a target="_blank" href="https://techcrunch.com/2024/03/21/redis-switches-licenses-acquires-speedb-to-go-beyond-its-core-in-memory-database/">Speedb → Redis Ltd.</a>
    <p>See the <a href="#licenses-redis">discussion</a> above. They are likely going to use Speedb to allow Redis to spill data to disk. Speedb's developers never explained what changes and improvements they made in their RocksDB fork (or I could not find it?). See Mark Callaghan's recent comparison of <a target="_blank" href="http://smalldatum.blogspot.com/2024/12/speedb-vs-rocksdb-on-large-server.html">Speedb vs. RocksDB</a>.
    </p></li>

    <li> <a target="_blank" href="https://www.reuters.com/markets/deals/openai-acquires-database-analytics-firm-rockset-2024-06-21/">Rockset → OpenAI</a>
    <p>This is big news for the company, but unfortunately they had to shutdown the DBaaS in September 2024. Rockset had a great engineering team with some of the best database engineers from Facebook. I just never liked how their DBMS stored three copies of your data in its indexes.
    </p></li>

    <li> <a target="_blank" href="https://www.cnbc.com/2024/06/04/databricks-is-buying-data-optimization-startup-tabular.html">Tabular → Databricks</a>
    <p>Again, see the <a href="#gangwar">discussion above</a>. Iceberg is the standard (sorry Hudi); even <a target="_blank" href="https://aws.amazon.com/about-aws/whats-new/2024/12/amazon-s3-tables-apache-iceberg-tables-analytics-workloads/">Amazon S3 now supports it</a>. It remains to be seen how the adoption of Polaris will evolve and whether they will be able to maintain compatibility in the long term.
    </p></li>

    <li> <a target="_blank" href="https://www.verta.ai/blog/cloudera-acquires-verta-operational-ai-platform-to-bring-trusted-hybrid-ai-to-the-enterprise">Verta.ai → Cloudera</a>
    <p>I guess Cloudera is still alive?
    </p></li>

    <li> <a target="_blank" href="https://www.warpstream.com/blog/warpstream-is-dead-long-live-warpstream">Warpstream → Confluent</a>
    <p>Rewriting Kafka in golang but then making it spill to S3. I'm happy for the Warpstream team, but Confluent could have done this themselves.
    </p></li>
</ul>

<h4>Funding:</h4>
<ul>
    <li><b>Databricks</b> - <a target="_blank" href="https://www.databricks.com/company/newsroom/press-releases/databricks-raising-10b-series-j-investment-62b-valuation">$10 billion Series J</a></li>
    <li><b>LanceDB</b> - <a target="_blank" href="https://siliconangle.com/2024/05/15/lancedb-raises-8m-speed-ai-models-open-source-vector-database/">$8 million Seed Round</a></li>
    <li><b>SDF</b> - <a target="_blank" href="https://www.geekwire.com/2024/data-warehousing-startup-sdf-led-by-microsoft-and-meta-vets-comes-out-of-stealth-mode/">$9 million Seed Round</a></li>
    <li><b>SpiceDB</b> - <a target="_blank" href="https://authzed.com/blog/series-a-funding">$12 million Series A</a></li>
    <li><b>TigerBeetle</b> - <a target="_blank" href="https://tigerbeetle.com/blog/2024-07-23-rediscovering-transaction-processing-from-history-and-first-principles">$24 million Series A</a></li>
</ul>

<p>There are a few more raises from <a target="_blank" href="https://cedardb.com/">CedarDB</a>, <a target="_blank" href="https://spiraldb.com/">SpiralDB</a>, and others but those amounts are not public yet.</p>
<h4>Deaths:</h4>
<ul>
    <li> <a target="_blank" href="https://www.infoq.com/news/2024/07/aws-kill-qldb/">Amazon QLDB</a>
    <p>If Amazon can't figure out how to make money on a blockchain database, then nobody can. And yes, I know QLDB is not a true P2P blockchain, but it's close enough.
    </p></li>

    <li> <a target="_blank" href="https://ottertune.com/">OtterTune</a>
    <p><a target="_blank" href="https://www.linkedin.com/in/dana-van-aken/">Dana</a>, <a target="_blank" href="https://www.linkedin.com/in/bohan-zhang-52b17714b">Bohan</a>, and I worked on this research project and startup for almost a decade. And now it is dead. I am disappointed at how a particular company treated us at the end, so they are forever banned from recruiting CMU-DB students. They know who they are and what they did.
    </p></li>
</ul>

<p>I want to also give special props to <a target="_blank" href="https://www.linkedin.com/in/andres-freund">Andres Freund</a> for discovering the <a target="_blank" href="https://arstechnica.com/security/2024/04/what-we-know-about-the-xz-utils-backdoor-that-almost-infected-the-world/">xz backdoor</a> in 2024 while working on Postgres at Microsoft. This attack was a <a target="_blank" href="https://twitter.com/thegrugq/status/1774392858101039419">two-year campaign</a> to inject malicious code into an important compression library widely used in computing. Although the backdoor targeted SSH and not Postgres directly, it is another example of why database engineers are some of the best programmers in the world.</p>
<!-- BEGIN: ANDY'S TAKE -->
<div>
<h3><img decoding="async" loading="lazy" src="https://www.cs.cmu.edu/~pavlo/images/blog/017/pavlo-head-sideview-lomo.png" alt="Andy's Head" width="32" height="41">Andy’s Take:</h3>
<p>Databricks has blown away all other fundraising in the world of databases for the second year in a row with a disgustingly brash <a target="_blank" href="https://www.prnewswire.com/news-releases/databricks-is-raising-10b-series-j-investment-at-62b-valuation-302333822.html">$10 billion Series J round</a>. This is after their <a target="_blank" href="https://www.databricks.com/company/newsroom/press-releases/databricks-raises-series-i-investment-43b-valuation">$500 million Series I</a> in 2023 and <a target="_blank" href="https://techcrunch.com/2021/08/31/databricks-raises-1-6b-at-38b-valuation-as-it-blasts-past-600m-arr/">$1.6 billion Series H</a> in 2021. What is different about this time is this funding was for <a target="_blank" href="https://sherwood.news/business/databricks-employees-are-cashing-in-on-its-series-j/">buying stock from employees</a> who were getting impatient about Databricks' inevitable IPO. CMU-DB has several alumni at Databricks, including a <a target="_blank" href="https://www.linkedin.com/in/prasmenon/">former #1 ranked Ph.D. student</a>. I know some of them are anxiously awaiting the Databricks IPO before deciding what to do next.</p>
<p>The upcoming year is going to be the <a target="_blank" href="https://youtu.be/UgpzENzpXUs">test of strength</a> for many database startups. Nobody wants to be the next <a target="_blank" href="https://share.chartiq.com/M53BOUC094.png">MariaDB Corporation</a>, and thus several are waiting to ride Databricks' wake before going IPO themselves. Declining interest rates in the upcoming year <a target="_blank" href="https://www.forbes.com/sites/donbutler/2024/10/09/interest-rates-and-the-search-for-liquidity-in-venture-capital/">may open up additional funding</a> for several database companies that have raised large amounts more than two years ago (e.g., CockroachDB, Starburst, Imply, DataStax, SingleStore, Firebolt). The one standout from this crowd is dbtLabs, which I have heard is comfortably crushing it.</p>
<p>See also the Database of Databases list of <a target="_blank" href="https://dbdb.io/browse?start-year=2024">new DBMSs released in 2024</a>.</p>
</div>
<!-- END: ANDY'S TAKE -->

<!-- *************** #5: LE *************** -->
<h2 id="larry">Can’t Stop, Won’t Stop </h2>
<!-- I only know two people over the age of 80: Mike Stonebraker and my father. Stonebraker is an intellectual powerhouse and is still crushing it. My father regrettably told me at Christmas last year that <a target="_blank" href="https://en.wikipedia.org/wiki/Anthony_Fauci">Anthony Fauci</a> should be in jail. -->

<p>Do you know who had their 80th birthday this year? The legendary Larry Ellison! Once again, we see that he is a man who refuses to settle down or <a target="_blank" href="https://youtu.be/hMX7bPGRw_E">be put into a box</a>. First, Larry propelled himself up Forbes' Billionaires List to become the <a target="_blank" href="https://www.forbes.com/sites/dereksaul/2024/09/10/larry-ellison-becomes-richer-than-zuckerberg-arnault-as-oracle-stock-rallies-to-record-high/">third richest person in the world</a>. In March 2024, the Oracle stock rose so much that he made <a target="_blank" href="https://www.cnbc.com/2024/03/12/larry-ellison-makes-15-billion-from-oracle-best-day-since-2021.html">$15 billion in a single day</a>. Flush with cash, Larry went shopping in July 2024 and signed a deal to purchase <a target="_blank" href="https://www.hollywoodreporter.com/business/business-news/paramount-larry-ellison-david-ellison-1236006769/">Paramount Studios at $6 billion</a> for his only son (third wife). He then decided to relax by buying a <a target="_blank" href="https://www.palmbeachdailynews.com/story/business/real-estate/2024/08/08/billionaire-ellison-buys-eau-palm-beach-resort-spa-near-palm-beach/74723944007/">Palm Beach resort for only $277 million</a>. These moves happened in just one year, and databases paid for all of it. But these are mere trifles compared to Larry's most significant accomplishment in 2024.</p>
<p>Everyone I know was surprised when our Larry Ellison news alerts woke us up in the middle of the night in November 2024. The headlines were touting how Larry helped the University of Michigan football program <a target="_blank" href="http://archive.today/2024.11.24-013436/https://frontofficesports.com/larry-ellison-michigan-nil-bryce-underwood/">recruit the premiere college quarterback</a>. The university had previously announced that this player was transferring from Louisiana State to Michigan. Their press release included a curious acknowledgement to "Larry and his wife Jolin" for helping with the recruiting effort. Reporters soon <a target="_blank" href="https://www.marketwatch.com/story/billionaire-larry-ellison-helped-give-a-high-school-student-10-million-to-play-football-for-michigan-and-gave-us-a-glimpse-behind-the-nil-curtain-6bf5d87f">confirmed</a> that this "Larry" was the one and only Larry Ellison! Larry contributed $12 million to the booster campaign to bankroll the best quarterback's move to Michigan.</p>
<p>The bigger mystery in this story was the identity of this "Jolin" person. Investigators found older photos of Larry watching a tennis match with a <a target="_blank" href="https://mgoblog.com/mgoboard/tennis-fans-who%27s-woman-michigan-hat-next-larry-ellison">woman wearing a Michigan hat</a>. Then, two weeks later, a major news organization broke the story at 5:30 am (my alerts woke me up again) that the <a target="_blank" href="http://archive.today/2024.12.07-023939/https://www.wsj.com/sports/football/michigan-recruiting-larry-ellison-bryce-underwood-842d2c9a">woman's identity was Jolin (Keren) Zhu</a> and they confirmed that she was Larry's new wife.</p>
<!-- BEGIN: ANDY'S TAKE -->
<div>
<h3><img decoding="async" loading="lazy" src="https://www.cs.cmu.edu/~pavlo/images/blog/017/pavlo-head-sideview-lomo.png" alt="Andy's Head" width="32" height="41">Andy’s Take:</h3>
<p>I am beaming with pride over what Larry accomplished in the past year. He famously did not graduate from any university and had no prior connection to the University of Michigan. And yet, because the love of his life went to Michigan about a decade ago, Larry made magic happen by writing a check for a measly $12 million (about 0.0055% of his net worth). I told Larry this especially means a lot to me because my <a target="_blank" href="https://web.eecs.umich.edu/~linmacse/">former #1 ranked Ph.D. student</a> is now a professor in Michigan's Computer Science department with their famous <a target="_blank" href="https://dbgroup.eecs.umich.edu/">Database Group</a>.</p>
<p>What is even more fantastic about this story is that Larry is again in love! Too many people struggle in today's world to find that special somebody. Dating apps are a mess, speed dating events are awkward, and it is now considered uncouth to hang around a playground to meet single parents when you do not have kids of your own. Then, just when you think you finally found the right person, it all falls apart when you learn they do not wash their socks regularly or like to put hot sauce on cold cereal. That is why everyone was telling me that Larry would never get married again after his <a target="_blank" href="https://web.archive.org/web/20101102010955/http://tech.fortune.cnn.com/tag/melanie-craft/">2010 divorce</a> from romance novelist Melanie Craft (fourth wife). Those people were telling me the same thing after his <a target="_blank" href="https://marketrealist.com/p/larry-ellison-girlfriend/">2020 divorce</a> from Nikita Kahn (fifth wife). But I knew better, and Larry proved me right with his surreptitious marriage to Keren Zhu (sixth wife)!</p>
</div>
<!-- END: ANDY'S TAKE -->

<!-- *************** CONCLUSION *************** -->
<h2>Conclusion</h2>

<p>I was planning on starting this article boasting how this is the first time in three years where I was celebrating NYE not sick. But then my biological daughter gave me COVID so I'm laid up with that. I got boosted back in September and they gave me Paxlovid, so I'll survive this.</p>
<p>I am disappointed that OtterTune is dead. But I learned a lot and got to work with many brilliant people. I am a big fan of <a target="_blank" href="https://www.intelcapital.com/">Intel Capital</a> and <a target="_blank" href="https://race.capital/">Race Capital</a> for sticking with us to the end. I hope to announce our next start-up soon (hint: it’s about databases).</p>
<p>In the meantime, I am happy to be back full-time at Carnegie Mellon University. <a target="_blank" href="https://jigneshpatel.org/">Jignesh Patel</a> and I have some baller research projects that we hope to turn out this upcoming year. I am also looking forward to teaching a new course on <a target="_blank" href="https://15799.courses.cs.cmu.edu/spring2025/">query optimization</a> this semester. I need to figure out to juice my stats because in September 2024, Wikipedia <a target="_blank" href="https://en.wikipedia.org/wiki/Wikipedia:Articles_for_deletion/Andy_Pavlo">removed the article about me</a> over not having enough citations.</p>
<p>We are staying true to <a target="_blank" href="https://youtu.be/APqWIjtzNGE?t=4941">DJ Mooshoo</a> while he is locked up in Cook County. We hope to free him in 2025.</p>
<p>Lastly, I want to give a shout-out to ByteBase for their article <a target="_blank" href="https://www.bytebase.com/blog/database-tool-review-2024/">Database Tools in 2024: A Year in Review</a>. In previous years, they emailed me asking for permission to translate my <a target="_blank" href="https://www.bytebase.com/blog/database-review-2021-andy-pavlo-chinese/">end-of-year database articles into Chinese</a> for their blog. This year, they could not wait for me to finish writing this one, so they jocked my flow and wrote their own off-brand article with the same title and premise.</p>
    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Ruby 3.4 Highlights (152 pts)]]></title>
            <link>https://blog.sinjakli.co.uk/2025/01/01/ruby-3-4-highlights/</link>
            <guid>42566141</guid>
            <pubDate>Wed, 01 Jan 2025 14:18:27 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://blog.sinjakli.co.uk/2025/01/01/ruby-3-4-highlights/">https://blog.sinjakli.co.uk/2025/01/01/ruby-3-4-highlights/</a>, See on <a href="https://news.ycombinator.com/item?id=42566141">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>

  

  <article>
    <p>Alright, you know the drill. The Ruby team do the hard work of putting together a new version packed with features and I pick my favourites and write about them.</p>

<p>If you’d like to read the full set of changes, they’re <a href="https://www.ruby-lang.org/en/news/2024/12/25/ruby-3-4-0-released/">over on the Ruby project website</a>. There’s plenty more good stuff in there—these were just the changes that stood out to me.</p>

<h3 id="default-block-parameter-name">Default block parameter name</h3>

<p>In many situations where we pass a block in Ruby, that block receives a single argument (<code>each</code>, <code>map</code>, <code>filter</code>, etc). A lot of the time, we end up giving that argument a fairly generic name like <code>item</code> because we know that we’re working through items in a list.</p>

<div><pre><code><span>[</span>
  <span>"beige chinos"</span><span>,</span>
  <span>"blue jorts"</span><span>,</span>
  <span>"rainbow jorts"</span><span>,</span>
<span>].</span><span>filter</span> <span>{</span> <span>|</span><span>item</span><span>|</span> <span>item</span> <span>=~</span> <span>/jorts/</span> <span>}</span>
<span># =&gt; ["blue jorts", "rainbow jorts"]</span>
</code></pre></div>

<p>In Ruby&nbsp;3.4, <code>it</code> has been <a href="https://bugs.ruby-lang.org/issues/18980">added as a default name</a> for the first parameter passed to a block. Rather than specifying a name in trivial cases like the one above, you can now write:</p>

<div><pre><code><span>[</span>
  <span>"beige chinos"</span><span>,</span>
  <span>"blue jorts"</span><span>,</span>
  <span>"rainbow jorts"</span><span>,</span>
<span>].</span><span>filter</span> <span>{</span> <span>it</span> <span>=~</span> <span>/jorts/</span> <span>}</span>
<span># =&gt; ["blue jorts", "rainbow jorts"]</span>
</code></pre></div>

<h3 id="better-connection-handling-happy-eyeballs-version-2">Better connection handling: “Happy Eyeballs Version 2”</h3>

<p>I’ll be honest: the reason this made my list this year is that seeing it in the changelog pointed me to an interesting RFC that I hadn’t heard of before! I spent a couple of hours down a rabbit hole reading RFCs and a few snippets of source code and I thought it was worth talking about.</p>

<p>Ruby&nbsp;3.4 adds an implementation of <a href="https://datatracker.ietf.org/doc/html/rfc8305">RFC8305</a> (<em>Happy Eyeballs Version 2: Better Connectivity Using Concurrency</em>) to its TCP socket implementation.</p>

<p>In a nutshell, the RFC is an attempt at describing a way for clients to handle multiple IP addresses being returned by DNS queries, particularly in the context of a dual-stack (IPv4 and IPV6) network. It describes how to make the DNS requests<sup id="fnref:a-vs-aaaa" role="doc-noteref"><a href="#fn:a-vs-aaaa" rel="footnote">1</a></sup>, how to order the addresses returned by them, and how to kick off concurrent connection attempts to those addresses in a way that balances giving the user a response quickly with not creating pointless load on servers<sup id="fnref:avoiding-overload" role="doc-noteref"><a href="#fn:avoiding-overload" rel="footnote">2</a></sup>.</p>

<p>RFC8305 builds on the earlier <a href="https://datatracker.ietf.org/doc/html/rfc6555">RFC6555</a>. RFC6555 talked in relatively high-level terms about what’s needed in an algorithm that uses a hostname to establish a connection in a dual-stack environment and made reference to an approach shared by Mozilla Firefox and Google Chrome at the time. RFC8305 goes beyond that and provides much more concrete descriptions of what should be done at each stage.</p>

<p>One thing I really like about their approach is that they provide <a href="https://datatracker.ietf.org/doc/html/rfc8305#section-8">sensible default values</a> for each of the tunable parameters of their algorithm, which were reached through empirical measurement of connection timings across a wide range of devices. A company with the reach of Apple, where this RFC originated, is well-placed to take measurements at the scale needed to find widely applicable defaults. It’s great to see the effort put into making the output of their work useful beyond the walls of their company.</p>

<p>I’m less bullish about the IPv6 transition than some of my friends. If that transition is going to be successful, it involves an unavoidable period of running dual-stack networks<sup id="fnref:maybe-avoidable" role="doc-noteref"><a href="#fn:maybe-avoidable" rel="footnote">3</a></sup>. When doing so caused trouble on my home network, I switched IPv6 off rather than trying to debug it<sup id="fnref:on-my-todo-list" role="doc-noteref"><a href="#fn:on-my-todo-list" rel="footnote">4</a></sup>. If we want people to embrace dual-stack networks—and ultimately IPv6—we need socket libraries that do the right thing by default.</p>

<h3 id="clearer-exception-backtraces">Clearer exception backtraces</h3>

<p>Okay, we’re done with the rabbit hole and we’ve got a simple one to round things off.</p>

<p>In Ruby&nbsp;3.4, exception backtraces <a href="https://bugs.ruby-lang.org/issues/19117">include the method owner</a> (class or module) as well as the name of the method that an exception was raised from.</p>

<p>Let’s look at some code that raises an exception:</p>

<div><pre><code><span>module</span> <span>Foo</span>
  <span>class</span> <span>Bar</span>
    <span>def</span> <span>inspect</span>
      <span>1</span> <span>+</span> <span>'1'</span>
    <span>end</span>
  <span>end</span>
<span>end</span>

<span>p</span> <span>Foo</span><span>::</span><span>Bar</span><span>.</span><span>new</span>
</code></pre></div>

<p>In Ruby&nbsp;3.3 and earlier, this would print a backtrace like:</p>

<div><pre><code>/tmp/foo.rb:4:in `+': String can't be coerced into Integer (TypeError)
	from /tmp/foo.rb:4:in `inspect'
	from /tmp/foo.rb:9:in `p'
	from /tmp/foo.rb:9:in `&lt;main&gt;'
</code></pre></div>

<p>Ruby&nbsp;3.4 prints out the name of the module/class that contains each of the methods in the stack track, like:</p>

<div><pre><code>/tmp/foo.rb:4:in `Integer#+': String can't be coerced into Integer (TypeError)
	from /tmp/foo.rb:4:in `Foo::Bar#inspect'
	from /tmp/foo.rb:9:in `Kernel#p'
	from /tmp/foo.rb:9:in `&lt;main&gt;'
</code></pre></div>

<p>It’s a small change, but I’m a big believer in anything that makes it quicker to understand what a piece of code is doing. I’m glad to see that the Ruby team still values developer experience so highly. It’s one of the best parts of writing the language.</p>

<p>That’s all from me. If you enjoyed this post, do get in touch with your own favourite feature from Ruby&nbsp;3.4 and the best cheese you’ve eaten this Christmas<sup id="fnref:cheese" role="doc-noteref"><a href="#fn:cheese" rel="footnote">5</a></sup>.</p>

<p>You can find me on <a href="https://tech.lgbt/@Sinjo">Mastodon</a> or <a href="https://bsky.app/profile/sinjo.dev">Bluesky</a>. ✌🏻💖🎄</p>



  </article>

</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[DOOM CAPTCHA (654 pts)]]></title>
            <link>https://doom-captcha.vercel.app/</link>
            <guid>42566112</guid>
            <pubDate>Wed, 01 Jan 2025 14:12:15 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://doom-captcha.vercel.app/">https://doom-captcha.vercel.app/</a>, See on <a href="https://news.ycombinator.com/item?id=42566112">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="modal">
    <h2>How it works</h2>
    <p>
      A CAPTCHA that lets you play DOOM® to prove you're human (for
      educational and entertainment purposes)
    </p>
    <p>
      The project works by leveraging <code>Emscripten</code> to compile a
      minimal port of Doom to <code>WebAssembly</code> and enable
      intercommunication between the <code>C</code>-based game runloop (<a href="https://github.com/rauchg/doom-captcha/tree/main/sdldoom-1.10/g_game.c"><code>g_game.c</code></a>) and the
      <code>JavaScript</code>-based CAPTCHA UI.
    </p>

    <p>
      Some extensions were made to the game to introduce relevant events
      needed for its usage in the context of a CAPTCHA.
    </p>

    <ul>
      <li>
        Started out with a minimal, <code>SDL</code>-based port of Doom that
        can be efficiently compiled to <code>WebAssembly</code>
      </li>
      <li>
        Tweaked the build to make it compatible with the shareware version of
        <code>wad</code> (<code>doom1.wad</code>) for legal use
      </li>
      <li>
        Introduced new unofficial process flags:
        <ul>
          <li>
            <code>-nomenu</code> (<a href="https://github.com/rauchg/doom-captcha/tree/main/sdldoom-1.10/m_menu.c"><code>m_menu.c</code></a>)
            to skip the main menu and jump straight into the game
          </li>
          <li>
            <code>-autoreborn</code> (<a href="https://github.com/rauchg/doom-captcha/tree/main/sdldoom-1.10/p_mobj.c"><code>p_mobj.c</code></a>)
            to automatically rebirth the player after a 2s delay
          </li>
        </ul>
      </li>
      <li>
        Introduced callbacks into <code>JS</code> land to be used by the
        CAPTCHA UI:
        <ul>
          <li><code>onPlayerBorn</code> when the player is born or reborn</li>
          <li><code>onPlayerKilled</code> when the player is killed</li>
          <li>
            <code>onEnemyKilled</code> when the main player kills an enemy
          </li>
        </ul>
      </li>
      <li>
        Tweaked the default process flags in
        <a href="https://github.com/rauchg/doom-captcha/tree/main/sdldoom-1.10/d_main.c"><code>d_main.c</code></a>
        to make the game more challenging:
        <ul>
          <li><code>-skill 5</code> sets the difficulty to "Nightmare!"</li>
          <li><code>-fast</code> makes it even harder</li>
          <li>
            <code>-warp e1m1</code> jumpstarts the game to where the action is
          </li>
          <li>
            <code>-nomenu</code> doesn't let the player trigger the main menu
          </li>
        </ul>
      </li>
    </ul>

    <p>
      <a href="https://v0.dev/chat/4X85A52Dzde?b=b_tOXbbZzZPgT">See the v0 UI generation</a>
      or
      <a target="_blank" href="https://github.com/rauchg/doom-captcha">get the source</a>.
    </p>

    <p>
      <em>Built on the shareware version of DOOM® released publically for
        non-commercial use. DOOM® is a registered trademark of id Software
        LLC, a ZeniMax Media company.</em>
    </p>
  </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Show HN: API Parrot – Automatically Reverse Engineer HTTP APIs (259 pts)]]></title>
            <link>https://apiparrot.com/</link>
            <guid>42565821</guid>
            <pubDate>Wed, 01 Jan 2025 13:15:03 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://apiparrot.com/">https://apiparrot.com/</a>, See on <a href="https://news.ycombinator.com/item?id=42565821">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><div><h2><span><span>A</span><span>u</span><span>t</span><span>o</span><span>m</span><span>a</span><span>t</span><span>e</span></span> Any API</h2><p>API Parrot is the tool specifically designed to reverse engineer the HTTP APIs of any website. Making life easier for developers looking to automate, integrate or scrape websites without public APIs.</p></div><div id="features" aria-label="Features"><h2>Everything You Need, All in One Place</h2><p>From recording requests to exporting executable code, API Parrot has you covered, making the workflow fast and efficient.</p></div><div id="use-cases" aria-label="Use Cases"><h2>One tool, many uses</h2><p>API Parrot is designed to be a general tool to help you and your team with a variety of tasks. Here are a few examples of what you can do with it.</p></div><section id="download"><img alt="" loading="lazy" width="3000" height="1080" decoding="async" data-nimg="1" src="https://apiparrot.com/_next/static/media/blackbg.8f11808e.png"><div><h2>Explore API Parrot Yourself</h2><p>Dive into the Beta version for free today. Download now and see how easy and effective API replication can be!</p></div></section><div id="contact" aria-label="What our customers are saying"><h2>Get in Touch</h2><p>Feel free to drop us an email at any time. Whether you have questions about API Parrot, need assistance with the software, or just want to say hello, we’re always ready to assist you.</p><h2><a href="mailto:contact@apiparrot.com">contact@apiparrot.com</a></h2><p>If you have suggestions or want to report a bug, please submitt a issue on our <a href="https://github.com/apiparrot/apiparrot-desktop-releases/issues">GitHub</a> page.</p></div></div><div><div><div><p><img alt="Logo" loading="lazy" width="45" height="45" decoding="async" data-nimg="1" srcset="https://apiparrot.com/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Flogo512.a21742d3.png&amp;w=48&amp;q=75 1x, https://apiparrot.com/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Flogo512.a21742d3.png&amp;w=96&amp;q=75 2x" src="https://apiparrot.com/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Flogo512.a21742d3.png&amp;w=96&amp;q=75"></p><h2>api_parrot</h2></div><nav aria-label="quick links"></nav></div><div><p>Copyright © <!-- -->2025<!-- --> API Parrot All rights reserved.</p></div></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[30% Drop In o1-Preview Accuracy When Putnam Problems Are Slightly Variated (439 pts)]]></title>
            <link>https://openreview.net/forum?id=YXnwlZe0yf&amp;noteId=yrsGpHd0Sf</link>
            <guid>42565606</guid>
            <pubDate>Wed, 01 Jan 2025 12:21:18 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://openreview.net/forum?id=YXnwlZe0yf&#x26;noteId=yrsGpHd0Sf">https://openreview.net/forum?id=YXnwlZe0yf&#x26;noteId=yrsGpHd0Sf</a>, See on <a href="https://news.ycombinator.com/item?id=42565606">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p><strong>Keywords<!-- -->:</strong> <span>Benchmarks, Large Language Models, Mathematical Reasoning, Mathematics, Reasoning, Machine Learning</span></p><p><strong>TL;DR<!-- -->:</strong> <span>Putnam-AXIOM is a challenging mathematical reasoning benchmark for LLMs, revealing significant reasoning performance gaps and the impact of data contamination.</span></p><p><strong>Abstract<!-- -->:</strong> <span>As large language models (LLMs) continue to advance, many existing benchmarks designed to evaluate their reasoning capabilities are becoming saturated. Therefore, we present the Putnam-AXIOM Original benchmark consisting of 236 mathematical problems from the William Lowell Putnam Mathematical Competition, along with detailed step-by-step solutions. To preserve the Putnam-AXIOM benchmark's validity and mitigate potential data contamination, we created the Putnam-AXIOM Variation benchmark with functional variations of 52 problems. By programmatically altering problem elements like variables and constants, we can generate unlimited novel, equally challenging problems not found online. We see that almost all models have significantly lower accuracy in the variations than the original problems. Our results reveal that OpenAI's o1-preview, the best performing model, achieves merely 41.95\% accuracy on the Putnam-AXIOM Original but experiences around a 30\% reduction in accuracy on the variations' dataset when compared to corresponding original problems.</span></p><p><strong>Concurrent Submissions<!-- -->:</strong> <span>ICLR 2025</span></p><p><strong>Submission Number<!-- -->:</strong> <span>86</span></p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Books I Loved Reading in 2024 (304 pts)]]></title>
            <link>https://thoughts.wyounas.com/p/books-i-enjoyed-most-in-2024</link>
            <guid>42564687</guid>
            <pubDate>Wed, 01 Jan 2025 08:16:09 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://thoughts.wyounas.com/p/books-i-enjoyed-most-in-2024">https://thoughts.wyounas.com/p/books-i-enjoyed-most-in-2024</a>, See on <a href="https://news.ycombinator.com/item?id=42564687">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><article><div dir="auto"><p>I was able to read several books this year. Here are some books I loved reading in 2024.</p><p><a href="https://www.hup.harvard.edu/books/9780674995888" rel="">Cicero Letter to Friends: Volume 1</a></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbc7cec03-cdfb-4eaa-87d2-768afccd3a3e_200x304.jpeg" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbc7cec03-cdfb-4eaa-87d2-768afccd3a3e_200x304.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbc7cec03-cdfb-4eaa-87d2-768afccd3a3e_200x304.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbc7cec03-cdfb-4eaa-87d2-768afccd3a3e_200x304.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbc7cec03-cdfb-4eaa-87d2-768afccd3a3e_200x304.jpeg 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbc7cec03-cdfb-4eaa-87d2-768afccd3a3e_200x304.jpeg" width="200" height="304" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/bc7cec03-cdfb-4eaa-87d2-768afccd3a3e_200x304.jpeg&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:304,&quot;width&quot;:200,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:&quot;Letters to Friends, Volume I&quot;,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:false,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false}" alt="Letters to Friends, Volume I" title="Letters to Friends, Volume I" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbc7cec03-cdfb-4eaa-87d2-768afccd3a3e_200x304.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbc7cec03-cdfb-4eaa-87d2-768afccd3a3e_200x304.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbc7cec03-cdfb-4eaa-87d2-768afccd3a3e_200x304.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbc7cec03-cdfb-4eaa-87d2-768afccd3a3e_200x304.jpeg 1456w" sizes="100vw"></picture></div></a></figure></div><p>Cicero was a Roman statesman, and not only was he an orator of the highest order, but he was also a captivating writer who penned beautiful prose. There are few individuals in history who have written prose more beautiful than his. In fact, he wrote such exquisite Latin that, at one time, education literally meant reading Cicero, Cicero, and some more Cicero.</p><p>This book is a collection of letters, translated from Latin to English, that he wrote to friends, family members, and colleagues. Sometimes, he wrote letters because he was concerned about a friend’s health; other times, he wrote to his family because he was worried about their well-being or needed to give them directions. Occasionally, he wrote to friends and colleagues to curry favor for people he knew.</p><p>Let me share a few excerpts. Sometimes he would talk about books, as in:</p><blockquote><p><br><span>In my command here I have put into practice the whole </span><em>Education of Cyrus,</em><span> a work which I read so often that I wore out the book.</span><br></p></blockquote><p>What follows is Cicero asking a friend for a favor for another friend:</p><blockquote><p>Now, my dear Paetus, be a friend, and take the whole affair upon yourself, and relieve Fabius of the worry. We need your name and sound judgment, and your personal influence too. Don’t let the brothers get into litigation and become embroiled in discreditable lawsuits. Mato and Pollio are Fabius’ enemies. Briefly, I assure you that I cannot write down in full how much you will oblige me if you put Fabius’ mind at ease. He thinks, and persuades me, that it all depends on you.</p></blockquote><p>Even though I was reading English translations of his letters, his prose remained captivating, even in translation. There are two reasons I like this book. First, it offers insight into the old times, life back then, and how things were. Second, he wrote engaging prose, sometimes even lyrical. Among countless examples, here are a few phrases I enjoyed:</p><blockquote><p>Please rest assured that my solicitude for your credit increases every day, though your own integrity and clemency have so enhanced it that any addition seems impossible.</p></blockquote><p>Another one: </p><blockquote><p>If he proves himself worthy of his forbears, as I expect and hope he will, some part of the credit will go to you. If he stumbles, the damage will be entirely his, not yours at all.</p></blockquote><p>And this one has a nice ring to it: </p><blockquote><p>If you do what is best for your health, you will best comply with my wishes. So think it over in that clever head of yours. I miss you, but I love you. Loving you, I want to see you fit and well; missing you, I want to see you as soon as possible. The former, there, must come first. So make it your chief concern to get well. Of your countless services to me this will be the one I shall most appreciate.</p></blockquote><p>How beautifully he weaves simple words to convey emotion.</p><p>In the truest sense of the word, he was a man of letters. Writing prose like his isn’t something an ordinary individual can easily emulate.</p><p><a href="https://www.amazon.com/Mathematical-Logic-Computer-Science-Third/dp/1447141288" rel="">Mathematical Logic for Computer Science by Ben-Ari</a></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F062df67b-bf6a-4aa0-80a5-e23156cb60ee_344x522.jpeg" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F062df67b-bf6a-4aa0-80a5-e23156cb60ee_344x522.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F062df67b-bf6a-4aa0-80a5-e23156cb60ee_344x522.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F062df67b-bf6a-4aa0-80a5-e23156cb60ee_344x522.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F062df67b-bf6a-4aa0-80a5-e23156cb60ee_344x522.jpeg 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F062df67b-bf6a-4aa0-80a5-e23156cb60ee_344x522.jpeg" width="300" height="455.2325581395349" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/062df67b-bf6a-4aa0-80a5-e23156cb60ee_344x522.jpeg&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:522,&quot;width&quot;:344,&quot;resizeWidth&quot;:300,&quot;bytes&quot;:null,&quot;alt&quot;:&quot;Mathematical Logic for Computer Science&quot;,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false}" alt="Mathematical Logic for Computer Science" title="Mathematical Logic for Computer Science" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F062df67b-bf6a-4aa0-80a5-e23156cb60ee_344x522.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F062df67b-bf6a-4aa0-80a5-e23156cb60ee_344x522.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F062df67b-bf6a-4aa0-80a5-e23156cb60ee_344x522.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F062df67b-bf6a-4aa0-80a5-e23156cb60ee_344x522.jpeg 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p>I wanted to brush up on logic for computer science, and this book came highly recommended by someone I trust. It did not disappoint. It’s a technical book, heavy on theory, so I had to read it slowly, ensuring I absorbed everything as I went along.</p><p>Some books aren’t easy to read and aren’t rewarding either, while others aren’t easy to read yet are rewarding. This one wasn’t easy, but it was certainly a rewarding experience for me. The best part of the book was its use of a first-principles approach.</p><p><span>The book covers propositional logic, deductive systems such as Gentzen (I even </span><a href="https://wyounas.github.io/cs/2024/10/29/gentzen-system/" rel="">blogged about it</a><span>) and Hilbert, Binary Decision Diagrams, SAT solvers, First-Order Logic, Temporal Logic, verification of sequential programs, and verification of concurrent programs. The last three topics intrigued me the most, and I liked the way the content was presented so much that I bought another book by the same author and read it.</span></p><p><a href="https://www.amazon.com/Philosophy-Way-Life-Spiritual-Exercises/dp/0631180338" rel="">Philosophy as a Way of Life by Pierre Hadot</a></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd2c44f10-a528-4216-9806-36fee5e33657_618x868.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd2c44f10-a528-4216-9806-36fee5e33657_618x868.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd2c44f10-a528-4216-9806-36fee5e33657_618x868.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd2c44f10-a528-4216-9806-36fee5e33657_618x868.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd2c44f10-a528-4216-9806-36fee5e33657_618x868.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd2c44f10-a528-4216-9806-36fee5e33657_618x868.png" width="304" height="426.97734627831716" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/d2c44f10-a528-4216-9806-36fee5e33657_618x868.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:868,&quot;width&quot;:618,&quot;resizeWidth&quot;:304,&quot;bytes&quot;:268299,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd2c44f10-a528-4216-9806-36fee5e33657_618x868.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd2c44f10-a528-4216-9806-36fee5e33657_618x868.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd2c44f10-a528-4216-9806-36fee5e33657_618x868.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd2c44f10-a528-4216-9806-36fee5e33657_618x868.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p>I came across this book accidentally, but I’m so glad I did. Pierre Hadot was a French philosopher who could not only philosophize but also write simply—an ability not common among top-tier philosophers, if I may say so with respect.</p><p>This work explores themes on how to use philosophy to live better lives. What better purpose could philosophy serve than this?</p><p>Hadot believed that the purpose of ancient philosophy was to transform souls, which is why philosophical teaching was primarily given in oral form. Nothing stirs the soul like a good spoken word, and the written part was, for the most part, a complement to this oral teaching. The oral tradition and dialogue were at the center of philosophy. While it was important to present the solution, it was even more important to show the “path traversed together in arriving at this solution.”</p><p><span>This book offered insights that were outside my sphere of knowledge. For one, if you want to truly understand why Marcus Aurelius wrote his </span><em>Meditations</em><span> the way he did, then this book is for you. You learn why he sought to reform rather than inform. You also discover the framework he used to write down his meditations. Further, you learn about Epictetus and his philosophy. For instance, one should not content oneself with philosophical discourses or texts, but strive to live a good life. As Epictetus once said, “our only occupation should be the cure of ourselves.” The essence of philosophy, then, is to produce good actions, to live virtuously and not viciously, to pursue truth and not ignorance.</span></p><p>He also talks about spiritual exercises in this book, which is both interesting and worth a post in its own right.</p><p><span>Philosophy shouldn’t be confined to specific institutions or individuals. Instead, we all should be able to use it to refine ourselves, to dialogue with others and with oneself, to examine our conscience, and to be masters of ourselves. The idea is not for anyone to “satisfy himself with discourse, with the conceptual architecture that he has constructed, without putting into question his own life.” Hadot calls this a </span><em>perpetual danger of philosophy,</em><span> where a philosopher takes refuge in the “reassuring universe of concepts and of discourse instead of going beyond discourse in order to take upon himself the risk of the radical transformation of himself.”</span></p><p>I feel Pierre Hadot’s aim in writing this book was to make philosophy accessible to common men and women, to help us use philosophy to live better lives. I believe he succeeded. There is much in this book that can serve us well.</p><p><a href="https://www.loebclassics.com/view/LCL146/2009/volume.xml" rel="">Asechylus’s Oresteia</a></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb6f7f1c9-9db6-42c8-ac4b-39c48ffd3cc0_200x314.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb6f7f1c9-9db6-42c8-ac4b-39c48ffd3cc0_200x314.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb6f7f1c9-9db6-42c8-ac4b-39c48ffd3cc0_200x314.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb6f7f1c9-9db6-42c8-ac4b-39c48ffd3cc0_200x314.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb6f7f1c9-9db6-42c8-ac4b-39c48ffd3cc0_200x314.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb6f7f1c9-9db6-42c8-ac4b-39c48ffd3cc0_200x314.png" width="200" height="314" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/b6f7f1c9-9db6-42c8-ac4b-39c48ffd3cc0_200x314.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:314,&quot;width&quot;:200,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:&quot;Book cover image.&quot;,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false}" alt="Book cover image." title="Book cover image." srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb6f7f1c9-9db6-42c8-ac4b-39c48ffd3cc0_200x314.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb6f7f1c9-9db6-42c8-ac4b-39c48ffd3cc0_200x314.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb6f7f1c9-9db6-42c8-ac4b-39c48ffd3cc0_200x314.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb6f7f1c9-9db6-42c8-ac4b-39c48ffd3cc0_200x314.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p><span>Aeschylus was not just a soldier but also the father of Greek tragedy and a figure whose influence still echoes through his works. He won first prizes often at the Dionysia (an ancient Greek festival that included theatrical performances of tragedies and comedies). </span><em>Oresteia</em><span>, a winner at the Dionysia, was a trilogy that explored the themes of justice, vengeance, and the evolution of societal order. </span></p><p>There is much wisdom in the trilogy. This following famous quote about “learning by suffering” is in the first play:</p><blockquote><p><span>Zeus, who set mortals on the road</span><br><span>to understanding, who made</span><br><span>“learning by suffering” into an effective law.</span><br><span>There drips before the heart, instead of sleep,</span><br><span>The misery of pain recalled: good sense comes to men.</span></p></blockquote><p>His thoughts on human psychology and the way he crafted his stories are why I liked this book. I have to admit, there is a lot in it that I need to absorb. I’ll surely be re-reading this one sometime soon. It’s one of those books that one ought to read multiple times in life because I’m certain that every time we read it, it will offer something new.</p><p><a href="https://www.amazon.com/Principles-Model-Checker-Mordechai-Ben-Ari/dp/1846287693/" rel="">Principles of the SPIN Model Checker by Ben-Ari</a></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F66e3f4be-ea50-4d70-bb4c-37cdc528e46a_343x522.jpeg" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F66e3f4be-ea50-4d70-bb4c-37cdc528e46a_343x522.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F66e3f4be-ea50-4d70-bb4c-37cdc528e46a_343x522.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F66e3f4be-ea50-4d70-bb4c-37cdc528e46a_343x522.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F66e3f4be-ea50-4d70-bb4c-37cdc528e46a_343x522.jpeg 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F66e3f4be-ea50-4d70-bb4c-37cdc528e46a_343x522.jpeg" width="305" height="464.16909620991254" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/66e3f4be-ea50-4d70-bb4c-37cdc528e46a_343x522.jpeg&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:522,&quot;width&quot;:343,&quot;resizeWidth&quot;:305,&quot;bytes&quot;:null,&quot;alt&quot;:&quot;Principles of the Spin Model Checker&quot;,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false}" alt="Principles of the Spin Model Checker" title="Principles of the Spin Model Checker" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F66e3f4be-ea50-4d70-bb4c-37cdc528e46a_343x522.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F66e3f4be-ea50-4d70-bb4c-37cdc528e46a_343x522.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F66e3f4be-ea50-4d70-bb4c-37cdc528e46a_343x522.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F66e3f4be-ea50-4d70-bb4c-37cdc528e46a_343x522.jpeg 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p>There are technical books that are thick and heavy yet offer little, and then there are books that are short yet offer insight and ignite your interest and curiosity even further. This book gave me both insight and a deeper curiosity about the subject.</p><p>This book introduces the SPIN model checker and exposes the reader to the nuances of verifying programs using SPIN. A nice feature of the book is how it explains concepts using thoughtfully developed, freely available tools (such as jSpin) that the author created for pedagogical purposes.</p><p><span>It’s a demanding book, but the content was interesting. The book presents details about concurrency, temporal logic, non-determinism, advanced SPIN topics, and even case studies. I’ve already </span><a href="https://wyounas.github.io/spin,/model-checking,/formal-methods/2024/11/10/how-control-structures-work-in-promela/" rel="">written</a><span> a </span><a href="https://wyounas.github.io/spin,/model-checking,/formal-methods/2024/11/18/how-rendezvous-channels-work-in-promela-spin/" rel="">few</a><span> </span><a href="https://wyounas.github.io/concurrency/2024/12/12/how-concurrency-works-a-visual-guide/" rel="">blog posts</a><span> about the topics I explored while reading this book, and I plan to write more. Overall, it’s a good book. I’m sure I’ll revisit the latter half again.</span></p><p><a href="https://www.amazon.com/Born-Standing-Up-Comics-Life/dp/1416553657" rel="">Born Standing Up by Steve Martin</a></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6b21c7ca-c390-402d-87e3-555801132b99_598x860.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6b21c7ca-c390-402d-87e3-555801132b99_598x860.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6b21c7ca-c390-402d-87e3-555801132b99_598x860.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6b21c7ca-c390-402d-87e3-555801132b99_598x860.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6b21c7ca-c390-402d-87e3-555801132b99_598x860.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6b21c7ca-c390-402d-87e3-555801132b99_598x860.png" width="300" height="431.438127090301" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/6b21c7ca-c390-402d-87e3-555801132b99_598x860.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:860,&quot;width&quot;:598,&quot;resizeWidth&quot;:300,&quot;bytes&quot;:419802,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6b21c7ca-c390-402d-87e3-555801132b99_598x860.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6b21c7ca-c390-402d-87e3-555801132b99_598x860.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6b21c7ca-c390-402d-87e3-555801132b99_598x860.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6b21c7ca-c390-402d-87e3-555801132b99_598x860.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p>It’s a nice, short, and moving memoir by the comedian and actor Steve Martin. The book details his early working days at shops and clubs, his relationships, and eventually his rise to fame. He shared his struggles, his failures, long hours of practice, his reflections to refine his craft, and his efforts to find his own unique voice as a comedian. Any memoir that only talks about successes and never discusses struggles, rejections, or weaknesses is suspect. This one is not. What I like about his writing is that it feels candid and, at times, funny.</p><p>From reading the book, it’s clear that recognition for one’s work takes time. What you can’t control is how and when you get recognition. What you can control is how hard you work and persevere. He firmly understood what he could control and focused on that. His was a long journey, full of obstacles, but he never gave up. He kept refining his craft, kept working, kept improving, and finally, he earned the recognition he much deserved.</p><p>Reading this book reinforces the idea that success is nine times patience times effort times reflection times some luck.</p><p><a href="https://www.amazon.com/Less-Than-One-Selected-Classics/dp/0374539057" rel="">Less Than One by Joseph Brodsky</a></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc686e998-8352-461a-b886-331b931f1a00_588x808.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc686e998-8352-461a-b886-331b931f1a00_588x808.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc686e998-8352-461a-b886-331b931f1a00_588x808.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc686e998-8352-461a-b886-331b931f1a00_588x808.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc686e998-8352-461a-b886-331b931f1a00_588x808.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc686e998-8352-461a-b886-331b931f1a00_588x808.png" width="304" height="417.74149659863946" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/c686e998-8352-461a-b886-331b931f1a00_588x808.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:808,&quot;width&quot;:588,&quot;resizeWidth&quot;:304,&quot;bytes&quot;:296661,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc686e998-8352-461a-b886-331b931f1a00_588x808.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc686e998-8352-461a-b886-331b931f1a00_588x808.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc686e998-8352-461a-b886-331b931f1a00_588x808.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc686e998-8352-461a-b886-331b931f1a00_588x808.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p><span>Joseph Brodsky was a poet, an essayist, and a Nobel Prize winner. This book is a collection of essays, and some of them are quite profound. One of his essays on Dostoevsky is particularly insightful; it left such an impression on me that I reflected on it and </span><a href="https://thoughts.wyounas.com/p/what-made-dostoevsky-immortal" rel="">wrote an essay of my own</a><span>.</span></p><p>He is one of those writers who frequently shares wisdom about literature or the human condition in his writing. You don’t stop reading him because he keeps you so engaged by offering insights on the topic at hand.</p><p>He’s also one of those writers who gives you unexpected angles on familiar things. For instance, his essay on Dostoevsky offers subtle insights into why Dostoevsky’s work is immortal. </p><p>One difference between masters and beginners is that masters have explored and mastered details that beginners may not even notice. Similarly, another takeaway from this book is understanding how experienced literary connoisseurs interpret and analyze literary works. The way he untangles and interprets some literary pieces, and the depth and detail he explores, isn’t just insightful—it’s something that someone like me couldn’t even notice at first. He has a whole essay on W.H. Auden’s poem, and the way he delves into it is simply fascinating.</p><p>All in all, it’s a good book with some thought-provoking writing.</p><p><a href="https://www.amazon.com/Wizard-Earthsea-Cycle/dp/0547773749" rel="">A Wizard of Earthsea by Ursula Le Guin</a></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F40bc6a68-9aa8-473f-9af5-593eafcd619a_590x934.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F40bc6a68-9aa8-473f-9af5-593eafcd619a_590x934.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F40bc6a68-9aa8-473f-9af5-593eafcd619a_590x934.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F40bc6a68-9aa8-473f-9af5-593eafcd619a_590x934.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F40bc6a68-9aa8-473f-9af5-593eafcd619a_590x934.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F40bc6a68-9aa8-473f-9af5-593eafcd619a_590x934.png" width="300" height="474.91525423728814" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/40bc6a68-9aa8-473f-9af5-593eafcd619a_590x934.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:934,&quot;width&quot;:590,&quot;resizeWidth&quot;:300,&quot;bytes&quot;:1057724,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F40bc6a68-9aa8-473f-9af5-593eafcd619a_590x934.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F40bc6a68-9aa8-473f-9af5-593eafcd619a_590x934.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F40bc6a68-9aa8-473f-9af5-593eafcd619a_590x934.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F40bc6a68-9aa8-473f-9af5-593eafcd619a_590x934.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p>I had never read anything by Ursula Le Guin, but then I came across some excerpts of her work online, and I thought I should give her a try. I was also going through a phase in my life where I believed reading more fiction would help power up my imagination. I picked this book up, and I liked it.</p><p>On the surface, it’s about wizards and magic, but there’s more to it. Every now and then, Ursula Le Guin sprinkles in some wisdom.</p><p>One reason I like this book is how the plot folds and then unfolds. Another reason is that you come across phrases that make you pause and think, like: </p><blockquote><p>It is very hard for evil to take hold of the unconsenting soul.</p></blockquote><p>Or this:</p><blockquote><p>To light a candle is to cast a shadow...</p></blockquote><p>Or this:</p><blockquote><p>I had forgotten how much light there is in the world, till you gave it back to me.</p></blockquote><p>I enjoyed the book, and I plan on exploring and reading more of her work.</p><p><a href="https://www.amazon.com/Best-Tagore-Introduced-Rudrangshu-Mukherjee/dp/1101908386" rel="">The Best of Tagore</a></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Feb310459-6b5c-4855-bfa3-a8c643f11d61_594x954.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Feb310459-6b5c-4855-bfa3-a8c643f11d61_594x954.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Feb310459-6b5c-4855-bfa3-a8c643f11d61_594x954.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Feb310459-6b5c-4855-bfa3-a8c643f11d61_594x954.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Feb310459-6b5c-4855-bfa3-a8c643f11d61_594x954.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Feb310459-6b5c-4855-bfa3-a8c643f11d61_594x954.png" width="302" height="485.030303030303" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/eb310459-6b5c-4855-bfa3-a8c643f11d61_594x954.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:954,&quot;width&quot;:594,&quot;resizeWidth&quot;:302,&quot;bytes&quot;:1008996,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Feb310459-6b5c-4855-bfa3-a8c643f11d61_594x954.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Feb310459-6b5c-4855-bfa3-a8c643f11d61_594x954.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Feb310459-6b5c-4855-bfa3-a8c643f11d61_594x954.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Feb310459-6b5c-4855-bfa3-a8c643f11d61_594x954.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p>Rabindranath Tagore was a Bengali poet, writer, composer, philosopher, social reformer, and Nobel Prize winner. I admire philosophers who not only talk but also put their words into action to better the world. Tagore not only left behind a remarkable body of work but also tried to make the world a better place through his actions—such as opening a school, which now boasts a list of distinguished alumni.</p><p>This book contains his essays and plays. It’s a big volume, and I bought it because it includes his most notable works, although, for now, I was mainly interested in his essays. Some of his essays are thought-provoking. His reflections on education, nationalism, and other topics are not only stirring but also showcase literary brilliance. The man could write.</p><p>Here is one of his quotes I particularly like:</p><blockquote><p><br><span>The one who plants trees, knowing that he will never sit in their shade, has at least started to understand the meaning of life.</span></p></blockquote><p><a href="https://www.amazon.com/Strangest-Man-Hidden-Dirac-Mystic/dp/0465022103" rel="">The Strangest Man by Graham Farmelo</a></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8cd51065-68fe-431d-8678-e9e8f3650bd0_594x870.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8cd51065-68fe-431d-8678-e9e8f3650bd0_594x870.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8cd51065-68fe-431d-8678-e9e8f3650bd0_594x870.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8cd51065-68fe-431d-8678-e9e8f3650bd0_594x870.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8cd51065-68fe-431d-8678-e9e8f3650bd0_594x870.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8cd51065-68fe-431d-8678-e9e8f3650bd0_594x870.png" width="306" height="448.1818181818182" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/8cd51065-68fe-431d-8678-e9e8f3650bd0_594x870.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:870,&quot;width&quot;:594,&quot;resizeWidth&quot;:306,&quot;bytes&quot;:762984,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8cd51065-68fe-431d-8678-e9e8f3650bd0_594x870.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8cd51065-68fe-431d-8678-e9e8f3650bd0_594x870.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8cd51065-68fe-431d-8678-e9e8f3650bd0_594x870.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8cd51065-68fe-431d-8678-e9e8f3650bd0_594x870.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p>This is a good biography of arguably the most brilliant scientist, Paul Dirac, famous for co-inventing the most revolutionary theory in the last hundred years or so—the theory of quantum mechanics. Paul Dirac was a man of very few words, hardworking, and a brilliant scientist who took pride in the beauty of mathematics.</p><p>Sometimes I believe an appalling childhood is a precondition for genius; Dirac had one. He once said, “I never had a childhood.” But thankfully, he received a superb education. He was not known to talk much, even during his childhood. The only time he spoke was when he politely corrected the errors of his teachers. Dirac believed he was a geometrical thinker, probably due to his excellent technical drawing skills learned at school. Eventually, he earned a PhD at Cambridge, and as a scientist, he never looked back. There is so much in this book that I should probably write about in a separate review sometime.</p><p>Dirac was also quite a character, and making small talk with him wasn’t easy. He once responded to the comment, “It’s a bit rainy, isn’t it?” by walking to the window, returning to his seat, and stating, “It is not now raining.” He was very careful with his speech; he once said, “I was always taught not to start a sentence until I knew how to finish it.”</p><p>His contributions to the field of physics are seminal. He definitely led a life worth living. I found this an engaging biography and thoroughly enjoyed reading it.</p><p><a href="https://www.amazon.com/Argumentative-Indian-Amartya-Sen/dp/0141012110" rel="">The argumentative Indian by Amartya Sen</a></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4e93be41-25de-492b-9036-201ac7856cfe_610x878.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4e93be41-25de-492b-9036-201ac7856cfe_610x878.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4e93be41-25de-492b-9036-201ac7856cfe_610x878.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4e93be41-25de-492b-9036-201ac7856cfe_610x878.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4e93be41-25de-492b-9036-201ac7856cfe_610x878.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4e93be41-25de-492b-9036-201ac7856cfe_610x878.png" width="300" height="431.8032786885246" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/4e93be41-25de-492b-9036-201ac7856cfe_610x878.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:878,&quot;width&quot;:610,&quot;resizeWidth&quot;:300,&quot;bytes&quot;:1043718,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4e93be41-25de-492b-9036-201ac7856cfe_610x878.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4e93be41-25de-492b-9036-201ac7856cfe_610x878.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4e93be41-25de-492b-9036-201ac7856cfe_610x878.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4e93be41-25de-492b-9036-201ac7856cfe_610x878.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p>I had not read any of Amartya Sen’s work before this book. Mr. Sen is a notable scholar and a winner of the Nobel Prize in Economics. It’s stimulating work as well as easy to read.</p><p>Although parts of South Asia are not commonly known to have an argumentative gene, Amartya Sen convincingly shows that Indian history is deeply rooted in an argumentative tradition. He masterfully weaves evidence and tales to make his point. There is much I learned that I didn’t know, even though I’m from South Asia (which was a bit embarrassing). For instance, I learned how poets from the working classes questioned social divisions and the barriers of disparate religions. He also cites historical evidence showing how the argumentative tradition was firmly rooted in India’s history. For example, Ashoka’s remarkable interest in the rules of discussion in the third century BC is evident in one of his edicts: “A person must not do reverence to his own sect or disparage the beliefs of another without reason.” In parts of South Asia today, different sects still find it difficult to sit and settle their differences even when the state intervenes, yet a few centuries ago, the king Akbar arranged interfaith dialogues.</p><p>Sen also thoroughly presents a convincing case for how India’s intellectual accomplishments were marred by biases associated with its colonial past. His critique of James Mill’s (father of John Stuart Mill) work on Indian history is particularly insightful.</p><p>This book also introduced me to Tagore and his ideas, such as his views on education and his critique of patriotism and nationalism.</p><p>Additionally, the book discusses the accomplished Indian filmmaker Satyajit Ray (who was awarded an Oscar for lifetime of cinematic excellence). After reading about him, I watched some of his movies, which are not only fascinating but also thought-provoking. Simple storytelling, yet stories that make you think.</p><p>Overall, this book illuminated some corners of my brain—it was educational.</p><p><a href="https://www.amazon.com/Nietzsche-Spirits-Cambridge-History-Philosophy/dp/0521567041" rel="">Human, All Too Human by Nietzsche</a></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8d11094b-ae20-48c0-8351-f4066bafebae_608x764.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8d11094b-ae20-48c0-8351-f4066bafebae_608x764.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8d11094b-ae20-48c0-8351-f4066bafebae_608x764.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8d11094b-ae20-48c0-8351-f4066bafebae_608x764.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8d11094b-ae20-48c0-8351-f4066bafebae_608x764.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8d11094b-ae20-48c0-8351-f4066bafebae_608x764.png" width="306" height="384.5131578947368" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/8d11094b-ae20-48c0-8351-f4066bafebae_608x764.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:764,&quot;width&quot;:608,&quot;resizeWidth&quot;:306,&quot;bytes&quot;:234442,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8d11094b-ae20-48c0-8351-f4066bafebae_608x764.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8d11094b-ae20-48c0-8351-f4066bafebae_608x764.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8d11094b-ae20-48c0-8351-f4066bafebae_608x764.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8d11094b-ae20-48c0-8351-f4066bafebae_608x764.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p>Nietzsche wasn’t trained as a philosopher; he was trained as a philologist, and he had a way with words. He wrote penetrating thoughts about culture, society, morals, arts, and politics in this multifaceted work. You may disagree with Nietzsche on ideas he considers first-rate, but you cannot disregard him as a second-rate writer.</p><p>Nietzsche uses an aphoristic style, which Wikipedia defines as: “An aphorism is a concise, terse, laconic, or memorable expression of a general truth or principle.” Why did he use that style? Perhaps because of his severe and disabling health problems. Or maybe he prized the aphoristic style because he believed “brevity is the soul of wit.” Some of the aphorisms in the book indeed show sharp wit.</p><p><span>One might speculate that his aphoristic style could lead to incoherence of thought in this work. I don’t find that to be the case. In several places, I found him laying the foundations for some of his notable ideas, such as </span><em>will to power</em><span>. One cannot be incoherent and simultaneously lay out influential ideas in philosophy.</span></p><p><span>One major theme I took away from this book is to bring anything I believe under much stronger scrutiny; only such scrutiny can bring us closer to the truth. The subtitle of the book is </span><em>A Book for Free Spirits</em><span>. What does he mean by a free spirit? It’s someone able to liberate themselves from the chains that hinder them, to scrupulously scrutinize ideas, and to show determination to undertake tasks requiring excellence, strength, and courage to break away from conventional values, traditions, and societal expectations.</span></p><p>In Nietzsche’s own words, a free spirit feels like a wanderer, rejects dependence on things like honors, money, or official positions, and is thankful for the struggles and uncertainties of life. A free spirit is curious and daring, works tirelessly, uses resources carefully, is inventive and flexible, and has a love of solitude.</p><p>On being bound by tradition, in one of his aphorisms he says:</p><blockquote><p><br><span>The less men are bound by tradition, the greater is the fermentation of motivations within them, and the greater, in consequence, their outward restlessness, their mingling together with one another, their polyphony of endeavors.</span></p></blockquote><p>On habits in drawing conclusions, he observes:</p><blockquote><p><br><span>The commonest erroneous conclusions drawn by mankind are these: a thing exists, therefore it has a right to. Here the conclusion is from the capacity to live to the fitness to live, from the fitness to live to the right to live. Then: an opinion makes happy, therefore it is a true opinion, its effect is good, therefore it itself is good and true.</span></p></blockquote><p><span>In the book, one can also see Nietzsche developing and laying the foundations for his ideas, such as </span><em>will to power</em><span>. For example:</span></p><blockquote><p><br><span>The reason the man of power is grateful is this. His benefactor has, through the help he has given him, as it were, laid hands on the sphere of the man of power and intruded into it: now, by way of requital, the man of power in turn lays hands on the sphere of his benefactor through the act of gratitude. It is a milder form of revenge. If he did not have the compensation of gratitude, the man of power would have appeared unpowerful and thenceforth counted as such. That is why every community of the good, that is to say originally the powerful, places gratitude among its first duties. Swift suggested that men are grateful in the same degree as they are revengeful.</span></p></blockquote><p>On intellect and morality, he writes:</p><blockquote><p><br><span>One has to have a good memory if one is to keep a promise. One has to have a powerful imagination if one is to feel sympathy. So closely is morality tied to the quality of the intellect.</span></p></blockquote><p><span>There is an interesting chapter titled </span><em>From the Souls of Artists and Writers</em><span>—interesting because art and writing interest me. On the sudden occurrence of inspiration versus gradual work leading to excellence, he states:</span></p><blockquote><p><br><span>Artists have an interest in the existence of a belief in the sudden occurrence of ideas, in so-called inspirations; as though the idea of a work of art, a poem, the basic proposition of a philosophy flashed down from heaven like a ray of divine grace. In reality, the imagination of a good artist or thinker is productive continually, of good, mediocre and bad things, but this power of judgement, sharpened and practised to the highest degree, rejects, selects, knots together; as we can now see from Beethoven’s notebooks how the most glorious melodies were put together gradually and as it were culled out of many beginnings.</span></p></blockquote><p>Nietzsche also shares a recipe for becoming a good novelist:</p><blockquote><p><br><span>The recipe for becoming a good novelist, for example, is easy to give, but to carry it out presupposes qualities one is accustomed to overlook when one says, ‘I do not have enough talent.’ One has only to make a hundred or so sketches for novels, none longer than two pages but of such distinctness that every word in them is necessary; one should write down anecdotes each day until one has learned how to give them the most pregnant and effective form; one should be tireless in collecting and describing human types and characters; one should above all relate things to others and listen to others relate, keeping one’s eyes and ears open for the effect produced on those present; one should travel like a landscape painter or costume designer; one should excerpt for oneself out of the individual sciences everything that will produce an artistic effect when it is well described; one should, finally, reflect on the motives of human actions, disdain no signpost to instruction about them, and be a collector of these things by day and night. One should continue in this many-sided exercise some ten years: what is then created in the workshop, however, will be fit to go out into the world. What, however, do most people do? They begin, not with the parts, but with the whole. Perhaps they chance to strike a right note, excite attention and from then on strike worse and worse notes, for good, natural reasons.</span></p></blockquote><p>On why thinkers write badly, Nietzsche remarks:</p><blockquote><p><br><span>Most thinkers write badly because they communicate to us not only their thoughts but also the thinking of their thoughts.</span></p></blockquote><p>And on who is the best author, he states:</p><blockquote><p><br><span>The best author will be he who is ashamed to become a writer.</span></p></blockquote><p>Nietzsche also has interesting thoughts favoring idleness:</p><blockquote><p><br><span>Scholars are ashamed of </span><em>otium</em><span> (leisure). But there is something noble about leisure and idleness. If idleness really is the beginning of all vice, then it is at any rate in the closest proximity to all virtue; the idle man is always a better man than the active. But when I speak of leisure and idleness, you do not think I am alluding to you, do you, you sluggers?</span></p></blockquote><p>And in another aphorism about leisure, he notes:</p><blockquote><p><br><span>The man who lies ill in bed sometimes discovers that what he is ill from is usually his office, his business, or his society, and that through them he has lost all circumspection with regard to himself: he acquires this wisdom from the leisure to which his illness has compelled him.</span></p></blockquote><p>This is the only book from which I want to share the most. Right now, as I write this, just like a child who wants to show all his favorite toys to his friends, I want to share all my favorite thoughts from this book. I have to admit, I have never read a philosopher who had more penetrating thoughts in a book. Some of his ideas hit you like a thunderbolt. His writing is penetrating, and he makes his point in as few words as possible. He means business. He should be read.</p><p><em>If you enjoyed this essay, or if it struck a chord with you, I would be honored if you shared it with someone who might appreciate it. Writing gives my life meaning—it’s a pursuit I hold close—and your support, whether through a subscription, a one-time donation, or a recommendation, motivates me to keep going. For this, I am deeply grateful. Thank you for being part of this journey.</em></p></div></article></div><div id="discussion"><h4>Discussion about this post</h4></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Cesium for Unreal – Bring the Real World to Unreal Engine (134 pts)]]></title>
            <link>https://cesium.com/platform/cesium-for-unreal/</link>
            <guid>42563845</guid>
            <pubDate>Wed, 01 Jan 2025 03:57:03 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://cesium.com/platform/cesium-for-unreal/">https://cesium.com/platform/cesium-for-unreal/</a>, See on <a href="https://news.ycombinator.com/item?id=42563845">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><h3 id="add-real-world-context-with-3d-content-and-pipelines-with-cesium-ion"><strong>Add real-world context with 3D content and pipelines with Cesium ion</strong></h3><h4 id="instant-access-to-global-3d-content">Instant access to global 3D content</h4><p>Cesium for Unreal plugin will ship with an integration for Cesium ion, allowing instant access to cloud-based global high-resolution 3D content including photogrammetry, terrain, imagery, and buildings.</p><h4 id="industry-leading-3d-tiling-pipelines">Industry-leading 3D tiling pipelines</h4><p>Transform your content into optimized spatially indexed 3D Tiles ready to be streamed to Unreal Engine using industry-leading content pipelines, available as part of your Cesium ion subscription or with Cesium ion Self-Hosted.</p><h4 id="online-and-in-your-own-environment-">Online and in your own environment </h4><p>Cesium for Unreal supports cloud and private network content and services based on open standards and APIs. Users can deploy in their own environment for a scalable, customizable, end-to-end workflow.</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Large Concept Models: Language modeling in a sentence representation space (153 pts)]]></title>
            <link>https://github.com/facebookresearch/large_concept_model</link>
            <guid>42563534</guid>
            <pubDate>Wed, 01 Jan 2025 02:38:22 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/facebookresearch/large_concept_model">https://github.com/facebookresearch/large_concept_model</a>, See on <a href="https://news.ycombinator.com/item?id=42563534">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><p dir="auto"><h2 tabindex="-1" dir="auto">Large Concept Models</h2><a id="user-content-large-concept-models" aria-label="Permalink: Large Concept Models" href="#large-concept-models"></a></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Language Modeling in a Sentence Representation Space</h2><a id="user-content-language-modeling-in-a-sentence-representation-space" aria-label="Permalink: Language Modeling in a Sentence Representation Space" href="#language-modeling-in-a-sentence-representation-space"></a></p>
<p dir="auto"><a href="https://ai.meta.com/blog/meta-fair-updates-agents-robustness-safety-architecture/" rel="nofollow">[Blog]</a> <a href="https://ai.meta.com/research/publications/large-concept-models-language-modeling-in-a-sentence-representation-space/" rel="nofollow">[Paper]</a></p>
<p dir="auto">This repository provides the official implementations and experiments for <a href="https://ai.meta.com/research/publications/large-concept-models-language-modeling-in-a-sentence-representation-space/" rel="nofollow">Large Concept Models</a> (<strong>LCM</strong>).</p>
<p dir="auto">
<a target="_blank" rel="noopener noreferrer" href="https://github.com/facebookresearch/large_concept_model/blob/main/space.svg"><img src="https://github.com/facebookresearch/large_concept_model/raw/main/space.svg" width="50%"></a>
</p>
<p dir="auto">The LCM operates on an explicit higher-level semantic representation,
which we name a "concept". Concepts are language- and modality-agnostic and represent a higher
level idea. In this work, a concept corresponds to a sentence, and we use the <a href="https://github.com/facebookresearch/SONAR">SONAR</a>
embedding space, which supports up to 200 languages in text and 57 languages in speech. See the list of supported languages <a href="https://github.com/facebookresearch/SONAR?tab=readme-ov-file#supported-languages-and-download-links">here</a>.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Approach</h2><a id="user-content-approach" aria-label="Permalink: Approach" href="#approach"></a></p>
<p dir="auto">
<a target="_blank" rel="noopener noreferrer" href="https://github.com/facebookresearch/large_concept_model/blob/main/lcm.svg"><img src="https://github.com/facebookresearch/large_concept_model/raw/main/lcm.svg" width="70%"></a>
</p>
<p dir="auto">The LCM is a sequence-to-sequence model in the concepts space trained to perform auto-regressive sentence prediction.
We explore multiple approaches:</p>
<ul dir="auto">
<li>MSE regression (<code>base_lcm</code> in this code).</li>
<li>Variants of diffusion-based generation (we include <code>two_tower_diffusion_lcm</code> in this release).</li>
<li>Models operating in a quantized SONAR space (coming soon).</li>
</ul>
<p dir="auto">These explorations are performed using 1.6B parameter models and training data in the order of 1.3T tokens. We include in this repository recipes to reproduce the training and finetuning of 1.6B MSE LCM and Two-tower diffusion LCM. See instructions <a href="#usage">below</a>.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Installing</h2><a id="user-content-installing" aria-label="Permalink: Installing" href="#installing"></a></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Using UV</h3><a id="user-content-using-uv" aria-label="Permalink: Using UV" href="#using-uv"></a></p>
<p dir="auto">The LCM repository relies on fairseq2. If you have <code>uv</code> installed on your system, you can install a virtual environment with all the necessary packages by running the following commands:</p>
<div dir="auto" data-snippet-clipboard-copy-content="uv sync --extra cpu --extra eval --extra data"><pre>uv sync --extra cpu --extra <span>eval</span> --extra data</pre></div>
<p dir="auto">You can also use <code>uv run</code> to run the demo commands with the correct environment.</p>
<p dir="auto">Note that we only provide requirements for <code>cpu</code> dependencies, if you want to use GPU support, you will have to choose the variants of torch and fairseq2 that work for your system.
For example for torch 2.5.1 with cuda 1.21, You would do something like:</p>
<div data-snippet-clipboard-copy-content="uv pip install torch==2.5.1 --extra-index-url https://download.pytorch.org/whl/cu121 --upgrade
uv pip install fairseq2==v0.3.0rc1 --pre --extra-index-url  https://fair.pkg.atmeta.com/fairseq2/whl/rc/pt2.5.1/cu121 --upgrade"><pre><code>uv pip install torch==2.5.1 --extra-index-url https://download.pytorch.org/whl/cu121 --upgrade
uv pip install fairseq2==v0.3.0rc1 --pre --extra-index-url  https://fair.pkg.atmeta.com/fairseq2/whl/rc/pt2.5.1/cu121 --upgrade
</code></pre></div>
<p dir="auto">Check <a href="https://github.com/facebookresearch/fairseq2?tab=readme-ov-file#variants">fairseq2 variants</a> for possible variants. Note that LCM currently relies on the release candidate for fairseq2 0.3.0 rc1.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Using pip</h3><a id="user-content-using-pip" aria-label="Permalink: Using pip" href="#using-pip"></a></p>
<p dir="auto">To install with pip, the commands are very similar, but you will have to manage your own environment and make sure to install fairseq2 manually first. For instance, for a <code>cpu</code> install.</p>
<div dir="auto" data-snippet-clipboard-copy-content="pip install --upgrade pip
pip install fairseq2==v0.3.0rc1 --pre --extra-index-url  https://fair.pkg.atmeta.com/fairseq2/whl/rc/pt2.5.1/cpu
pip install -e &quot;.[data,eval]&quot;"><pre>pip install --upgrade pip
pip install fairseq2==v0.3.0rc1 --pre --extra-index-url  https://fair.pkg.atmeta.com/fairseq2/whl/rc/pt2.5.1/cpu
pip install -e <span><span>"</span>.[data,eval]<span>"</span></span></pre></div>
<p dir="auto">If <a href="https://github.com/facebookresearch/fairseq2">fairseq2</a> does not provide a build for your machine, check the readme of that project to build it locally.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Usage</h2><a id="user-content-usage" aria-label="Permalink: Usage" href="#usage"></a></p>
<div dir="auto"><p dir="auto">Note</p><p dir="auto">If using <code>uv</code> prefix all commands with <code>uv run</code> to use the environment created by default in <code>.venv</code>, e.g.,
<code>uv run torchrun --standalone</code>.
Alternatively, you can activate the environment once and for all with <code>source .venv/bin/activate</code>.</p>
</div>
<p dir="auto"><h3 tabindex="-1" dir="auto">Preparing data</h3><a id="user-content-preparing-data" aria-label="Permalink: Preparing data" href="#preparing-data"></a></p>
<p dir="auto">The LCM can be trained and evaluated using textual data split in sentences and embedded with <a href="https://github.com/facebookresearch/SONAR/">SONAR</a>. We provide a sample processing pipeline that can be used to prepare such training data, you can run it with:</p>
<div data-snippet-clipboard-copy-content=" uv run --extra data scripts/prepare_wikipedia.py /output/dir/for/the/data"><pre><code> uv run --extra data scripts/prepare_wikipedia.py /output/dir/for/the/data
</code></pre></div>
<p dir="auto">This pipeline shows how to get a dataset from huggingface and process it with SONAR and <a href="https://arxiv.org/abs/2406.16678" rel="nofollow">SaT</a>. Check out the file for more details on processing your own data. While the script provides an example pulling data from huggingface, we also provide <a href="https://github.com/facebookresearch/stopes/tree/main/stopes/utils/sharding">APIs</a> to process jsonl, parquet and CSV.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Datacards</h3><a id="user-content-datacards" aria-label="Permalink: Datacards" href="#datacards"></a></p>
<p dir="auto">The trainer described below relies on datacards configuring the datasets. These datacards are yaml files with pointers to the dataset files (locally or on s3) and information on its schema. We provide some sample datacards in <a href="https://github.com/facebookresearch/large_concept_model/blob/main/lcm/datacards/datacards.yaml"><code>lcm/datacards/datacards.yaml</code></a>. Once you have processed some data, you can update the datacards with your paths.</p>
<p dir="auto"><h4 tabindex="-1" dir="auto">Fitting a normalizer</h4><a id="user-content-fitting-a-normalizer" aria-label="Permalink: Fitting a normalizer" href="#fitting-a-normalizer"></a></p>
<p dir="auto">To fit a new embedding space normalizer on a given weighted mixture of datasets
one can use the following command :</p>
<div dir="auto" data-snippet-clipboard-copy-content="python scripts/fit_embedding_normalizer.py --ds dataset1:4 dataset2:1 dataset3:10 --save_path &quot;path/to/new/normalizer.pt&quot; --max_nb_samples 1000000"><pre>python scripts/fit_embedding_normalizer.py --ds dataset1:4 dataset2:1 dataset3:10 --save_path <span><span>"</span>path/to/new/normalizer.pt<span>"</span></span> --max_nb_samples 1000000</pre></div>
<p dir="auto">Here, <code>dataset1</code>, <code>dataset2</code>, <code>dataset3</code> are the names of datasets declared in the datacards as shown above
and <code>(4, 1, 10)</code> their respective relative weights.
The resulting normalizer can be next declared as a model as shown in <code>lcm/cards/sonar_normalizer.yaml</code>
and referenced in all model training configs.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Pre-training models</h3><a id="user-content-pre-training-models" aria-label="Permalink: Pre-training models" href="#pre-training-models"></a></p>
<p dir="auto"><h4 tabindex="-1" dir="auto">Base MSE LCM</h4><a id="user-content-base-mse-lcm" aria-label="Permalink: Base MSE LCM" href="#base-mse-lcm"></a></p>
<p dir="auto">To train an MSE LCM, we will use one of the following commands:</p>
<p dir="auto"><strong>Option 1.</strong> Training with SLURM using <a href="https://github.com/facebookincubator/submitit">submitit</a> via <a href="https://github.com/facebookresearch/stopes/tree/main">stopes</a>'s launcher:</p>
<div dir="auto" data-snippet-clipboard-copy-content="python -m lcm.train \
    +pretrain=mse \
    ++trainer.output_dir=&quot;checkpoints/mse_lcm&quot; \
    ++trainer.experiment_name=training_mse_lcm \"><pre>python -m lcm.train \
    +pretrain=mse \
    ++trainer.output_dir=<span><span>"</span>checkpoints/mse_lcm<span>"</span></span> \
    ++trainer.experiment_name=training_mse_lcm \</pre></div>
<p dir="auto">With this command, we will submit a slurm job named <code>training_mse_lcm</code> with the recipe's requirements, in this case:</p>
<div dir="auto" data-snippet-clipboard-copy-content="requirements:
  nodes: 4
  tasks_per_node: 8
  gpus_per_node: 8
  cpus_per_task: 32
  mem_gb: 0
  timeout_min: 10000"><pre><span>requirements</span>:
  <span>nodes</span>: <span>4</span>
  <span>tasks_per_node</span>: <span>8</span>
  <span>gpus_per_node</span>: <span>8</span>
  <span>cpus_per_task</span>: <span>32</span>
  <span>mem_gb</span>: <span>0</span>
  <span>timeout_min</span>: <span>10000</span></pre></div>
<p dir="auto">You can override the job's requirements like the timeout limit and the launcher's slurm partition with:</p>
<div dir="auto" data-snippet-clipboard-copy-content="python -m lcm.train \
    +pretrain=mse \
    ++trainer.output_dir=&quot;checkpoints/mse_lcm&quot; \
    ++trainer.experiment_name=training_mse_lcm \
    ++trainer.requirements.timeout_min=100 \
    ++trainer.requirements.cpus_per_task=8 \
    ++launcher.partition=$partition_name"><pre>python -m lcm.train \
    +pretrain=mse \
    ++trainer.output_dir=<span><span>"</span>checkpoints/mse_lcm<span>"</span></span> \
    ++trainer.experiment_name=training_mse_lcm \
    ++trainer.requirements.timeout_min=100 \
    ++trainer.requirements.cpus_per_task=8 \
    ++launcher.partition=<span>$partition_name</span></pre></div>
<p dir="auto"><strong>Option 2.</strong> Training locally with <code>torchrun</code> (e.g. using only 2 GPUs) with a smaller batch size (overriding <code>++trainer.data_loading_config.max_tokens=1000</code>):</p>
<div dir="auto" data-snippet-clipboard-copy-content="CUDA_VISIBLE_DEVICES=0,1 torchrun --standalone --nnodes=1 --nproc-per-node=2 \
    -m lcm.train launcher=standalone \
    +pretrain=mse \
    ++trainer.data_loading_config.max_tokens=1000 \
    ++trainer.output_dir=&quot;checkpoints/mse_lcm&quot; \
    +trainer.use_submitit=false \"><pre>CUDA_VISIBLE_DEVICES=0,1 torchrun --standalone --nnodes=1 --nproc-per-node=2 \
    -m lcm.train launcher=standalone \
    +pretrain=mse \
    ++trainer.data_loading_config.max_tokens=1000 \
    ++trainer.output_dir=<span><span>"</span>checkpoints/mse_lcm<span>"</span></span> \
    +trainer.use_submitit=false \</pre></div>
<div dir="auto"><p dir="auto">Important</p><p dir="auto">Since we're changing the number of GPUs required by the recipe, this will not reproduce the experimental setup of the paper.</p>
</div>
<p dir="auto">The checkpoints directory <code>checkpoints/mse_lcm</code> will be structured as:</p>
<div data-snippet-clipboard-copy-content=".
├── checkpoints
│&nbsp;&nbsp; ├── step_2000
│&nbsp;&nbsp; ├── ...
│&nbsp;&nbsp; └── step_250000
├── config_logs
├── executor_logs
├── model_card.yaml
├── tb   # tensorboard logs
└── wandb  # W&amp;B logs"><pre><code>.
├── checkpoints
│&nbsp;&nbsp; ├── step_2000
│&nbsp;&nbsp; ├── ...
│&nbsp;&nbsp; └── step_250000
├── config_logs
├── executor_logs
├── model_card.yaml
├── tb   # tensorboard logs
└── wandb  # W&amp;B logs
</code></pre></div>
<p dir="auto">Note that W&amp;B logging is skipped unless <code>wandb</code> is available.
You can install <code>wandb</code> with <code>uv pip install wandb</code>.
W&amp;B arguments can be changed by overriding Hydra config values in the recipe:</p>
<div dir="auto" data-snippet-clipboard-copy-content="++trainer.wandb_project=$project_name
++trainer.wandb_run_name=$run_name"><pre>++trainer.wandb_project=<span>$project_name</span>
++trainer.wandb_run_name=<span>$run_name</span></pre></div>
<p dir="auto"><h4 tabindex="-1" dir="auto">Two-tower diffusion LCM</h4><a id="user-content-two-tower-diffusion-lcm" aria-label="Permalink: Two-tower diffusion LCM" href="#two-tower-diffusion-lcm"></a></p>
<p dir="auto">Similar to the base MSE LCM we can submit a training job following the recipe in <a href="https://github.com/facebookresearch/large_concept_model/blob/main/recipes/train/pretrain/two_tower.yaml">./recipes/train/pretrain/two_tower.yaml</a> via:</p>
<div dir="auto" data-snippet-clipboard-copy-content="python -m lcm.train \
    +pretrain=two_tower \
    ++trainer.output_dir=&quot;checkpoints/two_tower_lcm&quot; \
    ++trainer.experiment_name=training_two_tower_lcm \"><pre>python -m lcm.train \
    +pretrain=two_tower \
    ++trainer.output_dir=<span><span>"</span>checkpoints/two_tower_lcm<span>"</span></span> \
    ++trainer.experiment_name=training_two_tower_lcm \</pre></div>
<div dir="auto"><p dir="auto">Tip</p><p dir="auto">To understand the different ingredients of training recipes, check <a href="https://github.com/facebookresearch/large_concept_model/blob/main/recipes/train/README.md">this README</a>.</p>
</div>
<p dir="auto"><h3 tabindex="-1" dir="auto">Finetuning models</h3><a id="user-content-finetuning-models" aria-label="Permalink: Finetuning models" href="#finetuning-models"></a></p>
<p dir="auto">To finetune the previously pre-trained two-tower diffusion LCM on supervised data,  follow these steps:</p>
<p dir="auto"><strong>Step 1.</strong> Register the pre-trained checkpoint as a fairseq2 asset.</p>
<p dir="auto">You can finetune the final checkpoint with the card <code>checkpoints/two_tower_lcm/model_card.yaml</code> or any checkpoint after a specific number of training steps, e.g., <code>checkpoints/two_tower_lcm/checkpoints/step_2000/model_card.yaml</code>.
To register the selected checkpoint, copy the automatically created yaml file to <code>./lcm/cards/mycards.yaml</code> and rename the model to replace the default <code>on_the_fly_lcm</code>.
<code>./lcm/cards/mycards.yaml</code> will look like:</p>
<div dir="auto" data-snippet-clipboard-copy-content="__source__: inproc
 checkpoint: file://path_to/large_concept_model/checkpoints/two_tower_lcm/checkpoints/step_2000/model.pt
 model_arch: two_tower_diffusion_lcm_1_6B
 model_family: two_tower_diffusion_lcm
 name: my_pretrained_two_tower"><pre><span>__source__</span>: <span>inproc</span>
 <span>checkpoint</span>: <span>file://path_to/large_concept_model/checkpoints/two_tower_lcm/checkpoints/step_2000/model.pt</span>
 <span>model_arch</span>: <span>two_tower_diffusion_lcm_1_6B</span>
 <span>model_family</span>: <span>two_tower_diffusion_lcm</span>
 <span>name</span>: <span>my_pretrained_two_tower</span></pre></div>
<p dir="auto">For more on how to manage fairseq2 assets, see <a href="https://facebookresearch.github.io/fairseq2/nightly/basics/assets.html" rel="nofollow">documentation</a>.</p>
<p dir="auto"><strong>Step 2.</strong> Launch a finetuning job pointing to the model to finetune, in this instance <code>my_pretrained_two_tower</code>:</p>
<div dir="auto" data-snippet-clipboard-copy-content="CUDA_VISIBLE_DEVICES=0,1 torchrun --standalone --nnodes=1 --nproc-per-node=2 \
    -m lcm.train launcher=standalone \
    +finetune=two_tower \
    ++trainer.output_dir=&quot;checkpoints/finetune_two_tower_lcm&quot; \
    ++trainer.data_loading_config.max_tokens=1000 \
    +trainer.use_submitit=false \
    ++trainer.model_config_or_name=my_pretrained_two_tower"><pre>CUDA_VISIBLE_DEVICES=0,1 torchrun --standalone --nnodes=1 --nproc-per-node=2 \
    -m lcm.train launcher=standalone \
    +finetune=two_tower \
    ++trainer.output_dir=<span><span>"</span>checkpoints/finetune_two_tower_lcm<span>"</span></span> \
    ++trainer.data_loading_config.max_tokens=1000 \
    +trainer.use_submitit=false \
    ++trainer.model_config_or_name=my_pretrained_two_tower</pre></div>
<p dir="auto">or</p>
<div dir="auto" data-snippet-clipboard-copy-content="python -m lcm.train \
    +finetune=two_tower \
    ++trainer.output_dir=&quot;checkpoints/finetune_two_tower_lcm&quot; \
    ++trainer.experiment_name=finetune_two_tower_lcm \
    ++trainer.model_config_or_name=my_pretrained_two_tower"><pre>python -m lcm.train \
    +finetune=two_tower \
    ++trainer.output_dir=<span><span>"</span>checkpoints/finetune_two_tower_lcm<span>"</span></span> \
    ++trainer.experiment_name=finetune_two_tower_lcm \
    ++trainer.model_config_or_name=my_pretrained_two_tower</pre></div>
<p dir="auto">Similarly, to finetune an MSE LCM, follow the same instructions for registering a pre-trained checkpoint and submit a finetuning job with the appropriate recipe (<a href="https://github.com/facebookresearch/large_concept_model/blob/main/recipes/train/finetune/mse.yaml">./recipes/train/finetune/mse.yaml</a>) via:</p>
<div dir="auto" data-snippet-clipboard-copy-content="python -m lcm.train \
    +finetune=mse \
    ++trainer.output_dir=&quot;checkpoints/finetune_mse_lcm&quot; \
    ++trainer.experiment_name=finetune_mse_lcm \
    ++trainer.model_config_or_name=my_pretrained_mse_lcm"><pre>python -m lcm.train \
    +finetune=mse \
    ++trainer.output_dir=<span><span>"</span>checkpoints/finetune_mse_lcm<span>"</span></span> \
    ++trainer.experiment_name=finetune_mse_lcm \
    ++trainer.model_config_or_name=my_pretrained_mse_lcm</pre></div>
<p dir="auto"><h3 tabindex="-1" dir="auto">Evaluating models</h3><a id="user-content-evaluating-models" aria-label="Permalink: Evaluating models" href="#evaluating-models"></a></p>

<p dir="auto"><strong>Step 0.</strong> Download NLTK data required for evaluating ROUGE:</p>
<div dir="auto" data-snippet-clipboard-copy-content="python -m nltk.downloader punkt_tab"><pre><span>python</span> <span>-</span><span>m</span> <span>nltk</span>.<span>downloader</span> <span>punkt_tab</span></pre></div>
<p dir="auto"><strong>Step 1.</strong>
Generate and score outputs of a model either by pointing to its <code>model_card</code> yaml file or after registering it as a fairseq2 asset (the same way we registerd <code>my_pretrained_two_tower</code>):</p>
<div dir="auto" data-snippet-clipboard-copy-content="model_card=./checkpoints/finetune_two_tower_lcm/checkpoints/step_1000/model_card.yaml
OUTPUT_DIR=evaluation_outputs/two_tower

torchrun --standalone --nnodes=1 --nproc-per-node=1 -m lcm.evaluation  \
  --predictor two_tower_diffusion_lcm  \
  --show_progress true \
  --data_loading.max_samples 100 \
  --model_card ${model_card} \
  --launcher standalone \
  --dataset.source_suffix_text '[MODEL]:' \
  --tasks finetuning_data_lcm.validation \
   --task_args '{&quot;max_gen_len&quot;: 10, &quot;eos_config&quot;: {&quot;text&quot;: &quot;End of text.&quot;}}' \
  --data_loading.batch_size 4  --generator_batch_size 4 \
  --dump_dir ${OUTPUT_DIR} \
  --inference_timesteps 40 \
  --initial_noise_scale 0.6 \
  --guidance_scale 3 \
  --guidance_rescale 0.7"><pre>model_card=./checkpoints/finetune_two_tower_lcm/checkpoints/step_1000/model_card.yaml
OUTPUT_DIR=evaluation_outputs/two_tower

torchrun --standalone --nnodes=1 --nproc-per-node=1 -m lcm.evaluation  \
  --predictor two_tower_diffusion_lcm  \
  --show_progress <span>true</span> \
  --data_loading.max_samples 100 \
  --model_card <span>${model_card}</span> \
  --launcher standalone \
  --dataset.source_suffix_text <span><span>'</span>[MODEL]:<span>'</span></span> \
  --tasks finetuning_data_lcm.validation \
   --task_args <span><span>'</span>{"max_gen_len": 10, "eos_config": {"text": "End of text."}}<span>'</span></span> \
  --data_loading.batch_size 4  --generator_batch_size 4 \
  --dump_dir <span>${OUTPUT_DIR}</span> \
  --inference_timesteps 40 \
  --initial_noise_scale 0.6 \
  --guidance_scale 3 \
  --guidance_rescale 0.7</pre></div>
<p dir="auto">where in the example we are evaluating 100 samples only (<code>--data_loading.max_samples 100</code>) and limiting the model output length to 10 sentences (<code>--task_args '{"max_gen_len": 10}'</code>).</p>
<p dir="auto">Outputs dumped in <code>./evaluation_outputs/two_tower</code> will be structured as:</p>
<div data-snippet-clipboard-copy-content=".
├── metadata.jsonl
├── metrics.eval.jsonl
├── raw_results
├── results
└── tb"><pre><code>.
├── metadata.jsonl
├── metrics.eval.jsonl
├── raw_results
├── results
└── tb
</code></pre></div>
<p dir="auto">where <code>metrics.eval.jsonl</code> contains corpus-level scores.</p>
<p dir="auto">To evaluate an MSE LCM, we use the associated predictor (<code>base_lcm</code>) and evaluate with:</p>
<div dir="auto" data-snippet-clipboard-copy-content="model_card=./checkpoints/finetune_mse_lcm/checkpoints/step_1000/model_card.yaml
OUTPUT_DIR=evaluation_outputs/mse_lcm

torchrun --standalone --nnodes=1 --nproc-per-node=1 -m lcm.evaluation  \
  --predictor base_lcm --sample_latent_variable False \
  --show_progress true \
  --data_loading.max_samples 100 \
  --model_card ${model_card} \
  --launcher standalone \
  --dataset.source_suffix_text '[MODEL]:' \
  --tasks finetuning_data_lcm.validation \
   --task_args '{&quot;max_gen_len&quot;: 10, &quot;eos_config&quot;: {&quot;text&quot;: &quot;End of text.&quot;}}' \
  --data_loading.batch_size 4  --generator_batch_size 4 \
  --dump_dir ${OUTPUT_DIR} \"><pre>model_card=./checkpoints/finetune_mse_lcm/checkpoints/step_1000/model_card.yaml
OUTPUT_DIR=evaluation_outputs/mse_lcm

torchrun --standalone --nnodes=1 --nproc-per-node=1 -m lcm.evaluation  \
  --predictor base_lcm --sample_latent_variable False \
  --show_progress <span>true</span> \
  --data_loading.max_samples 100 \
  --model_card <span>${model_card}</span> \
  --launcher standalone \
  --dataset.source_suffix_text <span><span>'</span>[MODEL]:<span>'</span></span> \
  --tasks finetuning_data_lcm.validation \
   --task_args <span><span>'</span>{"max_gen_len": 10, "eos_config": {"text": "End of text."}}<span>'</span></span> \
  --data_loading.batch_size 4  --generator_batch_size 4 \
  --dump_dir <span>${OUTPUT_DIR}</span> \</pre></div>
<p dir="auto">Note that in this example, we only show how to evaluate the LCM on the same finetuning dataset (validation split). To evaluate in a downstream task, and compare results with the LLM, refer to the <a href="https://github.com/facebookresearch/large_concept_model/blob/main/examples/evaluation/README.md">Evaluation documentation</a>.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Contributing</h2><a id="user-content-contributing" aria-label="Permalink: Contributing" href="#contributing"></a></p>
<p dir="auto">See the <a href="https://github.com/facebookresearch/large_concept_model/blob/main/CONTRIBUTING.md">CONTRIBUTING</a> file for how to help out.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Citation</h2><a id="user-content-citation" aria-label="Permalink: Citation" href="#citation"></a></p>
<p dir="auto">If you use this codebase, please cite:</p>
<div data-snippet-clipboard-copy-content="@article{lcm2024,
  author = {{LCM team}, Lo\&quot;{i}c Barrault, Paul-Ambroise Duquenne, Maha Elbayad, Artyom Kozhevnikov, Belen Alastruey, Pierre Andrews, Mariano Coria, Guillaume Couairon, Marta R. Costa-juss\`{a}, David Dale, Hady Elsahar, Kevin Heffernan, Jo\~{a}o Maria Janeiro, Tuan Tran, Christophe Ropers, Eduardo Sánchez, Robin San Roman, Alexandre Mourachko, Safiyyah Saleem, Holger Schwenk},
  title = {{Large Concept Models}: Language Modeling in a Sentence Representation Space},
  publisher = {arXiv},
  year = {2024},
  url = {https://arxiv.org/abs/2412.08821},
}"><pre><code>@article{lcm2024,
  author = {{LCM team}, Lo\"{i}c Barrault, Paul-Ambroise Duquenne, Maha Elbayad, Artyom Kozhevnikov, Belen Alastruey, Pierre Andrews, Mariano Coria, Guillaume Couairon, Marta R. Costa-juss\`{a}, David Dale, Hady Elsahar, Kevin Heffernan, Jo\~{a}o Maria Janeiro, Tuan Tran, Christophe Ropers, Eduardo Sánchez, Robin San Roman, Alexandre Mourachko, Safiyyah Saleem, Holger Schwenk},
  title = {{Large Concept Models}: Language Modeling in a Sentence Representation Space},
  publisher = {arXiv},
  year = {2024},
  url = {https://arxiv.org/abs/2412.08821},
}
</code></pre></div>
<p dir="auto"><h2 tabindex="-1" dir="auto">License</h2><a id="user-content-license" aria-label="Permalink: License" href="#license"></a></p>
<p dir="auto">This code is released under the MIT license (see <a href="https://github.com/facebookresearch/large_concept_model/blob/main/LICENSE">LICENSE</a>).</p>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Static search trees: faster than binary search (550 pts)]]></title>
            <link>https://curiouscoding.nl/posts/static-search-tree/</link>
            <guid>42562847</guid>
            <pubDate>Wed, 01 Jan 2025 00:08:03 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://curiouscoding.nl/posts/static-search-tree/">https://curiouscoding.nl/posts/static-search-tree/</a>, See on <a href="https://news.ycombinator.com/item?id=42562847">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><div><p>Table of Contents</p><ul><li><span>1</span> <a href="#introduction">Introduction</a><ul><li><span>1.1</span> <a href="#problem-statement">Problem statement</a></li><li><span>1.2</span> <a href="#recommended-reading">Recommended reading</a></li><li><span>1.3</span> <a href="#binary-search-and-eytzinger-layout">Binary search and Eytzinger layout</a></li><li><span>1.4</span> <a href="#hugepages">Hugepages</a></li><li><span>1.5</span> <a href="#a-note-on-benchmarking">A note on benchmarking</a></li><li><span>1.6</span> <a href="#cache-lines">Cache lines</a></li><li><span>1.7</span> <a href="#s-trees-and-b-trees">S-trees and B-trees</a></li></ul></li><li><span>2</span> <a href="#optimizing-find">Optimizing <code>find</code></a><ul><li><span>2.1</span> <a href="#linear">Linear</a></li><li><span>2.2</span> <a href="#auto-vectorization">Auto-vectorization</a></li><li><span>2.3</span> <a href="#trailing-zeros">Trailing zeros</a></li><li><span>2.4</span> <a href="#popcount">Popcount</a></li><li><span>2.5</span> <a href="#manual-simd">Manual SIMD</a></li></ul></li><li><span>3</span> <a href="#optimizing-the-search">Optimizing the search</a><ul><li><span>3.1</span> <a href="#batching">Batching</a></li><li><span>3.2</span> <a href="#prefetching">Prefetching</a></li><li><span>3.3</span> <a href="#pointer-arithmetic">Pointer arithmetic</a><ul><li><span>3.3.1</span> <a href="#up-front-splat">Up-front splat</a></li><li><span>3.3.2</span> <a href="#byte-based-pointers">Byte-based pointers</a></li><li><span>3.3.3</span> <a href="#the-final-version">The final version</a></li></ul></li><li><span>3.4</span> <a href="#skip-prefetch">Skip prefetch</a></li><li><span>3.5</span> <a href="#interleave">Interleave</a></li></ul></li><li><span>4</span> <a href="#optimizing-the-tree-layout">Optimizing the tree layout</a><ul><li><span>4.1</span> <a href="#left-tree">Left-tree</a></li><li><span>4.2</span> <a href="#memory-layouts">Memory layouts</a></li><li><span>4.3</span> <a href="#node-size-b-15">Node size \(B=15\)</a><ul><li><span>4.3.1</span> <a href="#data-structure-size">Data structure size</a></li></ul></li><li><span>4.4</span> <a href="#summary">Summary</a></li></ul></li><li><span>5</span> <a href="#prefix-partitioning">Prefix partitioning</a><ul><li><span>5.1</span> <a href="#full-layout">Full layout</a></li><li><span>5.2</span> <a href="#compact-subtrees">Compact subtrees</a></li><li><span>5.3</span> <a href="#the-best-of-both-compact-first-level">The best of both: compact first level</a></li><li><span>5.4</span> <a href="#overlapping-trees">Overlapping trees</a></li><li><span>5.5</span> <a href="#human-data">Human data</a></li><li><span>5.6</span> <a href="#prefix-map">Prefix map</a></li><li><span>5.7</span> <a href="#prefix-summary">Summary</a></li></ul></li><li><span>6</span> <a href="#multi-threaded-comparison">Multi-threaded comparison</a></li><li><span>7</span> <a href="#conclusion">Conclusion</a><ul><li><span>7.1</span> <a href="#future-work">Future work</a><ul><li><span>7.1.1</span> <a href="#branchy-search">Branchy search</a></li><li><span>7.1.2</span> <a href="#interpolation-search">Interpolation search</a></li><li><span>7.1.3</span> <a href="#packing-data-smaller">Packing data smaller</a></li><li><span>7.1.4</span> <a href="#returning-indices-in-original-data">Returning indices in original data</a></li><li><span>7.1.5</span> <a href="#range-queries">Range queries</a></li></ul></li></ul></li></ul></div><p>In this post, we will implement a static search tree (S+ tree) for
high-throughput searching of sorted data, as <a href="https://en.algorithmica.org/hpc/data-structures/s-tree/">introduced</a> on Algorithmica.
We’ll mostly take the code presented there as a starting point, and optimize it
to its limits. For a large part, I’m simply taking the ‘future work’ ideas of that post
and implementing them. And then there will be a bunch of looking at assembly
code to shave off all the instructions we can.
Lastly, there will be one big addition to optimize throughput: <em>batching</em>.</p><p>All <strong>source code</strong>, including benchmarks and plotting code, is at <a href="https://github.com/RagnarGrootKoerkamp/suffix-array-searching/tree/master/static-search-tree">github:RagnarGrootKoerkamp/suffix-array-searching</a>.</p><h2 id="introduction"><span>1</span> Introduction
<a href="#introduction"></a></h2><h2 id="problem-statement"><span>1.1</span> Problem statement
<a href="#problem-statement"></a></h2><p><strong>Input.</strong> A sorted list of \(n\) 32bit unsigned integers <code>vals: Vec&lt;u32&gt;</code>.</p><p><strong>Output.</strong> A data structure that supports queries \(q\), returning the smallest
element of <code>vals</code> that is at least \(q\), or <code>u32::MAX</code> if no such element exists.
Optionally, the index of this element may also be returned.</p><p><strong>Metric.</strong> We optimize <em>throughput</em>. That is, the number of (independent) queries
that can be answered per second. The typical case is where we have a
sufficiently long <code>queries: &amp;[u32]</code> as input, and return a corresponding <code>answers: Vec&lt;u32&gt;</code>.</p><p>Note that we’ll usually report reciprocal throughput as <code>ns/query</code> (or just
<code>ns</code>), instead of <code>queries/s</code>. You can think of this as amortized (not <em>average</em>) time spent per query.</p><p><strong>Benchmarking setup.</strong> For now, we will assume that both the input and queries
are simply uniform random sampled 31bit integers<sup id="fnref:1"><a href="#fn:1" role="doc-noteref">1</a></sup>.</p><p><strong>Code.</strong>
In code, this can be modelled by the trait shown in <a href="#code-snippet--trait">Code Snippet 1</a>.</p><div><table><tbody><tr><td><pre tabindex="0"><code><span>1
</span><span>2
</span><span>3
</span><span>4
</span><span>5
</span><span>6
</span><span>7
</span><span>8
</span><span>9
</span></code></pre></td><td><pre tabindex="0"><code data-lang="rust"><span><span><span>trait</span><span> </span><span>SearchIndex</span><span> </span><span>{</span><span>
</span></span></span><span><span><span>    </span><span>/// Two functions with default implementations in terms of each other.
</span></span></span><span><span><span></span><span>    </span><span>fn</span> <span>query_one</span><span>(</span><span>&amp;</span><span>self</span><span>,</span><span> </span><span>query</span>: <span>u32</span><span>)</span><span> </span>-&gt; <span>u32</span> <span>{</span><span>
</span></span></span><span><span><span>        </span><span>Self</span>::<span>query</span><span>(</span><span>&amp;</span><span>vec!</span><span>[</span><span>query</span><span>])[</span><span>0</span><span>]</span><span>
</span></span></span><span><span><span>    </span><span>}</span><span>
</span></span></span><span><span><span>    </span><span>fn</span> <span>query</span><span>(</span><span>&amp;</span><span>self</span><span>,</span><span> </span><span>queries</span>: <span>&amp;</span><span>[</span><span>u32</span><span>])</span><span> </span>-&gt; <span>Vec</span><span>&lt;</span><span>u32</span><span>&gt;</span><span> </span><span>{</span><span>
</span></span></span><span><span><span>        </span><span>queries</span><span>.</span><span>iter</span><span>().</span><span>map</span><span>(</span><span>|&amp;</span><span>q</span><span>|</span><span> </span><span>Self</span>::<span>query_one</span><span>(</span><span>q</span><span>)).</span><span>collect</span><span>()</span><span>
</span></span></span><span><span><span>    </span><span>}</span><span>
</span></span></span><span><span><span></span><span>}</span><span>
</span></span></span></code></pre></td></tr></tbody></table></div><p><span><a href="#code-snippet--trait">Code Snippet 1</a>:</span>
Trait that our solution should implement.</p><h2 id="recommended-reading"><span>1.2</span> Recommended reading
<a href="#recommended-reading"></a></h2><p>The classical solution to this problem is <strong>binary search</strong>, which we will briefly
visit in the next section. A great paper on this and other search layouts is
<a href="#citeproc_bib_item_2">“Array Layouts for Comparison-Based Searching”</a> by Khuong and Morin (<a href="#citeproc_bib_item_2">2017</a>).
Algorithmica also has a <a href="https://en.algorithmica.org/hpc/data-structures/binary-search/">case study</a> based on that paper.</p><p>This post will focus on <strong>S+ trees</strong>, as introduced on Algorithmica in the
followup post, <a href="https://en.algorithmica.org/hpc/data-structures/s-tree/">static B-trees</a>. In the interest of my time, I will mostly assume
that you are familiar with that post.</p><p>I also recommend reading my work-in-progress <a href="https://curiouscoding.nl/posts/cpu-benchmarks">introduction to CPU performance</a>,
which contains some benchmarks pushing the CPU to its limits. We will use the
metrics obtained there as baseline to understand our optimization attempts.</p><p>Also helpful is the <a href="https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#=undefined&amp;techs=AVX_ALL">Intel Intrinsics Guide</a> when looking into SIMD instructions.
Note that we’ll only be using <code>AVX2</code> instructions here, as in, we’re assuming
intel. And we’re not assuming less available <code>AVX512</code> instructions (in
particular, since my laptop doesn’t have them).</p><h2 id="binary-search-and-eytzinger-layout"><span>1.3</span> Binary search and Eytzinger layout
<a href="#binary-search-and-eytzinger-layout"></a></h2><p>As a baseline, we will use the Rust standard library binary search implementation.</p><div><table><tbody><tr><td><pre tabindex="0"><code><span> 1
</span><span> 2
</span><span> 3
</span><span> 4
</span><span> 5
</span><span> 6
</span><span> 7
</span><span> 8
</span><span> 9
</span><span>10
</span></code></pre></td><td><pre tabindex="0"><code data-lang="rust"><span><span><span>pub</span><span> </span><span>struct</span> <span>SortedVec</span><span> </span><span>{</span><span>
</span></span></span><span><span><span>    </span><span>vals</span>: <span>Vec</span><span>&lt;</span><span>u32</span><span>&gt;</span><span>,</span><span>
</span></span></span><span><span><span></span><span>}</span><span>
</span></span></span><span><span><span>
</span></span></span><span><span><span></span><span>impl</span><span> </span><span>SortedVec</span><span> </span><span>{</span><span>
</span></span></span><span><span><span>    </span><span>pub</span><span> </span><span>fn</span> <span>binary_search_std</span><span>(</span><span>&amp;</span><span>self</span><span>,</span><span> </span><span>q</span>: <span>u32</span><span>)</span><span> </span>-&gt; <span>u32</span> <span>{</span><span>
</span></span></span><span><span><span>        </span><span>let</span><span> </span><span>idx</span><span> </span><span>=</span><span> </span><span>self</span><span>.</span><span>vals</span><span>.</span><span>binary_search</span><span>(</span><span>&amp;</span><span>q</span><span>).</span><span>unwrap_or_else</span><span>(</span><span>|</span><span>i</span><span>|</span><span> </span><span>i</span><span>);</span><span>
</span></span></span><span><span><span>        </span><span>self</span><span>.</span><span>vals</span><span>[</span><span>idx</span><span>]</span><span>
</span></span></span><span><span><span>    </span><span>}</span><span>
</span></span></span><span><span><span></span><span>}</span><span>
</span></span></span></code></pre></td></tr></tbody></table></div><p><span><a href="#code-snippet--binary-search">Code Snippet 2</a>:</span>
The binary search in the Rust standard library.</p><p>The main conclusion of the array layouts paper (<a href="#citeproc_bib_item_2">Khuong and Morin 2017</a>) is
that the Eytzinger layout is one of the best in practice.
This layout reorders the values in memory: the binary search effectively is a
binary search tree on the data, the root the middle node, then the nodes at
positions \(\frac 14 n\) and \(\frac 34 n\), then \(\frac 18n, \frac 38n, \frac 58n,
\frac 78n\), and so on. The main benefit of this layout is that all values needed
for the first steps of the binary search are close together, so they can be
cached efficiently. If we put the root at index \(1\), the two children of the
node at index \(i\) are at \(2i\) and \(2i+1\). This means that we can effectively
prefetch the next cache line, before knowing whether we need index \(2i\) or
\(2i+1\). This can be taken a step further and we can prefetch the cache line
containing indices \(16i\) to \(16i+15\), which are exactly the values needed 4
iterations from now.
For a large part, this can quite effectively hide the latency associated with
the traversal of the tree.</p><div><table><tbody><tr><td><pre tabindex="0"><code><span> 1
</span><span> 2
</span><span> 3
</span><span> 4
</span><span> 5
</span><span> 6
</span><span> 7
</span><span> 8
</span><span> 9
</span><span>10
</span><span>11
</span><span>12
</span><span>13
</span><span>14
</span><span>15
</span><span>16
</span><span>17
</span><span>18
</span><span>19
</span><span>20
</span><span>21
</span><span>22
</span></code></pre></td><td><pre tabindex="0"><code data-lang="rust"><span><span><span>pub</span><span> </span><span>struct</span> <span>Eytzinger</span><span> </span><span>{</span><span>
</span></span></span><span><span><span>    </span><span>/// The root of the tree is at index 1.
</span></span></span><span><span><span></span><span>    </span><span>vals</span>: <span>Vec</span><span>&lt;</span><span>u32</span><span>&gt;</span><span>,</span><span>
</span></span></span><span><span><span></span><span>}</span><span>
</span></span></span><span><span><span>
</span></span></span><span><span><span></span><span>impl</span><span> </span><span>Eytzinger</span><span> </span><span>{</span><span>
</span></span></span><span><span><span>    </span><span>/// L: number of levels ahead to prefetch.
</span></span></span><span><span><span></span><span>    </span><span>pub</span><span> </span><span>fn</span> <span>search_prefetch</span><span>&lt;</span><span>const</span><span> </span><span>L</span>: <span>usize</span><span>&gt;</span><span>(</span><span>&amp;</span><span>self</span><span>,</span><span> </span><span>q</span>: <span>u32</span><span>)</span><span> </span>-&gt; <span>u32</span> <span>{</span><span>
</span></span></span><span><span><span>        </span><span>let</span><span> </span><span>mut</span><span> </span><span>idx</span><span> </span><span>=</span><span> </span><span>1</span><span>;</span><span>
</span></span></span><span><span><span>        </span><span>while</span><span> </span><span>(</span><span>1</span><span> </span><span>&lt;&lt;</span><span> </span><span>L</span><span>)</span><span> </span><span>*</span><span> </span><span>idx</span><span> </span><span>&lt;</span><span> </span><span>self</span><span>.</span><span>vals</span><span>.</span><span>len</span><span>()</span><span> </span><span>{</span><span>
</span></span></span><span><span><span>            </span><span>idx</span><span> </span><span>=</span><span> </span><span>2</span><span> </span><span>*</span><span> </span><span>idx</span><span> </span><span>+</span><span> </span><span>(</span><span>q</span><span> </span><span>&gt;</span><span> </span><span>self</span><span>.</span><span>get</span><span>(</span><span>idx</span><span>))</span><span> </span><span>as</span><span> </span><span>usize</span><span>;</span><span>
</span></span></span><span><span><span>            </span><span>prefetch_index</span><span>(</span><span>&amp;</span><span>self</span><span>.</span><span>vals</span><span>,</span><span> </span><span>(</span><span>1</span><span> </span><span>&lt;&lt;</span><span> </span><span>L</span><span>)</span><span> </span><span>*</span><span> </span><span>idx</span><span>);</span><span>
</span></span></span><span><span><span>        </span><span>}</span><span>
</span></span></span><span><span><span>        </span><span>// The last few iterations don't need prefetching anymore.
</span></span></span><span><span><span></span><span>        </span><span>while</span><span> </span><span>idx</span><span> </span><span>&lt;</span><span> </span><span>self</span><span>.</span><span>vals</span><span>.</span><span>len</span><span>()</span><span> </span><span>{</span><span>
</span></span></span><span><span><span>            </span><span>idx</span><span> </span><span>=</span><span> </span><span>2</span><span> </span><span>*</span><span> </span><span>idx</span><span> </span><span>+</span><span> </span><span>(</span><span>q</span><span> </span><span>&gt;</span><span> </span><span>self</span><span>.</span><span>get</span><span>(</span><span>idx</span><span>))</span><span> </span><span>as</span><span> </span><span>usize</span><span>;</span><span>
</span></span></span><span><span><span>        </span><span>}</span><span>
</span></span></span><span><span><span>        </span><span>let</span><span> </span><span>zeros</span><span> </span><span>=</span><span> </span><span>idx</span><span>.</span><span>trailing_ones</span><span>()</span><span> </span><span>+</span><span> </span><span>1</span><span>;</span><span>
</span></span></span><span><span><span>        </span><span>let</span><span> </span><span>idx</span><span> </span><span>=</span><span> </span><span>idx</span><span> </span><span>&gt;&gt;</span><span> </span><span>zeros</span><span>;</span><span>
</span></span></span><span><span><span>        </span><span>self</span><span>.</span><span>get</span><span>(</span><span>idx</span><span>)</span><span>
</span></span></span><span><span><span>    </span><span>}</span><span>
</span></span></span><span><span><span></span><span>}</span><span>
</span></span></span></code></pre></td></tr></tbody></table></div><p><span><a href="#code-snippet--eytzinger">Code Snippet 3</a>:</span>
Implementation of searching the Eytzinger layout, with \(L=4\) levels of prefetching.</p><p>If we plot these two, we see that Eytzinger layout performs as good as binary
search when the array fits in L2 cache (<code>256kB</code> for me, the middle red line), but starts to be much
better than binary search as the array grows to be much larger than the L3 cache (<code>12MB</code>).
In the end, Eytzinger search is around 4 times faster, which nicely corresponds
to being able to prefetch 4 iterations of cache lines from memory at a time.</p><figure><a href="https://curiouscoding.nl/ox-hugo/1-binary-search.svg"><img src="https://curiouscoding.nl/ox-hugo/1-binary-search.svg" alt="Figure 1: Query throughput of binary search and Eytzinger layout as the size of the input increases. At 1GB input, binary search needs around 1150ns/query, while Eytzinger is 6x faster at 200ns/query."></a><figcaption><p><span>Figure 1: </span>Query throughput of binary search and Eytzinger layout as the size of the input increases. At <code>1GB</code> input, binary search needs around <code>1150ns/query</code>, while Eytzinger is 6x faster at <code>200ns/query</code>.</p></figcaption></figure><h2 id="hugepages"><span>1.4</span> Hugepages
<a href="#hugepages"></a></h2><p>For all experiments, we’ll make sure to allocate the tree using <code>2MB</code> <em>hugepages</em>
by default, instead of the usual <code>4kB</code> pages.
This reduces pressure on the <em>translation lookaside buffer</em> (TLB) that
translates virtual memory addresses to hardware memory addresses, since its
internal table of pages is much smaller when using hugepages, and hence can be
cached better.</p><p>With <em>transparent hugepages</em> enabled, they are automatically given out whenever
allocating an exact multiple of <code>2MB</code>, and so we always round up the allocation
for the tree to the next multiple of <code>2MB</code>. However, it turns out that small
allocations below <code>32MB</code> still go on the program’s <em>heap</em>, rather than asking
the kernel for new memory pages, causing them to not actually be hugepages.
Thus, all allocations we do are actually rounded up to the next multiple of
<code>32MB</code> instead.</p><p>All together, hugepages sometimes makes a small difference when the dataset is
indeed between <code>1MB</code> and <code>32MB</code> in size. Smaller data structures don’t really need
hugepages anyway. Enabling them for the Eytzinger layout as in the plot above
also gives a significant speedup for larger sizes.</p><h2 id="a-note-on-benchmarking"><span>1.5</span> A note on benchmarking
<a href="#a-note-on-benchmarking"></a></h2><p>The plots have the size of the input data on the logarithmic (bottom) x-axis. On the top,
they show the corresponding number of elements in the vector, which is 4 times
less, since each element is a <code>u32</code> spanning 4 bytes.
Measurements are taken at values \(2^i\), \(1.25 \cdot 2^i\), \(1.5\cdot 2^i\), and
\(1.75\cdot 2^i\).</p><p>The y-axis shows measured time per query. In the plot above, it says
<em>latency</em>, since it is benchmarked as <code>for q in queries { index.query(q); }</code>.
Even then, the pipelining and out-of-order execution of the CPU will make it
execute multiple iterations in parallel. Specifically, while it is waiting for
the last cache lines of iteration \(i\), it can already start executing the first
instructions of the next query. To measure the true latency, we would have to
introduce a <em>loop carried dependency</em> by making query \(i+1\) dependent on the
result of query \(i\).
However, the main goal of this post is to optimize for <em>throughput</em>, so we won’t
bother with that.</p><p>Thus, all plots will show the throughput of doing <code>index.query(all_queries)</code>.</p><p>For the benchmarks, I’m using my laptop’s <code>i7-10750H</code> CPU, with the frequency
fixed to <code>2.6GHz</code> using <a href="#code-snippet--pin">Code Snippet 4</a>.<sup id="fnref:2"><a href="#fn:2" role="doc-noteref">2</a></sup></p><div><table><tbody><tr><td><pre tabindex="0"><code><span>1
</span></code></pre></td><td><pre tabindex="0"><code data-lang="sh"><span><span>sudo cpupower frequency-set -g powersave -d 2.6GHz -u 2.6GHz
</span></span></code></pre></td></tr></tbody></table></div><p><span><a href="#code-snippet--pin">Code Snippet 4</a>:</span>
Pinning the CPU frequency to <code>2.6GHz</code>.</p><p>Also relevant are the sizes of the caches: <code>32KiB</code> L1 cache per core, <code>256KiB</code>
L2 cache per core, and <code>12MiB</code> L3 cache shared between the physical 6 cores.
Furthermore, hyper-threading is disabled.</p><p>All measurements are done 5 times. The line follows the median, and we show the
spread of the 2nd to 4th value (i.e., after discarding the minimum and maximum).
Observe that in most of the plot above, the spread is barely visible! Thus,
while especially the graph for binary search looks very noisy, that ’noise’ is
in fact completely reproducible. Indeed, it’s caused by effects of <em>cache
associativity</em>, as explained in the array layouts paper
(Khuong and Morin (<a href="#citeproc_bib_item_2">2017</a>); this post is long enough already).</p><h2 id="cache-lines"><span>1.6</span> Cache lines
<a href="#cache-lines"></a></h2><p>Main memory and the caches work at the level of <em>cache lines</em> consisting of 64
bytes (at least on my machine), or 16 <code>u32</code> values. Thus, even if you only read a single byte, if
the cache line containing that byte is not yet in the L1 cache, the entire thing
will be fetched from RAM or L3 or L2 into L1.</p><p>Plain binary search typically only uses a single value of each cache line,
until it gets to the end of the search where the last 16 values span just 1 or 2
cache lines.</p><p>They Eytzinger layout suffers the same problem: even though the next cache line
can be prefetched, it still only uses a single value in each.
This fundamentally means that both these search schemes are using the available
memory bandwidth quite inefficiently, and since most of what they are doing is
waiting for memory to come through, that’s not great.
Also, while that’s not relevant <em>yet</em>, when doing this with many threads in
parallel, or with batching, single-core RAM throughput and the throughput of the
main memory itself become a bottleneck.</p><p>It would be much better if <em>somehow</em>, we could use the information in each cache
line much more efficiently ;)</p><p>We can do that by storing our data in a different way. Instead of storing it
layer by layer, so that each iteration goes into a new layer,
we can store 4 layers of the tree at a time (<a href="#code-snippet--node">Code Snippet 5</a>). That takes 15 values, and could
nicely be padded into a full cache line. Then when we fetch a cache line, we can
use it for 4 iterations at once – much better!
On the other hand, now we can’t prefetch upcoming cache lines in advance
anymore, so that overall the latency will be the same. But we fetch up to 4
times fewer cache lines overall, which should help throughput.</p><p>Unfortunately, I don’t have code and plots here, because what I really want to
focus on is the next bit.</p><figure><a href="https://curiouscoding.nl/ox-hugo/packed-eytzinger.svg"><img src="https://curiouscoding.nl/ox-hugo/packed-eytzinger.svg" alt="Figure 2: The first two rows show how we could pack four layers of the Eytzinger search into a single cache line. The first follows a classic binary search layout, while the second applies the Eytzinger layout recursively. The third row shows an S-tree node instead. For simplicity and clarity, I’m using consecutive values, but in practice, this would be any list of sorted numbers."></a><figcaption><p><span>Figure 2: </span>The first two rows show how we could pack four layers of the Eytzinger search into a single cache line. The first follows a classic binary search layout, while the second applies the Eytzinger layout recursively. The third row shows an S-tree node instead. For simplicity and clarity, I’m using consecutive values, but in practice, this would be any list of sorted numbers.</p></figcaption></figure><h2 id="s-trees-and-b-trees"><span>1.7</span> S-trees and B-trees
<a href="#s-trees-and-b-trees"></a></h2><p>We just ended with a <em>node</em> of 15 values that represent a height-4 search tree
in which we can binary search. From there, it’s just a small step to S-trees.</p><p><strong>B-trees.</strong> But first I have to briefly mention B-trees though (<a href="https://en.wikipedia.org/wiki/B-tree">wikipedia</a>). Those are
the more classic dynamic variant, where nodes are linked together via pointers.
As wikipedia writes, they are typically used with much larger block sizes, for
example 4kB, since files read from disk usually come in 4kB chunks. Thus, they
also have much larger branching factors.</p><p><strong>S-trees.</strong> But we will instead use S-trees, as named so by Algorithmica. They
are a nice middle ground between the high branching factor of B-trees, and the
compactness of the Eytzinger layout.
Instead of interpreting the 15 values as a search tree, we can also store them
in a sorted way, and consider them as a 16-ary search tree: the 15 values simply
split the data in the subtree into 16 parts, and we can do a linear scan to find
which part to recurse into.
But if we store 15 values and one padding in a cache line, we might as well make
it 16 values and have a branching factor of 17 instead.</p><p><strong>S+ trees.</strong> B-trees and S-trees only store each value once, either in a leaf node or
in an internal node. This turns out to be somewhat annoying, since we must track
in which layer the result was found. To simplify this, we can store <em>all</em> values
as a leaf, and <em>duplicate</em> them in the internal nodes. This is then called a B+
tree or S+ tree. However, I will be lazy and just use S-tree to include this modification.</p><figure><a href="https://curiouscoding.nl/ox-hugo/full.svg"><img src="https://curiouscoding.nl/ox-hugo/full.svg" alt="Figure 3: An example of a ‘full’ S+ tree (that I will from now just call S-tree) on 18 values with nodes of size (B=2) and branching factor (B+1=3). Each internal node stores the smallest value in the subtree on its right. In memory, the layers are simply packed together behind each other."></a><figcaption><p><span>Figure 3: </span>An example of a ‘full’ S+ tree (that I will from now just call S-tree) on 18 values with nodes of size (B=2) and branching factor (B+1=3). Each internal node stores the smallest value in the subtree on its right. In memory, the layers are simply packed together behind each other.</p></figcaption></figure><p>A full S-tree can be navigated in a way similar to the Eytzinger layout: The
node (note: not<sup id="fnref:3"><a href="#fn:3" role="doc-noteref">3</a></sup> value) at index \(i\) has its \(B+1\) child-nodes at indices \((B+1)\cdot i + 1 + \{0, \dots, B\}\).</p><p>When the tree is only partially filled, the full layout can waste a lot of space
(<a href="#figure--stree-partial">Figure 4</a>). Instead, we can <em>pack</em> the layers together, by storing the
offset \(o_\ell\) of each layer.</p><p>The children of node \(o_\ell + i\) are then at \(o_{\ell+1} + (B+1)\cdot i + \{0, \dots, B\}\).</p><figure><a href="https://curiouscoding.nl/ox-hugo/partial.svg"><img src="https://curiouscoding.nl/ox-hugo/partial.svg" alt="Figure 4: The full representation can be inefficient. The packed representation removes the empty space, and explicitly stores the offset (o_ell) where each layer starts."></a><figcaption><p><span>Figure 4: </span>The <em>full</em> representation can be inefficient. The <em>packed</em> representation removes the empty space, and explicitly stores the offset (o_ell) where each layer starts.</p></figcaption></figure><p>At last, let’s have a look at some code. Each node in the tree is simply
represented as a list of \(N=16\) <code>u32</code> values. We explicitly ask that nodes are
aligned to 64byte cache line boundaries.</p><div><table><tbody><tr><td><pre tabindex="0"><code><span>1
</span><span>2
</span><span>3
</span><span>4
</span></code></pre></td><td><pre tabindex="0"><code data-lang="rust"><span><span><span>#[repr(align(64))]</span><span>
</span></span></span><span><span><span></span><span>pub</span><span> </span><span>struct</span> <span>TreeNode</span><span>&lt;</span><span>const</span><span> </span><span>N</span>: <span>usize</span><span>&gt;</span><span> </span><span>{</span><span>
</span></span></span><span><span><span>    </span><span>data</span>: <span>[</span><span>u32</span><span>;</span><span> </span><span>N</span><span>],</span><span>
</span></span></span><span><span><span></span><span>}</span><span>
</span></span></span></code></pre></td></tr></tbody></table></div><p><span><a href="#code-snippet--node">Code Snippet 5</a>:</span>
Search tree node, aligned to a 64 byte cache line. For now, N is always 16. The values in a node must always be sorted.</p><p>The S-tree itself is simply a list of nodes, and the offsets where each layer starts.</p><div><table><tbody><tr><td><pre tabindex="0"><code><span>1
</span><span>2
</span><span>3
</span><span>4
</span><span>5
</span><span>6
</span><span>7
</span><span>8
</span><span>9
</span></code></pre></td><td><pre tabindex="0"><code data-lang="rust"><span><span><span>/// N: #elements in a node, always 16.
</span></span></span><span><span><span>/// B: branching factor &lt;= N+1. Typically 17.
</span></span></span><span><span><span></span><span>pub</span><span> </span><span>struct</span> <span>STree</span><span>&lt;</span><span>const</span><span> </span><span>B</span>: <span>usize</span><span>,</span><span> </span><span>const</span><span> </span><span>N</span>: <span>usize</span><span>&gt;</span><span> </span><span>{</span><span>
</span></span></span><span><span><span>    </span><span>/// The list of tree nodes.
</span></span></span><span><span><span></span><span>    </span><span>tree</span>: <span>Vec</span><span>&lt;</span><span>TreeNode</span><span>&lt;</span><span>N</span><span>&gt;&gt;</span><span>,</span><span>
</span></span></span><span><span><span>    </span><span>/// The root is at index tree[offsets[0]].
</span></span></span><span><span><span></span><span>    </span><span>/// It's children start at tree[offsets[1]], and so on.
</span></span></span><span><span><span></span><span>    </span><span>offsets</span>: <span>Vec</span><span>&lt;</span><span>usize</span><span>&gt;</span><span>,</span><span>
</span></span></span><span><span><span></span><span>}</span><span>
</span></span></span></code></pre></td></tr></tbody></table></div><p><span><a href="#code-snippet--stree">Code Snippet 6</a>:</span>
The S-tree data structure. It depends on the number of values per node \(B\) (usually 16 but sometimes 15) and the size of each node \(N\) (always 16).</p><p>To save some space, and focus on the interesting part (to me, at least), I will
not show any code for constructing S-trees. It’s a whole bunch of uninteresting
fiddling with indices, and takes a lot of time to get right. Also, construction
is not optimized at all currently. Anyway, find the code <a href="https://github.com/RagnarGrootKoerkamp/suffix-array-searching/tree/master/static-search-tree/src">here</a>.</p><p>TODO: Reverse offsets.</p><p>What we <em>will</em> look at, is code for searching S-trees.</p><div><table><tbody><tr><td><pre tabindex="0"><code><span> 1
</span><span> 2
</span><span> 3
</span><span> 4
</span><span> 5
</span><span> 6
</span><span> 7
</span><span> 8
</span><span> 9
</span><span>10
</span><span>11
</span><span>12
</span><span>13
</span></code></pre></td><td><pre tabindex="0"><code data-lang="rust"><span><span><span>fn</span> <span>search</span><span>(</span><span>&amp;</span><span>self</span><span>,</span><span> </span><span>q</span>: <span>u32</span><span>,</span><span> </span><span>find</span>: <span>impl</span><span> </span><span>Fn</span><span>(</span><span>&amp;</span><span>TreeNode</span><span>&lt;</span><span>N</span><span>&gt;</span><span>,</span><span> </span><span>u32</span><span>)</span><span> </span>-&gt; <span>usize</span><span>)</span><span> </span>-&gt; <span>u32</span> <span>{</span><span>
</span></span></span><span><span><span>    </span><span>let</span><span> </span><span>mut</span><span> </span><span>k</span><span> </span><span>=</span><span> </span><span>0</span><span>;</span><span>
</span></span></span><span><span><span>    </span><span>for</span><span> </span><span>o</span><span> </span><span>in</span><span> </span><span>self</span><span>.</span><span>offsets</span><span>[</span><span>0</span><span>..</span><span>self</span><span>.</span><span>offsets</span><span>.</span><span>len</span><span>()</span><span>-</span><span>1</span><span>]</span><span> </span><span>{</span><span>
</span></span></span><span><span><span>        </span><span>let</span><span> </span><span>jump_to</span><span> </span><span>=</span><span> </span><span>find</span><span>(</span><span>self</span><span>.</span><span>node</span><span>(</span><span>o</span><span> </span><span>+</span><span> </span><span>k</span><span>),</span><span> </span><span>q</span><span>);</span><span>
</span></span></span><span><span><span>        </span><span>k</span><span> </span><span>=</span><span> </span><span>k</span><span> </span><span>*</span><span> </span><span>(</span><span>B</span><span> </span><span>+</span><span> </span><span>1</span><span>)</span><span> </span><span>+</span><span> </span><span>jump_to</span><span>;</span><span>
</span></span></span><span><span><span>    </span><span>}</span><span>
</span></span></span><span><span><span>
</span></span></span><span><span><span>    </span><span>let</span><span> </span><span>o</span><span> </span><span>=</span><span> </span><span>self</span><span>.</span><span>offsets</span><span>.</span><span>last</span><span>().</span><span>unwrap</span><span>();</span><span>
</span></span></span><span><span><span>    </span><span>// node(i) returns tree[i] using unchecked indexing.
</span></span></span><span><span><span></span><span>    </span><span>let</span><span> </span><span>mut</span><span> </span><span>idx</span><span> </span><span>=</span><span> </span><span>find</span><span>(</span><span>self</span><span>.</span><span>node</span><span>(</span><span>o</span><span> </span><span>+</span><span> </span><span>k</span><span>),</span><span> </span><span>q</span><span>);</span><span>
</span></span></span><span><span><span>    </span><span>// get(i, j) returns tree[i].data[j] using unchecked indexing.
</span></span></span><span><span><span></span><span>    </span><span>self</span><span>.</span><span>get</span><span>(</span><span>o</span><span> </span><span>+</span><span> </span><span>k</span><span> </span><span>+</span><span> </span><span>idx</span><span> </span><span>/</span><span> </span><span>N</span><span>,</span><span> </span><span>idx</span><span> </span><span>%</span><span> </span><span>N</span><span>)</span><span>
</span></span></span><span><span><span></span><span>}</span><span>
</span></span></span></code></pre></td></tr></tbody></table></div><p>Our first step will be optimizing the <code>find</code> function.</p><h2 id="optimizing-find"><span>2</span> Optimizing <code>find</code>
<a href="#optimizing-find"></a></h2><h2 id="linear"><span>2.1</span> Linear
<a href="#linear"></a></h2><p>Let’s first precisely define what we want <code>find</code> to do:
it’s input is a node with 16 sorted values and a query value \(q\), and it should return
the index of the first element that is at least \(q\).</p><p>Some simple code for this is <a href="#code-snippet--find-linear">Code Snippet 8</a>.</p><div><table><tbody><tr><td><pre tabindex="0"><code><span>1
</span><span>2
</span><span>3
</span><span>4
</span><span>5
</span><span>6
</span><span>7
</span><span>8
</span></code></pre></td><td><pre tabindex="0"><code data-lang="rust"><span><span><span>pub</span><span> </span><span>fn</span> <span>find_linear</span><span>(</span><span>&amp;</span><span>self</span><span>,</span><span> </span><span>q</span>: <span>u32</span><span>)</span><span> </span>-&gt; <span>usize</span> <span>{</span><span>
</span></span></span><span><span><span>    </span><span>for</span><span> </span><span>i</span><span> </span><span>in</span><span> </span><span>0</span><span>..</span><span>N</span><span> </span><span>{</span><span>
</span></span></span><span><span><span>        </span><span>if</span><span> </span><span>self</span><span>.</span><span>data</span><span>[</span><span>i</span><span>]</span><span> </span><span>&gt;=</span><span> </span><span>q</span><span> </span><span>{</span><span>
</span></span></span><span><span><span>            </span><span>return</span><span> </span><span>i</span><span>;</span><span>
</span></span></span><span><span><span>        </span><span>}</span><span>
</span></span></span><span><span><span>    </span><span>}</span><span>
</span></span></span><span><span><span>    </span><span>N</span><span>
</span></span></span><span><span><span></span><span>}</span><span>
</span></span></span></code></pre></td></tr></tbody></table></div><p><span><a href="#code-snippet--find-linear">Code Snippet 8</a>:</span>
A linear scan for the first element \(\geq q\), that breaks as soon as it is found.</p><p>The results are not very impressive yet.</p><figure><a href="https://curiouscoding.nl/ox-hugo/2-find-linear.svg"><img src="https://curiouscoding.nl/ox-hugo/2-find-linear.svg" alt="Figure 5: The initial version of our S-tree search is quite a bit slower than the Eytzinger layout. In this and following plots, ‘old’ lines will be dimmed, and the best previous and best new line slightly highlighted. Colours will be consistent from one plot to the next."></a><figcaption><p><span>Figure 5: </span>The initial version of our S-tree search is quite a bit slower than the Eytzinger layout. In this and following plots, ‘old’ lines will be dimmed, and the best previous and best new line slightly highlighted. Colours will be consistent from one plot to the next.</p></figcaption></figure><h2 id="auto-vectorization"><span>2.2</span> Auto-vectorization
<a href="#auto-vectorization"></a></h2><p>As it turns out, the <code>break;</code> in <a href="#code-snippet--find-linear">Code Snippet 8</a> is really bad for performance,
since the branch predictor can’t do a good job on it.</p><p>Instead, we can <em>count</em> the number of values less than \(q\), and return that as
the index of the first value \(\geq q\). (Example: all values \(\geq q\) index
gives index 0.)</p><div><table><tbody><tr><td><pre tabindex="0"><code><span>1
</span><span>2
</span><span>3
</span><span>4
</span><span>5
</span><span>6
</span><span>7
</span><span>8
</span><span>9
</span></code></pre></td><td><pre tabindex="0"><code data-lang="rust"><span><span><span>pub</span><span> </span><span>fn</span> <span>find_linear_count</span><span>(</span><span>&amp;</span><span>self</span><span>,</span><span> </span><span>q</span>: <span>u32</span><span>)</span><span> </span>-&gt; <span>usize</span> <span>{</span><span>
</span></span></span><span><span><span>    </span><span>let</span><span> </span><span>mut</span><span> </span><span>count</span><span> </span><span>=</span><span> </span><span>0</span><span>;</span><span>
</span></span></span><span><span><span>    </span><span>for</span><span> </span><span>i</span><span> </span><span>in</span><span> </span><span>0</span><span>..</span><span>N</span><span> </span><span>{</span><span>
</span></span></span><span><span><span>        </span><span>if</span><span> </span><span>self</span><span>.</span><span>data</span><span>[</span><span>i</span><span>]</span><span> </span><span>&lt;</span><span> </span><span>q</span><span> </span><span>{</span><span>
</span></span></span><span><span><span>            </span><span>count</span><span> </span><span>+=</span><span> </span><span>1</span><span>;</span><span>
</span></span></span><span><span><span>        </span><span>}</span><span>
</span></span></span><span><span><span>    </span><span>}</span><span>
</span></span></span><span><span><span>    </span><span>count</span><span>
</span></span></span><span><span><span></span><span>}</span><span>
</span></span></span></code></pre></td></tr></tbody></table></div><p><span><a href="#code-snippet--linear-count">Code Snippet 9</a>:</span>
Counting values \(&lt; q\) instead of an early break. The <code>if self.data[i] &lt; q</code> can be optimized into branchless code.</p><p>In fact, the code is not just branchless, but actually it’s auto-vectorized into
SIMD instructions!</p><div><table><tbody><tr><td><pre tabindex="0"><code><span> 1
</span><span> 2
</span><span> 3
</span><span> 4
</span><span> 5
</span><span> 6
</span><span> 7
</span><span> 8
</span><span> 9
</span><span>10
</span><span>11
</span><span>12
</span><span>13
</span><span>14
</span><span>15
</span></code></pre></td><td><pre tabindex="0"><code data-lang="asm"><span><span><span>vmovdqu</span>      <span>(</span><span>%rax</span><span>,</span><span>%rcx</span><span>),</span> <span>%ymm1</span>     <span>; load data[..8]
</span></span></span><span><span><span></span><span>vmovdqu</span>      <span>32</span><span>(</span><span>%rax</span><span>,</span><span>%rcx</span><span>),</span> <span>%ymm2</span>   <span>; load data[8..]
</span></span></span><span><span><span></span><span>vpbroadcastd</span> <span>%xmm0</span><span>,</span> <span>%ymm0</span>           <span>; 'splat' the query value
</span></span></span><span><span><span></span><span>vpmaxud</span>      <span>%ymm0</span><span>,</span> <span>%ymm2</span><span>,</span> <span>%ymm3</span>    <span>; v
</span></span></span><span><span><span></span><span>vpcmpeqd</span>     <span>%ymm3</span><span>,</span> <span>%ymm2</span><span>,</span> <span>%ymm2</span>    <span>; v
</span></span></span><span><span><span></span><span>vpmaxud</span>      <span>%ymm0</span><span>,</span> <span>%ymm1</span><span>,</span> <span>%ymm0</span>    <span>; v
</span></span></span><span><span><span></span><span>vpcmpeqd</span>     <span>%ymm0</span><span>,</span> <span>%ymm1</span><span>,</span> <span>%ymm0</span>    <span>; 4x compare query with values
</span></span></span><span><span><span></span><span>vpackssdw</span>    <span>%ymm2</span><span>,</span> <span>%ymm0</span><span>,</span> <span>%ymm0</span>    <span>;
</span></span></span><span><span><span></span><span>vpcmpeqd</span>     <span>%ymm1</span><span>,</span> <span>%ymm1</span><span>,</span> <span>%ymm1</span>    <span>; v
</span></span></span><span><span><span></span><span>vpxor</span>        <span>%ymm1</span><span>,</span> <span>%ymm0</span><span>,</span> <span>%ymm0</span>    <span>; 2x negate result
</span></span></span><span><span><span></span><span>vextracti128</span> <span>$1</span><span>,</span> <span>%ymm0</span><span>,</span> <span>%xmm1</span>       <span>; v
</span></span></span><span><span><span></span><span>vpacksswb</span>    <span>%xmm1</span><span>,</span> <span>%xmm0</span><span>,</span> <span>%xmm0</span>    <span>; v
</span></span></span><span><span><span></span><span>vpshufd</span>      <span>$216</span><span>,</span> <span>%xmm0</span><span>,</span> <span>%xmm0</span>     <span>; v
</span></span></span><span><span><span></span><span>vpmovmskb</span>    <span>%xmm0</span><span>,</span> <span>%ecx</span>            <span>; 4x extract mask
</span></span></span><span><span><span></span><span>popcntl</span>      <span>%ecx</span><span>,</span> <span>%ecx</span>             <span>; popcount the 16bit mask
</span></span></span></code></pre></td></tr></tbody></table></div><p><span><a href="#code-snippet--linear-count-asm">Code Snippet 10</a>:</span>
Code Snippet <a href="#org580fb2e">9</a> is auto-vectorized!</p><p>To save some space: you can find this and further results for this section in
<a href="#figure--find-results">Figure 34</a> at the end of the section.</p><p>This auto-vectorized version is over two times faster than the linear find,
and now clearly beats Eytzinger layout!</p><h2 id="trailing-zeros"><span>2.3</span> Trailing zeros
<a href="#trailing-zeros"></a></h2><p>We can also roll our own SIMD. The SIMD version of the original linear scan idea
does 16 comparisons in parallel, converts that to a bitmask, and then counts the
number of trailing zeros. Using <code>#[feature(portable_simd)]</code>, that looks like this:</p><div><table><tbody><tr><td><pre tabindex="0"><code><span>1
</span><span>2
</span><span>3
</span><span>4
</span><span>5
</span><span>6
</span></code></pre></td><td><pre tabindex="0"><code data-lang="rust"><span><span><span>pub</span><span> </span><span>fn</span> <span>find_ctz</span><span>(</span><span>&amp;</span><span>self</span><span>,</span><span> </span><span>q</span>: <span>u32</span><span>)</span><span> </span>-&gt; <span>usize</span> <span>{</span><span>
</span></span></span><span><span><span>    </span><span>let</span><span> </span><span>data</span>: <span>Simd</span><span>&lt;</span><span>u32</span><span>,</span><span> </span><span>N</span><span>&gt;</span><span> </span><span>=</span><span> </span><span>Simd</span>::<span>from_slice</span><span>(</span><span>&amp;</span><span>self</span><span>.</span><span>data</span><span>[</span><span>0</span><span>..</span><span>N</span><span>]);</span><span>
</span></span></span><span><span><span>    </span><span>let</span><span> </span><span>q</span><span> </span><span>=</span><span> </span><span>Simd</span>::<span>splat</span><span>(</span><span>q</span><span>);</span><span>
</span></span></span><span><span><span>    </span><span>let</span><span> </span><span>mask</span><span> </span><span>=</span><span> </span><span>q</span><span>.</span><span>simd_le</span><span>(</span><span>data</span><span>);</span><span>
</span></span></span><span><span><span>    </span><span>mask</span><span>.</span><span>first_set</span><span>().</span><span>unwrap_or</span><span>(</span><span>N</span><span>)</span><span>
</span></span></span><span><span><span></span><span>}</span><span>
</span></span></span></code></pre></td></tr></tbody></table></div><p><span><a href="#code-snippet--find-ctz">Code Snippet 11</a>:</span>
A <code>find</code> implementation using the <i>count-trailing-zeros</i> instruction.</p><div><table><tbody><tr><td><pre tabindex="0"><code><span> 1
</span><span> 2
</span><span> 3
</span><span> 4
</span><span> 5
</span><span> 6
</span><span> 7
</span><span> 8
</span><span> 9
</span><span>10
</span><span>11
</span></code></pre></td><td><pre tabindex="0"><code data-lang="asm"><span><span><span>vpminud</span>      <span>32</span><span>(</span><span>%rsi</span><span>,</span><span>%r8</span><span>),</span> <span>%ymm0</span><span>,</span> <span>%ymm1</span>  <span>; take min of data[8..] and query
</span></span></span><span><span><span></span><span>vpcmpeqd</span>     <span>%ymm1</span><span>,</span> <span>%ymm0</span><span>,</span> <span>%ymm1</span>         <span>; does the min equal query?
</span></span></span><span><span><span></span><span>vpminud</span>      <span>(</span><span>%rsi</span><span>,</span><span>%r8</span><span>),</span> <span>%ymm0</span><span>,</span> <span>%ymm2</span>    <span>; take min of data[..8] and query
</span></span></span><span><span><span></span><span>vpcmpeqd</span>     <span>%ymm2</span><span>,</span> <span>%ymm0</span><span>,</span> <span>%ymm2</span>         <span>; does the min equal query?
</span></span></span><span><span><span></span><span>vpackssdw</span>    <span>%ymm1</span><span>,</span> <span>%ymm2</span><span>,</span> <span>%ymm1</span>         <span>; pack the two results together, interleaved as 16bit words
</span></span></span><span><span><span></span><span>vextracti128</span> <span>$1</span><span>,</span> <span>%ymm1</span><span>,</span> <span>%xmm2</span>            <span>; extract half (both halves are equal)
</span></span></span><span><span><span></span><span>vpacksswb</span>    <span>%xmm2</span><span>,</span> <span>%xmm1</span><span>,</span> <span>%xmm1</span>         <span>; go down to 8bit values, but weirdly shuffled
</span></span></span><span><span><span></span><span>vpshufd</span>      <span>$216</span><span>,</span> <span>%xmm1</span><span>,</span> <span>%xmm1</span>          <span>; unshuffle
</span></span></span><span><span><span></span><span>vpmovmskb</span>    <span>%xmm1</span><span>,</span> <span>%r8d</span>                 <span>; extract the high bit of each 8bit value.
</span></span></span><span><span><span></span><span>orl</span>          <span>$65536</span><span>,</span><span>%r8d</span>                 <span>; set bit 16, to cover the unwrap_or(N)
</span></span></span><span><span><span></span><span>tzcntl</span>       <span>%r8d</span><span>,</span><span>%r15d</span>                  <span>; count trailing zeros
</span></span></span></code></pre></td></tr></tbody></table></div><p><span>Code Snippet 12:</span>
Assembly code for Code Snippet <a href="#orge6452ef">11</a>. Instead of ending with <code>popcntl</code>, this ends with <code>tzcntl</code>.</p><p>Now, let’s look at this generated code in a bit more detail.</p><p>First up: why does <code>simd_le</code> translate into <code>min</code> and <code>cmpeq</code>?</p><p>From checking the <a href="https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#=undefined&amp;techs=AVX_ALL&amp;text=_mm256_cmp">Intel Intrinsics Guide</a>, we find out that there are only signed
comparisons, while our data is unsigned. For now, let’s just assume that all
values fit in 31 bits and are at most <code>i32::MAX</code>. Then, we can transmute our input
to <code>Simd&lt;i32, 8&gt;</code> without changing its meaning.</p><div><p>Assumption</p><div><p>Both input values and queries are between <code>0</code> and <code>i32::MAX</code>.</p><p>Eventually we can fix this by either taking <code>i32</code> input directly, or by shifting
<code>u32</code> values to fit in the <code>i32</code> range.</p></div></div><div><table><tbody><tr><td><pre tabindex="0"><code><span> 1
</span><span> 2
</span><span> 3
</span><span> 4
</span><span> 5
</span><span> 6
</span><span> 7
</span><span> 8
</span><span> 9
</span><span>10
</span><span>11
</span></code></pre></td><td><pre tabindex="0"><code data-lang="diff"><span><span> pub fn find_ctz_signed(&amp;self, q: u32) -&gt; usize
</span></span><span><span> where
</span></span><span><span>     LaneCount&lt;N&gt;: SupportedLaneCount,
</span></span><span><span> {
</span></span><span><span><span>-    let data: Simd&lt;u32, N&gt; = Simd::from_slice(                   &amp;self.data[0..N]   );
</span></span></span><span><span><span></span><span>+    let data: Simd&lt;i32, N&gt; = Simd::from_slice(unsafe { transmute(&amp;self.data[0..N]) });
</span></span></span><span><span><span></span><span>-    let q = Simd::splat(q       );
</span></span></span><span><span><span></span><span>+    let q = Simd::splat(q as i32);
</span></span></span><span><span><span></span>     let mask = q.simd_le(data);
</span></span><span><span>     mask.first_set().unwrap_or(N)
</span></span><span><span> }
</span></span></code></pre></td></tr></tbody></table></div><p><span><a href="#code-snippet--ctz-signed">Code Snippet 13</a>:</span>
Same as before, but now using <code>i32</code> values instead of <code>u32</code>.</p><div><table><tbody><tr><td><pre tabindex="0"><code><span> 1
</span><span> 2
</span><span> 3
</span><span> 4
</span><span> 5
</span><span> 6
</span><span> 7
</span><span> 8
</span><span> 9
</span><span>10
</span><span>11
</span><span>12
</span><span>13
</span><span>14
</span></code></pre></td><td><pre tabindex="0"><code data-lang="diff"><span><span><span>-vpminud      32(%rsi,%r8), %ymm0, %ymm1
</span></span></span><span><span><span>-vpcmpeqd     %ymm1, %ymm0, %ymm1
</span></span></span><span><span><span></span><span>+vpcmpgtd     32(%rsi,%rdi), %ymm1, %ymm2 ; is query(%ymm1) &gt; data[8..]?
</span></span></span><span><span><span></span><span>-vpminud      (%rsi,%r8), %ymm0, %ymm2
</span></span></span><span><span><span>-vpcmpeqd     %ymm2, %ymm0, %ymm2
</span></span></span><span><span><span></span><span>+vpcmpgtd     (%rsi,%rdi), %ymm1, %ymm1   ; is query(%ymm1) &gt; data[..8]?
</span></span></span><span><span><span></span> vpackssdw    %ymm2, %ymm1, %ymm1         ; pack results
</span></span><span><span><span>+vpxor        %ymm0, %ymm1, %ymm1         ; negate results (ymm0 is all-ones)
</span></span></span><span><span><span></span> vextracti128 $1, %ymm1, %xmm2            ; extract u16x16
</span></span><span><span> vpacksswb    %xmm2, %xmm1, %xmm1         ; shuffle
</span></span><span><span> vpshufd      $216, %xmm1, %xmm1          ; extract u8x16
</span></span><span><span> vpmovmskb    %xmm1, %edi                 ; extract u16 mask
</span></span><span><span> orl          $65536,%edi                 ; add bit to get 16 when none set
</span></span><span><span> tzcntl       %edi,%edi                   ; count trailing zeros
</span></span></code></pre></td></tr></tbody></table></div><p><span><a href="#code-snippet--ctz-signed-asm">Code Snippet 14</a>:</span>
The two <code>vpminud</code> and <code>vpcmpeqd</code> instructions are gone now and merged into <code>vpcmpgtd</code>, but instead we got a <code>vpxor</code> back :/ (Ignore the different registers being used in the old versus the new version.)</p><p>It turns out there is only a <code>&gt;</code> instruction in SIMD, and not <code>&gt;=</code>, and so there
is no way to avoid inverting the result.</p><p>We also see a <code>vpshufd</code> instruction that feels <em>very</em> out of place. What’s
happening is that while packing the result of the 16 <code>u32</code> comparisons down to a
single 16bit value, data is interleaved in an unfortunate way, and we need to
fix that.
Here, Algorithmica takes the approach of ‘pre-shuffling’ the values in each
node to counter for the unshuffle instruction.
They also suggest using <code>popcount</code> instead, which is indeed what we’ll do next.</p><h2 id="popcount"><span>2.4</span> Popcount
<a href="#popcount"></a></h2><p>As we saw, the drawback of the trailing zero count approach is that the order of
the lanes must be preserved. Instead, we’ll now simply count the number of lanes
with a value less than the query, similar to the auto-vectorized SIMD before,
so that the order of lanes doesn’t matter.</p><div><table><tbody><tr><td><pre tabindex="0"><code><span> 1
</span><span> 2
</span><span> 3
</span><span> 4
</span><span> 5
</span><span> 6
</span><span> 7
</span><span> 8
</span><span> 9
</span><span>10
</span><span>11
</span></code></pre></td><td><pre tabindex="0"><code data-lang="diff"><span><span> pub fn find_popcnt_portable(&amp;self, q: u32) -&gt; usize
</span></span><span><span> where
</span></span><span><span>     LaneCount&lt;N&gt;: SupportedLaneCount,
</span></span><span><span> {
</span></span><span><span>     let data: Simd&lt;i32, N&gt; = Simd::from_slice(unsafe { transmute(&amp;self.data[0..N]) });
</span></span><span><span>     let q = Simd::splat(q as i32);
</span></span><span><span><span>-    let mask = q.simd_le(data);
</span></span></span><span><span><span></span><span>+    let mask = q.simd_gt(data);
</span></span></span><span><span><span></span><span>-    mask.first_set().unwrap_or(N)
</span></span></span><span><span><span></span><span>+    mask.to_bitmask().count_ones() as usize
</span></span></span><span><span><span></span> }
</span></span></code></pre></td></tr></tbody></table></div><p><span><a href="#code-snippet--popcount-1">Code Snippet 15</a>:</span>
Using popcount instead of trailing zeros.</p><div><table><tbody><tr><td><pre tabindex="0"><code><span> 1
</span><span> 2
</span><span> 3
</span><span> 4
</span><span> 5
</span><span> 6
</span><span> 7
</span><span> 8
</span><span> 9
</span><span>10
</span></code></pre></td><td><pre tabindex="0"><code data-lang="diff"><span><span> vpcmpgtd     32(%rsi,%rdi), %ymm0, %ymm1
</span></span><span><span> vpcmpgtd     (%rsi,%rdi), %ymm0, %ymm0
</span></span><span><span> vpackssdw    %ymm1, %ymm0, %ymm0     ; 1
</span></span><span><span><span>-vpxor        %ymm0, %ymm1, %ymm1
</span></span></span><span><span><span></span> vextracti128 $1, %ymm0, %xmm1        ; 2
</span></span><span><span> vpacksswb    %xmm1, %xmm0, %xmm0     ; 3
</span></span><span><span> vpshufd      $216, %xmm0, %xmm0      ; 4
</span></span><span><span> vpmovmskb    %xmm0, %edi             ; 5
</span></span><span><span><span>-orl          $65536,%edi
</span></span></span><span><span><span></span><span>+popcntl      %edi, %edi
</span></span></span></code></pre></td></tr></tbody></table></div><p><span><a href="#code-snippet--popcount-1-asm">Code Snippet 16</a>:</span>
the <code>xor</code> and <code>or</code> instructions are gone, but we are still stuck with the sequence of 5 instructions to go from the comparison results to an integer bitmask.</p><p>Ideally we would like to <code>movmsk</code> directly on the <code>u16x16</code> output of the first
pack instruction, <code>vpackssdw</code>, to get the highest bit of each of the 16 16-bit values.
Unfortunately, we are again let down by AVX2: there are <code>movemask</code> <a href="https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#=undefined&amp;techs=AVX_ALL&amp;text=movms">instructions</a>
for <code>u8</code>, <code>u32</code>, and <code>u64</code>, but not for <code>u16</code>.</p><p>Also, the <code>vpshufd</code> instruction is now provably useless, so it’s slightly
disappointing the compiler didn’t elide it. Time to write the SIMD by hand instead.</p><h2 id="manual-simd"><span>2.5</span> Manual SIMD
<a href="#manual-simd"></a></h2><p>As it turns out, we can get away without most of the packing!
Instead of using <code>vpmovmskb</code> (<code>_mm256_movemask_epi8</code>) on 8bit data, we can
actually just use it directly on the 16bit output of <code>vpackssdw</code>!
Since the comparison sets each lane to all-zeros or all-ones, we can safely read
the most significant <em>and</em> middle bit, and divide the count by two at the
end.<sup id="fnref:4"><a href="#fn:4" role="doc-noteref">4</a></sup></p><div><table><tbody><tr><td><pre tabindex="0"><code><span> 1
</span><span> 2
</span><span> 3
</span><span> 4
</span><span> 5
</span><span> 6
</span><span> 7
</span><span> 8
</span><span> 9
</span><span>10
</span><span>11
</span><span>12
</span><span>13
</span><span>14
</span><span>15
</span><span>16
</span><span>17
</span></code></pre></td><td><pre tabindex="0"><code data-lang="rust"><span><span><span>pub</span><span> </span><span>fn</span> <span>find_popcnt</span><span>(</span><span>&amp;</span><span>self</span><span>,</span><span> </span><span>q</span>: <span>u32</span><span>)</span><span> </span>-&gt; <span>usize</span> <span>{</span><span>
</span></span></span><span><span><span>    </span><span>// We explicitly require that N is 16.
</span></span></span><span><span><span></span><span>    </span><span>let</span><span> </span><span>low</span>: <span>Simd</span><span>&lt;</span><span>u32</span><span>,</span><span> </span><span>8</span><span>&gt;</span><span> </span><span>=</span><span> </span><span>Simd</span>::<span>from_slice</span><span>(</span><span>&amp;</span><span>self</span><span>.</span><span>data</span><span>[</span><span>0</span><span>..</span><span>N</span><span> </span><span>/</span><span> </span><span>2</span><span>]);</span><span>
</span></span></span><span><span><span>    </span><span>let</span><span> </span><span>high</span>: <span>Simd</span><span>&lt;</span><span>u32</span><span>,</span><span> </span><span>8</span><span>&gt;</span><span> </span><span>=</span><span> </span><span>Simd</span>::<span>from_slice</span><span>(</span><span>&amp;</span><span>self</span><span>.</span><span>data</span><span>[</span><span>N</span><span> </span><span>/</span><span> </span><span>2</span><span>..</span><span>N</span><span>]);</span><span>
</span></span></span><span><span><span>    </span><span>let</span><span> </span><span>q_simd</span><span> </span><span>=</span><span> </span><span>Simd</span>::<span>&lt;</span><span>_</span><span>,</span><span> </span><span>8</span><span>&gt;</span>::<span>splat</span><span>(</span><span>q</span><span> </span><span>as</span><span> </span><span>i32</span><span>);</span><span>
</span></span></span><span><span><span>    </span><span>unsafe</span><span> </span><span>{</span><span>
</span></span></span><span><span><span>        </span><span>use</span><span> </span><span>std</span>::<span>mem</span>::<span>transmute</span><span> </span><span>as</span><span> </span><span>t</span><span>;</span><span>
</span></span></span><span><span><span>        </span><span>// Transmute from u32 to i32.
</span></span></span><span><span><span></span><span>        </span><span>let</span><span> </span><span>mask_low</span><span> </span><span>=</span><span> </span><span>q_simd</span><span>.</span><span>simd_gt</span><span>(</span><span>t</span><span>(</span><span>low</span><span>));</span><span>
</span></span></span><span><span><span>        </span><span>let</span><span> </span><span>mask_high</span><span> </span><span>=</span><span> </span><span>q_simd</span><span>.</span><span>simd_gt</span><span>(</span><span>t</span><span>(</span><span>high</span><span>));</span><span>
</span></span></span><span><span><span>        </span><span>// Transmute from portable_simd to __m256i intrinsic types.
</span></span></span><span><span><span></span><span>        </span><span>let</span><span> </span><span>merged</span><span> </span><span>=</span><span> </span><span>_mm256_packs_epi32</span><span>(</span><span>t</span><span>(</span><span>mask_low</span><span>),</span><span> </span><span>t</span><span>(</span><span>mask_high</span><span>));</span><span>
</span></span></span><span><span><span>        </span><span>// 32 bits is sufficient to hold a count of 2 per lane.
</span></span></span><span><span><span></span><span>        </span><span>let</span><span> </span><span>mask</span>: <span>i32</span> <span>=</span><span> </span><span>_mm256_movemask_epi8</span><span>(</span><span>t</span><span>(</span><span>merged</span><span>));</span><span>
</span></span></span><span><span><span>        </span><span>mask</span><span>.</span><span>count_ones</span><span>()</span><span> </span><span>as</span><span> </span><span>usize</span><span> </span><span>/</span><span> </span><span>2</span><span>
</span></span></span><span><span><span>    </span><span>}</span><span>
</span></span></span><span><span><span></span><span>}</span><span>
</span></span></span></code></pre></td></tr></tbody></table></div><p><span><a href="#code-snippet--popcount">Code Snippet 17</a>:</span>
Manual version of the SIMD code, by explicitly using the intrinsics. This is kinda ugly now, and there's a lot of transmuting (casting) going on between <code>[u32; 8]</code>, <code>Simd&lt;u32, 8&gt;</code> and the native <code>__m256i</code> type, but we'll have to live with it.</p><div><table><tbody><tr><td><pre tabindex="0"><code><span>1
</span><span>2
</span><span>3
</span><span>4
</span><span>5
</span><span>6
</span><span>7
</span><span>8
</span><span>9
</span></code></pre></td><td><pre tabindex="0"><code data-lang="diff"><span><span> vpcmpgtd     (%rsi,%rdi), %ymm0, %ymm1
</span></span><span><span> vpcmpgtd     32(%rsi,%rdi), %ymm0, %ymm0
</span></span><span><span> vpackssdw    %ymm0, %ymm1, %ymm0
</span></span><span><span><span>-vextracti128 $1, %ymm0, %xmm1
</span></span></span><span><span><span>-vpacksswb    %xmm1, %xmm0, %xmm0
</span></span></span><span><span><span>-vpshufd      $216, %xmm0, %xmm0
</span></span></span><span><span><span>-vpmovmskb    %xmm0, %edi
</span></span></span><span><span><span></span><span>+vpmovmskb    %ymm0, %edi
</span></span></span><span><span><span></span> popcntl      %edi, %edi
</span></span></code></pre></td></tr></tbody></table></div><p><span><a href="#code-snippet--popcount-asm">Code Snippet 18</a>:</span>
Only 5 instructions total are left now. Note that there is no explicit division by 2, since this is absorbed into the pointer arithmetic in the remainder, after the function is inlined.</p><p>Now let’s have a look at the results of all this work.</p><figure><a href="https://curiouscoding.nl/ox-hugo/3-find.svg"><img src="https://curiouscoding.nl/ox-hugo/3-find.svg" alt="Figure 6: Using the S-tree with an optimized find function improves throughput from 240ns/query for Eytzinger to 140ns/query for the auto-vectorized one, and down to 115ns/query for the final hand-optimized version, which is over 2x speedup!"></a><figcaption><p><span>Figure 6: </span>Using the S-tree with an optimized <code>find</code> function improves throughput from <code>240ns/query</code> for Eytzinger to <code>140ns/query</code> for the auto-vectorized one, and down to <code>115ns/query</code> for the final hand-optimized version, which is over 2x speedup!</p></figcaption></figure><p>As can be seen very nicely in this plot, each single instruction that we remove
gives a small but consistent improvement in throughput. The biggest improvement
comes from the last step, where we indeed shaved off 3 instructions.</p><p>In fact, we can analyse this plot a bit more:</p><ul><li>For input up to \(2^6=64\) bytes, the performance is constant, since in this
case the ‘search tree’ only consists of the root node.</li><li>Up to input of size \(2^{10}\), the thee has two layers, and the performance is constant.</li><li>Similarly, we see the latency jumping up at size \(2^{14}\), \(2^{18}\), \(2^{22}\)
and \(2^{26}\), each time because a new layer is added to the tree. (Or rather,
the jumps are at powers of the branching factor \(B+1=17\) instead of \(2^4=16\), but you get the idea.)</li><li>In a way, we can also (handwaivily) interpret the x-axis as time: each time
the graph jumps up, the height of the jump is pretty much the time spent on
processing that one extra layer of the tree.</li><li>Once we exceed the size of L3 cache, things slow down quickly. At that
point, each extra layer of the tree adds a significant amount of time, since
waiting for RAM is inherently slow.</li><li>On the other hand, once we hit RAM, the slowdown is more smooth rather than
stepwise. This is because L3 is still able to cache a fraction of the
data structure, and that fraction only decreases slowly.</li><li>Again handwavily, we can also interpret the x-axis as a snapshot of space
usage at a fixed moment in time: the first three layers of the tree fit in L1.
The 4th and 5th layers fit in L2 and L3. Once the three is 6 layers deep, the
reads of that layer will mostly hit RAM, and any additional layers for sure
are going to RAM.</li></ul><p>From now on, this last version, <code>find_popcnt</code>, is the one we will be using.</p><h2 id="optimizing-the-search"><span>3</span> Optimizing the search
<a href="#optimizing-the-search"></a></h2><h2 id="batching"><span>3.1</span> Batching
<a href="#batching"></a></h2><p>As promised, the first improvement we’ll make is <em>batching</em>.
Instead of processing one query at a time, we can process multiple (many) queries
at once. This allows the CPU to work on multiple queries at the same time, and
in particular, it can have multiple (up to 10-12) in-progress requests to RAM at
a time. That way, instead of waiting for a latency of 80ns per read, we
effectively wait for 10 reads at the same time, lowering the amortized wait time
to around 8ns.</p><p>Batching very much benefits from the fact that we use an S+ tree instead of
S-tree, since each element is find in the last layer (at the same depth), and
hence the number of seach steps through the tree is the same for every element
in the batch.</p><div><table><tbody><tr><td><pre tabindex="0"><code><span> 1
</span><span> 2
</span><span> 3
</span><span> 4
</span><span> 5
</span><span> 6
</span><span> 7
</span><span> 8
</span><span> 9
</span><span>10
</span><span>11
</span><span>12
</span><span>13
</span><span>14
</span><span>15
</span></code></pre></td><td><pre tabindex="0"><code data-lang="rust"><span><span><span>fn</span> <span>batch</span><span>&lt;</span><span>const</span><span> </span><span>P</span>: <span>usize</span><span>&gt;</span><span>(</span><span>&amp;</span><span>self</span><span>,</span><span> </span><span>qb</span>: <span>&amp;</span><span>[</span><span>u32</span><span>;</span><span> </span><span>P</span><span>])</span><span> </span>-&gt; <span>[</span><span>u32</span><span>;</span><span> </span><span>P</span><span>]</span><span> </span><span>{</span><span>
</span></span></span><span><span><span>    </span><span>let</span><span> </span><span>mut</span><span> </span><span>k</span><span> </span><span>=</span><span> </span><span>[</span><span>0</span><span>;</span><span> </span><span>P</span><span>];</span><span>
</span></span></span><span><span><span>    </span><span>for</span><span> </span><span>[</span><span>o</span><span>,</span><span> </span><span>_o2</span><span>]</span><span> </span><span>in</span><span> </span><span>self</span><span>.</span><span>offsets</span><span>.</span><span>array_windows</span><span>()</span><span> </span><span>{</span><span>
</span></span></span><span><span><span>        </span><span>for</span><span> </span><span>i</span><span> </span><span>in</span><span> </span><span>0</span><span>..</span><span>P</span><span> </span><span>{</span><span>
</span></span></span><span><span><span>            </span><span>let</span><span> </span><span>jump_to</span><span> </span><span>=</span><span> </span><span>self</span><span>.</span><span>node</span><span>(</span><span>o</span><span> </span><span>+</span><span> </span><span>k</span><span>[</span><span>i</span><span>]).</span><span>find</span><span>(</span><span>qb</span><span>[</span><span>i</span><span>]);</span><span>
</span></span></span><span><span><span>            </span><span>k</span><span>[</span><span>i</span><span>]</span><span> </span><span>=</span><span> </span><span>k</span><span>[</span><span>i</span><span>]</span><span> </span><span>*</span><span> </span><span>(</span><span>B</span><span> </span><span>+</span><span> </span><span>1</span><span>)</span><span> </span><span>+</span><span> </span><span>jump_to</span><span>;</span><span>
</span></span></span><span><span><span>        </span><span>}</span><span>
</span></span></span><span><span><span>    </span><span>}</span><span>
</span></span></span><span><span><span>
</span></span></span><span><span><span>    </span><span>let</span><span> </span><span>o</span><span> </span><span>=</span><span> </span><span>self</span><span>.</span><span>offsets</span><span>.</span><span>last</span><span>().</span><span>unwrap</span><span>();</span><span>
</span></span></span><span><span><span>    </span><span>from_fn</span><span>(</span><span>|</span><span>i</span><span>|</span><span> </span><span>{</span><span>
</span></span></span><span><span><span>        </span><span>let</span><span> </span><span>idx</span><span> </span><span>=</span><span> </span><span>self</span><span>.</span><span>node</span><span>(</span><span>o</span><span> </span><span>+</span><span> </span><span>k</span><span>[</span><span>i</span><span>]).</span><span>find</span><span>(</span><span>qb</span><span>[</span><span>i</span><span>]);</span><span>
</span></span></span><span><span><span>        </span><span>self</span><span>.</span><span>get</span><span>(</span><span>o</span><span> </span><span>+</span><span> </span><span>k</span><span>[</span><span>i</span><span>]</span><span> </span><span>+</span><span> </span><span>idx</span><span> </span><span>/</span><span> </span><span>N</span><span>,</span><span> </span><span>idx</span><span> </span><span>%</span><span> </span><span>N</span><span>)</span><span>
</span></span></span><span><span><span>    </span><span>})</span><span>
</span></span></span><span><span><span></span><span>}</span><span>
</span></span></span></code></pre></td></tr></tbody></table></div><p><span><a href="#code-snippet--batch">Code Snippet 19</a>:</span>
The batching code is very similar to processing one query at a time. We just insert an additional loop over the batch of \(P\) items.</p><figure><a href="https://curiouscoding.nl/ox-hugo/4-batching.svg"><img src="https://curiouscoding.nl/ox-hugo/4-batching.svg" alt="Figure 7: Batch size 1 (red) performs very similar to our non-batched version (blue), around 115ns/query. Increasing the batch size to 2, 4, and 8 each time significantly improves performance, until it saturates at 45ns/query (2.5x faster) around 16."></a><figcaption><p><span>Figure 7: </span>Batch size 1 (red) performs very similar to our non-batched version (blue), around <code>115ns/query</code>. Increasing the batch size to 2, 4, and 8 each time significantly improves performance, until it saturates at <code>45ns/query</code> (2.5x faster) around 16.</p></figcaption></figure><p>One interesting observation is that going from batch size 1 to 2 does <em>not</em>
double the performance. I suspect this is because the CPU’s out-of-order
execution was already deep enough to effectively execute (almost) 2 queries in
parallel anyway. Going to a batch size of 4 and then 8 does provide a
significant speedup. Again going to 4 the speedup is relatively a bit less than
when going to 8, so probably even with batch size 4 the CPU is somewhat looking
ahead into the next batch of 4 already 🤯.</p><p>Throughput saturates at batch size 16 (or really, around 12 already), which
corresponds to the CPU having 12 <em>line fill buffers</em> and thus being able to
read up to 12 cache lines in parallel.</p><p>Nevertheless, we will settle on a batch size of 128, mostly because it leads to
slightly cleaner plots in the remainder. It is also every so slightly faster,
probably because the constant overhead of initializing a batch is smaller when
batches are larger.</p><h2 id="prefetching"><span>3.2</span> Prefetching
<a href="#prefetching"></a></h2><p>The CPU is already fetching multiple reads in parallel using out-of-order
execution, but we can also help out a bit by doing this explicitly using <em>prefetching</em>.
After processing a node, we determine the child node <code>k</code> that we need to visit
next, so we can directly request that node to be read from memory before
continuing with the rest of the batch.</p><div><table><tbody><tr><td><pre tabindex="0"><code><span> 1
</span><span> 2
</span><span> 3
</span><span> 4
</span><span> 5
</span><span> 6
</span><span> 7
</span><span> 8
</span><span> 9
</span><span>10
</span><span>11
</span><span>12
</span><span>13
</span><span>14
</span><span>15
</span><span>16
</span></code></pre></td><td><pre tabindex="0"><code data-lang="diff"><span><span> fn batch&lt;const P: usize&gt;(&amp;self, qb: &amp;[u32; P]) -&gt; [u32; P] {
</span></span><span><span>     let mut k = [0; P];
</span></span><span><span>     for [o, o2] in self.offsets.array_windows() {
</span></span><span><span>         for i in 0..P {
</span></span><span><span>             let jump_to = self.node(o + k[i]).find(qb[i]);
</span></span><span><span>             k[i] = k[i] * (B + 1) + jump_to;
</span></span><span><span><span>+            prefetch_index(&amp;self.tree, o2 + k[i]);
</span></span></span><span><span><span></span>         }
</span></span><span><span>     }
</span></span><span><span>
</span></span><span><span>     let o = self.offsets.last().unwrap();
</span></span><span><span>     from_fn(|i| {
</span></span><span><span>         let idx = self.node(o + k[i]).find(qb[i]);
</span></span><span><span>         self.get(o + k[i] + idx / N, idx % N)
</span></span><span><span>     })
</span></span><span><span> }
</span></span></code></pre></td></tr></tbody></table></div><p><span><a href="#code-snippet--prefetch">Code Snippet 20</a>:</span>
Prefetching the cache line/node for the next iteration ahead.</p><figure><a href="https://curiouscoding.nl/ox-hugo/5-prefetch.svg"><img src="https://curiouscoding.nl/ox-hugo/5-prefetch.svg" alt="Figure 8: Prefetching helps speeding things up once the data does not fit in L2 cache anymore, and gets us down from 45ns/query to 30ns/query for 1GB input."></a><figcaption><p><span>Figure 8: </span>Prefetching helps speeding things up once the data does not fit in L2 cache anymore, and gets us down from <code>45ns/query</code> to <code>30ns/query</code> for <code>1GB</code> input.</p></figcaption></figure><p>We observe a few things: first prefetching slightly slow things down while data
fits in L1 already, since in that case the instruction just doesn’t do anything anyway.
In L2, it makes the graph slightly more flat, indicating that already there, the
latency is already a little bit of a bottleneck.
In L3 this effect gets larger, and we get a nice smooth/horizontal graph, until
we hit RAM size. There, prefetching provides the biggest gains.</p><h2 id="pointer-arithmetic"><span>3.3</span> Pointer arithmetic
<a href="#pointer-arithmetic"></a></h2><p>Again, it’s time to look at some assembly code, now to optimize the search
function itself. Results are down below in <a href="#figure--pointer-arithmetic">Figure 9</a>.</p><h3 id="up-front-splat"><span>3.3.1</span> Up-front splat
<a href="#up-front-splat"></a></h3><p>First, we can note that the <code>find</code> function <code>splat</code>’s the query from a <code>u32</code> to
a <code>Simd&lt;u32, 8&gt;</code> on each call. It’s slightly nicer (but not really faster,
actually) to splat all the queries
up-front, and then reuse those.</p><div><table><tbody><tr><td><pre tabindex="0"><code><span> 1
</span><span> 2
</span><span> 3
</span><span> 4
</span><span> 5
</span><span> 6
</span><span> 7
</span><span> 8
</span><span> 9
</span><span>10
</span><span>11
</span><span>12
</span><span>13
</span><span>14
</span><span>15
</span><span>16
</span><span>17
</span><span>18
</span><span>19
</span><span>20
</span></code></pre></td><td><pre tabindex="0"><code data-lang="diff"><span><span> pub fn batch_splat&lt;const P: usize&gt;(&amp;self, qb: &amp;[u32; P]) -&gt; [u32; P] {
</span></span><span><span>     let mut k = [0; P];
</span></span><span><span><span>+    let q_simd = qb.map(|q| Simd::&lt;u32, 8&gt;::splat(q));
</span></span></span><span><span><span></span>
</span></span><span><span>     for [o, o2] in self.offsets.array_windows() {
</span></span><span><span>         for i in 0..P {
</span></span><span><span><span>-            let jump_to = self.node(o + k[i]).find      (qb[i]    );
</span></span></span><span><span><span></span><span>+            let jump_to = self.node(o + k[i]).find_splat(q_simd[i]);
</span></span></span><span><span><span></span>             k[i] = k[i] * (B + 1) + jump_to;
</span></span><span><span>             prefetch_index(&amp;self.tree, o2 + k[i]);
</span></span><span><span>         }
</span></span><span><span>     }
</span></span><span><span>
</span></span><span><span>     let o = self.offsets.last().unwrap();
</span></span><span><span>     from_fn(|i| {
</span></span><span><span><span>-        let idx = self.node(o + k[i]).find      (qb[i]    );
</span></span></span><span><span><span></span><span>+        let idx = self.node(o + k[i]).find_splat(q_simd[i]);
</span></span></span><span><span><span></span>         self.get(o + k[i] + idx / N, idx % N)
</span></span><span><span>     })
</span></span><span><span> }
</span></span></code></pre></td></tr></tbody></table></div><p><span><a href="#code-snippet--splat">Code Snippet 21</a>:</span>
<i>Hoisting</i> the <code>splat</code> out of the <i>loop</i> is slightly nicer, but not faster.</p><p>The assembly code for each iteration of the first loop now looks like this:</p><div><table><tbody><tr><td><pre tabindex="0"><code><span> 1
</span><span> 2
</span><span> 3
</span><span> 4
</span><span> 5
</span><span> 6
</span><span> 7
</span><span> 8
</span><span> 9
</span><span>10
</span><span>11
</span><span>12
</span><span>13
</span><span>14
</span><span>15
</span><span>16
</span><span>17
</span></code></pre></td><td><pre tabindex="0"><code data-lang="asm"><span><span><span>movq</span>         <span>(</span><span>%rsp</span><span>,</span><span>%r11</span><span>),</span><span>%r15</span>
</span></span><span><span><span>leaq</span>         <span>(</span><span>%r9</span><span>,</span><span>%r15</span><span>),</span><span>%r12</span>
</span></span><span><span><span>shlq</span>         <span>$6</span><span>,</span> <span>%r12</span>
</span></span><span><span><span>vmovdqa</span>      <span>1536</span><span>(</span><span>%rsp</span><span>,</span><span>%r11</span><span>,</span><span>4</span><span>),</span><span>%ymm0</span>
</span></span><span><span><span>vpcmpgtd</span>     <span>(</span><span>%rsi</span><span>,</span><span>%r12</span><span>),</span> <span>%ymm0</span><span>,</span> <span>%ymm1</span>
</span></span><span><span><span>vpcmpgtd</span>     <span>32</span><span>(</span><span>%rsi</span><span>,</span><span>%r12</span><span>),</span> <span>%ymm0</span><span>,</span> <span>%ymm0</span>
</span></span><span><span><span>vpackssdw</span>    <span>%ymm0</span><span>,</span> <span>%ymm1</span><span>,</span> <span>%ymm0</span>
</span></span><span><span><span>vpmovmskb</span>    <span>%ymm0</span><span>,</span> <span>%r12d</span>
</span></span><span><span><span>popcntl</span>      <span>%r12d</span><span>,</span> <span>%r12d</span>
</span></span><span><span><span>shrl</span>         <span>%r12d</span>
</span></span><span><span><span>movq</span>         <span>%r15</span><span>,</span><span>%r13</span>
</span></span><span><span><span>shlq</span>         <span>$4</span><span>,</span> <span>%r13</span>
</span></span><span><span><span>addq</span>         <span>%r15</span><span>,</span><span>%r13</span>
</span></span><span><span><span>addq</span>         <span>%r12</span><span>,</span><span>%r13</span>
</span></span><span><span><span>movq</span>         <span>%r13</span><span>,(</span><span>%rsp</span><span>,</span><span>%r11</span><span>)</span>
</span></span><span><span><span>shlq</span>         <span>$6</span><span>,</span> <span>%r13</span>
</span></span><span><span><span>prefetcht0</span>   <span>(</span><span>%r10</span><span>,</span><span>%r13</span><span>)</span>
</span></span></code></pre></td></tr></tbody></table></div><p><span>Code Snippet 22:</span>
Assembly code for each iteration of Code Snippet <a href="#orgaceea1d">21</a>. (Actually it's unrolled into two copied of this, but they're identical.)</p><h3 id="byte-based-pointers"><span>3.3.2</span> Byte-based pointers
<a href="#byte-based-pointers"></a></h3><p>Looking at the code above, we see two <code>shlq $6</code> instructions that multiply the
given value by \(64\). That’s because our tree nodes are 64 bytes large, and
hence, to get the \(i\)’th element of the array, we need to read at byte \(64\cdot
i\). For smaller element sizes, there are dedicated read instructions that
inline, say, an index multiplication by 8. But for a stride of 64, the compiler
has to generate ‘manual’ multiplications in the form of a shift.</p><p>Additionally, direct pointer-based lookups can be slightly more efficient here than
array-indexing: when doing <code>self.tree[o + k[i]]</code>, we can effectively pre-compute
the pointer to <code>self.tree[o]</code>, so that only <code>k[i]</code> still has to be added. Let’s
first look at that diff:</p><div><table><tbody><tr><td><pre tabindex="0"><code><span> 1
</span><span> 2
</span><span> 3
</span><span> 4
</span><span> 5
</span><span> 6
</span><span> 7
</span><span> 8
</span><span> 9
</span><span>10
</span><span>11
</span><span>12
</span><span>13
</span><span>14
</span><span>15
</span><span>16
</span><span>17
</span><span>18
</span><span>19
</span><span>20
</span><span>21
</span><span>22
</span><span>23
</span><span>24
</span><span>25
</span><span>26
</span><span>27
</span></code></pre></td><td><pre tabindex="0"><code data-lang="diff"><span><span> pub fn batch_ptr&lt;const P: usize&gt;(&amp;self, qb: &amp;[u32; P]) -&gt; [u32; P] {
</span></span><span><span>     let mut k = [0; P];
</span></span><span><span>     let q_simd = qb.map(|q| Simd::&lt;u32, 8&gt;::splat(q));
</span></span><span><span>
</span></span><span><span><span>+    // offsets[l] is a pointer to self.tree[self.offsets[l]]
</span></span></span><span><span><span>+    let offsets = self.offsets.iter()
</span></span></span><span><span><span>+        .map(|o| unsafe { self.tree.as_ptr().add(*o) })
</span></span></span><span><span><span>+        .collect_vec();
</span></span></span><span><span><span></span>
</span></span><span><span>     for [o, o2] in offsets.array_windows() {
</span></span><span><span>         for i in 0..P {
</span></span><span><span><span>-            let jump_to = self.node(o  +  k[i])  .find_splat(q_simd[i]);
</span></span></span><span><span><span></span><span>+            let jump_to = unsafe { *o.add(k[i]) }.find_splat(q_simd[i]);
</span></span></span><span><span><span></span>             k[i] = k[i] * (B + 1) + jump_to;
</span></span><span><span><span>-            prefetch_index(&amp;self.tree, o2 + k[i]);
</span></span></span><span><span><span></span><span>+            prefetch_ptr(unsafe { o2.add(k[i]) });
</span></span></span><span><span><span></span>         }
</span></span><span><span>     }
</span></span><span><span>
</span></span><span><span>     let o = offsets.last().unwrap();
</span></span><span><span>     from_fn(|i| {
</span></span><span><span><span>-        let idx = self.node(o  +  k[i])  .find_splat(q_simd[i]);
</span></span></span><span><span><span></span><span>+        let idx = unsafe { *o.add(k[i]) }.find_splat(q_simd[i]);
</span></span></span><span><span><span></span><span>-        self.get(o + k[i] + idx / N, idx % N)
</span></span></span><span><span><span></span><span>+        unsafe { *(*o.add(k[i] + idx / N)).data.get_unchecked(idx % N) }
</span></span></span><span><span><span></span>     })
</span></span><span><span> }
</span></span></code></pre></td></tr></tbody></table></div><p><span><a href="#code-snippet--ptr">Code Snippet 23</a>:</span>
Using pointer-based indexing instead of array indexing.</p><p>Now, we can avoid all the multiplications by 64, by just multiplying all <code>k[i]</code>
by 64 to start with:</p><div><table><tbody><tr><td><pre tabindex="0"><code><span> 1
</span><span> 2
</span><span> 3
</span><span> 4
</span><span> 5
</span><span> 6
</span><span> 7
</span><span> 8
</span><span> 9
</span><span>10
</span><span>11
</span><span>12
</span><span>13
</span><span>14
</span><span>15
</span><span>16
</span><span>17
</span><span>18
</span><span>19
</span><span>20
</span><span>21
</span><span>22
</span><span>23
</span><span>24
</span><span>25
</span><span>26
</span><span>27
</span><span>28
</span><span>29
</span></code></pre></td><td><pre tabindex="0"><code data-lang="diff"><span><span> pub fn batch_byte_ptr&lt;const P: usize&gt;(&amp;self, qb: &amp;[u32; P]) -&gt; [u32; P] {
</span></span><span><span>     let mut k = [0; P];
</span></span><span><span>     let q_simd = qb.map(|q| Simd::&lt;u32, 8&gt;::splat(q));
</span></span><span><span>
</span></span><span><span>     let offsets = self
</span></span><span><span>         .offsets
</span></span><span><span>         .iter()
</span></span><span><span>         .map(|o| unsafe { self.tree.as_ptr().add(*o) })
</span></span><span><span>         .collect_vec();
</span></span><span><span>
</span></span><span><span>     for [o, o2] in offsets.array_windows() {
</span></span><span><span>         for i in 0..P {
</span></span><span><span><span>-            let jump_to = unsafe { *o.     add(k[i]) }.find_splat(q_simd[i]);
</span></span></span><span><span><span></span><span>+            let jump_to = unsafe { *o.byte_add(k[i]) }.find_splat(q_simd[i]);
</span></span></span><span><span><span></span><span>-            k[i] = k[i] * (B + 1) + jump_to     ;
</span></span></span><span><span><span></span><span>+            k[i] = k[i] * (B + 1) + jump_to * 64;
</span></span></span><span><span><span></span><span>-            prefetch_ptr(unsafe { o2.     add(k[i]) });
</span></span></span><span><span><span></span><span>+            prefetch_ptr(unsafe { o2.byte_add(k[i]) });
</span></span></span><span><span><span></span>         }
</span></span><span><span>     }
</span></span><span><span>
</span></span><span><span>     let o = offsets.last().unwrap();
</span></span><span><span>     from_fn(|i| {
</span></span><span><span><span>-        let idx = unsafe { *o.     add(k[i]) }.find_splat(q_simd[i]);
</span></span></span><span><span><span></span><span>+        let idx = unsafe { *o.byte_add(k[i]) }.find_splat(q_simd[i]);
</span></span></span><span><span><span></span><span>-        unsafe { *(*o.add(k[i] + idx / N)).data.get_unchecked(idx % N) }
</span></span></span><span><span><span></span><span>+        unsafe { (o.byte_add(k[i]) as *const u32).add(idx).read() }
</span></span></span><span><span><span></span>     })
</span></span><span><span> }
</span></span></code></pre></td></tr></tbody></table></div><p><span><a href="#code-snippet--ptr64">Code Snippet 24</a>:</span>
We multiply <code>k[i]</code> by 64 up-front, and then call <code>byte_add</code> instead of the usual <code>add</code>.</p><p>Indeed, the generated code now goes down from 17 to 15 instructions, and we can
see in <a href="#figure--pointer-arithmetic">Figure 9</a> that this gives a significant speedup!</p><div><table><tbody><tr><td><pre tabindex="0"><code><span> 1
</span><span> 2
</span><span> 3
</span><span> 4
</span><span> 5
</span><span> 6
</span><span> 7
</span><span> 8
</span><span> 9
</span><span>10
</span><span>11
</span><span>12
</span><span>13
</span><span>14
</span><span>15
</span></code></pre></td><td><pre tabindex="0"><code data-lang="asm"><span><span><span>movq</span>         <span>32</span><span>(</span><span>%rsp</span><span>,</span><span>%rdi</span><span>),</span><span>%r8</span>
</span></span><span><span><span>vmovdqa</span>      <span>1568</span><span>(</span><span>%rsp</span><span>,</span><span>%rdi</span><span>,</span><span>4</span><span>),</span><span>%ymm0</span>
</span></span><span><span><span>vpcmpgtd</span>     <span>(</span><span>%rsi</span><span>,</span><span>%r8</span><span>),</span> <span>%ymm0</span><span>,</span> <span>%ymm1</span>
</span></span><span><span><span>vpcmpgtd</span>     <span>32</span><span>(</span><span>%rsi</span><span>,</span><span>%r8</span><span>),</span> <span>%ymm0</span><span>,</span> <span>%ymm0</span>
</span></span><span><span><span>vpackssdw</span>    <span>%ymm0</span><span>,</span> <span>%ymm1</span><span>,</span> <span>%ymm0</span>
</span></span><span><span><span>vpmovmskb</span>    <span>%ymm0</span><span>,</span> <span>%r9d</span>
</span></span><span><span><span>popcntl</span>      <span>%r9d</span><span>,</span> <span>%r9d</span>
</span></span><span><span><span>movq</span>         <span>%r8</span><span>,</span><span>%r10</span>
</span></span><span><span><span>shlq</span>         <span>$4</span><span>,</span> <span>%r10</span>
</span></span><span><span><span>addq</span>         <span>%r8</span><span>,</span><span>%r10</span>
</span></span><span><span><span>shll</span>         <span>$5</span><span>,</span> <span>%r9d</span>
</span></span><span><span><span>andl</span>         <span>$-64</span><span>,</span><span>%r9d</span>
</span></span><span><span><span>addq</span>         <span>%r10</span><span>,</span><span>%r9</span>
</span></span><span><span><span>movq</span>         <span>%r9</span><span>,</span><span>32</span><span>(</span><span>%rsp</span><span>,</span><span>%rdi</span><span>)</span>
</span></span><span><span><span>prefetcht0</span>   <span>(</span><span>%rcx</span><span>,</span><span>%r9</span><span>)</span>
</span></span></code></pre></td></tr></tbody></table></div><p><span><a href="#code-snippet--byte-ptr">Code Snippet 25</a>:</span>
When using byte-based pointers, we avoid some multiplications by 64.</p><h3 id="the-final-version"><span>3.3.3</span> The final version
<a href="#the-final-version"></a></h3><p>One particularity about the code above is the <code>andl $-64,%r9d</code>.
In line 6, the bitmask gets written there. Then in line 7, it’s popcounted.
Life 11 does a <code>shll $5</code>, i.e., a multiplication by 32, which is a combination
of the <code>/2</code> to compensate for the double-popcount and the <code>* 64</code>. Then, it does
the <code>and $-64</code>, where the mask of -64 is <code>111..11000000</code> which ends in 6 zeros.
But we just multiplied by 32, so all this does is zeroing out a single bit, in
case the popcount was odd. But we know for a fact that that can never be, so we
don’t actually need this <code>and</code> instruction.</p><p>To avoid it, we do this <code>/2*64 =&gt; *32</code> optimization manually.</p><div><table><tbody><tr><td><pre tabindex="0"><code><span> 1
</span><span> 2
</span><span> 3
</span><span> 4
</span><span> 5
</span><span> 6
</span><span> 7
</span><span> 8
</span><span> 9
</span><span>10
</span><span>11
</span><span>12
</span><span>13
</span><span>14
</span><span>15
</span><span>16
</span><span>17
</span><span>18
</span><span>19
</span><span>20
</span><span>21
</span><span>22
</span><span>23
</span><span>24
</span><span>25
</span><span>26
</span><span>27
</span><span>28
</span><span>29
</span><span>30
</span><span>31
</span><span>32
</span><span>33
</span><span>34
</span><span>35
</span><span>36
</span><span>37
</span><span>38
</span><span>39
</span><span>40
</span><span>41
</span></code></pre></td><td><pre tabindex="0"><code data-lang="diff"><span><span> pub fn find_splat64(&amp;self, q_simd: Simd&lt;u32, 8&gt;) -&gt; usize {
</span></span><span><span>     let low: Simd&lt;u32, 8&gt; = Simd::from_slice(&amp;self.data[0..N / 2]);
</span></span><span><span>     let high: Simd&lt;u32, 8&gt; = Simd::from_slice(&amp;self.data[N / 2..N]);
</span></span><span><span>     unsafe {
</span></span><span><span>         let q_simd: Simd&lt;i32, 8&gt; = t(q_simd);
</span></span><span><span>         let mask_low = q_simd.simd_gt(t(low));
</span></span><span><span>         let mask_high = q_simd.simd_gt(t(high));
</span></span><span><span>         use std::mem::transmute as t;
</span></span><span><span>         let merged = _mm256_packs_epi32(t(mask_low), t(mask_high));
</span></span><span><span>         let mask = _mm256_movemask_epi8(merged);
</span></span><span><span><span>-        mask.count_ones() as usize / 2
</span></span></span><span><span><span></span><span>+        mask.count_ones() as usize * 32
</span></span></span><span><span><span></span>     }
</span></span><span><span> }
</span></span><span><span>
</span></span><span><span> pub fn batch_byte_ptr&lt;const P: usize&gt;(&amp;self, qb: &amp;[u32; P]) -&gt; [u32; P] {
</span></span><span><span>     let mut k = [0; P];
</span></span><span><span>     let q_simd = qb.map(|q| Simd::&lt;u32, 8&gt;::splat(q));
</span></span><span><span>
</span></span><span><span>     let offsets = self
</span></span><span><span>         .offsets
</span></span><span><span>         .iter()
</span></span><span><span>         .map(|o| unsafe { self.tree.as_ptr().add(*o) })
</span></span><span><span>         .collect_vec();
</span></span><span><span>
</span></span><span><span>     for [o, o2] in offsets.array_windows() {
</span></span><span><span>         for i in 0..P {
</span></span><span><span><span>-            let jump_to = unsafe { *o.byte_add(k[i]) }.find_splat  (q_simd[i]);
</span></span></span><span><span><span></span><span>+            let jump_to = unsafe { *o.byte_add(k[i]) }.find_splat64(q_simd[i]);
</span></span></span><span><span><span></span><span>-            k[i] = k[i] * (B + 1) + jump_to * 64;
</span></span></span><span><span><span></span><span>+            k[i] = k[i] * (B + 1) + jump_to     ;
</span></span></span><span><span><span></span>             prefetch_ptr(unsafe { o2.byte_add(k[i]) });
</span></span><span><span>         }
</span></span><span><span>     }
</span></span><span><span>
</span></span><span><span>     let o = offsets.last().unwrap();
</span></span><span><span>     from_fn(|i| {
</span></span><span><span>         let idx = unsafe { *o.byte_add(k[i]) }.find_splat(q_simd[i]);
</span></span><span><span>         unsafe { (o.byte_add(k[i]) as *const u32).add(idx).read() }
</span></span><span><span>     })
</span></span><span><span> }
</span></span></code></pre></td></tr></tbody></table></div><p><span>Code Snippet 26:</span>
Manually merging <code>/2</code> and <code>*64</code> into <code>*32</code>.</p><p>Again, this gives a small speedup.</p><figure><a href="https://curiouscoding.nl/ox-hugo/6-improvements.svg"><img src="https://curiouscoding.nl/ox-hugo/6-improvements.svg" alt="Figure 9: Results of improving the search function bit by bit. Like before, the improvements are small but consistent. Throughput on 1GB input improves from 31ns/query to 28ns/query."></a><figcaption><p><span>Figure 9: </span>Results of improving the search function bit by bit. Like before, the improvements are small but consistent. Throughput on <code>1GB</code> input improves from <code>31ns/query</code> to <code>28ns/query</code>.</p></figcaption></figure><h2 id="skip-prefetch"><span>3.4</span> Skip prefetch
<a href="#skip-prefetch"></a></h2><p>Now we know that the first three levels of the graph fit in L1 cache, so
probably we can simply skip prefetching for those levels.</p><figure><a href="https://curiouscoding.nl/ox-hugo/7-skip-prefetch.svg"><img src="https://curiouscoding.nl/ox-hugo/7-skip-prefetch.svg" alt="Figure 10: Skipping the prefetch for the first layers is slightly slower."></a><figcaption><p><span>Figure 10: </span>Skipping the prefetch for the first layers is slightly slower.</p></figcaption></figure><p>As it turns out, skipping the prefetch does not help. Probably because the
prefetch is cheap if the data is already available, and there is a small chance
that the data we need was evicted to make room for other things, in which case
the prefetch <em>is</em> useful.</p><h2 id="interleave"><span>3.5</span> Interleave
<a href="#interleave"></a></h2><p>One other observation is that the first few layers are CPU bound, while the last
few layers are memory throughput bound.
By merging the two domains, we should be able to get a higher total throughput.
(Somewhat similar to how for a piece wise linear convex function \(f\), \(f((x+y)/2) &lt;
(f(x)+f(y))/2\) when \(x\) and \(y\) are on different pieces.)
Thus, maybe we could process two batches
of queries at the same time by processing layer \(i\) of one batch at the same
time as layer \(i+L/2\) of the other batch (where \(L\) is the height of the tree).
I implemented this, but unfortunately the result is not faster than what we had.</p><p>Or maybe we can split the work as: interleave the last level of one half
with <em>all but the last</em> level of the other half? Since the last-level memory
read takes most of the time. Also that turns out slower in practice.</p><p>What does give a small speedup: process the first <em>two</em> levels of the next batch
interleaved with the last prefetch of the current batch. Still the result is
only around <code>2ns</code> speedup, while code the (not shown ;") gets significantly more
messy.</p><p>What <em>does</em> work great, is interleaving <em>all</em> layers of the search: when the
tree has \(L\) layers, we can interleave \(L\) batches at a time, and then process
layer \(i\) of the \(i\)’th in-progress batch. Then we ‘shift out’ the completed
batch and store the answers to those queries, and ‘shift in’ a new batch.
This we, completely average the different workloads of all the layers, and
should achieve near-optimal performance given the CPU’s memory bandwidth to L3
and RAM (at least, that’s what I assume is the bottleneck now).</p><details><summary>Click to show code for interleaving.</summary><div><table><tbody><tr><td><pre tabindex="0"><code><span> 1
</span><span> 2
</span><span> 3
</span><span> 4
</span><span> 5
</span><span> 6
</span><span> 7
</span><span> 8
</span><span> 9
</span><span>10
</span><span>11
</span><span>12
</span><span>13
</span><span>14
</span><span>15
</span><span>16
</span><span>17
</span><span>18
</span><span>19
</span><span>20
</span><span>21
</span><span>22
</span><span>23
</span><span>24
</span><span>25
</span><span>26
</span><span>27
</span><span>28
</span><span>29
</span><span>30
</span><span>31
</span><span>32
</span><span>33
</span><span>34
</span><span>35
</span><span>36
</span><span>37
</span><span>38
</span><span>39
</span><span>40
</span><span>41
</span><span>42
</span><span>43
</span><span>44
</span><span>45
</span><span>46
</span><span>47
</span><span>48
</span><span>49
</span><span>50
</span><span>51
</span><span>52
</span><span>53
</span><span>54
</span><span>55
</span><span>56
</span><span>57
</span><span>58
</span><span>59
</span><span>60
</span><span>61
</span><span>62
</span><span>63
</span><span>64
</span><span>65
</span><span>66
</span><span>67
</span><span>68
</span><span>69
</span><span>70
</span><span>71
</span><span>72
</span><span>73
</span><span>74
</span><span>75
</span></code></pre></td><td><pre tabindex="0"><code data-lang="rust"><span><span><span>pub</span><span> </span><span>fn</span> <span>batch_interleave_full_128</span><span>(</span><span>&amp;</span><span>self</span><span>,</span><span> </span><span>qs</span>: <span>&amp;</span><span>[</span><span>u32</span><span>])</span><span> </span>-&gt; <span>Vec</span><span>&lt;</span><span>u32</span><span>&gt;</span><span> </span><span>{</span><span>
</span></span></span><span><span><span>    </span><span>match</span><span> </span><span>self</span><span>.</span><span>offsets</span><span>.</span><span>len</span><span>()</span><span> </span><span>{</span><span>
</span></span></span><span><span><span>        </span><span>// 1 batch of size 128
</span></span></span><span><span><span></span><span>        </span><span>1</span><span> </span><span>=&gt;</span><span> </span><span>self</span><span>.</span><span>batch_interleave_full</span>::<span>&lt;</span><span>128</span><span>,</span><span> </span><span>1</span><span>,</span><span> </span><span>128</span><span>&gt;</span><span>(</span><span>qs</span><span>),</span><span>
</span></span></span><span><span><span>        </span><span>// 2 batches of size 64 in parallel, with product 128
</span></span></span><span><span><span></span><span>        </span><span>2</span><span> </span><span>=&gt;</span><span> </span><span>self</span><span>.</span><span>batch_interleave_full</span>::<span>&lt;</span><span>64</span><span>,</span><span> </span><span>2</span><span>,</span><span> </span><span>128</span><span>&gt;</span><span>(</span><span>qs</span><span>),</span><span>
</span></span></span><span><span><span>        </span><span>// 3 batches of size 32 in parallel with product 96
</span></span></span><span><span><span></span><span>        </span><span>3</span><span> </span><span>=&gt;</span><span> </span><span>self</span><span>.</span><span>batch_interleave_full</span>::<span>&lt;</span><span>32</span><span>,</span><span> </span><span>3</span><span>,</span><span> </span><span>96</span><span>&gt;</span><span>(</span><span>qs</span><span>),</span><span>
</span></span></span><span><span><span>        </span><span>4</span><span> </span><span>=&gt;</span><span> </span><span>self</span><span>.</span><span>batch_interleave_full</span>::<span>&lt;</span><span>32</span><span>,</span><span> </span><span>4</span><span>,</span><span> </span><span>128</span><span>&gt;</span><span>(</span><span>qs</span><span>),</span><span>
</span></span></span><span><span><span>        </span><span>5</span><span> </span><span>=&gt;</span><span> </span><span>self</span><span>.</span><span>batch_interleave_full</span>::<span>&lt;</span><span>16</span><span>,</span><span> </span><span>5</span><span>,</span><span> </span><span>80</span><span>&gt;</span><span>(</span><span>qs</span><span>),</span><span>
</span></span></span><span><span><span>        </span><span>6</span><span> </span><span>=&gt;</span><span> </span><span>self</span><span>.</span><span>batch_interleave_full</span>::<span>&lt;</span><span>16</span><span>,</span><span> </span><span>6</span><span>,</span><span> </span><span>96</span><span>&gt;</span><span>(</span><span>qs</span><span>),</span><span>
</span></span></span><span><span><span>        </span><span>7</span><span> </span><span>=&gt;</span><span> </span><span>self</span><span>.</span><span>batch_interleave_full</span>::<span>&lt;</span><span>16</span><span>,</span><span> </span><span>7</span><span>,</span><span> </span><span>112</span><span>&gt;</span><span>(</span><span>qs</span><span>),</span><span>
</span></span></span><span><span><span>        </span><span>8</span><span> </span><span>=&gt;</span><span> </span><span>self</span><span>.</span><span>batch_interleave_full</span>::<span>&lt;</span><span>16</span><span>,</span><span> </span><span>8</span><span>,</span><span> </span><span>128</span><span>&gt;</span><span>(</span><span>qs</span><span>),</span><span>
</span></span></span><span><span><span>        </span><span>_</span><span> </span><span>=&gt;</span><span> </span><span>panic!</span><span>(</span><span>"Unsupported tree height </span><span>{}</span><span>"</span><span>,</span><span> </span><span>self</span><span>.</span><span>offsets</span><span>.</span><span>len</span><span>()),</span><span>
</span></span></span><span><span><span>    </span><span>}</span><span>
</span></span></span><span><span><span></span><span>}</span><span>
</span></span></span><span><span><span>
</span></span></span><span><span><span></span><span>pub</span><span> </span><span>fn</span> <span>batch_interleave_full</span><span>&lt;</span><span>const</span><span> </span><span>P</span>: <span>usize</span><span>,</span><span> </span><span>const</span><span> </span><span>L</span>: <span>usize</span><span>,</span><span> </span><span>const</span><span> </span><span>PL</span>: <span>usize</span><span>&gt;</span><span>(</span><span>
</span></span></span><span><span><span>    </span><span>&amp;</span><span>self</span><span>,</span><span>
</span></span></span><span><span><span>    </span><span>qs</span>: <span>&amp;</span><span>[</span><span>u32</span><span>],</span><span>
</span></span></span><span><span><span></span><span>)</span><span> </span>-&gt; <span>Vec</span><span>&lt;</span><span>u32</span><span>&gt;</span><span> </span><span>{</span><span>
</span></span></span><span><span><span>    </span><span>assert_eq!</span><span>(</span><span>self</span><span>.</span><span>offsets</span><span>.</span><span>len</span><span>(),</span><span> </span><span>L</span><span>);</span><span>
</span></span></span><span><span><span>
</span></span></span><span><span><span>    </span><span>let</span><span> </span><span>mut</span><span> </span><span>out</span><span> </span><span>=</span><span> </span><span>Vec</span>::<span>with_capacity</span><span>(</span><span>qs</span><span>.</span><span>len</span><span>());</span><span>
</span></span></span><span><span><span>    </span><span>let</span><span> </span><span>mut</span><span> </span><span>ans</span><span> </span><span>=</span><span> </span><span>[</span><span>0</span><span>;</span><span> </span><span>P</span><span>];</span><span>
</span></span></span><span><span><span>
</span></span></span><span><span><span>    </span><span>// Iterate over chunks of size P of queries.
</span></span></span><span><span><span></span><span>    </span><span>// Omitted: initialize
</span></span></span><span><span><span></span><span>    </span><span>let</span><span> </span><span>first_i</span><span> </span><span>=</span><span> </span><span>L</span><span>-</span><span>1</span><span>;</span><span>
</span></span></span><span><span><span>    </span><span>for</span><span> </span><span>chunk</span><span> </span><span>in</span><span> </span><span>qs</span><span>.</span><span>array_chunks</span>::<span>&lt;</span><span>P</span><span>&gt;</span><span>()</span><span> </span><span>{</span><span>
</span></span></span><span><span><span>        </span><span>let</span><span> </span><span>i</span><span> </span><span>=</span><span> </span><span>first_i</span><span>;</span><span>
</span></span></span><span><span><span>
</span></span></span><span><span><span>        </span><span>// Decrement first_i, modulo L.
</span></span></span><span><span><span></span><span>        </span><span>if</span><span> </span><span>first_i</span><span> </span><span>==</span><span> </span><span>0</span><span> </span><span>{</span><span>
</span></span></span><span><span><span>            </span><span>first_i</span><span> </span><span>=</span><span> </span><span>L</span><span>;</span><span>
</span></span></span><span><span><span>        </span><span>}</span><span>
</span></span></span><span><span><span>        </span><span>first_i</span><span> </span><span>-=</span><span> </span><span>1</span><span>;</span><span>
</span></span></span><span><span><span>
</span></span></span><span><span><span>        </span><span>// Process 1 element per chunk, starting at element first_i.
</span></span></span><span><span><span></span><span>        </span><span>// (Omitted: process first up-to L elements.)
</span></span></span><span><span><span></span><span>        </span><span>// Write output and read new queries from index j.
</span></span></span><span><span><span></span><span>        </span><span>let</span><span> </span><span>mut</span><span> </span><span>j</span><span> </span><span>=</span><span> </span><span>0</span><span>;</span><span>
</span></span></span><span><span><span>        </span><span>loop</span><span> </span><span>{</span><span>
</span></span></span><span><span><span>            </span><span>// First L-1 levels: do the usual thing.
</span></span></span><span><span><span></span><span>            </span><span>// The compiler will unroll this loop.
</span></span></span><span><span><span></span><span>            </span><span>for</span><span> </span><span>l</span><span> </span><span>in</span><span> </span><span>0</span><span>..</span><span>L</span><span> </span><span>-</span><span> </span><span>1</span><span> </span><span>{</span><span>
</span></span></span><span><span><span>                </span><span>let</span><span> </span><span>jump_to</span><span> </span><span>=</span><span> </span><span>unsafe</span><span> </span><span>{</span><span> </span><span>*</span><span>offsets</span><span>[</span><span>l</span><span>].</span><span>byte_add</span><span>(</span><span>k</span><span>[</span><span>i</span><span>])</span><span> </span><span>}.</span><span>find_splat64</span><span>(</span><span>q_simd</span><span>[</span><span>i</span><span>]);</span><span>
</span></span></span><span><span><span>                </span><span>k</span><span>[</span><span>i</span><span>]</span><span> </span><span>=</span><span> </span><span>k</span><span>[</span><span>i</span><span>]</span><span> </span><span>*</span><span> </span><span>(</span><span>B</span><span> </span><span>+</span><span> </span><span>1</span><span>)</span><span> </span><span>+</span><span> </span><span>jump_to</span><span>;</span><span>
</span></span></span><span><span><span>                </span><span>prefetch_ptr</span><span>(</span><span>unsafe</span><span> </span><span>{</span><span> </span><span>offsets</span><span>[</span><span>l</span><span> </span><span>+</span><span> </span><span>1</span><span>].</span><span>byte_add</span><span>(</span><span>k</span><span>[</span><span>i</span><span>])</span><span> </span><span>});</span><span>
</span></span></span><span><span><span>                </span><span>i</span><span> </span><span>+=</span><span> </span><span>1</span><span>;</span><span>
</span></span></span><span><span><span>            </span><span>}</span><span>
</span></span></span><span><span><span>
</span></span></span><span><span><span>            </span><span>// Last level: read answer.
</span></span></span><span><span><span></span><span>            </span><span>ans</span><span>[</span><span>j</span><span>]</span><span> </span><span>=</span><span> </span><span>{</span><span>
</span></span></span><span><span><span>                </span><span>let</span><span> </span><span>idx</span><span> </span><span>=</span><span> </span><span>unsafe</span><span> </span><span>{</span><span> </span><span>*</span><span>ol</span><span>.</span><span>byte_add</span><span>(</span><span>k</span><span>[</span><span>i</span><span>])</span><span> </span><span>}.</span><span>find_splat</span><span>(</span><span>q_simd</span><span>[</span><span>i</span><span>]);</span><span>
</span></span></span><span><span><span>                </span><span>unsafe</span><span> </span><span>{</span><span> </span><span>(</span><span>ol</span><span>.</span><span>byte_add</span><span>(</span><span>k</span><span>[</span><span>i</span><span>])</span><span> </span><span>as</span><span> </span><span>*</span><span>const</span><span> </span><span>u32</span><span>).</span><span>add</span><span>(</span><span>idx</span><span>).</span><span>read</span><span>()</span><span> </span><span>}</span><span>
</span></span></span><span><span><span>            </span><span>};</span><span>
</span></span></span><span><span><span>            </span><span>// Last level: reset index, and read new query.
</span></span></span><span><span><span></span><span>            </span><span>k</span><span>[</span><span>i</span><span>]</span><span> </span><span>=</span><span> </span><span>0</span><span>;</span><span>
</span></span></span><span><span><span>            </span><span>q_simd</span><span>[</span><span>i</span><span>]</span><span> </span><span>=</span><span> </span><span>Simd</span>::<span>splat</span><span>(</span><span>chunk</span><span>[</span><span>j</span><span>]);</span><span>
</span></span></span><span><span><span>
</span></span></span><span><span><span>            </span><span>i</span><span> </span><span>+=</span><span> </span><span>1</span><span>;</span><span>
</span></span></span><span><span><span>            </span><span>j</span><span> </span><span>+=</span><span> </span><span>1</span><span>;</span><span>
</span></span></span><span><span><span>
</span></span></span><span><span><span>            </span><span>if</span><span> </span><span>i</span><span> </span><span>&gt;</span><span> </span><span>PL</span><span> </span><span>-</span><span> </span><span>L</span><span> </span><span>{</span><span>
</span></span></span><span><span><span>                </span><span>break</span><span>;</span><span>
</span></span></span><span><span><span>            </span><span>}</span><span>
</span></span></span><span><span><span>        </span><span>}</span><span>
</span></span></span><span><span><span>        </span><span>// (Omitted: process last up-to L elements.)
</span></span></span><span><span><span></span><span>
</span></span></span><span><span><span>        </span><span>out</span><span>.</span><span>extend_from_slice</span><span>(</span><span>&amp;</span><span>ans</span><span>);</span><span>
</span></span></span><span><span><span>    </span><span>}</span><span>
</span></span></span><span><span><span>
</span></span></span><span><span><span>    </span><span>out</span><span>
</span></span></span><span><span><span></span><span>}</span><span>
</span></span></span></code></pre></td></tr></tbody></table></div><p><span>Code Snippet 27:</span>
In code, we interleave all layers by compiling a separate function for each height of the tree. Then the compiler can unroll the loop over the layers. There is a bunch of overhead in the code for the first and last iterations that's omitted.</p></details><figure><a href="https://curiouscoding.nl/ox-hugo/8-interleave.svg"><img src="https://curiouscoding.nl/ox-hugo/8-interleave.svg" alt="Figure 11: Interleaving all layers of the search binary search improves throughput from 29ns/query to 24ns/query."></a><figcaption><p><span>Figure 11: </span>Interleaving all layers of the search binary search improves throughput from <code>29ns/query</code> to <code>24ns/query</code>.</p></figcaption></figure><h2 id="optimizing-the-tree-layout"><span>4</span> Optimizing the tree layout
<a href="#optimizing-the-tree-layout"></a></h2><h2 id="left-tree"><span>4.1</span> Left-tree
<a href="#left-tree"></a></h2><p>So far, every internal node of the tree stores the minimum of the subtree on
it’s right (<a href="#figure--stree-full">Figure 3</a>, reproduced below).</p><figure><a href="https://curiouscoding.nl/ox-hugo/full.svg"><img src="https://curiouscoding.nl/ox-hugo/full.svg" alt="Figure 12: Usually in B+ trees, each node stores the minimum of it’s right subtree. Let’s call this a right (S+/B+) tree."></a><figcaption><p><span>Figure 12: </span>Usually in B+ trees, each node stores the minimum of it’s right subtree. Let’s call this a <em>right</em> (S+/B+) tree.</p></figcaption></figure><p>This turns out somewhat inefficient when searching values that are exactly in
between two subtrees (as <em>also</em> already suggested by Algorithmica), such as
\(5.5\). In that case, the search descends into the
leftmost (green) subtree with node \([2, 4]\). Then, it goes to the rightmost
(red) node \([4,5]\). There, we realize \(5.5 &gt; 5\), and thus we need the next value
in the red layer (which is stored as a single array), which is \(6\). The problem
now is that the red tree nodes exactly correspond to cache lines, and thus, the
\(6\) will be in a new cache line that needs to be fetched from memory.</p><p>Now consider the <em>left-max</em> tree below:</p><figure><a href="https://curiouscoding.nl/ox-hugo/flipped.svg"><img src="https://curiouscoding.nl/ox-hugo/flipped.svg" alt="Figure 13: In the left-max S+ tree, each internal node contains the maximum of its left subtree."></a><figcaption><p><span>Figure 13: </span>In the <em>left-max</em> S+ tree, each internal node contains the maximum of its <em>left</em> subtree.</p></figcaption></figure><p>Now if we search for \(5.5\), we descend into the middle subtree rooted at
\([7,9]\). Then we go left to the \([6,7]\) node, and end up reading \(6\) as the
first value \(\geq 5.5\). Now, the search directly steers toward the node
that actually contains the answer, instead of the one just before.</p><figure><a href="https://curiouscoding.nl/ox-hugo/9-left-max-tree.svg"><img src="https://curiouscoding.nl/ox-hugo/9-left-max-tree.svg" alt="Figure 14: The left-S tree brings runtime down from 24ns/query for the interleaved version to 22ns/query now."></a><figcaption><p><span>Figure 14: </span>The left-S tree brings runtime down from <code>24ns/query</code> for the interleaved version to <code>22ns/query</code> now.</p></figcaption></figure><h2 id="memory-layouts"><span>4.2</span> Memory layouts
<a href="#memory-layouts"></a></h2><p>Let’s now consider some alternative memory layouts.
So far, we were packing all layers in forward order, but the Algorithmica post
actually stores them in reverse, so we’ll try that too. The query code is
exactly the same, since the order of the layers is already encoded into the offsets.</p><p>Another potential improvement is to always store a <em>full</em> array. This may seem
very inefficient, but is actually not that bad when we make sure to use
uninitialized memory. In that case, untouched memory pages will simply never be
mapped, so that we waste on average only about 2MB
per layer when hugepages are enabled, and 14MB when there are 7 layers and the
entire array takes 1GB.</p><figure><a href="https://curiouscoding.nl/ox-hugo/layouts.svg"><img src="https://curiouscoding.nl/ox-hugo/layouts.svg" alt="Figure 15: So far we have been using the packed layout. We now also try the reversed layout as used by Algorithmica, and the full layout that allows simple arithmetic for indexing."></a><figcaption><p><span>Figure 15: </span>So far we have been using the packed layout. We now also try the <em>reversed</em> layout as used by Algorithmica, and the <em>full</em> layout that allows simple arithmetic for indexing.</p></figcaption></figure><p>A benefit of storing the full array is that instead of using the offsets, we can
simply compute the index in the next layer directly, as we did for the
Eytzinger search.</p><div><table><tbody><tr><td><pre tabindex="0"><code><span> 1
</span><span> 2
</span><span> 3
</span><span> 4
</span><span> 5
</span><span> 6
</span><span> 7
</span><span> 8
</span><span> 9
</span><span>10
</span><span>11
</span><span>12
</span><span>13
</span><span>14
</span><span>15
</span><span>16
</span><span>17
</span><span>18
</span><span>19
</span><span>20
</span><span>21
</span></code></pre></td><td><pre tabindex="0"><code data-lang="diff"><span><span> pub fn batch_ptr3_full&lt;const P: usize&gt;(&amp;self, qb: &amp;[u32; P]) -&gt; [u32; P] {
</span></span><span><span>     let mut k = [0; P];
</span></span><span><span>     let q_simd = qb.map(|q| Simd::&lt;u32, 8&gt;::splat(q));
</span></span><span><span>
</span></span><span><span><span>+    let o = self.tree.as_ptr();
</span></span></span><span><span><span></span>
</span></span><span><span><span>-    for [o, o2] in offsets.array_windows() {
</span></span></span><span><span><span></span><span>+    for _l      in 0..self.offsets.len() - 1 {
</span></span></span><span><span><span></span>         for i in 0..P {
</span></span><span><span>             let jump_to = unsafe { *o.byte_add(k[i]) }.find_splat64(q_simd[i]);
</span></span><span><span><span>-            k[i] = k[i] * (B + 1) + jump_to     ;
</span></span></span><span><span><span></span><span>+            k[i] = k[i] * (B + 1) + jump_to + 64;
</span></span></span><span><span><span></span>             prefetch_ptr(unsafe { o.byte_add(k[i]) });
</span></span><span><span>         }
</span></span><span><span>     }
</span></span><span><span>
</span></span><span><span>     from_fn(|i| {
</span></span><span><span>         let idx = unsafe { *o.byte_add(k[i]) }.find_splat(q_simd[i]);
</span></span><span><span>         unsafe { (o.byte_add(k[i]) as *const u32).add(idx).read() }
</span></span><span><span>     })
</span></span><span><span> }
</span></span></code></pre></td></tr></tbody></table></div><p><span>Code Snippet 28:</span>
When storing the array in full, we can drop the per-layer offsets and instead compute indices directly.</p><figure><a href="https://curiouscoding.nl/ox-hugo/9-params.svg"><img src="https://curiouscoding.nl/ox-hugo/9-params.svg" alt="Figure 16: Comparison with reverse and full memory layout, and full memory layout with using a dedicated _full search that computes indices directly."></a><figcaption><p><span>Figure 16: </span>Comparison with reverse and full memory layout, and full memory layout with using a dedicated <code>_full</code> search that computes indices directly.</p></figcaption></figure><p>As it turns out, neither of those layouts improves performance, and so we will
not use them going forward.</p><h2 id="node-size-b-15"><span>4.3</span> Node size \(B=15\)
<a href="#node-size-b-15"></a></h2><p>We can also try storing only 15 values per node, so that the branching factor
is 16. This has the benefit of making the multiplication by \(B+1\) (17 so far)
slightly simpler, since it replaces <code>x = (x&lt;&lt;4)+x</code> by <code>x = x&lt;&lt;4</code>.</p><figure><a href="https://curiouscoding.nl/ox-hugo/10-base15.svg"><img src="https://curiouscoding.nl/ox-hugo/10-base15.svg" alt="Figure 17: Storing 15 values per node. The lines in the bottom part of the plot show the overhead that each data structure has relative to the size of the input, capped at 1 (which corresponds to take double the size)."></a><figcaption><p><span>Figure 17: </span>Storing 15 values per node. The lines in the bottom part of the plot show the overhead that each data structure has relative to the size of the input, capped at 1 (which corresponds to take double the size).</p></figcaption></figure><p>When the tree has up to 5 layers and the data fits in L3 cache, using \(B=15\) is
indeed slightly faster when the number of layers in the tree is the same. On the
other hand, the lower branching factor of \(16\) requires an additional layer for smaller sizes than
when using branching factor \(17\). When the input is much larger than L3 cache
the speedup disappears, because RAM throughput becomes a common bottleneck.</p><h3 id="data-structure-size"><span>4.3.1</span> Data structure size
<a href="#data-structure-size"></a></h3><p>Plain binary search and the Eytzinger layout have pretty much no overhead.
Our S+ tree so far has around \(1/16=6.25\%\) overhead: \(1/17\) of the
values in the final layer is duplicated in the layer above, and \(1/17\) of
<em>those</em> is duplicated again, and so on, for a total of \(1/17 + 1/17^2 + \cdots =
1/16\).</p><p>Using node size \(15\) instead, increases the overhead:
Each node now only stores \(15\) instead of \(16\) elements, so that we already have
an overhead of \(1/15\). Furthermore the reduced branching factor increases the
duplication overhead fro \(1/16\) to \(1/15\) as well, for a total overhead of \(2/15
= 13.3\%\), which matches the dashed blue line in <a href="#figure--b15">Figure 17</a>.</p><h2 id="summary"><span>4.4</span> Summary
<a href="#summary"></a></h2><figure><a href="https://curiouscoding.nl/ox-hugo/11-summary.svg"><img src="https://curiouscoding.nl/ox-hugo/11-summary.svg" alt="Figure 18: A summary of all the improvements we made so far."></a><figcaption><p><span>Figure 18: </span>A summary of all the improvements we made so far.</p></figcaption></figure><p>Of all the improvements so far, only the interleaving is maybe a bit too much:
it is the only method that does not work batch-by-batch, but really benefits
from having the full input at once. And also its code is three times longer
than the plain batched query methods because the first and last few
iterations of each loop are handled separately.</p><h2 id="prefix-partitioning"><span>5</span> Prefix partitioning
<a href="#prefix-partitioning"></a></h2><p>So far, we’ve been doing a purely <em>comparison-based search</em>.
Now, it is time for something new: <em>partitioning</em> the input values.</p><p>The simplest form of the idea is to simply partition values by their top \(b\)
bits, into \(2^b\) parts. Then we can build \(2^b\) independent search trees and
search each query in one of them. If \(b=12\), this saves the first two levels of
the search (or slightly less, actually, since \(2^{12} = 16^3 &lt; 17^3\)).</p><h2 id="full-layout"><span>5.1</span> Full layout
<a href="#full-layout"></a></h2><p>In memory, we can store these trees very similar to the <em>full</em> layout we had
before, with the main differences that the first few layers are skipped and that
now there will be padding at the end of each part, rather than once at the end.</p><figure><a href="https://curiouscoding.nl/ox-hugo/prefix.svg"><img src="https://curiouscoding.nl/ox-hugo/prefix.svg" alt="Figure 19: The full partitioned layout concatenates the full trees for all parts ‘horizontally’. As a new detail, when a part is not full, the smallest value of the next part is appended in the leaf layer."></a><figcaption><p><span>Figure 19: </span>The <em>full</em> partitioned layout concatenates the full trees for all parts ‘horizontally’. As a new detail, when a part is not full, the smallest value of the next part is appended in the leaf layer.</p></figcaption></figure><p>For some choices of \(b\), it could happen that up to \(15/16\) of each tree is
padding. To reduce this overhead, we attempt to shrink \(b\) while keeping the
height of all trees the same: as long as all pairs of adjacent trees would
fit together in the same space, we decrease \(b\) by one. This way, all parts will
be filled for at least \(50\%\) when the elements are evenly distributed.</p><p>Once construction is done, the code for querying is very similar to before: we
only have to start the search for each query at the index of its part, given by
<code>q &gt;&gt; shift</code> for some value of <code>shift</code>, rather than at index \(0\).</p><div><table><tbody><tr><td><pre tabindex="0"><code><span> 1
</span><span> 2
</span><span> 3
</span><span> 4
</span><span> 5
</span><span> 6
</span><span> 7
</span><span> 8
</span><span> 9
</span><span>10
</span><span>11
</span><span>12
</span><span>13
</span><span>14
</span><span>15
</span><span>16
</span><span>17
</span><span>18
</span><span>19
</span><span>20
</span><span>21
</span><span>22
</span><span>23
</span><span>24
</span><span>25
</span><span>26
</span><span>27
</span><span>28
</span><span>29
</span></code></pre></td><td><pre tabindex="0"><code data-lang="diff"><span><span> pub fn search_prefix&lt;const P: usize&gt;(&amp;self, qb: &amp;[u32; P]) -&gt; [u32; P] {
</span></span><span><span>     let offsets = self
</span></span><span><span>         .offsets
</span></span><span><span>         .iter()
</span></span><span><span>         .map(|o| unsafe { self.tree.as_ptr().add(*o) })
</span></span><span><span>         .collect_vec();
</span></span><span><span>
</span></span><span><span>     // Initial parts, and prefetch them.
</span></span><span><span>     let o0 = offsets[0];
</span></span><span><span><span>-    let mut k = [0; P];
</span></span></span><span><span><span></span><span>+    let mut k = qb.map(|q| {
</span></span></span><span><span><span>+        (q as usize &gt;&gt; self.shift) * 64
</span></span></span><span><span><span>+    });
</span></span></span><span><span><span></span>     let q_simd = qb.map(|q| Simd::&lt;u32, 8&gt;::splat(q));
</span></span><span><span>
</span></span><span><span>     for [o, o2] in offsets.array_windows() {
</span></span><span><span>         for i in 0..P {
</span></span><span><span>             let jump_to = unsafe { *o.byte_add(k[i]) }.find_splat64(q_simd[i]);
</span></span><span><span>             k[i] = k[i] * (B + 1) + jump_to;
</span></span><span><span>             prefetch_ptr(unsafe { o2.byte_add(k[i]) });
</span></span><span><span>         }
</span></span><span><span>     }
</span></span><span><span>
</span></span><span><span>     let o = offsets.last().unwrap();
</span></span><span><span>     from_fn(|i| {
</span></span><span><span>         let idx = unsafe { *o.byte_add(k[i]) }.find_splat(q_simd[i]);
</span></span><span><span>         unsafe { (o.byte_add(k[i]) as *const u32).add(idx).read() }
</span></span><span><span>     })
</span></span><span><span> }
</span></span></code></pre></td></tr></tbody></table></div><p><span>Code Snippet 29:</span>
Searching the full layout of the partitioned tree starts in the partition in which each query belongs.</p><figure><a href="https://curiouscoding.nl/ox-hugo/20-prefix.svg"><img src="https://curiouscoding.nl/ox-hugo/20-prefix.svg" alt="Figure 20: The ‘simple’ partitioned tree, for (b_{textrm{max}}in {4,8,12,16,20}), shown as dotted lines."></a><figcaption><p><span>Figure 20: </span>The ‘simple’ partitioned tree, for (b_{textrm{max}}in {4,8,12,16,20}), shown as dotted lines.</p></figcaption></figure><p>We see that indeed, the partitioned tree has a space overhead varying between
\(0\) and \(1\), making this not yet useful in practice.
Larger \(b\) reduce the height of the remaining trees, and indeed we
see that queries are faster for larger \(b\). Especially for small trees there is
a significant speedup over interleaving. Somewhat surprisingly, none of the
partition sizes has faster queries than interleaving for large inputs. Also
important to note is that while partitioning is very fast for sizes up to L1
cache, this is only possible because they have \(\gg 1\) space overhead.</p><h2 id="compact-subtrees"><span>5.2</span> Compact subtrees
<a href="#compact-subtrees"></a></h2><p>Just like we used the <em>packed</em> layout before, we can also do that now, by simply
concatenating the representation of all packed subtrees.
We ensure that all subtrees are still padded into the same total size, but now
we only add as much padding as needed for the largest part, rather than padding
to <em>full</em> trees. Then, we give each tree the same layout in memory.</p><p>We’ll have offsets \(o_\ell\) of where each layer starts in the first tree, and we
store the constant size of the trees. That way, we can easily index each layer
of each part.</p><figure><a href="https://curiouscoding.nl/ox-hugo/prefix-compact.svg"><img src="https://curiouscoding.nl/ox-hugo/prefix-compact.svg" alt="Figure 21: Compared to before, Figure 19, the lowest level of each subtree now only takes 2 instead of 3 nodes."></a><figcaption><p><span>Figure 21: </span>Compared to before, <a href="#figure--prefix">Figure 19</a>, the lowest level of each subtree now only takes 2 instead of 3 nodes.</p></figcaption></figure><p>The code for querying does become slightly more complicated. Now, we must
explicitly track the part that each query belongs to, and compute all indices
based on the layer offset, the in-layer offset <code>k[i]</code>, <em>and</em> the part offset.</p><div><table><tbody><tr><td><pre tabindex="0"><code><span> 1
</span><span> 2
</span><span> 3
</span><span> 4
</span><span> 5
</span><span> 6
</span><span> 7
</span><span> 8
</span><span> 9
</span><span>10
</span><span>11
</span><span>12
</span><span>13
</span><span>14
</span><span>15
</span><span>16
</span><span>17
</span><span>18
</span><span>19
</span><span>20
</span><span>21
</span><span>22
</span><span>23
</span><span>24
</span><span>25
</span><span>26
</span><span>27
</span><span>28
</span><span>29
</span><span>30
</span><span>31
</span><span>32
</span><span>33
</span><span>34
</span></code></pre></td><td><pre tabindex="0"><code data-lang="diff"><span><span> pub fn search&lt;const P: usize&gt;(&amp;self, qb: &amp;[u32; P]) -&gt; [u32; P] {
</span></span><span><span>     let offsets = self
</span></span><span><span>         .offsets
</span></span><span><span>         .iter()
</span></span><span><span>         .map(|o| unsafe { self.tree.as_ptr().add(*o) })
</span></span><span><span>         .collect_vec();
</span></span><span><span>
</span></span><span><span>     // Initial parts, and prefetch them.
</span></span><span><span>     let o0 = offsets[0];
</span></span><span><span><span>+    let mut k: [usize; P] = [0; P];
</span></span></span><span><span><span>+    let parts: [usize; P] = qb.map(|q| {
</span></span></span><span><span><span>+        // byte offset of the part.
</span></span></span><span><span><span>+        (q as usize &gt;&gt; self.shift) * self.bpp * 64
</span></span></span><span><span><span>+    });
</span></span></span><span><span><span></span>     let q_simd = qb.map(|q| Simd::&lt;u32, 8&gt;::splat(q));
</span></span><span><span>
</span></span><span><span>     for [o, o2] in offsets.array_windows() {
</span></span><span><span>         for i in 0..P {
</span></span><span><span><span>-            let jump_to = unsafe { *o.byte_add(           k[i]) }.find_splat64(q_simd[i]);
</span></span></span><span><span><span></span><span>+            let jump_to = unsafe { *o.byte_add(parts[i] + k[i]) }.find_splat64(q_simd[i]);
</span></span></span><span><span><span></span>             k[i] = k[i] * (B + 1) + jump_to;
</span></span><span><span><span>-            prefetch_ptr(unsafe { o2.byte_add(           k[i]) });
</span></span></span><span><span><span></span><span>+            prefetch_ptr(unsafe { o2.byte_add(parts[i] + k[i]) });
</span></span></span><span><span><span></span>         }
</span></span><span><span>     }
</span></span><span><span>
</span></span><span><span>     let o = offsets.last().unwrap();
</span></span><span><span>     from_fn(|i| {
</span></span><span><span><span>-        let idx = unsafe { *o.byte_add(           k[i]) }.find_splat(q_simd[i]);
</span></span></span><span><span><span></span><span>+        let idx = unsafe { *o.byte_add(parts[i] + k[i]) }.find_splat(q_simd[i]);
</span></span></span><span><span><span></span><span>-        unsafe { (o.byte_add(           k[i]) as *const u32).add(idx).read() }
</span></span></span><span><span><span></span><span>+        unsafe { (o.byte_add(parts[i] + k[i]) as *const u32).add(idx).read() }
</span></span></span><span><span><span></span>     })
</span></span><span><span> }
</span></span></code></pre></td></tr></tbody></table></div><p><span>Code Snippet 30:</span>
The indexing for the packed subtrees requires explicitly tracking the part of each query. This slows things down a bit.</p><figure><a href="https://curiouscoding.nl/ox-hugo/21-compact.svg"><img src="https://curiouscoding.nl/ox-hugo/21-compact.svg" alt="Figure 22: Compared to the the simple/full layout before (dark blue dots for (b=16)), the compact layout (e.g. red dots for (b=16)) consistently uses less memory, but is slightly slower."></a><figcaption><p><span>Figure 22: </span>Compared to the the simple/full layout before (dark blue dots for (b=16)), the compact layout (e.g. red dots for (b=16)) consistently uses less memory, but is slightly slower.</p></figcaption></figure><p>For fixed \(b_{\textrm{max}}\), memory overhead of the compact layout is small as
long as the input is sufficiently large and the trees have sufficiently many
layers. Thus, this tree could be practical.
Unfortunately though, querying them is slightly slower than before,
because we must explicitly track the part of each query.</p><h2 id="the-best-of-both-compact-first-level"><span>5.3</span> The best of both: compact first level
<a href="#the-best-of-both-compact-first-level"></a></h2><p>As we just saw, storing the trees one by one slows queries down, so we would
like to avoid that. But on the other hand, the full layout can waste space.</p><p>Here, we combine the two ideas. We would like to store the <em>horizontal</em>
concatenation of the packed trees (each packed to the same size), but this is
complicated, because then levels would have a non-constant branching factor.
Instead, we can fully omit the last few (level 2) subtrees from each
tree, and pad those subtrees that <em>are</em> present to full subtrees.
This way, only the first level has a configurable branching factor \(B_1\), which we can
simply store after construction is done.</p><p>This layout takes slightly more space than before because the subtrees must
be full, but the overhead should typically be on the order of \(1/16\),
since (for uniform data) each tree will have \(\geq 9\) subtrees, of which only
the last is not full.</p><figure><a href="https://curiouscoding.nl/ox-hugo/prefix-l1.svg"><img src="https://curiouscoding.nl/ox-hugo/prefix-l1.svg" alt="Figure 23: We can also store the horizontal concatenation of all trees. Here, the number of subtrees can be fixed to be less than (B+1), and is (2) instead of (B+1=3). Although not shown, deeper layers must always be full and have a (B+1) branching factor."></a><figcaption><p><span>Figure 23: </span>We can also store the horizontal concatenation of all trees. Here, the number of subtrees can be fixed to be less than (B+1), and is (2) instead of (B+1=3). Although not shown, deeper layers must always be full and have a (B+1) branching factor.</p></figcaption></figure><div><table><tbody><tr><td><pre tabindex="0"><code><span> 1
</span><span> 2
</span><span> 3
</span><span> 4
</span><span> 5
</span><span> 6
</span><span> 7
</span><span> 8
</span><span> 9
</span><span>10
</span><span>11
</span><span>12
</span><span>13
</span><span>14
</span><span>15
</span><span>16
</span><span>17
</span><span>18
</span><span>19
</span><span>20
</span><span>21
</span><span>22
</span><span>23
</span><span>24
</span><span>25
</span><span>26
</span><span>27
</span><span>28
</span><span>29
</span><span>30
</span><span>31
</span><span>32
</span><span>33
</span><span>34
</span><span>35
</span><span>36
</span><span>37
</span><span>38
</span></code></pre></td><td><pre tabindex="0"><code data-lang="diff"><span><span> pub fn search_b1&lt;const P: usize&gt;(&amp;self, qb: &amp;[u32; P]) -&gt; [u32; P] {
</span></span><span><span>     let offsets = self
</span></span><span><span>         .offsets
</span></span><span><span>         .iter()
</span></span><span><span>         .map(|o| unsafe { self.tree.as_ptr().add(*o) })
</span></span><span><span>         .collect_vec();
</span></span><span><span>
</span></span><span><span>     let o0 = offsets[0];
</span></span><span><span>     let mut k: [usize; P] = qb.map(|q| {
</span></span><span><span>          (q as usize &gt;&gt; self.shift) * 64
</span></span><span><span>     });
</span></span><span><span>     let q_simd = qb.map(|q| Simd::&lt;u32, 8&gt;::splat(q));
</span></span><span><span>
</span></span><span><span><span>-    for         [o, o2]  in offsets.array_windows()        {
</span></span></span><span><span><span></span><span>+    if let Some([o1, o2]) = offsets.array_windows().next() {
</span></span></span><span><span><span></span>         for i in 0..P {
</span></span><span><span>             let jump_to = unsafe { *o.byte_add(k[i]) }.find_splat64(q_simd[i]);
</span></span><span><span><span>-            k[i] = k[i] * (B + 1) + jump_to;
</span></span></span><span><span><span></span><span>+            k[i] = k[i] * self.b1 + jump_to;
</span></span></span><span><span><span></span>             prefetch_ptr(unsafe { o2.byte_add(k[i]) });
</span></span><span><span>         }
</span></span><span><span>     }
</span></span><span><span>
</span></span><span><span><span>-    for [o, o2] in offsets     .array_windows() {
</span></span></span><span><span><span></span><span>+    for [o, o2] in offsets[1..].array_windows() {
</span></span></span><span><span><span></span>         for i in 0..P {
</span></span><span><span>             let jump_to = unsafe { *o.byte_add(k[i]) }.find_splat64(q_simd[i]);
</span></span><span><span>             k[i] = k[i] * (B + 1) + jump_to;
</span></span><span><span>             prefetch_ptr(unsafe { o2.byte_add(k[i]) });
</span></span><span><span>         }
</span></span><span><span>     }
</span></span><span><span>
</span></span><span><span>     let o = offsets.last().unwrap();
</span></span><span><span>     from_fn(|i| {
</span></span><span><span>         let idx = unsafe { *o.byte_add(k[i]) }.find_splat(q_simd[i]);
</span></span><span><span>         unsafe { (o.byte_add(k[i]) as *const u32).add(idx).read() }
</span></span><span><span>     })
</span></span><span><span> }
</span></span></code></pre></td></tr></tbody></table></div><p><span>Code Snippet 31:</span>
Now, the code is simple again, in that we don't need to explicitly track part indices. All that changes is that we handle the first iteration of the for loop separately, and use branching factor <code>self.b1</code> instead of <code>B+1</code> there.</p><figure><a href="https://curiouscoding.nl/ox-hugo/22-l1.svg"><img src="https://curiouscoding.nl/ox-hugo/22-l1.svg" alt="Figure 24: When compressing the first level, space usage is very similar to the compact layout before, and query speed is as fast as the full layout before."></a><figcaption><p><span>Figure 24: </span>When compressing the first level, space usage is very similar to the compact layout before, and query speed is as fast as the full layout before.</p></figcaption></figure><h2 id="overlapping-trees"><span>5.4</span> Overlapping trees
<a href="#overlapping-trees"></a></h2><p>A drawback of all the above methods is that memory usage is heavily influenced by the
largest part, since all parts must be at least as large. This is especially a
problem when the distribution of part sizes is very skewed.
We can avoid this by sharing storage between adjacent trees.
Let \(S_p\) be the number of subtrees for each part \(p\), and \(S_{max} = \max_p S_p\).
Then, we can define the <em>overlap</em> \(0\leq v\leq B\), and append only
\(B_1 = S_{max}-v\) new subtrees for each new part, rather than \(S_{max}\) as we
did before.
The values for each part are then simply appended where the previous part left
off, unless that subtree is ‘out-of-reach’ for the current part, in which
case first some padding is added.
This way, consecutive
parts can overlap and exchange memory, and we can somewhat ‘buffer’ the effect
of large parts.</p><figure><a href="https://curiouscoding.nl/ox-hugo/prefix-overlapping.svg"><img src="https://curiouscoding.nl/ox-hugo/prefix-overlapping.svg" alt="Figure 25: In this example, the third tree has (6) values in ([8, 12)) and requires (S_{max}=3) subtrees. We have an overlap of (v=1), so that for each additional tree, only (2) subtrees are added. We add padding elements in grey to ensure all elements are reachable from their own tree."></a><figcaption><p><span>Figure 25: </span>In this example, the third tree has (6) values in ([8, 12)) and requires (S_{max}=3) subtrees. We have an overlap of (v=1), so that for each additional tree, only (2) subtrees are added. We add padding elements in grey to ensure all elements are reachable from their own tree.</p></figcaption></figure><p>When the overlap is \(1\), as in the example above, the nodes in the first layer
each contain the maximum value of \(B\) subtrees. When the overlap is larger than
\(1\), the nodes in the first layer would contain overlapping values. Instead, we
store a single list of values, in which we can do <em>unaligned</em> reads to get the
right slice of \(B\) values that we need.</p><div><table><tbody><tr><td><pre tabindex="0"><code><span> 1
</span><span> 2
</span><span> 3
</span><span> 4
</span><span> 5
</span><span> 6
</span><span> 7
</span><span> 8
</span><span> 9
</span><span>10
</span><span>11
</span><span>12
</span><span>13
</span><span>14
</span><span>15
</span><span>16
</span><span>17
</span><span>18
</span><span>19
</span><span>20
</span><span>21
</span><span>22
</span><span>23
</span><span>24
</span><span>25
</span><span>26
</span><span>27
</span><span>28
</span><span>29
</span><span>30
</span><span>31
</span><span>32
</span><span>33
</span><span>34
</span><span>35
</span><span>36
</span><span>37
</span><span>38
</span><span>39
</span></code></pre></td><td><pre tabindex="0"><code data-lang="diff"><span><span> pub fn search&lt;const P: usize, const PF: bool&gt;(&amp;self, qb: &amp;[u32; P]) -&gt; [u32; P] {
</span></span><span><span>     let offsets = self
</span></span><span><span>         .offsets
</span></span><span><span>         .iter()
</span></span><span><span>         .map(|o| unsafe { self.tree.as_ptr().add(*o) })
</span></span><span><span>         .collect_vec();
</span></span><span><span>
</span></span><span><span>     let o0 = offsets[0];
</span></span><span><span>     let mut k: [usize; P] = qb.map(|q| {
</span></span><span><span><span>-        (q as usize &gt;&gt; self.shift) * 4 *  16
</span></span></span><span><span><span></span><span>+        (q as usize &gt;&gt; self.shift) * 4 * (16 - self.overlap)
</span></span></span><span><span><span></span>     });
</span></span><span><span>     let q_simd = qb.map(|q| Simd::&lt;u32, 8&gt;::splat(q));
</span></span><span><span>
</span></span><span><span>     if let Some([o1, o2]) = offsets.array_windows().next() {
</span></span><span><span>         for i in 0..P {
</span></span><span><span><span>+            // First level read may be unaligned.
</span></span></span><span><span><span></span><span>-            let jump_to = unsafe { *o.byte_add(k[i])                  }.find_splat64(q_simd[i]);
</span></span></span><span><span><span></span><span>+            let jump_to = unsafe {  o.byte_add(k[i]).read_unaligned() }.find_splat64(q_simd[i]);
</span></span></span><span><span><span></span>             k[i] = k[i] * self.l1 + jump_to;
</span></span><span><span>             prefetch_ptr(unsafe { o2.byte_add(k[i]) });
</span></span><span><span>         }
</span></span><span><span>     }
</span></span><span><span>
</span></span><span><span>     for [o, o2] in offsets[1..].array_windows() {
</span></span><span><span>         for i in 0..P {
</span></span><span><span>             let jump_to = unsafe { *o.byte_add(k[i]) }.find_splat64(q_simd[i]);
</span></span><span><span>             k[i] = k[i] * (B + 1) + jump_to;
</span></span><span><span>             prefetch_ptr(unsafe { o2.byte_add(k[i]) });
</span></span><span><span>         }
</span></span><span><span>     }
</span></span><span><span>
</span></span><span><span>     let o = offsets.last().unwrap();
</span></span><span><span>     from_fn(|i| {
</span></span><span><span><span>-        let idx = unsafe { *o.byte_add(k[i])                  }.find_splat(q_simd[i]);
</span></span></span><span><span><span></span><span>+        let idx = unsafe {  o.byte_add(k[i]).read_unaligned() }.find_splat(q_simd[i]);
</span></span></span><span><span><span></span>         unsafe { (o.byte_add(k[i]) as *const u32).add(idx).read() }
</span></span><span><span>     })
</span></span><span><span> }
</span></span></code></pre></td></tr></tbody></table></div><p><span>Code Snippet 32:</span>
Each part now contains \(16-v\) values, instead of the original 16. We use <code>read_unaligned</code> since we do not always read at 16-value boundaries anymore.</p><figure><a href="https://curiouscoding.nl/ox-hugo/23-overlap.svg"><img src="https://curiouscoding.nl/ox-hugo/23-overlap.svg" alt="Figure 26: Overlapping trees usually use less memory than the equivalent version with first-level compression, while being about as fast."></a><figcaption><p><span>Figure 26: </span>Overlapping trees usually use less memory than the equivalent version with first-level compression, while being about as fast.</p></figcaption></figure><h2 id="human-data"><span>5.5</span> Human data
<a href="#human-data"></a></h2><p>So far we’ve been testing with uniform random data, where the largest part
deviates form the mean size by around \(\sqrt n\). Now, let’s look at some real
data: k-mers of a human genome. DNA consists of <code>ACGT</code> characters that can be
encoded as 2 bits, so each string of \(k=16\) characters defines a 32 bit
integer<sup id="fnref:5"><a href="#fn:5" role="doc-noteref">5</a></sup>.
We then look at the first \(n\) k-mers of the human genome, starting at chromosome 1.</p><p>To give an idea, the plot below show for each k-mer of length \(k=12\) how often
it occurs in the full human genome. In total, there are around 3G
k-mers, and so the expected count for each k-mer is around 200. But instead,
we see k-mers that occur over 2 million times! So if we were to partition on the
first 24 bits, the size of the largest part is only around \(2^{-10}\) of the input,
rather than \(2^{-24}\).</p><p>The accumulated counts are shown in orange, where we also see a number of flat
regions caused by underrepresented k-mers.</p><figure><a href="https://curiouscoding.nl/ox-hugo/rank-curve.png"><img src="https://curiouscoding.nl/ox-hugo/rank-curve.png" alt="Figure 27: A plot showing k-mer counts for all (4^{12} = 16M) $k=12$-mers of the human genome. On random data each k-mer would occur around 200 times, but here we see some k-mers occurring over 2 million times."></a><figcaption><p><span>Figure 27: </span>A plot showing k-mer counts for all (4^{12} = 16M) $k=12$-mers of the human genome. On random data each k-mer would occur around 200 times, but here we see some k-mers occurring over 2 million times.</p></figcaption></figure><figure><a href="https://curiouscoding.nl/ox-hugo/23-overlap-human.svg"><img src="https://curiouscoding.nl/ox-hugo/23-overlap-human.svg" alt="Figure 28: Building the overlapping trees for k-mers of the human genome takes much more space, and even using only 16 parts regularly requires up to 50% overhead, making this data structure not quite practical."></a><figcaption><p><span>Figure 28: </span>Building the overlapping trees for k-mers of the human genome takes much more space, and even using only 16 parts regularly requires up to 50% overhead, making this data structure not quite practical.</p></figcaption></figure><h2 id="prefix-map"><span>5.6</span> Prefix map
<a href="#prefix-map"></a></h2><p>We need a way to handle unbalanced partition sizes, instead of mapping
everything linearly.
We can do this by simply storing the full tree compactly as we did before,
preceded by an array (in blue below) that points to the index of the first
subtree containing elements of the part. Like for the overlapping trees before,
the first layer is simply a list of the largest elements of all subtrees that
can be indexed anywhere (potentially unaligned).</p><figure><a href="https://curiouscoding.nl/ox-hugo/prefix-map.svg"><img src="https://curiouscoding.nl/ox-hugo/prefix-map.svg" alt="Figure 29: The prefix map, in blue, stores (2^b) elements, that for each $b$-bit prefix stores the index of the first subtree that contains an element of that prefix."></a><figcaption><p><span>Figure 29: </span>The prefix map, in blue, stores (2^b) elements, that for each $b$-bit prefix stores the index of the first subtree that contains an element of that prefix.</p></figcaption></figure><p>To answer a query, we first find its part, then read the block (16 elements)
starting at the pointed-to element, and then proceed as usual from the sub-tree onward.</p><div><table><tbody><tr><td><pre tabindex="0"><code><span> 1
</span><span> 2
</span><span> 3
</span><span> 4
</span><span> 5
</span><span> 6
</span><span> 7
</span><span> 8
</span><span> 9
</span><span>10
</span><span>11
</span><span>12
</span><span>13
</span><span>14
</span><span>15
</span><span>16
</span><span>17
</span><span>18
</span><span>19
</span><span>20
</span><span>21
</span><span>22
</span><span>23
</span><span>24
</span><span>25
</span><span>26
</span><span>27
</span><span>28
</span><span>29
</span><span>30
</span><span>31
</span><span>32
</span><span>33
</span><span>34
</span><span>35
</span><span>36
</span></code></pre></td><td><pre tabindex="0"><code data-lang="diff"><span><span> pub fn search&lt;const P: usize, const PF: bool&gt;(&amp;self, qb: &amp;[u32; P]) -&gt; [u32; P] {
</span></span><span><span>     let offsets = self
</span></span><span><span>         .offsets
</span></span><span><span>         .iter()
</span></span><span><span>         .map(|o| unsafe { self.tree.as_ptr().add(*o) })
</span></span><span><span>         .collect_vec();
</span></span><span><span>
</span></span><span><span>     let o0 = offsets[0];
</span></span><span><span>     let mut k: [usize; P] = qb.map(|q| {
</span></span><span><span><span>-                 4 * (16 - self.overlap)         * (q as usize &gt;&gt; self.shift)
</span></span></span><span><span><span></span><span>+        unsafe { 4 * *self.prefix_map.get_unchecked(q as usize &gt;&gt; self.shift) }
</span></span></span><span><span><span></span>     });
</span></span><span><span>     let q_simd = qb.map(|q| Simd::&lt;u32, 8&gt;::splat(q));
</span></span><span><span>
</span></span><span><span>     if let Some([o1, o2]) = offsets.array_windows().next() {
</span></span><span><span>         for i in 0..P {
</span></span><span><span>             let jump_to = unsafe {  o.byte_add(k[i]).read_unaligned() }.find_splat64(q_simd[i]);
</span></span><span><span>             k[i] = k[i] * self.l1 + jump_to;
</span></span><span><span>             prefetch_ptr(unsafe { o2.byte_add(k[i]) });
</span></span><span><span>         }
</span></span><span><span>     }
</span></span><span><span>
</span></span><span><span>     for [o, o2] in offsets[1..].array_windows() {
</span></span><span><span>         for i in 0..P {
</span></span><span><span>             let jump_to = unsafe { *o.byte_add(k[i]) }.find_splat64(q_simd[i]);
</span></span><span><span>             k[i] = k[i] * (B + 1) + jump_to;
</span></span><span><span>             prefetch_ptr(unsafe { o2.byte_add(k[i]) });
</span></span><span><span>         }
</span></span><span><span>     }
</span></span><span><span>
</span></span><span><span>     let o = offsets.last().unwrap();
</span></span><span><span>     from_fn(|i| {
</span></span><span><span>         let idx = unsafe {  o.byte_add(k[i]).read_unaligned() }.find_splat(q_simd[i]);
</span></span><span><span>         unsafe { (o.byte_add(k[i]) as *const u32).add(idx).read() }
</span></span><span><span>     })
</span></span><span><span> }
</span></span></code></pre></td></tr></tbody></table></div><p><span>Code Snippet 33:</span>
In code, the only thing that changes compared to the previous overlapping version is that instead of computing the start index linearly (and adapting the element layout accordingly), we use the <code>prefix_map</code> to jump directly to the right place in the packed tree representation.</p><figure><a href="https://curiouscoding.nl/ox-hugo/24-map.svg"><img src="https://curiouscoding.nl/ox-hugo/24-map.svg" alt="Figure 30: As long as there are more elements than parts and the tree has at least two layers, the space overhead of this representation is close to (1/16) again."></a><figcaption><p><span>Figure 30: </span>As long as there are more elements than parts and the tree has at least two layers, the space overhead of this representation is close to (1/16) again.</p></figcaption></figure><p>Although memory usage is now similar to the unpartitioned version, queries for
large inputs are slightly slower than those previous layouts due to the
additional index required.</p><p>We can also again do the interleaving queries. These are slightly faster for
small inputs, and around as fast as interleaving was without the partitioning.</p><figure><a href="https://curiouscoding.nl/ox-hugo/25-map-interleave.svg"><img src="https://curiouscoding.nl/ox-hugo/25-map-interleave.svg" alt="Figure 31: Prefix-map index with interleaving queries on random data."></a><figcaption><p><span>Figure 31: </span>Prefix-map index with interleaving queries on random data.</p></figcaption></figure><p>On human data, we see that the partitioned index is a bit faster in L1 and L2,
and consistently saves the time of roughly one layer in L3. For larger indices,
performance is still very similar to not using partitioning at all.</p><figure><a href="https://curiouscoding.nl/ox-hugo/25-map-interleave-human.svg"><img src="https://curiouscoding.nl/ox-hugo/25-map-interleave-human.svg" alt="Figure 32: Prefix-map with interleaving on human data."></a><figcaption><p><span>Figure 32: </span>Prefix-map with interleaving on human data.</p></figcaption></figure><h2 id="prefix-summary"><span>5.7</span> Summary
<a href="#prefix-summary"></a></h2><figure><a href="https://curiouscoding.nl/ox-hugo/27-summary.svg"><img src="https://curiouscoding.nl/ox-hugo/27-summary.svg" alt="Figure 33: Summary of partitioning results. Overall, it seems that partitioning does not provide when we already interleave queries."></a><figcaption><p><span>Figure 33: </span>Summary of partitioning results. Overall, it seems that partitioning does not provide when we already interleave queries.</p></figcaption></figure><h2 id="multi-threaded-comparison"><span>6</span> Multi-threaded comparison
<a href="#multi-threaded-comparison"></a></h2><figure><a href="https://curiouscoding.nl/ox-hugo/28-threads.svg"><img src="https://curiouscoding.nl/ox-hugo/28-threads.svg" alt="Figure 34: When using 6 threads, runtime goes down from 27ns to 7ns. Given that the speedup is less than 4x, we are now bottlenecked by total RAM throughput, and indeed methods that are slower for a single thread also reach near-optimal throughput now."></a><figcaption><p><span>Figure 34: </span>When using 6 threads, runtime goes down from <code>27ns</code> to <code>7ns</code>. Given that the speedup is less than 4x, we are now bottlenecked by total RAM throughput, and indeed methods that are slower for a single thread also reach near-optimal throughput now.</p></figcaption></figure><h2 id="conclusion"><span>7</span> Conclusion
<a href="#conclusion"></a></h2><p>All together, we went from <code>1150ns/query</code> for binary search on 4GB input to
<code>27ns</code> for the optimized S-tree with interleaved queries, over <code>40x</code> speedup!
A large part of this improvement is due to <strong>batching</strong> queries and <strong>prefetching</strong>
upcoming nodes. To get even higher throughput, <strong>interleaving</strong> queries at different
levels helps to balance the CPU-bound part of the computation with the
memory-bound part, so that we get a higher overall throughput. Using a <strong>15
elements per node</strong> instead of 16 also improves throughput somewhat, but doubles
the overhead of the data structure from 6.25% to 13.3%. For inputs that fit in
L3 cache that’s fine and the speedup is worthwhile, while for larger inputs the
speed is memory-bound anyway, so that there is no speedup while the additional
memory requirements are somewhat large.</p><p>We also looked into <strong>partitioning</strong> the data by prefix. While this does give some speedup,
it turns out that on skewed input data, the benefits quickly
diminish since the tree either requires a lot of buffer space, or else requires
an additional lookup to map each part to its location in the first level of the tree.
In the end, I’d say the additional complexity and dependency on the shape of
the input data of partitioning is not worth the speedup compared to simply using interleaved
queries directly.</p><h2 id="future-work"><span>7.1</span> Future work
<a href="#future-work"></a></h2><h3 id="branchy-search"><span>7.1.1</span> Branchy search
<a href="#branchy-search"></a></h3><p>All methods we considered are <em>branchless</em> and use the exact same number of
iterations for each query. Especially in combination with partitioning, it may
be possible to handle the few large parts independently from the usual
smaller parts. That way we could answer most queries with slightly fewer
iterations.</p><p>On the other hand, the layers saved would mostly be the quick lookups near the
root of the tree, and introducing branches to the code could possibly cause
quite a bit of delay due to mispredictions.</p><h3 id="interpolation-search"><span>7.1.2</span> Interpolation search
<a href="#interpolation-search"></a></h3><p>As we saw in the last plot above, total RAM throughput (rather than per-core
throughput) becomes a bottleneck once we’re using multiple threads.
Thus, the only way to improve total query throughput is to use strictly fewer RAM
accesses per query.
Prefix lookups won’t help, since they only replace the layers of the tree
that would otherwise fit in the cache. Instead, we could use <em>interpolation
search</em> (<a href="https://en.wikipedia.org/wiki/Interpolation_search">wikipedia</a>), where the estimated position of a query \(q\) is linearly
interpolated between known positions of surrounding elements. On random data, this only takes
\(O(\lg \lg n)\) iterations, rather than \(O(\lg n)\) for binary search, and could
save some RAM accesses. On the
other hand, when data is not random its worst case performance is \(O(n)\) rather
than the statically bounded \(O(\lg n)\).</p><p>The PLA-index (<a href="#citeproc_bib_item_1">Abrar and Medvedev 2024</a>) also uses a single interpolation step in a
precisely constructed piece wise linear approximation. The error after the
approximation is determined by some global upper bound, so that the number of remaining
search steps can be bounded as well.</p><h3 id="packing-data-smaller"><span>7.1.3</span> Packing data smaller
<a href="#packing-data-smaller"></a></h3><p>Another option to use the RAM lookups more efficiently would be to pack values
into 16 bits rather than the 32 bits we’ve been using so far. Especially if we
first do a 16 bit prefix lookup, we already know those bits anyway, so it would
suffice to only compare the last 16 bits of the query and values. This increases
the branching factor from 17 to 33, which reduces the number of layers of the
tree by around 1.5 for inputs of 1GB.</p><h3 id="returning-indices-in-original-data"><span>7.1.4</span> Returning indices in original data
<a href="#returning-indices-in-original-data"></a></h3><p>For various applications, it may be helpful to not only return the smallest
value \(\geq q\), but also the index in the original list of sorted values, for
example when storing an array with additional data for each item.</p><p>Since we use the S+ tree that stores all data in the bottom layer, this is
mostly straightforward. The <em>prefix map</em> partitioned tree also natively supports
this, while the other partitioned variants do not: they include buffer/padding
elements in their bottom layer, and hence we would need to store and look up the position
offset of each part separately.</p><h3 id="range-queries"><span>7.1.5</span> Range queries
<a href="#range-queries"></a></h3><p>We could extend the current query methods to a version that return both the
first value \(\geq q\) and the first value \(&gt;q\), so that the range of positions
corresponding to value \(q\) can be determined. In practice, the easiest way to do
this is by simply doubling the queries into \(q\) and \(q+1\). This will cause some
CPU overhead in the initial layers, but the query execution will remain
branch-free. When \(q\) is not found or only occurs a few times, they will mostly
fetch the same cache lines, so that memory is efficiently reused and the
bandwidth can be used for other queries.</p><p>In practice though, this seems only around 20% faster per individual query for 4GB input, so
around 60% slower for a range than for a single query. For small inputs, the
speedup is less, and sometimes querying ranges is even more than twice slower
than individual random queries.</p><h2 id="references">References
<a href="#references"></a></h2><div><p><a id="citeproc_bib_item_1"></a>Abrar, Md. Hasin, and Paul Medvedev. 2024. “Pla-Index: A K-Mer Index Exploiting Rank Curve Linearity.” Schloss Dagstuhl – Leibniz-Zentrum für Informatik. <a href="https://doi.org/10.4230/LIPICS.WABI.2024.13">https://doi.org/10.4230/LIPICS.WABI.2024.13</a>.</p><p><a id="citeproc_bib_item_2"></a>Khuong, Paul-Virak, and Pat Morin. 2017. “Array Layouts for Comparison-Based Searching.” <i>Acm Journal of Experimental Algorithmics</i> 22 (May): 1–39. <a href="https://doi.org/10.1145/3053370">https://doi.org/10.1145/3053370</a>.</p></div></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[HNInternal: Happy New Year 2025 (1130 pts)]]></title>
            <link>https://news.ycombinator.com/item?id=42562750</link>
            <guid>42562750</guid>
            <pubDate>Tue, 31 Dec 2024 23:48:36 GMT</pubDate>
            <description><![CDATA[<p>See on <a href="https://news.ycombinator.com/item?id=42562750">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
            <tbody><tr id="42563726"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_42563726" href="https://news.ycombinator.com/vote?id=42563726&amp;how=up&amp;goto=item%3Fid%3D42562750"></a></center>    </td><td><br><div><p>US Central Time here, so still a couple hours to go, but HAPPY NEW YEAR to y'all fellow HN-ers -- HN is probably the <i>least</i> guilt-ridden procrastination I do on the whole wide internet. Thank you to <i>every one</i> of you who has made HN among the most intellectually dense places on the web -- I wouldn't be where I'm today without reading all the insightful comments, submitted links and perspectives of all the HN-ers over the last 7 years.</p><p>My wish for the new year: hopefully, AI singularity is still a few years away and doesn't happen in 2025!</p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="42563903"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_42563903" href="https://news.ycombinator.com/vote?id=42563903&amp;how=up&amp;goto=item%3Fid%3D42562750"></a></center>    </td><td><br><div>
                  <p>Although this is just the usual digits incrementing periodically, take the opportunity to look back, reflect and hope for a brighter future for you, your friends, family and rest of the world.</p>
              </div></td></tr>
        </tbody></table></td></tr>
            <tr id="42563190"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_42563190" href="https://news.ycombinator.com/vote?id=42563190&amp;how=up&amp;goto=item%3Fid%3D42562750"></a></center>    </td><td><br><div>
                  <p>Same. I've been a lurker since 2008. Been hitting this site practically every day since. I've learned so much in the process. Thanks for everything hn, happy new year!</p>
              </div></td></tr>
        </tbody></table></td></tr>
                <tr id="42563929"><td></td></tr>
                  <tr id="42563935"><td></td></tr>
            <tr id="42563674"><td></td></tr>
            <tr id="42563924"><td></td></tr>
            <tr id="42563923"><td></td></tr>
            <tr id="42563481"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_42563481" href="https://news.ycombinator.com/vote?id=42563481&amp;how=up&amp;goto=item%3Fid%3D42562750"></a></center>    </td><td><br><div><p>Given all the division and change across the rest of the web (and by extension the world), glad to have a place where I can learn, share, grow, and maintain an open mind.</p><p>Happy new year :)</p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="42563907"><td></td></tr>
            <tr id="42563685"><td></td></tr>
            <tr id="42563853"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_42563853" href="https://news.ycombinator.com/vote?id=42563853&amp;how=up&amp;goto=item%3Fid%3D42562750"></a></center>    </td><td><br><div><p>Happy Year++</p><p>I'm thankful that this site exists to allow me to catch up on the news, events, and perspectives that I'm most interested in. And I'm even more thankful that it uses a simple, functional layout.</p><p>Keep rockin' into 2025, HN!</p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="42563901"><td></td></tr>
            <tr id="42563854"><td></td></tr>
            <tr id="42563735"><td></td></tr>
            <tr id="42563900"><td></td></tr>
                <tr id="42563910"><td></td></tr>
                  <tr id="42562923"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_42562923" href="https://news.ycombinator.com/vote?id=42562923&amp;how=up&amp;goto=item%3Fid%3D42562750"></a></center>    </td><td><br><div>
                  <p>Yea, I discovered this site late last year (2024) around October, and it's now a site I visit everyday. At least 4 times a day.</p>
              </div></td></tr>
        </tbody></table></td></tr>
            <tr id="42563881"><td></td></tr>
            <tr id="42563723"><td></td></tr>
                <tr id="42563896"><td></td></tr>
                  <tr id="42563598"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_42563598" href="https://news.ycombinator.com/vote?id=42563598&amp;how=up&amp;goto=item%3Fid%3D42562750"></a></center>    </td><td><br><div><p>The days are getting longer in the northern hemisphere and shorter in the southern hemisphere. And the start of the ISO 8601 dates is changing!</p><p>I also monitor the changes of the hexadecimal unix timestamps. The next time the first of 8 hex digits changes is in 2029.</p><pre><code>    from datetime import datetime
    '{:02X}'.format(int(datetime.now().timestamp()))
    datetime.fromtimestamp(0x70000000)
</code></pre><p>
And this spits out the year in Python: 3**2**2*5**2</p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="42563732"><td></td></tr>
                <tr id="42563768"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_42563768" href="https://news.ycombinator.com/vote?id=42563768&amp;how=up&amp;goto=item%3Fid%3D42562750"></a></center>    </td><td><br><div><p>Nice. A unique year indeed, since the last one was 1936, 89 years ago. It's obvious to me now - that's 2025-(45*2-1) and I can picture a row and a column at the edges of a grid being removed, wth one cell being shared between the row and column. I think maybe at some point I saw that. 1600 and 2500 are more obvious, and 2050 is the midpoint between them linearly.</p><p>The others in Python:</p><pre><code>    sum(range(1, 10))**2
    sum(n**3 for n in range(1, 10))</code></pre></div></td></tr>
        </tbody></table></td></tr>
                        <tr id="42563837"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_42563837" href="https://news.ycombinator.com/vote?id=42563837&amp;how=up&amp;goto=item%3Fid%3D42562750"></a></center>    </td><td><br><div>
                  <p>Same here. Am a forced lurker here as somehow my posts just never go beyond to anyone like it once used to.</p>
              </div></td></tr>
        </tbody></table></td></tr>
            <tr id="42563562"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_42563562" href="https://news.ycombinator.com/vote?id=42563562&amp;how=up&amp;goto=item%3Fid%3D42562750"></a></center>    </td><td><br><div><p>Happy New Year, whether you celebrated it in the past or the future.</p><p>Remember - the new year is not yet written. Amazing and unexpected things are sure to come.</p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="42563559"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_42563559" href="https://news.ycombinator.com/vote?id=42563559&amp;how=up&amp;goto=item%3Fid%3D42562750"></a></center>    </td><td><br><div>
                  <p>HN is our backyard for learning &amp; understanding new perspectives. To all - ಹೊಸ ವರ್ಷದ ಶುಭಾಶಯಗಳು!!</p>
              </div></td></tr>
        </tbody></table></td></tr>
                <tr id="42563593"><td></td></tr>
                  <tr id="42563914"><td></td></tr>
            <tr id="42563742"><td></td></tr>
            <tr id="42563663"><td></td></tr>
            <tr id="42563702"><td></td></tr>
            <tr id="42563833"><td></td></tr>
            <tr id="42563652"><td></td></tr>
            <tr id="42563691"><td></td></tr>
            <tr id="42563382"><td></td></tr>
            <tr id="42563112"><td></td></tr>
            <tr id="42563740"><td></td></tr>
            <tr id="42563711"><td></td></tr>
            <tr id="42563687"><td></td></tr>
            <tr id="42563764"><td></td></tr>
                <tr id="42563769"><td></td></tr>
            <tr id="42563898"><td></td></tr>
                  <tr id="42563458"><td></td></tr>
                <tr id="42563580"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_42563580" href="https://news.ycombinator.com/vote?id=42563580&amp;how=up&amp;goto=item%3Fid%3D42562750"></a></center>    </td><td><br><div><p>&gt; Happy New Year from the West coast!</p><p>This was around 18:10 PST so the greeting may have introduced a race condition.</p><p>Happy New Year anyway, logic be darned!</p></div></td></tr>
        </tbody></table></td></tr>
                  <tr id="42563488"><td></td></tr>
            <tr id="42563279"><td></td></tr>
            <tr id="42563636"><td></td></tr>
            <tr id="42563564"><td></td></tr>
            <tr id="42563689"><td></td></tr>
            <tr id="42563807"><td></td></tr>
            <tr id="42563754"><td></td></tr>
            </tbody></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Déjà vu: Ghostly CVEs in my terminal title (189 pts)]]></title>
            <link>https://dgl.cx/2024/12/ghostty-terminal-title</link>
            <guid>42562743</guid>
            <pubDate>Tue, 31 Dec 2024 23:47:56 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://dgl.cx/2024/12/ghostty-terminal-title">https://dgl.cx/2024/12/ghostty-terminal-title</a>, See on <a href="https://news.ycombinator.com/item?id=42562743">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
          <p><em>Exploring a security bug in Ghostty that is eerily familiar.</em></p>
<p>As I've <a href="https://www.youtube.com/watch?v=4kfDBNzStbs&amp;themeRefresh=1">spoken</a>
and <a href="https://dgl.cx/2023/09/ansi-terminal-security">written</a> about all modern
terminals are actually "emulating" something dating from the 1970s.</p>
<p>The full details are surprisingly complex and having a standard building block
for these things is important. We can probably do better, but it's hard to
change something so fundamental.</p>
<p>In Feburary 2003 HD Moore published a paper to bugtraq called <a href="https://seclists.org/bugtraq/2003/Feb/315">"Terminal
Emulator Security Issues"</a>.</p>
<p>On the 26<sup>th</sup> December 2024 Mitchell Hashimoto released <a href="https://ghostty.org/">Ghostty
1.0</a>. A new terminal emulator. The name even has a little
nod to the "tty" of old. I instantly tried it out and ran my terminal tester on
it:</p>
<p><img src="https://dgl.cx/2024/12/termtest-ghostty.png"></p><p>Oh dear.</p>
<p>That 2003 CVE is indeed <a href="https://nvd.nist.gov/vuln/detail/CVE-2003-0063">the one</a> HD Moore found many years ago. The new issue has been assigned <a href="https://github.com/ghostty-org/ghostty/security/advisories/GHSA-5hcq-3j4q-4v6p">CVE-2024-56803</a>.</p>
<h2>Why are we here again?</h2>
<p>First of all, given the number of terminals which have been affected by this,
this is clearly a fundamental issue.</p>
<p>The fundamental problem is that terminals use in-band signalling, that is
the ASCII escape character escapes from the mode the terminal is usually in
(printing text) and starts handling the input to it as something else. These
other things are known as escape sequences and they can change the colour or
ask the terminal to do various control things.</p>
<p>One of those control things is setting the title. That's useful, when you run
<code>cd</code> in your shell, it's quite nice the title of the window or tab updates so
you know which directory a particular terminal is in. However there is also a
sequence to query the title. These "query" sequences are particularly
problematic when combined with the in-band signalling nature of a terminal, if
the program running inside the terminal does not expect a reply at that moment,
it may handle it differently, or even treat it as user input.</p>
<p>In general any reply where the user can control the data should be considered
very carefully. Most terminals therefore disable title reporting by default or
even don't implement it. (A <a href="https://vin01.github.io/piptagole/escape-sequences/iterm2/rce/2024/06/16/iterm2-rce-window-title-tmux-integration.html">recent iTerm2
bug</a>
was a regression around the configuration option itself to disable it.)</p>
<p>In 2022 I discovered a <a href="https://www.openwall.com/lists/oss-security/2022/11/10/1">bug in
xterm</a> where the font
query could be used to inject user controllable text. This only worked on Zsh
and made use of the fact the "Escape" used as part of the escape sequence is
also the key you press to leave insert mode and enter the Vi normal mode.</p>
<h2>The Ghostty variant</h2>
<p>It turns out with Ghostty we can do something very similar to the xterm issue, in Zsh with vi mode enabled (<code>set -o vi</code>), simply outputting this sequence to the screen:</p>
<pre><code>printf '\e]0;iopen -a Calculator\a\e[21t\e]0;:accept-line\a\e[21t'
</code></pre>
<p>Results in it opening calculator (on macOS):</p>
<p><img src="https://dgl.cx/2024/12/ghostty-zsh-calc.jpg"></p><p>All <a href="https://github.com/zsh-users/zsh/commit/03f52f1da6a41529982482442360c9378211b4ce">released versions</a> of Zsh have a behaviour that they default the keymap to vi if the <code>$EDITOR</code> or <code>$VISUAL</code> environment variables contain "vi". So this is quite a common setup for users. This default has been removed after Zsh 5.9, but that is not yet released.</p>
<p>Bash with <code>set -o vi</code>:</p>
<pre><code>printf '\e]0;iopen -a Calculator\a\e[21t\e]0;vZZ\a\e[21t'
</code></pre>
<p><img src="https://dgl.cx/2024/12/ghostty-bash-calc.jpg"></p><p>Unlike Zsh this setting does have to be enabled by the user, so is likely to be a less common configuration.</p>
<h2>But is this an RCE?</h2>
<p><em>Spoiler: Yes</em></p>
<p>One aspect of this attack that isn't immediately clear is the input goes via your terminal, so it's like you typed it, even if you're connected to a remote system via SSH. If the remote system is compromised, it can decide it isn't interested in your input and make it get buffered, with it probably getting delivered locally.</p>
<p>We can demonstrate this with a simple script that stops the shell. In this case
I run it as another session of the user, but it could also be run via root on a
remote system if the system is compromised.</p>

<div data-id="SVYaMUpX3vM" onclick="if (!this.querySelector(&quot;iframe&quot;)) this.innerHTML = `<iframe width=1200 height=600 title=&quot;${this.querySelector(&quot;.yt-title&quot;).textContent}&quot; frameborder=0 allow=&quot;autoplay; clipboard-write; encrypted-media; picture-in-picture;&quot; allowfullscreen=&quot;&quot; src=&quot;https://www.youtube-nocookie.com/embed/${this.dataset.id}?autoplay=1&amp;rel=0&quot;></iframe>`;">
  <p><img src="https://dgl.cx/2024/12/yt-SVYaMUpX3vM.jpg"></p><p>SSH disconnect attack on Ghostty 1.0.0</p>
  <p>▶️
    
  </p>
</div>
<p>The "disconnect-ghosty" script used in the demo is below:</p>
<pre><code>#!/bin/bash
# David Leadbeater, 2024. http://©.st/dgl
pid=${1:?$'\e'"[GUsage: $0 pid-of-shell"}

tty="/dev/$(ps -otty -p$pid | tail -1)"

kill -STOP $pid
printf '\e]0;xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxiopen -a Calculator\a\e[21t\e[21t\e]0;:accept-line\a\e[21t' &gt; $tty
kill -9 $pid
</code></pre>
<h2>Fixes and mitigations</h2>
<p>Ghostty <a href="https://ghostty.org/docs/install/release-notes/1-0-1">1.0.1</a> is out now, <a href="https://github.com/ghostty-org/ghostty/commit/25a4a89ee3d31e35148d7e75064214efe2a057a1">fixing this</a> (by making it configurable and disabling it by default).</p>
<p>If for some reason you can't upgrade, the <a href="https://github.com/ghostty-org/ghostty/security/advisories/GHSA-5hcq-3j4q-4v6p">advisory</a> has a workaround where a fixed title will not let an attacker control the value reported back.</p>
<p>An alternative is to put the following in your <code>~/.zshrc</code>:</p>
<pre><code>function skip-osc-sequence() {
  local key
  while read -sk key &amp;&amp; (( $((#key)) != 0x1B &amp;&amp; $((#key)) != 0x07 )); do
    # empty body
  done
  if [[ $((#key)) = 27 ]]; then
    # ^[\
    read -sk key
  fi
}

zle -N skip-osc-sequence
bindkey '\e]' skip-osc-sequence
</code></pre>
<p>This makes Zsh skip over OSC replies rather than treat them as input. This is only a mitigation and you should upgrade as there may still be cases where you could be attacked (e.g. with the remote attack over SSH, careful timing could lead to a "torn" read that may mean the local shell doesn't see the start of the OSC sequence). It also doesn't hurt to leave in your config, as it provides defense-in-depth.</p>
<p>Or if using bash, put this in <code>~/.inputrc</code>:</p>
<pre><code>"\e]": skip-csi-sequence
"\e\\": skip-csi-sequence
</code></pre>
<p>This isn't as complete as the Zsh mitigation, as you could still be blindly
tricked to press Enter and run an unexpected command, but it works for this
particular issue in Ghostty.</p>
<p>Please don't see this post as any kind of attack against Ghostty, remember it
just had a 1.0 release. I've been using it and I'm writing this very post in
it. The "terminal inspector" is very nice for people interested in diving into
the internals of their terminal. Thanks to Mitchell for the quick fix.</p>
<p>If you want to see more on this subject you might like my <a href="https://www.youtube.com/watch?v=iIHw0KWgzAs">Microsoft BlueHat
talk from 2023</a>, or my longer <a href="https://dgl.cx/2023/09/ansi-terminal-security">in
depth post</a> on various issues with terminals.</p>
<p><span>31<sup>st</sup> December 2024</span>
</p>

        </div></div>]]></description>
        </item>
    </channel>
</rss>