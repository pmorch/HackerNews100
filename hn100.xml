<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0">
    <channel>
        <title>HN100 - Readable Contents</title>
        <link>https://hn.algolia.com/api/v1/search_by_date?tags=%28story,poll%29&amp;numericFilters=points%3E100</link>
        <description>Uses Readability to add bodies to the RSS feed</description>
        <lastBuildDate>Sun, 09 Jul 2023 12:00:04 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>en</language>
        <item>
            <title><![CDATA[Google's Privacy Policy Now Admits to Collecting All Your Data for AI Training (121 pts)]]></title>
            <link>https://www.pcgamer.com/in-case-there-was-any-doubt-googles-privacy-policy-now-explicitly-states-that-its-going-to-suck-up-all-your-data-to-train-its-ai/</link>
            <guid>36651717</guid>
            <pubDate>Sun, 09 Jul 2023 05:09:13 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.pcgamer.com/in-case-there-was-any-doubt-googles-privacy-policy-now-explicitly-states-that-its-going-to-suck-up-all-your-data-to-train-its-ai/">https://www.pcgamer.com/in-case-there-was-any-doubt-googles-privacy-policy-now-explicitly-states-that-its-going-to-suck-up-all-your-data-to-train-its-ai/</a>, See on <a href="https://news.ycombinator.com/item?id=36651717">Hacker News</a></p>
<div id="readability-page-1" class="page"><article aria-label="article" data-id="mTSjXezVBz6VRxDMMJKq8d">
<header>
<nav aria-label="Breadcrumbs">
<ol>
<li>
<a href="https://www.pcgamer.com/" aria-label="Return to Home">Home</a>
</li>
<li>
<a href="https://www.pcgamer.com/uk/news/" aria-label="Return to News">News</a>
</li>
</ol>
</nav>


</header>
<section>
<div itemprop="image" itemscope="" itemtype="https://schema.org/ImageObject">
<div>
<picture><source type="image/webp" alt="Google campus sign" onerror="if(this.src &amp;&amp; this.src.indexOf('missing-image.svg') !== -1){return true;};this.parentNode.replaceChild(window.missingImage(),this)" srcset="https://cdn.mos.cms.futurecdn.net/uyEje6YtnET6euVcPGbG3P-320-80.jpg.webp 320w, https://cdn.mos.cms.futurecdn.net/uyEje6YtnET6euVcPGbG3P-480-80.jpg.webp 480w, https://cdn.mos.cms.futurecdn.net/uyEje6YtnET6euVcPGbG3P-650-80.jpg.webp 650w, https://cdn.mos.cms.futurecdn.net/uyEje6YtnET6euVcPGbG3P-970-80.jpg.webp 970w, https://cdn.mos.cms.futurecdn.net/uyEje6YtnET6euVcPGbG3P-1024-80.jpg.webp 1024w, https://cdn.mos.cms.futurecdn.net/uyEje6YtnET6euVcPGbG3P-1200-80.jpg.webp 1200w, https://cdn.mos.cms.futurecdn.net/uyEje6YtnET6euVcPGbG3P-1920-80.jpg.webp 1920w" sizes="(min-width: 1000px) 970px, calc(100vw - 40px)" data-original-mos="https://cdn.mos.cms.futurecdn.net/uyEje6YtnET6euVcPGbG3P.jpg" data-pin-media="https://cdn.mos.cms.futurecdn.net/uyEje6YtnET6euVcPGbG3P.jpg"><source type="image/jpeg" alt="Google campus sign" onerror="if(this.src &amp;&amp; this.src.indexOf('missing-image.svg') !== -1){return true;};this.parentNode.replaceChild(window.missingImage(),this)" srcset="https://cdn.mos.cms.futurecdn.net/uyEje6YtnET6euVcPGbG3P-320-80.jpg 320w, https://cdn.mos.cms.futurecdn.net/uyEje6YtnET6euVcPGbG3P-480-80.jpg 480w, https://cdn.mos.cms.futurecdn.net/uyEje6YtnET6euVcPGbG3P-650-80.jpg 650w, https://cdn.mos.cms.futurecdn.net/uyEje6YtnET6euVcPGbG3P-970-80.jpg 970w, https://cdn.mos.cms.futurecdn.net/uyEje6YtnET6euVcPGbG3P-1024-80.jpg 1024w, https://cdn.mos.cms.futurecdn.net/uyEje6YtnET6euVcPGbG3P-1200-80.jpg 1200w, https://cdn.mos.cms.futurecdn.net/uyEje6YtnET6euVcPGbG3P-1920-80.jpg 1920w" sizes="(min-width: 1000px) 970px, calc(100vw - 40px)" data-original-mos="https://cdn.mos.cms.futurecdn.net/uyEje6YtnET6euVcPGbG3P.jpg" data-pin-media="https://cdn.mos.cms.futurecdn.net/uyEje6YtnET6euVcPGbG3P.jpg"><img src="https://cdn.mos.cms.futurecdn.net/uyEje6YtnET6euVcPGbG3P-320-80.jpg" alt="Google campus sign" onerror="if(this.src &amp;&amp; this.src.indexOf('missing-image.svg') !== -1){return true;};this.parentNode.replaceChild(window.missingImage(),this)" srcset="https://cdn.mos.cms.futurecdn.net/uyEje6YtnET6euVcPGbG3P-320-80.jpg 320w, https://cdn.mos.cms.futurecdn.net/uyEje6YtnET6euVcPGbG3P-480-80.jpg 480w, https://cdn.mos.cms.futurecdn.net/uyEje6YtnET6euVcPGbG3P-650-80.jpg 650w, https://cdn.mos.cms.futurecdn.net/uyEje6YtnET6euVcPGbG3P-970-80.jpg 970w, https://cdn.mos.cms.futurecdn.net/uyEje6YtnET6euVcPGbG3P-1024-80.jpg 1024w, https://cdn.mos.cms.futurecdn.net/uyEje6YtnET6euVcPGbG3P-1200-80.jpg 1200w, https://cdn.mos.cms.futurecdn.net/uyEje6YtnET6euVcPGbG3P-1920-80.jpg 1920w" sizes="(min-width: 1000px) 970px, calc(100vw - 40px)" data-original-mos="https://cdn.mos.cms.futurecdn.net/uyEje6YtnET6euVcPGbG3P.jpg" data-pin-media="https://cdn.mos.cms.futurecdn.net/uyEje6YtnET6euVcPGbG3P.jpg"></picture>
</div>
<meta itemprop="url" content="https://cdn.mos.cms.futurecdn.net/uyEje6YtnET6euVcPGbG3P.jpg">
<meta itemprop="height" content="600">
<meta itemprop="width" content="338">
<figcaption itemprop="caption description">
<span itemprop="copyrightHolder">(Image credit: Bloomberg (Getty Images))</span>
</figcaption>
</div>

<div id="article-body">
<p>To me, artificial intelligence is a lot like magnets: I have no idea how they work. But I do understand, in a very general sense, that AI is not actually intelligent. It's just data, collected on a massive scale, algorithmically digested, and spit out in conversational tones designed to make us think that the machine is "smart."</p><p>The popular versions of these systems, like ChatGPT, live and die based on the amount of data they can harvest, which essentially means they're reliant on you. And in case there's any doubt about what "you" means in this particular context, <a href="https://policies.google.com/privacy?hl=en#footnote-sources" target="_blank" data-url="https://policies.google.com/privacy?hl=en#footnote-sources">Google</a> (via <a href="https://www.techspot.com/news/99281-google-policy-update-confirms-itll-scrape-everything-you.html" target="_blank" data-url="https://www.techspot.com/news/99281-google-policy-update-confirms-itll-scrape-everything-you.html">Techspot</a>) has updated its privacy policy to explicitly state that pretty much anything you say or do online can be scooped up and used to train its AI models.</p><p>Naturally, Google collects data from your online activity, like the stuff you search for, the videos you watch, the things you buy, and the people you talk to, and the location data accessed through your Android mobile device. But "in some circumstances," it also collects information from "publicly accessible sources": If your name appears in a local newspaper article, for instance, Google may index the article and then share it with people searching for your name.</p><p>That in itself isn't new: What's changed, as can be seen on Google's <a href="https://policies.google.com/privacy/archive?hl=en" target="_blank" data-url="https://policies.google.com/privacy/archive?hl=en">policy updates page</a>, is how Google says it can use the information it picks up from those public sources. Previously, the policy stated that publicly available data could be used "to help train Google’s language models and build features like Google Translate." The latest update broadens the policy considerably: "We may collect information that’s publicly available online or from other public sources to help train Google’s AI models and build products and features like Google Translate, Bard, and Cloud AI capabilities."</p><p>Bard is essentially Google's answer to ChatGPT, announced <a href="https://www.pcgamer.com/google-unveils-its-own-chatgpt-like-ai-chatbot/" target="_blank">earlier this year</a>, and much like other AI models it hasn't been entirely smooth sailing. In April, for instance, a report claimed that several Google employees had urged the company not to roll out Bard because the information it provided in response to queries was "worse than useless" and effectively made the chatbot a "<a href="https://www.pcgamer.com/google-employees-reportedly-begged-it-not-to-release-pathological-liar-ai-chatbot-bard/" target="_blank">pathological liar</a>."</p><p>More data should, in theory at least, lead to better results for Google's bots. But updated privacy policy or not, the legal status of this behaviour has not been clearly established. OpenAI is facing multiple lawsuits over the way it harvests and uses data to train ChatGPT: Policies like the one recently implemented by Google might seem to make some of it fair game but, but as <a href="https://www.washingtonpost.com/technology/2023/06/28/openai-chatgpt-lawsuit-class-action/" target="_blank" data-url="https://www.washingtonpost.com/technology/2023/06/28/openai-chatgpt-lawsuit-class-action/">The Washington Post</a> reported, AI models will hoover up pretty much anything from Wikipedia pages to news posts and individual tweets, a habit that a growing number of people take issue with.&nbsp;</p><p>And not all of the material in question is in fact fair game: Authors Mona Awad and Paul Tremblay recently <a href="https://www.theguardian.com/books/2023/jul/05/authors-file-a-lawsuit-against-openai-for-unlawfully-ingesting-their-books" target="_blank" data-url="https://www.theguardian.com/books/2023/jul/05/authors-file-a-lawsuit-against-openai-for-unlawfully-ingesting-their-books">filed their own lawsuit</a> against OpenAI, alleging that ChatGPT violated copyright laws by using their works to train its AI model without permission.</p><p>I've reached out to Google for more information on its reasons for changing its privacy policies, and will update if I receive a reply.</p>
</div>
<div data-hydrate="true" id="slice-container-newsletterForm-articleInbodyContent"><section><p>Sign up to get the best content of the week, and great gaming deals, as picked by the editors.</p></section></div>
<div id="slice-container-authorBio"><p>Andy has been gaming on PCs from the very beginning, starting as a youngster with text adventures and primitive action games on a cassette-based TRS80. From there he graduated to the glory days of Sierra Online adventures and Microprose sims, ran a local BBS, learned how to build PCs, and developed a longstanding love of RPGs, immersive sims, and shooters. He began writing videogame news in 2007 for The Escapist and somehow managed to avoid getting fired until 2014, when he joined the storied ranks of PC Gamer. He covers all aspects of the industry, from new game announcements and patch notes to legal disputes, Twitch beefs, esports, and Henry Cavill. <em>Lots</em> of Henry Cavill.</p></div>



</section>


<div id="slice-container-relatedArticles"><p><h5>Most Popular</h5></p></div>

</article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[KDE for Travelers (104 pts)]]></title>
            <link>https://kde.org/for/travelers/</link>
            <guid>36651387</guid>
            <pubDate>Sun, 09 Jul 2023 03:59:55 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://kde.org/for/travelers/">https://kde.org/for/travelers/</a>, See on <a href="https://news.ycombinator.com/item?id=36651387">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="main"><main><div><p>Travel the World Using KDE Applications</p></div><div><figure><img src="https://apps.kde.org/app-icons/org.kde.itinerary.svg" alt="" title=""></figure><h2 id="kde-itinerary">KDE Itinerary</h2><p>KDE Itinerary is a digital travel assistant that protects your privacy. It makes collecting all the information about your travel inside a single application easy and straightforward. KDE Itinerary is available for <a href="https://plasma-mobile.org/">Plasma Mobile</a> and Android.</p><p><a href="https://community.kde.org/Android/FDroid#KDE_F-Droid_Release_Repository"><img src="https://kde.org/store_badges/fdroid/en.svg" alt="Get it on F-Droid"></a></p></div><div><div><h2 id="store-your-reservations">Store your reservations</h2><p>Store all the information about your reservations in Itinerary. This includes QR-codes, check-in times, arrivial times, real-time delays, seat reservations, coach layout, and more.</p><p>Itinerary supports train, bus and flight bookings, as well as hotel, restaurant, event and rental car reservations. Traveling in a group? Not a problem, Itinerary supports multi-traveler bookings.</p></div><p><img src="https://kde.org/for/travelers/itinerary-trip.png"></p></div><div><div><h2 id="local-first">Local first</h2><p>Itinerary automatically extracts booking data from various input formats. It's all performed locally on <strong>your</strong> device and your data is not sent to any remote servers.</p><p>This works best when using <a href="https://kontact.kde.org/components/kmail/">KMail</a> to extract tickets from your email and then <a href="https://kdeconnect.kde.org/">KDE Connect</a> to transfer tickets to your phone. This also works great with <a href="https://apps.nextcloud.com/apps/mail">Nextcloud Mail</a> and <a href="https://f-droid.org/en/packages/at.bitfire.davdroid/">DavDroid</a> to sync your tickets from <a href="https://nextcloud.com/">Nextcloud</a>.</p><figure><img src="https://kde.org/for/travelers/kmail.png" alt="KMail ticket extraction showing a train trip from Berlin to Tübingen" title="KMail ticket extraction showing a train trip from Berlin to Tübingen"></figure></div><p><img src="https://kde.org/for/travelers/itinerary-ticket.png"></p></div><div><div><h2 id="add-your-connections">Add your connections</h2><p>Aside from finding reservations automatically in your email, Itinerary lets you add train trips manually to your journey, find alternative connections if your train is cancelled, or, for some providers, import your train trip directly from your reservation number.</p></div><p><img src="https://kde.org/for/travelers/itinerary-connections.png"></p></div><div><div><h2 id="find-your-way">Find your way</h2><p>Powered by <a href="https://www.openstreetmap.org/">Open Street Map</a>, the indoor map at train stations or airports can be a life saver. Use Itinerary to locate your platform is, and, if you have seat reservation and the train layout is available, it can even show you exactly which platform section is best for you.</p><figure><img src="https://www.volkerkrause.eu/assets/posts/139/kde-itinerary-platform-section-highlighting.jpg" alt="Train station map in KDE Itinerary, highlighting relevant platform sections." title="Train station map in KDE Itinerary, highlighting relevant platform sections."></figure><p>The indoor map also shows you which shops and resturantes are currently open, which elevator is broken (yet again!), where the toilets are and where the correct exit is.</p></div><p><img src="https://kde.org/for/travelers/itinerary-opening-hours.png"></p></div><div><div><h2 id="real-time">Real time</h2><p>It is rare that a train or bus depart or arrive on time. Itinerary keeps you updated when delays are announced.</p><p>On supported train and long distance buses, Itinerary will also use the onboard APIs to fetch the current live status of the vehicle and keep you updated on your current position and any announcements.</p></div><p><img src="https://kde.org/for/travelers/itinerary-live-status.png"></p></div><div><h2 id="arianna">Arianna</h2><p>Arianna is an excellent ebook reader that lets you read your favorite books while traveling. Arianna will track your reading progress
and classify your books by genre and author automatically.</p><figure><img src="https://cdn.kde.org/screenshots/arianna/reader.png" alt="Screenshot of Arianna" title="Screenshot of Arianna"></figure></div><div><h2 id="kasts">Kasts</h2><p>Enjoy listening to podcasts on the move with Kasts! Subscribe to your favorite podcasts and get updated as soon as a new episode is out.</p><figure><img src="https://cdn.kde.org/screenshots/kasts/kasts-desktop.png" alt="Screenshot of Kasts" title="Screenshot of Kasts"></figure><p>Kasts is also available on Android and <a href="https://plasma-mobile.org/">Plasma Mobile</a>.</p></div><div><p><a href="https://community.kde.org/Android/FDroid#KDE_F-Droid_Release_Repository"><img src="https://kde.org/store_badges/fdroid/en.svg" alt="Get it on F-Droid"></a></p></div><div><h2 id="kgeotag">KGeoTag</h2><p>KGeoTag is a geotagging program. It lets you tag your images with geocoordinates by matching them to a corresponding GPX track or by manually setting them by drag and dropping the images, or entering the coordinates by hand.</p><p>It is very helpful in combination with <a href="https://www.kphotoalbum.org/">KPhotoAlbum</a> or <a href="https://www.digikam.org/">Digikam</a> when you want to organize your photo collection after your trip and then visualize all the locations you visited.</p><figure><img src="https://cdn.kde.org/screenshots/kgeotag/kgeotag_shadow.png" alt="Screenshot of KGeoTag showing a track on a map with some image preview allong the track" title="Screenshot of KGeoTag showing a track on a map with some image preview allong the track"></figure></div><div><h2 id="marble">Marble</h2><p>Explore the world with Marble. Marble contains a huge collection of maps that let you travel all over the globe from your desktop. Visit remote places via detailed satellite images, travel the world before the discovery of America, and check out the average temperature and precipitation in winter and summer in other countries.</p><p>Marble also allows you to bookmark your favorite locations and lets you find your way thanks to its routing algorithm and OpenStreetMap powered maps.</p><figure><img src="https://cdn.kde.org/screenshots/marble/marble-world.png" alt="Screenshot of Kasts" title="Screenshot of Kasts"></figure></div><div><h2 id="other-open-source-apps-for-you">Other open source apps for you</h2><p>Here are some other applications from other open source communities that will make your travels fun.</p></div></main></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Perl first commit: a “replacement” for Awk and sed (180 pts)]]></title>
            <link>https://github.com/Perl/perl5/commit/8d063cd8450e59ea1c611a2f4f5a21059a2804f1</link>
            <guid>36650120</guid>
            <pubDate>Sun, 09 Jul 2023 00:24:31 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/Perl/perl5/commit/8d063cd8450e59ea1c611a2f4f5a21059a2804f1">https://github.com/Perl/perl5/commit/8d063cd8450e59ea1c611a2f4f5a21059a2804f1</a>, See on <a href="https://news.ycombinator.com/item?id=36650120">Hacker News</a></p>
<div id="readability-page-1" class="page"><div itemscope="" itemtype="http://schema.org/SoftwareSourceCode" data-commit-hovercards-enabled="" data-discussion-hovercards-enabled="" data-issue-and-pr-hovercards-enabled="">
    <main id="js-repo-pjax-container">
      
  

    
    

    






  
  

  



<turbo-frame id="repo-content-turbo-frame" target="_top" data-turbo-action="advance">
    <div id="repo-content-pjax-container">
  <p>
  <h2>Commit</h2>
</p>

<p><a href="https://github.com/Perl/perl5/commit/8d063cd8450e59ea1c611a2f4f5a21059a2804f1" data-hotkey="y">Permalink</a></p>


<div>
  <p><a id="browse-at-time-link" href="https://github.com/Perl/perl5/tree/8d063cd8450e59ea1c611a2f4f5a21059a2804f1" rel="nofollow">Browse files</a></p><tool-tip id="tooltip-48b3911f-a378-408f-be27-09a795a734bd" for="browse-at-time-link" data-direction="ne" data-type="description" data-view-component="true">Browse the repository at this point in the history</tool-tip>
    <p>
      a "replacement" for awk and sed
    </p>

    <div><pre>[  Perl is kind of designed to make awk and sed semi-obsolete.  This posting
   will include the first 10 patches after the main source.  The following
   description is lifted from Larry's manpage. --r$  ]

   Perl is a interpreted language optimized for scanning arbitrary text
   files, extracting information from those text files, and printing
   reports based on that information.  It's also a good language for many
   system management tasks.  The language is intended to be practical
   (easy to use, efficient, complete) rather than beautiful (tiny,
   elegant, minimal).  It combines (in the author's opinion, anyway) some
   of the best features of C, sed, awk, and sh, so people familiar with
   those languages should have little difficulty with it.  (Language
   historians will also note some vestiges of csh, Pascal, and even
   BASIC-PLUS.) Expression syntax corresponds quite closely to C
   expression syntax.  If you have a problem that would ordinarily use sed
   or awk or sh, but it exceeds their capabilities or must run a little
   faster, and you don't want to write the silly thing in C, then perl may
   be for you.  There are also translators to turn your sed and awk
   scripts into perl scripts.</pre></div>

  <div>
  <include-fragment src="/Perl/perl5/branch_commits/8d063cd8450e59ea1c611a2f4f5a21059a2804f1" id="async-branches-list">
    
    <ul>
      <li>Loading branch information<span></span></li>
    </ul>
</include-fragment></div>


  
</div>


  


  <diff-layout>
    
        </diff-layout>


</div>

</turbo-frame>


    </main>
  </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[California needs real math education, not gimmicks (220 pts)]]></title>
            <link>https://www.noahpinion.blog/p/california-needs-real-math-education</link>
            <guid>36650010</guid>
            <pubDate>Sun, 09 Jul 2023 00:05:05 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.noahpinion.blog/p/california-needs-real-math-education">https://www.noahpinion.blog/p/california-needs-real-math-education</a>, See on <a href="https://news.ycombinator.com/item?id=36650010">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><article><div dir="auto"><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4431a65f-8a19-46b7-af11-ff6fd81b09b1_2000x810.jpeg" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4431a65f-8a19-46b7-af11-ff6fd81b09b1_2000x810.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4431a65f-8a19-46b7-af11-ff6fd81b09b1_2000x810.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4431a65f-8a19-46b7-af11-ff6fd81b09b1_2000x810.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4431a65f-8a19-46b7-af11-ff6fd81b09b1_2000x810.jpeg 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4431a65f-8a19-46b7-af11-ff6fd81b09b1_2000x810.jpeg" width="716" height="290.1373626373626" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/4431a65f-8a19-46b7-af11-ff6fd81b09b1_2000x810.jpeg&quot;,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:590,&quot;width&quot;:1456,&quot;resizeWidth&quot;:716,&quot;bytes&quot;:183684,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/jpeg&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:false,&quot;topImage&quot;:true,&quot;internalRedirect&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4431a65f-8a19-46b7-af11-ff6fd81b09b1_2000x810.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4431a65f-8a19-46b7-af11-ff6fd81b09b1_2000x810.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4431a65f-8a19-46b7-af11-ff6fd81b09b1_2000x810.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4431a65f-8a19-46b7-af11-ff6fd81b09b1_2000x810.jpeg 1456w" sizes="100vw" fetchpriority="high"></picture></div></a></figure></div><p><em><span>In an </span><a href="https://vimeo.com/65921206" rel="">old Saturday Night Live skit</a><span>, Chevy Chase, portraying Gerald Ford, says “It was my understanding that there would be no math.” Half a century later, it seems like this has become America’s national motto. Even as high-tech manufacturing has migrated relentlessly to China, plenty of Americans seem to think that they — or anyone — should be able to flourish in a modern economy without a functional understanding of mathematics. U.S. high school math scores </span><a href="https://www.bloomberg.com/opinion/articles/2019-12-12/u-s-schools-do-fine-in-international-rankings-except-in-math?sref=R8NfLgwS" rel="">lag significantly</a><span> behind other countries, even though scores in reading and science are above average, and our country is </span><a href="https://www.bloomberg.com/view/articles/2020-07-29/u-s-will-need-talented-refugees-if-skilled-immigrants-won-t-come?sref=R8NfLgwS" rel="">utterly dependent </a><span>on a continuous inflow of foreign talent for a number of critical STEM fields. Math, out of all subjects, seems to hold a special terror for Americans, who often seem to view the subject as a test of innate intelligence rather than </span><a href="https://www.theatlantic.com/education/archive/2013/10/the-myth-of-im-bad-at-math/280914/" rel="">a skill that can be acquired and honed</a><span> through hard work. </span></em></p><p><em><span>In response to lagging math scores, educators in California have been trying to water down math education — banning students from taking algebra in 8th grade, replacing advanced algebra classes with “data science” courses that don’t even teach the algebra required to understand basic statistics, and so on. I see this as an extremely wrongheaded move, and I’ve been meaning to write about it for a while. But data analyst </span><a href="https://twitter.com/ArmandDoma" rel="">Armand Domalewski</a><span>, a friend of mine, has been following the issue far more closely than I have, and has been outspoken about it on social media, so I thought I would ask him to take a crack at saying what needs to be said.</span></em></p><p>One of the strangest things about California is that it is simultaneously one of the technology capitals of the world and has some of the worst math scores for children in the entire United States. In practice, California has relied on a combination of pockets of home grown math excellence and imported math whizzes from around the globe to bridge the gap between the math skills it needs and the math skills it has. It works, somewhat—but we can and should do better by all of the kids in our state.</p><p>We have a chance to do exactly that with the release of a new California Math Framework (C.M.F.), a document used by the state to establish math curricula for all public schools in California. Unfortunately, that process has been hijacked by a “math reform” movement led by Stanford Professor Dr. Jo Boaler, who claims to advocate for a more inclusive way of teaching that would replace memorizing times tables with real-world problem-solving. Her worldview has gained credence in influential educational circles because, to many people, including myself, the basic premise is extremely appealing: replacing rote memorization with creative problem solving, making math more inclusive to all kinds of students, embracing a growth mindset, etc. all sound lovely. And honestly, I don’t think at a high level that these concepts are wrong—what is broken, however, is the specific implementation of these ideas as advocated by Dr. Boaler and implemented by California education policymakers.</p><div><p><span>Math </span><em>should </em><span>be more inclusive. Math </span><em>should </em><span>be more engaging. I think one of the biggest mistakes both Dr. Boaler’s supporters and detractors have made in this debate is to try to slot what should be a practical, fact based argument about optimal math education into an ideological struggle invoking silly phrases like Woke Math. Dr. Boaler and her fans did not make math education worse by being too left wing in their math—whatever that even means—but by sloppy with their science and lazy with their facts. You don’t make math education better by advocating for changes based on lies, and unfortunately, that is exactly what happened here. </span></p><p><span>Two of the major policy changes proposed in the draft CMF are already showing indications of disaster. The first is moving Algebra education out of 8th Grade.</span></p></div><div><p><span>One of the ideas underpinning the California Math Framework is the notion that math needs to be “detracked”—instead of allowing some students to take Algebra I in 8th grade, it would require all students to enroll in the same math curriculum until the 9th grade. Advocates argue&nbsp; this promotes equity, while detractors argue that it diminishes excellence. Years ago, I supported San Francisco’s efforts to rework the curriculum based on that argument—I believed those who said it would improve educational outcomes for the kids struggling the most without hurting those who were already succeeding. </span></p><p><span>I was wrong. Not only did pushing out 8th grade Algebra hurt kids who were at the top of their class by forcing them to pay for private classes or other workarounds to get the credits they needed to apply for UCs, the claim that it would help outcomes for kids who were struggling turned out to be a bald faced lie.</span></p></div><p><span>In 2017, </span><a href="http://www.sfusdmath.org/uploads/2/4/0/9/24098802/historic_shifts_in_math_show_promise.pdf" rel="">SFUSD</a><span> claimed a "dramatic increase in student comprehension" and a drop in Algebra 1 repeaters from 40% to 7%, and credited detracking. An analysis by</span><a href="https://www.familiesforsanfrancisco.com/updates/inequity-in-numbers" rel=""> Families for San Francisco</a><span> found that this claim was utter nonsense. Algebra 1 grades did not improve at all, and the only reason the repeat rate went down was because SFUSD straight up </span><em>eliminated the requirement that you had take an exit exam in order to progress!</em><span>&nbsp;</span></p><p>Not only that, but the group was unable to replicate the 40% to 7% drop using the data provided by SFUSD through a records request, and no other independent entity has been able to validate that number as well.</p><p><span>Page 6 of this </span><a href="https://static1.squarespace.com/static/60412a3a51d4863950d1bdf2/t/616e2f823696906267609f3f/1634611077888/Report-+Inequity+in+Numbers.pdf" rel="">report</a><span> is particularly damning–SFUSD provides numbers that, when reverse engineered with some basic Algebra, would imply a class size of 2475 students when the actual class size was 4011. What happened to the other 1,536 students? The shoddiness of this evidence did not stop Dr. Boaler from touting this as a major success for her ideas, and until very recently, did not stop the CMF from citing it as a major argument in favor of detracking.</span></p><div><p><span>Not only did detracking not achieve its stated goals of advancing math equity in San Francisco, it actually harmed Black and brown students. By the end of 10th grade, Algebra 2 enrollments of Black and brown students declined, since their families were less likely to afford the expensive work arounds that white and Asian families pursued. Instead, most of the district’s Black and Latino students ended up in a diluted “compression” course that lacked about 75% of the state’s</span><a href="https://www.cde.ca.gov/ci/ma/cf/documents/mathfwprecalculus.pdf" rel=""> precalculus</a><span> “+” standards, where the “+” standards are defined as “additional mathematics to prepare students for advanced courses,” making it difficult for students to pursue more advanced math in college. (Which is why, counter the claims of some detracking advocates, the UCs do not officially credit this compression course as “advanced math.”) </span></p><p><span>The result? They’re grim. If you compare </span><a href="https://www.educationnext.org/san-franciscos-detracking-experiment/" rel="">statewide results against SFUSD results on California’s Smarter Balanced tests,</a><span> which assess student performance across the state, you see that between 2015 and 2019, at the state level, the eleventh-grade Black-White student&nbsp; gap grew by 11 points—from 94 to 105—while in SFUSD, the gap expanded by 15 points (from 143 to 158). The outcomes are even worse for Hispanic students. The Hispanic-White gap at the state level gap grew by only 5 points, but in SF, it grew by 31 points.</span></p></div><p><span>As with all education data, there are always a million variables and you can never conclusively say that a single policy change caused a specific outcome, but at the very least, it is hard to argue that these 8th Grade Algebra changes advocated by Boaler helped SFUSD, and even harder to argue they serve </span><em>as a model for our state.&nbsp;</em></p><p>Unfortunately, that is not the only controversial policy change being pitched in the CMF: another poorly conceived notion is the replacement of the second year of Algebra with “data science.”</p><p><span>For decades, American math curriculum has followed a standard sequence: arithmetic, algebra, geometry, algebra II, precalculus and trigonometry, and calculus. The University of California required you to take three years of high-school math, culminating in Algebra II. In October 2020, the UC Board of Admissions and Relations with Schools (BOARS) </span><a href="https://senate.universityofcalifornia.edu/_files/committees/boars/documents/statement-on-mathematics-preparation-for-uc.pdf" rel="">recommended allowing alternatives</a><span> to the second year of algebra—including data science. Courses like “</span><a href="https://www.introdatascience.org/" rel="">Introduction to Data Science</a><span>,” developed by UCLA and “</span><a href="https://hsdatascience.youcubed.org/" rel="">Explorations in Data Science</a><span>,” developed by Dr. Boaler, started popping up. The argument was that these classes would teach data skills relevant to the 21st century, such as collecting and analyzing data on “real-world topics,” in contrast to Algebra II, which Boaler said was as relevant as “</span><a href="https://www.latimes.com/opinion/story/2019-10-23/math-high-school-algebra-data-statistics" rel="">sock darning and shorthand</a><strong><span>.” </span></strong><span>And look, in theory, this sounds nice. I mean, my job is literally data analyst—I analyze, evaluate, and interpret data for a living! When I first heard about this, I was thrilled. But when I thought about it a bit more, it gave me pause—the skills I use daily as a data analyst are based on a foundation of Algebra and Calculus. It didn’t quite make sense to me how you could </span><em>replace </em><span>Algebra II with data science—the formulas that make up linear regression, for example, don’t make any sense unless you have at least a basic grasp of algebra.&nbsp; Logarithms and trigonometric functions are pretty core to doing data science work! So I started digging into what was actually being taught in these “data science” courses and was…frankly, I was horrified.</span></p><p><span>While UC Admissions requirements state that Algebra II alternatives still have to “build on” certain core concepts in Algebra II, in practice this does not seem to be enforced. The “Introduction to Data Science” produced by UCLA contains </span><a href="https://www.ucladatascienceed.org/wp-content/uploads/California-Common-Core-Mathematics-Standards-addressed-by-IDS.pdf" rel="">very little Algebra II</a><strong> </strong><span>and “Explorations in Data Science” only claims to</span><a href="https://docs.google.com/presentation/d/e/2PACX-1vQ4tVbSk5qZsgARWwctjKa6joNKKYi7_jzi2-hkDyr7yGzQSQgKCndzhfaICVooye55ZZqCBEVXpxSv/pub?start=false&amp;loop=false&amp;delayms=3000&amp;slide=id.p" rel=""> teach the portions of Algebra II that overlap with statistics</a><span>, leaving huge swathes of math necessary for an eventual career in STEM completely untouched.&nbsp;</span></p><p><span>Frankly, reading the CMF does not give me the impression that its authors have a strong understanding of what data science </span><em>is</em><span>, exactly. It includes phrases like “the numbers are staggering: around 1.7 megabytes of digital data were created and stored every second for every person on Earth in 2020, and the vast majority of data goes unanalyzed.’ As </span><a href="https://sites.google.com/view/publiccommentsonthecmf/#h.w46loj4uaiev" rel="">Dr. Brian Conrad</a><span> points out, this is a nonsense statement. Is 1.7MB a large amount? (No, literally one JPEG can be that size.) Most of that is likely video, how exactly should that “data” be “analyzed”? And how do the authors even know it’s not being analyzed? After all, every video uploaded to YouTube gets tossed into an algorithm that produces viewing metrics, there are data scientists analyzing those uploads for trends, etc? (The authors probably found it by googling “impressive big data stats'', which spat out that statistic as the first result at the time the CMF was drafted.).&nbsp;</span></p><p><span>I wish this ignorance of the subject matter was limited to cute illustrative examples, but unfortunately, it permeates the basic thinking and structure of the document. The core issue of the CMF’s “data science” section is that it claims to be discussing data science while it is actually discussing </span><em>data literacy. </em><span>Don’t get me wrong—data literacy is good! Society would be better off if more people understood how to clean data or read a poll accurately. </span><em>But this is not data science and it is not math. </em><span>&nbsp;The CMF is replete with </span><a href="https://drive.google.com/file/d/1QI9XDw77ZlvwtcnLn_rKbhaWHo2UX-E2/view" rel="">statements</a><span> like “high-school data-science class students can learn to clean data sets – removing any data that is incorrect, corrupted, incorrectly formatted, duplicated, or incorrect in some other way [...] High school students can also learn to download and upload data, and develop the more sophisticated “data moves” that are important to learn if students are tackling real data sets.'' This teaches you how to use Excel, sure—but it does not teach you how regressions work, how statistical tests work, the multivariable calculus and linear algebra </span><em>you need to do the job of an actual data scientist!</em></p><p><span>In response to this, science and math professors across the state </span><a href="https://sites.google.com/view/mathindatamatters/home" rel="">have</a><span> been </span><a href="https://edsource.org/2022/proposed-mathematics-pathways-for-california-high-school-students-raise-equity-concerns/674400" rel="">raising</a><span> </span><a href="https://www.chronicle.com/article/the-university-of-california-changed-its-math-standards-some-faculty-arent-happy" rel="">alarms.</a><span> In response, Dr. Boaler turned to a tactic she often relies on—trying to wrap her ideas in the context of a broader culture war, painting critics as stodgy conservatives fighting her efforts to make math more equitable and diverse. She described her critics as those resisting change. The notion that teaching this version of data science rather than Algebra II is somehow more equitable permeates the CMF in often bizarre ways.&nbsp;</span></p><p><span>The CMF says data science is more </span><a href="https://sites.google.com/view/publiccommentsonthecmf/" rel="">equitable</a><span> than other STEM fields because “data scientists work together to address uncertainty in data while avoiding bias.”</span></p><p>Err, what? I’ve met many data scientists who do not work together or address uncertainty in data while avoiding bias, and many non-data science STEM professionals who do. There is absolutely nothing inherent to data science that makes it more collaborative or unbiased than other STEM fields…</p><div><p><span>It goes on to say…“Traditional mathematics lessons that have taught the subject as a set of procedures to follow have resulted in widespread disengagement as students see no relevance for their lives. This is particularly harmful for students of color and for girls…The data science field provides opportunities for equitable practice, with multiple opportunities for students to pursue answers to wonderings and to accept the reality that all students can excel in data science fields.'' </span></p><p><span>I agree that traditional math as currently taught does disengage a lot of students, and in particular women. But there is absolutely no evidence offered in the CMF to suggest that data science education would somehow be different, and there is something profoundly weird about the suggestion that students of color and girls can excel in data science fields but not excel in other fields of mathematics. The primary reason girls, for example, diverge from boys in math performance is </span><em><a href="https://techcrunch.com/2016/01/05/why-stems-future-rests-in-the-hands-of-12-year-old-girls/" rel="">because society teaches them that math is not for girls.</a></em><span> That is not something swapping out actual math for a watered down “data science” course can solve, and it’s pretty gross for people to claiming to be trying to make math more equitable for women and people of color to be pushing a program that will actually make them </span><em>worse at math. </em><span>Dr. Boaler and the CMF are basically saying “women and people of color aren’t doing as well in math, so we should just </span><em>give up on teaching them actual math</em><span>. It’s bananas!</span></p></div><p><span>But don’t take my white, male word for it—a group of Black UC faculty members in data science-related fields wrote a </span><a href="https://www.chronicle.com/article/the-university-of-california-changed-its-math-standards-some-faculty-arent-happy" rel="">letter</a><span> stating, “‘Introduction to Data Science’...make[s] claims that they specifically support learning for women and minorities, which are not only baseless, but fail to appreciate that they actually do the opposite and harm students from such groups by steering them away from being prepared for STEM majors.”</span></p><p><span>There’s a reason why these folks have been joined by other Black mathematicians around the country, such as </span><a href="https://www.chronicle.com/article/the-divider" rel="">Dr. Jelani Nelson,</a><span> in pushing back fiercely against the ideas around 8th Grade Algebra and data science proposed in the CMF. (And a reason, perhaps, that Dr. Boaler </span><a href="https://nypost.com/2022/04/08/stanford-prof-calls-cops-on-berkeley-prof-who-exposed-her-5k-hour-consulting-fee/" rel="">threatened to call the police on him for it!</a><span>) There’s a reason why Stanford Mathematics professor Dr. Brian Conrad&nbsp; wrote, in a comprehensive </span><a href="https://sites.google.com/view/publiccommentsonthecmf/?ref=stanfordreview.org#h.ns5n6hdqa4x8" rel="">takedown</a><span> of the CMF you really should read, that “whatever author is responsible for such a myopic view of mathematics should never again be involved in the setting of public policy guidance on math education.” There’s a reason why the </span><em><span>authors of papers Dr. Boaler cites to </span><a href="https://www.chronicle.com/article/the-divider?cid=gen_sign_in" rel="">back up her work consistently say she has misread and misrepresented their work</a></em><a href="https://www.chronicle.com/article/the-divider?cid=gen_sign_in" rel="">,</a><span> and that it does not support the claims she is making. And the reason, simply, is that her ideas have not worked. Forcing all children to defer Algebra until 9th grade,trying to squeeze two years of schooling into one year of a watered down “compression course” rejected by the University of California for not meeting its </span><a href="https://icas-ca.org/wp-content/uploads/2020/05/ICAS-Statement-Math-Competencies-2013.pdf" rel="">standards</a><span>, and replacing Algebra II with a glorified data literacy course masquerading as a “data science” course does not help high achieving kids or struggling kids or any kids in between—it hurts them all.</span></p><p><span>California students need different answers on math–what we’ve been doing for the past few decades hasn’t worked. But that doesn’t mean we should embrace the ideas embodied in the current CMF draft, which were built on decades of </span><a href="https://www.nonpartisaneducation.org/Review/Articles/v8n1.pdf" rel="">shoddy and dishonest academic research,</a><span> and throw up our hands at the notion of teaching underperforming kids advanced math entirely. The good news is that there are answers out there—we can learn from </span><a href="https://www.theatlantic.com/education/archive/2013/10/the-myth-of-im-bad-at-math/280914/" rel="">other countries teach math differently than we do,</a><span> we can integrate findings from the </span><a href="https://www.cis.org.au/publication/myths-that-undermine-maths-teaching/" rel="">“science of math”</a><span>, and more. Our kids deserve a better California Math Framework than the one we’re being offered now—let’s get it done.</span></p><p data-attrs="{&quot;url&quot;:&quot;https://www.noahpinion.blog/p/california-needs-real-math-education?utm_source=substack&amp;utm_medium=email&amp;utm_content=share&amp;action=share&quot;,&quot;text&quot;:&quot;Share&quot;,&quot;action&quot;:null,&quot;class&quot;:null}" data-component-name="ButtonCreateButton"><a href="https://www.noahpinion.blog/p/california-needs-real-math-education?utm_source=substack&amp;utm_medium=email&amp;utm_content=share&amp;action=share" rel=""><span>Share</span></a></p></div></article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Pocket gets worse the more you use it (2019) (113 pts)]]></title>
            <link>https://web.archive.org/web/20190512092903/https://old.reddit.com/r/dredmorbius/comments/5x2sfx/pocket_it_gets_worse_the_more_you_use_it/</link>
            <guid>36649740</guid>
            <pubDate>Sat, 08 Jul 2023 23:19:47 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://web.archive.org/web/20190512092903/https://old.reddit.com/r/dredmorbius/comments/5x2sfx/pocket_it_gets_worse_the_more_you_use_it/">https://web.archive.org/web/20190512092903/https://old.reddit.com/r/dredmorbius/comments/5x2sfx/pocket_it_gets_worse_the_more_you_use_it/</a>, See on <a href="https://news.ycombinator.com/item?id=36649740">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>Having used Pocket's article-archival-and-management tool -- sort of a bookmarks-on-steroids product -- for the past year or two, and with news that the company has just been acquired by Mozilla (it's been getting increasingly integrated into Firefox over the past year or so), I've realised one of my most fundamental complaints about it.</p>

<p><strong>The more I use Pocket, the worse it gets.</strong></p>

<p>As I've mentioned (a few too many times), I do a lot of research, much of it with articles and documents, and tools for managing the pile / heap / mountain / swamp are a constant and growing consideration.  Pocket addresses one part of the problem:  online HTML-based content.</p>

<p>I've compiled a large set of articles.  I'm a top-1% reader, according to an emailed report sent a month or so back.  Some 2,000 or so in Readability, prior to its demise, manually transferred in the week before that service was to go dark, and another 1,000 - 3,000 likely in Pocket itself.  I make copious use of tagging features.  My intent is to have <em>a vetted, known, classified, and useful set of references to comb through for specific research needs as I organise and write</em>.</p>

<p>The fact that I can't even say, not even <em>approximately</em>, how much material I've archived, is a profound signifier of the design failures and lack of consideration of use-cases.  The folks at Pocket seem to have given absolutely no thought to how people might use their product, or benefit by self-directed use.  (And it's not as if they've not heard:  I've shared this complaint with them multiple times over the past two years.)  Which is to say:  even at the most basic level, <em>the product isn't getting better or more useful</em>.</p>

<p>The problem is, the bigger the pile gets, the less manageable it becomes.</p>

<h2>Tags</h2>

<p>Great, I have an unlimited set of tags.</p>

<p>It takes me 45 seconds just to scroll from the top of the list to the end on the Android app.</p>

<p>The tags are not searchable.  A "type and autocomplete" feature -- you know, the sort of thing software has offered since the 1980s, would be peachy.  No such luck.</p>

<p>The tags don't auto-complete <em>and activate such that, say, I can hit &lt;enter&gt; to select one</em> when filing new material.</p>

<p>If I make a tyop, I cannot, say, <em>select a tag and edit it right there</em>.  No, I've got to:</p>

<ol>
<li>Switch to the "Tags" view.</li>
<li>Select the "Edit" option.  <em>Before</em> I search for the tag I want to edit.</li>
<li>Scroll through the list to where the tag in question is.  Given a 45s full-list scan, this takes an average of about 22 seconds.</li>
<li>Edit and save the tag.</li>
</ol>

<p>What <em>could</em> be a two-second, in-place operation, becomes an epic-journey-to-a-distant-land-and-quest-for-a-holy-grail, fraught with pitfalls and traps -- if you make the wrong turn, you waste time, have to backtrack, and start over again.  During all of which your flow-of-thought is being completely interrupted.</p>

<p>Desktop clients (Web) don't appear to be any better.</p>

<p>If I happen to typo a tab and want to delete it, in the Android app, with my Bluetooth keyboard attached ... I cannot.  I've got to switch to the <em>software</em> keyboard, re-select the tag in question (because, of course, it doesn't stay selected), then delete it.</p>

<p>It's not possible to filter by <em>multiple</em> tags.  Something which is hugely annoying as that is a fast and efficient way to cut through a large mass of material -- items which are cross-referenced and multi-tagged could be, say, filtered with the remaining tags within the set listed.  Check the ones you're interested in and you should end up with a small set of items of interest.</p>

<p>And finally:  the tags display is highly minimal and hidden under windows.  These should be presented <em>on the document itself</em> (head or foot, preferably), <em>with all tags visible at all times</em>.  Selecting a given tag should call up all content under it (and provide for further filtering by other tags).</p>

<h2>Search</h2>

<p>For a time, it seemed that Pocket had a full-text article search, able to use multiple words.  That ... seems to have vanished.</p>

<p><strong>Keep in mind:  Pocket archives its articles locally.  <em>The search can run over the local archive, and doesn't impose any server load.</em>  But it doesn't.</strong></p>

<p>No bueno.</p>

<p>Which means that <em>despite having an archive of data sitting on my own device, in text form, eminently searchable</em>, my best option is to try an online search (of a much larger corpus), and hoping I might land the items of interest.</p>

<p>This is, to say the least, slightly frustrating.</p>

<p>The fact that Pocket's search, such as it is, seems to be limited to <em>a single keyword</em>, to not support <code>"quoted strings"</code>, or <code>-excluded terms</code>, or field-specific criteria (author, date, publisher, website), ranges, etc., is ... similarly a staggering oversight.</p>

<p>Frankly, I'd get more utility (and am strongly considering how I might accomplish this) downloading or fetching content locally, and running various search/index tools over it.</p>

<h2>Search</h2>

<p>No, that's not a typo:  I'm referring to <em>in-document</em> search.  On the Android app, there is no text search <em>within</em> documents.</p>

<p>If I want to find a particular passage, I've either got to vgrep for it (scan manually), switch to one of the Web interfaces, or pull up the original article online.  Another staggering oversight.  (Though in fairness:  fairly common amongst Android apps, which only means the firing squad's work is all the larger.)</p>

<h2>Workflow</h2>

<p>There's no concept of workflow.</p>

<p>Generally, I'm stashing stuff for later review, on which I'll be associating it with various projects, dumping into a general file, or indicating it's been seen and found wanting.  A set of workflow-oriented features would help in this.  No such thing exists.</p>

<h2>Going from Browser to Pocket</h2>

<p>One of my most frequent operations is to open a Web page, discover that it is utterly fucked over in its page design, and want to open it in Pocket.  What I'd <em>like</em> to do is:</p>

<ol>
<li>Open the current tab immediately in Pocket, whilst closing the present window.</li>
</ol>

<p>What I've got to do instead is:</p>

<ol>
<li>Save to Pocket.</li>
<li>Try to close the current tab -- tricky at best on a mobile device given imprecise location control and click-vs-drag ambiguity, plus focus-stealing by Pocket meaning a keyboard &lt;ctrl&gt;-W generally doesn't work.</li>
<li>Punch the Pocket icon that appears to switch to Pocket.</li>
<li>Land on the Pocket <em>article list</em> rather than <em>the article I've just added to it</em>, requiring a 2nd step to get to that.</li>
</ol>

<p>A one-step process has become a 4-5 step process.  Every. Single. Time.</p>

<p>In the way of Pains Suffered Through Life, it's not among the largest.  But it's a telling failure of attention to detail, or consideration of What the User Might Want.</p>

<h2>Reputation</h2>

<p>Since I'm referencing my corpus, over time I'll build up a set of "hot" articles that are referenced more frequently than others.  I might want to have these turn up quickly in searches ... or perhaps exclude them to find other potentially relevant material.  Again, lack of any user-oriented statistics means this isn't supported.</p>

<p>Similarly, I'd like to be able to indicate reputation of sources, for accuracy or insight.  I don't make a habit of choosing a <em>large</em> amount of bogus material, but since one of my research areas is the concept of bogosity itself, there are a few.  And there are also sources and authorities who are particularly compelling.  Being able to track reputation by author, publication, and URL, would again be highly useful.</p>

<h2>Export lists</h2>

<p>A chief value of being able to categorise content is <em>to be able to call it up, and share it</em>, when desired.</p>

<p>I can ... very barely sort-of ... search through and find some subset of my articles.</p>

<p>What I <em>cannot</em> do, and what I've wanted to do many times, is to apply one of the non-existent advanced search features above, to select out a set of, say, 2-12 articles or references I think will be particularly useful to someone, and dump that as a set of URLs (to either Pocket or the original sources).  A hugely useful capability, and one which could well help to popularise Pocket itself.  But not present.</p>

<h2>Highlights and notes</h2>

<p>Again:  for research, I'm reading material for synthesis, not just pleasure.  Which means I want to make comments, mark relevant passages, etc.  There's no capability to do so.</p>

<h2>Paying for it wouldn't help</h2>

<p>I had, for a while, the trial-mode advanced usage features of Pocket.  That <em>may</em> have included full-text search (it's not clear that this was or wasn't included, and ... it's painfully difficult to find out just what the full-product features are).  There was a "suggested tags" feature, which was nice, though not essential, and I find I somewhat prefer <em>thinking</em> about tags <em>without</em> having them suggested to me (though the ability to check against suggestions would be useful).</p>

<p>But none of the other features I've listed are currently in the paid app.  There's been no visible work I'm aware toward any of them over the past year or so.</p>

<p>I'm familiar with arguments for paying for software.  I've never found them particularly convincing, as an individual user, particularly given my experience with Free Software over the years.  Whilst I've seen a fair number of FS projects with crappy user relations, in general I've found that:</p>

<ol>
<li>The software already anticipates my needs.</li>
<li>Developers are responsive to intelligent requests.</li>
<li>I can contribute myself, to my abilities -- a small number of bugfixes, rather more bug reports, occasional documentation.</li>
</ol>

<p>In the proprietary world, <em>if you are a significant customer</em>, it's possible to see requests built out.  Ordinary users, particularly in the Web world where userbases are measured in the 100s of millions or billions, rather less so.  In fact, generally, I've seen long-standing requests <em>from myself and numerous others</em> utterly ignored, for years.  Most especially if they are for "advanced user" features -- anything remotely generative.</p>

<p>A few hours ago I posted an item at Ello about the DMOZ hierarchy categories -- a list of 800,000+ classifications of online content.  I'd done some quick classification of it, on my Android tablet, using Termux, an add-on Linux environment with actually-capable shell tools, including auto-generating a Markdown table.  It's a small example of the power of such tools -- the ability to scan through nearly a million items and reduce them to a meaningful report in 18 rows, ready for publication.  (Mind:  Ello's table support appears broken, though I've also posted a copy to a Reddit sandbox.)</p>

<p><strong>That is the level of power and flexibility I expect from, no, demand from, my tools.  And am finding increasingly lacking.</strong></p>

<p>It took me most of a year to even discover Termux, and another several months to learn of the API features enabling clipboard interaction with the Android environment.  Which I'm using, incidentally, to compose this Reddit post, given the pains and pitfalls of using the Web interfaces on Android.</p>

<p>I suspect too that the individual-subscriber market isn't worth all that much to Pocket either.  Dealing with individual payments and the hassles thereof (from <em>both</em> sides:  credit card and identity fraud affect customers as well), make the margins afforded by support minimal.  Bundling and large-account sales are, with very few exceptions, where the money in software has been made.  Researching a set of Murphy's and related laws earlier, I came across Mark Miller's exception to Crane's Law:</p>

<blockquote>
<p>There are no "free lunches", but sometimes it costs more to collect money than to give away food.  </p>
</blockquote>

<p>That's among the motivating influences behind Free Software as well, though it helps to realise that costs can impose themselves in numerous ways.  An avoided cost of free software <em>can</em> be (though not always) the contributions and assistance of others in improving your product.  Going closed and proprietary loses that, though with possibly other beneficial trade-offs.</p>

<h2>What to do?</h2>

<p>Back to Pocket:  my view for now is much as I was treating Readability for the 2-3 years in which that project was obviously a dead man walking.  It was unsuitable to my needs, but marginally better than nothing at all, or other options.  The transition costs (to another proprietary tool, to a nonproprietary tool, my own solution) are all high.  I'm not sure how much metadata I can extract from Pocket, and loss of my tags would be a major hassle.  The evaluation cost of alternatives (Pinboard.in is highest on the list) is itself high.  I've been looking at an Emacs-based option, or ... something.  Using mutt or an email-storage format as an article reference tool has crossed my mind more than once.  With an IMAPS server, that gives me remote access, search, filter, annotation, and, with some add-on tagging or other classification systems, more organising capabilities.</p>

<p>Or some sort of DIY web-based interface, with some virtual filesystem approaches to addressing a <a href="https://web.archive.org/web/20190512092903/https://news.ycombinator.com/item?id=13751560">larger set of concepts</a> including metadata-as-search based on, say, title, author, other persons, dates, organisations, hashes, or content.</p>

<p>I'd also very much like to have other document formats -- PDFs, ePubs, DJVU, Mobi -- included.  For which, the capability to search meaningfully <em>within those docs, as text</em> would be handy (I'm getting well past the point of badly wanting a <code>pdfgrep</code> or <code>ocrgrep</code> type tool -- which reminds me that converting a substantial set of scanned books to eBook format(s) is another pending project).</p>

<p>Mostly though, I wonder why such things are, 25 years into the WWW, approaching 50 years from the birth of Unix, seventy years after Vannevar Bush's Memex proposal, so fucking hard to get right.  </p>

<p>Is it the problem itself, the people working on it, the dynamics of trying to commercialise such systems, or what?</p>

<hr>

<h2>Updates</h2>

<h3>17 March, 2017</h3>

<p>I'd submitted a request to Pocket support when I first posted this, about two weeks ago now.  Having heard no response, other than an automated acknowledgement, I've just submitted it again yesterday.  Again, an automated response but nothing more.</p>

<p>This is disappointing.</p>

<p>Pocket <em>have</em> tended to be responsive, and friendly, to requests, which was a ray of hope.  Mind:  actually <em>addressing</em> the substance of those requests through fixes and enhancements, not so much.  I'm aware that there can be long lists of such requests, that there's a lot of behind-the-scenes work, and more.  But over the course of 1-2 years, with some pretty significant issues, I'd hope to see <em>some</em> progress.  There's been ... none.</p>

<p>Elements of the product as it stands are good.  But at least for my use-case, it's the oversights which are increasingly glaring and inexcuseable.  I don't like rolling my own for all the obvious reasons, but it really seems as if what I'm looking for doesn't exist.</p>

<h3>7 May, 2017</h3>

<p>I did finally hear back from Pocket a few days ago, they've seen the comments here and acknowledged them.  For that alone, I'm grateful.</p>

<p>Foulups happen and messages get dropped, I've seen that.  I've <em>also</em> seen projects go completely dark (Readability had done that for a few years before shutting down), so ... it's a concerning sign.</p>

<p>I <em>would</em> like to see movement.  If not, I'm considering other options.  Semantic filesystems or something along those lines is starting to sound more interesting.</p>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Machine Unlearning Challenge (144 pts)]]></title>
            <link>https://ai.googleblog.com/2023/06/announcing-first-machine-unlearning.html</link>
            <guid>36649710</guid>
            <pubDate>Sat, 08 Jul 2023 23:14:44 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://ai.googleblog.com/2023/06/announcing-first-machine-unlearning.html">https://ai.googleblog.com/2023/06/announcing-first-machine-unlearning.html</a>, See on <a href="https://news.ycombinator.com/item?id=36649710">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="post-body-667019077470952746">
<p><span>Posted by Fabian Pedregosa and Eleni Triantafillou, Research Scientists, Google</span>

</p><p>
Deep learning has recently driven tremendous progress in a wide array of applications, ranging from <a href="https://imagen.research.google/">realistic image generation</a> and <a href="https://ai.googleblog.com/2023/06/retrieval-augmented-visual-language-pre.html">impressive retrieval systems</a> to <a href="https://blog.google/technology/ai/bard-google-ai-search-updates/">language models that can hold human-like conversations</a>. While this progress is very exciting, the widespread use of deep neural network models requires caution: as guided by Google’s AI <a href="https://ai.google/responsibility/principles/">Principles</a>, we seek to develop AI technologies responsibly by understanding and mitigating potential risks, such as the propagation and amplification of unfair biases and protecting user privacy.
</p> <p>
Fully erasing the influence of the data requested to be deleted is challenging since, aside from simply deleting it from databases where it’s stored, it also requires erasing the influence of that data on other artifacts such as trained machine learning models. Moreover, recent research [<a href="https://arxiv.org/abs/1610.05820">1</a>, <a href="https://arxiv.org/abs/2112.03570">2</a>] has shown that in some cases it may be possible to infer with high accuracy whether an example was used to train a machine learning model using <a href="https://en.wikipedia.org/wiki/Adversarial_machine_learning#Model_extraction">membership inference attacks</a> (MIAs). This can raise privacy concerns, as it implies that even if an individual's data is deleted from a database, it may still be possible to infer whether that individual's data was used to train a model. 
</p>
<p>
Given the above, <em>machine unlearning</em> is an emergent subfield of machine learning that aims to remove the influence of a specific subset of training examples — the "forget set" — from a trained model. Furthermore, an ideal unlearning algorithm would remove the influence of certain examples <em>while maintaining</em> other beneficial properties, such as the accuracy on the rest of the train set and generalization to held-out examples. A straightforward way to produce this unlearned model is to retrain the model on an adjusted training set that excludes the samples from the forget set. However, this is not always a viable option, as retraining deep models can be computationally expensive. An ideal unlearning algorithm would instead use the already-trained model as a starting point and efficiently make adjustments to remove the influence of the requested data.
</p>
<p>
Today we're thrilled to announce that we've teamed up with a broad group of academic and industrial researchers to organize the <a href="https://unlearning-challenge.github.io/">first Machine Unlearning Challenge</a>. The competition considers a realistic scenario in which after training, a certain subset of the training images must be forgotten to protect the privacy or rights of the individuals concerned. The competition will be hosted on <a href="https://www.kaggle.com/">Kaggle</a>, and submissions will be automatically scored in terms of both forgetting quality and model utility. We hope that this competition will help advance the state of the art in machine unlearning and encourage the development of efficient, effective and ethical unlearning algorithms.
</p>




<h2>Machine unlearning applications</h2>


<p>
Machine unlearning has applications beyond protecting user privacy. For instance, one can use unlearning to erase inaccurate or outdated information from trained models (e.g., due to errors in labeling or changes in the environment) or remove harmful, manipulated, or outlier data. 
</p>
<p>
The field of machine unlearning is related to other areas of machine learning such as <a href="https://en.wikipedia.org/wiki/Differential_privacy">differential privacy</a>, <a href="https://arxiv.org/abs/1802.07569">life-long learning</a>, and <a href="https://en.wikipedia.org/wiki/Fairness_(machine_learning)">fairness</a>. Differential privacy aims to guarantee that no particular training example has too large an influence on the trained model; a stronger goal compared to that of unlearning, which only requires erasing the influence of the designated forget set. Life-long learning research aims to design models that can learn continuously while maintaining previously-acquired skills. As work on unlearning progresses, it may also open additional ways to boost fairness in models, by correcting unfair biases or disparate treatment of members belonging to different groups (e.g., demographics, age groups, etc.).
</p>




<table><tbody><tr><td><a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiRnut8P03hlk5tKJPEEsqUl1DSlqN2ScdJeiaRfC3mWbQ_PBBwf7wBU9xgxuzr1GoqgkB6MwCa6Zrdo6LQxSOIPXIUrl1Yug73k2Q2zFI61VDAi9K21JOPox0Hc1CIh6ShKxW9Tgy45TYV3p3r5IiI7yxzzzOpzvbJ-5o3QVtjZn6vhDZLntnCcUSi1mb_/s720/image1.png" imageanchor="1"><img data-original-height="405" data-original-width="720" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiRnut8P03hlk5tKJPEEsqUl1DSlqN2ScdJeiaRfC3mWbQ_PBBwf7wBU9xgxuzr1GoqgkB6MwCa6Zrdo6LQxSOIPXIUrl1Yug73k2Q2zFI61VDAi9K21JOPox0Hc1CIh6ShKxW9Tgy45TYV3p3r5IiI7yxzzzOpzvbJ-5o3QVtjZn6vhDZLntnCcUSi1mb_/s16000/image1.png"></a></td></tr><tr><td><b>Anatomy of unlearning.</b> An unlearning algorithm takes as input a pre-trained model and one or more samples from the train set to unlearn (the "forget set"). From the model, forget set, and retain set, the unlearning algorithm produces an updated model. An ideal unlearning algorithm produces a model that is indistinguishable from the model trained without the forget set.</td></tr></tbody></table>



<h2>Challenges of machine unlearning</h2>


<p>
The problem of unlearning is complex and multifaceted as it involves several conflicting objectives: forgetting the requested data, maintaining the model’s utility (e.g., accuracy on retained and held-out data), and efficiency. Because of this, existing unlearning algorithms make different trade-offs. For example, full retraining achieves successful forgetting without damaging model utility, but with poor efficiency, while <a href="https://arxiv.org/abs/2007.02923">adding noise</a> to the weights achieves forgetting at the expense of utility. 
</p>
<p>
Furthermore, the evaluation of forgetting algorithms in the literature has so far been highly inconsistent. While some <a href="https://arxiv.org/abs/1911.04933">works</a> report the classification accuracy on the samples to unlearn, <a href="https://proceedings.mlr.press/v119/wu20b.html">others</a> report distance to the fully retrained model, and yet others use the error rate of membership inference attacks as a metric for forgetting quality [<a href="https://arxiv.org/abs/2302.09880">4</a>, <a href="https://arxiv.org/abs/2010.10981">5</a>, <a href="https://arxiv.org/abs/2005.02205">6</a>].
</p>
<p>
We believe that the inconsistency of evaluation metrics and the lack of a standardized protocol is a serious impediment to progress in the field — we are unable to make direct comparisons between different unlearning methods in the literature. This leaves us with a myopic view of the relative merits and drawbacks of different approaches, as well as open challenges and opportunities for developing improved algorithms. To address the issue of inconsistent evaluation and to advance the state of the art in the field of machine unlearning, we've teamed up with a broad group of academic and industrial researchers to organize the first unlearning challenge.
</p>




<h2>Announcing the first Machine Unlearning Challenge</h2>


<p>
We are pleased to announce the <a href="https://unlearning-challenge.github.io/">first Machine Unlearning Challenge</a>, which will be held as part of the <a href="https://neurips.cc/Conferences/2023/CompetitionTrack">NeurIPS 2023 Competition Track.</a> The goal of the competition is twofold. First, by unifying and standardizing the evaluation metrics for unlearning, we hope to identify the strengths and weaknesses of different algorithms through apples-to-apples comparisons. Second, by opening this competition to everyone, we hope to foster novel solutions and shed light on open challenges and opportunities.
</p>
<p>
The competition will be hosted on <a href="https://www.kaggle.com/">Kaggle</a> and run between mid-July 2023 and mid-September 2023. As part of the competition, today we're announcing the availability of the <a href="https://github.com/unlearning-challenge/starting-kit">starting kit</a>. This starting kit provides a foundation for participants to build and test their unlearning models on a toy dataset.
</p>
<p>
The competition considers a realistic scenario in which an age predictor has been trained on face images, and, after training, a certain subset of the training images must be forgotten to protect the privacy or rights of the individuals concerned. For this, we will make available as part of the starting kit a dataset of synthetic faces (samples shown below) and we'll also use several real-face datasets for evaluation of submissions. The participants are asked to submit code that takes as input the trained predictor, the forget and retain sets, and outputs the weights of a predictor that has unlearned the designated forget set. We will evaluate submissions based on both the strength of the forgetting algorithm and model utility. We will also enforce a hard cut-off that rejects unlearning algorithms that run slower than a fraction of the time it takes to retrain. A valuable outcome of this competition will be to characterize the trade-offs of different unlearning algorithms.
</p>




<table><tbody><tr><td><a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEijGdpNGKrQ9AskeRnXVSjPcFrjFPWs5TvXIAeD0gkJVL0hizxuJ4LL24rdKuNPUr86ivbaJZ5x-3dHBBQzLTbFYUWQ9p3ER5THVgv6xpOvK45_67ueGCtJsJVHrlkBKSfbz-21PrI2nkNGmoPcOkO_rqjR9W1-eDTxcjM6NNqqJkxMXMpRym_SYt3v6Wwn/s2000/image2.png" imageanchor="1"><img data-original-height="400" data-original-width="2000" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEijGdpNGKrQ9AskeRnXVSjPcFrjFPWs5TvXIAeD0gkJVL0hizxuJ4LL24rdKuNPUr86ivbaJZ5x-3dHBBQzLTbFYUWQ9p3ER5THVgv6xpOvK45_67ueGCtJsJVHrlkBKSfbz-21PrI2nkNGmoPcOkO_rqjR9W1-eDTxcjM6NNqqJkxMXMpRym_SYt3v6Wwn/s16000/image2.png"></a></td></tr><tr><td>Excerpt images from the <a href="https://github.com/microsoft/FaceSynthetics">Face Synthetics</a> dataset together with age annotations. The competition considers the scenario in which an age predictor has been trained on face images like the above, and, after training, a certain subset of the training images must be forgotten.</td></tr></tbody></table>



<p>
For evaluating forgetting, we will use tools inspired by MIAs, such as <a href="https://arxiv.org/abs/2112.03570">LiRA</a>. MIAs were first developed in the privacy and security literature and their goal is to infer which examples were part of the training set. Intuitively, if unlearning is successful, the unlearned model contains no traces of the forgotten examples, causing MIAs to fail: the attacker would be <em>unable</em> to infer that the forget set was, in fact, part of the original training set. In addition, we will also use statistical tests to quantify how different the distribution of unlearned models (produced by a particular submitted unlearning algorithm) is compared to the distribution of models retrained from scratch. For an ideal unlearning algorithm, these two will be indistinguishable. 
</p>



<h2>Conclusion</h2>


<p>
Machine unlearning is a powerful tool that has the potential to address several open problems in machine learning. As research in this area continues, we hope to see new methods that are more efficient, effective, and responsible. We are thrilled to have the opportunity via this competition to spark interest in this field, and we are looking forward to sharing our insights and findings with the community.
</p>



<h2>Acknowledgements</h2>


<p>
<em>The authors of this post are now part of Google DeepMind. We are writing this blog post on behalf of the organization team of the Unlearning Competition: Eleni Triantafillou*, Fabian Pedregosa* (*equal contribution), Meghdad Kurmanji, Kairan Zhao, Gintare Karolina Dziugaite, Peter Triantafillou, Ioannis Mitliagkas, Vincent Dumoulin, Lisheng Sun Hosoya, Peter Kairouz, Julio C. S. Jacques Junior, Jun Wan, Sergio Escalera and Isabelle Guyon.</em>
</p>

</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[You Suck at Excel (2015) [video] (164 pts)]]></title>
            <link>https://www.youtube.com/watch?v=0nbkaYsR94c</link>
            <guid>36649047</guid>
            <pubDate>Sat, 08 Jul 2023 21:41:02 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.youtube.com/watch?v=0nbkaYsR94c">https://www.youtube.com/watch?v=0nbkaYsR94c</a>, See on <a href="https://news.ycombinator.com/item?id=36649047">Hacker News</a></p>
&lt;Unparsable&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[HNInternal: Open Letter to Tim O’Reilly to Free the Perl Camel (151 pts)]]></title>
            <link>https://news.ycombinator.com/item?id=36648949</link>
            <guid>36648949</guid>
            <pubDate>Sat, 08 Jul 2023 21:27:44 GMT</pubDate>
            <description><![CDATA[<p>See on <a href="https://news.ycombinator.com/item?id=36648949">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p><span>This is (was?) O'Reilly's stance on the matter:<p><a href="https://web.archive.org/web/20180425080044/http://archive.oreilly.com/pub/a/oreilly/perl/usage" rel="nofollow noreferrer">https://web.archive.org/web/20180425080044/http://archive.or...</a></p><p>The Perl Camel Usage and Trademark Information</p><p>As most of you probably know, O'Reilly started putting animal images on the covers of our books about thirteen years ago. To millions of readers, the animals mean O'Reilly. They've become our signature "trade dress." We've also trademarked the association between particular animals and the subject of their books. After all, the only reason that people think of camels in association with Perl is because we used a camel on the cover of Programming Perl.</p><p>We recognize that things do get more complicated, though, when an image like the camel is so widely known that it comes to symbolize not just our products but also the entire Perl language. This is a good thing, and we want it to continue. But trademark law is sticky on this point. If a trademark isn't "protected" (by letters asking people not to use it, or by licenses that allow them to use it only in specific ways), it gets into the public domain and loses its protected status. If this happened, anyone could use the camel without restriction, including in ways that were detrimental to the language. For example, you might imagine a company creating a Perl-compatible language, branding it with a camel, and pushing it as the "official Perl" in an attempt to drive Larry Wall's Perl out of existence.</p><p>Another important issue is that a brand is strong in proportion to two things: its ubiquity and its distinctiveness. It's important that, just as we want one version of Perl (so we don't have the fragmentation that was the downfall of UNIX), we have one symbol for Perl. To protect the integrity and impact of that symbol, we need to maintain some artistic control over what kinds of camel images are used. We believe that "one camel" will strengthen the overall Perl brand.</p><p>In short, we're walking a fine line, trying to make the camel as available as possible as a symbol for Perl while protecting it as a trademark. So, here's our policy on using the camel image:</p><p>Non-commercial use</p><p>We will license the camel image widely for open source products and non-commercial sites related to Perl, requiring only an acknowledgement of its trademark status and a link to www.perl.com. To request the camel artwork, please send email to permissions@oreilly.com, indicating where, how, and for what purpose you plan to use the image. Please note that we generally do not allow alterations of the Perl camel artwork.</p><p>Some non-commercial sites currently using the Perl camel:</p><p>(snipped)</p><p>We also offer the Programming Republic of Perl logo for some non-commercial sites. Feel free to download these logos for use on your pages. Please make the logo a link to www.perl.com.</p><p>Some sites using the Programming Republic of Perl logo:</p><p>(snipped)</p><p>We may also license the Perl camel image for some commercial products and sites related to Perl. To inquire about the use of a camel image on any commercial product or site, please send email to permissions@oreilly.com with a description of the product or web site, indicating where and how you'd like to use the camel.</p><p>We've also created "Powered by Perl" buttons that any site using Perl may use on web pages. Feel free to download and use these buttons. Please make the buttons link to www.perl.com.</p><p>And the Camel FAQ:</p><p><a href="https://web.archive.org/web/20180123132933/http://archive.oreilly.com/pub/a/oreilly/perl/usage/faq.html" rel="nofollow noreferrer">https://web.archive.org/web/20180123132933/http://archive.or...</a></p><p>Q: So are you saying that O'Reilly has trademarked an entire animal?</p><p>A: No. When a company receives a trademark, it receives protection for a symbol in a particular category of products or services. For example, Owens Corning has trademarked the color pink. The whole color? No, only for insulation. O'Reilly has protected the camel image for books and online publications related to the Perl language, and related product and services. The only reason an association exists between camels and the Perl programming language is because we've used a camel image on our Perl-related products.</p><p>Q: Do you just own the particular Camel on the cover of Programming Perl, or all camels?</p><p>A: We own the particular camel image shown above, which has lead to an association between camels and the Perl language. If someone were to use a different camel on their Perl book, there could be confusion over which one "The Camel Book" referred to, and we might need to step in and stop use of that camel image. That's how trademarks work, helping to protect confusion in the marketplace.</p><p>Q: I want to design a T-shirt with the Perl camel on it. Do I need to get your permission?</p><p>A: Yes. But we're willing to make allowances for those of you who have creative ideas and want to do something fun for your friends. So, if the lifetime print run of the T-shirt design is less than 100, you may consider permission automatically granted. For larger print runs, please ask first. We promise to answer quickly!</p><p>Q: Why isn't your trademark just restricted to books?</p><p>A: We also do conferences, software, research, and online publishing in Perl, and we use the camel image for those things as well. We may want to camel-brand other Perl-related products in the future.</p><p>Q: I want to use $camel as a variable name in a Perl program. Do I need to acknowledge the trademark?</p><p>A: No.</p><p>Q: I want to use a cartoon camel as the logo for my software product. Is that okay?</p><p>A: It depends on what your product is, how it was developed, and how you intend to distribute it. Please send email to permissions@oreilly.com, with information about what you'd like to do, and we'll get back to you.</p><p>Q: I want to place a picture of a camel on my Perl web page. Am I allowed to do that? Do I have to use your camel?</p><p>A:Yes, as long as your page is non-commercial, and the context in which the camel is placed portrays Perl in a positive light. You will need to include the following language in small text somewhere on the page where the camel appears:</p><p>"The Perl camel image is a trademark of O'Reilly Media, Inc. Used with permission."</p><p>Please make the "O'Reilly Media, Inc." part of the statement a link to our home page (<a href="http://www.oreilly.com/" rel="nofollow noreferrer">http://www.oreilly.com</a>).</p><p>We'd encourage you to use the Perl camel we use, as it has wide recognition as "the Perl camel." But if you have another camel you'd like to use on a non-commercial site we generally would not object, so as long as the image is in no way derogatory.</p><p>Please note: If you use the "Powered by Perl" or the "Programming Republic of Perl" buttons, please make those active links to <a href="http://www.perl.com/" rel="nofollow noreferrer">http://www.perl.com</a>, not the O'Reilly home page.</p><p>Q: What is the Programming Republic of Perl logo?</p><p>A: The Programming Republic of Perl logo was developed some years ago for non-commercial use on web sites, and serves as a pointer to www.perl.com. Feel free to use it on any non-commercial pages. You can find it on the main Perl Camel Usage and Trademark Information page.</p><p>Q: Where can I find out more about camels?</p><p><a href="http://www.sandiegozoo.org/animalbytes/t-camel.html" rel="nofollow noreferrer">http://www.sandiegozoo.org/animalbytes/t-camel.html</a></p><p>If you have questions or comments about the Perl camel or any other O'Reilly trademarks, or if you want to use one of our trademarks in some way that we haven't explicitly described on this page, please send a detailed request to permissions@oreilly.com. For more information, see the Perl Camel FAQ.
              </p></span></p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Open sourcing the Nginx playground (167 pts)]]></title>
            <link>https://jvns.ca/blog/2023/07/08/open-sourcing-the-nginx-playground/</link>
            <guid>36648821</guid>
            <pubDate>Sat, 08 Jul 2023 21:11:17 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://jvns.ca/blog/2023/07/08/open-sourcing-the-nginx-playground/">https://jvns.ca/blog/2023/07/08/open-sourcing-the-nginx-playground/</a>, See on <a href="https://news.ycombinator.com/item?id=36648821">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
     

<p>Hello! In 2021 I released a small playground for testing nginx configurations
called <a href="https://nginx-playground.wizardzines.com/">nginx playground</a>. There’s a
<a href="https://jvns.ca/blog/2021/09/24/new-tool--an-nginx-playground/">blog post about it here</a>.</p>

<p>This is an extremely short post to say that at the time I didn’t make it open source,
but I am making it open source now. It’s not a lot of code but maybe it’ll be
interesting to someone.</p>

<p>Here’s <a href="https://github.com/jvns/nginx-playground/">the github repo</a>. The
frontend is in <code>static/</code> and the backend is in <code>api/</code>. The README is mostly an
extended apology for the developer experience and note that the project is
unmaintained. But I did test that the build instructions work!</p>

<h3 id="why-didn-t-i-open-source-this-before">why didn’t I open source this before?</h3>

<p>I’m not very good at open source. Some of the problems I have with open sourcing things are:</p>

<ul>
<li>I dislike (and am very bad at) maintaining open source projects – I usually
ignore basically all feature requests and most bug reports and then feel bad about it.
I handed off maintainership to both of the open source projects that I
started (<a href="https://github.com/rbspy/rbspy">rbspy</a> and <a href="https://github.com/rust-bpf/rust-bcc">rust-bcc</a>) to other people who are doing a MUCH better job than I ever did.</li>
<li>Sometimes the developer experience for the project is pretty bad</li>
<li>Sometimes there’s configuration in the project (like the <code>fly.toml</code> or the
analytics I have set up) which don’t really make sense for other people to
copy</li>
</ul>

<h3 id="new-approach-don-t-pretend-i-m-going-to-improve-it">new approach: don’t pretend I’m going to improve it</h3>

<p>In the past I’ve had some kind of belief that I’m going to improve the problems
with my code later. But I haven’t touched this project in more than a year and
I think it’s unlikely I’m going to go back to it unless it breaks in some dramatic way.</p>

<p>So instead of pretending I’m going to improve things, I decided to just:</p>

<ul>
<li>tell people in the README that the project is unmaintained</li>
<li>write down all the security caveats I know about</li>
<li>test the build instructions I wrote to make sure that they work (on a fresh machine, even!)</li>
<li>explain (but do not fix!!) some of the messy parts of the project</li>
</ul>

<h3 id="that-s-all">that’s all!</h3>

<p>Maybe I will open source more of my tiny projects in the future, we’ll see!
Thanks to <a href="https://www.changeset.nyc/">Sumana Harihareswara</a> for helping me
think through this.</p>

</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Is Htmx Gaining in Popularity? (138 pts)]]></title>
            <link>https://trends.builtwith.com/javascript/Htmx</link>
            <guid>36648817</guid>
            <pubDate>Sat, 08 Jul 2023 21:10:51 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://trends.builtwith.com/javascript/Htmx">https://trends.builtwith.com/javascript/Htmx</a>, See on <a href="https://news.ycombinator.com/item?id=36648817">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>

<div>
<div>
<p>
<label for="tk">Top 10k</label></p><svg width="25" height="20" style="margin-left: 0px;">
<g>
<line x1="5" y1="10" x2="35" y2="10"></line>
<circle style="opacity: 1" cx="15" cy="10" r="6"></circle>
</g></svg>
</div>
<div>
<p>
<label for="hk">Top 100k</label></p><svg width="25" height="20" style="margin-left: 0px;">
<g>
<line x1="5" y1="10" x2="35" y2="10"></line>
<circle style="opacity: 1" cx="15" cy="10" r="6"></circle>
</g></svg>
</div>
<div>
<p>
<label for="m">Top 1m</label></p><svg width="25" height="20" style="margin-left: 0px;">
<g>
<line x1="5" y1="10" x2="35" y2="10"></line>
<circle style="opacity: 1" cx="15" cy="10" r="6"></circle>
</g></svg>
</div>
<div>
<p>
<label for="ei">All Internet</label></p><svg width="25" height="20" style="margin-left: 0px;">
<g>
<line x1="5" y1="10" x2="35" y2="10"></line>
<circle style="opacity: 1" cx="15" cy="10" r="6"></circle>
</g></svg>
</div>
</div>

<div>
<div>
<p><img data-src="https://deo39crpw7zzn.cloudfront.net/thumb/0c-50-90-ed-cz-c1/n" alt="Htmx" width="48" height="48">
</p>
<div>
<p>Gives you access to AJAX, CSS Transitions, WebSockets and Server Sent Events directly in HTML, using attributes.</p>
<p><a href="https://htmx.org/" target="_blank" rel="nofollow noopener">https://htmx.org</a> </p>
<p><a href="https://trends.builtwith.com/javascript">JavaScript Libraries and Functions</a></p>
</div>
</div>
<div>
<h5>Htmx Customers</h5>
<p>
Get access to data on <a href="https://trends.builtwith.com/websitelist/Htmx/Historical">9,204 websites</a> that are Htmx Customers. We know of <a href="https://trends.builtwith.com/websitelist/Htmx">7,188 live websites</a> using Htmx and an additional 2,016 sites that used Htmx historically and <a href="https://trends.builtwith.com/websitelist/Htmx/Switzerland">178 websites in Switzerland</a>.</p>
<p>
<a href="https://trends.builtwith.com/websitelist/Htmx">
<svg>
<use xlink:href="#icon-arrow-alt-circle-down"></use></svg>
Download Lead List</a>
</p>
</div>
</div>
<div>
<div>
<p>
<h5><svg><use xlink:href="#icon-award"></use></svg>Htmx Awards</h5>
</p>
</div>
<div>
<div>
<ul>
</ul></div> <p>No awards yet.</p>
</div></div>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Build Personal ChatGPT Using Your Data (281 pts)]]></title>
            <link>https://github.com/raghavan/PdfGptIndexer</link>
            <guid>36648794</guid>
            <pubDate>Sat, 08 Jul 2023 21:07:55 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/raghavan/PdfGptIndexer">https://github.com/raghavan/PdfGptIndexer</a>, See on <a href="https://news.ycombinator.com/item?id=36648794">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-target="readme-toc.content">
            <article itemprop="text"><h2 tabindex="-1" dir="auto">PdfGptIndexer</h2>
<h2 tabindex="-1" dir="auto">Description</h2>
<p dir="auto">PdfGptIndexer is an efficient tool for indexing and searching PDF text data using OpenAI's GPT-2 model and FAISS (Facebook AI Similarity Search). This software is designed for rapid information retrieval and superior search accuracy.</p>
<h2 tabindex="-1" dir="auto">Libraries Used</h2>
<ol dir="auto">
<li><a href="https://github.com/deanmalmgren/textract">Textract</a> - A Python library for extracting text from any document.</li>
<li><a href="https://github.com/huggingface/transformers">Transformers</a> - A library by Hugging Face providing state-of-the-art general-purpose architectures for Natural Language Understanding (NLU) and Natural Language Generation (NLG).</li>
<li><a href="https://python.langchain.com/" rel="nofollow">Langchain</a> - A text processing and embeddings library.</li>
<li><a href="https://github.com/facebookresearch/faiss">FAISS (Facebook AI Similarity Search)</a> - A library for efficient similarity search and clustering of dense vectors.</li>
</ol>
<h2 tabindex="-1" dir="auto">Installing Dependencies</h2>
<p dir="auto">You can install all dependencies by running the following command:</p>
<div dir="auto" data-snippet-clipboard-copy-content="pip install langchain openai textract transformers langchain faiss-cpu"><pre>pip install langchain openai textract transformers langchain faiss-cpu</pre></div>
<h2 tabindex="-1" dir="auto">How It Works</h2>
<p dir="auto">The PdfGptIndexer operates in several stages:</p>
<ol dir="auto">
<li>It first processes a specified folder of PDF documents, extracting the text and splitting it into manageable chunks using a GPT-2 tokenizer from the Transformers library.</li>
<li>Each text chunk is then embedded using the OpenAI GPT-2 model through the LangChain library.</li>
<li>These embeddings are stored in a FAISS index, providing a compact and efficient storage method.</li>
<li>Finally, a query interface allows you to retrieve relevant information from the indexed data by asking questions. The application fetches and displays the most relevant text chunk.</li>
</ol>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://user-images.githubusercontent.com/131585/252057499-2e71dd82-bf4f-44db-b1ae-908cbb465deb.png"><img src="https://user-images.githubusercontent.com/131585/252057499-2e71dd82-bf4f-44db-b1ae-908cbb465deb.png" alt="Untitled-2023-06-16-1537"></a></p>
<h2 tabindex="-1" dir="auto">Advantages of Storing Embeddings Locally</h2>
<p dir="auto">Storing embeddings locally provides several advantages:</p>
<ol dir="auto">
<li>Speed: Once the embeddings are stored, retrieval of data is significantly faster as there's no need to compute embeddings in real-time.</li>
<li>Offline access: After the initial embedding creation, the data can be accessed offline.</li>
<li>Compute Savings: You only need to compute the embeddings once and reuse them, saving computational resources.</li>
<li>Scalability: This makes it feasible to work with large datasets that would be otherwise difficult to process in real-time.</li>
</ol>
<h2 tabindex="-1" dir="auto">Running the Program</h2>
<p dir="auto">To run the program, you should:</p>
<ol dir="auto">
<li>Make sure you have installed all dependencies.</li>
<li>Clone the repository to your local machine.</li>
<li>Navigate to the directory containing the Python script.</li>
<li>Replace "&lt;OPENAI_API_KEY&gt;" with your actual OpenAI API key in the script.</li>
<li>Finally, run the script with Python.</li>
</ol>
<div dir="auto" data-snippet-clipboard-copy-content="python3 pdf_gpt_indexer.py"><pre><span>python3</span> <span>pdf_gpt_indexer</span>.<span>py</span></pre></div>
<p dir="auto">Please ensure that the folders specified in the script for PDF documents and the output text files exist and are accessible. The query interface will start after the embeddings are computed and stored. You can exit the query interface by typing 'exit'.</p>
<h2 tabindex="-1" dir="auto">Exploring Custom Data with ChatGPT</h2>
<p dir="auto">Check out the post <a href="https://devden.raghavan.studio/p/chatgpt-using-your-own-data" rel="nofollow">here</a> for a comprehensive guide on how to utilize ChatGPT with your own custom data.</p>
</article>
          </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Learn Electronics by Practice (312 pts)]]></title>
            <link>https://beletronics.wordpress.com/</link>
            <guid>36647364</guid>
            <pubDate>Sat, 08 Jul 2023 18:35:06 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://beletronics.wordpress.com/">https://beletronics.wordpress.com/</a>, See on <a href="https://news.ycombinator.com/item?id=36647364">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><header>
<div>
<div>

<p>(residence of electronics enthusiasts)</p></div>


<nav aria-label="Primary" data-wp-interactive="" data-wp-context="{ &quot;core&quot;: { &quot;navigation&quot;: { &quot;isMenuOpen&quot;: { &quot;click&quot;: false, &quot;hover&quot;: false }, &quot;overlay&quot;: true, &quot;roleAttribute&quot;: &quot;&quot; } } }">
			<div id="modal-3" aria-label="Menu" data-wp-bind--aria-modal="selectors.core.navigation.isMenuOpen" data-wp-bind--role="selectors.core.navigation.roleAttribute" data-wp-effect="effects.core.navigation.initMenu" tabindex="-1" data-micromodal-close="" data-wp-class--has-modal-open="selectors.core.navigation.isMenuOpen" data-wp-class--is-menu-open="selectors.core.navigation.isMenuOpen" data-wp-on--keydown="actions.core.navigation.handleMenuKeydown" data-wp-on--focusout="actions.core.navigation.handleMenuFocusout">
							<ul><li><a href="https://beletronics.wordpress.com/mission/"><span>Mission</span></a></li><li><a href="https://beletronics.wordpress.com/about/"><span>About</span></a></li></ul>
						</div></nav></div>




</header>


<p><h2>Learn electronics by&nbsp;practice</h2></p>



<main>

<div>
<p>This website is dedicated to electronics enthusiasts and aspirants who believe that true knowledge comes through the persistence of constant practice.</p>



<p>The content is divided into parts presented hereafter. </p>



<figure><a href="https://beletronics.wordpress.com/learn-basic-electronics/"><img decoding="async" data-attachment-id="52" data-permalink="https://beletronics.wordpress.com/home/basic/" data-orig-file="https://beletronics.files.wordpress.com/2022/05/basic.png" data-orig-size="1497,729" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="basic" data-image-description="" data-image-caption="" data-medium-file="https://beletronics.files.wordpress.com/2022/05/basic.png?w=300" data-large-file="https://beletronics.files.wordpress.com/2022/05/basic.png?w=1024" src="https://beletronics.files.wordpress.com/2022/05/basic.png?w=1024" alt="Learn basic electronics" width="498" height="243" srcset="https://beletronics.files.wordpress.com/2022/05/basic.png?w=498 498w, https://beletronics.files.wordpress.com/2022/05/basic.png?w=996 996w, https://beletronics.files.wordpress.com/2022/05/basic.png?w=150 150w, https://beletronics.files.wordpress.com/2022/05/basic.png?w=300 300w, https://beletronics.files.wordpress.com/2022/05/basic.png?w=768 768w" sizes="(max-width: 498px) 100vw, 498px"></a><figcaption><a href="https://beletronics.wordpress.com/learn-basic-electronics/">Learn basic electronics – click to read</a></figcaption></figure>



<figure><a href="https://beletronics.wordpress.com/pong-game/"><img decoding="async" width="3660" height="1275" data-attachment-id="1345" data-permalink="https://beletronics.wordpress.com/home/pong-2/" data-orig-file="https://beletronics.files.wordpress.com/2022/05/pong-2.png" data-orig-size="3660,1275" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="pong-2" data-image-description="" data-image-caption="" data-medium-file="https://beletronics.files.wordpress.com/2022/05/pong-2.png?w=300" data-large-file="https://beletronics.files.wordpress.com/2022/05/pong-2.png?w=1024" src="https://beletronics.files.wordpress.com/2022/05/pong-2.png?w=1024" alt="" srcset="https://beletronics.files.wordpress.com/2022/05/pong-2.png?w=1024 1024w, https://beletronics.files.wordpress.com/2022/05/pong-2.png?w=2048 2048w, https://beletronics.files.wordpress.com/2022/05/pong-2.png?w=150 150w, https://beletronics.files.wordpress.com/2022/05/pong-2.png?w=300 300w, https://beletronics.files.wordpress.com/2022/05/pong-2.png?w=768 768w" sizes="(max-width: 1024px) 100vw, 1024px"></a><figcaption><a href="https://beletronics.wordpress.com/pong-game/">Create a legendary Pong game – click to read</a></figcaption></figure>



<figure><a href="https://beletronics.wordpress.com/cp4u-processor/"><img decoding="async" data-attachment-id="2594" data-permalink="https://beletronics.wordpress.com/home/cp4u/" data-orig-file="https://beletronics.files.wordpress.com/2022/06/cp4u.png" data-orig-size="2494,1636" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="cp4u" data-image-description="" data-image-caption="" data-medium-file="https://beletronics.files.wordpress.com/2022/06/cp4u.png?w=300" data-large-file="https://beletronics.files.wordpress.com/2022/06/cp4u.png?w=1024" src="https://beletronics.files.wordpress.com/2022/06/cp4u.png?w=1024" alt="" width="475" height="311" srcset="https://beletronics.files.wordpress.com/2022/06/cp4u.png?w=475 475w, https://beletronics.files.wordpress.com/2022/06/cp4u.png?w=948 948w, https://beletronics.files.wordpress.com/2022/06/cp4u.png?w=150 150w, https://beletronics.files.wordpress.com/2022/06/cp4u.png?w=300 300w, https://beletronics.files.wordpress.com/2022/06/cp4u.png?w=768 768w" sizes="(max-width: 475px) 100vw, 475px"></a><figcaption><a href="https://beletronics.wordpress.com/cp4u-processor/">Create a 4-bit processor – click to read</a></figcaption></figure>



<figure><a href="https://beletronics.wordpress.com/z80-computer/"><img decoding="async" data-attachment-id="1841" data-permalink="https://beletronics.wordpress.com/home/z80/" data-orig-file="https://beletronics.files.wordpress.com/2022/05/z80.jpg" data-orig-size="4007,2093" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;1.8&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;iPhone XS Max&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;1653660645&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;4.25&quot;,&quot;iso&quot;:&quot;320&quot;,&quot;shutter_speed&quot;:&quot;0.016666666666667&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;1&quot;,&quot;latitude&quot;:&quot;45.812958333333&quot;,&quot;longitude&quot;:&quot;15.942352777778&quot;}" data-image-title="z80" data-image-description="" data-image-caption="" data-medium-file="https://beletronics.files.wordpress.com/2022/05/z80.jpg?w=300" data-large-file="https://beletronics.files.wordpress.com/2022/05/z80.jpg?w=1024" src="https://beletronics.files.wordpress.com/2022/05/z80.jpg?w=1024" alt="" width="445" height="232" srcset="https://beletronics.files.wordpress.com/2022/05/z80.jpg?w=445 445w, https://beletronics.files.wordpress.com/2022/05/z80.jpg?w=888 888w, https://beletronics.files.wordpress.com/2022/05/z80.jpg?w=150 150w, https://beletronics.files.wordpress.com/2022/05/z80.jpg?w=300 300w, https://beletronics.files.wordpress.com/2022/05/z80.jpg?w=768 768w" sizes="(max-width: 445px) 100vw, 445px"></a><figcaption><a href="https://beletronics.wordpress.com/z80-computer/">Create a complete Z80 computer – click to read</a></figcaption></figure>
			
			
			</div></main>



<div>
<!-- You can start editing here. -->

	<h3 id="comments">
		6 responses to “Learn electronics by&nbsp;practice”	</h3>

	

	<ol>
			<li id="comment-2">
			<article id="div-comment-2">
				<!-- .comment-meta -->

				<div>
					<p>Very nice 🙂</p>
<p id="comment-like-2" data-liked="comment-not-liked"><a href="https://beletronics.wordpress.com/?like_comment=2&amp;_wpnonce=9ed7fde271" rel="nofollow" data-blog="206281479"><span>Like</span></a><span id="comment-like-count-2">Liked by <a href="#" data-like-count="1">1 person</a></span></p>
				</div><!-- .comment-content -->

				<div><p><a rel="nofollow" href="https://beletronics.wordpress.com/?replytocom=2#respond" data-commentid="2" data-postid="36" data-belowelement="div-comment-2" data-respondelement="respond" data-replyto="Reply to Heidobito" aria-label="Reply to Heidobito">Reply</a></p></div>			</article><!-- .comment-body -->
		<ul>
		<li id="comment-3">
			<article id="div-comment-3">
				<!-- .comment-meta -->

				<div>
					<p>Thank you!</p>
<p id="comment-like-3" data-liked="comment-not-liked"><a href="https://beletronics.wordpress.com/?like_comment=3&amp;_wpnonce=08fa246496" rel="nofollow" data-blog="206281479"><span>Like</span></a><span id="comment-like-count-3">Like</span></p>
				</div><!-- .comment-content -->

				<div><p><a rel="nofollow" href="https://beletronics.wordpress.com/?replytocom=3#respond" data-commentid="3" data-postid="36" data-belowelement="div-comment-3" data-respondelement="respond" data-replyto="Reply to daniel bele" aria-label="Reply to daniel bele">Reply</a></p></div>			</article><!-- .comment-body -->
		</li><!-- #comment-## -->
</ul><!-- .children -->
</li><!-- #comment-## -->
		<li id="comment-11">
			<article id="div-comment-11">
				<!-- .comment-meta -->

				<div>
					<p>Respect!<br>
Thank you for your effort and enthusiasm!</p>
<p id="comment-like-11" data-liked="comment-not-liked"><a href="https://beletronics.wordpress.com/?like_comment=11&amp;_wpnonce=f04ba9290e" rel="nofollow" data-blog="206281479"><span>Like</span></a><span id="comment-like-count-11">Liked by <a href="#" data-like-count="1">1 person</a></span></p>
				</div><!-- .comment-content -->

				<div><p><a rel="nofollow" href="https://beletronics.wordpress.com/?replytocom=11#respond" data-commentid="11" data-postid="36" data-belowelement="div-comment-11" data-respondelement="respond" data-replyto="Reply to Tomislav Valecic" aria-label="Reply to Tomislav Valecic">Reply</a></p></div>			</article><!-- .comment-body -->
		<ul>
		<li id="comment-12">
			<article id="div-comment-12">
				<!-- .comment-meta -->

				<div>
					<p>Thank you Tomislav!</p>
<p id="comment-like-12" data-liked="comment-not-liked"><a href="https://beletronics.wordpress.com/?like_comment=12&amp;_wpnonce=ddb5fad259" rel="nofollow" data-blog="206281479"><span>Like</span></a><span id="comment-like-count-12">Like</span></p>
				</div><!-- .comment-content -->

				<div><p><a rel="nofollow" href="https://beletronics.wordpress.com/?replytocom=12#respond" data-commentid="12" data-postid="36" data-belowelement="div-comment-12" data-respondelement="respond" data-replyto="Reply to daniel bele" aria-label="Reply to daniel bele">Reply</a></p></div>			</article><!-- .comment-body -->
		</li><!-- #comment-## -->
</ul><!-- .children -->
</li><!-- #comment-## -->
		<li id="comment-13">
			<article id="div-comment-13">
				<!-- .comment-meta -->

				<div>
					<p>Thank you, Bele! You’re the best.</p>
<p id="comment-like-13" data-liked="comment-not-liked"><a href="https://beletronics.wordpress.com/?like_comment=13&amp;_wpnonce=0f7e0dc91d" rel="nofollow" data-blog="206281479"><span>Like</span></a><span id="comment-like-count-13">Like</span></p>
				</div><!-- .comment-content -->

				<div><p><a rel="nofollow" href="https://beletronics.wordpress.com/?replytocom=13#respond" data-commentid="13" data-postid="36" data-belowelement="div-comment-13" data-respondelement="respond" data-replyto="Reply to Danko Kozar" aria-label="Reply to Danko Kozar">Reply</a></p></div>			</article><!-- .comment-body -->
		<ul>
		<li id="comment-14">
			<article id="div-comment-14">
				<!-- .comment-meta -->

				<div>
					<p>Thank you Danko. Means a lot, especially from you!</p>
<p id="comment-like-14" data-liked="comment-not-liked"><a href="https://beletronics.wordpress.com/?like_comment=14&amp;_wpnonce=f8e8a70822" rel="nofollow" data-blog="206281479"><span>Like</span></a><span id="comment-like-count-14">Like</span></p>
				</div><!-- .comment-content -->

				<div><p><a rel="nofollow" href="https://beletronics.wordpress.com/?replytocom=14#respond" data-commentid="14" data-postid="36" data-belowelement="div-comment-14" data-respondelement="respond" data-replyto="Reply to daniel bele" aria-label="Reply to daniel bele">Reply</a></p></div>			</article><!-- .comment-body -->
		</li><!-- #comment-## -->
</ul><!-- .children -->
</li><!-- #comment-## -->
	</ol>

	

	<div id="respond">
		<h3 id="reply-title">Leave a Reply <small></small></h3><form action="https://beletronics.wordpress.com/wp-comments-post.php" method="post" id="commentform" novalidate="">


<div>
	<p><label for="comment">Enter your comment here…</label></p>
</div>

		<div id="comment-form-identity">
	<div id="comment-form-nascar">
		<p>Fill in your details below or click an icon to log in:</p>
		<ul>
			
			<li>
				<a href="#comment-form-load-service:WordPress.com" id="postas-wordpress" title="Login via WordPress.com">
					<svg xmlns="http://www.w3.org/2000/svg" role="presentation" viewBox="0 0 24 24"><rect x="0" fill="none" width="24" height="24"></rect><g><path fill="#0087be" d="M12.158 12.786l-2.698 7.84c.806.236 1.657.365 2.54.365 1.047 0 2.05-.18 2.986-.51-.024-.037-.046-.078-.065-.123l-2.762-7.57zM3.008 12c0 3.56 2.07 6.634 5.068 8.092L3.788 8.342c-.5 1.117-.78 2.354-.78 3.658zm15.06-.454c0-1.112-.398-1.88-.74-2.48-.456-.74-.883-1.368-.883-2.11 0-.825.627-1.595 1.51-1.595.04 0 .078.006.116.008-1.598-1.464-3.73-2.36-6.07-2.36-3.14 0-5.904 1.613-7.512 4.053.21.008.41.012.58.012.94 0 2.395-.114 2.395-.114.484-.028.54.684.057.74 0 0-.487.058-1.03.086l3.275 9.74 1.968-5.902-1.4-3.838c-.485-.028-.944-.085-.944-.085-.486-.03-.43-.77.056-.742 0 0 1.484.114 2.368.114.94 0 2.397-.114 2.397-.114.486-.028.543.684.058.74 0 0-.488.058-1.03.086l3.25 9.665.897-2.997c.456-1.17.684-2.137.684-2.907zm1.82-3.86c.04.286.06.593.06.924 0 .912-.17 1.938-.683 3.22l-2.746 7.94c2.672-1.558 4.47-4.454 4.47-7.77 0-1.564-.4-3.033-1.1-4.314zM12 22C6.486 22 2 17.514 2 12S6.486 2 12 2s10 4.486 10 10-4.486 10-10 10z"></path></g></svg>				</a>
			</li>
			<li>
				<a href="#comment-form-load-service:Facebook" id="postas-facebook" title="Login via Facebook">
					<svg xmlns="http://www.w3.org/2000/svg" role="presentation" viewBox="0 0 24 24"><rect x="0" fill="none" width="24" height="24"></rect><g><path fill="#3B5998" d="M20.007 3H3.993C3.445 3 3 3.445 3 3.993v16.013c0 .55.445.994.993.994h8.62v-6.97H10.27V11.31h2.346V9.31c0-2.325 1.42-3.59 3.494-3.59.993 0 1.847.073 2.096.106v2.43h-1.438c-1.128 0-1.346.537-1.346 1.324v1.734h2.69l-.35 2.717h-2.34V21h4.587c.548 0 .993-.445.993-.993V3.993c0-.548-.445-.993-.993-.993z"></path></g></svg>				</a>
			</li>
		</ul>
	</div>

	<div id="comment-form-guest">
			<p><a href="https://gravatar.com/site/signup/" target="_blank">				<img src="https://1.gravatar.com/avatar/ad516503a11cd5ca435acc9bb6523536?s=25&amp;d=identicon&amp;forcedefault=y&amp;r=G" srcset="https://1.gravatar.com/avatar/ad516503a11cd5ca435acc9bb6523536?s=25&amp;d=identicon&amp;forcedefault=y&amp;r=G 1x, https://1.gravatar.com/avatar/ad516503a11cd5ca435acc9bb6523536?s=38&amp;d=identicon&amp;forcedefault=y&amp;r=G 1.5x, https://1.gravatar.com/avatar/ad516503a11cd5ca435acc9bb6523536?s=50&amp;d=identicon&amp;forcedefault=y&amp;r=G 2x, https://1.gravatar.com/avatar/ad516503a11cd5ca435acc9bb6523536?s=75&amp;d=identicon&amp;forcedefault=y&amp;r=G 3x, https://1.gravatar.com/avatar/ad516503a11cd5ca435acc9bb6523536?s=100&amp;d=identicon&amp;forcedefault=y&amp;r=G 4x" alt="Gravatar" width="25">
</a>			</p>

				<div>
				<div>
					<p><label for="email">Email <span>(required)</span> <span>(Address never made public)</span></label></p>
				</div>
				<div>
					<p><label for="author">Name <span>(required)</span></label></p>
				</div>
				<div>
					<p><label for="url">Website</label></p>
				</div>
			</div>
			
		</div>

	<div id="comment-form-wordpress">
			<p><img src="https://1.gravatar.com/avatar/ad516503a11cd5ca435acc9bb6523536?s=25&amp;d=identicon&amp;forcedefault=y&amp;r=G" srcset="https://1.gravatar.com/avatar/ad516503a11cd5ca435acc9bb6523536?s=25&amp;d=identicon&amp;forcedefault=y&amp;r=G 1x, https://1.gravatar.com/avatar/ad516503a11cd5ca435acc9bb6523536?s=38&amp;d=identicon&amp;forcedefault=y&amp;r=G 1.5x, https://1.gravatar.com/avatar/ad516503a11cd5ca435acc9bb6523536?s=50&amp;d=identicon&amp;forcedefault=y&amp;r=G 2x, https://1.gravatar.com/avatar/ad516503a11cd5ca435acc9bb6523536?s=75&amp;d=identicon&amp;forcedefault=y&amp;r=G 3x, https://1.gravatar.com/avatar/ad516503a11cd5ca435acc9bb6523536?s=100&amp;d=identicon&amp;forcedefault=y&amp;r=G 4x" alt="WordPress.com Logo" width="25">
			</p>

				<div>
				<p>
			<strong></strong>
			You are commenting using your WordPress.com account.			<span>
				(&nbsp;Log&nbsp;Out&nbsp;/&nbsp;
				<a href="#" onclick="javascript:HighlanderComments.switchAccount();return false;">Change</a>&nbsp;)
			</span>
			<span><svg xmlns="http://www.w3.org/2000/svg" role="presentation" viewBox="0 0 24 24"><rect x="0" fill="none" width="24" height="24"></rect><g><path fill="#0087be" d="M12.158 12.786l-2.698 7.84c.806.236 1.657.365 2.54.365 1.047 0 2.05-.18 2.986-.51-.024-.037-.046-.078-.065-.123l-2.762-7.57zM3.008 12c0 3.56 2.07 6.634 5.068 8.092L3.788 8.342c-.5 1.117-.78 2.354-.78 3.658zm15.06-.454c0-1.112-.398-1.88-.74-2.48-.456-.74-.883-1.368-.883-2.11 0-.825.627-1.595 1.51-1.595.04 0 .078.006.116.008-1.598-1.464-3.73-2.36-6.07-2.36-3.14 0-5.904 1.613-7.512 4.053.21.008.41.012.58.012.94 0 2.395-.114 2.395-.114.484-.028.54.684.057.74 0 0-.487.058-1.03.086l3.275 9.74 1.968-5.902-1.4-3.838c-.485-.028-.944-.085-.944-.085-.486-.03-.43-.77.056-.742 0 0 1.484.114 2.368.114.94 0 2.397-.114 2.397-.114.486-.028.543.684.058.74 0 0-.488.058-1.03.086l3.25 9.665.897-2.997c.456-1.17.684-2.137.684-2.907zm1.82-3.86c.04.286.06.593.06.924 0 .912-.17 1.938-.683 3.22l-2.746 7.94c2.672-1.558 4.47-4.454 4.47-7.77 0-1.564-.4-3.033-1.1-4.314zM12 22C6.486 22 2 17.514 2 12S6.486 2 12 2s10 4.486 10 10-4.486 10-10 10z"></path></g></svg></span>
		</p>
					</div>
	
		</div>

	<div id="comment-form-facebook">
			<p><img src="" alt="Facebook photo" width="25">
			</p>

				<div>
				<p>
			<strong></strong>
			You are commenting using your Facebook account.			<span>
				(&nbsp;Log&nbsp;Out&nbsp;/&nbsp;
				<a href="#" onclick="javascript:HighlanderComments.switchAccount();return false;">Change</a>&nbsp;)
			</span>
			<span><svg xmlns="http://www.w3.org/2000/svg" role="presentation" viewBox="0 0 24 24"><rect x="0" fill="none" width="24" height="24"></rect><g><path fill="#3B5998" d="M20.007 3H3.993C3.445 3 3 3.445 3 3.993v16.013c0 .55.445.994.993.994h8.62v-6.97H10.27V11.31h2.346V9.31c0-2.325 1.42-3.59 3.494-3.59.993 0 1.847.073 2.096.106v2.43h-1.438c-1.128 0-1.346.537-1.346 1.324v1.734h2.69l-.35 2.717h-2.34V21h4.587c.548 0 .993-.445.993-.993V3.993c0-.548-.445-.993-.993-.993z"></path></g></svg></span>
		</p>
					</div>
	
		</div>


	<div id="comment-form-load-service">
		<div><p>Cancel</p></div>
		<p>Connecting to %s</p>
	</div>

</div>



<div id="comment-form-subscribe">
	<p> <label id="subscribe-label" for="subscribe">Notify me of new comments via email.</label></p><p> <label id="subscribe-blog-label" for="subscribe_blog">Notify me of new posts via email.</label></p></div>

	






</form>	</div><!-- #respond -->
	</div>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Container Training (153 pts)]]></title>
            <link>https://container.training/</link>
            <guid>36647211</guid>
            <pubDate>Sat, 08 Jul 2023 18:20:10 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://container.training/">https://container.training/</a>, See on <a href="https://news.ycombinator.com/item?id=36647211">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
    <table>
      <tbody><tr><td colspan="3">Container Training</td></tr>
      <tr><td colspan="3">Note: while some workshops are delivered in other languages, slides are always in English.</td></tr>

      <tr><td colspan="3">Free Kubernetes intro course</td></tr>

      <tr>
      	<td>Getting Started With Kubernetes and Container Orchestration</td>
      	<td><a href="https://qconuk2019.container.training/"></a></td>
      	<td><a href="https://www.youtube.com/playlist?list=PLBAFXs0YjviJwCoxSUkUPhsSxDJzpZbJd"></a></td>
      </tr>
      <tr>
        <td>This is a live recording of a 1-day workshop that took place at QCON London in March 2019.</td>
      </tr>
      <tr>
        <td>If you're interested, we can deliver that workshop (or longer courses) to your team or organization.</td>
      </tr>
      <tr>
        <td>Contact <a href="mailto:jerome.petazzoni@gmail.com">Jérôme Petazzoni</a> to make that happen!</td>
      </tr>

      

      
        <tr><td colspan="3">Past workshops</td></tr>

        
          <tr>
            <td>Opérer Kubernetes (en français)</td>
            <td>
              
            </td>
            <td></td>
          </tr>
          <tr>
            <td>Delivered November 18th-19th, 2021 at ENIX SAS in streaming.</td>
          </tr>

        
          <tr>
            <td>Kubernetes avancé (en français)</td>
            <td>
              
            </td>
            <td></td>
          </tr>
          <tr>
            <td>Delivered November 8th-16th, 2021 at ENIX SAS in streaming.</td>
          </tr>

        
          <tr>
            <td>Packaging et CI/CD pour Kubernetes (en français)</td>
            <td>
              
            </td>
            <td></td>
          </tr>
          <tr>
            <td>Delivered October 11th-12th, 2021 at ENIX SAS in streaming.</td>
          </tr>

        
          <tr>
            <td>Fondamentaux Kubernetes (en français)</td>
            <td>
              
            </td>
            <td></td>
          </tr>
          <tr>
            <td>Delivered October 4th-7th, 2021 at ENIX SAS in streaming.</td>
          </tr>

        
          <tr>
            <td>Docker intensif (en français)</td>
            <td>
              
            </td>
            <td></td>
          </tr>
          <tr>
            <td>Delivered September 27th-29th, 2021 at ENIX SAS in streaming.</td>
          </tr>

        

        
          <tr>
            <td>... and at least <a href="https://container.training/past.html">111 more</a>.</td>
          </tr>
        
      

      
        <tr><td colspan="3">Recorded workshops</td></tr>

        
          <tr>
            <td>Getting started with Kubernetes and container orchestration</td>
            <td><a href="https://pycon2019.container.training/"></a></td>
            <td><a href="https://www.youtube.com/watch?v=J08MrW2NC1Y"></a></td>
          </tr>
          <tr>
            <td>Delivered May 1st, 2019 at PyCon in Cleveland, OH.</td>
          </tr>
        
          <tr>
            <td>Getting Started With Kubernetes and Container Orchestration</td>
            <td><a href="https://qconuk2019.container.training/"></a></td>
            <td><a href="https://www.youtube.com/playlist?list=PLBAFXs0YjviJwCoxSUkUPhsSxDJzpZbJd"></a></td>
          </tr>
          <tr>
            <td>Delivered March 8th, 2019 at QCON in London.</td>
          </tr>
        
          <tr>
            <td>Introduction to Docker and Containers</td>
            <td><a href="http://qconsf2017intro.container.training/"></a></td>
            <td><a href="https://www.youtube.com/playlist?list=PLBAFXs0YjviLgqTum8MkspG_8VzGl6C07"></a></td>
          </tr>
          <tr>
            <td>Delivered November 16th, 2017 at QCON SF in San Francisco, CA.</td>
          </tr>
        
          <tr>
            <td>Deploying and scaling microservices with Docker and Kubernetes</td>
            <td><a href="http://osseu17.container.training/"></a></td>
            <td><a href="https://www.youtube.com/playlist?list=PLBAFXs0YjviLrsyydCzxWrIP_1-wkcSHS"></a></td>
          </tr>
          <tr>
            <td>Delivered October 26th, 2017 at Open Source Summit Europe in Prague.</td>
          </tr>
        
          <tr>
            <td>Deploying &amp; Scaling microservices with Docker Swarm</td>
            <td><a href=""></a></td>
            <td><a href="https://www.youtube.com/watch?v=DABbqyJeG_E"></a></td>
          </tr>
          <tr>
            <td>Delivered July 25th, 2017 at devopsdays in Minneapolis, MN.</td>
          </tr>
        
          <tr>
            <td>Deploy and scale containers with Docker native, open source orchestration</td>
            <td><a href=""></a></td>
            <td><a href="https://www.youtube.com/watch?v=EuzoEaE6Cqs"></a></td>
          </tr>
          <tr>
            <td>Delivered May 18th, 2017 at PyCon in Portland, OR.</td>
          </tr>
        
          <tr>
            <td>Deploying and Scaling Applications with Docker Swarm</td>
            <td><a href="http://lisa16t1.container.training/"></a></td>
            <td><a href="https://www.youtube.com/playlist?list=PLBAFXs0YjviIDDhr8vIwCN1wkyNGXjbbc"></a></td>
          </tr>
          <tr>
            <td>Delivered December 6th, 2016 at LISA in Boston, MA.</td>
          </tr>
        
          <tr>
            <td>Introduction to Docker and containers</td>
            <td><a href="https://us.pycon.org/2016/site_media/media/tutorial_handouts/DockerSlides.pdf"></a></td>
            <td><a href="https://www.youtube.com/watch?v=ZVaRK10HBjo"></a></td>
          </tr>
          <tr>
            <td>Delivered May 29th, 2016 at PyCon in Portland, OR.</td>
          </tr>
        
      

      
        <tr><td colspan="3">Self-paced tutorials</td></tr>
        
          <tr>
            <td>Introduction to Docker and Containers</td>
            <td><a href="https://container.training/intro-selfpaced.yml.html"></a></td>
          </tr>
        
          <tr>
            <td>Container Orchestration with Docker and Swarm</td>
            <td><a href="https://container.training/swarm-selfpaced.yml.html"></a></td>
          </tr>
        
          <tr>
            <td>Deploying and Scaling Microservices with Docker and Kubernetes</td>
            <td><a href="https://container.training/kube-selfpaced.yml.html"></a></td>
          </tr>
        
      

      

      <tr><td></td></tr>

      <tr>
        <td>
          Maintained by Jérôme Petazzoni (<a href="https://twitter.com/jpetazzo">@jpetazzo</a>) and <a href="https://github.com/jpetazzo/container.training/graphs/contributors">contributors</a>.
        </td>
      </tr>
    </tbody></table>
  </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Why Use Pascal? (257 pts)]]></title>
            <link>https://castle-engine.io/why_pascal</link>
            <guid>36646890</guid>
            <pubDate>Sat, 08 Jul 2023 17:54:54 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://castle-engine.io/why_pascal">https://castle-engine.io/why_pascal</a>, See on <a href="https://news.ycombinator.com/item?id=36646890">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
<div>
<h3 id="_modern_clean_language_to_develop_maintainable_applications">2.1. Modern clean language to develop maintainable applications</h3>
<div>
<ul>
<li>
<p>Object Pascal is a modern programming language. It supports classes, units, properties, generics, interfaces, reflection, closures…​ Everything you expect from a modern OOP language.</p>
</li>
<li>
<p>The syntax puts emphasis on readable code.</p>
</li>
<li>
<p>The language is type-safe. E.g. special types for booleans, strings, chars, sets, enums, ranges. Type conversions are either really safe, or have to be done explicitly.</p>
</li>
<li>
<p>There are additional run-time checks, e.g. array range checking, integer overflow checking, assertions, memory leak checking. Notes:</p>
<div>
<ul>
<li>
<p>You can turn off these checks in <em>release</em> version, but use them in <em>debug</em>. When compiling using CGE build tool / editor, we have debug / release modes that automatically do this for you.</p>
</li>
<li>
<p><a href="https://github.com/michaliskambi/modern-pascal-introduction/wiki/What-are-range-and-overflow-checks-(and-errors)-in-Pascal">What are range and overflow checks (and errors) in Pascal</a></p>
</li>
<li>
<p><a href="https://castle-engine.io/detecting_memory_leaks_using_heaptrc">Detecting Memory Leaks</a></p>
</li>
</ul>
</div>
</li>
</ul>
</div>
</div>
<div>
<h3 id="_fast">2.2. Fast</h3>
<div>
<ul>
<li>
<p>It is compiled to a native code and so is fast <em>"out of the box"</em>. There’s seldom any need to do low-level optimizations.</p>
</li>
<li>
<p>But if you need to, language can be as low-level as you want. E.g. you can use pointers, do pointer math, write OS and CPU-specific code, even add pieces in assembly. You can work on the same level as C or C++ does.</p>
<div>
<table>
<tbody><tr>
<td>
<p>Note</p>
</td>
<td>
But you will probably not need to get too "low level" in usual applications. E.g. <a href="https://castle-engine.io/">Castle Game Engine</a> has <strong>zero assembler code</strong> to maximize portability and code readability and we’re still fast.
</td>
</tr>
</tbody></table>
</div>
</li>
<li>
<p>Compilation is also fast.</p>
<div>
<p><img src="https://castle-engine.io/images/not_resized/pascal-fast-compilation.webp" alt="pascal fast compilation">
</p>
</div>
<p>2.5 seconds to get desktop build, 10.1 seconds to get Android build <strong>of a new project, opened for the 1st time</strong>. Try to match that with your engine :)</p>
</li>
</ul>
</div>
</div>
<div>
<h3 id="_cross_platform">2.3. Cross-platform</h3>
<div>
<ul>
<li>
<p>Desktop (Windows, Linux, macOS, Raspberry Pi, FreeBSD, probably every Unix…​),</p>
</li>
<li>
<p>mobile (Android, iOS),</p>
</li>
<li>
<p>consoles (Nintendo Switch, special in CGE),</p>
</li>
<li>
<p>web (both WebAssembly and JS (using pas2js)).</p>
</li>
</ul>
</div>

</div>
<div>
<h3 id="_welcoming">2.4. Welcoming</h3>
<div>
<ul>
<li>
<p>In <a href="https://castle-engine.io/">Castle Game Engine</a> case, engine code and game code are in the same language. Every user is contributor!</p>
</li>
<li>
<p>And the engine is open-source.</p>
</li>
</ul>
</div>
<p>Don’t hesitate to fork CGE to adjust it to your needs.</p>
</div>
<div>
<h3 id="_general_purpose">2.5. General purpose</h3>
<p>There are existing libraries (units) in Pascal for everything:</p>
<div>
<ul>
<li>
<p>database</p>
</li>
<li>
<p>XML, JSON</p>
</li>
<li>
<p>A.I.</p>
</li>
<li>
<p>blockchain</p>
</li>
<li>
<p>networking</p>
</li>
</ul>
</div>
<p>Moreover you can easily integrate with (link to) any existing library with C API. Any renderer, sound library, physics - we can use everything.</p>

</div>
<div>
<h3 id="_ecosystem_of_tools">2.6. Ecosystem of tools</h3>
<div>
<ul>
<li>
<p><a href="https://www.freepascal.org/">FPC</a> - Free Pascal Compiler, open-source.</p>
</li>
<li>
<p><a href="https://www.lazarus-ide.org/">Lazarus</a> - IDE for Pascal, on top of FPC, also open-source.</p>
</li>
<li>
<p><a href="https://www.embarcadero.com/products/Delphi">Delphi</a> - commercial compiler and IDE for Pascal.</p>
</li>
<li>
<p><a href="https://castle-engine.io/vscode">VS Code</a> support - CGE, as well as many others in the Pascal ecosystem, explicitly support integration with VS Code.</p>
</li>
</ul>
</div>
</div>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[How and Why I Stopped Buying New Laptops (2020) (306 pts)]]></title>
            <link>https://solar.lowtechmagazine.com/2020/12/how-and-why-i-stopped-buying-new-laptops/</link>
            <guid>36646791</guid>
            <pubDate>Sat, 08 Jul 2023 17:47:28 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://solar.lowtechmagazine.com/2020/12/how-and-why-i-stopped-buying-new-laptops/">https://solar.lowtechmagazine.com/2020/12/how-and-why-i-stopped-buying-new-laptops/</a>, See on <a href="https://news.ycombinator.com/item?id=36646791">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
<article>
<div id="content"><div>
<figure data-imgstate="dither">
<img alt="Image: Low-tech Magazine is now written and published on a 2006 ThinkPad X60s." data-dither="/2020/12/how-and-why-i-stopped-buying-new-laptops/images/dithers/X60-on-its-side-white_dithered.png" data-original="https://solar.lowtechmagazine.com/2020/12/how-and-why-i-stopped-buying-new-laptops/images/X60-on-its-side-white_hu753fd43d283e60bf1aefb8ea5c1440fe_3642029_800x800_fit_q90_h2_box.webp" loading="lazy" src="https://solar.lowtechmagazine.com/2020/12/how-and-why-i-stopped-buying-new-laptops/images/dithers/X60-on-its-side-white_dithered.png"> </figure>
<div>
<figcaption>
<p>Image: Low-tech Magazine is now written and published on a 2006 ThinkPad X60s. <svg viewBox="0 0 100 100" xmlns="http://www.w3.org/2000/svg">
<rect height="24.28" width="24.28" x="13.51" y="13.58"></rect><rect height="24.28" width="24.28" x="37.93" y="37.86"></rect><rect height="24.28" width="24.28" x="62.21" y="13.58"></rect><rect height="24.28" width="24.28" x="13.51" y="62.14"></rect><rect height="24.28" width="24.28" x="62.21" y="62.14"></rect>
</svg></p>
<p><span>
  View original image
</span>
<span>
  View dithered image
</span>
</p>
</figcaption>
</div>
</div>
<p>Being an independent journalist – or an office worker if you wish – I always reasoned that I needed a decent computer and that I need to pay for quality. Between 2000 and 2017, I consumed three laptops that I bought new and which cost me around 5,000 euros in total – roughly 300 euros per year over the entire period. The average useful life of my three laptops was 5.7 years.</p>
<p>In 2017, somewhere between getting <a href="https://solar.lowtechmagazine.com/2018/09/how-to-build-a-low-tech-website/">my office</a>, I decided not to buy any more new laptops. Instead, I switched to a 2006 second-hand machine that I purchased online for 50 euros and which does everything that I want and need. Including a new battery and a simple hardware upgrade, I invested less than 150 euros.</p>
<p>If my 2006 laptop lasts as long as my other machines – if it runs for another 1.7 years – it will have cost me only 26 euros per year. That’s more than 10 times less than the cost of my previous laptops. In this article, I explain my motivations for not buying new laptops, and how you could do the same.</p>
<h2 id="energy-and-material-use-of-a-laptop">Energy and material use of a laptop</h2>
<p>Not buying new laptops saves a lot of money, but also a lot of resources and environmental destruction. According to the most recent life cycle analysis, it takes 3,010 to 4,340 megajoules of primary energy to make a laptop – this includes mining the materials, manufacturing the machine, and bringing it to market. <sup id="fnref:1"><a href="#fn:1" role="doc-noteref">1</a></sup></p>
<p>Each year, we purchase between 160 and 200 million laptops. Using the data above, this means that the production of laptops requires a yearly energy consumption of 480 to 868 petajoules, which corresponds to between one quarter and almost half of all solar PV energy produced worldwide in 2018 (2,023 petajoules). <sup id="fnref:2"><a href="#fn:2" role="doc-noteref">2</a></sup> The making of a laptop also involves a high material consumption, which includes a wide variety of minerals that may be considered scarce due to different types of constraints: economic, social, geochemical, and geopolitical. <sup id="fnref:3"><a href="#fn:3" role="doc-noteref">3</a></sup><sup id="fnref:4"><a href="#fn:4" role="doc-noteref">4</a></sup></p>
<p>The <a href="https://solar.lowtechmagazine.com/2009/06/the-monster-footprint-of-digital-technology/">production of microchips is a very energy- and material-intensive process</a>, but that is not the only problem. The high resource use of laptops is also because they have a very short lifespan. Most of the 160-200 million laptops sold each year are replacement purchases. The average laptop is replaced every 3 years (in business) to five years (elsewhere). <sup id="fnref1:3"><a href="#fn:3" role="doc-noteref">3</a></sup> My 5.7 years per laptop experience is not exceptional.</p>
<h2 id="laptops-dont-change">Laptops don’t change</h2>
<p>The study cited dates from 2011, and it refers to a machine made in 2001: a Dell Inspiron 2500.  You are forgiven for thinking that this “most recent life cycle analysis of a laptop” is outdated, but it’s not. A 2015 research paper discovered that the embodied energy of laptops is static over time. <sup id="fnref:5"><a href="#fn:5" role="doc-noteref">5</a></sup></p>
<p>The scientists disassembled 11 laptops of similar size, made between 1999 and 2008, and weighed the different components. Also, they measured the silicon die area for all motherboards and 30 DRAM cards produced over roughly the same period (until 2011). They found that the mass and material composition of all key components – battery, motherboard, hard drive, memory – did not change significantly, even though manufacturing processes became more efficient in energy and material use.</p>
<p>The reason is simple: improvements in functionality balance the efficiency gains obtained in the manufacturing process. Battery mass, memory, and hard disk drive mass decreased per unit of functionality but showed roughly constant totals per year. The same dynamic explains why newer laptops don’t show lower operational electricity consumption compared to older laptops. New laptops may be more energy-efficient per computational power, but these gains are offset by more computational power. <a href="https://solar.lowtechmagazine.com/2018/01/bedazzled-by-energy-efficiency/">Jevon’s paradox</a> is nowhere as evident as it is in computing.</p>
<h2 id="the-challenge">The challenge</h2>
<p>All this means that there’s no environmental or financial benefit whatsoever to replacing an old laptop with a new one. On the contrary, the only thing a consumer can do to improve their laptop’s ecological and economic sustainability is to use it for as long as possible. This is facilitated by the fact that laptops are now a mature technology and have more than sufficient computational power. One problem, though. Consumers who try to keep working on their old laptops are likely to end up frustrated. I shortly explain my frustrations below, and I’m pretty confident that they are not exceptional.</p>
<div>
<figure data-imgstate="dither">
<img alt="Image: The three new laptops I used from 2000 to 2017." data-dither="/2020/12/how-and-why-i-stopped-buying-new-laptops/images/dithers/3-laptops-white_dithered.png" data-original="https://solar.lowtechmagazine.com/2020/12/how-and-why-i-stopped-buying-new-laptops/images/3-laptops-white_hud91339aa34e3ccb38ac1a26db4506f2c_3604453_800x800_fit_q90_h2_box.webp" loading="lazy" src="https://solar.lowtechmagazine.com/2020/12/how-and-why-i-stopped-buying-new-laptops/images/dithers/3-laptops-white_dithered.png"> </figure>
<div>
<figcaption>
<p>Image: The three new laptops I used from 2000 to 2017. <svg viewBox="0 0 100 100" xmlns="http://www.w3.org/2000/svg">
<rect height="24.28" width="24.28" x="13.51" y="13.58"></rect><rect height="24.28" width="24.28" x="37.93" y="37.86"></rect><rect height="24.28" width="24.28" x="62.21" y="13.58"></rect><rect height="24.28" width="24.28" x="13.51" y="62.14"></rect><rect height="24.28" width="24.28" x="62.21" y="62.14"></rect>
</svg></p>
<p><span>
  View original image
</span>
<span>
  View dithered image
</span>
</p>
</figcaption>
</div>
</div>
<h2 id="my-first-laptop-apple-ibook-2000-2005">My first laptop: Apple iBook (2000-2005)</h2>
<p>In 2000, when I was working as a freelance science and tech journalist in Belgium, I bought my first laptop, an Apple iBook. Little more than two or three years later, the charger started malfunctioning. When informed of the price for a new charger, I was so disgusted with Apple’s sales practices – chargers are very cheap to produce, but Apple sold them for a lot of money – that I refused to buy it. Instead, I managed to keep the charger working for a few more years, first by putting it under the weight of books and furniture, and when that didn’t work anymore, by putting it in a firmly tightened clamp.</p>
<h2 id="my-second-laptop-ibm-thinkpad-r52-2005-2013">My second laptop: IBM ThinkPad R52 (2005-2013)</h2>
<p>When the charger eventually died entirely in 2005, I decided to look for a new laptop. I had only one demand: it should have a charger that lasts or is at least cheap to replace. I found more than I was looking for. I bought an <a href="http://www.thinkwiki.org/wiki/Category:R52" target="_blank">IBM Thinkpad R52</a>, and it was love at first use. My IBM laptop was the Apple iBook counterpart, not just in terms of design (a rectangular box available in all colours as long as it’s black). More importantly, the entire machine was built to last, built to be reliable, and built to be repairable.</p>
<p><a href="https://solar.lowtechmagazine.com/2019/06/how-to-make-wind-power-sustainable-again/">Circular and modular products are all the hype these days</a>, its lifetime could be extended endlessly by gradually repairing and replacing every part that it consists of. The question is not how we can evolve towards a circular economy, but instead why we continue to evolve away from it.</p>
<blockquote>
<p>The question is not how we can evolve towards a circular economy, but instead why we continue to evolve away from it.</p>
</blockquote>
<p>My Thinkpad was more expensive to buy than my iBook, but at least I didn’t spend all that money on a cute design but a decent computer. The charger gave no problems, and when I lost it during a trip and had to buy a new one, I could do so for a fair price. Little did I know that my happy purchase was going to be a once-in-a-lifetime experience.</p>
<div>
<figure data-imgstate="dither">
<img alt="Image: The IBM ThinkPad R52 from 2005." data-dither="/2020/12/how-and-why-i-stopped-buying-new-laptops/images/dithers/Thinkpad-r52-white_dithered.png" data-original="https://solar.lowtechmagazine.com/2020/12/how-and-why-i-stopped-buying-new-laptops/images/Thinkpad-r52-white_hu02c8b35b61eabe045fd7fdd9f38bf53d_4952865_800x800_fit_q90_h2_box.webp" loading="lazy" src="https://solar.lowtechmagazine.com/2020/12/how-and-why-i-stopped-buying-new-laptops/images/dithers/Thinkpad-r52-white_dithered.png"> </figure>
<div>
<figcaption>
<p>Image: The IBM ThinkPad R52 from 2005. <svg viewBox="0 0 100 100" xmlns="http://www.w3.org/2000/svg">
<rect height="24.28" width="24.28" x="13.51" y="13.58"></rect><rect height="24.28" width="24.28" x="37.93" y="37.86"></rect><rect height="24.28" width="24.28" x="62.21" y="13.58"></rect><rect height="24.28" width="24.28" x="13.51" y="62.14"></rect><rect height="24.28" width="24.28" x="62.21" y="62.14"></rect>
</svg></p>
<p><span>
  View original image
</span>
<span>
  View dithered image
</span>
</p>
</figcaption>
</div>
</div>
<h2 id="my-third-laptop-lenovo-thinkpad-t430-2013-2017">My third laptop: Lenovo Thinkpad T430 (2013-2017)</h2>
<p>Fast forward to 2013. I am now living in Spain and I’m running Low-tech Magazine. I’m still working on my IBM Thinkpad R52, but there are some problems on the horizon. First of all, Microsoft will soon force me to upgrade my operating system, because support for Windows XP is to end in 2014. I don’t feel like spending a couple of hundred euros on a new operating system that would be too demanding for my old laptop anyway. Furthermore, the laptop had gotten a bit slow, even after it had been restored to its factory settings. In short, I fell into the trap that the hardware and software industries have set up for us and made the mistake of thinking that I needed a new laptop.</p>
<p>Having been so fond of my Thinkpad, it was only logical to get a new one. Here’s the problem: in 2005, shortly after I had bought my first Thinkpad, Lenovo, a Chinese manufacturer that is now the largest computer maker in the world, bought IBM’s PC business. Chinese companies don’t have a reputation for building quality products, especially not at the time. However, since Lenovo was still selling Thinkpads that looked almost identical to those built by IBM, I decided to try my luck and bought a <a href="http://www.thinkwiki.org/wiki/Category:T430" target="_blank">Lenovo Thinkpad T430</a> in April 2013. At a steep price, but I assumed that quality had to be paid for.</p>
<p>My mistake was clear from the beginning. I had to send the new laptop back twice because its case was deformed. When I finally got one that didn’t wobble on my desk, I quickly ran into another problem: the keys started breaking off. I can still remember my disbelief when it happened for the first time. The IBM Thinkpad is known for its robust keyboard. If you want to break it, you need a hammer. Lenovo obviously didn’t find that so important and had quietly replaced the keyboard with an inferior one. Mind you, I can be an aggressive typist, but I have never broken any other keyboard.</p>
<p>I grumpily ordered a replacement key for 15 euros. In the months after that, replacement keys became a recurring cost. After spending more than 100 euros on plastic keys, which would soon break again, I calculated that my keyboard had 90 keys and that replacing them all just once would cost me 1,350 euros. I stopped using the keyboard altogether, temporarily finding a solution in an external keyboard. However, this was impractical, especially for working away from home – and why else would I want a laptop?</p>
<p>There was no getting around it anymore: I needed a new laptop. Again. But which one? For sure it would not be one made by Lenovo or Apple.</p>
<div>
<figure data-imgstate="dither">
<img alt="Image: Replacing all keys on my Lenovo T430 would have cost me 1,350 euros." data-dither="/2020/12/how-and-why-i-stopped-buying-new-laptops/images/dithers/broken-keyboard-white_dithered.png" data-original="https://solar.lowtechmagazine.com/2020/12/how-and-why-i-stopped-buying-new-laptops/images/broken-keyboard-white_huaf323a230a1b423848610266c7325189_7786692_800x800_fit_q90_h2_box.webp" loading="lazy" src="https://solar.lowtechmagazine.com/2020/12/how-and-why-i-stopped-buying-new-laptops/images/dithers/broken-keyboard-white_dithered.png"> </figure>
<div>
<figcaption>
<p>Image: Replacing all keys on my Lenovo T430 would have cost me 1,350 euros.  <svg viewBox="0 0 100 100" xmlns="http://www.w3.org/2000/svg">
<rect height="24.28" width="24.28" x="13.51" y="13.58"></rect><rect height="24.28" width="24.28" x="37.93" y="37.86"></rect><rect height="24.28" width="24.28" x="62.21" y="13.58"></rect><rect height="24.28" width="24.28" x="13.51" y="62.14"></rect><rect height="24.28" width="24.28" x="62.21" y="62.14"></rect>
</svg></p>
<p><span>
  View original image
</span>
<span>
  View dithered image
</span>
</p>
</figcaption>
</div>
</div>
<h2 id="my-fourth-laptop-ibm-thinkpad-x60s-2017-now">My fourth laptop: IBM Thinkpad X60s (2017-now)</h2>
<p>Not finding what I was looking for, I decided to go back in time. By now, it had dawned on me that new laptops are of inferior quality compared to older laptops, even if they carry a much higher price tag.  I found out that Lenovo switched keyboards around 2011 and started searching auction sites for Thinkpads built before that year. I could have changed back to my ThinkPad R52 from 2005, but by now, I had become accustomed to a Spanish keyboard, and the R52 had a Belgian one.</p>
<p>In April 2017, I settled on a used <a href="http://www.thinkwiki.org/wiki/Category:X60s" target="_blank">Thinkpad X60s</a> from 2006. <sup id="fnref:6"><a href="#fn:6" role="doc-noteref">6</a></sup> As of December 2020, the machine is in operation for almost 4 years and is 14 years old – three to five times older than the average laptop. If I loved my Thinkpad R52 from 2005, I adore my Thinkpad X60s from 2006. It’s just as sturdily built – it already survived a drop from a table on a concrete floor – but it’s much smaller and also lighter: 1.43 kg vs. 3.2 kg.</p>
<p>My 2006 Thinkpad X60s does everything I want it to do. I use it to write articles, do research, and maintain the websites. I have also used it on-stage to give lectures, projecting images on a large screen. There’s only one thing missing on my laptop, especially nowadays, and that’s a webcam. I solve this by firing up the cursed 2013 laptop with the broken keys whenever I need to, happy to give it some use that doesn’t involve its keyboard. It could also be solved by a switch to the <a href="http://www.thinkwiki.org/wiki/Category:X200" target="_blank">Thinkpad X200</a> from 2008, which is a newer version of the same model and has a webcam.</p>
<div>
<figure data-imgstate="dither">
<img alt="Image: My ThinkPad X60s." data-dither="/2020/12/how-and-why-i-stopped-buying-new-laptops/images/dithers/thinkpad-x60s-white_dithered.png" data-original="https://solar.lowtechmagazine.com/2020/12/how-and-why-i-stopped-buying-new-laptops/images/thinkpad-x60s-white_hu30a06554e31ecbc7d8a684e77fe1da4d_3965389_800x800_fit_q90_h2_box.webp" loading="lazy" src="https://solar.lowtechmagazine.com/2020/12/how-and-why-i-stopped-buying-new-laptops/images/dithers/thinkpad-x60s-white_dithered.png"> </figure>
<div>
<figcaption>
<p>Image: My ThinkPad X60s. <svg viewBox="0 0 100 100" xmlns="http://www.w3.org/2000/svg">
<rect height="24.28" width="24.28" x="13.51" y="13.58"></rect><rect height="24.28" width="24.28" x="37.93" y="37.86"></rect><rect height="24.28" width="24.28" x="62.21" y="13.58"></rect><rect height="24.28" width="24.28" x="13.51" y="62.14"></rect><rect height="24.28" width="24.28" x="62.21" y="62.14"></rect>
</svg></p>
<p><span>
  View original image
</span>
<span>
  View dithered image
</span>
</p>
</figcaption>
</div>
</div>
<h2 id="how-to-make-an-old-laptop-run-like-its-new">How to make an old laptop run like it’s new</h2>
<p>Not buying any more new laptops is not as simple as buying a used laptop. It’s advisable to upgrade the hardware, and it’s essential to downgrade the software. There are two things you need to do:</p>
<h2 id="1-use-low-energy-software">1. Use low energy software</h2>
<p>My laptop runs on <a href="https://www.linuxliteos.com/" target="_blank">Linux Lite</a>, one of several open-source operating systems <a href="https://lotoftech.com/10-best-lightweight-operating-system-for-old-computers/" target="_blank">specially designed to work on old computers</a>. The use of a Linux operating system is not a mere suggestion. There’s no way you’re going to revive an old laptop if you stick to Microsoft Windows or Apple OS because the machine would freeze instantly. Linux Lite does not have the flashy visuals of the newest Apple and Windows interfaces, but it has a familiar graphical interface and looks anything but obsolete. It takes very little space on the hard disk and demands even less computing power. The result is that an old laptop, despite its limited specifications, runs smoothly. I also use light browsers: <a href="https://vivaldi.com/" target="_blank">Vivaldi</a> and <a href="https://astian.org/en/midori-browser/" target="_blank">Midori</a>.</p>
<p>Having used Microsoft Windows for a long time, I find Linux operating systems to be remarkably better, even more so because they are free to download and install. Furthermore, Linux operating systems do not steal your personal data and do not try to lock you in, like the newest operating systems from both Microsoft and Apple do. That said, even with Linux, obsolescence cannot be ruled out. For example, Linux Lite will stop its support for 32-bit computers in 2021, which means that I will soon have to look for an alternative operating system, or buy a slightly younger 64-bit laptop.</p>
<h2 id="2-replace-the-hard-disk-drive-with-a-solid-state-drive">2. Replace the hard disk drive with a solid-state drive</h2>
<p>In recent years, solid-state drives (SSD) have become available and affordable, and they are much faster than hard disk drives (HDD). Although you can revive an old laptop by merely switching to a light-weight operating system, if you also replace the hard disk drive with a solid-state drive, you’ll have a machine that is just as fast as a brand new laptop. Depending on the storage capacity you want, an SSD will cost you between 20 euro (120 GB) and 100 euro (960 GB).</p>
<p>Installment is pretty straightforward and well documented online. Solid-state drives run silently and are more resistant to physical shock, but they have a shorter life expectancy than hard disk drives. Mine is now working for almost 4 years. It seems that both from an environmental and financial viewpoint, an old laptop with SSD is a much better choice than buying a new laptop, even if the solid-state drive needs replacement now and then.</p>
<h2 id="spare-laptops">Spare laptops</h2>
<p>Meanwhile, my strategy has evolved. I have bought two identical models for a similar price, in 2018 and early 2020, to use as spare laptops. Now I plan to keep working on these machines for as long as possible, having more than sufficient spare parts available. Since I bought the laptop, it had two technical issues. After roughly a year of use, the fan died. I had it repaired overnight in a tiny and messy IT shop run by a Chinese man in Antwerp, Belgium. He said that my patched fan would run for another six months, but it’s still working more than two years later.</p>
<p>Then, last year, my X60s suddenly refused to charge its battery, an issue that had also appeared with my cursed 2013 laptop. It seems to be a common problem with Thinkpads, but I could not solve it yet. Neither did I really have to because I had a spare laptop ready and started using that one whenever I needed or wanted to work outside.</p>
<div>
<figure data-imgstate="dither">
<img alt="Image: Three identical 2006 laptops, all in working order, for less than 200 euros." data-dither="/2020/12/how-and-why-i-stopped-buying-new-laptops/images/dithers/spare-laptops-white_dithered.png" data-original="https://solar.lowtechmagazine.com/2020/12/how-and-why-i-stopped-buying-new-laptops/images/spare-laptops-white_hu3766708df1a6ba8bdbeb0add86f0194b_6142617_800x800_fit_q90_h2_box.webp" loading="lazy" src="https://solar.lowtechmagazine.com/2020/12/how-and-why-i-stopped-buying-new-laptops/images/dithers/spare-laptops-white_dithered.png"> </figure>
<div>
<figcaption>
<p>Image: Three identical 2006 laptops, all in working order, for less than 200 euros. <svg viewBox="0 0 100 100" xmlns="http://www.w3.org/2000/svg">
<rect height="24.28" width="24.28" x="13.51" y="13.58"></rect><rect height="24.28" width="24.28" x="37.93" y="37.86"></rect><rect height="24.28" width="24.28" x="62.21" y="13.58"></rect><rect height="24.28" width="24.28" x="13.51" y="62.14"></rect><rect height="24.28" width="24.28" x="62.21" y="62.14"></rect>
</svg></p>
<p><span>
  View original image
</span>
<span>
  View dithered image
</span>
</p>
</figcaption>
</div>
</div>
<div>
<figure data-imgstate="dither">
<img alt="Image: Inside the Thinkpad X60s. Source: Hardware Maintenance Manual." data-dither="/2020/12/how-and-why-i-stopped-buying-new-laptops/images/dithers/thinkpad-inside_dithered.png" data-original="https://solar.lowtechmagazine.com/2020/12/how-and-why-i-stopped-buying-new-laptops/images/thinkpad-inside_huad20844be5ac8ba7ef947df62b823a02_178972_800x800_fit_q90_h2_box_3.webp" loading="lazy" src="https://solar.lowtechmagazine.com/2020/12/how-and-why-i-stopped-buying-new-laptops/images/dithers/thinkpad-inside_dithered.png"> </figure>

</div>
<h2 id="the-magical-sd-card">The magical SD-card</h2>
<p>Now to introduce you to my magical SD-card, which is another hardware upgrade that facilitates the use of old (but also new) laptops. Many people have their personal documents stored on their laptop’s hard drive and then make backups to external storage media if all goes well. I do it the other way around.</p>
<p>I have all my data on a 128 GB SD-card, which I can plug into any of the Thinkpads that I own. I then make monthly backups of the SD-card, which I store on an external storage medium, as well as regular backups of the documents that I am working on, which I temporarily store on the drive of the laptop that I am working on. This has proven to be very reliable, at least for me: I have stopped losing work due to computer problems and insufficient backups.</p>
<p>The other advantage is that I can work on any laptop that I want and that I’m not dependent on a particular machine to access my work. You can get similar advantages when you keep all your data in the cloud, but the SD-card is <a href="https://solar.lowtechmagazine.com/2015/10/why-we-need-a-speed-limit-for-the-internet/">the more sustainable option</a>, and it works without internet access.</p>
<p>Hypothetically, I could have up to two hard drive failures in one day and keep working as if nothing happened. Since I am now using both laptops alternately – one with battery, the other one without – I can also leave them at different locations and cycle between these places while carrying only the SD-card in my wallet. Try that with your brand new, expensive laptop. I can also use my laptops together if I need an extra screen.</p>
<p>In combination with a hard disk drive, the SD-card also increases the performance of an old laptop and can be an alternative to installing a solid-state drive. My spare laptop does not have one and it can be slow when browsing heavy-weight websites. However, thanks to the SD-card, opening a map or document happens almost instantly, as does scrolling through a document or saving it. The SD-card also keeps the hard disk running smoothly because it’s mostly empty. I don’t know how practical using an SD-card is for other laptops, but all my Thinkpads have a slot for them.</p>
<h2 id="the-costs">The costs</h2>
<p>Let’s make a complete cost calculation, including the investment in spare laptops and SD-card, and using today’s prices for both solid-state drives and SD-cards, which have become much cheaper since I have bought them:</p>
<ul>
<li>ThinkPad X60s: 50 euro</li>
<li>ThinkPad X60s spare laptop: 60 euro</li>
<li>ThinkPad X60 spare laptop: 75 euro</li>
<li>Two replacement batteries: 50 euro</li>
<li>240 GB solid-state drive: 30 euro</li>
<li>128 GB SD-card: 20 euro</li>
<li>Total: 285 euros</li>
</ul>
<p>Even if you buy all of this, you only spent 285 euros. For that price, you may be able to buy the crappiest new laptop on the market, but it surely won’t get you two spare laptops. If you manage to keep working with this lot for ten years, your laptop costs would be 28.5 euros per year. You may have to replace a few solid-state drives and SD-cards, but it won’t make much difference. Furthermore, you save the ecological damage that is caused by the production of a new laptop every 5.7 years.</p>
<div>
<figure data-imgstate="dither">
<img alt="Image: My laptop needs are met for the foreseeable future." data-dither="/2020/12/how-and-why-i-stopped-buying-new-laptops/images/dithers/spare-laptops-2-white_dithered.png" data-original="https://solar.lowtechmagazine.com/2020/12/how-and-why-i-stopped-buying-new-laptops/images/spare-laptops-2-white_hu2dc6db9faa5724cc3c1faebbecf747bd_5522562_800x800_fit_q90_h2_box.webp" loading="lazy" src="https://solar.lowtechmagazine.com/2020/12/how-and-why-i-stopped-buying-new-laptops/images/dithers/spare-laptops-2-white_dithered.png"> </figure>
<div>
<figcaption>
<p>Image: My laptop needs are met for the foreseeable future. <svg viewBox="0 0 100 100" xmlns="http://www.w3.org/2000/svg">
<rect height="24.28" width="24.28" x="13.51" y="13.58"></rect><rect height="24.28" width="24.28" x="37.93" y="37.86"></rect><rect height="24.28" width="24.28" x="62.21" y="13.58"></rect><rect height="24.28" width="24.28" x="13.51" y="62.14"></rect><rect height="24.28" width="24.28" x="62.21" y="62.14"></rect>
</svg></p>
<p><span>
  View original image
</span>
<span>
  View dithered image
</span>
</p>
</figcaption>
</div>
</div>
<h2 id="dont-take-it-too-far">Don’t take it too far</h2>
<p>Although I have used my Thinkpad X60s as an example, the same strategy works with other Thinkpad models – <a href="http://www.thinkwiki.org/wiki/ThinkPad_History" target="_blank">here’s an overview of all historical models</a> – and laptops from other brands (which I know nothing about). If you prefer not to buy on auction sites, you can walk to the nearest pawnshop and get a used laptop with a guarantee. The chances are that you don’t even need to buy anything, as many people have old laptops lying around.</p>
<p>There’s no need to go back to a 2006 machine. I hope it’s clear that I am trying to make a statement here, and I probably went as far back as one can while keeping things practical. My first try was a used ThinkPad X30 from 2002, but that was one step too far. It uses a different charger type, it has no SD-card slot, and I could not get the wireless internet connection working. For many people, it may serve to choose a somewhat younger laptop. That will give you a webcam and a 64-bit architecture, which makes things easier. Of course, you can also try to beat me and go back to the 1990s, but then you’ll have to do without USB and wireless internet connection.</p>
<p>Your choice of laptop also depends on what you want to do with it. If you use it mainly for writing, surfing the web, communication, and entertainment, you can do it as cheaply as I did. If you do graphical or audiovisual work, it’s more complicated, because in that case, you’re probably an Apple user. The same strategy could be applied, on a somewhat younger and more expensive laptop, but it would suggest switching from a Mac to a Linux operating system. When it comes to office applications, Linux is clearly better than its commercial alternatives. For a lack of experience, I cannot tell you if that holds for other software as well.</p>
<h2 id="this-is-a-hack-not-a-new-economical-model">This is a hack, not a new economical model</h2>
<p>Although capitalism could provide us with used laptops for decades to come, the strategy outlined above should be considered a hack, not an economical model. It’s a way to deal with or escape from an economic system that tries to force you and me to consume as much as possible. It’s an attempt to break that system, but it’s not a solution in itself. We need another economical model, in which we build all laptops like pre-2011 Thinkpads. As a consequence, laptop sales would go down, but that’s precisely what we need. Furthermore, with today’s computing efficiency, we could significantly reduce the operational and embodied energy use of a laptop if we reversed the trend towards ever higher functionality.</p>
<p>Significantly, hardware and software changes drive the fast obsolescence of computers, but the latter has now become the most crucial factor. A computer of 15 years old has all the hardware you need, but it’s not compatible with the newest (commercial) software. This is true for operating systems and every type of software, from games to office applications to websites. Consequently, to make laptop use more sustainable, the software industry would need to start making every new version of its products lighter instead of heavier. The lighter the software, the longer our laptops will last, and we will need less energy to use and produce them.</p>
<p>Kris De Decker</p>
<p>Images: Jordi Manrique Corominas, Adriana Parra, Roel Roscam Abbing</p>
<p>Proofreading: Eric Wagner</p>
</div>



</article>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[An ARM Assembler Written in Lisp (143 pts)]]></title>
            <link>http://forum.ulisp.com/t/an-arm-assembler-written-in-lisp/1237</link>
            <guid>36646277</guid>
            <pubDate>Sat, 08 Jul 2023 16:58:55 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="http://forum.ulisp.com/t/an-arm-assembler-written-in-lisp/1237">http://forum.ulisp.com/t/an-arm-assembler-written-in-lisp/1237</a>, See on <a href="https://news.ycombinator.com/item?id=36646277">Hacker News</a></p>
<div id="readability-page-1" class="page"><div itemprop="articleBody">
        <p>You can write machine-code functions in uLisp with the help of the ARM assembler written in Lisp, and I’ve recently updated it to make it more compact. It will now fit on a board with about 2000 objects of workspace, with room to spare to write assembler programs and run them.</p>
<p>This post describes how the latest version of the ARM assembler works. The aim is to help anyone who wants to extend the assembler to cater for ARM instructions that it doesn’t currently support. It will also be helpful if you want to write an assembler for another processor, or even design your own processor and write an assembler for it; Lisp is an excellent language to do this. For example, a printout of the whole ARM assembler fits on two A4 pages.</p>
<h3>Instruction encodings</h3>
<p>The starting point for writing an assembler is to get hold of a summary of the processor’s table of instruction encodings. For the ARM Thumb instruction set these are as follows:</p>

<p><em>ARM Thumb instruction encodings for instructions starting <span>#x0</span> to <span>#x8</span>.</em></p>

<p><em>ARM Thumb instruction encodings for instructions starting <span>#x9</span> to <span>#xF</span>.</em></p>
<p>You can see from these diagrams that the 16-bit instructions are arranged into consistent field patterns. This is true of most processor instruction sets, but some are more orderly than others (RISC-V is a nightmare!).</p>
<h4>An example - LSL</h4>
<p>As an example, consider the first instruction in the first table, LSL immediate:</p>

<p>This consists of:</p>
<ul>
<li>The four-bit value <span>#b0000</span>.</li>
<li>A one-bit <em>op</em> code, which is 0 for LSL and 1 for LSR.</li>
<li>An <em>immed5</em> value, which is a 5-bit integer from 0 to 31 giving the size of the left shift.</li>
<li>
<em>Lm</em>, which is a value from 0 to 7 representing the source register R0 to R7.</li>
<li>
<em>Ld</em>, which is a value from 0 to 7 representing the destination register R0 to R7.</li>
</ul>
<h3>Emitting bit fields</h3>
<p>The first function we need is <strong>emit</strong>, which takes a specification defining the widths of the bit fields, and a list of arguments, and packs the values of the arguments into the bit fields:</p>
<pre><code>(defun emit (bits &amp;rest args)
  (let ((word 0) (shift -28))
    (mapc #'(lambda (value)
              (let ((width (logand (ash bits shift) #xf)))
                (incf shift 4)
                (unless (zerop (ash value (- width))) (error "Won't fit"))
                (setq word (logior (ash word width) value))))
          args)
    word))
</code></pre>
<p>The first argument, <strong>bits</strong>, is a 32-bit hexadecimal number in which each hex digit specifies the width of the next bit field. The function <strong>emit</strong> reads the hex digits in <strong>bits</strong> from left to right, packs the appropriate number of bits from each argument into <strong>word</strong>, and then returns the result.</p>
<p>For example, the bit fields for the LSL instruction could be specified by:</p>
<pre><code>#x41533000
</code></pre>
<p>To make it easier to process the bit fields the widths are left-aligned, so you should add zeros to make the <strong>bits</strong> parameter eight hex digits.</p>
<p>The remaining arguments are the values to be packed into the bit fields. If any argument won’t fit into the corresponding bit field the error <strong>Won’t fit</strong> will be displayed.</p>
<p>So for example, to emit the op code for the instruction:</p>
<pre><code>LSL r7, r4, #31
</code></pre>
<p>evaluate:</p>
<pre><code>&gt; (emit #x41533000 0 0 31 4 7)
2023
</code></pre>
<p>If you print this as a 16-bit binary number with:</p>
<pre><code>&gt; (format t "~16,'0b" 2023)
0000011111100111
</code></pre>
<p>you can see that the values have been put into the correct fields as required.</p>
<h3>Specifying registers</h3>
<p>The next step is to be able to specify registers as r0 to r15, or their synonyms <strong>sp</strong> (for r13), <strong>lr</strong> (for r14), and <strong>pc</strong> (for r15). This is handled by the function <strong>regno</strong>:</p>
<pre><code>(defun regno (sym)
  (case sym (sp 13) (lr 14) (pc 15)
    (t (read-from-string (subseq (string sym) 1)))))
</code></pre>
<p>For example:</p>
<pre><code>&gt; (regno 'r12)
12
</code></pre>
<p>Finally, we can now define the LSL instruction as the convenient Lisp function <strong>$lsl</strong> as follows:</p>
<pre><code>(defun $lsl (argd argm immed5)
  (emit #x41533000 0 0 immed5 (regno argm) (regno argd))
</code></pre>
<p>This allows us to specify the instruction using syntax that’s close to ARM assembler syntax:</p>
<pre><code>&gt; ($lsl 'r7 'r4 31)
2023
</code></pre>
<p>I’ve used the convention that functions representing ARM instructions are prefixed by a $ sign; otherwise there would be a problem with instructions that are also existing Lisp functions, such as <strong>push</strong> and <strong>pop</strong>.</p>
<h3>Handling addressing modes</h3>
<p>The final complication is that some instruction mnemonics can generate different op codes, depending on the types of their arguments.</p>
<p>For example, there’s also a variant of LSL that shifts a register Rd by the shift value specified in the register Rs:</p>

<p>Using this syntax, the following assembler instruction shifts the value in R7 by the value in R1:</p>
<pre><code>LSL r7, r1
</code></pre>
<p>The block of register-to-register instructions that include LSL is handled by the routine <strong>reg-reg</strong>:</p>
<pre><code>(defun reg-reg (op argd argm)
  (emit #xa3300000 op (regno argm) (regno argd)))
</code></pre>
<p>Finally, we need to modify <strong>$lsl</strong> to include the register-to-register variant:</p>
<pre><code>(defun $lsl (argd argm &amp;optional arg2)
  (cond
   ((numberp arg2)
    (lsl-lsr-0 0 arg2 argm argd))
   ((numberp argm)
    (lsl-lsr-0 0 argm argd argd))
   (t
    (reg-reg #b0100000010 argd argm))))
</code></pre>
<p>where <strong>lsl-lsr-0</strong> is defined as:</p>
<pre><code>(defun lsl-lsr-0 (op immed5 argm argd)
  (emit #x41533000 0 op immed5 (regno argm) (regno argd)))
</code></pre>
<p>This expanded version of <strong>$lsl</strong> also handles the two-argument case where the source and destination registers are the same in an immediate shift; for example:</p>
<pre><code>($lsl 'r1 31)
</code></pre>
<h3>Running the assembler</h3>
<p>To run the assembler in uLisp you use the built-in command <strong>defcode</strong>, which generates an assembler listing, and puts the machine code into RAM so you can execute it as if it’s a normal Lisp function.</p>
<h4>Greatest Common Divisor example</h4>
<p>For example, to assemble a machine-code routine <strong>gcd</strong> to calculate Greatest Common Divisor you’d evaluate:</p>
<pre><code>; Greatest Common Divisor
(defcode gcd (x y)
  swap
  ($mov 'r2 'r1)
  ($mov 'r1 'r0)
  again
  ($mov 'r0 'r2)
  ($sub 'r2 'r2 'r1)
  ($blt swap)
  ($bne again)
  ($bx 'lr))
</code></pre>
<p>and you could then call:</p>
<pre><code>&gt; (gcd 3287 3460)
173
</code></pre>
<h3>Running the assembler in Common Lisp</h3>
<p>You can also run the ARM assembler in a standard Common Lisp implementation. The Common Lisp version of the ARM Assembler includes the following <strong>defcode</strong> macro that lets you assemble an ARM function and print the machine code, like the <strong>defcode</strong> special form built into uLisp:</p>
<pre><code>(defparameter *pc* 0)

(defmacro defcode (&amp;body body)
  (let ((*print-pretty* t) (assembler (cddr body)))
    (dotimes (pass 2)
      (setq *pc* 0)
      (mapc
       #'(lambda (ins)
           (cond
            ((atom ins)
             (unless (zerop pass) (format t "~4,'0x      ~(~a~)~%" *pc* ins))
             (set ins *pc*))
            ((listp (eval ins))
             (unless (zerop pass)
               (format t "~4,'0x ~4,'0x ~(~a~)~%" *pc* (first (eval ins)) ins)
               (format t "~4,'0x ~4,'0x~%" (+ *pc* 2) (second (eval ins))))
             (incf *pc* 4))
            (t
             (unless (zerop pass)
               (format t "~4,'0x ~4,'0x ~(~a~)~%" *pc* (eval ins) ins))
             (incf *pc* 2))))
       assembler)
      nil)))
</code></pre>
<p>Evaluating the <strong>Greatest Common Divisor example</strong> above generates the following output:</p>
<pre><code>0000      swap
0000 000A ($mov 'r2 'r1)
0002 0001 ($mov 'r1 'r0)
0004      again
0004 0010 ($mov 'r0 'r2)
0006 1A52 ($sub 'r2 'r2 'r1)
0008 DBFA ($blt swap)
000A D1FB ($bne again)
000C 4770 ($bx 'lr)
</code></pre>
<p>In this case you obviously won’t be able to run the machine code.</p>
<h3>Resources</h3>
<p>For both versions of the assembler see: <a href="https://github.com/technoblogy/lisp-arm-assembler">https://github.com/technoblogy/lisp-arm-assembler</a>.</p>
<p>For more information see <a href="http://www.ulisp.com/show?2YRU">ARM assembler overview</a>.</p>
<p>For a list of the ARM Thumb instructions supported by the assembler see: <a href="http://www.ulisp.com/show?30B8">ARM assembler instructions</a>.</p>
<p>For ARM assembler examples see: <a href="http://www.ulisp.com/show?30BD">ARM assembler examples</a>.</p>
<h3>Update</h3>
<p>6th July 2023: Updated the <strong>defcode</strong> macro to handle forward references.</p>
      </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Can a Rubik's Cube be brute-forced? (108 pts)]]></title>
            <link>https://www.stylewarning.com/posts/brute-force-rubiks-cube/</link>
            <guid>36645846</guid>
            <pubDate>Sat, 08 Jul 2023 16:19:54 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.stylewarning.com/posts/brute-force-rubiks-cube/">https://www.stylewarning.com/posts/brute-force-rubiks-cube/</a>, See on <a href="https://news.ycombinator.com/item?id=36645846">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="content">
<article>
<p><em>By Robert Smith</em></p>
<div>
<hr>
<h2>Contents</h2>
<nav id="TableOfContents">
<ol>
<li><a href="#introduction">Introduction</a></li>
<li><a href="#computer-puzzling-without-brute-force">Computer puzzling without brute-force</a></li>
<li><a href="#taking-a-step-back-puzzles-as-permutations">Taking a step back: puzzles as permutations</a></li>
<li><a href="#brute-force-still-ignorant-but-kinda-smart">Brute-force, still ignorant, but kinda smart?</a>
<ol>
<li><a href="#observation-1-decomposition-as-intersection">Observation #1: decomposition as intersection</a></li>
<li><a href="#observation-2-sorting-really-helps">Observation #2: sorting really helps!</a></li>
<li><a href="#what-is-a-move">What is a move?</a></li>
<li><a href="#what-is-a-word">What is a word?</a></li>
<li><a href="#observation-3-sorting-as-solving">Observation #3: sorting as solving</a></li>
<li><a href="#more-splitting">More splitting?</a></li>
<li><a href="#iterating-through-products-with-schroeppel--shamir">Iterating through products with Schroeppel–Shamir</a></li>
<li><a href="#permutation-tries">Permutation tries</a></li>
<li><a href="#the-4-list-algorithm-and-solving-the-rubiks-cube">The 4-List Algorithm and solving the Rubik’s Cube</a></li>
</ol>
</li>
<li><a href="#example-and-source-code">Example and source code</a></li>
<li><a href="#tips-for-optimizing-the-4-list-algorithm">Tips for optimizing the 4-List Algorithm</a></li>
<li><a href="#sample-benchmarks">Sample benchmarks</a></li>
<li><a href="#conclusion">Conclusion</a></li>
<li><a href="#references">References</a></li>
</ol>
</nav>
<hr>
</div>
<h2 id="introduction">Introduction</h2>
<p>When I was about 13, while still a middle-schooler, I became
fascinated with the Rubik’s Cube<sup id="fnref:1"><a href="#fn:1" role="doc-noteref">1</a></sup>. I never got terribly good
at solving it, maybe eventually getting into the 30 to 40 seconds
range. While I didn’t have a penchant for memorizing move sequences, I
was drawn into how we <em>find</em> these move sequences.</p>
<p>The story about my interest and exploration in the Rubik’s Cube is for
another post. Long story short, I got interested in “computer
puzzling”—using computers to manipulate combinatorial puzzles, like
the Rubik’s Cube, either to solve them quickly, to discover patterns,
or to find novel move sequences for use in speedcubing—and ever
since, I’ve been working on different programs for solving Rubik-like
puzzles.</p>
<p>Purely in principle, it shouldn’t be hard to solve a Rubik’s Cube with
a computer, right? Our program would have three parts:</p>
<ol>
<li>A model of the Rubik’s Cube, that is, some data structure that
represents a cube state.</li>
<li>Some functions which can simulate turns of each side.</li>
<li>A solving procedure which takes a scrambled cube, tries every
possible turn sequence, and stops when solved.</li>
</ol>
<p>Truth be known, and details aside, this is a provably correct method
for solving a Rubik’s Cube. If you leave your computer on long enough,
it will return a solution.</p>
<p>The problem is that it takes a long time. Probably longer than your
lifetime.</p>
<h2 id="computer-puzzling-without-brute-force">Computer puzzling without brute-force</h2>
<p>“Brute-force” generally means to try every possibility of something
without much of any strategy. Our method above is a brute-force
algorithm. Brute-force algorithms generally aren’t practical, because
if you have $N$ of something to explore, a brute-force algorithm will
take $O(N)$ time. For a Rubik’s Cube, $N$ is 43 quintillion—a very
large number.</p>
<p>It has been known, practically since the Rubik’s Cube’s inception,
that something else is needed to solve a Rubik’s Cube. Rubik’s Cube
solutions, obviously, take into account the specific structure and
properties of the cube so as to implicitly or explicitly avoid
mindless search. These methods have turned out to be:</p>
<ol>
<li>Solving methods for humans: memorize some sequences which let you
move only a few pieces around in isolation, and apply these
sequences mechanically until all pieces are in place. The more
sequences you memorize, the faster you’ll be.</li>
<li>Heuristic tree search: do a tree search (with e.g.,
iterative-deepening depth-first search<sup id="fnref:2"><a href="#fn:2" role="doc-noteref">2</a></sup>), but aggressively
prune off branches by way of clever heuristics<sup id="fnref:3"><a href="#fn:3" role="doc-noteref">3</a></sup>.</li>
<li>Phase-based solvers: a deeply mathematical way which involves
characterizing the Rubik’s Cube as a sequence of nested
(mathematical) subgroups so that each successive coset space small
enough that it can be solved by computer.</li>
</ol>
<p>Computer puzzling mostly deals with the latter two approaches, usually
in some combination. Both approaches lead to extraordinarily
high-performing solvers. For example:</p>
<ul>
<li>Korf’s algorithm (approach #2) finds optimal solutions—solutions
of shortest length—but can take hours to find one.</li>
<li>Thistlethwaite’s algorithm (approach #3) solves a cube in four
phases almost instantaneously. The solutions are guaranteed to be no
longer than triple the optimal length.</li>
</ul>
<p>The story may as well end here. We have slow but optimal ways of
solving the Rubik’s Cube, and fast but sub-optimal ways. Pick your
poison (sub-optimal or slow), depending on what you’re trying to
achieve.</p>
<h2 id="taking-a-step-back-puzzles-as-permutations">Taking a step back: puzzles as permutations</h2>
<p>It seems that any Rubik’s Cube solver <em>has</em> to know <em>something</em> about
the structure of the cube. It might be worth asking how little
structure we can get away with, so as to make whatever solving
algorithm we write generic over a broad class of puzzles.</p>
<p>For a brute-force algorithm with tree search, we would need something
like the following:</p>
<pre tabindex="0"><code>interface GenericPuzzle:
  type State
  type Move

  function isSolved(State) -&gt; Boolean
  function allMoves() -&gt; List(Move)
  function performMove(State, Move) -&gt; State
</code></pre><p>With this, we could write the following solver based off of
iterative-deepening depth-first search, which is totally generic on
the above interface.</p>
<pre tabindex="0"><code>function solve(State) -&gt; List(Move)
function solve(p):
  if isSolved(p):
    return []

  for maxDepth from 1 to infinity:
    solved?, solution = dfs(0, maxDepth, p)
    
    if solved?:
      return solution

function dfs(Integer, Integer, State, List(Move)) -&gt; (Boolean, List(Move))
function dfs(depth, maxDepth, p, s):
  if isSolved(p):
    return (True, s)

  if depth == maxDepth:
    return (False, [])

  for m in allMoves():
    p' = performMove(p, m)
    (solved?, solution) = dfs(depth+1, maxDepth, p', append(s, [m])

    if solved?:
      return (solved?, solution)
</code></pre><p>As discussed before, while this strategy is effective for problems
with small search spaces, it’s no help when the space is
large. Unfortunately, the <code>GenericPuzzle</code> interface doesn’t give us
much room for improvement. Can we still remain generic, while giving
us at least a little more room for exploring other algorithms?</p>
<p>The answer is yes, if we restrict ourselves to <em>permutation
puzzles</em>. Roughly speaking, a permutation puzzle is one where pieces
shift around according to a fixed and always available set of shifting
moves. The Rubik’s Cube is a phenomenal and non-trivial example: We
can label each mobile<sup id="fnref:4"><a href="#fn:4" role="doc-noteref">4</a></sup> sticker with a number 1 to 48, and
these stickers can always be shifted around with a twist of any of the
six sides. Since we can twist any of the six sides at any time, the
puzzle is a permutation puzzle. (Not all similar puzzles are
permutation puzzles. There are some puzzles which are “bandaged”, that
is, pieces of the puzzle are fused together, restricting some
available moves depending on the configuration.)</p>
<p>In this view, we looked at a solved configuration as a list of
numbers. For example, the solved Rubik’s Cube as a permutation would
be</p>
<p>$$
(1, 2, \ldots, 47, 48).
$$</p>
<p>When we turn a side, these numbers get permuted. For instance,
assuming a particular labeling of stickers with numbers, turning the
top face of a Rubik’s Cube might permute the first sticker in the list
to the third, the second sticker to the fifth, the third sticker to
the eighth, etc. We can use the same notation</p>
<p>$$
(3, 5, 8, 2, 7, 1, \ldots)
$$</p>
<p>This notation has two interpretations:</p>
<ol>
<li>The literal position of numbered stickers on a physical cube (with
an agreed upon labeling).</li>
<li>An instruction for how to relabel the stickers of a given cube.</li>
</ol>
<p>If we look at the notation under the second interpretation, a
permutation actually represents a <em>function</em> that’s applied to
<em>individual stickers</em>. For instance, if</p>
<p>$$
F := (3, 5, 8, 2, 7, 1, \ldots)
$$</p>
<p>then $F(1) = 3$, $F(2) = 5$, etc. All of the clockwise face
turns—Front, Right, Up, Back, Left, Down—of a Rubik’s Cube can be
described like so:</p>
<p>$$
\begin{align*}
F &amp;:= (1, 2, 3, 4, 5, 25, \ldots)\\
R &amp;:= (1, 2, 38, 4, 36, 6, \ldots)\\
U &amp;:= (3, 5, 8, 2, 7, 1, \ldots)\\
B &amp;:= (14, 12, 9, 4, 5, 6, \ldots)\\
L &amp;:= (17, 2, 3, 20, 5, 22, \ldots)\\
D &amp;:= (1, 2, 3, 4, 5, 6, \ldots, 48, 42, 47, 41, 44, 46)
\end{align*}
$$</p>
<p>We wrote some of the last elements of $D$ because a “down” move doesn’t
change the first six stickers in this labeling scheme.</p>
<p>This gives is a whole new interpretation of what it means to “solve” a
cube. Given a scrambled cube, we first write down the permutation that
describes how the stickers moved from a solved state to the scrambled
state. Let’s call it $s$. This is easy, because we can just read the
labeled stickers off of a cube one-by-one, in order. For example, $s$
might be:</p>
<p>$$
s := (27, 42, 30, 15, 39, 6, \ldots).
$$</p>
<p><em>This is a description of a function!</em> The value of $s(1)$ describes
how the first sticker of a cube will be shifted to its scrambled
position, in this case $27$. Next, solving a cube is finding a
sequence of $k$ moves $m_1, m_2, \ldots, m_k$ such that, for all $1\leq
i\leq 48$,</p>
<p>$$
i = m_k(m_{k-1}(\cdots(m_2(m_1(s(i)))))).
$$</p>
<p>Stated another way in function composition notation, the function</p>
<p>$$
m_k \circ m_{k-1} \circ \cdots \circ m_2 \circ m_1\circ s
$$</p>
<p>must be the identity function—a permutation that doesn’t move
anything.</p>
<p>In the permutation puzzle way of thinking, we can still implement our
<code>GenericPuzzle</code> interface:</p>
<ul>
<li><code>State</code> would be a permutation;</li>
<li><code>Move</code> would also be a permutation;</li>
<li><code>isSolved</code> would check if a permutation is $(1, 2, 3, \ldots)$;</li>
<li><code>allMoves</code> would be a hard-coded list of the possible moves, like
$F$, $R$, $U$, $B$, $L$, and $D$ for the Rubik’s cube; and</li>
<li><code>performMove</code> would take the input move permutation, and apply it as
a function to each element of the state permutation.</li>
</ul>
<p>This might even be <em>more</em> efficient than another choice of
representation, since permutations can be represented very efficiently
on a computer as packed arrays of bytes!</p>
<p>But we didn’t do all this mathematical groundwork just to goof around;
there’s something amazing lurking in these permutations.</p>
<h2 id="brute-force-still-ignorant-but-kinda-smart">Brute-force, still ignorant, but kinda smart?</h2>
<p>In the late 1980s, Adi Shamir<sup id="fnref:5"><a href="#fn:5" role="doc-noteref">5</a></sup> and his students made a
brilliant series of observations that came together to make for a
beautiful result. Unfortunately, to my knowledge, only two writings
exist on the topic.</p>
<ol>
<li>
<p>Shamir and his colleagues wrote a paper about it [1], sort of in
the style of a brief conference proceeding, but it’s very light on
details and skips implementation considerations. It’s the kind of
paper where you follow it, but you have to fill in a great number
of blanks to make anything from it work.</p>
</li>
<li>
<p>Shamir gave a talk sometime in the 80’s about his result, and
somebody (none other than Alan Bawden) wrote a brief email [2] to a
mailing list about his recollection of it.</p>
</li>
</ol>
<p>An amazing result, buried in history, without any good exposition that
I could find.</p>
<p>What’s the result? The essence of the result is this. Reminiscent of a
“meet in the middle” algorithm, if we want to brute-force a problem
that ordinarily requires visiting $N$ states to find an answer, we can
instead cleverly split the work into two searches that requires visits
to around $\sqrt{N}$ states. For a Rubik’s Cube, that cuts work
associated with 43 quintillion states, down to work associated with 6
billion states. The best part is, this is <em>still brute-force</em>;
virtually no knowledge of the structure of the problem is required to
make it work.</p>
<p>Let’s walk through the requisite steps and build up to the
result. I’ll attempt to write in a general framework (since it’s a
general algorithm), but make frequent appeals to the Rubik’s Cube
specifically.</p>
<h3 id="observation-1-decomposition-as-intersection">Observation #1: decomposition as intersection</h3>
<p>Suppose the following:</p>
<ul>
<li>We have a mysterious permutation $s$, say, a scrambled puzzle;</li>
<li>We have two sets of permutations $X$ and $Y$; and</li>
<li>We assume there’s an $\hat x\in X$ and $\hat y\in Y$ such that $s =
\hat y\circ \hat x$.</li>
</ul>
<p>The goal is to find precisely $\hat x$ and $\hat y$ are. The simplest
way to do this is to check every combination of elements in $X$ and
$Y$.</p>
<pre tabindex="0"><code>for x in X:
  for y in Y:
    when s = compose(y, x):
      return (x, y)
</code></pre><p>This will take time proportional to the product of the set sizes:
$O(\vert X\vert\cdot\vert Y\vert)$. Shamir noticed the following: If
$s=\hat y\circ\hat x$, then $\hat y^{-1}\circ s = \hat x$. With this, we
preprocess our $Y$ set to be instead</p>
<p>$$
Y' := \{y^{-1}\circ s : y\in Y\}.
$$</p>
<p>By doing this, there must be an element in common between $X$ and
$Y'$, since $\hat x\in X$ and $\hat y^{-1}\circ s\in Y'$ and those are
equal. So we’ve reduced the problem to determining what the
intersection between $X$ and $Y'$ is.</p>
<p>Once we find our $z$ which is in common with $X$ and $Y'$, then our
recovered permutation will be $\hat x = z$ and $\hat y = (z\circ
s^{-1})^{-1}$.</p>
<p>We’ve just established that the problem of decomposing an element like
$s$ is identical to the problem of calculating a set
intersection. Still, if we want to do the intersection, our intuition
tells us we still need a quadratic algorithm, which brings us to the
second observation.</p>
<h3 id="observation-2-sorting-really-helps">Observation #2: sorting really helps!</h3>
<p>Permutations have a natural ordering, called <em>lexicographic
ordering</em>. If you have two permutations, and you read their elements
left-to-right, you can compare them like ordinary numbers. Just
as $123 &lt; 213$, we can say that</p>
<p>$$
(1,2,3) &lt; (2,1,3).
$$</p>
<p>A nice property of this is that the identity permutation $(1, 2, 3,
\ldots)$ is the smallest permutation of a given size.</p>
<p>How does this help us? Well, suppose we sort our sets $X$ and $Y'$
into lists $L_X$ and $L_{Y'}$, so the permutations are in order. If
$L_X$ and $L_{Y'}$ have an element in common, we can find it in linear
time: $O(\min\{\vert X\vert, \vert Y'\vert\})$. How? Something like
the following:</p>
<pre tabindex="0"><code>function findCommon(Lx, Ly):
  x = pop(Lx)
  y = pop(Ly)
  loop:
    if x == y:
      return x
    
    if empty(Lx) or empty(Ly):
      error("No common elements found.")

    if x &lt; y:
      x = pop(Lx)
    else if x &gt; y:
      y = pop(Ly)
</code></pre><p>This works because we are essentially looking at all of the elements
of $L_X$ and $L_{Y'}$ together in sorted order. It’s like a merge
sort, without the merge part.</p>
<p>Before continuing, we should take a little scenic tour on a more
formal meaning of “moves” and “move sequences”, since ultimately any
permutation puzzle solving algorithm must produce them as output.</p>
<h3 id="what-is-a-move">What is a move?</h3>
<p>A quick bit about notation. If we have a permutation $f$, then its
inverse is written $f^{-1}$, and it’s $k$-fold repetition $f\circ
f\circ\cdots\circ f$ is written $f^k$. If we have a collection of
permutations $S := \{f_1, f_2, \ldots\}$, then we write the
following shorthands:</p>
<p>$$
\begin{align*}
S^{-1} &amp;:= \{f^{-1} : f \in S\}\\
S^{\times k} &amp;:= \{f^k : f \in S\}.
\end{align*}
$$</p>
<p>If $g$ is some permutation, we also write these shorthands:</p>
<p>$$
\begin{align*}
g\circ S &amp;:= \{g\circ f : f \in S\}\\
S\circ g &amp;:= \{f\circ g : f \in S\}.
\end{align*}
$$</p>
<p>Similarly, if $T := \{g_1, g_2, \ldots\}$, then we can write</p>
<p>$$
\begin{align*}
S\circ T &amp;:= \{f\circ g : f\in S, g\in T\}\\
&amp;= \{f_1\circ g_1, f_2\circ g_1, \ldots, f_1\circ g_2, \ldots\}.
\end{align*}
$$</p>
<p>With that out of the way, let’s talk about the concept of a single
“move”. What counts as a “move” in a permutation puzzle?</p>
<p>Really, we can choose any set of moves we please, so long as every
state of the puzzle is reachable through some combination of the
moves. For example, let</p>
<p>$$
C := \{F, R, U, B, L, D\},
$$</p>
<p>the basic and well understood ninety-degree clockwise moves of the
Rubik’s Cube. Indeed, $C$ itself is a fine definition of available
moves. All of the following are also valid definitions of moves:</p>
<p>$$
C\cup C^{-1},\quad C\cup C^{\times 2},\quad C^{-1},\quad C\cup C^{\times 2}\cup C^{-1},
$$</p>
<p>and so on. Perhaps surprisingly, we can take any element of $C$ and
remove it, and it would still be a valid set of moves for the Rubik’s
Cube<sup id="fnref:6"><a href="#fn:6" role="doc-noteref">6</a></sup>!</p>
<p>Which set of moves we select usually has little relevance
mathematically (they are all expressible as one another), but has
great relevance when we are synthesizing efficient move sequences, or when
we want to talk about “optimality”. For instance, consider a
counterclockwise move: $F^{-1}$. It’s natural to consider this a
single move, but if we consider our set to be $C$, then we’d have to
count it as three moves, since $F^{-1} = F\circ F\circ F = F^3$. What
about $F^2$? Is that one move or two? Speedcubers generally consider
$F^2$ to be one motion, so counting that as one move is natural, but
many computer puzzlers like the simplicity of $C\cup C^{-1}$, i.e.,
only ninety-degree turns<sup id="fnref:7"><a href="#fn:7" role="doc-noteref">7</a></sup>.</p>
<p>For the rest of this note, we’ll be in the former camp, where half-turns count as one, and we’ll denote this set of moves as:</p>
<p>$$
\bar C := C \cup C^{-1} \cup C^{\times 2}.
$$</p>
<h3 id="what-is-a-word">What is a word?</h3>
<p>After we agree on what we consider a move, we can be more specific as
to what we mean about move sequences. A <em>move sequence</em> is a possibly
empty list of moves. A move sequence can be <em>composed</em> to form the
permutation it represents. This composition operator is called
$\kappa$, and is easily defined. Let $M$ be a move set, and let $s =
[s_1, s_2, \ldots, s_n]$ be a sequence of $n$ moves with each
$s_{\bullet}$ a move from $M$. The <em>length</em> is $s$ is naturally $n$,
and its composition is defined as:</p>
<p>$$
\begin{align*}
\kappa([\,]) &amp;:= (1, 2, 3, \ldots)\\
\kappa([s_1, s_2, \ldots, s_{n-1}, s_n]) &amp;:= \kappa([s_1, s_2, \ldots, s_{n-1}])\circ s_n.
\end{align*}
$$</p>
<p>If $M$ is a move set, then the set of all move sequences (including
the empty sequence) is denoted $M^{*}$, a notation kindly borrowed
from formal language theory.</p>
<p>If we identify the elements of $M$ with symbols, then a move sequence
is called a <em>word</em>. We’ll always type symbols in $\texttt{typewriter}$
font. The moves $\{F, R, U, B, L, D\}$ have the symbols
$\{\texttt{F}, \texttt{R}, \texttt{U}, \texttt{B}, \texttt{L},
\texttt{D}\}$, an inverse $F^{-1}$ has the symbol $\texttt{F'}$, and
a square $F^2$ has the symbol $\texttt{F2}$. And we type words as
symbols joined together in <em>reverse</em> order<sup id="fnref:8"><a href="#fn:8" role="doc-noteref">8</a></sup>, so $[R^{-1},
U^2, L]$ can be represented by the word $\texttt{L U2 R'}$.</p>
<p>The distinction is subtle but important. In a computer program, a move
sequence is a list of permutations, while a word is a list of
symbols. A Rubik’s Cube solving program should take as input a
permutation, and output a word which when composed as a move sequence,
brings that permutation to identity.</p>
<p>When doing math, we often mix up all of these concepts since they have
little bearing on the correctness of an argument. Whether it’s the
permutation $F\circ R^{-1}$ or the move sequence $[F, R^{-1}]$ or the
word $\texttt{R' F}$ or otherwise, they all represent roughly
the same thing, but computers need to be explicit about which
representation is being manipulated.</p>
<p>So, in summary:</p>
<ul>
<li>A <strong>move set</strong> is a set of permutations that “count” as one move.</li>
<li>A <strong>move sequence</strong> is a list of moves from a move set.</li>
<li>The <strong>composition</strong> of a move sequence is the permutation that move sequence represents.</li>
<li>A <strong>symbol</strong> is a designator for a move in a move set.</li>
<li>A <strong>word</strong> is a sequence of symbols.</li>
</ul>
<p>Back to this brute-force thing…</p>
<h3 id="observation-3-sorting-as-solving">Observation #3: sorting as solving</h3>
<p>As silly as the example is, let’s suppose we know, for a fact, that a
Rubik’s Cube was mixed up using only six moves from $\bar C$. Since
$\bar C$ has 18 elements, without any optimization, we might have to
try $18^6$ move sequences to find a solution.</p>
<p>Instead of brute-forcing in that way, we can do another trick. Let <code>s</code>
be our scrambled permutation.</p>
<ol>
<li>
<p>Write out every combination of 3 moves into a table. The key would
be the permutation, and the value would be the word associated with
that permutation. Call this table <code>A</code>.</p>
</li>
<li>
<p>Sort <code>A</code> in ascending lexicographic order on the permutation.</p>
</li>
<li>
<p>Make a copy of <code>A</code>, call it <code>B</code>. For all <code>(perm, word)</code> in <code>B</code>,
reassign <code>perm := compose(invert(perm), s)</code>. We do this because of
Observation #1.</p>
</li>
<li>
<p>Sort <code>B</code>.</p>
</li>
<li>
<p>Call <code>x := findCommon(A, B)</code>. We do this via Observation #2.</p>
</li>
<li>
<p>Reconstruct a word equal to <code>s</code> by <code>A[x].word ++ reverse(B[x].word)</code>. We do this to recover a final result via Observation
#1.</p>
</li>
</ol>
<p>Since we have a word that brings us <em>from solved to <code>s</code></em>, we can
invert the word to bring us <em>from <code>s</code> to solved</em>.</p>
<p>By this method, we avoided visiting all $16^6$ move sequences by
instead pre-calculating two groups of $16^3$ sequences and exploring
them for an intersection. We have cut the amount of work down to its
square root.</p>
<p>If we generalize to length $n+m$ (for some splitting of $n$ and $m$),
then we can replace the work of visiting $16^{n+m}$ states with
$16^m + 16^n$ states, which is much better.</p>
<p>So we’re done? We now know that the Rubik’s Cube requires no more than
20 moves, so if we make two tables enumerating 10 moves, we should be
good?</p>
<p>Well, err, $16^{10} = 1,099,511,627,776$. Unless we have trillions of
resources to space, be it time or space, it’s still not going to work.</p>
<h3 id="more-splitting">More splitting?</h3>
<p>An enterprising computer science student, at this point, might smell
recursion. If we split once, can we split again? If we know a Rubik’s
Cube can be solved in 20 moves, can we split it into two 10 move
problems, and each of those into two 5 move problems?</p>
<p>The problem with this is that at the top layer of recursion, it’s
clear what we are solving. At lower layers, it’s no longer clear. What
<em>actually</em> is the recursive structure at play? And if we could do this
trick, couldn’t we decimate any brute-force problem of exponential
complexity (e.g., in number of moves) into one of linear?</p>
<p>That isn’t going to work, but we can be inspired by it. Let $L := \bar
C^5$ be the set of 5-move combinations from $\bar C$. The size of $L$
is going to be $621,649$ if we don’t store redundant
permutations. This is definitely possible to compute. Then our goal is
to find a decomposition of $s$ in terms of an element in $L\circ
L\circ L\circ L$. Using the same trick from Observation #1, suppose
there is a decomposition $$s = l_4\circ l_3\circ l_2\circ l_1.$$ Then
$$l_3^{-1}\circ l_4^{-1} \circ s = l_2\circ l_1.$$ So we create four
tables:</p>
<ul>
<li>$L_1 = L$,</li>
<li>$L_2 = L_1$,</li>
<li>$L_4 = L_1^{-1}$, and</li>
<li>$L_3 = L_4\circ s$.</li>
</ul>
<p>No, the $4$ before $3$ is not a typo! We put this in order to save on
computation and avoid redundant work. Now our goal is to find an
element in common between the two sets</p>
<p>$$
\begin{align*}
X &amp;= L_2 \circ L_1\\
Y &amp;= L_4 \circ L_3.
\end{align*}
$$</p>
<p>Somehow, we must do this without actually calculating all elements of
$L_i\circ L_j$. And, to add insult to injury, for <code>findCommon</code> to
work, we need to be able to go through the set in sorted order.</p>
<h3 id="iterating-through-products-with-schroeppel--shamir">Iterating through products with Schroeppel–Shamir</h3>
<p>Suppose we have two lists of positive numbers $A$ and $B$. How can we
print the elements of $\{a+b : a\in A, b\in B\}$ in numerical order
without explicitly constructing and sorting this set? Shamir and his
collaborator Schroeppel did so with the following algorithm.</p>
<ol>
<li>
<p>Sort $A$ in ascending order. Pop off the first (and therefore
smallest) element $a_1$.</p>
</li>
<li>
<p>Create a priority queue $Q$ and initialize it with $(a,b)$ with
priority $a_1 + b$ for all $b\in B$.</p>
</li>
<li>
<p>Repeat the following until $Q$ is empty:</p>
<ol>
<li>Pop $(a,b)$ off $Q$. This will form the next smallest sum, so print $a+b$.</li>
<li>Find $a'$ which immediately succeeds $a$ in our sorted list $A$.</li>
<li>Push $(a',b)$ with priority $a+b$ onto $Q$.</li>
</ol>
</li>
</ol>
<p>This algorithm will terminate, having printed each sum successively
with at most $O(\vert A\vert + \vert B\vert)$ space and almost linear
time. (The sorting and priority queue maintenance require some
logarithmic factors.)</p>
<p>With a little work, one can see why this works. In a sense it’s a
two-dimensional sorting problem, that depends on one crucial fact: If
$x \le y$ then $x+z \le y+z$. (This is to say that addition is
<em>monotonic</em>.) Given how the priority queue is constructed, it will
<em>always</em> contain the smallest sum.</p>
<p>Could we do this with permutations? If we have two lists of
permutations $A$ and $B$, and $a_1$ is the “smallest” (i.e.,
lexicographically least) permutation of $A$, and $b_1$ is the
“smallest” permutation of $B$, then it is <strong>patently not true</strong> that
$a_1\circ b_1$ is the smallest element of $A\circ B$. In symbols,</p>
<p>$$
(\min A) \circ (\min B) \neq \min (A\circ B).
$$</p>
<p>Similarly, if two permutations satisfy $a &lt; b$, then it is <strong>patently
not true</strong> that</p>
<p>$$
a\circ z &lt; b\circ z
$$</p>
<p>for a permutation $z$.</p>
<p>The monotonicity of addition is what allows us to do steps 3.2 and 3.3
so easily. If we did the same with permutations, we would no longer
have the guarantee that the minimum composition exists within the
queue.</p>
<p>This was the next hurdle Shamir cleared. Constant in the size of $A$
or $B$, Shamir found a way to solve the following problem: Given a
permutation $a\in A$ and $b\in B$, find the element $b'\in B$ such
that $a\circ b'$ immediately succeeds $a\circ b$. In other words, we
can generate, one-by-one, a sequence of $b$’s needed for step 3.2 and
3.3. With this algorithm (which we’ll describe in the next section),
our Shamir–Schroeppel algorithm for permutations becomes the
following:</p>
<p><strong>Algorithm (Walk Products)</strong>:</p>
<ol>
<li>Initialize an empty priority queue $Q$ whose elements are pairs of
permutations with priority determined by another permutation in
lexicographic ordering.</li>
<li>For each permutation $b\in B$:
<ol>
<li>With Shamir’s trick, find the $a\in B$ such that $a\circ b = \min (A\circ b)$.</li>
<li>Push $(a, b)$ onto $Q$ with priority $a\circ b$.</li>
</ol>
</li>
</ol>
<ul>
<li>(Invariant: At this point, we will certainly have $\min (A\circ B)$ in the queue.)</li>
</ul>
<ol start="3">
<li>Repeat the following until $Q$ is empty:
<ol>
<li>Pop $(a,b)$ off $Q$. This will form the next smallest $a\circ b$, so print it<sup id="fnref:9"><a href="#fn:9" role="doc-noteref">9</a></sup>.</li>
<li>With Shamir’s trick, find $a'$ such that $a'\circ b$ immediately succeeds $a\circ b$.</li>
<li>Push $(a',b)$ with priority $a'\circ b$ onto $Q$.</li>
</ol>
</li>
</ol>
<p>This algorithm will produce the elements of $A\circ B$, one-by-one in
lexicographic order.</p>
<p>What is Shamir’s trick? We need a data structure and a clever observation.</p>
<h3 id="permutation-tries">Permutation tries</h3>
<p>In order to handle sets of ordered permutations better, Shamir created
a data structure. I call it a permutation trie. A <em>permutation trie</em>
of size-$k$ permutations is a $k$-deep, $k$-ary tree, such that a
path from root-to-leaf follows the elements of a permutation. The leaf
contains data which we want to associate with the permutation.</p>
<p>For example, consider permutations of size $5$. Suppose we wanted to
associate the symbol $\texttt{p6}$ with the permutation
$(2,4,1,3,5)$. Then we would have a $5$-layer tree with a root node
$R$, such that $R[2][4][1][3][5] = \texttt{p6}$.</p>
<p>More generally, let’s associate the following symbols with the
following permutations in a permutation trie $R$:</p>
<p>$$
\begin{align*}
\texttt{p1} &amp;\leftarrow (1,2,3,4,5) &amp; \texttt{p2} &amp;\leftarrow (1,2,3,5,4) &amp; \texttt{p3} &amp;\leftarrow (1,2,4,3,5)\\
\texttt{p4} &amp;\leftarrow (1,2,5,3,4) &amp; \texttt{p5} &amp;\leftarrow (1,3,4,5,2) &amp; \texttt{p6} &amp;\leftarrow (2,4,1,3,5)\\
\texttt{p7} &amp;\leftarrow (4,1,3,2,5) &amp; \texttt{p8} &amp;\leftarrow (4,1,3,5,2) &amp; \texttt{p9} &amp;\leftarrow (5,1,2,3,4)\\
\end{align*}
$$</p>
<p>The trie would be a data structure that looks like this:</p>
<p><img src="https://www.stylewarning.com/posts/brute-force-rubiks-cube/images/perm-trie.svg" alt="An example permutatioen trie." decoding="async">
</p>
<p>Even though we don’t show them, conceptually, each node in the trie
has a full length-$5$ array, with some elements empty (i.e., there are
no children).</p>
<p>What’s good about this data structure? First and foremost, pre-order
traversal will visit the permutations in lexicographic order. We can
use this data structure to store two things at the leaves (i.e.,
$\texttt{p}n$):</p>
<ol>
<li>The actual permutation data structure representing that path, and</li>
<li>The word we used to construct that permutation.</li>
</ol>
<p>This is the data structure, and now we get to Shamir’s
insight. Suppose we have a permutation $s$ and a permutation trie $R$
(which represents a set of permutations), and we want to traverse
$s\circ R$ in lexicographic order. The naive way is to construct a new
trie, but we wish to avoid that. To explain the idea, we’ll choose a
concrete example.</p>
<p>Let’s use $R$ from above. Let $s := (3,1,4,2,5)$. (Note that $s\not\in
R$, but that’s not important.) We wish to find an $r'\in R$ such that
$s\circ r' = \min (s\circ R)$. Well, the smallest permutation would be
one such that $r'(1) = 2$, because then $s(r'(1)) = s(2) = 1$. Looking
at our trie $R$, we can see the only candidate is that associated with
$\texttt{p6}$: $(2,4,1,3,5)$, which is the minimum.</p>
<p>What about the next smallest $s\circ r''$? For ease, let’s call this
product $m$. We would want a permutatation such that $r''(1) = 4$,
because $m(1) = s(r''(1)) = s(1) = 2$. This time, there are two
candidates:</p>
<p>$$
(4,1,3,2,5)\qquad (4,1,3,5,2)
$$</p>
<p>So at least we know $m = (2, \ldots)$. To disambiguate, we need to
look at $r''(2)$. These are the same, likewise $r''(3)$, so we have no
degree of freedom at $2$ or $3$ to minimize the product. Thus $m = (2,
3, 4, \ldots)$. We have a choice at $r''(4)$, however. The best choice
is $r''(4) = 2$, because $m(4) = s(r''(4)) = s(2) = 1$, the smallest
possible choice. This disambiguates our choice of $r''$ to be
$(4,1,3,2,5)$ so that $m = (2,3,4,1,5)$.</p>
<p>We could repeat the procedure to find the next smallest product
$s\circ r'''$. What exactly is the procedure here? Well, we walked
down the tree $R$, but instead of walking down it straight, we instead
did so in a permuted order based on $s$—specifically
$s^{-1}$. Consider our normal algorithm for walking the tree<sup id="fnref:10"><a href="#fn:10" role="doc-noteref">10</a></sup> in
lexicographic order:</p>
<pre tabindex="0"><code>function walkLex(R):
  if notTree(R):
    print R
  else:
    for i from 1 to length(R):
      if R[i] exists:
        walkLex(R[i])
</code></pre><p>We can instead walk in <em>permuted</em> order, so that we produce a sequence
$[r, r'', r''', \ldots]$ such that</p>
<p>$$
s\circ r &lt; s \circ r' &lt; s \circ r''' &lt; \cdots,
$$</p>
<p>we modify our walking algorithm as so:</p>
<pre tabindex="0"><code>function walkProductLex(R, s):
  walk'(R, inverse(s))

function walk'(R, s):
  if notTree(R):
    print R
  else:
    for i from 1 to length(R):
      j = s(i)
      if R[j] exists:
        walk'(R[j], s)
</code></pre><p>Note that $s$ was inverted before the recursion to make quick permuting of each node.</p>
<p>With this, we have the remarkable ability to iterate through products
in lexicographic order, without having to enumerate them all and sort
them. This was the last and critical ingredient.</p>
<h3 id="the-4-list-algorithm-and-solving-the-rubiks-cube">The 4-List Algorithm and solving the Rubik’s Cube</h3>
<p>Now we want to put this all together to create the <em>4-List
Algorithm</em>. Let’s restate the problem in clear terms.</p>
<p><strong>Problem (4-List)</strong>: Let $s$ be a permutation. Let $L_1$, $L_2$,
$L_3$, and $L_4$ be sets of permutations such that we know $s\in
L_4\circ L_3\circ L_2\circ L_1$. Find $l_1\in L_1$, $l_2\in L_2$,
$l_3\in L_3$, and $l_4\in L_4$ such that $s = l_4\circ l_3\circ
l_2\circ l_1$.</p>
<p>Piecing together the elements above, we arrive at the 4-List Algorithm.</p>
<p><strong>Algorithm (4-List)</strong>:</p>
<ol>
<li>Construct $L'_3 := L_3^{-1}\circ s$ and $L'_4 := L_4^{-1}$.</li>
<li>Create two generators<sup id="fnref:11"><a href="#fn:11" role="doc-noteref">11</a></sup>: $X_1$ that walks $L_2\circ L_1$ in
lexicographic order, and $X_2$ that walks $L'_3\circ L'_4$ in
lexicographic order. Do this by using the <strong>Walk Products</strong>
algorithm, which itself is implemented by constructing permutation
tries and using <code>walkProductLex</code>.</li>
<li>Call <code>findCommon</code> on $X_2$ and $X_1$. This is guaranteed to find a
solution $(l_3^{-1},l_4^{-1}\circ s,l_2,l_1)$. Process the solution
to return $(l_4, l_3, l_2, l_1)$.</li>
</ol>
<p>The main difficulty of this algorithm, aside from implementing each
subroutine correctly, is plumbing the right data around.</p>
<p>Now, we can use this to solve a scrambled Rubik’s Cube $s$.</p>
<p><strong>Algorithm (Solve Cube)</strong>:</p>
<ol>
<li>Let $L = \bar C^5$, keeping a record of the words used to construct
each element of $L$. (We recommend immediately making a permutation
trie, where the leaves store the words.)</li>
<li>Apply the <strong>4-List Algorithm</strong> to the problem $s \in L\circ L\circ
L\circ L$ to produce $(l_4, l_3, l_2, l_1)$.</li>
<li>Return words $(w_4, w_3, w_2, w_1)$ associated with the
permutations $(l_4, l_3, l_2, l_1)$.</li>
</ol>
<p>Amazingly, this algorithm really works, and answers our blog post
question in the affirmative: <em>yes, the Rubik’s Cube can be
brute-forced</em>.</p>
<h2 id="example-and-source-code">Example and source code</h2>
<p>This algorithm is implemented in Common Lisp, in my computational
group theory package
<a href="https://github.com/stylewarning/cl-permutation">CL-PERMUTATION</a>. CL-PERMUTATION
already has built in support for Rubik’s Cubes as permutation
groups. Starting a new Common Lisp session, we have the following:</p>
<pre tabindex="0"><code>&gt; (ql:quickload '(:cl-permutation :cl-permutation-examples))
&gt; (in-package :cl-permutation)
&gt; (group-order (perm-examples:make-rubik-3x3))
43252003274489856000
&gt; (format t "~R" *)
forty-three quintillion two hundred fifty-two quadrillion three trillion
two hundred seventy-four billion four hundred eighty-nine million
eight hundred fifty-six thousand
NIL
</code></pre><p>The built-in Rubik’s Cube model only uses $\{F, R, U, B, L, D\}$, so
we make new generators corresponding to $\bar C$.</p>
<pre tabindex="0"><code>&gt; (defvar *c (loop :with cube := (perm-examples:make-rubik-3x3)
                   :for g :in (perm-group.generators cube)
                   :collect (perm-expt g 1)
                   :collect (perm-expt g 2)
                   :collect (perm-expt g 3)))
*C
&gt; (length *c)
18
</code></pre><p>Now we construct $\bar C^5$.</p>
<pre tabindex="0"><code>&gt; (defvar *c5 (generate-words-of-bounded-length *c 5))
*C5
&gt; (perm-tree-num-elements *c5)
621649
</code></pre><p>Note that this constructs a <code>perm-tree</code> object, which automatically
stores the words associated with each permutation generated.</p>
<p>Now let’s generate a random element of the cube group.</p>
<pre tabindex="0"><code>&gt; (defvar *s (random-group-element (perm-examples:make-rubik-3x3)))
*S
&gt; *s
#&lt;PERM 43 44 41 20 47 11 28 9 24 13 17 42 36 40 37 25 6 21 1 29 7 19 10 3 35 39 22 18 34 33 31 48 16 15 30 2 23 32 26 46 8 4 27 12 45 14 5 38&gt;
</code></pre><p>Lastly, we run the 4-list algorithm and wait.</p>
<pre tabindex="0"><code>&gt; (decompose-by-4-list *s *c5 *c5 *c5 *c5 :verbose t)
10,000,000: 52 sec @ 192,553 perms/sec; .0013% complete, eta 1114 hours 58 minutes
20,000,000: 48 sec @ 206,858 perms/sec; .0026% complete, eta 1037 hours 51 minutes
Evaluation took:
  145.094 seconds of real time
  145.097120 seconds of total run time (144.961382 user, 0.135738 system)
  [ Run times consist of 2.405 seconds GC time, and 142.693 seconds non-GC time. ]
  100.00% CPU
  421,375,385,955 processor cycles
  11,681,934,352 bytes consed

((8 11 14 2 4)
 (1 16 9 15 1)
 (7 6 18 8 15)
 (9 13 16 15 8))
</code></pre><p>We are pretty lucky this one ended in a mere 2 minutes 25 seconds! It
usually isn’t so prompt with an answer.</p>
<p>The results are printed as four words: our $l_4$, $l_3$, $l_2$, and
$l_1$. Each integer $n$ represents the 1-indexed $n$th permutation of
$\bar C$ (ordered by how it was constructed). We can create a more
traditional notation:</p>
<pre tabindex="0"><code>&gt; (defvar *solution (reduce #'append *))
*SOLUTION
&gt; (defun notation (ws)
    (dolist (w (reverse ws))
      (multiple-value-bind (move order)
          (floor (1- w) 3)
        (format t "~C~[~;2~;'~] "
                (aref "FRUBLD" move)
                order))))
NOTATION
&gt; (notation *solution)
U2 L' D L U' L' U2 D' R' U F L' U' D F R F2 L2 B2 U2
</code></pre><p>How do we know if this is correct? We need to check that the
composition of this word equals our random element, which we do by
composing the word (using something CL-PERMUTATION calls a “free-group
homomorphism”), inverting the permutation, and composing it with our
scramble to see that it brings us to an identity permutation.</p>
<pre tabindex="0"><code>&gt; (defvar *hom (free-group-&gt;perm-group-homomorphism
                (make-free-group 18)
                (generate-perm-group *c)))
*HOM
&gt; (perm-compose (perm-inverse (funcall *hom *solution)) *s)
#&lt;PERM 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48&gt;
</code></pre><p>Indeed, we found a reconstruction of our cube.</p>
<h2 id="tips-for-optimizing-the-4-list-algorithm">Tips for optimizing the 4-List Algorithm</h2>
<p>One of the most troubling aspects of implementing this algorithm is
making it fast enough. My initial implementation worked at a whopping
200 permutations per second. That’s incredibly slow, and meant that it
would take well over a century (in the worst case) for my program to
finish. Now, it works at about 190,000 permutations per second, with
an estimated worst-case search time of 2 months. (I haven’t
encountered a scrambled cube position which has taken more than 10
hours.)</p>
<p>Here are some ways I sped things up.</p>
<ol>
<li>Be economical with memory. When doing exploratory programming, it’s
desirable to tag and store everything, but each of those storages
and accesses take time.</li>
<li><em>Don’t</em> use actual arrays in the permutation trie. When I did that,
I ran out of memory. I instead opted for a sparse representation
using an “a-list” (that is, a linked list of <code>(index, value)</code>
pairs).</li>
<li>Make the permutation handling fast, like composition, equality
testing, and lexicographic ordering. I was originally using generic
arithmetic and 64-bits to represent each permutation element, and
it degraded speed.</li>
<li>Use a good priority queue implementation. You’ll be pushing and
popping hundreds of millions of elements.</li>
<li>Do some analysis and compress the permutation trie
representation. Most nodes of the trie will only contain one
value. If that’s the case, just store instead the permutation (and
whatever value associated with it) at the shallowest depth. This
will save a lot of time by avoiding a lot of needless (permuted)
recursion.</li>
</ol>
<p>If you have other tips for speeding up the algorithm, please email me!</p>
<h2 id="sample-benchmarks">Sample benchmarks</h2>
<p>In the following, we only consider the problem of solving the Rubik’s
Cube using the 4-list algorithm, assuming a solution length of 20
moves.</p>
<p>My computer is a ThinkPad 25th Anniversary Edition. It has an Intel
Core i7-7500U processor at 2.70 GHz, but boosting to 3.50 GHz. It has
32 GiB of RAM, but comfortably runs the solver with around 3–4 GiB.</p>
<p>The algorithm as implemented is able to check around 190,000 elements
per second.</p>
<p>Generating the move lists and pre-processing is a relatively fixed
cost. The lists can be generated once, but the preprocessing (i.e.,
composing the scramble with one of the lists) needs to happen each
solve. In my implementation, the initialization cost is consistently 9
seconds.</p>
<p>After initialization, the search is conducted. The run time varies
wildly, anywhere from seconds to hours.</p>
<ul>
<li>64 s, 188 billion CPU cycles, 4 GiB of allocation</li>
<li>165 s, 480 billion CPU cycles, 12 GiB of allocation</li>
<li>2210 s, 6 trillion CPU cycles, 162 GiB of allocation</li>
<li>4613 s, 13 trillion CPU cycles, 356 GiB of allocation</li>
<li>24010 s, 70 trillion CPU cycles, 2 TiB of allocation</li>
</ul>
<p>These are randomly sampled Rubik’s Cube scrambles, sorted by time.</p>
<p>In principle, with the current level of optimization, the algorithm
can take as much as 2 months to finish. I’m confident that my
implementation can be brought down a factor of 2, less confident it
can be easily brought down a factor of 50—but it wouldn’t surprise
me either way.</p>
<p>One interesting thing about this algorithm is that it seems to return
very, very quickly if the solution is 10 or fewer moves. Why? I
haven’t done a careful analysis, but I believe it is essentially
because the solution will be in $L_2\circ L_1$. The permutations $l_3$
and $l_4$ will be identity, which reduces to the problem of just
finding $s\in L_2\circ L_1$.</p>
<h2 id="conclusion">Conclusion</h2>
<p>“Meet in the middle” algorithms are old and well understood. When we
can’t brute-force an entire space, we can try splitting it in two and
try to combine them. That’s of course the spirit of the 4-List
Algorithm, but the devil is always in the details, and I hope this
blog post showed a lot of disparate facts needed to come together to
realize the algorithm.</p>
<p>I think the algorithm communicated by Shamir and his colleagues has
been remarkable but forgotten. While better algorithms exist for the
specific task of solving the Rubik’s Cube, the generality of the
4-List Algorithm ought not be understated.</p>
<h2 id="references">References</h2>
<ol>
<li>A. Fiat, S. Moses, A. Shamir, I. Shimshoni and G. Tardos, “Planning and learning in permutation groups,” 30th Annual Symposium on Foundations of Computer Science, Research Triangle Park, NC, USA, 1989, pp. 274–279, doi: 10.1109/SFCS.1989.63490. (<a href="https://ieeexplore.ieee.org/document/63490">Link</a>)</li>
<li>A. Bawden. “Shamir’s talk really was about how to solve the cube!”. Alan Bawden. From the <em>Cube Lovers</em> mailing list. 27 May 1987. (<a href="http://www.math.rwth-aachen.de/~Martin.Schoenert/Cube-Lovers/Alan_Bawden__Shamir%27s_talk_really_was_about_how_to_solve_the_cube!.html">Link</a>)</li>
</ol>
<section role="doc-endnotes">
<hr>
<ol>
<li id="fn:1" role="doc-endnote">
<p><em>The</em> Rubik’s Cube? Why not just “Rubik’s Cube”?!&nbsp;<a href="#fnref:1" role="doc-backlink">↩︎</a></p>
</li>
<li id="fn:2" role="doc-endnote">
<p><em>Iterative-deepening depth-first search</em> (IDDFS) is an interesting hybrid between breadth-first and depth-first search. Breadth-first search (BFS) can find an optimal path to a target, but requires lots of memory to keep track of nodes that have been seen. Depth-first search (DFS) uses almost no memory, but can’t guarantee finding the shortest path. IDDFS is an algorithm which tries DFS up to a maximum depth of 1, then of 2, then of 3, etc. until a path to the target is found. While we re-visit nodes in each successive increase in the maximum depth, the savings in memory and the guarantee of finding the shortest path usually make it worth it.&nbsp;<a href="#fnref:2" role="doc-backlink">↩︎</a></p>
</li>
<li id="fn:3" role="doc-endnote">
<p>A heuristic might be something like this. First, suppose we’ve built a table which maps every <em>corner</em> configuration (ignoring edges) to the number of moves needed to solve it. This problem can be brute-forced, as there are “only” $8!\cdot 3^7=88,179,340$ corner configurations. Suppose we are doing IDDFS to solve a whole Rubik’s Cube, and the algorithm is currently at a depth limit of 10. During our DFS (with a limited depth), we arrive at a position at depth 7, and want to decide if we shall continue with it. We can consult our corner configuration table: If we would require more than 3 moves to solve just the corners, then there’s no hope in continuing, since we’ll exceed our depth limit of 10. So we drop the line of search on this configuration entirely by returning from the depth-7 recursive call empty-handed.&nbsp;<a href="#fnref:3" role="doc-backlink">↩︎</a></p>
</li>
<li id="fn:4" role="doc-endnote">
<p>The centers are typically seen as immobile, and hence aren’t numbered.&nbsp;<a href="#fnref:4" role="doc-backlink">↩︎</a></p>
</li>
<li id="fn:5" role="doc-endnote">
<p>Shamir, isn’t that name familiar? Yes, he’s the ‘S’ from “RSA”, the encryption algorithm for which he and colleagues ‘R’ Rivest and ‘A’ Adleman won a Turing award.&nbsp;<a href="#fnref:5" role="doc-backlink">↩︎</a></p>
</li>
<li id="fn:6" role="doc-endnote">
<p>Formally, any subset of five elements of $C$ generates the Rubik’s Cube group.&nbsp;<a href="#fnref:6" role="doc-backlink">↩︎</a></p>
</li>
<li id="fn:7" role="doc-endnote">
<p>Rubik’s Cube enthusiasts have names for these concepts. If we measure the length of a move sequence by the number of quarter turns, we say we are measuring in the <em>quarter-turn metric</em> or <em>QTM</em>. If instead we are measuring the length of a move sequence by the number of face turns of any degree, we say we are measuring in the <em>half-turn metric</em> or <em>HTM</em>.&nbsp;<a href="#fnref:7" role="doc-backlink">↩︎</a></p>
</li>
<li id="fn:8" role="doc-endnote">
<p>Speedsolvers like to write words in last-to-first order, so they can read off the moves as they’re applied.&nbsp;<a href="#fnref:8" role="doc-backlink">↩︎</a></p>
</li>
<li id="fn:9" role="doc-endnote">
<p>A note on the phrase “print it”. We use the term “print it” to signify that the permutation has been constructed and it may be consumed. We might not literally <em>print it</em>, and instead <em>emit it</em> for use. What this means precisely depends on the programming language you’re using. In our final algorithm, we’ll actually need to explicitly construct generators, so keep that in mind.&nbsp;<a href="#fnref:9" role="doc-backlink">↩︎</a></p>
</li>
<li id="fn:10" role="doc-endnote">
<p>Again, as in the other footnote, we can see “walking” or “printing” or … as again a manifestation of a process of generating something one-by-one.&nbsp;<a href="#fnref:10" role="doc-backlink">↩︎</a></p>
</li>
<li id="fn:11" role="doc-endnote">
<p>Construct generators?! This is the third footnote dedicated to walking/printing/generating, because it’s important and sometimes difficult. Making a generator may be utterly trivial in your language (Scheme with <code>call/cc</code> or Python with <code>yield</code>), cumbersome (Common Lisp with <code>cl-cont</code>), or downright annoying. One trick we used when implementing the algorithm in Common Lisp is to keep track of where we are in the permutation trie by a permutation itself. We can always go to the next one if we can find the current one.&nbsp;<a href="#fnref:11" role="doc-backlink">↩︎</a></p>
</li>
</ol>
</section>
</article>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Reverse-engineering the 8086 processor's address and data pin circuits (115 pts)]]></title>
            <link>https://www.righto.com/2023/07/8086-pins.html</link>
            <guid>36645821</guid>
            <pubDate>Sat, 08 Jul 2023 16:17:33 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.righto.com/2023/07/8086-pins.html">https://www.righto.com/2023/07/8086-pins.html</a>, See on <a href="https://news.ycombinator.com/item?id=36645821">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="post-body-2795813864611579036" itemprop="description articleBody">


<p>The Intel 8086 microprocessor (1978) started the x86 architecture that continues to this day.
In this blog post, I'm focusing on a small part of the chip: the address and data pins that connect the chip to
external memory and I/O devices.
In many processors, this circuitry is straightforward, but it is complicated in the 8086 for two reasons.
First, Intel decided to package the 8086 as a 40-pin DIP, which didn't provide enough pins for all the functionality.
Instead, the 8086 multiplexes address, data, and status.
In other words, a pin can have multiple roles, providing an address bit at one time and a data bit at another time.</p>
<p>The second complication is that the 8086 has a 20-bit address space (due to its infamous segment registers), while the
data bus is 16 bits wide.
As will be seen, the "extra" four address bits have more impact than you might expect.
To summarize, 16 pins, called AD0-AD15, provide 16 bits of address and data.
The four remaining address pins (A16-A19) are multiplexed for use as status pins,
providing information about what the processor is doing for use by other parts of the system.
You might expect that the 8086 would thus have two types of pin circuits, but it turns out that there are four
distinct circuits, which I will discuss below.</p>
<p><a href="https://static.righto.com/images/8086-ad-pins/die-labeled.jpg"><img alt="The 8086 die under the microscope, with the main functional blocks and address pins labeled. Click this image (or any other) for a larger version." height="623" src="https://static.righto.com/images/8086-ad-pins/die-labeled-w700.jpg" title="The 8086 die under the microscope, with the main functional blocks and address pins labeled. Click this image (or any other) for a larger version." width="700"></a></p><p>The 8086 die under the microscope, with the main functional blocks and address pins labeled. Click this image (or any other) for a larger version.</p>
<p>The microscope image above shows the silicon die of the 8086.
In this image, the metal layer on top of the chip is visible, while the silicon and polysilicon underneath are obscured.
The square pads around the edge of the die are connected by tiny bond wires to the chip's 40 external pins.
The 20 address pins are labeled: Pins AD0 through AD15 function as
address and data pins. Pins A16 through A19 function as address pins and status pins.<span id="fnref:ad"><a href="#fn:ad">1</a></span>
The circuitry that controls the pins is highlighted in red.
Two internal busses are important for this discussion: the 20-bit AD bus (green) connects the AD pins to the rest of the CPU,
while the 16-bit C bus (blue) communicates with the registers.
These buses are connected through a circuit that can swap the byte order or shift the value.
(The lines on the diagram are simplified; the real wiring twists and turns to fit the layout.
Moreover, the C bus (blue) has its bits spread across the width of the register file.)</p>
<h2>Segment addressing in the 8086</h2>
<p>One goal of the 8086 design was to maintain backward compatibility with the earlier 8080 processor.<span id="fnref:compatibility"><a href="#fn:compatibility">2</a></span>
This had a major impact on the 8086's memory design, resulting in the much-hated segment registers.
The 8080 (like most of the 8-bit processors of the early 1970s) had a 16-bit address space, able to access 64K (65,536 bytes) of memory,
which was plenty at the time.
But due to the exponential growth in memory capacity described by Moore's Law, it was clear that the 8086 needed to
support much more. Intel decided on a 1-megabyte address space, requiring 20 address bits.
But Intel wanted to keep the 16-bit memory addresses used by the 8080.</p>
<p>The solution was to break memory into segments. Each segment was 64K long, so a 16-bit offset was sufficient to access memory
in a segment.
The segments were allocated in a 1-megabyte address space, with the result that you could access a megabyte of memory, but
only in 64K chunks.<span id="fnref:pointers"><a href="#fn:pointers">3</a></span>
Segment addresses were also 16 bits, but were shifted left by 4 bits (multiplied by 16) to support the 20-bit address space.</p>
<p>Thus, every memory access in the 8086 required a computation of the physical address.
The diagram below illustrates this process: the logical address consists of the segment base address and the offset within the segment.
The 16-bit segment register was shifted 4 bits and added to the 16-bit offset to yield the 20-bit physical memory address.</p>
<p><a href="https://static.righto.com/images/8086-ad-pins/physical-address-generation.jpg"><img alt="The segment register and the offset are added to create a 20-bit physical address.  From iAPX 86,88 User's Manual, page 2-13." height="260" src="https://static.righto.com/images/8086-ad-pins/physical-address-generation-w500.jpg" title="The segment register and the offset are added to create a 20-bit physical address.  From iAPX 86,88 User's Manual, page 2-13." width="500"></a></p><p>The segment register and the offset are added to create a 20-bit physical address.  From <a href="http://www.bitsavers.org/components/intel/_dataBooks/1981_iAPX_86_88_Users_Manual.pdf">iAPX 86,88 User's Manual</a>, page 2-13.</p>
<p>This address computation was not performed by the regular ALU (Arithmetic/Logic Unit), but by a separate adder that
was devoted to address computation.
The address adder is visible in the upper-left corner of the die photo.
I will discuss the address adder in more detail below.</p>
<h2>The AD bus and the C Bus</h2>
<p>The 8086 has multiple internal buses to move bits internally, but the relevant ones are the AD bus and the C bus.
The AD bus is a 20-bit bus that connects the 20 address/data pins to the internal circuitry.<span id="fnref:patent"><a href="#fn:patent">4</a></span>
A 16-bit bus called the C bus provides the connection between
the AD bus, the address adder and some of the registers.<span id="fnref:registers"><a href="#fn:registers">5</a></span>
The diagram below shows the connections.
The AD bus can be connected to the 20 address pins through latches. The low 16 pins can also be used for data input, while the upper 4 pins
can also be used for status output.
The address adder performs the 16-bit addition necessary for segment arithmetic. Its output is shifted left by four bits
(i.e. it has four 0 bits appended), producing the 20-bit result.
The inputs to the adder are provided by registers, a constant ROM that holds small constants such as +1 or -2, or the C bus.</p>
<p><a href="https://static.righto.com/images/8086-ad-pins/buses.jpg"><img alt="My reverse-engineered diagram showing how the AD bus and the C bus interact with the address pins." height="311" src="https://static.righto.com/images/8086-ad-pins/buses-w350.jpg" title="My reverse-engineered diagram showing how the AD bus and the C bus interact with the address pins." width="350"></a></p><p>My reverse-engineered diagram showing how the AD bus and the C bus interact with the address pins.</p>
<p>The shift/crossover circuit provides the interface between these two buses, handling the 20-bit to 16-bit conversion. The busses can be connected in three ways: direct, crossover, or shifted.<span id="fnref:swapping"><a href="#fn:swapping">6</a></span>
The direct mode connects the 16 bits of the C bus to the lower 16 bits of the address/data pins.
This is the standard mode for transferring data between the 8086's internal circuitry and the data pins.
The crossover mode performs the same connection but swaps the bytes. This is typically used for unaligned memory accesses, where the low memory byte corresponds to
the high register byte, or vice versa.
The shifted mode shifts the 20-bit AD bus value four positions to the right.
In this mode, the 16-bit output from the address adder goes to the 16-bit C bus.
(The shift is necessary to counteract the 4-bit shift applied to the address adder's output.)
Control circuitry selects the right operation for the shift/crossover circuit at the right time.<span id="fnref:shift"><a href="#fn:shift">7</a></span></p>
<p>Two of the registers are invisible to the programmer but play an important role in memory accesses.
The <code>IND</code> (Indirect) register specifies the memory address; it holds the 16-bit memory offset in a segment.
The <code>OPR</code> (Operand) register holds the data value.<span id="fnref:prefetch"><a href="#fn:prefetch">9</a></span>
The <code>IND</code> and <code>OPR</code> registers are not accessed directly by the programmer; the microcode for a machine instruction moves the appropriate
values to these registers prior to the write.</p>
<h2>Overview of a write cycle</h2>
<p>I hesitate to present a timing diagram, since I may scare off of my readers,
but the 8086's communication is designed around a four-step bus cycle.
The diagram below shows simplified timing for a write cycle, when the 8086 writes to memory or an I/O device.<span id="fnref:timing"><a href="#fn:timing">8</a></span>
The external bus activity is organized as four states, each one clock cycle long: T1, T2, T3, T4.
These T states are very important since they control what happens on the bus.
During T1, the 8086 outputs the address on the pins. During the T2, T3, and T4 states, the 8086 outputs the data word on the pins.
The important part for this discussion is that the pins are multiplexed depending on the T-state: the pins provide the address during T1 and data during
T2 through T4.</p>
<p><a href="https://static.righto.com/images/8086-ad-pins/write-cycle.jpg"><img alt="A typical write bus cycle consists of four T states. Based on The 8086 Family Users Manual, B-16." height="130" src="https://static.righto.com/images/8086-ad-pins/write-cycle-w700.jpg" title="A typical write bus cycle consists of four T states. Based on The 8086 Family Users Manual, B-16." width="700"></a></p><p>A typical write bus cycle consists of four T states. Based on The 8086 Family Users Manual, B-16.</p>
<p>There two undocumented T states that are important to the bus cycle.
The physical address is computed in the two clock cycles before T1 so the address will be available in T1.
I give these "invisible" T states the names TS (start) and T0.</p>
<h2>The address adder</h2>
<!--
This computation is a bit tricky because the input buses and the adder are 16 bits, but the physical address is 20 bits.
-->
<p>The operation of the address adder is a bit tricky since the 16-bit adder must generate a 20-bit physical address.
The adder has two 16-bit inputs: the B input is connected to the upper registers via the B bus, while the C input is connected to the C bus.
The segment register value is transferred over the B bus to the adder during the second half
of the TS state (that is, two clock cycles before the bus cycle becomes externally visible during T1).
Meanwhile, the address offset is transferred over the C bus to the adder, but the adder's C input shifts the value four bits to the right,
discarding the four low bits. (As will be explained later, the pin driver circuits latch these bits.)
The adder's output is shifted left four bits and transferred to the AD bus during the second half of T0. 
This produces the upper 16 bits of the 20-bit physical memory address.
This value is latched into the address output flip-flops at the start of T1, putting the computed address on the pins.
To summarize, the 20-bit address is generated by storing the 4 low-order bits during T0 and then the 16 high-order sum bits
during T1.</p>
<p>The address adder is not needed for segment arithmetic during T1 and T2.
To improve performance, the 8086 uses the adder during this idle time to increment or decrement memory addresses.
For instance, after popping a word from the stack, the stack pointer needs to be incremented by 2.
The address adder can do this increment "for free" during T1 and T2, leaving the ALU available for other operations.<span id="fnref:pipelining"><a href="#fn:pipelining">10</a></span>
Specifically, the adder updates the memory address in <code>IND</code>, incrementing it or decrementing it as appropriate.
First, the <code>IND</code> value is transferred over the B bus to the adder during the second half of T1.
Meanwhile, a constant (-3 to +2) is loaded from the Constant ROM and transferred to the adder's C input.
The output from the adder is transferred to the AD bus during the second half of T2.
As before, the output is shifted four bits to the left. However, the shift/crossover circuit between the AD bus and the C bus
is configured to shift four bits to the right, canceling the adder's shift.
The result is that the C bus gets the 16-bit sum from the adder, and this value is stored in the <code>IND</code> register.<span id="fnref:predecrement"><a href="#fn:predecrement">11</a></span>
For more information on the implemenation of the address adder, see my <a href="https://www.righto.com/2020/08/reverse-engineering-adder-inside-intel.html">previous blog post</a>.</p>
<!-- 
The use of the address pins is closely tied to the 8086's external timing.
The diagram below shows how a typical bus cycle is divided into four "T" states, each one corresponding to one clock cycle.
During T1, the CPU puts the memory address on the bus using the address pins.
During T3 and T4, the CPU writes to memory by putting the data value on the data pins.
Alternatively, the CPU reads from memory by reading the data value during T3 and T4.
State T2 acts a buffer period to ensure that memory and the CPU don't try to write to the bus at the same time.

![A typical bus cycle consists of four T states. Diagram from The 8086 Family Users Manual, figure 4-5.](bus-cycle.jpg "w500")
-->

<h2>The pin driver circuit</h2>
<p>Now I'll dive down to the hardware implementation of an output pin.
When the 8086 chip communicates with the outside world, it needs to provide relatively high currents.
The tiny logic transistors can't provide enough current, so the chip needs to use large output transistors.
To fit the large output transistors on the die, they are constructed of multiple wide transistors in parallel.<span id="fnref:ratio"><a href="#fn:ratio">12</a></span>
Moreover, the drivers use a somewhat unusual "superbuffer" circuit with two transistors: one to pull the output high, and one to pull the output low.<span id="fnref:superbuffer"><a href="#fn:superbuffer">13</a></span></p>
<p>The diagram below shows the transistor structure for one of the output pins (AD10), consisting of three
parallel transistors between the output and +5V, and five parallel transistors between the output and ground.
The die photo on the
left shows the metal layer on top of the die. This shows the power and ground wiring and the connections to
the transistors.
The photo on the right shows the die with the metal layer removed, showing the underlying silicon and the
polysilicon wiring on top.
A transistor gate is formed where a polysilicon wire crosses the doped silicon region. 
Combined, the +5V transistors are equivalent to about 60 typical transistors, while the ground transistors are
equivalent to about 100 typical transistors.
Thus, these transistors provide substantially more current to the output pin.</p>
<p><a href="https://static.righto.com/images/8086-ad-pins/output-transistor.jpg"><img alt="Two views of the output transistors for a pin. The first shows the metal layer, while the second shows the polysilicon and silicon." height="402" src="https://static.righto.com/images/8086-ad-pins/output-transistor-w800.jpg" title="Two views of the output transistors for a pin. The first shows the metal layer, while the second shows the polysilicon and silicon." width="800"></a></p><p>Two views of the output transistors for a pin. The first shows the metal layer, while the second shows the polysilicon and silicon.</p>
<h3>Tri-state output driver</h3>
<p>The output circuit for an address pin uses a tri-state buffer, which allows the output to be disabled
by putting it into a high-impedance "tri-state" configuration.
In this state, the output is not pulled high or low but is left floating.
This capability allows the pin to be used for data input.
It also allows external devices to device can take control of the bus, for instance, to perform
DMA (direct memory access).</p>
<p>The pin is driven by two large MOSFETs, one to pull the output high and one to pull it low.
(As described earlier, each large MOSFET is physically multiple transistors in parallel, but I'll ignore that for now.)
If both MOSFETs are off, the output floats, neither on nor off.</p>
<p><a href="https://static.righto.com/images/8086-ad-pins/output-circuit.jpg"><img alt="Schematic diagram of a &quot;typical&quot; address output pin." height="230" src="https://static.righto.com/images/8086-ad-pins/output-circuit-w400.jpg" title="Schematic diagram of a &quot;typical&quot; address output pin." width="400"></a></p><p>Schematic diagram of a "typical" address output pin.</p>
<p>The tri-state output is implemented by driving the MOSFETs with two "superbuffer"<span id="fnref:and"><a href="#fn:and">15</a></span> AND gates.
If the <code>enable</code> input is low, both AND gates produce a low output and both output transistors are off.
On the other hand, if <code>enable</code> is high, one AND gate will be on and one will be off.
The desired output value is loaded into a flip-flop to hold it,<span id="fnref:clock"><a href="#fn:clock">14</a></span>
and the flip-flop turns one of the output transistors on, driving the output pin high or low as appropriate.
(Conveniently, the flip-flop provides the data output Q and the inverted data output <span>Q</span>.)
Generally, the address pin outputs are enabled for T1-T4 of a write but only during T1 for a read.<span id="fnref:enable"><a href="#fn:enable">16</a></span></p>
<p>In the remainder of the discussion, I'll use the tri-state buffer symbol below, rather than showing the implementation of the buffer.</p>
<p><a href="https://static.righto.com/images/8086-ad-pins/output-simplified.jpg"><img alt="The output circuit, expressed with a tri-state buffer symbol." height="137" src="https://static.righto.com/images/8086-ad-pins/output-simplified-w350.jpg" title="The output circuit, expressed with a tri-state buffer symbol." width="350"></a></p><p>The output circuit, expressed with a tri-state buffer symbol.</p>
<h3>AD4-AD15</h3>
<p>Pins AD4-AD15 are "typical" pins, avoiding the special behavior of the top and bottom pins, so I'll discuss them first.
The behavior of these pins is that the value on the AD bus is latched by the circuit and then put on the output pin
under the control of the <code>enaable</code> signal.
The circuit has three parts: a multiplexer to select the output value, a flip-flop to hold the output value, and a tri-state driver to
provide the high-current output to the pin.
In more detail, the multiplexer selects either the value on the AD bus or the current output from the flip-flop.
That is, the multiplexer can either load a new value into the flip-flop or hold the existing value.<span id="fnref:implementation"><a href="#fn:implementation">17</a></span>
The flip-flop latches the input value on the falling edge of the clock, passing it to the output driver.
If the enable line is high, the output driver puts this value on the corresponding address pin.</p>
<!-- datasheet: output low tested at 2.0mA, output high tested at -400 microamps -->

<p><a href="https://static.righto.com/images/8086-ad-pins/ad415.jpg"><img alt="The output circuit for AD4-AD15 has a latch to hold the desired output value, an address or data bit." height="129" src="https://static.righto.com/images/8086-ad-pins/ad415-w400.jpg" title="The output circuit for AD4-AD15 has a latch to hold the desired output value, an address or data bit." width="400"></a></p><p>The output circuit for AD4-AD15 has a latch to hold the desired output value, an address or data bit.</p>
<p>For a write, the circuit latches the address value on the bus during the second half of T0 and puts it on the pins during T1.
During the second half of the T1 state, the data word is transferred from the <code>OPR</code> register over the C bus to the AD bus and loaded
into the AD pin latches.
The word is transferred from the latches to the pins during T2 and held for the remainder of the bus cycle.</p>
<h3>AD0-AD3</h3>
<p>The four low address bits have a more complex circuit because these address bits are latched from the bus before the address adder computes its sum, as described earlier.
The memory offset (before the segment addition) will be on the C bus during the second half of TS and is loaded into the lower
flip-flop. This flip-flop delays these bits for one clock cycle and then they are loaded into the upper flip-flop.
Thus, these four pins pick up the offset prior to the addition, while the other pins get the result of the segment addition.</p>
<p><a href="https://static.righto.com/images/8086-ad-pins/ad03.jpg"><img alt="The output circuit for AD0-AD3 has a second latch to hold the low address bits before the address adder computes the sum." height="174" src="https://static.righto.com/images/8086-ad-pins/ad03-w500.jpg" title="The output circuit for AD0-AD3 has a second latch to hold the low address bits before the address adder computes the sum." width="500"></a></p><p>The output circuit for AD0-AD3 has a second latch to hold the low address bits before the address adder computes the sum.</p>
<p>For data, the AD0-AD3 pins transfer data directly from the AD bus to the pin latch, bypassing the delay that was used to get the address bits.
That is, the AD0-AD3 pins have two paths: the delayed path used for addresses during T0 and the direct path otherwise used for data.
Thus, the multiplexer has three inputs: two for these two paths and a third loop-back input to hold the flip-flop value.</p>
<!--
ad-latch-load loads the  AD0-15 latches.
-->

<!--
If the memory access was an instruction fetch,
the address adder is immediately reused to update the instruction pointer (program counter).
In the second half of T1, one input of the address adder is loaded with the instruction pointer increment from the constant ROM (2 if a word was fetched).
This value is added to the instruction pointer value and
the updated instruction pointer value is written back in the second half of T2.
A similar process is used for other memory accesses that update a pointer, such as stack operations or string operations.

If another bus cycle follows, the T3 and T4 states act like the T0 and T1 states described above, preparing the next memory address.
Thus, address calculation is pipelined in the 8086: the address adder performs the segment computation during the last half of the previous bus cycle, so the physical memory address will be ready at the start of the bus cycle.

For a memory write, the address latches are reloaded during T1, loading them with the `OPR` register???
-->

<h3>A16-A19: status outputs</h3>
<p>The top four pins (A16-A19) are treated specially, since they are not used for data.
Instead, they provide processor status during T2-T4.<span id="fnref:status"><a href="#fn:status">18</a></span> The pin latches for these
pins are loaded with the address during T0 like the other pins, but loaded with status instead of data during T1.
The multiplexer at the input to the latch selects the address bit during T0 and the status bit during T1, and
holds the value otherwise.
The schematic below shows how this is implemented for A16, A17, and A19.</p>
<p><a href="https://static.righto.com/images/8086-ad-pins/ad1619.jpg"><img alt="The output circuit for AD16, AD17, and AD19 selects either an address output or a status output." height="115" src="https://static.righto.com/images/8086-ad-pins/ad1619-w400.jpg" title="The output circuit for AD16, AD17, and AD19 selects either an address output or a status output." width="400"></a></p><p>The output circuit for AD16, AD17, and AD19 selects either an address output or a status output.</p>
<p>Address pin A18 is different because it indicates the current status of the interrupt enable flag bit.
This status is updated every clock cycle, unlike the other pins.
To implement this, the pin has a different circuit that isn't latched,
so the status can be updated continuously.
The clocked transistors act as "pass transistors", passing the signal through when active.
When a pass transistor is turned off, the following logic gate holds the previous value due to the capacitance of the
wiring.
Thus, the pass transistors provide a way of holding the value through the clock cycle.
The flip-flops are implemented with pass transistors internally, so in a sense the circuit below is a flip-flop
that has been "exploded" to provide a second path for the interrupt status.</p>
<p><a href="https://static.righto.com/images/8086-ad-pins/ad18.jpg"><img alt="The output circuit for AD18 is different from the rest so the I flag status can be updated every clock cycle." height="162" src="https://static.righto.com/images/8086-ad-pins/ad18-w540.jpg" title="The output circuit for AD18 is different from the rest so the I flag status can be updated every clock cycle." width="540"></a></p><p>The output circuit for AD18 is different from the rest so the I flag status can be updated every clock cycle.</p>
<h2>Reads</h2>
<p>A memory or I/O read also uses a 4-state bus cycle, slightly different from the write cycle.
During T1, the address is provided on the pins, the same as for a write.
After that, however, the output circuits are tri-stated so they float, allowing the external memory to put data on the bus.
The read data on the pin is put on the AD bus at the start of the T4 state.
From there, the data passes through the crossover circuit to the C bus. Normally the 16 data bits pass straight through to
the C bus, but the bytes will be swapped if the memory access is unaligned.
From the C bus, the data is written to the <code>OPR</code> register, a byte or a word as appropriate.
(For an instruction prefetch, the word is written to a prefetch queue register instead.)</p>
<p><a href="https://static.righto.com/images/8086-ad-pins/read-cycle.jpg"><img alt="A typical read bus cycle consists of four T states. Based on The 8086 Family Users Manual, B-16." height="139" src="https://static.righto.com/images/8086-ad-pins/read-cycle-w600.jpg" title="A typical read bus cycle consists of four T states. Based on The 8086 Family Users Manual, B-16." width="600"></a></p><p>A typical read bus cycle consists of four T states. Based on The 8086 Family Users Manual, B-16.</p>
<p>To support data input on the AD0-AD15 pins, they have a circuit to buffer the input data and transfer it to the AD bus.
The incoming data bit is buffered by the two inverters and sampled when the clock is high.
If the enable' signal is low, the data bit is transferred to the AD bus when the clock is low.<span id="fnref:read-enable"><a href="#fn:read-enable">19</a></span>
The two MOSFETs act as a "superbuffer", providing enough current for the fairly long AD bus.
I'm not sure what the capacitor accomplishes, maybe avoiding a race condition if the data pin changes just as the clock goes low.<span id="fnref:race"><a href="#fn:race">20</a></span></p>
<p><a href="https://static.righto.com/images/8086-ad-pins/read-circuit.jpg"><img alt="Schematic of the input circuit for the data pins." height="134" src="https://static.righto.com/images/8086-ad-pins/read-circuit-w500.jpg" title="Schematic of the input circuit for the data pins." width="500"></a></p><p>Schematic of the input circuit for the data pins.</p>
<p>This circuit has a second role, precharging the AD bus high when the clock is low, if there's no data.
Precharging a bus is fairly common in the 8086 (and other NMOS processors) because NMOS transistors are better at pulling a
line low than pulling it high. Thus, it's often faster to precharge a line high before it's needed and then pull it low for a 0.<span id="fnref:adder"><a href="#fn:adder">21</a></span></p>
<p>Since pins A16-A19 are not used for data, they operate the same for reads as for writes: providing address bits and then status.</p>
<h2>The pin circuit on the die</h2>
<p>The diagram below shows how the pin circuitry appears on the die. The metal wiring has been removed to show the silicon and polysilicon.
The top half of the image is the input circuitry, reading a data bit from the pin and feeding it to the AD bus.
The lower half of the image is the output circuitry, reading an address or data bit from the AD bus and amplifying it for output
via the pad.
The light gray regions are doped, conductive silicon. The thin tan lines are polysilicon, which forms transistor gates where it crosses doped silicon.</p>
<p><a href="https://static.righto.com/images/8086-ad-pins/pin-labeled.jpg"><img alt="The input/output circuitry for an address/data pin. The metal layer has been removed to show the underlying silicon and polysilicon. Some crystals have formed where the bond pad was." height="482" src="https://static.righto.com/images/8086-ad-pins/pin-labeled-w600.jpg" title="The input/output circuitry for an address/data pin. The metal layer has been removed to show the underlying silicon and polysilicon. Some crystals have formed where the bond pad was." width="600"></a></p><p>The input/output circuitry for an address/data pin. The metal layer has been removed to show the underlying silicon and polysilicon. Some crystals have formed where the bond pad was.</p>
<h2>A historical look at pins and timing</h2>
<p>The number of pins on Intel chips has grown exponentially, more than a factor of 100 in 50 years.
In the early days, Intel management was convinced that a 16-pin package was large enough for any integrated circuit.
As a result, the Intel 4004 processor (1971) was crammed into a 16-pin package.
Intel chip designer Federico Faggin<span id="fnref:faggin"><a href="#fn:faggin">22</a></span> describes 16-pin packages as a completely silly requirement that was throwing away
performance,
but the "God-given 16 pins" was like a religion at Intel.
When Intel was forced to use 18 pins by the 1103 memory chip, it "was like the sky had dropped from heaven"
and he had "never seen so many long faces at Intel."
Although the 8008 processor (1972) was able to use 18 pins, this low pin count still harmed performance by forcing pins to be used for multiple
purposes.</p>
<p>The Intel 8080 (1974) had a larger, 40-pin package that allowed it to have 16 address pins and 8 data pins.
Intel stuck with this size for the 8086, even though competitors used larger packages with more pins.<span id="fnref:ti"><a href="#fn:ti">23</a></span>
As processors became more complex, the 40-pin package became infeasible and the pin count rapidly expanded;
The 80286 processor (1982) had a 68-pin package, while the
i386 (1985) had 132 pins; the i386 needed many more pins because it had a 32-bit data bus and a 24- or 32-bit address bus.
The i486 (1989) went to 196 pins while the original Pentium had 273 pins.
Nowadays, a modern <a href="https://www.intel.com/content/www/us/en/products/sku/232167/intel-core-i913900ks-processor-36m-cache-up-to-6-00-ghz/specifications.html">Core I9 processor</a> uses the <a href="https://en.wikipedia.org/wiki/LGA_1700">FCLGA1700</a> socket with a whopping 1700 contacts.</p>
<p>Looking at the history of Intel's bus timing, the 8086's complicated memory timing goes back to the Intel 8008 processor (1972). Instruction execution in the 8008 went through
a specific sequence of timing states; each clock cycle was assigned a particular state number.
Memory accesses took three cycles:
the address was sent to memory during states T1 and T2, half of the address at a time since there were only 8 address pins.
During state T3, a data byte was either transmitted to memory or read from memory.
Instruction execution took place during T4 and T5.
State signals from the 8008 chip indicated which state it was in.</p>
<!-- http://www.bitsavers.org/components/intel/MCS8/Intel_8008_8-Bit_Parallel_Central_Processing_Unit_Rev1_Apr72.pdf -->

<p>The 8080 used an even more complicated timing system.
An instruction consisted of one to five "machine cycles", numbered M1 through M5, where each machine cycle corresponded to
a memory or I/O access. Each machine cycle consisted of three to five states, T1 through T5, similar to the 8008 states.
The 8080 had 10 different types of machine cycle such as instruction fetch, memory read, memory write, stack read or write,
or I/O read or write. The status bits indicated the type of machine cycle.
The 8086 kept the T1 through T4 memory cycle. Because the 8086 decoupled instruction prefetching from execution, it no
longer had explicit M machine cycles. Instead, it used status bits to indicate 8 types of bus activity such as instruction
fetch, read data, or write I/O.</p>
<h2>Conclusions</h2>
<p>Well, address pins is another subject that I thought would be straightforward to explain but turned out to be surprisingly
complicated.
Many of the 8086's design decisions combine in the address pins: segmented addressing, backward compatibility, and the small 40-pin package.
Moreover, because memory accesses are critical to performance, Intel put a lot of effort into this circuitry.
Thus, the pin circuitry is tuned for particular purposes, especially pin A18 which is different from all the rest.</p>
<p>There is a lot more to say about memory accesses and how the 8086's Bus Interface Unit performs them.
The process is very complicated, with interacting state machines for memory operation and instruction prefetches, as well
as handling unaligned memory accesses.
I plan to write more, so 
follow me on Twitter <a href="https://twitter.com/kenshirriff">@kenshirriff</a> or <a href="https://www.righto.com/feeds/posts/default">RSS</a> for updates.
I've also started experimenting with Mastodon recently as <a href="https://oldbytes.space/@kenshirriff">@<span data-cfemail="eb808e859883829999828d8dab84878f89929f8e98c5989b8a888e">[email&nbsp;protected]</span></a>
and Bluesky as <a href="https://staging.bsky.app/profile/righto.com">@righto.com</a> so you can follow me there too.</p>
<h2>Notes and references</h2>


</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Langchain Is Pointless (319 pts)]]></title>
            <link>https://old.reddit.com/r/LangChain/comments/13fcw36/langchain_is_pointless/</link>
            <guid>36645575</guid>
            <pubDate>Sat, 08 Jul 2023 15:56:19 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://old.reddit.com/r/LangChain/comments/13fcw36/langchain_is_pointless/">https://old.reddit.com/r/LangChain/comments/13fcw36/langchain_is_pointless/</a>, See on <a href="https://news.ycombinator.com/item?id=36645575">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>It's filled with crap like this:</p>

<pre><code>    for i in range(n_results, 0, -1):
        try:
            return self._collection.query(
                query_texts=query_texts,
                query_embeddings=query_embeddings,
                n_results=i,
                where=where,
                **kwargs,
            )
</code></pre>

<p>and this:</p>

<pre><code>def embed_documents(self, texts: List[str]) -&gt; List[List[float]]:
    texts = list(map(lambda x: x.replace("\n", " "), texts))
    embeddings = self.client.encode(texts, **self.encode_kwargs)
    return embeddings.tolist()
</code></pre>

<p>and this:</p>

<pre><code>class CharacterTextSplitter(TextSplitter):
    """Implementation of splitting text that looks at characters."""

def __init__(self, separator: str = "\n\n", **kwargs: Any):
    """Create a new TextSplitter."""
    super().__init__(**kwargs)
    self._separator = separator

def split_text(self, text: str) -&gt; List[str]:
    """Split incoming text and return chunks."""
    # First we naively split the large input into a bunch of smaller ones.
    if self._separator:
        splits = text.split(self._separator)
    else:
        splits = list(text)
</code></pre>

<p>In short: <a href="https://i.imgur.com/OffEJTR.gifv">https://i.imgur.com/OffEJTR.gifv</a></p>

<p>Embeddings is just a do-nothing wrapper for SentenceTransformers. Chroma is just a do-nothing wrapper for ChromaDB. It's filled with "helper" functions that just call normal Python functions. A dedicated TextSplitter that calls split() from builtins.py? What? Why? Templates are no more useful than calling .replace() on a string. "texts" are just strings and "documents" are just a pointless dict that contain "texts." Just load the strings from your datasource yourself. The README is both grandiose and vague. The documentation is out-of-date and inconsistent. The import footprint is weirdly massive--highly modularized but nothing seems to do anything that'd take more than a few CPU cycles. There's not really a standard interoperable datatype, so you're actually led further afield than if you had just clearly defined the simple lists and strings required for hitting an LLM. </p>

<p>The very concept of chaining operations when interacting with LLMs doesn't really make sense to me: it's basically one <code>requests</code> call to a generation backend, but it's not like it even handles websockets and streaming for you. Why chain together wrapper classes when you can just do the operations yourself?</p>

<p>This seems like a beginner's project that blew up because it's riding a tidal wave of interest in the broader topic.</p>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Flickr Foundation is building a new bridge between Flickr and Wikimedia Commons (133 pts)]]></title>
            <link>https://diff.wikimedia.org/2023/07/07/flickr-foundation-is-building-a-new-bridge-between-flickr-and-wikimedia-commons/</link>
            <guid>36645448</guid>
            <pubDate>Sat, 08 Jul 2023 15:42:20 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://diff.wikimedia.org/2023/07/07/flickr-foundation-is-building-a-new-bridge-between-flickr-and-wikimedia-commons/">https://diff.wikimedia.org/2023/07/07/flickr-foundation-is-building-a-new-bridge-between-flickr-and-wikimedia-commons/</a>, See on <a href="https://news.ycombinator.com/item?id=36645448">Hacker News</a></p>
<div id="readability-page-1" class="page"><article id="post-97236">
	<div>
			


<p>We are pleased to announce a new partnership with the <a href="https://www.flickr.org/">Flickr Foundation</a> to extend the great work already done via the <a href="https://commons.wikimedia.org/wiki/Commons:Flickr2Commons">Flickr2Commons</a> tool to make it even easier to upload CC-licensed images from Flickr into Wikimedia Commons.&nbsp;</p>


<div>
<figure><a href="https://diff.wikimedia.org/?attachment_id=97298"><img decoding="async" loading="lazy" src="https://diff.wikimedia.org/wp-content/uploads/2023/07/Flickr_Foundation_Colour_Logo_Compact_Version.svg_.png?resize=256%2C231" alt="" width="256" height="231" srcset="https://diff.wikimedia.org/wp-content/uploads/2023/07/Flickr_Foundation_Colour_Logo_Compact_Version.svg_.png?resize=256%2C231?w=1024 1024w, https://diff.wikimedia.org/wp-content/uploads/2023/07/Flickr_Foundation_Colour_Logo_Compact_Version.svg_.png?resize=256%2C231?w=300 300w, https://diff.wikimedia.org/wp-content/uploads/2023/07/Flickr_Foundation_Colour_Logo_Compact_Version.svg_.png?resize=256%2C231?w=768 768w" sizes="(max-width: 256px) 100vw, 256px" data-recalc-dims="1"></a></figure></div>


<p>Wikipedia is a foundational source of information on the internet. It provides content to Google and other search engines, social media platforms, voice assistants, and, increasingly, AI applications. To illustrate that information, we have Wikimedia Commons, the central visual platform for Wikipedia and one of the primary sources for open licensed visual content online. You may not know that one of the largest sources for Wikimedia Commons is Flickr.&nbsp;&nbsp;</p>



<p>Since 2004, Flickr has been one of the most popular platforms for photographers and amateurs to upload photographs, videos, illustrations, and more online. It is also one of the largest online repositories of Creative Commons-licensed content. Flickr members can assign a license to their uploads, including those Creative Commons licenses accepted on Wikimedia Commons: Attribution (CC-BY), Attribution-ShareAlike (CC-BY-SA), Public Domain Dedication (CC0), and the Public Domain Mark.</p>





<p>In 2008, Flickr launched the <a href="https://www.flickr.com/commons">Flickr Commons</a> program, to increase public access to photography collections held at libraries, museums, and archives around the world. Images in Flickr Commons are shared with something a bit different from a license. It’s an <em>assertion</em> called&nbsp; “no known copyright restrictions.”&nbsp; The program supports <a href="https://www.flickr.com/commons/institutions">over 100 member institutions</a>, including <a href="https://www.flickr.com/photos/usnationalarchives/">The U.S. National Archives</a>, <a href="https://www.flickr.com/photos/nasacommons/">NASA on Commons</a>, the <a href="https://www.flickr.com/photos/nlscotland/">National Library of Scotland</a>, and <a href="https://www.flickr.com/photos/reykjavikmuseumofphotography/">Ljósmyndasafn Reykjavíkur</a>.</p>



<p>In 2022, the <a href="https://www.flickr.org/">Flickr Foundation</a> was established. It’s a US 501(c)(3) non-profit organization with the objective of safeguarding Flickr and its tens of billions of photos for the future. It seeks to develop and sustain “…an accessible social and technical infrastructure to protect [this] invaluable collection.”&nbsp;<br>This bridge between Flickr and Wikimedia Commons—which we’ve started calling “<strong>Flickypedia</strong>”—is one of the flagship projects of the Flickr Foundation. Building in partnership with the Wikimedia Foundation, and supported by the Culture and Heritage team, we will be building on the utility of the <a href="https://commons.wikimedia.org/wiki/Commons:Flickr2Commons">Flickr2Commons</a> tool, extending it, and then tending it for the long term.</p>



<p>This project has been mentioned in the 2023-2024 <a href="https://meta.wikimedia.org/wiki/Wikimedia_Foundation_Annual_Plan/2023-2024/Goals/Equity">Wikimedia Foundation Annual Plan</a> under the Equity / Culture &amp; Heritage section, in <a href="https://web.archive.org/web/20230602045536/https://mailchi.mp/c6a1d40b1748/new-news-for-you-about-flickrorg">this mailing list update</a> from Flickr Foundation, and <a href="https://commons.wikimedia.org/wiki/Commons:Village_pump/Archive/2023/06#Flickr_Foundation_adopts_Flickr2Commons">in the ensuing discussion</a> on Wikimedia Commons’s Village Pump.</p>



<h2>About Flickr2Commons</h2>



<p>Flickr2Commons is a popular tool used by Wikimedia Commons contributors to upload single or multiple files from Flickr into Wikimedia Commons. It was created by <a href="https://en.wikipedia.org/wiki/Magnus_Manske">Magnus Manske</a>, and first launched in 2013, ten years ago! The tool allows for user authentication, checks for the required licenses, includes a metadata editing step, and then file transfer.&nbsp;</p>



<h2>Metrics important for Flickypedia</h2>



<p>In order to gauge the possible reach of Flickypedia, we wanted to understand Flickr2Commons metrics. Magnus helped pull together the stats to show that roughly 5.4M files have been uploaded by about 2K users since launch. Using the <a href="https://hashtags.wmcloud.org/">Wikimedia Hashtags</a> tool, we can also see how much Flickr2Commons is used today. In June 2023 only, for example, 71,689 files were uploaded by 147 users.</p>


<div>
<figure><a href="https://diff.wikimedia.org/?attachment_id=97279"><img decoding="async" loading="lazy" width="1024" height="427" src="https://diff.wikimedia.org/wp-content/uploads/2023/07/edits_over_time-1.jpg?resize=1024%2C427" alt="" srcset="https://diff.wikimedia.org/wp-content/uploads/2023/07/edits_over_time-1.jpg?w=1920 1920w, https://diff.wikimedia.org/wp-content/uploads/2023/07/edits_over_time-1.jpg?w=300 300w, https://diff.wikimedia.org/wp-content/uploads/2023/07/edits_over_time-1.jpg?w=768 768w, https://diff.wikimedia.org/wp-content/uploads/2023/07/edits_over_time-1.jpg?w=1024 1024w, https://diff.wikimedia.org/wp-content/uploads/2023/07/edits_over_time-1.jpg?w=1536 1536w" sizes="(max-width: 1024px) 100vw, 1024px" data-recalc-dims="1"></a><figcaption>Number of edits (ie. uploads), with the Flickr2Commons tool in June 2023 (<a href="https://hashtags.wmcloud.org/graph/?query=flickr2commons&amp;project=&amp;startdate=2023-06-01&amp;enddate=2023-06-30&amp;search_type=or&amp;user=">Hashtags tool</a>)</figcaption></figure></div>


<p>We were also able to discover the most active users of Flickr2Commons in the last six months, from January to June 2023.</p>


<div>
<figure><a href="https://diff.wikimedia.org/?attachment_id=97281"><img decoding="async" loading="lazy" width="1024" height="1024" src="https://diff.wikimedia.org/wp-content/uploads/2023/07/top_users-2.jpg?resize=1024%2C1024" alt="" srcset="https://diff.wikimedia.org/wp-content/uploads/2023/07/top_users-2.jpg?w=1254 1254w, https://diff.wikimedia.org/wp-content/uploads/2023/07/top_users-2.jpg?w=150 150w, https://diff.wikimedia.org/wp-content/uploads/2023/07/top_users-2.jpg?w=300 300w, https://diff.wikimedia.org/wp-content/uploads/2023/07/top_users-2.jpg?w=768 768w, https://diff.wikimedia.org/wp-content/uploads/2023/07/top_users-2.jpg?w=1024 1024w" sizes="(max-width: 1024px) 100vw, 1024px" data-recalc-dims="1"></a><figcaption>Most active users of Flickr2Commons January to June 2023 (<a href="https://hashtags.wmcloud.org/graph/?query=flickr2commons&amp;project=&amp;startdate=2023-01-01&amp;enddate=2023-06-30&amp;search_type=or&amp;user=">Hashtags</a>)</figcaption></figure></div>


<p>It’s been great to collate all these usage statistics for Flickr2Commons—both the more recent numbers, but also in total over the last 10 years. Seeing it all together gives us a clear target for the new version to try to match.&nbsp;</p>



<p>It is also worth noting that another tool connects Flickr to Wikimedia Commons, called the UploadWizard. We’re bearing in mind that this means there will have been even more images from Flickr through that tool. Preparing these metrics has given us ideas on how we might make it even simpler to count into the future using Flickypedia.</p>



<h2>Our timeline</h2>



<p>The Flickypedia partnership project officially started in June 2023. We plan to spend the next six months or so building our Alpha (hopefully to show in October) and then Version 1.0 (hopefully December). Please <a href="https://commons.wikimedia.org/wiki/Commons_talk:Flickypedia">stay in touch</a> if you’d like to be involved in testing or have feedback about Flickr2Commons we should know about.</p>



<ul>
<li>For the July-December plan, please <a href="https://commons.wikimedia.org/wiki/Commons:Flickypedia">visit the project page</a> on Wikimedia Commons.</li>



<li>For feedback, please contact us via the <a href="https://commons.wikimedia.org/wiki/Commons_talk:Flickypedia">talk page</a> on Wikimedia Commons.</li>
</ul>
				<div id="translate-post">
					<p><img src="https://diff.wikimedia.org/wp-content/themes/interconnection/assets/images/translate-post.jpg" alt="">
					</p>

					<div>
						<h2>Can you help us translate this article?</h2>

						<p>In order for this article to reach as many people as possible we would like your help. Can you translate this article to get the message out?</p>

													<p><a href="https://diff.wikimedia.org/wp-login.php?redirect_to=%2F2023%2F07%2F07%2Fflickr-foundation-is-building-a-new-bridge-between-flickr-and-wikimedia-commons%2F%23translate-post">Start translation</a>
												</p></div>
				</div>
				
		</div>

	<!-- .entry-footer -->

</article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Superrational (132 pts)]]></title>
            <link>https://skunkledger.substack.com/p/superrational</link>
            <guid>36645240</guid>
            <pubDate>Sat, 08 Jul 2023 15:19:37 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://skunkledger.substack.com/p/superrational">https://skunkledger.substack.com/p/superrational</a>, See on <a href="https://news.ycombinator.com/item?id=36645240">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><article><div dir="auto"><p>“Will Vicktoreya Keyneslee Beauxpensees please come see the principal right now?” said the voice over the intercom.</p><p>The voice didn’t actually tell me where to go, but I met the principal at his office anyway, because I knew that was the Schelling point for where we could find each other. I pushed my luxuriant raven hair behind my ear demurely and used my delicate, doll-like hand to knock on the door.</p><p>During the three seconds I spent waiting for it to open, I thought sadly about how I’d never known my parents. They’d died when I was a baby, gored in a stag hunt. Instead, I’d been raised by a series of mysterious guardians. But everybody always sensed there was something different about me. While the other children played their foolish games, I would twirl my luxuriant raven hair and draw Laffer curves in sidewalk chalk. Even though people were constantly falling in love with me, I kept to myself because I was so absorbed in my daydreams of a result definitively demonstrating whether the natural growth rate of capital production was exogenous or endogenous to demand.</p><p>I was snapped out of my wistful reverie when the principal opened the door. In front of his desk sat the most popular girl in school, Ariela Nudge.</p><p>“Ariela tells me you tried to take the locker she’d been using all year and dismissed her preferences for it as ‘reference-dependent,’” the principal said to me.</p><p>“She was taking advantage of the absence of a market system with enforceable property rights to sabotage allocative efficiency!”</p><p>“Fine,” said the principal. “My stomach is rumbling because it's almost lunchtime, so I'm going to be extra hard on the two of you.</p><p>“I’ll speak to you each separately. If neither of you confesses to wrongdoing, I’ll give you both detention today. If you both confess, I’ll give you detention today and tomorrow. If one of you confesses but the other doesn’t, the one who confessed will go free and the other will get detention for a week.”</p><p>I computed all the outcomes very fast in my elegant, doelike head. If Ariela and I cooperated to both deny wrongdoing, then we would both get detention for just one day. But if she didn’t confess, it would be even better for me to confess and go free. And if she did confess, defecting on me, then it would also be best for me to confess to avoid a week’s detention. School was out later that month, so we wouldn’t have much of a chance to get back at each other. The only equilibrium was for us to both confess, so naturally that’s what I did.</p><p>But Ariela denied everything. “Ugh, Vicktoreya! Why would you act so unfairly!” she squealed when she learned she’d be staying after school all week.</p><p>I sighed. I had almost resigned myself to detention; that way at least I wouldn’t get made fun of for not going to watch the stupid zero-sum lacrosse game. But I knew it was in both of our best interests for her to learn her lesson. “Every reasonable person knows you don’t play tit-for-tat in a one-off.”</p><p>“That’s enough from you two,” the principal said. He turned to his secretary. “Will you please fill out the paperwork for Ariela’s punishment and give Vicktoreya a little talking-to?”</p><p>But as soon as he had left the room, the principal’s secretary said, “Never mind the detention, Ariela. You can go. Just keep your nose clean next time.”</p><p>Ariela skipped out the door back to class.</p><p>The secretary turned to me. “It’s hard for a principal to ensure that an agent acting on his behalf will carry out his will in a manner aligned with his incentives, isn’t it? But of course, you knew that already. Anyway, have a seat. I have something very important to tell you.”</p><p>She flipped off her hood. She was actually my mysterious guardian, Emhily, in disguise.</p><p>“Vicktoreya Keyneslee Beauxpensees, there’s something special that makes you different from all your peers. You parents were no mere Homo sapiens, and neither are you.”</p><p>My mysterious eyes shone with feeling seen.</p><p><span>“You are one of the last living members of </span><em>Homo economicus</em><span>, the mythical race of perfectly rational utility-maximizing economic agents.”</span></p><p>I gasped, covering my dainty mouth with the sharp sparkly nails on my slender fingers. “You mean I’m… I’m a tradeoff-talking rational economic person, or TOTREP?”</p><p>“Please don’t tell your classmates,” Emhily cautioned wisely. “Because of their status quo bias, they’ll assign undue psychological weight to your increased status and generate positional externalities.”</p><p>It was all I could do to contain my secret for the rest of the school day. As soon as the bell rang, I knew where I was headed: off to the mall to exchange monetary payment for the bundle of goods I deemed preferable to all others.</p><p>But when I turned around from picking out the blood-red lipstick that would most perfectly complement my porcelain skin and ethereally melancholy eyes, in strutted Ariela and her clique of western, educated, industrialized, rich, democratic preppies. I ducked behind a shelf.</p><p>“I love those shoes on you,” Ariela was saying to her best friend, Blinklynn Neucoque.</p><p>“I’ll probably go with them, then,” Blinklynn said.&nbsp;</p><p>I almost spilled my consumer surplus all over the floor. Imagine making a purchasing decision under the influence of herd mentality! Munching on my two marshmallows from the food court, I listened to Blinklynn go on.</p><p>“Where’s the price tag? I’d get these if they’re under like 80.”</p><p>“It says they’re thirty percent off online,” Ariela said. “Oh – but they take a week to ship.”</p><p>“And prom is Friday.”</p><p>The horror of having a temporal discounting function that was not only nonexponential but also discontinuous was too much for me to take. With a shudder, I went back to meticulously filling out the table in my notebook of the price and characteristics of every item in the store.</p><p>Friday rolled around: the big night. I didn’t have a date because nobody had shown up to my Gale-Shapley matchmaking session, but it didn’t matter because I looked ravishing in my dramatic lipstick, heavy eyeliner, exquisite dress, hair that I didn’t even have to style because it always fell perfectly, and shoes I cleverly picked out because they wouldn’t even be visible under the dress and I have a very low time preference and prom is only 0.0004% of my life so I’d just grabbed the cheapest ones on the rack.</p><p>Unfortunately, pretty soon my feet started to hurt. But I remembered that Blinklynn had clearly stated she valued her shoes at $80, so I went up and offered her $80. She refused. I was horrified.</p><p>“Your willingness to pay should equal your willingness to accept!” I protested.</p><p>“But then I wouldn’t have any shoes!”</p><p>“$95, to factor in transportation costs and compensate you for your assumption of risk?”</p><p>“You have lipstick on your teeth.”</p><p>Before I could sputter out a response, a mysterious stranger swept me off my feet.</p><p>Cradled in his sculpted arms, I turned to see his face. It was the most beautiful boy I'd ever seen in my life. "Hi, I'm Nashwell Maxington," he said. "I'm new here. I hope you don't mind me rescuing you, but I couldn't believe such a lovely girl with such luxuriant raven hair could have been victimized by others’ vulnerability to the endowment effect."</p><p>I gazed into his breathtakingly dark eyes. There was something deep and familiar about them, glimmering in his eye sockets like two spherical cows. Was it... could it be?</p><p>"I'm Vicktoreya Keyneslee Beauxpensees," I said. "An average blue whale weighs two hundred thousand pounds. How much do you think I weigh?"</p><p>He laughed. "Not more than a hundred pounds sopping wet."</p><p>A delighted shiver passed through my itty bitty little waist and ran down my spine.</p><p><span>"Anchoring has no effect on me," Nashwell said, "and judging by the fact that you tried, I think I know what you are." His voice dropped to a whisper. "I'm a </span><em>Homo economicus</em><span> too."</span></p><p>I gasped. "I thought I'd never meet another like me."</p><p>Nashwell smiled rationally. "Let me relieve you of these sunk costs." He slipped the shoes off my feet and beckoned me to dance.</p><p>We reached our saturation point of music consumption after one song. He took my hand and led me out the door for a moonlit stroll, where we walked and talked for hours. The transaction costs incurred by our interpersonal negotiations were infinitesimal; it felt magical.</p><p>But then Nashwell got a depressed look on his face.</p><p>"What's wrong?" I said.</p><p>"Vicktoreya, I'm smitten by your effortless good looks, outlier IQ, and captivating allure. I observe evidence that you may reciprocate my affections, but I can't generalize from this small sample, lest I fall prey to the fallacious law of small numbers. Most people hallucinate patterns in stochastic noise, but I could never allow myself to be fooled by randomness like that."</p><p>He paused to brood axiomatically and self-consistently.</p><p>"By the trivial rules of conjunction," he lamented, "it’s more likely that you’re beautiful, charming, brilliant, and swooning every time I speak than that you’re beautiful, charming, brilliant, swooning every time I speak, and in love with me.”</p><p>"Oh, Nashwell!" I exclaimed, deeply moved. "Forty-nine percent of the fibers of my being want to fall in love with you. But I can't let myself fall for you, because it would be irrational for my present self to allow my future self to vastly alter its value distribution away from my current one."</p><p>A ray of moonlight suddenly broke through the trees and shone dazzlingly onto his chiseled face. It seemed to strike him with an insight.</p><p>"Ah, my darling!” he said. "Consider this.</p><p>“We, two perfectly rational agents, can still fail to coordinate because we each generate strategies independently of the other. But if we instead performed costly signaling by undertaking elaborate social rituals whose participants are selected for compatibility, and if we subsequently wove the threads of our two lives into one so that future changes to both our decision functions would be influenced by a roughly identical set of inputs – then, my angel, to a first approximation we would be able to avoid multipolar traps by relying on the assumption that our strategies will be the same, and hence outcompete individual actors who fail to implement this meta-coordination.</p><p>“This is what I understand the romantics call ‘true love.’”</p><p>I gazed at him longingly.</p><p>"Plus," he added, "even if I’m wrong – marriage is heavily tax-advantaged."</p><p>I updated directly into his arms. We shared a breathtakingly beautiful kiss. As we held each other, I felt Nashwell's utility function slowly start to nest into mine. I immediately came of age.</p><p>Nine months later, our beautiful baby girl was born.</p><p>"Galaxii is a joy to have in class," the preschool teacher said. "Cooperative, behaves appropriately – she fits in so well with her peers."</p><p>I squeezed Nashwell’s hand.</p><p>"The other day," the teacher said, "I praised her for taking just one juice box when some of the other children were sneaking two. She replied, 'If we model individuals in a shared sociocultural milieu as adapting to similar selection pressures, many stable strategies incorporate altruistic cooperation.'"</p><p>Nashwell and I beamed with pride. That was our tiny TOTREP, all right.</p></div></article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Spying on a smartphone remotely by the authorities: feasibility and operation (109 pts)]]></title>
            <link>https://security.stackexchange.com/questions/271146/spying-on-a-smartphone-remotely-by-the-authorities-feasibility-and-operation</link>
            <guid>36644952</guid>
            <pubDate>Sat, 08 Jul 2023 14:49:51 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://security.stackexchange.com/questions/271146/spying-on-a-smartphone-remotely-by-the-authorities-feasibility-and-operation">https://security.stackexchange.com/questions/271146/spying-on-a-smartphone-remotely-by-the-authorities-feasibility-and-operation</a>, See on <a href="https://news.ycombinator.com/item?id=36644952">Hacker News</a></p>
<div id="readability-page-1" class="page"><div itemprop="text">
                
<p>French lawmakers agreed to a justice reform bill that includes a provision granting police the power to remotely activate suspects' geolocation, microphone and camera (<a href="https://www.lemonde.fr/en/france/article/2023/07/06/france-set-to-allow-police-to-spy-through-phones_6044269_7.html" rel="noreferrer">source</a>).</p>
<p>Following the senators, the deputies also gave their green light to allow certain features of smartphones and other devices to be activated remotely, thus turning them into surveillance trackers.</p>
<p>The deputies determined a list of professions "protected" from any capture: journalists, doctors, notaries, and bailiffs, in addition to lawyers, magistrates and members of parliament.</p>
<p><strong>Technically, will it be possible for them to set up this type of surveillance on phones? If so, how will they go about it?</strong></p>
<p>I wonder how authorities can effectively activate microphones and cameras from popular smartphones using iOS or Android.</p>
<p>I am also wondering how can the privacy of individuals not involved in any criminal activity be safeguarded when such wide-reaching surveillance measures are implemented.</p>
    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[When an app asks for permissions, it should have a “feed fake data” option (957 pts)]]></title>
            <link>https://mastodon.gamedev.place/@Nifflas/110668040598715116</link>
            <guid>36644895</guid>
            <pubDate>Sat, 08 Jul 2023 14:43:33 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://mastodon.gamedev.place/@Nifflas/110668040598715116">https://mastodon.gamedev.place/@Nifflas/110668040598715116</a>, See on <a href="https://news.ycombinator.com/item?id=36644895">Hacker News</a></p>
&lt;Unparsable&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[iVentoy (224 pts)]]></title>
            <link>https://www.iventoy.com/en/index.html</link>
            <guid>36644806</guid>
            <pubDate>Sat, 08 Jul 2023 14:35:37 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.iventoy.com/en/index.html">https://www.iventoy.com/en/index.html</a>, See on <a href="https://news.ycombinator.com/item?id=36644806">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="contentpage"> 


<h2>News</h2>

<p>
2023/07/05 ---  New release <a href="https://www.iventoy.com/en/download.html">iventoy-1.0.08 </a>
<span> <a href="https://www.iventoy.com/en/doc_news.html">More ...</a></span>
</p>

<h2>What is iVentoy</h2>
<p>   
    iVentoy is an enhanced version of the PXE server. <br>
    With iVentoy you can boot and install OS on multiple machines at the same time through the network.<br> 
    iVentoy is extremely easy to use, without complicated configuration, just put the ISO file in the specified location and select PXE boot in the client machine.<br>
    iVentoy supports x86 Legacy BIOS, IA32 UEFI, x86_64 UEFI and ARM64 UEFI mode at the same time.<br>
    iVentoy support 110+ common types of OS (Windows/WinPE/Linux/VMware) (<a href="https://www.iventoy.com/en/isolist.html">list</a>)。
</p>

<br>

<h2>Features</h2>
<div>
<div>
<ul>
  <li>Simple to use <a href="https://www.iventoy.com/en/doc_start.html">(Get Started)</a>  </li>  
  <li>Cross-platform, can run in both Windows and Linux.</li>      
  <li>Specially optimized for PXE scenarios, with flexible functions.</li>      
  <li>Directly boot ISO files, no extraction needed.</li>
  <li>Native boot menu style for Legacy &amp; UEFI</li> 
  <li>Directory layout corresponded boot menu.</li> 
  <li>Supports Legacy BIOS and IA32/X86_64/ARM64 UEFI mode.</li>    
  <li>Supports 110+ common types of OS (Windows/WinPE/Linux/VMware) </li>      
  <li>System or ISO level boot password protection.</li>  
  <li>Multiple devices to install different OSs at the same time.</li>
</ul>
</div>

<div>
<ul>  

  <li>Device filtering by MAC address.</li>
  <li>Support querying MAC address filtering status.</li>
  <li>Support MAC address attribution query.</li>
  <li>Client device information. (Manufacture, product name etc.)</li>
  <li>Directly get ISO internal files with HTTP.<a href="https://www.iventoy.com/en/doc_http_url.html">Notes</a></li>  
  <li>File Injection feature. <a href="https://www.iventoy.com/en/doc_injection.html">Notes</a> </li>  
  <li>Windows auto installation supported. <a href="https://www.iventoy.com/en/doc_autoinstall.html">Notes</a></li>
  <li>Linux auto installation supported. <a href="https://www.iventoy.com/en/doc_autoinstall.html">Notes</a></li>
  <li>Variables Expansion supported for install script <a href="https://www.iventoy.com/en/doc_autoinstall.html">Notes</a></li>
  <li>Automatically solve the driver missing during Linux installation.</li>
  
</ul>
</div>
</div>


<p><img src="https://www.iventoy.com/static/img/screen/boot_ground.png?v=4">
</p>





    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[A $182B Chip Maker: AMD's Labs – Full Documentary [video] (175 pts)]]></title>
            <link>https://www.youtube.com/watch?v=7H4eg2jOvVw</link>
            <guid>36644506</guid>
            <pubDate>Sat, 08 Jul 2023 14:02:20 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.youtube.com/watch?v=7H4eg2jOvVw">https://www.youtube.com/watch?v=7H4eg2jOvVw</a>, See on <a href="https://news.ycombinator.com/item?id=36644506">Hacker News</a></p>
&lt;Unparsable&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[Thermochromic Breadboard (136 pts)]]></title>
            <link>https://www.improwis.com/projects/hw_ThermochromicBreadboard/</link>
            <guid>36644026</guid>
            <pubDate>Sat, 08 Jul 2023 13:10:00 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.improwis.com/projects/hw_ThermochromicBreadboard/">https://www.improwis.com/projects/hw_ThermochromicBreadboard/</a>, See on <a href="https://news.ycombinator.com/item?id=36644026">Hacker News</a></p>
<div id="readability-page-1" class="page">

<hr><hr><a name="Why"></a><h2>Why
</h2>
<p>
When working on a&nbsp;<a href="https://en.wikipedia.org/wiki/Breadboard#Solderless_breadboard" title="Wikipedia link: Breadboard#Solderless_breadboard" target="_blank">solderless breadboard</a><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABAAAAAQBAMAAADt3eJSAAAAMFBMVEWEg4R6e3pIR0jp6Om5urmpqKmYmJhpamlXWFf8/fw8OjwoKCjY2dgZGBnIx8gEAgRb5tWnAAAAcklEQVR4nGOYCQUMmAwPlamKJyXOVDBM2DJT+zDb5DKGOdctc+MqJ1cyzOx9eWhD5JRIhpm+nNN+zDTwZJg5SWfOopkHgdonfJ2z2jMNZM4O06pKZhCDS/TgNk4QI3rrlN0vQYxJOjOrwXZNeTnzCJKlAFOARWjqKuE5AAAAAElFTkSuQmCC"> (not only there, but... well...),
it often happens that a&nbsp;power rating of some part is exceeded or its cooling is insufficient.
It would be beneficial to see such situations before the&nbsp;parts express their distress with a
smoke signal.
</p>
<hr><a name="How"></a><h2>How
</h2>
<p>
A breadborad was modified with a&nbsp;<a href="https://en.wikipedia.org/wiki/Thermochromism" title="Wikipedia link: Thermochromism" target="_blank">thermochromic paint</a><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABAAAAAQBAMAAADt3eJSAAAAMFBMVEWEg4R6e3pIR0jp6Om5urmpqKmYmJhpamlXWFf8/fw8OjwoKCjY2dgZGBnIx8gEAgRb5tWnAAAAcklEQVR4nGOYCQUMmAwPlamKJyXOVDBM2DJT+zDb5DKGOdctc+MqJ1cyzOx9eWhD5JRIhpm+nNN+zDTwZJg5SWfOopkHgdonfJ2z2jMNZM4O06pKZhCDS/TgNk4QI3rrlN0vQYxJOjOrwXZNeTnzCJKlAFOARWjqKuE5AAAAAElFTkSuQmCC">, mixed from a&nbsp;white
acrylic model-grade paint and a&nbsp;thermochromic pigment obtained from <a href="http://www.mutr.co.uk/" title="remote link: http://www.mutr.co.uk/" target="_blank">Middlesex University Teaching Resources</a><img src="data:image/gif;base64,R0lGODlhCgAKAIABAGZmZv///yH5BAEAAAEALAAAAAAKAAoAAAIVjA8Jx6FvVmrL1chu3c1h+mXRoxgFADs=">
webshop, using a&nbsp;makeshift pot made from the&nbsp;bottom part of a&nbsp;beverage can. 
The orange color was chosen on the&nbsp;basis of availability (read: mistakenly ordering four
oranges instead of intended four different hues.) The threshold temperature of the&nbsp;pigment
was chosen also on the&nbsp;basis of availability (they did not have any other than 29-30&nbsp;°C).
</p>
<p>
The active area, where the&nbsp;parts are located by at least one pin, was coated with the
mixed paint. Care was taken to not let too much of it drip into the&nbsp;pin holes.
The power buses were not painted.
</p>
<hr><a name="Results"></a><h2>Results
</h2>
<p>
The first tests were done during a&nbsp;particularly warm summer night, when the&nbsp;indoor temperature
reached close to threshold temperature of the&nbsp;pigment. At such conditions, the&nbsp;sensitivity of the
paint was outstanding; a&nbsp;250-milliwatt resistor loaded with 350&nbsp;milliwatts shown a&nbsp;color change around
its leg within several seconds.
</p>
<p>
The reverse change was much slower. Due to some hysteresis of the&nbsp;pigment and the&nbsp;closeness of the
room temperature to the&nbsp;threshold temperature, the&nbsp;thermal trace was present for a&nbsp;fairly long time
(minutes). Putting the&nbsp;board out of the&nbsp;window, where the&nbsp;temperature was slightly lower, markedly
accelerated restoring of the&nbsp;color.
</p>
<p>
The thermal trace (the discoloration of the&nbsp;pigment) tended to bleed around the&nbsp;board as its material
spread the&nbsp;heat. The width of the&nbsp;trace, and the&nbsp;speed of its spreading, can provide a&nbsp;visual clue
about how much is the&nbsp;part heating.
</p>
<p>
Caveat: The color change shows the&nbsp;part's color indirectly. There is a&nbsp;delay between the&nbsp;temperature
change of the&nbsp;part itself and temperature change of the&nbsp;board, which is usually provided via thermal
conduction through the&nbsp;part's legs. There is also a&nbsp;thermal differential between the&nbsp;part itself,
along its leg, and over the&nbsp;board.
</p>
<hr><a name="Possibleimprovements"></a><h2>Possible improvements
</h2>
<ul><li> Use a&nbsp;darker pigment with a&nbsp;different hue (most likely blue or black)
</li><li> Use a&nbsp;pigment with a&nbsp;different threshold temperature (e.g. 43&nbsp;°C, that could be optimal)
</li><li> Best: Use a&nbsp;pair of pigments with different colors and different thresholds, yielding two color changes at two temperatures
</li><li> Incorporate the&nbsp;pigment directly into the&nbsp;material the&nbsp;breadboard is made from
</li></ul><div><hr><div>
<table>
<tbody><tr><td colspan="2"><small>If you have any comments or questions about the topic, please let me know here:</small></td></tr>


<tr><td>Your name:</td><td></td></tr>
<tr><td>Your email:</td><td></td></tr>

<tr><td>Feedback:</td><td></td></tr>
<tr><td>&nbsp;</td><td></td></tr>

</tbody></table>
</div>
</div>

</div>]]></description>
        </item>
        <item>
            <title><![CDATA[If PEP 703 is accepted, Meta can commit three engineer-years to nogil CPython (576 pts)]]></title>
            <link>https://discuss.python.org/t/a-fast-free-threading-python/27903/99</link>
            <guid>36643670</guid>
            <pubDate>Sat, 08 Jul 2023 12:18:46 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://discuss.python.org/t/a-fast-free-threading-python/27903/99">https://discuss.python.org/t/a-fast-free-threading-python/27903/99</a>, See on <a href="https://news.ycombinator.com/item?id=36643670">Hacker News</a></p>
<div id="readability-page-1" class="page"><div itemscope="" itemtype="http://schema.org/DiscussionForumPosting">
      <meta itemprop="headline" content="A fast, free threading Python">
        <meta itemprop="articleSection" content="Ideas">
      <meta itemprop="keywords" content="">
      

          <div itemprop="comment" id="post_80" itemscope="" itemtype="http://schema.org/Comment">
              
<p>This will work as long as you give it an initial empty <code>Counter</code> to start with (otherwise it starts with <code>0</code> and complains)</p>
            </div>
          <div id="post_81" itemprop="comment" itemscope="" itemtype="http://schema.org/Comment">
            <div>
              <p><span itemprop="author" itemscope="" itemtype="http://schema.org/Person">
                <a itemprop="url" href="https://discuss.python.org/u/Rosuav"><span itemprop="name">Rosuav</span></a>
                (Chris Angelico)
              </span></p>


              <p><span>
                  <time itemprop="datePublished" datetime="2023-06-26T15:35:51Z">
                    June 26, 2023,  3:35pm
                  </time>
                  <meta itemprop="dateModified" content="2023-06-26T15:35:51Z">
              <span itemprop="position">81</span>
              </span>
            </p></div>
            <div itemprop="text">
              
<p>Ah thanks. Anyhow, the idea is to minimize the work done in the single-threaded “gather” phase at the end, by having each thread individually count in a lock-free way.</p>
            </div>

            

            

          </div>
          <div id="post_82" itemprop="comment" itemscope="" itemtype="http://schema.org/Comment">
            <div>
              <p><span itemprop="author" itemscope="" itemtype="http://schema.org/Person">
                <a itemprop="url" href="https://discuss.python.org/u/lieryan"><span itemprop="name">lieryan</span></a>
                (Lie Ryan)
              </span></p>


              <p><span>
                  <time itemprop="datePublished" datetime="2023-06-26T16:20:39Z">
                    June 26, 2023,  4:20pm
                  </time>
                  <meta itemprop="dateModified" content="2023-06-26T16:28:05Z">
              <span itemprop="position">82</span>
              </span>
            </p></div>
            <div itemprop="text">
              
<p>I don’t think that is true. If free threading is possible, the cat will be out of the bag, even developers that only cares about single threaded work will still be affected by threading issues. If a library starts a thread in the background for whatever reason, they can cause threading issue in my code even though I never subscribed for having threading problems.</p>
<p>Many libraries that had async-to-sync bridges spawns threads to simulate async tasks. Django, FastAPI, SQLAlchemy is just a few off the top of my head. And then there’s tools like IPython that starts a couple background threads for who knows what reasons.</p>
<p>Multithreading has a reputation for being hard. But really, I think they are considered hard <strong>because</strong> of the existence of free threading. Languages like Rust that doesn’t have free threading (or to be more precise, it has an almost free threading with some severe restrictions) actually fared better at making multithreading a lot easier to use.</p>
<p>The arena-based threading with subinterpreters I had mentioned earlier would be that similar sort of that almost-free threading with forced discipline.</p>
<p>One way to think of arena-based threading is that it’s basically like a dynamic/runtime borrow checker, enforcing acquisition of the arena locks before working with any objects owned by the arena. I think it can even be flexible enough to allow future experimentation with non standard arenas that have different borrowing rules.</p>
            </div>

            

            

          </div>
          <div id="post_83" itemprop="comment" itemscope="" itemtype="http://schema.org/Comment">
            <div>
              <p><span itemprop="author" itemscope="" itemtype="http://schema.org/Person">
                <a itemprop="url" href="https://discuss.python.org/u/lunixbochs"><span itemprop="name">lunixbochs</span></a>
                (Ryan Hileman)
              </span></p>


              <p><span>
                  <time itemprop="datePublished" datetime="2023-06-26T16:25:27Z">
                    June 26, 2023,  4:25pm
                  </time>
                  <meta itemprop="dateModified" content="2023-06-26T16:25:27Z">
              <span itemprop="position">83</span>
              </span>
            </p></div>
            <div itemprop="text">
              
<p>There are language-level tools like golang’s race condition detector, thread sanitizer, etc, which take the common mistakes and test for them. It’s also possible someone could implement something like a borrow checker or thread safety heuristics on top of python’s type system, e.g. with passthrough types along the lines of Mutable / Immutable / Shared / Local, and auditing nonlocal variable access or object types passed into threads.</p>

<p>This wouldn’t be the case with my proposal to make threads take a voluntary lock by default. In a sense, you could leave something like the GIL in place, but make it safe to release for specific threads while accessing python code/objects.</p>
            </div>

            

            

          </div>
          <div itemprop="comment" id="post_84" itemscope="" itemtype="http://schema.org/Comment">
              
<p>I don’t think anyone has demonstrated how this would happen, and I’d view it as a fundamental flaw in the implementation if it could.</p>
<p>I’m not saying it’s impossible, but I think it would be useful to have specific examples–even if only theoretical–before it’s considered a significant problem.</p>
            </div>
          <div id="post_85" itemprop="comment" itemscope="" itemtype="http://schema.org/Comment">
            <div>
              <p><span itemprop="author" itemscope="" itemtype="http://schema.org/Person">
                <a itemprop="url" href="https://discuss.python.org/u/lunixbochs"><span itemprop="name">lunixbochs</span></a>
                (Ryan Hileman)
              </span></p>


              <p><span>
                  <time itemprop="datePublished" datetime="2023-06-26T16:33:27Z">
                    June 26, 2023,  4:33pm
                  </time>
                  <meta itemprop="dateModified" content="2023-06-26T16:33:27Z">
              <span itemprop="position">85</span>
              </span>
            </p></div>
            <div itemprop="text">
              
<p>I guess this is a bit too open ended. I think the thread in question can only interact with your code unintentionally if you happen to share a resource with that thread in an unsafe way, furthermore to be scary it would need to be an unsafe way that isn’t possible today. Even with the GIL another thread can already do a lot of things, like mess with your file descriptors, stdout, signals. And threads sharing access to any variable is already inconsistent for non-atomic ops at the Python level.</p>
            </div>

            

            

          </div>
          <div id="post_86" itemprop="comment" itemscope="" itemtype="http://schema.org/Comment">
            <div>
              <p><span itemprop="author" itemscope="" itemtype="http://schema.org/Person">
                <a itemprop="url" href="https://discuss.python.org/u/smontanaro"><span itemprop="name">smontanaro</span></a>
                (Skip Montanaro)
              </span></p>


              <p><span>
                  <time itemprop="datePublished" datetime="2023-06-26T17:01:58Z">
                    June 26, 2023,  5:01pm
                  </time>
                  <meta itemprop="dateModified" content="2023-06-26T17:01:58Z">
              <span itemprop="position">86</span>
              </span>
            </p></div>
            <div itemprop="text">
              
<p>Can you provide a (hypothetical?) example?</p>
            </div>

            

            

          </div>
          <div id="post_87" itemprop="comment" itemscope="" itemtype="http://schema.org/Comment">
            <div>
              <p><span itemprop="author" itemscope="" itemtype="http://schema.org/Person">
                <a itemprop="url" href="https://discuss.python.org/u/Eclips4"><span itemprop="name">Eclips4</span></a>
                (Kirill Podoprigora)
              </span></p>


              <p><span>
                  <time itemprop="datePublished" datetime="2023-06-26T17:13:12Z">
                    June 26, 2023,  5:13pm
                  </time>
                  <meta itemprop="dateModified" content="2023-06-26T17:13:12Z">
              <span itemprop="position">87</span>
              </span>
            </p></div>
            <p>Yep, CPython can switch threads in the middle of these two operations. So, there’s a problem.</p>

            

            

          </div>
          <div id="post_88" itemprop="comment" itemscope="" itemtype="http://schema.org/Comment">
            <div>
              <p><span itemprop="author" itemscope="" itemtype="http://schema.org/Person">
                <a itemprop="url" href="https://discuss.python.org/u/ntessore"><span itemprop="name">ntessore</span></a>
                (Nicolas Tessore)
              </span></p>


              <p><span>
                  <time itemprop="datePublished" datetime="2023-06-26T21:16:35Z">
                    June 26, 2023,  9:16pm
                  </time>
                  <meta itemprop="dateModified" content="2023-06-26T21:16:35Z">
              <span itemprop="position">88</span>
              </span>
            </p></div>
            <div itemprop="text">
              <p>Again, of course. <sup><a href="#footnote-102472-1" id="footnote-ref-102472-1">[1]</a></sup> But I understood that <a href="https://discuss.python.org/u/pf_moore">@pf_moore</a> made the very fine point that due to specialisations we are discussing here (e.g. <code>BINARY_SUBSCR_DICT</code>), and hence the GIL, things which are nominally not thread-safe are effectively so in current CPython, because they are specialised to a single native instruction. And this, I think, only needs an explicit specification for whether or not such operations are to be considered effectively “atomic” or not. Otherwise, yes, these are just undiscovered bugs, currently protected by a CPython implementation detail.</p>
<hr>

<ol>
<li id="footnote-102472-1"><p>Except that there’s the timer. <a href="#footnote-ref-102472-1">↩︎</a></p>
</li>
</ol>
            </div>

            

            

          </div>
          <div id="post_89" itemprop="comment" itemscope="" itemtype="http://schema.org/Comment">
            <div>
              <p><span itemprop="author" itemscope="" itemtype="http://schema.org/Person">
                <a itemprop="url" href="https://discuss.python.org/u/tjreedy"><span itemprop="name">tjreedy</span></a>
                (Terry Jan Reedy)
              </span></p>


              <p><span>
                  <time itemprop="datePublished" datetime="2023-06-26T22:13:25Z">
                    June 26, 2023, 10:13pm
                  </time>
                  <meta itemprop="dateModified" content="2023-06-26T22:13:25Z">
              <span itemprop="position">89</span>
              </span>
            </p></div>
            <div itemprop="text">
              
<p>… of a library starting a background thread.  Not exactly a library, but idlelib.run has</p>
<pre><code>    sockthread = threading.Thread(target=manage_socket,
                                  name='SockThread',
                                  args=((LOCALHOST, port),))
</code></pre>
<p>as a result of which <code>threading.activecount()</code> and <code>theading.enumerate()</code> returns are greater when running on IDLE.  Someone once asked why the difference on Stackoverflow.   (I have not idea whether no-gil will require any change to <code>manage_socket</code> or that chance of user code having a problem.)</p>
            </div>

            

            

          </div>
          <div id="post_90" itemprop="comment" itemscope="" itemtype="http://schema.org/Comment">
            <div>
              <p><span itemprop="author" itemscope="" itemtype="http://schema.org/Person">
                <a itemprop="url" href="https://discuss.python.org/u/ppolewicz"><span itemprop="name">ppolewicz</span></a>
                (Pawel Polewicz)
              </span></p>


              <p><span>
                  <time itemprop="datePublished" datetime="2023-06-26T22:13:51Z">
                    June 26, 2023, 10:13pm
                  </time>
                  <meta itemprop="dateModified" content="2023-06-26T22:13:51Z">
              <span itemprop="position">90</span>
              </span>
            </p></div>
            <div itemprop="text">
              <pre><code>import threading

THREAD_COUNT = 3
BY_HOW_MUCH = 1_000_000


class Incrementor:
    def __init__(self):
        self.c = 0

def incr(incrementor, by_how_much):
    for i in range(by_how_much):
        incrementor.c += 1

incrementor = Incrementor()

threads = [
    threading.Thread(target=incr, args=(incrementor, BY_HOW_MUCH))
    for i in range(THREAD_COUNT)
]

for t in threads:
    t.start()

for t in threads:
    t.join()

print(incrementor.c)

</code></pre>
<p>prints 3 million when ran it on 3.10. Does it mean you can rely on <code>+=</code> being atomic when writing Python code? No! If you run it on 3.9 it prints between 1.5 and 2 million. Soon a Faster CPython team member can swoop in and change (not break!) it again.</p>
<p>BTW if Java and .net developers can have free threads, then so can we.</p>
            </div>

            

            

          </div>
          <div id="post_91" itemprop="comment" itemscope="" itemtype="http://schema.org/Comment">
            <div>
              <p><span itemprop="author" itemscope="" itemtype="http://schema.org/Person">
                <a itemprop="url" href="https://discuss.python.org/u/h-vetinari"><span itemprop="name">h-vetinari</span></a>
                (H. Vetinari)
              </span></p>


              <p><span>
                  <time itemprop="datePublished" datetime="2023-06-26T23:08:41Z">
                    June 26, 2023, 11:08pm
                  </time>
                  <meta itemprop="dateModified" content="2023-06-26T23:08:41Z">
              <span itemprop="position">91</span>
              </span>
            </p></div>
            <div itemprop="text">
              
<p>The word “can” here translates to (potentially) decades of work, which was the case for Java:</p>

<p>Yes we “can” (and likely should), but it requires serious commitment, and off-hand “others do it too” is not helpful here.</p>
<p>In the context Java and threading, it’s worth noting how threads commonly need quite a lot of developer-facing infrastructure (e.g. thread pools) that’s probably very hard to make beginner-friendly / “Pythonic”, and that they’re on a similarly large multi-{year,person} effort to move from free threads to virtual threads<sup><a href="#footnote-102482-1" id="footnote-ref-102482-1">[1]</a></sup> under <a href="https://openjdk.org/projects/loom/" rel="noopener nofollow ugc">Project “Loom”</a> (where – arguably – the boundaries to async programming start getting blurred), and encapsulating a lot of that in simpler interfaces through <a href="https://openjdk.org/jeps/453" rel="noopener nofollow ugc">“structured concurrency”</a>, which we have already (at least through <code>trio</code>).</p>
<p>All that to say: if we argue “Java can”, then we should also look at where those choices have led them, and what they consider as “moving forward” from there. But realistically, we’re very far from an apples-to-apples comparison in any case, and it’s better to leave that rhetorical tool hanging in the shed.</p>
<hr>

<ol>
<li id="footnote-102482-1"><p>latest <a href="https://openjdk.org/jeps/444" rel="noopener nofollow ugc">incarnation</a> in JDK 21 <a href="#footnote-ref-102482-1">↩︎</a></p>
</li>
</ol>
            </div>

            

            

          </div>
          <div id="post_92" itemprop="comment" itemscope="" itemtype="http://schema.org/Comment">
            <div>
              <p><span itemprop="author" itemscope="" itemtype="http://schema.org/Person">
                <a itemprop="url" href="https://discuss.python.org/u/ppolewicz"><span itemprop="name">ppolewicz</span></a>
                (Pawel Polewicz)
              </span></p>


              <p><span>
                  <time itemprop="datePublished" datetime="2023-06-27T04:39:41Z">
                    June 27, 2023,  4:39am
                  </time>
                  <meta itemprop="dateModified" content="2023-06-27T07:13:02Z">
              <span itemprop="position">92</span>
              </span>
            </p></div>
            <div itemprop="text">
              <p>I meant it from the user perspective, referring to “maybe PEP-684 is better, because it’s safer” part of the discussion. For years Python was “parallel, but…” and now adding subinterpreters will help, but it won’t solve the entire problem.</p>
<p>PEP-703 on the other hand, goes pretty much all the way (though stop-the-world GC may still be a limitation) for those willing to learn how to use it and for those who already have experience with threads from other languages. Python “popularity” will increase with projects choosing it for a multicore program when it will become an option.</p>
<p>Will some users hurt themselves with free threading? I’ve been tracking nogil for a long while now and from what I’ve seen, for someone who writes threadsafe code already (but not native extensions) it will be really hard to run into trouble.</p>
            </div>

            

            

          </div>
          <div id="post_93" itemprop="comment" itemscope="" itemtype="http://schema.org/Comment">
            <div>
              <p><span itemprop="author" itemscope="" itemtype="http://schema.org/Person">
                <a itemprop="url" href="https://discuss.python.org/u/bluss"><span itemprop="name">bluss</span></a>
                (bluss)
              </span></p>


              <p><span>
                  <time itemprop="datePublished" datetime="2023-06-27T19:25:35Z">
                    June 27, 2023,  7:25pm
                  </time>
                  <meta itemprop="dateModified" content="2023-06-27T19:30:00Z">
              <span itemprop="position">93</span>
              </span>
            </p></div>
            <div itemprop="text">
              <p>There are some multithreading traps inside glibc (relevant for Linux) that are unfortunate and sort of perennial issues (not just in Python!). <a href="http://rachelbythebay.com/w/2017/01/30/env/" rel="noopener nofollow ugc">For example getenv and setenv</a>; glibc maintains that multithreaded programs must not use setenv, it’s not thread safe.</p>
<p>A library could start a thread, and the library wants to and would use C getenv in this thread (getenv is allowed according to glibc in a multithreaded program, following the usual logic).<br>
The user’s program then has a threading issue: they must not use setenv, that could possibly cause segfaults (Python has setenv interfaces through os.environ and os.putenv).</p>
<p>(Does this issue exist in Python already today? Is there some mitigating factor that I don’t know about? How do subinterpreters deal with this? It would be great if C getenv/setenv had a major revision to be somewhat compatible with threading.)</p>
            </div>

            

            

          </div>
          <div id="post_94" itemprop="comment" itemscope="" itemtype="http://schema.org/Comment">
            <div>
              <p><span itemprop="author" itemscope="" itemtype="http://schema.org/Person">
                <a itemprop="url" href="https://discuss.python.org/u/ings"><span itemprop="name">ings</span></a>
                (Christoph)
              </span></p>


              <p><span>
                  <time itemprop="datePublished" datetime="2023-06-27T20:19:25Z">
                    June 27, 2023,  8:19pm
                  </time>
                  <meta itemprop="dateModified" content="2023-06-27T20:19:25Z">
              <span itemprop="position">94</span>
              </span>
            </p></div>
            <div itemprop="text">
              <p>I just want to throw in a use case which has not yet been discussed here and in the discussions of PEP 703: GUI toolkits.</p>
<p>GUI toolkits are naturally using threads and therefore an approach where free threading is replaced by a different concept like sub interpreters or multiprocessing is problematic in the design &amp; architecture of GUI applications written in python (because the toolkits being exposed in python are not aware of such concepts and most likely offer plain threading for offloading computational workload from the GUI). I’m a user of the PySide (Qt for python) project, and the PySide devs did struggle with the GIL as explained here <a href="https://www.qt.io/blog/qt-for-python-5.15.0-is-out" rel="noopener nofollow ugc">Qt for Python 5.15.0 is out!</a> (and also the links inside the document).</p>
<p>The problem of when it’s better to release or not to release the GIL in the C extensions of GUI toolkits is not straightforward, sometimes counter-intuitive (at least to me) and often there is a compromise involved depending on most common use cases but with drawbacks for other use cases.</p>
<p>Moreover, when creating GUI applications in python, you start struggling with the GIL when you have huge workloads happening in the background. My use case is a computer vision GUI which acts as a monitor and development environment for remote embedded systems. It is similar to what <a href="https://discuss.python.org/u/lunixbochs">@lunixbochs</a> reported for the realtime audio use case - keeping latencies and stutters on an acceptable level is unnecessarily difficult when you have to fight against the GIL mechanism.</p>
            </div>

            

            

          </div>
          <div id="post_95" itemprop="comment" itemscope="" itemtype="http://schema.org/Comment">
            <div>
              <p><span itemprop="author" itemscope="" itemtype="http://schema.org/Person">
                <a itemprop="url" href="https://discuss.python.org/u/smontanaro"><span itemprop="name">smontanaro</span></a>
                (Skip Montanaro)
              </span></p>


              <p><span>
                  <time itemprop="datePublished" datetime="2023-06-27T20:34:59Z">
                    June 27, 2023,  8:34pm
                  </time>
                  <meta itemprop="dateModified" content="2023-06-27T20:34:59Z">
              <span itemprop="position">95</span>
              </span>
            </p></div>
            <div itemprop="text">
              
<p>Wouldn’t this be a potential problem today? “Not thread-safe” doesn’t mean “not thread-safe only when used in a free-threaded environment.”</p>
<p>I’m not trying to be difficult, maybe a bit pedantic. The presence of the GIL can obscure threading bugs or make them rear their ugly heads less often, but it doesn’t make code thread-safe. <a href="https://discuss.python.org/u/colesbury">@colesbury</a>’s work to remove the GIL has done a lot to remove places in the interpreter, stdlib, and some third-party libraries that relied on the GIL (knowingly or not). Most (all? almost all?) of that work will have been in C code, not Python code.</p>
            </div>

            

            

          </div>
          <div id="post_96" itemprop="comment" itemscope="" itemtype="http://schema.org/Comment">
            <div>
              <p><span itemprop="author" itemscope="" itemtype="http://schema.org/Person">
                <a itemprop="url" href="https://discuss.python.org/u/bluss"><span itemprop="name">bluss</span></a>
                (bluss)
              </span></p>


              <p><span>
                  <time itemprop="datePublished" datetime="2023-06-28T20:06:44Z">
                    June 28, 2023,  8:06pm
                  </time>
                  <meta itemprop="dateModified" content="2023-06-28T20:06:44Z">
              <span itemprop="position">96</span>
              </span>
            </p></div>
            <div itemprop="text">
              
<p>I think it could be a problem today, don’t know, I have the same question. I’m here to be curious.</p>
<p>Again as an interesting anecdotal data point here is a port of that troublesome C code to Python: <a href="https://gist.github.com/bluss/a3d2ad94d55382f682897ee1efae6d74" rel="noopener nofollow ugc">mtenv.py</a><br>
Here’s how it runs:</p>
<ul>
<li>Using Python 3.11.3 this program loops for a long time without problem</li>
<li>I compiled and used nogil-3.12 commit 4526c07caee8f2e (current tip of the repo)<br>
and it runs 1-2 seconds before it <strong>segfaults</strong> in getenv just like the C code from 2017.</li>
</ul>
<p>This is a reduced example. It doesn’t look like a normal Python program, it has a strange shape so that it can reproduce a crash easily. But the fundamental elements can occur in normal Python programs - various C calls that libraries use that use getenv - let’s say mktime to use the example from that blog post - and for setenv we have plain interface to it in <code>os</code> (<code>os.getenv</code> is <em>not</em> a plain interface to C <code>getenv</code>).</p>
            </div>

            

            

          </div>
          <div id="post_97" itemprop="comment" itemscope="" itemtype="http://schema.org/Comment">
            
            <p>Yes, it is a problem today, without free threading. getenv + setenv thread safety is a problem for Python applications I run at work. We had to do a bunch of whackamole to work around segfaults resulting from extension libraries using getenv + setenv (for a while we gave up and used a terrible <code>LD_PRELOAD</code> hack)</p>

            

            

          </div>
          <div id="post_98" itemprop="comment" itemscope="" itemtype="http://schema.org/Comment">
            <div>
              <p><span itemprop="author" itemscope="" itemtype="http://schema.org/Person">
                <a itemprop="url" href="https://discuss.python.org/u/gpshead"><span itemprop="name">gpshead</span></a>
                (Gregory P. Smith)
              </span></p>


              <p><span>
                  <time itemprop="datePublished" datetime="2023-07-07T23:46:24Z">
                    July 7, 2023, 11:46pm
                  </time>
                  <meta itemprop="dateModified" content="2023-07-07T23:46:24Z">
              <span itemprop="position">98</span>
              </span>
            </p></div>
            <div itemprop="text">
              
<p>&amp;</p>

<p>At the high level this is the kind of thing I’d <strong>love someone to try creating</strong> for per-subinterpreter-GIL use! This is also quite hard, but I assume there are interested folks out there.</p>
<p>Intuitively I <em>expect</em> this winds up being the same problem that needs to be solved for free threading <em>(which PEP-703 appears to do)</em>: our pure reference counting model is the most significant reason we have a GIL - in order to share objects between multiple threads you need to make the reference counts work without that single lock.</p>
<p>Someone really needs to try creating explicitly shared objects implementation for CPython and subinterpreters to prove or disprove it’s actual utility. In the absence of that, I wouldn’t point to it and suggest it is a <em>better</em> solution. I consider it an open avenue of future work. <em>(Even if we get free threading, performant explicit sharing would be something I expect many would appreciate having.)</em></p>
            </div>

            

            

          </div>
          <div id="post_99" itemprop="comment" itemscope="" itemtype="http://schema.org/Comment">
            <div>
              <p><span itemprop="author" itemscope="" itemtype="http://schema.org/Person">
                <a itemprop="url" href="https://discuss.python.org/u/carljm"><span itemprop="name">carljm</span></a>
                (Carl Meyer)
              </span></p>


              <p><span>
                  <time itemprop="datePublished" datetime="2023-07-07T23:53:54Z">
                    July 7, 2023, 11:53pm
                  </time>
                  <meta itemprop="dateModified" content="2023-07-07T23:53:54Z">
              <span itemprop="position">99</span>
              </span>
            </p></div>
            <div itemprop="text">
              
<p>We’ve had a chance to discuss this internally with the right people. Our team believes in the value that nogil will provide, and we are committed to working collaboratively to improve Python for everyone.</p>
<p>If PEP 703 is accepted, Meta can commit to support in the form of three engineer-years (from engineers experienced working in CPython internals) between the acceptance of PEP 703 and the end of 2025, to collaborate with the core dev team on landing the PEP 703 implementation smoothly in CPython and on ongoing improvements to the compatibility and performance of nogil CPython.</p>
            </div>

            

            

          </div>
    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Scams upon scams: The data-driven advertising grift (228 pts)]]></title>
            <link>https://anotherangrywoman.com/2023/07/05/scams-upon-scams-the-data-driven-advertising-grift/</link>
            <guid>36643630</guid>
            <pubDate>Sat, 08 Jul 2023 12:12:31 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://anotherangrywoman.com/2023/07/05/scams-upon-scams-the-data-driven-advertising-grift/">https://anotherangrywoman.com/2023/07/05/scams-upon-scams-the-data-driven-advertising-grift/</a>, See on <a href="https://news.ycombinator.com/item?id=36643630">Hacker News</a></p>
<div id="readability-page-1" class="page"><article id="post-6748">
	
	<!-- .entry-header -->

	<div>
		
<p>Digital advertising is a scam from top to bottom. In fact, it’s several scams stacked on top of each other, wearing a trenchcoat, and some of the foundations of fibs are so effective that otherwise reasonable people entirely buy into them. </p>



<h2>Data-driven ads are anything but</h2>



<p>I’ll start with a few examples of the data which is definitely held on me, and just how entirely bad my targeted advertising is. </p>



<p>Facebook know my age and date of birth. They have had this data since I signed up for the website, 15 years ago. They know exactly how old I am. They also know where I live. Hell, sometimes I used to check into places with my location on. Despite knowing I am way north of 30 and way south of Birmingham, they are incredibly keen on advertising me events explicitly limited to people under the age of 30 in the Birmingham area. </p>



<p>Google knew I wanted to buy a mattress. They knew this because I googled it. And I clicked through to a brand selling mattresses, and I bought myself a mattress. The brand know I googled said mattress. Google know I clicked through. From Google’s own analytics, they ought to know I bought the mattress. Since buying that mattress, I’ve been constantly advertised mattresses, especially the one I already own and they know I already own. </p>



<p>Some might claim that in fact the advertisers are being incredibly smart and they’re advertising me activities for women under 30 in Birmingham so I go and tell my friends who are under 30 in Birmingham to go and do that. But of course, Facebook would also know that I don’t have any friends in that demographic. Or maybe that mattress seller is trying to tell me to refer a friend to buy that mattress by reminding me that I own a very nice mattress. In which case, why isn’t it advertising the referral programme, which I know they have because I received several emails and a physical leaflet about it with the fucking mattress?</p>



<p>The more simple answer is that the advertisers aren’t being data driven at all. They’re ticking default boxes or casting wider nets. I’m getting advertised mattresses because I have ~an interest in mattresses~. I’m getting activities for women under 30 in Birmingham because I’m under 40 and on the same island as Birmingham.</p>



<p>For all the buzzwords about “data-driven” and “smart” and whatever else you want to call it, the advertisers are just going “eh, sounds about right” and letting a robot automate their job. </p>



<p>This, then, is the first grift in the chain. Despite claiming to their boss that they’re using “data-driven” advertising, they’re targeting their ads even less than taking out a quarter page in the local newspaper. </p>



<h2>The product: they could spy on you (but don’t)</h2>



<p>Everyone is rightly nervy about the sheer quantity of data that big companies hold on us. Social media companies know all about your demographic information, social connections and interests. Amazon knows exactly when you have an outbreak of aphids because you buy things to kill the nasty little beasties, and it probably also knows when you’ve had a nasty breakup because nobody listens to Fleetwood Mac’s Rumours on repeat at 3am when they’re in a good place. Google basically knows everything about you. </p>



<p>At least that’s the theory. And that’s the product that they’re selling to advertisers. They have an enormous dataset from which everything an advertiser could ever dream of about a person can be garnered. They’re the world’s biggest, bestest spy network, which means they have quality data to help <em>your</em> business be the biggest, bestest business reaching the biggest, bestest customers.</p>



<p>At least that’s what they say. </p>



<p>Actual spying requires actual spies. There’s a reason intelligence agencies are such big employers: they have all of their fancy spy computers, but they know they need to hire humans to actually deduce patterns and sort signal from noise. They’re aware that a human brain is always superior to a computer in figuring this out, so they get humans to do the work.</p>



<p>Meanwhile, tech companies break into hives at the thought of getting a human to do a job. Their ethos is that if a human can do a task, a machine can do that task better, and not cost them anything such as salary, pensions or or a basic level of respect. Tech companies are fatally allergic to getting a human to do a human job, so content moderation is largely an algorithm looking for the word “boobies”. A tech company would go into anaphylactic shock at the very notion of employing a human to analyse their vast dataset.</p>



<p>So it’s all machine learning, and the machines are very, very stupid. Have you ever looked at your list inferred interests on a social media platform? If you ever tweeted “I don’t like Game of Thrones, it’s not for me,” you’ll be classified as interested in Game of Thrones and possibly get served ads for it. These machines may also attempt to deduce your age, gender, and so forth based on half-baked crap fed into them, and it seldom comes up right. Maybe that’s why it thinks I’m under 30 and in Birmingham. Perhaps I internet in a Brummie accent. </p>



<p>It’s no wonder that on multiple occasions, big tech has been caught out completely making things up when communicating with advertisers, and they continue to do so. Facebook was famously found to have inflated or outright fabricated video metrics. GA4 very quietly admits that the data is padded out with machine learning. The data is a lie, and a lot of it is because they literally haven’t the first clue on what to do with it, they just need to steeple their fingers and act all evil so advertisers think they have it.</p>



<p>Advertisers, then, are getting served a steaming turd on a plate rather than the medium-rare filet mignon they were promised. </p>



<p>And meanwhile, the spies don’t even need that data, because your posts are public anyway.</p>



<p>But enough about that. The problem is this grift is, too, built upon a grift. </p>



<h2>Marketing science is a grift</h2>



<p>I work in marketing, for my sins. This is mostly why I’m so entirely down on the marketing industry and many of the people who work in it. I also happen to have an MSc in psychology – actual psychology! – with a focus on behaviour change. </p>



<p>On day 1 of your class about behaviour change in a science course, you learn that behaviour change is not a simple matter of information in, behaviour out. Human behaviour, and changing it, is big and complex. </p>



<p>Meanwhile, on your marketing courses, which I have had the misfortune to attend, the model of changing behaviour is pretty much this: information in, behaviour out.</p>



<p>The thing with the entire “science” of marketing is the underpinning theory base is basic common sense which has been treated with a bit of a brand makeover, turned into a couple of overcomplicated diagrams with some neologisms obscuring meaning. Digital marketing has become very popular because baked into it are a whole bunch of metrics so you have something to show your manager that you’re not spending the entire day tending your geraniums, but do the metrics really mean anything?</p>



<p>The metrics that marketers are told they need are marketed to them by the marketing department of a company that specialises in making products for marketers. And that company was probably started up by someone who worked in marketing. </p>



<p>Marketing theory is never tested rigorously. The <s>common sense</s> incredibly sound scientific view based on heaps of scientific evidence view – showing your ads to people more likely to buy your product is more efficient because they’re more likely to buy your product anyway – is entirely untested.</p>



<p>There’s an anecdote that a glitch with Facebook led to ads no longer being targeted over a period of several weeks. And absolutely nobody noticed because the metrics all looked normal, the engagement and purchasing was just the same.</p>



<p>There isn’t any evidence to suggest that an ad targeted to 35 year old men with children with an interest in football is any more likely to result in sales of Football Dad socks than a poster for Football Dad socks at a bus stop. But an entire industry is based on pretending that this is the case.</p>



<h2>tl;dr</h2>



<p>Facebook will try to sell you Football Dad socks even if you’re a 55 year old childfree woman who posted once about hating football, because that data is utterly useless. </p>



<p>Spies are probably reading your posts though, no matter how boring.</p>



<p>_</p>



<p><em>Enjoyed what you read? Consider&nbsp;<a href="https://www.patreon.com/user?u=2332646">becoming a Patron&nbsp;</a>or&nbsp;<a href="https://www.paypal.me/stavvers">leave a tip</a></em></p>
	</div><!-- .entry-content -->

	
	<!-- .entry-footer -->

</article></div>]]></description>
        </item>
    </channel>
</rss>