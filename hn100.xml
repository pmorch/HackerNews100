<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0">
    <channel>
        <title>HN100 - Readable Contents</title>
        <link>https://hn.algolia.com/api/v1/search_by_date?tags=%28story,poll%29&amp;numericFilters=points%3E100</link>
        <description>Uses Readability to add bodies to the RSS feed</description>
        <lastBuildDate>Sat, 05 Oct 2024 05:30:01 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>en</language>
        <item>
            <title><![CDATA[Don't squander public trust on bullshit (168 pts)]]></title>
            <link>https://livboeree.substack.com/p/dont-squander-public-trust-on-bullshit</link>
            <guid>41746180</guid>
            <pubDate>Fri, 04 Oct 2024 22:42:07 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://livboeree.substack.com/p/dont-squander-public-trust-on-bullshit">https://livboeree.substack.com/p/dont-squander-public-trust-on-bullshit</a>, See on <a href="https://news.ycombinator.com/item?id=41746180">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><article><div dir="auto"><p>At 4.50am local time today, this statewide emergency alert was sent out to every cellphone in Texas:</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa86d5372-c29b-477f-8d4b-3fcc885a469c_1201x410.jpeg" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa86d5372-c29b-477f-8d4b-3fcc885a469c_1201x410.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa86d5372-c29b-477f-8d4b-3fcc885a469c_1201x410.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa86d5372-c29b-477f-8d4b-3fcc885a469c_1201x410.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa86d5372-c29b-477f-8d4b-3fcc885a469c_1201x410.jpeg 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa86d5372-c29b-477f-8d4b-3fcc885a469c_1201x410.jpeg" width="1201" height="410" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/a86d5372-c29b-477f-8d4b-3fcc885a469c_1201x410.jpeg&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:410,&quot;width&quot;:1201,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:123540,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/jpeg&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:false,&quot;topImage&quot;:true,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa86d5372-c29b-477f-8d4b-3fcc885a469c_1201x410.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa86d5372-c29b-477f-8d4b-3fcc885a469c_1201x410.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa86d5372-c29b-477f-8d4b-3fcc885a469c_1201x410.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa86d5372-c29b-477f-8d4b-3fcc885a469c_1201x410.jpeg 1456w" sizes="100vw" fetchpriority="high"></picture></div></a></figure></div><p>I don’t know who Seth Altman is, nor do I care. Why? Because Seth Altman’s offense took place in Lubbock, Texas. I live in Austin, Texas. Four hundred miles away. What I do care about however is the misuse of emergency alert systems and public trust.</p><p><span>Sending out a screeching alert to 30million+ people over 250 million square miles in the middle of the night should only be used in the absolute DIREST OF CIRCUMSTANCES… circumstances like “Texas is under threat from hurricane/chemical leak/nuclear weapons, seek shelter now!” It should </span><em>never</em><span> be used for something that’s utterly irrelevant to 99.99% of people. </span></p><p>Why? Because the public’s trust in government emergency protocols is already hanging by a thread, and in order for those protocols to work when we really need them, they will need to be received and listened to. Instead, Texans by the hundreds of thousands are now turning off their phone’s emergency alerts, possibly forever. Why would anyone with a life to lead leave them on and risk getting their sleep disrupted over personally inconsequential events hundreds of miles away? </p><p>Such an outcome could be truly dangerous for Texas in the long run. If and when a real major emergency strikes, we will no longer have this important tool of public awareness or coordination. And that’s just the second order effects! There are likely going to be some excess deaths today as a direct result—there are 30m+ people in Texas, many of whom are in weak cardiovascular health. I would not be surprised at all if hospitals report an spike in cardiovascular events today. Not to mention an increase in road accidents; Texas is a notoriously driving-heavy state, and few things are worse for safe driving than sleep impairment.</p><p>So I hope the local government takes a long hard look at its alert-pressing finger. We all know the lesson of the Boy Who Cried Wolf, exhausting his village with his over-zealous cries. Well this time the village is thirty million people. Heads need to roll.</p></div></article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Max Schrems wins privacy case against Meta over data on sexual orientation (105 pts)]]></title>
            <link>https://apnews.com/article/facebook-meta-schrems-privacy-80fd4e6c59f48a3b583d6665af3ede86</link>
            <guid>41745181</guid>
            <pubDate>Fri, 04 Oct 2024 20:17:13 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://apnews.com/article/facebook-meta-schrems-privacy-80fd4e6c59f48a3b583d6665af3ede86">https://apnews.com/article/facebook-meta-schrems-privacy-80fd4e6c59f48a3b583d6665af3ede86</a>, See on <a href="https://news.ycombinator.com/item?id=41745181">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
                                        <p>LONDON (AP) — The European Union’s top court said Friday that social media company Meta can’t use public information about a user’s sexual orientation obtained outside its platforms for personalized advertising under the bloc’s strict data privacy rules. </p><p>The decision from the Court of Justice of the European Union in Luxembourg is a <span><a data-gtm-enhancement-style="LinkEnhancementA" href="https://apnews.com/article/meta-facebook-instagram-whatsapp-ai-artificial-intelligence-8d6cb3424ee410c641d0acdb154601f5">victory for Austrian privacy activist Max Schrems</a></span>, who has been a thorn in the side of Big Tech companies over their compliance with 27-nation bloc’s data privacy rules. </p><p>The EU court issued its ruling after Austria’s supreme court asked for guidance in Schrems’ case on how to apply the privacy rules, known as the General Data Protection Regulation, or GDPR. </p><p>Schrems had complained that Facebook had processed personal data including information about his sexual orientation to target him with online advertising, even though he had never disclosed on his account that he was gay. The only time he had publicly revealed this fact was during a panel discussion. </p>
    

<p>“An online social network such as Facebook cannot use all of the personal data obtained for the purposes of targeted advertising, without restriction as to time and without distinction as to type of data,” the court said in a press release summarizing its decision. </p>



<p>Even though Schrems revealed he was gay in the panel discussion, that “does not authorise the operator of an online social network platform to process other data relating to his sexual orientation, obtained, as the case may be, outside that platform, with a view to aggregating and analysing those data, in order to offer him personalised advertising.” </p>
    
<p>Meta said it was awaiting publication of the court’s full judgment and that it “takes privacy very seriously.”</p><p>“Everyone using Facebook has access to a wide range of settings and tools that allow people to manage how we use their information,” the company said in a statement. </p>
    

<p>Schrems’ lawyer, Katharina Raabe-Stuppnig, lawyer representing Mr Schrems, welcomed the court’s decision. </p><p>“Meta has basically been building a huge data pool on users for 20 years now, and it is growing every day. However, EU law requires ‘data minimisation’,” she said in a statement. “Following this ruling only a small part of Meta’s data pool will be allowed to be used for advertising — even when users consent to ads.” </p>
                                    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[How were 70s versions of games like Pong built without a programmable computer? (114 pts)]]></title>
            <link>https://retrocomputing.stackexchange.com/questions/30698/how-were-the-70s-versions-of-pong-and-similar-games-implemented-without-a-progra</link>
            <guid>41745032</guid>
            <pubDate>Fri, 04 Oct 2024 19:57:38 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://retrocomputing.stackexchange.com/questions/30698/how-were-the-70s-versions-of-pong-and-similar-games-implemented-without-a-progra">https://retrocomputing.stackexchange.com/questions/30698/how-were-the-70s-versions-of-pong-and-similar-games-implemented-without-a-progra</a>, See on <a href="https://news.ycombinator.com/item?id=41745032">Hacker News</a></p>
<div id="readability-page-1" class="page"><div itemprop="text">
<p>They were made by mostly avoiding 'computing' concepts altogether, and treating it more like a mechanical thing.</p>
<p>For example with Pong a major component is usually timers - every xth of a second the timer will emit a signal. You have timers calibrated to match the horizontal refresh of the screen, so they'll 'ring' at the same point on each scanline. Then you have timers calibrated to the vertical refresh, so they'll ring on the same scanline each frame.</p>
<p>The ball is then just two discrete timers for vertical and horizontal position, and their rings are sent through an AND gate that will raise the voltage going to the display when both are ringing causing a white dot to appear. The paddles build on this concept with a medium length timer that can be started and stopped to define the length.</p>
<p>To move the ball or paddles the timers can be advanced or delayed by a control signal. Or more accurately the timers are always paused at a certain point, like during half of the horizontal blanking period. This pausing is then shortened once to advance the timer (move left/up) or increased once to delay it (move right/down).</p>
<p>Since both the paddle and the ball timers are emitting a '1' when they are to be displayed you can impliment collision detection by performing another AND operation. So if both a paddle and the ball are being drawn at the same moment you know they've collided and can adjust a latch controlling the ball direction accordingly.</p>
<p>If you get into the Atari 2600 you'll find that it's really weird compared to other consoles (sprites with no clearly defined X coordinate, instead only the ability to place it at the actual current location of the CRT beam or nudge it a small amount either way), but that it starts to make a lot of sense when you realize they were implementing their Pong logic for a programmable chip.</p>
    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Mitmproxy 11: Full HTTP/3 Support (143 pts)]]></title>
            <link>https://mitmproxy.org/posts/releases/mitmproxy-11/</link>
            <guid>41744434</guid>
            <pubDate>Fri, 04 Oct 2024 18:52:46 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://mitmproxy.org/posts/releases/mitmproxy-11/">https://mitmproxy.org/posts/releases/mitmproxy-11/</a>, See on <a href="https://news.ycombinator.com/item?id=41744434">Hacker News</a></p>
<div id="readability-page-1" class="page"><section>
            <p>We are excited to announce the release of mitmproxy 11, which introduces full support for HTTP/3 in both transparent
and reverse proxy modes. We’re also bringing in a ton of DNS improvements that we’ll cover in this blog post.</p>
<h5 id="editorial-note"><em>Editorial Note:</em></h5>
<p><em>Hi! I’m <a href="https://mitmproxy.org/authors/gaurav-jain/">Gaurav Jain</a>, one of the students selected for this year’s Google Summer of Code program to work on mitmproxy.
During this summer, I’ve worked on improving various low-level networking parts of mitmproxy some of which include
HTTP/3 and DNS. You can find my project report <a href="https://gist.github.com/errorxyz/af6f26549e9122f3ff3b93fd9d257df1">here</a>.</em></p>
<h2 id="http3">HTTP/3</h2>
<p>HTTP/3 now “just works” for reverse proxies. Your mitmproxy instance will listen for
both TCP and UDP packets and handle all HTTP versions thrown at it:</p>
<div><pre tabindex="0"><code data-lang="shell"><span><span>$ mitmproxy --mode reverse:https://http3.is
</span></span></code></pre></div><p>Our transparent proxy modes now all support HTTP/3 as well:</p>
<div><pre tabindex="0"><code data-lang="shell"><span><span>$ mitmproxy --mode wireguard
</span></span><span><span>$ mitmproxy --mode local
</span></span><span><span>$ mitmproxy --mode transparent
</span></span></code></pre></div><p>We have successfully tested HTTP/3 support with Firefox, Chrome, various cURL builds, and other clients to iron out
compatibility issues.
The only major limitation we are aware of at this time is that Chrome <a href="https://issues.chromium.org/issues/40138351#comment15">does not trust user-added Certificate Authorities for QUIC</a>.
This means you will either need to provide a publicly trusted certificate (e.g. from Let’s Encrypt), start Chrome with
a <a href="https://www.chromium.org/quic/playing-with-quic/#generate-certificates">command line switch</a>, or accept that it falls back to HTTP/2. Alternatively, Firefox doesn’t do such shenanigans.
For more HTTP/3 troubleshooting tips, you can check out <a href="https://github.com/mitmproxy/mitmproxy/issues/7025#issuecomment-2351138170">#7025</a>.</p>
<p>Bringing HTTP/3 support to mitmproxy is a major effort that was started in 2022 by <a href="https://mitmproxy.org/authors/manuel-meitinger/">Manuel Meitinger</a> and <a href="https://mitmproxy.org/authors/maximilian-hils/">Maximilian Hils</a>.
QUIC and HTTP/3 make up an increasing share of network traffic in the wild, and we’re super excited to have this ready
and enabled by default now!</p>
<h2 id="improved-dns-support">Improved DNS Support</h2>
<p>With the advent of DNS <a href="https://blog.cloudflare.com/speeding-up-https-and-http-3-negotiation-with-dns/">HTTPS records</a> and new privacy enhancements such as <a href="https://en.wikipedia.org/wiki/Server_Name_Indication#Encrypted_Client_Hello">Encrypted Client Hello (ECH)</a>, mitmproxy’s DNS
functionality is becoming increasingly important. We’re happy to share multiple advancements on this front:</p>
<h4 id="support-for-query-types-beyond-aaaaa">Support for Query Types Beyond A/AAAA</h4>
<p>mitmproxy’s old DNS implementation used <code>getaddrinfo</code> to resolve queries. This is convenient because everything is taken
care of by libc, but the <code>getaddrinfo</code> API only supports A/AAAA queries for IPv4 and IPv6 addresses. It doesn’t allow us
to answer queries for e.g. <a href="https://blog.cloudflare.com/speeding-up-https-and-http-3-negotiation-with-dns/">HTTPS records</a>, which are used to signal HTTP/3 support.</p>
<p>To overcome this limitation, we’ve reimplemented our DNS support on top of <a href="https://github.com/hickory-dns/hickory-dns">Hickory&nbsp;DNS</a>, a Rust-based DNS library.
Using Hickory, we now obtain the operating system’s default nameservers on Windows, Linux, and macOS and forward
non-A/AAAA queries there. This behavior can also be customized with the new <a href="https://docs.mitmproxy.org/stable/concepts-options/#dns_name_servers"><code>dns_name_servers</code> option</a>:</p>
<div><pre tabindex="0"><code data-lang="shell"><span><span>$ mitmdump --mode dns --set dns_name_servers<span>=</span>8.8.8.8
</span></span></code></pre></div><p><img src="https://mitmproxy.org/posts/releases/mitmproxy-11/dns.png" alt="dns"></p>
<h4 id="skipping-etchosts">Skipping /etc/hosts</h4>
<p>By switching to Hickory, we now also have the option to ignore the system’s hosts
file (<code>/etc/hosts</code> on Linux) with the new <a href="https://docs.mitmproxy.org/stable/concepts-options/#dns_use_hosts_file"><code>dns_use_hosts_file</code> option</a>. We plan to move mitmproxy’s internal
DNS resolution to Hickory as well, at which point this feature will become incredibly useful in allowing transparent
redirection on the same machine for specific domains. At the moment, such a setup would cause mitmproxy to recursively
connect to itself because we always take the hosts file into account.</p>
<div><pre tabindex="0"><code data-lang="shell"><span><span>$ echo <span>"192.0.2.1 mitmproxy.org"</span> &gt;&gt; /etc/hosts
</span></span><span><span>
</span></span><span><span>$ mitmdump --mode dns
</span></span><span><span>$ dig @127.0.0.1 +short mitmproxy.org
</span></span><span><span>192.0.2.1
</span></span><span><span>
</span></span><span><span>$ mitmdump --mode dns --set dns_use_hosts_file<span>=</span>false
</span></span><span><span>$ dig @127.0.0.1 +short mitmproxy.org
</span></span><span><span>3.161.82.13
</span></span></code></pre></div><h4 id="support-for-dns-over-tcp">Support for DNS-over-TCP</h4>
<p>DNS uses UDP by default, but may also use TCP to support records that do not fit into a single UDP packet. mitmproxy has
previously gotten away with only supporting UDP, but now that we support arbitrary query types, message size and thus
TCP support is more important. Long story short, DNS-over-TCP works with mitmproxy 11!</p>
<h4 id="stripping-encrypted-client-hello-ech-keys">Stripping Encrypted Client Hello (ECH) Keys</h4>
<p>Unless a custom certificate is configured, mitmproxy uses the Server Name Indication (SNI) transmitted in the TLS
ClientHello to construct a valid certificate. Conversely, if no SNI is present, we may not be able
to generate a certificate that is trusted by the client.</p>
<p><a href="https://en.wikipedia.org/wiki/Server_Name_Indication#Encrypted_Client_Hello">Encrypted Client Hello (ECH)</a> is an exciting new technology to increase privacy on the web. In short, the client uses
the new DNS HTTPS records to obtain an ECH key before establishing a connection, and then already encrypts the initial
ClientHello handshake message with that key. If both DNS queries and handshake are encrypted, passive intermediaries
cannot learn the target domain, only the target IP address (which is not conclusive for shared hosting and Content Delivery
Networks). This is a great advancement for privacy, but also breaks mitmproxy’s way of generating certificates.
To fix this, mitmproxy now strips ECH keys from HTTPS records. This way the client has no keys to encrypt the initial
handshake message with, and mitmproxy still learns the target domain and can construct a matching certificate.</p>
<p>Of course, ECH adds complexity for us and sometimes makes mitmproxy harder to use for our users. Nonetheless, we are
excited to see these privacy advancements being made for the rest of the web!</p>
<h2 id="acknowledgements">Acknowledgements</h2>
<p>This work supported by <a href="https://summerofcode.withgoogle.com/">Google Summer of Code</a> under the umbrella of the <a href="https://www.honeynet.org/">Honeynet&nbsp;Project</a>, and the
<a href="https://nlnet.nl/entrust/">NGI0 Entrust fund</a> established by <a href="https://nlnet.nl/">NLnet</a>. Thank you to my mentor <a href="https://mitmproxy.org/authors/maximilian-hils/">Maximilian Hils</a> for the
invaluable guidance and support.</p>
        </section></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Show HN: Open source framework OpenAI uses for Advanced Voice (124 pts)]]></title>
            <link>https://github.com/livekit/agents</link>
            <guid>41743327</guid>
            <pubDate>Fri, 04 Oct 2024 17:01:04 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/livekit/agents">https://github.com/livekit/agents</a>, See on <a href="https://news.ycombinator.com/item?id=41743327">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text">
<themed-picture data-catalyst-inline="true"><picture>
  <source media="(prefers-color-scheme: dark)" srcset="https://github.com/livekit/agents/raw/main/.github/banner_dark.png">
  <source media="(prefers-color-scheme: light)" srcset="https://github.com/livekit/agents/raw/main/.github/banner_light.png">
  <img alt="The LiveKit icon, the name of the repository and some sample code in the background." src="https://raw.githubusercontent.com/livekit/agents/main/.github/banner_light.png">
</picture></themed-picture>


<p>
Looking for the JS/TS library? Check out <a href="https://github.com/livekit/agents-js">AgentsJS</a></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">✨ [NEW] OpenAI Realtime API support</h2><a id="user-content--new-openai-realtime-api-support" aria-label="Permalink: ✨ [NEW] OpenAI Realtime API support" href="#-new-openai-realtime-api-support"></a></p>
<p dir="auto">We're partnering with OpenAI on a new <code>MultimodalAgent</code> API in the Agents framework. This class completely wraps OpenAI’s Realtime API, abstract away the raw wire protocol, and provide an ultra-low latency WebRTC transport between GPT-4o and your users’ devices. This same stack powers Advanced Voice in the ChatGPT app.</p>
<ul dir="auto">
<li>Try the Realtime API in our <a href="https://playground.livekit.io/" rel="nofollow">playground</a> [<a href="https://github.com/livekit-examples/realtime-playground">code</a>]</li>
<li>Check out our <a href="https://docs.livekit.io/agents/openai" rel="nofollow">guide</a> to building your first app with this new API</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">What is Agents?</h2><a id="user-content-what-is-agents" aria-label="Permalink: What is Agents?" href="#what-is-agents"></a></p>
<p dir="auto">The Agents framework allows you to build AI-driven server programs that can see, hear, and speak in realtime. Your agent connects with end user devices through a LiveKit session. During that session, your agent can process text, audio, images, or video streaming from a user's device, and have an AI model generate any combination of those same modalities as output, and stream them back to the user.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Features</h2><a id="user-content-features" aria-label="Permalink: Features" href="#features"></a></p>
<ul dir="auto">
<li>Plugins for popular LLMs, transcription and text-to-speech services, and RAG databases</li>
<li>High-level abstractions for building voice agents or assistants with automatic turn detection, interruption handling, function calling, and transcriptions</li>
<li>Compatible with LiveKit's <a href="https://github.com/livekit/sip">telephony stack</a>, allowing your agent to make calls to or receive calls from phones</li>
<li>Integrated load balancing system that manages pools of agents with edge-based dispatch, monitoring, and transparent failover</li>
<li>Running your agents is identical across localhost, <a href="https://github.com/livekit/livekit">self-hosted</a>, and <a href="https://cloud.livekit.io/" rel="nofollow">LiveKit Cloud</a> environments</li>
</ul>

<p dir="auto"><h2 tabindex="-1" dir="auto">Installation</h2><a id="user-content-installation" aria-label="Permalink: Installation" href="#installation"></a></p>
<p dir="auto">To install the core Agents library:</p>
<div dir="auto" data-snippet-clipboard-copy-content="pip install livekit-agents"><pre>pip install livekit-agents</pre></div>
<p dir="auto"><h2 tabindex="-1" dir="auto">Plugins</h2><a id="user-content-plugins" aria-label="Permalink: Plugins" href="#plugins"></a></p>
<p dir="auto">The framework includes a variety of plugins that make it easy to process streaming input or generate output. For example, there are plugins for converting text-to-speech or running inference with popular LLMs. Here's how you can install a plugin:</p>
<div dir="auto" data-snippet-clipboard-copy-content="pip install livekit-plugins-openai"><pre>pip install livekit-plugins-openai</pre></div>
<p dir="auto">The following plugins are available today:</p>
<markdown-accessiblity-table><table>
<thead>
<tr>
<th>Plugin</th>
<th>Features</th>
</tr>
</thead>
<tbody>
<tr>
<td><a href="https://pypi.org/project/livekit-plugins-anthropic/" rel="nofollow">livekit-plugins-anthropic</a></td>
<td>LLM</td>
</tr>
<tr>
<td><a href="https://pypi.org/project/livekit-plugins-azure/" rel="nofollow">livekit-plugins-azure</a></td>
<td>STT, TTS</td>
</tr>
<tr>
<td><a href="https://pypi.org/project/livekit-plugins-deepgram/" rel="nofollow">livekit-plugins-deepgram</a></td>
<td>STT</td>
</tr>
<tr>
<td><a href="https://pypi.org/project/livekit-plugins-cartesia/" rel="nofollow">livekit-plugins-cartesia</a></td>
<td>TTS</td>
</tr>
<tr>
<td><a href="https://pypi.org/project/livekit-plugins-elevenlabs/" rel="nofollow">livekit-plugins-elevenlabs</a></td>
<td>TTS</td>
</tr>
<tr>
<td><a href="https://pypi.org/project/livekit-plugins-playht/" rel="nofollow">livekit-plugins-playht</a></td>
<td>TTS</td>
</tr>
<tr>
<td><a href="https://pypi.org/project/livekit-plugins-google/" rel="nofollow">livekit-plugins-google</a></td>
<td>STT, TTS</td>
</tr>
<tr>
<td><a href="https://pypi.org/project/livekit-plugins-nltk/" rel="nofollow">livekit-plugins-nltk</a></td>
<td>Utilities for working with text</td>
</tr>
<tr>
<td><a href="https://pypi.org/project/livekit-plugins-rag/" rel="nofollow">livekit-plugins-rag</a></td>
<td>Utilities for performing RAG</td>
</tr>
<tr>
<td><a href="https://pypi.org/project/livekit-plugins-openai/" rel="nofollow">livekit-plugins-openai</a></td>
<td>LLM, STT, TTS, Assistants API, Realtime API</td>
</tr>
<tr>
<td><a href="https://pypi.org/project/livekit-plugins-silero/" rel="nofollow">livekit-plugins-silero</a></td>
<td>VAD</td>
</tr>
</tbody>
</table></markdown-accessiblity-table>
<p dir="auto"><h2 tabindex="-1" dir="auto">Documentation and guides</h2><a id="user-content-documentation-and-guides" aria-label="Permalink: Documentation and guides" href="#documentation-and-guides"></a></p>
<p dir="auto">Documentation on the framework and how to use it can be found <a href="https://docs.livekit.io/agents" rel="nofollow">here</a></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Example agents</h2><a id="user-content-example-agents" aria-label="Permalink: Example agents" href="#example-agents"></a></p>
<ul dir="auto">
<li>A basic voice agent using a pipeline of STT, LLM, and TTS [<a href="https://kitt.livekit.io/" rel="nofollow">demo</a> | <a href="https://github.com/livekit/agents/blob/main/examples/voice-pipeline-agent/minimal_assistant.py">code</a>]</li>
<li>Voice agent using the new OpenAI Realtime API [<a href="https://playground.livekit.io/" rel="nofollow">demo</a> | <a href="https://github.com/livekit-examples/realtime-playground">code</a>]</li>
<li>Super fast voice agent using Cerebras hosted Llama 3.1 [<a href="https://cerebras.vercel.app/" rel="nofollow">demo</a> | <a href="https://github.com/dsa/fast-voice-assistant/">code</a>]</li>
<li>Voice agent using Cartesia's Sonic model [<a href="https://cartesia-assistant.vercel.app/" rel="nofollow">demo</a>]</li>
<li>Agent that looks up the current weather via function call [<a href="https://github.com/livekit/agents/blob/main/examples/voice-pipeline-agent/function_calling_weather.py">code</a>]</li>
<li>Voice agent that performs a RAG-based lookup [<a href="https://github.com/livekit/agents/tree/main/examples/voice-pipeline-agent/simple-rag">code</a>]</li>
<li>Video agent that publishes a stream of RGB frames [<a href="https://github.com/livekit/agents/tree/main/examples/simple-color">code</a>]</li>
<li>Transcription agent that generates text captions from a user's speech [<a href="https://github.com/livekit/agents/tree/main/examples/speech-to-text">code</a>]</li>
<li>A chat agent you can text who will respond back with genereated speech [<a href="https://github.com/livekit/agents/tree/main/examples/text-to-speech">code</a>]</li>
<li>Localhost multi-agent conference call [<a href="https://github.com/dsa/multi-agent-meeting">code</a>]</li>
<li>Moderation agent that uses Hive to detect spam/abusive video [<a href="https://github.com/dsa/livekit-agents/tree/main/hive-moderation-agent">code</a>]</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Contributing</h2><a id="user-content-contributing" aria-label="Permalink: Contributing" href="#contributing"></a></p>
<p dir="auto">The Agents framework is under active development in a rapidly evolving field. We welcome and appreciate contributions of any kind, be it feedback, bugfixes, features, new plugins and tools, or better documentation. You can file issues under this repo, open a PR, or chat with us in LiveKit's <a href="https://livekit.io/join-slack" rel="nofollow">Slack community</a>.</p>

<markdown-accessiblity-table><table>
<thead><tr><th colspan="2">LiveKit Ecosystem</th></tr></thead>
<tbody>
<tr><td>Realtime SDKs</td><td><a href="https://github.com/livekit/client-sdk-js">Browser</a> · <a href="https://github.com/livekit/client-sdk-swift">iOS/macOS/visionOS</a> · <a href="https://github.com/livekit/client-sdk-android">Android</a> · <a href="https://github.com/livekit/client-sdk-flutter">Flutter</a> · <a href="https://github.com/livekit/client-sdk-react-native">React Native</a> · <a href="https://github.com/livekit/rust-sdks">Rust</a> · <a href="https://github.com/livekit/node-sdks">Node.js</a> · <a href="https://github.com/livekit/python-sdks">Python</a> · <a href="https://github.com/livekit/client-sdk-unity">Unity</a> · <a href="https://github.com/livekit/client-sdk-unity-web">Unity (WebGL)</a></td></tr><tr></tr>
<tr><td>Server APIs</td><td><a href="https://github.com/livekit/node-sdks">Node.js</a> · <a href="https://github.com/livekit/server-sdk-go">Golang</a> · <a href="https://github.com/livekit/server-sdk-ruby">Ruby</a> · <a href="https://github.com/livekit/server-sdk-kotlin">Java/Kotlin</a> · <a href="https://github.com/livekit/python-sdks">Python</a> · <a href="https://github.com/livekit/rust-sdks">Rust</a> · <a href="https://github.com/agence104/livekit-server-sdk-php">PHP (community)</a></td></tr><tr></tr>
<tr><td>UI Components</td><td><a href="https://github.com/livekit/components-js">React</a> · <a href="https://github.com/livekit/components-android">Android Compose</a> · <a href="https://github.com/livekit/components-swift">SwiftUI</a></td></tr><tr></tr>
<tr><td>Agents Frameworks</td><td><b>Python</b> · <a href="https://github.com/livekit/agents-js">Node.js</a> · <a href="https://github.com/livekit/agent-playground">Playground</a></td></tr><tr></tr>
<tr><td>Services</td><td><a href="https://github.com/livekit/livekit">LiveKit server</a> · <a href="https://github.com/livekit/egress">Egress</a> · <a href="https://github.com/livekit/ingress">Ingress</a> · <a href="https://github.com/livekit/sip">SIP</a></td></tr><tr></tr>
<tr><td>Resources</td><td><a href="https://docs.livekit.io/" rel="nofollow">Docs</a> · <a href="https://github.com/livekit-examples">Example apps</a> · <a href="https://livekit.io/cloud" rel="nofollow">Cloud</a> · <a href="https://docs.livekit.io/home/self-hosting/deployment" rel="nofollow">Self-hosting</a> · <a href="https://github.com/livekit/livekit-cli">CLI</a></td></tr>
</tbody>
</table></markdown-accessiblity-table>

</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[12 Months of Mandarin (366 pts)]]></title>
            <link>https://isaak.net/mandarin/</link>
            <guid>41742432</guid>
            <pubDate>Fri, 04 Oct 2024 15:28:51 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://isaak.net/mandarin/">https://isaak.net/mandarin/</a>, See on <a href="https://news.ycombinator.com/item?id=41742432">Hacker News</a></p>
<div id="readability-page-1" class="page"><article>
    <p>Estimates for achieving intermediate fluency in Mandarin Chinese range up to spending years and around 4000 total hours (2,200h classroom hours, 1,800 outside). I did it in 1500 hours total and less than a year.<sup><a href="#fn1" id="fnref1">[1]</a></sup></p>
<hr>
<section>
<ol>
<li id="fn1"><p>There is a lot of disagreement on language proficiency estimates. They are unreliable and inaccurate. My rough best estimates:<br>
Mean: <strong>2,200h classroom + ~1800h outside -&gt; 50% pass ILR 3 -&gt; true average level is ~ILR 2+</strong><br>
My journey: <strong>150h classroom + 1350h outside -&gt; 50/50 shot at passing ILR 3 exam -&gt; true average level ILR 2+</strong><br>
Detailed walkthrough: One of our best sources, the US Department of State, sadly has a lot of content under NDAs, including exams. That said, they <a href="https://www.state.gov/foreign-language-training/?ref=isaak.net">estimate</a> that reaching "General Professional Fluency" (<a href="https://www.govtilr.org/Skills/ILRscale2.htm?ref=isaak.net">ILR 3</a>) takes 2,200 <em>classroom</em> hours over 88 weeks. This is 2,200/88 = 25 hours a week. Diplomats describe studying a language at the DOS as "the hardest thing you will do in your life". <em>Classroom</em> hours do not include the heavy homework and content consumption. For them, this is not a 25h part-time breeze. It is life.<br>
What the program sounds like, it takes more like 45h a week for 88 weeks (3096h). Add some private language practice, travelling, and content consumption, and studying for the actual exam (which seems brutal). Now, getting to ILR3 fluency seems like taking <strong>well above 4000 hours</strong>, all included.<br>
This is being good on paper. These kinds of language proficiency levels tend to fall short in real-life. In fact, they fall short even on paper: Only some <a href="https://www.reddit.com/r/languagelearning/comments/rd19bj/success_rates_in_2011_and_2012_of_the_fsi_at/?ref=isaak.net">50% pass</a> the ILR3 test after the 88 weeks. In additio, tests are a mediocre proxy for actual communication ability. My sense from reading online is that the diplomats at ILR3 on paper have only truly mastered something like ILR2.<br>
Similarly, I spent <strong>some 1,500 hours total</strong>, including everything like classroom hours, tutoring hours, content consumption, and conversations. On paper, I would guess I am at ILR3, but that is probably an overestimate. So call it ILR2+.<br>
To be fair, also take generously take some time off that 4000-hour estimate: Call it usually takes <strong>3000 hours to get to a similarly strong</strong> ILR2+ level / on-paper ILR3. Hence the 3000h vs 1500h comparison.<br>
This is my rough outline of the core estimate. I do not really care enough to get into more detail or studying for passing tests. I just enjoy learning the language and using it. <a href="#fnref1">↩︎</a></p>
</li>
</ol>
</section>
<p>Over the last 365 days, I studied Mandarin for fun. With anki, tutors, and traveling accelerating my learning, I ended up getting to the level of comfortable conversational fluency. My Mandarin isn't perfect nor perfectly fluent, but I can now handle everything up to technical conversations in the area of my PhD.&nbsp;</p><p>For serious language learners, I also jotted down a longer list of methodds: <a href="https://isaak.net/mandarinmethods/">isaak.net/mandarinmethods</a></p><h2 id="humble-beginnings-%E2%80%94-%E7%AD%9A%E8%B7%AF%E8%93%9D%E7%BC%95">Humble Beginnings — 筚路蓝缕</h2><p><strong>Month 1:</strong> Last September, I was deep into my math undergrad. It was pretty dry. I was looking for some fun non-math side project.<sup><a href="#fn1" id="fnref1">[1]</a></sup> I flirted with French, Russian, archery, parkour, and Japanese. But those didn’t ignite my passion. I happened to watch a snippet of the anime Demon Slayer in an obscure <a href="https://www.youtube.com/watch?v=-VoT0TY0emM&amp;ref=isaak.net">Chinese fan dub</a>. Ironically, this caught my attention. I also had lots of Chinese friends, so why not learn a little Mandarin? Oh my, I had no idea how obsessed I'd end up with this "little" side project.</p>
<p>Berkeley had a breakneck-speed Mandarin beginner class. I loved it. Within a week, we learned pinyin. We learned the tones. We learned to read. We learned to write. Then started talking immediately, every single day. Talking in horribly horribly broken Chinese, but nevertheless having conversations.<sup><a href="#fn2" id="fnref2">[2]</a></sup> I learned the very most important survival vocabulary, like: <em>I am Isaak</em> and <em>Yes, I live in America</em> and <em>Sorry, no, I’m not a basketball player for the Golden State Warriors</em>.</p>
<hr>
<section>
<ol>
<li id="fn1"><p>I had virtually no math background, so I spent a lot of my time studying math from absolute scratch. It was brutal. It was rewarding. It also was very dry. Many months of many proofs, many \[ and some forgotten \], many months of many ∴ and many more □. <a href="#fnref1">↩︎</a></p>
</li>
<li id="fn2"><p>Between running from Chinese class to discrete math classes, I’d be staring into the orange California sunset sky, and mumbling random Demon Slayer anime phrases to myself in Chinese, like "Lightning Breath - First Slash and Lightning" (雷之呼吸，一之型，霹雳一闪!). Oops... <a href="#fnref2">↩︎</a></p>
</li>
</ol>
</section>
<figure><img src="https://isaak.net/content/images/2024/10/498B724F-3608-415A-A150-03724B5C9385_1_201_a-1.jpeg" alt="" loading="lazy" width="2000" height="1394" srcset="https://isaak.net/content/images/size/w600/2024/10/498B724F-3608-415A-A150-03724B5C9385_1_201_a-1.jpeg 600w, https://isaak.net/content/images/size/w1000/2024/10/498B724F-3608-415A-A150-03724B5C9385_1_201_a-1.jpeg 1000w, https://isaak.net/content/images/size/w1600/2024/10/498B724F-3608-415A-A150-03724B5C9385_1_201_a-1.jpeg 1600w, https://isaak.net/content/images/size/w2400/2024/10/498B724F-3608-415A-A150-03724B5C9385_1_201_a-1.jpeg 2400w" sizes="(min-width: 720px) 720px"><figcaption><i><em>Sorry no, I’m not Steph Curry. But come chat anyway! (Qingyuan, China)</em></i></figcaption></figure><p>The beginning was by far the hardest time, and many tuned out or dropped out. But I had lots of fun. I played a lot. I wrote a horrible poem about humanity colonizing Mars. My Chinese was absolute crap, but I was improving <em>fast</em>. Chinese is my fifth language, and I had a few tricks up my sleeve:</p><h2 id="intense-self-study-%E2%80%94-%E8%87%AA%E5%BC%BA%E4%B8%8D%E6%81%AF">Intense Self-Study — 自强不息</h2><p><strong>Month 3: </strong>Spaced repetition is a superweapon. The spaced repetition app Anki is the core reason why I was able to study Chinese efficiently. Alongside Anki, I adopted other methods to learn <em>faster</em>. </p><p>Frequency-based learning. Comprehensible input. Reading lots as soon as I could, especially graded readers. Buying a calligraphy pen-brush and learned how to write the 600 Chinese characters. FSRS. Creating a 100,000-card Anki megadeck. For all the nitty-gritty language learning tips, check out <a href="https://isaak.net/mandarinmethods/" rel="noreferrer">my methods post</a>. </p><p>Early on, I started watching anime dubs like Boruto or Scissor Seven. I really enjoyed myself despite barely understanding anything. Every few minutes I collected a new word to study. The content I watched in those early days felt like colorful images with funny sounds which occasionally made sense.&nbsp;</p><figure><img src="https://lh7-rt.googleusercontent.com/docsz/AD_4nXdkzR7VAlJdrPr9X0XuxXJFcdOB9gnxfeRHyrtUpagDZ2krk5NsaA9653_HhAGhzu1L4LmF-vgN9e_rkNWXRd6CRNZex7DUk9SFfjqcITATrrGw8Nx_BcWKjrvXo06bFKYRAtk6bLBYaLLsZ_bK-LIu2ezL?key=va-1OSyREuqkqYOZib3JNw" alt="" loading="lazy" width="379" height="213"><figcaption><i><em>Colorful images with funny sounds which occasionally make sense</em></i></figcaption></figure><p>On day 70 I reached a vocabulary of 1050, of which 460 characters.<sup><a href="#fn1" id="fnref1">[1]</a></sup></p>
<hr>
<section>
<ol>
<li id="fn1"><p>I don’t study for tests and never took one. Yet, note that in the major Chinese system to track language progress, the fourth tier out of six (old HSK4) requires a vocabulary of roughly 1200. <a href="#fnref1">↩︎</a></p>
</li>
</ol>
</section>
<p>The other superweapon I implemented was personalized tutoring. My first month studying Chinese was mostly in a 20-people class. But then, I took Bloom's Two-Sigma effect to heart and got myself lots of 1-1 tutoring. The more time I spent on tutoring, the more it accelerated my studies.<sup><a href="#fn1" id="fnref1">[1]</a></sup></p>
<hr>
<section>
<ol>
<li id="fn1"><p><a href="https://en.wikipedia.org/wiki/Bloom%27s_2_sigma_problem?ref=isaak.net">Bloom’s Two-Sigma</a> effect is the phenomenon of tutored students vastly outperforming students in normal classes. <a href="#fnref1">↩︎</a></p>
</li>
</ol>
</section>
<p>There’s legends like <a href="https://www.chinese-forums.com/forums/topic/43939-independent-chinese-study-review/?ref=isaak.net"><u>Tamu</u></a> spending dozens of hours with tutors, but I’d mostly spend up to six hours a week. More would start to detract from my main focus, which were still my math studies. My default for working with tutors was to lead a "normal" conversation. I had two strict rules for conversations with tutors: 1. Only Chinese, no English. 2. Correct every single mistake I make. </p><figure><img src="https://isaak.net/content/images/2024/10/IMG_3800.jpeg" alt="" loading="lazy" width="2000" height="2042" srcset="https://isaak.net/content/images/size/w600/2024/10/IMG_3800.jpeg 600w, https://isaak.net/content/images/size/w1000/2024/10/IMG_3800.jpeg 1000w, https://isaak.net/content/images/size/w1600/2024/10/IMG_3800.jpeg 1600w, https://isaak.net/content/images/size/w2400/2024/10/IMG_3800.jpeg 2400w" sizes="(min-width: 720px) 720px"><figcaption><i><em>I definitely lost all my social anxiety after 3 days of walking around tourist-packed Hawaii beaches loudly talking broken Chinese</em></i></figcaption></figure><p>At the start, this tutoring was excruciatingly slow. But it was very worth it. After the chat, I’d ask them to send me a summary of my key mistakes and newly learned vocabulary. It’d add that to my Anki.&nbsp;</p><p>I made lots of mistakes. I still do. Tutoring gives me a tight and fast feedback loop on fixing my mistakes.</p><h2 id="traveling-%E2%80%94-%E5%AD%A6%E4%BB%A5%E8%87%B4%E7%94%A8">Traveling — 学以致用</h2><p><strong>Month 4:</strong> Winter break was approaching. I knew all the Chinese I needed to know to travel. (<em>No, sorry, I’m not LeBron James.</em>) I figured out tourist visas, and just went for it. After Christmas in chafing-lips-freezing-cold Austria, I found myself wandering around fogged-up-glasses-humid Taipei.<sup><a href="#fn1" id="fnref1">[1]</a></sup></p>
<hr>
<section>
<ol>
<li id="fn1"><p>Winter in Taiwan was actually comfortable. Summer in Shanghai was crazy. <a href="#fnref1">↩︎</a></p>
</li>
</ol>
</section>
<figure><div><p><img src="https://isaak.net/content/images/2024/10/IMG_3882.jpeg" width="2000" height="2053" loading="lazy" alt="" srcset="https://isaak.net/content/images/size/w600/2024/10/IMG_3882.jpeg 600w, https://isaak.net/content/images/size/w1000/2024/10/IMG_3882.jpeg 1000w, https://isaak.net/content/images/size/w1600/2024/10/IMG_3882.jpeg 1600w, https://isaak.net/content/images/2024/10/IMG_3882.jpeg 2316w" sizes="(min-width: 720px) 720px"></p><p><img src="https://isaak.net/content/images/2024/10/conan-glasses.png" width="883" height="768" loading="lazy" alt="" srcset="https://isaak.net/content/images/size/w600/2024/10/conan-glasses.png 600w, https://isaak.net/content/images/2024/10/conan-glasses.png 883w" sizes="(min-width: 720px) 720px"></p></div><figcaption><p><i><em>Too humid to see, too humid to sweat, too cool to break a sweat </em></i><span>😎</span></p></figcaption></figure><p>I usually like to travel alone, and then figure out things on the spot. I’d walk around with airpods in, sugar-shocked from eating too many sugar gourds, explaining to my tutor in great detail what tasty novel things I ate at the night market. When not on a tutoring call, I’d sit in cafes and study, wander around markets, or talk to locals. </p><p>Being a foreigner with passable Mandarin is... amusing. When meditating in a small Buddhist temple town in middle-of-nowhere rural Sichuan, I was a local celebrity. 300 primary schooler filed past me, who definitely hadn't seen an Austrian-African foreigner speaking Mandarin. They totally lost it. It was fun, but also tiring. Eventually I preferred to be in the cities, where being a foreigner wasn’t a miracle.&nbsp;</p><p>At least, I got a good taste of why being famous must be great for exactly three minutes, and then quite frankly horrible forever after. Not again. I’m okay, thanks.</p><p>In total, I was in Mainland China three times this year, for a total of two months.<sup><a href="#fn1" id="fnref1">[1]</a></sup> It goes without saying that every journey gave me an enormous boost in my learning pace. The first time travelling got me from <em>broken</em> to <em>comfortable in all day-to-day situations</em>. Every time I travelled I learned roughly 1000 new words/characters. Every time immensely boosted my fluent expression and listening ability.</p>
<hr>
<section>
<ol>
<li id="fn1"><p>I ended up going to China again fairly soon, in Spring break. This time I went with an adventure-hungry Austrian friend who was also learning Chinese, and we pushed each other to get better and better. In total, I’m lucky to have spent almost 2 months of this year traveling and working remote, most of that time in Shanghai. <a href="#fnref1">↩︎</a></p>
</li>
</ol>
</section>
<figure><img src="https://lh7-rt.googleusercontent.com/docsz/AD_4nXfU23htFShMc4vmV-YwC2T4CE9qS_IPMTfnOep7B9g2MA-pXbecj4NJ3BB7BfuBLmAmSQ7NXGGUTjgwjzY6ejE6umiSeMRmT-O08hyAhaBLeXgdAdBm_7TUWinNUPsVUtDOWNYN9AjYLKqsUMb9cj2C3JAh?key=va-1OSyREuqkqYOZib3JNw" alt="" loading="lazy" width="194" height="145"><figcaption><i><em>Hot single qi sources near you (Emeishan, Sichuan)</em></i></figcaption></figure><h2 id="the-marathon-%E2%80%94-%E6%8C%81%E4%B9%8B%E4%BB%A5%E6%81%92">The Marathon — 持之以恒</h2><p><strong>Month 6:</strong> My Chinese still had far to go. Apart from the study sprints while traveling, I tried to keep up a consistently high pace back at home. Chinese wasn’t my focus then — math and neuro were. Chinese was consistently the largest side project, clocking some 15 hours a week.</p><p>Consistency was the most important part to keep a high pace of progress. Here’s what a typical focused day might’ve looked like:</p><ul><li>Wake up, 1 hour of Anki</li><li>Do my main thing for 8-9 hours (math undergrad, neuro grad school, …)</li><li>1 hour tutoring call before dinner some days</li><li>1 hour of Chinese content before sleep, e.g. anime dubs or books</li></ul><p>It was quite literally the marathon. Here’s my habitually doing Anki on a treadmill:</p><figure data-kg-thumbnail="https://isaak.net/content/media/2024/10/running-short-squarish-compressed-2_thumb.jpg" data-kg-custom-thumbnail="">
            <div>
                <video src="https://isaak.net/content/media/2024/10/running-short-squarish-compressed-2.mp4" poster="https://img.spacergif.org/v1/1080x1350/0a/spacer.png" width="1080" height="1350" loop="" autoplay="" muted="" playsinline="" preload="metadata"></video>
                
                <div>
                        <p>
                        
                        <span>0:00</span></p><p>
                            /<span>0:11</span>
                        </p>
                        </div>
            </div>
            
        <img src="https://isaak.net/content/media/2024/10/running-short-squarish-compressed-2_thumb.jpg"></figure><p>Some 7 months from start, I reached 5,000 known words/characters. The old highest level (HSK6) also required a vocabulary of 5,000 (different) words. So this was an epic goal to hit.<sup><a href="#fn1" id="fnref1">[1]</a></sup></p>
<hr>
<section>
<ol>
<li id="fn1"><p>Again, I wasn't taking studying for tests — I was studying for myself. But to compare, the (old) HSK6 requiring a vocabulary of 5,000 words and characters. Most of HSK6 is business vocabulary that's not useful to me. My goal was to pass my “personal HSK6”: 5,000 words which showed up commonly in the content I watched and loved. <a href="#fnref1">↩︎</a></p>
</li>
</ol>
</section>
<p>Immersion is best done while traveling. Still, I started to immerse myself as well as I could at home. My devices would be in Chinese. I started taking some notes primarily in Mandarin. I had lots of social support throughout too: I was lucky to be able to build new relationships entirely in Chinese. For example, my grad school supervisor, a Tsinghua graduate now at MIT, was more than happy to teach me neuroscience in Chinese. I had tutors. I turned older relationships fully Chinese. This had me constantly speaking Mandarin day-to-day.</p><p><strong>Month 12: </strong>Exactly 365 days after I started, I reached a vocabulary of 8000 words and characters.</p><figure><img src="https://lh7-rt.googleusercontent.com/docsz/AD_4nXdSe8Wt-7sEziRiPjp_mO0JZ6rHEK_kdZkBDlxJZ0e-7HBiN__o1wAXc8Fsdb8_Nu20BQTQWKO841-ADHl7o7VO1RB9XEtcyuQiQXyCRD5N4jYXjDtM76t_rpyLBg7dro0U04raEzKooGKAiPfYzuND38_U?key=va-1OSyREuqkqYOZib3JNw" alt="" loading="lazy" width="624" height="63"></figure><p>8000 words and characters makes most content I encounter relatively understandable. My vocabulary is a weird personal mix: Basics including everything up to HSK5, anime vocabulary, biology, mathematics, and random everyday stuff from travelling.</p><p>Vocabulary is only one part of fluency. It's important to keep eyes on the goal: The goal of any language is to communicate effectively. Prompted for feedback on my progress, my usually reserved tutor recently commented: “This was the fastest learning pace from zero to advanced conversational fluency I have ever seen." </p><p>That's kind, but being<em> fluent </em>feels like it’ll always be an overstatement. Especially for Chinese. I’m definitely not <em>Fluent™</em>. I sometimes still get my tones wrong. Full-speed native speech is sometimes still tough. Local dialects remain a complete mystery to me.</p><p>I’d say I’m <em>comfortable </em>with Chinese. I can <em>comfortably</em> travel in any Mandarin-speaking place. I can <em>comfortably </em>hold long conversations. I can <em>comfortably </em>watch most content. I can <em>comfortably </em>build relationships entirely in Mandarin.&nbsp;</p><p>The goal? I want to reach a level where the legendary Three-Body Problem will be <em>comfortably </em>readable.</p><hr><p>Curious about more? Check out my methods post: <a href="https://isaak.net/mandarinmethods/">isaak.net/mandarinmethods</a></p>
  </article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Show HN: One – A new React framework unifying web, native and local-first (392 pts)]]></title>
            <link>https://onestack.dev</link>
            <guid>41742278</guid>
            <pubDate>Fri, 04 Oct 2024 15:15:24 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://onestack.dev">https://onestack.dev</a>, See on <a href="https://news.ycombinator.com/item?id=41742278">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>Creating websites and&nbsp;apps is simply too complex.</p><p>One is a new React framework for web and<!-- --> <span aria-expanded="false" data-state="closed" data-tint-link="blue" aria-describedby="tooltip-content" role="button" tabindex="0"><span>native</span></span>, built on Vite. It&nbsp;simplifies things with<!-- --> <span aria-expanded="false" data-state="closed" data-tint-link="green" aria-describedby="tooltip-content" role="button" tabindex="0"><span>universal</span></span>, <a href="https://onestack.dev/docs/routing" role="link">typed routing</a> seamlessly across<!-- --> <span aria-expanded="false" data-state="closed" data-tint-link="purple" aria-describedby="tooltip-content" role="button" tabindex="0"><span>static</span></span>,<!-- --> <span aria-expanded="false" data-state="closed" data-tint-link="red" aria-describedby="tooltip-content" role="button" tabindex="0"><span>server</span></span>, and<!-- --> <span aria-expanded="false" data-state="closed" data-tint-link="pink" aria-describedby="tooltip-content" role="button" tabindex="0"><span>client</span></span> <!-- -->pages. Plus, an amazing new solution&nbsp;to&nbsp;data.</p><div id="zero"><p>Local, First</p><p>Simpler code, better results, cross-platform — that's&nbsp;the goal.</p><p>With One and <a href="https://tamagui.dev/">Tamagui</a>, we're close… but there's still <em>one</em> big pain point.<!-- --> <b>Let's&nbsp;talk&nbsp;about&nbsp;data</b>.</p><p>Native apps feel better and are easier to write thanks to client-side databases. Say&nbsp;bye&nbsp;to server boundaries, lose&nbsp;the glue code, mutate instantly, and have things Just&nbsp;Work™&nbsp;offline…</p><p>So, <b>why don't we use them on&nbsp;the&nbsp;web?</b></p><p>Well, web needs small bundles, and has limited storage. Add in sync, caching, composition… there's 0 great options.</p><p>It's why we're excited to partner with<!-- --> <b><a target="_blank" href="https://zerosync.dev/" role="link">Zero</a></b> <!-- -->to include it as the default, ejectable solution to data. Zero solves for all the above <a href="https://onestack.dev/docs/data" role="link">and&nbsp;more</a>. It even works with&nbsp;Postgres.</p><p>One<!-- --> <span><svg viewBox="0 0 590 590" width="20.65" height="20.65" style="border-radius:1000px;overflow:hidden;width:20.65px;height:20.65px"><svg width="590px" height="590px" viewBox="0 0 590 590"><defs><filter x="-93.3%" y="-81.2%" width="286.7%" height="262.4%" filterUnits="objectBoundingBox" id="filter-1"><feGaussianBlur stdDeviation="22" in="SourceGraphic"></feGaussianBlur></filter><filter x="-13.5%" y="-46.9%" width="126.9%" height="193.8%" filterUnits="objectBoundingBox" id="filter-2"><feGaussianBlur stdDeviation="20" in="SourceGraphic"></feGaussianBlur></filter><filter x="-23.9%" y="-22.5%" width="147.8%" height="145.1%" filterUnits="objectBoundingBox" id="filter-3"><feGaussianBlur stdDeviation="41" in="SourceGraphic"></feGaussianBlur></filter></defs><g id="favicon" stroke="none" stroke-width="1" fill="none" fill-rule="evenodd"><g transform="translate(-0, 0)" fill-rule="nonzero"><circle id="Oval" fill="#F5CA05" cx="295" cy="295" r="295"></circle><circle id="Oval" fill="#FFFFFF" cx="311" cy="230" r="110"></circle><path d="M299.32294,286 L339.7951,281.25 C342.598367,279.138889 344,276.324074 344,272.805556 C344,269.287037 342.247958,267 338.743875,265.944444 L329.282851,265.944444 L321.398664,178.333333 C320.347439,173.055556 318.244989,172 312.988864,172 C307.732739,172 305.63029,173.583333 304.053452,175.166667 C302.476615,176.75 302.476615,182.555556 301.951002,183.611111 C301.42539,184.666667 291.438753,188.361111 287.759465,189.944444 C284.080178,191.527778 284.080178,201.555556 287.759465,203.666667 C291.438753,205.777778 298.271715,202.083333 301.42539,204.722222 L307.207127,267.527778 C299.339925,268.871363 294.784617,270.102844 293.541203,271.222222 C291.676081,272.901289 290.91314,274.388889 291.438753,279.666667 C291.789161,283.185185 294.417223,285.296296 299.32294,286 Z" id="Path-7" fill="#000000"></path><ellipse id="Oval" fill="#FFFFFF" opacity="0.453031994" filter="url(#filter-1)" transform="translate(200.0089, 137.737) rotate(46) translate(-200.0089, -137.737)" cx="200.008945" cy="137.73703" rx="35.3577818" ry="40.6350626"></ellipse><path d="M521,138 C482.503431,98.2196247 448.723549,71.1799277 419.660355,56.880909 C376.065564,35.432381 347.543959,28.4841486 295.097041,26.8563104 C242.650124,25.2284722 225.598176,37.942459 183.728994,56.880909 C155.816206,69.5065424 119.573208,92.6834255 75,126.411558 C102.798028,89.5392443 133.411045,63.0262947 166.839053,46.8727095 C216.981065,22.6423316 259.733728,10 295.097041,10 C330.460355,10 373.740828,20.0085949 428.633136,46.8727095 C465.228008,64.7821192 496.016963,95.1578827 521,138 Z" id="Path-8" fill="#FFFFFF" opacity="0.773065476" filter="url(#filter-2)"></path><path d="M361.057245,44 C431.694309,123.939704 467.935264,174.984191 469.780109,197.133462 C472.547375,230.357369 482.654123,254.819372 459.752272,321.224371 C436.85042,387.629371 415.418677,407.823985 383.224042,440.562863 C361.760952,462.388781 259.019605,478.230174 75,488.087041 C207.883501,556.029014 286.171,590 309.862498,590 C333.553996,590 368.739389,581.727273 415.418677,565.181818 C481.196175,535.021945 525.881624,499.994866 549.475024,460.10058 C584.865123,400.259152 591.955643,340.867492 589.586372,292.181818 C587.2171,243.496144 582.366118,196.314838 555.280613,172.31528 C537.223611,156.315576 472.482488,113.543815 361.057245,44 Z" id="Path-9" fill="#000000" opacity="0.0963076637" filter="url(#filter-3)"></path></g></g></svg></svg></span> <!-- -->is working to make Zero great on server and client. Our proof of concept has no flickers, waterfalls, or config.</p><p>We love it, and think you will too.</p><p><span></span><a href="https://onestack.dev/docs/data" role="link"><span>Read More</span></a></p></div><a target="_blank" href="https://testflight.apple.com/join/aNcDUHZY" role="link"><div><p><img width="80" height="80" src="https://onestack.dev/testflight.webp" alt="Testflight Icon"></p><p>Try the demo app on Testflight</p></div></a><div><p>Team</p><p>Hello. We're the creators of <a href="https://tamagui.dev/" role="link">Tamagui</a>. We built One out of our experience at<!-- --> <a href="https://app.uniswap.org/" role="link">Uniswap</a> and creating<!-- --> <a href="https://tamagui.dev/takeout" role="link">Takeout</a>.</p></div><div><p>Copyright 2024 Tamagui, LLC</p></div></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Getting my daily news from a dot matrix printer (652 pts)]]></title>
            <link>https://aschmelyun.com/blog/getting-my-daily-news-from-a-dot-matrix-printer/</link>
            <guid>41742210</guid>
            <pubDate>Fri, 04 Oct 2024 15:08:42 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://aschmelyun.com/blog/getting-my-daily-news-from-a-dot-matrix-printer/">https://aschmelyun.com/blog/getting-my-daily-news-from-a-dot-matrix-printer/</a>, See on <a href="https://news.ycombinator.com/item?id=41742210">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
            <p>For a while now I've started my day by unlocking my phone and scrolling through different news and social media sites to see what's going on. It's not exactly <em>great</em> for my mental health and I've been trying to cut down on screen time for a while. I still want to stay up-to-date though, especially after I get up in the morning.</p>
<p>I recently purchased a dot matrix printer from eBay, and thought it would be a great excuse to have a custom "front page" printed out and ready for me each day. So, that's what I built!</p>
<p>Printer ASMR noises in the video below 👇</p>
<video width="270" height="360" controls="" title="Dotmatrix Printing Example">
  <source src="https://aschmelyun.com/storage/images/blog/dotmatrix_example_2_bt709.mp4" type="video/mp4">
</video>
<p>I'll take this article to dive in and show you what I used, how I set it up, and the <strong>PHP script</strong> that powers it all.</p>
<blockquote>
<p>Interested in that full source code? Check it out on the <a href="https://github.com/aschmelyun/dotmatrix-daily-news">GitHub repo</a>!</p>
</blockquote>
<h2>Purchasing the hardware</h2>
<p>The supply list for this project was pretty small, and with the exception of the printer, most of this can be found on Amazon or other online retailers.</p>
<ul>
<li>Dot matrix printer</li>
<li>Raspberry Pi Zero W [<a href="https://vilros.com/products/raspberry-pi-zero-w-basic-starter-kit-1">link</a>]</li>
<li>Serial to USB adapter [<a href="https://www.amazon.com/dp/B00IDU0T1Y?ref=ppx_yo2ov_dt_b_fed_asin_title&amp;th=1">link</a>]</li>
<li>Power supply</li>
</ul>
<p>The printer I purchased was a <a href="https://www.computerhistory.org/collections/catalog/102666267">Star NP-10</a> from what looks like the mid-80's. I can't be 100% sure, but any dot matrix printer with a serial port should do the trick. The prices range from about $80-120 USD, but I was able to get this one for about half that price because it was marked as "unsure if working".</p>
<p>It did need a little cleaning up and some tuning of the ink ribbon cartridge (isn't that cool, it's like a typewriter!), but after that it fired right up and ran through the test page print.</p>
<p>After that, I hooked everything up. The Raspberry Pi is connected to my WiFi, and then via USB to the serial port of the printer. After turning on the printer and <code>ssh</code>ing into the Pi, I can verify that the printer is available at <code>/dev/usb/lp0</code>.</p>
<p>Now, <strong>how do I get this thing to print?</strong></p>
<h2>Figuring out the printer's code</h2>
<p>Because the printer is available at <code>lp0</code> I wanted to see if I could just echo raw text to it and have it come through the printer. So I ran the following:</p>
<pre><code data-theme="material-theme-palenight" data-lang="bash"><!-- Syntax highlighted by torchlight.dev --><p><span>echo</span><span> </span><span>"</span><span>Hello, world!</span><span>"</span><span> </span><span>&gt;</span><span> </span><span>/dev/usb/lp0</span></p></code></pre>
<p>Which resulted in an error that the file couldn't be accessed. Bummer, a permissions issue. Easily fixed though with some <code>chmod</code>'ing:</p>
<pre><code data-theme="material-theme-palenight" data-lang="bash"><!-- Syntax highlighted by torchlight.dev --><p><span>sudo</span><span> </span><span>chmod</span><span> </span><span>666</span><span> </span><span>/dev/usb/lp0</span></p></code></pre>
<p>There might be a better way to handle that, but it allowed my echo to go through, and I saw the text available on the printer! Alright, I can send raw data to the printer via this file, so let's find a way to scale this up.</p>
<p>I use PHP as my language of choice in a day-to-day basis, and this is no exception. I write a basic script that accesses the file through <code>fopen()</code> and starts writing text to it. I try a few sentences, some spacing, and then some unicode art, but quickly find out that there's not as much character support on the printer as I was sending.</p>
<p><img src="https://aschmelyun.com/storage/images/blog/dotmatrix-encoding-errors.jpg" alt="Picture of a printed sheet showing a bunch of wrongly-encoded characters"></p>
<p>So I thought it was about time that I start digging into how this thing <em>actually works</em>. Thanks to the hard work and dedication of internet hoarders, I found a <a href="https://www.minuszerodegrees.net/manuals/Star%20Micronics/dot_matrix/Star%20Micronics%20-%20NP-10%20-%20Users%20Manual.pdf">full manual for the printer</a> scanned and uploaded as a PDF.</p>
<p>Come to find out, either because of the age or just the manufacturing decision, this printer has a <strong>very specific character set</strong> that it accepts. Loosely based off of the IBM PC's <a href="https://en.wikipedia.org/wiki/Code_page_437">Code Page 437</a> it consists mostly of your standard alpha-numeric characters, but with a small set of special symbols, lines, and boxes. Neat!</p>
<p>Sending these characters to the printer is pretty straightforward, I can just echo out the hex values with PHP like so:</p>
<pre><code data-theme="material-theme-palenight" data-lang="php"><!-- Syntax highlighted by torchlight.dev --><p><span>$</span><span>horizontalDouble </span><span>=</span><span> </span><span>"</span><span>\xCD</span><span>"</span><span>;</span></p><p><span>$</span><span>deg </span><span>=</span><span> </span><span>"</span><span>\xF8</span><span>"</span><span>;</span></p><p><span>echo</span><span> </span><span>str_repeat</span><span>($</span><span>horizontalDouble</span><span>,</span><span> </span><span>24</span><span>);</span></p><p><span>echo</span><span> </span><span>'</span><span>78</span><span>'</span><span> </span><span>.</span><span> </span><span>$</span><span>deg </span><span>.</span><span> </span><span>'</span><span>F</span><span>'</span><span> </span><span>.</span><span> PHP_EOL</span><span>;</span></p></code></pre>
<p>Alright, so I'm able to write text to the printer just fine, and include some special characters and design symbols. Now I need to figure out <em>what</em> I want to see every morning.</p>
<h2>Gathering the data</h2>
<p>I knew I wanted four distinct sections for my personal front page: <strong>weather, stocks, major news headlines, and a few top reddit posts</strong>. After all, that's usually what I end up look at on my phone in the morning.</p>
<p>Additionally, since this is an experimental project, I wanted to remain super cheap for this data, free if at all possible. Thankfully there's an amazing <a href="https://github.com/public-apis/public-apis">GitHub repo</a> for free and public APIs, so I just went through there and found the ones I needed.</p>
<ul>
<li>The weather pulls from <a href="https://open-meteo.com/en/docs">Open-Meteo</a> and no API key is needed</li>
<li>Stocks data pulls from <a href="https://twelvedata.com/docs">twelvedata</a> that offers a generous free tier</li>
<li>News headlines pull from <a href="https://developer.nytimes.com/get-started">NYTimes</a> which has a decent free tier, good enough for this project</li>
<li>Reddit posts pull from <a href="https://www.reddit.com/r/redditdev/comments/rvqirc/how_to_get_reddit_api_data_using_json/">Reddit JSON</a> which is free (but I had to spoof my User-Agent)</li>
</ul>
<p>For each of the sections, I wrote some basic PHP code to pull in the payload from the API endpoint and compile the data I wanted into a larger overall array. I only wanted specific stocks, types of headlines, and subreddit posts, and if any of the sections couldn't have data to present I just simply crash the script early so I can start it again at a later time.</p>
<p>This can be seen in this snippet which I use for pulling news headlines:</p>
<pre><code data-theme="material-theme-palenight" data-lang="php"><!-- Syntax highlighted by torchlight.dev --><p><span>// Get news headlines data</span></p><p><span>echo</span><span> </span><span>"</span><span>Fetching news headlines data...</span><span>"</span><span> </span><span>.</span><span> PHP_EOL</span><span>;</span></p><p><span>$</span><span>newsUrl </span><span>=</span><span> NEWS </span><span>.</span><span> </span><span>"</span><span>?api-key=</span><span>"</span><span> </span><span>.</span><span> NEWSKEY</span><span>;</span></p><p><span>$</span><span>newsData </span><span>=</span><span> </span><span>[];</span></p><p><span>$</span><span>newsAmount </span><span>=</span><span> </span><span>0</span><span>;</span></p><p><span>$</span><span>data </span><span>=</span><span> </span><span>json_decode</span><span>(</span><span>file_get_contents</span><span>($</span><span>newsUrl</span><span>),</span><span> </span><span>true);</span></p><p><span>if</span><span> </span><span>(!</span><span>isset</span><span>($</span><span>data</span><span>[</span><span>'</span><span>results</span><span>'</span><span>]))</span><span> </span><span>{</span></p><p><span>    </span><span>die</span><span>(</span><span>"</span><span>Unable to retrieve news data</span><span>"</span><span>);</span></p><p><span>}</span></p><p><span>foreach</span><span> </span><span>($</span><span>data</span><span>[</span><span>'</span><span>results</span><span>'</span><span>]</span><span> </span><span>as</span><span> </span><span>$</span><span>article</span><span>)</span><span> </span><span>{</span></p><p><span>    </span><span>if</span><span> </span><span>(</span></p><p><span>        </span><span>($</span><span>article</span><span>[</span><span>'</span><span>type</span><span>'</span><span>]</span><span> </span><span>===</span><span> </span><span>'</span><span>Article</span><span>'</span><span>)</span><span> </span><span>&amp;&amp;</span></p><p><span>        </span><span>(</span><span>in_array</span><span>($</span><span>article</span><span>[</span><span>'</span><span>section</span><span>'</span><span>],</span><span> </span><span>[</span><span>'</span><span>U.S.</span><span>'</span><span>,</span><span> </span><span>'</span><span>World</span><span>'</span><span>,</span><span> </span><span>'</span><span>Weather</span><span>'</span><span>,</span><span> </span><span>'</span><span>Arts</span><span>'</span><span>]))</span><span> </span><span>&amp;&amp;</span></p><p><span>        </span><span>($</span><span>newsAmount </span><span>&lt;</span><span> MAXNEWS</span><span>)</span></p><p><span>    </span><span>)</span><span> </span><span>{</span></p><p><span>        </span><span>$</span><span>newsData</span><span>[]</span><span> </span><span>=</span><span> </span><span>$</span><span>article</span><span>;</span></p><p><span>        </span><span>$</span><span>newsAmount</span><span>++;</span></p><p><span>    </span><span>}</span></p><p><span>}</span></p></code></pre>
<p>The <code>NEWS</code>, <code>NEWSKEY</code>, and <code>MAXNEWS</code> variables are all constants instantiated at the top of the script for easy editing.</p>
<p>Running this compiles everything I want to see displayed on the paper, but now I need to take on the actual task of formatting everything for the printer, and sending it the raw data.</p>
<h2>Printing out my front page</h2>
<p>I could just print out a heading for each section, but that's a little boring. I wanted a bit of <strong><em>flair</em></strong> to the project, so I decided to have a box at the top displaying the current date, day of the week, and my front page name all nicely bordered.</p>
<p>It took a little math, but I got everything working by using a combination of the hex values I talked about above, <code>str_repeat</code> and the knowledge that the page width for this printer is <strong>80 characters</strong>.</p>
<p>Now, just simply go through each section, print a little heading:</p>
<pre><code data-theme="material-theme-palenight" data-lang="php"><!-- Syntax highlighted by torchlight.dev --><p><span>str_repeat</span><span>($</span><span>horizontalSingle</span><span>,</span><span> </span><span>3</span><span>)</span><span> </span><span>.</span><span> </span><span>"</span><span> WEATHER </span><span>"</span><span> </span><span>.</span><span> </span><span>str_repeat</span><span>($</span><span>horizontalSingle</span><span>,</span><span> </span><span>(</span><span>PAGEWIDTH </span><span>-</span><span> </span><span>9</span><span>))</span><span> </span><span>.</span><span> </span><span>"</span><span>\n</span><span>"</span><span>;</span></p></code></pre>
<p>And then print out the data that I want displayed for that section:</p>
<pre><code data-theme="material-theme-palenight" data-lang="php"><!-- Syntax highlighted by torchlight.dev --><p><span>"</span><span>   </span><span>"</span><span> </span><span>.</span><span> </span><span>round</span><span>(($</span><span>weatherData</span><span>[</span><span>'</span><span>daily</span><span>'</span><span>][</span><span>'</span><span>daylight_duration</span><span>'</span><span>][</span><span>0</span><span>]</span><span> </span><span>/</span><span> </span><span>3600</span><span>),</span><span> </span><span>2</span><span>)</span><span> </span><span>.</span><span> </span><span>"</span><span>h of Sunlight  -  Sunrise: </span><span>"</span><span> </span><span>.</span><span> </span><span>date</span><span>(</span><span>'</span><span>g:ia</span><span>'</span><span>,</span><span> </span><span>strtotime</span><span>($</span><span>weatherData</span><span>[</span><span>'</span><span>daily</span><span>'</span><span>][</span><span>'</span><span>sunrise</span><span>'</span><span>][</span><span>0</span><span>]))</span><span> </span><span>.</span><span> </span><span>"</span><span>  -  Sunset: </span><span>"</span><span> </span><span>.</span><span> </span><span>date</span><span>(</span><span>'</span><span>g:ia</span><span>'</span><span>,</span><span> </span><span>strtotime</span><span>($</span><span>weatherData</span><span>[</span><span>'</span><span>daily</span><span>'</span><span>][</span><span>'</span><span>sunset</span><span>'</span><span>][</span><span>0</span><span>]))</span><span> </span><span>.</span><span> </span><span>"</span><span>\n</span><span>"</span><span>;</span></p></code></pre>
<p>For the weather and stocks, I knew I wouldn't hit the edge of the paper so I just wrote everything in single long lines. But that's different for the news headlines and Reddit posts.</p>
<p>If I just feed the printer one long line of text, it's smart enough to cut it and start printing on another line. But, I didn't want a word getting cut off in the middle and starting on the next line. So I implemented a small function to handle line length and instead return back an array of lines with a max length corresponding to the page width.</p>
<pre><code data-theme="material-theme-palenight" data-lang="php"><!-- Syntax highlighted by torchlight.dev --><p><span>function</span><span> </span><span>splitString</span><span>($</span><span>string</span><span>,</span><span> </span><span>$</span><span>maxLength </span><span>=</span><span> PAGEWIDTH</span><span>)</span><span> </span><span>{</span></p><p><span>    </span><span>$</span><span>result </span><span>=</span><span> </span><span>[];</span></p><p><span>    </span><span>$</span><span>words </span><span>=</span><span> </span><span>explode</span><span>(</span><span>'</span><span> </span><span>'</span><span>,</span><span> </span><span>$</span><span>string</span><span>);</span></p><p><span>    </span><span>$</span><span>currentLine </span><span>=</span><span> </span><span>''</span><span>;</span></p><p><span>    </span><span>foreach</span><span> </span><span>($</span><span>words </span><span>as</span><span> </span><span>$</span><span>word</span><span>)</span><span> </span><span>{</span></p><p><span>        </span><span>if</span><span> </span><span>(</span><span>strlen</span><span>($</span><span>currentLine </span><span>.</span><span> </span><span>$</span><span>word</span><span>)</span><span> </span><span>&lt;=</span><span> </span><span>$</span><span>maxLength</span><span>)</span><span> </span><span>{</span></p><p><span>            </span><span>$</span><span>currentLine </span><span>.=</span><span> </span><span>($</span><span>currentLine </span><span>?</span><span> </span><span>'</span><span> </span><span>'</span><span> </span><span>:</span><span> </span><span>''</span><span>)</span><span> </span><span>.</span><span> </span><span>$</span><span>word</span><span>;</span></p><p><span>        </span><span>}</span><span> </span><span>else</span><span> </span><span>{</span></p><p><span>            </span><span>if</span><span> </span><span>($</span><span>currentLine</span><span>)</span><span> </span><span>{</span></p><p><span>                </span><span>$</span><span>result</span><span>[]</span><span> </span><span>=</span><span> </span><span>$</span><span>currentLine</span><span>;</span></p><p><span>                </span><span>$</span><span>currentLine </span><span>=</span><span> </span><span>$</span><span>word</span><span>;</span></p><p><span>            </span><span>}</span><span> </span><span>else</span><span> </span><span>{</span></p><p><span>                </span><span>// If a single word is longer than maxLength, split it</span></p><p><span>                </span><span>$</span><span>result</span><span>[]</span><span> </span><span>=</span><span> </span><span>substr</span><span>($</span><span>word</span><span>,</span><span> </span><span>0</span><span>,</span><span> </span><span>$</span><span>maxLength</span><span>);</span></p><p><span>                </span><span>$</span><span>currentLine </span><span>=</span><span> </span><span>substr</span><span>($</span><span>word</span><span>,</span><span> </span><span>$</span><span>maxLength</span><span>);</span></p><p><span>            </span><span>}</span></p><p><span>        </span><span>}</span></p><p><span>    </span><span>}</span></p><p><span>    </span><span>if</span><span> </span><span>($</span><span>currentLine</span><span>)</span><span> </span><span>{</span></p><p><span>        </span><span>$</span><span>result</span><span>[]</span><span> </span><span>=</span><span> </span><span>$</span><span>currentLine</span><span>;</span></p><p><span>    </span><span>}</span></p><p><span>    </span><span>return</span><span> </span><span>$</span><span>result</span><span>;</span></p><p><span>}</span></p></code></pre>
<p>Then I can just use it like so:</p>
<pre><code data-theme="material-theme-palenight" data-lang="php"><!-- Syntax highlighted by torchlight.dev --><p><span>foreach</span><span> </span><span>(</span><span>splitString</span><span>($</span><span>redditPost</span><span>)</span><span> </span><span>as</span><span> </span><span>$</span><span>line</span><span>)</span><span> </span><span>{</span></p><p><span>    </span><span>fwrite</span><span>($</span><span>printer</span><span>,</span><span> </span><span>$</span><span>line</span><span>)</span><span> </span><span>.</span><span> </span><span>"</span><span>\n</span><span>"</span><span>;</span></p><p><span>}</span></p></code></pre>
<p>Now all that's left to do is run the script!</p>
<h2>Usage and wrapping up</h2>
<p>I can fire off the printer manually by just running <code>php print.php</code> but I've instead set up a cron job to handle it for me.</p>
<p>Every morning at around 8am it starts printing out my personalized front page. I rip it off and check it out in the morning while drinking my coffee.</p>
<p><img src="https://aschmelyun.com/storage/images/blog/dotmatrix-example-print.jpg" alt="Example page printed"></p>
<p>As silly as it might sound, it just feels better having that finite amount of news on a single sheet of paper. Of being able to stop there instead of endlessly scrolling through websites and social media apps.</p>
<p>Also, this was a super fun project and I'm hoping I can find more uses for this dot matrix printer. Working with physical hardware (especially older specimens like this) is always a blast for me, and being able to integrate them with new technology or use them in interesting ways just ignites pure passion and reinforces why I became a programmer in the first place.</p>
<p>So, what do you think? If you have any ideas for projects like this, or just have a question or comment, I'd love to hear it! Catch me on <a href="https://twitter.com/aschmelyun">Twitter</a> if you'd like to chat more.</p>


            <div>
                
                <p>Subscribe using the form below and about 1-2 times a month you'll receive an email containing helpful hints, new packages, and interesting articles I've found on PHP, JavaScript, Docker and more.</p>
                
            </div>
        </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[ESP32: leaving love notes and entering demoscene territory (2022) (129 pts)]]></title>
            <link>https://theor.xyz/esp32-love-notes-demoscene/</link>
            <guid>41741614</guid>
            <pubDate>Fri, 04 Oct 2024 14:10:57 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://theor.xyz/esp32-love-notes-demoscene/">https://theor.xyz/esp32-love-notes-demoscene/</a>, See on <a href="https://news.ycombinator.com/item?id=41741614">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-astro-cid-bvzihdzo=""><article data-astro-cid-bvzihdzo=""><h3 data-astro-cid-bvzihdzo="">A treatise on scope-creep and rabbit holes</h3><!-- {JSON.stringify(Astro.props)} --><!-- {pubDate && <FormattedDate date={pubDate} />}
        {
          updatedDate && (
            <div class="last-updated-on">
              Last updated on <FormattedDate date={updatedDate} />
            </div>
          )
        } --><hr data-astro-cid-bvzihdzo=""><nav><h2 id="table-of-content">Table of content</h2><ol><li><a href="#a-very-sensible-plan">A very sensible plan</a></li><li><a href="#iteration-speed-flashing-times-and-ota">Iteration speed: flashing times and OTA</a><ol><li><a href="#a-dead-end-wasm3-runtime">A dead-end: wasm3 runtime</a></li><li><a href="#a-simulator-with-sdl2">A simulator with SDL2</a></li><li><a href="#gnu-rocket-demoscene-tracker">GNU rocket: demoscene tracker</a></li><li><a href="#rocket-editor">Rocket editor</a></li></ol></li><li><a href="#upgrading-to-a-color-display">Upgrading to a color display</a><ol><li><a href="#tooling-for-palette-indexed-bitmaps">Tooling for palette-indexed bitmaps</a></li><li><a href="#rle-compression-and-delta-encoding-algorithm">RLE compression and delta encoding algorithm</a></li></ol></li><li><a href="#adding-sound-">Adding sound ?</a></li><li><a href="#laser-cutting-a-front-panel">Laser-Cutting a Front Panel</a></li><li><a href="#conclusion--on-scope-creep-and-rabbit-holes">Conclusion : On Scope-Creep and Rabbit Holes</a></li></ol></nav>





<p>I’ve been tinkering with arduino-like platforms recently (more on the midi synth soon) and got the idea to make a small message board to leave notes to my girlfriend. This is how I ended up with a simulator, an integration with a demoscene tracker, custom tools to make palette cycling effects, having to learn CAD to make a laser-cut front panel and more.</p>
<p><img src="https://theor.xyz/_astro/banner.1a01109f_ZulFlx.webp" alt="Result" width="1920" height="1080" loading="lazy" decoding="async"></p>
<h2 id="a-very-sensible-plan"><a data-link="" href="#a-very-sensible-plan"><span></span></a>A very sensible plan</h2>
<p>At first, I had a simple goal: Use an ESP32 (240mhz, 512k RAM, luxury !) and a SSD1306 display (128x64, monochrome). Use the ESP WiFi to change the message remotely. That was easy enough : the SSD1306 is supported by the <a href="https://learn.adafruit.com/adafruit-gfx-graphics-library">Adafruit-GFX</a> library, which makes simple rendering trivial.</p>
<p>I initially pulled the messages from a github-hosted txt file, which required a constant WiFi connection. I then reversed the whole thing and got the ESP32 to store the message in its EEPROM. Holding a ‘secret’ capacitive button starts a small web server with a primitive form to change the message. Not having to wait for a connection skips a 3-15 seconds loading time at start-up. This is the web server package: <a href="https://github.com/me-no-dev/ESPAsyncWebServer">https://github.com/me-no-dev/ESPAsyncWebServer</a></p>
<p>To avoid adding a power button, the board goes to sleep : when hibernating, it consumes less than 5 uA. The capacitive button on the front wakes it up.</p>
<p>I then started experimenting with fancier scenes : at first, particles showering the message, then I got into animations and…</p>
<h2 id="iteration-speed-flashing-times-and-ota"><a data-link="" href="#iteration-speed-flashing-times-and-ota"><span></span></a>Iteration speed: flashing times and OTA</h2>
<p>At this point, the flashing time got into the 40s range. To iterate on animation timing and sprite positioning, it’s less than ideal ; working at Unity, it’s like having to recompile the engine when moving an object.</p>
<p>I discovered the ESP32 it flashable over-the-air (OTA) using WiFi or bluetooth. There’s a neat package adding OTA upload to the web server: <a href="https://github.com/ayushsharma82/AsyncElegantOTA">AsyncElegantOTA</a> . You can then ’curl’ the firmware after the build to it.</p>
<h2 id="a-dead-end-wasm3-runtime"><a data-link="" href="#a-dead-end-wasm3-runtime"><span></span></a>A dead-end: wasm3 runtime</h2>
<p>The ESP32 has a nice little file system using the flash as storage, <a href="https://docs.espressif.com/projects/esp-idf/en/latest/esp32/api-reference/storage/spiffs.html">SPIFFS</a>. An interpreter loading code from there would completely circumvent the flashing step, leaving only a (quicker) file upload. Lua, MicroPython etc… seemed heavier than needed given the task at hand. WebAssembly is a great candidate, and lets the user choose which language to use, as long as it compiles to WASM.</p>
<p>I have a prototype using the fantastic <a href="https://github.com/wasm3/wasm3">Wasm3 interpreter</a>, which can run on embedded mcus : see the <a href="https://github.com/wasm3/embedded-wasm-apps">Embedded Wasm Apps</a> repo. I used <a href="https://www.assemblyscript.org/">AssemblyScript</a>, which is basically typescript-to-wasm, to output a web assembly binary, that would get ’curl’ed to the web-server, giving me hot-reloading on save. Quite a nice workflow ! The one painful part is having to write bindings manually. I didn’t find a tool to do so ; I’m sure it will happen, if it hasn’t already. A project close to my goals is <a href="https://wasm4.org/">WASM-4</a>, which I definitely need to investigate later.</p>
<p>We’re getting into <a href="https://en.wikipedia.org/wiki/Fantasy_video_game_console">Fantasy console</a> territory : if you’re interested, here is a short list of interesting platforms:</p>
<ul>
<li><a href="https://www.lexaloffle.com/pico-8.php">Pico8</a> is the most impressive one - it doesn’t run on embedded, but has an integrated sprite and tile editor, a music tracker, etc. Paid, closed source</li>
<li><a href="https://pixelvision8.itch.io/pv8">PixelVision8</a> is an open-source C# platform, quite close to Pico8 in terms of features. Supports Lua too, no embedded either</li>
<li><a href="https://wasm4.org/">Wasm4</a> apparently runs on MCUs: see <a href="https://twitter.com/alvaroviebrantz/status/1518343016011943939">Twitter</a></li>
</ul>
<h2 id="a-simulator-with-sdl2"><a data-link="" href="#a-simulator-with-sdl2"><span></span></a>A simulator with SDL2</h2>
<p>In parallel, I started working on a desktop simulator - a SDL2 window displaying the same scenes. I copy-pasted files and hacked them around until it worked. It gives me an even faster iteration loop AND debugging, at the cost of accuracy (at least in terms of performance).</p>
I still had to restart the app every time
I changed a timing/position/…, but it’s a 2-second long loop. Better !
<h2 id="gnu-rocket-demoscene-tracker"><a data-link="" href="#gnu-rocket-demoscene-tracker"><span></span></a>GNU rocket: demoscene tracker</h2>
<p>I then got a flashback : years ago, I stumbled upon <a href="https://github.com/rocket/rocket">GNU Rocket</a>, which is a “sync-tracker”, a kind of primitive video editing tool. Your program/demo declares a bunch of tracks : a track outputs a float value that varies according to time, eg. interpolating the x axis of a sprite position from 0 to 10 between the frames 12 and 36.</p>
<p>At authoring time, the Rocket client will connect to an editor, in which you can scrub time and edit values :</p>

<p>From the editor, you can then send an ’export’ command. It will save the data, in my case on the SPIFFS file system. The rocket client, if not connected to an editor, will open these files and play them, which is what I do in my “production” build.</p>
<p>In the simplest case, I have the client step linearly through time - it just moves forward. In some cases, the time change is itself an artefact of user interactions, allowing to rewind the timeline ; in another case, I have a rocket track which is itself interpreted as playing/pausing/looping the timeline.</p>
<h2 id="rocket-editor"><a data-link="" href="#rocket-editor"><span></span></a>Rocket editor</h2>
<p>The bundled editor uses Qt. It’s a great framework ; I have scars from building it.</p>
<p>I found [Emoon’s editor], which is a SDL2 app. Great, not DPI-aware, very small font. I eventually hacked <a href="https://github.com/edoreshef/ground-control">Ground Control</a> (fork <a href="https://github.com/theor/ground-control">here on my github</a>) to add font scaling, a more readable font and color palette (IMO) and a bunch of small fixes:</p>

<h2 id="upgrading-to-a-color-display"><a data-link="" href="#upgrading-to-a-color-display"><span></span></a>Upgrading to a color display</h2>
<p>Monochrome, 128x64 is limiting - I aimed for a grayscale display initially, but eventually settled on a <a href="https://www.adafruit.com/product/1431">SSD1351</a> OLED display, 128*128px by 16-bit colors.</p>
<p>My first realization was that, if Adafruit-GFX’s SSD1306 implementation was buffered, the 1351 one wasn’t. I switched to <a href="https://github.com/Bodmer/TFT_eSPI">TFT-eSPI</a>, a more complex but more optimized library. It has a sprite class, that I use as a back buffer. The scene writes everything there, then it’s sent once to the actual display.</p>
<p>I settled on 4-bit indexed bitmaps, with a 16 colors palette <em>per bitmap</em>, meaning I can have more than 16 colors on the screen in the end. The raw power of the ESP is enough to compensate for the lack of dedicated graphics hardware and gives me &gt;40fps pre-optimization.
Then I hit the usual wall: making art is hard and not what I’m good at. So as usual, I threw more code at it, on top of using the right tools for the job.</p>

<p><a href="https://www.aseprite.org/">aseprite</a> is a fantastic pixel art editor and supports both indexed palettes and animation. It allowed me to reuse bitmaps I found and constrain them to 16 colors. I then wrote a small WPF tool (old habits…) to visualize palette swaps, eg. interpolating the palette to fake a day-night cycle:</p>

<p>It also exports the palette and the bitmap as C header files, ready to be included in the project:</p>
<pre tabindex="0"><code><span><span>static</span><span> </span><span>const</span><span> </span><span>uint16_t</span><span> </span><span>river1_palette</span><span>[</span><span>16</span><span>]</span><span> </span><span>=</span><span> </span><span>{</span></span>
<span><span>    </span><span>0x</span><span>0841</span><span>,</span><span> </span><span>0x</span><span>FFFF</span><span>,</span><span> </span><span>0x</span><span>5901</span><span>,</span><span> </span><span>0x</span><span>7182</span><span>,</span></span>
<span><span>    </span><span>0x</span><span>CE17</span><span>,</span><span> </span><span>0x</span><span>8C0F</span><span>,</span><span> </span><span>0x</span><span>62EB</span><span>,</span><span> </span><span>0x</span><span>3145</span><span>,</span></span>
<span><span>    </span><span>0x</span><span>0000</span><span>,</span><span> </span><span>0x</span><span>0000</span><span>,</span><span> </span><span>0x</span><span>0000</span><span>,</span><span> </span><span>0x</span><span>0000</span><span>,</span></span>
<span><span>    </span><span>0x</span><span>0000</span><span>,</span><span> </span><span>0x</span><span>0000</span><span>,</span><span> </span><span>0x</span><span>0000</span><span>,</span><span> </span><span>0x</span><span>0000</span></span>
<span><span>}</span><span>;</span></span></code></pre>
<pre tabindex="0"><code><span><span>// 222 x 39 px</span></span>
<span><span>static</span><span> </span><span>const</span><span> </span><span>uint8_t</span><span> </span><span>bmp_river1</span><span>[</span><span>1545</span><span>]</span><span> PROGMEM </span><span>=</span><span> </span><span>{</span></span>
<span><span>    </span><span>0x</span><span>06</span><span>,</span><span> </span><span>0x</span><span>00</span><span>,</span><span> </span><span>0x</span><span>19</span><span>,</span><span> </span><span>0x</span><span>01</span><span>,</span><span> </span><span>0x</span><span>B1</span><span>,</span><span> </span><span>0x</span><span>DD</span><span>,</span><span> </span><span>0x</span><span>01</span><span>,</span><span> </span><span>0x</span><span>01</span><span>,</span></span>
<span><span>    // ...</span></span>
<span><span>}</span></span>
<span></span>
<span><span>static</span><span> </span><span>const</span><span> SpriteInfo spr_river1 </span><span>=</span><span> </span><span>{</span><span> bmp_river1</span><span>,</span><span> </span><span>222</span><span>,</span><span>39</span><span>,</span><span> </span><span>1545</span><span> </span><span>}</span><span>;</span></span></code></pre>
<p>Palette swapping and cycling are decades-old tricks - see the <a href="https://rasterscroll.com/mdgraphics/graphical-effects/palette-swapping/">RasterScroll Sega MegaDrive/Genesis doc here</a>.</p>
<p>I highly recommend that <a href="https://www.youtube.com/watch?v=aMcJ1Jvtef0">GDC talk from Mark Ferrari</a>, artist on The Secret of Monkey Island and, more recently, <a href="https://store.steampowered.com/app/569860/Thimbleweed_Park/">Thimbleweed Park</a>, going into details about those kinds of tricks and the tooling to make them. If you just want a quick demo, see his <a href="http://www.effectgames.com/demos/canvascycle/">palette cycling demo here</a>. A quick sample:</p>

<h2 id="rle-compression-and-delta-encoding-algorithm"><a data-link="" href="#rle-compression-and-delta-encoding-algorithm"><span></span></a>RLE compression and delta encoding algorithm</h2>
<p>Let’s consider another background image: ’222*89 px / 2 (4 bits per pixel) = 9879b’. Which is alright given the space available on the ESP32 but it does slow down the flashing (assets as SPIFFS files/hot reloading is something I forbade myself to get into) and… I just could not let it go.</p>
<p>I went with a simple RLE encoding. That frame goes from 9879 bytes to 2279 bytes, 23% of the original size.</p>
<p>I also have animations :</p>
<p><img src="https://theor.xyz/_astro/frames.e0d08955_ZesJH8.webp" alt="Frames" width="1626" height="1404" loading="lazy" decoding="async"></p>
<p>Instead of storing the whole frame every time, I decided to just store the delta to the first frame. Matching tooling:</p>
<p><img src="https://theor.xyz/_astro/frames-delta.8cb9ccd6_aDPJG.webp" alt="Delta to the first frame" width="1626" height="1404" loading="lazy" decoding="async"></p>
<p>Each subsequent frame went down to ~100 bytes on average. Quite a gain.</p>
<p>It’s not my first foray into compression : see the <a href="https://theor.xyz/unicode-graphs/">URL encoding of my unicode graph editor</a></p>
<h2 id="adding-sound-"><a data-link="" href="#adding-sound-"><span></span></a>Adding sound ?</h2>
<p>I’m now experimenting with the <a href="https://github.com/earlephilhower/ESP8266Audio">ESP8266Audio</a> lib (which also works with the ESP32). Very promising - it even supports .mod files. I’ m running it through an <a href="https://www.adafruit.com/product/3885">Adafruit STEMMA Speaker</a>, which is way enough. More to come later about that</p>
<h2 id="laser-cutting-a-front-panel"><a data-link="" href="#laser-cutting-a-front-panel"><span></span></a>Laser-Cutting a Front Panel</h2>
<p>I recently found that the fantastic <a href="https://www.lespacemaker.com/">lespacemaker</a> maker space is really close to my place - and they have a laser cutter. First draft using acrylic : <img src="https://theor.xyz/_astro/panel.8b5bc6aa_Zz77Hb.webp" alt="Acrylic front panel" width="1698" height="1150" loading="lazy" decoding="async"> Maybe I’ll make it a full case eventually… This was made using <a href="https://cadquery.readthedocs.io/en/latest/">CadQuery</a>, a python lib to CAD using code. I did try Fusion, Sketch up and more (too button-clicky, found myself dreaming about writing a plugin), openscad (too slow) and a few others, but in the end…</p>
<h2 id="conclusion--on-scope-creep-and-rabbit-holes"><a data-link="" href="#conclusion--on-scope-creep-and-rabbit-holes"><span></span></a>Conclusion : On Scope-Creep and Rabbit Holes</h2>
<p>No one knowing me will be surprised that this whole thing went down a deep and twisted rabbit hole. That’s alright ; unlike a few years ago, I eventually “shipped” something that ended up our very own kitchen counter. I also learned a lot in a various fascinating subjects (I2C vs SPI, capacitive buttons debouncing, palette cycling, etc.).</p>
<p>All of that definitely happened because it wasn’t a work project.</p>
<p>I think scope creep is alright when learning ; it gives you a wider rather than deeper knowledge, which can be valuable too. <em>Controlled</em> scope creep can lead to better products, at a greater risk. The risk evaluation is the critical part here.</p></article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Why Is Light So Fast? (127 pts)]]></title>
            <link>https://profmattstrassler.com/2024/10/01/why-is-the-speed-of-light-so-fast-part-1/</link>
            <guid>41741333</guid>
            <pubDate>Fri, 04 Oct 2024 13:46:28 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://profmattstrassler.com/2024/10/01/why-is-the-speed-of-light-so-fast-part-1/">https://profmattstrassler.com/2024/10/01/why-is-the-speed-of-light-so-fast-part-1/</a>, See on <a href="https://news.ycombinator.com/item?id=41741333">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-id="841aeed" data-element_type="widget" data-widget_type="post-comments.theme_comments">
				<!-- .comment-content -->

							<!-- .comment-body -->
		<!-- #comment-## -->
<!-- .children -->
<!-- #comment-## -->
<!-- .children -->
<!-- #comment-## -->
<!-- .children -->
<!-- #comment-## -->
		<li id="comment-458118">
			<article id="div-comment-458118">
				<!-- .comment-meta -->

				<div>
					<p>The law of conservation of energy states that the total energy of an isolated system remains constant; it is said to be conserved over time, however it further states that energy can neither be created nor destroyed – only converted from one form of energy to another. In other words Energy and mass have always existed, so yo cannot create something out of nothing but from the available mass and energy that has always existed. To my thinking and agreeable to Principles of Relativity, that even the so called vacuum of space or the so called ether is a form or energy or mass (so far unexplained as dark matter) as it has always existed, even if in another form (call it dark matter). As a theologian, I see the Wavicles as (organized power) from an intelligent being, that passes through all energy and mass under two laws. 1) To GOVERN His creatures, according to immutable laws (elements that are given shape, image and mass and which eventually will be converted to another (energy/mass) within a higher sphere/dominion/principality/dimension or realm. 2) To CONTROL the lower (non intelligent elements that make up mass as dust, plants, gas, planets, stars quasars, Pulsars etc, according to laws.<br>
There is no place where there is no law, neither is there no place where there is no mass or energy.<br>
There is a place currently unknown to most intelligent beings which is indicated as the ‘Nucleus of all Nuclei” of which I will defer to legitimise until a later time. In every discourse you make either in your book , podcast, Youtube or any medium, I see all you work as easily converted into theology. I certainly lack the intelligence as far as the mathematical equations that you have attained, never theless, your presentations make perfect sense to me in another bandwidth. Thanking you for your energy in helping many see beyond their limited horizons. Kind regards  Joseph</p>

				</div><!-- .comment-content -->

							</article><!-- .comment-body -->
		<ol>
		<li id="comment-458121">
			<article id="div-comment-458121">
				<!-- .comment-meta -->

				<div>
					<p>On this: “even the so called vacuum of space or the so called ether is a form or energy or mass (so far unexplained as dark matter) as it has always existed, even if in another form (call it dark matter).” I think you might want to read the book’s Chapters 1-8 carefully.  Things — objects — are not forms of energy.  Energy is carried <strong>by</strong> things.  The same is true of mass.  If you imagine making things out of energy or out of mass, you are misunderstanding what energy and mass are.  They are properties of substances, not the ingredients for substances.  Do not let loose language lead you astray on this.</p>
<p>Also, I’d like to ask you again to please hold your religious theorizing from this blog, particularly when it veers into scientific speculation with no experimental basis (as in “within a higher sphere/dominion…” or “Nucleus of all nuclei”.)  I’d encourage you to write about these ideas on a blog devoted to the larger questions of truth, philosophy, religion, metaphysics, and the like. This is a site devoted to the very limited methods and lessons of science.  </p>
<p>In science, we assume there are laws of nature and do experiments to understand what follows from those assumptions.  Remarkably, what we learn proves to be very powerful in the practical, material world. But we cannot use these methods to definitively address larger questions of truth, religion, fundamental origins, etc.  </p>
<p>You can try to obtain answers for such questions as you see fit, but these are not scientific answers, as they require assumptions beyond those that are necessary to do science.  You’re welcome to make those assumptions, of course. But the consequences of those assumptions are not scientific, and belong on a different blog than this one.  Here we walk a narrow, straight path, knowing that sticking to that path means that we can answer a few questions with great clarity, while leaving other important questions, including ones essential to being human, completely unaddressed.</p>

				</div><!-- .comment-content -->

							</article><!-- .comment-body -->
		</li><!-- #comment-## -->
		<li id="comment-458124">
			<article id="div-comment-458124">
				<!-- .comment-meta -->

				<div>
					<p>In GR, conservation laws are a little bit more complex.</p>
<p>At first, it was not clear how conservation of energy applied to GR, and that gave way to the discussions and debate of Hilbert, Klein, Einstein and Noether about this topic.</p>
<p>It was Noether who made this topic crystal-clear.</p>

				</div><!-- .comment-content -->

							</article><!-- .comment-body -->
		</li><!-- #comment-## -->
</ol><!-- .children -->
</li><!-- #comment-## -->
	<!-- .comment-list -->

		


		<p id="respond">
			<h3 id="reply-title">Leave a Reply<small></small></h3>			
		</p>

		
		<!-- .comments-area -->
		</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Waymo and Hyundai enter multi-year, strategic partnership (129 pts)]]></title>
            <link>https://waymo.com/blog/2024/10/waymo-and-hyundai-enter-partnership/</link>
            <guid>41741002</guid>
            <pubDate>Fri, 04 Oct 2024 13:08:18 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://waymo.com/blog/2024/10/waymo-and-hyundai-enter-partnership/">https://waymo.com/blog/2024/10/waymo-and-hyundai-enter-partnership/</a>, See on <a href="https://news.ycombinator.com/item?id=41741002">Hacker News</a></p>
<div id="readability-page-1" class="page"><article aria-labelledby="P0-7-title"><a href="https://waymo.com/blog/"><img alt="" role="presentation" src="https://waymo.com/static/images/blog/icon-left-arrow.svg"><img alt="" role="presentation" src="https://waymo.com/static/images/blog/icon-left-arrow-rollover.svg"><span>Back to all posts</span></a>

<section>
<div>
<picture>
<source srcset="https://images.ctfassets.net/e6t5diu0txbw/Rx4xRSbiRuFR5znXKCF43/1a33844fa1e0a9f5ac534cfe5b122333/WAYMO_X_HMC_4K_10032024.png?w=2880&amp;fm=webp 2x, https://images.ctfassets.net/e6t5diu0txbw/Rx4xRSbiRuFR5znXKCF43/1a33844fa1e0a9f5ac534cfe5b122333/WAYMO_X_HMC_4K_10032024.png?w=1440&amp;fm=webp" media="(min-width: 600px)" type="image/webp" width="3840" height="2160">
<source srcset="https://images.ctfassets.net/e6t5diu0txbw/Rx4xRSbiRuFR5znXKCF43/1a33844fa1e0a9f5ac534cfe5b122333/WAYMO_X_HMC_4K_10032024.png?w=2880 2x, https://images.ctfassets.net/e6t5diu0txbw/Rx4xRSbiRuFR5znXKCF43/1a33844fa1e0a9f5ac534cfe5b122333/WAYMO_X_HMC_4K_10032024.png?w=1440" media="(min-width: 600px)" type="image/png" width="3840" height="2160">
<source srcset="https://images.ctfassets.net/e6t5diu0txbw/Rx4xRSbiRuFR5znXKCF43/1a33844fa1e0a9f5ac534cfe5b122333/WAYMO_X_HMC_4K_10032024.png?w=2048&amp;fm=webp 2x, https://images.ctfassets.net/e6t5diu0txbw/Rx4xRSbiRuFR5znXKCF43/1a33844fa1e0a9f5ac534cfe5b122333/WAYMO_X_HMC_4K_10032024.png?w=1024&amp;fm=webp" media="(min-width: 600px) and (max-width: 1023px)" type="image/webp" width="3840" height="2160">
<source srcset="https://images.ctfassets.net/e6t5diu0txbw/Rx4xRSbiRuFR5znXKCF43/1a33844fa1e0a9f5ac534cfe5b122333/WAYMO_X_HMC_4K_10032024.png?w=2048 2x, https://images.ctfassets.net/e6t5diu0txbw/Rx4xRSbiRuFR5znXKCF43/1a33844fa1e0a9f5ac534cfe5b122333/WAYMO_X_HMC_4K_10032024.png?w=1024" media="(min-width: 600px) and (max-width: 1023px)" type="image/png" width="3840" height="2160">
<source srcset="https://images.ctfassets.net/e6t5diu0txbw/Rx4xRSbiRuFR5znXKCF43/1a33844fa1e0a9f5ac534cfe5b122333/WAYMO_X_HMC_4K_10032024.png?w=2048&amp;fm=webp 2x, https://images.ctfassets.net/e6t5diu0txbw/Rx4xRSbiRuFR5znXKCF43/1a33844fa1e0a9f5ac534cfe5b122333/WAYMO_X_HMC_4K_10032024.png?w=1024&amp;fm=webp" media="(max-width: 599px)" type="image/webp" width="3840" height="2160"><img alt="Waymo Driver technology integrated with Hyundai IONIQ 5 vehicle" loading="lazy" srcset="https://images.ctfassets.net/e6t5diu0txbw/Rx4xRSbiRuFR5znXKCF43/1a33844fa1e0a9f5ac534cfe5b122333/WAYMO_X_HMC_4K_10032024.png?w=2048 2x, https://images.ctfassets.net/e6t5diu0txbw/Rx4xRSbiRuFR5znXKCF43/1a33844fa1e0a9f5ac534cfe5b122333/WAYMO_X_HMC_4K_10032024.png?w=1024" src="https://images.ctfassets.net/e6t5diu0txbw/Rx4xRSbiRuFR5znXKCF43/1a33844fa1e0a9f5ac534cfe5b122333/WAYMO_X_HMC_4K_10032024.png?w=420" width="3840" height="2160">
</picture>
</div>
<div>
<p>Today, Waymo and Hyundai Motor Company announced they have entered into a multi-year, strategic partnership. In the first phase of this partnership, the companies will integrate Waymo’s sixth-generation fully autonomous technology – the Waymo Driver – into Hyundai’s all-electric IONIQ 5 SUV, which will be added to the Waymo One fleet over time.</p>
<p>The IONIQ 5 vehicles destined for the Waymo fleet will be assembled at the new Hyundai Motor Group Metaplant America (HMGMA) EV manufacturing facility in Georgia and then integrated with Waymo’s autonomous technology. The companies plan to produce a fleet of IONIQ 5s equipped with Waymo’s technology in significant volume over multiple years to support Waymo One’s growing scale. Initial on-road testing with Waymo-enabled IONIQ 5s will begin by late 2025 and become available to Waymo One riders in the years to follow.</p>
<p>“We are thrilled to partner with Hyundai as we further our mission to be the world's most trusted driver,” said Tekedra Mawakana, co-CEO, Waymo. “Hyundai’s focus on sustainability and strong electric vehicle roadmap makes them a great partner for us as we bring our fully autonomous service to more riders in more places.”</p>
<p>“Hyundai and Waymo share a vision to improve the safety, efficiency and convenience of how people move,” said José Muñoz, president and global COO of Hyundai Motor Company, and president and CEO of Hyundai Motor North America. “Waymo’s transformational technology is improving road safety where they operate, and the IONIQ 5 is the ideal vehicle to scale this further. The team at our new manufacturing facility is ready to allocate a significant number of vehicles for the Waymo One fleet as it continues to expand. Importantly, this is the first step in the partnership between the two companies and we are actively exploring additional opportunities for collaboration.”</p>
<p>“We recently <a href="https://www.hyundainews.com/en-us/releases/4227"><u>announced</u></a> the launch of Hyundai Motor Company’s autonomous vehicle foundry business to provide global autonomous driving companies with vehicles capable of implementing SAE Level 4 or higher autonomous driving technology,” said Chang Song, President and Head of Hyundai Motor Group’s Advanced Vehicle Platform (AVP) Division. “There is no better partner for our first agreement in this initiative than industry-leader Waymo.”</p>
<p>The Hyundai IONIQ 5 will be delivered to Waymo with specific autonomous-ready modifications like redundant hardware and power doors. The <a href="https://www.hyundai.com/worldwide/en/footer/corporate/awards/ioniq5"><u>award-winning</u></a>, all-electric vehicle will enable long driving shifts on a single charge, and its 800-volt architecture will minimize time out of service with some of the industry’s fastest charging speeds available. The IONIQ 5’s well-appointed and spacious interior will offer plenty of legroom, headroom, and rear cargo space for a comfortable rider experience.</p>
</div>
</section>
</article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[ESP8266 Analog Broadcast Television Interface (115 pts)]]></title>
            <link>https://github.com/cnlohr/channel3</link>
            <guid>41740978</guid>
            <pubDate>Fri, 04 Oct 2024 13:04:45 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/cnlohr/channel3">https://github.com/cnlohr/channel3</a>, See on <a href="https://news.ycombinator.com/item?id=41740978">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><p dir="auto"><h2 tabindex="-1" dir="auto">channel3</h2><a id="user-content-channel3" aria-label="Permalink: channel3" href="#channel3"></a></p>
<p dir="auto">ESP8266 Analog Broadcast Television Interface</p>
<p dir="auto">Hook an antenna up to GPIO3/RX, tune your analog TV to Channel 3.  Power the ESP on!</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Background and RF</h2><a id="user-content-background-and-rf" aria-label="Permalink: Background and RF" href="#background-and-rf"></a></p>
<p dir="auto">This uses the I2S Bus in the same way the esp8266ws2812i2s project does.  Difference is it cranks the output baud to 80 MHz.  We set up DMA buffers and let the CPU fill them as they pass through one line at a time.  The DMA interrupt fills in the buffers one word at a time.  The I2S bus shifts those buffers out at 80 MHz!</p>
<p dir="auto">You may say "But nyquist says you can't transmit or receive frequencies at more than 1/2 the sample rate (40 MHz in this case).  To a degree that is true.  Some people thought it may be overtones, but what happens in reality something stranger happens.  Everything you transmit is actually mirrored around 1/2 the sample rate (40 MHz).  So, transmitting 60 MHz on an 80 MHz bitclock creates a waveform both at 60 as well as 20.  This isn't perfect.  Some frequencies line up to the 80 MHz well, others do not.</p>
<p dir="auto">We store a bit pattern in the "premodulated_table" array.  This contains bitstreams for various signals, such as the "sync" level or "colorbust" level, or any of the visual colors.  This table's length of 1408 bits per color was chosen so that when sent out one bit at a time at 80 MHz, it works out to an even multiplier of the NTCS chroma frequency of 315.0/88.0 MHz, or 3.579545455 MHz.  You can calculate this by taking 1408/80MHz = 17.6us * 3.579545 MHz = 63 cycles, exactly.  Conveniently, it also works out to an even multiplier of 61.25 MHz, Channel 3's luma center.  17.6us * 61.25 MHz = 1078 cycles, exactly! When you modulate arbitrary frequencies, sometimes the cycles come out very uneven.</p>
<p dir="auto">In order to generate luma (the black and white portion) we modulate 61.25 MHz.  If we generate a strong signal, it is viewed as a very "dark", and a weak signal is a very "bright."  This means when we want to send out a sync pulse, we modulate it as loud as we can... when we want to modulate white, we put out barely any signal at all.  One thing you will notice is dot pour.  This is because the signal we are sending is so terrible.  The chroma signal is very dirty and has a repeating intensity pattern.  While the chroma lines up to the 1408 bit-wide repeating patten, the total number of pixels on the screen does not.  This causes the patterns created to roll down the screen.</p>
<p dir="auto">In order to generate color, we need to modulate in a chroma signal, 3.579MHz above the baseband.  The chroma is synchronized by a colorburst at the beginning of each line.  This also sets the level for the chroma.  Then, during the line, we can either choose a "color" that has a high coefficient at the chroma level, or a low one.  This determines how vivid the color is.  We can change phase to change the color's hue.</p>
<p dir="auto">This is basically a 1-bit dithering DAC, operating at a frequency below the nyquist, trying to encode luma and color at the same time.  Don't be surprised that the quality's terrible.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Code Layout</h2><a id="user-content-code-layout" aria-label="Permalink: Code Layout" href="#code-layout"></a></p>
<p dir="auto">Tables for handling the line-buffer state machine are (generated/stored?) in MayCbTables.h/c, and similar tables for creating the on-wire signal encoding are in synthtables.c.</p>
<p dir="auto">Functions to set up the DMA transfers, refill the buffers when they become empty, and change what kind of line should be sent based on the framebuffer contents are in video_broadcast.c. These functions handle all of the modulation.  This sets up the DMA, and an interrupt that is called when the DMA finishes a block (equal to one line).  Upon completion, it uses CbTable to decide what function to call to fill in the line.  The interrupt fills out the next line for DMA which keeps going.</p>
<p dir="auto">The framebuffer is updated by various demo screens located in user_main.c.</p>
<p dir="auto">custom_commands.c contain the custom commands used for the NTSC-specific aspects.  Using the common websockets interface there are two added commands.  These include "CO" and "CV" which set the operation mode (CO) and allow users to change the modulation table from a web interface (CV).</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Demo screens</h2><a id="user-content-demo-screens" aria-label="Permalink: Demo screens" href="#demo-screens"></a></p>
<p dir="auto">The following demo screens are available.  They normally tick through one after another (except ones after 10), unless the user disables this in the web browser.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Screen Modes</h2><a id="user-content-screen-modes" aria-label="Permalink: Screen Modes" href="#screen-modes"></a></p>
<ol dir="auto">
<li>Basic intro screen, shows IP address if available.</li>
<li>ESP8266 Features</li>
<li>Intro to and completion of framebuffer copy test.  Beware, running this screen too long deliberately will cause a crash.</li>
<li>Draw a bunch of lines... IN COLOR!</li>
<li>Matrix-based 3D engine demo.</li>
<li>Dynamic 3D mesh demo.</li>
<li>Pitch for this project's github.</li>
<li>Color screen with 16 color balls.</li>
<li>4x4 color swatches, useful for when you're messing with colors in the web GUI.</li>
</ol>
<p dir="auto"><h2 tabindex="-1" dir="auto">Web interface</h2><a id="user-content-web-interface" aria-label="Permalink: Web interface" href="#web-interface"></a></p>
<p dir="auto">The web interface is borrowing the web interface from esp8266ws2812i2s.  Power on the ESP, connect to it, then, point your web browser to <a href="http://192.168.4.1/" rel="nofollow">http://192.168.4.1</a>.  It has a new button "NTSC."  This gives you the option to allow demo to continue from screen to screen, or freeze at a specific screen.  You can specify the screen.  Additionally, for RF testing, you can jam a color.  Whenever the color jam is set to something 0 or above, it turns off all line drawing logic, and simply outputs that color continuously.  This will prevent TV sets from seeing it, however, you can see it on other RF equipment.</p>
<p dir="auto">It also has an interactive Javascript webworker system that lets you write code to make a new color!  You can create a new bitstream that will be transmitted when a specific color is hit.  You can edit the code and it is effective as you type.  It automatically re-starts the webworker every time you change it.</p>
<p dir="auto">You should only output -1 or +1 as that is all the ESP can output.  It will then run a DFT with a randomized window over a frequency area you choose.  Increase the DFT window, and it will increase your q (or precision).  Decrease, it decreases your q.  This is to help see how receivers like the TV really understand the signal and help illustrate how wacky this really is.</p>
<p dir="auto">You can try it in your own browser using this link: <a href="http://cnlohr.github.io/channel3/web/page/index.html" rel="nofollow">http://cnlohr.github.io/channel3/web/page/index.html</a>  Click NTSC and go to town.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Rawdraw and 3D</h2><a id="user-content-rawdraw-and-3d" aria-label="Permalink: Rawdraw and 3D" href="#rawdraw-and-3d"></a></p>
<p dir="auto">For all the 3D and text, I'm using a new modified version of my "rawdraw" library ( <a href="http://github.com/cnlohr/rawdraw">http://github.com/cnlohr/rawdraw</a> ) for 3D I'm using fixed point numbers, with 256 as the unit value, and the bottom 8 bits are the fractional component.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">PAL Modification</h2><a id="user-content-pal-modification" aria-label="Permalink: PAL Modification" href="#pal-modification"></a></p>
<p dir="auto">To allow for PAL broadcasts, the timings in the video_broadcast-library (formerly ntsc_broadcast) were modified. Since I only wanted to use this with a black an white TV, and PAL colour is actually quite complicated to do digitally, I didn't modify the broadcast_tables (synthtables.c). So the library broadcasts a PAL compliant B/W-Signal with NTSC Colour information (kind of like NTSC50).</p>
<p dir="auto">To enable PAL broadcasting you need to enable <code>OPTS += -DPAL</code> in user.cfg.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Youtube video</h2><a id="user-content-youtube-video" aria-label="Permalink: Youtube video" href="#youtube-video"></a></p>
<p dir="auto">Here is the original youtube video on this project:</p>
<p dir="auto"><a href="http://www.youtube.com/watch?v=SSiRkpgwVKY" rel="nofollow"><img src="https://camo.githubusercontent.com/6ca5bef23f57fb1753a71914c355b48095a077861f80a0be02d8980efd4951bf/687474703a2f2f696d672e796f75747562652e636f6d2f76692f535369526b706777564b592f302e6a7067" alt="NTSC Video on the ESP8266" data-canonical-src="http://img.youtube.com/vi/SSiRkpgwVKY/0.jpg"></a></p>
<p dir="auto">Here is the new video (with COLOR):</p>
<p dir="auto"><a href="http://www.youtube.com/watch?v=bcez5pcp55w" rel="nofollow"><img src="https://camo.githubusercontent.com/855520194c9983aa37d76337f1d16d103600704141d15cbc3d7ee961f2fbdf0e/687474703a2f2f696d672e796f75747562652e636f6d2f76692f6263657a357063703535772f302e6a7067" alt="Broadcasting COLOR Channel 3 on an ESP" data-canonical-src="http://img.youtube.com/vi/bcez5pcp55w/0.jpg"></a></p>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[AI at Meta: Movie Gen (846 pts)]]></title>
            <link>https://ai.meta.com/research/movie-gen/?_fb_noscript=1</link>
            <guid>41740965</guid>
            <pubDate>Fri, 04 Oct 2024 13:03:00 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://ai.meta.com/research/movie-gen/?_fb_noscript=1">https://ai.meta.com/research/movie-gen/?_fb_noscript=1</a>, See on <a href="https://news.ycombinator.com/item?id=41740965">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><div id="u_0_5_lx" data-scoped-css="fullscreen-hero-scoped-u_0_0_eV"><p>Introducing research for the most advanced media foundation AI models.</p></div><div><div><p>Movie Gen sets a new standard for immersive AI content</p><div><p>Our latest research breakthroughs demonstrate how you can use simple text inputs to produce custom videos and sounds, edit existing videos or transform your personal image into a unique video.</p></div></div><div><p>Text input summary: A girl is running across a beach and holding a kite. She's wearing jean shorts and a yellow t-shirt. The sun is shining down.</p><p>Text input summary: A woman is sitting on the grass of a pumpkin patch. She is wearing a scarf and holding a cup. The background is filled with rows of pumpkins.</p><p>Text input: Thunder cracks loudly, with an orchestral music track.</p></div></div><div><div><p>Generate videos from text</p><div><p>Produce unique videos from text to create a custom masterpiece. Movie Gen creates long high-definition videos at different aspect ratios—the first of its kind in the industry.</p></div></div><div><p>Text input summary: A sloth with pink sunglasses lays on a donut float in a pool. The sloth is holding a tropical drink. The world is tropical. The sunlight casts a shadow.</p><p>Text input summary: The camera is behind a man. The man is shirtless, wearing a green cloth around his waist. He is barefoot. With a fiery object in each hand, he creates wide circular motions. A calm sea is in the background. The atmosphere is mesmerizing, with the fire dance.</p><p>Text input summary: A fluffy koala bear surfs. It has a grey and white coat and a round nose. The surfboard is yellow. The koala bear is holding onto the surfboard with its paws. The koala bear’s facial expression is focused. The sun is shining.</p></div></div><div><p>Edit video with text</p><div><p>Transform existing videos with text inputs. Movie Gen enables precise video editing—from styles and transitions to fine-grained edits.</p></div></div><div><div><p>Produce personalized videos</p><div><p>Upload an image of yourself and transform it into a personalized video. Movie Gen’s cutting-edge model lets you create personalized videos that preserve human identity and motion.</p></div></div><div><p>Text input summary: A man is doing a scientific experiment in a lab with rainbow wallpaper. The man has a serious expression and is wearing glasses. He is wearing a white lab coat with a pen in the pocket. The man pours liquid into a glass beaker and a cloud of white smoke blooms.</p><p>Text input summary: A woman paints a canvas on an easel, in a wood-paneled room. The woman is wearing a white shirt. She has a calm expression as she concentrates on her work. A baby bear cub stands at her feet. The lighting is cool.</p><p>Text input summary: A woman DJ spins records on a rooftop in LA. She is wearing a pink jacket and giant headphones. There is a cheetah next to the woman. The background is a cityscape.</p></div></div><div><div><p>Create sound effects and soundtracks</p><div><p>Use video and text inputs to generate audio for your videos. Movie Gen allows you to create and extend sound effects, background music or entire soundtracks.</p></div></div><div><p>Text input: Rain pours against the cliff and the person, with music playing in the background.</p><p>Text input: Rustling leaves and snapping twigs, with an orchestral music track.</p><p>Text input: ATV engine roars and accelerates, with guitar music.</p><p>Text input: Wheels spinning, and a slamming sound as the skateboard lands on concrete.</p><p>Text input: A beautiful orchestral piece that evokes a sense of wonder.</p><p>Text input: Whistling sounds, followed by a sharp explosion and loud crackling.</p></div></div><div><h2>Learn more about Movie Gen</h2><div><div><p>Download our latest research paper to learn how we’ve set new industry benchmarks on media generation with AI.</p></div><p><a href="https://ai.meta.com/static-resource/movie-gen-research-paper" data-ms-clickable="true" data-ms="{&quot;creative&quot;:&quot;click_external-link&quot;,&quot;creative_detail&quot;:&quot;MovieGenResearchPaperStaticResource_LearnMoreAboutMovieGen_DownloadPaper&quot;}" target="_blank" rel="noreferrer noopener" data-lnfb-mode="ie" id="u_0_1u_ex">Download paper</a></p></div><div><div><div><p><img src="https://static.xx.fbcdn.net/rsrc.php/v3/y4/r/-PAXP-deijE.gif" alt="AI Generated image of a woman with long, curly hair wearing a dress with the sun behinder her"></p></div><p>BLOG</p><h3>How Meta Movie Gen could usher in a new AI-enabled era for content creators</h3></div><div><div><p><img src="https://static.xx.fbcdn.net/rsrc.php/v3/y4/r/-PAXP-deijE.gif" alt="Young man and woman sitting on a sofa looking at two open laptops"></p></div><p>RESPONSIBILITY</p><h3>Building AI around a core set of values to ensure trust and safety</h3></div></div></div></div><div><div id="u_0_1x_ST" data-scoped-css="fullscreen-hero-scoped-u_0_1_xf"><h2>Meta Movie Gen</h2><p>Introducing research for the most advanced media foundation AI models.</p></div><div><div><p>Movie Gen sets a new standard for immersive AI content</p><div><p>Our latest research breakthroughs demonstrate how you can use simple text inputs to produce custom videos and sounds, edit existing videos or transform your personal image into a unique video.</p></div></div><div><p>Text input summary: A girl is running across a beach and holding a kite. She's wearing jean shorts and a yellow t-shirt. The sun is shining down.</p><p>Text input summary: A woman is sitting on the grass of a pumpkin patch. She is wearing a scarf and holding a cup. The background is filled with rows of pumpkins.</p><p>Text input: Thunder cracks loudly, with an orchestral music track.</p><p>Text input: Transform the lantern into a bubble that soars into the air.</p></div></div><div><div><p>Generate videos from text</p><div><p>Produce unique videos from text to create a custom masterpiece. Movie Gen creates long high-definition videos at different aspect ratios—the first of its kind in the industry.</p></div></div><div><p>Text input summary: A sloth with pink sunglasses lays on a donut float in a pool. The sloth is holding a tropical drink. The world is tropical. The sunlight casts a shadow.</p><p>Text input summary: The camera is behind a man. The man is shirtless, wearing a green cloth around his waist. He is barefoot. With a fiery object in each hand, he creates wide circular motions. A calm sea is in the background. The atmosphere is mesmerizing, with the fire dance.</p><p>Text input summary: A fluffy koala bear surfs. It has a grey and white coat and a round nose. The surfboard is yellow. The koala bear is holding onto the surfboard with its paws. The koala bear’s facial expression is focused. The sun is shining.</p><p>Text input summary: A ghost in a white bedsheet faces a mirror. The ghost's reflection can be seen in the mirror. The ghost is in a dusty attic, filled with old beams, cloth-covered furniture. The attic is reflected in the mirror. The light is cool and natural. The ghost dances in front of the mirror.</p><p>Text input summary: A red-faced monkey with white fur is bathing in a natural hot spring. The monkey is playing in the water with a miniature sail ship in front of it, made of wood with a white sail and a small rudder. The hot spring is surrounded by lush greenery, with rocks and trees.</p></div></div><div><p>Edit video with text</p><div><p>Transform existing videos with text inputs. Movie Gen enables precise video editing—from styles and transitions to fine-grained edits.</p></div></div><div><div><p>Produce personalized videos</p><div><p>Upload an image of yourself and transform it into a personalized video. Movie Gen’s cutting-edge model lets you create personalized videos that preserve human identity and motion.</p></div></div><div><p>Text input summary: A man is doing a scientific experiment in a lab with rainbow wallpaper. The man has a serious expression and is wearing glasses. He is wearing a white lab coat with a pen in the pocket. The man pours liquid into a glass beaker and a cloud of white smoke blooms.</p><p>Text input summary: A woman paints a canvas on an easel, in a wood-paneled room. The woman is wearing a white shirt. She has a calm expression as she concentrates on her work. A baby bear cub stands at her feet. The lighting is cool.</p><p>Text input summary: Make a cute selfie video of a man and his dog. The man is wearing a black shirt. The dog is a beagle puppy. The background is a backyard patio, filled with trees. The man has a big smile on his face, as he tries to take the perfect selfie with his dog. The lighting is warm.</p><p>Text input summary: A man sits in the desert, wearing a wide-brimmed hat, a brown coat, and a scarf. The man holds a glass of amber-colored tea. The camera pans from the desert scenery to the person. The lighting is warm, with the sun casting a gentle glow on the scene.</p><p>Text input summary: A cowgirl wearing denim pants is on a white horse in an old western town. A leather belt cinches at her waist. The horse is majestic, with its coat gleaming in the sunlight. The Rocky Mountains are in the background.</p><p>Text input summary: A woman DJ spins records on a rooftop in LA. She is wearing a pink jacket and giant headphones. There is a cheetah next to the woman. The background is a cityscape.</p></div></div><div><div><p>Create sound effects and soundtracks</p><div><p>Use video and text inputs to generate audio for your videos. Movie Gen allows you to create and extend sound effects, background music or entire soundtracks.</p></div></div><div><p>Text input: Rain pours against the cliff and the person, with music playing in the background.</p><p>Text input: Rustling leaves and snapping twigs, with an orchestral music track.</p><p>Text input: ATV engine roars and accelerates, with guitar music.</p><p>Text input: Wheels spinning, and a slamming sound as the skateboard lands on concrete.</p><p>Text input: A beautiful orchestral piece that evokes a sense of wonder.</p><p>Text input: Whistling sounds, followed by a sharp explosion and loud crackling.</p></div></div><div><h2>Learn more about Movie Gen</h2><div><div><p>Download our latest research paper to learn how we’ve set new industry benchmarks on media generation with AI.</p></div><p><a href="https://ai.meta.com/static-resource/movie-gen-research-paper" data-ms-clickable="true" data-ms="{&quot;creative&quot;:&quot;click_external-link&quot;,&quot;creative_detail&quot;:&quot;MovieGenResearchPaperStaticResource_LearnMoreAboutMovieGen_DownloadPaper&quot;}" target="_blank" rel="noreferrer noopener" data-lnfb-mode="ie" id="u_0_4d_7u">Download paper</a></p></div><div><div><div><p><img src="https://static.xx.fbcdn.net/rsrc.php/v3/y4/r/-PAXP-deijE.gif" alt="AI Generated image of a woman with long, curly hair wearing a dress with the sun behind her"></p></div><p>BLOG</p><h3>How Meta Movie Gen could usher in a new AI-enabled era for content creators</h3></div><div><div><p><img src="https://static.xx.fbcdn.net/rsrc.php/v3/y4/r/-PAXP-deijE.gif" alt="Young man and woman sitting on a sofa looking at two open laptops"></p></div><p>RESPONSIBILITY</p><h3>Building AI around a core set of values to ensure trust and safety</h3></div></div></div></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Show HN: kew – A Terminal Music Player for Linux (123 pts)]]></title>
            <link>https://github.com/ravachol/kew</link>
            <guid>41740915</guid>
            <pubDate>Fri, 04 Oct 2024 12:56:54 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/ravachol/kew">https://github.com/ravachol/kew</a>, See on <a href="https://news.ycombinator.com/item?id=41740915">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><p dir="auto"><h2 tabindex="-1" dir="auto">kew</h2><a id="user-content-kew" aria-label="Permalink: kew" href="#kew"></a></p>
<p dir="auto"><a href="https://github.com/ravachol/kew/blob/master/LICENSE"><img src="https://camo.githubusercontent.com/67f3726fadde4d42395444ff82db1d647fbc4fc43157a33c8c4c2b98adbefd5c/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6c6963656e73652f7261766163686f6c2f6b65773f636f6c6f723d333333333333267374796c653d666f722d7468652d6261646765" alt="GitHub license" data-canonical-src="https://img.shields.io/github/license/ravachol/kew?color=333333&amp;style=for-the-badge"></a></p>
<p dir="auto">Listen to music in the terminal.</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/ravachol/kew/blob/main/images/kew-screenshot.png"><img src="https://github.com/ravachol/kew/raw/main/images/kew-screenshot.png" alt="Example screenshot"></a><br>
<em>Example screenshot running in Konsole: <a href="https://jenova7.bandcamp.com/album/lost-sci-fi-movie-themes" rel="nofollow">Jenova 7: Lost Sci-Fi Movie Themes</a>.</em></p>
<p dir="auto"><br>
kew (/kjuː/) is a terminal music player for Linux.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Features</h2><a id="user-content-features" aria-label="Permalink: Features" href="#features"></a></p>
<ul dir="auto">
<li>Search a music library with partial titles.</li>
<li>Creates a playlist based on a matched directory.</li>
<li>Control the player with previous, next and pause.</li>
<li>Edit the playlist by adding and removing songs.</li>
<li>Supports gapless playback (between files of the same format and type).</li>
<li>Supports MP3, FLAC, MPEG-4 (AAC, M4A), OPUS, OGG and WAV audio.</li>
<li>Private, no data is collected by kew.</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Installing</h2><a id="user-content-installing" aria-label="Permalink: Installing" href="#installing"></a></p>
<p dir="auto"><a href="https://repology.org/project/kew/versions" rel="nofollow"><img src="https://camo.githubusercontent.com/f07ff5705a9b1fa69d927d8b2357e8c2dacf97d25ac53c449073705037aa8787/68747470733a2f2f7265706f6c6f67792e6f72672f62616467652f766572746963616c2d616c6c7265706f732f6b65772e737667" alt="Packaging status" data-canonical-src="https://repology.org/badge/vertical-allrepos/kew.svg"></a></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Installing with package managers</h3><a id="user-content-installing-with-package-managers" aria-label="Permalink: Installing with package managers" href="#installing-with-package-managers"></a></p>
<p dir="auto">kew is available from Ubuntu 24.04, Debian 13.</p>
<div dir="auto" data-snippet-clipboard-copy-content="sudo apt install kew         (Debian, Ubuntu)
sudo yay -S kew              (Arch Linux, Manjaro)
sudo yay -S kew-git          (Arch Linux, Manjaro)
sudo zypper install kew      (OpenSUSE)
sudo pkg install kew         (FreeBSD)
brew install kew             (Linux Only, No MacOS)"><pre>sudo apt install kew         (Debian, Ubuntu)
sudo yay -S kew              (Arch Linux, Manjaro)
sudo yay -S kew-git          (Arch Linux, Manjaro)
sudo zypper install kew      (OpenSUSE)
sudo pkg install kew         (FreeBSD)
brew install kew             (Linux Only, No MacOS)</pre></div>
<p dir="auto"><h3 tabindex="-1" dir="auto">Installing with quick install script</h3><a id="user-content-installing-with-quick-install-script" aria-label="Permalink: Installing with quick install script" href="#installing-with-quick-install-script"></a></p>
<p dir="auto">To quickly install kew, just copy and paste this to your terminal (if you have curl installed):</p>
<div dir="auto" data-snippet-clipboard-copy-content="sudo bash -c &quot;curl https://raw.githubusercontent.com/ravachol/kew/main/install.sh | bash&quot;"><pre>sudo bash -c <span><span>"</span>curl https://raw.githubusercontent.com/ravachol/kew/main/install.sh | bash<span>"</span></span></pre></div>
<p dir="auto">Please note that this script might do a system update before installing kew.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Standalone AppImage for musl systems</h3><a id="user-content-standalone-appimage-for-musl-systems" aria-label="Permalink: Standalone AppImage for musl systems" href="#standalone-appimage-for-musl-systems"></a></p>
<p dir="auto">If you are running a musl-based system, for instance Alpine Linux, you can download a standalone appImage of kew:</p>
<p dir="auto"><a href="https://github.com/ravachol/kew/releases/tag/stable-musl">https://github.com/ravachol/kew/releases/tag/stable-musl</a></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Building the project manually</h3><a id="user-content-building-the-project-manually" aria-label="Permalink: Building the project manually" href="#building-the-project-manually"></a></p>
<p dir="auto">kew dependencies are:</p>
<ul dir="auto">
<li>FFmpeg</li>
<li>FFTW</li>
<li>Chafa</li>
<li>FreeImage</li>
<li>libopus</li>
<li>opusfile</li>
<li>libvorbis</li>
<li>pkg-config</li>
<li>glib2.0 and AVFormat. These should be installed with the others, if not install them.</li>
<li>libnotify (optional)</li>
</ul>
<p dir="auto">Install FFmpeg, FFTW, Chafa and FreeImage using your distro's package manager. For instance:</p>
<div dir="auto" data-snippet-clipboard-copy-content="apt install ffmpeg libfftw3-dev libopus-dev libopusfile-dev libvorbis-dev git gcc make libchafa-dev libfreeimage-dev libavformat-dev libglib2.0-dev libnotify-dev"><pre>apt install ffmpeg libfftw3-dev libopus-dev libopusfile-dev libvorbis-dev git gcc make libchafa-dev libfreeimage-dev libavformat-dev libglib2.0-dev libnotify-dev</pre></div>
<p dir="auto">Or:</p>
<div dir="auto" data-snippet-clipboard-copy-content="pacman -Syu ffmpeg fftw git gcc make chafa freeimage glib2 opus opusfile libvorbis libnotify"><pre>pacman -Syu ffmpeg fftw git gcc make chafa freeimage glib2 opus opusfile libvorbis libnotify</pre></div>
<p dir="auto">Or (for Fedora for instance):</p>
<div dir="auto" data-snippet-clipboard-copy-content="dnf install -y pkg-config ffmpeg-free-devel fftw-devel opus-devel opusfile-devel libvorbis-devel git gcc make chafa-devel freeimage-devel libavformat-free-devel libnotify-devel libatomic"><pre>dnf install -y pkg-config ffmpeg-free-devel fftw-devel opus-devel opusfile-devel libvorbis-devel git gcc make chafa-devel freeimage-devel libavformat-free-devel libnotify-devel libatomic</pre></div>
<p dir="auto">Notice that for some packages not only the library needs to be installed, but also development packages, for instance libopus-dev or opus-devel.</p>
<p dir="auto">Then run this (either git clone or unzip a release zip into a folder of your choice):</p>
<div dir="auto" data-snippet-clipboard-copy-content="git clone https://github.com/ravachol/kew.git"><pre>git clone https://github.com/ravachol/kew.git</pre></div>



<p dir="auto">A sixel (or equivalent) capable terminal is recommended, like Konsole or kitty, to display images properly.</p>
<p dir="auto">For a complete list of capable terminals, see this page: <a href="https://www.arewesixelyet.com/" rel="nofollow">Sixels in Terminal</a>.</p>
<p dir="auto"><h4 tabindex="-1" dir="auto">LibNotify is (should be) optional</h4><a id="user-content-libnotify-is-should-be-optional" aria-label="Permalink: LibNotify is (should be) optional" href="#libnotify-is-should-be-optional"></a></p>
<p dir="auto">By default, the build system will automatically detect if <code>libnotify</code> is available and include it and enable notifications if found.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Uninstalling</h3><a id="user-content-uninstalling" aria-label="Permalink: Uninstalling" href="#uninstalling"></a></p>
<p dir="auto">If you installed kew manually, simply run:</p>

<p dir="auto"><h2 tabindex="-1" dir="auto">Usage</h2><a id="user-content-usage" aria-label="Permalink: Usage" href="#usage"></a></p>
<p dir="auto">Run kew. It will first help you set the path to your music folder, then show you that folder's contents.</p>
<p dir="auto">kew can also be told to play a certain music from the command line. It automatically creates a playlist based on a partial name of a track or directory:</p>

<p dir="auto">This command plays all songs from "The Cure Greatest Hits" directory, provided it's in your music library.</p>
<p dir="auto">kew returns the first directory or file whose name matches the string you provide. It works best when your music library is organized in this way: artist folder-&gt;album folder(s)-&gt;track(s).</p>
<p dir="auto"><h4 tabindex="-1" dir="auto">Some Examples:</h4><a id="user-content-some-examples" aria-label="Permalink: Some Examples:" href="#some-examples"></a></p>
<div data-snippet-clipboard-copy-content="kew (starting kew with no arguments opens the library view where you can choose what to play)

kew all (plays all songs, up to 20 000, in your library, shuffled)

kew albums (plays all albums, up to 2000, randomly one after the other)

kew moonlight son (finds and plays moonlight sonata)

kew moon (finds and plays moonlight sonata)

kew beet (finds and plays all music files under &quot;beethoven&quot; directory)

kew dir <album name> (sometimes it's necessary to specify it's a directory you want)

kew song <song> (or a song)

kew list <playlist> (or a playlist)

kew shuffle <album name> (shuffles the playlist)

kew artistA:artistB:artistC (plays all three artists, shuffled)

kew --help, -? or -h

kew --version or -v

kew --nocover

kew --noui (completely hides the UI)

kew -q <song>, --quitonstop (exits after finishing playing the playlist)

kew -e <song>, --exact (specifies you want an exact (but not case sensitive) match, of for instance an album)

kew . loads kew.m3u

kew path &quot;/home/joe/Musik/&quot; (changes the path)
"><pre><code>kew (starting kew with no arguments opens the library view where you can choose what to play)

kew all (plays all songs, up to 20 000, in your library, shuffled)

kew albums (plays all albums, up to 2000, randomly one after the other)

kew moonlight son (finds and plays moonlight sonata)

kew moon (finds and plays moonlight sonata)

kew beet (finds and plays all music files under "beethoven" directory)

kew dir &lt;album name&gt; (sometimes it's necessary to specify it's a directory you want)

kew song &lt;song&gt; (or a song)

kew list &lt;playlist&gt; (or a playlist)

kew shuffle &lt;album name&gt; (shuffles the playlist)

kew artistA:artistB:artistC (plays all three artists, shuffled)

kew --help, -? or -h

kew --version or -v

kew --nocover

kew --noui (completely hides the UI)

kew -q &lt;song&gt;, --quitonstop (exits after finishing playing the playlist)

kew -e &lt;song&gt;, --exact (specifies you want an exact (but not case sensitive) match, of for instance an album)

kew . loads kew.m3u

kew path "/home/joe/Musik/" (changes the path)

</code></pre></div>
<p dir="auto">Put single-quotes inside quotes "guns n' roses"</p>
<p dir="auto"><h4 tabindex="-1" dir="auto">Key Bindings</h4><a id="user-content-key-bindings" aria-label="Permalink: Key Bindings" href="#key-bindings"></a></p>
<ul dir="auto">
<li>Use <kbd>+</kbd> (or <kbd>=</kbd>), <kbd>-</kbd> keys to adjust the volume.</li>
<li>Use <kbd>←</kbd>, <kbd>→</kbd> or <kbd>h</kbd>, <kbd>l</kbd> keys to switch tracks.</li>
<li><kbd>Space</kbd>, <kbd>p</kbd> to toggle pause.</li>
<li><kbd>F2</kbd> to show/hide the playlist and information about kew.</li>
<li><kbd>F3</kbd> to show/hide the library.</li>
<li><kbd>F4</kbd> to show/hide the track view.</li>
<li><kbd>F5</kbd> to search.</li>
<li><kbd>F6</kbd> to show/hide key bindings.</li>
<li><kbd>u</kbd> to update the library.</li>
<li><kbd>v</kbd> to toggle the spectrum visualizer.</li>
<li><kbd>i</kbd> to switch between using your regular color scheme or colors derived from the track cover.</li>
<li><kbd>b</kbd> to toggle album covers drawn in ascii or as a normal image.</li>
<li><kbd>r</kbd> to repeat the current song.</li>
<li><kbd>s</kbd> to shuffle the playlist.</li>
<li><kbd>a</kbd> to seek back.</li>
<li><kbd>d</kbd> to seek forward.</li>
<li><kbd>x</kbd> to save the currently loaded playlist to a m3u file in your music folder.</li>
<li><kbd>gg</kbd> go to first song.</li>
<li>number +<kbd>G</kbd>, <kbd>g</kbd> or <kbd>Enter</kbd>, go to specific song number in the playlist.</li>
<li><kbd>g</kbd> go to last song.</li>
<li>. to add current song to kew.m3u (run with "kew .").</li>
<li><kbd>Esc</kbd> to quit.</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Configuration</h2><a id="user-content-configuration" aria-label="Permalink: Configuration" href="#configuration"></a></p>
<p dir="auto">kew will create a config file, kewrc, in a kew folder in your default config directory for instance ~/.config/kew. There you can change key bindings, number of bars in the visualizer and whether to use the album cover for color, or your regular color scheme. You can also change the default color of the app here. To edit this file please make sure you quit kew first.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Nerd Fonts</h2><a id="user-content-nerd-fonts" aria-label="Permalink: Nerd Fonts" href="#nerd-fonts"></a></p>
<p dir="auto">kew looks better with Nerd Fonts: <a href="https://www.nerdfonts.com/" rel="nofollow">https://www.nerdfonts.com/</a>.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">License</h2><a id="user-content-license" aria-label="Permalink: License" href="#license"></a></p>
<p dir="auto">Licensed under GPL. <a href="https://github.com/ravachol/kew/blob/main/LICENSE">See LICENSE for more information</a>.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Attributions</h2><a id="user-content-attributions" aria-label="Permalink: Attributions" href="#attributions"></a></p>
<p dir="auto">kew makes use of the following great open source projects:</p>
<p dir="auto">Chafa by  Petter Jansson - <a href="https://hpjansson.org/chafa/" rel="nofollow">https://hpjansson.org/chafa/</a></p>
<p dir="auto">FFmpeg by FFmpeg team - <a href="https://ffmpeg.org/" rel="nofollow">https://ffmpeg.org/</a></p>
<p dir="auto">FFTW by Matteo Frigo and Steven G. Johnson - <a href="https://www.fftw.org/" rel="nofollow">https://www.fftw.org/</a></p>
<p dir="auto">Libopus by Opus - <a href="https://opus-codec.org/" rel="nofollow">https://opus-codec.org/</a></p>
<p dir="auto">Libvorbis by Xiph.org - <a href="https://xiph.org/" rel="nofollow">https://xiph.org/</a></p>
<p dir="auto">Miniaudio by David Reid - <a href="https://github.com/mackron/miniaudio">https://github.com/mackron/miniaudio</a></p>
<p dir="auto">Img_To_Txt by Danny Burrows - <a href="https://github.com/danny-burrows/img_to_txt">https://github.com/danny-burrows/img_to_txt</a></p>
<p dir="auto">Comments? Suggestions? Send mail to <a href="mailto:kew-music-player@proton.me">kew-music-player@proton.me</a>.</p>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Show HN: Chebyshev approximation calculator (open source web app) (234 pts)]]></title>
            <link>https://stuffmatic.com/chebyshev/</link>
            <guid>41740568</guid>
            <pubDate>Fri, 04 Oct 2024 12:09:18 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://stuffmatic.com/chebyshev/">https://stuffmatic.com/chebyshev/</a>, See on <a href="https://news.ycombinator.com/item?id=41740568">Hacker News</a></p>
&lt;Unparsable&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[Show HN: A tool for creating chord charts on the go (123 pts)]]></title>
            <link>https://tiniuc.com/chord-chart-memo/</link>
            <guid>41740134</guid>
            <pubDate>Fri, 04 Oct 2024 11:00:33 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://tiniuc.com/chord-chart-memo/">https://tiniuc.com/chord-chart-memo/</a>, See on <a href="https://news.ycombinator.com/item?id=41740134">Hacker News</a></p>
<div id="readability-page-1" class="page"><section>
  <article>
    
      
      
<ul id="frontmatter">
    <li>
        <time datetime="2024-10-04">October 04, 2024</time>
    </li>
    <span></span>
    <li> 854 words </li>
    <span></span>
    <li> 5 min </li>
</ul>

      <p>Chord Chart Memo lets you input chords with just two taps.</p>
<span id="continue-reading"></span><iframe width="560" height="315" src="https://www.youtube.com/embed/_-qrEx2TWVg?si=gqykZsHsevoXPbR0" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen=""></iframe>
<p><a href="https://play.google.com/store/apps/details?id=com.tiniuc.chord_chart_memo"><img src="https://tiniuc.com/img/GetItOnGooglePlay_Badge_Web_color_English.png"></a></p>
<h2 id="what-is-it">What is it?</h2>
<p>Chord Chart Memo is a tool for musicians and songwriters which lets you input chords with just two taps. Charts can be entered and modified quickly, and are saved entirely offline.</p>
<p>The app also makes it easy to transpose charts and work in different keys. Charts can be labeled with text notes, to act as section titles, or other reminders for performance.</p>
<p>The app features all the most frequently used chords. If you can't find your favourite chord, <a href="mailto:alex@tiniuc.com">send me an email!</a> Slash chords can also be typed in, to denote specific inversions, basslines or to simplify complex chords.</p>
<h2 id="design">Design</h2>
<p>The note keyboard lays out the 12 notes based on the Circle of Fifths, so that the first row consists of all major chords in the selected key, and the second row consists of the minor chords.</p>
<img src="https://tiniuc.com/chord-chart-memo/full-kb.jpg">
<p>The chord keyboard is laid out in columns for major, minor and dominant chords, making it easy to tell at a glance where you can find the chord you would like to input.</p>
<img src="https://tiniuc.com/chord-chart-memo/entering-cm.jpg">
<p>By changing the key, the note buttons are transposed but the relationship between the note buttons remains the same, so that entering a given chord progression will always use the same motions regardless of the selected key. For example, the I - IV - VIm - V7 progression below uses the same motions in A as it would in C, where the chords would be C F Am G7.</p>
<img src="https://tiniuc.com/chord-chart-memo/chords-in-a-major.jpg">
<h2 id="why-i-built-this">Why I built this</h2>
<p>I got tired of seeing fellow musicians use their phone's native note-taking app to laboriously type chord after chord. I got tired of seeing musicians fail to access charts on the internet because of poor cellular reception. And I also got tired of finding a chart and then having to transpose the chords in my head in order to accommodate other performers.</p>
<h2 id="how-i-built-this">How I built this</h2>
<p>I started working on this on Christmas Day 2022 - back then all it had was very basic chord input &amp; transposition. I then used it for rehearsals &amp; songwriting throughout most of 2023. I got great feedback from musicians who noticed the app, and I got motivated to polish it and ship something that is more widely useable, and also works on stage. During beta-testing I have been using it exclusively for my live performance. Several beta-testers have also switched to using Chord Chart Memo even after testing was complete.</p>
<p>I built this in my spare time, often taking months-long gaps in development because I did not have the time, or I was working on something else. But the secret sauce that let me be productive even after not looking at the code for six months was Godot Game Engine. Yes, Chord Chart Memo is entirely made in Godot! I've written <a href="https://tiniuc.com/godot-for-apps">another article that talks about my experience with Godot in general.</a></p>
<h2 id="lessons-learned">Lessons Learned</h2>
<h3 id="it-always-takes-longer-than-you-think">It always takes longer than you think</h3>
<p>This is in a lot of aspects a very simple app, but it still took me 1.5 years from conception to something that musicians can actually rely on. If I had worked on it more consistently, it would have taken less time, but I'd also have gotten a lot less experience from using it live &amp; watching other people, so I think I would be a lot less happy with the design.</p>
<h3 id="testing-with-actual-users-is-crucial">Testing with actual users is crucial</h3>
<p>I put this app in front of 20 musicians, and the feedback was priceless. Some of them got the design immediately, and they understood how to best use the app just by looking at it. When that was not the case, I had to figure out how to make those features more intuitive, such as by changing some labels on the buttons, or highlighting certain parts of the interface.</p>
<h3 id="godot-is-great-for-hobby-development">Godot is great for hobby development</h3>
<p>I'm sure the fact that I already knew Godot helped me a lot. But if I had to dive into, for example, Android Java API documentation &amp; relearn how to work on the app every time I took a 6 month break, I don't think I would have been motivated to finish Chord Chart Memo.</p>
<p>Godot's scripting is high level enough that it is very easy to be productive even if you only work with a tiny part of the engine. And everything about the editor is amazing: it looks the same way on screen as it does on my phone, adding new GUI elements and connecting them up is a breeze, and the live debugging works very well.</p>

      
  </article>
</section></div>]]></description>
        </item>
        <item>
            <title><![CDATA[159 employees are leaving Automattic as CEO’s fight with WP Engine escalates (251 pts)]]></title>
            <link>https://techcrunch.com/2024/10/04/159-employees-are-leaving-automattic-as-ceos-fight-with-wp-engine-escalates/</link>
            <guid>41738914</guid>
            <pubDate>Fri, 04 Oct 2024 07:55:49 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://techcrunch.com/2024/10/04/159-employees-are-leaving-automattic-as-ceos-fight-with-wp-engine-escalates/">https://techcrunch.com/2024/10/04/159-employees-are-leaving-automattic-as-ceos-fight-with-wp-engine-escalates/</a>, See on <a href="https://news.ycombinator.com/item?id=41738914">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
<p id="speakable-summary">Automattic CEO Matt Mullenweg said on Thursday that 159 employees (roughly 8.4% of staff) <a rel="nofollow" href="https://ma.tt/2024/10/alignment/">accepted a severance package</a> that the company had offered to those who disagreed with his direction of WordPress and his <a href="https://techcrunch.com/2024/10/01/wordpress-vs-wp-engine-drama-explained/">handling of the tussle with web hosting provider WP Engine</a>.</p>

<p>In <a href="https://ma.tt/2024/10/alignment/" target="_blank" rel="noreferrer noopener nofollow">a blog post</a>, Mullenweg said the package offered $30,000 or six months of salary, whichever is higher, but the employees who took it would not be eligible to be re-hired by Automattic. </p>







<p>Nearly 80% of people who took the offer worked in the company’s Ecosystem / WordPress division, and the rest were in Automattic’s Cosmos businesses, consisting of apps like Pocket Casts, Day One, Tumblr and Cloudup.</p>

<p>Mullenweg, who co-created WordPress and is arguably the face of the <a rel="nofollow" href="https://wordpress.org/">open-source project</a>, tried to put a positive spin on the announcement, writing that the company “decided to design the most generous buy-out package possible, we called it an Alignment Offer.”</p>

<p>“HR added some extra details to sweeten the deal; we wanted to make it as enticing as possible,” he wrote, and later on added: “159 people took the offer, 8.4% of the company, the other 91.6% gave up $126M of potential severance to stay!” </p>

<p>“It was an emotional roller coaster of a week. The day you hire someone, you aren’t expecting them to resign or be fired; you’re hoping for a long and mutually beneficial relationship. Every resignation stings a bit,” Mullenweg wrote.</p>

<p>Mullenweg and Automattic have been in a skirmish with WP Engine for almost two weeks now, in which the CEO has called WP Engine a “cancer to WordPress,” accusing it of wrongfully using the WordPress and WooCommerce trademarks, and <a href="https://techcrunch.com/2024/09/25/wordpress-org-bans-wp-engine-blocks-it-from-accessing-its-resources/">banning the company</a> from accessing the open-source WordPress.org resources.</p>


<p>Both WP Engine and Automattic have <a href="https://techcrunch.com/2024/09/23/wp-engine-sends-cease-and-desist-letter-to-automattic-over-mullenwegs-comments/">sent</a> each other <a href="https://techcrunch.com/2024/09/25/legal-ping-pong-in-the-wordpress-world-continues-automattic-now-sends-wp-engine-a-cease-and-desist-letter-alleging-trademark-infringement/" target="_blank" rel="noreferrer noopener">cease-and-desist letters</a>. And WP Engine earlier on Thursday <a href="https://techcrunch.com/2024/10/02/wp-engine-sues-automattic-and-wordpress-co-founder-matt-mullenweg/" target="_blank" rel="noopener">filed a lawsuit against Automattic and Mullenweg</a>, accusing the company and its CEO of “abuse of power,” extortion, and saying the WordPress co-creator has conflicts of interest in handling WordPress as an open-source project.</p>

<p>Automattic has so far called all of WP Engine’s claims meritless. “I stayed up last night reading WP Engine’s Complaint, trying to find any merit anywhere to it. The whole thing is meritless, and we look forward to the federal court’s consideration of their lawsuit,” the company’s legal representative, Neal Katyal, said in a <a rel="nofollow" href="https://automattic.com/2024/10/03/meritless/">blog post</a>.</p>

<p>Over the last few days, <a rel="nofollow" href="https://x.com/jeffr0/status/1841585100665872569">several</a> <a rel="nofollow" href="https://x.com/BoweFrankema/status/1841836017810092059">people</a> on <a rel="nofollow" href="https://x.com/kellie/status/1841210258422972536">X</a> have hinted about a severance offer being circulated among Automattic employees. Mullenweg also <a rel="nofollow" href="https://medium.com/@kelliepeterson/nice-guy-matt-mullenweg-ceo-of-wordpress-com-cries-foul-and-threatens-me-with-legal-action-f116ac57d862">allegedly DM’d a former employee who posted about the offer</a> and accused her of attacking the company and him.</p>







<p>Today, <a rel="nofollow" href="https://x.com/p3ob7o/status/1842078298030956576">some Automattic employees</a> who opted to keep their jobs <a rel="nofollow" href="https://x.com/richard_tabor/status/1842062384820633742">posted messages</a> in support of the company and Mullenweg.</p>

<p><em>You can contact this reporter at im@ivanmehta.com or on Signal: @ivan.42</em><br></p>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Reverse Engineering and Dismantling Kekz Headphones (195 pts)]]></title>
            <link>https://nv1t.github.io/blog/kekz-headphones/</link>
            <guid>41738552</guid>
            <pubDate>Fri, 04 Oct 2024 06:55:58 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://nv1t.github.io/blog/kekz-headphones/">https://nv1t.github.io/blog/kekz-headphones/</a>, See on <a href="https://news.ycombinator.com/item?id=41738552">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>Close to a year ago, I stumbled upon the Kekz Headphones, which seemed like an interesting approach on the whole digital audio device space. They claimed to work without any internet connection and all of the content already on the headphones itself. They are On-Ear Headphones, which work by placing a small chip (I call them “Kekz” or “Cookie”) into a little nook on the side and it plays an audio story.
I was intrigued, because there were some speculations going around, how they operate with those “Kekz”-Chips.</p><p>I invite you to join me on a journey into the inner workings of those headphones. We will talk about accessing the encrypted files on the device, breaking the crypto and discovering disclosure of data from customers.</p><h2 id="opening-the-headphones">Opening the headphones</h2><p>I sourced my headphones from “Kleinanzeigen” (something like craigslist, or facebook marketplace) to keep the research costs low and maybe get some cookies with it. I got the wonderful colour red.
<img src="https://nv1t.github.io/blog/img/2024/9fedcf0c1ac98650a8055d6744523e91.png" alt="Headphones opened up, PCBs hanging out, Kekz chips are lying to the left.">
After opening up the headphones, you will have 2 PCBs which are connected by 7 wires. Two speakers and a battery. The chinese lettering in the silk layer is just the colour description of the wires itself. You don’t see any interesting breakout for anything here. The Pin-Row in the middle is for the NFC antenna on the other side of the board. You see two Vias with the label <code>DP</code> and <code>DM</code>, which is on the USB line. This will be interesting at a later stage.</p><p><img src="https://nv1t.github.io/blog/img/2024/9c2962c7adb42ddbf4c88bb9f44b7d7a.jpg" alt="PCB of one of the ears, which has the important chips on it. Description is in the text.">
The first thing that stands out is a Jieli Chip, which appears to be the core component of the entire headset. These chips are mostly used in cheap Bluetooth hardware and are difficult to determine which version is currently running. From a quick search i think this Chip (<code>AC21BP0H733-51C8</code>) is probably some version of the <code>AC6951C</code>.</p><p>The chip on the bottom, <code>TSC9883</code>, is a NFC Reader IC, which I don’t care for.</p><p>It has two infrared proximity sensors to detect the ear and cookie insertion to prevent from reading a cookie more than once and stop when taking off the headphones.</p><p>On the right of the PCB you see an SD cardholder, which has a 32gb SD Card on the inside. The SD Card has a Fat32 Filesystem with 276 directories. There is an update, which ups that to around 369 directories. Each directory has multiple files with the extension <code>kez</code>, which are most likely encrypted.</p><p><img src="https://nv1t.github.io/blog/img/2024/87e3e2c1ef721f3e60733fc2e0e3149e.png" alt="Directory and Filelisting from the SD Card"></p><p>Interestingly, I was looking at the SD Card before and I connected the headphones to the accompanied Windows Application. After the connection, the files were gone and I was kinda puzzled, until I found the following code in the application:</p><div><pre tabindex="0"><code data-lang="csharp"><span><span><span>public</span> <span>static</span> <span>void</span> HideFolders()
</span></span><span><span>{
</span></span><span><span>	<span>if</span> (Globals.Drive == <span>null</span>)
</span></span><span><span>	{
</span></span><span><span>		<span>throw</span> <span>new</span> Exception(<span>"Drive not set"</span>);
</span></span><span><span>	}
</span></span><span><span>	<span>string</span> drive = Globals.Drive;
</span></span><span><span>	<span>string</span>[] directories = Directory.GetDirectories(drive);
</span></span><span><span>	<span>for</span> (<span>int</span> i = <span>0</span>; i &lt; directories.Length; i++)
</span></span><span><span>	{
</span></span><span><span>		DirectoryInfo directoryInfo = <span>new</span> DirectoryInfo(directories[i]);
</span></span><span><span>		<span>if</span> (<span>int</span>.TryParse(directoryInfo.Name, <span>out</span> <span>var</span> _))
</span></span><span><span>		{
</span></span><span><span>			directoryInfo.Attributes |= FileAttributes.Hidden;
</span></span><span><span>		}
</span></span><span><span>	}
</span></span><span><span>	<span>new</span> FileInfo(Path.Combine(drive, <span>"kekzId.json"</span>)).Attributes |= FileAttributes.Hidden;
</span></span><span><span>}
</span></span></code></pre></div><p>They seem to set the hidden Attribute on the first connect, so the files are not easily discovered.</p><p>We have multiple options to attack this system. We can either try to dump the firmware of the main controller chip and reverse engineer that, or we understand, how a “Kekz” works.</p><p>As we currently have little information about the controller, and we haven’t looked into the NFC Communication yet, let’s first check the cookies themselves and go the easy route.</p><h2 id="you-get-a-cookie-and-you-get-a-cookie">You get a cookie and you get a cookie.</h2><p>I have a <a href="https://proxmark.com/">Proxmark3</a> lying around and if we try to “just read” a Kekz, it will result in an error, as it seems to be locked:</p><pre tabindex="0"><code>[usb] pm3 --&gt; hf mfu info

[=] --- Tag Information --------------------------
[+]       TYPE: NTAG 213 144bytes (NT2H1311G0DU)

[=] --- Tag Counter
[=]        [02]: 00 00 00
[+]             - 00 tearing ( fail )

[=] --- Tag Signature
[=]     Elliptic curve parameters: NID_secp128r1
[=]              TAG IC Signature: 0000000000000000000000000000000000000000000000000000000000000000
[+]        Signature verification ( fail )

[=] --- Tag Version
[=]        Raw bytes: 00 53 04 02 01 00 0F 03
[=]        Vendor ID: 53, Shanghai Feiju Microelectronics Co. Ltd. China
[=]     Product type: NTAG
[=]  Product subtype: 02, 50pF
[=]    Major version: 01
[=]    Minor version: 00
[=]             Size: 0F, (256 &lt;-&gt; 128 bytes)
[=]    Protocol type: 03, ISO14443-3 Compliant
[?] Hint: try `hf mfu pwdgen -r` to get see known pwd gen algo suggestions
[=] ------------------------ Fingerprint -----------------------
[=] Reading tag memory...
[#] Cmd Error: 00
[#] Read block 0 error
[!] ⚠️  Failed reading card
[=] ------------------------------------------------------------

[=] Tag appears to be locked, try using a key to get more info
</code></pre><p>We could either Brute-Force (not sure, if this will result in a locked chip), or we can just sniff the communication between the headset and the cookie. By holding the proxmark near the reader of the headset and inserting a cookie, we will get the whole communication between the reader itself and this cookie, which reveals the authentication Key.</p><p><img src="https://nv1t.github.io/blog/img/2024/a1d514983cf67277c9e02790df905202.png" alt="Trace of the NFC communication with the password for the chips highlighted in a red box">
It then tries to read the 0 block and the 7th block. The 0 block is only the ID of the tag which bears no relevance. If we look into the 7th block and further, though, we can see a string “en002071696263”, if we dump the whole cookie. That is quite interesting.</p><pre tabindex="0"><code>[usb] pm3 --&gt; hf mfu dump -k FFFFFFFF
[+] TYPE: NTAG 213 144bytes (NT2H1311G0DU)
[+] Reading tag memory...

[=] MFU dump file information
[=] -------------------------------------------------------------
[...redacted...]
[=] -------------------------------------------------------------
[=] block#   | data        |lck| ascii
[=] ---------+-------------+---+------
[=]   0/0x00 | 53 BA 20 41 |   | S. A
[=]   1/0x01 | 46 70 00 01 |   | Fp..
[=]   2/0x02 | 37 48 00 00 |   | 7H..
[=]   3/0x03 | E1 10 12 00 | 0 | ....
[=]   4/0x04 | 01 03 A0 0C | 0 | ....
[=]   5/0x05 | 34 03 00 FE | 0 | 4...
[=]   6/0x06 | 00 00 00 00 | 0 | ....
[=]   7/0x07 | 65 6E 30 30 | 0 | en00
[=]   8/0x08 | 32 30 37 31 | 0 | 2071
[=]   9/0x09 | 36 39 36 32 | 0 | 6962
[=]  10/0x0A | 36 33 FE 00 | 0 | 63..
[...redacted...]
[=] ---------------------------------
[=] Using UID as filename
[+] saved 236 bytes to binary file /.proxmark3/files/hf-mfu-53BA2046700001-dump.bin
[+] saved 59 blocks to text file ./.proxmark3/files/hf-mfu-53BA2046700001-dump.eml
[+] saved to json file /.proxmark3/files/hf-mfu-53BA2046700001-dump.json
</code></pre><p>We can check our theory by copying over this string to another cookie, and you will discover, it works. Therefore, <strong>we can clone cookies now.</strong></p><p><img src="https://nv1t.github.io/blog/img/2024/8ca30bafd2e46943f989143f33dab0b8.png" alt="Image of a bunny and cat looking alike next to each other."></p><p>Even if the outside looks different, it plays the same content and is a bunny by heart.</p><p>At this point, it is possible to read this string and build a database to decrypt all content. We just have access to the ones we have already seen.
As those tags are 13.35Mhz, is is also possible to write them by your Phones NFC (you will see this later on)</p><h2 id="what-does-this-string-mean">What does this string mean?</h2><p>We have this weird string <code>en002071696263</code> which has something to do with playing the content and we know the content is probably one of those directories we did see earlier on the SD-card.</p><p>If we begin to delete one directory after the other, we can determine which directory has the desired content inside. For this cookie, the directory is <code>0020</code>. If we look into other cookies we will see the structure is:</p><table><thead><tr><th>Cookie</th><th>String</th></tr></thead><tbody><tr><td>Cookie Crew 1</td><td>en 0020 71696263</td></tr><tr><td>Cookie Crew 1</td><td>en 0020 71696263</td></tr><tr><td>Feuerwehrman Sam</td><td>en 0002 6161777a</td></tr><tr><td>Was ist Was</td><td>en 0031 67766172</td></tr><tr><td>Hotzenplotz</td><td>en 0006 73657463</td></tr></tbody></table><p>I can move those files to a directory <code>4444</code>, the files will be played, but garbage output. This means the <code>0020</code> is important for the decryption phase.
Renaming <code>0020</code> to something else will result in an “unbaked” Chip.
If i move the files to a directory <code>1020</code>, they will be played just fine, after rebranding the chip to <code>en102071696263</code></p><p>We could try more stuff to understand the encryption, but…let us recap</p><ol><li>the four integers number after <code>en</code> is the directory</li><li>they are partially important for the decryption.</li><li>four bytes in the end, we don’t know the purpose, but they are essential for decryption</li></ol><h2 id="moargive-me-moaaaaar">Moar…give me moaaaaar</h2><p>The only crux is, we only can decrypt stuff we have already seen, but i want to have an attack on everything.</p><p>We could brute force the 4 Bytes. Without any further assumption, this would be <code>255**4</code> possibilities, which is way to many.</p><p>But if we look into the last four bytes more closely, we can assume one last thing:
In our examples, the four bytes are hex representation for four small letters from the alphabet.</p><p>With this assumption, we can bring this down to <code>26**4</code>. That sounds more reasonable, but can we attack the crypto further?</p><p>Lucky for us, they published an application which can write a cookie named “Wunderkekz”. This App can encrypt arbitrary MP3 files to the correct <code>kez</code>-Fileformat. And more Lucky for us: it is written in csharp.</p><p>Therefore, we can take a look into the encryption routine (i translated it to python, variable naming directly from the original decompilation):</p><div><pre tabindex="0"><code data-lang="python"><span><span>str_crumb_hex <span>=</span> sys<span>.</span>argv[<span>3</span>] <span># #"E9-F5-33-6B" # Assuming this value based on your previous examples</span>
</span></span><span><span>directory_raw <span>=</span> sys<span>.</span>argv[<span>1</span>]
</span></span><span><span>filename_raw <span>=</span> sys<span>.</span>argv[<span>2</span>]
</span></span><span><span>
</span></span><span><span>directory <span>=</span> bytearray(directory_raw, <span>'ascii'</span>)
</span></span><span><span>filename <span>=</span> bytearray(filename_raw, <span>'ascii'</span>)
</span></span><span><span>array <span>=</span> str_crumb_hex<span>.</span>split(<span>'-'</span>)
</span></span><span><span>b, b2, b3, b4 <span>=</span> [int(value, <span>16</span>) <span>for</span> value <span>in</span> array]
</span></span><span><span>str_crumb_hex_unpacked <span>=</span> bytearray([b, b2, b3, b4])
</span></span><span><span>b5 <span>=</span> (str_crumb_hex_unpacked[<span>0</span>] <span>^</span> directory[<span>0</span>]) <span>&gt;&gt;</span> <span>4</span>
</span></span><span><span>b6 <span>=</span> (str_crumb_hex_unpacked[<span>1</span>] <span>^</span> directory[<span>1</span>]) <span>&gt;&gt;</span> <span>5</span>
</span></span><span><span>b7 <span>=</span> (str_crumb_hex_unpacked[<span>2</span>] <span>^</span> directory[<span>2</span>]) <span>&gt;&gt;</span> <span>3</span>
</span></span><span><span>b8 <span>=</span> (str_crumb_hex_unpacked[<span>3</span>] <span>^</span> directory[<span>3</span>]) <span>&gt;&gt;</span> <span>2</span>
</span></span><span><span>
</span></span><span><span>array3 <span>=</span> bytearray([b5, b6, b7, b8])
</span></span><span><span>
</span></span><span><span>b9 <span>=</span> (filename[<span>0</span>] <span>+</span> filename[<span>1</span>] <span>+</span> filename[<span>2</span>] <span>+</span> filename[<span>3</span>]) <span>%</span> <span>10</span> <span>-</span> <span>1</span>
</span></span><span><span><span>if</span> b9 <span>&gt;=</span> <span>9</span> <span>or</span> b9 <span>&lt;</span> <span>0</span>:
</span></span><span><span>    b9 <span>=</span> <span>6</span>
</span></span><span><span>
</span></span><span><span><span>with</span> open(<span>"</span><span>%s</span><span>/</span><span>%s</span><span>.mp3"</span> <span>%</span> (directory_raw,filename), <span>'rb'</span>) <span>as</span> file_stream:
</span></span><span><span>	array4 <span>=</span> bytearray(file_stream<span>.</span>read())
</span></span><span><span>	array5 <span>=</span> bytearray(len(array4))
</span></span><span><span>	<span>for</span> i <span>in</span> range(len(array4)):
</span></span><span><span>		array4[i] <span>^=</span> array3[i <span>%</span> <span>4</span>]
</span></span><span><span>		array5[i] <span>=</span> ((array4[i] <span>&gt;&gt;</span> (<span>8</span> <span>-</span> b9)) <span>|</span> (array4[i] <span>&lt;&lt;</span> b9)) <span>&amp;</span> <span>0xFF</span>
</span></span><span><span>
</span></span><span><span><span>with</span> open(<span>"</span><span>%s</span><span>/</span><span>%s</span><span>.kez"</span> <span>%</span> (directory_raw,filename),<span>'wb'</span>) <span>as</span> fh:
</span></span><span><span>	fh<span>.</span>write(array5)
</span></span></code></pre></div><ul><li>Opens the MP3 file for reading.</li><li>Reads the entire file into a byte array <code>array4</code>.</li><li>Creates a new byte array <code>array5</code> to hold the encrypted data.</li><li>Iterates through <code>array4</code>, performing the following operations on each byte:<ul><li>XORs the byte with an element of <code>array3</code> (which is some kind of key) (selected in a round-robin fashion).</li><li>Performs a bitwise rotation on the byte, using <code>b9</code> as the shift count, and stores the result in <code>array5</code>.</li></ul></li><li>Closes and disposes of the input file stream.</li></ul><h2 id="attacking-the-crypto">Attacking the Crypto</h2><p>As the shift and 4 Byte XOR key depends on the directory name and unknown 4 Byte Hex Key, we can pre calc a brute-force table which significantly limits the keyspace. We can safely assume the unknown 4-Byte Hex Key to be in a printable state.</p><p>This can be seen in the source code of the Kekz App As well (Source: <code>public static bool GenerateKekzCryptFiles</code>)</p><div><pre tabindex="0"><code data-lang="csharp"><span><span><span>string</span> s = RandomGenerator.RandomString(<span>4</span>, lowerCase: <span>true</span>);
</span></span><span><span>strCrumbHex = BitConverter.ToString(Encoding.ASCII.GetBytes(s));
</span></span></code></pre></div><p>The following factors reduce the keyspace significantly:</p><ol><li><strong>shifts reduces the keyspace down to 18 bits</strong>:<ul><li><strong><code>b5</code></strong> has 16 possible values (because it’s reduced to 4 bits after the shift).</li><li><strong><code>b6</code></strong> has 8 possible values (because it’s reduced to 3 bits after the shift).</li><li><strong><code>b7</code></strong> has 32 possible values (because it’s reduced to 5 bits after the shift).</li><li><strong><code>b8</code></strong> has 64 possible values (because it’s reduced to 6 bits after the shift).</li></ul></li><li><strong>XOR with the directory</strong>: Each character in the <code>directory</code> string has a limited value range from 48 (ASCII for ‘0’) to 57 (ASCII for ‘9’), or just 10 different possible values for each character.</li><li><strong>Collision</strong>: After the XOR and bit shifts, many different input values may end up producing the same result. The bit shifts cause significant information loss, and many different values could end up mapping to the same <code>b5, b6, b7, b8</code> combination, leading to a large number of collisions. These collisions reduce the number of unique keys generated.</li></ol><p>By writing all possible keys into a Dictionary, we don’t need to sort and uniq an array afterward.
This results in ~56 possible keys to decrypt the content.</p><div><pre tabindex="0"><code data-lang="python"><span><span>characters <span>=</span> <span>"abcdefghijklmnopqrstuvwxyz"</span>
</span></span><span><span>
</span></span><span><span><span>def</span> <span>keygen</span>(l):
</span></span><span><span>	<span>yield from</span> itertools<span>.</span>product(<span>*</span>([l] <span>*</span> <span>4</span>))
</span></span><span><span>
</span></span><span><span><span>def</span> <span>pre_calc_array3</span>(directory, filename):
</span></span><span><span>	ret <span>=</span> {}
</span></span><span><span>	<span>for</span> x <span>in</span> tqdm(keygen(characters),total<span>=</span>len(characters)<span>**</span><span>4</span>):
</span></span><span><span>		str_crumb_hex <span>=</span> <span>'-'</span><span>.</span>join([hex(ord(i))[<span>2</span>:] <span>for</span> i <span>in</span> x])
</span></span><span><span>		array <span>=</span> str_crumb_hex<span>.</span>split(<span>'-'</span>)
</span></span><span><span>		b, b2, b3, b4 <span>=</span> [int(value, <span>16</span>) <span>for</span> value <span>in</span> array]
</span></span><span><span>		str_crumb_hex_unpacked <span>=</span> bytearray([b, b2, b3, b4])
</span></span><span><span>		b5 <span>=</span> (str_crumb_hex_unpacked[<span>0</span>] <span>^</span> directory[<span>0</span>]) <span>&gt;&gt;</span> <span>4</span>
</span></span><span><span>		b6 <span>=</span> (str_crumb_hex_unpacked[<span>1</span>] <span>^</span> directory[<span>1</span>]) <span>&gt;&gt;</span> <span>5</span>
</span></span><span><span>		b7 <span>=</span> (str_crumb_hex_unpacked[<span>2</span>] <span>^</span> directory[<span>2</span>]) <span>&gt;&gt;</span> <span>3</span>
</span></span><span><span>		b8 <span>=</span> (str_crumb_hex_unpacked[<span>3</span>] <span>^</span> directory[<span>3</span>]) <span>&gt;&gt;</span> <span>2</span>
</span></span><span><span>		array3 <span>=</span> bytearray([b5, b6, b7, b8])
</span></span><span><span>
</span></span><span><span>		ret[<span>f</span><span>"</span><span>{</span>array3[<span>0</span>]<span>}</span><span>,</span><span>{</span>array3[<span>1</span>]<span>}</span><span>,</span><span>{</span>array3[<span>2</span>]<span>}</span><span>,</span><span>{</span>array3[<span>3</span>]<span>}</span><span>"</span>] <span>=</span> array3
</span></span><span><span>	<span>return</span> ret
</span></span></code></pre></div><p>Currently, i take one file, calculate the shift <code>b9</code> and decrypt the file multiple times for every possible key found from the <code>pre_calc_array3</code> method</p><div><pre tabindex="0"><code data-lang="python"><span><span>print(<span>"Starting Brute Force"</span>)
</span></span><span><span><span>for</span> i <span>in</span> tqdm(array3_poss):
</span></span><span><span>	na <span>=</span> i<span>.</span>replace(<span>','</span>,<span>'-'</span>)
</span></span><span><span>	array3 <span>=</span> array3_poss[i]
</span></span><span><span><span>#return True</span>
</span></span><span><span>	<span>with</span> open(<span>"</span><span>%s</span><span>/</span><span>%s</span><span>/</span><span>%s</span><span>.kez"</span> <span>%</span> (location,directory<span>.</span>decode(<span>'utf-8'</span>),filename<span>.</span>decode(<span>'utf-8'</span>)),<span>'rb'</span>) <span>as</span> fh:
</span></span><span><span>		array6 <span>=</span> bytearray(fh<span>.</span>read())
</span></span><span><span>		array4_reversed <span>=</span> bytearray(len(array6))
</span></span><span><span>		<span>for</span> i <span>in</span> range(len(array6)):
</span></span><span><span>			<span># Reverse the bitwise rotation</span>
</span></span><span><span>			array4_reversed[i] <span>=</span> ((array6[i] <span>&lt;&lt;</span> (<span>8</span> <span>-</span> b9)) <span>|</span> (array6[i] <span>&gt;&gt;</span> b9)) <span>&amp;</span> <span>0xFF</span>
</span></span><span><span>			<span># Reverse the XOR operation</span>
</span></span><span><span>			array4_reversed[i] <span>^=</span> array3[i <span>%</span> <span>4</span>]
</span></span><span><span>
</span></span><span><span>
</span></span><span><span>	<span>with</span> tempfile<span>.</span>NamedTemporaryFile(<span>'wb'</span>) <span>as</span> fh:
</span></span><span><span>		fh<span>.</span>write(array4_reversed)
</span></span><span><span>
</span></span><span><span>		returned_output <span>=</span> os<span>.</span>popen(<span>"mpck -q </span><span>%s</span><span>"</span> <span>%</span> (fh<span>.</span>name))<span>.</span>read()
</span></span><span><span>		<span>if</span> <span>": Ok"</span> <span>in</span> returned_output:
</span></span><span><span>			<span>break</span>
</span></span><span><span><span>else</span>:
</span></span><span><span>	<span>return</span> <span>False</span>
</span></span></code></pre></div><p>The Main Problem relies on checking for a valid MP3 files. Because of the shift and the 4 byte xor key you need to check every MP3 Frame, which takes time on larger files.
I currently use an external tool called “<a href="https://github.com/Sjord/checkmate">checkmate</a>”. It has the most robust MP3 validity solution. It basically checks every Frame. (Maybe Fork and implement it in Python? )</p><h2 id="do-you-have-an-app-for-that">Do you have an App for that?</h2><p>I’ve created, for my use and not publication, a small application to read and write the Cookies with my mobile phone. “Kekzmonster” takes in QR Codes, or reads the cookie with NFC and can back up all cookies in my possession. It is not intended to unlock content, which i don’t own.</p><p><img src="https://nv1t.github.io/blog/img/2024/749b63fc6e776f7ce19aad16eca2d7dc.png" alt="Screenshot of my Application Kekzmonster."></p><p>The Encryption/Decryption String can be written to any cookie with this application as well.</p><h2 id="files-without-breaking-the-headphones">Files without breaking the headphones</h2><p>We can now encrypt, decrypt and brute force the cookie content, but you won’t get ny files onto or from the headset on your own, without opening up the headphones and accessing the SD Card. Connecting the headphones to an USB port only charges them and they are listed within Linux as HID Device:</p><pre tabindex="0"><code>Bus 003 Device 012: ID 33f5:0001 Kekz Gmbh kekz headphone
Device Descriptor:
  bLength                18
  bDescriptorType         1
  bcdUSB               1.10
  bDeviceClass            0
  bDeviceSubClass         0
  bDeviceProtocol         0
  bMaxPacketSize0        64
  idVendor           0x33f5
  idProduct          0x0001
  bcdDevice            1.00
  iManufacturer           1 Kekz Gmbh
  iProduct                2 kekz headphone
  iSerial                 3 2021082200001002
  bNumConfigurations      1
  Configuration Descriptor:
    bLength                 9
    bDescriptorType         2
    wTotalLength       0x0022
    bNumInterfaces          1
    bConfigurationValue     1
    iConfiguration          0
    bmAttributes         0x80
      (Bus Powered)
    MaxPower              100mA
    Interface Descriptor:
      bLength                 9
      bDescriptorType         4
      bInterfaceNumber        0
      bAlternateSetting       0
      bNumEndpoints           1
      bInterfaceClass         3 Human Interface Device
      bInterfaceSubClass      0
      bInterfaceProtocol      0
      iInterface              0
        HID Device Descriptor:
          bLength                 9
          bDescriptorType        33
          bcdHID               2.01
          bCountryCode            0 Not supported
          bNumDescriptors         1
          bDescriptorType        34 Report
          wDescriptorLength      27
         Report Descriptors:
           ** UNAVAILABLE **
      Endpoint Descriptor:
        bLength                 7
        bDescriptorType         5
        bEndpointAddress     0x82  EP 2 IN
        bmAttributes            3
          Transfer Type            Interrupt
          Synch Type               None
          Usage Type               Data
        wMaxPacketSize     0x0008  1x 8 bytes
        bInterval               1
Device Status:     0x0000
  (Bus Powered)
</code></pre><p>We talked about the two vias <code>DP</code> and <code>DM</code>. Interestingly, we are not finding any other connection to the chip, no UART, no JTAG, nothing.
Looking into Jie-li, they are pretty weird chips. The documentation and various sources, state they are programmed over the normal USB Data lines. There are two main methods to control them.</p><h2 id="signaling-dpdm-on-usb">Signaling DP/DM on USB</h2><p>The normal way to put the chip into DFU mode would be sending a custom pullup/pulldown over the <code>D+</code> and <code>D-</code>:
<img src="https://nv1t.github.io/blog/img/2024/ad221d82128b7872b20afc5731412ff7.png" alt="">
After this signal, <code>D+</code> and <code>D-</code> gets pulled to Ground for <code>2ms</code> and the device boots up into the DFU Mode with uboot.</p><p>There are special programmer to achieve this, but i’ve seen a post, where somebody build his/her own programmer with an arduino.</p><p>I tried to achieve this with a raspberry pi pico and couldn’t get the Chip into DFU mode. I even tried to use the special programmer for this, but still…no luck.</p><p>In addition, i thought, the windows application does some magic to connect them and read/write content of the headphones. How?! That has to work without any extra hardware and you don’t have such control over the USB data lines from an operating system application.</p><h2 id="hid-communication">HID Communication</h2><p>The other more convenient option for these chips are: they might have special commands over HID which reconnect them in different stages. These are not really documented and can be different for each chip, as it depends on the firmware (i think so, that is what i got in rough translations).</p><h3 id="dfu-mode">DFU Mode</h3><p>Using the Python HID Library, this is effortless. The important thing to know is the <code>dfu_payload</code> which get’s send to the device.</p><div><pre tabindex="0"><code data-lang="python"><span><span>dfu_payload <span>=</span> [<span>0</span>, <span>85</span>, <span>170</span>, <span>1</span>, <span>2</span>, <span>3</span>, <span>4</span>, <span>170</span>, <span>85</span>]
</span></span><span><span>
</span></span><span><span>device <span>=</span> hid<span>.</span>device()
</span></span><span><span>
</span></span><span><span>device<span>.</span>open(vendor_id,product_id)
</span></span><span><span>print(<span>f</span><span>"HID: Found Device: </span><span>{</span>device<span>.</span>get_manufacturer_string()<span>}</span><span> </span><span>{</span>device<span>.</span>get_product_string()<span>}</span><span>"</span>)
</span></span><span><span>
</span></span><span><span>data <span>=</span> bytearray(byte_array_left_pad([<span>0</span>, <span>33</span>, <span>9</span>, <span>0</span>, <span>2</span>, <span>1</span>, <span>0</span>, <span>64</span>, <span>0</span>], <span>0</span>, <span>65</span>))
</span></span><span><span>buffer_payload <span>=</span> bytearray(byte_array_left_pad(dfu_payload, <span>0</span>, <span>65</span>))
</span></span><span><span>
</span></span><span><span>device<span>.</span>write(data)
</span></span><span><span>
</span></span><span><span><span>try</span>:
</span></span><span><span>	device<span>.</span>write(buffer_payload)
</span></span><span><span><span>except</span> <span>Exception</span>:
</span></span><span><span>	print(<span>"</span><span>\n</span><span>Communication Error or DFU Success..."</span>)
</span></span></code></pre></div><p>You can now flash new firmware on the chip.</p><h3 id="connecting-to-copy">Connecting to copy</h3><p>The same will go for the connection of the headphones as “normal USB Stick”. The payload is a little bit different:</p><div><pre tabindex="0"><code data-lang="python"><span><span>connect_payload <span>=</span> [<span>0</span>, <span>85</span>, <span>170</span>, <span>3</span>, <span>1</span>, <span>41</span>, <span>40</span>, <span>170</span>, <span>85</span>]
</span></span></code></pre></div><p>After sending this payload, the headphones disconnect and reconnect as a normal USB Stick.
Keep in mind, that if you run or ran the application, the files are with the Windows hidden attribute.</p><p>Success…we can now encrypt custom files, put them into custom directories and write our own cookies.</p><p><strong>We now fully pwn the headphones.</strong></p><h2 id="other-discoveries">Other Discoveries</h2><p>Browsing through the source code of the application and website, i found other things, which are weren’t mentioned before.</p><h2 id="creating-a-list-of-all-public-cookies">Creating a list of all public cookies</h2><p>There are multiple cookies, which are not public yet, but as we know from above, we can decrypt all of them. Unfortunately, we have no idea, what is in those directories.
But, i found a pretty neat trick to generate a list of all already public cookies:</p><p>The Kekz Webshop is/was built upon a WordPress installation. Fortunately for us, the upload’s directory has directory listing enabled. I can download all images ever shown in this webshop.
<img src="https://nv1t.github.io/blog/img/2024/35e598415e4725d4c97b4ce7a5e14b80.png" alt="Directory Listing of the 2024/11 uploads directory of the Kekz store"></p><p>These files are pretty important for us, because you see, every cookie has an ID, for “Raeuber Hotzenplotz”, it is 1-1.0066 and interestingly, the directory is “0006”.
<img src="https://nv1t.github.io/blog/img/2024/b45c09c7774d501681d013c89f0dd13c.png" alt="close Up Picture of a cookie with the ID highlighted with a red box">
If we look into all the images, we can see product images from the packaging as well. This packaging has a barcode with this ID as well.
<img src="https://nv1t.github.io/blog/img/2024/8f2f2a131f5e45e8890c7a60319f7cb8.png" alt="Partial Image of the back packaging of a cookie.">
Therefore, we can automate downloading all images and scanning for barcodes and extracting the directory.</p><p><strong>With this information, we can determine about 1/3 of the content is officially released already in the shop.</strong></p><h2 id="moar-wunderkekze">Moar Wunderkekze</h2><p>Apparently the directories 0990 until 0996 are used for the WunderKekzChips, whereas Green, Orange and Purple are already in circulation.</p><p>Maybe the plan is to add more cookies to the mix.</p><div><pre tabindex="0"><code data-lang="csharp"><span><span><span>return</span> wChip <span>switch</span>
</span></span><span><span>{
</span></span><span><span>	WunderkekzChipEnum.Green =&gt; <span>"0996"</span>,
</span></span><span><span>	WunderkekzChipEnum.Orange =&gt; <span>"0995"</span>,
</span></span><span><span>	WunderkekzChipEnum.Purple =&gt; <span>"0994"</span>,
</span></span><span><span>	WunderkekzChipEnum.NineThree =&gt; <span>"0993"</span>,
</span></span><span><span>	WunderkekzChipEnum.NineTwo =&gt; <span>"0992"</span>,
</span></span><span><span>	WunderkekzChipEnum.NineOne =&gt; <span>"0991"</span>,
</span></span><span><span>	WunderkekzChipEnum.NineZero =&gt; <span>"0990"</span>,
</span></span><span><span>	_ =&gt; <span>"0035"</span>,
</span></span><span><span>};
</span></span></code></pre></div><h2 id="user-data-collection">User Data collection</h2><p>This topic is not so nice.</p><p>While looking through the application, i discovered some not so nice stuff, which wasn’t mentioned in the privacy policy anywhere (<a href="https://web.archive.org/web/20240305082206/https://store.kekz.com/datenschutzerklaerung/">Archive</a>). Point 1.10 about the Kekz App was added at a later stage, after my disclosure emails.</p><h3 id="id3-tags">ID3 Tags</h3><p>ID3 tags are metadata containers used to store information about an MP3 audio file, such as the song’s title, artist, album, and other details. They help media players and libraries organize and display information about the audio files. The ID3 tags are stored within the MP3 file itself.
If you are using an Wunderkekz from the Kekz company, and you use the standard windows application (because there is no other), the ID3 tags are uploaded to an Azure Cosmos database.</p><div><pre tabindex="0"><code data-lang="csharp"><span><span>WunderkekzUploadMetadata wunderkekzUploadMetadata = <span>new</span> WunderkekzUploadMetadata();
</span></span><span><span><span>try</span>
</span></span><span><span>{
</span></span><span><span>	FileInfo fileInfo = <span>new</span> FileInfo(path);
</span></span><span><span>	wunderkekzUploadMetadata.FileName = fileInfo.Name;
</span></span><span><span>	wunderkekzUploadMetadata.FileSize = fileInfo.Length;
</span></span><span><span>	wunderkekzUploadMetadata.EventId = Globals.CurrentEventId;
</span></span><span><span>	<span>using</span> TagLib.File file = TagLib.File.Create(path);
</span></span><span><span>	wunderkekzUploadMetadata.Id3Title = file.Tag.Title;
</span></span><span><span>	wunderkekzUploadMetadata.Id3Artist = file.Tag.FirstPerformer;
</span></span><span><span>	wunderkekzUploadMetadata.Id3Album = file.Tag.Album;
</span></span><span><span>	wunderkekzUploadMetadata.Id3Year = (<span>int</span>)file.Tag.Year;
</span></span><span><span>	wunderkekzUploadMetadata.Id3Track = (<span>int</span>)file.Tag.Track;
</span></span><span><span>	wunderkekzUploadMetadata.Id3Genre = file.Tag.FirstGenre;
</span></span><span><span>	wunderkekzUploadMetadata.Id3Comment = file.Tag.Comment;
</span></span><span><span>	<span>return</span> wunderkekzUploadMetadata;
</span></span><span><span>}
</span></span><span><span><span>catch</span> (Exception ex)
</span></span><span><span>{
</span></span><span><span>	Trace.WriteLine(ex.Message);
</span></span><span><span>	<span>return</span> wunderkekzUploadMetadata;
</span></span><span><span>}
</span></span></code></pre></div><h3 id="geolocation">Geolocation</h3><p>Furthermore, the application tries not only uploading the ID3 Tags, but also geolocation data, which is most likely gathered from Wi-Fi triangulation from windows itself.</p><p>The MainView calls a GeoLocation Service:</p><div><pre tabindex="0"><code data-lang="java"><span><span><span>public</span> async Task<span>&lt;</span>string<span>&gt;</span> <span>GetCurrentLocation</span>()
</span></span><span><span>{
</span></span><span><span>	string strLocation <span>=</span> <span>null</span>;
</span></span><span><span>	<span>try</span>
</span></span><span><span>	{
</span></span><span><span>		Location lastLocation <span>=</span> await Geolocation.<span>Default</span>.<span>GetLastKnownLocationAsync</span>();
</span></span><span><span>		Location location <span>=</span> (await Geolocation.<span>Default</span>.<span>GetLocationAsync</span>()) <span>??</span> lastLocation;
</span></span><span><span>		<span>if</span> (location <span>!=</span> <span>null</span>)
</span></span><span><span>		{
</span></span><span><span>			DefaultInterpolatedStringHandler defaultInterpolatedStringHandler <span>=</span> <span>new</span> DefaultInterpolatedStringHandler(1, 2);
</span></span><span><span>			defaultInterpolatedStringHandler.<span>AppendFormatted</span>(location.<span>Latitude</span>);
</span></span><span><span>			defaultInterpolatedStringHandler.<span>AppendLiteral</span>(<span>":"</span>);
</span></span><span><span>			defaultInterpolatedStringHandler.<span>AppendFormatted</span>(location.<span>Longitude</span>);
</span></span><span><span>			strLocation <span>=</span> (Globals.<span>GeoData</span> <span>=</span> defaultInterpolatedStringHandler.<span>ToStringAndClear</span>());
</span></span><span><span>		}
</span></span><span><span>		<span>return</span> strLocation;
</span></span><span><span>	}
</span></span><span><span>	<span>catch</span> (Exception ex)
</span></span><span><span>	{
</span></span><span><span>		Console.<span>WriteLine</span>(ex.<span>Message</span>);
</span></span><span><span>		<span>return</span> strLocation;
</span></span><span><span>	}
</span></span><span><span>}
</span></span></code></pre></div><p>This is also save within the Cosmos DB as seen in already present locations:</p><div><pre tabindex="0"><code data-lang="json"><span><span>{
</span></span><span><span>  <span>"id"</span>: <span>"29c01f2e-f4cd-4671-9257-a7432b15fc0d"</span>,
</span></span><span><span>  <span>"EventTypeId"</span>: <span>"1"</span>,
</span></span><span><span>  <span>"DeviceGuid"</span>: <span>"3cf4caf5-3f6e-4199-bbf0-ba1abe69a6e9"</span>,
</span></span><span><span>  <span>"WunderkekzId"</span>: <span>"994"</span>,
</span></span><span><span>  <span>"GeoLocation"</span>: <span>"48,xxxxxxxxxxxxxx:11,xxxxxxxxxxxxxx"</span>,
</span></span><span><span>  <span>"UploadedAt"</span>: <span>"2023-10-24T21:03:28.4478194+02:00"</span>,
</span></span><span><span>  <span>"_rid"</span>: <span>"E9MJAL2dw5WRAAAAAAAAAA=="</span>,
</span></span><span><span>  <span>"_self"</span>: <span>"dbs/E9MJAA==/colls/E9MJAL2dw5U=/docs/E9MJAL2dw5WRAAAAAAAAAA==/"</span>,
</span></span><span><span>  <span>"_etag"</span>: <span>"\"3e009e56-0000-0d00-0000-653815010000\""</span>,
</span></span><span><span>  <span>"_attachments"</span>: <span>"attachments/"</span>,
</span></span><span><span>  <span>"_ts"</span>: <span>1698174209</span>
</span></span><span><span>}
</span></span><span><span>{
</span></span><span><span>  <span>"id"</span>: <span>"e3e1482f-1b62-4780-aff1-70593aa79d56"</span>,
</span></span><span><span>  <span>"EventTypeId"</span>: <span>"1"</span>,
</span></span><span><span>  <span>"DeviceGuid"</span>: <span>"a7bc4474-570d-4ece-b1f5-aa3e9a36c1ce"</span>,
</span></span><span><span>  <span>"WunderkekzId"</span>: <span>"995"</span>,
</span></span><span><span>  <span>"GeoLocation"</span>: <span>"48,xxxxxxxxxxxxxx:11,xxxxxxxxxxxxxx"</span>,
</span></span><span><span>  <span>"UploadedAt"</span>: <span>"2023-10-24T21:17:59.1191164+02:00"</span>,
</span></span><span><span>  <span>"_rid"</span>: <span>"E9MJAL2dw5WSAAAAAAAAAA=="</span>,
</span></span><span><span>  <span>"_self"</span>: <span>"dbs/E9MJAA==/colls/E9MJAL2dw5U=/docs/E9MJAL2dw5WSAAAAAAAAAA==/"</span>,
</span></span><span><span>  <span>"_etag"</span>: <span>"\"3e009f58-0000-0d00-0000-653818690000\""</span>,
</span></span><span><span>  <span>"_attachments"</span>: <span>"attachments/"</span>,
</span></span><span><span>  <span>"_ts"</span>: <span>1698175081</span>
</span></span><span><span>}
</span></span></code></pre></div><p>Because the Geolocation data is only cross-referenced with die Device GUID and not the content itself, the enforcement for regional content does not make sense in their newest copy of the privacy policy.</p><h3 id="pii-data">PII Data</h3><p>There is some PII Data involved, but i think those are just test data from some ordering processes. Even the headphones can only be cross-linked to the geolocation data and not the content played.
It could maybe be cross-referenced with the time, but haven’t checked into that, as the main concern is the data being public.</p><h3 id="who-is-special">Who is Special?</h3><p>In addition, the connection string to this Azure cosmos database is accessible within the Decompilation of the application itself and therefore is disclosed.</p><p>So everybody looking into the source code of the application can get information on usage of the headphones, location of some of the headphones and files listened too.</p><p><img src="https://nv1t.github.io/blog/img/2024/ddb71ee1385f009813e51273e8dd6d1d.png" alt="Heatmap of all Geolocation Data in Germany from the headphones usage">
There is some usage in Dublin as well, but only wanted to include the DACH Region.</p><p>Normalizing ID3 Tag Meta Data from various sources is really cumbersome. I tried, but i gave up pretty quickly, because it was just my own curiosity what kids are listening to these days. Let me say that: Bibi Blocksberg, Bibi &amp; Tina, Benjaming Bluemchen, Paw Patrol, Drei Fragezeichen, and various songs, are the all time favourits. (for the english speaking community: except Paw Patrol, are all Children Listening experiences from germany, which exist since 1970 or 1980)</p><h2 id="disclosure">Disclosure</h2><p><strong>19.10.2023</strong>: i reached out to the CTO of Kekz, who told me, he developed the headphones, but is no longer associated with the company. To my knowledge, he forwarded the information to the CEOs.
Never heard back.</p><p><strong>27.02.2024</strong>: i reached out to the CEOs of Kekz (Adin and Carl) with my security concerns. Never heard back.
-&gt; A few weeks later, the privacy policy was changed to include the Kekz-App, therefore i conclude my email was read</p><h2 id="open-questions">Open Questions</h2><ul><li>What is the full functionality of the Jieli-Chip? These chips are strange and challenging to identify. I only guessed which Chip it could be and got lucky with the HID Interface through the Windows Application</li><li>What does the other Jieli-Chip on the other PCB Do?</li><li>A full application to create a custom SD Card with a content manager to not only support the content already present on the Kekz Headphones, but furthermore all non-taken directories.</li><li>Are there more HID commands?</li><li>How good is the Geolocation Data sourced from a Laptop, which is probably triangulated Wifi Signals? Do the 30m-500m from the privacy policy hold up, or can it be narrowed down?</li><li>What is this PII Data in the Azure cosmos database?</li></ul><p>There are most likely more open questions on this one. You are welcome to research further on your own.</p><h2 id="references">References</h2><ul><li><strong>Kekz Information:</strong><ul><li><a href="https://futurezone.at/produkte/kekz-kopfhoerer-im-test-kinder-in-ihrer-eigenen-welt/401978339">https://futurezone.at/produkte/kekz-kopfhoerer-im-test-kinder-in-ihrer-eigenen-welt/401978339</a> (first article i have read about the Headphones)</li><li><a href="https://stadt-bremerhaven.de/kekz-drahtlose-kinderkopfhoerer-nach-dem-tonies-prinzip/">https://stadt-bremerhaven.de/kekz-drahtlose-kinderkopfhoerer-nach-dem-tonies-prinzip/</a> (Kekz article of Caschys blog, with wrong information in the comments on how they operate)</li><li><a href="https://store.kekz.com/haendlersuche/">https://store.kekz.com/haendlersuche/</a> (Kekz - Store search)</li><li><a href="https://apps.microsoft.com/detail/9NXL6Q53G5RX?hl=de-de&amp;gl=DE">https://apps.microsoft.com/detail/9NXL6Q53G5RX?hl=de-de&amp;gl=DE</a> (Kekz application)</li></ul></li><li><strong>Checking MP3s for validity:</strong><ul><li><a href="https://github.com/Sjord/checkmate">https://github.com/Sjord/checkmate</a> (checks every frame, but not python variant exists)</li><li><a href="https://peterextexia.com/blog/verifying-that-an-mp3-file-is-valid-in-python/">https://peterextexia.com/blog/verifying-that-an-mp3-file-is-valid-in-python/</a> (it kinda works, but get’s false positives and because some mp3 decode as valid, this does not work)</li></ul></li><li><strong>Jieli-Chip:</strong><ul><li><a href="https://www.zh-jieli.com/">https://www.zh-jieli.com/</a> (chip production)</li><li><a href="http://www.yunthinker.com/FileUpLoad/DownLoadInfosFile/637729990813300469.pdf">http://www.yunthinker.com/FileUpLoad/DownLoadInfosFile/637729990813300469.pdf</a> (potential Chip)</li><li><a href="https://github.com/christian-kramer/JieLi-AC690X-Familiarization">https://github.com/christian-kramer/JieLi-AC690X-Familiarization</a> (Adventures in figuring out how this incredibly ubiquitous, yet incredibly mysterious integrated circuit works.)</li><li><a href="https://github.com/kagaimiq/jl-uboot-tool/tree/main">https://github.com/kagaimiq/jl-uboot-tool/tree/main</a> (chip and protopcol description)</li><li><a href="https://github.com/kagaimiq/jielie/tree/main">https://github.com/kagaimiq/jielie/tree/main</a> (jielie nice!)</li><li><a href="https://el.jibun.atmarkit.co.jp/thousandiy/2022/09/18_bluetooth_audio_soc.html">https://el.jibun.atmarkit.co.jp/thousandiy/2022/09/18_bluetooth_audio_soc.html</a> (Which Chips are built into cheap Bluetooth Speaker)</li></ul></li><li><strong>Misc:</strong><ul><li><a href="https://www.luther-lawfirm.com/newsroom/blog/detail/reverse-engineering-nach-dem-geschaeftsgeheimnisgesetz-geschgehg-vertragliche-ausschlussmoeglichkeiten#:~:text=a%20GeschGehG%20ist%20das%20Reverse,Gegenst%C3%A4nden%20uneingeschr%C3%A4nkt%20erlaubt">https://www.luther-lawfirm.com/newsroom/blog/detail/reverse-engineering-nach-dem-geschaeftsgeheimnisgesetz-geschgehg-vertragliche-ausschlussmoeglichkeiten#:~:text=a%20GeschGehG%20ist%20das%20Reverse,Gegenst%C3%A4nden%20uneingeschr%C3%A4nkt%20erlaubt</a>. (Reverse Engineering Rechtliche Lage)</li></ul></li></ul></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Experimental web browser optimized for rabbit-holing (533 pts)]]></title>
            <link>https://szymonkaliski.com/projects/cartographist/</link>
            <guid>41738502</guid>
            <pubDate>Fri, 04 Oct 2024 06:47:57 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://szymonkaliski.com/projects/cartographist/">https://szymonkaliski.com/projects/cartographist/</a>, See on <a href="https://news.ycombinator.com/item?id=41738502">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>Cartographist is an experimental web browser optimized for rabbit-holing.</p>
<ul>
<li>Instead of opening new windows (with <code>cmd</code>-click), Cartographist spawns horizontally scrollable panes.</li>
<li>Instead of forcing you to find things in a linear history, Cartographist shows a tree-structured outline of your browsing:
<img src="https://szymonkaliski.com/projects/cartographist/history.jpg" loading="lazy" width="720" height="280"></li>
<li>Instead of always starting fresh, Cartographist can save, and load "trails" — the exact state of the session you've left — supporting researching topics over long periods of time.</li>
</ul>
<p>For more context about the project, you can check out the longer write-up below, which originally appeared as a part of <a href="https://szymonkaliski.com/newsletter/2022-01-03-q4-2021/"><span>Q4 2021</span></a> newsletter.</p>

<h2>Transcluded from <a href="https://szymonkaliski.com/newsletter/2022-01-03-q4-2021/#cartographist"><span>Cartographist</span></a></h2>
          
<p>During the summer of 2020 I played around with an idea for research-focused web browser, embodying some of the concepts from <a href="https://szymonkaliski.com/notes/browsing-vs-searching/"><span>Browsing vs Searching</span></a> note — <em>browsing</em> being an open-ended divergent activity, and <em>searching</em> understood as information retrieval.</p>

<p><a href="https://twitter.com/szymon_k/status/1289942401318977537" target="_blank"><span>I shared a preview on Twitter</span><span>&nbsp;↗</span></a>, to a surprisingly overwhelming response, but I got distracted with other things and never got back to the project. I still occasionally get requests for sharing this, so here it is: <a href="https://github.com/szymonkaliski/cartographist" target="_blank"><span>szymonkaliski/Cartographist</span><span>&nbsp;↗</span></a>.</p>
<p>The main idea of browsing in panes was inspired by <a href="http://andymatuschak.org/" target="_blank"><span>Andy Matuschak's website layout</span><span>&nbsp;↗</span></a> and some of <a href="http://nateparrott.com/" target="_blank"><span>Nate Parrot's</span><span>&nbsp;↗</span></a> experiments around stacking mobile web browser views next to each other (which I can't seem to find anymore, sorry).
This sort of layout has a long history, starting with <a href="https://en.wikipedia.org/wiki/Miller_columns" target="_blank"><span>Miller columns</span><span>&nbsp;↗</span></a> and the original Smalltalk class browser, and is a great interface for detail-in-context browsing.</p>
<p>As an aside, I also use this technique for navigating code with Vim, where a single shortcut goes to a definition of a function in a new pane:</p>
<p><img src="https://szymonkaliski.com/newsletter/2022-01-03-q4-2021/vim-panes.jpg" loading="lazy" width="3536" height="1790"></p><p>(<a href="https://gtoolkit.com/" target="_blank"><span>Glamorous Toolkit</span><span>&nbsp;↗</span></a> is worth checking out, as it takes the idea of pane browsing to another level)</p>
<p>In theory, I also really like the idea of disk-persisted history which allows for going back to a browsing session after a while, and consciously deciding which "topic" I am in.</p>
<p>Unfortunately, in practice, I don't think having the full history is that useful.
Yes, it's sometimes good to know how you ended up somewhere, but I think what's most valuable about "research" is the synthesis part — grabbing parts of larger wholes, rearranging, recombining, thinking with the material.
A small step in this direction could be persisting scroll position or maybe selection, and making the history editable — allowing users to remove dead ends, add notes, etc.</p>
<p>Additionally, it started to feel that I'm solving this problem on a wrong level.
For example, a good window manager could replace Cartographist almost completely —
I played around with columnar layout in <a href="https://szymonkaliski.com/projects/hhtwm/"><span>HHTWM</span></a> for a bit, but lack of horizontal scrolling makes it not that useful in the end.</p>
<p>Well, let me know if you have any ideas how to make Cartographist better! Is there anything interesting here that I'm not seeing? Could it be useful to you in any way?</p>
<hr>
<p>Code is open sourced: <a href="https://github.com/szymonkaliski/cartographist" target="_blank"><span>szymonkaliski/Cartographist</span><span>&nbsp;↗</span></a>.</p><h2><a href="#backlinks" id="backlinks"><span>Backlinks</span></a></h2><ol><li><a href="https://szymonkaliski.com/newsletter/2022-01-03-q4-2021/#:~:text=I%20shared%20a%20preview,here%20it%20is%3A%20szymonkaliski%2FCartographist.&amp;text=Additionally%2C%20it%20started%20to,useful%20in%20the%20end.&amp;text=Well%2C%20let%20me%20know,you%20in%20any%20way%3F"><span><span>2022-01-03</span><span><span>A Dog, Short Ramble on "Programming", MIDI→CV, and a Rabbit-Holing Web Browser</span><sup><span>3</span></sup></span></span></a></li></ol></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[New research says blue zones can be explained by comically flawed data (211 pts)]]></title>
            <link>https://www.independent.co.uk/life-style/blue-zones-netflix-ignobel-prize-b2622952.html</link>
            <guid>41738434</guid>
            <pubDate>Fri, 04 Oct 2024 06:32:44 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.independent.co.uk/life-style/blue-zones-netflix-ignobel-prize-b2622952.html">https://www.independent.co.uk/life-style/blue-zones-netflix-ignobel-prize-b2622952.html</a>, See on <a href="https://news.ycombinator.com/item?id=41738434">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="main"><div data-newsletter-key="receiveindylifestyle" data-component="Newsletter" data-loading="lazy" data-theme-name="base"><p><img src="https://static.independent.co.uk/static-assets/images/newsletter/lifestyle/lessons-in-lifestyle.png" loading="lazy" alt="Lessons in Lifestyle"></p><div><div><p><h3 data-nosnippet="">Stay ahead of the curve with our weekly guide to the latest trends, fashion, relationships and more</h3><h3>Stay ahead of the curve with our weekly guide to the latest trends, fashion, relationships and more </h3><h3 data-nosnippet="">Stay ahead of the curve with our weekly guide to the latest trends, fashion, relationships and more </h3></p></div><div role="form" aria-label="Newsletter signup form" id="reg-lite-form"><p><label for="newsletter-checkbox">I would like to be emailed about offers, events and updates from The Independent. Read our&nbsp;<a target="_blank" rel="noreferrer" href="https://www.independent.co.uk/service/privacy-policy-a6184181.html">privacy policy</a></label></p></div></div></div><p><span>T</span>hey were supposed to be real-life fountains of youth. In March 2000 the term <a href="https://www.independent.co.uk/life-style/health-and-families/blue-zones-netflix-diet-documentary-b2408185.html">“Blue Zone”</a> was first used to describe <a href="https://www.independent.co.uk/topic/sardinia">Sardinia</a>, an Italian island that appeared to be home to a statistically improbable number of people living past the age of 100. In the decades since, four more areas have been identified around the globe where locals apparently have an increased chance of becoming a centenarian: <a href="https://www.independent.co.uk/topic/okinawa">Okinawa</a> in Japan; Nicoya in <a href="https://www.independent.co.uk/topic/costa-rica">Costa Rica</a>; <a href="https://www.independent.co.uk/topic/ikaria">Ikaria</a> in Greece and Loma Linda in <a href="https://www.independent.co.uk/topic/california">California</a>. These so-called Blue Zones have inspired countless studies, <a href="https://www.independent.co.uk/arts-entertainment/books/reviews/the-blue-zones-solution-eating-and-living-like-the-world-s-healthiest-people-by-dan-buettner-book-review-bluezone-thinking-for-a-long-and-healthy-existence-10224244.html">cookbooks</a>, travel stories and even their own Netflix documentary series (<a href="https://www.independent.co.uk/life-style/health-and-families/blue-zones-netflix-diet-documentary-b2408185.html">2023’s <em>Live to 100: Secrets of the Blue Zones</em></a>). The trouble is, the outlandish claims about the life-giving properties of these regions just don’t stand up to close scrutiny.</p><p>Last month, Dr Saul Newman of the Oxford Institute of Population Ageing was awarded the Ig Nobel Prize for his work debunking Blue Zones. Newman’s investigation into serious flaws in the data about the world’s oldest people saw him take home an award that has been handed out since 1991 for scientific research that “makes people laugh, and then think.” Newman says that when he looked into the claims about Blue Zones he found a pattern of significant data being routinely ignored if it didn’t fit the desired narrative, and statistical anomalies that could be better explained by administrative errors or cases of pension fraud. “It’s as if you gave the captain of the Titanic nine goes at it and he’s smacked into the iceberg every time,” Newman tells <em>The Independent </em>of the research. “What’s most astounding is that nobody in the academic community seems to have thought it’s ridiculous before this. It’s absurd.”</p><p>Take Sardinia, the original Blue Zone. While it was purported to be home to crowds of centenarians, European Union figures show that the island only ranks around 36-44th for longevity in the continent. Many of those who were supposed to have reached very old age in their Italian idyll turned out to in fact be dead, they just hadn’t been reported as such to the authorities. “Sometimes the mafia is involved, sometimes it’s carers,” says Newman. “There’s a lot of cases in Italy where younger relatives have just kept claiming the pension even though granddad’s out the back in the olive garden.”</p><div><figure><div data-gallery-length="3"><p><img src="https://static.independent.co.uk/2024/10/02/23/GettyImages-1235544411.jpg" srcset="https://static.independent.co.uk/2024/10/02/23/GettyImages-1235544411.jpg?quality=75&amp;width=320&amp;auto=webp 320w, https://static.independent.co.uk/2024/10/02/23/GettyImages-1235544411.jpg?quality=75&amp;width=640&amp;auto=webp 640w" loading="lazy" alt="Natividad Talia Matarrita Fonseca, 93, at home in the supposed ‘Blue Zone’ of Nicoya, Costa Rica"></p></div><figcaption>Natividad Talia Matarrita Fonseca, 93, at home in the supposed ‘Blue Zone’ of Nicoya, Costa Rica<span> <!-- -->(<!-- -->Ezequiel Becerra/AFP via Getty Images<!-- -->)</span></figcaption></figure></div><p>Something similar seems to have been happening in Nicoya in Costa Rica, where around 40-50% of centenarians were found to have misreported their ages, and on the Greek island of Ikaria. In 2015, Germany requested that Greece audit their spending as a condition of their bailout during the financial crisis. Until then, Greece had been paying pensions to around 9,000 centenarians. After the audit, that figure dropped by 72%. “That number comes from the Greek minister who was handing out the pensions,” says Newman. “He’s the guy with the most incentive to minimize the problem of anybody in the world, and he’s saying that 72% are rubbish.”</p><figure><span><svg xmlns="http://www.w3.org/2000/svg" id="7892fd45317565c6" viewBox="0 0 80 47"><path fill="#ec1a2e" d="M21.18 46.99c9.4 0 17.18-7.73 17.18-17.13 0-9.46-7.72-17.12-17.12-17.12A17.2 17.2 0 0 0 3.99 29.86c0 3.74 1.29 7.47 3.48 10.5l-.13.12A23.6 23.6 0 0 1 1.29 24.4c0-12.75 10.36-23.3 23.1-23.3a24 24 0 0 1 11.53 2.89l.57-.96A26 26 0 0 0 24.33 0 24.3 24.3 0 0 0 0 24.4c0 14.09 9.72 22.59 21.18 22.59m41.47 0c9.4 0 17.18-7.73 17.18-17.13 0-9.46-7.72-17.12-17.12-17.12a17.2 17.2 0 0 0-17.25 17.12c0 3.74 1.29 7.47 3.48 10.5l-.13.12a23.6 23.6 0 0 1-6.05-16.08c0-12.75 10.36-23.3 23.1-23.3a24 24 0 0 1 11.53 2.89l.58-.96A26 26 0 0 0 65.8 0a24.33 24.33 0 0 0-24.33 24.4c0 14.09 9.72 22.59 21.18 22.59"></path></svg></span><div><blockquote><p>When they’ve measured Okinawa it doesn’t do any of the things they claim Blue Zones do. Not even close. It’s comedically wrong</p></blockquote></div><p>Dr Saul Newman</p></figure><p>Claims about the Blue Zones have gone far beyond simply saying that people in the regions live longer than average. Author Dan Buettner, who founded the Blue Zones LLC marketing company in 2008, has outlined what he calls the “Power 9” factors prevalent in these areas that contribute to long, healthy lives. These are: exercising by moving naturally, having a purpose, “downshifting” routines to reduce stress, stopping eating when 80% full, eating largely plant-based diets, drinking 1-2 glasses of wine per day, a sense of religious faith or belonging, putting their families first and living in positive social networks.</p><p>However, when Newman looked into whether these characteristics are true in Okinawa, he found a wide gap between the claims and reality. “There’s some extraordinary cognitive dissonance going on,” he says. “The Japanese run one of the largest and longest-running nutritional surveys in the world. It covers 96% of their citizens, and when they’ve measured Okinawa it doesn’t do any of the things they claim Blue Zones do. Not even close. It’s comedically wrong.”</p><p>Take for example the claim made on <a rel="nofollow" target="_blank" href="https://www.bluezones.com/2016/11/power-9/">BlueZones.com</a> that Okinawans are disproportionately filled with “Ikigai”, a sense of purpose in their life. That’s just not true: in fact, Okinawa has the 4th highest suicide rate in Japan. Similarly, claims that the area is particularly religious don’t hold water. “They’re the least religious place in Japan,” says Newman. “93% atheist.”</p><p>How did the idea that Okinawa is a great place to grow old take root in the first place? Newman attributes it in part to the fire-bombing of the area by the US during World War II, which caused the destruction of countless birth certificates and other records. “Within Okinawa, the distribution of centenarians is predicted by who had their Hall of Records blown up,” he says. “You have an occupying army of GIs who don’t really speak Japanese replacing birth certificates because they’ve just blown them sky high. In a town where this has happened, you have more centenarians.”</p><div><figure><div data-gallery-length="3"><p><img src="https://static.independent.co.uk/2024/10/02/23/GettyImages-1366067630.jpg" srcset="https://static.independent.co.uk/2024/10/02/23/GettyImages-1366067630.jpg?quality=75&amp;width=320&amp;auto=webp 320w, https://static.independent.co.uk/2024/10/02/23/GettyImages-1366067630.jpg?quality=75&amp;width=640&amp;auto=webp 640w" loading="lazy" alt="US Marines pass each other going to and from the battlefront during the Battle of Okinawa circa June 1945"></p></div><figcaption>US Marines pass each other going to and from the battlefront during the Battle of Okinawa circa June 1945<span> <!-- -->(<!-- -->Keystone/Hulton Archive/Getty Images<!-- -->)</span></figcaption></figure></div><p>This wouldn’t be the first time that dubious record-keeping has led to false claims of extreme longevity. For decades, the Guinness Book of Records stated that the world’s oldest man was Pierre Joubert, who had lived to the grand old age of 113 years, 124 days. It later turned out that record keepers had conflated a father’s birth date with the death date of his son, who was also named Pierre Joubert. In fact, the elder Joubert had died at the more reasonable age of 65 – a fact that had been written on his death certificate the whole time, if anybody had bothered to check.</p><p>For Newman, claims about the health benefits of the supposed Blue Zone lifestyle lose all credibility when you understand how unsupported the premise is by empirical data. “Firstly, does lying to the public matter?” he asks. “Secondly, the astounding thing is that one of the guidelines is that you should drink every day at twice the NHS heavy drinking guidelines. That is a recipe for alcoholism. It’s mad that it’s being propelled as health advice. If you were a doctor telling your patient to drink every day, you’d get disbarred.”</p><p>The suggestion that daily boozing is a cornerstone of a healthy lifestyle is particularly strange when you consider the fifth Blue Zone, Loma Linda. This small city in Southern California is home to a large community of tee-total Seventh Day Adventists. The area was added to the list in 2008, and then in 2020 Buettner sold Blue Zones LLC to Adventist Health, the health care system of the Seventh Day Adventist Church. “Why are Seventh Day Adventists, who claim this as their core belief, pushing people to drink?” asks Newman. “They’re supposed to be sober. It’s just baffling to me.”</p><p><em>The Independent </em>has approached Blue Zones LLC for comment.</p><p>In the end, it might be time to retire the concept of Blue Zones and write the whole thing off as a collective case of wishful thinking. “People do not want to go jogging,” says Newman. “They don’t want to give up drinking. They don’t want to give up smoking. They want there to be some far flung, exotic island where everything’s okay and if you eat the goji berries you’re gold. It’s a nice dream, and it has always, throughout the entirety of history, sold well. But is it true? I would suggest jogging.”</p></div></div>]]></description>
        </item>
    </channel>
</rss>