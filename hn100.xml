<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0">
    <channel>
        <title>HN100 - Readable Contents</title>
        <link>https://hn.algolia.com/api/v1/search_by_date?tags=%28story,poll%29&amp;numericFilters=points%3E100</link>
        <description>Uses Readability to add bodies to the RSS feed</description>
        <lastBuildDate>Tue, 06 May 2025 19:30:02 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>en</language>
        <item>
            <title><![CDATA[Matt Godbolt sold me on Rust (by showing me C++) (129 pts)]]></title>
            <link>https://www.collabora.com/news-and-blog/blog/2025/05/06/matt-godbolt-sold-me-on-rust-by-showing-me-c-plus-plus/</link>
            <guid>43907820</guid>
            <pubDate>Tue, 06 May 2025 17:51:03 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.collabora.com/news-and-blog/blog/2025/05/06/matt-godbolt-sold-me-on-rust-by-showing-me-c-plus-plus/">https://www.collabora.com/news-and-blog/blog/2025/05/06/matt-godbolt-sold-me-on-rust-by-showing-me-c-plus-plus/</a>, See on <a href="https://news.ycombinator.com/item?id=43907820">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
<p>Matt Godbolt, of <a href="https://godbolt.org/" target="_blank" rel="noopener">Compiler Explorer</a> fame, is awesome and you should scour the web for every single bit of content he puts out. That is exactly what I was doing when I watched <a href="https://www.youtube.com/watch?v=nLSm3Haxz0I" target="_blank" rel="noopener">Correct by Construction: APIs That Are Easy to Use and Hard to Misuse</a>. After 20+ years of working with C/C++, this theme resonates a lot with me.</p>
<p>While watching the talk I kept thinking "Yes! And that's why Rust does it that way." I came out at the end thinking that this talk was actually a great way of getting the intuition for how Rust helps you beyond the whole memory safety thing, and that is what this article intends to show.</p>
<p>But before we talk about that we should talk about the problems Matt raised and how he proposes to solve them in C++. Do yourself a favor and watch the full talk, but let me break one of them down!</p>
<h3>What's in a type</h3>
<p>Matt starts the talk by showing what a function that sends orders to a stock exchange might look like.</p>
<pre>void sendOrder(const char *symbol, bool buy, int quantity, double price)</pre>
<p>Before we go any further, let me just say he acknowledges floating point is not right for price and later talks about how he usually deals with it. But it makes for a nice example, bear with us.</p>
<p>Another obvious improvement that anyone used to this stuff will call out immediately is the bool for identifying a buy. That is error prone and Matt does call it out towards the end of this section.</p>
<p>But he first focuses on quantity and price and how C++ makes it really hard for you to stop callers from confusing them: the compiler will allow 1000.00 for quantity and 100 for price with no warnings, despite them being different types. It just silently converts.</p>
<p>How about some type aliasing?</p>
<pre>#include 

using Price = double;
using Quantity = int;

void sendOrder(const char *symbol, bool buy, int quantity, double price) {
  std::cout &lt;&lt; symbol &lt;&lt; " " &lt;&lt; buy &lt;&lt; " " &lt;&lt; quantity &lt;&lt; " " &lt;&lt; price
            &lt;&lt; std::endl;
}

int main(void) {
  sendOrder("GOOG", false, Quantity(100), Price(1000.00)); // Correct
  sendOrder("GOOG", false, Price(1000.00), Quantity(100)); // Wrong
}</pre>
<p>No luck! Both clang 19 and gcc 14 will take that and not complain - <em>even with -std=c++23 -Wall -Wextra -Wpedantic</em>, which I use for all of the C++ code in this article! A few rounds of improvements and we have the following version:</p>
<pre>#include 

class Price {
public:
  explicit Price(double price) : m_price(price) {};
  double m_price;
};

class Quantity {
public:
  explicit Quantity(unsigned int quantity) : m_quantity(quantity) {};
  unsigned int m_quantity;
};

void sendOrder(const char *symbol, bool buy, Quantity quantity, Price price) {
  std::cout &lt;&lt; symbol &lt;&lt; " " &lt;&lt; buy &lt;&lt; " " &lt;&lt; quantity.m_quantity &lt;&lt; " "
            &lt;&lt; price.m_price &lt;&lt; std::endl;
}

int main(void) {
  sendOrder("GOOG", false, Quantity(100), Price(1000.00));  // Correct
  sendOrder("GOOG", false, Quantity(-100), Price(1000.00)); // Wrong
}</pre>
<p>We have classes, we have explicit constructors (very important or C++ will trip you!), we have unsigned types… and now it's hard to put a Price where you want a Quantity! But we can still give Quantity a negative value without a single compiler warning, even though we moved to an unsigned type. A bit more magic and we can make that go away:</p>
<pre>#include 
#include 

class Price {
public:
  explicit Price(double price) : m_price(price) {};
  double m_price;
};

class Quantity {
public:
  template  explicit Quantity(T quantity) : m_quantity(quantity) {
    static_assert(std::is_unsigned(), "Please use only unsigned types");
  }

  unsigned int m_quantity;
};

void sendOrder(const char *symbol, bool buy, Quantity quantity, Price price) {
  std::cout &lt;&lt; symbol &lt;&lt; " " &lt;&lt; buy &lt;&lt; " " &lt;&lt; quantity.m_quantity &lt;&lt; " "
            &lt;&lt; price.m_price &lt;&lt; std::endl;
}

int main(void) {
  sendOrder("GOOG", false, Quantity(100u), Price(1000.00)); // Correct
  sendOrder("GOOG", false, Quantity(-100), Price(1000.00)); // Wrong
}</pre>
<p>Finally we can get clang (and gcc) to complain loudly that this is being misused. All it took was a templated constructor that performs a static assert at compile time. Nice!</p>
<pre>order/order-5.cpp:13:19: error: static assertion failed due to requirement 'std::is_unsigned()': Please use only unsigned types
   13 |     static_assert(std::is_unsigned(), "Please use only unsigned types");
      |                   ^~~~~~~~~~~~~~~~~~~~~
order/order-5.cpp:26:28: note: in instantiation of function template specialization 'Quantity::Quantity' requested here
   26 |   sendOrder("GOOG", false, Quantity(-100), Price(1000.00)); // Wrong
      |                            ^
1 error generated.</pre>
<p>It's a lot of code, but at least we now have compiler protection. We are good, there is no other way to misuse quantity and price. Right?</p>
<p>What if we need to pass in a value the user typed on a UI, such that we need to convert it from string? Well, you are out of luck again:</p>
<pre>sendOrder("GOOG", false, Quantity(static_cast(atoi("-100"))),
            Price(1000.00)); // Wrong</pre>
<p>Not only will it not fail to compile, but it will also not produce any errors at runtime. You will end up selling 4294967196 shares and going bankrupt.</p>
<p>Not ideal.</p>
<p>Matt keeps going, showing a few more magic tricks (and their pitfalls) to perform the runtime checks you will need to fully defend against this. I think this is a good point for us to stop on the C++ side and look at how Rust does, shall we?</p>
<h3>Enter Rust</h3>
<p>So is Rust any better? Rust has the benefit of decades of learning from all of these issues. And learn it did. Let's take a look. How does our first attempt fare?</p>
<pre>fn send_order(symbol: &amp;str, buy: bool, quantity: i64, price: f64) {
    println!("{symbol} {buy} {quantity} {price}");
}

fn main() {
    send_order("GOOG", false, 100, 1000.00); // Correct
    send_order("GOOG", false, 1000.00, 100); // Wrong
}</pre>
<p>After all the work we had to do with C++ it can't be this easy, can it?</p>
<pre>error[E0308]: arguments to this function are incorrect
 --&gt; order/order-1.rs:7:5
  |
7 |     send_order("GOOG", false, 1000.00, 100); // Wrong
  |     ^^^^^^^^^^                -------  --- expected `f64`, found `{integer}`
  |                               |
  |                               expected `i64`, found `{float}`
  |
note: function defined here
 --&gt; order/order-1.rs:1:4
  |
1 | fn send_order(symbol: &amp;str, buy: bool, quantity: i64, price: f64) {
  |    ^^^^^^^^^^ ------------  ---------  -------------  ----------
help: swap these arguments
  |
7 |     send_order("GOOG", false, 100, 1000.00); // Wrong
  |               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~</pre>
<p>Welp, what do you know? It even tells us the arguments we swapped, and everything. It's like we live in the future! Ok, but numbers are still easy to confuse, we could also create types like we did in C++ to make it extra explicit, right? Indeed, and let's throw in protection against negative values by switching from i64 to u64. Is it that easy?</p>
<pre>struct Price(pub f64);
struct Quantity(pub u64);

fn send_order(symbol: &amp;str, buy: bool, quantity: Quantity, price: Price) {
    println!("{symbol} {buy} {} {}", quantity.0, price.0);
}

fn main() {
    send_order("GOOG", false, Quantity(100), Price(1000.00)); // Correct
    send_order("GOOG", false, Quantity(-100), Price(1000.00)); // Wrong
}</pre>
<p>Yes, it is that easy:</p>
<pre>error[E0600]: cannot apply unary operator `-` to type `u64`
  --&gt; order/order-4.rs:10:40
   |
10 |     send_order("GOOG", false, Quantity(-100), Price(1000.00)); // Wrong
   |                                        ^^^^ cannot apply unary operator `-`
   |
   = note: unsigned values cannot be negated</pre>
<p>Ok, all that remains is the case where we need to convert the number from a string the user typed. I got you now, Rust… runtime input is not something you can fix at compile time, how would you ever be able to improve on the C++ case?</p>
<pre>struct Price(pub f64);
struct Quantity(pub u64);

fn send_order(symbol: &amp;str, buy: bool, quantity: Quantity, price: Price) {
    println!("{symbol} {buy} {} {}", quantity.0, price.0);
}

fn main() {
    send_order("GOOG", false, Quantity(100), Price(1000.00)); // Correct
    send_order(
        "GOOG",
        false,
        Quantity("-100".parse::()),
        Price(1000.00),
    ); // Wrong
}</pre>
<p>How about forcing the user to handle potential bad conversions caused by the target type not being able to represent the number we have on the string, does that help?</p>
<pre>error[E0308]: mismatched types
  --&gt; order/order-6.rs:13:18
   |
13 |         Quantity("-100".parse::()),
   |         -------- ^^^^^^^^^^^^^^^^^^^^^ expected `u64`, found `Result&lt;u64, ParseIntError&gt;`
   |         |
   |         arguments to this struct are incorrect
   |
   = note: expected type `u64`
              found enum `Result&lt;u64, ParseIntError&gt;`
note: tuple struct defined here
  --&gt; order/order-6.rs:2:8
   |
2  | struct Quantity(pub u64);
   |        ^^^^^^^^
help: consider using `Result::expect` to unwrap the `Result&lt;u64, ParseIntError&gt;` value, panicking if the value is a `Result::Err`
   |
13 |         Quantity("-100".parse::().expect("REASON")),
   |                                       +++++++++++++++++

error: aborting due to 1 previous error</pre>
<p>Yes, it does help, I can’t just blindly cast.</p>
<p>Damn, you’re good.</p>
<p>This error should be enough for me as a user of this API to understand that I should elegantly handle this possibility and maybe return an error all the way to the UI saying "no negative numbers, please". But what if I just go ahead and add that .expect() it is telling me about and then a user types a negative number? Well, then I get a crash at runtime. Better than going bankrupt? I would say so.</p>
<pre>&gt; ./order-6
GOOG false 100 1000
thread 'main' panicked at order/order-6.rs:16:18:
Quantities cannot be negative: ParseIntError { kind: InvalidDigit }
note: run with `RUST_BACKTRACE=1` environment variable to display a backtrace</pre>
<h3>Closing thoughts</h3>
<p>You know what's the most interesting part of this whole article? The thing Rust is very famous for, memory safety, did not feature at all. Sure, you could make the argument that mixing integers with floats is a memory issue, but you'd be stretching the definition most people have for memory safety.</p>
<p>What we learn in this exercise is that a well designed language can protect you from mistakes in ways that go way beyond stopping you from writing a use after free or a data race. The design can save you a lot of brain cycles by not forcing you to think about how to protect your code from the simplest mistakes — the language has your back.</p>
<p>Now, let's be honest. Most of the brain power you save will go, as a Rust beginner, to figuring out how to convince the borrow checker what you are doing is correct. It gets better, I promise. But I won't lie to you: your first few months with the borrow checker will suck.</p>
<p>Are we done here? Not really. There are two more topics that Matt covers in his talk that I will discuss in future articles, one of them related to my biggest pet peeve with C++. And for those of you who are thinking "if C++ had decades to learn it would also have features that are this well designed", well… let's just say you are in for a treat.</p>
<p>Again, go check <a href="https://xania.org/MattGodbolt" target="_blank" rel="noopener">Matt Godbolt</a>. Listen to what the man says, read everything he writes, watch every video of his talks on YouTube, and play with his Compiler Explorer. You'll learn a lot and have fun while doing it!</p>


</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Curl: We still have not seen a single valid security report done with AI help (185 pts)]]></title>
            <link>https://www.linkedin.com/posts/danielstenberg_hackerone-curl-activity-7324820893862363136-glb1</link>
            <guid>43907376</guid>
            <pubDate>Tue, 06 May 2025 17:07:58 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.linkedin.com/posts/danielstenberg_hackerone-curl-activity-7324820893862363136-glb1">https://www.linkedin.com/posts/danielstenberg_hackerone-curl-activity-7324820893862363136-glb1</a>, See on <a href="https://news.ycombinator.com/item?id=43907376">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="main-content" role="main">
      <section>
              <h2>
Daniel Stenberg’s Post</h2>

                
    

    
      

    <article data-activity-urn="urn:li:activity:7324820893862363136" data-featured-activity-urn="urn:li:activity:7324820893862363136" data-attributed-urn="urn:li:share:7324820893258420224">
<!---->
      
<!---->        

      <div data-test-id="main-feed-activity-card__entity-lockup">
          <a href="https://se.linkedin.com/in/danielstenberg?trk=public_post_feed-actor-image" data-tracking-control-name="public_post_feed-actor-image" data-tracking-will-navigate="">
            
      
      
<!---->          </a>
      <div>
        

            <p>
<!---->                curl CEO. Code Emitting Organism
            </p>

              <p><span>
                  <time>
                    

    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    

    
    
    
    
    
    
    
    
    
    
    
    
    
    

      2d
  
                      
                      <span>
                        Edited
                      </span>
                  </time>
<!---->              </span>
                </p></div>

<!---->    </div>

      
            
            
  <p dir="ltr" data-test-id="main-feed-activity-card__commentary">That's it. I've had it. I'm putting my foot down on this craziness.

1. Every reporter submitting security reports on <a href="https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Ffeed%2Fhashtag%2Fhackerone&amp;trk=public_post-text" target="_self" data-tracking-control-name="public_post-text" data-tracking-will-navigate="">#Hackerone</a> for <a href="https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Ffeed%2Fhashtag%2Fcurl&amp;trk=public_post-text" target="_self" data-tracking-control-name="public_post-text" data-tracking-will-navigate="">#curl</a> now needs to answer this question:

"Did you use an AI to find the problem or generate this submission?"

(and if they do select it, they can expect a stream of proof of actual intelligence follow-up questions)

2. We now ban every reporter INSTANTLY who submits reports we deem AI slop. A threshold has been reached. We are effectively being DDoSed. If we could, we would charge them for this waste of our time.

We still have not seen a single valid security report done with AI help.</p>



        

        
            
<!---->  
                  

      
          
        

      
          
        

      
            
      
    
    
    
    
    
    
    
    
    
    
    
    
    

    
    

      
  
  
        

      
                
    
    
    

    
  
                      

      
              
            

    
    
    
    
    
    

<!---->
    

    
    
    
    

      
  
  
            

    
    
    
    
    
    

<!---->
    

    
    
    
    

      
  
  
            

    
    
    
    
    
    

<!---->
    

    
    
    
    

      
  
  
            

    
    
    
    
    
    

<!---->
    

    
    
    
    

      
  
  
            

    
    
    
    
    
    

<!---->
    

    
    
    
    

      
  
  
            

    
    
    
    
    
    

<!---->
    

    
    
    
    

      
  
  
            

    
    
    
    
    
    

<!---->
    

    
    
    
    

      
  
  
            

    
    
    
    
    
    

<!---->
    

    
    
    
    

      
  
  
            

    
    
    
    
    
    

<!---->
    

    
    
    
    

      
  
  
              <a href="https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww%2Elinkedin%2Ecom%2Fposts%2Fdanielstenberg_hackerone-curl-activity-7324820893862363136-glb1&amp;trk=public_post_see-more-comments" data-test-id="main-feed-activity-card-with-comments__see-more-comments" data-tracking-control-name="public_post_see-more-comments" data-tracking-will-navigate="">
                See more comments
              </a>
      
        

      
              

    
    
    
    
    
    

    
  
        
    </article>
  
      
  
  
            </section>
      <section>

            <h2>
              Explore topics
            </h2>

<!---->
        
      </section>
    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[HNInternal: Launch HN: Exa (YC S21) – The web as a database (131 pts)]]></title>
            <link>https://news.ycombinator.com/item?id=43906841</link>
            <guid>43906841</guid>
            <pubDate>Tue, 06 May 2025 16:18:42 GMT</pubDate>
            <description><![CDATA[<p>See on <a href="https://news.ycombinator.com/item?id=43906841">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><td><table>
        <tbody><tr id="43906841">
      <td><span></span></td>      <td><center><a id="up_43906841" href="https://news.ycombinator.com/vote?id=43906841&amp;how=up&amp;goto=item%3Fid%3D43906841"></a></center></td><td><span><a href="https://news.ycombinator.com/item?id=43906841">Launch HN: Exa (YC S21) – The web as a database</a></span></td></tr><tr><td colspan="2"></td><td><span>
          <span id="score_43906841">101 points</span> by <a href="https://news.ycombinator.com/user?id=willbryk">willbryk</a> <span title="2025-05-06T16:18:42 1746548322"><a href="https://news.ycombinator.com/item?id=43906841">2 hours ago</a></span> <span id="unv_43906841"></span> | <a href="https://news.ycombinator.com/hide?id=43906841&amp;goto=item%3Fid%3D43906841">hide</a> | <a href="https://hn.algolia.com/?query=Launch%20HN%3A%20Exa%20%28YC%20S21%29%20%E2%80%93%20The%20web%20as%20a%20database&amp;type=story&amp;dateRange=all&amp;sort=byDate&amp;storyText=false&amp;prefix&amp;page=0">past</a> | <a href="https://news.ycombinator.com/fave?id=43906841&amp;auth=df5e66fa77c7d4c7f7658447ad84557f43795188">favorite</a> | <a href="https://news.ycombinator.com/item?id=43906841">43&nbsp;comments</a>        </span>
              </td></tr>
    <tr><td></td></tr><tr><td colspan="2"></td><td><div><p>Hey HN! We’re Will and Jeff from Exa (<a href="https://exa.ai/">https://exa.ai</a>). We recently launched Exa Websets, an embeddings-powered search engine designed to return exactly what you’re asking for. You can get precise results for complex queries like “all startups working on open-source developer tools based in SF, founded 2021-2025”. 
Demo here - <a href="https://youtu.be/Unt8hJmCxd4" rel="nofollow">https://youtu.be/Unt8hJmCxd4</a></p><p>We started working on Exa because we were frustrated that while LLM state-of-the-art is advancing every week, Google has gotten worse over time. The Internet used to feel like a magical information portal, but it doesn’t feel that way anymore when you’re constantly being pushed towards SEO-optimized clickbait.</p><p>Websets is a step in the opposite direction. For every search, we perform dozens of embedding searches over Exa’s vector database of the web to find good search candidates, then we run agentic workflows on each result to verify they match exactly what you asked for.</p><p>Websets results are good for two reasons. First, we train custom embedding models for our main search algorithm, instead of typical keyword matching search algorithms. Our embeddings models are trained specifically to return exactly the type of entity you ask for. In practice, that means if you search “startups working in nanotech”, keyword-based search engines return listicles about nanotech startups, because these listicles match the keywords in the query. In contrast, our embedding models return actual startup homepages, because these startup homepages match the meaning of the query.</p><p>The second is that LLMs provide the last-mile intelligence needed to verify every result. Each result and piece of data is backed with supporting references that we used to validate that the result is actually a match for your search criteria. That’s why Websets can take minutes or even hours to run, depending on your query and how many results you ask for. For valuable search queries, we think this is worth it.</p><p>Also notably, Websets are tables, not lists. You can add “enrichment” columns to find more information about each result, like “# of employees” or “does author have blog?”, and the cells asynchronously load in. This table format hopefully makes the web feel more like a database.</p><p>A few examples of searches that work with Websets:</p><p>- “Math blogs created by teachers from outside the US”: <a href="https://websets.exa.ai/cma1oz9xf007sis0ipzxgbamn">https://websets.exa.ai/cma1oz9xf007sis0ipzxgbamn</a></p><p>- "research paper about ways to avoid the O(n^2) attention problem in transformers, where one of the first author's first name starts with "A","B", "S", or "T", and it was written between 2018 and 2022”: <a href="https://websets.exa.ai/cm7dpml8c001ylnymum4sp11h">https://websets.exa.ai/cm7dpml8c001ylnymum4sp11h</a></p><p>- “US based healthcare companies, with over 100 employees and a technical founder": <a href="https://websets.exa.ai/cm6lc0dlk004ilecmzej76qx2">https://websets.exa.ai/cm6lc0dlk004ilecmzej76qx2</a></p><p>- “all software engineers in the Bay Area, with experience in startups, who know Rust and have published technical content before”: <a href="https://youtu.be/knjrlm1aibQ" rel="nofollow">https://youtu.be/knjrlm1aibQ</a></p><p>You can try it at <a href="https://websets.exa.ai/">https://websets.exa.ai/</a> and API docs are at <a href="https://docs.exa.ai/websets">https://docs.exa.ai/websets</a>. We’d love to hear your feedback!</p></div></td></tr>        <tr><td></td></tr><tr><td colspan="2"></td><td><form action="comment" method="post"></form></td></tr>  </tbody></table>
  </td></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Gemini 2.5 Pro Preview: even better coding performance (333 pts)]]></title>
            <link>https://developers.googleblog.com/en/gemini-2-5-pro-io-improved-coding-performance/</link>
            <guid>43906018</guid>
            <pubDate>Tue, 06 May 2025 15:10:00 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://developers.googleblog.com/en/gemini-2-5-pro-io-improved-coding-performance/">https://developers.googleblog.com/en/gemini-2-5-pro-io-improved-coding-performance/</a>, See on <a href="https://news.ycombinator.com/item?id=43906018">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>

    
      
    

    

    

    <section>
      
        
          <p><a href="https://developers.googleblog.com/en/search/?author=Logan+Kilpatrick">Logan Kilpatrick</a>
            
              <span>Senior Product Manager</span>
            
            
              <span>Gemini API and Google AI Studio</span>
            
          </p>
        

      
      </section>

    
    <div>
          

<div>
    <p data-block-key="2r1md">We’ve seen developers doing amazing things with <a href="https://deepmind.google/technologies/gemini/pro/">Gemini 2.5 Pro</a>, so we decided to release an updated version a couple of weeks early to get into developers hands sooner.</p><p data-block-key="9p6pm">Today we’re excited to release Gemini 2.5 Pro Preview (I/O edition). This update features even stronger coding capabilities, for you to start building with before Google I/O later this month. Expect meaningful improvements for front-end and UI development, alongside improvements in fundamental coding tasks such as transforming and editing code, and creating sophisticated agentic workflows.</p><blockquote data-block-key="5l7r8"><i><sup>“We found Gemini 2.5 Pro to be the best frontier model when it comes to "capability over latency" ratio. I look forward to rolling it out on Replit Agent whenever a latency-sensitive task needs to be accomplished with a high degree of reliability.”<br> –</sup></i> <b><sup>Michele Catasta, President,</sup></b> <b><sup>Replit</sup></b></blockquote><h2 data-block-key="pfowd" id="best-in-class-frontend-web-development"><b><br></b>Best-in-class frontend web development</h2><p data-block-key="f9jph">Gemini 2.5 Pro now ranks #1 on the <a href="https://web.lmarena.ai/leaderboard">WebDev Arena leaderboard</a>, which measures human preference for a model’s ability to build aesthetically pleasing and functional web apps. Drawing on this leading capability, Gemini 2.5 Pro powers Cursor’s innovative code agent and empowers our collaborations with companies like Cognition and Replit. Together, we're pushing the frontiers of agentic programming to unlock new possibilities for developers.</p>
</div>   

<div>
    <p><img src="https://storage.googleapis.com/gweb-developer-goog-blog-assets/images/webdev_leaderboard.original.png" alt="Gemini 2.5 Pro WebDev Elo Arena Leaderboard rankings">
        
        
    </p>
</div>
  <div>
    <blockquote data-block-key="b90aq"><i><sup>“The updated Gemini 2.5 Pro achieves leading performance on our junior-dev evals. It was the first-ever model that solved one of our evals involving a larger refactor of a request routing backend. It felt like a more senior developer because it was able to make correct judgement calls and choose good abstractions.”<br>–</sup></i> <b><sup>Silas Alberti, Founding Team,</sup></b> <b><sup>Cognition</sup></b></blockquote><h2 data-block-key="9da7r" id="gemini-2.5-pro-in-action"><br>Gemini 2.5 Pro in action</h2><p data-block-key="7t7gb">Gemini 2.5 Pro’s deep understanding of code, combined with powerful reasoning, continues to make Gemini 2.5 Pro the go-to model for developers. We’re particularly excited about how this model can be used in the following cases.</p><h3 data-block-key="43g6o" id="video-to-code"><b><br>Video to code</b></h3><p data-block-key="djkbd">Gemini 2.5 Pro delivers state-of-the-art video understanding, scoring 84.8% on the VideoMME benchmark. Combining this with coding enables new flows that were previously not possible with previous versions. For example, the <a href="https://aistudio.google.com/app/apps/bundled/video-to-learning-app?showPreview=true">Video to Learning App</a> in Google AI Studio demonstrates how Gemini 2.5 Pro creates an interactive learning app based on a single YouTube video. With improved video understanding and complete UI, the updated Gemini 2.5 Pro model delivers a more functional experience than the previous simple example.</p>
</div>    <div>
    <h3 data-block-key="nqxom" id="easier-feature-development"><b>Easier feature development</b></h3><p data-block-key="c8rhp">Gemini 2.5 Pro is strong at front-end web development, helping you get more done. Implementing new features means manually diving into design files and inspecting components to match style properties like colors, fonts, padding, margins, and borders then manually writing the CSS code needed to replicate those visual properties accurately. Now imagine using Gemini 2.5 Pro in an IDE and having the model generate new features, like adding a video player in the style of the other apps in the <a href="https://aistudio.google.com/apps/bundled/gemini_95?showPreview=true">Gemini 95</a> starter app.</p>
</div>    <div>
    <h3 data-block-key="sleq6" id="quick-concepts-to-working-apps"><b>Quick concepts to working apps</b></h3><p data-block-key="bo0rv">Bringing ideas to life with both functionality and a beautiful UI is made easier with Gemini 2.5 Pro. The new <a href="https://aistudio.google.com/apps/bundled/gemini-dictation?showPreview=true">dictation starter app</a>, built using the updated model, is a great example of this in action. Pay attention to some of the details like the wavelength animations, responsive design, and subtle button hover effects. By default, the model has a real taste for aesthetic web development while maintaining its steerability, helping developers quickly take a concept to a working web app. Gemini 2.5 Pro was able to design and code the microphone UI animation for the dictation starter app.</p>
</div>    <div>
    <h2 data-block-key="r4xha" id="start-building-with-gemini-2.5-pro">Start building with Gemini 2.5 Pro</h2><p data-block-key="71k83">You can build with Gemini 2.5 Pro with the Gemini API in <a href="https://aistudio.google.com/prompts/new_chat?models=gemini-2.5-pro-preview-05-06">Google AI Studio</a>, and enterprise customers can use <a href="https://console.cloud.google.com/freetrial?redirectPath=/vertex-ai/studio">Vertex AI</a>.</p><p data-block-key="61l9c">For developers already using Gemini 2.5 Pro, this new version will not only improve coding performance but will also address key developer feedback including reducing errors in function calling and improving function calling trigger rates. The previous iteration (03-25) now points to the most recent version (05-06), so no action is required to use the improved model, and it continues to be available at the same <a href="https://ai.google.dev/gemini-api/docs/pricing#gemini-2.5-pro-preview">price</a>. We have also <a href="https://modelcards.withgoogle.com/model-cards">updated the model card</a> with the new version of 2.5 Pro .</p><p data-block-key="5vcj8">We can’t wait to see the amazing apps you build!</p>
</div> 
      </div>
    

    

    
    
    
  </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Show HN: Clippy – 90s UI for local LLMs (367 pts)]]></title>
            <link>https://felixrieseberg.github.io/clippy/</link>
            <guid>43905942</guid>
            <pubDate>Tue, 06 May 2025 15:02:22 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://felixrieseberg.github.io/clippy/">https://felixrieseberg.github.io/clippy/</a>, See on <a href="https://news.ycombinator.com/item?id=43905942">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
          <p>
            Clippy lets you run a variety of large language models (LLMs)
            locally on your computer while sticking with a user interface of the
            1990s. It's a love letter and homage to the late, great Clippy - and
            the visual design created by Microsoft in that era.
          </p>
          <ul>
            <li>
              <strong>Simple, familiar, and classic chat interface</strong>.
              Send messages to your models, get a response.
            </li>
            <li>
              <strong>Batteries included: No complicated setup</strong>. Just
              open the app and chat away. Thanks to llama.cpp and
              <code>node-llama-cpp</code>, the app will automatically discover
              the most efficient way to run your models (Metal, CUDA, Vulkan,
              etc).
            </li>
            <li>
              <strong>Custom models, prompts, and parameters</strong>: Load your
              own downloaded models and play with the settings.
            </li>
            <li>
              <strong>Offline, local, free</strong>: Everything runs on your
              computers. The only network request Clippy makes is to check for
              updates (which you can disable).
            </li>
          </ul>
          <p>
            This app is not affiliated, approved, or supported by Microsoft.
            This project isn't trying to be your best chat bot. I'd like you to
            enjoy a weird mix of nostalgia for 1990s technology paired with one
            the most magical technologies we can run on our computers in 2025.
          </p>
          <p>Do you want to download Clippy for your computer?</p>
          
        </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Accents in latent spaces: How AI hears accent strength in English (130 pts)]]></title>
            <link>https://accent-strength.boldvoice.com/</link>
            <guid>43905299</guid>
            <pubDate>Tue, 06 May 2025 14:07:57 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://accent-strength.boldvoice.com/">https://accent-strength.boldvoice.com/</a>, See on <a href="https://news.ycombinator.com/item?id=43905299">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
        

        <section id="intro">
          <p>
            We work with accents a lot at
            <a href="https://boldvoice.com/" target="_blank" rel="noopener">BoldVoice</a>, the AI-powered accent coaching app for non-native English
            speakers. Accents are subtle patterns in speech—vowel shape, timing,
            pitch, and more. Usually, you need a linguist to make sense of these
            qualities. However, our goal at BoldVoice is to get machines to
            understand accents, and machines don’t think like linguists. So, we
            ask: how does a machine learning model understand an accent, and
            specifically, how strong it is?
          </p>
          <p>
            To begin this journey, we first introduce the “accent fingerprint,”
            an embedding that is generated by inferencing an English speech
            recording through BoldVoice’s large-scale accented speech model.
          </p>
          <figure>
            <pre><code>torch.Size([1, 768, 12])</code></pre>
            <figcaption>The accent fingerprint embedding dimensions</figcaption>
          </figure>
          <p>
            In this post we’ll show where the accent fingerprint lives in a
            latent space, how distances and directions in that space correspond
            to accent similarity and language background, and how we use it to
            coach our product management intern Victor, a non-native English
            speaker, toward the American English accent of our expert accent
            coach Eliza.
          </p>

          <h3>The Original Recordings</h3>
          <p>First off, here's how Victor sounds when speaking English:</p>
          <figure>
            <audio controls="" src="https://accent-strength.boldvoice.com/victor-original.wav"></audio>
            <figcaption>Victor (original recording)</figcaption>
          </figure>
          <p>
            Now have a listen to Eliza reading the same passage. Eliza is
            demonstrating our “target” American accent.
          </p>
          <figure>
            <audio controls="" src="https://accent-strength.boldvoice.com/eliza-original.wav"></audio>
            <figcaption>Eliza's recording</figcaption>
          </figure>
          <p>
            Compared to Eliza, who is an American English native speaker, Victor
            has a noticeably strong Chinese accent when speaking English.
          </p>
        </section>

        <section id="latent-space">
          <h3>The Latent Space</h3>
          <p>
            So that we can make sense of how the machine learning model
            understands both of these recordings, we now populate a latent space
            with 1,000 speech recordings sourced from our internal data
            representing varied levels of accent. Feel free to inspect the 2D
            visualization of the latent space<sup>1</sup> and hover over the
            points to see details about each recording.
          </p>

          <p>
            The full dimensional latent space contains information about speaker
            identity, accent, intelligibility, emotion, and other
            characteristics. This visualization has been pruned to show only the
            information relevant to "accent strength", that is "how strong is
            the speaker's accent relative to native speakers of English?"
          </p>
          <p>
            More specifically, we apply PLS regression to identify the latent
            space directions which correlate most with human accent strength
            ratings, and for the purpose of this visualization only, we apply 2D
            UMAP dimensionality reduction. The x-axis represents the first
            hidden dimension of accent strength, while the y-axis represents the
            second hidden dimension.<sup>2</sup>
          </p>

          <p>
            The below pseudocode shows how the dimensions of the latent space
            are selected:
          </p>
          <figure>
            <pre><code>accent_strength_directions = PLSRegression.fit(train_accent_fingerprints, train_accent_strength_ratings)
accent_strength_features = test_accent_fingerprints[accent_strength_directions]
visualization_features = UMAP(n_components=2).fit_transform(accent_strength_features)</code></pre>
          </figure>

          <p>
            <sup>1</sup> For the sake of brevity, we will use the term latent
            space to refer to both the full dimensional space, as well as the
            pruned 2D visualization.
          </p>
          <p>
            <sup>2</sup> The dimensions are not readily interpretable, are not
            orthogonal, and are chosen solely to maximize their utility for
            discriminating between accent strength in L2 English (English as a
            second language).
          </p>
        </section>

        <section id="victor-original">
          <h3>Plotting Accents</h3>
          <p>
            Now, let’s visualize the accent fingerprints of Victor’s and Eliza’s
            recordings in this latent space. You can see a purple diamond in the
            bottom left representing Eliza’s recording and a yellow diamond
            towards the top right representing Victor’s recording.
          </p>

          <p>
            From what we can see, the more towards the lower left of the plot a
            recording is, the more ”native sounding” and “less strong” its
            speaker’s accent is. Accordingly, we labeled the points as Native,
            Near Native, Advanced, Intermediate, and Beginner based on their
            distance from Eliza’s position in the latent space.
          </p>

          <p>
            Another finding we immediately see is that the latent space is not
            biased towards different native languages, as we don’t see any
            clustering based on the speaker’s native language, and a fairly
            uniform distribution of native languages across all proficiency
            levels.
          </p>

          <p>
            Now, let’s look at some creative ways that we can use our in-house
            suite of speech models and tools to help Victor get closer to
            Eliza’s accent.
          </p>
        </section>

        <section id="victor-cleaned">
          <h3>Cleaning the Background Noise</h3>
          <p>
            The first thing that jumps out is that Eliza’s recording is much
            cleaner than Victor’s. Perhaps it will be easier for him to focus on
            just the accent differences if we can get rid of the background
            noise in his recording?
          </p>
          <figure>
            <audio controls="" src="https://accent-strength.boldvoice.com/victor-cleaned.wav"></audio>
            <figcaption>Victor (cleaned recording)</figcaption>
          </figure>

          <p>
            Surprise! This didn’t change Victor's position in the latent space
            much, the cleaned recording lands very close to Victor's original at
            the top right of the latent space. This is a good sanity check that
            our latent space is working correctly—the recording quality and
            level background noise are not relevant to accent strength.
          </p>
        </section>

        <section id="victor-converted">
          <h3>Converting the Accent</h3>
          <p>
            Next, perhaps Victor finds it difficult to mimic Eliza’s accent
            because the register of his voice is so much lower than hers. So
            we’re going to use BoldVoice’s in-house accent conversion model to
            hear what Victor sounds like with Eliza's accent. (Yes, we can
            really do that—we'll share more about this in a future post.)
          </p>
          <figure>
            <audio controls="" src="https://accent-strength.boldvoice.com/victor-cleaned.wav"></audio>
            <figcaption>Victor's original recording</figcaption>
          </figure>
          <figure>
            <audio controls="" src="https://accent-strength.boldvoice.com/victor-converted.wav"></audio>
            <figcaption>Victor (converted recording)</figcaption>
          </figure>
          <p>
            As you can see, the position of Victor with Eliza’s accent is right
            next to Eliza’s original position in the latent space. Phonetically
            speaking, there are still some differences in vowel shapes,
            emphasis, pitch, and timing, but even without expert knowledge,
            Victor will have a much easier time mimicking Eliza’s accent now
            that it’s in his own voice.
          </p>
        </section>

        <section id="victor-after">
          <h3>Practicing the Accent</h3>
          <p>
            We left Victor with this audio of his voice with Eliza's accent for
            about 10 minutes to give him time to practice mimicking it. Here’s
            what Victor sounds like after that practice:
          </p>
          <figure>
            <audio controls="" src="https://accent-strength.boldvoice.com/victor-after.wav"></audio>
            <figcaption>Victor (after practice recording)</figcaption>
          </figure>
          <figure>
            <audio controls="" src="https://accent-strength.boldvoice.com/eliza-original.wav"></audio>
            <figcaption>Compare to Eliza's original recording</figcaption>
          </figure>
          <p>
            Not bad—Victor matched her timing, intonation and stress pretty
            well, but some of the vowel shapes still aren’t quite the same.
            Let’s see how far he is from Eliza in the latent space now.
          </p>
          <p>
            That’s quite an improvement! Victor’s new position in the latent
            space is right on the border of Intermediate and Advanced.
          </p>
          <p>
            If Victor wanted to move beyond this point, the sound-by-sound
            phonetic analysis available in the BoldVoice app would allow him to
            understand the patterns in pronunciation and stress that contribute
            to Eliza’s accent and teach him how to apply them in his own speech.
          </p>
        </section>

        <section id="conclusion">
          <h3>What did we learn?</h3>
          <ul>
            <li>
              This machine learning model can clearly distinguish the strength
              of a speaker’s accent.
            </li>
            <li>
              The model’s assessment of accent strength appears independent of
              the speaker's native language background.
            </li>
            <li>
              The accent strength of a speaker is something that can be changed
              with practice.
            </li>
            <li>
              Voice conversion technology can map a target accent onto a
              different voice, providing a useful tool for practice.
            </li>
            <li>
              Changes to the acoustic environment, such as denoising, don’t
              result in a large change in measured accent strength.
            </li>
          </ul>

          <h3>Applications and Next Steps</h3>
          <p>
            The accent strength metric derived from this model has several
            promising applications.
          </p>
          <ol>
            <li>
              It offers a quantitative way to track an English learner’s accent
              journey over multiple recordings by measuring their distance from
              a target accent profile in the latent space.
            </li>
            <li>
              This same quantitative approach can be applied to rigorously
              evaluate automatic speech recognition (ASR) systems for
              performance variations across different accent strengths.
            </li>
            <li>
              It can similarly monitor text-to-speech (TTS) systems for unwanted
              changes in accent, often referred to as “accent drift.”
            </li>
          </ol>

          <h3>Stay tuned for more!</h3>
          <p>
            Do you have any questions or comments? Or a suggestion for what you
            would like for us to cover in the future? Please reach out to us at
            <a href="https://accent-strength.boldvoice.com/cdn-cgi/l/email-protection#04616a636d6a6161766d6a6344666b6860726b6d67612a676b69"><span data-cfemail="f3969d949a9d9696819a9d94b3919c9f97859c9a9096dd909c9e">[email&nbsp;protected]</span></a>!
          </p>
          <p>
            In our next post, we'll demonstrate how to explore accent
            fingerprints (embeddings) directly without engineering them for any
            particular task, and go on a tour of the world’s accents in English.
          </p>
        </section>
      </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Nnd – a TUI debugger alternative to GDB, LLDB (166 pts)]]></title>
            <link>https://github.com/al13n321/nnd</link>
            <guid>43905185</guid>
            <pubDate>Tue, 06 May 2025 13:58:03 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/al13n321/nnd">https://github.com/al13n321/nnd</a>, See on <a href="https://news.ycombinator.com/item?id=43905185">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><p dir="auto">A debugger for Linux. Partially inspired by RemedyBG.</p>
<p dir="auto">Mom, can we have RAD Debugger on Linux?
No, we have a debugger at home.
Debugger at home:</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://private-user-images.githubusercontent.com/3629776/415967167-e0b03f1e-c1d1-4e38-a992-2ace7321bb75.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3NDY1NDkzMDMsIm5iZiI6MTc0NjU0OTAwMywicGF0aCI6Ii8zNjI5Nzc2LzQxNTk2NzE2Ny1lMGIwM2YxZS1jMWQxLTRlMzgtYTk5Mi0yYWNlNzMyMWJiNzUucG5nP1gtQW16LUFsZ29yaXRobT1BV1M0LUhNQUMtU0hBMjU2JlgtQW16LUNyZWRlbnRpYWw9QUtJQVZDT0RZTFNBNTNQUUs0WkElMkYyMDI1MDUwNiUyRnVzLWVhc3QtMSUyRnMzJTJGYXdzNF9yZXF1ZXN0JlgtQW16LURhdGU9MjAyNTA1MDZUMTYzMDAzWiZYLUFtei1FeHBpcmVzPTMwMCZYLUFtei1TaWduYXR1cmU9NTI0NDQ5Mzk2OTRiNjc0ZjIwYTNkMTQ5MGRjZmRkNmI3OWRlZWVmZjZhZGY0YzVjODQxOGE4NWJjZTQ2ZTEyMSZYLUFtei1TaWduZWRIZWFkZXJzPWhvc3QifQ.jkEItoHbxASwa7i85LMOW0maBm-6PJOjLUw8xGk4Qa0"><img src="https://private-user-images.githubusercontent.com/3629776/415967167-e0b03f1e-c1d1-4e38-a992-2ace7321bb75.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3NDY1NDkzMDMsIm5iZiI6MTc0NjU0OTAwMywicGF0aCI6Ii8zNjI5Nzc2LzQxNTk2NzE2Ny1lMGIwM2YxZS1jMWQxLTRlMzgtYTk5Mi0yYWNlNzMyMWJiNzUucG5nP1gtQW16LUFsZ29yaXRobT1BV1M0LUhNQUMtU0hBMjU2JlgtQW16LUNyZWRlbnRpYWw9QUtJQVZDT0RZTFNBNTNQUUs0WkElMkYyMDI1MDUwNiUyRnVzLWVhc3QtMSUyRnMzJTJGYXdzNF9yZXF1ZXN0JlgtQW16LURhdGU9MjAyNTA1MDZUMTYzMDAzWiZYLUFtei1FeHBpcmVzPTMwMCZYLUFtei1TaWduYXR1cmU9NTI0NDQ5Mzk2OTRiNjc0ZjIwYTNkMTQ5MGRjZmRkNmI3OWRlZWVmZjZhZGY0YzVjODQxOGE4NWJjZTQ2ZTEyMSZYLUFtei1TaWduZWRIZWFkZXJzPWhvc3QifQ.jkEItoHbxASwa7i85LMOW0maBm-6PJOjLUw8xGk4Qa0" alt="screenshot"></a></p>
<p dir="auto">Properties:</p>
<ul dir="auto">
<li>Fast.</li>
<li>TUI.</li>
<li>Not based on gdb or lldb, implemented mostly from scratch.</li>
<li>Works on large executables. (Tested mostly on 2.5 GB ClickHouse.)</li>
</ul>
<p dir="auto">What we mean by "fast":</p>
<ul dir="auto">
<li>Operations that can be instantaneous should be instantaneous. I.e. snappy UI, no random freezes, no long waits.
(Known exception: if the program has &gt;~2k threads things become pretty slow. This will be improved.)</li>
<li>Operations that can't be instantaneous (loading debug info, searching for functions and types) should be reasonably efficient, multi-threaded, asynchronous, cancellable, and have progress bars.</li>
</ul>
<p dir="auto">Limitations:</p>
<ul dir="auto">
<li>Linux only</li>
<li>x86 only</li>
<li>64-bit only</li>
<li>for native code only (e.g. C++ or Rust, not Java or Python)</li>
<li>TUI only (no REPL, no GUI)</li>
<li>no remote debugging (but works fine over ssh)</li>
<li>single process (doesn't follow forks)</li>
<li>no record/replay or backwards stepping</li>
</ul>
<p dir="auto">Development status:</p>
<ul dir="auto">
<li>Essential features are there. But different people consider different features essential, and probably many of them are not implemented. Let me know.</li>
<li>I use it every day and find it very helpful.</li>
<li>Not widely tested - I only tried it on a few machines and a few real executables.</li>
</ul>
<p dir="auto">Distributed as a single 6 MB executable file with no dependencies.</p>
<p dir="auto">"Installation":</p>
<div dir="auto" data-snippet-clipboard-copy-content="curl -L -o nnd 'https://github.com/al13n321/nnd/releases/latest/download/nnd'
chmod +x nnd
# try `./nnd --help` to get started"><pre>curl -L -o nnd <span><span>'</span>https://github.com/al13n321/nnd/releases/latest/download/nnd<span>'</span></span>
chmod +x nnd
<span><span>#</span> try `./nnd --help` to get started</span></pre></div>
<p dir="auto">Or build from source:</p>
<div dir="auto" data-snippet-clipboard-copy-content="# Prerequisites:
#  1. Install Rust.
#  2. Install musl target:
rustup target add x86_64-unknown-linux-musl
#  3. Install musl-tools
sudo apt install musl-tools

# Build:
cargo build --profile dbgo --bin nnd

# The executable is at target/x86_64-unknown-linux-musl/dbgo/nnd"><pre><span><span>#</span> Prerequisites:</span>
<span><span>#</span>  1. Install Rust.</span>
<span><span>#</span>  2. Install musl target:</span>
rustup target add x86_64-unknown-linux-musl
<span><span>#</span>  3. Install musl-tools</span>
sudo apt install musl-tools

<span><span>#</span> Build:</span>
cargo build --profile dbgo --bin nnd

<span><span>#</span> The executable is at target/x86_64-unknown-linux-musl/dbgo/nnd</span></pre></div>
<p dir="auto">Run <code>nnd --help</code> for documentation.</p>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[OpenAI reaches agreement to buy Windsurf for around $3B (155 pts)]]></title>
            <link>https://www.bloomberg.com/news/articles/2025-05-06/openai-reaches-agreement-to-buy-startup-windsurf-for-3-billion</link>
            <guid>43904233</guid>
            <pubDate>Tue, 06 May 2025 12:18:20 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.bloomberg.com/news/articles/2025-05-06/openai-reaches-agreement-to-buy-startup-windsurf-for-3-billion">https://www.bloomberg.com/news/articles/2025-05-06/openai-reaches-agreement-to-buy-startup-windsurf-for-3-billion</a>, See on <a href="https://news.ycombinator.com/item?id=43904233">Hacker News</a></p>
Couldn't get https://www.bloomberg.com/news/articles/2025-05-06/openai-reaches-agreement-to-buy-startup-windsurf-for-3-billion: Error: Request failed with status code 403]]></description>
        </item>
        <item>
            <title><![CDATA[Taking the bite out of Lyme disease (132 pts)]]></title>
            <link>https://news.northwestern.edu/stories/2025/04/taking-the-bite-out-of-lyme-disease/</link>
            <guid>43903959</guid>
            <pubDate>Tue, 06 May 2025 11:38:44 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://news.northwestern.edu/stories/2025/04/taking-the-bite-out-of-lyme-disease/">https://news.northwestern.edu/stories/2025/04/taking-the-bite-out-of-lyme-disease/</a>, See on <a href="https://news.ycombinator.com/item?id=43903959">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="main-content" tabindex="-1">
        

        <div id="story-top">
            
            <p>New studies offer insight into disease’s treatment, lingering symptoms</p>
            
        </div>

        <div id="hero">
            
                
                    
                        
                        <p><img src="https://news.northwestern.edu/assets/Stories/2025/04/lyme1940__FitMaxWzk3MCw2NTBd.jpg" alt="Hand with a deer tick on it" width="970" height="650" data-src="story"></p><p>
         Northwestern scientists have identified an antibiotic that cures Lyme disease at a fraction of the dosage of the current “gold standard” treatment and discovered what may cause a treated infection to mimic chronic illness in patients.  </p>


        </div>

        
            <div id="news-story">
                        <p>Lyme disease, a disease transmitted when deer ticks feed on infected animals like deer and rodents, and then bite humans, impacts nearly <a href="https://www.cdc.gov/lyme/data-research/facts-stats/index.html#:~:text=Recent%20estimates%20using%20other%20methods,not%20actually%20have%20Lyme%20disease.">half a million</a> individuals in the U.S. annually. Lyme can be devastating; but early treatment with antibiotics can prevent chronic symptoms like heart and neurological problems and arthritis from developing.&nbsp;</p>
<h2><strong>What’s new</strong></h2>
<p>In two new studies led by bacteriologist <a href="https://www.feinberg.northwestern.edu/faculty-profiles/az/profile.html?xid=62640">Brandon L. Jutras</a>, Northwestern scientists have identified an antibiotic that cures Lyme disease at a fraction of the dosage of the current “gold standard” treatment and discovered what may cause a treated infection to mimic chronic illness in patients. The studies were published in the journal Science Translational Medicine.</p>
<p>Jutras, who joined Northwestern faculty last summer, is an associate professor of microbiology-immunology at Northwestern University Feinberg School of Medicine and a member of the Center for Human Immunobiology at Northwestern. He has been studying Lyme disease for more than 15 years.</p>
<h2><strong>Besting the gold standard</strong></h2>
<p>The antibiotic doxycycline is the current gold standard treatment for Lyme. However, doxycycline and other generic antibiotics, wreak havoc on the microbiome, killing beneficial bacteria in the gut and causing troubling side effects even as it kills <em>Borrelia burgdorferi</em>, the bacteria that causes Lyme. In addition to its negative impact on the gut, doxycycline also fails to help between 10 and 20% of individuals who take it, and it is not approved for use in young children —&nbsp;who are at the highest risk of tick bites, and therefore, of developing Lyme.</p>
<p>More effective, or at least more specified, treatment options are needed as climate change extends tick seasons and <a href="https://www.epa.gov/climate-indicators/climate-change-indicators-lyme-disease#:~:text=Between%201992%20and%202022%2C%20the%20incidence%20of,per%20100%2C000%20people%20in%202022%20(Figure%201).&amp;text=Lyme%20disease%20is%20much%20more%20common%20in,Midwest%20than%20in%20other%20regions%20(Figure%202).">Lyme becomes more prevalent</a>.</p>
<p>Northwestern scientists identified that piperacillin, an antibiotic in the same class as penicillin, effectively cured mice of Lyme disease at 100-times less than the effective dose of doxycycline. At such a low dose, piperacillin also had the added benefit of “having virtually no impact on resident gut microbes,” <a href="https://www.science.org/doi/10.1126/scitranslmed.adr9091">according to the study</a>.</p>
<p>The team screened nearly 500 medicines in a drug library, using a molecular framework to understand potential interactions between antibiotics and the Borrelia bacteria. Once the group had a short list of potentials, they performed additional physiological, cellular and molecular tests to identify compounds that did not impact other bacteria.</p>
<p>The authors argue that piperacillin, which has already been FDA-approved as a safe treatment for pneumonia, could also be a candidate for preemptive interventions for those potentially exposed to Lyme (with a known deer tick bite).</p>
<p>They found that piperacillin exclusively interfered with the unusual cell wall synthesis pattern common to Lyme bacteria, preventing the bacteria from growing or dividing and ultimately leading to its death.</p>
<h2><strong>Understanding when Lyme lingers</strong></h2>
<p>Symptoms that persist long after Lyme disease is treated are not uncommon —&nbsp;a <a href="https://www.hopkinslyme.org/lyme-disease/treatment-and-prognosis-of-lyme-disease/#:~:text=Most%20patients%20with%20early%20Lyme,includes%20misdiagnoses%20and%20treatment%20delays.">2022 study found</a> that 14% of patients who were diagnosed and treated early with antibiotic therapy would still develop Post Treatment Lyme Disease (PTLD). Yet doctors puzzle over the condition’s causes and how to help their patients through symptoms ranging from severe fatigue and cognitive challenges to body pain and arthritis.</p>
<p>Northwestern scientists believe they now know what causes the treated infection to mimic chronic illness: The body may be responding to remnants of the Borrelia cell wall which breaks down during treatment yet lingers in the liver. (This matches one theory behind the underlying causes of long COVID-19, in that persisting viral molecules may encourage a strong, albeit unnecessary, immune response, according to Jutras.)</p>
<p>In <a href="https://www.science.org/doi/10.1126/scitranslmed.adr2955">another new study</a>, researchers tracked the biodistribution of peptidoglycan, a structural feature of virtually all bacterial cells and a common target of antibiotics, from different bacteria. They found that Lyme disease’s peptidoglycan persists for weeks to months.</p>
<p>Lyme’s peptidoglycan is structurally unique, and this difference may be behind its persistence in humans. Instead of looking the same as with other bacteria, the Lyme peptidoglycan is fundamentally distinct, which is facilitated in part by sucking up sugars from its tick vector. Upon bacterial cell death — by antibiotics or the immune system — surviving molecules tend to relocate to the liver, which can’t process the modified peptidoglycan. Without this modification, it seems likely that the peptidoglycan would clear right away, as in other infections.</p>
<p>“The unusual chemical properties of Borrelia peptidoglycan promote persistence, but it’s the individual patient response to the molecule that likely impacts the overall clinical outcome,” Jutras said. “Some patients will have a more robust or stronger immune response, which could result in a worse disease outcome, while the immune system of others may largely ignore the molecule. So, in essence, it’s not about whether the molecule is there or not, it’s more about how an individual responds to it.”</p>
<h2><strong>What’s next</strong></h2>
<p>Jutras hopes the groundbreaking findings will lead to development of more accurate tests, possibly for PTLD patients, and refined treatment options when antibiotics have failed. To effectively stymie PTLD, instead of neutralizing an infection that may no longer exist, efforts are underway to neutralize the inflammatory molecule.</p>
<p>Lyme prevention also remains a challenge — no approved human vaccine exists — and Jutras hopes his research moving forward will help with developing proactive strategies to diagnose and treat it.</p>
<p>“I think the future for Lyme disease patients is bright in that we are approaching an era of customized medicine, and we can potentially create a particular drug, or a combination to treat Lyme disease when others fail,” Jutras said. “The more we understand about the various strains and species of Lyme disease-causing Borrelia, the closer we get to a custom approach.”</p>
                        
                    </div>

            
                <div id="picks-stories">
                            
                            <h2>Editor’s Picks</h2>
                            
                            
                        </div>
            



        
        
        
    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Memory-safe sudo to become the default in Ubuntu (156 pts)]]></title>
            <link>https://trifectatech.org/blog/memory-safe-sudo-to-become-the-default-in-ubuntu/</link>
            <guid>43903853</guid>
            <pubDate>Tue, 06 May 2025 11:22:55 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://trifectatech.org/blog/memory-safe-sudo-to-become-the-default-in-ubuntu/">https://trifectatech.org/blog/memory-safe-sudo-to-become-the-default-in-ubuntu/</a>, See on <a href="https://news.ycombinator.com/item?id=43903853">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
        <article>
            
            <br>
            

            <div>
                <p><strong>May 6, 2025</strong> – Ubuntu 25.10 is set to adopt <strong>sudo-rs</strong> by default. Sudo-rs is a memory-safe reimplementation of the widely-used <code>sudo</code> utility, written in the Rust programming language.</p>
<p>This move is part of a broader effort by <strong>Canonical</strong> to improve the resilience and maintainability of core system components. Sudo-rs is developed by the <strong>Trifecta Tech Foundation (TTF)</strong>, a nonprofit organization that creates secure, open source building blocks for infrastructure software.</p>
<h3 id="a-memory-safe-future-for-ubuntu">A Memory-Safe Future for Ubuntu</h3>
<p>The decision to adopt sudo-rs is in line with Canonical’s commitment to <a href="https://discourse.ubuntu.com/t/carefully-but-purposefully-oxidising-ubuntu/56995">Carefully But Purposefully</a> increase the resilience of critical system software, by adopting Rust. Rust is a programming language with strong memory safety guarantees that eliminates many of the vulnerabilities that have historically plagued traditional C-based software.</p>
<blockquote>
<p>"I'm delighted to be investing in critical, low-level software utilities. Ubuntu is the most widely deployed Linux operating system, and by choosing to adopt sudo-rs I hope to accelerate the path to wider adoption across the Linux ecosystem".<br>
-- Jon Seager, VP Engineering Ubuntu at Canonical</p>
</blockquote>
<p>Sudo-rs is part of the Trifecta Tech Foundation's <a href="https://trifectatech.org/initiatives/privilege-boundary/">Privilege Boundary initiative</a>, which aims to handle privilege escalation with memory-safe alternatives.</p>
<blockquote>
<p>"While no piece of software - in any language - is flawless, we believe the transition to Rust in systems programming is a vital step forward. It is very exciting to see Ubuntu committing to sudo-rs and taking a leading role in moving the needle."<br>
-- Erik Jonkers, Chair of the Trifecta Tech Foundation.</p>
</blockquote>
<h3 id="preparing-for-mainstream-adoption-by-linux-distributions">Preparing for Mainstream Adoption by Linux Distributions</h3>
<p>To prepare for mainstream adoption, the maintainers of sudo-rs will complete work outlined in the project’s <a href="https://trifectatech.org/initiatives/workplans/sudo-rs/#current-work">Milestone 5 workplan</a>, including:</p>
<ul>
<li>Coarse-grained shell escape prevention (NOEXEC) on Linux</li>
<li>The ability to control AppArmor profiles</li>
<li><code>sudoedit</code></li>
<li>Support for Linux Kernels older than version 5.9, used by Ubuntu 20.04 LTS</li>
</ul>
<p>Canonical is sponsoring this milestone to make sudo-rs an even better implementation of the <code>sudo</code> command. With the above features, more users and system administrators should be able to use sudo-rs without any change to their current workflows.</p>
<p>The maintainers of sudo-rs are sticking to the "less is more" approach: some features of the original sudo will not be implemented in sudo-rs if they serve only highly niche use cases. The maintainers continue their collaboration with Todd Miller, the incumbent <code>sudo</code> maintainer for over thirty years, thus indirectly improving original sudo as a by-product of this engagement.</p>
<h3 id="targeting-ubuntu-25-10">Targeting Ubuntu 25.10</h3>
<p>Canonical plans to make sudo-rs the default in Ubuntu 25.10. This will allow time for acceptance testing by end-users and ensure that sudo-rs is battle-tested before its inclusion in the next Long Term Support (LTS) release: Ubuntu 26.04 LTS, which will be supported for a minimum of 12 years. We are looking forward to learning from the community how we can further improve sudo-rs.</p>
<hr>
<h3 id="about-trifecta-tech-foundation">About Trifecta Tech Foundation</h3>
<p><a href="https://trifectatech.org/"><strong>Trifecta Tech Foundation</strong></a> is a non-profit and a Public Benefit Organisation (501(c)(3) equivalant) that creates open-source building blocks for critical infrastructure software. Our initiatives Data compression, Time synchronization, Smart grid protocols and Privilege boundary, impact the digital security of millions of people. If you'd like to support our work, please contact us; see <a href="https://trifectatech.org/support/">trifectatech.org/support</a>.</p>
<h3 id="about-canonical">About Canonical</h3>
<p><strong>Canonical</strong>, the publisher of <strong>Ubuntu</strong>, provides open source software, security, support and services. Canonical's portfolio covers critical systems, from the smallest devices to the largest clouds, from the kernel to containers, from databases to AI. With customers that include top tech brands, emerging startups, governments and home users, Canonical delivers trusted open source for everyone.
Learn more at <a href="https://canonical.com/">canonical.com</a></p>
<h3 id="further-information">Further Information</h3>
<ul>
<li><a href="https://trifectatech.org/initiatives/privilege-boundary/">sudo-rs Project Page on Trifecta Tech Foundation</a></li>
<li><a href="https://github.com/trifectatechfoundation/sudo-rs">sudo-rs GitHub Repository</a></li>
<li><a href="https://discourse.ubuntu.com/t/carefully-but-purposefully-oxidising-ubuntu/56995">Carefully but Purposefully Oxidising Ubuntu</a></li>
</ul>
<p><em>For media inquiries, please contact Erik Jonkers, contact@trifectatech.org</em></p>

                <br>
                
                <hr>
            </div>
        </article>
    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Getting things “done” in large tech companies (248 pts)]]></title>
            <link>https://www.seangoedecke.com/getting-things-done/</link>
            <guid>43903741</guid>
            <pubDate>Tue, 06 May 2025 11:04:39 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.seangoedecke.com/getting-things-done/">https://www.seangoedecke.com/getting-things-done/</a>, See on <a href="https://news.ycombinator.com/item?id=43903741">Hacker News</a></p>
<div id="readability-page-1" class="page"><article><header></header><section><p>What does it mean to get things done? In the abstract, you can complete a mathematical proof or a problem set, but the real world is much fuzzier. Suppose I plant a tree in my backyard. Once the sapling is in the ground, is that done? Not really. There’s always more work to do: clearing the ground around it, watering, keeping pests away, pruning, and so on. Programming large web applications is more like planting a tree than completing a mathematical proof. Once you write a service, you can keep working on it forever if you want to.</p>
<p>In large tech companies, this fact is a trap for competent but unagentic engineers. They see an infinite queue of tasks that they’re capable of doing, and they start delivering a stream of marginal improvements to a particular subsystem<sup id="fnref-1"><a href="#fn-1">1</a></sup>. From their perspective, it feels like they’re crushing it. After all, they’re putting out work at their top speed: no downtime, no waiting on other teams. But they’re not doing their actual job, which is to deliver the most value they can to their company. From the perspective of their manager and skip-level, they’re not getting anything done.</p>
<p>What does it mean to get things done in large companies? Most importantly, it means <strong>finishing</strong> things. How can you finish things in a world where you can keep improving systems indefinitely? It means <strong>getting them to a point where the decision-makers at the company are happy</strong>. At that point, you have to declare victory and walk away! Go and do something else! I’ve seen many engineers stick around delivering one more tweak or one more refactor - long past the point where their work stopped being perceived as a successful project and started being perceived as wasted time. Better to deliver two more things in that time.</p>
<p>Second, it means delivering the kind of things that are <strong>legible</strong> to the decision-makers at the company: i.e. visible to your manager, plus 1-3 skip levels, depending on your title. The easiest way to do this is to deliver things that they already know about, such as projects that they’ve asked you to do, or incidents that are serious enough that they’re involved in them. It’s possible to make other work legible to them as well. If your work produces or saves money, that will make it immediately legible, for instance (or you could just be really convincing). By default, work you do isn’t legible: to the decision-makers, it’s generic technical nonsense. They don’t know whether it’s crucial high-impact work or pointless code reshuffling, and will tend to assume the latter.</p>
<p>In short, getting things done means getting them in a state where:</p>
<p>(a) executives at the company understand what’s happened, and
(b) are happy with it</p>
<p>To many, this will be an unsatisfying definition of what it means to get things done. Lots of engineers will want a more solid definition than “it’s a social construct”. However, as someone with a philosophy background, I have a healthy respect for social constructs. The concept “chair” is a social construct, and chairs are plenty real[2]. In some ways, “getting things done” is even more real. It can pay your rent! If you don’t respect it, it can even get you fired.</p>
<h3>Summary</h3>
<ul>
<li>Just because you’re doing work doesn’t mean you’re getting things done</li>
<li>Nothing is ever “done” in the non-abstract world. Everything can be worked on indefinitely</li>
<li>If you want to get things done you can’t be a gardener. You have to aim for a bullet-point list of achievements</li>
<li>So what’s “done” mean in that context? It means the company is happy with the state of it</li>
<li>Declare victory and walk away: go and do something else!</li>
</ul>
</section><p>If you liked this post, consider <a href="https://buttondown.com/seangoedecke" target="_blank">subscribing</a> to email updates about my new posts.</p><p>May 6, 2025<!-- -->&nbsp;│ Tags: <a href="https://www.seangoedecke.com/tags/shipping/">shipping</a>, <a href="https://www.seangoedecke.com/tags/tech%20companies/">tech companies</a></p><hr></article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Design and evaluation of a parrot-to-parrot video-calling system (2023) (113 pts)]]></title>
            <link>https://www.smithsonianmag.com/smart-news/scientists-taught-pet-parrots-to-video-call-each-other-and-the-birds-loved-it-180982041/</link>
            <guid>43903728</guid>
            <pubDate>Tue, 06 May 2025 11:03:28 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.smithsonianmag.com/smart-news/scientists-taught-pet-parrots-to-video-call-each-other-and-the-birds-loved-it-180982041/">https://www.smithsonianmag.com/smart-news/scientists-taught-pet-parrots-to-video-call-each-other-and-the-birds-loved-it-180982041/</a>, See on <a href="https://news.ycombinator.com/item?id=43903728">Hacker News</a></p>
Couldn't get https://www.smithsonianmag.com/smart-news/scientists-taught-pet-parrots-to-video-call-each-other-and-the-birds-loved-it-180982041/: Error: Request failed with status code 403]]></description>
        </item>
        <item>
            <title><![CDATA[Sneakers (1992) – 4K Restoration (318 pts)]]></title>
            <link>https://www.blu-ray.com/movies/Sneakers-Blu-ray/381411/</link>
            <guid>43902263</guid>
            <pubDate>Tue, 06 May 2025 06:15:13 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.blu-ray.com/movies/Sneakers-Blu-ray/381411/">https://www.blu-ray.com/movies/Sneakers-Blu-ray/381411/</a>, See on <a href="https://news.ycombinator.com/item?id=43902263">Hacker News</a></p>
<div id="readability-page-1" class="page"><div width="1262">
	<tbody><tr>
			<td>
				<br>
			</td>
			<td>
				
							<br>

							
<br>
<center>
</center>
			</td>
			<td>
				<br>
			</td>
<td>

<span>4K Restoration</span>
<span>
<a href="https://www.blu-ray.com/movies/movies.php?studioid=280" rel="nofollow">Kino Lorber</a> | <a href="https://www.blu-ray.com/movies/movies.php?year=1992" rel="nofollow">1992</a> | <span id="runtime" title="2 hr 6 min" onclick="var runtime = document.getElementById('runtime'); var tmp = runtime.title; runtime.title = runtime.innerHTML; runtime.innerHTML = tmp">126 min</span> | Rated PG-13 | <a alt="Sneakers Blu-ray Release Date April 22, 2025" title="Sneakers Blu-ray Release Date April 22, 2025" href="https://www.blu-ray.com/movies/releasedates.php?year=2025&amp;month=4#April22">Apr 22, 2025</a></span>
		<table>
		<tbody><tr>
<td id="menu_overview"><a id="menu_overview_link" href="https://www.blu-ray.com/movies/Sneakers-Blu-ray/381411/#Overview" rel="history">Overview</a></td>
<td id="menu_review" nowrap=""><a id="menu_review_link" href="https://www.blu-ray.com/movies/Sneakers-Blu-ray/381411/#Review" rel="history">Blu-ray review</a></td>
<td id="menu_screenshots"><a id="menu_screenshots_link" href="https://www.blu-ray.com/movies/Sneakers-Blu-ray/381411/#Screenshots" rel="history">Screenshots</a></td>
<td id="menu_screenshots_num"><small>(20)</small></td>
<td id="menu_packaging"><a id="menu_packaging_link" href="https://www.blu-ray.com/movies/Sneakers-Blu-ray/381411/#Packaging" rel="history">Packaging</a></td>
<td id="menu_userreviews"><a id="menu_userreviews_link" href="https://www.blu-ray.com/movies/Sneakers-Blu-ray/381411/#UserReviews" rel="history">User&nbsp;reviews</a></td>
<td id="menu_regioncoding"><a id="menu_regioncoding_link" href="https://www.blu-ray.com/movies/Sneakers-Blu-ray/381411/#RegionCoding" rel="history">Region&nbsp;coding</a></td>
<td id="menu_news"><a id="menu_news_link" href="https://www.blu-ray.com/movies/Sneakers-Blu-ray/381411/#News" rel="history">News</a></td>
<td id="menu_forum"><a id="menu_forum_link" href="https://www.blu-ray.com/movies/Sneakers-Blu-ray/381411/#Forum" rel="history">Forum</a></td>
<td><br></td>
		</tr>
		</tbody></table>
		<span><br></span>
		


<div id="movie_review_video">
<a name="#video"><h3 id="movie_review_video_heading">Sneakers Blu-ray, Video Quality</h3></a>
&nbsp; <p><img src="https://images.static-bluray.com/rating/b10.jpg" width="69" height="13" alt="5.0 of 5"></p><p><a href="https://www.blu-ray.com/movies/screenshot.php?movieid=381411&amp;position=2"><img id="load60766577487553" src="https://images.static-bluray.com/transparent.gif" loading="lazy" width="728" height="409"></a></p><p>
Presented in its original aspect ratio of 1.85:1, encoded with MPEG-4 AVC and granted a 1080p transfer, <i>Sneakers</i> arrives on Blu-ray courtesy of Kino Lorber. </p><p>

The release introduces a new 4K makeover of <i>Sneakers</i> struck from the original camera negative. The 4K makeover is also made available on 4K Blu-ray in <a href="https://www.blu-ray.com/movies/Sneakers-4K-Blu-ray/343185/#Review"><i>this</i></a> combo pack.</p><p>

I have only one other release of <i>Sneakers</i> in my library, which is <a href="https://www.blu-ray.com/movies/Sneakers-Blu-ray/65706/#Review"><i>this</i></a> Region-B release, produced by Universal Pictures-UK in 2013. I think that the previous presentation of the film is mostly good, but it has a dated appearance that was undeniable even more than a decade ago. (At that time, virtually all older masters that emerged from the major's vault produced very harsh visuals with unpleasant digital appearance. The old master that was used to produce the previous release of the film was one of the few that had fine organic qualities). After viewing the 4K makeover in its entirety on 4K Blu-ray and then spending time with it on the Blu-ray, I did not think that comparisons with the previous release were needed. The 4K makeover brings substantial improvements in all areas we scrutinize in our reviews, and they are very, very easy to appreciate on 4K Blu-ray and Blu-ray. For example, the darker/dark material where colored light and shadows constantly interact convey outstanding delineation, clarity, and depth. In some areas, there is simply a lot more to see, and the perception of depth is completely different now because of how entire ranges of nuances look. Also, the density levels are terrific on 4K Blu-ray and Blu-ray. The entire 4K makeover is graded with outstanding precision, too. All primaries and supporting nuances are impeccably set and balanced, allowing the visuals to reveal an enormously attractive period appearance. Because of the expanded color gamut and superior dynamic range of 4K, I think that the strength and accuracy of the color grade are more impressive in native 4K. However, I must make it clear that the Blu-ray still offers a mighty fine upgrade in quality. Image stability is excellent. The entire film looks spotless as well. (<i>Note</i>: This is a Region-A "locked" Blu-ray release. Therefore, you must have a native Region-A or Region-Free player in order to access its content). </p></div>
<div id="movie_review_audio">
<a name="#audio"><h3 id="movie_review_audio_heading">Sneakers Blu-ray, Audio Quality</h3></a>
&nbsp; <p><img src="https://images.static-bluray.com/rating/b10.jpg" width="69" height="13" alt="5.0 of 5"></p><p><a href="https://www.blu-ray.com/movies/screenshot.php?movieid=381411&amp;position=3"><img id="load48553884002189" src="https://images.static-bluray.com/transparent.gif" loading="lazy" width="728" height="409"></a></p><p>
There are two standard audio tracks on this release: English DTS-HD Master Audio 5.1 and English DTS-HD Master Audio 2.0. Optional English SDH subtitles are provided for the main feature. </p><p>

I viewed the new 4K makeover of <i>Sneakers</i> in its entirety and later spent time with it on the Blu-ray. The comments below are from our review of the 4K Blu-ray/Blu-ray combo pack release.</p><p>

I viewed a good portion of <i>Sneakers</i> with the 2.0 track and liked it a lot. It has a great dynamic range and is very, very healthy. The 5.1 track is good, too. However, I have always felt that it could have had more surround movement to impress as a 5.1 track. Some of the action sequences throughout the film create great opportunities for impressive surround movement, but there is hardly anything meaningful happening there. The dialog is clear, sharp, stable, and easy to follow. 

</p></div>



<div id="movie_news">
<h2>Sneakers: Other Editions</h2><table><tbody><tr>
<td><center>
<a data-globalproductid="1720666" data-globalparentid="35604" data-categoryid="7" data-productid="343185" href="https://www.blu-ray.com/link/link.php?url=https://www.blu-ray.com/movies/Sneakers-4K-Blu-ray/343185/" title="Sneakers 4K Blu-ray (1992)"><img loading="lazy" id="load62452077454263" src="https://images.static-bluray.com/transparent.gif" width="88" height="110" title="Sneakers 4K Blu-ray (1992)"></a><br>
<b>4K</b><br>
2-disc set<br><b>$31.49</b></center></td>
<td><center>
<a data-globalproductid="421530" data-globalparentid="35604" data-categoryid="7" data-productid="125225" href="https://www.blu-ray.com/link/link.php?url=https://www.blu-ray.com/movies/Sneakers-Blu-ray/125225/" title="Sneakers Blu-ray (1992)"><img loading="lazy" id="load23897872488079" src="https://images.static-bluray.com/transparent.gif" width="86" height="110" title="Sneakers Blu-ray (1992)"></a><br>
<b>Blu-ray</b><br>
1-disc</center></td>
<td><center>
<a data-globalproductid="127086" data-globalparentid="35604" data-categoryid="7" data-productid="48172" href="https://www.blu-ray.com/link/link.php?url=https://www.blu-ray.com/movies/Sneakers-Blu-ray/48172/" title="Sneakers Blu-ray (1992)"><img loading="lazy" id="load42182786428910" src="https://images.static-bluray.com/transparent.gif" width="85" height="110" title="Sneakers Blu-ray (1992)"></a><br>
<b>Blu-ray</b><br>
1-disc<br><b>$8.82</b></center></td>
<td colspan="5"></td>
</tr></tbody></table><br>

<h3>Sneakers Blu-ray, News and Updates</h3><p>
• <a href="https://www.blu-ray.com/news/?id=35937">Sneakers 4K Blu-ray</a></p><p><span> - February 5, 2025</span></p><p>Kino Lorber have detailed their upcoming 4K Blu-ray and Blu-ray releases of Phil Alden Robinson's Sneakers (1992), starring Robert Redford, Dan Aykroyd, Ben Kingsley, Mary McDonnell, and River Phoenix. The two releases are scheduled to arrive on the market on April ...</p>
<p>
• <a href="https://www.blu-ray.com/news/?id=33042">Sneakers 4K Blu-ray</a> <span> - August 6, 2023</span></p><p>Kino Lorber are preparing a 4K Blu-ray release of Phil Alden Robinson's Sneakers (1992), starring Robert Redford, Dan Aykroyd, Ben Kingsley, Mary McDonnell, and River Phoenix. The release is expected to arrive on the market later this year.</p>
<p>
• <a href="https://www.blu-ray.com/news/?id=15738">Upcoming Universal Blu-ray Releases</a> <span> - December 29, 2014</span></p><p>Universal Studios Home Entertainment will add three new titles to its Blu-ray catalog in February:  Steven Spielberg's Munich (2005), Robert Wise's The Andromeda Strain (1971), and  Phil Alden Robinson's Sneakers (1992).</p>
<p><b>»</b> Show more <a href="https://www.blu-ray.com/movies/Sneakers-Blu-ray-Forum/381411/#News" onclick="window.scrollTo(0, 0)" rel="history">related news posts</a> for Sneakers Blu-ray</p></div>












<br>






		

		</td>
		<td>
		</td>
		<td>
<a href="https://play.google.com/store/apps/details?id=com.bluray.android.mymovies"><img id="load72156764134705" src="https://images.static-bluray.com/transparent.gif" loading="lazy" width="148" height="52"></a>&nbsp;<a href="https://itunes.apple.com/us/app/my-movies-by-blu-ray.com/id514571960"><img id="load57773101475148" src="https://images.static-bluray.com/transparent.gif" loading="lazy" width="148" height="52"></a><br><span><br></span>
			
			<br>
		
		<br>
		
		<table>
		<tbody><tr>
			<td>
			</td>
			<td>
<p><a href="https://www.blu-ray.com/link/click.php?p=1569816&amp;tid=070" onclick="document.cookie = 'tid=070; expires='+new Date(new Date().getTime()+20*60*1000).toUTCString()+'; path=/; domain=.blu-ray.com'"><img src="https://images.static-bluray.com/movies/covers/316463_medium.jpg"></a><br>
<b>$11.89</b><br><span><b>-$2.1</b></span><br><span><small>1 hour ago</small></span></p><p><a href="https://www.blu-ray.com/link/click.php?p=1870368&amp;tid=070" onclick="document.cookie = 'tid=070; expires='+new Date(new Date().getTime()+20*60*1000).toUTCString()+'; path=/; domain=.blu-ray.com'"><img src="https://images.static-bluray.com/movies/covers/367676_medium.jpg"></a><br>
<b>$48.99</b><br><span><b>-$3</b></span><br><span><small>2 hours ago</small></span></p><p><a href="https://www.blu-ray.com/link/click.php?p=1464140&amp;tid=070" onclick="document.cookie = 'tid=070; expires='+new Date(new Date().getTime()+20*60*1000).toUTCString()+'; path=/; domain=.blu-ray.com'"><img src="https://images.static-bluray.com/movies/covers/297693_medium.jpg"></a><br>
<b>$57.34</b><br><span><b>-$22.61</b></span><br><span><small>2 hours ago</small></span></p><p><a href="https://www.blu-ray.com/link/click.php?p=1723705&amp;tid=070" onclick="document.cookie = 'tid=070; expires='+new Date(new Date().getTime()+20*60*1000).toUTCString()+'; path=/; domain=.blu-ray.com'"><img src="https://images.static-bluray.com/movies/covers/343800_medium.jpg"></a><br>
<b>$40</b><br><span><b>-$0.08</b></span><br><span><small>2 hours ago</small></span></p><p><a href="https://www.blu-ray.com/link/click.php?p=1904825&amp;tid=070" onclick="document.cookie = 'tid=070; expires='+new Date(new Date().getTime()+20*60*1000).toUTCString()+'; path=/; domain=.blu-ray.com'"><img src="https://images.static-bluray.com/movies/covers/373447_medium.jpg"></a><br>
<b>$194.99</b><br><span><b>-$3</b></span><br><span><small>3 hours ago</small></span></p><p><a href="https://www.blu-ray.com/link/click.php?p=1896880&amp;tid=070" onclick="document.cookie = 'tid=070; expires='+new Date(new Date().getTime()+20*60*1000).toUTCString()+'; path=/; domain=.blu-ray.com'"><img src="https://images.static-bluray.com/movies/covers/371342_medium.jpg"></a><br>
<b>$19.99</b><br><span><b>-$2.5</b></span><br><span><small>3 hours ago</small></span></p>				<br>
				<a href="https://www.blu-ray.com/deals/?sortby=time" rel="nofollow">Show new deals »</a>
				</td>
			<td>
			</td>
		</tr>
		</tbody></table>
		
<p>Trending Blu-ray Movies <img id="load76656009377113" src="https://images.static-bluray.com/transparent.gif" loading="lazy" width="18" height="12"></p>
<table>
<tbody><tr><td><span>1.</span>&nbsp;</td><td><a href="https://www.blu-ray.com/movies/Anora-4K-Blu-ray/379728/">Anora 4K</a></td></tr>
<tr><td><span>2.</span>&nbsp;</td><td><a href="https://www.blu-ray.com/movies/Dirty-Harry-4K-Blu-ray/379700/">Dirty Harry 4K</a></td></tr>
<tr><td><span>3.</span>&nbsp;</td><td><a href="https://www.blu-ray.com/movies/Dirty-Work-4K-Blu-ray/387582/">Dirty Work 4K</a></td></tr>
<tr><td><span>4.</span>&nbsp;</td><td><a href="https://www.blu-ray.com/movies/Lethal-Weapon-4K-Blu-ray/387483/">Lethal Weapon 4K</a></td></tr>
<tr><td><span>5.</span>&nbsp;</td><td><a href="https://www.blu-ray.com/movies/The-Outlaw-Josey-Wales-4K-Blu-ray/379701/">The Outlaw Josey Wales 4K</a></td></tr>
<tr><td><span>6.</span>&nbsp;</td><td><a href="https://www.blu-ray.com/movies/Pale-Rider-4K-Blu-ray/378291/">Pale Rider 4K</a></td></tr>
<tr><td><span>7.</span>&nbsp;</td><td><a href="https://www.blu-ray.com/movies/The-Golden-Child-4K-Blu-ray/387580/">The Golden Child 4K</a></td></tr>
<tr><td><span>8.</span>&nbsp;</td><td><a href="https://www.blu-ray.com/movies/Dirty-Harry-4K-Blu-ray/382391/">Dirty Harry 4K</a></td></tr>
<tr><td><span>9.</span>&nbsp;</td><td><a href="https://www.blu-ray.com/movies/Stripes-4K-Blu-ray/380958/">Stripes 4K</a></td></tr>
<tr><td><span>10.</span>&nbsp;</td><td><a href="https://www.blu-ray.com/movies/Jade-4K-Blu-ray/387579/">Jade 4K</a></td></tr>
<tr><td><span>11.</span>&nbsp;</td><td><a href="https://www.blu-ray.com/movies/Timecop-4K-Blu-ray/380144/">Timecop 4K</a></td></tr>
<tr><td><span>12.</span>&nbsp;</td><td><a href="https://www.blu-ray.com/movies/Tombstone-4K-Blu-ray/384615/">Tombstone 4K</a></td></tr>
<tr><td><span>13.</span>&nbsp;</td><td><a href="https://www.blu-ray.com/movies/The-Outlaw-Josey-Wales-4K-Blu-ray/382392/">The Outlaw Josey Wales 4K</a></td></tr>
<tr><td><span>14.</span>&nbsp;</td><td><a href="https://www.blu-ray.com/movies/Ace-Ventura-Pet-Detective-4K-Blu-ray/387416/">Ace Ventura: Pet Detective 4K</a></td></tr>
<tr><td><span>15.</span>&nbsp;</td><td><a href="https://www.blu-ray.com/movies/The-Rocky-Horror-Picture-Show-4K-Blu-ray/387515/">The Rocky Horror Picture Show 4K</a></td></tr>
</tbody></table><p>Trending in Theaters</p>
<table>
<tbody><tr><td><span>1.</span>&nbsp;</td><td><a href="https://www.blu-ray.com/Sinners/1897027/">Sinners</a></td></tr>
<tr><td><span>2.</span>&nbsp;</td><td><a href="https://www.blu-ray.com/The-Accountant-2/1872803/">The Accountant 2</a></td></tr>
<tr><td><span>3.</span>&nbsp;</td><td><a href="https://www.blu-ray.com/Until-Dawn/1902101/">Until Dawn</a></td></tr>
<tr><td><span>4.</span>&nbsp;</td><td><a href="https://www.blu-ray.com/Captain-America-Brave-New-World/1467785/">Captain America: Brave New World</a></td></tr>
<tr><td><span>5.</span>&nbsp;</td><td><a href="https://www.blu-ray.com/Heart-Eyes/1910716/">Heart Eyes</a></td></tr>
<tr><td><span>6.</span>&nbsp;</td><td><a href="https://www.blu-ray.com/Last-Breath/1856515/">Last Breath</a></td></tr>
<tr><td><span>7.</span>&nbsp;</td><td><a href="https://www.blu-ray.com/The-Legend-of-Ochi/1613853/">The Legend of Ochi</a></td></tr>
<tr><td><span>8.</span>&nbsp;</td><td><a href="https://www.blu-ray.com/Drop/1906284/">Drop</a></td></tr>
<tr><td><span>9.</span>&nbsp;</td><td><a href="https://www.blu-ray.com/Warfare/1879700/">Warfare</a></td></tr>
<tr><td><span>10.</span>&nbsp;</td><td><a href="https://www.blu-ray.com/Death-of-a-Unicorn/1755689/">Death of a Unicorn</a></td></tr>
<tr><td><span>11.</span>&nbsp;</td><td><a href="https://www.blu-ray.com/A-Minecraft-Movie/598331/">A Minecraft Movie</a></td></tr>
<tr><td><span>12.</span>&nbsp;</td><td><a href="https://www.blu-ray.com/Novocaine/1812774/">Novocaine</a></td></tr>
<tr><td><span>13.</span>&nbsp;</td><td><a href="https://www.blu-ray.com/Paddington-in-Peru/1452862/">Paddington in Peru</a></td></tr>
<tr><td><span>14.</span>&nbsp;</td><td><a href="https://www.blu-ray.com/Mickey-17/1594399/">Mickey 17</a></td></tr>
<tr><td><span>15.</span>&nbsp;</td><td><a href="https://www.blu-ray.com/Black-Bag/1856516/">Black Bag</a></td></tr>
<tr><td><span>16.</span>&nbsp;</td><td><a href="https://www.blu-ray.com/A-Working-Man/1834507/">A Working Man</a></td></tr>
<tr><td><span>17.</span>&nbsp;</td><td><a href="https://www.blu-ray.com/Hell-of-a-Summer/1941861/">Hell of a Summer</a></td></tr>
<tr><td><span>18.</span>&nbsp;</td><td><a href="https://www.blu-ray.com/Freaky-Tales/1688953/">Freaky Tales</a></td></tr>
<tr><td><span>19.</span>&nbsp;</td><td><a href="https://www.blu-ray.com/Love-Hurts/1908196/">Love Hurts</a></td></tr>
<tr><td><span>20.</span>&nbsp;</td><td><a href="https://www.blu-ray.com/The-Amateur/1710126/">The Amateur</a></td></tr>
<tr><td><span>21.</span>&nbsp;</td><td><a href="https://www.blu-ray.com/The-Day-the-Earth-Blew-Up-A-Looney-Tunes-Movie/1775994/">The Day the Earth Blew Up: A Looney Tunes Movie</a></td></tr>
<tr><td><span>22.</span>&nbsp;</td><td><a href="https://www.blu-ray.com/Neighborhood-Watch/1968058/">Neighborhood Watch</a></td></tr>
<tr><td><span>23.</span>&nbsp;</td><td><a href="https://www.blu-ray.com/On-Swift-Horses/1676554/">On Swift Horses</a></td></tr>
<tr><td><span>24.</span>&nbsp;</td><td><a href="https://www.blu-ray.com/Ash/1940611/">Ash</a></td></tr>
<tr><td><span>25.</span>&nbsp;</td><td><a href="https://www.blu-ray.com/The-Woman-in-the-Yard/1833692/">The Woman in the Yard</a></td></tr>
<tr><td><span>26.</span>&nbsp;</td><td><a href="https://www.blu-ray.com/The-Monkey/1856645/">The Monkey</a></td></tr>
<tr><td><span>27.</span>&nbsp;</td><td><a href="https://www.blu-ray.com/October-8/1948250/">October 8</a></td></tr>
<tr><td><span>28.</span>&nbsp;</td><td><a href="https://www.blu-ray.com/Locked/1947168/">Locked</a></td></tr>
<tr><td><span>29.</span>&nbsp;</td><td><a href="https://www.blu-ray.com/The-Ballad-of-Wallis-Island/1856514/">The Ballad of Wallis Island</a></td></tr>
<tr><td><span>30.</span>&nbsp;</td><td><a href="https://www.blu-ray.com/The-Friend/1899553/">The Friend</a></td></tr>
<tr><td><span>31.</span>&nbsp;</td><td><a href="https://www.blu-ray.com/The-Shrouds/1697619/">The Shrouds</a></td></tr>
<tr><td><span>32.</span>&nbsp;</td><td><a href="https://www.blu-ray.com/Snow-White/1450301/">Snow White</a></td></tr>
<tr><td><span>33.</span>&nbsp;</td><td><a href="https://www.blu-ray.com/The-Ugly-Stepsister/1968057/">The Ugly Stepsister</a></td></tr>
<tr><td><span>34.</span>&nbsp;</td><td><a href="https://www.blu-ray.com/Mob-Cops/1972363/">Mob Cops</a></td></tr>
<tr><td><span>35.</span>&nbsp;</td><td><a href="https://www.blu-ray.com/In-the-Lost-Lands/1710131/">In the Lost Lands</a></td></tr>
<tr><td><span>36.</span>&nbsp;</td><td><a href="https://www.blu-ray.com/The-Rule-of-Jenny-Pen/1950435/">The Rule of Jenny Pen</a></td></tr>
<tr><td><span>37.</span>&nbsp;</td><td><a href="https://www.blu-ray.com/Opus/1804841/">Opus</a></td></tr>
<tr><td><span>38.</span>&nbsp;</td><td><a href="https://www.blu-ray.com/The-Penguin-Lessons/1901077/">The Penguin Lessons</a></td></tr>
<tr><td><span>39.</span>&nbsp;</td><td><a href="https://www.blu-ray.com/The-Wedding-Banquet/1918822/">The Wedding Banquet</a></td></tr>
<tr><td><span>40.</span>&nbsp;</td><td><a href="https://www.blu-ray.com/Bring-Them-Down/1889967/">Bring Them Down</a></td></tr>
</tbody></table><br>
<table>
<tbody><tr><td colspan="2">
</td></tr><tr><td><span>1.</span>&nbsp;
</td><td><a href="https://www.blu-ray.com/link/link.php?url=https://www.blu-ray.com/movies/Pink-Floyd-Live-at-Pompeii-Blu-ray/383017/" rel="nofollow">Pink Floyd: Live at Pompeii</a><br>
</td></tr><tr><td><span>2.</span>&nbsp;
</td><td><a href="https://www.blu-ray.com/link/link.php?url=https://www.blu-ray.com/movies/Captain-America-Brave-New-World-4K-Blu-ray/385906/" rel="nofollow">Captain America: Brave New World 4K</a><br>
</td></tr><tr><td><span>3.</span>&nbsp;
</td><td><a href="https://www.blu-ray.com/link/link.php?url=https://www.blu-ray.com/movies/Alexander-Revisited-The-Final-Cut-4K-Blu-ray/387007/" rel="nofollow">Alexander Revisited: The Final Cut 4K</a><br>
</td></tr><tr><td><span>4.</span>&nbsp;
</td><td><a href="https://www.blu-ray.com/link/link.php?url=https://www.blu-ray.com/movies/Jaws-4K-Blu-ray/387417/" rel="nofollow">Jaws 4K</a><br>
</td></tr><tr><td><span>5.</span>&nbsp;
</td><td><a href="https://www.blu-ray.com/link/link.php?url=https://www.blu-ray.com/movies/Wicked-Blu-ray/376117/" rel="nofollow">Wicked</a><br>
</td></tr><tr><td><span>6.</span>&nbsp;
</td><td><a href="https://www.blu-ray.com/link/link.php?url=https://www.blu-ray.com/movies/Final-Destination-5-Film-Collection-Blu-ray/315427/" rel="nofollow">Final Destination 5 Film Collection</a><br>
</td></tr><tr><td><span>7.</span>&nbsp;
</td><td><a href="https://www.blu-ray.com/link/link.php?url=https://www.blu-ray.com/movies/Abigail-4K-Blu-ray/387010/" rel="nofollow">Abigail 4K</a><br>
</td></tr><tr><td><span>8.</span>&nbsp;
</td><td><a href="https://www.blu-ray.com/link/link.php?url=https://www.blu-ray.com/movies/Lilo-and-Stitch-4K-Blu-ray/385554/" rel="nofollow">Lilo &amp; Stitch 4K</a><br>
</td></tr><tr><td><span>9.</span>&nbsp;
</td><td><a href="https://www.blu-ray.com/link/link.php?url=https://www.blu-ray.com/movies/Captain-America-Brave-New-World-4K-Blu-ray/385905/" rel="nofollow">Captain America: Brave New World 4K</a><br>
</td></tr><tr><td><span>10.</span>&nbsp;
</td><td><a href="https://www.blu-ray.com/link/link.php?url=https://www.blu-ray.com/movies/Captain-America-Brave-New-World-Blu-ray/386295/" rel="nofollow">Captain America: Brave New World</a><br>
</td></tr><tr><td colspan="2">
&nbsp;&nbsp;<b>»</b> <a href="https://www.blu-ray.com/movies/top.php">See more top sellers</a></td></tr></tbody></table><br>
<table>
<tbody><tr><td colspan="2">
</td></tr><tr><td><span>1.</span>&nbsp;
</td><td><a href="https://www.blu-ray.com/link/link.php?url=https://www.blu-ray.com/movies/Captain-America-Brave-New-World-4K-Blu-ray/385906/" rel="nofollow">Captain America: Brave New World 4K</a><br>
</td></tr><tr><td><span>2.</span>&nbsp;
</td><td><a href="https://www.blu-ray.com/link/link.php?url=https://www.blu-ray.com/movies/Alexander-Revisited-The-Final-Cut-4K-Blu-ray/387007/" rel="nofollow">Alexander Revisited: The Final Cut 4K</a><br>
</td></tr><tr><td><span>3.</span>&nbsp;
</td><td><a href="https://www.blu-ray.com/link/link.php?url=https://www.blu-ray.com/movies/Jaws-4K-Blu-ray/387417/" rel="nofollow">Jaws 4K</a><br>
</td></tr><tr><td><span>4.</span>&nbsp;
</td><td><a href="https://www.blu-ray.com/link/link.php?url=https://www.blu-ray.com/movies/Abigail-4K-Blu-ray/387010/" rel="nofollow">Abigail 4K</a><br>
</td></tr><tr><td><span>5.</span>&nbsp;
</td><td><a href="https://www.blu-ray.com/link/link.php?url=https://www.blu-ray.com/movies/Captain-America-Brave-New-World-4K-Blu-ray/385905/" rel="nofollow">Captain America: Brave New World 4K</a><br>
</td></tr><tr><td><span>6.</span>&nbsp;
</td><td><a href="https://www.blu-ray.com/link/link.php?url=https://www.blu-ray.com/movies/Captain-America-Brave-New-World-Blu-ray/386295/" rel="nofollow">Captain America: Brave New World</a><br>
</td></tr><tr><td><span>7.</span>&nbsp;
</td><td><a href="https://www.blu-ray.com/link/link.php?url=https://www.blu-ray.com/movies/Looney-Tunes-Collectors-Vault-Volume-1-Blu-ray/384090/" rel="nofollow">Looney Tunes Collector's Vault: Vol...</a><br>
</td></tr><tr><td><span>8.</span>&nbsp;
</td><td><a href="https://www.blu-ray.com/link/link.php?url=https://www.blu-ray.com/movies/Sinners-4K-Blu-ray/384583/" rel="nofollow">Sinners 4K</a><br>
</td></tr><tr><td><span>9.</span>&nbsp;
</td><td><a href="https://www.blu-ray.com/link/link.php?url=https://www.blu-ray.com/movies/Grave-of-the-Fireflies-Blu-ray/387023/" rel="nofollow">Grave of the Fireflies</a><br>
</td></tr><tr><td><span>10.</span>&nbsp;
</td><td><a href="https://www.blu-ray.com/link/link.php?url=https://www.blu-ray.com/movies/Small-Soldiers-4K-Blu-ray/386359/" rel="nofollow">Small Soldiers 4K</a><br>
</td></tr><tr><td colspan="2">
&nbsp;&nbsp;<b>»</b> <a href="https://www.blu-ray.com/movies/top.php?show=preorders">See more pre-orders</a></td></tr></tbody></table><br>
<table>
<tbody><tr><td colspan="2">
</td></tr><tr><td><span>1.</span>&nbsp;
</td><td><a href="https://www.blu-ray.com/link/link.php?url=https://www.blu-ray.com/movies/Final-Destination-5-Film-Collection-Blu-ray/315427/" rel="nofollow">Final Destination 5 Film Collection</a><br>
<span>$12.99, Save 48%</span><br>
</td></tr><tr><td><span>2.</span>&nbsp;
</td><td><a href="https://www.blu-ray.com/link/link.php?url=https://www.blu-ray.com/movies/Solo-A-Star-Wars-Story-4K-Blu-ray/205706/" rel="nofollow">Solo: A Star Wars Story 4K</a><br>
<span>$16.79, Save 57%</span><br>
</td></tr><tr><td><span>3.</span>&nbsp;
</td><td><a href="https://www.blu-ray.com/link/link.php?url=https://www.blu-ray.com/movies/Gladiator-II-4K-Blu-ray/365746/" rel="nofollow">Gladiator II 4K</a><br>
<span>$19.96, Save 47%</span><br>
</td></tr><tr><td><span>4.</span>&nbsp;
</td><td><a href="https://www.blu-ray.com/link/link.php?url=https://www.blu-ray.com/movies/The-Day-the-Earth-Blew-Up-A-Looney-Tunes-Movie-Blu-ray/384325/" rel="nofollow">The Day the Earth Blew Up: A Looney...</a><br>
<span>$14.84, Save 45%</span><br>
</td></tr><tr><td><span>5.</span>&nbsp;
</td><td><a href="https://www.blu-ray.com/link/link.php?url=https://www.blu-ray.com/movies/Andor-The-Complete-First-Season-4K-Blu-ray/356918/" rel="nofollow">Andor: The Complete First Season 4K</a><br>
<span>$44.96, Save 48%</span><br>
</td></tr><tr><td><span>6.</span>&nbsp;
</td><td><a href="https://www.blu-ray.com/link/link.php?url=https://www.blu-ray.com/movies/Presence-4K-Blu-ray/382881/" rel="nofollow">Presence 4K</a><br>
<span>$21.99, Save 45%</span><br>
</td></tr><tr><td><span>7.</span>&nbsp;
</td><td><a href="https://www.blu-ray.com/link/link.php?url=https://www.blu-ray.com/movies/How-to-Train-Your-Dragon-3-Movie-Collection-Blu-ray/234903/" rel="nofollow">How to Train Your Dragon: 3-Movie C...</a><br>
<span>$12.59, Save 58%</span><br>
</td></tr><tr><td><span>8.</span>&nbsp;
</td><td><a href="https://www.blu-ray.com/link/link.php?url=https://www.blu-ray.com/movies/Blade-Runner-2049-4K-Blu-ray/189774/" rel="nofollow">Blade Runner 2049 4K</a><br>
<span>$14.99, Save 57%</span><br>
</td></tr><tr><td><span>9.</span>&nbsp;
</td><td><a href="https://www.blu-ray.com/link/link.php?url=https://www.blu-ray.com/movies/Andor-The-Complete-First-Season-Blu-ray/347799/" rel="nofollow">Andor: The Complete First Season</a><br>
<span>$39.96, Save 47%</span><br>
</td></tr><tr><td><span>10.</span>&nbsp;
</td><td><a href="https://www.blu-ray.com/link/link.php?url=https://www.blu-ray.com/movies/The-Mandalorian-The-Complete-Third-Season-4K-Blu-ray/374175/" rel="nofollow">The Mandalorian: The Complete Third...</a><br>
<span>$39.36, Save 48%</span><br>
</td></tr><tr><td colspan="2">
&nbsp;&nbsp;<b>»</b> <a href="https://www.blu-ray.com/movies/deals.php?action=bestdeals">See more deals</a></td></tr></tbody></table><br>	</td>
	</tr>
	</tbody></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[The curse of knowing how, or; fixing everything (799 pts)]]></title>
            <link>https://notashelf.dev/posts/curse-of-knowing</link>
            <guid>43902212</guid>
            <pubDate>Tue, 06 May 2025 06:01:23 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://notashelf.dev/posts/curse-of-knowing">https://notashelf.dev/posts/curse-of-knowing</a>, See on <a href="https://news.ycombinator.com/item?id=43902212">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-astro-cid-gjtny2mx="">  <p>It starts innocently.</p>
<p>You rename a batch of files with a ten-line Python script, or you alias a common
<code>git</code> command to shave off two keystrokes. Maybe you build a small shell
function to format JSON from the clipboard.</p>
<p>You’re not even trying to be clever. You’re just solving tiny problems. Making
the machine do what it should have done in the first place. And then something
happens. You cross a <em>threshold</em>. You look at your tools, your environment, your
operating system—even your editor—and suddenly <strong>everything</strong> is fair game.</p>
<p>You <em>could</em> rebuild that (if you wanted to).<br>
You could <em>improve</em> that (if you wanted to).</p>
<p>Then someone challenges you. As banter maybe, perhaps jokingly but also with a
dash of hope. Then the air in the room changes.</p>
<p>It suddenly becomes something else. It becomes:</p>
<p>You <em>should</em>.</p>
<p>And from that moment forward, the world is broken in new and specific ways that
only <em>you</em> can see.</p>
<h2 id="technical-capability-as-a-moral-weight">Technical Capability as a Moral Weight</h2>
<p>Before I could program, broken software was frustrating but ignorable. For years
I’ve simply “used” a computer, as a consumer. I was what companies were
concerned with tricking into buying their products, or subscribing to their
services. Not the technical geek that they prefer to avoid with their software
releases, or banning from their games based on an OS.</p>
<p>Now it has become <em>provocative</em>. I can see the patterns that I wish I couldn’t,
find oversights that I can attribute to a certain understanding (or the lack
thereof) of a certain concept and I can <em>hear</em> what has been echoing in the head
of the computer illiterate person who conjured the program I have to debug.</p>
<p>I notice flaws like a good surgeon notices a limp.<br>
Why the <em>hell</em> does this site send ten megabytes of JavaScript for a static
page?<br>
Why is the CLI output not parseable by <code>awk</code>?<br>
Why is this config hardcoded when it could be declarative?</p>
<p>Those things are <em>not</em> just questions, they are <em>accusations</em>. And,
unfortunately, they do not stop.</p>
<p>Now that I’ve learned to notice, my perception of software has changed in its
entirety.</p>
<p>Every piece of software becomes a TODO list.<br>
Every system becomes a scaffolding for a better one.<br>
Every inconvenience becomes an indictment of inaction.</p>
<h2 id="one-must-imagine-sisyphus-happy">One Must Imagine Sisyphus Happy</h2>
<p>Like Camus’ Sisyphus, we are condemned to push the boulder of our own systems
uphill—one fix, one refactor, one script at a time. But unlike the story of
Sisyphus, the curse is not placed onto you by some god. We built the boulder
ourselves. And we keep polishing it on the way up.</p>
<p>I’ve lost count of how many projects I have started that began with some
variation of “Yeah, I could build this <em>but better</em>.”</p>
<ul>
<li>A static site generator because the existing ones had too many opinions.</li>
<li>A note-taking tool because I didn’t like the way others structured metadata.</li>
<li>A CLI task runner because Make is cryptic and Taskfile is YAML hell.</li>
<li>A personal wiki engine in Rust, then in Go, then in Nim, then back to
Markdown.</li>
<li>A homelab dashboard because I don’t like webslop.</li>
</ul>
<p>The list continues, and trust me it <em>does</em> continue. My dev directory, as it
stands, is nearing 30 gigabytes.</p>
<p>If you ask me, I was solving real, innocent problems. But in hindsight, I was
also feeding something else: a compulsion to assert control. Every new tool I
built was a sandbox I <em>owned</em>: No weird bugs. No legacy constraints. No
decisions I didn’t entirely agree with. Until, of course, I became the legacy.</p>
<p>Kafka once wrote that “<strong>a cage went in search of a bird</strong>.” <sup><a href="#user-content-fn-1" id="user-content-fnref-1" data-footnote-ref="" aria-describedby="footnote-label">1</a></sup> That is what
these projects can become. Empty systems we keep building, waiting for purpose,
for clarity, for… salvation? I’m not sure how else would you call this
pursuit.</p>
<h2 id="entropy-is-undefeated">Entropy Is Undefeated</h2>
<p>Now let’s go back. Back to when we didn’t know better.</p>
<p>Software doesn’t stay solved. Every solution you write starts to rot the moment
it exists. Not now, not later, but eventually. Libraries deprecate. APIs
change. Performance regressions creep in. Your once-perfect tool breaks silently
because <code>libfoo.so</code> is now <code>libfoo.so.2</code>. <sup><a href="#user-content-fn-2" id="user-content-fnref-2" data-footnote-ref="" aria-describedby="footnote-label">2</a></sup></p>
<p>I <em>have</em> had scripts silently fail because a website changed its HTML layout.<br>
I <em>have</em> had configuration formats break because of upstream version bumps.<br>
I <em>have</em> had Docker containers die because Alpine Linux rotated a mirror URL.</p>
<p>In each case, the immediate emotional response was not just inconvenience but
something that moreso resembles <em>guilt</em>. I built this, and I do know better. How
could I not have foreseen this? Time to fix it.</p>
<p>If you replace every part of the system over time, is it still the same tool?
Does it still serve the same purpose? Do <em>you</em>?</p>
<h2 id="the-illusion-of-finality">The Illusion of Finality</h2>
<p>I think we lie to ourselves.</p>
<blockquote>
<p>“If I just get this setup right, I’ll never have to touch it again."<br>
"If I just write this one tool, my workflow will be seamless."<br>
"If I automate this, I’ll save time forever.” <sup><a href="#user-content-fn-3" id="user-content-fnref-3" data-footnote-ref="" aria-describedby="footnote-label">3</a></sup><br>
“Write once, run everywhere.” My ass.</p>
</blockquote>
<p>It is, I admit, a seductive lie. It frames programming as a conquest of sorts. A
series of battles you win, or challenges you complete. But the imaginary war
never ends. You don’t build a castle. You dig trenches. And they flood every
time it rains. The trials are <em>never</em> complete.</p>
<h2 id="technical-work-as-emotional-regulation">Technical Work as Emotional Regulation</h2>
<p>On the theme of filling this post with literary references, let me quote the
Stoic Marcus Aurelius.</p>
<blockquote>
<p>You have power over your mind—not outside events. Realize this, and you will
find strength.</p>
</blockquote>
<p>But programming lures us into believing we <em>can</em> control the outside events.
That is where the suffering begins. There is something deeper happening here.
This is <em>not</em> just about software.</p>
<p>I believe sometimes building things is how we self-soothe. We write a new tool
or a script because we are in a desperate need for a small victory. We write a
new tool because we are overwhelmed. Refactor it, not because the code is messy,
but your life is. We chase the perfect system because it gives us something to
hold onto when everything else is spinning. This is the lesson I’ve taken from
using NixOS.</p>
<p>I have written entire applications just to avoid thinking about why I was
unhappy. Programming gives you instant feedback. You run the thing, and it
works. Or it <em>doesn’t</em>, and you fix it. Either way, you’re <em>doing something</em>.</p>
<p>That kind of agency is addictive. Especially when the rest of life doesn’t offer
it. We program because we <em>can</em>, even when we shouldn’t. Because at least it
gives us something to rebel against.</p>
<h2 id="the-burnout-you-dont-see-coming">The Burnout You Don’t See Coming</h2>
<p>Burnout doesn not just come from overwork. It comes from <em>overresponsibility</em>.</p>
<p>And programming, once internalized deeply enough, makes everything feel like
your responsibility. The bloated website. The inefficient script. The clunky
onboarding process at your job. You <em>could</em> fix it. So why aren’t you?</p>
<p>The truth you are very well aware of is that you can’t fix it all. You <em>know</em>
this, you always knew it regardless of your level of skill. But try telling that
to the part of your brain that sees every inefficiency as a moral failing.</p>
<p>Nietzsche warned of gazing too long into the abyss. But he didn <em>not</em> warn what
happens when the abyss is a <code>Makefile</code> or a 30k line of code Typescript project.</p>
<h2 id="learning-to-let-go">Learning to Let Go</h2>
<p>So where is the exit? Is this akin to Sartre’s depiction of hell, where hell
<em>is</em> other people and how they interact with your software? Or is it some weird
backwards hell where people create software that you have to interact with?</p>
<p>The first step is recognizing that <em>not everything broken is yours to fix</em>.<br>
Not every tool needs replacing.<br>
Not every bad experience is a call to action.</p>
<p>Sometimes, it’s OK to just <em>use</em> the thing. Sometimes it’s enough to know <em>why</em>
it’s broken—even if you don’t fix it. Sometimes the most disciplined thing you
can do is <em>walk away</em> from the problem you know how to solve. There’s a kind of
strength in that.</p>
<p>Not apathy, no. Nor laziness. Just… some restraint.</p>
<h2 id="a-new-kind-of-skill">A New Kind of Skill</h2>
<p>What if the real skill isn’t technical mastery? Or better yet what if it’s
emotional clarity?</p>
<ul>
<li>Knowing which problems are worth your energy.</li>
<li>Knowing which projects are worth maintaining.</li>
<li>Knowing when you’re building to help—and when you’re building to cope.</li>
<li>Knowing when to stop.</li>
</ul>
<p>This is what I’m trying to learn now. After the excitement. After the obsession.
After the burnout. I’m trying to let things stay a little broken. Because I’ve
realized I don’t want to <em>fix everything</em>. I just want to feel OK in a world
that often isn’t. I can fix something, but not everything.</p>
<hr>
<p>You learn how to program. You learn how to fix things. But the hardest thing
you’ll ever learn is when to <em>leave them broken</em>.</p>
<p>And maybe that’s the most human skill of all.</p>
<section data-footnotes="">
<ol>
<li id="user-content-fn-1">
<p>From, I believe, Kafka’s <em>The Zurau Aphorisms</em>. <a href="#user-content-fnref-1" data-footnote-backref="" aria-label="Back to reference 1">↩</a></p>
</li>
<li id="user-content-fn-2">
<p>Nix solves this. Or does it? Nix was a can of worms of its own. <a href="#user-content-fnref-2" data-footnote-backref="" aria-label="Back to reference 2">↩</a></p>
</li>
<li id="user-content-fn-3">
<p>Remember when you spent 2 hours automating a 30 minute task? Yeah, it’s
that again. <a href="#user-content-fnref-3" data-footnote-backref="" aria-label="Back to reference 3">↩</a></p>
</li>
</ol>
</section>  </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[An Appeal to Apple from Anukari: one tiny macOS detail to make Anukari fast (305 pts)]]></title>
            <link>https://anukari.com/blog/devlog/an-appeal-to-apple</link>
            <guid>43901619</guid>
            <pubDate>Tue, 06 May 2025 03:40:10 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://anukari.com/blog/devlog/an-appeal-to-apple">https://anukari.com/blog/devlog/an-appeal-to-apple</a>, See on <a href="https://news.ycombinator.com/item?id=43901619">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p><strong>TL;DR: To make Anukari’s performance reliable across all Apple silicon macOS devices, I need to talk to someone on the Apple Metal team. It would be great if someone can connect me with the right person inside Apple, or direct them to my feedback request FB17475838 as well as this devlog entry.</strong></p>
<p><em>This is going to be a VERY LONG HIGHLY TECHNICAL post, so either buckle your seatbelt or leave while you still can.</em></p>
<h2>Background</h2>
<p>The <a href="https://anukari.com/">Anukari 3D Physics Synthesizer</a> simulates a large spring-mass model in real-time for audio generation. To support a nontrivial number of physics objects, it requires a GPU for the simulation. The physics code is ALU-bound, not memory-bound. All mutable state in the simulation is stored in the GPU’s threadgroup memory, which is roughly equivalent to a manually-allocated L1 cache, so it is extremely fast.</p>
<p>The typical use-case for Anukari is running it as an AudioUnit (AU) or VST3 plugin inside a host application like Pro Tools or Ableton, also called a Digital Audio Workstation (DAW). The DAW invokes Anukari for each audio buffer block, which is a request to generate/process N samples of audio. For each block, Anukari invokes the physics simulation GPU kernel, waits for the result, and returns.</p>
<p>The audio buffer block system is important because GPU kernel scheduling has a certain amount of latency overhead, and for real-time audio we have fixed time constraints. By amortizing the GPU scheduling latency over, say, 512 audio samples, it becomes negligible. But the runtime of the kernel itself is still very important.</p>
<h2>Basic Problem</h2>
<p>Apple’s macOS is obviously extremely clever about power management, and Apple silicon hardware is built to support the OS in achieving high power efficiency.</p>
<p>As with all modern hardware, the clock rate for Apple silicon chips can be slowed down to reduce power consumption. When the OS detects that the processing demand for a given chip is low (or non-existent), it can decrease the clock rate for that chip. This is awesome.</p>
<p>The problem is that due to the way Anukari runs inside a DAW and interacts with the GPU, the heuristics that macOS uses to determine whether there is sufficient demand upon the GPU to increase its clock rate do not work.</p>
<p>Consider the chart below. The CPU does some preparatory work, there’s a small gap which represents kernel invocation latency, and then the GPU does a large block of work. Finally there’s another small gap representing the real-time headroom.</p>
<p><img src="https://static.anukari.com/blog-media/cpu-gpu-time-sequence.jpg"></p><p><em>(An aside: chalkboards are way better than whiteboards, unless you enjoy getting high on noxious fumes. in which case whiteboards are the way to go.)</em></p>
<p>I don’t have any real knowledge of macOS’s heuristics for deciding when to increase the GPU clock speed, but I might reasonably guess that it relies on something like the <a href="https://en.wikipedia.org/wiki/Load_(computing)">load average</a>. In the diagram above, the GPU load average might be only 60%, because between audio buffer blocks it is idle. Perhaps this does not meet the threshold for increasing the GPU clock rate.</p>
<p>But this is terrible for Anukari, because to meet real-time constraints, it needs the absolute lowest latency possible, which requires the highest GPU clock rate. I’m not sure how low the Apple GPU clock rate can go, but it definitely goes low enough to make Anukari unusable.</p>
<p>To be clear, it’s pretty understandable that macOS handles this situation poorly, because the GPU is mostly used for throughput workflows like graphics or ML. Audio on the GPU is really new, and there are only a couple of companies doing it right now.</p>
<h2>Are you sure the clock rate is the problem?</h2>
<p><em>Oh yes.</em> Thankfully, Apple’s first-part Instruments tools that come with Xcode have a handy Metal profiler. Among other things, this is how I first learned that Anukari is ALU-bound.</p>
<p>The Metal profiler has an incredibly useful feature: it allows you to choose the Metal “Performance State” while profiling the application. This is not configurable outside of the profiler. This is how I first figured out that the GPU clock rate was the issue: Anukari works perfectly under the Maximum performance state, and abysmally under the Minimum performance state.</p>
<h2>Wait, Anukari mostly works great on macOS. How is that possible?</h2>
<p>Given the explanation above, this is a great question. Most people with macOS find that Anukari works great. For example, I intentionally bought a base-model Macbook M1 for development, so that if Anukari worked well for me, I’d know it worked well for people with beefier hardware.</p>
<p>So how is it that Anukari works great on macOS for most people? Well, as Steve Jobs said, “It’s better to be a pirate than to join the Navy.”</p>
<p>Without being able to rely on macOS to do the right thing, I thought like a pirate and came up with a workaround: in parallel with the audio computation on the GPU, Anukari runs a second workload on the GPU that is designed to create a high load average and trick macOS into clocking up the GPU. This workload is tuned to use as little of the GPU as possible, while still creating a big enough artificial load to trigger the clock heuristics.</p>
<p>In other words, it runs a spin loop to heat up the GPU. On my Macbook M1, this completely solves the issue. Anukari runs completely reliably. I called this strategy “waste makes haste” and it is documented in detail on my devlog <a href="https://anukari.com/blog/devlog/waste-makes-haste">here</a>.</p>
<p>To be clear, the spin loop is an unholy abomination and I hate that it’s necessary. But it is absolutely required for Anukari to work well for macOS users. And for most macOS users, it works great.</p>
<h2>So with the “waste makes haste” strategy, what’s the problem?</h2>
<p>Like I said, on my M1 things work perfectly. But then I released the Anukari Beta and some macOS users are having problems. What’s different?</p>
<p>First: I’m not certain. But I have a couple hypotheses.</p>
<p>Weirdly, it appears that most users with performance issues are using Pro or Max Apple hardware. These have additional GPU chiplets. I am completely speculating here, but Apple’s hardware is amazing so it stands to reason that each GPU chiplets's clock rate can be changed independently. Do you see where this is going?</p>
<p>If macOS is really smart, it should see that Anukari is running two independent GPU workloads: the physics kernel, and the spin kernel. Why not run those on separate GPU chiplets? That’s smart. And then, if I’m right that the GPU chiplets have independent clock rates, well, Anukari is hosed because the stupid spin workload will get a fast-clock GPU and the audio workload will get a slow-clock GPU.</p>
<p>I could be wrong. Another possibility is that the GPU has a single clock rate, and my spin workload is too conservative to convince the much more powerful GPU to clock up. Maybe my spin kernel can heat up an M1 GPU but not an M4 Pro GPU, because the M4 Pro is faster.</p>
<h2>What do you think macOS should do to fix this?</h2>
<p>Apple engineers will obviously know better than I do here, but I’ll present a couple of obvious possibilities.</p>
<p><strong>Solution 1:</strong> On macOS, audio processing is done on a thread (or group of threads) called an Audio Workgroup. These are explained in Apple’s documentation <a href="https://developer.apple.com/documentation/audiotoolbox/understanding-audio-workgroups?language=objc">here</a>. Within an Audio Workgroup, the OS understands that the threads have real-time constraints, and prioritizes those threads appropriately. This is actually a fantastic innovation, because before Audio Workgroups, it wasn’t really possible to do real-time audio processing safely across multiple threads without problems like priority inversion, etc.</p>
<p>The Audio Workgroup concept could be extended to cover processing on the GPU. Any MTLCommandQueue managed by an Audio Workgroup thread could be treated as real-time and the GPU clock could be adjusted accordingly.</p>
<p><strong>Solution 2:</strong> The Metal API could simply provide an option on MTLCommandQueue to indicate that it is real-time sensitive, and the clock for the GPU chiplet handling that queue could be adjusted accordingly.</p>
<p><strong>Solution 3:</strong> Someone could point out that I'm an idiot and there's already some way to get what I want, and this whole post was a waste of my time. This would be wonderful.</p>
<h2>Wouldn’t Apple’s new Game Mode help?</h2>
<p>Game Mode certainly seems similar to what Anukari needs. However, Game Mode is at the process-level, and Anukari is mostly used as a plugin inside other processes, which don’t support Game Mode, and anyway Anukari has no control. Also Anukari is usually not fullscreen, which Game Mode requires.</p>
<h2>What about Windows performance?</h2>
<p>Not a problem at all. I don’t know if it’s because Windows gives users more control over their system’s performance state, or if e.g. NVIDIA drivers are less careful about power consumption, or what. But the spin loop is not necessary on Windows.</p>
<p>It's not a great look for Apple that a Windows PC with a pretty wimpy GPU can run Anukari just fine, and the most expensive Mac M4 Max stutters, because obviously Apple's hardware is <em>incredible</em> and just needs to be let off the leash a bit.</p>
<h2>Why don’t you just pipeline the GPU code so that it saturates the GPU?</h2>
<p>For a throughput workload, this is exactly what I’d do. But Anukari is not throughput-sensitive, it is latency-sensitive.</p>
<p>The idea with pipelining is that maybe Anukari could schedule multiple physics simulation kernels in advance, so that the GPU could be processing the current audio sample block at the same time that the CPU is preparing the next block for the GPU. (This would also alleviate the kernel invocation latency overhead, but that’s not as important.)</p>
<p>But anyone who understands pipelining knows that it increases throughput at the cost of latency. Anukari processes audio in real-time, so each kernel invocation needs access to the real-time audio input data (e.g. from the microphone). Thus Anukari can’t do something like speculative execution where it processes the next audio block early, because it wouldn’t have the input data required to do so.</p>
<h2>Why don’t you run the spin kernel in the same MTLCommandQueue as the physics kernel?</h2>
<p>This solution might fix the problem where the spin and physics kernels end up running on separate GPU chiplets (if that is indeed the issue).</p>
<p>I <em>have</em> tried this, actually. The reason it doesn’t work is again because Anukari is latency-sensitive. What happens is that sometimes the spin kernel runs a little too long and cuts into the time for running the physics kernel. I experimented with small spin kernels, using volatile unified memory to allow the CPU to write an “exit kernel early” flag. Even with these hijinx, sometimes the spin kernel cut into physics kernel time.</p>
<h2>Why not just make the GPU code more efficient?</h2>
<p>Because the simulation is ALU-bound, there’s not much performance to be gained from the typical optimization low-hanging fruit: improving memory access patterns. To speed up Anukari’s physics kernel, the only thing that really helps is optimizing arithmetic throughput. For example, Anukari uses FP16 math where possible to better-saturate Apple’s ALUs. Instructions have been reordered using micro-benchmarks. All physics state is in L1 memory. Loads are reordered for vectorization. The list goes on.</p>
<p>Furthermore, Anukari heavily exploits the fact that threads within Apple’s SIMD-groups (roughly) share an instruction pointer. Different physics objects have highly divergent code branch paths, so simulating two types of objects within a SIMD-group is slow due to the requirement for instruction masking while the top-level branch paths are executed serially. To avoid this, Anukari dynamically optimizes the memory layout of physics objects to minimize the number of object types executed within each SIMD-group. This optimization is <a href="https://anukari.com/blog/devlog/the-new-warp-alignment-optimizer">described here in great detail</a>, and is a massive performance win.</p>
<p>What I'm trying to say is that I have <em>bent over backwards</em> to squeeze every last drop of performance out of Apple's hardware. It's been fun to do!</p>
<p>There are further arithmetic optimizations to be done, but they will be fairly marginal. We’re talking single-digit percentage point speedups. Anukari’s GPU code is already VERY FAST. If you’re curious, there’s more info on Anukari’s optimizations <a href="https://anukari.com/blog/devlog/tags/optimization">here</a>.</p>
<h2>Why use the GPU at all?</h2>
<p>On powerful machines, Anukari can simulate 768 - 1024 physics objects. Each object can be arbitrarily connected to other objects, meaning that they influence one another. Each object has to be stepped forward in an implicit Euler integration at the audio sample rate, typically 48,000 samples per second. Each object has somewhere between 3 and 10 parameters that affect its behavior. Some of the behaviors involve expensive math, like vector rotation, exp(), log(), etc.</p>
<p>This is simply not even close to feasible on the CPU! Trust me, I tried. It’s not even a little bit close. The GPU just has a ridiculous number of ALUs, it gives me explicit control over the L1 cache layout, and the concurrency constructs like threadgroup_barrier allow the physics integration steps to be done massively in parallel without consistency issues, without expensive CPU mutexes.</p>
<p>I’ll say it again: Anukari does not exist without GPU processing.</p>
<h2>Why should Apple care about what Anukari needs?</h2>
<p>Maybe they shouldn’t, I don’t know. Anukari is a tiny startup. It’s a niche product. It’s doing something weird.</p>
<p>On the other hand, the people who like Anukari really like it. Mick Gordon, the composer for IMO the greatest DOOM games of all time, randomly showed up and blew everyone away with an <a href="https://x.com/Mick_Gordon/status/1918146487948919222">incredible demo using Anukari</a>. Anukari is receiving praise from people who really love synthesizers like <a href="https://cdm.link/anukari-physics-based-instrument/">CDM</a>. Comments have appeared on random internet threads that I didn't start, like, <em>"it's the most creative plugin I've tried in the last 10 years.</em></p>
<p>But mostly, Anukari is using Apple’s hardware in what I consider an incredibly cool way, allowing users to do something that they’ve never been able to do before. And Apple’s hardware is completely up to the task. But it just needs a little push in the right direction.</p>
<h2>Why don’t you use GPU Audio’s APIs?</h2>
<p>I don’t really want to include this last question, but it is here to address the fact that the CEO of GPU Audio, Alexander Talashov, likes to drive by threads about Anukari and suggest that if Anukari used his APIs it would solve our problems. I would be so happy if this were true.</p>
<p>Alexander is a great guy, and his product (<a href="https://www.gpu.audio/">GPU Audio</a>) is a great product. I’ve met him in person, and he’s incredibly passionate about making the GPU accessible for DSP. Audio folks should check it out, it’s really cool. I wish GPU Audio huge success, and support their product.</p>
<p>But… GPU Audio has nothing useful for Anukari. Fundamentally the problem is that Anukari is not anything like a traditional DSP application. It’s a numerical differential equation integrator, far more similar to a video game physics engine than a DSP application. Sure, there are bits and bobs of DSP inside the physics engine, for example Anukari’s mics in the physics world do have compression, and that’s done in-line with the physics calculations on the GPU. But the vast majority of computation is Eulerian integration.</p>
<p>It needs to be understood that I’m programming the GPU at the bare Metal layer (yes pun intended), and am taking advantage of a number of hardware features and ridiculous domain-specific optimizations that are quite necessary to make this work. <em>And all I need is for Apple to reliably turn up the clock rate on the GPU.</em></p>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Critical CSS (182 pts)]]></title>
            <link>https://critical-css-extractor.kigo.studio/</link>
            <guid>43901495</guid>
            <pubDate>Tue, 06 May 2025 03:13:08 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://critical-css-extractor.kigo.studio/">https://critical-css-extractor.kigo.studio/</a>, See on <a href="https://news.ycombinator.com/item?id=43901495">Hacker News</a></p>
&lt;Unparsable&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[Google has most of my email because it has all of yours (2014) (230 pts)]]></title>
            <link>https://mako.cc/copyrighteous/google-has-most-of-my-email-because-it-has-all-of-yours</link>
            <guid>43901204</guid>
            <pubDate>Tue, 06 May 2025 02:09:29 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://mako.cc/copyrighteous/google-has-most-of-my-email-because-it-has-all-of-yours">https://mako.cc/copyrighteous/google-has-most-of-my-email-because-it-has-all-of-yours</a>, See on <a href="https://news.ycombinator.com/item?id=43901204">Hacker News</a></p>
<div id="readability-page-1" class="page"><article id="post-2527">
		<!-- .entry-header -->

	
	<div>
		<p><em>Republished by <a href="http://www.slate.com/blogs/future_tense/2014/05/13/don_t_use_gmail_here_s_how_to_determine_how_many_of_your_emails_google_may.html">Slate</a>. Translations available in <a href="http://www.slate.fr/life/87045/gmail-google-emails-prive">French (Français)</a>, <a href="http://www.genbeta.com/web/no-usas-gmail-da-igual-google-tiene-muchos-de-tus-emails">Spanish (Español)</a>, <a href="http://www.labazhou.net/2014/05/google-has-most-of-my-email-because-it-has-all-of-yours/">Chinese (中文)</a></em></p>
<p>For almost 15 years, I have run my own email server which I use for all of my non-work correspondence. I do so to keep <a href="http://autonomo.us/">autonomy</a>, control, and privacy over my email and so that no big company has copies of all of my personal email.</p>
<p>A few years ago, I was surprised to find out that my friend <a href="https://www.eff.org/about/staff/peter-eckersley">Peter Eckersley</a> — a very privacy conscious person who is Technology Projects Director at the <a href="https://eff.org/">EFF</a> — used Gmail. I asked him why he would willingly give Google copies of all his email. Peter pointed out that if all of your friends use Gmail, Google has your email anyway. Any time I email somebody who uses Gmail — and anytime they email me — Google has that email.</p>
<p>Since our conversation, I have often wondered just how much of my email Google really has. This weekend, I wrote a small program to go through all the email I have kept in my personal inbox since April 2004 (when Gmail was started) to find out.</p>
<p>One challenge with answering the question is that many people, like Peter, use Gmail to read, compose, and send email but they configure Gmail to send email from a non-<tt>gmail.com</tt> “From” address. To catch these, my program looks through each message’s headers that record which computers handled the message on its way to my server and to pick out messages that have traveled through <tt>google.com</tt>, <tt>gmail.com</tt>, or <tt>googlemail.com</tt>. Although I usually filter them, my personal mailbox contains emails sent through a number of mailing lists. Since these mailing lists often “hide” the true provenance of a message, I exclude all messages that are marked as coming from lists using the (usually invisible) “Precedence” header.</p>
<p>The following graph shows the numbers of emails in my personal inbox each week in red and the subset from Google in blue. Because the number of emails I receive week-to-week tends to vary quite a bit, I’ve included a <a href="https://en.wikipedia.org/wiki/LOESS">LOESS</a> “smoother” which shows a moving average over several weeks.</p>
<p><a href="https://mako.cc/copyrighteous/wp-content/uploads/2014/05/emails_gmail_over_time.png"><img fetchpriority="high" decoding="async" src="https://mako.cc/copyrighteous/wp-content/uploads/2014/05/emails_gmail_over_time.png" alt="Emails, total and from GMail, over time" width="720" height="432"></a>From eyeballing the graph, the answer to seems to be that, although it varies, about a third of the email in my inbox comes from Google!</p>
<p>Keep in mind that this is all of my personal email and includes automatic and computer generated mail from banks and retailers, etc. Although it is true that Google doesn’t have these messages, it suggests that the proportion of my truly “personal” email that comes via Google is probably much higher.</p>
<p>I would also like to know how much of the email I send goes <em>to</em> Google. I can do this by looking at emails in my inbox that I have replied to. This works if I am willing to assume that if I reply to an email sent from Google, it ends up back at Google. In some ways, doing this addresses the problem with the emails from retailers and banks since I am very unlikely to reply to those emails. In this sense, it also reflects a measure of more truly personal email.</p>
<p>I’ve broken down the proportions of emails I received that come from Google in the graph below for all email (top) and for emails I have replied to (bottom). In the graphs, the size of the dots represents the total number of emails counted to make that proportion. Once again, I’ve included the LOESS moving average.</p>
<p><a href="https://mako.cc/copyrighteous/wp-content/uploads/2014/05/emails_gmail_prop_over_time.png"><img decoding="async" src="https://mako.cc/copyrighteous/wp-content/uploads/2014/05/emails_gmail_prop_over_time.png" alt="Proportion of emails from GMail over time" width="720" height="576"></a>The answer is surprisingly large. Despite the fact that I spend hundreds of dollars a year and hours of work to host my own email server, Google has about half of my personal email! Last year, Google delivered 57% of the emails in my inbox that I replied to. They have delivered more than a third of all the email I’ve replied to every year since 2006 and more than half since 2010. On the upside, there is some indication that the proportion is going down. So far this year, only 51% of the emails I’ve replied to arrived from Google.</p>
<p>The numbers are higher than I imagined and reflect somewhat depressing news. They show how it’s complicated to think about privacy and autonomy for communication between parties. I’m not sure what to do except encourage others to consider, in the wake of the Snowden revelations and everything else, whether you really want Google to have all your email. And half of mine.</p>
<p>If you want to run the analysis on your own, you’re welcome to <a href="http://projects.mako.cc/source/?p=gmail-maildir-counter">the Python and R code I used to produce the numbers and graphs</a>.</p>
	</div><!-- .entry-content -->

	 <!-- .entry-footer -->
</article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[OpenAI reaches agreement to buy Windsurf for $3B (178 pts)]]></title>
            <link>https://www.bloomberg.com/news/articles/2025-05-06/openai-reaches-agreement-to-buy-startup-windsurf-for-3-billion</link>
            <guid>43900877</guid>
            <pubDate>Tue, 06 May 2025 00:57:48 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.bloomberg.com/news/articles/2025-05-06/openai-reaches-agreement-to-buy-startup-windsurf-for-3-billion">https://www.bloomberg.com/news/articles/2025-05-06/openai-reaches-agreement-to-buy-startup-windsurf-for-3-billion</a>, See on <a href="https://news.ycombinator.com/item?id=43900877">Hacker News</a></p>
Couldn't get https://www.bloomberg.com/news/articles/2025-05-06/openai-reaches-agreement-to-buy-startup-windsurf-for-3-billion: Error: Request failed with status code 403]]></description>
        </item>
        <item>
            <title><![CDATA[Analyzing Modern Nvidia GPU Cores (163 pts)]]></title>
            <link>https://arxiv.org/abs/2503.20481</link>
            <guid>43900463</guid>
            <pubDate>Mon, 05 May 2025 23:38:56 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://arxiv.org/abs/2503.20481">https://arxiv.org/abs/2503.20481</a>, See on <a href="https://news.ycombinator.com/item?id=43900463">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="content-inner">
    
    
                
    <p><a href="https://arxiv.org/pdf/2503.20481">View PDF</a></p><blockquote>
            <span>Abstract:</span>GPUs are the most popular platform for accelerating HPC workloads, such as artificial intelligence and science simulations. However, most microarchitectural research in academia relies on GPU core pipeline designs based on architectures that are more than 15 years old.
<br>This paper reverse engineers modern NVIDIA GPU cores, unveiling many key aspects of its design and explaining how GPUs leverage hardware-compiler techniques where the compiler guides hardware during execution. In particular, it reveals how the issue logic works including the policy of the issue scheduler, the structure of the register file and its associated cache, and multiple features of the memory pipeline. Moreover, it analyses how a simple instruction prefetcher based on a stream buffer fits well with modern NVIDIA GPUs and is likely to be used. Furthermore, we investigate the impact of the register file cache and the number of register file read ports on both simulation accuracy and performance.
<br>By modeling all these new discovered microarchitectural details, we achieve 18.24% lower mean absolute percentage error (MAPE) in execution cycles than previous state-of-the-art simulators, resulting in an average of 13.98% MAPE with respect to real hardware (NVIDIA RTX A6000). Also, we demonstrate that this new model stands for other NVIDIA architectures, such as Turing. Finally, we show that the software-based dependence management mechanism included in modern NVIDIA GPUs outperforms a hardware mechanism based on scoreboards in terms of performance and area.
    </blockquote>

    <!--CONTEXT-->
    
  </div><div>
      <h2>Submission history</h2><p> From: Rodrigo Huerta [<a href="https://arxiv.org/show-email/03496778/2503.20481" rel="nofollow">view email</a>]      <br>    <strong>[v1]</strong>
        Wed, 26 Mar 2025 12:10:53 UTC (246 KB)<br>
</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Pixels in Islamic art: square Kufic calligraphy (142 pts)]]></title>
            <link>https://uwithumlaut.wordpress.com/2020/07/24/pixels-in-islamic-art-square-kufic-calligraphy/</link>
            <guid>43899251</guid>
            <pubDate>Mon, 05 May 2025 20:42:03 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://uwithumlaut.wordpress.com/2020/07/24/pixels-in-islamic-art-square-kufic-calligraphy/">https://uwithumlaut.wordpress.com/2020/07/24/pixels-in-islamic-art-square-kufic-calligraphy/</a>, See on <a href="https://news.ycombinator.com/item?id=43899251">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
		
<p>When I was a little kid whenever we drove by mosques, I would be intrigued by the complex motifs they’re decorated by. I always tried to figure out the pattern; to me it was just a pattern, I never thought it can be writing because they didn’t look like letters from any alphabet I knew at the time. I remember thinking that I had finally found a full pattern that was only to be broken in the next square. As I grew older I learned that these pieces that resemble a labyrinth, at first sight, have so much more to offer than just lush visuals. Verses from the Quran were taken and turned into striking artwork carrying valuable messages. The more I learned about Islamic calligraphy and especially square Kufic, the more interesting it became and I will be delighted to share this fascination with my readers.</p>



<p>The importance of calligraphy in Islamic culture is indisputable. Unlike in Christian culture, the visual depiction of verses from the Holy Book is not used in decoration. Calligraphy shows itself reciting the Quran with beautiful writing decorating everything from mosques to plates to clothes to carpets. Islamic Calligraphy derives from two main styles; Naskh and Kufic. The peculiarity of Kufic Calligraphy is the straight and structured lettering. The Kufic style in itself comes in a variety of styles such as floriated, square, knotted, new style…  The patterns that appear complex and random comes with strict rules and systematics. This form of Islamic Art is gaining popularity again due to its moderln and graphic look and resurfacing the traditional alluring patterns.</p>



<figure><img data-attachment-id="388" data-permalink="https://uwithumlaut.wordpress.com/kufic_mjriam_nach_fabris/" data-orig-file="https://uwithumlaut.wordpress.com/wp-content/uploads/2020/07/kufic_mjriam_nach_fabris.jpg" data-orig-size="1312,1322" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;1&quot;}" data-image-title="kufic_mjriam_nach_fabris" data-image-description="" data-image-caption="" data-medium-file="https://uwithumlaut.wordpress.com/wp-content/uploads/2020/07/kufic_mjriam_nach_fabris.jpg?w=298" data-large-file="https://uwithumlaut.wordpress.com/wp-content/uploads/2020/07/kufic_mjriam_nach_fabris.jpg?w=750" width="1016" height="1023" src="https://uwithumlaut.wordpress.com/wp-content/uploads/2020/07/kufic_mjriam_nach_fabris.jpg?w=1016" alt="" srcset="https://uwithumlaut.wordpress.com/wp-content/uploads/2020/07/kufic_mjriam_nach_fabris.jpg?w=1016 1016w, https://uwithumlaut.wordpress.com/wp-content/uploads/2020/07/kufic_mjriam_nach_fabris.jpg?w=150 150w, https://uwithumlaut.wordpress.com/wp-content/uploads/2020/07/kufic_mjriam_nach_fabris.jpg?w=298 298w, https://uwithumlaut.wordpress.com/wp-content/uploads/2020/07/kufic_mjriam_nach_fabris.jpg?w=768 768w, https://uwithumlaut.wordpress.com/wp-content/uploads/2020/07/kufic_mjriam_nach_fabris.jpg 1312w" sizes="(max-width: 1016px) 100vw, 1016px"><figcaption>Work done by Sergio Fabris found on <a href="https://commons.wikimedia.org/wiki/File:Kufic_mjriam_nach_Fabris.jpg">https://commons.wikimedia.org/wiki/File:Kufic_mjriam_nach_Fabris.jpg</a> (no changes were made)              This image is licensed under the&nbsp;<a href="https://en.wikipedia.org/wiki/en:Creative_Commons">Creative Commons</a>&nbsp;<a href="https://creativecommons.org/licenses/by-sa/4.0/deed.en">Attribution-Share Alike 4.0 International</a>&nbsp;license.</figcaption></figure>



<p>Square Kufic dates all the way back to the 12th-13th century. Sometimes it is also referred to as banna-i which is a Farsi word and it is mostly used to refer to the architectural square Kufic. There are two main theories about how Square Kufic came into being. The first one suggests a fusion between the Arabic script and the Chinese seal script; the Arabic script was fitted into the forms of Chinese scripts and created square Kufic. The second theory argues that it came from architectural adaptations of Arabic script. The second theory seems to be more accreditable and I saw some articles mentioning this assumption as well. This is further supported by the first examples of square Kufic. These were seen in architecture and was done using bricks packed next to one another. The earliest example is from Ghazni (modern-day Afghanistan) in Sultan Meshud Tower also known as the Victory Towers or Masud III Tower.</p>



<div><figure><img data-attachment-id="386" data-permalink="https://uwithumlaut.wordpress.com/513px-shahada-svg_/" data-orig-file="https://uwithumlaut.wordpress.com/wp-content/uploads/2020/07/513px-shahada.svg_.png" data-orig-size="513,288" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="513px-shahada.svg_" data-image-description="" data-image-caption="" data-medium-file="https://uwithumlaut.wordpress.com/wp-content/uploads/2020/07/513px-shahada.svg_.png?w=300" data-large-file="https://uwithumlaut.wordpress.com/wp-content/uploads/2020/07/513px-shahada.svg_.png?w=513" width="513" height="288" src="https://uwithumlaut.wordpress.com/wp-content/uploads/2020/07/513px-shahada.svg_.png?w=513" alt="" srcset="https://uwithumlaut.wordpress.com/wp-content/uploads/2020/07/513px-shahada.svg_.png 513w, https://uwithumlaut.wordpress.com/wp-content/uploads/2020/07/513px-shahada.svg_.png?w=150 150w, https://uwithumlaut.wordpress.com/wp-content/uploads/2020/07/513px-shahada.svg_.png?w=300 300w" sizes="(max-width: 513px) 100vw, 513px"><figcaption>Shada written in square Kufic style, shaped like buildings</figcaption></figure></div>



<p>As I have mentioned above, Kufic calligraphy has strict rules but in square Kufic, these rules are often bent, and even broken. The letters and the overall script is skewed to fit it into shape. This is why sometimes it can be very hard to decipher what was written in the text. I think the fact that it is not always done to pass on the message openly excites me too. Having a somewhat hidden message in the image makes things a lot more amusing. The only rule that is always followed is keeping the filled out and empty spaces even especially when it comes to thickess. Although this is the rule, the artist might knowingly leave out spaces to create a pattern within the pattern. Typically, the diacritical dots are not used when writing down the script in this form but there are examples of square Kufic using diacritical dots as well. Sometimes dots are used not for diacritical purposes but as filler dots.&nbsp;</p>



<p>The technicalities can get a little bit confusing but I’ll do my best to simplify them and keep it short. The script is written commonly clockwise but it can be anticlockwise or a zigzag pattern. Furthermore, the direction of the writing can change midway to make it fit into the shape. Not only the direction of writing but even the letters and their forms are played with to fill up space evenly. Although the name is Square Kufic, there are many shapes the script can be fitted into. Most commonly it is rectangular shapes but it can be even other words that the Kufic is written into. Once a verse or a single word is written in Square Kufic form, it can be repeated to create a pattern. Sometimes the initial piece is rotated in various ways to create different patterns. This is why I think it is sort of like a pixel, it is the one of many which an image, pattern is created. I envisage that the flexibility it has in comparison to other forms of Kufic makes it easier to adapt to the aesthetic likes we have today. There are two modern-day square Kufic artists that I very much enjoy the work of. These are Kamal Boullata and Ahmed Moustafa. I wasn’t sure if I can include designs of these artists due to copyrights but I most definitely recommend a quick Google search. Ahmed Moustafa can combine different styles of Calligraphy creating multi-layered masterpieces. Kamal Boulata, a Palestinian Christian combines the traditional styles with modern movements combined with impeccable color harmonizations. He uses not only verses from the Quran but also the Gospel of Saint John.</p>



<div><figure><img data-attachment-id="391" data-permalink="https://uwithumlaut.wordpress.com/topkapi_scroll_p308-1/" data-orig-file="https://uwithumlaut.wordpress.com/wp-content/uploads/2020/07/topkapi_scroll_p308-1.jpg" data-orig-size="553,677" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="topkapi_scroll_p308-1" data-image-description="" data-image-caption="" data-medium-file="https://uwithumlaut.wordpress.com/wp-content/uploads/2020/07/topkapi_scroll_p308-1.jpg?w=245" data-large-file="https://uwithumlaut.wordpress.com/wp-content/uploads/2020/07/topkapi_scroll_p308-1.jpg?w=553" loading="lazy" width="553" height="677" src="https://uwithumlaut.wordpress.com/wp-content/uploads/2020/07/topkapi_scroll_p308-1.jpg?w=553" alt="" srcset="https://uwithumlaut.wordpress.com/wp-content/uploads/2020/07/topkapi_scroll_p308-1.jpg 553w, https://uwithumlaut.wordpress.com/wp-content/uploads/2020/07/topkapi_scroll_p308-1.jpg?w=123 123w, https://uwithumlaut.wordpress.com/wp-content/uploads/2020/07/topkapi_scroll_p308-1.jpg?w=245 245w" sizes="(max-width: 553px) 100vw, 553px"><figcaption>A pattern done by using square Kufic, from the Topkapı Scroll</figcaption></figure></div>



<p>Taking a step back to the past, one specific example of square Kufic I want to look at in more detail is located in Topkapı Palace, about 30 minutes away from where I am writing this article. The examples on this scroll are some of the oldest of its kind that survived to this day. It is not known exactly when, where, or by whom it was made, although we do know that it was done by a single person. It is assumed that it was done either 15th or 16th century in Iran but not known where exactly in Iran (there is different evidence pointing to different cities). The scroll is impressive in size and design. It is almost 30 meters long which is almost 100 feet. It is actually two different parchments that were combined into one scroll. The scroll shows the different geometrical shapes done in the Timurid architectural style. It is made as a guide for architectural designs and these include square Kufic as well. I can’t help but think how much one had to master different styles of design and calligraphy to be able to prepare a guide for it. Although I have been to the Topkapı Palace multiple times, I don’t remember seeing this scroll (well, the Palace is enormous) but I am very excited to go and examine it in detail.</p>



<figure><img data-attachment-id="383" data-permalink="https://uwithumlaut.wordpress.com/topkapc4b1-scroll/" data-orig-file="https://uwithumlaut.wordpress.com/wp-content/uploads/2020/07/topkapc4b1-scroll.png" data-orig-size="1920,1080" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="topkapc4b1-scroll" data-image-description="" data-image-caption="" data-medium-file="https://uwithumlaut.wordpress.com/wp-content/uploads/2020/07/topkapc4b1-scroll.png?w=300" data-large-file="https://uwithumlaut.wordpress.com/wp-content/uploads/2020/07/topkapc4b1-scroll.png?w=750" loading="lazy" src="https://uwithumlaut.wordpress.com/wp-content/uploads/2020/07/topkapc4b1-scroll.png?w=1024" alt="" width="780" height="438" srcset="https://uwithumlaut.wordpress.com/wp-content/uploads/2020/07/topkapc4b1-scroll.png?w=1024 1024w, https://uwithumlaut.wordpress.com/wp-content/uploads/2020/07/topkapc4b1-scroll.png?w=780 780w, https://uwithumlaut.wordpress.com/wp-content/uploads/2020/07/topkapc4b1-scroll.png?w=1557 1557w, https://uwithumlaut.wordpress.com/wp-content/uploads/2020/07/topkapc4b1-scroll.png?w=150 150w, https://uwithumlaut.wordpress.com/wp-content/uploads/2020/07/topkapc4b1-scroll.png?w=300 300w, https://uwithumlaut.wordpress.com/wp-content/uploads/2020/07/topkapc4b1-scroll.png?w=768 768w" sizes="(max-width: 780px) 100vw, 780px"></figure>



<p>Valérie González, an expert on Islamic visual culture, mentions “Sometimes calligraphy is so manipulated visually that the legibility is very difficult” in<a href="https://www.youtube.com/watch?v=f5aDyxoTr6g"> a lecture she gave in Casa Árabe</a>. Probably the eyes of the little girl I used to be were interested in this elaborate style because it was not meant to be understood right away. This form of art has so many layers to be discovered and every layer is even more valuable than the previous one. At first, the eyes are blessed with the intricate design. Then the mind tries to decipher the pattern and the words. Lastly, the meaning and the message makes it more memorable and leaves you with something to think over. Unfortunately, I am not able to decipher these on my own and I need a little bit of help to go deeper than the first layer. Nevertheless, my &nbsp;fascination exceeds what my mind comprehends.</p>







<p>For anyone who wants to learn more about calligraphy in general here is a documentary I very much enjoyed: <a href="https://www.youtube.com/watch?v=v9uuNagb4po">https://www.youtube.com/watch?v=v9uuNagb4po</a></p>



<p>Some of the useful technical information I found was on two sources;&nbsp;</p>



<p>First one is kufic.info, it is a website explaining many aspects of Square Kufic</p>



<p>The second one is the article here; <a href="https://design.tutsplus.com/tutorials/creative-arabic-calligraphy-square-kufic--cms-23012">https://design.tutsplus.com/tutorials/creative-arabic-calligraphy-square-kufic–cms-23012</a></p>



<p>If you speak Turkish, I found the article “Kufi Yazıda Geometrik Yorumlar Üzerine Bir Deneme” by Omiir Bakirer very helpful. It can be found on dergipark.</p>
	</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Replacing Kubernetes with systemd (2024) (382 pts)]]></title>
            <link>https://blog.yaakov.online/replacing-kubernetes-with-systemd/</link>
            <guid>43899236</guid>
            <pubDate>Mon, 05 May 2025 20:40:14 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://blog.yaakov.online/replacing-kubernetes-with-systemd/">https://blog.yaakov.online/replacing-kubernetes-with-systemd/</a>, See on <a href="https://news.ycombinator.com/item?id=43899236">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
                            <p>Yes, I'm fully aware those are two separate things, but hear me out here for a moment.</p><p>Back in 2018 I was hearing a lot of stuff from all angles and all sorts of friends and influences about Kubernetes, and from what I heard it seemed like a pretty promising piece of kit to use. At the time, I actually went out and bought a NUC to act as a little hypervisor so that I could play around with a small cluster at home.</p><p>Funnily enough, my blog post on this was <a href="https://blog.yaakov.online/learning-kubernetes-at-home/" rel="noreferrer">six years ago to the very day</a>.</p><p>The main lesson that I learned is that although Kubernetes is made up of all sorts of pieces and web services and sidecars and webhooks, basically acts as a giant <code>while</code> loop as follows:</p><pre><code>while (true)
{
    var current_state = GetCurrentState();
    var desired_state = GetDesiredState();
    var diff = CalculateDiff(current_state, desired_state);
    ApplyDiff(diff);
}</code></pre><p>If I said there should be a Pod here, and there wasn't, Kubernetes would create it. If I said there should be 3 replicas and there were 4, Kubernetes would get rid of one.</p><p>This actually extended out in really cool ways, such as with <a href="https://cert-manager.io/?ref=blog.yaakov.online" rel="noreferrer">cert-manager</a>. If I said there should be a valid TLS certificate for some domain, and told Kubernetes how it could request one, then if the certificate was missing or expiring, Kubernetes would go out and get a new certificate and install it in the web server automagically.</p><p>But as the memes go, what I was using Kubernetes for was fun to experiment with, but total overkill.</p><figure><img src="https://blog.yaakov.online/content/images/2024/02/C-NknkeUwAAxSQs.jpeg" alt="A few small pieces of wood tied down to a large flatbed truck." loading="lazy" width="2000" height="1984" srcset="https://blog.yaakov.online/content/images/size/w600/2024/02/C-NknkeUwAAxSQs.jpeg 600w, https://blog.yaakov.online/content/images/size/w1000/2024/02/C-NknkeUwAAxSQs.jpeg 1000w, https://blog.yaakov.online/content/images/size/w1600/2024/02/C-NknkeUwAAxSQs.jpeg 1600w, https://blog.yaakov.online/content/images/2024/02/C-NknkeUwAAxSQs.jpeg 2048w" sizes="(min-width: 720px) 720px"><figcaption><span>"Deployed my blog on Kubernetes." - @dexhorthy, </span><a href="https://twitter.com/dexhorthy/status/856639005462417409?lang=en&amp;ref=blog.yaakov.online"><span>https://twitter.com/dexhorthy/status/856639005462417409?lang=en</span></a></figcaption></figure><p>Whilst most problems I encountered did provide a legitimate learning experience, it turns out that Kubernetes, particularly on a NUC, is not bedroom-friendly. Kubernetes chews through a lot of resources, and <code>while (true)</code> loops tend to chew through a lot of CPU. This made my computers run constantly, run hot, keep the fan running, and made it hotter and noisier and harder to sleep.</p><p>Even in the cloud, this effect gets felt in different ways. My personal experience on Azure Kubernetes Service was that I immediately lose a massive chunk of RAM to their Kubernetes implementation, and it uses about 7-10% idle CPU on worker nodes. Even with single-instance <a href="https://microk8s.io/?ref=blog.yaakov.online" rel="noreferrer">Microk8s</a> on a small VPS I had an idle CPU load hovering around 12% on a 2x vCPU x86_64 box, and <a href="http://k3s.io/?ref=blog.yaakov.online" rel="noreferrer">K3S</a> which is supposed to be leaner is at about 6% constant CPU consumption on a 2x vCPU Ampere A1 machine.</p><p>(No guesses as to which cloud provider that latter one is running on.)</p><p>I even tried running Kubernetes on a Raspberry Pi but I couldn't actually find an implementation that would happily run without kicking up heat/fans and that would actually leave enough CPU behind for my workloads.</p><p>Yet the thing that kept bringing me back was the automation. Particularly with GitOps and <a href="https://www.weave.works/oss/flux/?ref=blog.yaakov.online" rel="noreferrer">Flux</a>, making changes was a breeze. With the container image automation and recent addition of <a href="https://fluxcd.io/flux/guides/webhook-receivers/?ref=blog.yaakov.online" rel="noreferrer">webhooks in Flux v2</a>, all I had to do was push a new container image and within seconds my servers had pulled the new container image and were running the new version of the application in production.</p><p>Every so often I would stick my head back out of Kubernetes and look at the rest of the world and see if there is something else that can do the same container automation and just like Noah's dove, I would come back empty-handed.</p><p>The only solutions I could find were "just recreate the whole container with all of the original command line arguments" as though I have the patience to manage that or remember each flag, or "here is some magic goop that works if you give it full control of <code>docker.sock</code>" which I never really liked the idea of.</p><p>I have even been sorely tempted to build my own thing but surely, <em>surely</em>, there is something out there already that does this, right?</p><p>Well, recently, I came across <a href="https://docs.podman.io/en/latest/markdown/podman-auto-update.1.html?ref=blog.yaakov.online" rel="noreferrer">Podman auto-updating</a>. The simplest way to explain <a href="https://podman.io/?ref=blog.yaakov.online" rel="noreferrer">Podman</a> is an alternative Docker CLI (yes I know that is oversimplified), but it has one particular feature that caught my eye.</p><p>Once you create a container, Podman can automatically <a href="https://docs.podman.io/en/latest/markdown/podman-generate-systemd.1.html?ref=blog.yaakov.online" rel="noreferrer">generate a systemd service</a> file to start and stop that container. Starting the service creates (or replaces) the container, and stopping the service removes the container. So that already takes care of my "manage each original flag" problem.</p><p>But the cherry on top if that if you tag your containers with <code>io.containers.autoupdate</code>, then once a day on a timer or on-demand when you <code>podman auto-update</code>, it will check for a new image and if there is one it will recreate the container for you with the new image.</p><p><a href="https://fedoramagazine.org/auto-updating-podman-containers-with-systemd/?ref=blog.yaakov.online" rel="noreferrer">This article from Fedora Magazine</a> basically gave me 99% of the magic sauce. There were only two more components I needed to make this work:</p><ol><li>Run <code>systemctl --user enable mycontainer.service</code> to make the container start up automatically, whenever I log in.</li><li>Run <code>loginctl enable-linger</code> so that I "log in" when the server starts up.</li></ol><p>These three components - Podman, systemd, and user lingering, now give me 99% of the benefit I was getting from Kubernetes with vastly reduced complexity and none of the CPU/memory hits associated with it.</p><p>I've migrated a full set of services from one VPS to a new one with half the vCPUs and RAM. It's only been running for a handful of hours so far, but I can see that it is running significantly lighter, snappier, and with a lower compute cost to boot, which gives me higher service density and more bang for my buck.</p><p>Of course, as my luck would have it, Podman integration with systemd appears to be deprecated already and they're now talking about defining containers in "Quadlet" files, whatever those are. I guess that will be something to learn some other time.</p>
                        </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Show HN: Real-time AI Voice Chat at ~500ms Latency (464 pts)]]></title>
            <link>https://github.com/KoljaB/RealtimeVoiceChat</link>
            <guid>43899028</guid>
            <pubDate>Mon, 05 May 2025 20:17:32 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/KoljaB/RealtimeVoiceChat">https://github.com/KoljaB/RealtimeVoiceChat</a>, See on <a href="https://news.ycombinator.com/item?id=43899028">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><p dir="auto"><h2 tabindex="-1" dir="auto">Real-Time AI Voice Chat 🎤💬🧠🔊</h2><a id="user-content-real-time-ai-voice-chat-" aria-label="Permalink: Real-Time AI Voice Chat 🎤💬🧠🔊" href="#real-time-ai-voice-chat-"></a></p>
<p dir="auto"><strong>Have a natural, spoken conversation with an AI!</strong></p>
<p dir="auto">This project lets you chat with a Large Language Model (LLM) using just your voice, receiving spoken responses in near real-time. Think of it as your own digital conversation partner.</p>
<details open="">
  <summary>
    
    <span aria-label="Video description FastVoiceTalk_compressed_step3_h264.mp4">FastVoiceTalk_compressed_step3_h264.mp4</span>
    <span></span>
  </summary>

  <video src="https://private-user-images.githubusercontent.com/7604638/440153612-16cc29a7-bec2-4dd0-a056-d213db798d8f.mp4?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3NDY0ODQ1MDIsIm5iZiI6MTc0NjQ4NDIwMiwicGF0aCI6Ii83NjA0NjM4LzQ0MDE1MzYxMi0xNmNjMjlhNy1iZWMyLTRkZDAtYTA1Ni1kMjEzZGI3OThkOGYubXA0P1gtQW16LUFsZ29yaXRobT1BV1M0LUhNQUMtU0hBMjU2JlgtQW16LUNyZWRlbnRpYWw9QUtJQVZDT0RZTFNBNTNQUUs0WkElMkYyMDI1MDUwNSUyRnVzLWVhc3QtMSUyRnMzJTJGYXdzNF9yZXF1ZXN0JlgtQW16LURhdGU9MjAyNTA1MDVUMjIzMDAyWiZYLUFtei1FeHBpcmVzPTMwMCZYLUFtei1TaWduYXR1cmU9MTY0YWVlN2NiNDc3ZjZkMDMzZGJlY2QyNGNlZTQzNDM0M2I1ZjJmM2EwOTgxYzVmMzZlYjU2MjdhNzIzNTRlNSZYLUFtei1TaWduZWRIZWFkZXJzPWhvc3QifQ.dbPAgkPJn2NKy48qo_JdokryT9DGts4KHQ767eBEWGs" data-canonical-src="https://private-user-images.githubusercontent.com/7604638/440153612-16cc29a7-bec2-4dd0-a056-d213db798d8f.mp4?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3NDY0ODQ1MDIsIm5iZiI6MTc0NjQ4NDIwMiwicGF0aCI6Ii83NjA0NjM4LzQ0MDE1MzYxMi0xNmNjMjlhNy1iZWMyLTRkZDAtYTA1Ni1kMjEzZGI3OThkOGYubXA0P1gtQW16LUFsZ29yaXRobT1BV1M0LUhNQUMtU0hBMjU2JlgtQW16LUNyZWRlbnRpYWw9QUtJQVZDT0RZTFNBNTNQUUs0WkElMkYyMDI1MDUwNSUyRnVzLWVhc3QtMSUyRnMzJTJGYXdzNF9yZXF1ZXN0JlgtQW16LURhdGU9MjAyNTA1MDVUMjIzMDAyWiZYLUFtei1FeHBpcmVzPTMwMCZYLUFtei1TaWduYXR1cmU9MTY0YWVlN2NiNDc3ZjZkMDMzZGJlY2QyNGNlZTQzNDM0M2I1ZjJmM2EwOTgxYzVmMzZlYjU2MjdhNzIzNTRlNSZYLUFtei1TaWduZWRIZWFkZXJzPWhvc3QifQ.dbPAgkPJn2NKy48qo_JdokryT9DGts4KHQ767eBEWGs" controls="controls" muted="muted">

  </video>
</details>

<p dir="auto"><em>(early preview - first reasonably stable version)</em></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">What's Under the Hood?</h2><a id="user-content-whats-under-the-hood" aria-label="Permalink: What's Under the Hood?" href="#whats-under-the-hood"></a></p>
<p dir="auto">A sophisticated client-server system built for low-latency interaction:</p>
<ol dir="auto">
<li>🎙️ <strong>Capture:</strong> Your voice is captured by your browser.</li>
<li>➡️ <strong>Stream:</strong> Audio chunks are whisked away via WebSockets to a Python backend.</li>
<li>✍️ <strong>Transcribe:</strong> <code>RealtimeSTT</code> rapidly converts your speech to text.</li>
<li>🤔 <strong>Think:</strong> The text is sent to an LLM (like Ollama or OpenAI) for processing.</li>
<li>🗣️ <strong>Synthesize:</strong> The AI's text response is turned back into speech using <code>RealtimeTTS</code>.</li>
<li>⬅️ <strong>Return:</strong> The generated audio is streamed back to your browser for playback.</li>
<li>🔄 <strong>Interrupt:</strong> Jump in anytime! The system handles interruptions gracefully.</li>
</ol>
<p dir="auto"><h2 tabindex="-1" dir="auto">Key Features ✨</h2><a id="user-content-key-features-" aria-label="Permalink: Key Features ✨" href="#key-features-"></a></p>
<ul dir="auto">
<li><strong>Fluid Conversation:</strong> Speak and listen, just like a real chat.</li>
<li><strong>Real-Time Feedback:</strong> See partial transcriptions and AI responses as they happen.</li>
<li><strong>Low Latency Focus:</strong> Optimized architecture using audio chunk streaming.</li>
<li><strong>Smart Turn-Taking:</strong> Dynamic silence detection (<code>turndetect.py</code>) adapts to the conversation pace.</li>
<li><strong>Flexible AI Brains:</strong> Pluggable LLM backends (Ollama default, OpenAI support via <code>llm_module.py</code>).</li>
<li><strong>Customizable Voices:</strong> Choose from different Text-to-Speech engines (Kokoro, Coqui, Orpheus via <code>audio_module.py</code>).</li>
<li><strong>Web Interface:</strong> Clean and simple UI using Vanilla JS and the Web Audio API.</li>
<li><strong>Dockerized Deployment:</strong> Recommended setup using Docker Compose for easier dependency management.</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Technology Stack 🛠️</h2><a id="user-content-technology-stack-️" aria-label="Permalink: Technology Stack 🛠️" href="#technology-stack-️"></a></p>
<ul dir="auto">
<li><strong>Backend:</strong> Python 3.x, FastAPI</li>
<li><strong>Frontend:</strong> HTML, CSS, JavaScript (Vanilla JS, Web Audio API, AudioWorklets)</li>
<li><strong>Communication:</strong> WebSockets</li>
<li><strong>Containerization:</strong> Docker, Docker Compose</li>
<li><strong>Core AI/ML Libraries:</strong>
<ul dir="auto">
<li><code>RealtimeSTT</code> (Speech-to-Text)</li>
<li><code>RealtimeTTS</code> (Text-to-Speech)</li>
<li><code>transformers</code> (Turn detection, Tokenization)</li>
<li><code>torch</code> / <code>torchaudio</code> (ML Framework)</li>
<li><code>ollama</code> / <code>openai</code> (LLM Clients)</li>
</ul>
</li>
<li><strong>Audio Processing:</strong> <code>numpy</code>, <code>scipy</code></li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Before You Dive In: Prerequisites 🏊‍♀️</h2><a id="user-content-before-you-dive-in-prerequisites-️" aria-label="Permalink: Before You Dive In: Prerequisites 🏊‍♀️" href="#before-you-dive-in-prerequisites-️"></a></p>
<p dir="auto">This project leverages powerful AI models, which have some requirements:</p>
<ul dir="auto">
<li><strong>Operating System:</strong>
<ul dir="auto">
<li><strong>Docker:</strong> Linux is recommended for the best GPU integration with Docker.</li>
<li><strong>Manual:</strong> The provided script (<code>install.bat</code>) is for Windows. Manual steps are possible on Linux/macOS but may require more troubleshooting (especially for DeepSpeed).</li>
</ul>
</li>
<li><strong>🐍 Python:</strong> 3.9 or higher (if setting up manually).</li>
<li><strong>🚀 GPU:</strong> <strong>A powerful CUDA-enabled NVIDIA GPU is <em>highly recommended</em></strong>, especially for faster STT (Whisper) and TTS (Coqui). Performance on CPU-only or weaker GPUs will be significantly slower.
<ul dir="auto">
<li>The setup assumes <strong>CUDA 12.1</strong>. Adjust PyTorch installation if you have a different CUDA version.</li>
<li><strong>Docker (Linux):</strong> Requires <a href="https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/latest/install-guide.html" rel="nofollow">NVIDIA Container Toolkit</a>.</li>
</ul>
</li>
<li><strong>🐳 Docker (Optional but Recommended):</strong> Docker Engine and Docker Compose v2+ for the containerized setup.</li>
<li><strong>🧠 Ollama (Optional):</strong> If using the Ollama backend <em>without</em> Docker, install it separately and pull your desired models. The Docker setup includes an Ollama service.</li>
<li><strong>🔑 OpenAI API Key (Optional):</strong> If using the OpenAI backend, set the <code>OPENAI_API_KEY</code> environment variable (e.g., in a <code>.env</code> file or passed to Docker).</li>
</ul>
<hr>
<p dir="auto"><h2 tabindex="-1" dir="auto">Getting Started: Installation &amp; Setup ⚙️</h2><a id="user-content-getting-started-installation--setup-️" aria-label="Permalink: Getting Started: Installation &amp; Setup ⚙️" href="#getting-started-installation--setup-️"></a></p>
<p dir="auto"><strong>Clone the repository first:</strong></p>
<div dir="auto" data-snippet-clipboard-copy-content="git clone https://github.com/KoljaB/RealtimeVoiceChat.git
cd RealtimeVoiceChat"><pre>git clone https://github.com/KoljaB/RealtimeVoiceChat.git
<span>cd</span> RealtimeVoiceChat</pre></div>
<p dir="auto">Now, choose your adventure:</p>
<details>
<summary><strong>🚀 Option A: Docker Installation (Recommended for Linux/GPU)</strong></summary>
<p dir="auto">This is the most straightforward method, bundling the application, dependencies, and even Ollama into manageable containers.</p>
<ol dir="auto">
<li>
<p dir="auto"><strong>Build the Docker images:</strong>
<em>(This takes time! It downloads base images, installs Python/ML dependencies, and pre-downloads the default STT model.)</em></p>

<p dir="auto"><em>(If you want to customize models/settings in <code>code/*.py</code>, do it <strong>before</strong> this step!)</em></p>
</li>
<li>
<p dir="auto"><strong>Start the services (App &amp; Ollama):</strong>
<em>(Runs containers in the background. GPU access is configured in <code>docker-compose.yml</code>.)</em></p>

<p dir="auto">Give them a minute to initialize.</p>
</li>
<li>
<p dir="auto"><strong>(Crucial!) Pull your desired Ollama Model:</strong>
<em>(This is done <em>after</em> startup to keep the main app image smaller and allow model changes without rebuilding. Execute this command to pull the default model into the running Ollama container.)</em></p>
<div dir="auto" data-snippet-clipboard-copy-content="# Pull the default model (adjust if you configured a different one in server.py)
docker compose exec ollama ollama pull hf.co/bartowski/huihui-ai_Mistral-Small-24B-Instruct-2501-abliterated-GGUF:Q4_K_M

# (Optional) Verify the model is available
docker compose exec ollama ollama list"><pre><span><span>#</span> Pull the default model (adjust if you configured a different one in server.py)</span>
docker compose <span>exec</span> ollama ollama pull hf.co/bartowski/huihui-ai_Mistral-Small-24B-Instruct-2501-abliterated-GGUF:Q4_K_M

<span><span>#</span> (Optional) Verify the model is available</span>
docker compose <span>exec</span> ollama ollama list</pre></div>
</li>
<li>
<p dir="auto"><strong>Stopping the Services:</strong></p>

</li>
<li>
<p dir="auto"><strong>Restarting:</strong></p>

</li>
<li>
<p dir="auto"><strong>Viewing Logs / Debugging:</strong></p>
<ul dir="auto">
<li>Follow app logs: <code>docker compose logs -f app</code></li>
<li>Follow Ollama logs: <code>docker compose logs -f ollama</code></li>
<li>Save logs to file: <code>docker compose logs app &gt; app_logs.txt</code></li>
</ul>
</li>
</ol>
</details>
<details>
<summary><strong>🛠️ Option B: Manual Installation (Windows Script / venv)</strong></summary>
<p dir="auto">This method requires managing the Python environment yourself. It offers more direct control but can be trickier, especially regarding ML dependencies.</p>
<p dir="auto"><strong>B1) Using the Windows Install Script:</strong></p>
<ol dir="auto">
<li>Ensure you meet the prerequisites (Python, potentially CUDA drivers).</li>
<li>Run the script. It attempts to create a venv, install PyTorch for CUDA 12.1, a compatible DeepSpeed wheel, and other requirements.

<em>(This opens a new command prompt within the activated virtual environment.)</em>
Proceed to the <strong>"Running the Application"</strong> section.</li>
</ol>
<p dir="auto"><strong>B2) Manual Steps (Linux/macOS/Windows):</strong></p>
<ol dir="auto">
<li>
<p dir="auto"><strong>Create &amp; Activate Virtual Environment:</strong></p>
<div dir="auto" data-snippet-clipboard-copy-content="python -m venv venv
# Linux/macOS:
source venv/bin/activate
# Windows:
.\venv\Scripts\activate"><pre>python -m venv venv
<span><span>#</span> Linux/macOS:</span>
<span>source</span> venv/bin/activate
<span><span>#</span> Windows:</span>
.<span>\v</span>env<span>\S</span>cripts<span>\a</span>ctivate</pre></div>
</li>
<li>
<p dir="auto"><strong>Upgrade Pip:</strong></p>
<div dir="auto" data-snippet-clipboard-copy-content="python -m pip install --upgrade pip"><pre>python -m pip install --upgrade pip</pre></div>
</li>
<li>
<p dir="auto"><strong>Navigate to Code Directory:</strong></p>

</li>
<li>
<p dir="auto"><strong>Install PyTorch (Crucial Step - Match Your Hardware!):</strong></p>
<ul dir="auto">
<li><strong>With NVIDIA GPU (CUDA 12.1 Example):</strong>
<div dir="auto" data-snippet-clipboard-copy-content="# Verify your CUDA version! Adjust 'cu121' and the URL if needed.
pip install torch==2.5.1+cu121 torchaudio==2.5.1+cu121 torchvision --index-url https://download.pytorch.org/whl/cu121"><pre><span><span>#</span> Verify your CUDA version! Adjust 'cu121' and the URL if needed.</span>
pip install torch==2.5.1+cu121 torchaudio==2.5.1+cu121 torchvision --index-url https://download.pytorch.org/whl/cu121</pre></div>
</li>
<li><strong>CPU Only (Expect Slow Performance):</strong>
<div dir="auto" data-snippet-clipboard-copy-content="# pip install torch torchaudio torchvision"><pre><span><span>#</span> pip install torch torchaudio torchvision</span></pre></div>
</li>
<li><em>Find other PyTorch versions:</em> <a href="https://pytorch.org/get-started/previous-versions/" rel="nofollow">https://pytorch.org/get-started/previous-versions/</a></li>
</ul>
</li>
<li>
<p dir="auto"><strong>Install Other Requirements:</strong></p>
<div dir="auto" data-snippet-clipboard-copy-content="pip install -r requirements.txt"><pre>pip install -r requirements.txt</pre></div>
<ul dir="auto">
<li><strong>Note on DeepSpeed:</strong> The <code>requirements.txt</code> may include DeepSpeed. Installation can be complex, especially on Windows. The <code>install.bat</code> tries a precompiled wheel. If manual installation fails, you might need to build it from source or consult resources like <a href="https://github.com/erew123/deepspeedpatcher">deepspeedpatcher</a> (use at your own risk). Coqui TTS performance benefits most from DeepSpeed.</li>
</ul>
</li>
</ol>
</details>
<hr>
<p dir="auto"><h2 tabindex="-1" dir="auto">Running the Application <g-emoji alias="arrow_forward">▶️</g-emoji></h2><a id="user-content-running-the-application-️" aria-label="Permalink: Running the Application ▶️" href="#running-the-application-️"></a></p>
<p dir="auto"><strong>If using Docker:</strong>
Your application is already running via <code>docker compose up -d</code>! Check logs using <code>docker compose logs -f app</code>.</p>
<p dir="auto"><strong>If using Manual/Script Installation:</strong></p>
<ol dir="auto">
<li><strong>Activate your virtual environment</strong> (if not already active):
<div dir="auto" data-snippet-clipboard-copy-content="# Linux/macOS: source ../venv/bin/activate
# Windows: ..\venv\Scripts\activate"><pre><span><span>#</span> Linux/macOS: source ../venv/bin/activate</span>
<span><span>#</span> Windows: ..\venv\Scripts\activate</span></pre></div>
</li>
<li><strong>Navigate to the <code>code</code> directory</strong> (if not already there):

</li>
<li><strong>Start the FastAPI server:</strong>

</li>
</ol>
<p dir="auto"><strong>Accessing the Client (Both Methods):</strong></p>
<ol dir="auto">
<li>Open your web browser to <code>http://localhost:8000</code> (or your server's IP if running remotely/in Docker on another machine).</li>
<li><strong>Grant microphone permissions</strong> when prompted.</li>
<li>Click <strong>"Start"</strong> to begin chatting! Use "Stop" to end and "Reset" to clear the conversation.</li>
</ol>
<hr>
<p dir="auto"><h2 tabindex="-1" dir="auto">Configuration Deep Dive 🔧</h2><a id="user-content-configuration-deep-dive-" aria-label="Permalink: Configuration Deep Dive 🔧" href="#configuration-deep-dive-"></a></p>
<p dir="auto">Want to tweak the AI's voice, brain, or how it listens? Modify the Python files in the <code>code/</code> directory.</p>
<p dir="auto"><strong><g-emoji alias="warning">⚠️</g-emoji> Important Docker Note:</strong> If using Docker, make any configuration changes <em>before</em> running <code>docker compose build</code> to ensure they are included in the image.</p>
<ul dir="auto">
<li>
<p dir="auto"><strong>TTS Engine &amp; Voice (<code>server.py</code>, <code>audio_module.py</code>):</strong></p>
<ul dir="auto">
<li>Change <code>START_ENGINE</code> in <code>server.py</code> to <code>"coqui"</code>, <code>"kokoro"</code>, or <code>"orpheus"</code>.</li>
<li>Adjust engine-specific settings (e.g., voice model path for Coqui, speaker ID for Orpheus, speed) within <code>AudioProcessor.__init__</code> in <code>audio_module.py</code>.</li>
</ul>
</li>
<li>
<p dir="auto"><strong>LLM Backend &amp; Model (<code>server.py</code>, <code>llm_module.py</code>):</strong></p>
<ul dir="auto">
<li>Set <code>LLM_START_PROVIDER</code> (<code>"ollama"</code> or <code>"openai"</code>) and <code>LLM_START_MODEL</code> (e.g., <code>"hf.co/..."</code> for Ollama, model name for OpenAI) in <code>server.py</code>. Remember to pull the Ollama model if using Docker (see Installation Step A3).</li>
<li>Customize the AI's personality by editing <code>system_prompt.txt</code>.</li>
</ul>
</li>
<li>
<p dir="auto"><strong>STT Settings (<code>transcribe.py</code>):</strong></p>
<ul dir="auto">
<li>Modify <code>DEFAULT_RECORDER_CONFIG</code> to change the Whisper model (<code>model</code>), language (<code>language</code>), silence thresholds (<code>silence_limit_seconds</code>), etc. The default <code>base.en</code> model is pre-downloaded during the Docker build.</li>
</ul>
</li>
<li>
<p dir="auto"><strong>Turn Detection Sensitivity (<code>turndetect.py</code>):</strong></p>
<ul dir="auto">
<li>Adjust pause duration constants within the <code>TurnDetector.update_settings</code> method.</li>
</ul>
</li>
<li>
<p dir="auto"><strong>SSL/HTTPS (<code>server.py</code>):</strong></p>
<ul dir="auto">
<li>Set <code>USE_SSL = True</code> and provide paths to your certificate (<code>SSL_CERT_PATH</code>) and key (<code>SSL_KEY_PATH</code>) files.</li>
<li><strong>Docker Users:</strong> You'll need to adjust <code>docker-compose.yml</code> to map the SSL port (e.g., 443) and potentially mount your certificate files as volumes.</li>
</ul>
<details>
<summary><strong>Generating Local SSL Certificates (Windows Example w/ mkcert)</strong></summary>
<ol dir="auto">
<li>Install Chocolatey package manager if you haven't already.</li>
<li>Install mkcert: <code>choco install mkcert</code></li>
<li>Run Command Prompt <em>as Administrator</em>.</li>
<li>Install a local Certificate Authority: <code>mkcert -install</code></li>
<li>Generate certs (replace <code>your.local.ip</code>): <code>mkcert localhost 127.0.0.1 ::1 your.local.ip</code>
<ul dir="auto">
<li>This creates <code>.pem</code> files (e.g., <code>localhost+3.pem</code> and <code>localhost+3-key.pem</code>) in the current directory. Update <code>SSL_CERT_PATH</code> and <code>SSL_KEY_PATH</code> in <code>server.py</code> accordingly. Remember to potentially mount these into your Docker container.</li>
</ul>
</li>
</ol>
</details>
</li>
</ul>
<hr>
<p dir="auto"><h2 tabindex="-1" dir="auto">Contributing 🤝</h2><a id="user-content-contributing-" aria-label="Permalink: Contributing 🤝" href="#contributing-"></a></p>
<p dir="auto">Got ideas or found a bug? Contributions are welcome! Feel free to open issues or submit pull requests.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">License 📜</h2><a id="user-content-license-" aria-label="Permalink: License 📜" href="#license-"></a></p>
<p dir="auto">The core codebase of this project is released under the <strong>MIT License</strong> (see the <a href="https://github.com/KoljaB/RealtimeVoiceChat/blob/main/LICENSE">LICENSE</a> file for details).</p>
<p dir="auto">This project relies on external specific TTS engines (like <code>Coqui XTTSv2</code>) and LLM providers which have their <strong>own licensing terms</strong>. Please ensure you comply with the licenses of all components you use.</p>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Databricks in talks to acquire startup Neon for about $1B (187 pts)]]></title>
            <link>https://www.upstartsmedia.com/p/scoop-databricks-talks-to-acquire-neon</link>
            <guid>43899016</guid>
            <pubDate>Mon, 05 May 2025 20:16:29 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.upstartsmedia.com/p/scoop-databricks-talks-to-acquire-neon">https://www.upstartsmedia.com/p/scoop-databricks-talks-to-acquire-neon</a>, See on <a href="https://news.ycombinator.com/item?id=43899016">Hacker News</a></p>
<div id="readability-page-1" class="page"><div dir="auto"><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9928bd89-81f5-42f4-bb84-ae87628c8122_5193x3466.jpeg" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9928bd89-81f5-42f4-bb84-ae87628c8122_5193x3466.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9928bd89-81f5-42f4-bb84-ae87628c8122_5193x3466.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9928bd89-81f5-42f4-bb84-ae87628c8122_5193x3466.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9928bd89-81f5-42f4-bb84-ae87628c8122_5193x3466.jpeg 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9928bd89-81f5-42f4-bb84-ae87628c8122_5193x3466.jpeg" width="1456" height="972" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/9928bd89-81f5-42f4-bb84-ae87628c8122_5193x3466.jpeg&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:972,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:5707007,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/jpeg&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:false,&quot;topImage&quot;:true,&quot;internalRedirect&quot;:&quot;https://www.upstartsmedia.com/i/162915455?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9928bd89-81f5-42f4-bb84-ae87628c8122_5193x3466.jpeg&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9928bd89-81f5-42f4-bb84-ae87628c8122_5193x3466.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9928bd89-81f5-42f4-bb84-ae87628c8122_5193x3466.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9928bd89-81f5-42f4-bb84-ae87628c8122_5193x3466.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9928bd89-81f5-42f4-bb84-ae87628c8122_5193x3466.jpeg 1456w" sizes="100vw" fetchpriority="high"></picture></div></a><figcaption>Databricks is busy buying up a bunch of startups to join its San Francisco office. CREDIT: Smith Collection/Gado/Getty Images</figcaption></figure></div><p>Data and AI unicorn Databricks is in talks to make a splash with another startup acquisition, Upstarts has learned.</p><p>Databricks is in advanced talks to acquire startup Neon, a maker of an open-source database engine, four sources tell Upstarts exclusively. Databricks is expected to pay in the ballpark of $1 billion for Neon, two of the sources say.</p><p data-attrs="{&quot;url&quot;:&quot;https://www.upstartsmedia.com/subscribe?coupon=e8236a37&amp;utm_content=162915455&quot;,&quot;text&quot;:&quot;Get 10% off for 1 year&quot;,&quot;action&quot;:null,&quot;class&quot;:null}" data-component-name="ButtonCreateButton"><a href="https://www.upstartsmedia.com/subscribe?coupon=e8236a37&amp;utm_content=162915455" rel=""><span>Get 10% off for 1 year</span></a></p><p>But despite some industry insiders describing the deal as if it’s done, the talks remain ongoing and could still fall through, several sources add. The total amount could also still exceed $1 billion when employee retention packages are factored in.</p><p>Neon and CEO Nikita Shamgunov did not respond to a comment request. Databricks declined to comment through a spokesperson.</p></div></div>]]></description>
        </item>
    </channel>
</rss>