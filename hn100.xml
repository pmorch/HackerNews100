<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0">
    <channel>
        <title>HN100 - Readable Contents</title>
        <link>https://hn.algolia.com/api/v1/search_by_date?tags=%28story,poll%29&amp;numericFilters=points%3E100</link>
        <description>Uses Readability to add bodies to the RSS feed</description>
        <lastBuildDate>Thu, 22 May 2025 12:30:02 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>en</language>
        <item>
            <title><![CDATA[Why does Debian change software? (149 pts)]]></title>
            <link>https://blog.liw.fi/posts/2025/why-debian-changes/</link>
            <guid>44059411</guid>
            <pubDate>Thu, 22 May 2025 06:50:28 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://blog.liw.fi/posts/2025/why-debian-changes/">https://blog.liw.fi/posts/2025/why-debian-changes/</a>, See on <a href="https://news.ycombinator.com/item?id=44059411">Hacker News</a></p>
<div id="readability-page-1" class="page"><article class="page">
    

    

    <section id="pagebody">
	<p>When I wrote <a href="https://blog.liw.fi/posts/2023/debian-reasons/">Why is Debian the way it is?</a>, a year and a half ago, I was asked to also cover why Debian changes the software it packages. Here’s a brief list of examples of why that happens:</p>
<ul>
<li><p>Software in Debian needs to follow certain policies as set by Debian over the years, and documented in the <a href="https://www.debian.org/doc/debian-policy/">Debian Policy Manual</a>. These are mostly mundane things like system wide configuration being in <code>/etc</code>, documentation in <code>/usr/share/doc</code>, and so on. Some of this is more intricate, like when names of executables can be the same in different packages.</p></li>
<li><p>Programs included in Debian need to work together in other ways. This might mean require changing one or both. As an example, they might need to agree where Unix domain socket exists, or what Unix user account they should run under.</p></li>
<li><p>Debian will remove code that “calls home” or tries to update software in a way that bypasses the Debian packaging system. This is done both for privacy reasons, and because updating software without going via the packaging system is usually problematic from a functional point of view, and always problematic from a security point of view.</p></li>
<li><p>Debian may fix bugs before they’re fixed in upstream, or may backport a bug fix to an earlier version. The goal here is to make life better for users of Debian. Debian does this especially for fixes to security problems, but also for other problems.</p></li>
<li><p>Debian avoids including anything in the main part of its package archive it can’t legally distribute. This applies to the source packages. This means, Debian may strip out those parts software that it doesn’t think are free according to the Debian Free Software Guidelines. The stripped-out parts might be moved to another package in the “non-free” part of Debian. An example might a manual that is licensed under the GNU Free Documentation License with immutable parts, or a logo that can’t be changed.</p></li>
<li><p>Debian has often added a manual page when the upstream doesn’t provide one.</p></li>
</ul>
<p>Thank you to Jonathan McDowell for help with this list. Opinions and mistakes are mine. Mine, I say!</p>

      </section>

  </article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Decibels Are Ridiculous (332 pts)]]></title>
            <link>https://lcamtuf.substack.com/p/decibels-are-ridiculous</link>
            <guid>44058778</guid>
            <pubDate>Thu, 22 May 2025 04:24:33 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://lcamtuf.substack.com/p/decibels-are-ridiculous">https://lcamtuf.substack.com/p/decibels-are-ridiculous</a>, See on <a href="https://news.ycombinator.com/item?id=44058778">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><article><div dir="auto"><p>I don’t have many pet peeves. That said, no matter how hard I try, I just can’t get over the sheer madness of the scientific “unit” we call the decibel (dB).</p><div id="vimeo-257122744" data-attrs="{&quot;videoId&quot;:&quot;257122744&quot;,&quot;videoKey&quot;:&quot;&quot;,&quot;belowTheFold&quot;:false}" data-component-name="VimeoToDOM"><p><iframe src="https://player.vimeo.com/video/257122744?autoplay=0" frameborder="0" gesture="media" allow="autoplay; fullscreen" allowautoplay="true" allowfullscreen="true"></iframe></p></div><p><span>What’s a decibel? Well, the most common answer is “uhh". The second most common answer is that it’s “a way to measure loudness”. But it isn’t! A decibel is not a unit in any conventional sense: it’s more akin to a prefix such as </span><em>mega-</em><span> in </span><em>megabyte. </em><span>It describes a change in magnitude.</span></p><p><span>On the face of it, the idea makes sense. We sometimes want to say that some quantity has increased 10× or 1,000× — and in engineering, these numbers can get large. In principle, we have several types of shorthand notations; for example, instead of “300,000” we can write “3e5”. That said, I don’t know how to pronounce “3e5”, so it’s not unreasonable to come up with a name for the exponent. “An increase of five </span><em>clerts</em><span>” could mean that something has grown by a factor of 10</span><sup>5</sup><span>. </span></p><p><span>This brings us to a pseudo-unit called the </span><em>bel</em><span>. At first blush, it works just like our clert: it’s specifies the exponent of a 10</span><em><sup>n </sup></em><span>multiplier. In other words, +1 bel is a 10× increase (10</span><em><sup>+1</sup></em><span>); meanwhile, -2 bels means a decrease of 100× (10</span><em><sup>-2</sup></em><span>). The bel is named in the honor of Alexander Bell; this is in the same tradition that prompted us to name the “wat” in honor of James Watt.</span></p><p><span>But wait, there’s a twist! The bel was originally devised for measuring power. In some cases, the dissipated power has a quadratic relationship to the applied voltage; for example, for resistive loads in electronics, we have </span><a href="https://lcamtuf.substack.com/p/primer-core-concepts-in-electronic" rel="">Joule’s law</a><span>:</span></p><p><span>In this scenario, if the applied voltage increases by a factor of </span><em><span>x</span><sup>1</sup></em><span>, the consumed power increases by a factor </span><em><span>x</span><sup>2</sup></em><span>. More generally, if voltage applied to a resistor increases </span><em>n </em><span>clerts, then power dissipation jumps up </span><em>2n</em><span> clerts. This is because (</span><em><span>x</span><sup>n</sup><span>)</span><sup>2</sup><span> = x</span><sup>2n</sup></em><span>. </span></p><p><span>Seeing this, some madman decided that 1 bel should always describe a 10× increase in power, </span><em>even if it’s applied to another base unit</em><span>. This means that if you’re talking about watts, +1 bel is an increase of 10×; but if you’re talking about volts, it’s an increase of √10×. This is nuts: it’s akin to saying that the </span><em>milli-</em><span> prefix should have different meanings depending on whether we’re talking about meters or liters. And what if you want to express other units on a similar scale — is frequency more like power or more like voltage?</span></p><p><span>The weirdness didn’t end there. For some reason, the bel — again, what started as a sensible 10× increment — was soon deemed too big to use. I don’t quite know why: in other aspects of life, decimal notation suits us just fine. Either way, instead of putting up with the occasional fractional value or switching to base 2, we divided the bel into ten steps known as </span><em>decibels</em><span>. In effect, we started raising 10 to a fractional power, producing irrational per-dB multipliers:</span></p><div data-component-name="Latex"><p><span>\(\begin{align}
\text{For power: } &amp; 10^\frac{1}{10} \approx 1.258925 \\
\text{For voltage: } &amp; \sqrt{10}^{\ \frac{1}{10}} = 10^{\frac{1}{2} \cdot \frac{1}{10}} = 10^\frac{1}{20} \approx 1.122018 \\
\end{align}\)</span></p></div><p>The original unit is long-forgotten; we use decibels exclusively.</p><p><span>What didn’t change from bels to decibels is that the concept describes nothing more than an exponent for a multiplier; the value is meaningless unless we know the base unit and the reference point (e.g., 1 V). As a matter of custom, however, both of these are often underspecified; in many fields, the decibel evolved into a standalone unit — or a collection thereof. This makes the decibel an </span><em>“if you know, you know”</em><span> kind of a deal.</span></p><p>For example, in acoustics, the “dB” unit actually corresponds to air pressure in pascals (quick quiz: is that a power-like or a voltage-like quantity?). As for the meaning of 0 dB, the measurement is usually indexed to a 1 kHz sound wave that exerts 20 μPa of pressure. This makes some sense: it’s roughly the threshold of human hearing. That said, no part of the “dB” label tells you that.</p><p>From this 0 dB origin, we derive two parallel acoustic scales. One measures the absolute sound pressure level with no regard to frequency; another is weighted to mimic human hearing. The latter peaks at 3 kHz, then tapers off sharply below 200 Hz and above 10 kHz.</p><p><span>Now, let’s imagine you’re trying to buy a microphone; the spec will give you a sensitivity figure such as -45 dB. Is that one of the two acoustic decibels? Fat chance! Here, the real unit is volts. The zero point on that scale describes a hypothetical microphone that produces a 1 V swing in response to a reference sound level. If the microphone you’re looking at is specified at -45 dB, it means that the measured swing is 1 V · 10</span><em><sup>-45/20</sup></em><span>, or about 6 mV.</span></p><p>But hold on: what’s that reference sound level? “Oh, I know,” you exclaim, “it’s the threshold of human hearing”. Nope! It’s 94 dB, roughly the loudness of a gas-powered lawnmower. We can play that game for a long time.</p><p><span>Even in situations where a decibel “unit” has a suffix to hint at what it describes, the naming schemes make little sense. For example, in radio applications, you can come across “dBm”. Is that decibel-meters? No, that would be silly: it’s a measurement of power relative to 1 milliwatt. So, it must follow that “dBμ” is related to 1 microwatt? Hah! That “μ” actually stands microvolts. And you </span><em>really</em><span> don’t want to confuse that with “dBu”…</span></p><p><em><span>I write </span><a href="https://lcamtuf.coredump.cx/offsite.shtml" rel="">well-researched, original articles</a><span> about geek culture, algorithms, and electronic circuit design (+ comparatively few rants about decibels). </span><strong>If you like the content, please subscribe.</strong><span> It’s increasingly difficult to stay in touch with readers via social media; my typical post on X is shown to less than 5% of my followers and gets a ~0.2% clickthrough rate.</span></em></p></div></article></div><div id="discussion"><h4>Discussion about this post</h4></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[ChatGPT Is a Gimmick (102 pts)]]></title>
            <link>https://hedgehogreview.com/web-features/thr/posts/chatgpt-is-a-gimmick</link>
            <guid>44058677</guid>
            <pubDate>Thu, 22 May 2025 04:04:25 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://hedgehogreview.com/web-features/thr/posts/chatgpt-is-a-gimmick">https://hedgehogreview.com/web-features/thr/posts/chatgpt-is-a-gimmick</a>, See on <a href="https://news.ycombinator.com/item?id=44058677">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
                <article>
                    
                    <p><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span>I recently attended a workshop on teaching with artificial intelligence at the university where I teach writing as a part-time adjunct. I had low hopes for the workshop, but I was also desperate. My students keep turning in essays that were obviously generated by AI, and I need to figure out what to do. I looked forward to hearing the keynote speaker, a former university president who made his name arguing that instructors should move all digital technology out of their classrooms so they and their students can focus on the human interactions that technology cannot replace.&nbsp;</span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></p>

<p><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span>At the workshop, though, the first thing he asked us to do was open our laptops, navigate to a couple of LLMs, and enter a prompt. As we did this, he kept talking. What was I supposed to pay attention to? Him, or my screen? While he jabbered, I prompted Claude.ai to write a short essay in response to one of the “innovative” topics it had proposed to me: a “Change My View Challenge,” based on a Reddit forum. It was important that I use the word&nbsp;<em>innovative</em>&nbsp;in my prompt, the presenter insisted. Omit&nbsp;<em>innovative</em>&nbsp;and you get different, presumably more pedestrian, results. Claude spat out the paper and told me it was proud of its work, which, after all, had a “clear thesis statement.” When I said I couldn’t find the statement, Claude replied, “You’re right to question this. Looking at the essay more carefully, there isn’t a single, explicit thesis statement that clearly states the central argument.” Thanks, genius.</span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></p>

<p><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span>Later, the presenter wanted to demonstrate the amazing feats of pedagogical customization AI is capable of. So he asked a model to create, on the spot, a brief podcast summary of his own book, adding that the model should employ baseball metaphors, because, in this experiment, the user is a jock who cares only about baseball. The idea seemed to be that students would better appreciate the book’s content if it were expressed in terms they are already familiar with. He pressed play. The resulting summary, offered by credibly bland digital hosts, was astonishingly shallow and stupid, wedding one tired cliché (education is “lighting a lamp”) to another (“beware of the curveballs”). Was anybody really learning anything from this?</span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></p>

<p><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span>The presenter seemed to be trying much too hard.&nbsp;<em>AI can do this! AI can do that!</em>&nbsp;He moved randomly from topic to topic, bouncing around the stage and making faux-shocked faces at his own pronouncements about the marvelous, career-disrupting implications of large language models. He seemed overstimulated and came across as impatient with the pace of human life and thought as it has&nbsp;been until now.&nbsp;</span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></p>

<p><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span>That speaker was not the most frenzied AI advocate I came across this spring. A recent issue of the&nbsp;<em>Chronicle of Higher Education</em>&nbsp;ran a fevered&nbsp;</span><a href="https://www.chronicle.com/article/are-you-ready-for-the-ai-university"><span>fantasy</span></a><span>&nbsp;by one Scott Latham, a professor of strategy at the Manning School of Business at the University of Massachusetts at Lowell. It is a vision featuring AI “agents” that will provide students a bespoke experience running from orientation through course instruction to job placement, all the while tracking their every frown and furrowed brow (because students will stare endlessly into cameras) and responding at each moment with the perfect remedy. “Human interaction is not as important to today’s students,” Latham claims, and so presumably the AI university will offer little. Compared to the current model of college, he promises, all of this will be better and&nbsp;cheaper—never mind growing evidence that LLMs are deteriorating and becoming&nbsp;</span><a href="https://futurism.com/the-byte/openai-o3-cost-per-query"><span>more expensive</span></a><span>.</span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></p>

<p><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span>And it&nbsp;<em>will</em>&nbsp;happen. In fact, the word&nbsp;<em>will</em>&nbsp;appears more than 130 times in the 4,000-word article.&nbsp;<em>Could</em>, only twice. “Predicting AI’s disruption is the easy part,” Latham writes. “The tough part is making people realize the inevitable.”&nbsp;</span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></p>

<p><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span>The claim of inevitability is crucial to technology hype cycles, from the railroad to television to AI. “A key strategy for a technology to gain market share and buy-in,” the scholars David Gray Widder and Mar Hicks&nbsp;</span><a href="https://ash.harvard.edu/resources/watching-the-generative-ai-hype-bubble-deflate/"><span>write</span></a><span>, “is to present it as an inevitable and necessary part of future infrastructure, encouraging the development of new, anticipatory infrastructures around it.”&nbsp;</span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></p>

<p><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span>The infrastructure demanded by AI is neither neutral nor cheap. It has well-known environmental costs, given the vast amount of electricity and water its data centers demand. Nor is the infrastructure only physical. It has already been built in the minds of students, who are becoming informational lotus-eaters, addicted to immediate, effort-free homework answers and adequate-seeming essays on demand.&nbsp;</span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></p>

<h2><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span>* * *</span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></h2>

<p><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span>Tot up the overeager salesmen, the questionable prophecies, and the clearly exorbitant costs, and it becomes clear: AI is not revolutionary. It’s a gimmick. It entices us with the prospect of sparing us drudgery, but it ultimately disappoints.&nbsp;</span>AI apologists are like hucksters at a county fair, fast-talking about some newfangled marvel. Universities are their gape-mouthed marks,&nbsp;&nbsp;<a href="https://www.insidehighered.com/news/tech-innovation/artificial-intelligence/2023/09/05/risks-and-rewards-higher-ed-should-know"><span>emptying their pockets</span></a><span>&nbsp;even while they are unsure about what they are buying or whether students will use it to learn or simply cheat with.</span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></p>

<p><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span>We call something a gimmick, the literary scholar&nbsp;</span><a href="https://www.chronicle.com/article/the-professor-of-gimmicks"><span>Sianne Ngai</span></a><span>&nbsp;points out, when it seems to be simultaneously working too hard and not hard enough. It appears both to save labor and to inflate it, like a fanciful Rube Goldberg device that allows you to sharpen a pencil merely by raising the sash on a window, which only initiates a chain of causation involving strings, pulleys, weights, levers, fire, flora, and fauna, including an opossum. The apparatus of a large language model really is remarkable. It takes in billions of pages of writing and figures out the configuration of words that will delight me just enough to feed it another prompt. There’s nothing else like it.</span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></p>

<p><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span>But look at what people actually use this wonder for: brain-dead books and videos, scam-filled ads, polished but boring homework essays. Another presenter at the workshop I attended said he used AI to help him decide what to give his kids for breakfast that morning.&nbsp;</span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></p>

<p><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span>Such disappointment inevitably accompanies the gimmick. “The gimmick lets us down,” Ngai writes, “only because it has also managed to pump us up.”&nbsp;<em>A universal culture-producing machine? Remarkable!</em>&nbsp;Then we see its results. Widder and Hicks note that the failed promise of AI “is not surprising, as generative AI does not so much represent the wave of the future as it does the ebb and flow of waves past.” MOOCs, NFTs, AR: We should be wise to the tricks by now. AI progress in cultural production already seems to have slowed because the models have run out of human-generated writing to “learn” from and increasingly feed on AI-produced content, gulping down a vile soup of their own ever-concentrating ordure.</span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></p>

<p><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span>The AI apologists must deny, or at least forestall, such disappointment. They insist that the time scales on AI’s technical progress are shortening—artificial general intelligence will be here in ten years; no, five; no, we’re just months away—even as they implore skeptics to give the technology a chance, because it’s still early days.&nbsp;</span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></p>

<p><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span>But do the apologists even believe it themselves? Latham, the professor of strategy, gives away the game at the end of his reverie. “None of this can happen, though,” he writes, “if professors and administrators continue to have their heads in the sand.” So it’s not inevitable after all? Whoops.</span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></p>

<h2><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span>* * *</span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></h2>

<p><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span>The scholars hectoring their colleagues to adopt AI are not all so ham-fisted. A more subtle and psychologically interesting argument about AI in higher ed shows how fine the line is between the huckster and the mark. Writing recently in&nbsp;<em>The New Yorker</em>, Princeton history professor D. Graham Burnett takes up where Latham leaves off, drawing a distinction between himself and his denialist colleagues: “[E]everyone seems intent on pretending that the most significant revolution in the world of thought in the past century&nbsp;<em>isn’t happening</em>.” This pretense, he writes, “is, simply, madness. And it won’t hold for long.”&nbsp;</span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></p>

<p><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span>Burnett’s essay appears to issue from deep doubts about the value of humanities research. Given the capabilities of AI to sift through archives, detect patterns within the contents, and perhaps incrementally advance what has been said about them, the value of an academic monograph seems to fall to zero. “The making of books such as those on my shelves,” Burnett writes, “each the labor of years or decades, is quickly becoming a matter of well-designed prompts. The question is no longer whether we can write such books; they can be written endlessly, for us. The question is, do we want to read them?”</span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></p>

<p><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span>The answer depends on the “we.” Does the median&nbsp;<em>New Yorker</em>&nbsp;subscriber want to read a monograph? No. Does a scholar fifty or five hundred years hence want to? Let’s put it out there and let them decide. That has been the value proposition of humanistic research for centuries.&nbsp;</span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></p>

<p><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span>Burnett supposes there might still be merit in teaching, even if scholarship is dead. It is a maneuver many an Ivy League PhD has made who finds himself or herself in a job with a heavy teaching load. Indeed, the overwhelming majority of humanities PhDs are in this position. Most have teaching-intensive jobs as adjuncts or, if they are on the tenure track, they teach four or five or six courses per semester at community colleges and regional universities. They never publish a single monograph and read few if any after they finish graduate school. For them, academia already looks like the near-future Burnett envisions.&nbsp;</span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></p>

<p><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span>Burnett decided to merge his teaching with his interest in AI. He assigned students in an undergraduate class—so far as I can tell, the&nbsp;</span><a href="https://registrar.princeton.edu/course-offerings?term=1254&amp;keywords=burnett"><span>only one</span></a><span>&nbsp;he taught this spring—to engage with a chatbot about human attention and turn the text into a short paper. He marvels at their output:</span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></p>

<blockquote>
<p><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span>Reading the results, on my living-room couch, turned out to be the most profound experience of my teaching career. I’m not sure how to describe it. In a basic way, I felt I was watching a new kind of creature being born, and also watching a generation come face to face with that birth: an encounter with something part sibling, part rival, part careless child-god, part mechanomorphic shadow—an alien familiar.</span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></p>
</blockquote>

<p><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span>In Burnett’s eyes, his students are not just creative in the ordinary sense of being able to turn nice phrases or make clever connections—already feats that lighten a grading load of dreary essays. No, Burnett’s students are conjurers, evokers, maybe minor deities, able to break the old material laws most of us labor under. I know this impulse. In moments of professional self-doubt, I have often tried to convince myself that my students were amazing, that their halting efforts were in fact brilliant, that my class, unique among the courses listed on their transcripts, unlocked something in them, that the students were teaching&nbsp;<em>me</em>, that, indeed, all I needed to do was get out of their way.&nbsp;</span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></p>

<p><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span>It is a lie many teachers tell themselves. And why not? It is not as if someone can fact-check it. Your scholarship, or lack thereof, is public; your students’ work occurs behind the high fence of the Family Educational Rights and Privacy Act. You tell yourself a story at once self-abnegating and self-aggrandizing. You tell it to raise the students up; you’re there for&nbsp;<em>them</em>, after all. And that much is true; you are there for them. But you tell the story as well to put down your unfeeling, ossified colleagues, the ones who don’t get it. You tell it so you won’t feel as old as they are, because you are spiritually closer to the young. To Burnett, the AI dialogs offered something genuinely new under the sun. “Each sheaf of paper I picked up,” he writes, “was more astonishing than the last.” Sure.</span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></p>

<p><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span>This is not to say I have never been astonished by my students’ writing. Burnett reports being moved to tears by one of his students’ interactions with a chatbot. I, too, have cried while reading student essays. I once had a student who started college only after he had retired from four decades working for the phone company. He was not a great writer, but he wrote from the heart. In a class on religious autobiography, he described going to the hospital to speak to the man, by then on his deathbed, who had murdered my student’s mother. The man asked my student to forgive him. My student did so. He forgave. The essay testified to a form of love few of us would be capable of. Sitting at my kitchen table, I read the essay a second time; I cried a second time. It was the rare piece of student writing that improved the world by its existence. It made mercy more widely known.</span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></p>

<h2><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span>* * *</span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></h2>

<p><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span>In the end, Burnett is essentially in the same place as his ostrich-headed colleagues, though it is not clear he realizes it.&nbsp;“You can no longer&nbsp;<em>make</em>&nbsp;students do the reading or the writing,” he writes, because they can make a machine do it for them. “So what’s left? Only this: give them work they want to do. And help them want to do it.” Is this a slip-up? A signal that AI-savvy students don’t need teachers after all? If students want to do the work, then they don’t need help wanting it. No, the only task remains the paradoxical one identified as far back as in Plato’s&nbsp;<em>Meno</em>: Give students work they don’t know they need to do. And yes, help them want to do it.&nbsp;</span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></p>

<p><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span>I have found, to overcome students’ resistance to learning, you often have to trick them. There’s the old bait-and-switch of offering grades, then seeing a few students learn to love learning itself. Worksheets are tricks, as are small-group discussions and even a teacher’s charisma. I’m sure I have used baseball analogies in class, too. In the face of the difficulty of reforming students’ desires, you can trick yourself into believing you’re doing it, and sleep well at night. I don’t know anyone for whom it’s a straightforward task. It’s&nbsp;<em>the</em>&nbsp;challenge for any teacher, and AI offers a tempting illusion to students—and evidently to some teachers—that there could be a shortcut.&nbsp;</span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></p>

<p><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span>The week Burnett’s article appeared, I visited the classroom of Ted Hadzi-Antich at Austin Community College. His honors political philosophy students were discussing the final section of James Baldwin’s&nbsp;<em>The Fire Next Time</em>. They wanted to talk about death. They sat in chairs in a&nbsp;circle, no desks walling them off from each other. I had hoped I could sit unobtrusively in the corner and take notes on the scene. “Learning doesn’t happen in the corner!” a student scolded. OK, fine. I was present. So I was implicated in what was about to occur.</span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></p>

<p><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span>For eighty minutes, the eleven students and Hadzi-Antich talked. I chimed in at the end. No students texted. None disappeared into a screen. Two cried. They held their highlighted copies of the reading in their laps, but they didn’t talk much about it. Even so, they connected Baldwin’s ideas to their varied life experiences, including loss, addiction, and bigotry. Two students disagreed with each other about the uses of the past. Later, they told me they each describe the other as their “antagonist.” They seemed to respect each other, having read each other’s writing and debated in class all semester. They know each other’s voice.</span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></p>

<p><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span>This is what Burnett seems to think is new, or newly exciting, thanks to his engagement with AI—something teachers such as Hadzi-Antich have been orchestrating in their classes since well before the dawn of AI. And in fact, Hadzi-Antich promotes text- and discussion-based education at community colleges through&nbsp;</span><a href="https://www.tgqf.org/"><span>The Great Questions Foundation</span></a><span>, which he directs. You don’t need to experience the technological sublime in order to see the value of giving students a book and asking them to read and discuss it. You do need to accept something like Max Weber’s idea of the teacher’s vocation, that of helping others “<em>to reckon with the ultimate meaning of</em>&nbsp;[their]&nbsp;<em>own actions</em>.” This reckoning is not a task you can expedite with a machine. If you try to spare yourself the labor, you fail.</span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></p>

<h2><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span>* * *</span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></h2>

<p><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span>Appropriately enough, this essay took me forever to write. I had a great deal of anger, frustration and sadness to draw on. I had evidence and critique, but I could not find an argument. I kept working. I changed the whole focus after I attended the AI workshop at school. That opened me up. I worked for days in the caesura between reading my students’ drafts and reading their finished research essays, knowing for sure that some had done all the work themselves because I had seen them put up the scaffolding and then build the essay, brick by brick. I knew that others would likely do very little, but I would be unable to prove it. As I worked, new articles and outrages about AI in education appeared daily, making me feel as though I were falling behind. Believe me, I wanted a shortcut.&nbsp;</span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></p>

<p><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span>I had not read&nbsp;<em>Theory of the Gimmick</em>&nbsp;before starting this essay, having only heard about it from my wife, a professor of literature. To get up to speed, I read an earlier article that Ngai turned into a chapter. I then skimmed my wife’s well-marked-up copy of the book, noting her underlines, her starred passages, her “hm!” in the margins. I leaned on the index to find passages on belief. I left most of the book unread.</span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></p>

<p><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span>I did these things because I know I am mortal. I cannot afford to read a 400-page work of frankly oblique theoretical prose. That is to cast no shade on Ngai. Her book deserves a close reading. I have only so much time. Even if that sounds like the same excuse students give for why they take AI shortcuts, I don’t think we mean the same thing.&nbsp;</span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></p>

<p><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span>Once I had a full draft, I asked my wife to read and comment on it. At the time, she was working on a paper about Herman Melville’s&nbsp;<em>The Confidence Man</em>&nbsp;and Max Weber’s “Science as a Vocation.” The latter is a locus classicus for our dinner-table conversation. She needed only to make a brief mention of a key passage in Weber for me to know what move the essay was missing. Her concerns, her attention, and the contingencies of her life and thought are all over this essay. It would have been different if she had been reading, say, William or Henry James that week. To a large extent, the intellectual circuit running through her life and mine, across two decades of talking about literature and culture, love and death, constitutes our life together.&nbsp;</span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></p>

<p><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span>After I got her feedback, I finally asked ChatGPT if generative AI could be considered a gimmick in Ngai’s sense. I did not read its answer carefully. Whenever I see the words cascade down my computer screen, I get a sinking feeling.&nbsp;<em>Do I really have to read this?</em>&nbsp;I know I am unlikely to find anything truly interesting or surprising, and the ease with which the words appear really does cheapen them.&nbsp;</span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></p>

<p><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span>ChatGPT reported back that it could indeed be considered a gimmick. Then I asked the gimmick how educators should approach its implementation in universities and schools. “The worst thing educational institutions could do is embrace AI uncritically as an inevitable ‘efficiency upgrade,’” it wrote, “because that would mean compounding the very gimmickry Ngai diagnoses: mistaking ease for value, and output for understanding.” Take that, Scott Latham. This is, of course, just what I hoped the machine would say. I have argued enough with the model that it probably knows I want it to be self-critical. I suspect it admits such things only to me. I wish it would tell university presidents, information officers, professors, and certainly students that even ChatGPT thinks it is a gimmick. But it only ever tells them what they want to hear, not what I want.&nbsp;</span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></p>

<p><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span>Ted Hadzi-Antich’s students read and discussed these words by James Baldwin: “Perhaps the whole root of our trouble, the human trouble, is that we will sacrifice all the beauty of our lives, will imprison ourselves in totems, taboos, crosses, blood sacrifices, steeples, races, mosques, armies, flags, nations” —I might add technologies and hype bubbles—“in order to deny the fact of death, which is the only fact we have.”&nbsp;</span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></p>

<p><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span>Part of a teacher’s job—certainly in the humanities, but even in professional fields like business—is to help students break out of their prisons, at least for an hour, so they can see and enhance the beauty of their own minds. It is to help them learn, together, to defend how they want to live, precisely because they, too, unlike a machine, will one day die.</span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></p>

<p><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span>I will sacrifice some length of my days to add depth to another person’s experience of the rest of theirs. Many did this for me. The work is slow. Its results often go unseen for years. But it is no gimmick.</span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></p>




                </article>

                            </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Kotlin-Lsp: Kotlin Language Server and Plugin for Visual Studio Code (113 pts)]]></title>
            <link>https://github.com/Kotlin/kotlin-lsp</link>
            <guid>44058299</guid>
            <pubDate>Thu, 22 May 2025 02:46:47 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/Kotlin/kotlin-lsp">https://github.com/Kotlin/kotlin-lsp</a>, See on <a href="https://news.ycombinator.com/item?id=44058299">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><p dir="auto"><h2 tabindex="-1" dir="auto">Language Server for Kotlin</h2><a id="user-content-language-server-for-kotlin" aria-label="Permalink: Language Server for Kotlin" href="#language-server-for-kotlin"></a></p>
<p dir="auto"><a href="https://kotlinlang.org/docs/components-stability.html" rel="nofollow"><img src="https://camo.githubusercontent.com/c8add7fbcddc03009a577eb21a377434029f70f6926fdc9d3820351ab71585dc/68747470733a2f2f6b6f746c2e696e2f6261646765732f6578706572696d656e74616c2e737667" alt="Kotlin Alpha" data-canonical-src="https://kotl.in/badges/experimental.svg"></a>
<a href="https://confluence.jetbrains.com/display/ALL/JetBrains+on+GitHub" rel="nofollow"><img src="https://camo.githubusercontent.com/3a1da9e18c90c77d3353249e6f25354aa47d4c693655086c1e53a5113f886e17/68747470733a2f2f6a622e67672f6261646765732f696e63756261746f722e737667" alt="JetBrains incubator project" data-canonical-src="https://jb.gg/badges/incubator.svg"></a></p>
<p dir="auto">Pre-alpha official Kotlin support for Visual Studio Code and an implementation of <a href="https://github.com/Microsoft/language-server-protocol">Language Server Protocol</a>
for the Kotlin language.</p>
<p dir="auto">The server is based on <a href="https://github.com/JetBrains/intellij-community">IntelliJ IDEA</a> and the <a href="https://github.com/JetBrains/intellij-community/tree/master/plugins/kotlin">IntelliJ IDEA Kotlin Plugin</a>
implementation.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Quick Start</h3><a id="user-content-quick-start" aria-label="Permalink: Quick Start" href="#quick-start"></a></p>
<ol dir="auto">
<li>Download the latest build of VSC extension via <a href="https://github.com/Kotlin/kotlin-lsp/blob/main/RELEASES.md">RELEASES.md</a></li>
<li>Install it as a VSC Extension via <code>Extensions | More Action | Install from VSIX</code>
<ul dir="auto">
<li>Alternatively, it is possible to drag-and-drop VSIX extension directly into <code>Extensions</code> tool window</li>
</ul>
</li>
<li>Ensure that your Java version is 17 or above</li>
<li>Open a folder with JVM-only Kotlin Gradle project and the project will be immediately recognized and LSP activated</li>
</ol>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/Kotlin/kotlin-lsp/blob/main/images/quickstart_sample.gif"><img src="https://github.com/Kotlin/kotlin-lsp/raw/main/images/quickstart_sample.gif" alt="quickstart_sample.gif" data-animated-image=""></a></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Supported features and Roadmap</h3><a id="user-content-supported-features-and-roadmap" aria-label="Permalink: Supported features and Roadmap" href="#supported-features-and-roadmap"></a></p>
<p dir="auto">The best way to track current capabilities and what is going to be supported in the next builds is this table:</p>
<blockquote>
<p dir="auto">Important note: currently, only JVM-only Kotlin Gradle projects are supported out-of-the box.</p>
</blockquote>
<ul>
<li> Project import
<ul>
<li> Gradle JVM project import</li>
<li> Gradle KMP project import</li>
<li> JSON-based build system agnostic import
<ul>
<li> Quickstart for JSON</li>
</ul>
</li>
<li> Maven/Amper import</li>
<li> Dumb mode for no build system at all</li>
</ul>
</li>
<li> Highlighting
<ul>
<li> Semantic highlighting</li>
</ul>
</li>
<li> Navigation
<ul>
<li> Navigation to Kotlin (source, binary)</li>
<li> Navigation to Kotlin builtins</li>
<li> Navigation to Java (source, binary)</li>
</ul>
</li>
<li> Code actions
<ul>
<li> Quickfixes (i.e. <code>replace with</code>)</li>
<li> Kotlin inspections</li>
<li> Organize imports</li>
<li> Go to reference</li>
</ul>
</li>
<li> Refactorings
<ul>
<li> Rename</li>
<li> Move</li>
<li> Change signature</li>
</ul>
</li>
<li> On-the-fly Kotlin diagnostics</li>
<li> Completion
<ul>
<li> Analysis-API based completion</li>
<li> IJ-based completion
<ul>
<li> Enable IJ-based completion</li>
</ul>
</li>
</ul>
</li>
<li> KDoc support
<ul>
<li> In-project documentation hovers</li>
<li> Dependencies/Java documentation hovers from <code>source.jar</code></li>
</ul>
</li>
<li> Code formatting</li>
<li> Fully-featured Windows support</li>
<li> Reactive updates from the filesystem</li>
</ul>
<p dir="auto"><h3 tabindex="-1" dir="auto">Project Status</h3><a id="user-content-project-status" aria-label="Permalink: Project Status" href="#project-status"></a></p>
<p dir="auto"><strong>The project is in an experimental, pre-alpha, exploratory phase</strong> with the intention to be productionized.</p>
<p dir="auto">We <a href="https://xkcd.com/1428/" rel="nofollow">move fast, break things</a>, and explore various aspects of the seamless developer experience
including Java interoperability, limits of IntelliJ capabilities as a standalone server, native binaries of the LSP, and
debug capabilities.</p>
<p dir="auto">The LSP supports most of the essential parts, but its final shape is not near to be defined and
even the most basic and core parts are being changed on a regular basis.</p>
<p dir="auto">So we have the corresponding stability guarantees -- <strong>none</strong>. It is okay to use it in your toy
projects, to experiment with it and to provide your feedback, but it is not recommended
to depend on its stability in your day-to-day work.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Supported platforms</h3><a id="user-content-supported-platforms" aria-label="Permalink: Supported platforms" href="#supported-platforms"></a></p>
<p dir="auto">In the current state, the golden path has been tested for Visual Studio Code with macOS and Linux platforms.</p>
<p dir="auto">You can use Kotlin LSP with other LSP-compliant editors, but configuration must be done manually.
Please note that Kotlin LSP uses pull-based diagnostics, so the editor must support that.</p>
<p dir="auto">You can find a standalone LSP launch script in <a href="https://github.com/Kotlin/kotlin-lsp/blob/main/scripts/kotlin-lsp.sh">kotlin-lsp.sh</a> along
with <em>very experimental</em> (aka "works on someone's machine") instructions that setup LSP for other editors in <a href="https://github.com/Kotlin/kotlin-lsp/blob/main/scripts">scripts</a> folder.
See <code>./kotlin-lsp.sh --help</code> for available options.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Source code</h3><a id="user-content-source-code" aria-label="Permalink: Source code" href="#source-code"></a></p>
<p dir="auto">Currently, the LSP implementation is partially closed-source, primarily for the sake of development speed convenience --
it heavily depends on parts of IntelliJ, Fleet, and our distributed Bazel build that allows us to
iterate quickly and experiment much faster, cutting corners and re-using internal infrastructure where it helps.
After the initial stabilization phase and defining the final set of capabilities, we will de-couple the LSP implementation from the internal repository
and build pipelines and open source it completely (with an explicit dependency on IntelliJ), this is a temporary constraint.
VSC extension is mirrored into <a href="https://github.com/Kotlin/kotlin-lsp/blob/main/kotlin-vscode">kotlin-vscode</a> as it does not depend on anything internal.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Feedback and issues</h3><a id="user-content-feedback-and-issues" aria-label="Permalink: Feedback and issues" href="#feedback-and-issues"></a></p>
<p dir="auto">The best way to provide your feedback or report an issue is to file a bug <a href="https://github.com/Kotlin/kotlin-lsp/issues/new">in GitHub issues</a>.</p>
<p dir="auto">As a temporary limitation, direct contributions are not supported as this repository is a read-only mirror,
but it is possible to open a PR into the documentation, and it will be integrated manually by maintainers.</p>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Getting a paper accepted (142 pts)]]></title>
            <link>https://maxwellforbes.com/posts/how-to-get-a-paper-accepted/</link>
            <guid>44057841</guid>
            <pubDate>Thu, 22 May 2025 01:19:41 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://maxwellforbes.com/posts/how-to-get-a-paper-accepted/">https://maxwellforbes.com/posts/how-to-get-a-paper-accepted/</a>, See on <a href="https://news.ycombinator.com/item?id=44057841">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
      <p>In 2019, I submitted a paper that was rejected with review scores 2.5, 3, 3. One week later, I resubmitted it with minor changes, and it was accepted with scores 4, 4.5, 4.5.<sup><a href="#fn1" id="fnref1">01</a></sup> For context, that’s an almost unspeakably dramatic jump in scores, from “middling reject” to “strong accept.”</p>
<p>This post shows exactly those changes. We’ll frame them in two parts:</p>
<ol>
<li>Polish page 1 for acceptance</li>
<li>Use the remaining pages to avoid rejection</li>
</ol>
<p>Page 1 has four parts: title, abstract, Figure 1, and introduction. We’ll make them specific, memorable, clear, communicate value, and hook the reader. Reviewers mostly decide accept vs reject by page 1. So we optimize the judgment-before-scroll.</p>
<p>Then, to make sure our paper isn’t rejected, we’ll do due diligence in the rest of it by including stuff like baselines, ablations, statistical significance, and human evaluation.</p>
<p>The tweaks that get the paper accepted—unexpectedly, happily—also improve the actual science contribution. But if you’re tempted to be evil, read this footnote.<sup><a href="#fn2" id="fnref2">02</a></sup> The full rejected and accepted submissions are available for download at the end.</p>
<!-- TBD: Where  Writing (abstract / intro / conclusion): top-down, boring, focused on past work, vague, implying incremental and failed &rarr; specific, interesting, tension/release cycles, difficulty and benefit-motivated, with unique hooks, arguing **value** -->
<!-- Two axes that might not be worth it -->
<!-- These changes can be sorted into two buckets:
1. Completeness --- Baselines, ablations (often also: human evaluations)
2. Clarity --- Everything else. -->
<h2 id="page-1-is-80percent-of-your-paper" tabindex="-1">Page 1 Is 80% of Your Paper </h2>
<blockquote>
<p>A paper has five parts:</p>
<ol>
<li>Title</li>
<li>Figure 1</li>
<li>Abstract</li>
<li>Introduction</li>
<li>Rest of the paper</li>
</ol>
<p>Spend equal time on each of these.</p>
<p>— <em>Me misquoting<sup><a href="#fn3" id="fnref3">03</a></sup> <a href="https://www.youtube.com/watch?v=imEtTnQKt4M">Jitendra Malik quoting Don Geman</a></em></p>
</blockquote>
<p>Around 80% of a paper’s perceived quality is established on page 1. The title, Figure 1, abstract, and half the introduction are all there. It’s like a book’s cover.</p>
<p>Throughout this post, I’ll show the rejected and accepted versions of the paper I mentioned at the top with the dramatic score swing. Here are both page 1s:</p>

        
        
<p><span>
<span>Top:</span>
<span>Left:</span>
</span>
Rejected page 1.<span>
<span>Bottom:</span>
<span>Right:</span>
</span>
Accepted page 1.</p>
<!-- This isn't totally trivial, because a big reason is "I didn't get it" (which is internalized as "it's boring" and then spelled "low impact" in the review). -->
<p>First, consider page 1’s first impression:<sup><a href="#fn4" id="fnref4">04</a></sup></p>
<ul>
<li>Is the Figure 1 colorful and eye-catching?</li>
<li>Is the title unexpected? Maybe it has one intriguing word?</li>
<li>Are there any curious terms (bolded or italicized)?</li>
<li>Is the introduction (hopefully not) full of citations?<sup><a href="#fn5" id="fnref5">05</a></sup></li>
</ul>
<h3 id="choose-a-specific-memorable-title" tabindex="-1">Choose A Specific Memorable Title </h3>
<p>Rejected: <em>Visually Grounded Comparative Language Generation</em> — too general. Any work that uses pictures and generates comparisons could use this title. I picked this title because I thought it argued for the generality of the method. But a too-general title is off-putting because it comes across as over-claiming. And a big part of our method <em>does</em> rely on our domain: we specifically use a biological taxonomy to create our dataset.</p>
<p>Accepted: <em>Neural Naturalist: Generating Fine-grained Image Comparisons</em> — specific and memorable. In addition to branding (more next), <em>naturalist</em> establishes the domain, and <em>fine-grained</em> narrows the task. Skeptical academics appreciate the clarity of saying what you did. The title is fully unique to our work.<sup><a href="#fn6" id="fnref6">06</a></sup></p>
<h3 id="maybe-add-branding" tabindex="-1">Maybe Add Branding </h3>
<p>I used to dislike branding in papers. It felt presumptuous to claim a proper noun for your research paper and to expect readers to memorize it. And many of the names sound corny.</p>
<p>Now, while I still often feel a pang of annoyance, it is outweighed by the recognition that it’s much easier to remember and discuss concepts which have a name. <em>Neural naturalist</em> or <em>Birds-to-Words</em> instead of “our 2019 EMNLP paper about generating comparative image captions…”</p>
<p>That said, I still dislike throwaway names—those with no conceptual link, or which don’t feel earned. I don’t think every paper needs one. But I think it helped for this paper.</p>
<h3 id="show-screamingly-obvious-value-in-figure-1" tabindex="-1">Show Screamingly Obvious Value in Figure 1 </h3>
<p>The main point is that your paper’s value should be <em>obvious,</em> not that is must be <em>enormous.</em></p>

        
        
<p><span>
<span>Top:</span>
<span>Left:</span>
</span>
Rejected Figure 1.<span>
<span>Bottom:</span>
<span>Right:</span>
</span>
Accepted Figure 1.</p>
<p>A Figure 1 should</p>
<ul>
<li>draw readers in</li>
<li>clearly demonstrate describe both what the work does and its value</li>
<li>be comprehensible without the caption.</li>
</ul>
<p>The old Figure 1 showed two separate comparisons, but the link between them wasn’t clear. The bottom row just all look like owls to a non-expert. And the descriptions are long and boring.</p>
<p>The new Figure 1 makes the works’ focus explicit by anchoring with the same left image, and labeling each comparison with a perceptual difficulty (“high” vs “medium”). It annotates the operation (“vs” = comparison) and the result (“highly detailed” vs “fewer details”). At this point, the paper’s mechanics and unique characteristic has been established: we use different language to compare things based on how similar they look. Finally, to make the long descriptions more approachable and interesting, we’ve highlighted two components (features and parts, with orange underlines and green bubbles).</p>
<p>A problem with making Figure 1s—and describing your research in general—is that you know so much about it, it’s impossible to mentally model what it’d be like to learn about your work for the first time. Spending time away from your work is extremely helpful here, if possible. I think I benefitted by having the conference review period (a month or two?) away from the paper, so I could come back to it with fresh eyes and rethink how best to illustrate it.</p>
<p>I’ve <a href="https://maxwellforbes.com/posts/figure-creation-tutorial-making-a-figure-1/">written about Figure 1s</a> before. Even at the peak of my Figure 1 game, it was normal to make ten drafts before submitting.</p>
<h3 id="end-each-caption-with-the-takeaway" tabindex="-1">End Each Caption with the Takeaway </h3>
<p>I think this is the single best paper-writing hack I’ve ever learned.</p>
<p>This Figure 1 is so information-dense nearly the whole caption is the takeaway (yellow). Compare vs the old caption which, has side note (red) taking nearly 1/3 of the (extremely valuable front-page) real estate!</p>

        
        
<p><span>
<span>Top:</span>
<span>Left:</span>
</span>
Rejected Fig 1 caption.<span>
<span>Bottom:</span>
<span>Right:</span>
</span>
Accepted Fig 1 caption.</p>
<p>A takeaway message explains not what is literally being shown in the figure (that comes first), but what you should think about it.</p>
<p>It might feel strange to do this in scientific writing, because it feels like it crosses the boundary from description into interpretation. But I urge you to do it, especially for less formal fields like computer science because:</p>
<ul>
<li>
<p>You’re saving readers time trying to understand what point you are trying to make by just writing it out.<sup><a href="#fn7" id="fnref7">07</a></sup></p>
</li>
<li>
<p>With good captions, you can understand the whole paper by only looking at the figures. Many (most?) future readers will read your paper this way.</p>
</li>
<li>
<p>The scientific reader has a <em>grain of salt</em> mindset about everything you write anyway, so don’t stress about the ‘interpretation’ aspect.</p>
</li>
</ul>
<p>If you aren’t trying to prove a point, well, perhaps reconsider that figure.</p>
<p>I got even more brazen about takeaways in future papers, even writing bolded “Takeaway:” in the caption itself.<sup><a href="#fn8" id="fnref8">08</a></sup></p>
<h3 id="the-abstract:-a-specific-valuable-hook" tabindex="-1">The Abstract: A Specific Valuable Hook </h3>
<p>A classic mistake for a certain type of nerd (e.g., me) is to write top-down, going from general concepts to your specific topic. This is tempting because it feels orderly and taxonomic.</p>

        
        
<p><span>
<span>Top:</span>
<span>Left:</span>
</span>
Rejected abstract.<span>
<span>Bottom:</span>
<span>Right:</span>
</span>
Accepted abstract.</p>
<p>But this turns out terribly, as you can see in the rejected abstract. It’s both boring and feels over-claiming. After top-down framing, and an aside, there’s a ‘betrayal’ of scope when we reveal our actual task.<sup><a href="#fn9" id="fnref9">09</a></sup></p>
<p>Everything is more specific in the revised abstract: what we study, our contributions (dataset and model), all the way to literal descriptions of specific birds and the task done in human evaluations. There’s a results teaser, and a hint of a unique hook. It’s not only more specific, it’s more fun and compelling to read.</p>
<p>You don’t think your reader wants to have fun and read something compelling? Try reviewing conference papers. Enjoyable writing is like water in a desert. Reviewers won’t even realize why they’re happy, the’ll just like the paper. Read <a href="https://arxiv.org/pdf/1804.02767">YOLOv3</a> and tell me you don’t enjoy it.<sup><a href="#fn10" id="fnref10">10</a></sup></p>
<h3 id="use-tensionrelease-cycles-in-the-intro" tabindex="-1">Use Tension/Release Cycles in the Intro </h3>
<p>Can you believe we’re still on page 1? It’s that important.</p>
<p>Here we’re discussing specifically the <em>portion of the introduction visible on page 1.</em> We’re optimizing for what we could call judgment-before-scroll.</p>
<p>My original draft was so bad it’s easy to improve. But if I could write something this bad as a 4th year PhD student, others could too.</p>

        <div>
            <p><img src="https://maxwellforbes.com/assets/posts/phd-metagame/paper-04-intro-acl-annotated.moz90.jpg" loading="lazy" decoding="async" width="855" height="487" data-thumbhash-b64="ckgGFIaAypeXeIebeIh/dvSnhQ==" srcset="https://maxwellforbes.com/assets/eleventyImgs/sq1kFXzrR5-428.jpeg 428w, https://maxwellforbes.com/assets/eleventyImgs/sq1kFXzrR5-855.jpeg 855w" sizes="(max-width: 30em) 100vw, (max-width: 704px) 100vw, 704px">
            </p>
        </div>
        
        <div>
            <p><img src="https://maxwellforbes.com/assets/posts/phd-metagame/paper-04-intro-emnlp-annotated.moz90.jpg" loading="lazy" decoding="async" width="828" height="434" data-thumbhash-b64="dhgGFIK4xadqhoZvhoWDwGT15Q==" srcset="https://maxwellforbes.com/assets/eleventyImgs/09sVDo5ypm-414.jpeg 414w, https://maxwellforbes.com/assets/eleventyImgs/09sVDo5ypm-828.jpeg 828w" sizes="(max-width: 30em) 100vw, (max-width: 704px) 100vw, 704px">
            </p>
        </div>
        
<p><strong>Top:</strong> Rejected page 1 intro. <strong>Bottom:</strong> Accepted page 1 intro.</p>
<p>My original introduction completely lacks any mention of a problem, and is devoid of tension. It begins with a top-down pile of related work, then side-swipes our own paper with negative implications.</p>
<p>The revised introduction launches straightaway into the problem.</p>
<p>It uses tension/release cycles at multiple resolutions to build up the stakes of the problem and the perceived value of solving it. First, at the paragraph-scale: ¶ 1+2 builds up the problem (tension), ¶ 3 presents our solution (release). Then at sentence-scale, unstable language creates tension: <em>“but,” “difficult,” “strain,” “while X, Y,” “unfortunately.”</em></p>
<p>On the backbone of these tension/release cycles, we spend the first two paragraphs setting up our task as being specific, difficult, valuable, and unique. And I really mean each of those adjectives. The final bit of visible text (on page 1) introduces a concrete contribution, our dataset.</p>
<p>I hesitate to recommend a video here because it’s both slightly abstract and eighty minutes long, but <a href="https://www.youtube.com/watch?v=vtIzMaLkCaM">Larry McEnerney’s talk on Effective Writing</a> is the single best material I’ve seen on thinking about your writing. I saw it way after grad school, but I wish I’d seen it during because I spent a lot of time blindly reverse engineering bits of it (on display in this essay). Some relevant key points:</p>
<ol>
<li>All of life before your job, people (teachers) have been paid to read your writing</li>
<li>Now that they’re not, your writing must deliver <em>value</em> (which is often entertainment)</li>
<li>High-value text poses <em>problems</em> with tension-filled language, articulating costs or benefits</li>
</ol>
<p>I didn’t understand this framing (of problem, tension, value) while writing the revision. But in hindsight, it’s shockingly clear how faithfully the improved draft adheres to it.</p>
<h2 id="use-the-rest-of-the-paper-to-avoid-all-reasons-for-rejection" tabindex="-1">Use the Rest of the Paper to Avoid All Reasons for Rejection </h2>
<p>If we’ve done our job, reviewers have now finished reading page 1 and want to accept our paper. Our job now is to let them. How?</p>
<p>Surprise, I have another great two-step process. It uses <a href="https://maxwellforbes.com/garage/thinking-in-reverse/">thinking in reverse</a>:</p>
<ol>
<li>Think of all the reasons a reviewer might reject your paper</li>
<li>Avoid everything in 1.</li>
</ol>
<p>The more obvious reasons for rejection have to do with completeness: “you didn’t compare against method X.” But those are often used as objective crutches to justify a gut decision based on lack of clarity. So we must ensure completeness and also polish up the clarity.</p>
<p>After page 1, the main changes I made are:</p>
<ul>
<li>improving all the figures and tables (clarity)</li>
<li>adding baselines (completeness)</li>
<li>adding ablations (completeness)</li>
<li>rewriting the conclusion (clarity)</li>
</ul>
<p>For reference, other common additions are:</p>
<ul>
<li>human evaluations (completeness)</li>
<li>statistical significance (completeness)</li>
</ul>
<p>The running text is nearly identical. This is great because someone skimming the paper—looking at only figures, tables, and the conclusion—can enjoy all the improvements.</p>
<h3 id="make-figures-dense-and-beautiful" tabindex="-1">Make Figures Dense and Beautiful </h3>
<p>There’s this complicated part of the paper called <em>pivot-branch sampling.</em> I was very excited about it but nobody else cared about it. (I think not even my coauthors, though they were too kind to ever say so).</p>
<p>I had the decency to relegate most of <em>pivot-branch sampling</em> to the appendix, but it has to be mentioned a little bit in the body because it’s in a dataset paper.</p>
<p>Still, the clarity just wasn’t there. Figure 2 was supposed to help, but it didn’t. In the revision, I added some graphics, which helps quickly get the idea across.</p>

        
        
<p><span>
<span>Top:</span>
<span>Left:</span>
</span>
Rejected Figure 2.<span>
<span>Bottom:</span>
<span>Right:</span>
</span>
Accepted Figure 2.</p>
<p>In the rejected version, I thought lighter gray text would be nice because there’s a design rule that you shouldn’t use pure black. But it contrasted weirdly with the paper’s body text, which has a maddeningly adjacent font and <em>is</em> pure black.</p>
<p>In the accepted version, I went with a sans-serif, black text which helped the figure feel solid and distinct. And more importantly, I used the real estate to illustrate a complicated thing with a natural visual (the <em>pivot-branch sampling</em>).</p>
<h3 id="go-ahead-and-invent-a-helpful-taxonomy" tabindex="-1">Go Ahead and Invent a Helpful Taxonomy </h3>
<p>The first reviewers were confused about our dataset. Was it interesting or valuable?</p>
<p>I had shot myself in the foot with crappy writing that situated the contribution as incremental and marginally different (see abstract and intro sections above), but there’s no harm in over-correcting, right?</p>
<p>We first introduced this table—new in the revision—just to contrast example sentences from the most related datasets. This alone would have been great because <a href="https://maxwellforbes.com/posts/use-examples/">examples are densely impactful brain magic</a>.</p>
<p>But one of the biggest brain blasts I had was realizing that I could simply invent helpful axes (circled) along which to compare the datasets.</p>
<p><img src="https://maxwellforbes.com/assets/posts/phd-metagame/paper-06-examples-emnlp-only-annotated-v3.moz90.jpg" alt="" loading="lazy" decoding="async" width="826" height="348" data-thumbhash-b64="ORgGC4Knhrh6dgd3d3CK9sg=" srcset="https://maxwellforbes.com/assets/eleventyImgs/1CUouNTKWs-413.jpeg 413w, https://maxwellforbes.com/assets/eleventyImgs/1CUouNTKWs-826.jpeg 826w" sizes="(max-width: 30em) 100vw, (max-width: 704px) 100vw, 704px"></p>
<p>Dataset comparison table (new in accepted version).</p>
<p>Not only are examples incredibly helpful to get a flavor of things, the taxonomy I made up helps with quantitative (ish) framing.</p>
<p>Inventing the dataset taxonomy helped free up my brain from imaginary rules. For example, the data citations wouldn’t fit in the table without destroying the alignment. What to do? Well, I simply moved them to the caption. Can you do that? Nobody complained.</p>
<h3 id="sprinkle-in-graphics-for-variety" tabindex="-1">Sprinkle in Graphics for Variety </h3>
<p>A chart helps break up the visual rhythm of a paper. Plus, it can demonstrate a property that’s otherwise hard to grasp. (Here: that we have longer text than other datasets.)</p>

        
        
<p><span>
<span>Top:</span>
<span>Left:</span>
</span>
Rejected dataset stats.<span>
<span>Bottom:</span>
<span>Right:</span>
</span>
Accepted dataset stats.</p>
<p>Don’t forget, we’re still putting the takeaway message at the end of the caption.</p>
<h3 id="make-your-contribution-shine" tabindex="-1">Make Your Contribution Shine </h3>
<p>I had done a bad job highlighting how interesting the model was. In the revision, I not only drew out the components we ablated (yellow, red), but I used color to link them to the results table later in the paper. As a bonus, we now have warm colors (yellow, red) for the encoder and cool colors (blue, green) for the decoder.</p>

        <div>
            <p><img src="https://maxwellforbes.com/assets/posts/phd-metagame/paper-08-model-acl.moz90.jpg" loading="lazy" decoding="async" width="813" height="356" data-thumbhash-b64="NggGA4KWpraEePiXBm5QwFc=" srcset="https://maxwellforbes.com/assets/eleventyImgs/1mTfhrkpJa-407.jpeg 407w, https://maxwellforbes.com/assets/eleventyImgs/1mTfhrkpJa-813.jpeg 813w" sizes="(max-width: 30em) 100vw, (max-width: 704px) 100vw, 704px">
            </p>
        </div>
        
        <div>
            <p><img src="https://maxwellforbes.com/assets/posts/phd-metagame/paper-09-model-emnlp-annotated.moz90.jpg" loading="lazy" decoding="async" width="807" height="455" data-thumbhash-b64="NRgCFIKZ0tuTZpdvm65LgAkiiA==" srcset="https://maxwellforbes.com/assets/eleventyImgs/b0VIsbgV7v-404.jpeg 404w, https://maxwellforbes.com/assets/eleventyImgs/b0VIsbgV7v-807.jpeg 807w" sizes="(max-width: 30em) 100vw, (max-width: 704px) 100vw, 704px">
            </p>
        </div>
        
<p><strong>Top:</strong> Rejected model figure. <strong>Bottom:</strong> Accepted model figure.</p>
<p>I help the reader out by telling them in advance what configuration of the model works best as the takeaway sentence of the caption. This is another good trick to remember: don’t withhold information to surprise readers. They like to know early and often. I am guilty of this and it’s still a hard habit to break.</p>
<h3 id="delete-stuff-around-the-23-mark" tabindex="-1">Delete Stuff Around the 2/3 Mark </h3>
<p>Several changes above take up more space. Where do we cut?</p>
<p>In an eight-page paper, pages five through seven probably contain good candidates.</p>
<p>Fortunately, we already had a figure with an excessive number of outputs. I’m a <a href="https://maxwellforbes.com/posts/use-examples/#example-outputs">big fan</a> of showing your system’s outputs, so I’d included nine scenarios (i.e., eighteen total photos and paragraphs). This is great, but trimming to six scenarios still leaves plenty. Plus it let us be pickier with which ones were included.<sup><a href="#fn11" id="fnref11">11</a></sup></p>
<p><img src="https://maxwellforbes.com/assets/posts/phd-metagame/paper-10-outputs-acl-annotated.moz90.jpg" alt="" loading="lazy" decoding="async" width="835" height="798" data-thumbhash-b64="8hcGD4KYl/aSiHh7hkhnmIeEk3UIc74P" srcset="https://maxwellforbes.com/assets/eleventyImgs/lqFK_OJWcv-418.jpeg 418w, https://maxwellforbes.com/assets/eleventyImgs/lqFK_OJWcv-835.jpeg 835w" sizes="(max-width: 30em) 100vw, (max-width: 704px) 100vw, 704px"></p>
<p>Example outputs. Top row removed in the resubmission.</p>
<p>Notice there’s no takeaway sentence here. Rules are guidelines. If the takeaway feels belabored and out-of-place, omit it.</p>
<h3 id="add-everything-you-might-ask-for" tabindex="-1">Add Everything You Might Ask For </h3>
<p>This is where the <em>thinking in reverse</em> part comes in at full force. Think of the most common reviewer complaints and avoid them.</p>
<p>The easiest reasons reviewers could give to reject you were:</p>
<ul>
<li>lack of baselines</li>
<li>lack of ablations</li>
<li>lack of human evaluation</li>
</ul>
<p>So, add those things.</p>

        <div>
            <p><img src="https://maxwellforbes.com/assets/posts/phd-metagame/paper-11-results-acl.moz90.jpg" loading="lazy" decoding="async" width="812" height="283" data-thumbhash-b64="OwgCAoCVl4eviMiJAAAAAAA=" srcset="https://maxwellforbes.com/assets/eleventyImgs/zK_7WVSYJw-406.jpeg 406w, https://maxwellforbes.com/assets/eleventyImgs/zK_7WVSYJw-812.jpeg 812w" sizes="(max-width: 30em) 100vw, (max-width: 704px) 100vw, 704px">
            </p>
        </div>
        
        <div>
            <p><img src="https://maxwellforbes.com/assets/posts/phd-metagame/paper-11-results-emnlp-annotated.moz90.jpg" loading="lazy" decoding="async" width="806" height="366" data-thumbhash-b64="9hcGC4J1lqd/d5ealkLvmQs=" srcset="https://maxwellforbes.com/assets/eleventyImgs/NsjqcUwM5N-403.jpeg 403w, https://maxwellforbes.com/assets/eleventyImgs/NsjqcUwM5N-806.jpeg 806w" sizes="(max-width: 30em) 100vw, (max-width: 704px) 100vw, 704px">
            </p>
        </div>
        
<p><strong>Top:</strong> Rejected results. <strong>Bottom:</strong> Accepted results.</p>
<p>My favorite part is in the takeaway (yellow), we highlight and explain a weak-looking result (blue).</p>
<p>The baselines and ablations took relatively little work to run and probably improved the actual science contribution (more on that soon).</p>
<p>We already had a dream human evaluation, which is getting people to use the captions for an objective task (i.e., can you pick which animal is which?) rather than scoring them on subjective quality metrics (e.g., how fluent is the text 1–5?). No changes there.</p>
<h3 id="go-a-little-overboard" tabindex="-1">Go a Little Overboard </h3>
<p>Somehow we made space for an enormous table of ablations. Running lots of ablations<sup><a href="#fn12" id="fnref12">12</a></sup> is a luxury of having a small dataset.<sup><a href="#fn13" id="fnref13">13</a></sup></p>
<p><img src="https://maxwellforbes.com/assets/posts/phd-metagame/paper-12-ablations-emnlp-only-annotated.moz90.jpg" alt="" loading="lazy" decoding="async" width="823" height="547" data-thumbhash-b64="OggCDYLmyUWfVZdgd/d5Zv6QnKB5" srcset="https://maxwellforbes.com/assets/eleventyImgs/w2Mp4edgr_-412.jpeg 412w, https://maxwellforbes.com/assets/eleventyImgs/w2Mp4edgr_-823.jpeg 823w" sizes="(max-width: 30em) 100vw, (max-width: 704px) 100vw, 704px"></p>
<p>Ablations table (new in the revision.)</p>
<p>You don’t have to go overboard in the ablations. Just maybe somewhere. In future papers, I went overboard in the appendix. Including lots of information (tastefully) shows that you really care and that you did a lot of work.</p>
<h3 id="the-three-sentence-conclusion" tabindex="-1">The Three-Sentence Conclusion </h3>
<p>To revise the conclusion, distill the advice from the abstract and introduction. Also, remove all the framing. We’re left with a concrete, three-sentence highlight reel.</p>

        
        
<p><span>
<span>Top:</span>
<span>Left:</span>
</span>
Rejected conclusion.<span>
<span>Bottom:</span>
<span>Right:</span>
</span>
Accepted conclusion.</p>
<p>Normal writing advice would say something like this: Write your conclusion using three sentences:</p>
<ol>
<li>What do we do?</li>
<li>Why is it great?</li>
<li>Why does it matter?</li>
</ol>
<p>But check out the rejected conclusion. It (roughly) follows this structure too! The real improvement in the revision is specificity.</p>
<h2 id="the-science-thing-was-improved" tabindex="-1">The Science Thing Was Improved </h2>
<p>After making these mostly aesthetic revisions and seeing the paper accepted with dramatically higher scores, the initial thrill inevitably wore off. I grew more cynical of science. While we had improved the framing of our work, I thought, the core <em>science thing</em> we achieved was the same—the dataset, the model, the human evaluation, and the overall task framing itself (which is the hardest part).</p>
<p>Now, I believe such seemingly-surface dressings actually strengthen the underlying <em>science thing.</em> Let me try to convince you why.</p>
<p>The primary objects of modern science are research papers. Research papers are acts of communication. Few people will actually download and use our dataset. Nobody will download and use our model—they can’t, it’s locked inside Google’s proprietary stack.<sup><a href="#fn14" id="fnref14">14</a></sup> But anyone who reads our paper could learn from what we did, and all the revisions to clarity and completeness improve how much they can learn per minute spent reading. And it’s not just a pace thing, there’s a threshold of clarity that divides <em>learned nothing</em> from <em>got at least one new idea.</em></p>
<p>Science is communication.<sup><a href="#fn15" id="fnref15">15</a></sup> Dramatically improving communication improves the science.</p>
<blockquote>
<p>Aside: The idea of ‘making a reader want to read more’ has an unexpected link to game development. You’d think there’d be no need for such antics in a scientific research paper, yet dull obtuse prose can scare off readers, obscure the message, and deflate the contribution’s impact. Getting readers to the end—at least of page 1—is a necessary goal to optimize for. Just so with game design and ‘hooks.’ Games employ several hooks to draw players along, which might quickly be lumped into: stories build tension, todo lists beg completion, and ‘number goes up.’ Omitting these entirely robs a game of ‘stickiness,’<sup><a href="#fn16" id="fnref16">16</a></sup> leading players to grow bored and stop early. In both papers and games, we must learn to make the object sufficiently engaging so that its consumer is driven to experience the bulk of our creation.</p>
</blockquote>
<h2 id="appendix:-full-pdfs" tabindex="-1">Appendix: Full PDFs </h2>
<p>If you’d like to check out the original, raw PDFs that we submitted, they’re available for download here. The appendices (i.e., supplementary material) are nearly identical, but I’ve also included them for completeness.</p>
<ul>
<li><a href="https://maxwellforbes.com/assets/posts/phd-metagame/ACL_2019___Visually_Grounded_Comparative_Language_Generation.pdf">(Rejected) ACL 2019 submission</a> and <a href="https://maxwellforbes.com/assets/posts/phd-metagame/_SUPPL__ACL_2019___Visually_Grounded_Comparative_Language_Generation.pdf">appendix</a></li>
<li><a href="https://maxwellforbes.com/assets/posts/phd-metagame/EMNLP_2019___Neural_Naturalist_-_submission.pdf">(Accepted) EMNLP 2019 submission</a> and <a href="https://maxwellforbes.com/assets/posts/phd-metagame/EMNLP_2019___Neural_Naturalist__Supplementary_Material_.pdf">appendix</a></li>
</ul>
<section>
<p>Footnotes</p>
<hr>
<ol>
<li id="fn1"><p>Review scores spanned 1–5, with 5 = “consider for best paper,” and 3 = “weak accept.” The conferences were both of equal prestige (ACL and EMNLP respectively). Also, I use “I” for simplicity, but as always, this was work done with coauthors. <a href="#fnref1">↩︎</a></p>
</li>
<li id="fn2"><p>Please do good work before optimizing your paper. I’m assuming in this post that you are doing quality research, and you want it to be published to further your career. You need to get past the gatekeeping reviewers. In other words, please use this process for good and not evil. But if you do use it for evil, it’s not a big deal either. Another ignored paper will be in a conference instead of just on Arxiv. <a href="#fnref2">↩︎</a></p>
</li>
<li id="fn3"><p>I added “Figure 1,” but I stand by my revision. Thanks to Kenneth Marino and David Freire for finding the source of this quote. Jitendra’s talk is great—I watched it after writing the first draft of this and couldn’t believe how much overlap there was! (I never saw his talk, but someone who went told me about that quote.) Also, aside, don’t get hung up on senior advisors thinking they actually spend as much time working on the title as you do writing the rest of the paper. Yes the title is really, really important, but they don’t. Let them think they do. <a href="#fnref3">↩︎</a></p>
</li>
<li id="fn4"><p>Its bird’s-eye (ahem) view. <a href="#fnref4">↩︎</a></p>
</li>
<li id="fn5"><p>It’s OK if so, but it’s a different vibe, and probably harder to pull off—more in line with an opinion piece. <a href="#fnref5">↩︎</a></p>
</li>
<li id="fn6"><p>Having now watched Jitendra’s talk (linked in the quote above), he articulates this brilliantly: the title should “evoke the key concept of the paper” and “be memorable.” But my favorite part: “think about it in terms of the conditional entropy;” your title should only be able to describe your paper and no one else’s (at a conference). <a href="#fnref6">↩︎</a></p>
</li>
<li id="fn7"><p>I must point out again that your point will be so obvious to you because it’s why you spent hours making the figure, but a new reader may barely spend enough time looking at your thing to understand what the axes are. Help them out. Even stuff like “higher is better” is helpful unless completely trivial. <a href="#fnref7">↩︎</a></p>
</li>
<li id="fn8"><p>E.g., check this one from <em><a href="https://arxiv.org/pdf/2107.01294">Scarecrow</a> (Dou &amp; me et al., 2022)</em> <img src="https://maxwellforbes.com/assets/posts/phd-metagame/paper-02-aside-takeaway-example.moz90.jpg" alt="" loading="lazy" decoding="async" width="556" height="601" data-thumbhash-b64="9wcGBwBUpfp9hZebeKaKuYeG+Ll7oEcC" srcset="https://maxwellforbes.com/assets/eleventyImgs/2GWTAXXnjl-278.jpeg 278w, https://maxwellforbes.com/assets/eleventyImgs/2GWTAXXnjl-556.jpeg 556w" sizes="(max-width: 30em) 100vw, (max-width: 704px) 100vw, 704px"> This is a great example because the table’s interpretation is so complicated that even I (who wrote it) had forgotten what the takeaway was supposed to be a few years later, and would not have easily rediscovered it.  <a href="#fnref8">↩︎</a></p>
</li>
<li id="fn9"><p>Why does do we feel betrayed? I think because there’s an implicit promise that if you’re talking about something, your paper is going to address it. So if you’re outlining broad swaths of a field, even if in an attempt to just situate your work, it can come across as implying that <em>you’re contributing to</em> this whole grand situation. There’s a delicate balance to strike. Some context in the intro or related work is often necessary. <a href="#fnref9">↩︎</a></p>
</li>
<li id="fn10"><p>As with everything, strike a balance. Engaging writing and very unique hooks—e.g., having the phrases ‘citizen science’ and ‘biodiversity’ in an NLP paper—must come as sprinkles on top of a solid contribution that appropriately satisfies the community’s expectations. <a href="#fnref10">↩︎</a></p>
</li>
<li id="fn11"><p>I think the other place we saved the most space was in the qualitative analysis. I could probably write eight pages of only qualitative model analysis, so I always end up with too much in the first draft. <a href="#fnref11">↩︎</a></p>
</li>
<li id="fn12"><p>The blind bolding of higher numbers without statistical significance tests is truly heinous, I know. I hope somebody has standardized tests that you run on output metrics by now to do this. (Just kidding, I’m sure they haven’t.) <a href="#fnref12">↩︎</a></p>
</li>
<li id="fn13"><p>Also, being somewhere like Google. DeepMind wasn’t busy with the TPUs that week so we added a bunch of flags and let them go brrr. But the dataset is so small that by the time Google’s ancient behemoth cluster system had made a dashboard where I could see how the run was going, it had already ran over the whole training dataset (potentially many times, memory is failing me). <a href="#fnref13">↩︎</a></p>
</li>
<li id="fn14"><p>Even if it were open source, let me tell you from first-hand experience that getting someone’s research code to run is no small feat, especially under even marginally different conditions. <a href="#fnref14">↩︎</a></p>
</li>
<li id="fn15"><p>See <a href="https://maxwellforbes.com/posts/dont-try-to-reform-science/">Science 1 vs Science 2</a> in this essay series for more of this argument. <a href="#fnref15">↩︎</a></p>
</li>
<li id="fn16"><p>On the other hand, leaning too hard into them and using darker patterns (like gambling mechanics) can cause addiction (and bankruptcy). <a href="#fnref16">↩︎</a></p>
</li>
</ol>
</section>

    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Gemini Diffusion (641 pts)]]></title>
            <link>https://simonwillison.net/2025/May/21/gemini-diffusion/</link>
            <guid>44057820</guid>
            <pubDate>Thu, 22 May 2025 01:13:50 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://simonwillison.net/2025/May/21/gemini-diffusion/">https://simonwillison.net/2025/May/21/gemini-diffusion/</a>, See on <a href="https://news.ycombinator.com/item?id=44057820">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>



<p><strong><a href="https://deepmind.google/models/gemini-diffusion/">Gemini Diffusion</a></strong>. Another of the announcements from Google I/O yesterday was Gemini Diffusion, Google's first LLM to use diffusion (similar to image models like Imagen and Stable Diffusion) in place of transformers.</p>
<p>Google describe it like this:</p>
<blockquote>
<p>Traditional autoregressive language models generate text one word – or token – at a time. This sequential process can be slow, and limit the quality and coherence of the output.</p>
<p>Diffusion models work differently. Instead of predicting text directly, they learn to generate outputs by refining noise, step-by-step. This means they can iterate on a solution very quickly and error correct during the generation process. This helps them excel at tasks like editing, including in the context of math and code.</p>
</blockquote>
<p>The key feature then is <em>speed</em>. I made it through the waitlist and tried it out just now and <em>wow</em>, they are not kidding about it being fast.</p>
<p>In this video I prompt it with "Build a simulated chat app" and it responds at 857 tokens/second, resulting in an interactive HTML+JavaScript page (embedded in the chat tool, Claude Artifacts style) within single digit seconds.</p>


<p>The performance feels similar to <a href="https://simonwillison.net/2024/Oct/31/cerebras-coder/">the Cerebras Coder tool</a>, which used Cerebras to run Llama3.1-70b at around 2,000 tokens/second.</p>
<p>How good is the model? I've not seen any independent benchmarks yet, but Google's landing page for it promises "the performance of Gemini 2.0 Flash-Lite at 5x the speed" so presumably they think it's comparable to Gemini 2.0 Flash-Lite, one of their least expensive models.</p>
<p>Prior to this the only commercial grade diffusion model I've encountered is <a href="https://www.inceptionlabs.ai/introducing-mercury">Inception Mercury</a> back in February this year.</p>
<p><strong>Update</strong>: a correction from <a href="https://news.ycombinator.com/item?id=44057820#44057939">synapsomorphy on Hacker News</a>:</p>
<blockquote>
<p>Diffusion isn't in place of transformers, it's in place of autoregression. Prior diffusion LLMs like <a href="https://www.inceptionlabs.ai/introducing-mercury">Mercury</a> still use a transformer, but there's no causal masking, so the entire input is processed all at once and the output generation is obviously different. I very strongly suspect this is also using a transformer.</p>
</blockquote>



</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Show HN: Display any CSV file as a searchable, filterable, pretty HTML table (159 pts)]]></title>
            <link>https://github.com/derekeder/csv-to-html-table</link>
            <guid>44057612</guid>
            <pubDate>Thu, 22 May 2025 00:31:01 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/derekeder/csv-to-html-table">https://github.com/derekeder/csv-to-html-table</a>, See on <a href="https://news.ycombinator.com/item?id=44057612">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><p dir="auto"><h2 tabindex="-1" dir="auto">CSV to HTML Table</h2><a id="user-content-csv-to-html-table" aria-label="Permalink: CSV to HTML Table" href="#csv-to-html-table"></a></p>
<p dir="auto">Display any CSV file as a searchable, filterable, pretty <a href="https://www.scaler.com/topics/html/tables-in-html/" rel="nofollow">HTML table</a>. Done in 100% JavaScript.</p>
<p dir="auto">Check out the working demo: <a href="https://csv-to-html-table.netlify.app/" rel="nofollow">https://csv-to-html-table.netlify.app/</a></p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer nofollow" href="https://user-images.githubusercontent.com/919583/112696463-d7ddd000-8e53-11eb-8f0e-084794450943.png"><img src="https://user-images.githubusercontent.com/919583/112696463-d7ddd000-8e53-11eb-8f0e-084794450943.png" alt="Screen Shot 2021-03-26 at 4 53 39 PM"></a></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Usage</h2><a id="user-content-usage" aria-label="Permalink: Usage" href="#usage"></a></p>
<p dir="auto"><h4 tabindex="-1" dir="auto">1. Clone this repository (in the command line)</h4><a id="user-content-1-clone-this-repository-in-the-command-line" aria-label="Permalink: 1. Clone this repository (in the command line)" href="#1-clone-this-repository-in-the-command-line"></a></p>
<div dir="auto" data-snippet-clipboard-copy-content="git clone git@github.com:derekeder/csv-to-html-table.git
cd csv-to-html-table"><pre>git clone git@github.com:derekeder/csv-to-html-table.git
<span>cd</span> csv-to-html-table</pre></div>
<p dir="auto"><h4 tabindex="-1" dir="auto">2. Add your CSV file to the <code>data/</code> folder</h4><a id="user-content-2-add-your-csv-file-to-the-data-folder" aria-label="Permalink: 2. Add your CSV file to the data/ folder" href="#2-add-your-csv-file-to-the-data-folder"></a></p>
<p dir="auto"><h4 tabindex="-1" dir="auto">3. In <code>index.html</code> set your options in the <code>CsvToHtmlTable.init()</code> function</h4><a id="user-content-3-in-indexhtml-set-your-options-in-the-csvtohtmltableinit-function" aria-label="Permalink: 3. In index.html set your options in the CsvToHtmlTable.init() function" href="#3-in-indexhtml-set-your-options-in-the-csvtohtmltableinit-function"></a></p>
<div dir="auto" data-snippet-clipboard-copy-content="<script>
  CsvToHtmlTable.init({
    csv_path: 'data/Health Clinics in Chicago.csv', 
    element: 'table-container', 
    allow_download: true,
    csv_options: {separator: ',', delimiter: '&quot;'},
    datatables_options: {&quot;paging&quot;: false}
  });
</script>"><pre><span>&lt;</span><span>script</span><span>&gt;</span>
  <span>CsvToHtmlTable</span><span>.</span><span>init</span><span>(</span><span>{</span>
    <span>csv_path</span>: <span>'data/Health Clinics in Chicago.csv'</span><span>,</span> 
    <span>element</span>: <span>'table-container'</span><span>,</span> 
    <span>allow_download</span>: <span>true</span><span>,</span>
    <span>csv_options</span>: <span>{</span><span>separator</span>: <span>','</span><span>,</span> <span>delimiter</span>: <span>'"'</span><span>}</span><span>,</span>
    <span>datatables_options</span>: <span>{</span><span>"paging"</span>: <span>false</span><span>}</span>
  <span>}</span><span>)</span><span>;</span>
<span>&lt;/</span><span>script</span><span>&gt;</span></pre></div>
<p dir="auto"><h5 tabindex="-1" dir="auto">Available options</h5><a id="user-content-available-options" aria-label="Permalink: Available options" href="#available-options"></a></p>
<ul dir="auto">
<li><code>csv_path</code> Path to your CSV file.</li>
<li><code>element</code> The HTML element to render your table to. Defaults to <code>table-container</code></li>
<li><code>allow_download</code> if true, shows a link to download the CSV file. Defaults to <code>false</code></li>
<li><code>csv_options</code> jQuery CSV configuration. Use this if you want to use a custom <code>delimiter</code> or <code>separator</code> in your input file. See <a href="https://code.google.com/p/jquery-csv/wiki/API#$.csv.toArrays%28%29" rel="nofollow">their documentation</a>.</li>
<li><code>datatables_options</code> DataTables configuration. See <a href="http://datatables.net/reference/option/" rel="nofollow">their documentation</a>.</li>
<li><code>custom_formatting</code> <strong>New!</strong> A list of column indexes and custom functions to format your data (see below)</li>
</ul>
<p dir="auto"><h5 tabindex="-1" dir="auto">Custom formatting</h5><a id="user-content-custom-formatting" aria-label="Permalink: Custom formatting" href="#custom-formatting"></a></p>
<p dir="auto">If you want to do custom formatting for one or more column, you can pass in an array of arrays containing the index of the column and a custom function for formatting it. You can pass in multiple formatters and they will be executed in order.</p>
<p dir="auto">The custom functions must take in one parameter (the value in the cell) and return a HTML string:</p>
<p dir="auto">Example:</p>
<div dir="auto" data-snippet-clipboard-copy-content="<script>

  //my custom function that creates a hyperlink
  function format_link(link){
    if (link)
      return &quot;<a href='&quot; + link + &quot;' target='_blank'>&quot; + link + &quot;</a>&quot;;
    else
      return &quot;&quot;;
  }

  //initializing the table
  CsvToHtmlTable.init({
    csv_path: 'data/Health Clinics in Chicago.csv', 
    element: 'table-container', 
    allow_download: true,
    csv_options: {separator: ',', delimiter: '&quot;'},
    datatables_options: {&quot;paging&quot;: false},
    custom_formatting: [[4, format_link]] //execute the function on the 4th column of every row
  });
</script>"><pre><span>&lt;</span><span>script</span><span>&gt;</span>

  <span>//my custom function that creates a hyperlink</span>
  <span>function</span> <span>format_link</span><span>(</span><span>link</span><span>)</span><span>{</span>
    <span>if</span> <span>(</span><span>link</span><span>)</span>
      <span>return</span> <span>"&lt;a href='"</span> <span>+</span> <span>link</span> <span>+</span> <span>"' target='_blank'&gt;"</span> <span>+</span> <span>link</span> <span>+</span> <span>"&lt;/a&gt;"</span><span>;</span>
    <span>else</span>
      <span>return</span> <span>""</span><span>;</span>
  <span>}</span>

  <span>//initializing the table</span>
  <span>CsvToHtmlTable</span><span>.</span><span>init</span><span>(</span><span>{</span>
    <span>csv_path</span>: <span>'data/Health Clinics in Chicago.csv'</span><span>,</span> 
    <span>element</span>: <span>'table-container'</span><span>,</span> 
    <span>allow_download</span>: <span>true</span><span>,</span>
    <span>csv_options</span>: <span>{</span><span>separator</span>: <span>','</span><span>,</span> <span>delimiter</span>: <span>'"'</span><span>}</span><span>,</span>
    <span>datatables_options</span>: <span>{</span><span>"paging"</span>: <span>false</span><span>}</span><span>,</span>
    <span>custom_formatting</span>: <span>[</span><span>[</span><span>4</span><span>,</span> <span>format_link</span><span>]</span><span>]</span> <span>//execute the function on the 4th column of every row</span>
  <span>}</span><span>)</span><span>;</span>
<span>&lt;/</span><span>script</span><span>&gt;</span></pre></div>
<p dir="auto">Note that you should take care about HTML escaping to avoid <a href="https://www.owasp.org/index.php/Cross-site_Scripting_(XSS)" rel="nofollow">XSS</a> or broken layout.
jQuery has a nice function <a href="https://api.jquery.com/text/" rel="nofollow">text()</a> which safely escapes HTML from value.</p>
<p dir="auto"><h4 tabindex="-1" dir="auto">4. Run it</h4><a id="user-content-4-run-it" aria-label="Permalink: 4. Run it" href="#4-run-it"></a></p>
<p dir="auto">You can run this locally using this handy python command:</p>
<div dir="auto" data-snippet-clipboard-copy-content="python -m SimpleHTTPServer"><pre>python -m SimpleHTTPServer</pre></div>
<p dir="auto">...or with Python 3:</p>

<p dir="auto">navigate to <a href="http://localhost:8000/" rel="nofollow">http://localhost:8000/</a></p>
<p dir="auto"><h4 tabindex="-1" dir="auto">5. Deploy it</h4><a id="user-content-5-deploy-it" aria-label="Permalink: 5. Deploy it" href="#5-deploy-it"></a></p>
<p dir="auto"><strong>GitHub pages</strong> You can host your table on GitHub pages for free! Once you've made all your changes and committed them, push everything in the <code>master</code> branch to <code>gh-pages</code> which automatically enables GitHub pages.</p>
<div dir="auto" data-snippet-clipboard-copy-content="git push origin master:gh-pages"><pre>git push origin master:gh-pages</pre></div>
<p dir="auto">Then navigate to <a href="http://your-github-username.github.io/csv-to-html-table/" rel="nofollow">http://your-github-username.github.io/csv-to-html-table/</a></p>
<p dir="auto">Read more on working with <a href="https://help.github.com/articles/user-organization-and-project-pages/#project-pages">GitHub pages projects</a>.</p>
<p dir="auto"><strong>Web server</strong> This project should work on any web server. Upload this entire project (including all the <code>css</code>, <code>data</code>, <code>fonts</code> and <code>js</code> folders) to a public folder on your server using FTP.</p>
<p dir="auto"><h4 tabindex="-1" dir="auto">6. iframe it (optional)</h4><a id="user-content-6-iframe-it-optional" aria-label="Permalink: 6. iframe it (optional)" href="#6-iframe-it-optional"></a></p>
<p dir="auto">Want to embed your nifty table on your website? You can use an <a href="http://www.w3schools.com/tags/tag_iframe.asp" rel="nofollow">iframe</a>. Once you've deployed your table (above in step 5) you can link to it in an iframe right in your HTML.</p>
<div dir="auto" data-snippet-clipboard-copy-content="<iframe style=&quot;border-style: none;&quot; src=&quot;http://derekeder.github.io/csv-to-html-table/&quot; height=&quot;950&quot; width=&quot;600&quot;></iframe>"><pre><span>&lt;</span><span>iframe</span> <span>style</span>="<span>border-style: none;</span>" <span>src</span>="<span>http://derekeder.github.io/csv-to-html-table/</span>" <span>height</span>="<span>950</span>" <span>width</span>="<span>600</span>"<span>&gt;</span><span>&lt;/</span><span>iframe</span><span>&gt;</span></pre></div>
<p dir="auto"><h2 tabindex="-1" dir="auto">Dependencies</h2><a id="user-content-dependencies" aria-label="Permalink: Dependencies" href="#dependencies"></a></p>
<ul dir="auto">
<li><a href="http://getbootstrap.com/" rel="nofollow">Bootstrap 4</a> - Responsive HTML, CSS and Javascript framework</li>
<li><a href="https://jquery.com/" rel="nofollow">jQuery</a> - a fast, small, and feature-rich JavaScript library</li>
<li><a href="https://github.com/evanplaice/jquery-csv/">jQuery CSV</a> - Parse CSV (Comma Separated Values) to Javascript arrays or dictionaries.</li>
<li><a href="http://datatables.net/" rel="nofollow">DataTables</a> - add advanced interaction controls to any HTML table.</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Common issues/troubleshooting</h2><a id="user-content-common-issuestroubleshooting" aria-label="Permalink: Common issues/troubleshooting" href="#common-issuestroubleshooting"></a></p>
<p dir="auto">If your table isn't displaying any data, try the following:</p>
<ol dir="auto">
<li>Use the <a href="https://developers.google.com/chrome-developer-tools/docs/console" rel="nofollow">Chrome developer console</a> or install <a href="http://getfirebug.com/" rel="nofollow">Firebug</a> for FireFox. This will allow you to debug your javascript.</li>
<li>Open your table in the browser and open the javascript console
<ul dir="auto">
<li>Chrome developer console on a Mac: Option+Command+J</li>
<li>Chrome developer console on a PC: Control+Shift+J</li>
<li>Firebug in Firefox: Tools =&gt; Web Developer =&gt; Firebug =&gt; Open Firebug)</li>
</ul>
</li>
<li>If you do see javascript errors, the error will tell you what line it is failing on. Best to start by going there!</li>
</ol>
<p dir="auto"><h2 tabindex="-1" dir="auto">Errors / Bugs</h2><a id="user-content-errors--bugs" aria-label="Permalink: Errors / Bugs" href="#errors--bugs"></a></p>
<p dir="auto">If something is not behaving intuitively, it is a bug, and should be reported.
Report it here: <a href="https://github.com/derekeder/csv-to-html-table/issues">https://github.com/derekeder/csv-to-html-table/issues</a></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Contributors</h2><a id="user-content-contributors" aria-label="Permalink: Contributors" href="#contributors"></a></p>
<ul dir="auto">
<li><a href="http://derekeder.com/" rel="nofollow">Derek Eder</a> - primary contributor</li>
<li><a href="https://github.com/ychaouche">ychaouche</a> - <a href="https://github.com/derekeder/csv-to-html-table/pull/30" data-hovercard-type="pull_request" data-hovercard-url="/derekeder/csv-to-html-table/pull/30/hovercard">javascript tag fixes</a></li>
<li><a href="https://github.com/b-meson">Freddy Martinez</a> - <a href="https://github.com/derekeder/csv-to-html-table/pull/17" data-hovercard-type="pull_request" data-hovercard-url="/derekeder/csv-to-html-table/pull/17/hovercard">localized javascript libraries</a></li>
<li><a href="https://github.com/stokito">Sergey Ponomarev</a> - <a href="https://github.com/derekeder/csv-to-html-table/pull/60" data-hovercard-type="pull_request" data-hovercard-url="/derekeder/csv-to-html-table/pull/60/hovercard">CSV escaped in HTML output</a></li>
<li><a href="https://github.com/djibe">djibe</a> - <a href="https://github.com/djibe/csv-to-html-table">Bootstrap 4 and latest DataTables</a></li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Note on Patches/Pull Requests</h2><a id="user-content-note-on-patchespull-requests" aria-label="Permalink: Note on Patches/Pull Requests" href="#note-on-patchespull-requests"></a></p>
<ul dir="auto">
<li>Fork the project.</li>
<li>Make your feature addition or bug fix.</li>
<li>Send a pull request. Bonus points for topic branches.</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Copyright</h2><a id="user-content-copyright" aria-label="Permalink: Copyright" href="#copyright"></a></p>
<p dir="auto">Copyright (c) 2018 Derek Eder. Released under the <a href="https://github.com/derekeder/csv-to-html-table/blob/master/LICENSE">MIT License</a>.</p>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Should I Block ICMP? (111 pts)]]></title>
            <link>http://shouldiblockicmp.com/</link>
            <guid>44057219</guid>
            <pubDate>Wed, 21 May 2025 23:17:29 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="http://shouldiblockicmp.com/">http://shouldiblockicmp.com/</a>, See on <a href="https://news.ycombinator.com/item?id=44057219">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
                    <div>
                        
                        <h2>No!!</h2>
                        </div>

                    <div id="TheProblem">
                        <h4>The Problem</h4>
                        <p>Many network administrators feel that ICMP is a security risk, and should therefore always be blocked
                        at the firewall.  It is true that ICMP does have some security issues associated with it, and that a lot
                        of ICMP should be blocked.  But this is no reason to block all ICMP traffic!</p>
                        <p>ICMP has many important features; some are useful for troubleshooting, while some are essential for a
                        network to function correctly.  Here are details of some of the important ICMP traffic that you should
                        know about, and consider allowing through your network.</p>
                        <br>
                    </div>

                    <div id="EchoReqReply">
                        <h4>Echo Request and Echo Reply<br>
                            <small>IPv4 - Echo Request (Type8, Code0) and Echo Reply (Type0, Code0)<br>
                                IPv6 - Echo Request (Type128, Code0) and Echo Reply (Type129, Code0)</small>
                        </h4>
                        <p>We all know these ones - ping is one of the first troubleshooting tools that we all learn.  Yes, if
                        you enable it, it means that your host is now discoverable - but wasn't your web server already
                        listening on port 80 anyway?  Sure, block this if you really want at your border to your DMZ, but
                        blocking ping traffic inside your network isn't going to get you much, except harder troubleshooting
                        ("Can you ping your default gateway?", "No, but I never can, so that doesn't tell me anything!").</p>
                        <p>Remember you can also allow this with a given direction in mind; you could decide to let Echo
                        Requests out from your network to the Internet, and Echo Replies from the Internet to your network, but
                        not vice versa.</p>
                        <br>
                    </div>

                    <div id="FragNeed">
                        <h4>Fragmentation Needed (IPv4) / Packet Too Big (IPv6)<br>
                            <small>IPv4 - (Type3, Code4)<br>
                                IPv6 - (Type2, Code0)</small>
                        </h4>
                        <p>These ones are important.  VERY important.  They are an essential component in 
                        <a href="http://en.wikipedia.org/wiki/Path_MTU_Discovery">Path MTU Discovery</a> (PMTUD), which is an
                        essential part of TCP that allows two hosts to adjust their 
                        <a href="http://en.wikipedia.org/wiki/Maximum_segment_size">TCP Maximum Segment Size</a> (MSS) value to
                        one that will fit in the smallest 
                        <a href="http://en.wikipedia.org/wiki/Maximum_transmission_unit">MTU</a> along the path of links
                        between the two hosts.  If two hosts have a smaller MTU than their own local link on the path between
                        them, and have no means of discovering this, traffic gets silently black-holed; in other words, "you're
                        gonna have a bad time".</p>
                        <p>IPv4 packets with the 
                        <a href="http://en.wikipedia.org/wiki/IPv4#Flags">DF bit</a> set (that's most of them!), or IPv6 packets
                        (remember there's no fragmentation by routers in IPv6), that are too large for a router to transmit
                        across an interface will result in that router dropping the packet, and generating a Fragmentation
                        Needed / Packet Too Big ICMP error back to the source, which also contains the MTU of the too-small
                        link.  If this error cannot get through to the sender, then the sender will just interpret the lack of
                        ACKs from the receiver as congestion/loss and re-transmit, which of course will also get dropped.  This
                        kind of behaviour is tough to troubleshoot because of course the TCP handshakes all work fine, as they
                        are small packets, but then the session appears to stall as soon as any bulk data transmission
                        occurs.</p>
                        <p><a href="https://www.ietf.org/rfc/rfc4821.txt">RFC 4821</a> was developed to help hosts get around
                        this problem using Packetization Layer Path MTU Discovery (PLPMTUD), which discovers the path MTU by
                        incrementally increasing the MSS to try to find a suitable value for the path.  This removes the
                        dependency on ICMP, and is available in most OS network stacks, but is not as efficient as learning
                        directly what the maximum MTU should be.  So please just allow these ICMP messages through in the first
                        place, okay?</p>
                        <br>
                    </div>

                    <div id="TimeExc">
                        <h4>Time Exceeded<br>
                            <small>IPv4 - (Type11, Code0)<br>
                                IPv6 - (Type3, Code0)</small>
                        </h4>
                        <p>Traceroute is a very useful tool for troubleshooting network connections between two hosts, detailing
                        each hop on the path.  It does this by sending a packet with a TTL of 1 so that the first hop sends back
                        a Time Exceeded message (including its own source IP), then sending a packet with a TTL of 2, and so on,
                        to discover each hop on the path.</p>
                        <p>Remember when you've run it before, and you get a hop or two that can't be discovered in the middle
                        of your trace?  Or even worse, you try to traceroute to a host and *every* hop can't be discovered.
                        Annoying, right?  That's because the person running those routers (or your local firewall) decided to
                        block ICMP Time Exceeded messages.  Don't be that guy, okay?</p>
                        <br>
                    </div>

                    <div id="NDP">
                        <h4>NDP and SLAAC (IPv6)<br>
                        <small>Router Solicitation (RS) (Type133, Code0)<br>
                            Router Advertisement (RA) (Type134, Code0)<br>
                            Neighbour Solicitation (NS) (Type135, Code0)<br>
                            Neighbour Advertisement (NA) (Type136, Code0)<br>
                            Redirect (Type137, Code0)</small>
                        </h4>
                        <p>While IPv4 used 
                        <a href="http://en.wikipedia.org/wiki/Address_Resolution_Protocol">Address Resolution Protocol</a> (ARP)
                        for layer 2 to 3 mappings, IPv6 takes a different approach, in the form of 
                        <a href="http://en.wikipedia.org/wiki/Neighbor_Discovery_Protocol">Neighbour Discovery Protocol</a>
                        (NDP).  NDP provides many functions, including router discovery, prefix discovery, address resolution,
                        and many more besides.  In addition to NDP, 
                        <a href="http://en.wikipedia.org/wiki/IPv6_address#Stateless_address_autoconfiguration">StateLess Address AutoConfiguration</a> 
                        (SLAAC) allows a host to be dynamically configured on the network, similar in concept to DHCP (although
                        DHCPv6 does exist for finer-grained control).</p>
                        <p>These five ICMP types should be permitted within your network (not across your border) in order for
                        these features of IPv6 to function correctly.</p>
                        <br>
                    </div>

                    <div id="RateLim">
                        <h4>A Word About Rate Limiting</h4>
                        <p>While ICMP messages like the ones covered on this page can be very useful, remember that generating
                        all of these messages takes CPU time on your routers, and generates traffic.  Do you really expect that
                        you should be getting 1000 pings a second through your firewall in a normal situation?  Would that be
                        considered legitimate traffic if you saw it?  Nope, probably not.  Rate limit all of these ICMP traffic
                        types as you see fit for your network; it's a good line of defence that should not be ignored.</p>
                        <br>
                    </div>

                    <div id="Read">
                        <h4>Read, Research, Understand</h4>
                        <p>Given that the "to block or not to block" discussion for ICMP seems to always result in confusion,
                        anger, and borderline fanatical disagreements, go ahead and read up on the topic yourself.  Spend time
                        understanding it as fully as you can; there are plenty of links throughout this page alone.  Then you
                        can form your own opinion and make an informed choice about what is best for your network.</p>
                        <br>
                    </div>

                    
                    <!-- Ad unit 1 -->
                    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[I have tinnitus. I don't recommend it (139 pts)]]></title>
            <link>https://blog.greg.technology/2025/05/20/tinnitus.html</link>
            <guid>44057044</guid>
            <pubDate>Wed, 21 May 2025 22:50:10 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://blog.greg.technology/2025/05/20/tinnitus.html">https://blog.greg.technology/2025/05/20/tinnitus.html</a>, See on <a href="https://news.ycombinator.com/item?id=44057044">Hacker News</a></p>
<div id="readability-page-1" class="page"><div aria-label="Content">
        <article itemscope="" itemtype="http://schema.org/BlogPosting">
  

  <div itemprop="articleBody"><p>i went to a show in november of last year (it’s may right now) and ever since, i have tinnitus. i don’t recommend it.</p>

<p>i’ve been going to electronic music shows for a while and had been very careless with my ears - because it felt like i would always be okay. after the show, especially in the morning, you’d have the next-day tinnitus, but then it would go away.</p>

<p>of course, this one never went away. was i closer to the speakers that time, was the music louder, was it something about the frequencies. was it all of the accumulated careless listening.</p>

<p>i’ll say three things:</p>

<ol>
  <li>
    <p>if you attended a show at a venue that had cool lights, but it turned out that some people lost their sight because of the cool lasers there, the venue would be shut down, there would be an investigation, there would be lawsuits, etc.</p>

    <p>however, if you go to a venue today and the sound is so loud that people become permanently disabled because of that… as far as i know, nothing happens. why?</p>
  </li>
  <li>
    <p>i’ve been weirdly lucky in that - in addition to the tinnitus - it now physically hurts when there’s a loud sound. loud sounds didn’t use to hurt - so now, i am one of those people that plugs their ears when an emergency vehicle goes by with the siren blaring. plugging ears used to feel ridiculous to me (like, come on, it’s just a siren), but now it hurts and i get it.</p>

    <p>so there’s a blessing there of course (i <em>feel</em> when a sound is “too” loud), but i still wish i didn’t have tinnitus.</p>
  </li>
  <li>
    <p>some of my argentian friends and i joke about my very “dad” ways around them (it’s a love language, what do you want from me): i remind them to wear their helmets, and i myself have started to wear a reflective vest (in addition to a helmet) when biking. they call this “seguridad” which is both spanish for security and has “dad” at the end which is very funny. it’s a good nickname.</p>

    <p>so yeah, just to be that seguridad on you - do wear something in your ears when going to shows. do ride with a helmet. and absolutely don’t fuck around with lasers, especially the stupid green ones that people wave around. might as well wave an ak-47 around.</p>
  </li>
</ol>

<p>it is frighteningly easy to permanently injure yourself and become disabled. and it will leave you with sadness and regret and you will keep wondering - was it inevitable. you might as well protect yourself.</p>

<p>xx</p>
</div>
</article>

      </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[The Machine Stops (1909) (106 pts)]]></title>
            <link>https://standardebooks.org/ebooks/e-m-forster/short-fiction/text/the-machine-stops</link>
            <guid>44056407</guid>
            <pubDate>Wed, 21 May 2025 21:18:07 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://standardebooks.org/ebooks/e-m-forster/short-fiction/text/the-machine-stops">https://standardebooks.org/ebooks/e-m-forster/short-fiction/text/the-machine-stops</a>, See on <a href="https://news.ycombinator.com/item?id=44056407">Hacker News</a></p>
<div id="readability-page-1" class="page"><div epub:type="bodymatter z3998:fiction">
		<article id="the-machine-stops" epub:type="se:short-story">
			
			<section id="the-machine-stops-1" epub:type="chapter">
				<hgroup>
					<h3>
						<span epub:type="label">Part</span>
						<span epub:type="ordinal z3998:roman">I</span>
					</h3>
					<p epub:type="title">The Airship</p>
				</hgroup>
				<p>Imagine, if you can, a small room, hexagonal in shape, like the cell of a bee. It is lighted neither by window nor by lamp, yet it is filled with a soft radiance. There are no apertures for ventilation, yet the air is fresh. There are no musical instruments, and yet, at the moment that my meditation opens, this room is throbbing with melodious sounds. An armchair is in the centre, by its side a reading-desk⁠—that is all the furniture. And in the armchair there sits a swaddled lump of flesh⁠—a woman, about five feet high, with a face as white as a fungus. It is to her that the little room belongs.</p>
				<p>An electric bell rang.</p>
				<p>The woman touched a switch and the music was silent.</p>
				<p>“I suppose I must see who it is,” she thought, and set her chair in motion. The chair, like the music, was worked by machinery, and it rolled her to the other side of the room, where the bell still rang importunately.</p>
				<p>“Who is it?” she called. Her voice was irritable, for she had been interrupted often since the music began. She knew several thousand people; in certain directions human intercourse had advanced enormously.</p>
				<p>But when she listened into the receiver, her white face wrinkled into smiles, and she said:</p>
				<p>“Very well. Let us talk, I will isolate myself. I do not expect anything important will happen for the next five minutes⁠—for I can give you fully five minutes, Kuno. Then I must deliver my lecture on ‘Music during the Australian Period.’ ”</p>
				<p>She touched the isolation knob, so that no one else could speak to her. Then she touched the lighting apparatus, and the little room was plunged into darkness.</p>
				<p>“Be quick!” she called, her irritation returning. “Be quick, Kuno; here I am in the dark wasting my time.”</p>
				<p>But it was fully fifteen seconds before the round plate that she held in her hands began to glow. A faint blue light shot across it, darkening to purple, and presently she could see the image of her son, who lived on the other side of the earth, and he could see her.</p>
				<p>“Kuno, how slow you are.”</p>
				<p>He smiled gravely.</p>
				<p>“I really believe you enjoy dawdling.”</p>
				<p>“I have called you before, mother, but you were always busy or isolated. I have something particular to say.”</p>
				<p>“What is it, dearest boy? Be quick. Why could you not send it by pneumatic post?”</p>
				<p>“Because I prefer saying such a thing. I want⁠—”</p>
				<p>“Well?”</p>
				<p>“I want you to come and see me.”</p>
				<p>Vashti watched his face in the blue plate.</p>
				<p>“But I can see you!” she exclaimed. “What more do you want?”</p>
				<p>“I want to see you not through the Machine,” said Kuno. “I want to speak to you not through the wearisome Machine.”</p>
				<p>“Oh, hush!” said his mother, vaguely shocked. “You mustn’t say anything against the Machine.”</p>
				<p>“Why not?”</p>
				<p>“One mustn’t.”</p>
				<p>“You talk as if a god had made the Machine,” cried the other. “I believe that you pray to it when you are unhappy. Men made it, do not forget that. Great men, but men. The Machine is much, but it is not everything. I see something like you in this plate, but I do not see you. I hear something like you through this telephone, but I do not hear you. That is why I want you to come. Come and stop with me. Pay me a visit, so that we can meet face to face, and talk about the hopes that are in my mind.”</p>
				<p>She replied that she could scarcely spare the time for a visit.</p>
				<p>“The airship barely takes two days to fly between me and you.”</p>
				<p>“I dislike airships.”</p>
				<p>“Why?”</p>
				<p>“I dislike seeing the horrible brown earth, and the sea, and the stars when it is dark. I get no ideas in an airship.”</p>
				<p>“I do not get them anywhere else.”</p>
				<p>“What kind of ideas can the air give you?”</p>
				<p>He paused for an instant.</p>
				<p>“Do you not know four big stars that form an oblong, and three stars close together in the middle of the oblong, and hanging from these stars, three other stars?”</p>
				<p>“No, I do not. I dislike the stars. But did they give you an idea? How interesting; tell me.”</p>
				<p>“I had an idea that they were like a man.”</p>
				<p>“I do not understand.”</p>
				<p>“The four big stars are the man’s shoulders and his knees. The three stars in the middle are like the belts that men wore once, and the three stars hanging are like a sword.”</p>
				<p>“A sword?”</p>
				<p>“Men carried swords about with them, to kill animals and other men.”</p>
				<p>“It does not strike me as a very good idea, but it is certainly original. When did it come to you first?”</p>
				<p>“In the airship⁠—” He broke off, and she fancied that he looked sad. She could not be sure, for the Machine did not transmit <em>nuances</em> of expression. It only gave a general idea of people⁠—an idea that was good enough for all practical purposes, Vashti thought. The imponderable bloom, declared by a discredited philosophy to be the actual essence of intercourse, was rightly ignored by the Machine, just as the imponderable bloom of the grape was ignored by the manufacturers of artificial fruit. Something “good enough” had long since been accepted by our race.</p>
				<p>“The truth is,” he continued, “that I want to see these stars again. They are curious stars. I want to see them not from the airship, but from the surface of the earth, as our ancestors did, thousands of years ago. I want to visit the surface of the earth.”</p>
				<p>She was shocked again.</p>
				<p>“Mother, you must come, if only to explain to me what is the harm of visiting the surface of the earth.”</p>
				<p>“No harm,” she replied, controlling herself. “But no advantage. The surface of the earth is only dust and mud, no life remains on it, and you would need a respirator, or the cold of the outer air would kill you. One dies immediately in the outer air.”</p>
				<p>“I know; of course I shall take all precautions.”</p>
				<p>“And besides⁠—”</p>
				<p>“Well?”</p>
				<p>She considered, and chose her words with care. Her son had a queer temper, and she wished to dissuade him from the expedition.</p>
				<p>“It is contrary to the spirit of the age,” she asserted.</p>
				<p>“Do you mean by that, contrary to the Machine?”</p>
				<p>“In a sense, but⁠—”</p>
				<p>His image in the blue plate faded.</p>
				<p>“Kuno!”</p>
				<p>He had isolated himself.</p>
				<p>For a moment Vashti felt lonely.</p>
				<p>Then she generated the light, and the sight of her room, flooded with radiance and studded with electric buttons, revived her. There were buttons and switches everywhere⁠—buttons to call for food, for music, for clothing. There was the hot-bath button, by pressure of which a basin of (imitation) marble rose out of the floor, filled to the brim with a warm deodorised liquid. There was the cold-bath button. There was the button that produced literature. And there were of course the buttons by which she communicated with her friends. The room, though it contained nothing, was in touch with all that she cared for in the world.</p>
				<p>Vashti’s next move was to turn off the isolation-switch, and all the accumulations of the last three minutes burst upon her. The room was filled with the noise of bells, and speaking-tubes. What was the new food like? Could she recommend it? Had she had any ideas lately? Might one tell her one’s own ideas? Would she make an engagement to visit the public nurseries at an early date?⁠—say this day month.</p>
				<p>To most of these questions she replied with irritation⁠—a growing quality in that accelerated age. She said that the new food was horrible. That she could not visit the public nurseries through press of engagements. That she had no ideas of her own but had just been told one⁠—that four stars and three in the middle were like a man: she doubted there was much in it. Then she switched off her correspondents, for it was time to deliver her lecture on Australian music.</p>
				<p>The clumsy system of public gatherings had been long since abandoned; neither Vashti nor her audience stirred from their rooms. Seated in her armchair she spoke, while they in their armchairs heard her, fairly well, and saw her, fairly well. She opened with a humorous account of music in the pre-Mongolian epoch, and went on to describe the great outburst of song that followed the Chinese conquest. Remote and primaeval as were the methods of I-San-So and the Brisbane school, she yet felt (she said) that study of them might repay the musician of today: they had freshness; they had, above all, ideas.</p>
				<p>Her lecture, which lasted ten minutes, was well received, and at its conclusion she and many of her audience listened to a lecture on the sea; there were ideas to be got from the sea; the speaker had donned a respirator and visited it lately. Then she fed, talked to many friends, had a bath, talked again, and summoned her bed.</p>
				<p>The bed was not to her liking. It was too large, and she had a feeling for a small bed. Complaint was useless, for beds were of the same dimension all over the world, and to have had an alternative size would have involved vast alterations in the Machine. Vashti isolated herself⁠—it was necessary, for neither day nor night existed under the ground⁠—and reviewed all that had happened since she had summoned the bed last. Ideas? Scarcely any. Events⁠—was Kuno’s invitation an event?</p>
				<p>By her side, on the little reading-desk, was a survival from the ages of litter⁠—one book. This was the Book of the Machine. In it were instructions against every possible contingency. If she was hot or cold or dyspeptic or at loss for a word, she went to the book, and it told her which button to press. The Central Committee published it. In accordance with a growing habit, it was richly bound.</p>
				<p>Sitting up in the bed, she took it reverently in her hands. She glanced round the glowing room as if someone might be watching her. Then, half ashamed, half joyful, she murmured “O Machine! O Machine!” and raised the volume to her lips. Thrice she kissed it, thrice inclined her head, thrice she felt the delirium of acquiescence. Her ritual performed, she turned to page 1,367, which gave the times of the departure of the airships from the island in the southern hemisphere, under whose soil she lived, to the island in the northern hemisphere, whereunder lived her son.</p>
				<p>She thought, “I have not the time.”</p>
				<p>She made the room dark and slept; she awoke and made the room light; she ate and exchanged ideas with her friends, and listened to music and attended lectures; she made the room dark and slept. Above her, beneath her, and around her, the Machine hummed eternally; she did not notice the noise, for she had been born with it in her ears. The earth, carrying her, hummed as it sped through silence, turning her now to the invisible sun, now to the invisible stars. She awoke and made the room light.</p>
				<p>“Kuno!”</p>
				<p>“I will not talk to you,” he answered, “until you come.”</p>
				<p>“Have you been on the surface of the earth since we spoke last?”</p>
				<p>His image faded.</p>
				<p>Again she consulted the book. She became very nervous and lay back in her chair palpitating. Think of her as without teeth or hair. Presently she directed the chair to the wall, and pressed an unfamiliar button. The wall swung apart slowly. Through the opening she saw a tunnel that curved slightly, so that its goal was not visible. Should she go to see her son, here was the beginning of the journey.</p>
				<p>Of course she knew all about the communication-system. There was nothing mysterious in it. She would summon a car and it would fly with her down the tunnel until it reached the lift that communicated with the airship station: the system had been in use for many, many years, long before the universal establishment of the Machine. And of course she had studied the civilisation that had immediately preceded her own⁠—the civilisation that had mistaken the functions of the system, and had used it for bringing people to things, instead of for bringing things to people. Those funny old days, when men went for change of air instead of changing the air in their rooms! And yet⁠—she was frightened of the tunnel: she had not seen it since her last child was born. It curved⁠—but not quite as she remembered; it was brilliant⁠—but not quite as brilliant as a lecturer had suggested. Vashti was seized with the terrors of direct experience. She shrank back into the room, and the wall closed up again.</p>
				<p>“Kuno,” she said, “I cannot come to see you. I am not well.”</p>
				<p>Immediately an enormous apparatus fell on to her out of the ceiling, a thermometer was automatically inserted between her lips, a stethoscope was automatically laid upon her heart. She lay powerless. Cool pads soothed her forehead. Kuno had telegraphed to her doctor.</p>
				<p>So the human passions still blundered up and down in the Machine. Vashti drank the medicine that the doctor projected into her mouth, and the machinery retired into the ceiling. The voice of Kuno was heard asking how she felt.</p>
				<p>“Better.” Then with irritation: “But why do you not come to me instead?”</p>
				<p>“Because I cannot leave this place.”</p>
				<p>“Why?”</p>
				<p>“Because, any moment, something tremendous may happen.”</p>
				<p>“Have you been on the surface of the earth yet?”</p>
				<p>“Not yet.”</p>
				<p>“Then what is it?”</p>
				<p>“I will not tell you through the Machine.”</p>
				<p>She resumed her life.</p>
				<p>But she thought of Kuno as a baby, his birth, his removal to the public nurseries, her one visit to him there, his visits to her⁠—visits which stopped when the Machine had assigned him a room on the other side of the earth. “Parents, duties of,” said the book of the Machine, “cease at the moment of birth. <abbr epub:type="z3998:initialism">P.</abbr> 422,327,483.” True, but there was something special about Kuno⁠—indeed there had been something special about all her children⁠—and, after all, she must brave the journey if he desired it. And “something tremendous might happen.” What did that mean? The nonsense of a youthful man, no doubt, but she must go. Again she pressed the unfamiliar button, again the wall swung back, and she saw the tunnel that curved out of sight. Clasping the Book, she rose, tottered on to the platform, and summoned the car. Her room closed behind her: the journey to the northern hemisphere had begun.</p>
				<p>Of course it was perfectly easy. The car approached and in it she found armchairs exactly like her own. When she signalled, it stopped, and she tottered into the lift. One other passenger was in the lift, the first fellow creature she had seen face to face for months. Few travelled in these days, for, thanks to the advance of science, the earth was exactly alike all over. Rapid intercourse, from which the previous civilisation had hoped so much, had ended by defeating itself. What was the good of going to Peking when it was just like Shrewsbury? Why return to Shrewsbury when it would be just like Peking? Men seldom moved their bodies; all unrest was concentrated in the soul.</p>
				<p>The airship service was a relic from the former age. It was kept up, because it was easier to keep it up than to stop it or to diminish it, but it now far exceeded the wants of the population. Vessel after vessel would rise from the vomitories of Rye or of Christchurch (I use the antique names), would sail into the crowded sky, and would draw up at the wharves of the south⁠—empty. So nicely adjusted was the system, so independent of meteorology, that the sky, whether calm or cloudy, resembled a vast kaleidoscope whereon the same patterns periodically recurred. The ship on which Vashti sailed started now at sunset, now at dawn. But always, as it passed above Rheims, it would neighbour the ship that served between Helsingfors and the Brazils, and, every third time it surmounted the Alps, the fleet of Palermo would cross its track behind. Night and day, wind and storm, tide and earthquake, impeded man no longer. He had harnessed Leviathan. All the old literature, with its praise of Nature, and its fear of Nature, rang false as the prattle of a child.</p>
				<p>Yet as Vashti saw the vast flank of the ship, stained with exposure to the outer air, her horror of direct experience returned. It was not quite like the airship in the cinematophote. For one thing it smelt⁠—not strongly or unpleasantly, but it did smell, and with her eyes shut she should have known that a new thing was close to her. Then she had to walk to it from the lift, had to submit to glances from the other passengers. The man in front dropped his Book⁠—no great matter, but it disquieted them all. In the rooms, if the Book was dropped, the floor raised it mechanically, but the gangway to the airship was not so prepared, and the sacred volume lay motionless. They stopped⁠—the thing was unforeseen⁠—and the man, instead of picking up his property, felt the muscles of his arm to see how they had failed him. Then someone actually said with direct utterance: “We shall be late”⁠—and they trooped on board, Vashti treading on the pages as she did so.</p>
				<p>Inside, her anxiety increased. The arrangements were old-fashioned and rough. There was even a female attendant, to whom she would have to announce her wants during the voyage. Of course a revolving platform ran the length of the boat, but she was expected to walk from it to her cabin. Some cabins were better than others, and she did not get the best. She thought the attendant had been unfair, and spasms of rage shook her. The glass valves had closed, she could not go back. She saw, at the end of the vestibule, the lift in which she had ascended going quietly up and down, empty. Beneath those corridors of shining tiles were rooms, tier below tier, reaching far into the earth, and in each room there sat a human being, eating, or sleeping, or producing ideas. And buried deep in the hive was her own room. Vashti was afraid.</p>
				<p>“O Machine! O Machine!” she murmured, and caressed her Book, and was comforted.</p>
				<p>Then the sides of the vestibule seemed to melt together, as do the passages that we see in dreams, the lift vanished, the Book that had been dropped slid to the left and vanished, polished tiles rushed by like a stream of water, there was a slight jar, and the airship, issuing from its tunnel, soared above the waters of a tropical ocean.</p>
				<p>It was night. For a moment she saw the coast of Sumatra edged by the phosphorescence of waves, and crowned by lighthouses, still sending forth their disregarded beams. These also vanished, and only the stars distracted her. They were not motionless, but swayed to and fro above her head, thronging out of one skylight into another, as if the universe and not the airship was careening. And, as often happens on clear nights, they seemed now to be in perspective, now on a plane; now piled tier beyond tier into the infinite heavens, now concealing infinity, a roof limiting forever the visions of men. In either case they seemed intolerable. “Are we to travel in the dark?” called the passengers angrily, and the attendant, who had been careless, generated the light, and pulled down the blinds of pliable metal. When the airships had been built, the desire to look direct at things still lingered in the world. Hence the extraordinary number of skylights and windows, and the proportionate discomfort to those who were civilised and refined. Even in Vashti’s cabin one star peeped through a flaw in the blind, and after a few hours’ uneasy slumber, she was disturbed by an unfamiliar glow, which was the dawn.</p>
				<p>Quick as the ship had sped westwards, the earth had rolled eastwards quicker still, and had dragged back Vashti and her companions towards the sun. Science could prolong the night, but only for a little, and those high hopes of neutralising the earth’s diurnal revolution had passed, together with hopes that were possibly higher. To “keep pace with the sun,” or even to outstrip it, had been the aim of the civilisation preceding this. Racing aeroplanes had been built for the purpose, capable of enormous speed, and steered by the greatest intellects of the epoch. Round the globe they went, round and round, westward, westward, round and round, amidst humanity’s applause. In vain. The globe went eastward quicker still, horrible accidents occurred, and the Committee of the Machine, at the time rising into prominence, declared the pursuit illegal, unmechanical, and punishable by Homelessness.</p>
				<p>Of Homelessness more will be said later.</p>
				<p>Doubtless the Committee was right. Yet the attempt to “defeat the sun” aroused the last common interest that our race experienced about the heavenly bodies, or indeed about anything. It was the last time that men were compacted by thinking of a power outside the world. The sun had conquered, yet it was the end of his spiritual dominion. Dawn, midday, twilight, the zodiacal path, touched neither men’s lives nor their hearts, and science retreated into the ground, to concentrate herself upon problems that she was certain of solving.</p>
				<p>So when Vashti found her cabin invaded by a rosy finger of light, she was annoyed, and tried to adjust the blind. But the blind flew up altogether, and she saw through the skylight small pink clouds, swaying against a background of blue, and as the sun crept higher, its radiance entered direct, brimming down the wall, like a golden sea. It rose and fell with the airship’s motion, just as waves rise and fall, but it advanced steadily, as a tide advances. Unless she was careful, it would strike her face. A spasm of horror shook her and she rang for the attendant. The attendant too was horrified, but she could do nothing; it was not her place to mend the blind. She could only suggest that the lady should change her cabin, which she accordingly prepared to do.</p>
				<p>People were almost exactly alike all over the world, but the attendant of the airship, perhaps owing to her exceptional duties, had grown a little out of the common. She had often to address passengers with direct speech, and this had given her a certain roughness and originality of manner. When Vashti swerved away from the sunbeams with a cry, she behaved barbarically⁠—she put out her hand to steady her.</p>
				<p>“How dare you!” exclaimed the passenger. “You forget yourself!”</p>
				<p>The woman was confused, and apologised for not having let her fall. People never touched one another. The custom had become obsolete, owing to the Machine.</p>
				<p>“Where are we now?” asked Vashti haughtily.</p>
				<p>“We are over Asia,” said the attendant, anxious to be polite.</p>
				<p>“Asia?”</p>
				<p>“You must excuse my common way of speaking. I have got into the habit of calling places over which I pass by their unmechanical names.”</p>
				<p>“Oh, I remember Asia. The Mongols came from it.”</p>
				<p>“Beneath us, in the open air, stood a city that was once called Simla.”</p>
				<p>“Have you ever heard of the Mongols and of the Brisbane school?”</p>
				<p>“No.”</p>
				<p>“Brisbane also stood in the open air.”</p>
				<p>“Those mountains to the right⁠—let me show you them.” She pushed back a metal blind. The main chain of the Himalayas was revealed. “They were once called the Roof of the World, those mountains.”</p>
				<p>“What a foolish name!”</p>
				<p>“You must remember that, before the dawn of civilisation, they seemed to be an impenetrable wall that touched the stars. It was supposed that no one but the gods could exist above their summits. How we have advanced, thanks to the Machine!”</p>
				<p>“How we have advanced, thanks to the Machine!” said Vashti.</p>
				<p>“How we have advanced, thanks to the Machine!” echoed the passenger who had dropped his Book the night before, and who was standing in the passage.</p>
				<p>“And that white stuff in the cracks?⁠—what is it?”</p>
				<p>“I have forgotten its name.”</p>
				<p>“Cover the window, please. These mountains give me no ideas.”</p>
				<p>The northern aspect of the Himalayas was in deep shadow: on the Indian slope the sun had just prevailed. The forests had been destroyed during the literature epoch for the purpose of making newspaper-pulp, but the snows were awakening to their morning glory, and clouds still hung on the breasts of Kinchinjunga. In the plain were seen the ruins of cities, with diminished rivers creeping by their walls, and by the sides of these were sometimes the signs of vomitories, marking the cities of today. Over the whole prospect airships rushed, crossing and intercrossing with incredible <em>aplomb</em>, and rising nonchalantly when they desired to escape the perturbations of the lower atmosphere and to traverse the Roof of the World.</p>
				<p>“We have indeed advanced, thanks to the Machine,” repeated the attendant, and hid the Himalayas behind a metal blind.</p>
				<p>The day dragged wearily forward. The passengers sat each in his cabin, avoiding one another with an almost physical repulsion and longing to be once more under the surface of the earth. There were eight or ten of them, mostly young males, sent out from the public nurseries to inhabit the rooms of those who had died in various parts of the earth. The man who had dropped his Book was on the homeward journey. He had been sent to Sumatra for the purpose of propagating the race. Vashti alone was travelling by her private will.</p>
				<p>At midday she took a second glance at the earth. The airship was crossing another range of mountains, but she could see little, owing to clouds. Masses of black rock hovered below her, and merged indistinctly into grey. Their shapes were fantastic; one of them resembled a prostrate man.</p>
				<p>“No ideas here,” murmured Vashti, and hid the Caucasus behind a metal blind.</p>
				<p>In the evening she looked again. They were crossing a golden sea, in which lay many small islands and one peninsula.</p>
				<p>She repeated, “No ideas here,” and hid Greece behind a metal blind.</p>
			</section>
			<section id="the-machine-stops-2" epub:type="chapter">
				<hgroup>
					<h3>
						<span epub:type="label">Part</span>
						<span epub:type="ordinal z3998:roman">II</span>
					</h3>
					<p epub:type="title">The Mending Apparatus</p>
				</hgroup>
				<p>By a vestibule, by a lift, by a tubular railway, by a platform, by a sliding door⁠—by reversing all the steps of her departure did Vashti arrive at her son’s room, which exactly resembled her own. She might well declare that the visit was superfluous. The buttons, the knobs, the reading-desk with the Book, the temperature, the atmosphere, the illumination⁠—all were exactly the same. And if Kuno himself, flesh of her flesh, stood close beside her at last, what profit was there in that? She was too well-bred to shake him by the hand.</p>
				<p>Averting her eyes, she spoke as follows:</p>
				<p>“Here I am. I have had the most terrible journey and greatly retarded the development of my soul. It is not worth it, Kuno, it is not worth it. My time is too precious. The sunlight almost touched me, and I have met with the rudest people. I can only stop a few minutes. Say what you want to say, and then I must return.”</p>
				<p>“I have been threatened with Homelessness,” said Kuno.</p>
				<p>She looked at him now.</p>
				<p>“I have been threatened with Homelessness, and I could not tell you such a thing through the Machine.”</p>
				<p>Homelessness means death. The victim is exposed to the air, which kills him.</p>
				<p>“I have been outside since I spoke to you last. The tremendous thing has happened, and they have discovered me.”</p>
				<p>“But why shouldn’t you go outside!” she exclaimed. “It is perfectly legal, perfectly mechanical, to visit the surface of the earth. I have lately been to a lecture on the sea; there is no objection to that; one simply summons a respirator and gets an Egression-permit. It is not the kind of thing that spiritually-minded people do, and I begged you not to do it, but there is no legal objection to it.”</p>
				<p>“I did not get an Egression-permit.”</p>
				<p>“Then how did you get out?”</p>
				<p>“I found out a way of my own.”</p>
				<p>The phrase conveyed no meaning to her, and he had to repeat it.</p>
				<p>“A way of your own?” she whispered. “But that would be wrong.”</p>
				<p>“Why?”</p>
				<p>The question shocked her beyond measure.</p>
				<p>“You are beginning to worship the Machine,” he said coldly. “You think it irreligious of me to have found out a way of my own. It was just what the Committee thought, when they threatened me with Homelessness.”</p>
				<p>At this she grew angry. “I worship nothing!” she cried. “I am most advanced. I don’t think you irreligious, for there is no such thing as religion left. All the fear and the superstition that existed once have been destroyed by the Machine. I only meant that to find out a way of your own was⁠—Besides, there is no new way out.”</p>
				<p>“So it is always supposed.”</p>
				<p>“Except through the vomitories, for which one must have an Egression-permit, it is impossible to get out. The Book says so.”</p>
				<p>“Well, the Book’s wrong, for I have been out on my feet.”</p>
				<p>For Kuno was possessed of a certain physical strength.</p>
				<p>By these days it was a demerit to be muscular. Each infant was examined at birth, and all who promised undue strength were destroyed. Humanitarians may protest, but it would have been no true kindness to let an athlete live; he would never have been happy in that state of life to which the Machine had called him; he would have yearned for trees to climb, rivers to bathe in, meadows and hills against which he might measure his body. Man must be adapted to his surroundings, must he not? In the dawn of the world our weakly must be exposed on Mount Taygetus, in its twilight our strong will suffer euthanasia, that the Machine may progress, that the Machine may progress, that the Machine may progress eternally.</p>
				<p>“You know that we have lost the sense of space. We say ‘space is annihilated,’ but we have annihilated not space, but the sense thereof. We have lost a part of ourselves. I determined to recover it, and I began by walking up and down the platform of the railway outside my room. Up and down, until I was tired, and so did recapture the meaning of ‘Near’ and ‘Far.’ ‘Near’ is a place to which I can get quickly <em>on my feet</em>, not a place to which the train or the airship will take me quickly. ‘Far’ is a place to which I cannot get quickly on my feet; the vomitory is ‘far,’ though I could be there in thirty-eight seconds by summoning the train. Man is the measure. That was my first lesson. Man’s feet are the measure for distance, his hands are the measure for ownership, his body is the measure for all that is lovable and desirable and strong. Then I went further: it was then that I called to you for the first time, and you would not come.</p>
				<p>“This city, as you know, is built deep beneath the surface of the earth, with only the vomitories protruding. Having paced the platform outside my own room, I took the lift to the next platform and paced that also, and so with each in turn, until I came to the topmost, above which begins the earth. All the platforms were exactly alike, and all that I gained by visiting them was to develop my sense of space and my muscles. I think I should have been content with this⁠—it is not a little thing⁠—but as I walked and brooded, it occurred to me that our cities had been built in the days when men still breathed the outer air, and that there had been ventilation shafts for the workmen. I could think of nothing but these ventilation shafts. Had they been destroyed by all the food-tubes and medicine-tubes and music-tubes that the Machine has evolved lately? Or did traces of them remain? One thing was certain. If I came upon them anywhere, it would be in the railway-tunnels of the topmost story. Everywhere else, all space was accounted for.</p>
				<p>“I am telling my story quickly, but don’t think that I was not a coward or that your answers never depressed me. It is not the proper thing, it is not mechanical, it is not decent to walk along a railway-tunnel. I did not fear that I might tread upon a live rail and be killed. I feared something far more intangible⁠—doing what was not contemplated by the Machine. Then I said to myself, ‘Man is the measure,’ and I went, and after many visits I found an opening.</p>
				<p>“The tunnels, of course, were lighted. Everything is light, artificial light; darkness is the exception. So when I saw a black gap in the tiles, I knew that it was an exception, and rejoiced. I put in my arm⁠—I could put in no more at first⁠—and waved it round and round in ecstasy. I loosened another tile, and put in my head, and shouted into the darkness: ‘I am coming, I shall do it yet,’ and my voice reverberated down endless passages. I seemed to hear the spirits of those dead workmen who had returned each evening to the starlight and to their wives, and all the generations who had lived in the open air called back to me, ‘You will do it yet, you are coming.’ ”</p>
				<p>He paused, and, absurd as he was, his last words moved her. For Kuno had lately asked to be a father, and his request had been refused by the Committee. His was not a type that the Machine desired to hand on.</p>
				<p>“Then a train passed. It brushed by me, but I thrust my head and arms into the hole. I had done enough for one day, so I crawled back to the platform, went down in the lift, and summoned my bed. Ah, what dreams! And again I called you, and again you refused.”</p>
				<p>She shook her head and said:</p>
				<p>“Don’t. Don’t talk of these terrible things. You make me miserable. You are throwing civilisation away.”</p>
				<p>“But I had got back the sense of space and a man cannot rest then. I determined to get in at the hole and climb the shaft. And so I exercised my arms. Day after day I went through ridiculous movements, until my flesh ached, and I could hang by my hands and hold the pillow of my bed outstretched for many minutes. Then I summoned a respirator, and started.</p>
				<p>“It was easy at first. The mortar had somehow rotted, and I soon pushed some more tiles in, and clambered after them into the darkness, and the spirits of the dead comforted me. I don’t know what I mean by that. I just say what I felt. I felt, for the first time, that a protest had been lodged against corruption, and that even as the dead were comforting me, so I was comforting the unborn. I felt that humanity existed, and that it existed without clothes. How can I possibly explain this? It was naked, humanity seemed naked, and all these tubes and buttons and machineries neither came into the world with us, nor will they follow us out, nor do they matter supremely while we are here. Had I been strong, I would have torn off every garment I had, and gone out into the outer air unswaddled. But this is not for me, nor perhaps for my generation. I climbed with my respirator and my hygienic clothes and my dietetic tabloids! Better thus than not at all.</p>
				<p>“There was a ladder, made of some primaeval metal. The light from the railway fell upon its lowest rungs, and I saw that it led straight upwards out of the rubble at the bottom of the shaft. Perhaps our ancestors ran up and down it a dozen times daily, in their building. As I climbed, the rough edges cut through my gloves so that my hands bled. The light helped me for a little, and then came darkness and, worse still, silence which pierced my ears like a sword. The Machine hums! Did you know that? Its hum penetrates our blood, and may even guide our thoughts. Who knows! I was getting beyond its power. Then I thought: ‘This silence means that I am doing wrong.’ But I heard voices in the silence, and again they strengthened me.” He laughed. “I had need of them. The next moment I cracked my head against something.”</p>
				<p>She sighed.</p>
				<p>“I had reached one of those pneumatic stoppers that defend us from the outer air. You may have noticed them on the airship. Pitch dark, my feet on the rungs of an invisible ladder, my hands cut; I cannot explain how I lived through this part, but the voices still comforted me, and I felt for fastenings. The stopper, I suppose, was about eight feet across. I passed my hand over it as far as I could reach. It was perfectly smooth. I felt it almost to the centre. Not quite to the centre, for my arm was too short. Then the voice said: ‘Jump. It is worth it. There may be a handle in the centre, and you may catch hold of it and so come to us your own way. And if there is no handle, so that you may fall and are dashed to pieces⁠—it is still worth it: you will still come to us your own way.’ So I jumped. There was a handle, and⁠—”</p>
				<p>He paused. Tears gathered in his mother’s eyes. She knew that he was fated. If he did not die today he would die tomorrow. There was not room for such a person in the world. And with her pity disgust mingled. She was ashamed at having borne such a son, she who had always been so respectable and so full of ideas. Was he really the little boy to whom she had taught the use of his stops and buttons, and to whom she had given his first lessons in the Book? The very hair that disfigured his lip showed that he was reverting to some savage type. On atavism the Machine can have no mercy.</p>
				<p>“There was a handle, and I did catch it. I hung tranced over the darkness and heard the hum of these workings as the last whisper in a dying dream. All the things I had cared about and all the people I had spoken to through tubes appeared infinitely little. Meanwhile the handle revolved. My weight had set something in motion and I span slowly, and then⁠—</p>
				<p>“I cannot describe it. I was lying with my face to the sunshine. Blood poured from my nose and ears and I heard a tremendous roaring. The stopper, with me clinging to it, had simply been blown out of the earth, and the air that we make down here was escaping through the vent into the air above. It burst up like a fountain. I crawled back to it⁠—for the upper air hurts⁠—and, as it were, I took great sips from the edge. My respirator had flown goodness knows where, my clothes were torn. I just lay with my lips close to the hole, and I sipped until the bleeding stopped. You can imagine nothing so curious. This hollow in the grass⁠—I will speak of it in a minute⁠—the sun shining into it, not brilliantly but through marbled clouds⁠—the peace, the nonchalance, the sense of space, and, brushing my cheek, the roaring fountain of our artificial air! Soon I spied my respirator, bobbing up and down in the current high above my head, and higher still were many airships. But no one ever looks out of airships, and in my case they could not have picked me up. There I was, stranded. The sun shone a little way down the shaft, and revealed the topmost rung of the ladder, but it was hopeless trying to reach it. I should either have been tossed up again by the escape, or else have fallen in, and died. I could only lie on the grass, sipping and sipping, and from time to time glancing around me.</p>
				<p>“I knew that I was in Wessex, for I had taken care to go to a lecture on the subject before starting. Wessex lies above the room in which we are talking now. It was once an important state. Its kings held all the southern coast from the Andredswald to Cornwall, while the Wansdyke protected them on the north, running over the high ground. The lecturer was only concerned with the rise of Wessex, so I do not know how long it remained an international power, nor would the knowledge have assisted me. To tell the truth I could do nothing but laugh, during this part. There was I, with a pneumatic stopper by my side and a respirator bobbing over my head, imprisoned, all three of us, in a grass-grown hollow that was edged with fern.”</p>
				<p>Then he grew grave again.</p>
				<p>“Lucky for me that it was a hollow. For the air began to fall back into it and to fill it as water fills a bowl. I could crawl about. Presently I stood. I breathed a mixture, in which the air that hurts predominated whenever I tried to climb the sides. This was not so bad. I had not lost my tabloids and remained ridiculously cheerful, and as for the Machine, I forgot about it altogether. My one aim now was to get to the top, where the ferns were, and to view whatever objects lay beyond.</p>
				<p>“I rushed the slope. The new air was still too bitter for me and I came rolling back, after a momentary vision of something grey. The sun grew very feeble, and I remembered that he was in Scorpio⁠—I had been to a lecture on that too. If the sun is in Scorpio and you are in Wessex, it means that you must be as quick as you can, or it will get too dark. (This is the first bit of useful information I have ever got from a lecture, and I expect it will be the last.) It made me try frantically to breathe the new air, and to advance as far as I dared out of my pond. The hollow filled so slowly. At times I thought that the fountain played with less vigour. My respirator seemed to dance nearer the earth; the roar was decreasing.”</p>
				<p>He broke off.</p>
				<p>“I don’t think this is interesting you. The rest will interest you even less. There are no ideas in it, and I wish that I had not troubled you to come. We are too different, mother.”</p>
				<p>She told him to continue.</p>
				<p>“It was evening before I climbed the bank. The sun had very nearly slipped out of the sky by this time, and I could not get a good view. You, who have just crossed the Roof of the World, will not want to hear an account of the little hills that I saw⁠—low colourless hills. But to me they were living and the turf that covered them was a skin, under which their muscles rippled, and I felt that those hills had called with incalculable force to men in the past, and that men had loved them. Now they sleep⁠—perhaps forever. They commune with humanity in dreams. Happy the man, happy the woman, who awakes the hills of Wessex. For though they sleep, they will never die.”</p>
				<p>His voice rose passionately.</p>
				<p>“Cannot you see, cannot all your lecturers see, that it is we who are dying, and that down here the only thing that really lives is the Machine? We created the Machine, to do our will, but we cannot make it do our will now. It has robbed us of the sense of space and of the sense of touch, it has blurred every human relation and narrowed down love to a carnal act, it has paralysed our bodies and our wills, and now it compels us to worship it. The Machine develops⁠—but not on our lines. The Machine proceeds⁠—but not to our goal. We only exist as the blood corpuscles that course through its arteries, and if it could work without us, it would let us die. Oh, I have no remedy⁠—or, at least, only one⁠—to tell men again and again that I have seen the hills of Wessex as Ælfrid saw them when he overthrew the Danes.</p>
				<p>“So the sun set. I forgot to mention that a belt of mist lay between my hill and other hills, and that it was the colour of pearl.”</p>
				<p>He broke off for the second time.</p>
				<p>“Go on,” said his mother wearily.</p>
				<p>He shook his head.</p>
				<p>“Go on. Nothing that you say can distress me now. I am hardened.”</p>
				<p>“I had meant to tell you the rest, but I cannot: I know that I cannot: goodbye.”</p>
				<p>Vashti stood irresolute. All her nerves were tingling with his blasphemies. But she was also inquisitive.</p>
				<p>“This is unfair,” she complained. “You have called me across the world to hear your story, and hear it I will. Tell me⁠—as briefly as possible, for this is a disastrous waste of time⁠—tell me how you returned to civilisation.”</p>
				<p>“Oh⁠—that!” he said, starting. “You would like to hear about civilisation. Certainly. Had I got to where my respirator fell down?”</p>
				<p>“No⁠—but I understand everything now. You put on your respirator, and managed to walk along the surface of the earth to a vomitory, and there your conduct was reported to the Central Committee.”</p>
				<p>“By no means.”</p>
				<p>He passed his hand over his forehead, as if dispelling some strong impression. Then, resuming his narrative, he warmed to it again.</p>
				<p>“My respirator fell about sunset. I had mentioned that the fountain seemed feebler, had I not.”</p>
				<p>“Yes.”</p>
				<p>“About sunset, it let the respirator fall. As I said, I had entirely forgotten about the Machine, and I paid no great attention at the time, being occupied with other things. I had my pool of air, into which I could dip when the outer keenness became intolerable, and which would possibly remain for days, provided that no wind sprang up to disperse it. Not until it was too late, did I realize what the stoppage of the escape implied. You see⁠—the gap in the tunnel had been mended; the Mending Apparatus; the Mending Apparatus, was after me.</p>
				<p>“One other warning I had, but I neglected it. The sky at night was clearer than it had been in the day, and the moon, which was about half the sky behind the sun, shone into the dell at moments quite brightly. I was in my usual place⁠—on the boundary between the two atmospheres⁠—when I thought I saw something dark move across the bottom of the dell, and vanish into the shaft. In my folly, I ran down. I bent over and listened, and I thought I heard a faint scraping noise in the depths.</p>
				<p>“At this⁠—but it was too late⁠—I took alarm. I determined to put on my respirator and to walk right out of the dell. But my respirator had gone. I knew exactly where it had fallen⁠—between the stopper and the aperture⁠—and I could even feel the mark that it had made in the turf. It had gone, and I realized that something evil was at work, and I had better escape to the other air, and, if I must die, die running towards the cloud that had been the colour of a pearl. I never started. Out of the shaft⁠—it is too horrible. A worm, a long white worm, had crawled out of the shaft and was gliding over the moonlit grass.</p>
				<p>“I screamed. I did everything that I should not have done, I stamped upon the creature instead of flying from it, and it at once curled round the ankle. Then we fought. The worm let me run all over the dell, but edged up my leg as I ran. ‘Help!’ I cried. (That part is too awful. It belongs to the part that you will never know.) ‘Help!’ I cried. (Why cannot we suffer in silence?) ‘Help!’ I cried. Then my feet were wound together, I fell, I was dragged away from the dear ferns and the living hills, and past the great metal stopper (I can tell you this part), and I thought it might save me again if I caught hold of the handle. It also was enwrapped, it also. Oh, the whole dell was full of the things. They were searching it in all directions, they were denuding it, and the white snouts of others peeped out of the hole, ready if needed. Everything that could be moved they brought⁠—brushwood, bundles of fern, everything, and down we all went intertwined into hell. The last things that I saw, ere the stopper closed after us, were certain stars, and I felt that a man of my sort lived in the sky. For I did fight, I fought till the very end, and it was only my head hitting against the ladder that quieted me. I woke up in this room. The worms had vanished. I was surrounded by artificial air, artificial light, artificial peace, and my friends were calling to me down speaking-tubes to know whether I had come across any new ideas lately.”</p>
				<p>Here his story ended. Discussion of it was impossible, and Vashti turned to go.</p>
				<p>“It will end in Homelessness,” she said quietly.</p>
				<p>“I wish it would,” retorted Kuno.</p>
				<p>“The Machine has been most merciful.”</p>
				<p>“I prefer the mercy of God.”</p>
				<p>“By that superstitious phrase, do you mean that you could live in the outer air?”</p>
				<p>“Yes.”</p>
				<p>“Have you ever seen, round the vomitories, the bones of those who were extruded after the Great Rebellion?”</p>
				<p>“Yes.”</p>
				<p>“They were left where they perished for our edification. A few crawled away, but they perished, too⁠—who can doubt it? And so with the Homeless of our own day. The surface of the earth supports life no longer.”</p>
				<p>“Indeed.”</p>
				<p>“Ferns and a little grass may survive, but all higher forms have perished. Has any airship detected them?”</p>
				<p>“No.”</p>
				<p>“Has any lecturer dealt with them?”</p>
				<p>“No.”</p>
				<p>“Then why this obstinacy?”</p>
				<p>“Because I have seen them,” he exploded.</p>
				<p>“Seen <em>what</em>?”</p>
				<p>“Because I have seen her in the twilight⁠—because she came to my help when I called⁠—because she, too, was entangled by the worms, and, luckier than I, was killed by one of them piercing her throat.”</p>
				<p>He was mad. Vashti departed, nor, in the troubles that followed, did she ever see his face again.</p>
			</section>
			<section id="the-machine-stops-3" epub:type="chapter">
				<hgroup>
					<h3>
						<span epub:type="label">Part</span>
						<span epub:type="ordinal z3998:roman">III</span>
					</h3>
					<p epub:type="title">The Homeless</p>
				</hgroup>
				<p>During the years that followed Kuno’s escapade, two important developments took place in the Machine. On the surface they were revolutionary, but in either case men’s minds had been prepared beforehand, and they did but express tendencies that were latent already.</p>
				<p>The first of these was the abolition of respirators.</p>
				<p>Advanced thinkers, like Vashti, had always held it foolish to visit the surface of the earth. Airships might be necessary, but what was the good of going out for mere curiosity and crawling along for a mile or two in a terrestrial motor? The habit was vulgar and perhaps faintly improper: it was unproductive of ideas, and had no connection with the habits that really mattered. So respirators were abolished, and with them, of course, the terrestrial motors, and except for a few lecturers, who complained that they were debarred access to their subject-matter, the development was accepted quietly. Those who still wanted to know what the earth was like had after all only to listen to some gramophone, or to look into some cinematophote. And even the lecturers acquiesced when they found that a lecture on the sea was none the less stimulating when compiled out of other lectures that had already been delivered on the same subject. “Beware of firsthand ideas!” exclaimed one of the most advanced of them. “Firsthand ideas do not really exist. They are but the physical impressions produced by love and fear, and on this gross foundation who could erect a philosophy? Let your ideas be secondhand, and if possible tenth-hand, for then they will be far removed from that disturbing element⁠—direct observation. Do not learn anything about this subject of mine⁠—the French Revolution. Learn instead what I think that Enicharmon thought Urizen thought Gutch thought Ho-Yung thought Chi-Bo-Sing thought Lafcadio Hearn thought Carlyle thought Mirabeau said about the French Revolution. Through the medium of these eight great minds, the blood that was shed at Paris and the windows that were broken at Versailles will be clarified to an idea which you may employ most profitably in your daily lives. But be sure that the intermediates are many and varied, for in history one authority exists to counteract another. Urizen must counteract the scepticism of Ho-Yung and Enicharmon, I must myself counteract the impetuosity of Gutch. You who listen to me are in a better position to judge about the French Revolution than I am. Your descendants will be even in a better position than you, for they will learn what you think I think, and yet another intermediate will be added to the chain. And in time”⁠—his voice rose⁠—“there will come a generation that has got beyond facts, beyond impressions, a generation absolutely colourless, a generation</p>
				<blockquote epub:type="z3998:verse">
					<p>
						<span>‘seraphically free</span>
						<br>
						<span>From taint of personality,’</span>
					</p>
				</blockquote>
				<p>which will see the French Revolution not as it happened, nor as they would like it to have happened, but as it would have happened, had it taken place in the days of the Machine.”</p>
				<p>Tremendous applause greeted this lecture, which did but voice a feeling already latent in the minds of men⁠—a feeling that terrestrial facts must be ignored, and that the abolition of respirators was a positive gain. It was even suggested that airships should be abolished too. This was not done, because airships had somehow worked themselves into the Machine’s system. But year by year they were used less, and mentioned less by thoughtful men.</p>
				<p>The second great development was the reestablishment of religion.</p>
				<p>This, too, had been voiced in the celebrated lecture. No one could mistake the reverent tone in which the peroration had concluded, and it awakened a responsive echo in the heart of each. Those who had long worshipped silently, now began to talk. They described the strange feeling of peace that came over them when they handled the Book of the Machine, the pleasure that it was to repeat certain numerals out of it, however little meaning those numerals conveyed to the outward ear, the ecstasy of touching a button, however unimportant, or of ringing an electric bell, however superfluously.</p>
				<p>“The Machine,” they exclaimed, “feeds us and clothes us and houses us; through it we speak to one another, through it we see one another, in it we have our being. The Machine is the friend of ideas and the enemy of superstition: the Machine is omnipotent, eternal; blessed is the Machine.” And before long this allocution was printed on the first page of the Book, and in subsequent editions the ritual swelled into a complicated system of praise and prayer. The word “religion” was sedulously avoided, and in theory the Machine was still the creation and the implement of man. But in practice all, save a few retrogrades, worshipped it as divine. Nor was it worshipped in unity. One believer would be chiefly impressed by the blue optic plates, through which he saw other believers; another by the mending apparatus, which sinful Kuno had compared to worms; another by the lifts, another by the Book. And each would pray to this or to that, and ask it to intercede for him with the Machine as a whole. Persecution⁠—that also was present. It did not break out, for reasons that will be set forward shortly. But it was latent, and all who did not accept the minimum known as “undenominational Mechanism” lived in danger of Homelessness, which means death, as we know.</p>
				<p>To attribute these two great developments to the Central Committee, is to take a very narrow view of civilisation. The Central Committee announced the developments, it is true, but they were no more the cause of them than were the kings of the imperialistic period the cause of war. Rather did they yield to some invincible pressure, which came no one knew whither, and which, when gratified, was succeeded by some new pressure equally invincible. To such a state of affairs it is convenient to give the name of progress. No one confessed the Machine was out of hand. Year by year it was served with increased efficiency and decreased intelligence. The better a man knew his own duties upon it, the less he understood the duties of his neighbour, and in all the world there was not one who understood the monster as a whole. Those master brains had perished. They had left full directions, it is true, and their successors had each of them mastered a portion of those directions. But Humanity, in its desire for comfort, had overreached itself. It had exploited the riches of nature too far. Quietly and complacently, it was sinking into decadence, and progress had come to mean the progress of the Machine.</p>
				<p>As for Vashti, her life went peacefully forward until the final disaster. She made her room dark and slept; she awoke and made the room light. She lectured and attended lectures. She exchanged ideas with her innumerable friends and believed she was growing more spiritual. At times a friend was granted Euthanasia, and left his or her room for the homelessness that is beyond all human conception. Vashti did not much mind. After an unsuccessful lecture, she would sometimes ask for Euthanasia herself. But the death-rate was not permitted to exceed the birthrate, and the Machine had hitherto refused it to her.</p>
				<p>The troubles began quietly, long before she was conscious of them.</p>
				<p>One day she was astonished at receiving a message from her son. They never communicated, having nothing in common, and she had only heard indirectly that he was still alive, and had been transferred from the northern hemisphere, where he had behaved so mischievously, to the southern⁠—indeed, to a room not far from her own.</p>
				<p>“Does he want me to visit him?” she thought. “Never again, never. And I have not the time.”</p>
				<p>No, it was madness of another kind.</p>
				<p>He refused to visualize his face upon the blue plate, and speaking out of the darkness with solemnity said:</p>
				<p>“The Machine stops.”</p>
				<p>“What do you say?”</p>
				<p>“The Machine is stopping, I know it, I know the signs.”</p>
				<p>She burst into a peal of laughter. He heard her and was angry, and they spoke no more.</p>
				<p>“Can you imagine anything more absurd?” she cried to a friend. “A man who was my son believes that the Machine is stopping. It would be impious if it was not mad.”</p>
				<p>“The Machine is stopping?” her friend replied. “What does that mean? The phrase conveys nothing to me.”</p>
				<p>“Nor to me.”</p>
				<p>“He does not refer, I suppose, to the trouble there has been lately with the music?”</p>
				<p>“Oh no, of course not. Let us talk about music.”</p>
				<p>“Have you complained to the authorities?”</p>
				<p>“Yes, and they say it wants mending, and referred me to the Committee of the Mending Apparatus. I complained of those curious gasping sighs that disfigure the symphonies of the Brisbane school. They sound like someone in pain. The Committee of the Mending Apparatus say that it shall be remedied shortly.”</p>
				<p>Obscurely worried, she resumed her life. For one thing, the defect in the music irritated her. For another thing, she could not forget Kuno’s speech. If he had known that the music was out of repair⁠—he could not know it, for he detested music⁠—if he had known that it was wrong, “the Machine stops” was exactly the venomous sort of remark he would have made. Of course he had made it at a venture, but the coincidence annoyed her, and she spoke with some petulance to the Committee of the Mending Apparatus.</p>
				<p>They replied, as before, that the defect would be set right shortly.</p>
				<p>“Shortly! At once!” she retorted. “Why should I be worried by imperfect music? Things are always put right at once. If you do not mend it at once, I shall complain to the Central Committee.”</p>
				<p>“No personal complaints are received by the Central Committee,” the Committee of the Mending Apparatus replied.</p>
				<p>“Through whom am I to make my complaint, then?”</p>
				<p>“Through us.”</p>
				<p>“I complain then.”</p>
				<p>“Your complaint shall be forwarded in its turn.”</p>
				<p>“Have others complained?”</p>
				<p>This question was unmechanical, and the Committee of the Mending Apparatus refused to answer it.</p>
				<p>“It is too bad!” she exclaimed to another of her friends. “There never was such an unfortunate woman as myself. I can never be sure of my music now. It gets worse and worse each time I summon it.”</p>
				<p>“I too have my troubles,” the friend replied. “Sometimes my ideas are interrupted by a slight jarring noise.”</p>
				<p>“What is it?”</p>
				<p>“I do not know whether it is inside my head, or inside the wall.”</p>
				<p>“Complain, in either case.”</p>
				<p>“I have complained, and my complaint will be forwarded in its turn to the Central Committee.”</p>
				<p>Time passed, and they resented the defects no longer. The defects had not been remedied, but the human tissues in that latter day had become so subservient, that they readily adapted themselves to every caprice of the Machine. The sigh at the crisis of the Brisbane symphony no longer irritated Vashti; she accepted it as part of the melody. The jarring noise, whether in the head or in the wall, was no longer resented by her friend. And so with the mouldy artificial fruit, so with the bath water that began to stink, so with the defective rhymes that the poetry machine had taken to emit. All were bitterly complained of at first, and then acquiesced in and forgotten. Things went from bad to worse unchallenged.</p>
				<p>It was otherwise with the failure of the sleeping apparatus. That was a more serious stoppage. There came a day when over the whole world⁠—in Sumatra, in Wessex, in the innumerable cities of Courland and Brazil⁠—the beds, when summoned by their tired owners, failed to appear. It may seem a ludicrous matter, but from it we may date the collapse of humanity. The Committee responsible for the failure was assailed by complainants, whom it referred, as usual, to the Committee of the Mending Apparatus, who in its turn assured them that their complaints would be forwarded to the Central Committee. But the discontent grew, for mankind was not yet sufficiently adaptable to do without sleeping.</p>
				<p>“Someone is meddling with the Machine⁠—” they began.</p>
				<p>“Someone is trying to make himself king, to reintroduce the personal element.”</p>
				<p>“Punish that man with Homelessness.”</p>
				<p>“To the rescue! Avenge the Machine! Avenge the Machine!”</p>
				<p>“War! Kill the man!”</p>
				<p>But the Committee of the Mending Apparatus now came forward, and allayed the panic with well-chosen words. It confessed that the Mending Apparatus was itself in need of repair.</p>
				<p>The effect of this frank confession was admirable.</p>
				<p>“Of course,” said a famous lecturer⁠—he of the French Revolution, who gilded each new decay with splendour⁠—“of course we shall not press our complaints now. The Mending Apparatus has treated us so well in the past that we all sympathize with it, and will wait patiently for its recovery. In its own good time it will resume its duties. Meanwhile let us do without our beds, our tabloids, our other little wants. Such, I feel sure, would be the wish of the Machine.”</p>
				<p>Thousands of miles away his audience applauded. The Machine still linked them. Under the seas, beneath the roots of the mountains, ran the wires through which they saw and heard, the enormous eyes and ears that were their heritage, and the hum of many workings clothed their thoughts in one garment of subserviency. Only the old and the sick remained ungrateful, for it was rumoured that Euthanasia, too, was out of order, and that pain had reappeared among men.</p>
				<p>It became difficult to read. A blight entered the atmosphere and dulled its luminosity. At times Vashti could scarcely see across her room. The air, too, was foul. Loud were the complaints, impotent the remedies, heroic the tone of the lecturer as he cried: “Courage, courage! What matter so long as the Machine goes on? To it the darkness and the light are one.” And though things improved again after a time, the old brilliancy was never recaptured, and humanity never recovered from its entrance into twilight. There was an hysterical talk of “measures,” of “provisional dictatorship,” and the inhabitants of Sumatra were asked to familiarize themselves with the workings of the central power station, the said power station being situated in France. But for the most part panic reigned, and men spent their strength praying to their Books, tangible proofs of the Machine’s omnipotence. There were gradations of terror⁠—at times came rumours of hope⁠—the Mending Apparatus was almost mended⁠—the enemies of the Machine had been got under⁠—new “nerve-centres” were evolving which would do the work even more magnificently than before. But there came a day when, without the slightest warning, without any previous hint of feebleness, the entire communication-system broke down, all over the world, and the world, as they understood it, ended.</p>
				<p>Vashti was lecturing at the time and her earlier remarks had been punctuated with applause. As she proceeded the audience became silent, and at the conclusion there was no sound. Somewhat displeased, she called to a friend who was a specialist in sympathy. No sound: doubtless the friend was sleeping. And so with the next friend whom she tried to summon, and so with the next, until she remembered Kuno’s cryptic remark, “The Machine stops.”</p>
				<p>The phrase still conveyed nothing. If Eternity was stopping it would of course be set going shortly.</p>
				<p>For example, there was still a little light and air⁠—the atmosphere had improved a few hours previously. There was still the Book, and while there was the Book there was security.</p>
				<p>Then she broke down, for with the cessation of activity came an unexpected terror⁠—silence.</p>
				<p>She had never known silence, and the coming of it nearly killed her⁠—it did kill many thousands of people outright. Ever since her birth she had been surrounded by the steady hum. It was to the ear what artificial air was to the lungs, and agonizing pains shot across her head. And scarcely knowing what she did, she stumbled forward and pressed the unfamiliar button, the one that opened the door of her cell.</p>
				<p>Now the door of the cell worked on a simple hinge of its own. It was not connected with the central power station, dying far away in France. It opened, rousing immoderate hopes in Vashti, for she thought that the Machine had been mended. It opened, and she saw the dim tunnel that curved far away towards freedom. One look, and then she shrank back. For the tunnel was full of people⁠—she was almost the last in that city to have taken alarm.</p>
				<p>People at any time repelled her, and these were nightmares from her worst dreams. People were crawling about, people were screaming, whimpering, gasping for breath, touching each other, vanishing in the dark, and ever and anon being pushed off the platform on to the live rail. Some were fighting round the electric bells, trying to summon trains which could not be summoned. Others were yelling for Euthanasia or for respirators, or blaspheming the Machine. Others stood at the doors of their cells fearing, like herself, either to stop in them or to leave them. And behind all the uproar was silence⁠—the silence which is the voice of the earth and of the generations who have gone.</p>
				<p>No⁠—it was worse than solitude. She closed the door again and sat down to wait for the end. The disintegration went on, accompanied by horrible cracks and rumbling. The valves that restrained the Medical Apparatus must have been weakened, for it ruptured and hung hideously from the ceiling. The floor heaved and fell and flung her from her chair. A tube oozed towards her serpent fashion. And at last the final horror approached⁠—light began to ebb, and she knew that civilisation’s long day was closing.</p>
				<p>She whirled round, praying to be saved from this, at any rate, kissing the Book, pressing button after button. The uproar outside was increasing, and even penetrated the wall. Slowly the brilliancy of her cell was dimmed, the reflections faded from her metal switches. Now she could not see the reading-stand, now not the Book, though she held it in her hand. Light followed the flight of sound, air was following light, and the original void returned to the cavern from which it had been so long excluded. Vashti continued to whirl, like the devotees of an earlier religion, screaming, praying, striking at the buttons with bleeding hands.</p>
				<p>It was thus that she opened her prison and escaped⁠—escaped in the spirit: at least so it seems to me, ere my meditation closes. That she escapes in the body⁠—I cannot perceive that. She struck, by chance, the switch that released the door, and the rush of foul air on her skin, the loud throbbing whispers in her ears, told her that she was facing the tunnel again, and that tremendous platform on which she had seen men fighting. They were not fighting now. Only the whispers remained, and the little whimpering groans. They were dying by hundreds out in the dark.</p>
				<p>She burst into tears.</p>
				<p>Tears answered her.</p>
				<p>They wept for humanity, those two, not for themselves. They could not bear that this should be the end. Ere silence was completed their hearts were opened, and they knew what had been important on the earth. Man, the flower of all flesh, the noblest of all creatures visible, man who had once made god in his image, and had mirrored his strength on the constellations, beautiful naked man was dying, strangled in the garments that he had woven. Century after century had he toiled, and here was his reward. Truly the garment had seemed heavenly at first, shot with the colours of culture, sewn with the threads of self-denial. And heavenly it had been so long as it was a garment and no more, so long as man could shed it at will and live by the essence that is his soul, and the essence, equally divine, that is his body. The sin against the body⁠—it was for that they wept in chief; the centuries of wrong against the muscles and the nerves, and those five portals by which we can alone apprehend⁠—glozing it over with talk of evolution, until the body was white pap, the home of ideas as colourless, last sloshy stirrings of a spirit that had grasped the stars.</p>
				<p>“Where are you?” she sobbed.</p>
				<p>His voice in the darkness said, “Here.”</p>
				<p>“Is there any hope, Kuno?”</p>
				<p>“None for us.”</p>
				<p>“Where are you?”</p>
				<p>She crawled towards him over the bodies of the dead. His blood spurted over her hands.</p>
				<p>“Quicker,” he gasped, “I am dying⁠—but we touch, we talk, not through the Machine.”</p>
				<p>He kissed her.</p>
				<p>“We have come back to our own. We die, but we have recaptured life, as it was in Wessex, when Ælfrid overthrew the Danes. We know what they know outside, they who dwelt in the cloud that is the colour of a pearl.”</p>
				<p>“But, Kuno, is it true? Are there still men on the surface of the earth? Is this⁠—this tunnel, this poisoned darkness⁠—really not the end?”</p>
				<p>He replied:</p>
				<p>“I have seen them, spoken to them, loved them. They are hiding in the mist and the ferns until our civilisation stops. Today they are the Homeless⁠—tomorrow⁠—”</p>
				<p>“Oh, tomorrow⁠—some fool will start the Machine again, tomorrow.”</p>
				<p>“Never,” said Kuno, “never. Humanity has learnt its lesson.”</p>
				<p>As he spoke, the whole city was broken like a honeycomb. An airship had sailed in through the vomitory into a ruined wharf. It crashed downwards, exploding as it went, rending gallery after gallery with its wings of steel. For a moment they saw the nations of the dead, and, before they joined them, scraps of the untainted sky.</p>
			</section>
		</article>
	</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Rocky Linux 10 Will Support RISC-V (156 pts)]]></title>
            <link>https://rockylinux.org/news/rockylinux-support-for-riscv</link>
            <guid>44056104</guid>
            <pubDate>Wed, 21 May 2025 20:40:58 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://rockylinux.org/news/rockylinux-support-for-riscv">https://rockylinux.org/news/rockylinux-support-for-riscv</a>, See on <a href="https://news.ycombinator.com/item?id=44056104">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><h2>🏗️ Rocky Linux 10 Will Support RISC-V!</h2>
<p>We're excited to announce that <strong>Rocky Linux 10 will officially support RISC-V!</strong></p>
<p>Thanks to the incredible work of the Fedora RISC-V Community and Rocky's <a href="https://chat.rockylinux.org/">AltArch SIG</a>, this release will include a <strong>riscv64gc</strong> build, targeting the same platforms supported by Fedora—such as the <strong>StarFive VisionFive 2 (VF2)</strong>, <strong>QEMU</strong>, and the <strong>SiFive HiFive Premier P550</strong>.</p>
<blockquote>
<p>Learn more about Fedora’s RISC-V journey:<br>
<a href="https://fedoramagazine.org/risc-v-and-fedora-all-aboard/">fedoramagazine.org/risc-v-and-fedora-all-aboard</a></p>
</blockquote>
<h3>🔧 Highlights:</h3>
<ul>
<li>✅ Works out-of-the-box on the VisionFive 2 and in QEMU, using the standard EL10 kernel.</li>
<li>🧩 Supports the P550 and similar platforms via vendor kernels, though some features may be limited.</li>
<li>🧬 Built on an upstream-first approach—actively collaborating with the Fedora community to advance RISC-V support across the ecosystem.</li>
<li>🆕 New hardware targets and extensions (like RVA23) can be enabled by the AltArch SIG—jump in and get involved!</li>
</ul>
<p>Special thanks to <strong>RISC-V International</strong>, <strong>RISE</strong>, <strong>Rivos, Inc.</strong>, and the <strong>Fedora community</strong> for their ongoing technical and hardware support.</p>
<hr>
<h2>❓ Frequently Asked: Rocky Linux RISC-V</h2>
<h3>Is this a Primary architecture?</h3>
<p>The RISC-V builds for Rocky Linux 10 will be considered an Alternative Architecture--though unlike ppc64le and s390x, build failures for riscv64 will <strong>not</strong> be considered fatal and will not block the release of the other architectures. In short, package updates for Rocky Linux will not be bottlenecked on waiting for RISC-V versions to build, or on fixing failures unique to the archicture.</p>
<h3>🔍 What hardware is supported?</h3>






























<table><thead><tr><th>Hardware</th><th>Status</th><th>Notes</th></tr></thead><tbody><tr><td><strong>StarFive VisionFive 2</strong></td><td>✅ Supported</td><td>Recommended board; standard kernel support</td></tr><tr><td><strong>QEMU</strong></td><td>✅ Supported</td><td>Ideal for testing and evaluation</td></tr><tr><td><strong>SiFive HiFive P550</strong></td><td>⚠️ Limited support</td><td>Vendor kernel;  some feature limitations</td></tr><tr><td><strong>Milk-V / Banana Pi</strong></td><td>🚧 Not yet supported</td><td>Under consideration as mainline support matures</td></tr></tbody></table>
<h3>🌟 What makes this different?</h3>
<ul>
<li>This has been a <strong>community-driven initiative</strong> since early 2024, collaborating with upstream Fedora RISC-V efforts.</li>
<li>The compiler stack was <strong>bootstrapped from Fedora RISC-V</strong>, with necessary backports to EL10 to enable the port--with many already contributed upstreamed.</li>
<li>Expect <strong>rapid iteration and growth</strong>, with your help and feedback!</li>
</ul>
<hr>
<h2>🧭 Getting Started with RISC-V on Rocky</h2>
<ul>
<li>📥 <strong>Download</strong> the Rocky Linux 10 RISC-V image (coming soon).</li>
<li>📘 <strong>Read the install guide</strong> (also coming soon).</li>
<li>💬 <strong>Join the conversation</strong> in our Mattermost <code>SIG/Altarch</code> channel:<br>
<a href="https://chat.rockylinux.org/">chat.rockylinux.org</a></li>
</ul>
<p>Stay updated via <a href="https://bsky.app/profile/rockylinux.bsky.social">Bluesky</a>, <a href="https://www.linkedin.com/company/rocky-linux">LinkedIn</a>, or subscribe to our <strong>RSS feed</strong>.</p>
<hr>
<h2>🌍 The Road Ahead</h2>
<p>From x86_64 to Arm, PowerPC to S390X—and now RISC-V—<strong>Rocky Linux 10</strong> represents our biggest step yet toward a truly open, cross-architecture ecosystem.</p>
<p>Whether you're deploying rock-solid production systems or tinkering with open hardware, <strong>Rocky Linux 10 has a place for you.</strong></p>
<p>Let’s build it together.</p>
<p><em>By Neil Hanlon (Infrastructure Lead) &amp; Alexia Stein (Community Lead)</em></p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Show HN: ClipJS – Edit your videos from a PC or phone (122 pts)]]></title>
            <link>https://clipjs.vercel.app/</link>
            <guid>44055542</guid>
            <pubDate>Wed, 21 May 2025 19:50:37 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://clipjs.vercel.app/">https://clipjs.vercel.app/</a>, See on <a href="https://news.ycombinator.com/item?id=44055542">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><div><p>Welcome to<span></span></p><p><span>Edit your videos from your PC or phone no downloads, no registration, no watermarks.</span><span>Online, Free and Open Source</span></p></div><div><h2><span>What Can it do?</span></h2><div><article><figure><img alt="No Watermarks" loading="lazy" width="18" height="18" decoding="async" data-nimg="1" src="https://www.svgrepo.com/show/343214/no-sign.svg"></figure><div><h5>No Watermarks</h5><p>Edit your videos without any watermarks.</p></div></article><article><figure><img alt="No Ads" loading="lazy" width="18" height="18" decoding="async" data-nimg="1" src="https://www.svgrepo.com/show/508210/ad.svg"></figure><div><h5>No Ads</h5><p>Edit and render without having to watch any ads.</p></div></article><article><figure><img alt="No Registration" loading="lazy" width="18" height="18" decoding="async" data-nimg="1" src="https://www.svgrepo.com/show/318470/login.svg"></figure><div><h5>No Registration</h5><p>Start using the app instantly no sign-up, no account, just get to work.</p></div></article><article><figure><img alt="Speed" loading="lazy" width="18" height="18" decoding="async" data-nimg="1" src="https://www.svgrepo.com/show/441028/speed.svg"></figure><div><h5>Speed</h5><p>Everything runs in your browser so there's no need to upload files to third-party services or wait around.</p></div></article><article><figure><img alt="Trim Videos" loading="lazy" width="18" height="18" decoding="async" data-nimg="1" src="https://www.svgrepo.com/show/348321/cut.svg"></figure><div><h5>Trim Videos</h5><p>Trim video to remove unwanted parts, reduce videos to their most important sections.</p></div></article><article><figure><img alt="Combine" loading="lazy" width="18" height="18" decoding="async" data-nimg="1" src="https://www.svgrepo.com/show/413830/combine.svg"></figure><div><h5>Combine</h5><p>Combine multiple videos, images, texts and audio into one.</p></div></article></div></div></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[For algorithms, a little memory outweighs a lot of time (279 pts)]]></title>
            <link>https://www.quantamagazine.org/for-algorithms-a-little-memory-outweighs-a-lot-of-time-20250521/</link>
            <guid>44055347</guid>
            <pubDate>Wed, 21 May 2025 19:34:20 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.quantamagazine.org/for-algorithms-a-little-memory-outweighs-a-lot-of-time-20250521/">https://www.quantamagazine.org/for-algorithms-a-little-memory-outweighs-a-lot-of-time-20250521/</a>, See on <a href="https://news.ycombinator.com/item?id=44055347">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="postBody">
                <div>
        <p>
            One computer scientist’s “stunning” proof is the first progress in 50 years on one of the most famous questions in computer science.        </p>
        
    </div>
<figure>
    
</figure>
    <figure>
        <div>
                            <p><img width="2560" height="1440" src="https://www.quantamagazine.org/wp-content/uploads/2025/05/Space-Complexity-Breakthrough_crIrene-Perez-Lede.webp" alt="" decoding="async" fetchpriority="high" srcset="https://www.quantamagazine.org/wp-content/uploads/2025/05/Space-Complexity-Breakthrough_crIrene-Perez-Lede.webp 2560w, https://www.quantamagazine.org/wp-content/uploads/2025/05/Space-Complexity-Breakthrough_crIrene-Perez-Lede-1720x968.webp 1720w, https://www.quantamagazine.org/wp-content/uploads/2025/05/Space-Complexity-Breakthrough_crIrene-Perez-Lede-520x293.webp 520w, https://www.quantamagazine.org/wp-content/uploads/2025/05/Space-Complexity-Breakthrough_crIrene-Perez-Lede-768x432.webp 768w, https://www.quantamagazine.org/wp-content/uploads/2025/05/Space-Complexity-Breakthrough_crIrene-Perez-Lede-1536x864.webp 1536w, https://www.quantamagazine.org/wp-content/uploads/2025/05/Space-Complexity-Breakthrough_crIrene-Perez-Lede-2048x1152.webp 2048w" sizes="(max-width: 2560px) 100vw, 2560px">                </p>
                        </div>
        <figcaption>
    <div>
            <p>Irene Pérez for&nbsp;<em>Quanta Magazine</em></p>
        </div>
</figcaption>
    </figure>
<div data-role="selectable">
    <p><span>O</span>ne July afternoon in 2024, <a href="https://people.csail.mit.edu/rrw/">Ryan Williams</a> set out to prove himself wrong. Two months had passed since he’d hit upon a startling discovery about the relationship between time and memory in computing. It was a rough sketch of a mathematical proof that memory was more powerful than computer scientists believed: A small amount would be as helpful as a lot of time in all conceivable computations. That sounded so improbable that he assumed something had to be wrong, and he promptly set the proof aside to focus on less crazy ideas. Now, he’d finally carved out time to find the error.</p>
<p>That’s not what happened. After hours of poring over his argument, Williams couldn’t find a single flaw.</p>
<p>“I just thought I was losing my mind,” said Williams, a theoretical computer scientist at the Massachusetts Institute of Technology. For the first time, he began to entertain the possibility that maybe, just maybe, memory really was as powerful as his work suggested.</p>
<p>Over the months that followed, he fleshed out the details, scrutinized every step, and solicited feedback from a handful of other researchers. In February, he finally <a href="https://arxiv.org/abs/2502.17779">posted his proof online</a>, to widespread acclaim.</p>
<p>“It’s amazing. It’s beautiful,” said <a href="https://www.math.ias.edu/avi/home">Avi Wigderson</a>, a theoretical computer scientist at the Institute for Advanced Study in Princeton, New Jersey. As soon as he heard the news, Wigderson sent Williams a congratulatory email. Its subject line: “You blew my mind.”</p>
<p>Time and memory (also called space) are the two most fundamental resources in computation: Every algorithm takes some time to run, and requires some space to store data while it’s running. Until now, the only known algorithms for accomplishing certain tasks required an amount of space roughly proportional to their runtime, and researchers had long assumed there’s no way to do better. Williams’ proof established a mathematical procedure for transforming any algorithm — no matter what it does — into a form that uses much less space.</p>
</div>
    <figure>
        <div>
                            <p><img width="2301" height="1577" src="https://www.quantamagazine.org/wp-content/uploads/2025/05/RyanWilliams-cr.KatherineTaylor-CloseUp.webp" alt="" decoding="async" srcset="https://www.quantamagazine.org/wp-content/uploads/2025/05/RyanWilliams-cr.KatherineTaylor-CloseUp.webp 2301w, https://www.quantamagazine.org/wp-content/uploads/2025/05/RyanWilliams-cr.KatherineTaylor-CloseUp-1720x1179.webp 1720w, https://www.quantamagazine.org/wp-content/uploads/2025/05/RyanWilliams-cr.KatherineTaylor-CloseUp-520x356.webp 520w, https://www.quantamagazine.org/wp-content/uploads/2025/05/RyanWilliams-cr.KatherineTaylor-CloseUp-768x526.webp 768w, https://www.quantamagazine.org/wp-content/uploads/2025/05/RyanWilliams-cr.KatherineTaylor-CloseUp-1536x1053.webp 1536w, https://www.quantamagazine.org/wp-content/uploads/2025/05/RyanWilliams-cr.KatherineTaylor-CloseUp-2048x1404.webp 2048w" sizes="(max-width: 2301px) 100vw, 2301px">                </p>
                        </div>
        <figcaption>
    <div>
                            <p>Ryan Williams stunned his colleagues with a milestone proof about the relationship between time and space in computation.</p>
            <p>Katherine Taylor for&nbsp;<em>Quanta Magazine</em></p>
        </div>
</figcaption>
    </figure>
<div data-role="selectable">
    <p>What’s more, this result — a statement about what you can compute given a certain amount of space — also implies a second result, about what you cannot compute in a certain amount of time. This second result isn’t surprising in itself: Researchers expected it to be true, but they had no idea how to prove it. Williams’ solution, based on his sweeping first result, feels almost cartoonishly excessive, akin to proving a suspected murderer guilty by establishing an ironclad alibi for everyone else on the planet. It could also offer a new way to attack one of the oldest open problems in computer science.</p>
<p>“It’s a pretty stunning result, and a massive advance,” said <a href="https://www.cs.washington.edu/people/faculty/paul-beame/">Paul Beame</a>, a computer scientist at the University of Washington. “It’s a little bit less of a surprise that it’s Ryan doing this.”</p>
<h2><strong>Space to Roam</strong></h2>
<p>Williams, 46, has an open, expressive face and a hint of gray in his hair. His office, which looks out over the colorful spires of MIT’s Stata Center, is another illustration of the creative use of space. A pair of yoga mats have transformed a window ledge into a makeshift reading nook, and the desk is tucked into an oddly shaped corner, freeing up room for a couch facing a large whiteboard brimming with mathematical scribblings.</p>
<p>MIT is a long way from Williams’ childhood home in rural Alabama, where there was no shortage of space. He grew up on a 50-acre farm and first saw a computer at age 7, when his mother drove him across the county for a special academic enrichment class. He recalled being transfixed by a simple program for generating a digital fireworks display.</p>
<p>“It was taking a random color and sending it in a random direction from the middle of the monitor,” Williams said. “You couldn’t have predicted what picture you’re going to get.” The world of computers seemed a wild and wonderful playground, full of infinite possibilities. Young Williams was hooked.</p>
<p>“I was writing programs to myself, on paper, to be run on a future computer,” he said. “My parents didn’t really know what to do with me.”</p>
</div>
    <figure>
        <div>
                            <p><img width="1799" height="2560" src="https://www.quantamagazine.org/wp-content/uploads/2025/05/RyanWilliams-cr.KatherineTaylor-Window-scaled.webp" alt="" decoding="async" srcset="https://www.quantamagazine.org/wp-content/uploads/2025/05/RyanWilliams-cr.KatherineTaylor-Window-scaled.webp 1799w, https://www.quantamagazine.org/wp-content/uploads/2025/05/RyanWilliams-cr.KatherineTaylor-Window-1209x1720.webp 1209w, https://www.quantamagazine.org/wp-content/uploads/2025/05/RyanWilliams-cr.KatherineTaylor-Window-365x520.webp 365w, https://www.quantamagazine.org/wp-content/uploads/2025/05/RyanWilliams-cr.KatherineTaylor-Window-768x1093.webp 768w, https://www.quantamagazine.org/wp-content/uploads/2025/05/RyanWilliams-cr.KatherineTaylor-Window-1079x1536.webp 1079w, https://www.quantamagazine.org/wp-content/uploads/2025/05/RyanWilliams-cr.KatherineTaylor-Window-1439x2048.webp 1439w" sizes="(max-width: 1799px) 100vw, 1799px">                </p>
                        </div>
        <figcaption>
    <div>
                            <p>Williams’ office, like his new result, makes creative use of space.</p>
            <p>Katherine Taylor for&nbsp;<em>Quanta Magazine</em></p>
        </div>
</figcaption>
    </figure>
<div data-role="selectable">
    <p>As he grew older, Williams advanced from imaginary computers to real ones. For his last two years of high school, he transferred to the Alabama School of Math and Science, a prestigious public boarding school, where he first encountered the theoretical side of computer science.</p>
<p>“I realized that there was a wider world of things out there, and that there was a way to think mathematically about computers,” he said. “That’s what I wanted to do.”</p>
<p>Williams was especially drawn to a branch of theoretical computer science called computational complexity theory. It deals with the resources (such as time and space) that are needed to solve computational problems such as sorting lists or factoring numbers. Most problems can be solved by many different algorithms, each with its own demands on time and space. Complexity theorists sort problems into categories, called complexity classes, based on the resource demands of the best algorithms for solving them — that is, the algorithms that run fastest or use the least amount of space.</p>
<p>But how do you make the study of computational resources mathematically rigorous? You won’t get far if you try to analyze time and space by comparing minutes to megabytes. To make any progress, you need to start with the right definitions.</p>
<h2><strong>Getting Resourceful</strong></h2>

<p>Those definitions emerged from the work of Juris Hartmanis, a pioneering computer scientist who had experience making do with limited resources. He was born in 1928 into a prominent Latvian family, but his childhood was disrupted by the outbreak of World War II. Occupying Soviet forces arrested and executed his father, and after the war Hartmanis finished high school in a refugee camp. He went on to university, where he excelled even though he couldn’t afford textbooks.</p>
<p>“I compensated by taking very detailed notes in lectures,” he recalled in a <a href="https://dl.acm.org/doi/10.1145/1141880.1775727">2009 interview</a>. “There is a certain advantage to [having] to improvise and overcome difficulties.” Hartmanis immigrated to the United States in 1949, and worked a series of odd jobs — building agricultural machinery, manufacturing steel and even serving as a butler — while studying mathematics at the University of Kansas City. He went on to a spectacularly successful career in the young field of theoretical computer science.</p>
<p>In 1960, while working at the General Electric research laboratory in Schenectady, New York, Hartmanis met Richard Stearns, a graduate student doing a summer internship. In a pair of <a href="https://www.ams.org/journals/tran/1965-117-00/S0002-9947-1965-0170805-7/">groundbreaking</a> <a href="https://ieeexplore.ieee.org/document/5397244/">papers</a> they established precise mathematical definitions for time and space. These definitions gave researchers the language they needed to compare the two resources and sort problems into complexity classes.</p>
</div>
    <figure>
        <div>
                            <p><img width="1800" height="1033" src="https://www.quantamagazine.org/wp-content/uploads/2025/05/Juris-Hartmanis.webp" alt="" decoding="async" srcset="https://www.quantamagazine.org/wp-content/uploads/2025/05/Juris-Hartmanis.webp 1800w, https://www.quantamagazine.org/wp-content/uploads/2025/05/Juris-Hartmanis-1720x987.webp 1720w, https://www.quantamagazine.org/wp-content/uploads/2025/05/Juris-Hartmanis-520x298.webp 520w, https://www.quantamagazine.org/wp-content/uploads/2025/05/Juris-Hartmanis-768x441.webp 768w, https://www.quantamagazine.org/wp-content/uploads/2025/05/Juris-Hartmanis-1536x881.webp 1536w" sizes="(max-width: 1800px) 100vw, 1800px">                </p>
                        </div>
        <figcaption>
    <div>
                            <p>In the 1960s, Juris Hartmanis established the definitions that computer scientists use to analyze space and time.</p>
            <p>Division of Rare and Manuscript Collections, Cornell University Library</p>
        </div>
</figcaption>
    </figure>
<div data-role="selectable">
    <p>One of the most important classes goes by the humble name “P.” Roughly speaking, it encompasses all problems that can be solved in a reasonable amount of time. An analogous complexity class for space is dubbed “PSPACE.”</p>
<p>The relationship between these two classes is one of the central questions of complexity theory. Every problem in P is also in PSPACE, because fast algorithms just don’t have enough time to fill up much space in a computer’s memory. If the reverse statement were also true, the two classes would be equivalent: Space and time would have comparable computational power. But complexity theorists suspect that PSPACE is a much larger class, containing many problems that aren’t in P. In other words, they believe that space is a far more powerful computational resource than time. This belief stems from the fact that algorithms can use the same small chunk of memory over and over, while time isn’t as forgiving — once it passes, you can’t get it back.</p>
<p>“The intuition is just so simple,” Williams said. “You can reuse space, but you can’t reuse time.”</p>
<p>But intuition isn’t good enough for complexity theorists: They want rigorous proof. To prove that PSPACE is larger than P, researchers would have to show that for some problems in PSPACE, fast algorithms are categorically impossible. Where would they even start?</p>
<h2><strong>A Space Odyssey</strong></h2>
<p>As it happened, they started at Cornell University, where Hartmanis moved in 1965 to head the newly established computer science department. Under his leadership it quickly became a center of research in complexity theory, and in the early 1970s, a pair of researchers there, John Hopcroft and Wolfgang Paul, set out to establish a precise link between time and space.</p>

<p>Hopcroft and Paul knew that to resolve the P versus PSPACE problem, they’d have to prove that you can’t do certain computations in a limited amount of time. But it’s hard to prove a negative. Instead, they decided to flip the problem on its head and explore what you can do with limited space. They hoped to prove that algorithms given a certain space budget can solve all the same problems as algorithms with a slightly larger time budget. That would indicate that space is at least slightly more powerful than time — a small but necessary step toward showing that PSPACE is larger than P.</p>
<p>With that goal in mind, they turned to a method that complexity theorists call simulation, which involves transforming existing algorithms into new ones that solve the same problems, but with different amounts of space and time. To understand the basic idea, imagine you’re given a fast algorithm for alphabetizing your bookshelf, but it requires you to lay out your books in dozens of small piles. You might prefer an approach that takes up less space in your apartment, even if it takes longer. A simulation is a mathematical procedure you could use to get a more suitable algorithm: Feed it the original, and it’ll give you a new algorithm that saves space at the expense of time.</p>
<p>Algorithm designers have long studied these space-time trade-offs for specific tasks like sorting. But to establish a general relationship between time and space, Hopcroft and Paul needed something more comprehensive: a space-saving simulation procedure that works for every algorithm, no matter what problem it solves. They expected this generality to come at a cost. A universal simulation can’t exploit the details of any specific problem, so it probably won’t save as much space as a specialized simulation. But when Hopcroft and Paul started their work, there were no known universal methods for saving space at all. Even saving a small amount would be progress.</p>
<p>The breakthrough came in 1975, after Hopcroft and Paul teamed up with a young researcher named <a href="https://people.seas.harvard.edu/~valiant/">Leslie Valiant</a>. The trio devised a <a href="https://dl.acm.org/doi/10.1145/322003.322015">universal simulation procedure</a> that always saves a bit of space. No matter what algorithm you give it, you’ll get an equivalent one whose space budget is slightly smaller than the original algorithm’s time budget.</p>
<p>“Anything you can do in so much time, you can also do in slightly less space,” Valiant said. It was the first major step in the quest to connect space and time.</p>
<figure>
    <p><img width="1600" height="1556" src="https://www.quantamagazine.org/wp-content/uploads/2025/05/Leslie-Valiant_cr-Katherine-Taylor.webp" alt="" decoding="async" srcset="https://www.quantamagazine.org/wp-content/uploads/2025/05/Leslie-Valiant_cr-Katherine-Taylor.webp 1600w, https://www.quantamagazine.org/wp-content/uploads/2025/05/Leslie-Valiant_cr-Katherine-Taylor-520x506.webp 520w, https://www.quantamagazine.org/wp-content/uploads/2025/05/Leslie-Valiant_cr-Katherine-Taylor-768x747.webp 768w, https://www.quantamagazine.org/wp-content/uploads/2025/05/Leslie-Valiant_cr-Katherine-Taylor-1536x1494.webp 1536w" sizes="(max-width: 1600px) 100vw, 1600px">    </p>
            <figcaption>
                            <p>In 1975, Leslie Valiant helped prove that space is a slightly more powerful computational resource than time.</p>
            <p>Katherine Taylor for&nbsp;<em>Quanta Magazine</em></p>
        </figcaption>
    </figure>

<p>But then progress stalled, and complexity theorists began to suspect that they’d hit a fundamental barrier. The problem was precisely the universal character of Hopcroft, Paul and Valiant’s simulation. While many problems can be solved with much less space than time, some intuitively seemed like they’d need nearly as much space as time. If so, more space-efficient universal simulations would be impossible. Paul and two other researchers soon proved that they are <a href="https://dl.acm.org/doi/10.1145/800113.803643">indeed impossible</a>, provided you make one seemingly obvious assumption: Different chunks of data can’t occupy the same space in memory at the same time.</p>
<p>“Everybody took it for granted that you cannot do better,” Wigderson said.</p>
<p>Paul’s result suggested that resolving the P versus PSPACE problem would require abandoning simulation altogether in favor of a different approach, but nobody had any good ideas. That was where the problem stood for 50 years — until Williams finally broke the logjam.</p>
<p>First, he had to get through college.</p>
<h2><strong>Complexity Classes</strong></h2>
<p>In 1996, the time came for Williams to apply to colleges. He knew that pursuing complexity theory would take him far from home, but his parents made it clear that the West Coast and Canada were out of the question. Among his remaining options, Cornell stood out to him for its prominent place in the history of his favorite discipline.</p>
<p>“This guy Hartmanis defined the time and space complexity classes,” he recalled thinking. “That was important for me.”</p>
<p>Williams was admitted to Cornell with generous financial aid and arrived in the fall of 1997, planning to do whatever it took to become a complexity theorist himself. His single-mindedness stuck out to his fellow students.</p>
<p>“He was just super-duper into complexity theory,” said <a href="https://www.cs.utexas.edu/people/faculty-researchers/scott-aaronson">Scott Aaronson</a>, a computer scientist at the University of Texas, Austin, who overlapped with Williams at Cornell.</p>
</div>
    <figure>
        <div>
                            <p><img width="2560" height="1398" src="https://www.quantamagazine.org/wp-content/uploads/2025/05/RyanWilliams-cr.KatherineTaylor-YellowWall-scaled.webp" alt="" decoding="async" srcset="https://www.quantamagazine.org/wp-content/uploads/2025/05/RyanWilliams-cr.KatherineTaylor-YellowWall-scaled.webp 2560w, https://www.quantamagazine.org/wp-content/uploads/2025/05/RyanWilliams-cr.KatherineTaylor-YellowWall-1720x939.webp 1720w, https://www.quantamagazine.org/wp-content/uploads/2025/05/RyanWilliams-cr.KatherineTaylor-YellowWall-520x284.webp 520w, https://www.quantamagazine.org/wp-content/uploads/2025/05/RyanWilliams-cr.KatherineTaylor-YellowWall-768x419.webp 768w, https://www.quantamagazine.org/wp-content/uploads/2025/05/RyanWilliams-cr.KatherineTaylor-YellowWall-1536x839.webp 1536w, https://www.quantamagazine.org/wp-content/uploads/2025/05/RyanWilliams-cr.KatherineTaylor-YellowWall-2048x1118.webp 2048w" sizes="(max-width: 2560px) 100vw, 2560px">                </p>
                        </div>
        <figcaption>
    <div>
                            <p>Williams grew interested in the relationship between space and time as an undergraduate, but never found an opportunity to work on it until last year.</p>
            <p>Katherine Taylor for&nbsp;<em>Quanta Magazine</em></p>
        </div>
</figcaption>
    </figure>
<div data-role="selectable">
    <p>But by sophomore year, Williams was struggling to keep up in courses that emphasized mathematical rigor over intuition. After he got a middling grade in a class on the theory of computing, the teacher suggested he consider other careers. But Williams wouldn’t give up his dream. He doubled down and enrolled in a graduate theory course, hoping that a stellar grade in the harder class would look impressive on his grad school applications. The professor teaching that graduate course was Hartmanis, by then an elder statesman in the field.</p>
<p>Williams began attending Hartmanis’ office hours every week, where he was almost always the only student who showed up. His tenacity paid off: he earned an A in the course, and Hartmanis agreed to advise him on an independent research project the following semester. Their weekly meetings continued throughout Williams’ time in college. Hartmanis encouraged him to cultivate an individual approach to complexity research and gently steered him away from dead ends.</p>
<p>“I was deeply influenced by him then,” Williams said. “I guess I still am now.”</p>

<p>But despite earning a coveted graduate research fellowship from the National Science Foundation, Williams was rejected by every doctoral program he applied to. On hearing the news, Hartmanis phoned a colleague, then turned around and congratulated Williams on getting accepted into a yearlong master’s program at Cornell. A year later Williams again applied to various doctoral programs, and with that extra research experience under his belt, he found success.</p>
<p>Williams continued working in complexity theory in grad school and the years that followed. In 2010, four years after receiving his doctorate, he proved a <a href="https://ieeexplore.ieee.org/document/5959801">milestone result</a> — a small step, but the largest in decades, toward solving the <a href="https://www.quantamagazine.org/complexity-theorys-50-year-journey-to-the-limits-of-knowledge-20230817/">most famous question in theoretical computer science</a>, about the nature of hard problems. That result cemented Williams’ reputation, and he went on to write dozens of other papers on different topics in complexity theory.</p>
<p>P versus PSPACE wasn’t one of them. Williams had been fascinated by the problem since he first encountered it in college. He’d even supplemented his computer science curriculum with courses in logic and philosophy, seeking inspiration from other perspectives on time and space, to no avail.</p>
<p>“It’s always been in the back of my mind,” Williams said. “I just couldn’t think of anything interesting enough to say about it.”</p>
<p>Last year, he finally found the opportunity he’d been waiting for.</p>
<h2><strong>Squishy Pebbles</strong></h2>
<p>The story of Williams’ new result started with progress on a different question about memory in computation: What problems can be solved with extremely limited space? In 2010, the pioneering complexity theorist Stephen Cook and his collaborators invented a task, called the <a href="http://arxiv.org/abs/1005.2642">tree evaluation problem</a>, that they proved would be impossible for any algorithm with a space budget below a specific threshold. But there was a loophole. The proof relied on the same commonsense assumption that Paul and his colleagues had made decades earlier: Algorithms can’t store new data in space that’s already full.</p>
<p>For over a decade, complexity theorists tried to close that loophole. Then, in 2023, <a href="https://www.falsifian.org/">Cook’s son James</a> and a researcher named <a href="https://iuuk.mff.cuni.cz/~iwmertz/">Ian Mertz</a> blew it wide open, devising <a href="https://dl.acm.org/doi/10.1145/3618260.3649664">an algorithm</a> that solved the tree evaluation problem using <a href="https://www.quantamagazine.org/catalytic-computing-taps-the-full-power-of-a-full-hard-drive-20250218/">much less space</a> than anyone thought possible. The elder Cook’s proof had assumed that bits of data were like pebbles that have to occupy separate places in an algorithm’s memory. But it turns out that’s not the only way to store data. “We can actually think about these pebbles as things that we can squish a little bit on top of each other,” Beame said.</p>
</div>
    <figure>
        <div>
                            <p><img width="2101" height="2379" src="https://www.quantamagazine.org/wp-content/uploads/2025/05/JamesCook_crColinMoris-36.webp" alt="" decoding="async" srcset="https://www.quantamagazine.org/wp-content/uploads/2025/05/JamesCook_crColinMoris-36.webp 2101w, https://www.quantamagazine.org/wp-content/uploads/2025/05/JamesCook_crColinMoris-36-1519x1720.webp 1519w, https://www.quantamagazine.org/wp-content/uploads/2025/05/JamesCook_crColinMoris-36-459x520.webp 459w, https://www.quantamagazine.org/wp-content/uploads/2025/05/JamesCook_crColinMoris-36-768x870.webp 768w, https://www.quantamagazine.org/wp-content/uploads/2025/05/JamesCook_crColinMoris-36-1357x1536.webp 1357w, https://www.quantamagazine.org/wp-content/uploads/2025/05/JamesCook_crColinMoris-36-1809x2048.webp 1809w" sizes="(max-width: 2101px) 100vw, 2101px">                </p>
                                <p><img width="1716" height="1817" src="https://www.quantamagazine.org/wp-content/uploads/2025/05/IanMertz_crMichalKoucky.webp" alt="" decoding="async" srcset="https://www.quantamagazine.org/wp-content/uploads/2025/05/IanMertz_crMichalKoucky.webp 1716w, https://www.quantamagazine.org/wp-content/uploads/2025/05/IanMertz_crMichalKoucky-1624x1720.webp 1624w, https://www.quantamagazine.org/wp-content/uploads/2025/05/IanMertz_crMichalKoucky-491x520.webp 491w, https://www.quantamagazine.org/wp-content/uploads/2025/05/IanMertz_crMichalKoucky-768x813.webp 768w, https://www.quantamagazine.org/wp-content/uploads/2025/05/IanMertz_crMichalKoucky-1451x1536.webp 1451w" sizes="(max-width: 1716px) 100vw, 1716px">                </p>
                        </div>
        <figcaption>
    <div>
                            <p>James Cook (left) and Ian Mertz recently devised a new algorithm that solved a specific problem using much less space than anyone thought possible.</p>
            <p>Colin Morris; Michal Koucký</p>
        </div>
</figcaption>
    </figure>
<div data-role="selectable">
    <p>Cook and Mertz’s algorithm roused the curiosity of many researchers, but it wasn’t clear that it had any applications beyond the tree evaluation problem. “Nobody saw how central it is to time versus space itself,” Wigderson said.</p>
<p>Ryan Williams was the exception. In spring 2024, a group of students gave a presentation about the Cook and Mertz paper as their final project in a class he was teaching. Their enthusiasm inspired him to take a closer look. As soon as he did, an idea jumped out at him. Cook and Mertz’s method, he realized, was really a general-purpose tool for reducing space usage. Why not use it to power a new universal simulation linking time and space — like the one designed by Hopcroft, Paul and Valiant, but better?</p>
<p>That classic result was a way to transform any algorithm with a given time budget into a new algorithm with a slightly smaller space budget. Williams saw that a simulation based on squishy pebbles would make the new algorithm’s space usage much smaller — roughly equal to the square root of the original algorithm’s time budget. That new space-efficient algorithm would also be much slower, so the simulation was not likely to have practical applications. But from a theoretical point of view, it was nothing short of revolutionary.</p>

<p>For 50 years, researchers had assumed it was impossible to improve Hopcroft, Paul and Valiant’s universal simulation. Williams’ idea — if it worked — wouldn’t just beat their record — it would demolish it.</p>
<p>“I thought about it, and I was like, ‘Well, that just simply can’t be true,’” Williams said. He set it aside and didn’t come back to it until that fateful day in July, when he tried to find the flaw in the argument and failed. After he realized that there was no flaw, he spent months writing and rewriting the proof to make it as clear as possible.</p>
<p>At the end of February, Williams finally <a href="https://arxiv.org/abs/2502.17779">put the finished paper online</a>. Cook and Mertz were as surprised as everyone else. “I had to go take a long walk before doing anything else,” Mertz said.</p>
<p>Valiant got a sneak preview of Williams’ improvement on his decades-old result during his morning commute. For years, he’s taught at Harvard University, just down the road from Williams’ office at MIT. They’d met before, but they didn’t know they lived in the same neighborhood until they bumped into each other on the bus on a snowy February day, a few weeks before the result was public. Williams described his proof to the startled Valiant and promised to send along his paper.</p>
<p>“I was very, very impressed,” Valiant said. “If you get any mathematical result which is the best thing in 50 years, you must be doing something right.”</p>
<h2><strong>PSPACE: The Final Frontier</strong></h2>
<p>With his new simulation, Williams had proved a positive result about the computational power of space: Algorithms that use relatively little space can solve all problems that require a somewhat larger amount of time. Then, using just a few lines of math, he flipped that around and proved a negative result about the computational power of time: At least a few problems can’t be solved unless you use more time than space. That second, narrower result is in line with what researchers expected. The weird part is how Williams got there, by first proving a result that applies to all algorithms, no matter what problems they solve.</p>
<p>“I still have a hard time believing it,” Williams said. “It just seems too good to be true.”</p>
</div>
    <figure>
        <div>
                            <p><img width="1967" height="2560" src="https://www.quantamagazine.org/wp-content/uploads/2025/05/RyanWilliams-cr.KatherineTaylor-Stairs-scaled.webp" alt="" decoding="async" srcset="https://www.quantamagazine.org/wp-content/uploads/2025/05/RyanWilliams-cr.KatherineTaylor-Stairs-scaled.webp 1967w, https://www.quantamagazine.org/wp-content/uploads/2025/05/RyanWilliams-cr.KatherineTaylor-Stairs-1322x1720.webp 1322w, https://www.quantamagazine.org/wp-content/uploads/2025/05/RyanWilliams-cr.KatherineTaylor-Stairs-400x520.webp 400w, https://www.quantamagazine.org/wp-content/uploads/2025/05/RyanWilliams-cr.KatherineTaylor-Stairs-768x999.webp 768w, https://www.quantamagazine.org/wp-content/uploads/2025/05/RyanWilliams-cr.KatherineTaylor-Stairs-1180x1536.webp 1180w, https://www.quantamagazine.org/wp-content/uploads/2025/05/RyanWilliams-cr.KatherineTaylor-Stairs-1574x2048.webp 1574w" sizes="(max-width: 1967px) 100vw, 1967px">                </p>
                        </div>
        <figcaption>
    <div>
                            <p>Williams used Cook and Mertz’s technique to establish a stronger link between space and time — the first progress on that problem in 50 years.</p>
            <p>Katherine Taylor for&nbsp;<em>Quanta Magazine</em></p>
        </div>
</figcaption>
    </figure>
<div data-role="selectable">
    <p>Phrased in qualitative terms, Williams’ second result may sound like the long-sought solution to the P versus PSPACE problem. The difference is a matter of scale. P and PSPACE are very broad complexity classes, while Williams’ results work at a finer level. He established a quantitative gap between the power of space and the power of time, and to prove that PSPACE is larger than P, researchers will have to make that gap much, much wider.</p>
<p>That’s a daunting challenge, akin to prying apart a sidewalk crack with a crowbar until it’s as wide as the Grand Canyon. But it might be possible to get there by using a modified version of Williams’ simulation procedure that repeats the key step many times, saving a bit of space each time. It’s like a way to repeatedly ratchet up the length of your crowbar — make it big enough, and you can pry open anything. That repeated improvement doesn’t work with the current version of the algorithm, but researchers don’t know whether that’s a fundamental limitation.</p>
        
        
<p>“It could be an ultimate bottleneck, or it could be a 50-year bottleneck,” Valiant said. “Or it could be something which maybe someone can solve next week.”</p>
<p>If the problem is solved next week, Williams will be kicking himself. Before he wrote the paper, he spent months trying and failing to extend his result. But even if such an extension is not possible, Williams is confident that more space exploration is bound to lead somewhere interesting — perhaps progress on an entirely different problem.</p>
<p>“I can never prove precisely the things that I want to prove,” he said. “But often, the thing I prove is way better than what I wanted.”</p>
<p><em>Editor’s note: Scott Aaronson is a member of&nbsp;</em>Quanta Magazine<em>’s&nbsp;</em><a href="https://www.quantamagazine.org/about/"><em>advisory board</em></a><em>.</em></p>
</div>
                
                
            </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[The curious tale of Bhutan's playable record postage stamps (2015) (110 pts)]]></title>
            <link>https://thevinylfactory.com/features/the-curious-tale-of-bhutans-playable-record-postage-stamps/</link>
            <guid>44054775</guid>
            <pubDate>Wed, 21 May 2025 18:45:18 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://thevinylfactory.com/features/the-curious-tale-of-bhutans-playable-record-postage-stamps/">https://thevinylfactory.com/features/the-curious-tale-of-bhutans-playable-record-postage-stamps/</a>, See on <a href="https://news.ycombinator.com/item?id=44054775">Hacker News</a></p>
<div id="readability-page-1" class="page"><article id="51435" itemscope="" itemtype="http://schema.org/BlogPosting">

        
        <section>

        <!-- VIDEO -->
                        <!--<div id='featured-vf-video' class='video-preview video-preview-single-post'>
                                    </div>-->
                        <!-- END VIDEO -->

            <!-- TIMELINE -->
                        <!-- END TIMELINE -->

            
            <div><p><h4>Published on</h4><h3>December 30, 2015</h3></p><p><h4>Category</h4><h3>Features</h3></p><p><h4>Share</h4><h3></h3></p></div><div><h4>Share</h4></div>
            <div itemprop="articleBody">
                
			<div>
			<p><img alt="" src="https://secure.gravatar.com/avatar/520ffa5590e017bcb5628730985bd695?s=140&amp;d=mm&amp;r=g" srcset="https://secure.gravatar.com/avatar/520ffa5590e017bcb5628730985bd695?s=280&amp;d=mm&amp;r=g 2x" height="140" width="140" loading="lazy" decoding="async"></p>
			
		</div>
	
<p><img loading="lazy" src="https://thevinylfactory.com/wp-content/uploads/2015/12/bhutan-stamps-cover.png" width="665" height="400" alt="">
</p>

            
                <p><img decoding="async" src="http://www.thevinylfactory.com/wp-content/uploads/2015/12/cover.png" alt="cover" width="665" height="400" srcset="https://thevinylfactory.com/wp-content/uploads/2015/12/cover.png 665w, https://thevinylfactory.com/wp-content/uploads/2015/12/cover-640x385.png 640w, https://thevinylfactory.com/wp-content/uploads/2015/12/cover-320x192.png 320w" sizes="(max-width: 665px) 100vw, 665px"></p>
<p><strong>Made of plastic and embossed with a melody, these <a href="http://www.thevinylfactory.com/vinyl-factory-news/listen-to-tiny-vinyl-record-postage-stamps-that-were-used-to-send-letters-in-1972/" target="_blank">tiny record stamps</a> are among the strangest, most enchanting bits of vinyl out there. Chris May investigates how the Buddhist Kingdom of Bhutan came to produce the world’s “first talking stamps”.</strong></p>
<p>Quite possibly the smallest vinyl records that can still be played with a stylus, those of you after a set will be happy to know the stamps still sporadically appear on <a href="http://www.ebay.com/itm/BHUTAN-Talking-Stamps-Gramophone-Records-With-Folk-Songs-And-Folk-Tales-/121793379895?hash=item1c5b737e37:g:Co4AAOSwaNBUf71F" target="_blank">eBay</a>, with prices pushed up by that geekiest of venn diagrams between stamp and record collectors.</p>
<p>Listen to a haunting version of the Bhutanese national anthem, as recorded from a stamp, below.</p>

<hr>
<p>If you’re looking to make a smart investment in vinyl, among the most promising items to look out for right now are a set of Bhutanese talking-stamps. Issued in 1972 in a set of seven, the stamps are miniature, one-sided, 33 1⁄3 rpm vinyl records playable on a standard turntable. You peeled off the backing paper and stuck them on an envelope or postcard. Content includes Bhutanese folk songs and histories of the country in English and Dzongkha, the local language. </p>
<p>For decades, the stamps were dismissed by the philatelic establishment as tacky novelties and were, correspondingly, as cheap as chips. In 1993, when they were first listed in the collectors’ bible, Scott’s Standard Postage Stamp Catalogue, a mint condition set was valued at around £17, the equivalent of £28 at 2015 prices. But following their recent discovery by collectors of rare vinyl, particularly in the US, you will be lucky to pick a set up for less than £300. Some observers predict the price will double, or even triple, over the next few years.</p>
<p><img decoding="async" loading="lazy" src="http://www.thevinylfactory.com/wp-content/uploads/2015/12/Bhutanrecord001.jpg" alt="Bhutanrecord001" width="465" height="514" srcset="https://thevinylfactory.com/wp-content/uploads/2015/12/Bhutanrecord001.jpg 465w, https://thevinylfactory.com/wp-content/uploads/2015/12/Bhutanrecord001-320x354.jpg 320w" sizes="(max-width: 465px) 100vw, 465px"></p>
<p>Bhutan’s talking stamps were the invention of the American adventurer Burt Todd, who was born into a wealthy Pittsburgh steel-producing dynasty in 1924. Appropriately, this was the same year F. Scott Fitzgerald completed <em>The Great Gatsby</em>, for the globe-trotting, airplane-piloting, big game-hunting Todd resembled the novel’s protagonist. Todd’s many other quixotic ventures included setting up a rum-making distillery for the government of Fiji, and buying the Rolls-Royces belonging to Indian maharajas who had been impoverished by the loss of British subsidies when the country gained independence, reselling them on the international classic-car market. </p>
<p>After wartime service in the US Army Air Corps and keen to avoid, or at least delay, joining the family business, Todd opted to enrol at Oxford University. Lacking the required  qualifications, he was told that the only person who could authorise his enrolment was in rural Norway on his honeymoon. Todd flew to Norway, tracked the man down and enrolled. He graduated in 1949 with a degree in law.</p>
<p><img decoding="async" loading="lazy" src="http://www.thevinylfactory.com/wp-content/uploads/2015/12/bhutan_stamps2sdfsdfdsf.jpg" alt="bhutan_stamps2sdfsdfdsf" width="465" height="310" srcset="https://thevinylfactory.com/wp-content/uploads/2015/12/bhutan_stamps2sdfsdfdsf.jpg 465w, https://thevinylfactory.com/wp-content/uploads/2015/12/bhutan_stamps2sdfsdfdsf-320x213.jpg 320w" sizes="(max-width: 465px) 100vw, 465px"></p>
<p>At Oxford, one of Todd’s friends was Ashi Kesang Choden-Dorfi, the future queen of Bhutan. In 1951, Choden-Dorfi invited Todd to Bhutan for her wedding to King Jigme Dorji-Wangchuck. There were no airfields in Bhutan, and few roads, and Todd had to travel much of the way from India on foot and horseback. His account of the journey appeared in the December 1952 issue of National Geographic magazine. Two years later, he honeymooned in Bhutan.</p>
<p>In the late 1950s, Dorji-Wangchuck asked Todd to help Bhutan raise a $10m loan from the World Bank. Todd went to Washington to lobby the bank, but the loan was refused: Bhutan was in a border dispute with its larger neighbour India, with whom the bank wanted to keep on good terms. But an official suggested Bhutan raised the money by issuing postage stamps, as had other small countries such as Monaco.</p>
<p><img decoding="async" loading="lazy" src="http://www.thevinylfactory.com/wp-content/uploads/2015/12/bhutan-stamps-all.jpg" alt="bhutan stamps all" width="665" height="742" srcset="https://thevinylfactory.com/wp-content/uploads/2015/12/bhutan-stamps-all.jpg 665w, https://thevinylfactory.com/wp-content/uploads/2015/12/bhutan-stamps-all-640x714.jpg 640w, https://thevinylfactory.com/wp-content/uploads/2015/12/bhutan-stamps-all-320x357.jpg 320w" sizes="(max-width: 665px) 100vw, 665px"></p>
<p>In 1960, with a royal warrant to set up Bhutan’s stamp-issuing programme, Todd founded the Bhutan Stamp Agency. His programme started out conventionally with designs featuring such exotica as the Bhutanese royal crest, a yak, a maharaja, a Himalayan fortress, a soldier from the royal bodyguard and a Buddhist monastery. The designs were pretty but lacked enough novelty to get noticed on the world stamp-collecting market. Todd realised that to sell large numbers of stamps he needed a more attention-gaining approach. </p>
<p>Todd’s first innovation, in 1966, was a circular stamp marking the 40th anniversary of Dorji-Wangchuk’s coronation, using a heraldic design embossed in gold on rainbow-coloured paper. It sold well. Next up was a series of triangular stamps featuring the yeti, the Himalayas’ fabled abominable-snowman. In 1967 came a set of 3D stamps on the theme of the space exploration; Todd’s stamps showing astronauts and rockets were laminated on prismatic ribbed-plastic and gave a convincing 3D effect. Over 200,000 sets were sold. Other successful issues included a set of Buddhist banners printed on silk, a set of traditional sculptures die-stamped in plastic, perfumed stamps and stamps made out of steel foil. </p>
<p><img decoding="async" loading="lazy" src="http://www.thevinylfactory.com/wp-content/uploads/2015/12/Bhutan.jpg" alt="Bhutan" width="665" height="397" srcset="https://thevinylfactory.com/wp-content/uploads/2015/12/Bhutan.jpg 665w, https://thevinylfactory.com/wp-content/uploads/2015/12/Bhutan-640x382.jpg 640w, https://thevinylfactory.com/wp-content/uploads/2015/12/Bhutan-320x191.jpg 320w" sizes="(max-width: 665px) 100vw, 665px"></p>
<p>The talking stamps were the crowning glory of Todd’s programme. The set consisted of a yellow on red design containing a capsule history of Bhutan in Dzongkha, a gold on green with the national anthem, a silver on blue with the history of Bhutan narrated by Todd in English, a silver on purple featuring a folk song, a silver on black with second folk song, a red on white with a third, and a black on yellow containing the English-language history and two of the folk songs. </p>
<p>Todd passed in 2006, but his flair for innovative stamp designs ran in the family. Shortly before he died, his daughter Frances notched up another first with a series of CD-ROM stamps carrying documentary videos about Bhutan.</p>

                

                
            </div>

            
    


        </section>

        
        
<section>
	<picture>
		<source media="(max-width: 480px)" srcset="https://thevinylfactory.com/wp-content/themes/The-Stores/assets/images/newsletter-bg-sm.webp" type="image/webp">
		<source media="(min-width: 481px)" srcset="https://thevinylfactory.com/wp-content/themes/The-Stores/assets/images/newsletter-bg.webp" type="image/webp">
		<img src="https://thevinylfactory.com/wp-content/themes/The-Stores/assets/images/newsletter-bg-sm.webp" loading="lazy" alt="Vinyl Record Playing">
	</picture>
	<div>
		<h4>Sign up to our weekly newsletter</h4>
		<p>Never miss a thing</p>
		 

	</div>
</section>
    </article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Possible new dwarf planet found in our solar system (138 pts)]]></title>
            <link>https://www.minorplanetcenter.net/mpec/K25/K25K47.html</link>
            <guid>44054620</guid>
            <pubDate>Wed, 21 May 2025 18:32:24 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.minorplanetcenter.net/mpec/K25/K25K47.html">https://www.minorplanetcenter.net/mpec/K25/K25K47.html</a>, See on <a href="https://news.ycombinator.com/item?id=44054620">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="main">

<!-- Navigation Menu block -->





<!-- Main content block -->


<p>
The following <a href="https://www.minorplanetcenter.net/iau/services/MPEC.html"><i>Minor Planet Electronic
Circular</i></a> may be linked-to from your own Web pages, but must
<b>not</b> otherwise be <a href="https://www.minorplanetcenter.net/iau/WWWPolicy.html">redistributed electronically</a>.
</p><p>
A form allowing access to any <i>MPEC</i> is at
<a href="https://www.minorplanetcenter.net/mpec/K25/K25K47.html#form">the bottom of this page</a>.
</p><hr><center>
<a href="https://www.minorplanetcenter.net/mpec/K25/K25K46.html"><img src="https://www.minorplanetcenter.net/iau/figs/LArrow.gif" alt=""> Read <i>MPEC</i> 2025-K46</a>
<a href="https://www.minorplanetcenter.net/mpec/K25/K25K48.html"><img src="https://www.minorplanetcenter.net/iau/figs/RArrow.gif" alt=""> Read <i>MPEC</i> 2025-K48</a>
</center>
<hr>
<pre><b>M.P.E.C. 2025-K47                                  Issued 2025 May 21, 17:10 UT

     The Minor Planet Electronic Circulars contain information on unusual
         minor planets, routine data on comets and natural satellites,
         and occasional editorial announcements.  They are published
    on behalf of Division F of the International Astronomical Union by the
          Minor Planet Center, Smithsonian Astrophysical Observatory,
                          Cambridge, MA 02138, U.S.A.
			  
             Prepared using the Tamkin Foundation Computer Network

                              MPC@CFA.HARVARD.EDU
            URL https://www.minorplanetcenter.net/    ISSN 1523-6714

                               <b>2017 OF201</b>                                


Observations:
     K17OK1F &amp;C2011 08 31.47629901 36 40.144+25 44 31.95         22.66rWEK047T14
     K17OK1F &amp;C2011 08 31.51931001 36 40.074+25 44 32.08         22.60rWEK047T14
     K17OK1F &amp;C2011 08 31.56247701 36 39.996+25 44 32.11         22.63rWEK047T14
     K17OK1F &amp;C2011 09 05.48962201 36 31.220+25 44 32.41         22.58rWEK047T14
     K17OK1F &amp;C2011 11 24.38075501 33 14.889+25 29 41.15         22.59rWEK047T14
     K17OK1F &amp;C2011 11 30.38590401 33 02.846+25 27 58.06         22.61rWEK047T14
     K17OK1F &amp;C2011 11 30.40231601 33 02.814+25 27 57.75         22.61rWEK047T14
     K17OK1F &amp;C2011 12 01.39764901 33 00.946+25 27 40.74         22.63rWEK047T14
     K17OK1F &amp;C2012 01 28.24831901 32 30.863+25 15 22.20         22.66rWEK047T14
     K17OK1F NC2014 08 13.41269901 42 46.672+26 25 58.51         21.79zUEK047W84
     K17OK1F NC2015 10 11.22165101 42 46.775+26 37 44.50         22.54rUEK047W84
     K17OK1F NC2015 10 11.22339201 42 46.783+26 37 44.42         23.70gUEK047W84
     K17OK1F*NC2017 07 23.43911401 48 22.616+27 02 09.48         21.77zUEK047W84
     K17OK1F NC2017 09 17.30671401 47 24.008+27 06 52.60         22.71rUEK047W84
     K17OK1F NC2017 11 12.13920001 45 04.120+26 57 08.36         24.32gUEK047W84
     K17OK1F NC2018 07 26.42913801 50 10.026+27 15 20.25         22.02zUEK047W84
     K17OK1F NC2018 08 10.38536501 50 06.204+27 17 54.30         23.49gUEK047W84
     K17OK1F NC2018 08 11.37484001 50 05.580+27 18 02.49         22.62rUEK047W84
     K17OK1F NC2018 10 31.16980401 47 24.018+27 13 04.03         22.65rUEK047W84

Observer details:
T14 Canada-France-Hawaii Telescope, Maunakea.  Observer .  Measurers J. Li, S.
    Cheng.  3.6-m f/4.18 reflector + CCD.
W84 Cerro Tololo-DECam.  Observer .  Measurers J. Li, S. Cheng.  4.0-m f/2.87
    reflector + CCD.

Orbital elements:
2017 OF201                                                                      
Epoch 2025 May 5.0 TT = JDT 2460800.5                   Alexandersen            
M   1.30220              (2000.0)            P               Q                  
n   0.00003775     Peri.  338.15750     +0.60705834     +0.78128316             
a 880.0169161      Node   328.66696     -0.68151060     +0.41790320             
e   0.9485897      Incl.   16.21146     -0.40868506     +0.46363083             
P    26106         H    3.55          G   0.15           U   9                  
Residuals in seconds of arc
110831 T14  0.1-  0.1-    111201 T14  0.1+  0.0     171112 W84  0.1-  0.0
110831 T14  0.0   0.0     120128 T14  0.1+  0.0     180726 W84  0.0   0.0
110831 T14  0.1-  0.0     140813 W84  0.0   0.0     180810 W84  0.0   0.2-
110905 T14  0.0   0.0     151011 W84  0.1-  0.2+    180811 W84  0.0   0.1-
111124 T14  0.0   0.0     151011 W84  0.1+  0.1+    181031 W84  0.2-  0.1+
111130 T14  0.0   0.0     170723 W84  0.3+  0.0
111130 T14  0.0   0.0     170917 W84  0.0   0.1-

Ephemeris:
2017 OF201               a,e,i = 880.02, 0.95, 16                q = 45.2419
Date    TT    R. A. (2000) Decl.     Delta      r    Elong.  Phase     V
2025 04 21    01 58 57.6 +28 10 11  91.5165 90.5532    16.5     0.2    23.2
...
2025 05 06    01 59 42.4 +28 14 03  91.5444 90.5793    16.8     0.2    23.2
...
2025 05 14    02 00 05.4 +28 16 19  91.5340 90.5932    21.3     0.2    23.2
...
2025 05 20    02 00 22.1 +28 18 04  91.5150 90.6036    25.6     0.3    23.2
2025 05 21    02 00 24.8 +28 18 22  91.5110 90.6053    26.4     0.3    23.2
2025 05 22    02 00 27.5 +28 18 40  91.5066 90.6071    27.1     0.3    23.2
...
2025 05 28    02 00 43.2 +28 20 29  91.4754 90.6175    32.0     0.3    23.2
...
2025 06 05    02 01 02.6 +28 22 55  91.4202 90.6314    38.8     0.4    23.2
...
2025 06 20    02 01 33.6 +28 27 25  91.2800 90.6575    52.0     0.5    23.2

M. P. C. Staff               (C) Copyright 2025 MPC           M.P.E.C. 2025-K47
</b></pre><p><b>
<hr><center>
<a href="https://www.minorplanetcenter.net/mpec/K25/K25K46.html"><img src="https://www.minorplanetcenter.net/iau/figs/LArrow.gif" alt=""> Read <i>MPEC</i> 2025-K46</a>
<a href="https://www.minorplanetcenter.net/mpec/K25/K25K48.html"><img src="https://www.minorplanetcenter.net/iau/figs/RArrow.gif" alt=""> Read <i>MPEC</i> 2025-K48</a>
</center>
<hr><ul>
<li>1997-B01 (the full form)
</li><li>J97B01 (the packed version of the full form)
</li><li>B01 (the abbreviated form)
</li></ul>
<!-- Body postamble -->
      </b></p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[An upgraded dev experience in Google AI Studio (164 pts)]]></title>
            <link>https://developers.googleblog.com/en/google-ai-studio-native-code-generation-agentic-tools-upgrade/</link>
            <guid>44054185</guid>
            <pubDate>Wed, 21 May 2025 17:53:40 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://developers.googleblog.com/en/google-ai-studio-native-code-generation-agentic-tools-upgrade/">https://developers.googleblog.com/en/google-ai-studio-native-code-generation-agentic-tools-upgrade/</a>, See on <a href="https://news.ycombinator.com/item?id=44054185">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>

    
      
    

    

    

    

    
    <div>
          

<div>
    <p data-block-key="f2l3d"><br><a href="https://aistudio.google.com/apps">Google AI Studio</a> is the fastest place to start building with the Gemini API, with access to our most capable models, including Gemini 2.5 preview models, and generative media models like Imagen, Lyria RealTime, and Veo. At Google I/O, we announced new features to help you build and deploy complete applications, new model capabilities, and new features in the <a href="https://ai.google.dev/gemini-api/docs/migrate">Google Gen AI SDK</a>.</p><h2 data-block-key="lsqlg" id="build-apps-with-gemini-2.5-pro-code-generation"><b><br></b>Build apps with Gemini 2.5 Pro code generation</h2><p data-block-key="cve2q">Gemini 2.5 Pro is incredible at coding, so we’re excited to bring it to Google AI Studio’s native code editor. It’s tightly optimized with our Gen AI SDK so it’s easier to generate apps with a simple text, image, or video prompt. The new <a href="https://aistudio.google.com/apps">Build</a> tab is now your gateway to quickly build and deploy AI-powered web apps. We’ve also launched new <a href="http://aistudio.google.com/apps?source=showcase">showcase</a> examples to experiment with new models and more.</p>
</div>   

<div>
    
        <video autoplay="" loop="" muted="" playsinline="" poster="https://storage.googleapis.com/gweb-developer-goog-blog-assets/original_videos/wagtailvideo-yru3jo8a_thumb.jpg">
<source src="https://storage.googleapis.com/gweb-developer-goog-blog-assets/original_videos/text-adventure-imagen-gemini-google-ai-studio.mp4" type="video/mp4">
<p>Sorry, your browser doesn't support playback for this video</p>

</video>
    
    
        
            <p>Video of text adventure game being generated with Imagen + Gemini
(video sped up for illustrative purposes)</p>
        
    
</div>  <p data-block-key="ysb9i">In addition to app generation from a single prompt, you can continue to iterate your web app over chat. This allows you to make changes, view diffs, and even jump back to previous checkpoints to revert edits.</p>   

<div>
    
        <video autoplay="" loop="" muted="" playsinline="" poster="https://storage.googleapis.com/gweb-developer-goog-blog-assets/original_videos/wagtailvideo-6txj0c5t_thumb.jpg">
<source src="https://storage.googleapis.com/gweb-developer-goog-blog-assets/original_videos/code-generation-imagen-gemini-google-ai-studio.mp4" type="video/mp4">
<p>Sorry, your browser doesn't support playback for this video</p>

</video>
    
    
        
            <p>Compare code versions and use expanded file structure to help manage project development (video sped up for illustrative purposes)</p>
        
    
</div>  <p data-block-key="c7qcj">You can also deploy those newly created apps in a single click to <a href="https://cloud.google.com/run">Cloud Run</a>.</p>   

<div>
    
        <video autoplay="" loop="" muted="" playsinline="" poster="https://storage.googleapis.com/gweb-developer-goog-blog-assets/original_videos/wagtailvideo-cezbp4n9_thumb.jpg">
<source src="https://storage.googleapis.com/gweb-developer-goog-blog-assets/original_videos/CR_deploy_mviV4si.mp4" type="video/mp4">
<p>Sorry, your browser doesn't support playback for this video</p>

</video>
    
    
        
            <p>Quickly deploy your created apps on Cloud Run (video sped up for illustrative purposes)</p>
        
    
</div>  <div>
    <p data-block-key="ysb9i">Google AI Studio apps and generated code leverage a unique placeholder API key, allowing Google AI Studio to proxy all Gemini API calls. Consequently, when you share your app with Google AI Studio, all API usage by its users is attributed to their Google AI Studio free of charge usage, completely bypassing your own API key and quota. You can read more in our <a href="https://aistudio.google.com/app/apps?source=faq">FAQ</a>.</p><p data-block-key="de0nv">This feature is experimental, so you should always check code before sharing your project externally. Our one-shot generation has been primarily optimized to work with Gemini and Imagen models, with support for more models and tool calls coming soon.</p><h2 data-block-key="cqlne" id=""><b><br></b>Multimodal generation made easy in Google AI Studio</h2><p data-block-key="3qmu4">We've been working hard to get Google DeepMind’s advanced multimodal models into developers’ toolboxes, faster. The new <a href="http://aistudio.google.com/gen-media">Generate Media</a> page centralizes the discovery of Imagen, Veo, Gemini with native image generation, and new native speech generation models. Plus, experience interactive music generation with Lyria RealTime with the PromptDJ apps built in Google AI Studio.</p>
</div>  <div>
    
    
        
            <p>Explore our generative media models from the Google AI Studio Generate Media tab (video sped up for illustrative purposes)</p>
        
    
</div>  <div>
    <h2 data-block-key="lgpyc" id="new-native-audio-for-the-live-api-and-text-to-speech-(tts)">New native audio for the Live API and text-to-speech (TTS)</h2><p data-block-key="bn3mp">With Gemini 2.5 Flash native audio dialog in preview in the Live API, the model now generates even more natural responses with support for over 30 voices. We’ve also added proactive audio so the model can distinguish between the speaker and background conversations, so it knows when to respond. This makes it possible for you to build conversational AI agents and experiences that feel more intuitive and natural.</p>
</div>  <div>
    
    
        
            <p>Try native audio dialog in the Google AI Studio Stream tab  (video sped up for illustrative purposes)</p>
        
    
</div>  <p data-block-key="ysb9i">In addition to the Live API, we’ve announced Gemini 2.5 Pro and Flash previews for text-to-speech (TTS) that support native audio output. Now you can craft single and multi-speaker output with flexible control over delivery style.</p>  <div>
    
    
        
            <p>Generate speech using the new TTS capabilities  (video sped up for illustrative purposes)</p>
        
    
</div>  <div>
    <p data-block-key="ysb9i">Try native audio in the Live API from the <a href="http://aistudio.google.com/live">Stream</a> tab and experience new TTS capabilities via <a href="https://aistudio.google.com/generate-speech">Generate Speech</a>.</p><h2 data-block-key="qvly9" id="model-context-protocol-(mcp)-support"><br>Model Context Protocol (MCP) support</h2><p data-block-key="di62g">Model Context Protocol (MCP) definitions are also now natively supported in the Google Gen AI SDK for easier integration with a growing number of open-source tools. We’ve included a demo <a href="https://aistudio.google.com/apps/bundled/mcp_maps_basic">app</a> that shows how you can use an MCP server within Google AI Studio that combines Google Maps and the Gemini API</p>
</div>   


    
    <div>
        <p><img src="https://storage.googleapis.com/gweb-developer-goog-blog-assets/images/image6_Ygpnr0j.original.png" alt="Model Context Protocol (MCP)">
            
            
        </p>
    </div>
  <div>
    <h2 data-block-key="1ftqy" id="new-url-context-tool">New URL Context tool</h2><p data-block-key="1gutl">URL Context is a new experimental tool that gives the model the ability to retrieve and reference content from links you provide. This is helpful for fact-checking, comparison, summarization, and deeper research.</p>
</div>   

<div>
    
        <video autoplay="" loop="" muted="" playsinline="" poster="https://storage.googleapis.com/gweb-developer-goog-blog-assets/original_videos/wagtailvideo-dtlzdb87_thumb.jpg">
<source src="https://storage.googleapis.com/gweb-developer-goog-blog-assets/original_videos/URLContext.mp4" type="video/mp4">
<p>Sorry, your browser doesn't support playback for this video</p>

</video>
    
    
</div>  <div>
    <h2 data-block-key="mfh8o" id="start-building-in-google-ai-studio">Start building in Google AI Studio</h2><p data-block-key="5qalh">We’re thrilled to bring all of these updates to Google AI Studio, making it the place for developers to explore and build with the latest models Google has to offer.</p><p data-block-key="7loqp">Explore this announcement and all Google I/O 2025 updates on <a href="https://io.google/2025/?utm_source=blogpost&amp;utm_medium=pr&amp;utm_campaign=event&amp;utm_content=">io.google</a> starting May 22.</p>
</div> 
      </div>
    

    

    
    
    
  </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[LLM function calls don't scale; code orchestration is simpler, more effective (242 pts)]]></title>
            <link>https://jngiam.bearblog.dev/mcp-large-data/</link>
            <guid>44053744</guid>
            <pubDate>Wed, 21 May 2025 17:18:52 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://jngiam.bearblog.dev/mcp-large-data/">https://jngiam.bearblog.dev/mcp-large-data/</a>, See on <a href="https://news.ycombinator.com/item?id=44053744">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
    

    
        
    

    
        

        <p>
            <i>
                <time datetime="2025-05-20T15:25Z">
                    20 May, 2025
                </time>
            </i>
        </p>
    

    <p><em>TL;DR: Giving LLMs the full output of tool calls is costly and slow. Output schemas will enable us to get structured data, so we can let the LLM orchestrate processing with generated code. Tool calling in code is simplifying and effective.</em></p>
<hr>
<p>One common practice for working with MCP tools calls is to put the outputs from a tool back into the LLM as a message, and ask the LLM for the next step. The hope here is that the model figures out how to interpret the data, and identify the correct next action to take.</p>
<p>This can work beautifully when the amount of data is small, but we found that when we tried MCP servers with real-world data, it quickly breaks down.</p>
<h3 id="mcp-in-the-real-world">MCP in the real-world</h3><p>We use Linear and Intercom at our company. We connected to their latest official MCP servers released last week to understand how they were returning tool calls.</p>
<p>It turns out that both servers returned large JSON blobs in their text content. These appeared to be similar to their APIs, with the exception that the text content did not come with any pre-defined schemas. This meant that the only reasonable way to parse them was to have the LLM interpret the data.</p>
<p><img src="https://storage.googleapis.com/lutra-2407-public/42067e56d7b5238d3d7432718f1224679a1a449711a2e406295a3aeb98108e63.png" alt="Linear MCP returns list of issues as a big JSON blob"></p>
<p>These JSON blobs are huge! When we asked Linear's MCP to list issues in our project, the tool call defaulted to returning only 50 issues, ~70k characters corresponding to ~25k tokens.</p>
<p><img src="https://storage.googleapis.com/lutra-2407-public/dda9436e83214ae28c73b19ca4fdc43d81f5c1613a45ada3160dc4aaa048f07d.png" alt="ID fields become lots of tokens"></p>
<p>The JSON contains lots of <code>id</code> fields that take up many tokens, and are not semantically meaningful.</p>
<p>When using Claude with MCPs, it seems that the entire JSON blob gets sent back to the model verbatim.</p>
<p>This approach quickly runs into issues. For example, if we wanted to get the AI to sort all the issues by due date and display them, it would need to reproduce all the issues verbatim as output tokens! It'd be slow, costly, and could potentially miss data.</p>
<p>The data in our issues also often contained a lot of distracting information: steps to reproduce an issue, errors, maybe even prompts a user used, or instructions to follow up with a user. The model could fail to emit some of these data accurately, or even worse, deviate from the original instructions.</p>
<h3 id="data-vs-orchestration">Data vs Orchestration</h3><p>The core problem here is that we're confounding orchestration and data processing together in the same chat thread.</p>
<p>The "multi-agent" approach tries to address this by spinning up another chat thread ("agent") to focus only on the data processing piece. It performs better when carefully tuned, but it's still awkward when our data is already well structured.</p>
<p>If the MCP servers are already returning data in a JSON format, it seems much more natural to parse the data, and instead operate on the structured data. Back to our sorting example, rather than asking the LLM to reproduce the outputs directly, we could instead run a <code>sort</code> operation on the data, and return the new array. No hallucinations and this scales to any size of inputs.</p>
<h3 id="code-execution-as-data-processing">Code execution as data processing</h3><p>This sounds oddly familiar as we already have code interpreters with AI. When we start to bring code execution as a fundamental way to process data from MCP tools (<a href="https://machinelearning.apple.com/research/codeact">Code Act</a>, <a href="https://huggingface.co/blog/smolagents">Smol Agents</a>), this opens the door to scalable ways for AI models to work.</p>
<p><strong>Variables as memory.</strong> Instead of having an external memory system, the LLM can use variables (system memory) to store any data. Storing a memory is assigning a value to a variable, peeking at the variable is printing it, and the model can choose to pass the variable as an argument when calling another function. Even better, if the language used is well-typed, the model also leverages the schema.</p>
<p><strong>Tool chaining.</strong> Code can orchestrate multiple function calls: performing them in parallel or taking the outputs from one or more calls and using them as inputs into another. The dependencies between the function calls are implicitly represented via the computation graph the code represents. Importantly, the LLM is not required to regurgitate the data, and we have guarantees of completeness.</p>
<p><strong>Scalable processing.</strong> Transforming large amounts of data is naturally possible with code. The model can choose to use loops, or lean on libraries such as NumPy or pandas for large data transformations.</p>
<p>Code can also call other LLMs under the hood: you can have the LLM write code that calls LLMs for unstructured data processing (LLM-inception).</p>
<h3 id="is-mcp-ready">Is MCP ready?</h3><p>MCP specs already define input schemas, and they’ve just introduced  <a href="https://github.com/modelcontextprotocol/modelcontextprotocol/pull/371">output schemas</a>.</p>
<p>Once output schemas are widespread, we expect them to unlock use cases on large datasets: building custom dashboards, creating weekly reports on tickets completed, or having the autonomous agents monitor and nudge stalled issues forward.</p>
<h3 id="what-makes-code-execution-hard">What makes code execution hard?</h3><p>The challenge now shifts to the MCP client side. Most execution environments today run in a tightly controlled sandbox; security is paramount as we're dealing with user-/AI-generated code.</p>
<p>Allowing an execution environment to also access MCPs, tools, and user data requires careful design to where API keys are stored, and how tools are exposed.</p>
<p>In our designs, we created sandboxed environments that are keyed with specific API access, the models are provided with documentation on how to call these APIs such that they are able send/retrieve information without ever seeing secrets.</p>
<p>Most execution environments are stateful (e.g., they may rely on running Jupyter kernels for each user session). This is hard to manage and expensive if users expect to be able to come back to AI task sessions later. A stateless-but-persistent execution environment is paramount for long running (multi-day) task sessions.</p>
<p>These constraints are creating what we think is a new category of runtimes - "AI runtimes" that use LLMs to orchestrate and perform tasks. We're still in the early phases of working out all the details for this code-execution approach, and we'd love feedback from anyone tackling similar problems. If you're interested in our approach, you can head to <a href="https://lutra.ai/">Lutra</a> to give it a try.</p>


    

    
        

        
            


        
    


  </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Storefront Web Components (145 pts)]]></title>
            <link>https://shopify.dev/docs/api/storefront-web-components</link>
            <guid>44053603</guid>
            <pubDate>Wed, 21 May 2025 17:08:58 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://shopify.dev/docs/api/storefront-web-components">https://shopify.dev/docs/api/storefront-web-components</a>, See on <a href="https://news.ycombinator.com/item?id=44053603">Hacker News</a></p>
&lt;Unparsable&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[Collaborative Text Editing Without CRDTs or OT (246 pts)]]></title>
            <link>https://mattweidner.com/2025/05/21/text-without-crdts.html</link>
            <guid>44053560</guid>
            <pubDate>Wed, 21 May 2025 17:05:45 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://mattweidner.com/2025/05/21/text-without-crdts.html">https://mattweidner.com/2025/05/21/text-without-crdts.html</a>, See on <a href="https://news.ycombinator.com/item?id=44053560">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>

    

    <p><i>
    Matthew Weidner |
    
    May 21st, 2025
    
    <br>
    <a href="https://mattweidner.com/">Home</a> | <a href="https://mattweidner.com/feed.xml">RSS Feed</a>
    <br>
    Keywords: text editing, server reconciliation, Articulated
    </i></p>

    <p>Collaborative text editing is arguably the hardest feature to implement in a collaborative app. Even if you use a central server and a fully general solution to optimistic local updates (<a href="https://mattweidner.com/2024/06/04/server-architectures.html#1-server-reconciliation">server reconciliation</a>), text editing in particular requires fancy algorithms - specifically, the <a href="https://mattweidner.com/2024/06/04/server-architectures.html#challenge-text-and-lists">core of a text-editing CRDT or OT</a>.</p>

<p>Or rather, that’s what I thought until recently. This blog post describes an alternative, straightforward approach to collaborative text editing, without Conflict-free Replicated Data Types (CRDTs) or Operational Transformation (OT). By making text editing flexible and easy to DIY, I hope that the approach will let you create rich collaborative apps that are challenging to build on top of a black-box CRDT/OT library.</p>

<p>This post builds off of my previous post, <a href="https://mattweidner.com/2024/06/04/server-architectures.html">Architectures for Central Server Collaboration</a>, though I’ll recap ideas as needed to keep it self-contained. In particular, I’ll generally assume that your collaborative app has a central server. A <a href="#decentralized-variants">later section</a> extends the approach to decentralized/server-optional collaboration, revealing surprising links to several text-editing CRDTs.</p>

<p>As usual for collaborative text editing, any technique in this post can also be applied to general collaborative lists: ingredients in a recipe, a powerpoint filmstrip, spreadsheet rows and columns, etc.</p>

<p><strong>Sources:</strong> I learned the main idea of this approach from a <a href="https://news.ycombinator.com/item?id=41100477">Hacker News comment</a> by <a href="https://x.com/wcools/">Wim Cools</a> from Thymer. It is also used by <a href="https://jazz.tools/docs/react/using-covalues/colists">Jazz’s CoLists</a>. I do not know of an existing public description of the approach - in particular, I have not found it in any paper on <a href="https://crdt.tech/papers.html">crdt.tech</a> - but given its simplicity, others have likely used the approach as well.
The extension to decentralized collaboration is based on <a href="https://arxiv.org/abs/1805.04263">OpSets: Sequential Specifications for Replicated Datatypes</a> by Martin Kleppmann, Victor B. F. Gomes, Dominic P. Mulligan, and Alastair R. Beresford (2018).</p>

<ul id="toc"></ul>
<!-- Filled in by script at end. -->

<h2 id="core-problem">Core Problem</h2>

<p><em>Recap of <a href="https://mattweidner.com/2024/06/04/server-architectures.html#challenge-text-and-lists">Architectures for Central Server Collaboration - Challenge: Text and Lists</a></em></p>

<p>Let’s start by focusing on one part of the collaborative text editing problem: submitting operations to the server. When a user types a word on their client device, we need to communicate this operation to the server, so that the server can update its own (authoritative) state.</p>

<p>It’s tempting to model the text as an array of characters and send operations to the server that operate on the array representation, like “insert ‘ the’ at index 17”. However, this fails when there are multiple concurrent editors, as shown in Figure 1:</p>

<p><a id="figure-1"></a>
<img src="https://mattweidner.com/assets/img/server-architectures/index_rebasing.png" alt="See caption"></p>
<p><i><b>Figure 1.</b> Bob submits the operation "insert ‘ the’ at index 17" to the central server. But before his edit arrives, the server applies Alice's concurrent operation "insert ‘ gray’ at index 3". So it no longer makes sense to apply Bob's operation at index 17; the server must "rebase" it to index 22.</i></p>

<p>The core problem we must solve is: <strong>What operations should clients send to the server, and how should the server interpret them, so that the server updates its own text in the “obvious” correct way?</strong></p>

<blockquote>
  <p>This “index rebasing” challenge is best known for real-time collaborative apps like Google Docs, but technically, it can also affect non-real-time apps—e.g., a web form that inserts items into a list. The problem can even appear in single-threaded local apps, which need to transform text/list indices for features like inline comments and edit histories.</p>
</blockquote>

<h2 id="issues-with-existing-solutions">Issues with Existing Solutions</h2>

<p>Existing solutions to this core problem fall into two camps, Conflict-free Replicated Data Types (CRDTs) and Operational Transformation (OT). Roughly:</p>

<ul>
  <li><a href="https://crdt.tech/">CRDTs</a> (2005+) assign an immutable ID (<a href="https://mattweidner.com/2022/02/10/collaborative-data-design.html#position-def">“position”</a>) to each character and sort these IDs using a mathematical total order - often a tree traversal over a special kind of tree.</li>
  <li><a href="https://en.wikipedia.org/wiki/Operational_transformation">OT</a> (1989+) directly “transforms” operations to account for concurrent edits. In the above example, the server’s OT subroutine would transform “insert ‘ the’ at index 17” against “insert ‘ gray’ at index 3”, yielding “insert ‘ the’ at index 22”.</li>
</ul>

<p>Both CRDT and OT algorithms are used in practice. OT is used by Google Docs; the <a href="https://docs.yjs.dev/">Yjs</a> CRDT library is used by numerous apps. I’ve personally spent several years thinking and writing about text-editing <a href="https://mattweidner.com/2022/02/10/collaborative-data-design.html#list-crdt">CRDTs</a> <a href="https://mattweidner.com/2022/10/21/basic-list-crdt.html">and</a> <a href="https://mattweidner.com/2023/04/13/position-strings.html">closely</a> <a href="https://mattweidner.com/2024/04/29/list-positions.html">related</a> algorithms. So why does this blog post introduce a different approach, and what makes it better?</p>

<p>The main issue with both CRDTs and OT is their <em>conceptual complexity</em>. Text-editing CRDTs’ total orders are subtle algorithms defined in academic papers, often challenging to read. OT algorithms must satisfy algebraic “transformation properties” that have quadratically many cases and are <a href="https://hal.inria.fr/inria-00071213/">frequently flawed</a> without formal verification.</p>

<p>Complicated algorithms lead to even more complicated implementations, and CRDT/OT implementations are notoriously difficult. Thus you often use them through libraries, implemented by experts, which you can’t customize without a similar level of expertise. Because collaboration is a full-stack concern, the libraries are also full-stack, taking over core parts of your application: they present as a networked black box that inputs operations and outputs state.</p>

<p>This monolithic, unmodifiable approach becomes a liability when your app requires features that the library author did not anticipate. For example, I would struggle to do any of the following with an existing text-editing CRDT or OT library:</p>

<ol>
  <li>Divide the state between disk and memory, only loading the necessary parts of a large document.</li>
  <li>Enforce sub-document permissions on the server, e.g., some users are only allowed to edit certain paragraphs or use specific formatting.</li>
  <li>Allow “suggested changes” in the style of Google Docs, either inline or next to the text. (I <a href="https://github.com/mweidner037/list-positions-demos/tree/master/suggested-changes">tried to implement this</a> with my own CRDT library, but got stuck because suggestions could migrate away from their target text - precisely because I had no control over the CRDT’s total order.)</li>
  <li>Store the text in a key-value representation that is easy to sync over an existing key-value store (e.g. Replicache), without merely storing the entire text as a blob or the entire operation history as an array.</li>
  <li>Support operations beyond insert/delete: moving text, document tree manipulations, paragraph splitting/merging, etc.</li>
</ol>

<p>In contrast, the approach described in this blog post is dead simple. I hope that you will soon understand it well enough to do-it-yourself if you desire. Once that’s done, you can modify the internals and add features at will.</p>

<h2 id="the-approach">The Approach</h2>

<h2 id="main-idea">Main Idea</h2>

<p>The main idea of our approach is straightforward:</p>

<ul>
  <li>Label each text character with a globally unique ID (e.g., a UUID), so that we can refer to it in a consistent way across time - instead of using an array index that changes constantly. That is, our core data structure has type <code>Array&lt;{ id: ID; char: string }&gt;</code>.</li>
  <li>Clients send the server “insert after” operations that reference an existing ID. E.g., in <a href="#figure-1">Figure 1</a> above, Bob’s operation would be “insert ‘ the’ after <code>f1bdb70a</code>”, where <code>f1bdb70a</code> is the ID of ‘n’ in ‘on’.</li>
  <li>The server interprets “insert after” operations literally: it looks up the target ID and inserts the new characters immediately after it.</li>
</ul>

<p>Verify for yourself that this approach would handle both of Figure 1’s edits in the “obvious” correct way.</p>

<h2 id="some-corrections">Some Corrections</h2>

<p>You may have noticed two small issues with the above description. Luckily, they are both easy to correct.</p>

<p>First, when Bob sends his operation to the server, he also needs to specify the new elements’ IDs: “insert ‘ the’ after <code>f1bdb70a</code> with ids […]”. In other words, he must tell the server what pairs <code>{ id, char }</code> to add to its internal list, instead of just the new characters. (Having Bob generate the IDs, instead of waiting for the server to assign them, lets him reference those IDs in subsequent “insert after” operations before receiving a response to his first operation.)</p>

<p>Second, it’s possible for Bob to send an operation like “insert ‘foo’ after <code>26085702</code>”, but before his operation reaches the server, another user concurrently deletes the character with ID <code>26085702</code>. If the server literally deletes its pair <code>{ id: "26085702", char: "x" }</code> in response to the concurrent operation, then it won’t know where to insert Bob’s text anymore. The server can work around this by storing IDs in its internal list even after the corresponding characters are deleted: the server’s state becomes a list</p>

<div><pre><code><span>Array</span><span>&lt;</span><span>{</span> <span>id</span><span>:</span> <span>ID</span><span>;</span> <span>char</span><span>?:</span> <span>string</span><span>;</span> <span>isDeleted</span><span>:</span> <span>boolean</span> <span>}</span><span>&gt;</span><span>.</span>
</code></pre></div>

<p>Of course, deleted entries don’t show up in the user-visible text.</p>

<p>In summary, our corrected approach for client -&gt; server communication is as follows:</p>

<div><pre><code>Client &amp; server text state: Each stores a list of characters labeled by IDs, including deleted IDs.
- Type: `Array&lt;{ id: ID; char?: string; isDeleted: boolean }&gt;`
- Corresponding text: `list.filter(elt =&gt; !elt.isDeleted).map(elt =&gt; elt.char).join('')`

Typing a character `char`:
1. Client looks up the ID of the character just before the insertion point, `before`.
2. Client generates a globally unique ID for the new character (e.g., a UUID): `id`.
3. Client sends the server the operation: "insert ${char} after ${before} with id ${id}".
4. Server applies this operation to its own state literally:
  a. Looks up `before` in its own list, including deleted entries.
  b. Inserts `{ id, char, isDeleted: false }` immediately after `before`'s entry in the list.

Deleting a character:
1. Client looks up the ID of the deleted character, `id`.
2. Client sends the server the operation: "delete the entry with id ${id}".
3. Server looks up the entry for `id` and sets `entry.isDeleted = true` if not already.
</code></pre></div>

<p>You probably still have practical concerns with the above approach: e.g., it’s inefficient to store a UUID for every character. I’ll discuss optimizations <a href="#helper-library-articulated">later</a>.</p>

<h2 id="client-side">Client Side</h2>

<p><em>Recap of <a href="https://mattweidner.com/2024/06/04/server-architectures.html#1-server-reconciliation">Architectures for Central Server Collaboration - Server Reconciliation</a></em></p>

<p>The approach described so far lets us send operations from clients to the server, updating the server’s state. We managed to solve the <a href="#core-problem">core problem</a> in a straightforward way, without combing through any CRDT or OT papers.</p>

<p>For true Google-Docs style collaborative text editing, we also want to let users see the effects of their own operations immediately, without waiting for a response from the server. That is, clients should be allowed to perform optimistic local updates.</p>

<p>Optimistic local updates cause trouble when:</p>

<ul>
  <li>A client possesses pending local operations - operations that it performed locally but were not yet acknowledged by the server.</li>
  <li>Before receiving an acknowledgment for those operations, the client receives a new remote operation from the server, necessarily concurrent to its pending local operations.</li>
</ul>

<p><a id="figure-2"></a>
<img src="https://mattweidner.com/assets/img/text-without-crdts/text_pending_local_operation.png" alt="See caption"></p>
<p><i><b>Figure 2.</b> Bob submits the operation "insert ‘ the’ after &lt;n's ID&gt;" to the central server. But before the server acknowledges his operation, he receives the remote operation "insert ‘ gray’ after &lt;e's ID&gt;". What state should Bob's client display, incorporating both the remote operation and Bob's pending local operation?</i></p>

<p>At this point, you might say: We have concurrent operations, received in different orders by the client and the server, who must ultimately end up in the same state. Doesn’t that mean we need a CRDT?</p>

<p>Luckily, the answer is no! There is a fully general solution to optimistic local updates, <a href="https://mattweidner.com/2024/06/04/server-architectures.html#1-server-reconciliation">server reconciliation</a>, which is in particular compatible with our “insert after” operations. Briefly, the way you update a client in response to a new remote operation <code>R</code> is:</p>

<ol>
  <li>Undo all pending local operations. This rewinds the state to the client’s previous view of the server’s state.</li>
  <li>Apply the remote operation(s). This brings the client up-to-date with the server’s state.</li>
  <li>Redo any pending local operations that are still pending, i.e., they were not acknowledged as part of the remote batch.</li>
</ol>

<p><img src="https://mattweidner.com/assets/img/server-architectures/server_reconciliation.png" alt="Starting in optimistic local state S+L1+L2+L3, Step 1 leads to state S, Step 2 leads to state S+R, and Step 3 leads to state S+R+L1+L2+L3."></p>

<p><i>The Server Reconciliation way to process a remote operation <code>R</code> in the presence of pending local operations <code>L1, L2, L3</code>.</i></p>

<!-- Step 1 could involve literal undo commands, using a local stack that records how to undo each pending local mutation. Or, you could store the app state as a persistent data structure and directly restore the state before the first pending local mutation. Or, the server could tell you the exact new state to use (= its own latest state), which you use directly as the result of step 2.

Either way, server reconciliation completes our straightforward approach to collaborative text editing. -->

<blockquote>
  <p>There is another strategy that is even simpler than server reconciliation: forbid clients from processing remote operations whenever they possess pending local operations. I learned of this strategy from <a href="https://docs.powersync.com/architecture/consistency">PowerSync</a>.</p>

  <p>For example, in <a href="#figure-2">Figure 2</a>, Bob’s client would ignore the first message from the server, instead waiting for the server to process Bob’s message and send the resulting state. Once it receives that state, Bob’s client can directly overwrite its own state. Unless Bob has performed even more operations in the meantime - then his client needs to ignore the server’s second message and wait for a third, etc.</p>

  <p>Note that this strategy can led to unbounded delays if Bob types continuously or has high network latency, so it is not as “real-time” as server reconciliation.</p>
</blockquote>

<h2 id="difference-from-crdts">Difference from CRDTs</h2>

<p>You may object that the above approach sounds a lot like a CRDT. It does share some features: in particular, its assignment of an ID to each character and its use of <code>isDeleted</code> markers (tombstones).</p>

<p>The difference is that our approach handles order in a straightforward and flexible way: clients tell the server to insert X after Y and it does exactly that, or whatever else you program it to do. This contrasts with text-editing CRDTs, in which IDs are ordered for you by a fancy algorithm. That ordering algorithm is what differs between the numerous text-editing CRDTs, and it’s the complicated part of any CRDT paper; we get to avoid it entirely.</p>

<h2 id="discussion">Discussion</h2>

<p>The previous section described the whole approach in what I hope is enough detail to start implementing it yourself (though first check out <a href="#helper-library-articulated">Articulated</a> below). Let’s now discuss consequences and extensions of the approach.</p>

<h2 id="concurrent-insertions">Concurrent Insertions</h2>

<p>With any collaborative text-editing algorithm, the most interesting theoretical question is: What happens when multiple users type in the same place concurrently?</p>

<p>For example, staring from the text “My name is”, suppose Charlie types “ Charlie”, while concurrently, Dave types “ Dave”. If Charlie’s operation reaches the server first, what is the final text?</p>

<p>Let’s check:</p>

<ul>
  <li>Charlie’s operation says “insert ‘ Charlie’ after &lt;ID of ‘s’ in ‘is’&gt;”. The server processes this literally, giving the text “My name is Charlie”.</li>
  <li>Dave’s operation likewise says “insert ‘ Dave’ after &lt;ID of ‘s’ in ‘is’&gt;”. The server again processes this literally - inserting after the ‘s’ in ‘is’, irrespective of the concurrent text appearing afterwards - giving the text “My name is Dave Charlie”.</li>
</ul>

<p>In summary, concurrent insertions at the same place end up in the <em>reverse</em> of the order that the server received their operations. More generally, even without concurrency, insert-after operations with the same target ID end up in reverse order. For example, if Dave typed his name backwards as (e, left arrow, v, left arrow, a, left arrow, D), then each operation would be “insert after &lt;ID of ‘s’ in ‘is’&gt;”, and the resulting text would be the reverse of the server’s receipt order: Dave. (This might remind you of a popular CRDT. I’ll talk about that <a href="#decentralized-variants">later</a>.)</p>

<p>Observe that the concurrently-inserted words “ Charlie” and “ Dave” ended up one after the other, instead of becoming interleaved character-by-character (unlike <a href="https://doi.org/10.1145/3301419.3323972">some text-editing CRDTs</a>). That would work even if Charlie and Dave sent each character as a separate operation. Indeed, Dave inserts the ‘a’ in Dave after the ‘D’ (i.e., the insert-after operation references D’s ID), ‘v’ after ‘a’, etc.; so when the server processes these individual operations, it updates its state as</p>

<div><pre><code>"My name is D Charlie" -&gt; "My name is Da Charlie"
-&gt; "My name is Dav Charlie" -&gt; "My name is Dave Charlie"
</code></pre></div>

<p>in spite of the unexpected “ Charlie” afterwards.</p>

<p>The same cannot be said for backwards (right-to-left) insertions: if Dave and Charlie both typed their names with copious use of left arrow, and the server received those operations in an interleaved order, then the resulting text would also be interleaved. In practice, this could only happen if Charlie and Dave were both online simultaneously (so that their messages could be interleaved) but stubbornly ignored each other’s in-progress edits.</p>

<h2 id="flexible-operations">Flexible Operations</h2>

<p>So far, the only text-editing operations I’ve described are “insert after” and “delete”, which the server applies literally. However, the approach supports many more possible operations. In fact, thanks to <a href="#client-side">our use of server reconciliation</a>, the server has the flexibility to do essentially anything in response to a client operation - clients will eventually end up in the same state regardless. This contrasts with CRDT and OT algorithms, which only allow operations that satisfy strict algebraic rules.</p>

<p>For example, consider the concurrent insertion example from the previous section. The final result, “My name is Dave Charlie”, isn’t very reasonable, even though it satisfies a <a href="https://www.cs.ox.ac.uk/people/hongseok.yang/paper/podc16-full.pdf">mathematical specification</a> for collaborative text-editing. A fancy server could do something more intelligent for insertions like Dave’s that are at the same place as a previously-received concurrent insertion. For example:</p>

<ol>
  <li>Ignore any such operation (treat it as a no-op).</li>
  <li>Add the IDs to the internal list, but mark them as deleted immediately. (This is a still no-op from the users’ perspective, but it allows the server to process future operations from Dave that reference his earlier IDs.)</li>
  <li>Insert the text, but apply special formatting to both words to flag them for review.</li>
  <li>Convert Dave’s edits to a “suggestion” displayed alongside the main text.</li>
  <li>Ask an LLM how to best fix the text. (Be warned that Dave’s client may have trouble rebasing any further optimistic operations on top of the resulting state.)</li>
</ol>

<p>Clients can also send operations with fancier semantics than “insert after” to better capture user intent - thus increasing the odds that the server’s eventual state is reasonable in spite of concurrent edits. A simple example is “insert before”, the reversed version of “insert after”. E.g., if a user creates a heading above a paragraph, their client could “insert before” the paragraph’s first character, to prevent the header from ending up in the middle of an unrelated addition to the previous paragraph.</p>

<p>Another example is a “fix typo” operation that adds a character to a word only if that word still exists and hasn’t already been fixed. E.g., the client tells the server: “insert ‘u’ after the ‘o’ in ‘color’ that has ID X, but only if the surrounding word is still ‘color’”. That way, if another user deletes ‘color’ before the fix-typo operation reaches the server, you don’t end up with a ‘u’ in the middle of nowhere. (This example avoids an issue brought up by <a href="https://www.moment.dev/blog/lies-i-was-told-pt-1">Alex Clemmer</a>).</p>

<p>You can even define operations whose insertion positions change once they reach the server. E.g., the server could handle concurrent insertions at the same position by reordering them to be alphabetical. Or, if you add “move” operations for drag-and-drop, then the server can choose to process “insert after” operations within the moved text in the obvious way - insert them within the moved text instead of at its original location. This contrasts with text-editing CRDTs and my own CRDT-ish libraries (<a href="https://mattweidner.com/2023/04/13/position-strings.html">position-strings</a> and <a href="https://mattweidner.com/2024/04/29/list-positions.html">list-positions</a>), which fix each character’s position in a global total order as soon as the user types it.</p>

<blockquote>
  <p>Some of these flexible operations can technically be implemented on top of a CRDT, by having the server initiate its own operations after a client operation (e.g., apply “delete” operations to some text that it wants to ignore). However, I don’t know of CRDT implementations that support “insert before” operations, un-deleting IDs, or changing where an ID ends up in the list.</p>
</blockquote>

<h2 id="formatting-rich-text">Formatting (Rich Text)</h2>

<p>Rich text enhances plain text with inline formatting (bold, font size, hyperlinks, …), among other features. To handle rich text in our approach, when a user formats a range of text, we of course want to translate the ends of that range into character IDs instead of indices: “apply bold formatting from ID X to ID Y”. (Or perhaps: “apply bold formatting from ID X inclusive to ID Y exclusive”, so that concurrent insertions at the end of the range are also bolded.)</p>

<p>When used alongside a rich-text editor such as ProseMirror, the server can apply such operations literally: look up the current array indices of X and Y, and tell the local ProseMirror state to bold that range of text. ProseMirror will take care of remembering the bold span so that, when the server later receives an insertion within the span, it knows to bold that text too. (Unless the server decides to otherwise, e.g., in response to an operation “insert ‘…’ after ID Z with bold set to false”.)</p>

<p>I believe this simple extension to our approach takes care of the tricky conflict-resolution parts of collaborative rich-text formatting. However, I still recommend reading the <a href="https://www.inkandswitch.com/peritext/">Peritext essay</a> for insight into the semantics of collaborative rich-text - what operations clients should send to the server, and how the server should process them.</p>

<h2 id="decentralized-variants">Decentralized Variants</h2>

<p><em>More info in <a href="https://mattweidner.com/2024/06/04/server-architectures.html#appendix-from-centralized-to-decentralized">Architectures for Central Server Collaboration - Appendix: From Centralized to Decentralized</a></em></p>

<p>I’ve so far assumed that your app has a central server, which assigns a total order to operations (namely, the order that the server receives them) and updates its authoritative state in response to those operations.</p>

<p>If you don’t have a central server or your app is server-optional, you can instead assign an eventual total order to operations in a <em>decentralized</em> way. For example, order operations using <a href="https://mattweidner.com/2023/09/26/crdt-survey-3.html#lww-lamport-timestamps">Lamport timestamps</a>. Then treat “the result of processing the operations I’ve received so far in order” as the authoritative state on each client. Our approach’s per-character IDs and “insert after” operations work equally well with this decentralized, “non”-server reconciliation.</p>

<p>Technically, the resulting algorithm is a text-editing CRDT: it’s a decentralized, eventually consistent algorithm for collaborative text editing. I hope that it is easier to understand and implement than a typical text-editing CRDT - it involved no trees or mathematical proofs - and the above remarks on <a href="#flexible-operations">Flexible Operations</a> and <a href="#formatting-rich-text">Formatting</a> still hold in the decentralized setting.</p>

<p>Nonetheless, you might ask: if “non”-server reconciliation plus our “insert after” operations yields a text-editing CRDT, which CRDT is it? The answer is:</p>

<ul>
  <li>If you order operations using Lamport timestamps, the resulting list order is equivalent to RGA / Causal Trees. (RGA’s sibling sort - reverse Lamport timestamp order - corresponds exactly to the reverse-order behavior I described <a href="#concurrent-insertions">earlier</a>.)</li>
  <li>If you order operations using Lamport timestamps and add formatting operations like above, the resulting behavior is quite similar to <a href="https://www.inkandswitch.com/peritext/">Peritext</a>. (The Lamport timestamp order on formatting operations corresponds to Peritext’s Lamport-timestamp-ordered stack of formatting spans.)</li>
  <li>If you order operations using a topological sort - e.g., append them to an RGA list CRDT and use its list order - the resulting list order is equivalent to <a href="https://mattweidner.com/2022/10/21/basic-list-crdt.html">Fugue</a>. (The topological sort’s non-interleaving property corresponds to Fugue’s non-interleaving of backwards insertions.)</li>
</ul>

<blockquote>
  <p>I have not written out a proof of these claims in detail, but I’m happy to discuss my reasoning if you <a href="https://mattweidner.com/">contact me</a>.</p>
</blockquote>

<h2 id="helper-library-articulated">Helper Library: Articulated</h2>

<p>Recall that each device’s state in our approach is a list</p>

<div><pre><code><span>Array</span><span>&lt;</span><span>{</span> <span>id</span><span>:</span> <span>ID</span><span>;</span> <span>char</span><span>?:</span> <span>string</span><span>;</span> <span>isDeleted</span><span>:</span> <span>boolean</span> <span>}</span><span>&gt;</span><span>;</span>
</code></pre></div>

<p>In practice, you often want to store the actual text elsewhere - e.g., as a ProseMirror state - so our approach really just needs a list</p>

<div><pre><code><span>Array</span><span>&lt;</span><span>{</span> <span>id</span><span>:</span> <span>ID</span><span>;</span> <span>isDeleted</span><span>:</span> <span>boolean</span> <span>}</span><span>&gt;</span><span>;</span>
</code></pre></div>

<p>There are a few main tasks that you’ll perform on this list:</p>

<ol>
  <li>Convert between IDs and their current array indices, so that you can talk to the text-editing UI (e.g. ProseMirror).</li>
  <li>Insert a new ID after a specified ID.</li>
  <li>Mark an ID as deleted.</li>
  <li>Convert the state to and from a serialized form for storage.</li>
</ol>

<p>A literal array is not great at any of these tasks. Tasks 1-3 take linear time, and the array’s memory and storage space are large - an entire object and UUID per character!</p>

<p><a href="https://github.com/mweidner037/articulated/">Articulated</a> is a small npm library I made to help out. Its <code>IdList</code> data structure provides the same functionality as the above array, but with optimizations similar to those in popular text-editing CRDT libraries:</p>

<ul>
  <li>IDs have the form <code>{ bunchId, counter }</code>, where <code>bunchId</code> is a UUID that can be shared between a “bunch” of IDs with varying <code>counter</code>. When IDs in a bunch appear alongside each other - e.g., in the common case of left-to-right insertions - <code>IdList</code> stores them as a single object in memory and in the serialized state.</li>
  <li>The core data structure is a B+Tree instead of an array, allowing <code>log</code> or <code>log^2</code> time method calls.</li>
</ul>

<p>As an added feature, IdList is a <a href="https://en.wikipedia.org/wiki/Persistent_data_structure">persistent</a> data structure. This is great for server reconciliation: each client can cheaply store a copy of the latest state they received from the server alongside their optimistic state, making it trivial to rollback to the server’s last state when they receive a remote operation.</p>

<p>You can check out the <a href="https://github.com/mweidner037/articulated/#articulated">docs</a> and (very preliminary) <a href="https://github.com/mweidner037/articulated-demos">demos</a> to learn more. Or, read through the code for <a href="https://github.com/mweidner037/articulated/blob/master/test/id_list_simple.ts">IdListSimple</a> - it’s a simple, &lt; 300 SLOC implementation of IdList that omits its optimizations and persistence but is otherwise functionally identical (verified by fuzz tests).</p>

<p>I hope that, within the context of a server reconciliation architecture, Articulated can serve a similar purpose to an optimized CRDT library, but with the flexibility and other advantages described in this blog post.</p>




        
    <!-- Footer -->
    <hr>
    <p>
    <a href="https://mattweidner.com/">Home</a>
    • Matthew Weidner
    • Common Curriculum / CMU PhD student
    • mweidner037 [at] gmail.com
    • <a href="https://twitter.com/MatthewWeidner3">@MatthewWeidner3</a>
    • <a href="https://bsky.app/profile/mweidner.bsky.social">@mweidner.bsky.social</a>
    • <a href="https://www.linkedin.com/in/matthew-weidner-99715412a">LinkedIn</a>
    • <a href="https://github.com/mweidner037/">GitHub</a>
    </p>
  </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[OpenAI to buy AI startup from Jony Ive (752 pts)]]></title>
            <link>https://www.bloomberg.com/news/articles/2025-05-21/openai-to-buy-apple-veteran-jony-ive-s-ai-device-startup-in-6-5-billion-deal</link>
            <guid>44053518</guid>
            <pubDate>Wed, 21 May 2025 17:01:35 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.bloomberg.com/news/articles/2025-05-21/openai-to-buy-apple-veteran-jony-ive-s-ai-device-startup-in-6-5-billion-deal">https://www.bloomberg.com/news/articles/2025-05-21/openai-to-buy-apple-veteran-jony-ive-s-ai-device-startup-in-6-5-billion-deal</a>, See on <a href="https://news.ycombinator.com/item?id=44053518">Hacker News</a></p>
Couldn't get https://www.bloomberg.com/news/articles/2025-05-21/openai-to-buy-apple-veteran-jony-ive-s-ai-device-startup-in-6-5-billion-deal: Error: Request failed with status code 403]]></description>
        </item>
        <item>
            <title><![CDATA[By Default, Signal Doesn't Recall (507 pts)]]></title>
            <link>https://signal.org/blog/signal-doesnt-recall/</link>
            <guid>44053364</guid>
            <pubDate>Wed, 21 May 2025 16:46:53 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://signal.org/blog/signal-doesnt-recall/">https://signal.org/blog/signal-doesnt-recall/</a>, See on <a href="https://news.ycombinator.com/item?id=44053364">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p><img src="https://signal.org/blog/images/recall-header.png" alt="A screenshot of a Microsoft Windows desktop. Microsoft Paint and Minesweeper are visible behind a black rectangular window that is empty except for graffiti-style text that says &quot;SIGNAL WAS HERE&quot;."></p><p>Signal Desktop now includes support for a new “Screen security” setting that is designed to help prevent your own computer from capturing screenshots of your Signal chats on Windows. This setting is automatically enabled by default in Signal Desktop on Windows 11.</p><p>If you’re wondering why we’re only implementing this on Windows right now, it’s because the purpose of this setting is to protect your Signal messages from <a href="https://support.microsoft.com/en-us/windows/retrace-your-steps-with-recall-aa03f8a0-a78b-4b3e-b0a1-2eb8ac48701c">Microsoft Recall</a>.</p><p>First announced on May 20, 2024, Microsoft Recall takes screenshots of your apps every few seconds as you use your computer and then stores them in an easily searchable database. In Microsoft’s <a href="https://blogs.windows.com/windowsexperience/2024/06/07/update-on-the-recall-preview-feature-for-copilot-pcs/">own words</a>, its goal is to act as a sort of “photographic memory” for everything that you do on your computer. The words that other people chose to describe Recall upon its debut were decidedly less positive.<sup id="fnref:1"><a href="#fn:1" rel="footnote" role="doc-noteref">1</a></sup> After an intense <a href="https://www.wired.com/story/microsoft-recall-off-default-security-concerns/">security backlash</a> and significant public outcry, Microsoft quickly pulled the feature.</p><p>It’s a one-year anniversary that nobody wants to celebrate, but Recall <a href="https://arstechnica.com/security/2025/04/microsoft-is-putting-privacy-endangering-recall-back-into-windows-11/">is back</a> and Signal is ready.</p><p>Although Microsoft made several adjustments over the past twelve months in response to critical feedback, the revamped version of Recall still places any content that’s displayed within privacy-preserving apps like Signal at risk. As a result, we are enabling an extra layer of protection by default on Windows 11 in order to help maintain the security of Signal Desktop on that platform even though it introduces some usability trade-offs. Microsoft has simply given us no other option.</p><h2 id="fade-to-black">Fade to Black</h2><p>If you attempt to take a screenshot of Signal Desktop when screen security is enabled, nothing will appear. This limitation can be frustrating, but it might look familiar to you if you’ve ever had the audacity to try and take a screenshot of a movie or TV show on Windows. According to Microsoft’s <a href="https://learn.microsoft.com/en-us/windows/client-management/manage-recall#information-for-developers">official developer documentation</a>, setting the correct <a href="https://en.wikipedia.org/wiki/Digital_rights_management">Digital Rights Management</a> (DRM) flag on the application window will ensure that “content won’t show up in Recall or any other screenshot application.” So that’s exactly what Signal Desktop is now doing on Windows 11 by default.</p><p><img src="https://signal.org/blog/images/recall-drm-screenplay.jpg" alt="A stylized close-up crop of a movie screenplay that says &quot;INT. COPILOT+ PC MANUFACTURING FACILITY - NIGHT - METALLIC SHELVES in endless rows stretch into the darkness. Two figures crouch in the shadows. ALICE: DRM technology has been consistently used against us. BOB: It won't be the first time we've turned the tables. ALICE: My life has always felt like a movie.&quot;"></p><p>Apps like Signal have essentially no control over what content Recall is able to capture, and implementing “DRM” that works for you (not against you) is the best choice that we had. It’s like a scene in a movie where the villain has switched sides, and you can’t screenshot this one by default either.</p><h2 id="warning-shots">Warning Shots</h2><p>Microsoft has launched Recall without granular settings for app developers that would enable Signal to easily protect privacy, which is a glaring omission that limits our choices. Signal is using the tools that are available to us even though we recognize that there are many legitimate use cases where someone might need to take a screenshot. For example, some accessibility software (such as screen readers or magnification tools for people who are visually impaired) may not function correctly otherwise.</p><p>To help mitigate this issue, we made the setting easy to disable <em>(Signal Settings → Privacy → Screen security)</em>, but it’s difficult to accidentally disable. Turning off “Screen security” in Signal Desktop on Windows 11 will always display a warning and require confirmation in order to continue.</p><p><img src="https://signal.org/blog/images/recall-warning.png" alt="A screenshot of a warning dialog box that says &quot;Disable screen security? If disabled, this may allow Microsoft Windows to capture screenshots of Signal and use them for features that may not be private.&quot;"></p><p>This setting is local to your computer and doesn’t apply to screenshots on other devices. If you are communicating with someone who uses a screen reader on macOS or Linux, for example, keeping screen security enabled on your side won’t prevent them from taking screenshots or adversely affect any accessibility software they may be using.</p><p>We hope that the AI teams building systems like Recall will think through these implications more carefully in the future. Apps like Signal shouldn’t have to implement “one weird trick” in order to maintain the privacy and integrity of their services without proper developer tools. People who care about privacy shouldn’t be forced to sacrifice accessibility upon the altar of AI aspirations either.</p><h2 id="future-recallections">Future Recallections</h2><p>“Take a screenshot every few seconds” legitimately sounds like a suggestion from a low-parameter LLM that was given a prompt like “How do I add an arbitrary AI feature to my operating system as quickly as possible in order to make investors happy?” — but more sophisticated threats are on the horizon.</p><p>The integration of AI agents with pervasive permissions, questionable security hygiene, and an insatiable hunger for data has the potential to break <a href="https://techcrunch.com/2025/03/07/signal-president-meredith-whittaker-calls-out-agentic-ai-as-having-profound-security-and-privacy-issues/">the blood-brain barrier</a> between applications and operating systems. This poses a significant threat to Signal, and to every privacy-preserving application in general.</p><p>People everywhere rely on Signal to protect their communication, including human rights workers, governments, board rooms, militaries, and millions of individuals around the world for whom privacy is an existential matter. Apps like Signal must maintain their ability to prioritize security by default in a way that can be <a href="https://github.com/signalapp">publicly validated</a>. It’s imperative that privacy-preserving apps retain the ability to uphold these promises on every platform, including Microsoft Windows.</p><p>In order to do this, the ecosystem needs to do its part too. Operating system vendors, especially those who are shipping AI agents, need to ensure that the developers of apps like Signal always have the necessary tools and options at their disposal to reject granting OS-level AI systems access to any sensitive information within their apps.<sup id="fnref:2"><a href="#fn:2" rel="footnote" role="doc-noteref">2</a></sup></p><p><a href="https://www.cbsnews.com/news/ceo-zuckerberg-facebooks-5-core-values/">“Move fast and break things”</a> is going to be a tough habit for the tech industry to, well, break. But <a href="https://en.wikipedia.org/wiki/Minimum_viable_product">MVP</a> shouldn’t also stand for “Minimum Viable Precautions.” It’s ultimately up to companies like Microsoft to ensure that their platforms remain a suitable foundation for privacy-preserving applications like Signal. If that ever stops being the case, we’ll have to stop supporting those platforms.</p><p>Messaging apps are a window into your entire life. They’re where we share our favorite memories, fall in love, complain, smile, cry, and express who we really are. Given this reality, private messaging apps like Signal deserve to be treated with at least the same level of caution that’s afforded to a web browser’s private or incognito browsing window — which Microsoft has <a href="https://support.microsoft.com/en-us/windows/filtering-apps-websites-and-sensitive-information-in-recall-a4c28bee-e200-4a4a-b60d-c0522b404a5b">already excluded from Recall by default</a>.</p><p>Screen security for Signal Desktop on Microsoft Windows is rolling out now, and enabled by default on Windows 11. We’d like to express our sincere appreciation to the Signal community for helping us test this release during the beta period. We couldn’t do this work without your support.</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[The Era of the Business Idiot (103 pts)]]></title>
            <link>https://www.wheresyoured.at/the-era-of-the-business-idiot/</link>
            <guid>44053328</guid>
            <pubDate>Wed, 21 May 2025 16:43:19 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.wheresyoured.at/the-era-of-the-business-idiot/">https://www.wheresyoured.at/the-era-of-the-business-idiot/</a>, See on <a href="https://news.ycombinator.com/item?id=44053328">Hacker News</a></p>
<div id="readability-page-1" class="page"><article>
      <p><strong><em>Fair warning: this is the longest thing I've written on this newsletter. I do apologize.</em></strong></p><p><a href="https://www.youtube.com/watch?v=H-U3SxlCMGY&amp;ref=wheresyoured.at" rel="noreferrer"><strong><em>Soundtrack: EL-P - $4 Vic</em></strong></a></p><p><strong><em>Listen to my podcast </em></strong><a href="http://linktr.ee/betteroffline?ref=wheresyoured.at" rel="noreferrer"><strong><em>Better Offline</em></strong></a><strong><em>. We have </em></strong><a href="https://cottonbureau.com/people/better-offline?ref=wheresyoured.at" rel="noreferrer"><strong><em>merch</em></strong></a><strong><em>. </em></strong></p><hr><p>Last week,<u> </u><a href="https://bloom.bg/4kf9Vya?ref=wheresyoured.at"><u>Bloomberg profiled Microsoft CEO Satya Nadella</u></a>, revealing that he's either a liar or a specific kind of idiot.</p><p>The article revealed that — assume we believe him, and this wasn’t merely a thinly-veiled advert for Microsoft’s AI tech — Copilot consumes Nadella’s life outside the office as well at work. </p><blockquote>He likes podcasts, but instead of listening to them, he loads transcripts into the Copilot app on his iPhone so he can chat with the voice assistant about the content of an episode in the car on his commute to Redmond. At the office, he relies on Copilot to deliver summaries of messages he receives in Outlook and Teams and toggles among at least 10 custom agents from Copilot Studio. He views them as his AI chiefs of staff, delegating meeting prep, research and other tasks to the bots. “I’m an email typist,” Nadella jokes of his job, noting that Copilot is thankfully very good at triaging his messages.&nbsp;</blockquote><p>None of these tasks are things that require you to use AI. You can read your messages on Outlook and Teams without having them summarized — and I’d argue that a well-written email is one that doesn’t require a summary. Podcasts are not there "to be chatted about" with an AI. Preparing for meetings isn't something that requires AI, nor is research, unless, of course, <strong><em>you don't really give a shit about the actual content of what you're reading or the message of what you're saying, just that you are "saying the right thing."</em></strong></p><p>To be clear, I am deeply unconvinced that Nadella actually runs his life in this way, but if he does, Microsoft’s board should fire him immediately.</p><p>In any case, the article is rambling, cloying, and<u> </u><a href="https://www.businessinsider.com/deepmind-mustafa-suleyman-google-allegations-bullying-settlements-2021-8?r=US&amp;IR=T&amp;ref=wheresyoured.at"><u>ignores Microsoft AI CEO Mustafa Suleyman's documented history of abusing his workers</u></a>. Ten custom agents that do what? What do you mean by "other tasks"? Why are these questions never asked? Is it because the reporters know they won't get an answer? Is it because the reporters are too polite to ask more probing questions, knowing that these anecdotes are likely entirely made up as a means to promote a flagging AI ecosystem that cost billions to construct, but doesn’t really seem to do anything, and the reporter in question doesn’t want to force Satya to build a bigger house of cards than he needs to.&nbsp;</p><p>Or is it because we, as a society, do not want to look too closely at the powerful? Is it because we've handed our economy to men that<a href="https://www.thestreet.com/personal-finance/satya-nadella-net-worth?ref=wheresyoured.at"><u> get paid $79 million a year to do a job they can't seem to describe</u></a>, and even that, they would sooner offload to a bunch of unreliable AI models than actually do?</p><p>We live in the era of the symbolic executive, when "being good at stuff" matters far less than the appearance of doing stuff, where "what's useful" is dictated not by outputs or metrics that one can measure but rather the vibes passed between managers and executives that have worked their entire careers to escape the world of work. Our economy is run by people that don't participate in it and our tech companies are directed by people that don't experience the problems they allege to solve for their customers, as the modern executive is no longer a person with demands or responsibilities beyond<a href="https://www.wheresyoured.at/tss/"><u> their allegiance to shareholder value</u></a>.</p><p>I, however, believe the problem runs a little deeper than the economy, which is a symptom of a bigger, virulent, and treatment-resistant plague that has infected the minds of those currently twigging at the levers of power — and really, the only levers that actually matter.&nbsp;</p><p>The incentives behind effectively everything we do have been broken by decades of neoliberal thinking, where the idea of a company — an entity created to do a thing in exchange for money —has been drained of all meaning beyond the continued domination and extraction of everything around it, focusing heavily on short-term gains and growth at all costs. In doing so, the definition of a “good business” has changed from one that makes good products at a fair price to a sustainable and loyal market, to one that can display the most stock price growth from quarter to quarter.&nbsp;</p><p>This is the <a href="https://www.wheresyoured.at/the-rot-economy/"><u>Rot Economy</u></a>, which is a useful description for how tech companies have voluntarily degraded their core products in order to placate shareholders, transforming useful — and sometimes beloved — services into a hollow shell of their former selves as a means of expressing growth. But it’s worth noting that this transformation isn’t constrained to the tech industry, nor was it a phenomena that occurred when the tech industry entered its current VC-fuelled, publicly-traded incarnation.&nbsp;</p><p>In <a href="https://www.wheresyoured.at/tss/"><u>The Shareholder Supremacy</u></a>, I drew a line from an early 20th-century court ruling, to former General Electric CEO Jack Welch, to the current tech industry, but there’s one figure I didn’t pay as much attention to, and I regrettably now have to do so.</p><p>Famed Chicago School economist (and dweller of Hell) Milton Friedman<a href="https://www.nytimes.com/1970/09/13/archives/a-friedman-doctrine-the-social-responsibility-of-business-is-to.html?ref=wheresyoured.at"><u> once argued in his 1970 doctrine</u></a> that those who didn’t focus on shareholder value were “unwitting pup­pets of the intellectual forces that have been undermining the basis of a free society these past decades," and that any social responsibility — say, treating workers well, doing <em>anything</em> other than focus on shareholder value — is tantamount to an executive taxing his shareholders by "spending their money" on their own personal beliefs.</p><p>Friedman was a fundamentalist when it came to unrestricted, unfettered capitalism, and this zealotry surpassed any sense of basic human morality — if he had any — at times. For example, in his book, Capitalism and Friedman, he argued that companies should be allowed to discriminate on racial grounds because the owner might suffer should they be required to hire an equally or better-qualified Black person.&nbsp;</p><p>Bear in mind, this was written at the height of the civil rights movement, just six years before the assassination of Martin Luther King, and when America was rapidly waking up to the evils of racism and segregation (a process, I add, is ongoing and sadly not complete). This is a direct quote:&nbsp;</p><blockquote>“...consider a situation in which there are grocery stores serving a neighborhood inhabited by people who have a strong aversion to being waited on by Negro clerks. Suppose one of the grocery stores has a vacancy for a clerk and the first applicant qualified in other respects happens to be a Negro. Let us suppose that as a result of the law the store is required to hire him. The effect of this action will be to reduce the business done by this store and to impose losses on the owner. If the preference of the community is strong enough, it may even cause the store to close. When the owner of the store hires white clerks in preference to Negroes in the absence of the law, he may not be expressing any preference or prejudice, or taste of his own. He may simply be transmitting the tastes of the community. He is, as it were, producing the services for the consumers that the consumers are willing to pay for. Nonetheless, he is harmed, and indeed may be the only one harmed appreciably, by a law which prohibits him from engaging in this activity, that is, prohibits him from pandering to the tastes of the community for having a white rather than a Negro clerk. The consumers, whose preferences the law is intended to curb, will be affected substantially only to the extent that the number of stores is limited and hence they must pay higher prices because one store has gone out of business.”</blockquote><p>Friedman was grotesque. I am not religious, but I hope Hell exists if only for him.&nbsp;</p><p>The broader point I’m trying to make is that neoliberalism is inherently selfish, believing that the free market should reign supreme, bereft of government intervention, regulation or interference, thinking that somehow these terms will enable "freedom" rather than a kind of market-dominated quasi-dictatorship where our entire lives are dominated by the whims of the affluent, and that there is no institution that can possibly push back against them.&nbsp;</p><p>Friedman himself makes the facile argument that economic freedom — which, he says, is synonymous with unfettered capitalism — is a necessary condition of unfettered political freedom. Obviously, that’s bollocks, although it’s an argument that’s proven persuasive with a certain class of people that are either intellectually or morally hollow (or both).</p><p>Neoliberalism also represents a kind of modern-day feudalism, dividing society based on whether someone is a shareholder or not, with the former taking precedence and the latter seen as irrelevant at best, or disposable at worst. It’s curious that Friedman saw economic freedom — a state that is non-interventionist in economic matters — as essential for political freedom, while also failing to see equality as the same.&nbsp;</p><p>I realize this is all very big and clunky, but I want you to understand how these incentives have fundamentally changed everything, and why they are responsible for the rot we see in our society and our workplaces. When your <em>only</em> incentive is shareholder value, and you raise shareholder value as a platonic ideal, <em>everything else </em>is secondary, including <em>the customer you are selling something to. Friedman himself makes a moral case for discrimination, because shareholder value — in his example, the store owner — matters more than racial equality at its most basic level.&nbsp;</em></p><p>When you care only about shareholder value, the only job you have is to promote further exploitation and dominance — not to have happy customers, not to make your company "a good place to work," not to make a good product, not to make a difference or contribute to anything other than further growth.</p><p>While this is, to anyone with a vapor of an intellectual or moral dimension, absolutely fucking stupid, it’s an idea that’s proven depressingly endemic among the managerial elite, in part because it has entered the culture, and because it is hammered across in MBA classes and corporate training seminars.&nbsp;</p><p>In simpler terms, modern business theory trains executives not to be good at something, or to make a company based on their particular skills, but to "find a market opportunity" and exploit it. The Chief Executive —<a href="https://www.businessinsider.com/ceo-replace-ai-job-employees-executives-save-money-salary-2023-9?ref=wheresyoured.at"><u> who makes over 300 times more than their average worker</u></a> — is no longer a leadership position, but a kind of figurehead measured on their ability to continually grow the market capitalization of their company. It is a position inherently defined by its <em>lack</em> of labor, the amorphousness of its purpose and its lack of any clear responsibility.&nbsp;</p><p>While CEOs <em>do</em> get fired when things go badly, it's often after a prolonged period of decline and stagnancy, and almost always comes with some sort of payoff — and when I say "badly," I mean that growth has slowed to the point that even firing masses of people doesn't make things better.&nbsp;</p><blockquote><strong>Sidebar: </strong>I also note that “fired” means something different when it comes to top execs. Excluding those fired due to criminal levels of malfeasance — like Robert Moffat, the man once tipped to be the next CEO of IBM, had he not been convicted of securities fraud and jailed for six months, thus losing nearly $85m in benefits — most ousted corporate leaders often enjoy generous severance packages, far beyond the usual “two weeks of pay and COBRA.” WeWork founder Adam Neumann’s $200m in cash and $225m in (now-worthless) stock is perhaps the most egregious example of this.&nbsp;</blockquote><p>We have, as a society, reframed all business leadership — which is increasingly broad, consisting of all management from the C-suite down — to be the equivalent of a mall cop, a person that exists to make sure people are working without having any real accountability for the work themselves, or to even understand the work itself.</p><p>When the leader of a company doesn't participate in or respect the production of the goods that enriches them, it creates a culture that enables similarly vacuous leaders on all levels. Management as a concept no longer means doing "work," but establishing cultures of dominance and value extraction. A CEO isn't measured on happy customers or even how good their revenue is today, but how good revenue might be <em>tomorrow</em> and whether those customers are paying them more. A "manager," much like a CEO, is no longer a position with any real responsibility — they're there to make sure you're working, to know enough about your work that they can <em>sort of </em>tell you what to do, but somehow the job of "telling you what to do" doesn't come with it any actual work, and the instructions don’t need to be useful or even meaningful.</p><p>Decades of direct erosion of the very concept of leadership means that the people running companies have been selected not based on their actual efficacy — especially as the position became defined by its lack of actual production — but on whether they resemble what a manager or executive is meant to look like based on the work that somebody else did.</p><p>That’s how someone like David Zaslav, a lawyer by trade and arguably the worst CEO in the entertainment industry, managed to become the head of Warner Brothers (<a href="https://www.cnbc.com/video/2020/03/02/discoverys-david-zaslav-reflects-on-jack-welchs-legacy.html?ref=wheresyoured.at"><u>that, and kissing up to Jack Welch, who he called a “big brother” that “picked him up like a friend”</u></a>). It’s how Carly Fiorina — an MBA by trade — went on to become the head of HP, <a href="https://fortune.com/2015/09/21/carly-fiorina-hp-ceo-business-record/?ref=wheresyoured.at"><u>only to drive the company into a ditch where it stopped innovating</u></a>, and largely missed the biggest opportunities of the early Internet era. The three CEOs that followed (Mark Hurd (<a href="https://www.businessinsider.com/backlash-against-hewlett-packard-grows-it-seems-mark-hurd-fired-because-company-scared-of-bad-pr-over-bogus-sexual-harassment-allegation-2010-8?ref=wheresyoured.at"><u>who was ousted after fudging expense reports to send money to a love interest</u></a> <em>and </em><a href="https://www.businessinsider.com/wait-a-minute-why-does-mark-hurd-get-50-million-severance-when-he-lied-in-his-expense-reports-2010-8?ref=wheresyoured.at"><em><u>still got tens of millions of dollars in severance</u></em></a><em>)</em>, Leo Apotheker (<a href="https://www.nytimes.com/2015/10/09/business/leo-apotheker-may-have-been-worse-hp-chief-than-carly-fiorina.html?ref=wheresyoured.at"><u>who the New York Times suggests may have been worse than Fiorina</u></a>), and Meg Whitman (<a href="https://www.businessinsider.com/the-top-7-ways-meg-whitman-is-failing-at-hp-2012-5?ref=wheresyoured.at"><u>famous for being a terrible CEO at HP</u></a> and <a href="https://www.theguardian.com/tv-and-radio/2020/jun/28/quibi-netflix-jeffrey-katzenberg-crash?ref=wheresyoured.at"><u>co-founding doomed video startup Quibi</u></a>) similarly came from a non-tech background, and similarly did a shitty job, in part because they didn’t understand the company or the products or the customers.&nbsp;</p><p>Management has, over the course of the past few decades, eroded the very fabric of corporate America, and I'd argue it’s done the same in multiple other western economies, too.</p><p>I’d also argue that this kind of dumb management thinking also infected the highest echelons of politics across the world, and especially in the UK, my country of birth and where I lived until 2011, delivering the same kind of disastrous effects but at a macro level, as they impacted not a single corporate entity but the very institutions of the state. I’m not naive. I don’t think that the average politician is a salt-of-the-earth type, someone who did a normal job and then decided to enter politics. Especially not in the UK, where the trappings of class permeate everything, and we’re yet to shake off the noxious influence of the aristocracy and constitutionally-mandated hereditary privilege. Our political elite often comes from one of two universities (Oxford and Cambridge, the alma mater of 20% of current UK Members of Parliament) and a handful of fee-paying schools (like Eton, which is a hellmouth for the worst people to ever exist, and educated 20 of the UK’s 55 prime ministers).&nbsp;</p><p>The UK has never been an egalitarian society. And yet, things have changed markedly in the past few decades.The difference between now and then is that the silver-spooned elite was, whether because they believed it or because it was politically expedient, not totally contemptuous of those at the bottom of the economic ladder.&nbsp;</p><p>I was born in the midst of the Thatcher government, and my formative years were spent as British society tried to restructure itself after her reforms. Thatcher, famously, was an acolyte of the Friedman school of thought, and spent her nearly twelve years in office dismantling the state and pushing the culture towards an American-style individualism, once famously quipping that there was “no such thing as society.”&nbsp;</p><p>She didn’t understand how things worked, but was nonetheless completely convinced of the power of the market to handle what was the functions of the state — from housing to energy and water. The end result of this political and cultural shift was, in the long run, disastrous.&nbsp;</p><p>The UK has the smallest houses in the OECD, the <a href="https://www.pbctoday.co.uk/news/planning-construction-news/english-housing-is-worst-in-europe-report-finds/133243/?ref=wheresyoured.at"><u>smallest housing stock of any developed country</u></a>,&nbsp; and some of the worst affordability. The privatization of the UK’s water infrastructure meant that money that would previously go towards infrastructure upgrades was, instead, funnelled to shareholders in the form of dividends. As a result, Britain is literally unable to process human waste and is <a href="https://www.bbc.co.uk/news/explainers-62631320?ref=wheresyoured.at"><u>actively dumping millions of liters of human sewage into its waterways and coastline</u></a>. When Britain privatized its energy companies, the new management sold or <a href="https://www.newstatesman.com/the-weekend-essay/2023/02/running-on-fumes-britain-squandered-gas-wealth?ref=wheresyoured.at"><u>closed the vast majority of its gas storage infrastructure</u></a>. As a result, when the Ukraine War sparked, and natural gas prices surged, Britain had some of the smallest reserves of any country in Europe, and was forced to buy gas at the market prices — which were several times higher than their pre-war levels.&nbsp;</p><p>I’m no fan of Thatcher, and like Friedman, I too wish hell exists, if only for the both of them. I wrote the above to emphasize the consequences of this clueless managerial thinking on a macro level — where the impacts aren’t just declining tech products or white-collar layoffs, but rather the emergence of generational crises in housing, energy, and the environment. These crises were obvious consequences of decisions made by someone whose belief in the free market was almost absolute, and whose fundamentalist beliefs surpassed the actual informed understanding of those working in energy, or housing, or water.&nbsp;</p><p>As the legendary advertiser Stanley Pollitt once said, “bullshit baffles brains.” The sweeping changes we’ve seen, both in our economy and in our society, has led to an unprecedented, gilded age of bullshit where nothing matters, and things — things of actual substance — matter nothing.&nbsp;</p><p>We live in a symbolic economy where we apply for jobs, writing CVs and cover letters to resemble a certain kind of hire, with our resume read by someone who doesn't do or understand our job, but yet is responsible for determining whether we’re worthy of going to the next step of the hiring process. All this so that we might get an interview with a manager or executive who will decide whether they think we can do it. We are managed by people <em>whose job is implicitly not to do work, but oversee it. </em>We are, as children (and young adults), encouraged to aspire to become a manager or executive, to "own our own business," to "have people that work for us," and the terms of our society are, by default, that management is <em>not a role you work at, so much as a position you hold — </em>a figurehead that passes the buck and makes far more of them than you do.</p><p>This problem, I believe, has poisoned the fabric of almost every part of modern business, elevating people that don't do work to oversee companies that make things they don't understand, creating substrates of management that do not do anything but create further distance from actually doing a job.</p><p>While some of you might automatically think I'm talking about Graeber's concept of<a href="https://en.wikipedia.org/wiki/Bullshit_Jobs?ref=wheresyoured.at"><u> Bullshit Jobs</u></a>, this is far, far bigger. The system as it stands selects people at all levels of management <em>specifically because they resemble the kind of specious, work-averse dullard that runs seemingly every company —</em> a person built to go from meeting to meeting with the vague consternation that suggests they're "busy."</p><p>As a result, the higher you get up in an organization, the further you get from the customer, the problem you've solving, and any of the actual work, and the higher up you get<em>, the more power you have to change the conditions of the business.</em> On some level, modern corporate power structures are a giant game of telephone where vibes beget further vibes, where managers only kind-of-sort-of understand what's going on, and the more vague one's understanding is, the more likely you are to lean toward what's good, or easy, or makes you feel warm and fuzzy inside.</p><p>The system selects for people comfortable in these roles, creating org charts full of people that become harder and harder to justify other than "they've been here a while." They do not do "work" on the "product," and their answer as to why would be "what, am I meant to go down on the line and use a machine?" or "am I meant to call a customer and make a sale?" and the answer is yes, you lazy fucking piece of shit, you should do that once in a while, or at the very least go and watch or listen to somebody else do so, and do so regularly.</p><p>But that's not what a manager does, right? Management isn't <em>work, </em>it's about <em>thinking really hard</em> and <em>telling people what to do.</em> It's about <em>making the calls.</em> It's about "managing people," and that can mean just about anything, but often means "who do I take credit from or pass blame to," because modern management has been stripped of all meaning other than continually reinforcing power structures for the next manager up.</p><p>This system creates products for these people, because these people are more often than not the ones in power — they are your boss, your boss' boss, and their boss too. Big companies build products sold by specious executives or managers to other specious executives, and thus the products themselves stop resembling things that solve <em>problems</em> so much as they <em>resemble a solution</em>. After all, the person buying it — at least at the scale of a public company — isn’t necessarily the recipient of the final product, so they too are trained (and selected) to make calls based on vibes.</p><p>I believe the scale of this problem is society-wide, and it is, at its core, a destruction of what it means to be a leader, and a valorization of selfishness, isolationist thinking, turning labor into a faceless resource, which naturally leads to seeing customers in an equally faceless way, their problems generalized, their pain points viewed as parts of a powerpoint rather than anything that your company earnestly tries to solve or even really thinks about. And that assumes that said pain points are even considered to begin with, or not ignored in favor of a fictitious and purely hypothetical pain point.&nbsp;</p><p>People — be they the ones you're paying or paying you — become numbers. We have created and elevated an entirely new class of person, the nebulous "manager," and told decades-worth of children that's what they should aspire to, that the next step from doing a job is for us to tell other people to do a job, until we're able to one day tell <em>those</em> people how to do their job, with each rung on the corporate ladder further distancing ourselves from anything that interacts with reality.</p><p>The real breaking point is fairly simple: the higher up you go at a company, the further you are from problems or purpose. Everything is abstract — the people that work for you, the people you work for, and even the tasks you do.&nbsp;</p><p>We train people — from a young age! — to generalize and distance oneself from actual tasks, to aspire to doing managerial work, because managers are well-paid and "know what's going on," even if they haven't actually known what was going on for years, if they ever did so. This phenomenon has led to the stigmatization of blue-collar work (and the subsequent evisceration of practical trade and technical education across most of the developed world) in favor of universities. Society respects an MBA more than a plumber, even though the latter benefits society more — though I concede that both roles involve, on some level, shit, with the plumber unblocking it and the MBA spewing it.&nbsp;</p><blockquote><strong>Sidebar: </strong>Hey, have you noticed how most of the calls for people to return to the office come not from people who actually do the jobs, but occupy managerial roles? More on that later.&nbsp;</blockquote><p>I believe this process has created a symbolic society — one where people are elevated not by any actual ability to do something or knowledge they may have, but by their ability to make the right noises and look the right way to get ahead. The power structures of modern society are run by <em>business idiots</em> — people that have learned enough to impress the people above them, because the business idiots have had power for decades. They have bred out true meritocracy or achievement or value-creation in favor of symbolic growth and superficial intelligence, because <em>real work is hard, and there are so many of them in power they've all found a way to work together.</em></p><p>I need you to understand how widespread this problem is, because it is why everything feels fucking <em>wrong.</em></p><hr><p>Think of the Business Idiot as a kind of con artist, except the con has become the standard way of doing business for an alarmingly large part of society.&nbsp;</p><p>The Business Idiot is&nbsp; the manager that doesn't seem to do anything but keeps being promoted, and<a href="https://x.com/levie/status/1919573650547622130?ref=wheresyoured.at"><u> the chief executive officer of a public company that says boring, specious nonsense</u></a> about AI. They're the tenured professor that you wish would die, the administrator whose only job appears to be opening and closing their laptop, the consultant that can come up with a million reasons to charge you more money yet not one metric to judge their success by, the marketing executive that's worked exactly three years at every major cloud player but does not appear to have done anything, and the investor that invests "based on founders," but really means "guys that look at sound exactly like them."</p><p>These people are present throughout the private and public sector, and our governments too, and they paradoxically do nothing of substance, but somehow damage everything they touch. This isn’t to say our public and private sector is entirely useless — just that these people have poisoned so many parts of our power structures that avoiding them is impossible.&nbsp;&nbsp;</p><p>Our economy is oriented around them — made easier and more illogical for their benefit — because their literal only goal in life has been to take and use power. The Business Idiot is also an authoritarian, and will do whatever they need to — including harming the institution they work for, or those closest to them, like their co-workers or their community — as a means of avoiding true accountability or responsibility.</p><p>Decades of neoliberalism has incentivized their rise, because when you incentivize society to become <em>management</em> — to "manage or run a company" rather than do something for a reason or purpose — you are incentivizing a kind of corporate narcissism, one that bleeds into whatever field the person goes into, be it public or private. We go to college as a means of getting a job after college using the grades we got in college, rendering many students desperate to get the best grades they can versus "learn" anything, because our economy is riddled with power structures controlled by people that <em>don't know stuff and find it offensive when you remind them.</em></p><p>Our society is in the thrall of dumb management, and functions as such. Every government, the top quarter of every org chart, features little Neros who, instead of battling the fire engulfing Rome, are sat in their palaces strumming an off-key version of “Wonderwall” on the lyre and grumbling about how the firefighters need to work harder, and maybe we could replace them with an LLM and a smart sprinkler system.&nbsp;</p><p>Every institution keeps its core constituents and labor forces at arms-length, and effectively anything built at scale quickly becomes distanced from both the customer and laborer. This disconnection — or alienation — sits at the center of almost every problem I've ever talked about. Why would companies push generative AI in seemingly every part of their service, even though customers don't like it and it doesn't really work?</p><p>It's simple: they neither know nor care what the customer wants, barely know how their businesses function, barely know what their products do, and barely understand what their workers are doing, meaning that generative AI feels magical, because it does an impression of somebody doing a job, which is an accurate way of describing how most executives and middle managers operate.</p><hr><p>Let me get a little more specific.</p><p><a href="https://www.wheresyoured.at/reality-check/"><u>An IBM study based on conversations with 2,000 global CEOs</u></a> recently found that only 25% of AI initiatives have delivered their expected ROI over the last few years, and, worse still, "64% of CEOs surveyed acknowledge that the risk of falling behind drives investment in some technologies before they have a clear understanding of the value they bring to the organization." 50% of respondents also found that "the pace of recent investments has left their organization with disconnected, piecemeal technology," <em>almost as if they don't know what they're doing and are just putting AI in stuff for no reason.</em></p><p>Johnson &amp; Johnson recently decided to "shift from broad generative AI experimentation to a focused approach on high-value use cases"<a href="https://www.wsj.com/articles/johnson-johnson-pivots-its-ai-strategy-a9d0631f?ref=wheresyoured.at"><u> according to the Wall Street Journal</u></a>, adding that "only 10 to 15% of use cases were driving about 80% of the value." Their last two CEOs (Alex Gorsky and Joaquin Duato) both have MBAs, with current CEO Duato's previous ten years at Johnson &amp; Johnson being "<a href="https://www.linkedin.com/in/joaquinduato/details/experience/?ref=wheresyoured.at"><u>some sort of Chairman or Vice President</u></a>," and the previous two CEOs (Alex Gorsky and William Weldon) were both pharmaceutical sales and marketing people.&nbsp;</p><p>Fun fact about Alex Gorsky! During his first tenure at Johnson &amp; Johnson he led marketing of products<a href="https://www.nytimes.com/2010/01/16/business/16drug.html?ref=wheresyoured.at"><u> <strong>that deliberately underplayed some drugs' side effects and paid off the largest nursing home pharmacy in America to sell more drugs to old people</strong></u></a><strong>.</strong></p><p>The term "executive" loosely refers to a person who moves around numbers and hopes for the best. The modern executive does not "lead," but prod, their managers hall monitors for organizations run predominantly by people that, by design, are entirely removed from the business itself even <em>in</em> roles like marketing and sales, where CMOs and VPs bark orders without really participating in the process.</p><p>We talk eagerly about how young people in entry level jobs should "earn their stripes" by doing "grunt work," and that too is the neoliberal poison in the veins of our society, because, by definition, your very first experience of the workforce is working hard enough <em>so that you don't have to work as hard.</em></p><p>And anyway, the same managerial types who bitch about the entitlement and unrealistic expectations of young people are the same ones that also eviscerated the bottom rung of the career ladder — typically by offshoring many of these roles, or <em>consolidating</em> them into the responsibilities of their increasingly burned-out senior workers — or see AI as a way to eliminate what they see as an optional cost center, and not the future of their workforce.&nbsp;</p><p>Society berated people for "<a href="https://www.wheresyoured.at/quiet-quitting-and-the-death-of-office/"><u>quiet quitting</u></a>," a ghastly euphemism for “doing the job as specified in your employment contract,” in 2022 because journalism is enthralled by the management class, and because the management class has so thoroughly rewritten the concept of what "labor" means that people got called lazy for <em>literally doing their jobs</em>. The middle manager brain doesn't see a worker as somebody hired and paid for a job, but as an asset that must provide a return. As a result, if another asset comes along that could potentially provide a bigger return — like an offshore worker, or an AI agent — that middle manager won’t hesitate to drop them.&nbsp;</p><p>Artificial intelligence is the ultimate panacea for the Business Idiot — a tool that gives an impression of productivity with far more production than the Business Idiot themselves.<a href="https://www.theinformation.com/articles/bots-bust-servicenows-bill-mcdermott-makes-bet-ai?rc=kz8jh3&amp;ref=wheresyoured.at"><u> The Information reported recently</u></a> that ServiceNow CEO Bill McDermott — the chief executive of a company with a market capitalization of over $200 billion, despite the fact that, like SalesForce, nobody really knows what it does&nbsp; — chose to push AI across his whole organization (both in product and in practice) based on the mental consideration I'd usually associate with a raven finding a shiny object:</p><blockquote>When ChatGPT debuted in November 2022, McDermott joined his executives around a boardroom table and they played with the chatbot together. From there, he made a quick decision. “Bill’s like, ‘Let me make it clear to everybody here, everything you do: AI, AI, AI, AI, AI,’” recalled Tzitzon, the ServiceNow vice chair.<p>To begin a customer meeting on AI, McDermott has asked his salespeople to do what amounts to their best impression of him: Present AI not as a matter of bots or databases but in grand-sounding terms, like “business transformation.</p><p>During the push to grow AI, McDermott has insisted his managers improve efficiency across their teams. He is laser-focused on a sales team’s participation rate. “Let’s assume you’re a manager, and you have 12 direct reports,” he said. “Now let’s assume out of those 12, two people did good, which was so good that the manager was 110% of plan. I don’t think that’s good. I tell the manager: ‘What did the other 10 do?’”</p></blockquote><p>You'll notice that all of this is complete nonsense. What do you mean "efficiency"? What does that quote even mean? 110% of plan? What're you on about? Did you hit your head on something Bill?</p><p>I'd wager Bill is concussion-free — and an example of a true Business Idiot — a person with incredible power and wealth that makes decisions not based on knowing stuff or caring about his customers, but on the latest shiny thing that makes him think "<em>line go up</em>." No, really, that's Bill McDermott's thing. Back in 2022,<a href="https://finance.yahoo.com/news/veteran-tech-exec-bullish-on-mark-zuckerberg-metaverse-172635544.html?ref=wheresyoured.at"><u> he said to Yahoo Finance the metaverse</u></a> was "real" and that ServiceNow could help someone "create an e-mall in the metaverse" and have a futuristic store of some sort. One might wonder how ServiceNow provided that, and the answer is it didn't. I cannot find a single product that it’s offered that includes it.</p><p>Bill, like any of these CEOs, doesn't really <em>know stuff</em>, or even <em>do</em> stuff, he just <em>is. </em>The corporate equivalent of a stain on a carpet that nobody really knows how it got there, but hasn’t been removed. The modern executive is symbolic, and the media has — due to the large amount of Business Idiots running these outlets and middle managers stuffed into the editorial class — been trained to never ask difficult questions, such as "what the fuck are you talking about, Bill?" or even the humble "what does that mean?" or "how would you do that?" or saying "I'm not sure I understand, would you mind explaining?"</p><p>Perhaps the last part is the symptom of the overall problem. So many layers of editorial and managerial power are filled full of people that don't know anything, and there's never anyone crueler about ignorance than somebody that's ignorant themselves.&nbsp;</p><p>Worse still, in many fields — journalism included — we are rarely rewarded for knowing things or being "right," but being right in the way that keeps the people with the keys from scraping them across our cars.&nbsp; We are, however, rewarded for saying <em>the right thing</em> at the <em>right time</em>, which more often than not means resembling our (white, male) superiors, speaking like our peers, and delivering results <em>in the way that makes everybody feel happiest.</em></p><hr><p>A great example of our vibes-based society was back in October 2021,<a href="https://www.wheresyoured.at/the-new-anti-remote-propaganda-wants/"><u> where a Washington Post article written by two Harvard professors</u></a> rallied against remote work<a href="https://www.wheresyoured.at/microsofts-meaningless-remote-work/"><u> by citing a Microsoft-funded anti-remote study</u></a> and quoting 130-year-old economist Alfred Marshall about how "workers gather in dense clusters,"<a href="https://www.academia.edu/4446735/Race_and_Nation_in_Marshalls_Histories?ref=wheresyoured.at"><u> ignoring the fact that Marshall was so racist they've had to write papers about it</u></a>,<a href="https://mises.org/wire/keynes-eugenics-race-and-population-control?ref=wheresyoured.at"><u> how excited he was about eugenics</u></a>, <strong><em>or the fact he was writing about fucking factories.</em></strong></p><p>Remote work terrifies the Business Idiot, because it removes the performative layer that allowed them to stomp around and feel important, reducing their work to, well...work.<a href="https://www.huffpost.com/entry/black-workers-prefer-remote-work-racist-office_l_60c8f805e4b0f7e7ccf59fa1?ref=wheresyoured.at"><u> Office culture is inherently heteronormative and white</u></a>, and<a href="https://www.pbs.org/newshour/economy/report-black-women-less-likely-to-be-promoted-supported-by-their-managers?ref=wheresyoured.at"><u> black women are less likely to be promoted by their managers</u></a>, and continuing the existence of "The Office" is all about making sure The Business Idiot reigns supreme. Removing the ability for the managerial hall monitors to look at you and try and work out what you're doing without ever really helping is a big part of being a manager — and if you're a manager reading this and saying you don't do this, I challenge you to talk to another person that doesn't confirm your biases.</p><p>The Business Idiot reigns supreme. Their existence holds up almost every public company, and remote work was the first time they willingly raised their heads. Google demanded employees return to the office in 2021 —<a href="https://www.cnet.com/news/google-employees-angered-by-search-giants-hypocritical-remote-work-policies/?ref=wheresyoured.at"><u> but let one executive work remotely <em>from New Zealand</em></u></a><em> because absolutely none of the decisionmaking was done with people that actually do work.</em> While we can&nbsp; (well, you can, I'm not interested) debate whether <em>exclusively</em> working remote is as productive, the Return To Office push was almost entirely done in two ways:</p><ol><li>Executives demanding people return to the office.</li><li>Journalists asking executives if remote work was good or not, entirely ignoring the people <em>actually doing the work.</em></li></ol><p><a href="https://www.wheresyoured.at/the-anti-workforce/#:~:text=As%20I%20mentioned,and%20calls%20made."><u>The New York Times, The Washington Post, The Wall Street Journal, and many, many other outlets</u></a> all fell for this crap because the Business Idiots have captured our media too, training even talented journalists to defer to power at every turn. When every power structure is stuffed full of do-nothing management types that have learned exactly as little as they need to as a means to get by, it's inevitable that journalism caters to them — specious, thoughtless reproductions of the powerful's ideas.</p><p>Look at the coverage of AI, or the<a href="https://www.wheresyoured.at/mark-zuckerberg-is-a-liar-and-hes/"><u> metaverse</u></a>, or<a href="https://www.wheresyoured.at/the-emperors-new-blockchain/"><u> cryptocurrency</u></a>, or<a href="https://www.wheresyoured.at/clubhouse-is-the-big-stinker-that/"><u> Clubhouse</u></a>. Look at how willingly reporters will accept narratives not based on practical experience or <em>what the technology can do</em>, but what the powerful (and the popular) are suddenly interested in. Every single tech bubble followed the same path, and that path was paved with flawed, deferential and specious journalism, from small blogs to the biggest mastheads.</p><p>Look at how reporters talk to executives — not just the way they ask things (<a href="https://www.theverge.com/24158374/google-ceo-sundar-pichai-ai-search-gemini-future-of-the-internet-web-openai-decoder-interview?ref=wheresyoured.at"><u>like Nilay Patel's 100+ word questions to Sundar Pichai</u></a> in his abominable interview), but the things they accept them saying, and the willingness reporters have to just accept what they're told. Satya Nadella is the CEO of a company with a market capitalization of over $3 trillion. I have no idea how you, as a reporter, do not say "Satya, what the fuck? You're outsourcing most of your life to generative AI? That's insane!" or even "do you really do that?" and then asking further questions.</p><p>But that would get you in trouble. The editorial class <em>is</em> the managerial class now, and has spent decades mentoring young reporters to <em>not</em> ask questions, to <em>not </em>push back, to <em>believe </em>that a big, strong, powerful company CEO would never mislead them.<a href="https://thebaffler.com/latest/the-miseducation-of-kara-swisher-ongweso?ref=wheresyoured.at"><u> Kara Swisher's half-assed interviews</u></a> are considered "daring" and "critical" because journalism has, at large, lost its teeth, breeding reporters rewarded for knowing a little bit about a few things and punishing those who ask too many questions or refuse to fall in line.</p><p>The reason they don't want you to ask these questions is that the Business Idiot isn't big on answers. Editors that tell you not to push too hard are doing so because they know the executive likely won't have an answer. It isn't just about the PR person that trained them, but the fact that these men more often than not have only a glancing understanding of their underlying business.</p><p>Yet in the same way that Business Idiots penetrated every other part of society, they eventually found their way to journalism. While we can (and should) scream at<a href="https://www.wheresyoured.at/the-anti-economy/"><u> the disconnected idiots that ran Vice into the ground</u></a>, the problem is <em>everywhere</em>,<em> </em>because the Business Idiots aren't just at the top, but infecting the power structures underlying every newsroom.</p><p>While there are many really great editors, there are plenty more that barely understand the articles they edit, the stories they commission, or that make reporters pull punches for fear of advertiser blowback.</p><p>That, <a href="https://www.wheresyoured.at/the-myth-of-mentorship-and-how-weve/"><u>and mentorship is dead across effectively all parts of society</u></a>, meaning that most reporters (as with many jobs) learn by watching each other, which means they all make sure to not ask the rough questions, and not push too hard against the party/market/company messaging until everybody else does it.</p><p>And under these conditions, <em>Business Idiots thrive.</em></p><hr><p>The Business Idiot's reign is one of speciousness and shortcuts, of acquisition, of dominance and of theft. Mentoring people is something you do to pass on knowledge — it may make them <em>grateful</em> to you, but it ultimately, in the mind of a Business Idiot, creates a competitor or rival.&nbsp;</p><p>Investing in talent, or worker conditions, or even really <em>work</em> itself would require you to know what you're talking about, or actually do work, which doesn't make sense when you're talking to a worker. They're the ones who're meant to work! You're there to manage them! Yet they keep talking back — asking questions about the work you want them to do, asking <em>you</em> to step in and help on something — and all of that's so annoying. Just know the stuff already! Get it done! I have to go to lunch and then go back out to another lunch!&nbsp;</p><p>I believe this is the predominant mindset across most of the powerful, to the point that everything in the world is constructed to reaffirm their beliefs rather than follow any logical path. Our stock market is inherently illogical, driven not by whether a company is good or bad,<a href="https://www.wheresyoured.at/the-rot-economy/"><u> but whether it can show growth</u></a>, even if said growth is horrifically unprofitable, and I'd argue it's because the market has no idea how to make <em>intelligent</em> decisions, just <em>complex</em> ones that mean that you don't really need to understand the business so much as you understand the associated vibes of the industry.</p><p>Friedman's influence and Reagan's policies have allowed our markets to be dominated by Business Idiocy, where a bad company can be a good stock because everybody (IE: other traders and the business press) likes how it looks, which allows the Business Idiots to continue making profit using illogical and partially-rigged market-making, with the business press helpfully pushing up their narratives.&nbsp;</p><p>This also keeps regular people from accumulating too much wealth — if regular people could set the tone for the markets as "a company that makes something people like and people pay them for it and they make more money than they spend," that might make things a little <em>too </em>even.</p><p><a href="https://www.wsj.com/business/earnings/coreweave-posts-sales-jump-in-first-quarter-since-ipo-d8eda266?ref=wheresyoured.at"><u>It doesn't matter that CoreWeave quite literally does not have enough money for its capital expenditures and lost over $300m in the last quarter</u></a> because its<a href="https://www.datacenterdynamics.com/en/news/coreweaves-revenue-up-420-yoy-in-first-earnings-call-since-ipo/?ref=wheresyoured.at"><u> year-over-year growth was 420%</u></a>. It<a href="https://www.wheresyoured.at/core-incompetency/#:~:text=Problem%20Loan%20Number%202%3A%20DDTL%202.0"><u> doesn't matter that it has October loan payments that will crush the life out of the company either</u></a>. These narratives are fed to the media knowing that the media will print them, because thinking too hard about a stock would mean the Business Idiot had to think also, <em>and that is not why they are in this business.</em></p><p>The "AI trade" is the Business Idiot's nirvana — a fascination for a managerial class that long since gave up any kind of meaningful contribution to the bottom line, as moving away from the fundamental creation of value as a business naturally leads to the same kind of specious value that one finds from generative AI.</p><p>I’m not even saying that there’s no returns, or that LLMs don’t do anything, or even that there’s no possible commercial use for generative AI. They just don’t do enough, almost by design, and we’re watching companies desperately try and contort them into something, anything that works, pretending so fucking hard they’ll stake their entire futures on the idea. <em>Just fucking work, will you?</em> Agentforce doesn’t make any money, it sucks, but god damn is Marc Benioff going to make you bear witness.</p><p><a href="https://www.theinformation.com/articles/ai-giving-salesforce-boost?rc=kz8jh3&amp;ref=wheresyoured.at"><u>Does it matter that Agentforce doesn't make Salesforce any money</u></a>? No! Because Benioff and Salesforce have got rich selling to fellow Business Idiots who then shove Salesforce into their organization without thinking about who would use it or how they'd use it other than in the most general ways. Agentforce was — and is — a fundamentally boring and insane product, charging $2 a conversation for a chatbot that, to quote The Information, provides customers with "...incorrect answers — AI hallucinations — while testing how the software handles customer service queries."</p><p>But this shit is <em>catnip</em> to the Business Idiot, because the Business Idiot ideally never has to deal with work, workers or customers. Generative AI doesn’t do enough to actually help us be better at our jobs, but it gives a good enough impression of something useful so that it can convince someone really stupid that doesn’t understand what you do that they don’t need you, sometimes.</p><p>A generative output is a kind of generic, soulless version of production, one that resembles exactly how a know-nothing executive or manager would summarise your work. OpenAI's "Deep Research" wows<a href="https://www.nytimes.com/2025/03/04/opinion/ezra-klein-podcast-ben-buchanan.html?ref=wheresyoured.at"> <u>professional Business Idiot Ezra Klein</u></a> because he doesn't seem to realize that part of research <em>is the research itself, not just the output</em>, as you <em>learn about stuff as you research a topic, allowing you to come to a conclusion.</em> The concept of an "agent" is the erotic dream of the managerial sect — a worker that they can personally command to generate product that they can say is their own, all without ever having to know or do anything other than the bare minimum of keeping up appearances, which is the entirety of the Business Idiot's resume.</p><p>And because the Business Idiot's career has been built on only knowing exactly enough to get by, they don't dig into Large Language Models any further than hammering away at ChatGPT and saying "we must put AI in everything<em> now.</em>" Yet the real problem is that for every Business Idiot selling a product, there are many more that will buy it, which has worked in the past for Software as a Service (SaaS) companies that<a href="https://www.wheresyoured.at/saaspocalypse-now/"> <u>grew fat and happy hocking giant annual contracts and continual upsells</u></a>, because CIOs and CTOs work for Business Idiot CEOs that demand that they "put AI in everything now," a nonsensical and desperate remit that's part growth-lust and part ignorance, borne of the fear that one gets when they're out of their depth.</p><p>Look at every single institution installing some sort of ChatGPT integration, and then look for the Business Idiot. Perhaps it's Cal State University Chanceller Mildred Garcia,<a href="https://www.calstate.edu/csu-system/news/Pages/CSU-AI-Powered-Initiative.aspx?ref=wheresyoured.at"> <u>who claimed</u></a> that giving everybody a ChatGPT subscription would "elevate...students' educational experience across all fields of study, empower [its] faculty's teaching and research, and help provide the highly educated workforce that will drive California's future AI-driven economy," a nonsensical series of words to justify<a href="https://bsky.app/profile/jbyoder.org/post/3lpagmird5s2b?ref=wheresyoured.at"> <u>a $16.9 million-a-year single-vendor no-bid contract</u></a> to a product that is best known as either a shitty search engine or<a href="https://nymag.com/intelligencer/article/openai-chatgpt-ai-cheating-education-college-students-school.html?ref=wheresyoured.at"> <u>a way to cheat at college</u></a>.</p><p>In some ways, Sam Altman is the Business Idiot's antichrist, taking advantage of a society where the powerful rarely know much other than what they want to control or dominate. ChatGPT and other AI tools are, for the most part, sold based on what they might do in the future to people that will never really use them, and<a href="https://www.youtube.com/watch?v=55L7HYUFg-c&amp;ref=wheresyoured.at"> <u>Altman has done well</u></a> to<a href="https://www.reuters.com/technology/openai-ceo-altman-says-davos-future-ai-depends-energy-breakthrough-2024-01-16/?ref=wheresyoured.at"> <u>manipulate</u></a>,<a href="https://www.thewrap.com/openai-ceo-sam-altman-ai-freedom-chatgpt/?ref=wheresyoured.at"> <u>pester</u></a> and<a href="https://www.cnn.com/2023/10/31/tech/sam-altman-ai-risk-taker?ref=wheresyoured.at"> <u>terrify</u></a> those in power with the idea that they might miss out on <em>something.</em> Does anyone know what it is? No, they don't, because the powerful are Business Idiots too, willing to accept anything that somebody brings along that makes them feel good, or bad in a way that they can make headlines with.</p><blockquote><a href="https://www.gov.ca.gov/2022/05/04/governor-newsom-signs-blockchain-executive-order-to-spur-responsible-web3-innovation-grow-jobs-and-protect-consumers/?ref=wheresyoured.at"><u>Hey, whatever happened to Gavin Newsom's Blockchain executive order</u></a>? Did that do anything?</blockquote><p>In any case, Altman's whole Sloppenheimer motif has worked wonders on the Business Idiots in the markets and global governments that fear what artificial intelligence could do, even if they can't really define artificial intelligence, or what it could do, or what they're scared of. The fear of China's "rise in AI" is one partially based on sinophobia, and partially based on the fact that China has their own Business Idiots willing to shove hundreds of millions of dollars into data centers.</p><p>Generative AI has created a reckoning between the Business Idiot and the rest of society, its forced adoption and proliferation providing a meager return for the massive investment of capital and the revulsion it causes in many people, not just in the Business Idiot's excitement in replacing them, but how <em>wrong the Business Idiot is.</em></p><p>While there are many people that dick around with ChatGPT, years since it launched we still can't find a clean way to say what it <em>does</em> or why it <em>matters</em> other than the fact that everybody agreed it did. The media, now piloted by Business Idiots, has found itself declawed, its reporters unprepared, unwilling and unsupported, the backbone torn out of most newsrooms for fear that being too critical is somehow "not being objective," despite the fact that <em>what you choose to cover objectively is still subjective.</em></p><p>Reporters still, to this day,<a href="https://www.wheresyoured.at/openai-is-a-systemic-risk-to-the-tech-industry-2/"> <u>as these companies burn billions of dollars</u></a> to<a href="https://www.wheresyoured.at/wheres-the-money/"> <u>make an industry the size of the free-to-play gaming industry</u></a>, refuse to say things that bluntly because "the cost of inference is coming down" and "these companies have some of the smartest people in the world." They ignore the truth as it sits in front of them — <strong>that the combined annual recurring revenue of The Information's comprehensive database of every generative AI company is less than $10 billion,</strong> or <strong>$4 billion if you remove Anthropic and OpenAI.</strong></p><p>ChatGPT's popularity is the ultimate Business Idiot success story — the "fastest growing product in Silicon Valley history" that didn't grow because it was useful, or good, or able to do anything in particular, but because a media controlled by Business Idiots decided it was "the next big thing" and started talking about it nonstop since November 2022, guaranteeing that everybody would try it, even if even <strong>to this day</strong> the company can't really explain <em>what it is you're meant to use it for.</em></p><p>Much like the Business Idiot themselves, ChatGPT doesn't need to <em>do</em> anything specific. It just needs to make the right sounds at the right times to impress people that barely care what it does other than make them feel futuristic.</p><p>Real people — regular people, not Business Idiots, not middle managers, not executive coaches, not MBAs, not CEOs — have seen this for what it was early and often, but real people are seldom the ones with the keys, and the media — even the people writing good stuff — regularly fails to directly and clearly say what's going on.&nbsp;</p><p>The media is scared of doing the wrong thing — of "getting in trouble" with someone for "misquoting them" or "misreading what they said" — and in a society where in-depth knowledge is subordinate to knowing enough catchphrases, the fight often doesn't feel worth it even with an editor's blessing.</p><p>I also want to be clear that this goes far beyond <em>money.</em> Editors aren't just scared of advertisers being upset. They know that if narratives have to shift toward more critical, thoughtful coverage, they too will have to be more thoughtful and knowledgeable, which is rough when you are a Business Idiot and got there by editing the right people in a way that didn't help them in the slightest.</p><hr><p>Nothing about what I'm saying should suggest the Business Idiot is weak. In fact, Business Idiots are fully in control —<a href="https://www.theatlantic.com/ideas/archive/2021/09/manager-work-life-changes/620096/?ref=wheresyoured.at"> <u>we have too many managers</u></a>, and our most powerful positions are valorized for <em>not</em> knowing stuff, for having a <em>general</em> view that can "take the big picture," not realizing that a big picture <em>is usually made up of lots of little brush strokes.</em></p><p>Yet there are, eventually, consequences for everything being controlled by Business Idiots.</p><p>Our current society — an unfair, unjust one dominated by half-broken tech products that make their owners billions — is the real punishment wrought by growth, a brain drain in corporate society, one that leads it to doing illogical things and somehow making money. It doesn't make any fucking sense that generative AI got this big. The returns aren't there, the outcomes aren't there, and any sensible society would've put a gun to a ChatGPT and aggressively pulled the trigger.</p><p>Instead it's the symbolic future of capitalism — one that celebrates mediocrity and costs billions of dollars, every human work it can consume, and the destruction of our planet, all because everybody has kind of agreed that this is what they're all doing, with nobody able to give a convincing explanation of what that even is. Generative AI is revolting both in how overstated its abilities are and in how it continually tests how low a standard someone will take for a product, both in its outputs and in the desperate companies trying to integrate it into everything, and its proliferation throughout society and organizations is already fundamentally harmful.</p><p>We’re not just drowning in a sea of slop — we’re in a constant state of corporate AI beta tests, new “features” sprouting out of our products like new limbs that sometimes function normally but often attempt to strangle us. It’s unclear if companies forcing these products on us have contempt for us or simply don’t know what good looks like. Or perhaps it's both, with the Business Idiot resenting us for not scarfing down whatever they serve us, as that's what's worked before.&nbsp;</p><p>They don't really understand their customers — they understand <em>what a customer pays for </em>and <em>how a purchase is made</em>, you know, like the leaders of banks and asset managers during the subprime mortgage crisis didn't really think about whether people could <em>pay</em> those mortgages, just that they <em>needed a lot of them to put in a CDO.</em></p><p>The Business Idiot's economy is one built for other Business Idiots. They can only make things that sell to companies that must always be in flux — which is the preferred environment of the Business Idiot, because if they're not perpetually starting new initiatives and jumping on new "innovations," they'd actually have to interact with the underlying production of the company.&nbsp;</p><p>Does the software work? Sometimes! Do successful companies exist that sell like this? Sure! But look at today's software and tell me with a straight face that things <em>feel good to use.</em></p><p>And something like generative AI was inevitable: an industry claiming to change the world that never really does so, full of businesses that don’t function as businesses, full of flimflam and half-truths used to impress people who will likely never interact with it, or do so in only a passing way. By chasing out the people that actually build things in favour of the people that sell them, our economy is built on production puppetry — just like generative AI, and especially like ChatGPT.&nbsp;</p><p>These people are antithetical to what’s good in the world, and their power deprives us of happiness, the ability to thrive, and honestly any <em>true</em> innovation. The Business Idiot thrives on alienation — on distancing themselves from the customer and the thing they consume, and in many ways from society itself. <a href="https://www.businessinsider.com/mark-zuckerberg-destroyed-friendship-replace-ai-companions-loneliness-2025-5?ref=wheresyoured.at"><u>Mark Zuckerberg wants us to have fake friends</u></a>, <a href="https://www.inc.com/ben-sherry/sam-altman-says-ai-agents-will-transform-the-workforce-in-2025/91103146?ref=wheresyoured.at"><u>Sam Altman wants us to have fake colleagues</u></a>, and an increasingly loud group of executives salivate at the idea of replacing us with a fake version of us that will make a shittier version of what we make for a customer that said executive doesn’t fucking care about.&nbsp;</p><p>They’re building products for other people that don’t interact with the real world. We are no longer their customers, and so, we’re worth even less than before — which, as is the case in a world dominated by shareholder supremacy, not all that much.</p><p>They do not exist to make <em>us</em> better — the Business Idiot doesn’t really care about the real world, or what you do, or who you are, or anything other than your contribution to their power and wealth. This is why so many squealing little middle managers look up to the Musks and Altmans of the world, because they see in them the same kind of specious corporate authoritarian, someone above work, and thinking, and knowledge.&nbsp;</p><hr><p>One of the most remarkable things about the Business Idiot is their near-invulnerability.</p><p>Modern management is resource control, shifting blame away from the manager (who should hold responsibility. After all, if they don’t, why do they have that job?) onto the laborer, knowing that the organization and the media will back it up.&nbsp;</p><p>While you may think I’m making a generalization, the 2021-2023 anti-remote work push in the media was grotesque proof of where the media’s true allegiance lies — <a href="https://www.wheresyoured.at/manufacturing-consent-for-the-office/"><u>the media happily manufactured consent</u></a> <a href="https://www.wheresyoured.at/death-to-office-jerks/"><u>for return-to-office</u></a> <a href="https://www.wheresyoured.at/the-death-of-company-culture/"><u>mandates from large companies</u></a> <a href="https://www.wheresyoured.at/the-pro-office-silent-majority-is/"><u>by framing remote work as some sort of destructive force</u></a>, <a href="https://www.wheresyoured.at/nobody-works-eight-hours-a-day-and/"><u>doing all they can to disguise how modern management has no fucking idea how the workplace actually works</u></a>.&nbsp;</p><p>These articles were effectively fan fiction for managers and bosses demanding we return to the office — ridiculous statements about how remote work “<a href="https://www.wheresyoured.at/the-myth-of-mentorship-and-how-weve/"><u>failed young people</u></a>” (it didn’t) or <a href="https://www.wheresyoured.at/your-workplace-friendships-are-your/"><u>how employees needed remote work more than their employers</u></a> because “<a href="https://www.wsj.com/opinion/working-at-home-remote-work-office-anxiety-depression-drinking-alcohol-covid-mental-illness-11646426004?ref=wheresyoured.at"><u>the chitchat, lunches and happy hours” are so important</u></a>. Had any of those reporters spoken to an actual worker, they’d say that they value more time with their families, rather than the grind of a daily commute softened with the promise of an occasional company pizza party — which usually happens outside of the typical working hours, anyway.&nbsp;</p><p>These articles rarely (if ever) cared about whether <a href="https://www.peoplemanagement.co.uk/article/1918460/half-workers-say-remote-work-boosts-productivity-few-employers-agree-survey-finds?ref=wheresyoured.at"><u>remote work was more productive</u></a>, or <a href="https://hbr.org/2023/01/research-where-managers-and-employees-disagree-about-remote-work?ref=wheresyoured.at"><u>that the disconnect appeared to be between managers and workers</u></a>. It was, from the very beginning, about crushing the life out of a movement that gave workers more flexibility and mobility while suppressing managers’ ability to hide how little work they did. <a href="https://www.cnbc.com/2023/10/17/the-no-1-challenge-holding-companies-back-from-offering-remote-work.html?ref=wheresyoured.at"><u>I give credit to CNBC in 2023 for saying the quiet part out loud</u></a> — that “...the biggest disadvantage of remote work that employers cite is how difficult it is to observe and monitor employees” — because when you can’t do that, you have to (<em>eugh!</em>) actually know what they’re doing and understand their work.&nbsp;</p><p>Yet higher up the chain, the invulnerability continues.&nbsp;</p><p>CEOs may get fired — <a href="https://www.bloomberg.com/news/newsletters/2024-12-03/ceos-getting-fired-from-their-jobs-at-record-pace-in-2024?ref=wheresyoured.at"><u>and more are getting fired than ever</u></a>, although sadly not the ones we want — but always receive some sort of golden parachute payoff at the end before walking into another role at another organization doing exactly the same level of nothing.&nbsp;&nbsp;</p><p>Yet before that happens, A CEO is allowed to pull basically every lever before they take a single ounce of accountability — laying people off, pay freezes, moving from salaried to contracted workers, closing down sites, cutting certain products, or even <em>spending more fucking money</em>. If you or I misallocated billions of dollars on stupid ideas we’d be fired. CEOs, somehow, get paid more.</p><p>Let me give you an example. Microsoft CEO Satya Nadella said that the “ultimate computer…is the mixed reality world” and that Microsoft would be “<a href="https://www.siliconrepublic.com/business/microsoft-ceo-satya-nadella-augmented-reality?ref=wheresyoured.at"><u>inventing new computers and new computing</u></a>” in 2016, <a href="http://o/?ref=wheresyoured.at"><u>pushing his senior executives to tell reporters that Hololens was Microsoft’s next wave of computing in 2017</u></a>, <a href="https://www.zdnet.com/article/microsoft-ceo-nadella-hololens-for-war-is-fine-if-its-used-by-a-democracy/?ref=wheresyoured.at"><u>selling hundreds of millions of dollars’ worth of headsets to the military in 2019</u></a>, then <a href="https://www.engineering.com/hololens-2-demo-goes-horribly-wrong-at-build-2019/?ref=wheresyoured.at"><u>debuting HoloLens 2 at BUILD 2019 only for the on-stage demo to break in realtime</u></a>, <a href="https://www.businessinsider.com/satya-nadella-microsoft-ceo-referendum-on-capitalism-2020-10?ref=wheresyoured.at"><u>calling for a referendum on capitalism in 2020</u></a>, then <a href="https://www.wheresyoured.at/content/files/wp-content/uploads/prod/2021/11/microsoft-ignite-2021-satya-nadella.pdf"><u>saying he couldn’t overstate the breakthrough of the metaverse in 2021</u></a>. <a href="https://www.computerworld.com/article/1618790/as-microsoft-embraces-ai-it-says-sayonara-to-the-metaverse.html?ref=wheresyoured.at"><u>Let’s see what he said about it (props to Preston Gralla of ComputerWorld for finding this)</u></a>:</p><blockquote>Nadella, in that 2021 keynote, made big promises: “When we talk about the metaverse, we’re describing both a new platform and a new application type, similar to how we talked about the web and websites in the early ’90s…. In a sense, the metaverse enables us to embed computing into the real world and to embed the real world into computing, bringing real presence to any digital space. For years, we’ve talked about creating this digital representation of the world, but now, we actually have the opportunity to go into that world and participate in it.”</blockquote><p>As Gralla notes, Nadella said Microsoft would be, “…beefing up development in projects such as its Mixed Reality Tool Kit MRTK, the virtual reality workspace project AltspaceVR (which it had bought back in 2017), its HoloLens virtual reality headset, and its industrial metaverse unit, among others,” before <a href="https://www.forbes.com/sites/rosemariemiller/2023/02/14/microsofts-industrial-metaverse-aspirations-can-wait/?sh=49b8c11a140a&amp;ref=wheresyoured.at"><u>firing all 100% members of its industrial Metaverse core team</u></a> <a href="https://redmondmag.com/articles/2023/01/24/microsoft-cuts-mixed-reality-toolkit-team-and-plans-to-end-altspacevr.aspx?ref=wheresyoured.at"><u>along with those behind MRTK</u></a> and <a href="https://www.roadtovr.com/microsoft-social-vr-xr-interface-layoffs/?ref=wheresyoured.at"><u>shutting down AltSpace VR (which it acquired in 2017)</u></a> in 2023, <a href="https://www.theverge.com/2024/10/1/24259369/microsoft-hololens-2-discontinuation-support?ref=wheresyoured.at"><u>before discontinuing HoloLens 2 entirely in 2024</u></a>.</p><p>Nadella was transparently copying <a href="https://www.wheresyoured.at/mark-zuckerberg-is-a-liar-and-hes/"><u>Meta and Mark Zuckerberg’s ridiculous “metaverse”</u></a> play, and <em>absolutely nothing happened to him as a result.</em> The media — outlets like <a href="https://www.theverge.com/22588022/mark-zuckerberg-facebook-ceo-metaverse-interview?ref=wheresyoured.at"><u>The Verge</u></a> and independents like <a href="https://stratechery.com/2021/microsoft-and-the-metaverse/?ref=wheresyoured.at"><u>Ben Thompson</u></a> — happily boosted the metaverse idea when it was announced and conveniently forgot it the second that Microsoft and Meta wanted to talk about AI (no, really, both <a href="https://www.theverge.com/2024/4/18/24134370/mark-zuckerberg-meta-interview-llama-3-ai-assistant-race?ref=wheresyoured.at"><u>The Verge </u></a>and <a href="https://stratechery.com/2024/interviews-with-microsoft-ceo-satya-nadella-and-cto-kevin-scott-about-the-ai-platform-shift/?ref=wheresyoured.at"><u>Ben Thompson</u></a> were ready and waiting) without a second’s consideration about what was previously said.&nbsp;</p><p>A true Business Idiot never admits wrongdoing, and the more powerful the Business Idiot is, the more likely there are power structures that exist to avoid them having to do so. The media, captured by other Business Idiots, has become poisoned by power, deferring to its whims and ideals and treating CEOs with more respect, dignity and intelligence than anyone that works for them. When a big company decides they want to “do AI,” the natural reaction is to ask “how?” and write down the answer rather than think about whether it’s possible or whether the company might profit (say, by increasing their shareholder price) by having whatever they say printed ad verbatim.&nbsp;</p><p>These people aren’t challenged by the media, or their employees, because their employees are vulnerable all the time, and often encouraged to buy into whatever bullshit is du jour like hostages being held by a terrorist group that eventually fall victim to Stockholm syndrome. They’re only challenged by shareholders, who are agnostic about idiocy because it’s not core to value in any meaningful sense, as we’ve seen with crypto, the metaverse and AI, and shareholders will tolerate infinite levels of idiocy if it boosts the value of their holdings.&nbsp;</p><p>It goes further too. 2021 saw the largest amount of venture capital invested in the last decade, <a href="https://news.crunchbase.com/business/global-vc-funding-unicorns-2021-monthly-recap/?ref=wheresyoured.at"><u>a record-breaking $643 billion</u></a>, <a href="https://news.crunchbase.com/startups/na-vc-startup-funding-2021-recap/?ref=wheresyoured.at#North%20America%20overall"><u>with a remarkable $329.5 billion of that invested in the US alone</u></a>. Some of the biggest deals include Amazon reseller aggregator Thrasio, which<a href="https://www.vccircle.com/private-equity-firm-silver-lake-leads-1-bn-investment-in-thrasio?ref=wheresyoured.at"><u> raised $1 billion in October 2021</u></a> and <a href="https://www.cnbc.com/2024/04/24/amazon-aggregator-thrasio-loses-ceo-other-top-execs-in-bankruptcy.html?ref=wheresyoured.at"><u>filed for bankruptcy in February 2025</u></a>, cloud security company Lacework, which<a href="https://news.crunchbase.com/venture/biggest-vc-startup-funding-deals-sierra-space-lacework-heyday/?ref=wheresyoured.at"><u> raised $525 million in January 2021 then $1.3 billion in October 2021</u></a> and <a href="https://www.sdxcentral.com/news/cloud-security-unicorn-wiz-in-talks-to-acquire-rival-lacework/?ref=wheresyoured.at"><u>was rumoured to be up for sale</u></a> to Wiz, <a href="https://www.calcalistech.com/ctechnews/article/hj3v0algc?ref=wheresyoured.at"><u>only for the deal to collapse</u></a>, and autonomous car company Cruise, <a href="https://www.goingvc.com/post/a-year-in-review-what-happened-in-venture-capital-in-2021?ref=wheresyoured.at"><u>which raised $2.75 billion 2021</u></a> and <a href="https://www.roadtoautonomy.com/gm-pulls-plug-cruise/?ref=wheresyoured.at#:~:text=On%20December%2010th%2C%20GM%2C%20seemingly,operations%20next%20week%20in%20Houston."><u>was killed off in December 2024</u></a>.&nbsp;</p><p>The people who lose their livelihoods — those who took stock in lieu of cash compensation, and those who end up getting laid off at the end — are always workers, while people like Lacework co-CEO Jay Parikh (<a href="https://www.theinformation.com/articles/how-lacework-went-from-cybersecurity-stardom-to-fire-sale-talks?rc=kz8jh3&amp;ref=wheresyoured.at"><u>who oversaw “reckless spending” and “management dysfunction” according to The Information</u></a>) can walk into highly-paid positions at companies like Microsoft, <a href="https://www.crn.com/news/security/2024/ex-lacework-ceo-jay-parikh-to-join-microsoft-in-senior-exec-role?ref=wheresyoured.at"><u>as he did in October 2024</u></a> a few months after <a href="https://www.globenewswire.com/news-release/2024/08/02/2923644/0/en/Fortinet-Completes-Acquisition-of-Lacework.html?ref=wheresyoured.at"><u>a fire sale to cybersecurity Fortinet for around $200 million according to analysts</u></a>.&nbsp;</p><p>It doesn’t matter if you’re wrong, or if you run your company badly, because the Business Idiot is infallible, and judged too by fellow disconnected Business Idiots. In a just society, nobody would want to touch any of the C-suite that oversaw a company that handed out Nintendo Switches to literally anyone who booked a meeting (as with Lacework). Instead, the stank remains on the employees alone.&nbsp;</p><blockquote>One point about this: Meta’s most recent layoffs were <a href="https://www.businessinsider.com/meta-layoffs-low-performance-cuts-review-cycle-politics-2025-4?ref=wheresyoured.at"><u>explicitly said to target low-performers</u></a>, needlessly harming the future job prospects of those handed a pink slip in an already fucked tech job market. It was cruel and pointless and — I’m certain — a big fat lie. <p>Meta is spending big on AI, has spent big on the metaverse (which went nowhere), and owns two dying platforms (Instagram and Facebook) and one that’s hard to monetize (WhatsApp). It needs to get costs down and improve margins. Layoffs are one way to do that. And things are getting bad enough that Meta is now, </p><a href="https://www.theinformation.com/articles/meta-asked-amazon-microsoft-help-fund-llama?rc=kz8jh3&amp;ref=wheresyoured.at"><u>according to The Information</u></a>, walking around Silicon Valley begging other big tech companies for money to train their open source “Llama” LLM.<p>The “low-performer” jibe is an unnecessary twist of the knife, demonstrating that Meta would happily throw its workers under the bus if it serves their interests — because the optics of firing low-performers is different to, say, firing a bunch of people because you keep spunking money on dead-end vanity projects and me-too products that nobody wants or uses.&nbsp;</p><p>Mark Zuckerberg, I add, owns an island on Hawaii. The idea that he even thinks this much about Meta is disgraceful. Go outside, you fucking freak.</p></blockquote><p>It’s so easy, and perhaps inevitable, to feel a sense of nihilism about it all. Nothing matters. It’s all symbolic. Our world is filled with companies run by people who don’t interact with the business, and that raise money from venture capitalists that neither run businesses nor really have any experience doing so. And despite the fact that these people exist several abstractions from reality, the things that they do and the decisions they make impact us all. And it’s hard to imagine how to fix it.&nbsp;</p><hr><p>We live in a system of iniquity, dominated by people that do not interact with the real world who have created an entire system run by their fellow Business Idiots. <a href="https://www.wheresyoured.at/the-rot-economy/"><u>The Rot Economy’s</u></a> growth-at-all-costs mania is a symptom of the grander problem of <a href="https://www.wheresyoured.at/tss/"><u>shareholder supremacy</u></a>, and the single-minded economic focus on shareholder value inevitably ends at an economy run by <em>and for</em> Business Idiots. There is a line, and it ends here — with layoffs, the destruction of our planet and our economy and our society, and a rising tide of human misery that nobody really knows where it comes from, and so, we don’t know who to blame, and for what.</p><p>If our economy actually worked as a true meritocracy — where we didn’t have companies run by people who don’t use their products or understand how they’re made, and who hire similarly-specious people — these people would collapse under the pressure of having to know their ass from their earhole.</p><p>Yet none of this would be possible without the enabling layers, and those layers are teeming with both Business Idiots and those unfortunate enough to have learned from them. The tech media has enabled every single bubble, without exception, accepting every single narrative fed to them by VCs and startups, with even critical reporters still accepting <a href="https://www.wheresyoured.at/openai-is-a-systemic-risk-to-the-tech-industry-2/"><u>the lunacy of a company like </u></a>OpenAI just because everybody else does too.</p><p>Let’s be honest, when you remove all the money, our current tech industry is a disgrace.</p><p>Our economy is held up by NVIDIA, a company that makes most of its money selling GPUs to other companies primarily so that they can start losing the money selling software that might eventually make them money, just not today. NVIDIA is defined by massive peaks and valleys, as it jumps on trends and bandwagons at the right time, despite knowing that these bandwagons always come to an abrupt halt.</p><p>The other companies feature Tesla, a meme stock car company with a deteriorating brand and a chief executive famous for his divorces from both reality and multiple women along with a flagrant racism that may cost the company its life. A company that we are watching die in real time, with a stagnant line-up and actual fucking competition from companies that are spending big on innovation.</p><p>In Europe and elsewhere, <a href="https://www.bbc.com/news/articles/cd65d583qvzo?ref=wheresyoured.at"><u>BYD is eating Tesla’s lunch</u></a>, offering better products for half the price — and far less stigma. And this is just the first big Chinese automotive brand to go global. Others — <a href="https://www.reuters.com/business/autos-transportation/chinas-chery-reports-384-jump-2024-sales-26-mln-vehicles-2025-01-07/?ref=wheresyoured.at"><u>like Chery</u></a> — are enjoying rapid growth outside of China, because these cars are actually quite good and affordable, even when you factor in the impact of things like tariffs.&nbsp;</p><p>Hey, remember when Tesla fired all the people in its charging network — despite that being one of the most profitable and valuable parts of the business? <a href="https://www.reuters.com/business/autos-transportation/inside-story-elon-musks-mass-firings-tesla-supercharger-staff-2024-05-15/?ref=wheresyoured.at"><u>And then hired them back because it turns out they were actually useful?</u></a></p><p>This is a good example of managerial alienation — decisions made by non-workers who don’t understand their customers, their businesses, or the work their employees do. And let’s not forget about the Cybertruck, a monstrosity both in how it looks and <a href="https://www.splinter.com/teslas-cybertruck-is-an-unmitigated-disaster?ref=wheresyoured.at"><u>how it’s sold</u></a>, and that’s illegal in the majority of developed countries because it is a death-trap for drivers and pedestrians alike. Oh, and that nobody actually wants, with <a href="https://insideevs.com/news/758619/tesla-cybertruck-quarter-worth-inventory/?ref=wheresyoured.at"><u>Tesla sitting on a quarter’s worth of inventory that it can’t sell</u></a>.&nbsp;</p><p>Elsewhere is Meta, a collapsing social network with 99% of its revenue based on advertising to an increasingly-aged population and a monopoly so flagrantly abusive in its contempt for its customers that it’s at times difficult to call Instagram or Facebook social networks.</p><p>Mark Zuckerberg had to admit to the Senate Judiciary Committee that <a href="https://www.reuters.com/technology/meta-trial-5-key-moments-zuckerbergs-testimony-2025-04-16/?ref=wheresyoured.at"><u>people don’t use Facebook as a social network anymore</u></a>. The reason why is because <a href="https://www.wheresyoured.at/were-watching-facebook-die/"><u>the platform is so fucking rotten</u></a>, <a href="https://www.wheresyoured.at/killingfacebook/"><u>run by a company alienated from its user base</u></a>, its decrepit product actively hostile to anybody trying to use it.&nbsp;</p><p>And, more fundamentally, what’s the point of posting on Facebook if your friends won’t see it, because Meta’s algorithm decided it wouldn’t drive engagement?&nbsp;</p><p>Meta is a monument to disconnection, a company that runs in counter to its own mission to connect people, run by Mark Zuckerberg, a man who hasn’t had a good idea since he stole it from the Winklevoss Brothers. The solution to all that ails him? Adding generative AI to every part of Meta, which…uh…was meant to do something other <a href="https://finance.yahoo.com/news/meta-raised-high-end-ai-210737148.html?ref=wheresyoured.at"><u>than burn $72 billion in capital expenditures in 2025</u></a>, right? It isn’t clear what was meant to happen, but the Wall Street Journal reports that Meta’s AI chatbots are, and I quote, “<a href="https://www.wsj.com/tech/ai/meta-ai-chatbots-sex-a25311bf?ref=wheresyoured.at"><u>empowered to engage in ‘romantic role-play’ that can turn explicit” — even with children</u></a>. In a civil society, Zuckerberg would be ousted immediately for creating a pedophilic chatbot, — instead, <a href="https://www.cnbc.com/2025/04/30/meta-q1-earnings-report-2025.html?ref=wheresyoured.at"><u>four days after the story ran, everyone cheered Meta’s better-than-expected quarterly earnings</u></a>.</p><p>In Redmond, Microsoft sits atop multiple monopolies, <a href="https://www.axios.com/2025/05/01/xbox-price-increase-trump-tariffs-microsoft?ref=wheresyoured.at"><u>using tariffs as a means to juice flailing Xbox revenue</u></a> as it invests billions of dollars in OpenAI so that OpenAI can spend billions of dollars on cloud compute, losing billions more in the process, requiring Microsoft to invest further money to keep them alive. All <a href="https://blogs.microsoft.com/blog/2023/02/07/reinventing-search-with-a-new-ai-powered-microsoft-bing-and-edge-your-copilot-for-the-web/?ref=wheresyoured.at"><u>because Microsoft wanted generative AI in Bing</u></a>. What a fucking waste!&nbsp;</p><p>While also raising the costs of its office suite — <a href="https://www.wheresyoured.at/monopoly-money/#:~:text=And%20now%20for%20a%20little%20history%20lesson%20%E2%80%94%20and%20a%20story%20about%20how%20remarkably%20different%20the%20computing%20market%20could%20look%20today%20if%20said%20legislation%20had%20succeeded."><u>which it’s only able to hold a monopoly on because it acted so underhandedly in the 1990s</u></a>.</p><p>Amazon lumbers listlessly through life, its giant labor-abuse machine shipping things overnight at whatever cost necessary to crush the life out of any other source of commerce, its cloud services and storage arm, unsure who to copy next. Is it Microsoft? Is it Google? Who knows! But<a href="https://finance.yahoo.com/news/amazon-stock-falls-as-raymond-james-downgrades-shares-citing-tariff-headwinds-and-limited-ai-monetization-144747355.html?ref=wheresyoured.at"><u> one analyst believes it’s making $5 billion in revenue from AI in 2025</u></a> — and spending $105 billion in capital expenditures. There are slot machines with a better ROI than this shit.&nbsp;</p><p>Again, it’s a company that’s totally exploitative of its customers, no longer acting as a platform that helps people find the shit they need, but <a href="https://nymag.com/intelligencer/article/is-amazon-turning-into-temu.html?ref=wheresyoured.at"><u>to direct them to the products that pay the most for prime advertising real-estate, no matter whether they are good or safe</u></a>.</p><p>Let’s be clear: <a href="https://www.youtube.com/watch?v=y83BS_mK9GE&amp;ref=wheresyoured.at"><u>Amazon’s recklessness will kill someone</u></a>, <a href="https://www.youtube.com/watch?v=B90_SNNbcoU&amp;ref=wheresyoured.at"><u>if it hasn’t already</u></a>.&nbsp;&nbsp;&nbsp;</p><p>Then there’s the worst of them — Google. Most famous for its namesake, <a href="https://www.wheresyoured.at/the-men-who-killed-google/"><u>a search engine that it has juiced as hard as possible</u></a>, and will continue to juice before the <a href="https://www.npr.org/2025/04/21/nx-s1-5369404/google-doj-opening-statements-remedies-trial?ref=wheresyoured.at"><u>inevitable antitrust sentencing that would rob it of its power</u></a>, <a href="https://www.theverge.com/news/650665/google-loses-ad-tech-antitrust-monopoly-lawsuit?ref=wheresyoured.at"><u>along with the severance of its advertising monopoly</u></a>. But don’t worry, Google also has a generative AI thing, for some reason, and no, you don’t have a choice about using it, because it’s now stapled onto Google Search and <a href="https://gemini.google/assistant/?hl=en&amp;ref=wheresyoured.at"><u>Google Assistant</u></a>.&nbsp;</p><p>At no point do any of these companies seem to be focused on making our lives better, or selling us any kind of real future. They exist to maintain the status quo, where cloud computing allows them to retain their various fiefdoms.</p><p>They’re alienated from people.</p><p>They’re alienated from workers.</p><p>They’re alienated from their customers.</p><p>They’re alienated from the world.</p><p>They’re deeply antisocial and misanthropic — as demonstrated by Zuck’s moronic AI social network comments.</p><p>And AI is a symptom of a reckoning of this stupidity and hubris.</p><p>They cut, and cut, and stagnated. Their hope is a product that will be adopted by billions of imaginary customers and companies, and will allow them to cut further without becoming just a PO Box and a domain name.</p><p>We have to recognize that what we’re seeing now with generative AI isn’t a fluke or a bug, but a feature of a system that’s rapacious and short-term by its very nature, and doesn’t define value as we do, because “value” gets defined by a faceless shareholder as “growth.” And this system can only exist with the contribution of the business idiot. These are the vanguard — the foot soldiers — of this system, and a key reason why everything is so terrible all the time, and why nothing seems to be getting better.&nbsp;</p><p>Breaking from that status-quo would require a level of bravery that they don’t have — and perhaps isn’t possible in the current economic system.&nbsp;</p><p>These people are powerful, and have big platforms. They’re people like Derek Thompson, famed co-author of the “<a href="https://foreignpolicy.com/2025/05/09/abundance-review-klein-thompson-progressive-policy/?ref=wheresyoured.at"><u>abundance</u></a>” agenda, <a href="https://bsky.app/profile/drewharwell.com/post/3lpmme2m6bk2u?ref=wheresyoured.at"><u>who celebrates the idea of a fictitious version of ChatGPT that can entirely plan and execute a 5-year-old’s birthday party</u></a>, or his co-author Ezra Klein, who, while recording a podcast where his researchers likely listened, <a href="https://www.nytimes.com/2025/03/04/opinion/ezra-klein-podcast-ben-buchanan.html?ref=wheresyoured.at"><u>talked proudly about replacing their work with OpenAI’s broken Deep Research product</u></a>, because anything that can be outsourced must be, and all research is “looking at stuff that is relevant.”</p><p>And really, that’s the most grotesque part about Business Idiots. They see every part of our lives as a series of inputs and outputs They boast about how many books they’ve read rather than the content of said books, <a href="https://finance.yahoo.com/news/simple-math-musk-said-100-134527947.html?ref=wheresyoured.at"><u>about how many hours they work</u></a> (even though they never, ever work that many), <a href="https://aftermath.site/aftermath-hours-podcast-elon-musk-asmongold-twitch-path-of-exile?ref=wheresyoured.at"><u>about high level they are in a video game they clearly don’t play</u></a>, about the money they’ve raised and the scale they’ve raised it at, and <a href="https://www.ft.com/content/b1804820-c74b-4d37-b112-1df882629541?ref=wheresyoured.at"><u>about how expensive and fancy their kitchen gadgets are</u></a>. Everything is dominance, acquisition, growth and possession over any lived experience, because their world is one where the journey doesn’t matter, because <em>their journeys are riddled with privilege and the persecution of others in the pursuit of success.&nbsp;</em></p><p>These people don’t want to automate <em>work</em>, they want to automate existence. They fantasize about hitting a button and something happening, because experiencing — living! — is beneath them, or at least <em>your </em>lives and <em>your</em> wants and <em>your </em>joy are. They don’t want to plan their kids’ birthday parties. They don’t want to research things. They don’t value culture or art or beauty. They want to skip to the end, hit fast-forward on anything, because human struggle is for the poor or unworthy.&nbsp;</p><p>When you are steeped in privilege and/or have earned everything through a mixture of stolen labor and office pantomime, the idea of “effort” is always negative. The process of creation — or affection, of love, of kindness, of using time not just for an action or output — is disgusting to the Business Idiot, because those are times they could be focused on themselves, or some nebulous self-serving “vision” that is, when stripped back to its fundamental truth, is either moronic or malevolent. They don’t realise that you hire a worker for the worker’s work rather than just the work themselves, which is why they don’t see why it’s so insulting to outsource their interactions with human beings.&nbsp;</p><p>You’ll notice these people never bring up examples of automating actual work — the mind-numbing grunt work that we all face in the workplace — because they neither know nor care what that is. Their “problems” are the things that frustrate them, like dealing with other people, or existing outside of the gilded circles of socialite fucks or plutocrats, or just things that are an inevitable facet of working life, like reading an email. Your son’s birthday party or <a href="https://www.wired.com/story/using-chatgpt-for-interpersonal-relationship-advice/?ref=wheresyoured.at"><u>a conflict with a friend can</u></a>, indeed, be stressful, but these are not problems to be automated out. They are the struggles that make us human, the things that make us grow, the things that make us who we are, which isn’t a problem for anybody other than <em>somebody who doesn’t believe they need to change in any way. </em>It's both powerful and powerless at the same time — a nihilistic way of seeing our lives as a collection of events we accept or dismiss like a system prompt, the desperate pursuit of such efficient living that you barely feel a thing until you die.&nbsp;</p><p>I’ve spent years writing about these people without giving them a name, because categorizing anything is difficult. I can’t tell you how long it took for me to synthesize the Rot Economy from the broader trends I saw in tech and elsewhere, how long it took for me to thread that particular needle, to identify the various threads that unified events that are otherwise separate and distinct.</p><p>I am but one person. Everything you’ve read in my newsletter to this point has been something I’ve had to learn. Building an argument and turning it into words — often at the same time — that other people will read doesn’t come naturally to <em>anyone</em>. It’s something you have to deliberately work at.&nbsp; It’s imperfect. There are typos. These newsletters increase in length and breadth and have so many links, and I will never, ever change my process, because part of said process is learning, relearning, processing, getting pissed off, writing, rewriting, and so on and so forth.&nbsp;</p><p>This process makes what I do possible, and the idea of having someone automate it disgusts me, not because I’m special or important, but because my work is not the result of me reading a bunch of links or writing a bunch of words. This piece is not just 13,000 words long — it’s the result of the 800,000 or more words I wrote before it, the hundreds of stories I’ve read in the past, the hours of conversations with friends and editors, years of accumulating knowledge and, yes, growing with the work itself.&nbsp;</p><p>This is not something that you create through a summation of content vomited by an AI, but the chaotic histories of a human being mashed against the challenge of trying to process it. Anyone who believes otherwise is a fucking moron — or, better put, just another Business Idiot.</p>
    </article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Introducing the Llama Startup Program (173 pts)]]></title>
            <link>https://ai.meta.com/blog/llama-startup-program/?_fb_noscript=1</link>
            <guid>44052984</guid>
            <pubDate>Wed, 21 May 2025 16:10:06 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://ai.meta.com/blog/llama-startup-program/?_fb_noscript=1">https://ai.meta.com/blog/llama-startup-program/?_fb_noscript=1</a>, See on <a href="https://news.ycombinator.com/item?id=44052984">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><div><p>We’re excited to announce the <a href="https://www.llama.com/programs/startups/" target="_blank" data-lnfb-mode="ie"><u>Llama Startup Program</u></a>, a new initiative to empower early-stage startups to innovate and build generative AI applications with <a href="https://www.llama.com/" target="_blank" data-lnfb-mode="ie"><u>Llama</u></a>. Members of the Llama Startup Program will receive resources and support from Llama experts along their journey, including help getting started with Llama and resources necessary to succeed and thrive in a competitive and fast-moving landscape.</p><h2>Why we’re launching the Llama Startup Program</h2><div><p>Early-stage startups are agile and creative, making them uniquely positioned to accelerate high-impact innovation with Llama. In a recent <a href="https://about.fb.com/news/2025/05/new-study-shows-open-source-ai-catalyst-economic-growth/" target="_blank" data-lnfb-mode="ie"><u>Linux Foundation study</u></a><a href="https://about.fb.com/news/2025/05/new-study-shows-open-source-ai-catalyst-economic-growth/" target="_blank" data-lnfb-mode="ie">,</a> 94% of organizations say they’ve already adopted AI tools and models, and of those, 89% are using some form of open source technology—such as Llama—in their AI infrastructure.</p><p>We want to give our Llama Startup Program members a competitive edge by offering direct support from the Llama team and may help to fund their use of Llama models. We hope this support accelerates their development process and enhances their ability to deliver innovative solutions. We’ve seen how our <a href="https://about.fb.com/news/2025/04/llama-impact-grant-recipients/" target="_blank" data-lnfb-mode="ie"><u>Llama Impact Grants</u></a> have helped recipients innovate and deliver economic opportunity, and we believe that supporting developers through the Llama Startup Program will help early-stage startups take flight.</p></div></div><div><h2>What are the benefits of being part of the Llama Startup Program?</h2><div><p>During the initial phase of the Llama Startup Program, we're reimbursing the cost of using Llama through hosted APIs via cloud inference providers. Members may receive up to $6,000 USD per month for up to six months to help them offset the costs of building and enhancing their generative AI solutions. This funding enables startups to experiment, innovate, and scale their solutions without the immediate financial burden, enabling them to focus on what truly matters: creating impactful technologies.</p><p>Members of the program will receive hands-on technical support from the Llama team. Our experts will work closely with them to get started and explore advanced use cases of Llama that could benefit their startups. This direct access to technical expertise ensures that developers can effectively leverage Llama’s capabilities, optimize solutions, and overcome technical challenges they may encounter as they start building.</p></div><h2>Who should apply?</h2><div><p>The Llama Startup Program is an exciting opportunity for early-stage startups in the United States that are ready to innovate with generative AI. We invite startups that are incorporated, have raised less than $10 million USD in funding, and have at least one developer on staff to apply. The Llama Startup Program is ideal for those building generative AI applications across a variety of industries, including technology and software, financial services, healthcare and life sciences, telecommunications, and retail and eCommerce. Join us for the opportunity to receive cloud reimbursements of up to $6,000 USD per month for up to six months, technical resources, and a vibrant community—all while contributing valuable feedback to enhance the Llama experience. If your startup is ready to grow and make a meaningful impact, we encourage you to apply and be part of this transformative program.</p><p>Applications for the initial cohort close on May 30, 2025 at 6:00 pm PT.</p></div></div></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[ZEUS – A new two-petawatt laser facility at the University of Michigan (107 pts)]]></title>
            <link>https://news.engin.umich.edu/2025/05/the-us-has-a-new-most-powerful-laser/</link>
            <guid>44052418</guid>
            <pubDate>Wed, 21 May 2025 15:17:10 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://news.engin.umich.edu/2025/05/the-us-has-a-new-most-powerful-laser/">https://news.engin.umich.edu/2025/05/the-us-has-a-new-most-powerful-laser/</a>, See on <a href="https://news.ycombinator.com/item?id=44052418">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><figure><img width="672" height="448" src="https://news.engin.umich.edu/wp-content/uploads/2025/05/ti-sapph-crystal-2PWLaser.jpg" alt="The circular glass optics and silvery metal mounts that create the laser path appear tinted pink through the round window of the crystal." decoding="async" fetchpriority="high" srcset="https://news.engin.umich.edu/wp-content/uploads/2025/05/ti-sapph-crystal-2PWLaser.jpg 672w, https://news.engin.umich.edu/wp-content/uploads/2025/05/ti-sapph-crystal-2PWLaser-300x200.jpg 300w, https://news.engin.umich.edu/wp-content/uploads/2025/05/ti-sapph-crystal-2PWLaser-120x80.jpg 120w, https://news.engin.umich.edu/wp-content/uploads/2025/05/ti-sapph-crystal-2PWLaser-500x333.jpg 500w" sizes="(max-width: 672px) 100vw, 672px"></figure>


<div><p><time datetime="2025-05-19T13:51:45-04:00">May 19, 2025</time></p>

<p><time datetime="2025-05-21T08:39:07-04:00">May 21, 2025</time></p></div>




<p>Hitting 2 petawatts, the NSF-funded ZEUS facility at U-M enables research that could improve medicine, national security, materials science and more. </p>







</div><div>
<div>




<div data-initially-open="false" data-click-to-close="true" data-auto-close="true" data-scroll="false" data-scroll-offset="0"><h2 id="at-162540" role="button">Karl Krushelnick</h2><div id="ac-162540">
<figure><img decoding="async" width="120" height="120" src="https://news.engin.umich.edu/wp-content/uploads/2021/03/krushelnick-portrait.jpg" alt="Portrait of Karl Krushelnick."></figure>



<p><a href="https://ners.engin.umich.edu/people/krushelnick-karl/" target="_blank" rel="noreferrer noopener">See full bio</a></p>



<p>Director of the Gérard Mourou Center for Ultrafast Optical Science</p>



<p>Henry J Gomberg Collegiate Professor of Engineering</p>



<p>Professor of Nuclear Engineering and Radiological Sciences</p>
</div></div>



<div data-initially-open="false" data-click-to-close="true" data-auto-close="true" data-scroll="false" data-scroll-offset="0"><h2 id="at-162541" role="button">Anatoly Maksimchuk</h2><div id="ac-162541">
<figure><img loading="lazy" decoding="async" width="214" height="214" src="https://news.engin.umich.edu/wp-content/uploads/2022/09/Maksimchuk_small-214x300-1-e1663244765298.jpg" alt="Anatoly Maksimchuk" srcset="https://news.engin.umich.edu/wp-content/uploads/2022/09/Maksimchuk_small-214x300-1-e1663244765298.jpg 214w, https://news.engin.umich.edu/wp-content/uploads/2022/09/Maksimchuk_small-214x300-1-e1663244765298-150x150.jpg 150w, https://news.engin.umich.edu/wp-content/uploads/2022/09/Maksimchuk_small-214x300-1-e1663244765298-120x120.jpg 120w" sizes="auto, (max-width: 214px) 100vw, 214px"></figure>



<p><a href="https://cuos.engin.umich.edu/researchgroups/hfs/profiles/anatoly-maksimchuk/" target="_blank" rel="noreferrer noopener">See full bio</a></p>



<p>Research Scientist in Electrical and Computer Engineering</p>
</div></div>



<div data-initially-open="false" data-click-to-close="true" data-auto-close="true" data-scroll="false" data-scroll-offset="0"><h2 id="at-162542" role="button">John Nees</h2><div id="ac-162542">
<figure><img loading="lazy" decoding="async" width="350" height="350" src="https://news.engin.umich.edu/wp-content/uploads/2025/05/John-Nees-portrait.jpg" alt="Portrait of John Ness." srcset="https://news.engin.umich.edu/wp-content/uploads/2025/05/John-Nees-portrait.jpg 350w, https://news.engin.umich.edu/wp-content/uploads/2025/05/John-Nees-portrait-300x300.jpg 300w, https://news.engin.umich.edu/wp-content/uploads/2025/05/John-Nees-portrait-150x150.jpg 150w, https://news.engin.umich.edu/wp-content/uploads/2025/05/John-Nees-portrait-120x120.jpg 120w" sizes="auto, (max-width: 350px) 100vw, 350px"></figure>



<p><a href="https://cuos.engin.umich.edu/researchgroups/hfs/profiles/john-nees/" target="_blank" rel="noreferrer noopener">See full bio</a></p>



<p>Research Scientist in Electrical and Computer Engineering</p>
</div></div>



<div data-initially-open="false" data-click-to-close="true" data-auto-close="true" data-scroll="false" data-scroll-offset="0"><h2 id="at-162543" role="button">Franko Bayer</h2><div id="ac-162543">
<figure><img loading="lazy" decoding="async" width="350" height="350" src="https://news.engin.umich.edu/wp-content/uploads/2021/08/Franko-Bayer-portrait.jpg" alt="Franko Bayer portrait" srcset="https://news.engin.umich.edu/wp-content/uploads/2021/08/Franko-Bayer-portrait.jpg 350w, https://news.engin.umich.edu/wp-content/uploads/2021/08/Franko-Bayer-portrait-300x300.jpg 300w, https://news.engin.umich.edu/wp-content/uploads/2021/08/Franko-Bayer-portrait-150x150.jpg 150w, https://news.engin.umich.edu/wp-content/uploads/2021/08/Franko-Bayer-portrait-120x120.jpg 120w" sizes="auto, (max-width: 350px) 100vw, 350px"></figure>



<p><a href="https://cuos.engin.umich.edu/researchgroups/hfs/profiles/john-nees/" target="_blank" rel="noreferrer noopener">See full bio</a></p>



<p>Project Manager for Zettawatt-Equivalent Ultrashort Pulse Laser System (ZEUS)</p>
</div></div>
</div>



<p>The ZEUS laser facility at the University of Michigan has roughly doubled the peak power of any other laser in the U.S. with its first official experiment at 2 petawatts (2 quadrillion watts).&nbsp;</p>



<p>At more than 100 times the global electricity power output, this huge power lasts only for the brief duration of its laser pulse—just 25 quintillionths of a second long.</p>



<p>“This milestone marks the beginning of experiments that move into unexplored territory for American high field science,” said <a href="https://ners.engin.umich.edu/people/krushelnick-karl/" target="_blank" rel="noreferrer noopener">Karl Krushelnick</a>, director of the Gérard Mourou Center for Ultrafast Optical Science, which houses ZEUS.</p>



<p>Research at ZEUS will have <a href="https://news.engin.umich.edu/2019/09/most-powerful-laser-in-the-us-to-be-built-at-michigan/">applications in medicine, national security, materials science and astrophysics, in addition to plasma science and quantum physics</a>. Supported by the U.S. National Science Foundation, ZEUS is a user facility—meaning that research teams from all over the country and internationally can submit experiment proposals that go through an independent selection process.&nbsp;</p>



<p>“One of the great things about ZEUS is it’s not just one big laser hammer, but you can split the light into multiple beams,” said <a href="https://faculty.sites.uci.edu/fdollar/lab-members/franklin-dollar/" target="_blank" rel="noreferrer noopener">Franklin Dollar</a>, professor of physics and astronomy at the University of California, Irvine, whose team is running the first user experiment at 2 petawatts. “Having a national resource like this, which awards time to users whose experimental concepts are most promising for advancing scientific priorities, is really bringing high-intensity laser science back to the U.S.”</p>



<div>




<p>Dollar’s team and the ZEUS team aim to produce electron beams with energies equivalent to those made in particle accelerators that are hundreds of meters in length.&nbsp; This would be 5-10 times higher energy than any electron beams previously produced at the ZEUS facility.</p>
</div>



<p>“We aim to reach higher electron energies using two separate laser beams—one to form a guiding channel and the other to accelerate electrons through it,” said <a href="https://cuos.engin.umich.edu/researchgroups/hfs/profiles/anatoly-maksimchuk/" target="_blank" rel="noreferrer noopener">Anatoly Maksimchuk</a>, U-M research scientist in electrical and computer engineering, who leads the development of the experimental areas.</p>



<figure><img loading="lazy" decoding="async" width="672" height="448" src="https://news.engin.umich.edu/wp-content/uploads/2025/05/ti-sapph-crystal-2PWLaser.jpg" alt="The circular glass optics and silvery metal mounts that create the laser path appear tinted pink through the round window of the crystal." srcset="https://news.engin.umich.edu/wp-content/uploads/2025/05/ti-sapph-crystal-2PWLaser.jpg 672w, https://news.engin.umich.edu/wp-content/uploads/2025/05/ti-sapph-crystal-2PWLaser-300x200.jpg 300w, https://news.engin.umich.edu/wp-content/uploads/2025/05/ti-sapph-crystal-2PWLaser-120x80.jpg 120w, https://news.engin.umich.edu/wp-content/uploads/2025/05/ti-sapph-crystal-2PWLaser-500x333.jpg 500w" sizes="auto, (max-width: 672px) 100vw, 672px"><figcaption>A view through the titanium-sapphire crystal that helps to transfer power into ZEUS’s laser pulses. At two petawatts, ZEUS is now the most powerful laser in the U.S. Photo: Marcin Szczepanski/Michigan Engineering</figcaption></figure>



<p>They hope to do this in part with a redesigned target. They lengthened the cell that holds the gas that the laser pulse rams into, helium in this experiment. This interaction produces plasma, ripping electrons off the atoms so that the gas becomes a soup of free electrons and positively charged ions. Those electrons get accelerated behind the laser pulse-like wakesurfers close behind a speedboat, a phenomenon called wakefield acceleration.&nbsp;</p>



<p>Light moves slower through plasma, enabling the electrons to catch up to it. In a less dense, longer target, the electrons spend more time accelerating before they catch up to the laser pulse, enabling them to hit higher top speeds.&nbsp;</p>



<p>This demonstration of ZEUS’s power paves the way for the signature experiment, expected later this year, when the accelerated electrons will collide with laser pulses coming the opposite way. In the moving frame of the electrons, the 3-petawatt laser pulse will seem to be a million times more powerful—a zettawatt-scale pulse. This gives ZEUS its full name of “Zettawatt Equivalent Ultrashort laser pulse System.”</p>



<p>“The fundamental research done at the NSF ZEUS facility has many possible applications, including better imaging methods for soft tissues and advancing the technology used to treat cancer and other diseases,” said Vyacheslav Lukin, program director in the NSF Division of Physics, which oversees the ZEUS project. “Scientists using the unique capabilities of ZEUS will expand the frontiers of human knowledge in new ways and provide new opportunities for American innovation and economic growth.”</p>



<p>The ZEUS facility fits in a space similar in size to a school gymnasium. At one corner of the room, a laser produces the initial infrared pulse. Optical devices called diffraction gratings stretch it out in time so that when the pump lasers dump power into the pulse, it doesn’t get so intense that it starts tearing the air apart. At its biggest, the pulse is 12 inches across and a few feet long.&nbsp;</p>



<p>After four rounds of pump lasers adding energy, the pulse enters the vacuum chambers. Another set of gratings flattens it to a 12-inch disk that is just 8 microns thick—about 10 times thinner than a piece of printer paper. Even at 12 inches across, its intensity could turn the air into plasma, but then it is focused down to 0.8 microns wide to deliver maximum intensity to the experiments.</p>



<figure><img loading="lazy" decoding="async" width="672" height="448" src="https://news.engin.umich.edu/wp-content/uploads/2025/05/laser-line-2PWLaser.jpg" alt="Two men in hairnets, beard nets, lab coats, gloves and protective eyeglasses peer through spyglass-like devices at the optics inside the laser cabinet, mostly circular and transparent devices mounted in metal stands." srcset="https://news.engin.umich.edu/wp-content/uploads/2025/05/laser-line-2PWLaser.jpg 672w, https://news.engin.umich.edu/wp-content/uploads/2025/05/laser-line-2PWLaser-300x200.jpg 300w, https://news.engin.umich.edu/wp-content/uploads/2025/05/laser-line-2PWLaser-120x80.jpg 120w, https://news.engin.umich.edu/wp-content/uploads/2025/05/laser-line-2PWLaser-500x333.jpg 500w" sizes="auto, (max-width: 672px) 100vw, 672px"><figcaption>John Nees, left, and laser engineer Richard Van Camp, right, check the alignment of the optics inside the cabinet where the amplification of the laser pulses occurs. At two petawatts, ZEUS is now the most powerful laser in the U.S. Photo: Marcin Szczepanski/Michigan Engineering</figcaption></figure>



<p>“As a midscale-sized facility, we can operate more nimbly than large-scale facilities like particle accelerators or the National Ignition Facility,” said <a href="https://cuos.engin.umich.edu/researchgroups/hfs/profiles/john-nees/" target="_blank" rel="noreferrer noopener">John Nees</a>, U-M research scientist in electrical and computer engineering, who leads the ZEUS laser construction. “This openness attracts new ideas from a broader community of scientists.”</p>



<p>The road to 2 petawatts has been slow and careful. Just getting the pieces they need to assemble the system has been harder than expected. The biggest challenge is a sapphire crystal, infused with titanium atoms. Almost 7 inches in diameter, it is the critical component of the final amplifier of the system, which brings the laser pulse to full power.</p>



<figure><img loading="lazy" decoding="async" width="672" height="448" src="https://news.engin.umich.edu/wp-content/uploads/2025/05/target-area-2PWLaser.jpg" alt="wo men in lab coats, hair nets, gloves and protective eyeglasses adjust the optics inside a circular metal tank. Metal tubes enter the tank from the sides, and more optical components are secured to a table outside." srcset="https://news.engin.umich.edu/wp-content/uploads/2025/05/target-area-2PWLaser.jpg 672w, https://news.engin.umich.edu/wp-content/uploads/2025/05/target-area-2PWLaser-300x200.jpg 300w, https://news.engin.umich.edu/wp-content/uploads/2025/05/target-area-2PWLaser-120x80.jpg 120w, https://news.engin.umich.edu/wp-content/uploads/2025/05/target-area-2PWLaser-500x333.jpg 500w" sizes="auto, (max-width: 672px) 100vw, 672px"><figcaption>John Nees (left) and laser engineer Paul Campbell (right) work in Target Area 1, where the first 2 petawatt user experiment will take place. ZEUS is now the most powerful laser in the U.S. Photo: Marcin Szczepanski/Michigan Engineering</figcaption></figure>



<p>“The crystal that we’re going to get in the summer will get us to 3 petawatts, and it took four and a half years to manufacture,” said <a href="https://eecs.engin.umich.edu/people/bayer-franko/" target="_blank" rel="noreferrer noopener">Franko Bayer</a>, project manager for ZEUS. “The size of the titanium sapphire crystal we have, there are only a few in the world.”</p>



<p>In the meantime, jumping from the 300 terawatt power of the previous HERCULES laser to just 1 petawatt on ZEUS resulted in worrying darkening of the gratings. First, they had to determine the cause: Were they permanently damaged or just darkened by carbon deposits from the powerful beam tearing up molecules floating in the imperfect vacuum chamber?&nbsp;</p>



<p>When it turned out to be carbon deposits, Nees and the laser team had to figure out how many laser shots could run safely between cleanings. If the gratings became too dark, they could distort the laser pulses in a way that damages optics further along the path.</p>



<figure><img loading="lazy" decoding="async" width="672" height="448" src="https://news.engin.umich.edu/wp-content/uploads/2025/05/burn-paper-2PWLaser.jpg" alt="A man in a hair net, beard net, lab coat and protective eyeglasses holds up a piece of paper with a 12-inch gray circular mark in its center. The gray is slightly deeper in some areas and lighter in others, as if colored in pencil over wood grain. Behind, screens show graphs visualizing characteristics of the beam." srcset="https://news.engin.umich.edu/wp-content/uploads/2025/05/burn-paper-2PWLaser.jpg 672w, https://news.engin.umich.edu/wp-content/uploads/2025/05/burn-paper-2PWLaser-300x200.jpg 300w, https://news.engin.umich.edu/wp-content/uploads/2025/05/burn-paper-2PWLaser-120x80.jpg 120w, https://news.engin.umich.edu/wp-content/uploads/2025/05/burn-paper-2PWLaser-500x333.jpg 500w" sizes="auto, (max-width: 672px) 100vw, 672px"><figcaption>Gregg Sucha, laser engineer, holds up a laser burn mark on photographic paper in a control room of the ZEUS lab. This test reveals any potentially damaging hot spots in the expanded laser pulse as it enters the compressor that will shrink it into a tiny, intense and powerful laser pulse. The lines come from imperfections in the final amplifying crystal, which must be replaced before ZEUS can reach its full power of 3 petawatts. At two petawatts, ZEUS is now the most powerful laser in the U.S. Photo: Marcin Szczepanski/Michigan Engineering</figcaption></figure>



<p>Finally, the ZEUS team has already spent a total of 15 months running user experiments since the grand opening in October 2023 because there is still plenty of science that could be done with a 1 petawatt laser. So far, it has welcomed 11 separate experiments with a total of 58 experimenters from 22 institutions, including international researchers. Over the next year—between user experiments—the ZEUS team will continue upgrading the system toward its full power.</p>



<p>Krushelnick is also the Henry J. Gomberg Collegiate Professor of Engineering and a professor of nuclear engineering and radiological sciences, electrical and computer engineering and physics.</p>
















</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Discord Unveiled: A Comprehensive Dataset of Public Communication (2015-2024) (137 pts)]]></title>
            <link>https://arxiv.org/abs/2502.00627</link>
            <guid>44052041</guid>
            <pubDate>Wed, 21 May 2025 14:45:38 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://arxiv.org/abs/2502.00627">https://arxiv.org/abs/2502.00627</a>, See on <a href="https://news.ycombinator.com/item?id=44052041">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="content-inner">
    
    
    <div><p><span>Authors:</span><a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Aquino,+Y" rel="nofollow">Yan Aquino</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Bento,+P" rel="nofollow">Pedro Bento</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Buzelin,+A" rel="nofollow">Arthur Buzelin</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Dayrell,+L" rel="nofollow">Lucas Dayrell</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Malaquias,+S" rel="nofollow">Samira Malaquias</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Santana,+C" rel="nofollow">Caio Santana</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Estanislau,+V" rel="nofollow">Victoria Estanislau</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Dutenhefner,+P" rel="nofollow">Pedro Dutenhefner</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Evangelista,+G+H+G" rel="nofollow">Guilherme H. G. Evangelista</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Porf%C3%ADrio,+L+G" rel="nofollow">Luisa G. Porfírio</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Grossi,+C+S" rel="nofollow">Caio Souza Grossi</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Rigueira,+P+B" rel="nofollow">Pedro B. Rigueira</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Almeida,+V" rel="nofollow">Virgilio Almeida</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Pappa,+G+L" rel="nofollow">Gisele L. Pappa</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Meira,+W" rel="nofollow">Wagner Meira Jr</a></p></div>            
    <p><a href="https://arxiv.org/pdf/2502.00627">View PDF</a>
    <a href="https://arxiv.org/html/2502.00627v1">HTML (experimental)</a></p><blockquote>
            <span>Abstract:</span>Discord has evolved from a gaming-focused communication tool into a versatile platform supporting diverse online communities. Despite its large user base and active public servers, academic research on Discord remains limited due to data accessibility challenges. This paper introduces Discord Unveiled: A Comprehensive Dataset of Public Communication (2015-2024), the most extensive Discord public server's data to date. The dataset comprises over 2.05 billion messages from 4.74 million users across 3,167 public servers, representing approximately 10% of servers listed in Discord's Discovery feature. Spanning from Discord's launch in 2015 to the end of 2024, it offers a robust temporal and thematic framework for analyzing decentralized moderation, community governance, information dissemination, and social dynamics. Data was collected through Discord's public API, adhering to ethical guidelines and privacy standards via anonymization techniques. Organized into structured JSON files, the dataset facilitates seamless integration with computational social science methodologies. Preliminary analyses reveal significant trends in user engagement, bot utilization, and linguistic diversity, with English predominating alongside substantial representations of Spanish, French, and Portuguese. Additionally, prevalent community themes such as social, art, music, and memes highlight Discord's expansion beyond its gaming origins.
    </blockquote>

    <!--CONTEXT-->
    
  </div><div>
      <h2>Submission history</h2><p> From: Yan Aquino Amorim [<a href="https://arxiv.org/show-email/debf74d4/2502.00627" rel="nofollow">view email</a>]      <br>    <strong>[v1]</strong>
        Sun, 2 Feb 2025 02:17:14 UTC (1,433 KB)<br>
</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Animated Factorization (243 pts)]]></title>
            <link>http://www.datapointed.net/visualizations/math/factorization/animated-diagrams/</link>
            <guid>44051958</guid>
            <pubDate>Wed, 21 May 2025 14:39:37 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="http://www.datapointed.net/visualizations/math/factorization/animated-diagrams/">http://www.datapointed.net/visualizations/math/factorization/animated-diagrams/</a>, See on <a href="https://news.ycombinator.com/item?id=44051958">Hacker News</a></p>
<div id="readability-page-1" class="page">
<div id="enchilada">



<div id="credits">
<p><a href="http://www.datapointed.net/2012/10/animated-factorization-diagrams/" onclick="window.open(this.href);return false;">About</a>
</p></div>
<canvas width="300" height="300" id="canvas"></canvas>
</div>


</div>]]></description>
        </item>
        <item>
            <title><![CDATA[Devstral (461 pts)]]></title>
            <link>https://mistral.ai/news/devstral</link>
            <guid>44051733</guid>
            <pubDate>Wed, 21 May 2025 14:21:10 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://mistral.ai/news/devstral">https://mistral.ai/news/devstral</a>, See on <a href="https://news.ycombinator.com/item?id=44051733">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p dir="ltr">Today we introduce Devstral, our agentic LLM for software engineering tasks. Devstral is built under a collaboration between Mistral AI and <a href="https://www.all-hands.dev/">All Hands AI</a> 🙌, and outperforms all open-source models on SWE-Bench Verified by a large margin. We release Devstral under the Apache 2.0 license.&nbsp;</p>
<p dir="ltr"><img src="https://cms.mistral.ai/assets/a8f418f6-f7ee-4f21-8ab8-08bd76c37186.png?width=1600&amp;height=882" alt="Devstral Swe"></p>
<h2 dir="ltr">Agentic LLMs for software development</h2>
<p dir="ltr">While typical LLMs are excellent at atomic coding tasks such as writing standalone functions or code completion, they currently struggle to solve real-world software engineering problems. Real-world development requires contextualising code within a large codebase, identifying relationships between disparate components, and identifying subtle bugs in intricate functions.&nbsp;</p>
<p dir="ltr">Devstral is designed to tackle this problem. Devstral is trained to solve real GitHub issues; it runs over code agent scaffolds such as OpenHands or SWE-Agent, which define the interface between the model and the test cases. Here, we show Devstral’s performance on the popular SWE-Bench Verified benchmark, a dataset of 500 real-world GitHub issues which have been manually screened for correctness.</p>
<p dir="ltr">Devstral achieves a score of 46.8% on SWE-Bench Verified, outperforming prior open-source SoTA models by more than 6% points. When evaluated under the same test scaffold (OpenHands, provided by <a href="https://www.all-hands.dev/">All Hands AI</a> 🙌), Devstral exceeds far larger models such as Deepseek-V3-0324 (671B) and Qwen3 232B-A22B.&nbsp;</p>
<p dir="ltr">In the table below, we also compare Devstral to closed and open models evaluated under any scaffold (including ones custom for the model). Here, we find that Devstral achieves substantially better performance than a number of closed-source alternatives. For example, Devstral surpasses the recent GPT-4.1-mini by over 20%.&nbsp;</p>


<h2 dir="ltr">Versatile: local deployment ↔️ enterprise use ↔️ copilots</h2>
<p dir="ltr">Devstral is light enough to run on a single RTX 4090 or a Mac with 32GB RAM, making it an ideal choice for local deployment and on-device use. Coding platforms such as <a href="https://github.com/All-Hands-AI/OpenHands">OpenHands</a> can allow the model to interact with local codebases and provide fast resolution to issues. To try it yourself, view the <a href="https://docs.all-hands.dev/modules/usage/llms/local-llms">documentation</a> or <a href="https://www.youtube.com/watch?v=oV9tAkS2Xic">tutorial video</a>.</p>
<p dir="ltr">The performance of the model also makes it a suitable choice for agentic coding on privacy-sensitive repositories in enterprises, especially ones subject to stringent security and compliance requirements.&nbsp;</p>
<p dir="ltr">Finally, if you’re building or using an agentic coding IDE, plugin, or environment, Devstral is a great choice to add to your model selector.&nbsp;</p>
<h2 dir="ltr">Availability</h2>
<p dir="ltr">We release this model for free under an Apache 2.0 license for the community to build on, customize, and accelerate autonomous software development. To try it for yourself, head over to our <a href="https://huggingface.co/mistralai/Devstral-Small-2505">model card</a>.&nbsp;</p>
<p dir="ltr">The model is also available on our API under the name devstral-small-2505 at the same price as Mistral Small 3.1: $0.1/M input tokens and $0.3/M output tokens.&nbsp;</p>
<p dir="ltr">Should you choose to self-deploy, you can download the model on <a href="https://huggingface.co/mistralai/Devstral-Small-2505">HuggingFace</a>, <a href="https://ollama.com/library/devstral">Ollama</a>, <a href="https://www.kaggle.com/models/mistral-ai/devstral-small-2505">Kaggle</a>, <a href="https://docs.unsloth.ai/basics/devstral">Unsloth</a>, <a href="https://lmstudio.ai/model/devstral-small-2505-MLX">LM Studio</a>&nbsp;starting today.&nbsp;</p>
<p dir="ltr">For enterprise deployments that require fine-tuning on private codebases, or higher-fidelity customization such as continued pre-training or distilling Devstral’s capabilities into other models, please <a href="https://mistral.ai/contact">contact us</a> to connect with our applied AI team.&nbsp;</p>
<h2 dir="ltr">What’s next</h2>
<p dir="ltr">Devstral is a research preview and we welcome feedback! We’re hard at work building a larger agentic coding model that will be available in the coming weeks.</p>
<p dir="ltr">Interested in discussing how we can help your team put Devstral to use, and about our portfolio of models, products and solutions? <a href="https://mistral.ai/contact">Contact us</a> and we’ll be happy to help.</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Things that have a bigger impact than coding assistants (105 pts)]]></title>
            <link>https://codemanship.wordpress.com/2025/05/21/five-boring-things-that-have-a-bigger-impact-than-a-i-coding-assistants-on-dev-team-productivity/</link>
            <guid>44050843</guid>
            <pubDate>Wed, 21 May 2025 12:39:23 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://codemanship.wordpress.com/2025/05/21/five-boring-things-that-have-a-bigger-impact-than-a-i-coding-assistants-on-dev-team-productivity/">https://codemanship.wordpress.com/2025/05/21/five-boring-things-that-have-a-bigger-impact-than-a-i-coding-assistants-on-dev-team-productivity/</a>, See on <a href="https://news.ycombinator.com/item?id=44050843">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="page">
		<p><a href="#content">
			Skip to content		</a></p><!-- .site-header -->

		<div id="content">
	<main id="main">
		
<article id="post-2175">
	<!-- .entry-header -->

	
	
	<div>
		
<figure><a href="https://codemanship.wordpress.com/wp-content/uploads/2025/05/autocomplete.png"><img data-attachment-id="2177" data-permalink="https://codemanship.wordpress.com/2025/05/21/five-boring-things-that-have-a-bigger-impact-than-a-i-coding-assistants-on-dev-team-productivity/autocomplete/" data-orig-file="https://codemanship.wordpress.com/wp-content/uploads/2025/05/autocomplete.png" data-orig-size="1240,720" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="Autocomplete" data-image-description="" data-image-caption="" data-medium-file="https://codemanship.wordpress.com/wp-content/uploads/2025/05/autocomplete.png?w=300" data-large-file="https://codemanship.wordpress.com/wp-content/uploads/2025/05/autocomplete.png?w=840" width="1024" height="594" src="https://codemanship.wordpress.com/wp-content/uploads/2025/05/autocomplete.png?w=1024" alt="" srcset="https://codemanship.wordpress.com/wp-content/uploads/2025/05/autocomplete.png?w=1024 1024w, https://codemanship.wordpress.com/wp-content/uploads/2025/05/autocomplete.png?w=150 150w, https://codemanship.wordpress.com/wp-content/uploads/2025/05/autocomplete.png?w=300 300w, https://codemanship.wordpress.com/wp-content/uploads/2025/05/autocomplete.png?w=768 768w, https://codemanship.wordpress.com/wp-content/uploads/2025/05/autocomplete.png 1240w" sizes="(max-width: 709px) 85vw, (max-width: 909px) 67vw, (max-width: 1362px) 62vw, 840px"></a></figure>



<p>Here are 5 factors that make a bigger difference to software development outcomes than “A.I.” coding assistants, but teams don’t address because they’re “old news, granddad!”</p>



<ul>
<li>Smaller teams are better value/$ spent</li>



<li>More frequent releases accelerate learning what has real value</li>



<li>Limiting work in progress – solving one problem at a time – increases delivery throughput</li>



<li>Cross-functional teams experience fewer bottlenecks and blockers than specialised teams</li>



<li>Empowered, self-organising teams spend less time waiting for decisions and more time getting sh*t done</li>
</ul>



<p>Now, I appreciate that every one of these is a can of worms that many organisations simply do not wish to open. They all have deep implications, and require foundational changes not just to the way we work, but the way we <em>think</em>.</p>



<p>For example, smaller, more frequent releases implies software’s in a shippable state more often, which implies faster build &amp; test cycles… and down the rabbit hole we go: into testing pyramids and separation of concerns and micro-cycles with continuous testing, continuous integration, continuous code review and… Come to think of it, <a href="https://codemanship.co.uk/">the stuff I teach</a> 🙂</p>



<p>Another example, empowering teams requires a pretty high level of psychological safety. When people are afraid to fail, they’re afraid to <em>try </em>– to make calls, to take initiative, to just f-ing do it! The culture of an organisation, which may have evolved over many years, is a hard thing to reshape. There’s often a lot of unspoken rules – sure, you <em>say </em>your door is always open, but… It takes much work and many iterations to shift those underlying patterns in the way we interact.</p>



<p>But waiting on the other side of that long journey is a high capability to rapidly and sustainably create and adapt working software that meets rapidly-changing business needs. Software agility Nirvana.</p>



<p>We already know from the data (e.g., DORA) that “A.I.” coding assistants don’t unlock that door.</p>

<div>
	<p><img referrerpolicy="no-referrer" alt="Unknown's avatar" src="https://1.gravatar.com/avatar/aadcfbd208e65a0829f5d259192a5886454439a708e425500c59c535e28ae081?s=42&amp;d=identicon&amp;r=G" srcset="https://1.gravatar.com/avatar/aadcfbd208e65a0829f5d259192a5886454439a708e425500c59c535e28ae081?s=42&amp;d=identicon&amp;r=G 1x, https://1.gravatar.com/avatar/aadcfbd208e65a0829f5d259192a5886454439a708e425500c59c535e28ae081?s=63&amp;d=identicon&amp;r=G 1.5x, https://1.gravatar.com/avatar/aadcfbd208e65a0829f5d259192a5886454439a708e425500c59c535e28ae081?s=84&amp;d=identicon&amp;r=G 2x, https://1.gravatar.com/avatar/aadcfbd208e65a0829f5d259192a5886454439a708e425500c59c535e28ae081?s=126&amp;d=identicon&amp;r=G 3x, https://1.gravatar.com/avatar/aadcfbd208e65a0829f5d259192a5886454439a708e425500c59c535e28ae081?s=168&amp;d=identicon&amp;r=G 4x" height="42" width="42" loading="lazy" decoding="async">	</p><!-- .author-avatar -->

	<!-- .author-description -->
</div><!-- .author-info -->
	</div><!-- .entry-content -->

	<!-- .entry-footer -->
</article><!-- #post-2175 -->

<!-- .comments-area -->

	<nav aria-label="Posts">
		<h2>Post navigation</h2>
		
	</nav>
	</main><!-- .site-main -->

	
</div><!-- .site-content -->

		<!-- .site-footer -->
	</div></div>]]></description>
        </item>
    </channel>
</rss>