<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0">
    <channel>
        <title>HN100 - Readable Contents</title>
        <link>https://hn.algolia.com/api/v1/search_by_date?tags=%28story,poll%29&amp;numericFilters=points%3E100</link>
        <description>Uses Readability to add bodies to the RSS feed</description>
        <lastBuildDate>Mon, 23 Dec 2024 01:30:01 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>en</language>
        <item>
            <title><![CDATA[German watchdog orders Sam Altman's biometric ID project World to delete data (108 pts)]]></title>
            <link>https://www.euronews.com/next/2024/12/19/german-watchdog-orders-sam-altmans-biometric-id-project-world-to-delete-data</link>
            <guid>42489072</guid>
            <pubDate>Sun, 22 Dec 2024 21:02:34 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.euronews.com/next/2024/12/19/german-watchdog-orders-sam-altmans-biometric-id-project-world-to-delete-data">https://www.euronews.com/next/2024/12/19/german-watchdog-orders-sam-altmans-biometric-id-project-world-to-delete-data</a>, See on <a href="https://news.ycombinator.com/item?id=42489072">Hacker News</a></p>
&lt;Unparsable&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[Twtxt is a decentralised, minimalist microblogging service for hackers (132 pts)]]></title>
            <link>https://twtxt.readthedocs.io/en/latest/index.html</link>
            <guid>42488983</guid>
            <pubDate>Sun, 22 Dec 2024 20:51:44 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://twtxt.readthedocs.io/en/latest/index.html">https://twtxt.readthedocs.io/en/latest/index.html</a>, See on <a href="https://news.ycombinator.com/item?id=42488983">Hacker News</a></p>
Couldn't get https://twtxt.readthedocs.io/en/latest/index.html: Error: timeout of 10000ms exceeded]]></description>
        </item>
        <item>
            <title><![CDATA[The Jujutsu version control system (108 pts)]]></title>
            <link>https://neugierig.org/software/blog/2024/12/jujutsu.html</link>
            <guid>42488112</guid>
            <pubDate>Sun, 22 Dec 2024 18:29:23 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://neugierig.org/software/blog/2024/12/jujutsu.html">https://neugierig.org/software/blog/2024/12/jujutsu.html</a>, See on <a href="https://news.ycombinator.com/item?id=42488112">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>



<p><a href="https://github.com/martinvonz/jj">Jujutsu</a> is a new version control system that
seems pretty nice!</p>
<p>The first few times I tried it I bounced off
<a href="https://martinvonz.github.io/jj/latest/tutorial/">the docs</a>, which to my taste
has too much detail up front before I got the big picture. Someone else maybe
had a similar experience and wrote an
<a href="https://steveklabnik.github.io/jujutsu-tutorial/">alternative tutorial</a> but
it's in a rambly bloggy style that is also too focused on the commands for me.</p>
<p>I suspect, much like writing a monad tutorial, the path of understanding is
actually writing it down. So here's an attempt from me at an introduction /
tutorial.</p>
<p>Perhaps unlike the others, my goal is that this is high-level enough to read and
think about, without providing so much detail that it washes over you. Don't try
to memorize the commands here or anything, they're just here to communicate the
ideas. At the end if you're curious to try I recommend the docs found on their
website.</p>
<h2>Overview</h2>
<p>Omitting details, you can think of Jujutsu (hereafter "jj") as a new Git
frontend. The underlying data is still stored in Git. The difference is how you
interact with your files locally, with a different conceptual model and a
different set of commands.</p>
<p>Git quiz: are commits snapshots of file state or diffs? The technical answer is
subtle — as a user you usually interact with them as diffs, while conceptually
they are snapshots, but concretely they are stored as deltas. The more useful
answer is that thinking about the details obfuscates the conceptual model.
Similarly, to describe jj in terms of what happens in Git is tempting but I
think ultimately clouds the explanation.</p>
<p>In practice what this means is try to put your knowledge of Git on hold, but
also be aware you can use jj and continue to interoperate with the larger Git
ecosystem, including e.g. pushing to GitHub.</p>
<h2>The big idea: everything is commits</h2>
<p>The purpose of a version control system is to keep track of the history of your
code. But interestingly in most, as soon as you edit a file locally in your
working copy, that new history ("I have edited file X starting on version Y") is
is in a kind of limbo state outside of the system and managed separately.</p>
<p>This is so pervasive it's almost difficult to see. But consider how a command
like <code>git diff</code> has one mode that takes two commits to diff, and then a bunch of
other modes and flags to operate on the other kinds of things it tracks. You can
get a diff against your working copy but there's no way to name "the working
copy" in the diff command. (Git in particular adds the additional
not-quite-a-commit state of the index, with even more flavors of attendant
commands. The ultimate Git quiz: what are the different soft/hard/mixed
behaviors of <code>git reset</code>?)</p>
<p>Another example: consider how if you have a working copy change and and want to
check out other code you either have to put it in a new place (<code>git stash</code>, a
fourth place separate from the others) or make a temporary commit. Or how if you
have a working copy change that you want to transplant elsewhere you
<code>git checkout -m</code>, but to move around committed changes it's <code>git rebase</code>.</p>
<p>In jj, in contrast, your working copy state is always a commit. When making a
new change this is a new (descriptionless) commit. Any edit you make on disk is
immediately reflected in the current commit.</p>
<p>So many things fall out of this simple decision!</p>
<ul>
<li>
<p><code>jj diff</code> shows a commit's diff. With no argument it's the current commit's
diff, i.e. your current working copy's diff; otherwise you can specify which
historical one you want. Many other jj commands similarly have a pleasing
symmetry about their behavior like this.</p>
</li>
<li>
<p>You can draft the commit message for a work in progress commit before you're
done, using the same command you'd use to edit any other commit message. There
is no final <code>jj commit</code> command, the commit is implicit. (Instead you <code>jj new</code>
to start a new empty commit when done.)</p>
</li>
<li>
<p>You never need to "stash" your current work to go do something else, it is
already stored in the current commit, and easy to jump back to.</p>
</li>
<li>
<p>In Git, to fix a typo in an old commit, you might make a new commit then
<code>git rebase -i</code> to move the patch around. In jj you directly check out the old
commit (because working copy == commit) and edit the files with no further
commands.
(<a href="https://lottia.net/notes/0013-git-jujutsu-miniature.html">This blog post</a>
walks through a real-world operation like this with Git and jj side by side.)</p>
</li>
</ul>
<p>From a Git perspective, jj is very "rebasey". Editing a file is like a
<code>git commit --amend</code>, and in the "fix a typo" move above the edit implictly
rebases any downstream commits. To make that work out there are some other
conceptual leaps around conflict handling and branches that will come below
after the basics.</p>
<h2>The basic workflow</h2>
<p>In a Git repo:</p>
<pre><code>$ jj git init --colocate
</code></pre>
<p>This creates a <code>.jj</code> dir that works with the Git repo. Git commands will still
work but can be confusing.</p>
<p>The plain <code>jj</code> command runs <code>jj log</code>, showing recent commits. Here it is from
the repo for this blog:</p>
<pre><code>$ jj       
@  zyqszntn evan.martin@gmail.com 2024-12-12 11:58:52 21b06db8
│  (no description set)
○  pmnzyyru evan.martin@gmail.com 2024-12-12 11:58:48 86355427
│  unfinished drafts
◆  szzpmvlz evan.martin@gmail.com 2024-09-18 09:08:15 fcb1507d
│  syscalls
~
</code></pre>
<p>The leftmost letter string is the "change id", which is the identifier you use
to refer to the change in a command like <code>diff</code>. They are stable across edits,
unlike the Git hashes on the right. In the terminal the change ids are colored
to show the necessary prefix to uniquely refer to them (a single letter) in
commands.</p>
<p>The topmost commit <code>zyqszntn</code> is the current one, containing this blog post as I
write it. As you would expect, if I run <code>jj status</code> it shows me the list of
edited files, and if I run <code>jj diff</code> it shows me a diff.</p>
<p>I can give it a description now or when I'm done:</p>
<pre><code>$ jj desc -m 'post about jujutsu'
</code></pre>
<p>And then create a new commit for the next change:</p>
<pre><code>$ jj new
</code></pre>
<h2>Iterating on changes</h2>
<p>That's enough for trivial changes, but often I work on more significant changes
where I might lose context across days. There are two ways you might do this
depending on how you work.</p>
<p>The first is to just describe your change as above and keep on editing it,
without running <code>jj new</code>. Each subsequent edit will update the change as you go.
This is simple to operate but it means <code>jj diff</code> will always show the whole
diff. In Git this is similar to just keeping a lot of edits in your working
copy.</p>
<p>The other option is called
"<a href="https://steveklabnik.github.io/jujutsu-tutorial/real-world-workflows/the-squash-workflow.html">the squash workflow</a>"
in the tutorial book. In this, when you do new work you <code>jj new</code> to create a new
distinct commit from your existing work, and when you are happy with it (by e.g.
examining <code>jj diff</code>, which shows you just the working copy's new changes) you
run <code>jj squash</code> to flush these new changes into the previous commit. To me this
feels pretty analogous to using the Git index as a staging area for a complex
change, or perhaps repeatedly using <code>git commit --amend</code>.</p>
<h2>Moving around and editing history</h2>
<p>These commands like <code>jj diff</code> and <code>jj desc</code> work on the current commit (or any
explicitly requested via the <code>-r flag</code>).</p>
<p>To switch the working copy to an existing change, it's <code>jj edit &lt;changeid&gt;</code>.
Again, any changes you make here, to the files or descriptions, or by making new
changes and squashing them, work directly on the historical commit you are
editing. I repeat this because it is both weird and obvious in retrospect.</p>
<h2>Conflicts</h2>
<p>Any operations on history cause implicit rebases that happen silently. Rebases
can conflict. jj has interesting handling of how this works.</p>
<p>In Git, rebase resolution happens through your working copy, so there is again
extra state around "rebase in progress" and <code>git rebase --continue</code>. In jj
instead, conflicting commits are just recorded as conflicting and marked as such
in the history, so rebases always "succeed" even if they produce a string of
conflicting commits.</p>
<p>If you go to fix a conflicting commit (via <code>jj edit</code> as above), you edit the
files as usual and once the conflict markers are removed it's no longer
considered conflicting.</p>
<p>As usual, once you make a history edit, downstream changes are again rebased,
possibly resolving their conflicted state after your edit. Again, the jj pattern
of "all of the relevant information is modeled in the commits" without having a
separate rebase mode with state etc. is a recurring powerful theme.</p>
<p>I don't have a lot of experience with this yet so I can't comment on how well it
works, except that the times I've ran into it I was pleasantly surprised. The jj
docs seem proud of the modeling and behavior here which makes me think it's
plausibly sophisticated.</p>
<h2>Branches</h2>
<p>jj doesn't have named branches, but rather only keeps track of commits. Because
of the way jj juggles commits, where it's trivial to start adding commits at
random points in history, branch names are not as useful. In my experience so
far having useful commit descriptions is enough to keep track of what I'm
working on. Coming from Git the lack of named branches is surprising, but I
believe this is comfortable for Mercurial users and Monotone worked similarly (I
think?).</p>
<p>It's worth highlighting the absence of branches because in particular when
interoperating with Git you still do need branches, if only to push. There is
support in jj for this (where "bookmarks" are pointers to specific commits) but
it feels a little clunky. On the other hand, I probably have Stockholm syndrome
about the <code>git push</code> syntax.</p>
<h2>What's missing: VSCode</h2>
<p>Working with jj made me realize how much I rely on VSCode's Git support, for
both viewing diffs and for merges.</p>
<p>When editing a given commit in jj, Git thinks all the files in the commit are in
the working tree and not the index. In other words, in the VSCode UI the current
diff shows up as pending changes just as they would in Git. This works pretty
well and is about all I would expect. I haven't yet touched the buttons that
interact with Git's index, for fear of what jj will do with it.</p>
<p>For technical reasons I do not quite understand — possibly VSCode only does
three-way file merges and jj needs three-way directory merges? — the two do not
quite cooperate for resolving conflicts. The jj docs recommend meld and I have
used meld in the past but I hadn't quite realized how VSCode had hooked me until
I missed using it for a merge.</p>
<h2>The future</h2>
<p>The author of jj works at Google and is possibly making it for the Google
internal version control system. (Above I wrote that jj is a Git frontend, but
officially it has pluggable backends; I'm just unlikely to ever see a non-Git
one.)</p>
<p>When I
<a href="https://neugierig.org/software/blog/2021/07/leaving-google.html">left Google three years ago</a>
I recall they were trying to figure out what to do about either making Git
scale, or adopting Mercurial, or what. I remember talking to someone involved in
this area and thinking "realistically your users have to use Git to work with
the larger world, so anything else you do is pure cost". I found
<a href="https://ahal.ca/blog/2024/jujutsu-mercurial-haven/">this post from a Mercurial fan</a>
about jj an interesting read in how it talks about Mercurial shortcomings it
fixes. From that perspective it is pretty interesting: it can replace the places
you currently use Git, while also providing a superior UI.</p>
<p>In all, jj seems pretty polished, has been around for years, and has a pretty
simple exit strategy if things go wrong — just bail out to the Git repo. I aim
to continue using it.</p>
<p>PS: every time I read the name "jujutsu" I kept thinking it was a misspelling of
"jiu-jitsu", the martial art. But the Japanese word is じゅうじゅつ, it's
actually it's jiu-jitsu that is misspelled!
<a href="https://www.saferclimbing.org/en/article_misc/jiu-jitsu-or-jujutsu-romanisation-of-japanese">Read a longer article about it</a>.</p>

</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[The Risk of Cancer Fades as We Get Older, and We May Know Why (153 pts)]]></title>
            <link>https://www.sciencealert.com/the-risk-of-cancer-fades-as-we-get-older-and-we-may-finally-know-why</link>
            <guid>42487301</guid>
            <pubDate>Sun, 22 Dec 2024 16:25:00 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.sciencealert.com/the-risk-of-cancer-fades-as-we-get-older-and-we-may-finally-know-why">https://www.sciencealert.com/the-risk-of-cancer-fades-as-we-get-older-and-we-may-finally-know-why</a>, See on <a href="https://news.ycombinator.com/item?id=42487301">Hacker News</a></p>
Couldn't get https://www.sciencealert.com/the-risk-of-cancer-fades-as-we-get-older-and-we-may-finally-know-why: Error: timeout of 10000ms exceeded]]></description>
        </item>
        <item>
            <title><![CDATA[How bloom filters made SQLite 10x faster (194 pts)]]></title>
            <link>https://avi.im/blag/2024/sqlite-past-present-future/</link>
            <guid>42486610</guid>
            <pubDate>Sun, 22 Dec 2024 14:44:30 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://avi.im/blag/2024/sqlite-past-present-future/">https://avi.im/blag/2024/sqlite-past-present-future/</a>, See on <a href="https://news.ycombinator.com/item?id=42486610">Hacker News</a></p>
Couldn't get https://avi.im/blag/2024/sqlite-past-present-future/: Error: timeout of 10000ms exceeded]]></description>
        </item>
        <item>
            <title><![CDATA[GPT-5 is behind schedule (198 pts)]]></title>
            <link>https://www.wsj.com/tech/ai/openai-gpt5-orion-delays-639e7693</link>
            <guid>42485938</guid>
            <pubDate>Sun, 22 Dec 2024 12:29:33 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.wsj.com/tech/ai/openai-gpt5-orion-delays-639e7693">https://www.wsj.com/tech/ai/openai-gpt5-orion-delays-639e7693</a>, See on <a href="https://news.ycombinator.com/item?id=42485938">Hacker News</a></p>
Couldn't get https://www.wsj.com/tech/ai/openai-gpt5-orion-delays-639e7693: Error: timeout of 10000ms exceeded]]></description>
        </item>
        <item>
            <title><![CDATA[Decoding the telephony signals in Pink Floyd's 'The Wall' (258 pts)]]></title>
            <link>https://corelatus.com/blog/Decoding_the_telephony_signals_in_Pink_Floyd_s__The_Wall_.html</link>
            <guid>42485795</guid>
            <pubDate>Sun, 22 Dec 2024 11:49:12 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://corelatus.com/blog/Decoding_the_telephony_signals_in_Pink_Floyd_s__The_Wall_.html">https://corelatus.com/blog/Decoding_the_telephony_signals_in_Pink_Floyd_s__The_Wall_.html</a>, See on <a href="https://news.ycombinator.com/item?id=42485795">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="articles">

<h2>
Decoding the telephony signals in Pink Floyd's 'The Wall'</h2>
<p>
<em>Posted December 4th 2024</em></p>
<p>
  I like puzzles. Recently, someone asked me to identify the telephone
  network signalling in <em>The Wall</em>, a 1982 film featuring Pink
  Floyd. The signalling is audible when the main character, Pink,
  calls London from a payphone in Los Angeles,
  in <a href="https://youtu.be/yvG3WPYAXHM?si=ISfv0O_w2BC3v2ZI&amp;t=2162">this
    scene</a> (Youtube).
</p>

<p>Here's a five second audio clip from when Pink calls:</p>

<p>
  <audio src="https://corelatus.com/blog/static/2024_12_pinkfloyd.wav" controls="">
    (HTML5 audio player isn't supported by your browser)
  </audio>
</p>

<!--- =========================================================== --->
<h2>What's in the clip?</h2>

<p>
  The clip starts with some speech overlapping a dial-tone which in
  turn overlaps some rapid tone combinations, a ring tone and some
  pops, clicks and music. It ends with an answer tone.
</p>

<p>
  The most characteristic part is the telephone number encoded in the
  rapid tone combinations. Around 1980, when the film was made,
  different parts of the world used similar, but incompatible,
  tone-based signalling schemes. They were all based on the same idea:
  there are six or eight possible tones, and each digit is represented
  by a combination of two tones.
</p>

<h2>Let's examine a spectrogram</h2>

<p>
  <em>SoX</em>, an audio editing tool for PCs, can make charts that
  show the spectral components of the audio over time. The horizontal
  axis represents time, the vertical axis frequency, and darker
  sections show more audio power, and lighter sections less.
</p>

<p><img src="https://corelatus.com/blog/static/2024_12_spectrogram.png" alt="spectrogram of signalling">
</p>

<p>
  Signalling tones appear as dark horizontal lines in the spectrogram,
  with the digit signalling visible from 0.7 to 1.8 seconds. That part
  of the signalling has tones at roughly <b>700, 900, 1100, 1300, 1500
  and 1700 Hz</b>.
</p>

<h2>Which signalling standards were in common use?</h2>

<h3>DTMF (ITU-T Q.23 and Q.24)</h3>
<p>
  Everyone's heard DTMF (Dual Tone Multi Frequency). It's the sound
  your phone makes when you interact with one of those "<em>Press 1 if
  you are a new customer. Press 2 if you have a billing enquiry. Press
  3...</em>" systems. DTMF is still used by many fixed-line telephones
  to set up a call.
</p>

<p>
  In DTMF, each digit is encoded by playing a "high" tone and a "low"
  tone. The low ones can be 697, 770, 852 or 941 Hz.  The high ones
  1209, 1336, 1477 and 1633 Hz.
</p>

<p>None of the pairs in the audio match this, so <b>it's not
    DTMF</b>. Here's an audio clip of what it <em>would</em> sound like
  if we used DTMF signalling for the same number, with about the same
  speed of tones:
</p>

<p>
  <audio src="https://corelatus.com/blog/static/2024_12_dtmf.wav" controls="">
    (HTML5 audio player isn't supported by your browser)
  </audio>
</p>

<h3>CAS R2 (ITU-T Q.400—490)</h3>

<p>
  CAS R2 uses a two-out-of-six tone scheme with the frequencies 1380,
  1500, 1620, 1740, 1860 and 1980 Hz for one call direction and 1140,
  1020, 900, 780, 660 and 540 Hz for the other.  None of these are a
  good match for the tones we heard. Besides, Pink is in the USA, and
  the USA did not use CAS R2, so <b>it's not CAS</b>.
</p>

<p>This is what the digit signalling would have sounded like if
  CAS were used:</p>



<h3>SS5 (ITU-T Q.153 and Q.154)</h3>
<p>
  SS5 also uses a two-out-of-six scheme with the frequencies 700, 900,
  1100, 1300, 1500 and 1700 Hz.  This matches most of what we can
  hear, and SS5 is the signalling system most likely used for a call
  from the USA to the UK in the early 1980s.
</p>

<p>This is what the digit signalling sounds like in SS5, when
re-generated to get rid of all the other sounds:</p>

<p>
  <audio src="https://corelatus.com/blog/static/2024_12_ss5.wav" controls="true">
    (HTML5 audio player isn't supported by your browser)
  </audio>
  <br>
</p>

<h3>SS7 (ITU-T Q.703—)</h3>

<p>
  <b>It can't be SS7</b>. Signalling system No. 7 (SS7) doesn't use
  tones at all; it's all digital. SS7 is carried separately from the
  audio channel, so it can't be heard by callers. SS7 wasn't in
  common use until later in the 1980s.
</p>

<h3>Comparing spectrograms</h3>

<p>
  I made a spectrogram which combines all three signalling types on
  the same chart. The difference between DTMF and SS5 is subtle, but
  recognisable. CAS is obviously different.
</p>

<p><img src="https://corelatus.com/blog/static/2024_12_synthetic_spectrogram.png" alt="spectrogram of signalling">
</p>

<h2>Let's feed the audio to some telecom hardware</h2>

  <p>
    I injected the audio file into a timeslot of an E1 line, connected
    it to Corelatus' hardware and started an <em>ss5_registersig_monitor</em>.
  </p>

<p>
  The input audio has a lot of noise in addition to the signalling,
  but these protocols are robust enough for the digital filters in the
  hardware to be able to decode and timestamp the dialled digits
  anyway. Now, we know that the number signalling we hear was <b>044
  1831</b>. The next step is to analyse the frequencies present at the
  start time for each tone. I re-analysed the audio file
  with <em>SoX</em>, which did an FFT on snippets of the audio to find
  the actual tone frequencies at the times there were tones, like
  this:
</p>

<pre>    sox input.wav -n trim 0.700 0.060 stat -freq
</pre>

<p>The results are:</p>

  <table>
    <tbody><tr><th>Time</th><th>Frequencies</th><th>Interpretation</th></tr>
    <tr><td>0—1200 ms</td><td>483 Hz</td><td>dial tone</td>
    </tr><tr><td>729 </td><td>1105 + 1710</td><td>KP1 (start)</td>
    </tr><tr><td>891</td><td>1304 + 1507</td><td>0</td>
    </tr><tr><td>999</td><td>1306 + 703</td><td>4</td>
    </tr><tr><td>1107</td><td>1306 + 701</td><td>4</td>
    </tr><tr><td>1215</td><td>703 + 888</td><td>1</td>
    </tr><tr><td>1269</td><td>902 + 1503</td><td>8</td>
    </tr><tr><td>1377</td><td>902 + 1101</td><td>3</td>
    </tr><tr><td>1566</td><td>701 + 900</td><td>1</td>
    </tr><tr><td>1674</td><td>1501 + 1705</td><td>KF (stop)</td>
    </tr><tr><td>3800</td><td>2418 </td><td>Answer tone</td>
  </tr></tbody></table>

<p>
  At this point, I'm certain <b>the signalling is SS5</b>. It uses the
  correct frequencies to transmit digits. It uses the correct digit
  timing. It obeys the SS5 rules for having KP1 before the digits and
  KF after the digits. It uses a tone close to 2400 Hz to indicate
  that the call was answered.
</p>


<p>
  I've also listed the dial tone at the beginning, and the 2400 Hz
  <em>seizing</em> tone at the end. SS5 also uses a 2600 Hz tone,
  which is infamous for its use in blue box phreaking (telephone
  fraud) in the 1980s.
</p>

<h2>How was the film's audio made?</h2>

<p>
  My best guess is that, at the time the film was made, callers could
  hear the inter-exchange signalling during operator-assisted calls in
  the US. That would have allowed the sound engineer to record a real
  telephone in the US and accurately capture the feeling of a
  long-distance call. The number itself was probably made-up: it's too
  short and the area code doesn't seem valid.
</p>

<p>The audio was then cut and mixed to make the dial tone overlap the
  signalling. It sounds better that way and fits the scene's timing.
</p>


<h2>Addendum, 18. December 2024: the audio also appears in 'Young Lust'</h2>

<p>
  It turns out that an extended version of the same phone call appears
  near the end of 'Young Lust', a track on the album 'The Wall'. Other
  engineers with actual experience of 1970s telephone networks have
  <a href="https://telephoneworld.org/landline-telephone-history/pink-floyds-young-lust-explained-and-demystified/"> also analysed the signalling
  </a> in an interesting article with a host of details and background
  I didn't know about, including the likely names of the people in the
  call.
</p>

<p>
  It's nice to know that I got the digit decoding right, we both
  concluded it was 044 1831. One surprise is that the number called is
  probably a shortened real number in London, rather than a completely
  fabricated one as I suspected earlier. Most likely, several digits
  between the '1' and the '8' are cut out. Keith Monahan's analysis
  noted a <em>very ugly splice point</em> there, whereas I only
  briefly wondered why the digit start times are fairly regular for
  all digits except that the '8' starts early and the final '1' starts
  late.
</p>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Stopping by Woods on a Snowy Evening (193 pts)]]></title>
            <link>https://poets.org/poem/stopping-woods-snowy-evening</link>
            <guid>42485689</guid>
            <pubDate>Sun, 22 Dec 2024 11:21:04 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://poets.org/poem/stopping-woods-snowy-evening">https://poets.org/poem/stopping-woods-snowy-evening</a>, See on <a href="https://news.ycombinator.com/item?id=42485689">Hacker News</a></p>
Couldn't get https://poets.org/poem/stopping-woods-snowy-evening: Error: timeout of 10000ms exceeded]]></description>
        </item>
        <item>
            <title><![CDATA[We use our own hardware at Fastmail (757 pts)]]></title>
            <link>https://www.fastmail.com/blog/why-we-use-our-own-hardware/</link>
            <guid>42485124</guid>
            <pubDate>Sun, 22 Dec 2024 08:36:31 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.fastmail.com/blog/why-we-use-our-own-hardware/">https://www.fastmail.com/blog/why-we-use-our-own-hardware/</a>, See on <a href="https://news.ycombinator.com/item?id=42485124">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-pagefind-body="" data-cms-edit="content"> <p>This is the twenty-second post in the <a href="https://www.fastmail.com/blog/fastmail-advent-2024/">Fastmail Advent 2024</a> series. The previous post was <a href="https://www.fastmail.com/blog/fastmail-in-a-box/">Dec 21: Fastmail In A Box</a>. Check back tomorrow for another post.</p> <h2 id="why-we-use-our-own-hardware" tabindex="-1">Why we use our own hardware</h2> <p>There has recently been talk of <a href="https://www.google.com/search?q=Cloud+Repatriation" target="_blank" rel="noopener">cloud repatriation</a> where companies are moving from the cloud to on premises, with some particularly <a href="https://basecamp.com/cloud-exit" target="_blank" rel="noopener">noisy examples</a>.</p> <p>Fastmail has a long history of using our <a href="https://www.fastmail.com/blog/standalone-mail-servers/" target="_blank" rel="noopener">own</a> <a href="https://www.fastmail.com/blog/getting-the-most-out-of-hardware/" target="_blank" rel="noopener">hardware</a>. We have over two decades of experience running and optimising our systems to use our own <a href="https://en.wikipedia.org/wiki/Bare-metal_server" target="_blank" rel="noopener">bare metal</a> servers efficiently.</p> <p>We get way better cost optimisation compared to moving everything to the cloud because:</p> <ol> <li>We understand our short, medium and long term usage patterns, requirements and growth very well. This means we can plan our hardware purchases ahead of time and don’t need the fast dynamic scaling that cloud provides.</li> <li>We have in house operations experience installing, configuring and running our own hardware and networking. These are skills we’ve had to maintain and grow in house since we’ve been doing this for 25 years.</li> <li>We are able to use our hardware for long periods. We find our hardware can provide useful life for anywhere from 5-10 years depending on what it is and when in the global technology cycle it was bought, meaning we can amortise and depreciate the cost of any hardware over many years.</li> </ol> <p>Yes, that means we have to do more ourselves, including planning, choosing, buying, installing, etc, but the tradeoff for us has and we believe continues to be significantly worth it.</p> <h2 id="hardware-over-the-years" tabindex="-1">Hardware over the years</h2> <p>Of course over the 25 years we’ve been running Fastmail we’ve been through a number of hardware changes. For many years, our IMAP server storage platform was a combination of <a href="https://www.urbandictionary.com/define.php?term=Spinning%20Rust" target="_blank" rel="noopener">spinning rust</a> drives and <a href="https://www.areca.com.tw/" target="_blank" rel="noopener">ARECA RAID controllers</a>. We tended to use faster 15k RPM SAS drives in <a href="https://en.wikipedia.org/wiki/Standard_RAID_levels#RAID_1" target="_blank" rel="noopener">RAID1</a> for our hot meta data, and 7.2k RPM SATA drives in <a href="https://en.wikipedia.org/wiki/Standard_RAID_levels#RAID_6" target="_blank" rel="noopener">RAID6</a> for our main email blob data.</p> <p>In fact it was slightly more complex than this. Email blobs were written to the fast RAID1 SAS volumes on delivery, but then a separate archiving process would move them to the SATA volumes at low server activity times. Support for all of this had been added into <a href="https://www.cyrusimap.org/" target="_blank" rel="noopener">cyrus</a> and our tooling over the years in the form of separate “meta”, “data” and <a href="https://www.cyrusimap.org/3.8/imap/reference/admin/locations/archive-partitions.html" target="_blank" rel="noopener">“archive”</a> partitions.</p> <h2 id="moving-to-nv-me-ss-ds" tabindex="-1">Moving to NVMe SSDs</h2> <p>A few years ago however we made our biggest hardware upgrade ever. We moved all our email servers to a new <a href="https://www.supermicro.com/en/aplus/system/2u/2113/as-2113s-wn24rt.cfm" target="_blank" rel="noopener">2U AMD platform</a> with pure <a href="https://www.solidigm.com/products/data-center.html" target="_blank" rel="noopener">NVMe SSDs</a>. The density increase (24 x 2.5" NVMe drives vs 12 x 3.5" SATA drives per 2U) and performance increase was enormous. We found that these new servers performed even better than our initial expectations.</p> <p>At the time we upgraded however NVMe RAID controllers weren’t widely available. So we had to decide on how to handle redundancy. We considered a RAID-less setup using raw SSDs drives on each machine with synchronous application level replication to other machines, but the software changes required were going to be more complex than expected.</p> <p>We were looking at using classic Linux <a href="https://en.wikipedia.org/wiki/Mdadm" target="_blank" rel="noopener">mdadm RAID</a>, but the <a href="https://en.wikipedia.org/wiki/RAID#Atomicity" target="_blank" rel="noopener">write hole</a> was a concern and the <a href="https://docs.kernel.org/driver-api/md/raid5-cache.html" target="_blank" rel="noopener">write cache</a> didn’t seem well tested at the time.</p> <p>We decided to have a look at <a href="https://arstechnica.com/information-technology/2020/05/zfs-101-understanding-zfs-storage-and-performance/" target="_blank" rel="noopener">ZFS</a> and at least test it out.</p> <p>Despite some of the cyrus on disk database structures being fairly hostile to <a href="https://en.wikipedia.org/wiki/ZFS#Copy-on-write_transactional_model" target="_blank" rel="noopener">ZFS Copy-on-write</a> semantics, they were still incredibly fast at all the IO we threw at them. And there were some other wins as well.</p> <h2 id="zfs-compression-and-tuning" tabindex="-1">ZFS compression and tuning</h2> <p>When we rolled out ZFS for our email servers we also enabled <a href="https://freebsdfoundation.org/wp-content/uploads/2021/05/Zstandard-Compression-in-OpenZFS.pdf" target="_blank" rel="noopener">transparent Zstandard compression</a>. This has worked very well for us, saving about 40% space on all our email data.</p> <p>We’ve also recently done some additional calculations to see if we could tune some of the parameters better. We sampled 1 million emails at random and calculated how many blocks would be required to store those emails uncompressed, and then with <a href="https://klarasystems.com/articles/tuning-recordsize-in-openzfs/" target="_blank" rel="noopener">ZFS record sizes</a> of 32k, 128k or 512k and zstd-3 or zstd-9 compression options. Although ZFS <a href="https://en.wikipedia.org/wiki/ZFS#ZFS's_approach:_RAID-Z_and_mirroring" target="_blank" rel="noopener">RAIDz2</a> seems conceptually similar to classic RAID6, the way it <a href="https://ibug.io/blog/2023/10/zfs-block-size/" target="_blank" rel="noopener">actually stores blocks of data</a> is quite different and so you have to take into account volblocksize, how files are split into logical recordsize blocks, and number of drives when doing calculations.</p> <pre><code>               Emails: 1,026,000
           Raw blocks: 34,140,142
 32k &amp; zstd-3, blocks: 23,004,447 = 32.6% saving
 32k &amp; zstd-9, blocks: 22,721,178 = 33.4% saving
128k &amp; zstd-3, blocks: 20,512,759 = 39.9% saving
128k &amp; zstd-9, blocks: 20,261,445 = 40.7% saving
512k &amp; zstd-3, blocks: 19,917,418 = 41.7% saving
512k &amp; zstd-9, blocks: 19,666,970 = 42.4% saving
</code></pre> <p>This showed that the defaults of 128k record size and zstd-3 were already pretty good. Moving to a record size of 512k improved compression over 128k by a bit over 4%. Given all meta data is cached separately, this seems a worthwhile improvement with no significant downside. Moving to zstd-9 improved compression over zstd-3 by about 2%. Given the CPU cost of compression at zstd-9 is about 4x zstd-3, even though emails are immutable and tend to be kept for a long time, we’ve decided not to implement this change.</p> <h2 id="zfs-encryption" tabindex="-1">ZFS encryption</h2> <p>We always enable <a href="https://en.wikipedia.org/wiki/Data_at_rest#Encryption" target="_blank" rel="noopener">encryption at rest</a> on all of our drives. This was usually done with <a href="https://en.wikipedia.org/wiki/Linux_Unified_Key_Setup" target="_blank" rel="noopener">LUKS</a>. But with ZFS this was <a href="https://arstechnica.com/gadgets/2021/06/a-quick-start-guide-to-openzfs-native-encryption/" target="_blank" rel="noopener">built in</a>. Again, this reduces overall system complexity.</p> <h2 id="going-all-in-on-zfs" tabindex="-1">Going all in on ZFS</h2> <p>So after the success of our initial testing, we decided to go all in on ZFS for all our large data storage needs. We’ve now been using ZFS for all our email servers for over 3 years and have been very happy with it. We’ve also moved over all our database, log and backup servers to using ZFS on NVMe SSDs as well with equally good results.</p> <h2 id="ssd-lifetimes" tabindex="-1">SSD lifetimes</h2> <p>The flash memory in SSDs has a finite life and <a href="https://en.wikipedia.org/wiki/Flash_memory#Write_endurance" target="_blank" rel="noopener">finite number of times it can be written to</a>. SSDs employ increasingly complex <a href="https://en.wikipedia.org/wiki/Wear_leveling" target="_blank" rel="noopener">wear levelling</a> algorithms to spread out writes and increase drive lifetime. You’ll often see the quoted endurance of an enterprise SSD as either an absolute figure of “Lifetime Writes”/“Total bytes written” like 65 PBW (petabytes written) or a relative per-day figure of “Drive writes per day” like 0.3, which you can convert to lifetime figure by multiplying by the drive size and the drive expected lifetime which is often assumed to be 5 years.</p> <p>Although we could calculate IO rates for existing <a href="https://en.wikipedia.org/wiki/Hard_disk_drive" target="_blank" rel="noopener">HDD</a> systems, we were making a significant number of changes moving to the new systems. Switching to a COW filesystem like ZFS, removing the special casing meta/data/archive partitions, and the massive latency reduction and performance improvements mean that things that might have taken extra time previously and ended up batching IO together, are now so fast it actually causes additional separated IO actions.</p> <p>So one big unknown question we had was how fast would the SSDs wear in our actual production environment? After several years, we now have some clear data. From one server at random but this is fairly consistent across the fleet of our oldest servers:</p> <pre><code># smartctl -a /dev/nvme14
...
Percentage Used:                    4%
</code></pre> <p>At this rate, we’ll replace these drives due to increased drive sizes, or entirely new physical drive formats (such <a href="https://www.snia.org/forums/cmsi/knowledge/formfactors" target="_blank" rel="noopener">E3.S</a> which appears to finally be gaining traction) long before they get close to their rated write capacity.</p> <p>We’ve also anecdotally found SSDs just to be much more reliable compared to HDDs for us. Although we’ve only ever used <a href="https://www.micron.com/products/storage/ssd/data-center-ssd/" target="_blank" rel="noopener">datacenter</a> <a href="https://www.solidigm.com/products/data-center.html" target="_blank" rel="noopener">class</a> SSDs and <a href="https://www.seagate.com/www-content/datasheets/pdfs/exos-7-e8-data-sheet-DS1957-1-1709US-en_US.pdf" target="_blank" rel="noopener">HDDs</a> failures and replacements every few weeks were a regular occurrence on the old fleet of servers. Over the last 3+ years, we’ve only seen a couple of SSD failures in total across the entire upgraded fleet of servers. This is easily less than one tenth the failure rate we used to have with HDDs.</p> <h2 id="storage-cost-calculation" tabindex="-1">Storage cost calculation</h2> <p>After converting all our email storage to NVMe SSDs, we were recently looking at our data backup solution. At the time it consisted of a number of older 2U servers with 12 x 3.5" SATA drive bays and we decided to do some cost calculations on:</p> <ol> <li>Move to cloud storage.</li> <li>Upgrade the HD drives in existing servers.</li> <li>Upgrade to SSD NVMe machines.</li> </ol> <h3 id="1-cloud-storage" tabindex="-1">1. Cloud storage:</h3> <p>Looking at various providers, the per TB per month price, and then a yearly price for 1000Tb/1Pb (prices as at Dec 2024)</p> <ul> <li><a href="https://aws.amazon.com/s3/pricing/" target="_blank" rel="noopener">Amazon S3</a> - $21 -&gt; $252,000/y</li> <li><a href="https://developers.cloudflare.com/r2/pricing/" target="_blank" rel="noopener">Cloudflare R2</a> - $15 -&gt; $180,000/y</li> <li><a href="https://wasabi.com/pricing" target="_blank" rel="noopener">Wasabi</a> - $6.99 -&gt; $83,880/y</li> <li><a href="https://www.backblaze.com/cloud-storage/pricing" target="_blank" rel="noopener">Backblaze B2</a> - $6 -&gt; $72,000/y</li> <li><a href="https://aws.amazon.com/s3/pricing/" target="_blank" rel="noopener">Amazon S3 Glacier Instant Retrieval</a> - $4 -&gt; $48,000/y</li> <li><a href="https://aws.amazon.com/s3/pricing/" target="_blank" rel="noopener">Amazon S3 Glacier Deep Archive (12 hour retrieval time)</a> - $0.99 -&gt; $11,880/y</li> </ul> <p>Some of these (e.g. Amazon) have potentially significant bandwidth fees as well.</p> <p>It’s interesting seeing the spread of prices here. Some also have a bunch of weird edge cases as well. e.g. “The S3 Glacier Flexible Retrieval and S3 Glacier Deep Archive storage classes require an additional 32 KB of data per object”. Given the large retrieval time and extra overhead per-object, you’d probably want to store small incremental backups in regular S3, then when you’ve gathered enough, build a biggish object to push down to Glacier. This adds implementation complexity.</p> <ul> <li><em>Pros</em>: No limit to amount we store. Assuming we use S3 compatible API, can choose between multiple providers.</li> <li><em>Cons</em>: Implementation cost of converting existing backup system that assumes local POSIX files to S3 style object API is uncertain and possibly significant. Lowest cost options require extra careful consideration around implementation details and special limitations. Ongoing monthly cost that will only increase as amount of data we store increases. Uncertain if prices will go down or not, or even go up. Possible significant bandwidth costs depending on provider.</li> </ul> <h3 id="2-upgrade-hd-ds" tabindex="-1">2. Upgrade HDDs</h3> <p><a href="https://www.seagate.com/au/en/products/enterprise-drives/exos-x/x24/" target="_blank" rel="noopener">Seagate Exos 24 HDs</a> are 3.5" 24T HDDs. This would allow us to triple the storage on existing servers. Each HDD is about $500, so upgrading one 2U machine would be about $6,000 and have storage of 220T or so.</p> <ul> <li><em>Pros</em>: Reuses existing hardware we already have. Upgrades can be done a machine at a time. Fairly low price</li> <li><em>Cons</em>: Will existing units handle 24T drives? What’s the rebuild time on drive failure look like? It’s almost a day for 8T drives already, so possibly nearly a week for a failed 24T drive? Is there enough IO performance to handle daily backups at capacity?</li> </ul> <h3 id="3-upgrade-to-new-hardware" tabindex="-1">3. Upgrade to new hardware</h3> <p>As we know, SSDs are denser (2.5" -&gt; 24 per 2U vs 3.5" -&gt; 12 per 2U), more reliable, and now higher capacity - <a href="https://www.solidigm.com/products/data-center/d5/p5336.html#form=U.2%2015mm&amp;cap=61.44TB" target="_blank" rel="noopener">up to 61T per 2.5" drive</a>. A single 2U server with 24 x 61T drives with 2 x 12 RAIDz2 = 1220T. Each drive is <a href="https://www.newegg.com/solidigm-61-44tb-d5-p5336/p/N82E16820318031" target="_blank" rel="noopener">about $7k</a> right now, prices fluctuate. So all up 24 x $7k = $168k + ~$20k server =~ $190k for &gt; 1000T storage one-time cost.</p> <ul> <li><em>Pros</em>: <strong>Much</strong> higher sequential and random IO than HDDs will ever have. Price &lt; 1 year of standard S3 storage. Internal to our WAN, no bandwidth costs and very low latency. No new development required, existing backup system will just work. Consolidate on single 2U platform for all storage (cyrus, db, backups) and SSD for all storage. Significant space and power savings over existing HDD based servers</li> <li><em>Cons</em>: Greater up front cost. Still need to predict and buy more servers as backups grow.</li> </ul> <p>One thing you don’t see in this calculation is datacenter space, power, cooling, etc. The reason is that compared to the amortised yearly cost of a storage server like this, these are actually reasonably minimal these days, on the order of $3000/2U/year. Calculating person time is harder. We have a lot of home built automation systems that mean installing and running one more server has minimal marginal cost.</p> <h3 id="result" tabindex="-1">Result</h3> <p>We ended up going with the the new 2U servers option:</p> <p><picture><source type="image/webp" srcset="https://www.fastmail.com/assets/images/nvme-imap-servers-eeGMRYrXlR-375.webp 375w, https://www.fastmail.com/assets/images/nvme-imap-servers-eeGMRYrXlR-750.webp 750w, https://www.fastmail.com/assets/images/nvme-imap-servers-eeGMRYrXlR-1500.webp 1500w" sizes="(max-width: 425px) 375px, 750px"><img alt="NVME IMAP Servers" loading="lazy" decoding="async" src="https://www.fastmail.com/assets/images/nvme-imap-servers-eeGMRYrXlR-375.png" width="1500" height="559" srcset="https://www.fastmail.com/assets/images/nvme-imap-servers-eeGMRYrXlR-375.png 375w, https://www.fastmail.com/assets/images/nvme-imap-servers-eeGMRYrXlR-750.png 750w, https://www.fastmail.com/assets/images/nvme-imap-servers-eeGMRYrXlR-1500.png 1500w" sizes="(max-width: 425px) 375px, 750px"></picture></p> <ul> <li>The 2U AMD NVMe platform with ZFS is a platform we have experience with already</li> <li>SSDs are much more reliable and much higher IO compared to HDDs</li> <li>No uncertainty around super large HDDs, RAID controllers, rebuild times, shuffling data around, etc.</li> <li>Significant space and power saving over existing HDD based servers</li> <li>No new development required, can use existing backup system and code</li> <li>Long expected hardware lifetime, controlled upfront cost, can depreciate hardware cost</li> </ul> <p>So far this has worked out very well. The machines have bonded 25Gbps networks and when filling them from scratch we were able to saturate the network links streaming around 5Gbytes/second of data from our IMAP servers, compressing and writing it all down to a RAIDz2 zstd-3 compressed ZFS dataset.</p> <h2 id="conclusion" tabindex="-1">Conclusion</h2> <p>Running your own hardware might not be for everyone and has distinct tradeoffs. But when you have the experience and the knowledge of how you expect to scale, the cost improvements can be significant.</p> </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Murder Mystery: GCC builds failing after sbuild refactoring (114 pts)]]></title>
            <link>https://www.linux.it/~ema/posts/murder-mystery-gcc-builds-failing-after-sbuild-refactoring/</link>
            <guid>42484911</guid>
            <pubDate>Sun, 22 Dec 2024 07:31:22 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.linux.it/~ema/posts/murder-mystery-gcc-builds-failing-after-sbuild-refactoring/">https://www.linux.it/~ema/posts/murder-mystery-gcc-builds-failing-after-sbuild-refactoring/</a>, See on <a href="https://news.ycombinator.com/item?id=42484911">Hacker News</a></p>
Couldn't get https://www.linux.it/~ema/posts/murder-mystery-gcc-builds-failing-after-sbuild-refactoring/: Error: timeout of 10000ms exceeded]]></description>
        </item>
        <item>
            <title><![CDATA[The essays of Michel de Montaigne online (169 pts)]]></title>
            <link>https://hyperessays.net/</link>
            <guid>42484527</guid>
            <pubDate>Sun, 22 Dec 2024 05:16:37 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://hyperessays.net/">https://hyperessays.net/</a>, See on <a href="https://news.ycombinator.com/item?id=42484527">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p><svg width="100%" height="100%" viewBox="0 0 300 300" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:serif="http://www.serif.com/" style="fill-rule:evenodd;clip-rule:evenodd;stroke-linejoin:round;stroke-miterlimit:2;"><g transform="matrix(1.12327,0,0,1.12327,-18.4907,-0.775614)"><title id="logoTitle">HyperEssays’s logo</title><desc id="logoDesc">A group of three horizontal yellow bars over a thin diagonal red line balancing on a thick blue circle.</desc><path d="M150 155.935c-25.808.0-46.762 20.953-46.762 46.762.0 25.808 20.954 46.761 46.762 46.761s46.762-20.953 46.762-46.761c0-25.809-20.954-46.762-46.762-46.762zm0 32.049C158.12 187.984 164.712 194.577 164.712 202.697S158.12 217.409 150 217.409 135.288 210.817 135.288 202.697 141.88 187.984 150 187.984z" style="fill:rgb(42,103,188);"></path></g><g transform="matrix(2.77397,-0.278409,0.0786955,0.784093,-481.796,161.307)"><path d="M172.106 68.354 279.894 67.646v-12L172.106 56.354v12z" style="fill:rgb(239,64,61);"></path></g><g transform="matrix(1,0,0,0.960409,0,4.6509)"><rect x="100.855" y="94.566" width="96.29" height="22.907" style="fill:rgb(235,174,11);"></rect></g><g transform="matrix(1,0,0,0.960409,0,-26.7149)"><rect x="100.855" y="94.566" width="96.29" height="22.907" style="fill:rgb(235,174,11);"></rect></g><g transform="matrix(1,0,0,0.960409,0,-58.0808)"><rect x="100.855" y="94.566" width="96.29" height="22.907" style="fill:rgb(235,174,11);"></rect></g></svg></p></div><div><p>HyperEssays is a project to create a <b>modern and accessible online edition</b> of the <a href="https://hyperessays.net/essays/toc/"><i>Essays</i> of Michel de Montaigne</a>.</p><p>HyperEssays.net hosts <b>four editions</b> of the <i>Essays</i>:</p><ol><li>A <a href="https://hyperessays.net/gournay/toc/"><span role="text" aria-label="fifteen ninety eight">1598</span> edition, in middle French</a>, edited by Marie de Gournay. This is a slightly revised version of Gournay’s original edition published in <span role="text" aria-label="fifteen ninety five">1595.</span></li><li>A complete and searchable edition of <a href="https://hyperessays.net/florio/toc/">John Florio’s <span role="text" aria-label="sixteen o three">1603</span> translation of the <i>Essays</i></a>, in early modern English.</li><li>A <a href="https://hyperessays.net/cotton/toc/"><span role="text" aria-label="sixteen eighty five">1685</span> translation by Charles Cotton</a>, also in early modern English. Only some chapters of this edition have been copyedited and posted.</li><li>A <a href="https://hyperessays.net/essays/toc/">complete and searchable modern edition of the <i>Essays</i></a> based on W. Carew Hazlitt’s <span role="text" aria-label="eighteen seventy seven">1877</span> update of Charles Cotton’s translation. I am slowly replacing the Cotton/Hazlitt translation with a contemporary one and adding new notes.</li></ol><p>My goals with HyperEssays are to provide <b>context and tools</b> for first-time readers of the <i>Essays</i> and to design a <b>lasting resource</b> for all interested in Montaigne’s work.</p><p>To that end, I copyedit, update, and annotate the original text and its translations. I tag them for indexing and searching, and format them them for easy reading on smartphones, desktop computers, and tablets. In addition, I prepare and provide <b>free chapter PDFs</b> for offline reading.</p><p>You can help make HyperEssays a reliable online resource by <a href="https://hyperessays.net/support/">supporting this project.</a> With your contribution, this site can continue to grow and remain free and accessible to all.</p><h3>What are the <i>Essays</i> about?</h3><p>The <i>Essays</i> is not a single, cohesive book but a collection of short and long pieces on <b>various subjects</b> such as religion, horses, friendship, sleep, law, or suicide, which Montaigne wrote over more than <b>twenty years</b>. His goals for the book and the circumstances under which he worked on it <b>changed over time</b>.</p><p>The first edition, published in 1580, comprised two books. Eight years later, an updated edition included hundreds of revisions and a new, third book. By the time of his death, in 1592, Montaigne had planned many more changes, which were in­cor­po­rat­ed in the first posthumous edition of 1595.</p><p>So, while you can read the <i>Essays</i> from beginning to end, starting with Montaigne’s address <a href="https://hyperessays.net/essays/to-the-reader/"><i>To the Reader</i></a>, you can also follow John Cage’s advice and “<b>begin any­where.</b>”</p><p>Pick from a selection of some of the most well-known chapters:</p><ul><li><a href="https://hyperessays.net/essays/to-philosophize-is-to-learn-to-die/"><i>To Philosophize Is to Learn to Die</i></a>,</li><li><a href="https://hyperessays.net/essays/on-the-education-of-children/"><i>On the Education of Children</i></a>,</li><li><a href="https://hyperessays.net/essays/on-friendship/"><i>On Friendship</i></a>,</li><li><a href="https://hyperessays.net/essays/on-cannibals/"><i>On Cannibals</i></a>,</li><li><a href="https://hyperessays.net/essays/on-books/"><i>On Books</i></a>,</li><li><a href="https://hyperessays.net/essays/apology-for-raymond-sebond/"><i>Apology for Raymond Sebond</i></a>,</li><li><a href="https://hyperessays.net/essays/on-some-verses-of-virgil/"><i>On Some Verses of Virgil</i></a>,</li><li><a href="https://hyperessays.net/essays/on-coaches/"><i>On Coaches</i></a>,</li><li><a href="https://hyperessays.net/essays/on-experience/"><i>On Experience</i></a>.</li></ul><p>Or look at the <a href="https://hyperessays.net/essays/toc/">table of contents</a> and let your curiosity guide you.</p><h3>Who was Michel de Montaigne?</h3><p>Michel de Montaigne, the author of the <i>Essays,</i> is often described as a sixteenth-century French philosopher. But was Montaigne actually a philosopher? And did he really retire from the world to write in solitude for years, as is commonly believed?</p><p>In <a href="https://hyperessays.net/on-montaigne/"><i>On Montaigne</i></a>, I address these questions and provide <b>biographical context</b> to better understand the <i>Essays.</i> The companion <a href="https://hyperessays.net/timeline/">timeline</a> provides a <b>chronological overview</b> of his life.</p><p>If you want to learn more about him, I recommend these <a href="https://hyperessays.net/four-biographies-of-montaigne/">four biographies of Montaigne</a> (along with two modern translations of the <i>Essays</i>). Each one is engaging but written with a different audience in mind.</p><h3>Recent updates</h3><p>Copy<wbr>editing, translating, writing notes, updating metadata … the work never ends. This is HyperEssays’s work log, a list of the chapters I’ve been working on:</p><ul id="wip"><li>Dec 21, 2024 · <a href="https://hyperessays.net/essays/on-repentance/"><i>On Repentance</i></a></li><li>Dec 21, 2024 · <a href="https://hyperessays.net/essays/on-pedantry/"><i>On Pedantry</i></a></li><li>Dec 20, 2024 · <a href="https://hyperessays.net/essays/let-others-judge-of-our-happiness-after-our-death/"><i>Let Others Judge of Our Happiness after Our Death</i></a></li><li>Dec 18, 2024 · <a href="https://hyperessays.net/gournay/book/II/chapter/15/"><i>Que nostre desir s’accroist par la malaisance</i></a></li><li>Dec 15, 2024 · <a href="https://hyperessays.net/gournay/book/II/chapter/14/"><i>Comme nostre esprit s’empesche soy-mesmes</i></a></li><li>Dec 15, 2024 · <a href="https://hyperessays.net/essays/on-repentance/"><i>On Repentance</i></a></li><li>Dec 15, 2024 · <a href="https://hyperessays.net/gournay/book/II/chapter/13/"><i>De juger de la mort d’autruy</i></a></li><li>Dec 14, 2024 · <a href="https://hyperessays.net/gournay/book/II/chapter/12/"><i>Apologie de Raimond de Sebonde</i></a></li><li>Dec 10, 2024 · <a href="https://hyperessays.net/essays/a-consideration-on-cicero/"><i>A Consideration on Cicero</i></a></li><li>Dec 9, 2024 · <a href="https://hyperessays.net/essays/apology-for-raymond-sebond/"><i>Apology for Raymond Sebond</i></a></li><li>Nov 29, 2024 · <a href="https://hyperessays.net/gournay/book/II/chapter/11/"><i>De la cruauté</i></a></li><li>Nov 26, 2024 · <a href="https://hyperessays.net/essays/let-others-judge-of-our-happiness-after-our-death/"><i>Let Others Judge of Our Happiness after Our Death</i></a></li></ul><p>Work on <i>HyperEssays</i> started on January <span>17, 2020</span> and likely won’t be completed for many years.</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Slow deployment causes meetings (2015) (175 pts)]]></title>
            <link>https://tidyfirst.substack.com/p/slow-deployment-causes-meetings</link>
            <guid>42484139</guid>
            <pubDate>Sun, 22 Dec 2024 03:12:23 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://tidyfirst.substack.com/p/slow-deployment-causes-meetings">https://tidyfirst.substack.com/p/slow-deployment-causes-meetings</a>, See on <a href="https://news.ycombinator.com/item?id=42484139">Hacker News</a></p>
<div id="readability-page-1" class="page"><div dir="auto"><blockquote><p>First published 2016.</p></blockquote><p>“I can’t get any code out with all these meetings.” What if this perennial engineer complaint has causation backwards? Adding and removing organizational overhead is relatively easy compared to increasing an organization’s capacity to deploy code. What if meetings and reviews are an organization’s adaptive response to avoid overloading deployment?</p><p>Chuck Rossi&nbsp;[ed: legendary release manager at early-to-middle Facebook] made the observation that there seem to be a fixed number of changes Facebook can handle in one deployment. If we want more changes, we need more deployments. This has led to a steady increase in deployment pace over the past five years, from weekly to daily to thrice daily deployments of our PHP code and from six to four to two week cycles for deploying our mobile apps. This improvement has been driven primarily by the release engineering team (I’m a fan, can you tell?)</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F22d3f693-f2f5-4ea7-86bb-e01ef2f613dd_1268x877.jpeg" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F22d3f693-f2f5-4ea7-86bb-e01ef2f613dd_1268x877.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F22d3f693-f2f5-4ea7-86bb-e01ef2f613dd_1268x877.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F22d3f693-f2f5-4ea7-86bb-e01ef2f613dd_1268x877.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F22d3f693-f2f5-4ea7-86bb-e01ef2f613dd_1268x877.jpeg 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F22d3f693-f2f5-4ea7-86bb-e01ef2f613dd_1268x877.jpeg" width="1268" height="877" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/22d3f693-f2f5-4ea7-86bb-e01ef2f613dd_1268x877.jpeg&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:877,&quot;width&quot;:1268,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:641969,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/jpeg&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:false,&quot;topImage&quot;:true,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F22d3f693-f2f5-4ea7-86bb-e01ef2f613dd_1268x877.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F22d3f693-f2f5-4ea7-86bb-e01ef2f613dd_1268x877.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F22d3f693-f2f5-4ea7-86bb-e01ef2f613dd_1268x877.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F22d3f693-f2f5-4ea7-86bb-e01ef2f613dd_1268x877.jpeg 1456w" sizes="100vw" fetchpriority="high"></picture></div></a><figcaption><em>Approaching maximum changes/deployment triggers more frequent deployments</em></figcaption></figure></div><p>As I was drifting off to sleep yesterday, I visualized the sawtooth-shaped “changes per deployment” graph and it struck me that maybe we had organizational overhead all wrong. “Changes per deployment” seems like an inelastic metric. It’s possible to improve, but only with great effort over time. What happens when the number of changes produced exceeds the current threshold? Changes per deployment doesn’t change. The number of changes has to go down.</p><p>How? By increasing overhead—meetings, reviews, handoffs, overhead and eventually by killing enthusiasm and initiative. Nobody is going to own up to doing it on purpose, but perhaps the organization’s emergent response is locally optimal—change the thing that is easiest to change that will relieve the pressure.</p><p>Increasing overhead initiates a positive feedback loop: less getting done -&gt; more pressure -&gt; more mistakes -&gt; even fewer changes per deployment -&gt; more overhead -&gt; less getting done. Isolated efforts to reduce overhead increase pressure and increase overhead.</p><p>If you want more changes to get through, you need to expand the far end of the hose, to increase deployment capacity. You can do this the hard way, by reducing the deployment cycle and dealing with the ensuing chaos, or the harder way, by increasing the number of changes per deployment (better tests, better monitoring, better isolation between elements, better social relationships on the team). But don’t try to reduce overhead. That’ll just lead inevitably to a series of meetings on how to reduce meetings. At least that will keep you from trying to ship too much code, though.</p><blockquote><p>This essay is an example of the Thinkie Reverse Causality. It’s one of the most fun Thinkies to deploy because they ideas seem just so wrong at first.</p></blockquote></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Rosetta 2 creator leaves Apple to work on Lean full-time (363 pts)]]></title>
            <link>https://www.linkedin.com/posts/leonardo-de-moura-26a27b5_leanlang-leanprover-leanfro-activity-7274523099394400256-0F0x</link>
            <guid>42483895</guid>
            <pubDate>Sun, 22 Dec 2024 02:01:51 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.linkedin.com/posts/leonardo-de-moura-26a27b5_leanlang-leanprover-leanfro-activity-7274523099394400256-0F0x">https://www.linkedin.com/posts/leonardo-de-moura-26a27b5_leanlang-leanprover-leanfro-activity-7274523099394400256-0F0x</a>, See on <a href="https://news.ycombinator.com/item?id=42483895">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="main-content" role="main">
      <section>
              <h2>                  Leonardo de Moura’s Post</h2>

                
    

    
      

    <article data-activity-urn="urn:li:activity:7274523099394400256" data-featured-activity-urn="urn:li:activity:7274523099394400256" data-attributed-urn="urn:li:share:7274523098685562880">
<!---->
      
              
    
    
    
    
    
    
    
    
    
    
    
    

    
    
    
    
    
    
    
    
    
    
    

    
    

<!---->  
        

      <div data-test-id="main-feed-activity-card__entity-lockup">
          <a href="https://www.linkedin.com/in/leonardo-de-moura-26a27b5?trk=public_post_feed-actor-image" data-tracking-control-name="public_post_feed-actor-image" data-tracking-will-navigate="">
            
      
  
<!---->          </a>
      <div>
        

            <p>
<!---->                Senior Principal Applied Scientist at AWS, and Chief Architect at Lean FRO (non-profit)
            </p>

            <p><span>
                <time>
                  

    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    

    
    
    
    
    
    
    
    
    
    
    
    
    
    

      5d
  
<!---->                </time>
<!---->            </span>
                </p></div>

<!---->    </div>

      
            
            
  <p dir="ltr" data-test-id="main-feed-activity-card__commentary">I am thrilled to welcome <a href="https://www.linkedin.com/in/cameronzwarich?trk=public_post-text" target="_self" data-tracking-control-name="public_post-text" data-tracking-will-navigate="">Cameron Zwarich</a> to the Lean FRO! As the brilliant creator of Rosetta 2 and an exceptional software developer with over 15 years of experience at Apple specializing in low-level systems software, Cameron will focus on enhancing Lean's code generator. I can’t wait to see the incredible impact his expertise will have on the Lean ecosystem!
<a href="https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Ffeed%2Fhashtag%2Fleanlang&amp;trk=public_post-text" target="_self" data-tracking-control-name="public_post-text" data-tracking-will-navigate="">#LeanLang</a> <a href="https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Ffeed%2Fhashtag%2Fleanprover&amp;trk=public_post-text" target="_self" data-tracking-control-name="public_post-text" data-tracking-will-navigate="">#LeanProver</a> <a href="https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Ffeed%2Fhashtag%2Fleanfro&amp;trk=public_post-text" target="_self" data-tracking-control-name="public_post-text" data-tracking-will-navigate="">#LeanFRO</a></p>



        

        
            
<!---->  
                  

      
          
        

      
          
        

      
            
      
    
    
    
    
    
    
    
    
    
    
    
    
    

    
    

      
  
  
        

      
                
    
    
    

    
  
                      

      
              
            

    
    
    
    
    
    

<!---->
    

    
    

      
  
  
<!---->      
        

      
              

    
    
    
    
    
    

    
  
        
    </article>
  
      
  
  
            </section>
      <section>

            <h2>
              Explore topics
            </h2>

<!---->
        
      </section>
    </div></div>]]></description>
        </item>
    </channel>
</rss>