<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0">
    <channel>
        <title>HN100 - Readable Contents</title>
        <link>https://hn.algolia.com/api/v1/search_by_date?tags=%28story,poll%29&amp;numericFilters=points%3E100</link>
        <description>Uses Readability to add bodies to the RSS feed</description>
        <lastBuildDate>Wed, 29 Jan 2025 06:30:24 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>en</language>
        <item>
            <title><![CDATA[It's official: Research has found that libraries make everything better (176 pts)]]></title>
            <link>https://lithub.com/its-official-research-has-found-that-libraries-make-everything-better/</link>
            <guid>42860240</guid>
            <pubDate>Wed, 29 Jan 2025 01:11:08 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://lithub.com/its-official-research-has-found-that-libraries-make-everything-better/">https://lithub.com/its-official-research-has-found-that-libraries-make-everything-better/</a>, See on <a href="https://news.ycombinator.com/item?id=42860240">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
				
				
				
				
				<p>Science has backed up what many of us have long been saying: the library <em>rocks</em>. <a href="https://www.nypl.org/spotlight/libraries-well-being-report" target="_blank">A study from the New York Public Library</a> surveyed 1,974 users on how the library makes them feel and how it affects their lives, and the results are overwhelmingly positive.</p>
<p>The researchers’ analysis (which used positive psychology’s PERMA model, if that means anything to you) discovered that libraries are good for people, their well-being, and their communities. Not only that, but the positive societal impacts are more pronounced in lower-income communities, even more reason to make sure we’re funding and supporting libraries. Don’t let the ghosts of Reagan and Thatcher tell you otherwise, government can help people!</p>
<p>Some top-line statistics from the study:</p>
<p>– 92% of respondents reported feeling somewhat to very “calm / peaceful” after visiting the Library<br>
– 74% of respondents reported that their library use positively affects how equipped they feel to cope with the world<br>
– 90% of respondents reported that their Library use positively affects how much they love to learn new things<br>
– 88% of respondents reported that their Library use has supported their personal growth</p>
<p>Those are some big numbers and some uniformly good news — people are not only feeling better about themselves and their world after a visit to the library, but they’re feeling more secure in their world too.</p>
<p>The individual outcomes are undeniable: 89% of respondents said that the library had a positive effect on them having “more appreciation for things [they] did not know much about before” and 77% said the library made them feel “that what [they] do in [their] life is valuable and worthwhile.” You can get books at the library, but you can also fight your existential dread.</p>
<p>People are also moving away from doomerism in the stacks: 82% of visitors said use of the library “positively affects how optimistic they are about the future.” That’s not just for people visiting the brick-and-mortar library either: 58% of e-only users also get a sense of optimism from library interactions. It honestly feels like a miracle that anything connected to the internet would make people feel good, so this is a big win.</p>
<p>The community feelings the library engenders are very encouraging too: 75% say libraries gave them more positive feelings of “empathy towards others who may be different from [them],” 72% said it made them feel more connected to others, 66% felt “seen and heard,” and 70% felt like they are “part of a community.” Most touching to me is that 59% said the library had a positive effect on their “feeling that there are people in your life who really care about [them].”</p>
<p>What I find most charming in this study are the quotes, which the researchers highlight in “Patron Voices” sections. They’re full of great little lines, like people calling the library “a touchstone” and “a place to rely on,” and that “knowing it’s there makes me feel better about my life in the city.”</p>
<p>I really had to hold myself back from including too many of these patron quotes, because in a month when I’ve been feeling so, so down, reading all the nice things people have to say about the library felt like a hug from an old friend. Here are just some of them:</p>
<p>– “Space where I can just be me”<br>
– “Books transport me”<br>
– “Islands of calm, and I find balance within them”<br>
– “It offers us hope that we can do something, that we can make a change, that we can advance”<br>
– “Surrounded me with life’s possibilities”<br>
– “Makes me feel useful”<br>
– “The library gives you a sense of direction”</p>
<p>Tell me you didn’t tear up at that, and pal, <em>I’ll show you a liar</em>. Also these quotes are a great opportunity for some uplifting found poetry, if anyone’s looking for a new chapbook project.</p>
<p>So the takeaway? If you’re feeling unmotivated and unconnected, the library has now been scientifically proven to improve your well-being, the perfect antidote to all the push alerts and doomscrolling that’s bringing you down.</p>
				
										
									
				

				

			</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Discovery Coding (113 pts)]]></title>
            <link>https://jimmyhmiller.github.io/discovery-coding</link>
            <guid>42860128</guid>
            <pubDate>Wed, 29 Jan 2025 00:53:30 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://jimmyhmiller.github.io/discovery-coding">https://jimmyhmiller.github.io/discovery-coding</a>, See on <a href="https://news.ycombinator.com/item?id=42860128">Hacker News</a></p>
Couldn't get https://jimmyhmiller.github.io/discovery-coding: Error: timeout of 10000ms exceeded]]></description>
        </item>
        <item>
            <title><![CDATA[Goodbye, Slopify (298 pts)]]></title>
            <link>https://alexeystar.com/blog/slopify/</link>
            <guid>42860113</guid>
            <pubDate>Wed, 29 Jan 2025 00:51:25 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://alexeystar.com/blog/slopify/">https://alexeystar.com/blog/slopify/</a>, See on <a href="https://news.ycombinator.com/item?id=42860113">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="content"><figure><img src="https://alexeystar.com/blog/slopify/slopify2.png" alt="Screenshot from Spotify app suggesting AI music"><figcaption></figcaption></figure><p>My Discover Weekly playlist on Spotify is finally poisoned by AI slop.</p><p>There’s no way I’m going to continue a subscription to a streaming service that suggests AI-generated music with AI-generated album covers. A bloated, Electron-based desktop application with a terrible UI is only going to add up.</p><p>So goodbye, Slopify. Thank you for pushing me towards supporting more real artists and buying music that I can actually own.</p><p><time>2025-01-26</time></p><p><span><a href="https://alexeystar.com/blog/cats-2024/">←<br>Cat-alog of the Year</a></span></p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Departing the New York Times (146 pts)]]></title>
            <link>https://contrarian.substack.com/p/departing-the-new-york-times</link>
            <guid>42859669</guid>
            <pubDate>Tue, 28 Jan 2025 23:47:48 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://contrarian.substack.com/p/departing-the-new-york-times">https://contrarian.substack.com/p/departing-the-new-york-times</a>, See on <a href="https://news.ycombinator.com/item?id=42859669">Hacker News</a></p>
Couldn't get https://contrarian.substack.com/p/departing-the-new-york-times: Error: timeout of 10000ms exceeded]]></description>
        </item>
        <item>
            <title><![CDATA[Deferred resignation email to federal employees (118 pts)]]></title>
            <link>https://www.opm.gov/fork</link>
            <guid>42859552</guid>
            <pubDate>Tue, 28 Jan 2025 23:31:48 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.opm.gov/fork">https://www.opm.gov/fork</a>, See on <a href="https://news.ycombinator.com/item?id=42859552">Hacker News</a></p>
Couldn't get https://www.opm.gov/fork: Error: timeout of 10000ms exceeded]]></description>
        </item>
        <item>
            <title><![CDATA[DeepSeek's multi-head latent attention and other KV cache tricks (226 pts)]]></title>
            <link>https://www.pyspur.dev/blog/multi-head-latent-attention-kv-cache-paper-list</link>
            <guid>42858741</guid>
            <pubDate>Tue, 28 Jan 2025 22:11:36 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.pyspur.dev/blog/multi-head-latent-attention-kv-cache-paper-list">https://www.pyspur.dev/blog/multi-head-latent-attention-kv-cache-paper-list</a>, See on <a href="https://news.ycombinator.com/item?id=42858741">Hacker News</a></p>
<div id="readability-page-1" class="page"><section id="blog"><div><!--$--><p><img alt="DeepSeek's Multi-Head Latent Attention and Other KV Cache Tricks" loading="lazy" width="1920" height="1080" decoding="async" data-nimg="1" srcset="https://www.pyspur.dev/_next/image?url=%2Fblog%2Fkv-cache%2Fmla.png&amp;w=1920&amp;q=75&amp;dpl=dpl_3Agp3pPUHQkFT3ejk3F3i1kgTjfP 1x, https://www.pyspur.dev/_next/image?url=%2Fblog%2Fkv-cache%2Fmla.png&amp;w=3840&amp;q=75&amp;dpl=dpl_3Agp3pPUHQkFT3ejk3F3i1kgTjfP 2x" src="https://www.pyspur.dev/_next/image?url=%2Fblog%2Fkv-cache%2Fmla.png&amp;w=3840&amp;q=75&amp;dpl=dpl_3Agp3pPUHQkFT3ejk3F3i1kgTjfP"></p><!--/$--><div><!--$--><p><time datetime="2025-01-21">January 21, 2025 (1w ago)</time><span>•</span></p><!--/$--></div><div><a target="_blank" rel="noopener noreferrer" href="https://twitter.com/jeankaddour"><img alt="Jean Kaddour" loading="lazy" width="40" height="40" decoding="async" data-nimg="1" srcset="https://www.pyspur.dev/_next/image?url=%2Fauthor.jpg&amp;w=48&amp;q=75&amp;dpl=dpl_3Agp3pPUHQkFT3ejk3F3i1kgTjfP 1x, https://www.pyspur.dev/_next/image?url=%2Fauthor.jpg&amp;w=96&amp;q=75&amp;dpl=dpl_3Agp3pPUHQkFT3ejk3F3i1kgTjfP 2x" src="https://www.pyspur.dev/_next/image?url=%2Fauthor.jpg&amp;w=96&amp;q=75&amp;dpl=dpl_3Agp3pPUHQkFT3ejk3F3i1kgTjfP"></a></div><article><p><strong>Overview</strong>:</p>
<ol>
<li><strong>Introduction</strong>: We'll explore how Key-Value (KV) caches make language models like ChatGPT and DeepSeek faster at generating text, by making a clever trade-off between memory usage and computation time.</li>
<li><strong>MLA and other Tricks</strong>: We'll then look at 11 recent research papers, including <strong>DeepSeek's Multi-head Latent Attention (MLA)</strong>, that build upon this basic idea to make LLM inference even more time-efficient.</li>
</ol>
<hr>
<h2>Understanding the Problem: Why Text Generation is Slow</h2>
<p>Let's start with a simple analogy. Imagine you're writing a story, and for each new word you write, you need to re-read the entire story so far to maintain consistency. The longer your story gets, the more time you spend re-reading. This is exactly what large language models face during text generation!</p>
<h3>The Basic Building Block: Self-Attention</h3>
<p>At the heart of modern language models is a mechanism called <strong>self-attention</strong>. For a sequence of <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>n</mi></mrow><annotation encoding="application/x-tex">n</annotation></semantics></math></span></span> tokens (think of tokens as roughly corresponding to words), each token needs to "look at" or "attend to" all other tokens to understand the context.</p>
<p>This looking-at-everything process has a computational cost that grows with the sequence length:</p>
<ul>
<li>For <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>n</mi></mrow><annotation encoding="application/x-tex">n</annotation></semantics></math></span></span> tokens, each token needs to look at all <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>n</mi></mrow><annotation encoding="application/x-tex">n</annotation></semantics></math></span></span> tokens</li>
<li>This means the cost is proportional to <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>n</mi><mo>×</mo><mi>n</mi><mo>=</mo><msup><mi>n</mi><mn>2</mn></msup></mrow><annotation encoding="application/x-tex">n \times n = n^2</annotation></semantics></math></span></span></li>
<li>In mathematical notation, we write this as <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>O</mi><mo stretchy="false">(</mo><msup><mi>n</mi><mn>2</mn></msup><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">O(n^2)</annotation></semantics></math></span></span> complexity</li>
</ul>
<h3>The Real Problem: Generating Text One Token at a Time</h3>
<p>When a language model generates text, it does so one token at a time, and this is where things get computationally expensive:</p>
<ol>
<li><strong>First token</strong>: Look at 1 token (cost: <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>O</mi><mo stretchy="false">(</mo><msup><mn>1</mn><mn>2</mn></msup><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">O(1^2)</annotation></semantics></math></span></span>)</li>
<li><strong>Second token</strong>: Look at 2 tokens (cost: <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>O</mi><mo stretchy="false">(</mo><msup><mn>2</mn><mn>2</mn></msup><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">O(2^2)</annotation></semantics></math></span></span>)</li>
<li><strong>Third token</strong>: Look at 3 tokens (cost: <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>O</mi><mo stretchy="false">(</mo><msup><mn>3</mn><mn>2</mn></msup><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">O(3^2)</annotation></semantics></math></span></span>)</li>
<li>And so on until the <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>n</mi></mrow><annotation encoding="application/x-tex">n</annotation></semantics></math></span></span>-th token: Look at <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>n</mi></mrow><annotation encoding="application/x-tex">n</annotation></semantics></math></span></span> tokens (cost: <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>O</mi><mo stretchy="false">(</mo><msup><mi>n</mi><mn>2</mn></msup><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">O(n^2)</annotation></semantics></math></span></span>)</li>
</ol>
<p>If we add up all these costs for generating a sequence of length <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>n</mi></mrow><annotation encoding="application/x-tex">n</annotation></semantics></math></span></span>, we get:</p>
<span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi>O</mi><mo stretchy="false">(</mo><msup><mn>1</mn><mn>2</mn></msup><mo>+</mo><msup><mn>2</mn><mn>2</mn></msup><mo>+</mo><msup><mn>3</mn><mn>2</mn></msup><mo>+</mo><mo>⋯</mo><mo>+</mo><msup><mi>n</mi><mn>2</mn></msup><mo stretchy="false">)</mo><mo>≈</mo><mi>O</mi><mo stretchy="false">(</mo><msup><mi>n</mi><mn>3</mn></msup><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">O(1^2 + 2^2 + 3^2 + \dots + n^2) \approx O(n^3)</annotation></semantics></math></span></span></span>
<p>This <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>O</mi><mo stretchy="false">(</mo><msup><mi>n</mi><mn>3</mn></msup><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">O(n^3)</annotation></semantics></math></span></span> cost means that as your text gets longer, the generation time grows <strong>extremely quickly</strong>. For example, generating a sequence twice as long takes roughly <strong>eight times</strong> as long! Clearly, we need a better approach.</p>
<hr>
<h2>The Solution: Key-Value (KV) Cache</h2>
<p>The key insight behind KV caching is that we're doing a lot of redundant work. When generating each new token, we're recomputing things for all previous tokens that we've already processed before. Let's see how we can fix this.</p>
<h3>What is a Key-Value Cache?</h3>
<p>Think of a KV cache like a smart notepad where we write down important information about each token the first time we see it. For each token, we compute and store two things:</p>
<ol>
<li>A <strong>key</strong> (<span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>k</mi></mrow><annotation encoding="application/x-tex">k</annotation></semantics></math></span></span>): Think of this as an addressing mechanism - it helps determine how relevant this token is to future tokens</li>
<li>A <strong>value</strong> (<span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>v</mi></mrow><annotation encoding="application/x-tex">v</annotation></semantics></math></span></span>): Think of this as the actual information that gets used when this token is found to be relevant</li>
</ol>
<p>Mathematically, we compute these as:</p>
<ul>
<li>Key: <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>k</mi><mo>=</mo><mi>x</mi><msub><mi>W</mi><mi>K</mi></msub></mrow><annotation encoding="application/x-tex">k = x W_K</annotation></semantics></math></span></span> (where <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>x</mi></mrow><annotation encoding="application/x-tex">x</annotation></semantics></math></span></span> is the token and <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>W</mi><mi>K</mi></msub></mrow><annotation encoding="application/x-tex">W_K</annotation></semantics></math></span></span> is a learned transformation)</li>
<li>Value: <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>v</mi><mo>=</mo><mi>x</mi><msub><mi>W</mi><mi>V</mi></msub></mrow><annotation encoding="application/x-tex">v = x W_V</annotation></semantics></math></span></span> (where <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>W</mi><mi>V</mi></msub></mrow><annotation encoding="application/x-tex">W_V</annotation></semantics></math></span></span> is another learned transformation)</li>
</ul>
<p>When generating a new token, we use its query (computed similarly to keys) to find relevant information in our cache by comparing it with all stored keys. The matching values are then used to help generate the token.</p>
<h3>How the KV Cache Makes Things Faster</h3>
<p>With a KV cache, the process becomes much more efficient:</p>
<ol>
<li>When we see a new token, we only need to compute its key and value <strong>once</strong></li>
<li>For all future tokens, we can just look up these pre-computed values from our cache</li>
<li>This means each new token only needs to do a small amount of new work, instead of redoing all previous computations</li>
</ol>
<p>The trade-off is clear:</p>
<ul>
<li>We use more memory to store all the keys and values. For a model with:
<ul>
<li><span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>L</mi></mrow><annotation encoding="application/x-tex">L</annotation></semantics></math></span></span> layers</li>
<li><span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>H</mi></mrow><annotation encoding="application/x-tex">H</annotation></semantics></math></span></span> attention heads</li>
<li>Sequence length <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>n</mi></mrow><annotation encoding="application/x-tex">n</annotation></semantics></math></span></span></li>
<li>Key/value dimension <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>d</mi><mi>k</mi></msub></mrow><annotation encoding="application/x-tex">d_k</annotation></semantics></math></span></span>
The total memory cost is <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>L</mi><mo>×</mo><mi>H</mi><mo>×</mo><mi>n</mi><mo>×</mo><msub><mi>d</mi><mi>k</mi></msub><mo>×</mo><mn>2</mn></mrow><annotation encoding="application/x-tex">L \times H \times n \times d_k \times 2</annotation></semantics></math></span></span> values (the factor of 2 accounts for both keys and values).
This grows linearly with sequence length (<span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>O</mi><mo stretchy="false">(</mo><mi>n</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">O(n)</annotation></semantics></math></span></span>), but the constant factors can be substantial for large models.</li>
</ul>
</li>
<li>But in return, we reduce the computation cost from <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>O</mi><mo stretchy="false">(</mo><msup><mi>n</mi><mn>3</mn></msup><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">O(n^3)</annotation></semantics></math></span></span> to <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>O</mi><mo stretchy="false">(</mo><msup><mi>n</mi><mn>2</mn></msup><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">O(n^2)</annotation></semantics></math></span></span></li>
</ul>
<p>To understand why it's <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>O</mi><mo stretchy="false">(</mo><msup><mi>n</mi><mn>2</mn></msup><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">O(n^2)</annotation></semantics></math></span></span>, let's look at the cost at each step:</p>
<ol>
<li><strong>Step 1</strong>: Process 1 token → cost <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>O</mi><mo stretchy="false">(</mo><mn>1</mn><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">O(1)</annotation></semantics></math></span></span></li>
<li><strong>Step 2</strong>: Process 1 new token + look at 1 cached token → cost <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>O</mi><mo stretchy="false">(</mo><mn>2</mn><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">O(2)</annotation></semantics></math></span></span></li>
<li><strong>Step 3</strong>: Process 1 new token + look at 2 cached tokens → cost <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>O</mi><mo stretchy="false">(</mo><mn>3</mn><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">O(3)</annotation></semantics></math></span></span></li>
<li>And so on...</li>
</ol>
<p>Adding these up:</p>
<span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi>O</mi><mo stretchy="false">(</mo><mn>1</mn><mo>+</mo><mn>2</mn><mo>+</mo><mn>3</mn><mo>+</mo><mo>⋯</mo><mo>+</mo><mi>n</mi><mo stretchy="false">)</mo><mo>=</mo><mi>O</mi><mo stretchy="false">(</mo><msup><mi>n</mi><mn>2</mn></msup><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">O(1 + 2 + 3 + \dots + n) = O(n^2)</annotation></semantics></math></span></span></span>
<p>This is a dramatic improvement over <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>O</mi><mo stretchy="false">(</mo><msup><mi>n</mi><mn>3</mn></msup><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">O(n^3)</annotation></semantics></math></span></span>! While we still have to do the fundamental work of looking at all previous tokens (<span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>O</mi><mo stretchy="false">(</mo><msup><mi>n</mi><mn>2</mn></msup><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">O(n^2)</annotation></semantics></math></span></span>), we avoid the costly recomputation at each step.</p>
<hr>
<h2>The Memory Challenge: Why We Need Better Solutions</h2>
<p>While KV cache is a powerful optimization, it comes with a significant memory cost. Let's look at a concrete example using a modern large language model like Llama3 70B with:</p>
<ul>
<li><span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>L</mi><mo>=</mo><mn>80</mn></mrow><annotation encoding="application/x-tex">L = 80</annotation></semantics></math></span></span> layers</li>
<li><span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>H</mi><mo>=</mo><mn>64</mn></mrow><annotation encoding="application/x-tex">H = 64</annotation></semantics></math></span></span> attention heads</li>
<li><span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>B</mi><mo>=</mo><mn>8</mn></mrow><annotation encoding="application/x-tex">B = 8</annotation></semantics></math></span></span> batch size of 8 sequences</li>
<li><span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>d</mi><mi>k</mi></msub><mo>=</mo><mn>128</mn></mrow><annotation encoding="application/x-tex">d_k = 128</annotation></semantics></math></span></span> key/value dimension</li>
<li>16-bit precision</li>
</ul>
<p>The memory required for a batch of 8 sequences of 1000 tokens each would be:</p>
<span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi>L</mi><mo>×</mo><mi>H</mi><mo>×</mo><mi>B</mi><mo>×</mo><mi>n</mi><mo>×</mo><msub><mi>d</mi><mi>k</mi></msub><mo>×</mo><mn>2</mn><mo>×</mo><mn>2</mn><mtext>&nbsp;bytes</mtext><mo>=</mo><mn>80</mn><mo>×</mo><mn>64</mn><mo>×</mo><mn>8</mn><mo>×</mo><mn>1000</mn><mo>×</mo><mn>128</mn><mo>×</mo><mn>2</mn><mo>×</mo><mn>2</mn><mtext>&nbsp;bytes</mtext><mo>=</mo><mn>20.97</mn><mtext>GB</mtext></mrow><annotation encoding="application/x-tex">L \times H \times B \times n \times d_k \times 2 \times 2 \text{ bytes} = 80 \times 64 \times 8 \times 1000 \times 128 \times 2 \times 2 \text{ bytes} = 20.97\text{GB}</annotation></semantics></math></span></span></span>
<p>This substantial memory usage creates several challenges:</p>
<ol>
<li><strong>Scales linearly</strong> with sequence length</li>
<li><strong>Multiplies</strong> with batch size for parallel processing</li>
<li><strong>Limits</strong> the maximum context length we can handle</li>
<li><strong>Constrains</strong> deployment on memory-limited devices</li>
</ol>
<p>These challenges have sparked a wave of innovation in the research community, leading to various techniques for optimizing KV cache usage. Let's explore these cutting-edge solutions.</p>
<h2>Can we improve over naive KV caches?</h2>
<p>The following papers represent key innovations in KV cache optimization. We'll explore them through three main approaches: token selection, post-hoc compression techniques, and architectural redesigns.</p>
<h2>Token Selection and Pruning Approaches</h2>
<h3>1) <a href="https://arxiv.org/abs/2306.14048">Heavy-Hitter Oracle (H2O)</a></h3>
<p><img src="https://www.pyspur.dev/blog/kv-cache/h2o_alg.png" alt=""></p>
<p>H2O introduces the concept of identifying and preserving important tokens in the KV cache:</p>
<ul>
<li><strong>Heavy-Hitter Tokens</strong>: H2O identifies tokens with the highest accumulated attention scores during generation, following a power-law distribution. These tokens are critical for model functionality and are prioritized in the cache.</li>
<li><strong>Dynamic Submodular Eviction</strong>: The method frames cache management as an optimization problem with a submodular objective function <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>F</mi><mo stretchy="false">(</mo><mi>S</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">F(S)</annotation></semantics></math></span></span> that quantifies the importance of a token set <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>S</mi></mrow><annotation encoding="application/x-tex">S</annotation></semantics></math></span></span>:
<span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi>F</mi><mo stretchy="false">(</mo><mi>S</mi><mo stretchy="false">)</mo><mo>=</mo><munder><mo>∑</mo><mrow><mi>i</mi><mo>∈</mo><mi>S</mi></mrow></munder><msub><mi>A</mi><mi>i</mi></msub></mrow><annotation encoding="application/x-tex">F(S) = \sum_{i \in S} A_{i}</annotation></semantics></math></span></span></span>
where <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>A</mi><mi>i</mi></msub></mrow><annotation encoding="application/x-tex">A_i</annotation></semantics></math></span></span> is the accumulated attention score for token <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>i</mi></mrow><annotation encoding="application/x-tex">i</annotation></semantics></math></span></span>. The cache <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>S</mi><mi>t</mi></msub></mrow><annotation encoding="application/x-tex">S_t</annotation></semantics></math></span></span> is updated by:
<span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><msub><mi>S</mi><mi>t</mi></msub><mo>=</mo><msub><mtext>argmax</mtext><mrow><mi>S</mi><mo>⊆</mo><msub><mi>S</mi><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow></msub><mo>∪</mo><mo stretchy="false">{</mo><mi>i</mi><mo stretchy="false">}</mo><mo separator="true">,</mo><mi mathvariant="normal">∣</mi><mi>S</mi><mi mathvariant="normal">∣</mi><mo>≤</mo><mi>k</mi></mrow></msub><mtext> </mtext><mi>F</mi><mo stretchy="false">(</mo><mi>S</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">S_t = \text{argmax}_{S \subseteq S_{t-1} \cup \{i\}, |S| \leq k} \, F(S)</annotation></semantics></math></span></span></span>
ensuring that at most one token is evicted per step. This greedy algorithm is computationally efficient and guarantees near-optimal performance under submodular constraints.</li>
<li><strong>Results</strong>: Achieves <strong>5× reduction</strong> in KV cache size with negligible accuracy loss and up to <strong>29×</strong> throughput improvement.</li>
</ul>
<h3>2) <a href="https://arxiv.org/abs/2309.17453">StreamLLM</a></h3>
<p><img src="https://www.pyspur.dev/blog/kv-cache/streamingLLM.png" alt=""></p>
<ul>
<li>The authors observe the phenomenon of <strong>Attention Sinks</strong>: Initial tokens that act as natural attention anchors during decoding
<ul>
<li>Without these attention sink tokens, the performance of naive window attention drops</li>
</ul>
</li>
<li>Based on that observation, they introduce a <strong>Rolling Cache</strong> for recent context with retained initial tokens, enabling infinite-length sequence processing.</li>
<li>They show that these sink tokens can also be <strong>trained</strong>; serving as dedicated attention anchors, reducing reliance on multiple initial tokens.</li>
</ul>
<h3>3) <a href="https://arxiv.org/abs/2406.12335">Value-Aware Token Pruning (VATP)</a></h3>
<p><img src="https://www.pyspur.dev/blog/kv-cache/vatp.png" alt=""></p>
<p>VATP extends H2O's token importance concept by considering both attention patterns and value vector properties:</p>
<ul>
<li><strong>Importance Scoring</strong>: Combines attention scores with value vector information:
<span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><msubsup><mi>I</mi><mi>k</mi><mi>t</mi></msubsup><mo>=</mo><msubsup><mi>S</mi><mi>k</mi><mi>t</mi></msubsup><mo>⋅</mo><mi mathvariant="normal">∥</mi><msub><mi>v</mi><mi>k</mi></msub><msub><mi mathvariant="normal">∥</mi><mn>1</mn></msub><mo separator="true">,</mo><mspace width="1em"></mspace><msubsup><mi>S</mi><mi>k</mi><mi>t</mi></msubsup><mo>=</mo><munder><mo>∑</mo><mrow><mi>k</mi><mo>≤</mo><mi>j</mi><mo>≤</mo><mi>t</mi></mrow></munder><msub><mi>a</mi><mrow><mi>j</mi><mo separator="true">,</mo><mi>k</mi></mrow></msub></mrow><annotation encoding="application/x-tex">I_k^t = S_k^t \cdot \|v_k\|_1, \quad S_k^t = \sum_{k \leq j \leq t} a_{j,k}</annotation></semantics></math></span></span></span>
where <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msubsup><mi>S</mi><mi>k</mi><mi>t</mi></msubsup></mrow><annotation encoding="application/x-tex">S_k^t</annotation></semantics></math></span></span> is the accumulated attention score and <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="normal">∥</mi><msub><mi>v</mi><mi>k</mi></msub><msub><mi mathvariant="normal">∥</mi><mn>1</mn></msub></mrow><annotation encoding="application/x-tex">\|v_k\|_1</annotation></semantics></math></span></span> is the value vector's L1 norm.</li>
<li><strong>Token Pruning</strong>: Tokens are ranked by <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msubsup><mi>I</mi><mi>k</mi><mi>t</mi></msubsup></mrow><annotation encoding="application/x-tex">I_k^t</annotation></semantics></math></span></span>, and those with the lowest scores are pruned, while <strong>attention sink tokens</strong> (e.g., start or newline tokens) are preserved to prevent performance degradation.</li>
<li><strong>Performance and Efficiency</strong>:
<ul>
<li>Outperforms baselines like H2O and Scissorhands in 12–14 out of 16 LongBench tasks.</li>
<li>Achieves effective <strong>50% compression</strong> with minimal performance loss.</li>
<li>Introduces negligible computational overhead and is compatible with FlashAttention when integrated with Scissorhands.</li>
</ul>
</li>
</ul>
<h2>Post-hoc Compression Techniques</h2>
<p>These methods compress or optimize the KV cache while preserving the standard transformer architecture.</p>
<h3>4) <a href="https://arxiv.org/pdf/2310.01801">Adaptive KV Compression (FastGen)</a></h3>
<p><img src="https://www.pyspur.dev/blog/kv-cache/fastgen_1.png" alt=""></p>
<p>FastGen introduces adaptive compression based on attention patterns observed at run-time:</p>
<ul>
<li><strong>Attention Profiling</strong>: during prompt encoding, FastGen identifies attention patterns and selects compression policies <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>C</mi><mo>∗</mo></msup></mrow><annotation encoding="application/x-tex">C^*</annotation></semantics></math></span></span> that minimize memory cost while preserving attention recovery:
<span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><msup><mi>C</mi><mo>∗</mo></msup><mo>=</mo><mi>arg</mi><mo>⁡</mo><munder><mrow><mi>min</mi><mo>⁡</mo></mrow><mrow><mi>C</mi><mo>∈</mo><mi mathvariant="script">C</mi></mrow></munder><mtext>CacheMemoryCost</mtext><mo stretchy="false">(</mo><mi>C</mi><mo stretchy="false">)</mo><mspace width="1em"></mspace><mtext>s.t.</mtext><mspace width="1em"></mspace><mi mathvariant="normal">∥</mi><mi>A</mi><mo>−</mo><mtext>softmax</mtext><mo stretchy="false">(</mo><mi>Q</mi><msubsup><mi>K</mi><mi>C</mi><mi>T</mi></msubsup><mo stretchy="false">)</mo><mi mathvariant="normal">∥</mi><mo>≤</mo><mn>1</mn><mo>−</mo><mi>T</mi><mi mathvariant="normal">.</mi></mrow><annotation encoding="application/x-tex">C^* = \arg\min_{C \in \mathcal{C}} \text{CacheMemoryCost}(C) \quad \text{s.t.} \quad \|A - \text{softmax}(QK_C^T)\| \leq 1 - T.</annotation></semantics></math></span></span></span>
</li>
<li><strong>Adaptive Compression Policies</strong>:
<ul>
<li>Compression strategies include:
<ul>
<li><strong>Special Tokens</strong> (<span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>C</mi><mtext>special</mtext></msub></mrow><annotation encoding="application/x-tex">C_{\text{special}}</annotation></semantics></math></span></span>): Retain only special tokens.</li>
<li><strong>Locality</strong> (<span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>C</mi><mtext>local</mtext></msub></mrow><annotation encoding="application/x-tex">C_{\text{local}}</annotation></semantics></math></span></span>): Evict tokens beyond a relative distance <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>r</mi><mi>l</mi></msub></mrow><annotation encoding="application/x-tex">r_l</annotation></semantics></math></span></span>.</li>
<li><strong>Frequency</strong> (<span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>C</mi><mtext>frequent</mtext></msub></mrow><annotation encoding="application/x-tex">C_{\text{frequent}}</annotation></semantics></math></span></span>): Keep tokens with high cumulative attention scores (<span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>r</mi><mi>f</mi></msub></mrow><annotation encoding="application/x-tex">r_f</annotation></semantics></math></span></span>).</li>
<li><strong>Hybrid Policies</strong> combine strategies, starting with <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>C</mi><mtext>special</mtext></msub></mrow><annotation encoding="application/x-tex">C_{\text{special}}</annotation></semantics></math></span></span>, and applies them adaptively to each head:
<span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi mathvariant="script">C</mi><mo>=</mo><mo stretchy="false">{</mo><msub><mi>C</mi><mtext>special</mtext></msub><mo separator="true">,</mo><msub><mi>C</mi><mtext>special</mtext></msub><mo>+</mo><msub><mi>C</mi><mtext>punct</mtext></msub><mo separator="true">,</mo><mo>…</mo><mo separator="true">,</mo><msub><mi>C</mi><mtext>full</mtext></msub><mo stretchy="false">}</mo><mi mathvariant="normal">.</mi></mrow><annotation encoding="application/x-tex">\mathcal{C} = \{C_{\text{special}}, C_{\text{special}} + C_{\text{punct}}, \ldots, C_{\text{full}}\}.</annotation></semantics></math></span></span></span>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<ol start="4">
<li><strong>Token Generation</strong>:
<ul>
<li>During decoding, pre-selected compression policies manage the KV cache efficiently:
<span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><msub><mi>K</mi><msub><mi>C</mi><mi>i</mi></msub></msub><mo separator="true">,</mo><msub><mi>V</mi><msub><mi>C</mi><mi>i</mi></msub></msub><mo>=</mo><mi>f</mi><mo stretchy="false">(</mo><mi>K</mi><mo separator="true">,</mo><mi>V</mi><mo separator="true">,</mo><msub><mi>C</mi><mi>i</mi></msub><mo stretchy="false">)</mo><mi mathvariant="normal">.</mi></mrow><annotation encoding="application/x-tex">K_{C_i}, V_{C_i} = f(K, V, C_i).</annotation></semantics></math></span></span></span>
</li>
</ul>
</li>
</ol>
<h3>5) <a href="https://arxiv.org/pdf/2403.09636">Dynamic Memory Compression (DMC)</a></h3>
<p><img src="https://www.pyspur.dev/blog/kv-cache/dmc.png" alt=""></p>
<p>DMC introduces adaptive token merging:</p>
<ul>
<li><strong>Decision Mechanism</strong>: At time <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>t</mi></mrow><annotation encoding="application/x-tex">t</annotation></semantics></math></span></span>, predicts merge decisions <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>α</mi><mi>t</mi></msub></mrow><annotation encoding="application/x-tex">\alpha_t</annotation></semantics></math></span></span> and weights <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>ω</mi><mi>t</mi></msub></mrow><annotation encoding="application/x-tex">\omega_t</annotation></semantics></math></span></span>:
<span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><msub><mi>α</mi><mi>t</mi></msub><mo>=</mo><mo stretchy="false">⌊</mo><mtext>sigmoid</mtext><mo stretchy="false">(</mo><msub><mi>k</mi><mi>t</mi></msub><mo stretchy="false">[</mo><mn>0</mn><mo stretchy="false">]</mo><mo stretchy="false">)</mo><mo stretchy="false">⌉</mo><mo separator="true">,</mo><mspace width="1em"></mspace><msub><mi>ω</mi><mi>t</mi></msub><mo>=</mo><mtext>sigmoid</mtext><mo stretchy="false">(</mo><msub><mi>q</mi><mi>t</mi></msub><mo stretchy="false">[</mo><mn>0</mn><mo stretchy="false">]</mo><mo stretchy="false">)</mo><mi mathvariant="normal">.</mi></mrow><annotation encoding="application/x-tex">\alpha_t = \lfloor \text{sigmoid}(k_t[0]) \rceil, \quad \omega_t = \text{sigmoid}(q_t[0]).</annotation></semantics></math></span></span></span>
</li>
<li><strong>Weighted Merging</strong>: When <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>α</mi><mi>t</mi></msub><mo>=</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">\alpha_t = 1</annotation></semantics></math></span></span>, merges current and previous entries:
<span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><msup><mi>k</mi><mo mathvariant="normal" lspace="0em" rspace="0em">′</mo></msup><mo>=</mo><mfrac><mrow><msub><mi>ω</mi><mi>t</mi></msub><msub><mi>k</mi><mi>t</mi></msub><mo>+</mo><msub><mi>z</mi><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow></msub><msub><mi>k</mi><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow></msub></mrow><mrow><msub><mi>ω</mi><mi>t</mi></msub><mo>+</mo><msub><mi>z</mi><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow></msub></mrow></mfrac><mo separator="true">,</mo><mspace width="1em"></mspace><msup><mi>v</mi><mo mathvariant="normal" lspace="0em" rspace="0em">′</mo></msup><mo>=</mo><mfrac><mrow><msub><mi>ω</mi><mi>t</mi></msub><msub><mi>v</mi><mi>t</mi></msub><mo>+</mo><msub><mi>z</mi><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow></msub><msub><mi>v</mi><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow></msub></mrow><mrow><msub><mi>ω</mi><mi>t</mi></msub><mo>+</mo><msub><mi>z</mi><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow></msub></mrow></mfrac><mo separator="true">,</mo></mrow><annotation encoding="application/x-tex">k' = \frac{\omega_t k_t + z_{t-1} k_{t-1}}{\omega_t + z_{t-1}}, \quad v' = \frac{\omega_t v_t + z_{t-1} v_{t-1}}{\omega_t + z_{t-1}},</annotation></semantics></math></span></span></span>
where <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>z</mi><mi>t</mi></msub><mo>=</mo><msub><mi>z</mi><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow></msub><mo>+</mo><msub><mi>ω</mi><mi>t</mi></msub></mrow><annotation encoding="application/x-tex">z_t = z_{t-1} + \omega_t</annotation></semantics></math></span></span> accumulates importance weights.</li>
<li><strong>Training</strong>:
<ul>
<li>Uses a Gumbel-Sigmoid relaxation for <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>α</mi><mi>t</mi></msub></mrow><annotation encoding="application/x-tex">\alpha_t</annotation></semantics></math></span></span> to allow end-to-end training with gradient descent:
<span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><msub><mi>α</mi><mi>t</mi></msub><mo>∼</mo><mtext>Gumbel-Sigmoid</mtext><mo stretchy="false">(</mo><msub><mi>k</mi><mi>t</mi></msub><mo stretchy="false">[</mo><mn>0</mn><mo stretchy="false">]</mo><mo separator="true">,</mo><mi>τ</mi><mo stretchy="false">)</mo><mo separator="true">,</mo></mrow><annotation encoding="application/x-tex">\alpha_t \sim \text{Gumbel-Sigmoid}(k_t[0], \tau),</annotation></semantics></math></span></span></span>
where <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>τ</mi></mrow><annotation encoding="application/x-tex">\tau</annotation></semantics></math></span></span> is a temperature parameter.</li>
<li>Optimizes a combined objective:
<span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi mathvariant="script">L</mi><mo>=</mo><msub><mi mathvariant="script">L</mi><mtext>LM</mtext></msub><mo>+</mo><mi>λ</mi><mi>max</mi><mo>⁡</mo><mrow><mo fence="true">(</mo><mn>0</mn><mo separator="true">,</mo><mfrac><mi>n</mi><mtext>CR</mtext></mfrac><mo>−</mo><munder><mo>∑</mo><mi>t</mi></munder><msub><mi>α</mi><mi>t</mi></msub><mo fence="true">)</mo></mrow><mo separator="true">,</mo></mrow><annotation encoding="application/x-tex">\mathcal{L} = \mathcal{L}_{\text{LM}} + \lambda \max\left(0, \frac{n}{\text{CR}} - \sum_{t} \alpha_t \right),</annotation></semantics></math></span></span></span>
where <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi mathvariant="script">L</mi><mtext>LM</mtext></msub></mrow><annotation encoding="application/x-tex">\mathcal{L}_{\text{LM}}</annotation></semantics></math></span></span> is the language modeling loss, and the second term encourages the model to match a target compression ratio (CR).</li>
</ul>
</li>
<li><strong>Results</strong>: Up to <strong>8× compression</strong> with maintained performance.</li>
</ul>
<h3>6) <a href="https://arxiv.org/pdf/2406.11430"><span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>L</mi><mn>2</mn></msub></mrow><annotation encoding="application/x-tex">L_2</annotation></semantics></math></span></span> Norm-Based Compression</a></h3>
<p><img src="https://www.pyspur.dev/blog/kv-cache/l2.png" alt=""></p>
<p>This paper presents a surprising observation: A clear correlation between the <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>L</mi><mn>2</mn></msub></mrow><annotation encoding="application/x-tex">L_2</annotation></semantics></math></span></span> norm and the attention scores over cached KV pairs, where a low <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>L</mi><mn>2</mn></msub></mrow><annotation encoding="application/x-tex">L_2</annotation></semantics></math></span></span> norm of a key embedding usually leads to a high attention score during decoding. Consequently, they introduce a simple but effective compression objective:</p>
<ul>
<li><strong>Norm-Based Selection</strong>: For a set of cached keys <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>K</mi><mo>=</mo><mo stretchy="false">{</mo><msub><mi>k</mi><mn>1</mn></msub><mo separator="true">,</mo><msub><mi>k</mi><mn>2</mn></msub><mo separator="true">,</mo><mo>…</mo><mo separator="true">,</mo><msub><mi>k</mi><mi>n</mi></msub><mo stretchy="false">}</mo></mrow><annotation encoding="application/x-tex">K = \{k_1, k_2, \dots, k_n\}</annotation></semantics></math></span></span>, computes and sorts key norms:
<span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi mathvariant="normal">∥</mi><msub><mi>k</mi><mi>i</mi></msub><msub><mi mathvariant="normal">∥</mi><mn>2</mn></msub><mo>=</mo><msqrt><mrow><munderover><mo>∑</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mi>d</mi></munderover><msubsup><mi>k</mi><mrow><mi>i</mi><mo separator="true">,</mo><mi>j</mi></mrow><mn>2</mn></msubsup></mrow></msqrt></mrow><annotation encoding="application/x-tex">\|k_i\|_2 = \sqrt{\sum_{j=1}^d k_{i,j}^2}</annotation></semantics></math></span></span></span>
</li>
<li><strong>Sorting and Selection</strong>: To compress the KV cache, sort all keys by their L2 norm values:
<span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><msub><mi>K</mi><mtext>sorted</mtext></msub><mo>=</mo><mtext>Sort</mtext><mo fence="false" stretchy="true" minsize="1.2em" maxsize="1.2em">(</mo><mo stretchy="false">{</mo><mi mathvariant="normal">∥</mi><msub><mi>k</mi><mn>1</mn></msub><msub><mi mathvariant="normal">∥</mi><mn>2</mn></msub><mo separator="true">,</mo><mi mathvariant="normal">∥</mi><msub><mi>k</mi><mn>2</mn></msub><msub><mi mathvariant="normal">∥</mi><mn>2</mn></msub><mo separator="true">,</mo><mo>…</mo><mo separator="true">,</mo><mi mathvariant="normal">∥</mi><msub><mi>k</mi><mi>n</mi></msub><msub><mi mathvariant="normal">∥</mi><mn>2</mn></msub><mo stretchy="false">}</mo><mo fence="false" stretchy="true" minsize="1.2em" maxsize="1.2em">)</mo></mrow><annotation encoding="application/x-tex">K_{\text{sorted}} = \text{Sort}\big(\{\|k_1\|_2, \|k_2\|_2, \dots, \|k_n\|_2\}\big)</annotation></semantics></math></span></span></span>
Retain the top-<span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>m</mi></mrow><annotation encoding="application/x-tex">m</annotation></semantics></math></span></span> keys with lowest norms, where <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>m</mi><mo>=</mo><mo stretchy="false">⌊</mo><mi>c</mi><mo>⋅</mo><mi>n</mi><mo stretchy="false">⌋</mo></mrow><annotation encoding="application/x-tex">m = \lfloor c \cdot n \rfloor</annotation></semantics></math></span></span> and <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>c</mi></mrow><annotation encoding="application/x-tex">c</annotation></semantics></math></span></span> is the compression ratio.</li>
<li><strong>Compressed Cache</strong>: The compressed key-value cache consists of:
<span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><msub><mi>K</mi><mtext>compressed</mtext></msub><mo>=</mo><mo stretchy="false">{</mo><msub><mi>k</mi><mi>i</mi></msub><mo>∣</mo><mi mathvariant="normal">∥</mi><msub><mi>k</mi><mi>i</mi></msub><msub><mi mathvariant="normal">∥</mi><mn>2</mn></msub><mo>∈</mo><msub><mi>K</mi><mtext>sorted</mtext></msub><mo stretchy="false">[</mo><mn>1</mn><mo>:</mo><mi>m</mi><mo stretchy="false">]</mo><mo stretchy="false">}</mo><mo separator="true">,</mo><mspace width="1em"></mspace><msub><mi>V</mi><mtext>compressed</mtext></msub><mo>=</mo><mo stretchy="false">{</mo><msub><mi>v</mi><mi>i</mi></msub><mo>∣</mo><msub><mi>k</mi><mi>i</mi></msub><mo>∈</mo><msub><mi>K</mi><mtext>compressed</mtext></msub><mo stretchy="false">}</mo></mrow><annotation encoding="application/x-tex">K_{\text{compressed}} = \{k_i \mid \|k_i\|_2 \in K_{\text{sorted}}[1:m]\}, \quad V_{\text{compressed}} = \{v_i \mid k_i \in K_{\text{compressed}}\}</annotation></semantics></math></span></span></span>
</li>
<li>Due to its simplicity, this approach maintains compatibility with <strong>FlashAttention</strong>.</li>
</ul>
<h2>Architectural Redesigns</h2>
<p>These approaches change the Transformers architecture to handle KV caches more efficiently, often incorporating compression directly into the architecture.</p>
<h3>7) <a href="https://arxiv.org/pdf/2305.13245">Multi-Query Attention (MQA)</a></h3>
<p><img src="https://www.pyspur.dev/blog/kv-cache/mqa.png" alt=""></p>
<ul>
<li><strong>Key Idea</strong>: MQA reduces the KV cache size by sharing a <strong>single key-value head</strong> across all query heads, replacing the traditional Multi-Head Attention (MHA):
<span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi>K</mi><mo>=</mo><mi>X</mi><msub><mi>W</mi><mi>K</mi></msub><mo separator="true">,</mo><mspace width="1em"></mspace><mi>V</mi><mo>=</mo><mi>X</mi><msub><mi>W</mi><mi>V</mi></msub><mo separator="true">,</mo></mrow><annotation encoding="application/x-tex">K = XW_K, \quad V = XW_V,</annotation></semantics></math></span></span></span>
where <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>K</mi></mrow><annotation encoding="application/x-tex">K </annotation></semantics></math></span></span> and <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>V</mi></mrow><annotation encoding="application/x-tex">V </annotation></semantics></math></span></span> are the shared key and value projections.</li>
<li><strong>Benefits</strong>: Reduces the KV cache size by a factor of <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>H</mi></mrow><annotation encoding="application/x-tex">H </annotation></semantics></math></span></span> (the number of attention heads), significantly lowering memory bandwidth overhead.</li>
<li><strong>Trade-Off</strong>: While MQA is faster, it often suffers from <strong>quality degradation</strong>, especially in tasks requiring diverse attention patterns.</li>
</ul>
<h3>8) <a href="https://arxiv.org/abs/2305.13245">Group-Query Attention (GQA)</a></h3>
<p><img src="https://www.pyspur.dev/blog/kv-cache/gqa.png" alt=""></p>
<ul>
<li><strong>Key Idea</strong>: GQA interpolates between full multi-head attention and MQA to offering a scalable trade-off between inference speed and model quality. It divides query heads into <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>G</mi></mrow><annotation encoding="application/x-tex">G </annotation></semantics></math></span></span> groups, where each group shares a single key-value head:
<span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><msub><mi>K</mi><mtext>group</mtext></msub><mo>=</mo><mfrac><mn>1</mn><mrow><mi mathvariant="normal">∣</mi><mi>G</mi><mi mathvariant="normal">∣</mi></mrow></mfrac><munder><mo>∑</mo><mrow><mi>h</mi><mo>∈</mo><mi>G</mi></mrow></munder><msub><mi>K</mi><mi>h</mi></msub><mo separator="true">,</mo><mspace width="1em"></mspace><msub><mi>V</mi><mtext>group</mtext></msub><mo>=</mo><mfrac><mn>1</mn><mrow><mi mathvariant="normal">∣</mi><mi>G</mi><mi mathvariant="normal">∣</mi></mrow></mfrac><munder><mo>∑</mo><mrow><mi>h</mi><mo>∈</mo><mi>G</mi></mrow></munder><msub><mi>V</mi><mi>h</mi></msub></mrow><annotation encoding="application/x-tex">K_{\text{group}} = \frac{1}{|G|} \sum_{h \in G} K_h, \quad V_{\text{group}} = \frac{1}{|G|} \sum_{h \in G} V_h</annotation></semantics></math></span></span></span>
<ul>
<li><strong>GQA-1</strong>: Equivalent to MQA (<span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>G</mi><mo>=</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">G = 1 </annotation></semantics></math></span></span>).</li>
<li><strong>GQA-<span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>H</mi></mrow><annotation encoding="application/x-tex">H </annotation></semantics></math></span></span></strong>: Equivalent to MHA (<span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>G</mi><mo>=</mo><mi>H</mi></mrow><annotation encoding="application/x-tex">G = H </annotation></semantics></math></span></span>).</li>
</ul>
</li>
<li><strong>Uptraining</strong>: GQA can be introduced to existing pre-trained models through fine-tuning:
<ul>
<li>First, convert MHA checkpoints to GQA by <strong>mean pooling</strong> key and value heads into groups</li>
<li>Then fine-tune ("uptrain") the model briefly to adapt to the new attention pattern</li>
<li>This adaptation process requires only <strong>5% of the original pre-training compute</strong>, making it very efficient</li>
<li>The resulting model maintains quality while gaining GQA's memory benefits</li>
</ul>
</li>
</ul>
<h3>9) <a href="https://arxiv.org/abs/2405.04434">Multi-head Latent Attention (MLA)</a></h3>
<p><img src="https://www.pyspur.dev/blog/kv-cache/mla.png" alt=""></p>
<p>DeepSeek's <strong>Multi-Head Latent Attention (MLA)</strong> takes a novel approach to reducing KV cache overhead. While MQA and GQA achieve this through head-sharing, MLA instead employs a <strong>low-rank latent compression</strong> technique that maintains the benefits of multiple attention heads.</p>
<ul>
<li>MLA reduces KV cache size by compressing keys and values into low-dimensional latent vectors before reconstruction.</li>
<li>It down-project key-value embeddings into a compressed latent space:
<span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><msub><mi>c</mi><mrow><mtext>KV</mtext><mo separator="true">,</mo><mi>t</mi></mrow></msub><mo>=</mo><msub><mi>W</mi><mtext>DKV</mtext></msub><msub><mi>h</mi><mi>t</mi></msub><mo separator="true">,</mo><mspace width="1em"></mspace><msub><mi>k</mi><mi>C</mi></msub><mo>=</mo><msub><mi>W</mi><mtext>UK</mtext></msub><msub><mi>c</mi><mrow><mtext>KV</mtext><mo separator="true">,</mo><mi>t</mi></mrow></msub><mo separator="true">,</mo><mspace width="1em"></mspace><msub><mi>v</mi><mi>C</mi></msub><mo>=</mo><msub><mi>W</mi><mtext>UV</mtext></msub><msub><mi>c</mi><mrow><mtext>KV</mtext><mo separator="true">,</mo><mi>t</mi></mrow></msub></mrow><annotation encoding="application/x-tex">c_{\text{KV}, t} = W_{\text{DKV}} h_t, \quad k_C = W_{\text{UK}} c_{\text{KV}, t}, \quad v_C = W_{\text{UV}} c_{\text{KV}, t}</annotation></semantics></math></span></span></span>
where <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>W</mi><mtext>DKV</mtext></msub></mrow><annotation encoding="application/x-tex">W_{\text{DKV}}</annotation></semantics></math></span></span> is the down-projection matrix, and <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>W</mi><mtext>UK</mtext></msub></mrow><annotation encoding="application/x-tex">W_{\text{UK}}</annotation></semantics></math></span></span>, <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>W</mi><mtext>UV</mtext></msub></mrow><annotation encoding="application/x-tex">W_{\text{UV}}</annotation></semantics></math></span></span> are up-projection matrices for keys and values.</li>
<li>It retains per-head flexibility through compressed representations, unlike MQA's complete head sharing.</li>
<li>It introduces <strong>Rotary Positional Embeddings (RoPE)</strong> for decoupling position-aware keys:
<span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><msub><mi>k</mi><mi>R</mi></msub><mo>=</mo><mtext>RoPE</mtext><mo stretchy="false">(</mo><msub><mi>W</mi><mrow><mi>K</mi><mi>R</mi></mrow></msub><msub><mi>h</mi><mi>t</mi></msub><mo stretchy="false">)</mo><mo separator="true">,</mo><mspace width="1em"></mspace><msub><mi>k</mi><mi>t</mi></msub><mo>=</mo><mo stretchy="false">[</mo><msub><mi>k</mi><mi>C</mi></msub><mo separator="true">;</mo><msub><mi>k</mi><mi>R</mi></msub><mo stretchy="false">]</mo></mrow><annotation encoding="application/x-tex">k_R = \text{RoPE}(W_{KR} h_t), \quad k_t = [k_C; k_R]</annotation></semantics></math></span></span></span>
This reduces KV cache storage further by caching only compressed latent vectors <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>c</mi><mtext>KV</mtext></msub></mrow><annotation encoding="application/x-tex">c_{\text{KV}}</annotation></semantics></math></span></span> and positional keys <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>k</mi><mi>R</mi></msub></mrow><annotation encoding="application/x-tex">k_R</annotation></semantics></math></span></span>.</li>
</ul>
<h3>10) <a href="https://arxiv.org/pdf/2404.14469">SnapKV</a></h3>
<p><img src="https://www.pyspur.dev/blog/kv-cache/snapKV.png" alt=""></p>
<ul>
<li>SnapKV introduces an <strong>Observation Window</strong>: Uses end-of-prompt tokens to identify attention patterns:
<span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi>C</mi><mo>=</mo><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>0</mn></mrow><msub><mi>L</mi><mtext>obs</mtext></msub></munderover><msub><mi>W</mi><mtext>obs</mtext></msub><mo stretchy="false">[</mo><mo>:</mo><mo separator="true">,</mo><mi>i</mi><mo separator="true">,</mo><mo>:</mo><mo stretchy="false">]</mo><mo separator="true">,</mo><mspace width="1em"></mspace><mi>I</mi><mo>=</mo><msub><mtext>Top</mtext><mi>k</mi></msub><mo stretchy="false">(</mo><mi>C</mi><mo separator="true">,</mo><mi>k</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">C = \sum_{i=0}^{L_{\text{obs}}} W_{\text{obs}}[:, i, :], \quad I = \text{Top}_k(C, k)</annotation></semantics></math></span></span></span>
where <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>W</mi><mtext>obs</mtext></msub></mrow><annotation encoding="application/x-tex">W_{\text{obs}}</annotation></semantics></math></span></span> represents the attention weights, and <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>k</mi></mrow><annotation encoding="application/x-tex">k</annotation></semantics></math></span></span> is determined by the compression rate.</li>
<li><strong>Compression</strong>: Clusters features around the selected positions using a pooling layer to preserve context completeness.</li>
</ul>
<h3>11) <a href="https://arxiv.org/pdf/2405.05254">You Only Cache Once (YOCO)</a></h3>
<p><img src="https://www.pyspur.dev/blog/kv-cache/yoco_2.png" alt=""></p>
<p>YOCO modifies the transformer architecture for caching:</p>
<ul>
<li><strong>Global Cache</strong>: Uses a decoder-decoder design with a single shared KV cache.</li>
<li><strong>Complexity Reduction</strong>: Reduces memory from <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>O</mi><mo stretchy="false">(</mo><mi>N</mi><mo>×</mo><mi>L</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">O(N \times L)</annotation></semantics></math></span></span> to <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>O</mi><mo stretchy="false">(</mo><mi>N</mi><mo>+</mo><mi>L</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">O(N + L)</annotation></semantics></math></span></span>, where <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>N</mi></mrow><annotation encoding="application/x-tex">N</annotation></semantics></math></span></span> is sequence length and <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>L</mi></mrow><annotation encoding="application/x-tex">L</annotation></semantics></math></span></span> is the number of layers.</li>
<li><strong>Efficient Attention</strong>: The self-decoder employs <strong>sliding-window attention</strong> or <strong>gated retention</strong>, enabling constant memory usage (<span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>O</mi><mo stretchy="false">(</mo><mi>C</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">O(C)</annotation></semantics></math></span></span>, where <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>C</mi></mrow><annotation encoding="application/x-tex">C</annotation></semantics></math></span></span> is a small window size).</li>
</ul>
<hr>
<h2>Conclusion</h2>
<p>Key-Value caching techniques are central to scaling and optimizing Transformer-based models for real-world use. Innovations like dynamic eviction, compression, and structured approximations continue to push the boundaries on what is possible in long-context or resource-constrained scenarios. KV caching remains a lively research area, offering both theoretical insights and practical improvements.</p>
<p>PS: This blog post is mostly AI-generated using a PySpur workflow with minor human edits.</p></article></div></section></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Questions censored by DeepSeek (319 pts)]]></title>
            <link>https://www.promptfoo.dev/blog/deepseek-censorship/</link>
            <guid>42858552</guid>
            <pubDate>Tue, 28 Jan 2025 21:54:36 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.promptfoo.dev/blog/deepseek-censorship/">https://www.promptfoo.dev/blog/deepseek-censorship/</a>, See on <a href="https://news.ycombinator.com/item?id=42858552">Hacker News</a></p>
Couldn't get https://www.promptfoo.dev/blog/deepseek-censorship/: Error: timeout of 10000ms exceeded]]></description>
        </item>
        <item>
            <title><![CDATA[Show HN: DeepSeek Your HN Profile (104 pts)]]></title>
            <link>https://hn-wrapped.kadoa.com/</link>
            <guid>42857604</guid>
            <pubDate>Tue, 28 Jan 2025 20:35:06 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://hn-wrapped.kadoa.com/">https://hn-wrapped.kadoa.com/</a>, See on <a href="https://news.ycombinator.com/item?id=42857604">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><h2>Hacker News Wrapped</h2><p>Let DeepSeek analyze your HN profile to give you highlights and trends.</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Parkinsons patient "feels cured" with new adaptive deep brain stimulation device (207 pts)]]></title>
            <link>https://www.bbc.com/news/articles/ckgn49r069wo</link>
            <guid>42857293</guid>
            <pubDate>Tue, 28 Jan 2025 20:07:12 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.bbc.com/news/articles/ckgn49r069wo">https://www.bbc.com/news/articles/ckgn49r069wo</a>, See on <a href="https://news.ycombinator.com/item?id=42857293">Hacker News</a></p>
<div id="readability-page-1" class="page"><article><div data-testid="byline-new" data-component="byline-block"><p><span>Sharon Barbour</span></p><p><span>BBC North East &amp; Cumbria health correspondent<!-- --></span></p></div><figure><div data-component="image-block"><p><img src="https://www.bbc.com/bbcx/grey-placeholder.png"><img sizes="(min-width: 1280px) 50vw, (min-width: 1008px) 66vw, 96vw" srcset="https://ichef.bbci.co.uk/news/240/cpsprodpb/0c97/live/1d017000-dccb-11ef-8633-c742f1bc9aea.jpg.webp 240w,https://ichef.bbci.co.uk/news/320/cpsprodpb/0c97/live/1d017000-dccb-11ef-8633-c742f1bc9aea.jpg.webp 320w,https://ichef.bbci.co.uk/news/480/cpsprodpb/0c97/live/1d017000-dccb-11ef-8633-c742f1bc9aea.jpg.webp 480w,https://ichef.bbci.co.uk/news/640/cpsprodpb/0c97/live/1d017000-dccb-11ef-8633-c742f1bc9aea.jpg.webp 640w,https://ichef.bbci.co.uk/news/800/cpsprodpb/0c97/live/1d017000-dccb-11ef-8633-c742f1bc9aea.jpg.webp 800w,https://ichef.bbci.co.uk/news/1024/cpsprodpb/0c97/live/1d017000-dccb-11ef-8633-c742f1bc9aea.jpg.webp 1024w,https://ichef.bbci.co.uk/news/1536/cpsprodpb/0c97/live/1d017000-dccb-11ef-8633-c742f1bc9aea.jpg.webp 1536w" src="https://ichef.bbci.co.uk/news/480/cpsprodpb/0c97/live/1d017000-dccb-11ef-8633-c742f1bc9aea.jpg.webp" alt="BBC Kevin Hill is sitting on his sofa and has opened his blue shirt to show a lump in his chest. This is where a small computer has been implanted. It is connected to wires that go deep into his brain, to control his Parkinson's disease.    "><span>BBC</span></p></div><p data-component="caption-block"><figcaption>Kevin Hill said he is able to go for days now without thinking about his Parkinson's<!-- --></figcaption></p></figure><div data-component="text-block"><p>A man fitted with a pioneering, computer-controlled brain implant to tackle his Parkinson's disease says it works so well he is sometimes able to forget he has the condition.<!-- --></p><p>A small computer inserted into Kevin Hill's chest wall 12 months ago is connected to wires running into the brain which can send electrical signals and an update means it can now read his brain activity.<!-- --></p><p>The 65-year-old from Sunderland said it has been so successful he feels like he has "been cured".<!-- --></p><p>Surgeons in Newcastle hope an adapted version of the deep brain stimulation system will have a "huge impact" on the quality of life of patients with the disease.<!-- --></p></div><p>Mr Hill said: "I forget about Parkinson's for days and days and days."<!-- --></p><p data-component="subheadline-block"><h2>Kitchen ban<!-- --></h2></p><p><i id="warning---contains-a-distressing-image"><b id="warning---contains-a-distressing-image">Warning - contains a distressing image<!-- --></b></i></p><div data-component="text-block"><p>He began getting symptoms, including trembling in his thumb, in his 40s and started suffering nightmares and insomnia.<!-- --></p><p>He was banned by his wife from going into the kitchen because his hand shook so much he spilled or dropped hot drinks and even cut the end of his finger off.<!-- --></p><p>In 2017 he visited his GP and was diagnosed with Parkinson's.<!-- --></p><p>He was told there were medicines but no cure, but there was a new treatment – deep brain stimulation (DBS) – and tests proved he was suitable for the surgery.<!-- --></p><p>It involved an implant that runs deep into the brain to an area the size of a grain of rice. <!-- --></p></div><figure><div data-component="image-block"><p><img src="https://www.bbc.com/bbcx/grey-placeholder.png"><img sizes="(min-width: 1280px) 50vw, (min-width: 1008px) 66vw, 96vw" srcset="https://ichef.bbci.co.uk/news/240/cpsprodpb/fd5a/live/8e66be50-dcc9-11ef-8633-c742f1bc9aea.jpg.webp 240w,https://ichef.bbci.co.uk/news/320/cpsprodpb/fd5a/live/8e66be50-dcc9-11ef-8633-c742f1bc9aea.jpg.webp 320w,https://ichef.bbci.co.uk/news/480/cpsprodpb/fd5a/live/8e66be50-dcc9-11ef-8633-c742f1bc9aea.jpg.webp 480w,https://ichef.bbci.co.uk/news/640/cpsprodpb/fd5a/live/8e66be50-dcc9-11ef-8633-c742f1bc9aea.jpg.webp 640w,https://ichef.bbci.co.uk/news/800/cpsprodpb/fd5a/live/8e66be50-dcc9-11ef-8633-c742f1bc9aea.jpg.webp 800w,https://ichef.bbci.co.uk/news/1024/cpsprodpb/fd5a/live/8e66be50-dcc9-11ef-8633-c742f1bc9aea.jpg.webp 1024w,https://ichef.bbci.co.uk/news/1536/cpsprodpb/fd5a/live/8e66be50-dcc9-11ef-8633-c742f1bc9aea.jpg.webp 1536w" src="https://ichef.bbci.co.uk/news/480/cpsprodpb/fd5a/live/8e66be50-dcc9-11ef-8633-c742f1bc9aea.jpg.webp" alt="NEWCASTLE HOSPITALS Kevin is with a nurse at hospital as the new system is re-programmed and switched on. They are both looking at a computer screen which is wired to his chest. "><span>NEWCASTLE HOSPITALS</span></p></div><p data-component="caption-block"><figcaption>Mr Hill originally had to go to hospital to have the system reprogrammed, but with updates it can now do that automatically<!-- --></figcaption></p></figure><div data-component="text-block"><p>The computer in his chest is connected to two thin wires that thread up the back of his neck. <!-- --></p><p>It carries the electrical messages that can manage his Parkinson's symptoms.<!-- --></p><p>Mr Hill described the computer as the size and shape of "a Jaffa Cake".<!-- --></p><p>When it was switched on after surgery he said the impact was dramatic.<!-- --></p><p>After years of sleepless nights, and being unable to manage the uncontrollable shaking of his arm and leg, his tremors "stopped instantly".<!-- --></p><p>Mr Hill said he stared at his still hand and "couldn't believe it". His wife burst into tears.<!-- --></p><p>The life he once knew came back, meaning he was able to go to the pub and see his friends again.<!-- --></p><p>He bought a bike and was even allowed back into the kitchen.<!-- --></p></div><figure><div data-component="image-block"><p><img src="https://www.bbc.com/bbcx/grey-placeholder.png"><img sizes="(min-width: 1280px) 50vw, (min-width: 1008px) 66vw, 96vw" srcset="https://ichef.bbci.co.uk/news/240/cpsprodpb/a235/live/2b93b270-dc32-11ef-acf7-0d8b5f8aaf71.jpg.webp 240w,https://ichef.bbci.co.uk/news/320/cpsprodpb/a235/live/2b93b270-dc32-11ef-acf7-0d8b5f8aaf71.jpg.webp 320w,https://ichef.bbci.co.uk/news/480/cpsprodpb/a235/live/2b93b270-dc32-11ef-acf7-0d8b5f8aaf71.jpg.webp 480w,https://ichef.bbci.co.uk/news/640/cpsprodpb/a235/live/2b93b270-dc32-11ef-acf7-0d8b5f8aaf71.jpg.webp 640w,https://ichef.bbci.co.uk/news/800/cpsprodpb/a235/live/2b93b270-dc32-11ef-acf7-0d8b5f8aaf71.jpg.webp 800w,https://ichef.bbci.co.uk/news/1024/cpsprodpb/a235/live/2b93b270-dc32-11ef-acf7-0d8b5f8aaf71.jpg.webp 1024w,https://ichef.bbci.co.uk/news/1536/cpsprodpb/a235/live/2b93b270-dc32-11ef-acf7-0d8b5f8aaf71.jpg.webp 1536w" src="https://ichef.bbci.co.uk/news/480/cpsprodpb/a235/live/2b93b270-dc32-11ef-acf7-0d8b5f8aaf71.jpg.webp" alt="KEVIN HILL Kevin's head shaved after surgery. You can see the stitches in his skull where he had the operation to implant the wires into his brain.   "><span>KEVIN HILL</span></p></div><p data-component="caption-block"><figcaption>A brain implant links to the computer in Mr Hill's chest<!-- --></figcaption></p></figure><div data-component="text-block"><p>For the last year he has had to go to hospital regularly to have his system re-programmed to better control his symptoms.<!-- --></p><p>Now, a new updated version called "adaptive deep brain stimulation" has been designed to re-programme the system in real time.<!-- --></p><p>It can also read a patient's brain signals which doctors say should mean even better control of symptoms.<!-- --></p></div><figure><div data-component="image-block"><p><img src="https://www.bbc.com/bbcx/grey-placeholder.png"><img sizes="(min-width: 1280px) 50vw, (min-width: 1008px) 66vw, 96vw" srcset="https://ichef.bbci.co.uk/news/240/cpsprodpb/0f16/live/a7a5d160-d983-11ef-ba6a-b744abd00a0a.png.webp 240w,https://ichef.bbci.co.uk/news/320/cpsprodpb/0f16/live/a7a5d160-d983-11ef-ba6a-b744abd00a0a.png.webp 320w,https://ichef.bbci.co.uk/news/480/cpsprodpb/0f16/live/a7a5d160-d983-11ef-ba6a-b744abd00a0a.png.webp 480w,https://ichef.bbci.co.uk/news/640/cpsprodpb/0f16/live/a7a5d160-d983-11ef-ba6a-b744abd00a0a.png.webp 640w,https://ichef.bbci.co.uk/news/800/cpsprodpb/0f16/live/a7a5d160-d983-11ef-ba6a-b744abd00a0a.png.webp 800w,https://ichef.bbci.co.uk/news/1024/cpsprodpb/0f16/live/a7a5d160-d983-11ef-ba6a-b744abd00a0a.png.webp 1024w,https://ichef.bbci.co.uk/news/1536/cpsprodpb/0f16/live/a7a5d160-d983-11ef-ba6a-b744abd00a0a.png.webp 1536w" src="https://ichef.bbci.co.uk/news/480/cpsprodpb/0f16/live/a7a5d160-d983-11ef-ba6a-b744abd00a0a.png.webp" alt="NEWCASTLE HOSPITALS NHS TRUST A mid-shot of Mr Akbar Hussain, a neurosurgeon at Newcastle Hospitals. He is pictured wearing his blue surgical scrubs.
    "><span>NEWCASTLE HOSPITALS NHS TRUST</span></p></div><p data-component="caption-block"><figcaption>Neurosurgeon Akbar Hussain said recent changes to the device would be very significant to patients' quality of life<!-- --></figcaption></p></figure><div data-component="text-block"><p>Akbar Hussain, a neurosurgeon at Newcastle Hospitals, is one of the first doctors in the world to offer the new adaptive Brainsense treatment, developed by Medtronic.<!-- --></p><p>He said: "The amazing thing about the adaptive version is that the electrical impulses provided to the brain by the device are controlled and adjusted automatically, according to individual patient's recordings from the device in their chest.<!-- --></p><p>"The biological signals generated within the person themselves are enough to alter the treatment given by the implant. <!-- --></p><p>"These changes could be taking place by the minute or hour, meaning the treatment is truly responsive to the exact needs of each individual.<!-- --></p><p>"It's exciting. Hopefully this will have a huge impact and be very significant for the patients' quality of life."<!-- --></p></div><figure><div data-component="image-block"><p><img src="https://www.bbc.com/bbcx/grey-placeholder.png"><img sizes="(min-width: 1280px) 50vw, (min-width: 1008px) 66vw, 96vw" srcset="https://ichef.bbci.co.uk/news/240/cpsprodpb/bf88/live/bd001e50-dcce-11ef-bc01-8f2c83dad217.jpg.webp 240w,https://ichef.bbci.co.uk/news/320/cpsprodpb/bf88/live/bd001e50-dcce-11ef-bc01-8f2c83dad217.jpg.webp 320w,https://ichef.bbci.co.uk/news/480/cpsprodpb/bf88/live/bd001e50-dcce-11ef-bc01-8f2c83dad217.jpg.webp 480w,https://ichef.bbci.co.uk/news/640/cpsprodpb/bf88/live/bd001e50-dcce-11ef-bc01-8f2c83dad217.jpg.webp 640w,https://ichef.bbci.co.uk/news/800/cpsprodpb/bf88/live/bd001e50-dcce-11ef-bc01-8f2c83dad217.jpg.webp 800w,https://ichef.bbci.co.uk/news/1024/cpsprodpb/bf88/live/bd001e50-dcce-11ef-bc01-8f2c83dad217.jpg.webp 1024w,https://ichef.bbci.co.uk/news/1536/cpsprodpb/bf88/live/bd001e50-dcce-11ef-bc01-8f2c83dad217.jpg.webp 1536w" src="https://ichef.bbci.co.uk/news/480/cpsprodpb/bf88/live/bd001e50-dcce-11ef-bc01-8f2c83dad217.jpg.webp" alt="Kevin Hill wearing a fluourescent green jacket and holding his bike and a helmet and smiling."></p></div><p data-component="caption-block"><figcaption>Kevin Hill says his old life has returned since having the surgery<!-- --></figcaption></p></figure><div data-component="text-block"><p>Dr Becky Jones, from the charity Parkinson's UK, said: "Current DBS can be life changing and has the promise to be even more effective if it could be responsive to the needs of the individual. Brainsense represents a major step towards this.<!-- --></p><p>"While evidence is still being gathered to assess the benefits of adaptive DBS versus the standard type, it's great to see movement towards this becoming a new, more effective treatment for people with Parkinson's."<!-- --></p><p>About 153,000 people in the UK are living with Parkinson's disease, a progressive neurological disorder affecting the brain and nervous system. <!-- --></p><p>The number is expected to increase due to population growth and ageing. <!-- --></p></div><p><i id="follow-bbc-sunderland-on">Follow BBC Sunderland on <!-- --></i><a target="_blank" href="https://x.com/BBCNEandCumbria"><i id="x">X<!-- --></i></a><i id=",">, <!-- --></i><a target="_blank" href="https://www.facebook.com/BBCSunderland"><i id="facebook">Facebook<!-- --></i></a><i id=",">, <!-- --></i><a target="_blank" href="https://bbc.in/3yyMYUI"><i id="nextdoor">Nextdoor<!-- --></i></a><i id="and"> and <!-- --></i><a target="_blank" href="https://www.instagram.com/bbcneandcumbria/"><i id="instagram">Instagram<!-- --></i></a><i id=".-send-your-story-ideas-to-northeastandcumbria@bbc.co.uk.">. Send your story ideas to northeastandcumbria@bbc.co.uk.<!-- --></i></p><div data-component="links-block"><p><span data-testid="links-title">More stories on this topic</span></p></div></article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[HNInternal: Ask HN: Are YC startups *actually* hiring? (103 pts)]]></title>
            <link>https://news.ycombinator.com/item?id=42856752</link>
            <guid>42856752</guid>
            <pubDate>Tue, 28 Jan 2025 19:24:41 GMT</pubDate>
            <description><![CDATA[<p>See on <a href="https://news.ycombinator.com/item?id=42856752">Hacker News</a></p>
Couldn't get https://news.ycombinator.com/item?id=42856752: Error: timeout of 10000ms exceeded]]></description>
        </item>
        <item>
            <title><![CDATA[Windows 7 boots slower if you set a solid background color (101 pts)]]></title>
            <link>https://support.microsoft.com/en-gb/topic/the-welcome-screen-may-be-displayed-for-30-seconds-during-the-logon-process-after-you-set-a-solid-color-as-the-desktop-background-in-windows-7-or-in-windows-server-2008-r2-b4565ced-703a-cc85-bf9c-6b3d586d6421</link>
            <guid>42856262</guid>
            <pubDate>Tue, 28 Jan 2025 18:48:42 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://support.microsoft.com/en-gb/topic/the-welcome-screen-may-be-displayed-for-30-seconds-during-the-logon-process-after-you-set-a-solid-color-as-the-desktop-background-in-windows-7-or-in-windows-server-2008-r2-b4565ced-703a-cc85-bf9c-6b3d586d6421">https://support.microsoft.com/en-gb/topic/the-welcome-screen-may-be-displayed-for-30-seconds-during-the-logon-process-after-you-set-a-solid-color-as-the-desktop-background-in-windows-7-or-in-windows-server-2008-r2-b4565ced-703a-cc85-bf9c-6b3d586d6421</a>, See on <a href="https://news.ycombinator.com/item?id=42856262">Hacker News</a></p>
Couldn't get https://support.microsoft.com/en-gb/topic/the-welcome-screen-may-be-displayed-for-30-seconds-during-the-logon-process-after-you-set-a-solid-color-as-the-desktop-background-in-windows-7-or-in-windows-server-2008-r2-b4565ced-703a-cc85-bf9c-6b3d586d6421: Error: timeout of 10000ms exceeded]]></description>
        </item>
        <item>
            <title><![CDATA[New speculative attacks on Apple CPUs (763 pts)]]></title>
            <link>https://predictors.fail/</link>
            <guid>42856023</guid>
            <pubDate>Tue, 28 Jan 2025 18:31:34 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://predictors.fail/">https://predictors.fail/</a>, See on <a href="https://news.ycombinator.com/item?id=42856023">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
				<section id="demos">
					<h4>Demos</h4>

					<h5>Leaking Proton Mail's Inbox Data</h5>
					<p>We train the M3 CPU's LVP via sandboxed JavaScript code running inside WebKit (Safari's browsing engine). When the mouse cursor is over our demo webpage, our proof-of-concept opens Proton Mail's inbox in a new window, but uses the same process to render the inbox. This brings the inbox content into the address space, making it accessible with a sandbox escape. Finally, we use the LVP to craft an arbitrary read primitive to anywhere in this address space, recovering the sender and subject lines shown on the inbox page.</p>
					

					<h5>Reading The Great Gatsby Using Load Address Prediction</h5>
					<p>We demonstrate an LAP proof-of-concept on the Apple M2 CPU that recovers a secret string. The string holds the first paragraph of The Great Gatsby, but is never architecturally accessed. At the LAP's incorrectly guessed memory address, we place a pointer to the characters of the string. Subsequently, we train and activate the LAP.</p>
					

					<h5>Reading Harry Potter Using Load Value Prediction</h5>
					<p>On the Apple M3 CPU, we demonstrate an LVP proof-of-concept that recovers the first paragraph of Harry Potter and the Sorcerer's Stone, which is also never architecturally read by the CPU core. We cause the LVP to predict and access an incorrect array index. There, we place the pointer to the string's characters, which the CPU then dereferences.
					</p>
					
				</section>

				<section id="people">
					<h4>The People Behind
						<span>SLAP and FLOP</span>
					</h4>
					<div>
								<ul>
									<li><a href="https://jas0n.kim/">Jason Kim </a><span><a href="https://www.gatech.edu/">Georgia Institute of
												Technology</a></span></li>
									<li><a href="https://sites.cc.gatech.edu/grads/j/jchuang9/">Jalen Chuang</a> <span><a href="https://www.gatech.edu/">Georgia
												Institute of
												Technology</a></span></li>
									<li><a href="https://faculty.cc.gatech.edu/~genkin/">Daniel Genkin</a> <span><a href="https://www.gatech.edu/">Georgia Institute of
												Technology</a></span>
									</li>
									<li><a href="https://yuval.yarom.org/">Yuval Yarom</a> <span><a href="https://www.ruhr-uni-bochum.de/">Ruhr University
												Bochum</a></span></li>
								</ul>
							</div>
				</section>

				<section id="qa">
					<h4>Frequently Asked <span>Questions</span></h4>

					<h5>SLAP and FLOP Basics</h5>
					<div id="accordion" aria-labelledby="basics-question-1">
									<p>The affected Apple devices are the following:</p>
									<ul>
										<li>All Mac laptops from 2022-present (MacBook Air, MacBook Pro)</li>
										<li>All Mac desktops from 2023-present (Mac Mini, iMac, Mac Studio, Mac Pro) </li>
										<li>All iPad Pro, Air, and Mini models from September 2021-present (Pro 6th and 7th gen., Air 6th gen., Mini 6th gen.)</li>
										<li>All iPhones from September 2021-present (All 13, 14, 15, and 16 models, SE 3rd gen.)</li>
									</ul>
								</div>
					<div id="accordion" aria-labelledby="basics-question-2">
								<p>There are hardware and software measures to ensure that two open webpages are isolated from each other, 
									   preventing one of them form (maliciously) reading the other's contents. SLAP and FLOP break these protections, 
									   allowing attacker pages to read sensitive login-protected data from target webpages. In our work, we show
										that this data ranges from location history to credit card information.</p>
							</div>
					<div id="accordion" aria-labelledby="basics-question-3">
								<p> While FLOP has an actionable mitigation, implementing it requires patches from software vendors and cannot be done by users. 
									Apple has communicated to us that they plan to address these issues in an
										upcoming security update, hence it is important to enable automatic updates and
										ensure that your devices are running the latest operating system and applications.</p>
							</div>
					<div id="accordion" aria-labelledby="basics-question-4">
								<p>We have not yet observed load address prediction or load value prediction 
									   in other processor vendors' products, such as Intel, AMD, Qualcomm, or Ampere.</p>
							</div>
					<div id="accordion" aria-labelledby="basics-question-5">
								<p> We do not know, as we have not tested other browsers such as Firefox. </p>
							</div>
					<div id="accordion" aria-labelledby="basics-question-6">
								<p>Since SLAP and FLOP are microarchitecture-based attacks, they do not leave any traces in
										the system's log files. While cached copies of previously visited websites may
										be present in the web browser, it is difficult to automatically detect malicious
										code patterns that exploit hardware vulnerabilities.</p>
							</div>
					<div id="accordion" aria-labelledby="basics-question-7">
								<p>So far, we do not have any evidence that either SLAP or FLOP has been used in the
										wild.</p>
							</div>
					<div id="accordion" aria-labelledby="basics-question-8">
								<p>We disclosed SLAP to Apple on May 24, 2024, and FLOP on September 3, 2024.</p>
							</div>

					<h5>Technical Questions</h5>
					<div id="accordion" aria-labelledby="tech-question-1">
									<p>
										Most computer bugs arise from mistakes in programming, such as missing bounds
										checks 	or use-after-frees. However, a side-channel attack exploits the implementation
										of a computer's hardware to attack it, even if the software it runs is a secure algorithm.
										Systems can leak sensitive data through sound, electromagnetic radiation, or
										thermal throttling, just for a few examples.
									</p>
									<p>
										Many side channels, including ones we use for SLAP and FLOP, comes from the
										CPU's microarchitecture. Whenever an attacker and target run on the physical CPU, they share the CPU's
										internal resources such as cores, caches, and internal buffers.
										Sharing resources leads to contention, and contention can be measured indirectly
										through several variables like timing or power consumption.
										These measurements leave fingerprints on the target's behavior on the CPU.
										Accordingly, an attacker can abuse this to make inferences about the target's
										secrets even if they are isolated at the process level or the hypervisor level.
									</p>
								</div>
					<div id="accordion" aria-labelledby="tech-question-2">
									<p>
										Virtually all modern CPUs use a performance optimization where they predict the
										control flow the CPU should take (such as branches and returns), should the
										outcome not be readily available. Once a prediction is made, the CPU will execute instructions along
										the prediction, a process called speculative execution. If the CPU realizes it had
										mispredicted, it must revert all changes in the state it performed after the
										prediction. Nearly all desktop and mobile CPUs exhibit this behavior, regardless of
										manufacturer (such as Apple, AMD, or Intel).
									</p>
									<p>
										<a href="https://spectreattack.com/">Spectre</a> is a hardware vulnerability in
										virtually all modern CPUs that occurs when speculative execution backfires.
										While the CPU should ideally revert all changes in state, speculative execution leaves
										traces in the CPU's microarchitectural state and especially the cache. A Spectre
										attack coerces the CPU into speculatively executing the wrong flow of
										instructions. If this wrong flow has instructions depending on sensitive data, their value can
										be inferred through a side channel even after the CPU realizes the mistake and
										reverts its changes. An adversary can abuse this behavior to read data that they cannot
										normally access through program semantics. Because speculative execution is an
										important part of CPU performance that is infeasible to simply remove as a
										countermeasure, Spectre continues to be dangerous to software even years after
										its discovery.
									</p>
									<p>
										In SLAP and FLOP, we demonstrate that recent Apple CPUs go beyond this, not only
										predicting the control flow the CPU should take, but also the data flow the CPU
										should operate on if data are not readily available from the memory subsystem.
										Unlike Spectre, mispredictions on data flow do not directly result in the CPU
										speculatively executing the wrong instructions. Instead, they result in the CPU
										executing arbitrary instructions on the wrong data. However, we show this can be combined with
										indirection techniques to execute wrong instructions.
									</p>
								</div>
					<div id="accordion" aria-labelledby="tech-question-3">
									<p>To orchestrate SLAP, we begin by reverse engineering Apple's implementation of Load Address Prediction (LAP). 
									We discover that if we train the LAP on striding memory addresses, the LAP will access address the next sequence 
									in the striding pattern and compute using the data in that address, even if the program never actually accesses it.
									Here, we note that this is different from hardware prefetching. While prefetchers may bring the data inside the predicted addresses, 
									they do not speculatively execute downstream instructions based on the prediction. </p>
									<p>Next, we find an attack surface in Safari. Previously, <a href="https://ileakage.com/">iLeakage</a> demonstrated a corner case 
									in Safari's isolation scheme where an adversary's webpage can coerce an arbitrary target webpage to be handled by the same process. 
									We find that when this occurs, the two webpages also share internal memory allocation regions for data, such as strings. 
									In turn, this allows the adversary to jump the LAP to the target webpage's string and trick the CPU into operating on it, eventually leaking the string's content over a covert channel.</p>
								</div>
					<div id="accordion" aria-labelledby="tech-question-4">
									<p>Similarly to SLAP, we reverse engineer the Load Value Prediction (LVP) mechanism in Apple CPUs. 
									We found that if the LVP sees the same data value being repeatedly returned from the memory subsystem for the same load instruction, 
									the LVP will attempt to guess the load's outcome the next time that load instruction executes, even if the memory accessed by the load now contains a completely different value! 
									Therefore, using the LVP, we can trick the CPU into computing on incorrect data values.</p>
									<p>We first demonstrate the dangers stemming from LVP in Safari, whose JavaScript engine first vets the type information of JavaScript data structures before determining the 
									appropriate computations to run on them. If we train the LVP on the load instruction that retrieves this type information, we can cause code that is only supposed to run for 
									one data structure on another data structure, causing speculative type confusion, and obtaining a read primitive to arbitrary 64-bit addresses. </p>
									
									<p> Next, we move to Chrome, where internal table data structures for calling WebAssembly functions also vet the signature of each function before calling them with arguments. 
									Here, we show that the LVP allows us to run a function with the wrong arguments (e.g., pointer instead of integer), again resulting in a type confusion based primitive for reading arbitrary memory addresses.</p>
								</div>
					<div id="accordion" aria-labelledby="tech-question-5">
									<p>SLAP exploits a phenomenon in Safari where strings that belong to different webpages can be allocated within a close distance to each other, and thus discloses cross-origin strings that are allocated in proximity to the adversary's own strings. On the other hand, FLOP is a speculative type confusion attack that causes the CPU to bypass integrity checks on data structures, resulting in memory read primitives from arbitrary addresses in Safari and Chrome.</p>
									<p>
									Furthermore, the underlying CPU microarchitecture that SLAP and FLOP exploit are also different. SLAP uses the Load Address Predictor (LAP), while FLOP uses the Load Value Predictor (LVP).
									As suggested by their names, the LAP predicts addresses while the LVP predicts values. 
									Consider the following statement: "The CPU accesses memory at address 0xdeadbeef, 
									which contains the value 0x1234." The next time the CPU performs a memory access, the LAP predicts the next address, i.e., what 0xdeadbeef will change to. Meanwhile, the LVP predicts the next value returned from memory, that is, what 0x1234 will change to. 
									Going deeper in detail, we observe their internal structures are also different. For instance, the LAP requires a longer training sequence than the LVP to activate reliably, but only the LAP can observe strides and generate predictions accordingly.</p>
								</div>
					<div id="accordion" aria-labelledby="tech-question-6">
								<p>
										JavaScript and WebAssembly are two programming languages that make up the
										backbone of interactive webpages, such as online games and video streaming
										services. JavaScript can update the content of the website directly, while
										WebAssembly is used for high-performance web applications. Ultimately,
										WebAssembly interfaces with JavaScript to deliver dynamic content to users.
										Since both are sandboxed in a browser environment, side-channel attacks are
										notably more difficult to implement in these languages. However, the impact is
										drastically greater, as browsers execute both types of code automatically and do
										not require the user to download the malicious program.
									</p>
							</div>
					<div id="accordion" aria-labelledby="tech-question-7">
									<p>For leaking secrets, both SLAP and FLOP are confined to the address space they are trained in. 
									As pointed out by <a href="https://ileakage.com/">iLeakage</a>, Safari lacks Site Isolation, a measure used to enforce that two different webpages not from the same domain can never be handled by the same process.
									Thus, in Safari it is possible for an adversary's webpage to be handled by the same process (and thus address space) with an arbitrary webpage, increasing the attack surface including LAP- and LVP-based exploits.
									 </p>
									
									<p>On the other hand, although Chrome is equipped with Site Isolation, we demonstrate that it is not a perfect mitigation. We show the real-world existence of corner cases, where two subdomains of the same site can be merged into one process, again leading to LAP- and LVP-based attacks.</p>
								</div>

					<h5>Miscellaneous</h5>
					<div id="accordion" aria-labelledby="misc-question-1">
									<p>Yes, with rights waived via <a href="https://creativecommons.org/publicdomain/zero/1.0/">CC0</a>. You can
										right-click each logo, which should list an option to save the image as a file.
									</p></div>
					
					

				</section>
				<section id="acknowledgments">
					<h4>Acknowledgments</h4>
					<div><p>
							This research was supported by
							the Air Force Office of Scientific Research (AFOSR) under award number FA9550-24-1-0079;
							the Alfred P Sloan Research Fellowship;
							an ARC Discovery Project number DP210102670;
							the Defense Advanced Research Projects Agency (DARPA) under contract numbers
							W912CG-23-C-0022,
							the Deutsche Forschungsgemeinschaft (DFG, German Research Foundation) under Germany's
							Excellence Strategy - EXC 2092 CASA - 390781972;
							and gifts from Qualcomm, Cisco (SLAP), and Zama (FLOP).
							</p><p>
							The views and conclusions contained in this document are those of the authors and should not
							be interpreted as representing the official policies, either expressed or implied, of the
							U.S. Government.
						</p></div>
				</section>
			</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Berkeley Researchers Replicate DeepSeek R1's Core Tech for Just $30: A Small Mod (159 pts)]]></title>
            <link>https://xyzlabs.substack.com/p/berkeley-researchers-replicate-deepseek</link>
            <guid>42855283</guid>
            <pubDate>Tue, 28 Jan 2025 17:36:31 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://xyzlabs.substack.com/p/berkeley-researchers-replicate-deepseek">https://xyzlabs.substack.com/p/berkeley-researchers-replicate-deepseek</a>, See on <a href="https://news.ycombinator.com/item?id=42855283">Hacker News</a></p>
Couldn't get https://xyzlabs.substack.com/p/berkeley-researchers-replicate-deepseek: Error: timeout of 10000ms exceeded]]></description>
        </item>
        <item>
            <title><![CDATA[Using uv as your shebang line (317 pts)]]></title>
            <link>https://akrabat.com/using-uv-as-your-shebang-line/</link>
            <guid>42855258</guid>
            <pubDate>Tue, 28 Jan 2025 17:35:05 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://akrabat.com/using-uv-as-your-shebang-line/">https://akrabat.com/using-uv-as-your-shebang-line/</a>, See on <a href="https://news.ycombinator.com/item?id=42855258">Hacker News</a></p>
<div id="readability-page-1" class="page"><article id="post-7254">
            
                <div>

<p>I create a fair few scripts in my <tt>~/bin/</tt> directory to automate tasks. Since discovering <a href="https://akrabat.com/defining-python-dependencies-at-the-top-of-the-file/"><tt>uv</tt> and inline script metadata</a>, I’ve started using Python far more for these.</p>
<p>As <tt>~/bin</tt> is on my path, I want to run the script by calling it directly on the command line. To do this, I use this shebang:</p>
<pre>#!/usr/bin/env -S uv run --script
</pre>
<p>The command line will now run <tt>uv run --script</tt> and pass the file as the argument. <tt>uv</tt> ignores the shebang and then runs the rest of the file as a normal Python file. </p>
<p>Once I’ve ensured that that script has executable permissions via <tt>chmod a+x {filname}</tt>, I’m now good to go with simple command line scripts written in Python that automatically handle their dependencies!</p>


</div>
                        <p>This article was posted on
                                <time datetime="2025-01-28 11:00:00" title="2025-01-28 11:00:00">28 January 2025</time>
                                in <span><a href="https://akrabat.com/category/python/" rel="category tag">Python</a></span>
                        </p>
                                              </article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[How has DeepSeek improved the Transformer architecture? (225 pts)]]></title>
            <link>https://epoch.ai/gradient-updates/how-has-deepseek-improved-the-transformer-architecture</link>
            <guid>42855170</guid>
            <pubDate>Tue, 28 Jan 2025 17:29:07 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://epoch.ai/gradient-updates/how-has-deepseek-improved-the-transformer-architecture">https://epoch.ai/gradient-updates/how-has-deepseek-improved-the-transformer-architecture</a>, See on <a href="https://news.ycombinator.com/item?id=42855170">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
          <p>DeepSeek has recently released DeepSeek v3, which is currently state-of-the-art in benchmark performance among open-weight models, alongside <a href="https://arxiv.org/abs/2412.19437">a technical report</a> describing in some detail the training of the model. Impressively, they’ve achieved this SOTA performance by only using 2.8 million H800 hours of training hardware time—equivalent to about 4e24 FLOP if we assume 40% MFU. This is about ten times less training compute than the similarly performing Llama 3.1 405B.</p>

<p>In this issue, I’ll cover some of the important architectural improvements that DeepSeek highlight in their report and why we should expect them to result in better performance compared to a vanilla Transformer. The full technical report contains plenty of non-architectural details as well, and I strongly recommend reading it if you want to get a better idea of the engineering problems that have to be solved when orchestrating a moderate-sized training run.</p>

<figure>
  <img src="https://epoch.ai/assets/images/gradient-updates/2025/how-has-deepseek-improved-the-transformer-architecture/figure-1.png">
<figcaption>
    <p>Figure 1: The DeepSeek v3 architecture with its two most important improvements: DeepSeekMoE and multi-head latent attention (MLA). Multi-token prediction is not shown. From the DeepSeek v3 technical report.</p>
  </figcaption>
</figure>



<h2 id="multi-head-latent-attention-mla">Multi-head latent attention (MLA)</h2>

<p>Multi-head latent attention (abbreviated as MLA) is the most important architectural innovation in DeepSeek’s models for long-context inference. This technique was first introduced in DeepSeek v2 and is a superior way to reduce the size of the KV cache compared to traditional methods such as grouped-query and multi-query attention.</p>

<p>I’ll start with a brief explanation of what the KV cache is all about. If you’re familiar with this, you can skip directly to the next subsection.</p>

<h3 id="what-is-the-kv-cache-and-why-does-it-matter">What is the KV cache and why does it matter?</h3>

<p>When a Transformer is used to generate tokens sequentially during inference, it needs to see the context of all of the past tokens when deciding which token to output next. The naive way to do this is to simply do a forward pass including all past tokens every time we want to generate a new token, but this is inefficient because those past tokens have already been processed before. We would just be recomputing results we’ve already obtained previously and discarded.</p>

<p>To avoid this recomputation, it’s efficient to cache the relevant internal state of the Transformer for all past tokens and then retrieve the results from this cache when we need them for future tokens. Because the only way past tokens have an influence on future tokens is through their key and value vectors in the attention mechanism, it suffices to cache these vectors. This is where the name key-value cache, or KV cache for short, comes from.</p>

<p>This works well when context lengths are short, but can start to become expensive when they become long. This is because cache reads are not free: we need to save all those vectors in GPU high-bandwidth memory (HBM) and then load them into the tensor cores when we need to involve them in a computation. If each token needs to know all of its past context, this means for each token we generate we must read the entire past KV cache from HBM.</p>

<p>In a vanilla Transformer using a standard multi-head attention mechanism, the number of KV cache parameters per past token can be expressed as:</p>

<p>2 * attention head dimension * number of attention heads * number of Transformer blocks</p>

<p>For instance, GPT-3 had 96 attention heads with 128 dimensions each and 96 blocks, so for each token we’d need a KV cache of 2.36M parameters, or 4.7 MB at a precision of 2 bytes per KV cache parameter.</p>

<p>GPT-3 didn’t support long context windows, but if for the moment we assume it did, then each additional token generated at a 100K context length would require 470 GB of memory reads, or around 140 ms of H100 time given the H100’s HBM bandwidth of 3.3 TB/s. The price per million tokens generated at $2 per hour per H100 would then be $80, around 5 times more expensive than Claude 3.5 Sonnet’s price to the customer (which is likely significantly above its cost to Anthropic itself). This naive cost can be brought down e.g. by speculative sampling, but it gives a decent ballpark estimate.</p>

<p>This rough calculation shows why it’s crucial to find ways to reduce the size of the KV cache when we’re working with context lengths of 100K or above. The most popular way in open-source models so far has been grouped-query attention. In this architectural setting, we assign multiple query heads to each pair of key and value heads, effectively grouping the query heads together - hence the name of the method. This cuts down the size of the KV cache by a factor equal to the group size we’ve chosen. In models such as Llama 3.3 70B and Mistral Large 2, grouped-query attention reduces the KV cache size by around an order of magnitude.</p>

<h3 id="beating-grouped-query-attention">Beating grouped-query attention</h3>

<p>The fundamental problem with methods such as grouped-query attention or KV cache quantization is that they involve compromising on model quality in order to reduce the size of the KV cache. Instead of this, DeepSeek has found a way to reduce the KV cache size <em>without</em> compromising on quality, at least in their internal experiments.</p>

<p>They accomplish this by turning the computation of key and value vectors from the residual stream into a two-step process. In a vanilla Transformer, key and value vectors are computed by directly multiplying the residual stream vector by a matrix of the shape</p>

<p>(number of heads · head dimension) x (model dimension)</p>

<p>DeepSeek’s method essentially forces this matrix to be low rank: they pick a latent dimension and express it as the product of two matrices, one with dimensions latent times model and another with dimensions (number of heads · head dimension) times latent. Then, during inference, we only cache the latent vectors and not the full keys and values. We can then shrink the size of the KV cache by making the latent dimension smaller.</p>

<figure>
  <img src="https://epoch.ai/assets/images/gradient-updates/2025/how-has-deepseek-improved-the-transformer-architecture/figure-2.png">
<figcaption>
    <p>Figure 2: An illustration of multi-head latent attention from the DeepSeek v2 technical report.</p>
  </figcaption>
</figure>

<p>Naively, this shouldn’t fix our problem, because we would have to recompute the actual keys and values every time we need to generate a new token. After all, we need the full vectors for attention to work, not their latents. Multi-head latent attention is based on the clever observation that this is actually not true, because we can merge the matrix multiplications that would compute the upscaled key and value vectors from their latents with the query and post-attention projections, respectively.</p>

<p>The reason low-rank compression is so effective is because there’s plenty of information overlap between what different attention heads need to know about. If we used low-rank compression on the key and value vectors of individual heads instead of all keys and values of all heads stacked together, the method would simply be equivalent to using a smaller head dimension to begin with and we would get no gain. Exploiting the fact that different heads need access to the same information is essential for the mechanism of multi-head latent attention.</p>

<p>Methods such as grouped-query attention exploit the possibility of the same overlap, but they do so ineffectively by forcing attention heads that are grouped together to all respond similarly to queries. In other words, information sharing becomes coupled to having identical behavior in some restricted sense, a clearly undesirable property. Low-rank compression, on the other hand, allows the same <em>information</em> to be used in <em>very different ways</em> by different heads. In theory, this could even have beneficial regularizing effects on training, and DeepSeek reports finding such effects in their technical reports.</p>

<p>I see this as one of those innovations that look obvious in retrospect but that require a good understanding of what attention heads are actually doing to come up with. Once you see the approach, it’s immediately obvious that it cannot be any worse than grouped-query attention and it’s also likely to be significantly better. However, coming up with the idea of trying this is another matter.</p>

<h2 id="mixture-of-experts-innovations">Mixture-of-experts innovations</h2>

<p>One of the most popular improvements to the vanilla Transformer was the introduction of mixture-of-experts (MoE) models. These models divide the feedforward blocks of a Transformer into multiple distinct experts and add a routing mechanism which sends each token to a small number of these experts in a context-dependent manner. This means the model can have more parameters than it activates for each specific token, in a sense decoupling how much the model knows from the arithmetic cost of processing individual tokens. Probably the most influential model that is currently known to be an MoE is the original GPT-4.</p>

<p>Expert routing algorithms work as follows: once we exit the attention block of any layer, we have a residual stream vector that is the output. Each expert has a corresponding expert vector of the same dimension, and we decide which experts will become activated by looking at which ones have the highest inner products with the current residual stream.</p>

<p>The problem with this is that it introduces a rather ill-behaved discontinuous function with a discrete image at the heart of the model, in sharp contrast to vanilla Transformers which implement continuous input-output relations. This causes gradient descent optimization methods to behave poorly in MoE training, often resulting in “routing collapse”, where the model gets stuck always activating the same few experts for every token instead of spreading its knowledge and computation around all of the available experts.</p>

<p>To get an intuition for routing collapse, consider attempting to train a model such as GPT-4 with 16 experts in total and 2 experts active per token. Now, suppose that for random initialization reasons two of these experts just happen to be the best performing ones at the start. Gradient descent will then reinforce the tendency to pick these experts. This will mean these experts will get almost all of the gradient signals during updates and become better while other experts lag behind, and so the other experts will continue not being picked, producing a positive feedback loop that results in other experts never getting chosen or trained.</p>

<p>The fundamental issue is that gradient descent just heads in the direction that’s locally best. This usually works fine in the very high dimensional optimization problems encountered in neural network training. However, when our neural network is so discontinuous in its behavior, even the high dimensionality of the problem space may not save us from failure.</p>

<p>It is nontrivial to address these training difficulties. DeepSeek v3 does so by combining several different innovations, each of which I will discuss in turn.</p>

<h3 id="auxiliary-loss-free-load-balancing">Auxiliary-loss-free load balancing</h3>

<p>A popular method for avoiding routing collapse is to force “balanced routing”, i.e. the property that each expert is activated roughly an equal number of times over a sufficiently large batch, by adding to the training loss a term measuring how imbalanced the expert routing was in a particular batch. This term is called an “auxiliary loss” and it makes intuitive sense that introducing it pushes the model towards balanced routing. However, the DeepSeek v3 technical report notes that such an auxiliary loss hurts model performance even if it ensures balanced routing.</p>

<p>Their alternative is to add expert-specific bias terms to the routing mechanism which get added to the expert affinities. These bias terms are not updated through gradient descent but are instead adjusted throughout training to ensure load balance: if a particular expert is not getting as many hits as we think it should, then we can slightly bump up its bias term by a fixed small amount every gradient step until it does. The technical report notes this achieves better performance than relying on an auxiliary loss while still ensuring appropriate load balance.</p>

<h3 id="shared-experts">Shared experts</h3>

<p>A serious problem with the above method of addressing routing collapse is that it assumes, without any justification, that an optimally trained MoE would have balanced routing. However, this is a dubious assumption.</p>

<p>To see why, consider that any large language model likely has a small amount of information that it uses a lot, while it has a lot of information that it uses rather infrequently. For instance, almost any English request made to an LLM requires the model to know how to speak English, but almost no request made to an LLM would require it to know who the King of France was in the year 1510. So it’s quite plausible the optimal MoE should have a few experts which are accessed a lot and store “common information”, while having others which are accessed sparsely and store “specialized information”.</p>

<p>If we force balanced routing, we lose the ability to implement such a routing setup and have to redundantly duplicate information across different experts. However, if we don’t force balanced routing, we face the risk of routing collapse. To escape this dilemma, DeepSeek separates experts into two types: <em>shared experts</em> and <em>routed experts</em>. Shared experts are always routed to no matter what: they are excluded from both expert affinity calculations and any possible routing imbalance loss term. We concern ourselves with ensuring balanced routing only for routed experts.</p>

<p>The key observation here is that “routing collapse” is an extreme situation where the likelihood of each individual expert being chosen is either 1 or 0. Naive load balancing addresses this by trying to push the distribution to be uniform, i.e. every expert should have the same chance of being selected. However, if our sole concern is to avoid routing collapse then there’s no reason for us to target specifically a uniform distribution. DeepSeek v3 instead targets a distribution where each expert is either selected for sure (probability 1) or selected with some fixed probability p &gt; 0 for each token.</p>

<p>I think it’s likely even this distribution is not optimal and a better choice of distribution will yield better MoE models, but it’s already a significant improvement over just forcing a uniform distribution.</p>

<h2 id="multi-token-prediction">Multi-token prediction</h2>

<p>The final change that DeepSeek v3 makes to the vanilla Transformer is the ability to predict multiple tokens out for each forward pass of the model. This allows them to use a multi-token prediction objective during training instead of strict next-token prediction, and they demonstrate a performance improvement from this change in ablation experiments.</p>

<p>The basic idea is the following: we first do an ordinary forward pass for next-token prediction. As we would in a vanilla Transformer, we use the final residual stream vector to generate next token probabilities through unembedding and softmax. However, unlike in a vanilla Transformer, we <em>also</em> feed this vector into a subsequent Transformer block, and we use the output of that block to make predictions about the second next token. We can iterate this as much as we like, though DeepSeek v3 only predicts two tokens out during training.</p>

<figure>
  <img src="https://epoch.ai/assets/images/gradient-updates/2025/how-has-deepseek-improved-the-transformer-architecture/figure-3.png">
<figcaption>
    <p>Figure 3: An illustration of DeepSeek v3’s multi-token prediction setup taken from its technical report.</p>
  </figcaption>
</figure>

<p>They incorporate these predictions about further out tokens into the training objective by adding an additional cross-entropy term to the training loss with a weight that can be tuned up or down as a hyperparameter. This not only gives them an additional target to get signal from during training but also allows the model to be used to <a href="https://arxiv.org/abs/2211.17192">speculatively decode</a> itself. We can generate a few tokens in each forward pass and then show them to the model to decide from which point we need to reject the proposed continuation.</p>

<p>DeepSeek v3 only uses multi-token prediction up to the second next token, and the acceptance rate the technical report quotes for second token prediction is between 85% and 90%. This is quite impressive and should allow nearly double the inference speed (in units of tokens per second per user) at a fixed price per token if we use the aforementioned speculative decoding setup. It doesn’t look worse than the acceptance probabilities one would get when decoding Llama 3 405B with Llama 3 70B, and might even be better.</p>

<p>I’m curious what they would have obtained had they predicted further out than the second next token. If e.g. each subsequent token gives us a 15% relative reduction in acceptance, it might be possible to squeeze out some more gain from this speculative decoding setup by predicting a few more tokens out.</p>

<h2 id="conclusion">Conclusion</h2>

<p>I see many of the improvements made by DeepSeek as “obvious in retrospect”: they are the kind of innovations that, had someone asked me in advance about them, I would have said were good ideas. However, as I’ve said earlier, this doesn’t mean it’s easy to come up with the ideas in the first place.</p>

<p>I’ve heard many people express the sentiment that the DeepSeek team has “good taste” in research. Based just on these architectural improvements I think that assessment is right. None of these improvements seem like they were found as a result of some brute-force search through possible ideas. Instead, they look like they were carefully devised by researchers who understood how a Transformer works and how its various architectural deficiencies can be addressed.</p>

<p>If I had to guess where similar improvements are likely to be found next, probably prioritization of compute would be a good bet. Right now, a Transformer spends the same amount of compute per token regardless of which token it’s processing or predicting. This seems intuitively inefficient: the model should think more if it’s making a harder prediction and less if it’s making an easier one. To some extent this can be incorporated into an inference setup through variable test-time compute scaling, but I think there should also be a way to incorporate it into the architecture of the base models directly.</p>


          
        </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Sam Altman said startups with $10M were 'hopeless' competing with OpenAI (138 pts)]]></title>
            <link>https://www.tomshardware.com/tech-industry/artificial-intelligence/sam-altman-said-startups-with-only-usd10-million-were-totally-hopeless-competing-with-openai-deepseeks-disruption-says-otherwise</link>
            <guid>42854525</guid>
            <pubDate>Tue, 28 Jan 2025 16:48:58 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.tomshardware.com/tech-industry/artificial-intelligence/sam-altman-said-startups-with-only-usd10-million-were-totally-hopeless-competing-with-openai-deepseeks-disruption-says-otherwise">https://www.tomshardware.com/tech-industry/artificial-intelligence/sam-altman-said-startups-with-only-usd10-million-were-totally-hopeless-competing-with-openai-deepseeks-disruption-says-otherwise</a>, See on <a href="https://news.ycombinator.com/item?id=42854525">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-widget-type="contentparsed" id="content">

<section>
<div>
<div>
<picture data-new-v2-image="true">
<source type="image/webp" srcset="https://cdn.mos.cms.futurecdn.net/4ee5JvxCs6AZfG3DdobJMZ-1920-80.jpg.webp 1920w, https://cdn.mos.cms.futurecdn.net/4ee5JvxCs6AZfG3DdobJMZ-1200-80.jpg.webp 1200w, https://cdn.mos.cms.futurecdn.net/4ee5JvxCs6AZfG3DdobJMZ-1024-80.jpg.webp 1024w, https://cdn.mos.cms.futurecdn.net/4ee5JvxCs6AZfG3DdobJMZ-970-80.jpg.webp 970w, https://cdn.mos.cms.futurecdn.net/4ee5JvxCs6AZfG3DdobJMZ-650-80.jpg.webp 650w, https://cdn.mos.cms.futurecdn.net/4ee5JvxCs6AZfG3DdobJMZ-480-80.jpg.webp 480w, https://cdn.mos.cms.futurecdn.net/4ee5JvxCs6AZfG3DdobJMZ-320-80.jpg.webp 320w" sizes="(min-width: 1000px) 600px, calc(100vw - 40px)">
<img src="https://cdn.mos.cms.futurecdn.net/4ee5JvxCs6AZfG3DdobJMZ-1200-80.jpg" alt="Sam Altman at a Q&amp;amp;A in June 2023" srcset="https://cdn.mos.cms.futurecdn.net/4ee5JvxCs6AZfG3DdobJMZ-1920-80.jpg 1920w, https://cdn.mos.cms.futurecdn.net/4ee5JvxCs6AZfG3DdobJMZ-1200-80.jpg 1200w, https://cdn.mos.cms.futurecdn.net/4ee5JvxCs6AZfG3DdobJMZ-1024-80.jpg 1024w, https://cdn.mos.cms.futurecdn.net/4ee5JvxCs6AZfG3DdobJMZ-970-80.jpg 970w, https://cdn.mos.cms.futurecdn.net/4ee5JvxCs6AZfG3DdobJMZ-650-80.jpg 650w, https://cdn.mos.cms.futurecdn.net/4ee5JvxCs6AZfG3DdobJMZ-480-80.jpg 480w, https://cdn.mos.cms.futurecdn.net/4ee5JvxCs6AZfG3DdobJMZ-320-80.jpg 320w" sizes="(min-width: 1000px) 600px, calc(100vw - 40px)" data-original-mos="https://cdn.mos.cms.futurecdn.net/4ee5JvxCs6AZfG3DdobJMZ.jpg" data-pin-media="https://cdn.mos.cms.futurecdn.net/4ee5JvxCs6AZfG3DdobJMZ.jpg" data-pin-nopin="true" fetchpriority="high" crossorigin="anonymous">
</picture>
</div>
<figcaption>
<span>(Image credit: ET Now video)</span>
</figcaption>
</div>

<div id="article-body">
<p>Sam Altman's comments on the prospects of startups hoping to break through in the AI business may have come back to bite him. Several posts on X (and probably other platforms) ridicule the <a data-analytics-id="inline-link" href="https://www.tomshardware.com/tech-industry/artificial-intelligence/openai-and-microsoft-reportedly-planning-dollar100-billion-datacenter-project-for-an-ai-supercomputer" data-before-rewrite-localise="https://www.tomshardware.com/tech-industry/artificial-intelligence/openai-and-microsoft-reportedly-planning-dollar100-billion-datacenter-project-for-an-ai-supercomputer">OpenAI</a> boss and co-founder's dismissal of potential competition emanating from the startup scene, particularly those with only limited financial resources in the range of $10 million. Altman's comments were made during a Q&amp;A session after a 'Conversations' presentation to India VCs, recorded in June 2023. The comments seem way off the mark in early 2025, with DeepSeek now on the scene claiming its groundbreaking model only cost $5.6 million to train.</p><div><blockquote data-lang="en"><p lang="en" dir="ltr">This is pretty hilarious in retrospect.In India in 2023, Altman was asked how if a small, smart team with a budget of $10 million could build something substantial within AI.His reply: "It’s totally hopeless to compete with us on training foundation models" https://t.co/pdYIhV2x1m<a href="https://twitter.com/cantworkitout/status/1884113684978622538" data-url="https://twitter.com/cantworkitout/status/1884113684978622538" target="_blank" referrerpolicy="no-referrer-when-downgrade" data-hl-processed="none">January 28, 2025</a></p></blockquote></div><p>Entrepreneur Arnaud Bertrand reckons that Altman's response in the above clip is "pretty hilarious in retrospect." In other words, Bertrand thinks Altman's dismissal of the Indian VC's question about startups challenging the likes of OpenAI showed a startling lack of foresight.</p><p>The video begins with the VC stating that there is a very vibrant startup ecosystem in India. He goes on to muse whether Altman might see a gap in the AI business, one which an Indian startup could fill. More specifically, the VC asks whether a trio of super-smart engineers from India "with say, not $100M, but $10M – could build something truly substantial?"</p><p>Altman's response was quite dismissive of the VC's well-mannered query. "Look, the way this works is we're going to tell you it's totally hopeless to compete with us on training foundation models. You shouldn't try, and it's your job to try anyway, and I believe both of those things," was Altman's disjointed stream-of-consciousness style reply, followed by audience titters. "I think it is pretty hopeless," he added, possibly wishing to soften his initial response.</p><div><blockquote data-lang="en"><p lang="en" dir="ltr">deepseek's r1 is an impressive model, particularly around what they're able to deliver for the price.we will obviously deliver much better models and also it's legit invigorating to have a new competitor! we will pull up some releases.<a href="https://twitter.com/cantworkitout/status/1884066337103962416" data-url="https://twitter.com/cantworkitout/status/1884066337103962416" target="_blank" referrerpolicy="no-referrer-when-downgrade" data-hl-processed="none">January 28, 2025</a></p></blockquote></div><p>To Altman's credit, earlier today he posted a thread on X praising the catalyst behind his recent social media ridiculing. The launch of China's DeepSeek has caused significant AI business and <a data-analytics-id="inline-link" href="https://www.tomshardware.com/tech-industry/artificial-intelligence/nvidia-loses-usd589-billion-in-market-cap-broad-stock-plunge-triggered-by-deepseek-ai-release" data-before-rewrite-localise="https://www.tomshardware.com/tech-industry/artificial-intelligence/nvidia-loses-usd589-billion-in-market-cap-broad-stock-plunge-triggered-by-deepseek-ai-release">tech industry tremors</a>, and Altman has now publicly admitted it is "an impressive model, particularly around what they're able to deliver for the price."</p><p>Nevertheless, like the funding-hungry CEO he is, Altman quickly turned the thread around to OpenAI promising jam tomorrow, with the execution of the firm's roadmap, amazing next-gen AI models, and "bringing you all AGI and beyond."</p><p>The amount of money DeepSeek truly spent on training its model, which it claims is $5.6 million, is contested. However, despite those contentions, it is clear that the company pulled off training a frontier model with disruptively low costs, shocking the US titans of AI.</p><div data-hydrate="true" id="slice-container-newsletterForm-articleInbodyContent-HyBY2hdXNd75Zxm2t4xDaJ"><section><p>Get Tom's Hardware's best news and in-depth reviews, straight to your inbox.</p></section></div>
</div>



<!-- Drop in a standard article here maybe? -->


<div id="slice-container-authorBio-HyBY2hdXNd75Zxm2t4xDaJ"><p>Mark Tyson is a news editor at Tom's Hardware. He enjoys covering the full breadth of PC tech; from business and semiconductor design to products approaching the edge of reason.</p></div>
</section>





<div id="slice-container-relatedArticles"><p><h3>Most Popular</h3></p></div>








</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Bitwarden introduces mandatory 2FA for new devices (217 pts)]]></title>
            <link>https://bitwarden.com/help/new-device-verification/</link>
            <guid>42853696</guid>
            <pubDate>Tue, 28 Jan 2025 15:51:26 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://bitwarden.com/help/new-device-verification/">https://bitwarden.com/help/new-device-verification/</a>, See on <a href="https://news.ycombinator.com/item?id=42853696">Hacker News</a></p>
<div id="readability-page-1" class="page"><article><p>To keep your account safe and secure, in February 2025, Bitwarden will require additional verification <strong>for users who do not use two-step login</strong>. After entering your Bitwarden master password, you will be prompted to enter a one-time verification code sent to your account email to complete the login process <strong>when logging in from a device you have not logged in to previously</strong>. For example, if you are logging in to a mobile app or a browser extension that you have used before, you will not receive this prompt.</p><p>Most users will not experience this prompt unless they are frequently logging into new devices. This verification is only needed for new devices or after clearing browser cookies.</p><p>If you regularly access your email, retrieving the verification code should be straightforward. If you prefer not to rely on your Bitwarden account email for verification, you can set up two-step login through an Authenticator app, a hardware key, or two-step login via a different email.</p><p>Users affected by this change will see the following in-product communication and should have received an email informing them of the change:</p><figure><img alt="New device verification announcement" id="3194724a-920f-5941-8c03-e54e6e214cb2" height="532" width="468" src="https://res.cloudinary.com/bw-com/image/upload/f_auto/v1/ctf/7rncvj1f8mw7/2tYcj1sDClqQ23ANh7nrIq/b88330b0a660fa47baaf59c68e0b9178/541c2ef1-b086-4489-a21f-a21fa6c1a905.png?_a=DAJAUVWIZAA0"><figcaption><cite>New device verification announcement</cite></figcaption></figure><p id="faqs"><a href="#faqs" title="#faqs"><svg version="1.1" viewBox="0 0 32 32"><path d="M30.939 9.669l-7.372-0.014 1.332-8.422c0.022-0.138 0.016-0.279-0.016-0.415s-0.092-0.264-0.174-0.377c-0.082-0.113-0.186-0.209-0.305-0.282s-0.251-0.122-0.389-0.144-0.279-0.016-0.415 0.016c-0.136 0.033-0.264 0.092-0.377 0.174-0.229 0.166-0.382 0.415-0.426 0.694l-1.388 8.75-8.728-0.018 1.316-8.31c0.028-0.141 0.027-0.286-0.002-0.426s-0.087-0.274-0.169-0.391c-0.082-0.118-0.187-0.218-0.309-0.294s-0.257-0.127-0.399-0.15c-0.142-0.022-0.287-0.016-0.426 0.019s-0.27 0.097-0.384 0.184c-0.114 0.087-0.21 0.195-0.282 0.32s-0.117 0.262-0.134 0.405l-1.366 8.64-7.776-0.016c-0.283 0-0.554 0.112-0.754 0.312s-0.312 0.471-0.312 0.754c0 0.283 0.112 0.554 0.312 0.754s0.471 0.312 0.754 0.312l7.442 0.014-1.4 8.876-7.718-0.016c-0.283 0-0.554 0.112-0.754 0.312s-0.312 0.471-0.312 0.754c0 0.283 0.112 0.554 0.312 0.754s0.471 0.312 0.754 0.312l7.384 0.016-1.266 8c-0.044 0.279 0.024 0.564 0.19 0.793s0.416 0.382 0.694 0.427c0.052 0.009 0.104 0.013 0.156 0.014 0.254 0 0.499-0.091 0.692-0.256s0.32-0.394 0.358-0.644l1.32-8.33 8.73 0.018-1.248 7.89c-0.044 0.279 0.024 0.564 0.19 0.793s0.415 0.382 0.694 0.427c0.056 0.009 0.112 0.013 0.168 0.014 0.253-0.001 0.498-0.092 0.691-0.257s0.32-0.393 0.359-0.643l1.3-8.218 7.762 0.016c0.283 0 0.554-0.112 0.754-0.312s0.312-0.471 0.312-0.754c0-0.283-0.112-0.554-0.312-0.754s-0.471-0.312-0.754-0.312l-7.428-0.016 1.4-8.876 7.704 0.016c0.283 0 0.554-0.112 0.754-0.312s0.312-0.471 0.312-0.754-0.112-0.554-0.312-0.754c-0.2-0.2-0.471-0.312-0.754-0.312h0.014zM19.671 20.657l-8.732-0.018 1.4-8.876 8.73 0.018-1.398 8.876z"></path></svg></a><h2>FAQs</h2></p><h4>When will this happen?</h4><p>This change will go into effect starting February 2025.</p><h4>Why is Bitwarden implementing this?</h4><p>Bitwarden is implementing this change to enhance security for users who don't have two-step login activated. If someone gains access to your password, they still won't be able to log into your account without secondary verification (the code sent to your email). This extra layer helps protect your data from hackers who often target weak or exposed passwords to gain unauthorized access.</p><h4>When will I get prompted for this verification?</h4><p>You will only get prompted for this verification when logging in from new devices. If you’re logging into a device that you’ve used before, you will not be prompted.&nbsp; </p><h4>What is considered a new device?&nbsp;</h4><p>A new device is any device that hasn't been previously used to log into your Bitwarden account. This could include a new phone, tablet, computer, or browser extension that you’ve never logged in from before. When you log in from a new device, you'll be asked to verify your identity via a one-time code sent to your email.&nbsp;</p><p>Other scenarios that will initiate a new device will be:</p><ul><li><p>If you uninstall and reinstall the mobile or desktop app</p></li><li><p>Clearing browser cookies&nbsp;<br></p></li></ul><h4>My email credentials are saved in Bitwarden. Will I be locked out of Bitwarden?</h4><p>Email verification codes will only be required on new devices for users that do not have two-step login enabled. You will not see this prompt on previously logged in devices and you will log in as normal with your account email and your master password.&nbsp;</p><p>If you are logging into a new device, your Bitwarden account email will receive a one-time verification code. If you have access to your email, i.e. a persistent logged in email on your mobile phone, then you will be able to grab the one-time verification code to log in. Once logged in to the new device, you will not be prompted again for the verification code.&nbsp;</p><p>If you regularly log into your email using credentials saved in Bitwarden or do not want to rely on your email for verification, you should set up two-step login that will be independent from the Bitwarden account email. This includes an authenticator app, security key, or email-based two-step login with a different email. Having any 2FA method active will opt the user out of the email-based new device verification. Users with 2FA active should also save their Bitwarden <a href="https://bitwarden.com/help/two-step-recovery-code/">recovery code</a> in a safe place.</p><h4>Who is excluded from this account email-based new device verification?</h4><p>The following categories of logins are excluded:</p><ul><li><p>Users who have two-step login set up are excluded.</p></li><li><p>Users who log in with SSO, a passkey, or with an API key are excluded.</p></li><li><p>Self-hosted users are excluded.</p></li><li><p>Users who log in from a device where they have previously logged in are excluded.</p></li></ul><h4>My organization uses SSO, do my users have to complete new device verification?</h4><p>No. Users logging in with SSO will be exempt and not asked to verify the login on a new device. However, if a user, without two-step login enabled, logs in with a username and password without going through SSO, they will be asked to verify the new device.</p><h4>I do not want to share my real email with Bitwarden, how can I set up my account?</h4><p>Users who want to remain anonymous have several options available:</p><ul><li><p>Use a two-step login option that doesn’t require an email, including an authenticator app, security key, or email-based two-step login with a different email.</p></li><li><p>Use an email alias forwarding service.</p></li><li><p>Self-host Bitwarden.</p></li></ul><p>Bitwarden encourages users to have an active email, as Bitwarden sends important security alerts like failed login attempts.</p><div id="help-page-suggestions"><p><span><h3>Suggest changes to this page</h3><p>How can we improve this page for you? <br>For technical, billing, and product questions, please <a href="https://bitwarden.com/contact/">contact support</a></p></span></p></div></article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Boom XB-1 First Supersonic Flight [video] (522 pts)]]></title>
            <link>https://www.youtube.com/watch?v=-qisIViAHwI</link>
            <guid>42853633</guid>
            <pubDate>Tue, 28 Jan 2025 15:46:53 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.youtube.com/watch?v=-qisIViAHwI">https://www.youtube.com/watch?v=-qisIViAHwI</a>, See on <a href="https://news.ycombinator.com/item?id=42853633">Hacker News</a></p>
Couldn't get https://www.youtube.com/watch?v=-qisIViAHwI: Error: timeout of 10000ms exceeded]]></description>
        </item>
        <item>
            <title><![CDATA[Almost one in 10 people use the same four-digit PIN (154 pts)]]></title>
            <link>https://www.abc.net.au/news/2025-01-28/almost-one-in-ten-people-use-the-same-four-digit-pin/103946842</link>
            <guid>42853617</guid>
            <pubDate>Tue, 28 Jan 2025 15:45:25 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.abc.net.au/news/2025-01-28/almost-one-in-ten-people-use-the-same-four-digit-pin/103946842">https://www.abc.net.au/news/2025-01-28/almost-one-in-ten-people-use-the-same-four-digit-pin/103946842</a>, See on <a href="https://news.ycombinator.com/item?id=42853617">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-component="Decoy" data-key="article"><p>Find out if you're one of them.</p><p>The last line of security for much of your digital life probably isn't as secure as you think.</p><p>Whether it's to unlock your smartphone, access your online banking or get cash out of the ATM, a four-digit PIN is often there to keep your secrets and your money safe.</p><p>It’s an important little code, but not all choices are equally secure.</p><p>That's why we analysed 29 million of them from <a href="https://haveibeenpwned.com/" data-component="Link" target="_blank" rel="noopener noreferrer">Have I Been Pwned?</a> – an Australian-run site that helps people all over the world find out if they've been affected by data breaches.</p><p>The most commonly used PINs turned out to be staggeringly popular, meaning they're particularly easy to guess when phones and bank cards fall into the wrong hands.</p><p>This grid of green squares might remind you of an old Space Invaders game, but it's actually something like a mind-reader.</p><figure role="group" data-print="inline-media" aria-labelledby="104257640" data-component="Figure" data-uri="coremedia://image/104257640"><div><p><img alt="A grid of all 10,000 PIN codes, with two bright lines across it showing where the most popular ones are" sizes="100vw" src="https://live-production.wcms.abc-cdn.net.au/29b2240ce9d896959a78aa2e10cbdfd8?src" loading="lazy" data-component="Image" data-lazy="true"></p></div><figcaption id="104257640" data-component="Figure__figcaption"> <!-- -->How popular each of the 10,000 possible PIN codes are.</figcaption></figure><p>It's going to let us peer inside and find out why humans choose some PINs more than others.</p><p>Every square in the grid represents a four-digit code.</p><p>We've highlighted <strong>4560</strong> as an example to get you across how it works.</p><p>The grid is arranged by splitting the digits of each code into pairs.</p><p>The first two digits (<strong>45</strong>) are taken from the vertical axis, and the last two digits (<strong>60</strong>) are from the horizontal one.</p><p>The brighter the square, the more popular the code – which means the blocks of bright squares are the ones we need to avoid when choosing a PIN.</p><p>So, which number is the clear favourite? Chances are you've used it at some stage.</p><figure role="group" data-print="inline-media" aria-labelledby="104257642" data-component="Figure" data-uri="coremedia://image/104257642"><div><p><img alt="1234 is highlighted on the grid of all possible four-digit codes." sizes="100vw" src="https://live-production.wcms.abc-cdn.net.au/87cdc6eee4aef6f923fe3374c144a23f?src" loading="lazy" data-component="Image" data-lazy="true"></p></div><figcaption id="104257642" data-component="Figure__figcaption"> <!-- -->1234 is easily the most popular four-digit PIN.</figcaption></figure><p><strong>1234</strong> is the most popular choice by a huge margin, accounting for nearly one in 10 of the millions of PINs we looked at.</p><p>And then there's the diagonal line running from the bottom-left corner to the top-right one.</p><p>It stands out, and that's because it's made up of PINs that use repeated digits...</p><p>... like <strong>0000</strong>, which is the second most popular code.</p><figure role="group" data-print="inline-media" aria-labelledby="104257644" data-component="Figure" data-uri="coremedia://image/104257644"><div><p><img alt="1111 is highlighted on the grid of all possible four-digit codes." sizes="100vw" src="https://live-production.wcms.abc-cdn.net.au/1cf7163c5868ca943d8b42247f185296?src" loading="lazy" data-component="Image" data-lazy="true"></p></div><figcaption id="104257644" data-component="Figure__figcaption"> <!-- -->1111 sits on the diagonal line from bottom-left to top-right.</figcaption></figure><p>And right behind it is <strong>1111</strong>.</p><p><strong>1212</strong> and <strong>4444</strong> are in the top ten as well.</p><figure role="group" data-print="inline-media" aria-labelledby="104257646" data-component="Figure" data-uri="coremedia://image/104257646"><div><p><img alt="1212 and 4444 are highlighted on the grid of all possible four-digit codes." sizes="100vw" src="https://live-production.wcms.abc-cdn.net.au/363ac8ca9bb7865ddb26fc788dea7ef4?src" loading="lazy" data-component="Image" data-lazy="true"></p></div><figcaption id="104257646" data-component="Figure__figcaption"> <!-- -->1212 and 4444 are also popular choices with repeated digits.</figcaption></figure><p>There's also a (broken) horizontal line, split between <strong>19</strong> and <strong>20</strong> for the first two digits.</p><p>What does that remind you of?</p><figure role="group" data-print="inline-media" aria-labelledby="104257648" data-component="Figure" data-uri="coremedia://image/104257648"><div><p><img alt="2004 and 1986 is highlighted on the grid of all possible four-digit codes." sizes="100vw" src="https://live-production.wcms.abc-cdn.net.au/dbaef0e2a139e7aec8bdb5ca03a15dab?src" loading="lazy" data-component="Image" data-lazy="true"></p></div><figcaption id="104257648" data-component="Figure__figcaption"> <!-- -->1986 is the most popular year to use as a PIN code.</figcaption></figure><p>They are the birth years of people who are alive today.</p><p><strong>1986</strong> is the most popular of these, while <strong>2004</strong> is also in the top 20.</p><p>There's also a block-ish area around the bottom left that needs some explaining.</p><figure role="group" data-print="inline-media" aria-labelledby="104257650" data-component="Figure" data-uri="coremedia://image/104257650"><div><p><img alt="2512 is highlighted on the grid of all possible four-digit codes. It sits inside a block from 0101 to 3112" sizes="100vw" src="https://live-production.wcms.abc-cdn.net.au/78449ac5a9d503082ab3763a8b9b4c82?src" loading="lazy" data-component="Image" data-lazy="true"></p></div><figcaption id="104257650" data-component="Figure__figcaption"> <!-- -->Christmas day (2512) is a popular choice.</figcaption></figure><p>These are all the combinations that could represent dates like <strong>2512</strong>.</p><p><strong>2902</strong> is not as popular as its neighbours, but that's probably because it only comes around once every four years.</p><figure role="group" data-print="inline-media" aria-labelledby="104257652" data-component="Figure" data-uri="coremedia://image/104257652"><div><p><img alt="0229 is highlighted on the grid of all possible four-digit codes." sizes="100vw" src="https://live-production.wcms.abc-cdn.net.au/109c46995b22d40228b6e203074066fc?src" loading="lazy" data-component="Image" data-lazy="true"></p></div><figcaption id="104257652" data-component="Figure__figcaption"> <!-- -->The US style of formatting dates is also popular.</figcaption></figure><p>If you live in the United States, you'd be using <strong>0229</strong> instead.</p><p>This explains the slightly less prominent, yet almost perfectly symmetrical grid overlapping the other one.</p><p>But what about the other popular codes that don't fall on any of these special lines or grids?</p><p>The reason for choosing <strong>4321</strong> is no real mystery. It's just <strong>1234</strong> in reverse.</p><p>Some people have tried to be clever; they've mixed things up by choosing <strong>1342</strong>.</p><p>So many of them, in fact, that it's the 4th most popular code of all.</p><p><strong>2580</strong> might seem like a strange one to be in the top 40…</p><p>…until you realise it draws a line directly down the keypad on a phone.</p><p>It makes sense why some four-digit codes are chosen again and again, but this phenomenon brings with it a serious security risk.</p><p>Even though there are 10,000 possible combinations, when humans get involved that equation changes dramatically.</p><p>If someone wants to unlock a stolen phone – or retrieve money from an ATM – and only have five guesses, this data suggests they still have a one-in-eight chance of guessing correctly.</p><p>And, while it's harder to visualise, there is a similar weakness to be found in regular passwords too.</p><p><strong>1234</strong> was as high as fourth on a list of common passwords compiled by NordPass VPN.</p><p>Even when people have the entire keyboard to choose from, the only choices that were more popular were <strong>123456</strong>, "admin" and "password".</p><p>All in all, it paints a worrying picture of the last line of defence for our digital lives.</p><p>Earlier this year, journalists attending a briefing at the UK's National Cyber Security Centre (NCSC) were given a security code to access the building's facilities.</p><p>The code they were given was <strong>1234</strong>.</p><p>The NCSC later clarified this was <a href="https://www.theregister.com/2024/05/10/ncsc_entry_code/" data-component="Link">only a temporary code used for the briefing</a>.</p><p>And there's a lesson in that: if you're one of the millions of people using an ill-advised PIN, perhaps yours should be temporary too.</p><p>Remember, it's never too late to change yours to something more secure.</p><h2 data-component="Heading">The top 50 codes to avoid</h2><p><em>These are the 50 most popular codes in the full Have I Been Pwned? dataset, in order of popularity.</em></p><div data-component="ContentOverflow"><table data-component="Table"><thead><tr><th>Ranking</th><th>Code</th><th>Popularity</th></tr></thead><tbody><tr><td>1</td><td><strong>1234</strong></td><td>9.0%</td></tr><tr><td>2</td><td><strong>1111</strong></td><td>1.6%</td></tr><tr><td>3</td><td><strong>0000</strong></td><td>1.1%</td></tr><tr><td>4</td><td><strong>1342</strong></td><td>0.6%</td></tr><tr><td>5</td><td><strong>1212</strong></td><td>0.4%</td></tr><tr><td>6</td><td><strong>2222</strong></td><td>0.3%</td></tr><tr><td>7</td><td><strong>4444</strong></td><td>0.3%</td></tr><tr><td>8</td><td><strong>1122</strong></td><td>0.3%</td></tr><tr><td>9</td><td><strong>1986</strong></td><td>0.3%</td></tr><tr><td>10</td><td><strong>2020</strong></td><td>0.3%</td></tr><tr><td>11</td><td><strong>7777</strong></td><td>0.3%</td></tr><tr><td>12</td><td><strong>5555</strong></td><td>0.3%</td></tr><tr><td>13</td><td><strong>1989</strong></td><td>0.3%</td></tr><tr><td>14</td><td><strong>9999</strong></td><td>0.2%</td></tr><tr><td>15</td><td><strong>6969</strong></td><td>0.2%</td></tr><tr><td>16</td><td><strong>2004</strong></td><td>0.2%</td></tr><tr><td>17</td><td><strong>1010</strong></td><td>0.2%</td></tr><tr><td>18</td><td><strong>4321</strong></td><td>0.2%</td></tr><tr><td>19</td><td><strong>6666</strong></td><td>0.2%</td></tr><tr><td>20</td><td><strong>1984</strong></td><td>0.2%</td></tr><tr><td>21</td><td><strong>1987</strong></td><td>0.2%</td></tr><tr><td>22</td><td><strong>1985</strong></td><td>0.2%</td></tr><tr><td>23</td><td><strong>8888</strong></td><td>0.2%</td></tr><tr><td>24</td><td><strong>2000</strong></td><td>0.2%</td></tr><tr><td>25</td><td><strong>1980</strong></td><td>0.2%</td></tr><tr><td>26</td><td><strong>1988</strong></td><td>0.2%</td></tr><tr><td>27</td><td><strong>1982</strong></td><td>0.2%</td></tr><tr><td>28</td><td><strong>2580</strong></td><td>0.2%</td></tr><tr><td>29</td><td><strong>1313</strong></td><td>0.2%</td></tr><tr><td>30</td><td><strong>1990</strong></td><td>0.2%</td></tr><tr><td>31</td><td><strong>1991</strong></td><td>0.2%</td></tr><tr><td>32</td><td><strong>1983</strong></td><td>0.2%</td></tr><tr><td>33</td><td><strong>1978</strong></td><td>0.2%</td></tr><tr><td>34</td><td><strong>1979</strong></td><td>0.2%</td></tr><tr><td>35</td><td><strong>1995</strong></td><td>0.2%</td></tr><tr><td>36</td><td><strong>1994</strong></td><td>0.2%</td></tr><tr><td>37</td><td><strong>1977</strong></td><td>0.2%</td></tr><tr><td>38</td><td><strong>1981</strong></td><td>0.2%</td></tr><tr><td>39</td><td><strong>3333</strong></td><td>0.2%</td></tr><tr><td>40</td><td><strong>1992</strong></td><td>0.2%</td></tr><tr><td>41</td><td><strong>1975</strong></td><td>0.2%</td></tr><tr><td>42</td><td><strong>2005</strong></td><td>0.2%</td></tr><tr><td>43</td><td><strong>1993</strong></td><td>0.2%</td></tr><tr><td>44</td><td><strong>1976</strong></td><td>0.2%</td></tr><tr><td>45</td><td><strong>1996</strong></td><td>0.2%</td></tr><tr><td>46</td><td><strong>2002</strong></td><td>0.2%</td></tr><tr><td>47</td><td><strong>1973</strong></td><td>0.2%</td></tr><tr><td>48</td><td><strong>2468</strong></td><td>0.2%</td></tr><tr><td>49</td><td><strong>1998</strong></td><td>0.1%</td></tr><tr><td>50</td><td><strong>1974</strong></td><td>0.1%</td></tr></tbody></table></div><ul data-component="List" role="list"><li data-component="ListItem"><span></span><strong>1234</strong></li><li data-component="ListItem"><span></span><strong>1111</strong></li><li data-component="ListItem"><span></span><strong>0000</strong></li><li data-component="ListItem"><span></span><strong>1342</strong></li><li data-component="ListItem"><span></span><strong>1212</strong></li><li data-component="ListItem"><span></span><strong>2222</strong></li><li data-component="ListItem"><span></span><strong>4444</strong></li><li data-component="ListItem"><span></span><strong>1122</strong></li><li data-component="ListItem"><span></span><strong>1986</strong></li><li data-component="ListItem"><span></span><strong>2020</strong></li><li data-component="ListItem"><span></span><strong>7777</strong></li><li data-component="ListItem"><span></span><strong>5555</strong></li><li data-component="ListItem"><span></span><strong>1989</strong></li><li data-component="ListItem"><span></span><strong>9999</strong></li><li data-component="ListItem"><span></span><strong>6969</strong></li><li data-component="ListItem"><span></span><strong>2004</strong></li><li data-component="ListItem"><span></span><strong>1010</strong></li><li data-component="ListItem"><span></span><strong>4321</strong></li><li data-component="ListItem"><span></span><strong>6666</strong></li><li data-component="ListItem"><span></span><strong>1984</strong></li><li data-component="ListItem"><span></span><strong>1987</strong></li><li data-component="ListItem"><span></span><strong>1985</strong></li><li data-component="ListItem"><span></span><strong>8888</strong></li><li data-component="ListItem"><span></span><strong>2000</strong></li><li data-component="ListItem"><span></span><strong>1980</strong></li><li data-component="ListItem"><span></span><strong>1988</strong></li><li data-component="ListItem"><span></span><strong>1982</strong></li><li data-component="ListItem"><span></span><strong>2580</strong></li><li data-component="ListItem"><span></span><strong>1313</strong></li><li data-component="ListItem"><span></span><strong>1990</strong></li><li data-component="ListItem"><span></span><strong>1991</strong></li><li data-component="ListItem"><span></span><strong>1983</strong></li><li data-component="ListItem"><span></span><strong>1978</strong></li><li data-component="ListItem"><span></span><strong>1979</strong></li><li data-component="ListItem"><span></span><strong>1995</strong></li><li data-component="ListItem"><span></span><strong>1994</strong></li><li data-component="ListItem"><span></span><strong>1977</strong></li><li data-component="ListItem"><span></span><strong>1981</strong></li><li data-component="ListItem"><span></span><strong>3333</strong></li><li data-component="ListItem"><span></span><strong>1992</strong></li><li data-component="ListItem"><span></span><strong>1975</strong></li><li data-component="ListItem"><span></span><strong>2005</strong></li><li data-component="ListItem"><span></span><strong>1993</strong></li><li data-component="ListItem"><span></span><strong>1976</strong></li><li data-component="ListItem"><span></span><strong>1996</strong></li><li data-component="ListItem"><span></span><strong>2002</strong></li><li data-component="ListItem"><span></span><strong>1973</strong></li><li data-component="ListItem"><span></span><strong>2468</strong></li><li data-component="ListItem"><span></span><strong>1998</strong></li><li data-component="ListItem"><span></span><strong>1974</strong></li></ul><h2 data-component="Heading">About this story</h2><ul data-component="List" role="list"><li data-component="ListItem"><span></span>This visualisation was inspired by similar work from <a href="http://datagenetics.com/blog/september32012/index.html" data-component="Link">Nick Berry</a> in 2013</li><li data-component="ListItem"><span></span>The popularity of each PIN code was retrieved from <a href="https://haveibeenpwned.com/Passwords" data-component="Link">Have I Been Pwned's pwned passwords API</a>, which includes passwords leaked from a variety of sources and likely contains duplicate data. While it isn't a perfect data set, it aligns with likely usage patterns, even if it's just because people repeat their PIN codes on their computers.</li></ul><h2 data-component="Heading">Credits</h2><ul data-component="List" role="list"><li data-component="ListItem"><span></span><strong>Reporting and development:</strong> <a href="https://www.abc.net.au/news/julian-fell/13905936" data-component="ContentLink" data-uri="coremedia://person/13905936">Julian Fell</a></li><li data-component="ListItem"><span></span><strong>Design:</strong> <a href="https://www.abc.net.au/news/teresa-tan/9250964" data-component="ContentLink" data-uri="coremedia://person/9250964">Teresa Tan</a></li><li data-component="ListItem"><span></span><strong>Editing:</strong> <a href="https://www.abc.net.au/news/cristen-tilley/4942860" data-component="ContentLink" data-uri="coremedia://person/4942860">Cristen Tilley</a></li></ul></div><p><span data-component="Text">Posted<!-- -->&nbsp;</span><time data-component="ScreenReaderOnly" datetime="2025-01-27T18:53:33.000Z">22 hours ago</time><time data-component="Text">Mon 27 Jan 2025 at 6:53pm</time>, <span data-component="Text">updated<!-- -->&nbsp;</span><time data-component="ScreenReaderOnly" datetime="2025-01-28T01:28:13.000Z">16 hours ago</time><time data-component="Text">Tue 28 Jan 2025 at 1:28am</time></p></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Maxima in the browser using Embedded Common Lisp on WASM (197 pts)]]></title>
            <link>https://maxima-on-wasm.pages.dev/</link>
            <guid>42853528</guid>
            <pubDate>Tue, 28 Jan 2025 15:37:32 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://maxima-on-wasm.pages.dev/">https://maxima-on-wasm.pages.dev/</a>, See on <a href="https://news.ycombinator.com/item?id=42853528">Hacker News</a></p>
Couldn't get https://maxima-on-wasm.pages.dev/: Error: timeout of 10000ms exceeded]]></description>
        </item>
        <item>
            <title><![CDATA[IAC confirms existence of a Super-earth in the habitable zone of a Sun-like Star (295 pts)]]></title>
            <link>https://www.iac.es/en/outreach/news/iac-confirms-existence-super-earth-habitable-zone-sun-star</link>
            <guid>42853174</guid>
            <pubDate>Tue, 28 Jan 2025 15:09:52 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.iac.es/en/outreach/news/iac-confirms-existence-super-earth-habitable-zone-sun-star">https://www.iac.es/en/outreach/news/iac-confirms-existence-super-earth-habitable-zone-sun-star</a>, See on <a href="https://news.ycombinator.com/item?id=42853174">Hacker News</a></p>
Couldn't get https://www.iac.es/en/outreach/news/iac-confirms-existence-super-earth-habitable-zone-sun-star: Error: timeout of 10000ms exceeded]]></description>
        </item>
        <item>
            <title><![CDATA[DeepSeek: X2 Speed for WASM with SIMD (792 pts)]]></title>
            <link>https://simonwillison.net/2025/Jan/27/llamacpp-pr/</link>
            <guid>42852866</guid>
            <pubDate>Tue, 28 Jan 2025 14:44:06 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://simonwillison.net/2025/Jan/27/llamacpp-pr/">https://simonwillison.net/2025/Jan/27/llamacpp-pr/</a>, See on <a href="https://news.ycombinator.com/item?id=42852866">Hacker News</a></p>
Couldn't get https://simonwillison.net/2025/Jan/27/llamacpp-pr/: Error: timeout of 10000ms exceeded]]></description>
        </item>
        <item>
            <title><![CDATA[Janus Pro 1B running 100% locally in-browser on WebGPU (202 pts)]]></title>
            <link>https://old.reddit.com/r/LocalLLaMA/comments/1ibnso0/janus_pro_1b_running_100_locally_inbrowser_on/</link>
            <guid>42852400</guid>
            <pubDate>Tue, 28 Jan 2025 14:04:21 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://old.reddit.com/r/LocalLLaMA/comments/1ibnso0/janus_pro_1b_running_100_locally_inbrowser_on/">https://old.reddit.com/r/LocalLLaMA/comments/1ibnso0/janus_pro_1b_running_100_locally_inbrowser_on/</a>, See on <a href="https://news.ycombinator.com/item?id=42852400">Hacker News</a></p>
Couldn't get https://old.reddit.com/r/LocalLLaMA/comments/1ibnso0/janus_pro_1b_running_100_locally_inbrowser_on/: Error: timeout of 10000ms exceeded]]></description>
        </item>
        <item>
            <title><![CDATA[Osaka bans smoking on all of its streets, vaping included (109 pts)]]></title>
            <link>https://soranews24.com/2025/01/28/osaka-bans-smoking-on-all-of-its-streets-violators-will-be-fined-by-patrols-vaping-included/</link>
            <guid>42852073</guid>
            <pubDate>Tue, 28 Jan 2025 13:31:44 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://soranews24.com/2025/01/28/osaka-bans-smoking-on-all-of-its-streets-violators-will-be-fined-by-patrols-vaping-included/">https://soranews24.com/2025/01/28/osaka-bans-smoking-on-all-of-its-streets-violators-will-be-fined-by-patrols-vaping-included/</a>, See on <a href="https://news.ycombinator.com/item?id=42852073">Hacker News</a></p>
Couldn't get https://soranews24.com/2025/01/28/osaka-bans-smoking-on-all-of-its-streets-violators-will-be-fined-by-patrols-vaping-included/: Error: timeout of 10000ms exceeded]]></description>
        </item>
        <item>
            <title><![CDATA[US pauses all Federal aid and grants (231 pts)]]></title>
            <link>https://www.bbc.com/news/articles/c77rdy6gzy5o</link>
            <guid>42851248</guid>
            <pubDate>Tue, 28 Jan 2025 11:34:54 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.bbc.com/news/articles/c77rdy6gzy5o">https://www.bbc.com/news/articles/c77rdy6gzy5o</a>, See on <a href="https://news.ycombinator.com/item?id=42851248">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-testid="byline-new" data-component="byline-block"><p><span>James FitzGerald &amp; Ana Faguy</span></p><p><span>BBC News<!-- --></span></p></div><div data-component="text-block"><p>US President Donald Trump has paused grants, loans and other federal assistance, according to a leaked government memo verified by the BBC's US partner, CBS News.<!-- --></p><p>In the memo, the acting head of the Office of Management and Budget (OMB) calls on government agencies to ensure spending is consistent with Trump's priorities.<!-- --></p><p>The full impact of the pause is not yet clear, although the memo specifies that Medicare and Social Security benefits are not affected. It comes days after the US halted nearly all foreign aid.<!-- --></p><p>The move has been criticised by members of the rival Democratic Party who warn of "devastating consequences" on programmes that people rely on. <!-- --></p></div><div data-component="text-block"><p>Diane Yentel of the National Council of Nonprofits said the order could stop cancer research, food assistance and suicide hotlines.<!-- --></p><p>Given the spending that is now on hold was apportioned by Congress, it is likely this will face legal challenges about the scope of presidential power.<!-- --></p><p>The memo, signed by acting OMB chief Matthew Vaeth, calls on government agencies to temporarily pause their financial assistance programmes, so they can review spending that could be impacted by the various orders Trump has signed .<!-- --></p><p>It says this encompasses "financial assistance for foreign aid, nongovernmental organizations, DEI, woke gender ideology, and the green new deal".<!-- --></p></div><div data-component="text-block"><p>A deadline of 17:00 EST (22:00 GMT) has been set. Each agency is told to pause the issuing of new awards as well as the disbursement of funds under existing awards. <!-- --></p><p>The memo further demands that all agencies report which programmes have been paused by 10 February. <!-- --></p><p>The White House has not yet commented officially on the leaked document.<!-- --></p><p>Democrats in Washington DC were quick to sound an alarm of concern about the plan. <!-- --></p><p>The top Democratic appropriators in Congress - Washington Senator Patty Murray and Connecticut Congresswoman Rosa DeLauro - sent a letter to the White House Monday evening expressing their "extreme alarm" with the memo. <!-- --></p><p>"The scope of what you are ordering is breathtaking, unprecedented, and will have devastating consequences across the country," the congresswomen wrote. "We write today to urge you in the strongest possible terms to uphold the law and the Constitution and ensure all federal resources are delivered in accordance with the law."<!-- --></p><p>The Democratic minority leader of the US Senate, Chuck Schumer, was also critical of the pause: "Congress approved these investments and they are not optional; they are the law." <!-- --></p><p>He added: "It will mean missed payrolls and rent payments and everything in between: chaos for everything from universities to non-profit charities."<!-- --></p><p>The move follows last week's news that the Department of State had issued a halt to nearly all existing foreign assistance and paused new aid, according to an internal memo sent to officials and US embassies abroad. <!-- --></p><p>It appeared to affect everything from development assistance to military aid, making exceptions only for emergency food aid and for military funding for Israel and Egypt.<!-- --></p><p>Trump had earlier issued an executive order for a 90-day pause in foreign development assistance pending a review of efficiencies and consistency with his foreign policy. <!-- --></p><p>The US is the world's biggest international aid donor, having spent $68bn (£66bn) in 2023 according to government figures. The State Department notice appears to affect everything from development assistance to military aid.<!-- --></p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Cleveland police used AI to justify a search warrant. It derailed a murder case (134 pts)]]></title>
            <link>https://www.cleveland.com/news/2025/01/cleveland-police-used-ai-to-justify-a-search-warrant-it-has-derailed-a-murder-case.html</link>
            <guid>42851124</guid>
            <pubDate>Tue, 28 Jan 2025 11:17:54 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.cleveland.com/news/2025/01/cleveland-police-used-ai-to-justify-a-search-warrant-it-has-derailed-a-murder-case.html">https://www.cleveland.com/news/2025/01/cleveland-police-used-ai-to-justify-a-search-warrant-it-has-derailed-a-murder-case.html</a>, See on <a href="https://news.ycombinator.com/item?id=42851124">Hacker News</a></p>
Couldn't get https://www.cleveland.com/news/2025/01/cleveland-police-used-ai-to-justify-a-search-warrant-it-has-derailed-a-murder-case.html: Error: timeout of 10000ms exceeded]]></description>
        </item>
        <item>
            <title><![CDATA[US Civil servants are being asked who they voted for in 2024 election (138 pts)]]></title>
            <link>https://www.independent.co.uk/news/world/americas/us-politics/trump-civil-service-loyalty-transition-b2678674.html</link>
            <guid>42850644</guid>
            <pubDate>Tue, 28 Jan 2025 09:58:43 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.independent.co.uk/news/world/americas/us-politics/trump-civil-service-loyalty-transition-b2678674.html">https://www.independent.co.uk/news/world/americas/us-politics/trump-civil-service-loyalty-transition-b2678674.html</a>, See on <a href="https://news.ycombinator.com/item?id=42850644">Hacker News</a></p>
Couldn't get https://www.independent.co.uk/news/world/americas/us-politics/trump-civil-service-loyalty-transition-b2678674.html: Error: timeout of 10000ms exceeded]]></description>
        </item>
        <item>
            <title><![CDATA[DeepSeek-R1 with Dynamic 1.58-bit Quantization (667 pts)]]></title>
            <link>https://unsloth.ai/blog/deepseekr1-dynamic</link>
            <guid>42850222</guid>
            <pubDate>Tue, 28 Jan 2025 08:52:47 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://unsloth.ai/blog/deepseekr1-dynamic">https://unsloth.ai/blog/deepseekr1-dynamic</a>, See on <a href="https://news.ycombinator.com/item?id=42850222">Hacker News</a></p>
Couldn't get https://unsloth.ai/blog/deepseekr1-dynamic: Error: timeout of 10000ms exceeded]]></description>
        </item>
        <item>
            <title><![CDATA[Reinforcement Learning – A Reference (106 pts)]]></title>
            <link>https://jakubhalmes.substack.com/p/reinforcement-learning-a-reference</link>
            <guid>42850111</guid>
            <pubDate>Tue, 28 Jan 2025 08:31:53 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://jakubhalmes.substack.com/p/reinforcement-learning-a-reference">https://jakubhalmes.substack.com/p/reinforcement-learning-a-reference</a>, See on <a href="https://news.ycombinator.com/item?id=42850111">Hacker News</a></p>
Couldn't get https://jakubhalmes.substack.com/p/reinforcement-learning-a-reference: Error: timeout of 10000ms exceeded]]></description>
        </item>
        <item>
            <title><![CDATA[FTC Takes Action Against GoDaddy for Alleged Lax Data Security (202 pts)]]></title>
            <link>https://www.ftc.gov/news-events/news/press-releases/2025/01/ftc-takes-action-against-godaddy-alleged-lax-data-security-its-website-hosting-services</link>
            <guid>42849632</guid>
            <pubDate>Tue, 28 Jan 2025 07:02:57 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.ftc.gov/news-events/news/press-releases/2025/01/ftc-takes-action-against-godaddy-alleged-lax-data-security-its-website-hosting-services">https://www.ftc.gov/news-events/news/press-releases/2025/01/ftc-takes-action-against-godaddy-alleged-lax-data-security-its-website-hosting-services</a>, See on <a href="https://news.ycombinator.com/item?id=42849632">Hacker News</a></p>
Couldn't get https://www.ftc.gov/news-events/news/press-releases/2025/01/ftc-takes-action-against-godaddy-alleged-lax-data-security-its-website-hosting-services: Error: timeout of 10000ms exceeded]]></description>
        </item>
        <item>
            <title><![CDATA[Open-R1: an open reproduction of DeepSeek-R1 (380 pts)]]></title>
            <link>https://huggingface.co/blog/open-r1</link>
            <guid>42849536</guid>
            <pubDate>Tue, 28 Jan 2025 06:40:47 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://huggingface.co/blog/open-r1">https://huggingface.co/blog/open-r1</a>, See on <a href="https://news.ycombinator.com/item?id=42849536">Hacker News</a></p>
Couldn't get https://huggingface.co/blog/open-r1: Error: timeout of 10000ms exceeded]]></description>
        </item>
    </channel>
</rss>