<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0">
    <channel>
        <title>HN100 - Readable Contents</title>
        <link>https://hn.algolia.com/api/v1/search_by_date?tags=%28story,poll%29&amp;numericFilters=points%3E100</link>
        <description>Uses Readability to add bodies to the RSS feed</description>
        <lastBuildDate>Sun, 03 Aug 2025 15:30:02 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>en</language>
        <item>
            <title><![CDATA[Show HN: Visualize your dev project with an AI roadmap tool (130 pts)]]></title>
            <link>https://www.archaltect.pro/</link>
            <guid>44775751</guid>
            <pubDate>Sun, 03 Aug 2025 11:11:29 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.archaltect.pro/">https://www.archaltect.pro/</a>, See on <a href="https://news.ycombinator.com/item?id=44775751">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><nav><div><p><img alt="ArchAltect Logo" loading="lazy" width="40" height="40" decoding="async" data-nimg="1" src="https://www.archaltect.pro/logo.svg"></p></div></nav><div><div><p><span>AI-Powered Project Intelligence</span></p></div><div><h2>Turn Your<!-- --> <span><span>Web Ideas</span></span><br>Into<!-- --> <span><svg viewBox="0 0 48 32" fill="currentColor"><path d="M4 16c8 0 8-8 16-8s8 8 16 8-8 8-16 8-8-8-16-8z"></path></svg></span></h2></div><div><p>Transform abstract<span> <!-- -->web project concepts<!-- --> </span>into concrete, actionable plans with AI that understands your vision and guides you to completion.</p></div></div><div id="workflow"><div><h2>Why ArchAltect is a<!-- --> <span><span>Game-Changer</span></span></h2><p>Experience the power of<!-- --> <span>AI-driven project management</span> <!-- -->that transforms how you turn ideas into reality.</p></div><div><div><h3>AI-Powered Intelligence</h3><p>Our AI understands your vision and breaks down complex projects into manageable, actionable steps.</p></div><div><h3>Crystal Clear Roadmaps</h3><p>Get comprehensive project roadmaps that guide you from conception to completion with precision.</p></div><div><h3>Lightning Fast Setup</h3><p>Transform ideas into detailed project plans in minutes, not hours or days.</p></div><div><h3>Complete Lifecycle Management</h3><p>From initial brainstorming to final delivery, manage every aspect of your project seamlessly.</p></div></div><div><div><h3>From Concept to Completion</h3></div><div><div><h4>Ideation</h4><p>Share your vision and watch AI transform it into structured plans</p></div><div><h4>Execution</h4><p>Follow detailed roadmaps with task-by-task guidance</p></div><div><h4>Delivery</h4><p>Complete projects with confidence and celebrate success</p></div></div></div></div><div id="features"><div><h2>Features That<!-- --> <span><span>Set Us Apart</span><svg viewBox="0 0 100 8" fill="currentColor"><path d="M0 4c20 0 20-4 40-4s20 4 40 4-20 4-40 4-20-4-40-4z"></path></svg></span></h2><p>Discover the key features that make ArchAltect the ultimate platform for transforming<!-- --> <span>web ideas into successful projects</span>.</p></div><div><div><h3>Complete Project Lifecycle</h3><p>From initial brainstorming to final delivery, manage every phase of your project with AI-powered insights and recommendations.</p></div><div><h3>Real-Time Progress Tracking</h3><p>Stay on top of your projects with intelligent progress monitoring and predictive timeline adjustments.</p></div><div><p>Forever Yours ✨</p><h3>Permanent Access</h3><p>Even after your subscription ends, you maintain complete  access to all your past projects.</p></div><div><h3>Secure &amp; Reliable</h3><p>Enterprise-grade security ensures your projects and data are always protected and accessible when you need them.</p></div></div></div><div><div><h2>See Your Projects Come to<!-- --> <span><span>Life</span></span></h2><p>From simple ideas to complex applications - watch how ArchAltect transforms your vision into<!-- --> <span>structured, manageable projects</span>.</p></div><div><div><div><p>Comprehensive overview of all your active projects with progress tracking</p></div><p><img alt="Project Dashboard" loading="lazy" width="1200" height="800" decoding="async" data-nimg="1" srcset="https://www.archaltect.pro/_next/image?url=%2Fdashboard-archaltect.jpg&amp;w=1200&amp;q=75 1x, https://www.archaltect.pro/_next/image?url=%2Fdashboard-archaltect.jpg&amp;w=3840&amp;q=75 2x" src="https://www.archaltect.pro/_next/image?url=%2Fdashboard-archaltect.jpg&amp;w=3840&amp;q=75"></p></div><div><div><p>Detailed task breakdown with progress tracking and milestone management</p></div><p><img alt="Project Management" loading="lazy" width="1200" height="800" decoding="async" data-nimg="1" srcset="https://www.archaltect.pro/_next/image?url=%2Froadmap-overview.jpg&amp;w=1200&amp;q=75 1x, https://www.archaltect.pro/_next/image?url=%2Froadmap-overview.jpg&amp;w=3840&amp;q=75 2x" src="https://www.archaltect.pro/_next/image?url=%2Froadmap-overview.jpg&amp;w=3840&amp;q=75"></p></div></div><div><div><div><p>Deep dive into individual tasks with subtasks and resource planning</p></div><p><img alt="Detailed Planning" loading="lazy" width="1200" height="800" decoding="async" data-nimg="1" srcset="https://www.archaltect.pro/_next/image?url=%2Ftasks-overview.jpg&amp;w=1200&amp;q=75 1x, https://www.archaltect.pro/_next/image?url=%2Ftasks-overview.jpg&amp;w=3840&amp;q=75 2x" src="https://www.archaltect.pro/_next/image?url=%2Ftasks-overview.jpg&amp;w=3840&amp;q=75"></p></div><div><div><p>Share your project progress with stakeholders through beautiful public links</p></div><p><img alt="Shareable Project View" loading="lazy" width="1200" height="800" decoding="async" data-nimg="1" srcset="https://www.archaltect.pro/_next/image?url=%2Freadonly-overview-progress.jpg&amp;w=1200&amp;q=75 1x, https://www.archaltect.pro/_next/image?url=%2Freadonly-overview-progress.jpg&amp;w=3840&amp;q=75 2x" src="https://www.archaltect.pro/_next/image?url=%2Freadonly-overview-progress.jpg&amp;w=3840&amp;q=75"></p></div></div></div><div id="demo"><div><h2>See ArchAltect in<!-- --> <span><span>Action</span></span></h2><p>Watch how effortlessly you can transform your ideas into<!-- --> <span>structured, actionable projects</span>.</p></div><div><div><h4>Quick Setup</h4><p>Watch how fast you can create your first project roadmap</p></div><div><h4>Smart Planning</h4><p>See AI break down complex projects into manageable tasks</p></div><div><h4>Progress Tracking</h4><p>Experience real-time insights and progress monitoring</p></div></div></div><div><div><p><span>Ready to Transform Your Ideas?</span></p></div><div><h2>Start Building Your <br><span></span> <span>Today</span></h2></div><p>Join thousands of creators who've turned their biggest ideas into<!-- --> <span>successful projects</span> <!-- -->with ArchAltect's AI-powered platform.</p><div><p><span>•</span></p><div><p><span>Start building immediately</span></p></div></div></div></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Tokens are getting more expensive (113 pts)]]></title>
            <link>https://ethanding.substack.com/p/ai-subscriptions-get-short-squeezed</link>
            <guid>44775700</guid>
            <pubDate>Sun, 03 Aug 2025 11:01:37 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://ethanding.substack.com/p/ai-subscriptions-get-short-squeezed">https://ethanding.substack.com/p/ai-subscriptions-get-short-squeezed</a>, See on <a href="https://news.ycombinator.com/item?id=44775700">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><article><div dir="auto"><blockquote><p><em>note: i’m kinda tired of the “levered beta” metaphor, i have one more topic i want to cover on this topic related to cognition, and then i’ll go back to my normal writing</em></p></blockquote><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/$s_!gRPl!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fdda348d4-6225-4d52-b435-e5692a64ce79_700x449.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!gRPl!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fdda348d4-6225-4d52-b435-e5692a64ce79_700x449.png 424w, https://substackcdn.com/image/fetch/$s_!gRPl!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fdda348d4-6225-4d52-b435-e5692a64ce79_700x449.png 848w, https://substackcdn.com/image/fetch/$s_!gRPl!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fdda348d4-6225-4d52-b435-e5692a64ce79_700x449.png 1272w, https://substackcdn.com/image/fetch/$s_!gRPl!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fdda348d4-6225-4d52-b435-e5692a64ce79_700x449.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/$s_!gRPl!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fdda348d4-6225-4d52-b435-e5692a64ce79_700x449.png" width="700" height="449" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/dda348d4-6225-4d52-b435-e5692a64ce79_700x449.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:449,&quot;width&quot;:700,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:472047,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:false,&quot;topImage&quot;:true,&quot;internalRedirect&quot;:&quot;https://ethanding.substack.com/i/169776630?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fdda348d4-6225-4d52-b435-e5692a64ce79_700x449.png&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/$s_!gRPl!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fdda348d4-6225-4d52-b435-e5692a64ce79_700x449.png 424w, https://substackcdn.com/image/fetch/$s_!gRPl!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fdda348d4-6225-4d52-b435-e5692a64ce79_700x449.png 848w, https://substackcdn.com/image/fetch/$s_!gRPl!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fdda348d4-6225-4d52-b435-e5692a64ce79_700x449.png 1272w, https://substackcdn.com/image/fetch/$s_!gRPl!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fdda348d4-6225-4d52-b435-e5692a64ce79_700x449.png 1456w" sizes="100vw" fetchpriority="high"></picture></div></a></figure></div><p><span>imagine you start a company knowing that consumers won't pay more than $20/month. fine, you think, classic vc playbook - charge at cost, sacrifice margins for growth. you've done the math on cac, ltv, all that. but here's where it gets interesting: you've seen the </span><a href="https://a16z.com/llmflation-llm-inference-cost/" rel="">a16z chart showing llm costs dropping 10x every year</a><span>.</span></p><p>so you think: i'll break even today at $20/month, and when models get 10x cheaper next year, boom - 90% margins. the losses are temporary. the profits are inevitable.</p><p>it’s so simple a VC associate could understand it:</p><ul><li><p>year 1: break even at $20/month</p></li><li><p>year 2: 90% margins as compute drops 10x</p></li><li><p>year 3: yacht shopping</p></li></ul><p>it’s an understandable strategy: "the cost of LLM inference has dropped by a factor of 3 every 6 months, we’ll be fine”</p><p>but after 18 months, margins are about as negative as they’ve ever been… windsurf’s been sold for parts, and claude code has had to roll back their original unlimited $200/mo tier this week.</p><p>companies are still bleeding. the models got cheaper - gpt-3.5 costs 10x less than it used to. but somehow the margins got worse, not better.</p><p>something doesn't add up?</p><p>gpt-3.5 is 10x cheaper than it was. it's also as desirable as a flip phone at an iphone launch.</p><p>when a new model is released as the SOTA, 99% of the demand immediatley shifts over to it. consumers expect this of their products as well.</p><p>now look at the actual pricing history of frontier models, the ones that 99% of the demand is for at any given time:</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/$s_!AEJI!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc608a094-bee9-4952-8cf9-6ff08188f77b_1038x1184.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!AEJI!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc608a094-bee9-4952-8cf9-6ff08188f77b_1038x1184.png 424w, https://substackcdn.com/image/fetch/$s_!AEJI!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc608a094-bee9-4952-8cf9-6ff08188f77b_1038x1184.png 848w, https://substackcdn.com/image/fetch/$s_!AEJI!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc608a094-bee9-4952-8cf9-6ff08188f77b_1038x1184.png 1272w, https://substackcdn.com/image/fetch/$s_!AEJI!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc608a094-bee9-4952-8cf9-6ff08188f77b_1038x1184.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/$s_!AEJI!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc608a094-bee9-4952-8cf9-6ff08188f77b_1038x1184.png" width="1038" height="1184" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/c608a094-bee9-4952-8cf9-6ff08188f77b_1038x1184.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:1184,&quot;width&quot;:1038,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:284400,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://ethanding.substack.com/i/169776630?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc608a094-bee9-4952-8cf9-6ff08188f77b_1038x1184.png&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/$s_!AEJI!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc608a094-bee9-4952-8cf9-6ff08188f77b_1038x1184.png 424w, https://substackcdn.com/image/fetch/$s_!AEJI!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc608a094-bee9-4952-8cf9-6ff08188f77b_1038x1184.png 848w, https://substackcdn.com/image/fetch/$s_!AEJI!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc608a094-bee9-4952-8cf9-6ff08188f77b_1038x1184.png 1272w, https://substackcdn.com/image/fetch/$s_!AEJI!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc608a094-bee9-4952-8cf9-6ff08188f77b_1038x1184.png 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption>source: i made it up</figcaption></figure></div><p>notice something?</p><ul><li><p>when gpt-4 launched at $60, everyone used it despite gpt-3.5 (the previous SOTA) being 26x cheaper.</p></li><li><p>when claude 3 opus launched at $60, people switched despite gpt-4 getting price cuts.</p></li></ul><p>the 10x cost reduction is real, but only for models that might as well be running on a commodore 64.</p><p>so this is the first faulty pillar of the “costs will drop” strategy: demand exists for "the best language model," period. and the best model always costs about the same, because that's what the edge of inference costs today.</p><p>saying "this car is so much cheaper now!" while pointing at a 1995 honda civic misses the point. sure, that specific car is cheaper. but the 2025 toyota camry MSRPs at $30K.</p><p>when you're spending time with an ai—whether coding, writing, or thinking—you always max out on quality. nobody opens claude and thinks, "you know what? let me use the shitty version to save my boss some money." we're cognitively greedy creatures. we want the best brain we can get, especially if we’re balancing the other side with our time.</p><p>"okay, but that's still manageable, right? we just stay at breakeven forever?"</p><p>oh, sweet summer child.</p><p>while it's true each generation of frontier model didn't get more expensive per token, something else happened. something worse. the number of tokens they consumed went absolutely nuclear.</p><p>chatgpt used to reply to a one sentence question with a one sentence reply. now deep research will spend 3 minutes planning, and 20 minutes reading, and another 5 minutes re-writing a report for you while o3 will just run for 20-minutes to answer “hello there”.</p><p>the explosion of rl and test-time compute has resulted in something nobody saw coming: the length of a task that ai can complete has been doubling every six months. what used to return 1,000 tokens is now returning 100,000.</p><p>the math gets genuinely insane when you extrapolate:</p><p>today, a 20-minute "deep research" run costs about $1. by 2027, we'll have agents that can run for 24 hours straight without losing the plot… combine that with the static price of the frontier? that’s a $4320 run. per day. per user. with the ability to run multiple asynchronously.</p><p>once we can deploy agents to run workloads for 24 hours asynchronously, we won't be giving them one instruction and waiting for feedback. we'll be scheduling them in batches. entire fleets of ai workers, attacking problems in parallel, burning tokens like it's 1999.</p><p>obviously - and i cannot stress this enough - a $20/month subscription cannot even support a user making a single $1 deep research run a day. but that's exactly what we're racing toward. every improvement in model capability is an improvement in how much compute they can meaningfully consume at a time</p><p>it's like building a more fuel-efficient engine, then using the efficiency gains to build a monster truck. sure, you're getting more miles per gallon. you're also using 50x more gallons.</p><p>this is the short squeeze that forced windsurf to get margin called [insert thing] — and any “flat rate subscription + useful token intensive” business model startup is looking down the barrel of.</p><p>claude code's max-unlimited experiment was the most sophisticated attempt at weathering this storm we've seen. they tried every trick in the book and still got obliterated.</p><p>their playbook was genuinely clever:</p><p><strong>1. charge 10x the price point</strong></p><p>$200/month when cursor charges $20. start with more buffer before the bleeding begins.</p><p><strong>2. auto-scale models based on load</strong></p><p>switch from opus ($75/m tokens) to sonnet ($15/m) when things get heavy. optimize with haiku for reading. like aws autoscaling, but for brains.</p><p>they almost certainly built this behavior directly into the model weights, which is a paradigm shift we’ll probably see a lot more of</p><p><strong>3. offload processing to user machines</strong></p><p>why spin up your own sandboxes when users have perfectly good cpus sitting idle?</p><p>and despite all this engineering brilliance, token consumption still went supernova.</p><p>ten. billion. tokens. that's 12,500 copies of war and peace. in a month.</p><p>how? even at 10-minute runs, how does someone drive 10b tokens?</p><p>turns out 10-20 minute continuous runs are just long enough for people to discover the for loop. once you decouple token consumption from human time-in-app, physics takes over. set claude on a task. have it check its work. refactor. optimize. repeat until bankruptcy.</p><p>users became api orchestrators running 24/7 code transformation engines on anthropic's dime. the evolution from chat to agent happened overnight. 1000x increase in consumption. phase transition, not gradual change.</p><p>so anthropic rolled back unlimited. they could've tried $2000/month, but the lesson isn't that they didn't charge enough, it’s that there’s no way to offer unlimited usage in this new world under any subscription model.</p><p><span>it's that </span><strong>there is no flat subscription price that works in this new world.</strong></p><p>the math has fundamentally broken.</p><p>this leaves everyone else in an impossible position.</p><p>every ai company knows usage-based pricing would save them. they also know it would kill them. while you're being responsible with $0.01/1k tokens, your vc-funded competitor offers unlimited for $20/month.</p><p>guess where users go?</p><p>classic prisoner's dilemma:</p><ul><li><p>everyone charges usage-based → sustainable industry</p></li><li><p>everyone charges flat-rate → race to the bottom</p></li><li><p>you charge usage, others charge flat → you die alone</p></li><li><p>you charge flat, others charge usage → you win (then die later)</p></li></ul><p>so everyone defects. everyone subsidizes power users. everyone posts hockey stick growth charts. everyone eventually posts "important pricing updates."</p><p>cursor, lovable, replit - they all know the math. they chose growth today, profits tomorrow, bankruptcy eventually but that's the next ceo's problem.</p><p>honestly? probably right. in a land grab, market share beats margins. as long as vcs keep writing checks to paper over unit economics...</p><p>ask jasper what happens when the music stops.</p><p>is it even possible to avoid the token short squeeze?</p><p>recently cognition has been rumored to be raising at $15b, while externally they haven’t even reported $100m arr [i’d guess it’s closer to $50m]. this contrasts with the $10b valuation that cursor raised on at $500m arr with a much steeper curve. more than eight times the revenue, two-thirds the valuation. what do vcs know about cognition that we don't? they're both ai agents that write code. has cognition figured out a way out of the death spiral? [more on this next time I guess]</p><p>there are three ways out:</p><p><strong>1. usage-based pricing from day one</strong></p><p>no subsidies. no "acquire now, monetize later." just honest economics. sounds great in theory.</p><p>except show me the consumer usage-based ai company that's exploding. consumers hate metered billing. they'd rather overpay for unlimited than get surprised by a bill. every successful consumer subscription - netflix, spotify, chatgpt - is flat rate. the moment you add a meter, growth dies.</p><p><strong>2. insane switching costs ⇒ high margins</strong></p><p>this is what devins all in on. they’ve recently announced their citi and goldman sachs parterships, deploying devin to 40,000 software engineers at each company. at $20/mo this is a $10M project, but here’s a question: would you rather have $10M of ARR from goldman sachs or $500m from prosumer devleopers?</p><p><span>the answer is obvious: six-month implementations, compliance reviews, security audits, procurement hell mean that that goldman sachs revenue is hard to win — but once you win it it’s </span><strong>impossible</strong><span> to churn. you only get those contracts if the singular decision maker at the bank is staking their reputation on you — and everyone will do everything they can to make it work.</span></p><p>this is also why the largest software companies outside of the hyperscalers are all system-of-record companies that sell to those exact personas [CRM / ERP / EHRs]. they also all make 80-90% margins because the harder it is to churn, the less price sensitive your buyer is.</p><p>by the time a competitor shows up, you're so deep in the bureaucracy that switching would require another six-month sales cycle. it's not that you can't leave. it's that your cfo would rather die than go through another vendor evaluation.</p><p><strong>3. vertical integration ⇒ make money on the infra</strong></p><p>this is replit’s game: bundle the coding agent with your application hosting, database management, deployment monitoring, logging, etc. lose money on every token, but capture value at every single other layer of the stack for this new generation of developers… just look at how vertically integrated replit is</p><p>use ai as a loss leader to drive consumption of aws-competitive services. you're not selling inference. you're selling everything else, and inference is just marketing spend.</p><p>the genius is that code generation naturally creates demand for hosting. every app needs somewhere to run. every database needs management. every deployment needs monitoring. let openai and anthropic race inference to zero while you own everything else.</p><p>the companies still playing flat-rate-grow-at-all-costs? dead companies walking. they just have very expensive funerals scheduled for q4.</p><p>i keep seeing founders point to the "models will be 10x cheaper next year!" like it’s a life raft. sure. and your users will expect 20x more from them. the goalpost is sprinting away from you.</p><p>remember windsurf? they couldn’t figure out a way to manueveur out bc of the pressure that cursor put on their p&amp;l. even anthropic with the most vertically integrated application layer on the planet can’t make a flat subscription with unlimited usage work.</p><div data-component-name="DigestPostEmbed"><a href="https://ethanding.substack.com/p/levered-beta-is-all-you-need" rel="noopener" target="_blank"><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!IS5S!,w_140,h_140,c_fill,f_webp,q_auto:good,fl_progressive:steep,g_auto/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe9aec1fc-6628-4b82-90d5-a85dc469724d_790x316.png"><img src="https://substackcdn.com/image/fetch/$s_!IS5S!,w_140,h_140,c_fill,f_auto,q_auto:good,fl_progressive:steep,g_auto/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe9aec1fc-6628-4b82-90d5-a85dc469724d_790x316.png" sizes="100vw" alt="levered beta is all you need " width="140" height="140"></picture></div></a></div><p>while the summary from “levered beta is all you need“: which is that being early beats being smart, is still true, being early w/o a plan also means you’re getting to the graveyard first. there's no google writing $2.4b checks for negative margin businesses. there's no "we'll figure it out later" when later means your aws bill is larger than your revenue.</p><p>so how do you build a business in this world? the short answer is be a neocloud — which is the title of my next thing. </p><p>but at least the models will be 10x cheaper next year.</p><p><em>thank you to mark hay, ben mains, nikunj kothari, bryan bischof, andy jiang, vedika jain, and aman kishore for reading drafts of this</em></p></div></article></div><div id="discussion"><h4>Discussion about this post</h4></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[If You're Remote, Ramble (336 pts)]]></title>
            <link>https://stephango.com/ramblings</link>
            <guid>44775563</guid>
            <pubDate>Sun, 03 Aug 2025 10:32:51 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://stephango.com/ramblings">https://stephango.com/ramblings</a>, See on <a href="https://news.ycombinator.com/item?id=44775563">Hacker News</a></p>
<div id="readability-page-1" class="page"><article> 

        <p>A tip for remote teams of 2-10 people. Create a personal “ramblings” channel for each teammate in your team’s chat app of choice.</p>

<p>Ramblings channels let everyone share what’s on their mind without cluttering group channels. Think of them as personal journals or microblogs inside your team’s chat app, a lightweight way to add ambient social cohesion.</p>

<p>People typically post short updates 1-3 times per week. Common topics include:</p>

<ul>
  <li>ideas related to current projects</li>
  <li>musings about blog posts, articles, user feedback</li>
  <li>“what if” suggestions</li>
  <li>photos from recent trips or hobbies</li>
  <li>rubber ducking a problem</li>
</ul>

<p>Each ramblings channel should be named after the team member, and only that person can post top-level messages. Others can reply in threads, but not start new ones.</p>

<p>All the ramblings channels should be in a <em>Ramblings</em> section at the bottom of the channel list. They should be muted by default, with no expectation that anyone else will read them.</p>

<p>We started experimenting with ramblings at <a href="https://stephango.com/obsidian">Obsidian</a> two years ago, and they’ve been surprisingly sticky. We have no scheduled meetings, so ramblings are our equivalent of water cooler talk. We want as much deep focus time as possible, so ramblings help us stay connected while minimizing interruptions.</p>

<p>Because they are so free and loose, some of our best ideas emerge from ramblings. They’re often the source of feature ideas, small prototypes, and creative solutions to long-standing problems.</p>

<p>About once a year, we do a week-long in-person meetup. Ramblings have been one successful way we keep the human connection going throughout the rest of the year.</p>

      </article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Twenty Eighth International Obfuscated C Code Contest (222 pts)]]></title>
            <link>https://www.ioccc.org/2024/index.html</link>
            <guid>44774104</guid>
            <pubDate>Sun, 03 Aug 2025 04:34:36 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.ioccc.org/2024/index.html">https://www.ioccc.org/2024/index.html</a>, See on <a href="https://news.ycombinator.com/item?id=44774104">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="content">

<!-- END: this line ends content from: inc/before-content.default.html -->
<!-- START: this line starts content for HTML phase 21 by: bin/pandoc-wrapper.sh via bin/md2html.sh -->

<!-- BEFORE: 1st line of markdown file: 2024/README.md -->
<h2 id="twenty-eighth-international-obfuscated-c-code-contest">Twenty Eighth International Obfuscated C Code Contest</h2>
<h2 id="where-to-start">Where to start</h2>
<p>See below for links to the <a href="#inventory">2024 winning IOCCC entries</a>.</p>
<p>Check out the <code>index.html</code> web pages for each winning entry. They have
most of the information you need to compile and run the winning program.
Take a look at the winning source code and try to figure out how it works.
You might also want to check out the author’s remarks for even more details.</p>
<p>You may <a href="https://www.ioccc.org/2024/2024.tar.bz2">download all winning entries</a> in the form
of a compressed tarball, for this year’s contest.</p>

<p>This year marked the <a href="https://www.ioccc.org/faq.html#ioccc_start">40th anniversary of the IOCCC</a>.
The <strong>IOCCC28</strong> opened submissions from <strong>2025-03-05 23:19:17.131107 UTC</strong> to <strong>2025-06-05
04:03:02.010099 UTC</strong> after a 4 year pause.</p>
<p>A significant portion of those 4 years was an effort by a <a href="https://www.ioccc.org/thanks-for-help.html">number of people</a>,
with over 6168+ commits, to rebuild the <a href="https://www.ioccc.org/index.html">Official IOCCC website</a>
in what became known as the <strong>Great Fork Merge</strong>.</p>
<p>Another portion of those 4 years went into retooling the IOCCC, including the creation of
the <a href="https://github.com/ioccc-src/mkiocccentry">mkiocccentry toolkit</a>, a new
<a href="https://www.ioccc.org/next/register.html">IOCCC registration process</a>, and a new
<a href="https://submit.ioccc.org/">IOCCC submit server</a> (which may not be up if the contest is closed).</p>
<p>The <strong>major motivation</strong> for the above was to:</p>
<ul>
<li><p>Present <a href="https://www.ioccc.org/years.html">IOCCC winners</a> that that honors the contributions of all of the <a href="https://www.ioccc.org/authors.html">winning authors</a> from <a href="https://www.ioccc.org/location.html">around the world</a>,</p></li>
<li><p>Reduce the administrative effort required by the <a href="https://www.ioccc.org/judges.html">IOCCC Judges</a> to run the contest, and</p></li>
<li><p>Minimize the time between when the winners of the IOCCC are selected and when the source is made available.</p></li>
</ul>
<p>On the last point, for <strong>IOCCC28</strong> the Judging started on <strong>2025-06-05 04:03:02.010099 UTC</strong> and then the winners were selected by the <a href="https://www.ioccc.org/judges.html">IOCCC Judges</a> on <strong>2025-07-07 20:05:07.000000 UTC</strong>:</p>
<ul>
<li><p>Winners were announced during a live the <a href="https://www.youtube.com/@OurFavoriteUniverse">Our Favorite Universe</a> YouTube Channel that started on <strong>2025-08-02 18:00:00.000000 UTC</strong></p></li>
<li><p>Near the end of the 2 hour XXX-add-show-URL-here-XXX YouTube show, the <strong>source code</strong> of the <a href="#inventory">IOCCC28 winning entries</a> was published on <a href="https://github.com/ioccc-src/winner">IOCCC GitHub winner repo</a> and on the <a href="https://www.ioccc.org/index.html">Official IOCCC website</a>.</p></li>
</ul>
<p><strong>No longer will there be many months</strong> between when the winners are
announced and their source code is made available. For <strong>IOCCC28</strong>
the time was <strong>less than 2 hours</strong>. :-)</p>
<h3 id="increased-submission-quantity-and-quality">Increased submission quantity and quality</h3>
<p>The submissions to <strong>IOCCC28</strong> were remarkable when compared with previous contests, in both the
<strong>increased quantity and quality</strong> of the submissions. As a result the judging process for <strong>IOCCC28</strong>
was more challenging and required greater effort. Thankfully due to the above mentioned improvements,
the judging process <strong>took only 33 days</strong>. Under the previous system it <strong>would have taken much longer</strong>!</p>
<p>One thing that began to emerge after we exits the early rounds of judging and entered the middle rounds
of judging was that we were very likely to have more winning entries than in previous contests.
Even so, the final rounds of judging required more rounds and tool longer than in any previous IOCCC.</p>
<p>At the end of the final round of the final rounds of judging, we had a record <strong>23 winners of IOCCC28</strong>,
eclipsing the record of 15 winners!</p>
<p>While we suspect that the 4 year gap between <strong>IOCCC27</strong> and <strong>IOCCC28</strong> allowed people extra
time to improve their submissions, we also believe that people submitting to the IOCCC have
adept in obfuscation and have become more skilled in the programming in the C language.</p>
<h3 id="code-size-and-rule-2">Code size and Rule 2</h3>
<p>We were pleased to observe that while the <a href="https://www.ioccc.org/faq.html#size_rule_history">IOCCC size limit</a>
increased by about 21%, we received many submissions that were well under the new
<a href="https://www.ioccc.org/next/rules.html#rule2">Rule 2 size limit</a> including a fair number of quality
small and medium sized submissions. Half of the <strong>IOCCC28 winners</strong> were less than
<em>2/3 of the size limit</em>, and 10 of the <strong>IOCCC28 winners</strong> were less than
<em>1/2 of the size limit</em>!</p>
<p><strong>IMPORTANT NOTE</strong>: The <a href="https://www.ioccc.org/faq.html#size_rule_history">IOCCC size limit</a> increase for <strong>IOCCC28</strong>
was the first size increase in over 10 years. Given the quality of submissions we received
that were well under the limit, we do <strong>NOT</strong> expect to change the
<a href="https://www.ioccc.org/next/rules.html#rule2">Rule 2 size limit</a> for <strong>at least another 10 years</strong> (and likely longer).</p>
<p><strong>IMPORTANT HINT</strong>: Only 3 of 23 the <strong>IOCCC28 winners</strong> came close to the <a href="https://www.ioccc.org/next/rules.html#rule2">Rule 2 size limit</a>.
Large code size isn’t everything. :-) Those submitting to future contents should
<strong>take a careful note</strong> of that fact.</p>
<h3 id="rules-and-guidelines-for-this-contest">Rules and Guidelines for this contest</h3>
<p>Here are the final versions of the IOCCC rules and guidelines that were in effect for this contest:</p>
<ul>
<li><a href="https://www.ioccc.org/2024/rules.html">2024 rules</a> version <strong>28.30 2025-03-03</strong></li>
<li><a href="https://www.ioccc.org/2024/guidelines.html">2024 guidelines</a> version <strong>28.47 2025-03-15</strong></li>
</ul>
<h3 id="looking-forward-to-the-next-contest">Looking forward to the next contest</h3>
<p>The <a href="https://www.ioccc.org/judges.html">IOCCC Judges</a> realize that the <a href="https://www.ioccc.org/next/rules.html">IOCCC rules</a> and
the <a href="https://www.ioccc.org/next/guidelines.html">IOCCC guidelines</a> need to be <strong>greatly improved and streamlined</strong>.
We decided to not overhaul them for <strong>IOCCC28</strong> so as to <strong>NOT</strong> delay the opening of the contest.</p>
<p>After the <a href="https://www.ioccc.org/judges.html">IOCCC Judges</a> take a short <em>IOCCC Vacation</em> (we have been working solid
on <strong>IOCCC28</strong> since <strong>2020 Dec 30</strong>), we plan to update, improve and streamline the
<a href="https://www.ioccc.org/next/rules.html">IOCCC rules</a> and the <a href="https://www.ioccc.org/next/guidelines.html">IOCCC guidelines</a> for <strong>IOCCC29</strong>.
Then we will address whatever bug fixes in the form of GitHub Pull Requests to both the
<a href="https://github.com/ioccc-src/winner">IOCCC GitHub winner repo</a> and the
<a href="https://github.com/ioccc-src/mkiocccentry">mkiocccentry toolkit repo</a> that may be pending.</p>
<p>Once we are all caught up, we will set the opening date for <strong>IOCCC29</strong>. It is our plan
to open <strong>IOCCC29</strong> sometime in <strong>2025 December</strong>.</p>

<p>There were a number of remarkable winning entries for <strong>IOCCC28</strong>. We wish to call
attention to a few of them:</p>
<ul>
<li><a href="https://www.ioccc.org/2024/kurdyukov1/index.html">2024/kurdyukov1</a></li>
</ul>
<p>This entry is a fun revisit of the theme of <a href="https://www.ioccc.org/2000/natori/index.html">2000/natori</a>,
only better and without using any floating point arithmetic. You are invited
to try and understand why the constant <code>2551443</code> is common to both entries.</p>
<ul>
<li><a href="https://www.ioccc.org/2024/cable1/index.html">2024/cable1</a></li>
</ul>
<p>This entry claims to be “<em>the world’s smallest LLM (large language
model) inference engine</em>”: running open-source model based on Meta’s
LLaMA 2 with 7 billion parameters. After downloading the model via the
<a href="https://github.com/ioccc-src/winner/blob/master/2024/cable1/get_model.sh">2024/cable1/get_model.sh</a> script,
we invite you explore this <strong>ChatIOCCC</strong> tool. While not super fast,
the tool’s output can be amusing.</p>
<p>You might wish to ask <strong>ChatIOCCC</strong>: “<em>Why did this entry win IOCCC28?</em>”?</p>
<p>We encourage you to read and understand the
<a href="https://github.com/ioccc-src/winner/blob/master/2024/cable1/prog.c">2024/cable1/prog.c source code</a>. Then see
if the <strong>ChatIOCCC</strong> tool can offer any new, or even correct insights
into the code’s obfuscation. :-)</p>
<ul>
<li><a href="https://www.ioccc.org/2024/howe/index.html">2024/howe</a></li>
</ul>
<p>“<em>Eh</em>”? The source code, with its UTF-8 conforming accent gives you a
polite editor that is more functional an <code>ed(1)</code>.</p>
<p>We invite you to use the program to view the UTF-8 fun in the
<a href="https://github.com/ioccc-src/winner/blob/master/2024/howe/prog.c">2024/howe/prog.c source code</a>.</p>
<p><strong>HINT to emacs users</strong>: type <strong>Q</strong> or press <strong>CTRL+C</strong> to exit. 🤓</p>
<ul>
<li><a href="https://www.ioccc.org/2024/stedolan/index.html">2024/stedolan</a></li>
</ul>
<p>When you view the <a href="https://github.com/ioccc-src/winner/blob/master/2024/stedolan/prog.c">prog.c source code</a>,
notice magic numbers. This <strong>one liner</strong> packs a significant amount
of obfuscation in only 135 bytes!</p>
<p>Try giving 10 hex digits to standard input of the program. Then try a
different hex digits value. Then feed the MD5 hash of the
<a href="https://github.com/ioccc-src/winner/blob/master/2024/stedolan/prog.c">2024/stedolan/prog.c source code</a> and
“<em>C</em>” what happens. :-)</p>
<ul>
<li><a href="https://www.ioccc.org/2024/endoh1/index.html">2024/endoh1</a></li>
</ul>
<p>The <a href="https://github.com/ioccc-src/winner/blob/master/2024/endoh1/try.sh">2024/endoh1/try.sh</a> will carry you on
a journey that tests your patience as well as the patience of the C-preprocessor.</p>
<p>Building a 8x8 image requires order of 20 seconds of C-preprocessor
time, when running 6 jobs on 4 CPU cores in parallel. Unfortunately,
with such a tiny image, it is hard to appreciate the image being rendered.</p>
<p>So you might try building a 32x32 form of the image, invoking the
C-preprocessor 1024 times. When using 6 jobs on 4 CPU cores in parallel,
this might take about 5.5 minutes to complete. However, the small image
only gives you a hint of the image being rendered.</p>
<p>If you have the time you might try building a 128x128 image, invoking
the C-preprocessor 16384 times. With 6 jobs on 4 CPU cores in parallel,
this will take about 1.5 hours. At this scale you might consider it
worthwhile to render a more detailed image.</p>
<p>Using 6 jobs on 4 CPU cores in parallel, to build a 512x512 image invoking
the C-preprocessor 262144 times over a period of about 23.5 hours, you
will be given an image with a reasonable amount of detail to appreciate
what is being rendered.</p>
<p>One fun aspect of the <a href="https://github.com/ioccc-src/winner/blob/master/2024/endoh1/prog.c">2024/endoh1/prog.c source code</a>
is that when it is compiled an executed, the program outputs C code.
That C code, call it <code>rt.c</code> will fail to compile! But that is OK because the C-preprocessor
has enough work to do without bothering the C compiler. :-)</p>
<p>We plan to recognise those who report that they <a href="https://www.ioccc.org/2024/endoh1/index.html#larger">rendered a new record sized image</a>.</p>
<ul>
<li><a href="https://www.ioccc.org/2024/weaver/index.html">2024/weaver</a></li>
</ul>
<p>As the expression goes, “Seeing (and hearing) is believing”: or at least can lead to better understanding of the code. :o)</p>
<p>After studying the <a href="https://github.com/ioccc-src/winner/blob/master/2024/weaver/prog.c">2024/weaver/prog.c source code</a>,
we suggest you compile and run the program for more information.</p>
<ul>
<li><a href="https://www.ioccc.org/2024/mills/index.html">2024/mills</a></li>
</ul>
<p>This won the “<strong>Prize in ℤ₃</strong>”, but what does that mean? Well,
the “<em>double-struck Z</em>” is <strong>NOT</strong> a reference to a set of integers,
nor is the “<em>double-struck Z subscript 3</em>” a reference to a ring of
3-adic integers. Instead, it is a reference to the 3rd version of a
certain <a href="https://en.wikipedia.org/wiki/Z-machine">virtual machine</a>.</p>
<p>If you use the <a href="https://github.com/ioccc-src/winner/blob/master/2024/mills/try.sh">2024/mills/try.sh</a> script, you will first be given
an opportunity to view the <a href="https://github.com/ioccc-src/winner/blob/master/2024/mills/prog.c">2024/mills/prog.c source code</a>.
Then you will be given 10 choices to play.</p>
<p><strong>HINT</strong>: If you select <strong>0</strong>: Try to not be eaten by a Grue when
reading/navigating the twisty mazes of this source code! :-)</p>
<ul>
<li><a href="https://www.ioccc.org/2024/codemeow/index.html">2024/codemeow</a></li>
</ul>
<p>As you study the <a href="https://github.com/ioccc-src/winner/blob/master/2024/codemeow/prog.c">2024/codemeow/prog.c source code</a>,
try not to lose track of the forest of obfuscation while tracking down the execution path trees.</p>
<p>We think this code will “grow on you” as the C expression goes just a compiled C code “<em>grows on parse trees</em>”. :-)</p>
<ul>
<li><a href="https://www.ioccc.org/2024/kurdyukov2/index.html">2024/kurdyukov2</a></li>
</ul>
<p>If seeing is believing, and you believe you understand the
<a href="https://github.com/ioccc-src/winner/blob/master/2024/kurdyukov2/prog.c">2024/kurdyukov2/prog.c source code</a>,
then you might be able to see how the code words.</p>
<p><strong>QUESTION</strong>: How many of the “<em>magic numbers</em>” in the program do you understand?</p>
<ul>
<li><a href="https://www.ioccc.org/2024/kurdyukov3/index.html">2024/kurdyukov3</a></li>
</ul>
<p>The prize, “<strong>virtual quietus</strong>”, is a virtual reference to the doom that awaits you when
you run this code: let alone when your try to figure out how the author was able to
encode the virtual machine in under <strong>2.5K bytes</strong> of <a href="https://github.com/ioccc-src/winner/blob/master/2024/kurdyukov3/prog.c">prog.c source code</a>!</p>
<p><strong>NOTE</strong>: The implementation runs without sound or mouse
input. You will need reply on your keyboard skills. The
<a href="https://github.com/ioccc-src/winner/blob/master/2024/kurdyukov3/try.sh">2024/kurdyukov3/try.sh</a>
demonstration script will give you some useful keyboard hints.</p>
<ul>
<li><a href="https://www.ioccc.org/2024/cable2/index.html">2024/cable2</a></li>
</ul>
<p>To those who follow the concept of “<em>seeing is believing</em>” will very
likely be “<em>deceived</em>” when they try to see the source in terminal window:</p>
<pre><code>    cat 2024/cable2/prog.c</code></pre>
<p>Viewing the <a href="https://github.com/ioccc-src/winner/blob/master/2024/cable2/prog.c">prog.c source code</a> in most editors
might suggest that the is something fishy 🐠 with the code.</p>
<ul>
<li><a href="https://www.ioccc.org/2024/straadt/index.html">2024/straadt</a></li>
</ul>
<p>Have you ever wondered would what would happen if you recursively removed
files starting from the root directory? Are you curious what happens when
you launch a <a href="https://en.wikipedia.org/wiki/Fork_bomb">fork bomb</a>?
You can safely give those and other scenarios under this virtual machine.</p>
<p>We invite you to explore the <a href="https://github.com/ioccc-src/winner/blob/master/2024/straadt/prog.c">prog.c source code</a>
to explore this interesting virtual machine implementation. And as a bonus, a
full-fledged C64 emulator is included!</p>
<ul>
<li><a href="https://www.ioccc.org/2024/ferguson2/index.html">2024/ferguson2</a></li>
</ul>
<p>The “<strong>Prize in yil-tas</strong>” provides a hint as to what this marvelous
winning entry is going, provided that you are someone familiar in at
least 3 languages (<strong>hint</strong>: One of them is C, another id English).</p>
<p>The IOCCC is honored to present this entry for your enjoyment and you ponder
the <a href="https://github.com/ioccc-src/winner/blob/master/2024/ferguson2/prog.c">prog.c source code</a>.</p>
<ul>
<li><a href="https://www.ioccc.org/2024/carlini/index.html">2024/carlini</a></li>
</ul>
<p>The IOCCC has seen a number of winning entries that allow code designed to
run on early Intel processors, however none of them emulate a processor as
early as the <a href="https://en.wikipedia.org/wiki/Intel_4004">Intel 4004</a>.
Launched in 1971, the 4004 was the core of the first commercially marketed
microprocessor chipset.</p>
<p>The <a href="https://github.com/ioccc-src/winner/blob/master/2024/carlini/prog.c">2024/carlini/prog.c</a>
code runs rather slow, so patience is required to watch
<a href="https://en.wikipedia.org/wiki/Intel_4004#/media/File:Intel_C4004.jpg">4004</a>
code run. This is very understandable when you realize that the
<a href="https://github.com/ioccc-src/winner/blob/master/2024/carlini/prog.c">2024/carlini/prog.c</a> is actually
emulating the entire
<a href="https://en.wikipedia.org/wiki/Intel_4004#/media/File:Chip_layout_from_the_development_phase_of_the_Intel_4004_from_1971,_the_first_microprocessor_of_the_world_(cropped_and_edited_image).jpg">4004 chip circuit</a>
by computing the output of each gate in a loop, effectively running the CPU and associated RAM/ROM chips!</p>

<ul>
<li><p>Depending on the address of a variable, as not all platforms support
<a href="https://en.wikipedia.org/wiki/Address_space_layout_randomization">ASLR</a>,
as the single source of pseudo-randomness isn’t a good idea. It’s
much better to mix in other sources of variability (e.g.&nbsp;time, process
ID, etc.) so that subsequent runs will behave differently.</p></li>
<li><p>If a submission’s obfuscation strongly resembles a past IOCCC winning
entry, and the submission’s obfuscation is mainly in the C code source,
then the submission likely to not make it into the final rounds of judging.</p></li>
<li><p>When working with modern C compilers, it’s crucial to explicitly
declare variable and function types. Don’t assume they’ll
automatically default to an <code>int</code>.</p></li>
<li><p>Sadly, K&amp;C-style C code does not compile well under modern C compilers.</p></li>
<li><p>A submission that only works under gcc or clang does not work as well
as submissions that can work under either compiler.</p></li>
<li><p>Shrinking an existing open-source program to meet rule 2 size might
be an impressive feat of code shrinking, but it might not be enough to
make it into the final rounds of judging. Originality is key, and this
might not be the most innovative approach.</p></li>
<li><p>If your submission relies on mathematical algorithms, be sure that the
C code that implements the mathematics is well obfuscated.</p></li>
<li><p>Just a friendly reminder that before you upload your submissions,
uncompress the compressed tarball into a different directory. This way,
you can be sure you didn’t miss uploading any important files!</p></li>
<li><p>Using a lot of goto statements to make your code harder to understand
might not help it pass the final rounds of judging.</p></li>
</ul>
<h3 id="encouragement-for-those-who-did-not-win-this-year">Encouragement for those who did not win this year</h3>
<p>We know many of you that submitted to the IOCCC put in a ton of effort
into your submissions for this year’s IOCCC. We can’t just give out
awards to everyone. That would mean taking away from the submissions
that we think are the best and deserve to win.</p>
<p>Sometimes, a final round submission might be good enough to be a
winning IOCCC entry, only to be beaten by a similar, but slightly better
submission. If you think this happened with your submission, consider
submitting an enhanced version to the next IOCCC.</p>
<p><strong>PLEASE DO NOT</strong> give up hope! There are some submissions that have been
submitted with revisions, multiple times before rising to the level of
a winning IOCCC entry. You might also want to try with a different type
of submission altogether for the next IOCCC.</p>
<p>If you’re not planning to improve and resubmit your non-winning entry
for the next IOCCC, you’re welcome to publish it.</p>
<h2 id="on-compiling-and-running-winning-entries">On Compiling and running winning entries</h2>
<p>Some C compilers aren’t as great as they could be. If yours isn’t
working well, you might want to try compiling with an updated version
of clang and/or gcc instead.</p>
<p>If you encounter problems in compiling and/or running the winning entries, see the FAQ on:</p>
<ul>
<li><a href="https://www.ioccc.org/faq.html#compiling">Compiling IOCCC entries</a></li>
<li><a href="https://www.ioccc.org/faq.html#dependencies">IOCCC entry dependencies</a></li>
<li><a href="https://www.ioccc.org/faq.html#compile_problems">Problems compiling entries</a></li>
<li><a href="https://www.ioccc.org/faq.html#running_entries">Running IOCCC entries</a></li>
</ul>
<p>For additional information on how to submit fixes, see the FAQ on:</p>
<ul>
<li><a href="https://www.ioccc.org/faq.html#fix_an_entry">How to submit a fix</a> - how to submit a fix to an entry</li>
<li><a href="https://www.ioccc.org/faq.html#fix_author">Update author information</a> - how to correct or update an IOCCC author’s information</li>
</ul>
<h3 id="for-even-more-information">For even more information</h3>
<ul>
<li><a href="https://www.ioccc.org/faq.html#report_website_problem">Reporting an IOCCC website problem</a></li>
<li><a href="https://www.ioccc.org/faq.html#fix_website">Submitting a fix to the IOCCC website</a></li>
<li><a href="https://www.ioccc.org/contact.html">How to contact the IOCCC</a> - up to date contact details</li>
<li><a href="https://www.ioccc.org/faq.html">IOCCC FAQ</a> - additional information on the IOCCC</li>
<li><a href="https://www.ioccc.org/">www.ioccc.org</a> - the primary IOCCC website</li>
</ul>
<!--

    Copyright © 1984-2025 by Landon Curt Noll and Leonid A. Broukhis.  All Rights Reserved.

    You are free to share and adapt this file under the terms of this license:

        Creative Commons Attribution-ShareAlike 4.0 International (CC BY-SA 4.0)

    For more information, see:

        https://creativecommons.org/licenses/by-sa/4.0/

-->
<!-- AFTER: last line of markdown file: 2024/README.md -->

<!-- END: this line ends content for HTML phase 21 by: bin/pandoc-wrapper.sh via bin/md2html.sh -->
<!-- START: this line starts content for HTML phase 22 by: bin/output-year-index.sh via bin/md2html.sh -->

<!-- START: this line starts content generated by: bin/output-year-index.sh -->

<div id="inventory">
<p id="2024">
<h2 id="winning-entries-of-2024---the-28th-ioccc">Winning Entries of 2024 - The 28th IOCCC</h2>
</p>
</div>
<p><a href="https://www.ioccc.org/2024/2024.tar.bz2"><strong>Download all winning entries from 2024</strong></a></p>
<ul>
<li><a href="https://www.ioccc.org/2024/anders/index.html"><strong>2024/anders</strong></a> - <strong>Prize in imitative rebooting</strong></li>
<li><a href="https://www.ioccc.org/2024/burton/index.html"><strong>2024/burton</strong></a> - <strong>Prize in pentagrammatology</strong></li>
<li><a href="https://www.ioccc.org/2024/cable1/index.html"><strong>2024/cable1</strong></a> - <strong>Prize in bot talk</strong></li>
<li><a href="https://www.ioccc.org/2024/cable2/index.html"><strong>2024/cable2</strong></a> - <strong>Prize in murky waters</strong></li>
<li><a href="https://www.ioccc.org/2024/carlini/index.html"><strong>2024/carlini</strong></a> - <strong>Prize in perfect timing</strong></li>
<li><a href="https://www.ioccc.org/2024/codemeow/index.html"><strong>2024/codemeow</strong></a> - <strong>Prize in tray planting</strong></li>
<li><a href="https://www.ioccc.org/2024/endoh1/index.html"><strong>2024/endoh1</strong></a> - <strong>Prize in patient pointillism</strong></li>
<li><a href="https://www.ioccc.org/2024/endoh2/index.html"><strong>2024/endoh2</strong></a> - <strong>Prize in solid body physics</strong></li>
<li><a href="https://www.ioccc.org/2024/ferguson1/index.html"><strong>2024/ferguson1</strong></a> - <strong>Prize in diabolical logistics</strong></li>
<li><a href="https://www.ioccc.org/2024/ferguson2/index.html"><strong>2024/ferguson2</strong></a> - <strong>Prize in yil-tas</strong></li>
<li><a href="https://www.ioccc.org/2024/howe/index.html"><strong>2024/howe</strong></a> - <strong>Nice accent eh-ward</strong></li>
<li><a href="https://www.ioccc.org/2024/kramer/index.html"><strong>2024/kramer</strong></a> - <strong>Prize in linguistic arithmetic</strong></li>
<li><a href="https://www.ioccc.org/2024/kurdyukov1/index.html"><strong>2024/kurdyukov1</strong></a> - <strong>Prize in phased periodicity</strong></li>
<li><a href="https://www.ioccc.org/2024/kurdyukov2/index.html"><strong>2024/kurdyukov2</strong></a> - <strong>Prize in art restoration</strong></li>
<li><a href="https://www.ioccc.org/2024/kurdyukov3/index.html"><strong>2024/kurdyukov3</strong></a> - <strong>Prize in virtual quietus</strong></li>
<li><a href="https://www.ioccc.org/2024/kurdyukov4/index.html"><strong>2024/kurdyukov4</strong></a> - <strong>Prize in embiggening</strong></li>
<li><a href="https://www.ioccc.org/2024/maffiodo/index.html"><strong>2024/maffiodo</strong></a> - <strong>Prize in creative interpretation</strong></li>
<li><a href="https://www.ioccc.org/2024/mills/index.html"><strong>2024/mills</strong></a> - <strong>Prize in ℤ₃</strong></li>
<li><a href="https://www.ioccc.org/2024/stedolan/index.html"><strong>2024/stedolan</strong></a> - <strong>Best one liner</strong></li>
<li><a href="https://www.ioccc.org/2024/straadt/index.html"><strong>2024/straadt</strong></a> - <strong>Prize in sound coding</strong></li>
<li><a href="https://www.ioccc.org/2024/tmarrec/index.html"><strong>2024/tmarrec</strong></a> - <strong>Prize in cyclonic coding</strong></li>
<li><a href="https://www.ioccc.org/2024/tompng/index.html"><strong>2024/tompng</strong></a> - <strong>Prize in quasi bijection</strong></li>
<li><a href="https://www.ioccc.org/2024/weaver/index.html"><strong>2024/weaver</strong></a> - <strong>Sur prize</strong></li>
</ul>
<hr>
<h4>
Jump to: <a href="#">top</a>
</h4>

<!-- END: next line ends content generated by: bin/output-year-index.sh -->

<!-- END: this line ends content for HTML phase 22 by: bin/output-year-index.sh via bin/md2html.sh -->
<!-- START: this line starts content from: inc/after-content.default.html -->

    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[C++26 Reflections adventures and compile-time UML (106 pts)]]></title>
            <link>https://www.reachablecode.com/2025/07/31/c26-reflections-adventures-compile-time-uml/</link>
            <guid>44772917</guid>
            <pubDate>Sun, 03 Aug 2025 00:12:15 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.reachablecode.com/2025/07/31/c26-reflections-adventures-compile-time-uml/">https://www.reachablecode.com/2025/07/31/c26-reflections-adventures-compile-time-uml/</a>, See on <a href="https://news.ycombinator.com/item?id=44772917">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
		<div>
<figure><img fetchpriority="high" decoding="async" width="502" height="385" src="https://www.reachablecode.com/wp-content/uploads/2025/07/uml_reflections_c26.png" alt="" srcset="https://www.reachablecode.com/wp-content/uploads/2025/07/uml_reflections_c26.png 502w, https://www.reachablecode.com/wp-content/uploads/2025/07/uml_reflections_c26-300x230.png 300w" sizes="(max-width: 502px) 100vw, 502px"></figure></div>






<p>The first thing I do every time I need to learn a new codebase is to start drawing the UML diagram of its classes, and usually give up soon after I started. The process of doing it manually is certainly useful, but now with reflections I figure it would be fun to try to generate it instead.</p>



<p>With C++26 reflections[1] the general consensus is that the magnitude of language change is comparable to what happened with C++11. After my little experiment with it, I would cautiously agree. So how does one go about creating a (Plant)UML diagram at compile time? With something like <a href="https://godbolt.org/z/P39K4vYEz">this</a>.</p>



<p>With the spoilers out of the way let’s dig in the details.</p>



<p>P2996[1] introduces a couple of operators, namely lift <code>^^</code> and splice<code> [: :]</code>. The first one “lifts” a type or variable into a “meta” space, and the “splice” one (which imho should have been called “grounding operator”) does the opposite.</p>



<p>The first thing to understand is that regardless of what we apply the lift operator (^^) to, it creates a <code>std::meta::info</code> type. This means that some <code>info</code> objects will be <em>reflections of types</em>, and some will be <em>reflections of values</em>. This creates confusion in my head, as at times one needs to check which <em>kind</em> of <code>info </code>something is, but there are good reasons for this and are well explained in section 2.2 of the paper. With that in mind let’s start coding.</p>



<div><pre data-lang="C++"><code>int main() {
  MyClass s;
  std::string_view dot_graph_uml = make_class_graph&lt;MyClass&gt;();

  std::cout &lt;&lt; dot_graph_uml &lt;&lt; std::endl;
}</code></pre></div>



<p><em>(edit: thanks <a href="https://old.reddit.com/r/cpp/comments/1mevtwm/c26_reflections_adventures_compile_time_uml/n6cp0w9/" data-type="link" data-id="https://old.reddit.com/r/cpp/comments/1mevtwm/c26_reflections_adventures_compile_time_uml/n6cp0w9/"> u/katzdm-cpp</a> for the suggestion to use <code>std::string_view</code>)</em> <s>You’ll notice right away there is an ugly, and seemingly avoidable char pointer. Let’s get back to that later.</s> Not much happens in <code>main()</code> besides us calling function template that is just a wrapper for what we actually care about:</p>



<div><pre data-lang="C++"><code>template&lt;typename U&gt;
consteval std::string_view make_class_graph() { 
  std::string graph = "@startuml \nskinparam linetype ortho \n";

  std::vector&lt;std::meta::info&gt; already_drawn;
  graph += make_class_graph_impl(^^U, already_drawn);
  graph += "@enduml";

  return std::define_static_string(graph);
}</code></pre></div>



<p>Here we see the first interesting thing, something called <code>std::define_static_string</code>[2]. What this does is take what is a compile time <code>std::string</code> and create a string literal, which we can return from a consteval function. If we were to try to return the<code> std::string</code> we would be greeted with the compiler error</p>



<pre><code>&lt;source&gt;:116:24: note: pointer to subobject of heap-allocated object is not a constant expression
/opt/compiler-explorer/clang-bb-p2996-trunk-20250703/bin/../include/c++/v1/__memory/allocator.h:117:13: note: heap allocation performed here
  117 |     return {allocate(__n), __n};</code></pre>



<p>Which makes sense as we cannot expect to create an object we allocate on the heap at compile time, then have this object <em>exist</em> in the heap also at runtime. This is why we end up with a big fat literal string that holds our final UML diagram, which we can do whatever we want with in <code>main()</code>. Looking at the assembly you’ll see the end result:</p>



<div><pre data-lang="C"><code> .asciz  "@startuml \nskinparam linetype ortho \ntogether {\n class \"MyClass\"\n  \"MyClass\"*--\"unsigned int\"\n  \"MyClass\"*--\"unsigned int\"\n  \"MyClass\"*--\"unsigned int\"\n  \"MyClass\"*--\"Nested\"\n  \"MyClass\"-up-|&gt;\"MyBase\"\n}\ntogether {\n class \"Nested\"\n  \"Nested\"*--\"int\"\n  \"Nested\"*--\"int\"\n  \"Nested\"*--\"MyClass\"\n}\ntogether {\n class \"MyBase\"\n  \"MyBase\"*--\"vector&lt;int, allocator&lt;int&gt;&gt;\"\n}\ntogether {\n class \"vector&lt;int, allocator&lt;int&gt;&gt;\"\n}\n@enduml"
</code></pre></div>



<p>Now that we have a magic function that “defines a static string” for us, let’s dig into the actual reflection bits. The first thing to note is what we pass to <code>make_class_graph_impl</code>, which shows the use of the <em>lift</em> operator, <code>^^U</code>. This creates a std::meta::info object that in this case is a <em>reflection of a type</em>. The Impl function itself, as you may have guessed, is meant to be recursive and takes a second argument that we’ll explain later:</p>



<div><pre data-lang="C++"><code>consteval std::string make_class_graph_impl(std::meta::info head, std::vector&lt;std::meta::info&gt;&amp; already_drawn)
{
  [...]

  constexpr auto ctx = std::meta::access_context::current();
  std::string uml_diagram;
    
  uml_diagram += "together {\n class " + add_quotes(display_string_of(head)) + "\n";
  
  const std::string indent = "  ";

  // members
  for (std::meta::info field_info : std::meta::nonstatic_data_members_of(head, ctx))
  {
    uml_diagram += indent +  add_quotes(display_string_of(head)) + composition_arrow + add_quotes(display_string_of(remove_ptr_cv_type_of(field_info))) + "\n";
  }
[...]</code></pre></div>



<p>First let’s talk about the new “context” object: the main paper[1] describes it as “<em>a class that represents a namespace, class, or function from which queries pertaining to access rules may be performed[…]</em>“, and it was introduced in [3] as a mean to resolve the “encapsulation” issue. This context comes in 3 flavours:</p>



<ul>
<li><code>std::meta::access_context::current()</code>: for accessing the public stuff in the current scope</li>



<li>s<code>td::meta::access_context::unprivileged()</code>: for accessing public stuff in the global scope</li>



<li><code>std::meta::access_context::unchecked()</code>: for accessing anything in the global scope</li>
</ul>







<p>After that the interesting things are <code>std::meta::nonstatic_data_members_of(head, ctx)</code>, and <code>std::meta::info::display_string_of(head)</code>. Those are pretty self explanatory, and they do make “metaprogramming” as easy as “programming”!</p>



<p>It gets a bit trickier later, when we have to recurse</p>



<div><pre data-lang="C++"><code>uml_diagram += make_class_graph_impl(remove_ptr_cv_type_of(field_info), already_drawn);</code></pre></div>



<p>Besides the very convenient fact that we can keep track of what we iterated over already simply using a <code>std::vector&lt;std::meta::info&gt;</code>, we do define a custom function with the terrible name of <code>remove_ptr_cv_type_of</code>.</p>



<div><pre data-lang="C++"><code>consteval auto remove_ptr_cv_type_of(std::meta::info r) -&gt; std::meta::info {
  return decay(remove_pointer(is_type(r) ? r : type_of(r)));
}</code></pre></div>



<p>This does what it says on the box, as in our UML we don’t want to distinguish between const/volatile/pointers whathaveyou. The important and interesting bit however is <code>std::meta::is_type(..)</code>. This function tells us what kind of <em>info</em> object we have, as they can be reflections of anything, and therefore we need to test whether or not we have a reflection of a type or a value and apply std::meta::type_of only when necessary. This is a bit of a pain, but imho it’s a small price to pay for Reflections.</p>



<p>That is basically it, we just need to try it out<a href="https://www.plantuml.com/plantuml/png/hOx12i8m44Jl-OebHp5_8ANeMVzXQPSsc6vATYCKzTyrO2r2eHTlRsQPcQr5CMaU0XYGg-SH4muGF9DE8q5CsaSm6ZlIdXBS3JG1HS2UfrC1Qs3XdNCsi_YEgGNF-iVZXKIf_R5T7jsZUiiZ2bdpNFjoYSrOoih0jZNlh3VFi_35ahbHep4TFEiUC8JOu4TMrNhFLg8cRlCGNW00" data-type="link" data-id="//www.plantuml.com/plantuml/png/hOx12i8m44Jl-OebHp5_8ANeMVzXQPSsc6vATYCKzTyrO2r2eHTlRsQPcQr5CMaU0XYGg-SH4muGF9DE8q5CsaSm6ZlIdXBS3JG1HS2UfrC1Qs3XdNCsi_YEgGNF-iVZXKIf_R5T7jsZUiiZ2bdpNFjoYSrOoih0jZNlh3VFi_35ahbHep4TFEiUC8JOu4TMrNhFLg8cRlCGNW00"> plotting our PlantUML result</a>, which gets printed out at runtime in this case. </p>



<h2>References</h2>



<p>[1] <a href="https://www.open-std.org/jtc1/sc22/wg21/docs/papers/2025/p2996r12.html">https://www.open-std.org/jtc1/sc22/wg21/docs/papers/2025/p2996r12.html</a></p>



<p>[2] <a href="https://www.open-std.org/jtc1/sc22/wg21/docs/papers/2025/p3491r3.html">https://www.open-std.org/jtc1/sc22/wg21/docs/papers/2025/p3491r3.html</a></p>



<p>[3] <a href="https://www.open-std.org/jtc1/sc22/wg21/docs/papers/2025/p3547r0.html">https://www.open-std.org/jtc1/sc22/wg21/docs/papers/2025/p3547r0.html</a></p>
	</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[HTML-in-Canvas (174 pts)]]></title>
            <link>https://github.com/WICG/html-in-canvas</link>
            <guid>44772177</guid>
            <pubDate>Sat, 02 Aug 2025 22:26:47 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/WICG/html-in-canvas">https://github.com/WICG/html-in-canvas</a>, See on <a href="https://news.ycombinator.com/item?id=44772177">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><p dir="auto"><h2 tabindex="-1" dir="auto">HTML-in-Canvas</h2><a id="user-content-html-in-canvas" aria-label="Permalink: HTML-in-Canvas" href="#html-in-canvas"></a></p>
<p dir="auto">We propose new HTML Canvas APIs for rendering HTML content into the canvas for Canvas 2D and WebGL.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Status</h2><a id="user-content-status" aria-label="Permalink: Status" href="#status"></a></p>
<p dir="auto"><strong>Authors:</strong> <a href="mailto:schenney@igalia.com">Stephen Chenney</a>, <a href="mailto:chrishtr@google.com">Chris Harrelson</a>, <a href="mailto:khushalsagar@google.com">Khushal Sagar</a>, <a href="mailto:vmpstr@chromium.org">Vladimir Levin</a>, <a href="mailto:fserb@chromium.org">Fernando Serboncini</a></p>
<p dir="auto"><strong>Champions:</strong> <a href="mailto:schenney@igalia.com">Stephen Chenney</a>, <a href="mailto:chrishtr@google.com">Chris Harrelson</a></p>
<p dir="auto">This proposal is a subset of a <a href="https://github.com/WICG/html-in-canvas/blob/main/placeElement">previous proposal</a> covering APIs to allow live HTML elements.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Motivation</h2><a id="user-content-motivation" aria-label="Permalink: Motivation" href="#motivation"></a></p>
<p dir="auto">There is no web API to easily render complex layouts of text and other content into a <code>&lt;canvas&gt;</code>. As a result, <code>&lt;canvas&gt;</code>-based content suffers in accessibilty, internationalization, performance and quality.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Use cases</h3><a id="user-content-use-cases" aria-label="Permalink: Use cases" href="#use-cases"></a></p>
<ul dir="auto">
<li><strong>Styled, Laid Out Content in Canvas.</strong> There’s a strong need for better styled text support in Canvas. Examples include chart components (legend, axes, etc.), rich content boxes in creative tools, and in-game menus.</li>
<li><strong>Accessibility Improvements.</strong> There is currently no guarantee that the canvas fallback content currently used for <code>&lt;canvas&gt;</code> accessibility always matches the rendered content, and such fallback content can be hard to generate. With this API, elements drawn into the canvas bitmap will match their corresponding canvas fallback.</li>
<li><strong>Composing HTML Elements with Shaders.</strong> A limited set of CSS shaders, such as filter effects, are already available, but there is a desire to use general WebGL shaders with HTML.</li>
<li><strong>HTML Rendering in a 3D Context.</strong> 3D aspects of sites and games need to render rich 2D content into surfaces within a 3D scene.</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Proposed solution: <code>layoutsubtree</code>, <code>drawElement</code>, <code>texElement2D</code> and <code>setHitTestRegions</code></h2><a id="user-content-proposed-solution-layoutsubtree-drawelement-texelement2d-and-sethittestregions" aria-label="Permalink: Proposed solution: layoutsubtree, drawElement, texElement2D and setHitTestRegions" href="#proposed-solution-layoutsubtree-drawelement-texelement2d-and-sethittestregions"></a></p>
<ul dir="auto">
<li>the <code>layoutsubtree</code> attribute on a <code>&lt;canvas&gt;</code> element allows its descendant elements to have layout (*), and causes the direct children of the <code>&lt;canvas&gt;</code> to have a stacking context and become a containing block for all descendants. Descendant elements of the <code>&lt;canvas&gt;</code> still do not paint or hit-test, and are not discovered by UA algorithms like find-in-page.</li>
<li>The <code>CanvasRenderingContext2D.drawElement(element, x, y, options)</code> method renders <code>element</code> and its subtree into a 2D canvas at offset x and y, so long as <code>element</code> is a direct child of the <code>&lt;canvas&gt;</code>. It has no effect if <code>layoutsubtree</code> is not specified on the <code>&lt;canvas&gt;</code>. The <code>options</code> dictionary, if given, has a single option that preserves user privacy in the drawn content, allowing readback or use in WebGL.</li>
<li>The <code>WebGLRenderingContext.texElement2D(..., element)</code> method renders <code>element</code> into a WebGL texture. It has no effect if <code>layoutsubtree</code> is not specified on the <code>&lt;canvas&gt;</code>.</li>
<li>The <code>CanvasRenderingContext2D.setHitTestRegions([{element: ., rect: {x: x, y: y, width: ..., height: ...}, ...])</code> (and <code>WebGLRenderingContext.setHitTestRegions(...)</code>) API takes a list of elements and <code>&lt;canvas&gt;</code>-relative rects indicating where the
element paints relative to the backing buffer of the canvas. These rects are then used to redirect hit tests for mouse and touch events automatically from the <code>&lt;canvas&gt;</code> element to the drawn element.</li>
</ul>
<p dir="auto">(*) Without <code>layoutsubtree</code>, geometry APIs such as <code>getBoundingClientRect()</code> on these elements return an empty rect. They do have computed styles, however, and are keyboard-focusable.</p>
<p dir="auto"><code>drawElement(element ...)</code> takes the CTM (current transform matrix) of the canvas into consideration. The image drawn into the canvas is sized to <code>element</code>'s border box size; element outsize that bounds (including ink and layout overflow) are clipped. The <code>drawElement(element, x, y, dwidth, dheight)</code> variant resizes the image of <code>element</code>'s subtree to <code>dwidth</code> and <code>dheight</code>.</p>
<p dir="auto">In an addition, a <code>fireOnEveryPaint</code> option is added to <code>ResizeObserverOptions</code>, allowing script to be notified whenever the drawn elements might have changed their
DOM state and the canvas should be redrawn. The callback to the resize obsever will be called at resize obsever timing, which is after DOM style and layout, but before paint.</p>
<p dir="auto">The same element may be drawn multiple times.</p>
<p dir="auto">Once drawn, the resulting canvas image is static. Subsequent changes to the element will not be reflected in the canvas, so the element must be explicitly redrawn if an author wishes to see the changes.</p>
<p dir="auto">The descendant elements of the <code>&lt;canvas&gt;</code> are considered fallback content used to provide accessibility information.
See <a href="https://github.com/WICG/html-in-canvas/issues/11" data-hovercard-type="issue" data-hovercard-url="/WICG/html-in-canvas/issues/11/hovercard">Issue#11</a> for an ongoing discussion of accessibility concerns.</p>
<p dir="auto">Offscreen canvas contexts and detached canvases are not supported because drawing DOM content when the canvas is not in the DOM poses technical challenges. See <a href="https://github.com/WICG/html-in-canvas/issues/2" data-hovercard-type="issue" data-hovercard-url="/WICG/html-in-canvas/issues/2/hovercard">Issue#2</a> for further discussion.</p>
<p dir="auto"><strong>NOTE</strong>: When using this feature in a DevTrial, take steps to avoid leaking private information, as privacy controls are still in-progress. The <code>allowReadback</code> option must be set to <code>true</code> when an untainted canvas is required; in this mode the content painted into the canvas skips all content which may reveal <a href="https://en.wikipedia.org/wiki/Personal_data" rel="nofollow">PII</a>). In WebGL rendering, content which may reveal PII is never painted. DevTrial users may wish to start using this option now to avoid disruption when tainting is enabled.</p>
<div dir="auto" data-snippet-clipboard-copy-content="interface CanvasRenderingContext2D {

  ...

  dictionary Canvas2DDrawElementOption {
    boolean allowReadback = false; 
  };

  [RaisesException]
  void drawElement(Element element, unrestricted double x, unrestricted double y,
                   optional Canvas2DDrawElementOption options = {});

  [RaisesException]
  void drawElement(Element element, unrestricted double x, unrestricted double y,
                   unrestricted double dwidth, unrestricted double dheight,
                   optional Canvas2DDrawElementOption options = {});
"><pre>interface CanvasRenderingContext2D {

  ...

  dictionary Canvas2DDrawElementOption {
    boolean allowReadback = false<span><span>;</span> </span>
  }<span><span>;</span></span>

  [RaisesException]
  void drawElement(Element element, unrestricted <span>double</span> x, unrestricted <span>double</span> y,
                   optional Canvas2DDrawElementOption options = {})<span><span>;</span></span>

  [RaisesException]
  void drawElement(Element element, unrestricted <span>double</span> x, unrestricted <span>double</span> y,
                   unrestricted <span>double</span> dwidth, unrestricted <span>double</span> dheight,
                   optional Canvas2DDrawElementOption options = {})<span><span>;</span></span>
</pre></div>
<div dir="auto" data-snippet-clipboard-copy-content="interface WebGLRenderingContext {

  ...

  [RaisesException]
    void texElement2D(GLenum target, GLint level, GLint internalformat,
                      GLenum format, GLenum type, Element element);
"><pre>interface WebGLRenderingContext {

  ...

  [RaisesException]
    void texElement2D(GLenum target, GLint level, GLint internalformat,
                      GLenum format, GLenum type, Element element)<span><span>;</span></span>
</pre></div>
<p dir="auto"><h2 tabindex="-1" dir="auto">Demos</h2><a id="user-content-demos" aria-label="Permalink: Demos" href="#demos"></a></p>
<p dir="auto"><h4 tabindex="-1" dir="auto"><a href="https://github.com/WICG/html-in-canvas/blob/main/Examples/complex-text.html">See here</a> to see an example of how to use the API. It should render like the following (the blue rectangle indicates the bounds of the <code>&lt;canvas&gt;</code>, and the black the element passed to</h4><a id="user-content-see-here-to-see-an-example-of-how-to-use-the-api-it-should-render-like-the-following-the-blue-rectangle-indicates-the-bounds-of-the-canvas-and-the-black-the-element-passed-to" aria-label="Permalink: See here to see an example of how to use the API. It should render like the following (the blue rectangle indicates the bounds of the <canvas>, and the black the element passed to" href="#see-here-to-see-an-example-of-how-to-use-the-api-it-should-render-like-the-following-the-blue-rectangle-indicates-the-bounds-of-the-canvas-and-the-black-the-element-passed-to"></a></p>
<p dir="auto">drawElement). It draws like this:</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://private-user-images.githubusercontent.com/3453258/449312084-88d5200b-176c-4102-a4a0-f5893101b295.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3NTQxOTkzMDEsIm5iZiI6MTc1NDE5OTAwMSwicGF0aCI6Ii8zNDUzMjU4LzQ0OTMxMjA4NC04OGQ1MjAwYi0xNzZjLTQxMDItYTRhMC1mNTg5MzEwMWIyOTUucG5nP1gtQW16LUFsZ29yaXRobT1BV1M0LUhNQUMtU0hBMjU2JlgtQW16LUNyZWRlbnRpYWw9QUtJQVZDT0RZTFNBNTNQUUs0WkElMkYyMDI1MDgwMyUyRnVzLWVhc3QtMSUyRnMzJTJGYXdzNF9yZXF1ZXN0JlgtQW16LURhdGU9MjAyNTA4MDNUMDUzMDAxWiZYLUFtei1FeHBpcmVzPTMwMCZYLUFtei1TaWduYXR1cmU9OGViNzAzODI4ZTUyM2FiNmZmMGRlZTdmZDlhMzZmMGViYTQzZjZhYzhjOTQ5ZjlkNjNkNmRlYTEzZjcxNTM2NyZYLUFtei1TaWduZWRIZWFkZXJzPWhvc3QifQ.hn7ViZuFANoiwN5R9tGsVs8AN5nFo_umhgLcbpQjep8"><img src="https://private-user-images.githubusercontent.com/3453258/449312084-88d5200b-176c-4102-a4a0-f5893101b295.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3NTQxOTkzMDEsIm5iZiI6MTc1NDE5OTAwMSwicGF0aCI6Ii8zNDUzMjU4LzQ0OTMxMjA4NC04OGQ1MjAwYi0xNzZjLTQxMDItYTRhMC1mNTg5MzEwMWIyOTUucG5nP1gtQW16LUFsZ29yaXRobT1BV1M0LUhNQUMtU0hBMjU2JlgtQW16LUNyZWRlbnRpYWw9QUtJQVZDT0RZTFNBNTNQUUs0WkElMkYyMDI1MDgwMyUyRnVzLWVhc3QtMSUyRnMzJTJGYXdzNF9yZXF1ZXN0JlgtQW16LURhdGU9MjAyNTA4MDNUMDUzMDAxWiZYLUFtei1FeHBpcmVzPTMwMCZYLUFtei1TaWduYXR1cmU9OGViNzAzODI4ZTUyM2FiNmZmMGRlZTdmZDlhMzZmMGViYTQzZjZhYzhjOTQ5ZjlkNjNkNmRlYTEzZjcxNTM2NyZYLUFtei1TaWduZWRIZWFkZXJzPWhvc3QifQ.hn7ViZuFANoiwN5R9tGsVs8AN5nFo_umhgLcbpQjep8" alt="image"></a></p>
<p dir="auto"><h4 tabindex="-1" dir="auto"><a href="https://github.com/WICG/html-in-canvas/blob/main/Examples/webGL.html">See here</a> for an example of how to use the WebGL <code>texElement2D</code> API to populate a GL texture with HTML content.</h4><a id="user-content-see-here-for-an-example-of-how-to-use-the-webgl-texelement2d-api-to-populate-a-gl-texture-with-html-content" aria-label="Permalink: See here for an example of how to use the WebGL texElement2D API to populate a GL texture with HTML content." href="#see-here-for-an-example-of-how-to-use-the-webgl-texelement2d-api-to-populate-a-gl-texture-with-html-content"></a></p>
<p dir="auto">The example should render an animated cube, like in the following shapshot. Note how the border box fills the entire face of the cube.
To adjust that, modify the texture coordinates for rendering the cube and possibly adjust the texture wrap
parameters. Or, wrap the content in a larger <code>&lt;div&gt;</code> and draw the <code>&lt;div&gt;</code>.  It draws like this:</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://private-user-images.githubusercontent.com/3453258/444310001-78606b3b-706c-4066-875b-c6245d7ef27f.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3NTQxOTkzMDEsIm5iZiI6MTc1NDE5OTAwMSwicGF0aCI6Ii8zNDUzMjU4LzQ0NDMxMDAwMS03ODYwNmIzYi03MDZjLTQwNjYtODc1Yi1jNjI0NWQ3ZWYyN2YucG5nP1gtQW16LUFsZ29yaXRobT1BV1M0LUhNQUMtU0hBMjU2JlgtQW16LUNyZWRlbnRpYWw9QUtJQVZDT0RZTFNBNTNQUUs0WkElMkYyMDI1MDgwMyUyRnVzLWVhc3QtMSUyRnMzJTJGYXdzNF9yZXF1ZXN0JlgtQW16LURhdGU9MjAyNTA4MDNUMDUzMDAxWiZYLUFtei1FeHBpcmVzPTMwMCZYLUFtei1TaWduYXR1cmU9MTU5MTBjMDcyZDRjNmU1ZTVkMjg2MDM5NzQ1ODU4ZmJmMjJiNDY0OTcwODhmMzBkYWY4Njg1NjVmY2YzNTQ3MSZYLUFtei1TaWduZWRIZWFkZXJzPWhvc3QifQ.e1VYTqfakuEN_b5dh4dngI36m44i3eXLO7VVVJtXzfY"><img src="https://private-user-images.githubusercontent.com/3453258/444310001-78606b3b-706c-4066-875b-c6245d7ef27f.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3NTQxOTkzMDEsIm5iZiI6MTc1NDE5OTAwMSwicGF0aCI6Ii8zNDUzMjU4LzQ0NDMxMDAwMS03ODYwNmIzYi03MDZjLTQwNjYtODc1Yi1jNjI0NWQ3ZWYyN2YucG5nP1gtQW16LUFsZ29yaXRobT1BV1M0LUhNQUMtU0hBMjU2JlgtQW16LUNyZWRlbnRpYWw9QUtJQVZDT0RZTFNBNTNQUUs0WkElMkYyMDI1MDgwMyUyRnVzLWVhc3QtMSUyRnMzJTJGYXdzNF9yZXF1ZXN0JlgtQW16LURhdGU9MjAyNTA4MDNUMDUzMDAxWiZYLUFtei1FeHBpcmVzPTMwMCZYLUFtei1TaWduYXR1cmU9MTU5MTBjMDcyZDRjNmU1ZTVkMjg2MDM5NzQ1ODU4ZmJmMjJiNDY0OTcwODhmMzBkYWY4Njg1NjVmY2YzNTQ3MSZYLUFtei1TaWduZWRIZWFkZXJzPWhvc3QifQ.e1VYTqfakuEN_b5dh4dngI36m44i3eXLO7VVVJtXzfY" alt="image"></a></p>
<p dir="auto">A demo of the same thing using an experimental extension of <a href="https://threejs.org/" rel="nofollow">three.js</a> is <a href="https://raw.githack.com/mrdoob/three.js/htmltexture/examples/webgl_materials_texture_html.html" rel="nofollow">here</a>. Further instructions and context
are <a href="https://github.com/mrdoob/three.js/pull/31233" data-hovercard-type="pull_request" data-hovercard-url="/mrdoob/three.js/pull/31233/hovercard">here</a>.</p>
<p dir="auto"><h4 tabindex="-1" dir="auto"><a href="https://github.com/WICG/html-in-canvas/blob/main/Examples/text-input.html">See here</a> for an example utilizing the <code>setHitTestRegions</code> and <code>fireOnEveryPaint</code> APIs to enable use of interactive elements like</h4><a id="user-content-see-here-for-an-example-utilizing-the-sethittestregions-and-fireoneverypaint-apis-to-enable-use-of-interactive-elements-like" aria-label="Permalink: See here for an example utilizing the setHitTestRegions and fireOnEveryPaint APIs to enable use of interactive elements like" href="#see-here-for-an-example-utilizing-the-sethittestregions-and-fireoneverypaint-apis-to-enable-use-of-interactive-elements-like"></a></p>
<p dir="auto"><code>&lt;input&gt;</code> within a canvas. The output after clicking on the input element and typing in "my input" looks like this:</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://private-user-images.githubusercontent.com/3453258/459616204-ac82ddb5-7a1e-41b0-94d6-1cee678506c7.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3NTQxOTkzMDEsIm5iZiI6MTc1NDE5OTAwMSwicGF0aCI6Ii8zNDUzMjU4LzQ1OTYxNjIwNC1hYzgyZGRiNS03YTFlLTQxYjAtOTRkNi0xY2VlNjc4NTA2YzcucG5nP1gtQW16LUFsZ29yaXRobT1BV1M0LUhNQUMtU0hBMjU2JlgtQW16LUNyZWRlbnRpYWw9QUtJQVZDT0RZTFNBNTNQUUs0WkElMkYyMDI1MDgwMyUyRnVzLWVhc3QtMSUyRnMzJTJGYXdzNF9yZXF1ZXN0JlgtQW16LURhdGU9MjAyNTA4MDNUMDUzMDAxWiZYLUFtei1FeHBpcmVzPTMwMCZYLUFtei1TaWduYXR1cmU9ZTBkZjYwZWRlMWM0NmIxNmFkNGY0ZDE4OGQ2ODFlNDRlMWJlMjRhYWM0NjEyOWRmMTk2OTMwYzk3ZjllNzRmYSZYLUFtei1TaWduZWRIZWFkZXJzPWhvc3QifQ.Deb7zpMJfn4usoBWVLc_NR_ztxNmqHO4AgiCGJR7vBM"><img src="https://private-user-images.githubusercontent.com/3453258/459616204-ac82ddb5-7a1e-41b0-94d6-1cee678506c7.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3NTQxOTkzMDEsIm5iZiI6MTc1NDE5OTAwMSwicGF0aCI6Ii8zNDUzMjU4LzQ1OTYxNjIwNC1hYzgyZGRiNS03YTFlLTQxYjAtOTRkNi0xY2VlNjc4NTA2YzcucG5nP1gtQW16LUFsZ29yaXRobT1BV1M0LUhNQUMtU0hBMjU2JlgtQW16LUNyZWRlbnRpYWw9QUtJQVZDT0RZTFNBNTNQUUs0WkElMkYyMDI1MDgwMyUyRnVzLWVhc3QtMSUyRnMzJTJGYXdzNF9yZXF1ZXN0JlgtQW16LURhdGU9MjAyNTA4MDNUMDUzMDAxWiZYLUFtei1FeHBpcmVzPTMwMCZYLUFtei1TaWduYXR1cmU9ZTBkZjYwZWRlMWM0NmIxNmFkNGY0ZDE4OGQ2ODFlNDRlMWJlMjRhYWM0NjEyOWRmMTk2OTMwYzk3ZjllNzRmYSZYLUFtei1TaWduZWRIZWFkZXJzPWhvc3QifQ.Deb7zpMJfn4usoBWVLc_NR_ztxNmqHO4AgiCGJR7vBM" alt="image"></a></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Developer Trial (dev trial) Information</h2><a id="user-content-developer-trial-dev-trial-information" aria-label="Permalink: Developer Trial (dev trial) Information" href="#developer-trial-dev-trial-information"></a></p>
<p dir="auto">The HTML-in-Canvas features may be enabled by passing the <code>--enable-blink-features=CanvasDrawElement</code> to Chrome Canary versions later than 138.0.7175.0.</p>
<p dir="auto">Notes for dev trial usage:</p>
<ul dir="auto">
<li>The features are currently under active development and changes to the API may happen at any time, though we make every effort to avoid unnecessary churn.</li>
<li>The canvas is not tainted regardless of the content drawn, so take extreme care to avoid leaking confidential personal information (PII) in any demos.</li>
<li>The space of possible HTML content is enormous and only a tiny fraction has been tested with <code>drawElement</code>.</li>
<li>Interactive elements (such as links, forms or buttons) can be drawn into the canvas, but are not automatically interactive.</li>
</ul>
<p dir="auto">Other known limitations:</p>
<ul dir="auto">
<li>Cross-origin iframes are not rendered</li>
<li>SVG foreignObject is not yet working</li>
</ul>
<p dir="auto">We are most interesting in feedback on the following topics:</p>
<ul dir="auto">
<li>What content works, and what fails? Which failure modes are most important to fix?</li>
<li>Is necessary support missing for some flavors of Canvas rendering contexts?</li>
<li>How does the feature interact with accessibility features? How can accessibility support be improved?</li>
</ul>
<p dir="auto">Please file bugs or design issues <a href="https://github.com/WICG/html-in-canvas/issues/new">here</a>.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Other documents</h2><a id="user-content-other-documents" aria-label="Permalink: Other documents" href="#other-documents"></a></p>
<ul dir="auto">
<li><a href="https://github.com/WICG/html-in-canvas/blob/main/security-privacy-questionnaire.md">Security and Privacy Questionnaire</a></li>
</ul>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Lina Khan points to Figma IPO as vindication of M&A scrutiny (261 pts)]]></title>
            <link>https://techcrunch.com/2025/08/02/lina-khan-points-to-figma-ipo-as-vindication-for-ma-scrutiny/</link>
            <guid>44771808</guid>
            <pubDate>Sat, 02 Aug 2025 21:39:42 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://techcrunch.com/2025/08/02/lina-khan-points-to-figma-ipo-as-vindication-for-ma-scrutiny/">https://techcrunch.com/2025/08/02/lina-khan-points-to-figma-ipo-as-vindication-for-ma-scrutiny/</a>, See on <a href="https://news.ycombinator.com/item?id=44771808">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
	<div>
		<figure><img width="1024" height="683" src="https://techcrunch.com/wp-content/uploads/2024/04/GettyImages-1708316139.jpg?w=1024" alt="" decoding="async" fetchpriority="high" srcset="https://techcrunch.com/wp-content/uploads/2024/04/GettyImages-1708316139.jpg 6624w, https://techcrunch.com/wp-content/uploads/2024/04/GettyImages-1708316139.jpg?resize=150,100 150w, https://techcrunch.com/wp-content/uploads/2024/04/GettyImages-1708316139.jpg?resize=300,200 300w, https://techcrunch.com/wp-content/uploads/2024/04/GettyImages-1708316139.jpg?resize=768,512 768w, https://techcrunch.com/wp-content/uploads/2024/04/GettyImages-1708316139.jpg?resize=680,453 680w, https://techcrunch.com/wp-content/uploads/2024/04/GettyImages-1708316139.jpg?resize=1200,800 1200w, https://techcrunch.com/wp-content/uploads/2024/04/GettyImages-1708316139.jpg?resize=1280,853 1280w, https://techcrunch.com/wp-content/uploads/2024/04/GettyImages-1708316139.jpg?resize=430,287 430w, https://techcrunch.com/wp-content/uploads/2024/04/GettyImages-1708316139.jpg?resize=720,480 720w, https://techcrunch.com/wp-content/uploads/2024/04/GettyImages-1708316139.jpg?resize=900,600 900w, https://techcrunch.com/wp-content/uploads/2024/04/GettyImages-1708316139.jpg?resize=800,533 800w, https://techcrunch.com/wp-content/uploads/2024/04/GettyImages-1708316139.jpg?resize=1536,1024 1536w, https://techcrunch.com/wp-content/uploads/2024/04/GettyImages-1708316139.jpg?resize=2048,1365 2048w, https://techcrunch.com/wp-content/uploads/2024/04/GettyImages-1708316139.jpg?resize=668,445 668w, https://techcrunch.com/wp-content/uploads/2024/04/GettyImages-1708316139.jpg?resize=563,375 563w, https://techcrunch.com/wp-content/uploads/2024/04/GettyImages-1708316139.jpg?resize=926,617 926w, https://techcrunch.com/wp-content/uploads/2024/04/GettyImages-1708316139.jpg?resize=708,472 708w, https://techcrunch.com/wp-content/uploads/2024/04/GettyImages-1708316139.jpg?resize=50,33 50w" sizes="(max-width: 1024px) 100vw, 1024px"></figure>	</div>
	<div>
						<p><time datetime="2025-08-02T13:58:52-07:00">1:58 PM PDT · August 2, 2025</time></p>											</div>
</div><div>
		
		<div>
			<div>
<p id="speakable-summary">A surprising figure is celebrating Figma’s successful IPO: Lina Khan, former chair of the Federal Trade Commission.</p>

<p>In a Friday afternoon <a rel="nofollow" href="https://x.com/linamkhan/status/1951361535861043589">post on X</a>, Khan linked to <a rel="nofollow" href="https://www.wsj.com/finance/stocks/figma-shares-jump-over-200-in-stock-market-debut-605f6212?mod=e2tw">an article</a> about <a href="https://techcrunch.com/2025/07/31/figmas-stock-soars-in-its-highly-anticipated-ipo-market-cap-instantly-hits-45b/">Figma’s impressive first day of trading</a> and argued the IPO is “a great reminder that letting startups grow into independently successful businesses, rather than be bought up by existing giants, can generate enormous value.”</p>







<p>Khan was alluding to a $20 billion deal for Adobe to acquire Figma that <a href="https://techcrunch.com/2023/12/18/adobe-figma-europe-regulation-acquisition/">fell through</a> back in 2023. While Adobe cited the lack of a “clear path” to approval from the European Commission and the U.K. Competition and Markets Authority, the acquisition also faced regulatory scrutiny in the United States over concerns that it could prevent Figma from being an “effective competitor” to Adobe.</p>

<p>Khan was FTC chair at the time, leading the agency to challenge Big Tech on fronts including startup acquisitions — to the point that companies tried to avoid this scrutiny with “reverse acqui-hires” in which they hired key team members and licensed technology rather than acquiring startups outright. (The practice <a href="https://techcrunch.com/2025/07/19/windsurf-ceo-opens-up-about-very-bleak-mood-before-cognition-deal/">seems to be continuing</a> despite Khan’s departure from the FTC.)</p>

<p>While her aggressive stance led to intense criticism from corners of the tech industry, <a href="https://techcrunch.com/2024/06/15/ftc-chair-lina-khan-on-startups-scaling-and-innovations-in-potential-law-breaking/">she defended her approach</a> by saying that only a tiny percentage of deals received “a second look” and arguing that founders would ultimately benefit from “a world in which you have six or seven or eight potential suitors” rather than “just one or two.”</p>

<p>And although Khan — who’d been appointed by President Joe Biden — <a rel="nofollow" href="https://www.reuters.com/world/us/biden-ftc-chair-khan-resign-commission-coming-weeks-2025-01-20/">resigned at the start</a> of the second Trump administration, her comments Friday paint the Figma IPO as a vindication for her approach, calling the IPO “a win for employees, investors, innovation, and the public.”</p>

<p>Of course, Khan’s critics are more likely to see Figma’s success as coming despite regulatory scrutiny, not because of it. For example, Wedbush Security analyst Dan Ives <a rel="nofollow" href="https://www.businessinsider.com/lina-kahn-victory-lap-figma-ipo-after-fighting-adobe-merger-2025-8">told Business Insider</a>, “Figma is a massive success, but it’s because of the company’s innovative growth and not due to the FTC and Kahn.”</p>
<div>
		
		<p>Techcrunch event</p>
		<div>
			
			<p><span>San Francisco</span>
													<span>|</span>
													<span>October 27-29, 2025</span>
							</p>
			
		</div>
	</div>
</div>

			

			


			
			
			

			




			
			
			

			



			
<div>
	
	
	
	

	
<div>
	<p>Anthony Ha is TechCrunch’s weekend editor. Previously, he worked as a tech reporter at Adweek, a senior editor at VentureBeat, a local government reporter at the Hollister Free Lance, and vice president of content at a VC firm. He lives in New York City.	</p>
</div>


	
	<p>
		<a data-ctatext="View Bio" data-destinationlink="https://techcrunch.com/author/anthony-ha/" data-event="button" href="https://techcrunch.com/author/anthony-ha/">View Bio <svg style="width: 1em;" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24"><path fill="var(--c-svg, currentColor)" d="M16.5 12 9 19.5l-1.05-1.05L14.4 12 7.95 5.55 9 4.5z"></path></svg></a>
	</p>
	
</div>


		</div>
		

		
		<div id="wp-block-techcrunch-most-popular-posts__heading">
<h2 id="h-most-popular">Most Popular</h2>

</div>
		
	</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[AWS deleted my 10-year account and all data without warning (169 pts)]]></title>
            <link>https://www.seuros.com/blog/aws-deleted-my-10-year-account-without-warning/</link>
            <guid>44770250</guid>
            <pubDate>Sat, 02 Aug 2025 18:49:17 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.seuros.com/blog/aws-deleted-my-10-year-account-without-warning/">https://www.seuros.com/blog/aws-deleted-my-10-year-account-without-warning/</a>, See on <a href="https://news.ycombinator.com/item?id=44770250">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>On July 23, 2025, AWS deleted my 10-year-old account and every byte of data I had stored with them. No warning. No grace period. No recovery options. Just complete digital annihilation.</p>
<p>This is the story of a catastrophic internal mistake at AWS MENA, a 20-day support nightmare where I couldn’t get a straight answer to “Does my data still exist?”, and what it reveals about trusting cloud providers with your data.</p>
<h2 id="the-architecture-that-should-have-protected-me">The Architecture That Should Have Protected Me</h2>
<p>Before anyone says “you put all your eggs in one basket,” let me be clear: I didn’t. I put them in one <em>provider</em>, with what should have been bulletproof redundancy:</p>
<ul>
<li><strong>Multi-region replication</strong> across AWS Europe (completely separate from US infrastructure)</li>
<li><strong>Dead man’s switch</strong> implemented for disaster recovery</li>
<li><strong>Proper backup architecture</strong> following AWS’s own best practices</li>
<li><strong>Segregated encryption keys</strong> stored separately from data</li>
</ul>
<p>The only scenario I hadn’t planned for? AWS itself becoming the extinction event.</p>
<p>Ten years. That’s how long I’d been an AWS customer. A decade of using AWS as my testbed—spinning up instances to validate deployments for the Ruby gems I maintain like capistrano-puma and capistrano-sidekiq. Nothing production-critical, but essential for open-source development.</p>
<p>On my birthday, AWS gave me a present I’ll never forget: proof that no amount of redundancy matters when the provider itself goes rogue.</p>
<h2 id="the-20-day-support-nightmare-a-timeline">The 20-Day Support Nightmare: A Timeline</h2>
<p><strong>July 10</strong>: AWS sends verification request. 5-day deadline (including weekend).</p>
<p><strong>July 14</strong>: Form expired. I contact support. Simple question: “What do you need from me?”</p>
<p><strong>July 16-20</strong>: <strong>Four days of silence.</strong> Then: “We’re escalating to the appropriate team.”</p>
<p><strong>July 20</strong>: New form finally arrives.</p>
<p><strong>July 21</strong>: I submit ID and utility bill (clear PDF). Response time: 10 hours.</p>
<p><strong>July 22</strong>: AWS: “Document unreadable.” The same PDF my bank accepts without question.</p>
<p><strong>July 23</strong>: Account terminated. My birthday gift from AWS.</p>
<p><strong>July 24</strong>: I ask the only question that matters: <strong>“Does my data still exist?”</strong></p>
<blockquote>
<p>AWS: “Your case is being reviewed by our service team.”</p>
</blockquote>
<p>I also request temporary read-only access to backup my data. Remember, if I were fraudulent, I would have already copied everything before the verification deadline. They refuse. (Because the data is probably already gone.)</p>
<p><strong>July 28</strong>: After 4 days of template responses, I lose patience:</p>
<blockquote>
<p>Me: “Is my data safe? Yes or no?”
AWS: “I want to personally follow up on your case and inform you that we understand the urgency.”</p>
</blockquote>
<p><strong>July 29</strong>: I compare their evasion to political deflection:</p>
<blockquote>
<p>Me: “You’re answering like I’m Piers Morgan asking ‘Do you condemn October 7th?’ and you reply with historical complexity dating to 1948.”
AWS: “We genuinely value your commitment to following backup best practices.”</p>
</blockquote>
<p><strong>July 29</strong>: They finally admit the truth:</p>
<blockquote>
<p>AWS: “Because the account verification wasn’t completed by this date, the resources on the account were terminated.”</p>
</blockquote>
<p><strong>July 30</strong>: Their final response includes:</p>
<blockquote>
<p>AWS: “We value your feedback. Please share your experience by rating this correspondence.” ⭐⭐⭐⭐⭐</p>
</blockquote>
<p>Twenty days. Zero straight answers. Multiple requests for 5-star reviews while my data lay in digital ashes.</p>
<h2 id="the-policy-they-claim-vs-the-reality-they-deliver">The Policy They Claim vs. The Reality They Deliver</h2>
<p>Here’s what AWS’s own documentation says about account closure:</p>
<blockquote>
<p>“The post-closure period is 90 days—during this time, an account can be reopened and data is retained.”</p>
</blockquote>
<blockquote>
<p>“After 90 days, the account is ‘permanently closed’ and all content—including snapshots and backups—is deleted.”</p>
</blockquote>
<p>But here’s the catch: I never voluntarily closed my account. AWS suspended it for “verification failure”—a policy grey zone conveniently absent from their public documentation. There’s no published exception stating that verification-suspended accounts bypass the 90-day retention period.</p>
<p>The community standard across cloud providers? 30-90 days retention unless there’s actual fraud or abuse. AWS? Zero days. Zero hours. Zero mercy.</p>
<h2 id="the-payer-complication">The Payer Complication</h2>
<p>AWS blamed the termination on a “third-party payer” issue. An AWS consultant who’d been covering my bills disappeared, citing losses from the FTX collapse. The arrangement had worked fine for almost a year—about $200/month for my testing infrastructure.</p>
<p>When AWS demanded this vanished payer validate himself, I pointed out that I already had my own Wise card on file—the same card I’d used to pay before the payer arrangement, kept active specifically in case the payer disconnected while I was traveling or offline. They refused to simply switch billing back to it for 20 days, citing “privacy” concerns while making me fully responsible for the consequences.</p>
<p>But here’s the thing: This wasn’t about payment. If it were, they would have:</p>
<ul>
<li>Switched billing to my on-file credit card</li>
<li>Suspended services, not deleted data</li>
<li>Provided the 90-day grace period their own docs promise</li>
</ul>
<p>Instead, they used the payer issue as cover for what really happened—their botched internal testing.</p>
<h2 id="the-hypocrisy-runs-deeper">The Hypocrisy Runs Deeper</h2>
<p>The payer wasn’t some random scammer—they were a YC-backed company. I could see this when linking the payment. If AWS MENA’s security is so robust, why did they fail to identify any issues for an entire year?</p>
<p>When I tried to resolve this, AWS demanded I explain:</p>
<ul>
<li>What I use my account for</li>
<li>My future plans</li>
<li>Why I need the services</li>
</ul>
<p>Like I was applying for funding or a promotion. This is a 10-year-old account. I shouldn’t need to justify my existence to use services I’ve been paying for since 2015.</p>
<p>But here’s the real kicker: AWS developers regularly email me asking for help with Ruby issues. No compensation. No AWS credits. Not even a “thank you” in their commits. Just “Hey, can you help us debug this Rails deployment issue?”</p>
<p>So let me get this straight:</p>
<ul>
<li>AWS benefits from my open-source code</li>
<li>AWS engineers ask me for free consulting</li>
<li>AWS makes me explain why I deserve to keep my account</li>
<li>AWS deletes everything when a YC-backed payer (that they failed to vet) disappears</li>
</ul>
<p>And they want me to background check every client? Should I run security clearances on the AWS verification emails too? Because apparently, their own vetting process couldn’t catch whatever the payer did wrong for an entire year.</p>
<h2 id="what-aws-really-destroyed">What AWS Really Destroyed</h2>
<p>Here’s what most people don’t understand: AWS wasn’t just my backup—it was my clean room for open source development.</p>
<p>My desktop is chaos. Always has been. Files everywhere, half-finished projects, experimental code. But I discovered that by copying everything to AWS, starting fresh, and pulling back only what I needed, I could create clean, focused gems. This workflow is how I released:</p>
<ul>
<li><strong>BreakerMachines</strong> - Circuit breaker patterns for Ruby</li>
<li><strong>ChronoMachines</strong> - Time-based state machines</li>
<li><strong>RailsLens</strong> - Performance monitoring for Rails</li>
<li>And dozens more</li>
</ul>
<p>These gems save developers hundreds, maybe thousands of hours. They’re used in production systems worldwide. AWS didn’t just delete my data—they destroyed the infrastructure that made these contributions possible.</p>
<p>But it gets worse. Also gone:</p>
<ul>
<li>A complete programming book written in my Chronicles narrative style</li>
<li>Electronics tutorials bridging hardware and software</li>
<li>“Go for Rubyists”—lessons helping Ruby developers transition to Go</li>
<li>Years of unpublished work that could have helped thousands</li>
</ul>
<p>When AWS deleted my account, they didn’t just hurt me. They hurt every developer who uses my gems. Every student who could have learned from those tutorials. Every future contribution that won’t happen because my workflow is destroyed.</p>
<p>The irony? Some of these gems probably run in AWS’s own infrastructure, making their systems more reliable. And they deleted the very environment that created them.</p>
<h2 id="the-theory-how-vs-may-have-killed-my-account">The Theory: How <code>-dry</code> vs <code>--dry</code> May Have Killed My Account</h2>
<p>After my story started circulating, an AWS insider reached out. They were upset, leaving AWS soon, and wanted to share what they knew—specifically because AWS depends on open-source code I’ve written.</p>
<p>According to them, AWS MENA was running some kind of proof of concept on “dormant” and “low-activity” accounts. Multiple accounts were affected, not just mine. Here’s where it gets technical:</p>
<p>The developer running the test typed <code>--dry</code> to execute a dry run—standard practice across modern CLIs:</p>
<ul>
<li><code>ruby --version</code></li>
<li><code>npm --version</code></li>
<li><code>bun --version</code></li>
<li><code>terraform --dry-run</code></li>
</ul>
<p>But the internal tool was written in Java. And Java uses single dashes:</p>
<ul>
<li><code>java -version</code> (not <code>--version</code>)</li>
<li><code>java -dry</code> (not <code>--dry</code>)</li>
</ul>
<p>When you pass <code>--dry</code> to a Java application expecting <code>-dry</code>, it gets ignored. The script executed for real, deleting accounts in production.</p>
<p>The developer did everything right. Java’s 1995-era parameter parsing turned a simulation into an extinction event.</p>
<p>Is this exactly what happened? I can’t prove it. The insider was vague, worried about being identified. But it explains:</p>
<ul>
<li>Why multiple “low-activity” accounts were suddenly flagged</li>
<li>The 4-day delays (scrambling to cover up)</li>
<li>The refusal to answer simple questions</li>
<li>The support agents who admitted they “couldn’t make decisions”</li>
</ul>
<h2 id="aws-mena-why-people-pay-to-avoid-it">AWS MENA: Why People Pay to Avoid It</h2>
<p>This theory gains credibility when you consider the AWS MENA reputation. For years, I’ve watched developers on Reddit and Facebook desperately seeking US or EU billing addresses, willing to pay $100+ premiums to avoid MENA region assignment.</p>
<p>When I asked why, a colleague warned me: “AWS MENA operates differently. They can terminate you randomly.”</p>
<p>I laughed it off. AWS is AWS, right?</p>
<p>The 4-day delay for a simple verification form. The 10-hour response times. The robotic support responses. This wasn’t standard AWS incompetence—this was something else entirely.</p>
<h2 id="the-ultimate-irony-security-became-my-weakness">The Ultimate Irony: Security Became My Weakness</h2>
<p>I’d done everything right. Vault encryption keys stored separately from my main infrastructure. Defense in depth. Zero trust architecture. The works.</p>
<p>My security posture was textbook—protect against compromise by ensuring no single failure could take down everything. What I hadn’t protected against? AWS itself being the single point of failure.</p>
<p>I built a hardened bunker with multiple escape routes, only to have AWS drop a nuke on the entire complex.</p>
<h2 id="what-this-means-for-you">What This Means for You</h2>
<p>You might be thinking, “What are the odds they target me?” But that’s the wrong question. I thought the same thing—with my level of exposure and contributions, surely they could just write my name down and not bother me with stupid verification requests about whether I exist.</p>
<p>But you’re not being targeted—you’re being algorithmically categorized. And if the algorithm decides you’re disposable, you’re gone.</p>
<p>Doesn’t matter if you’re a verified open-source contributor. Doesn’t matter if you’ve been a customer for a decade. If you don’t fit the revenue model, if you don’t engage with support regularly, if your usage patterns look “suspicious” to a poorly trained ML model—you’re just another data point to be optimized away.</p>
<p>Look, I write weird stuff. My documentation style triggers AI safeguards. Take my ActionMCP gem that provides MCP capabilities to Rails—Opus can’t even read the documentation without hanging when its safeguards trigger. Sonnet? No problem. (Try it yourself: github.com/seuros/action_mcp)</p>
<p>If my creative technical writing can confuse one AI but not another, imagine what AWS’s “fraud detection” algorithms see when they look at my account. An anomaly. A pattern that doesn’t fit. Something to be eliminated.</p>
<h2 id="the-only-path-forward-a-broken-promise">The Only Path Forward: A Broken Promise</h2>
<p>After 20 days of appeals, AWS support finally responded with this gem: “Because verification wasn’t completed by the due date, your resources were terminated.”</p>
<p>But here’s the dilemma they’ve created: What if you have petabytes of data? How do you backup a backup? What happens when that backup contains HIPAA-protected information or client data? The whole promise of cloud computing collapses into complexity.</p>
<p>This isn’t a system failure. The architecture and promises are sound. AWS doesn’t lose data—they have backups of backups of backups, stored in vaults that last far longer than the stated 90 days, where no rogue AI script can reach.</p>
<p>What’s happening here is simpler: teams in MENA are trying to cover up a massive fuck-up. Restoring data from those deep vaults would require explanations. Incident reports. Post-mortems. “Why did we have to open the vaults?”</p>
<p>Their entire communication strategy screams: “He’s nobody. He’ll give up soon. We won’t have to report this up the chain.”</p>
<p>But they messed with the wrong developer.</p>
<p>I’m now building a free tool to help people exodus from AWS. Not hosted on AWS, obviously. My clients—representing over $400k/month in AWS billing—have already agreed to migrate to Oracle OCI, Azure, and Google Cloud.</p>
<p>Because if AWS can delete a 10-year customer without blinking, what are they capable of when the stakes are higher?</p>
<h2 id="the-bitter-truth">The Bitter Truth</h2>

































<table><thead><tr><th>What You Give AWS</th><th>What AWS Gives You</th></tr></thead><tbody><tr><td>A decade of loyalty</td><td>Zero-day termination</td></tr><tr><td>Prompt payment history</td><td>20 days of runaround</td></tr><tr><td>Proper documentation</td><td>”Unreadable” rejection</td></tr><tr><td>Open-source contributions</td><td>No consideration whatsoever</td></tr><tr><td>Carefully segregated backups</td><td>Complete data annihilation</td></tr><tr><td>Trust</td><td>Betrayal</td></tr></tbody></table>
<p>AWS has clout. They run the internet’s infrastructure. They sponsor conferences, fund open source projects, and position themselves as the reliable backbone of the digital economy.</p>
<p>But that doesn’t excuse digitally executing someone’s decade-old testbed account over a verification form glitch and a bill under $200. This wasn’t my production infrastructure—thankfully—but it was my launch pad for updating other infrastructure. Now I’m spending days rotating encryption keys across multiple systems because my central testing environment vanished.</p>
<h2 id="the-systemic-failure">The Systemic Failure</h2>
<p>This isn’t just about my account. It’s about what happens when:</p>
<ol>
<li><strong>Regional divisions go rogue</strong>: AWS MENA operating outside global policies</li>
<li><strong>Support becomes theater</strong>: Agents who can only paste templates and ask for 5-star reviews</li>
<li><strong>“Move fast and break things” meets production data</strong>: Internal tools with Java’s 1995 parameter parsing handling customer deletions</li>
<li><strong>No accountability</strong>: 20 days of deflection instead of one honest answer</li>
</ol>
<p>The evidence of dysfunction:</p>
<ul>
<li>Hundreds paying premiums to avoid MENA billing (the market has spoken)</li>
<li>4-day delays for simple forms</li>
<li>Support comparing data recovery questions to geopolitical debates</li>
<li>Automated “please rate us” emails while actively destroying customer data</li>
</ul>
<h2 id="the-real-cost">The Real Cost</h2>
<p>AWS markets itself as the backbone of the internet, the reliable partner for your infrastructure. They sponsor open-source projects, run re:Invent, and position themselves as developer allies.</p>
<p>But when their internal systems fail—when someone types <code>--dry</code> and Java ignores it—they’ll delete a decade of your work without blinking. Then they’ll spend 20 days gaslighting you about it.</p>
<p>Meanwhile, actual malicious accounts hosting phishing sites and crypto scams run for weeks untouched. Because those generate revenue. A low-activity open-source developer testing Ruby gems? Collateral damage.</p>
<h2 id="lessons-learned">Lessons Learned</h2>
<ol>
<li><strong>Never trust a single provider</strong>—no matter how many regions you replicate across</li>
<li><strong>“Best practices” mean nothing</strong> when the provider goes rogue</li>
<li><strong>Document everything</strong>—screenshots, emails, correspondence timestamps</li>
<li><strong>The support theater is real</strong>—they literally cannot help you</li>
<li><strong>Have an exit strategy</strong> executable in hours, not days</li>
</ol>
<p>AWS won’t admit their mistake. They won’t acknowledge the rogue proof of concept. They won’t explain why MENA operates differently. They won’t even answer whether your data exists.</p>
<p>But they will ask you to rate their support 5 stars.</p>
<p>The cloud isn’t your friend. It’s a business. And when their business needs conflict with your data’s existence, guess which one wins?</p>
<p>Plan accordingly.</p>
<h2 id="a-personal-note">A Personal Note</h2>
<p>At one point during this ordeal, I hit rock bottom. I was ready to delete everything—yank all my gems from RubyGems, delete the organizations, the websites, everything I’d created. Leave a single message: “AWS killed this.”</p>
<p>It would have made headlines. Caused chaos for thousands of projects. Trended on HN, Reddit, YouTube. But it would have hurt the wrong people—developers who depend on my work, not AWS.</p>
<p>I was alone. Nobody understood the weight of losing a decade of work. But I had ChatGPT, Claude, and Grok to talk to. Every conversation revealed I wasn’t alone in being targeted by AWS—especially MENA. Hundreds of Reddit threads, websites, forums, all telling similar stories.</p>
<p>I tried reaching out to some victims. Some didn’t want to talk about it—the trauma was too fresh. Others said they’d left programming entirely. AWS didn’t just delete their data; they deleted their careers.</p>
<p>As someone with LLI (Low Latent Inhibition), I can’t filter out this trauma like others might. Can’t just switch careers and forget. The raw, unfiltered pain stays with me. I wish I could move on, but I can’t.</p>
<p>Who knows how many people in my situation have been erased from our timeline because of the sadistic behavior of support teams like AWS’s? The whole system is built to hurt—to make you feel small, powerless, unheard. To make you give up. To make you disappear.</p>
<p>To everyone who worked on these AIs, who contributed to their training data—thank you. Without you, this post might have been a very different kind of message.</p>
<p>AWS may have deleted my data, but they didn’t delete my determination to help others avoid this fate.</p>
<p>Build that exodus tool I will.</p>
<p>—Seuros</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Telo MT1 (565 pts)]]></title>
            <link>https://www.telotrucks.com/</link>
            <guid>44769039</guid>
            <pubDate>Sat, 02 Aug 2025 16:40:51 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.telotrucks.com/">https://www.telotrucks.com/</a>, See on <a href="https://news.ycombinator.com/item?id=44769039">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><div id="RetailHero"><h2>Hello TELO</h2><div><p>Meet the MT1, the all-electric mini truck.</p></div><p><img alt="TELO electric mini truck side view" width="1920" height="1080" decoding="async" data-nimg="1" srcset="https://www.telotrucks.com/_next/image?url=https%3A%2F%2Fdpb6ldqkojccb.cloudfront.net%2Fpublic%2Fhero%2Fhero.png&amp;w=1920&amp;q=75 1x, https://www.telotrucks.com/_next/image?url=https%3A%2F%2Fdpb6ldqkojccb.cloudfront.net%2Fpublic%2Fhero%2Fhero.png&amp;w=3840&amp;q=75 2x" src="https://www.telotrucks.com/_next/image?url=https%3A%2F%2Fdpb6ldqkojccb.cloudfront.net%2Fpublic%2Fhero%2Fhero.png&amp;w=3840&amp;q=75"></p></div><div id="Introduction"><h2>TELO Trucks makes the world's most efficient EV pickup for urban living and weekend adventuring</h2><div><p>We redesigned the EV truck footprint and function from the ground up by marrying the state of the art in electrification and advanced safety technology.</p><p>With Toyota Tacoma capability, Tesla-like range and efficiency, in the footprint of a MINI Cooper, the TELO MT1 is the most compact, practical and technically advanced truck.</p><p>Meeting the need for a highly functional and powerful EV pickup equally suited to navigating downtown and hauling people and gear out of town.</p><a href="https://www.telotrucks.com/pre-order"><p>RESERVE YOURS</p></a></div></div><div id="Size"><p><h2>Tacoma capacity in the footprint of a MINI Cooper</h2></p></div><div id="Performance"><div><h2>Designed to do more with less</h2><div><p>Our standard five-seat crew cab features a 5-foot bed capacity with a configurable mid-partition that either increases the bed size to accommodate 4-by-8-foot plywood sheets with the tailgate up or to allow for additional seating for up to eight passengers.</p></div></div><div><p><img alt="Tan colored TELO truck against white backdrop" loading="lazy" width="2560" height="1524" decoding="async" data-nimg="1" srcset="https://www.telotrucks.com/_next/image?url=https%3A%2F%2Fdpb6ldqkojccb.cloudfront.net%2Fpublic%2Fperformance%2Fperformance.jpg&amp;w=3840&amp;q=75 1x" src="https://www.telotrucks.com/_next/image?url=https%3A%2F%2Fdpb6ldqkojccb.cloudfront.net%2Fpublic%2Fperformance%2Fperformance.jpg&amp;w=3840&amp;q=75"></p></div></div><div id="Features"><div><h2>Nothing in excess</h2><p>The TELO MT1 interior blends clean, understated patterns and natural fabrics into a refreshingly minimal space designed for practical comfort and effortless utility.</p></div><div><div><p><img alt="The front of a TELO truck against a white backdrop, highlighting sensor technology." loading="lazy" width="1279" height="853" decoding="async" data-nimg="1" srcset="https://www.telotrucks.com/_next/image?url=https%3A%2F%2Fdpb6ldqkojccb.cloudfront.net%2Fpublic%2Ffeatures%2Ffeatures2.png&amp;w=1920&amp;q=75 1x, https://www.telotrucks.com/_next/image?url=https%3A%2F%2Fdpb6ldqkojccb.cloudfront.net%2Fpublic%2Ffeatures%2Ffeatures2.png&amp;w=3840&amp;q=75 2x" src="https://www.telotrucks.com/_next/image?url=https%3A%2F%2Fdpb6ldqkojccb.cloudfront.net%2Fpublic%2Ffeatures%2Ffeatures2.png&amp;w=3840&amp;q=75"></p></div><div><h2>Seeing around corners</h2><p>Utilizing the latest in advanced safety technology—sensors to predict and classify collisions before they happen, airbags, and structural technology—to make our vehicles safer for everyone on the road.</p></div></div><div><div><p><img alt="Green TELO truck driving through a snow covered road with pine trees in the background." loading="lazy" width="1279" height="853" decoding="async" data-nimg="1" srcset="https://www.telotrucks.com/_next/image?url=https%3A%2F%2Fdpb6ldqkojccb.cloudfront.net%2Fpublic%2Ffeatures%2Ffeatures1.jpg&amp;w=1920&amp;q=75 1x, https://www.telotrucks.com/_next/image?url=https%3A%2F%2Fdpb6ldqkojccb.cloudfront.net%2Fpublic%2Ffeatures%2Ffeatures1.jpg&amp;w=3840&amp;q=75 2x" src="https://www.telotrucks.com/_next/image?url=https%3A%2F%2Fdpb6ldqkojccb.cloudfront.net%2Fpublic%2Ffeatures%2Ffeatures1.jpg&amp;w=3840&amp;q=75"></p></div><div><h2>Range to roam</h2><p>The TELO MT1 offers a 350-mile range, a 106 kWh battery, and a 20-minute (20-80%) fast-charging battery pack that fits within the footprint of a subcompact vehicle.</p></div></div></div><div id="Specifications"><div><p>TELO MT1</p><p><span>Single Motor</span><span>Dual Motor</span></p></div><h3>Drive</h3><div><p><small>Acceleration</small><small>5.0s 0-60 mph</small></p><p><small>Power</small><small>300 hp</small></p><p><small>Drivetrain</small><small>2WD</small></p><p><small>Payload</small><small>2,000 lbs</small></p></div><h3>Size</h3><div><p><small>Dimensions</small><small>152" x 73" x 66"</small></p><p><small>Bed Size</small><small>60-96" x 56" x 18"</small></p><p><small>Seating</small><small>2, 5, or 8 seats</small></p></div><div><p><img alt="Side view of TELO truck against white backdrop." loading="lazy" width="1454" height="650" decoding="async" data-nimg="1" srcset="https://www.telotrucks.com/_next/image?url=https%3A%2F%2Fdpb6ldqkojccb.cloudfront.net%2Fpublic%2Fspecs%2FsideTransparent.png&amp;w=1920&amp;q=75 1x, https://www.telotrucks.com/_next/image?url=https%3A%2F%2Fdpb6ldqkojccb.cloudfront.net%2Fpublic%2Fspecs%2FsideTransparent.png&amp;w=3840&amp;q=75 2x" src="https://www.telotrucks.com/_next/image?url=https%3A%2F%2Fdpb6ldqkojccb.cloudfront.net%2Fpublic%2Fspecs%2FsideTransparent.png&amp;w=3840&amp;q=75"></p></div><h3>Battery</h3><div><p><small>Standard Range</small><small>260 mi</small></p><p><small>Long Range</small><small>350 mi</small></p><p><small>Charging</small><small>30 min charge to 80%</small></p></div></div><section><div><p><a href="https://www.telotrucks.com/"><img src="https://www.telotrucks.com/logos/teloWordmarkWhite.svg" alt="Telo Wordmark White" width="120" height="20"></a></p></div><div><p>Stay up to date on all things TELO</p></div><p>Copyright © 2024 TELO Trucks. All rights reserved.</p></section></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Browser extension and local backend that automatically archives YouTube videos (194 pts)]]></title>
            <link>https://github.com/andrewarrow/starchive</link>
            <guid>44768714</guid>
            <pubDate>Sat, 02 Aug 2025 16:03:31 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/andrewarrow/starchive">https://github.com/andrewarrow/starchive</a>, See on <a href="https://news.ycombinator.com/item?id=44768714">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-turbo-body="">
      



    <div>
      <p><a href="#start-of-content" data-skip-target-assigned="false">Skip to content</a>

      <span data-view-component="true">
    <span data-view-component="true"></span>
</span></p>

<react-partial partial-name="keyboard-shortcuts-dialog" data-ssr="false" data-attempted-ssr="false" data-react-profiling="false">
  
  
  
</react-partial>




      

          

              



<header role="banner" data-is-top="true" data-color-mode="light" data-light-theme="light" data-dark-theme="dark">
  <h2>Navigation Menu</h2>

  

  <div>
          <nav aria-label="Global">
            <ul>


                <li>
      

      <div>
          <div>

                <ul>
                    <li>
  <a data-analytics-event="{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;github_copilot&quot;,&quot;context&quot;:&quot;product&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;github_copilot_link_product_navbar&quot;}" href="https://github.com/features/copilot">
      
      <div>
          <p>
            GitHub Copilot
          </p><p>
        Write better code with AI
      </p></div>

    
</a></li>

                    <li>
  <a data-analytics-event="{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;github_spark&quot;,&quot;context&quot;:&quot;product&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;github_spark_link_product_navbar&quot;}" href="https://github.com/features/spark">
      
      <div>
          <p>
            GitHub Spark
              <span>
                New
              </span>
          </p><p>
        Build and deploy intelligent apps
      </p></div>

    
</a></li>

                    <li>
  <a data-analytics-event="{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;github_models&quot;,&quot;context&quot;:&quot;product&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;github_models_link_product_navbar&quot;}" href="https://github.com/features/models">
      
      <div>
          <p>
            GitHub Models
              <span>
                New
              </span>
          </p><p>
        Manage and compare prompts
      </p></div>

    
</a></li>

                    <li>
  <a data-analytics-event="{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;github_advanced_security&quot;,&quot;context&quot;:&quot;product&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;github_advanced_security_link_product_navbar&quot;}" href="https://github.com/security/advanced-security">
      
      <div>
          <p>
            GitHub Advanced Security
          </p><p>
        Find and fix vulnerabilities
      </p></div>

    
</a></li>

                    <li>
  <a data-analytics-event="{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;actions&quot;,&quot;context&quot;:&quot;product&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;actions_link_product_navbar&quot;}" href="https://github.com/features/actions">
      
      <div>
          <p>
            Actions
          </p><p>
        Automate any workflow
      </p></div>

    
</a></li>

                    
                </ul>
              </div>
          <div>

                <ul>
                    <li>
  <a data-analytics-event="{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;codespaces&quot;,&quot;context&quot;:&quot;product&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;codespaces_link_product_navbar&quot;}" href="https://github.com/features/codespaces">
      
      <div>
          <p>
            Codespaces
          </p><p>
        Instant dev environments
      </p></div>

    
</a></li>

                    <li>
  <a data-analytics-event="{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;issues&quot;,&quot;context&quot;:&quot;product&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;issues_link_product_navbar&quot;}" href="https://github.com/features/issues">
      
      <div>
          <p>
            Issues
          </p><p>
        Plan and track work
      </p></div>

    
</a></li>

                    <li>
  <a data-analytics-event="{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;code_review&quot;,&quot;context&quot;:&quot;product&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;code_review_link_product_navbar&quot;}" href="https://github.com/features/code-review">
      
      <div>
          <p>
            Code Review
          </p><p>
        Manage code changes
      </p></div>

    
</a></li>

                    <li>
  <a data-analytics-event="{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;discussions&quot;,&quot;context&quot;:&quot;product&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;discussions_link_product_navbar&quot;}" href="https://github.com/features/discussions">
      
      <div>
          <p>
            Discussions
          </p><p>
        Collaborate outside of code
      </p></div>

    
</a></li>

                    <li>
  <a data-analytics-event="{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;code_search&quot;,&quot;context&quot;:&quot;product&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;code_search_link_product_navbar&quot;}" href="https://github.com/features/code-search">
      
      <div>
          <p>
            Code Search
          </p><p>
        Find more, search less
      </p></div>

    
</a></li>

                </ul>
              </div>
          

      </div>
</li>


                <li>
      

      
</li>


                <li>
      

      <div>
                    <p><span id="resources-explore-heading">Explore</span></p><ul aria-labelledby="resources-explore-heading">
                    <li>
  <a target="_blank" data-analytics-event="{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;learning_pathways&quot;,&quot;context&quot;:&quot;resources&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;learning_pathways_link_resources_navbar&quot;}" href="https://resources.github.com/learn/pathways">
      Learning Pathways

    
</a></li>

                    <li>
  <a target="_blank" data-analytics-event="{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;events_amp_webinars&quot;,&quot;context&quot;:&quot;resources&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;events_amp_webinars_link_resources_navbar&quot;}" href="https://resources.github.com/">
      Events &amp; Webinars

    
</a></li>

                    <li>
  <a data-analytics-event="{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;ebooks_amp_whitepapers&quot;,&quot;context&quot;:&quot;resources&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;ebooks_amp_whitepapers_link_resources_navbar&quot;}" href="https://github.com/resources/whitepapers">
      Ebooks &amp; Whitepapers

    
</a></li>

                    <li>
  <a data-analytics-event="{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;customer_stories&quot;,&quot;context&quot;:&quot;resources&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;customer_stories_link_resources_navbar&quot;}" href="https://github.com/customer-stories">
      Customer Stories

    
</a></li>

                    <li>
  <a target="_blank" data-analytics-event="{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;partners&quot;,&quot;context&quot;:&quot;resources&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;partners_link_resources_navbar&quot;}" href="https://partner.github.com/">
      Partners

    
</a></li>

                    <li>
  <a data-analytics-event="{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;executive_insights&quot;,&quot;context&quot;:&quot;resources&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;executive_insights_link_resources_navbar&quot;}" href="https://github.com/solutions/executive-insights">
      Executive Insights

    
</a></li>

                </ul>
              </div>
</li>


                <li>
      

      <div>
              <div>

                <ul>
                    <li>
  <a data-analytics-event="{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;github_sponsors&quot;,&quot;context&quot;:&quot;open_source&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;github_sponsors_link_open_source_navbar&quot;}" href="https://github.com/sponsors">
      
      <div>
          <p>
            GitHub Sponsors
          </p><p>
        Fund open source developers
      </p></div>

    
</a></li>

                </ul>
              </div>
              <div>

                <ul>
                    <li>
  <a data-analytics-event="{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;the_readme_project&quot;,&quot;context&quot;:&quot;open_source&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;the_readme_project_link_open_source_navbar&quot;}" href="https://github.com/readme">
      
      <div>
          <p>
            The ReadME Project
          </p><p>
        GitHub community articles
      </p></div>

    
</a></li>

                </ul>
              </div>
              
          </div>
</li>


                <li>
      

      <div>

                <ul>
                    <li>
  <a data-analytics-event="{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;enterprise_platform&quot;,&quot;context&quot;:&quot;enterprise&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;enterprise_platform_link_enterprise_navbar&quot;}" href="https://github.com/enterprise">
      
      <div>
          <p>
            Enterprise platform
          </p><p>
        AI-powered developer platform
      </p></div>

    
</a></li>

                </ul>
              </div>
</li>


                <li>
    <a data-analytics-event="{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;pricing&quot;,&quot;context&quot;:&quot;global&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;pricing_link_global_navbar&quot;}" href="https://github.com/pricing">Pricing</a>
</li>

            </ul>
          </nav>

        <div>
                


<qbsearch-input data-scope="repo:andrewarrow/starchive" data-custom-scopes-path="/search/custom_scopes" data-delete-custom-scopes-csrf="r3Njjc2VoyfBMS_uM84gIn84LOiQZ9TG9nH3PBMNTyyCiR-ws2-AwBO1t1lxRqB4_zfrwsKjzFr_t3xZX-4IMA" data-max-custom-scopes="10" data-header-redesign-enabled="false" data-initial-value="" data-blackbird-suggestions-path="/search/suggestions" data-jump-to-suggestions-path="/_graphql/GetSuggestedNavigationDestinations" data-current-repository="andrewarrow/starchive" data-current-org="" data-current-owner="andrewarrow" data-logged-in="false" data-copilot-chat-enabled="false" data-nl-search-enabled="false" data-retain-scroll-position="true">
  <div data-modal-dialog-overlay="" data-action="click:qbsearch-input#searchInputContainerClicked">
  <modal-dialog data-action="close:qbsearch-input#handleClose cancel:qbsearch-input#handleClose" data-target="qbsearch-input.searchSuggestionsDialog" role="dialog" id="search-suggestions-dialog" aria-modal="true" aria-labelledby="search-suggestions-dialog-header" data-view-component="true">
      <h2 id="search-suggestions-dialog-header">Search code, repositories, users, issues, pull requests...</h2>
    
</modal-dialog></div>
  
  <div>
    
<dialog-helper>
  <dialog data-target="qbsearch-input.feedbackDialog" data-action="close:qbsearch-input#handleDialogClose cancel:qbsearch-input#handleDialogClose" id="feedback-dialog" aria-modal="true" aria-labelledby="feedback-dialog-title" aria-describedby="feedback-dialog-description" data-view-component="true">
    <div data-view-component="true">
    <p>
      <h2 id="feedback-dialog-title">
        Provide feedback
      </h2>
        
    </p>
    
  </div>
      <scrollable-region data-labelled-by="feedback-dialog-title">
        
      </scrollable-region>
      
</dialog></dialog-helper>

    <custom-scopes data-target="qbsearch-input.customScopesManager">
    
<dialog-helper>
  <dialog data-target="custom-scopes.customScopesModalDialog" data-action="close:qbsearch-input#handleDialogClose cancel:qbsearch-input#handleDialogClose" id="custom-scopes-dialog" aria-modal="true" aria-labelledby="custom-scopes-dialog-title" aria-describedby="custom-scopes-dialog-description" data-view-component="true">
    <div data-view-component="true">
    <p>
      <h2 id="custom-scopes-dialog-title">
        Saved searches
      </h2>
        <h2 id="custom-scopes-dialog-description">Use saved searches to filter your results more quickly</h2>
    </p>
    
  </div>
      <scrollable-region data-labelled-by="custom-scopes-dialog-title">
        
      </scrollable-region>
      
</dialog></dialog-helper>
    </custom-scopes>
  </div>
</qbsearch-input>


            

              <p><a href="https://github.com/signup?ref_cta=Sign+up&amp;ref_loc=header+logged+out&amp;ref_page=%2F%3Cuser-name%3E%2F%3Crepo-name%3E&amp;source=header-repo&amp;source_repo=andrewarrow%2Fstarchive" data-hydro-click="{&quot;event_type&quot;:&quot;authentication.click&quot;,&quot;payload&quot;:{&quot;location_in_page&quot;:&quot;site header menu&quot;,&quot;repository_id&quot;:null,&quot;auth_type&quot;:&quot;SIGN_UP&quot;,&quot;originating_url&quot;:&quot;https://github.com/andrewarrow/starchive&quot;,&quot;user_id&quot;:null}}" data-hydro-click-hmac="923c06f2b7ce1c8e0c23c815fd7ae815629beb47dacde9776070db32835566ae" data-analytics-event="{&quot;category&quot;:&quot;Sign up&quot;,&quot;action&quot;:&quot;click to sign up for account&quot;,&quot;label&quot;:&quot;ref_page:/<user-name>/<repo-name>;ref_cta:Sign up;ref_loc:header logged out&quot;}">
                Sign up
              </a></p><p>
    <react-partial-anchor>
      <tool-tip id="tooltip-83183d26-916f-409b-a61d-9ff07dc08767" for="icon-button-94a14246-0836-446d-91bc-3cfeeb301084" popover="manual" data-direction="s" data-type="label" data-view-component="true">Appearance settings</tool-tip>

      <template data-target="react-partial-anchor.template">
        <link crossorigin="anonymous" media="all" rel="stylesheet" href="https://github.githubassets.com/assets/appearance-settings.76259b61ecc822265749.module.css">

<react-partial partial-name="appearance-settings" data-ssr="false" data-attempted-ssr="false" data-react-profiling="false">
  
  <script type="application/json" data-target="react-partial.embeddedData">{"props":{}}</script>
  <div data-target="react-partial.reactRoot"></div>
</react-partial>

      </template>
    </react-partial-anchor>
  </p>

          </div>
      </div>
</header>

      
    </div>

  








    


    






  <div itemscope="" itemtype="http://schema.org/SoftwareSourceCode" data-commit-hovercards-enabled="" data-discussion-hovercards-enabled="" data-issue-and-pr-hovercards-enabled="" data-project-hovercards-enabled="">
    <main id="js-repo-pjax-container">
      
  





    






  
  

  



<turbo-frame id="repo-content-turbo-frame" target="_top" data-turbo-action="advance">
    <div data-view-component="true" id="repo-content-pjax-container">      


<react-partial partial-name="repos-overview" data-ssr="true" data-attempted-ssr="true" data-react-profiling="false">
  
  
  <div data-hpc="true" data-target="react-partial.reactRoot"><article itemprop="text"><p dir="auto"><h2 tabindex="-1" dir="auto">Starchive</h2><a id="user-content-starchive" aria-label="Permalink: Starchive" href="#starchive"></a></p>
<p dir="auto">A browser extension and local backend system that automatically archives YouTube videos when visited. The system consists of a Firefox extension that detects YouTube video pages and a Go backend that downloads the videos using yt-dlp.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Components</h2><a id="user-content-components" aria-label="Permalink: Components" href="#components"></a></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Backend (Go)</h3><a id="user-content-backend-go" aria-label="Permalink: Backend (Go)" href="#backend-go"></a></p>
<ul dir="auto">
<li><strong>HTTP Server</strong> (<code>main.go</code>): Runs on port 3009 with two endpoints:
<ul dir="auto">
<li><code>/</code>: Basic health check endpoint</li>
<li><code>/youtube</code>: POST endpoint that accepts video IDs and triggers downloads</li>
</ul>
</li>
<li><strong>Video Downloader</strong> (<code>youtube.go</code>): Uses yt-dlp and ffmpeg to download YouTube videos and convert them to MOV format with h264_videotoolbox encoding</li>
<li><strong>Subtitle Support</strong>: Downloads English subtitles in VTT format (currently disabled in retry loop)</li>
</ul>
<p dir="auto"><h3 tabindex="-1" dir="auto">Browser Extension (Firefox)</h3><a id="user-content-browser-extension-firefox" aria-label="Permalink: Browser Extension (Firefox)" href="#browser-extension-firefox"></a></p>
<ul dir="auto">
<li><strong>Manifest</strong> (<code>manifest.json</code>): Defines extension permissions and structure</li>
<li><strong>Content Script</strong> (<code>content.js</code>): Automatically detects YouTube video pages and extracts video IDs from URLs</li>
<li><strong>Background Script</strong> (<code>background.js</code>): Handles communication between content script and backend API</li>
<li><strong>Popup Interface</strong> (<code>popup.html/js</code>): Provides a simple UI for manual data fetching</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">How It Works</h2><a id="user-content-how-it-works" aria-label="Permalink: How It Works" href="#how-it-works"></a></p>
<ol dir="auto">
<li>When you visit a YouTube video page, the content script automatically detects the video ID from the URL</li>
<li>The video ID is sent to the background script, which makes a POST request to <code>http://localhost:3009/youtube</code></li>
<li>The Go backend receives the video ID and uses yt-dlp to download the video</li>
<li>Videos are saved to the <code>./data/</code> directory and converted to MOV format using ffmpeg with hardware acceleration</li>
<li>The system also supports subtitle downloading (though currently limited to 1 attempt)</li>
</ol>
<p dir="auto"><h2 tabindex="-1" dir="auto">Setup</h2><a id="user-content-setup" aria-label="Permalink: Setup" href="#setup"></a></p>
<ol dir="auto">
<li>Start the Go backend: <code>go run .</code></li>
<li>Load the Firefox extension from the <code>firefox/</code> directory</li>
<li>Visit any YouTube video page to automatically trigger archiving</li>
</ol>
<p dir="auto"><h2 tabindex="-1" dir="auto">Dependencies</h2><a id="user-content-dependencies" aria-label="Permalink: Dependencies" href="#dependencies"></a></p>
<ul dir="auto">
<li><strong>yt-dlp</strong>: For downloading YouTube videos</li>
<li><strong>ffmpeg</strong>: For video format conversion and processing</li>
<li><strong>Go</strong>: Backend server runtime</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Other Starchive</h2><a id="user-content-other-starchive" aria-label="Permalink: Other Starchive" href="#other-starchive"></a></p>
<p dir="auto">Not associated with <a href="https://www.starchive.io/" rel="nofollow">https://www.starchive.io/</a></p>
</article></div>
</react-partial>

      </div>

</turbo-frame>


    </main>
  </div>

          



    <ghcc-consent id="ghcc" data-locale="en" data-initial-cookie-consent-allowed="" data-cookie-consent-required="true"></ghcc-consent>



  

    <template id="site-details-dialog">
  <details class="details-reset details-overlay details-overlay-dark lh-default color-fg-default hx_rsm" open="">
    <summary role="button" aria-label="Close dialog"></summary>
    <details-dialog class="Box Box--overlay d-flex flex-column anim-fade-in fast hx_rsm-dialog hx_rsm-modal">
      <button class="Box-btn-octicon m-0 btn-octicon position-absolute right-0 top-0" type="button" aria-label="Close dialog" data-close-dialog="">
        <svg aria-hidden="true" height="16" viewBox="0 0 16 16" version="1.1" width="16" data-view-component="true" class="octicon octicon-x">
    <path d="M3.72 3.72a.75.75 0 0 1 1.06 0L8 6.94l3.22-3.22a.749.749 0 0 1 1.275.326.749.749 0 0 1-.215.734L9.06 8l3.22 3.22a.749.749 0 0 1-.326 1.275.749.749 0 0 1-.734-.215L8 9.06l-3.22 3.22a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042L6.94 8 3.72 4.78a.75.75 0 0 1 0-1.06Z"></path>
</svg>
      </button>
      <div class="octocat-spinner my-6 js-details-dialog-spinner"></div>
    </details-dialog>
  </details>
</template>

    

    <template id="snippet-clipboard-copy-button">
  <div class="zeroclipboard-container position-absolute right-0 top-0">
    <clipboard-copy aria-label="Copy" class="ClipboardButton btn js-clipboard-copy m-2 p-0" data-copy-feedback="Copied!" data-tooltip-direction="w">
      <svg aria-hidden="true" height="16" viewBox="0 0 16 16" version="1.1" width="16" data-view-component="true" class="octicon octicon-copy js-clipboard-copy-icon m-2">
    <path d="M0 6.75C0 5.784.784 5 1.75 5h1.5a.75.75 0 0 1 0 1.5h-1.5a.25.25 0 0 0-.25.25v7.5c0 .138.112.25.25.25h7.5a.25.25 0 0 0 .25-.25v-1.5a.75.75 0 0 1 1.5 0v1.5A1.75 1.75 0 0 1 9.25 16h-7.5A1.75 1.75 0 0 1 0 14.25Z"></path><path d="M5 1.75C5 .784 5.784 0 6.75 0h7.5C15.216 0 16 .784 16 1.75v7.5A1.75 1.75 0 0 1 14.25 11h-7.5A1.75 1.75 0 0 1 5 9.25Zm1.75-.25a.25.25 0 0 0-.25.25v7.5c0 .138.112.25.25.25h7.5a.25.25 0 0 0 .25-.25v-7.5a.25.25 0 0 0-.25-.25Z"></path>
</svg>
      <svg aria-hidden="true" height="16" viewBox="0 0 16 16" version="1.1" width="16" data-view-component="true" class="octicon octicon-check js-clipboard-check-icon color-fg-success d-none m-2">
    <path d="M13.78 4.22a.75.75 0 0 1 0 1.06l-7.25 7.25a.75.75 0 0 1-1.06 0L2.22 9.28a.751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018L6 10.94l6.72-6.72a.75.75 0 0 1 1.06 0Z"></path>
</svg>
    </clipboard-copy>
  </div>
</template>
<template id="snippet-clipboard-copy-button-unpositioned">
  <div class="zeroclipboard-container">
    <clipboard-copy aria-label="Copy" class="ClipboardButton btn btn-invisible js-clipboard-copy m-2 p-0 d-flex flex-justify-center flex-items-center" data-copy-feedback="Copied!" data-tooltip-direction="w">
      <svg aria-hidden="true" height="16" viewBox="0 0 16 16" version="1.1" width="16" data-view-component="true" class="octicon octicon-copy js-clipboard-copy-icon">
    <path d="M0 6.75C0 5.784.784 5 1.75 5h1.5a.75.75 0 0 1 0 1.5h-1.5a.25.25 0 0 0-.25.25v7.5c0 .138.112.25.25.25h7.5a.25.25 0 0 0 .25-.25v-1.5a.75.75 0 0 1 1.5 0v1.5A1.75 1.75 0 0 1 9.25 16h-7.5A1.75 1.75 0 0 1 0 14.25Z"></path><path d="M5 1.75C5 .784 5.784 0 6.75 0h7.5C15.216 0 16 .784 16 1.75v7.5A1.75 1.75 0 0 1 14.25 11h-7.5A1.75 1.75 0 0 1 5 9.25Zm1.75-.25a.25.25 0 0 0-.25.25v7.5c0 .138.112.25.25.25h7.5a.25.25 0 0 0 .25-.25v-7.5a.25.25 0 0 0-.25-.25Z"></path>
</svg>
      <svg aria-hidden="true" height="16" viewBox="0 0 16 16" version="1.1" width="16" data-view-component="true" class="octicon octicon-check js-clipboard-check-icon color-fg-success d-none">
    <path d="M13.78 4.22a.75.75 0 0 1 0 1.06l-7.25 7.25a.75.75 0 0 1-1.06 0L2.22 9.28a.751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018L6 10.94l6.72-6.72a.75.75 0 0 1 1.06 0Z"></path>
</svg>
    </clipboard-copy>
  </div>
</template>




    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Why Exercise Is a Miracle Drug (250 pts)]]></title>
            <link>https://www.derekthompson.org/p/the-sunday-morning-post-why-exercise</link>
            <guid>44768704</guid>
            <pubDate>Sat, 02 Aug 2025 16:02:16 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.derekthompson.org/p/the-sunday-morning-post-why-exercise">https://www.derekthompson.org/p/the-sunday-morning-post-why-exercise</a>, See on <a href="https://news.ycombinator.com/item?id=44768704">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><article><div dir="auto"><p>Welcome back to The Sunday Morning Post, this newsletter’s weekly rundown of the most interesting and important stuff I’m seeing in science, technology, economics, and beyond. Comments are open. Leave tips, papers, studies, tweets, posts, questions, and graphs in the comments, if you think they’ll serve for future editions.</p><p><span>Euan Ashley has </span><a href="https://open.spotify.com/episode/4llcv2QIXnxKeMzaqRZ1dS" rel="">claimed</a><span> that exercise is the “single most potent medical invention” ever—more broadly effective than any medicine discovered in the natural world or devised in a laboratory. In 2025, this is the sort of rah-rah sentiment about working out that one might associate with a Make America Healthy Again ambassador rather than, say, the chair of medicine at Stanford University. So, what makes Ashley’s claim significant is that he </span><em>is</em><span> the chair of medicine at Stanford University.</span></p><p><span>Last year, Ashley and a large team of scientists conducted an elaborate experiment on the effects of exercise on the mammalian body. In one test, Ashley put rats on tiny treadmills, worked them out for weeks, and cut into them to investigate how their organs and vessels responded to the workout compared to a control group of more sedentary rodents.</span><span><a data-component-name="FootnoteAnchorToDOM" id="footnote-anchor-1-168158155" href="https://www.derekthompson.org/p/the-sunday-morning-post-why-exercise#footnote-1-168158155" target="_self" rel="">1</a></span><span> The results were spectacular. Exercise transformed just about every tissue and molecular system that Ashley and his co-authors studied—not just the muscles and heart, but also the liver, adrenal glands, fat, and immune system. </span></p><p>When I asked Ashley if it was possible to design a drug that mimicked the observed effects of exercise, he was emphatic that, no, this was not possible. The benefits of exercise seem too broad for any one therapy to mimic. To a best approximation, aerobic fitness and weight-training seem to increase our metabolism, improve mitochondrial function, fortify our immune system, reduce inflammation, improve tissue-specific adaptations, and protect against disease.</p><p><span>The latest entry in the Exercise Is Magic file comes from </span><a href="https://www.nejm.org/do/10.1056/NEJMdo008039/full/" rel="">the New England Journal of Medicine</a><span>. In a recent study, 900 cancer patients who had undergone surgery on their advanced colon cancer were randomly assigned to two groups. One group got a “structured exercise program.” They went to behavioral support sessions and attended supervised exercise classes every few weeks for several years. The other group received only basic information about diet and health.</span></p><p>Compared to the control group, the exercise group saw “significantly” more years without cancer, a 7 percentage point increase in the overall survival rate after 8 years, and a dramatic reduction in new primary cancers. Exercise, it seems, doesn’t just prevent disease; it can also save your life after you get sick.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/$s_!t4il!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffa837333-1baa-463a-9b82-2a0422978694_1282x1254.jpeg" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!t4il!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffa837333-1baa-463a-9b82-2a0422978694_1282x1254.jpeg 424w, https://substackcdn.com/image/fetch/$s_!t4il!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffa837333-1baa-463a-9b82-2a0422978694_1282x1254.jpeg 848w, https://substackcdn.com/image/fetch/$s_!t4il!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffa837333-1baa-463a-9b82-2a0422978694_1282x1254.jpeg 1272w, https://substackcdn.com/image/fetch/$s_!t4il!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffa837333-1baa-463a-9b82-2a0422978694_1282x1254.jpeg 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/$s_!t4il!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffa837333-1baa-463a-9b82-2a0422978694_1282x1254.jpeg" width="1282" height="1254" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/fa837333-1baa-463a-9b82-2a0422978694_1282x1254.jpeg&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:1254,&quot;width&quot;:1282,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:128015,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/jpeg&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://www.derekthompson.org/i/168158155?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffa837333-1baa-463a-9b82-2a0422978694_1282x1254.jpeg&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/$s_!t4il!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffa837333-1baa-463a-9b82-2a0422978694_1282x1254.jpeg 424w, https://substackcdn.com/image/fetch/$s_!t4il!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffa837333-1baa-463a-9b82-2a0422978694_1282x1254.jpeg 848w, https://substackcdn.com/image/fetch/$s_!t4il!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffa837333-1baa-463a-9b82-2a0422978694_1282x1254.jpeg 1272w, https://substackcdn.com/image/fetch/$s_!t4il!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffa837333-1baa-463a-9b82-2a0422978694_1282x1254.jpeg 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p><span>The author Daniel Lieberman </span><a href="https://www.amazon.com/Exercised-Something-Evolved-Healthy-Rewarding/dp/052543478X/ref=asc_df_052543478X?mcid=1734c528e04d3fb5b8f542ded7df910a&amp;hvocijid=17701023827466789008-052543478X-&amp;hvexpln=73&amp;tag=hyprod-20&amp;linkCode=df0&amp;hvadid=721245378154&amp;hvpos=&amp;hvnetw=g&amp;hvrand=17701023827466789008&amp;hvpone=&amp;hvptwo=&amp;hvqmt=&amp;hvdev=c&amp;hvdvcmdl=&amp;hvlocint=&amp;hvlocphy=9061285&amp;hvtargid=pla-2281435177818&amp;psc=1" rel="">has put it well</a><span>: Exercise is healthy and rewarding even though it’s something we never evolved to do. To adapt to the physical ease of the modern world, people invented a variety of weight-resistant devices and bodily movements that allow today’s population to simulate the arduous tasks that were once necessary to make it through a life, and this strange pantomime of physical stress seems to do more for us at a molecular level than any therapy or intervention in the history of medicine.</span><span><a data-component-name="FootnoteAnchorToDOM" id="footnote-anchor-2-168158155" href="https://www.derekthompson.org/p/the-sunday-morning-post-why-exercise#footnote-2-168158155" target="_self" rel="">2</a></span><span> </span></p><p>Soon after overseeing cuts to more than 80 percent of programs at the United States Agency for International Development, Secretary of State Marco Rubio said in remarks in July that USAID “has little to show since the end of the Cold War.” In the field of global health, this analysis may be off by about 100 million.</p><p><span>In June, The Lancet </span><a href="https://www.thelancet.com/journals/lancet/article/PIIS0140-6736(25)01186-9/fulltext" rel="">published</a><span> an evaluation of 20 years of USAID funding by a team of global researchers from Brazil, Spain, Mozambique, and the United States. They calculated that US global health and development spending has spared about 90 million deaths in low-income countries, including:</span></p><ul><li><p>25 million lives saved from HIV/AIDS</p></li><li><p>11 million from diarrheal disease</p></li><li><p>9 million from lower respiratory infections</p></li><li><p>9 million from "neglected" tropical diseases, such as dengue and river blindness</p></li><li><p>8 million from malaria</p></li><li><p>5 million from tuberculosis</p></li><li><p>2 million from nutritional deficiencies</p></li></ul><p>All this was achieved with a program that accounts for about 0.8 percent of the federal budget and about 1/400th of America’s total national spending. This is a staggering return on moral investment.</p><p>When I think about the case for global health spending, I think about the first days of my life. Nobody chooses how they come into the world. I achieved nothing to earn my birth in a hospital in Washington, D.C., as opposed to a clinic in Maputo or Kinshasa, or to deserve the automatic guarantee of American citizenship upon my first breath of air. That I was born in the capital of the world’s richest country is one of the greatest strokes of luck in my life—a pure accident of timing and gametes. There is no way to pay back this good fortune, and wallowing in guilt over it would do nothing, either. The quiet miracle of charity and global aid is that the uneven distribution of global wealth creates an asymmetry by which relatively trivial amounts of money from the rich can prevent immense suffering and death among the poor. Interventions as simple as bed nets, antiretroviral therapies for HIV/AIDS, and the distribution of commonplace vaccines and therapies in impoverished rural areas can save an astonishing number of lives, while costing a rich country an amount of money that makes practically no difference to any their citizen’s day-to-day. </p><p>Americans are blessed to be in possession of a kind of sorcerer’s stone—a bit of policy alchemy that can transform one-four hundredth of our spending into 100 million saved lives, in less than half a century. I think we should use it.</p></div></article></div><div id="discussion"><h4>Discussion about this post</h4></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Hiding secret codes in light protects against fake videos (111 pts)]]></title>
            <link>https://news.cornell.edu/stories/2025/07/hiding-secret-codes-light-protects-against-fake-videos</link>
            <guid>44768513</guid>
            <pubDate>Sat, 02 Aug 2025 15:40:27 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://news.cornell.edu/stories/2025/07/hiding-secret-codes-light-protects-against-fake-videos">https://news.cornell.edu/stories/2025/07/hiding-secret-codes-light-protects-against-fake-videos</a>, See on <a href="https://news.ycombinator.com/item?id=44768513">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p><span>Fact-checkers may have a new tool in the fight against misinformation.</span></p><p><span>A team of Cornell researchers has developed a way to “watermark” light in videos, which they can use to detect if video is fake or has been manipulated.</span></p><p><span>The idea is to hide information in&nbsp;</span><span lang="EN">nearly-invisible fluctuations&nbsp;</span><span>of lighting at important events and locations, such as interviews and press conferences or even entire buildings, like the United Nations Headquarters. These fluctuations are designed to go unnoticed by humans, but are recorded as a hidden watermark in any video captured under the special lighting, which could be programmed into computer screens, photography lamps and built-in lighting. Each watermarked light source has a secret code that can be used to check for the corresponding watermark in the video and reveal any malicious editing.</span></p><p><span>Peter Michael, a graduate student in the field of computer science who led the work, will present the study, “</span><a href="https://dl.acm.org/doi/10.1145/3742892"><span lang="EN-US">Noise-Coded Illumination for Forensic and Photometric Video</span></a><span>,” on Aug. 10 at SIGGRAPH 2025 in Vancouver, British Columbia.</span></p><p><span>Editing video footage in a misleading way is nothing new. But with generative AI and social media, it is faster and easier to spread misinformation than ever before.</span></p><div data-entity-type="media_embed" data-entity-uuid="81e96f5b-00ee-4079-90cc-e141e2af36fe" data-embed-button="image_embed" data-entity-embed-display="view_mode:media_embed.image_caption" data-langcode="en" data-entity-embed-display-settings="[]"><figure>
  


            <figcaption><p>Davis and Michael at work in Davis’ Gates Hall lab.</p>
</figcaption>
      
</figure>
</div>
<p><span>“Video used to be treated as a source of truth, but that’s no longer an assumption we can make,” said&nbsp;</span><a href="https://www.cs.cornell.edu/abe/group/"><span lang="EN-US">Abe Davis</span></a><span>, assistant professor of computer science in the Cornell Ann S. Bowers College of Computing and Information Science, who first conceived of the idea. “Now you can pretty much create video of whatever you want. That can be fun, but also problematic, because it’s only getting harder to tell what’s real.”</span></p><p><span>To address these concerns, researchers had previously designed techniques to watermark digital video files directly, with tiny changes to specific pixels that can be used to identify unmanipulated footage or tell if a video was created by AI. However, these approaches depend on the video creator using a specific camera or AI model – a level of compliance that may be unrealistic to expect from potential bad actors.</span></p><p><span>By embedding the code in the lighting, the new method ensures that any real video of the subject contains the secret watermark, regardless of who captured it. The team showed that programmable light sources, like computer screens and certain types of room lighting, can be coded with a small piece of software, while older lights, like many off-the-shelf lamps, can be coded by attaching a small computer chip about the size of a postage stamp. The program on the chip varies the brightness of the light according to the secret code.</span></p><p><span>So, what secret information is hidden in these watermarks, and how does it reveal when video is fake? “Each watermark carries a low-fidelity time-stamped version of the unmanipulated video under slightly different lighting. We call these code videos,” Davis said. “When someone manipulates a video, the manipulated parts start to contradict what we see in these code videos, which lets us see where changes were made. And if someone tries to generate fake video with AI, the resulting code videos just look like random variations.”</span></p><p><span lang="EN">Part of the challenge in this work was getting the code&nbsp;to be&nbsp;largely imperceptible to humans.</span><span> “We used studies from human perception literature to inform our design of the coded light,” Michael said. “The code is also designed to look like random variations that already occur in light called ‘noise,’ which also makes it difficult to detect, unless you know the secret code.”</span></p><div data-entity-type="media_embed" data-entity-uuid="ed8d7c0a-42c7-49af-8f46-fa60a607228a" data-embed-button="video_embed" data-entity-embed-display="view_mode:media_embed.image_caption" data-langcode="en" data-entity-embed-display-settings="[]"><figure>
  
  

            <figcaption><p>A video explaining Noise-Coded Illumination.</p>
</figcaption>
      
</figure>
</div>
<p><span>If an adversary cuts out footage, such as from an interview or political speech, a forensic analyst with the secret code can see the gaps. And if the adversary adds or replaces objects, the altered parts generally appear black in recovered code videos.</span></p><p><span>The team has successfully used up to three separate codes for different lights in the same scene. With each additional code, the patterns become more complicated and harder to fake.</span></p><p><span>“Even if an adversary knows the technique is being used and somehow figures out the codes, their job is still a lot harder,” Davis said. “Instead of faking the light for just one video, they have to fake each code video separately, and all those fakes have to agree with each other.”</span></p><p><span>They have also verified that this approach works in some outdoor settings and on people with different skin tones.</span></p><p><span>Davis and Michael caution, however, that the fight against misinformation is an arms race, and adversaries will continue to devise new ways to deceive.</span></p><p><span>“This is an important ongoing problem,” Davis said. “It’s not going to go away, and in fact, it’s only going to harder.”</span></p><p><span>Zekun Hao, Ph.D. ’23, and Serge Belongie of the University of Copenhagen are co-authors on the study.</span></p><p><span>The work received partial support from a National Defense Science and Engineering Graduate Fellowship and the Pioneer Centre for AI.</span></p><p><em><span>Patricia Waldron is a writer for the Cornell Ann S. Bowers College of Computing and Information Science.</span></em></p></div></div>]]></description>
        </item>
    </channel>
</rss>