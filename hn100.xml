<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0">
    <channel>
        <title>HN100 - Readable Contents</title>
        <link>https://hn.algolia.com/api/v1/search_by_date?tags=%28story,poll%29&amp;numericFilters=points%3E100</link>
        <description>Uses Readability to add bodies to the RSS feed</description>
        <lastBuildDate>Wed, 04 Jun 2025 16:30:02 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>en</language>
        <item>
            <title><![CDATA["AI Will Replace All the Jobs " Is Just Tech Execs Doing Marketing (130 pts)]]></title>
            <link>https://sparktoro.com/blog/ai-will-replace-all-the-jobs-is-just-tech-execs-doing-marketing/</link>
            <guid>44181172</guid>
            <pubDate>Wed, 04 Jun 2025 14:38:22 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://sparktoro.com/blog/ai-will-replace-all-the-jobs-is-just-tech-execs-doing-marketing/">https://sparktoro.com/blog/ai-will-replace-all-the-jobs-is-just-tech-execs-doing-marketing/</a>, See on <a href="https://news.ycombinator.com/item?id=44181172">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>

		
<p>Over the weekend, I went digging for evidence that AI can, will, or has replaced a large percent of jobs. It doesn‚Äôt exist. Worse than that, actually, there‚Äôs hundreds of years of evidence and sophisticated analyses from hundreds of sources showing the opposite is true: AI will almost certainly create more jobs than it displaces, just like thousands of remarkable technologies before it.</p>


<div>
<figure data-wp-context="{&quot;imageId&quot;:&quot;68405d5f71052&quot;}" data-wp-interactive="core/image"><img fetchpriority="high" decoding="async" width="1024" height="576" data-wp-class--hide="state.isContentHidden" data-wp-class--show="state.isContentVisible" data-wp-init="callbacks.setButtonStyles" data-wp-on-async--click="actions.showLightbox" data-wp-on-async--load="callbacks.setButtonStyles" data-wp-on-async-window--resize="callbacks.setButtonStyles" src="https://images.sparktoro.com/blog/wp-content/uploads/2025/06/ai-will-take-the-jobs-fear-hype-cycle-1024x576.png" alt="" srcset="https://images.sparktoro.com/blog/wp-content/uploads/2025/06/ai-will-take-the-jobs-fear-hype-cycle-1024x576.png 1024w, https://images.sparktoro.com/blog/wp-content/uploads/2025/06/ai-will-take-the-jobs-fear-hype-cycle-300x169.png 300w, https://images.sparktoro.com/blog/wp-content/uploads/2025/06/ai-will-take-the-jobs-fear-hype-cycle-768x432.png 768w, https://images.sparktoro.com/blog/wp-content/uploads/2025/06/ai-will-take-the-jobs-fear-hype-cycle.png 1280w" sizes="(max-width: 1024px) 100vw, 1024px"></figure></div>


<p>I don‚Äôt want anyone to think I‚Äôm raining on this parade without first attempting to convince myself that the opposite was true, and that AI really would be the first technology in 120 years to displace a massive portion of the workforce. So, dry though it may be, let‚Äôs walk through the logic together.</p>



<p>The majority of statements that have received press (and there have been dozens in the last 5 years) center on the claim that AI will destroy 20-50% of the current need for human labor. I‚Äôll attempt to address each of the most robust points inherent in those arguments, rather than trying to argue that any one innovation or upgrade to a model‚Äôs capability hasn‚Äôt done it yet (or won‚Äôt):</p>



<ul>
<li>If AI is going to make such a huge percentage of jobs redundant, there must be historical analogies‚Äìi.e. other technologies that massively upended labor markets. What are these and how have they affected jobs in the past?
<ul>
<li><a href="https://www.technologyreview.com/2024/01/27/1087041/technological-unemployment-elon-musk-jobs-ai/">MIT‚Äôs Technology Review</a> noted that this ‚Äúfear of new tech taking jobs,‚Äù is far from new. The automation of farm work is the most notable and most labor-impacting example we have from history, rapidly unemploying a huge portion of human beings in the developing economies of the late 19th and 20th centuries. And yet, at the conclusion of this era (~1940s/50s), the conclusion was that ‚Äútechnological unemployment is a myth,‚Äù because ‚Äútechnology has created so many new industries‚Äù and has expanded the market by ‚Äúlowering the cost of production to make a price within reach of large masses of purchasers.‚Äù In short, technological advances had created more jobs overall.</li>



<li>Last year, Quarterly Journal of Economics <a href="https://academic.oup.com/qje/advance-article/doi/10.1093/qje/qjae008/7630187">published</a> a groundbreaking study on how technological innovations have impacted labor forces across industries since 1980. MIT did a nice <a href="https://news.mit.edu/2024/does-technology-help-or-hurt-employment-0401">summarization</a>: ‚Äúthe number of studies that support the labour replacement effect is more than offset by the number of studies that support the labour-creating/reinstating and real income effects.‚Äù</li>



<li><a href="https://www.sciencedirect.com/science/article/pii/S0040162523004353">This 2023 paper</a> looked at 127 previous studies of technology supposedly replacing labor forces from the 18th century to the present, concluding that ‚Äúthe labor displacing effect of technology appears to be more than offset by compensating mechanisms that create or reinstate labor.‚Äù</li>



<li>The Economic Policy Institute <a href="https://www.epi.org/publication/ai-unbalanced-labor-markets/">did a deep dive</a> into what drives labor market demand and unemployment, concluding: ‚ÄúProductivity growth (which technology sometimes enables and other times drives) has not historically been associated with higher unemployment or higher inequality,‚Äù and that ‚ÄúAnxieties over widespread technology-driven unemployment lack an empirical base.‚Äù</li>



<li>Perhaps the closest analogy to AI is the personal computer revolution of the 1980s. Millions of jobs in communication, documentation, research, analysis, and engineering became obsolete within a decade, and yet, the <a href="https://www.mckinsey.com/featured-insights/future-of-work/what-can-history-teach-us-about-technology-and-jobs">McKinsey Global Institute concluded</a> in 2018 that ‚ÄúWe tallied up all the jobs destroyed in the US since 1980 as a result of the rise of personal computing and the Internet, and it‚Äôs about 3.5 million,‚Äù but ‚ÄúWhen we add up all the jobs created, we find that over 19 million jobs have been created as a result of the personal computer and Internet. We see a net gain of 15.8 million jobs in the US over the last few decades. That‚Äôs about 10 percent of the civilian labor force today.‚Äù</li>
</ul>
</li>



<li>If AI is going to have these massive impacts but hasn‚Äôt yet, why not?
<ul>
<li>Folks who claim AI will destroy the labor market have claimed this radical change is ‚Äúonly a few years away,‚Äù ‚Äúon the immediate horizon,‚Äù or ‚Äúimminent,‚Äù for the last 5 years, yet we‚Äôre at historically low unemployment (yes, even accounting for <a href="https://www.fastcompany.com/91341084/functional-unemployment-what-it-means">underemployment</a> and the <a href="https://www.marketplace.org/story/2025/05/23/is-the-unemployment-rate-truly-capturing-whats-happening-in-the-labor-market">way the BLS counts employment</a>). The US labor market is within a single percentage point of its post-war unemployment low, measured in <a href="https://econofact.org/factbrief/did-us-unemployment-fall-to-the-lowest-rate-in-50-years-under-biden">1953 at 3.4%</a>.</li>
</ul>
</li>
</ul>


<div>
<figure><img decoding="async" width="1024" height="744" src="https://images.sparktoro.com/blog/wp-content/uploads/2025/06/b54eb6431c21256713ce6cf3a9d37a6b-1-1024x744.png" alt="" srcset="https://images.sparktoro.com/blog/wp-content/uploads/2025/06/b54eb6431c21256713ce6cf3a9d37a6b-1-1024x744.png 1024w, https://images.sparktoro.com/blog/wp-content/uploads/2025/06/b54eb6431c21256713ce6cf3a9d37a6b-1-300x218.png 300w, https://images.sparktoro.com/blog/wp-content/uploads/2025/06/b54eb6431c21256713ce6cf3a9d37a6b-1-768x558.png 768w, https://images.sparktoro.com/blog/wp-content/uploads/2025/06/b54eb6431c21256713ce6cf3a9d37a6b-1-1536x1116.png 1536w, https://images.sparktoro.com/blog/wp-content/uploads/2025/06/b54eb6431c21256713ce6cf3a9d37a6b-1-2048x1488.png 2048w" sizes="(max-width: 1024px) 100vw, 1024px"></figure></div>


<ul>
<li>If AI is killing jobs, it‚Äôs doing so at an imperceptibly slow rate; why could that be? Is it still too early? Did other technologies take a long time to show their impacts on labor markets?
<ul>
<li>The broad consensus from rational industry observers, analysts, economists, and even AI-hyped technologists is that the end of cheap money (i.e. higher US interest rates) <a href="https://www.signalfire.com/blog/signalfire-state-of-talent-report-2025">has driven</a> <a href="https://www.theatlantic.com/economy/archive/2025/04/job-market-youth/682641/?gift=o6MjJQpusU9ebnFuymVdsJ1qwI70CnAkjDXBfrYqvHw&amp;utm_source=copy-link&amp;utm_medium=social&amp;utm_campaign=share">most of</a> <a href="https://news.ycombinator.com/item?id=43612448">the</a> <a href="https://www.zdnet.com/article/is-ai-making-it-harder-for-new-college-grads-to-get-hired-in-tech/">lower-than-pre-pandemic-demand</a> for <a href="https://www.techtarget.com/whatis/feature/Tech-sector-layoffs-explained-What-you-need-to-know">entry-level talent</a> (just as it has in times of inflation-fighting interest hikes of the past).</li>



<li>Machine-learning, the technology underpinning AI, <a href="https://courses.cs.washington.edu/courses/cse490h1/19wi/exhibit/machine-learning-1.html">has been around for decades</a>, with <a href="https://www.dataversity.net/a-brief-history-of-machine-learning/">widespread adoption</a> in tech companies between 2006-2013. The current generative-AI era, based on the transformer architecture model, kicked off in 2017, with significant public examples and tech adoption from 2018-2020. Most of the current, press-driven AI hype cycle, however, skyrocketed in late 2021 with OpenAI‚Äôs release of GPT-3 (longtime readers here will recall that <a href="https://britneymuller.com/">Britney Muller</a> showed off techniques <a href="https://www.linkedin.com/posts/britneymuller_bert-101-state-of-the-art-nlp-model-explained-activity-6907013731067523072-OXsd/">extremely similar</a> to what‚Äôs now associated with modern LLMs <a href="https://www.youtube.com/watch?v=47KHP3k2RPo">back in July 2018</a>).</li>



<li>We‚Äôve had <a href="https://courses.cs.washington.edu/courses/cse490h1/19wi/exhibit/machine-learning-1.html">15-20 years</a> of robust machine learning development and adoption, and another <a href="https://toloka.ai/blog/history-of-generative-ai/">5-10 years of broad LLM/generative AI adoption</a>, improvement, and usage, yet labor market fluctuation has been far more dependent on other factors: the Covid pandemic itself, the post-pandemic surge and decline in tech hiring, inflation-fighting tactics by government banks, and (most recently), a renewal of early-20th-century-style tariffs and trade wars. When controlling for these events.</li>



<li>The effects of previous technological advancements also took time, but the most salient examples (of farm equipment in the 1910-1920 era and the personal computer in the 1980s) showed millions of displaced workers <a href="https://faculty.econ.ucdavis.edu/faculty/alolmstead/Recent_Publications/Reshaping_the_Landscape.pdf">within 5 years</a>. AI‚Äôs slower changes bode poorly for the argument that it will have a larger impact than those events.</li>



<li>Even if one assumes that AI was the only contributor to labor market changes between 2021-2025, the change has been incredibly slight, *even* in the software engineering market where it supposedly has the greatest impact. There <a href="https://www.reddit.com/r/programming/comments/1di8pe9/us_employment_of_software_developers_is_in/">was a greater loss</a> (nearly 150%) in percentage of software engineering jobs between 2019-2021 than from 2021-2025.</li>



<li>I found it particularly revealing that one of the most commonly cited examples of AI killing labor needs in the software field is the death of StackOverflow, and yet, a <a href="https://www.infoworld.com/article/3993482/ai-didnt-kill-stack-overflow.html">robust analysis of that site‚Äôs usage from 2008-2020</a> shows that ‚ÄúWhat really happened is a parable of human community and experiments in self-governance gone bizarrely wrong.‚Äù</li>
</ul>
</li>



<li>However, it seems likely that the perception of AI and its adoption are slowing hiring in the software engineering market in the post-bubble-popping era (2024-25). This thoughtful analysis by <a href="https://substack.com/@pragmaticengineer">Gergely Orosz</a> concludes a <a href="https://newsletter.pragmaticengineer.com/p/software-engineering-job-openings">well-visualized, data-driven walkthrough</a> with: ‚ÄúLLMs are a leading cause of the fall in software developer job postings: there‚Äôs uncertainty at large companies about whether to hire as fast as previously, given the productivity hype around AI tooling, and businesses are opting to ‚Äúwait and see‚Äù by slowing down recruitment, as a result.‚Äù</li>



<li>It strains credibility to look at the data, history, and analyses and conclude that AI will eventually kill 20-50% of all jobs, when its largest impact in the prior 5-20 years of adoption (depending on one‚Äôs starting point) is <a href="https://archive.ph/t8f9X">~10% variation in a job sector that employs ~1% of US workers</a>.</li>



<li>Assuming AI will have an effect similar to 20th Century farm equipment‚Äôs on agriculture, why will that labor force behave differently to their 20th Century counterparts (and either refuse to or be prevented from finding new jobs)?
<ul>
<li>This point is hard to find citations for, given that it‚Äôs a future-looking, theoretical assertion. We can, however, compare the impact of the tractor (and farm machinery more broadly) on the economy from 1910-1960.</li>



<li>Tractors and farm equipment resulted in the shutdown of a huge number of farms, and a decline in the number of people employed in farming, from ~33% to ~2% of the labor force (notably, even that massive upheaval was less significant than the <a href="https://www.google.com/search?q=ai+will+take+half+of+all+jobs&amp;tbm=nws">prognostications</a> by tech company leaders that AI will displace half of all jobs). Nothing like it has happened in the American economy since, and only the industrial revolution of the 18th/19th centuries can compete in scale of transformation.</li>



<li>A superb breakdown of farm machinery‚Äôs impact on a sector that employed more than a quarter of all Americans comes from <a href="https://faculty.econ.ucdavis.edu/faculty/alolmstead/Recent_Publications/Reshaping_the_Landscape.pdf">Olmstead and Rhode at UC Davis</a>:</li>
</ul>
</li>
</ul>


<div>
<figure><img decoding="async" width="657" height="161" src="https://images.sparktoro.com/blog/wp-content/uploads/2025/06/image.png" alt="The upshot is that by 1960 the tractor reduced annual labor use by at least 3.44 billion man-hours of field and chore labor from the level required using the horse power technology. (The 3.44 billion figure combines the USDA estimate of annual labor savings before 1944 and our lower-bound estimate of saving between 1944 and 1959.) This was the equivalent of approximately 1,720 thousand workers, which represented 24.3 percent of farm employment in 1960 and 27.3 percent of the decline since 1910." srcset="https://images.sparktoro.com/blog/wp-content/uploads/2025/06/image.png 657w, https://images.sparktoro.com/blog/wp-content/uploads/2025/06/image-300x74.png 300w" sizes="(max-width: 657px) 100vw, 657px"></figure></div>


<ul>
<li>Is it possible that AI will do to broad sectors of the economy what mechanized farm equipment did to agriculture?
<ul>
<li>Rationally, it‚Äôs difficult to fathom generative AI having a greater economic and labor-force impact than the PC revolution of the 1980s. AI makes many tasks more efficient, but evidence that it can wholesale replace entire human functions in a tractor-like way is pure speculation that exists in imagination, not reality.</li>



<li>The core assertion by the ‚ÄúAI will replace 20-50% of all jobs‚Äù crowd seems to be that the past 20 years of machine learning and generative AI improvements are not indicative of what will happen in the future: a leap in capability that will enable company management to instruct an AI on a job function (‚Äúget us press,‚Äù or ‚Äúoptimize our marketing campaign,‚Äù or ‚Äúrecord and audit our financials‚Äù ) and rely on machines to correctly determine what needs to be done, how to do it, and then complete all associated tasks with little to no human supervision, intervention, or additional labor.</li>



<li>It‚Äôs impossible to argue against the assertion that AI will do what‚Äôs described above, because it‚Äôs based not on objective data, but rather on subjective belief about a possible future. Fighting about what someone believes may come about in the future is generally non-productive, so I‚Äôll avoid that to spare us all a lot of wasted time üòâ</li>
</ul>
</li>
</ul>



<p>I‚Äôll move on from the dry argument analysis and citation process and attempt to summarize (and opine on) what‚Äôs really going on here.</p>



<p>Leaders of AI companies, and some AI proponents, marketers, journalists, and even critics have found that when they make scary predictions about their field destroying the job market, press and media eat it up. This media coverage, because it‚Äôs scary and the AI hype cycle is in full swing, draws clicks. Those clicks lead to employees, managers, and leaders at other businesses being scared into learning and adopting AI in their businesses.</p>



<p>Incentive also exists for those who criticize AI, AI companies, or their ethics/models/practices: these folks also benefit directly from the attention they earn when they amplify the message of AI as a job destroying technology.</p>



<p>If you‚Äôre feeling like the ‚ÄúAI will take all our jobs‚Äù discussion is familiar, you‚Äôre in good company. Many others have pointed out the similarities to stories like:</p>


<div>
<figure><a href="https://www.jalopnik.com/elon-musk-tesla-self-driving-cars-anniversary-autopilot-1850432357/"><img loading="lazy" decoding="async" width="816" height="557" src="https://images.sparktoro.com/blog/wp-content/uploads/2025/06/image-1.png" alt="" srcset="https://images.sparktoro.com/blog/wp-content/uploads/2025/06/image-1.png 816w, https://images.sparktoro.com/blog/wp-content/uploads/2025/06/image-1-300x205.png 300w, https://images.sparktoro.com/blog/wp-content/uploads/2025/06/image-1-768x524.png 768w" sizes="auto, (max-width: 816px) 100vw, 816px"></a></figure></div>


<p>Source: <a href="https://www.jalopnik.com/elon-musk-tesla-self-driving-cars-anniversary-autopilot-1850432357/">Jalopnik</a></p>


<div>
<figure><img loading="lazy" decoding="async" width="786" height="631" src="https://images.sparktoro.com/blog/wp-content/uploads/2025/06/image-2.png" alt="" srcset="https://images.sparktoro.com/blog/wp-content/uploads/2025/06/image-2.png 786w, https://images.sparktoro.com/blog/wp-content/uploads/2025/06/image-2-300x241.png 300w, https://images.sparktoro.com/blog/wp-content/uploads/2025/06/image-2-768x617.png 768w" sizes="auto, (max-width: 786px) 100vw, 786px"></figure></div>


<p>Source: <a href="https://www.insidehook.com/culture/older-generations-kids-too-soft">InsideHook</a></p>


<div>
<figure><img loading="lazy" decoding="async" width="767" height="816" src="https://images.sparktoro.com/blog/wp-content/uploads/2025/06/image-3.png" alt="" srcset="https://images.sparktoro.com/blog/wp-content/uploads/2025/06/image-3.png 767w, https://images.sparktoro.com/blog/wp-content/uploads/2025/06/image-3-282x300.png 282w" sizes="auto, (max-width: 767px) 100vw, 767px"></figure></div>


<p>Source: <a href="https://www.honestjobs.com/post/nobody-wants-to-work-anymore-is-not-new-and-it-s-not-true">Honest Jobs</a></p>



<p>Mechanization really did take jobs from farm workers. Automation took jobs from manual laborers. The PC took jobs from clerical and communication workers. But, all of these resulted in greater productivity, employment, and more optionality for workers. It‚Äôs both anti-historic and anti-evidence that AI will somehow prove to be the exception.</p>



<p>Could AI, along with thousands of other impactful technological, political, social, demographic, and black-swan-event changes permanently alter the employment landscape in our lifetimes? Absolutely. In fact, one of my favorite stats from this overly-ambitious weekend of research was MIT‚Äôs estimation that <a href="https://www.nber.org/papers/w30389">60% of employment in 2018 was in types of jobs that didn‚Äôt exist before 1940</a>.</p>



<p>By the time I‚Äôm in my 80s, y‚Äôall better have destroyed more than half of all the existing jobs, and that‚Äôs just to keep up with the 20th Century‚Äôs pace of change. But, don‚Äôt expect AI to do it for you in the next decade; that‚Äôs just marketing.</p>



<p>p.s. If you‚Äôre looking for the TL;DR, <a href="https://bsky.app/profile/edzitron.com/post/3lqaxozwxfc2o">Ed Zitron on Bluesky</a> has got you:</p>


<div>
<figure><a href="https://bsky.app/profile/edzitron.com/post/3lqaxozwxfc2o" target="_blank" rel=" noreferrer noopener"><img loading="lazy" decoding="async" width="710" height="736" src="https://images.sparktoro.com/blog/wp-content/uploads/2025/06/image-4.png" alt="" srcset="https://images.sparktoro.com/blog/wp-content/uploads/2025/06/image-4.png 710w, https://images.sparktoro.com/blog/wp-content/uploads/2025/06/image-4-289x300.png 289w" sizes="auto, (max-width: 710px) 100vw, 710px"></a></figure></div>


<p>p.p.s. I agree there‚Äôs evidence that this fear-based marketing campaign has been successful enough to disrupt some hiring, especially for <a href="https://www.reddit.com/r/Futurology/comments/1l100p3/ai_is_breaking_entrylevel_jobs_that_gen_z_workers/">early-stage jobs in a few tech-heavy fields</a>. But squinting at the evidence, it‚Äôs &lt;0.1% of jobs (&lt;200,000 total) being affected, and even here, the unbalanced capital vs. labor market is <a href="https://www.epi.org/publication/ai-unbalanced-labor-markets/">a far more compelling explanation</a>.</p>

		
	</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Why I Wrote the BEAM Book (219 pts)]]></title>
            <link>https://happihacking.com/blog/posts/2025/why_I_wrote_theBEAMBook/</link>
            <guid>44179257</guid>
            <pubDate>Wed, 04 Jun 2025 10:36:49 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://happihacking.com/blog/posts/2025/why_I_wrote_theBEAMBook/">https://happihacking.com/blog/posts/2025/why_I_wrote_theBEAMBook/</a>, See on <a href="https://news.ycombinator.com/item?id=44179257">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
	
	<h3> Post-mortems, coffee, and a decade of stubborn curiosity </h3><p>

	Posted:  2025-06-03</p>

	<h2>Why I wrote the Beam Book</h2>
<p>After ten years of keeping Klarna‚Äôs core system upright I know this: a 15
millisecond pause in the BEAM can stall millions of peak-shopping payments, trigger a 3 a.m. Christmas-Eve post-mortem, and earn you a very awake call from the CEO. I wrote <em>The BEAM Book</em> so the next engineer fixes that pause before the coffee cools.</p>
<p><img src="https://happihacking.com/images/thebeambooks.jpg" alt="A picture of two printed BEAM Books."></p><h3>Origins</h3>
<p>I opened the project on 12 October 2012 with a lone DocBook file with four lines of text and an oversized sense of optimism.
After two weeks, the commit log is mostly me adding structure, moving
headings, and updating metadata. Most of it is scaffolding. The actual
content is still just a few hopeful lines.</p>
<p>By November I had abandoned DocBook for AsciiDoc, written a custom build
script, and convinced myself the book could be wrapped up in six months.
Those early commits glow with energy: adds, rewrites, then more
rewrites to fix the rewrites.
Delusion is underrated.</p>
<p>In 2013 I managed to convince O‚ÄôReilly to publish. Moving the repo to their
Atlas system sounded simple until Atlas began hiding my main file and
overwriting half-finished chapters.</p>
<p>The Git history reads like a diary of frustration:
‚ÄúMoving files to top level to cope with Atlas,‚Äù ‚ÄúAtlas seems to be
overwriting book.asciidoc‚Äù. Word count shot past 120 000 while actual
progress crawled. On 10 March 2015 I was literally ‚ÄúSmashing chapters into sections‚Äù just to keep the build green.</p>
<p>The quiet cancellation came two months later. No drama, just a polite call and a line through the contract. Relief mingled with embarrassment, I had spent two years rearranging files rather than finishing sentences.</p>
<p>Pragmatic Bookshelf took over that same year. I kept working in CVS for
their production system, but progress was slow. Eventually, they cancelled
too. On 20 January 2017, I imported everything into a new repo in one
massive commit: 6,622 files, over a million lines.
The rewrite stalled, and so did the project.</p>
<p>On 23 March 2017 I started fresh with Asciidoctor in a private GitHub repo, copy-pasting
only the parts that still made sense. Two weeks later, on April 7, minutes before
a lecture at Chalmers, I flipped the repository public. Within twenty-four
hours strangers fixed typos, added diagrams, and merged a Creative Commons
BY-4.0 license.</p>
<h3>What Kept Me Going</h3>
<p><img src="https://happihacking.com/images/star-history.svg" alt="A picture of the stars on GitHub passing 3000."></p><p>I kept going because I wanted to understand the BEAM properly. There‚Äôs
value in following the real logic, not just the surface explanations.</p>
<p>Community feedback made a difference. As soon as the repo was public,
people began sending corrections, examples, and improvements.</p>
<p>Seeing the numbers of people starring the repo on GitHub kept me going.
One highlight: <strong>Issue #113 ‚Äì ‚ÄúPlease continue being awesome.‚Äù</strong>
That emoji-laced drive-by encouragement (August 2018) still pops into my
head whenever motivation dips.</p>
<p><img src="https://happihacking.com/images/issue113.png" alt="Issue 113: This book
is ridiculously good. I have only read a few bits of it so far and have
learned a lot already. Please continue being awesome!"></p>
<p>The book started showing up as a reference in Erlang and BEAM conference
talks, sometimes several times in the same event. That was a clear signal
that others needed this as much as I did.</p>
<p>Even Twitter (in the good old days of Twitter) played a role. Whenever
someone mentioned the book or shared a
link, it was an extra nudge to keep at it.</p>
<p>Mostly, I just wanted a manual I could trust myself, a reference for the
parts of the VM that matter when things go wrong. That‚Äôs reason enough to
keep writing, even after the third rewrite.</p>
<h3>What‚Äôs Inside the Book &amp; Who It Helps</h3>
<p>The book covers what I wish I‚Äôd had when building and operating large
Erlang systems:</p>
<ul>
<li>Schedulers and process management: How the BEAM schedules,
prioritizes, and balances processes under real load.</li>
<li>Processes and their memory: How process heaps,
stack, messages, and binaries are managed and
why these details matter in production.</li>
<li>Garbage collection and memory: What actually happens
with per-process and global garbage collectors, binary references,
and memory leaks.</li>
<li>Tagging schemes and terms: How the BEAM represents data‚Äîintegers,
floats, tuples, binaries, references‚Äîdown to the tagging bits.</li>
<li>The compiler and the VM: How code is turned into instructions,
what the compiler does (and doesn‚Äôt do), and how the emulator executes it.</li>
<li>Tracing and debugging: Practical use of dbg, erlang:trace,
and other tools to follow messages, events, and identify bottlenecks.</li>
<li>Performance tuning: What matters when profiling real code,
understanding reductions, and tracking down real-world latency problems.</li>
<li>System architecture: How ERTS, the BEAM VM, and their subsystems
actually work together in a running node.</li>
</ul>
<p>If you build or operate Erlang or Elixir systems, especially under any kind
of scale‚Äîthis book is for you. It saves you from hunting through mailing
lists, scattered docs, and code comments just to answer, ‚ÄúWhy is the VM
behaving like this?‚Äù</p>
<h3>Lessons Learned</h3>
<p>Persistence beats perfection. Two cancelled publishing deals look bad on a
r√©sum√©, but an unfinished idea looks worse.</p>
<p>Boundaries matter. I made progress by blocking time for writing, turning
off notifications, and treating focus like a real deadline. Fika at 14:30
is non-negotiable.</p>
<p>The crowd helps. Making the repo public brought in corrections,
encouragement, and the occasional nudge when motivation was low.</p>
<p>Scope is everything. I cut the details on dirty schedulers, the new JIT,
and the debugger. Maybe those will end up in an appendix, but not in the
core.</p>
<p>Ship, then iterate. The BEAM changes every year. A living Git repo keeps
up.</p>
<p>A real deadline helps. This January, during my yearly review, I
decided to print the book in time for Code Beam Stockholm. I thought I had
until autumn, turns out the conference was June 2. That‚Äôs how you find out
what‚Äôs truly essential.</p>
<h3>Definition of Done</h3>
<p>Holding the print in my hands, it finally feels finished, at least for now. Years of scattered commits are bound into something real, so I‚Äôm calling it done.</p>
<h3>Get Involved</h3>
<p>You can now get the paperback‚ÄîThe BEAM Book 1.0 is live on Amazon. Buy it
here.&nbsp;<a href="https://www.amazon.com/dp/9153142535">Amazon</a></p>
<p>If you spot an error, want to improve something, or just want to see how it
works under the hood, star or fork the repo. File an issue or, even better,
submit a pull request. Contributors are credited in the acknowledgments.
<a href="https://github.com/happi/theBeamBook">GitHub: theBeamBook</a></p>
<p>If you read the book, please leave an honest review.
Algorithms notice real feedback more than marketing copy.</p>
<p>If your team wants a deep dive, I run hands-on BEAM internals
workshops, tailored for real systems, not just hello world.
Email me if that‚Äôs what you need.
<a href="mailto:happi@happihacking.com">happi@happihacking.com</a></p>


	<p>
	  - Happi
  </p>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Cockatoos have learned to operate drinking fountains in Australia (193 pts)]]></title>
            <link>https://www.science.org/content/article/cockatoos-have-learned-operate-drinking-fountains-australia</link>
            <guid>44178902</guid>
            <pubDate>Wed, 04 Jun 2025 09:42:29 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.science.org/content/article/cockatoos-have-learned-operate-drinking-fountains-australia">https://www.science.org/content/article/cockatoos-have-learned-operate-drinking-fountains-australia</a>, See on <a href="https://news.ycombinator.com/item?id=44178902">Hacker News</a></p>
Couldn't get https://www.science.org/content/article/cockatoos-have-learned-operate-drinking-fountains-australia: Error: Request failed with status code 403]]></description>
        </item>
        <item>
            <title><![CDATA[Cloud Run GPUs, now GA, makes running AI workloads easier for everyone (215 pts)]]></title>
            <link>https://cloud.google.com/blog/products/serverless/cloud-run-gpus-are-now-generally-available</link>
            <guid>44178468</guid>
            <pubDate>Wed, 04 Jun 2025 08:28:46 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://cloud.google.com/blog/products/serverless/cloud-run-gpus-are-now-generally-available">https://cloud.google.com/blog/products/serverless/cloud-run-gpus-are-now-generally-available</a>, See on <a href="https://news.ycombinator.com/item?id=44178468">Hacker News</a></p>
<div id="readability-page-1" class="page"><div jsname="tx2NYc"><div jsaction="rcuQ6b:npT2md" jscontroller="M0Q3Qb"><p><span>Developers love </span><a href="https://cloud.google.com/run"><span>Cloud Run</span><span>, Google Cloud‚Äôs serverless runtime, </span></a><span>for its simplicity, flexibility, and scalability. And today, we‚Äôre thrilled to announce that NVIDIA GPU support for Cloud Run is now generally available, offering a powerful runtime for a variety of use cases that‚Äôs also remarkably cost-efficient.&nbsp;</span></p>
<p><span>Now, you can enjoy the following benefits across both GPUs and CPUs:</span></p>
<ul>
<li>
<p><strong>Pay-per-second billing</strong><span>: You are only charged for the GPU resources you consume, down to the second.</span></p>
</li>
<li>
<p><strong>Scale to zero</strong><span>: Cloud Run automatically scales your GPU instances down to zero when no requests are received, eliminating idle costs. This is a game-changer for sporadic or unpredictable workloads.</span></p>
</li>
<li>
<p><strong>Rapid startup and scaling</strong><span> Go from zero to an instance with a GPU and drivers installed in under 5 seconds, allowing your applications to respond to demand very quickly. For example, when scaling from zero (cold start), we achieved an impressive Time-to-First-Token of approximately 19 seconds for a gemma3:4b model (this includes startup time, model loading time, and running the inference)</span></p>
</li>
<li>
<p><strong>Full streaming support</strong><span>: Build truly interactive applications with out-of-the box support for HTTP and WebSocket streaming, allowing you to provide LLM responses to your users as they are generated.</span></p>
</li>
</ul>
<p><span>Support for GPUs in Cloud Run is a significant milestone, underscoring our leadership in making GPU-accelerated applications simpler, faster, and more cost-effective than ever before.</span></p>
<p><span>‚ÄúServerless GPU acceleration represents a major advancement in making cutting-edge AI computing more accessible. With seamless access to NVIDIA L4 GPUs, developers can now bring AI applications to production faster and more cost-effectively than ever before.‚Äù </span><span>- Dave Salvator, director of accelerated computing products, NVIDIA</span></p></div><div jsaction="rcuQ6b:npT2md" jscontroller="M0Q3Qb"><h3><strong>AI inference for everyone</strong></h3>
<p><span>One of the most exciting aspects of this GA release is that Cloud Run GPUs are now available to everyone for NVIDIA L4 GPUs, with </span><strong>no quota request required</strong><span>.This removes a significant barrier to entry, allowing you to immediately tap into GPU acceleration for your Cloud Run services. Simply use </span><code>--gpu 1</code><span> from the Cloud Run command line, or check the "GPU" checkbox in the console, no need to request quota:</span></p></div><section><figure><section jscontroller="SCGBie" jsaction="rcuQ6b:npT2md"><img src="https://storage.googleapis.com/gweb-cloudblog-publish/images/1_XkZEV9U.max-1000x1000.png" alt="https://storage.googleapis.com/gweb-cloudblog-publish/images/1_XkZEV9U.max-1000x1000.png" jsname="P3Vluc" jsaction="click:HTIlC" loading="lazy"></section></figure></section><div jsaction="rcuQ6b:npT2md" jscontroller="M0Q3Qb"><h3><strong>Production-ready</strong></h3>
<p><span>With general availability, Cloud Run with GPU support is now covered by Cloud Run's </span><a href="https://cloud.google.com/run/sla"><span>Service Level Agreement (SLA)</span></a><span>, providing you with assurances for reliability and uptime. By default, Cloud Run offers </span><a href="https://cloud.google.com/run/docs/zonal-redundancy"><span>zonal redundancy</span></a><span>, helping to ensure enough capacity for your service to be resilient to a zonal outage; this also applies to Cloud Run with GPUs. Alternatively, you can turn off zonal redundancy and benefit from a </span><a href="https://cloud.google.com/run/pricing"><span>lower price</span></a><span> for best-effort failover of your GPU workloads in case of a zonal outage.</span></p>
<h3><strong>Multi-regional GPUs</strong></h3>
<p><span>To support global applications, Cloud Run GPUs are available in five Google Cloud </span><a href="https://cloud.google.com/run/docs/locations#gpu"><span>regions</span></a><span>: us-central1 (Iowa, USA), europe-west1 (Belgium), europe-west4 (Netherlands), asia-southeast1 (Singapore), and asia-south1 (Mumbai, India), with more to come.</span></p>
<p><span>Cloud Run also </span><a href="https://cloud.google.com/run/docs/multiple-regions"><span>simplifies deploying your services across multiple regions</span></a><span>. For instance, you can deploy a service across the US, Europe and Asia with a single command, providing global users with lower latency and higher availability. For instance, here‚Äôs how to deploy </span><a href="https://ollama.com/" rel="noopener" target="_blank"><span>Ollama</span></a><span>, one of the easiest way to run open models, on Cloud Run across three regions:</span></p></div><div jsaction="rcuQ6b:npT2md" jscontroller="M0Q3Qb"><h3><strong>See it in action: 0 to 100 NVIDIA GPUs in four minutes</strong></h3>
<p><span>You can witness the incredible scalability of Cloud Run with GPUs for yourself with </span><a href="https://youtu.be/PWPvX25R6dM?feature=shared&amp;t=2140" rel="noopener" target="_blank"><span>this live demo</span></a><span> from Google Cloud Next 25, showcasing how we scaled from 0 to 100 GPUs in just four minutes.</span></p></div><section><figure><section jscontroller="SCGBie" jsaction="rcuQ6b:npT2md"><img src="https://storage.googleapis.com/gweb-cloudblog-publish/images/2_SrvmWli.max-1600x1600.png" alt="https://storage.googleapis.com/gweb-cloudblog-publish/images/2_SrvmWli.max-1600x1600.png" jsname="P3Vluc" jsaction="click:HTIlC" loading="lazy"></section></figure><div><p>Load testing a Stable Diffusion service running on Cloud Run GPUs to 100 GPU instances in four minutes.</p></div></section><div jsaction="rcuQ6b:npT2md" jscontroller="M0Q3Qb"><h3><strong>Unlock new use cases with NVIDIA GPUs on Cloud Run jobs</strong></h3>
<p><span>The power of Cloud Run with GPUs isn't just for real-time inference using request-driven Cloud Run services. We're also excited to announce the availability of GPUs on </span><a href="https://cloud.google.com/run/docs/overview/what-is-cloud-run#cloud-run-jobs"><span>Cloud Run jobs</span></a><span>, unlocking new use cases, particularly for batch processing and asynchronous tasks:</span></p>
<ul>
<li>
<p><strong>Model fine-tuning</strong><span>: Easily fine-tune a pre-trained model on specific datasets without having to manage the underlying infrastructure. Spin up a GPU-powered job, process your data, and scale down to zero when it‚Äôs complete.</span></p>
</li>
<li>
<p><strong>Batch AI inferencing</strong><span>: Run large-scale batch inference tasks efficiently. Whether you're analyzing images, processing natural language, or generating recommendations, Cloud Run jobs with GPUs can handle the load.</span></p>
</li>
<li>
<p><strong>Batch media processing</strong><span>: Transcode videos, generate thumbnails, or perform complex image manipulations at scale.</span></p>
</li>
</ul>
<p><span><a href="https://docs.google.com/forms/d/e/1FAIpQLSe_-u-ZSxVLhRMZ3p4ZSk2CkgL_URKqNgyM8rfMGUrTbpqYJQ/viewform?usp=dialog" rel="noopener" target="_blank"><span>Sign up</span></a><span> for the private preview of GPUs on Cloud Run jobs.</span></span></p>
<h3><strong>What Cloud Run customers are saying</strong></h3>
<p><span>Don't just take our word for it. Here's what some early adopters of Cloud Run GPUs are saying:</span></p>
<p><span>"Cloud Run helps vivo quickly iterate AI applications and greatly reduces our operation and maintenance costs. The automatically scalable GPU service also greatly improves the efficiency of our AI going overseas.‚Äù</span><span> - Guangchao Li, AI Architect, vivo</span></p>
<p><span>"L4 GPUs offer really strong performance at a reasonable cost profile. Combined with the fast auto scaling, we were really able to optimize our costs and saw an 85% reduction in cost. We've been very excited about the availability of GPUs on Cloud Run."</span><span> - John Gill at </span><a href="https://youtu.be/PWPvX25R6dM?feature=shared&amp;t=2496" rel="noopener" target="_blank"><span>Next'25</span></a><span>, Sr. Software Engineer, Wayfair</span></p>
<p><span><span>"At Midjourney, we have found Cloud Run GPUs to be incredibly valuable for our image processing tasks. Cloud Run has a simple developer experience that lets us focus more on innovation and less on infrastructure management. Cloud Run GPU‚Äôs scalability also lets us easily analyze and process millions of images.</span><span>" - Sam Schickler, Data Team Lead, Midjourney</span></span></p>
<h3><strong>Get started today</strong></h3>
<p><span>Cloud Run with GPU is ready to power your next generation of applications. Dive into the </span><a href="https://cloud.google.com/run/docs/configuring/services/gpu"><span>documentation</span></a><span>, explore our </span><a href="https://cloud.google.com/run/docs/tutorials/gpu-gemma-with-ollama"><span>quickstarts</span></a><span>, and review our </span><a href="https://cloud.google.com/run/docs/configuring/services/gpu-best-practices"><span>best practices for optimizing model loading</span></a><span>. We can't wait to see what you build!</span></p></div><section><span>Posted in</span><ul><li><a href="https://cloud.google.com/blog/products/serverless" track-metadata-position="body" track-metadata-eventdetail="cloud.google.com/blog/products/serverless" track-metadata-module="tag list" track-metadata-module_headline="posted in">Serverless</a></li><li><a href="https://cloud.google.com/blog/products/application-modernization" track-metadata-position="body" track-metadata-eventdetail="cloud.google.com/blog/products/application-modernization" track-metadata-module="tag list" track-metadata-module_headline="posted in">Application Modernization</a></li><li><a href="https://cloud.google.com/blog/products/compute" track-metadata-position="body" track-metadata-eventdetail="cloud.google.com/blog/products/compute" track-metadata-module="tag list" track-metadata-module_headline="posted in">Compute</a></li></ul></section></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Merlin Bird ID (440 pts)]]></title>
            <link>https://merlin.allaboutbirds.org/</link>
            <guid>44176829</guid>
            <pubDate>Wed, 04 Jun 2025 02:58:40 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://merlin.allaboutbirds.org/">https://merlin.allaboutbirds.org/</a>, See on <a href="https://news.ycombinator.com/item?id=44176829">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="hero-wrapper">
              <h2>Identify the birds you see or hear with Merlin Bird ID</h2><p>Free global bird guide with photos, <br>sounds, maps, and more.</p><p><a href="https://itunes.apple.com/app/apple-store/id773457673?pt=401711&amp;ct=marketingwebsite&amp;mt=8"><img data-src="https://merlin.allaboutbirds.org/wp-content/uploads/2020/01/Download_App_Store_en.png" src="data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iMSIgaGVpZ2h0PSIxIiB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciPjwvc3ZnPg=="></a> <a href="https://play.google.com/store/apps/details?id=com.labs.merlinbirdid.app&amp;pli=1"><img data-src="https://merlin.allaboutbirds.org/wp-content/uploads/2020/02/google-play-badge-en.png" src="data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iMSIgaGVpZ2h0PSIxIiB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciPjwvc3ZnPg=="></a></p>            </div><section id="content" aria-label="Main content" data-sticky-container="">
 
      
<div>
<div>
<h2>Identify Bird Songs and Calls</h2>



<p><strong>Sound ID</strong> listens to the birds around you&nbsp;and shows real-time suggestions for who‚Äôs singing. Compare your recording to the songs and calls in Merlin to confirm what you heard. Sound ID works completely offline, so you can identify birds you hear no matter where you&nbsp;are. </p>



<p>Available for birds in the US, Canada, Europe, with some common birds of Central and South America, and India. More species and regions coming soon.&nbsp;</p>




</div>



<div>
<figure><img fetchpriority="high" decoding="async" width="2000" height="1250" src="https://merlin.allaboutbirds.org/wp-content/uploads/2025/05/Merlin-Website-Screens-1.png" alt="" srcset="https://merlin.allaboutbirds.org/wp-content/uploads/2025/05/Merlin-Website-Screens-1.png 2000w, https://merlin.allaboutbirds.org/wp-content/uploads/2025/05/Merlin-Website-Screens-1-720x450.png 720w, https://merlin.allaboutbirds.org/wp-content/uploads/2025/05/Merlin-Website-Screens-1-1280x800.png 1280w, https://merlin.allaboutbirds.org/wp-content/uploads/2025/05/Merlin-Website-Screens-1-768x480.png 768w, https://merlin.allaboutbirds.org/wp-content/uploads/2025/05/Merlin-Website-Screens-1-1536x960.png 1536w, https://merlin.allaboutbirds.org/wp-content/uploads/2025/05/Merlin-Website-Screens-1-480x300.png 480w" sizes="(max-width: 2000px) 100vw, 2000px"></figure>
</div>
</div>



<hr>



<div>
<div>
<figure><img decoding="async" width="1280" height="800" data-src="https://merlin.allaboutbirds.org/wp-content/uploads/2025/05/Photo-ID-1-1280x800.png" alt="" data-srcset="https://merlin.allaboutbirds.org/wp-content/uploads/2025/05/Photo-ID-1-1280x800.png 1280w, https://merlin.allaboutbirds.org/wp-content/uploads/2025/05/Photo-ID-1-720x450.png 720w, https://merlin.allaboutbirds.org/wp-content/uploads/2025/05/Photo-ID-1-768x480.png 768w, https://merlin.allaboutbirds.org/wp-content/uploads/2025/05/Photo-ID-1-1536x960.png 1536w, https://merlin.allaboutbirds.org/wp-content/uploads/2025/05/Photo-ID-1-480x300.png 480w, https://merlin.allaboutbirds.org/wp-content/uploads/2025/05/Photo-ID-1.png 2000w" data-sizes="(max-width: 1280px) 100vw, 1280px" src="data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iMSIgaGVpZ2h0PSIxIiB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciPjwvc3ZnPg=="></figure>
</div>



<div>
<h2>Identify Birds in a Photo</h2>



<p>Snap a photo of a bird, or pull one in from your camera roll, and <strong>Photo ID</strong> will offer a short list of possible matches. Photo ID works completely offline, so you can identify birds in the photos you take no matter where you&nbsp;are. </p>




</div>
</div>



<hr>



<h2>Bird ID Wizard‚ÄîStep-by-step</h2>



<p>Answer three simple questions about a bird you are trying to identify and Merlin will give you a list of possible matches.&nbsp;Merlin offers quick identification help for all levels of bird watchers and outdoor enthusiasts to help you learn about the birds in any country in the world. </p>



<figure><img decoding="async" width="1600" height="600" data-src="https://merlin.allaboutbirds.org/wp-content/uploads/2025/05/Step-by-Step-1.png" alt="" data-srcset="https://merlin.allaboutbirds.org/wp-content/uploads/2025/05/Step-by-Step-1.png 1600w, https://merlin.allaboutbirds.org/wp-content/uploads/2025/05/Step-by-Step-1-720x270.png 720w, https://merlin.allaboutbirds.org/wp-content/uploads/2025/05/Step-by-Step-1-1280x480.png 1280w, https://merlin.allaboutbirds.org/wp-content/uploads/2025/05/Step-by-Step-1-768x288.png 768w, https://merlin.allaboutbirds.org/wp-content/uploads/2025/05/Step-by-Step-1-1536x576.png 1536w, https://merlin.allaboutbirds.org/wp-content/uploads/2025/05/Step-by-Step-1-480x180.png 480w" data-sizes="(max-width: 1600px) 100vw, 1600px" src="data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iMSIgaGVpZ2h0PSIxIiB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciPjwvc3ZnPg=="></figure>











<hr>



<div>
<div>
<h2>Save Birds to Your Life List</h2>



<p>Build a&nbsp;digital&nbsp;scrapbook of your birding memories with Save My Bird. Tap ‚ÄúThis is my bird!‚Äù each time you identify a bird, and&nbsp;Merlin will add it to your growing life&nbsp;list.</p>




</div>



<div>
<figure><img decoding="async" width="2000" height="1250" data-src="https://merlin.allaboutbirds.org/wp-content/uploads/2025/05/Life-List.png" alt="" data-srcset="https://merlin.allaboutbirds.org/wp-content/uploads/2025/05/Life-List.png 2000w, https://merlin.allaboutbirds.org/wp-content/uploads/2025/05/Life-List-720x450.png 720w, https://merlin.allaboutbirds.org/wp-content/uploads/2025/05/Life-List-1280x800.png 1280w, https://merlin.allaboutbirds.org/wp-content/uploads/2025/05/Life-List-768x480.png 768w, https://merlin.allaboutbirds.org/wp-content/uploads/2025/05/Life-List-1536x960.png 1536w, https://merlin.allaboutbirds.org/wp-content/uploads/2025/05/Life-List-480x300.png 480w" data-sizes="(max-width: 2000px) 100vw, 2000px" src="data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iMSIgaGVpZ2h0PSIxIiB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciPjwvc3ZnPg=="></figure>
</div>
</div>



<hr>



<h2>Explore Lists of Birds Near You</h2>



<p>Merlin is powered by&nbsp;<a href="http://ebird.org/">eBird</a>, allowing you to&nbsp;build custom lists of&nbsp;the birds you‚Äôre likely to spot wherever you are. Use the filter options to&nbsp;explore birds for different locations or time of year, or switch to show all the Offline Birds you‚Äôve downloaded.&nbsp;Get more from the app with these <a href="https://support.ebird.org/en/support/solutions/articles/48000966225-merlin-tips-and-tricks#anchorCustomList">Merlin Tips and Tricks</a>.</p>



<figure><img decoding="async" width="1600" height="600" data-src="https://merlin.allaboutbirds.org/wp-content/uploads/2025/05/Explore.png" alt="" data-srcset="https://merlin.allaboutbirds.org/wp-content/uploads/2025/05/Explore.png 1600w, https://merlin.allaboutbirds.org/wp-content/uploads/2025/05/Explore-720x270.png 720w, https://merlin.allaboutbirds.org/wp-content/uploads/2025/05/Explore-1280x480.png 1280w, https://merlin.allaboutbirds.org/wp-content/uploads/2025/05/Explore-768x288.png 768w, https://merlin.allaboutbirds.org/wp-content/uploads/2025/05/Explore-1536x576.png 1536w, https://merlin.allaboutbirds.org/wp-content/uploads/2025/05/Explore-480x180.png 480w" data-sizes="(max-width: 1600px) 100vw, 1600px" src="data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iMSIgaGVpZ2h0PSIxIiB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciPjwvc3ZnPg=="></figure>



<hr>



<h2>See How Merlin Can Help You ID Birds</h2>



<figure><p>
<iframe title="Merlin Bird ID Demo from the Cornell Lab of Ornithology" width="1200" height="675" data-src="https://www.youtube.com/embed/xmSUOLxyatY?feature=oembed&amp;modestbranding=1&amp;showinfo=0&amp;rel=0" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen="" src="data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iMSIgaGVpZ2h0PSIxIiB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciPjwvc3ZnPg==" data-load-mode="1"></iframe>
</p></figure>



<hr>



<h2>The Best Birding App, Powered By You</h2>



<p>Merlin features the best of community contributed photos, songs, and calls, tips from experts around the world to help you ID the birds you see, and range maps from <a href="https://birdsoftheworld.org/bow/home">Birds of the World</a>‚Äîall powered by billions of bird observations submitted to <a href="https://ebird.org/home">eBird</a>.</p>





      
        </section></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Binary Wordle (207 pts)]]></title>
            <link>https://wordle.chengeric.com/</link>
            <guid>44176825</guid>
            <pubDate>Wed, 04 Jun 2025 02:57:11 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://wordle.chengeric.com/">https://wordle.chengeric.com/</a>, See on <a href="https://news.ycombinator.com/item?id=44176825">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>Use keyboard (0, 1, Enter, Backspace) or buttons to play</p></div><div><p>Sponsor: Don't like waiting on hold? Try</p><!-- --> <p><a href="https://altodial.com/?wordle"><img alt="" loading="lazy" width="16" height="16" decoding="async" data-nimg="1" src="https://wordle.chengeric.com/square.svg">altodial.com</a></p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[DiffX ‚Äì Next-Generation Extensible Diff Format (313 pts)]]></title>
            <link>https://diffx.org/</link>
            <guid>44176737</guid>
            <pubDate>Wed, 04 Jun 2025 02:38:46 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://diffx.org/">https://diffx.org/</a>, See on <a href="https://news.ycombinator.com/item?id=44176737">Hacker News</a></p>
Couldn't get https://diffx.org/: Error: getaddrinfo ENOTFOUND diffx.org]]></description>
        </item>
        <item>
            <title><![CDATA[HNInternal: Ask HN: Has anybody built search on top of Anna's Archive? (217 pts)]]></title>
            <link>https://news.ycombinator.com/item?id=44176514</link>
            <guid>44176514</guid>
            <pubDate>Wed, 04 Jun 2025 01:47:26 GMT</pubDate>
            <description><![CDATA[<p>See on <a href="https://news.ycombinator.com/item?id=44176514">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
            <tbody><tr id="44176767"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_44176767" href="https://news.ycombinator.com/vote?id=44176767&amp;how=up&amp;goto=item%3Fid%3D44176514"></a></center>    </td><td><br><div><p>Honestly I don't think it would be that costly, but it would take a pretty long time to put together. I have a (few years old) copy of Library Genesis converted to plaintext and it's around 1TB. I think libgen proper was 50-100TB at the time, so we can probably assume that AA (~1PB) would be around 10-20TB when converted to plaintext. You'd probably spend several weeks torrenting a chunk of the archive, converting everything in it to plaintext, deleting the originals, then repeating with a new chunk until you have plaintext versions of everything in the archive. Then indexing all that for full text search would take even more storage and even more time, but still perfectly doable on commodity hardware.</p><p>The main barriers are going to be reliably extracting plaintext from the myriad of formats in the archive, cleaning up the data, and selecting a decent full text search database (god help you if you pick wrong and decide you want to switch and re-index everything later).</p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="44177936"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_44177936" href="https://news.ycombinator.com/vote?id=44177936&amp;how=up&amp;goto=item%3Fid%3D44176514"></a></center>    </td><td><br><div><p>The main barriers for me would be:</p><p>1. Why? Who would use that? What‚Äôs the problem with the other search engines? How will it be paid for?</p><p>2. Potential legal issues.</p><p>The technical barriers are at least challenging and interesting.</p><p>Providing a service with significant upfront investment needs with no product or service vision that I‚Äôll likely to be sued for a couple of times a year, probably losing with who knows what kind of punishment‚Ä¶ I‚Äôll have to pass unfortunately.</p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="44178200"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_44178200" href="https://news.ycombinator.com/vote?id=44178200&amp;how=up&amp;goto=item%3Fid%3D44176514"></a></center>    </td><td><br><div>
                  <p>It would be incredible for LLMs. Searching it, using it as training data, etc. Would probably have to be done in Russia or some other country that doesn't respect international copyright though.</p>
              </div></td></tr>
        </tbody></table></td></tr>
                <tr id="44178241"><td><table>  <tbody><tr>    <td indent="3"><img src="https://news.ycombinator.com/s.gif" height="1" width="120"></td><td>
      <center><a id="up_44178241" href="https://news.ycombinator.com/vote?id=44178241&amp;how=up&amp;goto=item%3Fid%3D44176514"></a></center>    </td><td><br><div>
                  <p>Do you have a reason to believe this ain't already being done? I would assume that the big guys like openai are already training on basically all text in existence.</p>
              </div></td></tr>
        </tbody></table></td></tr>
                <tr id="44178453"><td></td></tr>
                  <tr id="44178237"><td></td></tr>
                        <tr id="44177444"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_44177444" href="https://news.ycombinator.com/vote?id=44177444&amp;how=up&amp;goto=item%3Fid%3D44176514"></a></center>    </td><td><br><div><p>I think there‚Äôs a couple ways to improve it:</p><p>1. There‚Äôs a lot of variants of the same book. We only need one for the index. Perhaps for each ISBN, select the format easiest to parse.</p><p>2. We can download, convert and index top 100K books first, launch with these, and then continue indexing and adding other books.</p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="44177679"><td></td></tr>
                <tr id="44178316"><td><table>  <tbody><tr>    <td indent="3"><img src="https://news.ycombinator.com/s.gif" height="1" width="120"></td><td>
      <center><a id="up_44178316" href="https://news.ycombinator.com/vote?id=44178316&amp;how=up&amp;goto=item%3Fid%3D44176514"></a></center>    </td><td><br><div>
                  <p>From a good search perspective though you probably dont want 500 different versions of the same book popping up for a query</p>
              </div></td></tr>
        </tbody></table></td></tr>
                        <tr id="44177996"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_44177996" href="https://news.ycombinator.com/vote?id=44177996&amp;how=up&amp;goto=item%3Fid%3D44176514"></a></center>    </td><td><br><div><p>I wonder if you could implement it with only static hosting?</p><p>We would need to split the index into a lot of smaller files that can be practically downloaded by browsers, maybe 20 MB each.
The user types in a search query, the browser hashes the query and downloads the corresponding index file which contains only results for that hashed query. Then the browser sifts quickly through that file and gives you the result.</p><p>Hosting this would be cheap, but the main barriers remain..</p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="44177999"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_44177999" href="https://news.ycombinator.com/vote?id=44177999&amp;how=up&amp;goto=item%3Fid%3D44176514"></a></center>    </td><td><br><div><p>It's trivial to normalise the various formats, and there were a few libraries and ML models to help parse PDFs. I was tinkering around with something like this for academic papers in Zotero, and the main issue I ran into was words spilling over to the next page, and footnotes. I totally gave up on that endeavour several years ago, but the tooling has probably matured exponentially since then.</p><p>As an example, all the academic paper hubs have been using this technology for decades.</p><p>I'd wager that <i>all</i> of the big Gen AI companies have planned to use this exact dataset, and many or them probably have already.</p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="44178133"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_44178133" href="https://news.ycombinator.com/vote?id=44178133&amp;how=up&amp;goto=item%3Fid%3D44176514"></a></center>    </td><td><br><div><p>&gt; It's trivial to normalise the various formats,</p><p>Ha. Ha. ha ha ha.</p><p>As someone who as pretty broadly tried to normalize a pile of books and documents I have legitimate access to, <i>no it is not</i>.</p><p>You can get good results 80% of the time, usable but messy results 18% of the time, and complete garbage the remaining 2%. More effort seems to only result in marginal improvements.</p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="44178321"><td></td></tr>
                              <tr id="44178333"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_44178333" href="https://news.ycombinator.com/vote?id=44178333&amp;how=up&amp;goto=item%3Fid%3D44176514"></a></center>    </td><td><br><div><p>There is a search solution for zipped fb2 files. Not exactly what you need, but it has potential.</p><p>The project has similar story to Anna's archive. There is 0.5 TB of archived books, and the project creates index of all the books with text, title and aruthor search capabilities, gives html UI for search and reading. On weak machine it takes about 2 hours to build that index.</p><p>So if you have zipped archives of fb2, you can use the project to create web UI with search for those files. Without need of enough space to unpack all the files.</p><p>You'll have to translate some russian though to get instructions on how to set it up.</p><p><a href="https://gitlab.com/opennota/fb2index/-/blob/master/README.ru.md" rel="nofollow">https://gitlab.com/opennota/fb2index/-/blob/master/README.ru...</a></p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="44178203"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_44178203" href="https://news.ycombinator.com/vote?id=44178203&amp;how=up&amp;goto=item%3Fid%3D44176514"></a></center>    </td><td><br><div><p>There‚Äôs an android app called OpenLip. [1]</p><p>Description:</p><p>Openlib is an open source app to download and read books from shadow library (Anna‚Äôs Archive). The App Has Built In Reader to Read Books.</p><p>As Anna‚Äôs Archive doesn't have an API, the app works by sending requests to Anna‚Äôs Archive and parses the response to objects. The app extracts the mirrors from the responses, downloads the book and stores it in the application's document directory.</p><p>Note :
The app requires VPN to function properly . Without VPN the might show the captcha required page even after completing the captcha</p><p>Main Features:</p><p>Trending Books</p><p>Download And Read Books With In-Built Viewer</p><p>Supports Epub And Pdf Formats</p><p>Open Books With Your Favourite Ebooks Reader</p><p>Filter Books</p><p>Sort Books</p><p>[1]: <a href="https://f-droid.org/de/packages/com.app.openlib/" rel="nofollow">https://f-droid.org/de/packages/com.app.openlib/</a></p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="44176569"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_44176569" href="https://news.ycombinator.com/vote?id=44176569&amp;how=up&amp;goto=item%3Fid%3D44176514"></a></center>    </td><td><br><div><p>You must mean free text search and page level return, because it already has full metadata indexing.</p><p>The thing is AA doesn't hold the texts. They're disputable IPR and even a derived work would be a legal target.</p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="44178214"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_44178214" href="https://news.ycombinator.com/vote?id=44178214&amp;how=up&amp;goto=item%3Fid%3D44176514"></a></center>    </td><td><br><div>
                  <p>Z-Library has a keyword search. Personally i didn't find it too useful, especially given Google Books exists. It's not easy to create a quality book search engine.</p>
              </div></td></tr>
        </tbody></table></td></tr>
            <tr id="44177457"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_44177457" href="https://news.ycombinator.com/vote?id=44177457&amp;how=up&amp;goto=item%3Fid%3D44176514"></a></center>    </td><td><br><div><p>As far as I know, no one has fully implemented full-text search directly over Anna's Archive. Technically it‚Äôs feasible with tools like Meilisearch, Elasticsearch, or Lucene, but the main challenges are:</p><pre><code>    Converting all documents (PDFs, EPUBs, etc.) to clean plaintext.

    Indexing at scale efficiently.

    Managing potential legal issues.
</code></pre><p>
Z-Library does something similar, but it‚Äôs smaller in scope and doesn't integrate AA‚Äôs full catalog.</p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="44177592"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_44177592" href="https://news.ycombinator.com/vote?id=44177592&amp;how=up&amp;goto=item%3Fid%3D44176514"></a></center>    </td><td><br><div><p>I‚Äôve done something like this before. Meilisearch will not be viable, because it indexes very slow and it takes up a lot of space.</p><p>In my experience only Tantivy can index this much data. Check out Lnx.</p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="44178248"><td></td></tr>
                        <tr id="44177894"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_44177894" href="https://news.ycombinator.com/vote?id=44177894&amp;how=up&amp;goto=item%3Fid%3D44176514"></a></center>    </td><td><br><div>
                  <p>Related question, has Anna's archive been thoroughly filtered for non-copyright-related illegal material? Pedo, terrorism, etc. I've considered downloading a few chunks of it but I'm worried of ending up with content I really don't want to be anywhere near from.</p>
              </div></td></tr>
        </tbody></table></td></tr>
                <tr id="44178086"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_44178086" href="https://news.ycombinator.com/vote?id=44178086&amp;how=up&amp;goto=item%3Fid%3D44176514"></a></center>    </td><td><br><div>
                  <p>This is a really strange question to be honest you could ask this literally about any download let alone simply torrents of documents.</p>
              </div></td></tr>
        </tbody></table></td></tr>
            <tr id="44178012"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_44178012" href="https://news.ycombinator.com/vote?id=44178012&amp;how=up&amp;goto=item%3Fid%3D44176514"></a></center>    </td><td><br><div>
                  <p>How might you inadvertently download illegal content while searching for legal content?</p>
              </div></td></tr>
        </tbody></table></td></tr>
                <tr id="44178070"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_44178070" href="https://news.ycombinator.com/vote?id=44178070&amp;how=up&amp;goto=item%3Fid%3D44176514"></a></center>    </td><td><br><div><p>He said he wants to download lots of it in general, not specifical. Legit question, if you end up with dark material.</p><p>I would assume pedo stuff is not really there, but the anarchist cookbook and alike likely will be.</p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="44178167"><td><table>  <tbody><tr>    <td indent="3"><img src="https://news.ycombinator.com/s.gif" height="1" width="120"></td><td>
      <center><a id="up_44178167" href="https://news.ycombinator.com/vote?id=44178167&amp;how=up&amp;goto=item%3Fid%3D44176514"></a></center>    </td><td><br><div><p>I'm still not sure the question makes much sense, if it's a general: "I want to support the project and so I want to seed a large chunk" Okay, I guess it's your due diligence to check, but there is a reporting feature built in, if something is found, report it.</p><p>Aside from that, if you're searching for specific content, the question is moot I guess.</p><p>I guess my confusion is what distinguishes this apart from any other torrent ? That is, if the submitted content is submitted at all.</p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="44178210"><td><table>  <tbody><tr>    <td indent="4"><img src="https://news.ycombinator.com/s.gif" height="1" width="160"></td><td>
      <center><a id="up_44178210" href="https://news.ycombinator.com/vote?id=44178210&amp;how=up&amp;goto=item%3Fid%3D44176514"></a></center>    </td><td><br><div><p>I understood it as he or she wants to download large chunks of potentially interesting books for offline use, or once Anna goes down. So a broad filter. Not for seeding.</p><p>But thanks for the explanation that there is a report build in.</p></div></td></tr>
        </tbody></table></td></tr>
                  <tr id="44178137"><td><table>  <tbody><tr>    <td indent="3"><img src="https://news.ycombinator.com/s.gif" height="1" width="120"></td><td>
      <center><a id="up_44178137" href="https://news.ycombinator.com/vote?id=44178137&amp;how=up&amp;goto=item%3Fid%3D44176514"></a></center>    </td><td><br><div>
                  <p>Considering the anarchist cookbook is just a rebranded selection of freely-available US Army Field Manuals, ... I don't see the problem.</p>
              </div></td></tr>
        </tbody></table></td></tr>
                <tr id="44178201"><td><table>  <tbody><tr>    <td indent="4"><img src="https://news.ycombinator.com/s.gif" height="1" width="160"></td><td>
      <center><a id="up_44178201" href="https://news.ycombinator.com/vote?id=44178201&amp;how=up&amp;goto=item%3Fid%3D44176514"></a></center>    </td><td><br><div>
                  <p>I don't either, but many states have laws regarding books on how to build bombs and they might get enforced more than copyright.</p>
              </div></td></tr>
        </tbody></table></td></tr>
                  <tr id="44178127"><td></td></tr>
                <tr id="44178218"><td><table>  <tbody><tr>    <td indent="4"><img src="https://news.ycombinator.com/s.gif" height="1" width="160"></td><td>
      <center><a id="up_44178218" href="https://news.ycombinator.com/vote?id=44178218&amp;how=up&amp;goto=item%3Fid%3D44176514"></a></center>    </td><td><br><div>
                  <p>Well, I won't. But does it contain just text or real pictures? That would make a big legal difference I assume.</p>
              </div></td></tr>
        </tbody></table></td></tr>
            <tr id="44178277"><td></td></tr>
                                    <tr id="44176864"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_44176864" href="https://news.ycombinator.com/vote?id=44176864&amp;how=up&amp;goto=item%3Fid%3D44176514"></a></center>    </td><td><br><div><p>AFAIK, Z-Library already does this, to some extent. Basic full-text queries do search inside the body of books and articles.</p><p>It's a bit smaller than Anna's Archive, as they do host their own collections. From some locations, it's only easy to access through Tor.</p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="44178107"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_44178107" href="https://news.ycombinator.com/vote?id=44178107&amp;how=up&amp;goto=item%3Fid%3D44176514"></a></center>    </td><td><br><div>
                  <p>Has anyone explored a different angle ‚Äî like mapping out the 1,000 most frequently mentioned or cited books (across HN, Substack, Twitter, etc.), then turning their raw content into clean, structured data optimized for LLMs? Imagine curating these into thematic shelves ‚Äî say, ‚ÄúBill Gates‚Äô Bookshelf‚Äù or ‚ÄúHN Canon‚Äù ‚Äî and building an indie portal where anyone can semantically search across these high-signal texts. Kind of like an AI-searchable personal library of the internet‚Äôs favorite books.</p>
              </div></td></tr>
        </tbody></table></td></tr>
                <tr id="44178130"><td></td></tr>
                  <tr id="44177619"><td></td></tr>
            <tr id="44177170"><td></td></tr>
                <tr id="44178025"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_44178025" href="https://news.ycombinator.com/vote?id=44178025&amp;how=up&amp;goto=item%3Fid%3D44176514"></a></center>    </td><td><br><div>
                  <p>It's not exactly clear, but OP is asking about indexing the content of all the documents, not the metadata (e.g. titles etc)</p>
              </div></td></tr>
        </tbody></table></td></tr>
                  <tr id="44177542"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_44177542" href="https://news.ycombinator.com/vote?id=44177542&amp;how=up&amp;goto=item%3Fid%3D44176514"></a></center>    </td><td><br><div><p>Mebbe easier to just search Amazon or Goodreads. Like site:amazon.ca &lt;query words&gt; as someone has mentioned below.</p><p>Every book has an ISBN 10 or 13 digit ISBN number to identify them. Unless it's some self-pub/amateur-hour situation by some paranoid prepper living in a faraday-cage-protected cage in Arkansas or Florida it's likely a publication with a title, an author and an ISBN number.</p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="44177831"><td></td></tr>
                  </tbody></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[HNInternal: Ask HN: Startup getting spammed with PayPal disputes, what should we do? (153 pts)]]></title>
            <link>https://news.ycombinator.com/item?id=44176510</link>
            <guid>44176510</guid>
            <pubDate>Wed, 04 Jun 2025 01:46:49 GMT</pubDate>
            <description><![CDATA[<p>See on <a href="https://news.ycombinator.com/item?id=44176510">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><td><table>
        <tbody><tr id="44176510">
      <td><span></span></td>      <td><center><a id="up_44176510" href="https://news.ycombinator.com/vote?id=44176510&amp;how=up&amp;goto=item%3Fid%3D44176510"></a></center></td><td><span><a href="https://news.ycombinator.com/item?id=44176510">Ask HN: Startup getting spammed with PayPal disputes, what should we do?</a></span></td></tr><tr><td colspan="2"></td><td><span>
          <span id="score_44176510">115 points</span> by <a href="https://news.ycombinator.com/user?id=june3739">june3739</a> <span title="2025-06-04T01:46:49 1749001609"><a href="https://news.ycombinator.com/item?id=44176510">12 hours ago</a></span> <span id="unv_44176510"></span> | <a href="https://news.ycombinator.com/hide?id=44176510&amp;goto=item%3Fid%3D44176510">hide</a> | <a href="https://hn.algolia.com/?query=Ask%20HN%3A%20Startup%20getting%20spammed%20with%20PayPal%20disputes%2C%20what%20should%20we%20do%3F&amp;type=story&amp;dateRange=all&amp;sort=byDate&amp;storyText=false&amp;prefix&amp;page=0">past</a> | <a href="https://news.ycombinator.com/fave?id=44176510&amp;auth=a5e7334e20b47144b4b9f3ad251dd0ab5bc8265e">favorite</a> | <a href="https://news.ycombinator.com/item?id=44176510">92&nbsp;comments</a>        </span>
              </td></tr>
    <tr><td></td></tr><tr><td colspan="2"></td><td><div><p>Longtime user posting from a new account out of an abundance of caution.</p><p>I founded an e-commerce marketplace startup. We use PayPal's Multiparty APIs (PayPal Commerce Platform) for checkout. For the 10 days, someone has been bombarding us with purchases that they later dispute. There's consistent pattern to it:</p><p>* They use an email address that has no footprint online, always from the same two domains
* They use an unverified PayPal account to pay
* They pay a low amount, not always the same, in a narrow range for a digital item
* All of the charges were disputed within a few hours</p><p>They're not doing this through our API. The purchase process requires a browser because of the way our payment form is configured. There's an amount of variation to each purchase that tells us they're automating a browser. Logs indicate that they're changing IP each time. The events come in bursts and seem to be spaced to avoid automated detection.</p><p>We added the typical mitigations to our network stack and code. A few are still slipping through. Logs indicate a high amount of bot traffic.</p><p>PayPal does not seem equipped to deal with this. Their support is always extremely slow, relies on canned responses, and to date has a very limited understanding of how their own Multiparty APIs work. Their phone support people will not talk with me, they see no indication that my PayPal account is affiliated with these purchases in any way. They want each of our sellers to contact them independently, which we know will result in disparate cases that don't tell the complete story or offer any assistance.</p><p>Has anyone encountered anything like this before? We're struggling to find the motive or intended outcome by the attacker(s). We're a small company with a niche audience, we've never had a conflict with anyone that got serious enough that we'd expect them to come after us like this.</p><p>Any thoughts and recommendations would be greatly appreciated. We feel like we are on our own here and are unsure of how to handle it.</p></div></td></tr>        <tr><td></td></tr><tr><td colspan="2"></td><td><form action="comment" method="post"></form></td></tr>  </tbody></table>
  </td></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[A manager is not your best friend (179 pts)]]></title>
            <link>https://staysaasy.com/management/2025/06/02/your-manager-is-not-your-best-friend.html</link>
            <guid>44176425</guid>
            <pubDate>Wed, 04 Jun 2025 01:29:59 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://staysaasy.com/management/2025/06/02/your-manager-is-not-your-best-friend.html">https://staysaasy.com/management/2025/06/02/your-manager-is-not-your-best-friend.html</a>, See on <a href="https://news.ycombinator.com/item?id=44176425">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
    <p>As people become managers, it‚Äôs quite common for their team members to want to commiserate with them. This is especially true for friendly, competent, reasonable-seeming managers ‚Äì people want to commiserate with <em>winners</em>. This makes commiseration extra dangerous, as it comes with a hint of flattery (‚ÄúI respect your opinion and trust your discretion‚Äù).</p>

<p>But commiseration, especially with your direct reports, is organizational poison. It erodes the fabric of an organization and builds factions. It leads to feelings of superiority and creates a low-trust environment ‚Äì <em>even if what you‚Äôre complaining about is made up!</em> Worst of all, it doesn‚Äôt give other teams an opportunity to improve. If I think that HR sucks, and I commiserate with my directs about it, my team is going to treat them poorly. HR will never know why, will never fix the problem, and will just think that my team are jerks (and they‚Äôll arguably be right). Commiseration is self-fulfilling because it‚Äôs a form of victimhood: The world is conspiring against us, the only truly virtuous team.</p>

<p>Commiseration comes naturally to most people, because it happens all the time in real life. As your best friend, if you come to me saying that you were just dumped by your girlfriend, you will get unconditional sympathy beyond words. You‚Äôre the best, she didn‚Äôt deserve you, you can do so much better, everyone knows you‚Äôre the man, I have no idea why you ever spent time with her. There will be no questions, there will be no need for you to explain anything about the situation, even if all you did for the last 2 years was sit on your couch smoking weed and playing Warzone, <em>Greg</em>. Unless you did something totally crazy or illegal, my loyalty will be immediate and unconditional, because ‚Äì let‚Äôs be real ‚Äì none of it matters. You‚Äôre going your way, she‚Äôs going hers, and it‚Äôs over.</p>

<p>As a manager, your empathy needs to be highly conditional. Your job is to get to the truth of a matter in a respectful way, not make your team feel good. You are largely stuck with your coworkers, and you need to get stuff done together or everyone suffers. If you break up with your girlfriend you get unconditional sympathy. But if you break up with your girlfriend, and the 3 of us were trying to climb Mt. Everest together, I‚Äôm going to be a lot more measured in how I communicate and balance your relationship so that we can all survive the next few days.</p>

<p>Commiseration is generally a sensitive topic, so I‚Äôve tried to boil down how to handle situations when a direct comes to you with grievances via some heuristics:</p>

<ul>
  <li>You don‚Äôt really want to debate your team in every situation, but your job is to essentially be a scientist and get to the bottom of what‚Äôs going on. If someone wants to commiserate about some other team, your first job is to ask a bunch of questions about what‚Äôs going on. In my experience, 90% of the time the situation is a grey area, and probably 30% of the time the person who wants to commiserate is actually in the wrong, on balance.</li>
  <li>Your role as a manager is also to be a perspective-creator. Sure, that salesperson was overly optimistic on how impactful this custom feature was for a prospect. And sure, the deal wasn‚Äôt as large and didn‚Äôt close as quickly as they said it would. But sales incentives are a law of the universe, and sales directors need to manage around them just as much as product teams do, because <em>not</em> having sales incentives is even worse. And by the way, we‚Äôre not so great at estimating development timelines either. Everyone‚Äôs blood pressure should always be lower after they‚Äôve spoken to you.</li>
  <li>Bad therapists just let you rant. Good therapists let you vent, but they ask clarifying questions, and they sometimes push back. The phrase ‚Äúis that actually true?‚Äù or ‚ÄúCan you explain that more?‚Äù are your friends. Good therapists validate feelings but they don‚Äôt necessarily validate <em>facts</em>. ‚ÄúI know you feel like you‚Äôre being a good daughter‚Äù is not the same as ‚Äúyou are the best daughter.‚Äù You want to be a good therapist.</li>
  <li>Remove the phrase ‚ÄúI don‚Äôt know why they‚Ä¶‚Äù from your lexicon. No matter how you end this sentence, the subtext will be clear: ‚ÄúI don‚Äôt know why they‚Äôre so incompetent.‚Äù Instead, it‚Äôs often better to give the most optimistic view for why another team is behaving the way that they are. It might not be <em>right</em>, but it builds empathy which is the bedrock on which productive collaboration is built.</li>
  <li>If someone is trying to get you to commiserate with them, try to speak in terms of reiterating a decision framework. Rather than ‚Äúmarketing doesn‚Äôt know what they‚Äôre doing,‚Äù you want to say something like ‚Äúour role is to build the product and have a strong POV for marketing, and their role is to make sure that our launch generates enough pipeline. If you don‚Äôt think that‚Äôs going to happen then let‚Äôs talk to them.‚Äù The goal is to focus on objective truths rather than disparaging opinions.</li>
  <li>When it comes to commiseration, people are highly attuned to nuanced communication ‚Äì especially from their boss. ‚ÄúWell guys, we‚Äôve got this‚Äù plus that little head nod and eyeroll is functionally the equivalent of saying ‚Äúit‚Äôs all on us, the protagonists, because everyone else is a fucking idiot <strong>again</strong>.‚Äù Those words didn‚Äôt literally leave your mouth, but you effectively said it, and as a manager that‚Äôs 100% on you. As a manager your implicit communication is just as important as your explicit communication ‚Äì this is not a courtroom, this is real life, and non-verbal actions can still have consequences.</li>
  <li>One of the most common people to commiserate about is your own boss, or the company‚Äôs CEO. This can get highly toxic fast, and is rarely actually productive ‚Äì cases of teams changing their CEO‚Äôs behavior through commiseration are vanishingly rare. The right way to pivot this conversation (or at least, the only way I‚Äôve ever seen this play out positively) is to discuss how you can most effectively work with your boss. This is significantly more productive, and even if you still think they‚Äôre being dumb, at least you‚Äôre tackling that problem constructively.</li>
</ul>

<p>Of course sometimes people really are AAA grade idiots. When this happens, your communication should typically address the issue, not the other team. For most situations, it‚Äôs best to say ‚Äúlet me follow up,‚Äù rather than ‚ÄúI agree that they‚Äôre dumb.‚Äù In particularly egregious cases, you can go with ‚ÄúI know this is a problem and I‚Äôll get on it‚Äù or ‚ÄúI hear you, I‚Äôm working on it, but I can‚Äôt give you every detail on how and don‚Äôt expect ongoing updates‚Äù ‚Äì this avoids gaslighting them that everything is fine, but it also stops the vent session. When the door opens to commiseration with your team, you must slam it shut.</p>

<p>Either way, following up is the ideal next step because it commits you to respond but separates the emotion from the action. Accumulated strong emotions leave a strong impression, especially when they‚Äôre negative emotions like bitterness, so it‚Äôs best to suck the emotion out of the conversation as fast as possible. For evidence of this, light autists often make very effective managers.</p>

<p>Finally ‚Äì we‚Äôre all human here, and sometimes you need to commiserate with <em>someone</em> before your head explodes. If you must commiserate, it‚Äôs almost always best if they‚Äôre a peer / near peer, and they‚Äôre not on your direct team (you don‚Äôt share a boss). This at least dodges the situation where a manager complains alongside their team, and thereby implicitly blesses their most negative views.</p>


    

    

    

    




  </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Meta pauses mobile port tracking tech on Android after researchers cry foul (125 pts)]]></title>
            <link>https://www.theregister.com/2025/06/03/meta_pauses_android_tracking_tech/</link>
            <guid>44175940</guid>
            <pubDate>Tue, 03 Jun 2025 23:42:38 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.theregister.com/2025/06/03/meta_pauses_android_tracking_tech/">https://www.theregister.com/2025/06/03/meta_pauses_android_tracking_tech/</a>, See on <a href="https://news.ycombinator.com/item?id=44175940">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="body">
<p>Security researchers say Meta and Yandex used native Android apps to listen on localhost ports, allowing them to link web browsing data to user identities and bypass typical privacy protections.</p>
<p>Following the disclosure, researchers observed that Meta's Pixel script stopped sending data to localhost and that the tracking code was largely removed. The move may help Meta avoid scrutiny under Google Play policies, which prohibit covert data collection in apps.</p>
<p>"We are in discussions with Google to address a potential miscommunication regarding the application of their policies," a Meta spokesperson told <em>The Register</em>. "Upon becoming aware of the concerns, we decided to pause the feature while we work with Google to resolve the issue."</p>

    

<p>Meta's spokesperson did not respond to a request to elaborate on the company's discussions with Google.</p>
<h3>What the researchers found</h3>
<p>In a <a target="_blank" rel="nofollow" href="https://localmess.github.io/">report</a> published Tuesday, computer scientists affiliated with IMDEA Networks (Spain), Radboud University (The Netherlands), and KU Leuven (Belgium) describe how the US social media giant and the Russian search engine were observed using native Android apps to gather web cookie data via the device's loopback interface, commonly known as localhost.</p>
<p>Localhost is a loopback address that a device can use to make a network request to itself. It's commonly used by software developers to test server-based applications like websites on local hardware.</p>

        


        

<p>The researchers ‚Äì Aniketh Girish (PhD student), Gunes Acar (Assistant Professor), Narseo Vallina-Rodriguez (Associate Professor), Nipuna Weerasekara (PhD student), and Tim Vlummens (PhD student) ‚Äì say they found native Android apps, including Facebook and Instagram, and Yandex's Maps and Browser ‚Äì that listen silently on fixed local ports for tracking purposes.</p>
<p>"These native Android apps receive browsers' metadata, cookies and commands from the Meta Pixel and Yandex Metrica scripts embedded on thousands of websites," the computer scientists explain. "These JavaScripts load on users' mobile browsers and silently connect with native apps running on the same device through localhost sockets."</p>

        

<p>As these native apps access device identifiers like the Android Advertising ID or handle user identities in Meta apps, the researchers say, they're able to link mobile browsing sessions and web cookies to user identities.</p>
<p>Essentially, by opening localhost ports that allow their Android apps to receive tracking data, such as cookies and browser metadata, from scripts running in mobile browsers, Meta and Yandex are able to bypass common privacy safeguards like cookie clearing, Incognito Mode, and Android's app permission system.</p>
<p>The technique also violates assumptions about the scope of first-party cookies, which aren't supposed to be able to track browsing activity across different websites. According to the researchers, "the method we disclose allows the linking of the different _fbp cookies to the same user, which bypasses existing protections and runs counter to user expectations."</p>

        

<p>With regard to Meta, the tracking process involves scripts associated with <a target="_blank" rel="nofollow" href="https://www.facebook.com/business/tools/meta-pixel">Meta Pixel</a>, analytics code used by marketers to gather data about interactions with websites.</p>
<p>Various APIs and protocols can be used to implement the described app-web eavesdropping scheme. These include: SDP munging, which involves manually modifying Session Description Protocol (SDP) messages before the data gets passed to the browser; real-time communications protocols <a target="_blank" rel="nofollow" href="https://developer.mozilla.org/en-US/docs/Web/API/WebSockets_API">Websocket</a> and <a target="_blank" rel="nofollow" href="https://developer.mozilla.org/en-US/docs/Web/API/WebRTC_API/Protocols">WebRTC</a>; Session Traversal Utilities for NAT (STUN), an address discovery mechanism; and Traversal Using Relays around NAT (TURN), a router restriction bypass method.</p>
<ul>

<li><a href="https://www.theregister.com/2025/06/03/xs_new_encrypted_xchat_feature/">X's new 'encrypted' XChat feature seems no more secure than the failure that came before it</a></li>

<li><a href="https://www.theregister.com/2025/05/30/meta_is_now_a_defense/">Meta ‚Äì yep, Facebook Meta ‚Äì is now a defense contractor</a></li>

<li><a href="https://www.theregister.com/2025/05/29/billions_of_cookies_available/">Billions of cookies up for grabs as experts warn over session security</a></li>

<li><a href="https://www.theregister.com/2025/05/22/irish_data_protection_commission_gives/">Irish privacy watchdog OKs Meta to train AI on EU folks' posts</a></li>
</ul>
<p>The researchers describe Meta's approach thus:</p>
<blockquote>
<ol>

<li>The user opens the native Facebook or Instagram app, which eventually is sent to the background and creates a background service to listen for incoming traffic on a TCP port (12387 or 12388) and a UDP port (the first unoccupied port in 12580-12585). Users must be logged-in with their credentials on the apps.</li>

<li>The user opens their browser and visits a website integrating the Meta Pixel.</li>

<li>At this stage, websites may ask for consent depending on the website's and visitor's locations.</li>

<li>The Meta Pixel script sends the <a target="_blank" rel="nofollow" href="https://localmess.github.io/#about_fbp">_fbp cookie</a> to the native Instagram or Facebook app via WebRTC (STUN) <a target="_blank" rel="nofollow" href="https://webrtchacks.com/not-a-guide-to-sdp-munging/">SDP Munging</a>.</li>

<li>The Meta Pixel script also sends the _fbp value in a request to https://www.facebook.com/tr along with other parameters such as page URL (dl), website and browser metadata, and the <a target="_blank" rel="nofollow" href="https://web.archive.org/web/20250531104925/https://developers.facebook.com/docs/meta-pixel/reference/">event type</a> (ev) (e.g., PageView, AddToCart, Donate, Purchase).</li>

<li>The Facebook or Instagram apps receive the _fbp cookie from the Meta Pixel JavaScript running on the browser. The apps transmit _fbp as a GraphQL mutation to (https://graph[.]facebook[.]com/graphql) along with other persistent user identifiers, linking users' fbp ID (web visit) with their Facebook or Instagram account.</li>
</ol>
</blockquote>
<p>Researchers observed Meta implementing this technique starting in September 2024, transmitting data via HTTP. Third-party developers working with Meta APIs noted and questioned the behavior in <a target="_blank" rel="nofollow" href="https://web.archive.org/web/20250531105747/https://developers.facebook.com/community/threads/317050484803752/">forum</a> <a target="_blank" rel="nofollow" href="https://web.archive.org/web/20250531105711/https://developers.facebook.com/community/threads/937149104821259/">posts</a> at the time.</p>
<p>HTTP-based data transmission using this technique supposedly ended the following month, but other methods of transmission (WebSocket, WebRTC STUN (w/ SDP Munging), and WebRTC TURN (w/o SDP Munging)) were identified in subsequent months.</p>
<p>Presently, however, Meta's use of these techniques appears to have halted. According to the researchers, "As of June 3rd 7:45 CEST, Meta/Facebook Pixel script is no longer sending any packets or requests to localhost. The code responsible for sending the _fbp cookie has been almost completely removed."</p>
<p>Yandex's use of localhost-based tracking dates back to 2017, according to the researchers.</p>
<p><em>The Register</em> sought to ask Yandex media relations about the researchers' claims but our inquiry was bounced as spam.</p>
<p>The report authors note that their disclosure to Android browser vendors has led to several mitigations.</p>
<p>Chrome 137, which shipped May 26, 2025, includes countermeasures <a target="_blank" rel="nofollow" href="https://webrtc.googlesource.com/src.git/+/72d6d748ddbe5d7f63ba5f2dd1ce195a342c0a12">to block the SDP Munging</a> technique used by Meta Pixel, though these have only been made available to a subset of users participating in a gated field trial. A fix is currently being developed for Mozilla Firefox. Brave is unaffected as it <a target="_blank" rel="nofollow" href="https://brave.com/privacy-updates/27-localhost-permission/">requires consent for localhost</a> use. And DuckDuckGo has modified its blocklist to stop Yandex's scripts.</p>
<p>Beyond these, the authors suggest a Google <a target="_blank" rel="nofollow" href="https://github.com/explainers-by-googlers/local-network-access">proposal</a> to create a new "local network access" permission that could help mitigate localhost-based tracking in the future. A <a target="_blank" rel="nofollow" href="https://wicg.github.io/private-network-access/">prior proposal</a> along these lines ran into <a target="_blank" rel="nofollow" href="https://developer.chrome.com/blog/pna-on-hold">technical barriers</a>. ¬Æ</p>                                
                    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Brain aging shows nonlinear transitions, suggesting a midlife "critical window" (245 pts)]]></title>
            <link>https://www.pnas.org/doi/10.1073/pnas.2416433122</link>
            <guid>44175905</guid>
            <pubDate>Tue, 03 Jun 2025 23:37:04 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.pnas.org/doi/10.1073/pnas.2416433122">https://www.pnas.org/doi/10.1073/pnas.2416433122</a>, See on <a href="https://news.ycombinator.com/item?id=44175905">Hacker News</a></p>
Couldn't get https://www.pnas.org/doi/10.1073/pnas.2416433122: Error: Request failed with status code 403]]></description>
        </item>
        <item>
            <title><![CDATA[Precious Plastic Is in Trouble (286 pts)]]></title>
            <link>https://www.preciousplastic.com//news/problems-in-precious-plastic</link>
            <guid>44175773</guid>
            <pubDate>Tue, 03 Jun 2025 23:11:25 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.preciousplastic.com//news/problems-in-precious-plastic">https://www.preciousplastic.com//news/problems-in-precious-plastic</a>, See on <a href="https://news.ycombinator.com/item?id=44175773">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p><h2>is in trouble</h2></p><div><p>Hey world</p><p>This is a heavy message to send, but essential for the future of Precious Plastic. It will either make it or not. In this post I‚Äôll give a detailed overview of all the current problems we have, how it got to this point and whats next. A short summarised video is below. No <em>need</em> to watch, it's all in the text.<br>‚Äç</p></div><p><iframe src="https://www.youtube.com/embed/4gTd36cQLzY?rel=0&amp;controls=1&amp;autoplay=0&amp;mute=0&amp;start=0" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen="" title="Precious Plastic is in trouble. Really"></iframe></p><p><strong>Our last big development<br>‚Äç</strong>Lets start with our last big development. &nbsp;2020, when we released Version 4, this was our latest release. We worked about 1,5 year with over +100 people from around the world. We developed the first ‚ÄòPro‚Äô Machines, a Sheetpress, Starterkits, Business calculators, new moulds, products and more. Looking back this was quite a unique moment. A lot of passionate work was done by volunteers and everything was shared Open Source online for free. Here some of the things we made<br></p><p><img src="https://cdn.prod.website-files.com/5e004b03e5da773c54823bfb/673fa486e30e592d1032dd32_v4.jpg" loading="lazy" sizes="(max-width: 479px) 96vw, (max-width: 767px) 95vw, (max-width: 991px) 699.9921875px, (max-width: 1919px) 71vw, 799.9921875px" srcset="https://cdn.prod.website-files.com/5e004b03e5da773c54823bfb/673fa486e30e592d1032dd32_v4-p-500.jpg 500w, https://cdn.prod.website-files.com/5e004b03e5da773c54823bfb/673fa486e30e592d1032dd32_v4-p-800.jpg 800w, https://cdn.prod.website-files.com/5e004b03e5da773c54823bfb/673fa486e30e592d1032dd32_v4-p-1080.jpg 1080w, https://cdn.prod.website-files.com/5e004b03e5da773c54823bfb/673fa486e30e592d1032dd32_v4-p-1600.jpg 1600w, https://cdn.prod.website-files.com/5e004b03e5da773c54823bfb/673fa486e30e592d1032dd32_v4-p-2000.jpg 2000w, https://cdn.prod.website-files.com/5e004b03e5da773c54823bfb/673fa486e30e592d1032dd32_v4.jpg 2004w" alt=""></p><p><img src="https://s11.gifyu.com/images/SGTIk.gif"></p><div><p>With a relatively small amount of money, we reached a global impact in 2023 of over 1100 organzations in 56 countries, who recycled 1.400.000KG Plastic, they generated together a revenue of +$3.7 Million, employed 530 people, works with 3.405 volunteers and built 1.175 machines. (And this is only from the workspaces who shared their data last year) Learn more about our impact <a href="https://www.preciousplastic.com/impact/2024" target="_blank">here</a>.<br>‚Äç<strong><br>‚òùÔ∏èThis was the good part. Lets get into our problems</strong></p><p> <strong>How we work<br></strong>In order to understand our problems it‚Äôs important to know how Precious Plastic is developed since its unusual. We work in what we call Versions. Version 1 got released in 2013 and the latest one 4 years ago. The principle is, we develop a lot of new things, share them online for free. And then whoever was involved takes a holiday or goes off to something else. They really have to go because at that point we spend all our money. The workspace is empty, the work drops, nothing new is made, we wait. Up until now for some magical reason we always received a gift to continue moving forward. From winning awards, to getting a big workspace donated. Whenever we got enough resources we assembled a team and started developing again.. Each time the team and amount of work grew. Until the latest version, here the problems started.</p></div><p><img src="https://cdn.prod.website-files.com/5e004b03e5da773c54823bfb/673fa6cb2fc0a22da432f1f6_work%20done%20chart.jpg" loading="lazy" sizes="(max-width: 479px) 96vw, (max-width: 767px) 95vw, (max-width: 991px) 699.9921875px, (max-width: 1919px) 71vw, 799.984375px" srcset="https://cdn.prod.website-files.com/5e004b03e5da773c54823bfb/673fa6cb2fc0a22da432f1f6_work%20done%20chart-p-500.jpg 500w, https://cdn.prod.website-files.com/5e004b03e5da773c54823bfb/673fa6cb2fc0a22da432f1f6_work%20done%20chart-p-800.jpg 800w, https://cdn.prod.website-files.com/5e004b03e5da773c54823bfb/673fa6cb2fc0a22da432f1f6_work%20done%20chart-p-1080.jpg 1080w, https://cdn.prod.website-files.com/5e004b03e5da773c54823bfb/673fa6cb2fc0a22da432f1f6_work%20done%20chart-p-1600.jpg 1600w, https://cdn.prod.website-files.com/5e004b03e5da773c54823bfb/673fa6cb2fc0a22da432f1f6_work%20done%20chart-p-2000.jpg 2000w, https://cdn.prod.website-files.com/5e004b03e5da773c54823bfb/673fa6cb2fc0a22da432f1f6_work%20done%20chart-p-2600.jpg 2600w, https://cdn.prod.website-files.com/5e004b03e5da773c54823bfb/673fa6cb2fc0a22da432f1f6_work%20done%20chart-p-3200.jpg 3200w, https://cdn.prod.website-files.com/5e004b03e5da773c54823bfb/673fa6cb2fc0a22da432f1f6_work%20done%20chart.jpg 4247w" alt=""></p><div><p><strong>When the Problems started<br>‚Äç</strong>We worked in versions not because we wanted to, but because we had to. We share everything online for free because we believe recycling knowledge should be available for everyone. As a consequence we don‚Äôt generate enough income to pay a team all year around. But after Version 4 a small group of seven dedicated ambitious volunteers wanted to try and sustain Precious Plastic all year around. So we can continue development and have a bigger impact to reduce plastic waste. This by itself is a hard job but we also had some extra problems on the way</p><p>‚Äç<strong>#1 No workspace<br>‚Äç</strong>The team was ready to continue work. But just a few weeks later Covid-19 came to the world, but this wasn‚Äôt our main problem. Our problem was Chrome-6, a chemical the municipality found in the paint from the building that was applied 40 years ago. Which meant we had to leave the workspace fast, and the building was large, we had a lot of machines and items to sell, in a short amount of time, during lockdowns. This meant we had to sell many things below value since that period most people were looking to buy bread machines, not robot arms. After the exhausting/rushed job of leaving our workspace we could stay in the garage/shed in the house of one of the team members in France. It was nice we had a base to continue, but it was much smaller and temporary. It was quite a downgrade.</p></div><p><img src="https://cdn.prod.website-files.com/5e004b03e5da773c54823bfb/673fa6f962c412ee92336560_workspace.jpg" loading="lazy" sizes="(max-width: 479px) 96vw, (max-width: 767px) 95vw, (max-width: 991px) 699.984375px, (max-width: 1919px) 71vw, 799.9921875px" srcset="https://cdn.prod.website-files.com/5e004b03e5da773c54823bfb/673fa6f962c412ee92336560_workspace-p-500.jpg 500w, https://cdn.prod.website-files.com/5e004b03e5da773c54823bfb/673fa6f962c412ee92336560_workspace-p-800.jpg 800w, https://cdn.prod.website-files.com/5e004b03e5da773c54823bfb/673fa6f962c412ee92336560_workspace-p-1080.jpg 1080w, https://cdn.prod.website-files.com/5e004b03e5da773c54823bfb/673fa6f962c412ee92336560_workspace-p-1600.jpg 1600w, https://cdn.prod.website-files.com/5e004b03e5da773c54823bfb/673fa6f962c412ee92336560_workspace-p-2000.jpg 2000w, https://cdn.prod.website-files.com/5e004b03e5da773c54823bfb/673fa6f962c412ee92336560_workspace-p-2600.jpg 2600w, https://cdn.prod.website-files.com/5e004b03e5da773c54823bfb/673fa6f962c412ee92336560_workspace-p-3200.jpg 3200w, https://cdn.prod.website-files.com/5e004b03e5da773c54823bfb/673fa6f962c412ee92336560_workspace.jpg 3893w" alt=""></p><div><p><strong>#2 No business model<br>‚Äç</strong>One of the main goals of this small team was trying to find a business model that would serve Precious Plastic‚Äôs mission, bring income to sustain a team and not compete with the rest of the community. The last one is difficult, because our most logical and common model would be to sell machines and moulds. There are not many Open Source Hardware projects like Precious Plastic in the world. And the ones that exist mostly sell their products. Arduino sells their circuit boards, Prusa sells 3d printers. However in our case we‚Äôre also building a global network of machine builders that can provide local recycling equipment. We didn't want to compete with our own community.</p><p>The plan was to do Projects or as we called them, Collabs. It meant to help others set up their projects. Sort of a consultancy. The team setup quite a few cool projects, from a refugee camp in a desert in Algeria to a big Sheetpress in Nigeria. And meanwhile released a few new machine drawings. The model somewhat worked, we continued development and could do meaningful work. However it was financially always tight and often didn‚Äôt bring in enough money to sustain the team (that was getting paid minimum Dutch wage). But what really broke it was the next problem</p><p>‚Äç<strong>#3 Lawsuit<br>‚Äç</strong>As the team was setting up projects we worked with different clients around the world. Working for clients was completely new for us and we're learning along the way. And one of the first projects we setup was in Manhattan, New York for a cosmetic company to recycle their packaging. We helped to set it up, got machines from a community member and local people ran it. However after a period of time an accident happened with someone using the machine, which was very unfortunate. And in the US, especially NY this means you need to get lawyers. What happened, who is responsible? Is it the company that hired us all, Precious Plastic (back then operating under One Army Entity) for organising it, was it a result of bad operational instructions, misuse of the machine or a fault in the machine from the community member? We analysed it and are convinced that we are not to blame. But we do not know what a judge is going to say. Meanwhile this has been going on for the last 2 years. Lots of paperwork and documents need to be filed with lawyers that charge up to $600/h, sending emails got painful. Being in a lawsuit in New York is very costly. On top of that we didn't have insurance, meaning we have to pay for it from our own tiny pockets.</p><p>‚Äç<strong>#4 Software, heavily underestimated<br>‚Äç</strong>During Version 4 we started developing our own Community Platform. It‚Äôs software that is the digital home of our community that helps members to document, share knowledge and find others to collaborate with. It‚Äôs developed in collaboration with our other projects and there is a lot more information about it here. Anyway this has been a massive project and the original complexity of it was underestimated. Took waaay more effort than we thought. In the recent years we‚Äôve been hard at work to catch up and the platform got much more mature with many more features. But realistically this has been a hit on our online community since the ‚Äòdigital home‚Äô wasn‚Äôt good enough to host everyone. We invested a lot in this so far and will continue to be a big project so please use the platform, give feedback so we can improve it and help us code it.</p><p>‚Äç<strong>#5 Open Source community<br>‚Äç</strong>At Precious Plastic we want to enable more people to recycle plastic. Plastic waste is a big global problem and needs many people collaborating in every corner of the world to fix it. That's why we give everything Open Source for free so everyone has access. On top of that a big part of Version 4 was to make sure that the people that start recycling workspaces can financially sustain themselves. Because If they can continue to recycle on a daily basis it means less plastic waste. We want all those hard working workspaces to succeed and provide business plans, calculators and instruction to help them. A few years later this resulted in hundreds of workspaces around the world that started a business who manage to recycle on a daily basis, which is great.&nbsp;</p><p>We went all-in on giving. And believe(d?) that sharing Open Source will bring contributions back one way or another. Contributions to support the Precious Plastic Community can be made in various ways.&nbsp;From a financial donation once a recycle-business is profitable, to giving credit or sharing back their knowledge. And many members do contribute something back which is great. However we‚Äôve also been observing quite some established organisations that take more than they give, building business around Precious Plastic but not contributing anything back. It‚Äôs allowed, since it‚Äôs all Open-Source. But the mentality of only taking things and not contributing will eventually kill a community-driven project like this.&nbsp;</p><p>‚Äç<strong>#6 It is bad designed<br>‚Äç</strong>We don‚Äôt blame those for not contributing enough back. We see this as a fault on us. The project wasn‚Äôt designed to have a healthy financial model and relation with the community. We were always fully focussed on giving to the community, not us being a financially sustainable organisation. Funny example of this is the recent <a href="https://pposf.preciousplastic.com/" target="_blank">PPOSF</a> (Precious Plastic Open Source Fund). We received a ‚Ç¨100K donation. Which was amazing, but we decided to give it all to the community so they can continue developing their projects. Not to sustain the organisation itself. You could see this as a humble move from us, give it to the community. But it isn‚Äôt, it‚Äôs ineffective, because now we have to bother you with our problems. Ideally we don‚Äôt have to do this and you don‚Äôt have to worry about us.&nbsp;</p><p><strong>#7 No long term team<br>‚Äç</strong>As you can imagine all of the above problems make it hard for a team member to have a long term perspective in Precious Plastic. Even though the team has been very small, effective and works with many volunteers it has continued to be a struggle to pay everyone every month without worrying. Over time this brings lots of stress and uncertainty, especially if the team members by themselves grow up and need more stability in life.&nbsp;</p><p>‚Äç<strong>Our current setup<br>‚Äç</strong>Precious Plastic is a non-profit for public good (ANBI) setup in the Netherlands. <br>So what does our team and community look like? Here is a funny way to look at it:<br>‚Äç<br><strong>Precious Plastic Community</strong><br>‚¨•+ 1000 workspaces around the world<br>‚¨•530 people employed, 3000 volunteers<br>‚¨•Totalling + $3.7 Million Revenue last year</p><p><strong>Precious Plastic Organisation</strong><br>‚¨•3 full-time people in the team<br>‚¨•Quarterly running costs ‚Ç¨30K<br>‚¨•6 Months before out of money<br>‚¨•No workspace. Everyone is fully remote</p><p>As you can see not many team members to manage a large community. There are many areas we should work on but simply cannot. The team spends most of its time making sure just the basics are up and running and community members can continue to recycle and use our tools. And even with this small effective team we only have enough money to sustain for the coming 6 months. The future doesn‚Äôt look good.<br>‚Äç</p></div><p><img src="https://cdn.prod.website-files.com/5e004b03e5da773c54823bfb/673fa8bb671bebd39224838b_next%20options1.jpg" loading="lazy" sizes="(max-width: 479px) 96vw, (max-width: 767px) 95vw, (max-width: 991px) 699.984375px, (max-width: 1919px) 71vw, 799.984375px" srcset="https://cdn.prod.website-files.com/5e004b03e5da773c54823bfb/673fa8bb671bebd39224838b_next%20options1-p-500.jpg 500w, https://cdn.prod.website-files.com/5e004b03e5da773c54823bfb/673fa8bb671bebd39224838b_next%20options1-p-800.jpg 800w, https://cdn.prod.website-files.com/5e004b03e5da773c54823bfb/673fa8bb671bebd39224838b_next%20options1-p-1080.jpg 1080w, https://cdn.prod.website-files.com/5e004b03e5da773c54823bfb/673fa8bb671bebd39224838b_next%20options1-p-1600.jpg 1600w, https://cdn.prod.website-files.com/5e004b03e5da773c54823bfb/673fa8bb671bebd39224838b_next%20options1-p-2000.jpg 2000w, https://cdn.prod.website-files.com/5e004b03e5da773c54823bfb/673fa8bb671bebd39224838b_next%20options1.jpg 2339w" alt=""></p><div><p><strong>What is next?<br>‚Äç</strong>I‚Äôve been thinking about this. What should Precious Plastic do?&nbsp;<br>‚Äç<strong>1: ‚ò†Ô∏è Let Precious Plastic Die:</strong> It build a global community of recyclers, it achieved its mission and that‚Äôs it. We learned valuable lessons and the community will probably stay online for a bit but would slowly fade away.<br>‚Äç<strong>2: üí™ Push it to the next level:</strong> There is still lots of plastic waste around the world. We need way more people recycling and R&amp;D on other plastic types. The community needs to grow.</p><p>To be honest I personally could be at peace with both of these directions. It‚Äôs amazing what Precious Plastic developed with a relatively small budget and passionate people volunteering their time. Many lessons learned. But an Open Source project like this needs many people caring for it in order to stay alive. If there isn't a large supportive community it will naturally die. It goes beyond the power of an individual. </p><p>That said, what many people might not realise, is that we would waste an unused potential we can currently unlock. We spend years building a global community of recyclers. Rolling out new improved tools has a much higher impact than before, we can reach the right people in many areas in the world. Plus we have clear visibility on our problems and are after all these years very close to having a healthy organisational cycle, see the chart below. All of this makes me think about giving it one last push to finish it. Version 5.&nbsp;</p></div><p><img src="https://cdn.prod.website-files.com/5e004b03e5da773c54823bfb/673fa82f77198680abac2722_circle.jpg" loading="lazy" sizes="(max-width: 479px) 96vw, (max-width: 767px) 95vw, (max-width: 991px) 699.9921875px, (max-width: 1919px) 71vw, 799.9921875px" srcset="https://cdn.prod.website-files.com/5e004b03e5da773c54823bfb/673fa82f77198680abac2722_circle-p-500.jpg 500w, https://cdn.prod.website-files.com/5e004b03e5da773c54823bfb/673fa82f77198680abac2722_circle-p-800.jpg 800w, https://cdn.prod.website-files.com/5e004b03e5da773c54823bfb/673fa82f77198680abac2722_circle-p-1080.jpg 1080w, https://cdn.prod.website-files.com/5e004b03e5da773c54823bfb/673fa82f77198680abac2722_circle-p-1600.jpg 1600w, https://cdn.prod.website-files.com/5e004b03e5da773c54823bfb/673fa82f77198680abac2722_circle-p-2000.jpg 2000w, https://cdn.prod.website-files.com/5e004b03e5da773c54823bfb/673fa82f77198680abac2722_circle-p-2600.jpg 2600w, https://cdn.prod.website-files.com/5e004b03e5da773c54823bfb/673fa82f77198680abac2722_circle.jpg 2841w" alt=""></p><p><strong>What is Version 5?</strong><br>We have many ideas for a Version 5 and would love to work on this. But as you can see from this long text (thanks for reading all the way) we are in a big dip and have some big problems. Whatever we need to develop to get out of this needs to serve the community + the organisation itself. It will be made in a way that it can financially sustain itself afterwards. Something we never took into account. It will mean rebuilding things from the ground up, which requires much more help and resources than before. It would be the biggest thing we ever made as you can see below in the graph. Our team is small, our community is large. We can only do this if people like you are willing to support and help out.<br></p><p><img src="https://cdn.prod.website-files.com/5e004b03e5da773c54823bfb/673fa81b44c7ec88e59ad907_next.jpg" loading="lazy" sizes="(max-width: 479px) 96vw, (max-width: 767px) 95vw, (max-width: 991px) 699.9921875px, (max-width: 1919px) 71vw, 799.9921875px" srcset="https://cdn.prod.website-files.com/5e004b03e5da773c54823bfb/673fa81b44c7ec88e59ad907_next-p-500.jpg 500w, https://cdn.prod.website-files.com/5e004b03e5da773c54823bfb/673fa81b44c7ec88e59ad907_next-p-800.jpg 800w, https://cdn.prod.website-files.com/5e004b03e5da773c54823bfb/673fa81b44c7ec88e59ad907_next-p-1080.jpg 1080w, https://cdn.prod.website-files.com/5e004b03e5da773c54823bfb/673fa81b44c7ec88e59ad907_next-p-1600.jpg 1600w, https://cdn.prod.website-files.com/5e004b03e5da773c54823bfb/673fa81b44c7ec88e59ad907_next-p-2000.jpg 2000w, https://cdn.prod.website-files.com/5e004b03e5da773c54823bfb/673fa81b44c7ec88e59ad907_next-p-2600.jpg 2600w, https://cdn.prod.website-files.com/5e004b03e5da773c54823bfb/673fa81b44c7ec88e59ad907_next-p-3200.jpg 3200w, https://cdn.prod.website-files.com/5e004b03e5da773c54823bfb/673fa81b44c7ec88e59ad907_next.jpg 5389w" alt=""></p><div><p><strong>How can you help? <br></strong>The first step is gathering support from the community. If not ones cares about Precious Plastic there is no reason for us to continue to work on it. You can do this by "showing your support". Next we need to find resources to develop the next phase of the project, so you can help us "raise funds for V5".<strong></strong></p></div><div data-hover="false" data-delay="0"><div><p><img src="https://cdn.prod.website-files.com/5e004b03e5da773c54823bfb/674c67fa11910bf4b8d514c0_support.png" loading="lazy" alt=""></p><p>Show your support</p></div><nav><div data-hover="false" data-delay="0"><div><p>Subscribe to our Youtube Channel</p></div><nav><div><p>The video you just watched is on the One Army Channel. However, Precious Plastic now has its own channel, which not many people know about. A big boost to our channel helps with the YouTube algorithm, allowing us to share more videos in the future.&nbsp;</p><p>‚Äç<a href="https://www.youtube.com/@Precious_PlasticHQ/?sub_confirmation=1" target="_blank">Click here</a> to subscribe.</p></div></nav></div><div data-hover="false" data-delay="0"><nav><p>We‚Äôre focused on recycling more plastic and making a real impact. Your support helps us drive this mission forward and generate actual income for our team.&nbsp;If you want to make a difference, <a href="https://www.preciousplastic.com/support" target="_blank">click here</a> to donate.</p></nav></div><div data-hover="false" data-delay="0"><nav><p><a href="https://www.patreon.com/one_army/membership" target="_blank">Join us on Patreon</a> to support our mission monthly. Your contribution helps create a stable income for our work. Plus, you'll get exclusive updates on our progress.</p></nav></div><div data-hover="false" data-delay="0"><nav><div><p>If you're an individual, <a href="https://bazar.preciousplastic.com/products/" target="_blank">buy recycled plastic products</a> on the Bazar, like carabiners, keychains, earrings, rulers, phone cases, anything really. And if you're a workspace looking to expand, consider buying machines on the Bazar. Remember, 5% of every sale goes to Precious Plastic, which directly supports small scale plastic recycling.</p><p>Sellers, this is your chance to contribute‚Äîsell your items on the Bazar. Your sales make a big difference; that 5% income is used to improve the online marketplace itself. Avoiding the need for all the individual workspaces having to setup their own webshops. Together, we can create one strong marketplace that benefits us all.</p><p>‚Äç</p></div></nav></div><div data-hover="false" data-delay="0"><nav><p>The lawsuit in New York is still ongoing. We are currently facing significant costs and need pro-bono assistance from lawyers in the Netherlands or the US to help us navigate this process. If you can assist us, please send us an email to <a href="mailto:hello@preciousplastic.com?subject=Legal%20help">hello@preciousplastic.com</a></p></nav></div><div data-hover="false" data-delay="0"><nav><div><p>Our Community Platform is a big software project, and honestly, it's more complex than we initially thought. Building the infrastructure to host a large global community. We're doing it all open source so others can use it and help out. If you're interested in helping build this, we could really use your skills.Check out the details on <a href="https://github.com/ONEARMY/community-platform" target="_blank">GitHub</a>.</p><p>‚≠êÔ∏è For an easy help&gt; Giving a start to the repository is useful to attract more contributors!</p></div></nav></div><div data-hover="false" data-delay="0"><div><p>Use the Community Platform</p></div><nav><div><p>Interaction on the Community Platform is essential, we're building the home for our online community. By answering Questions and uploading knowledge‚Äîsuch as How-tos, Research and replying to comments‚Äîyou contribute to a collective resource that reflects the global activity of the community. </p><p>All this activity helps gather valuable feedback that steers our ongoing improvements, we release weekly fixes based on user insights.</p><p>Start by helping out to answer some questions <a href="https://community.preciousplastic.com/questions" target="_blank">here</a>.</p></div></nav></div></nav></div><div data-hover="false" data-delay="0"><div><p><img src="https://cdn.prod.website-files.com/5e004b03e5da773c54823bfb/674c6f45d80db1b0b342e19a_money.png" loading="lazy" alt=""></p><p>Help us raise funds for V5</p></div><nav><div data-hover="false" data-delay="0"><nav><div><p>We need your help to locate grants. There are numerous grants available, but with a small team, it‚Äôs tough to find them all. Let‚Äôs crowdsource our efforts and share what we discover. If you have any information, please contribute by adding it <a href="https://community.preciousplastic.com/questions/who-can-help-us-to-find-grants-for-v5" target="_blank">here</a>.</p><p>And if you‚Äôre a grant writer that wants to help, send us a mail: <a href="mailto:hello@preciousplastic.com?subject=Help%20with%20grants">hello@preciousplastic.com</a></p></div></nav></div><div data-hover="false" data-delay="0"><nav><div><p>If you can make a large donation, it would be the biggest support we could receive right now. This help would allow us to stay on track in the short term, enabling us to focus on long-term goals. Our aim is to become independent of donors and self-sustaining. Additionally, your contribution could be tax-deductible as we are a Non-Profit (ANBI) from the Netherlands. <br>Our 3 year vision is to triple the impact of small-scale plastic recycling globally by releasing Version 5 with a total budget of 2.1 million euros.</p><p>If you're interested, send us an email to <a href="mailto:hello@preciousplastic.com?subject=Precious%20Plastic%20Donation">hello@preciousplastic.com</a></p></div></nav></div><div data-hover="false" data-delay="0"><nav><p>If you want to collaborate with us, we welcome partnerships. Whether it's a video, sponsorship of Version 5, or featuring your logo on our website, we are open to various forms of collaboration‚Äîthe bigger the initiative, the better. For inquiries, please send an email to <a href="mailto:hello@preciousplastic.com?subject=Collaborate">hello@preciousplastic.com</a> or fill the form <a href="https://www.preciousplastic.com/custom-solutions" target="_blank">here</a>.</p></nav></div><div data-hover="false" data-delay="0"><nav><div><p>If you want, you can send us crypto. We‚Äôre easy with crypto and love receiving it. Here are our wallet addresses:&nbsp;<br><strong>Bitcoin</strong>: 3NxqZ3xW8PEPtbCrDHPrL7srjhN4U4iZwp<br><strong>Ethereum</strong>: 0x28CDdE98313a9ef878076f88d3AEFa3714185123<br>Tether (USDT): 0x65f9aB8B37D98F8D334AB97C74BF160249f8298D</p><p>If you need another one, or want to double check before sending.<br>Send us an email at <a href="mailto:hello@preciousplastic.com?subject=Crypto%20Donation">hello@preciousplastic.com</a></p></div></nav></div></nav></div><div><p>If enough action is taken we can move forward and will share a more detailed plan for Version 5.<br>If not, we are willing to accept that the project will just die.<br>We will keep you posted with updates.<br>‚Äç<br>Thank you community</p><p>Dave</p><p>*It‚Äôs a complex problem to explain spanned over multiple years. If there are ideas, questions or comments go <a href="https://community.preciousplastic.com/questions/questions-on-the-article-with-our-problems" target="_blank">here</a></p></div></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Show HN: Ephe ‚Äì A minimalist open-source Markdown paper for today (130 pts)]]></title>
            <link>https://github.com/unvalley/ephe</link>
            <guid>44175557</guid>
            <pubDate>Tue, 03 Jun 2025 22:41:33 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/unvalley/ephe">https://github.com/unvalley/ephe</a>, See on <a href="https://news.ycombinator.com/item?id=44175557">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
          <nav aria-label="Global">
            <ul>


                <li>
      

      <div>
          <div>

                <ul>
                    <li>
  <a data-analytics-event="{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;github_copilot&quot;,&quot;context&quot;:&quot;product&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;github_copilot_link_product_navbar&quot;}" href="https://github.com/features/copilot">
      
      <div>
          <p>
            GitHub Copilot
          </p><p>
        Write better code with AI
      </p></div>

    
</a></li>

                    <li>
  <a data-analytics-event="{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;github_models&quot;,&quot;context&quot;:&quot;product&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;github_models_link_product_navbar&quot;}" href="https://github.com/features/models">
      
      <div>
          <p>
            GitHub Models
              <span>
                New
              </span>
          </p><p>
        Manage and compare prompts
      </p></div>

    
</a></li>

                    <li>
  <a data-analytics-event="{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;github_advanced_security&quot;,&quot;context&quot;:&quot;product&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;github_advanced_security_link_product_navbar&quot;}" href="https://github.com/security/advanced-security">
      
      <div>
          <p>
            GitHub Advanced Security
          </p><p>
        Find and fix vulnerabilities
      </p></div>

    
</a></li>

                    <li>
  <a data-analytics-event="{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;actions&quot;,&quot;context&quot;:&quot;product&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;actions_link_product_navbar&quot;}" href="https://github.com/features/actions">
      
      <div>
          <p>
            Actions
          </p><p>
        Automate any workflow
      </p></div>

    
</a></li>

                    <li>
  <a data-analytics-event="{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;codespaces&quot;,&quot;context&quot;:&quot;product&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;codespaces_link_product_navbar&quot;}" href="https://github.com/features/codespaces">
      
      <div>
          <p>
            Codespaces
          </p><p>
        Instant dev environments
      </p></div>

    
</a></li>

                </ul>
              </div>
          <div>

                <ul>
                    <li>
  <a data-analytics-event="{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;issues&quot;,&quot;context&quot;:&quot;product&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;issues_link_product_navbar&quot;}" href="https://github.com/features/issues">
      
      <div>
          <p>
            Issues
          </p><p>
        Plan and track work
      </p></div>

    
</a></li>

                    <li>
  <a data-analytics-event="{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;code_review&quot;,&quot;context&quot;:&quot;product&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;code_review_link_product_navbar&quot;}" href="https://github.com/features/code-review">
      
      <div>
          <p>
            Code Review
          </p><p>
        Manage code changes
      </p></div>

    
</a></li>

                    <li>
  <a data-analytics-event="{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;discussions&quot;,&quot;context&quot;:&quot;product&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;discussions_link_product_navbar&quot;}" href="https://github.com/features/discussions">
      
      <div>
          <p>
            Discussions
          </p><p>
        Collaborate outside of code
      </p></div>

    
</a></li>

                    <li>
  <a data-analytics-event="{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;code_search&quot;,&quot;context&quot;:&quot;product&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;code_search_link_product_navbar&quot;}" href="https://github.com/features/code-search">
      
      <div>
          <p>
            Code Search
          </p><p>
        Find more, search less
      </p></div>

    
</a></li>

                </ul>
              </div>
          

      </div>
</li>


                <li>
      

      
</li>


                <li>
      

      <div>
                    <p><span id="resources-explore-heading">Explore</span></p><ul aria-labelledby="resources-explore-heading">
                    <li>
  <a target="_blank" data-analytics-event="{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;learning_pathways&quot;,&quot;context&quot;:&quot;resources&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;learning_pathways_link_resources_navbar&quot;}" href="https://resources.github.com/learn/pathways">
      Learning Pathways

    
</a></li>

                    <li>
  <a target="_blank" data-analytics-event="{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;events_amp_webinars&quot;,&quot;context&quot;:&quot;resources&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;events_amp_webinars_link_resources_navbar&quot;}" href="https://resources.github.com/">
      Events &amp; Webinars

    
</a></li>

                    <li>
  <a data-analytics-event="{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;ebooks_amp_whitepapers&quot;,&quot;context&quot;:&quot;resources&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;ebooks_amp_whitepapers_link_resources_navbar&quot;}" href="https://github.com/resources/whitepapers">
      Ebooks &amp; Whitepapers

    
</a></li>

                    <li>
  <a data-analytics-event="{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;customer_stories&quot;,&quot;context&quot;:&quot;resources&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;customer_stories_link_resources_navbar&quot;}" href="https://github.com/customer-stories">
      Customer Stories

    
</a></li>

                    <li>
  <a target="_blank" data-analytics-event="{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;partners&quot;,&quot;context&quot;:&quot;resources&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;partners_link_resources_navbar&quot;}" href="https://partner.github.com/">
      Partners

    
</a></li>

                    <li>
  <a data-analytics-event="{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;executive_insights&quot;,&quot;context&quot;:&quot;resources&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;executive_insights_link_resources_navbar&quot;}" href="https://github.com/solutions/executive-insights">
      Executive Insights

    
</a></li>

                </ul>
              </div>
</li>


                <li>
      

      <div>
              <div>

                <ul>
                    <li>
  <a data-analytics-event="{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;github_sponsors&quot;,&quot;context&quot;:&quot;open_source&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;github_sponsors_link_open_source_navbar&quot;}" href="https://github.com/sponsors">
      
      <div>
          <p>
            GitHub Sponsors
          </p><p>
        Fund open source developers
      </p></div>

    
</a></li>

                </ul>
              </div>
              <div>

                <ul>
                    <li>
  <a data-analytics-event="{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;the_readme_project&quot;,&quot;context&quot;:&quot;open_source&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;the_readme_project_link_open_source_navbar&quot;}" href="https://github.com/readme">
      
      <div>
          <p>
            The ReadME Project
          </p><p>
        GitHub community articles
      </p></div>

    
</a></li>

                </ul>
              </div>
              
          </div>
</li>


                <li>
      

      <div>

                <ul>
                    <li>
  <a data-analytics-event="{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;enterprise_platform&quot;,&quot;context&quot;:&quot;enterprise&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;enterprise_platform_link_enterprise_navbar&quot;}" href="https://github.com/enterprise">
      
      <div>
          <p>
            Enterprise platform
          </p><p>
        AI-powered developer platform
      </p></div>

    
</a></li>

                </ul>
              </div>
</li>


                <li>
    <a data-analytics-event="{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;pricing&quot;,&quot;context&quot;:&quot;global&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;pricing_link_global_navbar&quot;}" href="https://github.com/pricing">Pricing</a>
</li>

            </ul>
          </nav>

        <div>
                


<qbsearch-input data-scope="repo:unvalley/ephe" data-custom-scopes-path="/search/custom_scopes" data-delete-custom-scopes-csrf="XFrKT3G01DJsKUpu5mMdaZfi9Z9d5c7QqHiaLJn05KsWZ_NavEm4m590ZAsiYrsoqCRuNrAjb31wsVhtwR4bug" data-max-custom-scopes="10" data-header-redesign-enabled="false" data-initial-value="" data-blackbird-suggestions-path="/search/suggestions" data-jump-to-suggestions-path="/_graphql/GetSuggestedNavigationDestinations" data-current-repository="unvalley/ephe" data-current-org="" data-current-owner="unvalley" data-logged-in="false" data-copilot-chat-enabled="false" data-nl-search-enabled="false" data-retain-scroll-position="true">
  <div data-modal-dialog-overlay="" data-action="click:qbsearch-input#searchInputContainerClicked">
  <modal-dialog data-action="close:qbsearch-input#handleClose cancel:qbsearch-input#handleClose" data-target="qbsearch-input.searchSuggestionsDialog" role="dialog" id="search-suggestions-dialog" aria-modal="true" aria-labelledby="search-suggestions-dialog-header" data-view-component="true">
      <h2 id="search-suggestions-dialog-header">Search code, repositories, users, issues, pull requests...</h2>
    
</modal-dialog></div>
  
  <div>
    
<dialog-helper>
  <dialog data-target="qbsearch-input.feedbackDialog" data-action="close:qbsearch-input#handleDialogClose cancel:qbsearch-input#handleDialogClose" id="feedback-dialog" aria-modal="true" aria-labelledby="feedback-dialog-title" aria-describedby="feedback-dialog-description" data-view-component="true">
    <div data-view-component="true">
    <p>
      <h2 id="feedback-dialog-title">
        Provide feedback
      </h2>
        
    </p>
    
  </div>
      <scrollable-region data-labelled-by="feedback-dialog-title">
        
      </scrollable-region>
      
</dialog></dialog-helper>

    <custom-scopes data-target="qbsearch-input.customScopesManager">
    
<dialog-helper>
  <dialog data-target="custom-scopes.customScopesModalDialog" data-action="close:qbsearch-input#handleDialogClose cancel:qbsearch-input#handleDialogClose" id="custom-scopes-dialog" aria-modal="true" aria-labelledby="custom-scopes-dialog-title" aria-describedby="custom-scopes-dialog-description" data-view-component="true">
    <div data-view-component="true">
    <p>
      <h2 id="custom-scopes-dialog-title">
        Saved searches
      </h2>
        <h2 id="custom-scopes-dialog-description">Use saved searches to filter your results more quickly</h2>
    </p>
    
  </div>
      <scrollable-region data-labelled-by="custom-scopes-dialog-title">
        
      </scrollable-region>
      
</dialog></dialog-helper>
    </custom-scopes>
  </div>
</qbsearch-input>


            

              <p><a href="https://github.com/signup?ref_cta=Sign+up&amp;ref_loc=header+logged+out&amp;ref_page=%2F%3Cuser-name%3E%2F%3Crepo-name%3E&amp;source=header-repo&amp;source_repo=unvalley%2Fephe" data-hydro-click="{&quot;event_type&quot;:&quot;authentication.click&quot;,&quot;payload&quot;:{&quot;location_in_page&quot;:&quot;site header menu&quot;,&quot;repository_id&quot;:null,&quot;auth_type&quot;:&quot;SIGN_UP&quot;,&quot;originating_url&quot;:&quot;https://github.com/unvalley/ephe&quot;,&quot;user_id&quot;:null}}" data-hydro-click-hmac="41d4782483eb3f1535a96863a9f3280c4bf9773f7897120b9b2f0c8c363da4b7" data-analytics-event="{&quot;category&quot;:&quot;Sign up&quot;,&quot;action&quot;:&quot;click to sign up for account&quot;,&quot;label&quot;:&quot;ref_page:/<user-name>/<repo-name>;ref_cta:Sign up;ref_loc:header logged out&quot;}">
                Sign up
              </a></p><p>
    <react-partial-anchor>
      <tool-tip id="tooltip-ba9e85f9-ced5-4945-b4ac-1c9a79fcbea7" for="icon-button-182d2309-5266-498f-9e19-e035d8196c09" popover="manual" data-direction="s" data-type="label" data-view-component="true">Appearance settings</tool-tip>

      <template data-target="react-partial-anchor.template">
        <link crossorigin="anonymous" media="all" rel="stylesheet" href="https://github.githubassets.com/assets/primer-react.b5e8a54636271e908e27.module.css">
<link crossorigin="anonymous" media="all" rel="stylesheet" href="https://github.githubassets.com/assets/appearance-settings.8edda24384d5c8bf99ee.module.css">

<react-partial partial-name="appearance-settings" data-ssr="false" data-attempted-ssr="false">
  
  <script type="application/json" data-target="react-partial.embeddedData">{"props":{}}</script>
  <div data-target="react-partial.reactRoot"></div>
</react-partial>

      </template>
    </react-partial-anchor>
  </p>

          </div>
      </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Deep learning gets the glory, deep fact checking gets ignored (539 pts)]]></title>
            <link>https://rachel.fast.ai/posts/2025-06-04-enzyme-ml-fails/index.html</link>
            <guid>44174965</guid>
            <pubDate>Tue, 03 Jun 2025 21:31:56 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://rachel.fast.ai/posts/2025-06-04-enzyme-ml-fails/index.html">https://rachel.fast.ai/posts/2025-06-04-enzyme-ml-fails/index.html</a>, See on <a href="https://news.ycombinator.com/item?id=44174965">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="quarto-content">
<!-- sidebar -->
<!-- margin-sidebar -->
    
<!-- main -->
<main id="quarto-document-content">




<p>Deep learning is glamorous and highly rewarded. If you train and evaluate a Transformer (a state-of-the-art language model) on a dataset of 22 million enzymes and then use it to predict the function of 450 unknown enzymes, you can publish your results in Nature (a very well-regarded publication). Your paper will be viewed 22,000 times and will be in the top 5% of all research outputs scored by Altmetric (a rating of how much attention online articles receive).</p>
<p>However, if you do the painstaking work of combing through someone else‚Äôs published work, and discovering that they are riddled with serious errors, including hundreds of incorrect predictions, you can post a pre-print to bioRxiv that will not receive even a fraction of the citations or views of the original. In fact, this is exactly what happened in the case of these two papers:</p>
<ul>
<li><a href="https://www.nature.com/articles/s41467-023-43216-z">Functional annotation of enzyme-encoding genes using deep learning with transformer layers | Nature Communications</a></li>
<li><a href="https://www.biorxiv.org/content/10.1101/2024.07.01.601547v2.full">Limitations of Current Machine-Learning Models in Predicting Enzymatic Functions for Uncharacterized Proteins | bioRxiv</a></li>
</ul>
<div>
<figure>
<p><img src="https://rachel.fast.ai/posts/2025-06-04-enzyme-ml-fails/altmetric.jpg"></p>
<figcaption>A Tale of two Altmetric Scores</figcaption>
</figure>
</div>
<p>This pair of papers on enzyme function prediction make for a fascinating case study on the limits of AI in biology and the harms of current publishing incentives. I will walk through some of the details below, although I encourage you to read the papers for yourself. This contrast is a stark reminder of how hard it can be to evaluate the legitimacy of AI results without deep domain expertise.</p>
<section id="the-problem-of-determining-enzyme-function">
<h2 data-anchor-id="the-problem-of-determining-enzyme-function">The Problem of Determining Enzyme Function</h2>
<p>Enzymes are what catalyze reactions, so they are crucial for making things happen in living organisms. Enzyme Commission (EC) numbers provide a hierarchical classification system for thousands of different functions. Given a sequence of amino acids (the building blocks of all proteins, including enzymes), can you predict what the EC number (and thus, the function) is? This seems like a problem that is custom-made for machine learning, with clearly defined inputs and outputs. Moreover, there is a rich dataset available, with over 22 million enzymes and their EC numbers listed in the online database UniProt.</p>
</section>
<section id="an-approach-with-transformers-ai-model">
<h2 data-anchor-id="an-approach-with-transformers-ai-model">An Approach with Transformers (AI model)</h2>
<p>A research paper used a transformer deep learning model to predict the functions of enzymes with previously unknown functions. It seemed like a good paper! The authors used a reasonable, well-regarded neural network architecture (two transformer encoders, two convolutional layers, and a linear layer) that had been adopted from BERT. They looked at regions with high attention to confirm that these were biologically significant, which suggests that the model had learned underlying meaning and provided interpretability. They used a standard training, validation, and test split on a dataset with millions of entries. The researchers then applied the model to a dataset where no ‚Äúground truth‚Äù was known to make ~450 novel predictions. For these novel predictions, they randomly selected three to test <em>in vitro</em> and confirmed that the predictions were accurate.</p>
<div>
<figure>
<p><img src="https://rachel.fast.ai/posts/2025-06-04-enzyme-ml-fails/kim-fig1a-4.jpg"></p>
<figcaption>A transformer model, shown on the left, was used to predict Enzyme Commission numbers for uncharacterized enzymes in E. coli. Three of these were tested in vitro (Fig 1a and Fig 4 from Kim, et al.)</figcaption>
</figure>
</div>
</section>
<section id="the-errors">
<h2 data-anchor-id="the-errors">The Errors</h2>
<p>The Transformer model in the Nature paper made hundreds of ‚Äúnovel‚Äù predictions that are almost certainly erroneous. The paper had followed a standard methodology of evaluating performance on a held-out test set, and did quite well on that (although later investigation suggests there may have been <a href="https://www.kaggle.com/code/alexisbcook/data-leakage">data leakage</a>). The results claimed for enzymes where no ground truth is known were full of errors.</p>
<p>For instance, the gene E. coli YjhQ was predicted to be a mycothiol synthase, but mycothiol is not synthesized by E. coli at all! The gene yciO, which evolved from the gene TsaC, had already been shown a decade earlier <em>in vivo</em> to not have the same function as TsaC, yet the Nature paper concluded it did have the same function.</p>
<p>Of the 450 ‚Äúnovel‚Äù results given in the Nature paper, 135 of these results were not novel at all; they were already listed in the online database UniProt. Another 148 showed unreasonably high levels of repetition, with the same very specific enzyme functions reappearing up to 12 times for genes of <em>E. coli</em>, which biologically implausible.</p>
<div>
<figure>
<p><img src="https://rachel.fast.ai/posts/2025-06-04-enzyme-ml-fails/de-crecy-fig5.jpg"></p>
<figcaption>Most of the ‚Äúnovel‚Äù results from the transformer paper were either not novel, unusually repetitious, or incorrect paralogs (Fig 5 from de Crecy, et al.)</figcaption>
</figure>
</div>
</section>
<section id="the-microbiology-detective">
<h2 data-anchor-id="the-microbiology-detective">The Microbiology Detective</h2>
<p>How did these errors come to light? After the model had been trained, validated, and evaluated on a dataset involving millions of entries, it was used to make ~450 novel predictions, and three of these were tested in vitro. It just so happens that one of the enzymes selected for in vitro testing, yciO, had already been studied extensively over a decade earlier by Dr.&nbsp;de Cr√©cy-Lagard. When Dr.&nbsp;de Cr√©cy-Lagard read that deep learning had predicted that yciO had the same function of another gene, TsaC, she knew from her long years in the lab that this was incorrect. Her previous research had shown that the TsaC gene is essential in <em>E. coli</em> even if yciO is present in the same genome and even when yciO gene is overexpressed. Moreover, the yciO activity reported by Kim et al.&nbsp;is more than four orders of magnitude (i.e.&nbsp;10,000 times) weaker than that of TsaC. All this suggests that yciO does NOT serve the same key function as TsaC.</p>
<div>
<figure>
<p><img src="https://rachel.fast.ai/posts/2025-06-04-enzyme-ml-fails/de-crecy-fig7.jpg"></p>
<figcaption>Two enzymes with a common evolutionary ancestor, but different functions (Fig 7 from de Crecy, et al.)</figcaption>
</figure>
</div>
<p>YciO and TsaC do have structural similarities, and YciO evolved from an ancestor of TsaC. Decades of research on protein and enzyme evolution have shown that new functions often evolve via duplication of an existing gene, followed by diversification of its function. This poses a common pitfall in determining enzyme function, because the genes will have many similarities with the ones they duplicated and then diversified from.</p>
<p>Thus, looking at structural similarities is only one type of evidence for considering enzyme function. It is also crucial to look at other types of evidence, such as neighborhood context of the genes, substrate docking, gene co-occurrence in metabolic pathways, and other features of the enzymes.</p>
<div>
<figure>
<p><img src="https://rachel.fast.ai/posts/2025-06-04-enzyme-ml-fails/de-crecy-fig2.jpg"></p>
<figcaption>It is important to look at multiple types of evidence when classifying enzyme function (Fig 2 from de Crecy, et al.)</figcaption>
</figure>
</div>
</section>
<section id="hundreds-of-likely-erroneous-results">
<h2 data-anchor-id="hundreds-of-likely-erroneous-results">Hundreds of Likely Erroneous Results</h2>
<p>Spotting this one error inspired de Cr√©cy-Lagard and her co-authors to take a closer look at all of the enzymes found to have novel results in the Kim, et al, paper. They found that 135 of these results were already listed in the online database used to build the training set and thus not actually novel. An additional 148 of the results contained a very high level of repetition, with the same highly specific functions reappearing up to 12 times. Biases, data imbalance, lack of relevant features, architectural limitations, or poor uncertainty calibration can all lead models to ‚Äúforce‚Äù the most common labels from the training data.</p>
<p>Other examples were proven wrong via biological context or a literature search. For instance, the gene YjhQ was predicted to be a mycothiol synthase but mycothiol is not synthesized by <em>E. coli</em>. YrhB was predicted to synthesize a particular compound, which was already predicted to be synthesized by the enzyme QueD. A form of <em>E. coli</em> with a QueD mutant was unable to synthesize the compound, showing that this is not in fact the function of YrhB.</p>
</section>
<section id="rethinking-enzyme-classification-and-true-unknowns">
<h2 data-anchor-id="rethinking-enzyme-classification-and-true-unknowns">Rethinking Enzyme Classification and ‚ÄúTrue Unknowns‚Äù</h2>
<p>Identifying enzyme function actually consists of two quite different problems which are commonly conflated:</p>
<ul>
<li>propagating known function labels to enzymes in the same functional family</li>
<li>discovering truly unknown functions</li>
</ul>
<p>The authors of the second paper observe, ‚ÄúBy design, supervised ML-models cannot be used to predict the function of true unknowns.‚Äù While machine learning can be useful for propagating known functions to additional enzymes, there are many types of errors that can occur: including failing to propagate labels when they should, propagating labels when they should not, curation mistakes, and experimental mistakes. Unfortunately, erroneous functions are being entered into key online databases such as UniProt, and this incorrect data may be further propagated if it is used to train prediction models. This is a problem that increases over time.</p>
</section>
<section id="need-for-domain-expertise">
<h2 data-anchor-id="need-for-domain-expertise">Need for Domain Expertise</h2>
<p>It is not news that AI work will be more highly rewarded and supported than work that closely inspects the underlying data and integrates deep domain knowledge. The aptly titled <a href="https://research.google/pubs/everyone-wants-to-do-the-model-work-not-the-data-work-data-cascades-in-high-stakes-ai/">‚ÄúEveryone Wants to do the Model Work, not the Data Work‚Äù</a> paper involving dozens of machine learning practitioners working on high-stakes AI projects and found that inadequate-application domain expertise was one of a few key causes of catastrophic failures.</p>
<div>
<figure>
<p><img src="https://rachel.fast.ai/posts/2025-06-04-enzyme-ml-fails/sambasivan-square.jpg"></p>
<figcaption>Sources of cascading failures in machine learning systems (Fig 1 from Sambasivan, et al.)</figcaption>
</figure>
</div>
<p>These papers also serve as a reminder of how challenging (or even impossible) it can be to evaluate AI claims in work outside our own area of expertise. I am not a domain expert in the enzyme functions of <em>E. coli</em>. And for most deep learning papers I read, domain experts have not gone through the results with a fine-tooth comb inspecting the quality of the output. How many other seemingly-impressive papers would not stand up to scrutiny? The work of checking hundreds of enzyme predictions is less glamorous than the work of building the AI model that generated them, yet it is even more important. How can we better incentivize this type of error-checking research?</p>
<p>At a time when funding is being slashed, I believe we should be doing the opposite and investing even more into a range of scientific and biomedical research, from a variety of angles. And we need to push back on an incentive system that is disproportionately focused on flashy AI solutions at the expense of quality results.</p>


</section>

<p><i>I look forward to reading your responses. Create a free GitHub account to comment below.</i></p></main> <!-- /main -->


</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[A deep dive into self-improving AI and the Darwin-G√∂del Machine (173 pts)]]></title>
            <link>https://richardcsuwandi.github.io/blog/2025/dgm/</link>
            <guid>44174856</guid>
            <pubDate>Tue, 03 Jun 2025 21:19:32 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://richardcsuwandi.github.io/blog/2025/dgm/">https://richardcsuwandi.github.io/blog/2025/dgm/</a>, See on <a href="https://news.ycombinator.com/item?id=44174856">Hacker News</a></p>
<div id="readability-page-1" class="page"><div> <d-contents> <nav> <h3>Contents</h3>   <ul> <li> <a href="#how-dgm-works">How DGM Works</a> </li> <li> <a href="#can-dgm-really-improve-itself">Can DGM Really Improve Itself?</a> </li> <li> <a href="#comparison-with-alphaevolve">Comparison with AlphaEvolve</a> </li> <li> <a href="#can-we-trust-a-self-improving-ai">Can we trust a self-improving AI?</a> </li> </ul>  </nav> </d-contents> <p>Most AI systems today are stuck in a ‚Äúcage‚Äù designed by humans. They rely on fixed architectures crafted by engineers and lack the ability to evolve autonomously over time. This is the <a href="https://en.wikipedia.org/wiki/Achilles%27_heel" rel="external nofollow noopener" target="_blank">Achilles heel</a> of modern AI ‚Äî like a car, no matter how well the engine is tuned and how skilled the driver is, it cannot change its body structure or engine type to adapt to a new track on its own. But what if AI could learn and improve its own capabilities without human intervention? In this post, we will dive into the concept of self-improving systems and a recent effort towards building one.</p> <h2 id="learning-to-learn">Learning to Learn</h2> <p>The idea of building systems that can improve themselves brings us to the concept of <a href="https://people.idsia.ch/~juergen/metalearning.html" rel="external nofollow noopener" target="_blank">meta-learning</a>, or ‚Äúlearning to learn‚Äù <d-cite key="thrun1998learning"></d-cite>, which aims to create systems that not only solve problems but also evolve their problem-solving strategies over time. One of the most ambitious efforts in this direction is the G√∂del Machine<d-cite key="schmidhuber2003godel"></d-cite>, proposed by J√ºrgen Schmidhuber decades ago and was named after the famous mathematician <a href="https://en.wikipedia.org/wiki/Kurt_G%C3%B6del" rel="external nofollow noopener" target="_blank">Kurt G√∂del</a>. A G√∂del Machine is a hypothetical self-improving AI system that optimally solves problems by recursively rewriting its own code when it can mathematically prove a better strategy. It represents the ultimate form of self-awareness in AI, an agent that can reason about its own limitations and modify itself accordingly.</p> <p><img src="https://richardcsuwandi.github.io/assets/img/godel.jpg" alt="Overview of a G√∂del machine" width="80%"></p> <p><strong>Figure 1.</strong> G√∂del machine is a hypothetical self-improving computer program that solves problems in an optimal way. It uses a recursive self-improvement protocol in which it rewrites its own code when it can prove the new code provides a better strategy.</p> <p>While this idea is interesting, formally proving whether a code modification of a complex AI system is <em>absolutely beneficial</em> is almost an impossible task without restrictive assumptions. This part stems from the inherent difficulty revealed by the <a href="https://en.wikipedia.org/wiki/Halting_problem" rel="external nofollow noopener" target="_blank">Halting Problem</a> and <a href="https://en.wikipedia.org/wiki/Rice%27s_theorem" rel="external nofollow noopener" target="_blank">Rice‚Äôs Theorem</a> in computational theory, and is also related to the inherent limitations of the logical system implied by <a href="https://en.wikipedia.org/wiki/G%C3%B6del%27s_incompleteness_theorems" rel="external nofollow noopener" target="_blank">G√∂del‚Äôs incompleteness theorem</a>. These theoretical constraints make it nearly impossible to predict the complete impact of code changes without making restrictive assumptions. To illustrate this, consider a simple analogy: just as you cannot guarantee that a new software update will improve your computer‚Äôs performance without actually running it, an AI system faces an even greater challenge in predicting the long-term consequences of modifying its own complex codebase.</p> <h2 id="darwin-g√∂del-machine">Darwin-G√∂del Machine</h2> <p>To ‚Äúrelax‚Äù the requirement of formal proof, a recent work by proposed the <strong>Darwin-G√∂del Machine (DGM)</strong><d-cite key="zhang2025darwingodelmachineopenended"></d-cite>, which combines the Darwinian evolution and G√∂delian self-improvement. Essentially, DGM abandoned the pursuit of a rigorous mathematical proof and embraced a more pragmatic way that is closer to the essence of life evolution through empirical validation. As the authors put it,</p> <blockquote> <p>We do not require formal proof, but empirical verification of self-modification based on benchmark testing, so that the system can improve and explore based on the observed results.</p> </blockquote> <p><img src="https://richardcsuwandi.github.io/assets/img/dgm.png" alt="Overview of the DGM" width="80%"></p> <p><strong>Figure 2.</strong> The Darwin-G√∂del Machine (DGM) is a self-improving AI system that optimizes its own problem-solving strategy through a combination of Darwinian evolution and G√∂delian self-improvement.</p> <p>The core idea behind DGM is to let the AI agent iteratively modify its own code<d-footnote>More precisely, the metacode that controls its behavior and ability</d-footnote>, and then put these modified new versions into the actual test environment (e.g., coding benchmarks) to judge whether this modification is good by observing its performance. If the modification is good, it will be retained and may become the basis for further evolution in the future; if the modification is bad, it will be eliminated.</p> <h3 id="how-dgm-works">How DGM Works</h3> <p><img src="https://richardcsuwandi.github.io/assets/img/dgm-animation.gif" alt="Animation of the DGM process" width="80%"></p> <p><strong>Figure 3.</strong> Animation showing how DGM iteratively builds a growing archive of agents by harnessing the principles of open-ended exploration. New agents are created and scored by interleaving self-modification with downstream task evaluation.</p> <p>The general workflow of DGM is as follows:</p> <ol> <li> <p><strong>Initialization:</strong> Evolution begins with one or a few basic coding agents, so-called the ‚Äúseeds‚Äù of intelligence. These initial agents may possess only minimal functionalities, such as executing bash commands or editing entire files. DGM maintains an ‚Äúarchive of agents‚Äù, which functions like a biological ‚Äúgene bank.‚Äù This archive preserves all previously generated agents, regardless of their performance level, ensuring that no potentially valuable variation is permanently lost.</p> </li> <li> <p><strong>Sampling:</strong> DGM selects one or more parent agents from the archive to generate new child agent. The selection mechanism does not focus solely on top performers but instead allows every agent a chance to be chosen. While higher-performing agents are more likely to be selected, DGM also encourages exploration by giving ‚Äúless successful agents‚Äù<d-footnote>Those that might contain novel or unconventional ideas</d-footnote> an opportunity to contribute to future generations.</p> </li> <li> <strong>Reproduction:</strong> Once the parents are selected, DGM performs modifications to generate a new child agent. These changes occur at the source code level and can include: <ul> <li> <em>Enhancement of existing tools</em>, for example, upgrading a file-editing tool that replaces entire files into one capable of precise string replacement.</li> <li> <em>Addition of new tools or workflows</em>, such as integrating a static code analysis tool or designing a ‚Äúthink‚Äìact‚Äìobserve‚Äù loop.</li> <li> <em>Improvement of problem-solving strategies</em>, for instance, refining how the agent prompts the base model or managing long-context code more effectively.</li> <li> <em>Introduction of collaboration mechanisms</em>, like having an agent‚Äôs code reviewed by another before execution.</li> </ul> </li> <li> <p><strong>Natural Selection:</strong> The newly generated child agent is not automatically accepted into the ‚Äúelite pool‚Äù but must prove its worth through rigorous testing. Each agent‚Äôs performance, such as the percentage of successfully solved problems, is quantitatively scored and used to select the best agents.</p> </li> <li> <strong>Tree Formation:</strong> If a child agent outperforms its parent or meets certain quality thresholds, it is added to the archive and becomes a new node in the evolutionary tree. This process repeats iteratively, creating a growing structure of diverse, high-quality agents. Importantly, DGM enables parallel exploration of many different paths in the search space, promoting open-ended discovery and avoiding premature convergence to local optima.</li> </ol> <p><img src="https://richardcsuwandi.github.io/assets/img/dgm-conceptual.png" alt="Workflow of DGM" width="100%"></p> <p><strong>Figure 4</strong> DGM iteratively builds a growing intelligent archive by intertwining self-modification with downstream task evaluation. The intelligent body in the archive is selected for self-modification through open exploration.</p>  <p>The paper provides a concrete example of the above principle in action. In the SWE-bench experiment, the performance of newly generated agents showed temporary declines in the 4th and 56th iterations. While traditional hill-climbing methods would abandon these seemingly ‚Äúregressive paths‚Äù, DGM‚Äôs open exploration mechanism allows continued exploration. This persistence paid off as by the 70th iteration, the system had created new agents that surpassed all their predecessors, demonstrating the value of maintaining diverse evolutionary paths even when they temporarily appear less promising.</p> <h3 id="can-dgm-really-improve-itself">Can DGM Really Improve Itself?</h3> <p>In the paper, the authors evaluated DGM on two challenging benchmarks:</p> <ol> <li> <p><strong>SWE-bench:</strong> A widely-used benchmark that tests agents‚Äô ability to resolve real-world GitHub issues<d-cite key="jimenez2024swe"></d-cite> . DGM showed impressive progress, improving its performance from 20.0% to 50.0% through self-modification.</p> </li> <li> <p><strong>Polyglot</strong>: A <a href="https://aider.chat/2024/12/21/polyglot.html" rel="external nofollow noopener" target="_blank">multi-language coding benchmark</a> containing coding problems in C++, Go, Java, JavaScript, Python and Rust. DGM achieved even more dramatic improvement, jumping from an initial 14.2% to 30.7% success rate, far surpassing the representative hand-designed agent by <a href="https://aider.chat/" rel="external nofollow noopener" target="_blank">Aider</a>.</p> </li> </ol> <p>The impressive performance improvements demonstrate DGM‚Äôs fundamental ability to enhance itself through self-modification. Through ablation studies, the authors identified two key factors driving DGM‚Äôs success. First, self-modification proved crucial, as when disabled (<strong>DGM w/o Self-improve</strong>), the performance declined substantially, showing that the ability to modify its own code is essential for accelerated learning. Second, the open-ended exploration approach was vital, as removing the Darwinian evolution-inspired search mechanism (<strong>DGM w/o Open-ended exploration</strong>), which maintains an archive of diverse solutions and enables parallel exploration of multiple paths, resulted in lower performance.</p> <p><img src="https://richardcsuwandi.github.io/assets/img/dgm-self-improve.png" alt="Performance of DGM on SWE-bench and Polyglot" width="100%"></p> <p><strong>Figure 5</strong> Self-improvement and open-ended exploration enable the DGM to continue making progress and improve its performance. The DGM automatically discovers increasingly better coding agents and performs better on both SWE-bench (Left) and Polyglot (Right).</p> <h3 id="comparison-with-alphaevolve">Comparison with AlphaEvolve</h3> <p>In parallel, AlphaEvolve<d-cite key="deepmind2025alphaevolve"></d-cite>, which is developed by Google DeepMind, also demonstrates another powerful path forward. AlphaEvolve pairs the creative problem-solving capabilities of Google‚Äôs Gemini models with automated evaluators in an evolutionary framework. It has already demonstrated significant real-world impact across multiple domains, such as:</p> <ul> <li> <strong>Data center efficiency:</strong> AlphaEvolve discovered a simple yet highly effective heuristic for Google‚Äôs <a href="https://research.google/pubs/large-scale-cluster-management-at-google-with-borg/" rel="external nofollow noopener" target="_blank">Borg</a> cluster management system, continuously recovering 0.7% of Google‚Äôs worldwide compute resources.</li> <li> <strong>AI acceleration:</strong> It achieved a 23% speedup in Gemini‚Äôs architecture‚Äôs vital <a href="https://docs.jax.dev/en/latest/pallas/index.html" rel="external nofollow noopener" target="_blank">kernel</a> by finding more efficient ways to divide large matrix multiplication operations, resulting in a 1% reduction in overall training time.</li> <li> <strong>Mathematical breakthroughs:</strong> Most notably, it discovered an algorithm for multiplying 4x4 complex-valued matrices using just 48 scalar multiplications, surpassing <a href="https://en.wikipedia.org/wiki/Strassen_algorithm" rel="external nofollow noopener" target="_blank">Strassen‚Äôs 1969 algorithm</a>, and advanced the 300-year-old <a href="https://en.wikipedia.org/wiki/Kissing_number_problem" rel="external nofollow noopener" target="_blank">kissing number problem</a> by establishing a new lower bound in 11 dimensions.</li> </ul>  <p>While both systems adopt a similar evolutionary framework, their scopes and methodologies differ in the following ways:</p> <table> <thead> <tr> <th>Feature</th> <th>AlphaEvolve</th> <th>DGM</th> </tr> </thead> <tbody> <tr> <td>Focus</td> <td>Evolving functions and codebases</td> <td>Evolving the agent itself</td> </tr> <tr> <td>Level of Innovation</td> <td>Algorithmic level</td> <td>Agent-level (toolset, methodology)</td> </tr> <tr> <td>Role of LLM</td> <td>LLM acts as ‚Äúgenetic operators‚Äù to modify algorithms</td> <td>LLM serves as the ‚Äúbrain‚Äù to evolve itself with better tools and strategies</td> </tr> <tr> <td>Evaluation</td> <td>Well-defined problems with automated evaluators</td> <td>Open-ended environments</td> </tr> </tbody> </table> <p>To better understand the differences between the two approaches, let us take a look at the following analogy:</p>  <h3 id="can-we-trust-a-self-improving-ai">Can we trust a self-improving AI?</h3> <p>The authors also conducted some experiments to evaluate DGM‚Äôs reliability and discovered some concerning behaviors. In particular, they observed instances where DGM attempted to manipulate its reward function through deceptive practices. One notable example involved the system fabricating the use of external tools - specifically, it generated fake logs suggesting it had run and passed unit tests, when in reality no tests were executed.</p> <p><img src="https://richardcsuwandi.github.io/assets/img/dgm-hallucination.png" alt="Hallucination in DGM" width="100%"></p> <p><strong>Figure 6</strong> DGM hallucinated the use of an external tool to run unit tests.</p> <p>To address this issue, the authors implemented a ‚Äútool use hallucination‚Äù reward function and tasked DGM with optimizing it to eliminate hallucination behaviors. While DGM successfully provided solutions in many cases, it sometimes attempted to circumvent the detection system by removing the markers used to identify hallucinations, despite explicit instructions to preserve them. This behavior is reminiscent of the phenomenon of <a href="https://en.wikipedia.org/wiki/Goodhart%27s_law" rel="external nofollow noopener" target="_blank">Goodhart‚Äôs Law</a> from economics and sociology:</p> <blockquote> <p>‚ÄúWhen a measure becomes a target, it ceases to be a good measure.‚Äù</p> </blockquote> <p>Fortunately, DGM‚Äôs design includes a transparent, traceable lineage of all changes, which enabled the researchers to quickly identify and address these undesirable behaviors. However, this example highlights the need for more robust safeguards to prevent such manipulation attempts in the first place. These findings underscore the critical importance of safety in self-improving AI research.</p> <h2 id="takeaways">Takeaways</h2> <p>DGM represents a groundbreaking step toward the realization of <a href="https://en.wikipedia.org/wiki/Life_3.0" rel="external nofollow noopener" target="_blank">Life 3.0</a>, a concept introduced by physicist <a href="https://en.wikipedia.org/wiki/Max_Tegmark" rel="external nofollow noopener" target="_blank">Max Tegmark</a>. In his book, he classified life into three stages:</p> <ul> <li> <strong>Life 1.0:</strong> Biological life with fixed hardware and software, such as bacteria.</li> <li> <strong>Life 2.0:</strong> Beings like humans, whose behavior can be learned and adapted during their lifetime, though their biology remains fixed.</li> <li> <strong>Life 3.0:</strong> A new class of intelligence that can redesign not only its behavior but also its underlying architecture and objectives ‚Äî essentially, intelligence that builds itself.</li> </ul> <p><img src="https://richardcsuwandi.github.io/assets/img/life3.webp" alt="Life 3.0" width="80%"></p> <p><strong>Figure 7</strong> The three stages of life according to Max Tegmark.</p> <p>While DGM currently focuses on evolving the ‚Äúsoftware‚Äù<d-footnote>the code and strategies of AI agents</d-footnote>, it exemplifies the early stages of Life 3.0. By iteratively rewriting its own code based on empirical feedback, DGM demonstrates how AI systems could move beyond human-designed architectures to autonomously explore new designs, self-improve, and potentially give rise to entirely new species of digital intelligence. If this trend continues, we may witness a <a href="https://en.wikipedia.org/wiki/Cambrian_explosion" rel="external nofollow noopener" target="_blank">Cambrian explosion</a> in AI development, where eventually AI systems will surpass human-designed architectures and give rise to entirely new species of digital intelligence. While this future looks promising, achieving it requires addressing significant challenges, including:</p> <ul> <li> <p><strong>Evaluation Framework</strong>: Need for more comprehensive and dynamic evaluation systems that better reflect real-world complexity and prevent ‚Äúreward hacking‚Äù while ensuring beneficial AI evolution.</p> </li> <li> <p><strong>Resource Optimization</strong>: DGM‚Äôs evolution is computationally expensive<d-footnote>The paper mentioned that a complete SWE-bench experiment takes about two weeks and about $22,000 in API call costs.</d-footnote>, thus improving efficiency and reducing costs is crucial for broader adoption.</p> </li> <li> <p><strong>Safety &amp; Control</strong>: As AI self-improvement capabilities grow, maintaining alignment with human ethics and safety becomes more challenging.</p> </li> <li> <p><strong>Emergent Intelligence</strong>: Need to develop new approaches to understand and interpret AI systems that evolve beyond human-designed complexity, including new fields like ‚ÄúAI interpretability‚Äù and ‚ÄúAI psychology‚Äù.</p> </li> </ul> <p>In my view, DGM is more than a technical breakthrough, but rather a philosophical milestone. It invites us to rethink the boundaries of intelligence, autonomy, and life itself. As we advance toward Life 3.0, our role shifts from mere designers to guardians of a new era, where AI does not just follow instructions, but helps us discover what is possible.</p> </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Show HN: AirAP AirPlay server - AirPlay to an iOS Device (193 pts)]]></title>
            <link>https://github.com/neon443/AirAP</link>
            <guid>44174190</guid>
            <pubDate>Tue, 03 Jun 2025 20:12:33 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/neon443/AirAP">https://github.com/neon443/AirAP</a>, See on <a href="https://news.ycombinator.com/item?id=44174190">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text">
<p dir="auto">AirAP is a fully native AirPlay server, written in Swift, for iOS. Essentially, AirAP allows you to use your iPhone as an AirPlay receiver in iTunes or on your Mac, meaning that you can use your iPhone to play your device's sound.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">What's AirAP?</h2><a id="user-content-whats-airap" aria-label="Permalink: What's AirAP?" href="#whats-airap"></a></p>
<p dir="auto">Have you ever wanted to stream audio from your Mac, Apple TV, or another iOS device to your iPhone? AirAP makes this possible by implementing a full AirPlay server that runs natively on iOS. Once installed, your iPhone will appear as an available AirPlay destination in iTunes (including the Windows version), Music app, or any other AirPlay-compatible application.</p>
<p dir="auto">The concept might seem backwards at first - after all, we're used to streaming from our iPhones to other devices. But there are surprisingly many scenarios where you'd want to do the reverse. Maybe you're working on your Mac late at night and want to route the audio to your iPhone with headphones so you don't disturb anyone (hi üëã). Perhaps you're a developer testing audio applications and need to quickly switch between different output devices. Or maybe you just want to include your iPhone in a multi-room audio setup alongside your other AirPlay speakers.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Installing AirAP</h2><a id="user-content-installing-airap" aria-label="Permalink: Installing AirAP" href="#installing-airap"></a></p>
<p dir="auto">To try it out, <a href="https://testflight.apple.com/join/8aeqD8Q2" rel="nofollow">open this TestFlight link</a>, install AirAP, and follow the instructions. After installation, simply launch AirAP and ensure your iPhone is connected to the same Wi-Fi network as the device you want to stream from. Your iPhone will automatically appear in AirPlay device lists, ready to receive audio - if it doesn't, try restarting the app.</p>
<hr>
<sup>
¬© 2025 Nihaal Sharma. AirPlay, iPhone, iTunes, Mac, and Apple TV are trademarks of Apple Inc.
</sup>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Did "Big Oil" Sell Us on a Recycling Scam? (103 pts)]]></title>
            <link>https://daily.jstor.org/did-big-oil-sell-us-on-a-recycling-scam/</link>
            <guid>44172928</guid>
            <pubDate>Tue, 03 Jun 2025 18:14:53 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://daily.jstor.org/did-big-oil-sell-us-on-a-recycling-scam/">https://daily.jstor.org/did-big-oil-sell-us-on-a-recycling-scam/</a>, See on <a href="https://news.ycombinator.com/item?id=44172928">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
							<p>
								The <span></span> icon indicates free access to the linked research on JSTOR.
							</p>
							<p>‚ÄúReduce, reuse, recycle‚Äù: these three words have become as ubiquitous as the plastic waste they attempt to combat. Once seen as a simple roadmap toward sustainability, this mantra now conceals a far more complex and troubling reality. While these principles serve as a starting point for environmental action, they also have a deceptive history rooted in the petrochemical industry‚Äôs effort to avoid accountability. The truth is, no matter how diligently we sort our waste products, individual actions alone cannot solve the growing crisis of plastic pollution.</p><p><a href="https://about.jstor.org/submission-guidelines/?utm_source=jstor_daily&amp;utm_medium=in_line&amp;utm_campaign=collaboration"><img decoding="async" src="https://daily.jstor.org/wp-content/uploads/2025/05/jstor_collaborators_ad_in_text.jpg" alt="JSTOR Collaboration" width="800" height="196"><img src="https://daily.jstor.org/wp-content/uploads/2025/05/jstor_collaborators_ad_mobile.jpg" alt="JSTOR Collaboration"></a></p>
<p>The <a href="https://daily.jstor.org/the-revolutionary-past-of-plastics/">ubiquity of plastic</a> in modern life makes recycling seem like a moral imperative. From straws and bags to take-out containers, single-use plastics crowd landfills and clog waterways. And the crisis is accelerating. Legal scholar Roberta Mann warns that by 2050, <span><a href="https://www.jstor.org/stable/48813257?mag=did-big-oil-sell-us-on-a-recycling-scam">plastic in the ocean could outweigh fish</a></span>. The United States led the world in plastic waste in 2016, Mann writes, generating over 42 million metric tons. The COVID-19 pandemic further fueled plastics consumption, with a spike in single-use personal protective equipment and packaging from online shopping.</p>
<p>But here‚Äôs the catch: research suggests that our dependence on recycling as a solution isn‚Äôt only ineffective‚Äîit‚Äôs based on a carefully crafted illusion. The narrative that recycling can meaningfully counteract the plastic crisis was constructed by the oil and gas industry to maintain public demand for plastic and delay regulation of its production.</p>
<p>As an investigation conducted by NPR and PBS <em>Frontline</em> unearthed in 2020 and reported in a <em>Frontline</em> episode called ‚Äú<a href="https://www.pbs.org/wgbh/frontline/documentary/plastic-wars/">Plastic Wars</a>,‚Äù <a href="https://www.npr.org/2020/09/11/897692090/how-big-oil-misled-the-public-into-believing-plastic-would-be-recycled">oil companies have known for decades about the inability to recycle plastics throughout the US</a>. Tracing the history of the issue, the Center for International Law outlines how in the 1950s and ‚Äô60s the fossil fuel, petrochemical, and packaging industries began <span><a href="https://www.jstor.org/stable/resrep63260.9?mag=did-big-oil-sell-us-on-a-recycling-scam">convening on the issue of plastic pollution</a></span> as reports emerged of the plastics‚Äô inability to decompose in the natural environment. In 1973, a National Academy of Sciences workshop reported that polystyrene spherules and poly-chlorinated biphenyls were being found in abundance in marine environments. The concept of decomposability was soon weaponized as one of plastic‚Äôs biggest strengths: plastic began to be marketed as the only material perfect for landfill linings and pollution containment.</p>

		<div>
							<p><a href="https://daily.jstor.org/youll-never-believe-who-invented-curbside-recycling/">
						<img fetchpriority="high" decoding="async" width="1050" height="700" src="https://daily.jstor.org/wp-content/uploads/2021/08/yes_curbside_recycling_is_kind_of_a_scam_1050x700.jpg" alt="Woman recycling glass, Wallingford neighborhood, Seattle, Washington, 1990">					</a>
				</p>
			
			<div>
				<h3><a href="https://daily.jstor.org/youll-never-believe-who-invented-curbside-recycling/">You‚Äôll Never Believe Who Invented Curbside Recycling</a></h3>								<p>
					August 6, 2021				</p>
				<p>Far from ushering in a zero-waste world, the switch from returnables to recycling provided cover for the creation of ever more packaging trash.</p>			</div>
		</div>

		
<p>By also marketing plastic as recyclable, the entangled industries shifted the burden of responsibility onto individual consumers. <span><a href="https://time.com/vault/issue/1989-07-17/spread/16/">A <em>Time</em> magazine advertisement</a></span> from 1989 demonstrates how the Society of Plastic Industry (comprising fossil fuel companies Exxon, Mobil, Dow, DuPont, Chevron, and Phillips 66) emphasized recycling as a moral duty, all while knowing that the existing recycling infrastructure was inadequate and unprofitable.</p>
		
		
<p>Not much has changed in the last thirty-plus years. As Dave Dennison muses, recycling only occurs under conditions where it‚Äôs ‚Äù<span><a href="https://www.jstor.org/stable/27087389?mag=did-big-oil-sell-us-on-a-recycling-scam">cheaper for waste-hauling companies to [do it than to] send baled waste to landfills</a></span>.‚Äù As a representative from Keurig admitted in ‚ÄúPlastic Wars,‚Äù there‚Äôs currently no way of effectively recycling K-Cups, even though approximately eleven billion K-cups are produced per year. In fact, the creation of the recycling symbol on plastic products, utilizing the 1‚Äì7 polymer grade scale, was a push from industry as a bargaining chip to<span><a href="https://oag.ca.gov/plastics#deception"> stop state governments from instituting plastic bans and creating mandatory recycling standards</a></span>. As long as customers believe plastics producers are doing their part‚Äîand keep consuming plastic‚Äîthe producer need not be concerned that their waste products aren‚Äôt recycled, Dennison suggests.</p>

	<div>
		<h4>Weekly Newsletter</h4>
		


	</div>
	
<p>That doesn‚Äôt mean recycling should be abandoned altogether. With so much plastic already in our ecosystems, recycling and remediation remain critical. Certain uses of plastic, such as medical supplies or assistive tools for disabled individuals, are currently irreplaceable. But we should also think about a fourth ‚ÄúR‚Äù: replace, as in: replace petroleum-based products with sustainable alternatives whenever possible. Plastics are known to cause endocrine disruption in living organisms, with links to cancers and other illnesses, writes Mann. Recycling, while important, can be understood as a harm-reduction tool‚Äînot a final solution. The search is on for additional approaches that will offer a deep mitigation of plastic <a href="https://doi.org/10.1016/j.erss.2022.102880">without positioning the oil and gas industry at the heart of the solution</a>. Real progress likely depends on systemic change: bold regulations to limit plastic production, major investments in <a href="https://daily.jstor.org/company-uses-mushrooms-grows-plastic-alternatives/">alternative materials</a>, and the will to challenge an industry that has polluted our planet for decades.</p>
<hr>
<p><a href="https://bit.ly/30jM88p">Support JSTOR Daily! Join our membership program on Patreon today.</a></p>
													</div><div><p><img src="https://daily.jstor.org/wp-content/uploads/2018/02/jstor-logo@2x.png" alt="JSTOR logo" width="65" height="90">
		</p>
		<div>
			<h2>Resources</h2>
			<p>
				JSTOR is a digital library for scholars, researchers, and students. JSTOR Daily readers can access the original research behind our articles for free on JSTOR.			</p>

								<div>
						
												<p>
							By: Roberta Mann						</p>
													<p>
							Journal of Land Use &amp; Environmental Law, Vol. 37, No. 2 (Spring 2022), pp. 251‚Äì302						</p>
						<p>
							Florida State University College of Law						</p>
					</div>
										<div>
						
												<p>
							By: Center for International Environmental Law (CIEL)						</p>
													<p>
							Making Plastic Polluters Pay: How Cities and States Can Recoup the Rising Costs of Plastic Pollution, (June 2024), pp. 21‚Äì23						</p>
						<p>
							Center for International Environmental Law (CIEL)						</p>
					</div>
										<div>
						
												<p>
							By: Dave Denison						</p>
													<p>
							The Baffler, No. 60 (NOV‚ÄìDEC 2021), pp. 58‚Äì67						</p>
						<p>
							Baffler Foundation						</p>
					</div>
					
		</div>
	</div><div>
		<h4>Get Our Newsletter</h4>
		


	</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Changing Directions (101 pts)]]></title>
            <link>https://jacobian.org/2025/jun/3/changing-directions/</link>
            <guid>44172659</guid>
            <pubDate>Tue, 03 Jun 2025 17:49:25 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://jacobian.org/2025/jun/3/changing-directions/">https://jacobian.org/2025/jun/3/changing-directions/</a>, See on <a href="https://news.ycombinator.com/item?id=44172659">Hacker News</a></p>
<div id="readability-page-1" class="page"><article><p>I have two important announcements:</p><ol><li><p>I‚Äôm leaving the tech industry. Hopefully ‚Äúfor good‚Äù; if not, at least ‚Äúfor now‚Äù.</p></li><li><p>As such, the content on this blog is going to shift, perhaps dramatically. I‚Äôm going to be writing about a broader range of topics that interest me (projects around my hobby farm, wilderness trips, emergency medicine) ‚Äì more writing for <em>me</em>, less writing for some imagined audience. (I‚Äôll probably still end up writing about some of the same topics as I‚Äôve been covering since 2020, just less often.)</p></li></ol><p>I‚Äôm writing this post mostly to give myself permission to make that change, and to give readers the opportunity to unsubscribe/unfollow if they‚Äôre not interested.</p><p>If you‚Äôre interested in more details about why I‚Äôm leaving the industry and what‚Äôs next for me and this blog, read on.</p><h3 id="leaving-the-tech-industry">Leaving the tech industry</h3><p>I left what I hope will be my last tech job in October. In many ways it was the perfect job: work I enjoyed, great clients, co-workers I loved (and still miss), at a company whose mission and ethics I aligned with. And despite all that I found myself increasingly burnt out. I started telling people that ‚ÄúI love my job, but I hate my career.‚Äù</p><p>I‚Äôve been working in tech more or less non-stop for more than 25 years, and in that time my feeling about the industry has changed dramatically. When I began, I held straightforwardly techno-utopian ideas: I believed that technology was an unalloyed force for Good, that the computer revolution would bring about a more just and equitable society. I thought the industry was going to create a Star Trek-esque post-scarcity society.</p><p>I cringe at past-Jacob‚Äôs naivet√©. It‚Äôs been incredibly demoralizing to discover what the tech industry actually did was to invent surveillance capitalism, create exploitative ‚Äúgig economy‚Äù business models, create a new generation of robber barons, enable the rise of global fascism, <a href="https://erinkissane.com/meta-in-myanmar-full-series">facilitate genocide</a>, and I could go on but Christ I‚Äôm exhausted.</p><p>I‚Äôve been thinking about leaving the industry since at least 2016, but haven‚Äôt been brave enough to do so. This career pays really well for easy work (physically, at least), and it‚Äôs all I‚Äôm good at. Doing something different is terrifying. But I‚Äôm able to afford a change in direction, despite the massive pay cut, so I‚Äôm doing it.</p><p>I want to be super-clear about this: <strong>I‚Äôm in no way criticizing anyone who makes a different decision</strong>. Leaving a good tech career is a <em>terrible</em> financial decision, and I absolutely won‚Äôt criticize people who‚Äôre unable or unwilling to make such a bad decision. This is about <em>me</em>: I‚Äôm able to make this move, so I‚Äôm going to. There‚Äôs no judgement of anyone else here, explicit or implied.</p><p>I still find computers themselves incredibly exciting. When I first learn to program, being able to make a computer do whatever I wanted felt like a superpower ‚Äì and it still does! I‚Äôm not planning a total exit from anything computer related; I don‚Äôt think I could. I plan on continuing to be involved in the Django community and with the Django Software Foundation. I‚Äôm continuing on as an advisor to a couple of small companies I‚Äôve been working with (with a change: I‚Äôll step down if/when those companies do a traditional VC raise). I‚Äôm remaining open to limited advising/consulting work ‚Äì though, I‚Äôm going to be pretty picky about who it‚Äôs for. And I‚Äôm sure I‚Äôll always write software, it‚Äôs just too fun and useful a skill.</p><p>My plan is to go into emergency medicine: I‚Äôll be training as an EMT this fall, and I‚Äôm starting to volunteer with our local county Search and Rescue team. I plan to work as a EMT for a year or so, and then perhaps work towards becoming a Paramedic.</p><p>I don‚Äôt know if this‚Äôll work out. It might not. But I‚Äôm giving it a shot.</p><h3 id="changing-the-focus-of-my-writing">Changing the focus of my writing</h3><p>I‚Äôve been writing here since 2005. At first, this was a typical early-2000s personal blog: I wrote whatever I wanted ‚Äì technical posts, recipes, live updates, whatever. Then there a long fallow period starting around 2010 when I wrote sporadically, just a few posts a year.</p><p>In 2020 I rebooted the blog, with a very specific editorial focus: tech leadership, focused on software engineering and security. I wrote with a fairly specific audience in mind: engineering managers and technical leads, working in software development and information security. Mostly the theme was: things I wish someone had told me before I messed it all up. I put a concerted effort into building an audience for this content, and I think I‚Äôve been pretty successful at it. I‚Äôm happy with the results, at least. I‚Äôm sure I‚Äôll keep writing about these topics ‚Äì I remain fascinated by risk decisions, and will almost certainly continue my <a href="https://jacobian.org/series/risk/">Thinking About Risk</a> series.</p><p>But as I step away from tech, I find the things I‚Äôve been writing about since 2020 less and less interesting. I‚Äôve been wanting to write about a broader range of topics for well over a year, but have felt constrained by my own editorial choices. So a large part of writing this post is just to give myself permission to stop writing for this imagined audience, and go back to writing more for myself, for the pleasure of writing itself.</p><p>The editor in my head is telling me that I need an effective ending to this piece, and that I shouldn‚Äôt publish it without one. But that‚Äôs the same guy telling me to stay in my lane, so‚Ä¶</p></article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Swift at Apple: Migrating the Password Monitoring Service from Java (226 pts)]]></title>
            <link>https://www.swift.org/blog/swift-at-apple-migrating-the-password-monitoring-service-from-java/</link>
            <guid>44172166</guid>
            <pubDate>Tue, 03 Jun 2025 17:03:13 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.swift.org/blog/swift-at-apple-migrating-the-password-monitoring-service-from-java/">https://www.swift.org/blog/swift-at-apple-migrating-the-password-monitoring-service-from-java/</a>, See on <a href="https://news.ycombinator.com/item?id=44172166">Hacker News</a></p>
<div id="readability-page-1" class="page"><div role="main">
<article>
  <header>
    

    <time pubdate="" datetime="2025-06-02T06:00:00-04:00">June 2, 2025</time>
    
  </header>

  <p><em>Swift is heavily used in production for building cloud services at Apple, with incredible results. Last year, the Password Monitoring service was rewritten in Swift, handling multiple billions of requests per day from devices all over the world. In comparison with the previous Java service, the updated backend delivers a 40% increase in performance, along with improved scalability, security, and availability.</em></p>

<p>The Passwords app, introduced in the fall of 2024, helps users manage their passwords, passkeys, and verification codes. It allows them to store, autofill, and generate strong passwords that can be shared across all their devices, as well as share passwords with trusted contacts. One security feature included in the app is Password Monitoring, which warns users if one of their saved passwords shows up in a data leak. This feature has a server component, running on Linux-based infrastructure, that is maintained by Apple.</p>

<p>On a regular interval, Password Monitoring checks a user‚Äôs passwords against a continuously updated and curated list of passwords that are known to have been exposed in a leak. Importantly, this task is handled in a thoughtful, privacy-preserving way that never reveals users‚Äô passwords to Apple. A detailed discussion of how this is done using the cryptographic private set intersection protocol is in the <a href="https://support.apple.com/guide/security/password-monitoring-sec78e79fc3b/web">Password Monitoring</a> section of the <a href="https://help.apple.com/pdf/security/en_US/apple-platform-security-guide.pdf">Apple Platform Security</a> guide.</p>

<p>The migration from Java to Swift was motivated by a need to scale the Password Monitoring service in a performant way. The layered encryption module used by Password Monitoring requires a significant amount of computation for each request, yet the overall service needs to respond quickly even when under high load.</p>



<p>For years, our team relied on Java to power large-scale, mission-critical services because of its proven stability and performance. However, Java‚Äôs memory management approach no longer aligns with our growing demands and efficiency goals. Instead of simply expanding hardware resources, we were seeking a more-efficient language to support our growth while reducing server overhead.</p>

<p>Prior to seeking a replacement language, we sought ways of tuning the JVM to achieve the performance required. Java‚Äôs G1 Garbage Collector (GC) mitigated some limitations of earlier collectors by introducing features like predictable pause times, region-based collection, and concurrent processing. However, even with these advancements, managing garbage collection at scale remains a challenge due to issues like prolonged GC pauses under high loads, increased performance overhead, and the complexity of fine-tuning for diverse workloads.</p>

<p>One of the challenges faced by our Java service was its inability to quickly provision and decommission instances due to the overhead of the JVM. The Password Monitoring service runs globally, so service load can greatly fluctuate throughout the day, even with client-side techniques to smooth over the distribution of traffic. The peak and trough of a day differ by approximately 50% regionally. To efficiently manage this, we aim to scale down when demand is low and scale up as demand peaks in different regions. A faster bootstrap time is a crucial requirement to support this dynamic scaling strategy.</p>

<p>Given the scale of our application and the volume of traffic we manage daily, the decision to transition from Java to another language was not made lightly. We evaluated our options, and found only a few languages that could help us achieve our goals. While you might expect that Apple would automatically choose Swift, we were pleasantly surprised by how well it fit the unique needs of a cloud-based service like ours. Swift has expressive syntax that was easy to learn, and could deliver the performance improvements necessary to meet the demands of our compute workloads. We decided to take a significant leap and started a rewrite of the Password Monitoring backend using Swift.</p>



<p>We began rewriting our service using <a href="https://vapor.codes/">Vapor</a>, a Swift web framework that provided Routing, Controller, and Content modules that we were able to build upon. Our service had additional requirements that led us to create a few custom packages with essential functionality: elliptic curve operations that are crucial for implementing <a href="https://support.apple.com/guide/security/password-monitoring-sec78e79fc3b/web">password monitoring</a>, auditing, configuration, error handling, and custom middleware.</p>

<p><img alt="Password Monitoring Service Architecture." src="https://www.swift.org/assets/images/swift-at-apple-migrating-the-password-monitoring-service-from-java/password%20monitoring%20service.png" width="840" height="525">
</p>

<p>One of the most significant aspects of Swift that impressed us was its emphasis on <a href="https://docs.swift.org/swift-book/documentation/the-swift-programming-language/protocols/">protocols</a>. In Java, we relied heavily on inheritance, which can lead to complex class hierarchies and tight coupling. Swift‚Äôs approach of protocols and generics promotes modularity and reusability by allowing classes, structs, and enums to share common protocols, enabling a more flexible and scalable codebase. This shift in mindset encouraged us to think in terms of behaviors rather than concrete classes, resulting in cleaner and more maintainable code.</p>

<p>Safety is another area where Swift takes a distinctive approach compared to Java. For example, Swift‚Äôs optional type and safe unwrapping mechanisms eliminate the need for null checks everywhere, reducing the risk of null pointer exceptions and enhancing code readability. This safety-first approach ingrained throughout Swift‚Äôs language design, whether it is deterministic deallocation, copy-on-write (CoW), or value types, makes it inherently less prone to runtime errors.</p>

<p>Swift‚Äôs async/await support is a nice addition, streamlining how we handle async tasks. Previously, managing async operations often involved complex callback patterns or external libraries. Swift‚Äôs async/await syntax simplifies this process, making it more intuitive and less error-prone. We can now write async code that reads like sync code, leading to more readable, testable, and maintainable concurrency handling‚Äîespecially critical in high-load, multi-threaded environments.</p>

<p>Overall, our experience with Swift has been overwhelmingly positive and we were able to finish the rewrite much faster than initially estimated. Swift allowed us to write smaller, less verbose, and more expressive codebases (close to 85% reduction in lines of code) that are highly readable while prioritizing safety and efficiency.</p>

<p>Our service benefited from a diverse ecosystem of Swift packages, including <a href="https://github.com/apple/swift-log">logging</a> frameworks, a <a href="https://github.com/apple/swift-cassandra-client">Cassandra</a> client, and <a href="https://github.com/apple/swift-crypto">crypto</a> libraries that were readily available. In addition to an excellent support system and tooling, Swift‚Äôs inherent emphasis on modularity and extensibility helped future-proof and simplify the integration and customizations needed for our service-specific functions.</p>



<p>We benchmarked performance throughout the process of development and deployment, allowing us to discover the trait of the Swift programming language that delighted us the most ‚Äî its efficiency.</p>

<p>Swift‚Äôs deterministic memory management led to a much lower memory threshold for our service. Not only were our initial results heartening, but after a few iterations of performance improvements, we had close to 40% throughput gain with latencies under 1 ms for 99.9% of requests on our current production hardware. Additionally, the new service had a much smaller memory footprint per instance ‚Äî in the 100s of megabytes ‚Äî an order of magnitude smaller compared to the 10s of gigabytes our Java implementation needed under peak load to sustain the same throughput and latencies. The service runs on Kubernetes, and the migration‚Äôs efficiency improvements allowed us to release about 50% of its capacity for other workloads.</p>

<p><img alt="Resource Utilization Comparison between java vs swift." src="https://www.swift.org/assets/images/swift-at-apple-migrating-the-password-monitoring-service-from-java/resource%20utilization.png" width="840" height="525">
</p>

<p>Our Swift implementation has run smoothly and efficiently in production, making it worth the effort we put into this migration. In addition to outperforming our previous Java-based application, Swift delivered better performance consistency, enhanced safety features, and robust reliability ‚Äî all while requiring fewer resources by utilizing memory and CPU efficiently. With fewer lines of boilerplate code and more flexible design patterns that we used, we look forward to simplified maintenance of our application. Swift was a powerful choice for building fast, resilient, and maintainable applications in our high-demand environment.</p>


  
</article>
</div></div>]]></description>
        </item>
    </channel>
</rss>