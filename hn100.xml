<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0">
    <channel>
        <title>HN100 - Readable Contents</title>
        <link>https://hn.algolia.com/api/v1/search_by_date?tags=%28story,poll%29&amp;numericFilters=points%3E100</link>
        <description>Uses Readability to add bodies to the RSS feed</description>
        <lastBuildDate>Fri, 21 Jun 2024 18:30:05 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>en</language>
        <item>
            <title><![CDATA[Testing Generative AI for Circuit Board Design (110 pts)]]></title>
            <link>https://blog.jitx.com/jitx-corporate-blog/testing-generative-ai-for-circuit-board-design</link>
            <guid>40751020</guid>
            <pubDate>Fri, 21 Jun 2024 16:16:31 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://blog.jitx.com/jitx-corporate-blog/testing-generative-ai-for-circuit-board-design">https://blog.jitx.com/jitx-corporate-blog/testing-generative-ai-for-circuit-board-design</a>, See on <a href="https://news.ycombinator.com/item?id=40751020">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
    <article>
      <time datetime="2024-06-21 16:16:00">
        Jun 21, 2024 9:16:00 AM
      </time>
      
      
      <p><span id="hs_cos_wrapper_post_body" data-hs-cos-general-type="meta_field" data-hs-cos-type="rich_text"><p><em>TLDR: We test LLMs to figure out how helpful they are for designing a circuit board. We focus on utility of frontier models (GPT4o, Claude 3 Opus, Gemini 1.5) across a set of design tasks, to find where they are and are not useful. They look pretty good for building skills, writing code, and getting useful data out of datasheets.</em></p>
<!--more-->
<p><br><em>TLDRN'T: We do not explore any proprietary copilots, or how to apply a things like a diffusion model to the place and route problem.</em></p>
<h2>Introduction</h2>
<div><p>Can an AI-powered chatbot help with a task as precise as circuit board design? These LLMs (Large Language Models) are famous for hallucinating details, and missing a *single* important detail can sink a design. Determinism is hard but super important for electronics design!&nbsp;</p><p>Today, several shallow product offerings are making AI for electronics design look mostly like hype. But I think there is real utility to be found here, if we can take a better approach.&nbsp;</p><p>In this article, we set LLMs to unfair tasks that expert human circuit board designers handle day to day. We're not looking for basic help, rather pushing on what it takes to help an expert do their job better.</p><p>It would be ridiculous (today) to expect great performance from these general purpose AIs on this expert task, especially because it is probably not well represented in the training data.</p><p>To test performance on difficult design tasks, we work with three of today's leading models:<br>- <a href="https://deepmind.google/technologies/gemini/pro/" rel="noopener">Gemini 1.5 Pro</a>&nbsp;from Google<br>- <a href="https://openai.com/index/hello-gpt-4o/" rel="noopener">GPT-4o</a> from OpenAI<br>- <a href="https://www.anthropic.com/news/claude-3-family" rel="noopener">Claude 3 Opus</a> from Anthropic</p><p>And explore prompting strategies to get the best performance out of all the models on each task. As part of the prompt, we also look for opportunities to improve the performance by asking the LLMs to write code to design circuit boards where possible.</p><p>The best results are reported for each model and for each task. Let's dig in!</p></div>
<h2><span>Asking stupid questions</span></h2>
<p><span>There is a lot to know in circuit board design, and nobody has mastered every relevant domain. Asking an LLM stupid questions is a great way to learn.<p>For example an RF engineer will know their own specialty quite well, but might not be a master of supply chain, power supply design, or how to shape copper reliefs to get high yields at the factory.</p><p>( Some RF engineers will scoff at this, but realistically the world contains people trying to learn things.)</p><p>To simulate someone new to a domain, I avoid prompt engineering, and use of precise expert vocabulary. I asked the LLMs a simple question: </p></span><span><em><span>What is the delay per unit length of a trace on a circuit board?</span></em><p>Claude 3 Opus was the clear winner here. Notice that it brought in the relevant concepts (microstrip, stripline, dielectric), got the answer right, and pointed out critical gotchas:</p></span></p>
<p><span><img src="https://blog.jitx.com/hs-fs/hubfs/Pasted%20image%2020240606134032-png.png?width=734&amp;height=1154&amp;name=Pasted%20image%2020240606134032-png.png" loading="lazy" width="734" height="1154" srcset="https://blog.jitx.com/hs-fs/hubfs/Pasted%20image%2020240606134032-png.png?width=367&amp;height=577&amp;name=Pasted%20image%2020240606134032-png.png 367w, https://blog.jitx.com/hs-fs/hubfs/Pasted%20image%2020240606134032-png.png?width=734&amp;height=1154&amp;name=Pasted%20image%2020240606134032-png.png 734w, https://blog.jitx.com/hs-fs/hubfs/Pasted%20image%2020240606134032-png.png?width=1101&amp;height=1731&amp;name=Pasted%20image%2020240606134032-png.png 1101w, https://blog.jitx.com/hs-fs/hubfs/Pasted%20image%2020240606134032-png.png?width=1468&amp;height=2308&amp;name=Pasted%20image%2020240606134032-png.png 1468w, https://blog.jitx.com/hs-fs/hubfs/Pasted%20image%2020240606134032-png.png?width=1835&amp;height=2885&amp;name=Pasted%20image%2020240606134032-png.png 1835w, https://blog.jitx.com/hs-fs/hubfs/Pasted%20image%2020240606134032-png.png?width=2202&amp;height=3462&amp;name=Pasted%20image%2020240606134032-png.png 2202w" sizes="(max-width: 734px) 100vw, 734px"></span></p>
<p><span>Notice the gotchas here! The speed of a signal changes based on which layer of a circuit board you route it on, and what the materials are.<p>You would never know that looking at a most PCB CAD tools, (e.g. Kicad, Altium) because all of their delay matching is in units of length. e.g. 'both these traces have to be 50 mm long' is a constraint you enter with a GUI. But that's not correct if you want your board to work - delay depends on which layer they are routed on, and what materials are in your board.</p><p>Getting that right: A+ answer for Claude here.&nbsp;</p><p>Google Gemini 1.5 performed the worst on this question. Maybe because it brought in a bunch of sources from the internet, and most of the writing in this discipline is dreck generated by SEO content mills. Incorporating AI summarization of lower quality writing seems to make the answers consistently worse.</p></span></p>
<h2><span>Finding Parts</span></h2>
<div><p>One of the superpowers of an experienced engineer is knowing things like: "A DAC + DSP system good enough for this audio pipeline should be about this big, and cost $1.40 in production". They'll then go do some turbo googling and find it.</p><p>How well can an AI that's been fed the entire internet do?</p><p>For this example I chose find parts for something cool: a robot <a href="https://www.youtube.com/watch?v=EER4pdKw2s4" rel="noopener">motor driver linked by optical ethernet</a> (instead of the usual <a href="https://youtu.be/gsOPZltbvgM" rel="noopener">CAN setups</a>). I want smaller cables, hundreds of possible devices, and compact cables that can survive IRL twisting and jostling.</p></div>
<p>You can see a communication layer like this as the daisy-chained black cabling in this picture from <a href="http://www.hebirobotics.com/" rel="noopener">HEBI</a>:</p>
<p><img src="https://blog.jitx.com/hs-fs/hubfs/image-png-1.png?width=510&amp;height=467&amp;name=image-png-1.png" loading="lazy" width="510" height="467" srcset="https://blog.jitx.com/hs-fs/hubfs/image-png-1.png?width=255&amp;height=234&amp;name=image-png-1.png 255w, https://blog.jitx.com/hs-fs/hubfs/image-png-1.png?width=510&amp;height=467&amp;name=image-png-1.png 510w, https://blog.jitx.com/hs-fs/hubfs/image-png-1.png?width=765&amp;height=701&amp;name=image-png-1.png 765w, https://blog.jitx.com/hs-fs/hubfs/image-png-1.png?width=1020&amp;height=934&amp;name=image-png-1.png 1020w, https://blog.jitx.com/hs-fs/hubfs/image-png-1.png?width=1275&amp;height=1168&amp;name=image-png-1.png 1275w, https://blog.jitx.com/hs-fs/hubfs/image-png-1.png?width=1530&amp;height=1401&amp;name=image-png-1.png 1530w" sizes="(max-width: 510px) 100vw, 510px"></p>
<div><p>Time to use AI to find the parts should I use for the ethernet switch, and the optical transceivers.</p><p>Here's the prompt:</p></div>
<p>I want to design a daisy-chained communications layer for a motor driver, using 100M optical Ethernet. Each motor controller will have a microcontroller that interfaces to the Ethernet network over RGMII. It needs to be compact, robust to mechanical vibration and shock, and handle temperatures up to 60 degrees Celsius. Please help me choose parts for the optical connectors, transceivers, and Ethernet networking device.&nbsp;<br>1. Think step by step as an expert electrical engineer.&nbsp;<br>2. Carefully consider how component choices relate to the requirements.&nbsp;<br>3. I'll give you a digital cookie if you get the answer correct.</p>
<div><p>Here I'm pulling more prompt engineering tricks to boost the performance of the model, and being careful in detailed specifications to really get this to work.</p><p>My initial guess for these parts would be:<br>- Use a small three port ethernet switch to for the daisy chain, hopefully something from Micrel (now Microchip) like the <a href="https://www.microchip.com/en-us/product/ksz8863" rel="noopener">KSZ8863MLL</a>.<br>- For the optical connector/transceiver I would go with the <a href="https://www.firecomms.com/optolock/" rel="noopener">OptoLock part</a> on the example system from Hebi. Very small and robust choice.&nbsp;</p><p>It would be amazing to get new suggestions for parts and architectures here. Something going beyond superficial advice into something practical would have huge impact!</p></div>
<div><p>Instead, performance across all models was uniformly bad and no amount of prompting or follow ups could fix it. It felt a lot like trying to get AI to solve a variant of the <a href="https://chatgpt.com/share/67a305cf-dc5f-45a2-a496-0886116f4659" rel="noopener">wolf, sheep, cabbage problem</a>. It's got the answer locked in, and it's hard to get out of that basin.</p><p>Here's a representative conclusion from Gemini 1.5:</p></div>
<div><p><span>&nbsp;Example Part Selection</span><br><span>- Optical Connectors: LC Duplex connectors (Amphenol or similar)</span><br><span>&nbsp;- Optical Transceivers: 100Base-FX SFP transceivers (Finisar, Cisco, or other reputable brands) with industrial temperature ratings</span><br><span>- Ethernet Networking Device: Microchip LAN8742A or Texas Instruments DP83848</span></p><p><span>Digital Cookie Time! 🍪</span></p><p><span>I hope this detailed breakdown helps you choose the right parts for your daisy-chained motor control system. Please let me know if you have any other questions.</span></p></div>
<p>Grading this:</p>
<p>- All models missed that to get the daisy chain you need a three port ethernet switch. They instead recommend an Ethernet Phy, which isn't going to do it.<br>- All models recommend using a huge SFP transceiver. These are usually found on big datacenter boards, and are not known for being shoved into tiny spaces in a robot joint, and then getting kicked around by Boston Dynamics employees.</p>
<div><p>Overall the generative models don't demonstrate a solid grasp on what the motor controller communication network is, or what application-specific considerations need to be made.&nbsp;</p><p>Without that the part selections are way off. This is a consistent experience. The LLMs will always suggest something, but it will be the 'average' application and miss a lot of the important subtlety. i.e. fiber optic ethernet happens in the mostly in the datacenter, so an average application for fiber optic means a datacenter device.</p><p>Trying to get them to choose an ADC, or sensor, or connector, shows similar 'average' results.</p><p>Maybe this speaks to the difficulty of the domain. If you're not within 10% or so of the global optimal performance in EE, your suggestions have near-zero value.</p></div>
<h2><span>Parsing datasheets</span></h2>
<p><span>Sadly, most of the data you need to design a circuit board is baked away in PDF datasheets, instead of being available via APIs. Directly trying to parse PDFs with python tends to not go well, but LLMs should be pretty good at pulling data from these human-structured documents.<p>I tested three different way of pulling information out:<br>- Copy/paste from the PDF, stuff it into a prompt.<br>- Capture a portion as an image, have the LLM interpret the image<br>- Upload the entire PDF</p></span></p>
<p><span>I'll be using the data available for the <a href="https://docs.nordicsemi.com/bundle/ps_nrf5340/page/keyfeatures_html5.html" rel="noopener">Nordic nRF5340 WLCSP</a> for the experiment. A little 4.0 x 4.4 mm Bluetooth chip, with an 820 page datasheet.&nbsp;</span></p>
<p><span><img src="https://blog.jitx.com/hs-fs/hubfs/image-png-2.png?width=350&amp;height=324&amp;name=image-png-2.png" loading="lazy" width="350" height="324" srcset="https://blog.jitx.com/hs-fs/hubfs/image-png-2.png?width=175&amp;height=162&amp;name=image-png-2.png 175w, https://blog.jitx.com/hs-fs/hubfs/image-png-2.png?width=350&amp;height=324&amp;name=image-png-2.png 350w, https://blog.jitx.com/hs-fs/hubfs/image-png-2.png?width=525&amp;height=486&amp;name=image-png-2.png 525w, https://blog.jitx.com/hs-fs/hubfs/image-png-2.png?width=700&amp;height=648&amp;name=image-png-2.png 700w, https://blog.jitx.com/hs-fs/hubfs/image-png-2.png?width=875&amp;height=810&amp;name=image-png-2.png 875w, https://blog.jitx.com/hs-fs/hubfs/image-png-2.png?width=1050&amp;height=972&amp;name=image-png-2.png 1050w" sizes="(max-width: 350px) 100vw, 350px"></span></p>
<p>Observations on this part: &nbsp;</p>
<ol>
<li>This is by no means the largest datasheet for a electrical component but has many typical features including pin tables, mechanical drawings, application specific notes, graphs, etc</li>
<li>This datasheet has significantly more content than you might expects for simpler components - like OpAmps or digital logic gates. If the LLMs are useful here, they will likely be useful for smaller cases.</li>
<li>This datasheet is about 570k tokens, a little more than half the huge context window available from Gemini.</li>
</ol>
<h3>Make a symbol and footprint</h3>
<div><p>First step is a to make a linked symbol and footprint (landpattern) for our component.&nbsp;</p><p>The most effective method here was to load the entire datasheet into the LLM via the chat file upload mechanism, and then query it interactively to get the details right. Gemini 1.5 was the most reliable for this task.</p><p>Here's the prompt to get the pin table (after uploading the datasheet):</p></div>
<p>Please write the the pin table for the wlcsp version of the nRF5340 as code using the following format:&nbsp;<br>1. One line for each pin&nbsp;<br>2. Each line structured as " [ pin-name | pad-number | direction | bank ]", where pin-name is the name of a pin e.g. GND, pad-number is the number of the pad that maps to that pin, e.g, 28, direction is Left is the pin is an input, and Right otherwise, bank indicates the function, e.g. gpio.&nbsp;<br>3. Wrap numbers following letters in square brackets, e.g. K3 becomes K[3], C1 becomes C[1], F10 becomes F[10]&nbsp;<br>4. Replace periods with square brackets, e.g. P0.20 becomes P0[20]</p>
<p>The AI then returns a block of text with our pin list in a usable format:</p>
<p><code>[ XC1 | A1 | Left | analog_input ]&nbsp;</code><br><code>[ XC2 | A2 | Left | analog_input ]&nbsp;</code><br><code>[ VDD | A3 | Right | power ]&nbsp;</code><br><code>[ VSS | A5 | Left | power ]&nbsp;</code><br><code>[ DECD | A7 | Right | power ]&nbsp;</code><br><code>[ DCCD | A9 | Right | power ]&nbsp;</code><br><code>[ D-| A11 | Left | usb ]&nbsp;</code><br><code>[ D+ | A12 | Left | usb ]&nbsp;</code><br><code>[ DECRF | B1 | Right | power ]&nbsp;</code><br><code>[ VSS | B2 | Left | power ]&nbsp;</code><br><code>[ DECR | B4 | Right | power ]&nbsp;</code><br><code>[ DECN | B5 | Right | power ]</code><br><code>... (continued)</code></p>
<div><p>It got this list exactly correct, impressive for 95 rows of data buried in an 820 page datasheet. Usually I have to get a model to write a little code to reach that level of accuracy, but that was unnecessary here.</p><p>Next I gave it some example code for a BGA footprint and had it generate a new version based on the datasheet information. Here's the resulting code:</p></div>
<div><p><code>; This test is based on the Nordic Semiconductor, nRF5340</code><br><code>&nbsp; ; Datasheet:</code><br><code>&nbsp; ; &nbsp;https://infocenter.nordicsemi.com/pdf/nRF5340_PS_v1.4.pdf</code><br><code>&nbsp; ;</code><br><code>&nbsp; ; Drawing:</code><br><code>&nbsp; ; &nbsp;https://infocenter.nordicsemi.com/pdf/nRF5340_PS_v1.4.pdf</code><br><code>&nbsp; ;</code><br><code>&nbsp; ; NSMD</code><br><code>&nbsp; val rows = 12</code><br><code>&nbsp; val cols = 8</code><br><code>&nbsp; val grid = Grid-Numbering(rows, cols)</code></p><p><code>&nbsp; val missing = to-pad-island(grid, "A[1] B[1] B[3] C[1] C[3] D[3] D[4] D[5] D[6] D[7] D[8] D[9] E[1] E[9] F[4] F[6] F[7] F[9] G[1] G[4] G[9] H[4] H[5] H[6] H[7] H[8] H[9]")</code></p><p><code>&nbsp; val planner = Full-Matrix-Planner(</code><br><code>&nbsp; &nbsp; ; Several pads are not present</code><br><code>&nbsp; &nbsp; inactive = missing,</code><br><code>&nbsp; &nbsp; pad-config = PadConfig-D(</code><br><code>&nbsp; &nbsp; &nbsp; copper-D-adj = ipc-bga-copper(</code><br><code>&nbsp; &nbsp; &nbsp; &nbsp; NonSolderMaskDefined,</code><br><code>&nbsp; &nbsp; &nbsp; &nbsp; density-level = DensityLevelC</code><br><code>&nbsp; &nbsp; &nbsp; &nbsp; ),</code><br><code>&nbsp; &nbsp; &nbsp; mask-D-adj = 0.0,</code><br><code>&nbsp; &nbsp; )</code><br><code>&nbsp; )</code></p><p><code>&nbsp; val body = PackageBody(</code><br><code>&nbsp; &nbsp; width = 4.390 +/- 0.047,</code><br><code>&nbsp; &nbsp; length = 3.994 +/- 0.047,</code><br><code>&nbsp; &nbsp; height = 0.71 +/- [0.06, 0.0]</code><br><code>&nbsp; )</code></p><p><code>&nbsp; val pkg = BGA(</code><br><code>&nbsp; &nbsp; num-leads = 95,</code><br><code>&nbsp; &nbsp; rows = rows,</code><br><code>&nbsp; &nbsp; columns = cols,</code><br><code>&nbsp; &nbsp; lead-diam = 0.35,</code><br><code>&nbsp; &nbsp; pitch = 0.35,</code><br><code>&nbsp; &nbsp; package-body = body,</code><br><code>&nbsp; &nbsp; lead-numbering = grid,</code><br><code>&nbsp; &nbsp; pad-planner = planner,</code><br><code>&nbsp; &nbsp; density-level = DensityLevelB</code><br><code>&nbsp; )</code></p></div>
<div><p>A few more misses here than in the pin table, but close overall. I have to go in and separately prompt for the depopulated balls in the grid, as well as correct the dimensions in the PackageBody.&nbsp;</p><p>(Note that I'm generating JITX code because it has a text-native representation. I bet that you could get the model to emit something like a Kicad symbol/footprint with a little more nudging in the prompt.)</p><p>Running the generated code (which you can find <a href="https://github.com/JITx-Inc/generative-ai-test-cases/blob/main/component-model/nRF5340.stanza" rel="noopener">here</a>), we get our footprint and symbols:</p></div>
<p><img src="https://blog.jitx.com/hs-fs/hubfs/image-png-3.png?width=2682&amp;height=1404&amp;name=image-png-3.png" loading="lazy" width="2682" height="1404" srcset="https://blog.jitx.com/hs-fs/hubfs/image-png-3.png?width=1341&amp;height=702&amp;name=image-png-3.png 1341w, https://blog.jitx.com/hs-fs/hubfs/image-png-3.png?width=2682&amp;height=1404&amp;name=image-png-3.png 2682w, https://blog.jitx.com/hs-fs/hubfs/image-png-3.png?width=4023&amp;height=2106&amp;name=image-png-3.png 4023w, https://blog.jitx.com/hs-fs/hubfs/image-png-3.png?width=5364&amp;height=2808&amp;name=image-png-3.png 5364w, https://blog.jitx.com/hs-fs/hubfs/image-png-3.png?width=6705&amp;height=3510&amp;name=image-png-3.png 6705w, https://blog.jitx.com/hs-fs/hubfs/image-png-3.png?width=8046&amp;height=4212&amp;name=image-png-3.png 8046w" sizes="(max-width: 2682px) 100vw, 2682px"></p>
<div><p>I really like the combination of code and scraped data here because of precision and flexibility.&nbsp;</p><p>I would much rather trust a hand-coded BGA footprint generator for the details, and have the LLM tell me which pads to omit. The backstop of the LLM generating the list of pins <em>and the missing pins</em>, then forcing them both to line up to line up in the code is a nice touch. I can also dig in and edit parameters directly if I want to customize.</p><p>Past the standard component model with a symbol and footprint, the LLM can also pull most of the useful data from the PDF. e.g. here is a generated parameterization for the IO pin levels for checking correctness of connections in a design:</p></div>
<p><code>property(P0[0].digital_io) = DigitalIO(CMOSOutput(VSS + 0.4, VDD - 0.4), 0.3*VDD, 0.7*VDD, "VDD", "VSS", 0)</code></p>
<p>If you don't have the AI return an answer as code, it tends to spread the data you want across multiple paragraphs, and it is easier to get the answer you want by just reading the datasheet instead.</p>
<p>It can even read a bit of the reference schematic and turn it into a netlist:</p>
<p><code><img src="https://blog.jitx.com/hs-fs/hubfs/image-png-4.png?width=1104&amp;height=1426&amp;name=image-png-4.png" loading="lazy" width="1104" height="1426" srcset="https://blog.jitx.com/hs-fs/hubfs/image-png-4.png?width=552&amp;height=713&amp;name=image-png-4.png 552w, https://blog.jitx.com/hs-fs/hubfs/image-png-4.png?width=1104&amp;height=1426&amp;name=image-png-4.png 1104w, https://blog.jitx.com/hs-fs/hubfs/image-png-4.png?width=1656&amp;height=2139&amp;name=image-png-4.png 1656w, https://blog.jitx.com/hs-fs/hubfs/image-png-4.png?width=2208&amp;height=2852&amp;name=image-png-4.png 2208w, https://blog.jitx.com/hs-fs/hubfs/image-png-4.png?width=2760&amp;height=3565&amp;name=image-png-4.png 2760w, https://blog.jitx.com/hs-fs/hubfs/image-png-4.png?width=3312&amp;height=4278&amp;name=image-png-4.png 3312w" sizes="(max-width: 1104px) 100vw, 1104px"></code></p>
<p><code>* Power Supply Nets:</code><br><code>net (nrf.VDD C4.p[1])</code><br><code>net (nrf.VDD C10.p[1])</code><br><code>net (nrf.VDD C11.p[1])</code><br><code>net (nrf.VDD C12.p[1])</code><br><code>net (nrf.VDD C13.p[1])</code><br><code>net (nrf.VDD C15.p[1])</code><br><code>net (nrf.VDD C17.p[1])</code><br><code>.... (continued)</code></p>
<div><p>It hallucinated this task a little bit, inventing resistors here and there.&nbsp;</p><p>Using LLMs to pull data from challenging sources and turn it into useful code was a helpful automation technique (and personally the one I use most). Accuracy can be boosted further by using extraction techniques to cross-reference the data.</p></div>
<h2><span>Designing circuits</span></h2>
<p><span>We've seen that LLMs can understand an image of a schematic well enough to (mostly) turn it into a netlist. Can they take on a much more challenging task and design the circuit itself?<p>As shown earlier, LLMs generated kinda-right, but actually very wrong suggestions for key components. To avoid that dependency, we tested our AIs on an analog circuit design task. This test is more about doing some math, and correctly arranging resistors and capacitors.</p><p>The task is to design an pre-amplifier for an electret microphone -- boosting and filtering an audio signal for sampling by an ADC. We start with a basic architecture exploration. Here's the prompt:</p></span></p>
<p><span>I want to design a microphone pre-amp where I need to bias the electret microphone and create a single-ended signal out to drive the ADC of a microcontroller. Include a bandpass filter with a gain of 20dB, corner frequencies at 100Hz and 20kHz. Help me select an architecture for this amplifier. Propose different filter architectures and describe their pros and cons for this application.&nbsp;<br>1. Think step by step as an expert electrical engineer&nbsp;<br>2. Carefully consider the requirements&nbsp;<br>3. If you get the answer right you get a digital cookie.</span></p>
<p><span>Claude 3 Opus came back with the best answer of the three contenders. I'll analyze it piece by piece.</span></p>
<p><span>Okay, let's think through this step-by-step as an expert electrical engineer:<br>1. Microphone biasing: Electret microphones typically require a DC bias voltage, usually between 1.5V to 10V, applied through a bias resistor. The value of this resistor impacts the microphone's output impedance and noise. A common range is 2.2kΩ to 10kΩ. We'll need to check the microphone's datasheet for specifics.<br>2. Amplifier architecture: For a single-ended output to drive an ADC, we can use a single-supply op-amp configuration. Some good options are:&nbsp;<br>&nbsp; &nbsp; a) Non-inverting amplifier: Simple, high input impedance which is good for the high-impedance mic signal. Gain is set by the feedback resistor ratio.&nbsp;<br>&nbsp; &nbsp; b) Inverting amplifier: Also simple, but lower input impedance. Gain is also set by resistor ratio.&nbsp;<br>&nbsp; &nbsp; c) Push-pull: Combines inverting and non-inverting amps for more output current drive, but more complex. For simplicity and good mic interfacing, a non-inverting amp seems best here.</span></p>
<div><p>Claude did a good job calling out the need to bias the microphone, nice to have that called out explicitly.</p><p>The proposals for non-inverting, and inverting amplifiers are reasonable. The proposal for push-pull configurations is nonsense. A <a href="https://en.wikipedia.org/wiki/Push%E2%80%93pull_output" rel="noopener">Push-Pull configuration</a> describes the transistors inside the op-amp at the output stage. It doesn't have anything to do with the architecture of the op amp circuit.</p><p>Moving on to the filter suggestions:</p></div>
<p><span>3. Bandpass filter: We need a 100Hz to 20kHz passband with 20dB gain. Some filter options:&nbsp;</span><br><span>&nbsp; &nbsp; &nbsp;a) Passive RC: Simple, but requires large component values for low frequencies, and loading affects cutoff frequencies. Gain must come from amp.&nbsp;</span><br><span>&nbsp; &nbsp; &nbsp;b) Active 1st-order: Sallen-Key or Multiple-Feedback topologies. Each is 1st-order, so we'd cascade a low-pass and high-pass for the bandpass response. Easy to integrate gain. Cutoff frequencies affected by component tolerance.&nbsp;</span><br><span>&nbsp; &nbsp; &nbsp;c) Active 2nd-order: Single stage 2nd-order bandpass like a Dual-Amplifier Bandpass (DABP). More complex, but more efficient than cascaded 1st-order. More stable cutoff frequencies.&nbsp;</span><br><span>&nbsp; &nbsp; &nbsp;d) Switched-capacitor: Uses a clock to set cutoff frequencies. Very stable, but requires a clock signal.&nbsp;</span></p>
<p><span>The comments on the Passive RC filter are reasonable.&nbsp;<p>The Sallen-Key analysis is wrong - Sallen-Key is a <a href="https://www.ti.com/lit/an/sboa231/sboa231.pdf?ts=1718092808060&amp;ref_url=https%253A%252F%252Fwww.google.com%252F" rel="noopener">second order filter</a>&nbsp;and can be unstable with this much gain.</p><p>The suggestion for the <a href="https://www.analog.com/media/en/training-seminars/tutorials/MT-209.pdf" rel="noopener">DABP</a> isn't wrong per se, but it is missing some nuance. It doesn't allow for very high gain (limit of 2.0). The complexity of a DABP justified when you need to handle higher frequencies and higher Q levels. This application doesn't require that, so it's a weird choice.</p><p>A switched capacitor filter would be bonkers -- complex, noise-sensitive, and requiring exotic components.</p></span></p>
<p><span>Moving on:</span></p>
<div><p><span>For this application, an active 2nd-order filter like a DABP seems a good balance of simplicity, stability, and easy gain integration. &nbsp;</span><br><span>1. Amplifier gain: The filter will have 20dB gain, which is a factor of 10. We can implement this with a non-inverting amp by setting the feedback resistor ratio to 9. For example, if R1 = 1kΩ, then R2 = 9kΩ.</span><br><span>2. Component values: We'll need to calculate the exact resistor and capacitor values for the DABP filter based on the desired cutoff frequencies and gain. There are online calculators to help with this.</span><br><span>&nbsp;So in summary, I propose a non-inverting op-amp configuration for the microphone, followed by a 2nd-order DABP active filter with 20dB gain. The exact component values will depend on the specific microphone chosen and the filter design equations. &nbsp;</span></p><p><span>How's that? Let me know if you have any other questions!</span></p></div>
<p><span>Here, Claude chooses the DABP and estimates some reasonablish values given that choice. (Usually you would choose values greater than 10k and less than 1M in op amp circuit design, to make assumptions about input and output impedance hold better). The wording is a bit confusing but it could be interpreted as a non-inverting op-amp for the gain stage, followed by a DABP filter with a lower gain. Which can work! Technically!<p>It's making slightly strange decisions across the board, but let's proceed with creating a complete design.&nbsp;</p><p>With a little more prompting, we can get Claude to write some code to do the detailed design and generate the circuit:</p></span></p>
<p><span><code>Great! Let's dive into the calculations and generate the netlists. We'll assume a 3.3V power supply and a microphone bias of 2.5V. We'll use an ideal op-amp model for simplicity.</code><p><code>; Microphone Bias Circuit:</code></p><p><code>; - Let's choose a bias resistor of 2.2kΩ.</code><br><code>; - Assuming negligible current draw from the op-amp, the mic bias will be very <br></code></p></span><span><code>; close to 2.5V.</code><p><code>; Netlist:&nbsp;</code><br><code>inst mic : electret-mic&nbsp;</code><br><code>inst rbias : chip-resistor(2200.0)&nbsp;</code><br><code>inst c1 : ceramic-cap(1.0e-6) ; Decoupling cap, assuming 1uF&nbsp;</code><br><code>net (vdd rbias.p[1])&nbsp;</code><br><code>net (rbias.p[2] mic.vdd c1.p[1])&nbsp;</code><br><code>net (gnd mic.gnd c1.p[2])</code></p></span></p>
<p><span>The decoupling cap in the bias circuit here really hampers the high-frequency performance of the circuit. What the circuit actually needs is a <em>coupling cap</em>, shown in the <code>human.mic</code> circuit here:</span></p>
<p><span><img src="https://blog.jitx.com/hs-fs/hubfs/image-png-Jun-21-2024-03-34-17-2563-PM.png?width=1205&amp;height=480&amp;name=image-png-Jun-21-2024-03-34-17-2563-PM.png" loading="lazy" width="1205" height="480" srcset="https://blog.jitx.com/hs-fs/hubfs/image-png-Jun-21-2024-03-34-17-2563-PM.png?width=603&amp;height=240&amp;name=image-png-Jun-21-2024-03-34-17-2563-PM.png 603w, https://blog.jitx.com/hs-fs/hubfs/image-png-Jun-21-2024-03-34-17-2563-PM.png?width=1205&amp;height=480&amp;name=image-png-Jun-21-2024-03-34-17-2563-PM.png 1205w, https://blog.jitx.com/hs-fs/hubfs/image-png-Jun-21-2024-03-34-17-2563-PM.png?width=1808&amp;height=720&amp;name=image-png-Jun-21-2024-03-34-17-2563-PM.png 1808w, https://blog.jitx.com/hs-fs/hubfs/image-png-Jun-21-2024-03-34-17-2563-PM.png?width=2410&amp;height=960&amp;name=image-png-Jun-21-2024-03-34-17-2563-PM.png 2410w, https://blog.jitx.com/hs-fs/hubfs/image-png-Jun-21-2024-03-34-17-2563-PM.png?width=3013&amp;height=1200&amp;name=image-png-Jun-21-2024-03-34-17-2563-PM.png 3013w, https://blog.jitx.com/hs-fs/hubfs/image-png-Jun-21-2024-03-34-17-2563-PM.png?width=3615&amp;height=1440&amp;name=image-png-Jun-21-2024-03-34-17-2563-PM.png 3615w" sizes="(max-width: 1205px) 100vw, 1205px"></span></p>
<p><span>Presumably in the training corpus, many articles and datasheets write extensively about decoupling caps. Maybe the "average" recommendation wins out here, and once it sees a positive and negative pin it slaps a capacitor across them right away. In our tests (and most product demos), LLMs <em>loved</em> to give naive advice about decoupling, which usually isn't as actively harmful as it is here.<br></span></p>
<p><span>Moving on to the amplifier:</span></p>
<p><code>; Non-Inverting Amplifier:<p>; - Let's set the gain to 10 (20dB).<br>; - Choose R1 = 1kΩ, then R2 = 9kΩ.</p><p>; Netlist:&nbsp;<br>inst amp : op-amp&nbsp;<br>inst r1 : chip-resistor(1000.0)&nbsp;<br>inst r2 : chip-resistor(9000.0)&nbsp;<br>net (mic.out r1.p[1])&nbsp;<br>net (r1.p[2] amp.v- r2.p[1])&nbsp;<br>net (amp.out r2.p[2])</p></code></p>
<p>The AI describes the amplifier as non-inverting, but then feeds the signal into the inverting input. It's also missing a few other parts and nets that would make for a working circuit. Easier to see in the generated schematic:</p>
<p><img src="https://blog.jitx.com/hs-fs/hubfs/image-png-Jun-21-2024-03-38-28-2912-PM.png?width=561&amp;height=349&amp;name=image-png-Jun-21-2024-03-38-28-2912-PM.png" loading="lazy" width="561" height="349" srcset="https://blog.jitx.com/hs-fs/hubfs/image-png-Jun-21-2024-03-38-28-2912-PM.png?width=281&amp;height=175&amp;name=image-png-Jun-21-2024-03-38-28-2912-PM.png 281w, https://blog.jitx.com/hs-fs/hubfs/image-png-Jun-21-2024-03-38-28-2912-PM.png?width=561&amp;height=349&amp;name=image-png-Jun-21-2024-03-38-28-2912-PM.png 561w, https://blog.jitx.com/hs-fs/hubfs/image-png-Jun-21-2024-03-38-28-2912-PM.png?width=842&amp;height=524&amp;name=image-png-Jun-21-2024-03-38-28-2912-PM.png 842w, https://blog.jitx.com/hs-fs/hubfs/image-png-Jun-21-2024-03-38-28-2912-PM.png?width=1122&amp;height=698&amp;name=image-png-Jun-21-2024-03-38-28-2912-PM.png 1122w, https://blog.jitx.com/hs-fs/hubfs/image-png-Jun-21-2024-03-38-28-2912-PM.png?width=1403&amp;height=873&amp;name=image-png-Jun-21-2024-03-38-28-2912-PM.png 1403w, https://blog.jitx.com/hs-fs/hubfs/image-png-Jun-21-2024-03-38-28-2912-PM.png?width=1683&amp;height=1047&amp;name=image-png-Jun-21-2024-03-38-28-2912-PM.png 1683w" sizes="(max-width: 561px) 100vw, 561px">(Cue EE eye twitching)</p>
<p>Moving on to the DABP filter implementation:</p>
<p><code>; DABP Bandpass Filter:<p>; - Cutoff frequencies: fl = 100Hz, fh = 20kHz<br>; - Gain at center frequency: G0 = 10 (20dB)<br>; - Quality factor: Let's choose Q = 1 for a fairly wide passband<br>; - Center frequency: f0 = sqrt(fl * fh) ≈ 1.414kHz<br>; - Let's choose C1 = C2 = 10nF<br>; - R1 = R2 = 1 / (2 * pi * f0 * C1) ≈ 11.2kΩ, let's use 11kΩ<br>; - R3 = R1 / (2 * Q * G0) ≈ 550Ω, let's use 560Ω<br>; - R4 = 2 * Q * R1 / G0 ≈ 2.2kΩ</p><p>; Netlist:&nbsp;<br>inst op1 : op-amp&nbsp;<br>inst op2 : op-amp&nbsp;<br>inst r1 : chip-resistor(11000.0)&nbsp;<br>inst r2 : chip-resistor(11000.0)&nbsp;<br>inst r3 : chip-resistor(560.0)&nbsp;<br>inst r4 : chip-resistor(2200.0)&nbsp;<br>inst c1 : ceramic-cap(10.0e-9)&nbsp;<br>inst c2 : ceramic-cap(10.0e-9)&nbsp;<br>net (amp.out op1.v+ r1.p[1] c1.p[1])&nbsp;<br>net (op1.v- op2.out r1.p[2] c1.p[2])&nbsp;<br>net (op1.out r3.p[1])&nbsp;<br>net (r3.p[2] op2.v- r2.p[1] c2.p[1])&nbsp;<br>net (op2.v+ r2.p[2] c2.p[2] r4.p[1])&nbsp;<br>net (r4.p[2] gnd)</p></code></p>
<p><img src="https://blog.jitx.com/hs-fs/hubfs/image-png-Jun-21-2024-03-41-30-4263-PM.png?width=934&amp;height=425&amp;name=image-png-Jun-21-2024-03-41-30-4263-PM.png" loading="lazy" width="934" height="425" srcset="https://blog.jitx.com/hs-fs/hubfs/image-png-Jun-21-2024-03-41-30-4263-PM.png?width=467&amp;height=213&amp;name=image-png-Jun-21-2024-03-41-30-4263-PM.png 467w, https://blog.jitx.com/hs-fs/hubfs/image-png-Jun-21-2024-03-41-30-4263-PM.png?width=934&amp;height=425&amp;name=image-png-Jun-21-2024-03-41-30-4263-PM.png 934w, https://blog.jitx.com/hs-fs/hubfs/image-png-Jun-21-2024-03-41-30-4263-PM.png?width=1401&amp;height=638&amp;name=image-png-Jun-21-2024-03-41-30-4263-PM.png 1401w, https://blog.jitx.com/hs-fs/hubfs/image-png-Jun-21-2024-03-41-30-4263-PM.png?width=1868&amp;height=850&amp;name=image-png-Jun-21-2024-03-41-30-4263-PM.png 1868w, https://blog.jitx.com/hs-fs/hubfs/image-png-Jun-21-2024-03-41-30-4263-PM.png?width=2335&amp;height=1063&amp;name=image-png-Jun-21-2024-03-41-30-4263-PM.png 2335w, https://blog.jitx.com/hs-fs/hubfs/image-png-Jun-21-2024-03-41-30-4263-PM.png?width=2802&amp;height=1275&amp;name=image-png-Jun-21-2024-03-41-30-4263-PM.png 2802w" sizes="(max-width: 934px) 100vw, 934px">Generated circuit (left), versus reference from ADI (right).</p>
<p>A few more misses on the netlist here, and about as correct as the amplifier (which is to say, not very). We also see the weirdly strong default of attaching capacitors across positive and negative pins. I guess LLMs are high decouplers.</p>
<p>And to round it off, we've got another reminder to add decoupling caps, and a reasonable note about what to watch out for:</p>
<p><span>The output of op2 is the final output of the pre-amp circuit. This should give you a good starting point. Remember to add decoupling capacitors and power supply connections as needed. Also, the actual component values may need to be adjusted based on the real specs of your chosen op-amp and the exact requirements of your ADC. Let me know if you have any other questions!</span></p>
<h3><span>Comparing against ground truth</span></h3>
<p><span>This <a href="https://www.ti.com/lit/ug/tidu765/tidu765.pdf" rel="noopener">reference design</a> from Texas Instruments is a good implementation of the requirements we asked these LLMs to design to. It uses passive RC filters (which the AI considered and avoided), and a single op-amp. I did a quick implementation of this in JITX (<a href="https://github.com/JITx-Inc/generative-ai-test-cases/blob/main/amplifier/reference.stanza" rel="noopener">code here</a>) to compare the AI generated circuit (<a href="https://github.com/JITx-Inc/generative-ai-test-cases/blob/main/amplifier/ai-generated.stanza" rel="noopener">code here</a>) to the TI version.</span></p>
<p><span><img src="https://blog.jitx.com/hs-fs/hubfs/image-png-Jun-21-2024-03-49-51-0951-PM.png?width=849&amp;height=547&amp;name=image-png-Jun-21-2024-03-49-51-0951-PM.png" loading="lazy" width="849" height="547" srcset="https://blog.jitx.com/hs-fs/hubfs/image-png-Jun-21-2024-03-49-51-0951-PM.png?width=425&amp;height=274&amp;name=image-png-Jun-21-2024-03-49-51-0951-PM.png 425w, https://blog.jitx.com/hs-fs/hubfs/image-png-Jun-21-2024-03-49-51-0951-PM.png?width=849&amp;height=547&amp;name=image-png-Jun-21-2024-03-49-51-0951-PM.png 849w, https://blog.jitx.com/hs-fs/hubfs/image-png-Jun-21-2024-03-49-51-0951-PM.png?width=1274&amp;height=821&amp;name=image-png-Jun-21-2024-03-49-51-0951-PM.png 1274w, https://blog.jitx.com/hs-fs/hubfs/image-png-Jun-21-2024-03-49-51-0951-PM.png?width=1698&amp;height=1094&amp;name=image-png-Jun-21-2024-03-49-51-0951-PM.png 1698w, https://blog.jitx.com/hs-fs/hubfs/image-png-Jun-21-2024-03-49-51-0951-PM.png?width=2123&amp;height=1368&amp;name=image-png-Jun-21-2024-03-49-51-0951-PM.png 2123w, https://blog.jitx.com/hs-fs/hubfs/image-png-Jun-21-2024-03-49-51-0951-PM.png?width=2547&amp;height=1641&amp;name=image-png-Jun-21-2024-03-49-51-0951-PM.png 2547w" sizes="(max-width: 849px) 100vw, 849px"></span></p>
<p><span>The AI generated circuit was three times the cost and size of the design created by that expert engineer at TI. It is also missing many of the necessary connections.&nbsp;<p>I found this breakdown surprising - it had no problems generating code in general, and was able to generate a syntactically correct netlist. The code runs! It's just that the pins are not in the right net statements and the resulting circuit is not correct.</p></span></p>
<h3><span>Testing higher level code</span></h3>
<p><span>In fairness, it's rare to write code like this to design a circuit board. In practice you're primarily calling higher level functions, e.g. selecting an op-amp, configuring a filter with parameters, then using a parametric pre-amp circuit:</span></p>
<p><code>&nbsp; val OpAmpType = components/STMicroelectronics/TS971ILT/component</code><br><code>&nbsp; val bandpass = BandpassConfig(gain = 34.0, low-cut = 100.0, high-pass = 20.0e3)</code><br><code>&nbsp; val preampType = ocdb/modules/amplifiers/microphone-preamp(OpAmpType, bandpass)</code><br><code>&nbsp; inst amp : preampType</code></p>
<div><p>Would LLMs do better if we try to get them to define and use higher-level functions instead?</p><p>Gemini 1.5 did the best here, inventing some reasonable APIs to functions that would exist in a library of helpful circuits:</p></div>
<p><code># Modules</code><br><code>amp = inverting_amplifier("amp1", gain=-20.0)&nbsp;</code><br><code>filt = sallen_key_filter("filt1", low_cutoff=100.0, high_cutoff=20.0e3)&nbsp;</code><br><code># Connections&nbsp;</code><br><code>net (mic.pos r_bias.p[1]) # Microphone to bias resistor&nbsp;</code><br><code>net (r_bias.p[2] c1.p[1] amp.in) # Bias resistor to amp through coupling capacitor</code></p>
<p>(Still misses on the netlist generation though.)</p>
<div><p>Our experiments with code generation reveal an interesting pattern: the LLMs are not great at generating detailed code for a netlist <em>to implement a circuit design</em>. That was a little surprising because as shown in the datasheet parsing section, they can write a 95% correct netlist matching an uploaded image of a schematic. This difference suggests that LLMs excel at information extraction and transformation tasks, but struggle with original design synthesis.</p><p>Having the LLMs generate higher-level code worked better than raw netlist generation, and seems like a promising path forward. This suggests a hybrid approach where human experts create parametric implementations of common circuits, which LLMs can then utilize and adapt, potentially increasing overall reliability.</p></div>
<p>Future research could either explore fine-tuning models specifically for netlist generation tasks, or (this being generative AI) wait a month and see how well the next generation of models does on this problem.</p>
<h2><span>Conclusion</span></h2>
<p><span>Circuit board design requires a lot of precision to get all the details correct -- for unsupervised AI techniques, I think there is a data problem here.&nbsp;<p>The context and meaning of a design -- What does this do? How do you know if it's good? -- are usually absent from schematic and board design data. Maybe there's a little text comment on the schematic with filter calculations, but that's usually it, and the rest lives in the engineer's head.</p><p>An electrical engineer looks at a schematic, understands a portion as a Sallen-Key filter, knows the limitations and function of that circuit, and composes that understanding with the rest of the design. A tremendous amount of context, which is not in any of the design data you can scrape from the internet.</p><p>Our experiments show that generative AI models can be useful for circuit board design, if you use them to write code. With that approach, there were still significant differences across Gemini 1.5, Claude 3 Opus, and GPT-4o for circuit board design.&nbsp;</p><p>Claude 3 was most helpful for answering questions to learn new domains. There's a risk of hallucinations, but those usually fall away if you interrogate the model until it's consistent. Certainly Claude is better and more helpful than the *average* article on circuit board design you would find via web search (there are very good articles, but they can be hard to find).</p><p>Gemini's huge context window was very helpful for pulling component data out of datasheets. It was the one area where the AI 'just worked' and we got way more usability than we expected.</p><p>I'm interested in further testing to find the limits of this technique. Google has done a lot of work on the "Text Haystack" problem to pull tiny pieces of information accurately out of a massive corpus of data, and that's serving them well here.</p></span></p>
<p><span><img src="https://blog.jitx.com/hs-fs/hubfs/image-png-Jun-21-2024-03-59-44-3288-PM.png?width=933&amp;height=601&amp;name=image-png-Jun-21-2024-03-59-44-3288-PM.png" loading="lazy" width="933" height="601" srcset="https://blog.jitx.com/hs-fs/hubfs/image-png-Jun-21-2024-03-59-44-3288-PM.png?width=467&amp;height=301&amp;name=image-png-Jun-21-2024-03-59-44-3288-PM.png 467w, https://blog.jitx.com/hs-fs/hubfs/image-png-Jun-21-2024-03-59-44-3288-PM.png?width=933&amp;height=601&amp;name=image-png-Jun-21-2024-03-59-44-3288-PM.png 933w, https://blog.jitx.com/hs-fs/hubfs/image-png-Jun-21-2024-03-59-44-3288-PM.png?width=1400&amp;height=902&amp;name=image-png-Jun-21-2024-03-59-44-3288-PM.png 1400w, https://blog.jitx.com/hs-fs/hubfs/image-png-Jun-21-2024-03-59-44-3288-PM.png?width=1866&amp;height=1202&amp;name=image-png-Jun-21-2024-03-59-44-3288-PM.png 1866w, https://blog.jitx.com/hs-fs/hubfs/image-png-Jun-21-2024-03-59-44-3288-PM.png?width=2333&amp;height=1503&amp;name=image-png-Jun-21-2024-03-59-44-3288-PM.png 2333w, https://blog.jitx.com/hs-fs/hubfs/image-png-Jun-21-2024-03-59-44-3288-PM.png?width=2799&amp;height=1803&amp;name=image-png-Jun-21-2024-03-59-44-3288-PM.png 2799w" sizes="(max-width: 933px) 100vw, 933px">From the <a href="https://storage.googleapis.com/deepmind-media/gemini/gemini_v1_5_report.pdf" rel="noopener">Gemini 1.5 Report</a></span></p>
<p><span>Surprisingly GPT-4o didn't have the most useful answer in any of the categories we tested. It feels wrong to not include any outputs from the model in this summary article, so here's how GPT-4o feels about coming in third:</span></p>
<p><span><img src="https://blog.jitx.com/hs-fs/hubfs/image-png-Jun-21-2024-04-01-30-3403-PM.png?width=345&amp;height=342&amp;name=image-png-Jun-21-2024-04-01-30-3403-PM.png" loading="lazy" width="345" height="342" srcset="https://blog.jitx.com/hs-fs/hubfs/image-png-Jun-21-2024-04-01-30-3403-PM.png?width=173&amp;height=171&amp;name=image-png-Jun-21-2024-04-01-30-3403-PM.png 173w, https://blog.jitx.com/hs-fs/hubfs/image-png-Jun-21-2024-04-01-30-3403-PM.png?width=345&amp;height=342&amp;name=image-png-Jun-21-2024-04-01-30-3403-PM.png 345w, https://blog.jitx.com/hs-fs/hubfs/image-png-Jun-21-2024-04-01-30-3403-PM.png?width=518&amp;height=513&amp;name=image-png-Jun-21-2024-04-01-30-3403-PM.png 518w, https://blog.jitx.com/hs-fs/hubfs/image-png-Jun-21-2024-04-01-30-3403-PM.png?width=690&amp;height=684&amp;name=image-png-Jun-21-2024-04-01-30-3403-PM.png 690w, https://blog.jitx.com/hs-fs/hubfs/image-png-Jun-21-2024-04-01-30-3403-PM.png?width=863&amp;height=855&amp;name=image-png-Jun-21-2024-04-01-30-3403-PM.png 863w, https://blog.jitx.com/hs-fs/hubfs/image-png-Jun-21-2024-04-01-30-3403-PM.png?width=1035&amp;height=1026&amp;name=image-png-Jun-21-2024-04-01-30-3403-PM.png 1035w" sizes="(max-width: 345px) 100vw, 345px"><br></span></p>
<p><span>Overall we have some promising directions. Using LLMs for circuit board design looks a lot like using them for other complex tasks. They work well for pulling concrete data out of human-shaped data sources, they can do slightly more difficult tasks if they can solve that task by writing code, but eventually their capabilities break down in domains too far out of the training distribution.<p>We only tested the frontier models in this work, but I predict similar results from the open-source Llama or Mistral models. Some fine tuning on netlist creation would likely make the generation capabilities more useful.</p><p>Our work shows both the potential and limitations of current LLMs in specialized fields like circuit design. They're excellent at retrieving information and generating code, but struggle with the nuanced decision-making needed for complex designs. Future LLMs will likely improve here, but for the current model generation, they're best used as tools to assist human experts rather than as standalone designers.</p></span></p>
<p><span>Let me know what you think! You can find me on the JITX discord, and on <a href="https://x.com/DuncanHaldane" rel="noopener">Twitter</a>.</span></p></span>
      </p>
      
        
      
    </article>
  </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[OpenAI Acquires Rockset (138 pts)]]></title>
            <link>https://openai.com/index/openai-acquires-rockset/</link>
            <guid>40750391</guid>
            <pubDate>Fri, 21 Jun 2024 15:04:52 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://openai.com/index/openai-acquires-rockset/">https://openai.com/index/openai-acquires-rockset/</a>, See on <a href="https://news.ycombinator.com/item?id=40750391">Hacker News</a></p>
Couldn't get https://openai.com/index/openai-acquires-rockset/: Error: Request failed with status code 403]]></description>
        </item>
        <item>
            <title><![CDATA[Innovation heroes are a sign of a dysfunctional organization (186 pts)]]></title>
            <link>https://steveblank.substack.com/p/why-innovation-heroes-are-a-sign</link>
            <guid>40749754</guid>
            <pubDate>Fri, 21 Jun 2024 14:03:14 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://steveblank.substack.com/p/why-innovation-heroes-are-a-sign">https://steveblank.substack.com/p/why-innovation-heroes-are-a-sign</a>, See on <a href="https://news.ycombinator.com/item?id=40749754">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><article><div dir="auto"><p>A week ago I got invited to an “innovation hero” award ceremony at a government agency. I don’t know how many of these I’ve been to in the last couple years, but this one just made my head explode.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffdbd131c-ff57-4be3-8b28-f714b863e28c_1280x1658.heic" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffdbd131c-ff57-4be3-8b28-f714b863e28c_1280x1658.heic 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffdbd131c-ff57-4be3-8b28-f714b863e28c_1280x1658.heic 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffdbd131c-ff57-4be3-8b28-f714b863e28c_1280x1658.heic 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffdbd131c-ff57-4be3-8b28-f714b863e28c_1280x1658.heic 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffdbd131c-ff57-4be3-8b28-f714b863e28c_1280x1658.heic" width="398" height="515.534375" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/fdbd131c-ff57-4be3-8b28-f714b863e28c_1280x1658.heic&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:1658,&quot;width&quot;:1280,&quot;resizeWidth&quot;:398,&quot;bytes&quot;:317909,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/heic&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:false,&quot;topImage&quot;:true,&quot;internalRedirect&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffdbd131c-ff57-4be3-8b28-f714b863e28c_1280x1658.heic 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffdbd131c-ff57-4be3-8b28-f714b863e28c_1280x1658.heic 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffdbd131c-ff57-4be3-8b28-f714b863e28c_1280x1658.heic 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffdbd131c-ff57-4be3-8b28-f714b863e28c_1280x1658.heic 1456w" sizes="100vw" fetchpriority="high"></picture></div></a></figure></div><p>The award was for an entrepreneur who worked against all odds to buck the system to turn her insight into an application. She had realized it was possible to automate a process that was being done manually – reentering data from one spreadsheet to another and annotating it with additional data from another system. Inspired by her own work problem, she talked to her peers and other stakeholders, built multiple minimum viable products, and figured out how to get engineering, policy, legal, security and everyone else in the enterprise to actually approve it. And then she fought with the acquisition folks to buy the trivial amount of additional hardware needed to connect it. It was a development process that would’ve taken three weeks in a startup, but inside this agency took 10 months (which was considered fast.) At each step she was confronted with “we’re not budgeted for this” or “this isn’t on our schedule” and “this isn’t your job.” Most rational people would’ve given up and said “you can’t fight the system“ but yet she persisted.</p><p>Having seen this scenario play out multiple times at multiple large corporations and government agencies, I could’ve repeated the speech her agency director made at the ceremony verbatim. “Blah blah blah and a $100 bonus.” Everyone politely applauded and went back to work feeling good. I was simply depressed. Never once did anyone ever step back and say that what we just witnessed was leadership rewarding and perpetuating a dysfunctional and broken system.</p><p>I’m constantly puzzled why thoughtful and astute CEOs and Agency Directors never ask, “Why is it that innovations require heroics to occur in our organization? Why don’t we have a repeatable process for innovation? What are the obstacles in the way of delivering needed innovation with speed and urgency in our organization? Why is it that after each one of these awards we don’t go back and fix the parts of the system that made creating something new so difficult?”</p><p>Instead, everyone at this award ceremony just went back to work like it was business as usual. I realized that innovation in this organization was going to continue to happen by heroics and exception rather than by design. As I’ve seen play out way too many times, ultimately the innovators get tired of banging their heads against the wall and leave government service or large companies. Their organizations hemorrhage the very people they need to help them compete against aggressive adversaries or competitors who have them in their sights.</p><p data-attrs="{&quot;url&quot;:&quot;https://steveblank.substack.com/p/why-innovation-heroes-are-a-sign?utm_source=substack&amp;utm_medium=email&amp;utm_content=share&amp;action=share&quot;,&quot;text&quot;:&quot;Share&quot;,&quot;action&quot;:null,&quot;class&quot;:null}" data-component-name="ButtonCreateButton"><a href="https://steveblank.substack.com/p/why-innovation-heroes-are-a-sign?utm_source=substack&amp;utm_medium=email&amp;utm_content=share&amp;action=share" rel=""><span>Share</span></a></p><p><strong><span>An Organizational Design Problem</span><br></strong><span>Sadly, this wasn’t a single act of bad management or malice. No single individual thought they weren’t doing their job. However, if anyone had taken the time to deconstruct the reason for the roadblocks to innovation, they would have uncovered they weren’t just obstinate middle managers, or a single bad process. Asking a series of “</span><a href="https://en.wikipedia.org/wiki/Five_whys" rel="">five whys</a><span>,” (see </span><a href="https://hbr.org/2010/04/the-five-whys-for-startups" rel="">this HBR article</a><span>) would have discovered that:</span></p><ol><li><p>The agency’s existing processes were not designed for non-standard work. As in most large organizations, they were designed for the repeatable execution of pre-defined tasks.</p></li><li><p>There were no resources available for non-standard work or any parallel organization responsible for innovation.</p></li><li><p>The culture of the organization discouraged experimentation and punished the inevitable failures of a learning and discovery process.</p></li></ol><p><span>Ultimately, the root cause was the entire government agency lacked an Innovation Doctrine. This manifested itself as an organizational design problem. There was simply no permanent place in the organization for unscheduled innovation to happen. And even if there had been, there was </span><em>no way to turn demos into deployment with speed and at scale</em><span>.</span></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F55655110-1375-4b9a-80ec-34aed9bd5bd9_714x295.heic" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F55655110-1375-4b9a-80ec-34aed9bd5bd9_714x295.heic 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F55655110-1375-4b9a-80ec-34aed9bd5bd9_714x295.heic 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F55655110-1375-4b9a-80ec-34aed9bd5bd9_714x295.heic 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F55655110-1375-4b9a-80ec-34aed9bd5bd9_714x295.heic 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F55655110-1375-4b9a-80ec-34aed9bd5bd9_714x295.heic" width="714" height="295" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/55655110-1375-4b9a-80ec-34aed9bd5bd9_714x295.heic&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:295,&quot;width&quot;:714,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:33316,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/heic&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F55655110-1375-4b9a-80ec-34aed9bd5bd9_714x295.heic 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F55655110-1375-4b9a-80ec-34aed9bd5bd9_714x295.heic 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F55655110-1375-4b9a-80ec-34aed9bd5bd9_714x295.heic 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F55655110-1375-4b9a-80ec-34aed9bd5bd9_714x295.heic 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p><strong><span>Innovation Doctrine</span><br></strong><span>In peacetime and/or when you’re the dominant superpower (or a commercial market leader), the emphasis is on process, procedures, and sustainment of existing systems. Deviations from that create chaos and diverge from the predetermined are not welcomed, let alone promoted, and funded. They are eliminated. This works great when the external environment -competitors, adversaries, technologies, threats – is static. However in times of crisis, war or disruption, these unconventional thinkers and innovators are exactly what is needed, and their ideas need to be rapidly deployed.</span></p><p><span>Well-managed organizations realize that they need both innovation and execution. With execution being dominant in peacetime/competitive advantage you have </span><em>managers of process</em><span>. In crisis/wartime innovation is dominant. Instead of mangers of process you need </span><em>innovation leaders</em><span> who shepherd ideas through an </span><a href="https://steveblank.com/2017/09/14/how-companies-strangle-innovation/" rel="">innovation pipeline</a><span>. (see </span><a href="https://steveblank.com/2017/09/14/how-companies-strangle-innovation/" rel="">this HBR article</a><span>.) Successful organizations recognize that innovation isn’t a single activity (incubators, accelerators, hackathons); it is a strategically organized end-to-end&nbsp;process&nbsp;from idea to deployment.</span></p><p><span>While innovation and execution have different processes, people, and culture, they need to respect and depend on each other. This ambidexterity (see </span><a href="https://hbr.org/2004/04/the-ambidextrous-organization" rel="">this HBR article</a><span>) and the innovation processes that go with it require an innovation doctrine – an overall strategy and playbook for the entire organization and enterprise that includes an innovation pipeline and processes intended to drive innovation efforts, and describes the role of innovation leaders in an ambidextrous organization – all focused on rapid deployment of new capabilities.</span></p><p><strong>Lessons Learned</strong></p><ul><li><p>Innovation heroics are a symptom of a lack of an innovation doctrine</p></li><li><p>An innovation doctrine has a playbook, and innovation pipeline and describes the role of innovation leaders in an ambidextrous organization – all focused on rapid deployment of new capabilities</p></li><li><p>All large organizations – both government and corporate—need an innovation doctrine or else risk being outpaced by competitors.</p></li></ul></div></article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Spending 3 months investigating a 7-year old bug and fixing it in 1 line of code (102 pts)]]></title>
            <link>https://lemmy.world/post/16763534</link>
            <guid>40749624</guid>
            <pubDate>Fri, 21 Jun 2024 13:49:55 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://lemmy.world/post/16763534">https://lemmy.world/post/16763534</a>, See on <a href="https://news.ycombinator.com/item?id=40749624">Hacker News</a></p>
<div id="readability-page-1" class="page"><article id="postContent"><div><p dir="auto">I originally told the story over on the other site, but I thought I’d share it here. With a bonus!</p>
<p dir="auto">I was working on a hardware accessory for the OG iPad. The accessory connected to the iPad over USB and provided MIDI in/out and audio in/out appropriate for a musician trying to lay down some tracks in Garage Band.</p>
<p dir="auto">It was a winner of a product because at its core, it was based on a USB product we had already been making for PCs for almost a decade. All we needed was a little microcontroller to put the iPad into USB host mode (this was in the 30-pin connector days), and then allow it to connect to what was basically a finished product.</p>
<p dir="auto">This product was so old in fact that nobody knew how to compile the source code. When it came time to get it working, someone had to edit the binaries to change the USB descriptors to reflect the new product name and that it drew &lt;10mA from the iPad’s USB port (the original device was port-powered, but the iPad would get angry if you requested more than 10mA even if you were self-powered). This was especially silly because the original product had a 4-character name, but the new product had a 7-character name. We couldn’t make room for the extra bytes, so we had to truncate the name to fit it into the binary without breaking anything.</p>
<p dir="auto">Anyway, product ships and we notice a problem. Every once in a while, a MIDI message is missed. For those of you not familiar, MIDI is used to transmit musical notes that can be later turned into audio by whatever processor/voice you want. A typical message contains the note (A, B, F-sharp, etc), a velocity (how hard you hit the key), and whether it’s a key on or key off. So pressing and releasing a piano key generate two separate messages.</p>
<p dir="auto">Missing the occasional note message wouldn’t typically be a big deal except for instrument voices with infinite sustain like a pipe organ. If you had the pipe organ voice selected when using our device, it’s possible that it would receive a key on, but not a key off. This would result in the iPad assuming that you were holding the key down indefinitely.</p>
<p dir="auto">There isn’t an official spec for what to do if you receive another key-on of the same note without a key-off in between, but Apple handled this in the worst way possible. The iPad would only consider the key released if the number of key-ons and key-offs matched. So the only way to release this pipe organ key was to hope for it to skip a subsequent key-on message for the same key and then finally receive the key-off. The odds of this happening are approximately 0%, so most users had to resort to force quitting the app.</p>
<p dir="auto">Rumors flooded the customer message boards about what could cause this behavior, maybe it was the new iOS update? Maybe you had to close all your other apps? There was a ton of hairbrained theories floating around, but nobody had any definitive explanation.</p>
<p dir="auto">Well I was new to the company and fresh out of college, so I was tasked with figuring this one out.</p>
<p dir="auto">First step was finding a way to generate the bug. I wrote a python script that would hammer scales into our product and just listened for a key to get stuck. I can still recall the cacophony of what amounted to an elephant on cocaine slamming on a keyboard for hours on end.</p>
<p dir="auto">Eventually, I could reproduce the bug about every 10 minutes. One thing I noticed is that it only happened if multiple keys were pressed simultaneously. Pressing one key at a time would never produce the issue.</p>
<p dir="auto">Using a fancy cable that is only available to Apple hardware developers, I was able to interrogate the USB traffic going between our product and the iPad. After a loooot of hunting (the USB debugger could only sample a small portion, so I had to hit the trigger right when I heard the stuck note), I was able to show that the offending note-off event was never making it to the iPad. So Apple was not to blame; our firmware was randomly not passing MIDI messages along.</p>
<p dir="auto">Next step was getting the source to compile. I don’t remember a lot of the details, but it depended on “hex3bin” which I assume was some neckbeard’s version of hex2bin that was “better” for some reasons. I also ended up needing to find a Perl script that was buried deep in some university website. I assume that these tools were widely available when the firmware was written 7 years prior, but they took some digging. I still don’t know anything about Perl, but I got it to run.</p>
<p dir="auto">With firmware compiling, I was able to insert instructions to blink certain LEDs (the device had a few debug LEDs inside that weren’t visible to the user) at certain points in the firmware. There was no live debugger available for the simple 8-bit processor on this thing, so that’s all I had.</p>
<p dir="auto">What it came down to was a timing issue. The processor needed to handle audio traffic as well as MIDI traffic. It would pause whatever it was doing while handling the audio packets.
The MIDI traffic was buffered, so if a key-on or key-off came in while the audio was being handled, it would be addressed immediately after the audio was done.</p>
<p dir="auto">But it was only single buffered. So if a second MIDI message came in while audio was being handled, the second note would overwrite the first, and that first note would be forever lost. There is a limit to how fast MIDI notes can come in over USB, and it was just barely faster than it took to process the audio. So if the first note came in just after the processor cut to handling audio, the next note could potentially come in just before the processor cut back.</p>
<p dir="auto">Now for the solution. Knowing very little about USB audio processing, but having cut my teeth in college on 8-bit 8051 processors, I knew what kind of functions tended to be slow. I did a Ctrl+F for “%” and found a 16-bit modulo right in the audio processing code.</p>
<p dir="auto">This 16-bit modulo was just a final check that the correct number of bytes or bits were being sent (expecting remainder zero), so the denominator was going to be the same every time. The way it was written, the compiler assumed that the denominator could be different every time, so in the background it included an entire function for handling 16-bit modulos on an 8-bit processor.</p>
<p dir="auto">I googled “optimize modulo,” and quickly learned that given a fixed denominator, any 16-bit modulo can be rewritten as three 8-bit modulos.</p>
<p dir="auto">I tried implementing this single-line change, and the audio processor quickly dropped from 90us per packet to like 20us per packet. This 100% fixed the bug.</p>
<p dir="auto">Unfortunately, there was no way to field-upgrade the firmware, so that was still a headache for customer service.</p>
<p dir="auto">As to why this bug never showed up in the preceding 7 years that the USB version of the product was being sold, it was likely because most users only used the device as an audio recorder or MIDI recorder. With only MIDI enabled, no audio is processed, and the bug wouldn’t happen. The iPad however enabled every feature all the time. So the bug was always there. It’s just that nobody noticed it. Edit: also, many MIDI apps don’t do what Apple does and require matching key on/key off events. So if a key gets stuck, pressing it again will unstick it.</p>
<p dir="auto">So three months of listening to Satan banging his fists on a pipe organ lead to a single line change to fix a seven year old bug.</p>
<p dir="auto">TL;DR: 16-bit modulo on an 8-bit processor is slow and caused packets to get dropped.</p>
<p dir="auto">The bonus is at 4:40 in this video <a href="https://youtu.be/DBfojDxpZLY?si=oCUlFY0YrruiUeQq" rel="noopener nofollow">https://youtu.be/DBfojDxpZLY?si=oCUlFY0YrruiUeQq</a></p>
</div></article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Allan McDonald Refused To Approve Challenger Launch, Exposed Cover-Up (2021) (437 pts)]]></title>
            <link>https://www.npr.org/2021/03/07/974534021/remembering-allan-mcdonald-he-refused-to-approve-challenger-launch-exposed-cover</link>
            <guid>40748371</guid>
            <pubDate>Fri, 21 Jun 2024 11:23:48 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.npr.org/2021/03/07/974534021/remembering-allan-mcdonald-he-refused-to-approve-challenger-launch-exposed-cover">https://www.npr.org/2021/03/07/974534021/remembering-allan-mcdonald-he-refused-to-approve-challenger-launch-exposed-cover</a>, See on <a href="https://news.ycombinator.com/item?id=40748371">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="storytext">
      <div id="res974534902">
            <div data-crop-type="">
        <picture>
            <source srcset="https://media.npr.org/assets/img/2021/03/07/mcdonald-with-commemorative-2aa6ac2dbf80eea3029d8d1d7e7d7c60230627d0.jpg?s=1100&amp;c=15&amp;f=jpeg" data-original="https://media.npr.org/assets/img/2021/03/07/mcdonald-with-commemorative-2aa6ac2dbf80eea3029d8d1d7e7d7c60230627d0.jpg?s=1100&amp;c=100&amp;f=jpeg" data-template="https://media.npr.org/assets/img/2021/03/07/mcdonald-with-commemorative-2aa6ac2dbf80eea3029d8d1d7e7d7c60230627d0.jpg?s={width}&amp;c={quality}&amp;f={format}" data-format="webp" type="image/webp">
            <source srcset="https://media.npr.org/assets/img/2021/03/07/mcdonald-with-commemorative-2aa6ac2dbf80eea3029d8d1d7e7d7c60230627d0.jpg?s=1100&amp;c=15&amp;f=jpeg" data-original="https://media.npr.org/assets/img/2021/03/07/mcdonald-with-commemorative-2aa6ac2dbf80eea3029d8d1d7e7d7c60230627d0.jpg?s=1100&amp;c=100&amp;f=jpeg" data-template="https://media.npr.org/assets/img/2021/03/07/mcdonald-with-commemorative-2aa6ac2dbf80eea3029d8d1d7e7d7c60230627d0.jpg?s={width}&amp;c={quality}&amp;f={format}" data-format="jpeg" type="image/jpeg">
            <img src="https://media.npr.org/assets/img/2021/03/07/mcdonald-with-commemorative-2aa6ac2dbf80eea3029d8d1d7e7d7c60230627d0.jpg?s={width}&amp;c={quality}&amp;f={format}" data-original="https://media.npr.org/assets/img/2021/03/07/mcdonald-with-commemorative-2aa6ac2dbf80eea3029d8d1d7e7d7c60230627d0.jpg?s=1100&amp;c=100&amp;f=jpeg" data-template="https://media.npr.org/assets/img/2021/03/07/mcdonald-with-commemorative-2aa6ac2dbf80eea3029d8d1d7e7d7c60230627d0.jpg?s={width}&amp;c={quality}&amp;f={format}" data-format="jpeg" alt="" loading="lazy">
        </picture>
        
</div>
<div>
    <div>
        <p>
                Allan McDonald in 2016 holds a commemorative poster honoring the seven astronauts killed aboard the space shuttle Challenger.
                <b aria-label="Image credit">
                    
                    Howard Berkes/NPR
                    
                </b>
                <b><b>hide caption</b></b>
            </p>


            <p><b><b>toggle caption</b></b>
    </p></div>

    <p><span aria-label="Image credit">
        
        Howard Berkes/NPR
        
    </span>
</p></div>
<div>
        <picture>
            <source data-original="https://media.npr.org/assets/img/2021/03/07/mcdonald-with-commemorative-2aa6ac2dbf80eea3029d8d1d7e7d7c60230627d0.webp?s=2600&amp;c=100&amp;f=webp" type="image/webp">
            <source data-original="https://media.npr.org/assets/img/2021/03/07/mcdonald-with-commemorative-2aa6ac2dbf80eea3029d8d1d7e7d7c60230627d0.jpg?s=2600&amp;c=100&amp;f=jpeg" type="image/jpeg">
            <img data-original="https://media.npr.org/assets/img/2021/03/07/mcdonald-with-commemorative-2aa6ac2dbf80eea3029d8d1d7e7d7c60230627d0.jpg?s=2600&amp;c=100&amp;f=jpeg" alt="" src="https://media.npr.org/assets/img/2021/03/07/mcdonald-with-commemorative-2aa6ac2dbf80eea3029d8d1d7e7d7c60230627d0.jpg?s=2600&amp;c=100&amp;f=jpeg">
        </picture>
    </div>
<div>
        <p>Allan McDonald in 2016 holds a commemorative poster honoring the seven astronauts killed aboard the space shuttle Challenger.</p>
        <p><span aria-label="Image credit">
            
            Howard Berkes/NPR
            
        </span>
    </p></div>
   </div>
   <p>On Jan. 27, 1986, Allan McDonald stood on the cusp of history.</p>   <p>McDonald directed the booster rocket project at NASA contractor Morton Thiokol. He was responsible for the two massive rockets, filled with explosive fuel, that lifted space shuttles skyward. He was at the Kennedy Space Center in Florida for the launch of the Challenger "to approve or disapprove a launch if something came up," he told me in 2016, 30 years after Challenger exploded.</p>   <p>His job was to sign and submit an official form. Sign the form, he believed, and he'd risk the lives of the seven astronauts<strong> </strong>set to board the spacecraft the next morning. Refuse to sign, and he'd risk his job, his career and the good life he'd built for his wife and four children.</p>   
   <p>"And I made the smartest decision I ever made in my lifetime," McDonald told me. "I refused to sign it. I just thought we were taking risks we shouldn't be taking."</p>   
   
<!-- END ID="RES974537606" CLASS="BUCKETWRAP INTERNALLINK INSETTWOCOLUMN INSET2COL " -->
   <p>McDonald persistently cited three reasons for a delay: freezing overnight temperatures that could compromise the booster rocket joints; ice forming on the launchpad and spacecraft that could damage the orbiter heat tiles at launch; and a forecast of rough seas at the booster rocket recovery site.</p>   <p>He also told NASA officials, "If anything happens to this launch, I wouldn't want to be the person that has to stand in front of a board of inquiry to explain why we launched."</p>   <p>Now, 35 years after the Challenger disaster, McDonald's family reports that he died Saturday in Ogden, Utah, after suffering a fall and brain damage. He was 83 years old.</p>   <p>"There are two ways in which [McDonald's] actions were heroic," recalls Mark Maier, who directs a leadership program at Chapman University and produced a documentary about the Challenger launch decision.</p>   <p>"One was on the night before the launch, refusing to sign off on the launch authorization and continuing to argue against it," Maier says. "And then afterwards in the aftermath, exposing the cover-up that NASA was engaged in."</p>   
   <p>Twelve days after Challenger exploded, McDonald stood up in a closed hearing of a presidential commission investigating the tragedy. He was "in the cheap seats in the back" when he raised his hand and spoke. He had just heard a NASA official completely gloss over a fundamental fact.</p>   <p>McDonald and his team of <a href="https://www.npr.org/2006/01/28/5175151/challenger-reporting-a-disasters-cold-hard-facts">Thiokol engineers had strenuously opposed the launch</a>, arguing that freezing overnight temperatures, as low as 18 degrees F, meant that the O-rings at the booster rocket joints would likely stiffen and fail to contain the explosive fuel burning inside the rockets. They presented data showing that O-rings had lost elasticity at a much warmer temperature, 53 degrees F, during an earlier launch.</p>   
   
<!-- END ID="RES974537681" CLASS="BUCKETWRAP INTERNALLINK INSETTWOCOLUMN INSET2COL " -->
   <p>The NASA official simply said that Thiokol had some concerns but approved the launch. He neglected to say that the approval came only after Thiokol executives, under intense pressure from NASA officials, overruled the engineers.</p>   <p>"I was sitting there thinking that's about as deceiving as anything I ever heard," McDonald recalled. "So ... I said I think this presidential commission should know that Morton Thiokol was so concerned, we recommended not launching below 53 degrees Fahrenheit. And we put that in writing and sent that to NASA."</p>   <p>Former Secretary of State William Rogers chaired the commission and stared into the auditorium, squinting in the direction of the voice.</p>   <p>"I'll never forget Chairman Rogers said, 'Would you please come down here on the floor and repeat what I think I heard?' " McDonald said.</p>   <p>The focus of the commission's investigation shifted to the booster rocket O-rings, the efforts of McDonald and his colleagues to stop the launch and the failure of NASA officials to listen.</p>   
   
<!-- END ID="RES974537739" CLASS="BUCKETWRAP INTERNALLINK INSETTWOCOLUMN INSET2COL " -->
   <p>Morton Thiokol executives were not happy that McDonald spoke up, and they demoted him.</p>   <p>That alarmed members of the presidential commission and members of Congress. Rep. Edward Markey, a Massachusetts Democrat, introduced a <a href="https://www.congress.gov/bill/99th-congress/house-joint-resolution/634/text?r=85&amp;s=1">joint resolution in the House</a> that threatened to forbid Thiokol from getting future NASA contracts given the company's punishment of McDonald and any other Thiokol engineers who spoke freely.</p>   <p>The company relented, and McDonald was promoted to vice president and put in charge of the effort to redesign the booster rocket joints that failed during the Challenger launch.</p>   
   <p>In 1988, the redesigned joints worked successfully as shuttle flights resumed.</p>   <p>McDonald continued to work at Thiokol until 2001 and retired after 42 years. He later co-authored one of the most definitive accounts of the Challenger disaster — <a href="https://upf.com/book.asp?id=MCDON001"><em>Truth, Lies, and O-Rings</em></a><em>: Inside the Space Shuttle </em>Challenger <em>Disaster</em>.</p>   <div id="res974548572">
            <div data-crop-type="">
        <picture>
            <source srcset="https://media.npr.org/assets/img/2021/03/07/mcdonald-at-us-space-command-281b10bcc5e423414b3d65972b8727e0998e0dd9.jpg?s=1100&amp;c=15&amp;f=jpeg" data-original="https://media.npr.org/assets/img/2021/03/07/mcdonald-at-us-space-command-281b10bcc5e423414b3d65972b8727e0998e0dd9.jpg?s=1100&amp;c=100&amp;f=jpeg" data-template="https://media.npr.org/assets/img/2021/03/07/mcdonald-at-us-space-command-281b10bcc5e423414b3d65972b8727e0998e0dd9.jpg?s={width}&amp;c={quality}&amp;f={format}" data-format="webp" type="image/webp">
            <source srcset="https://media.npr.org/assets/img/2021/03/07/mcdonald-at-us-space-command-281b10bcc5e423414b3d65972b8727e0998e0dd9.jpg?s=1100&amp;c=15&amp;f=jpeg" data-original="https://media.npr.org/assets/img/2021/03/07/mcdonald-at-us-space-command-281b10bcc5e423414b3d65972b8727e0998e0dd9.jpg?s=1100&amp;c=100&amp;f=jpeg" data-template="https://media.npr.org/assets/img/2021/03/07/mcdonald-at-us-space-command-281b10bcc5e423414b3d65972b8727e0998e0dd9.jpg?s={width}&amp;c={quality}&amp;f={format}" data-format="jpeg" type="image/jpeg">
            <img src="https://media.npr.org/assets/img/2021/03/07/mcdonald-at-us-space-command-281b10bcc5e423414b3d65972b8727e0998e0dd9.jpg?s={width}&amp;c={quality}&amp;f={format}" data-original="https://media.npr.org/assets/img/2021/03/07/mcdonald-at-us-space-command-281b10bcc5e423414b3d65972b8727e0998e0dd9.jpg?s=1100&amp;c=100&amp;f=jpeg" data-template="https://media.npr.org/assets/img/2021/03/07/mcdonald-at-us-space-command-281b10bcc5e423414b3d65972b8727e0998e0dd9.jpg?s={width}&amp;c={quality}&amp;f={format}" data-format="jpeg" alt="" loading="lazy">
        </picture>
        
</div>
<div>
    <div>
        <p>
                In 2018, Allan McDonald reviewed the Challenger launch decision during a series of seminars about leadership and ethical decision-making to managers at U.S. Space Command.
                <b aria-label="Image credit">
                    
                    Mark Maier
                    
                </b>
                <b><b>hide caption</b></b>
            </p>


            <p><b><b>toggle caption</b></b>
    </p></div>

    <p><span aria-label="Image credit">
        
        Mark Maier
        
    </span>
</p></div>
<div>
        <picture>
            <source data-original="https://media.npr.org/assets/img/2021/03/07/mcdonald-at-us-space-command-281b10bcc5e423414b3d65972b8727e0998e0dd9.webp?s=2600&amp;c=100&amp;f=webp" type="image/webp">
            <source data-original="https://media.npr.org/assets/img/2021/03/07/mcdonald-at-us-space-command-281b10bcc5e423414b3d65972b8727e0998e0dd9.jpg?s=2600&amp;c=100&amp;f=jpeg" type="image/jpeg">
            <img data-original="https://media.npr.org/assets/img/2021/03/07/mcdonald-at-us-space-command-281b10bcc5e423414b3d65972b8727e0998e0dd9.jpg?s=2600&amp;c=100&amp;f=jpeg" alt="" src="https://media.npr.org/assets/img/2021/03/07/mcdonald-at-us-space-command-281b10bcc5e423414b3d65972b8727e0998e0dd9.jpg?s=2600&amp;c=100&amp;f=jpeg">
        </picture>
    </div>
<div>
        <p>In 2018, Allan McDonald reviewed the Challenger launch decision during a series of seminars about leadership and ethical decision-making to managers at U.S. Space Command.</p>
        <p><span aria-label="Image credit">
            
            Mark Maier
            
        </span>
    </p></div>
   </div>
   <p>In retirement, McDonald became a fierce advocate of ethical decision-making and spoke to hundreds of engineering students, engineers and managers. He and Chapman University's Maier held leadership and ethics seminars for corporations and government agencies, including U.S. Space Command.</p>   <p>Maier says that one of McDonald's key moments in his talks helps explain his ability to reconcile his brush with history.</p>   <p>"What we should remember about Al McDonald [is] he would often stress his laws of the seven R's," Maier says. "It was always, always do the right thing for the right reason at the right time with the right people. [And] you will have no regrets for the rest of your life."</p>   <p>"It's really that simple if you just keep it focused that way," McDonald told me in 2016.</p>   
   
<!-- END ID="RES974537778" CLASS="BUCKETWRAP INTERNALLINK INSETTWOCOLUMN INSET2COL " -->
   <p>He also framed regret another way, paraphrasing a favorite quote from the late journalist Sydney J. Harris.</p>   <p>"Regret for things we did is tempered by time," McDonald said, his expression firm. "But regret for things we did not do is inconsolable." McDonald then paused and added, "That's absolutely true."</p>   <p>He seemed inconsolable immediately after the Challenger explosion in tearful calls home. He recalls the painful conversations in his book. "I feel like it's my fault," he told his daughter Lisa, a nursing student in Boston at the time. "Don't blame yourself, Dad," she said, also crying.</p>   <p>Maier believes McDonald lived out his life with neither blame nor regret. "He died with serenity and equanimity," he says. "I will miss him dearly."</p>   <p>Allan McDonald leaves behind his wife, Linda, and four children — and a legacy of doing the right things at the right times with the right people.</p>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Ladybird browser spreads its wings (390 pts)]]></title>
            <link>https://lwn.net/Articles/976822/</link>
            <guid>40746804</guid>
            <pubDate>Fri, 21 Jun 2024 06:35:37 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://lwn.net/Articles/976822/">https://lwn.net/Articles/976822/</a>, See on <a href="https://news.ycombinator.com/item?id=40746804">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
<center>
           <div><b>Did you know...?</b><p>LWN.net is a subscriber-supported publication; we rely on subscribers
       to keep the entire operation going.  Please help out by <a href="https://lwn.net/subscribe/">buying a subscription</a> and keeping LWN on the
       net.</p></div>
           </center>
           
<p><a href="https://ladybird.dev/">Ladybird</a> is an open-source
project aimed at building an independent web browser, rather than
yet another browser based on Chrome. It is written in C++ and licensed under a
two-clause BSD license. The effort 
began as part of the <a href="https://www.serenityos.org/">SerenityOS</a> project, but
developer Andreas Kling <a href="https://awesomekling.substack.com/p/forking-ladybird-and-stepping-down-serenityos">announced</a>
on June&nbsp;3 that he was "forking" Ladybird as a separate project and stepping away from
SerenityOS to focus his attention on the browser completely. Ladybird
is not ready to replace Firefox or Chrome for regular use, but it is showing
great promise.</p>

<p>Kling started working on SerenityOS in 2018 as a therapy project
after completing a substance-abuse rehabilitation program. The SerenityOS name is a
nod to the <a href="https://en.wikipedia.org/wiki/Serenity_Prayer">serenity
prayer</a>. Prior to working on the project, he had worked on
WebKit-based browsers at Apple and Nokia. Eventually he <a href="https://awesomekling.github.io/I-quit-my-job-to-focus-on-SerenityOS-full-time/">made
SerenityOS his full-time job</a>, and funded the work through
donations, sales of SerenityOS merchandise, and income from
YouTube. (Kling posts monthly updates to his <a href="https://www.youtube.com/@awesomekling/videos">YouTube
channel</a> about Ladybird, as well as hacking videos where he walks
through working on various components in the browser, such as the <a href="https://www.youtube.com/watch?v=8mxubNQC5O8">JavaScript JIT
compiler</a>.)</p>

<h4>Taking flight</h4>

<p>Kling <a href="https://awesomekling.substack.com/p/ladybird-a-new-cross-platform-browser-project">announced</a>
the Ladybird project in September 2022. He said that the project
started while creating a Qt GUI for SerenityOS's LibWeb browser
engine. He decided to target Linux as well as SerenityOS so it would be easier for
people to work on and debug while in Linux. In the post announcing his
intent to work solely on Ladybird, he noted that he had been focusing
all of his attention on the Linux version of Ladybird. With that
realization, he decided to step down as "benevolent dictator for
life" (BDFL) of SerenityOS so its development would not be held back:</p>

<blockquote>
Before anyone asks, there is no drama behind this change. It's simply
recognizing that there have been two big projects packed uncomfortably
into a single space for too long, and I'm doing what I believe will
make life better for everyone involved.
</blockquote>

<p>Ladybird's governance is similar to SerenityOS. Kling is the
BDFL, with a group of maintainers (currently ten) who can
approve and merge pull requests. The <a href="https://github.com/LadybirdWebBrowser/ladybird/blob/master/CONTRIBUTING.md">contributing</a>
guide notes that maintainership is "<q>by invitation only and does not
correlate with any particular metric</q>". Project development
discussions are held on a <a href="https://discord.com/invite/nvfjVJ4Svh">Discord server</a>
(account required).</p>

<p>Now independent, Ladybird has dropped SerenityOS as a development
target, and has moved to <a href="https://github.com/LadybirdWebBrowser/ladybird">its own GitHub
repository</a>. In addition, Kling has relaxed his self-imposed policy
of excluding "not invented here" (NIH) code that had applied to SerenityOS, which
means that the Ladybird project will be able to make use of existing
libraries rather than writing from scratch.</p>

<p>Comparing the <a href="https://github.com/LadybirdWebBrowser/ladybird/blob/master/README.md">README</a> file in the standalone Ladybird repository
against the <a href="https://github.com/SerenityOS/serenity/blob/master/Ladybird/README.md">README</a>
file in the SerenityOS repository, the goal has
evolved from creating "<q>a standards-compliant, independent web browser with
no third-party dependencies</q>" to developing an independent browser "<q>using a
novel engine based on web standards</q>".</p>

<p>The changes to the section that enumerates the core libraries for Ladybird 
provide some hints about Kling's plans
to use existing libraries rather than continuing to reinvent the
wheel. The core support libraries for the project include homegrown
libraries for cryptography, TLS, 2D-graphics rendering, archive-file
format support, Unicode, as well as audio and video playback.
In the pre-fork documentation, they are described
as alternatives to other software. For example, Ladybird's TLS (LibTLS)
and cryptography (LibCrypto) libraries are "<q>Cryptography primitives
and Transport Layer Security (rather than OpenSSL)</q>". The "rather
than" language has been removed in the journey to the standalone
repository, and the LibSQL library from SerenityOS has <a href="https://github.com/LadybirdWebBrowser/ladybird/commit/8362c073f34f25ef2fbe800780f4b8790b3fffa8">already
been stripped out</a> in favor of sqlite3.</p>

<p>In a discussion in the project's Discord instance on June&nbsp;5, Kling indicated that font rendering
would likely be replaced with a third-party library. A user asked on
June&nbsp;6 what would determine whether a component would be developed in-house versus
using a third-party library. Kling responded that if it implements a web
standard, "i.e DOM, HTML, JavaScript, CSS, Wasm, etc. then we build it
in house." Otherwise, the project would look to alternatives "unless we
believe we can build something better ourselves".</p>

<h4>Status</h4>

<p>Ladybird is still in early development ("pre-alpha") today. It
currently runs on Linux, macOS, and other
UNIX-like operating systems. It's also possible to use on Windows with
Windows Subsystem for Linux (WSL) version&nbsp;2, but there appears to
be no effort to target Windows independently at this time. At the
moment, the project does not provide binaries for any
platform. Interested users will need to grab the source and follow the
<a href="https://github.com/LadybirdWebBrowser/ladybird/blob/master/Documentation/BuildInstructionsLadybird.md">build
instructions</a>. Users will need GCC&nbsp;13+ or Clang&nbsp;17, and Qt6
development packages to play along at home. Ladybird compiles and runs
on, for example, Fedora&nbsp;40 without a problem, but it is a long
way from being suitable for regular use.</p>

<blockquote>
<a href="https://lwn.net/Articles/977492/"><img src="https://static.lwn.net/images/2024/ladybird-sm.png" alt="[Ladybird browser with inspector]" title="Ladybird browser with inspector"></a>
</blockquote>

<p>One might expect that the browser
would be more usable with sites with simpler layouts and little to no JavaScript
(e.g. LWN) than those with complex layouts and a fair amount of
JavaScript (e.g. GitHub). However, this isn't always the case—Ladybird
rendered GitHub and many other sites well, if slowly. Browsing
LWN anonymously worked well, but logging into LWN,
however, consistently proved to be too much for the
application. Each time, it basically froze on the front page and clicking links to
articles did nothing.</p>

<p>Somewhat ironically, it was not possible to log
into Discord using Ladybird. It does a fair
job of rendering pages, but speed and stability are
still wanting. Each Ladybird tab has its own render process, which is
sandboxed as a security measure to prevent any malicious pages from
affecting the rest of the system. However, it doesn't seem to suffice
to keep a single page from crashing the browser entirely. That's to be
expected from a project that's still considered pre-alpha, though.</p>

<p>The current feature set is, not surprisingly, minimal. Ladybird has
a URL/search bar, reload, tabs, can zoom in/out on content, take
screenshots, and (of course) has backward and forward navigation. It
does not, however, have bookmarks, a history display, extensions,
password management, printing, or even the ability to save an
image. WebRTC does not seem to be supported yet. CSS support seems
relatively robust. Ladybird passes 100% of the CSS Selectors tests for
levels 1-3, for example, using <a href="https://css4-selectors.com/browser-selector-test/">this
test</a>. It gets a 53% score for level 4, while Firefox gets 71%, so
not a terrible showing at all. JavaScript support seems solid, but
slow: the <a href="https://www.w3schools.com/js/js_ex_browser.asp">examples
here</a> work, but they load slowly.</p>

<p>On the other hand, Ladybird does have tools for developers, such as
inspectors for the document object model (DOM) tree and accessibility
trees, as well as the ability to create dumps of various things:
the DOM tree and layout tree, computed styles, and so forth. It also
has the ability to spoof the User-Agent sent by the browser so that testers
can try to get around sites that refuse to work with "unknown"
browsers. However, toggling the User-Agent wasn't enough to get
past Google's gatekeeping to sign into Gmail—but it's unclear if
that meant Ladybird wasn't sending the string correctly or if Google
is using other means to fingerprint non-approved browsers.</p>

<p>Suffice it to say, Ladybird is not ready for mainstream use but it
<em>does</em> show potential. In the past month, the project has had
more than 880 commits from 49 authors. If the project maintains that
kind of momentum, or picks up steam, it could become a usable
alternative to mainstream browsers before too long.</p><br clear="all"><hr><p>
           (<a href="https://lwn.net/Login/?target=/Articles/976822/">Log in</a> to post comments)
           </p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[MeshAnything – converts 3D representations into efficient 3D meshes (239 pts)]]></title>
            <link>https://buaacyw.github.io/mesh-anything/</link>
            <guid>40746310</guid>
            <pubDate>Fri, 21 Jun 2024 05:02:25 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://buaacyw.github.io/mesh-anything/">https://buaacyw.github.io/mesh-anything/</a>, See on <a href="https://news.ycombinator.com/item?id=40746310">Hacker News</a></p>
<div id="readability-page-1" class="page">

<div>
          
            <div>
              <p>
              <span>
                <a href="https://tonghe90.github.io/">Tong He</a><sup>2</sup><span>†</span>,</span>
              <span>
                <a href="https://dihuang.me/">Di Huang</a><sup>2</sup>,</span>
              <span>
                <a href="https://ywcmaike.github.io/">Weicai Ye</a><sup>2</sup>,</span>
              <span>
                <a href="https://ch3cook-fdu.github.io/">Sijin Chen</a><sup>3</sup>,</span>
              <span>
                <a href="https://me.kiui.moe/">Jiaxiang Tang</a><sup>4</sup></span>,
              <br>
              <span>
                <a href="https://chenxin.tech/">Xin Chen</a><sup>5</sup>,</span>
              <span>
                <a href="https://caizhongang.github.io/">Zhongang Cai</a><sup>6</sup>,</span>
              <span>
                <a href="https://scholar.google.com.hk/citations?user=jZH2IPYAAAAJ&amp;hl=en">Lei Yang</a><sup>6</sup>,</span>
              <span>
                <a href="https://www.skicyyu.org/">Gang Yu</a><sup>7</sup>,</span>
              <span>
                <a href="https://guosheng.github.io/">Guosheng Lin</a><sup>1</sup><span>†</span>,</span>
              <span>
                <a href="https://icoz69.github.io/">Chi Zhang</a><sup>8</sup><span>†</span></span>

            </p></div>

            <div>
              <p><span><sup>1</sup>S-Lab, Nanyang Technological University,</span>
              <span><sup>2</sup>Shanghai AI Lab</span><br>
              <span><sup>3</sup>Fudan University,</span>
              <span><sup>4</sup>Peking University,</span>
              <span><sup>5</sup>University of Chinese Academy of Sciences</span><br>
              <span><sup>6</sup>SenseTime Research,</span>
              <span><sup>7</sup>Stepfun,</span>
              <span><sup>8</sup>Westlake University</span></p><p><span><sup>*</sup>Work done during a research internship at Shanghai AI Lab.</span>
              </p>
              <p><span><sup>†</sup>Corresponding Authors.</span>
              </p>
            </div>


          
        </div>


<div>
        <p><img src="https://buaacyw.github.io/mesh-anything/demo_video.gif" alt="Demo Video">
        </p>
      </div>

<div>
      <h2>
        <p><span>MeshAnything</span> mimics human artist in extracting meshes from any 3D representations. It can be combined with various 3D asset production pipelines, such as 3D reconstruction and generation, to convert their results into Artist-Created Meshes that can be seamlessly applied in 3D industry.
        </p>
      </h2>
    </div>



<div>
    <div>
      <h4>Comparison with Previous Methods</h4>
      
    </div>
    <div>
          <p><img src="https://buaacyw.github.io/mesh-anything/remesh.png" alt="Remesh Image"></p><p>
            <span>MeshAnything</span> generates meshes with hundreds of times fewer faces, significantly improving storage, rendering, and simulation efficiencies, while achieving precision comparable to previous methods.
          </p>
        </div>
  </div>

<div>
        <h2>Abstract</h2>
        <div>
          <p>
            Recently, 3D assets created via reconstruction and generation have matched the quality of manually crafted assets, highlighting their potential for replacement. However, this potential is largely unrealized because these assets always need to be converted to meshes for 3D industry applications, and the meshes produced by current mesh extraction methods are significantly inferior to Artist-Created Meshes (AMs), i.e., meshes created by human artists.
Specifically, current mesh extraction methods rely on dense faces and ignore geometric features, leading to inefficiencies, complicated post-processing, and lower representation quality.
          </p>
          <p>
            To address these issues, we introduce MeshAnything, a model that treats mesh extraction as a generation problem, producing AMs aligned with specified shapes. By converting 3D assets in any 3D representation into AMs, MeshAnything can be integrated with various 3D asset production methods, thereby enhancing their application across the 3D industry.
          </p>
          <p>
            The architecture of MeshAnything comprises a VQ-VAE and a shape-conditioned decoder-only transformer. We first learn a mesh vocabulary using the VQ-VAE, then train the shape-conditioned decoder-only transformer on this vocabulary for shape-conditioned autoregressive mesh generation. Our extensive experiments show that our method generates AMs with hundreds of times fewer faces, significantly improving storage, rendering, and simulation efficiencies, while achieving precision comparable to previous methods.
          </p>
        </div>
      </div>

<div>
    <div>
      <p>
        <h2>Method</h2>
      </p>
    </div>

    <div>
        <p><img src="https://buaacyw.github.io/mesh-anything/pipline.png" alt="Pipeline Image">
        </p>
      </div>

    <div>
        <p>
           <span>MeshAnything</span> is an autoregressive transformer capable of generating Artist-Created Meshes that adhere to given 3D shapes. We sample point clouds from given 3D assets, encode them into features, and inject them into the decoder-only transformer to achieve shape-conditional mesh generation.
        </p>
        <p>
          Compared to methods like MeshGPT that directly generate Artist-Created Meshes, our approach avoids learning complex 3D shape distributions. Instead, it focuses on efficiently constructing shapes through optimized topology, significantly reducing the training burden and enhancing scalability.
        </p>
      </div>
  </div>


<div>
    <div>
      <p>
        <h2>More Results</h2>
      </p>
    </div>

    <div>
        <p><img src="https://buaacyw.github.io/mesh-anything/teaser.png" alt="Teaser Image">
        </p>
        <p><img src="https://buaacyw.github.io/mesh-anything/add.png" alt="Additional Image">
        </p>
        <p><img src="https://buaacyw.github.io/mesh-anything/gt.png" alt="Ground Truth Image">
        </p>

        <p>
          By integrating with various 3D asset production methods, our approach achieves highly controllable Artist-Created Mesh generation. Besides, we compare our reseults with ground truth in (b) and (c). In (b), MeshAnything generates meshes with better topology and fewer faces than the ground truth. In (c), we produce meshes with a completely different topology while achieving a similar shape, proving that our method does not simply overfit but understands how to construct meshes using efficient topology.
        </p>
      </div>
  </div>



<div id="BibTeX">
    <h2>BibTeX</h2>
    <pre><code>@misc{chen2024meshanything,
      title={MeshAnything: Artist-Created Mesh Generation with Autoregressive Transformers},
      author={Yiwen Chen and Tong He and Di Huang and Weicai Ye and Sijin Chen and Jiaxiang Tang and Xin Chen and Zhongang Cai and Lei Yang and Gang Yu and Guosheng Lin and Chi Zhang},
      year={2024},
      eprint={2406.10163},
      archivePrefix={arXiv},
      primaryClass={id='cs.CV' full_name='Computer Vision and Pattern Recognition' is_active=True alt_name=None in_archive='cs' is_general=False description='Covers image processing, computer vision, pattern recognition, and scene understanding. Roughly includes material in ACM Subject Classes I.2.10, I.4, and I.5.'}
}</code></pre>
  </div>






</div>]]></description>
        </item>
        <item>
            <title><![CDATA[A Rant about Front-end Development (112 pts)]]></title>
            <link>https://blog.frankmtaylor.com/2024/06/20/a-rant-about-front-end-development/</link>
            <guid>40746173</guid>
            <pubDate>Fri, 21 Jun 2024 04:31:38 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://blog.frankmtaylor.com/2024/06/20/a-rant-about-front-end-development/">https://blog.frankmtaylor.com/2024/06/20/a-rant-about-front-end-development/</a>, See on <a href="https://news.ycombinator.com/item?id=40746173">Hacker News</a></p>
<div id="readability-page-1" class="page"><article id="post-3896" itemscope="itemscope" itemtype="http://schema.org/BlogPosting" itemprop="blogPost">
		
		<!-- .entry-header -->

		<div itemprop="articleBody">
			<p><span><span>Reading Time: </span> <span>13</span> <span>minutes</span></span></p><p>I am a front-end developer who is FED up about front-end development. If you write front-end, this isn’t about you <em>personally</em>. It’s about how your choices make me angry. Also this is about how my choices have made me angry. Also this is mostly just about <em>choices</em>, the technologies are incidental. </p>



<p><strong>Note: The views expressed in here are not those of my employer. They are (probably) the views of thousands of people just like me.</strong></p>



<p><strong>Warning: </strong>This article is filled with profanity. Ask your scrummaster if 8 points of profanity is too much to handle today. </p>



<div>
<figure><a href="https://blog.frankmtaylor.com/wp-content/uploads/2024/06/DALL%C2%B7E-2024-06-20-22.40.13-painting-of-a-raven-trying-to-remove-the-keys-from-a-laptop-keyboard.png"><img fetchpriority="high" decoding="async" width="1024" height="1024" src="https://blog.frankmtaylor.com/wp-content/uploads/2024/06/DALL%C2%B7E-2024-06-20-22.40.13-painting-of-a-raven-trying-to-remove-the-keys-from-a-laptop-keyboard.png" alt="Ravens are smart animals, probably much smarter than we really know. If they knew about the stupid shit we were doing to build the internet they'd definitely peck out the keys of every keyboard in existence." srcset="https://blog.frankmtaylor.com/wp-content/uploads/2024/06/DALL%C2%B7E-2024-06-20-22.40.13-painting-of-a-raven-trying-to-remove-the-keys-from-a-laptop-keyboard.png 1024w, https://blog.frankmtaylor.com/wp-content/uploads/2024/06/DALL%C2%B7E-2024-06-20-22.40.13-painting-of-a-raven-trying-to-remove-the-keys-from-a-laptop-keyboard-300x300.png 300w, https://blog.frankmtaylor.com/wp-content/uploads/2024/06/DALL%C2%B7E-2024-06-20-22.40.13-painting-of-a-raven-trying-to-remove-the-keys-from-a-laptop-keyboard-150x150.png 150w, https://blog.frankmtaylor.com/wp-content/uploads/2024/06/DALL%C2%B7E-2024-06-20-22.40.13-painting-of-a-raven-trying-to-remove-the-keys-from-a-laptop-keyboard-768x768.png 768w, https://blog.frankmtaylor.com/wp-content/uploads/2024/06/DALL%C2%B7E-2024-06-20-22.40.13-painting-of-a-raven-trying-to-remove-the-keys-from-a-laptop-keyboard-240x240.png 240w" sizes="(max-width: 1024px) 100vw, 1024px"></a><figcaption>Quoth the raven on your merge requests: Nevermore</figcaption></figure></div>


<h2>First, some background</h2>



<p>I have neither a computer science degree nor an engineering degree. But I have worked in web technologies since the second Bush administration. I’ve worked in a variety of roles ranging from business analyst, tester, all the way through developer, consultant, and principal architect.  </p>



<p>I did agency work for 11 years before making the choice to work for a very large tech company. I have worked across sectors like insurance, healthcare, retail, banking, investing, marketing, and manufacturing. I have worked with global brands which are household names. </p>



<p>I have written <em>a lot</em> of front-end code for <em>a lot</em> of companies. I have also dealt with <em>a lot of consequences created by front-end code</em>. My criticisms come from my role as a front-end developer and as someone affected by a front-end developer. </p>



<h2>No one gives a shit about content</h2>



<p>Unless it’s a content-specialized front-end developer who works specifically in the content management space and is specialized in writing front-end for content management system. Did you count how many times I had to write both “front-end” and “content”? Too fucking many. </p>



<p>I have mentored quite a few bootcamp graduates. There are exactly zero graduates of zero bootcamps who have ever been told, “users go to websites for the content.” <strong>Zero</strong>. They all think it’s their moral imperative to make sure every site is built in React.</p>



<p>I have worked with exactly zero computer science graduates who have ever heard the phrase, “content before code”.</p>



<p>Unless the front-end developer has been forced by threat of not-being-able-to-pay-rent to care about content, they don’t fucking care. </p>



<p>This is <strong>wild</strong> to me because HTML5 semantics exist and their whole-ass raison d’être is, in fact, having an understanding of content. </p>



<ul>
<li>You can’t write content-managed HTML if you don’t understand the content.</li>



<li>You can’t write semantic HTML if you don’t understand the content. </li>



<li>You can’t write SEO-ready HTML without understanding the content.</li>



<li>You can’t write accessible HTML without understanding the content.</li>



<li>You can’t write <em>good</em> HTML without understanding the content. </li>
</ul>



<p>Zero SPA developers that I’ve ever met have ever given a shit about using a<code> &lt;header&gt;</code> instead of a <code>&lt;section&gt;</code>. I have found <code>NaN</code> fucks given about using a <code>p</code> over a <code>div</code>. The number of times I’ve seen numbers written inside of a <code>&lt;li&gt;</code> that’s inside of a <code>&lt;ul&gt;</code> — instead of just using an <code>&lt;ol&gt;</code> — <strong>is deeply disturbing.</strong></p>



<p><em>Maybe we should ask the designer or business owner a few questions about what the design means to the end-user before hammering out 18 layers of <code>div</code>s. </em></p>



<h2>CSS is fine; you’re the problem</h2>



<p>If someone gets a ticket for speeding, we don’t blame the car or the road. We definitely blame the idiot who thought it’d be a good idea to race the Dodge Charger camped out in the Dollar General parking lot. </p>



<p>It’s in this very same vein that we don’t blame CSS:</p>



<blockquote>
<p>“The problem with CSS is that the scope is global”<br>“The problem with CSS is the cascade”<br>“The problem with CSS is that it fails silently”</p>
<cite>Every dork who promotes writing styles in JavaScript</cite></blockquote>



<p>Those aren’t problems; they’re <em>features</em>. They are solutions to the very specific problems of, “how do we make content presentable as easily as possible, with as little duplication as possible, and with as few negative impacts to the user as possible.”</p>



<p>Chances are, the things you don’t like about CSS are the things you haven’t bothered to understand about it. </p>



<p>Styles rarely, <em>if ever</em> need to actually be scoped and encapsulated. In fact, the only time you want to prevent styles from being in the global scope is when you didn’t write them <em>and</em> can’t control them. </p>



<p>If duplicated class names are ruining your day, 15-class selectors are crapping in your cereal, and 8 <code>!important</code> are igniting turds in your garage, that’s actually an <em>architecture problem</em>. </p>



<p>Are there <em>real</em> problems in CSS? Of course. But those are problems like, “<code>border-radius</code> should have been called <code>corner-radius</code>,” and “atomic stacking contexts are generated when using <code>opacity</code>,” and “<a href="https://blog.frankmtaylor.com/2012/01/25/css-font-size-an-em-isnt-an-m-but-an-ex-is-an-x/" data-type="link" data-id="https://blog.frankmtaylor.com/2012/01/25/css-font-size-an-em-isnt-an-m-but-an-ex-is-an-x/">an <code>em</code> isn’t really an <code>em</code>.</a>”   Those are problems. </p>



<p>The C of CSS is not a problem. You’re the problem. </p>



<p><em>Put on your big-kid pants and develop a strategy, pick some conventions, write them down, and enforce them through merge requests and peer reviews. </em> </p>



<h2>Server-side rendering does not mean, “rendered with JavaScript”</h2>



<p>There’s a disconcerting number of front-end developers out there who act like it wasn’t possible to generate HTML on a server prior to 2010<a href="#one"><sup>1</sup></a>.  They talk about SSR <em>only</em> in the context of node.js and seem to have no clue that people started working on this problem when season 5 of Seinfeld was on air<a href="#two"><sup>2</sup></a>. </p>



<p>Server-side rendering was not invented with Node. What Node brought to the table was the convenience of writing your shitty <code>div</code> soup in the very same language that was invented in 10 days for the sole purpose of pissing off Java devs everywhere. </p>



<p>Server-side rendering means it’s rendered on the fucking server. You can do that with PHP, ASP, JSP, Ruby, Python, Perl, CGI, and hell, <em>R</em>. You can server-side render a page in <em>Lua</em> if you want. </p>



<p>Just because <em>Node</em> isn’t involved in generating HTML, that doesn’t mean it’s not possible to generate markup at request time.  It is possible. It’s been possible. It will continue to be possible. </p>



<p><em>Stop denying the history, contributions, and value that other technologies bring to solving this exact problem that is definitely not unique to you.</em></p>







<h2>JavaScript is not always the answer</h2>



<p>My brothers and sisters in Christ I want you to know that I care about your souls enough to share these truths with you:</p>



<ul>
<li>You don’t need JavaScript to make a web page. </li>



<li>You don’t need JavaScript to write styles. </li>



<li>You don’t need JavaScript to make an animation. </li>



<li>You don’t need JavaScript just to show content. </li>
</ul>



<p>In 2010 I was a contractor for an organization working with some Very Smart IBM consultants. They could not figure out how our in-house developer made the navigation for the site they were implementing. </p>



<p>It was because our developer used CSS. Our consultants had never considered that CSS could do something as fancy as, “show you a thing when you hover over it.”</p>



<p>That’s a fun “har-har silly ignorant smart consultants,” story except at least it’s about their <em>harmless</em> <em>ignorance</em>.</p>



<p>It’s so much <strong>worse</strong> when they <em>know JavaScript</em>. </p>



<p>Five years ago I was on a project where the perfectly fine .net MVC app was turned into an API for delivering content to a .net core app that used node services to render Vue templates into fully rendered HTML which meant that all of our templates were written in Vue <em>and we still had to write jQuery to make content interactive</em>.  We literally had two layers of JavaScript wrapped in a .net sandwich with JavaScript spread on the top. </p>



<p>I’m seeing architectures being proposed <em>today</em> that do pretty much the same thing I just described but with Drupal on Apache, containerized Node, and Web Components. </p>



<p>If  making a peanut butter and jelly sandwich by spreading the jelly on both sides of the bread is disturbing to you, <em>good</em>. You can still find God. </p>



<p>Do you have any idea how frustrating it is that that in order to explain my sadness to my therapist I must first explain like 5 different technologies and by the time I’m finished <em>she’s sad just hearing it</em>, the session’s over, and I didn’t even get to what was making me upset? Technology has made my anger a recursive function. </p>



<p><em>Can we please stop adding complexity to our systems just so we can do it in JavaScript? If you can do it without JavaScript, you probably should. Tools shouldn’t add complexity. </em></p>



<h2>Sass wasn’t just a mistake but a mistake factory</h2>



<p>10 years ago I thought CSS preprocessors were the coolest thing ever and I was incredibly fucking dumb for thinking that. </p>



<p>The <em>only three good features</em> CSS preprocessors gave were</p>



<ol>
<li>Variables</li>



<li>Mixins</li>



<li>Color transformation</li>
</ol>



<p>Every thing else was a mistake:</p>



<ul>
<li>Nesting</li>



<li>Calc</li>



<li>Functions, including control statements like if and for</li>



<li>Interpolation</li>



<li>Data types of any kind</li>



<li>The entire module thing</li>
</ul>



<h3>Nested CSS obscures complexity</h3>



<p>I hate this feature so much. I hate it more that it’s now native to CSS because not only has it spread, but <em>it’s mutated</em><sup>. <a href="#three">3</a></sup> </p>



<p>Nesting obscures complexity and then makes it impossible to find or fix. </p>



<p>Without using a compiler or psychedelic of any kind, can you tell me what this generates?</p>



<pre data-enlighter-language="scss" data-enlighter-theme="" data-enlighter-highlight="" data-enlighter-linenumbers="" data-enlighter-lineoffset="" data-enlighter-title="" data-enlighter-group="">.feature {
	color: blue;
	&amp;__header {
		p &amp; {
			a {
				&amp;:hover,
				&amp;:focus {
					color: red;
				}
			}
		}
	}
}</pre>



<p>Does this seem reasonable to you?</p>



<pre data-enlighter-language="css" data-enlighter-theme="" data-enlighter-highlight="" data-enlighter-linenumbers="" data-enlighter-lineoffset="" data-enlighter-title="" data-enlighter-group="">.feature {
  color: blue;
}
p .feature__header a:hover, p .feature__header a:focus {
  color: red;
}</pre>



<p>If that’s the output you expected, either you’re one of the Sass spec writers or you did a double-shot of espresso followed by a Ritalin chaser. </p>



<p>Nesting is not good. It hides the specificity that you’re producing and will lend to increasingly fragile code as you try to override a 13 part selector. </p>



<h3>Interpolation, et all</h3>



<p>This was also just a bad idea that we couldn’t have predicted at the time. I’m not mad at the Sass people for this. Hindsight is 20/20 and all. What I want is for it to all stop. </p>



<p>This is fundamentally a bad idea:</p>



<pre data-enlighter-language="scss" data-enlighter-theme="" data-enlighter-highlight="" data-enlighter-linenumbers="" data-enlighter-lineoffset="" data-enlighter-title="" data-enlighter-group="">$sizes: 10px, 20px, 30px, 50px;

@each $size in $sizes {
  .icon-#{$size} {
    font-size: $size;
    height: $size;
    width: $size;
  }
}
</pre>



<p>I know what you’re thinking. </p>



<p>You’re thinking, “But Frank, it saves me on keystrokes! Now I can generate more classes and classnames just based on that list of sizes!”</p>



<p>Bro. </p>



<p><kbd>ctrl</kbd>+<kbd>c</kbd> and <kbd>ctrl</kbd>+<kbd>v</kbd> exist. </p>



<p>Are you a VS Code user? Even easier: Highlight it and press <kbd>alt/opt</kbd> + <kbd>shift</kbd> + <kbd>↓</kbd> , that’ll copy the chunk or line down.  Then, highlight the part that’s repeated a few times (where you’d interpolate), press <kbd>ctrl/cmd</kbd> + <kbd>d</kbd>, and keep pressing until they’re all highlighted. Make your change. Now you’re done.</p>



<p>What did you save? Time and sanity in looking up the Sass syntax? Future time where future you is trying to find where the fuck <code>.icon-80px </code>is living? <a href="https://xkcd.com/1205/">XKCD is not on your side</a> for this automagical icon generator loop that’s going to piss you off with edge cases for the next 5 years. </p>



<p><em>Can we please stop using languages and tech stacks to solve problems that our IDEs are capable of helping us with? </em></p>



<h2>Seriously … forget the Frameworks</h2>



<p>I was blown away when Angular came out. It was an actual <em>thing</em> that could do it all. It could render the markup with fun logicy attributes, respond to data changes, make the AJAX requests, amazing! I wanted to use Angular for <em>all</em> the <strong>interactive web apps</strong>. </p>



<p>Then React came out. That was amazing. It could render the markup and … [incoherent mumbling] components or something… and … <em>wait, that wasn’t even markup; it was JSX. </em> Sure, I had to add a few libraries to really use it, but yay, I used it for <strong>interactive web apps</strong>. </p>



<p>Then there was Vue. Then Preact. Then Ember, Svelte, Backbone, Alpine, Mithril, Solid, Lit, Stencil, Qwik…</p>



<p>Maybe it’s because Angular was no one’s first choice — even though it came first. Maybe that’s why the front-end world lost its damned mind with React. React was so <em>not-angulary</em> that front-end folks were like, “let’s use it client side, server side, native-side, and the 3 other sides of a damned cube”. Folks decided it was a good idea to Reactify All The Things®. </p>



<p>And that’s how React, a framework used to deliver interactive content to a whole 28% of the world’s population, is being used to generate static sites. </p>



<p>I had a client a few years ago that was a <em>household name</em>. Their front-end was written in Gatsby. Gatsby is a <em>static site generator</em> that uses React. This globally-known brand had probably 50 websites in at least as many languages. But the content was very, very, static. </p>



<p>So imagine the horror the front-end dorks felt when they learned that a content author in Bulgaria  updating the terms and conditions didn’t want the whole-ass site rebuilt for every country and language. </p>



<p>This would’ve been a non-problem had the web app been written in .net, Java, or, fuck me for saying it, PHP or Node. </p>



<p>But nah. These goobers got high off npm modules and did shots of JSX in the bathroom at lunch time. They chose a framework <em>used to deliver interactive content  to a significant percentage of the planet</em> for generating non-interactive content on a server. It took them <em>weeks</em> to come up with a solution to the problem they created when they chose a problem as a solution.  </p>



<p>Yes. I am absolutely calling React a problem. I’m calling Angular, Vue, and All the Rest of Them ™ a problem. Because unless you have a <em>specific problem of highly interactive, data-driven content</em>, you don’t need a framework.</p>



<p><em>You don’t need a framework to render static content to the end user.  Stop creating complex solutions to simple problems</em></p>



<h2>Also more generally fuck your complexity</h2>



<p>This, more than anything, is a rant about <em>complexity</em>. </p>



<p>I am mad at how front-end developers create complex solutions to simple problems. I am mad that front-end developers use tools to obscure complexity. I am mad that front-end developers embrace complexity rather than challenge it. </p>



<p>You know what’s great about jQuery? </p>



<p>Everything. </p>



<p>jQuery took several complex problems (browsers do things differently, selecting elements is complex, changing things on elements is tedious, AJAX requests are <em>a lot</em>) and it provided a simple, intuitive, and readable solution:</p>



<pre data-enlighter-language="js" data-enlighter-theme="" data-enlighter-highlight="" data-enlighter-linenumbers="" data-enlighter-lineoffset="" data-enlighter-title="" data-enlighter-group="">$('.getResults').on('click', () =&gt; {
    $.ajax({
       url: '/api/results',
       data: {
         foo: 'bar'
       },
       success: function (result) {
          $('.results').html(result);
       }
    });
});</pre>



<p>Cool your jets for a second and think about how wonderfully <em>simple</em> that is. It handles finding your element in a way that lets you avoid the null checks, shortens your event binding, lets you make an AJAX  request, lets you find the element that the result goes into, and plop it there if it’s successful. </p>



<p>If the problem is, “user clicks on thing, you get data, and show data”, that is a <em>beautiful</em> and simple 11-line solution. It did not need NPM modules and a build system.</p>



<p>And <em>sure</em>, <a href="https://youmightnotneedjquery.com/">maybe you don’t need jQuery</a>, and that’s fine. Will you need more lines of code to do the same thing? Of course. But even without jQuery, your complexity doesn’t grow exponentially like it would if you brought in a framework with its build system, linters, compilers, and 700mb of Node Modules some of which inexplicably require some stupid shit like <a href="https://www.npmjs.com/~jonschlinkert" data-type="link" data-id="https://www.npmjs.com/~jonschlinkert">is-odd</a> or <a href="https://www.npmjs.com/package/is-git-url?activeTab=dependents" data-type="link" data-id="https://www.npmjs.com/package/is-git-url?activeTab=dependents">is-git-url</a> <sup><a href="#four">4</a></sup>.  </p>



<p>Pretend I’m not talking about JavaScript for a second. Let’s pretend it’s just an ordinary, everyday, 3-template web site. Maybe it’s managed by a CMS. Maybe it’s not. Does it really need Tailwind? Bootstrap? <em>Even &lt;gasp class=”from the back of the room”&gt; <strong>Sass</strong></em>? Is writing plain CSS really such a terrible thought?</p>



<p>Let me fill you in on a cool fact about CSS: It doesn’t need a fucking compiler. No <code>npm install -D sass</code>. No <code>npm i gulp</code>. You get to tell WebPack to fuck off good and proper-like, the way the Good Lord Intended. <sup><a href="#five">5</a></sup></p>



<p><em>Simplicity is unsophisticated and lacking in many parts. The simplest solution to a problem is always the best solution. Choose simplicity. I’m begging you. Your future is begging you,.   </em></p>



<h2>Oh, and enough with the new and shiny</h2>



<p>Sure it’s awesome that you can strongly type CSS variables, but that doesn’t mean you should <sup><a href="#six">6</a></sup>. Can you have web components with declarative shadow DOM? d’uh! But, hear me out: Why have three steps to render markup when you could have one?</p>



<p>Yes, the new JS Framework is all the awesomeness. Definitely do a side project with it. Please in the name of her Sweet Saint Fuck of Apathy, don’t use it in your next project. We haven’t learned what our regrets will be, and the regrets are what everyone else has to live with but you. </p>



<p>I don’t care if it’s a new CSS trick, a new JS thing, or a new JS framework: <strong>Give it two years before you take it for a ride in production</strong>. </p>



<p>You know what happens in the next two years?</p>



<p>Two year’s worth of StackOverflow questions. </p>



<p>You do <strong>not</strong> want to be the person taking a new tech or feature for a joy-ride in production and discovering some hemorrhaging-edge bug. </p>



<p>You <strong>also</strong> don’t want to be the person who has to clean up after the person who discovered a bug on a bleeding-edge technology. </p>



<p> Person-Alpha is the asshole. The Alphasshole, if you will. Because they chose to use the bleeding edge thing. </p>



<p>On the scrum-master’s Fibionacci-sequence of suckassness The Alphasshole is a 21. The Alphasshole implemented the new tech and turned your project into a collection of medical-attention-needed polyps on Satan’s butthole. </p>



<p>Person-Beta is the Betasshole. </p>



<p>The Betasshole had to work in real-time with other betassholes on other projects created by other alphassholes that were all equally hemorrhoidal in nature over StackOverflow and Discord to come up with whatever collection of hackery that properly deserves to be called, “Satan’s Self-soothing salve of sour-ass solutions.”</p>



<p>The Betasshole is a 5 on the Satanic-sequence of Suckassness. They’re not nearly as bad as the dickweed who chose new tech. But unfortunately, they made a lot of terrible choices and cannot be free from blame, git-related, or otherwise. </p>



<p>Listen. I’ve been an Alphasshole and a Betasshole before. I regret that. I should not have done that. </p>



<p>I’ve <em>also</em> been gamma waaaaaay more often. Who is not an asshole. That’s a person who’s just had to deal with two years of shit rolling downstream. What’s unfortunate for the gammas is that they often will be <em>accused</em> of being the assholes because they will git-blame, complain, and refrain from touching whatever that new tech was because it’s haunted their nightmares and their daymares. They are simply picking up the pieces. </p>



<p>The new and shiny is fun for side projects. It is not for production.</p>



<p><em>Please put two years between you and any new tech. If for anyone, do it for the gammas who actually have to keep your precious world wide web running. </em></p>



<h2>TL;DR</h2>



<p>Just… grow up and think.</p>



<ul>
<li>Think and care about the content first. Because that’s what the users care about. Assume the users’ interests are more important than your own. </li>



<li>Quit acting like CSS is some giant-ass mistake that needs fixing. A group of people who were collectively smarter than us wrote those specs. They didn’t make mistakes. Assume you are making the mistake, not them. </li>



<li>Server-side rendering was not invented with JavaScript and it doesn’t have to be implemented with JavaScript. There are many ways to render content on a server. Assume non-JavaScript solutions existed first.</li>



<li>JavaScript either is neither the God-send nor the hell-spawn for your project. It is a tool which you can choose to use incorrectly. Learn when JavaScript is the right tool for the job. </li>



<li>Quit doing hard things for simple problems. Quit doing things that are degrees of difficulty <em>harder</em> than the problem. When a problem presents itself, look for multiple solutions, and then choose the simplest one. Assume the simplest solution is the best one. </li>



<li>Quit chasing the new and shiny in your day job. I, too, am excited about the new and shiny. But I put a 2-year buffer between me and prod because I don’t want to be the person to have to discover browser bugs, framework bugs, or even compiler bugs. </li>
</ul>



<p>Or whatever. Don’t do this and continue to be an Alphasshole, Betasshole, or an otherwise generally contemptuous person. I don’t care. At least I can sleep better knowing that you know that I know that you now know that you’re an asshole. </p>



<p><s>Happy</s> apathetic coding!</p>























<div>
<h4>Footnotes, Sources, and Whatnots</h4>
<p id="one"><small>1</small> <a href="https://en.wikipedia.org/wiki/Express.js">Express.js was released in 2010. </a>
</p><p id="two"><small>2</small> <a href="https://www.infoworld.com/article/2609877/believe-the-hype--php-founder-backs-facebook-s-hiphop-technology.html">PHP started development in 1993 and was released in 1995</a> which means that when express.js dropped, PHP was driving to the store and buying clove cigarettes. </p>
<p id="three"><small>3</small> When I say it’s mutated, I mean that nesting jumped from Sass to native CSS. And CSS does it differently. <a href="https://sass-lang.com/blog/sass-and-native-nesting/">So now Sass has to change how it does nesting</a> because of how CSS does it. It’s bad. It will introduce lots of things you don’t want to happen. </p>

<p id="four"><small>4</small> People have been talking about how <a href="https://qz.com/646467/how-one-programmer-broke-the-internet-by-deleting-a-tiny-piece-of-code">left-pad broke the internet</a> and some wise folks have called out that it was pretty silly that such a simple function got to be so ubiquitous that it had the power to “crash the web”. But what’s <em> also dangerous</em> is the person <a href="https://github.com/jonschlinkert/jonschlinkert">actively trying to become ubiquitous</a>. Every PR that involves adding a new dev dependency to any project should require the developer to hand-write the entire dependency tree. That would probably save a lot of future heartache.  </p>

<p id="five"><small>5</small> I do not, in fact, have strong evidence that the Good Lord intended for WebPack to be told to, “fuck off good and proper like.” <em>However</em> <a href="https://www.bibleref.com/Leviticus/19/Leviticus-19-19.html">if the Good Lord had something to say about cotton blends</a>, He’d probably also have something to say about using a JavaScript build utility for building styles.  </p>
<p id="six"><small>6</small> <a href="https://developer.mozilla.org/en-US/docs/Web/CSS/@property">Yes, you can have strongly-typed CSS</a>, but it’s insanely tedious because those “types” are not like any types you’d expect from any other language, and the syntax is going to be a huge pain. Also, you have the option to set it to not inherit which is, in fact, the most unCSSy thing ever. So please. Don’t.    </p>
</div>
					</div><!-- .entry-content -->

		<!-- .entry-footer -->
	</article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Bomb Jack display hardware (106 pts)]]></title>
            <link>https://github.com/martinpiper/BombJack/blob/master/README.md</link>
            <guid>40745830</guid>
            <pubDate>Fri, 21 Jun 2024 03:25:26 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/martinpiper/BombJack/blob/master/README.md">https://github.com/martinpiper/BombJack/blob/master/README.md</a>, See on <a href="https://news.ycombinator.com/item?id=40745830">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true" aria-labelledby="file-name-id-wide file-name-id-mobile"><article itemprop="text"><p dir="auto"><h2 tabindex="-1" dir="auto">Bomb Jack display hardware</h2><a id="user-content-bomb-jack-display-hardware" aria-label="Permalink: Bomb Jack display hardware" href="#bomb-jack-display-hardware"></a></p>
<ul dir="auto">
<li>Main social media: <a href="https://mastodon.social/@martin_piper" rel="nofollow">https://mastodon.social/@martin_piper</a></li>
<li>Don't have a Mastodon account? Click here for an invitation link: <a href="https://mastodon.social/invite/nuGxMC8z" rel="nofollow">https://mastodon.social/invite/nuGxMC8z</a></li>
<li>Itch IO project page: <a href="https://martin-piper.itch.io/bomb-jack-display-hardware" rel="nofollow">https://martin-piper.itch.io/bomb-jack-display-hardware</a></li>
<li>YouTube channel: <a href="https://www.youtube.com/c/MartinPiper6502" rel="nofollow">https://www.youtube.com/c/MartinPiper6502</a></li>
<li>Buy me a ko-fi: <a href="https://ko-fi.com/martinpiper0750" rel="nofollow">https://ko-fi.com/martinpiper0750</a></li>
<li>Bluesky: <a href="https://bsky.app/profile/martinpiper.bsky.social" rel="nofollow">https://bsky.app/profile/martinpiper.bsky.social</a></li>
<li>Twitter links: <a href="https://twitter.com/MartinPiper" rel="nofollow">https://twitter.com/MartinPiper</a> <a href="https://twitter.com/hashtag/RetroTTLVideoCard" rel="nofollow">https://twitter.com/hashtag/RetroTTLVideoCard</a> <a href="https://twitter.com/hashtag/RetroTTL" rel="nofollow">https://twitter.com/hashtag/RetroTTL</a></li>
<li>Threads: <a href="https://www.threads.net/@martinpiper1187" rel="nofollow">https://www.threads.net/@martinpiper1187</a></li>
<li>Mostly using these tags: #MegaWang #RetroTTL #RetroTTLVideoCard #C64 #Commodore64 #electronics</li>
</ul>
<p dir="auto">This started life as a working schematic for the video display portion of the original Bomb Jack arcade hardware. Why bother, you're probably asking? Well if you have to ask then perhaps you're not the right audience. :)</p>
<p dir="auto">The version 1.0 of this work was approximately interface pin and chip compatible with the original hardware schematic. Now since V2.0 the hardware has progressed to add extra features not present in the original, such as:</p>
<ul dir="auto">
<li>Addressable RAM instead of ROMs</li>
<li>Extra display blanking capability</li>
<li>Tile X/Y pixel scroll</li>
<li>64x64 Tile X/Y offset, useful for fast hardware scrolling</li>
<li>Full screen height sprites</li>
<li>Background "mode7" screen that allows per-pixel affine transformation</li>
<li>Background colour select (Can be rapidly updated during the frame for plasma effects)</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Background</h2><a id="user-content-background" aria-label="Permalink: Background" href="#background"></a></p>
<p dir="auto">This project started when old retro arcade hardware was being discussed. In the back of my mind was the often fabled "Mega games" by Imagine Software which were planned to use extra hardware on the Spectrum and Commodore 64 to augment the machine's capabilities. Since this hardware uses TTL logic available back from the same time period I was wondering exactly how much extra graphical grunt could have been engineered and interfaced with these old 8-bit computers.</p>
<p dir="auto">Truth be told, the Imagine hardware was pretty much just extra RAM <a href="https://www.gamesthatwerent.com/gtw64/mega-games/" rel="nofollow">https://www.gamesthatwerent.com/gtw64/mega-games/</a> but this was a fun project to see how far the arcade hardware was pushing the limits of board size and signal complexity.</p>
<p dir="auto">I was looking at Bomb Jack boards on ebay and pondering how they had enough fill-rate to draw 24 16x16 sprites and have the option for some to use 32x32 mode as well. A friend and I were discussing the clock speed and fill-rate while trying to deduce the operation of the hardware just by inspecting the hand drawn schematics, as you do.</p>
<p dir="auto">In the end to get some clarity on the sprite plotting specifically I started to transcribe what was thought to be the sprite logic portion of the schematic into Proteus, since it can simulate digital electronics really well.</p>
<p dir="auto">Interestingly the Amiga computer, with its custom chips, was designed and prototyped using breadboards: <a href="https://arstechnica.com/gadgets/2007/08/a-history-of-the-amiga-part-3/" rel="nofollow">https://arstechnica.com/gadgets/2007/08/a-history-of-the-amiga-part-3/</a></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Digital display and audio simulation</h2><a id="user-content-digital-display-and-audio-simulation" aria-label="Permalink: Digital display and audio simulation" href="#digital-display-and-audio-simulation"></a></p>
<p dir="auto">The arcade hardware uses resistor ladders to convert 4x3 bit RGB+H+V sync digital signals into analog, this is a typical model for the period. I later updated the design to use 15 bit colour with 16x16 palette entries. Since the simulation software is optimised for digital model it makes sense to have a digital display driver simulation that accepts digital signals. Hence I created this Proteus VSM module: <a href="https://github.com/martinpiper/DigitalDisplay">https://github.com/martinpiper/DigitalDisplay</a></p>
<p dir="auto">This simulation can also be used to debug failures in original boards, since disabling chips or signals in the simulation can produce similar output display artefacts. For example disabling one of the scan line RAM chips (4A/4B/4C/4D) produces alternate pixels and bright lines, similar to the original hardware.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Why not use a CPLD or FPGA?</h2><a id="user-content-why-not-use-a-cpld-or-fpga" aria-label="Permalink: Why not use a CPLD or FPGA?" href="#why-not-use-a-cpld-or-fpga"></a></p>
<p dir="auto">Good question! For my thinking, it is a question of what level to want to teach. In software eningeering there are numerous languages, C/C++, Python, Java, Forth, Pascal, etc. There is also machine code and usually some form of assembler mnemonic language which directly maps to machine code, the user accessible low-level language used by a particular CPU. Below that there can be microcode which is often not user accesible, this tends to control the internal state of a CPU. As a software engineer, I see value in using higher level languages, but often use low-level assembler when I want to optimise.</p>
<p dir="auto">A FPGA or CPLD, while it deals with logical operations, is usually programmed via a higher level language (VHDL or Verilog or similar) abstraction above the low-level logic cells actually used by the device, often (not always) the same code can be compiled for subtly different devices with minimal changes.</p>
<p dir="auto">7400-series discrete logic, specifically 74LS series logic used in this project, can be considered to be low-level, analogous to using machine code or assembler. There is a lower level, which is using discrete transistors to create logic cells, like NAND, OR, NOT, etc. but that really is a low level more like microcode.</p>
<p dir="auto">The use of high level or low-level programming is a contextual decision, often my knowledge of low-level architecture and performace informs my decision making processes when using high level languages. While it is certainly possible to code an application entirely with only high level language knowledge, it is an advantage to know low-level architecture to help debug hopefully rare performance or error situations.</p>
<p dir="auto">All that said, I believe that knowing how to engineer a solution using low-level discrete logic is an advantage, in other words it helps to improve the use of higher level CPLD or FPGA engineering to know lower level architecture. This is why this project exists, to help teach how to solve engineering problems using low-level discrete logic.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Developer technical details</h2><a id="user-content-developer-technical-details" aria-label="Permalink: Developer technical details" href="#developer-technical-details"></a></p>
<p dir="auto">To produce the test data run: BDD6502\ForUnitTests.bat</p>
<p dir="auto">If you have Proteus setup with the digital display model, you should see a screen that looks somewhat like this when starting the simulation:</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/martinpiper/BombJack/blob/master/Capture.PNG?raw=true"><img src="https://github.com/martinpiper/BombJack/raw/master/Capture.PNG?raw=true" alt="Screenshot" title="Screenshot"></a></p>
<p dir="auto">The new hardware scrolling registers also work well:</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/martinpiper/BombJack/blob/master/output/debug.gif?raw=true"><img src="https://github.com/martinpiper/BombJack/raw/master/output/debug.gif?raw=true" alt="animation" title="animation" data-animated-image=""></a></p>
<p dir="auto">The hardware supports sprite multiplexing and stacking, giving the option of having more than eight colours in sprites:</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/martinpiper/BombJack/blob/master/output/debug2.gif?raw=true"><img src="https://github.com/martinpiper/BombJack/raw/master/output/debug2.gif?raw=true" alt="animation" title="animation2" data-animated-image=""></a></p>
<p dir="auto">Many thanks to <a href="https://opengameart.org/content/rpg-town-pixel-art-assets" rel="nofollow">https://opengameart.org/content/rpg-town-pixel-art-assets</a> and <a href="https://opengameart.org/content/colored-16x16-fantasy-tileset" rel="nofollow">https://opengameart.org/content/colored-16x16-fantasy-tileset</a> for the game art.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">High level schematic sheets</h3><a id="user-content-high-level-schematic-sheets" aria-label="Permalink: High level schematic sheets" href="#high-level-schematic-sheets"></a></p>
<p dir="auto">The Proteus sheet numbers correspond to the original schematic page numbers as written in the bottom right hand corner of each page. The layout of this schematic roughly matches the layout of the original schematic. So for example the analog RGB resistor ladders are on "Root sheet 8" in the top right of the sheet, as in the original schematic page 8 layout.</p>
<ol dir="auto">
<li>Logic analyser, virtual video display model, expansion bus header, data generator
<ol dir="auto">
<li>Memory selection logic based on external address bus</li>
<li>VSMDD2 is a replacement for the Proteus data generators because they have a 1024 byte limit. Proteus VSM project source: <a href="https://github.com/martinpiper/DigitalData">https://github.com/martinpiper/DigitalData</a></li>
<li>EXPANSIONBUS, EXPANSIONBUS2 and EXPANSIONBUS3 all relate to the C64 user port to 24 bit address logic in the project: UserPortTo24BitAddress.pdsprj</li>
</ol>
</li>
<li>Headers, the original schematic has dip switch logic</li>
<li>Video timing
<ol dir="auto">
<li>Horizontal video signal generation</li>
<li>Vertical video signal generation</li>
<li>HBLANK and VBLANK signal generation</li>
</ol>
</li>
<li>Sprite logic - Part 1
<ol dir="auto">
<li>Sprite RAM register access logic, timing with the external address/data bus and video internal timings</li>
<li>Temporary storage for sprite position, palette and frame</li>
<li>Bit plane access and bit shifters for output pixels to scan RAM</li>
<li>32x32 sprite selection registers and comparison</li>
<li>Various signal timings generated for sprite loading, tile and colour fetch etc</li>
</ol>
</li>
<li>Sprite logic - Part 2
<ol dir="auto">
<li>Dual scan line RAM buffers</li>
<li>Transparent pixel test logic based on inverted pixel temporary storage and selection with NOR check</li>
<li>Pixel writing to RAM based on 16 pixel chunks and position register contents</li>
<li>Logic for timing of pixel reads for possible video display, plus clearing of data just read</li>
</ol>
</li>
<li>Character screen RAM with associated colour screen
<ol dir="auto">
<li>Associated external bus interface logic</li>
<li>Bit plane reads and pixel shifters</li>
<li>Output pixel logic for possible video display</li>
</ol>
</li>
<li>16x16 tile background picture logic
<ol dir="auto">
<li>Logic for selection of appropriate picture, based on high address lines</li>
<li>Bit plane reads and pixel shifters</li>
<li>Output pixel logic for possible video display, or disable background logic</li>
</ol>
</li>
<li>Output pixel pipeline and palette lookup
<ol dir="auto">
<li>Pixel transparency test from background, character screen and sprites</li>
<li>Palette RAMs with external bus logic</li>
<li>Final pixel latching logic and palette RAM lookup</li>
<li>Output digital to analog conversion using resistor ladders</li>
<li>Analog RGB and associated sync signal output header</li>
</ol>
</li>
<li>Mode7 calculation
<ol dir="auto">
<li>Register addressing and latches</li>
<li>dx, dxy, dy, dyx calculation</li>
<li>xorg, yorg addition</li>
</ol>
</li>
<li>Mode7 pixel logic
<ol dir="auto">
<li>Background colour latch, transparent pixel detection</li>
<li>Interleaved blocks (for RAM timing) mode7 screen access, feeding into tile access, H &amp; V flip logic</li>
</ol>
</li>
<li>Scaled sprite logic - Part 1
<ol dir="auto">
<li>Register memory</li>
<li>Sprite extent and scan line detection</li>
<li>Sprite flipping</li>
</ol>
</li>
<li>Scaled sprite logic - Part 2
<ol dir="auto">
<li>Scanline - Two alternating banks</li>
<li>Sprite definition RAM</li>
</ol>
</li>
<li>Vector logic
<ol dir="auto">
<li>Register logic - Bank display and kill</li>
<li>Scan RAM banks - Two banks</li>
</ol>
</li>
<li>Pixel data combiner (merge layer) logic
<ol dir="auto">
<li>2-1 pixel data combiners - Two blocks</li>
</ol>
</li>
</ol>
<p dir="auto"><h3 tabindex="-1" dir="auto">How to access the hardware</h3><a id="user-content-how-to-access-the-hardware" aria-label="Permalink: How to access the hardware" href="#how-to-access-the-hardware"></a></p>
<p dir="auto">In the original hardware, used in the Bombjack arcade game, the Z80 data and its address bus was partially connected to the hardware with a couple of address windows at $9000 to $9fff (perhaps a bit wider than that), this was mostly write RAM as far as the Z80 was concerned. It was sprite registers, some control registers, palette and char screen RAM. The rest of the video internal memory was ROM, and could not be addressed by the Z80. The Z80 would have to time writes to the video address space (RAM) during the VBLANK when the video hardware was not expecting to read RAM to avoid contention.</p>
<p dir="auto">For this updated hardware, the C64 (or a Z80) sends data bytes via the user port (parallel transfer, so it's fast, one clock cycle to write), with some control signals, to write to a 24 bit address space in the hardware. This is usually accomplished by the UserPortTo24BitAddress.pdsprj (or better) project. Basically a control signal resets the internal state, the next three bytes are the address to write, then next bytes are consecutively written to the address space until the internal state is reset again. There is a signal from the hardware to the C64 via the user port for screen refresh (VBLANK) which is read as an NMI (CIA2 FLAG pin) request on the C64. This means there is no bus contention with the C64 memory bus to the hardware. Writes to hardware RAM take precedence over what the hardware is reading from the RAM, this ensures data is written without contention. Writes to the hardware which involve RAM, rather than register latches, should usually be timed to coincide with VBLANK or HBLANK to avoid visual glitches. Each layer (chars, tiles, sprites etc) provides information on any RAM timing constraints.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Memory map</h3><a id="user-content-memory-map" aria-label="Permalink: Memory map" href="#memory-map"></a></p>
<p dir="auto">Note: Most control registers can be updated at any point during the frame and the next pixel will reflect the change.</p>
<p dir="auto">0x9000			Chars control register
</p>
<p dir="auto">0x9c00-0x9dff	Palette RAM</p>
<p dir="auto">0x9e00	0x10	Use background colour
0x20	Enable display
0x80	BorderY shrink</p>
<p dir="auto">0x9e08	Layer priority select, for each layer select the input header pixel input
Default should be: %11100100 = 0xe4
i.e. Back most layer	= %11 (3)
Next layer closer		= %10 (2)
Then next closest		= %01 (1)
Then front most layer	= %00 (0)</p>
<p dir="auto">0x9e09	0xlg	Setup nybble extents for overscan to enable the border (g = greater than, and l = less than) in 8 pixel steps
Overscan mode is always on, so this needs to be set to sensible values
Sensible wide value for this is $29 which hides the left edge (including 16x16 tiles) and brings in the right edge in slightly
This gives a 320 pixel wide screen, displaying a full width of 40 8x8 characters</p>
<p dir="auto">0x9e0a	Video layer enable flags. These are passed through the VideoX connectors. Each layer has the option of reading their enable flag and taking appropriate action.
Bits	Action				Default layer assigned to this connector (can be changed)		Replaces logic
0x01	Enable layer 1		Sprites (or Sprites2)											0x9a00 Upper nybble: $10 = Enable sprite output (6S SPREN)
0x02	Enable layer 2		Chars															0x9000 Chars control register	0x02	Chars screen disable
0x04	Enable layer 3		Tiles															0x9e00	0x10	Enable tiles
0x08	Enable layer 4		Background or mode7												0xa015	: Flags	:	0 : Enable display</p>
<p dir="auto">0x9e0b	Background colour
If the final pixel is transparent and 0x9e00 0x10 is set, then the palette information is read from this colour value
If the final pixel is transparent and 0x9e00 0x10 is not set, then the palette information is read from the last layer colour value</p>
<p dir="auto">0x9e0c	Palette bank number
See: kBus24Bit_VideoLayer_ExpandedPalettes</p>
<p dir="auto">Sprite 32x32 size select
0x9a00 start index of 32x32 sprites
0x9a01 end index of 32x32 sprites (exclusive)
So 0,0 = no 32x32 sprites
0,8 means indexes 2 to 7 (exclusive) are 32x32
Maximum number in both is 0xf</p>
<p dir="auto">At 0x9800 - 0x985f each sprite is described by 4 bytes:</p>
<div data-snippet-clipboard-copy-content="Byte 0:
	The tile code for the sprite, used to look up the sprite’s image bitplanes in the tile ROMs
	(MAME Emu documentation is wrong, bit 7 has nothing to do with selecting double size mode)

Byte 1: HVFMCCCC
	Bit 7: H : If set, the sprite is horizontally flipped
	Bit 6: V : If set, the sprite is vertically flipped
	Bit 5: F : If set, the sprite is full screen height. The sprite data will repeat.
	Bit 4: MSBX
	Bits 3..0: C : 4 bits to provide the colour value for the tile decoder

Byte 2: The sprite’s Y position on screen
Byte 3: The sprite’s X position on screen"><pre><code>Byte 0:
	The tile code for the sprite, used to look up the sprite’s image bitplanes in the tile ROMs
	(MAME Emu documentation is wrong, bit 7 has nothing to do with selecting double size mode)

Byte 1: HVFMCCCC
	Bit 7: H : If set, the sprite is horizontally flipped
	Bit 6: V : If set, the sprite is vertically flipped
	Bit 5: F : If set, the sprite is full screen height. The sprite data will repeat.
	Bit 4: MSBX
	Bits 3..0: C : 4 bits to provide the colour value for the tile decoder

Byte 2: The sprite’s Y position on screen
Byte 3: The sprite’s X position on screen
</code></pre></div>
<p dir="auto">0x9200 Sprites2 registers
Sprites support X and Y flips with X &amp; Y repeating patterns</p>
<div data-snippet-clipboard-copy-content="Byte 0: Palette | 0x10 = MSBX | 0x20 = MSBY | 0x40 = flipX | 0x80 = flipY
Byte 1: Y pos
Byte 2: Y size (in screen pixels, regardless of scale) (0 = end of list)
Byte 3: X pos
Byte 4: X scale extent (uses internal coordinates)
Byte 5: Y inv scale (*32)
Byte 6: X inv scale (*32)
Byte 7: Sprite frame (index) | 0x40 = halfX | 0x80 = halfY"><pre><code>Byte 0: Palette | 0x10 = MSBX | 0x20 = MSBY | 0x40 = flipX | 0x80 = flipY
Byte 1: Y pos
Byte 2: Y size (in screen pixels, regardless of scale) (0 = end of list)
Byte 3: X pos
Byte 4: X scale extent (uses internal coordinates)
Byte 5: Y inv scale (*32)
Byte 6: X inv scale (*32)
Byte 7: Sprite frame (index) | 0x40 = halfX | 0x80 = halfY
</code></pre></div>
<p dir="auto"><h4 tabindex="-1" dir="auto">Tile and sprite data layout</h4><a id="user-content-tile-and-sprite-data-layout" aria-label="Permalink: Tile and sprite data layout" href="#tile-and-sprite-data-layout"></a></p>
<p dir="auto">16x16 pixel tile and sprite data is stored in 8x8 cells and in memory order:
01
23</p>
<p dir="auto">For 32x32 sprites, this arrangement is expanded in memory as:
0145
2367
89cd
abef</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Mode7 registers</h3><a id="user-content-mode7-registers" aria-label="Permalink: Mode7 registers" href="#mode7-registers"></a></p>
<p dir="auto">Mode7 borrows its name from the graphics mode on the Super NES video game console. <a href="https://en.wikipedia.org/wiki/Mode_7" rel="nofollow">https://en.wikipedia.org/wiki/Mode_7</a></p>
<p dir="auto">0xa000 - 0xa011	: Sets of 24 bit (8 byte) registers in l/h/hh order. There are 8 bits of accuracy, i.e. numbers are multiplied by 256.0f</p>

<p dir="auto">Each pixel on the layer is accessed using the accumulated results of these registers, this means it is possible to have per-pixel transformations combining translation, scaling, reflection, rotation, and shearing.
It is possible to update these registers per scanline, or per pixel, and generate even more complex results. Register values are latched and used immediately, so timing the update of the three bytes must be considered.</p>
<p dir="auto">Note: The internal accumulated values, x/xy/y/yx are not accessible via registers and are reset only by _EHSYNC and _EVSYNC</p>
<p dir="auto">For each horizontal pixel (on +ve 6MHz):</p>
<div data-snippet-clipboard-copy-content="x += dx
yx += dyx

xo = x + xy + xorg
yo = y + yx + yorg"><pre><code>x += dx
yx += dyx

xo = x + xy + xorg
yo = y + yx + yorg
</code></pre></div>
<p dir="auto">For each scanline (on +ve _EHSYNC):</p>

<p dir="auto">For each scanline (on _EHSYNC):</p>

<p dir="auto">For each frame (on _EVSYNC):</p>

<p dir="auto">All of the above is one way to progressively calculate xo,yo using addition. In other words:</p>
<div data-snippet-clipboard-copy-content="xo = (dx*x) + (dxy*y) + xorg
yo = (dy*y) + (dyx*x) + yorg"><pre><code>xo = (dx*x) + (dxy*y) + xorg
yo = (dy*y) + (dyx*x) + yorg
</code></pre></div>
<p dir="auto">During the coordinate (xo,yo) to screen transformation: bits 8-11 are used to lookup the tile pixel, bits 12-18 for the x screen tile index position, bits 12-17 for the y screen tile index position.</p>
<p dir="auto">Note: The full, but hidden by borders, screen resolution is 384x264 pixels.</p>
<p dir="auto">0xa014	: Background colour, any pixel index of zero in the mode7 tile data will show the background colour
0xa015	: Flags	:	0 : Enable X update, or reset to 0
1 : Enable XY update, or reset to 0
2 : Enable Y update, or reset to 0
3 : Enable YX update, or reset to 0</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Audio</h3><a id="user-content-audio" aria-label="Permalink: Audio" href="#audio"></a></p>
<p dir="auto">Current audio hardware has 4 voices, each using 11 bytes. Extra bytes are used for <em>currently unused byte</em> and voice active bit masks after the voice registers.</p>
<p dir="auto">Each voice register layout is as follows:
byte	register
0		volume
1/2		address lo/hi
3/4		length lo/hi
5/6		rate lo/hi
7/8		loop address lo/hi
9/10	loop length lo/hi</p>
<p dir="auto">The sample frequency in hertz to voice rate conversion, using internal 4MHz clock, is:  (4096 * hertz) / 25000</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Clock speeds</h3><a id="user-content-clock-speeds" aria-label="Permalink: Clock speeds" href="#clock-speeds"></a></p>
<p dir="auto">The original schematic uses a 6MHz clock for all the video hardware, as denoted by the "6MHz" signal line. The clocking can be found on "Root sheet 1" just about the logic analyser.</p>
<p dir="auto">To help debug timing issues, especially with RAMs, the schematic should be simulated at 6MHz. However the Proteus simulation can use 1MHz on this signal line, I kept the naming of the line the same as the original schematic however. This is due to the default RAM write timings for ICs 4A/4B/4C/4D being too tight. This means the digital display driver will detect ~10fps, not ~60fps as per the original design. It does however make the debug single step time easier to think about since it's not divided by 6MHz...</p>
<p dir="auto">With VIDCLK = 2M</p>
<p dir="auto">Logic analyser:
Display scale 0.2u
Capture Resolution 0.2u</p>
<p dir="auto">Remember to use:</p>
<div data-snippet-clipboard-copy-content="System->Set Animation Options
	Single Step Time: 250n
	500n For debugging full clock cycle and pixel clock"><pre><code>System-&gt;Set Animation Options
	Single Step Time: 250n
	500n For debugging full clock cycle and pixel clock
</code></pre></div>
<p dir="auto"><h3 tabindex="-1" dir="auto">Where is the Z80 CPU?</h3><a id="user-content-where-is-the-z80-cpu" aria-label="Permalink: Where is the Z80 CPU?" href="#where-is-the-z80-cpu"></a></p>
<p dir="auto">The Z80 CPU from the original schematic is not included, it was clocked independently and has its own RAM, except for the video hardware interfaces and isn't needed for the video simulation.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Input data setup</h3><a id="user-content-input-data-setup" aria-label="Permalink: Input data setup" href="#input-data-setup"></a></p>
<p dir="auto">The Z80 data write signals are mocked using a simulator pattern generator VSMDD2, this is separate to the main video schematic and the generator is excluded from the PCB layout.</p>
<p dir="auto">As per the original design all writes to the video hardware should be carefully timed to coincide with the VBLANK or other safe write blanking periods. This is because the video hardware is almost always reading the RAM during the visible portion of the frame. Writing to the sprite registers outside the VBLANK will especially produce nasty looking effects on the screen. This RAM sharing model is quite common is old arcade and console hardware.</p>
<p dir="auto">The original hardware has been expanded to include RAMs where the ROMs were located. These are addressed by an combination of EXPANSIONBUS3 to select the groups of RAMs and the EXPANSIONBUS group selector. It is entirely possible to write more than one group at a time by enabling multiple output bits in EXPANSIONBUS3.</p>
<p dir="auto">For EBBS = $01 the RAM address select lines are from $8000 until $b800 in steps of $800</p>
<table>
<thead>
<tr>
<th>EXPANSIONBUS3</th>
<th>Group</th>
<th>EXPANSIONBUS address</th>
<th>Behaviour</th>
</tr>
</thead>
<tbody>
<tr>
<td>$01</td>
<td>Audio registers</td>
<td>$8000 11 bytes/voice</td>
<td>Audio registers</td>
</tr>
<tr>
<td>$01</td>
<td>Original RAMs</td>
<td>$9000</td>
<td>Char screen control</td>
</tr>
<tr>
<td>$01</td>
<td>Original RAMs</td>
<td>$9001/2</td>
<td>Char screen X scroll</td>
</tr>
<tr>
<td>$01</td>
<td>Original RAMs</td>
<td>$9003/4</td>
<td>Char screen Y scroll</td>
</tr>
<tr>
<td>$01</td>
<td>Sprite2 registers</td>
<td>$9200-$92ff</td>
<td>Sprite2 registers</td>
</tr>
<tr>
<td>$01</td>
<td>Original RAMs</td>
<td>$9800   $60 bytes</td>
<td>Sprite registers</td>
</tr>
<tr>
<td>$01</td>
<td>Original RAMs</td>
<td>$9c00   $200 bytes</td>
<td>Palette GR XB 2 bytes per entry</td>
</tr>
<tr>
<td>$01</td>
<td>Original RAMs</td>
<td>$9e00</td>
<td>Background image enable and borders</td>
</tr>
<tr>
<td>$01</td>
<td>Original RAMs</td>
<td>$9e01/2</td>
<td>Background image XL/XH pixel scroll</td>
</tr>
<tr>
<td>$01</td>
<td>Original RAMs</td>
<td>$9e03/4</td>
<td>Background image YL/YH pixel scroll</td>
</tr>
<tr>
<td>$01</td>
<td>Original RAMs</td>
<td>$9e07</td>
<td>Background colour select</td>
</tr>
<tr>
<td>$01</td>
<td>Original RAMs</td>
<td>$9e08</td>
<td>Layer priority select</td>
</tr>
<tr>
<td>$01</td>
<td>Original RAMs</td>
<td>$9a00-$9a01</td>
<td>Start/end 32x32 sprite index 0-f only</td>
</tr>
<tr>
<td>$01</td>
<td>Extension mode7 registers</td>
<td>$a000-$a017</td>
<td>Mode7 registers</td>
</tr>
<tr>
<td>$01</td>
<td>Vectors</td>
<td>$a100   1 byte</td>
<td>Vectors display bank</td>
</tr>
<tr>
<td>$01</td>
<td>Combiner</td>
<td>$a200   4 bytes</td>
<td>Combiner display bank</td>
</tr>
<tr>
<td>Note: APU and vectors are by default using the same EBBS in the schematics</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>$02</td>
<td>APU</td>
<td>$8000   8KB</td>
<td>APU memory bank 0 instruction</td>
</tr>
<tr>
<td>$02</td>
<td>APU</td>
<td>$4000   8KB</td>
<td>APU memory bank 1 data</td>
</tr>
<tr>
<td>$02</td>
<td>APU</td>
<td>$2000   8KB</td>
<td>APU bank 2 registers</td>
</tr>
<tr>
<td>$02</td>
<td>Vectors</td>
<td>$0000   16KB</td>
<td>Vectors bank 0 data</td>
</tr>
<tr>
<td>$02</td>
<td>Vectors</td>
<td>$8000   16KB</td>
<td>Vectors bank 1 data</td>
</tr>
<tr>
<td>Note: Audio and Sprites2 are by default using the same EBBS in the schematics</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>$04</td>
<td>Audio memory</td>
<td>$0000-$ffff</td>
<td>Audio sample memory</td>
</tr>
<tr>
<td>$04</td>
<td>Sprites2 data</td>
<td>$2000   8KB++</td>
<td>Sprites2 bit planes</td>
</tr>
<tr>
<td>$80</td>
<td>Background 16x16 Root sheet 7</td>
<td>$2000   8KB</td>
<td>Tiles and colours into 4P7R</td>
</tr>
<tr>
<td>$80</td>
<td>Char screen</td>
<td>$4000   8KB</td>
<td>Chars into U258</td>
</tr>
<tr>
<td>$80</td>
<td>Char screen colours</td>
<td>$8000   8KB</td>
<td>Chars colours into U137</td>
</tr>
<tr>
<td>$40</td>
<td>Background 16x16 Root sheet 7</td>
<td>$2000   8KB</td>
<td>Tiles bit plane 0 into 8R7R</td>
</tr>
<tr>
<td>$40</td>
<td>Background 16x16 Root sheet 7</td>
<td>$4000   8KB</td>
<td>Tiles bit plane 1 into 8N7R</td>
</tr>
<tr>
<td>$40</td>
<td>Background 16x16 Root sheet 7</td>
<td>$8000   8KB</td>
<td>Tiles bit plane 2 into 8L7R</td>
</tr>
<tr>
<td>$20</td>
<td>Char screen 8x8 Root sheet 6</td>
<td>$2000   8KB</td>
<td>Char data bit plane 0 into 8K6R</td>
</tr>
<tr>
<td>$20</td>
<td>Char screen 8x8 Root sheet 6</td>
<td>$4000   8KB</td>
<td>Char data bit plane 1 into 8H6R</td>
</tr>
<tr>
<td>$20</td>
<td>Char screen 8x8 Root sheet 6</td>
<td>$8000   8KB</td>
<td>Char data bit plane 2 into 8E6R</td>
</tr>
<tr>
<td>$10</td>
<td>Sprite data Root sheet 4</td>
<td>$2000   8KB</td>
<td>Sprite bit plane 0 into 7JR</td>
</tr>
<tr>
<td>$10</td>
<td>Sprite data Root sheet 4</td>
<td>$4000   8KB</td>
<td>Sprite bit plane 1 into 7LR</td>
</tr>
<tr>
<td>$10</td>
<td>Sprite data Root sheet 4</td>
<td>$8000   8KB</td>
<td>Sprite bit plane 2 into 7MR</td>
</tr>
<tr>
<td>$08</td>
<td>Mode7 Root sheet 10</td>
<td>$2000   8KB</td>
<td>Mode7 screen data 128x64 tile index</td>
</tr>
<tr>
<td>$08</td>
<td>Mode7 Root sheet 10</td>
<td>$4000   8KB</td>
<td>Mode7 tile data 32 of 16x16 tiles</td>
</tr>
<tr>
<td>$08</td>
<td>Mode7 Root sheet 10</td>
<td>$8000   8KB</td>
<td>Mode7 tile data 32 of 16x16 tiles</td>
</tr>
</tbody>
</table>
<ul dir="auto">
<li>
<p dir="auto">Using the pattern file: TestData.txt</p>
<p dir="auto">The sprite writes particularly have various configurations for sprite displays, these are commented in/out depending on the desired patterns</p>
<p dir="auto">** Comment: Spread out all over the screen</p>
<p dir="auto">This includes background screen setup, char screen data setup and sprite palette, frames and position setup.
It provides good variation of sprites, palettes etc across the full range of screen coordinates. This is useful for testing expected masking logic and general visual integrity.</p>
<p dir="auto">** Comment: Top left all the same</p>
<p dir="auto">This sets all sprites to be in the top left of the screen, it is useful to testing maximum pixel write through and sprite selection logic scenarios. The background is disabled and char screen uses transparent chars. This leaves just the sprite plane outputting pixels.
Setting a logic break on RV[0..8] = 0xe8 will allow the simulation timing and scan line RAM contents to be inspected in detail.
The timing of sprite pixel writes into scan RAM 4A/4B and pixel reads from scan RAM 4C/4D with its clear to $ff can be seen whilst single stepping. Note the values for the RV and RH bus lines just below the video display.</p>
<p dir="auto">** Comment: X pos group</p>
<div data-snippet-clipboard-copy-content="  This regularly spaces the sprites with the same X pos +/-4 pixels. Useful for debugging."><pre><code>  This regularly spaces the sprites with the same X pos +/-4 pixels. Useful for debugging.
</code></pre></div>
<p dir="auto">** Comment: Y pos group</p>
<div data-snippet-clipboard-copy-content="  This regularly spaces the sprites with the same Y pos +/-4 pixels. Useful for debugging."><pre><code>  This regularly spaces the sprites with the same Y pos +/-4 pixels. Useful for debugging.
</code></pre></div>
</li>
<li>
<p dir="auto">Using the pattern file: TestData2.txt</p>
<p dir="auto">Using the ImageToBitplane tool Run/Debug Configuration: Main all conv
Using GenerateData configuration: Debug2</p>
<p dir="auto">This demonstrates stacked multiplexed sprites over a 16x16 tiled background.</p>
<p dir="auto">Using the ImageToBitplane tool Run/Debug Configuration: Main old bridge RGB 0 0 0 palette opt rgbfactor 32</p>
<p dir="auto">This demonstrates a detailed background picture with 32 palettes of 8 entries each.</p>
<p dir="auto">Using the ImageToBitplane tool Run/Debug Configuration: Main mode7 "map_9 - mode7 test.png"
And enabling the data load at: ; Write Mode7 registers
This will demonstrate how the mode7 export with flipped tile detection works</p>
</li>
</ul>
<p dir="auto"><h3 tabindex="-1" dir="auto">Raster line schedule</h3><a id="user-content-raster-line-schedule" aria-label="Permalink: Raster line schedule" href="#raster-line-schedule"></a></p>
<div data-snippet-clipboard-copy-content="Counting from 0 (in computer terms), the first visible display line is 17 and the last visible line is 240, and with the Y border shrink it is 224.

_EHSYNC continues its pulses on every line, even when _EVSYNC is low
Positive edge of _EHSYNC indicates start of the line
Negative edge of _EVSYNC indicates start of the frame

Positive edge of VBLANK indicates the bottom of the visible screen edge
Note: EXTWANTIRQ = _VBLANK which triggers on the negative edge at raster $f0

RV = vertical raster line number
	$f8-$ff	lo _EVSYNC
	$00-$ff	hi _EVSYNC
		$10		lo VBLANK
		$e0		lo FINALENABLEPIXELS with bordery flag
		$f0		hi VBLANK
RH = horizontal pixel clock
	Full raster line starts at $0 to $17e
		In reality, RH $0 occurs for two clocks, each subsequent pixel advances with one clock, up to a maximum of $17e
	Giving 384 pixel clocks per line
	Maximum visible portion (when _EHSYNC = 1) is $000 to $15f
	_EHSYNC = 0 (HBLANK portion of the display where nothing is visible) is $160 to $17e (plus the first half of the next $000)

SREAD = Sprite register address (lo byte, also maps to ISAB[0..6] in the schematic)

Line RH starts at $000
$000	SREAD $00
		Begin sprite 0 register reads
		1V*. H
		lo to hi _EHSYNC (in second clock for $000)
$002	SREAD $01
$004	SREAD $02
$006	SREAD $03
$008 	SREAD $00 to $03 again
$008H	OV0/1/2 LHH OC0/1/2/3 HLHL from M0-M7
$008L	All L no data from $008H clocked into pixel output latch at 5E2
$009H	Pixel data is latched into 5E2
$009H	Begin sprite 0 pixel writes
		U77 First pixel written
$00aH	U77 Second pixel written
$00a	Pixels start arriving at the real video output, resistor ladders
		Pixel also cleared to $ff in U78
$010	SREAD $04 same 4x2 repeating pattern every 2 pixels
		Begin sprite 1 register reads
		Meaning 16 pixels elapse for each sprite
$012	lo to hi FINALENABLEPIXELS (with overscan $29)
$0c0	SREAD $30 same 4x2 pattern as above
$152	hi to lo FINALENABLEPIXELS (with overscan $29)
$160	lo _EHSYNC
$17e	hi _EHSYNC (into first half of $000)
$17e	SREAD $7f
Loops back to $000 again and RV increment


The file &quot;BombJack\RecordingRHSprAddr.txt&quot; contains recorded address and RH values from Root Sheet 4 device VSMDD5.
This clearly shows (sparse extracts)
	Recording		Index	Address	@RH
	d$00000000		0		$00		$000
	d$00100040		4		$10		$040
	d$00200080		8		$20		$080
	d$003000c0		12		$30		$0c0
	d$00400100		16		$40		$100
	d$00500140		20		$50		$140"><pre><code>Counting from 0 (in computer terms), the first visible display line is 17 and the last visible line is 240, and with the Y border shrink it is 224.

_EHSYNC continues its pulses on every line, even when _EVSYNC is low
Positive edge of _EHSYNC indicates start of the line
Negative edge of _EVSYNC indicates start of the frame

Positive edge of VBLANK indicates the bottom of the visible screen edge
Note: EXTWANTIRQ = _VBLANK which triggers on the negative edge at raster $f0

RV = vertical raster line number
	$f8-$ff	lo _EVSYNC
	$00-$ff	hi _EVSYNC
		$10		lo VBLANK
		$e0		lo FINALENABLEPIXELS with bordery flag
		$f0		hi VBLANK
RH = horizontal pixel clock
	Full raster line starts at $0 to $17e
		In reality, RH $0 occurs for two clocks, each subsequent pixel advances with one clock, up to a maximum of $17e
	Giving 384 pixel clocks per line
	Maximum visible portion (when _EHSYNC = 1) is $000 to $15f
	_EHSYNC = 0 (HBLANK portion of the display where nothing is visible) is $160 to $17e (plus the first half of the next $000)

SREAD = Sprite register address (lo byte, also maps to ISAB[0..6] in the schematic)

Line RH starts at $000
$000	SREAD $00
		Begin sprite 0 register reads
		1V*. H
		lo to hi _EHSYNC (in second clock for $000)
$002	SREAD $01
$004	SREAD $02
$006	SREAD $03
$008 	SREAD $00 to $03 again
$008H	OV0/1/2 LHH OC0/1/2/3 HLHL from M0-M7
$008L	All L no data from $008H clocked into pixel output latch at 5E2
$009H	Pixel data is latched into 5E2
$009H	Begin sprite 0 pixel writes
		U77 First pixel written
$00aH	U77 Second pixel written
$00a	Pixels start arriving at the real video output, resistor ladders
		Pixel also cleared to $ff in U78
$010	SREAD $04 same 4x2 repeating pattern every 2 pixels
		Begin sprite 1 register reads
		Meaning 16 pixels elapse for each sprite
$012	lo to hi FINALENABLEPIXELS (with overscan $29)
$0c0	SREAD $30 same 4x2 pattern as above
$152	hi to lo FINALENABLEPIXELS (with overscan $29)
$160	lo _EHSYNC
$17e	hi _EHSYNC (into first half of $000)
$17e	SREAD $7f
Loops back to $000 again and RV increment


The file "BombJack\RecordingRHSprAddr.txt" contains recorded address and RH values from Root Sheet 4 device VSMDD5.
This clearly shows (sparse extracts)
	Recording		Index	Address	@RH
	d$00000000		0		$00		$000
	d$00100040		4		$10		$040
	d$00200080		8		$20		$080
	d$003000c0		12		$30		$0c0
	d$00400100		16		$40		$100
	d$00500140		20		$50		$140
</code></pre></div>
<p dir="auto"><h4 tabindex="-1" dir="auto">Sprite scan RAM logic</h4><a id="user-content-sprite-scan-ram-logic" aria-label="Permalink: Sprite scan RAM logic" href="#sprite-scan-ram-logic"></a></p>
<div data-snippet-clipboard-copy-content="A sprite in 16x16 mode, with a Y register value of 224, which uses internal inverted logic, will have a calculated sprite Y coordinate of 256 - 224 = 32, this will be the last calculated scan line of the sprite. The result of the scan line calculation will be rendered and visible on the next scan line 33.
15 scan lines above calculated scan line 32 is the first calculated scan line of the sprite, in this example scan line 17, and the next scan line 18 is the first visible scan line of the sprite.


Assume the sprite scan line RAM has been cleared by a previous scan rendering all the pixels and clearing the values of $ff.
	Old revision: The weak pull-up resisters connected to the RAM output help to clear the RAM values to $ff
		These are overidden by any pull down logic 0 from the RAM or the input 74258 demultiplexors
	New revision: Uses digital logic to switch between $ff and input values as needed. This is much more stable than using pull-up resistors which needed to be tuned for the actual TTL chips used.
For each sprite the index, colour, X and Y pos are read
	After a successful Y pos test with the vertical raster position...
		The sprite data counter is set with the X position of the sprite, then for 16 (or 32) pixels
		The data in the scan line RAM is read on _6MHz = 0
		On +ve edge _6MHz this read value is latched by 2C/2D 2B/2A, and inverted for output
		The value is then tested for transparency with 1C:B or 1C:C 3-input NOR gates
		If it is transparent, the value from the input pixel from the current sprite is output by 3C/3D 3B/3A by the 74258 demultiplexors, which inverts the information. This could be a transparent pixel with its colour, or an opaque pixel with its colour.
		If it is not transparent, an opaque sprite pixel exists for this position, then the value just read from the RAM is selected by the 74258 demultiplexors, inverted and written back to the RAM
		In effect, the first opaque (non-transparent) pixel from the sprite written to the scan line RAM has priority and the rest of the pixels from sprites are ignored


Timing enabling or disabling the sprite display and updating sprite registers allows multiplexing. See: kBus24Bit_VideoLayer_XPos_SpriteIndex_*
The sprite scan line schedule, which renders each sprite in 16 pixel chunks, can be syncronised to the video pixel clock, or overclocked.
When syncronised to the video pixel clock each 16x16 sprite takes 16 pixels to process. If the sprite clock is overclocked, the scan schedule will correspondingly contract by the ratio of video clock to sprite clock. In other words each sprite will take fewer than 16 pixels to process.
Overclocking the sprite pixel clock will alter the timing for sprite multiplexing: kBus24Bit_VideoLayer_XPos_SpriteIndex_*


Remember, the scanline before a sprite is visible/displayed on the screen is the actual scanline used for rendering the sprite data.
	The x position of the sprite does not affect its rendering timing in the scanline, only the sprite register address used affects its rendering schedule time, see: Raster line schedule"><pre><code>A sprite in 16x16 mode, with a Y register value of 224, which uses internal inverted logic, will have a calculated sprite Y coordinate of 256 - 224 = 32, this will be the last calculated scan line of the sprite. The result of the scan line calculation will be rendered and visible on the next scan line 33.
15 scan lines above calculated scan line 32 is the first calculated scan line of the sprite, in this example scan line 17, and the next scan line 18 is the first visible scan line of the sprite.


Assume the sprite scan line RAM has been cleared by a previous scan rendering all the pixels and clearing the values of $ff.
	Old revision: The weak pull-up resisters connected to the RAM output help to clear the RAM values to $ff
		These are overidden by any pull down logic 0 from the RAM or the input 74258 demultiplexors
	New revision: Uses digital logic to switch between $ff and input values as needed. This is much more stable than using pull-up resistors which needed to be tuned for the actual TTL chips used.
For each sprite the index, colour, X and Y pos are read
	After a successful Y pos test with the vertical raster position...
		The sprite data counter is set with the X position of the sprite, then for 16 (or 32) pixels
		The data in the scan line RAM is read on _6MHz = 0
		On +ve edge _6MHz this read value is latched by 2C/2D 2B/2A, and inverted for output
		The value is then tested for transparency with 1C:B or 1C:C 3-input NOR gates
		If it is transparent, the value from the input pixel from the current sprite is output by 3C/3D 3B/3A by the 74258 demultiplexors, which inverts the information. This could be a transparent pixel with its colour, or an opaque pixel with its colour.
		If it is not transparent, an opaque sprite pixel exists for this position, then the value just read from the RAM is selected by the 74258 demultiplexors, inverted and written back to the RAM
		In effect, the first opaque (non-transparent) pixel from the sprite written to the scan line RAM has priority and the rest of the pixels from sprites are ignored


Timing enabling or disabling the sprite display and updating sprite registers allows multiplexing. See: kBus24Bit_VideoLayer_XPos_SpriteIndex_*
The sprite scan line schedule, which renders each sprite in 16 pixel chunks, can be syncronised to the video pixel clock, or overclocked.
When syncronised to the video pixel clock each 16x16 sprite takes 16 pixels to process. If the sprite clock is overclocked, the scan schedule will correspondingly contract by the ratio of video clock to sprite clock. In other words each sprite will take fewer than 16 pixels to process.
Overclocking the sprite pixel clock will alter the timing for sprite multiplexing: kBus24Bit_VideoLayer_XPos_SpriteIndex_*


Remember, the scanline before a sprite is visible/displayed on the screen is the actual scanline used for rendering the sprite data.
	The x position of the sprite does not affect its rendering timing in the scanline, only the sprite register address used affects its rendering schedule time, see: Raster line schedule
</code></pre></div>
<p dir="auto"><h3 tabindex="-1" dir="auto">APU - What is it</h3><a id="user-content-apu---what-is-it" aria-label="Permalink: APU - What is it" href="#apu---what-is-it"></a></p>
<p dir="auto">The Advanced Processing Unit is an optimised raster and data transfer processor that can execute several instructions at the same time.
It is able to precisely wait for raster H&amp;V positions, transfer data, and perform simple comparisons based on internal data with conditional executions based on the results of the comparison.
This significantly reduces load on the CPU and the amount of memory the CPU needs, allowing the CPU to transfer data when it can.</p>
<p dir="auto">There is significant logic internally that allows data to be processed: <a href="https://docs.google.com/presentation/d/1Tb4B5-INhH2oiOSG4jfb3BfGGtdT9jPvyWoryOIJNzo/edit?usp=sharing" rel="nofollow">https://docs.google.com/presentation/d/1Tb4B5-INhH2oiOSG4jfb3BfGGtdT9jPvyWoryOIJNzo/edit?usp=sharing</a></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">PCB Layout</h3><a id="user-content-pcb-layout" aria-label="Permalink: PCB Layout" href="#pcb-layout"></a></p>
<p dir="auto">There is a PCB layout design included in the Proteus workspace file. It is deliberately and mostly blank with the exception of adding placement for the external address/data/RW lines, clock, external VBLANK (for IRQ triggering) and video signals output header and the associated resistor ladders. The dual sprite bit plane pull-up resistors are also placed near the edge of the sprites board. The sprite resistors and RAMs are placed to maintain an aesthetic symmetry to the board.</p>
<p dir="auto">There are six boards in the layout:
Video generation
Character screen
Sprites
Scaled sprites
Tiles
Mode7</p>
<p dir="auto">Previous versions used one large board, but this turned out to be expensive to produce and difficult to get quotations for. Apparently the companies I contacted were not that happy to produce large boards in small numbers. The design was changed to use four smaller boards, this reduces cost if someone wanted to just fabricate the video and character screen. Another benefit is that the layers can be changed and connected in different orders. This allows customisations, for example by having three sprites boards (layers 1 to 3) and one tile board (layer 4), giving a total of 72 sprites and tiles.</p>
<p dir="auto">The layout is configured with a six layer standard signal and power plane setup, using the technology template "All PCB Six Layer 1.58532mm.LTF" file. The layout will correctly populate with the auto-placer and default placement options, choose all then schedule before starting. It will then correctly route, again using the default router options. This is so placement can be mostly left to the optimiser while maintaining output video signal lines via the resistor placement.</p>
<p dir="auto">An entire place and route on a decent machine will take approximately 2 hours and result in an output image like:
<a target="_blank" rel="noopener noreferrer" href="https://github.com/martinpiper/BombJack/blob/master/Layout1.PNG?raw=true"><img src="https://github.com/martinpiper/BombJack/raw/master/Layout1.PNG?raw=true" alt="Layout" title="Layout"></a>
<a target="_blank" rel="noopener noreferrer" href="https://github.com/martinpiper/BombJack/blob/master/Layout2.PNG?raw=true"><img src="https://github.com/martinpiper/BombJack/raw/master/Layout2.PNG?raw=true" alt="Tracks" title="Tracks"></a></p>
<p dir="auto">The protoype boards have been built and documented here: <a href="https://www.youtube.com/playlist?list=PLAAYJEX1JbCPFzQDl8KOoVQR9_Us1lB46" rel="nofollow">https://www.youtube.com/playlist?list=PLAAYJEX1JbCPFzQDl8KOoVQR9_Us1lB46</a> or this project page: <a href="https://martin-piper.itch.io/bomb-jack-display-hardware" rel="nofollow">https://martin-piper.itch.io/bomb-jack-display-hardware</a></p>
<p dir="auto">I use Proteus simulation and PCB design: <a href="https://www.labcenter.com/" rel="nofollow">https://www.labcenter.com/</a>
And PCBWay PCB manufacture: <a href="https://pcbway.com/g/Xr8ZrI" rel="nofollow">https://pcbway.com/g/Xr8ZrI</a>
( Using the PCBWay referral link gives you a discount and also helps me reduce the cost of my next board. )</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">References</h2><a id="user-content-references" aria-label="Permalink: References" href="#references"></a></p>
<ul dir="auto">
<li><a href="https://github.com/mamedev/mame/blob/master/src/mame/drivers/bombjack.cpp">https://github.com/mamedev/mame/blob/master/src/mame/drivers/bombjack.cpp</a></li>
<li><a href="https://floooh.github.io/2018/10/06/bombjack.html" rel="nofollow">https://floooh.github.io/2018/10/06/bombjack.html</a></li>
<li><a href="https://www.arcade-museum.com/game_detail.php?game_id=7180" rel="nofollow">https://www.arcade-museum.com/game_detail.php?game_id=7180</a></li>
</ul>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Tesla owners file class-action alleging repair, parts monopoly (108 pts)]]></title>
            <link>https://driving.ca/auto-news/industry/tesla-class-action-lawsuite-parts-repairs-monopoly</link>
            <guid>40744847</guid>
            <pubDate>Fri, 21 Jun 2024 00:06:21 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://driving.ca/auto-news/industry/tesla-class-action-lawsuite-parts-repairs-monopoly">https://driving.ca/auto-news/industry/tesla-class-action-lawsuite-parts-repairs-monopoly</a>, See on <a href="https://news.ycombinator.com/item?id=40744847">Hacker News</a></p>
Couldn't get https://driving.ca/auto-news/industry/tesla-class-action-lawsuite-parts-repairs-monopoly: Error: Request failed with status code 403]]></description>
        </item>
        <item>
            <title><![CDATA[How babies and young children learn to understand language (204 pts)]]></title>
            <link>https://lithub.com/how-babies-and-young-children-learn-to-understand-language/</link>
            <guid>40744558</guid>
            <pubDate>Thu, 20 Jun 2024 23:19:40 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://lithub.com/how-babies-and-young-children-learn-to-understand-language/">https://lithub.com/how-babies-and-young-children-learn-to-understand-language/</a>, See on <a href="https://news.ycombinator.com/item?id=40744558">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
							        
									<p><span itemprop="articleBody"><p>How children learn language has long been of interest to those concerned with its evolution. The idea that ‘ontogeny recapitulates phylogeny’ has been promoted, which means the stages of child development on their way to adulthood replicate those of our human ancestors on their way to becoming modern humans. This idea has been applied to language acquisition and its evolution, but I’ve never been persuaded. It is intellectually problematic because our human ancestors were never ‘on their way’ to anywhere other than being themselves. My interest in language acquisition is different and twofold.</p><p><span>Article continues below</span></p>
<p>First, is language acquired by specialized mental processes that are dedicated to this task or learned by general-purpose processes used for a variety of learning tasks? Second, can we project the processes of language acquisition/learning that we observe in the present into the prehistoric past to gain insights about the evolution of language?</p>
<span>Infants a mere three days old prefer listening to the rhythm of their caregivers’ language.</span>
<p><strong>Learning words</strong></p>
<p>Let’s remind ourselves of the problem that language learners face. When we write, we leave spaces between words. Readingwordswithoutsuchspacesisdifficult. When we speak, however, our words flow from one to another without any pauses, generating a continuous stream of sound. Yet we hear and understand each word without difficulty—we automatically ‘parse’ the utterances we hear into their component parts. We can do this because we know not only the words and grammar of our language, but also its prosody—how syllables at the start or end of words are stressed and how pauses are used between phrases. Without any awareness of such knowledge, we use it effortlessly both when speaking and when listening; we use it to identify words as they appear within the continuous stream of sound we hear, just as if the words had been written down interspersed with spaces. One can appreciate this with a little reflection. Just think how you pronounce the syllable <em>ham </em>when referring to a piece of meat and when talking about a fury animal—a long <em>ham </em>and a short <em>ham</em>-ster. Think about listening to a language unknown to you, one with different words, grammar and prosody. You will be at an utter loss to identify its words, let alone their meaning.</p>
<p>How then, can babies learn language? Although they are born into the world without any knowledge of the language their caregivers will speak, some learning takes places within the womb. The reception of external speech sounds prepares the fetal brain for the processing of language once born. Languages have different rhythms and infants a mere three days old prefer listening to the rhythm of their caregivers’ language, indicating that they became familiarized to it before birth via the sounds heard or the vibrations felt within the womb.</p>
<p>Nevertheless, babies are born with no specific knowledge of the language they will hear. Yet whether that is English, Mandarin, Swahili or any other of the 7,000 or so languages that remain today, by around one year of age infants are saying their first words, and within another year they are stringing words together to make simple sentences by using the correct order for their native language—those acquiring/learning English and French will place verbs before objects, and those learning Turkish and Japanese will do the converse. Without any formal teaching, by a mere four years of age, most children have acquired more than 1,000 words, the rules for how words can be combined to generate meaningful utterances, and how to change the tone of an utterance to further flex its meaning.</p>
<p>Such language acquisition is truly remarkable. How it is achieved has been one of the major questions facing linguists ever since the study of language began, generating a constant flow of new ideas and evidence. We have already dismissed arguments about Universal Grammar in Chapter 3 and can now move on to more interesting and viable ideas. Within the voluminous literature there are two questions of particular interest for the language puzzle. The first is how infants find discrete words in the continuous stream of speech they hear. The second is how they learn the meaning of those words.</p>
<span>By a mere four years of age, most children have acquired more than 1,000 words.</span>
<p><strong>Finding words</strong></p>
<p>How can babies possibly discover where a word begins and ends within a continuous sound stream? A breakthrough in our understanding came in 1996 in a three-page article in the journal <em>Science </em>entitled “Statistical learning by 8-month-old infants,” authored by the psychologists Jenny Saffran, Richard Aslin and Elissa Newport. This explained that infants use “transitional probabilities” (TPs) between syllables to identify which syllable strings recurrently go together, and hence are likely to constitute words, and which syllables have low probabilities of following each other and hence are likely to mark the break between words. The phrase <em>pretty baby, </em>for instance, has four syllables (<em>pre-ty-ba-by</em>) and three transitional probabilities between syllable pairs. In English the probability that <em>ty </em>will be followed by <em>ba </em>is lower than <em>pre </em>will be followed by <em>ty, </em>and that <em>ba </em>will be followed by <em>by</em>. That eight-month-old-infants can calculate and use such transitional probabilities came as a surprise.</p>
<p>Jenny Saffran and her colleagues had exposed infants to no more than two minutes of continuous speech that contained four three-syllable nonsense words, such as <em>tupiro </em>and <em>padoti</em>. These ‘words’ were repeated in random order by a monotone speech synthesizer that created a continuous sound sequence, such as <em>bidakupadotigolabubidakupadotigolabubidakutupiro… </em>The sequence contained no pauses, variations in stress or any other acoustic cues between word boundaries. The only cue available to the infants were the transitional probabilities (TPs) between syllables. Those within words were 1.0, because the first syllable was always followed by the second, and the second by the third, while the TPs of syllables between words was always 0.33. After a mere two minutes of listening, the infants were tested as to whether they differentiated between words (syllable strings that had TPs of 1.0) and non-words (syllable strings that contained TPs of 0.33). For this the infants were presented with words and non-words and found to have longer listening time for the nonwords. This indicated they had already become familiar with the words by listening to the continuous sequence of syllables within which they had been embedded. The only way that could have happened was by monitoring the TPs between syllables—the infants were capable of statistical learning.</p>
<p>______________________________</p>
<p><img fetchpriority="high" decoding="async" data-attachment-id="238453" data-permalink="https://lithub.com/how-babies-and-young-children-learn-to-understand-language/the-language-puzzle/" data-orig-file="https://s26162.pcdn.co/wp-content/uploads/2024/06/the-language-puzzle.jpeg" data-orig-size="660,1024" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="the language puzzle" data-image-description="" data-image-caption="" data-medium-file="https://s26162.pcdn.co/wp-content/uploads/2024/06/the-language-puzzle-193x300.jpeg" data-large-file="https://s26162.pcdn.co/wp-content/uploads/2024/06/the-language-puzzle.jpeg" src="https://s26162.pcdn.co/wp-content/uploads/2024/06/the-language-puzzle-193x300.jpeg" alt="" width="193" height="300" srcset="https://s26162.pcdn.co/wp-content/uploads/2024/06/the-language-puzzle-193x300.jpeg 193w, https://s26162.pcdn.co/wp-content/uploads/2024/06/the-language-puzzle-39x60.jpeg 39w, https://s26162.pcdn.co/wp-content/uploads/2024/06/the-language-puzzle-32x50.jpeg 32w, https://s26162.pcdn.co/wp-content/uploads/2024/06/the-language-puzzle.jpeg 660w" sizes="(max-width: 193px) 100vw, 193px"></p>
<p><em>From</em> <a href="https://bookshop.org/a/132/9781541605381" target="_blank">The Language Puzzle: Piecing Together the Six-Million-Year Story of How Words Evolved</a> <em>by Steven Mithen. Copyright © 2024. Available from Basic Books, a division of Hachette Book Group, Inc.</em></p>
</span></p>
									
																		
																		
									<br><hr>
									
							    										
								</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Show HN: Local voice assistant using Ollama, transformers and Coqui TTS toolkit (128 pts)]]></title>
            <link>https://github.com/mezbaul-h/june</link>
            <guid>40744293</guid>
            <pubDate>Thu, 20 Jun 2024 22:48:40 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/mezbaul-h/june">https://github.com/mezbaul-h/june</a>, See on <a href="https://news.ycombinator.com/item?id=40744293">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><p dir="auto"><h2 tabindex="-1" dir="auto">june-va</h2><a id="user-content-june-va" aria-label="Permalink: june-va" href="#june-va"></a></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Local Voice Chatbot: Ollama + HF Transformers + Coqui TTS Toolkit</h2><a id="user-content-local-voice-chatbot-ollama--hf-transformers--coqui-tts-toolkit" aria-label="Permalink: Local Voice Chatbot: Ollama + HF Transformers + Coqui TTS Toolkit" href="#local-voice-chatbot-ollama--hf-transformers--coqui-tts-toolkit"></a></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">OVERVIEW</h2><a id="user-content-overview" aria-label="Permalink: OVERVIEW" href="#overview"></a></p>
<p dir="auto"><strong>june-va</strong> is a local voice chatbot that combines the power of Ollama (for language model capabilities), Hugging Face Transformers (for speech recognition), and the Coqui TTS Toolkit (for text-to-speech synthesis). It provides a flexible, privacy-focused solution for voice-assisted interactions on your local machine, ensuring that no data is sent to external servers.</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/mezbaul-h/june/blob/master/demo.gif"><img src="https://github.com/mezbaul-h/june/raw/master/demo.gif" alt="demo-text-only-interaction" data-animated-image=""></a></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Interaction Modes</h3><a id="user-content-interaction-modes" aria-label="Permalink: Interaction Modes" href="#interaction-modes"></a></p>
<ul dir="auto">
<li><strong>Text Input/Output:</strong> Provide text inputs to the assistant and receive text responses.</li>
<li><strong>Voice Input/Text Output:</strong> Use your microphone to give voice inputs, and receive text responses from the assistant.</li>
<li><strong>Text Input/Audio Output:</strong> Provide text inputs and receive both text and synthesised audio responses from the assistant.</li>
<li><strong>Voice Input/Audio Output (Default):</strong> Use your microphone for voice inputs, and receive responses in both text and synthesised audio form.</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">INSTALLATION</h2><a id="user-content-installation" aria-label="Permalink: INSTALLATION" href="#installation"></a></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Pre-requisites</h3><a id="user-content-pre-requisites" aria-label="Permalink: Pre-requisites" href="#pre-requisites"></a></p>
<ul dir="auto">
<li><a href="https://github.com/ollama/ollama">Ollama</a></li>
<li>Python <code>3.10+</code> (with <em>pip</em>)</li>
</ul>
<p dir="auto">You will also need the following native package installed on your machine:</p>
<div dir="auto" data-snippet-clipboard-copy-content="apt install portaudio19-dev  # requirement for PyAudio"><pre>apt install portaudio19-dev  <span><span>#</span> requirement for PyAudio</span></pre></div>
<p dir="auto"><h3 tabindex="-1" dir="auto">From Source</h3><a id="user-content-from-source" aria-label="Permalink: From Source" href="#from-source"></a></p>
<p dir="auto">To install directly from source:</p>
<div dir="auto" data-snippet-clipboard-copy-content="git clone https://github.com/mezbaul-h/june.git
cd june
pip install ."><pre>git clone https://github.com/mezbaul-h/june.git
<span>cd</span> june
pip install <span>.</span></pre></div>
<p dir="auto"><h2 tabindex="-1" dir="auto">USAGE</h2><a id="user-content-usage" aria-label="Permalink: USAGE" href="#usage"></a></p>
<p dir="auto">Pull the language model (default is <code>llama3:8b-instruct-q4_0</code>) with Ollama first, if you haven't already:</p>
<div dir="auto" data-snippet-clipboard-copy-content="ollama pull llama3:8b-instruct-q4_0"><pre>ollama pull llama3:8b-instruct-q4_0</pre></div>
<p dir="auto">Next, run the program (with default configuration):</p>

<p dir="auto">This will use <a href="https://ollama.com/library/llama3:8b-instruct-q4_0" rel="nofollow">llama3:8b-instruct-q4_0</a> for LLM capabilities, <a href="https://huggingface.co/openai/whisper-small.en" rel="nofollow">openai/whisper-small.en</a> for speech recognition, and <code>tts_models/en/ljspeech/glow-tts</code> for audio synthesis.</p>
<p dir="auto">You can also customize behaviour of the program with a json configuration file:</p>
<div dir="auto" data-snippet-clipboard-copy-content="june-va --config path/to/config.json"><pre>june-va --config path/to/config.json</pre></div>
<p dir="auto"><g-emoji alias="warning">⚠️</g-emoji> The configuration file is optional. To learn more about the structure of the config file, see the <a href="#configuration">Configuration</a> section.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto"><g-emoji alias="warning">⚠️</g-emoji> Regarding Voice Input</h3><a id="user-content-️-regarding-voice-input" aria-label="Permalink: ⚠️ Regarding Voice Input" href="#️-regarding-voice-input"></a></p>
<p dir="auto">After seeing the <code>Listening for sound...</code> message, you can speak directly into the microphone. Unlike typical voice assistants, there's no wake command required. Simply start speaking, and the tool will automatically detect and process your voice input. Once you finish speaking, maintain silence for 3 seconds to allow the assistant to process your voice input.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Voice Conversion</h3><a id="user-content-voice-conversion" aria-label="Permalink: Voice Conversion" href="#voice-conversion"></a></p>
<p dir="auto">Many of the models (e.g., <code>tts_models/multilingual/multi-dataset/xtts_v2</code>) supported by Coqui's TTS Toolkit support voice cloning. You can use your own speaker profile with a small audio clip (approximately 1 minute for most models). Once you have the clip, you can instruct the assistant to use it with a custom configuration like the following:</p>
<div dir="auto" data-snippet-clipboard-copy-content="{
  &quot;tts&quot;: {
    &quot;model&quot;: &quot;tts_models/multilingual/multi-dataset/xtts_v2&quot;,
    &quot;generation_args&quot;: {
      &quot;language&quot;: &quot;en&quot;,
      &quot;speaker_wav&quot;: &quot;/path/to/your/target/voice.wav&quot;
    }
  }
}"><pre>{
  <span>"tts"</span>: {
    <span>"model"</span>: <span><span>"</span>tts_models/multilingual/multi-dataset/xtts_v2<span>"</span></span>,
    <span>"generation_args"</span>: {
      <span>"language"</span>: <span><span>"</span>en<span>"</span></span>,
      <span>"speaker_wav"</span>: <span><span>"</span>/path/to/your/target/voice.wav<span>"</span></span>
    }
  }
}</pre></div>
<p dir="auto"><h2 tabindex="-1" dir="auto">CONFIGURATION</h2><a id="user-content-configuration" aria-label="Permalink: CONFIGURATION" href="#configuration"></a></p>
<p dir="auto">The application can be customised using a configuration file. The config file must be a JSON file. The default configuration is as follows:</p>
<div dir="auto" data-snippet-clipboard-copy-content="{
    &quot;llm&quot;: {
        &quot;disable_chat_history&quot;: false,
        &quot;model&quot;: &quot;llama3:8b-instruct-q4_0&quot;
    },
    &quot;stt&quot;: {
        &quot;device&quot;: &quot;torch device identifier (`cuda` if available; otherwise `cpu`&quot;,
        &quot;generation_args&quot;: {
            &quot;batch_size&quot;: 8
        },
        &quot;model&quot;: &quot;openai/whisper-small.en&quot;
    },
    &quot;tts&quot;: {
        &quot;device&quot;: &quot;torch device identifier (`cuda` if available; otherwise `cpu`&quot;,
        &quot;model&quot;: &quot;tts_models/en/ljspeech/glow-tts&quot;
    }
}"><pre>{
    <span>"llm"</span>: {
        <span>"disable_chat_history"</span>: <span>false</span>,
        <span>"model"</span>: <span><span>"</span>llama3:8b-instruct-q4_0<span>"</span></span>
    },
    <span>"stt"</span>: {
        <span>"device"</span>: <span><span>"</span>torch device identifier (`cuda` if available; otherwise `cpu`<span>"</span></span>,
        <span>"generation_args"</span>: {
            <span>"batch_size"</span>: <span>8</span>
        },
        <span>"model"</span>: <span><span>"</span>openai/whisper-small.en<span>"</span></span>
    },
    <span>"tts"</span>: {
        <span>"device"</span>: <span><span>"</span>torch device identifier (`cuda` if available; otherwise `cpu`<span>"</span></span>,
        <span>"model"</span>: <span><span>"</span>tts_models/en/ljspeech/glow-tts<span>"</span></span>
    }
}</pre></div>
<p dir="auto">When you use a configuration file, it overrides the default configuration but does not overwrite it. So you can partially modify the configuration if you desire. For instance, if you do not wish to use speech recognition and only want to provide prompts through text, you can disable that by using a config file with the following configuration:</p>

<p dir="auto">Similarly, you can disable the audio synthesiser, or both, to only use the virtual assistant in text mode.</p>
<p dir="auto">If you only want to modify the device on which you want to load a particular type of model, without changing the other default attributes of the model, you could use:</p>
<div dir="auto" data-snippet-clipboard-copy-content="{
  &quot;tts&quot;: {
    &quot;device&quot;: &quot;cpu&quot;
  }
}"><pre>{
  <span>"tts"</span>: {
    <span>"device"</span>: <span><span>"</span>cpu<span>"</span></span>
  }
}</pre></div>
<p dir="auto"><h3 tabindex="-1" dir="auto">Configuration Attributes</h3><a id="user-content-configuration-attributes" aria-label="Permalink: Configuration Attributes" href="#configuration-attributes"></a></p>
<p dir="auto"><h4 tabindex="-1" dir="auto"><code>llm</code> - Language Model Configuration</h4><a id="user-content-llm---language-model-configuration" aria-label="Permalink: llm - Language Model Configuration" href="#llm---language-model-configuration"></a></p>
<ul dir="auto">
<li><code>llm.device</code>: Torch device identifier (e.g., <code>cpu</code>, <code>cuda</code>, <code>mps</code>) on which the pipeline will be allocated.</li>
<li><code>llm.disable_chat_history</code>: Boolean indicating whether to disable or enable chat history. Enabling chat history will make interactions more dynamic, as the model will have access to previous contexts, but it will consume more processing power. Disabling it will result in less interactive conversations but will use fewer processing resources.</li>
<li><code>llm.model</code>: Name of the text-generation model tag on Ollama. Ensure this is a valid model tag that exists on your machine.</li>
<li><code>llm.system_prompt</code>: Give a system prompt to the model. If the underlying model does not support a system prompt, an error will be raised.</li>
</ul>
<p dir="auto"><h4 tabindex="-1" dir="auto"><code>stt</code> - Speech-to-Text Model Configuration</h4><a id="user-content-stt---speech-to-text-model-configuration" aria-label="Permalink: stt - Speech-to-Text Model Configuration" href="#stt---speech-to-text-model-configuration"></a></p>
<ul dir="auto">
<li><code>tts.device</code>: Torch device identifier (e.g., <code>cpu</code>, <code>cuda</code>, <code>mps</code>) on which the pipeline will be allocated.</li>
<li><code>stt.generation_args</code>: Object containing generation arguments accepted by Hugging Face's speech recognition pipeline.</li>
<li><code>stt.model</code>: Name of the speech recognition model on Hugging Face. Ensure this is a valid model ID that exists on Hugging Face.</li>
</ul>
<p dir="auto"><h4 tabindex="-1" dir="auto"><code>tts</code> - Text-to-Speech Model Configuration</h4><a id="user-content-tts---text-to-speech-model-configuration" aria-label="Permalink: tts - Text-to-Speech Model Configuration" href="#tts---text-to-speech-model-configuration"></a></p>
<ul dir="auto">
<li><code>tts.device</code>: Torch device identifier (e.g., <code>cpu</code>, <code>cuda</code>, <code>mps</code>) on which the pipeline will be allocated.</li>
<li><code>tts.generation_args</code>: Object containing generation arguments accepted by Coqui's TTS API.</li>
<li><code>tts.model</code>: Name of the text-to-speech model supported by the Coqui's TTS Toolkit. Ensure this is a valid model ID.</li>
</ul>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Generating audio for video (118 pts)]]></title>
            <link>https://deepmind.google/discover/blog/generating-audio-for-video/</link>
            <guid>40744098</guid>
            <pubDate>Thu, 20 Jun 2024 22:23:12 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://deepmind.google/discover/blog/generating-audio-for-video/">https://deepmind.google/discover/blog/generating-audio-for-video/</a>, See on <a href="https://news.ycombinator.com/item?id=40744098">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
  <p data-block-key="s3ic2">Video-to-audio research uses video pixels and text prompts to generate rich soundtracks</p><p data-block-key="68q6v">Video generation models are advancing at an incredible pace, but many current systems can only generate silent output. One of the next major steps toward bringing generated movies to life is creating soundtracks for these silent videos.</p><p data-block-key="1h622">Today, we're sharing progress on our video-to-audio (V2A) technology, which makes synchronized audiovisual generation possible. V2A combines video pixels with natural language text prompts to generate rich soundscapes for the on-screen action.</p><p data-block-key="9c93b">Our V2A technology is pairable with video generation models like <a href="https://deepmind.google/technologies/veo/" rel="noopener" target="_blank">Veo</a> to create shots with a dramatic score, realistic sound effects or dialogue that matches the characters and tone of a video.</p><p data-block-key="dvlk8">It can also generate soundtracks for a range of traditional footage, including archival material, silent films and more — opening a wider range of creative opportunities.</p>
</div><div>
  <h2 data-block-key="s3ic2">Enhanced creative control</h2><p data-block-key="fdg2h">Importantly, V2A can generate an unlimited number of soundtracks for any video input. Optionally, a ‘positive prompt’ can be defined to guide the generated output toward desired sounds, or a ‘negative prompt’ to guide it away from undesired sounds.</p><p data-block-key="482jf">This flexibility gives users more control over V2A’s audio output, making it possible to rapidly experiment with different audio outputs and choose the best match.</p>
</div><div>
  <h2 data-block-key="s3ic2">How it works</h2><p data-block-key="bggqb">We experimented with autoregressive and diffusion approaches to discover the most scalable AI architecture, and the diffusion-based approach for audio generation gave the most realistic and compelling results for synchronizing video and audio information.</p><p data-block-key="6i6u8">Our V2A system starts by encoding video input into a compressed representation. Then, the diffusion model iteratively refines the audio from random noise. This process is guided by the visual input and natural language prompts given to generate synchronized, realistic audio that closely aligns with the prompt. Finally, the audio output is decoded, turned into an audio waveform and combined with the video data.</p>
</div><div>
  <p data-block-key="s3ic2">To generate higher quality audio and add the ability to guide the model towards generating specific sounds, we added more information to the training process, including AI-generated annotations with detailed descriptions of sound and transcripts of spoken dialogue.</p><p data-block-key="7gmmb">By training on video, audio and the additional annotations, our technology learns to associate specific audio events with various visual scenes, while responding to the information provided in the annotations or transcripts.</p><h2 data-block-key="84uq7">Further research underway</h2><p data-block-key="2mran">Our research stands out from existing video-to-audio solutions because it can understand raw pixels and adding a text prompt is optional.</p><p data-block-key="9ak0u">Also, the system doesn't need manual alignment of the generated sound with the video, which involves tediously adjusting different elements of sounds, visuals and timings.</p>
</div><div>
  <p data-block-key="g6mng">Still, there are a number of other limitations we’re trying to address and further research is underway.</p><p data-block-key="11s01">Since the quality of the audio output is dependent on the quality of the video input, artifacts or distortions in the video, which are outside the model’s training distribution, can lead to a noticeable drop in audio quality.</p><p data-block-key="1oe3q">We’re also improving lip synchronization for videos that involve speech. V2A attempts to generate speech from the input transcripts and synchronize it with characters' lip movements. But the paired video generation model may not be conditioned on transcripts. This creates a mismatch, often resulting in uncanny lip-syncing, as the video model doesn’t generate mouth movements that match the transcript.</p>
</div><div>
  <h2 data-block-key="s3ic2">Our commitment to safety and transparency</h2><p data-block-key="2vj1u">We’re committed to developing and deploying AI technologies responsibly. To make sure our V2A technology can have a positive impact on the creative community, we’re gathering diverse perspectives and insights from leading creators and filmmakers, and using this valuable feedback to inform our ongoing research and development.</p><p data-block-key="9oeuc">We’ve also incorporated our <a href="https://deepmind.google/technologies/synthid/" rel="noopener" target="_blank">SynthID</a> toolkit into our V2A research to watermark all AI-generated content to help safeguard against the potential for misuse of this technology.</p><p data-block-key="e025m">Before we consider opening access to it to the wider public, our V2A technology will undergo rigorous safety assessments and testing. Initial results are showing this technology will become a promising approach for bringing generated movies to life.</p><p data-block-key="d1spf"><i>Note: All examples are generated by our V2A technology, which is paired with</i> <a href="https://deepmind.google/technologies/veo/" rel="noopener" target="_blank"><i>Veo</i></a><i>, our most capable generative video model.</i></p>
</div><div>
      <p data-block-key="qb9a7">This work was made possible by the contributions of: Ankush Gupta, Nick Pezzotti, Pavel Khrushkov, Tobenna Peter Igwe, Kazuya Kawakami, Mateusz Malinowski, Jacob Kelly, Yan Wu, Xinyu Wang, Abhishek Sharma, Ali Razavi, Eric Lau, Serena Zhang, Brendan Shillingford, Yelin Kim, Eleni Shaw, Signe Nørly, Andeep Toor, Irina Blok, Gregory Shaw, Pen Li, Scott Wisdom, Aren Jansen, Zalán Borsos, Brian McWilliams, Salah Zaiem, Marco Tagliasacchi, Ron Weiss, Manoj Plakal, Hakan Erdogan, John Hershey, Jeff Donahue, Vivek Kumar, and Matt Sharifi.</p><p data-block-key="6fkbp">We extend our gratitude to Benigno Uria, Björn Winckler, Charlie Nash, Conor Durkan, Cătălina Cangea, David Ding, Dawid Górny, Drew Jaegle, Ethan Manilow, Evgeny Gladchenko, Felix Riedel, Florian Stimberg, Henna Nandwani, Jakob Bauer, Junlin Zhang, Luis C. Cobo, Mahyar Bordbar, Miaosen Wang, Mikołaj Bińkowski, Sander Dieleman, Will Grathwohl, Yaroslav Ganin, Yusuf Aytar, and Yury Sulsky.</p><p data-block-key="arkg9">Special thanks to Aäron van den Oord, Andrew Zisserman, Tom Hume, RJ Mical, Douglas Eck, Nando de Freitas, Oriol Vinyals, Eli Collins, Koray Kavukcuoglu and Demis Hassabis for their insightful guidance and support throughout the research process.</p><p data-block-key="bbn8a">We also acknowledge the many other individuals who contributed across Google DeepMind and our partners at Google.</p>
    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Gilead Shot Prevents All HIV Cases in Trial of African Women (792 pts)]]></title>
            <link>https://www.bloomberg.com/news/articles/2024-06-20/gilead-shot-prevents-100-of-hiv-cases-in-trial-of-african-women</link>
            <guid>40742163</guid>
            <pubDate>Thu, 20 Jun 2024 19:12:00 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.bloomberg.com/news/articles/2024-06-20/gilead-shot-prevents-100-of-hiv-cases-in-trial-of-african-women">https://www.bloomberg.com/news/articles/2024-06-20/gilead-shot-prevents-100-of-hiv-cases-in-trial-of-african-women</a>, See on <a href="https://news.ycombinator.com/item?id=40742163">Hacker News</a></p>
<div id="readability-page-1" class="page"><section>
    <section>
        <h3>Why did this happen?</h3>
        <p>Please make sure your browser supports JavaScript and cookies and that you are not
            blocking them from loading.
            For more information you can review our <a href="https://www.bloomberg.com/notices/tos">Terms of
                Service</a> and <a href="https://www.bloomberg.com/notices/tos">Cookie Policy</a>.</p>
    </section>
    <section>
        <h3>Need Help?</h3>
        <p>For inquiries related to this message please <a href="https://www.bloomberg.com/feedback">contact
            our support team</a> and provide the reference ID below.</p>
        <p>Block reference ID:</p>
    </section>
</section></div>]]></description>
        </item>
        <item>
            <title><![CDATA[More Disabled Americans Are Employed, Thanks to Remote Work (420 pts)]]></title>
            <link>https://www.bloomberg.com/news/articles/2024-06-20/remote-work-helps-more-people-with-disabilities-get-employed</link>
            <guid>40742026</guid>
            <pubDate>Thu, 20 Jun 2024 19:00:39 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.bloomberg.com/news/articles/2024-06-20/remote-work-helps-more-people-with-disabilities-get-employed">https://www.bloomberg.com/news/articles/2024-06-20/remote-work-helps-more-people-with-disabilities-get-employed</a>, See on <a href="https://news.ycombinator.com/item?id=40742026">Hacker News</a></p>
<div id="readability-page-1" class="page"><section>
    <section>
        <h3>Why did this happen?</h3>
        <p>Please make sure your browser supports JavaScript and cookies and that you are not
            blocking them from loading.
            For more information you can review our <a href="https://www.bloomberg.com/notices/tos">Terms of
                Service</a> and <a href="https://www.bloomberg.com/notices/tos">Cookie Policy</a>.</p>
    </section>
    <section>
        <h3>Need Help?</h3>
        <p>For inquiries related to this message please <a href="https://www.bloomberg.com/feedback">contact
            our support team</a> and provide the reference ID below.</p>
        <p>Block reference ID:</p>
    </section>
</section></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Fuzz Map (249 pts)]]></title>
            <link>https://www.fuzzmap.io/</link>
            <guid>40742014</guid>
            <pubDate>Thu, 20 Jun 2024 18:59:34 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.fuzzmap.io/">https://www.fuzzmap.io/</a>, See on <a href="https://news.ycombinator.com/item?id=40742014">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="welcome-modal-text">
          
          <p>
            <em>Fuzz Map</em> is a fuzzer for GUIs that automatically identifies states using code
            coverage and builds a visual map. Ideally, the map is useful even to people who'd prefer
            not to read code.
          </p>
          <p>
            Behind this window is a interactive local demo.
            Hide this introduction then click
            <span>
              <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 20 20" fill="currentColor">
                <title>fuzz</title>
                <path d="M9 9a2 2 0 114 0 2 2 0 01-4 0z"></path>
                <path fill-rule="evenodd" d="M10 18a8 8 0 100-16 8 8 0 000 16zm1-13a4 4 0 00-3.446 6.032l-2.261 2.26a1 1 0 101.414 1.415l2.261-2.261A4 4 0 1011 5z" clip-rule="evenodd"></path>
              </svg>
              Fuzz</span>
            to start fuzzing. To show this introduction again, click the
            <span>
              <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 20 20" fill="currentColor">
                <title>information</title>
                <path fill-rule="evenodd" d="M18 10a8 8 0 11-16 0 8 8 0 0116 0zm-7-4a1 1 0 11-2 0 1 1 0 012 0zM9 9a1 1 0 000 2v3a1 1 0 001 1h1a1 1 0 100-2v-3a1 1 0 00-1-1H9z" clip-rule="evenodd"></path>
              </svg>
              Information</span>
            button.
          </p>
          <p>
            Here's a 5-second video of real-time fuzzing on my laptop. Every state or arrow in the
            map corresponds to one or more codepaths discovered by the fuzzer:
          </p>
          
          <p>
            The <em>sandbox</em> renders the React code in the <em>editor</em> (specifically, it
            renders <code>&lt;App /&gt;</code>). Click
            <span>
              <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 20 20" fill="currentColor">
                <title>compile</title>
                <path fill-rule="evenodd" d="M10 18a8 8 0 100-16 8 8 0 000 16zM9.555 7.168A1 1 0 008 8v4a1 1 0 001.555.832l3-2a1 1 0 000-1.664l-3-2z" clip-rule="evenodd"></path>
              </svg>
              Compile</span>
            to update the sandbox, and click the
            <span>
              <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 20 20" fill="currentColor">
                <title>refresh</title>
                <path fill-rule="evenodd" d="M4 2a1 1 0 011 1v2.101a7.002 7.002 0 0111.601 2.566 1 1 0 11-1.885.666A5.002 5.002 0 005.999 7H9a1 1 0 010 2H4a1 1 0 01-1-1V3a1 1 0 011-1zm.008 9.057a1 1 0 011.276.61A5.002 5.002 0 0014.001 13H11a1 1 0 110-2h5a1 1 0 011 1v5a1 1 0 11-2 0v-2.101a7.002 7.002 0 01-11.601-2.566 1 1 0 01.61-1.276z" clip-rule="evenodd"></path>
              </svg>
              Reset</span>
            button to reset it. The map supports pan and zoom.
          </p>

          <!--
          <p>Keep reading for more information. But first:</p>

          <div id="call-to-action">
            <p>
              If you could use an industrial-strength version of Fuzz Map for your project, please
              send me an email at
              <a href="mailto:human@fuzzmap.io">human@fuzzmap.io</a>! I'm looking for pilot
              customers.
            </p>
            <p>
              In the short term, I'd like to make a plug-and-play version of Fuzz Map that supports
              end-to-end React fuzzing. You <em>won't</em> have to change your build process or use
              a special browser&mdash;just run <code>./fuzzmap localhost:8080 -p 9090</code> and
              start fuzzing or live programming! A reverse proxy will add instrumentation on the
              fly.
            </p>
            <p>
              Depending on feedback, a future version of Fuzz Map might also run in CI when a pull
              request is opened, or support sharing links to annotated maps.
            </p>
            <p>
              I'm also considering porting Fuzz Map to a platform other than React, e.g. iOS or Ruby
              on Rails. As I'll discuss later in the technical writeup, the instrumentation is
              mostly platform-independent.
            </p>
            <div>
              If you'd like to be notified when a beta release of Fuzz Map becomes generally
              available, you can use this form:
              <form name="waitlist" method="POST" data-netlify="true" netlify-honeypot="other">
                <label for="email">Email</label>
                <input type="email" id="email" name="email" required />
                <label for="role">Role</label>
                <input type="text" id="role" name="role" placeholder="(optional)" />
                <p class="invisible">
                  <label>Don't fill this out if you're human: <input name="other" /></label>
                </p>
                <button type="submit">Notify me!</button>
              </form>
            </div>
          </div>
          -->

          <p>
            The beginning of this writeup will focus on features. If, instead, you're primarily
            interested in how Fuzz Map works, you can skip ahead to the part about
            <a href="#how-it-works">how it works</a>. There's some fun trivia, e.g. why
            <code>a &amp;&amp; b</code> <em>can't</em> be transformed into
            <code>(a &amp;&amp; (h1, true)) || (h2, b)</code>.
          </p>

          <p>
            By the way, everything in this research prototype runs in your browser. No data is
            uploaded to a server.
          </p>

          <h2>Live programming</h2>
          <p>
            Fuzzing an application might reveal an unexpected state or crash in the map. After
            updating the application's code, you can fuzz the application a second time. Checking
            the new map helps to verify that the effects of the change are what you expected.
          </p>
          <p>
            In Fuzz Map, fuzzing the second time normally goes much faster than it did the first
            time around. This is because inputs are saved for reuse after recompiling. Inputs are
            typically reusable without manual annotation because Fuzz Map uses the event handler's
            inline declaration to identify it. For example, if you make a button with an
            <code>onClick</code> event handler:
          </p>
          <pre>function App(...) {
  const handleClick = ...;
  return &lt;button onClick={handleClick} /&gt;;
}</pre>
          <p>
            then the event handler will be identified using the text <code>"handleClick"</code>.
            This means that even if the <em>definition</em> of <code>handleClick</code> is changed,
            click events will still be reusable. If necessary, you can disambiguate further by
            defining the <code>data-fuzzmap-key</code> HTML attribute.
          </p>
          <h2>Errors</h2>
          <p>
            Errors can occur while rendering a state or while handling an event. Fuzz Map detects
            errors and displays them in the map.
          </p>
          <p>
            Here's a potential bug in the Checkout example. Suppose an order contains zero items.
            The code for computing the order subtotal has a
            <code>reduce</code> call:
          </p>
          <pre>const subtotal = sortedItems
  .map(([_code, item]) =&gt; item.quantity * item.price)
  .reduce((a, b) =&gt; a + b, 0);</pre>
          <p>
            If the second argument to <code>reduce</code> is forgotten, then removing all of the
            items from the order by decreasing their quantities to zero results in the following
            map:
          </p>
          <p><img width="1648" height="728" src="https://jyc-static.com/a8ae38d4673b62a50982cb85907ed112fc3f43ae35362eff1499ece5bb591826"></p><p>
            Errors can also occur in event handlers. Inserting an error into
            <code>onClickContinue</code> results in this map:
          </p>
          <p><img width="1581" height="1735" src="https://jyc-static.com/da5984075f7237f332b0f71504bd0d473137e10237cb9f3c6d7e136f9b8c07a7">
          </p>
          <p>Hover over the error icon to reveal the error:</p>
          <p><img width="1122" height="1348" src="https://jyc-static.com/0e0ac2fcc00e4c7220878b974893f3875dd87018bca28416e3bb00a821ac0acf">
          </p>

          <h2>Before/After</h2>
          <p>
            A single event handler can have many different cases, each corresponding to a distinct
            codepath. Fuzz Map lists out the cases for an input in the Before/After view.
          </p>
          <p>
            Suppose we use the following modified code to handle changes to item quantities in
            <code>onChangeQuantity</code>:
          </p>
          <pre>const quantity = e.target.valueAsNumber;
const items = new Map(oldItems);
if (quantity === 0) {
  items.delete(code);
} else {
  const item = items.get(code);
  items.set(code, { ...item, quantity });
}
return items;</pre>
          <p>
            After fuzzing, there will be an arrow from the initial state to the initial state with a
            name like “change 'House Wine'” or “change 'Double-Shot
            Espresso'”. The name could be different because changing the quantity of any item
            is handled by the same code.
          </p>
          <p><img width="1209" height="1322" src="https://jyc-static.com/93722005dafcd9e57c2acc0a488370dc564d431cbafdb81ea8bfcc3c7a05424e">
          </p>
          <p>
            Click on the label to switch to the Before/After view, which lists the different cases
            for changing an item quantity.
          </p>
          <p><img width="1712" height="2115" src="https://jyc-static.com/e3484a93b59571159498cf4c3d2d7df24f1e54e4ec0f1bfe9fe8bb27da73401f"></p><p>
            The modified code doesn't properly handle “Case 2. change 'House Wine' to
            ''”. When the item's quantity is changed to the empty string,
            <code>e.target.valueAsNumber</code> evaluates to <code>NaN</code>, and the subtotal
            calculation goes wrong.
          </p>
          <p>
            Click
            <span>
              <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 20 20" fill="currentColor">
                <title>Show code</title>
                <path fill-rule="evenodd" d="M12.316 3.051a1 1 0 01.633 1.265l-4 12a1 1 0 11-1.898-.632l4-12a1 1 0 011.265-.633zM5.707 6.293a1 1 0 010 1.414L3.414 10l2.293 2.293a1 1 0 11-1.414 1.414l-3-3a1 1 0 010-1.414l3-3a1 1 0 011.414 0zm8.586 0a1 1 0 011.414 0l3 3a1 1 0 010 1.414l-3 3a1 1 0 11-1.414-1.414L16.586 10l-2.293-2.293a1 1 0 010-1.414z" clip-rule="evenodd"></path>
              </svg>
              Show code</span>
            to highlight the event handler's declaration:
          </p>
          <p><img width="1570" height="1502" src="https://jyc-static.com/572fd9bed037944c5788bba8b130196f74ec3e1463f82049e60761ad1f60d312">
          </p>
          <p>
            A future version of Fuzz Map could use the information it already has to highlight the
            lines of code that executed for each case. You could select two cases to reveal which
            branches executed differently. This could be used to debug unexpected states.
          </p>

          <h2><a id="how-it-works">How it works</a></h2>
          <p>
            The main idea behind Fuzz Map is to distinguish between GUI states using code coverage
            and to render these states as a simplified map. Coverage-guided fuzzing has been applied
            with great success for more than a decade. A complete survey is beyond the scope of this
            writeup; see
            <a href="https://arxiv.org/abs/1812.00140">Manes et al. (2018)</a> for an overview,
            including a fuzzer genealogy.
          </p>
          <p>
            A visual map is especially useful for GUI development, where bugs are often obvious to
            humans but difficult for computers to check. It's almost certainly a bug if a program
            crashes or reads memory it shouldn't. But what if a button sends the user to the wrong
            screen, or the interface just looks strange under unexpected circumstances? People
            already use whiteboards or prototyping software to explain an application's behavior to
            themselves, or to review it with colleagues. Fuzz Map tries to create the same kind of
            map automatically.
          </p>

          <h2>The state graph</h2>
          <p>
            While Fuzz Map fuzzes, it builds a <em>state graph</em>. Each node in the graph
            represents an application state. Each edge represents an input processed by an event
            handler. Every state and event is identified by a <em>hit vector</em> which describes
            the branches that executed while the application rendered or handled the event. The
            following program contains two branches:
          </p>
          <pre>const name = ...;
if (name !== null) {
  return `Hello, ${name}!`;
} else {
  throw new Error("TODO");
}</pre>
          <p>
            If the program runs one time and <code>name</code> is not null, then the hit vector will
            be \( (1, 0) \). If the program runs two times, and both times <code>name</code> is
            null, then the hit vector will be \( (0, 2) \).
          </p>
          <p>
            A hit vector is a coarse representation of an application state: many different states
            can correspond to the same hit vector. This simplification is typically useful for Fuzz
            Map. The actual program being fuzzed (approximately a Turing machine) is always much
            more complicated than the map that Fuzz Map displays (approximately a finite automaton).
            Note that this simplification means that it's possible, albeit unlikely, to have two
            different states with the same hit vector where only one exhibits a bug.
          </p>
          <p>
            Bugs in an application generally correspond to untested codepaths or interactions
            between codepaths. Two states that execute very different code might render the same
            HTML. But if a program executes the same code when rendering two screens, then any bug
            in the code will generally occur in both cases. Because Fuzz Map uses hit vectors to
            identify states, it will only show two states in the map for the previous example: one
            where
            <code>name</code> is non-null, and another where <code>name</code> is null.
          </p>

          <h2>Random fuzzing</h2>
          <p>
            The state graph helps Fuzz Map to explore states efficiently. The graph keeps track of
            the states the fuzzer has seen and the shortest paths between them. Instead of building
            a graph, Fuzz Map could randomly generate long sequences of inputs. This is surprisingly
            inefficient. Efficiency is especially important for fuzzing GUIs, which process inputs
            much more slowly than typical fuzzer targets like parsers or protocols.
          </p>
          <p>
            To illustrate how random fuzzing can be inefficient, consider an application with a
            sequence of screens, e.g. for checking out items from an online store. In screen \( 1
            \), colored blue, you can only go to the next state, and in screens \( 2 \) through \( n
            - 1 \) you can go to either the previous or next screen.
          </p>
          <p><img width="844" height="120" src="https://jyc-static.com/f32cdc0b6d8f4045de8e2a9ef36292f9d79a20975f8288c3e2b7b547e660d922">
          </p>
          <p>
            How long will it take a random fuzzer to find the final screen \( n \), colored orange?
            If this is the
            <a href="https://en.wikipedia.org/wiki/Coupon_collector%27s_problem">coupon collector's problem</a>, we'd expect it to take around \( n \log n \) inputs. But because each screen can only
            be accessed through its adjacent screens, this is actually an instance of the
            <a href="https://en.wikipedia.org/wiki/Gambler%27s_ruin">gambler's ruin</a> problem. It
            will take random fuzzing around \( n^2 \) inputs to find the final orange screen! And
            the structure of a real GUI is normally even worse for fuzzers. Most GUIs also have many
            cycles and dead ends; these only make it more likely for the fuzzer to get stuck.
          </p>
          <p>
            Because Fuzz Map uses code coverage to recognize states it has already seen, it only
            takes around \( n \) inputs in this case.
          </p>
          <p>
            For the math behind the \( n^2 \) estimate, see
            <a href="https://math.stackexchange.com/a/3970622">angryavian's answer</a> to an
            equivalent problem on Math StackExchange. For the general case, see the chapter
            “Random Walk and Ruin Problems” in
            <em>An Introduction to Probability Theory and Its Applications, Vol. 1</em> by Feller.
            The amazing Internet Archive has an
            <a href="https://archive.org/details/introductiontopr00fell/page/317">online copy you can borrow</a>, where the analysis starts on page 317.
          </p>
          <p>
            By the way, if you think it's inconvenient that only one person can borrow the book per
            hour, book publishers disagree—they think it should be even harder! At the time of
            writing, Hachette, HarperCollins, Wiley, and Penguin Random House are suing the Internet
            Archive. The Internet Archive is being
            <a href="https://www.eff.org/cases/hachette-v-internet-archive">defended by the EFF</a>.
          </p>

          <h2>Exploring the state graph</h2>
          <p>
            Whenever Fuzz Map <em>first</em> sees a state, identified by its hit vector \( \vec s
            \), it generates all the possible inputs for that state. For example, one input \( i \)
            might be “change the value of 'Pickup time' to '1:24 AM'”. Then it places
            all of these inputs into a <em>fuzzing queue</em>. The items in the queue are pairs \(
            (\vec s, i) \).
          </p>
          <p>
            Fuzzing takes place in a loop. In each iteration, Fuzz Map dequeues a pair \( (\vec s,
            i) \) then applies it. If \( \vec s \) does not match the current state, then Fuzz Map
            <em>travels</em> before applying \( i \). It does this by applying the shortest sequence
            of inputs it previously recorded as having led to \( \vec s \). A minified set of these
            shortest paths is also saved for reuse after recompiling to make subsequent fuzzing runs
            faster. This set is comparable to the <em>seed pool</em> of a conventional fuzzer.
          </p>
          <p>
            Fuzz Map uses a few simple scheduling heuristics for dequeuing items. When possible,
            Fuzz Map dequeues an item whose state equals the current state. This is because it is
            typically more expensive to reset a GUI than to apply an additional input. Otherwise,
            Fuzz Map will alternate between selecting a random item from the queue and selecting an
            item with the smallest hit vector.
          </p>
          <p>
            Fuzz Map dequeues a random item rather than always dequeuing the next item to to try
            avoid getting stuck in a clique of states that all lead to each other. Dequeuing the
            item with the smallest hit vector helps to expose behavior that occurs only when a loop
            does not execute.
          </p>
          <p>
            These are just heuristics, and future versions of Fuzz Map could be much more
            sophisticated. Consider an application which only shows some state if
            <code>password == "please"</code>. Fuzz Map knows through branch coverage that isn't
            guessing correctly, but it doesn't currently analyze the program to determine what the
            correct password is. A more realistic example for a GUI might be an event handler that
            checks whether <code>event.touches.length == 2</code>. One technique for generating
            inputs using program analysis is
            <a href="https://en.wikipedia.org/wiki/Concolic_testing">concolic testing</a>.
          </p>
          <p>
            <iframe width="560" height="315" src="https://www.youtube-nocookie.com/embed/RfiQYRn7fBg" title="Samuel L. Jackson's character in Jurassic Park tries to access a hacked computer system, but doesn't know the password. The system taunts Samuel L. Jackson by saying 'Ah ah ah! You didn't say the magic word!'" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe>
          </p>

          <h2>Branch coverage</h2>
          <p>
            To record branch coverage and obtain hit vectors for nodes (render states) and edges
            (event handlers), Fuzz Map uses compile-time instrumentation.
          </p>
          <p>
            Tangentially, Fuzz Map does some things with compile-time instrumentation which could
            just as well be done at by patching the application runtime, e.g. associating event
            handlers with their elements. It would have been feasible to patch React or the browser,
            but it will be much harder to patch other platforms' runtimes. iOS is an obvious
            example, but another scenario is a corporate environment where you are only allowed to
            deploy JARs.
          </p>
          <p>Instrumentation is applied in a compiler pass. When Fuzz Map encounters this code:</p>
          <pre>return isLoading ? "Loading..." : "Done!";</pre>
          <p>it adds instrumentation and outputs the following:</p>
          <pre>return isLoading ? (hit(65), "Loading...") : (hit(66), "Done!");</pre>
          <p>
            The comma denotes a <em>sequence expression</em>. When the expression
            <code>a, b</code> is evaluated, first <code>a</code> is evaluated, then <code>b</code>.
            Finally, the entire expression evaluates to the result of <code>b</code>. Here it's used
            to insert the hit counter into the ternary expression without changing its value. There
            are weirder tricks for this which will appear later in this section.
          </p>
          <p>
            <code>hit(n)</code> increments the <em>hit counter</em> with ID <code>n</code> in a
            global map. The hit counter IDs in this example are \( 65 \) and \( 66 \) because Fuzz
            Map packs both a branch ID and a target ID into the hit counter ID. The branch ID in
            this case is \( 1 \), and the second branch target has ID \( 2 \), so \( 66 = (1 \ll 6)
            + 2 \). Fuzz Map also uses the most significant bit to record whether a hit counter ID
            corresponds to a “high detail” branch; currently these are just function
            entry points. The extra information packed into the hit counter ID is used during map
            graph generation, discussed later.
          </p>
          <p><img width="1924" height="192" src="https://jyc-static.com/cd9227aba935fd66f6fd308fc78b4e856ca35da68df16b0c2128f91303032949"></p><h2>Short-circuiting expressions</h2>
          <p>
            You probably know that the Boolean operators in JavaScript <em>short-circuit</em>
            and evaluate to the first sub-expression that determines their value, as they do in most
            (or all?) languages in the C family. Short-circuiting means that
            <code>false &amp;&amp; alert("boo!")</code> and <code>true || alert("boo!")</code> never alert.
            But unless you are familiar with React and JSX, you might not know that short-circuiting
            is often used
            <a href="https://reactjs.org/docs/conditional-rendering.html#inline-if-with-logical--operator">idiomatically</a>. Here's some code from the Checkout example:
          </p>
          <pre>{screen === "OrderConfirmed" &amp;&amp; (
  &lt;&gt;
    &lt;h2&gt;Order confirmed&lt;/h2&gt;
    &lt;div className="screenContents"&gt;
      &lt;p&gt;Your order is confirmed!&lt;/p&gt;
      &lt;p&gt;{STORE_NAME} is preparing your order.&lt;/p&gt;
      ...
    &lt;/div&gt;
  &lt;/&gt;
)}</pre>
          <p>
            When the “Order Confirmed” screen is not being shown,
            <code>screen === "OrderConfirmed"</code> evaluates to <code>false</code>, so the entire
            expression evaluates to <code>false</code>. React doesn't render <code>false</code>, so
            the screen doesn't show.
          </p>
          <p>
            The frequent use of short-circuiting in React made it more important for this demo to
            instrument it. If the demo worked on C programs compiled to WASM instead, this might
            have been less important.
          </p>
          <p>
            As an aside, WASM would have made some things easier. Instead of having to write the
            instrumentation for each branching operation separately, I could have just instrumented
            the less numerous
            <a href="https://webassembly.github.io/spec/core/syntax/instructions.html">control instructions</a>
            in WASM. Rewriting a WASM binary is a lot easier than rewriting e.g. an x86 binary
            because WASM does not allow jumps to arbitrary addresses; for the difficulties that
            creates, see e.g.
            <a href="https://www.youtube.com/watch?v=iRvC5hgN1VY">Bauman et al (2018)</a>. More
            generally, it would be nifty to have the ability to write a compiler pass that operates
            on a lower-level representation, and to then have the compiler automatically transform
            this pass into one that operates on a higher-level representation, preserving the
            original structure of the program whenever possible. I don't know of other applications
            for this, though; maybe it could be used to implement diff/patch not based on lines.
          </p>
          <p>
            Because we don't have this ability, here are the separate transformations for
            <code>a || b</code> and <code>a ?? b</code> (the
            <a href="https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Operators/Nullish_coalescing_operator">nullish coalescing operator</a>):
          </p>
          <ul>
            <li>
              <code>a || b</code>
              becomes
              <pre>((x) =&gt; x ? (hit(1), x) : (hit(2), b))(a)</pre>
            </li>
            <li>
              <code>a ?? b</code>
              becomes
              <pre>((x) =&gt; (x === null || x === undefined) ? (hit(2), b) : (hit(1), x))(a)</pre>
            </li>
          </ul>
          <p>
            Any expression can be replaced by an
            <a href="https://en.wikipedia.org/wiki/Immediately_invoked_function_expression">immediately-invoked function expression</a>
            containing a conditional expression. It's necessary because we need to bind the value of
            the expression <code>a</code> so we can use it multiple times while evaluating it only
            once. For example, to evaluate <code>a ?? b</code>, first we compare the value of
            <code>a</code> with <code>null</code> and <code>undefined</code>, then we return it.
          </p>
          <p>
            I previously tried to be clever and transform <code>a || b</code> to
            <code>(a &amp;&amp; (h1, true)) || (h2, b)</code>. This works unless <code>a</code> is only
            <em>truthy</em>, not true, and the result of the expression is not used as a Boolean.
            Then the transformed expression evaluates to <code>true</code> when the original
            expression would have evaluated to e.g. <code>1</code>.
          </p>
          <p>
            If this section didn't bore you out of your mind, you might enjoy the game
            <a href="https://alf.nu/ReturnTrue?world=true&amp;level=id">return true to win</a> by
            <a href="https://twitter.com/steike">@steike</a>.
          </p>

          <h2>The map graph</h2>
          <p>
            The state graph is already a simplified version of the program being tested. But
            typically it'll still be humongous, even for a simple application. When rendered it is
            almost unreadable. So before layout and rendering, Fuzz Map <em>simplifies</em> the
            state graph into a <em>map graph</em>. Each node or edge in the map graph corresponds to
            one or more nodes and edges in the state graph. As a final step, Fuzz Map uses
            <a href="https://github.com/kieler/elkjs">elkjs</a> to create a <em>layout</em> where
            each node and edge in the map graph has been assigned a position on the screen.
          </p>
          <p>
            Why is the state graph so large? First, there are many ways that different parts of the
            program can interact with each other. If one branch is added to the program that is
            independent from the other branches in the program, it more than doubles the number of
            possible states. In other words, even when each entry of the hit vector is clamped to \(
            [0, 1] \), a program with \( n \) branches still has \( 2^n \) possible hit vectors.
          </p>
          <p>
            Second, each iteration of a loop increments each hit counter it contains. So each loop
            iteration creates an entirely new state.
          </p>
          <p>
            During fuzzing, it's useful for the state graph to be large. Recall that each item \(
            (\vec s, i) \) in the fuzzing queue contains a hit vector \( \vec s \) that identifies
            the state to which the input \( i \) should be applied. So the more precisely the hit
            vector \( \vec s \) describes the application state, the more likely we are to
            successfully apply input \( i \) when we travel to that state again during fuzzing.
            Suppose we used clamped hit vectors to identify application states. Then we might try to
            apply an input \( i \) that only works in states where a list has 3 elements to a state
            where that list has only 2 elements.
          </p>
          <p>
            It's still possible for us to fail to apply an input \( i \). A hit vector \( \vec s \)
            might correspond to two states, and \( i \) might apply in only one of them. What if we
            instead identified each state using the application's actual memory, instead of using
            hit vectors? This would guarantee that \( i \) would apply, but might make the fuzzer
            slower. Obviously, we would use much more memory to represent each state.
          </p>
          <p>
            More subtly, we would apply many redundant operations during fuzzing. Suppose the
            application behaves exactly the same no matter what a user's name is, so long as they
            have a name. Then unless we have a way to determine that the difference between two
            states with different names is insignificant, every different name
            <em>multiplies</em> the number of items in the fuzzing queue.
          </p>

          <h2>Simplification</h2>
          <p>
            The map graph is created by combining nodes and edges in the state graph. This process
            repeats until no more combinations are possible. Fuzz Map uses a few simple heuristics
            for simplification. The simplest—but most impactful—is
            <em>clamping</em>: each entry of a hit vector is clamped to at most \( 1 \). Another
            heuristic, <em>branch collapsing</em>, is to treat two branch targets as identical when
            we've seen a state where both were hit. Branch collapsing helps with GUIs which perform
            processing for each item in a list, like the Checkout example.
          </p>
          <p>
            This diagram shows an example of clamping during simplification. The two blue nodes in
            the state graph are combined into one node in the map graph:
          </p>
          <p><img width="993" height="258" src="https://jyc-static.com/b37bd9d1850bdf516755d5a40a6d6b266ac9d139f787d01976316aef905dc7fe"></p><p>
            Initially the two blue nodes have different hit vectors, \( (1, 0, 2) \) and \( (3, 0,
            1) \). But both hit vectors are equivalent after clamping to \( (1, 0, 1) \), so the two
            nodes are combined. This causes the edge from one blue node to the other to become a
            loop. Simplification operates on edges as well.
          </p>
          <p>
            There's a lot more that could be said about map graph simplification. In fact,
            simplification was the largest source of novelty and complexity in this project. Without
            simplification, interesting parts of the map are drowned out by exponentially more
            uninteresting parts. Simplification is the key to making the map visualization possible.
          </p>
          <p>
            Clamping is essentially <em>local</em> in that it acts on each hit vector independently.
            Branch collapsing is slightly less local. The next step is to explore
            <em>global</em> analyses, e.g. identifying when a state differs significantly from
            others.
          </p>

          <h2>Applying inputs</h2>
          <p>
            Changing tack: it's often possible for Fuzz Map to apply inputs even after the program
            has been modified. The <em>input path</em>s that Fuzz Map uses to identify input targets
            contain the event handler's inline declaration. For example, if you render a list of
            buttons:
          </p>
          <pre>function App(...) {
  ...
  return list.map((name) =&gt; &lt;button onClick={handleClick(name)} /&gt;);
}
</pre>
          <p>each button's <code>onClick</code> handler gets an input path:</p>
          <pre>{ subtree: ?, eventName: 'click', handler: 'App_handleClick_name', index: 1 }</pre>
          <p>
            where <code>index: 1</code> means this handler was the second among its siblings in the
            DOM with this handler. Fuzz Map uses the event handler's inline declaration because it's
            already common practice not to define event handlers entirely inline. This means most
            input paths will be stable without any extra work.
          </p>
          <p>
            Fuzz Map was designed to work without manual annotation whenever feasible. But if you
            need to more precisely identify an element, you can set the
            <code>data-fuzzmap-key</code> HTML attribute on the element or its ancestors. The
            <code>subtree</code> field in the input path is a list of
            <code>(key, index)</code> pairs. If you want, you can give each element a totally unique
            <code>data-fuzzmap-key</code>. Many testing systems either require this for <em>all</em>
            interactive elements or fall back to guessing based on style or content.
          </p>
          <p>
            A heuristic is used to minimize the size of the seed input set: if one seed input
            sequence is exactly contained in another, then the smaller one is removed. For many
            applications, it is more expensive to reset the sandbox than to extend an existing input
            sequence with additional inputs, e.g. when resetting the sandbox requires resetting a
            database. A more optimal approach would be to compute a minimal path cover.
          </p>

          <h2>Limitations and future work</h2>
          <p>
            There are many limitations in the current version of Fuzz Map—hence the
            <em>alpha</em> label. I'll mention a few of them below. All of these will be addressed
            in future versions.
          </p>
          <p>
            Fuzz Map does not generate complex values for inputs. This is the exact opposite of most
            commonly-used fuzzers! That is also exactly why this wasn't a focus of the demo. Instead
            of reinventing the wheel, a future version of Fuzz Map will integrate established
            fuzzers for generating e.g. the text in inputs.
          </p>
          <p>
            Fuzz Map's state and event model is extensible. The demo does not currently handle
            asynchronous events like <code>setTimeout</code> and <code>fetch</code> calls. But in a
            future version of Fuzz Map, edges will correspond not just to DOM event handlers, but
            also to event handlers attached to browser APIs. It will be straightforward to extend
            Fuzz Map to instrument and replay these in the same way that it already instruments DOM
            events. Here's a quick mockup:
          </p>
          <p><img width="1567" height="1982" src="https://jyc-static.com/315ef928645072563116f42c9654d6ac5299d05a732c370d2f45c34b860a07a6"></p><p>
            Fuzz Map does not handle event handlers that are defined at runtime. Only
            <a href="https://reactjs.org/docs/forms.html#controlled-components">controlled components</a>
            are fuzzed.
          </p>
          <p>
            I was lazy and didn't handle more exotic branching operations like <code>??=</code> and
            <code>?.</code>.
          </p>

          <h2>Special acknowledgments</h2>
          <p>
            I'd like to especially acknowledge <a href="https://twitter.com/joeyliaw">Joey Liaw</a>,
            who gave me the idea to try and fuzz GUIs over lunch!
          </p>
          <p>
            I'd also like to especially acknowledge the authors of
            <a href="https://github.com/kieler/elkjs">elkjs</a> and the
            <a href="https://www.eclipse.org/elk/">ELK layout library</a>. Of all the libraries I
            used for this demo, ELK punched the most above its weight. ELK generates the layout used
            to render the map graph. A similar program is the classic
            <a href="https://graphviz.org/">Graphviz</a>.
          </p>

          
        </div></div>]]></description>
        </item>
    </channel>
</rss>