<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0">
    <channel>
        <title>HN100 - Readable Contents</title>
        <link>https://hn.algolia.com/api/v1/search_by_date?tags=%28story,poll%29&amp;numericFilters=points%3E100</link>
        <description>Uses Readability to add bodies to the RSS feed</description>
        <lastBuildDate>Thu, 27 Nov 2025 17:30:05 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>en</language>
        <item>
            <title><![CDATA[We're Losing Our Voice to LLMs (208 pts)]]></title>
            <link>https://tonyalicea.dev/blog/were-losing-our-voice-to-llms/</link>
            <guid>46069771</guid>
            <pubDate>Thu, 27 Nov 2025 14:51:01 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://tonyalicea.dev/blog/were-losing-our-voice-to-llms/">https://tonyalicea.dev/blog/were-losing-our-voice-to-llms/</a>, See on <a href="https://news.ycombinator.com/item?id=46069771">Hacker News</a></p>
<div id="readability-page-1" class="page"><article>
  
  <p><img src="https://tonyalicea.dev/assets/blogimages/coloredpencils.png" alt="Colored pencils">
</p>

<p>Social media has become a reminder of something precious we are losing in the age of LLMs: <strong>unique voices</strong>.</p>
<p>Over time, it has become obvious just how many posts are being generated by an LLM. The tell is the voice. Every post sounds like it was posted by the same social media manager.</p>
<p>If you rely on an LLM to write all your posts, you are making a mistake.</p>
<p>Your voice is an asset. Not just what you want to say, but how you say it.</p>
<p>Your voice is unique. It is formed from your lifetime of lived experiences. No one's voice will be exactly like yours.</p>
<p>Your voice becomes recognizable. Over many posts it becomes something people subconsciously connect with, recognize, trust, and look forward to.</p>
<p>Your voice provides the framework for the impression you leave in a job interview, while networking at a meet-up, or with a co-worker.</p>
<p>Years ago I got a job thanks to my blog posts. A manager wanted my voice influencing their organization. Your voice is an asset.</p>
<p>Your voice matures and becomes even more unique with time and practice.</p>
<p>LLMs can rob you of that voice, and the rest of us lose something precious in the process.</p>
<p>Having an LLM write "in your voice" is not the same. Your voice is not static. It changes with the tides of your life and state of mind. Your most impactful message may come because it was the right moment and you were in the right frame of mind.</p>
<p>Let your voice grow with use. Let it be unique.</p>
<p>Do not let one of your greatest assets fade into atrophy, wilted by cognitive laziness.</p>
<p>Write in <em>your</em> voice.</p>
<p>I do not care what the linguistic remix machine juggles into being.</p>
<p>I care what you have to say.</p>

</article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[The State of GPL Propagation to AI Models (111 pts)]]></title>
            <link>https://shujisado.org/2025/11/27/gpl-propagates-to-ai-models-trained-on-gpl-code/</link>
            <guid>46068777</guid>
            <pubDate>Thu, 27 Nov 2025 12:48:12 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://shujisado.org/2025/11/27/gpl-propagates-to-ai-models-trained-on-gpl-code/">https://shujisado.org/2025/11/27/gpl-propagates-to-ai-models-trained-on-gpl-code/</a>, See on <a href="https://news.ycombinator.com/item?id=46068777">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
<p>When GitHub Copilot was launched in 2021, the fact that its training data included a vast amount of Open Source code publicly available on GitHub attracted significant attention, sparking lively debates regarding licensing. While there were issues concerning conditions such as attribution required by most licenses, there was a particularly high volume of discourse suggesting that the conditions of copyleft licenses, such as the GNU General Public License (GNU GPL), would propagate to the model itself, necessitating that the entire model be released under the same license. The propagation of the GPL is a concept that many modern software engineers have naturally accepted; thus, for an engineer with a straightforward sensibility, it is a perfectly natural progression to think that if GPL code is included in some form, copyleft applies and the license propagates.</p>



<p>However, as of 2025, the theory that the license of the source code propagates to AI models trained on Open Source code is not seen as frequently as it was back then. Although some ardent believers in software freedom still advocate for such theories, it appears they are being overwhelmed by the benefits of AI coding, which has overwhelmingly permeated the programming field. Amidst this trend, even I sometimes succumb to the illusion that such a theory never existed in the first place.</p>



<p>Has the theory that the license of training code propagates to such AI models been completely refuted?</p>



<p>Actually, it has not. This issue remains an indeterminate problem where lawsuits are still ongoing and the judgments of major national governments have not been made clear. In this article, I will explain the current situation of this license propagation theory, namely “GPL propagates to AI models trained on GPL code,” and connect it to points of discussion such as the legal positioning of models and the nature of the freedom we pursue in the AI domain.</p>



<ol><li><a href="https://shujisado.org/2025/11/27/gpl-propagates-to-ai-models-trained-on-gpl-code/#the-current-standing-in-two-lawsuits">The Current Standing in Two Lawsuits</a><ol><li><a href="https://shujisado.org/2025/11/27/gpl-propagates-to-ai-models-trained-on-gpl-code/#doe-v-github-copilot-class-action-the-persisting-claim-of-open-source-license-violation">Doe v. GitHub (Copilot Class Action): The Persisting Claim of Open Source License Violation</a></li><li><a href="https://shujisado.org/2025/11/27/gpl-propagates-to-ai-models-trained-on-gpl-code/#gema-v-openai-the-theory-treating-memory-in-models-as-legal-reproduction">GEMA v. OpenAI: The Theory Treating “Memory” in Models as Legal Reproduction</a></li><li><a href="https://shujisado.org/2025/11/27/gpl-propagates-to-ai-models-trained-on-gpl-code/#possibilities-derived-from-the-current-status-of-the-two-lawsuits">Possibilities Derived from the Current Status of the Two Lawsuits</a></li><li><a href="https://shujisado.org/2025/11/27/gpl-propagates-to-ai-models-trained-on-gpl-code/#treatment-under-japanese-law">Treatment under Japanese Law</a></li></ol></li><li><a href="https://shujisado.org/2025/11/27/gpl-propagates-to-ai-models-trained-on-gpl-code/#arguments-negating-the-theory-of-license-propagation-to-models">Arguments Negating the Theory of License Propagation to Models</a><ol><li><a href="https://shujisado.org/2025/11/27/gpl-propagates-to-ai-models-trained-on-gpl-code/#arguments-for-negation-at-the-copyright-law-layer">Arguments for Negation at the Copyright Law Layer</a></li><li><a href="https://shujisado.org/2025/11/27/gpl-propagates-to-ai-models-trained-on-gpl-code/#arguments-for-negation-at-the-gpl-text-layer">Arguments for Negation at the GPL Text Layer</a></li><li><a href="https://shujisado.org/2025/11/27/gpl-propagates-to-ai-models-trained-on-gpl-code/#arguments-for-negation-at-the-technical-layer">Arguments for Negation at the Technical Layer</a></li><li><a href="https://shujisado.org/2025/11/27/gpl-propagates-to-ai-models-trained-on-gpl-code/#practical-and-policy-arguments-for-negation">Practical and Policy Arguments for Negation</a></li></ol></li><li><a href="https://shujisado.org/2025/11/27/gpl-propagates-to-ai-models-trained-on-gpl-code/#the-stance-of-osi-and-fsf">The Stance of OSI and FSF</a></li><li><a href="https://shujisado.org/2025/11/27/gpl-propagates-to-ai-models-trained-on-gpl-code/#summary">Summary</a></li><li><a href="https://shujisado.org/2025/11/27/gpl-propagates-to-ai-models-trained-on-gpl-code/#references">References</a></li></ol>



<p><strong>Note:&nbsp;</strong>This article is an English translation of a post&nbsp;<a href="https://shujisado.com/2025/11/25/gpl_propagation_to_ai_models/">originally written in Japanese.</a>&nbsp;While it assumes a Japanese reader, I believe it may also be useful for an English-speaking audience.</p>



<hr>



<h2 id="the-current-standing-in-two-lawsuits">The Current Standing in Two Lawsuits</h2>



<p>First, let us organize what the “GPL propagation theory to AI models” entails. This is the idea that when an AI model ingests GPL code as training data, the model itself constitutes a derivative work (derivative) of the GPL code; therefore, when distributing the model, the copyleft conditions of the GPL, such as the obligation to disclose source code, apply. In other words, it is not a question of whether the output of the model is similar to the GPL code, but a theory that “since the model itself is a derivative containing GPL code, the GPL extends to the model.” While there were many voices supporting this theory around 2021, as mentioned earlier, it is no longer the mainstream of the discussion today. However, two major ongoing lawsuits can be cited as grounds that this theory has not been completely denied. These are <em>Doe v. GitHub</em> (the Copilot class action) filed in the United States and <em>GEMA v. OpenAI</em> filed in Germany. I will explain the history and current status of each lawsuit below.</p>



<h3 id="doe-v-github-copilot-class-action-the-persisting-claim-of-open-source-license-violation">Doe v. GitHub (Copilot Class Action): The Persisting Claim of Open Source License Violation</h3>



<p>In the Copilot class action filed at the end of 2022 in relation to GitHub Copilot, anonymous developers became plaintiffs and argued that GitHub, Microsoft, and OpenAI trained their models on source code from public repositories without permission, inviting massive license violations through Copilot. Specifically, they viewed it as problematic that when Copilot reproduces part of the code that served as the training source in its output, it does not perform the author attribution or copyright notice required by licenses such as MIT or Apache-2.0 at all, and furthermore, it indiscriminately trains on and outputs code under licenses that impose copyleft conditions like the GPL, thereby trampling on license clauses. The plaintiffs claimed this was a contractual violation of open source licenses and also sought damages and injunctions, asserting that it constituted a violation of the Digital Millennium Copyright Act (DMCA) under copyright law.</p>



<p>In this case, several decisions have already been handed down by the United States District Court for the Northern District of California, and many of the plaintiffs’ claims have been dismissed. What were dismissed were mainly peripheral claims such as DMCA clause violations, privacy policy violations, unjust enrichment, and torts, but some DMCA violations and the claim of “violation of open source licenses” (breach of contract) are still alive. Regarding the latter specifically, the argument is that despite the plaintiffs’ code being published under licenses like GPL or MIT, the defendants failed to comply with the author attribution or the obligation to publish derivatives under the same license, which constitutes a contractual violation. Although the court did not recognize claims for monetary damages because the plaintiffs could not demonstrate a specific amount of damage, it determined that there were sufficient grounds for the claim for injunctive relief against the license violation itself. As a result, the plaintiffs are permitted to continue the lawsuit seeking an order prohibiting the act of Copilot reproducing others’ code without appropriate license indications.</p>



<p>As is clear from the above history, “violation of open source licenses in training data” is still being contested in court in the Copilot litigation, and this is one of the reasons why the theory of license propagation to models has not been completely denied. The plaintiffs’ claim in this lawsuit does not directly demand the release of the model itself under the GPL, but it legally pursues the point that license conditions were ignored in the process of training and output; consequently, it suggests that “if the handling does not follow the license of the training data, the act of providing the model could be illegal.” Furthermore, the court has not clearly rejected this logic at this stage and has indicated a judgment that the use of open source code is accompanied by license obligations, and providing tools that ignore this could constitute a tort subject to injunction.</p>



<p>However, it is necessary to note that the claims in the Copilot litigation are legally framed as breach of contract (license) or DMCA violation, and are not a direct copyright argument that “the model is a derivative work of GPL code.” No judgment has been shown stepping so far as to mandate the disclosure of the entire model under the GPL license. The actual judgment is conservative, stating “monetary damages have not been shown, but there is room for future injunctive relief,” and does not mention the obligation to disclose the model itself. In other words, at present, there is no judicial precedent directly addressing the “GPL propagation theory to models,” and the situation is one where the issue raised regarding license violation of the source code remains alive in the judicial arena.</p>



<h3 id="gema-v-openai-the-theory-treating-memory-in-models-as-legal-reproduction">GEMA v. OpenAI: The Theory Treating “Memory” in Models as Legal Reproduction</h3>



<p>Another important lawsuit is the case where the German music copyright collective GEMA sued OpenAI. This is a copyright lawsuit concerning the unauthorized training and output of lyrics by an AI model, not AI code generation, but it carries significant theoretical implications related to “license propagation to models” even if not directly related to GPL.</p>



<p>In November 2025, the Munich I Regional Court handed down a judgment on this lawsuit, indicating regarding the matter where the ChatGPT model had memorized and reproduced the lyrics of 9 famous German songs, that the act of “memory” inside the model itself falls under the act of reproduction under copyright law. According to the judgment, the lyrics under the plaintiff’s management were “fixed” in the models of ChatGPT’s GPT-4 and 4o, and the situation was such that the lyrics were output almost verbatim just by the user giving a simple prompt. Based on this, the court determined that the model contains “parameters that memorized the work” internally, and if it is possible to reproduce an expression substantially identical to the original work for a human by means of an appropriate prompt, that memory itself falls under “reproduction” in Article 16 of the German Copyright Act. Furthermore, it determined that the act of actually outputting lyrics in response to a prompt is also a separate act of reproduction, and providing lyrics to the user falls under the act of making available to the public (public transmission). Also, it ruled that since all of these are done without the permission of the rights holder, they deviate from the scope justified by the TDM (Text and Data Mining) exception in the EU DSM Copyright Directive.</p>



<p>The important point of this judgment is that it clearly acknowledged that “if a work is recorded inside the model in a reproducible form, that state itself can constitute copyright infringement.” The court cited the text of the EU InfoSoc Directive that “reproduction includes copies in any form or manner, and does not need to be directly perceptible to humans,” and stated that in the spirit of this, even if the lyrics are encoded within the model’s parameters, it amounts to the creation of a reproduction. It went as far as to mention that “encoding in the form of probabilistic weights does not prevent it from being considered a copy,” showing a strong recognition that differences in technical formats cannot avoid the nature of reproduction under copyright law. Also, since the fact that the model could output the lyrics was not coincidental but highly consistent, it was factually found that “the direct incorporation of the essential part of the training data” occurred rather than the result of statistical learning. As a result, the Munich District Court recognized OpenAI’s liability for injunction and damages regarding the output act of the lyrics in question, and further ordered the provision of information regarding training data and output content for the future. However, this judgment is the first instance, and since OpenAI has indicated an intention to appeal, it is expected to be a continuing dispute.</p>



<p>The noteworthy theory shown by this GEMA judgment is the extension of the concept of reproduction under copyright law to the interior of the model. That is, if the work used as training data remains within the model and can be reproduced with a simple operation, it means the model already contains a reproduction of that work. This theory is groundbreaking in that it deems “the model contains the source work,” and indeed, in a commentary by Osborne Clarke, it is evaluated that “in contrast to the judgment of the English High Court in the <em>Getty v. Stability AI</em> case, the Munich District Court explicitly recognized the possibility that the AI model contains copies of the training material.” Standing on this view, the model is not merely a result of analysis, but depending on the case, can be evaluated as an aggregate of the training data itself.</p>



<p>However, it is necessary to keep in mind that this judgment is based on an extreme case where a complete match output was obtained with short text such as lyrics. The court itself stated, “Normally, temporary reproduction for learning remains within the purpose of analysis and does not infringe on the rights holder’s market, but in this case, the model holds the work in a restorable form and exceeds the scope of analysis,” emphasizing that the judgment is limited to “cases where the model performs complete reproduction.” Also, as the UK case shows, judicial decisions vary by country, and a legal consensus on this issue has not yet been formed.</p>



<p>Nevertheless, the judgment this time, which declared that the recording of a work inside a model is a reproduction, can become a major basis supporting the license propagation theory. This is because, while the premise for discussing GPL propagation is “whether the model can be said to be a reproduction or derivative work of the GPL code,” the logic of the Munich District Court legally certified exactly that “a model can be a reproduction of training data”.</p>



<h3 id="possibilities-derived-from-the-current-status-of-the-two-lawsuits">Possibilities Derived from the Current Status of the Two Lawsuits</h3>



<p>From the two lawsuits above, we can consider the path through which the theory of license propagation to AI models might be recognized in the future.</p>



<p>Let us assume the worst-case scenario from the perspective of AI operators, where these lawsuits are finalized with the plaintiffs winning. In the Copilot litigation, the judgment that “model providers must comply with the license conditions of the training source code” would be established, and in the GEMA litigation, the legal principle that “the model encompasses reproductions of the work” would be established. When these two intersect, the conclusion that “since an AI model containing GPL code is a reproduction or derivative work of the GPL code, the conditions of the GPL directly apply to its provision” is theoretically derived. That is, the possibility emerges that the theory of GPL propagation to models is effectively ratified by the judiciary.</p>



<p>Specifically, if the model memorizes and contains GPL code fragments internally, the act of distributing or providing that model to a third party may be regarded as the distribution of a reproduction of GPL code; in that case, the act of distribution under conditions other than GPL would be evaluated as a GPL license violation. If a GPL violation is established, there would be room to argue for remedies such as injunctions and claims for damages, as well as forced GPL compliance demanding the disclosure of the entire model under the same license, just as in the case of ordinary software. In fact, the remedies GEMA sought from OpenAI included disclosure regarding training data and output content, and although this is in the context of musical works, this can be said to be a type of disclosure request to make transparent “what the model learned and contains.” In the case of a GPL violation as well, the possibility cannot be denied that demands such as “disclosure of the GPL code parts contained inside the model” or “source disclosure in a form that allows reconstruction of the model” would emerge in seeking license compliance.</p>



<p>Even if not reaching such an extreme conclusion, an intermediate scenario could involve imposing certain restrictions on model providers. For example, the Copilot litigation might be settled or judged by taking measures such as “attaching a license and author attribution at the time of output if existing code of a certain length or more is included in the generated code,” or technically mandating the implementation of filters so that GPL code fragments are not extracted or reproduced from the model. In fact, GitHub, the developer of Copilot, has already introduced an optional feature that “excludes from suggestions if the candidate code matches existing code on large-scale repositories,” attempting to reduce litigation risk. Also regarding OpenAI, there are reports that it strengthened filters so that ChatGPT does not output copyrighted lyrics as they are, in response to the GEMA judgment.</p>



<p>While these are not license propagation itself legally, in practice, they indicate that the industry is steering in the direction of “ensuring the model does not potentially infringe license conditions.” In the future, there is a possibility that guidelines for excluding data with specific license terms like GPL at the model training stage, or mechanisms and systems to guarantee that there is no license-infringing output by conducting output inspections after training, will be established.</p>



<p>In any case, until these two lawsuits are completely settled and the subsequent legislative response is determined, the “theory of GPL propagation to models” has not completely disappeared. It is a scenario that could suddenly become realistic depending on future judgments, and even if the plaintiffs lose in the lawsuits, there is a possibility that support for this theory will reignite within the open source community. It is necessary to note that while it is currently an “undetermined theory not shouted as loudly as before,” that does not mean it has been legally completely denied and resolved. As our community, we need to carefully consider countermeasures while observing these trends and taking into account the legal systems of each country and opposing arguments described in the latter half of this article.</p>



<h3 id="treatment-under-japanese-law">Treatment under Japanese Law</h3>



<p>Based on the trends of the overseas lawsuits mentioned above, I will also organize the relationship between AI models, copyrighted works, and licenses under Japanese law. In Japan, Article 30-4 of the Copyright Act, introduced by the 2018 amendment, exists as a provision that comprehensively legalizes reproduction acts associated with machine learning. Furthermore, in March 2024, the Copyright Division of the Council for Cultural Affairs of the Agency for Cultural Affairs published a guideline-like document titled “Thought on AI and Copyright” (hereinafter “the Thought”), presenting a legal organization divided into the development/training stage and the generation/utilization stage of generative AI.</p>



<p>According to “the Thought,” reproduction performed basically for the purpose of AI training is legal as long as it satisfies “information analysis not for the purpose of enjoying the thoughts or sentiments expressed in the work” as defined in Article 30-4. Therefore, acts of collecting and reproducing a wide range of data from the internet to create a training dataset for research and development purposes can be done without the permission of the rights holders in principle. However, what is important is whether an “purpose of enjoyment” is mixed into that training act. “The Thought” states that if training is conducted with the purpose of “intentionally reproducing all or part of the creative expression of a specific work in the training data as the output of generative AI,” it is evaluated as having a concurrent purpose of enjoying the work rather than mere information analysis, and thus lacks the application of Article 30-4. As a typical example of this, “overfitting” is cited, and acts such as making a model memorize specific groups of works through additional training to cause it to output something similar to those works are judged to have a purpose of enjoyment.</p>



<p>Furthermore, “the Thought” also mentions the legal treatment of trained models, stating first that “trained models created by AI training cannot be said to be reproductions of the works used for training in many cases.” This is the view that since the model can generate outputs unrelated to the original in response to various inputs in a general-purpose manner, the model itself is not a copy of any specific work.</p>



<p>However, “the Thought” simultaneously acknowledges the possibility that, exceptionally, in cases where “the trained model is in a state of generating products with similarity to the work that was training data with high frequency,” the creative expression of the original work remains in the model, and it may be evaluated as a reproduction. It also points out that in such cases, the model is positioned as a machine for copyright infringement, and a claim for injunction may be recognized. In short, usually the model is merely statistical data and not the work itself, but if it has turned into a device for spewing out specific works almost as they are, it can be treated as an infringing item; this thinking shares parts with the content of the GEMA judgment.</p>



<p>It is necessary to note that the above organization is strictly a discussion of the scope of application of rights limitation provisions (exception provisions) under the Copyright Act, and does not touch upon the validity of contracts or license clauses. The Agency for Cultural Affairs document discusses from the perspective of “whether it is copyright infringement or not,” and does not deny that even if the training act is legal, contractual liability may arise if it violates terms of service or open source licenses separately. Also, no in-depth view has been shown regarding the propagation of copyleft clauses like the GPL. In Japan’s Copyright Act, there is no override provision where rights limitation provisions like Article 30-4 take precedence over contract conditions, and the “Contract Guidelines on Utilization of AI and Data” by the Ministry of Economy, Trade and Industry suggests the possibility that if there is a contract prohibiting data use between parties, that contract takes precedence.</p>



<p>Therefore, if the license is regarded as a valid contract, even if “training is legal” under Article 30-4 of the Copyright Act, the risk remains that it becomes a “violation of license conditions” under contract law, and it can be said that at least there is no official view organizing the theory of GPL propagation to models. In other words, currently, while the legality of model training acts is recognized quite broadly under the Copyright Act, license violation is left to general civil theory, and there is no clear guideline on, for example, “whether the act of publicly distributing a model trained on GPL code constitutes a GPL license violation.” Overall, the legal organization in Japan is in a situation of “safe in principle at the copyright layer, but blank at the contract layer.” Hence, the discussion in Japan regarding the theory of GPL propagation to models relies on future judicial judgments and legislative trends, and at present, there is no choice but to consider operational guidelines carefully following the organization by the Agency for Cultural Affairs.</p>



<hr>



<h2 id="arguments-negating-the-theory-of-license-propagation-to-models">Arguments Negating the Theory of License Propagation to Models</h2>



<p>As seen in the previous sections, the theory of GPL propagation to models is not legally zero. However, many legal experts and engineers point out that this theory has serious detrimental effects. Here, I present representative arguments negating the theory of license propagation to models from the layers of copyright law, GPL text, technology, and practical policy.</p>



<h3 id="arguments-for-negation-at-the-copyright-law-layer">Arguments for Negation at the Copyright Law Layer</h3>



<p>First, under copyright law, it is unreasonable to regard an AI model as a “derivative work” or “reproduction” of the training source works. In many cases, the expressions of specific works are not stored inside the model in a form recognizable to humans. The model merely holds statistical abstractions where text and code have been converted into weight parameters, and that itself is not a creative expression to humans at all. A “derivative work” under copyright law refers to a creation that incorporates the essential features of the expression of the original work in a form that can be directly perceived, but one cannot directly perceive the creativity of the original code from the model’s weights. In other words, the model does not show the nature of a work directly enough to be evaluated as encompassing the original code. For example, the High Court of Justice in the UK stated in the judgment of the <em>Getty v. Stability AI</em> case that “the Stable Diffusion model itself is not an infringing copy of the training images,” showing a negative view on regarding the model itself as a reproduction of works. Thus, there are many cautious positions internationally regarding regarding the model itself as an accumulation of works or a compilation work.</p>



<p>Also, the output generated by the model involves probabilistic and statistical transformations, and in many cases, things that do not resemble the training source at all are output. Even if a match or similarity occurs by chance, it is difficult to prove whether it is a reproduction relying on the original or an accidental similarity. It is not realistic to conduct the certification of reliance and similarity required to discuss copyright infringement for the entire model. Ultimately, in the framework of copyright law, there is no choice but to judge “whether the model relies on a specific work” on a work-by-work basis, and recognizing uniform copyrightability or infringing nature for the model itself is a large leap. As organized in Japanese law where the model is not considered a reproduction in most cases, the schematic of model equals work is considered unreasonable under copyright law.</p>



<h3 id="arguments-for-negation-at-the-gpl-text-layer">Arguments for Negation at the GPL Text Layer</h3>



<p>Next, looking at the license text and intent of the GPL itself, doubts are cast on the interpretation that GPL propagates to AI models. For example, in the text of GPLv2, the target of copyleft is limited to “derivative works” of the original code provided under GPL and “works that contain the Program.” Typically, this has been interpreted as software created by modifying or incorporating GPL code, or software combined (linked) with GPL code. In the case of an AI model, it is extremely unclear which part of the original GPL code the model “contains.” Even if the model could memorize fragments of the GPL code used for training, it is a tiny fraction when viewed from the entire model, and most parts are occupied by parameters unrelated to the GPL code. There is no clear assumption shown by the GPL drafters as to whether a statistical model that may partially encapsulate information derived from GPL code can be said to be “a work containing the Program”.</p>



<p>Furthermore, GPLv3 requires the provision of software source code in a “preferred form for modification.” If an AI model is a GPL derivative, the problem arises as to what that preferred form for modification would be. The model weights themselves have low readability and editability for humans, and are hard to call a “preferred form for modification.” If we ask whether the training data is the source code, the original trained GPL code itself cannot be said to be the source of the model, nor is it clear if it refers to the entire vast and heterogeneous training dataset. It is difficult to define what should be disclosed to redistribute the model under GPL compliance, and it could lead to an extreme conclusion that all code and data used for model training must be disclosed. While this is what some freedom believers aim for, it can only be said to be unrealistic in reality, and it deviates from the point of the GPL’s intent to enable users to modify and build from source. Thus, existing GPL provisions are not designed to directly cover products like AI models, and forcing their application causes discrepancies in both text and operation.</p>



<p>In fact, in the “Open Source AI Definition” compiled by the OSI (Open Source Initiative) in 2023, regarding “information necessary for modification” of the model, it stopped at stating that sufficiently detailed information about the training data should be disclosed, and did not require the provision of the training data itself in its entirety. Also, it states that model weights and training code should be published under OSI-approved licenses.</p>



<p>In addition, the FSF (Free Software Foundation) itself does not believe that the current GPL interpretation alone can guarantee freedom in the AI domain, and announced in 2024 that it has started formulating “conditions for machine learning applications to be free.” There, the directionality is shown that “the four freedoms should be guaranteed to users including not only software but also raw training data and model parameters,” but this conversely is a recognition that this is not guaranteed under current licenses. The FSF also points out that “since model parameters cannot be said to be source comprehensible to humans, modification through retraining is more realistic than direct editing,” and can be said to be cautious about treating models on the extension of existing GPL. Overall, claiming GPL propagation univocally to AI models that fall outside the wording and assumptions of GPL provisions is unreasonable from the perspective of interpretation.</p>



<h3 id="arguments-for-negation-at-the-technical-layer">Arguments for Negation at the Technical Layer</h3>



<p>There are also strong counterarguments from a technical perspective against the theory of GPL propagation to models. AI models, particularly those called large language models, basically hold huge statistical trends internally and do not store the original code or text as they are like a database. Returning a specific output for a specific input is merely generation according to a probability distribution, and it is not guaranteed that the same output as the training data is always obtained. If the model does not perform verbatim reproduction of training data except for a very small number of exceptional cases, evaluating it as “containing GPL code” within the model does not fit the technical reality. In fact, the OpenAI side argued in the GEMA lawsuit that “the model does not memorize individual training data, but merely reflects knowledge learned from the entire dataset in parameters.” This argument was not accepted by the Munich District Court, but that was because there was a clear example of lyric reproduction; conversely, unless there is a clear example of reproduction, the view would be that “the model is a lump of statistical knowledge”.</p>



<p>Furthermore, although it has been confirmed that models can output fragments of training data, that proportion is considered extremely limited when viewed from the whole. Regarding the whole as a reproduction based on the existence of partial memory is like claiming the whole is a reproduction of a photograph just because it contains a tiny mosaic-like fragment in an image, which is an excessive generalization. Technically, it is difficult to quantitatively measure how far specific parameters of the model retain the influence of the original data, and the correspondence between the model and training data remains statistical and difficult to draw a line. Therefore, criteria such as “how similar must it be for GPL to propagate?” cannot be established in the first place. The judgment of infringement or not has to be done on an individual output basis, and this would not be consistent with the idea of applying a single license to the entire model. From the technical aspect, since the model is basically a statistical transformation and the majority is unrelated to GPL code, applying GPL collectively can be said to be irrational.</p>



<h3 id="practical-and-policy-arguments-for-negation">Practical and Policy Arguments for Negation</h3>



<p>Finally, major demerits can be pointed out regarding the theory of license propagation to models from practical and policy perspectives. What would happen if this GPL propagation theory were legally recognized? As an extreme example, if 1 million code repositories were used for training a certain large-scale model, all the various licenses contained in them (GPL, MIT, Apache, proprietary, etc.) would “propagate” to the model, and the model provider would have to distribute the model in a form that complies with all 1 million license clauses. As a practical matter, there would be combinations where conditions contradict, such as GPLv2 and Apache-2.0, and attaching and managing a huge collection of copyright notices for one model is nothing but unrealistic. Applying all licenses to an AI model created from training data with mixed licenses is practically bankrupt, and eventually, the only thing that can be done to avoid it would be to exclude code with copyleft licenses like GPL from the training data from the start.</p>



<p>Is such a situation really desirable for our community? The spirit of the GPL is to promote the free sharing and development of software. However, if asserting excessive propagation to AI models causes companies to avoid using GPL code, and as a result, the value held by GPL software is not utilized in the AI era, it would be putting the cart before the horse. In the field of software development, many companies take a policy of not mixing GPL code into their own products, but similarly, if it becomes “do not include GPL in our AI training data,” GPL projects could lose value as data sources. Furthermore, the current legal battles surrounding AI are leaning more towards monetary compensation and regulatory rule-making, and the reality is that they are proceeding in a different vector from the direction of code sharing idealized by GPL. If only the theory of GPL propagation to models walks alone, in reality, only data exclusion and closing off to avoid litigation risks will progress, and there is a fear that it will not lead to the expansion of free software culture.</p>



<p>Policy-wise as well, governments of each country are carefully considering the use of copyrighted works in AI, but at present, there is no example establishing an explicit rule that “license violation of training data generates legal liability for the model.” Even in the EU AI Act, while there are provisions regarding the quality and transparency of training data, it does not demand compliance with open source licenses. Rather, from the perspective of promoting open science and innovation, the movement to allow text and data mining under rights limitations is strong. In Japan as well, as mentioned earlier, the direction is to broadly recognize information analysis use under Article 30-4, and the policy of forcibly applying licenses to AI models is not mainstream in current international discussions.</p>



<p>Based on the above, the theory of license propagation to models is highly likely to cause disadvantages to open source on both practical and policy fronts, and can be said not to be a realistic solution. What is important is how to realize the “freedom of software,” which is the philosophy of open source, in the AI era; the opinion that this should be attempted through realistic means such as ensuring transparency and promoting open model development rather than extreme legal interpretations is potent, and this is something I have consistently argued as well.</p>



<hr>



<h2 id="the-stance-of-osi-and-fsf">The Stance of OSI and FSF</h2>



<p>I will also organize what stance major organizations in the open source (and free software) community are currently taking in relation to the theory of GPL propagation to AI models. Representative organizations are the Open Source Initiative (OSI) and the Free Software Foundation (FSF); while they share the goal of software freedom, they do not necessarily take the same approach regarding AI models and training data.</p>



<p>First, the OSI formulated the “Open Source AI Definition” (OSAID) in 2024, defining the requirements for an AI system to be called open source. This definition states that the four freedoms (use, study, modify, redistribute) similar to software should be guaranteed for AI systems as well, and defines requirements regarding “forms necessary for modification” to realize that, requiring the disclosure of the following three elements.</p>



<ul>
<li>Data Information: Provide sufficiently detailed information about the data used for training so that a skilled person can reconstruct an equivalent model.
<ul>
<li>This does not make publishing the training data itself in its entirety mandatory, but requires disclosing the origin, scope, nature, and acquisition method if there is data that cannot be published, listing data that can be published, and providing information on data available from third parties.</li>
</ul>
</li>



<li>Code: Publish the complete set of source code for training and running the model under an OSI-approved license.</li>



<li>Parameters: Publish the model weights (parameters) under OSI-approved conditions.</li>
</ul>



<p>It should be noted that while OSI states that information regarding the code used for training and training data is indispensable in addition to model weights to realize “Open Source AI,” it does not require the complete disclosure of the training data itself. This is a flexible stance that, for example, if raw data cannot be published due to privacy or confidentiality, explaining the nature of the data by clarifying that fact can substitute. Also, the legal mechanism to ensure free use of model parameters is an issue to be clarified in the future, and at present, no conclusion has been reached on legal rights control (e.g., presence or absence of copyrightability) over parameters either.</p>



<p>As can be read from these, the OSI promotes opening up AI models at the level of the open source definition in principle, but keeps the handling of training data to requirements at the information disclosure level. Thereby, it can be said that the OSI avoids adopting the theory of license propagation to models to demand training data disclosure, and is exploring a realistic solution that first guarantees transparency and reproducibility. In principle, it could be said that the OSI denied the GPL propagation theory at the time of publishing the OSAID definition. Note that I am probably the one who sealed the mandatory argument for training data in the final stage of this definition’s formulation process, and I believe this was the correct judgment.</p>



<p>On the other hand, the FSF and FSF Europe (FSFE) take a stance more faithful to fundamental principles. FSFE declared as of 2021 that “for an AI application to be free, both its training code and training data must be published under a free software license.” That is, to modify or verify the model, one must be able to obtain it including the training data, and therefore both must be free. Also, the FSF itself stated in a 2024 statement, “Under current understanding, for an ML application to be called free, all training data and the scripts processing it must satisfy the four freedoms,” trying to extend the requirements of freedom to data. Thus, FSF/FSFE stands on the position that a model with undisclosed training data is unfree as a whole even if the software part is free.</p>



<p>However, the FSF simultaneously states to the effect that “whether a non-free machine learning application is ethically unjust depends on the case,” mentioning that there can be “legitimate moral reasons” for not being able to publish training data (personal information) of a medical diagnosis AI, for example. In that case, it implies that although that AI is non-free, its use might be ethically permitted due to social utility. One can see an attitude of seeking a compromise between the FSF’s ideal and reality here, but in any case, there is no mistake that the FSF ultimately aims for freedom including training data.</p>



<p>So, does the FSF support the theory of GPL propagation to AI models? Not necessarily. Their claim is closer to an ethical standard or ideal image rather than legal enforceability, and they are not arguing that it applies to models as an interpretation of the current GPL license. Rather, as mentioned before, they are at the stage of trying to create new standards and agreements. Even in the white paper on the Copilot issue funded by the FSF, while legal points such as copyright and license violation are discussed, substantially it has a strong aspect of being told as a GPL compliance problem for users (downstream developers) concerned that they bear the risk of GPL violation if Copilot’s output contains GPL code fragments. This is a caution to developers using AI coding tools rather than GPL application to the model itself, and is different from an approach forcing GPL compliance directly on model providers.</p>



<p>The Software Freedom Conservancy (SFC) naturally has a strong interest in this issue but is also cautious in some respects. The SFC started the protest campaign “Give Up GitHub” against GitHub in 2022, condemning Copilot’s methods as contrary to the philosophy of open source, and is also involved in the Copilot class action. However, in an SFC blog post, regarding this lawsuit, it showed concern about “the risk of interpretations deviating from the principles of the open source community being brought in,” and called on the plaintiffs’ side to comply with community-led GPL enforcement principles as well. The SFC also states that Copilot’s act is an “unprecedented license violation,” and while not fully denying the GPL propagation theory, it can be interpreted as fearing that a judicial precedent undesirable for the community might be created depending on the result of the legal battle. The SFC might be said to be carefully balancing between the aspect of pursuing GPL propagation and the risk of entrusting it to the judiciary.</p>



<p>Finally, what is concerned as the free software camp is that excessive propagation of licenses might conversely invite results that impair freedom. Both OSI and FSF ultimately want to make AI something open that anyone can utilize, but they are carefully assessing whether increasing the purity of legal theory in demands for full data disclosure really leads to achieving the objective. Considering the demerits such as the avoidance of open data due to excessive propagation interpretation or the atrophy effect due to a flurry of lawsuits, I feel that the major organizations share a commonality in that it is essential not to lose sight of the big picture of spreading freedom. Rather than inciting GPL application to models, the pursuit of realistic solutions such as how to make models and data open and which parts should be relaxed in line with reality will likely continue in the future.</p>



<hr>



<h2 id="summary">Summary</h2>



<p>I have looked at the current state of the theory of GPL propagation to AI models above, and as a conclusion, this theory is in a halfway position where “it is not touted as loudly as before, but it has not completely disappeared.” As a result of points such as license violation of training data and reproduction within the model beginning to be scrutinized in lawsuits like the Copilot class action and <em>GEMA v. OpenAI</em>, it even appears that the hurdle for infringement certification is lowering. In fact, the Munich District Court’s judgment deemed model memory as reproduction, and the claim of open source license violation survives in the Copilot litigation.</p>



<p>However, on the other hand, the hurdle for the propagation of licenses like GPL remains high. There is a large gap between infringement being recognized and the conclusion that the entire model must be disclosed under GPL etc. immediately. What the current lawsuits are seeking is also injunctions and damages, not the forced GPL-ization of the model. There are zero examples where the judiciary supported the theory of GPL propagation to models itself, and it is a legally uncharted territory. Even if that claim were attempted somewhere in the future, it would face the legal, technical, and practical counterarguments mentioned earlier.</p>



<p>However, the situation has fluid parts, and there is a possibility that the line will shift depending on the policies of each country and the trends of the community. For example, if pressure from rights holder groups strengthens in Europe, there is a possibility that guidelines including license compliance will be formulated. Also, if a consensus is formed within the community regarding the state of copyleft in the AI era, a new license might appear. If such changes occur, a phase where the theory of propagation to models is re-evaluated will also arrive.</p>



<p>To offer my personal opinion, what is important at this moment is the perspective of how to balance software freedom and freedom in the AI domain. Instead of blindly trying to apply the philosophy of copyleft to AI, it is necessary to think about what is best to maximize freedom while considering the technical nature and industrial structure peculiar to AI. Fortunately, solutions to practical problems such as the open publication of large-scale AI models, dataset cleaning methods, and automated attachment of license notices are already being explored by the open source community. Promoting such voluntary efforts and supporting them with legal frameworks as necessary will likely be the key to balancing freedom and development.</p>



<p>The theory of GPL propagation to models is a point where judgment is divided on whether it is an ideal to be pursued or a nightmare to be avoided. However, as stated in this article, seeing the situation in the current year of 2025, it is not a situation where it will become reality immediately, and the majority of the community is likely maintaining a cautious stance. Although it is speculated that trial and error will continue in the judicial, legislative, and technical aspects in the future, as our community, we need to continue exploring the point of compatibility between technological innovation and software freedom without jumping to hasty conclusions. That process itself can be said to be a new challenge in the AI era on the extension of the free software spirit.</p>



<h2 id="references">References</h2>



<ul>
<li>GitHub Copilot litigation: <a href="https://githubcopilotlitigation.com/">https://githubcopilotlitigation.com/</a></li>



<li>GEMA v. OpenAI Judgment text: <a href="https://aifray.com/wp-content/uploads/2025/11/42-O-14139-24-Endurteil.pdf">https://aifray.com/wp-content/uploads/2025/11/42-O-14139-24-Endurteil.pdf</a></li>



<li>GEMA vs. OpenAI | AI memorisation is a reproduction relevant to copyright law, and the TDM exception does not help in LLM training, Munich I Regional Court holds: <a href="https://www.osborneclarke.com/insights/gema-vs-openai-ai-memorisation-reproduction-relevant-copyright-law-and-tdm-exception-does">https://www.osborneclarke.com/insights/gema-vs-openai-ai-memorisation-reproduction-relevant-copyright-law-and-tdm-exception-does</a></li>



<li>Impressions on GEMA v. OpenAI (Munich I Regional Court) Judgment: <a href="https://shujisado.com/2025/11/15/gema-v-openai/">https://shujisado.com/2025/11/15/gema-v-openai/</a></li>



<li>Draft thought on AI and Copyright, Agency for Cultural Affairs: <a href="https://www.bunka.go.jp/seisaku/bunkashingikai/chosakuken/pdf/94037901_01.pdf">https://www.bunka.go.jp/seisaku/bunkashingikai/chosakuken/pdf/94037901_01.pdf</a></li>



<li>Contract Guidelines on Utilization of AI and Data: <a href="https://www.meti.go.jp/policy/mono_info_service/connected_industries/sharing_and_utilization/20180615001-1.pdf">https://www.meti.go.jp/policy/mono_info_service/connected_industries/sharing_and_utilization/20180615001-1.pdf</a></li>



<li>Open Source AI: <a href="https://opensource.org/ai">https://opensource.org/ai</a></li>



<li>Is publication of complete training data necessary for AI models to be Open Source?: <a href="https://shujisado.com/2025/02/18/need_for_training_data_in_opensource_ai/">https://shujisado.com/2025/02/18/need_for_training_data_in_opensource_ai/</a></li>



<li>Controlling technology at the age of Artificial Intelligence: a Free Software perspective: <a href="https://fsfe.org/freesoftware/artificial-intelligence.en.html">https://fsfe.org/freesoftware/artificial-intelligence.en.html</a></li>



<li>FSF is working on freedom in machine learning applications: <a href="https://www.fsf.org/news/fsf-is-working-on-freedom-in-machine-learning-applications">https://www.fsf.org/news/fsf-is-working-on-freedom-in-machine-learning-applications</a></li>



<li>Give Up GitHub!: <a href="https://sfconservancy.org/GiveUpGitHub/">https://sfconservancy.org/GiveUpGitHub/</a></li>



<li>On the filing of the Class Action Law Suit over GitHub’s Copilot: <a href="https://sfconservancy.org/news/2022/nov/04/class-action-lawsuit-filing-copilot/">https://sfconservancy.org/news/2022/nov/04/class-action-lawsuit-filing-copilot/</a></li>
</ul>



<figure><img data-attachment-id="252" data-permalink="https://shujisado.org/2025/11/27/gpl-propagates-to-ai-models-trained-on-gpl-code/chatgpt-image-2025%e5%b9%b411%e6%9c%8827%e6%97%a5-21_13_41/" data-orig-file="https://shujisado.org/wp-content/uploads/2025/11/chatgpt-image-2025e5b9b411e69c8827e697a5-21_13_41.png" data-orig-size="1536,1024" data-comments-opened="0" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="ChatGPT Image 2025年11月27日 21_13_41" data-image-description="" data-image-caption="" data-medium-file="https://shujisado.org/wp-content/uploads/2025/11/chatgpt-image-2025e5b9b411e69c8827e697a5-21_13_41.png?w=300" data-large-file="https://shujisado.org/wp-content/uploads/2025/11/chatgpt-image-2025e5b9b411e69c8827e697a5-21_13_41.png?w=1024" width="1024" height="682" src="https://shujisado.org/wp-content/uploads/2025/11/chatgpt-image-2025e5b9b411e69c8827e697a5-21_13_41.png?w=1024" alt="" srcset="https://shujisado.org/wp-content/uploads/2025/11/chatgpt-image-2025e5b9b411e69c8827e697a5-21_13_41.png?w=1024 1024w, https://shujisado.org/wp-content/uploads/2025/11/chatgpt-image-2025e5b9b411e69c8827e697a5-21_13_41.png?w=150 150w, https://shujisado.org/wp-content/uploads/2025/11/chatgpt-image-2025e5b9b411e69c8827e697a5-21_13_41.png?w=300 300w, https://shujisado.org/wp-content/uploads/2025/11/chatgpt-image-2025e5b9b411e69c8827e697a5-21_13_41.png?w=768 768w, https://shujisado.org/wp-content/uploads/2025/11/chatgpt-image-2025e5b9b411e69c8827e697a5-21_13_41.png?w=1440 1440w, https://shujisado.org/wp-content/uploads/2025/11/chatgpt-image-2025e5b9b411e69c8827e697a5-21_13_41.png 1536w" sizes="(max-width: 1024px) 100vw, 1024px"></figure>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Arthur Conan Doyle explored men’s mental health through Sherlock Holmes (156 pts)]]></title>
            <link>https://scienceclock.com/arthur-conan-doyle-delved-into-mens-mental-health-through-his-sherlock-holmes-stories/</link>
            <guid>46068015</guid>
            <pubDate>Thu, 27 Nov 2025 10:54:02 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://scienceclock.com/arthur-conan-doyle-delved-into-mens-mental-health-through-his-sherlock-holmes-stories/">https://scienceclock.com/arthur-conan-doyle-delved-into-mens-mental-health-through-his-sherlock-holmes-stories/</a>, See on <a href="https://news.ycombinator.com/item?id=46068015">Hacker News</a></p>
<div id="readability-page-1" class="page"><div itemprop="articleBody">
<p><strong>Note:</strong> This article is republished from <em>The Conversation</em> under a Creative Commons license. It includes links to external sites that may earn a commission for purchases. We did not add these links and have kept the original content intact.</p>



<p>Arthur Conan Doyle was not just one of the world’s best crime fiction writers. He was a progressive wordsmith who brought light to controversial and taboo subjects. One of those taboo subjects was male vulnerability and mental health problems – a topic of personal significance to the author.</p>



<p>Doyle was a <a href="https://theconversation.com/topics/children-10" target="_blank" rel="noopener">vulnerable child</a>. His father, Charles, <a href="https://www.nationalgalleries.org/art-and-artists/artists/charles-altamont-doyle#:%7E:text=Doyle%20did%2C%20however%2C%20suffer%20from,the%20creator%20of%20Sherlock%20Holmes" target="_blank" rel="noopener">was an alcoholic</a>, which led to financial troubles in the family. Charles was admitted to an asylum in 1881 and spent the next 12 years in various <a href="https://www.cambridge.org/core/journals/advances-in-psychiatric-treatment/article/arthur-conan-doyle-the-many-faces-of-sherlock-holmes/3AEE8A3782A8F420740219E88A34FE53" target="_blank" rel="noopener">mental care establishments</a>. So began Doyle’s interest in male vulnerability and mental health.</p>



<p>The character of Sherlock Holmes is a true expression of male vulnerability that does not equate it with weakness. Doyle does not represent Holmes as infallible, but as a man others can relate to – he battles with drug addiction, loneliness and depression. His genius thrives in part because of these vulnerabilities, not despite them.</p>




<p>Many of Doyle’s Sherlock Holmes stories examine male characters facing emotional catastrophe, betrayal or moral dilemmas. In works such as <a href="https://uk.bookshop.org/a/15793/9781913603380" target="_blank" rel="noopener">The Man with the Twisted Lip</a> (1891), <a href="https://uk.bookshop.org/a/15793/9781913603403" target="_blank" rel="noopener">The Adventure of the Engineer’s Thumb</a> (1892) and <a href="https://uk.bookshop.org/a/15793/9781782266594" target="_blank" rel="noopener">The Stockbroker’s Clerk</a> (1894), Holmes’s male clients approach him with problems layered with emotional turmoil, fear and failure.</p>



<p>In The Man with the Twisted Lip, for example, a man named Neville St Clair hides his double life. He tells his family that he is a respectable entrepreneur going to London on business. In reality he is begging on the city streets. He lives this double life due to fear and shame over the inability to pay off his debts. “It was a long fight between my pride and the money,” he explains, “but the dollars won at last.”</p>



<p><strong>Also Read: </strong><a href="https://scienceclock.com/who-looks-smarter-the-quick-thinker-or-the-careful-thinker/"><strong>Who Looks Smarter: The Quick Thinker or the Careful Thinker?</strong></a></p>



<p>“I would have endured imprisonment, ay, even execution, rather than have left my miserable secret as a family blot to my children,” St Clair says. In having his character consider execution to protect his and his family’s reputation, Doyle explored the societal expectations of Victorian masculinity and how men struggled with such pressures.</p>




<p>The Stockbroker’s Clerk also examines male suicide, as well as economic and professional anxieties. When Holmes reveals the crimes of Harry Pinner, the man attempts suicide rather than face prison.</p>



<p>In The Engineer’s Thumb, hydraulic engineer Victor is treated physically by Watson and mentally by Holmes. As Doyle writes: “Round one of his hands he had a handkerchief wrapped, which was mottled all over with bloodstains. He was young, not more than five-and-twenty, I should say, with a strong masculine face; but he was exceedingly pale and gave me the impression of a man who was suffering from some strong agitation, which it took all his strength of mind to control.”</p>



<p>The physical injury marks Victor as a victim of physical violence. Watson suggests that Victor is using all his mental capabilities to keep calm about his severe pain. Holmes treats Victor’s mind as he listens to his story: “Pray lie down there and make yourself absolutely at home. Tell us what you can, but stop when you are tired, and keep up your strength with a little stimulant.”</p>



<p><strong>Also Read: <a href="https://scienceclock.com/study-of-3-million-finnish-adults-finds-non-voters-tend-to-die-earlier/">Study of 3 Million Finnish Adults Finds Non-Voters Tend to Die Earlier</a></strong></p>




<p>Holmes is a protector, a confidante and a comforter in this scene. He provides Victor with breakfast, induces him to lie down and offers him a stimulant (more than likely brandy).</p>



<p>The extremity of violence that Victor has endured has escalated to mental trauma. In having Holmes treat Victor’s mental trauma while Watson treats his physical pain, Doyle showed the importance psychological support for men of the age.</p>



<p>Holmes was a highly popular character. To contemporary readers, his drug use and dysfunctional clients were seen as markers of his genius rather than a reflection of the significant social issues that men faced during this period. But today, they offer a window into the mental struggles of Victorian men, and a point of connection between readers of the past and present.</p>



<hr>



<p><em>Looking for something good? Cut through the noise with a carefully curated selection of the latest releases, live events and exhibitions, straight to your inbox every fortnight, on Fridays. <a href="https://theconversation.com/uk/newsletters/something-good-156" target="_blank" rel="noopener">Sign up here</a>.</em></p>




<hr>



<p><em>This article features references to books that have been included for editorial reasons, and may contain links to <a href="http://bookshop.org/" target="_blank" rel="noopener">bookshop.org</a>. If you click on one of the links and go on to buy something from <a href="http://bookshop.org/" target="_blank" rel="noopener">bookshop.org</a> The Conversation UK may earn a commission.</em></p>



<p><a href="https://theconversation.com/profiles/emma-linford-1297058" target="_blank" rel="noopener">Emma Linford</a>, Honorary research associate, English literature, <em><a href="https://theconversation.com/institutions/university-of-hull-1191" target="_blank" rel="noopener">University of Hull</a></em></p>



<p><strong>This article is republished from <a href="https://theconversation.com/" target="_blank" rel="noopener">The Conversation</a> under a Creative Commons license. Read the <a href="https://theconversation.com/arthur-conan-doyle-explored-mens-mental-health-through-his-sherlock-holmes-stories-246728" target="_blank" rel="noopener">original article</a>.</strong></p>



<hr>







<!-- CONTENT END 1 -->
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Ray Marching Soft Shadows in 2D (2020) (140 pts)]]></title>
            <link>https://www.rykap.com/2020/09/23/distance-fields/</link>
            <guid>46066695</guid>
            <pubDate>Thu, 27 Nov 2025 07:31:24 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.rykap.com/2020/09/23/distance-fields/">https://www.rykap.com/2020/09/23/distance-fields/</a>, See on <a href="https://news.ycombinator.com/item?id=46066695">Hacker News</a></p>
<div id="readability-page-1" class="page"><div role="main">
        




<article>
  

<p><em>Disclaimer: the demos on this page use WebGL features that aren’t available on some mobile devices.</em></p>

<p>A couple of weeks ago I tweeted a video of a toy graphics project (below). It’s not done, but a lot of people liked it which was surprising and fun! A few people asked how it works, so that’s what this post is about.</p>



<p>Under the hood it uses something called a distance field. A distance field is an image like the one below that tells you how far each pixel is from your shape. Light grey pixels are close to the shape and dark grey pixels are far from it.</p>

<img src="https://www.rykap.com/images/ray-marching/distance-field.png">

<p>When the demo starts up, it draws some text on a 2D canvas and generates a distance field of it. It uses <a href="https://github.com/ryankaplan/gpu-distance-field">a library I wrote</a> that generates distance fields really quickly. If you’re curious how the library works, I wrote about that <a href="http://rykap.com/graphics/skew/2016/02/25/voronoi-diagrams/">here</a>.</p>

<p>Our lighting scheme works like this: when processing a particular pixel we consider a ray from it to the light, like so…</p>

<img src="https://www.rykap.com/images/ray-marching/ray-march-1.png">

<p>If the ray intersects a glyph, the pixel we’re shading must be in shadow because there’s something between it and the light.</p>

<p>The simplest way to check this would be to move along the ray in 1px increments, starting from the pixel we’re shading and ending at the light, repeatedly asking the distance field if we’re distance 0 from a shape. This would work, but it’d be really slow.</p>

<p>We could pick some specific length like 30px and move in increments of that size, but then we risk jumping over glyphs that are smaller than 30px. We might think we’re not in shadow when we should be.</p>

<p><strong>Ray marching’s core idea is this: the distance field tells you how far you are from the closest glyph. You can safely advance along your ray by that distance without skipping over any glyphs.</strong></p>

<p>Let’s walk through an example. We start as pictured above and ask the distance field how far we are from any glyph. Turns out in this case that the answer is 95px (pictured left). This means that we can move 95px along our ray without skipping over anything!</p>

<img src="https://www.rykap.com/images/ray-marching/ray-march-2.png">

<p>Now we’re a little closer to the light. We repeat the process until we hit the ascender of the b! If the b glyph weren’t there, we’d have kept going until we hit the light.</p>

<p>Below is a demo that shows the ray marching steps for a given pixel. The red box is the pixel we’re shading, and each circle along the ray represents a ray marching step and the distance from the scene at that step.</p>

<p>Try dragging the light and the pixel around to build an intuition for it.</p>



<p>Below is GLSL to implement this technique. It assumes you’ve defined a function <code>getDistance</code> that samples the distance field.</p>

<div><pre><code><span>vec2</span> <span>rayOrigin</span> <span>=</span> <span>...;</span>
<span>vec2</span> <span>rayDirection</span> <span>=</span> <span>...;</span>

<span>float</span> <span>rayProgress</span> <span>=</span> <span>0</span><span>;</span>
<span>while</span> <span>(</span><span>true</span><span>)</span> <span>{</span>
  <span>if</span> <span>(</span><span>rayProgress</span> <span>&gt;</span> <span>distance</span><span>(</span><span>rayOrigin</span><span>,</span> <span>lightPosition</span><span>))</span> <span>{</span>
    <span>// We hit the light! This pixel is not in shadow.</span>
    <span>return</span> <span>1</span><span>.;</span>
  <span>}</span>

  <span>float</span> <span>sceneDist</span> <span>=</span> <span>getDistance</span><span>(</span>
    <span>rayOrigin</span> <span>+</span> <span>rayProgress</span> <span>*</span> <span>rayDirection</span><span>);</span>
  <span>if</span> <span>(</span><span>sceneDist</span> <span>&lt;=</span> <span>0</span><span>.)</span> <span>{</span>
    <span>// We hit a shape! This pixel is in shadow.</span>
    <span>return</span> <span>0</span><span>.;</span>
  <span>}</span>

  <span>rayProgress</span> <span>+=</span> <span>sceneDist</span><span>;</span>
<span>}</span>
</code></pre></div>

<p>It turns out that some pixels are really expensive to process. So in practice we use a for-loop instead of a while loop – that way we bail out if we’ve done too many steps. A common “slow case” in ray marching is when a ray is parallel to the edge of a shape in the scene…</p>



<p>The approach I’ve described so far will get you a scene that looks like the one below.</p>



<p>It’s cool, but the shadows are sharp which doesn’t look very good. The shadows in the demo look more like this…</p>

<img src="https://www.rykap.com/images/ray-marching/desired-shadows.png">

<p>One big disclaimer is that they’re not physically realistic! Real shadows look like hard shadows where the edges have been fuzzed. This approach does something slightly different: all pixels that were previously in shadow are still fully in shadow. We’ve just added a penumbra of partially shaded pixels around them.</p>

<p>The upside is that they’re pretty and fast to compute, and that’s what I care about! There are three “rules” involved in computing them.</p>

<p><strong>Rule 1:</strong> The closer a ray gets to intersecting a shape, the more its pixel should be shadowed. In the image below there are two similar rays (their distances to the shape pictured in yellow and green). We want the one that gets closer to touching the corner to be more shadowed.</p>

<img src="https://www.rykap.com/images/ray-marching/rule-1.png">

<p>This is cheap to compute because the variable <code>sceneDist</code> tells us how far we are from the closest shape at each ray marching step. So the smallest value of <code>sceneDist</code> across all steps is a good approximation for the yellow and green lines in the image above.</p>

<p><strong>Rule 2:</strong> if the pixel we’re shading is far from the point where it almost intersects a shape, we want the shadow to spread out more.</p>

<img src="https://www.rykap.com/images/ray-marching/rule-2.png">

<p>Consider two pixels along the ray above. One is closer to the almost-intersection and is lighter (its distance is the green line). The other is farther and darker (its distance is the yellow line). In general: the further a pixel is from its almost intersection, the more “in shadow” we should make it.</p>

<p>This is cheap to compute because the variable <code>rayProgress</code> is the length of the green and yellow lines in the image above.</p>

<p>So: we previously returned <code>1.0</code> for pixels that weren’t in shadow. To implement rules 1 and 2, we compute <code>sceneDist / rayProgress</code> on each ray marching step, keep track of its minimum value, and return that instead.</p>

<div><pre><code><span>vec2</span> <span>rayOrigin</span> <span>=</span> <span>...;</span>
<span>vec2</span> <span>rayDirection</span> <span>=</span> <span>...;</span>
<span>float</span> <span>rayProgress</span> <span>=</span> <span>0</span><span>.;</span>
<span>float</span> <span>stopAt</span> <span>=</span> <span>distance</span><span>(</span><span>samplePt</span><span>,</span> <span>lightPosition</span><span>);</span>
<span>float</span> <span>lightContribution</span> <span>=</span> <span>1</span><span>.;</span>
<span>for</span> <span>(</span><span>int</span> <span>i</span> <span>=</span> <span>0</span><span>;</span> <span>i</span> <span>&lt;</span> <span>64</span><span>;</span> <span>i</span><span>++</span><span>)</span> <span>{</span>
  <span>if</span> <span>(</span><span>rayProgress</span> <span>&gt;</span> <span>stopAt</span><span>)</span> <span>{</span>
    <span>return</span> <span>lightContribution</span><span>;</span>
  <span>}</span>

  <span>// `getDistance` samples our distance field texture.</span>
  <span>float</span> <span>sceneDist</span> <span>=</span> <span>getDistance</span><span>(</span>
    <span>rayOrigin</span> <span>+</span> <span>rayProgress</span> <span>*</span> <span>rayDirection</span><span>);</span>
  <span>if</span> <span>(</span><span>sceneDist</span> <span>&lt;=</span> <span>0</span><span>.)</span> <span>{</span>
    <span>// We hit a shape! This pixel is in shadow.</span>
    <span>return</span> <span>0</span><span>.;</span>
  <span>}</span>

  <span>lightContribution</span> <span>=</span> <span>min</span><span>(</span>
    <span>lightContribution</span><span>,</span>
    <span>sceneDist</span> <span>/</span> <span>rayProgress</span>
  <span>);</span>

  <span>rayProgress</span> <span>+=</span> <span>sceneDist</span><span>;</span>
<span>}</span>

<span>// Ray-marching took more than 64 steps!</span>
<span>return</span> <span>0</span><span>.;</span>
</code></pre></div>

<p>This ratio feels kind of magical to me because it doesn’t correspond to any physical value. So let’s build some intuition for it by thinking through why it might take on particular values…</p>

<ul>
  <li>
    <p>If <code>sceneDist / rayProgress &gt;= 1</code>, then either <code>sceneDist</code> is big or <code>rayProgress</code> is small (relative to each other). In the former case we’re far from any shapes and we shouldn’t be in shadow, so a light value of <code>1</code> makes sense. In the latter case, the pixel we’re shadowing is really close to an object casting a shadow and the shadow isn’t fuzzy yet, so a light value of <code>1</code> makes sense.</p>
  </li>
  <li>
    <p>The ratio is <code>0</code> only when <code>sceneDist</code> is <code>0</code>. This corresponds to rays that intersect an object and whose pixels are in shadow.</p>
  </li>
</ul>

<p>And here’s a demo of what we have so far…</p>



<p><strong>Rule #3</strong> is the most straightforward one: light gets weaker the further you get from it.</p>

<p>Instead of returning the minimum value of <code>sceneDist / rayProgress</code> verbatim, we multiply it by a <code>distanceFactor</code> which is <code>1</code> right next to the light, <code>0</code> far away from it, and gets quadratically smaller as you move away from it.</p>

<p>All together, the code for the approach so far looks like this…</p>

<div><pre><code><span>vec2</span> <span>rayOrigin</span> <span>=</span> <span>...;</span>
<span>vec2</span> <span>rayDirection</span> <span>=</span> <span>...;</span>
<span>float</span> <span>rayProgress</span> <span>=</span> <span>0</span><span>.;</span>
<span>float</span> <span>stopAt</span> <span>=</span> <span>distance</span><span>(</span><span>samplePt</span><span>,</span> <span>lightPosition</span><span>);</span>
<span>float</span> <span>lightContribution</span> <span>=</span> <span>1</span><span>.;</span>
<span>for</span> <span>(</span><span>int</span> <span>i</span> <span>=</span> <span>0</span><span>;</span> <span>i</span> <span>&lt;</span> <span>64</span><span>;</span> <span>i</span><span>++</span><span>)</span> <span>{</span>
  <span>if</span> <span>(</span><span>rayProgress</span> <span>&gt;</span> <span>stopAt</span><span>)</span> <span>{</span>
    <span>// We hit the light!</span>
    <span>float</span> <span>LIGHT_RADIUS_PX</span> <span>=</span> <span>800</span><span>.;</span>

    <span>// fadeRatio is 1.0 next to the light and 0. at</span>
    <span>// LIGHT_RADIUS_PX away.</span>
    <span>float</span> <span>fadeRatio</span> <span>=</span>
      <span>1</span><span>.</span><span>0</span> <span>-</span> <span>clamp</span><span>(</span><span>stopAt</span> <span>/</span> <span>LIGHT_RADIUS_PX</span><span>,</span> <span>0</span><span>.,</span> <span>1</span><span>.);</span>

    <span>// We'd like the light to fade off quadratically instead of</span>
    <span>// linearly.</span>
    <span>float</span> <span>distanceFactor</span> <span>=</span> <span>pow</span><span>(</span><span>fadeRatio</span><span>,</span> <span>2</span><span>.);</span>
    <span>return</span> <span>lightContribution</span> <span>*</span> <span>distanceFactor</span><span>;</span>
  <span>}</span>

  <span>// `getDistance` samples our distance field texture.</span>
  <span>float</span> <span>sceneDist</span> <span>=</span> <span>getDistance</span><span>(</span><span>rayOrigin</span> <span>+</span> <span>rayProgress</span> <span>*</span> <span>rayDirection</span><span>);</span>
  <span>if</span> <span>(</span><span>sceneDist</span> <span>&lt;=</span> <span>0</span><span>.)</span> <span>{</span>
    <span>// We hit a shape! This pixel is in shadow.</span>
    <span>return</span> <span>0</span><span>.;</span>
  <span>}</span>

  <span>lightContribution</span> <span>=</span> <span>min</span><span>(</span>
    <span>lightContribution</span><span>,</span>
    <span>sceneDist</span> <span>/</span> <span>rayProgress</span>
  <span>);</span>

  <span>rayProgress</span> <span>+=</span> <span>sceneDist</span><span>;</span>
<span>}</span>

<span>// Ray-marching took more than 64 steps!</span>
<span>return</span> <span>0</span><span>.;</span>
</code></pre></div>

<p>I forget where I found this soft-shadow technique, but I definitely didn’t invent it. Inigo Quilez <a href="https://www.iquilezles.org/www/articles/rmshadows/rmshadows.htm">has a great post on it</a> where he talks about using it in 3D.</p>

<p>Inigo’s post also talks about a gotcha with this approach that you might have noticed in the demos above: it causes banding artifacts. This is because Rule 1 assumes that the smallest value of <code>sceneDist</code> across all steps is a good approximation for the distance from a ray to the scene. This is not always true because we sometimes take very few ray marching steps.</p>

<p>So in my demo I use an improved approximation that Inigo writes about in his post. I also use another trick that is more effective but less performant: instead of advancing by <code>sceneDist</code> on each ray marching step, I advance by something like <code>sceneDist * randomJitter</code> where <code>randomJitter</code> is between <code>0</code> and <code>1</code>.</p>

<p>This improves the approximation because we’re adding more steps to our ray march. But we could do that by advancing by <code>sceneDist * .3</code>. The random jitter ensures that pixels next to each other don’t end up in the same band. This makes the result a little grainy which isn’t great. But I think looks better than banding… This is an aspect of the demo that I’m still not satisfied with, so if you have ideas for how to improve it please tell me!</p>

<p>Overall my demo has a few extra tweaks that I might write about in future but this is the core of it. Thanks for reading! If you have questions or comments, let me know <a href="https://twitter.com/ryanjkaplan">on Twitter</a>.</p>

<p><em>Thank you to Jessica Liu, Susan Wang, Matt Nichols and Kenrick Rilee for giving feedback on early drafts of this post! Also, if you enjoyed this post you might enjoy working with me at <a href="https://www.figma.com/careers/">Figma</a>!</em></p>

</article>






  
  
  






      </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Mixpanel Security Breach (144 pts)]]></title>
            <link>https://mixpanel.com/blog/sms-security-incident/</link>
            <guid>46066522</guid>
            <pubDate>Thu, 27 Nov 2025 07:02:40 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://mixpanel.com/blog/sms-security-incident/">https://mixpanel.com/blog/sms-security-incident/</a>, See on <a href="https://news.ycombinator.com/item?id=46066522">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><div><p><span>Out of transparency and our desire to share with our community, this blog post contains key information about a recent security incident that impacted a limited number of our customers.&nbsp;On November 8th, 2025, Mixpanel detected a smishing campaign and promptly executed our incident response processes. We took comprehensive steps to contain and eradicate unauthorized access and secure impacted user accounts. We engaged external cybersecurity partners to remediate and respond to the incident.</span></p></div><div><p><span>We proactively communicated with all impacted customers.&nbsp;If you have not heard from us directly, you were not impacted. We continue to prioritize security as a core tenant of our company, products and services. We are committed to supporting our customers and communicating transparently about this incident.&nbsp;</span></p></div><div><p><h2><span><strong>What we did in response</strong></span></h2></p></div><div><ul><span><li>Secured affected accounts</li><li>Revoked all active sessions and sign-ins</li><li>Rotated compromised Mixpanel credentials for impacted accounts</li><li>Blocked malicious IP addresses</li><li>Registered IOCs in our SIEM platform</li><li>Performed global password resets for all Mixpanel employees</li><li>Engaged third-party forensics firm to advise on containment and eradication measures</li><li>Performed a forensic review of authentication, session, and export logs across impacted accounts</li><li>Implemented additional controls to detect and block similar activity going forward.</li><li>Engaged with law enforcement and external cybersecurity advisors</li></span></ul></div><div><p><h2><span><strong>What you should know</strong></span></h2></p></div><div><ul><span><li>If you received a communication from us, please review it for the steps we have taken to secure your account, as well as next steps.&nbsp;</li><li>If you did not receive a communication from us, no action is required. Your accounts were not impacted.</li></span></ul></div><div><p><span>If you have any questions about this incident, please contact support@mixpanel.com.</span></p></div></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[The Nerd Reich – Silicon Valley Fascism and the War on Democracy (202 pts)]]></title>
            <link>https://www.simonandschuster.com/books/The-Nerd-Reich/Gil-Duran/9781668221402</link>
            <guid>46066482</guid>
            <pubDate>Thu, 27 Nov 2025 06:53:17 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.simonandschuster.com/books/The-Nerd-Reich/Gil-Duran/9781668221402">https://www.simonandschuster.com/books/The-Nerd-Reich/Gil-Duran/9781668221402</a>, See on <a href="https://news.ycombinator.com/item?id=46066482">Hacker News</a></p>
Couldn't get https://www.simonandschuster.com/books/The-Nerd-Reich/Gil-Duran/9781668221402: Error: Request failed with status code 403]]></description>
        </item>
        <item>
            <title><![CDATA[Linux Kernel Explorer (419 pts)]]></title>
            <link>https://reverser.dev/linux-kernel-explorer</link>
            <guid>46066280</guid>
            <pubDate>Thu, 27 Nov 2025 06:17:37 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://reverser.dev/linux-kernel-explorer">https://reverser.dev/linux-kernel-explorer</a>, See on <a href="https://news.ycombinator.com/item?id=46066280">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>The kernel isn't a process—it's the system. It serves user processes, reacts to context, and enforces separation and control.</p><ul><li><strong>The Kernel Is Not a Process</strong>: It's the always-present authority bridging hardware and software.</li><li><strong>Serving the Process</strong>: Orchestrates syscalls, interrupts, and scheduling to keep user tasks running.</li><li><strong>System of Layers</strong>: Virtual, mapped, isolated, and controlled—structure at runtime.</li></ul><div><h4>📚 Study Files</h4><div><p><span>init/main.c</span></p><p><span>kernel/fork.c</span></p><p><span>include/linux/sched.h</span></p><p><span>arch/x86/kernel/entry_64.S</span></p></div></div><div><div><div><p>1</p><!-- --><p>. </p><!-- --><p>What is the fundamental difference between the kernel and a process?</p></div><div><p><span>A<!-- -->.</span><span>The kernel is a special process with elevated privileges</span></p><p><span>B<!-- -->.</span><span>The kernel is not a process—it's the system itself that serves processes</span></p><p><span>C<!-- -->.</span><span>The kernel is just a library that processes link against</span></p><p><span>D<!-- -->.</span><span>There is no difference; they are the same thing</span></p></div></div><div><div><p>2</p><!-- --><p>. </p><!-- --><p>How does the kernel primarily serve user processes?</p></div><div><p><span>A<!-- -->.</span><span>By running as a background daemon</span></p><p><span>B<!-- -->.</span><span>By orchestrating syscalls, interrupts, and scheduling</span></p><p><span>C<!-- -->.</span><span>By providing a GUI interface</span></p><p><span>D<!-- -->.</span><span>By compiling user code</span></p></div></div><div><div><p>3</p><!-- --><p>. </p><!-- --><p>What characterizes the kernel's system of layers?</p></div><div><p><span>A<!-- -->.</span><span>Physical, tangible, and direct</span></p><p><span>B<!-- -->.</span><span>Simple and flat with no hierarchy</span></p><p><span>C<!-- -->.</span><span>Virtual, mapped, isolated, and controlled</span></p><p><span>D<!-- -->.</span><span>User-accessible and modifiable</span></p></div></div></div></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Music eases surgery and speeds recovery, study finds (155 pts)]]></title>
            <link>https://www.bbc.com/news/articles/c231dv9zpz3o</link>
            <guid>46065817</guid>
            <pubDate>Thu, 27 Nov 2025 04:55:57 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.bbc.com/news/articles/c231dv9zpz3o">https://www.bbc.com/news/articles/c231dv9zpz3o</a>, See on <a href="https://news.ycombinator.com/item?id=46065817">Hacker News</a></p>
<div id="readability-page-1" class="page"><article><p data-component="headline-block"><h2>Music eases surgery and speeds recovery, Indian study finds</h2></p><div data-testid="byline-new" data-component="byline-block"><p><span data-testid="byline-new-contributors"><div data-testid="byline-new-contributors-contributor-0"><p><span>Soutik Biswas</span><span data-testid="byline-new-contributors-contributor-0-role-location">India correspondent</span></p></div></span></p></div><figure><div data-component="image-block"><p><img src="https://static.files.bbci.co.uk/bbcdotcom/web/20251120-101800-81b2dab7dc-web-2.34.1-2/grey-placeholder.png" aria-label="image unavailable"><img sizes="(min-width: 1280px) 50vw, (min-width: 1008px) 66vw, 96vw" srcset="https://ichef.bbci.co.uk/news/240/cpsprodpb/6712/live/6ce97290-c89b-11f0-bd7c-1744b857cc58.jpg.webp 240w,https://ichef.bbci.co.uk/news/320/cpsprodpb/6712/live/6ce97290-c89b-11f0-bd7c-1744b857cc58.jpg.webp 320w,https://ichef.bbci.co.uk/news/480/cpsprodpb/6712/live/6ce97290-c89b-11f0-bd7c-1744b857cc58.jpg.webp 480w,https://ichef.bbci.co.uk/news/640/cpsprodpb/6712/live/6ce97290-c89b-11f0-bd7c-1744b857cc58.jpg.webp 640w,https://ichef.bbci.co.uk/news/800/cpsprodpb/6712/live/6ce97290-c89b-11f0-bd7c-1744b857cc58.jpg.webp 800w,https://ichef.bbci.co.uk/news/1024/cpsprodpb/6712/live/6ce97290-c89b-11f0-bd7c-1744b857cc58.jpg.webp 1024w,https://ichef.bbci.co.uk/news/1536/cpsprodpb/6712/live/6ce97290-c89b-11f0-bd7c-1744b857cc58.jpg.webp 1536w" src="https://ichef.bbci.co.uk/news/480/cpsprodpb/6712/live/6ce97290-c89b-11f0-bd7c-1744b857cc58.jpg.webp" loading="eager" alt="BBC A patient with headphones playing music during surgery in a hospital in Delhi"><span>BBC</span></p></div><p data-component="caption-block"><figcaption>A patient with headphones playing music during surgery in a hospital in Delhi</figcaption></p></figure><div data-component="text-block"><p>Under the harsh lights of an operating theatre in the Indian capital, Delhi, a woman lies motionless as surgeons prepare to remove her gallbladder.</p><p>She is under general anaesthesia: unconscious, insensate and rendered completely still by a blend of drugs that induce deep sleep, block memory, blunt pain and temporarily paralyse her muscles.</p><p>Yet, amid the hum of monitors and the steady rhythm of the surgical team, a gentle stream of flute music plays through the headphones placed over her ears.</p><p>Even as the drugs silence much of her brain, its auditory pathway remains partly active. When she wakes up, she will regain consciousness more quickly and clearly because she required lower doses of anaesthetic drugs such as propofol and opioid painkillers than patients who heard no music.</p><p>That, at least, is what a <a target="_blank" href="https://mmd.iammonline.com/index.php/musmed/article/view/1111">new peer-reviewed study</a> from Delhi's Maulana Azad Medical College and Lok Nayak Hospital suggests. The research, published in the journal Music and Medicine, offers some of the strongest evidence yet that music played during general anaesthesia can modestly but meaningfully reduce drug requirements and improve recovery.</p><p>The study focuses on patients undergoing laparoscopic cholecystectomy, the standard keyhole operation to remove the gallbladder. The procedure is short - usually under an hour - and demands a particularly swift, "clear-headed" recovery.</p><p>To understand why the researchers turned to music, it helps to decode the modern practice of anaesthesia.</p><p>"Our aim is early discharge after surgery," says Dr Farah Husain, senior specialist in anaesthesia and certified music therapist for the study. "Patients need to wake up clear-headed, alert and oriented, and ideally pain-free. With better pain management, the stress response is curtailed." </p><p>Achieving that requires a carefully balanced mix of five or six drugs that together keep the patient asleep, block pain, prevent memory of the surgery and relax the muscles.</p></div><figure><div data-component="image-block"><p><img src="https://static.files.bbci.co.uk/bbcdotcom/web/20251120-101800-81b2dab7dc-web-2.34.1-2/grey-placeholder.png" aria-label="image unavailable"><img sizes="(min-width: 1280px) 50vw, (min-width: 1008px) 66vw, 96vw" srcset="https://ichef.bbci.co.uk/news/240/cpsprodpb/1dd7/live/f3017100-c2c1-11f0-8669-5560f5c90fbe.jpg.webp 240w,https://ichef.bbci.co.uk/news/320/cpsprodpb/1dd7/live/f3017100-c2c1-11f0-8669-5560f5c90fbe.jpg.webp 320w,https://ichef.bbci.co.uk/news/480/cpsprodpb/1dd7/live/f3017100-c2c1-11f0-8669-5560f5c90fbe.jpg.webp 480w,https://ichef.bbci.co.uk/news/640/cpsprodpb/1dd7/live/f3017100-c2c1-11f0-8669-5560f5c90fbe.jpg.webp 640w,https://ichef.bbci.co.uk/news/800/cpsprodpb/1dd7/live/f3017100-c2c1-11f0-8669-5560f5c90fbe.jpg.webp 800w,https://ichef.bbci.co.uk/news/1024/cpsprodpb/1dd7/live/f3017100-c2c1-11f0-8669-5560f5c90fbe.jpg.webp 1024w,https://ichef.bbci.co.uk/news/1536/cpsprodpb/1dd7/live/f3017100-c2c1-11f0-8669-5560f5c90fbe.jpg.webp 1536w" src="https://ichef.bbci.co.uk/news/480/cpsprodpb/1dd7/live/f3017100-c2c1-11f0-8669-5560f5c90fbe.jpg.webp" loading="lazy" alt="Getty Images Indian surgeons medical team performing surgery in operation theater at hospital"><span>Getty Images</span></p></div><p data-component="caption-block"><figcaption>Patients need to wake up clear-headed and ideally pain-free after surgery</figcaption></p></figure><div data-component="text-block"><p>In procedures like laparoscopic gallbladder removal, anaesthesiologists now often supplement this drug regimen with regional "blocks" - ultrasound-guided injections that numb nerves in the abdominal wall. </p><p>"General anaesthesia plus blocks is the norm," says Dr Tanvi Goel, primary investigator and a former senior resident of Maulana Azad Medical College. "We've been doing this for decades."</p><p>But the body does not take to surgery easily. Even under anaesthesia, it reacts: heart rate rises, hormones surge, blood pressure spikes. Reducing and managing this cascade is one of the central goals of modern surgical care. Dr Husain explains that the stress response can slow recovery and worsen inflammation, highlighting why careful management is so important.</p><p>The stress starts even before the first cut, with intubation - the insertion of a breathing tube into the windpipe.</p><p>To do this, the anaesthesiologist uses a laryngoscope to lift the tongue and soft tissues at the base of the throat, obtain a clear view of the vocal cords, and guide the tube into the trachea. It's a routine step in general anaesthesia that keeps the airway open and allows precise control of the patient's breathing while they are unconscious.</p><p>"The laryngoscopy and intubation are considered the most stressful response during general anaesthesia," says Dr Sonia Wadhawan, director-professor of anaesthesia and intensive care at Maulana Azad Medical College and supervisor of the study.</p><p>"Although the patient is unconscious and will remember nothing, their body still reacts to the stress with changes in heart rate, blood pressure, and stress hormones."</p><p>To be sure, the drugs have evolved. The old ether masks have vanished. In their place are intravenous agents - most notably propofol, the hypnotic made infamous by <a target="_self" href="https://www.bbc.co.uk/news/newsbeat-15634083">Michael Jackson's death</a> but prized in operating theatres for its rapid onset and clean recovery. "Propofol acts within about 12 seconds," notes Dr Goel. "We prefer it for short surgeries like laparoscopic cholecystectomy because it avoids the 'hangover' caused by inhalational gases."</p><p>The team of researchers wanted to know whether music could reduce how much propofol and fentanyl (an opioid painkiller) patients required. Less drugs means faster awakening, steadier vital signs and reduced side effects.</p><p>So they designed a study. A pilot involving eight patients led to a full 11-month trial of 56 adults, aged roughly 20 to 45, randomly assigned to two groups. All received the same five-drug regimen: a drug that prevents nausea and vomiting, a sedative, fentanyl, propofol and a muscle relaxant. Both groups wore noise-cancelling headphones - but only one heard music.</p><p>"We asked patients to select from two calming instrumental pieces - soft flute or piano," says Dr Husain. "The unconscious mind still has areas that remain active. Even if the music isn't explicitly recalled, implicit awareness can lead to beneficial effects."</p></div><figure><div data-component="image-block"><p><img src="https://static.files.bbci.co.uk/bbcdotcom/web/20251120-101800-81b2dab7dc-web-2.34.1-2/grey-placeholder.png" aria-label="image unavailable"><img sizes="(min-width: 1280px) 50vw, (min-width: 1008px) 66vw, 96vw" srcset="https://ichef.bbci.co.uk/news/240/cpsprodpb/dd22/live/a51ebd90-c2c0-11f0-bd8a-f3f60edded8d.jpg.webp 240w,https://ichef.bbci.co.uk/news/320/cpsprodpb/dd22/live/a51ebd90-c2c0-11f0-bd8a-f3f60edded8d.jpg.webp 320w,https://ichef.bbci.co.uk/news/480/cpsprodpb/dd22/live/a51ebd90-c2c0-11f0-bd8a-f3f60edded8d.jpg.webp 480w,https://ichef.bbci.co.uk/news/640/cpsprodpb/dd22/live/a51ebd90-c2c0-11f0-bd8a-f3f60edded8d.jpg.webp 640w,https://ichef.bbci.co.uk/news/800/cpsprodpb/dd22/live/a51ebd90-c2c0-11f0-bd8a-f3f60edded8d.jpg.webp 800w,https://ichef.bbci.co.uk/news/1024/cpsprodpb/dd22/live/a51ebd90-c2c0-11f0-bd8a-f3f60edded8d.jpg.webp 1024w,https://ichef.bbci.co.uk/news/1536/cpsprodpb/dd22/live/a51ebd90-c2c0-11f0-bd8a-f3f60edded8d.jpg.webp 1536w" src="https://ichef.bbci.co.uk/news/480/cpsprodpb/dd22/live/a51ebd90-c2c0-11f0-bd8a-f3f60edded8d.jpg.webp" loading="lazy" alt="A pilot on eight patients led to a full trial of 56 adults randomly assigned to two groups "></p></div><p data-component="caption-block"><figcaption>A pilot involving eight patients led to a full trial of 56 adults randomly assigned to two groups </figcaption></p></figure><div data-component="text-block"><p>The results were striking.</p><p>Patients exposed to music required lower doses of propofol and fentanyl. They experienced smoother recoveries, lower cortisol or stress-hormone levels and a much better control of blood pressure during the surgery. "Since the ability to hear remains intact under anaesthesia," the researchers write, "music can still shape the brain's internal state."</p><p>Clearly, music seemed to quieten the internal storm. "The auditory pathway remains active even when you're unconscious," says Dr Wadhawan. "You may not remember the music, but the brain registers it."</p><p>The idea that the mind behind the anaesthetic veil is not entirely silent has long intrigued scientists. Rare cases of "intraoperative awareness" show patients recalling fragments of operating-room conversation. </p><p>If the brain is capable of picking up and remembering stressful experiences during surgery - even when a patient is unconscious - then it might also be able to register positive or comforting experiences, like music, even without conscious memory.</p><p>"We're only beginning to explore how the unconscious mind responds to non-pharmacological interventions like music," says Dr Husain. "It's a way of humanising the operating room."</p><p>Music therapy is not new to medicine; it has long been used in psychiatry, stroke rehabilitation and palliative care. But its entry into the intensely technical, machine-governed world of anaesthesia marks a quiet shift.</p><p>If such a simple intervention can reduce drug use and speed recovery - even modestly - it could reshape how hospitals think about surgical wellbeing.</p><p>As the research team prepares its next study exploring music-aided sedation, building on earlier findings, one truth is already humming through the data: even when the body is still and the mind asleep, it appears a few gentle notes can help the healing begin.</p></div></article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Coq: The World's Best Macro Assembler? (2013) [pdf] (115 pts)]]></title>
            <link>https://nickbenton.name/coqasm.pdf</link>
            <guid>46065698</guid>
            <pubDate>Thu, 27 Nov 2025 04:34:56 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://nickbenton.name/coqasm.pdf">https://nickbenton.name/coqasm.pdf</a>, See on <a href="https://news.ycombinator.com/item?id=46065698">Hacker News</a></p>
&lt;Not HTML&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[DIY NAS: 2026 Edition (318 pts)]]></title>
            <link>https://blog.briancmoses.com/2025/11/diy-nas-2026-edition.html</link>
            <guid>46065034</guid>
            <pubDate>Thu, 27 Nov 2025 02:54:23 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://blog.briancmoses.com/2025/11/diy-nas-2026-edition.html">https://blog.briancmoses.com/2025/11/diy-nas-2026-edition.html</a>, See on <a href="https://news.ycombinator.com/item?id=46065034">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>Fourteen years ago, my storage needs outpaced my capacity and I began to look into building a network attached storage server. I had a few criteria in mind and was curious to see if anyone had _ recently_ shared something similar, but I couldn’t find anything that was relevant.</p>

<p>In fact,  I found that the communities I was looking for answers in were actively hostile towards what I wanted to do.  This resulted in my decision <a href="https://blog.briancmoses.com/2012/01/do-it-yourself-network-attached-storage.html" title="My First DIY NAS: Building a NAS: Choosing Hardware">to build my own DIY NAS</a> and share that as one of my very first blogs.</p>

<p>Much to my surprise, people were very interested in that blog! Ever since, I’ve been building a similar DIY NAS machine almost every year trying to satisfy the curiosity of other prospective DIY NAS builders.</p>

<p>Here are those criteria:</p>

<ol>
  <li><strong>Small form factor</strong>: It’s not the case for me any more, but at the time the space was limited in my office. I always assume that space in everybody’s office is limited. As a result, I want my DIY NAS builds to occupy as little of that office space as I can.</li>
  <li><strong>At least six drive bays</strong>: Back when I built my NAS, it took about four drives’ worth of storage to meet my storage needs. Plus I desired two empty drive bays for future use. However, in the years since hard drive capacities have increased dramatically. At some point in the future, I may reduce this to four drive bays.</li>
  <li><strong>An integrated, low power CPU</strong>: I intend my DIY NAS to run 24 hours a day, 7 days a week, and 52 weeks a year. When it comes to power consumption, that can do some damage on your electric bill! Thankfully our electricity here isn’t as expensive as others’ in the United States, or even further outside its borders, but I try and keep power consumption in mind when picking components for a DIY NAS build.</li>
  <li><strong>Homelab potential</strong>: It does not take up a lot of CPU horsepower for a NAS to serve up files, which means that on modern hardware there’s a lot of untapped potential in a DIY NAS for virtual machines or containers to self-host services.</li>
</ol>

<p>It’s important to remember that <strong><em>these are my criteria</em></strong>, and not necessarily yours. Every DIY NAS builder should be making their own list of criteria and reconcile all of their component purchases against the criteria that’s important to them.</p>

<h2 id="is-it-even-a-good-time-to-build-a-nas">Is it even a good time to build a NAS?</h2>

<p>As I prepared to build this NAS, component prices disappointed me. Hard drives, SSDs, and RAM prices were all rising. Based on what I’ve been told, I expect Intel CPU prices to increase as well. My contact at Topton has been encouraging me to stock up on motherboards while they still have some in inventory. Based on what’s been explained to me, I expect the motherboard’s prices to rise and for their availability to potentially dwindle.</p>

<p><a href="https://blog.briancmoses.com/images/2026/diynas/ssd_prices.png" target="_blank"><img src="https://blog.briancmoses.com/images/2026/diynas/ssd_prices_600.png" alt="1TB NVMe SSD prices"></a></p>

<p>In short, the economy sucks and the price of DIY NAS components is a pretty good reflection of just how sucky things are becoming. I briefly considered not publishing a DIY NAS build this year hoping that things would improve a few months down the road. But then I asked myself, “What if it’s even worse in a few months?”</p>

<p>I sure hope things get better, but I fear and expect that they’ll get worse.</p>

<h2 id="motherboard-and-cpu">Motherboard and CPU</h2>
<p>I built <a href="https://blog.briancmoses.com/2023/03/diy-nas-2023-edition.html" title="DIY NAS: 2023 Edition">my first DIY NAS with a Topton motherboard in 2023</a>. Each DIY NAS since then has also featured a Topton motherboard. My only complaint about the motherboards has been that buying them from one of the Chinese e-tail sites like AliExpress is considered problematic by some. With every DIY NAS build, I try and go through all the motherboards that I can find  while searching for something with a better value proposition, but for each of the past three years I’ve landed on the latest offering from Topton.</p>

<p><a href="https://s.click.aliexpress.com/e/_c2zJj1aF" target="_blank" title="Topton N22 N355 Mini-ITX NAS Motherboard (8xSATA3, 2xM.2, 1x10Gbps, 2x2.5Gbps)"><img src="https://blog.briancmoses.com/images/2026/diynas/pp/topton_n22_175.png"></a>For the <em>DIY NAS: 2026 Edition</em>, I chose the <a href="https://s.click.aliexpress.com/e/_c2zJj1aF" title="Topton N22 N355 Mini-ITX NAS Motherboard (8xSATA3, 2xM.2, 1x10Gbps, 2x2.5Gbps)">Topton N22 motherboard</a> with the <a href="https://www.intel.com/content/www/us/en/products/sku/231803/intel-processor-n100-6m-cache-up-to-3-40-ghz/specifications.html" title="Intel® Processor N100">Intel Core 3 N355</a> CPU. The motherboard is similar to last year’s <a href="https://www.ebay.com/itm/127014563991" title="Topton N18 N100/N150/N305 Mini-ITX NAS Motherboard:6xSATA/2xM.2/2x2.5GbE/1x10GbE">Topton N18</a> but has incrementally more compelling features, particularly the extra 2 SATA ports, the PCI-e x1 slot, and the N355 CPU!</p>

<ul>
  <li>Mini-ITX Form Factor</li>
  <li><a href="https://www.intel.com/content/www/us/en/products/sku/231803/intel-processor-n100-6m-cache-up-to-3-40-ghz/specifications.html" target="_blank" title="Intel® Processor N100">Intel® Processor Core 3 N355</a>
    <ul>
      <li>8 cores / 8 threads / Max Turbo 3.9GHz</li>
      <li>15 W TDP</li>
      <li>Integrated GPU with Intel Quick Sync Video</li>
    </ul>
  </li>
  <li>1 x DDR5 SO-DIMM</li>
  <li>8 x SATA 3.0 Ports (Asmedia ASM1164)</li>
  <li>2 x M.2 NVMe Slots (PCIe 3.0 x1)</li>
  <li>1 x 10Gbps NIC (Marvell AQC113C)</li>
  <li>2 x 2.5Gbps NICs (Intel i226-V)</li>
  <li>1 x PCI-e x1 or M.2 E-Key slot</li>
</ul>

<p>I opted for the motherboard with the <a href="https://www.intel.com/content/www/us/en/products/sku/231803/intel-processor-n100-6m-cache-up-to-3-40-ghz/specifications.html" target="_blank" title="Intel® Processor N100">Intel Core 3 N355</a> CPU. This makes the server a more capable homelab machine than prior years’ DIY NAS builds. The extra cores and threads come in handy for streaming media, replacing your cloud storage, facilitating home automation, hosting game servers, etc.</p>

<h2 id="case">Case</h2>
<p>Just like Topton has been making great motherboards for DIY NAS machines, JONSBO has been steadily releasing great cases for DIY NAS machines. This year SilverStone Technology released a new case, the <a href="https://www.amazon.com/SilverStone-Technology-hot-swappable-Performance-SST-CS383/dp/B0FDPNRYG9?&amp;linkCode=ll1&amp;tag=diynas2026-20&amp;linkId=61605bf99f3551af48cd50574aa64608&amp;language=en_US&amp;ref_=as_li_ss_tl" target="_blank" title="SilverStone Technology CS383 8-Bay hot-swappable high Performance Full Tower NAS Chassis, SST-CS383">CS383</a> (<a href="https://www.silverstonetek.com/en/product/info/computer-chassis/cs383/" title="SilverStone Technology CS383 Specs">specs</a>) which I was <strong><em>very interested</em></strong> in buying one for the <em>DIY NAS: 2026 Edition</em>. Unfortunately it carries a pretty hefty price tag to go along with all of its incredible features!</p>

<p><a href="https://www.amazon.com/gp/product/B0D1YH7D3J?th=1&amp;linkCode=ll1&amp;tag=diynas2026-20&amp;linkId=5636db0ca3c62e86da5f02b641c0f283&amp;language=en_US&amp;ref_=as_li_ss_tl" target="_blank" title="JONSBO N4 Black NAS Pc Case, Walnut Wood, 8-Drive Bay/6 x 3.5-inch HDD (4 hot-swap,2 Non hot-swap),2 x 2.5-inch SSD, Micro ATX Chassis"><img src="https://blog.briancmoses.com/images/2026/diynas/pp/jonsbo-n4_200.png"></a>The <a href="https://www.amazon.com/gp/product/B0D1YH7D3J?th=1&amp;linkCode=ll1&amp;tag=diynas2026-20&amp;linkId=5636db0ca3c62e86da5f02b641c0f283&amp;language=en_US&amp;ref_=as_li_ss_tl" title="JONSBO N4 Black NAS Pc Case, Walnut Wood, 8-Drive Bay/6 x 3.5-inch HDD (4 hot-swap,2 Non hot-swap),2 x 2.5-inch SSD, Micro ATX Chassis">JONSBO N4</a> (<a href="https://www.jonsbo.com/en/products/N4Black.html" title="JONSBO N4 Case specs">specs</a>) is a third the price, adheres to my “smaller footprint” criteria, and it is rather impressive on its own. It’s a <em>tiny</em> bit larger case than last year’s DIY NAS, but I really like that it has drive bays for six 3.5” drives and two 2.5” drives.</p>

<p>Although, it’s peculiar in that two of the 3.5” drive bays (and the two 2.5” drive bays) aren’t attached to a SATA backplane and can’t be swapped anywhere as easily as the other four 3.5” bays. However, this peculiar decision seems to have caused the JONSBO N4 to sell for a bit less ($20-$40) than similar offerings from JONSBO. At its price, it’s a compelling value proposition!</p>

<h3 id="case-fan">Case Fan</h3>
<p><a href="https://www.amazon.com/dp/B09C6DQDNT?th=1&amp;linkCode=ll1&amp;tag=diynas2026-20&amp;linkId=a917ad99ae5994be300a55a6e8fc8fe2&amp;language=en_US&amp;ref_=as_li_ss_tl" target="_blank" title="Noctua NF-A12x25 PWM chromax.Black.swap, Premium Quiet Fan, 4-Pin (120mm, Black) "><img src="https://blog.briancmoses.com/images/2026/diynas/pp/NF-A12x25_175.png"></a>In the past, I’ve found that the fans which come with JONSBO cases are too noisy. They’ve been noisy for two reasons; the design quality of the fans make them  loud. And the fans are constantly running at their top speed because of the fan header they’re plugged into on the cases’ SATA backplanes.</p>

<p>I anticipated that fan efficiency and noise would be a problem, so I picked out the <a href="https://www.amazon.com/dp/B09C6DQDNT?th=1&amp;linkCode=ll1&amp;tag=diynas2026-20&amp;linkId=a917ad99ae5994be300a55a6e8fc8fe2&amp;language=en_US&amp;ref_=as_li_ss_tl" target="_blank" title="Noctua NF-A12x25 PWM chromax.Black.swap, Premium Quiet Fan, 4-Pin (120mm, Black) ">Noctua NF-A12x25 PWM</a> to solve it. Firstly, swapping in a high-quality fan that pushes more air <strong><em>and</em></strong> generates less noise–especially at its top speed–is a good first step. Secondly, I’d address the problem by plugging the fan into the motherboard’s <code>SYS_FAN</code> header instead of on the SATA backplane. This provides the opportunity to tune the fan’s RPMs directly in the BIOS and generate far less noise.</p>

<h2 id="ram">RAM</h2>
<p><a href="https://www.amazon.com/Crucial-4800MHz-Laptop-Memory-CT32G48C40S5/dp/B09RVNMGFH?th=1&amp;linkCode=ll1&amp;tag=diynas2026-20&amp;linkId=8a98ba9480c4e010efbe51f71f992fbd&amp;language=en_US&amp;ref_=as_li_ss_tl" target="_blank" title="Crucial 32GB DDR5 RAM 4800MHz (CT32G48C40S5)"><img src="https://blog.briancmoses.com/images/2026/diynas/pp/CT32G48C40S5-C16A1_175.png"></a>The first time I first asked myself, “Should I even build the <em>DIY NAS: 2026 Edition</em>?” came as I was checking prices on DDR5 memory. Thankfully for me I had leftover RAM after purchasing DDR5 4800MHz SODIMMs for the <a href="https://blog.briancmoses.com/2024/11/diy-nas-2025-edition.html" title="DIY NAS: 2025 Edition">DIY NAS: 2025 Edition</a>,  <a href="https://blog.briancmoses.com/2025/03/an-all-flash-diy-nas-that-fits-in-my-pocket.html" title="An all-flash DIY NAS that fits in my pocket!">the Pocket Mini NAS</a>, and then again for the <a href="https://blog.briancmoses.com/2025/09/diy-nas-2025-texas-linux-fest-edition.html" title="DIY NAS: 2025 Texas Linux Fest Edition">DIY NAS that I built and gave away at 2025’s Texas Linux Fest</a>. I was personally thankful that I had one brand new  32GB DDR5 4800MHz SODIMM laying around, but I was wildly disappointed for everybody who will try and follow this build when I saw the price of those same SODIMMs.</p>

<p>Regardless, I felt a <a href="https://www.amazon.com/Crucial-4800MHz-Laptop-Memory-CT32G48C40S5/dp/B09RVNMGFH?th=1&amp;linkCode=ll1&amp;tag=diynas2026-20&amp;linkId=8a98ba9480c4e010efbe51f71f992fbd&amp;language=en_US&amp;ref_=as_li_ss_tl" target="_blank" title="Crucial 32GB DDR5 RAM 4800MHz (CT32G48C40S5)">Crucial 32GB DDR5 4800MHz SODIMM</a> (<a href="https://www.crucial.com/memory/ddr5/CT32G48C40S5" title="Crucial 32GB DDR5-4800 SODIMM specs">specs</a>) was the right amount of RAM to get started with for a DIY NAS build in 2025. Whether you just need storage or you wish to also host virtual machines, you will benefit from having more than the bare minimum recommendation of RAM. I really wanted to buy a <a href="https://www.amazon.com/Crucial-5600MT-5200MT-4800MT-CT48G56C46S5/dp/B0C79T2CP7?th=1&amp;linkCode=ll1&amp;tag=diynas2026-20&amp;linkId=36fd2ef6ac56fee00617824bfe0a0676&amp;language=en_US&amp;ref_=as_li_ss_tl" title="Crucial 48GB DDR5 RAM, 5600MHz (or 5200MHz or 4800MHz) Laptop Memory, SODIMM 262-Pin, Compatible with 13th Gen Intel Core and AMD Ryzen 6000 - CT48G56C46S5 ">48GB DDR5 4800MHZ SODIMM</a> for this DIY NAS build, but I couldn’t talk myself into spending the $250-$300 that it would’ve wound up costing.</p>

<h2 id="storage">Storage</h2>

<p>A quick disclaimer about all the drives that I purchased for the <em>DIY NAS: 2026 Edition</em>, I already had all of them! I tend to buy things when I see them on sale and as a result, I have a collection of brand new parts for machines in my homelab or for upcoming projects. I raided that collection of spare parts for the <em>DIY NAS: 2026 Edition</em>.</p>

<h3 id="boot-drive">Boot Drive</h3>

<p>If you ranked the drives in your DIY NAS in order of importance, the boot drive should be the least-important drive. That is <strong><em>not</em></strong> saying that boot drive isn’t performing an important function, but I am suggesting that you shouldn’t invest a bunch of energy and money into picking the optimal boot drive.</p>

<p><a href="https://www.amazon.com/dp/B0963SGYGF?&amp;linkCode=ll1&amp;tag=diynas2026-20&amp;linkId=335ccbcac1cb1beb1ad0bbbe951f4cea&amp;language=en_US&amp;ref_=as_li_ss_tl" target="_blank" title="Silicon Power 128GB A55 SATA SSD (SU128GBSS3A55S25AH)"><img src="https://blog.briancmoses.com/images/2026/diynas/pp/sp-a55-128gb_175.png"></a>Because the <a href="https://www.amazon.com/gp/product/B0D1YH7D3J?th=1&amp;linkCode=ll1&amp;tag=diynas2026-20&amp;linkId=5636db0ca3c62e86da5f02b641c0f283&amp;language=en_US&amp;ref_=as_li_ss_tl" title="JONSBO N4 Black NAS Pc Case, Walnut Wood, 8-Drive Bay/6 x 3.5-inch HDD (4 hot-swap,2 Non hot-swap),2 x 2.5-inch SSD, Micro ATX Chassis">JONSBO N4</a> has a pair of 2.5” drive bays, I decided that a 2.5” SATA SSD would be ideal for the boot drives. As a rule of thumb, I try and spend less than $30 per boot drive in my DIY NAS builds.</p>

<p>Ultimately I selected a pair of <a href="https://www.amazon.com/dp/B0963SGYGF?&amp;linkCode=ll1&amp;tag=diynas2026-20&amp;linkId=335ccbcac1cb1beb1ad0bbbe951f4cea&amp;language=en_US&amp;ref_=as_li_ss_tl" target="_blank" title="Silicon Power 128GB A55 SATA SSD (SU128GBSS3A55S25AH)">128GB Silicon Power A55 SSDs</a> (<a href="https://www.silicon-power.com/product-detail/ace_a55/" title="Silicon Power 128GB A55 SATA SSD specs">specs</a>). I’ve used these before, I’d use them again in the future, and I even have four of their higher capacity (1TB) SSDs in a pool in my own NAS.</p>

<h3 id="app-and-virtual-machine-nvme-ssds">App and Virtual Machine NVMe SSDs</h3>

<p>Self-hosting apps and virtual machines on your DIY NAS has really exploded in the past few years. The developers of NAS appliance packages have made it much easier and the self-hosted products themselves have become as good–or often better–than things you’re probably subscribing to today. Because of that, I saved the highest-performing storage options on the <a href="https://s.click.aliexpress.com/e/_c2zJj1aF" target="_blank" title="Topton N22 N355 Mini-ITX NAS Motherboard (8xSATA3, 2xM.2, 1x10Gbps, 2x2.5Gbps)">Topton N22 motherboard</a> for apps and VMs.</p>

<p><a href="https://www.amazon.com/dp/B07ZGJVTZK?&amp;linkCode=ll1&amp;tag=diynas2026-20&amp;linkId=27613853bea52aac53f1068188e5c5e3&amp;language=en_US&amp;ref_=as_li_ss_tl" target="_blank" title="Teamgroup MP44 1TB NVMe SSD"><img src="https://blog.briancmoses.com/images/2026/diynas/pp/SiliconPower-SSD-P34A60-Product_175.png"></a>However, it’s important to point out that these M.2 slots are PCI-e version 3 and capped at a single PCI-e lane. This is a consequence of the limited number of PCI-e lanes available for each of the CPU options available for  the <a href="https://s.click.aliexpress.com/e/_c2zJj1aF" title="Topton N22 N355 Mini-ITX NAS Motherboard (8xSATA3, 2xM.2, 1x10Gbps, 2x2.5Gbps)">Topton N22 motherboard</a> (N100, N150, N305, and N355).</p>

<p>I opted for a NVMe drive that was a good value rather than a high performer and chose two of the <a href="https://www.amazon.com/dp/B07ZGJVTZK?&amp;linkCode=ll1&amp;tag=diynas2026-20&amp;linkId=27613853bea52aac53f1068188e5c5e3&amp;language=en_US&amp;ref_=as_li_ss_tl" target="_blank" title="Teamgroup MP44 1TB NVMe SSD">Silicon Power 1TB M.2 NVMe SSDs (SP001TBP34A60M28)</a> (<a href="https://www.silicon-power.com/product-detail/PCIe_Gen_3x4_P34A60/">specs</a>).</p>

<h3 id="bulk-storage-hard-disk-drives">Bulk Storage Hard Disk Drives</h3>
<p><a href="https://serverpartdeals.com/collections/hard-drives" target="_blank" title="ServerPartDeas: One of Brian's favorite places to shop for hard drives."><img src="https://blog.briancmoses.com/images/2025/diynas/pp/ST18000NM003D_175.png"></a>Thanks to rising prices, I opted to do like I’ve done with past DIY NAS builds and skip buying hard drives for the <em>DIY NAS: 2026 Edition</em>.</p>

<p>When planning your DIY NAS, it is good to always remember that <strong><em>storage will ultimately be your costliest and most important expense</em></strong>.</p>

<p>Here’s a few things to consider when buying hard drives:</p>

<ol>
  <li>Determine your hardware redundancy preferences. I recommend having two hard disk drives’ worth of redundancy (RAIDZ2, RAID6, etc.)</li>
  <li>Focus on price-per-terabyte when comparing prices of drives.</li>
  <li>Do some <a href="https://github.com/Spearfoot/disk-burnin-and-testing" target="_blank" title="Spearfoot's disk-burnin-and-testing Script on GitHub">burn in testing</a> of your hard drives before putting them to use.</li>
  <li>When buying new drives of the same model, try and buy them from multiple vendors to increase the chances of buying drives manufactured in separate batches.</li>
  <li>Plan Ahead! Understand the rate that your storage grows so that you can craft a strategy to grow your storage down the road.</li>
  <li>Being cheap today can and will paint you into a corner that’s quite expensive to get out of.</li>
  <li>Understand that RAID is not a backup!</li>
</ol>

<p>Thankfully, I’ve collected a bunch of my own decomissioned hard drives which I used to thoroughly test this DIY NAS build.</p>

<h2 id="sata-cables">SATA Cables</h2>
<p><a href="https://www.amazon.com/gp/product/B08C2LJBLW?th=1&amp;linkCode=ll1&amp;tag=diynas2026-20&amp;linkId=18fa5e170ef32c966b38bd844a0b8b35&amp;language=en_US&amp;ref_=as_li_ss_tl" target="_blank" title="OIKWAN SFF-8643 Host to 4 X SATA Breakout Cable"><img src="https://blog.briancmoses.com/images/2026/diynas/pp/sff-8643-cable_175.png"></a>One of the under-the-radar features of the <a href="https://s.click.aliexpress.com/e/_c2zJj1aF" title="Topton N22 N355 Mini-ITX NAS Motherboard (8xSATA3, 2xM.2, 1x10Gbps, 2x2.5Gbps)">Topton N22 motherboard</a> might be one of my favorite features! The motherboard’s Asmedia ASM1164 SATA controllers sit behind two SFF-8643 connectors. These connectors provide two advantages for these motherboards:</p>

<ol>
  <li>Saves room on the motherboard’s PCB.</li>
  <li><a href="https://www.amazon.com/gp/product/B08C2LJBLW?th=1&amp;linkCode=ll1&amp;tag=diynas2026-20&amp;linkId=18fa5e170ef32c966b38bd844a0b8b35&amp;language=en_US&amp;ref_=as_li_ss_tl" target="_blank" title="OIKWAN SFF-8643 Host to 4 X SATA Breakout Cable">SFF-8643 to 4x SATA breakout cables</a> reduces the amount of cable management hassle.</li>
</ol>

<h2 id="power-supply">Power Supply</h2>
<p>The one thing that I have routinely disliked about building small form factor DIY NAS machines is the price tag that accompanies a small form factor power supply (SFX) like is required with the <a href="https://www.amazon.com/gp/product/B0D1YH7D3J?th=1&amp;linkCode=ll1&amp;tag=diynas2026-20&amp;linkId=5636db0ca3c62e86da5f02b641c0f283&amp;language=en_US&amp;ref_=as_li_ss_tl" target="_blank" title="JONSBO N4 Black NAS Pc Case, Walnut Wood, 8-Drive Bay/6 x 3.5-inch HDD (4 hot-swap,2 Non hot-swap),2 x 2.5-inch SSD, Micro ATX Chassis">JONSBO N4</a>.</p>

<p><a href="https://www.amazon.com/gp/product/B075M5FRQS?th=1&amp;linkCode=ll1&amp;tag=diynas2026-20&amp;linkId=a8eca3bbe367f280660bd7085d88d1e0&amp;language=en_US&amp;ref_=as_li_ss_tl" target="_blank" title="SilverStone Technology SX500-G 500W SFX Fully Modular 80 Plus Gold PSU"><img src="https://blog.briancmoses.com/images/2026/diynas/pp/SX500-G_175.png"></a>I wound up choosing the <a href="https://www.amazon.com/gp/product/B075M5FRQS?th=1&amp;linkCode=ll1&amp;tag=diynas2026-20&amp;linkId=a8eca3bbe367f280660bd7085d88d1e0&amp;language=en_US&amp;ref_=as_li_ss_tl" title="SilverStone Technology SX500-G 500W SFX Fully Modular 80 Plus Gold PSU">SilversStone Technology SX500-G</a> (<a href="https://www.silverstonetek.com/en/product/info/power-supplies/SX500-G/" title="SilverStone Technology SX500-G 500W specs">specs</a>) which I had used earlier in the year for the <a href="https://blog.briancmoses.com/2025/09/diy-nas-2025-texas-linux-fest-edition.html" title="DIY NAS: 2025 Texas Linux Fest Edition">DIY NAS I gave away at Texas Linux Fest</a>. Its 500W rating exceeds the needs of all the components that I’d picked out for the <em>DIY NAS: 2026 Edition</em>. Plus the power supply’s <a href="https://en.wikipedia.org/wiki/80_Plus">80 Plus Gold</a> rating aligns well with my criteria for power efficiency.</p>



<p>Regardless of whether it was called FreeNAS, TrueNAS, TrueNAS CORE, TrueNAS SCALE, or now <a href="https://www.truenas.com/truenas-community-edition/" target="_blank" title="TrueNAS Community Edition">TrueNAS Community Edition</a>, the storage appliance product(s) from iXSystems have always been my go-to choice. For each yearly DIY NAS build, I wander over to the <a href="https://www.truenas.com/software-status/">TrueNAS Software Status page</a> and look at the state of the current builds.</p>

<p>I’m conservative with my personal NAS setup. However, for these blog builds, I typically choose Early Adopter releases. This year that’s <a href="https://www.truenas.com/docs/scale/25.10/gettingstarted/versionnotes/" target="_blank" title="TrueNAS 25.10 (Goldeye) Version Notes">TrueNAS 25.10.0.1 (aka Goldeye)</a>. I enjoy being able to use these DIY NAS builds as a preview to the latest and greatest that TrueNAS has to offer.</p>

<p>I repeatedly choose TrueNAS because it’s what I’ve become accustomed to; it’s legitimately an enterprise-grade storage product, which is exactly the quality of solution that I want my data to depend on. At the same time it does not feel like you need a specialized certification and a truckload of enterprise storage experience to meet set up a NAS that exceeds your needs at home.</p>

<p><a href="https://www.truenas.com/truenas-community-edition/" target="_blank" title="TrueNAS Community Edition"><img src="https://blog.briancmoses.com/images/2026/diynas/truenas_open_storage-logo-full-color-rgb.png"></a></p>

<p>Many times I have been asked, “Why not <em>&lt;insert NAS appliance or OS here&gt;</em>?”  My answer to that question is, TrueNAS has always done everything that I need it to and they haven’t given me any reason to consider anything else. As a result, there’s never been a need for me to evaluate something else.</p>

<h2 id="partslist">Final Parts List</h2>

<table>
	<tbody><tr>
		<th>Component</th>
		<th colspan="2">Part Name</th>
		<th>Qty</th>
		<th>Cost</th>
	</tr>
	<tr>
		<td>Motherboard</td>	
		<td><a href="https://s.click.aliexpress.com/e/_c2zJj1aF" target="_blank">Topton N22 (w/ N355 CPU) NAS Motherboard</a></td>	
		<td><a href="https://www.toptonpc.com/product/intel-n100-6-bay-nas-motherboard-110g-2i226-v-2-5g-3lan-6sata3-0-2m-2-nvme-1ddr5-4800mhz-soft-router-firewall-itx-mainboard/" target="_blank">specs</a></td>	
		<td>1</td>	
		<td>$446.40</td>	
	</tr>
	<tr>
		<td>CPU</td>	
		<td>Intel Core 3 N355</td>	
		<td><a href="https://www.intel.com/content/www/us/en/products/sku/241639/intel-core-3-processor-n355-6m-cache-up-to-3-90-ghz/specifications.html" target="_blank">specs</a></td>	
		<td>1</td>	
		<td>N/A</td>	
	</tr>		
	<tr>
		<td>Memory</td>	
		<td><a href="https://www.amazon.com/Crucial-5200MHz-4800MHz-Laptop-CT32G52C42S5/dp/B0BNXYRR9Y?th=1&amp;linkCode=ll1&amp;tag=diynas2026-20&amp;linkId=acb7e10801bf0cf26d03423c185819cb&amp;language=en_US&amp;ref_=as_li_ss_tl" target="_blank">
Crucial RAM 32GB DDR5 4800MHz SODIMM (CT32G48C40S5) </a></td>	
		<td><a href="https://www.crucial.com/memory/ddr5/CT32G48C40S5" target="_blank">specs</a></td>	
		<td>1</td>	
		<td>$172.96</td>	
	</tr>	
	<tr>
		<td>Case</td>	
		<td><a href="https://www.amazon.com/gp/product/B0D1YH7D3J?th=1&amp;linkCode=ll1&amp;tag=diynas2026-20&amp;linkId=5636db0ca3c62e86da5f02b641c0f283&amp;language=en_US&amp;ref_=as_li_ss_tl" target="_blank">JONSBO N4</a></td>
		<td><a href="https://www.jonsbo.com/en/products/N2White.html" target="_blank">specs</a></td>	
		<td>1</td>	
		<td>$121.59</td>	
	</tr>
		<tr>
		<td>Case Fan</td>	
		<td><a href="https://www.amazon.com/dp/B09C6DQDNT?th=1&amp;linkCode=ll1&amp;tag=diynas2026-20&amp;linkId=a917ad99ae5994be300a55a6e8fc8fe2&amp;language=en_US&amp;ref_=as_li_ss_tl" target="_blank">Noctua NF-A12x25 PWM chromax.Black.swap</a></td>
		<td><a href="https://www.noctua.at/en/products/nf-a12x25-pwm-chromax-black-swap" target="_blank">specs</a></td>	
		<td>1</td>	
		<td>$37.95</td>	
	</tr>
	<tr>
		<td>Power Supply</td>	
		<td><a href="https://www.amazon.com/gp/product/B075M5FRQS?th=1&amp;linkCode=ll1&amp;tag=diynas2026-20&amp;linkId=a8eca3bbe367f280660bd7085d88d1e0&amp;language=en_US&amp;ref_=as_li_ss_tl" target="_blank">SilverStone 500W SFX Power Supply SST-SX500-G)</a></td>	
		<td><a href="https://www.silverstonetek.com/en/product/info/power-supplies/SX500-G/" target="_blank">specs</a></td>	
		<td>1</td>	
		<td>$142.34</td>	
	</tr>
	<tr>
		<td>Boot Drive</td>	
		<td><a href="https://www.amazon.com/dp/B0963SGYGF?&amp;linkCode=ll1&amp;tag=diynas2026-20&amp;linkId=335ccbcac1cb1beb1ad0bbbe951f4cea&amp;language=en_US&amp;ref_=as_li_ss_tl" target="_blank">Silicon Power 128GB A55 SATA SSD</a></td>	
		<td><a href="https://www.silicon-power.com/product-detail/ace_a55/" target="_blank">specs</a></td>	
		<td>2</td>	
		<td>$21.97</td>	
	</tr>
	<tr>
		<td>Apps/VM Drives</td>	
		<td><a href="https://www.amazon.com/dp/B07ZGJVTZK?&amp;linkCode=ll1&amp;tag=diynas2026-20&amp;linkId=27613853bea52aac53f1068188e5c5e3&amp;language=en_US&amp;ref_=as_li_ss_tl" target="_blank">Silicon Power 1TB - NVMe M.2 SSD (SP001TBP34A60M28) </a></td>	
		<td><a href="https://www.teamgroupinc.com/en/product-detail/ssd/TEAMGROUP/mp44/mp44-TM8FPW001T0C101/" target="_blank">specs</a></td>	
		<td>2</td>	
		<td>$99.99</td>	
	</tr>			
	<tr>
		<td>SATA Cables</td>	
		<td><a href="https://www.amazon.com/SATA-III-Cable-1M-Set-6Gbps-SATA-Replacement-Computer-Server/dp/B09Q359C7Z?th=1&amp;linkCode=ll1&amp;tag=diynas2026-20&amp;linkId=02df14f462c50c4b883f5daa8709f653&amp;language=en_US&amp;ref_=as_li_ss_tl" target="_blank">OIKWAN SFF-8643 Host to 4 X SATA Breakout Cable</a></td>	
		<td>N/A</td>	
		<td>2</td>	
		<td>$11.99</td>	
	</tr>	
	<tr>
		<td colspan="4">Price <i>without</i> Storage: </td>
		<td>$989.36</td>
	</tr>		
	<tr>
		<td colspan="4">Total Price: </td>
		<td>$1,189.34</td>
	</tr>
</tbody></table>




<h2 id="hardware-assembly-bios-configuration-and-burn-in">Hardware Assembly, BIOS Configuration, and Burn-In</h2>

<h3 id="hardware-assembly">Hardware Assembly</h3>

<p>I wanted the smallest possible DIY NAS. The <a href="https://www.amazon.com/gp/product/B0D1YH7D3J?th=1&amp;linkCode=ll1&amp;tag=diynas2026-20&amp;linkId=5636db0ca3c62e86da5f02b641c0f283&amp;language=en_US&amp;ref_=as_li_ss_tl" target="_blank" title="JONSBO N4 Black NAS Pc Case, Walnut Wood, 8-Drive Bay/6 x 3.5-inch HDD (4 hot-swap,2 Non hot-swap),2 x 2.5-inch SSD, Micro ATX Chassis">JONSBO N4</a> case initially felt too large since it accommodates Micro ATX motherboards. However, I grew to accept its slightly larger footprint. However, putting the <a href="https://s.click.aliexpress.com/e/_c2zJj1aF" title="Topton N22 N355 Mini-ITX NAS Motherboard (8xSATA3, 2xM.2, 1x10Gbps, 2x2.5Gbps)">Topton N22 motherboard</a> into the case felt roomy and luxurious. Building the <em>DIY NAS: 2026 Edition</em> compared to prior years’ felt a lot like coming home to put on sweatpants and a t-shirt after wearing a suit and tie all day long.</p>

<p>I wasn’t too fond of the cable-management of the power supply’s cables. The layout of the case pretty much makes the front of the power supply inaccessible once it is installed. One consequence of this is that the power cable which powered the SATA backplane initially prevented the 120mm case fan from spinning up. That issue was relatively minor and was resolved with zip ties.</p>

<p>Overall, I felt pretty good about the assembly of the <em>DIY NAS: 2026 Edition</em>, but things would take a turn for the worse when I decided to fill all the 3.5-inch drive bays up with some of my decommissioned 8TB HDDs. Now this is probably my fault, I wouldn’t be surprised at all that the manual of the <a href="https://www.amazon.com/gp/product/B0D1YH7D3J?th=1&amp;linkCode=ll1&amp;tag=diynas2026-20&amp;linkId=5636db0ca3c62e86da5f02b641c0f283&amp;language=en_US&amp;ref_=as_li_ss_tl" target="_blank" title="JONSBO N4 Black NAS Pc Case, Walnut Wood, 8-Drive Bay/6 x 3.5-inch HDD (4 hot-swap,2 Non hot-swap),2 x 2.5-inch SSD, Micro ATX Chassis">JONSBO N4</a> warned me against this, but putting the drives in last turned out to be a major pain in the neck for each of the four drive bays <strong><em>without</em></strong> a SATA backplane.</p>

<p><img src="https://blog.briancmoses.com/images/2026/diynas/800/diynas2026_backwards.png" alt="DIY NAS: 2026 Edition with half its drives installed backwards!"></p>

<p>I had wrongly guessed that you accessed those drives’ power and data ports from the front of the case. I worked really hard to route the cables and even managed to install all of the drives before realizing my error and learning my lesson.  I’m understanding now why the <a href="https://www.amazon.com/gp/product/B0D1YH7D3J?th=1&amp;linkCode=ll1&amp;tag=diynas2026-20&amp;linkId=5636db0ca3c62e86da5f02b641c0f283&amp;language=en_US&amp;ref_=as_li_ss_tl" target="_blank" title="JONSBO N4 Black NAS Pc Case, Walnut Wood, 8-Drive Bay/6 x 3.5-inch HDD (4 hot-swap,2 Non hot-swap),2 x 2.5-inch SSD, Micro ATX Chassis">JONSBO N4</a> is cheaper than all of its siblings. Partly because there’s a missing SATA backplane, but also because those other 4 drive bays’ layout is frustrating.</p>

<p>Don’t let my last couple paragraphs sour you on the <a href="https://www.amazon.com/gp/product/B0D1YH7D3J?th=1&amp;linkCode=ll1&amp;tag=diynas2026-20&amp;linkId=5636db0ca3c62e86da5f02b641c0f283&amp;language=en_US&amp;ref_=as_li_ss_tl" target="_blank" title="JONSBO N4 Black NAS Pc Case, Walnut Wood, 8-Drive Bay/6 x 3.5-inch HDD (4 hot-swap,2 Non hot-swap),2 x 2.5-inch SSD, Micro ATX Chassis">JONSBO N4</a>, though. I still really like its size, it feels big when you’re working in it with a Mini ITX motherboard. If you wind up deciding to use the JONSBO N4, then I suggest that you put those four drives and their cables in first before you do anything else. That would’ve made a world of difference for me. Actually looking at the documentation before getting started might have saved me quite a bit of aggravation, too!</p>

<p>If I have ruined the JONSBO N4 for you, then check out the <a href="https://www.amazon.com/Mini-ITX-Chassis-Computer-Aluminum-Support/dp/B0CMVBMVHT?&amp;linkCode=ll1&amp;tag=diynas2026-20&amp;linkId=9140995b2e4f34809c8202effd2750ac&amp;language=en_US&amp;ref_=as_li_ss_tl" target="_blank" title="JONSBO N3 Mini-ITX NAS PC Chassis">JONSBO N3</a>. It’s <strong><em>eight</em></strong> 3.5-inch drive bays pair up really nicely with the <a href="https://s.click.aliexpress.com/e/_c2zJj1aF" title="Topton N22 N355 Mini-ITX NAS Motherboard (8xSATA3, 2xM.2, 1x10Gbps, 2x2.5Gbps)">Topton N22 motherboard</a>. You can see what I thought of the JONSBO N3 by reading the <a href="https://blog.briancmoses.com/2024/09/diy-nas-2024-edition-and-econonas.html" title="DIY NAS: 2024 Edition">DIY NAS: 2024 Edition</a> blog.</p>



<h3 id="bios-configuration">BIOS Configuration</h3>

<p>Generally speaking, I do as little as I possibly can in the BIOS. Normally I strive to only set the time and change the boot order. However, I did a bit more for the <em>DIY NAS: 2026 Edition</em> since I’m using the <code>SYS_FAN</code> header for the fan which is responsible for cooling the hard drives.  Here are the changes that I made in the BIOS:</p>

<ol>
  <li>Set the <strong>System Date</strong> and <strong>System Time</strong> to Greenwich Mean Time
    <ol>
      <li>Advanced
        <ol>
          <li>Hardware Monitor ( Advanced)
            <ol>
              <li>Set <strong>SYS SmartFan Mode</strong> to  <code>Disabled</code>.</li>
              <li>Set the <strong>Manual PWM Setting</strong> (for <code>SYS_FAN</code>) to 180.</li>
            </ol>
          </li>
        </ol>
      </li>
      <li>Set <strong>PWRON After Power Loss</strong> to <code>Always On</code></li>
      <li>Boot
        <ol>
          <li>Set <strong>Boot Option #1</strong> to the TrueNAS boot device.</li>
        </ol>
      </li>
    </ol>
  </li>
</ol>

<p>I’m not at all interested in venturing into the rabbit’s hole of trying to completely minimize how much power the NAS uses. However, I imagine there’s some opportunities for power savings lurking in the BIOS. I didn’t go looking for them myself, but if you’re intrepid enough to do so here’s a few suggestions that I have to save some additional power:</p>

<ul>
  <li>Disable the onboard audio.</li>
  <li>Disable any network interfaces that you don’t wind up using.</li>
  <li>Tinker with the CPU settings.</li>
  <li>Got other suggestions?  <em>Share them in the comments!</em></li>
</ul>

<h3 id="burn-in">Burn-In</h3>

<p>Because all of the hardware is brand-new to me brand-new components are not guaranteed to be free of defects, I always do a little bit of burn-in testing to establish some trust in the hardware that I’ve picked out for each DIY NAS build. While I think doing <em>some</em> burn-in testing critically important, I also think the value of subsequent burn-in testing drops the more that you do. Don’t get too carried away and do your own  burn-in testing in moderation!</p>

<h4 id="memtest86"><a href="https://www.memtest.org/" target="_blank" title="Memtest86+">Memtest86+</a></h4>

<p>I <em>always</em> use Memtest86+ to burn-in the RAM. I always run at least 3+ passes of Memtest86+. Typically, I run many more passes because I tend to let the system keep running additional passes overnight. Secondarily, running these many passes give the CPU a little bit of work to do and there’s enough information displayed by Memtest86+ to give me confidence in the CPU and its settings.</p>

<p><a href="https://blog.briancmoses.com/images/2026/diynas/diynas2026-memtest_1280.png"><img src="https://blog.briancmoses.com/images/2026/diynas/diynas2026-memtest_600.png" alt="Memtest86+"></a></p>

<h4 id="hard-drives">Hard Drives</h4>

<p>The failure rate of hard drives is highest when the drives are new and then again when they’re old. Regardless of type of hard drives that I buy or when I buy them, I always do some disk burn in. I tend to run <a href="https://github.com/Spearfoot/disk-burnin-and-testing" target="_blank" title="Spearfoot's disk-burnin-and-testing Script on GitHub">Spearfoot’s Disk Burn-in and Testing script</a> on all of my new drives. However executing this script against all of the drives can take quite a long time, even if you use something like  <code>tmux</code> to run the tests in parallel.</p>

<h2 id="initial-truenas-ce-setup">Initial TrueNAS CE Setup</h2>

<p>There’s always a little bit of setup that I do for a new TrueNAS machine. This isn’t intended to be an all inclusive step-by-step guide for all the things you should do with your DIY NAS. Instead, it’s more of a list of things I kept track of while I made sure that the <em>DIY NAS: 2026 Edition</em> was functional enough for me to finish writing this blog. That being said, I do think your NAS would be rather functional if you decided to do the same configuration.</p>

<ol>
  <li>Updated the hostname to <code>diynas2026</code>
    <ol>
      <li><em>Note:  This is only to avoid issues with another NAS on my network.</em></li>
    </ol>
  </li>
  <li>Updated the timezone.</li>
  <li>Enabled the following services and set them to start automatically.
    <ol>
      <li>SMB</li>
      <li>SSH</li>
      <li>NFS</li>
    </ol>
  </li>
  <li>Enabled password login for the <code>truenas_admin</code> user.
    <ul>
      <li><em>Note: If I were planning to use this DIY NAS long-term, I wouldn’t have done this. Using SSH keys for authentication is a better idea</em>.</li>
    </ul>
  </li>
  <li>Edited the TrueNAS Dashboard widgets to reflect the 10Gb interface (<code>enp1s0</code>).</li>
  <li>Created a pool named <code>flash</code> which consisted of mirrored vdev using the <a href="https://www.amazon.com/dp/B07ZGJVTZK?&amp;linkCode=ll1&amp;tag=diynas2026-20&amp;linkId=27613853bea52aac53f1068188e5c5e3&amp;language=en_US&amp;ref_=as_li_ss_tl" target="_blank" title="Teamgroup MP44 1TB NVMe SSD">Teamgroup MP44 1TB NVMe SSDs</a>.</li>
  <li>Created a pool named <code>rust</code> which consisted of a single RAID-Z2 vdev using eight hard drives that I had sitting on my shelf after they were decomissioned.</li>
  <li>Configured the Apps to use the <code>flash</code> pool for the apps’ dataset.</li>
  <li>Made sure that the System Dataset Pool was set to <code>flash</code>.</li>
  <li>Confirmed that there were Scrub Tasks set up for the <code>flash</code> and <code>rust</code> pools.</li>
  <li>Created a dataset on each pool for testing;  <code>flash-test</code> and <code>rust-test</code></li>
  <li>Installed the <a href="https://github.com/AnalogJ/scrutiny" target="_blank" title="Scrutiny: WebUI for smartd S.M.A.R.T monitoring">Scrutiny</a> app found in the App Catalog.</li>
</ol>

<p>If I were planning to keep this NAS and use it for my own purposes, I would also:</p>

<ol>
  <li>Set up a <a href="https://www.truenas.com/docs/scale/25.10/scaletutorials/credentials/certificates/settingupletsencryptcertificates/" target="_blank" title="Creating ACME Certificates">Let’s Encrypt certificate</a>.</li>
  <li>Hook up the NAS to <a href="https://networkupstools.org/stable-hcl.html" target="_blank" title="Network UPS Tools Hardware Compatability List">a compatible UPS</a>, enable the UPS service, and configure the UPS service to shut down the NAS before the battery runs out of juice.</li>
  <li><a href="https://www.truenas.com/docs/scale/25.10/scaleuireference/toptoolbar/alerts/alertsettingsscreen/" target="_blank" title="TrueNAS Scale Alert Settings Screens">Set up system email alert service</a>.</li>
  <li>Create <a href="https://www.truenas.com/docs/scale/25.10/scaletutorials/dataprotection/replication/" target="_blank" title="TrueNAS Replication Tasks">replication tasks</a> to back up critical data to my <a href="https://blog.briancmoses.com/2024/06/one-3d-printed-nas-wasn-t-enough-so-i-printed-two-more.html" title="One 3D-printed NAS wasn't enough, so I printed two more!">off-site NAS</a>.</li>
  <li>Add the new NAS to my Tailscale tailnet using the <a href="https://github.com/truenas/apps/tree/master/ix-dev/community/tailscale" target="_blank" title="Tailscale from the official TrueNAS App Catalog">Tailscale app from the official catalog</a>.</li>
  <li>As the NAS is seeded with data, create and maintain a suite of <a href="https://www.truenas.com/docs/scale/25.10/scaletutorials/dataprotection/periodicsnapshottasksscale/" target="_blank" title="Adding Periodic Snapshot Tasks">snapshot tasks</a> tailored to the importance of the different data being stored on the NAS.</li>
  <li>Set up S.M.A.R.T. tests for all of the drives:
    <ol>
      <li>Weekly Short Test</li>
      <li>Monthly Long Test</li>
    </ol>
  </li>
</ol>

<h2 id="benchmarks">Benchmarks</h2>

<p>Just about every year, I benchmark each DIY NAS build and almost always come to the same conclusion; the NAS will outperform your network at home. Your first bottleneck is almost always going to be the network and the overlwhelming majority of us have gigabit networks at home–but that’s slowly changing since 2.5Gbps and 10Gbps network hardware has started to get reasonable lately.</p>

<p>Even though I always come to the same conclusion, I still like to do the benchmarks for two reasons:</p>

<ol>
  <li>It helps me build confidence that the <em>DIY NAS: 2026 Edition</em> works well.</li>
  <li>People tend to enjoy consuming benchmarks <strong><em>and</em></strong> it’s fun for me to see the DIY NAS’ network card get saturated during the testing.</li>
</ol>

<h3 id="throughput">Throughput</h3>

<p>I like to do three categories of tests to measure the throughput of the NAS:</p>

<ol>
  <li>Use <a href="https://iperf.fr/" target="_blank" title="iPerf - The ultimate speed test tool for TCP, UDP and SCTP">iperf3</a> to benchmark throughput between my NAS and another machine on my network.</li>
  <li>Benchmark the throughput of the pool(s) locally on the NAS using <code>fio</code>.</li>
  <li>Set up SMB shares on each of the pools and then benchmark the throughput when using those shares.</li>
</ol>

<p>Every year I try and mention that Tom Lawrence from <a href="https://lawrencesystems.com/" target="_blank">Lawrence Systems</a> published <a href="https://youtu.be/TASYaWTkg0U">a great video about benchmarking storage with FIO</a> and shared <a href="https://forums.lawrencesystems.com/t/linux-benchmarking-with-fio/11122">the FIO commands from his video in their forums</a>. I use these FIO commands constantly as a reference point for testing ZFS pools’ throughput. Importantly I’d like to point out that, in that same video, Tom says something very wise:</p>

<blockquote>
  <p><a href="https://youtu.be/TASYaWTkg0U?t=53" target="_blank">There are lies, damn lies, and then there are benchmarks!</a></p>
</blockquote>

<table>
  <thead>
    <tr>
      <th><strong>Tool</strong></th>
      <th><strong>Pool</strong></th>
      <th><strong>Test<br>Size</strong></th>
      <th><strong>Random<br>Write<br>IOPS</strong></th>
      <th><strong>Random<br>Read<br>IOPS</strong></th>
      <th><strong>Sequential<br>Write<br>(MB/s)</strong></th>
      <th><strong>Sequential<br>Read<br>(MB/s)</strong></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>FIO</td>
      <td><code>flash</code></td>
      <td>4G</td>
      <td>1906.00</td>
      <td>2200.00</td>
      <td>548.00</td>
      <td>1214.00</td>
    </tr>
    <tr>
      <td>FIO</td>
      <td><code>flash</code></td>
      <td>32G</td>
      <td>2132.00</td>
      <td>3012.00</td>
      <td>544.00</td>
      <td>1211.00</td>
    </tr>
    <tr>
      <td>FIO</td>
      <td><code>rust</code></td>
      <td>4G</td>
      <td>1352.00</td>
      <td>108.00</td>
      <td>367.00</td>
      <td>530.00</td>
    </tr>
    <tr>
      <td>FIO</td>
      <td><code>rust</code></td>
      <td>32G</td>
      <td>1474.00</td>
      <td>326.00</td>
      <td>368.00</td>
      <td>544.00</td>
    </tr>
    <tr>
      <td>CrystalDiskMark</td>
      <td><code>flash</code></td>
      <td>4GiB</td>
      <td>5858.89</td>
      <td>50409.91</td>
      <td>1104.64</td>
      <td>956.70</td>
    </tr>
    <tr>
      <td>CrystalDiskMark</td>
      <td><code>flash</code></td>
      <td>32GiB</td>
      <td>4193.36</td>
      <td>31047.36</td>
      <td>635.42</td>
      <td>946.20</td>
    </tr>
    <tr>
      <td>CrystalDiskMark</td>
      <td><code>rust</code></td>
      <td>4GiB</td>
      <td>5226.50</td>
      <td>46239.01</td>
      <td>756.23</td>
      <td>655.32</td>
    </tr>
    <tr>
      <td>CrystalDiskMark</td>
      <td><code>rust</code></td>
      <td>32GiB</td>
      <td>3794.43</td>
      <td>12809.33</td>
      <td>759.38</td>
      <td>677.02</td>
    </tr>
  </tbody>
</table>



<p>What do I think these benchmarks and my use of the <em>DIY NAS: 2026 Edition</em> tell me?  In the grand scheme of things, not a whole lot.</p>

<p>However, these benchmarks do back up what I expected, the <em>DIY NAS: 2026 Edition</em> is quite capable and more than ready to meet my storage needs. I especially like that the CrystalDiskMark benchmark of the SMB shares were <strong><em>both faster than a SATA SSD</em></strong>, and the throughput to the share on the <code>flash</code> pool practically saturated the NAS’ 10GbE network connection.</p>

<h4 id="fio-tests">FIO Tests</h4>

<p>Every time I benchmark a NAS, I seem to either be refining what I tried in prior years or completely reinventing the wheel. As a result, I wouldn’t recommend comparing these results with results that I shared in prior years’ DIY NAS build blogs. I haven’t really put a ton of effort into developing a standard suite of benchmarks. Things in my homelab change enough between DIY NAS blogs that trying to create and maintain an environment for a standard suite of benchmarks is beyond what my budget, spare time, and attention span will allow.</p>

<p>I’m going to paste these <code>fio</code> commands here in the blog for my own use in future DIY NAS build blogs. If you wind up building something similar, these <em>might</em> be helpful to measure your new NAS’ filesystem’s performance and compare it to mine!</p>

<div><pre><code>## Random Write IOPS
fio --randrepeat=1 --ioengine=libaio --direct=1 --name=test --filename=test --bs=128k --size=4G --readwrite=randwrite --ramp_time=10
fio --randrepeat=1 --ioengine=libaio --direct=1 --name=test --filename=test --bs=128k --size=32G --readwrite=randwrite --ramp_time=10

## Random Read IOPS
fio --randrepeat=1 --ioengine=libaio --direct=1 --name=test --filename=test --bs=128k --size=4G --readwrite=randread --ramp_time=10
fio --randrepeat=1 --ioengine=libaio --direct=1 --name=test --filename=test --bs=128k --size=32G --readwrite=randread --ramp_time=10

## Sequential Write (MB/s)
fio --randrepeat=1 --ioengine=libaio --direct=1 --name=test --filename=test --bs=4M --size=4G --readwrite=write --ramp_time=10
fio --randrepeat=1 --ioengine=libaio --direct=1 --name=test --filename=test --bs=4M --size=32G --readwrite=write --ramp_time=10

## Sequential Read (MB/s)
fio --randrepeat=1 --ioengine=libaio --direct=1  --name=test --filename=test --bs=4M --size=4G --readwrite=read --ramp_time=10
fio --randrepeat=1 --ioengine=libaio --direct=1  --name=test --filename=test --bs=4M --size=32G --readwrite=read --ramp_time=10
</code></pre></div>

<p><a href="https://blog.briancmoses.com/images/2026/diynas/combined-network-throughput.png"><img src="https://blog.briancmoses.com/images/2026/diynas/800/combined-network-throughput.png" alt="Perfmon of Networkthroughput for reading files over SMB and during iperf3 tests."></a></p>

<h3 id="power-consumption">Power Consumption</h3>

<p>One not-so-obvious cost of running a DIY NAS is how much power it consumes. While I specifically tried to pick items that were efficient in terms of power consumption, it’s also important to realize that all the other bells and whistles on the awesome <a href="https://s.click.aliexpress.com/e/_c2zJj1aF" target="_blank" title="Topton N22 N355 Mini-ITX NAS Motherboard (8xSATA3, 2xM.2, 1x10Gbps, 2x2.5Gbps)">Topton N18 NAS motherboard</a> consume power, too. And that the biggest consumer of power in a NAS is almost always the hard disk drives.</p>

<p>Thanks to my tinkering with <a href="https://blog.briancmoses.com/categories/home-automation/" title="Home Automation blogs">home automation</a>, I have a plethora of <a href="https://www.amazon.com/gp/product/B0CJR4QZ45?th=1&amp;linkCode=ll1&amp;tag=diynas2026-20&amp;linkId=331f4bd20d1cfe3c50b64077966b0ecc&amp;language=en_US&amp;ref_=as_li_ss_tl" target="_blank" title="Minoston Z-Wave 800 Series Zwave Plug with Energy Monitoring, Power Meter Z-Wave Outlet Switch (MP31ZP)">smart outlets</a> which are capable of power monitoring. I used those smart outlets for most of my power monitoring. But I also have a <a href="https://www.amazon.com/P3-P4400-Electricity-Usage-Monitor/dp/B00009MDBU?crid=BQE7RDYQK9CM&amp;th=1&amp;linkCode=ll1&amp;tag=diynas2026-20&amp;linkId=c0efb3c2ef49e3049ce3e77825adedc0&amp;language=en_US&amp;ref_=as_li_ss_tl" title="P3 P4400 Kill A Watt Electricity Usage Monitor">Kill a Watt P400</a> that I also use for some of the shorter tests:</p>

<ul>
  <li>Power consumed during a handful of specific tasks:
    <ul>
      <li>Idle while running TrueNAS</li>
      <li>RAM Burn-in (~14 passes of Memtest86+)</li>
      <li>An 8-hour throughput benchmark copying randomly-sized files to the NAS using SMB.</li>
    </ul>
  </li>
  <li>Total consumed during the build, burn-in, and use of the <em>DIY NAS: 2026 Edition</em>.</li>
</ul>

<table>
  <thead>
    <tr>
      <th><strong>Task</strong></th>
      <th><strong>Duration</strong></th>
      <th><strong>Max Wattage</strong></th>
      <th><strong>Avg. Wattage</strong></th>
      <th><strong>Total Consumption</strong></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Boot</td>
      <td>10 min.</td>
      <td>200.00 W</td>
      <td>120.00 W</td>
      <td>0.02 kWh</td>
    </tr>
    <tr>
      <td>Idle</td>
      <td>3 hr.</td>
      <td>90.00 W</td>
      <td>66.67 W</td>
      <td>0.20 kWh</td>
    </tr>
    <tr>
      <td>RAM Burn-in</td>
      <td>18 hr.</td>
      <td>104.00 W</td>
      <td>91.67 W</td>
      <td>1.65 kWh</td>
    </tr>
    <tr>
      <td>SMB Benchmark of HDDs</td>
      <td>8 hr.</td>
      <td>107.00 W</td>
      <td>85.00 W</td>
      <td>0.68 kWh</td>
    </tr>
    <tr>
      <td><strong>Total</strong></td>
      <td><strong>108 hr.</strong></td>
      <td><strong>237.80 W</strong></td>
      <td><strong>66.49 W</strong></td>
      <td><strong>7.17 kWh</strong></td>
    </tr>
  </tbody>
</table>



<h2 id="what-about-an-econonas">What about an EconoNAS?</h2>

<p><img src="https://blog.briancmoses.com/images/2026/diynas/ai-dollar-bill-sign.png">Shortly before prices skyrocketed, I decided I wasn’t very interested in doing a separate EconoNAS builds. Several months ago, I realized that there were several off-the-shelf NAS machines that were more-than-capable of running TrueNAS and they were selling at economical prices that couldn’t be topped by a DIY approach. I will dive deeper into this in a future blog, eventually … <strong><em>maybe</em></strong>?</p>

<p>All that being said–it’d be incredibly easy to make some compromises which result in the <em>DIY NAS: 2026 Edition</em> becoming quite a bit more economical. Here’s a list of changes that I would consider to be more budget-friendly:</p>

<ul>
  <li>Different motherboard/CPU combo: <a href="https://www.ebay.com/itm/127014563991" target="_blank" title="Topton N18 N100/N150/N305 Mini-ITX NAS Motherboard:6xSATA/2xM.2/2x2.5GbE/1x10GbE">N18 w/ N100 CPU</a> (-$224), <a href="https://www.ebay.com/itm/127014563991" title="Topton N18 N100/N150/N305 Mini-ITX NAS Motherboard:6xSATA/2xM.2/2x2.5GbE/1x10GbE">N18 w/ N150 CPU</a> (-$214), or <a href="https://www.ebay.com/itm/127525459090" title="Topton N22 Motherboard w/ N150 CPU on Brian's eBay Store">N22 w/ N150 CPU</a> (-$180)</li>
  <li><a href="https://amzn.to/4ai7M2V" target="_blank">16GB of DDR5 RAM</a> (-$39) instead of 32GB.</li>
  <li><a href="https://amzn.to/4qSQBLa" target="_blank">Thermal Right TL-C12015 Slim Fan</a> instead of the Noctua NF-A12x25 (-$26)</li>
  <li><a href="https://amzn.to/3LFJSnP" target="_blank">Apevia SFX-AP500W Power Supply</a> (-$104)</li>
  <li>Skip the redundancy for the boot pool (-$22)</li>
</ul>

<p>Altogether, these savings could add up to more than $400, which is pretty considerable!  If you made all of these changes, you’d have something that’s going to be nearly equivalent to the <em>DIY NAS: 2026 Edition</em> but at a fraction of the price.</p>

<p><a href="https://blog.briancmoses.com/images/2026/diynas/ai-brian-ock-buying-drives.png"><img src="https://blog.briancmoses.com/images/2026/diynas/ai-brian-ock-buying-drives_640.png" alt="Brian as a Super Villian shopping for economical DIY NAS deals!"></a></p>

<h2 id="what-am-i-going-to-do-with-the-diy-nas-2026-edition">What am I going to do with the DIY NAS: 2026 Edition?!</h2>

<p>My DIY NAS is aging quite gracefully, but I’ve recently been wondering about replacing it. Shortly before ordering all the parts for the <em>DIY NAS: 2026 Edition</em>, I briefly considered using this year’s DIY NAS build to replace my personal NAS. However, I decided not to do that. Then prices skyrocketed and I shelved the idea of building a replacement for my own NAS and I nearly shelved the idea of a DIY NAS in 2026!</p>

<p>So that begs the question, “What is Brian going to do with the <em>DIY NAS: 2026 Edition</em>?”</p>

<p>I’m going to auction it off on the <a href="https://www.ebay.com/str/briancmosesdotcom" target="_blank" title="briancmosesdotcom store on eBay">briancmosesdotcom store on eBay</a>! Shortly after publishing this blog, I’ll list it on eBay. In response to skyrocketing prices for PC components, I’m going to do a no-reserve auction. At the end of the auction, the highest bidder wins and hopefully they’ll get a pretty good deal!</p>

<p><a href="https://www.ebay.com/itm/127527163731" target="_blank"><img src="https://blog.briancmoses.com/images/2026/diynas/ebay-auction_800.png" alt="eBay auction of the DIY NAS: 2026 Edition"></a></p>

<h2 id="final-thoughts">Final Thoughts</h2>

<p>Overall, I’m pleased with the <em>DIY NAS: 2026 Edition</em>. The <a href="https://s.click.aliexpress.com/e/_c2zJj1aF" target="_blank" title="Topton N22 N355 Mini-ITX NAS Motherboard (8xSATA3, 2xM.2, 1x10Gbps, 2x2.5Gbps)">Topton N22 motherboard</a> is a significant improvement over last year’s <a href="https://www.ebay.com/itm/127014563991" title="Topton N18 N100/N150/N305 Mini-ITX NAS Motherboard:6xSATA/2xM.2/2x2.5GbE/1x10GbE">Topton N18 motherboard</a>, primarily due to its extra two SATA ports. This provides 33.3% more gross storage capacity.</p>

<p>While testing, I found the <a href="https://www.intel.com/content/www/us/en/products/sku/231803/intel-processor-n100-6m-cache-up-to-3-40-ghz/specifications.html" target="_blank" title="Intel® Processor N100">Intel Core 3 N355 CPU</a> somewhat excessive for basic NAS functions. However, the substantial untapped CPU horsepower offers luxurious performance potential. This makes the build compelling for anyone planning extensive self-hosting projects.</p>

<p>I have mixed feelings about the <a href="https://www.amazon.com/gp/product/B0D1YH7D3J?th=1&amp;linkCode=ll1&amp;tag=diynas2026-20&amp;linkId=5636db0ca3c62e86da5f02b641c0f283&amp;language=en_US&amp;ref_=as_li_ss_tl" target="_blank" title="JONSBO N4 Black NAS Pc Case, Walnut Wood, 8-Drive Bay/6 x 3.5-inch HDD (4 hot-swap,2 Non hot-swap),2 x 2.5-inch SSD, Micro ATX Chassis">JONSBO N4 case</a>. The four right-side drive bays lack SATA backplane connectivity. Without creative cabling solutions, individual drive replacement becomes challenging. However, the case’s ~$125 price point compensates for this inconvenience. I anticipate that those the cost savings will justify the compromise for most builders. If I were to build the <em>DIY NAS: 2026 Edition</em> all over again, I’d be tempted to use the <a href="https://www.amazon.com/Mini-ITX-Chassis-Computer-Aluminum-Support/dp/B0CMVBMVHT?&amp;linkCode=ll1&amp;tag=diynas2026-20&amp;linkId=9140995b2e4f34809c8202effd2750ac&amp;language=en_US&amp;ref_=as_li_ss_tl" title="JONSBO N3 Mini-ITX NAS PC Chassis">JONSBO N3 case</a> or even the <a href="https://www.jonsbo.com/en/products/N6Black.html" title="JONSBO N6">JONSBO N6</a> which isn’t quite obtainable, yet.</p>

<p>The DIY NAS: 2026 Edition delivers excellent performance and superior specifications. In my opinion, it represents better value than off-the-shelf alternatives:</p>

<ul>
  <li><a href="https://www.amazon.com/QNAP-TS-832PX-4G-High-Capacity-10GbE-2-5GbE/dp/B08P42JR89?th=1&amp;linkCode=ll1&amp;tag=diynas2026-20&amp;linkId=8d7e5630a6e8c1a01d6f791658a8652e&amp;language=en_US&amp;ref_=as_li_ss_tl" target="_blank" title="QNAP TS-832PX-4G 8 Bay High-Capacity NAS with 10GbE SFP+ and 2.5GbE ">QNAP TS-832PX-4G</a> ($880)</li>
  <li><a href="https://www.amazon.com/Asustor-Lockerstor-Enterprise-Attached-Quad-Core/dp/B07Y2BJZKC?th=1&amp;linkCode=ll1&amp;tag=diynas2026-20&amp;linkId=9ba0f3af9b719585c327dbf5b1d14c9a&amp;language=en_US&amp;ref_=as_li_ss_tl" target="_blank" title="Asustor Lockerstor 8 AS6508T 8 Bay NAS - Network Storage Enclosure, Quad Core 2.1GHz CPU, 8GB RAM DDR4, M.2 NVMe SSD ">Asustor Lockerstor 8 AS6508T</a> ($960)</li>
  <li><a href="https://www.amazon.com/UGREEN-DXP8800-Plus-Attached-Diskless/dp/B0D22HGH4N?&amp;linkCode=ll1&amp;tag=diynas2026-20&amp;linkId=351f2f13b717fb8ca116986ad227ec9c&amp;language=en_US&amp;ref_=as_li_ss_tl" target="_blank" title="UGREEN NASync DXP8800 Plus 8-Bay Desktop NAS, Intel i5 1235u 10-Core CPU, 8GB DDR5 RAM, 128G SSD, 2X 10GbE, 2X M.2 NVMe Slots, 8K HDMI, 2X TBT4, Network Attached Storage (Diskless) ">UGREEN NASync DXP8800</a> ($1200)</li>
</ul>

<p>Building your own NAS provides significant advantages. Years later, you can upgrade RAM, motherboard, case, or add PCI-e (x1) expansion cards. These off-the-shelf alternatives offer severely limited upgrade paths.</p>

<p>Is 2026 finally the year that you decide to build your DIY NAS?  I hope that it is!  Share your experience building your NAS in the comments below or come tell us about it in the <a href="https://blog.briancmoses.com/discord" title="The Butter, What?! Discord Server">#diynas-and-homelab channel on the Butter, What?! Discord server</a>!</p>

<!-- Tiny Slider Script: diynas2026-->


<!-- Tiny Slider Script: diynas2026-assembly-->


</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Green Card Interviews End in Handcuffs for Spouses of U.S. Citizens (207 pts)]]></title>
            <link>https://www.nytimes.com/2025/11/26/us/trump-green-card-interview-arrests.html</link>
            <guid>46065015</guid>
            <pubDate>Thu, 27 Nov 2025 02:51:31 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.nytimes.com/2025/11/26/us/trump-green-card-interview-arrests.html">https://www.nytimes.com/2025/11/26/us/trump-green-card-interview-arrests.html</a>, See on <a href="https://news.ycombinator.com/item?id=46065015">Hacker News</a></p>
Couldn't get https://www.nytimes.com/2025/11/26/us/trump-green-card-interview-arrests.html: Error: Request failed with status code 403]]></description>
        </item>
        <item>
            <title><![CDATA[Penpot: The Open-Source Figma (557 pts)]]></title>
            <link>https://github.com/penpot/penpot</link>
            <guid>46064757</guid>
            <pubDate>Thu, 27 Nov 2025 02:14:36 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/penpot/penpot">https://github.com/penpot/penpot</a>, See on <a href="https://news.ycombinator.com/item?id=46064757">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><themed-picture data-catalyst-inline="true"><picture>
  <source media="(prefers-color-scheme: dark)" srcset="https://camo.githubusercontent.com/6f4a230c051d6c3d15a0286b0441f113c4baac20752ab4ec7e276e329f2273c0/68747470733a2f2f70656e706f742e6170702f696d616765732f726561646d652f6769746875622d6461726b2d6d6f64652e706e67" data-canonical-src="https://penpot.app/images/readme/github-dark-mode.png">
  <source media="(prefers-color-scheme: light)" srcset="https://camo.githubusercontent.com/c1fc85084a7b31db77a330977ca9b00c97390ee20b0aec50c70561657a554721/68747470733a2f2f70656e706f742e6170702f696d616765732f726561646d652f6769746875622d6c696768742d6d6f64652e706e67" data-canonical-src="https://penpot.app/images/readme/github-light-mode.png">
  <img alt="penpot header image" src="https://camo.githubusercontent.com/c1fc85084a7b31db77a330977ca9b00c97390ee20b0aec50c70561657a554721/68747470733a2f2f70656e706f742e6170702f696d616765732f726561646d652f6769746875622d6c696768742d6d6f64652e706e67" data-canonical-src="https://penpot.app/images/readme/github-light-mode.png">
</picture></themed-picture>
<p dir="auto">
<a href="https://www.mozilla.org/en-US/MPL/2.0" rel="nofollow"><img alt="License: MPL-2.0" src="https://camo.githubusercontent.com/0d433bf1013a998c8b0db6c4c9eefd9edae1111aad142e998918e179fdd5c54a/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f4d504c2d322e302d626c75652e737667" data-canonical-src="https://img.shields.io/badge/MPL-2.0-blue.svg"></a>
<a href="https://community.penpot.app/" rel="nofollow"><img alt="Penpot Community" src="https://camo.githubusercontent.com/608d7ad63d5dd51331463cdcc583715033bf50f77ea52e68f94cb7ea26f95e1b/68747470733a2f2f696d672e736869656c64732e696f2f646973636f757273652f706f7374733f7365727665723d6874747073253341253246253246636f6d6d756e6974792e70656e706f742e617070" data-canonical-src="https://img.shields.io/discourse/posts?server=https%3A%2F%2Fcommunity.penpot.app"></a>
<a href="https://tree.taiga.io/project/penpot/" title="Managed with Taiga.io" rel="nofollow"><img alt="Managed with Taiga.io" src="https://camo.githubusercontent.com/3352207d59d443f294c31f36995ecedf5212b7e7b1ed4b7d4ca825f0c739e5c2/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f6d616e61676564253230776974682d54414947412e696f2d3730396631342e737667" data-canonical-src="https://img.shields.io/badge/managed%20with-TAIGA.io-709f14.svg"></a>
<a href="https://gitpod.io/#https://github.com/penpot/penpot" rel="nofollow"><img alt="Gitpod ready-to-code" src="https://camo.githubusercontent.com/c01324668ea00cd2b02dc9fbf541676fb30543b69ef99a070d62a110917126d0/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f476974706f642d72656164792d2d746f2d2d636f64652d626c75653f6c6f676f3d676974706f64" data-canonical-src="https://img.shields.io/badge/Gitpod-ready--to--code-blue?logo=gitpod"></a>
</p>
<p dir="auto">
    <a href="https://penpot.app/" rel="nofollow"><b>Website</b></a>  •
    <a href="https://help.penpot.app/user-guide/" rel="nofollow"><b>User Guide</b></a>  •
    <a href="https://penpot.app/learning-center" rel="nofollow"><b>Learning Center</b></a>  •
    <a href="https://community.penpot.app/" rel="nofollow"><b>Community</b></a>
</p>
<p dir="auto">
    <a href="https://www.youtube.com/@Penpot" rel="nofollow"><b>Youtube</b></a>  •
    <a href="https://peertube.kaleidos.net/a/penpot_app/video-channels" rel="nofollow"><b>Peertube</b></a>  •
    <a href="https://www.linkedin.com/company/penpot/" rel="nofollow"><b>Linkedin</b></a>  •
    <a href="https://instagram.com/penpot.app" rel="nofollow"><b>Instagram</b></a>  •
    <a href="https://fosstodon.org/@penpot/" rel="nofollow"><b>Mastodon</b></a>  •
    <a href="https://bsky.app/profile/penpot.app" rel="nofollow"><b>Bluesky</b></a>  •
    <a href="https://twitter.com/penpotapp" rel="nofollow"><b>X</b></a>
</p>
<br>
<details open="">
  <summary>
    
    <span>Penpot_OpenYourEyes_.mp4</span>
    <span></span>
  </summary>

  <video src="https://private-user-images.githubusercontent.com/210353798/451453487-7c67fd7c-04d3-4c9b-88ec-b6f5e23f8332.mp4?jwt=eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3NjQyMjUzMDEsIm5iZiI6MTc2NDIyNTAwMSwicGF0aCI6Ii8yMTAzNTM3OTgvNDUxNDUzNDg3LTdjNjdmZDdjLTA0ZDMtNGM5Yi04OGVjLWI2ZjVlMjNmODMzMi5tcDQ_WC1BbXotQWxnb3JpdGhtPUFXUzQtSE1BQy1TSEEyNTYmWC1BbXotQ3JlZGVudGlhbD1BS0lBVkNPRFlMU0E1M1BRSzRaQSUyRjIwMjUxMTI3JTJGdXMtZWFzdC0xJTJGczMlMkZhd3M0X3JlcXVlc3QmWC1BbXotRGF0ZT0yMDI1MTEyN1QwNjMwMDFaJlgtQW16LUV4cGlyZXM9MzAwJlgtQW16LVNpZ25hdHVyZT1mOTFiYjA4NDU1Mzk5OGFiZTY3ZGIwNjZlNzRhY2E0OGI5NmUxM2UxMmI3MzFjZDhhMjI0YWIzMDBmNGMyNDJlJlgtQW16LVNpZ25lZEhlYWRlcnM9aG9zdCJ9.JAjDN-7eHZr_4NbEoQTO4qnSCwwq2ptsgERx06UQVdc" data-canonical-src="https://private-user-images.githubusercontent.com/210353798/451453487-7c67fd7c-04d3-4c9b-88ec-b6f5e23f8332.mp4?jwt=eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3NjQyMjUzMDEsIm5iZiI6MTc2NDIyNTAwMSwicGF0aCI6Ii8yMTAzNTM3OTgvNDUxNDUzNDg3LTdjNjdmZDdjLTA0ZDMtNGM5Yi04OGVjLWI2ZjVlMjNmODMzMi5tcDQ_WC1BbXotQWxnb3JpdGhtPUFXUzQtSE1BQy1TSEEyNTYmWC1BbXotQ3JlZGVudGlhbD1BS0lBVkNPRFlMU0E1M1BRSzRaQSUyRjIwMjUxMTI3JTJGdXMtZWFzdC0xJTJGczMlMkZhd3M0X3JlcXVlc3QmWC1BbXotRGF0ZT0yMDI1MTEyN1QwNjMwMDFaJlgtQW16LUV4cGlyZXM9MzAwJlgtQW16LVNpZ25hdHVyZT1mOTFiYjA4NDU1Mzk5OGFiZTY3ZGIwNjZlNzRhY2E0OGI5NmUxM2UxMmI3MzFjZDhhMjI0YWIzMDBmNGMyNDJlJlgtQW16LVNpZ25lZEhlYWRlcnM9aG9zdCJ9.JAjDN-7eHZr_4NbEoQTO4qnSCwwq2ptsgERx06UQVdc" controls="controls" muted="muted">

  </video>
</details>


<p dir="auto">Penpot is the first <strong>open-source</strong> design tool for design and code collaboration. Designers can create stunning designs, interactive prototypes, design systems at scale, while developers enjoy ready-to-use code and make their workflow easy and fast. And all of this with no handoff drama.</p>
<p dir="auto">Available on browser or self-hosted, Penpot works with open standards like SVG, CSS, HTML and JSON, and it’s free!</p>
<p dir="auto">The latest updates take Penpot even further. It’s the first design tool to integrate native <a href="https://penpot.dev/collaboration/design-tokens" rel="nofollow">design tokens</a>—a single source of truth to improve efficiency and collaboration between product design and development.
With the <a href="https://penpot.app/dev-diaries" rel="nofollow">huge 2.0 release</a>, Penpot took the platform to a whole new level. This update introduces the ground-breaking <a href="https://penpot.app/penpot-2.0" rel="nofollow">CSS Grid Layout feature</a>, a complete UI redesign, a new Components system, and much more.
For organizations that need extra service for its teams, <a href="https://cal.com/team/penpot/talk-to-us" rel="nofollow">get in touch</a></p>
<p dir="auto">🎇 Design, code, and Open Source meet at <a href="https://penpot.app/penpotfest" rel="nofollow">Penpot Fest</a>! Be part of the 2025 edition in Madrid, Spain, on October 9-10.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Table of contents</h2><a id="user-content-table-of-contents" aria-label="Permalink: Table of contents" href="#table-of-contents"></a></p>
<ul dir="auto">
<li><a href="#why-penpot">Why Penpot</a></li>
<li><a href="#getting-started">Getting Started</a></li>
<li><a href="#community">Community</a></li>
<li><a href="#contributing">Contributing</a></li>
<li><a href="#resources">Resources</a></li>
<li><a href="#license">License</a></li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Why Penpot</h2><a id="user-content-why-penpot" aria-label="Permalink: Why Penpot" href="#why-penpot"></a></p>
<p dir="auto">Penpot expresses designs as code. Designers can do their best work and see it will be beautifully implemented by developers in a two-way collaboration.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Plugin system</h3><a id="user-content-plugin-system" aria-label="Permalink: Plugin system" href="#plugin-system"></a></p>
<p dir="auto"><a href="https://penpot.app/penpothub/plugins" rel="nofollow">Penpot plugins</a> let you expand the platform's capabilities, give you the flexibility to integrate it with other apps, and design custom solutions.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Designed for developers</h3><a id="user-content-designed-for-developers" aria-label="Permalink: Designed for developers" href="#designed-for-developers"></a></p>
<p dir="auto">Penpot was built to serve both designers and developers and create a fluid design-code process. You have the choice to enjoy real-time collaboration or play "solo".</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Inspect mode</h3><a id="user-content-inspect-mode" aria-label="Permalink: Inspect mode" href="#inspect-mode"></a></p>
<p dir="auto">Work with ready-to-use code and make your workflow easy and fast. The inspect tab gives instant access to SVG, CSS and HTML code.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Self host your own instance</h3><a id="user-content-self-host-your-own-instance" aria-label="Permalink: Self host your own instance" href="#self-host-your-own-instance"></a></p>
<p dir="auto">Provide your team or organization with a completely owned collaborative design tool. Use Penpot's cloud service or deploy your own Penpot server.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Integrations</h3><a id="user-content-integrations" aria-label="Permalink: Integrations" href="#integrations"></a></p>
<p dir="auto">Penpot offers integration into the development toolchain, thanks to its support for webhooks and an API accessible through access tokens.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Building Design Systems: design tokens, components and variants</h3><a id="user-content-building-design-systems-design-tokens-components-and-variants" aria-label="Permalink: Building Design Systems: design tokens, components and variants" href="#building-design-systems-design-tokens-components-and-variants"></a></p>
<p dir="auto">Penpot brings design systems to code-minded teams: a single source of truth with native Design Tokens, Components, and Variants for scalable, reusable, and consistent UI across projects and platforms.</p>

<p dir="auto">
 <a target="_blank" rel="noopener noreferrer" href="https://private-user-images.githubusercontent.com/210353798/492141777-cce75ad6-f783-473f-8803-da9eb8255fef.jpg?jwt=eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3NjQyMjUzMDEsIm5iZiI6MTc2NDIyNTAwMSwicGF0aCI6Ii8yMTAzNTM3OTgvNDkyMTQxNzc3LWNjZTc1YWQ2LWY3ODMtNDczZi04ODAzLWRhOWViODI1NWZlZi5qcGc_WC1BbXotQWxnb3JpdGhtPUFXUzQtSE1BQy1TSEEyNTYmWC1BbXotQ3JlZGVudGlhbD1BS0lBVkNPRFlMU0E1M1BRSzRaQSUyRjIwMjUxMTI3JTJGdXMtZWFzdC0xJTJGczMlMkZhd3M0X3JlcXVlc3QmWC1BbXotRGF0ZT0yMDI1MTEyN1QwNjMwMDFaJlgtQW16LUV4cGlyZXM9MzAwJlgtQW16LVNpZ25hdHVyZT02YTIzZjUwNTM0ZDg1MGU0NWUyNzM1NjMyNDdlMThkZmMxMzQ0YzhjZDQ4MDNlMTU0MzA2Y2YzNGFmZjRiYTI3JlgtQW16LVNpZ25lZEhlYWRlcnM9aG9zdCJ9.NF2okr9Jy_WMyKYXacnLGLgmJSIbxJ3fEoFS7zRXFQI"><img src="https://private-user-images.githubusercontent.com/210353798/492141777-cce75ad6-f783-473f-8803-da9eb8255fef.jpg?jwt=eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3NjQyMjUzMDEsIm5iZiI6MTc2NDIyNTAwMSwicGF0aCI6Ii8yMTAzNTM3OTgvNDkyMTQxNzc3LWNjZTc1YWQ2LWY3ODMtNDczZi04ODAzLWRhOWViODI1NWZlZi5qcGc_WC1BbXotQWxnb3JpdGhtPUFXUzQtSE1BQy1TSEEyNTYmWC1BbXotQ3JlZGVudGlhbD1BS0lBVkNPRFlMU0E1M1BRSzRaQSUyRjIwMjUxMTI3JTJGdXMtZWFzdC0xJTJGczMlMkZhd3M0X3JlcXVlc3QmWC1BbXotRGF0ZT0yMDI1MTEyN1QwNjMwMDFaJlgtQW16LUV4cGlyZXM9MzAwJlgtQW16LVNpZ25hdHVyZT02YTIzZjUwNTM0ZDg1MGU0NWUyNzM1NjMyNDdlMThkZmMxMzQ0YzhjZDQ4MDNlMTU0MzA2Y2YzNGFmZjRiYTI3JlgtQW16LVNpZ25lZEhlYWRlcnM9aG9zdCJ9.NF2okr9Jy_WMyKYXacnLGLgmJSIbxJ3fEoFS7zRXFQI"></a>
</p>

<p dir="auto"><h2 tabindex="-1" dir="auto">Getting started</h2><a id="user-content-getting-started" aria-label="Permalink: Getting started" href="#getting-started"></a></p>
<p dir="auto">Penpot is the only design &amp; prototype platform that is deployment agnostic. You can use it in our <a href="https://design.penpot.app/" rel="nofollow">SAAS</a> or deploy it anywhere.</p>
<p dir="auto">Learn how to install it with Docker, Kubernetes, Elestio or other options on <a href="https://penpot.app/self-host" rel="nofollow">our website</a>.
<br></p>
<p dir="auto">
  <a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/2c5f051576d896757ae0630691a5067d45d1184b19adf4c4d995b87df72a1897/68747470733a2f2f736974652d6173736574732e706c61736d69632e6170702f32313638636635323464643534336361656666333233383465623965613061312e737667"><img src="https://camo.githubusercontent.com/2c5f051576d896757ae0630691a5067d45d1184b19adf4c4d995b87df72a1897/68747470733a2f2f736974652d6173736574732e706c61736d69632e6170702f32313638636635323464643534336361656666333233383465623965613061312e737667" alt="Open Source" data-canonical-src="https://site-assets.plasmic.app/2168cf524dd543caeff32384eb9ea0a1.svg"></a>
</p>

<p dir="auto"><h2 tabindex="-1" dir="auto">Community</h2><a id="user-content-community" aria-label="Permalink: Community" href="#community"></a></p>
<p dir="auto">We love the Open Source software community. Contributing is our passion and if it’s yours too, participate and <a href="https://community.penpot.app/c/help-us-improve-penpot/7" rel="nofollow">improve</a> Penpot. All your designs, code and ideas are welcome!</p>
<p dir="auto">If you need help or have any questions; if you’d like to share your experience using Penpot or get inspired; if you’d rather meet our community of developers and designers, <a href="https://community.penpot.app/" rel="nofollow">join our Community</a>!</p>
<p dir="auto">You will find the following categories:</p>
<ul dir="auto">
<li><a href="https://community.penpot.app/c/ask-for-help-using-penpot/6" rel="nofollow">Ask the Community</a></li>
<li><a href="https://community.penpot.app/c/technical/8" rel="nofollow">Troubleshooting</a></li>
<li><a href="https://community.penpot.app/c/help-us-improve-penpot/7" rel="nofollow">Help us Improve Penpot</a></li>
<li><a href="https://community.penpot.app/c/madewithpenpot/9" rel="nofollow">#MadeWithPenpot</a></li>
<li><a href="https://community.penpot.app/c/announcements/5" rel="nofollow">Events and Announcements</a></li>
<li><a href="https://community.penpot.app/c/inside-penpot/21" rel="nofollow">Inside Penpot</a></li>
<li><a href="https://community.penpot.app/c/penpot-in-your-language/12" rel="nofollow">Penpot in your language</a></li>
<li><a href="https://community.penpot.app/c/design-and-code-essentials/22" rel="nofollow">Design and Code Essentials</a></li>
</ul>

<p dir="auto">
  <a target="_blank" rel="noopener noreferrer" href="https://private-user-images.githubusercontent.com/5446186/320413592-6ac62220-a16c-46c9-ab21-d24ae357ed03.png?jwt=eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3NjQyMjUzMDEsIm5iZiI6MTc2NDIyNTAwMSwicGF0aCI6Ii81NDQ2MTg2LzMyMDQxMzU5Mi02YWM2MjIyMC1hMTZjLTQ2YzktYWIyMS1kMjRhZTM1N2VkMDMucG5nP1gtQW16LUFsZ29yaXRobT1BV1M0LUhNQUMtU0hBMjU2JlgtQW16LUNyZWRlbnRpYWw9QUtJQVZDT0RZTFNBNTNQUUs0WkElMkYyMDI1MTEyNyUyRnVzLWVhc3QtMSUyRnMzJTJGYXdzNF9yZXF1ZXN0JlgtQW16LURhdGU9MjAyNTExMjdUMDYzMDAxWiZYLUFtei1FeHBpcmVzPTMwMCZYLUFtei1TaWduYXR1cmU9MDQ1ZGQxMGQ2NTExZTFjYjAwMDY5ZjFiNmNiNzllYWFjZmQwNzczNDRmZjY3NDE1MmEwZDY0MjYwODdkYjFjMyZYLUFtei1TaWduZWRIZWFkZXJzPWhvc3QifQ.imeTph-Hw-93Jq36UmxJwCiVVvB51q4m_8xkx00iJzk"><img src="https://private-user-images.githubusercontent.com/5446186/320413592-6ac62220-a16c-46c9-ab21-d24ae357ed03.png?jwt=eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3NjQyMjUzMDEsIm5iZiI6MTc2NDIyNTAwMSwicGF0aCI6Ii81NDQ2MTg2LzMyMDQxMzU5Mi02YWM2MjIyMC1hMTZjLTQ2YzktYWIyMS1kMjRhZTM1N2VkMDMucG5nP1gtQW16LUFsZ29yaXRobT1BV1M0LUhNQUMtU0hBMjU2JlgtQW16LUNyZWRlbnRpYWw9QUtJQVZDT0RZTFNBNTNQUUs0WkElMkYyMDI1MTEyNyUyRnVzLWVhc3QtMSUyRnMzJTJGYXdzNF9yZXF1ZXN0JlgtQW16LURhdGU9MjAyNTExMjdUMDYzMDAxWiZYLUFtei1FeHBpcmVzPTMwMCZYLUFtei1TaWduYXR1cmU9MDQ1ZGQxMGQ2NTExZTFjYjAwMDY5ZjFiNmNiNzllYWFjZmQwNzczNDRmZjY3NDE1MmEwZDY0MjYwODdkYjFjMyZYLUFtei1TaWduZWRIZWFkZXJzPWhvc3QifQ.imeTph-Hw-93Jq36UmxJwCiVVvB51q4m_8xkx00iJzk" alt="Community"></a>
</p>

<p dir="auto"><h3 tabindex="-1" dir="auto">Code of Conduct</h3><a id="user-content-code-of-conduct" aria-label="Permalink: Code of Conduct" href="#code-of-conduct"></a></p>
<p dir="auto">Anyone who contributes to Penpot, whether through code, in the community, or at an event, must adhere to the
<a href="https://help.penpot.app/contributing-guide/coc/" rel="nofollow">code of conduct</a> and foster a positive and safe environment.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Contributing</h2><a id="user-content-contributing" aria-label="Permalink: Contributing" href="#contributing"></a></p>
<p dir="auto">Any contribution will make a difference to improve Penpot. How can you get involved?</p>
<p dir="auto">Choose your way:</p>
<ul dir="auto">
<li>Create and <a href="https://penpot.app/libraries-templates.html" rel="nofollow">share Libraries &amp; Templates</a> that will be helpful for the community</li>
<li>Invite your <a href="https://design.penpot.app/#/auth/register" rel="nofollow">team to join</a></li>
<li>Give this repo a star and follow us on Social Media: <a href="https://fosstodon.org/@penpot/" rel="nofollow">Mastodon</a>, <a href="https://www.youtube.com/c/Penpot" rel="nofollow">Youtube</a>, <a href="https://instagram.com/penpot.app" rel="nofollow">Instagram</a>, <a href="https://www.linkedin.com/company/penpotdesign" rel="nofollow">Linkedin</a>,  <a href="https://peertube.kaleidos.net/a/penpot_app" rel="nofollow">Peertube</a>, <a href="https://twitter.com/penpotapp" rel="nofollow">X</a> and <a href="https://bsky.app/profile/penpot.app" rel="nofollow">BlueSky</a></li>
<li>Participate in the <a href="https://community.penpot.app/" rel="nofollow">Community</a> space by asking and answering questions; reacting to others’ articles;  opening your own conversations and following along on decisions affecting the project.</li>
<li>Report bugs with our easy <a href="https://help.penpot.app/contributing-guide/reporting-bugs/" rel="nofollow">guide for bugs hunting</a> or <a href="https://github.com/penpot/penpot/issues">GitHub issues</a></li>
<li>Become a <a href="https://help.penpot.app/contributing-guide/translations" rel="nofollow">translator</a></li>
<li>Give feedback: <a href="mailto:support@penpot.app">Email us</a></li>
<li><strong>Contribute to Penpot's code:</strong> <a href="https://www.youtube.com/watch?v=TpN0osiY-8k" rel="nofollow">Watch this video</a> by Alejandro Alonso, CIO and developer at Penpot, where he gives us a hands-on demo of how to use Penpot’s repository and make changes in both front and back end</li>
</ul>
<p dir="auto">To find (almost) everything you need to know on how to contribute to Penpot, refer to the <a href="https://help.penpot.app/contributing-guide/" rel="nofollow">contributing guide</a>.</p>

<p dir="auto">
  <a target="_blank" rel="noopener noreferrer" href="https://private-user-images.githubusercontent.com/5446186/320413622-fea18923-dc06-49be-86ad-c3496a7956e6.png?jwt=eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3NjQyMjUzMDEsIm5iZiI6MTc2NDIyNTAwMSwicGF0aCI6Ii81NDQ2MTg2LzMyMDQxMzYyMi1mZWExODkyMy1kYzA2LTQ5YmUtODZhZC1jMzQ5NmE3OTU2ZTYucG5nP1gtQW16LUFsZ29yaXRobT1BV1M0LUhNQUMtU0hBMjU2JlgtQW16LUNyZWRlbnRpYWw9QUtJQVZDT0RZTFNBNTNQUUs0WkElMkYyMDI1MTEyNyUyRnVzLWVhc3QtMSUyRnMzJTJGYXdzNF9yZXF1ZXN0JlgtQW16LURhdGU9MjAyNTExMjdUMDYzMDAxWiZYLUFtei1FeHBpcmVzPTMwMCZYLUFtei1TaWduYXR1cmU9MWIyY2Q2OGFjMTRmYzM2ZmRiMTkyZjExZWUzNTNlMmE5YTg4MzI3MzU1Zjg3OWE3NWQzMjI4YjE1NGU0Y2Q1YyZYLUFtei1TaWduZWRIZWFkZXJzPWhvc3QifQ.aexCcjmHLC-KhcY6cBBOcnx1zLRflqht7aOfy8uxjRE"><img src="https://private-user-images.githubusercontent.com/5446186/320413622-fea18923-dc06-49be-86ad-c3496a7956e6.png?jwt=eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3NjQyMjUzMDEsIm5iZiI6MTc2NDIyNTAwMSwicGF0aCI6Ii81NDQ2MTg2LzMyMDQxMzYyMi1mZWExODkyMy1kYzA2LTQ5YmUtODZhZC1jMzQ5NmE3OTU2ZTYucG5nP1gtQW16LUFsZ29yaXRobT1BV1M0LUhNQUMtU0hBMjU2JlgtQW16LUNyZWRlbnRpYWw9QUtJQVZDT0RZTFNBNTNQUUs0WkElMkYyMDI1MTEyNyUyRnVzLWVhc3QtMSUyRnMzJTJGYXdzNF9yZXF1ZXN0JlgtQW16LURhdGU9MjAyNTExMjdUMDYzMDAxWiZYLUFtei1FeHBpcmVzPTMwMCZYLUFtei1TaWduYXR1cmU9MWIyY2Q2OGFjMTRmYzM2ZmRiMTkyZjExZWUzNTNlMmE5YTg4MzI3MzU1Zjg3OWE3NWQzMjI4YjE1NGU0Y2Q1YyZYLUFtei1TaWduZWRIZWFkZXJzPWhvc3QifQ.aexCcjmHLC-KhcY6cBBOcnx1zLRflqht7aOfy8uxjRE" alt="Libraries and templates"></a>
</p>

<p dir="auto"><h2 tabindex="-1" dir="auto">Resources</h2><a id="user-content-resources" aria-label="Permalink: Resources" href="#resources"></a></p>
<p dir="auto">You can ask and answer questions, have open-ended conversations, and follow along on decisions affecting the project.</p>
<p dir="auto">💾 <a href="https://help.penpot.app/technical-guide/" rel="nofollow">Documentation</a></p>
<p dir="auto">🚀 <a href="https://help.penpot.app/technical-guide/getting-started/" rel="nofollow">Getting Started</a></p>
<p dir="auto">✏️ <a href="https://www.youtube.com/playlist?list=PLgcCPfOv5v54WpXhHmNO7T-YC7AE-SRsr" rel="nofollow">Tutorials</a></p>
<p dir="auto">🏘️ <a href="https://help.penpot.app/technical-guide/developer/architecture/" rel="nofollow">Architecture</a></p>
<p dir="auto">📚 <a href="https://penpot.app/dev-diaries.html" rel="nofollow">Dev Diaries</a></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">License</h2><a id="user-content-license" aria-label="Permalink: License" href="#license"></a></p>
<div data-snippet-clipboard-copy-content="This Source Code Form is subject to the terms of the Mozilla Public
License, v. 2.0. If a copy of the MPL was not distributed with this
file, You can obtain one at http://mozilla.org/MPL/2.0/.

Copyright (c) KALEIDOS INC"><pre><code>This Source Code Form is subject to the terms of the Mozilla Public
License, v. 2.0. If a copy of the MPL was not distributed with this
file, You can obtain one at http://mozilla.org/MPL/2.0/.

Copyright (c) KALEIDOS INC
</code></pre></div>
<p dir="auto">Penpot is a Kaleidos’ <a href="https://kaleidos.net/" rel="nofollow">open source project</a></p>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Migrating the Main Zig Repository from GitHub to Codeberg (813 pts)]]></title>
            <link>https://ziglang.org/news/migrating-from-github-to-codeberg/</link>
            <guid>46064571</guid>
            <pubDate>Thu, 27 Nov 2025 01:49:00 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://ziglang.org/news/migrating-from-github-to-codeberg/">https://ziglang.org/news/migrating-from-github-to-codeberg/</a>, See on <a href="https://news.ycombinator.com/item?id=46064571">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="content" role="main">
    
      <p><a id="back" href="https://ziglang.org/news/">
        ← Back to
        <b>News</b>
        page
      </a></p>
    <h3>November 26, 2025</h3>
    
    <p><a href="https://codeberg.org/ziglang/zig" target="_blank">https://codeberg.org/ziglang/zig</a></p><p>Ever since <a href="https://codeberg.org/ziglang/zig/src/commit/8e08cf4bec80b87a7a22a18086a3db5c2c0f1772" target="_blank"><code>git init</code> ten years ago</a>, Zig has been hosted on GitHub. Unfortunately, when it <a href="https://github.blog/news-insights/company-news/github-microsoft/" target="_blank">sold out to Microsoft</a>, the <a href="https://craphound.com/category/enshittification/" target="_blank">clock started ticking</a>. “Please just give me 5 years before everything goes to shit,” I thought to myself. And here we are, 7 years later, living on borrowed time.</p><p>Putting aside GitHub’s <a href="https://www.vox.com/recode/2019/10/9/20906605/github-ice-contract-immigration-ice-dan-friedman" target="_blank">relationship with ICE</a>, it’s abundantly clear that the talented folks who used to work on the product have moved on to bigger and better things, with the remaining losers eager to inflict some kind of bloated, buggy JavaScript framework on us in the name of progress. Stuff that used to be snappy is now sluggish and often entirely broken.</p><p>More importantly, Actions is <a href="https://github.com/actions/runner/issues/3792#issuecomment-3182746514" target="_blank">created by monkeys</a> and <a href="https://github.com/actions/runner/issues/385" target="_blank">completely neglected</a>. After the <a href="https://www.businessinsider.com/github-ceo-developers-embrace-ai-or-get-out-2025-8" target="_blank">CEO of GitHub said to “embrace AI or get out”</a>, it seems the lackeys at Microsoft took the hint, because GitHub Actions started “vibe-scheduling”; choosing jobs to run seemingly at random. Combined with other bugs and inability to manually intervene, this causes our CI system to get so backed up that not even master branch commits get checked.</p><p>Rather than wasting donation money on more CI hardware to work around this crumbling infrastructure, we’ve opted to switch Git hosting providers instead.</p><p>As a bonus, we look forward to fewer violations (exhibit <a href="https://github.com/ziglang/zig/pull/25974" target="_blank">A</a>, <a href="https://github.com/ziglang/zig/pull/24983" target="_blank">B</a>, <a href="https://github.com/ziglang/www.ziglang.org/pull/502" target="_blank">C</a>) of our <a href="https://ziglang.org/code-of-conduct/#strict-no-llm-no-ai-policy">strict no LLM / no AI policy</a>, which I believe are at least in part due to GitHub aggressively pushing the “file an issue with Copilot” feature in everyone’s face.</p><h2>GitHub Sponsors</h2><p>The only concern we have in leaving GitHub behind has to do with GitHub Sponsors. This product was key to Zig’s early fundraising success, and it <a href="https://ziglang.org/news/2025-financials/">remains a large portion of our revenue today</a>. I can’t thank <a href="https://devonzuegel.com/" target="_blank">Devon Zuegel</a> enough. She appeared like an angel from heaven and single-handedly made GitHub into a viable source of income for thousands of developers. Under her leadership, the future of GitHub Sponsors looked bright, but sadly for us, she, too, moved on to bigger and better things. Since she left, that product as well has been neglected and is already starting to decline.</p><p>Although GitHub Sponsors is a large fraction of Zig Software Foundation’s donation income, <strong>we consider it a liability</strong>. We humbly ask if you, reader, are currently donating through GitHub Sponsors, that you consider <a href="https://ziglang.org/zsf/">moving your recurring donation to Every.org</a>, which is itself a non-profit organization.</p><p>As part of this, we are sunsetting the GitHub Sponsors perks. These perks are things like getting your name onto the home page, and getting your name into the release notes, based on how much you donate monthly. We are working with the folks at Every.org so that we can offer the equivalent perks through that platform.</p><h2>Migration Plan</h2><p>Effective immediately, I have made <a href="https://github.com/ziglang/zig/" target="_blank">ziglang/zig on GitHub</a> read-only, and the canonical origin/master branch of the main Zig project repository is <code>https://codeberg.org/ziglang/zig.git</code>.</p><p>Thank you to the Forgejo contributors who helped us with our issues switching to the platform, as well as the Codeberg folks who worked with us on the migration - in particular <a href="https://codeberg.org/earl-warren" target="_blank">Earl Warren</a>, <a href="https://codeberg.org/fnetX" target="_blank">Otto</a>, <a href="https://codeberg.org/Gusted" target="_blank">Gusted</a>, and <a href="https://codeberg.org/mfenniak" target="_blank">Mathieu Fenniak</a>.</p><p>In the end, we opted for a simple strategy, sidestepping GitHub’s aggressive vendor lock-in: leave the existing issues open and unmigrated, but start counting issues at 30000 on Codeberg so that all issue numbers remain unambiguous. Let us please consider the GitHub issues that remain open as metaphorically “copy-on-write”. <strong>Please leave all your existing GitHub issues and pull requests alone</strong>. No need to move your stuff over to Codeberg unless you need to make edits, additional comments, or rebase. <strong>We’re still going to look at the already open pull requests and issues</strong>; don’t worry.</p><hr><p>In this modern era of acquisitions, weak antitrust regulations, and platform capitalism leading to extreme concentrations of wealth, non-profits remain a bastion defending what remains of the commons.</p><p>Happy hacking,</p><p>Andrew</p>
    
  </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[AdBlock and Signal are for terrorists, according to French govt (2023) [video] (128 pts)]]></title>
            <link>https://www.youtube.com/watch?v=1q1hjmwLqe4</link>
            <guid>46063915</guid>
            <pubDate>Thu, 27 Nov 2025 00:20:37 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.youtube.com/watch?v=1q1hjmwLqe4">https://www.youtube.com/watch?v=1q1hjmwLqe4</a>, See on <a href="https://news.ycombinator.com/item?id=46063915">Hacker News</a></p>
&lt;Unparsable&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[C100 Developer Terminal (111 pts)]]></title>
            <link>https://caligra.com/</link>
            <guid>46063450</guid>
            <pubDate>Wed, 26 Nov 2025 23:22:24 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://caligra.com/">https://caligra.com/</a>, See on <a href="https://news.ycombinator.com/item?id=46063450">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-astro-cid-obnvinql=""> <p data-astro-cid-obnvinql="">
Most people don’t write code or manage data,
      and consumer devices are designed accordingly.
</p> <p data-astro-cid-obnvinql="">
But change isn’t made by most people.
      Progress comes from the people whose work
      improves our understanding and ability.
</p> <p data-astro-cid-obnvinql="">
Scientists and artists. Engineers and designers. Hackers and painters.
</p> <p data-astro-cid-obnvinql="">
We think the world needs a brand of computing that stands behind
      creative technical work, dedicated to creating instead of consuming.
</p> <p data-astro-cid-obnvinql="">
Caligra is a new computer company. Our goal is to help you make 
      the future.
</p> </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Running Unsupported iOS on Deprecated Devices (192 pts)]]></title>
            <link>https://nyansatan.github.io/run-unsupported-ios/</link>
            <guid>46063272</guid>
            <pubDate>Wed, 26 Nov 2025 22:57:56 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://nyansatan.github.io/run-unsupported-ios/">https://nyansatan.github.io/run-unsupported-ios/</a>, See on <a href="https://news.ycombinator.com/item?id=46063272">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
            

<p>Created on 26.11.25</p>

<p>Earlier this year I <a href="https://youtu.be/VTbShvf97kI">demoed</a> iOS 6 running on an iPod touch 3 - a device that Apple never gave iOS 6 to, making iOS 5.1.1 the latest build it can run</p>
<p>A few months later I also released a <a href="https://github.com/NyanSatan/SundanceInH2A">script</a> that generates an iOS 6 restore image installable on that iPod touch model</p>
<p>This article describes technical details behind this work. Certain proficiency in iOS internals is assumed</p>
<p><img alt="" src="https://nyansatan.github.io/resources/projects/sundanceh2a.jpg"></p>
<h2 id="ill-show-you-what-ios-is-made-of">I'll show you what iOS is made of</h2>
<p>First of all, let's recap what software components iOS consists of:</p>
<ol>
<li>
<p><strong>iBoot</strong> - the bootloader. Has 4 different types for different scenarios - iBSS, iBEC, LLB and iBoot</p>
</li>
<li>
<p><strong>Kernelcache</strong> - the OS kernel + kernel extensions (drivers) built into a single binary blob</p>
</li>
<li>
<p><strong>DeviceTree</strong> - structured list of hardware used by specific device model + some parameters that specify software behavior. The copy included in an IPSW is more of a template that is heavily modified by iBoot before jumping into kernel</p>
</li>
<li>
<p>Userspace filesystem - tiny <strong>restore ramdisk</strong> used purely for OS installation or the actual <strong>root filesystem</strong> of iOS installed persistently</p>
</li>
<li>
<p>Various firmwares for coprocessors, be they internal or external to the main SoC - like, baseband, Wi-Fi, Bluetooth, multitouch and etc.</p>
</li>
</ol>
<h2 id="iphone-3gs-tests">iPhone 3GS tests</h2>
<p>iPhone 3GS was released the same year as iPod touch 3 (2009), and has a very similar hardware (<strong>S5L8920X</strong> SoC vs. <strong>S5L8922X</strong>). But the most important part is that it actually got iOS 6 officially</p>
<p>Before doing anything on the iPod I decided to try to boot iOS 6.0 with iOS 5.1.1 iBoot &amp; DeviceTree on the iPhone and see what's gonna break and how</p>
<h2 id="devicetree">DeviceTree</h2>
<p>The most broken thing was DeviceTree - iOS 6 added a lot of new nodes and properties. To fix it in automated manner I wrote a stupid Python script that decodes and computes a diff between 2 DeviceTrees. Such diff can also be applied to another DeviceTree</p>
<p>The script is available in the <strong>SundanceInH2A</strong> <a href="https://github.com/NyanSatan/SundanceInH2A/blob/master/dt/ddt.py">repo</a></p>
<p>As I mentioned above a lot of things in a DeviceTree is filled by iBoot at runtime. One of such new properties is <code>nvram-proxy-data</code> in <code>chosen</code> node</p>
<p>The property must contain a raw NVRAM dump - leaving it empty will make kernel get stuck somewhere very early</p>
<p>For iPod touch 3 I also had to clean-up the diff out of iPhone-specific things before applying it to iPod's 5.1.1 DeviceTree</p>
<h2 id="iboot">iBoot</h2>
<p>iBoot didn't require any major changes in this case. Just typical Image3 signature check patch, boot-args injection and <code>debug-enabled</code> patch so kernel is going to actually respect AMFI boot-args</p>
<p>One important thing is to actually populate <code>nvram-proxy-data</code> dynamically, at least for normal boots (aka non-restore). Restore boot will be fine with some random NVRAM hardcoded into DeviceTree, but normal one will overwrite your actual NVRAM with the random one if it decides to sync it at some point</p>
<p>I do it by replacing a call to <code>UpdateDeviceTree()</code> with my own little function that calls the real <code>UpdateDeviceTree()</code>, but also populates actual <code>nvram-proxy-data</code> and <code>random-seed</code> (this one shouldn't be of any importance)</p>
<p>For boot-args I always add <code>amfi=0xff</code> to disable code-signing, but that's pretty cannonical as well</p>
<p>Please note that other iBoot+kernel combos might require more changes - if you ever try something and it doesn't work, I recommend looking into DeviceTree differences (both the initial template and how iBoot fills it) and also <code>boot_args</code> structure iBoot passes to kernel (not to be confused with boot-args <em>string</em>, the <code>boot_args</code> <em>structure</em> is a different thing)</p>
<h2 id="kernelcache">Kernelcache</h2>
<p>The most complex part. iPod touch 3 never got iOS 6 officialy, yes, but it was rumored that initially it was meant to have it, but Apple's marketing team said no. Either way, almost every internal iOS 6 build got both standalone S5L8922X kernel and even standalone kexts (including ones specific to iPod touch 3)</p>
<p>The question is how to load them all simultaneously. My initial idea was to do it just as older Mac OS X could do - load all kexts dynamically on bootloader level. Long story short, my strategy was the following:</p>
<ol>
<li>In iBoot context, load all kexts from filesystem - binary itself + Info.plist</li>
<li>Lay them out in memory and add corresponding entries to <code>chosen/memory-map</code> node of DeviceTree</li>
<li>Boot standalone kernel which will then pick them up and load</li>
</ol>
<p>The sad outcome:</p>
<pre><code>panic(cpu 0 caller 0x802e5223): "kern_return_t kxld_link_file(KXLDContext *, u_char *, u_long, const char *, void *, KXLDDependency *, u_int, u_char **, kxld_addr_t *) (com.apple.kec.corecrypto) called in kernel without kxld support"
</code></pre>
<p>The kernel has all the code to pick them up, but not to actually link...</p>
<h3 id="glueing-a-prelinked-kernelcache">Glueing a prelinked kernelcache</h3>
<p>So creating a legit kernelcache is the only way after all. I was already imagining all the horrors of writing software to parse and apply <code>LINKEDIT</code> and etc., but then it occured to me! Mac OS X (before Apple Silicon) was generating such kernelcaches somehow! What if we use that logic to build our iOS kernelcache?</p>
<pre><code>kcgen \
    -c output.bin \
    $(cat n18.10A403.kextlist | sed 's/^/--bundle-id /') \
    -kernel kernels_kexts_10A63970m/mach.development.s5l8922x \
    -arch armv7 \
    -all-personalities \
    -strip-symbols \
    -uncompressed \
    -- \
    kernels_kexts_10A63970m/Extensions
</code></pre>
<p>I used <code>/usr/local/bin/kcgen</code> from internal Sierra build (can be found online as "Phoenix A1708.dmg"), but it seems that even latest macOS <code>kextcache</code> can do it (included by default)</p>
<p>Here is a breakdown of the options:</p>
<ul>
<li>
<p><code>-c output.bin</code> - output file to write resulting kernelcache to</p>
</li>
<li>
<p><code>$(cat n18.10A403.kextlist | sed 's/^/--bundle-id /')</code> - this weird expression appends <code>--bundle-id</code> to every line from the file at <code>n18.10A403.kextlist</code>. This is to specify which kexts we'd like to include. How I created such list is described below</p>
</li>
<li>
<p><code>-arch armv7</code> - obviously only build armv7 slice</p>
</li>
<li>
<p><code>-all-personalities</code> - very important flag that prevents <em>irrelevant</em> IOKit personalities to be stripped. "Irrelevant" as in "irrelevant to current machine", meaning everything <em>relevant</em> to iPod touch 3 is going to be stripped</p>
</li>
<li>
<p><code>-strip-symbols</code> - strips unnecessary symbols. This flag can be omitted theoretically, but I recommend keeping it to make resulting kernelcache smaller</p>
</li>
<li>
<p><code>-uncompressed</code> - do not apply compression. Since we'll have to change one little thing later, compression would have to be reapplied anyway</p>
</li>
<li>
<p><code>--</code> means the rest of the args will point to directories to grab kexts from</p>
</li>
<li>
<p><code>kernels_kexts_10A63970m/Extensions</code> is a path to a folder containing kexts</p>
</li>
</ul>
<p>The little thing to do is to remove fat header. For some reason, it creates a fat Mach-O with a single slice. iBoot doesn't like it, so let's strip it:</p>
<pre><code>lipo -thin armv7 output.bin -o output.thin.bin
</code></pre>
<p>The kernel cache is ready now! Just needs to be compressed and packaged into Image3 container</p>
<h4 id="about-kext-lists">About kext lists</h4>
<p>Once again I compared iPhone 3GS' iOS 5.1.1 vs. 6.0 - some kexts were added, some removed, some changed their bundle IDs, some were irrelevant for iPod touch 3</p>
<p>Do not forget to include the pseudo-extensions as well!</p>
<p>Samples can be found in <strong>SundanceInH2A</strong> <a href="https://github.com/NyanSatan/SundanceInH2A/tree/master/kc/kext_lists">repository</a></p>
<h4 id="about-iokit-personalities">About IOKit personalities</h4>
<p>In this specific case I had to patch up Info.plist of the Wi-Fi kext. As always there is a sample in the <a href="https://github.com/NyanSatan/SundanceInH2A/blob/master/kc/iokit_personalities.plist">repo</a></p>
<h2 id="restore-ramdisk-filesystem">Restore ramdisk filesystem</h2>
<p>Pretty cannonical here. I patched <code>asr</code> as usual and also had to move <code>options.n88.plist</code> to <code>options.n18.plist</code> so it can lay out partitions properly</p>
<p>However, I also have to install the iBoot exploit. To do that I reimplement <code>rc.boot</code> binary:</p>
<ol>
<li>
<p>Remount ramdisk and set <code>umask</code> just like the original one does</p>
</li>
<li>
<p>Call <code>restored_external</code>, but with <code>-server</code> argument, so it doesn't reboot after finishing restore</p>
</li>
<li>
<p>If restore was completed properly, I add a third partition, write the exploit there and set <code>boot-partition</code> to <code>2</code></p>
</li>
<li>
<p>Reboot the device</p>
</li>
</ol>
<p>My implementation is available guess where? Yes, in the <a href="https://github.com/NyanSatan/SundanceInH2A/tree/master/rc_boot">repository</a></p>
<h2 id="root-filesystem">Root filesystem</h2>
<p>This needed a lot of changes:</p>
<ol>
<li>
<p>Add matching SpringBoard's hardware feature plist (<code>/System/Library/CoreServices/SpringBoard.app/N18AP.plist</code> in this case)</p>
<ul>
<li>
<p>I took the iOS 5.1.1 variant as a base and added iOS 6 specific capabilities</p>
</li>
<li>
<p>I tried to keep <em>original</em> enough Home screen icon order by <em>merging</em> iPod touch 3 iOS 5.1.1 and iPod touch 4 6.x layouts</p>
</li>
</ul>
</li>
<li>
<p>Add multitouch &amp; Wi-Fi firmwares</p>
<ul>
<li>I use versions from 5.1.1</li>
</ul>
</li>
<li>
<p>Add Bluetooth firmware and scripts</p>
<ul>
<li>
<p>This is more complicated, as those are all hardcoded into <code>/usr/sbin/BlueTool</code></p>
</li>
<li>
<p>Luckily, they can also be overriden by files in <code>/etc/bluetool</code> - as always check my code for reference</p>
</li>
<li>
<p>I extracted both firmware and scripts from 5.1.1 <code>BlueTool</code></p>
</li>
</ul>
</li>
<li>
<p><strong>FairPlay</strong> daemon is limited to <code>N88AP</code> (iPhone 3GS)</p>
<ul>
<li>
<p>It has <code>LimitLoadToHardware</code> key in its' LaunchDaemon plist</p>
</li>
<li>
<p>But if we simply remove the key, it works on iPod touch 3 as well</p>
</li>
<li>
<p>This is important, because otherwise we cannot activate device through Apple's servers</p>
</li>
<li>
<p>This trick will be harder to pull off on iOS 6.1+ because they load LaunchDaemons from a signed cache. Still can be bypassed in many ways - for instance, patching <code>launchd</code> or forcefully loading another plist via <code>launchctl</code></p>
</li>
</ul>
</li>
<li>
<p>DYLD shared cache patches</p>
<ol>
<li>
<p>Product ID map patch</p>
<ul>
<li>iOS 6 brings a concept of "product ID" in the form of a long byte sequence</li>
<li>It is filled by iBoot into <code>product</code> node of DeviceTree (which didn't even exist before)</li>
<li>I hardcode the value of iPhone 3GS straight into DeviceTree (<code>8784AE8D7066B0F0136BE91DCFE632A436FFD6FB</code>)</li>
<li>There is also a short form of this identifier - 16-bit integer - which existed before iOS 6</li>
<li>iPhone 3GS is <code>0x2714</code> and the iPod is <code>0x2715</code></li>
<li><strong>MobileGestalt</strong> framework has a table that matches the short form by the long one - I swap <code>0x2714</code> with <code>0x2715</code> there</li>
<li>I believe it's better for iTunes and etc.</li>
</ul>
</li>
<li>
<p><code>getDeviceVariant()</code> patch</p>
<ul>
<li><strong>MobileGestalt</strong> once again messes us up our business</li>
<li><em>Device variant</em> is a letter - usually "A" or "B"</li>
<li>It seems to depend on Wi-Fi transciever vendor used in exact device (?)</li>
<li>iOS 6 fails miserably to determine this value for iPod touch 3</li>
<li>This crashes activation process, for example</li>
<li>To fix it, I patch the function to always return "A" (in form of <code>CFString</code>)</li>
</ul>
</li>
<li>
<p>Fixing code signature</p>
<ul>
<li>This is much easier than most people think</li>
<li>Shared cache files have the same format of signature as normal Mach-Os</li>
<li>And since it's just ad-hoc, all you need to do is to recalculate SHA-1 hash for pages you modified and update the signature</li>
<li>So easy, it can be done with just a hex-editor</li>
</ul>
</li>
</ol>
</li>
</ol>
<h2 id="the-iboot-exploit">The iBoot exploit</h2>
<p>iOS 5 iBoot had a bug in <strong>HFS+</strong> filesystem driver. I did make an exploit many years ago but it was <em>bad</em>. Like, truly <em>bad</em>. I reimplemented it from scratch for this project making it deterministic (hopefully...)</p>
<p>This subject probably deserves a separate article</p>
<h2 id="conclusion-future-plans">Conclusion &amp; future plans</h2>
<p>This was not easy to do, and yet easier than I expected initially</p>
<p>After releasing the tool many people asked me about jailbreaking. The old tools are not going to work, but it should be easy to just patch the kernel and drop Cydia tarball onto the filesystem. I guess I will give it a try later</p>
<p>There was another device that Apple dropped support for in that year - iPad 1. I will try that soon enough as well</p>
<p>I hope that the information from this write-up will help you making other crazy combinations, like iOS 4 on iPhone 4S or iOS 5 on iPad mini 1</p>
        </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Bring Bathroom Doors Back to Hotels (747 pts)]]></title>
            <link>https://bringbackdoors.com/</link>
            <guid>46063072</guid>
            <pubDate>Wed, 26 Nov 2025 22:26:36 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://bringbackdoors.com/">https://bringbackdoors.com/</a>, See on <a href="https://news.ycombinator.com/item?id=46063072">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
<p>I’m done. I’m done arriving at hotels and discovering that they have removed the bathroom door. Something that should be as standard as having a bed, has been sacrificed in the name of “aesthetic”.</p>



<p>I get it, you can save on material costs and make the room feel bigger, but what about my dignity??? I can’t save that when you don’t include a bathroom door.</p>



<p>It’s why I’ve built this website, where I compiled hotels that are guaranteed to have bathroom doors, and hotels that need to work on privacy. </p>



<p>I’ve emailed hundreds of hotels and I asked them two things: do your doors close all the way, and are they made of glass? Everyone that says yes to their doors closing, and no to being made of glass has been sorted by price range and city for you to easily find places to stay that are <strong><em>guaranteed</em></strong> to have a bathroom door.</p>







<hr>



<p>Quickly check to see if the hotel you’re thinking of booking has been reported as lacking in doors by a previous guest.</p>







<hr>



<p>Finally, this passion project could not exist without people submitting hotels without bathroom doors for public shaming. If you’ve stayed at a doorless hotel send me an email with the hotel name to bringbackdoors@gmail.com, or send me a <a href="https://www.instagram.com/bring_back_doors/" target="_blank" rel="noreferrer noopener">DM on Instagram</a> with the hotel name and a photo of the doorless setup to be publicly posted.</p>



<p>Let’s name and shame these hotels to protect the dignity of future travelers.</p>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[EU Council approves Chat Control mandate for negotiation with Parliament (135 pts)]]></title>
            <link>https://www.techradar.com/vpn/vpn-privacy-security/chat-control-eu-lawmakers-finally-agree-on-the-voluntary-scanning-of-your-private-chats</link>
            <guid>46062777</guid>
            <pubDate>Wed, 26 Nov 2025 21:52:14 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.techradar.com/vpn/vpn-privacy-security/chat-control-eu-lawmakers-finally-agree-on-the-voluntary-scanning-of-your-private-chats">https://www.techradar.com/vpn/vpn-privacy-security/chat-control-eu-lawmakers-finally-agree-on-the-voluntary-scanning-of-your-private-chats</a>, See on <a href="https://news.ycombinator.com/item?id=46062777">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-widget-type="contentparsed" id="content">
<section>
<div>
<div>
<picture data-new-v2-image="true">
<source type="image/webp" srcset="https://cdn.mos.cms.futurecdn.net/57n4iicSB8ANTSiAk56Hjk-1920-80.jpg.webp 1920w, https://cdn.mos.cms.futurecdn.net/57n4iicSB8ANTSiAk56Hjk-1200-80.jpg.webp 1200w, https://cdn.mos.cms.futurecdn.net/57n4iicSB8ANTSiAk56Hjk-1024-80.jpg.webp 1024w, https://cdn.mos.cms.futurecdn.net/57n4iicSB8ANTSiAk56Hjk-970-80.jpg.webp 970w, https://cdn.mos.cms.futurecdn.net/57n4iicSB8ANTSiAk56Hjk-650-80.jpg.webp 650w, https://cdn.mos.cms.futurecdn.net/57n4iicSB8ANTSiAk56Hjk-480-80.jpg.webp 480w, https://cdn.mos.cms.futurecdn.net/57n4iicSB8ANTSiAk56Hjk-320-80.jpg.webp 320w" sizes="(min-width: 1000px) 600px, calc(100vw - 40px)">
<img src="https://cdn.mos.cms.futurecdn.net/57n4iicSB8ANTSiAk56Hjk.jpg" alt="Danish Justice Minister Peter Hummelgaard gives a doorstep statement after a briefing on drones at the Ministry of Justice on September 29, 2025, following recent drone disturbances over Denmark. " srcset="https://cdn.mos.cms.futurecdn.net/57n4iicSB8ANTSiAk56Hjk-1920-80.jpg 1920w, https://cdn.mos.cms.futurecdn.net/57n4iicSB8ANTSiAk56Hjk-1200-80.jpg 1200w, https://cdn.mos.cms.futurecdn.net/57n4iicSB8ANTSiAk56Hjk-1024-80.jpg 1024w, https://cdn.mos.cms.futurecdn.net/57n4iicSB8ANTSiAk56Hjk-970-80.jpg 970w, https://cdn.mos.cms.futurecdn.net/57n4iicSB8ANTSiAk56Hjk-650-80.jpg 650w, https://cdn.mos.cms.futurecdn.net/57n4iicSB8ANTSiAk56Hjk-480-80.jpg 480w, https://cdn.mos.cms.futurecdn.net/57n4iicSB8ANTSiAk56Hjk-320-80.jpg 320w" sizes="(min-width: 1000px) 600px, calc(100vw - 40px)" data-new-v2-image="true" data-original-mos="https://cdn.mos.cms.futurecdn.net/57n4iicSB8ANTSiAk56Hjk.jpg" data-pin-media="https://cdn.mos.cms.futurecdn.net/57n4iicSB8ANTSiAk56Hjk.jpg" data-pin-nopin="true" fetchpriority="high">
</picture>
</div>
<figcaption>
<span>Image credit: Pixabay</span>
<span>(Image credit: Photo by Thomas Traasdahl / Ritzau Scanpix / AFP) / Denmark OUT (Photo by THOMAS TRAASDAHL/Ritzau Scanpix/AFP via Getty Images)</span>
</figcaption>
</div>
<div id="article-body">
<hr id="08a96fae-3168-4aa8-a240-3c7c795865f3"><ul id="4835e0bb-7e10-4625-bba4-a68465e272ed"><li><strong>The EU Council reached an agreement on the Child Sexual Abuse Regulation </strong></li><li><strong>Voluntary chat scanning remains in the bill despite privacy backlash</strong></li><li><strong>The Council now prepares to start negotiations with the Parliament</strong></li></ul><hr id="d0446557-3ed4-42aa-ad16-b21bc874733a"><p id="635fc126-2793-403b-8e3c-6df60d61901e">The EU Council has finally reached an agreement on the controversial Child Sexual Abuse Regulation (CSAR) after more than three years of failed attempts.</p><p>Nicknamed <a data-analytics-id="inline-link" href="https://www.techradar.com/vpn/vpn-privacy-security/the-eu-has-never-been-closer-to-agreeing-on-the-scanning-of-your-private-chats-but-how-did-we-get-here" data-mrf-recirculation="inline-link" data-before-rewrite-localise="https://www.techradar.com/vpn/vpn-privacy-security/the-eu-has-never-been-closer-to-agreeing-on-the-scanning-of-your-private-chats-but-how-did-we-get-here">Chat Control</a> by its critics, the agreement has kept cryptographers, technologists, encrypted service providers, and privacy experts alike in turmoil since its inception.</p><p id="635fc126-2793-403b-8e3c-6df60d61901e-2">Presidency after presidency, the bill has taken many shapes. But its most controversial feature is an obligation for all messaging service providers operating in the EU – including those using end-to-end-encryption – to scan their users' private chats on the lookout for child sexual abuse material (CSAM).</p><p>At the beginning of the month, the Danish Presidency decided to change its approach with a <a data-analytics-id="inline-link" href="https://www.techradar.com/vpn/vpn-privacy-security/chat-control-isnt-dead-denmark-has-a-new-proposal-heres-all-we-know" data-mrf-recirculation="inline-link" data-before-rewrite-localise="https://www.techradar.com/vpn/vpn-privacy-security/chat-control-isnt-dead-denmark-has-a-new-proposal-heres-all-we-know">new compromise text</a> that makes the chat scanning voluntary, instead. That turned to be a winning move, with the proposal managing to reach an agreement in the Council on Wednesday, November 26, 2025.</p><p>Privacy experts are unlikely to celebrate, though. The decision came a few days after a group of scientists wrote yet another open letter warning that the latest text still "<a data-analytics-id="inline-link" href="https://www.techradar.com/vpn/vpn-privacy-security/chat-control-brings-high-risks-to-society-say-privacy-experts" data-mrf-recirculation="inline-link" data-before-rewrite-localise="https://www.techradar.com/vpn/vpn-privacy-security/chat-control-brings-high-risks-to-society-say-privacy-experts">brings high risks to society</a>." That's after other privacy experts deemed the new proposal a "<a data-analytics-id="inline-link" href="https://www.techradar.com/vpn/vpn-privacy-security/this-is-a-political-deception-new-chat-control-convinces-lawmakers-but-not-privacy-experts-yet" data-mrf-recirculation="inline-link" data-before-rewrite-localise="https://www.techradar.com/vpn/vpn-privacy-security/this-is-a-political-deception-new-chat-control-convinces-lawmakers-but-not-privacy-experts-yet">political deception</a>" rather than an actual fix.</p><p>The EU Council is now preparing to start negotiations with the European Parliament, hoping to agree on the final terms of the regulation.</p><h2 id="what-we-know-about-the-council-agreement-3">What we know about the Council agreement</h2><figure data-bordeaux-image-check="" id="a727a966-0ac8-4307-8a20-81f415e8cea9"><div><p> <picture data-new-v2-image="true">
<source type="image/webp" srcset="https://cdn.mos.cms.futurecdn.net/5NsvMB6arEjzGkWxkstoDj-1200-80.jpg.webp 1200w, https://cdn.mos.cms.futurecdn.net/5NsvMB6arEjzGkWxkstoDj-1024-80.jpg.webp 1024w, https://cdn.mos.cms.futurecdn.net/5NsvMB6arEjzGkWxkstoDj-970-80.jpg.webp 970w, https://cdn.mos.cms.futurecdn.net/5NsvMB6arEjzGkWxkstoDj-650-80.jpg.webp 650w, https://cdn.mos.cms.futurecdn.net/5NsvMB6arEjzGkWxkstoDj-480-80.jpg.webp 480w, https://cdn.mos.cms.futurecdn.net/5NsvMB6arEjzGkWxkstoDj-320-80.jpg.webp 320w" sizes="(min-width: 1000px) 970px, calc(100vw - 40px)">
<img src="https://cdn.mos.cms.futurecdn.net/5NsvMB6arEjzGkWxkstoDj.jpg" alt="EU flags outside administrative building" srcset="https://cdn.mos.cms.futurecdn.net/5NsvMB6arEjzGkWxkstoDj-1200-80.jpg 1200w, https://cdn.mos.cms.futurecdn.net/5NsvMB6arEjzGkWxkstoDj-1024-80.jpg 1024w, https://cdn.mos.cms.futurecdn.net/5NsvMB6arEjzGkWxkstoDj-970-80.jpg 970w, https://cdn.mos.cms.futurecdn.net/5NsvMB6arEjzGkWxkstoDj-650-80.jpg 650w, https://cdn.mos.cms.futurecdn.net/5NsvMB6arEjzGkWxkstoDj-480-80.jpg 480w, https://cdn.mos.cms.futurecdn.net/5NsvMB6arEjzGkWxkstoDj-320-80.jpg 320w" sizes="(min-width: 1000px) 970px, calc(100vw - 40px)" loading="lazy" data-new-v2-image="true" data-original-mos="https://cdn.mos.cms.futurecdn.net/5NsvMB6arEjzGkWxkstoDj.jpg" data-pin-media="https://cdn.mos.cms.futurecdn.net/5NsvMB6arEjzGkWxkstoDj.jpg">
</picture></p></div><figcaption itemprop="caption description"><span itemprop="copyrightHolder">(Image credit: Pixabay)</span></figcaption></figure><p id="87bd328d-d282-4788-996b-7b8857d2514d">As per the <a data-analytics-id="inline-link" href="https://www.consilium.europa.eu/en/press/press-releases/2025/11/26/child-sexual-abuse-council-reaches-position-on-law-protecting-children-from-online-abuse/" target="_blank" data-url="https://www.consilium.europa.eu/en/press/press-releases/2025/11/26/child-sexual-abuse-council-reaches-position-on-law-protecting-children-from-online-abuse/" referrerpolicy="no-referrer-when-downgrade" data-hl-processed="none" data-mrf-recirculation="inline-link">EU Council announcement</a>, the new law imposes a series of obligations on digital companies. Under the new rules, online service providers will be required to assess how their platforms could be misused and, based on the results, may need to "implement mitigating measures to counter that risk," the Council notes.</p><p>The Council also introduces three risk categories of online services. Those deemed to be a high-risk can be forced "to contribute to the development of technologies to mitigate the risks relating to their services." Voluntary scanning also remains in the bill.</p><p>A new EU agency is then tasked to oversee the implementation of the new rules.</p><p>"I'm glad that the member states have finally agreed on a way forward that includes a number of obligations for providers of communication services to combat the spread of child sexual abuse material," said Danish Minister for Justice, Peter Hummelgaard.</p><p>But concerns about how the agreement threatens our digital rights persist, with one person on the forum, <a data-analytics-id="inline-link" href="https://news.ycombinator.com/item?id=46056358" target="_blank" rel="nofollow" data-url="https://news.ycombinator.com/item?id=46056358" referrerpolicy="no-referrer-when-downgrade" data-hl-processed="none" data-mrf-recirculation="inline-link">Hacker News</a>, saying the Danish "government has today turned the EU into a tool for total surveillance, I don't know if there can be any return from."</p><p>As trilogue negotiations approach, the ongoing challenge for legislators remains striking the right balance between halting abuse online, without compromising on fundamental rights and strong <a data-analytics-id="inline-link" href="https://www.techradar.com/vpn/what-is-encryption" data-mrf-recirculation="inline-link" data-before-rewrite-localise="https://www.techradar.com/vpn/what-is-encryption">encryption</a>.</p><hr id="8a971148-d155-4c6b-8674-991e17b84f0b"><p id="e3ea0db0-4dda-45c6-bde9-c0378b0a11b0"><a data-analytics-id="inline-link" href="https://news.google.com/publications/CAAqKAgKIiJDQklTRXdnTWFnOEtEWFJsWTJoeVlXUmhjaTVqYjIwb0FBUAE?hl=en-GB&amp;gl=GB&amp;ceid=GB%3Aen" target="_blank" data-url="https://news.google.com/publications/CAAqKAgKIiJDQklTRXdnTWFnOEtEWFJsWTJoeVlXUmhjaTVqYjIwb0FBUAE?hl=en-GB&amp;gl=GB&amp;ceid=GB%3Aen" referrerpolicy="no-referrer-when-downgrade" data-hl-processed="none" data-mrf-recirculation="inline-link"><em><strong>Follow TechRadar on Google News</strong></em></a> and<em> </em><a data-analytics-id="inline-link" href="https://www.google.com/preferences/source?q=techradar.com" target="_blank" data-url="https://www.google.com/preferences/source?q=techradar.com" referrerpolicy="no-referrer-when-downgrade" data-hl-processed="none" data-mrf-recirculation="inline-link"><em><strong>add us as a preferred source</strong></em></a><em> to get our expert news, reviews, and opinion in your feeds. Make sure to click the Follow button!</em></p><hr id="02ae0dcd-6a1b-47f9-a3aa-48554e0b9ffe">
</div>



<div id="slice-container-authorBio-ARKCmKLT39YKzdWm6qkqB8"><p>Chiara is a multimedia journalist committed to covering stories to help promote the rights and denounce the abuses of the digital side of life – wherever cybersecurity, markets, and politics tangle up. She believes an open, uncensored, and private internet is a basic human need and wants to use her knowledge of VPNs to help readers take back control. She writes news, interviews, and analysis on data privacy, online censorship, digital rights, tech policies, and security software, with a special focus on VPNs, for TechRadar and TechRadar Pro. Got a story, tip-off, or something tech-interesting to say? Reach out to chiara.castro@futurenet.com</p></div>
</section>




</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[The EU made Apple adopt new Wi-Fi standards, and now Android can support AirDrop (549 pts)]]></title>
            <link>https://arstechnica.com/gadgets/2025/11/the-eu-made-apple-adopt-new-wi-fi-standards-and-now-android-can-support-airdrop/</link>
            <guid>46062504</guid>
            <pubDate>Wed, 26 Nov 2025 21:25:36 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://arstechnica.com/gadgets/2025/11/the-eu-made-apple-adopt-new-wi-fi-standards-and-now-android-can-support-airdrop/">https://arstechnica.com/gadgets/2025/11/the-eu-made-apple-adopt-new-wi-fi-standards-and-now-android-can-support-airdrop/</a>, See on <a href="https://news.ycombinator.com/item?id=46062504">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="app">
    <p><a href="#main">
  Skip to content
</a></p>



<main id="main">
            <article data-id="2128654">
  
  <header>
  <div>
    <div>
      <p><span>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 40 40"><defs><clipPath id="section-gadgets_svg__a"><path fill="none" d="M0 0h40v40H0z"></path></clipPath><clipPath id="section-gadgets_svg__b"><path fill="none" d="M0 0h40v40H0z"></path></clipPath></defs><g clip-path="url(#section-gadgets_svg__a)"><g fill="currentColor" clip-path="url(#section-gadgets_svg__b)"><path d="M38 22c1.1 0 2-.9 2-2s-.9-2-2-2h-2v-6h2c1.1 0 2-.9 2-2s-.9-2-2-2h-2V4h-4V2c0-1.1-.9-2-2-2s-2 .9-2 2v2h-6V2c0-1.1-.9-2-2-2s-2 .9-2 2v2h-6V2c0-1.1-.9-2-2-2S8 .9 8 2v2H4v4H2c-1.1 0-2 .9-2 2s.9 2 2 2h2v6H2c-1.1 0-2 .9-2 2s.9 2 2 2h2v6H2c-1.1 0-2 .9-2 2s.9 2 2 2h2v4h4v2c0 1.1.9 2 2 2s2-.9 2-2v-2h6v2c0 1.1.9 2 2 2s2-.9 2-2v-2h6v2c0 1.1.9 2 2 2s2-.9 2-2v-2h4v-4h2c1.1 0 2-.9 2-2s-.9-2-2-2h-2v-6zm-6 10H8V8h24z"></path><path d="M24.7 17.3 20 12h-7.1c-.6 0-1 .4-1 1s.4 1 1 1h6.3l4.1 4.7L20 22h8v-8z"></path><path d="m15.2 22.7 4.7 5.3H27c.6 0 1-.4 1-1s-.4-1-1-1h-6.3l-4.1-4.7 3.3-3.3h-8v8z"></path></g></g></svg>
  </span>
  <span>
    cats and dogs living together
  </span>
</p>
    </div>

    

    <p>
      Google’s Pixel 10 works with AirDrop, and other phones should follow later.
    </p>

          
    
    <div>
            <p><a data-pswp-width="1920" data-pswp-height="1080" data-pswp-srcset="https://cdn.arstechnica.net/wp-content/uploads/2025/11/Google-Pixel-10-4.jpg 1920w, https://cdn.arstechnica.net/wp-content/uploads/2025/11/Google-Pixel-10-4-640x360.jpg 640w, https://cdn.arstechnica.net/wp-content/uploads/2025/11/Google-Pixel-10-4-1024x576.jpg 1024w, https://cdn.arstechnica.net/wp-content/uploads/2025/11/Google-Pixel-10-4-768x432.jpg 768w, https://cdn.arstechnica.net/wp-content/uploads/2025/11/Google-Pixel-10-4-1536x864.jpg 1536w, https://cdn.arstechnica.net/wp-content/uploads/2025/11/Google-Pixel-10-4-384x216.jpg 384w, https://cdn.arstechnica.net/wp-content/uploads/2025/11/Google-Pixel-10-4-1152x648.jpg 1152w, https://cdn.arstechnica.net/wp-content/uploads/2025/11/Google-Pixel-10-4-980x551.jpg 980w, https://cdn.arstechnica.net/wp-content/uploads/2025/11/Google-Pixel-10-4-1440x810.jpg 1440w" data-cropped="false" href="https://cdn.arstechnica.net/wp-content/uploads/2025/11/Google-Pixel-10-4.jpg" target="_blank">
              <img width="1920" height="1080" src="https://cdn.arstechnica.net/wp-content/uploads/2025/11/Google-Pixel-10-4.jpg" alt="" loading="eager" decoding="async" fetchpriority="high" srcset="https://cdn.arstechnica.net/wp-content/uploads/2025/11/Google-Pixel-10-4.jpg 1920w, https://cdn.arstechnica.net/wp-content/uploads/2025/11/Google-Pixel-10-4-640x360.jpg 640w, https://cdn.arstechnica.net/wp-content/uploads/2025/11/Google-Pixel-10-4-1024x576.jpg 1024w, https://cdn.arstechnica.net/wp-content/uploads/2025/11/Google-Pixel-10-4-768x432.jpg 768w, https://cdn.arstechnica.net/wp-content/uploads/2025/11/Google-Pixel-10-4-1536x864.jpg 1536w, https://cdn.arstechnica.net/wp-content/uploads/2025/11/Google-Pixel-10-4-384x216.jpg 384w, https://cdn.arstechnica.net/wp-content/uploads/2025/11/Google-Pixel-10-4-1152x648.jpg 1152w, https://cdn.arstechnica.net/wp-content/uploads/2025/11/Google-Pixel-10-4-980x551.jpg 980w, https://cdn.arstechnica.net/wp-content/uploads/2025/11/Google-Pixel-10-4-1440x810.jpg 1440w" sizes="(max-width: 1920px) 100vw, 1920px">
            </a></p><div id="caption-2128706">
    
    <p>
      Google's Pixel 10 series now features compatibility with Apple's AirDrop.

              <span>
          Credit:

          
          Ryan Whitwam

                  </span>
          </p>
  </div>
          </div>

    <div>
    
    <p>
      Google's Pixel 10 series now features compatibility with Apple's AirDrop.

              <span>
          Credit:

          
          Ryan Whitwam

                  </span>
          </p>
  </div>
  </div>
</header>


  

  
      
    
    <div>
                      
                      
          
<p>Last year, Apple <a href="https://arstechnica.com/gadgets/2024/09/ios-18-brings-rcs-to-major-carrier-iphones-but-prepaid-plans-are-still-waiting/">finally added support</a> for Rich Communications Services (RCS) texting to its platforms, improving consistency, reliability, and <a href="https://arstechnica.com/gadgets/2025/03/rcs-texting-updates-will-bring-end-to-end-encryption-to-green-bubble-chats/">security</a> when exchanging green-bubble texts between the competing iPhone and Android ecosystems. Today, Google is announcing another small step forward in interoperability, pointing to a slightly less annoying future for friend groups or households where not everyone owns an iPhone.</p>
<p>Google <a href="https://blog.google/products/android/quick-share-airdrop/">has updated</a> Android’s Quick Share feature to support Apple’s AirDrop, which allows users of Apple devices to share files directly using a local peer-to-peer Wi-Fi connection. Apple devices with AirDrop enabled and set to “everyone for 10 minutes” mode will show up in the Quick Share device list just like another Android phone would, and Android devices that support this new Quick Share version will also show up in the AirDrop menu.</p>
<p>Google will only support this feature on the Pixel 10 series, at least to start. The company is “looking forward to improving the experience and expanding it to more Android devices,” but it didn’t announce anything about a timeline or any hardware or software requirements. Quick Share also won’t work with AirDrop devices working in the default “contacts only” mode, though Google “[welcomes] the opportunity to work with Apple to enable ‘Contacts Only’ mode in the future.” (Reading between the lines: Google and Apple are not currently working together to enable this, and <a href="https://www.theverge.com/news/825228/iphone-airdrop-android-quick-share-pixel-10">Google confirmed to The Verge</a> that Apple hadn’t been involved in this at all.)</p>
<p>Like AirDrop, Google notes that files shared via Quick Share are transferred directly between devices, without being sent to either company’s servers first.</p>
<p>Google shared a little more information in <a href="https://security.googleblog.com/2025/11/android-quick-share-support-for-airdrop-security.html">a separate post about Quick Share’s security</a>, crediting Android’s use of the memory-safe Rust programming language with making secure file sharing between platforms possible.</p>
<p>“Its compiler enforces strict ownership and borrowing rules at compile time, which guarantees memory safety,” writes Google VP of Platforms Security and Privacy Dave Kleidermacher. “Rust removes entire classes of memory-related bugs. This means our implementation is inherently resilient against attackers attempting to use maliciously crafted data packets to exploit memory errors.”</p>

          
                      
                  </div>
                    
        
          
    
    <div>
          
          
<h2>Why is this happening now?</h2>
<p>Google doesn’t mention it in either Quick Share post, but if you’re wondering why it’s suddenly possible for Quick Share to work with AirDrop, it can almost certainly be credited to European Union regulations imposed under the Digital Markets Act (DMA).</p>
<p>Let’s start with how AirDrop works. Like many of Apple’s “<a href="https://support.apple.com/en-us/108046">Continuity</a>” features that rely on wireless communication between devices, AirDrop uses Bluetooth to allow devices to find each other, and a fast peer-to-peer Wi-Fi connection to actually transfer files and other data. This isn’t exotic hardware; all smartphones, tablets, and computers sold today include some flavor of Bluetooth and Wi-Fi.</p>
<p>But to make those Continuity features work, Apple also developed a proprietary protocol called Apple Wireless Direct Link (AWDL) to facilitate the actual connection between devices and the data transfer. Because this wasn’t a standard anyone could use, other companies couldn’t try to make their own wireless sharing features compatible with AirDrop.</p>
<p>But <a href="https://ec.europa.eu/commission/presscorner/detail/en/ip_25_816">earlier this year</a>, the EU <a href="https://digital-markets-act.ec.europa.eu/questions-and-answers/interoperability_en">adopted new specification decisions</a> that required Apple to adopt new interoperable wireless standards, starting in this year’s iOS 26 release. If you don’t want to wade through the regulatory documents, <a href="https://www.ditto.com/blog/cross-platform-p2p-wi-fi-how-the-eu-killed-awdl">this post</a> from cloud services company Ditto is a useful timeline of events written in plainer language.</p>

<figure>
    <p><img width="1206" height="902" src="https://cdn.arstechnica.net/wp-content/uploads/2025/11/IMG_6433.jpg" alt="" decoding="async" loading="lazy" srcset="https://cdn.arstechnica.net/wp-content/uploads/2025/11/IMG_6433.jpg 1206w, https://cdn.arstechnica.net/wp-content/uploads/2025/11/IMG_6433-640x479.jpg 640w, https://cdn.arstechnica.net/wp-content/uploads/2025/11/IMG_6433-1024x766.jpg 1024w, https://cdn.arstechnica.net/wp-content/uploads/2025/11/IMG_6433-768x574.jpg 768w, https://cdn.arstechnica.net/wp-content/uploads/2025/11/IMG_6433-980x733.jpg 980w" sizes="auto, (max-width: 1206px) 100vw, 1206px">
                  </p>
          <figcaption>
        <div>
    
    <p>
      Setting AirDrop to “everyone for 10 minutes” mode on an iPhone.

              <span>
          Credit:

          
          Andrew Cunningham

                  </span>
          </p>
  </div>
      </figcaption>
      </figure>

<p>The rulings required Apple to add support for the Wi-Fi Alliance’s <a href="https://www.wi-fi.org/alternative-topologies">Wi-Fi Aware standard</a> instead of AWDL—and in fact required Apple to deprecate AWDL and to help add its features to Wi-Fi Aware so that any device could benefit from them. This wasn’t quite the imposition it sounded like; Wi-Fi Aware <a href="https://owlink.org/wiki/">was developed with Apple’s help</a>, based on the work Apple had already done on AWDL. But it meant that Apple could no longer keep other companies out of AirDrop by using a functionally similar but private communication protocol instead of the standardized version.</p>

          
                  </div>
                    
        
          
    
    <div>

        
        <div>
          
          
<p>In some ways, Apple’s journey to Wi-Fi Aware recalls the iPhone’s journey to USB-C: first, Apple developed a proprietary port that achieved some of the same goals as USB-C; Apple then contributed work to what would become the standardized USB-C connector; but then the company hesitated to actually adopt the standardized port in its phones until its hand was <a href="https://arstechnica.com/gadgets/2022/12/the-clock-is-rapidly-ticking-on-apples-lightning-charger/">forced by regulators</a>.</p>
<p>In any case, Wi-Fi Aware was added to iOS 26 and iPadOS 26, and <a href="https://developer.apple.com/documentation/WiFiAware">Apple’s developer documentation</a> lists the specific hardware that supports it (the iPhone 12 and later, and most iPads released within the last three or four years). For Android users, that likely means that Quick Share will only work with AirDrop on those devices, if they’ve been updated to iOS/iPadOS 26 or later. Google <a href="https://developer.android.com/develop/connectivity/wifi/wifi-aware">has supported Wi-Fi Aware</a> in Android since version 8.0, so it should at least theoretically be possible for most modern Android phones to add support for the feature in software updates somewhere down the line.</p>
<p>Apple’s hardware support list also suggests that Android phones <em>won’t</em> work with AirDrop on the Mac, since macOS 26 isn’t listed as a supported operating system on Apple’s Wi-Fi Aware (it’s likely not a coincidence that macOS is not considered to be a “gatekeeper” operating system under the DMA, as both iOS and iPadOS are).</p>
<p>If I had to guess why neither of Google’s Quick Share posts mentions Wi-Fi interoperability standards or the DMA, it may be because Google has been <a href="https://arstechnica.com/tech-policy/2024/03/on-dma-eve-google-whines-apple-sounds-alarms-and-tiktok-wants-out/">complaining</a> about <a href="https://arstechnica.com/google/2025/10/uk-antitrust-regulator-takes-aim-at-googles-search-dominance/">various aspects of the law</a> and its enforcement since <a href="https://arstechnica.com/tech-policy/2022/03/eu-to-regulate-gatekeepers-in-crackdown-on-google-apple-amazon-facebook/">before it was even passed</a> (as have many US tech companies designated as gatekeepers by the law). Google has occasionally tried to take advantage of the DMA, as it did <a href="https://arstechnica.com/gadgets/2023/11/google-argues-imessage-should-be-regulated-by-the-eus-digital-markets-act/">when it argued</a> that Apple’s iMessage service should be opened up. But it may be that Google doesn’t want to explicitly credit or praise the DMA in its press releases when the company is <a href="https://arstechnica.com/apple/2025/03/eu-accuses-google-and-apple-of-stifling-competition-under-digital-markets-act/">facing the possibility of huge fines</a> under the same law.</p>
<p>The New York Times <a href="https://www.nytimes.com/2025/11/17/technology/europe-big-tech.html">reported earlier this week</a> that EU regulators are considering changes to some of its tech regulations, citing concerns about “overregulation” and “competitiveness,” but that the EU was not currently considering changes to the DMA. For its part, Apple recently <a href="https://arstechnica.com/tech-policy/2025/09/apple-demands-eu-repeal-the-digital-markets-act/">called for the DMA to be repealed entirely</a>.</p>


          
                  </div>

                  
          






  <div>
  <div>
          <p><a href="https://arstechnica.com/author/andrew_cunningham/"><img src="https://cdn.arstechnica.net/wp-content/uploads/2016/05/a.cunningham-45-1.jpg" alt="Photo of Andrew Cunningham"></a></p>
  </div>

  <div>
    

    <p>
      Andrew is a Senior Technology Reporter at Ars Technica, with a focus on consumer tech including computer hardware and in-depth reviews of operating systems like Windows and macOS. Andrew lives in Philadelphia and co-hosts a weekly book podcast called <a href="https://overduepodcast.com/">Overdue</a>.
    </p>
  </div>
</div>


  <p>
    <a href="https://arstechnica.com/gadgets/2025/11/the-eu-made-apple-adopt-new-wi-fi-standards-and-now-android-can-support-airdrop/#comments" title="118 comments">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 80 80"><defs><clipPath id="bubble-zero_svg__a"><path fill="none" stroke-width="0" d="M0 0h80v80H0z"></path></clipPath><clipPath id="bubble-zero_svg__b"><path fill="none" stroke-width="0" d="M0 0h80v80H0z"></path></clipPath></defs><g clip-path="url(#bubble-zero_svg__a)"><g fill="currentColor" clip-path="url(#bubble-zero_svg__b)"><path d="M80 40c0 22.09-17.91 40-40 40S0 62.09 0 40 17.91 0 40 0s40 17.91 40 40"></path><path d="M40 40 .59 76.58C-.67 77.84.22 80 2.01 80H40z"></path></g></g></svg>
    118 Comments
  </a>
      </p>
              </div>
  </article>


  


  


  <div>
    <header>
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 40 26"><defs><clipPath id="most-read_svg__a"><path fill="none" d="M0 0h40v26H0z"></path></clipPath><clipPath id="most-read_svg__b"><path fill="none" d="M0 0h40v26H0z"></path></clipPath></defs><g clip-path="url(#most-read_svg__a)"><g fill="none" clip-path="url(#most-read_svg__b)"><path fill="currentColor" d="M20 2h.8q1.5 0 3 .6c.6.2 1.1.4 1.7.6 1.3.5 2.6 1.3 3.9 2.1.6.4 1.2.8 1.8 1.3 2.9 2.3 5.1 4.9 6.3 6.4-1.1 1.5-3.4 4-6.3 6.4-.6.5-1.2.9-1.8 1.3q-1.95 1.35-3.9 2.1c-.6.2-1.1.4-1.7.6q-1.5.45-3 .6h-1.6q-1.5 0-3-.6c-.6-.2-1.1-.4-1.7-.6-1.3-.5-2.6-1.3-3.9-2.1-.6-.4-1.2-.8-1.8-1.3-2.9-2.3-5.1-4.9-6.3-6.4 1.1-1.5 3.4-4 6.3-6.4.6-.5 1.2-.9 1.8-1.3q1.95-1.35 3.9-2.1c.6-.2 1.1-.4 1.7-.6q1.5-.45 3-.6zm0-2h-1c-1.2 0-2.3.3-3.4.6-.6.2-1.3.4-1.9.7-1.5.6-2.9 1.4-4.3 2.3-.7.5-1.3.9-1.9 1.4C2.9 8.7 0 13 0 13s2.9 4.3 7.5 7.9c.6.5 1.3 1 1.9 1.4 1.3.9 2.7 1.7 4.3 2.3.6.3 1.3.5 1.9.7 1.1.3 2.3.6 3.4.6h2c1.2 0 2.3-.3 3.4-.6.6-.2 1.3-.4 1.9-.7 1.5-.6 2.9-1.4 4.3-2.3.7-.5 1.3-.9 1.9-1.4C37.1 17.3 40 13 40 13s-2.9-4.3-7.5-7.9c-.6-.5-1.3-1-1.9-1.4-1.3-.9-2.8-1.7-4.3-2.3-.6-.3-1.3-.5-1.9-.7C23.3.4 22.1.1 21 .1h-1"></path><path fill="#ff4e00" d="M20 5c-4.4 0-8 3.6-8 8s3.6 8 8 8 8-3.6 8-8-3.6-8-8-8m0 11c-1.7 0-3-1.3-3-3s1.3-3 3-3 3 1.3 3 3-1.3 3-3 3"></path></g></g></svg>
      
    </header>
    <ol>
              <li>
                      <a href="https://arstechnica.com/gadgets/2025/11/plexs-crackdown-on-free-remote-streaming-access-starts-this-week/">
              <img src="https://cdn.arstechnica.net/wp-content/uploads/2024/02/plex1-768x432.png" alt="Listing image for first story in Most Read: Plex’s crackdown on free remote streaming access starts this week" decoding="async" loading="lazy">
            </a>
                    
        </li>
                    <li>
                    
        </li>
                    <li>
                    
        </li>
                    <li>
                    
        </li>
                    <li>
                    
        </li>
                  </ol>
</div>
  </main>





  </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Crews claim Boring Company failed to pay workers and snubbed OSHA concerns (124 pts)]]></title>
            <link>https://nashvillebanner.com/2025/11/25/boring-company-nashville-shane-trucking-and-excavating/</link>
            <guid>46061840</guid>
            <pubDate>Wed, 26 Nov 2025 20:14:17 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://nashvillebanner.com/2025/11/25/boring-company-nashville-shane-trucking-and-excavating/">https://nashvillebanner.com/2025/11/25/boring-company-nashville-shane-trucking-and-excavating/</a>, See on <a href="https://news.ycombinator.com/item?id=46061840">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>

		
		

<p>Willie Shane broke the asphalt on Elon Musk’s Music City Loop project this summer. Seven of his crew had been the sole excavators, fabricators and dump trucking company on The Boring Company’s proposed tunnel through Nashville for months.</p>

<p>Then came Monday night, when they walked off the site.</p>

<p>“I moved the equipment myself,” Shane said in an interview with the <em>Banner</em> on Tuesday.&nbsp;</p>

<p>“We were really skeptical from the beginning, and then since then, things pretty much just went downhill,” he added.&nbsp;</p>


<p>Musk’s company has <a href="https://nashvillebanner.com/2025/08/08/elon-musk-music-city-loop-proposal/">a spotty record of completing similar tunnels in other cities</a>, often snagging on government regulations and contractual issues. When Shane’s company, Shane Trucking and Excavating, which works with major local clients like the Grand Ole Opry and the Nashville International Airport, was approached by The Boring Company, he said he had some reservations.&nbsp;</p>

<p>“I told them very bluntly — and I don’t want this to come across like egotistical — but I told them, ‘Hey, my dad worked really hard to build a reputation in Nashville, and my brother and I work very hard to keep that reputation,’” Shane said. “If you guys are actually serious about doing this, you need to be 100 percent serious, because this is going to be our reputation as part of this too.”&nbsp;</p>

<p>After being reassured, Shane’s team took the job in July.&nbsp;</p>


<p>He and his crew left the state-owned property on Rosa L Parks Boulevard, where they had been working on the proposed 9-mile tunnel from the state capitol to the airport after months of safety and financial issues with Musk’s company.&nbsp;</p>

<p>It started about a month in with a change in pay.</p>

<p>“We were supposed to be paid every 15 days. And then they switched accounting firms, and then it went from 15 days to 60,” Shane said. Now it’s been 123 days since they started digging, and Shane says The Boring Company has only paid out about five percent of what he’s owed.&nbsp;</p>

<p>According to Shane, he has still been able to pay his employees on time, but the local trucking company is left holding the bag for money unpaid by The Boring Company. Other subcontractors, he says, have also severed ties due to nonpayment on the project.&nbsp;</p>

<p>The final straw that caused Shane to pull his crew from the site was when multiple employees reported that a representative of The Boring Company was soliciting them to bail on Shane and work directly for TBC on Monday.&nbsp;</p>


<p>“One of their head guys texts two of my welders, offering them a job for $45 an hour from his work phone,” Shane described, noting that the same TBC employee denied sending the texts when confronted with screenshots. “That’s actually a breach of contract.”&nbsp;</p>

<p>Shane also says he and other vendors have filed multiple OSHA safety complaints since working on the site but have gotten no response. His biggest concerns have been Boring employees on the jobsite not wearing proper personal protective equipment, such as hard hats, and unsafe shoring, which he says he’s repeatedly complained about to the Boring Company.&nbsp;</p>

<p>“Where we’re digging, we’re so far down, there should be concrete and different structures like that to hold the slope back from falling on you while you’re working,” Shane explained. “Where most people use concrete, they currently have — I’m not even kidding — they currently have wood. They had us install wood 2x12s.”&nbsp;</p>

<p>The safety concerns are why Shane says he decided to make the issue public.&nbsp;</p>

<p>“We’re not coming forward in like a vindictive way,” Shane said. “I just don’t want someone to get hurt, sure, and then, in the future, I have to be like, ‘Dang, I worked on there, and I turned a blind eye to it.’”</p>


<p>In the meantime, Shane said that the amount of backpay owed to his company is in the six figures and that he has retained a lawyer.&nbsp;</p>

<h3>Boring Company response</h3>

<p>After the <em>Banner</em> contacted The Boring Company about Shane’s claims, Vice President David Buss said he connected with Shane and would make good on the outstanding invoices by the end of the day Wednesday and would do a “full audit” on the error.&nbsp;</p>

<p>“It does look like we had some invoicing errors on that,” Buss told the <em>Banner</em>. “It was, you know, unfortunately, too common of a thing, but I assured them that we are going to make sure that invoices are wired tomorrow.”</p>

<p>Buss later clarified that he does not believe The Boring Company has a “common” practice of missing payments to vendors, but rather missed payments happen sometimes during “the normal course of business.”</p>

<p>“You hate to have an unhappy vendor. We certainly aim to have great relationships,” Buss said. “And so my goal will be to figure out what happened in this incident and then make sure that that’s not extrapolated to any other incidents.”</p>


<p>Buss also said he was looking into Shane’s claims about The Boring Company trying to hire contractors.&nbsp;</p>

<p>“It is definitely not our practice to try to poach anybody, so I understand the frustrations on their side,” Buss said. “Hopefully it’s something where we’re able to smooth that over and correct some of the things that happened on site and that led to this.”</p>

<p>Asked about the safety complaints, Buss said Shane did not raise any concerns on their call Tuesday and said he was unaware of any OSHA complaints, but would look into it.&nbsp;</p>

<p>“Safety is existential to our company,” Buss said. “We thankfully have a long history of seven years of tunneling in Las Vegas, and we’ve had one construction-related injury that was not the company’s fault in a violation.”</p>

<h3>Hiring headaches</h3>

<p>According to Buss, the projected timeline had not changed, and work had not been slowed by the crews’ departure from the site. Shane, however, painted a different picture.&nbsp;</p>


<p>“Actually, we were the crew that was building the tunnel boring machine. So there’s nobody building the tunnel boring machine right now, and the Boring Company has been trying to hire welders, but they haven’t been able to secure any help,” Shane said Tuesday, noting that many prospective employees won’t work on the project because of Musk’s reputation.&nbsp;</p>

<p>“A lot of people don’t like Elon and their payment terms; the way that they pay their employees, is not traditional,” Shane said.&nbsp;</p>

<p>Buss denied any hiring trouble.&nbsp;</p>

<p>“We’ve had zero issues finding great talent thus far in Nashville,” Buss said. “I think we’ve hired about 14 people now, and we’re going to start to grow the team as we begin mining operations.”&nbsp;</p>

<p>Instability and safety have been pervasive concerns around the project since its <a href="https://nashvillebanner.com/2025/07/29/elon-musk-underground-music-city-loop/">hurried public rollout this summer</a>, in which<a href="https://nashvillebanner.com/2025/08/13/nashville-music-city-loop-concerns/"> little-to-no public input was received by the state</a> before <a href="https://nashvillebanner.com/2025/08/01/elon-musk-music-city-loop/">approving a lease of the state-owned property</a> where digging is taking place.</p>

<p>As reports of a second Boring tunnel under Broadway and West End surfaced, Boring Company CEO Steve Davis hosted a two-hour live update session on X, the social media website also owned by Musk Monday evening, in which he touted progress on the Music City Loop and described the project as smoothly underway, with boring set to begin around January after the proper permits are secured.&nbsp;</p>

<p>An hour later, Shane’s team left the site.&nbsp;</p>

<p>During Davis’ virtual meeting, members of the public could submit questions, some of which were answered by Boring Company leadership. Many of those questions came from State Sen. Heidi Campbell (D-Nashville), who represents the area and has been a vocal critic of the project since it was announced.&nbsp;</p>

<p>“I would say the promotional session that they had last night on on Twitter was disingenuous at best, if not dishonest, because it was, it sounded like a utopian project and then, lo and behold, the very next day, we find out that there are people leaving the site because they’re not getting paid and they’re not being treated well,” Campbell told the <em>Banner</em>.</p>

<p>In addition to her concerns about irreparable damage to the site and whether the project would even be completed, Campbell said she was concerned about the state’s liability if there were unsafe working conditions on the leased property and whether there was any way for lawmakers to stop the process.</p>

<p>“There is nothing to hold The Boring Company accountable for any of these things,” Campbell said of the lease. “They’ve already dug a big hole. But then on top of it, if they move forward, forward in any capacity, they have not proven that they are reliable to take care of the damage that they cause.”</p>

<p>When Shane first spoke to the <em>Banner</em>, he said he did not intend to return to the job even if they received payment, noting that his employees had expressed discomfort “because they didn’t feel the management there was very good.”</p>

<p>Hours later, after hearing from Buss, Shane said he would consider returning “if they correct the situation on their end.”</p>

<p><em>Demetria Kalodimos contributed to this report</em>.</p>




	</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[S&box is now an open source game engine (373 pts)]]></title>
            <link>https://sbox.game/news/update-25-11-26</link>
            <guid>46061682</guid>
            <pubDate>Wed, 26 Nov 2025 19:58:27 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://sbox.game/news/update-25-11-26">https://sbox.game/news/update-25-11-26</a>, See on <a href="https://news.ycombinator.com/item?id=46061682">Hacker News</a></p>
&lt;Unparsable&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[Don't Download Apps (388 pts)]]></title>
            <link>https://blog.calebjay.com/posts/dont-download-apps/</link>
            <guid>46061623</guid>
            <pubDate>Wed, 26 Nov 2025 19:51:52 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://blog.calebjay.com/posts/dont-download-apps/">https://blog.calebjay.com/posts/dont-download-apps/</a>, See on <a href="https://news.ycombinator.com/item?id=46061623">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
    
<header>
  
  <h3>
    <small>
      Updated: <time datetime="2025-11-25 02:16:47 -0800 -0800">25 Nov 2025</time>
    </small>
  </h3>
  <nav>
    <ul>
      <li>
        <a href="https://blog.calebjay.com/">Home</a>
      </li>
      <li>
        <a href="http://www.calebjay.com/">Calebjay.com</a>
      </li>
    </ul>
  </nav>
</header>


    
<main>
    <article>
      <section>
        <p>Companies want you to download apps. Here in Taiwan it’s particularly bad: I’ve had shop staff tell
me about some discount if you download their app, and when I decline, say something like “It’s really easy! Here, just give me your phone and I’ll do it for you.”
Once when I was setting up my phone plan, the staff wanted my phone to, idk, note my IMEI or something, and then when I wasn’t paying attention,
installed a local e-commerce app, using my new phone number and name as login details,
then proudly told me, “Now you get 300NTD off your first phone bill!” Thanks, for 10$ I can get weekly text and email
spam from Shopee, great.</p>
<p>So first tip, in Taiwan, never hand your phone over the counter.</p>
<p>Second tip, never download the app. Corps have all sorts of ways to try to convince you: Use the app to order in-store rather than the kiosk,
get free chicken nuggets. Download our app at checkout, get a discount. Whatever the reason, don’t do it, you’re giving more than you’re getting.</p>
<p>Two reasons.</p>
<p>First, we’ve entered an era defined by <a href="https://en.wikipedia.org/wiki/Surveillance_capitalism">surveillance capitalism</a>. Companies try to get
as much data on you as possible, and then treat you differently based on the data they have on file for you. We all know this as seeing poorly-tuned
ads (you just bought a refridgerator? You must love refridgerators! Here’s 100 refridgerator ads), but the new trend is <a href="https://pluralistic.net/2025/06/24/price-discrimination/">surveillance pricing</a>.
A company will know that you just got paid and so charge you just a bit more for your chicken nuggets than they do when you haven’t been paid in two weeks.
Annoying, don’t download them app, don’t give them more data than they already have.</p>
<p>The other scary thing though is that that gives the power of currency valuation to companies. WIthout surveillance pricing, everyone pays the same for a cheeseburger.
Rich people can buy more cheeseburgers, sure, but at least the price of cheeseburgers is pegged against a dollar, so if someone starts charging too much for cheeseburgers,
you can take your dollars to a competitor. Once companies can start charging individual prices, the global economy doesn’t determine how many cheeseburgers your dollars can buy,
McDonald’s does. Way too much power to give to these companies that already have too much power.</p>
<p>Second reason, binding arbitration. Binding arbitration is when you sign an agreement with someone that has a clause that says,
“if there’s a dispupte, we don’t sue eachother, instead we go through a private process outside the court system and let a mediator decide the outcome.”
Bonus, unlike judges, whose salaries are paid for by the taxpayers and therefore you don’t pay a “judge fee” when you go to court (mostly), a mediator
needs to be hired. Guess who hires them? Not you!</p>
<p>Walking into a restaurant to buy a cheeseburger, there’s no way a company can force you to enter a contractual agreement that includes binding arbitration.
Downloading an app, however, requires agreeing to a “Terms of Service,” and those can <em>absolutely</em> include a binding arbitration clause, and that clause
can be applied even to cases outside the app. This happened to Jeffrey Piccolo when his wife died of food poisoning in a Disney World. Disney made a motion
to dismiss because a couple years back, Jeffrey had signed up for a free trial of Disney+, which included a binding arbitration clause, which meant that
if Jeffrey wanted to complain about how Disney murdered his wife, they’d have to settle it out of court with a mediator that Disney hired. No jury, no judge,
no oversight. In the end the only reason Disney dropped this motion is because the news picked it up. That won’t always happen.</p>
<p>At least in the USA, binding arbitration is totally cool according to the Supreme Court, so don’t count on the government to save you. You need to take
personal steps to make sure you aren’t signing your rights away. So, don’t download apps.</p>
<p>Predictions: Sometime in the next 5 years, someone will be forced into arbitration with Uber after being hit by one of their self driving cars, because
they use Uber Eats. Sometime in the next 5 years, someone’s house will burn down from their Tesla exploding, and they’ll be forced into arbitration
because they had a Twitter account, and Twitter is now a subsidiary of TeXla. Sometime in the next 5 years, an Amazon employee who lost a finger on the job
will be forced into arbitration because they have a WaPo subscription.</p>
<p>If you want to learn more, <a href="https://pluralistic.net/2025/10/27/shit-shack/">Cory Doctorow</a> covers the topic in much more detail.</p>

      </section>
    </article>
</main>

    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Alan.app – Add a Border to macOS Active Window (144 pts)]]></title>
            <link>https://tyler.io/2025/11/alan/</link>
            <guid>46061239</guid>
            <pubDate>Wed, 26 Nov 2025 19:12:40 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://tyler.io/2025/11/alan/">https://tyler.io/2025/11/alan/</a>, See on <a href="https://news.ycombinator.com/item?id=46061239">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="page">
		<main id="main">
			
<article id="post-2770" itemtype="https://schema.org/CreativeWork" itemscope="">
	<div itemprop="text">
			
<p>Maybe it’s because my eyes are getting old or maybe it’s because <a href="https://tyler.io/2020/09/surtainly-not/#active-window">the contrast between windows</a> on macOS keeps getting worse. Either way, I built a tiny Mac app last night that draws a border around the active window. I named it “Alan”.</p>



<p>In Alan’s preferences, you can choose a preferred border width and colors for both light and dark mode.</p>



<p>That’s it. That’s the app.</p>



<p>You can <a href="https://github.com/tylerhall/Alan/releases/tag/v1.0">download a notarized copy of Alan here</a>.</p>



<p>Here’s a short demo video.</p>







<p>If you want to hide Alan’s icon from the Dock, you can set a hidden preference by running this Terminal command. Then, relaunch the app.</p>



<pre><code>defaults write studio.retina.Alan hideDock -bool true</code></pre>




		</div>
</article>
		</main>
	</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Fara-7B: An efficient agentic model for computer use (144 pts)]]></title>
            <link>https://github.com/microsoft/fara</link>
            <guid>46061208</guid>
            <pubDate>Wed, 26 Nov 2025 19:10:24 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/microsoft/fara">https://github.com/microsoft/fara</a>, See on <a href="https://news.ycombinator.com/item?id=46061208">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><div dir="auto">
<p dir="auto"><h2 tabindex="-1" dir="auto">Fara-7B: An Efficient Agentic Model for Computer Use</h2><a id="user-content-fara-7b-an-efficient-agentic-model-for-computer-use" aria-label="Permalink: Fara-7B: An Efficient Agentic Model for Computer Use" href="#fara-7b-an-efficient-agentic-model-for-computer-use"></a></p>
<p><a target="_blank" rel="noopener noreferrer" href="https://github.com/microsoft/fara/blob/main/figures/model_accuracy_vs_cost_v2_glm_cost_updated.png"><img src="https://github.com/microsoft/fara/raw/main/figures/model_accuracy_vs_cost_v2_glm_cost_updated.png" alt="Fara-7B Performance" width="600"></a></p><p dir="auto"><a href="https://aka.ms/msaif/fara" rel="nofollow"><img src="https://camo.githubusercontent.com/574f57b77832fd1f5d69a888107eb3277e18e3303126f19249ad2b30d580486f/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f4d6963726f736f66742d50726f6a6563742d3030373844343f6c6f676f3d6d6963726f736f6674" alt="Microsoft" data-canonical-src="https://img.shields.io/badge/Microsoft-Project-0078D4?logo=microsoft"></a>
<a href="https://huggingface.co/microsoft/Fara-7b" rel="nofollow"><img src="https://camo.githubusercontent.com/ed9df35495f85af0083bca138e68732525f68d58b4ddbeb5630183bc6d3b69d7/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f2546302539462541342539372d4d6f64656c2d79656c6c6f77" alt="Hugging Face Model" data-canonical-src="https://img.shields.io/badge/%F0%9F%A4%97-Model-yellow"></a>
<a href="https://aka.ms/foundry-fara-7b" rel="nofollow"><img src="https://camo.githubusercontent.com/ff19c70146434f8197bfa03a0efa2212bfdbb073213cf78fa3b8030a6b1a8764/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f417a7572652d466f756e6472792d303038394436" alt="Foundry" data-canonical-src="https://img.shields.io/badge/Azure-Foundry-0089D6"></a>
<a href="https://huggingface.co/datasets/microsoft/WebTailBench" rel="nofollow"><img src="https://camo.githubusercontent.com/26ab73ace4d059a01c9888876b5ec3b98e969ba97b49bccbd420e15a3fa66d59/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f2546302539462541342539372d5765625461696c42656e6368253230446174617365742d6f72616e6765" alt="Dataset" data-canonical-src="https://img.shields.io/badge/%F0%9F%A4%97-WebTailBench%20Dataset-orange"></a></p>
</div>
<hr>
<p dir="auto"><h2 tabindex="-1" dir="auto">Overview</h2><a id="user-content-overview" aria-label="Permalink: Overview" href="#overview"></a></p>
<p dir="auto"><strong>Fara-7B</strong> is Microsoft's first <strong>agentic small language model (SLM)</strong> designed specifically for computer use. With only 7 billion parameters, Fara-7B is an ultra-compact Computer Use Agent (CUA) that achieves state-of-the-art performance within its size class and is competitive with larger, more resource-intensive agentic systems.</p>
<p dir="auto">Try Fara-7B locally as follows (see <a href="##Installation">Installation</a> for detailed instructions):</p>
<div dir="auto" data-snippet-clipboard-copy-content="# 1. Clone repository
git clone https://github.com/microsoft/fara.git
cd fara

# 2. Setup environment
python3 -m venv .venv 
source .venv/bin/activate
pip install -e .
playwright install"><pre><span><span>#</span> 1. Clone repository</span>
git clone https://github.com/microsoft/fara.git
<span>cd</span> fara

<span><span>#</span> 2. Setup environment</span>
python3 -m venv .venv 
<span>source</span> .venv/bin/activate
pip install -e <span>.</span>
playwright install</pre></div>
<p dir="auto">Then in one process, host the model:</p>
<div dir="auto" data-snippet-clipboard-copy-content="vllm serve &quot;microsoft/Fara-7B&quot; --port 5000 --dtype auto "><pre>vllm serve <span><span>"</span>microsoft/Fara-7B<span>"</span></span> --port 5000 --dtype auto </pre></div>
<p dir="auto">Then you can iterative query it with:</p>
<div dir="auto" data-snippet-clipboard-copy-content="fara-cli --task &quot;whats the weather in new york now&quot;"><pre>fara-cli --task <span><span>"</span>whats the weather in new york now<span>"</span></span></pre></div>
<p dir="auto">Hint: might need to do <code>--tensor-parallel-size 2</code> with vllm command if you run out of memory</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">What Makes Fara-7B Unique</h3><a id="user-content-what-makes-fara-7b-unique" aria-label="Permalink: What Makes Fara-7B Unique" href="#what-makes-fara-7b-unique"></a></p>
<p dir="auto">Unlike traditional chat models that generate text-based responses, Fara-7B leverages computer interfaces—mouse and keyboard—to perform multi-step tasks on behalf of users. The model:</p>
<ul dir="auto">
<li><strong>Operates visually</strong> by perceiving webpages and taking actions like scrolling, typing, and clicking on directly predicted coordinates</li>
<li><strong>Uses the same modalities as humans</strong> to interact with computers—no accessibility trees or separate parsing models required</li>
<li><strong>Enables on-device deployment</strong> due to its compact 7B parameter size, resulting in reduced latency and improved privacy as user data remains local</li>
<li><strong>Completes tasks efficiently</strong>, averaging only ~16 steps per task compared to ~41 for comparable models</li>
</ul>
<p dir="auto">Fara-7B is trained using a novel synthetic data generation pipeline built on the <a href="https://www.microsoft.com/en-us/research/articles/magentic-one-a-generalist-multi-agent-system-for-solving-complex-tasks/" rel="nofollow">Magentic-One</a> multi-agent framework, with 145K trajectories covering diverse websites, task types, and difficulty levels. The model is based on <a href="https://arxiv.org/abs/2502.13923" rel="nofollow">Qwen2.5-VL-7B</a> and trained with supervised fine-tuning.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Key Capabilities</h3><a id="user-content-key-capabilities" aria-label="Permalink: Key Capabilities" href="#key-capabilities"></a></p>
<p dir="auto">Fara-7B can automate everyday web tasks including:</p>
<ul dir="auto">
<li>Searching for information and summarizing results</li>
<li>Filling out forms and managing accounts</li>
<li>Booking travel, movie tickets, and restaurant reservations</li>
<li>Shopping and comparing prices across retailers</li>
<li>Finding job postings and real estate listings</li>
</ul>
<p dir="auto"><h3 tabindex="-1" dir="auto">Performance Highlights</h3><a id="user-content-performance-highlights" aria-label="Permalink: Performance Highlights" href="#performance-highlights"></a></p>
<p dir="auto">Fara-7B achieves state-of-the-art results across multiple web agent benchmarks, outperforming both comparable-sized models and larger systems:</p>
<markdown-accessiblity-table><table>
<thead>
<tr>
<th>Model</th>
<th>Params</th>
<th>WebVoyager</th>
<th>Online-M2W</th>
<th>DeepShop</th>
<th>WebTailBench</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>SoM Agents</strong></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>SoM Agent (GPT-4o-0513)</td>
<td>-</td>
<td>90.6</td>
<td>57.7</td>
<td>49.1</td>
<td>60.4</td>
</tr>
<tr>
<td>SoM Agent (o3-mini)</td>
<td>-</td>
<td>79.3</td>
<td>55.4</td>
<td>49.7</td>
<td>52.7</td>
</tr>
<tr>
<td>SoM Agent (GPT-4o)</td>
<td>-</td>
<td>65.1</td>
<td>34.6</td>
<td>16.0</td>
<td>30.8</td>
</tr>
<tr>
<td>GLM-4.1V-9B-Thinking</td>
<td>9B</td>
<td>66.8</td>
<td>33.9</td>
<td>32.0</td>
<td>22.4</td>
</tr>
<tr>
<td><strong>Computer Use Models</strong></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>OpenAI computer-use-preview</td>
<td>-</td>
<td>70.9</td>
<td>42.9</td>
<td>24.7</td>
<td>25.7</td>
</tr>
<tr>
<td>UI-TARS-1.5-7B</td>
<td>7B</td>
<td>66.4</td>
<td>31.3</td>
<td>11.6</td>
<td>19.5</td>
</tr>
<tr>
<td><strong>Fara-7B</strong></td>
<td><strong>7B</strong></td>
<td><strong>73.5</strong></td>
<td><strong>34.1</strong></td>
<td><strong>26.2</strong></td>
<td><strong>38.4</strong></td>
</tr>
</tbody>
</table></markdown-accessiblity-table>
<p dir="auto"><em>Table: Online agent evaluation results showing success rates (%) across four web benchmarks. Results are averaged over 3 runs.</em></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">WebTailBench: A New Benchmark for Real-World Web Tasks</h3><a id="user-content-webtailbench-a-new-benchmark-for-real-world-web-tasks" aria-label="Permalink: WebTailBench: A New Benchmark for Real-World Web Tasks" href="#webtailbench-a-new-benchmark-for-real-world-web-tasks"></a></p>
<p dir="auto">We are releasing <strong><a href="https://huggingface.co/datasets/microsoft/WebTailBench" rel="nofollow">WebTailBench</a></strong>, a new evaluation benchmark focusing on 11 real-world task types that are underrepresented or missing in existing benchmarks. The benchmark includes 609 tasks across diverse categories, with the first 8 segments testing single skills or objectives (usually on a single website), and the remaining 3 evaluating more difficult multi-step or cross-site tasks.</p>
<p dir="auto"><h4 tabindex="-1" dir="auto">WebTailBench Detailed Results</h4><a id="user-content-webtailbench-detailed-results" aria-label="Permalink: WebTailBench Detailed Results" href="#webtailbench-detailed-results"></a></p>
<markdown-accessiblity-table><table>
<thead>
<tr>
<th>Task Segment</th>
<th>Tasks</th>
<th>SoM GPT-4o-0513</th>
<th>SoM o3-mini</th>
<th>SoM GPT-4o</th>
<th>GLM-4.1V-9B</th>
<th>OAI Comp-Use</th>
<th>UI-TARS-1.5</th>
<th><strong>Fara-7B</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Single-Site Tasks</strong></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>Shopping</td>
<td>56</td>
<td>62.5</td>
<td>71.4</td>
<td>38.1</td>
<td>31.0</td>
<td>42.3</td>
<td>41.1</td>
<td><strong>52.4</strong></td>
</tr>
<tr>
<td>Flights</td>
<td>51</td>
<td>60.1</td>
<td>39.2</td>
<td>11.1</td>
<td>10.5</td>
<td>17.6</td>
<td>10.5</td>
<td><strong>37.9</strong></td>
</tr>
<tr>
<td>Hotels</td>
<td>52</td>
<td>68.6</td>
<td>56.4</td>
<td>31.4</td>
<td>19.9</td>
<td>26.9</td>
<td>35.3</td>
<td><strong>53.8</strong></td>
</tr>
<tr>
<td>Restaurants</td>
<td>52</td>
<td>67.9</td>
<td>59.6</td>
<td>47.4</td>
<td>32.1</td>
<td>35.9</td>
<td>22.4</td>
<td><strong>47.4</strong></td>
</tr>
<tr>
<td>Activities</td>
<td>80</td>
<td>70.4</td>
<td>62.9</td>
<td>41.7</td>
<td>26.3</td>
<td>30.4</td>
<td>9.6</td>
<td><strong>36.3</strong></td>
</tr>
<tr>
<td>Ticketing</td>
<td>57</td>
<td>58.5</td>
<td>56.7</td>
<td>37.4</td>
<td>35.7</td>
<td>49.7</td>
<td>30.4</td>
<td><strong>38.6</strong></td>
</tr>
<tr>
<td>Real Estate</td>
<td>48</td>
<td>34.0</td>
<td>17.4</td>
<td>20.1</td>
<td>16.0</td>
<td>9.0</td>
<td>9.7</td>
<td><strong>23.6</strong></td>
</tr>
<tr>
<td>Jobs/Careers</td>
<td>50</td>
<td>49.3</td>
<td>44.0</td>
<td>32.7</td>
<td>22.7</td>
<td>20.7</td>
<td>20.7</td>
<td><strong>28.0</strong></td>
</tr>
<tr>
<td><strong>Multi-Step Tasks</strong></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>Shopping List (2 items)</td>
<td>51</td>
<td>66.0</td>
<td>62.7</td>
<td>17.0</td>
<td>7.8</td>
<td>34.0</td>
<td>20.9</td>
<td><strong>49.0</strong></td>
</tr>
<tr>
<td>Comparison Shopping</td>
<td>57</td>
<td>67.3</td>
<td>59.1</td>
<td>27.5</td>
<td>22.8</td>
<td>1.2</td>
<td>8.8</td>
<td><strong>32.7</strong></td>
</tr>
<tr>
<td>Compositional Tasks</td>
<td>55</td>
<td>51.5</td>
<td>39.4</td>
<td>26.7</td>
<td>17.0</td>
<td>10.3</td>
<td>9.1</td>
<td><strong>23.0</strong></td>
</tr>
<tr>
<td><strong>Overall</strong></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>Macro Average</td>
<td>609</td>
<td>59.7</td>
<td>51.7</td>
<td>30.1</td>
<td>22.0</td>
<td>25.3</td>
<td>19.9</td>
<td><strong>38.4</strong></td>
</tr>
<tr>
<td>Micro Average</td>
<td>609</td>
<td>60.4</td>
<td>52.7</td>
<td>30.8</td>
<td>22.4</td>
<td>25.7</td>
<td>19.5</td>
<td><strong>38.4</strong></td>
</tr>
</tbody>
</table></markdown-accessiblity-table>
<p dir="auto"><em>Table: Breakdown of WebTailBench results across all 11 segments. Success rates (%) are averaged over 3 independent runs. Fara-7B achieves the highest performance among computer-use models across all task categories.</em></p>
<p dir="auto"><strong>Coming Soon:</strong></p>
<ul dir="auto">
<li>Task Verification pipeline for LLM-as-a-judge evaluation</li>
<li>Official human annotations of WebTailBench (in partnership with BrowserBase)</li>
</ul>
<p dir="auto"><h3 tabindex="-1" dir="auto">Evaluation Infrastructure</h3><a id="user-content-evaluation-infrastructure" aria-label="Permalink: Evaluation Infrastructure" href="#evaluation-infrastructure"></a></p>
<p dir="auto">Our evaluation setup leverages:</p>
<ol dir="auto">
<li><strong>Playwright</strong> - A cross-browser automation framework that replicates browser environments</li>
<li><strong>Abstract Web Agent Interface</strong> - Allows integration of any model from any source into the evaluation environment</li>
<li><strong>Fara-Agent Class</strong> - Reference implementation for running the Fara model</li>
</ol>
<blockquote>
<p dir="auto"><strong>Note:</strong> Fara-7B is an experimental release designed to invite hands-on exploration and feedback from the community. We recommend running it in a sandboxed environment, monitoring its execution, and avoiding sensitive data or high-risk domains.</p>
</blockquote>
<hr>
<p dir="auto"><h2 tabindex="-1" dir="auto">Installation</h2><a id="user-content-installation" aria-label="Permalink: Installation" href="#installation"></a></p>
<p dir="auto">Install the package using either UV or pip:</p>

<p dir="auto">or</p>

<p dir="auto">Then install Playwright browsers:</p>

<hr>
<p dir="auto"><h2 tabindex="-1" dir="auto">Hosting the Model</h2><a id="user-content-hosting-the-model" aria-label="Permalink: Hosting the Model" href="#hosting-the-model"></a></p>
<p dir="auto"><strong>Recommended:</strong> The easiest way to get started is using Azure Foundry hosting, which requires no GPU hardware or model downloads. Alternatively, you can self-host with VLLM if you have GPU resources available.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Azure Foundry Hosting (Recommended)</h3><a id="user-content-azure-foundry-hosting-recommended" aria-label="Permalink: Azure Foundry Hosting (Recommended)" href="#azure-foundry-hosting-recommended"></a></p>
<p dir="auto">Deploy Fara-7B on <a href="https://ai.azure.com/explore/models/Fara-7B/version/2/registry/azureml-msr" rel="nofollow">Azure Foundry</a> without needing to download weights or manage GPU infrastructure.</p>
<p dir="auto"><strong>Setup:</strong></p>
<ol dir="auto">
<li>Deploy the Fara-7B model on Azure Foundry and obtain your endpoint URL and API key</li>
<li>Add your endpoint details to the existing <code>endpoint_configs/</code> directory (example configs are already provided):</li>
</ol>
<div dir="auto" data-snippet-clipboard-copy-content="# Edit one of the existing config files or create a new one
# endpoint_configs/fara-7b-hosting-ansrz.json (example format):
{
    &quot;model&quot;: &quot;Fara-7B&quot;,
    &quot;base_url&quot;: &quot;https://your-endpoint.inference.ml.azure.com/&quot;,
    &quot;api_key&quot;: &quot;YOUR_API_KEY_HERE&quot;
}"><pre><span><span>#</span> Edit one of the existing config files or create a new one</span>
<span><span>#</span> endpoint_configs/fara-7b-hosting-ansrz.json (example format):</span>
{
    <span><span>"</span>model<span>"</span></span>: <span><span>"</span>Fara-7B<span>"</span></span>,
    <span><span>"</span>base_url<span>"</span></span>: <span><span>"</span>https://your-endpoint.inference.ml.azure.com/<span>"</span></span>,
    <span><span>"</span>api_key<span>"</span></span>: <span><span>"</span>YOUR_API_KEY_HERE<span>"</span></span>
}</pre></div>
<ol start="3" dir="auto">
<li>Run the Fara agent:</li>
</ol>
<div dir="auto" data-snippet-clipboard-copy-content="fara-cli --task &quot;how many pages does wikipedia have&quot; --start_page &quot;https://www.bing.com&quot;"><pre>fara-cli --task <span><span>"</span>how many pages does wikipedia have<span>"</span></span> --start_page <span><span>"</span>https://www.bing.com<span>"</span></span></pre></div>
<p dir="auto">That's it! No GPU or model downloads required.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Self-hosting with VLLM</h3><a id="user-content-self-hosting-with-vllm" aria-label="Permalink: Self-hosting with VLLM" href="#self-hosting-with-vllm"></a></p>
<p dir="auto">If you have access to GPU resources, you can self-host Fara-7B using VLLM. This requires a GPU machine with sufficient VRAM.</p>
<p dir="auto">All that is required is to run the following command to start the VLLM server:</p>
<div dir="auto" data-snippet-clipboard-copy-content="vllm serve &quot;microsoft/Fara-7B&quot; --port 5000 --dtype auto "><pre>vllm serve <span><span>"</span>microsoft/Fara-7B<span>"</span></span> --port 5000 --dtype auto </pre></div>
<p dir="auto"><h3 tabindex="-1" dir="auto">Testing the Fara Agent</h3><a id="user-content-testing-the-fara-agent" aria-label="Permalink: Testing the Fara Agent" href="#testing-the-fara-agent"></a></p>
<p dir="auto">Run the test script to see Fara in action:</p>
<div dir="auto" data-snippet-clipboard-copy-content="fara-cli --task &quot;how many pages does wikipedia have&quot; --start_page &quot;https://www.bing.com&quot; --endpoint_config endpoint_configs/azure_foundry_config.json [--headful] [--downloads_folder &quot;/path/to/downloads&quot;] [--save_screenshots] [--max_rounds 100] [--browserbase]"><pre>fara-cli --task <span><span>"</span>how many pages does wikipedia have<span>"</span></span> --start_page <span><span>"</span>https://www.bing.com<span>"</span></span> --endpoint_config endpoint_configs/azure_foundry_config.json [--headful] [--downloads_folder <span><span>"</span>/path/to/downloads<span>"</span></span>] [--save_screenshots] [--max_rounds 100] [--browserbase]</pre></div>
<p dir="auto">In self-hosting scenario the <code>endpoint_config</code> points to <code>endpoint_configs/vllm_config.json</code> from the VLLM server above.</p>
<p dir="auto">If you set <code>--browserbase</code>, export environment variables for the API key and project ID.</p>
<p dir="auto"><h4 tabindex="-1" dir="auto">Expected Output</h4><a id="user-content-expected-output" aria-label="Permalink: Expected Output" href="#expected-output"></a></p>
<div data-snippet-clipboard-copy-content="Initializing Browser...
Browser Running... Starting Fara Agent...
##########################################
Task: how many pages does wikipedia have
##########################################
Running Fara...


Thought #1: To find the current number of Wikipedia pages, I'll search for the latest Wikipedia page count statistics.
Action #1: executing tool 'web_search' with arguments {&quot;action&quot;: &quot;web_search&quot;, &quot;query&quot;: &quot;Wikipedia total number of articles&quot;}
Observation#1: I typed 'Wikipedia total number of articles' into the browser search bar.

Thought #2: Wikipedia currently has 7,095,446 articles.
Action #2: executing tool 'terminate' with arguments {&quot;action&quot;: &quot;terminate&quot;, &quot;status&quot;: &quot;success&quot;}
Observation#2: Wikipedia currently has 7,095,446 articles.

Final Answer: Wikipedia currently has 7,095,446 articles.

Enter another task (or press Enter to exit): "><pre><code>Initializing Browser...
Browser Running... Starting Fara Agent...
##########################################
Task: how many pages does wikipedia have
##########################################
Running Fara...


Thought #1: To find the current number of Wikipedia pages, I'll search for the latest Wikipedia page count statistics.
Action #1: executing tool 'web_search' with arguments {"action": "web_search", "query": "Wikipedia total number of articles"}
Observation#1: I typed 'Wikipedia total number of articles' into the browser search bar.

Thought #2: Wikipedia currently has 7,095,446 articles.
Action #2: executing tool 'terminate' with arguments {"action": "terminate", "status": "success"}
Observation#2: Wikipedia currently has 7,095,446 articles.

Final Answer: Wikipedia currently has 7,095,446 articles.

Enter another task (or press Enter to exit): 
</code></pre></div>
<hr>
<p dir="auto"><h2 tabindex="-1" dir="auto">Reproducibility</h2><a id="user-content-reproducibility" aria-label="Permalink: Reproducibility" href="#reproducibility"></a></p>
<p dir="auto">We provide a framework in <code>webeval/</code> to reproduce our results on WebVoyager and OnlineMind2Web.
Agentic evaluations on live websites present unique challenges due to day-to-day changes. We implement several measures to ensure reliable and comparable evaluations:</p>
<p dir="auto"><strong>BrowserBase Integration</strong>
We employ BrowserBase to manage browser session hosting, enabling reliable browser instance management.</p>
<p dir="auto"><strong>Time-sensitive Task Updates</strong>
Tasks in benchmarks like WebVoyager can become stale or impossible. We:</p>
<ul dir="auto">
<li>Removed ~48 impossible tasks from the original WebVoyager benchmark</li>
<li>Updated ~50 tasks with future dates to keep them achievable</li>
<li>Example: <em>"Search for a hotel in Bali from Jan 1 to Jan 4, 2024"</em> → <em>"Search for a hotel in Bali from Jan 1 to Jan 4, 2026"</em></li>
<li>Our updated WebVoyager benchmark is available at <code>webeval/data/webvoyager/WebVoyager_data_08312025.jsonl</code></li>
</ul>
<p dir="auto"><strong>Environment Error Handling</strong>
Browser errors (connection drops, page timeouts) are handled robustly:</p>
<ul dir="auto">
<li>Trajectories are retried up to 5 times when environment errors occur</li>
<li>Complete yet incorrect trajectories are never retried</li>
<li>Each retry starts with a fresh browser session, with no retained state</li>
</ul>
<p dir="auto"><strong>Step Budget</strong>
Each trajectory is capped at a maximum of 100 actions across all online benchmarks. Trajectories exceeding this budget without choosing to stop are considered incorrect.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">WebEval Package Installation</h2><a id="user-content-webeval-package-installation" aria-label="Permalink: WebEval Package Installation" href="#webeval-package-installation"></a></p>
<div dir="auto" data-snippet-clipboard-copy-content="conda create --name fara_webeval python=3.12
conda activate fara_webeval

# Install fara package
pip install -e .

# Install autogen submodule
git submodule update --init --recursive
cd autogen/python/packages
pip install -e autogen-core
pip install -e autogen-ext

# Install webeval
cd webeval
pip install -e .

# Install playwright
playwright install"><pre>conda create --name fara_webeval python=3.12
conda activate fara_webeval

<span><span>#</span> Install fara package</span>
pip install -e <span>.</span>

<span><span>#</span> Install autogen submodule</span>
git submodule update --init --recursive
<span>cd</span> autogen/python/packages
pip install -e autogen-core
pip install -e autogen-ext

<span><span>#</span> Install webeval</span>
<span>cd</span> webeval
pip install -e <span>.</span>

<span><span>#</span> Install playwright</span>
playwright install</pre></div>
<p dir="auto"><h2 tabindex="-1" dir="auto">Running Evaluations</h2><a id="user-content-running-evaluations" aria-label="Permalink: Running Evaluations" href="#running-evaluations"></a></p>
<p dir="auto">Navigate to the scripts directory:</p>

<p dir="auto">Make sure you set a valid OpenAI GPT-4o endpoint in <code>endpoint_configs_gpt4o/dev</code> in order to run the WebVoyager LLM-as-a-judge!</p>
<p dir="auto"><strong>Option 1: Self-hosted VLLM</strong></p>
<div dir="auto" data-snippet-clipboard-copy-content="python webvoyager.py --model_url /path/where/you/want/to/download/model/ --model_port 5000 --eval_oai_config ../endpoint_configs_gpt4o/dev/ --out_url /path/to/save/eval/files --device_id 0,1 --processes 1 --run_id 1 --max_rounds 100"><pre>python webvoyager.py --model_url /path/where/you/want/to/download/model/ --model_port 5000 --eval_oai_config ../endpoint_configs_gpt4o/dev/ --out_url /path/to/save/eval/files --device_id 0,1 --processes 1 --run_id 1 --max_rounds 100</pre></div>
<p dir="auto"><strong>Option 2: Azure Foundry Deployment</strong></p>
<p dir="auto">Deploy <a href="https://ai.azure.com/explore/models/Fara-7B/version/2/registry/azureml-msr" rel="nofollow">Fara-7B on Foundry endpoint(s)</a>, then place endpoint URLs and keys in JSONs under <code>endpoint_configs/</code>:</p>
<div dir="auto" data-snippet-clipboard-copy-content="python webvoyager.py --model_endpoint ../../endpoint_configs/ --eval_oai_config ../endpoint_configs_gpt4o/dev/ --out_url /path/to/save/eval/files --processes 1 --run_id 1_endpoint --max_rounds 100"><pre>python webvoyager.py --model_endpoint ../../endpoint_configs/ --eval_oai_config ../endpoint_configs_gpt4o/dev/ --out_url /path/to/save/eval/files --processes 1 --run_id 1_endpoint --max_rounds 100</pre></div>
<p dir="auto"><h3 tabindex="-1" dir="auto">Notes</h3><a id="user-content-notes" aria-label="Permalink: Notes" href="#notes"></a></p>
<ul dir="auto">
<li>We use the same LLM-as-a-judge prompts and model (GPT-4o) as WebVoyager, hence the <code>--eval_oai_config</code> argument</li>
<li>Set <code>--browserbase</code> for browser session management (requires exported API key and project ID environment variables)</li>
<li>Avoid overloading a single VLLM deployment with more than ~10 concurrent processes due to known issues</li>
<li>See debugging output in <code>fara/webeval/scripts/stdout.txt</code></li>
</ul>
<hr>
<p dir="auto"><h2 tabindex="-1" dir="auto">Analyzing Evaluation Results</h2><a id="user-content-analyzing-evaluation-results" aria-label="Permalink: Analyzing Evaluation Results" href="#analyzing-evaluation-results"></a></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Evaluation Output Structure</h3><a id="user-content-evaluation-output-structure" aria-label="Permalink: Evaluation Output Structure" href="#evaluation-output-structure"></a></p>
<p dir="auto">Evaluation results are stored under <code>--out_url</code> in folders organized by:</p>
<ul dir="auto">
<li>Model name</li>
<li>Dataset</li>
<li>Username</li>
<li>Run ID</li>
</ul>
<p dir="auto">Example path:</p>
<div data-snippet-clipboard-copy-content="/runs/WebSurfer-fara-100-max_n_images-3/fara-7b/<username>/WebVoyager_WebVoyager_data_08312025.jsonl/<run_id>"><pre><code>/runs/WebSurfer-fara-100-max_n_images-3/fara-7b/&lt;username&gt;/WebVoyager_WebVoyager_data_08312025.jsonl/&lt;run_id&gt;
</code></pre></div>
<p dir="auto">Each evaluation folder contains:</p>
<ul dir="auto">
<li><code>gpt_eval/</code> - LLM-as-a-judge evaluation results</li>
<li><code>traj/</code> - Per-task trajectory subdirectories containing:
<ul dir="auto">
<li><code>final_answer.json</code> (e.g., <code>Amazon--1_final_answer.json</code>) - <code>&lt;no_answer&gt;</code> indicates abortion or step budget exceeded</li>
<li><code>scores/gpt_eval.json</code> - LLM judge scores</li>
<li><code>web_surfer.log</code> - Action history and errors</li>
<li><code>screenshot_X.png</code> - Screenshots captured before each action X</li>
</ul>
</li>
</ul>
<p dir="auto"><h3 tabindex="-1" dir="auto">Running Analysis</h3><a id="user-content-running-analysis" aria-label="Permalink: Running Analysis" href="#running-analysis"></a></p>
<p dir="auto">Use the analysis notebook to compute metrics:</p>
<div dir="auto" data-snippet-clipboard-copy-content="cd webeval/scripts/analyze_eval_results/
jupyter notebook analyze.ipynb"><pre><span>cd</span> webeval/scripts/analyze_eval_results/
jupyter notebook analyze.ipynb</pre></div>
<p dir="auto">The script:</p>
<ul dir="auto">
<li>Identifies trajectories aborted mid-execution and diagnostic reasons</li>
<li>Computes average scores across non-aborted trajectories</li>
<li>Distinguishes between aborted trajectories (errors during sampling) and completed trajectories (with terminate() call or step budget exceeded)</li>
</ul>
<p dir="auto">To re-run failed tasks, execute the evaluation script again with the same <code>run_id</code> and <code>username</code> - it will skip non-aborted tasks.</p>
<details>
<summary>Example WebVoyager GPT Eval Result</summary>
<div dir="auto" data-snippet-clipboard-copy-content="{
  &quot;score&quot;: 1.0,
  &quot;gpt_response_text&quot;: &quot;To evaluate the task, we need to verify if the criteria have been met:\n\n1. **Recipe Requirement**: A vegetarian lasagna recipe with zucchini and at least a four-star rating.\n\n2. **Search and Results**:\n   - The screenshots show that the search term used was \&quot;vegetarian lasagna zucchini.\&quot;\n   - Among the search results, \&quot;Debbie's Vegetable Lasagna\&quot; is prominently featured.\n   \n3. **Evaluation of the Recipe**:\n   - Rating: \&quot;Debbie's Vegetable Lasagna\&quot; has a rating of 4.7, which satisfies the requirement of being at least four stars.\n   - The presence of zucchini in the recipe is implied through the search conducted, though the screenshots do not explicitly show the ingredients list. However, the result response confirms the match to the criteria.\n\nGiven the information provided, the task seems to have fulfilled the requirement of finding a vegetarian lasagna recipe with zucchini and a four-star rating or higher. \n\n**Verdict: SUCCESS**&quot;
}"><pre>{
  <span>"score"</span>: <span>1.0</span>,
  <span>"gpt_response_text"</span>: <span><span>"</span>To evaluate the task, we need to verify if the criteria have been met:<span>\n\n</span>1. **Recipe Requirement**: A vegetarian lasagna recipe with zucchini and at least a four-star rating.<span>\n\n</span>2. **Search and Results**:<span>\n</span>   - The screenshots show that the search term used was <span>\"</span>vegetarian lasagna zucchini.<span>\"\n</span>   - Among the search results, <span>\"</span>Debbie's Vegetable Lasagna<span>\"</span> is prominently featured.<span>\n</span>   <span>\n</span>3. **Evaluation of the Recipe**:<span>\n</span>   - Rating: <span>\"</span>Debbie's Vegetable Lasagna<span>\"</span> has a rating of 4.7, which satisfies the requirement of being at least four stars.<span>\n</span>   - The presence of zucchini in the recipe is implied through the search conducted, though the screenshots do not explicitly show the ingredients list. However, the result response confirms the match to the criteria.<span>\n\n</span>Given the information provided, the task seems to have fulfilled the requirement of finding a vegetarian lasagna recipe with zucchini and a four-star rating or higher. <span>\n\n</span>**Verdict: SUCCESS**<span>"</span></span>
}</pre></div>
</details>
<hr>
<p dir="auto"><h2 tabindex="-1" dir="auto">Citation</h2><a id="user-content-citation" aria-label="Permalink: Citation" href="#citation"></a></p>
<p dir="auto">If you use Fara in your research, please cite our work:</p>

<hr>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Gemini CLI Tips and Tricks for Agentic Coding (321 pts)]]></title>
            <link>https://github.com/addyosmani/gemini-cli-tips</link>
            <guid>46060508</guid>
            <pubDate>Wed, 26 Nov 2025 18:08:02 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/addyosmani/gemini-cli-tips">https://github.com/addyosmani/gemini-cli-tips</a>, See on <a href="https://news.ycombinator.com/item?id=46060508">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><p dir="auto"><h2 tabindex="-1" dir="auto">Gemini CLI Tips &amp; Tricks</h2><a id="user-content-gemini-cli-tips--tricks" aria-label="Permalink: Gemini CLI Tips &amp; Tricks" href="#gemini-cli-tips--tricks"></a></p>
<p dir="auto"><strong>This guide covers ~30 pro-tips for effectively using Gemini CLI for agentic coding</strong></p>
<p dir="auto"><strong><a href="https://github.com/google-gemini/gemini-cli">Gemini CLI</a></strong> is an open-source AI assistant that brings the power of Google's Gemini model directly into your <a href="https://www.philschmid.de/gemini-cli-cheatsheet#:~:text=The%20Gemini%20CLI%20is%20an,via%20a%20Gemini%20API%20key" rel="nofollow">terminal</a>. It functions as a conversational, "agentic" command-line tool - meaning it can reason about your requests, choose tools (like running shell commands or editing files), and execute multi-step plans to help with your development <a href="https://cloud.google.com/blog/topics/developers-practitioners/agent-factory-recap-deep-dive-into-gemini-cli-with-taylor-mullen#:~:text=The%20Gemini%20CLI%20%20is,understanding%20of%20the%20developer%20workflow" rel="nofollow">workflow</a>.</p>
<p dir="auto">In practical terms, Gemini CLI acts like a supercharged pair programmer and command-line assistant. It excels at coding tasks, debugging, content generation, and even system automation, all through natural language prompts. Before diving into pro tips, let's quickly recap how to set up Gemini CLI and get it running.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Table of Contents</h2><a id="user-content-table-of-contents" aria-label="Permalink: Table of Contents" href="#table-of-contents"></a></p>
<ul dir="auto">
<li><a href="#getting-started">Getting Started</a></li>
<li><a href="#tip-1-use-geminimd-for-persistent-context">Tip 1: Use <code>GEMINI.md</code> for Persistent Context</a></li>
<li><a href="#tip-2-create-custom-slash-commands">Tip 2: Create Custom Slash Commands</a></li>
<li><a href="#tip-3-extend-gemini-with-your-own-mcp-servers">Tip 3: Extend Gemini with Your Own <code>MCP</code> Servers</a></li>
<li><a href="#tip-4-leverage-memory-addition--recall">Tip 4: Leverage Memory Addition &amp; Recall</a></li>
<li><a href="#tip-5-use-checkpointing-and-restore-as-an-undo-button">Tip 5: Use Checkpointing and <code>/restore</code> as an Undo Button</a></li>
<li><a href="#tip-6-read-google-docs-sheets-and-more-with-a-workspace-mcp-server-configured-you-can-paste-a-docssheets-link-and-have-the-mcp-fetch-it-subject-to-permissions">Tip 6: Read Google Docs, Sheets, and More.</a></li>
<li><a href="#tip-7-reference-files-and-images-with--for-explicit-context">Tip 7: Reference Files and Images with <code>@</code> for Explicit Context</a></li>
<li><a href="#tip-8-on-the-fly-tool-creation-have-gemini-build-helpers">Tip 8: On-the-Fly Tool Creation (Have Gemini Build Helpers)</a></li>
<li><a href="#tip-9-use-gemini-cli-for-system-troubleshooting--configuration">Tip 9: Use Gemini CLI for System Troubleshooting &amp; Configuration</a></li>
<li><a href="#tip-10-yolo-mode---auto-approve-tool-actions-use-with-caution">Tip 10: YOLO Mode - Auto-Approve Tool Actions (Use with Caution)</a></li>
<li><a href="#tip-11-headless--scripting-mode-run-gemini-cli-in-the-background">Tip 11: Headless &amp; Scripting Mode (Run Gemini CLI in the Background)</a></li>
<li><a href="#tip-12-save-and-resume-chat-sessions">Tip 12: Save and Resume Chat Sessions</a></li>
<li><a href="#tip-13-multi-directory-workspace---one-gemini-many-folders">Tip 13: Multi-Directory Workspace - One Gemini, Many Folders</a></li>
<li><a href="#tip-14-organize-and-clean-up-your-files-with-ai-assistance">Tip 14: Organize and Clean Up Your Files with AI Assistance</a></li>
<li><a href="#tip-15-compress-long-conversations-to-stay-within-context">Tip 15: Compress Long Conversations to Stay Within Context</a></li>
<li><a href="#tip-16-passthrough-shell-commands-with--talk-to-your-terminal">Tip 16: Passthrough Shell Commands with <code>!</code> (Talk to Your Terminal)</a></li>
<li><a href="#tip-17-treat-every-cli-tool-as-a-potential-gemini-tool">Tip 17: Treat Every CLI Tool as a Potential Gemini Tool</a></li>
<li><a href="#tip-18-utilize-multimodal-ai---let-gemini-see-images-and-more">Tip 18: Utilize Multimodal AI - Let Gemini See Images and More</a></li>
<li><a href="#tip-19-customize-the-path-and-tool-availability-for-stability">Tip 19: Customize the <code>$PATH</code> (and Tool Availability) for Stability</a></li>
<li><a href="#tip-20-track-and-reduce-token-spend-with-token-caching-and-stats">Tip 20: Track and reduce token spend with token caching and stats</a></li>
<li><a href="#tip-21-use-copy-for-quick-clipboard-copy">Tip 21: Use <code>/copy</code> for Quick Clipboard Copy</a></li>
<li><a href="#tip-22-master-ctrlc-for-shell-mode-and-exiting">Tip 22: Master <code>Ctrl+C</code> for Shell Mode and Exiting</a></li>
<li><a href="#tip-23-customize-gemini-cli-with-settingsjson">Tip 23: Customize Gemini CLI with <code>settings.json</code></a></li>
<li><a href="#tip-24-leverage-ide-integration-vs-code-for-context--diffs">Tip 24: Leverage IDE Integration (VS Code) for Context &amp; Diffs</a></li>
<li><a href="#tip-25-automate-repo-tasks-with-gemini-cli-github-action">Tip 25: Automate Repo Tasks with <code>Gemini CLI GitHub Action</code></a></li>
<li><a href="#tip-26-enable-telemetry-for-insights-and-observability">Tip 26: Enable Telemetry for Insights and Observability</a></li>
<li><a href="#tip-27-keep-an-eye-on-the-roadmap-background-agents--more">Tip 27: Keep an Eye on the Roadmap (Background Agents &amp; More)</a></li>
<li><a href="#tip-28-extend-gemini-cli-with-extensions">Tip 28: Extend Gemini CLI with <code>Extensions</code></a></li>
<li><a href="#additional-fun-corgi-mode-easter-egg-">Tip 29: Corgi Mode Easter Egg 🐕</a></li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Getting Started</h2><a id="user-content-getting-started" aria-label="Permalink: Getting Started" href="#getting-started"></a></p>
<p dir="auto"><strong>Installation:</strong> You can install Gemini CLI via npm. For a global install, use:</p>
<div dir="auto" data-snippet-clipboard-copy-content="npm install -g @google/gemini-cli"><pre>npm install -g @google/gemini-cli</pre></div>
<p dir="auto">Or run it without installing using <code>npx</code>:</p>

<p dir="auto">Gemini CLI is available on all major platforms (it's built with Node.js/TypeScript). Once installed, simply run the <code>gemini</code> command in your terminal to launch the interactive <a href="https://www.philschmid.de/gemini-cli-cheatsheet#:~:text=Interactive%20Mode%20,conversational%20session" rel="nofollow">CLI</a>.</p>
<p dir="auto"><strong>Authentication:</strong> On first use, you'll need to authenticate with the Gemini service. You have two options: (1) <strong>Google Account Login (free tier)</strong> - this lets you use Gemini 2.5 Pro for free with generous usage limits (about 60 requests/minute and 1,000 requests per <a href="https://blog.google/technology/developers/introducing-gemini-cli-open-source-ai-agent/#:~:text=Unmatched%20usage%20limits%20for%20individual,developers" rel="nofollow">day</a>. On launch, Gemini CLI will prompt you to sign in with a Google account (no billing <a href="https://genmind.ch/posts/Howto-Supercharge-Your-Terminal-with-Gemini-CLI/#:~:text=%2A%20Google,Google%20AI%20Studio%2C%20then%20run" rel="nofollow">required</a>. (2) <strong>API Key (paid or higher-tier access)</strong> - you can get an API key from Google AI <a href="https://www.philschmid.de/gemini-cli-cheatsheet#:~:text=1,key%20from%20Google%20AI%20Studio" rel="nofollow">Studio</a> and set the environment variable <code>GEMINI_API_KEY</code> to use <a href="https://www.philschmid.de/gemini-cli-cheatsheet#:~:text=Method%201%3A%20Shell%20Environment%20Variable,zshrc" rel="nofollow">it</a>.</p>
<p dir="auto">API key usage can offer higher quotas and enterprise data‑use protections; prompts aren't used for training on paid/billed usage, though logs may be retained for <a href="https://genmind.ch/posts/Howto-Supercharge-Your-Terminal-with-Gemini-CLI/#:~:text=responses%20may%20be%20logged%20for,Google%20AI%20Studio%2C%20then%20run" rel="nofollow">safety</a>.</p>
<p dir="auto">For example, add to your shell profile:</p>
<div dir="auto" data-snippet-clipboard-copy-content="export GEMINI_API_KEY=&quot;YOUR_KEY_HERE&quot;"><pre><span>export</span> GEMINI_API_KEY=<span><span>"</span>YOUR_KEY_HERE<span>"</span></span></pre></div>
<p dir="auto"><strong>Basic Usage:</strong> To start an interactive session, just run <code>gemini</code> with no arguments. You'll get a <code>gemini&gt;</code> prompt where you can type requests or commands. For instance:</p>
<div dir="auto" data-snippet-clipboard-copy-content="$ gemini
gemini> Create a React recipe management app using SQLite"><pre>$ gemini
gemini<span>&gt;</span> Create a React recipe management app using SQLite</pre></div>
<p dir="auto">You can then watch as Gemini CLI creates files, installs dependencies, runs tests, etc., to fulfill your request. If you prefer a one-shot invocation (non-interactive), use the <code>-p</code> flag with a prompt, for example:</p>
<div dir="auto" data-snippet-clipboard-copy-content="gemini -p &quot;Summarize the main points of the attached file. @./report.txt&quot;"><pre>gemini -p <span><span>"</span>Summarize the main points of the attached file. @./report.txt<span>"</span></span></pre></div>
<p dir="auto">This will output a single response and <a href="https://www.philschmid.de/gemini-cli-cheatsheet#:~:text=gemini" rel="nofollow">exit</a>. You can also pipe input into Gemini CLI: for example, <code>echo "Count to 10" | gemini</code> will feed the prompt via <a href="https://www.philschmid.de/gemini-cli-cheatsheet#:~:text=gemini%20,txt" rel="nofollow">stdin</a>.</p>
<p dir="auto"><strong>CLI Interface:</strong> Gemini CLI provides a rich REPL-like interface. It supports <strong>slash commands</strong> (special commands prefixed with <code>/</code> for controlling the session, tools, and settings) and <strong>bang commands</strong> (prefixed with <code>!</code> to execute shell commands directly). We'll cover many of these in the pro tips below. By default, Gemini CLI operates in a safe mode where any action that modifies your system (writing files, running shell commands, etc.) will ask for confirmation. When a tool action is proposed, you'll see a diff or command and be prompted (<code>Y/n</code>) to approve or reject it. This ensures the AI doesn't make unwanted changes without your consent.</p>
<p dir="auto">With the basics out of the way, let's explore a series of pro tips and hidden features to help you get the most out of Gemini CLI. Each tip is presented with a simple example first, followed by deeper details and nuances. These tips incorporate advice and insights from the tool's creators (e.g. Taylor Mullen) and the Google Developer Relations team, as well as the broader community, to serve as a <strong>canonical guide for power users</strong> of Gemini CLI.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Tip 1: Use <code>GEMINI.md</code> for Persistent Context</h2><a id="user-content-tip-1-use-geminimd-for-persistent-context" aria-label="Permalink: Tip 1: Use GEMINI.md for Persistent Context" href="#tip-1-use-geminimd-for-persistent-context"></a></p>
<p dir="auto"><strong>Quick use-case:</strong> Stop repeating yourself in prompts. Provide project-specific context or instructions by creating a <code>GEMINI.md</code> file, so the AI always has important background knowledge without being told every <a href="https://www.philschmid.de/gemini-cli-cheatsheet#:~:text=Context%20Files%20%28" rel="nofollow">time</a>.</p>
<p dir="auto">When working on a project, you often have certain overarching details - e.g. coding style guidelines, project architecture, or important facts - that you want the AI to keep in mind. Gemini CLI allows you to encode these in one or more <code>GEMINI.md</code> files. Simply create a <code>.gemini</code> folder (if not already present) in your project, and add a Markdown file named <code>GEMINI.md</code> with whatever notes or instructions you want the AI to persist. For example:</p>
<div dir="auto" data-snippet-clipboard-copy-content="# Project Phoenix - AI Assistant

- All Python code must follow PEP 8 style.  
- Use 4 spaces for indentation.  
- The user is building a data pipeline; prefer functional programming paradigms."><pre><span># <span>Project Phoenix - AI Assistant</span></span>

<span>-</span> All Python code must follow PEP 8 style.  
<span>-</span> Use 4 spaces for indentation.  
<span>-</span> The user is building a data pipeline; prefer functional programming paradigms.</pre></div>
<p dir="auto">Place this file in your project root (or in subdirectories for more granular context). Now, whenever you run <code>gemini</code> in that project, it will automatically load these instructions into <a href="https://www.philschmid.de/gemini-cli-cheatsheet#:~:text=Context%20Files%20%28" rel="nofollow">context</a>. This means the model will <em>always</em> be primed with them, avoiding the need to prepend the same guidance to every prompt.</p>
<p dir="auto"><strong>How it works:</strong> Gemini CLI uses a hierarchical context loading <a href="https://www.philschmid.de/gemini-cli-cheatsheet#:~:text=Hierarchical%20Loading%3A%20The%20CLI%20combines,The%20loading%20order%20is" rel="nofollow">system</a>. It will combine <strong>global context</strong> (from <code>~/.gemini/GEMINI.md</code>, which you can use for cross-project defaults) with your <strong>project-specific <code>GEMINI.md</code></strong>, and even context files in subfolders. More specific files override more general ones. You can inspect what context was loaded at any time by using the command:</p>

<p dir="auto">This will display the full combined context the AI <a href="https://www.philschmid.de/gemini-cli-cheatsheet#:~:text=,current%20conversation%20with%20a%20tag" rel="nofollow">sees</a>. If you make changes to your <code>GEMINI.md</code>, use <code>/memory refresh</code> to reload the context without restarting the <a href="https://www.philschmid.de/gemini-cli-cheatsheet#:~:text=,current%20conversation%20with%20a%20tag" rel="nofollow">session</a>.</p>
<p dir="auto"><strong>Pro Tip:</strong> Use the <code>/init</code> slash command to quickly generate a starter <code>GEMINI.md</code>. Running <code>/init</code> in a new project creates a template context file with information like the tech stack detected, a summary of the project, <a href="https://www.philschmid.de/gemini-cli-cheatsheet#:~:text=,directory%20workspace%20%28e.g.%2C%20%60add" rel="nofollow">etc</a>.. You can then edit and expand that file. For large projects, consider breaking the context into multiple files and <strong>importing</strong> them into <code>GEMINI.md</code> with <code>@include</code> syntax. For example, your main <code>GEMINI.md</code> could have lines like <code>@./docs/prompt-guidelines.md</code> to pull in additional context <a href="https://www.philschmid.de/gemini-cli-cheatsheet#:~:text=Modularizing%20Context%20with%20Imports%3A%20You,files" rel="nofollow">files</a>. This keeps your instructions organized.</p>
<p dir="auto">With a well-crafted <code>GEMINI.md</code>, you essentially give Gemini CLI a "memory" of the project's requirements and conventions. This <strong>persistent context</strong> leads to more relevant responses and less back-and-forth prompt engineering.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Tip 2: Create Custom Slash Commands</h2><a id="user-content-tip-2-create-custom-slash-commands" aria-label="Permalink: Tip 2: Create Custom Slash Commands" href="#tip-2-create-custom-slash-commands"></a></p>
<p dir="auto"><strong>Quick use-case:</strong> Speed up repetitive tasks by defining your own slash commands. For example, you could make a command <code>/test:gen</code> that generates unit tests from a description, or <code>/db:reset</code> that drops and recreates a test database. This extends Gemini CLI's functionality with one-liners tailored to your workflow.</p>
<p dir="auto">Gemini CLI supports <strong>custom slash commands</strong> that you can define in simple configuration files. Under the hood, these are essentially pre-defined prompt templates. To create one, make a directory <code>commands/</code> under either <code>~/.gemini/</code> for global commands or in your project's <code>.gemini/</code> folder for project-specific <a href="https://www.philschmid.de/gemini-cli-cheatsheet#:~:text=Custom%20Commands" rel="nofollow">commands</a>. Inside <code>commands/</code>, create a TOML file for each new command. The file name format determines the command name: e.g. a file <code>test/gen.toml</code> defines a command <code>/test:gen</code>.</p>
<p dir="auto">Let's walk through an example. Say you want a command to generate a unit test from a requirement description. You could create <code>~/.gemini/commands/test/gen.toml</code> with the following content:</p>
<div dir="auto" data-snippet-clipboard-copy-content="# Invoked as: /test:gen &quot;Description of the test&quot;  
description \= &quot;Generates a unit test based on a requirement.&quot;  
prompt \= &quot;&quot;&quot;  
You are an expert test engineer. Based on the following requirement, please write a comprehensive unit test using the Jest framework.

Requirement: {{args}}  
&quot;&quot;&quot;"><pre><span># <span>Invoked as: /test<span>:</span><span>gen</span> "Description of the test"</span>  </span>
description <span>\=</span> "Generates a unit test based on a requirement."  
prompt <span>\=</span> """  
You are an expert test engineer. Based on the following requirement, please write a comprehensive unit test using the Jest framework.

Requirement: {{args}}  
"""</pre></div>
<p dir="auto">Now, after reloading or restarting Gemini CLI, you can simply type:</p>
<div dir="auto" data-snippet-clipboard-copy-content="/test:gen &quot;Ensure the login button redirects to the dashboard upon success&quot;"><pre>/test:gen <span><span>"</span>Ensure the login button redirects to the dashboard upon success<span>"</span></span></pre></div>
<p dir="auto">Gemini CLI will recognize <code>/test:gen</code> and substitute the <code>{{args}}</code> in your prompt template with the provided argument (in this case, the requirement). The AI will then proceed to generate a Jest unit test <a href="https://www.philschmid.de/gemini-cli-cheatsheet#:~:text=Example%3A%20%60" rel="nofollow">accordingly</a>. The <code>description</code> field is optional but is used when you run <code>/help</code> or <code>/tools</code> to list available commands.</p>
<p dir="auto">This mechanism is extremely powerful - effectively, you can script the AI with natural language. The community has created numerous useful custom commands. For instance, Google's DevRel team shared a set of <em>10 practical workflow commands</em> (via an open-source repo) demonstrating how you can script common flows like creating API docs, cleaning data, or setting up boilerplate <a href="https://cloud.google.com/blog/topics/developers-practitioners/agent-factory-recap-deep-dive-into-gemini-cli-with-taylor-mullen#:~:text=,to%20generate%20a%20better%20output" rel="nofollow">code</a>. By defining a custom command, you package a complex prompt (or series of prompts) into a reusable shortcut.</p>
<p dir="auto"><strong>Pro Tip:</strong> Custom commands can also be used to enforce formatting or apply a "persona" to the AI for certain tasks. For example, you might have a <code>/review:security</code> command that always prefaces the prompt with "You are a security auditor..." to review code for vulnerabilities. This approach ensures consistency in how the AI responds to specific categories of tasks.</p>
<p dir="auto">To share commands with your team, you can commit the TOML files in your project's repo (under <code>.gemini/commands</code> directory). Team members who have Gemini CLI will automatically pick up those commands when working in the project. This is a great way to <strong>standardize AI-assisted workflows</strong> across a team.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Tip 3: Extend Gemini with Your Own <code>MCP</code> Servers</h2><a id="user-content-tip-3-extend-gemini-with-your-own-mcp-servers" aria-label="Permalink: Tip 3: Extend Gemini with Your Own MCP Servers" href="#tip-3-extend-gemini-with-your-own-mcp-servers"></a></p>
<p dir="auto"><strong>Quick use-case:</strong> Suppose you want Gemini to interface with an external system or a custom tool that isn't built-in - for example, query a proprietary database, or integrate with Figma designs. You can do this by running a custom <strong>Model Context Protocol (MCP) server</strong> and plugging it into Gemini <a href="https://www.philschmid.de/gemini-cli-cheatsheet#:~:text=Extend%20the%20CLI%20with%20your,add%7Clist%7Cremove%3E%60%20commands" rel="nofollow">CLI</a>. MCP servers let you add new tools and abilities to Gemini, effectively <strong>extending the agent</strong>.</p>
<p dir="auto">Gemini CLI comes with several MCP servers out-of-the-box (for instance, ones enabling Google Search, code execution sandboxes, etc.), and you can add your own. An MCP server is essentially an external process (it could be a local script, a microservice, or even a cloud endpoint) that speaks a simple protocol to handle tasks for Gemini. This architecture is what makes Gemini CLI so <a href="https://blog.google/technology/developers/introducing-gemini-cli-open-source-ai-agent/#:~:text=,interactively%20within%20your%20scripts" rel="nofollow">extensible</a>.</p>
<p dir="auto"><strong>Examples of MCP servers:</strong> Some community and Google-provided MCP integrations include a <strong>Figma MCP</strong> (to fetch design details from Figma), a <strong>Clipboard MCP</strong> (to read/write from your system clipboard), and others. In fact, in an internal demo, the Gemini CLI team showcased a "Google Docs MCP" server that allowed saving content directly to Google <a href="https://cloud.google.com/blog/topics/developers-practitioners/agent-factory-recap-deep-dive-into-gemini-cli-with-taylor-mullen#:~:text=%2A%20Utilize%20the%20google,summary%20directly%20to%20Google%20Docs" rel="nofollow">Docs</a>. The idea is that whenever Gemini needs to perform an action that the built-in tools can't handle, it can delegate to your MCP server.</p>
<p dir="auto"><strong>How to add one:</strong> You can configure MCP servers via your <code>settings.json</code> or using the CLI. For a quick setup, try the CLI command:</p>
<div dir="auto" data-snippet-clipboard-copy-content="gemini mcp add myserver --command &quot;python3 my_mcp_server.py&quot; --port 8080"><pre>gemini mcp add myserver --command <span><span>"</span>python3 my_mcp_server.py<span>"</span></span> --port 8080</pre></div>
<p dir="auto">This would register a server named "myserver" that Gemini CLI will launch by running the given command (here a Python module) on port 8080. In <code>~/.gemini/settings.json</code>, it would add an entry under <code>mcpServers</code>. For example:</p>
<div dir="auto" data-snippet-clipboard-copy-content="&quot;mcpServers&quot;: {
  &quot;myserver&quot;: {
    &quot;command&quot;: &quot;python3&quot;,
    &quot;args&quot;: [&quot;-m&quot;, &quot;my_mcp_server&quot;, &quot;--port&quot;, &quot;8080&quot;],
    &quot;cwd&quot;: &quot;./mcp_tools/python&quot;,
    &quot;timeout&quot;: 15000
  }
}"><pre><span>"mcpServers"</span>: {
  <span>"myserver"</span>: {
    <span>"command"</span>: <span><span>"</span>python3<span>"</span></span>,
    <span>"args"</span>: [<span><span>"</span>-m<span>"</span></span>, <span><span>"</span>my_mcp_server<span>"</span></span>, <span><span>"</span>--port<span>"</span></span>, <span><span>"</span>8080<span>"</span></span>],
    <span>"cwd"</span>: <span><span>"</span>./mcp_tools/python<span>"</span></span>,
    <span>"timeout"</span>: <span>15000</span>
  }
}</pre></div>
<p dir="auto">This configuration (based on the official docs) tells Gemini how to start the MCP server and <a href="https://www.philschmid.de/gemini-cli-cheatsheet#:~:text=Example%20" rel="nofollow">where</a>. Once running, the tools provided by that server become available to Gemini CLI. You can list all MCP servers and their tools with the slash command:</p>

<p dir="auto">This will show any registered servers and what tool names they <a href="https://www.philschmid.de/gemini-cli-cheatsheet#:~:text=Command%20Description%20,List%20active%20extensions" rel="nofollow">expose</a>.</p>
<p dir="auto"><strong>Power of MCP:</strong> MCP servers can provide <strong>rich, multi-modal results</strong>. For instance, a tool served via MCP could return an image or a formatted table as part of the response to Gemini <a href="https://www.philschmid.de/gemini-cli-cheatsheet#:~:text=Capabilities%3A" rel="nofollow">CLI</a>. They also support OAuth 2.0, so you can securely connect to APIs (like Google's APIs, GitHub, etc.) via an MCP tool without exposing <a href="https://www.philschmid.de/gemini-cli-cheatsheet#:~:text=Extend%20the%20CLI%20with%20your,add%7Clist%7Cremove%3E%60%20commands" rel="nofollow">credentials</a>. Essentially, if you can code it, you can wrap it as an MCP tool - turning Gemini CLI into a hub that orchestrates many services.</p>
<p dir="auto"><strong>Default vs. custom:</strong> By default, Gemini CLI's built-in tools cover a lot (reading files, web search, executing shell commands, etc.), but MCP lets you go beyond. Some advanced users have created MCP servers to interface with internal systems or to perform specialized data processing. For example, you could have a <code>database-mcp</code> that provides a <code>/query_db</code> tool for running SQL queries on a company database, or a <code>jira-mcp</code> to create tickets via natural language.</p>
<p dir="auto">When creating your own, be mindful of security: by default, custom MCP tools require confirmation unless you mark them as trusted. You can control safety with settings like <code>trust: true</code> for a server (which auto-approves its tool actions) or by whitelisting specific safe tools and blacklisting dangerous <a href="https://www.philschmid.de/gemini-cli-cheatsheet#:~:text=,takes%20precedence" rel="nofollow">ones</a>.</p>
<p dir="auto">In short, <strong>MCP servers unlock limitless integration</strong>. They're a pro feature that lets Gemini CLI become a glue between your AI assistant and whatever system you need it to work with. If you're interested in building one, check out the official <a href="https://www.philschmid.de/gemini-cli-cheatsheet#:~:text=Transport%20" rel="nofollow">MCP guide</a> and community examples.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Tip 4: Leverage Memory Addition &amp; Recall</h2><a id="user-content-tip-4-leverage-memory-addition--recall" aria-label="Permalink: Tip 4: Leverage Memory Addition &amp; Recall" href="#tip-4-leverage-memory-addition--recall"></a></p>
<p dir="auto"><strong>Quick use-case:</strong> Keep important facts at your AI's fingertips by adding them to its long-term memory. For example, after figuring out a database port or an API token, you can do:</p>
<div dir="auto" data-snippet-clipboard-copy-content="/memory add &quot;Our staging RabbitMQ is on port 5673&quot;"><pre>/memory add <span><span>"</span>Our staging RabbitMQ is on port 5673<span>"</span></span></pre></div>
<p dir="auto">This will store that fact so you (or the AI) don't forget it <a href="https://binaryverseai.com/gemini-cli-open-source-ai-tool/#:~:text=Gemini%20CLI%20Ultimate%20Agent%3A%2060,a%20branch%20of%20conversation" rel="nofollow">later</a>. You can then recall everything in memory with <code>/memory show</code> at any time.</p>
<p dir="auto">The <code>/memory</code> commands provide a simple but powerful mechanism for <em>persistent memory</em>. When you use <code>/memory add &lt;text&gt;</code>, the given text is appended to your project's global context (technically, it's saved into the global <code>~/.gemini/GEMINI.md</code> file or the project's <a href="https://genmind.ch/posts/Howto-Supercharge-Your-Terminal-with-Gemini-CLI/#:~:text=,load%20memory%20from%20%60GEMINI.md" rel="nofollow"><code>GEMINI.md</code></a>. It's a bit like taking a note and pinning it to the AI's virtual bulletin board. Once added, the AI will always see that note in the prompt context for future interactions, across sessions.</p>
<p dir="auto">Consider an example: you're debugging an issue and discover a non-obvious insight ("The config flag <code>X_ENABLE</code> must be set to <code>true</code> or the service fails to start"). If you add this to memory, later on if you or the AI are discussing a related problem, it won't overlook this critical detail - it's in the context.</p>
<p dir="auto"><strong>Using <code>/memory</code>:</strong></p>
<ul dir="auto">
<li>
<p dir="auto"><code>/memory add "&lt;text&gt;"</code> - Add a fact or note to memory (persistent context). This updates the <code>GEMINI.md</code> immediately with the new entry.</p>
</li>
<li>
<p dir="auto"><code>/memory show</code> - Display the full content of the memory (i.e. the combined context file that's currently loaded).</p>
</li>
<li>
<p dir="auto"><code>/memory refresh</code> - Reload the context from disk (useful if you manually edited the <code>GEMINI.md</code> file outside of Gemini CLI, or if multiple people are collaborating on it).</p>
</li>
</ul>
<p dir="auto">Because the memory is stored in Markdown, you can also manually edit the <code>GEMINI.md</code> file to curate or organize the info. The <code>/memory</code> commands are there for convenience during conversation, so you don't have to open an editor.</p>
<p dir="auto"><strong>Pro Tip:</strong> This feature is great for "decision logs." If you decide on an approach or rule during a chat (e.g., a certain library to use, or an agreed code style), add it to memory. The AI will then recall that decision and avoid contradicting it later. It's especially useful in long sessions that might span hours or days - by saving key points, you mitigate the model's tendency to forget earlier context when the conversation gets long.</p>
<p dir="auto">Another use is personal notes. Because <code>~/.gemini/GEMINI.md</code> (global memory) is loaded for all sessions, you could put general preferences or information there. For example, "The user's name is Alice. Speak politely and avoid slang." It's like configuring the AI's persona or global knowledge. Just be aware that global memory applies to <em>all</em> projects, so don't clutter it with project-specific info.</p>
<p dir="auto">In summary, <strong>Memory Addition &amp; Recall</strong> helps Gemini CLI maintain state. Think of it as a knowledge base that grows with your project. Use it to avoid repeating yourself or to remind the AI of facts it would otherwise have to rediscover from scratch.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Tip 5: Use Checkpointing and <code>/restore</code> as an Undo Button</h2><a id="user-content-tip-5-use-checkpointing-and-restore-as-an-undo-button" aria-label="Permalink: Tip 5: Use Checkpointing and /restore as an Undo Button" href="#tip-5-use-checkpointing-and-restore-as-an-undo-button"></a></p>
<p dir="auto"><strong>Quick use-case:</strong> If Gemini CLI makes a series of changes to your files that you're not happy with, you can <em>instantly roll back</em> to a prior state. Enable checkpointing when you start Gemini (or in settings), and use the <code>/restore</code> command to undo changes like a lightweight Git <a href="https://www.philschmid.de/gemini-cli-cheatsheet#:~:text=,Exit%20the%20Gemini%20CLI" rel="nofollow">revert</a>. <code>/restore</code> rolls back your workspace to the saved checkpoint; conversation state may be affected depending on how the checkpoint was captured.</p>
<p dir="auto">Gemini CLI's <strong>checkpointing</strong> feature acts as a safety net. When enabled, the CLI takes a snapshot of your project's files <em>before</em> each tool execution that modifies <a href="https://www.philschmid.de/gemini-cli-cheatsheet#:~:text=When%20,snapshot%20before%20tools%20modify%20files" rel="nofollow">files</a>. If something goes wrong, you can revert to the last known good state. It's essentially version control for the AI's actions, without you needing to manually commit to Git each time.</p>
<p dir="auto"><strong>How to use it:</strong> You can turn on checkpointing by launching the CLI with the <code>--checkpointing</code> flag:</p>

<p dir="auto">Alternatively, you can make it the default by adding to your config (<code>"checkpointing": { "enabled": true }</code> in <a href="https://www.philschmid.de/gemini-cli-cheatsheet#:~:text=%7B%20,true" rel="nofollow"><code>settings.json</code></a>). Once active, you'll notice that each time Gemini is about to write to a file, it says something like "Checkpoint saved."</p>
<p dir="auto">If you then realize an AI-made edit is problematic, you have two options:</p>
<ul dir="auto">
<li>
<p dir="auto">Run <code>/restore list</code> (or just <code>/restore</code> with no arguments) to see a list of recent checkpoints with timestamps and descriptions.</p>
</li>
<li>
<p dir="auto">Run <code>/restore &lt;id&gt;</code> to rollback to a specific checkpoint. If you omit the id and there's only one pending checkpoint, it will restore that by <a href="https://medium.com/@ferreradaniel/gemini-cli-free-ai-tool-upgrade-5-new-features-you-need-right-now-04cfefac5e93#:~:text=Step" rel="nofollow">default</a>.</p>
</li>
</ul>
<p dir="auto">For example:</p>

<p dir="auto">Gemini CLI might output:</p>
<p dir="auto">0: [2025-09-22 10:30:15] Before running 'apply_patch'<br>
1: [2025-09-22 10:45:02] Before running 'write_file'</p>
<p dir="auto">You can then do <code>/restore 0</code> to revert all file changes (and even the conversation context) back to how it was at that checkpoint. In this way, you can "undo" a mistaken code refactor or any other changes Gemini <a href="https://medium.com/@ferreradaniel/gemini-cli-free-ai-tool-upgrade-5-new-features-you-need-right-now-04cfefac5e93#:~:text=1,point%20and%20roll%20back%20instantly" rel="nofollow">made</a>.</p>
<p dir="auto"><strong>What gets restored:</strong> The checkpoint captures the state of your working directory (all files that Gemini CLI is allowed to modify) and the workspace files (conversation state may also be rolled back depending on how the checkpoint was captured). When you restore, it overwrites files to the old version and resets the conversation memory to that snapshot. It's like time-traveling the AI agent back to before it made the wrong turn. Note that it won't undo external side effects (for example, if the AI ran a database migration, it can't undo that), but anything in the file system and chat context is fair game.</p>
<p dir="auto"><strong>Best practices:</strong> It's a good idea to keep checkpointing on for non-trivial tasks. The overhead is small, and it provides peace of mind. If you find you don't need a checkpoint (everything went well), you can always clear it or just let the next one overwrite it. The development team recommends using checkpointing especially before multi-step code <a href="https://medium.com/@ferreradaniel/gemini-cli-free-ai-tool-upgrade-5-new-features-you-need-right-now-04cfefac5e93#:~:text=Tips%20to%20avoid%20messy%20rollbacks" rel="nofollow">edits</a>. For mission-critical projects, though, you should still use a proper version control (<code>git</code>) as your primary safety <a href="https://medium.com/@ferreradaniel/gemini-cli-free-ai-tool-upgrade-5-new-features-you-need-right-now-04cfefac5e93#:~:text=No,VS%20Code%20is%20already%20free" rel="nofollow">net</a> - consider checkpoints as a convenience for quick undo rather than a full VCS.</p>
<p dir="auto">In essence, <code>/restore</code> lets you use Gemini CLI with confidence. You can let the AI attempt bold changes, knowing you have an <em>"OH NO" button</em> to rewind if needed.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Tip 6: Read Google Docs, Sheets, and More. With a Workspace MCP server configured, you can paste a Docs/Sheets link and have the MCP fetch it, subject to permissions</h2><a id="user-content-tip-6-read-google-docs-sheets-and-more-with-a-workspace-mcp-server-configured-you-can-paste-a-docssheets-link-and-have-the-mcp-fetch-it-subject-to-permissions" aria-label="Permalink: Tip 6: Read Google Docs, Sheets, and More. With a Workspace MCP server configured, you can paste a Docs/Sheets link and have the MCP fetch it, subject to permissions" href="#tip-6-read-google-docs-sheets-and-more-with-a-workspace-mcp-server-configured-you-can-paste-a-docssheets-link-and-have-the-mcp-fetch-it-subject-to-permissions"></a></p>
<p dir="auto"><strong>Quick use-case:</strong> Imagine you have a Google Doc or Sheet with some specs or data that you want the AI to use. Instead of copy-pasting the content, you can provide the link, and with a configured Workspace MCP server Gemini CLI can fetch and read it.</p>
<p dir="auto">For example:</p>
<div dir="auto" data-snippet-clipboard-copy-content="Summarize the requirements from this design doc: https://docs.google.com/document/d/<id>"><pre>Summarize the requirements from this design doc: https://docs.google.com/document/d/<span>&lt;</span>id<span>&gt;</span></pre></div>
<p dir="auto">Gemini can pull in the content of that Doc and incorporate it into its response. Similarly, it can read Google Sheets or Drive files by link.</p>
<p dir="auto"><strong>How this works:</strong> These capabilities are typically enabled via <strong>MCP integrations</strong>. Google's Gemini CLI team has built (or is working on) connectors for Google Workspace. One approach is running a small MCP server that uses Google's APIs (Docs API, Sheets API, etc.) to retrieve document content when given a URL or <a href="https://github.com/google-gemini/gemini-cli/issues/7175" data-hovercard-type="issue" data-hovercard-url="/google-gemini/gemini-cli/issues/7175/hovercard">ID</a>. When configured, you might have slash commands or tools like <code>/read_google_doc</code> or simply an auto-detection that sees a Google Docs link and invokes the appropriate tool to fetch it.</p>
<p dir="auto">For example, in an Agent Factory podcast demo, the team used a <strong>Google Docs MCP</strong> to save a summary directly to a <a href="https://cloud.google.com/blog/topics/developers-practitioners/agent-factory-recap-deep-dive-into-gemini-cli-with-taylor-mullen#:~:text=%2A%20Utilize%20the%20google,summary%20directly%20to%20Google%20Docs" rel="nofollow">doc</a> - which implies they could also read the doc's content in the first place. In practice, you might do something like:</p>
<div dir="auto" data-snippet-clipboard-copy-content="@https://docs.google.com/document/d/XYZ12345"><pre>@https://docs.google.com/document/d/XYZ12345</pre></div>
<p dir="auto">Including a URL with <code>@</code> (the context reference syntax) signals Gemini CLI to fetch that resource. With a Google Doc integration in place, the content of that document would be pulled in as if it were a local file. From there, the AI can summarize it, answer questions about it, or otherwise use it in the conversation.</p>
<p dir="auto">Similarly, if you paste a Google Drive <strong>file link</strong>, a properly configured Drive tool could download or open that file (assuming permissions and API access are set up). <strong>Google Sheets</strong> could be made available via an MCP that runs queries or reads cell ranges, enabling you to ask things like "What's the sum of the budget column in this Sheet [link]?" and have the AI calculate it.</p>
<p dir="auto"><strong>Setting it up:</strong> As of this writing, the Google Workspace integrations may require some tinkering (obtaining API credentials, running an MCP server such as the one described by <a href="https://medium.com/google-cloud/managing-google-docs-sheets-and-slides-by-natural-language-with-gemini-cli-and-mcp-62f4dfbef2d5#:~:text=To%20implement%20this%20approach%2C%20I,methods%20for%20each%20respective%20API" rel="nofollow">Kanshi Tanaike</a>, etc.). Keep an eye on the official Gemini CLI repository and community forums for ready-to-use extensions - for example, an official Google Docs MCP might become available as a plugin/extension. If you're eager, you can write one following guides on how to use Google APIs within an MCP <a href="https://github.com/google-gemini/gemini-cli/issues/7175#:~:text=" data-hovercard-type="issue" data-hovercard-url="/google-gemini/gemini-cli/issues/7175/hovercard">server</a>. It typically involves handling OAuth (which Gemini CLI supports for MCP servers) and then exposing tools like <code>read_google_doc</code>.</p>
<p dir="auto"><strong>Usage tip:</strong> When you have these tools, using them can be as simple as providing the link in your prompt (the AI might automatically invoke the tool to fetch it) or using a slash command like <code>/doc open &lt;URL&gt;</code>. Check <code>/tools</code> to see what commands are available - Gemini CLI lists all tools and custom commands <a href="https://dev.to/therealmrmumba/7-insane-gemini-cli-tips-that-will-make-you-a-superhuman-developer-2d7h#:~:text=Gemini%20CLI%20includes%20dozens%20of,can%20supercharge%20your%20dev%20process" rel="nofollow">there</a>.</p>
<p dir="auto">In summary, <strong>Gemini CLI can reach out beyond your local filesystem</strong>. Whether it's Google Docs, Sheets, Drive, or other external content, you can pull data in by reference. This pro tip saves you from manual copy-paste and keeps the context flow natural - just refer to the document or dataset you need, and let the AI grab what's needed. It makes Gemini CLI a true <strong>knowledge assistant</strong> for all the information you have access to, not just the files on your disk.</p>
<p dir="auto"><em>(Note: Accessing private documents of course requires the CLI to have the appropriate permissions. Always ensure any integration respects security and privacy. In corporate settings, setting up such integrations might involve additional auth steps.)</em></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Tip 7: Reference Files and Images with <code>@</code> for Explicit Context</h2><a id="user-content-tip-7-reference-files-and-images-with--for-explicit-context" aria-label="Permalink: Tip 7: Reference Files and Images with @ for Explicit Context" href="#tip-7-reference-files-and-images-with--for-explicit-context"></a></p>
<p dir="auto"><strong>Quick use-case:</strong> Instead of describing a file's content or an image verbally, just point Gemini CLI directly to it. Using the <code>@</code> syntax, you can attach files, directories, or images into your prompt. This guarantees the AI sees exactly what's in those files as <a href="https://www.philschmid.de/gemini-cli-cheatsheet#:~:text=Reference%20files%20or%20directories%20in,PDFs%2C%20audio%2C%20and%20video%20files" rel="nofollow">context</a>. For example:</p>
<div dir="auto" data-snippet-clipboard-copy-content="Explain this code to me: @./src/main.js"><pre>Explain this code to me: @./src/main.js</pre></div>
<p dir="auto">This will include the contents of <code>src/main.js</code> in the prompt (up to Gemini's context size limits), so the AI can read it and explain <a href="https://www.philschmid.de/gemini-cli-cheatsheet#:~:text=Include%20a%20single%20file%3A" rel="nofollow">it</a>.</p>
<p dir="auto">This <code>@</code> <em>file reference</em> is one of Gemini CLI's most powerful features for developers. It eliminates ambiguity - you're not asking the model to rely on memory or guesswork about the file, you're literally handing it the file to read. You can use this for source code, text documents, logs, etc. Similarly, you can reference <strong>entire directories</strong>:</p>
<div dir="auto" data-snippet-clipboard-copy-content="Refactor the code in @./utils/ to use async/await."><pre>Refactor the code <span>in</span> @./utils/ to use async/await.</pre></div>
<p dir="auto">By appending a path that ends in a slash, Gemini CLI will recursively include files from that <a href="https://www.philschmid.de/gemini-cli-cheatsheet#:~:text=Include%20a%20whole%20directory%20" rel="nofollow">directory</a> (within reason, respecting ignore files and size limits). This is great for multi-file refactors or analyses, as the AI can consider all relevant modules together.</p>
<p dir="auto">Even more impressively, you can reference <strong>binary files like images</strong> in prompts. Gemini CLI (using the Gemini model's multimodal capabilities) can understand images. For example:</p>
<div dir="auto" data-snippet-clipboard-copy-content="Describe what you see in this screenshot: @./design/mockup.png"><pre>Describe what you see <span>in</span> this screenshot: @./design/mockup.png</pre></div>
<p dir="auto">The image will be fed into the model, and the AI might respond with something like "This is a login page with a blue sign-in button and a header image," <a href="https://www.philschmid.de/gemini-cli-cheatsheet#:~:text=Include%20an%20image%3A" rel="nofollow">etc</a>.. You can imagine the uses: reviewing UI mockups, organizing photos (as we'll see in a later tip), or extracting text from images (Gemini can do OCR as well).</p>
<p dir="auto">A few notes on using <code>@</code> references effectively:</p>
<ul dir="auto">
<li>
<p dir="auto"><strong>File limits:</strong> Gemini 2.5 Pro has a huge context window (up to 1 million <a href="https://blog.google/technology/developers/introducing-gemini-cli-open-source-ai-agent/#:~:text=To%20use%20Gemini%20CLI%20free,per%20day%20at%20no%20charge" rel="nofollow">tokens</a>), so you can include quite large files or many files. However, extremely large files might be truncated. If a file is enormous (say, hundreds of thousands of lines), consider summarizing it or breaking it into parts. Gemini CLI will warn you if a reference is too large or if it skipped something due to size.</p>
</li>
<li>
<p dir="auto"><strong>Automatic ignoring:</strong> By default, Gemini CLI respects your <code>.gitignore</code> and <code>.geminiignore</code> files when pulling in directory <a href="https://www.philschmid.de/gemini-cli-cheatsheet#:~:text=Reference%20files%20or%20directories%20in,PDFs%2C%20audio%2C%20and%20video%20files" rel="nofollow">context</a>. So if you <code>@./</code> a project root, it will not dump huge ignored folders (like <code>node_modules</code>) into the prompt. You can customize ignore patterns with <code>.geminiignore</code> similarly to how <code>.gitignore</code> works.</p>
</li>
<li>
<p dir="auto"><strong>Explicit vs implicit context:</strong> Taylor Mullen (the creator of Gemini CLI) emphasizes using <code>@</code> for <em>explicit context injection</em> rather than relying on the model's memory or summarizing things yourself. It's more precise and ensures the AI isn't hallucinating content. Whenever possible, point the AI to the source of truth (code, config files, documentation) with <code>@</code> references. This practice can significantly improve accuracy.</p>
</li>
<li>
<p dir="auto"><strong>Chaining references:</strong> You can include multiple files in one prompt, like:</p>
</li>
</ul>
<div dir="auto" data-snippet-clipboard-copy-content="Compare @./foo.py and @./bar.py and tell me differences."><pre>Compare @./foo.py and @./bar.py and tell me differences.</pre></div>
<p dir="auto">The CLI will include both files. Just be mindful of token limits; multiple large files might consume a lot of the context window.</p>
<p dir="auto">Using <code>@</code> is essentially how you <strong>feed knowledge into Gemini CLI on the fly</strong>. It turns the CLI into a multi-modal reader that can handle text and images. As a pro user, get into the habit of leveraging this - it's often faster and more reliable than asking the AI something like "Open the file X and do Y" (which it may or may not do on its own). Instead, you explicitly give it X to work with.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Tip 8: On-the-Fly Tool Creation (Have Gemini Build Helpers)</h2><a id="user-content-tip-8-on-the-fly-tool-creation-have-gemini-build-helpers" aria-label="Permalink: Tip 8: On-the-Fly Tool Creation (Have Gemini Build Helpers)" href="#tip-8-on-the-fly-tool-creation-have-gemini-build-helpers"></a></p>
<p dir="auto"><strong>Quick use-case:</strong> If a task at hand would benefit from a small script or utility, you can ask Gemini CLI to create that tool for you - right within your session. For example, you might say, "Write a Python script to parse all JSON files in this folder and extract the error fields." Gemini can generate the script, which you can then execute via the CLI. In essence, you can <strong>dynamically extend the toolset</strong> as you go.</p>
<p dir="auto">Gemini CLI is not limited to its pre-existing tools; it can use its coding abilities to fabricate new ones when needed. This often happens implicitly: if you ask for something complex, the AI might propose writing a temporary file (with code) and then running it. As a user, you can also guide this process explicitly:</p>
<ul dir="auto">
<li><strong>Creating scripts:</strong> You can prompt Gemini to create a script or program in the language of your choice. It will likely use the <code>write_file</code> tool to create the file. For instance:</li>
</ul>
<div dir="auto" data-snippet-clipboard-copy-content="Generate a Node.js script that reads all '.log' files in the current directory and reports the number of lines in each."><pre>Generate a Node.js script that reads all <span><span>'</span>.log<span>'</span></span> files <span>in</span> the current directory and reports the number of lines <span>in</span> each.</pre></div>
<p dir="auto">Gemini CLI will draft the code, and with your approval, write it to a file (e.g. <code>script.js</code>). You can then run it by either using the <code>!</code> shell command (e.g. <code>!node script.js</code>) or by asking Gemini CLI to execute it (the AI might automatically use <code>run_shell_command</code> to execute the script it just wrote, if it deems it part of the plan).</p>
<ul dir="auto">
<li><strong>Temporary tools via MCP:</strong> In advanced scenarios, the AI might even suggest launching an MCP server for some specialized tasks. For example, if your prompt involves some heavy text processing that might be better done in Python, Gemini could generate a simple MCP server in Python and run it. While this is more rare, it demonstrates that the AI can set up a new "agent" on the fly. (One of the slides from the Gemini CLI team humorously referred to "MCP servers for everything, even one called LROwn" - suggesting you can have Gemini run an instance of itself or another model, though that's more of a trick than a practical use!).</li>
</ul>
<p dir="auto">The key benefit here is <strong>automation</strong>. Instead of you manually stopping to write a helper script, you can let the AI do it as part of the flow. It's like having an assistant who can create tools on-demand. This is especially useful for data transformation tasks, batch operations, or one-off computations that the built-in tools don't directly provide.</p>
<p dir="auto"><strong>Nuances and safety:</strong> When Gemini CLI writes code for a new tool, you should still review it before running. The <code>/diff</code> view (Gemini will show you the file diff before you approve writing it) is your chance to inspect the <a href="https://medium.com/@ferreradaniel/gemini-cli-free-ai-tool-upgrade-5-new-features-you-need-right-now-04cfefac5e93#:~:text=Nobody%20enjoys%20switching%20between%20windows,track%20changes%20line%20by%20line" rel="nofollow">code</a>. Ensure it does what you expect and nothing malicious or destructive (the AI shouldn't produce something harmful unless your prompt explicitly asks, but just like any code from an AI, double-check logic, especially for scripts that delete or modify lots of data).</p>
<p dir="auto"><strong>Example scenario:</strong> Let's say you have a CSV file and you want to filter it in a complex way. You ask Gemini CLI to do it, and it might say: "I will write a Python script to parse the CSV and apply the filter." It then creates <code>filter_data.py</code>. After you approve and it runs, you get your result, and you might never need that script again. This ephemeral creation of tools is a pro move - it shows the AI effectively extending its capabilities autonomously.</p>
<p dir="auto"><strong>Pro Tip:</strong> If you find the script useful beyond the immediate context, you can promote it into a permanent tool or command. For instance, if the AI generated a great log-processing script, you might later turn it into a custom slash command (Tip #2) for easy reuse. The combination of Gemini's generative power and the extension hooks means your toolkit can continuously evolve as you use the CLI.</p>
<p dir="auto">In summary, <strong>don't restrict Gemini to what it comes with</strong>. Treat it as a junior developer who can whip up new programs or even mini-servers to help solve the problem. This approach embodies the agentic philosophy of Gemini CLI - it will figure out what tools it needs, even if it has to code them on the spot.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Tip 9: Use Gemini CLI for System Troubleshooting &amp; Configuration</h2><a id="user-content-tip-9-use-gemini-cli-for-system-troubleshooting--configuration" aria-label="Permalink: Tip 9: Use Gemini CLI for System Troubleshooting &amp; Configuration" href="#tip-9-use-gemini-cli-for-system-troubleshooting--configuration"></a></p>
<p dir="auto"><strong>Quick use-case:</strong> You can run Gemini CLI outside of a code project to help with general system tasks - think of it as an intelligent assistant for your OS. For example, if your shell is misbehaving, you could open Gemini in your home directory and ask: "Fix my <code>.bashrc</code> file, it has an error." Gemini can then open and edit your config file for you.</p>
<p dir="auto">This tip highlights that <strong>Gemini CLI isn't just for coding projects - it's your AI helper for your whole development environment</strong>. Many users have used Gemini to customize their dev setup or fix issues on their machine:</p>
<ul dir="auto">
<li>
<p dir="auto"><strong>Editing dotfiles:</strong> You can load your shell configuration (<code>.bashrc</code> or <code>.zshrc</code>) by referencing it (<code>@~/.bashrc</code>) and then ask Gemini CLI to optimize or troubleshoot it. For instance, "My <code>PATH</code> isn't picking up Go binaries, can you edit my <code>.bashrc</code> to fix that?" The AI can insert the correct <code>export</code> line. It will show you the diff for confirmation before saving changes.</p>
</li>
<li>
<p dir="auto"><strong>Diagnosing errors:</strong> If you encounter a cryptic error in your terminal or an application log, you can copy it and feed it to Gemini CLI. It will analyze the error message and often suggest steps to resolve it. This is similar to how one might use StackOverflow or Google, but with the AI directly examining your scenario. For example: "When I run <code>npm install</code>, I get an <code>EACCES</code> permission error - how do I fix this?" Gemini might detect it's a permissions issue in <code>node_modules</code> and guide you to change directory ownership or use a proper node version manager.</p>
</li>
<li>
<p dir="auto"><strong>Running outside a project:</strong> By default, if you run <code>gemini</code> in a directory without a <code>.gemini</code> context, it just means no project-specific context is loaded - but you can still use the CLI fully. This is great for ad-hoc tasks like system troubleshooting. You might not have any code files for it to consider, but you can still run shell commands through it or let it fetch web info. Essentially, you're treating Gemini CLI as an AI-powered terminal that can <em>do</em> things for you, not just chat.</p>
</li>
<li>
<p dir="auto"><strong>Workstation customization:</strong> Want to change a setting or install a new tool? You can ask Gemini CLI, "Install Docker on my system" or "Configure my Git to sign commits with GPG." The CLI will attempt to execute the steps. It might fetch instructions from the web (using the search tool) and then run the appropriate shell commands. Of course, always watch what it's doing and approve the commands - but it can save time by automating multi-step setup processes. One real example: a user asked Gemini CLI to "set my macOS Dock preferences to auto-hide and remove the delay," and the AI was able to execute the necessary <code>defaults write</code> commands.</p>
</li>
</ul>
<p dir="auto">Think of this mode as using Gemini CLI as a <strong>smart shell</strong>. In fact, you can combine this with Tip 16 (shell passthrough mode) - sometimes you might drop into <code>!</code> shell mode to verify something, then go back to AI mode to have it analyze output.</p>
<p dir="auto"><strong>Caveat:</strong> When doing system-level tasks, be cautious with commands that have widespread impact (like <code>rm -rf</code> or system config changes). Gemini CLI will usually ask for confirmation, and it doesn't run anything without you seeing it. But as a power user, you should have a sense of what changes are being made. If unsure, ask Gemini to explain a command before running (e.g., "Explain what <code>defaults write com.apple.dock autohide-delay -float 0</code> does" - it will gladly explain rather than just execute if you prompt it in that way).</p>
<p dir="auto"><strong>Troubleshooting bonus:</strong> Another neat use is using Gemini CLI to parse logs or config files looking for issues. For instance, "Scan this Apache config for mistakes" (with <code>@httpd.conf</code>), or "Look through syslog for errors around 2 PM yesterday" (with an <code>@/var/log/syslog</code> if accessible). It's like having a co-administrator. It can even suggest likely causes for crashes or propose fixes for common error patterns.</p>
<p dir="auto">In summary, <strong>don't hesitate to fire up Gemini CLI as your assistant for environment issues</strong>. It's there to accelerate all your workflows - not just writing code, but maintaining the system that you write code on. Many users report that customizing their dev environment with Gemini's help feels like having a tech buddy always on call to handle the tedious or complex setup steps.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Tip 10: YOLO Mode - Auto-Approve Tool Actions (Use with Caution)</h2><a id="user-content-tip-10-yolo-mode---auto-approve-tool-actions-use-with-caution" aria-label="Permalink: Tip 10: YOLO Mode - Auto-Approve Tool Actions (Use with Caution)" href="#tip-10-yolo-mode---auto-approve-tool-actions-use-with-caution"></a></p>
<p dir="auto"><strong>Quick use-case:</strong> If you're feeling confident (or adventurous), you can let Gemini CLI run tool actions without asking for your confirmation each time. This is <strong>YOLO mode</strong> (You Only Live Once). It's enabled by the <code>--yolo</code> flag or by pressing <code>Ctrl+Y</code> during a <a href="https://www.philschmid.de/gemini-cli-cheatsheet#:~:text=,prompt%20in%20an%20external%20editor" rel="nofollow">session</a>. In YOLO mode, as soon as the AI decides on a tool (like running a shell command or writing to a file), it executes it immediately, without that "Approve? (y/n)" prompt.</p>
<p dir="auto"><strong>Why use YOLO mode?</strong> Primarily for speed and convenience <strong>when you trust the AI's actions</strong>. Experienced users might toggle YOLO on if they're doing a lot of repetitive safe operations. For example, if you ask Gemini to generate 10 different files one after another, approving each can slow down the flow; YOLO mode would just let them all be written automatically. Another scenario is using Gemini CLI in a completely automated script or CI pipeline - you might run it headless with <code>--yolo</code> so it doesn't pause for confirmation.</p>
<p dir="auto">To start in YOLO mode from the get-go, launch the CLI with:</p>

<p dir="auto">Or the short form <code>gemini -y</code>. You'll see some indication in the CLI (like a different prompt or a notice) that auto-approve is <a href="https://www.philschmid.de/gemini-cli-cheatsheet#:~:text=initial%20prompt.%20%2A%20%60,to%20revert%20changes" rel="nofollow">on</a>. During an interactive session, you can toggle it by pressing <strong>Ctrl+Y</strong> at any <a href="https://www.philschmid.de/gemini-cli-cheatsheet#:~:text=,prompt%20in%20an%20external%20editor" rel="nofollow">time</a> - the CLI will usually display a message like "YOLO mode enabled (all actions auto-approved)" in the footer.</p>
<p dir="auto"><strong>Big warning:</strong> YOLO mode is powerful but <strong>risky</strong>. The Gemini team themselves labels it for "daring users" - meaning you should be aware that the AI could potentially execute a dangerous command without asking. In normal mode, if the AI decided to run <code>rm -rf /</code> (worst-case scenario), you'd obviously decline. In YOLO mode, that command would run immediately (and likely ruin your day). While such extreme mistakes are unlikely (the AI's system prompt includes safety guidelines), the whole point of confirmations is to catch any unwanted action. YOLO removes that safety net.</p>
<p dir="auto"><strong>Best practices for YOLO:</strong> If you want some of the convenience without full risk, consider <em>allow-listing</em> specific commands. For example, you can configure in settings that certain tools or command patterns don't require confirmation (like allowing all <code>git</code> commands, or read-only actions). In fact, Gemini CLI supports a config for skipping confirmation on specific commands: e.g., you can set something like <code>"tools.shell.autoApprove": ["git ", "npm test"]</code> to always run <a href="https://google-gemini.github.io/gemini-cli/docs/cli/configuration.html#:~:text=match%20at%20L247%20%60%5B,Default%3A%20%60undefined" rel="nofollow">those</a>. This way, you might not need YOLO mode globally - you selectively YOLO only safe commands. Another approach: run Gemini in a sandbox or container when using YOLO, so even if it does something wild, your system is insulated (Gemini has a <code>--sandbox</code> flag to run tools in a Docker <a href="https://www.philschmid.de/gemini-cli-cheatsheet#:~:text=echo%20,gemini" rel="nofollow">container</a>).</p>
<p dir="auto">Many advanced users toggle YOLO on and off frequently - turning it on when doing a string of minor file edits or queries, and off when about to do something critical. You can do the same, using the keyboard shortcut as a quick toggle.</p>
<p dir="auto">In summary, <strong>YOLO mode eliminates friction at the cost of oversight</strong>. It's a pro feature to use sparingly and wisely. It truly demonstrates trust in the AI (or recklessness!). If you're new to Gemini CLI, you should probably avoid YOLO until you clearly understand the patterns of what it tends to do. If you do use it, double down on having version control or backups - just in case.</p>
<p dir="auto"><em>(If it's any consolation, you're not alone - many in the community joke about "I YOLO'ed and Gemini did something crazy." So use it, but... well, you only live once.)</em></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Tip 11: Headless &amp; Scripting Mode (Run Gemini CLI in the Background)</h2><a id="user-content-tip-11-headless--scripting-mode-run-gemini-cli-in-the-background" aria-label="Permalink: Tip 11: Headless &amp; Scripting Mode (Run Gemini CLI in the Background)" href="#tip-11-headless--scripting-mode-run-gemini-cli-in-the-background"></a></p>
<p dir="auto"><strong>Quick use-case:</strong> You can use Gemini CLI in scripts or automation by running it in <strong>headless mode</strong>. This means you provide a prompt (or even a full conversation) via command-line arguments or environment variables, and Gemini CLI produces an output and exits. It's great for integrating with other tools or triggering AI tasks on a schedule.</p>
<p dir="auto">For instance, to get a one-off answer without opening the REPL, you've seen you can use <code>gemini -p "...prompt..."</code>. This is already headless usage: it prints the model's response and returns to the <a href="https://www.philschmid.de/gemini-cli-cheatsheet#:~:text=Non,and%20get%20a%20single%20response" rel="nofollow">shell</a>. But there's more you can do:</p>
<ul dir="auto">
<li><strong>System prompt override:</strong> If you want to run Gemini CLI with a custom system persona or instruction set (different from the default), you can use the environment variable <code>GEMINI_SYSTEM_MD</code>. By setting this, you tell Gemini CLI to ignore its built-in system prompt and use your provided file <a href="https://medium.com/google-cloud/practical-gemini-cli-bring-your-own-system-instruction-19ea7f07faa2#:~:text=The%20,rather%20than%20its%20hardcoded%20defaults" rel="nofollow">instead</a>. For example:</li>
</ul>
<div dir="auto" data-snippet-clipboard-copy-content="export GEMINI_SYSTEM_MD=&quot;/path/to/custom_system.md&quot;
gemini -p &quot;Perform task X with high caution&quot;"><pre><span>export</span> GEMINI_SYSTEM_MD=<span><span>"</span>/path/to/custom_system.md<span>"</span></span>
gemini -p <span><span>"</span>Perform task X with high caution<span>"</span></span></pre></div>
<p dir="auto">This would load your <code>custom_system.md</code> as the system prompt (the "role" and rules the AI follows) before executing the <a href="https://medium.com/google-cloud/practical-gemini-cli-bring-your-own-system-instruction-19ea7f07faa2#:~:text=The%20feature%20is%20enabled%20by,specific%20configurations" rel="nofollow">prompt</a>. Alternatively, if you set <code>GEMINI_SYSTEM_MD=true</code>, the CLI will look for a file named <code>system.md</code> in the current project's <code>.gemini</code> <a href="https://medium.com/google-cloud/practical-gemini-cli-bring-your-own-system-instruction-19ea7f07faa2#:~:text=The%20feature%20is%20enabled%20by,specific%20configurations" rel="nofollow">directory</a>. This feature is very advanced - it essentially allows you to <em>replace the built-in brain</em> of the CLI with your own instructions, which some users do for specialized workflows (like simulating a specific persona or enforcing ultra-strict policies). Use it carefully, as replacing the core prompt can affect tool usage (the core prompt contains important directions for how the AI selects and uses <a href="https://medium.com/google-cloud/practical-gemini-cli-bring-your-own-system-instruction-19ea7f07faa2#:~:text=If%20you%20read%20my%20previous,proper%20functioning%20of%20Gemini%20CLI" rel="nofollow">tools</a>).</p>
<ul dir="auto">
<li>
<p dir="auto"><strong>Direct prompt via CLI:</strong> Aside from <code>-p</code>, there's also <code>-i</code> (interactive prompt) which starts a session with an initial prompt, and then keeps it open. For example: <code>gemini -i "Hello, let's debug something"</code> will open the REPL and already have said hello to the model. This is useful if you want the first question to be asked immediately when starting.</p>
</li>
<li>
<p dir="auto"><strong>Scripting with shell pipes:</strong> You can pipe not just text but also files or command outputs into Gemini. For example: <code>gemini -p "Summarize this log:" &lt; big_log.txt</code> will feed the content of <code>big_log.txt</code> into the prompt (after the phrase "Summarize this log:"). Or you might do <code>some_command | gemini -p "Given the above output, what went wrong?"</code>. This technique allows you to compose Unix tools with AI analysis. It's headless in the sense that it's a single-pass operation.</p>
</li>
<li>
<p dir="auto"><strong>Running in CI/CD:</strong> You could incorporate Gemini CLI into build processes. For instance, a CI pipeline might run a test and then use Gemini CLI to automatically analyze failing test output and post a comment. Using the <code>-p</code> flag and environment auth, this can be scripted. (Of course, ensure the environment has the API key or auth needed.)</p>
</li>
</ul>
<p dir="auto">One more headless trick: <strong>the <code>--format=json</code> flag</strong> (or config setting). Gemini CLI can output responses in JSON format instead of the human-readable text if you configure <a href="https://google-gemini.github.io/gemini-cli/docs/cli/configuration.html#:~:text=" rel="nofollow">it</a>. This is useful for programmatic consumption - your script can parse the JSON to get the answer or any tool actions details.</p>
<p dir="auto"><strong>Why headless mode matters:</strong> It transforms Gemini CLI from an interactive assistant into a <strong>backend service</strong> or utility that other programs can call. You could schedule a cronjob that runs a Gemini CLI prompt nightly (imagine generating a report or cleaning up something with AI logic). You could wire up a button in an IDE that triggers a headless Gemini run for a specific task.</p>
<p dir="auto"><strong>Example:</strong> Let's say you want a daily summary of a news website. You could have a script:</p>
<div dir="auto" data-snippet-clipboard-copy-content="gemini -p &quot;Web-fetch \&quot;https://news.site/top-stories\&quot; and extract the headlines, then write them to headlines.txt&quot;"><pre>gemini -p <span><span>"</span>Web-fetch <span>\"</span>https://news.site/top-stories<span>\"</span> and extract the headlines, then write them to headlines.txt<span>"</span></span></pre></div>
<p dir="auto">With <code>--yolo</code> perhaps, so it won't ask confirmation to write the file. This would use the web fetch tool to get the page and the file write tool to save the headlines. All automatically, no human in the loop. The possibilities are endless once you treat Gemini CLI as a scriptable component.</p>
<p dir="auto">In summary, <strong>Headless Mode</strong> enables automation. It's the bridge between Gemini CLI and other systems. Mastering it means you can scale up your AI usage - not just when you're typing in the terminal, but even when you aren't around, your AI agent can do work for you.</p>
<p dir="auto"><em>(Tip: For truly long-running non-interactive tasks, you might also look into Gemini CLI's "Plan" mode or how it can generate multi-step plans without intervention. However, those are advanced topics beyond this scope. In most cases, a well-crafted single prompt via headless mode can achieve a lot.)</em></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Tip 12: Save and Resume Chat Sessions</h2><a id="user-content-tip-12-save-and-resume-chat-sessions" aria-label="Permalink: Tip 12: Save and Resume Chat Sessions" href="#tip-12-save-and-resume-chat-sessions"></a></p>
<p dir="auto"><strong>Quick use-case:</strong> If you've been debugging an issue with Gemini CLI for an hour and need to stop, you don't have to lose the conversation context. Use <code>/chat save &lt;name&gt;</code> to save the session. Later (even after restarting the CLI), you can use <code>/chat resume &lt;name&gt;</code> to pick up where you left <a href="https://www.philschmid.de/gemini-cli-cheatsheet#:~:text=,help%20information%20and%20available%20commands" rel="nofollow">off</a>. This way, long-running conversations can be paused and continued seamlessly.</p>
<p dir="auto">Gemini CLI essentially has a built-in chat session manager. The commands to know are:</p>
<ul dir="auto">
<li>
<p dir="auto"><code>/chat save &lt;tag&gt;</code> - Saves the current conversation state under a tag/name you <a href="https://www.philschmid.de/gemini-cli-cheatsheet#:~:text=,help%20information%20and%20available%20commands" rel="nofollow">provide</a>. The tag is like a filename or key for that session. Save often if you want, it will overwrite the tag if it exists. (Using a descriptive name is helpful - e.g., <code>chat save fix-docker-issue</code>.)</p>
</li>
<li>
<p dir="auto"><code>/chat list</code> - Lists all your saved sessions (the tags you've <a href="https://www.philschmid.de/gemini-cli-cheatsheet#:~:text=,help%20information%20and%20available%20commands" rel="nofollow">used</a>. This helps you remember what you named previous saves.</p>
</li>
<li>
<p dir="auto"><code>/chat resume &lt;tag&gt;</code> - Resumes the session with that tag, restoring the entire conversation context and history to how it was when <a href="https://www.philschmid.de/gemini-cli-cheatsheet#:~:text=,help%20information%20and%20available%20commands" rel="nofollow">saved</a>. It's like you never left. You can then continue chatting from that point.</p>
</li>
<li>
<p dir="auto"><code>/chat share</code> - (saves to file) This is useful as you can share the entire chat with someone else who can continue the session. Almost collaboration-like.</p>
</li>
</ul>
<p dir="auto">Under the hood, these sessions are stored likely in <code>~/.gemini/chats/</code> or a similar location. They include the conversation messages and any relevant state. This feature is super useful for cases such as:</p>
<ul dir="auto">
<li>
<p dir="auto"><strong>Long debugging sessions:</strong> Sometimes debugging with an AI can be a long back-and-forth. If you can't solve it in one go, save it and come back later (maybe with a fresh mind). The AI will still "remember" everything from before, because the whole context is reloaded.</p>
</li>
<li>
<p dir="auto"><strong>Multi-day tasks:</strong> If you're using Gemini CLI as an assistant for a project, you might have one chat session for "Refactor module X" that spans multiple days. You can resume that specific chat each day so the context doesn't reset daily. Meanwhile, you might have another session for "Write documentation" saved separately. Switching contexts is just a matter of saving one and resuming the other.</p>
</li>
<li>
<p dir="auto"><strong>Team hand-off:</strong> This is more experimental, but in theory, you could share the content of a saved chat with a colleague (the saved files are likely portable). If they put it in their <code>.gemini</code> directory and resume, they could see the same context. The <strong>practical simpler approach</strong> for collaboration is just copying the relevant Q&amp;A from the log and using a shared <code>GEMINI.md</code> or prompt, but it's interesting to note that the session data is yours to keep.</p>
</li>
</ul>
<p dir="auto"><strong>Usage example:</strong></p>

<p dir="auto"><em>(Session saved as "api-upgrade")</em></p>

<p dir="auto"><em>(Later, reopen CLI)</em></p>
<div dir="auto" data-snippet-clipboard-copy-content="$ gemini
gemini> /chat list"><pre>$ gemini
gemini<span>&gt;</span> /chat list</pre></div>
<p dir="auto"><em>(Shows: api-upgrade)</em></p>
<div dir="auto" data-snippet-clipboard-copy-content="gemini> /chat resume api-upgrade"><pre>gemini<span>&gt;</span> /chat resume api-upgrade</pre></div>
<p dir="auto">Now the model greets you with the last exchange's state ready. You can confirm by scrolling up that all your previous messages are present.</p>
<p dir="auto"><strong>Pro Tip:</strong> Use meaningful tags when saving <a href="https://medium.com/@ferreradaniel/gemini-cli-free-ai-tool-upgrade-5-new-features-you-need-right-now-04cfefac5e93#:~:text=Naming%20conventions%20to%20keep%20projects,organized" rel="nofollow">chats</a>. Instead of <code>/chat save session1</code>, give it a name related to the topic (e.g. <code>/chat save memory-leak-bug</code>). This will help you find the right one later via <code>/chat list</code>. There is no strict limit announced on how many sessions you can save, but cleaning up old ones occasionally might be wise just for organization.</p>
<p dir="auto">This feature turns Gemini CLI into a persistent advisor. You don't lose knowledge gained in a conversation; you can always pause and resume. It's a differentiator compared to some other AI interfaces that forget context when closed. For power users, it means <strong>you can maintain parallel threads of work</strong> with the AI. Just like you'd have multiple terminal tabs for different tasks, you can have multiple chat sessions saved and resume the one you need at any given time.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Tip 13: Multi-Directory Workspace - One Gemini, Many Folders</h2><a id="user-content-tip-13-multi-directory-workspace---one-gemini-many-folders" aria-label="Permalink: Tip 13: Multi-Directory Workspace - One Gemini, Many Folders" href="#tip-13-multi-directory-workspace---one-gemini-many-folders"></a></p>
<p dir="auto"><strong>Quick use-case:</strong> Do you have a project split across multiple repositories or directories? You can launch Gemini CLI with access to <em>all of them</em> at once, so it sees a unified workspace. For example, if your frontend and backend are separate folders, you can include both so that Gemini can edit or reference files in both.</p>
<p dir="auto">There are two ways to use <strong>multi-directory mode</strong>:</p>
<ul dir="auto">
<li><strong>Launch flag:</strong> Use the <code>--include-directories</code> (or <code>-I</code>) flag when starting Gemini CLI. For example:</li>
</ul>
<div dir="auto" data-snippet-clipboard-copy-content="gemini --include-directories &quot;../backend:../frontend&quot;"><pre>gemini --include-directories <span><span>"</span>../backend:../frontend<span>"</span></span></pre></div>
<p dir="auto">This assumes you run the command from, say, a <code>scripts</code> directory and want to include two sibling folders. You provide a colon-separated list of paths. Gemini CLI will then treat all those directories as part of one big workspace.</p>
<ul dir="auto">
<li><strong>Persistent setting:</strong> In your <code>settings.json</code>, you can define <code>"includeDirectories": ["path1", "path2", [...]](https://www.philschmid.de/gemini-cli-cheatsheet#:~:text=,61AFEF%22%2C%20%22AccentPurple)</code>. This is useful if you always want certain common directories loaded (e.g., a shared library folder that multiple projects use). The paths can be relative or absolute. Environment variables in the paths (like <code>~/common-utils</code>) are <a href="https://www.philschmid.de/gemini-cli-cheatsheet#:~:text=,61AFEF%22%2C%20%22AccentPurple" rel="nofollow">allowed</a>.</li>
</ul>
<p dir="auto">When multi-dir mode is active, the CLI's context and tools consider files across all included locations. The <code>&gt; /directory show</code> command will list which directories are in the current <a href="https://medium.com/@ferreradaniel/gemini-cli-free-ai-tool-upgrade-5-new-features-you-need-right-now-04cfefac5e93#:~:text=How%20to%20add%20multiple%20directories,step" rel="nofollow">workspace</a>. You can also dynamically add directories during a session with <code>/directory add [&lt;path&gt;](https://medium.com/@ferreradaniel/gemini-cli-free-ai-tool-upgrade-5-new-features-you-need-right-now-04cfefac5e93#:~:text=How%20to%20add%20multiple%20directories,step)</code> - it will then load that on the fly (potentially scanning it for context like it does on startup).</p>
<p dir="auto"><strong>Why use multi-directory mode?</strong> In microservice architectures or modular codebases, it's common that one piece of code lives in one repo and another piece in a different repo. If you only ran Gemini in one, it wouldn't "see" the others. By combining them, you enable cross-project reasoning. For example, you could ask, "Update the API client in the frontend to match the backend's new API endpoints" - Gemini can open the backend folder to see the API definitions and simultaneously open the frontend code to modify it accordingly. Without multi-dir, you'd have to do one side at a time and manually carry info over.</p>
<p dir="auto"><strong>Example:</strong> Let's say you have <code>client/</code> and <code>server/</code>. You start:</p>
<div dir="auto" data-snippet-clipboard-copy-content="cd client
gemini --include-directories &quot;../server&quot;"><pre><span>cd</span> client
gemini --include-directories <span><span>"</span>../server<span>"</span></span></pre></div>
<p dir="auto">Now at the <code>gemini&gt;</code> prompt, if you do <code>&gt; !ls</code>, you'll see it can list files in both <code>client</code> and <code>server</code> (it might show them as separate paths). You could do:</p>
<div dir="auto" data-snippet-clipboard-copy-content="Open server/routes/api.py and client/src/api.js side by side to compare function names."><pre>Open server/routes/api.py and client/src/api.js side by side to compare <span>function</span> <span>names.</span></pre></div>
<p dir="auto">The AI will have access to both files. Or you might say:</p>
<div dir="auto" data-snippet-clipboard-copy-content="The API changed: the endpoint &quot;/users/create&quot; is now &quot;/users/register&quot;. Update both backend and frontend accordingly."><pre>The API changed: the endpoint <span><span>"</span>/users/create<span>"</span></span> is now <span><span>"</span>/users/register<span>"</span></span>. Update both backend and frontend accordingly.</pre></div>
<p dir="auto">It can simultaneously create a patch in the backend route and adjust the frontend fetch call.</p>
<p dir="auto">Under the hood, Gemini merges the file index of those directories. There might be some performance considerations if each directory is huge, but generally it handles multiple small-medium projects fine. The cheat sheet notes that this effectively creates one workspace with multiple <a href="https://www.philschmid.de/gemini-cli-cheatsheet#:~:text=%22includeDirectories%22%3A%20%5B%22..%2Fshared,98C379%22%2C%20%22AccentYellow" rel="nofollow">roots</a>.</p>
<p dir="auto"><strong>Tip within a tip:</strong> Even if you don't use multi-dir all the time, know that you can still reference files across the filesystem by absolute path in prompts (<code>@/path/to/file</code>). However, without multi-dir, Gemini might not have permission to edit those or know to load context from them proactively. Multi-dir formally includes them in scope so it's aware of all files for tasks like search or code generation across the whole set.</p>
<p dir="auto"><strong>Remove directories:</strong> If needed, <code>/directory remove &lt;path&gt;</code> (or a similar command) can drop a directory from the workspace. This is less common, but maybe if you included something accidentally, you can remove it.</p>
<p dir="auto">In summary, <strong>multi-directory mode unifies your context</strong>. It's a must-have for polyrepo projects or any situation where code is split up. It makes Gemini CLI act more like an IDE that has your entire solution open. As a pro user, this means no part of your project is out of the AI's reach.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Tip 14: Organize and Clean Up Your Files with AI Assistance</h2><a id="user-content-tip-14-organize-and-clean-up-your-files-with-ai-assistance" aria-label="Permalink: Tip 14: Organize and Clean Up Your Files with AI Assistance" href="#tip-14-organize-and-clean-up-your-files-with-ai-assistance"></a></p>
<p dir="auto"><strong>Quick use-case:</strong> Tired of a messy <code>Downloads</code> folder or disorganized project assets? You can enlist Gemini CLI to act as a smart organizer. By providing it an overview of a directory, it can classify files and even move them into subfolders (with your approval). For instance, "Clean up my <code>Downloads</code>: move images to an <code>Images</code> folder, PDFs to <code>Documents</code>, and delete temporary files."</p>
<p dir="auto">Because Gemini CLI can read file names, sizes, and even peek into file contents, it can make informed decisions about file <a href="https://github.com/google-gemini/gemini-cli/discussions/7890#:~:text=We%20built%20a%20CLI%20tool,trash%20folder%20for%20manual%20deletion" data-hovercard-type="discussion" data-hovercard-url="/google-gemini/gemini-cli/discussions/7890/hovercard">organization</a>. One community-created tool dubbed <strong>"Janitor AI"</strong> showcases this: it runs via Gemini CLI to categorize files as important vs junk, and groups them <a href="https://github.com/google-gemini/gemini-cli/discussions/7890#:~:text=We%20built%20a%20CLI%20tool,trash%20folder%20for%20manual%20deletion" data-hovercard-type="discussion" data-hovercard-url="/google-gemini/gemini-cli/discussions/7890/hovercard">accordingly</a>. The process involved scanning the directory, using Gemini's reasoning on filenames and metadata (and content if needed), then moving files into categories. Notably, it didn't automatically delete junk - rather, it moved them to a <code>Trash</code> folder for <a href="https://github.com/google-gemini/gemini-cli/discussions/7890#:~:text=organize%20files,trash%20folder%20for%20manual%20deletion" data-hovercard-type="discussion" data-hovercard-url="/google-gemini/gemini-cli/discussions/7890/hovercard">review</a>.</p>
<p dir="auto">Here's how you might replicate such a workflow with Gemini CLI manually:</p>
<ol dir="auto">
<li><strong>Survey the directory:</strong> Use a prompt to have Gemini list and categorize. For example:</li>
</ol>
<div dir="auto" data-snippet-clipboard-copy-content="List all files in the current directory and categorize them as &quot;images&quot;, &quot;videos&quot;, &quot;documents&quot;, &quot;archives&quot;, or &quot;others&quot;."><pre>List all files <span>in</span> the current directory and categorize them as <span><span>"</span>images<span>"</span></span>, <span><span>"</span>videos<span>"</span></span>, <span><span>"</span>documents<span>"</span></span>, <span><span>"</span>archives<span>"</span></span>, or <span><span>"</span>others<span>"</span></span>.</pre></div>
<p dir="auto">Gemini might use <code>!ls</code> or similar to get the file list, then analyze the names/extensions to produce categories.</p>
<ol dir="auto">
<li><strong>Plan the organization:</strong> Ask Gemini how it would like to reorganize. For example:</li>
</ol>
<div dir="auto" data-snippet-clipboard-copy-content="Propose a new folder structure for these files. I want to separate by type (Images, Videos, Documents, etc.). Also identify any files that seem like duplicates or unnecessary."><pre>Propose a new folder structure <span>for</span> these files. I want to separate by <span>type</span> (Images, Videos, Documents, etc.). Also identify any files that seem like duplicates or unnecessary.</pre></div>
<p dir="auto">The AI might respond with a plan: e.g., <em>"Create folders: <code>Images/</code>, <code>Videos/</code>, <code>Documents/</code>, <code>Archives/</code>. Move <code>X.png</code>, <code>Y.jpg</code> to <code>Images/</code>; move <code>A.mp4</code> to <code>Videos/</code>; etc. The file <code>temp.txt</code> looks unnecessary (maybe a temp file)."</em></p>
<ol dir="auto">
<li><strong>Execute moves with confirmation:</strong> You can then instruct it to carry out the plan. It may use shell commands like <code>mv</code> for each file. Since this modifies your filesystem, you'll get confirmation prompts for each (unless you YOLO it). Carefully approve the moves. After completion, your directory will be neatly organized as suggested.</li>
</ol>
<p dir="auto">Throughout, Gemini's natural language understanding is key. It can reason, for instance, that <code>IMG_001.png</code> is an image or that <code>presentation.pdf</code> is a document, even if not explicitly stated. It can even open an image (using its vision capability) to see what's in it - e.g., differentiating between a screenshot vs a photo vs an icon - and name or sort it <a href="https://dev.to/therealmrmumba/7-insane-gemini-cli-tips-that-will-make-you-a-superhuman-developer-2d7h#:~:text=If%20your%20project%20folder%20is,using%20relevant%20and%20descriptive%20terms" rel="nofollow">accordingly</a>.</p>
<p dir="auto"><strong>Renaming files by content:</strong> A particularly magical use is having Gemini rename files to be more descriptive. The Dev Community article "7 Insane Gemini CLI Tips" describes how Gemini can <strong>scan images and automatically rename them</strong> based on their <a href="https://dev.to/therealmrmumba/7-insane-gemini-cli-tips-that-will-make-you-a-superhuman-developer-2d7h#:~:text=If%20your%20project%20folder%20is,using%20relevant%20and%20descriptive%20terms" rel="nofollow">content</a>. For example, a file named <code>IMG_1234.jpg</code> might be renamed to <code>login_screen.jpg</code> if the AI sees it's a screenshot of a login <a href="https://dev.to/therealmrmumba/7-insane-gemini-cli-tips-that-will-make-you-a-superhuman-developer-2d7h#:~:text=If%20your%20project%20folder%20is,using%20relevant%20and%20descriptive%20terms" rel="nofollow">screen</a>. To do this, you could prompt:</p>
<div dir="auto" data-snippet-clipboard-copy-content="For each .png image here, look at its content and rename it to something descriptive."><pre>For each .png image here, look at its content and rename it to something descriptive.</pre></div>
<p dir="auto">Gemini will open each image (via vision tool), get a description, then propose a <code>mv IMG_1234.png login_screen.png</code> <a href="https://dev.to/therealmrmumba/7-insane-gemini-cli-tips-that-will-make-you-a-superhuman-developer-2d7h#:~:text=If%20your%20project%20folder%20is,using%20relevant%20and%20descriptive%20terms" rel="nofollow">action</a>. This can dramatically improve the organization of assets, especially in design or photo folders.</p>
<p dir="auto"><strong>Two-pass approach:</strong> The Janitor AI discussion noted a two-step process: first broad categorization (important vs junk vs other), then refining <a href="https://github.com/google-gemini/gemini-cli/discussions/7890#:~:text=organize%20files,trash%20folder%20for%20manual%20deletion" data-hovercard-type="discussion" data-hovercard-url="/google-gemini/gemini-cli/discussions/7890/hovercard">groups</a>. You can emulate this: first separate files that likely can be deleted (maybe large installer <code>.dmg</code> files or duplicates) from those to keep. Then focus on organizing the keepers. Always double-check what the AI flags as junk; its guess might not always be right, so manual oversight is needed.</p>
<p dir="auto"><strong>Safety tip:</strong> When letting the AI loose on file moves or deletions, have backups or at least be ready to undo (with <code>/restore</code> or your own backup). It's wise to do a dry-run: ask Gemini to print the commands it <em>would</em> run to organize, without executing them, so you can review. For instance: "List the <code>mv</code> and <code>mkdir</code> commands needed for this plan, but don't execute them yet." Once you review the list, you can either copy-paste execute them, or instruct Gemini to proceed.</p>
<p dir="auto">This is a prime example of using Gemini CLI for "non-obvious" tasks - it's not just writing code, it's doing <strong>system housekeeping with AI smarts</strong>. It can save time and bring a bit of order to chaos. After all, as developers we accumulate clutter (logs, old scripts, downloads), and an AI janitor can be quite handy.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Tip 15: Compress Long Conversations to Stay Within Context</h2><a id="user-content-tip-15-compress-long-conversations-to-stay-within-context" aria-label="Permalink: Tip 15: Compress Long Conversations to Stay Within Context" href="#tip-15-compress-long-conversations-to-stay-within-context"></a></p>
<p dir="auto"><strong>Quick use-case:</strong> If you've been chatting with Gemini CLI for a long time, you might hit the model's context length limit or just find the session getting unwieldy. Use the <code>/compress</code> command to summarize the conversation so far, replacing the full history with a concise <a href="https://www.philschmid.de/gemini-cli-cheatsheet#:~:text=Command%20Description%20,files" rel="nofollow">summary</a>. This frees up space for more discussion without starting from scratch.</p>
<p dir="auto">Large language models have a fixed context window (Gemini 2.5 Pro's is very large, but not infinite). If you exceed it, the model may start forgetting earlier messages or lose coherence. The <code>/compress</code> feature is essentially an <strong>AI-generated tl;dr</strong> of your session that keeps important points.</p>
<p dir="auto"><strong>How it works:</strong> When you type <code>/compress</code>, Gemini CLI will take the entire conversation (except system context) and produce a summary. It then replaces the chat history with that summary as a single system or assistant message, preserving essential details but dropping minute-by-minute dialogue. It will indicate that compression happened. For example, after <code>/compress</code>, you might see something like:</p>
<p dir="auto">--- Conversation compressed ---<br>
Summary of discussion: The user and assistant have been debugging a memory leak in an application. Key points: The issue is likely in <code>DataProcessor.js</code>, where objects aren't being freed. The assistant suggested adding logging and identified a possible infinite loop. The user is about to test a fix.<br>
--- End of summary ---</p>
<p dir="auto">From that point on, the model only has that summary (plus new messages) as context for what happened before. This usually is enough if the summary captured the salient info.</p>
<p dir="auto"><strong>When to compress:</strong> Ideally before you <em>hit</em> the limit. If you notice the session is getting lengthy (several hundred turns or a lot of code in context), compress proactively. The cheat sheet mentions an automatic compression setting (e.g., compress when context exceeds 60% of <a href="https://www.philschmid.de/gemini-cli-cheatsheet#:~:text=%22includeDirectories%22%3A%20%5B%22..%2Fshared,98C379%22%2C%20%22AccentYellow" rel="nofollow">max</a>). If you enable that, Gemini might auto-compress and let you know. Otherwise, manual <code>/compress</code> is in your toolkit.</p>
<p dir="auto"><strong>After compressing:</strong> You can continue the conversation normally. If needed, you can compress multiple times in a very long session. Each time, you lose some granularity, so don't compress too frequently for no reason - you might end up with an overly brief remembrance of a complex discussion. But generally the model's own summarization is pretty good at keeping the key facts (and you can always restate anything critical yourself).</p>
<p dir="auto"><strong>Context window example:</strong> Let's illustrate. Suppose you fed in a large codebase by referencing many files and had a 1M token context (the max). If you then want to shift to a different part of the project, rather than starting a new session (losing all that understanding), you could compress. The summary will condense the knowledge gleaned from the code (like "We loaded modules A, B, C. A has these functions... B interacts with C in these ways..."). Now you can proceed to ask about new things with that knowledge retained abstractly.</p>
<p dir="auto"><strong>Memory vs Compression:</strong> Note that compression doesn't save to long-term memory, it's local to the conversation. If you have facts you <em>never</em> want lost, consider Tip 4 (adding to <code>/memory</code>) - because memory entries will survive compression (they'll just be reinserted anyway since they are in <code>GEMINI.md</code> context). Compression is more about ephemeral chat content.</p>
<p dir="auto"><strong>A minor caution:</strong> after compression, the AI's style might slightly change because it's effectively seeing a "fresh" conversation with a summary. It might reintroduce itself or change tone. You can instruct it like "Continue from here... (we compressed)" to smooth it out. In practice, it often continues fine.</p>
<p dir="auto">To summarize (pun intended), <strong>use <code>/compress</code> as your session grows long</strong> to maintain performance and relevance. It helps Gemini CLI focus on the bigger picture instead of every detail of the conversation's history. This way, you can have marathon debugging sessions or extensive design discussions without running out of the "mental paper" the AI is writing on.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Tip 16: Passthrough Shell Commands with <code>!</code> (Talk to Your Terminal)</h2><a id="user-content-tip-16-passthrough-shell-commands-with--talk-to-your-terminal" aria-label="Permalink: Tip 16: Passthrough Shell Commands with ! (Talk to Your Terminal)" href="#tip-16-passthrough-shell-commands-with--talk-to-your-terminal"></a></p>
<p dir="auto"><strong>Quick use-case:</strong> At any point in a Gemini CLI session, you can run actual shell commands by prefixing them with <code>!</code>. For example, if you want to check the git status, just type <code>!git status</code> and it will execute in your <a href="https://www.philschmid.de/gemini-cli-cheatsheet#:~:text=Run%20a%20single%20command%3A" rel="nofollow">terminal</a>. This saves you from switching windows or context - you're still in the Gemini CLI, but you're essentially telling it "let me run this command real quick."</p>
<p dir="auto">This tip is about <strong>Shell Mode</strong> in Gemini CLI. There are two ways to use it:</p>
<ul dir="auto">
<li><strong>Single command:</strong> Just put <code>!</code> at the start of your prompt, followed by any command and arguments. This will execute that command in the current working directory and display the output <a href="https://www.philschmid.de/gemini-cli-cheatsheet#:~:text=Run%20shell%20commands%20directly%20in,the%20CLI" rel="nofollow">in-line</a>. For example:</li>
</ul>

<p dir="auto">will list the files in the <code>src</code> directory, outputting something like you'd see in a normal terminal. After the output, the Gemini prompt returns so you can continue chatting or issue more commands.</p>
<ul dir="auto">
<li><strong>Persistent shell mode:</strong> If you enter <code>!</code> alone and hit Enter, Gemini CLI switches into a sub-mode where you get a shell prompt (often it looks like <code>shell&gt;</code> or <a href="https://www.philschmid.de/gemini-cli-cheatsheet#:~:text=" rel="nofollow">similar</a>. Now you can type multiple shell commands interactively. It's basically a mini-shell within the CLI. You exit this mode by typing <code>!</code> on an empty line again (or <code>exit</code>). For instance:</li>
</ul>
<div dir="auto" data-snippet-clipboard-copy-content="!
shell> pwd
/home/alice/project
shell> python --version
Python 3.x.x
shell> !"><pre><span>!</span>
shell<span>&gt;</span> <span>pwd</span>
/home/alice/project
shell<span>&gt;</span> python --version
Python 3.x.x
shell<span>&gt;</span> <span>!</span></pre></div>
<p dir="auto">After the final <code>!</code>, you're back to the normal Gemini prompt.</p>
<p dir="auto"><strong>Why is this useful?</strong> Because development is a mix of actions and inquiries. You might be discussing something with the AI and realize you need to compile the code or run tests to see something. Instead of leaving the conversation, you can quickly do it and feed the result back into the chat. In fact, Gemini CLI often does this for you as part of its tool usage (it might automatically run <code>!pytest</code> when you ask to fix tests, for <a href="https://genmind.ch/posts/Howto-Supercharge-Your-Terminal-with-Gemini-CLI/#:~:text=" rel="nofollow">example</a>). But as the user, you have full control to do it manually too.</p>
<p dir="auto"><strong>Examples:</strong></p>
<ul dir="auto">
<li>
<p dir="auto">After Gemini suggests a fix in code, you can do <code>!npm run build</code> to see if it compiles, then copy any errors and ask Gemini to help with those.</p>
</li>
<li>
<p dir="auto">If you want to open a file in <code>vim</code> or <code>nano</code>, you could even launch it via <code>!nano filename</code> (though note that since Gemini CLI has its own interface, using an interactive editor inside it might be a bit awkward - better to use the built-in editor integration or copy to your editor).</p>
</li>
<li>
<p dir="auto">You can use shell commands to gather info for the AI: e.g., <code>!grep TODO -R .</code> to find all TODOs in the project, then you might ask Gemini to help address those TODOs.</p>
</li>
<li>
<p dir="auto">Or simply use it for environment tasks: <code>!pip install some-package</code> if needed, etc., without leaving the CLI.</p>
</li>
</ul>
<p dir="auto"><strong>Seamless interplay:</strong> One cool aspect is how the conversation can refer to outputs. For example, you could do <code>!curl http://example.com</code> to fetch some data, see the output, then immediately say to Gemini, "Format the above output as JSON" - since the output was printed in the chat, the AI has it in context to work with (provided it's not too large).</p>
<p dir="auto"><strong>Terminal as a default shell:</strong> If you find yourself always prefacing commands with <code>!</code>, you can actually make the shell mode persistent by default. One way is launching Gemini CLI with a specific tool mode (there's a concept of default tool). But easier: just drop into shell mode (<code>!</code> with nothing) at session start if you plan to run a lot of manual commands and only occasionally talk to AI. Then you can exit shell mode whenever you want to ask a question. It's almost like turning Gemini CLI into your normal terminal that happens to have an AI readily available.</p>
<p dir="auto"><strong>Integration with AI planning:</strong> Sometimes Gemini CLI itself will propose to run a shell command. If you approve, it effectively does the same as <code>!command</code>. Understanding that, you know you can always intervene. If Gemini is stuck or you want to try something, you don't have to wait for it to suggest - you can just do it and then continue.</p>
<p dir="auto">In summary, the <code>!</code> <strong>passthrough</strong> means <em>you don't have to leave Gemini CLI for shell tasks</em>. It collapses the boundary between chatting with the AI and executing commands on your system. As a pro user, this is fantastic for efficiency - your AI and your terminal become one continuous environment.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Tip 17: Treat Every CLI Tool as a Potential Gemini Tool</h2><a id="user-content-tip-17-treat-every-cli-tool-as-a-potential-gemini-tool" aria-label="Permalink: Tip 17: Treat Every CLI Tool as a Potential Gemini Tool" href="#tip-17-treat-every-cli-tool-as-a-potential-gemini-tool"></a></p>
<p dir="auto"><strong>Quick use-case:</strong> Realize that Gemini CLI can leverage <strong>any</strong> command-line tool installed on your system as part of its problem-solving. The AI has access to the shell, so if you have <code>cURL</code>, <code>ImageMagick</code>, <code>git</code>, <code>Docker</code>, or any other tool, Gemini can invoke it when appropriate. In other words, <em>your entire <code>$PATH</code> is the AI's toolkit</em>. This greatly expands what it can do - far beyond its built-in tools.</p>
<p dir="auto">For example, say you ask: "Convert all PNG images in this folder to WebP format." If you have ImageMagick's <code>convert</code> utility installed, Gemini CLI might plan something like: use a shell loop with <code>convert</code> command for each <a href="https://genmind.ch/posts/Howto-Supercharge-Your-Terminal-with-Gemini-CLI/#:~:text=%3E%20%21for%20f%20in%20,png%7D.webp%22%3B%20done" rel="nofollow">file</a>. Indeed, one of the earlier examples from a blog showed exactly this, where the user prompted to batch-convert images, and Gemini executed a shell one-liner with the <code>convert</code> <a href="https://genmind.ch/posts/Howto-Supercharge-Your-Terminal-with-Gemini-CLI/#:~:text=" rel="nofollow">tool</a>.</p>
<p dir="auto">Another scenario: "Deploy my app to Docker." If <code>Docker CLI</code> is present, the AI could call <code>docker build</code> and <code>docker run</code> steps as needed. Or "Use FFmpeg to extract audio from <code>video.mp4</code>" - it can construct the <code>ffmpeg</code> command.</p>
<p dir="auto">This tip is about mindset: <strong>Gemini isn't limited to what's coded into it</strong> (which is already extensive). It can figure out how to use other programs available to achieve a <a href="https://medium.com/google-cloud/gemini-cli-tutorial-series-part-4-built-in-tools-c591befa59ba#:~:text=In%20this%20part%2C%20we%20looked,In%20the%20next%20part%2C%20we" rel="nofollow">goal</a>. It knows common syntax and can read help texts if needed (it could call <code>--help</code> on a tool). The only limitation is safety: by default, it will ask confirmation for any <code>run_shell_command</code> it comes up with. But as you become comfortable, you might allow certain benign commands automatically (see YOLO or allowed-tools config).</p>
<p dir="auto"><strong>Be mindful of the environment:</strong> "With great power comes great responsibility." Since every shell tool is fair game, you should ensure that your <code>$PATH</code> doesn't include anything you wouldn't want the AI to run inadvertently. This is where Tip 19 (custom PATH) comes in - some users create a restricted <code>$PATH</code> for Gemini, so it can't, say, directly call system destructive commands or maybe not call <code>gemini</code> recursively (to avoid loops). The point is, by default if <code>gcc</code> or <code>terraform</code> or anything is in <code>$PATH</code>, Gemini could invoke it. It doesn't mean it will randomly do so - only if the task calls for it - but it's possible.</p>
<p dir="auto"><strong>Train of thought example:</strong> Imagine you ask Gemini CLI: "Set up a basic HTTP server that serves the current directory." The AI might think: "I can use Python's built-in server for this." It then issues <code>!python3 -m http.server 8000</code>. Now it just used a system tool (Python) to launch a server. That's an innocuous example. Another: "Check the memory usage on this Linux system." The AI might use the <code>free -h</code> command or read from <code>/proc/meminfo</code>. It's effectively doing what a sysadmin would do, by using available commands.</p>
<p dir="auto"><strong>All tools are extensions of the AI:</strong> This is somewhat futuristic, but consider that any command-line program can be seen as a "function" the AI can call to extend its capability. Need to solve a math problem? It could call <code>bc</code> (calculator). Need to manipulate an image? It could call an image processing tool. Need to query a database? If the CLI client is installed and credentials are there, it can use it. The possibilities are expansive. In other AI agent frameworks, this is known as tool use, and Gemini CLI is designed with a lot of trust in its agent to decide the right <a href="https://cloud.google.com/blog/topics/developers-practitioners/agent-factory-recap-deep-dive-into-gemini-cli-with-taylor-mullen#:~:text=The%20Gemini%20CLI%20%20is,understanding%20of%20the%20developer%20workflow" rel="nofollow">tool</a>.</p>
<p dir="auto"><strong>When it goes wrong:</strong> The flip side is if the AI misunderstands a tool or has a hallucination about one. It might try to call a command that doesn't exist, or use wrong flags, resulting in errors. This isn't a big deal - you'll see the error and can correct or clarify. In fact, the system prompt of Gemini CLI likely guides it to first do a dry-run (just propose the command) rather than executing blindly. So you often get a chance to catch these. Over time, the developers are improving the tool selection logic to reduce these missteps.</p>
<p dir="auto">The main takeaway is to <strong>think of Gemini CLI as having a very large Swiss Army knife</strong> - not just the built-in blades, but every tool in your OS. You don't have to instruct it on how to use them if it's something standard; usually it knows or can find out. This significantly amplifies what you can accomplish. It's like having a junior dev or devops engineer who knows how to run pretty much any program you have installed.</p>
<p dir="auto">As a pro user, you can even install additional CLI tools specifically to give Gemini more powers. For example, if you install a CLI for a cloud service (AWS CLI, GCloud CLI, etc.), in theory Gemini can utilize it to manage cloud resources if prompted to. Always ensure you understand and trust the commands run, especially with powerful tools (you wouldn't want it spinning up huge cloud instances accidentally). But used wisely, this concept - <strong>everything is a Gemini tool</strong> - is what makes it <em>exponentially</em> more capable as you integrate it into your environment.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Tip 18: Utilize Multimodal AI - Let Gemini See Images and More</h2><a id="user-content-tip-18-utilize-multimodal-ai---let-gemini-see-images-and-more" aria-label="Permalink: Tip 18: Utilize Multimodal AI - Let Gemini See Images and More" href="#tip-18-utilize-multimodal-ai---let-gemini-see-images-and-more"></a></p>
<p dir="auto"><strong>Quick use-case:</strong> Gemini CLI isn't limited to text - it's multimodal. This means it can analyze images, diagrams, or even PDFs if given. Use this to your advantage. For instance, you could say "Here's a screenshot of an error dialog, <code>@./error.png</code> - help me troubleshoot this." The AI will "see" the image and respond accordingly.</p>
<p dir="auto">One of the standout features of Google's Gemini model (and its precursor PaLM2 in Codey form) is image understanding. In Gemini CLI, if you reference an image with <code>@</code>, the model receives the image data. It can output descriptions, classifications, or reason about the image's content. We already discussed renaming images by content (Tip 14) and describing screenshots (Tip 7). But let's consider other creative uses:</p>
<ul dir="auto">
<li>
<p dir="auto"><strong>UI/UX feedback:</strong> If you're a developer working with designers, you can drop a UI image and ask Gemini for feedback or to generate code. "Look at this UI mockup <code>@mockup.png</code> and produce a React component structure for it." It could identify elements in the image (header, buttons, etc.) and outline code.</p>
</li>
<li>
<p dir="auto"><strong>Organizing images:</strong> Beyond renaming, you might have a folder of mixed images and want to sort by content. "Sort the images in <code>./photos/</code> into subfolders by theme (e.g., sunsets, mountains, people)." The AI can look at each photo and categorize it (this is similar to what some photo apps do with AI - now you can do it with your own script via Gemini).</p>
</li>
<li>
<p dir="auto"><strong>OCR and data extraction:</strong> If you have a screenshot of error text or a photo of a document, Gemini can often read the text from it. For example, "Extract the text from <code>invoice.png</code> and put it into a structured format." As shown in a Google Cloud blog example, Gemini CLI can process a set of invoice images and output a table of their <a href="https://medium.com/google-cloud/gemini-cli-tutorial-series-part-4-built-in-tools-c591befa59ba#:~:text=Press%20enter%20or%20click%20to,view%20image%20in%20full%20size" rel="nofollow">info</a>. It basically did OCR + understanding to get invoice numbers, dates, amounts from pictures of invoices. That's an advanced use-case but entirely possible with the multimodal model under the hood.</p>
</li>
<li>
<p dir="auto"><strong>Understanding graphs or charts:</strong> If you have a graph screenshot, you could ask "Explain this chart's key insights <code>@chart.png</code>." It might interpret the axes and trends. Accuracy can vary, but it's a nifty try.</p>
</li>
</ul>
<p dir="auto">To make this practical: when you <code>@image.png</code>, ensure the image isn't too huge (though the model can handle reasonably large images). The CLI will likely encode it and send it to the model. The response might include descriptions or further actions. You can mix text and image references in one prompt too.</p>
<p dir="auto"><strong>Non-image modalities:</strong> The CLI and model potentially can handle PDFs and audio too, by converting them via tools. For example, if you <code>@report.pdf</code>, Gemini CLI might use a PDF-to-text tool under the hood to extract text and then summarize. If you <code>@audio.mp3</code> and ask for a transcript, it might use an audio-to-text tool (like a speech recognition function). The cheat sheet suggests referencing PDFs, audio, video files is <a href="https://www.philschmid.de/gemini-cli-cheatsheet#:~:text=Reference%20files%20or%20directories%20in,PDFs%2C%20audio%2C%20and%20video%20files" rel="nofollow">supported</a>, presumably by invoking appropriate internal tools or APIs. So, "transcribe this interview audio: <code>@interview.wav</code>" could actually work (if not now, likely soon, since underlying Google APIs for speech-to-text could be plugged in).</p>
<p dir="auto"><strong>Rich outputs:</strong> Multimodal also means the AI can return images in responses if integrated (though in CLI it usually won't <em>display</em> them directly, but it could save an image file or output ASCII art, etc.). The MCP capability mentioned that tools can return <a href="https://www.philschmid.de/gemini-cli-cheatsheet#:~:text=Capabilities%3A" rel="nofollow">images</a>. For instance, an AI drawing tool could generate an image and Gemini CLI could present it (maybe by opening it or giving a link).</p>
<p dir="auto"><strong>Important:</strong> The CLI itself is text-based, so you won't <em>see</em> the image in the terminal (unless it's capable of ASCII previews). You'll just get the analysis. So this is mostly about reading images, not displaying them. If you're in VS Code integration, it might show images in the chat view.</p>
<p dir="auto">In summary, <strong>don't forget the "I" in GUI when using Gemini CLI</strong> - it can handle the visual just as well as the textual in many cases. This opens up workflows like visual debugging, design help, data extraction from screenshots, etc., all under the same tool. It's a differentiator that some other CLI tools may not have yet. And as models improve, this multimodal support will only get more powerful, so it's a future-proof skill to exploit.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Tip 19: Customize the <code>$PATH</code> (and Tool Availability) for Stability</h2><a id="user-content-tip-19-customize-the-path-and-tool-availability-for-stability" aria-label="Permalink: Tip 19: Customize the $PATH (and Tool Availability) for Stability" href="#tip-19-customize-the-path-and-tool-availability-for-stability"></a></p>
<p dir="auto"><strong>Quick use-case:</strong> If you ever find Gemini CLI getting confused or invoking the wrong programs, consider running it with a tailored <code>$PATH</code>. By limiting or ordering the available executables, you can prevent the AI from, say, calling a similarly named script that you didn't intend. Essentially, you sandbox its tool access to known-good tools.</p>
<p dir="auto">For most users, this isn't an issue, but for pro users with lots of custom scripts or multiple versions of tools, it can be helpful. One reason mentioned by the developers is avoiding infinite loops or weird <a href="https://github.com/google-gemini/gemini-cli/discussions/7890#:~:text=We%20built%20a%20CLI%20tool,trash%20folder%20for%20manual%20deletion" data-hovercard-type="discussion" data-hovercard-url="/google-gemini/gemini-cli/discussions/7890/hovercard">behavior</a>. For example, if <code>gemini</code> itself is in <code>$PATH</code>, an AI gone awry might recursively call <code>gemini</code> from within Gemini (a strange scenario, but theoretically possible). Or perhaps you have a command named <code>test</code> that conflicts with something - the AI might call the wrong one.</p>
<p dir="auto"><strong>How to set PATH for Gemini:</strong> Easiest is inline on launch:</p>
<div dir="auto" data-snippet-clipboard-copy-content="PATH=/usr/bin:/usr/local/bin gemini"><pre>PATH=/usr/bin:/usr/local/bin gemini</pre></div>
<p dir="auto">This runs Gemini CLI with a restricted <code>$PATH</code> of just those directories. You might exclude directories where experimental or dangerous scripts lie. Alternatively, create a small shell script wrapper that purges or adjusts <code>$PATH</code> then exec's <code>gemini</code>.</p>
<p dir="auto">Another approach is using environment or config to explicitly disable certain tools. For instance, if you absolutely never want the AI to use <code>rm</code> or some destructive tool, you could technically create an alias or dummy <code>rm</code> in a safe <code>$PATH</code> that does nothing (though this could interfere with normal operations, so maybe not that one). A better method is the <strong>exclude list</strong> in settings. In an extension or <code>settings.json</code>, you can exclude tool <a href="https://www.philschmid.de/gemini-cli-cheatsheet#:~:text=" rel="nofollow">names</a>. E.g.,</p>
<div dir="auto" data-snippet-clipboard-copy-content="&quot;excludeTools&quot;: [&quot;run_shell_command&quot;]"><pre><span>"excludeTools"</span>: [<span><span>"</span>run_shell_command<span>"</span></span>]</pre></div>
<p dir="auto">This extreme example would stop <em>all</em> shell commands from running (making Gemini effectively read-only). More granular, there was mention of skipping confirmation for some; similarly you might configure something like:</p>
<div dir="auto" data-snippet-clipboard-copy-content="&quot;tools&quot;: {
  &quot;exclude&quot;: [&quot;apt-get&quot;, &quot;shutdown&quot;]
}"><pre><span>"tools"</span>: {
  <span>"exclude"</span>: [<span><span>"</span>apt-get<span>"</span></span>, <span><span>"</span>shutdown<span>"</span></span>]
}</pre></div>
<p dir="auto"><em>(This syntax is illustrative; consult docs for exact usage.)</em></p>
<p dir="auto">The principle is, by controlling the environment, you reduce risk of the AI doing something dumb with a tool it shouldn't. It's akin to child-proofing the house.</p>
<p dir="auto"><strong>Prevent infinite loops:</strong> One user scenario was a loop where Gemini kept reading its own output or re-reading files <a href="https://support.google.com/gemini/thread/337650803/infinite-loops-with-tool-code-in-answers?hl=en#:~:text=Community%20support,screen%20with%20weird%20scrolling" rel="nofollow">repeatedly</a>. Custom <code>$PATH</code> can't directly fix logic loops, but one cause could be if the AI calls a command that triggers itself. Ensuring it can't accidentally spawn another AI instance (like calling <code>bard</code> or <code>gemini</code> command, if it thought to do so) is good. Removing those from <code>$PATH</code> (or renaming them for that session) helps.</p>
<p dir="auto"><strong>Isolation via sandbox:</strong> Another alternative to messing with <code>$PATH</code> is using <code>--sandbox</code> mode (which uses Docker or Podman to run tools in an isolated <a href="https://www.philschmid.de/gemini-cli-cheatsheet#:~:text=echo%20,gemini" rel="nofollow">environment</a>). In that case, the AI's actions are contained and have only the tools that sandbox image provides. You could supply a Docker image with a curated set of tools. This is heavy-handed but very safe.</p>
<p dir="auto"><strong>Custom PATH for specific tasks:</strong> You might have different <code>$PATH</code> setups for different projects. For example, in one project you want it to use a specific version of Node or a local toolchain. Launching <code>gemini</code> with the <code>$PATH</code> that points to those versions will ensure the AI uses the right one. Essentially, treat Gemini CLI like any user - it uses whatever environment you give it. So if you need it to pick <code>gcc-10</code> vs <code>gcc-12</code>, adjust <code>$PATH</code> or <code>CC</code> env var accordingly.</p>
<p dir="auto"><strong>In summary:</strong> <em>Guard rails.</em> As a power user, you have the ability to fine-tune the operating conditions of the AI. If you ever find a pattern of undesirable behavior tied to tool usage, tweaking <code>$PATH</code> is a quick remedy. For everyday use, you likely won't need this, but it's a pro tip to keep in mind if you integrate Gemini CLI into automation or CI: give it a controlled environment. That way, you know exactly what it can and cannot do, which increases reliability.</p>
<hr>
<p dir="auto"><h2 tabindex="-1" dir="auto">Tip 20: Track and reduce token spend with token caching and stats</h2><a id="user-content-tip-20-track-and-reduce-token-spend-with-token-caching-and-stats" aria-label="Permalink: Tip 20: Track and reduce token spend with token caching and stats" href="#tip-20-track-and-reduce-token-spend-with-token-caching-and-stats"></a></p>
<p dir="auto">If you run long chats or repeatedly attach the same big files, you can cut cost and latency by turning on token caching and monitoring usage. With an API key or Vertex AI auth, Gemini CLI automatically reuses previously sent system instructions and context, so follow‑up requests are cheaper. You can see the savings live in the CLI.</p>
<p dir="auto"><strong>How to use it</strong></p>
<p dir="auto">Use an auth mode that enables caching. Token caching is available when you authenticate with a Gemini API key or Vertex AI. It is not available with OAuth login today. <a href="https://google-gemini.github.io/gemini-cli/docs/cli/token-caching.html" rel="nofollow">Google Gemini</a></p>
<p dir="auto">Inspect your usage and cache hits. Run the <code>stats</code> command during a session. It shows total tokens and a <code>cached</code> field when caching is active.</p>

<p dir="auto">The command's description and cached reporting behavior are documented in the commands reference and FAQ. <a href="https://google-gemini.github.io/gemini-cli/docs/cli/commands.html?utm_source=chatgpt.com" rel="nofollow">Google Gemini+1</a></p>
<p dir="auto">Capture metrics in scripts. When running headless, output JSON and parse the <code>stats</code> block, which includes <code>tokens.cached</code> for each model:</p>
<div dir="auto" data-snippet-clipboard-copy-content="gemini -p &quot;Summarize README&quot; --output-format json"><pre>gemini -p <span><span>"</span>Summarize README<span>"</span></span> --output-format json</pre></div>
<p dir="auto">The headless guide documents the JSON schema with cached token counts. <a href="https://google-gemini.github.io/gemini-cli/docs/cli/headless.html" rel="nofollow">Google Gemini</a></p>
<p dir="auto">Save a session summary to file: For CI or budget tracking, write a JSON session summary to disk.</p>
<div dir="auto" data-snippet-clipboard-copy-content="gemini -p &quot;Analyze logs&quot; --session-summary usage.json"><pre>gemini -p <span><span>"</span>Analyze logs<span>"</span></span> --session-summary usage.json</pre></div>
<p dir="auto">This flag is listed in the changelog. <a href="https://google-gemini.github.io/gemini-cli/docs/changelogs/" rel="nofollow">Google Gemini</a></p>
<p dir="auto">With API key or Vertex auth, the CLI automatically reuses previously sent context so later turns send fewer tokens. Keeping <code>GEMINI.md</code> and large file references stable across turns increases cache hits; you'll see that reflected in stats as cached tokens.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Tip 21: Use <code>/copy</code> for Quick Clipboard Copy</h2><a id="user-content-tip-21-use-copy-for-quick-clipboard-copy" aria-label="Permalink: Tip 21: Use /copy for Quick Clipboard Copy" href="#tip-21-use-copy-for-quick-clipboard-copy"></a></p>
<p dir="auto"><strong>Quick use-case:</strong> Instantly copy the latest answer or code snippet from Gemini CLI to your system clipboard, without any extraneous formatting or line <a href="https://google-gemini.github.io/gemini-cli/docs/cli/commands.html#:~:text=,for%20easy%20sharing%20or%20reuse" rel="nofollow">numbers</a>. This is perfect for quickly pasting AI-generated code into your editor or sharing a result with a teammate.</p>
<p dir="auto">When Gemini CLI provides an answer (especially a multi-line code block), you often want to reuse it elsewhere. The <code>/copy</code> slash command makes this effortless by copying <em>the last output produced by the CLI</em> directly to your <a href="https://google-gemini.github.io/gemini-cli/docs/cli/commands.html#:~:text=,for%20easy%20sharing%20or%20reuse" rel="nofollow">clipboard</a>. Unlike manual selection (which can grab line numbers or prompt text), <code>/copy</code> grabs only the raw response content. For example, if Gemini just generated a 50-line Python script, simply typing <code>/copy</code> will put that entire script into your clipboard, ready to paste - no need to scroll and select text. Under the hood, Gemini CLI uses the appropriate clipboard utility for your platform (e.g. <code>pbcopy</code> on macOS, <code>clip</code> on <a href="https://google-gemini.github.io/gemini-cli/docs/cli/commands.html#:~:text=,clip" rel="nofollow">Windows</a>. Once you run the command, you'll typically see a confirmation message, and then you can paste the copied text wherever you need it.</p>
<p dir="auto"><strong>How it works:</strong> The <code>/copy</code> command requires that your system has a clipboard tool <a href="https://google-gemini.github.io/gemini-cli/docs/cli/commands.html#:~:text=,clip" rel="nofollow">available</a>. On macOS and Windows, the required tools (<code>pbcopy</code> and <code>clip</code> respectively) are usually pre-installed. On Linux, you may need to install <code>xclip</code> or <code>xsel</code> for <code>/copy</code> to <a href="https://google-gemini.github.io/gemini-cli/docs/cli/commands.html#:~:text=,clip" rel="nofollow">function</a>. After ensuring that, you can use <code>/copy</code> anytime after Gemini CLI prints an answer. It will capture the <em>entire</em> last response (even if it's long) and omit any internal numbering or formatting the CLI may show on-screen. This saves you from dealing with unwanted artifacts when transferring the content. It's a small feature, but a huge time-saver when you're iterating on code or compiling a report generated by the AI.</p>
<p dir="auto"><strong>Pro Tip:</strong> If you find the <code>/copy</code> command isn't working, double-check that your clipboard utilities are installed and accessible. For instance, Ubuntu users should run <code>sudo apt install xclip</code> to enable clipboard <a href="https://google-gemini.github.io/gemini-cli/docs/cli/commands.html#:~:text=,clip" rel="nofollow">copying</a>. Once set up, <code>/copy</code> lets you share Gemini's outputs with zero friction - copy, paste, and you're done.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Tip 22: Master <code>Ctrl+C</code> for Shell Mode and Exiting</h2><a id="user-content-tip-22-master-ctrlc-for-shell-mode-and-exiting" aria-label="Permalink: Tip 22: Master Ctrl+C for Shell Mode and Exiting" href="#tip-22-master-ctrlc-for-shell-mode-and-exiting"></a></p>
<p dir="auto"><strong>Quick use-case:</strong> Cleanly interrupt Gemini CLI or exit shell mode with a single keypress - and quit the CLI entirely with a quick double-tap - thanks to the versatile <strong>Ctrl+C</strong> <a href="https://www.howtouselinux.com/post/the-complete-google-gemini-cli-cheat-sheet-and-guide#:~:text=Shortcut%20Description%20,Press%20twice%20to%20confirm" rel="nofollow">shortcut</a>. This gives you immediate control when you need to stop or exit.</p>
<p dir="auto">Gemini CLI operates like a REPL, and knowing how to break out of operations is essential. Pressing <strong>Ctrl+C</strong> once will cancel the current action or clear any input you've started typing, essentially acting as an "abort" <a href="https://www.howtouselinux.com/post/the-complete-google-gemini-cli-cheat-sheet-and-guide#:~:text=Shortcut%20Description%20,Press%20twice%20to%20confirm" rel="nofollow">command</a>. For example, if the AI is generating a lengthy answer and you've seen enough, hit <code>Ctrl+C</code> - the generation stops immediately. If you had started typing a prompt but want to discard it, <code>Ctrl+C</code> will wipe the input line so you can start <a href="https://www.howtouselinux.com/post/the-complete-google-gemini-cli-cheat-sheet-and-guide#:~:text=Shortcut%20Description%20,Press%20twice%20to%20confirm" rel="nofollow">fresh</a>. Additionally, if you are in <strong>shell mode</strong> (activated by typing <code>!</code> to run shell commands), a single <code>Ctrl+C</code> will exit shell mode and return you to the normal Gemini prompt (it sends an interrupt to the shell process <a href="https://milvus.io/ai-quick-reference/how-do-i-use-gemini-cli-for-shell-command-generation#:~:text=The%20shell%20integration%20also%20includes,where%20you%20can%20generate%20commands" rel="nofollow">running</a>. This is extremely handy if a shell command is hanging or you simply want to get back to AI mode.</p>
<p dir="auto">Pressing <strong>Ctrl+C twice</strong> in a row is the shortcut to exit Gemini CLI <a href="https://www.howtouselinux.com/post/the-complete-google-gemini-cli-cheat-sheet-and-guide#:~:text=Shortcut%20Description%20,Press%20twice%20to%20confirm" rel="nofollow">entirely</a>. Think of it as "<code>Ctrl+C</code> to cancel, and <code>Ctrl+C</code> again to quit." This double-tap signals the CLI to terminate the session (you'll see a goodbye message or the program will close). It's a faster alternative to typing <code>/quit</code> or closing the terminal window, allowing you to gracefully shut down the CLI from the keyboard. Do note that a single <code>Ctrl+C</code> will not quit if there's input to clear or an operation to interrupt - it requires that second press (when the prompt is idle) to fully <a href="https://www.howtouselinux.com/post/the-complete-google-gemini-cli-cheat-sheet-and-guide#:~:text=Shortcut%20Description%20,Press%20twice%20to%20confirm" rel="nofollow">exit</a>. This design prevents accidentally closing the session when you only meant to stop the current output.</p>
<p dir="auto"><strong>Pro Tip:</strong> In shell mode, you can also press the <strong>Esc</strong> key to leave shell mode and return to Gemini's chat mode without terminating the <a href="https://milvus.io/ai-quick-reference/how-do-i-use-gemini-cli-for-shell-command-generation#:~:text=The%20shell%20integration%20also%20includes,where%20you%20can%20generate%20commands" rel="nofollow">CLI</a>. And if you prefer a more formal exit, the <code>/quit</code> command is always available to cleanly end the session. Lastly, Unix users can use <strong>Ctrl+D</strong> (EOF) at an empty prompt to exit as well - Gemini CLI will prompt for confirmation if <a href="https://www.howtouselinux.com/post/the-complete-google-gemini-cli-cheat-sheet-and-guide#:~:text=Shortcut%20Description%20,Press%20twice%20to%20confirm" rel="nofollow">needed</a>. But for most cases, mastering the single- and double-tap of <code>Ctrl+C</code> is the quickest way to stay in control.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Tip 23: Customize Gemini CLI with <code>settings.json</code></h2><a id="user-content-tip-23-customize-gemini-cli-with-settingsjson" aria-label="Permalink: Tip 23: Customize Gemini CLI with settings.json" href="#tip-23-customize-gemini-cli-with-settingsjson"></a></p>
<p dir="auto"><strong>Quick use-case:</strong> Adapt the CLI's behavior and appearance to your preferences or project conventions by editing the <code>settings.json</code> config file, instead of sticking with one-size-fits-all <a href="https://www.philschmid.de/gemini-cli-cheatsheet#:~:text=%2A%20%60autoAccept%60%3A%20Auto,to%20disable%20usage%20statistics" rel="nofollow">defaults</a>. This lets you enforce things like theme, tool usage rules, or editor mode across all your sessions.</p>
<p dir="auto">Gemini CLI is highly configurable. In your home directory (<code>~/.gemini/</code>) or project folder (<code>.gemini/</code> within your repo), you can create a <code>settings.json</code> file to override default <a href="https://www.philschmid.de/gemini-cli-cheatsheet#:~:text=Customize%20the%20CLI%20by%20creating,applied%20with%20the%20following%20precedence" rel="nofollow">settings</a>. Nearly every aspect of the CLI can be tuned here - from visual theme to tool permissions. The CLI merges settings from multiple levels: system-wide defaults, your user settings, and project-specific settings (project settings override user <a href="https://www.philschmid.de/gemini-cli-cheatsheet#:~:text=Customize%20the%20CLI%20by%20creating,applied%20with%20the%20following%20precedence" rel="nofollow">settings</a>. For example, you might have a global preference for a dark theme, but a particular project might require stricter tool sandboxing; you can handle this via different <code>settings.json</code> files at each level.</p>
<p dir="auto">Inside <code>settings.json</code>, options are specified as JSON key-value pairs. Here's a snippet illustrating some useful customizations:</p>
<div dir="auto" data-snippet-clipboard-copy-content="{
&quot;theme&quot;: &quot;GitHub&quot;,
&quot;autoAccept&quot;: false,
&quot;vimMode&quot;: true,
&quot;sandbox&quot;: &quot;docker&quot;,
&quot;includeDirectories&quot;: [&quot;../shared-library&quot;, &quot;~/common-utils&quot;],
&quot;usageStatisticsEnabled&quot;: true
}"><pre>{
<span>"theme"</span>: <span><span>"</span>GitHub<span>"</span></span>,
<span>"autoAccept"</span>: <span>false</span>,
<span>"vimMode"</span>: <span>true</span>,
<span>"sandbox"</span>: <span><span>"</span>docker<span>"</span></span>,
<span>"includeDirectories"</span>: [<span><span>"</span>../shared-library<span>"</span></span>, <span><span>"</span>~/common-utils<span>"</span></span>],
<span>"usageStatisticsEnabled"</span>: <span>true</span>
}</pre></div>
<p dir="auto">In this example, we set the theme to "GitHub" (a popular color scheme), disable <code>autoAccept</code> (so the CLI will always ask before running potentially altering tools), enable Vim keybindings for the input editor, and enforce using Docker for tool sandboxing. We also added some directories to the workspace context (<code>includeDirectories</code>) so Gemini can see code in shared paths by <a href="https://www.philschmid.de/gemini-cli-cheatsheet#:~:text=%7B%20,utils" rel="nofollow">default</a>. Finally, we kept <code>usageStatisticsEnabled</code> true to collect basic usage stats (which feeds into telemetry, if <a href="https://www.philschmid.de/gemini-cli-cheatsheet#:~:text=%2A%20%60autoAccept%60%3A%20Auto,to%20disable%20usage%20statistics" rel="nofollow">enabled</a>. There are many more settings available - like defining custom color themes, adjusting token limits, or whitelisting/blacklisting specific tools - all documented in the configuration <a href="https://www.philschmid.de/gemini-cli-cheatsheet#:~:text=%2A%20%60autoAccept%60%3A%20Auto,to%20disable%20usage%20statistics" rel="nofollow">guide</a>. By tailoring these, you ensure Gemini CLI behaves optimally for <em>your</em> workflow (for instance, some developers always want <code>vimMode</code> on for efficiency, while others might prefer the default editor).</p>
<p dir="auto">One convenient way to edit settings is via the built-in settings UI. Run the command <code>/settings</code> in Gemini CLI, and it will open an interactive editor for your <a href="https://google-gemini.github.io/gemini-cli/docs/cli/commands.html#:~:text=,their%20current%20values%2C%20and%20modify" rel="nofollow">configuration</a>. This interface lets you browse and search settings with descriptions, and prevents JSON syntax errors by validating inputs. You can tweak colors, toggle features like <code>yolo</code> (auto-approval), adjust checkpointing (file save/restore behavior), and more through a friendly <a href="https://google-gemini.github.io/gemini-cli/docs/cli/commands.html#:~:text=,their%20current%20values%2C%20and%20modify" rel="nofollow">menu</a>. Changes are saved to your <code>settings.json</code>, and some take effect immediately (others might require restarting the CLI).</p>
<p dir="auto"><strong>Pro Tip:</strong> Maintain separate project-specific <code>settings.json</code> files for different needs. For example, on a team project you might set <code>"sandbox": "docker"</code> and <code>"excludeTools": ["run_shell_command"]</code> to lock down dangerous operations, while your personal projects might allow direct shell commands. Gemini CLI will automatically pick up the nearest <code>.gemini/settings.json</code> in your project directory tree and merge it with your global <a href="https://www.philschmid.de/gemini-cli-cheatsheet#:~:text=Customize%20the%20CLI%20by%20creating,applied%20with%20the%20following%20precedence" rel="nofollow"><code>~/.gemini/settings.json</code></a>. Also, don't forget you can quickly adjust visual preferences: try <code>/theme</code> to interactively switch themes without editing the file, which is great for finding a comfortable <a href="https://www.howtouselinux.com/post/the-complete-google-gemini-cli-cheat-sheet-and-guide#:~:text=Command%20Description%20,tag%3E%60Save%20the%20current%20conversation" rel="nofollow">look</a>. Once you find one, put it in <code>settings.json</code> to make it permanent.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Tip 24: Leverage IDE Integration (VS Code) for Context &amp; Diffs</h2><a id="user-content-tip-24-leverage-ide-integration-vs-code-for-context--diffs" aria-label="Permalink: Tip 24: Leverage IDE Integration (VS Code) for Context &amp; Diffs" href="#tip-24-leverage-ide-integration-vs-code-for-context--diffs"></a></p>
<p dir="auto"><strong>Quick use-case:</strong> Supercharge Gemini CLI by hooking it into VS Code - the CLI will automatically know which files you're working on and even open AI-proposed code changes in VS Code's diff editor for <a href="https://developers.googleblog.com/en/gemini-cli-vs-code-native-diffing-context-aware-workflows/?source=post_page-----26afd3422028---------------------------------------#:~:text=,working%20on%20at%20the%20moment" rel="nofollow">you</a>. This creates a seamless loop between AI assistant and your coding workspace.</p>
<p dir="auto">One of Gemini CLI's powerful features is its <strong>IDE integration</strong> with Visual Studio Code. By installing the official <em>Gemini CLI Companion</em> extension in VS Code and connecting it, you allow Gemini CLI to become "context-aware" of your <a href="https://developers.googleblog.com/en/gemini-cli-vs-code-native-diffing-context-aware-workflows/?source=post_page-----26afd3422028---------------------------------------#:~:text=,working%20on%20at%20the%20moment" rel="nofollow">editor</a>. What does this mean in practice? When connected, Gemini knows about the files you have open, your current cursor location, and any text you've selected in VS <a href="https://developers.googleblog.com/en/gemini-cli-vs-code-native-diffing-context-aware-workflows/?source=post_page-----26afd3422028---------------------------------------#:~:text=,working%20on%20at%20the%20moment" rel="nofollow">Code</a>. All that information is fed into the AI's context. So if you ask, "Explain this function," Gemini CLI can see the exact function you've highlighted and give a relevant answer, without you needing to copy-paste code into the prompt. The integration shares up to your 10 most recently opened files, plus selection and cursor info, giving the model a rich understanding of your <a href="https://gemini-cli.xyz/docs/en/ide-integration#:~:text=,reject%20the%20suggested%20changes%20seamlessly" rel="nofollow">workspace</a>.</p>
<p dir="auto">Another huge benefit is <strong>native diffing</strong> of code changes. When Gemini CLI suggests modifications to your code (for example, "refactor this function" and it produces a patch), it can open those changes in VS Code's diff viewer <a href="https://developers.googleblog.com/en/gemini-cli-vs-code-native-diffing-context-aware-workflows/?source=post_page-----26afd3422028---------------------------------------#:~:text=%2A%20Native%20in,the%20code%20right%20within%20this" rel="nofollow">automatically</a>. You'll see a side-by-side diff in VS Code showing the proposed edits. You can then use VS Code's familiar interface to review the changes, make any manual tweaks, and even accept the patch with a click. The CLI and editor stay in sync - if you accept the diff in VS Code, Gemini CLI knows and continues the session with those changes applied. This tight loop means you no longer have to copy code from the terminal to your editor; the AI's suggestions flow straight into your development environment.</p>
<p dir="auto"><strong>How to set it up:</strong> If you start Gemini CLI inside VS Code's integrated terminal, it will detect VS Code and usually prompt you to install/connect the extension <a href="https://medium.com/google-cloud/gemini-cli-tutorial-series-part-10-gemini-cli-vs-code-integration-26afd3422028#:~:text=Press%20enter%20or%20click%20to,view%20image%20in%20full%20size" rel="nofollow">automatically</a>. You can agree and it will run the necessary <code>/ide install</code> step. If you don't see a prompt (or you're enabling it later), simply open Gemini CLI and run the command: <code>/ide install</code>. This will fetch and install the "Gemini CLI Companion" extension into VS Code for <a href="https://developers.googleblog.com/en/gemini-cli-vs-code-native-diffing-context-aware-workflows/?source=post_page-----26afd3422028---------------------------------------#:~:text=2%3A%20One,install%20the%20necessary%20companion%20extension" rel="nofollow">you</a>. Next, run <code>/ide enable</code> to establish the <a href="https://developers.googleblog.com/en/gemini-cli-vs-code-native-diffing-context-aware-workflows/?source=post_page-----26afd3422028---------------------------------------#:~:text=3%3A%20Toggle%20integration%3A%20After%20the,can%20easily%20manage%20the%20integration" rel="nofollow">connection</a> - the CLI will then indicate it's linked to VS Code. You can verify at any time with <code>/ide status</code>, which will show if it's connected and list which editor and files are being <a href="https://gemini-cli.xyz/docs/en/ide-integration#:~:text=Checking%20the%20Status" rel="nofollow">tracked</a>. From then on, Gemini CLI will automatically receive context from VS Code (open files, selections) and will open diffs in VS Code when needed. It essentially turns Gemini CLI into an AI pair programmer that lives in your terminal but operates with full awareness of your IDE.</p>
<p dir="auto">Currently, VS Code is the primary supported editor for this <a href="https://gemini-cli.xyz/docs/en/ide-integration#:~:text=better%20and%20enables%20powerful%20features,editor%20diffing" rel="nofollow">integration</a>. (Other editors that support VS Code extensions, like VSCodium or some JetBrains via a plugin, may work via the same extension, but officially it's VS Code for now.) The design is open though - there's an IDE Companion Spec for developing similar integrations with other <a href="https://gemini-cli.xyz/docs/en/ide-integration#:~:text=better%20and%20enables%20powerful%20features,editor%20diffing" rel="nofollow">editors</a>. So down the road we might see first-class support for IDEs like IntelliJ or Vim via community extensions.</p>
<p dir="auto"><strong>Pro Tip:</strong> Once connected, you can use VS Code's Command Palette to control Gemini CLI without leaving the <a href="https://gemini-cli.xyz/docs/en/ide-integration#:~:text=,Ctrl%2BShift%2BP" rel="nofollow">editor</a>. For example, press <strong>Ctrl+Shift+P</strong> (Cmd+Shift+P on Mac) and try commands like <strong>"Gemini CLI: Run"</strong> (to launch a new CLI session in the terminal), <strong>"Gemini CLI: Accept Diff"</strong> (to approve and apply an open diff), or <strong>"Gemini CLI: Close Diff Editor"</strong> (to reject <a href="https://gemini-cli.xyz/docs/en/ide-integration#:~:text=,Ctrl%2BShift%2BP" rel="nofollow">changes</a>. These shortcuts can streamline your workflow even further. And remember, you don't always have to start the CLI manually - if you enable the integration, Gemini CLI essentially becomes an AI co-developer inside VS Code, watching context and ready to help as you work on code.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Tip 25: Automate Repo Tasks with <code>Gemini CLI GitHub Action</code></h2><a id="user-content-tip-25-automate-repo-tasks-with-gemini-cli-github-action" aria-label="Permalink: Tip 25: Automate Repo Tasks with Gemini CLI GitHub Action" href="#tip-25-automate-repo-tasks-with-gemini-cli-github-action"></a></p>
<p dir="auto"><strong>Quick use-case:</strong> Put Gemini to work on GitHub - use the <strong>Gemini CLI GitHub Action</strong> to autonomously triage new issues and review pull requests in your repository, acting as an AI teammate that handles routine dev <a href="https://blog.google/technology/developers/introducing-gemini-cli-github-actions/#:~:text=1,write%20tests%20for%20this" rel="nofollow">tasks</a>.</p>
<p dir="auto">Gemini CLI isn't just for interactive terminal sessions; it can also run in CI/CD pipelines via GitHub Actions. Google has provided a ready-made <strong>Gemini CLI GitHub Action</strong> (currently in beta) that integrates into your repo's <a href="https://blog.google/technology/developers/introducing-gemini-cli-github-actions/#:~:text=It%E2%80%99s%20now%20in%20beta%2C%20available,cli" rel="nofollow">workflows</a>. This effectively deploys an AI agent into your project on GitHub. It runs in the background, triggered by repository <a href="https://blog.google/technology/developers/introducing-gemini-cli-github-actions/#:~:text=Triggered%20by%20events%20like%20new,do%2C%20and%20gets%20it%20done" rel="nofollow">events</a>. For example, when someone opens a <strong>new issue</strong>, the Gemini Action can automatically analyze the issue description, apply relevant labels, and even prioritize it or suggest duplicates (this is the "intelligent issue triage" <a href="https://blog.google/technology/developers/introducing-gemini-cli-github-actions/#:~:text=1,attention%20on%20what%20matters%20most" rel="nofollow">workflow</a>. When a <strong>pull request</strong> is opened, the Action kicks in to provide an <strong>AI code review</strong> - it will comment on the PR with insights about code quality, potential bugs, or stylistic <a href="https://blog.google/technology/developers/introducing-gemini-cli-github-actions/#:~:text=attention%20on%20what%20matters%20most,more%20complex%20tasks%20and%20decisions" rel="nofollow">improvements</a>. This gives maintainers immediate feedback on the PR before any human even looks at it. Perhaps the coolest feature is <strong>on-demand collaboration</strong>: team members can mention <code>@gemini-cli</code> in an issue or PR comment and give it an instruction, like "<code>@gemini-cli</code> please write unit tests for this". The Action will pick that up and Gemini CLI will attempt to fulfill the request (adding a commit with new tests, for <a href="https://blog.google/technology/developers/introducing-gemini-cli-github-actions/#:~:text=freeing%20up%20reviewers%20to%20focus,write%20tests%20for%20this" rel="nofollow">instance</a>. It's like having an AI assistant living in your repo, ready to do chores when asked.</p>
<p dir="auto">Setting up the Gemini CLI GitHub Action is straightforward. First, ensure you have Gemini CLI version <strong>0.1.18 or later</strong> installed locally (this ensures compatibility with the <a href="https://blog.google/technology/developers/introducing-gemini-cli-github-actions/#:~:text=Gemini%20CLI%20GitHub%20Actions%20is,for%20individual%20users%20available%20soon" rel="nofollow">Action</a>. Then, in Gemini CLI run the special command: <a href="https://blog.google/technology/developers/introducing-gemini-cli-github-actions/#:~:text=To%20get%20started%2C%20download%20Gemini,cli" rel="nofollow"><code>/setup-github</code></a>. This command generates the necessary workflow files in your repository (it will guide you through authentication if needed). Specifically, it adds YAML workflow files (for issue triage, PR review, etc.) under <code>.github/workflows/</code>. You will need to add your Gemini API key to the repo's secrets (as <code>GEMINI_API_KEY</code>) so the Action can use the Gemini <a href="https://github.com/google-github-actions/run-gemini-cli#:~:text=Store%20your%20API%20key%20as,in%20your%20repository">API</a>. Once that's done and the workflows are committed, the GitHub Action springs to life - from that point on, Gemini CLI will autonomously respond to new issues and PRs according to those workflows.</p>
<p dir="auto">Because this Action is essentially running Gemini CLI in an automated way, you can customize it just like you would your CLI. The default setup comes with three workflows (issue triage, PR review, and a general mention-triggered assistant) which are **fully open-source and <a href="https://blog.google/technology/developers/introducing-gemini-cli-github-actions/#:~:text=Think%20of%20these%20initial%20workflows,into%20Gemini%20CLI%20GitHub%20Actions" rel="nofollow">editable**</a>. You can tweak the YAML to adjust what the AI does, or even add new workflows. For instance, you might create a nightly workflow that uses Gemini CLI to scan your repository for outdated dependencies or to update a README based on recent code changes - the possibilities are endless. The key benefit here is offloading mundane or time-consuming tasks to an AI agent so that human developers can focus on harder problems. And since it runs on GitHub's infrastructure, it doesn't require your intervention - it's truly a "set and forget" AI helper.</p>
<p dir="auto"><strong>Pro Tip:</strong> Keep an eye on the Action's output in the GitHub Actions logs for transparency. The Gemini CLI Action logs will show what prompts it ran and what changes it made or suggested. This can both build trust and help you refine its behavior. Also, the team has built enterprise-grade safeguards into the Action - e.g., you can require that all shell commands the AI tries to run in a workflow are allow-listed by <a href="https://blog.google/technology/developers/introducing-gemini-cli-github-actions/#:~:text=in%20your%20environment%2C%20drastically%20reducing,your%20preferred%20observability%20platform%2C%20like" rel="nofollow">you</a>. So don't hesitate to use it even on serious projects. And if you come up with a cool custom workflow using Gemini CLI, consider contributing it back to the community - the project welcomes new ideas in their repo!</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Tip 26: Enable Telemetry for Insights and Observability</h2><a id="user-content-tip-26-enable-telemetry-for-insights-and-observability" aria-label="Permalink: Tip 26: Enable Telemetry for Insights and Observability" href="#tip-26-enable-telemetry-for-insights-and-observability"></a></p>
<p dir="auto"><strong>Quick use-case:</strong> Gain deeper insight into how Gemini CLI is being used and performing by turning on its built-in <strong>OpenTelemetry</strong> instrumentation - monitor metrics, logs, and traces of your AI sessions to analyze usage patterns or troubleshoot <a href="https://google-gemini.github.io/gemini-cli/docs/cli/telemetry.html#:~:text=,across%20teams%2C%20track%20costs%2C%20ensure" rel="nofollow">issues</a>.</p>
<p dir="auto">For developers who like to measure and optimize, Gemini CLI offers an observability feature that exposes what's happening under the hood. By leveraging <strong>OpenTelemetry (OTEL)</strong>, Gemini CLI can emit structured telemetry data about your <a href="https://google-gemini.github.io/gemini-cli/docs/cli/telemetry.html#:~:text=Built%20on%20OpenTelemetry%20%E2%80%94%20the,Gemini%20CLI%E2%80%99s%20observability%20system%20provides" rel="nofollow">sessions</a>. This includes things like metrics (e.g. how many tokens used, response latency), logs of actions taken, and even traces of tool calls. With telemetry enabled, you can answer questions like: <em>Which custom command do I use most often? How many times did the AI edit files in this project this week? What's the average response time when I ask the CLI to run tests?</em> Such data is invaluable for understanding usage patterns and <a href="https://google-gemini.github.io/gemini-cli/docs/cli/telemetry.html#:~:text=,across%20teams%2C%20track%20costs%2C%20ensure" rel="nofollow">performance</a>. Teams can use it to see how developers are interacting with the AI assistant and where bottlenecks might be.</p>
<p dir="auto">By default, telemetry is <strong>off</strong> (Gemini respects privacy and performance). You can opt-in by setting <code>"telemetry.enabled": true</code> in your <code>settings.json</code> or by starting Gemini CLI with the flag <a href="https://google-gemini.github.io/gemini-cli/docs/cli/telemetry.html#:~:text=Setting%20Environment%20Variable%20CLI%20Flag,grpc" rel="nofollow"><code>--telemetry</code></a>. Additionally, you choose the <strong>target</strong> for the telemetry data: it can be logged <strong>locally</strong> or sent to a backend like Google Cloud. For a quick start, you might set <code>"telemetry.target": "local"</code> - with this, Gemini will simply write telemetry data to a local file (by default) or to a custom path you specify via <code>["outfile"](https://google-gemini.github.io/gemini-cli/docs/cli/telemetry.html#:~:text=disable%20telemetry%20,file%20path)</code>. The local telemetry includes JSON logs you can parse or feed into tools. For more robust monitoring, set <code>"target": "gcp"</code> (Google Cloud) or even integrate with other OpenTelemetry-compatible systems like Jaeger or <a href="https://google-gemini.github.io/gemini-cli/docs/cli/telemetry.html#:~:text=,between%20backends%20without%20changing%20your" rel="nofollow">Datadog</a>. In fact, Gemini CLI's OTEL support is vendor-neutral - you can export data to just about any observability stack you prefer (Google Cloud Operations, Prometheus, <a href="https://google-gemini.github.io/gemini-cli/docs/cli/telemetry.html#:~:text=,between%20backends%20without%20changing%20your" rel="nofollow">etc.</a>. Google provides a streamlined path for Cloud: if you point to GCP, the CLI can send data directly to Cloud Logging and Cloud Monitoring in your project, where you can use the usual dashboards and alerting <a href="https://google-gemini.github.io/gemini-cli/docs/cli/telemetry.html#:~:text=2,explorer%20%2A%20Traces%3A%20https%3A%2F%2Fconsole.cloud.google.com%2Ftraces%2Flist" rel="nofollow">tools</a>.</p>
<p dir="auto">What kind of insights can you get? The telemetry captures events like tool executions, errors, and important milestones. It also records metrics such as prompt processing time and token counts per <a href="https://medium.com/google-cloud/gemini-cli-tutorial-series-part-13-gemini-cli-observability-c410806bc112#:~:text=,integrate%20with%20existing%20monitoring%20infrastructure" rel="nofollow">prompt</a>. For usage analytics, you might aggregate how many times each slash command is used across your team, or how often code generation is invoked. For performance monitoring, you could track if responses have gotten slower, which might indicate hitting API rate limits or model changes. And for debugging, you can see errors or exceptions thrown by tools (e.g., a <code>run_shell_command</code> failure) logged with context. All this data can be visualized if you send it to a platform like Google Cloud's Monitoring - for example, you can create a dashboard of "tokens used per day" or "error rate of tool X". It essentially gives you a window into the AI's "brain" and your usage, which is especially helpful in enterprise settings to ensure everything runs <a href="https://medium.com/google-cloud/gemini-cli-tutorial-series-part-13-gemini-cli-observability-c410806bc112#:~:text=resource%20utilization%20%2A%20%20Real,integrate%20with%20existing%20monitoring%20infrastructure" rel="nofollow">smoothly</a>.</p>
<p dir="auto">Enabling telemetry does introduce some overhead (extra data processing), so you might not keep it on 100% of the time for personal use. However, it's fantastic for debugging sessions or for intermittent health checks. One approach is to enable it on a CI server or in your team's shared environment to collect stats, while leaving it off locally unless needed. Remember, you can always toggle it on the fly: update settings and use <code>/memory refresh</code> if needed to reload, or restart Gemini CLI with <code>--telemetry</code> flag. Also, all telemetry is under your control - it respects your environment variables for endpoint and credentials, so data goes only where you intend it to. This feature turns Gemini CLI from a black box into an observatory, shining light on how the AI agent interacts with your world, so you can continuously improve that interaction.</p>
<p dir="auto"><strong>Pro Tip:</strong> If you just want a quick view of your current session's stats (without full telemetry), use the <code>/stats</code> command. It will output metrics like token usage and session length right in the <a href="https://www.howtouselinux.com/post/the-complete-google-gemini-cli-cheat-sheet-and-guide#:~:text=Command%20Description%20,tag%3E%60Save%20the%20current%20conversation" rel="nofollow">CLI</a>. This is a lightweight way to see immediate numbers. But for long-term or multi-session analysis, telemetry is the way to go. And if you're sending telemetry to a cloud project, consider setting up dashboards or alerts (e.g., alert if error rate spikes or token usage hits a threshold) - this can proactively catch issues in how Gemini CLI is being used in your team.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Tip 27: Keep an Eye on the Roadmap (Background Agents &amp; More)</h2><a id="user-content-tip-27-keep-an-eye-on-the-roadmap-background-agents--more" aria-label="Permalink: Tip 27: Keep an Eye on the Roadmap (Background Agents &amp; More)" href="#tip-27-keep-an-eye-on-the-roadmap-background-agents--more"></a></p>
<p dir="auto"><strong>Quick use-case:</strong> Stay informed about upcoming Gemini CLI features - by following the public <strong>Gemini CLI roadmap</strong>, you'll know about major planned enhancements (like <em>background agents for long-running tasks</em>) before they <a href="https://google-gemini.github.io/gemini-cli/ROADMAP.html#:~:text=quality.%20,related%20to%20security%20and%20privacy" rel="nofollow">arrive</a>, allowing you to plan and give feedback.</p>
<p dir="auto">Gemini CLI is evolving rapidly, with new releases coming out frequently, so it's wise to track what's on the horizon. Google maintains a <strong>public roadmap</strong> for Gemini CLI on GitHub, detailing the key focus areas and features targeted for the near <a href="https://google-gemini.github.io/gemini-cli/ROADMAP.html#:~:text=This%20document%20outlines%20our%20approach,live%20in%20our%20GitHub%20Issues" rel="nofollow">future</a>. This is essentially a living document (and set of issues) where you can see what the developers are working on and what's in the pipeline. For instance, one exciting item on the roadmap is support for <strong>background agents</strong> - the ability to spawn autonomous agents that run in the background to handle tasks continuously or <a href="https://google-gemini.github.io/gemini-cli/ROADMAP.html#:~:text=quality.%20,related%20to%20security%20and%20privacy" rel="nofollow">asynchronously</a>. According to the roadmap discussion, these background agents would let you delegate long-running processes to Gemini CLI without tying up your interactive session. You could, say, start a background agent that monitors your project for certain events or periodically executes tasks, either on your local machine or even by deploying to a service like Cloud <a href="https://github.com/google-gemini/gemini-cli/issues/4168#:~:text=How%20will%20it%20work%3F" data-hovercard-type="issue" data-hovercard-url="/google-gemini/gemini-cli/issues/4168/hovercard">Run</a>. This feature aims to "enable long-running, autonomous tasks and proactive assistance" right from the <a href="https://google-gemini.github.io/gemini-cli/ROADMAP.html#:~:text=quality.%20,related%20to%20security%20and%20privacy" rel="nofollow">CLI</a>, essentially extending Gemini CLI's usefulness beyond just on-demand queries.</p>
<p dir="auto">By keeping tabs on the roadmap, you'll also learn about other planned features. These could include new tool integrations, support for additional Gemini model versions, UI/UX improvements, and more. The roadmap is usually organized by "areas" (for example, <em>Extensibility</em>, <em>Model</em>, <em>Background</em>, etc.) and often tagged with milestones (like a target quarter for <a href="https://google-gemini.github.io/gemini-cli/ROADMAP.html#:~:text=Our%20roadmap%20is%20managed%20directly,more%20detailed%20list%20of%20tasks" rel="nofollow">delivery</a>]. It's not a guarantee of when something will land, but it gives a good idea of the team's priorities. Since the project is open-source, you can even dive into the linked GitHub issues for each roadmap item to see design proposals and progress. For developers who rely on Gemini CLI, this transparency means you can anticipate changes - maybe an API is adding a feature you need, or a breaking change might be coming that you want to prepare for.</p>
<p dir="auto">Following the roadmap can be as simple as bookmarking the GitHub project board or issue labeled "Roadmap" and checking periodically. Some major updates (like the introduction of Extensions or the IDE integration) were hinted at in the roadmap before they were officially announced, so you get a sneak peek. Additionally, the Gemini CLI team often encourages community feedback on those future features. If you have ideas or use cases for something like background agents, you can usually comment on the issue or discussion thread to influence its development.</p>
<p dir="auto"><strong>Pro Tip:</strong> Since Gemini CLI is open source (Apache 2.0 licensed), you can do more than just watch the roadmap - you can participate! The maintainers welcome contributions, especially for items aligned with the <a href="https://google-gemini.github.io/gemini-cli/ROADMAP.html#:~:text=As%20an%20Apache%202,opening%20an%20issue%20for%20discussion" rel="nofollow">roadmap</a>. If there's a feature you really care about, consider contributing code or testing once it's in preview. At the very least, you can open a feature request if something you need isn't on the roadmap <a href="https://google-gemini.github.io/gemini-cli/ROADMAP.html#:~:text=As%20an%20Apache%202,opening%20an%20issue%20for%20discussion" rel="nofollow">yet</a>. The roadmap page itself provides guidance on how to propose changes. Engaging with the project not only keeps you in the loop but also lets you shape the tool that you use. After all, Gemini CLI is built with community involvement in mind, and many recent features (like certain extensions and tools) started as community suggestions.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Tip 28: Extend Gemini CLI with <code>Extensions</code></h2><a id="user-content-tip-28-extend-gemini-cli-with-extensions" aria-label="Permalink: Tip 28: Extend Gemini CLI with Extensions" href="#tip-28-extend-gemini-cli-with-extensions"></a></p>
<p dir="auto"><strong>Quick use-case:</strong> Add new capabilities to Gemini CLI by installing plug-and-play <strong>extensions</strong> - for example, integrate with your favorite database or cloud service - expanding the AI's toolset without any heavy lifting on your <a href="https://blog.google/technology/developers/gemini-cli-extensions/#:~:text=Gemini%20CLI%20is%20an%20open,design%20platforms%20to%20payment%20services" rel="nofollow">part</a>. It's like installing apps for your CLI to teach it new tricks.</p>
<p dir="auto">Extensions are a game-changer introduced in late 2025: they allow you to <strong>customize and expand</strong> Gemini CLI's functionality in a modular <a href="https://blog.google/technology/developers/gemini-cli-extensions/#:~:text=Gemini%20CLI%20is%20an%20open,design%20platforms%20to%20payment%20services" rel="nofollow">way</a>. An extension is essentially a bundle of configurations (and optionally code) that connects Gemini CLI to an external tool or service. For instance, Google released a suite of extensions for Google Cloud - there's one that helps deploy apps to Cloud Run, one for managing BigQuery, one for analyzing application security, and <a href="https://blog.google/technology/developers/gemini-cli-extensions/#:~:text=In%20just%20three%20months%20since,source%20community" rel="nofollow">more</a>. Partners and community developers have built extensions for all sorts of things: Dynatrace (monitoring), Elastic (search analytics), Figma (design assets), Shopify, Snyk (security scans), Stripe (payments), and the list is <a href="https://blog.google/technology/developers/gemini-cli-extensions/#:~:text=In%20just%20three%20months%20since,source%20community" rel="nofollow">growing</a>. By installing an appropriate extension, you instantly grant Gemini CLI the ability to use new domain-specific tools. The beauty is that these extensions come with a pre-defined <strong>"playbook"</strong> that teaches the AI how to use the new tools <a href="https://blog.google/technology/developers/gemini-cli-extensions/#:~:text=Gemini%20CLI%20is%20an%20open,design%20platforms%20to%20payment%20services" rel="nofollow">effectively</a>. That means once installed, you can ask Gemini CLI to perform tasks with those services and it will know the proper APIs or commands to invoke, as if it had that knowledge built-in.</p>
<p dir="auto">Using extensions is very straightforward. The CLI has a command to manage them: <code>gemini extensions install &lt;URL&gt;</code>. Typically, you provide the URL of the extension's GitHub repo or a local path, and the CLI will fetch and install <a href="https://blog.google/technology/developers/gemini-cli-extensions/#:~:text=It%E2%80%99s%20easy%20to%20install%20an,%E2%80%9D%20from%20your%20command%20line" rel="nofollow">it</a>. For example, to install an official extension, you might run: <code>gemini extensions install https://github.com/google-gemini/gemini-cli-extension-cloud-run</code>. Within seconds, the extension is added to your environment (stored under <code>~/.gemini/extensions/</code> or your project's <code>.gemini/extensions/</code> folder). You can then see it by running <code>/extensions</code> in the CLI, which lists active <a href="https://google-gemini.github.io/gemini-cli/docs/cli/commands.html#:~:text=,See%20Gemini%20CLI%20Extensions" rel="nofollow">extensions</a>. From that point on, the AI has new tools at its disposal. If it's a Cloud Run extension, you could say "Deploy my app to Cloud Run," and Gemini CLI will actually be able to execute that (by calling the underlying <code>gcloud</code> commands through the extension's tools). Essentially, extensions function as first-class expansions of Gemini CLI's capabilities, but you opt-in to the ones you need.</p>
<p dir="auto">There's an <strong>open ecosystem</strong> around extensions. Google has an official Extensions page listing available <a href="https://blog.google/technology/developers/gemini-cli-extensions/#:~:text=Access%20an%20open%2C%20growing%20ecosystem,of%20partners%20and%20builders" rel="nofollow">extensions</a>, and because the framework is open, anyone can create and share their own. If you have a particular internal API or workflow, you can build an extension for it so that Gemini CLI can assist with it. Writing an extension is easier than it sounds: you typically create a directory (say, <code>my-extension/</code>) with a file <code>gemini-extension.json</code> describing what tools or context to <a href="https://www.philschmid.de/gemini-cli-cheatsheet#:~:text=Extensions" rel="nofollow">add</a>. You might define new slash commands or specify remote APIs the AI can call. No need to modify Gemini CLI's core - just drop in your extension. The CLI is designed to load these at runtime. Many extensions consist of adding custom <em>MCP tools</em> (Model Context Protocol servers or functions) that the AI can use. For example, an extension could add a <code>/translate</code> command by hooking into an external translation API; once installed, the AI knows how to use <code>/translate</code>. The key benefit is <strong>modularity</strong>: you install only the extensions you want, keeping the CLI lightweight, but you have the option to integrate virtually anything.</p>
<p dir="auto">To manage extensions, besides the <code>install</code> command, you can update or remove them via similar CLI commands (<code>gemini extensions update</code> or just by removing the folder). It's wise to occasionally check for updates on extensions you use, as they may receive improvements. The CLI might introduce an "extensions marketplace" style interface in the future, but for now, exploring the GitHub repositories and official catalog is the way to discover new ones. Some popular ones at launch include the GenAI <strong>Genkit</strong> extension (for building generative AI apps), and a variety of Google Cloud extensions that cover CI/CD, database admin, and more.</p>
<p dir="auto"><strong>Pro Tip:</strong> If you're building your own extension, start by looking at existing ones for examples. The official documentation provides an <strong>Extensions Guide</strong> with the schema and <a href="https://www.philschmid.de/gemini-cli-cheatsheet#:~:text=Extensions" rel="nofollow">capabilities</a>. A simple way to create a private extension is to use the <code>@include</code> functionality in <code>GEMINI.md</code> to inject scripts or context, but a full extension gives you more power (like packaging tools). Also, since extensions can include context files, you can use them to preload domain knowledge. Imagine an extension for your company's internal API that includes a summary of the API and a tool to call it - the AI would then know how to handle requests related to that API. In short, extensions open up a new world where Gemini CLI can interface with anything. Keep an eye on the extensions marketplace for new additions, and don't hesitate to share any useful extension you create with the community - you might just help thousands of other <a href="https://blog.google/technology/developers/gemini-cli-extensions/#:~:text=Gemini%20CLI%20extensions%20are%20here,and%20build%20your%20own%20extension" rel="nofollow">developers</a>.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Additional Fun: Corgi Mode Easter Egg 🐕</h2><a id="user-content-additional-fun-corgi-mode-easter-egg-" aria-label="Permalink: Additional Fun: Corgi Mode Easter Egg 🐕" href="#additional-fun-corgi-mode-easter-egg-"></a></p>
<p dir="auto">Lastly, not a productivity tip but a delightful easter egg - try the command <code>*/corgi*</code> in Gemini CLI. This toggles <strong>"corgi mode"</strong>, which makes a cute corgi animation run across your <a href="https://medium.com/@ferreradaniel/gemini-cli-free-ai-tool-upgrade-5-new-features-you-need-right-now-04cfefac5e93#:~:text=Easter%20Egg%3A%20Corgi%20Mode%20in,Gemini%20CLI" rel="nofollow">terminal</a>! It doesn't help you code any better, but it can certainly lighten the mood during a long coding session. You'll see an ASCII art corgi dashing in the CLI interface. To turn it off, just run <code>/corgi</code> again.</p>
<p dir="auto">This is a purely for-fun feature the team added (and yes, there's even a tongue-in-cheek <a href="https://github.com/google-gemini/gemini-cli/issues/5674#:~:text=How%20about%20you%20NOT%20implement,this%20needed%3F%20Because%20people" data-hovercard-type="issue" data-hovercard-url="/google-gemini/gemini-cli/issues/5674/hovercard">debate</a> about spending dev time on corgi mode). It shows that the creators hide some whimsy in the tool. So when you need a quick break or a smile, give <code>/corgi</code> a try. 🐕🎉</p>
<p dir="auto"><em>(Rumor has it there might be other easter eggs or modes - who knows? Perhaps a "/partyparrot" or similar. The cheat sheet or help command lists <code>/corgi</code>, so it's not a secret, just underused. Now you're in on the joke!)</em></p>
<hr>
<p dir="auto"><strong>Conclusion:</strong></p>
<p dir="auto">We've covered a comprehensive list of pro tips and features for Gemini CLI. From setting up persistent context with <code>GEMINI.md</code>, to writing custom commands and using advanced tools like MCP servers, to leveraging multi-modal inputs and automating workflows, there's a lot this AI command-line assistant can do. As an external developer, you can integrate Gemini CLI into your daily routine - it's like a powerful ally in your terminal that can handle tedious tasks, provide insights, and even troubleshoot your environment.</p>
<p dir="auto">Gemini CLI is evolving rapidly (being open-source with community contributions), so new features and improvements are constantly on the horizon. By mastering the pro tips in this guide, you'll be well-positioned to harness the full potential of this tool. It's not just about using an AI model - it's about integrating AI deeply into how you develop and manage software.</p>
<p dir="auto">Happy coding with Gemini CLI, and have fun exploring just how far your "AI agent in the terminal" can take you.</p>
<p dir="auto"><strong>You now have a Swiss-army knife of AI at your fingertips - use it wisely, and it will make you a more productive (and perhaps happier) developer</strong>!</p>
</article></div></div>]]></description>
        </item>
    </channel>
</rss>