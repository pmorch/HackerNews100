<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0">
    <channel>
        <title>HN100 - Readable Contents</title>
        <link>https://hn.algolia.com/api/v1/search_by_date?tags=%28story,poll%29&amp;numericFilters=points%3E100</link>
        <description>Uses Readability to add bodies to the RSS feed</description>
        <lastBuildDate>Mon, 19 Jan 2026 14:30:07 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>en</language>
        <item>
            <title><![CDATA[Article by article, how Big Tech shaped the EU's roll-back of digital rights (183 pts)]]></title>
            <link>https://corporateeurope.org/en/2026/01/article-article-how-big-tech-shaped-eus-roll-back-digital-rights</link>
            <guid>46678430</guid>
            <pubDate>Mon, 19 Jan 2026 12:53:28 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://corporateeurope.org/en/2026/01/article-article-how-big-tech-shaped-eus-roll-back-digital-rights">https://corporateeurope.org/en/2026/01/article-article-how-big-tech-shaped-eus-roll-back-digital-rights</a>, See on <a href="https://news.ycombinator.com/item?id=46678430">Hacker News</a></p>
<div id="readability-page-1" class="page"><div role="main">
    <a id="main-content" tabindex="-1"></a>

    <div id="content">
    


<div id="block-entityviewcontent">

  

  <p><img loading="lazy" src="https://corporateeurope.org/sites/default/files/styles/image_l/public/2026-01/IMG_0198%281%29.jpeg?itok=mF_GNG30" width="800" height="533" alt="Action in November against the Digital Omnibus">



  </p>


</div>







<div id="block-mainpagecontent">

  

  

      
  
            <p id="docs-internal-guid-8d970413-7fff-a660-cab2-d5a54af12fbd" dir="ltr">In a new analysis by Corporate Europe Observatory and LobbyControl, we trace Big Tech's fingerprints on the Digital Omnibus proposals - a major deregulation of EU digital laws including the GDPR and the AI Act. They are helped in this attempt by the Trump administration and the European far right.</p>
      
      <div>
              <div><p id="docs-internal-guid-baf45d50-7fff-fae2-9151-ff9c7515bb69" dir="ltr">At the end of November 2025, Ursula von der Leyen gave Trump and his tech oligarchs an early Christmas present: an unprecedented attack on digital rights. In its so-called Digital Omnibus, the European Commission proposed weakening important rules designed to protect us from Big Tech’s abuses of power.</p>
<p dir="ltr">These are the protections that keep everyone's data safe, governments and companies accountable, protect people from having artificial intelligence (AI) systems decide their life opportunities, and ultimately keep our societies free from unchecked surveillance.</p>
<p dir="ltr">At the same time, the Digital Omnibus is part of the European Commission's&nbsp;<a href="https://corporateeurope.org/en/deregulation-watch">deregulation agenda</a>, which threatens key social and environmental standards in Europe. Ironically this deregulation agenda is being promoted by the Commission as a way to make the EU 'competitive' – despite in reality actively empowering US Big Tech companies that dominate the field.</p>
<p dir="ltr">The Digital Omnibus was immediately&nbsp;<a href="https://noyb.eu/en/digital-omnibus-eu-commission-wants-wreck-core-gdpr-principles">heavily</a>&nbsp;<a href="https://edri.org/our-work/europe-is-dismantling-its-digital-rights-from-within/">criticised</a> by&nbsp;<a href="https://www.amnesty.org/en/latest/news/2025/11/eu-digital-omnibus-proposals-will-tear-apart-accountability-on-digital-rights/">numerous</a>&nbsp;<a href="https://www.beuc.eu/press-release/eus-plan-simplify-digital-laws-benefit-mainly-large-companies-expense-consumers">civil</a> society organisations.&nbsp;<a href="https://www.politico.eu/article/brussels-police-world-digital-tech-us-china-regulations/">Politico</a> even called it the end of the ‘Brussels effect’ – that is, that European tech regulations are adopted in other countries – and wrote that “Washington is [now] setting the pace on deregulation in Europe.”</p>
<p dir="ltr">To show the extent of Big Tech’s influence on the Digital Omnibus, we compared the Commission’s proposals with the lobbying positions from Big Tech and its associations.&nbsp;</p>
<p dir="ltr">The proposals in the Digital Omnibus concern both data protection and rules for AI. While the EU mistakenly speaks of benefits for European corporations, it is clear that weak digital rules strengthen the power of Google, Microsoft, Meta etc, thereby jeopardising the goal of becoming more independent from Big Tech and the US.&nbsp;</p>
<p dir="ltr">In the past, Big Tech has repeatedly spread the one-sided lobbying message that data protection hinders economic growth and innovation,&nbsp;<a href="https://euneedsai.com/">especially</a> with regard to AI. This includes exceptions for SMEs and a fundamental focus on making&nbsp;<a href="https://www.lobbyregister.bundestag.de/media/2f/ca/502331/Stellungnahme-Gutachten-SG2503310295.pdf">more use of data</a> instead of protecting it.</p>
<p dir="ltr">Tech companies are spreading these messages with a record-breaking lobbying budget, a huge lobbying network, and support from the Trump administration. The digital industry’s annual lobby spending has grown from&nbsp;<a href="https://corporateeurope.org/en/2025/10/big-tech-lobby-budgets-hit-record-levels">€113 million in 2023 to €151 million today</a> – an increase of 33.6 percent in just two years.</p>
<p dir="ltr">Now, the European Commission appears to be bowing to this lobbying pressure and adopting key lobbying messages from Google, Microsoft, Meta and their many lobby organisations in its Digital Omnibus.&nbsp;</p>
<p dir="ltr">Here we break down these industry lobbying messages, how they have been adopted by the Commission as proposed text changes, and what the real world impacts could be.</p>
</div>
              <div><h2 id="docs-internal-guid-8c742d9c-7fff-2d73-6bd2-abdc2b2f3b0b">How the Commission aims to weaken the GDPR and ePrivacy</h2>
<p>The General Data Protection Regulation (GDPR) is the backbone of the EU’s digital rulebook. While the Commission&nbsp;<a href="https://ec.europa.eu/commission/presscorner/detail/fr/speech_25_2732">claims</a> it is only giving the GDPR a “face-lift”, its proposed changes -&nbsp; from the definition of personal data to the use of data for training AI - will have far-reaching consequences to people’s rights, and will benefit Big Tech’s problematic business model based on massive data extraction.</p>
</div>
              <div><h3 id="docs-internal-guid-9e92ada9-7fff-22f1-9b6c-122a6665ff03">Limiting the definition of personal data&nbsp;</h3>
<p dir="ltr">The Commission intends to stop classifying pseudonymised data (ie swapping out a user's identifiable name for a code or number) as personal data if a company claims it cannot identify a person, thereby exempting it from GDPR protection. This rule would also apply even when other actors ( for instance data brokers) can still identify individuals based on the pseudonymised data.</p>
<p dir="ltr">As the digital rights organisations&nbsp;<a href="https://noyb.eu/en/digital-omnibus-eu-commission-wants-wreck-core-gdpr-principles">Noyb</a> and&nbsp;<a href="https://edri.org/our-work/commissions-digital-omnibus-is-a-major-rollback-of-eu-digital-protections/">EDRi</a> have pointed out, this change turns a universal rule into a subjective one. GDPR protections will only apply when a company has the means to identify a person based on the data it holds. This gives huge leeway to companies to decide not to apply the GDPR arguing that they can’t identify a person. Worse, data can be sold to other companies or data brokers that do have the means to re-identify individuals.&nbsp;</p>
<p dir="ltr">But even if data is never sold or passed on to third parties, the proposed subjective approach would still severely narrow the scope of the GDPR. Big Tech companies such as Meta and Google for instance could use personal data for online tracking by claiming that the data cannot be traced back to a natural person and is therefore not covered by the GDPR.</p>
</div>
              <div>
<dl>
<dt><strong>Digital Omnibus</strong></dt>
<dd>
<p dir="ltr"><strong>Proposed changed text to article 4(1) of the GDPR in the digital omnibus in italics:</strong> “Information relating to a natural person is not necessarily personal data for every other person or entity, merely because another entity can identify that natural person.&nbsp;</p>
<p dir="ltr"><em>Information shall not be personal for a given entity where that entity cannot identify the natural person to whom the information relates, taking into account the means reasonably likely to be used by that entity. Such information does not become personal for that entity merely because a potential subsequent recipient has means reasonably likely to be used to identify the natural person to whom the information relates.</em></p>
</dd>
</dl>
</div>
              <div><dl>
<dt>Big Tech’s lobby position&nbsp;&nbsp;</dt>
<dd>
<p id="docs-internal-guid-faedc10c-7fff-a82e-6b7c-f7f0c86a1bc4" dir="ltr">This move closely reflects Big Tech's lobby position. The industry&nbsp; has long been calling for greater commercial use of personal data. The use of anonymous and pseudonymous data in particular would contribute to this.</p>
<p dir="ltr"><strong>DigitalEurope</strong>, (which counts all Big Tech companies among its members),&nbsp;<a href="https://corporateeurope.org/sites/default/files/2026-01/DigitalEurope%20lobby%20paper.pdf">wrote</a>: “Clarify that pseudonymised data is not personal data when recipients cannot reasonably re-identify individuals.”</p>
<p dir="ltr"><strong>Microsoft Germany</strong> also&nbsp;<a href="https://www.lobbyregister.bundestag.de/media/92/8d/322766/Stellungnahme-Gutachten-SG2406280060.pdf">lobbied</a> for weakening the definition along similar lines.</p>
</dd>
</dl>
</div>
              <div><h3 id="docs-internal-guid-20d6b5a7-7fff-42d2-a79e-520cb267b9e1">Limiting your right to access your own data&nbsp;</h3>
<p dir="ltr"><strong>Summary:&nbsp;</strong>Currently, anyone can request a copy of their personal data from any company or organisation that holds it. However, the Commission intends to limit this right if a person ‘abuses’ it.</p>
<p dir="ltr">This will severely limit the rights of individuals to know which of their data is being held by Big Tech. For instance,&nbsp;<a href="https://www.workerinfoexchange.org/post/historic-digital-rights-win-for-wie-and-the-adcu-over-uber-and-ola-at-amsterdam-court-of-appeal">in 2023 Uber and Ola drivers who were ‘robo-fired’ won a court case</a> against the company after it refused access to their work-related information. Ola tried to argue that the drivers requests for data amounted to an abuse of data protection rights, an excuse that the Commission now wants to give a legal basis.</p>
<p dir="ltr">This will make it harder to hold Big Tech to account and to contest their unlawful practices. “The proposal threatens to dismantle a tool of counter-power”, as the academic René Mahieu&nbsp;<a href="https://verfassungsblog.de/digital-omnibus-right-of-access-to-personal-data/">writes</a>.&nbsp;</p>
<p dir="ltr">Contrary to the claims made by industry, and adopted by the German Government, it is not citizens who have ‘abused’ their right to access their own data, but tech companies that have disregarded this right. According to the privacy organisation NOYB<a href="https://noyb.eu/sites/default/files/2025-12/noyb%20Digital%20Omnibus%20Report%20V1.pdf"> 90 percent of data access requests are not respected</a>. In one case, it took&nbsp;<a href="https://noyb.eu/en/noyb-win-youtube-ordered-honour-users-right-access">more than five years&nbsp;</a>for Youtube to respect a particular data access request.</p>
</div>
              <div><dl>
<dt>Digital Omnibus</dt>
<dd>
<p dir="ltr"><strong>Proposed changed text to article 12(5) of the GDPR in the digital omnibus in italics: “</strong>Where requests from a data subject are manifestly unfounded or excessive, in particular because of their repetitive character&nbsp;<em>or also, for requests under Article 15 because the data subject abuses the rights conferred by this regulation for purposes other than the protection of their data</em>, the controller may either: a) charge a reasonable fee [...] or refuse to act on the request.</p>
<p dir="ltr">The controller shall bear the burden of demonstrating&nbsp;<em>that</em> the&nbsp;<em>request is</em> manifestly unfounded&nbsp;<em>or that there are reasonable grounds to believe that it is excessive</em>.”</p>
</dd>
</dl>
</div>
              <div><dl>
<dt><strong>Big Tech’s lobby position&nbsp;&nbsp;</strong></dt>
<dd>
<p dir="ltr"><strong>The German Government lobbied for this change&nbsp;</strong>in an&nbsp;<a href="https://noyb.eu/sites/default/files/2025-11/German%20Proposal%20for%20simplification%20of%20the%20GDPR.pdf">influential but controversial position paper</a>. What has largely gone under the radar, however, is that these proposals were actually&nbsp;<a href="https://www.lobbycontrol.de/pressemitteilung/digitalgipfel-weniger-datenschutz-mehr-macht-fuer-big-tech-123225/">pushed</a> by Big Tech companies.</p>
<p dir="ltr">In a&nbsp;<a href="https://www.lobbyregister.bundestag.de/inhalte-der-interessenvertretung/stellungnahmengutachtensuche/SG2510270013">lobby paper</a> dated 16 August 2025,&nbsp;<strong>Google</strong> called on the German Government to "Introduce a ‘disproportionate efforts’ exemption to compliance with Articles 15-22 GDPR". With regard to Article 12(5), Google proposed the following addition highlighted in bold:&nbsp;</p>
<p dir="ltr">“Where requests from a data subject are manifestly unfounded or excessive, in particular because of their repetitive character,&nbsp;<strong>or, taking into account the scope of the processing and the cost of implementation, where responding to the request would involve a disproportionate effort</strong>, the controller may either: (a) charge a reasonable fee taking into account the administrative costs of providing the information or communication or taking the action requested; or (b) refuse to act on the request.”</p>
</dd>
</dl>
</div>
              <div><h3 id="docs-internal-guid-0be98fd3-7fff-5dd0-ab4d-99f12b489c19"><strong>Using your personal data for training AI</strong></h3>
<p dir="ltr">Generative AI models are being trained on enormous amounts of data. The Commission intends to permit the training of AI models with personal data, including highly sensitive data such as sexuality, political beliefs, or ethnicity, without active consent. People’s data will only be protected from being used for training AI models if they explicitly opt-out.</p>
<p dir="ltr">Tech companies can basically hoover up any personal data on the internet to train their AI models without active consent (opt-out would still be possible). The protection of sensitive data for training AI such as political beliefs, union membership or sexuality is also weakened.</p>
<p dir="ltr">There is a risk of ‘data leakage’ whereby AI systems reproduce the personal data it has been trained on or produce fake information. In one such case a journalist was<a href="https://www.abc.net.au/news/2024-11-04/ai-artificial-intelligence-hallucinations-defamation-chatgpt/104518612"> falsely accused by a Microsoft chatbot of child abuse</a> when in fact he had just published articles on criminal court cases about it. The AI system, in essence a statistical programme, had conflated this information and had made him out to be a criminal.</p>
<p dir="ltr">Major tech companies such as Meta, Google and X stand to benefit as they can train their AI models with massive troves of personal data collected through their platforms.&nbsp;</p>
<p dir="ltr">Big Tech companies are spending enormous amounts,&nbsp;<a href="https://www.cnbc.com/2025/10/31/big-tech-ai-spending-billions-microsoft-google-software-subscriptions.html">possibly as much as US$550 billion in 2026</a>, to dominate the AI market. Loosening rules on AI data collection plays directly into their hands.</p>
</div>
              <div><dl>
<dt>Digital Omnibus</dt>
<dd>
<p id="docs-internal-guid-3b899312-7fff-49a3-e7a2-3628b41a1914" dir="ltr"><strong>Proposed text:&nbsp;</strong></p>
<ul>
<li dir="ltr"><strong>The digital omnibus introduces a new article 88c in the GDPR introducing the use of personal data for AI training as a legitimate interest</strong>:&nbsp;<em>“Where the processing of personal data is necessary for the interests of the controller in the context of the development and operation of an AI system such processing may be pursued for legitimate interests within the meaning of Article 6(1)(f)”</em><br>&nbsp;</li>
<li dir="ltr"><strong>The digital omnibus also waters down protections on using sensitive data for AI training by introducing article 9(5) to the GDPR</strong>:<em> “For processing referred to in point (k) of paragraph 2, appropriate organisational and technical measures shall be implemented to avoid the collection and otherwise processing of special categories of personal data. Where, despite the implementation of such measures, the controller identifies special categories of personal data in the datasets used for training, testing or validation or in the AI system or AI model, the controller shall remove such data. If removal of those data requires disproportionate effort, the controller shall in any event effectively protect without undue delay such data from being used to produce outputs, from being disclosed or otherwise made available to third parties.”</em></li>
</ul>
</dd>
</dl>
</div>
              <div><dl>
<dt>Big Tech’s lobby position</dt>
<dd>
<p id="docs-internal-guid-3eeab578-7fff-3789-9ae3-187d8abc0314" dir="ltr">This has been a top priority of Big Tech lobbying. Almost every trade association and company has lobbied both the Commission and member states on that topic.</p>
<p dir="ltr">Big Tech lobby organisation<strong>&nbsp;</strong><a href="https://ec.europa.eu/info/law/better-regulation/have-your-say/initiatives/14855-Simplification-digital-package-and-omnibus/F33088547_en"><strong>CCIA</strong></a>: “It is crucial to reaffirm the role of legitimate interest as a lawful basis under the GDPR for responsible AI innovation, moving beyond the non-binding EDPB opinion to provide harmonised legal certainty for AI training.”</p>
<p dir="ltr"><a href="https://ec.europa.eu/info/law/better-regulation/have-your-say/initiatives/14855-Simplification-digital-package-and-omnibus/F33103770_en"><strong>DigitalEurope</strong></a>: “Reinforce the use of ‘legitimate interest’ as a ground to process personal data for key use cases such as product development – including of AI models – and security.”</p>
<p dir="ltr">Big Tech lobby organisation<a href="https://corporateeurope.org/sites/default/files/2026-01/DOT%20Europe%20letter%20to%20Danish%20government.pdf"><strong> Dot Europe</strong></a> (in a lobby letter to the Danish Government): “GDPR Article 9 strictly limits the processing of special category data (e.g., race, ethnicity, health), posing challenges for AI development, particularly in healthcare. AI models need access to sensitive data to ensure accuracy, fairness, and cultural relevance.”</p>
</dd>
</dl>
</div>
              <div><h3 id="docs-internal-guid-670509c7-7fff-9a7b-7fc9-3369e98f1371"><strong>Weakening rules on automated decision-making&nbsp;</strong></h3>
<p dir="ltr">Currently, automated systems cannot be used to make decisions with legal effect or for online profiling. A human must be in the loop. The Commission’s proposal is a structural shift from a general prohibition on automated decision-making but with a few narrow exceptions towards an authorisation regime where a company can employ automated decision-making whenever it thinks this is “necessary”.</p>
<p dir="ltr">Important decisions including credit scoring, ‘robo-firings’, profiling, and welfare benefits could in the future be taken by automated decision-making without human intervention. This change will increasingly expose people to possibly flawed and biased algorithms which could make life-changing decisions, including if you get a loan or are fired from your job. Moreover these algorithms are generally black boxes, meaning it can be hard to uncover evidence of bias. Scandals in the&nbsp;<a href="https://www.amnesty.org/en/latest/news/2021/10/xenophobic-machines-dutch-child-benefit-scandal/">Netherlands</a> and&nbsp;<a href="https://www.bbc.com/news/world-australia-66130105">Australia</a> already show how thousands of people can be wrongly targeted with devastating effects.</p>
<p dir="ltr">In 2024, a subsidiary of the food delivery platform Glovo&nbsp;<a href="https://edri.org/our-work/italian-dpas-e5m-fine-against-glovo-marks-milestone-for-workers-rights/">was fined</a> €5 million by the Italian data protection authority under article 22 of the GDPR for violating workers' rights. The platform had used its rating system to automatically assign orders or ‘deactivate’ (read: ‘fire’) workers based on their ratings.</p>
<p dir="ltr">While the drastic weakening of article 22 will benefit a range of different sectors, from the insurance and banking sector to gig economy companies, Big Tech is also set to profit.&nbsp;</p>
<p dir="ltr">At the moment, social media giants employ thousands of underpaid workers to review harmful or illegal content on social media. This change will allow Big Tech companies to fully automate content moderation, cutting these costs essentially down to zero. Since the inauguration of Trump, Meta has<a href="https://www.theguardian.com/technology/2025/apr/23/meta-hastily-changed-moderation-policy-with-little-regard-to-impact-says-oversight-board"> fired thousands of content moderators</a>. Amnesty International<a href="https://www.amnesty.org/en/latest/news/2025/02/meta-new-policy-changes/"> has warned&nbsp;</a>that replacing content moderators with automated systems could amplify the most harmful content including content inciting racial hatred.</p>
</div>
              <div><dl>
<dt>Digital Omnibus</dt>
<dd>
<p id="docs-internal-guid-5e95c669-7fff-85a3-b42e-c9abac657944" dir="ltr"><strong>Proposed text to Article 22 of the GDPR in italics</strong>: “<em>A decision which produces legal effects for a data subject or similarly significantly affects him or her may be based solely on automated processing, including profiling, only where that decision</em>: (a) is necessary for entering into, or performance of, a contract between the data subject and a data controller<em> regardless of whether the decision could be taken otherwise than by solely automated means</em>.”</p>
</dd>
</dl>
</div>
              <div><dl>
<dt>Big Tech’s lobby position&nbsp;&nbsp;</dt>
<dd>
<p id="docs-internal-guid-a08c610f-7fff-3c44-3663-0ddb62903a26" dir="ltr">While Big Tech companies have been complaining about the overlap between article 22 of the GDPR with the AI Act and the Platform Work Directive, it seems it was mainly insurance sector lobbying that was decisive in rolling back the protection on automated decision-making (Big Tech is however still set to benefit from this change). In 2023, the European Court of Justice<a href="https://curia.europa.eu/juris/liste.jsf?num=C-634/21"> ruled in a landmark case&nbsp;</a>that credit scores based on profiling cannot be used by banks and insurance companies to decide on granting a loan or other financial products. The Digital Omnibus might now undermine that ruling.&nbsp;</p>
<p dir="ltr"><a href="https://ec.europa.eu/info/law/better-regulation/have-your-say/initiatives/14855-Simplification-digital-package-and-omnibus/F33088532_en"><strong>Insurance Europe</strong></a>: "Automated-decision making should be allowed as long as it is subject to safeguard mechanisms. To ensure that Art. 22 does not become an obstacle to the development of new digital solutions, it should be clarified that it is a right of the data subject and not an ex-ante prohibition."</p>
<p dir="ltr">Big Tech lobby organisation&nbsp;<a href="https://ec.europa.eu/info/law/better-regulation/have-your-say/initiatives/14855-Simplification-digital-package-and-omnibus/F33088547_en"><strong>CCIA</strong></a>: “The definitions of the General Data Protection Regulation’s (GDPR) ‘automated individual decision-making’ (Article 22), the AI Act’s ‘AI system’ (Article 3(1)), and the Platform Work Directive’s (PWD) for automated decision-making systems often overlap.”</p>
</dd>
</dl>
</div>
              <div><h3 id="docs-internal-guid-7d6fe0e9-7fff-bd1c-753a-6bddaf70988d"><strong>Folding parts of ePrivacy into the GDPR</strong></h3>
<p dir="ltr">Cookies are the backbone of the AdTech industry, used to trace our online activities in order to target us with personalised ads. Article 5(3) of the ePrivacy directive requires websites and apps to ask for prior consent before storing cookies. The Commission now wants to ‘fold’ parts of article 5(3) into the GDPR. This replaces a categorical, consent-based mechanism with a more flexible framework based on balancing and exceptions.</p>
<p dir="ltr">Folding ePrivacy into the GDPR creates a more permissive system that allows companies to use exceptions to track behaviour. The&nbsp;<a href="https://netzpolitik.org/2025/databroker-files-all-you-need-to-know-about-how-adtech-data-exposes-the-eu-to-espionage/">Databroker Files</a> demonstrated that commercial datasets which contain millions of locations could actually be used to spy on the public in Europe. These and other examples show the risks to our privacy are real: reporting shows how the vast trade in location data from smartphones can be traced back to individuals showing where they were at a specific time.</p>
<p dir="ltr">It will allow them to do even more of what they already do: track you&nbsp;<a href="https://techcrunch.com/2020/12/10/france-fines-google-120m-and-amazon-42m-for-dropping-tracking-cookies-without-consent/">without your consent</a>. Big Tech firms have been&nbsp;<a href="https://techcrunch.com/2021/11/01/digging-into-googles-push-to-freeze-eprivacy/">lobbying for years against ePrivacy</a> as it could undermine their invasive business model based on surveillance ads.&nbsp;</p>
<p>Several Big Tech firms have moreover&nbsp;<a href="https://techcrunch.com/2020/12/10/france-fines-google-120m-and-amazon-42m-for-dropping-tracking-cookies-without-consent/">faced fines&nbsp;</a>for tracking users without consent. This change might let these companies get away with their most problematic practices.</p>
</div>
              <div><dl>
<dt>Digital Omnibus</dt>
<dd>
<p id="docs-internal-guid-a1220957-7fff-28f4-8fce-45d6d0d0f6b0" dir="ltr"><strong>New text added to article 5(3) of the ePrivacy directive in italics</strong>: “<em>This paragraph shall not apply if the subscriber or user is a natural person, and the information stored or accessed constitutes or leads to the processing of personal data.”</em></p>
<p dir="ltr"><strong>A new GDPR article 88a takes over instead which also introduces a series of exceptions to ask for consent</strong> including when “creating aggregated information about the usage of an online service to measure the audience of such a service, where it is carried out by the controller of that online service solely for its own use”.</p>
</dd>
</dl>
</div>
              <div><dl>
<dt>Big Tech’s lobby position</dt>
<dd>
<p id="docs-internal-guid-616d9873-7fff-185e-3391-0eaa46c03ead" dir="ltr">The telecom sector, publishers and the tech industry have lobbied for years against strong privacy protections as guaranteed by the ePrivacy directive. In 2018 a major Big Tech driven lobby campaign&nbsp;<a href="https://corporateeurope.org/en/power-lobbies/2018/06/shutting-down-eprivacy-lobby-bandwagon-targets-council">prevented</a> efforts to strengthen the ePrivacy Directive. A court document showed Google revealing that “we have been successful in slowing down and delaying the [ePrivacy Regulation] process and have been working behind the scenes hand in hand with the other companies.” The digital omnibus is another step in dismantling ePrivacy protections with all major players pushing for the changes as proposed by the Commission.</p>
<p dir="ltr"><a href="https://ec.europa.eu/info/law/better-regulation/have-your-say/initiatives/14855-Simplification-digital-package-and-omnibus/F33088017_en"><strong>Google</strong></a>: “The most effective simplification is to delete Article 5(3) from the ePrivacy directive and govern all data processing related to cookies under the GDPR risk-based framework. Alternatively, a significant step toward simplification would be to amend Article 5(3) to extend the scope of permitted exemptions to allow specific, low-risk processing activities that are essential both for the functioning of a safe and sustainable digital ecosystem as well as for user experience. This would create clear exemptions for functions such as first-party audience measurement, ad frequency capping, and anti-fraud measures—allowing them to operate without generating unnecessary consent requests.”&nbsp;&nbsp;</p>
<p dir="ltr"><a href="https://corporateeurope.org/sites/default/files/2026-01/Microsoft%20lobby%20paper%20data%20union%20strategy.pdf"><strong>Microsoft</strong></a><strong>:</strong> “The “cookie rule” in article 5 (3) eP[rivacy] D[irective] could be moved to the GDPR or, if kept in, rendered more flexible by allowing cookie placement without consent in a wider range of circumstances, e.g. for security, software updates, anti-fraud, and analytics.”</p>
</dd>
</dl>
</div>
              <div><h2 id="docs-internal-guid-e42d9ba0-7fff-743a-578b-669027a52594"><strong>How the Commission aims to weaken the AI Act</strong></h2>
<p dir="ltr">"Europe is open for AI and for business!" Ursula von der Leyen tweeted during the AI Action Summit in Paris. In its single-minded priority to “win the global AI race”, the Commission is slashing rules and protections against risky AI systems. A year-long&nbsp;<a href="https://corporateeurope.org/en/2025/11/preparing-roll-back-digital-rights-commissions-secretive-meetings-industry">lobby campaign</a> by the Trump administration and Big Tech to delay the implementation of the AI Act has clearly paid off.</p>
</div>
              <div><h3 id="docs-internal-guid-f2d642be-7fff-e6de-2e0c-7fe036554457"><strong>No Checks and Balances for risky AI systems&nbsp;</strong></h3>
<p dir="ltr">A controversial win for Big Tech firms during the AI Act negotiations was allowing companies to “self-assess” if they believe an AI system is high-risk. To compensate for that loophole, industry had to register these AI systems in a public database. Now this transparency failsafe will also be removed, basically giving tech companies a free hand in deciding if an AI system is risky without any public oversight.&nbsp;</p>
<p dir="ltr">The risk to fundamental rights these high-risk AI systems pose are far from hypothetical.&nbsp; From<a href="https://www.bbc.com/news/business-54698858">&nbsp;algorithmic-powered employee firings</a> to<a href="https://www.axios.com/2020/08/19/england-exams-algorithm-grading">&nbsp;biased algorithms that disadvantage students</a> based on their socio-economic background, highly problematic AI systems are already in circulation. The AI Act lets companies self-assess if these AI systems are high-risk or not, and should therefore comply with requirements such as proper risk management, accuracy, and transparency.</p>
<p dir="ltr">The digital omnibus will worsen an already huge loophole in the AI Act with potentially disastrous impacts on our rights.</p>
<p dir="ltr">Not only can AI companies already self-assess if their AI systems are risky, the digital omnibus will remove any possibility of public oversight of that assessment, giving these companies a blank check to do as they please without any accountability mechanism.</p>
<p dir="ltr">In<a href="https://www.linkedin.com/feed/update/urn:li:activity:7396855577064398849/"> a reaction on LinkedIn</a> Daniel Leufer from the NGO Access Now called this “the biggest, most ridiculous loophole in the AI Act that will let unscrupulous providers unilaterally exempt themselves from the AI Act's obligations with oversight”.&nbsp;&nbsp;</p>
</div>
              <div><dl>
<dt>Digital Omnibus</dt>
<dd>
<p id="docs-internal-guid-aa943702-7fff-4db2-83eb-5cc135dae204" dir="ltr"><strong>Paragraph 2 of article 49 of the AI Act is deleted.</strong></p>
</dd>
</dl>
</div>
              <div><dl>
<dt>Big Tech’s lobby position</dt>
<dd>
<p id="docs-internal-guid-27e01355-7fff-f562-cfea-5cf2ab5716da" dir="ltr">The Commission’s proposals are completely in line with the lobby position of the two lobby organisations Dot Europe and DigitalEurope that count Big Tech members as its members.&nbsp;</p>
<p dir="ltr"><a href="https://ec.europa.eu/info/law/better-regulation/have-your-say/initiatives/14855-Simplification-digital-package-and-omnibus/F33103770_en"><strong>DigitalEurope</strong></a>: “Abolish the mandatory registration of AI systems, along with the related EU and Member State databases.”</p>
<p dir="ltr"><a href="https://ec.europa.eu/info/law/better-regulation/have-your-say/initiatives/14855-Simplification-digital-package-and-omnibus/F33087422_en"><strong>Dot Europe</strong></a><strong>:</strong> “when a provider of AI systems provides concrete justifications that its AI system does not pose a significant risk of harm to the health, safety or fundamental rights of natural persons per Article 6(3), it should not be required to register its system in the high-risk AI database per Article 49.”&nbsp;&nbsp;</p>
</dd>
</dl>
</div>
              <div><h3 id="docs-internal-guid-6a019008-7fff-431e-f228-8639b04b8077"><strong>Delay in the implementation of the AI Act</strong></h3>
<p dir="ltr">The Commission intends to postpone the implementation of part of the AI Regulation by almost a year and a half. This means giving Big Tech more than 12 months to continue releasing potentially risky systems onto the market without any safeguards.</p>
<p dir="ltr">This proposal would enable companies to continue to release risky AI systems for at least a year onto the market without any safeguards. Moreover, as the<a href="https://cdt.org/wp-content/uploads/2025/12/CDT-Europe-Brief-Digital-Omnibus-Threatens-Hard-Won-AI-Safeguards.pdf"> Center for Democracy and Technology points out</a>, delaying the parts of the AI Act on high-risk AI systems, will also obstruct the ban of the most dangerous AI systems, leaving dangerous practices such as&nbsp;<a href="https://edri.org/our-work/emotion-misrecognition/">emotion recognition systems</a> and facial recognition AI used in public spaces on the market for longer.</p>
<p dir="ltr">Delaying is<a href="https://corporateeurope.org/en/2023/11/how-pesticide-lobby-sabotaging-eu-pesticide-reduction-law-sur">&nbsp;a tried and tested industry lobbying tactic</a>. It will give Big Tech more time to further water down the AI Act. Already, tech lobbyists are&nbsp;<a href="https://ccianet.org/news/2025/12/dont-let-digital-simplification-stall-eu-member-states-warned-by-tech-sector/">calling</a> for the further deregulation of the AI Act.</p>
</div>
              <div><dl>
<dt>Big Tech’s lobby position&nbsp;</dt>
<dd>
<p id="docs-internal-guid-b5a2e1d9-7fff-3947-9538-0a5ddb96be61" dir="ltr">A delay in the implementation of the AI Act is a central demand in a&nbsp;<a href="https://corporateeurope.org/en/2025/11/preparing-roll-back-digital-rights-commissions-secretive-meetings-industry">year-long tech lobby campaign</a> which was backed by the Trump administration.</p>
<p dir="ltr"><a href="https://ec.europa.eu/info/law/better-regulation/have-your-say/initiatives/14855-Simplification-digital-package-and-omnibus/F33088547_en"><strong>CCIA</strong></a>: “The first priority should be to delay AI Act implementation until at least 12 months after relevant guidance, codes of practice, or technical standards become available.”</p>
<p dir="ltr"><a href="https://ec.europa.eu/info/law/better-regulation/have-your-say/initiatives/14855-Simplification-digital-package-and-omnibus/F33103770_en"><strong>DigitalEurope</strong></a>: “Delay the application of high-risk AI requirements until at least 12 months after relevant harmonised standards are published, allowing sufficient time for adaptation.”</p>
<p dir="ltr"><a href="https://ec.europa.eu/info/law/better-regulation/have-your-say/initiatives/14855-Simplification-digital-package-and-omnibus/F33089184_en"><strong>Meta</strong></a><strong>:&nbsp;</strong>“It is critical to first pause the implementation and enforcement of the [AI Act]. This pause will provide the necessary time to undertake meaningful reforms without risking the EU falling behind in the global AI race.&nbsp;</p>
</dd>
</dl>
</div>
              <div><h3 id="docs-internal-guid-25437b55-7fff-a246-ab30-33677ccc0348"><strong>Using your sensitive data to train AI&nbsp;</strong></h3>
<p dir="ltr">The AI Act under narrow circumstances allowed the use of sensitive data for mitigation of high-risk AI models to prevent bias and discrimination. This exception is now expanded to all AI systems based on the assessment of companies if the processing is necessary (see also above as part of the changes to the GDPR).</p>
<p dir="ltr">This will allow intrusive gathering of your most sensitive personal data to train AI systems. Also see above “Using your personal data for training AI.</p>
<p dir="ltr">While Big Tech claims that more data is necessary for detecting bias,&nbsp;<a href="https://www.sciencedirect.com/science/article/pii/S0160791X25003173">research</a>&nbsp;<a href="https://edri.org/wp-content/uploads/2021/09/EDRi_Beyond-Debiasing-Report_Online.pdf">suggests</a> that debiasing -&nbsp; certain statistical techniques to ‘correct’ bias in databases that are used to train AI - is often ineffective and is unable to detect the many forms and contexts in which discrimination and bias manifests. Instead, it is a technical fix that enables Big Tech companies to collect yet more sensitive personal data to train their AI models while creating the illusion of ethical AI, all while encouraging the widespread adoption of AI across all sectors of society.</p>
</div>
              <div><dl>
<dt>Digital Omnibus</dt>
<dd>
<p id="docs-internal-guid-14fc2415-7fff-f8e5-5791-dc9231e90f97" dir="ltr"><strong>The digital omnibus introduces article 4(a) to the AI Act</strong>: “To the extent necessary to ensure bias detection and correction in relation to high-risk AI systems in accordance with Article 10 (2), points (f) and (g), of this Regulation, providers of such systems may exceptionally process special categories of personal data.”</p>
</dd>
</dl>
</div>
              <div><dl>
<dt>Big Tech’s lobby position</dt>
<dd>
<p id="docs-internal-guid-a8b18b69-7fff-bdab-e1e3-069b36e29e69" dir="ltr">The tech lobby constantly portrays data protection as a major obstacle to AI training and has therefore repeatedly lobbied, either specifically or in general terms, for the weakening of data protection.&nbsp;</p>
<p dir="ltr"><a href="https://ec.europa.eu/info/law/better-regulation/have-your-say/initiatives/14855-Simplification-digital-package-and-omnibus/F33088017_en"><strong>Google</strong></a>: “We propose extending the allowance in Article 10(5) to permit the necessary data processing for bias detection and correction across all AI systems and general purpose AI models. Extending this provision will provide a harmonized legal basis for developers to proactively build the fair, representative, and trustworthy AI that aligns with the EU’s core values and benefits all citizens. It will also reduce the risk of AI models and systems perpetuating or amplifying societal discrimination, irrespective of their specific AI Act risk classification.”&nbsp;&nbsp;</p>
<p dir="ltr">Big Tech lobby organisation<strong>&nbsp;</strong><a href="https://ec.europa.eu/info/law/better-regulation/have-your-say/initiatives/14855-Simplification-digital-package-and-omnibus/F33087752_en"><strong>Information Technology Industry Council (ITI)</strong></a>: “The AI Act's Article 10(5) allowance for special categories of personal data processing for bias mitigation should be extended to the training of all AI systems and GPAI models, not just those classified as "high-risk.”</p>
</dd>
</dl>
</div>
              <div><h2 id="docs-internal-guid-c7899791-7fff-10f8-e917-ea91e4494d4c"><strong>A Big Tech-far right alliance in the making?</strong></h2>
<p dir="ltr">The Commission’s digital omnibus received widespread criticism. Civil society organisations, think tanks, experts, and political groups in the European Parliament from the left to the centre all perceived the Commission’s proposals as handouts to Big Tech and the Trump administration.</p>
<p dir="ltr">But while the Social Democrats in the Parliament called the digital omnibus&nbsp;<a href="https://www.socialistsanddemocrats.eu/newsroom/sds-dont-deregulate-and-weaken-eus-digital-legal-framework-protects-people">unacceptable deregulation</a>,&nbsp;<a href="https://www.politico.eu/article/ursula-von-der-leyen-eu-parliament-showdown-digital-red-tape-crusade/">far right parties</a> quickly came to the support of the Commission.</p>
<p dir="ltr">Big Tech lobbying of the European Parliament also shifted in higher gear. Lobbying of the far-right seems to have become a particular priority for Meta, and to a lesser extent Google. While during the previous parliamentary mandate, Meta only met once with a far-right MEP, during this parliamentary mandate it has&nbsp;<a href="https://corporateeurope.org/en/media/6519">already met 38 times&nbsp;</a>with MEPs from the ECR, the Patriots and the Europe of Sovereign Nations Group. The digital omnibus is a key priority in those meetings. In the week of 8 December 2025, Meta met with four far right MEPs with most of those meetings mentioning the digital omnibus.&nbsp;</p>
<p dir="ltr">Google has also not shied away from meeting far-right MEPs. A few days after the launch of the digital omnibus, the Head of Public Affairs of Google France&nbsp;<a href="https://www.instagram.com/p/DRfV_JpDdhf/?img_index=2">joined a dinner party</a> in Strasbourg hosted by six French MEPs from the far right Rassemblement National.&nbsp;</p>
</div>
              <div>

  

  <p><img loading="lazy" src="https://corporateeurope.org/sites/default/files/styles/image_l/public/2026-01/Screenshot%20from%202026-01-13%2010-06-50.png?itok=gFs9cpmp" width="800" height="579" alt="Google France at a dinner party of far-right MEPs of Rassemblement National ">



  </p>

<div>
    
            <p>The Head of Public Affairs of Google France&nbsp;joined at a dinner party in Strasbourg hosted by six MEPs from the far right Rassemblement National.</p>
      
            <p>Souce: Instagram</p>
      
  </div>

</div>
              <div><p dir="ltr">Big Tech's lobbying strategy in the US, where it has&nbsp;<a href="https://www.theguardian.com/global/2025/dec/15/ai-trump-openai-google-data-centers">aligned</a> itself with the Trump administration, now appears to have been extended to the European Parliament.</p>
<p dir="ltr">As outlined in this article, the digital omnibus is not just an unprecedented attack on our digital rights – it also closely mirrors Big Tech lobbying positions. The Commission’s deregulation agenda threatens to undermine years of progress in reining these tech giants and protecting our privacy.&nbsp;</p>
<p dir="ltr">The emerging far right - Big Tech alliance in the European Parliament points towards an even more alarming trend. It should now be clear to all that the Commission’s deregulation agenda isn't just opening the door to Big Tech, it's inviting the far right in.</p>
<p dir="ltr">However, this outcome is not inevitable. The European Parliament now has a crucial opportunity to stop this dangerous proposal and defend the hard-won data protection safeguards.</p>
<p dir="ltr">The Digital Omnibus has received massive pushback, from civil society organsations, from within parliament and from member states, including Malta, which recently requested more time to scrutinise the proposal.</p>
<p dir="ltr">What happens next depends on whether we manage to increase the pressure.&nbsp;</p>
<p dir="ltr">Now is the time to make our voices heard and make it crystal clear to the European Parliament and national governments that they must stand up for our privacy, freedom of expression and democratic control over technology, and reject the Digital Omnibus.</p>
</div>
          </div>
  



</div>

<div id="block-views-block-random-promotion-block-1"><p>This article continues after the banner</p>
<a href="https://corporateeurope.org/en/newsletter"><div>

  

  <p><img loading="lazy" src="https://corporateeurope.org/sites/default/files/styles/image_l/public/2023-06/banner_newsletter.png?itok=AePb7Pff" width="800" height="200" alt="Subscribe to our newsletter">



  </p>


</div>

</a></div>
  </div>

  </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Amazon is ending all inventory commingling as of March 31, 2026 (191 pts)]]></title>
            <link>https://twitter.com/ghhughes/status/2012824754319753456</link>
            <guid>46678205</guid>
            <pubDate>Mon, 19 Jan 2026 12:24:14 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://twitter.com/ghhughes/status/2012824754319753456">https://twitter.com/ghhughes/status/2012824754319753456</a>, See on <a href="https://news.ycombinator.com/item?id=46678205">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="ScriptLoadFailure"><form action="" method="GET"><div><p><span>Something went wrong, but don’t fret — let’s give it another shot.</span></p><p><img alt="⚠️" draggable="false" src="https://abs-0.twimg.com/emoji/v2/svg/26a0.svg"><span> Some privacy related extensions may cause issues on x.com. Please disable them and try again.</span></p></div></form></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Wikipedia: WikiProject AI Cleanup (143 pts)]]></title>
            <link>https://en.wikipedia.org/wiki/Wikipedia:WikiProject_AI_Cleanup</link>
            <guid>46677106</guid>
            <pubDate>Mon, 19 Jan 2026 10:09:38 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://en.wikipedia.org/wiki/Wikipedia:WikiProject_AI_Cleanup">https://en.wikipedia.org/wiki/Wikipedia:WikiProject_AI_Cleanup</a>, See on <a href="https://news.ycombinator.com/item?id=46677106">Hacker News</a></p>
Couldn't get https://en.wikipedia.org/wiki/Wikipedia:WikiProject_AI_Cleanup: Error: Request failed with status code 403]]></description>
        </item>
        <item>
            <title><![CDATA[Radboud University selects Fairphone as standard smartphone for employees (308 pts)]]></title>
            <link>https://www.ru.nl/en/staff/news/radboud-university-selects-fairphone-as-standard-smartphone-for-employees</link>
            <guid>46676276</guid>
            <pubDate>Mon, 19 Jan 2026 08:23:04 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.ru.nl/en/staff/news/radboud-university-selects-fairphone-as-standard-smartphone-for-employees">https://www.ru.nl/en/staff/news/radboud-university-selects-fairphone-as-standard-smartphone-for-employees</a>, See on <a href="https://news.ycombinator.com/item?id=46676276">Hacker News</a></p>
<div id="readability-page-1" class="page"><p>
    Do you require a (replacement) smartphone for your work at Radboud University? If so, there is a strong possibility that you will receive a Fairphone from 1 February 2026 onwards. Radboud University has decided to choose Fairphone as its standard company smartphone model for reasons of sustainability, cost efficiency and management support.
  </p><div>
            <p>The Fairphone is a sustainable smartphone with easily replaceable parts such as the battery and screen. This makes the device last longer. Fair and recycled materials, such as plastic and aluminium, are used as much as possible in the production of this smartphone. Fairphone also pays attention to good and safe working conditions in its factories.</p><p>Fairphones are issued to employees by the Information &amp; Library Services (ILS) division. In addition to new Fairphones, the university can also reissue used Samsung devices where possible. These are Samsung devices that have already been returned and still meet the technical and age requirements. As long as these devices are still available, not every employee will receive a Fairphone immediately. Employees who have an iPhone from Radboud University can continue to use it as long as the device is still functioning. However, returned iPhones will no longer be reissued.</p><p>Employees who prefer to use their private phone for work can request an RU SIM card for this purpose. The costs for using your own device will not be reimbursed. Naturally, smartphone models that have already been issued will continue to be supported by ILS colleagues, as will privately purchased smartphone models used for work.</p><h2>Cost-effective and easier management</h2><p>Due to its longer lifespan, the total cost of a Fairphone is lower than that of comparable devices. In addition, Radboud University only needs to purchase, manage and support one standard model. This results in smaller stock, easier management and faster support. Manuals and instructions also only need to be maintained for one device.<br>Furthermore, less investment is required in knowledge of different models/brands. This also helps to speed up incident handling and, where necessary, smartphone replacement.</p><h2>Circularity strategy</h2><p>Fairphone offers a five-year warranty and long-term software support for up to eight years. This means that devices need to be replaced less quickly. This fits in with Radboud University's circularity strategy, which focuses on the longest possible use and reuse of ICT hardware.</p>
    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[A decentralized peer-to-peer messaging application that operates over Bluetooth (312 pts)]]></title>
            <link>https://bitchat.free/</link>
            <guid>46675853</guid>
            <pubDate>Mon, 19 Jan 2026 07:14:19 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://bitchat.free/">https://bitchat.free/</a>, See on <a href="https://news.ycombinator.com/item?id=46675853">Hacker News</a></p>
<div id="readability-page-1" class="page">

<pre>##\       ##\   ##\               ##\                  ##\     
## |      \__|  ## |              ## |                 ## |    
#######\  ##\ ######\    #######\ #######\   ######\ ######\   
##  __##\ ## |\_##  _|  ##  _____|##  __##\  \____##\\_##  _|  
## |  ## |## |  ## |    ## /      ## |  ## | ####### | ## |    
## |  ## |## |  ## |##\ ## |      ## |  ## |##  __## | ## |##\ 
#######  |## |  \####  |\#######\ ## |  ## |\####### | \####  |
\_______/ \__|   \____/  \_______|\__|  \__| \_______|  \____/ 
</pre>

<p>
bitchat is a decentralized peer-to-peer messaging application that operates over bluetooth mesh networks.
no internet required, no servers, no phone numbers.
</p>

<p>
traditional messaging apps depend on centralized infrastructure that can be monitored, censored, or disabled.
bitchat creates ad-hoc communication networks using only the devices present in physical proximity.
each device acts as both client and server, automatically discovering peers and relaying messages across multiple hops to extend the network's reach.
</p>

<p>
this approach provides censorship resistance, surveillance resistance, and infrastructure independence.
the network remains functional during internet outages, natural disasters, protests, or in regions with limited connectivity.
</p>

<h2>software</h2>

<p>
<b>ios/macos version:</b><br>
appstore: <a href="https://apps.apple.com/us/app/bitchat-mesh/id6748219622">bitchat mesh</a><br>
source code: <a href="https://github.com/permissionlesstech/bitchat">https://github.com/permissionlesstech/bitchat</a><br>
supports ios 16.0+ and macos 13.0+. build using xcode with xcodegen or swift package manager.
</p>

<p>
<b>android version:</b><br>
play store: <a href="https://play.google.com/store/apps/details?id=com.bitchat.droid">bitchat</a><br>
source code: <a href="https://github.com/permissionlesstech/bitchat-android">https://github.com/permissionlesstech/bitchat-android</a><br>
apk releases: <a href="https://github.com/permissionlesstech/bitchat-android/releases">https://github.com/permissionlesstech/bitchat-android/releases</a><br>
supports android 8.0+ (api 26). full protocol compatibility with ios version.
</p>

<h2>documentation</h2>

<p>
technical whitepaper: <a href="https://github.com/permissionlesstech/bitchat/blob/main/WHITEPAPER.md">whitepaper.md</a>
</p>

<p>
the software is released into the public domain.
</p>

<hr>
<small>permissionlesstech · <a href="https://bitchat.free/contact.html">contact</a></small>



</div>]]></description>
        </item>
        <item>
            <title><![CDATA[Show HN: Pdfwithlove – PDF tools that run 100% locally (no uploads, no back end) (163 pts)]]></title>
            <link>https://pdfwithlove.netlify.app</link>
            <guid>46675231</guid>
            <pubDate>Mon, 19 Jan 2026 05:04:07 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://pdfwithlove.netlify.app">https://pdfwithlove.netlify.app</a>, See on <a href="https://news.ycombinator.com/item?id=46675231">Hacker News</a></p>
&lt;Unparsable&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[The Code-Only Agent (104 pts)]]></title>
            <link>https://rijnard.com/blog/the-code-only-agent</link>
            <guid>46674416</guid>
            <pubDate>Mon, 19 Jan 2026 02:27:07 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://rijnard.com/blog/the-code-only-agent">https://rijnard.com/blog/the-code-only-agent</a>, See on <a href="https://news.ycombinator.com/item?id=46674416">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
        <article>
          
          <p>
            When Code Execution Really is All You Need
          </p>

          <p>
            <img src="https://rijnard.com/assets/code-only.svg" alt="Code-Only Agent">
          </p>

          <p>
            If you're building an agent, you're probably overwhelmed. Tools.
            MCP. Subagents. Skills. The ecosystem pushes you toward complexity,
            toward "the right way" to do things. You should know: Concepts like
            "Skills" and "MCP" are actually outcomes of an
            <i>ongoing learning process</i> of humans figuring stuff out. The
            space is <i>wide open</i> for exploration. With this mindset I
            wanted to try something different. Simplify the assumptions.
          </p>

          <p>
            What if the agent only had
            <b><i><code>one tool</code></i></b>? Not just any tool, but the most powerful one. The
            <b><code>Turing-complete</code></b> one: <b><i>execute code</i></b>.
          </p>

          <p>
            Truly one tool means: no `bash`, no `ls`, no `grep`. Only
            <b><code>execute_code</code></b>. And you <i>enforce</i> it.
          </p>

          <p>
            When you watch an agent run, you might think: "I wonder what tools
            it'll use to figure this out. Oh look, it ran `ls`. That makes
            sense. Next, `grep`. Cool."
          </p>
          <p>
            The simpler Code-Only paradigm makes that question irrelevant. The
            question shifts from "what tools?" to "what code will it produce?"
            And that's when things get interesting.
          </p>

					<h2><span><code>execute_code</code></span>: One Tool to Rule Them All</h2>

          <p>Traditional prompting works like this:</p>

          <p>
            &gt; Agent, do <b><i>thing</i></b>
            <br>
            &gt; Agent
            <span><b><i>responds</i></b></span>
            with <b><i>thing</i></b>
          </p>

          <p>Contrast with:</p>

          <p>
            &gt; Agent, do <b><i>thing</i></b>
            <br>
            &gt; Agent
            <span><b><i>creates and runs code</i></b></span>
            to do <b><i>thing</i></b>
          </p>

          <p>
            It does this every time. No, really,
            <b><i><code>every</code></i></b>
            time. Pick a runtime for our Code-Only agent, say Python. It needs
            to find a file? It writes Python code to find the file and executes
            the code. Maybe it runs <b><i>rglob</i></b>. Maybe it does <b><i>os.walk</i></b>.
          </p>

          <p>
            It needs to create a script that crawls a website? It doesn't write
            the script to your filesystem (reminder: there's no
            <b><i>create_file</i></b> tool to do that!). It
            <b><i>writes code to output a script that crawls a website</i></b>.<sup><a href="#fn1">1</a></sup>
          </p>

          <p>
            We make it so that there is literally no way for the agent to
            <i><b>do</b></i> anything productive without
            <b><i>writing code</i></b>.
          </p>

          <p>
            So what? Why do this? You're probably thinking, how is this useful?
            Just give it `bash` tool already man.
          </p>

          <p>
            Let's think a bit more deeply what's happening. Traditional agents
            respond with something. Tell it to find some DNA pattern across 100
            files. It might `ls` and `grep`, it might do that in some
            nondeterministic order, it'll figure out <i><b>an answer</b></i> and
            maybe you continue interacting because it missed a directory or you
            added more files. After some time, you end up with a conversation of
            tool calls, responses, and an answer.
          </p>

          <p>
            At some point the agent might even write a Python script to do this
            DNA pattern finding. That would be a lucky happy path, because we
            could rerun that script or update it later... Wait, that's handy...
            actually, more than handy... isn't that
            <code><b><i>ideal</i></b></code>? Wouldn't it be better if we told it to write a script at the
            start? You see, the Code-Only agent doesn't need to be told to write
            a script. It
            <code><b><i>has</i></b></code>
            to, because that's literally the only way for it to do anything of
            substance.
          </p>

          <p>
            The Code-Only agent produces something more precise than an answer
            in natural language. It produces a code <b><i>witness</i></b> of an
            answer. The answer is the output from running the code. The agent
            can interpret that output in natural language (or by writing code),
            but the "work" is codified in a very literal sense. The Code-Only
            agent doesn't respond with something. It produces a code witness
            that outputs something.
          </p>

          <p>
           <span>Try ❯❯ <a href="https://github.com/rvantonder/execute_code_py">Code-Only plugin for Claude Code</a>
            </span>
          </p>

          <h2>Code witnesses are semantic guarantees</h2>

          <p>
            Let's follow the consequences. The code witness must abide by
            certain rules: The rules imposed by the language runtime semantics
            (e.g., of Python). That's not a "next token" process. That's not a
            "LLM figures out sequence of tool calls, no that's not what I
            wanted". It's piece of code. A piece of code! Our one-tool agent has
            a wonderful property: It went through latent space to produce
            something that has a defined semantics, repeatably runnable, and
            imminently comprehensible (for humans or agents alike to reason
            about). This is nondeterministic LLM token-generation projected into
            the space of Turing-complete code, an executable description of
            behavior as we best understand it.
          </p>

          <p>
            Is a Code-Only agent really enough, or too extreme? I'll be frank: I
            pursued this extreme after two things (1) inspiration from articles in <a href="#further-reading">Further Reading</a> below (2) being annoyed at agents for not comprehensively and
            exhaustively analyzing 1000s of files on my laptop. They would skip,
            take shortcuts, hallucinate. I knew how to solve part of that
            problem: create a
            <b><i><code>programmatic</code></i></b>
            loop and try have fresh instances/prompts to do the work
            comprehensively. I can rely on the semantics of a loop written in
            Python. Take this idea further, and you realize that for anything
            long-running and computable (e.g., bash or some tool), you actually
            want the real McCoy: the full witness of code, a trace of why things
            work or don't work. The Code-Only agent
            <code><i><b>enforces</b></i></code>
            that principle.
          </p>

          <p>
            Code-Only agents are not too extreme. I think they're the only way
            forward for computable things. If you're writing travel blog posts,
            you accept the LLMs answer (and you don't need to run tools for
            that). When something is computable though, Code-Only is the only
            path to a
            <b><i><code>fully trustworthy</code></i></b>
            way to make progress where you need guarantees (subject to
            the semantics that your language of choice guarantees, of course). When I say
            guarantees, I mean that in the looser sense, and also in a
            <u><a href="https://en.wikipedia.org/wiki/Formal_verification">Formal</a></u>
            sense. Which beckons: What happens when we use a language like
            <u><a href="https://lean-lang.org/">Lean</a></u> with some of the
            strongest guarantees? Did we not observe that
            <u><a href="https://en.wikipedia.org/wiki/Curry%E2%80%93Howard_correspondence">programs are proofs</a></u>?
          </p>

          <p>
            This lens says the Code-Only agent is a producer of proofs,
            witnesses of computational behavior in the world of
            proofs-as-programs. An LLM in a loop forced to produce proofs, run
            proofs, interpret proof results. That's all.
          </p>

          <h2>Going Code-Only</h2>

          <p>
            So you want to go Code-Only. What happens? The paradigm is simple,
            but the design choices are surprising.
          </p>

          <p>
            First, the harness. The LLM's output is code, and you execute that
            code. What should be communicated back? Exit code makes sense. What
            about output? What if the output is very large? Since you're running
            code, you can specify the result type that running the code should
            return.
          </p>

          <p>
            I've personally, e.g., had the tool return results directly if under
            a certain threshold (1K bytes). This would go into the session context.
						Alternatively, write the results to a JSON
            file on disk if it exceeds the threshold. This avoids context blowup and the result tells the
            agent about the output file path written to disk. How best to pass
            results, persist them, and optimize for size and context fill are
            open questions. You also want to define a way to deal with `stdout`
            and `stderr`: Do you expose these to the agent? Do you summarize
            before exposing?
          </p>

          <p>
            Next, enforcement. Let's say you're using Claude Code. It's not
            enough to persuade it to always create and run code. It turns out
            it's surprisingly twisty to force Claude Code into a single tool
            (maybe support for this will improve). The best plugin-based
            solution I found is a tool PreHook that catches banned tool uses.
            This wastes some iterations when Claude Code tries to use a tool
            that's not allowed, but it learns to stop attempting filesystem
            reads/writes. An initial prompt helps direct.
          </p>

          <p>
            Next, the language runtime. Python, TypeScript, Rust, Bash. Any
            language capable of being executed is fair game, but you'll need to
            think through whether it works for your domain. Dynamic languages
            like Python are interesting because you can run code natively in the
            agent's own runtime, rather than through subprocess calls. Likewise
            TypeScript/JS can be injected into TypeScript-based agents (see
            <a href="#further-reading">Further Reading</a>).
          </p>

          <p>
            Once you get into the Code-Only mindset, you'll see the potential
            for composition and reuse. Claude Skills define reusable processes
            in natural language. What's the equivalent for a Code-Only agent?
            I'm not sure a Skills equivalent exists yet, but I anticipate it
            will take shape soon: code as building blocks for specific domains
            where Code-Only agents compose programmatic patterns. How is that
            different from calling APIs? APIs form part of the reusable blocks,
            but their composition (loops, parallelism, asynchrony) is what a
            Code-Only agent generates.
          </p>

          <p>
            What about heterogeneous languages and runtimes for our `execute_tool`? I don't think we've thought that far yet.
          </p>

          <h2 id="further-reading">Further Reading</h2>

          <p>
            The agent landscape is quickly evolving. My thoughts on how the
            Code-Only paradigm fits into inspiring articles and trends, from
            most recent and going back:
          </p>

          <ul>
            <li>
              <u><b><a href="https://prose.md/">prose.md</a></b></u>
              (Jan 2026) — Code-Only reduces prompts to executable code (with
              loops and statement sequences). Prose expands prompts into natural
              language with program-like constructs (also loops, sequences,
              parallelism). The interplay of natural language for agent
              orchestration and rigid semantics for agent execution could be
              extremely powerful.
            </li>
            <li>
              <u><b><a href="https://steve-yegge.medium.com/welcome-to-gas-town-4f25ee16dd04">Welcome to Gas Town</a></b></u>
              (Jan 2026) — Agent orchestration gone berserk. Tool running is the low-level
              operation at the bottom of the agent stack. Code-Only fits as the
              primitive: no matter how many agents you orchestrate, each one
              reduces to generating and executing code.
            </li>
            <li>
              <u><b><a href="https://www.anthropic.com/engineering/code-execution-with-mcp">Anthropic Code Execution with MCP article</a></b></u>
              (Nov 2025) — MCP-centric view of exposing MCP servers as code API
              and not tool calls. Code-Only is simpler and more general. It
              doesn't care about MCP, and casting the MCP interface as an API is
              a mechanical necessity that acknowledges the power of going
              Code-Only.
            </li>
            <li>
              <u><b><a href="https://www.anthropic.com/engineering/equipping-agents-for-the-real-world-with-agent-skills">Anthropic Agent Skills article</a></b></u>
              (Oct 2025) — Skills embody reusable processes framed in natural
              language. They can generate and run code, but that's not their
              only purpose. Code-Only is narrower (but computationally
              all-powerful): the reusable unit is always executable. The analog
              to Skills manifests as pluggable executable pieces: functions,
              loops, composable routines over APIs.
            </li>
            <li>
              <u><b><a href="https://blog.cloudflare.com/code-mode/">Cloudflare Code Mode article</a></b></u>
              (Sep 2025) — Possibly the earliest concrete single-code-tool
              implementation. Code Mode converts MCP tools into a TypeScript API
              and gives the agent one tool: execute TypeScript. Their insight is
              pragmatic: LLMs write better code than tool calls because of
              training data. In its most general sense, going Code-Only doesn't
              need to rely on MCP or APIs, and encapsulates all code execution
              concerns.
            </li>
            <li>
              <u><b><a href="https://ghuntley.com/ralph/">Ralph Wiggum as a "software engineer"</a></b></u>
              (Jul 2025) — A programmatic loop over agents (agent
              orchestration). Huntley describes it as "deterministically bad in
              a nondeterministic world". Code-Only inverts this a bit:
              projection of a nondeterministic model into deterministic
              execution. Agent orchestration on top of an agent's Code-Only
              inner-loop could be a powerful combination.
            </li>
            <li>
              <u><b><a href="https://lucumr.pocoo.org/2025/7/3/tools/">Tools: Code is All You Need</a></b></u>
              (Jul 2025) — Raises code as a first-order concern for agents.
              Ronacher's observation: asking an LLM to write a script to
              transform markdown makes it possible to reason about and trust the
              process. The script is reviewable, repeatable, composable.
              Code-Only takes this further where every action becomes a script
              you can reason about.
            </li>
            <li>
              <u><b><a href="https://ampcode.com/how-to-build-an-agent">How to Build an Agent</a></b></u>
              (Apr 2025) — The cleanest way to achieve a Code-Only agent today
              may be to build it from scratch. Tweaking current agents like
              Claude Code to enforce a single tool means friction. Thorsten's
              article is a lucid account for building an agent loop with tool
              calls. If you want to enforce Code-Only, this makes it easy to do
              it yourself.
            </li>
          </ul>

          <h2>What's Next</h2>

          <p>
            Two directions feel inevitable. First, agent orchestration. Tools
            like <u><a href="https://prose.md/">prose.md</a></u> let you compose
            agents in natural language with program-like constructs. What
            happens when those agents are Code-Only in their inner loop? You get
            natural language for coordination, rigid semantics for execution.
            The best of both.
          </p>

          <p>
            Second, hybrid tooling. Skills work well for processes that live in
            natural language. Code-Only works well for processes that need
            guarantees. We'll see agents that fluidly mix both: Skills for
            orchestration and intent, Code-Only for computation and precision.
            The line between "prompting an agent" and "programming an agent"
            will blur until it disappears.
          </p>

          <p>
            <span>Try ❯❯ <a href="https://github.com/rvantonder/execute_code_py">Code-Only plugin for Claude Code</a>
            </span>
          </p>

          <hr>
          <p id="fn1">
            <sup>1</sup>There is something beautifully
            <a href="https://en.wikipedia.org/wiki/Quine_(computing)">quine-like</a>
            about this agent. I've always
            <a href="https://github.com/rvantonder/pentaquine">loved quines</a>.
          </p>
					<small>
									<span>Timestamped</span> <i></i> 9 Jan 2026
          </small>
   				<br>
        </article>
      </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Show HN: I quit coding years ago. AI brought me back (210 pts)]]></title>
            <link>https://calquio.com/finance/compound-interest</link>
            <guid>46673809</guid>
            <pubDate>Mon, 19 Jan 2026 00:50:20 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://calquio.com/finance/compound-interest">https://calquio.com/finance/compound-interest</a>, See on <a href="https://news.ycombinator.com/item?id=46673809">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><article><div><header><p>Calculate how your investments grow over time with compound interest.</p></header><div aria-label="Calculator"><div><p><span>Contributed</span><span>$10,000</span></p><p><span>Interest</span><span>+$63,281</span></p><p><span>Return</span><span>632.81%</span></p></div><div data-slot="card"><div><p>1</p><h3>How much are you investing?</h3></div><div><p>2</p><h3>What return do you expect?</h3></div><div><p>3</p><h3>How long will you invest?</h3></div></div><div data-slot="card" data-radix-scroll-area-viewport="" dir="ltr"><table data-slot="table"><thead data-slot="table-header"><tr data-slot="table-row"><th data-slot="table-head">Year</th><th data-slot="table-head">Start Balance</th><th data-slot="table-head">Interest</th><th data-slot="table-head">Contributions</th><th data-slot="table-head">End Balance</th></tr></thead><tbody data-slot="table-body"><tr data-slot="table-row"><td data-slot="table-cell">1</td><td data-slot="table-cell">$10,000</td><td data-slot="table-cell">+<!-- -->$1,047</td><td data-slot="table-cell">-</td><td data-slot="table-cell">$11,047</td></tr><tr data-slot="table-row"><td data-slot="table-cell">2</td><td data-slot="table-cell">$11,047</td><td data-slot="table-cell">+<!-- -->$1,157</td><td data-slot="table-cell">-</td><td data-slot="table-cell">$12,204</td></tr><tr data-slot="table-row"><td data-slot="table-cell">3</td><td data-slot="table-cell">$12,204</td><td data-slot="table-cell">+<!-- -->$1,278</td><td data-slot="table-cell">-</td><td data-slot="table-cell">$13,482</td></tr><tr data-slot="table-row"><td data-slot="table-cell">4</td><td data-slot="table-cell">$13,482</td><td data-slot="table-cell">+<!-- -->$1,412</td><td data-slot="table-cell">-</td><td data-slot="table-cell">$14,894</td></tr><tr data-slot="table-row"><td data-slot="table-cell">5</td><td data-slot="table-cell">$14,894</td><td data-slot="table-cell">+<!-- -->$1,560</td><td data-slot="table-cell">-</td><td data-slot="table-cell">$16,453</td></tr><tr data-slot="table-row"><td data-slot="table-cell">6</td><td data-slot="table-cell">$16,453</td><td data-slot="table-cell">+<!-- -->$1,723</td><td data-slot="table-cell">-</td><td data-slot="table-cell">$18,176</td></tr><tr data-slot="table-row"><td data-slot="table-cell">7</td><td data-slot="table-cell">$18,176</td><td data-slot="table-cell">+<!-- -->$1,903</td><td data-slot="table-cell">-</td><td data-slot="table-cell">$20,079</td></tr><tr data-slot="table-row"><td data-slot="table-cell">8</td><td data-slot="table-cell">$20,079</td><td data-slot="table-cell">+<!-- -->$2,103</td><td data-slot="table-cell">-</td><td data-slot="table-cell">$22,182</td></tr><tr data-slot="table-row"><td data-slot="table-cell">9</td><td data-slot="table-cell">$22,182</td><td data-slot="table-cell">+<!-- -->$2,323</td><td data-slot="table-cell">-</td><td data-slot="table-cell">$24,504</td></tr><tr data-slot="table-row"><td data-slot="table-cell">10</td><td data-slot="table-cell">$24,504</td><td data-slot="table-cell">+<!-- -->$2,566</td><td data-slot="table-cell">-</td><td data-slot="table-cell">$27,070</td></tr><tr data-slot="table-row"><td data-slot="table-cell">11</td><td data-slot="table-cell">$27,070</td><td data-slot="table-cell">+<!-- -->$2,835</td><td data-slot="table-cell">-</td><td data-slot="table-cell">$29,905</td></tr><tr data-slot="table-row"><td data-slot="table-cell">12</td><td data-slot="table-cell">$29,905</td><td data-slot="table-cell">+<!-- -->$3,131</td><td data-slot="table-cell">-</td><td data-slot="table-cell">$33,036</td></tr><tr data-slot="table-row"><td data-slot="table-cell">13</td><td data-slot="table-cell">$33,036</td><td data-slot="table-cell">+<!-- -->$3,459</td><td data-slot="table-cell">-</td><td data-slot="table-cell">$36,496</td></tr><tr data-slot="table-row"><td data-slot="table-cell">14</td><td data-slot="table-cell">$36,496</td><td data-slot="table-cell">+<!-- -->$3,822</td><td data-slot="table-cell">-</td><td data-slot="table-cell">$40,317</td></tr><tr data-slot="table-row"><td data-slot="table-cell">15</td><td data-slot="table-cell">$40,317</td><td data-slot="table-cell">+<!-- -->$4,222</td><td data-slot="table-cell">-</td><td data-slot="table-cell">$44,539</td></tr><tr data-slot="table-row"><td data-slot="table-cell">16</td><td data-slot="table-cell">$44,539</td><td data-slot="table-cell">+<!-- -->$4,664</td><td data-slot="table-cell">-</td><td data-slot="table-cell">$49,203</td></tr><tr data-slot="table-row"><td data-slot="table-cell">17</td><td data-slot="table-cell">$49,203</td><td data-slot="table-cell">+<!-- -->$5,152</td><td data-slot="table-cell">-</td><td data-slot="table-cell">$54,355</td></tr><tr data-slot="table-row"><td data-slot="table-cell">18</td><td data-slot="table-cell">$54,355</td><td data-slot="table-cell">+<!-- -->$5,692</td><td data-slot="table-cell">-</td><td data-slot="table-cell">$60,047</td></tr><tr data-slot="table-row"><td data-slot="table-cell">19</td><td data-slot="table-cell">$60,047</td><td data-slot="table-cell">+<!-- -->$6,288</td><td data-slot="table-cell">-</td><td data-slot="table-cell">$66,335</td></tr><tr data-slot="table-row"><td data-slot="table-cell">20</td><td data-slot="table-cell">$66,335</td><td data-slot="table-cell">+<!-- -->$6,946</td><td data-slot="table-cell">-</td><td data-slot="table-cell">$73,281</td></tr></tbody></table></div></div><section><h2>You May Also Like</h2><p><a href="https://calquio.com/finance/daily-compound-interest"><span>Daily Compound Interest</span></a><a href="https://calquio.com/finance/apy-calculator"><span>APY Calculator</span></a><a href="https://calquio.com/finance/future-value"><span>Future Value</span></a><a href="https://calquio.com/finance/savings-goal"><span>Savings Goal</span></a></p></section><section aria-label="Educational Content"><h2 id="what-is-compound-interest">What is Compound Interest?</h2>
<p>Compound interest is <strong>interest calculated on both the initial principal and the accumulated interest</strong> from previous periods. Unlike simple interest, which only earns interest on the original amount, compound interest allows your money to grow exponentially over time.</p>
<p>Albert Einstein reportedly called compound interest "the eighth wonder of the world," saying: <em>"He who understands it, earns it; he who doesn't, pays it."</em></p>
<h2 id="formula">The Compound Interest Formula</h2>
<p>The basic formula for compound interest is:</p>
<p><span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi>A</mi><mo>=</mo><mi>P</mi><msup><mrow><mo fence="true">(</mo><mn>1</mn><mo>+</mo><mfrac><mi>r</mi><mi>n</mi></mfrac><mo fence="true">)</mo></mrow><mrow><mi>n</mi><mi>t</mi></mrow></msup></mrow><annotation encoding="application/x-tex">A = P \left(1 + \frac{r}{n}\right)^{nt}</annotation></semantics></math></span></span></span></p>
<p>Where:</p>
<ul>
<li><strong>A</strong> = Final amount (principal + interest)</li>
<li><strong>P</strong> = Principal (initial investment)</li>
<li><strong>r</strong> = Annual interest rate (as a decimal)</li>
<li><strong>n</strong> = Number of times interest compounds per year</li>
<li><strong>t</strong> = Time in years</li>
</ul>
<p>For continuous compounding, the formula becomes:</p>
<p><span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi>A</mi><mo>=</mo><mi>P</mi><msup><mi>e</mi><mrow><mi>r</mi><mi>t</mi></mrow></msup></mrow><annotation encoding="application/x-tex">A = Pe^{rt}</annotation></semantics></math></span></span></span></p>
<h2 id="rule-of-72">The Rule of 72</h2>
<p>A quick mental math trick to estimate how long it takes to double your money:</p>
<p><span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mtext>Years&nbsp;to&nbsp;double</mtext><mo>=</mo><mfrac><mn>72</mn><mtext>Interest&nbsp;Rate</mtext></mfrac></mrow><annotation encoding="application/x-tex">\text{Years to double} = \frac{72}{\text{Interest Rate}}</annotation></semantics></math></span></span></span></p>
<p>For example:</p>
<ul>
<li>At <strong>6%</strong> interest: 72 ÷ 6 = <strong>12 years</strong> to double</li>
<li>At <strong>8%</strong> interest: 72 ÷ 8 = <strong>9 years</strong> to double</li>
<li>At <strong>12%</strong> interest: 72 ÷ 12 = <strong>6 years</strong> to double</li>
</ul>
<div><p>The Rule of 72 is a quick approximation. For more precise calculations, use the formula above or our calculator!</p></div>
<h2 id="compound-frequency">Why Compound Frequency Matters</h2>
<p>The more frequently interest compounds, the more you earn. Think of it as: <strong>how often the bank calculates and adds interest to your balance</strong>.</p>
<ul>
<li><strong>Annual compounding</strong>: Interest added once per year</li>
<li><strong>Monthly compounding</strong>: Interest added 12 times per year</li>
<li><strong>Daily compounding</strong>: Interest added 365 times per year</li>
<li><strong>Continuous compounding</strong>: Interest added infinitely (theoretical maximum)</li>
</ul>
<p>At a 10% annual rate on $10,000 over 10 years:</p>
<ul>
<li>Annual compounding: $25,937</li>
<li>Monthly compounding: $27,070</li>
<li>Daily compounding: $27,179</li>
<li>Continuous compounding: $27,183</li>
</ul>
<h2 id="tips">Tips for Maximizing Compound Interest</h2>
<ol>
<li><strong>Start early</strong> – Time is your greatest ally. Even small amounts grow significantly over decades.</li>
<li><strong>Be consistent</strong> – Regular contributions amplify the effect of compounding.</li>
<li><strong>Reinvest returns</strong> – Don't withdraw interest; let it compound.</li>
<li><strong>Seek higher rates</strong> – Even a 1% difference compounds to significant amounts over time.</li>
<li><strong>Minimize fees</strong> – High fees erode your compounding gains.</li>
</ol></section></div></article><!--$--><!--/$--></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[High-speed train collision in Spain kills at least 21 (203 pts)]]></title>
            <link>https://www.bbc.com/news/articles/cedw6ylpynyo</link>
            <guid>46673453</guid>
            <pubDate>Sun, 18 Jan 2026 23:54:43 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.bbc.com/news/articles/cedw6ylpynyo">https://www.bbc.com/news/articles/cedw6ylpynyo</a>, See on <a href="https://news.ycombinator.com/item?id=46673453">Hacker News</a></p>
<div id="readability-page-1" class="page"><article><div data-testid="byline" data-component="byline-block"><p><span data-testid="byline-contributors"><p><span>Harry Sekulich<!-- -->,</span><span data-testid="byline-contributors-contributor-0-role-location">BBC News<!-- -->,</span></p><p><span>Guy Hedgecoe<!-- -->,</span><span data-testid="byline-contributors-contributor-1-role-location">Madrid</span><span>and</span></p><p><span>Rachel Hagan<!-- -->,</span><span data-testid="byline-contributors-contributor-2-role-location">BBC News</span></p></span></p></div><p data-component="caption-block"><figcaption>Footage shows emergency workers at scene of derailment</figcaption></p><div data-component="text-block"><p>At least 39 people have died in a train collision in southern Spain and dozens more have been injured in the country's worst rail crash in more than a decade, Spain's Civil Guard has said.</p><p>Carriages on a Madrid-bound train derailed and crossed over to the opposite tracks, colliding with an oncoming train in Adamuz on Sunday evening.</p><p>Four hundred passengers and staff were onboard both trains, the rail networks said. Emergency services treated 122 people, with 48, including five children, still in hospital. Of those, 11 adults and one child are in intensive care.</p><p>Spanish Transport Minister Óscar Puente said the death toll "is not yet final", as officials launched an investigation.</p></div><div data-component="text-block"><p>Puente described the incident as "extremely strange". All the railway experts consulted by the government "are extremely baffled by the accident", he told reporters in Madrid.</p><p>Rail network operator Adif said the collision happened at 19:45 local time (18:45 GMT), about an hour after the train left Málaga heading north<b id=""> </b>to Madrid, when it derailed on a straight stretch of track near the city of Córdoba.</p><p>The force of the crash pushed the carriages of the second train into an embankment, Puente said. He added that most of those killed and injured were in the front carriages of the second train, which was travelling south<b id=""> </b>from Madrid to Huelva.</p><p>The type of train involved in the crash was a Freccia 1000, which can reach top speeds of 400 km/h (250 mph), a spokesperson for the Italian rail company Ferrovie dello Stato told Reuters news agency.</p><p>Rescue teams said the twisted wreckage of the trains made it difficult to recover people trapped inside the carriages.</p><p>Córdoba fire chief Francisco Carmona told Spanish public broadcaster RTVE: "We have even had to remove a dead person to be able to reach someone alive. It is hard, tricky work."</p></div><figure><div data-component="image-block"><p><img src="https://static.files.bbci.co.uk/bbcdotcom/web/20260113-125315-2e65791d43-web-2.37.1-4/grey-placeholder.png" aria-label="image unavailable"><img sizes="(min-width: 1280px) 50vw, (min-width: 1008px) 66vw, 96vw" srcset="https://ichef.bbci.co.uk/news/240/cpsprodpb/5490/live/430d8b10-f4be-11f0-a422-4ba8a094a8fa.png.webp 240w,https://ichef.bbci.co.uk/news/320/cpsprodpb/5490/live/430d8b10-f4be-11f0-a422-4ba8a094a8fa.png.webp 320w,https://ichef.bbci.co.uk/news/480/cpsprodpb/5490/live/430d8b10-f4be-11f0-a422-4ba8a094a8fa.png.webp 480w,https://ichef.bbci.co.uk/news/640/cpsprodpb/5490/live/430d8b10-f4be-11f0-a422-4ba8a094a8fa.png.webp 640w,https://ichef.bbci.co.uk/news/800/cpsprodpb/5490/live/430d8b10-f4be-11f0-a422-4ba8a094a8fa.png.webp 800w,https://ichef.bbci.co.uk/news/1024/cpsprodpb/5490/live/430d8b10-f4be-11f0-a422-4ba8a094a8fa.png.webp 1024w,https://ichef.bbci.co.uk/news/1536/cpsprodpb/5490/live/430d8b10-f4be-11f0-a422-4ba8a094a8fa.png.webp 1536w" src="https://ichef.bbci.co.uk/news/480/cpsprodpb/5490/live/430d8b10-f4be-11f0-a422-4ba8a094a8fa.png.webp" loading="eager" alt="A map of Spain highlighting a section of the country’s high‑speed rail network. A blue line marks the high‑speed rail route running between Madrid in central Spain and Málaga in the south. A red dot marks Adamuz in the province of Córdoba near the midpoint of the route, where the two trains collided. 
"></p></div></figure><div data-component="text-block"><p>Salvador Jimenez, a journalist with RTVE who was on one of the trains, said the impact felt like an "earthquake". </p><p>"I was in the first carriage. There was a moment when it felt like an earthquake and the train had indeed derailed," Jimenez said.</p><p>Footage from the scene appears to show some train carriages had tipped over on their sides. Rescue workers can be seen scaling the train to pull people out of the lopsided train doors and windows.</p><p>A Madrid-bound passenger, José, told public broadcaster Canal Sur: "There were people and screaming, calling for doctors."</p></div><figure><div data-component="image-block"><p><img src="https://static.files.bbci.co.uk/bbcdotcom/web/20260113-125315-2e65791d43-web-2.37.1-4/grey-placeholder.png" aria-label="image unavailable"><img sizes="(min-width: 1280px) 50vw, (min-width: 1008px) 66vw, 96vw" srcset="https://ichef.bbci.co.uk/news/240/cpsprodpb/3010/live/e418d220-f4e7-11f0-a422-4ba8a094a8fa.jpg.webp 240w,https://ichef.bbci.co.uk/news/320/cpsprodpb/3010/live/e418d220-f4e7-11f0-a422-4ba8a094a8fa.jpg.webp 320w,https://ichef.bbci.co.uk/news/480/cpsprodpb/3010/live/e418d220-f4e7-11f0-a422-4ba8a094a8fa.jpg.webp 480w,https://ichef.bbci.co.uk/news/640/cpsprodpb/3010/live/e418d220-f4e7-11f0-a422-4ba8a094a8fa.jpg.webp 640w,https://ichef.bbci.co.uk/news/800/cpsprodpb/3010/live/e418d220-f4e7-11f0-a422-4ba8a094a8fa.jpg.webp 800w,https://ichef.bbci.co.uk/news/1024/cpsprodpb/3010/live/e418d220-f4e7-11f0-a422-4ba8a094a8fa.jpg.webp 1024w,https://ichef.bbci.co.uk/news/1536/cpsprodpb/3010/live/e418d220-f4e7-11f0-a422-4ba8a094a8fa.jpg.webp 1536w" src="https://ichef.bbci.co.uk/news/480/cpsprodpb/3010/live/e418d220-f4e7-11f0-a422-4ba8a094a8fa.jpg.webp" loading="lazy" alt="Reuters A person affected by a deadly train derailment is transferred for treatment to the Caseta Municipal in the town of Adamuz, after a high-speed train derailed."><span>Reuters</span></p></div><p data-component="caption-block"><figcaption>Passengers were taken to hospital and advanced medical posts near the site of the crash</figcaption></p></figure><div data-component="text-block"><p>All rail services between Madrid and Andalusia were suspended following the accident and are expected to remain closed all day on Monday.</p><p>Iryo, a private rail company that operated the journey from Málaga, said around 300 passengers were on board the train that first derailed, while the other train – operated by the state-funded firm Renfe – had around 100 passengers.</p><p>The official cause is not yet known. An investigation is not expected to determine what happened for at least a month, according to the transport minister. </p><p>Spain's Prime Minister, Pedro Sánchez, said the country will endure a "night of deep pain". </p><p>The mayor of Adamuz, Rafael Moreno, was one of the first people on the scene of the accident, describing it as "a nightmare".</p><p>King Felipe VI and Queen Letizia said they were following news of the disaster "with great concern".</p><p>"We extend our most heartfelt condolences to the relatives and loved ones of the dead, as well as our love and wishes for a swift recovery to the injured," the royal palace said on X.</p><p>The emergency agency in the region of Andalusia urged any crash survivors to contact their families or post on social media that they are alive.</p></div><figure><div data-component="image-block"><p><img src="https://static.files.bbci.co.uk/bbcdotcom/web/20260113-125315-2e65791d43-web-2.37.1-4/grey-placeholder.png" aria-label="image unavailable"><img sizes="(min-width: 1280px) 50vw, (min-width: 1008px) 66vw, 96vw" srcset="https://ichef.bbci.co.uk/news/240/cpsprodpb/5ff9/live/d20c4730-f4ea-11f0-a422-4ba8a094a8fa.jpg.webp 240w,https://ichef.bbci.co.uk/news/320/cpsprodpb/5ff9/live/d20c4730-f4ea-11f0-a422-4ba8a094a8fa.jpg.webp 320w,https://ichef.bbci.co.uk/news/480/cpsprodpb/5ff9/live/d20c4730-f4ea-11f0-a422-4ba8a094a8fa.jpg.webp 480w,https://ichef.bbci.co.uk/news/640/cpsprodpb/5ff9/live/d20c4730-f4ea-11f0-a422-4ba8a094a8fa.jpg.webp 640w,https://ichef.bbci.co.uk/news/800/cpsprodpb/5ff9/live/d20c4730-f4ea-11f0-a422-4ba8a094a8fa.jpg.webp 800w,https://ichef.bbci.co.uk/news/1024/cpsprodpb/5ff9/live/d20c4730-f4ea-11f0-a422-4ba8a094a8fa.jpg.webp 1024w,https://ichef.bbci.co.uk/news/1536/cpsprodpb/5ff9/live/d20c4730-f4ea-11f0-a422-4ba8a094a8fa.jpg.webp 1536w" src="https://ichef.bbci.co.uk/news/480/cpsprodpb/5ff9/live/d20c4730-f4ea-11f0-a422-4ba8a094a8fa.jpg.webp" loading="lazy" alt="EPA A woman is consoled by a woman wearing a fluorescent official jacket at the terminal of a train station"><span>EPA</span></p></div><p data-component="caption-block"><figcaption>Friends and relatives have been seeking information about their loved ones on board either train.</figcaption></p></figure><div data-component="text-block"><p>Advanced medical posts were set up for impacted passengers to be treated for injuries and transferred to hospital. Adif said it set up spaces for relatives of the victims at Atocha, Seville, Córdoba, Málaga and Huelva stations. </p><p>The Spanish Red Cross has deployed emergency support services to the scene, while also offering counselling to families nearby.</p><p>Miguel Ángel Rodríguez from the Red Cross told RNE radio: "The families are going through a situation of great anxiety due to the lack of information. These are very distressing moments."</p></div><figure><div data-component="image-block"><p><img src="https://static.files.bbci.co.uk/bbcdotcom/web/20260113-125315-2e65791d43-web-2.37.1-4/grey-placeholder.png" aria-label="image unavailable"><img sizes="(min-width: 1280px) 50vw, (min-width: 1008px) 66vw, 96vw" srcset="https://ichef.bbci.co.uk/news/240/cpsprodpb/c2ec/live/ab593da0-f4d1-11f0-b5f7-49f0357294ff.jpg.webp 240w,https://ichef.bbci.co.uk/news/320/cpsprodpb/c2ec/live/ab593da0-f4d1-11f0-b5f7-49f0357294ff.jpg.webp 320w,https://ichef.bbci.co.uk/news/480/cpsprodpb/c2ec/live/ab593da0-f4d1-11f0-b5f7-49f0357294ff.jpg.webp 480w,https://ichef.bbci.co.uk/news/640/cpsprodpb/c2ec/live/ab593da0-f4d1-11f0-b5f7-49f0357294ff.jpg.webp 640w,https://ichef.bbci.co.uk/news/800/cpsprodpb/c2ec/live/ab593da0-f4d1-11f0-b5f7-49f0357294ff.jpg.webp 800w,https://ichef.bbci.co.uk/news/1024/cpsprodpb/c2ec/live/ab593da0-f4d1-11f0-b5f7-49f0357294ff.jpg.webp 1024w,https://ichef.bbci.co.uk/news/1536/cpsprodpb/c2ec/live/ab593da0-f4d1-11f0-b5f7-49f0357294ff.jpg.webp 1536w" src="https://ichef.bbci.co.uk/news/480/cpsprodpb/c2ec/live/ab593da0-f4d1-11f0-b5f7-49f0357294ff.jpg.webp" loading="lazy" alt="Reuters A patient in a hospital stretcher and dozens of others are draped in blankets, given water, and helped by medical workers at the Caseta Municipal."><span>Reuters</span></p></div><p data-component="caption-block"><figcaption>The foyer of the hospital close to the crash site, Caseta Municipal in Adamuz, filled with affected passengers.</figcaption></p></figure><div data-component="text-block"><p>French President Emmanuel Macron, Italian Prime Minister Giorgia Meloni and European Commission chief Ursula von der Leyen have published statements offering condolences. </p><p>"My thoughts are with the victims, their families and the entire Spanish people. France stands by your side," Macron wrote on social media.</p><p>In 2013, Spain suffered its worst high-speed train derailment in Galicia, north-west Spain, which left 80 people dead and 140 others injured. </p><p>Spain's high-speed rail network is the second largest in the world, behind China, connecting more than 50 cities across the country. Adif data shows the Spanish rail is more than 4,000km long (2,485 miles)<b id=".">.</b></p></div><figure><div data-component="image-block"><p><img src="https://static.files.bbci.co.uk/bbcdotcom/web/20260113-125315-2e65791d43-web-2.37.1-4/grey-placeholder.png" aria-label="image unavailable"><img sizes="(min-width: 1280px) 50vw, (min-width: 1008px) 66vw, 96vw" srcset="https://ichef.bbci.co.uk/news/240/cpsprodpb/c4f5/live/f98172d0-f504-11f0-b385-5f48925de19a.png.webp 240w,https://ichef.bbci.co.uk/news/320/cpsprodpb/c4f5/live/f98172d0-f504-11f0-b385-5f48925de19a.png.webp 320w,https://ichef.bbci.co.uk/news/480/cpsprodpb/c4f5/live/f98172d0-f504-11f0-b385-5f48925de19a.png.webp 480w,https://ichef.bbci.co.uk/news/640/cpsprodpb/c4f5/live/f98172d0-f504-11f0-b385-5f48925de19a.png.webp 640w,https://ichef.bbci.co.uk/news/800/cpsprodpb/c4f5/live/f98172d0-f504-11f0-b385-5f48925de19a.png.webp 800w,https://ichef.bbci.co.uk/news/1024/cpsprodpb/c4f5/live/f98172d0-f504-11f0-b385-5f48925de19a.png.webp 1024w,https://ichef.bbci.co.uk/news/1536/cpsprodpb/c4f5/live/f98172d0-f504-11f0-b385-5f48925de19a.png.webp 1536w" src="https://ichef.bbci.co.uk/news/480/cpsprodpb/c4f5/live/f98172d0-f504-11f0-b385-5f48925de19a.png.webp" loading="lazy" alt="A thin, grey banner promoting the News Daily newsletter. On the right, there is a graphic of an orange sphere with two concentric crescent shapes around it in a red-orange gradient, like a sound wave. The banner reads: &quot;The latest news in your inbox first thing.”
"></p></div></figure><p>Get our flagship newsletter with all the headlines you need to start the day. <a target="_self" href="https://www.bbc.co.uk/newsletters/zhp28xs">Sign up here.</a></p></article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Prediction: Microsoft will eventually ship a Windows-themed Linux distro (178 pts)]]></title>
            <link>https://gamesbymason.com/blog/2026/microsoft/</link>
            <guid>46673264</guid>
            <pubDate>Sun, 18 Jan 2026 23:24:55 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://gamesbymason.com/blog/2026/microsoft/">https://gamesbymason.com/blog/2026/microsoft/</a>, See on <a href="https://news.ycombinator.com/item?id=46673264">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="page">
    <p><span>January 18, 2026</span> • <span>Mason Remaley</span> •
      <a href="https://gamesbymason.com/blog/tags/#tech">tech</a><a href="https://gamesbymason.com/blog/tags/#gamedev">gamedev</a>
    </p>

    
      
        <div id="intro"><p><img src="https://gamesbymason.com/blog/2026/microsoft/ms-danger.jpg" alt="i'm in danger meme with the windows logo"></p><p>When you think Microsoft you probably don’t think sense of humor. And yet, I’m convinced that Microsoft is going to do a very specific <em>very</em> funny thing within our lifetimes.</p><p>In 2017 I predicted that most programmers would lose their employer &lt;-&gt; employee bargaining power in the next 15-25 years. This was a pretty controversial take at the time, and I never wrote about it publicly, so it’s hard to claim too much credit for being right.</p><p>This time I want the credit, so I’m posting my prediction publicly while everyone still thinks it’s ridiculous:</p><p><strong>I predict that within 15 years Microsoft will discontinue Windows in favor of a Windows themed Linux distribution.</strong></p><p>Sound crazy? Hear me out.</p></div>
      
      
    
      
      
        <div id="table-of-contents">
          <h2><a href="#table-of-contents">Table of Contents</a></h2>
          <div><ul>
<li><a href="#table-of-contents">Table of Contents</a></li><li><a href="#everyone-knows-windows-is-getting-worse">Everyone Knows Windows Is Getting Worse</a></li><li><a href="#meanwhile-linux-is-improving">Meanwhile, Linux Is Improving</a></li><li><a href="#gaming-is-already-a-better-experience-on-linux">Gaming Is Already A Better Experience On Linux</a></li><li><a href="#everyone-else-will-follow-the-gamers">Everyone Else Will Follow The Gamers</a></li><li><a href="#microsoft-ships-linux">Microsoft Ships Linux</a></li></ul></div>
        </div>
      
    
      
        <div id="everyone-knows-windows-is-getting-worse"><h2><a href="#everyone-knows-windows-is-getting-worse">Everyone Knows Windows Is Getting Worse</a></h2><figure><img src="https://gamesbymason.com/blog/2026/microsoft/blue-screen.jpg">
<figcaption>There’s no escape! Well, for now anyway.</figcaption></figure><p>Even people who choose to daily drive Windows agree with this statement. Nobody <a href="https://youtu.be/1cX4t5-YpHQ" target="_blank">looks forward to</a> new Windows releases. If you’re a Windows user and you’re not complaining, you’ve likely internalized Windows bugs as just “what computers are like.”</p><p><em>Explorer.exe is preventing Windows from shutting down</em>.</p><p>Over the past 20 years, I’ve gone through phases of daily driving Linux, macOS, and Windows. People had their favorites, and Windows was always the most jank, but for much of this time all three OSs were viable options for getting things done.</p><p>The reality is that Microsoft no longer has the <a href="https://github.com/actions/runner/issues/3792" target="_blank">expertise</a> in house (or the incentive) to <a href="https://pureinfotech.com/move-taskbar-top-side-windows-11/" target="_blank">maintain Windows</a>, <a href="https://github.com/haskell/directory/issues/110" target="_blank">never mind</a> make <a href="https://windowsreport.com/windows-10-keeps-installing-candy-crush-saga/" target="_blank">improvements</a> to it. As a <a href="https://web.archive.org/web/20251231052258/https://www.linkedin.com/feed/update/urn:li:activity:7407863239289729024/" target="_blank">professional</a> <a href="https://gamesbymason.com/blog/2022/asking-windows-nicely/" target="_blank">programmer</a>, I <a href="https://www.techemails.com/p/bill-gates-tries-to-install-movie-maker" target="_blank">no longer</a> consider <a href="https://github.com/microsoft/terminal/issues/12143" target="_blank">Windows</a> a <a href="https://randomascii.wordpress.com/2021/02/16/arranging-invisible-icons-in-quadratic-time/" target="_blank">viable option</a> for <a href="https://randomascii.wordpress.com/2017/07/09/24-core-cpu-and-i-cant-move-my-mouse/" target="_blank">serious work</a>.</p><p>I think there’s some good stuff happening at the kernel level, but it’s squandered by everything that’s built on top of it. Heck, you can’t even use it to give a slideshow without risking Windows Update deciding to reinstall your whole OS mid presentation! Ask me how I know.</p><p>Sure, you can try to trick Windows into turning off the features you don’t like. <em>Good luck.</em> Daily driving Windows in 2025 means playing cat and mouse with Microsoft. You try to get your work done, <a href="https://asawicki.info/news_1796_how_i_fixed_my_app_taking_5_minutes_to_start" target="_blank">Microsoft tries to stop you.</a></p><p>If you’re a programmer who’s used to Windows and you think I’m being overly harsh, I encourage you to spend a couple weeks in any other operating system.</p><p>It doesn’t have to be this way.</p></div>
      
      
    
      
        <div id="meanwhile-linux-is-improving"><h2><a href="#meanwhile-linux-is-improving">Meanwhile, Linux Is Improving</a></h2><figure><img src="https://gamesbymason.com/blog/2026/microsoft/linux.png">
<figcaption>My NixOS + Ghostty + Hyprland + Flow Control dev setup.</figcaption></figure><p>Linux is <a href="https://xkcd.com/456/" target="_blank">famously not user friendly</a>. However, while the Windows user experience has been declining in quality, Linux’s has been getting better.</p><p>Is it a great experience for non technical users? Not really. But I mean, is Windows?</p><p>Don’t get me wrong, <a href="https://xkcd.com/2501/" target="_blank">non technical people aren’t going to be installing Linux on their own any time soon.</a> They’re not installing Windows either though–it’s just what comes on their computer.</p><p>I don’t honestly think most normal people would notice if someone replaced their Windows installation with KDE Plasma or GNOME or something. So long as their files and desktop background didn’t change, they’d just think that the Windows 11 update they’d been trying to dodge finally installed without their consent and moved a few buttons around.</p></div>
      
      
    
      
        <div id="gaming-is-already-a-better-experience-on-linux"><h2><a href="#gaming-is-already-a-better-experience-on-linux">Gaming Is Already A Better Experience On Linux</a></h2><p><img src="https://gamesbymason.com/blog/2026/microsoft/steam-deck.jpg" alt="a Steam Deck running Way of Rhea"></p><p>At least if you have an AMD card.</p><p>Gamers as a whole haven’t realized this yet, but they will. There are distros popping up focused on being good for gaming, I personally boot into Linux rather than Windows when I want to game despite having both installed, as an engine developer I’ve found AMD + Linux to be the best combo for input latency whereas Nvidia + Windows is the worst.</p><p>PC gamers love to go all out for their hobby. It’s a point of pride. And good for them–IMO being willing to go all out for a hobby is a cool personality trait. Once the PC gamers catch wind of the fact you can get mild perf improvements, no more auto updates, lower input latency, and some tech cred points just by booting into a free operating system…is the burden of proof still on me?</p><p>There’s also a lot of money riding on Linux as a viable gaming platform because of Valve. The Steam Deck runs Linux, and ships with a compatibility layer <a href="https://github.com/ValveSoftware/Proton" target="_blank">Proton</a> (based on <a href="https://www.winehq.org/" target="_blank">Wine</a>) that lets it run Windows games.</p><p>This sounds slow, but it’s not. <em>Wine</em> famously stands for <em>Wine Is Not an Emulator</em>. It is, in fact, not an emulator–it’s just an alternate implementation of win32 and friends. An alternate implementation could be slower or faster; in practice it’s a wash because games make minimal use of these APIs.</p><p>Valve also ships the <a href="https://gitlab.steamos.cloud/steamrt/steamrt/" target="_blank">Steam Linux Runtime</a> to mitigate some of the <a href="https://gamesbymason.com/blog/2025/statically-linking-pipewire/">challenges</a> of shipping native software for Linux. It’s a good thing, because shipping native binaries for Linux is a nightmare–this post is a prediction about Microsoft, not Linux propaganda, Linux has its issues too.</p><p>So why did Valve choose to ship Linux when virtually all games on Steam already run on Windows, and so few natively support Linux? In fact, why even ship hardware at all? Steam is already a money printer.</p><p>I think it’s pretty safe to say that Valve sees PC gaming’s dependence on Windows as a liability to their business model. The business function of the Steam Deck isn’t to generate revenue, it’s to push Proton, and the business function of Proton is to mitigate dependence on Windows.</p><p>Fair enough, right?</p></div>
      
      
    
      
        <div id="everyone-else-will-follow-the-gamers"><h2><a href="#everyone-else-will-follow-the-gamers">Everyone Else Will Follow The Gamers</a></h2><figure><a href="https://www.youtube.com/watch?v=vLlWrt-zmTo" target="_blank"><img src="https://gamesbymason.com/blog/2026/microsoft/friends.png">
<figcaption>“Hey, listen to the man–he’s got a Zen spin on Windows 95!”</figcaption></a></figure><p>“Normal” people don’t tend to spend a lot of money on PCs. You can get a lot done on a smart phone or tablet, or a cheap Windows laptop. If they want to spend more they either buy a MacBook, or they buy whatever the gamers are buying.</p><p>Once the gamers start moving over to Linux, stores will start selling prebuilt machines with Linux installed and marketing them towards gamers. Normal people who want a fast PC for cheap and aren’t sure what to get will pick up a gaming PC like they do today, but instead of a Windows machine with a bunch of crapware preinstalled, it’ll be a Linux machine with a bunch of crapware preinstalled.</p><p>Businesses will hold out on making the switch out of fear of breaking compatibility with the rare but important remaining native productivity tools. And then…</p></div>
      
      
    
      
        <div id="microsoft-ships-linux"><h2><a href="#microsoft-ships-linux">Microsoft Ships Linux</a></h2><figure><img src="https://gamesbymason.com/blog/2026/microsoft/foreshadowing.png">
<figcaption>foreshadowing.png</figcaption></figure><p>Microsoft will eventually cave and do the funniest thing imaginable: they’ll ship a Windows themed Linux distro that can run Windows executables out of the box using Wine, and discontinue support for Windows proper.</p><p>They’ll make a half hearted attempt to keep some vendor lock in by building in special integrations with their SaaS stuff or by patching Wine and refusing to upstream it, but it won’t really work because nobody really likes Microsoft software anyway.</p><p>There you have it. Microsoft doing the funniest thing imaginable. If I’m wrong, you can laugh at me in 15 years.</p></div>
      
      
    
    

    
  </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Show HN: Beats, a web-based drum machine (113 pts)]]></title>
            <link>https://beats.lasagna.pizza</link>
            <guid>46672181</guid>
            <pubDate>Sun, 18 Jan 2026 21:10:08 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://beats.lasagna.pizza">https://beats.lasagna.pizza</a>, See on <a href="https://news.ycombinator.com/item?id=46672181">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>beats</p><div data-controller="beat-maker"><div data-beat-maker-target="controls"><div><p> <span>BPM</span></p></div></div><template id="instrument-template"><div class="instrument-row"><div class="instrument-header"><h3 data-name=""></h3></div><div class="instrument-grid" data-steps=""></div><div class="instrument-controls"><button class="remove-instrument" data-action="click->beat-maker#removeInstrument" data-index="">DEL</button><div class="instrument-visualizer"></div><button class="mute-instrument" data-action="click->beat-maker#muteInstrument" data-index="">MUTE</button></div></div></template><template id="step-button-template"><button data-action="click->beat-maker#toggleStep" data-instrument="" data-step="" data-step-id="" class="step-button"></button></template></div></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Police Invested Millions in Shadowy Phone-Tracking Software Won't Say How Used (343 pts)]]></title>
            <link>https://www.texasobserver.org/texas-police-invest-tangles-sheriff-surveillance/</link>
            <guid>46672150</guid>
            <pubDate>Sun, 18 Jan 2026 21:05:14 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.texasobserver.org/texas-police-invest-tangles-sheriff-surveillance/">https://www.texasobserver.org/texas-police-invest-tangles-sheriff-surveillance/</a>, See on <a href="https://news.ycombinator.com/item?id=46672150">Hacker News</a></p>
Couldn't get https://www.texasobserver.org/texas-police-invest-tangles-sheriff-surveillance/: Error: Request failed with status code 403]]></description>
        </item>
        <item>
            <title><![CDATA[Show HN: Dock – Slack minus the bloat, tax, and 90-day memory loss (153 pts)]]></title>
            <link>https://getdock.io/</link>
            <guid>46671952</guid>
            <pubDate>Sun, 18 Jan 2026 20:42:49 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://getdock.io/">https://getdock.io/</a>, See on <a href="https://news.ycombinator.com/item?id=46671952">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
					<p>⚡ async + sync</p>
					<h3>Work your way</h3>
					<p>Async messages for deep work. Real-time chat when it matters. Work across timezones without the noise.</p>
					<p>🎯 decisions</p>
					<h3>"What did we decide?" Answered.</h3>
					<p>Decisions get lost in chat. Not here. One click to mark, instant recall months later from your Decisions inbox.</p>
					<p>🔒 your data</p>
					<h3>Secure. No lock-in. Ever.</h3>
					<p>SOC 2 compliant infrastructure. Your data encrypted in transit and at rest. One-click data import/export. Your data stays yours.<br><a href="https://getdock.io/faq#security">Learn more →</a></p>
				</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Dead Internet Theory (432 pts)]]></title>
            <link>https://kudmitry.com/articles/dead-internet-theory/</link>
            <guid>46671731</guid>
            <pubDate>Sun, 18 Jan 2026 20:19:07 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://kudmitry.com/articles/dead-internet-theory/">https://kudmitry.com/articles/dead-internet-theory/</a>, See on <a href="https://news.ycombinator.com/item?id=46671731">Hacker News</a></p>
<div id="readability-page-1" class="page"><article>  <header>    <div> <p><time datetime="2026-01-18T19:00:00.000Z"> Jan 18, 2026 </time></p><p><span>·</span> <time datetime="PT6M"> 6 min read
</time> </p>  </div> <p><span>
#Technology </span><span>
#Internet </span> </p> </header> <p>The other day I was browsing my one-and-only social network — which is not a social network, but I’m tired of arguing with people online about it — <a href="https://news.ycombinator.com/">HackerNews</a>.
It’s like this dark corner of the internet, where anonymous tech-enthusiasts, scientists, entrepreneurs, and internet-trolls, like to lurk.
I like HackerNews.
It helps me stay up-to-date about recent tech news (like <a href="https://news.ycombinator.com/item?id=46646645">Cloudflare acquiring Astro</a> which makes me happy for the Astro team, but also sad and worried since I really like Astro, and big-tech has a tendency to ruin things); it <em>mostly</em> avoids politics; and it’s not a social network.</p>
<p>And, in the fashion of HackerNews, I stumbled upon someone sharing their open-source project.
It’s great to see people work on their projects and decide to show them to the world.
I think people underestimate the fear of actually shipping stuff, which involves sharing it with the world.</p>
<p>Upon glancing at the comment section, I started to see other anonymous participants questioning the validity of said open-source project in terms of how much of it was AI-generated.
I grabbed my popcorn, and started to follow this thread.
More accusations started to appear: the commit timeline does not make sense; the code has AI-generated comments; etc.
And at the same time, the author tried to reply to every comment claiming that they wrote this 100% without using AI.</p>
<hr>
<p>I don’t mind people using AI to write code, even though I tried to resist it myself, until eventually succumbing to it.
But I think it’s fair to disclose the use of AI, especially in open-source software.
People on the internet are, mostly, anonymous, and it’s not always possible to verify the claims or expertise of particular individuals.
But as the amount of code is growing, considering that everyone is using AI to generate whatever-app they want, it’s impossible to verify every piece of code we are going to use.
So it’s fair to know, I think, if some project is AI generated and to what extent.
In the end, LLMs are just probabilistic next-token generators.
And while they are getting extremely good at most simple tasks, they have the potential to wreak havoc with harder problems or edge-cases (especially if there are no experienced engineers, with domain knowledge, to review the generated code).</p>
<hr>
<p>As I was following this thread, I stared to see a pattern: the comments of the author looked AI generated too:</p>
<ul>
<li>The use of em-dashes, which on most keyboard require a special key-combination that most people don’t know, and while in markdown two dashes will render as em-dash, this is not true of HackerNews (hence, you often see <code>--</code> in HackerNews comments, where the author is probably used to Markdown renderer turning it into em-dash)</li>
<li>The notorious “you are absolutely right”, which no living human ever used before, at least not that I know of</li>
<li>The other notorious “let me know if you want to [do that thing] or [explore this other thing]” at the end of the sentence</li>
</ul>
<p>I was sitting there, refreshing the page, seeing the author being confronted with use of AI in both their code and their comments, while the author claiming to have not used AI at all.
Honestly, I was thinking I was going insane.
Am I wrong to suspect them?
What if people DO USE em-dashes in real life?
What if English is not their native language and in their native language it’s fine to use phrases like “you are absolutely right”?
Is this even a real person?
Are the people who are commenting real?</p>
<p>And then it hit me.
We have reached the <a href="https://en.wikipedia.org/wiki/Dead_Internet_theory">Dead Internet</a>.
The Dead Internet Theory claims that since around 2016 (a whooping 10 years already), the internet is mainly dead, i.e. most interactions are between bots, and most content is machine generated to either sell you stuff, or game the SEO game (in order to sell you stuff).</p>
<p>I’m <del>ashamed</del> proud to say that I spent a good portion of my teenage years on the internet, chatting and learning from real people who knew more than me.
Back in the early 2000s, there were barely bots on the internet.
The average non-tech human didn’t know anything about phpBB forums, and the weird people with pseudonyms who hanged-out in there.
I spent countless hours inside IRC channels, and on phpBB forums, learning things like network programming, OS-development, game-development, and of course web-development (which became my profession for almost two decades now).
I’m basically a graduate of the Internet University.
Back then, nobody had doubts that they were talking to a human-being.
Sure, you could think that you spoke to a hot girl, who in reality was a fat guy, but hey, at least they were real!</p>
<p>But today, I no longer know what is real.
I saw a picture on LinkedIn, from a real tech company, posting about their “office vibes” and their happy employees.
And then I went to the comment section, and sure enough this picture is AI generated (mangled text that does not make sense, weird hand artifacts).
It was posted by an employee of the company, it showed other employees of said company, and it was altered with AI to showcase a different reality.
Hell, maybe the people on the picture do not even exist!</p>
<p>And these are mild examples.
I don’t use social networks (and no, HackerNews is <strong>not</strong> a social network), but I hear horror stories about AI generated content on Facebook, Xitter, TikTok, ranging from photos of giants that built the pyramids in Egypt, all the way to short videos of pretty girls saying that the EU is bad for Poland.</p>
<p>I honestly got sad that day.
Hopeless, if I could say.
AI is easily available to the masses, which allow them to generate shitload of AI-slop.
People no longer need to write comments or code, they can just feed this to AI agents who will generate the next “you are absolutely right” masterpiece.</p>
<p>I like technology.
I like software engineering, and the concept of the internet where people could share knowledge and create communities.
Were there malicious actors back then on the internet?
For sure.
But what I am seeing today, makes me question whether the future we are headed to is a future where technology is useful anymore.
Or, rather, it’s a future where bots talk with bots, and human knowledge just gets recycled and repackaged into “10 step to fix your [daily problem] you are having” for the sake of selling you more stuff.</p>  </article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Around 1,500 soldiers on standby for deployment to Minneapolis (117 pts)]]></title>
            <link>https://www.bbc.co.uk/news/articles/c74v0pxg2nvo</link>
            <guid>46671086</guid>
            <pubDate>Sun, 18 Jan 2026 19:09:43 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.bbc.co.uk/news/articles/c74v0pxg2nvo">https://www.bbc.co.uk/news/articles/c74v0pxg2nvo</a>, See on <a href="https://news.ycombinator.com/item?id=46671086">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-component="text-block"><p><b>Soldiers are on standby for possible deployment to Minneapolis, a US defence official has told CBS News, the BBC's US partner. </b></p><p>The official said the 1,500 soldiers, currently in Alaska, are an option for US President Donald Trump if he decided to use active duty military personnel, as anti-Immigration and Customs Enforcement (ICE) demonstrations continued in the city on Saturday.</p><p>No decision has yet been made on whether to deploy the soldiers from Alaska, the official said.</p><p>Minnesota officials have urged protesters to stay orderly and peaceful during demonstrations after an ICE agent <a href="https://www.bbc.co.uk/news/articles/c1jepdjy256o">shot dead US citizen Renee Good earlier this month</a>. </p></div><div data-component="text-block"><p>The soldiers are part of the 11th Airborne Division in Fort Wainwright, the official added.</p><p>In a Sunday interview on CBS Face the Nation, Minneapolis Mayor Jacob Frey condemned Trump's threat to send more troops into the city, saying the existing presence of federal ICE agents is already an "occupying force that has quite literally invaded our city".</p><p>"You can go through whatever rhetorical flourish you want, but when you have 3,000 ICE agents and border control come to the city, when you've got this supposed threat of 1,500 military coming to the city, yeah, that's very much what it feels like," Frey said.</p><p>Last week, Trump <a href="https://www.bbc.co.uk/news/articles/cwy15596p98o">threatened to invoke the Insurrection Act</a>, a rarely used law that allows active-duty military personnel to be deployed for law enforcement duties inside the US.</p><p>It comes as a US federal judge issued an order limiting the crowd control tactics that can be used by ICE agents towards "peaceful and unobstructive" protesters in Minneapolis.</p><p>On Friday, Judge Katherine Menendez ruled that federal agents <a href="https://www.bbc.co.uk/news/articles/cvgprwlplqwo">cannot arrest or pepper spray peaceful demonstrations</a>, including those monitoring or observing ICE agents. </p><p>But Homeland Security Secretary Kristi Noem said in a Sunday interview on CBS's Face the Nation that the judge's ruling is "a little ridiculous" because it "didn't change anything". </p><p>"We only use those chemical agents when there's violence happening and perpetuating and you need to be able to establish law and order to keep people safe," Noem said. "So that judge's order didn't change anything for how we're operating on the ground, because it's basically telling us to do what we've already been doing."</p><p>The state's National Guard has been mobilised and placed on alert by Governor Tim Walz, and other law enforcement officers were deployed to Minneapolis ahead of the anti-ICE demonstrations.</p><p>Recent protests in the city were sparked by widespread action by ICE in the city, and follow Good's death on 7 January.</p><p>City leaders said Good was there as a legal observer of ICE activity.</p><p>But the Trump administration has called her a "domestic terrorist".</p><p>Good's death sparked protests across the country, with many people holding signs that read "Justice for Renee".</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Prediction markets are ushering in a world in which news becomes about gambling (211 pts)]]></title>
            <link>https://www.msn.com/en-us/money/markets/america-is-slow-walking-into-a-polymarket-disaster/ar-AA1Upfdb</link>
            <guid>46670524</guid>
            <pubDate>Sun, 18 Jan 2026 18:20:25 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.msn.com/en-us/money/markets/america-is-slow-walking-into-a-polymarket-disaster/ar-AA1Upfdb">https://www.msn.com/en-us/money/markets/america-is-slow-walking-into-a-polymarket-disaster/ar-AA1Upfdb</a>, See on <a href="https://news.ycombinator.com/item?id=46670524">Hacker News</a></p>
&lt;Unparsable&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[Flux 2 Klein pure C inference (370 pts)]]></title>
            <link>https://github.com/antirez/flux2.c</link>
            <guid>46670279</guid>
            <pubDate>Sun, 18 Jan 2026 18:01:58 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/antirez/flux2.c">https://github.com/antirez/flux2.c</a>, See on <a href="https://news.ycombinator.com/item?id=46670279">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><p dir="auto"><h2 tabindex="-1" dir="auto">FLUX.2-klein-4B Pure C Implementation</h2><a id="user-content-flux2-klein-4b-pure-c-implementation" aria-label="Permalink: FLUX.2-klein-4B Pure C Implementation" href="#flux2-klein-4b-pure-c-implementation"></a></p>
<p dir="auto">This program generates images from text prompts (and optionally from other images) using the FLUX.2-klein-4B model from Black Forest Labs. It can be used as a library as well, and is implemented entirely in C, with zero external dependencies beyond the C standard library. MPS and BLAS acceleration are optional but recommended.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">An experiment in AI code generation and open source software</h2><a id="user-content-an-experiment-in-ai-code-generation-and-open-source-software" aria-label="Permalink: An experiment in AI code generation and open source software" href="#an-experiment-in-ai-code-generation-and-open-source-software"></a></p>
<p dir="auto">I (the human here, Salvatore) wanted to test code generation with a more ambitious task, over the weekend. This is the result. It is my first open source project where I wrote zero lines of code. I believe that inference systems not using the Python stack (which I do not appreciate) are a way to free open models usage and make AI more accessible. There is already a project <a href="https://github.com/leejet/stable-diffusion.cpp">doing the inference of diffusion models in C / C++</a> that supports multiple models, and is based on GGML. I wanted to see if, with the assistance of modern AI, I could reproduce this work in a more concise way, from scratch, in a weekend. Looks like it is possible.</p>
<p dir="auto">This code base was written with Claude Code, using the Claude Max plan, the small one of ~80 euros per month. I almost reached the limits but this plan was definitely sufficient for such a large task, which was surprising. In order to simplify the usage of this software, no quantization is used, nor do you need to convert the model. It runs directly with the safetensors model as input, using floats.</p>
<p dir="auto">Even if the code was generated using AI, my help in steering towards the right design, implementation choices, and correctness has been vital during the development. I learned quite a few things about working with non trivial projects and AI.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Quick Start</h2><a id="user-content-quick-start" aria-label="Permalink: Quick Start" href="#quick-start"></a></p>
<div dir="auto" data-snippet-clipboard-copy-content="# Build (choose your backend)
make mps       # Apple Silicon (fastest)
# or: make blas    # Intel Mac / Linux with OpenBLAS
# or: make generic # Pure C, no dependencies

# Download the model (~16GB)
pip install huggingface_hub
python download_model.py

# Generate an image
./flux -d flux-klein-model -p &quot;A woman wearing sunglasses&quot; -o output.png"><pre><span><span>#</span> Build (choose your backend)</span>
make mps       <span><span>#</span> Apple Silicon (fastest)</span>
<span><span>#</span> or: make blas    # Intel Mac / Linux with OpenBLAS</span>
<span><span>#</span> or: make generic # Pure C, no dependencies</span>

<span><span>#</span> Download the model (~16GB)</span>
pip install huggingface_hub
python download_model.py

<span><span>#</span> Generate an image</span>
./flux -d flux-klein-model -p <span><span>"</span>A woman wearing sunglasses<span>"</span></span> -o output.png</pre></div>
<p dir="auto">That's it. No Python runtime, no PyTorch, no CUDA toolkit required at inference time.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Example Output</h2><a id="user-content-example-output" aria-label="Permalink: Example Output" href="#example-output"></a></p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/antirez/flux2.c/blob/main/images/woman_with_sunglasses.png"><img src="https://github.com/antirez/flux2.c/raw/main/images/woman_with_sunglasses.png" alt="Woman with sunglasses"></a></p>
<p dir="auto"><em>Generated with: <code>./flux -d flux-klein-model -p "A picture of a woman in 1960 America. Sunglasses. ASA 400 film. Black and White." -W 250 -H 250 -o /tmp/woman.png</code>, and later processed with image to image generation via <code>./flux -d flux-klein-model -i /tmp/woman.png -o /tmp/woman2.png -p "oil painting of woman with sunglasses" -v -H 256 -W 256</code></em></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Features</h2><a id="user-content-features" aria-label="Permalink: Features" href="#features"></a></p>
<ul dir="auto">
<li><strong>Zero dependencies</strong>: Pure C implementation, works standalone. BLAS optional for ~30x speedup (Apple Accelerate on macOS, OpenBLAS on Linux)</li>
<li><strong>Metal GPU acceleration</strong>: Automatic on Apple Silicon Macs</li>
<li><strong>Text-to-image</strong>: Generate images from text prompts</li>
<li><strong>Image-to-image</strong>: Transform existing images guided by prompts</li>
<li><strong>Integrated text encoder</strong>: Qwen3-4B encoder built-in, no external embedding computation needed</li>
<li><strong>Memory efficient</strong>: Automatic encoder release after encoding (~8GB freed)</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Usage</h2><a id="user-content-usage" aria-label="Permalink: Usage" href="#usage"></a></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Text-to-Image</h3><a id="user-content-text-to-image" aria-label="Permalink: Text-to-Image" href="#text-to-image"></a></p>
<div dir="auto" data-snippet-clipboard-copy-content="./flux -d flux-klein-model -p &quot;A fluffy orange cat sitting on a windowsill&quot; -o cat.png"><pre>./flux -d flux-klein-model -p <span><span>"</span>A fluffy orange cat sitting on a windowsill<span>"</span></span> -o cat.png</pre></div>
<p dir="auto"><h3 tabindex="-1" dir="auto">Image-to-Image</h3><a id="user-content-image-to-image" aria-label="Permalink: Image-to-Image" href="#image-to-image"></a></p>
<p dir="auto">Transform an existing image based on a prompt:</p>
<div dir="auto" data-snippet-clipboard-copy-content="./flux -d flux-klein-model -p &quot;oil painting style&quot; -i photo.png -o painting.png -t 0.7"><pre>./flux -d flux-klein-model -p <span><span>"</span>oil painting style<span>"</span></span> -i photo.png -o painting.png -t 0.7</pre></div>
<p dir="auto">The <code>-t</code> (strength) parameter controls how much the image changes:</p>
<ul dir="auto">
<li><code>0.0</code> = no change (output equals input)</li>
<li><code>1.0</code> = full generation (input only provides composition hint)</li>
<li><code>0.7</code> = good balance for style transfer</li>
</ul>
<p dir="auto"><h3 tabindex="-1" dir="auto">Command Line Options</h3><a id="user-content-command-line-options" aria-label="Permalink: Command Line Options" href="#command-line-options"></a></p>
<p dir="auto"><strong>Required:</strong></p>
<div data-snippet-clipboard-copy-content="-d, --dir PATH        Path to model directory
-p, --prompt TEXT     Text prompt for generation
-o, --output PATH     Output image path (.png or .ppm)"><pre><code>-d, --dir PATH        Path to model directory
-p, --prompt TEXT     Text prompt for generation
-o, --output PATH     Output image path (.png or .ppm)
</code></pre></div>
<p dir="auto"><strong>Generation options:</strong></p>
<div data-snippet-clipboard-copy-content="-W, --width N         Output width in pixels (default: 256)
-H, --height N        Output height in pixels (default: 256)
-s, --steps N         Sampling steps (default: 4)
-S, --seed N          Random seed for reproducibility"><pre><code>-W, --width N         Output width in pixels (default: 256)
-H, --height N        Output height in pixels (default: 256)
-s, --steps N         Sampling steps (default: 4)
-S, --seed N          Random seed for reproducibility
</code></pre></div>
<p dir="auto"><strong>Image-to-image options:</strong></p>
<div data-snippet-clipboard-copy-content="-i, --input PATH      Input image for img2img
-t, --strength N      How much to change the image, 0.0-1.0 (default: 0.75)"><pre><code>-i, --input PATH      Input image for img2img
-t, --strength N      How much to change the image, 0.0-1.0 (default: 0.75)
</code></pre></div>
<p dir="auto"><strong>Output options:</strong></p>
<div data-snippet-clipboard-copy-content="-q, --quiet           Silent mode, no output
-v, --verbose         Show detailed config and timing info"><pre><code>-q, --quiet           Silent mode, no output
-v, --verbose         Show detailed config and timing info
</code></pre></div>
<p dir="auto"><strong>Other options:</strong></p>
<div data-snippet-clipboard-copy-content="-e, --embeddings PATH Load pre-computed text embeddings (advanced)
-h, --help            Show help"><pre><code>-e, --embeddings PATH Load pre-computed text embeddings (advanced)
-h, --help            Show help
</code></pre></div>
<p dir="auto"><h3 tabindex="-1" dir="auto">Reproducibility</h3><a id="user-content-reproducibility" aria-label="Permalink: Reproducibility" href="#reproducibility"></a></p>
<p dir="auto">The seed is always printed to stderr, even when random:</p>
<div data-snippet-clipboard-copy-content="$ ./flux -d flux-klein-model -p &quot;a landscape&quot; -o out.png
Seed: 1705612345
out.png"><pre><code>$ ./flux -d flux-klein-model -p "a landscape" -o out.png
Seed: 1705612345
out.png
</code></pre></div>
<p dir="auto">To reproduce the same image, use the printed seed:</p>
<div data-snippet-clipboard-copy-content="$ ./flux -d flux-klein-model -p &quot;a landscape&quot; -o out.png -S 1705612345"><pre><code>$ ./flux -d flux-klein-model -p "a landscape" -o out.png -S 1705612345
</code></pre></div>
<p dir="auto"><h2 tabindex="-1" dir="auto">Building</h2><a id="user-content-building" aria-label="Permalink: Building" href="#building"></a></p>
<p dir="auto">Choose a backend when building:</p>
<div dir="auto" data-snippet-clipboard-copy-content="make            # Show available backends
make generic    # Pure C, no dependencies (slow)
make blas       # BLAS acceleration (~30x faster)
make mps        # Apple Silicon Metal GPU (fastest, macOS only)"><pre>make            <span><span>#</span> Show available backends</span>
make generic    <span><span>#</span> Pure C, no dependencies (slow)</span>
make blas       <span><span>#</span> BLAS acceleration (~30x faster)</span>
make mps        <span><span>#</span> Apple Silicon Metal GPU (fastest, macOS only)</span></pre></div>
<p dir="auto"><strong>Recommended:</strong></p>
<ul dir="auto">
<li>macOS Apple Silicon: <code>make mps</code></li>
<li>macOS Intel: <code>make blas</code></li>
<li>Linux with OpenBLAS: <code>make blas</code></li>
<li>Linux without OpenBLAS: <code>make generic</code></li>
</ul>
<p dir="auto">For <code>make blas</code> on Linux, install OpenBLAS first:</p>
<div dir="auto" data-snippet-clipboard-copy-content="# Ubuntu/Debian
sudo apt install libopenblas-dev

# Fedora
sudo dnf install openblas-devel"><pre><span><span>#</span> Ubuntu/Debian</span>
sudo apt install libopenblas-dev

<span><span>#</span> Fedora</span>
sudo dnf install openblas-devel</pre></div>
<p dir="auto">Other targets:</p>
<div dir="auto" data-snippet-clipboard-copy-content="make clean      # Clean build artifacts
make info       # Show available backends for this platform
make test       # Run reference image test"><pre>make clean      <span><span>#</span> Clean build artifacts</span>
make info       <span><span>#</span> Show available backends for this platform</span>
make <span>test</span>       <span><span>#</span> Run reference image test</span></pre></div>
<p dir="auto"><h2 tabindex="-1" dir="auto">Model Download</h2><a id="user-content-model-download" aria-label="Permalink: Model Download" href="#model-download"></a></p>
<p dir="auto">The model weights are downloaded from HuggingFace:</p>
<div dir="auto" data-snippet-clipboard-copy-content="pip install huggingface_hub
python download_model.py"><pre>pip install huggingface_hub
python download_model.py</pre></div>
<p dir="auto">This downloads approximately 16GB to <code>./flux-klein-model</code>:</p>
<ul dir="auto">
<li>VAE (~300MB)</li>
<li>Transformer (~4GB)</li>
<li>Qwen3-4B Text Encoder (~8GB)</li>
<li>Tokenizer</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Technical Details</h2><a id="user-content-technical-details" aria-label="Permalink: Technical Details" href="#technical-details"></a></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Model Architecture</h3><a id="user-content-model-architecture" aria-label="Permalink: Model Architecture" href="#model-architecture"></a></p>
<p dir="auto"><strong>FLUX.2-klein-4B</strong> is a rectified flow transformer optimized for fast inference:</p>
<markdown-accessiblity-table><table>
<thead>
<tr>
<th>Component</th>
<th>Architecture</th>
</tr>
</thead>
<tbody>
<tr>
<td>Transformer</td>
<td>5 double blocks + 20 single blocks, 3072 hidden dim, 24 attention heads</td>
</tr>
<tr>
<td>VAE</td>
<td>AutoencoderKL, 128 latent channels, 8x spatial compression</td>
</tr>
<tr>
<td>Text Encoder</td>
<td>Qwen3-4B, 36 layers, 2560 hidden dim</td>
</tr>
</tbody>
</table></markdown-accessiblity-table>
<p dir="auto"><strong>Inference steps</strong>: This is a distilled model that produces good results with exactly 4 sampling steps.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Memory Requirements</h3><a id="user-content-memory-requirements" aria-label="Permalink: Memory Requirements" href="#memory-requirements"></a></p>
<markdown-accessiblity-table><table>
<thead>
<tr>
<th>Phase</th>
<th>Memory</th>
</tr>
</thead>
<tbody>
<tr>
<td>Text encoding</td>
<td>~8GB (encoder weights)</td>
</tr>
<tr>
<td>Diffusion</td>
<td>~8GB (transformer ~4GB + VAE ~300MB + activations)</td>
</tr>
<tr>
<td>Peak</td>
<td>~16GB (if encoder not released)</td>
</tr>
</tbody>
</table></markdown-accessiblity-table>
<p dir="auto">The text encoder is automatically released after encoding, reducing peak memory during diffusion. If you generate multiple images with different prompts, the encoder reloads automatically.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Resolution Limits</h3><a id="user-content-resolution-limits" aria-label="Permalink: Resolution Limits" href="#resolution-limits"></a></p>
<p dir="auto"><strong>Maximum resolution</strong>: 1024x1024 pixels. Higher resolutions require prohibitive memory for the attention mechanisms.</p>
<p dir="auto"><strong>Minimum resolution</strong>: 64x64 pixels.</p>
<p dir="auto">Dimensions should be multiples of 16 (the VAE downsampling factor).</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">C Library API</h2><a id="user-content-c-library-api" aria-label="Permalink: C Library API" href="#c-library-api"></a></p>
<p dir="auto">The library can be integrated into your own C/C++ projects. Link against <code>libflux.a</code> and include <code>flux.h</code>.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Text-to-Image Generation</h3><a id="user-content-text-to-image-generation" aria-label="Permalink: Text-to-Image Generation" href="#text-to-image-generation"></a></p>
<p dir="auto">Here's a complete program that generates an image from a text prompt:</p>
<div dir="auto" data-snippet-clipboard-copy-content="#include &quot;flux.h&quot;
#include <stdio.h>

int main(void) {
    /* Load the model. This loads VAE, transformer, and text encoder. */
    flux_ctx *ctx = flux_load_dir(&quot;flux-klein-model&quot;);
    if (!ctx) {
        fprintf(stderr, &quot;Failed to load model: %s\n&quot;, flux_get_error());
        return 1;
    }

    /* Configure generation parameters. Start with defaults and customize. */
    flux_params params = FLUX_PARAMS_DEFAULT;
    params.width = 512;
    params.height = 512;
    params.seed = 42;  /* Use -1 for random seed */

    /* Generate the image. This handles text encoding, diffusion, and VAE decode. */
    flux_image *img = flux_generate(ctx, &quot;A fluffy orange cat in a sunbeam&quot;, &amp;params);
    if (!img) {
        fprintf(stderr, &quot;Generation failed: %s\n&quot;, flux_get_error());
        flux_free(ctx);
        return 1;
    }

    /* Save to file. Format is determined by extension (.png or .ppm). */
    flux_image_save(img, &quot;cat.png&quot;);
    printf(&quot;Saved cat.png (%dx%d)\n&quot;, img->width, img->height);

    /* Clean up */
    flux_image_free(img);
    flux_free(ctx);
    return 0;
}"><pre><span>#include</span> <span>"flux.h"</span>
<span>#include</span> <span>&lt;stdio.h&gt;</span>

<span>int</span> <span>main</span>(<span>void</span>) {
    <span>/* Load the model. This loads VAE, transformer, and text encoder. */</span>
    <span>flux_ctx</span> <span>*</span><span>ctx</span> <span>=</span> <span>flux_load_dir</span>(<span>"flux-klein-model"</span>);
    <span>if</span> (!<span>ctx</span>) {
        <span>fprintf</span>(<span>stderr</span>, <span>"Failed to load model: %s\n"</span>, <span>flux_get_error</span>());
        <span>return</span> <span>1</span>;
    }

    <span>/* Configure generation parameters. Start with defaults and customize. */</span>
    <span>flux_params</span> <span>params</span> <span>=</span> <span>FLUX_PARAMS_DEFAULT</span>;
    <span>params</span>.<span>width</span> <span>=</span> <span>512</span>;
    <span>params</span>.<span>height</span> <span>=</span> <span>512</span>;
    <span>params</span>.<span>seed</span> <span>=</span> <span>42</span>;  <span>/* Use -1 for random seed */</span>

    <span>/* Generate the image. This handles text encoding, diffusion, and VAE decode. */</span>
    <span>flux_image</span> <span>*</span><span>img</span> <span>=</span> <span>flux_generate</span>(<span>ctx</span>, <span>"A fluffy orange cat in a sunbeam"</span>, <span>&amp;</span><span>params</span>);
    <span>if</span> (!<span>img</span>) {
        <span>fprintf</span>(<span>stderr</span>, <span>"Generation failed: %s\n"</span>, <span>flux_get_error</span>());
        <span>flux_free</span>(<span>ctx</span>);
        <span>return</span> <span>1</span>;
    }

    <span>/* Save to file. Format is determined by extension (.png or .ppm). */</span>
    <span>flux_image_save</span>(<span>img</span>, <span>"cat.png"</span>);
    <span>printf</span>(<span>"Saved cat.png (%dx%d)\n"</span>, <span>img</span><span>-&gt;</span><span>width</span>, <span>img</span><span>-&gt;</span><span>height</span>);

    <span>/* Clean up */</span>
    <span>flux_image_free</span>(<span>img</span>);
    <span>flux_free</span>(<span>ctx</span>);
    <span>return</span> <span>0</span>;
}</pre></div>
<p dir="auto">Compile with:</p>
<div dir="auto" data-snippet-clipboard-copy-content="gcc -o myapp myapp.c -L. -lflux -lm -framework Accelerate  # macOS
gcc -o myapp myapp.c -L. -lflux -lm -lopenblas              # Linux"><pre>gcc -o myapp myapp.c -L. -lflux -lm -framework Accelerate  <span><span>#</span> macOS</span>
gcc -o myapp myapp.c -L. -lflux -lm -lopenblas              <span><span>#</span> Linux</span></pre></div>
<p dir="auto"><h3 tabindex="-1" dir="auto">Image-to-Image Transformation</h3><a id="user-content-image-to-image-transformation" aria-label="Permalink: Image-to-Image Transformation" href="#image-to-image-transformation"></a></p>
<p dir="auto">Transform an existing image guided by a text prompt. The <code>strength</code> parameter controls how much the image changes:</p>
<div dir="auto" data-snippet-clipboard-copy-content="#include &quot;flux.h&quot;
#include <stdio.h>

int main(void) {
    flux_ctx *ctx = flux_load_dir(&quot;flux-klein-model&quot;);
    if (!ctx) return 1;

    /* Load the input image */
    flux_image *photo = flux_image_load(&quot;photo.png&quot;);
    if (!photo) {
        fprintf(stderr, &quot;Failed to load image\n&quot;);
        flux_free(ctx);
        return 1;
    }

    /* Set up parameters. Output size defaults to input size. */
    flux_params params = FLUX_PARAMS_DEFAULT;
    params.strength = 0.7;  /* 0.0 = no change, 1.0 = full regeneration */
    params.seed = 123;

    /* Transform the image */
    flux_image *painting = flux_img2img(ctx, &quot;oil painting, impressionist style&quot;,
                                         photo, &amp;params);
    flux_image_free(photo);  /* Done with input */

    if (!painting) {
        fprintf(stderr, &quot;Transformation failed: %s\n&quot;, flux_get_error());
        flux_free(ctx);
        return 1;
    }

    flux_image_save(painting, &quot;painting.png&quot;);
    printf(&quot;Saved painting.png\n&quot;);

    flux_image_free(painting);
    flux_free(ctx);
    return 0;
}"><pre><span>#include</span> <span>"flux.h"</span>
<span>#include</span> <span>&lt;stdio.h&gt;</span>

<span>int</span> <span>main</span>(<span>void</span>) {
    <span>flux_ctx</span> <span>*</span><span>ctx</span> <span>=</span> <span>flux_load_dir</span>(<span>"flux-klein-model"</span>);
    <span>if</span> (!<span>ctx</span>) <span>return</span> <span>1</span>;

    <span>/* Load the input image */</span>
    <span>flux_image</span> <span>*</span><span>photo</span> <span>=</span> <span>flux_image_load</span>(<span>"photo.png"</span>);
    <span>if</span> (!<span>photo</span>) {
        <span>fprintf</span>(<span>stderr</span>, <span>"Failed to load image\n"</span>);
        <span>flux_free</span>(<span>ctx</span>);
        <span>return</span> <span>1</span>;
    }

    <span>/* Set up parameters. Output size defaults to input size. */</span>
    <span>flux_params</span> <span>params</span> <span>=</span> <span>FLUX_PARAMS_DEFAULT</span>;
    <span>params</span>.<span>strength</span> <span>=</span> <span>0.7</span>;  <span>/* 0.0 = no change, 1.0 = full regeneration */</span>
    <span>params</span>.<span>seed</span> <span>=</span> <span>123</span>;

    <span>/* Transform the image */</span>
    <span>flux_image</span> <span>*</span><span>painting</span> <span>=</span> <span>flux_img2img</span>(<span>ctx</span>, <span>"oil painting, impressionist style"</span>,
                                         <span>photo</span>, <span>&amp;</span><span>params</span>);
    <span>flux_image_free</span>(<span>photo</span>);  <span>/* Done with input */</span>

    <span>if</span> (!<span>painting</span>) {
        <span>fprintf</span>(<span>stderr</span>, <span>"Transformation failed: %s\n"</span>, <span>flux_get_error</span>());
        <span>flux_free</span>(<span>ctx</span>);
        <span>return</span> <span>1</span>;
    }

    <span>flux_image_save</span>(<span>painting</span>, <span>"painting.png"</span>);
    <span>printf</span>(<span>"Saved painting.png\n"</span>);

    <span>flux_image_free</span>(<span>painting</span>);
    <span>flux_free</span>(<span>ctx</span>);
    <span>return</span> <span>0</span>;
}</pre></div>
<p dir="auto"><strong>Strength values:</strong></p>
<ul dir="auto">
<li><code>0.3</code> - Subtle style transfer, preserves most details</li>
<li><code>0.5</code> - Moderate transformation</li>
<li><code>0.7</code> - Strong transformation, good for style transfer</li>
<li><code>0.9</code> - Almost complete regeneration, keeps only composition</li>
</ul>
<p dir="auto"><h3 tabindex="-1" dir="auto">Generating Multiple Images</h3><a id="user-content-generating-multiple-images" aria-label="Permalink: Generating Multiple Images" href="#generating-multiple-images"></a></p>
<p dir="auto">When generating multiple images with different seeds but the same prompt, you can avoid reloading the text encoder:</p>
<div dir="auto" data-snippet-clipboard-copy-content="flux_ctx *ctx = flux_load_dir(&quot;flux-klein-model&quot;);
flux_params params = FLUX_PARAMS_DEFAULT;
params.width = 256;
params.height = 256;

/* Generate 5 variations with different seeds */
for (int i = 0; i < 5; i++) {
    flux_set_seed(1000 + i);

    flux_image *img = flux_generate(ctx, &quot;A mountain landscape at sunset&quot;, &amp;params);

    char filename[64];
    snprintf(filename, sizeof(filename), &quot;landscape_%d.png&quot;, i);
    flux_image_save(img, filename);
    flux_image_free(img);
}

flux_free(ctx);"><pre><span>flux_ctx</span> <span>*</span><span>ctx</span> <span>=</span> <span>flux_load_dir</span>(<span>"flux-klein-model"</span>);
<span>flux_params</span> <span>params</span> <span>=</span> <span>FLUX_PARAMS_DEFAULT</span>;
<span>params</span>.<span>width</span> <span>=</span> <span>256</span>;
<span>params</span>.<span>height</span> <span>=</span> <span>256</span>;

<span>/* Generate 5 variations with different seeds */</span>
<span>for</span> (<span>int</span> <span>i</span> <span>=</span> <span>0</span>; <span>i</span> <span>&lt;</span> <span>5</span>; <span>i</span><span>++</span>) {
    <span>flux_set_seed</span>(<span>1000</span> <span>+</span> <span>i</span>);

    <span>flux_image</span> <span>*</span><span>img</span> <span>=</span> <span>flux_generate</span>(<span>ctx</span>, <span>"A mountain landscape at sunset"</span>, <span>&amp;</span><span>params</span>);

    <span>char</span> <span>filename</span>[<span>64</span>];
    <span>snprintf</span>(<span>filename</span>, <span>sizeof</span>(<span>filename</span>), <span>"landscape_%d.png"</span>, <span>i</span>);
    <span>flux_image_save</span>(<span>img</span>, <span>filename</span>);
    <span>flux_image_free</span>(<span>img</span>);
}

<span>flux_free</span>(<span>ctx</span>);</pre></div>
<p dir="auto">Note: The text encoder (~8GB) is automatically released after the first generation to save memory. It reloads automatically if you use a different prompt.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Error Handling</h3><a id="user-content-error-handling" aria-label="Permalink: Error Handling" href="#error-handling"></a></p>
<p dir="auto">All functions that can fail return NULL on error. Use <code>flux_get_error()</code> to get a description:</p>
<div dir="auto" data-snippet-clipboard-copy-content="flux_ctx *ctx = flux_load_dir(&quot;nonexistent-model&quot;);
if (!ctx) {
    fprintf(stderr, &quot;Error: %s\n&quot;, flux_get_error());
    /* Prints something like: &quot;Failed to load VAE - cannot generate images&quot; */
    return 1;
}"><pre><span>flux_ctx</span> <span>*</span><span>ctx</span> <span>=</span> <span>flux_load_dir</span>(<span>"nonexistent-model"</span>);
<span>if</span> (!<span>ctx</span>) {
    <span>fprintf</span>(<span>stderr</span>, <span>"Error: %s\n"</span>, <span>flux_get_error</span>());
    <span>/* Prints something like: "Failed to load VAE - cannot generate images" */</span>
    <span>return</span> <span>1</span>;
}</pre></div>
<p dir="auto"><h3 tabindex="-1" dir="auto">API Reference</h3><a id="user-content-api-reference" aria-label="Permalink: API Reference" href="#api-reference"></a></p>
<p dir="auto"><strong>Core functions:</strong></p>
<div dir="auto" data-snippet-clipboard-copy-content="flux_ctx *flux_load_dir(const char *model_dir);   /* Load model, returns NULL on error */
void flux_free(flux_ctx *ctx);                     /* Free all resources */

flux_image *flux_generate(flux_ctx *ctx, const char *prompt, const flux_params *params);
flux_image *flux_img2img(flux_ctx *ctx, const char *prompt, const flux_image *input,
                          const flux_params *params);"><pre><span>flux_ctx</span> <span>*</span><span>flux_load_dir</span>(<span>const</span> <span>char</span> <span>*</span><span>model_dir</span>);   <span>/* Load model, returns NULL on error */</span>
<span>void</span> <span>flux_free</span>(<span>flux_ctx</span> <span>*</span><span>ctx</span>);                     <span>/* Free all resources */</span>

<span>flux_image</span> <span>*</span><span>flux_generate</span>(<span>flux_ctx</span> <span>*</span><span>ctx</span>, <span>const</span> <span>char</span> <span>*</span><span>prompt</span>, <span>const</span> <span>flux_params</span> <span>*</span><span>params</span>);
<span>flux_image</span> <span>*</span><span>flux_img2img</span>(<span>flux_ctx</span> <span>*</span><span>ctx</span>, <span>const</span> <span>char</span> <span>*</span><span>prompt</span>, <span>const</span> <span>flux_image</span> <span>*</span><span>input</span>,
                          <span>const</span> <span>flux_params</span> <span>*</span><span>params</span>);</pre></div>
<p dir="auto"><strong>Image handling:</strong></p>
<div dir="auto" data-snippet-clipboard-copy-content="flux_image *flux_image_load(const char *path);     /* Load PNG or PPM */
int flux_image_save(const flux_image *img, const char *path);  /* 0=success, -1=error */
flux_image *flux_image_resize(const flux_image *img, int new_w, int new_h);
void flux_image_free(flux_image *img);"><pre><span>flux_image</span> <span>*</span><span>flux_image_load</span>(<span>const</span> <span>char</span> <span>*</span><span>path</span>);     <span>/* Load PNG or PPM */</span>
<span>int</span> <span>flux_image_save</span>(<span>const</span> <span>flux_image</span> <span>*</span><span>img</span>, <span>const</span> <span>char</span> <span>*</span><span>path</span>);  <span>/* 0=success, -1=error */</span>
<span>flux_image</span> <span>*</span><span>flux_image_resize</span>(<span>const</span> <span>flux_image</span> <span>*</span><span>img</span>, <span>int</span> <span>new_w</span>, <span>int</span> <span>new_h</span>);
<span>void</span> <span>flux_image_free</span>(<span>flux_image</span> <span>*</span><span>img</span>);</pre></div>
<p dir="auto"><strong>Utilities:</strong></p>
<div dir="auto" data-snippet-clipboard-copy-content="void flux_set_seed(int64_t seed);                  /* Set RNG seed for reproducibility */
const char *flux_get_error(void);                  /* Get last error message */
void flux_release_text_encoder(flux_ctx *ctx);     /* Manually free ~8GB (optional) */"><pre><span>void</span> <span>flux_set_seed</span>(<span>int64_t</span> <span>seed</span>);                  <span>/* Set RNG seed for reproducibility */</span>
<span>const</span> <span>char</span> <span>*</span><span>flux_get_error</span>(<span>void</span>);                  <span>/* Get last error message */</span>
<span>void</span> <span>flux_release_text_encoder</span>(<span>flux_ctx</span> <span>*</span><span>ctx</span>);     <span>/* Manually free ~8GB (optional) */</span></pre></div>
<p dir="auto"><h3 tabindex="-1" dir="auto">Parameters</h3><a id="user-content-parameters" aria-label="Permalink: Parameters" href="#parameters"></a></p>
<div dir="auto" data-snippet-clipboard-copy-content="typedef struct {
    int width;              /* Output width in pixels (default: 256) */
    int height;             /* Output height in pixels (default: 256) */
    int num_steps;          /* Denoising steps, use 4 for klein (default: 4) */
    float guidance_scale;   /* CFG scale, use 1.0 for klein (default: 1.0) */
    int64_t seed;           /* Random seed, -1 for random (default: -1) */
    float strength;         /* img2img only: 0.0-1.0 (default: 0.75) */
} flux_params;

/* Initialize with sensible defaults */
#define FLUX_PARAMS_DEFAULT { 256, 256, 4, 1.0f, -1, 0.75f }"><pre><span>typedef</span> <span>struct</span> {
    <span>int</span> <span>width</span>;              <span>/* Output width in pixels (default: 256) */</span>
    <span>int</span> <span>height</span>;             <span>/* Output height in pixels (default: 256) */</span>
    <span>int</span> <span>num_steps</span>;          <span>/* Denoising steps, use 4 for klein (default: 4) */</span>
    <span>float</span> <span>guidance_scale</span>;   <span>/* CFG scale, use 1.0 for klein (default: 1.0) */</span>
    <span>int64_t</span> <span>seed</span>;           <span>/* Random seed, -1 for random (default: -1) */</span>
    <span>float</span> <span>strength</span>;         <span>/* img2img only: 0.0-1.0 (default: 0.75) */</span>
} <span>flux_params</span>;

<span>/* Initialize with sensible defaults */</span>
<span>#define</span> <span>FLUX_PARAMS_DEFAULT</span> { 256, 256, 4, 1.0f, -1, 0.75f }</pre></div>
<p dir="auto"><h2 tabindex="-1" dir="auto">License</h2><a id="user-content-license" aria-label="Permalink: License" href="#license"></a></p>
<p dir="auto">MIT</p>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Show HN: Lume 0.2 – Build and Run macOS VMs with unattended setup (134 pts)]]></title>
            <link>https://cua.ai/docs/lume/guide/getting-started/introduction</link>
            <guid>46670181</guid>
            <pubDate>Sun, 18 Jan 2026 17:53:21 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://cua.ai/docs/lume/guide/getting-started/introduction">https://cua.ai/docs/lume/guide/getting-started/introduction</a>, See on <a href="https://news.ycombinator.com/item?id=46670181">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="nd-page"><header id="nd-tocnav" data-state="closed"></header><article><div><p>Introduction to Lume - the macOS VM CLI and framework</p></div><div><p>Lume is a VM runtime for building AI agents, running CI/CD pipelines, and automating macOS. It uses Apple's native Virtualization Framework to run macOS and Linux VMs at near-native speed on Apple Silicon.</p>
<div><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="10"></circle><path d="M12 16v-4"></path><path d="M12 8h.01"></path></svg><div><p>MIT License</p><p>Lume is open-source and MIT licensed. If you find it useful, we'd appreciate a <a href="https://github.com/trycua/cua" rel="noreferrer noopener" target="_blank">star on GitHub</a>!</p></div></div>
<div><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="10"></circle><path d="M12 16v-4"></path><path d="M12 8h.01"></path></svg><div><p>Cloud macOS Sandboxes</p><p>We're piloting a managed service for customers who want to run cloud macOS sandboxes for CI/CD and agent workloads. <a href="https://cal.com/cua/cua-demo?overlayCalendar=true" rel="noreferrer noopener" target="_blank">Book a demo</a> if you're interested.</p></div></div>
<figure dir="ltr" tabindex="-1"><div role="region" tabindex="0"><pre><code><span><span>lume</span><span> create</span><span> test-vm</span><span> --os</span><span> macos</span><span> --ipsw</span><span> latest</span></span>
<span><span>lume</span><span> run</span><span> test-vm</span></span></code></pre></div></figure>
<p>A single binary with an HTTP API. Create a VM, run it headlessly, control it programmatically.</p>

<p><img alt="Lume Architecture" loading="lazy" width="1892" height="1604" decoding="async" data-nimg="1" sizes="(max-width: 768px) 100vw, (max-width: 1200px) 70vw, 900px" srcset="https://cua.ai/docs/_next/image?url=%2Fdocs%2F_next%2Fstatic%2Fmedia%2Flume-architecture.f28fc4a4.png&amp;w=640&amp;q=75 640w, https://cua.ai/docs/_next/image?url=%2Fdocs%2F_next%2Fstatic%2Fmedia%2Flume-architecture.f28fc4a4.png&amp;w=750&amp;q=75 750w, https://cua.ai/docs/_next/image?url=%2Fdocs%2F_next%2Fstatic%2Fmedia%2Flume-architecture.f28fc4a4.png&amp;w=828&amp;q=75 828w, https://cua.ai/docs/_next/image?url=%2Fdocs%2F_next%2Fstatic%2Fmedia%2Flume-architecture.f28fc4a4.png&amp;w=1080&amp;q=75 1080w, https://cua.ai/docs/_next/image?url=%2Fdocs%2F_next%2Fstatic%2Fmedia%2Flume-architecture.f28fc4a4.png&amp;w=1200&amp;q=75 1200w, https://cua.ai/docs/_next/image?url=%2Fdocs%2F_next%2Fstatic%2Fmedia%2Flume-architecture.f28fc4a4.png&amp;w=1920&amp;q=75 1920w, https://cua.ai/docs/_next/image?url=%2Fdocs%2F_next%2Fstatic%2Fmedia%2Flume-architecture.f28fc4a4.png&amp;w=2048&amp;q=75 2048w, https://cua.ai/docs/_next/image?url=%2Fdocs%2F_next%2Fstatic%2Fmedia%2Flume-architecture.f28fc4a4.png&amp;w=3840&amp;q=75 3840w" src="https://cua.ai/docs/_next/image?url=%2Fdocs%2F_next%2Fstatic%2Fmedia%2Flume-architecture.f28fc4a4.png&amp;w=3840&amp;q=75"></p>
<p>You can use Lume directly via CLI, or run <code>lume serve</code> to expose an HTTP API for programmatic access. The <a href="https://cua.ai/docs/cua/reference/computer-sdk">Computer SDK</a> uses this API to automate macOS interactions.</p>

<p>Lume is a thin layer over Apple's <a href="https://developer.apple.com/documentation/virtualization" rel="noreferrer noopener" target="_blank">Virtualization Framework</a>, which provides hardware-accelerated virtualization on Apple Silicon. This gives you:</p>
<ul>
<li><strong>Native speed</strong> — CPU instructions execute directly via hardware virtualization</li>
<li><strong>Paravirtualized graphics</strong> — Basic GPU support via Apple's virtualization layer (limited to GPU Family 5)</li>
<li><strong>Efficient storage</strong> — Sparse disk files only consume actual usage, not allocated size</li>
<li><strong>Rosetta 2 support</strong> — Run x86 Linux binaries in ARM Linux VMs</li>
<li><strong>Automated golden images</strong> — Go from IPSW to fully configured macOS VM without manual intervention</li>
<li><strong>Registry support</strong> — Pull and push VM images from GHCR or GCS registries</li>
</ul>

<p><strong>Testing across macOS versions</strong> — Spin up a VM with a specific macOS version, test your software, tear it down. No need to maintain multiple physical machines.</p>
<p><strong>Automating macOS tasks</strong> — Combine Lume with <a href="https://cua.ai/docs/lume/guide/fundamentals/unattended-setup">Unattended Setup</a> to create pre-configured VMs. The setup automation uses VNC and OCR to click through the Setup Assistant without manual intervention.</p>
<p><strong>Running CI/CD locally</strong> — Test your macOS builds in isolated VMs before pushing to remote CI. The <code>--no-display</code> flag runs VMs headlessly.</p>
<p><strong>Sandboxing risky operations</strong> — Need to test untrusted software or destructive scripts? Run them in a VM, then delete it. Clone a known-good VM to reset to a clean state instantly.</p>
<p><strong>Building AI agents</strong> — Lume powers the <a href="https://cua.ai/docs/cua/reference/computer-sdk">Cua Computer SDK</a>, providing VMs that AI models can interact with through screenshots and input simulation.</p>
<div><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="10"></circle><path d="M12 16v-4"></path><path d="M12 8h.01"></path></svg><div><p>Used by Anthropic</p><p>Apple's Virtualization Framework—the same technology Lume is built on—powers <a href="https://support.claude.com/en/articles/13345190-getting-started-with-cowork" rel="noreferrer noopener" target="_blank">Claude Cowork</a>, Anthropic's sandboxed environment for Claude Code. It downloads a Linux root filesystem and boots it in an isolated VM where Claude can safely execute commands without access to your broader system.</p></div></div>

<p>Lume requires Apple Silicon—it won't work on Intel Macs or other platforms.</p>

<p>Ready to try it? <a href="https://cua.ai/docs/lume/guide/getting-started/installation">Install Lume</a> and create your first VM in the <a href="https://cua.ai/docs/lume/guide/getting-started/quickstart">Quickstart</a>.</p></div></article><div id="nd-toc"><h3 id="toc-title"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M15 18H3"></path><path d="M17 6H3"></path><path d="M21 12H3"></path></svg>On this page</h3></div></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Gaussian Splatting – A$AP Rocky Helicopter Music Video (692 pts)]]></title>
            <link>https://radiancefields.com/a-ap-rocky-releases-helicopter-music-video-featuring-gaussian-splatting</link>
            <guid>46670024</guid>
            <pubDate>Sun, 18 Jan 2026 17:40:55 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://radiancefields.com/a-ap-rocky-releases-helicopter-music-video-featuring-gaussian-splatting">https://radiancefields.com/a-ap-rocky-releases-helicopter-music-video-featuring-gaussian-splatting</a>, See on <a href="https://news.ycombinator.com/item?id=46670024">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-framer-name="Content" id="1migl9j" data-framer-component-type="RichTextContainer"><p>Believe it or not, A$AP Rocky is a huge fan of radiance fields.</p><p>Yesterday, when A$AP Rocky released the music video for <em>Helicopter</em>, many viewers focused on the chaos, the motion, and the unmistakable early MTV energy of the piece. What’s easier to miss, unless you know what you’re looking at, is that nearly every human performance in the video was captured volumetrically and rendered as dynamic splats.</p><p>I spoke with <!--$--><a href="https://evercoast.com/" target="_blank" rel="noopener">Evercoast</a><!--/$-->, the team responsible for capturing the performances, as well as Chris Rutledge, the project’s CG Supervisor at <!--$--><a href="https://vimeo.com/grinmachine" target="_blank" rel="noopener">Grin Machine</a><!--/$-->, and Wilfred Driscoll of WildCapture and <!--$--><a href="https://fitsu.ai/" target="_blank" rel="noopener">Fitsū.ai</a><!--/$-->, to understand how <em>Helicopter</em> came together and why this project represents one of the most ambitious real world deployments of dynamic gaussian splatting in a major music release to date.</p><p>The decision to shoot <em>Helicopter</em> volumetrically wasn’t driven by technology for technology’s sake. According to the team, the director Dan Strait approached the project in July with a clear creative goal to capture human performance in a way that would allow radical freedom in post-production. This would have been either impractical or prohibitively expensive using conventional filming and VFX pipelines.</p><p>Chris told me he’d been tracking volumetric performance capture for years, fascinated by emerging techniques that could enable visuals that simply weren’t possible before. Two years ago, he began pitching the idea to directors in his circle, including Dan, as a “someday” workflow. When Dan came back this summer and said he wanted to use volumetric capture for the entire video, the proliferation of gaussian splatting enabled them to take it on.</p><div data-width="fill"><article role="presentation"><img decoding="async" src="https://i.ytimg.com/vi_webp/g1-46Nu3HxQ/sddefault.webp"></article></div><p>The aesthetic leans heavily into kinetic motion. Dancers colliding, bodies suspended in midair, chaotic fight scenes, and performers interacting with props that later dissolve into something else entirely. Every punch, slam, pull-up, and fall you see was physically performed and captured in 3D.</p><p>Almost every human figure in the video, including Rocky himself, was recorded volumetrically using Evercoast’s system. It’s all real performance, preserved spatially.</p><p>This is not the first time that A$AP Rocky has featured a radiance field in one of his music videos. The 2023 music video for <em>Shittin’ Me</em> featured several NeRFs and even the GUI for Instant-NGP, which you can spot throughout the piece.</p><div data-width="fill"><article role="presentation"><img decoding="async" src="https://i.ytimg.com/vi_webp/Oh8b_P7_AuA/sddefault.webp"></article></div><p>The primary shoot for <em>Helicopter</em> took place in August in Los Angeles. Evercoast deployed a 56 camera RGB-D array, synchronized across two Dell workstations. Performers were suspended from wires, hanging upside down, doing pull-ups on ceiling-mounted bars, swinging props, and performing stunts, all inside the capture volume.</p><p>Scenes that appear surreal in the final video were, in reality, grounded in very physical setups, such as wooden planks standing in for helicopter blades, real wire rigs, and real props. The volumetric data allowed those elements to be removed, recomposed, or entirely recontextualized later without losing the authenticity of the human motion.</p><p>Over the course of the shoot, Evercoast recorded more than 10 terabytes of raw data, ultimately rendering roughly 30 minutes of final splatted footage, exported as PLY sequences totaling around one terabyte.</p><p>That data was then brought into Houdini, where the post production team used CG Nomads GSOPs for manipulation and sequencing, and OTOY’s OctaneRender for final rendering. Thanks to this combination, the production team was also able to relight the splats.</p><p>One of the more powerful aspects of the workflow was Evercoast’s ability to preview volumetric captures at multiple stages. The director could see live spatial feedback on set, generate quick mesh based previews seconds after a take, and later review fully rendered splats through Evercoast’s web player before downloading massive PLY sequences for Houdini.</p><p>In practice, this meant creative decisions could be made rapidly and cheaply, without committing to heavy downstream processing until the team knew exactly what they wanted. It’s a workflow that more closely resembles simulation than traditional filming.</p><p>Chris also discovered that Octane’s Houdini integration had matured, and that Octane’s early splat support was far enough along to enable relighting. According to the team, the ability to relight splats, introduce shadowing, and achieve a more dimensional “3D video” look was a major reason the final aesthetic lands the way it does.</p><p>The team also used Blender heavily for layout and previs, converting splat sequences into lightweight proxy caches for scene planning. Wilfred described how WildCapture’s internal tooling was used selectively to introduce temporal consistency. In his words, the team derived primitive pose estimation skeletons that could be used to transfer motion, support collision setups, and allow Houdini’s simulation toolset to handle rigid body, soft body, and more physically grounded interactions.</p><p>One recurring reaction to the video has been confusion. Viewers assume the imagery is AI-generated. According to Evercoast, that couldn’t be further from the truth. Every stunt, every swing, every fall was physically performed and captured in real space. What makes it feel synthetic is the freedom volumetric capture affords. You aren’t limited by the camera’s composition. You have free rein to explore, reposition cameras after the fact, break spatial continuity, and recombine performances in ways that 2D simply can’t.</p><p>In other words, radiance field technology isn’t replacing reality. It’s preserving everything.</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Statement by Denmark, Finland, France, Germany, Netherlands, Norway, Sweden, UK (217 pts)]]></title>
            <link>https://www.bundesregierung.de/breg-de/aktuelles/statement-by-denmark-finland-france-germany-the-netherlands-norway-sweden-and-the-united-kingdom-2403016</link>
            <guid>46669945</guid>
            <pubDate>Sun, 18 Jan 2026 17:33:25 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.bundesregierung.de/breg-de/aktuelles/statement-by-denmark-finland-france-germany-the-netherlands-norway-sweden-and-the-united-kingdom-2403016">https://www.bundesregierung.de/breg-de/aktuelles/statement-by-denmark-finland-france-germany-the-netherlands-norway-sweden-and-the-united-kingdom-2403016</a>, See on <a href="https://news.ycombinator.com/item?id=46669945">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>As members of NATO, we are committed to strengthening Arctic security as a shared transatlantic interest. The pre-coordinated Danish exercise „Arctic Endurance“ conducted with Allies, responds to this necessity. It poses no threat to anyone.</p><p>We stand in full solidarity with the Kingdom of Denmark and the people of Greenland. Building on the process begun last week, we stand ready to engage in a dialogue based on the principles of sovereignty and territorial integrity that we stand firmly behind.</p><p>Tariff threats undermine transatlantic relations and risk a dangerous downward spiral. We will continue to stand united and coordinated in our response. We are committed to upholding our sovereignty.</p><p><strong>Höflichkeitsübersetzung:</strong></p><p><strong>Erklärung von Dänemark, Finnland, Frankreich, Deutschland, Niederlande, Norwegen, Schweden und des Vereinigten Königreichs</strong></p><p>Als Alliierte der NATO sind wir der Stärkung der Sicherheit in der Arktis verpflichtet. Dies ist ein gemeinsames transatlantisches Interesse. Die von Dänemark koordinierte Übung „Arctic Endurance“, welche gemeinsam mit Alliierten durchgeführt wird, ist eine Antwort auf die Notwendigkeit größerer Sicherheit in der Arktis. Die Übung stellt für niemanden eine Bedrohung dar.</p><p>Wir stehen in voller Solidarität an der Seite des Königreichs Dänemark und der Bevölkerung Grönlands. Aufbauend auf dem letzte Woche begonnenen Prozess sind wir bereit in einen Dialog einzutreten, auf Grundlage der Prinzipien der Souveränität und territorialen Integrität. Wir stehen fest zu diesen Prinzipien.</p><p>Zolldrohungen untergraben die transatlantischen Beziehungen und bergen das Risiko einer Eskalation. Wir werden weiterhin geeint und koordiniert reagieren. Wir sind entschlossen, unsere Souveränität zu wahren.</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Sins of the Children (162 pts)]]></title>
            <link>https://asteriskmag.com/issues/07/sins-of-the-children</link>
            <guid>46669663</guid>
            <pubDate>Sun, 18 Jan 2026 17:08:58 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://asteriskmag.com/issues/07/sins-of-the-children">https://asteriskmag.com/issues/07/sins-of-the-children</a>, See on <a href="https://news.ycombinator.com/item?id=46669663">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>When we reached the weather station it was so comprehensively trashed you’d think it’d been dropped from orbit. Torn apart and the pieces stomped on, the edges corrugated with dents and corroded with fluids. Something on this planet really didn’t want us to know when it was going to rain.</p><p>“This is coming out of the use-budget,” Greffin said mournfully. She worked in Resources, liaising with the orbiting <em>Garveneer</em> to get what we needed. And we’d already <em>needed</em> plenty to get ourselves set up planetside.</p><p>“What’s our culprit and how do we kill it?” Merrit asked.</p><p>The three of us had set the station up three days before and somehow it had riled the locals. Probably the sonic and radio chatter from using bounce-back to map meteorological systems. But nothing we’d seen on all of Chelicer 14d was big or aggressive enough to do this damage.</p><p>I had my slate out to review our evolving catalog of Chelicer xenofauna. Merrit was on his haunches, studying the shrapnel; Greffin had a link to base camp at the farms, going over inventory to see what we could repurpose. Around us and the wreckage stretched the local scrub. Sedentary life on Chelicer was either low and spiny or tall and thin with a sort of puffball arrangement at the top. The land — the world — was dry, the ecosystem impoverished and short on species. My unfinished xenobio report went long on the idea that Chelicer had been lush in the past, and we’d arrived to find what had stabilized out of a catastrophic dry spell, or maybe some serious solar flare activity. There were no great forests to give cover to alien tigers. On Chelicer nothing grew past a shrub. One meter for the spiny stuff, two for the puffball poles. And the weather station had been up on high ground, ten klicks’ visibility in any direction. We were <em>safe</em>.</p><p>I heard a far-off <em>chunk</em>. A mechanical sound that — in the second’s pause before it impacted — I didn’t even connect with <em>life</em>.</p></div><div><p>The thing that came down right beside us was three meters high with a massive articulated body. A bug, really, Chelicer style. Eight crooked legs out from a central hub like all the mobile life here had, but most of what we’d seen was gracile, delicate, and came up to your waist. Even the Farmers — which we’d pegged as the most advanced species around — were only a meter and a half tall, and most of that was stilting limbs. This thing was <em>not</em> gracile. Every segment and joint of it was ridgy, armored, and spiky. It was dun and khaki like the planet’s dust, but too big to have hidden anywhere nearby, towering over the scrub. There were spread vanes like sails projecting from its back, but it <em>couldn’t</em> have flown under organic power. It must have weighed five tons.</p><p>We just stared. In that moment, when we could have run or called for help<em>, </em>we goggled at it. The stalked globes of its eyes looked back, devoid of living connection. A vast armored monster, airdropped from nowhere.</p><p>I saw the motion, off on a neighboring hillside. There was a second monster out there, surprisingly hard to spot. It hunkered down, drawing its limbs in.</p><p><em>Chunk!</em> That same sound. The thing on the hillside was gone.</p><p>A second later it was on us, coming down right in front of Merrit. I thought of mechanical advantage, the tricks you could do with a rigid exoskeleton. I thought of fleas, but on an absurd macro scale. It <em>jumped</em> and came down on eight legs that must have been shock absorbers par excellence.</p><p>Chelicer life doesn’t quite have a front or a back, built around that hub of legs. The mouth is on the underside and that’s what this thing tilted at Merrit.&nbsp;</p><p>I’d dissected some of the Farmers and they had an arrangement like eight knuckly stumps to mumble over their food with. These new arrivals had a setup like a sphincter made of scissor blades and nutcrackers, more an industrial process than biology. We’d seen what those tools had done to the weather station already. Right then we were more concerned with what it did to Merrit.&nbsp;</p><p>He was just crouched there, midway through sifting the wreckage. The monster took instant offense. Its mouthparts extended out and just … macerated him. Chopped and crushed so that in a heartbeat there was nothing left that looked remotely human, just a wadded bloody ball of flesh and splintered bone and rags of suit.&nbsp;</p><p>Greffin and I started shooting. Our guns were so badly printed you could see the mold lines. They chewed up their own mass for ammo in a spray of flechettes. True to our miserly resource budget, most of that barrage just slanted off the things’ carapaces, and I knew we were both going to follow Merrit into extinction, carved up and spat out with alien contempt. Except then Greffin hit a joint, and one monster was suddenly down a leg. That, apparently, was enough. We watched them ratchet down for takeoff, still shrugging off our fire, then ping upward. I recorded the flight of one, desperately trying to keep it in my field of vision. Without that, who’d believe us? Alien mega-fleas utilizing sheer mechanical tension to jump a half kilometer at a time.</p><p>We bagged what was left of Merrit. And I grabbed the leg when the skimmer came for evac. Because it was proof that here be monsters.</p><p>The Farmers had been the tipping point, the reason to establish a human presence planetside. Yes, 14d was a unique world, unknown alien ecosphere, all that. But if it hadn’t held anything <em>useful</em> then the <em>Garveneer</em> would have focused elsewhere in the Chelicer system. And if the world had only offered mineral wealth, we’d have a robot mining operation stripping the place instead. All that unique ecosphere would have been flensed from the planet’s surface as an incidental side effect of our efforts. But on this world, the valuable thing <em>was</em> the biology, which needed more finesse. A human presence on the ground. Meaning a whole team of us thawed off the shelves and given this chance to justify our existence on the payroll.</p><p>Which was now under threat, as were we. We evacuated back to the farms with our grisly souvenirs.</p><p>The Concerns that have spearheaded humanity’s expansion from star to star have refined an efficient system for exploiting exoplants. When a Concern builds farms, that means a continent’s span of identical fields, robot tended. Everything growing and being harvested at an accelerated rate, processed and dried for minimal weight in transit. Turned into the Ship’s Reconstitute we’re all thoroughly sick of eating. The stuff from the Chelicer farms can look mighty good in comparison, which is a shame because a bite would kill you stone dead. But then they’re not <em>our</em> farms. They’re a thing the locals were doing long before we arrived.</p><p>The locals — Species 11 — are like spiders only ganglier. Four stilty legs interspersed with four spindly arms, and a hub of a body in the middle, high enough to come up to your waist. We called them Farmers from the start because it’s what they do: tend great stretches of this one crop. Not even a very exciting-looking crop, sort of a warty purple potato-looking thing, except it turns out to be superefficient at concentrating the elements in the crappy soil they’ve got here. Many of which elements are useful to <em>us</em>, for our superconductors and our computational substructures and all that good stuff. When we discovered <em>that</em>, you can be damn sure we moved in and took possession double time. Built our processing plant and started making off with a big chunk of the crop.&nbsp;</p><p>What did the locals think of this? My professional xenobiologist’s opinion was they didn’t think a damn thing. They didn’t <em>react</em> at all. The whole farming schtick they had going was just instinct, like ants, only they didn’t even <em>defend</em> anything. When they got in the way of the machines, they got chewed up. We thought at the time they’d evolved with no natural predators.&nbsp;&nbsp;</p><p>We sure as hell were wrong about that.</p><p>Greffin and I made our reports. The dozen on-planet crew came to commiserate, meaning get the gory details. We told everyone to carry a gun and know the emergency drill. Chelicer had an apex predator we hadn’t known about. After which cautionary tales, I was left facing up to the mission’s biggest pain in my ass, namely FenJuan.</p><p>FenJuan had screwed up royally on some past previous assignments, was my guess. They’d been something senior, and something had gone south in expensive ways. Meaning FenJuan slumming it on our team was an invisible mark against every one of us, because their personnel file came with baggage. Worse, they were my immediate colleague in biosciences, the two of us responsible for figuring out the local biochemistry.</p><p>“Stort,” they addressed me, frosty as always.</p><p>“Fen,” I replied with just as much love.</p><p>“My samples?” they said. Because they didn’t <em>do</em> fieldwork, just like they didn’t <em>do </em>basic human interaction, just sat at base camp and bitched.</p><p>And I’d <em>given</em> them samples previously. I’d cut a chunk out of a dozen critters on four other excursions and brought them back. And I’d just seen a work colleague turned to paste by some local monster-bug neither my nor FenJuan’s science had accounted for. But in the Concerns you don’t get time off for inefficient foibles like grief or trauma, so I made do with snarling at FenJuan that they’d had all the damn samples they were getting from me and if that wasn’t good enough then maybe <em>they</em> were the problem.</p><p>“When I say, ‘Get me a selection so I can run comparative studies,’” they snapped, “I do not mean just go snip bits off the Farmers and call the job done. A man is dead because we don’t understand the world here.”</p><p>Which was turning it back on me, making it <em>my </em>fault. And which wasn’t true to boot. I told them that if they were having difficulty distinguishing between samples maybe they didn’t have the basic analytical skills required for the task. The structures that they’d pegged as the local equivalent of a genome were probably just some essential organelle that every damn beastie possessed, and the real genome-equivalent had gone completely under FenJuan’s radar.&nbsp;</p><p>“You want a sample?” I asked FenJuan. “For real? Cut your own out of this. You can be absolutely <em>sure</em> it doesn’t come from a Farmer.” And I pointed them at the leg, the one we’d shot off the big bouncing bastard.</p><p>Shouting at people works, when you’re not allowed time off to process death. Works remarkably well, if it’s the only outlet you’ve got. Just as well. There would be plenty of both shouting and death in everyone’s future.</p></div><div><p>I liked to sit outside to complete my reports. Chelicer has good sun, if you’ve had the treatments to ward off skin damage. I wrote up my thoughts on our giant killer flea problem, watching the Farmers pick their way across the vast fields of “Species 13 Resource” as per official Concern designation, or the Chelicetato as our vulgar parlance had it. They groped over each tuber in turn, then pissed out the right chemicals to help the things grow. The Farmers were a remarkable find. We’d have gone way over budget making robot gardeners even half as efficient. Worth fighting off a few giant bugs for.</p><p>Past the processing plants, the elevator cable stretched into forever. Up there was the <em>Garveneer</em>, our home away from home, taking every processed tuber we could hoik out of the ground. And our little outpost here was just the beginning. There were tens of millions of Farmers all over the planet, wherever the conditions suited their crop, all ready to become part of the industrial agriculture of the Concerns. We’d struck the jackpot when we surveyed Chelicer 14d.</p><p>Greffin had been going through recent survey images, looking for monsters. She sent me what she found. Holes like burrows I could have driven a ground-car into, written off as geological because nothing we’d seen could have made them. Now we knew better. Maybe the monster fleas had emerged only recently. Maybe there was a cicada thing going on, killer flea season. We made some recommendations for the next security meeting, and I did a tour of the turret guns that had been put in with the processing plant and never needed since.</p><p>Doing that put me in FenJuan’s orbit and I braced myself for the sandpaper of their company. They were deep in analyzing the giant leg, though, or thin-sliced samples thereof. They had a few dead Farmers too — there were plenty of aimless ones not working, now we’d harvested their plots.&nbsp;</p><p>“Stort,” they said, not the usual bark, but thoughtful. On the screens was a variety of different views of microscopic-scale Chelicer cell structure. The spiral-walled cones that FenJuan reckoned were hereditary information, and that they’d been unspooling and trying to decode. Sections had been flagged up on each, identical one to another.</p><p>“Junk DNA,” I said, and waited for their usual invective. It didn’t come, though. FenJuan actually nodded a little, a tiny iota of acknowledgment I’d said something that wasn’t stupid. And Earth life accumulates a certain amount of genetic junk, right? Stuff in the genome that’s been switched off, acquired from bacteria, or from benign transcription errors carried on down through the generations. But FenJuan reckoned something like 90 percent of any given beastie’s hereditary was this unused junk.&nbsp;</p><p>I wanted to say they were imagining things. I wanted to say it was a crap planet with crap aliens who had crap hereditary code, and us coming along to exploit them was the best thing that could have happened. That was how my encounters with FenJuan generally went. It was basically entertainment for the rest of the team.</p><p>I didn’t say any of that. FenJuan and I looked at each other, not quite ready to bury the hatchet, but maybe agreeing there was a bigger problem out there to save that mutual hatchet for.</p><p>The attack came the next day, and we weren’t prepared.</p><p>I heard the sound, distant, echoing across flat farmland from the dry hills. <em>Chunk.</em> For two whole seconds I was thinking some piece of machinery had gone wrong and how that was someone else’s problem. And then the first of them came down, just like before. Crashing onto the roof of the processing plant hard enough to buckle the plastic composite. Leering over the edge like a gargoyle. I swear it was twice the size of the one that killed Merrit.&nbsp;</p><p>I was shouting. Most of us were shouting, but I still caught a rapid heavy drumroll underneath the human noise. <em>Chunkchunkchunkchunkchunkchunkchunk…</em></p><p>They started dropping down all round us. We were running for the plant, because it was the most reinforced building and that was the emergency drill. Someone got word to the guns that their services were needed, and they started running friend-or-foe algorithms as a dozen human beings fled frantically into their arcs of fire.&nbsp;</p><p>One of the death-fleas crashed down in front of me, outspread sails barely slowing it. The articulation of its legs popped and twisted, absorbing the force of impact. A gun hammered chips out of its carapace. It lunged forward and snipped someone — one of the resources team I think — right in half with its scissor-blade face. I screamed and just about ducked through the shadow of its wings, not knowing if I’d get killed by its jaws or our own turrets.</p><p>Most of us got inside. They didn’t break in after us, but only because they didn’t try. Maybe object permanence isn’t a big thing on Chelicer: Once we were out of sight they seemed to forget us, through they chewed up all the guns.</p><p>Through our cameras, we got to see all the rest of what they did.</p><p>The Farmers, it turned out, <em>had</em> natural predators. Or they did in death-flea season. The monsters went to town, mostly on the Farmers that didn’t have anything left to farm, because they were just milling about. It was a massacre. And though they were weird alien spider guys, and you can’t really anthropomorphize that, we were all surprisingly cut up. It wasn’t that they were getting slaughtered out there. It was that they were <em>ours</em>. Our livelihood, our profit, the injection of resources that was earning us our wage-worth.</p><p>The massacre was monopolizing our attention, so the <em>real</em> damage went almost unnoticed until the earthquakelike convulsion that cracked every wall and trashed the processor floor. For a moment the problem was so <em>big</em> I couldn’t work out what had happened.</p><p>The elevator cable. Something about it — maybe just that it was the biggest thing around — had drawn their ire. A half dozen of the bastards had jumped to it, and those mouthparts had sawn through the supertensile material like it was string.</p><p>That took a long while to clear up. The actual cable was, after all, a long weighted strand that stretched a good way out of atmosphere and into space, and our actual <em>ship</em> was tethered at the halfway point. The <em>Garveneer</em> decoupled sharpish, you can be sure, and the vast length of the cable, cut free at its anchor, just vanished upward and sideways like the blade of God’s own scythe, on its way toward the outer reaches of the system.&nbsp;</p><p>We were stuck on-planet for some time, and we’d just had it demonstrated to us that the death-fleas were more than capable of carving their way into our compromised fortress if they wanted. Yes, our lords and masters in the Concern could shuttle us back to orbit, but that would require circumstances to fall into a very narrow gap indeed. That (1) it wasn’t worth continuing work on Chelicer 14d, and (2) it was actually worth retrieving us, rather than writing us off.&nbsp;</p><p>You can imagine the mood on the ground as we waited for their decision. We all gathered in the surviving common space and tried to convince ourselves we weren’t screwed. All except FenJuan, who didn’t <em>do</em> social graces, but just kept on studying the samples, which our remaining instruments couldn’t tell apart.</p><p>In the end, after they’d left us hanging for five days, there was a meeting. A handful of us on a staticky link to the chief director safe aboard the <em>Garveneer</em>. We were ready to be bawled out for a colossal loss of resources. That was the very best we thought we’d get. Instead, though, the Great Man was onside. The harvest from Chelicer had been very good indeed, solving a variety of rare elements shortages none of us knew the Concern had. This world we had worked on was the new hope of further human expansion. If only we could solve our little pest problem.</p><p>“We need to keep you folks safe,” said the director heartily. I looked over the recommendations. What they actually wanted to keep safe was the harvest, of course, which meant the Farmers. By then we had images from all over the planet of sporadic attacks on Farmer colonies. Death-fleas picking off the weak. Nothing as sustained as we’d seen at our base camp, but plainly a part of the circle of life in these parts.</p><p>“Our engineers up here are working on a new cable,” the Great Man told us. “But drones, too. Hunter drones. A whole fleet of them. We can justify the cost, given the potential resource revenue you’ve demonstrated. We’re proposing a global initiative to wipe out these things.”</p><p>“Wipe out the species, Director?” FenJuan clarified.&nbsp;</p><p>“Given the losses we’ve sustained and the clear threat to productivity, it’s the leading proposal. But I’m here for your thoughts.” That cheery smile of his. “Stort?”</p><p>“We’re obviously still adjusting our picture of the ecosphere to incorporate these things,” I said. “Given the low species count on-world, having an apex predator that only emerges sporadically makes some sense. What happens if we remove it? We can’t know. If this was a matter of wanting to preserve a working natural ecosystem I’d say there would be too many potential imbalances generated by cropping the top of the food chain. But.”</p><p>“But,” the director agreed. Because we were not, after all, interested in preserving the ecosystem. Just that part of it that worked for us.</p><p>FenJuan’s eyes were boring into me; I didn’t meet them. “Historically,” I said, “in a managed agricultural paradigm, removal of the top predators has been accomplished very profitably. Wolves, sheep, so on. It’s not as though we’re going to have a problem with some Farmer population explosion. If some other species booms, we can manage the consequences. I say do it.”</p><p>“Director,” FenJuan put in, unasked. “I have yet to come to any understanding of the biology or relationships involved here. There’s a commonality between species I can’t account for. This world plainly went through some severe ecological crisis that left a depauperate web of interdependence. We don’t know—”</p><p>On our screens, the director settled back in his big chair. “We know all we need to. What this world could be worth to us. How much damage those beasts are capable of doing. An <em>elevator cable</em>! We’ll conduct a localized culling in your region first. Barring any obvious consequence, we can roll it out to the rest of the world and follow up with plant and personnel wherever these Farmer creatures are to be found.” His smile was genuinely pleased, a man who’s going to see a nice bonus. “Well done, all. I know it’s been tough, but you’re heroes.”</p><p>The local cull, when it first happened, was something to watch. Drone footage wheeling and spinning as our machines found and chased the fleas. Killed them as they leapt through the air, as they landed thunderously on the ground, as they emerged from their burrows. Wiping them out within 200 klicks of the processing plant.&nbsp;</p><p>And nothing broke. The Farmers kept on farming. The crops grew. The crops that, at a cellular level, seemed weirdly indistinguishable from the things that tended them. FenJuan was raising issues every day, by then. Desperate to communicate how weird their results were. Not doing their job, because their <em>job</em> was solely and specifically to identify aspects of the local biochemistry that could be profitably exploited. Instead of which, they were going nuts about how every critter just seemed to have this enormous bolus of unused genetic-equivalent information, with a huge overlap between species. And I think they’d just about worked it out, except by then they’d made such a nuisance of themselves that FenJuan was the very last person our bosses up in orbit wanted to hear from. Did this discovery open up new vistas of planetary exploitation for our already profitable operation? No? Then pipe down and stop using up comms resources.</p><p>The people the director <em>did </em>want to hear from were designing and deploying the hunter-killers. Our expanded drone fleet was greenlit: hundreds of machines shipped downwell and let loose across the globe. Wherever they found the fleas, they destroyed them. We felt we were liberators. Whole populations of Farmers could live without those monstrous shadows falling on them. Yes, we were making a species extinct, but it wasn’t a <em>nice</em> species. We were already on the next phase of occupation, a 10-year building plan where we’d fill the planet with farms and processing plants, replicating our first outpost over and over until there wasn’t an inch of the world that wasn’t working for us.</p><p>A couple of years into our agricultural expansion, the cacti disappeared. Not cacti, obviously. Species 43 in the Concern bestiary, but cactus enough that the name had stuck. We had a look one morning and there just wasn’t any more of it left. I suggested maybe it had been living off some sort of death-flea by-products, though the timing seemed unusually lethargic for that kind of interaction. I ended up working alongside FenJuan, and we found drone footage of the cacti stuff getting up and running around, so that Species 43 turned out to be the larval-or-something form of Species 22, and we had to recalibrate the records.&nbsp;</p><p>At around the same time, the little hairy critters that were Species 38 rooted down and grew long spires with puffballs on them, making them actually Species 17. Half a year later our existing Species 11s lost their poles and became another sort of thing we’d already seen, and so on and so on. To most of us it was a curiosity. To FenJuan it was a crawling horror that I was starting to share. All their snapping, bitching at me for not <em>seeing</em>,<em> </em>and I’d just written it all off as someone pissed their Concern work record was full of demerits. Except they’d been right and I’d been wrong.</p><p>There were no more cacti. That was what scared FenJuan. We watched a wave of transformations. Each form turned into something else, but none of it turned into the cactuslike Species 43. Then it was something else, where our current batch just metamorphosed and there were no new ones. None at all, anywhere on Chelicer. The dry country became less and less inhabited as species after species vanished away.</p><p>Or not species. That was what FenJuan had been trying to understand. Developmental stages. Not a circle of life, but a life cycle.</p><p>Our prized cheliceratos, which had been putting out runners and new tubers happily for over a decade, were suddenly ambulatory one morning, sprouting a thicket of spindly legs and just giving up their life of being agricultural produce. <em>That </em>got people’s attention. Around the same time one weird round critter rooted down where the Farmers were and became the new Chelicetato crop, and the dumbest of our colleagues reckoned that was all OK then. FenJuan and I had stopped trying to raise the alarm, by then, because it obviously wasn’t going to help. Soon after, some buried fungal-looking thing we’d found no use for sprouted legs and became new Farmers. And the old farmers … died off. Wore out, natural causes. Leaving only the least dregs we’d left of their crop. From which a handful of stunted things crawled, devouring their own left-behind husks and the last corpses of their tenders. They were tiny, but we recognized them even as they began to wearily dig down into the parched, lifeless soil. Nascent fleas, entering that dormant part of their cycle from which they would emerge, at some future date, into a world devoid of anything that could sustain them. Behind them, the whole ecosystem of life stages had been rolled up. There was nothing left of it. They were the last.</p><p>&nbsp;As we had harvested and plundered, we had been watching a decade-long series of transformations. One that had definitively ended. Life on Chelicer vanished. Plant forms, bug forms, just about every macrobiological creature dying off one at a time and not being replaced by a new generation. As though death had asked them to form an orderly queue.&nbsp;</p><p>There had been a mass extinction in Chelicer’s past, FenJuan and I reckoned. Something that had killed off everything except a hardy species that inherited an utterly impoverished planetary biome. Colder at the poles, warmer at the equator, but barren, desperate. So, over the ages, that species had developed to exploit every last opportunity that the world had left to it, not through speciation but through adaption of its life cycle. Gathering the meager resources of the world, concentrating them in living forms that could be harvested in turn. Sedentary stages, mobile stages, squeezing every possible niche of everything that could be gained and then transforming into the next phase of its long and complex chain of shapes. A desperate ecosystem of one, harvesting and gathering and recycling, each stage into the next, surviving everything thrown at it. Except us, who came and severed a single link utterly and irrevocably. Cut one thread and watched the whole unravel over a mere decade.</p><p>FenJuan and I were last off the planet, on the final elevator car along with the last salvage from our farming operations. It was on us, we had been told. We were the biologists, and we should have seen it coming. And they were right; we should. But all that would have done was salve our professional pride. I don’t believe for a moment they’d have listened to us if we’d said <em>Stop</em>. <em>Stop</em> isn’t the way of the Concerns. <em>Stop</em> doesn’t meet quotas or hit targets.</p><p>We stepped into the elevator car, FenJuan and I. We looked back over a world unrelieved by messy, complicated stuff, such as life. A <em>failed commercial opportunity</em>, as the report would say.&nbsp;</p><p>I wanted to say something. Possibly <em>You were right</em>. But what good would it do? We were both going to be back on ice when we reached the ship, with personnel files so dire they’ll probably never thaw us out again. But, like the life of Chelicer, we’re not important, compared to the bigger picture of the Concerns and their expansion. We humans go on, world to world, star to star, making the universe our own. But on Chelicer there will only ever be dust.</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[The Nobel Prize and the Laureate Are Inseparable (290 pts)]]></title>
            <link>https://www.nobelpeaceprize.org/press/press-releases/the-nobel-prize-and-the-laureate-are-inseparable</link>
            <guid>46669404</guid>
            <pubDate>Sun, 18 Jan 2026 16:50:37 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.nobelpeaceprize.org/press/press-releases/the-nobel-prize-and-the-laureate-are-inseparable">https://www.nobelpeaceprize.org/press/press-releases/the-nobel-prize-and-the-laureate-are-inseparable</a>, See on <a href="https://news.ycombinator.com/item?id=46669404">Hacker News</a></p>
<div id="readability-page-1" class="page"><p>The medal and the diploma are the physical symbols confirming that an individual or organisation has been awarded the Nobel Peace Prize. The prize itself – the honour and recognition – remains inseparably linked to the person or organisation designated as the laureate by the Norwegian Nobel Committee. </p><div property="articleBody" id="body-37279"><p><span><span>A Nobel Peace Prize laureate receives two central symbols of the prize: a gold medal and a diploma. In addition, the prize money is awarded separately. Regardless of what may happen to the medal, the diploma, or the prize money, it is and remains the original laureate who is recorded in history as the recipient of the prize. Even if the medal or diploma later comes into someone else’s possession, this does not alter who was awarded the Nobel Peace Prize.</span></span></p>

<p><span><span>A laureate cannot share the prize with others, nor transfer it once it has been announced. A Nobel Peace Prize can also never be revoked. The decision is final and applies for all time.</span></span></p>

<p><span><span>The Norwegian Nobel Committee does not see it as their role to engage in day-to-day commentary on Peace Prize laureates or the political processes that they are engaged in. The prize is awarded on the basis of the laureate' contributions by the time that the committee’s decision is taken.</span></span></p>

<p><span><span>The Committee does not comment on laureates’ subsequent statements, decisions, or actions. Any ongoing assessments or choices made by laureates must be understood as their own responsibility.</span></span></p>

<p><span><span>There are no restrictions in the statutes of the Nobel Foundation on what a laureate may do with the medal, the diploma, or the prize money. This means that a laureate is free to keep, give away, sell, or donate these items.</span></span></p>

<p><span><span>A number of Nobel medals are displayed in museums around the world. Several Nobel laureates have also chosen to give away or sell their medals:</span></span></p>

<ul>
	<li><span><span><strong>Kofi Annan</strong> (Peace Prize 2001): In February 2024, his widow, Nane Annan, donated both the medal and the diploma to the United Nations Office in Geneva, where they are now permanently on display. She stated that she wished his legacy to continue inspiring future generations.</span></span></li>
	<li><span><span><strong>Christian Lous Lange</strong> (Peace Prize 1921): The medal of Norway’s first Nobel Peace Prize laureate has been on long-term loan from the Lange family to the Nobel Peace Center in Oslo since 2005. It is now displayed in the Medal Chamber and is the only original Peace Prize medal permanently exhibited to the public in Norway.</span></span></li>
	<li><span><span><strong>Dmitry Muratov</strong> (Peace Prize 2021): The Russian journalist sold his medal for USD 103.5 million in June 2022. The entire sum was donated to UNICEF’s fund for Ukrainian refugee children. This is the highest price ever paid for a Nobel Prize medal.</span></span></li>
	<li><span><span><strong>David Thouless</strong> (Physics Prize 2016): His family donated the medal to Trinity Hall, University of Cambridge, where it is displayed to inspire students.</span></span></li>
	<li><span><span><strong>James Watson</strong> (Medicine Prize 1962): In 2014, his medal was sold for USD 4.76 million. The controversial DNA researcher stated that parts of the proceeds would be used for research purposes. The medal was purchased by Russian billionaire Alisher Usmanov, who later returned it to Watson.</span></span></li>
	<li><span><span><strong>Leon Lederman</strong> (Physics Prize 1988): He sold his medal in 2015 for USD 765,002 to cover medical expenses related to dementia.</span></span></li>
	<li><span><span><strong>Knut Hamsun</strong> (Literature Prize 1920): In 1943, the Norwegian author Knut Hamsun travelled to Germany and met with Propaganda Minister Joseph Goebbels. After returning to Norway, he sent his Nobel medal to Goebbels as a gesture of thanks for the meeting. Goebbels was honoured by the gift. The present whereabouts of the medal are unknown.</span></span></li>
</ul>

<p><span><span><strong>Facts&nbsp;about&nbsp;the&nbsp;Gold Medal&nbsp;</strong></span></span></p>

<p><span><span>The&nbsp;medal&nbsp;is&nbsp;cast&nbsp;in 18-carat gold,&nbsp;weighs&nbsp;196 grams, and&nbsp;measures&nbsp;6.6&nbsp;centimetres&nbsp;in diameter. It&nbsp;was&nbsp;designed&nbsp;by&nbsp;the&nbsp;Norwegian&nbsp;sculptor&nbsp;Gustav Vigeland in 1901. The&nbsp;obverse&nbsp;features&nbsp;a&nbsp;portrait&nbsp;of&nbsp;Alfred Nobel,&nbsp;while&nbsp;the&nbsp;reverse&nbsp;depicts&nbsp;three&nbsp;naked&nbsp;men&nbsp;with&nbsp;their&nbsp;arms&nbsp;around&nbsp;one&nbsp;another’s&nbsp;shoulders, symbolising&nbsp;fraternity. The Latin&nbsp;inscription&nbsp;pro pace et&nbsp;fraternitate&nbsp;gentium&nbsp;means&nbsp;“for&nbsp;peace&nbsp;and&nbsp;the&nbsp;fraternity&nbsp;of&nbsp;nations.”&nbsp;</span></span></p>



</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Statement by Denmark, Finland, France, Germany, the Netherlands,Norway,Sweden,UK (501 pts)]]></title>
            <link>https://www.presidentti.fi/statement-by-denmark-finland-france-germany-the-netherlands-norway-sweden-and-the-united-kingdom-englanniksi/</link>
            <guid>46669025</guid>
            <pubDate>Sun, 18 Jan 2026 16:17:39 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.presidentti.fi/statement-by-denmark-finland-france-germany-the-netherlands-norway-sweden-and-the-united-kingdom-englanniksi/">https://www.presidentti.fi/statement-by-denmark-finland-france-germany-the-netherlands-norway-sweden-and-the-united-kingdom-englanniksi/</a>, See on <a href="https://news.ycombinator.com/item?id=46669025">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
<p>As members of NATO, we are committed to strengthening Arctic security as a shared transatlantic interest. The pre-coordinated Danish exercise ”Arctic Endurance” conducted with Allies, responds to this necessity. It poses no threat to anyone.</p>



<p>We stand in full solidarity with the Kingdom of Denmark and the people of Greenland. Building on the process begun last week, we stand ready to engage in a dialogue based on the principles of sovereignty and territorial integrity that we stand firmly behind.</p>



<p>Tariff threats undermine transatlantic relations and risk a dangerous downward spiral. We will continue to stand united and coordinated in our response. We are committed to upholding our sovereignty.</p>
</div></div>]]></description>
        </item>
    </channel>
</rss>